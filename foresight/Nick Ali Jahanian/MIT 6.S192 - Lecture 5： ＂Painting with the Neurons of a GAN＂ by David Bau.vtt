WEBVTT

00:00.000 --> 00:04.880
Hello, everyone, welcome back to our course, a deep learning

00:04.880 --> 00:12.840
for art, acetic and creativity. Today, it is our pleasure to

00:12.840 --> 00:18.280
have very a specialist speaker, David Bao, and I just let him

00:18.280 --> 00:22.640
to introduce him a little more, because I think it's very

00:22.640 --> 00:29.400
inspiring for many students, the path that he has come to

00:29.400 --> 00:35.040
this point and for future. Please go ahead. So I was, I

00:35.040 --> 00:43.920
want to give a little background since I am a post

00:44.120 --> 00:46.760
industry academic, I spent a bunch of years as a software

00:46.760 --> 00:50.760
engineer at Google before coming back to MIT. And I want to

00:51.000 --> 00:56.720
give a little bit of insight in my thinking there. So, you know,

00:56.720 --> 00:58.320
the reason it's really interesting to be in computer

00:58.320 --> 01:01.600
science right now is because the field is changing. The dream

01:01.640 --> 01:06.000
of having self programmed computers is one of the oldest

01:06.000 --> 01:12.200
dreams in computer science, but it's never been a reality. Even

01:12.200 --> 01:15.640
though we've studied machine learning for a long time, I think

01:15.640 --> 01:19.120
that until just a few years ago, machine learning was really

01:19.120 --> 01:22.920
more accurately called, it would have been more accurately

01:22.920 --> 01:27.520
called the art of accurate counting. You know, statistics,

01:29.160 --> 01:33.160
you know, understanding the statistics of, you know, how

01:33.160 --> 01:37.080
frequent words are and by grams or, you know, certain image

01:37.080 --> 01:41.760
statistics or something like that. And, and, and if you if you

01:41.760 --> 01:45.600
understand statistics well, then, then, then, then, you know,

01:45.600 --> 01:50.920
you could do some nice tricks. But I think that until recently

01:50.960 --> 01:55.760
really calling these things sort of self programmed systems

01:56.400 --> 01:58.680
would have been an overstatement. But I don't think it's

01:58.680 --> 02:01.960
really an overstatement anymore. I think that these machine

02:01.960 --> 02:07.640
learning models are really learning non trivial things. And

02:07.680 --> 02:10.600
it leads to all sorts of questions about, you know,

02:10.680 --> 02:14.840
what should we be doing as programmers? What does it mean

02:14.840 --> 02:17.280
to do software engineering? And so I thought it was very

02:17.280 --> 02:20.840
interesting time to come back to academia. That's, that's why

02:20.880 --> 02:24.920
I'm here. And I actually think that that's one of the choices

02:24.960 --> 02:28.200
you face when you're trying to decide between industry and

02:28.200 --> 02:33.400
academia. And I think in industry, you will have lots of

02:33.400 --> 02:37.920
resources to make things work to make the next widget or the

02:37.920 --> 02:42.160
application. And, you know, there are great places, Google is a

02:42.160 --> 02:44.840
great place, we can really push state of the art in that and

02:44.840 --> 02:49.480
do really neat stuff. I think that there's less of a push in

02:49.480 --> 02:53.800
industry to ask the question, Why? You know, why do things

02:53.800 --> 02:57.440
work? Why are we doing what we're doing? Where is it going to

02:57.440 --> 03:00.040
lead in either unintended consequences and things like

03:00.040 --> 03:02.800
that? You know, we, we tend not to ask those questions too much

03:02.800 --> 03:06.160
industry, because there's so much to emphasize on, you know, the

03:06.160 --> 03:12.640
how of how to how to get it to work. And so, and so, so I

03:12.640 --> 03:17.020
thought it was a time to, to switch tracks and start asking

03:17.020 --> 03:19.200
why because the field is changing so dramatically. And I

03:19.200 --> 03:23.280
think that, you know, I'd encourage people who have an

03:23.280 --> 03:28.840
interest in these type of questions to, to realize you

03:28.840 --> 03:31.480
can really make a real contribution by taking the

03:31.480 --> 03:36.480
academic track as well. So okay, so let me introduce my talk.

03:36.480 --> 03:39.760
So it's about painting with neurons of general adversarial

03:39.760 --> 03:45.720
networks. It comes out of work from asking why, you know, why

03:45.760 --> 03:53.600
do these networks do what they do? And so, so let me, let me

03:53.600 --> 03:58.320
advance here. Am I in full screen? So do you see the, do you

03:58.320 --> 04:01.440
see the like the full screen slideshow I can't see what I'm

04:01.440 --> 04:04.080
projecting? Or do you see like all my notes and all that stuff?

04:04.200 --> 04:08.120
Yeah, I can see it. But also maybe a student can tell us.

04:09.400 --> 04:10.000
Yeah, okay.

04:10.360 --> 04:11.000
That's fine.

04:11.400 --> 04:16.480
Is everything okay? Yeah, it's a full screen slide. Hopefully,

04:16.520 --> 04:21.680
it's okay. So, so, okay. So the main problem that we're looking

04:21.680 --> 04:25.520
at here, and I'm not sure why the, the images are overlapped in

04:25.520 --> 04:29.400
the right way. Hopefully, the layout will get fixed. So we're

04:29.400 --> 04:33.920
going to next slides. But the, the, the, the, the main problem

04:34.160 --> 04:39.160
surrounding my talk is image generation. And so, for the last

04:39.200 --> 04:41.560
few years, there's been this question, how do you make a

04:41.560 --> 04:46.960
state of the art program to generate realistic images? And,

04:47.320 --> 04:49.560
you know, the general process is first you want to collect a

04:49.560 --> 04:52.640
data set of real images, like these pictures of buildings on

04:52.640 --> 04:59.160
the right. And, and then you want to, you know, train some sort

04:59.160 --> 05:02.200
of program, some sort of generator network to generate

05:02.760 --> 05:06.040
those programs. And so, so, you know, it's been a puzzle. There's

05:06.040 --> 05:08.920
a lot of different ways you could imagine doing this. And so

05:09.760 --> 05:12.680
people have been puzzling, how do you train such a thing? How do

05:12.680 --> 05:15.240
you even supervise it? You know, what should the, what should

05:15.240 --> 05:19.560
the inputs and the outputs of the network be? And, and, and the

05:19.560 --> 05:22.200
thing that has really been working the best in recent

05:22.200 --> 05:24.640
years is, you know, in architecture, you guys have all

05:24.640 --> 05:28.760
heard of called GANs, generative adversarial networks. And the

05:28.760 --> 05:32.400
trick for GANs is to reduce it down to a simpler problem that

05:32.400 --> 05:36.240
we know what we're doing. And so the simpler problem that

05:36.240 --> 05:40.160
they're recognized when designing GANs was that

05:40.440 --> 05:42.880
generating images, we don't really know how to do, but

05:42.880 --> 05:46.320
classifying images, gosh, that is an easy problem. We can

05:46.320 --> 05:51.240
classify images. And so, so what we could do is we could train a

05:51.240 --> 05:55.960
classifier on this really easy task, which is given two sets

05:56.000 --> 06:01.040
of pixels, which image is real, and which image is not a real

06:01.040 --> 06:05.480
photograph. And it turns out that for most arrangements of

06:05.480 --> 06:08.560
pixels, this is a very easy task to train a discriminator on

06:08.560 --> 06:11.560
it gets very good, you know, very quickly, we'll start getting

06:11.560 --> 06:16.520
100% accurately on that. And so, so but the neat thing is that

06:16.520 --> 06:18.320
once we have a discriminator that can tell the difference

06:18.320 --> 06:22.360
between a fake image and a real image, then we can hook it up

06:22.360 --> 06:25.600
to our generator, and we can say, All right, we didn't know how

06:25.600 --> 06:29.560
to tell you, generator, how to make a real image. But you know

06:29.560 --> 06:32.480
what this discriminator can tell you, because all you have to do

06:32.480 --> 06:36.520
is generate patterns of pixels that fool the discriminator, if

06:36.520 --> 06:40.520
you can make the discriminator think it's real, then it must be

06:40.800 --> 06:46.960
better than random. Now, the problem is that, even though the

06:46.960 --> 06:49.920
discriminator can get very accurate at telling what's real,

06:51.200 --> 06:54.720
they, the generator will also be very good at learning how to

06:54.720 --> 06:57.800
fool the discriminator without working very hard, it'll realize

06:58.000 --> 07:00.960
that aha, the only thing I need to do to make the discriminator

07:00.960 --> 07:04.360
think is real is put some blue sky in there and put some texture

07:04.360 --> 07:07.440
that kind of looks like, you know, building texture. And, and

07:07.440 --> 07:09.640
the discriminator will say, Well, that totally looks real,

07:09.640 --> 07:13.120
there's a sky, you know, there's, there's the right, the right

07:13.120 --> 07:16.120
colors for buildings and some vertical lines and things. Ah,

07:16.120 --> 07:18.760
that's totally real. But as a human, we look at that, we think,

07:18.760 --> 07:21.880
Oh, that's not a very realistic image at all. So the trick is to

07:21.880 --> 07:24.560
iterate this process to go back and forth after the generator

07:24.560 --> 07:29.000
can generate sort of halfway looking real images, then have

07:29.000 --> 07:32.680
the discriminator say, Ah, well, that's actually fake. And

07:32.680 --> 07:35.120
we're going to tell the difference between those new fakes,

07:35.120 --> 07:38.200
those better fakes, and actual real photographs, and the

07:38.200 --> 07:41.120
discriminator has to now work harder at getting better. And so

07:41.120 --> 07:45.000
if you, if you alternate these processes, then, then you end up

07:45.000 --> 07:48.280
very conversion to very, very good generators that can generate

07:48.280 --> 07:55.600
very realistic images. And they, you know, the typical learning

07:55.600 --> 07:59.440
process is actually just to do only one step of iteration

08:00.040 --> 08:03.240
between the discriminator and generator and just alternate that.

08:03.240 --> 08:06.360
So by the time you're done, you've played this game, you

08:06.360 --> 08:08.520
know, millions and millions of times back and forth between

08:08.520 --> 08:11.200
the generator and the discriminator. But the new thing

08:11.240 --> 08:14.360
that's happening here is that it can generate these images that

08:14.360 --> 08:20.800
look very realistic in the end. But let's see. So Oh, here's

08:20.800 --> 08:25.000
another picture. So we'll get this images out that look very

08:25.000 --> 08:28.400
realistic in the end. And we'll get this generator, which is

08:28.400 --> 08:32.440
just a deterministic function that takes actually the input of

08:32.440 --> 08:36.480
the generator is actually just a random vector. So we'll take

08:36.480 --> 08:39.680
these relatively small random vectors like 512 dimensional

08:39.680 --> 08:42.480
random vector, and we'll put it into this thing. And it's been

08:42.480 --> 08:45.760
trained so that no matter what it outputs, it will look very

08:45.760 --> 08:48.720
realistic, like this example image here. Or if I change a

08:48.720 --> 08:51.680
vector, I'll get a different image out and it will again look

08:51.680 --> 08:54.440
very realistic, even if it looks completely different. And so

08:54.480 --> 08:57.080
it's just a deterministic function that really wants to

08:57.080 --> 09:01.920
make realistic images. And, and so here's like a sample of like

09:02.320 --> 09:05.640
output from a generator. And you can see that after millions of

09:05.640 --> 09:09.920
these sort of generative training steps, where it's

09:09.920 --> 09:12.800
pitted against a discriminator, it actually gets to be pretty

09:12.800 --> 09:16.840
good. And so this is a style game v2. It's a model that was

09:16.840 --> 09:22.040
published last year. And, and it's, you know, currently the

09:22.040 --> 09:26.240
state of the art in generating realistic images of certain

09:26.800 --> 09:31.000
certain types of image distributions. And, and so when

09:31.040 --> 09:33.840
when you look at a collection of images like this, you might

09:33.840 --> 09:36.880
think, actually, the first time I looked at the output of some of

09:36.880 --> 09:42.840
these state of the organs, I was confused between the training

09:42.840 --> 09:46.360
set, and the generated output, this is not the training set,

09:46.360 --> 09:51.280
this is actually what the generator is producing. And so, so

09:51.280 --> 09:55.600
you see all sorts of interesting effects here. And so the one of

09:55.600 --> 10:00.200
the questions to ask is, what the heck is the model doing

10:00.200 --> 10:03.280
inside? Can we understand the underlying algorithm? And what

10:03.280 --> 10:06.760
the characteristics of that algorithm is like, why does this

10:06.760 --> 10:13.040
work? And so one of the funny things that you'll notice is

10:13.480 --> 10:16.680
that some of the images have these strange artifacts, like

10:16.680 --> 10:22.440
take a look at this one here. So this, this scan is pretty good.

10:22.560 --> 10:28.360
It's this generator is so good that it actually has noticed that

10:28.360 --> 10:33.720
the training distribution that is imitating has some percentage

10:33.720 --> 10:39.040
of images that were stolen off of shutter stock. And they still

10:39.040 --> 10:43.600
have the watermark on them. And, and, and, and the generator

10:43.600 --> 10:46.660
says, well, if I want to make things look realistic, I better

10:47.140 --> 10:51.580
put watermarks on some percentage of my images too. It

10:51.580 --> 10:55.740
learns it's got to protect its own copyright. So, so it, it

10:55.740 --> 11:01.900
does that. And so something like 6% of the output images from

11:02.300 --> 11:05.140
state of the art style, again, will have these kind of

11:05.140 --> 11:09.620
artifacts that show the same type of watermarks that were on

11:09.620 --> 11:14.180
the training set. This is the Elson Church training set. And

11:14.220 --> 11:17.780
so, so yeah, this kind of watermarks like this. But the

11:17.780 --> 11:22.260
reason I thought this was cool was that it, it's this very

11:22.260 --> 11:25.740
clear thing that the image generator does, but it doesn't

11:25.740 --> 11:29.580
always do it. Like most of the time when it generates images,

11:29.860 --> 11:33.260
it generates images without a watermark, but sometimes you get

11:33.260 --> 11:37.900
these watermarks. And so, and so it's, it's almost like this

11:37.900 --> 11:44.020
binary decision. It's like, there must be a switch that the

11:44.020 --> 11:47.940
network has at some point where it decides whether it's going to

11:47.940 --> 11:51.300
put a watermark on an image or not. And so we can kind of ask

11:51.300 --> 11:53.900
the question, where's that switch? Is there a neuron

11:53.900 --> 11:58.420
somewhere in this network, which is, which is controlling the

11:58.420 --> 12:02.700
watermarkness. And so, so I went on a hunt for this, this,

12:02.700 --> 12:05.140
this particular network has about 30 million parameters,

12:05.140 --> 12:09.260
which sounds like a lot, but it's just a deterministic computer

12:09.260 --> 12:14.300
program in the end. And, and it's not that hard to go hunting

12:14.340 --> 12:17.780
for things like this, you just, you can make an algorithm that

12:18.380 --> 12:21.020
has a heuristic that determines whether it's a watermark or not

12:21.020 --> 12:25.340
and just go hunting for, for things that correlate with that.

12:25.900 --> 12:32.060
And so I'll show you what I found. So at layer five, I found

12:32.060 --> 12:37.140
this very interesting neuron that did correlate with watermarks

12:37.140 --> 12:42.180
a lot. It was activating whenever images look like this in

12:42.180 --> 12:46.340
the end. And, and not only that, but because it's at layer five,

12:46.980 --> 12:51.340
it has a has a location for where the image activates. And I'll

12:51.340 --> 12:55.340
show you where, where it's activating. So, so this neuron

12:55.340 --> 12:58.220
is activating, you know, at these middle parts of images,

12:58.220 --> 13:00.820
whenever the image is showing a watermark. And there are other

13:00.820 --> 13:03.540
neurons that have similar behavior, like so for example,

13:03.540 --> 13:08.900
there's this neuron 234 at the same layer. And it activates in

13:08.900 --> 13:11.580
regions like this, both in the middle watermark and the bottom

13:12.020 --> 13:16.820
bar that shows up. And there's about, if you hunt through the

13:16.820 --> 13:21.620
neural network, you find about 30 neurons that are similar and

13:21.660 --> 13:28.220
behave like this. And so that's, that's pretty cool. So then the

13:28.220 --> 13:31.780
question is, well, do these things really act like a switch?

13:31.820 --> 13:35.260
What if we've removed these neurons from the network? What if

13:35.260 --> 13:39.060
we force them all off? What if we turn, what if we force these

13:39.060 --> 13:43.220
neurons to be off all the time? That will happen. So normally,

13:43.220 --> 13:47.420
we think of these neural networks as completely opaque

13:47.420 --> 13:51.980
systems. We train them end to end, they're just, you know,

13:52.060 --> 13:55.420
these big black box functions. And we normally think of the

13:55.420 --> 13:58.780
functions as computing things where everything depends on

13:58.780 --> 14:00.820
everything. And so if you randomly rip through the

14:00.820 --> 14:05.180
function and remove some of its operations, then maybe you

14:05.180 --> 14:08.980
expect to get total nonsense out just garbage or noise. But we

14:08.980 --> 14:11.580
found these particular neurons that really correlate to this

14:11.580 --> 14:14.100
thing. So let's see what happens when we turn them off. Do we

14:14.100 --> 14:18.180
get anything intelligible at all? So this is what the network

14:18.180 --> 14:21.780
generated before these are the watermark images I showed you

14:21.780 --> 14:25.140
before. And I'll show you what happens if I turn off these 30

14:25.700 --> 14:31.060
watermark neurons. So I'm going to give the network the same

14:31.060 --> 14:35.020
input. But turn off these neurons during its computation,

14:35.020 --> 14:37.060
and you can see what the output looks like. So you can see

14:37.500 --> 14:41.020
before chain, you know, forcing these neurons off and after

14:41.020 --> 14:45.660
forcing those neurons off. The images are still very

14:45.780 --> 14:50.340
intelligible, they look realistic still. But now the

14:50.380 --> 14:54.620
watermarks are gone. So I thought I was when I when I saw

14:54.620 --> 14:58.100
this, I was pretty excited, because it's like, Oh, there are

14:58.100 --> 15:00.980
switches inside the networks. And these networks are doing all

15:00.980 --> 15:03.940
sorts of amazing things, not just like showing watermarks. You

15:03.940 --> 15:06.380
know, so when I first found this, it was on Progressive GAN,

15:06.380 --> 15:09.500
which is a year earlier than a couple years earlier, the images

15:09.500 --> 15:12.980
didn't look quite as good. But but still in Progressive GAN,

15:12.980 --> 15:17.460
they do all sorts of amazing things, like they will arrange a

15:17.460 --> 15:21.700
scene with a river and trees and grass and, you know, building

15:21.700 --> 15:25.140
architectures with all sorts of different features. And you can

15:25.140 --> 15:28.460
ask, you know, is there a switch to turn on and off clouds in

15:28.460 --> 15:32.540
the skies or switch to turn on and off trees or windows and

15:32.540 --> 15:40.500
buildings? And, and so I went hunting for that. And, and, and

15:40.500 --> 15:44.220
the way I went hunting is I tested every neuron one at a time

15:44.220 --> 15:46.780
I inverted the test. So basically, I look at each neuron,

15:47.140 --> 15:51.020
and I say, Where is it activating? And, and I asked a

15:51.020 --> 15:55.420
question, is it activating an interesting part of different

15:55.420 --> 15:59.020
images? So for example, if I took this one neuron here, and I

15:59.020 --> 16:02.100
see where it's activating when it's generating this image, you

16:02.100 --> 16:06.340
can see it's very hot on the right and on the left, but not

16:06.340 --> 16:11.580
much up in the sky. And on this very same neuron, when we

16:11.580 --> 16:16.060
generate a different image with a different input, this very same

16:16.060 --> 16:19.140
neuron is not activating very much anywhere in this this image.

16:19.980 --> 16:23.460
But if we generate another image, then it will activate in a

16:23.460 --> 16:28.140
specific area here, mostly on the lower left part of this image.

16:28.140 --> 16:30.500
And you can see what's on the lower left. There's a, there's a

16:30.500 --> 16:34.340
tree there. And so it kind of gives you the hypothesis that

16:34.340 --> 16:37.700
maybe this neuron is correlated with trees somehow. So

16:37.700 --> 16:40.980
obviously, we can, we can do this, we can collect this

16:41.220 --> 16:45.860
information over thousands of examples of generated images by

16:45.860 --> 16:50.660
looking at where the neuron is activating, we can ask what what

16:50.660 --> 16:54.300
kind of thing is in the image, what kind of objects, what are

16:54.300 --> 16:58.180
the semantics of the image in the location that the the neurons

16:58.180 --> 17:03.740
are in. And we can just repeat that test process, you know,

17:03.780 --> 17:08.340
thousands of times to see if the neuron is agreeing with any

17:08.340 --> 17:12.700
particular kind of semantics that are in the images. So if, if

17:12.700 --> 17:15.740
the, if the neurons are showing up where the trees are all the

17:15.740 --> 17:19.780
time, we can just count and see if if that's if that's true in

17:19.780 --> 17:23.820
general. And we can also look for correlations with other

17:23.820 --> 17:27.580
things. So what I did is I, I searched for correlations with

17:27.580 --> 17:30.340
thousands of different, you know, hundreds of different,

17:31.140 --> 17:34.340
different types of semantics and object classes, different parts

17:34.340 --> 17:37.660
of buildings or, or objects or other things that can show up

17:37.660 --> 17:41.100
in a scene. And so what do we find? Well, we do find, you

17:41.100 --> 17:43.260
know, there's a neuron that correlates with trees, just like

17:43.260 --> 17:46.300
the one I was showing you. There's actually a few that are

17:46.300 --> 17:49.380
like that. And there's also neurons that correlate with

17:49.380 --> 17:53.540
other things like domes, or, or other building parts like

17:53.540 --> 17:57.980
windows and doors. And, and if you change the model to look at

17:57.980 --> 18:01.220
other things, then you can find neurons that correlate with

18:01.220 --> 18:06.780
things like windows, or chairs, or other things that they show

18:06.780 --> 18:10.420
up in, in the scene. And so this is actually pretty neat,

18:10.420 --> 18:17.260
because this model was trained unsupervised by any labels. All

18:17.260 --> 18:22.300
we did is we told it, generate realistic looking scenes,

18:22.660 --> 18:28.700
realistic looking photos. And, and, and we did not train it

18:28.700 --> 18:33.460
with any labels, we didn't tell it that these are photos of

18:35.380 --> 18:39.100
scenes that have big windows, and these are photos of scenes

18:39.100 --> 18:41.980
that have little windows or anything like that. Or, or, or

18:41.980 --> 18:45.300
here's where the windows are. But what happened was, the

18:45.300 --> 18:49.220
network discovered that it had to, you know, learn a

18:49.220 --> 18:55.900
representation, where windows are represented differently from

18:55.900 --> 19:02.260
the way chairs are represented. But somehow, even though, you

19:02.260 --> 19:04.500
know, windows can look very different from each other and

19:04.500 --> 19:06.860
chairs can look very different from each other, that the

19:06.860 --> 19:09.380
network has this represent this this component of this

19:09.380 --> 19:16.580
representation, this neuron that activates on all these chairs,

19:16.820 --> 19:19.660
despite the amazing amount of diversity that it shows, like

19:19.700 --> 19:23.700
none of these chairs really look similar to each other, they

19:23.700 --> 19:26.060
have different colors and different textures, and they're

19:26.060 --> 19:30.140
oriented in different ways. And yet, the same neuron is

19:30.140 --> 19:33.260
activating on all of them, the same thing goes for other

19:33.260 --> 19:37.100
things that show up in these images. So does anybody have

19:37.100 --> 19:40.060
any questions about, about, about this? I'd love this to be a

19:40.060 --> 19:41.860
little bit more interactive than the way I'm doing the talk.

19:41.860 --> 19:44.860
So let me open the floor for a question for a minute.

19:47.420 --> 19:52.140
Has anybody tried playing with the internals of GANs yet? I'd

19:52.140 --> 19:58.820
love to see if has anybody like generated images using a GAN

19:58.820 --> 19:59.340
before?

20:01.900 --> 20:08.700
No, but I do have a question. Yes. So what was like the end goal

20:08.700 --> 20:13.460
or the larger reason behind finding all of these neurons

20:13.900 --> 20:18.820
that correspond to different objects or features?

20:19.540 --> 20:24.300
Well, when I was originally looking at it, my original goal

20:24.300 --> 20:30.340
was just to understand how these models did their computation.

20:30.540 --> 20:36.180
So asking the question why. But the neat thing is that after I

20:36.180 --> 20:39.740
found this structure, then it became clear that there are

20:39.740 --> 20:44.780
new applications that you can build on top of it. And I think

20:44.780 --> 20:46.780
that's one of the cool things that comes out of this sort of

20:46.780 --> 20:50.660
academic style inquiry is, you know, originally, I was just

20:50.660 --> 20:52.980
looking to make catalogs like this. This is a catalog of all

20:52.980 --> 20:55.540
the different types of correlations that I found with

20:55.540 --> 20:58.220
neurons inside a model for generating kitchens, and the

20:58.220 --> 21:01.860
kinds of, you know, the patterns you see. And, and, you know,

21:01.860 --> 21:04.500
I've done this before for classifiers. And, you know, when

21:04.500 --> 21:06.700
you do for generators, you get different patterns. And so I was

21:06.740 --> 21:09.980
just really interested in making these maps of seeing what is

21:09.980 --> 21:14.460
computed at what layer, you know, where and how accurately. So

21:14.460 --> 21:19.020
this is, you know, the progressive gain has, depending

21:19.020 --> 21:22.500
on the resolution has about 15 layers. And if you sort of

21:22.500 --> 21:25.740
chart what you see in different layers, you can see this this

21:25.740 --> 21:28.460
really interesting thing phenomenon where it's in the

21:28.460 --> 21:32.940
middle layers that you get these highly semantic correlated

21:32.940 --> 21:36.300
neurons. But then as you get to the later layers, then they

21:36.340 --> 21:39.260
tend to be more physical. And there's not as many semantic

21:39.260 --> 21:42.380
objects. So it's like in layer five, we have things that really

21:42.380 --> 21:45.820
correlate with ovens and chairs and windows and doors, even

21:45.820 --> 21:49.860
though like a window kind of looks like an oven. The model

21:50.220 --> 21:53.700
clearly has different neurons that correlate with windows from

21:53.700 --> 21:58.700
ones that look like ovens. And so so that so that's that's that

21:58.700 --> 22:02.500
so I was originally interested in just mapping things out. But

22:03.460 --> 22:07.420
the correlations were so striking that it leads to these

22:07.420 --> 22:10.580
interesting applications that you can build. And I can I'll show

22:10.580 --> 22:16.220
you some in the next step. Let me before I do that, let me see

22:16.220 --> 22:18.220
if anybody else has a question as well.

22:21.340 --> 22:25.420
Yeah, David, I was hoping that you could also show us the

22:25.420 --> 22:29.740
application at some point, which I think these are very good to

22:29.780 --> 22:34.980
see why you asked this question. I mean, yes, it's more

22:35.780 --> 22:38.620
That's great. Let me let me zoom out to the application. So

22:40.060 --> 22:43.220
so the the neat thing is that just like we could turn off

22:43.220 --> 22:47.860
watermarks, we can turn on and off things in image generation.

22:47.860 --> 22:50.060
So for example, if I find all the neurons that correlate with

22:50.060 --> 22:53.740
trees, and I turn them off, you can see what happens. I'm going

22:53.740 --> 22:57.900
to turn them off sort of one at a time here. And so originally,

22:58.340 --> 23:01.460
the image will just be generated this. But if I turn off some

23:01.460 --> 23:05.980
tree neurons, you can see that we can actually remove the trees

23:05.980 --> 23:09.460
from the scene. And the cool thing is that this is different

23:09.460 --> 23:13.420
from Photoshop. If you went and you tried to erase trees from an

23:13.420 --> 23:17.180
image, then you'd have this puzzle of what would happen

23:17.220 --> 23:20.300
about stuff that was occluded by the trees like what's going on

23:20.300 --> 23:24.660
behind there. And so this image generator is actually it's got

23:24.700 --> 23:27.940
this latent model that has an understanding of what the scene

23:27.940 --> 23:31.340
is. And so, and even has an understanding of things that is

23:31.340 --> 23:35.100
not explicitly drawing. So if you remove the trees from the scene,

23:35.380 --> 23:39.220
then it'll come up with a reasonable looking, you know,

23:39.460 --> 23:44.340
image to draw what was behind the trees, or you can do the

23:44.340 --> 23:49.340
opposite. Which is you can take neurons that were not originally

23:49.340 --> 23:53.780
on in a generated scene and turn them on. So if I take a set of

23:53.780 --> 23:56.660
neurons that correlate with doors, and I turn them on in a

23:56.660 --> 23:59.540
certain location, and you can see what happens in the generated

23:59.540 --> 24:02.380
image, you know, I'll get this door in the scene, not only

24:02.380 --> 24:05.900
will it just be a door, but it'll be it'll have like an

24:05.900 --> 24:11.220
appropriate size and your orientation and style for for

24:11.220 --> 24:13.940
the building that it's in. So if I take exactly the same neurons,

24:14.460 --> 24:17.820
and I activate them in a different location like here in

24:17.820 --> 24:21.140
this building, then even though it's exactly the same neurons

24:21.140 --> 24:24.740
exactly the same activation that I've done, I get a different

24:24.740 --> 24:28.100
door that is like a much smaller has a different style and so on.

24:28.100 --> 24:31.660
It's appropriate to the building that it's in. If I if I try to

24:31.660 --> 24:35.140
put a door in a place that would make sense, like by turning on

24:35.140 --> 24:39.820
neurons up in the sky, then it like will like not do anything.

24:40.860 --> 24:44.300
This is this is the actual output of what happens if I turn on

24:44.300 --> 24:48.180
the exact same neurons up in this location. So there's a lot of

24:48.220 --> 24:52.340
interesting context sensitivity that you can measure. But one of

24:52.340 --> 24:54.380
the cool things that you can do is you can actually hook this up

24:54.380 --> 24:59.900
to a paintbrush user interface, like I can find neurons that

24:59.900 --> 25:03.700
correlate with domes or doors or things. And if I want to add

25:03.700 --> 25:06.300
doors to a building, I can just sort of paint them on. And the

25:06.300 --> 25:09.380
doors will show up and you can see the orientation of the doors

25:09.380 --> 25:12.980
is appropriate to the wall that you put them in. If I just say I

25:12.980 --> 25:16.780
want trees, it'll put trunks and leaves, you know, in the right

25:16.780 --> 25:19.100
place in the trees or plants with a plant, plant them on the

25:19.100 --> 25:22.700
ground. If I take grass and I can turn the grass neurons off and

25:22.700 --> 25:26.180
remove grass from the scene and it'll come up with what the

25:26.180 --> 25:29.260
scene should look like instead. And so I can kind of do these

25:30.220 --> 25:33.900
semantic manipulations directly. Oh, here we're turning on domes

25:33.900 --> 25:40.220
and you can see it will turn the top of the the church from a

25:40.220 --> 25:44.140
spiral to a dome, but it also sort of stitched the dome into

25:44.140 --> 25:47.740
place to make it look good here. I'm removing grass again. We

25:47.740 --> 25:53.020
can like put a door in the scene. And if I if I put you know,

25:53.020 --> 25:56.540
sort of put a door in the wall, then it'll it'll come up with

25:56.540 --> 25:59.900
like the appropriate location and style orientation for the door

25:59.900 --> 26:04.940
even if I draw very roughly. So when I'm drawing, every time I

26:04.940 --> 26:07.900
touch the surface here, what I'm really doing is I'm just turning

26:07.900 --> 26:13.140
on a few neurons. And and I'm letting the the math of the

26:13.140 --> 26:19.780
GAN generator deal with all of the the details of how to arrange

26:19.780 --> 26:22.180
the actual pixel. So does that does that sort of give you a

26:22.180 --> 26:25.260
sense? Does that answer your question for like, you know,

26:25.260 --> 26:28.340
what kinds of things you can do with this by understanding what's

26:28.340 --> 26:31.300
going on in the interior of the model? Maybe now I should stop

26:31.300 --> 26:36.060
it. Oh, yes, go ahead. Are these different neurons for like

26:36.060 --> 26:41.060
doors in different areas? No, no. So when I when you click on the

26:41.060 --> 26:47.780
door button on the left, I am picking 20 neurons that are

26:47.780 --> 26:51.540
the door neurons. So by doing the statistical analysis ahead of

26:51.540 --> 26:55.340
time that I showed you earlier, I've identified 20 neurons that

26:55.340 --> 26:59.860
correlate very strongly with the presence of doors. And when you

27:00.020 --> 27:04.020
click on the button on the left, I am picking those neurons. Now,

27:04.220 --> 27:06.540
it's a convolutional network. So there's this translation

27:06.540 --> 27:11.460
and depends those neurons appear at every pixel. And so what you

27:11.460 --> 27:14.660
can do is you can just turn on those neurons in random pixels

27:14.660 --> 27:15.460
that you touch.

27:16.980 --> 27:19.180
Changing where the neurons are, that was what I didn't

27:19.180 --> 27:21.860
understand. Does that make sense? So, so because it's a

27:21.860 --> 27:24.900
convolutional network, so it's actually it's it's like the neural

27:24.900 --> 27:29.020
network is cloned at every location. It's the same neural

27:29.020 --> 27:32.820
network that's being used to process every, you know, patch

27:32.820 --> 27:37.700
or patch of pixels in the image. And, and so if I asked for a

27:37.700 --> 27:40.300
door in a place that wouldn't really make sense, then it

27:40.300 --> 27:43.180
won't put a door there. If I asked for a door in a place that

27:43.180 --> 27:45.420
makes sense, it'll make a big intervention, it'll stick a big

27:45.420 --> 27:48.500
door there, which you can see. So I could be very rough about

27:48.500 --> 27:53.660
where I put a door and it'll like put it in the right place. So

27:53.660 --> 27:59.100
that's that's the idea. So let me let me zoom around here, I'll

27:59.100 --> 28:04.220
show you a couple other things that you can do. So now there's

28:04.220 --> 28:07.340
some limitations to this. And I'll just show you some of the

28:07.340 --> 28:09.300
techniques that you can use to get around the limitation. So one

28:09.300 --> 28:12.100
of the problems is that, you know, we can do all this cool

28:12.100 --> 28:15.900
editing, but we can do this editing of a randomly generated

28:15.900 --> 28:21.220
image. And, and so, so when I posted this demo on on the web,

28:21.260 --> 28:23.660
you know, an artist called me and said, Hey, you know, I love

28:23.660 --> 28:27.060
how you can edit images, I can edit this image of a kitchen

28:27.060 --> 28:30.780
here. But that's not the kitchen I want to edit, I want to edit

28:30.780 --> 28:36.100
my own kitchen, right? Like here's a photo of my kitchen. And I

28:36.100 --> 28:41.740
want to edit that one. And I had to explain to them, you know,

28:41.740 --> 28:45.660
they said, Oh, can you just load into your demo? My my kitchen

28:45.660 --> 28:49.340
instead of yours. And I had to explain, no, no, no, that's not

28:49.340 --> 28:53.460
how GANs work. They're unconditional generators. You

28:53.460 --> 28:59.540
know, you give it a random vector of 512 numbers. And it

28:59.540 --> 29:03.900
decides what image to make. And then once it decides what image

29:03.900 --> 29:06.780
to make, then you can edit it. And so I'm sorry, I can't edit

29:06.780 --> 29:09.980
your kitchen. And so they were very disappointed by that

29:09.980 --> 29:12.140
because they had all sorts of ideas of things they wanted to

29:12.140 --> 29:18.260
do. And so now the problem is that, you know, the problem

29:18.260 --> 29:22.380
could be solved if we could find the random vector, some random

29:22.420 --> 29:26.420
vector that that output the kitchen image or the specific

29:26.420 --> 29:31.380
real photo that I wanted. The problem is how do I find it 512

29:31.380 --> 29:36.860
dimensional vectors as pretty big vector space. And and so I

29:36.860 --> 29:40.700
don't know if my GAN can actually generate this image or not. So

29:40.700 --> 29:44.260
one of the things you can do is you can just treat this as a as

29:44.260 --> 29:47.500
an inversion problem. You can take the neural network and you

29:47.500 --> 29:51.900
can learn how to run it backwards. Basically, you know,

29:52.300 --> 29:55.340
think of the neural network as a function G, and you want to

29:55.340 --> 29:58.300
learn G inverse. So you can treat that as another training

29:58.300 --> 30:00.580
problem. And there's a bunch of tricks and I won't go into all

30:00.580 --> 30:04.740
the tricks here. But but basically, the idea is that you

30:04.740 --> 30:08.140
can actually find a Z that comes closest to generating your image

30:08.140 --> 30:13.580
by by training and doing a couple other tricks. And you can

30:13.580 --> 30:18.660
actually get a Z that will generate your image pretty

30:18.660 --> 30:23.460
closely. But the thing that's a little bit sad is it also

30:23.460 --> 30:27.860
reveals things that the network cannot do. So so this network

30:28.740 --> 30:31.660
is capable of generating this image that I'm showing you here.

30:32.220 --> 30:35.340
But the original kitchen that I started with look like this. So

30:35.340 --> 30:37.420
you can see what the differences are. I've lost a lot of

30:37.420 --> 30:42.700
stuff. Right. So, you know, I can use the GAN to edit this

30:42.700 --> 30:46.940
image. But this image is not exactly what I started with. And

30:46.940 --> 30:53.980
so. So one of the pieces of science that that I did is I

30:53.980 --> 30:58.260
asked a question, you know, is there some way that we can

30:58.260 --> 31:01.500
actually make this work? Can we actually, you know, get the

31:01.500 --> 31:06.620
network to output a real photo that that the user gave us? We

31:06.620 --> 31:10.060
get the network to output this sort of simplified version of

31:10.060 --> 31:15.060
it. It turns out that if I modify the weights of the

31:15.060 --> 31:21.460
network, I can actually fine tune the network to get it so that

31:21.460 --> 31:24.900
a very, very nearby network with weights that are almost the

31:24.900 --> 31:31.060
same as the original actually hits this target image. Exactly.

31:31.820 --> 31:36.900
And so so there's a bunch of details in the right way of

31:36.900 --> 31:39.620
doing this. But it turns out that, you know, you don't actually

31:39.620 --> 31:42.780
have to change much if you change the fine grained weights

31:42.900 --> 31:47.300
of a network. You can you can change a lot of the details of

31:48.340 --> 31:52.500
what images actually get generated. And and and if you

31:52.540 --> 31:57.180
are given a target image to get you can actually tweak tweak any

31:57.180 --> 32:01.740
network to generate exactly that target image if we want. And

32:01.740 --> 32:02.100
so

32:04.100 --> 32:07.700
so you know, so yeah, we can get all the objects back. But the

32:07.700 --> 32:09.820
new thing is we haven't really changed the network much. So we

32:09.820 --> 32:13.300
can still do editing. So like if we take the window correlated

32:13.300 --> 32:16.340
neurons, we can take our modified network, we can turn them

32:16.340 --> 32:22.180
on. And and now we can like add a window. Let's see if we show

32:22.180 --> 32:24.380
that. Yeah, so this here's outlook. So we get this nice

32:24.380 --> 32:28.900
window here. And the scene is began is doing its cool tricks of

32:28.900 --> 32:31.860
orienting the window properly, doing some reasonable things. And

32:31.860 --> 32:35.620
it has some really interesting effects that are non trivial

32:35.620 --> 32:38.820
here. Some of them are good and some are bad. So for example,

32:38.860 --> 32:41.740
all I did was turn on the neurons in this location saying I

32:41.740 --> 32:47.460
want windows. And it did it. But look what else it did. It also

32:47.460 --> 32:50.020
added these reflections right here on the counter. And so this

32:50.340 --> 32:54.140
this kitchen guy does this a lot like adds adds non local

32:54.140 --> 32:56.660
reflections where it thinks that there's a shiny table. And so

32:56.660 --> 32:59.300
the cool thing here is that after I did all the inversion and

32:59.300 --> 33:02.340
stuff, this guy actually thinks that there's a shiny table here

33:02.340 --> 33:04.860
and it's right. And it thinks that if I add a window here,

33:04.860 --> 33:07.340
they should add reflection. That's right. Also, but look

33:07.340 --> 33:10.860
what else happened up here. See this lamp up here. When I first

33:10.860 --> 33:12.980
lifted this in low resolution, I thought, Oh, maybe it turned off

33:12.980 --> 33:15.060
the lamp because once you have windows, you don't need the light

33:15.060 --> 33:18.700
on. But no, it didn't do that. It just messed up the lamp. It's

33:18.700 --> 33:22.820
just total it took this whole area up here and just and just

33:22.820 --> 33:27.580
distorted it badly. And so so that that's a little dissatisfying.

33:27.580 --> 33:32.020
It means that this fine tuning thing, where we get again to,

33:33.020 --> 33:37.660
you know, target a specific user image, when I do when I try to

33:37.660 --> 33:40.580
teach it all the details, I'm not really teaching it what the lamp

33:40.580 --> 33:44.620
was, I was just sort of showing it how to arrange the pixels. And

33:44.620 --> 33:49.540
again, made its best guess on how to generalize how the image

33:49.540 --> 33:52.180
should look differently. If I change something like out of

33:52.180 --> 33:55.340
window, but with only one example of a lamp that looks like

33:55.340 --> 33:57.860
this, it generalized wrong, it has no idea what should happen to

33:57.860 --> 34:00.940
that lamp when I when I add a window. So this is this question

34:00.980 --> 34:05.340
of like how to make changes in a network with with with

34:05.340 --> 34:10.260
achieving good generalization is, which is a good question. And

34:10.300 --> 34:13.820
it was, there was something that puzzled me for a year after

34:13.820 --> 34:17.380
doing this work. But but the work is still pretty cool, you can

34:17.380 --> 34:21.460
still use it for modifying real photos. So here's like a photo of

34:22.020 --> 34:26.700
I got off of Wikipedia of like some real locations. And you can

34:26.740 --> 34:29.860
you can edit them, I can add grass, I can add doors, I can add

34:29.860 --> 34:33.860
domes, you know, just like, just like the the the other

34:33.860 --> 34:36.380
campaign app, except I can actually start with a real photo

34:36.380 --> 34:38.980
that you give me. And I can invert that photo through the

34:38.980 --> 34:41.820
network, get a good starting image, fine tune the network to

34:41.860 --> 34:45.540
make it make it output, you know, the target image and edit that

34:45.540 --> 34:48.540
image, add bigger domes, and it'll sort of match the

34:48.540 --> 34:52.780
architectural style. And, and, and, you know, do different things

34:52.780 --> 34:55.700
like that, I can add domes, remove domes, add doors, you

34:55.700 --> 34:58.900
know, things like that. Let me see if I can get this video here

34:58.940 --> 35:04.140
to show. So this is the status center. Let's add some doors

35:04.140 --> 35:08.380
here. So you get the idea, I'm doing exactly the same

35:08.380 --> 35:11.820
intervention that I did before. And it's it's opinion just like

35:11.820 --> 35:15.260
before, it will not add doors in places that it doesn't think are

35:15.300 --> 35:17.900
not good places for a door, it has some opinions about where

35:17.900 --> 35:20.740
doors are allowed, it likes to put them in brick walls. It

35:20.740 --> 35:25.780
thinks it's okay to put a door in a tower, like that architectural

35:25.780 --> 35:28.880
detail. Oh, I put domes here. It's happy to put domes on top of

35:28.880 --> 35:31.560
buildings. It's not happy to put a dome like in the middle of the

35:31.560 --> 35:35.520
sky. It's not happy to put a door in the middle of the sky. But

35:35.520 --> 35:40.760
you know, it put trees in different places. And, and so

35:40.760 --> 35:42.160
there are things that it understands, there are things

35:42.160 --> 35:44.320
that it doesn't understand very well, it's sort of making a

35:44.320 --> 35:47.160
guess of what the structure of the image is. It doesn't know what

35:47.160 --> 35:50.480
to make of my advisor, you know, sort of planting grass in front

35:50.480 --> 35:53.920
of him. And that's not very realistic. But you kind of get

35:53.920 --> 35:57.280
a feel for what the structure and knowledge of the model is by

35:57.280 --> 35:59.560
doing these kind of interventions. So this was really

35:59.560 --> 36:02.840
cool. I think it got a lot of people's attention. Adobe

36:02.840 --> 36:07.080
noticed this stuff, and has been busy trying to make different

36:07.080 --> 36:12.280
painting applications using, you know, GAN technology that are

36:12.320 --> 36:17.160
I think partially inspired by by by this kind of discovery. So

36:17.440 --> 36:22.920
David, I have a question. Yep. This is really cool. Question is,

36:23.280 --> 36:28.240
when you modify, for instance, churches, I assume you have

36:28.600 --> 36:33.800
trained your GAN on a church data set. Yes, that's correct.

36:33.840 --> 36:37.680
What about when you do it on the real images, for instance, in

36:37.680 --> 36:42.920
this case, you know, your advisor? Yes. So actually, both of

36:42.920 --> 36:45.880
these are using the church data set as well. So the church

36:45.880 --> 36:46.400
data set,

36:46.440 --> 36:50.760
interesting that even you have trained again on church, you

36:50.760 --> 36:52.480
can depict a person.

36:53.280 --> 36:58.320
Yes. So this is so the GAN. Now, you have to keep in mind that

36:58.320 --> 37:02.480
what I've done here is I fine tuned the GAN. So you can

37:02.480 --> 37:05.280
actually, you know, you can actually get you can actually

37:05.280 --> 37:11.160
get a GAN to do a lot of things by fine tuning it. So I've I've

37:11.160 --> 37:16.240
told the GAN, please basically overfit on this target image. So

37:16.240 --> 37:23.880
the GAN, you know, has 30 million parameters. And, and you

37:23.880 --> 37:27.200
know, an image only has, you know, 10,000 pixels, and it has

37:27.200 --> 37:31.520
plenty of excess capacity to memorize the details that I

37:31.520 --> 37:36.320
might want to do. And so what I've done is this as I've taken

37:36.320 --> 37:40.400
the image, I've asked again, through my inversion techniques,

37:40.440 --> 37:44.880
what is the closest church image that you can generate that

37:44.880 --> 37:48.000
looks like my thing. And you get a different image. I don't

37:48.000 --> 37:51.240
have the image to show you here, but you get an image that looks

37:51.240 --> 37:53.800
kind of more church like it's a little bit, it'll be

37:53.800 --> 37:56.800
architectural have the right kind of shape, the kind of right

37:56.800 --> 38:00.280
textures. But you know, it won't show my advisor here and

38:00.280 --> 38:03.720
things like that. It'll be, it'll be this rough approximation

38:04.240 --> 38:10.040
for that my my image, but that is in the domain of what the GAN

38:10.040 --> 38:14.280
can actually generate. Then I say, Okay, that's not what I

38:14.280 --> 38:17.560
want to do. I want to actually edit this photo. So let's fine

38:17.560 --> 38:24.680
tune that network so that so that given that same Z instead of

38:24.680 --> 38:27.400
generating the church that you would normally generate, I want

38:27.400 --> 38:31.560
you to generate this image, change the weight slightly, get it

38:31.560 --> 38:36.520
so that that Z targets this. And, and so that's what I've done

38:36.520 --> 38:39.320
here. But I've tried I've done that in a way where I try not to

38:39.320 --> 38:42.240
change the weights too much. I just try to change the weights. I

38:42.280 --> 38:44.800
change the fine grained layers. And I don't change the coarse

38:44.800 --> 38:48.320
grain layers. And I, and I have a regularizer to make sure the

38:48.320 --> 38:53.760
weights don't change too much. And that you are changing the

38:53.800 --> 38:59.440
pre trained weights, or you are putting some extra weights, and

38:59.440 --> 39:03.960
then you place them. Oh, here, I'm actually changing the pre

39:03.960 --> 39:08.640
trained weights. So the network has 15 layers. I'm actually

39:08.640 --> 39:12.480
going and I'm changing some of those layers. I'm not adding

39:12.480 --> 39:16.920
anything new to the network. I'm just changing the weights in

39:16.920 --> 39:21.480
the network itself. Now, now what I've done here is I've

39:21.480 --> 39:25.720
overfit the network to this one image. The network is not

39:25.800 --> 39:30.040
generalizing this knowledge. So for example, you can draw

39:30.040 --> 39:34.720
Antonio in this one image. But if I look in the network, if I

39:34.720 --> 39:37.840
probe it a lot and see, can it ever generate Antonio in a

39:37.840 --> 39:41.960
different setting in a different image? It cannot. In

39:41.960 --> 39:46.640
fact, you know, as much as we probe things, it really doesn't

39:46.640 --> 39:49.160
look like we've changed the output of the network in any

39:49.160 --> 39:53.720
meaningful way for any image, except for this one. It's almost

39:53.720 --> 39:57.480
like, you know, the network generates this really complicated

39:57.480 --> 40:01.520
manifold of realistic images. And we've told we've picked up one

40:01.520 --> 40:04.680
point of the manifold, and we've dragged it over to pass to

40:04.680 --> 40:08.160
this point. But we've done it in a very local way. So it's

40:08.160 --> 40:11.280
really not affected any other points of what the GAN is

40:11.280 --> 40:18.200
generating. And so so but but for the purposes of doing this

40:18.200 --> 40:20.120
kind of application, it doesn't matter that it's not

40:20.120 --> 40:22.600
generalizing because the user doesn't care about a different

40:22.600 --> 40:25.000
photo, they just care about their own photo. So it's a pretty

40:25.000 --> 40:29.040
cool. It's pretty cool technique anyway, even though it's sort

40:29.040 --> 40:32.920
of not the classical goal of machine learning. Does that make

40:32.960 --> 40:37.840
sense? Yeah, it does. And I wonder if the user has more

40:37.840 --> 40:43.760
images of themselves with that over time, and make the network

40:43.800 --> 40:45.640
even better in generation?

40:47.440 --> 40:50.480
Yes, this is the big question. And I played with this for many

40:50.480 --> 40:52.720
months, and I haven't got it to work. And if anybody can figure

40:52.720 --> 40:54.760
out how to get to work, I feel like it's one of the holy grails

40:55.160 --> 40:58.160
of like how to add a new thing to a generator. So like, the

40:58.160 --> 41:01.120
generator knows about all these things that knows about trees

41:01.120 --> 41:06.400
and knows about all these architectural pieces, you know. But

41:06.400 --> 41:08.880
what if I came along with something new? What if I was

41:10.680 --> 41:14.360
what if my what if I what if I work for GM and I want to sell

41:14.360 --> 41:17.040
Cadillacs, then I then I might come to one of these models and

41:17.040 --> 41:19.640
say, you know what, you should draw cars. In fact, I want you to

41:19.640 --> 41:22.680
draw specific cars. I want you to draw Cadillacs in front of all

41:22.680 --> 41:27.760
these buildings. How would I add Cadillacs to my model or add

41:27.760 --> 41:31.120
Antonio to my model or something like that? And we don't know how

41:31.120 --> 41:33.400
to do that yet. Although I'm going to show you a little bit of

41:33.400 --> 41:38.200
work, where we can do something that's very similar. And if I

41:38.200 --> 41:41.120
don't know if I have time to, to go over this, but I'm going to

41:41.160 --> 41:43.440
I'm going to zoom through this because I'm so excited by this

41:43.440 --> 41:50.680
work. So, so, so it's motivated by this, this sort of question,

41:50.720 --> 41:55.640
which is, you know, we have a model of like drawing towers,

41:55.640 --> 41:59.400
let's say, right? But there are things in the world that we might

41:59.400 --> 42:03.400
want to model that we don't have a data set for. For example, you

42:03.400 --> 42:07.080
know, in in in Decatur County, Illinois, there's this courthouse

42:07.080 --> 42:09.880
that has a tree growing out the top of the tower. It started

42:09.880 --> 42:12.800
growing out there by accident, but the people in the town love

42:12.800 --> 42:16.240
it. And so but it's but there's no so like if I want to get a

42:16.240 --> 42:20.360
generative model to draw trees growing out of tops of towers, I

42:20.360 --> 42:24.240
can't do that in a classical way because I can't create a big

42:24.240 --> 42:27.840
data set of a million buildings that have trees growing on the

42:27.840 --> 42:30.520
top of the towers, because they don't exist. It's just this one.

42:31.360 --> 42:36.640
And so now if if the point is I want to generate images of this

42:36.640 --> 42:40.840
type, you know, well, I could use a regular image editor, I

42:40.840 --> 42:43.200
can take any building of a tower, and of course, I can stick a

42:43.200 --> 42:47.000
tree on it, right? I could use my, you know, again, painting

42:47.000 --> 42:50.080
method to, you know, activate tree neurons or something like

42:50.080 --> 42:53.280
that. But no, no, that's not what I'm asking. I'm asking this

42:53.280 --> 42:57.240
other question of like, how can we stick tree towers into my

42:57.240 --> 43:01.160
model? How do I modify the model to have this new concept in it?

43:01.480 --> 43:03.880
Like I start with this model that has all these weights that

43:03.920 --> 43:06.800
encode all these rules for how buildings look and things like

43:06.800 --> 43:11.080
that. And I want to create a new model that has new weights that

43:11.080 --> 43:14.120
encode new rules. So for example, the old model could generate

43:14.120 --> 43:17.200
all these buildings that of towers that look normal have

43:17.200 --> 43:20.880
spires, you know, pointy tops. And I want to make a new model

43:20.880 --> 43:24.560
that has weights, they encode a different rule, so that like,

43:24.680 --> 43:28.200
they have trees growing out the top, right, or any rule that I

43:28.200 --> 43:32.800
choose, right? And it turns out that this is actually possible.

43:32.800 --> 43:36.160
So this is different from the technique that I showed you

43:36.160 --> 43:39.080
before, because in this technique, it's actually

43:39.080 --> 43:43.240
generalizing. This is, you know, if you use this technique, not

43:43.240 --> 43:48.120
only you change the output of one image of the GAN to have like

43:48.120 --> 43:53.480
some effect, but we can actually change the outputs for a whole

43:53.480 --> 43:58.360
class of, you know, a large subset of the outputs of the GANs to

43:58.360 --> 44:04.000
follow a different rule, like any pointy tower output will have

44:04.040 --> 44:08.000
trees instead of pointy towers. And so so I'll just show you a

44:08.000 --> 44:10.360
little bit of like the interaction here of what it

44:10.360 --> 44:15.280
looks like when you get our method into an application. So I

44:16.200 --> 44:18.680
let's see if I can get this to work. So here, what I'm showing

44:18.680 --> 44:22.840
you is the output of a style GAN be to generating churches, you

44:22.840 --> 44:26.680
can kind of, and there are three parts of this UI, there's an

44:26.680 --> 44:30.920
image viewer, then what you do is you can select a rule that you

44:30.920 --> 44:33.440
want to change, and then you can specify how you want to change

44:33.440 --> 44:35.800
your rule. So there's three parts of this little user

44:35.800 --> 44:39.040
interface. And I'll just show you sort of how how the effect

44:39.040 --> 44:43.520
looks by showing you one of the interactions. So you can kind of

44:43.520 --> 44:48.560
use the image viewer to scroll through lots of examples of of

44:48.560 --> 44:52.400
what the the generator is capable of generating. And then we

44:52.400 --> 44:54.800
can go to these examples and we can say, Hey, you know what I'm

44:54.800 --> 44:58.760
really interested in? I'm interested in this rule of how

44:58.760 --> 45:02.200
to generate pointy towers. And so I can select a few pointy

45:02.200 --> 45:06.320
towers. And you can think of this as what I'm looking for is

45:06.320 --> 45:09.360
the neurons that are responsible for the shape. And so I can

45:09.360 --> 45:13.920
select a few examples and I can say, Hey, what other, what other

45:13.920 --> 45:17.600
outputs of the GAN share the same representation? And, and it'll

45:17.600 --> 45:20.120
show me, Oh, yes, the GAN is generalizing this way, these

45:20.120 --> 45:23.520
other pointy towers are represented the same way as the

45:23.520 --> 45:27.040
ones that you chose. And then I can go and I can say, All right,

45:27.040 --> 45:33.400
I want to redefine how these pointy towers are rendered by

45:33.400 --> 45:36.640
this generator, I want them to be rendered like this tree here.

45:36.640 --> 45:41.240
So I can copy the tree from one output of the generator, and I

45:41.240 --> 45:45.680
can paste it into where I would like that tree to show up. I

45:45.680 --> 45:49.440
wanted to show up instead of pointy towers. And then I can

45:49.440 --> 45:54.000
say, Okay, now insert this new rule into the model, compute

45:54.000 --> 45:57.840
what the right changes to change the model. And then after I do

45:57.840 --> 46:02.240
that, that takes about a second to do the math to figure out how

46:02.240 --> 46:06.080
to change a rule. And then after I do that, then I get the GAN to

46:06.120 --> 46:10.480
generate new images. And, and they look like this, you know,

46:10.520 --> 46:14.760
like the tops of the towers, now have trees on them instead. So

46:14.760 --> 46:17.600
you can see how that looks. And it's not just affecting that

46:17.600 --> 46:22.680
one image, it's affecting all the pointy tower images. I can do a

46:22.680 --> 46:26.400
little search for more pointy tower images. And, and do I have

46:26.400 --> 46:29.760
that here in my thing? Yeah, so here's a search for more pointy

46:29.760 --> 46:32.120
tower images. And you can see they, you know, they all have

46:32.120 --> 46:36.040
gotten these trees sprouting out the top of it, like some sort

46:36.040 --> 46:41.880
of dystopian tree world where vegetation is taking over the

46:41.880 --> 46:46.600
planet. And, and so you can do this in a bunch of things, I'm

46:46.600 --> 46:50.880
gonna skip over some of the technical things here, or some

46:50.880 --> 46:53.640
of the other examples of what you can do here. You can edit

46:53.640 --> 46:56.600
reflections and things like that. I've got other videos that you

46:56.600 --> 46:59.400
can look for on the internet. But I wanted to show you a sense

46:59.400 --> 47:02.800
for what we're doing inside when we do this kind of thing. So

47:02.880 --> 47:07.280
like I showed you before that again, has is like, got all these

47:07.280 --> 47:11.920
convolutional layers stacked up, it's about 15 layers. And what

47:11.920 --> 47:16.120
what, what, what the discovery was that led to this application

47:16.640 --> 47:20.920
was that each one of those layers can be thought of as

47:20.920 --> 47:26.000
solving a very simple, separate problem from the other layers.

47:26.360 --> 47:29.800
And what is that simple problem? It, it can be treated like a

47:29.800 --> 47:34.840
memory, where the layer is solving this problem of matching

47:34.840 --> 47:39.680
key value pairs that it's memorized. So every location

47:39.760 --> 47:44.320
has a feature vector that you can think of as a key. And what

47:44.360 --> 47:46.760
and the key each key like, you know, represents a certain

47:46.760 --> 47:49.360
type of context, like, you know, the middles of towers or the

47:49.360 --> 47:52.640
tops of towers or something like that. And what you can think

47:52.680 --> 48:00.680
of the map as as as storing is what should be what is like the

48:00.680 --> 48:05.000
pattern of features that should be rendered whenever that

48:05.000 --> 48:08.120
context comes up. Right. So you can think of it as just

48:08.120 --> 48:14.760
basically key value store. And and so so this whole idea of

48:14.800 --> 48:17.880
using a matrix as a key value stores and it's like the oldest

48:17.880 --> 48:23.560
idea in neural networks. People observe back in the 1970s,

48:24.080 --> 48:28.360
that if you have a single layer neural network, you can treat

48:28.360 --> 48:32.320
it as a as an approximate key value store that remembers keys

48:32.320 --> 48:38.320
with minimal error. And and so if you had a set of keys and a

48:38.320 --> 48:41.320
set of values you want to store, and you ask what is the

48:41.320 --> 48:44.480
optimal single layer neural network that you'd use to store

48:44.480 --> 48:48.840
it. It's actually, you know, classical linear algebra, it's

48:48.840 --> 48:51.840
like the solution to a least squares problem. So what we can

48:51.840 --> 48:56.880
hypothesize is that in these very, very fancy, you know, 2020,

48:56.920 --> 49:02.680
you know, 50 years later, deep neural networks, actually, each

49:02.680 --> 49:05.920
layer is just acting as one of these. Now, which keys are being

49:05.920 --> 49:09.600
stored and what values were being stored? We don't know. But

49:09.600 --> 49:12.240
we could hypothesize that there is some set of things that are

49:12.240 --> 49:16.680
being memorized, some set of keys and values. And so that that

49:16.680 --> 49:19.560
maybe this weight matrix that we have is the solution to the

49:19.560 --> 49:23.200
assembly squares problem. So the cool thing that we can do is we

49:23.200 --> 49:25.880
can say we can ask the question, what would the weight matrix

49:25.880 --> 49:30.120
look like if we changed one of the rules? What if we had one new

49:30.120 --> 49:34.280
key value pair that we wanted to change? Then what would the

49:34.280 --> 49:38.120
weight matrix be? Instead, we want all the other things that

49:38.120 --> 49:41.680
the network has memorized to still be memorized with minimal

49:41.720 --> 49:45.000
error, just as before, except we're going to give this new

49:45.000 --> 49:48.360
constraint, we want to write a new key value pair into it. And it

49:48.360 --> 49:50.320
turns out that that's also least squares problems and

49:50.320 --> 49:51.800
constrained least squares problem, we can write down the

49:51.800 --> 49:55.320
solution in this form. And the cool thing about these two, the

49:55.320 --> 49:58.640
squares problems is that they cancel each other out. Most of

49:58.640 --> 50:03.080
the terms are the same. And, and, and we can actually ask the

50:03.080 --> 50:07.360
question, how would the weights have to change if we add a new

50:07.360 --> 50:11.480
key value pair, without knowing which values were written into

50:11.480 --> 50:15.360
the network before, we don't actually have to know what the

50:15.360 --> 50:20.600
old key value pairs were, we can just assume that the network

50:20.600 --> 50:25.320
was optimal as storing all these key value pairs. And, and the

50:25.320 --> 50:29.640
math for like how to write a new key value pair comes out the

50:29.640 --> 50:32.120
same anyway. So, so that's there's there's a little bit of a

50:32.120 --> 50:37.320
mathematical insight and trick here. But what it allows us to do

50:37.320 --> 50:42.440
is it allows us to find exactly what we want to do to change one

50:42.440 --> 50:44.720
thing that the network is memorized, you do this rank one

50:44.720 --> 50:50.600
update in a specific direction. And, and you can take a key and

50:50.600 --> 50:54.200
change it to any value you want. And that will, you know, the

50:54.200 --> 50:59.760
same form will minimize error for, for other keys, regardless of

50:59.760 --> 51:03.400
what value we write, it's almost like it really is a form of

51:03.440 --> 51:07.320
memory, that we're changing. So our method is basically you

51:07.320 --> 51:13.080
find a key by asking the user to select a few contexts that look

51:13.080 --> 51:16.040
the same, we average them to get a good key. Then we ask for a

51:16.040 --> 51:19.240
copy paste example to get a goal. That's the new value that we

51:19.240 --> 51:21.840
want to write into the key of the memory. And then we do this

51:21.840 --> 51:28.000
math to, to find how to change w in the direction of the key

51:28.000 --> 51:33.520
only, we find a rank one update that does this. And so, and so

51:33.520 --> 51:35.480
that avoids changing other rules. So we can do this on a bunch

51:35.480 --> 51:39.120
of different GAN models. And, and so you can see, like, you

51:39.120 --> 51:44.520
know, people like to change people's expressions here. So

51:44.520 --> 51:47.280
what we're doing is a little different from what you normally

51:47.280 --> 51:49.160
do to change people's expressions. And again, what

51:49.160 --> 51:51.440
we're doing is we're actually going to rewrite the GAN. So it

51:51.440 --> 51:53.520
only outputs people who are smiling, we're going to take all

51:53.520 --> 51:55.880
the frowns, we're saying, okay, there's, there's a rule for

51:55.880 --> 51:58.040
frowns, we're going to change that to a rule for smiles by

51:58.040 --> 52:01.680
showing an example. And so by patching frowns to smiles, now

52:01.680 --> 52:04.880
we have a model that just outputs people who are smiling. Now

52:04.880 --> 52:08.440
we live in a happy world. So that's, that's, that's pretty

52:08.440 --> 52:11.080
cool. And now, of course, we could have done that by, you

52:11.080 --> 52:15.080
know, changing the training set by collecting only training

52:15.080 --> 52:17.480
data of people who are smiling. But the neat thing is that you

52:17.480 --> 52:20.560
can also do this for things where you don't have a training

52:20.560 --> 52:23.080
set that looks like it. So for example, there's a, there's a

52:23.080 --> 52:27.160
rule in the model for how eyebrows should look on kids. So

52:27.160 --> 52:31.080
you can see that kids have these very wispy light eyebrows that

52:31.080 --> 52:34.480
don't have much hair. So we can find that rule by identifying a

52:34.480 --> 52:37.400
few examples that gives us a rank one direction in the weight

52:37.400 --> 52:41.000
matrix. And then we can redefine it, we can write a new thing

52:41.000 --> 52:43.280
into it and say, you know what, we want the eyebrows to look

52:43.280 --> 52:47.000
like this, like that's very bushy much sash. And, you know,

52:47.000 --> 52:51.000
paste into one example, do the math. And then now we can change

52:51.040 --> 52:54.000
weights in a way that generalizes. So now all the kids had

52:54.000 --> 52:58.440
these very bushy, you know, eyebrows. And it's something

52:58.440 --> 53:01.520
that we wouldn't have been able to get by collecting training

53:01.520 --> 53:03.800
set because we don't have kids that look like this in the real

53:03.800 --> 53:07.280
world. It's something that just comes out of our imagination. So

53:07.280 --> 53:11.720
this is kind of the thing. I kind of feel like this is the big

53:11.720 --> 53:15.720
reason why, why, why be interested in how these models

53:15.720 --> 53:20.640
are working inside. And the reason to be so interested in it is

53:20.640 --> 53:24.560
because as long as we don't look inside our models, then we're

53:24.560 --> 53:27.960
really constrained. Because the only thing that our models can

53:27.960 --> 53:32.480
really do is imitate the real world. We can collect huge

53:32.480 --> 53:36.040
amounts of data. And the models that we create, we'll just get

53:36.040 --> 53:39.680
better and better at imitating the way that the data is the way

53:39.680 --> 53:43.480
the world is today. And I kind of feel like it goes a little bit

53:43.480 --> 53:46.600
against why I was interested in computer science years ago when I

53:46.600 --> 53:49.200
entered it in the first place. Because the amazing thing about

53:49.200 --> 53:53.360
computer science is that you can use it to create algorithms

53:53.360 --> 53:56.480
that represent things in the world that don't exist yet,

53:56.520 --> 53:59.520
things that you can only imagine. And so machine learning is

53:59.520 --> 54:02.400
sort of on this path right now, where we're getting very, very

54:02.400 --> 54:06.640
good at replicating the way the world is. And we're going to be

54:06.640 --> 54:10.280
confronted with this question of how do we use these techniques

54:10.440 --> 54:14.080
to actually create new worlds that don't exist yet that are the

54:14.080 --> 54:18.400
way that we want them to be. And I think that this really

54:18.400 --> 54:23.880
going to require us to not just get models that are just really

54:23.880 --> 54:27.720
good at imitating, but also models that are understandable to

54:27.720 --> 54:33.160
people so that we can change their rules inside, and then use

54:33.160 --> 54:36.480
them to create things that are based on our imagination instead

54:36.480 --> 54:44.320
of just the training data. And so here's a fun thing here, I

54:44.360 --> 54:50.560
think, if I want to be fair to the horses, you notice that none

54:50.560 --> 54:55.080
of the horses in this horse generating GAN get to wear hats

54:55.120 --> 54:58.560
even though all the people get to wear hats. So we can change

54:58.560 --> 55:03.280
that by taking a hat from a person and inserting it into our

55:03.280 --> 55:06.440
GAN's model of what a horse's head should look like. And now

55:06.440 --> 55:11.680
horses get to wear hats, right? And so, so let's build a better

55:11.680 --> 55:18.000
world. And, and allow people to change the rules of the world by

55:18.000 --> 55:22.360
making the rules more visible and and manipulatable by humans.

55:22.760 --> 55:28.640
That's that's sort of the goal of the whole thing. So any

55:28.640 --> 55:29.960
questions? Any questions?

55:30.280 --> 55:31.960
I have a question. Yes.

55:32.080 --> 55:35.920
Does this method work with multiple different models? Or is

55:35.920 --> 55:39.600
it only successful when like, taking a hat from within this

55:39.600 --> 55:42.080
model and put it on a horse?

55:42.160 --> 55:49.000
So right now, this, this method is only able to take it, it's

55:49.240 --> 55:53.120
it's only able to rewire one model. So I can take one part of a

55:53.120 --> 55:56.720
model and rewire it to a different model, you're sort of

55:56.720 --> 55:59.200
asking the transplant question. So I'm sort of at the point

55:59.200 --> 56:03.520
where, you know, it's like a surgeon, I can like connect one

56:03.520 --> 56:05.720
blood vessel to another blood vessel in the same human, right?

56:05.720 --> 56:07.480
And you're sort of asking the question, well, can I do a heart

56:07.480 --> 56:10.240
transplant? Can I take a heart out of one person put another one?

56:10.560 --> 56:15.720
And I cannot do that yet. It turns out to be harder. And, but I

56:15.760 --> 56:20.680
but it is a it is an obvious goal. And I, and I feel confident

56:20.680 --> 56:25.200
that if we understand well enough, all the things that make

56:25.200 --> 56:28.400
these computations work, what is needed for the care and

56:28.400 --> 56:31.600
feeding of a computational module? What is a computational

56:31.600 --> 56:35.200
module inside a big learning system? Then we should, you

56:35.240 --> 56:38.640
know, it should be a goal to be able to move a piece of

56:38.640 --> 56:43.160
computation from one neural network to another one. Does that

56:43.160 --> 56:43.640
make sense?

56:45.480 --> 56:46.040
Yes, thank you.

56:46.400 --> 56:49.840
Yep. That's a really great question, by the way. I think

56:49.840 --> 56:55.120
it's, I think it's fundamental. Any other questions?

56:59.280 --> 57:04.520
This is not too well articulated question. I was just

57:04.520 --> 57:08.440
curious what you, what are your thoughts about this? I think

57:08.440 --> 57:14.000
this is this like neural nets have tendency to like avoid the

57:14.000 --> 57:16.760
responsibility of the results, like everything is done in the

57:16.800 --> 57:19.920
hidden layers and sort of shrug off shrug off the

57:19.920 --> 57:23.600
responsibility about the results. And I thought it was like

57:23.600 --> 57:28.160
interesting how you set the objective towards something as

57:28.200 --> 57:33.400
abstract as realistic. And here, like how you define the concept

57:33.400 --> 57:37.600
of being realistic is based on the big data you collected from

57:37.600 --> 57:42.680
the web, but but oftentimes some like fake images sometimes

57:42.680 --> 57:48.080
look even more realistic than real images. And I don't know,

57:48.080 --> 57:51.280
like tree growing on top of the building may look fairly

57:51.280 --> 57:56.680
realistic for some people, but maybe for plant experts, maybe

57:56.720 --> 58:02.480
it would not. Right. So I don't know, like, I think this might

58:02.640 --> 58:07.280
result in like the blurring between the it's making us hard

58:07.280 --> 58:10.040
to distinguish between the real and the fake or something like

58:10.040 --> 58:10.560
that. I don't know.

58:11.120 --> 58:17.400
Yes, yes. No, I think that there are so so the we're we're

58:17.400 --> 58:22.600
unaccustomed to making it easy for making programs that make

58:22.600 --> 58:26.040
such realistic renderings of the world. And it's actually a

58:26.040 --> 58:30.680
concern. I think that, you know, people have misused this

58:30.720 --> 58:33.720
technology already that we you know, we use we you know, there's

58:33.720 --> 58:38.960
the whole deep fakes phenomenon. But even without like faking

58:38.960 --> 58:46.040
videos, people people have you know, used face generators to

58:46.040 --> 58:49.160
make lots of fake Facebook profiles and things like that,

58:49.160 --> 58:52.240
you know, pretending there are millions of people that exist

58:52.240 --> 58:56.240
that don't actually exist and things like that. So so even

58:56.280 --> 58:58.640
before you sort of do manipulations of the world, I

58:58.640 --> 59:03.720
think that there's already this problem of of of, you know,

59:03.720 --> 59:08.200
pretending that there's a lot of data that there actually isn't

59:08.240 --> 59:11.120
by using these generator models. And so I think that there's

59:13.640 --> 59:18.000
you know, the whole the whole question of fakes is a very

59:18.000 --> 59:25.160
serious question, like how do we how do we function society if

59:25.160 --> 59:29.680
we don't know what's real and what's fake. Now, it's not a

59:29.680 --> 59:35.720
totally new issue. You don't need a state of the art deep

59:35.760 --> 59:39.760
learning model to make fake, you know, people have made fake

59:39.760 --> 59:43.760
photoshopped by hand forever, people write can write text that

59:43.760 --> 59:47.640
has all sorts of lies forever. In fact, that's probably more

59:47.640 --> 59:51.560
effective than you know, trying to train a deep learning model

59:51.560 --> 59:55.040
and, you know, sort of make it work. But I think it's I think

59:55.040 --> 59:57.560
it's a, you know, it's still an important question because the

59:57.560 --> 01:00:02.000
easier we make it to make fakes, you start to get issues like a

01:00:02.000 --> 01:00:06.160
scalable fakes, where it's not just one, one photo that is a lie

01:00:06.160 --> 01:00:09.920
or one article is lie, you could generate millions. And I think

01:00:09.920 --> 01:00:12.560
that there are serious issues with that. So there's some pretty

01:00:12.560 --> 01:00:17.360
interesting work in forensics for detecting fakes, and things

01:00:17.360 --> 01:00:21.480
like that. That I think is important to invest in as well

01:00:21.480 --> 01:00:25.240
as as we as we advance the state of the art and this kind of

01:00:25.240 --> 01:00:31.040
thing. So I so so I don't want to minimize the implications of

01:00:31.040 --> 01:00:34.120
this type of thing. I think that for the type of work that I'm

01:00:34.120 --> 01:00:36.520
doing, I think that you observe that the tree kind of looks

01:00:36.520 --> 01:00:38.960
realistic, it's not super realistic. You know, if you're a

01:00:38.960 --> 01:00:45.000
plant expert, it's just sort of, you know, sort of there. I

01:00:45.040 --> 01:00:47.400
think the same thing with hats, they don't really super look

01:00:47.400 --> 01:00:50.120
like hats. And so I think that we're, we're sort of the stage

01:00:50.120 --> 01:00:54.000
where they're really exciting where the implication of what

01:00:54.000 --> 01:01:00.480
I've done here, I think is the idea that the, you know, learning

01:01:00.840 --> 01:01:04.440
how these models are working inside, by understanding what

01:01:04.440 --> 01:01:07.720
the internal structure of the models is, is really the, that

01:01:07.960 --> 01:01:11.880
the the exciting part that that it's starting to give a little

01:01:11.880 --> 01:01:17.240
insight on how we might untangle and disassemble what the

01:01:17.240 --> 01:01:21.560
internal logic is, that is being learned by these, these deep

01:01:21.560 --> 01:01:26.400
networks. And, and I'm actually, I feel like this is, I feel

01:01:26.400 --> 01:01:28.400
like there's a different issue other than fakes, which is

01:01:28.400 --> 01:01:32.640
actually has some ethical implications, which is transparency

01:01:32.640 --> 01:01:35.200
of deep networks. Because one thing that they're not really

01:01:35.200 --> 01:01:38.680
good at doing is when you have a deep network do something

01:01:38.680 --> 01:01:41.960
amazing, they're really not good at answering the question, why?

01:01:42.320 --> 01:01:45.800
Why did you do that? Why did you choose to render it this way?

01:01:45.800 --> 01:01:49.680
Why did you choose to pick these objects to put in the scene?

01:01:49.680 --> 01:01:53.560
Or why did you choose to deny me some credit or to, you know, to

01:01:53.560 --> 01:01:56.920
make some other decision that we were at, you know, depending on

01:01:56.920 --> 01:02:00.600
neural networks to do. And I think that if we can understand

01:02:01.320 --> 01:02:05.560
how to disassemble the rules that are being applied inside the

01:02:05.560 --> 01:02:08.440
network for it to make its decision, then I think that we'll

01:02:08.480 --> 01:02:13.240
will be, we'll have a way of asking why. And by looking at

01:02:13.240 --> 01:02:16.320
the computation directly. So that's my, that's one of my

01:02:16.320 --> 01:02:18.240
goals and one of my hopes in doing this kind of work.

01:02:21.720 --> 01:02:26.520
Definitely, I can see some of the worst of you about like,

01:02:27.960 --> 01:02:31.240
about the transparency of the neural network, especially when

01:02:31.240 --> 01:02:36.120
you, when you show the example where you detected a single

01:02:36.120 --> 01:02:40.120
neuron that contributes to the watermark thing, I think that

01:02:41.360 --> 01:02:42.400
it was really interesting.

01:02:43.720 --> 01:02:47.440
Yeah, I think so too. I was surprised that it worked because

01:02:47.440 --> 01:02:50.280
we normally think of neural X is very, very, very opaque.

01:02:54.160 --> 01:02:59.320
I also have a small question regarding artifacts. So I think

01:02:59.560 --> 01:03:04.160
in the beginning, you talked about how you segmented the

01:03:04.240 --> 01:03:10.600
network with like masks that were classified before by mapping

01:03:10.720 --> 01:03:15.720
neurons and beginning layers, which create things. But could

01:03:15.760 --> 01:03:20.360
like, can that be also used to figure out where artifacts or

01:03:20.360 --> 01:03:24.400
anomalies are generated to make gains better?

01:03:24.960 --> 01:03:30.880
Yeah, actually, I don't have a picture of it here. But in my

01:03:30.880 --> 01:03:34.640
work where I was looking for neurons, originally, it's called

01:03:34.920 --> 01:03:37.400
the paper is called GAN dissection, you can you can

01:03:37.400 --> 01:03:42.760
Google for it. And, and I showed that in that paper, we

01:03:42.760 --> 01:03:45.800
analyze some of the pre trained GANs that came from a previous

01:03:46.000 --> 01:03:50.280
work from NVIDIA called progressive GAN, we analyzed

01:03:50.280 --> 01:03:52.880
some of the pre trained models, and we found that they actually

01:03:52.880 --> 01:03:56.560
are neurons that correlate with bad looking artifacts in a

01:03:56.560 --> 01:04:00.240
scene. And if you turn those neurons off, you can actually

01:04:00.640 --> 01:04:05.080
not only improve the quality of the output of the GAN, just

01:04:05.080 --> 01:04:08.760
qualitatively like you can get these artifacts to not show up

01:04:08.760 --> 01:04:13.760
but using standard measures of GAN, you know, statistical

01:04:13.760 --> 01:04:17.720
measures of GAN image fidelity at large scale. By removing these

01:04:17.720 --> 01:04:21.280
neurons, you can actually improve the what we call the FID

01:04:21.280 --> 01:04:25.120
scores of these GANs when we tested on like 50,000 images. And

01:04:25.120 --> 01:04:29.320
so, so that's actually very weird to me, that's, it was a big

01:04:29.320 --> 01:04:35.760
surprise. Because, because we, we train these things using, you

01:04:35.760 --> 01:04:39.720
know, powerful optimization techniques, using, you know,

01:04:39.920 --> 01:04:42.600
billions of floating point operations, you know, training

01:04:42.600 --> 01:04:46.040
these things on big expensive GPUs for a long period of time. And

01:04:46.040 --> 01:04:50.000
the idea that a human can come along, and do a simple looking

01:04:50.120 --> 01:04:53.040
visualization, pick out a few neurons based on things that

01:04:53.160 --> 01:04:57.120
don't look good. And improve the model by turning those neurons

01:04:57.120 --> 01:05:01.920
off. It was like it shouldn't be possible, right? If it was so

01:05:01.920 --> 01:05:05.360
easy to improve the model that way, why couldn't the optimizer

01:05:05.920 --> 01:05:10.520
find it? And so, so I think that was, that was, that was pretty

01:05:10.520 --> 01:05:17.240
interesting. I have not repeated that experiment on the latest

01:05:17.280 --> 01:05:21.280
GANs, which are actually much better the style GANs. To

01:05:21.280 --> 01:05:25.280
architecture, they went back and they analyzed a bunch of the

01:05:25.280 --> 01:05:29.920
artifacts that show up in this, this family of GANs. And they,

01:05:29.960 --> 01:05:31.920
they found that there are certain learning methods that they

01:05:31.920 --> 01:05:35.400
can do to remove the artifacts or reduce them somewhat. And so I

01:05:35.400 --> 01:05:40.400
don't know if a human can still beat the current generation of

01:05:40.400 --> 01:05:43.520
GANs, it'd be worth going back and seeing that phenomenon is

01:05:43.520 --> 01:05:44.000
still there.

01:05:44.880 --> 01:05:47.000
That's pretty cool. Thank you. Yep.

01:05:51.000 --> 01:05:55.000
Okay, excellent. Thank you so much, David. It was really

01:05:55.240 --> 01:06:02.400
fascinating topic and talk and more interesting to me, asking

01:06:02.400 --> 01:06:06.600
the right questions, asking questions and learning to ask

01:06:06.600 --> 01:06:10.160
the right questions. It's really interesting. And I think that

01:06:10.280 --> 01:06:13.280
it opened paths to many of us.

01:06:14.040 --> 01:06:17.400
Excellent. Hey, thank you for the opportunity to talk to the

01:06:17.400 --> 01:06:23.600
group here today. I always enjoy the, the chance to interact

01:06:23.600 --> 01:06:27.200
with folks about this. If anybody wants to send other questions

01:06:27.200 --> 01:06:32.360
about it, of course, you can always send me a note. And, you

01:06:32.360 --> 01:06:33.520
know, I love this stuff.

01:06:33.800 --> 01:06:38.280
Yeah, definitely. I think that it would be great to follow

01:06:38.640 --> 01:06:43.960
follow your work on your GitHub and your website, and

01:06:43.960 --> 01:06:47.160
especially for students who play with the tools that you have,

01:06:47.160 --> 01:06:50.520
so they have them get an understanding of how these two

01:06:50.520 --> 01:06:53.360
work and make them curious about the work.

01:06:53.880 --> 01:06:54.360
Cool.

01:06:55.080 --> 01:06:56.560
Excellent. Thank you so much.

01:06:57.320 --> 01:06:58.840
Thank you, Ali. Thank you, everybody.

01:06:59.440 --> 01:07:01.480
Thank you. Bye now.

