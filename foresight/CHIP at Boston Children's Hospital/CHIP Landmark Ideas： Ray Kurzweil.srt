1
00:00:00,000 --> 00:00:10,600
Alright. As folks join the Zoom, I'm going to get started. Welcome everyone to the CHIP

2
00:00:10,600 --> 00:00:18,240
Landmark Ideas series. Today we'll be hearing about rewriting biology with artificial intelligence

3
00:00:18,240 --> 00:00:25,680
for Ray Kurzweil, an inventor and futurist. I'm Ken Mandel. I direct the Computational

4
00:00:25,680 --> 00:00:31,920
Health Informatics program at Boston Children's Hospital. The program was founded in 1994

5
00:00:31,920 --> 00:00:37,560
for a multidisciplinary applied research and education program. To learn more you can

6
00:00:37,560 --> 00:00:45,520
visit www.chip.org. The Landmark Ideas series is an event series featuring thought leaders

7
00:00:45,520 --> 00:00:57,720
across healthcare, informatics, IT, big science, innovation, and more. Dr. Kurzweil is one

8
00:00:57,720 --> 00:01:06,120
of the world's leading inventors, thinkers, and futurists. He creates and predicts using

9
00:01:06,120 --> 00:01:12,560
tools and ideas from the field of pattern recognition. He invented many technologies

10
00:01:12,560 --> 00:01:19,820
familiar to us today, including flatbed scanning, optical character recognition, and text-to-speech

11
00:01:19,820 --> 00:01:26,320
synthesis. He won a Grammy for creating a music synthesizer used by Stevie Wonder that

12
00:01:26,320 --> 00:01:32,920
was capable of recreating the grand piano and other orchestral instruments. He was awarded

13
00:01:32,920 --> 00:01:39,840
the National Medal of Technology. His best-selling books include the New York Times bestsellers

14
00:01:39,840 --> 00:01:45,320
The Singularity is Near and How to Create a Mind. Larry Page brought Kurzweil into

15
00:01:45,320 --> 00:01:55,520
Google as a principal researcher and AI visionary. I'll just mention one connection to Chip.

16
00:01:55,520 --> 00:02:02,520
Ben Rice, a faculty member, when he was a student at MIT, worked with Ray to develop

17
00:02:02,520 --> 00:02:10,960
a text-to-speech interface for that synthesizer so that Stevie Wonder and other non-sided musicians

18
00:02:10,960 --> 00:02:19,200
could interact with the extensive visual navigation interface. The Singularity is a very important

19
00:02:19,200 --> 00:02:26,040
idea of Dr. Kurzweil. This is the point in time when artificial intelligence will surpass

20
00:02:26,040 --> 00:02:35,760
human intelligence, resulting in rapid technological growth that will fundamentally change civilization.

21
00:02:35,760 --> 00:02:43,520
And in order to understand when machines surpass biology, Ray has delved deeply into an understanding

22
00:02:43,520 --> 00:02:50,600
of biology, and we're immensely looking forward to hearing and learning and joining him in

23
00:02:50,600 --> 00:03:01,000
that understanding today. You've got us all looking forward, and the chat and the Q&A have

24
00:03:01,000 --> 00:03:09,880
lit up. Let me start with one question. You're joining us for the seminar five days after

25
00:03:09,880 --> 00:03:18,480
the release of OpenAI's chat GPT, which astounded many across the world in its ability to synthesize

26
00:03:18,480 --> 00:03:24,840
natural language responses to really complicated questions and assignments. And if you've gotten

27
00:03:24,840 --> 00:03:31,880
to glimpse this technology, could you place it on the Kurzweil map toward the Singularity?

28
00:03:31,880 --> 00:03:40,040
Is this a step forward? Is it a distraction? Is it related in any way?

29
00:03:40,040 --> 00:03:48,640
Large language models occurred three years ago, and they seemed quite compelling. They weren't totally

30
00:03:48,640 --> 00:04:03,320
fully there. You could chat with it, and sometimes it would kind of break down. The amount of new

31
00:04:03,400 --> 00:04:11,360
ideas that are going into large language models has been astounding. And so it's like every other

32
00:04:11,360 --> 00:04:17,920
week there's a new large language model and some new variation that's more and more realistic.

33
00:04:17,920 --> 00:04:32,320
So that's going to continue to happen. And so this is just another issue. I mean, there are some

34
00:04:32,360 --> 00:04:44,200
things I think that aren't quite right with that particular model you mentioned. But people have

35
00:04:44,200 --> 00:04:50,040
actually interacted with these things and some people say they're sentient. I don't actually think

36
00:04:50,040 --> 00:04:55,080
they're sentient yet, but I think they're actually moving in that direction. And that's actually

37
00:04:55,080 --> 00:05:03,120
not a scientific issue. It's actually a philosophical issue as to what you consider sentient or not.

38
00:05:03,120 --> 00:05:11,120
Although it's a very important issue, because I would chat with Marvin Minsky, who is my mentor for

39
00:05:11,120 --> 00:05:17,920
50 years, and he said that sentience is not scientific, so therefore forget it, it's an illusion.

40
00:05:18,920 --> 00:05:27,000
That's not my opinion. If you have a world that had no sentience in it, it may well not exist.

41
00:05:27,000 --> 00:05:35,560
But yes, there was a sizable advance, but there's more to come.

42
00:05:36,560 --> 00:05:46,640
Let me ask a question from Charlotte, please. A philosopher, an AI-informed philosopher.

43
00:05:46,640 --> 00:05:53,680
What do you make of the criticism that there's more to intelligence than brute processing speed and

44
00:05:53,680 --> 00:06:00,000
pattern recognition? That if we want to pass the Turing test, we need to learn more about our own

45
00:06:01,000 --> 00:06:14,440
intelligence evolved. And I'll just paraphrase you in The Singularity is Near, comparing cognition to

46
00:06:14,440 --> 00:06:21,840
chaotic computing models where unpredictable interaction of millions of processes, many of which

47
00:06:21,840 --> 00:06:29,760
contain random and unpredictable elements, provide unexpected and appropriate answers to subtle

48
00:06:29,760 --> 00:06:40,240
questions of recognition. And so in this chaotic computing, how can you address Charlotte's question

49
00:06:41,280 --> 00:06:51,920
about our own intelligence and the path forward AI? It is a good observation,

50
00:06:52,080 --> 00:07:05,840
but chaos and unpredictability can also be simulated in computers. Large language models do that,

51
00:07:05,840 --> 00:07:14,640
because you can't always predict how it's going to answer. And a lot of these models,

52
00:07:14,640 --> 00:07:19,720
you can actually ask the same question multiple times and get different answers. So it depends

53
00:07:19,800 --> 00:07:27,400
on kind of the mood of the large language model at that time. And to make it more realistic,

54
00:07:27,400 --> 00:07:44,560
it does have to take that level into account when it answers. At first, we could ask a question and

55
00:07:44,640 --> 00:07:49,680
give you a paragraph that could answer your question. Now, it can actually give you several

56
00:07:49,680 --> 00:08:02,080
pages. It can't, though, give you a whole novel that can be coherent and answer your question. So

57
00:08:02,080 --> 00:08:09,040
it's not able to do what humans can do. Not many humans can do it, but some humans can write a

58
00:08:09,040 --> 00:08:15,760
whole novel that would answer a question. So that's the answer. It has to actually

59
00:08:17,040 --> 00:08:28,320
cover a large amount of material, have an unpredictable element, but also all be coherent

60
00:08:29,120 --> 00:08:44,000
as one work. And we're seeing that happen gradually. Each new large language model is able

61
00:08:44,000 --> 00:08:53,120
to actually cover a much broader array of material. But it definitely can handle stuff that is not

62
00:08:54,080 --> 00:09:01,360
it's not just giving you a predictable amount of

63
00:09:07,120 --> 00:09:14,640
it's it has a way that is not really totally predictable.

64
00:09:16,960 --> 00:09:22,800
So along those lines, let me pose Jane Bernstein's question, which is what is your definition

65
00:09:22,880 --> 00:09:23,680
of intelligence?

66
00:09:29,520 --> 00:09:35,040
I mean, intelligence is to solve difficult problems

67
00:09:40,400 --> 00:09:48,400
with limitations of resources, including time. So you can't take, you know, a million years to solve

68
00:09:48,400 --> 00:10:00,000
a problem. If you can solve it quickly, then you're showing intelligence. And that's why

69
00:10:00,000 --> 00:10:03,520
somebody who's more intelligent might be able to solve problems more quickly.

70
00:10:06,480 --> 00:10:13,520
But we're seeing that in area after area. I mean, Alpha Fold, for example, can actually do things

71
00:10:13,520 --> 00:10:23,920
that humans can't do very quickly or to play something like Go. It goes way beyond what humans

72
00:10:23,920 --> 00:10:32,560
can do. In fact, Lisa Dahl, who's the best human player in Go in the world, says he's not going

73
00:10:32,560 --> 00:10:39,920
to play Go anymore because machines can play it so much better than he can. But that's actually

74
00:10:40,000 --> 00:10:43,200
not my view that it's going to replace us. I think we're going to actually make ourselves

75
00:10:43,200 --> 00:10:52,480
smarter by merging with it, as I said. So I'll ask a question from Sharon Weinstock,

76
00:10:52,480 --> 00:10:59,840
with AI taking over physical and intellectual achievements and individuals living longer.

77
00:10:59,840 --> 00:11:06,160
Do you have thoughts on society and whether individuals risk lacking a purpose?

78
00:11:10,240 --> 00:11:12,240
Well, it's good to hear from you, Sharon.

79
00:11:17,920 --> 00:11:25,840
That's the whole point of our merging with intelligence. I mean, if AI with something

80
00:11:25,840 --> 00:11:35,200
separate from us, it's definitely going to do everything that go way beyond what humans can do.

81
00:11:35,920 --> 00:11:43,520
So we really have to merge with them to make ourselves smarter. But that's why we create these

82
00:11:43,520 --> 00:11:53,360
things. I mean, we're separate from other animals and that we can think of a solution

83
00:11:54,240 --> 00:12:03,280
implemented and then make ourselves better. So if you take what human beings were doing

84
00:12:04,000 --> 00:12:14,800
for work 200 years ago, 80% had to do with creating food. That's now down to 2%.

85
00:12:16,560 --> 00:12:20,560
And so if I were to say, oh, well, you know, all these jobs are going to go away

86
00:12:20,560 --> 00:12:25,360
and machines are going to do them, people say, oh, well, there's nothing for us to do.

87
00:12:26,240 --> 00:12:33,040
But actually, the percentage of people that are employed has gone way up.

88
00:12:34,080 --> 00:12:37,520
The amount of money that we're making per hour has gone way up.

89
00:12:40,320 --> 00:12:45,760
And they say, well, okay, but what are we going to be doing? I said, well, you're going to be doing

90
00:12:45,760 --> 00:12:51,440
IT engineering and protein folding. And no one would have any idea what we're talking about,

91
00:12:52,320 --> 00:12:57,600
because those ideas didn't exist. So we're going to make ourselves smarter.

92
00:12:58,800 --> 00:13:05,680
That's why we create these capabilities. And so it's not going to be us versus AI.

93
00:13:05,680 --> 00:13:11,840
AI is going to go inside of us and make us much smarter than we were before.

94
00:13:13,200 --> 00:13:19,040
So yes, I think if we did not do that, then it would be very difficult to know what you and

95
00:13:19,040 --> 00:13:22,240
beings would be doing, because machines would be doing everything better.

96
00:13:24,000 --> 00:13:27,120
But we're going to be doing it because the AI is going to work through us.

97
00:13:30,560 --> 00:13:36,720
Ronald Wilkinson has a question that relates to your idea of whether it's a dystopian

98
00:13:38,560 --> 00:13:47,040
society or other. But really more specific, he says that he would expect people with various

99
00:13:47,040 --> 00:13:53,440
political and or personal agendas to harness the increasing power of AI for their own purposes.

100
00:13:53,440 --> 00:14:00,080
It will not necessarily be to the long-term benefit of humankind as a whole. So how does this balance

101
00:14:00,080 --> 00:14:06,880
out? Could you go through that again? I don't quite understand. Individuals

102
00:14:07,600 --> 00:14:15,120
with political and personal agendas may use AI for purposes that are not

103
00:14:16,800 --> 00:14:22,000
beneficial to mankind. How does that balance out?

104
00:14:23,520 --> 00:14:31,440
Well, I mean, every new technology has positive and negative aspects. The railroad did tremendous

105
00:14:32,160 --> 00:14:41,680
destruction, but it also benefited society. So it's not that technology is always positive,

106
00:14:42,560 --> 00:14:48,080
social networks. I mean, there's certainly a lot of commentary as to how it is negative,

107
00:14:49,040 --> 00:14:55,760
and that's true. But no one actually would want to do completely without social networks.

108
00:14:56,720 --> 00:15:11,200
And I make the case that we're actually using technology and measuring the kinds of things

109
00:15:11,200 --> 00:15:22,560
that we associate with positive social benefit is actually increasing as the technology gets

110
00:15:22,560 --> 00:15:28,880
better. And that's actually not known. I mean, if you ask a poll as to whether these things are

111
00:15:28,880 --> 00:15:33,360
getting better or worse, people will say they're getting worse, whereas they're actually getting

112
00:15:33,360 --> 00:15:41,680
better. But it's not that everything is positive. I mean, there are negative aspects of it,

113
00:15:41,680 --> 00:15:46,080
and that's why we need to keep working on how we use these technologies.

114
00:15:47,040 --> 00:15:57,520
Here's a question from Mark. The singularity is near. In that book, you speculated that the risk

115
00:15:57,520 --> 00:16:03,760
of bioterrorism, engineering of viruses will become an existential threat.

116
00:16:05,040 --> 00:16:10,400
Since then, do you think this risk to humanity has increased or decreased?

117
00:16:16,800 --> 00:16:26,640
I don't think it's increased. I mean, I have a chapter in singularity is near,

118
00:16:26,640 --> 00:16:35,920
and there's also another one in singularity is nearer on risks. And all of these technologies

119
00:16:36,880 --> 00:16:40,080
have risks, and they could also do us in.

120
00:16:45,120 --> 00:16:54,960
And I don't think the likelihood of that has increased, but I remain optimistic.

121
00:16:56,160 --> 00:16:59,360
And if you look at the actual history of how we use technology,

122
00:16:59,600 --> 00:17:08,320
you could point to various things that should have gone wrong. Like every single job that

123
00:17:08,320 --> 00:17:18,560
we had in 1900, a year ago, a century ago, is gone, and yet we're still working and making

124
00:17:18,560 --> 00:17:30,800
actually more money. So the way we've used technology has been very beneficial to you

125
00:17:30,800 --> 00:17:39,360
in being so far. From Greganus Ova, one of our faculty, Professor at Harvard Medical School,

126
00:17:40,080 --> 00:17:47,440
AI comes with large energy resource demands and rare mineral material needs to build the hardware.

127
00:17:47,440 --> 00:17:51,840
How do you see these international global tensions, especially the interaction

128
00:17:51,840 --> 00:17:59,040
pervasive AI and the climate? I mean, computers don't use that much energy.

129
00:18:02,720 --> 00:18:09,600
In fact, that's the least of our energy needs. And that's a whole other issue we didn't get into,

130
00:18:09,600 --> 00:18:22,240
but the creation of renewable energy sources is on an exponential, a very good chart that shows

131
00:18:23,520 --> 00:18:28,960
all of the renewable energies, and it's on an exponential. And if you follow that out,

132
00:18:30,000 --> 00:18:34,880
we'll be able to provide all of our energy needs on a renewable basis in 10 years.

133
00:18:35,520 --> 00:18:49,440
And at that point, we'll be using one part out of 5,000 parts of the sunlight that hits the earth.

134
00:18:49,440 --> 00:18:57,840
So we have plenty of headroom in that. So we'll actually be able to deal with climate change

135
00:18:58,320 --> 00:19:09,360
through renewable sources. But in terms of what we're using, computers are not that expensive.

136
00:19:12,880 --> 00:19:20,720
From Tim Miller, will the singularity lead to a decrease in class conflict? Much of the gain

137
00:19:20,720 --> 00:19:25,760
in productivity and wealth in the last 50 years has been concentrated in the 1 percent

138
00:19:26,560 --> 00:19:32,720
as inflation adjusted earnings in the working class have stagnated. Are you concerned about

139
00:19:32,720 --> 00:19:41,520
gains in productivity due to AI being unevenly distributed? And Don Goldman similarly comes in

140
00:19:41,520 --> 00:19:50,880
with this related question about inequities that, for example, we saw exacerbated during the COVID

141
00:19:50,880 --> 00:20:01,280
pandemic. I mean, my observation is that more and more people from more and more backgrounds are

142
00:20:01,280 --> 00:20:10,560
participating, which didn't used to third world countries like in Africa, South America, and so on

143
00:20:11,680 --> 00:20:19,600
did not participate to the same extent, whereas they are participating far more dramatically today.

144
00:20:20,960 --> 00:20:30,240
Countries that were really under the weather in terms of being able to participate in these types

145
00:20:30,240 --> 00:20:44,800
of advances are now participating to very smart, very large extent. So I mean, that's my view.

146
00:20:45,520 --> 00:20:52,640
Question from Bill Akava, one of our faculty. A machine can easily beat the best human player

147
00:20:52,640 --> 00:20:58,000
at computer chess, but even a young child can move pieces on the physical board better than

148
00:20:58,000 --> 00:21:04,320
any general purpose robot can. Do you imagine embodied machines will ever pass a physical

149
00:21:04,320 --> 00:21:07,520
Turing test in the real physical world? And if so, when?

150
00:21:07,520 --> 00:21:17,440
Yeah, we're making less progress with robotic machines, but that's also coming along.

151
00:21:18,320 --> 00:21:24,480
And it can also use the same type of machine learning. And we're going to see, I think,

152
00:21:24,480 --> 00:21:29,440
tremendous amount of advances in robotics over the next 10 years.

153
00:21:29,760 --> 00:21:38,400
And for a science fiction a question from Ju Chang, how do you envision society once

154
00:21:38,400 --> 00:21:44,880
individual brains can interface with a cloud? Will individuality still exist? It seems you

155
00:21:44,880 --> 00:21:49,760
imagine human intelligence coalescing into a singular consciousness.

156
00:21:52,320 --> 00:21:56,560
Yes, definitely. I mean, that's one of the requirements of being able to connect to the

157
00:21:56,560 --> 00:22:03,200
cloud is that this is your portion of the cloud and other people can't access it.

158
00:22:04,240 --> 00:22:12,080
And we're actually doing very well on that. And all of our phones connect to the cloud,

159
00:22:12,960 --> 00:22:17,680
and we don't see people complain that other people are getting access to it.

160
00:22:19,600 --> 00:22:25,120
So we're actually doing pretty well on that. But definitely you'll be able to maintain your own

161
00:22:26,880 --> 00:22:32,400
level of personality and differences.

162
00:22:36,240 --> 00:22:39,920
And I think we'll actually be more different than we are today,

163
00:22:41,520 --> 00:22:45,760
given the kinds of skills that we'll be able to develop.

164
00:22:46,720 --> 00:23:00,400
Great. Well, Ray, this has been a spectacular hour we've gotten to spend with you. And I can tell

165
00:23:00,400 --> 00:23:08,400
you that in the lead up to it, I was contacted by many of the folks who were on this webinar

166
00:23:08,400 --> 00:23:16,240
with us today, very excited to meet a celebrity. They never thought they'd have the opportunity to

167
00:23:16,240 --> 00:23:25,360
interact with this closely. So I thank you very kindly. And I also thank you for doing this later

168
00:23:25,360 --> 00:23:34,560
in the evening as you're out at Oxford giving, entertaining the students there as well.

169
00:23:35,360 --> 00:23:39,520
Yeah, it's been great to interact with you and all of your colleagues. It's been,

170
00:23:40,880 --> 00:23:49,440
I've enjoyed it a great deal. Great. Thank you. Let me therefore thank you again,

171
00:23:49,440 --> 00:24:00,640
and I'm going to return to the slides for just a moment to remind people of our upcoming talks

172
00:24:00,640 --> 00:24:08,080
in the series, including I'll highlight Rich Minor next month inventor of the

173
00:24:08,080 --> 00:24:18,640
Android operating system. Also, a Googler who actually resides a lot of the time here in New

174
00:24:18,640 --> 00:24:29,920
England. And the blind folks to be sure to reach out to us at CHIC. If you're interested in

175
00:24:30,640 --> 00:24:40,800
training, directing, researching, teaching, being in our seminar series. Thank you very much.

176
00:24:40,800 --> 00:24:46,960
And we will see you next month.

