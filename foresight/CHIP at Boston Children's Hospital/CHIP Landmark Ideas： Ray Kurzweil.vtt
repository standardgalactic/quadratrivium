WEBVTT

00:00.000 --> 00:10.600
Alright. As folks join the Zoom, I'm going to get started. Welcome everyone to the CHIP

00:10.600 --> 00:18.240
Landmark Ideas series. Today we'll be hearing about rewriting biology with artificial intelligence

00:18.240 --> 00:25.680
for Ray Kurzweil, an inventor and futurist. I'm Ken Mandel. I direct the Computational

00:25.680 --> 00:31.920
Health Informatics program at Boston Children's Hospital. The program was founded in 1994

00:31.920 --> 00:37.560
for a multidisciplinary applied research and education program. To learn more you can

00:37.560 --> 00:45.520
visit www.chip.org. The Landmark Ideas series is an event series featuring thought leaders

00:45.520 --> 00:57.720
across healthcare, informatics, IT, big science, innovation, and more. Dr. Kurzweil is one

00:57.720 --> 01:06.120
of the world's leading inventors, thinkers, and futurists. He creates and predicts using

01:06.120 --> 01:12.560
tools and ideas from the field of pattern recognition. He invented many technologies

01:12.560 --> 01:19.820
familiar to us today, including flatbed scanning, optical character recognition, and text-to-speech

01:19.820 --> 01:26.320
synthesis. He won a Grammy for creating a music synthesizer used by Stevie Wonder that

01:26.320 --> 01:32.920
was capable of recreating the grand piano and other orchestral instruments. He was awarded

01:32.920 --> 01:39.840
the National Medal of Technology. His best-selling books include the New York Times bestsellers

01:39.840 --> 01:45.320
The Singularity is Near and How to Create a Mind. Larry Page brought Kurzweil into

01:45.320 --> 01:55.520
Google as a principal researcher and AI visionary. I'll just mention one connection to Chip.

01:55.520 --> 02:02.520
Ben Rice, a faculty member, when he was a student at MIT, worked with Ray to develop

02:02.520 --> 02:10.960
a text-to-speech interface for that synthesizer so that Stevie Wonder and other non-sided musicians

02:10.960 --> 02:19.200
could interact with the extensive visual navigation interface. The Singularity is a very important

02:19.200 --> 02:26.040
idea of Dr. Kurzweil. This is the point in time when artificial intelligence will surpass

02:26.040 --> 02:35.760
human intelligence, resulting in rapid technological growth that will fundamentally change civilization.

02:35.760 --> 02:43.520
And in order to understand when machines surpass biology, Ray has delved deeply into an understanding

02:43.520 --> 02:50.600
of biology, and we're immensely looking forward to hearing and learning and joining him in

02:50.600 --> 03:01.000
that understanding today. You've got us all looking forward, and the chat and the Q&A have

03:01.000 --> 03:09.880
lit up. Let me start with one question. You're joining us for the seminar five days after

03:09.880 --> 03:18.480
the release of OpenAI's chat GPT, which astounded many across the world in its ability to synthesize

03:18.480 --> 03:24.840
natural language responses to really complicated questions and assignments. And if you've gotten

03:24.840 --> 03:31.880
to glimpse this technology, could you place it on the Kurzweil map toward the Singularity?

03:31.880 --> 03:40.040
Is this a step forward? Is it a distraction? Is it related in any way?

03:40.040 --> 03:48.640
Large language models occurred three years ago, and they seemed quite compelling. They weren't totally

03:48.640 --> 04:03.320
fully there. You could chat with it, and sometimes it would kind of break down. The amount of new

04:03.400 --> 04:11.360
ideas that are going into large language models has been astounding. And so it's like every other

04:11.360 --> 04:17.920
week there's a new large language model and some new variation that's more and more realistic.

04:17.920 --> 04:32.320
So that's going to continue to happen. And so this is just another issue. I mean, there are some

04:32.360 --> 04:44.200
things I think that aren't quite right with that particular model you mentioned. But people have

04:44.200 --> 04:50.040
actually interacted with these things and some people say they're sentient. I don't actually think

04:50.040 --> 04:55.080
they're sentient yet, but I think they're actually moving in that direction. And that's actually

04:55.080 --> 05:03.120
not a scientific issue. It's actually a philosophical issue as to what you consider sentient or not.

05:03.120 --> 05:11.120
Although it's a very important issue, because I would chat with Marvin Minsky, who is my mentor for

05:11.120 --> 05:17.920
50 years, and he said that sentience is not scientific, so therefore forget it, it's an illusion.

05:18.920 --> 05:27.000
That's not my opinion. If you have a world that had no sentience in it, it may well not exist.

05:27.000 --> 05:35.560
But yes, there was a sizable advance, but there's more to come.

05:36.560 --> 05:46.640
Let me ask a question from Charlotte, please. A philosopher, an AI-informed philosopher.

05:46.640 --> 05:53.680
What do you make of the criticism that there's more to intelligence than brute processing speed and

05:53.680 --> 06:00.000
pattern recognition? That if we want to pass the Turing test, we need to learn more about our own

06:01.000 --> 06:14.440
intelligence evolved. And I'll just paraphrase you in The Singularity is Near, comparing cognition to

06:14.440 --> 06:21.840
chaotic computing models where unpredictable interaction of millions of processes, many of which

06:21.840 --> 06:29.760
contain random and unpredictable elements, provide unexpected and appropriate answers to subtle

06:29.760 --> 06:40.240
questions of recognition. And so in this chaotic computing, how can you address Charlotte's question

06:41.280 --> 06:51.920
about our own intelligence and the path forward AI? It is a good observation,

06:52.080 --> 07:05.840
but chaos and unpredictability can also be simulated in computers. Large language models do that,

07:05.840 --> 07:14.640
because you can't always predict how it's going to answer. And a lot of these models,

07:14.640 --> 07:19.720
you can actually ask the same question multiple times and get different answers. So it depends

07:19.800 --> 07:27.400
on kind of the mood of the large language model at that time. And to make it more realistic,

07:27.400 --> 07:44.560
it does have to take that level into account when it answers. At first, we could ask a question and

07:44.640 --> 07:49.680
give you a paragraph that could answer your question. Now, it can actually give you several

07:49.680 --> 08:02.080
pages. It can't, though, give you a whole novel that can be coherent and answer your question. So

08:02.080 --> 08:09.040
it's not able to do what humans can do. Not many humans can do it, but some humans can write a

08:09.040 --> 08:15.760
whole novel that would answer a question. So that's the answer. It has to actually

08:17.040 --> 08:28.320
cover a large amount of material, have an unpredictable element, but also all be coherent

08:29.120 --> 08:44.000
as one work. And we're seeing that happen gradually. Each new large language model is able

08:44.000 --> 08:53.120
to actually cover a much broader array of material. But it definitely can handle stuff that is not

08:54.080 --> 09:01.360
it's not just giving you a predictable amount of

09:07.120 --> 09:14.640
it's it has a way that is not really totally predictable.

09:16.960 --> 09:22.800
So along those lines, let me pose Jane Bernstein's question, which is what is your definition

09:22.880 --> 09:23.680
of intelligence?

09:29.520 --> 09:35.040
I mean, intelligence is to solve difficult problems

09:40.400 --> 09:48.400
with limitations of resources, including time. So you can't take, you know, a million years to solve

09:48.400 --> 10:00.000
a problem. If you can solve it quickly, then you're showing intelligence. And that's why

10:00.000 --> 10:03.520
somebody who's more intelligent might be able to solve problems more quickly.

10:06.480 --> 10:13.520
But we're seeing that in area after area. I mean, Alpha Fold, for example, can actually do things

10:13.520 --> 10:23.920
that humans can't do very quickly or to play something like Go. It goes way beyond what humans

10:23.920 --> 10:32.560
can do. In fact, Lisa Dahl, who's the best human player in Go in the world, says he's not going

10:32.560 --> 10:39.920
to play Go anymore because machines can play it so much better than he can. But that's actually

10:40.000 --> 10:43.200
not my view that it's going to replace us. I think we're going to actually make ourselves

10:43.200 --> 10:52.480
smarter by merging with it, as I said. So I'll ask a question from Sharon Weinstock,

10:52.480 --> 10:59.840
with AI taking over physical and intellectual achievements and individuals living longer.

10:59.840 --> 11:06.160
Do you have thoughts on society and whether individuals risk lacking a purpose?

11:10.240 --> 11:12.240
Well, it's good to hear from you, Sharon.

11:17.920 --> 11:25.840
That's the whole point of our merging with intelligence. I mean, if AI with something

11:25.840 --> 11:35.200
separate from us, it's definitely going to do everything that go way beyond what humans can do.

11:35.920 --> 11:43.520
So we really have to merge with them to make ourselves smarter. But that's why we create these

11:43.520 --> 11:53.360
things. I mean, we're separate from other animals and that we can think of a solution

11:54.240 --> 12:03.280
implemented and then make ourselves better. So if you take what human beings were doing

12:04.000 --> 12:14.800
for work 200 years ago, 80% had to do with creating food. That's now down to 2%.

12:16.560 --> 12:20.560
And so if I were to say, oh, well, you know, all these jobs are going to go away

12:20.560 --> 12:25.360
and machines are going to do them, people say, oh, well, there's nothing for us to do.

12:26.240 --> 12:33.040
But actually, the percentage of people that are employed has gone way up.

12:34.080 --> 12:37.520
The amount of money that we're making per hour has gone way up.

12:40.320 --> 12:45.760
And they say, well, okay, but what are we going to be doing? I said, well, you're going to be doing

12:45.760 --> 12:51.440
IT engineering and protein folding. And no one would have any idea what we're talking about,

12:52.320 --> 12:57.600
because those ideas didn't exist. So we're going to make ourselves smarter.

12:58.800 --> 13:05.680
That's why we create these capabilities. And so it's not going to be us versus AI.

13:05.680 --> 13:11.840
AI is going to go inside of us and make us much smarter than we were before.

13:13.200 --> 13:19.040
So yes, I think if we did not do that, then it would be very difficult to know what you and

13:19.040 --> 13:22.240
beings would be doing, because machines would be doing everything better.

13:24.000 --> 13:27.120
But we're going to be doing it because the AI is going to work through us.

13:30.560 --> 13:36.720
Ronald Wilkinson has a question that relates to your idea of whether it's a dystopian

13:38.560 --> 13:47.040
society or other. But really more specific, he says that he would expect people with various

13:47.040 --> 13:53.440
political and or personal agendas to harness the increasing power of AI for their own purposes.

13:53.440 --> 14:00.080
It will not necessarily be to the long-term benefit of humankind as a whole. So how does this balance

14:00.080 --> 14:06.880
out? Could you go through that again? I don't quite understand. Individuals

14:07.600 --> 14:15.120
with political and personal agendas may use AI for purposes that are not

14:16.800 --> 14:22.000
beneficial to mankind. How does that balance out?

14:23.520 --> 14:31.440
Well, I mean, every new technology has positive and negative aspects. The railroad did tremendous

14:32.160 --> 14:41.680
destruction, but it also benefited society. So it's not that technology is always positive,

14:42.560 --> 14:48.080
social networks. I mean, there's certainly a lot of commentary as to how it is negative,

14:49.040 --> 14:55.760
and that's true. But no one actually would want to do completely without social networks.

14:56.720 --> 15:11.200
And I make the case that we're actually using technology and measuring the kinds of things

15:11.200 --> 15:22.560
that we associate with positive social benefit is actually increasing as the technology gets

15:22.560 --> 15:28.880
better. And that's actually not known. I mean, if you ask a poll as to whether these things are

15:28.880 --> 15:33.360
getting better or worse, people will say they're getting worse, whereas they're actually getting

15:33.360 --> 15:41.680
better. But it's not that everything is positive. I mean, there are negative aspects of it,

15:41.680 --> 15:46.080
and that's why we need to keep working on how we use these technologies.

15:47.040 --> 15:57.520
Here's a question from Mark. The singularity is near. In that book, you speculated that the risk

15:57.520 --> 16:03.760
of bioterrorism, engineering of viruses will become an existential threat.

16:05.040 --> 16:10.400
Since then, do you think this risk to humanity has increased or decreased?

16:16.800 --> 16:26.640
I don't think it's increased. I mean, I have a chapter in singularity is near,

16:26.640 --> 16:35.920
and there's also another one in singularity is nearer on risks. And all of these technologies

16:36.880 --> 16:40.080
have risks, and they could also do us in.

16:45.120 --> 16:54.960
And I don't think the likelihood of that has increased, but I remain optimistic.

16:56.160 --> 16:59.360
And if you look at the actual history of how we use technology,

16:59.600 --> 17:08.320
you could point to various things that should have gone wrong. Like every single job that

17:08.320 --> 17:18.560
we had in 1900, a year ago, a century ago, is gone, and yet we're still working and making

17:18.560 --> 17:30.800
actually more money. So the way we've used technology has been very beneficial to you

17:30.800 --> 17:39.360
in being so far. From Greganus Ova, one of our faculty, Professor at Harvard Medical School,

17:40.080 --> 17:47.440
AI comes with large energy resource demands and rare mineral material needs to build the hardware.

17:47.440 --> 17:51.840
How do you see these international global tensions, especially the interaction

17:51.840 --> 17:59.040
pervasive AI and the climate? I mean, computers don't use that much energy.

18:02.720 --> 18:09.600
In fact, that's the least of our energy needs. And that's a whole other issue we didn't get into,

18:09.600 --> 18:22.240
but the creation of renewable energy sources is on an exponential, a very good chart that shows

18:23.520 --> 18:28.960
all of the renewable energies, and it's on an exponential. And if you follow that out,

18:30.000 --> 18:34.880
we'll be able to provide all of our energy needs on a renewable basis in 10 years.

18:35.520 --> 18:49.440
And at that point, we'll be using one part out of 5,000 parts of the sunlight that hits the earth.

18:49.440 --> 18:57.840
So we have plenty of headroom in that. So we'll actually be able to deal with climate change

18:58.320 --> 19:09.360
through renewable sources. But in terms of what we're using, computers are not that expensive.

19:12.880 --> 19:20.720
From Tim Miller, will the singularity lead to a decrease in class conflict? Much of the gain

19:20.720 --> 19:25.760
in productivity and wealth in the last 50 years has been concentrated in the 1 percent

19:26.560 --> 19:32.720
as inflation adjusted earnings in the working class have stagnated. Are you concerned about

19:32.720 --> 19:41.520
gains in productivity due to AI being unevenly distributed? And Don Goldman similarly comes in

19:41.520 --> 19:50.880
with this related question about inequities that, for example, we saw exacerbated during the COVID

19:50.880 --> 20:01.280
pandemic. I mean, my observation is that more and more people from more and more backgrounds are

20:01.280 --> 20:10.560
participating, which didn't used to third world countries like in Africa, South America, and so on

20:11.680 --> 20:19.600
did not participate to the same extent, whereas they are participating far more dramatically today.

20:20.960 --> 20:30.240
Countries that were really under the weather in terms of being able to participate in these types

20:30.240 --> 20:44.800
of advances are now participating to very smart, very large extent. So I mean, that's my view.

20:45.520 --> 20:52.640
Question from Bill Akava, one of our faculty. A machine can easily beat the best human player

20:52.640 --> 20:58.000
at computer chess, but even a young child can move pieces on the physical board better than

20:58.000 --> 21:04.320
any general purpose robot can. Do you imagine embodied machines will ever pass a physical

21:04.320 --> 21:07.520
Turing test in the real physical world? And if so, when?

21:07.520 --> 21:17.440
Yeah, we're making less progress with robotic machines, but that's also coming along.

21:18.320 --> 21:24.480
And it can also use the same type of machine learning. And we're going to see, I think,

21:24.480 --> 21:29.440
tremendous amount of advances in robotics over the next 10 years.

21:29.760 --> 21:38.400
And for a science fiction a question from Ju Chang, how do you envision society once

21:38.400 --> 21:44.880
individual brains can interface with a cloud? Will individuality still exist? It seems you

21:44.880 --> 21:49.760
imagine human intelligence coalescing into a singular consciousness.

21:52.320 --> 21:56.560
Yes, definitely. I mean, that's one of the requirements of being able to connect to the

21:56.560 --> 22:03.200
cloud is that this is your portion of the cloud and other people can't access it.

22:04.240 --> 22:12.080
And we're actually doing very well on that. And all of our phones connect to the cloud,

22:12.960 --> 22:17.680
and we don't see people complain that other people are getting access to it.

22:19.600 --> 22:25.120
So we're actually doing pretty well on that. But definitely you'll be able to maintain your own

22:26.880 --> 22:32.400
level of personality and differences.

22:36.240 --> 22:39.920
And I think we'll actually be more different than we are today,

22:41.520 --> 22:45.760
given the kinds of skills that we'll be able to develop.

22:46.720 --> 23:00.400
Great. Well, Ray, this has been a spectacular hour we've gotten to spend with you. And I can tell

23:00.400 --> 23:08.400
you that in the lead up to it, I was contacted by many of the folks who were on this webinar

23:08.400 --> 23:16.240
with us today, very excited to meet a celebrity. They never thought they'd have the opportunity to

23:16.240 --> 23:25.360
interact with this closely. So I thank you very kindly. And I also thank you for doing this later

23:25.360 --> 23:34.560
in the evening as you're out at Oxford giving, entertaining the students there as well.

23:35.360 --> 23:39.520
Yeah, it's been great to interact with you and all of your colleagues. It's been,

23:40.880 --> 23:49.440
I've enjoyed it a great deal. Great. Thank you. Let me therefore thank you again,

23:49.440 --> 24:00.640
and I'm going to return to the slides for just a moment to remind people of our upcoming talks

24:00.640 --> 24:08.080
in the series, including I'll highlight Rich Minor next month inventor of the

24:08.080 --> 24:18.640
Android operating system. Also, a Googler who actually resides a lot of the time here in New

24:18.640 --> 24:29.920
England. And the blind folks to be sure to reach out to us at CHIC. If you're interested in

24:30.640 --> 24:40.800
training, directing, researching, teaching, being in our seminar series. Thank you very much.

24:40.800 --> 24:46.960
And we will see you next month.

