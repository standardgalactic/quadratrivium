1
00:00:00,000 --> 00:00:03,920
Does an artificially intelligent chatbot understand what it's chatting about?

2
00:00:05,120 --> 00:00:08,800
A year ago I'd have answered this question with clearly not.

3
00:00:08,800 --> 00:00:12,320
It's just a turbocharger to complete or a stochastic parrot,

4
00:00:12,320 --> 00:00:17,520
as people more eloquent than me have put it, though for all I know they too might be chatbots.

5
00:00:17,520 --> 00:00:22,320
But I've now arrived at the conclusion that the AIs that we use today

6
00:00:22,320 --> 00:00:25,760
do understand what they're doing, if not very much of it.

7
00:00:26,400 --> 00:00:30,800
I'm not saying this just to be controversial, I actually believe it, I believe,

8
00:00:30,800 --> 00:00:34,000
though I have a feeling I might come to regret this video.

9
00:00:34,000 --> 00:00:38,720
I got hung up on this question not because I care so much about chatbots,

10
00:00:38,720 --> 00:00:44,000
but because it echoes the often made claim that no one understands quantum mechanics.

11
00:00:44,720 --> 00:00:50,400
But if we can use quantum mechanics, then doesn't that mean that we understand it

12
00:00:50,400 --> 00:00:56,800
at least to some extent? And consequently, if an AI can use language,

13
00:00:56,800 --> 00:01:00,560
then doesn't that mean it understands it at least to some extent?

14
00:01:01,440 --> 00:01:06,240
What do we mean by understanding? Does chatGPT understand quantum mechanics?

15
00:01:06,880 --> 00:01:11,360
And will AI soon be conscious? That's what we'll talk about today.

16
00:01:15,680 --> 00:01:19,280
The question whether a computer program understands what it's doing certainly

17
00:01:19,280 --> 00:01:25,440
isn't new. In 1980, the American philosopher John Searle argued that the answer is no,

18
00:01:25,440 --> 00:01:29,760
using a thought experiment that's become known as the Chinese Room.

19
00:01:29,760 --> 00:01:34,720
Searle imagines himself in a windowless room with a rulebook and a dropbox.

20
00:01:34,720 --> 00:01:40,320
If someone drops him a note written in Chinese, he looks up the symbols in his rulebook.

21
00:01:40,320 --> 00:01:45,440
The rulebook gives him an English translation, which he returns as an answer through a slit in

22
00:01:45,440 --> 00:01:50,480
the door, no dealt drawing on the everyday experience of a professor of philosophy.

23
00:01:50,480 --> 00:01:55,840
Searle argues that the person outside the room might believe that there's someone inside who

24
00:01:55,840 --> 00:02:01,600
understands Chinese. But really, he still doesn't understand the word of it, he's just following

25
00:02:01,600 --> 00:02:07,360
the rules he's been given. Searle argues that a computer program works like that,

26
00:02:07,360 --> 00:02:11,360
without any true understanding, just following the rules.

27
00:02:11,360 --> 00:02:16,080
There are two standard objections that people bring forward against Searle's argument.

28
00:02:16,080 --> 00:02:22,240
One is that the system which understands Chinese isn't just the person inside the room,

29
00:02:22,240 --> 00:02:28,240
but the person including the rulebook. So saying that the person doesn't understand Chinese

30
00:02:28,240 --> 00:02:33,120
might be correct, but doesn't answer the question because in Searle's analogy,

31
00:02:33,120 --> 00:02:36,720
the person alone doesn't represent the computer program.

32
00:02:36,800 --> 00:02:41,600
Another objection is that it might well be correct that Searle and his rulebook don't

33
00:02:41,600 --> 00:02:47,600
understand Chinese, but that's because the input is so limited. Language lacks the physical

34
00:02:47,600 --> 00:02:53,920
information that we have learned to associate with words. Software that had the same physical

35
00:02:53,920 --> 00:02:59,360
information could develop understanding as we do. Unless, of course, we live in a computer

36
00:02:59,360 --> 00:03:04,080
simulation in which case you can file complaints using the contact form in the bottom right corner

37
00:03:04,080 --> 00:03:09,120
of your frontal lobe. I think both of these objections missed the point, but before I explain

38
00:03:09,120 --> 00:03:14,960
that, I want to introduce you to the quantum room. Quantum mechanics works pretty much like

39
00:03:14,960 --> 00:03:21,360
Searle's Chinese room. It's a rulebook, a set of equations with instructions for how to use them.

40
00:03:21,360 --> 00:03:26,000
You give me a question, I look into my rulebook that I keep in my windowless room,

41
00:03:26,000 --> 00:03:31,440
and I return an answer to you through the slit in the door. Do I understand quantum mechanics?

42
00:03:32,240 --> 00:03:38,240
Searle would probably argue no. Indeed, for the most part, physicists today aren't even in the

43
00:03:38,240 --> 00:03:42,960
room, because who wants to spend their time sitting in a windowless room with a dropbox

44
00:03:42,960 --> 00:03:48,800
when they can instead sit in a windowless room with their laser? No, we're now the ones putting

45
00:03:48,800 --> 00:03:55,440
a question into the dropbox, so to speak, by feeding it into a computer. The computer crunches

46
00:03:55,440 --> 00:04:02,320
the numbers and returns an answer. Do we understand those answers? Have we gone too far with shut up

47
00:04:02,320 --> 00:04:08,240
and calculate? Is the room even there when no one looks? Those are all very interesting questions,

48
00:04:08,240 --> 00:04:13,920
but let's not get carried away. We were trying to talk about chatbots, so let's have a look at those.

49
00:04:14,480 --> 00:04:20,400
Today's language generating models are somewhat more sophisticated than just lookup tables like

50
00:04:20,480 --> 00:04:26,800
Searle imagined, and what better way is there to explain how they work than asking one itself?

51
00:04:28,160 --> 00:04:32,400
Language generating models, like me, are built using deep learning techniques,

52
00:04:32,400 --> 00:04:37,600
specifically a type of neural network. These models are trained on large amounts of text data,

53
00:04:37,600 --> 00:04:42,800
such as books, articles, and websites, and learn to generate language by identifying patterns

54
00:04:42,800 --> 00:04:47,600
and relationships between words and phrases. When generating language, the model takes an

55
00:04:47,600 --> 00:04:52,240
initial input, such as a prompt or a question, and uses the patterns it has learned to generate a

56
00:04:52,240 --> 00:04:57,680
response. The generated text is not simply copied from the training data, but rather the model uses

57
00:04:57,680 --> 00:05:05,520
the patterns it has learned to create new, original text. Well, that was not awkward at all, but yes,

58
00:05:05,520 --> 00:05:11,520
neural networks indeed learn similar to how humans learn. They don't just memorize input,

59
00:05:11,520 --> 00:05:18,160
they identify patterns and extrapolate them. They still have many differences to the human brain,

60
00:05:18,160 --> 00:05:24,320
at least at the moment. Most importantly, the neurons in a neural network are themselves

61
00:05:24,320 --> 00:05:30,320
part of the algorithm and not physical as they are in the human brain. And the human brain

62
00:05:30,320 --> 00:05:36,480
has a lot more structure with parts specialized for particular purposes, but neural networks do

63
00:05:36,560 --> 00:05:43,040
capture some aspects of how humans learn. And that brings us to the first important point when

64
00:05:43,040 --> 00:05:48,880
it comes to the question of understanding. Suppose you have children in elementary school

65
00:05:48,880 --> 00:05:55,440
and have them memorize the multiplication tables up to 10. If you want to test whether they understood

66
00:05:55,440 --> 00:06:02,160
multiplication, you asked them something that wasn't on the tables. We want to test whether

67
00:06:02,160 --> 00:06:08,640
they have identified the pattern and can use it on something else. If you're in the Chinese room

68
00:06:08,640 --> 00:06:15,600
with a long list of examples, you can't answer a question that isn't on the list. This is indeed

69
00:06:15,600 --> 00:06:21,920
not what anyone means by understanding, so I'd say Sol is right on that account. But this is

70
00:06:21,920 --> 00:06:28,320
not what neural networks do. Neural networks do instead exactly what we mean by understanding

71
00:06:28,320 --> 00:06:34,080
when we apply it to humans. They extract the pattern and apply it to something they haven't

72
00:06:34,080 --> 00:06:40,400
seen before. But this brings up another question. How do you know that that's what it's doing?

73
00:06:41,040 --> 00:06:46,560
If you ask a child to multiply two numbers, how do you know they haven't just memorized the result?

74
00:06:47,280 --> 00:06:54,160
Well, you don't. If you want to know whether someone or something understands, looking at the

75
00:06:54,160 --> 00:07:00,800
input and output isn't enough. You could always produce the output by a lookup table rather than

76
00:07:00,800 --> 00:07:06,560
with a system that has learned to identify patterns. And you can well understand something

77
00:07:06,560 --> 00:07:12,800
without producing any output, like you might understand this video without any output other

78
00:07:12,800 --> 00:07:19,440
than maybe the occasional frown. I'd therefore say that what we mean by understanding something

79
00:07:19,440 --> 00:07:24,320
is the ability to create a useful model of the thing we're trying to understand.

80
00:07:24,880 --> 00:07:31,040
The model is something I have in my head that I can ask questions about the real thing and that

81
00:07:31,040 --> 00:07:37,920
it's useful means it has to be reasonably correct. It captures at least some properties of the real

82
00:07:37,920 --> 00:07:44,720
thing. In mathematical terms, you might say there's an isomorphism, a one-to-one map between the model

83
00:07:44,720 --> 00:07:52,400
and the real thing. I have a model, for example, for cows. Cows stand on meadows, have four legs

84
00:07:52,400 --> 00:07:58,880
and sometimes go moo. If you pull in the right place, moo comes out. Not a particularly sophisticated

85
00:07:58,880 --> 00:08:04,560
model, I admit, but I'll work on it once cows start watching YouTube. Understanding then is

86
00:08:04,560 --> 00:08:09,840
something that happens inside a system. You can probe parts of this understanding with input

87
00:08:09,840 --> 00:08:15,920
output tests, but that alone can't settle the question. When we're talking about neural networks,

88
00:08:15,920 --> 00:08:22,000
however, we actually know they're not lookup tables because we've programmed them and trained

89
00:08:22,000 --> 00:08:28,160
them. So we can be pretty sure they actually must have a model of the thing they've been trained for

90
00:08:28,800 --> 00:08:34,720
somewhere in their neural weights. In fact, at this moment in the history of mankind, we can be

91
00:08:34,720 --> 00:08:39,840
more confident that neural nets understand something than your average first grader because,

92
00:08:39,840 --> 00:08:45,280
for all we can tell, the first graders just ask a chatbot. Let's then look at the question of

93
00:08:45,280 --> 00:08:52,560
who understands what and why. We have a model of the human body in our brain. This allows us to

94
00:08:52,560 --> 00:08:59,120
understand what effects our movements will have, how humans move in general, and which parts belong

95
00:08:59,120 --> 00:09:06,640
where. We notice immediately if something is off. But if you train an AI on two-dimensional images,

96
00:09:06,640 --> 00:09:13,200
it doesn't automatically map those images onto a 3D model. This is why it'll sometimes create

97
00:09:13,200 --> 00:09:19,200
weird things like people with half a leg or three arms or something like that. This, for example,

98
00:09:19,200 --> 00:09:25,760
is mid-journey trying to show a person tying their shoelaces. They look kind of right because it's

99
00:09:25,760 --> 00:09:31,120
what the AI was trained to do to produce an image that looks kind of right, but they don't actually

100
00:09:31,120 --> 00:09:37,120
capture the real thing. If you take understanding to mean that it has a model of what's going on,

101
00:09:37,120 --> 00:09:43,840
then these AIs almost certainly understand the relation between shadows and lights. But does it

102
00:09:43,840 --> 00:09:49,360
know that shadows and light are created by electromagnetic radiation bouncing off or being

103
00:09:49,360 --> 00:09:55,520
absorbed by three-dimensional bodies? It can't because it never got that information.

104
00:09:55,520 --> 00:10:01,680
You can instead give an AI a 3D model and train it to match images to that 3D model.

105
00:10:01,680 --> 00:10:07,040
This is basically how deep fakes work. And in this case, I'd say that the AI actually does

106
00:10:07,040 --> 00:10:13,680
partly understand the motion of certain body parts. The issue with chatbots is more complicated

107
00:10:13,680 --> 00:10:19,920
because language is much more loosely tied to reality than videos or photographs. Language

108
00:10:19,920 --> 00:10:24,800
is a method that humans have invented to exchange information about these models that

109
00:10:24,800 --> 00:10:32,400
we have in our own heads. Written language is, moreover, a reduced version of spoken language.

110
00:10:32,400 --> 00:10:37,920
It does capture some essence of reality in relations between words. And if you train a neural

111
00:10:37,920 --> 00:10:44,640
network on that, it'll learn those relations, but a lot of information will be missing. Take

112
00:10:44,640 --> 00:10:51,280
the sentence, what goes up must come down. That's, for reasonably common initial conditions,

113
00:10:51,280 --> 00:10:57,520
a statement about Newton's law of gravity. Further text analysis might tell you that by

114
00:10:57,520 --> 00:11:03,600
down we mean towards the ground, and that the ground is a planet called Earth, which is a sphere,

115
00:11:03,600 --> 00:11:10,720
and so on. From that alone, you may have no idea what any of these words mean, but you know how

116
00:11:10,720 --> 00:11:16,720
they are related. And indeed, if you ask chatGPT what happens when you throw a stone into the air,

117
00:11:16,800 --> 00:11:21,440
it'll tell you the bluntly obvious and several flawlessly correct paragraphs.

118
00:11:22,160 --> 00:11:28,560
But the language model can't do more than try to infer relations between words

119
00:11:28,560 --> 00:11:34,800
because it didn't get any other data. This is why chatGPT is ridiculously bad at anything that

120
00:11:34,800 --> 00:11:40,800
requires, for example, understanding spatial relationships, like latitude. I asked it whether

121
00:11:40,800 --> 00:11:46,080
Windsor UK is further north or south than Toronto, Canada, and they told me,

122
00:11:46,640 --> 00:11:53,440
Windsor is located at approximately 51.5 degrees north latitude, while Toronto is located at

123
00:11:53,440 --> 00:12:00,720
approximately 43.7 degrees north latitude. Therefore, Toronto is further north than Windsor.

124
00:12:00,720 --> 00:12:07,680
It'll quote the latitudes correctly, but draw the exactly wrong conclusion. It's a funny mistake

125
00:12:07,680 --> 00:12:12,960
because it'd be easy to fix by quiping it with a three-dimensional model of planet Earth,

126
00:12:12,960 --> 00:12:17,120
but it doesn't have such a model. It only knows the relations between words.

127
00:12:17,760 --> 00:12:23,360
For the same reason chatGPT has some rather elementary misunderstandings about quantum mechanics.

128
00:12:23,920 --> 00:12:30,560
But let me ask you first. Imagine you have two entangled particles and you separate them.

129
00:12:30,560 --> 00:12:36,160
One goes left and the other goes right, but like couples after a fight, they're still linked,

130
00:12:36,160 --> 00:12:42,720
whether they want to or not. That they are entangled means that they share a measurable property,

131
00:12:42,720 --> 00:12:48,480
but you don't know which particle has which share. It could be, for example, that they each

132
00:12:48,480 --> 00:12:55,200
either have spin plus or minus one, and the spin has to add up to zero. If you measure them,

133
00:12:55,200 --> 00:13:01,280
either the one going left has spin plus one and the one going right minus one or the other way

134
00:13:02,080 --> 00:13:08,800
and if you measure one particle, you know immediately what the spin of the other particle is.

135
00:13:08,800 --> 00:13:14,800
But let's say you don't measure them right away. Instead, you first perform an operation on one of

136
00:13:14,800 --> 00:13:20,320
the particles. This is physics, so when I say operation, I don't mean heart surgery, but

137
00:13:20,320 --> 00:13:26,880
something a little more sophisticated. For example, you flip its spin. Such an operation is not a

138
00:13:26,880 --> 00:13:32,560
measurement because it doesn't allow you to determine what the spin is. If you do this on

139
00:13:32,560 --> 00:13:38,720
one particle, what happens to the other particle? If you don't know the answer, that's perfectly

140
00:13:38,720 --> 00:13:44,640
fine because you can't answer the question from what I've told you. The correct answer is that

141
00:13:44,640 --> 00:13:51,040
nothing happens to the other particle. This is obvious if you know how the mathematics works

142
00:13:51,040 --> 00:13:58,240
because if you flip the spin, that operation only acts on one side. But it's not obvious from a

143
00:13:58,240 --> 00:14:03,840
verbal description of quantum mechanics, which is why it's a common confusion in the popular

144
00:14:03,840 --> 00:14:10,960
science press. Because of that, it's a confusion that chat GPT is likely to have too. And indeed,

145
00:14:10,960 --> 00:14:17,360
when I asked that question, it got it wrong. So I'd recommend you don't trust chat GPT on

146
00:14:17,360 --> 00:14:24,320
quantum mechanics until it speaks fluent latich. But ask it any word-related question and it

147
00:14:24,320 --> 00:14:30,880
shines. One of the best uses for chat GPT that I have found is English grammar or word use questions.

148
00:14:30,880 --> 00:14:37,120
As I was working on this video, for example, I was wondering whether Dropbox is actually a word

149
00:14:37,120 --> 00:14:42,240
or just the name of an app. How am I supposed to know? I've never heard anyone use the word for

150
00:14:42,240 --> 00:14:49,040
anything besides the app. If you type this question into your search engine of choice,

151
00:14:49,040 --> 00:14:55,280
the only thing you get is a gazillion hits explaining how Dropbox the app works.

152
00:14:56,080 --> 00:15:02,000
Ask the question to chat GPT and it'll tell you that yes, Dropbox is a word that English

153
00:15:02,000 --> 00:15:08,880
native speakers will understand. For the same reason, chat GPT is really good at listing pros

154
00:15:08,880 --> 00:15:15,600
and cons for certain arguments because those are words which stand in relation to the question.

155
00:15:15,600 --> 00:15:21,200
It's also good at finding technical terms and keywords from rather vague verbal descriptions.

156
00:15:21,920 --> 00:15:28,160
For example, I asked it, what's the name for this effect where things get shorter when you move at

157
00:15:28,160 --> 00:15:34,560
high speed? It explained, the name of the effect you are referring to is length contraction or

158
00:15:34,560 --> 00:15:39,120
Lawrence contraction. It is a consequence of the theory of special relativity.

159
00:15:39,680 --> 00:15:44,080
Which is perfectly correct. But don't ask it how English words are pronounced,

160
00:15:44,080 --> 00:15:49,520
it makes even more mistakes than I do. What does this tell us about whether we

161
00:15:49,520 --> 00:15:56,080
understand quantum mechanics? I've argued that understanding can't be inferred from the relation

162
00:15:56,080 --> 00:16:02,240
between input and output alone. The relevant question is instead whether a system has a model

163
00:16:02,240 --> 00:16:07,680
of what it's trying to understand, a model that it can use to explain what's going on.

164
00:16:08,320 --> 00:16:15,120
And I'd say this is definitely the case for physicists who use quantum mechanics. I have a

165
00:16:15,120 --> 00:16:20,960
model inside my head for how quantum mechanics works. It's a set of equations that I have used

166
00:16:20,960 --> 00:16:26,800
many times that I know how to apply and use to answer questions. And I'm sure the same is the

167
00:16:26,800 --> 00:16:32,480
case for other physicists. The problem with quantum mechanics is that those equations

168
00:16:32,480 --> 00:16:39,200
do not correspond to words we use in everyday language. Most of the problems we see with

169
00:16:39,200 --> 00:16:45,520
understanding quantum mechanics come from the impossibility of expressing the equations in words.

170
00:16:46,160 --> 00:16:51,360
At least in English. For all I know, you can do it in Chinese. Maybe that explains why the

171
00:16:51,360 --> 00:16:57,440
Chinese are so good with quantum technologies. It is of course possible to just convert equations

172
00:16:57,440 --> 00:17:03,760
into words by reading them out. But we normally don't do that. What we do in science communication

173
00:17:03,760 --> 00:17:11,040
is kind of a mixture with metaphors and attempts to explain some of the maths. And that conveys

174
00:17:11,040 --> 00:17:18,080
some aspects of how the equations work. But if you take the words too literally, they stop making

175
00:17:18,080 --> 00:17:24,320
sense. But equations aren't necessary for understanding. You can also gain understanding

176
00:17:24,320 --> 00:17:30,080
of quantum mechanics by games or apps that visualize the behavior of the equations,

177
00:17:30,080 --> 00:17:36,000
like those that I talked about in an earlier video. That too will allow you to build a model

178
00:17:36,000 --> 00:17:42,480
inside your head for how quantum mechanics works. This is why I'd also say that if we use computer

179
00:17:42,560 --> 00:17:49,120
simulations and visualizations in science, especially for complex problems, that doesn't mean

180
00:17:49,120 --> 00:17:55,360
we've given up on understanding. Visualizing the behavior of a system and probing it and seeing

181
00:17:55,360 --> 00:18:02,080
what it does is another way of building a model in your head. There is another reason why physicists

182
00:18:02,080 --> 00:18:06,800
say they don't understand quantum mechanics, which is that it's internally inconsistent.

183
00:18:06,800 --> 00:18:11,680
I've talked about this a few times before and it's somewhat off topic here, so I don't want to get

184
00:18:11,680 --> 00:18:17,280
into this again. Let me just say that there are problems with quantum mechanics that go beyond

185
00:18:17,280 --> 00:18:23,840
the difficulty of expressing it in words. So where will the AI boom leaders? First of all,

186
00:18:23,840 --> 00:18:29,440
it's rather foreseeable that before long, we'll all have a personalized AI that'll offer anything

187
00:18:29,440 --> 00:18:35,520
from financial advice to relationship counseling. The more you can afford to pay, the better it'll

188
00:18:35,520 --> 00:18:40,720
be and the free version will suggest you marry the Prince of Nigeria. Of course, people are going

189
00:18:40,720 --> 00:18:45,840
to complain it'll destroy the world and all, but it'll happen anyway because when has the risk

190
00:18:45,840 --> 00:18:50,880
of destroying the world ever stopped us from doing anything if there was money to make with it?

191
00:18:51,440 --> 00:18:56,400
The best and biggest AIs will be those of big companies and governments,

192
00:18:56,400 --> 00:19:02,960
and that's almost guaranteed to increase wealth disparities. We're also going to see YouTube

193
00:19:02,960 --> 00:19:10,160
flooded by human avatars and other funky AI-generated visuals because it's much faster and cheaper

194
00:19:10,160 --> 00:19:15,440
than getting a human to retext or go out and film that old-fashioned thing called reality.

195
00:19:16,000 --> 00:19:21,120
But I don't think this trend will last long because it'll be extremely difficult to make

196
00:19:21,120 --> 00:19:27,280
money with it. The easier it becomes to create artificial footage, the more people will look

197
00:19:27,280 --> 00:19:32,800
for authenticity, so that stupid German accent might eventually actually be good for something.

198
00:19:32,800 --> 00:19:38,240
If nothing else, it makes me difficult to simulate. Will AI eventually become conscious?

199
00:19:38,800 --> 00:19:44,320
Of course. There's nothing magic about the human brain, it's just a lot of connections that process

200
00:19:44,320 --> 00:19:50,880
a lot of information. If we can be conscious, computers can do it too, and it will happen

201
00:19:50,880 --> 00:19:58,320
eventually. How will we know? Like understanding, you can't probe consciousness just by observing

202
00:19:58,320 --> 00:20:04,400
what goes in and comes out. If you'd really want to know, you'd have to look what's going on inside

203
00:20:04,400 --> 00:20:10,160
and at the moment, that wouldn't help because we don't know how to identify consciousness in any

204
00:20:10,160 --> 00:20:16,880
case. Basically, we can't answer the question. But personally, I find this extremely interesting

205
00:20:16,880 --> 00:20:23,040
because we're about to create an intelligent species that'll be very different from our own.

206
00:20:23,040 --> 00:20:27,120
And if we're dumb enough to cause our own extinction this way, then I guess that's what

207
00:20:27,120 --> 00:20:33,280
we deserve. Meanwhile, enjoy the ride. At least for now, the best tool we have for understanding

208
00:20:33,280 --> 00:20:38,800
the world is the human brain. But if you really want to understand quantum mechanics or neural

209
00:20:38,800 --> 00:20:45,200
networks, then passively watching a video isn't enough. You have to actively engage with the

210
00:20:45,200 --> 00:20:51,600
material. Brilliant.org, who have been sponsoring this video, is a great place for that. Brilliant

211
00:20:51,600 --> 00:20:57,760
offers courses on a large variety of subjects in science and mathematics, and they add new content

212
00:20:57,760 --> 00:21:03,920
every month. The great thing about their courses is that they're all interactive with visualizations

213
00:21:03,920 --> 00:21:10,000
and follow-up questions. So you can check right away whether you can apply what you've learned,

214
00:21:10,000 --> 00:21:16,000
and that's really what understanding is all about. What I need to freshen up my knowledge or want to

215
00:21:16,000 --> 00:21:22,400
learn something new, first thing I do is look it up on Brilliant. To get some background on the physics

216
00:21:22,400 --> 00:21:28,240
in this video, check out, for example, their courses on neural networks and quantum objects,

217
00:21:28,240 --> 00:21:35,200
or even better, check out my own course about quantum mechanics. My course gives you an introduction

218
00:21:35,200 --> 00:21:41,600
to interference, superpositions and entanglement, the uncertainty principle, and Bell's theorem.

219
00:21:42,240 --> 00:21:47,360
You don't need to be an expert to take this course. I've worked together with Brilliant so

220
00:21:47,360 --> 00:21:52,560
that you can start from the very basics. If you're interested in trying Brilliant out,

221
00:21:52,560 --> 00:21:58,800
use our link brilliant.org slash Sabine and sign up for free trial, where you'll get to try out

222
00:21:58,800 --> 00:22:04,800
everything Brilliant has to offer for 30 days. The first 200 subscribers using this link will also

223
00:22:04,800 --> 00:22:13,600
get 20% off the annual premium subscription. Thanks for watching. See you next week.

