WEBVTT

00:00.000 --> 00:03.920
Does an artificially intelligent chatbot understand what it's chatting about?

00:05.120 --> 00:08.800
A year ago I'd have answered this question with clearly not.

00:08.800 --> 00:12.320
It's just a turbocharger to complete or a stochastic parrot,

00:12.320 --> 00:17.520
as people more eloquent than me have put it, though for all I know they too might be chatbots.

00:17.520 --> 00:22.320
But I've now arrived at the conclusion that the AIs that we use today

00:22.320 --> 00:25.760
do understand what they're doing, if not very much of it.

00:26.400 --> 00:30.800
I'm not saying this just to be controversial, I actually believe it, I believe,

00:30.800 --> 00:34.000
though I have a feeling I might come to regret this video.

00:34.000 --> 00:38.720
I got hung up on this question not because I care so much about chatbots,

00:38.720 --> 00:44.000
but because it echoes the often made claim that no one understands quantum mechanics.

00:44.720 --> 00:50.400
But if we can use quantum mechanics, then doesn't that mean that we understand it

00:50.400 --> 00:56.800
at least to some extent? And consequently, if an AI can use language,

00:56.800 --> 01:00.560
then doesn't that mean it understands it at least to some extent?

01:01.440 --> 01:06.240
What do we mean by understanding? Does chatGPT understand quantum mechanics?

01:06.880 --> 01:11.360
And will AI soon be conscious? That's what we'll talk about today.

01:15.680 --> 01:19.280
The question whether a computer program understands what it's doing certainly

01:19.280 --> 01:25.440
isn't new. In 1980, the American philosopher John Searle argued that the answer is no,

01:25.440 --> 01:29.760
using a thought experiment that's become known as the Chinese Room.

01:29.760 --> 01:34.720
Searle imagines himself in a windowless room with a rulebook and a dropbox.

01:34.720 --> 01:40.320
If someone drops him a note written in Chinese, he looks up the symbols in his rulebook.

01:40.320 --> 01:45.440
The rulebook gives him an English translation, which he returns as an answer through a slit in

01:45.440 --> 01:50.480
the door, no dealt drawing on the everyday experience of a professor of philosophy.

01:50.480 --> 01:55.840
Searle argues that the person outside the room might believe that there's someone inside who

01:55.840 --> 02:01.600
understands Chinese. But really, he still doesn't understand the word of it, he's just following

02:01.600 --> 02:07.360
the rules he's been given. Searle argues that a computer program works like that,

02:07.360 --> 02:11.360
without any true understanding, just following the rules.

02:11.360 --> 02:16.080
There are two standard objections that people bring forward against Searle's argument.

02:16.080 --> 02:22.240
One is that the system which understands Chinese isn't just the person inside the room,

02:22.240 --> 02:28.240
but the person including the rulebook. So saying that the person doesn't understand Chinese

02:28.240 --> 02:33.120
might be correct, but doesn't answer the question because in Searle's analogy,

02:33.120 --> 02:36.720
the person alone doesn't represent the computer program.

02:36.800 --> 02:41.600
Another objection is that it might well be correct that Searle and his rulebook don't

02:41.600 --> 02:47.600
understand Chinese, but that's because the input is so limited. Language lacks the physical

02:47.600 --> 02:53.920
information that we have learned to associate with words. Software that had the same physical

02:53.920 --> 02:59.360
information could develop understanding as we do. Unless, of course, we live in a computer

02:59.360 --> 03:04.080
simulation in which case you can file complaints using the contact form in the bottom right corner

03:04.080 --> 03:09.120
of your frontal lobe. I think both of these objections missed the point, but before I explain

03:09.120 --> 03:14.960
that, I want to introduce you to the quantum room. Quantum mechanics works pretty much like

03:14.960 --> 03:21.360
Searle's Chinese room. It's a rulebook, a set of equations with instructions for how to use them.

03:21.360 --> 03:26.000
You give me a question, I look into my rulebook that I keep in my windowless room,

03:26.000 --> 03:31.440
and I return an answer to you through the slit in the door. Do I understand quantum mechanics?

03:32.240 --> 03:38.240
Searle would probably argue no. Indeed, for the most part, physicists today aren't even in the

03:38.240 --> 03:42.960
room, because who wants to spend their time sitting in a windowless room with a dropbox

03:42.960 --> 03:48.800
when they can instead sit in a windowless room with their laser? No, we're now the ones putting

03:48.800 --> 03:55.440
a question into the dropbox, so to speak, by feeding it into a computer. The computer crunches

03:55.440 --> 04:02.320
the numbers and returns an answer. Do we understand those answers? Have we gone too far with shut up

04:02.320 --> 04:08.240
and calculate? Is the room even there when no one looks? Those are all very interesting questions,

04:08.240 --> 04:13.920
but let's not get carried away. We were trying to talk about chatbots, so let's have a look at those.

04:14.480 --> 04:20.400
Today's language generating models are somewhat more sophisticated than just lookup tables like

04:20.480 --> 04:26.800
Searle imagined, and what better way is there to explain how they work than asking one itself?

04:28.160 --> 04:32.400
Language generating models, like me, are built using deep learning techniques,

04:32.400 --> 04:37.600
specifically a type of neural network. These models are trained on large amounts of text data,

04:37.600 --> 04:42.800
such as books, articles, and websites, and learn to generate language by identifying patterns

04:42.800 --> 04:47.600
and relationships between words and phrases. When generating language, the model takes an

04:47.600 --> 04:52.240
initial input, such as a prompt or a question, and uses the patterns it has learned to generate a

04:52.240 --> 04:57.680
response. The generated text is not simply copied from the training data, but rather the model uses

04:57.680 --> 05:05.520
the patterns it has learned to create new, original text. Well, that was not awkward at all, but yes,

05:05.520 --> 05:11.520
neural networks indeed learn similar to how humans learn. They don't just memorize input,

05:11.520 --> 05:18.160
they identify patterns and extrapolate them. They still have many differences to the human brain,

05:18.160 --> 05:24.320
at least at the moment. Most importantly, the neurons in a neural network are themselves

05:24.320 --> 05:30.320
part of the algorithm and not physical as they are in the human brain. And the human brain

05:30.320 --> 05:36.480
has a lot more structure with parts specialized for particular purposes, but neural networks do

05:36.560 --> 05:43.040
capture some aspects of how humans learn. And that brings us to the first important point when

05:43.040 --> 05:48.880
it comes to the question of understanding. Suppose you have children in elementary school

05:48.880 --> 05:55.440
and have them memorize the multiplication tables up to 10. If you want to test whether they understood

05:55.440 --> 06:02.160
multiplication, you asked them something that wasn't on the tables. We want to test whether

06:02.160 --> 06:08.640
they have identified the pattern and can use it on something else. If you're in the Chinese room

06:08.640 --> 06:15.600
with a long list of examples, you can't answer a question that isn't on the list. This is indeed

06:15.600 --> 06:21.920
not what anyone means by understanding, so I'd say Sol is right on that account. But this is

06:21.920 --> 06:28.320
not what neural networks do. Neural networks do instead exactly what we mean by understanding

06:28.320 --> 06:34.080
when we apply it to humans. They extract the pattern and apply it to something they haven't

06:34.080 --> 06:40.400
seen before. But this brings up another question. How do you know that that's what it's doing?

06:41.040 --> 06:46.560
If you ask a child to multiply two numbers, how do you know they haven't just memorized the result?

06:47.280 --> 06:54.160
Well, you don't. If you want to know whether someone or something understands, looking at the

06:54.160 --> 07:00.800
input and output isn't enough. You could always produce the output by a lookup table rather than

07:00.800 --> 07:06.560
with a system that has learned to identify patterns. And you can well understand something

07:06.560 --> 07:12.800
without producing any output, like you might understand this video without any output other

07:12.800 --> 07:19.440
than maybe the occasional frown. I'd therefore say that what we mean by understanding something

07:19.440 --> 07:24.320
is the ability to create a useful model of the thing we're trying to understand.

07:24.880 --> 07:31.040
The model is something I have in my head that I can ask questions about the real thing and that

07:31.040 --> 07:37.920
it's useful means it has to be reasonably correct. It captures at least some properties of the real

07:37.920 --> 07:44.720
thing. In mathematical terms, you might say there's an isomorphism, a one-to-one map between the model

07:44.720 --> 07:52.400
and the real thing. I have a model, for example, for cows. Cows stand on meadows, have four legs

07:52.400 --> 07:58.880
and sometimes go moo. If you pull in the right place, moo comes out. Not a particularly sophisticated

07:58.880 --> 08:04.560
model, I admit, but I'll work on it once cows start watching YouTube. Understanding then is

08:04.560 --> 08:09.840
something that happens inside a system. You can probe parts of this understanding with input

08:09.840 --> 08:15.920
output tests, but that alone can't settle the question. When we're talking about neural networks,

08:15.920 --> 08:22.000
however, we actually know they're not lookup tables because we've programmed them and trained

08:22.000 --> 08:28.160
them. So we can be pretty sure they actually must have a model of the thing they've been trained for

08:28.800 --> 08:34.720
somewhere in their neural weights. In fact, at this moment in the history of mankind, we can be

08:34.720 --> 08:39.840
more confident that neural nets understand something than your average first grader because,

08:39.840 --> 08:45.280
for all we can tell, the first graders just ask a chatbot. Let's then look at the question of

08:45.280 --> 08:52.560
who understands what and why. We have a model of the human body in our brain. This allows us to

08:52.560 --> 08:59.120
understand what effects our movements will have, how humans move in general, and which parts belong

08:59.120 --> 09:06.640
where. We notice immediately if something is off. But if you train an AI on two-dimensional images,

09:06.640 --> 09:13.200
it doesn't automatically map those images onto a 3D model. This is why it'll sometimes create

09:13.200 --> 09:19.200
weird things like people with half a leg or three arms or something like that. This, for example,

09:19.200 --> 09:25.760
is mid-journey trying to show a person tying their shoelaces. They look kind of right because it's

09:25.760 --> 09:31.120
what the AI was trained to do to produce an image that looks kind of right, but they don't actually

09:31.120 --> 09:37.120
capture the real thing. If you take understanding to mean that it has a model of what's going on,

09:37.120 --> 09:43.840
then these AIs almost certainly understand the relation between shadows and lights. But does it

09:43.840 --> 09:49.360
know that shadows and light are created by electromagnetic radiation bouncing off or being

09:49.360 --> 09:55.520
absorbed by three-dimensional bodies? It can't because it never got that information.

09:55.520 --> 10:01.680
You can instead give an AI a 3D model and train it to match images to that 3D model.

10:01.680 --> 10:07.040
This is basically how deep fakes work. And in this case, I'd say that the AI actually does

10:07.040 --> 10:13.680
partly understand the motion of certain body parts. The issue with chatbots is more complicated

10:13.680 --> 10:19.920
because language is much more loosely tied to reality than videos or photographs. Language

10:19.920 --> 10:24.800
is a method that humans have invented to exchange information about these models that

10:24.800 --> 10:32.400
we have in our own heads. Written language is, moreover, a reduced version of spoken language.

10:32.400 --> 10:37.920
It does capture some essence of reality in relations between words. And if you train a neural

10:37.920 --> 10:44.640
network on that, it'll learn those relations, but a lot of information will be missing. Take

10:44.640 --> 10:51.280
the sentence, what goes up must come down. That's, for reasonably common initial conditions,

10:51.280 --> 10:57.520
a statement about Newton's law of gravity. Further text analysis might tell you that by

10:57.520 --> 11:03.600
down we mean towards the ground, and that the ground is a planet called Earth, which is a sphere,

11:03.600 --> 11:10.720
and so on. From that alone, you may have no idea what any of these words mean, but you know how

11:10.720 --> 11:16.720
they are related. And indeed, if you ask chatGPT what happens when you throw a stone into the air,

11:16.800 --> 11:21.440
it'll tell you the bluntly obvious and several flawlessly correct paragraphs.

11:22.160 --> 11:28.560
But the language model can't do more than try to infer relations between words

11:28.560 --> 11:34.800
because it didn't get any other data. This is why chatGPT is ridiculously bad at anything that

11:34.800 --> 11:40.800
requires, for example, understanding spatial relationships, like latitude. I asked it whether

11:40.800 --> 11:46.080
Windsor UK is further north or south than Toronto, Canada, and they told me,

11:46.640 --> 11:53.440
Windsor is located at approximately 51.5 degrees north latitude, while Toronto is located at

11:53.440 --> 12:00.720
approximately 43.7 degrees north latitude. Therefore, Toronto is further north than Windsor.

12:00.720 --> 12:07.680
It'll quote the latitudes correctly, but draw the exactly wrong conclusion. It's a funny mistake

12:07.680 --> 12:12.960
because it'd be easy to fix by quiping it with a three-dimensional model of planet Earth,

12:12.960 --> 12:17.120
but it doesn't have such a model. It only knows the relations between words.

12:17.760 --> 12:23.360
For the same reason chatGPT has some rather elementary misunderstandings about quantum mechanics.

12:23.920 --> 12:30.560
But let me ask you first. Imagine you have two entangled particles and you separate them.

12:30.560 --> 12:36.160
One goes left and the other goes right, but like couples after a fight, they're still linked,

12:36.160 --> 12:42.720
whether they want to or not. That they are entangled means that they share a measurable property,

12:42.720 --> 12:48.480
but you don't know which particle has which share. It could be, for example, that they each

12:48.480 --> 12:55.200
either have spin plus or minus one, and the spin has to add up to zero. If you measure them,

12:55.200 --> 13:01.280
either the one going left has spin plus one and the one going right minus one or the other way

13:02.080 --> 13:08.800
and if you measure one particle, you know immediately what the spin of the other particle is.

13:08.800 --> 13:14.800
But let's say you don't measure them right away. Instead, you first perform an operation on one of

13:14.800 --> 13:20.320
the particles. This is physics, so when I say operation, I don't mean heart surgery, but

13:20.320 --> 13:26.880
something a little more sophisticated. For example, you flip its spin. Such an operation is not a

13:26.880 --> 13:32.560
measurement because it doesn't allow you to determine what the spin is. If you do this on

13:32.560 --> 13:38.720
one particle, what happens to the other particle? If you don't know the answer, that's perfectly

13:38.720 --> 13:44.640
fine because you can't answer the question from what I've told you. The correct answer is that

13:44.640 --> 13:51.040
nothing happens to the other particle. This is obvious if you know how the mathematics works

13:51.040 --> 13:58.240
because if you flip the spin, that operation only acts on one side. But it's not obvious from a

13:58.240 --> 14:03.840
verbal description of quantum mechanics, which is why it's a common confusion in the popular

14:03.840 --> 14:10.960
science press. Because of that, it's a confusion that chat GPT is likely to have too. And indeed,

14:10.960 --> 14:17.360
when I asked that question, it got it wrong. So I'd recommend you don't trust chat GPT on

14:17.360 --> 14:24.320
quantum mechanics until it speaks fluent latich. But ask it any word-related question and it

14:24.320 --> 14:30.880
shines. One of the best uses for chat GPT that I have found is English grammar or word use questions.

14:30.880 --> 14:37.120
As I was working on this video, for example, I was wondering whether Dropbox is actually a word

14:37.120 --> 14:42.240
or just the name of an app. How am I supposed to know? I've never heard anyone use the word for

14:42.240 --> 14:49.040
anything besides the app. If you type this question into your search engine of choice,

14:49.040 --> 14:55.280
the only thing you get is a gazillion hits explaining how Dropbox the app works.

14:56.080 --> 15:02.000
Ask the question to chat GPT and it'll tell you that yes, Dropbox is a word that English

15:02.000 --> 15:08.880
native speakers will understand. For the same reason, chat GPT is really good at listing pros

15:08.880 --> 15:15.600
and cons for certain arguments because those are words which stand in relation to the question.

15:15.600 --> 15:21.200
It's also good at finding technical terms and keywords from rather vague verbal descriptions.

15:21.920 --> 15:28.160
For example, I asked it, what's the name for this effect where things get shorter when you move at

15:28.160 --> 15:34.560
high speed? It explained, the name of the effect you are referring to is length contraction or

15:34.560 --> 15:39.120
Lawrence contraction. It is a consequence of the theory of special relativity.

15:39.680 --> 15:44.080
Which is perfectly correct. But don't ask it how English words are pronounced,

15:44.080 --> 15:49.520
it makes even more mistakes than I do. What does this tell us about whether we

15:49.520 --> 15:56.080
understand quantum mechanics? I've argued that understanding can't be inferred from the relation

15:56.080 --> 16:02.240
between input and output alone. The relevant question is instead whether a system has a model

16:02.240 --> 16:07.680
of what it's trying to understand, a model that it can use to explain what's going on.

16:08.320 --> 16:15.120
And I'd say this is definitely the case for physicists who use quantum mechanics. I have a

16:15.120 --> 16:20.960
model inside my head for how quantum mechanics works. It's a set of equations that I have used

16:20.960 --> 16:26.800
many times that I know how to apply and use to answer questions. And I'm sure the same is the

16:26.800 --> 16:32.480
case for other physicists. The problem with quantum mechanics is that those equations

16:32.480 --> 16:39.200
do not correspond to words we use in everyday language. Most of the problems we see with

16:39.200 --> 16:45.520
understanding quantum mechanics come from the impossibility of expressing the equations in words.

16:46.160 --> 16:51.360
At least in English. For all I know, you can do it in Chinese. Maybe that explains why the

16:51.360 --> 16:57.440
Chinese are so good with quantum technologies. It is of course possible to just convert equations

16:57.440 --> 17:03.760
into words by reading them out. But we normally don't do that. What we do in science communication

17:03.760 --> 17:11.040
is kind of a mixture with metaphors and attempts to explain some of the maths. And that conveys

17:11.040 --> 17:18.080
some aspects of how the equations work. But if you take the words too literally, they stop making

17:18.080 --> 17:24.320
sense. But equations aren't necessary for understanding. You can also gain understanding

17:24.320 --> 17:30.080
of quantum mechanics by games or apps that visualize the behavior of the equations,

17:30.080 --> 17:36.000
like those that I talked about in an earlier video. That too will allow you to build a model

17:36.000 --> 17:42.480
inside your head for how quantum mechanics works. This is why I'd also say that if we use computer

17:42.560 --> 17:49.120
simulations and visualizations in science, especially for complex problems, that doesn't mean

17:49.120 --> 17:55.360
we've given up on understanding. Visualizing the behavior of a system and probing it and seeing

17:55.360 --> 18:02.080
what it does is another way of building a model in your head. There is another reason why physicists

18:02.080 --> 18:06.800
say they don't understand quantum mechanics, which is that it's internally inconsistent.

18:06.800 --> 18:11.680
I've talked about this a few times before and it's somewhat off topic here, so I don't want to get

18:11.680 --> 18:17.280
into this again. Let me just say that there are problems with quantum mechanics that go beyond

18:17.280 --> 18:23.840
the difficulty of expressing it in words. So where will the AI boom leaders? First of all,

18:23.840 --> 18:29.440
it's rather foreseeable that before long, we'll all have a personalized AI that'll offer anything

18:29.440 --> 18:35.520
from financial advice to relationship counseling. The more you can afford to pay, the better it'll

18:35.520 --> 18:40.720
be and the free version will suggest you marry the Prince of Nigeria. Of course, people are going

18:40.720 --> 18:45.840
to complain it'll destroy the world and all, but it'll happen anyway because when has the risk

18:45.840 --> 18:50.880
of destroying the world ever stopped us from doing anything if there was money to make with it?

18:51.440 --> 18:56.400
The best and biggest AIs will be those of big companies and governments,

18:56.400 --> 19:02.960
and that's almost guaranteed to increase wealth disparities. We're also going to see YouTube

19:02.960 --> 19:10.160
flooded by human avatars and other funky AI-generated visuals because it's much faster and cheaper

19:10.160 --> 19:15.440
than getting a human to retext or go out and film that old-fashioned thing called reality.

19:16.000 --> 19:21.120
But I don't think this trend will last long because it'll be extremely difficult to make

19:21.120 --> 19:27.280
money with it. The easier it becomes to create artificial footage, the more people will look

19:27.280 --> 19:32.800
for authenticity, so that stupid German accent might eventually actually be good for something.

19:32.800 --> 19:38.240
If nothing else, it makes me difficult to simulate. Will AI eventually become conscious?

19:38.800 --> 19:44.320
Of course. There's nothing magic about the human brain, it's just a lot of connections that process

19:44.320 --> 19:50.880
a lot of information. If we can be conscious, computers can do it too, and it will happen

19:50.880 --> 19:58.320
eventually. How will we know? Like understanding, you can't probe consciousness just by observing

19:58.320 --> 20:04.400
what goes in and comes out. If you'd really want to know, you'd have to look what's going on inside

20:04.400 --> 20:10.160
and at the moment, that wouldn't help because we don't know how to identify consciousness in any

20:10.160 --> 20:16.880
case. Basically, we can't answer the question. But personally, I find this extremely interesting

20:16.880 --> 20:23.040
because we're about to create an intelligent species that'll be very different from our own.

20:23.040 --> 20:27.120
And if we're dumb enough to cause our own extinction this way, then I guess that's what

20:27.120 --> 20:33.280
we deserve. Meanwhile, enjoy the ride. At least for now, the best tool we have for understanding

20:33.280 --> 20:38.800
the world is the human brain. But if you really want to understand quantum mechanics or neural

20:38.800 --> 20:45.200
networks, then passively watching a video isn't enough. You have to actively engage with the

20:45.200 --> 20:51.600
material. Brilliant.org, who have been sponsoring this video, is a great place for that. Brilliant

20:51.600 --> 20:57.760
offers courses on a large variety of subjects in science and mathematics, and they add new content

20:57.760 --> 21:03.920
every month. The great thing about their courses is that they're all interactive with visualizations

21:03.920 --> 21:10.000
and follow-up questions. So you can check right away whether you can apply what you've learned,

21:10.000 --> 21:16.000
and that's really what understanding is all about. What I need to freshen up my knowledge or want to

21:16.000 --> 21:22.400
learn something new, first thing I do is look it up on Brilliant. To get some background on the physics

21:22.400 --> 21:28.240
in this video, check out, for example, their courses on neural networks and quantum objects,

21:28.240 --> 21:35.200
or even better, check out my own course about quantum mechanics. My course gives you an introduction

21:35.200 --> 21:41.600
to interference, superpositions and entanglement, the uncertainty principle, and Bell's theorem.

21:42.240 --> 21:47.360
You don't need to be an expert to take this course. I've worked together with Brilliant so

21:47.360 --> 21:52.560
that you can start from the very basics. If you're interested in trying Brilliant out,

21:52.560 --> 21:58.800
use our link brilliant.org slash Sabine and sign up for free trial, where you'll get to try out

21:58.800 --> 22:04.800
everything Brilliant has to offer for 30 days. The first 200 subscribers using this link will also

22:04.800 --> 22:13.600
get 20% off the annual premium subscription. Thanks for watching. See you next week.

