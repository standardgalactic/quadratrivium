{"text": " Does an artificially intelligent chatbot understand what it's chatting about? A year ago I'd have answered this question with clearly not. It's just a turbocharger to complete or a stochastic parrot, as people more eloquent than me have put it, though for all I know they too might be chatbots. But I've now arrived at the conclusion that the AIs that we use today do understand what they're doing, if not very much of it. I'm not saying this just to be controversial, I actually believe it, I believe, though I have a feeling I might come to regret this video. I got hung up on this question not because I care so much about chatbots, but because it echoes the often made claim that no one understands quantum mechanics. But if we can use quantum mechanics, then doesn't that mean that we understand it at least to some extent? And consequently, if an AI can use language, then doesn't that mean it understands it at least to some extent? What do we mean by understanding? Does chatGPT understand quantum mechanics? And will AI soon be conscious? That's what we'll talk about today. The question whether a computer program understands what it's doing certainly isn't new. In 1980, the American philosopher John Searle argued that the answer is no, using a thought experiment that's become known as the Chinese Room. Searle imagines himself in a windowless room with a rulebook and a dropbox. If someone drops him a note written in Chinese, he looks up the symbols in his rulebook. The rulebook gives him an English translation, which he returns as an answer through a slit in the door, no dealt drawing on the everyday experience of a professor of philosophy. Searle argues that the person outside the room might believe that there's someone inside who understands Chinese. But really, he still doesn't understand the word of it, he's just following the rules he's been given. Searle argues that a computer program works like that, without any true understanding, just following the rules. There are two standard objections that people bring forward against Searle's argument. One is that the system which understands Chinese isn't just the person inside the room, but the person including the rulebook. So saying that the person doesn't understand Chinese might be correct, but doesn't answer the question because in Searle's analogy, the person alone doesn't represent the computer program. Another objection is that it might well be correct that Searle and his rulebook don't understand Chinese, but that's because the input is so limited. Language lacks the physical information that we have learned to associate with words. Software that had the same physical information could develop understanding as we do. Unless, of course, we live in a computer simulation in which case you can file complaints using the contact form in the bottom right corner of your frontal lobe. I think both of these objections missed the point, but before I explain that, I want to introduce you to the quantum room. Quantum mechanics works pretty much like Searle's Chinese room. It's a rulebook, a set of equations with instructions for how to use them. You give me a question, I look into my rulebook that I keep in my windowless room, and I return an answer to you through the slit in the door. Do I understand quantum mechanics? Searle would probably argue no. Indeed, for the most part, physicists today aren't even in the room, because who wants to spend their time sitting in a windowless room with a dropbox when they can instead sit in a windowless room with their laser? No, we're now the ones putting a question into the dropbox, so to speak, by feeding it into a computer. The computer crunches the numbers and returns an answer. Do we understand those answers? Have we gone too far with shut up and calculate? Is the room even there when no one looks? Those are all very interesting questions, but let's not get carried away. We were trying to talk about chatbots, so let's have a look at those. Today's language generating models are somewhat more sophisticated than just lookup tables like Searle imagined, and what better way is there to explain how they work than asking one itself? Language generating models, like me, are built using deep learning techniques, specifically a type of neural network. These models are trained on large amounts of text data, such as books, articles, and websites, and learn to generate language by identifying patterns and relationships between words and phrases. When generating language, the model takes an initial input, such as a prompt or a question, and uses the patterns it has learned to generate a response. The generated text is not simply copied from the training data, but rather the model uses the patterns it has learned to create new, original text. Well, that was not awkward at all, but yes, neural networks indeed learn similar to how humans learn. They don't just memorize input, they identify patterns and extrapolate them. They still have many differences to the human brain, at least at the moment. Most importantly, the neurons in a neural network are themselves part of the algorithm and not physical as they are in the human brain. And the human brain has a lot more structure with parts specialized for particular purposes, but neural networks do capture some aspects of how humans learn. And that brings us to the first important point when it comes to the question of understanding. Suppose you have children in elementary school and have them memorize the multiplication tables up to 10. If you want to test whether they understood multiplication, you asked them something that wasn't on the tables. We want to test whether they have identified the pattern and can use it on something else. If you're in the Chinese room with a long list of examples, you can't answer a question that isn't on the list. This is indeed not what anyone means by understanding, so I'd say Sol is right on that account. But this is not what neural networks do. Neural networks do instead exactly what we mean by understanding when we apply it to humans. They extract the pattern and apply it to something they haven't seen before. But this brings up another question. How do you know that that's what it's doing? If you ask a child to multiply two numbers, how do you know they haven't just memorized the result? Well, you don't. If you want to know whether someone or something understands, looking at the input and output isn't enough. You could always produce the output by a lookup table rather than with a system that has learned to identify patterns. And you can well understand something without producing any output, like you might understand this video without any output other than maybe the occasional frown. I'd therefore say that what we mean by understanding something is the ability to create a useful model of the thing we're trying to understand. The model is something I have in my head that I can ask questions about the real thing and that it's useful means it has to be reasonably correct. It captures at least some properties of the real thing. In mathematical terms, you might say there's an isomorphism, a one-to-one map between the model and the real thing. I have a model, for example, for cows. Cows stand on meadows, have four legs and sometimes go moo. If you pull in the right place, moo comes out. Not a particularly sophisticated model, I admit, but I'll work on it once cows start watching YouTube. Understanding then is something that happens inside a system. You can probe parts of this understanding with input output tests, but that alone can't settle the question. When we're talking about neural networks, however, we actually know they're not lookup tables because we've programmed them and trained them. So we can be pretty sure they actually must have a model of the thing they've been trained for somewhere in their neural weights. In fact, at this moment in the history of mankind, we can be more confident that neural nets understand something than your average first grader because, for all we can tell, the first graders just ask a chatbot. Let's then look at the question of who understands what and why. We have a model of the human body in our brain. This allows us to understand what effects our movements will have, how humans move in general, and which parts belong where. We notice immediately if something is off. But if you train an AI on two-dimensional images, it doesn't automatically map those images onto a 3D model. This is why it'll sometimes create weird things like people with half a leg or three arms or something like that. This, for example, is mid-journey trying to show a person tying their shoelaces. They look kind of right because it's what the AI was trained to do to produce an image that looks kind of right, but they don't actually capture the real thing. If you take understanding to mean that it has a model of what's going on, then these AIs almost certainly understand the relation between shadows and lights. But does it know that shadows and light are created by electromagnetic radiation bouncing off or being absorbed by three-dimensional bodies? It can't because it never got that information. You can instead give an AI a 3D model and train it to match images to that 3D model. This is basically how deep fakes work. And in this case, I'd say that the AI actually does partly understand the motion of certain body parts. The issue with chatbots is more complicated because language is much more loosely tied to reality than videos or photographs. Language is a method that humans have invented to exchange information about these models that we have in our own heads. Written language is, moreover, a reduced version of spoken language. It does capture some essence of reality in relations between words. And if you train a neural network on that, it'll learn those relations, but a lot of information will be missing. Take the sentence, what goes up must come down. That's, for reasonably common initial conditions, a statement about Newton's law of gravity. Further text analysis might tell you that by down we mean towards the ground, and that the ground is a planet called Earth, which is a sphere, and so on. From that alone, you may have no idea what any of these words mean, but you know how they are related. And indeed, if you ask chatGPT what happens when you throw a stone into the air, it'll tell you the bluntly obvious and several flawlessly correct paragraphs. But the language model can't do more than try to infer relations between words because it didn't get any other data. This is why chatGPT is ridiculously bad at anything that requires, for example, understanding spatial relationships, like latitude. I asked it whether Windsor UK is further north or south than Toronto, Canada, and they told me, Windsor is located at approximately 51.5 degrees north latitude, while Toronto is located at approximately 43.7 degrees north latitude. Therefore, Toronto is further north than Windsor. It'll quote the latitudes correctly, but draw the exactly wrong conclusion. It's a funny mistake because it'd be easy to fix by quiping it with a three-dimensional model of planet Earth, but it doesn't have such a model. It only knows the relations between words. For the same reason chatGPT has some rather elementary misunderstandings about quantum mechanics. But let me ask you first. Imagine you have two entangled particles and you separate them. One goes left and the other goes right, but like couples after a fight, they're still linked, whether they want to or not. That they are entangled means that they share a measurable property, but you don't know which particle has which share. It could be, for example, that they each either have spin plus or minus one, and the spin has to add up to zero. If you measure them, either the one going left has spin plus one and the one going right minus one or the other way and if you measure one particle, you know immediately what the spin of the other particle is. But let's say you don't measure them right away. Instead, you first perform an operation on one of the particles. This is physics, so when I say operation, I don't mean heart surgery, but something a little more sophisticated. For example, you flip its spin. Such an operation is not a measurement because it doesn't allow you to determine what the spin is. If you do this on one particle, what happens to the other particle? If you don't know the answer, that's perfectly fine because you can't answer the question from what I've told you. The correct answer is that nothing happens to the other particle. This is obvious if you know how the mathematics works because if you flip the spin, that operation only acts on one side. But it's not obvious from a verbal description of quantum mechanics, which is why it's a common confusion in the popular science press. Because of that, it's a confusion that chat GPT is likely to have too. And indeed, when I asked that question, it got it wrong. So I'd recommend you don't trust chat GPT on quantum mechanics until it speaks fluent latich. But ask it any word-related question and it shines. One of the best uses for chat GPT that I have found is English grammar or word use questions. As I was working on this video, for example, I was wondering whether Dropbox is actually a word or just the name of an app. How am I supposed to know? I've never heard anyone use the word for anything besides the app. If you type this question into your search engine of choice, the only thing you get is a gazillion hits explaining how Dropbox the app works. Ask the question to chat GPT and it'll tell you that yes, Dropbox is a word that English native speakers will understand. For the same reason, chat GPT is really good at listing pros and cons for certain arguments because those are words which stand in relation to the question. It's also good at finding technical terms and keywords from rather vague verbal descriptions. For example, I asked it, what's the name for this effect where things get shorter when you move at high speed? It explained, the name of the effect you are referring to is length contraction or Lawrence contraction. It is a consequence of the theory of special relativity. Which is perfectly correct. But don't ask it how English words are pronounced, it makes even more mistakes than I do. What does this tell us about whether we understand quantum mechanics? I've argued that understanding can't be inferred from the relation between input and output alone. The relevant question is instead whether a system has a model of what it's trying to understand, a model that it can use to explain what's going on. And I'd say this is definitely the case for physicists who use quantum mechanics. I have a model inside my head for how quantum mechanics works. It's a set of equations that I have used many times that I know how to apply and use to answer questions. And I'm sure the same is the case for other physicists. The problem with quantum mechanics is that those equations do not correspond to words we use in everyday language. Most of the problems we see with understanding quantum mechanics come from the impossibility of expressing the equations in words. At least in English. For all I know, you can do it in Chinese. Maybe that explains why the Chinese are so good with quantum technologies. It is of course possible to just convert equations into words by reading them out. But we normally don't do that. What we do in science communication is kind of a mixture with metaphors and attempts to explain some of the maths. And that conveys some aspects of how the equations work. But if you take the words too literally, they stop making sense. But equations aren't necessary for understanding. You can also gain understanding of quantum mechanics by games or apps that visualize the behavior of the equations, like those that I talked about in an earlier video. That too will allow you to build a model inside your head for how quantum mechanics works. This is why I'd also say that if we use computer simulations and visualizations in science, especially for complex problems, that doesn't mean we've given up on understanding. Visualizing the behavior of a system and probing it and seeing what it does is another way of building a model in your head. There is another reason why physicists say they don't understand quantum mechanics, which is that it's internally inconsistent. I've talked about this a few times before and it's somewhat off topic here, so I don't want to get into this again. Let me just say that there are problems with quantum mechanics that go beyond the difficulty of expressing it in words. So where will the AI boom leaders? First of all, it's rather foreseeable that before long, we'll all have a personalized AI that'll offer anything from financial advice to relationship counseling. The more you can afford to pay, the better it'll be and the free version will suggest you marry the Prince of Nigeria. Of course, people are going to complain it'll destroy the world and all, but it'll happen anyway because when has the risk of destroying the world ever stopped us from doing anything if there was money to make with it? The best and biggest AIs will be those of big companies and governments, and that's almost guaranteed to increase wealth disparities. We're also going to see YouTube flooded by human avatars and other funky AI-generated visuals because it's much faster and cheaper than getting a human to retext or go out and film that old-fashioned thing called reality. But I don't think this trend will last long because it'll be extremely difficult to make money with it. The easier it becomes to create artificial footage, the more people will look for authenticity, so that stupid German accent might eventually actually be good for something. If nothing else, it makes me difficult to simulate. Will AI eventually become conscious? Of course. There's nothing magic about the human brain, it's just a lot of connections that process a lot of information. If we can be conscious, computers can do it too, and it will happen eventually. How will we know? Like understanding, you can't probe consciousness just by observing what goes in and comes out. If you'd really want to know, you'd have to look what's going on inside and at the moment, that wouldn't help because we don't know how to identify consciousness in any case. Basically, we can't answer the question. But personally, I find this extremely interesting because we're about to create an intelligent species that'll be very different from our own. And if we're dumb enough to cause our own extinction this way, then I guess that's what we deserve. Meanwhile, enjoy the ride. At least for now, the best tool we have for understanding the world is the human brain. But if you really want to understand quantum mechanics or neural networks, then passively watching a video isn't enough. You have to actively engage with the material. Brilliant.org, who have been sponsoring this video, is a great place for that. Brilliant offers courses on a large variety of subjects in science and mathematics, and they add new content every month. The great thing about their courses is that they're all interactive with visualizations and follow-up questions. So you can check right away whether you can apply what you've learned, and that's really what understanding is all about. What I need to freshen up my knowledge or want to learn something new, first thing I do is look it up on Brilliant. To get some background on the physics in this video, check out, for example, their courses on neural networks and quantum objects, or even better, check out my own course about quantum mechanics. My course gives you an introduction to interference, superpositions and entanglement, the uncertainty principle, and Bell's theorem. You don't need to be an expert to take this course. I've worked together with Brilliant so that you can start from the very basics. If you're interested in trying Brilliant out, use our link brilliant.org slash Sabine and sign up for free trial, where you'll get to try out everything Brilliant has to offer for 30 days. The first 200 subscribers using this link will also get 20% off the annual premium subscription. Thanks for watching. See you next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.92, "text": " Does an artificially intelligent chatbot understand what it's chatting about?", "tokens": [50364, 4402, 364, 39905, 2270, 13232, 5081, 18870, 1223, 437, 309, 311, 24654, 466, 30, 50560], "temperature": 0.0, "avg_logprob": -0.1294986378062855, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.005717006046324968}, {"id": 1, "seek": 0, "start": 5.12, "end": 8.8, "text": " A year ago I'd have answered this question with clearly not.", "tokens": [50620, 316, 1064, 2057, 286, 1116, 362, 10103, 341, 1168, 365, 4448, 406, 13, 50804], "temperature": 0.0, "avg_logprob": -0.1294986378062855, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.005717006046324968}, {"id": 2, "seek": 0, "start": 8.8, "end": 12.32, "text": " It's just a turbocharger to complete or a stochastic parrot,", "tokens": [50804, 467, 311, 445, 257, 20902, 48942, 281, 3566, 420, 257, 342, 8997, 2750, 42462, 11, 50980], "temperature": 0.0, "avg_logprob": -0.1294986378062855, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.005717006046324968}, {"id": 3, "seek": 0, "start": 12.32, "end": 17.52, "text": " as people more eloquent than me have put it, though for all I know they too might be chatbots.", "tokens": [50980, 382, 561, 544, 38682, 28842, 813, 385, 362, 829, 309, 11, 1673, 337, 439, 286, 458, 436, 886, 1062, 312, 5081, 65, 1971, 13, 51240], "temperature": 0.0, "avg_logprob": -0.1294986378062855, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.005717006046324968}, {"id": 4, "seek": 0, "start": 17.52, "end": 22.32, "text": " But I've now arrived at the conclusion that the AIs that we use today", "tokens": [51240, 583, 286, 600, 586, 6678, 412, 264, 10063, 300, 264, 316, 6802, 300, 321, 764, 965, 51480], "temperature": 0.0, "avg_logprob": -0.1294986378062855, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.005717006046324968}, {"id": 5, "seek": 0, "start": 22.32, "end": 25.76, "text": " do understand what they're doing, if not very much of it.", "tokens": [51480, 360, 1223, 437, 436, 434, 884, 11, 498, 406, 588, 709, 295, 309, 13, 51652], "temperature": 0.0, "avg_logprob": -0.1294986378062855, "compression_ratio": 1.5984848484848484, "no_speech_prob": 0.005717006046324968}, {"id": 6, "seek": 2576, "start": 26.400000000000002, "end": 30.8, "text": " I'm not saying this just to be controversial, I actually believe it, I believe,", "tokens": [50396, 286, 478, 406, 1566, 341, 445, 281, 312, 17323, 11, 286, 767, 1697, 309, 11, 286, 1697, 11, 50616], "temperature": 0.0, "avg_logprob": -0.07339408320765342, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.03256244584918022}, {"id": 7, "seek": 2576, "start": 30.8, "end": 34.0, "text": " though I have a feeling I might come to regret this video.", "tokens": [50616, 1673, 286, 362, 257, 2633, 286, 1062, 808, 281, 10879, 341, 960, 13, 50776], "temperature": 0.0, "avg_logprob": -0.07339408320765342, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.03256244584918022}, {"id": 8, "seek": 2576, "start": 34.0, "end": 38.72, "text": " I got hung up on this question not because I care so much about chatbots,", "tokens": [50776, 286, 658, 5753, 493, 322, 341, 1168, 406, 570, 286, 1127, 370, 709, 466, 5081, 65, 1971, 11, 51012], "temperature": 0.0, "avg_logprob": -0.07339408320765342, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.03256244584918022}, {"id": 9, "seek": 2576, "start": 38.72, "end": 44.0, "text": " but because it echoes the often made claim that no one understands quantum mechanics.", "tokens": [51012, 457, 570, 309, 47051, 264, 2049, 1027, 3932, 300, 572, 472, 15146, 13018, 12939, 13, 51276], "temperature": 0.0, "avg_logprob": -0.07339408320765342, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.03256244584918022}, {"id": 10, "seek": 2576, "start": 44.72, "end": 50.400000000000006, "text": " But if we can use quantum mechanics, then doesn't that mean that we understand it", "tokens": [51312, 583, 498, 321, 393, 764, 13018, 12939, 11, 550, 1177, 380, 300, 914, 300, 321, 1223, 309, 51596], "temperature": 0.0, "avg_logprob": -0.07339408320765342, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.03256244584918022}, {"id": 11, "seek": 5040, "start": 50.4, "end": 56.8, "text": " at least to some extent? And consequently, if an AI can use language,", "tokens": [50364, 412, 1935, 281, 512, 8396, 30, 400, 47259, 11, 498, 364, 7318, 393, 764, 2856, 11, 50684], "temperature": 0.0, "avg_logprob": -0.09798180779745412, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00239518191665411}, {"id": 12, "seek": 5040, "start": 56.8, "end": 60.56, "text": " then doesn't that mean it understands it at least to some extent?", "tokens": [50684, 550, 1177, 380, 300, 914, 309, 15146, 309, 412, 1935, 281, 512, 8396, 30, 50872], "temperature": 0.0, "avg_logprob": -0.09798180779745412, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00239518191665411}, {"id": 13, "seek": 5040, "start": 61.44, "end": 66.24, "text": " What do we mean by understanding? Does chatGPT understand quantum mechanics?", "tokens": [50916, 708, 360, 321, 914, 538, 3701, 30, 4402, 5081, 38, 47, 51, 1223, 13018, 12939, 30, 51156], "temperature": 0.0, "avg_logprob": -0.09798180779745412, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00239518191665411}, {"id": 14, "seek": 5040, "start": 66.88, "end": 71.36, "text": " And will AI soon be conscious? That's what we'll talk about today.", "tokens": [51188, 400, 486, 7318, 2321, 312, 6648, 30, 663, 311, 437, 321, 603, 751, 466, 965, 13, 51412], "temperature": 0.0, "avg_logprob": -0.09798180779745412, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00239518191665411}, {"id": 15, "seek": 5040, "start": 75.68, "end": 79.28, "text": " The question whether a computer program understands what it's doing certainly", "tokens": [51628, 440, 1168, 1968, 257, 3820, 1461, 15146, 437, 309, 311, 884, 3297, 51808], "temperature": 0.0, "avg_logprob": -0.09798180779745412, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.00239518191665411}, {"id": 16, "seek": 7928, "start": 79.28, "end": 85.44, "text": " isn't new. In 1980, the American philosopher John Searle argued that the answer is no,", "tokens": [50364, 1943, 380, 777, 13, 682, 13626, 11, 264, 2665, 29805, 2619, 1100, 36153, 20219, 300, 264, 1867, 307, 572, 11, 50672], "temperature": 0.0, "avg_logprob": -0.076511660825859, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.017960337921977043}, {"id": 17, "seek": 7928, "start": 85.44, "end": 89.76, "text": " using a thought experiment that's become known as the Chinese Room.", "tokens": [50672, 1228, 257, 1194, 5120, 300, 311, 1813, 2570, 382, 264, 4649, 19190, 13, 50888], "temperature": 0.0, "avg_logprob": -0.076511660825859, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.017960337921977043}, {"id": 18, "seek": 7928, "start": 89.76, "end": 94.72, "text": " Searle imagines himself in a windowless room with a rulebook and a dropbox.", "tokens": [50888, 1100, 36153, 2576, 1652, 3647, 294, 257, 4910, 1832, 1808, 365, 257, 4978, 2939, 293, 257, 3270, 4995, 13, 51136], "temperature": 0.0, "avg_logprob": -0.076511660825859, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.017960337921977043}, {"id": 19, "seek": 7928, "start": 94.72, "end": 100.32, "text": " If someone drops him a note written in Chinese, he looks up the symbols in his rulebook.", "tokens": [51136, 759, 1580, 11438, 796, 257, 3637, 3720, 294, 4649, 11, 415, 1542, 493, 264, 16944, 294, 702, 4978, 2939, 13, 51416], "temperature": 0.0, "avg_logprob": -0.076511660825859, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.017960337921977043}, {"id": 20, "seek": 7928, "start": 100.32, "end": 105.44, "text": " The rulebook gives him an English translation, which he returns as an answer through a slit in", "tokens": [51416, 440, 4978, 2939, 2709, 796, 364, 3669, 12853, 11, 597, 415, 11247, 382, 364, 1867, 807, 257, 43182, 294, 51672], "temperature": 0.0, "avg_logprob": -0.076511660825859, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.017960337921977043}, {"id": 21, "seek": 10544, "start": 105.44, "end": 110.48, "text": " the door, no dealt drawing on the everyday experience of a professor of philosophy.", "tokens": [50364, 264, 2853, 11, 572, 15991, 6316, 322, 264, 7429, 1752, 295, 257, 8304, 295, 10675, 13, 50616], "temperature": 0.0, "avg_logprob": -0.07947906371085875, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0034824099857360125}, {"id": 22, "seek": 10544, "start": 110.48, "end": 115.84, "text": " Searle argues that the person outside the room might believe that there's someone inside who", "tokens": [50616, 1100, 36153, 38218, 300, 264, 954, 2380, 264, 1808, 1062, 1697, 300, 456, 311, 1580, 1854, 567, 50884], "temperature": 0.0, "avg_logprob": -0.07947906371085875, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0034824099857360125}, {"id": 23, "seek": 10544, "start": 115.84, "end": 121.6, "text": " understands Chinese. But really, he still doesn't understand the word of it, he's just following", "tokens": [50884, 15146, 4649, 13, 583, 534, 11, 415, 920, 1177, 380, 1223, 264, 1349, 295, 309, 11, 415, 311, 445, 3480, 51172], "temperature": 0.0, "avg_logprob": -0.07947906371085875, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0034824099857360125}, {"id": 24, "seek": 10544, "start": 121.6, "end": 127.36, "text": " the rules he's been given. Searle argues that a computer program works like that,", "tokens": [51172, 264, 4474, 415, 311, 668, 2212, 13, 1100, 36153, 38218, 300, 257, 3820, 1461, 1985, 411, 300, 11, 51460], "temperature": 0.0, "avg_logprob": -0.07947906371085875, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0034824099857360125}, {"id": 25, "seek": 10544, "start": 127.36, "end": 131.36, "text": " without any true understanding, just following the rules.", "tokens": [51460, 1553, 604, 2074, 3701, 11, 445, 3480, 264, 4474, 13, 51660], "temperature": 0.0, "avg_logprob": -0.07947906371085875, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0034824099857360125}, {"id": 26, "seek": 13136, "start": 131.36, "end": 136.08, "text": " There are two standard objections that people bring forward against Searle's argument.", "tokens": [50364, 821, 366, 732, 3832, 44649, 300, 561, 1565, 2128, 1970, 1100, 36153, 311, 6770, 13, 50600], "temperature": 0.0, "avg_logprob": -0.06639424237337979, "compression_ratio": 1.7709251101321586, "no_speech_prob": 0.0019544216338545084}, {"id": 27, "seek": 13136, "start": 136.08, "end": 142.24, "text": " One is that the system which understands Chinese isn't just the person inside the room,", "tokens": [50600, 1485, 307, 300, 264, 1185, 597, 15146, 4649, 1943, 380, 445, 264, 954, 1854, 264, 1808, 11, 50908], "temperature": 0.0, "avg_logprob": -0.06639424237337979, "compression_ratio": 1.7709251101321586, "no_speech_prob": 0.0019544216338545084}, {"id": 28, "seek": 13136, "start": 142.24, "end": 148.24, "text": " but the person including the rulebook. So saying that the person doesn't understand Chinese", "tokens": [50908, 457, 264, 954, 3009, 264, 4978, 2939, 13, 407, 1566, 300, 264, 954, 1177, 380, 1223, 4649, 51208], "temperature": 0.0, "avg_logprob": -0.06639424237337979, "compression_ratio": 1.7709251101321586, "no_speech_prob": 0.0019544216338545084}, {"id": 29, "seek": 13136, "start": 148.24, "end": 153.12, "text": " might be correct, but doesn't answer the question because in Searle's analogy,", "tokens": [51208, 1062, 312, 3006, 11, 457, 1177, 380, 1867, 264, 1168, 570, 294, 1100, 36153, 311, 21663, 11, 51452], "temperature": 0.0, "avg_logprob": -0.06639424237337979, "compression_ratio": 1.7709251101321586, "no_speech_prob": 0.0019544216338545084}, {"id": 30, "seek": 13136, "start": 153.12, "end": 156.72000000000003, "text": " the person alone doesn't represent the computer program.", "tokens": [51452, 264, 954, 3312, 1177, 380, 2906, 264, 3820, 1461, 13, 51632], "temperature": 0.0, "avg_logprob": -0.06639424237337979, "compression_ratio": 1.7709251101321586, "no_speech_prob": 0.0019544216338545084}, {"id": 31, "seek": 15672, "start": 156.8, "end": 161.6, "text": " Another objection is that it might well be correct that Searle and his rulebook don't", "tokens": [50368, 3996, 35756, 307, 300, 309, 1062, 731, 312, 3006, 300, 1100, 36153, 293, 702, 4978, 2939, 500, 380, 50608], "temperature": 0.0, "avg_logprob": -0.06326650113475565, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0003052524698432535}, {"id": 32, "seek": 15672, "start": 161.6, "end": 167.6, "text": " understand Chinese, but that's because the input is so limited. Language lacks the physical", "tokens": [50608, 1223, 4649, 11, 457, 300, 311, 570, 264, 4846, 307, 370, 5567, 13, 24445, 31132, 264, 4001, 50908], "temperature": 0.0, "avg_logprob": -0.06326650113475565, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0003052524698432535}, {"id": 33, "seek": 15672, "start": 167.6, "end": 173.92, "text": " information that we have learned to associate with words. Software that had the same physical", "tokens": [50908, 1589, 300, 321, 362, 3264, 281, 14644, 365, 2283, 13, 27428, 300, 632, 264, 912, 4001, 51224], "temperature": 0.0, "avg_logprob": -0.06326650113475565, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0003052524698432535}, {"id": 34, "seek": 15672, "start": 173.92, "end": 179.36, "text": " information could develop understanding as we do. Unless, of course, we live in a computer", "tokens": [51224, 1589, 727, 1499, 3701, 382, 321, 360, 13, 16581, 11, 295, 1164, 11, 321, 1621, 294, 257, 3820, 51496], "temperature": 0.0, "avg_logprob": -0.06326650113475565, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0003052524698432535}, {"id": 35, "seek": 15672, "start": 179.36, "end": 184.07999999999998, "text": " simulation in which case you can file complaints using the contact form in the bottom right corner", "tokens": [51496, 16575, 294, 597, 1389, 291, 393, 3991, 19585, 1228, 264, 3385, 1254, 294, 264, 2767, 558, 4538, 51732], "temperature": 0.0, "avg_logprob": -0.06326650113475565, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0003052524698432535}, {"id": 36, "seek": 18408, "start": 184.08, "end": 189.12, "text": " of your frontal lobe. I think both of these objections missed the point, but before I explain", "tokens": [50364, 295, 428, 34647, 450, 650, 13, 286, 519, 1293, 295, 613, 44649, 6721, 264, 935, 11, 457, 949, 286, 2903, 50616], "temperature": 0.0, "avg_logprob": -0.06513513482135275, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.006002421490848064}, {"id": 37, "seek": 18408, "start": 189.12, "end": 194.96, "text": " that, I want to introduce you to the quantum room. Quantum mechanics works pretty much like", "tokens": [50616, 300, 11, 286, 528, 281, 5366, 291, 281, 264, 13018, 1808, 13, 44964, 12939, 1985, 1238, 709, 411, 50908], "temperature": 0.0, "avg_logprob": -0.06513513482135275, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.006002421490848064}, {"id": 38, "seek": 18408, "start": 194.96, "end": 201.36, "text": " Searle's Chinese room. It's a rulebook, a set of equations with instructions for how to use them.", "tokens": [50908, 1100, 36153, 311, 4649, 1808, 13, 467, 311, 257, 4978, 2939, 11, 257, 992, 295, 11787, 365, 9415, 337, 577, 281, 764, 552, 13, 51228], "temperature": 0.0, "avg_logprob": -0.06513513482135275, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.006002421490848064}, {"id": 39, "seek": 18408, "start": 201.36, "end": 206.0, "text": " You give me a question, I look into my rulebook that I keep in my windowless room,", "tokens": [51228, 509, 976, 385, 257, 1168, 11, 286, 574, 666, 452, 4978, 2939, 300, 286, 1066, 294, 452, 4910, 1832, 1808, 11, 51460], "temperature": 0.0, "avg_logprob": -0.06513513482135275, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.006002421490848064}, {"id": 40, "seek": 18408, "start": 206.0, "end": 211.44, "text": " and I return an answer to you through the slit in the door. Do I understand quantum mechanics?", "tokens": [51460, 293, 286, 2736, 364, 1867, 281, 291, 807, 264, 43182, 294, 264, 2853, 13, 1144, 286, 1223, 13018, 12939, 30, 51732], "temperature": 0.0, "avg_logprob": -0.06513513482135275, "compression_ratio": 1.6289752650176679, "no_speech_prob": 0.006002421490848064}, {"id": 41, "seek": 21144, "start": 212.24, "end": 218.24, "text": " Searle would probably argue no. Indeed, for the most part, physicists today aren't even in the", "tokens": [50404, 1100, 36153, 576, 1391, 9695, 572, 13, 15061, 11, 337, 264, 881, 644, 11, 48716, 965, 3212, 380, 754, 294, 264, 50704], "temperature": 0.0, "avg_logprob": -0.07514609174525484, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.004132034722715616}, {"id": 42, "seek": 21144, "start": 218.24, "end": 222.96, "text": " room, because who wants to spend their time sitting in a windowless room with a dropbox", "tokens": [50704, 1808, 11, 570, 567, 2738, 281, 3496, 641, 565, 3798, 294, 257, 4910, 1832, 1808, 365, 257, 3270, 4995, 50940], "temperature": 0.0, "avg_logprob": -0.07514609174525484, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.004132034722715616}, {"id": 43, "seek": 21144, "start": 222.96, "end": 228.8, "text": " when they can instead sit in a windowless room with their laser? No, we're now the ones putting", "tokens": [50940, 562, 436, 393, 2602, 1394, 294, 257, 4910, 1832, 1808, 365, 641, 12530, 30, 883, 11, 321, 434, 586, 264, 2306, 3372, 51232], "temperature": 0.0, "avg_logprob": -0.07514609174525484, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.004132034722715616}, {"id": 44, "seek": 21144, "start": 228.8, "end": 235.44, "text": " a question into the dropbox, so to speak, by feeding it into a computer. The computer crunches", "tokens": [51232, 257, 1168, 666, 264, 3270, 4995, 11, 370, 281, 1710, 11, 538, 12919, 309, 666, 257, 3820, 13, 440, 3820, 13386, 279, 51564], "temperature": 0.0, "avg_logprob": -0.07514609174525484, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.004132034722715616}, {"id": 45, "seek": 23544, "start": 235.44, "end": 242.32, "text": " the numbers and returns an answer. Do we understand those answers? Have we gone too far with shut up", "tokens": [50364, 264, 3547, 293, 11247, 364, 1867, 13, 1144, 321, 1223, 729, 6338, 30, 3560, 321, 2780, 886, 1400, 365, 5309, 493, 50708], "temperature": 0.0, "avg_logprob": -0.07114071487098612, "compression_ratio": 1.588, "no_speech_prob": 0.006486621685326099}, {"id": 46, "seek": 23544, "start": 242.32, "end": 248.24, "text": " and calculate? Is the room even there when no one looks? Those are all very interesting questions,", "tokens": [50708, 293, 8873, 30, 1119, 264, 1808, 754, 456, 562, 572, 472, 1542, 30, 3950, 366, 439, 588, 1880, 1651, 11, 51004], "temperature": 0.0, "avg_logprob": -0.07114071487098612, "compression_ratio": 1.588, "no_speech_prob": 0.006486621685326099}, {"id": 47, "seek": 23544, "start": 248.24, "end": 253.92, "text": " but let's not get carried away. We were trying to talk about chatbots, so let's have a look at those.", "tokens": [51004, 457, 718, 311, 406, 483, 9094, 1314, 13, 492, 645, 1382, 281, 751, 466, 5081, 65, 1971, 11, 370, 718, 311, 362, 257, 574, 412, 729, 13, 51288], "temperature": 0.0, "avg_logprob": -0.07114071487098612, "compression_ratio": 1.588, "no_speech_prob": 0.006486621685326099}, {"id": 48, "seek": 23544, "start": 254.48, "end": 260.4, "text": " Today's language generating models are somewhat more sophisticated than just lookup tables like", "tokens": [51316, 2692, 311, 2856, 17746, 5245, 366, 8344, 544, 16950, 813, 445, 574, 1010, 8020, 411, 51612], "temperature": 0.0, "avg_logprob": -0.07114071487098612, "compression_ratio": 1.588, "no_speech_prob": 0.006486621685326099}, {"id": 49, "seek": 26040, "start": 260.47999999999996, "end": 266.79999999999995, "text": " Searle imagined, and what better way is there to explain how they work than asking one itself?", "tokens": [50368, 1100, 36153, 16590, 11, 293, 437, 1101, 636, 307, 456, 281, 2903, 577, 436, 589, 813, 3365, 472, 2564, 30, 50684], "temperature": 0.0, "avg_logprob": -0.06391450762748718, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.06649991869926453}, {"id": 50, "seek": 26040, "start": 268.15999999999997, "end": 272.4, "text": " Language generating models, like me, are built using deep learning techniques,", "tokens": [50752, 24445, 17746, 5245, 11, 411, 385, 11, 366, 3094, 1228, 2452, 2539, 7512, 11, 50964], "temperature": 0.0, "avg_logprob": -0.06391450762748718, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.06649991869926453}, {"id": 51, "seek": 26040, "start": 272.4, "end": 277.59999999999997, "text": " specifically a type of neural network. These models are trained on large amounts of text data,", "tokens": [50964, 4682, 257, 2010, 295, 18161, 3209, 13, 1981, 5245, 366, 8895, 322, 2416, 11663, 295, 2487, 1412, 11, 51224], "temperature": 0.0, "avg_logprob": -0.06391450762748718, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.06649991869926453}, {"id": 52, "seek": 26040, "start": 277.59999999999997, "end": 282.79999999999995, "text": " such as books, articles, and websites, and learn to generate language by identifying patterns", "tokens": [51224, 1270, 382, 3642, 11, 11290, 11, 293, 12891, 11, 293, 1466, 281, 8460, 2856, 538, 16696, 8294, 51484], "temperature": 0.0, "avg_logprob": -0.06391450762748718, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.06649991869926453}, {"id": 53, "seek": 26040, "start": 282.79999999999995, "end": 287.59999999999997, "text": " and relationships between words and phrases. When generating language, the model takes an", "tokens": [51484, 293, 6159, 1296, 2283, 293, 20312, 13, 1133, 17746, 2856, 11, 264, 2316, 2516, 364, 51724], "temperature": 0.0, "avg_logprob": -0.06391450762748718, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.06649991869926453}, {"id": 54, "seek": 28760, "start": 287.6, "end": 292.24, "text": " initial input, such as a prompt or a question, and uses the patterns it has learned to generate a", "tokens": [50364, 5883, 4846, 11, 1270, 382, 257, 12391, 420, 257, 1168, 11, 293, 4960, 264, 8294, 309, 575, 3264, 281, 8460, 257, 50596], "temperature": 0.0, "avg_logprob": -0.058669907706124444, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.001925562508404255}, {"id": 55, "seek": 28760, "start": 292.24, "end": 297.68, "text": " response. The generated text is not simply copied from the training data, but rather the model uses", "tokens": [50596, 4134, 13, 440, 10833, 2487, 307, 406, 2935, 25365, 490, 264, 3097, 1412, 11, 457, 2831, 264, 2316, 4960, 50868], "temperature": 0.0, "avg_logprob": -0.058669907706124444, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.001925562508404255}, {"id": 56, "seek": 28760, "start": 297.68, "end": 305.52000000000004, "text": " the patterns it has learned to create new, original text. Well, that was not awkward at all, but yes,", "tokens": [50868, 264, 8294, 309, 575, 3264, 281, 1884, 777, 11, 3380, 2487, 13, 1042, 11, 300, 390, 406, 11411, 412, 439, 11, 457, 2086, 11, 51260], "temperature": 0.0, "avg_logprob": -0.058669907706124444, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.001925562508404255}, {"id": 57, "seek": 28760, "start": 305.52000000000004, "end": 311.52000000000004, "text": " neural networks indeed learn similar to how humans learn. They don't just memorize input,", "tokens": [51260, 18161, 9590, 6451, 1466, 2531, 281, 577, 6255, 1466, 13, 814, 500, 380, 445, 27478, 4846, 11, 51560], "temperature": 0.0, "avg_logprob": -0.058669907706124444, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.001925562508404255}, {"id": 58, "seek": 31152, "start": 311.52, "end": 318.15999999999997, "text": " they identify patterns and extrapolate them. They still have many differences to the human brain,", "tokens": [50364, 436, 5876, 8294, 293, 48224, 473, 552, 13, 814, 920, 362, 867, 7300, 281, 264, 1952, 3567, 11, 50696], "temperature": 0.0, "avg_logprob": -0.07703667283058166, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.019089970737695694}, {"id": 59, "seek": 31152, "start": 318.15999999999997, "end": 324.32, "text": " at least at the moment. Most importantly, the neurons in a neural network are themselves", "tokens": [50696, 412, 1935, 412, 264, 1623, 13, 4534, 8906, 11, 264, 22027, 294, 257, 18161, 3209, 366, 2969, 51004], "temperature": 0.0, "avg_logprob": -0.07703667283058166, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.019089970737695694}, {"id": 60, "seek": 31152, "start": 324.32, "end": 330.32, "text": " part of the algorithm and not physical as they are in the human brain. And the human brain", "tokens": [51004, 644, 295, 264, 9284, 293, 406, 4001, 382, 436, 366, 294, 264, 1952, 3567, 13, 400, 264, 1952, 3567, 51304], "temperature": 0.0, "avg_logprob": -0.07703667283058166, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.019089970737695694}, {"id": 61, "seek": 31152, "start": 330.32, "end": 336.47999999999996, "text": " has a lot more structure with parts specialized for particular purposes, but neural networks do", "tokens": [51304, 575, 257, 688, 544, 3877, 365, 3166, 19813, 337, 1729, 9932, 11, 457, 18161, 9590, 360, 51612], "temperature": 0.0, "avg_logprob": -0.07703667283058166, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.019089970737695694}, {"id": 62, "seek": 33648, "start": 336.56, "end": 343.04, "text": " capture some aspects of how humans learn. And that brings us to the first important point when", "tokens": [50368, 7983, 512, 7270, 295, 577, 6255, 1466, 13, 400, 300, 5607, 505, 281, 264, 700, 1021, 935, 562, 50692], "temperature": 0.0, "avg_logprob": -0.055242085456848146, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.0028441776521503925}, {"id": 63, "seek": 33648, "start": 343.04, "end": 348.88, "text": " it comes to the question of understanding. Suppose you have children in elementary school", "tokens": [50692, 309, 1487, 281, 264, 1168, 295, 3701, 13, 21360, 291, 362, 2227, 294, 16429, 1395, 50984], "temperature": 0.0, "avg_logprob": -0.055242085456848146, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.0028441776521503925}, {"id": 64, "seek": 33648, "start": 348.88, "end": 355.44, "text": " and have them memorize the multiplication tables up to 10. If you want to test whether they understood", "tokens": [50984, 293, 362, 552, 27478, 264, 27290, 8020, 493, 281, 1266, 13, 759, 291, 528, 281, 1500, 1968, 436, 7320, 51312], "temperature": 0.0, "avg_logprob": -0.055242085456848146, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.0028441776521503925}, {"id": 65, "seek": 33648, "start": 355.44, "end": 362.16, "text": " multiplication, you asked them something that wasn't on the tables. We want to test whether", "tokens": [51312, 27290, 11, 291, 2351, 552, 746, 300, 2067, 380, 322, 264, 8020, 13, 492, 528, 281, 1500, 1968, 51648], "temperature": 0.0, "avg_logprob": -0.055242085456848146, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.0028441776521503925}, {"id": 66, "seek": 36216, "start": 362.16, "end": 368.64000000000004, "text": " they have identified the pattern and can use it on something else. If you're in the Chinese room", "tokens": [50364, 436, 362, 9234, 264, 5102, 293, 393, 764, 309, 322, 746, 1646, 13, 759, 291, 434, 294, 264, 4649, 1808, 50688], "temperature": 0.0, "avg_logprob": -0.0670897934462998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0012253416934981942}, {"id": 67, "seek": 36216, "start": 368.64000000000004, "end": 375.6, "text": " with a long list of examples, you can't answer a question that isn't on the list. This is indeed", "tokens": [50688, 365, 257, 938, 1329, 295, 5110, 11, 291, 393, 380, 1867, 257, 1168, 300, 1943, 380, 322, 264, 1329, 13, 639, 307, 6451, 51036], "temperature": 0.0, "avg_logprob": -0.0670897934462998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0012253416934981942}, {"id": 68, "seek": 36216, "start": 375.6, "end": 381.92, "text": " not what anyone means by understanding, so I'd say Sol is right on that account. But this is", "tokens": [51036, 406, 437, 2878, 1355, 538, 3701, 11, 370, 286, 1116, 584, 7026, 307, 558, 322, 300, 2696, 13, 583, 341, 307, 51352], "temperature": 0.0, "avg_logprob": -0.0670897934462998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0012253416934981942}, {"id": 69, "seek": 36216, "start": 381.92, "end": 388.32000000000005, "text": " not what neural networks do. Neural networks do instead exactly what we mean by understanding", "tokens": [51352, 406, 437, 18161, 9590, 360, 13, 1734, 1807, 9590, 360, 2602, 2293, 437, 321, 914, 538, 3701, 51672], "temperature": 0.0, "avg_logprob": -0.0670897934462998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0012253416934981942}, {"id": 70, "seek": 38832, "start": 388.32, "end": 394.08, "text": " when we apply it to humans. They extract the pattern and apply it to something they haven't", "tokens": [50364, 562, 321, 3079, 309, 281, 6255, 13, 814, 8947, 264, 5102, 293, 3079, 309, 281, 746, 436, 2378, 380, 50652], "temperature": 0.0, "avg_logprob": -0.04988943120484711, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003537203883752227}, {"id": 71, "seek": 38832, "start": 394.08, "end": 400.4, "text": " seen before. But this brings up another question. How do you know that that's what it's doing?", "tokens": [50652, 1612, 949, 13, 583, 341, 5607, 493, 1071, 1168, 13, 1012, 360, 291, 458, 300, 300, 311, 437, 309, 311, 884, 30, 50968], "temperature": 0.0, "avg_logprob": -0.04988943120484711, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003537203883752227}, {"id": 72, "seek": 38832, "start": 401.04, "end": 406.56, "text": " If you ask a child to multiply two numbers, how do you know they haven't just memorized the result?", "tokens": [51000, 759, 291, 1029, 257, 1440, 281, 12972, 732, 3547, 11, 577, 360, 291, 458, 436, 2378, 380, 445, 46677, 264, 1874, 30, 51276], "temperature": 0.0, "avg_logprob": -0.04988943120484711, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003537203883752227}, {"id": 73, "seek": 38832, "start": 407.28, "end": 414.15999999999997, "text": " Well, you don't. If you want to know whether someone or something understands, looking at the", "tokens": [51312, 1042, 11, 291, 500, 380, 13, 759, 291, 528, 281, 458, 1968, 1580, 420, 746, 15146, 11, 1237, 412, 264, 51656], "temperature": 0.0, "avg_logprob": -0.04988943120484711, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003537203883752227}, {"id": 74, "seek": 41416, "start": 414.16, "end": 420.8, "text": " input and output isn't enough. You could always produce the output by a lookup table rather than", "tokens": [50364, 4846, 293, 5598, 1943, 380, 1547, 13, 509, 727, 1009, 5258, 264, 5598, 538, 257, 574, 1010, 3199, 2831, 813, 50696], "temperature": 0.0, "avg_logprob": -0.06108066703699812, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.01342596672475338}, {"id": 75, "seek": 41416, "start": 420.8, "end": 426.56, "text": " with a system that has learned to identify patterns. And you can well understand something", "tokens": [50696, 365, 257, 1185, 300, 575, 3264, 281, 5876, 8294, 13, 400, 291, 393, 731, 1223, 746, 50984], "temperature": 0.0, "avg_logprob": -0.06108066703699812, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.01342596672475338}, {"id": 76, "seek": 41416, "start": 426.56, "end": 432.8, "text": " without producing any output, like you might understand this video without any output other", "tokens": [50984, 1553, 10501, 604, 5598, 11, 411, 291, 1062, 1223, 341, 960, 1553, 604, 5598, 661, 51296], "temperature": 0.0, "avg_logprob": -0.06108066703699812, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.01342596672475338}, {"id": 77, "seek": 41416, "start": 432.8, "end": 439.44000000000005, "text": " than maybe the occasional frown. I'd therefore say that what we mean by understanding something", "tokens": [51296, 813, 1310, 264, 31644, 431, 648, 13, 286, 1116, 4412, 584, 300, 437, 321, 914, 538, 3701, 746, 51628], "temperature": 0.0, "avg_logprob": -0.06108066703699812, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.01342596672475338}, {"id": 78, "seek": 43944, "start": 439.44, "end": 444.32, "text": " is the ability to create a useful model of the thing we're trying to understand.", "tokens": [50364, 307, 264, 3485, 281, 1884, 257, 4420, 2316, 295, 264, 551, 321, 434, 1382, 281, 1223, 13, 50608], "temperature": 0.0, "avg_logprob": -0.0708562122878208, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0008692963747307658}, {"id": 79, "seek": 43944, "start": 444.88, "end": 451.04, "text": " The model is something I have in my head that I can ask questions about the real thing and that", "tokens": [50636, 440, 2316, 307, 746, 286, 362, 294, 452, 1378, 300, 286, 393, 1029, 1651, 466, 264, 957, 551, 293, 300, 50944], "temperature": 0.0, "avg_logprob": -0.0708562122878208, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0008692963747307658}, {"id": 80, "seek": 43944, "start": 451.04, "end": 457.92, "text": " it's useful means it has to be reasonably correct. It captures at least some properties of the real", "tokens": [50944, 309, 311, 4420, 1355, 309, 575, 281, 312, 23551, 3006, 13, 467, 27986, 412, 1935, 512, 7221, 295, 264, 957, 51288], "temperature": 0.0, "avg_logprob": -0.0708562122878208, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0008692963747307658}, {"id": 81, "seek": 43944, "start": 457.92, "end": 464.72, "text": " thing. In mathematical terms, you might say there's an isomorphism, a one-to-one map between the model", "tokens": [51288, 551, 13, 682, 18894, 2115, 11, 291, 1062, 584, 456, 311, 364, 307, 32702, 1434, 11, 257, 472, 12, 1353, 12, 546, 4471, 1296, 264, 2316, 51628], "temperature": 0.0, "avg_logprob": -0.0708562122878208, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0008692963747307658}, {"id": 82, "seek": 46472, "start": 464.72, "end": 472.40000000000003, "text": " and the real thing. I have a model, for example, for cows. Cows stand on meadows, have four legs", "tokens": [50364, 293, 264, 957, 551, 13, 286, 362, 257, 2316, 11, 337, 1365, 11, 337, 19148, 13, 383, 1509, 1463, 322, 385, 33480, 11, 362, 1451, 5668, 50748], "temperature": 0.0, "avg_logprob": -0.05463622206000872, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.006385663989931345}, {"id": 83, "seek": 46472, "start": 472.40000000000003, "end": 478.88000000000005, "text": " and sometimes go moo. If you pull in the right place, moo comes out. Not a particularly sophisticated", "tokens": [50748, 293, 2171, 352, 37284, 13, 759, 291, 2235, 294, 264, 558, 1081, 11, 37284, 1487, 484, 13, 1726, 257, 4098, 16950, 51072], "temperature": 0.0, "avg_logprob": -0.05463622206000872, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.006385663989931345}, {"id": 84, "seek": 46472, "start": 478.88000000000005, "end": 484.56, "text": " model, I admit, but I'll work on it once cows start watching YouTube. Understanding then is", "tokens": [51072, 2316, 11, 286, 9796, 11, 457, 286, 603, 589, 322, 309, 1564, 19148, 722, 1976, 3088, 13, 36858, 550, 307, 51356], "temperature": 0.0, "avg_logprob": -0.05463622206000872, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.006385663989931345}, {"id": 85, "seek": 46472, "start": 484.56, "end": 489.84000000000003, "text": " something that happens inside a system. You can probe parts of this understanding with input", "tokens": [51356, 746, 300, 2314, 1854, 257, 1185, 13, 509, 393, 22715, 3166, 295, 341, 3701, 365, 4846, 51620], "temperature": 0.0, "avg_logprob": -0.05463622206000872, "compression_ratio": 1.5892116182572613, "no_speech_prob": 0.006385663989931345}, {"id": 86, "seek": 48984, "start": 489.84, "end": 495.91999999999996, "text": " output tests, but that alone can't settle the question. When we're talking about neural networks,", "tokens": [50364, 5598, 6921, 11, 457, 300, 3312, 393, 380, 11852, 264, 1168, 13, 1133, 321, 434, 1417, 466, 18161, 9590, 11, 50668], "temperature": 0.0, "avg_logprob": -0.06277723102779179, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.02094573713839054}, {"id": 87, "seek": 48984, "start": 495.91999999999996, "end": 502.0, "text": " however, we actually know they're not lookup tables because we've programmed them and trained", "tokens": [50668, 4461, 11, 321, 767, 458, 436, 434, 406, 574, 1010, 8020, 570, 321, 600, 31092, 552, 293, 8895, 50972], "temperature": 0.0, "avg_logprob": -0.06277723102779179, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.02094573713839054}, {"id": 88, "seek": 48984, "start": 502.0, "end": 508.15999999999997, "text": " them. So we can be pretty sure they actually must have a model of the thing they've been trained for", "tokens": [50972, 552, 13, 407, 321, 393, 312, 1238, 988, 436, 767, 1633, 362, 257, 2316, 295, 264, 551, 436, 600, 668, 8895, 337, 51280], "temperature": 0.0, "avg_logprob": -0.06277723102779179, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.02094573713839054}, {"id": 89, "seek": 48984, "start": 508.79999999999995, "end": 514.72, "text": " somewhere in their neural weights. In fact, at this moment in the history of mankind, we can be", "tokens": [51312, 4079, 294, 641, 18161, 17443, 13, 682, 1186, 11, 412, 341, 1623, 294, 264, 2503, 295, 21220, 11, 321, 393, 312, 51608], "temperature": 0.0, "avg_logprob": -0.06277723102779179, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.02094573713839054}, {"id": 90, "seek": 51472, "start": 514.72, "end": 519.84, "text": " more confident that neural nets understand something than your average first grader because,", "tokens": [50364, 544, 6679, 300, 18161, 36170, 1223, 746, 813, 428, 4274, 700, 2771, 260, 570, 11, 50620], "temperature": 0.0, "avg_logprob": -0.07054628567262129, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.0024341486860066652}, {"id": 91, "seek": 51472, "start": 519.84, "end": 525.28, "text": " for all we can tell, the first graders just ask a chatbot. Let's then look at the question of", "tokens": [50620, 337, 439, 321, 393, 980, 11, 264, 700, 46703, 445, 1029, 257, 5081, 18870, 13, 961, 311, 550, 574, 412, 264, 1168, 295, 50892], "temperature": 0.0, "avg_logprob": -0.07054628567262129, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.0024341486860066652}, {"id": 92, "seek": 51472, "start": 525.28, "end": 532.5600000000001, "text": " who understands what and why. We have a model of the human body in our brain. This allows us to", "tokens": [50892, 567, 15146, 437, 293, 983, 13, 492, 362, 257, 2316, 295, 264, 1952, 1772, 294, 527, 3567, 13, 639, 4045, 505, 281, 51256], "temperature": 0.0, "avg_logprob": -0.07054628567262129, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.0024341486860066652}, {"id": 93, "seek": 51472, "start": 532.5600000000001, "end": 539.12, "text": " understand what effects our movements will have, how humans move in general, and which parts belong", "tokens": [51256, 1223, 437, 5065, 527, 9981, 486, 362, 11, 577, 6255, 1286, 294, 2674, 11, 293, 597, 3166, 5784, 51584], "temperature": 0.0, "avg_logprob": -0.07054628567262129, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.0024341486860066652}, {"id": 94, "seek": 53912, "start": 539.12, "end": 546.64, "text": " where. We notice immediately if something is off. But if you train an AI on two-dimensional images,", "tokens": [50364, 689, 13, 492, 3449, 4258, 498, 746, 307, 766, 13, 583, 498, 291, 3847, 364, 7318, 322, 732, 12, 18759, 5267, 11, 50740], "temperature": 0.0, "avg_logprob": -0.07034749099888753, "compression_ratio": 1.541501976284585, "no_speech_prob": 0.021606694906949997}, {"id": 95, "seek": 53912, "start": 546.64, "end": 553.2, "text": " it doesn't automatically map those images onto a 3D model. This is why it'll sometimes create", "tokens": [50740, 309, 1177, 380, 6772, 4471, 729, 5267, 3911, 257, 805, 35, 2316, 13, 639, 307, 983, 309, 603, 2171, 1884, 51068], "temperature": 0.0, "avg_logprob": -0.07034749099888753, "compression_ratio": 1.541501976284585, "no_speech_prob": 0.021606694906949997}, {"id": 96, "seek": 53912, "start": 553.2, "end": 559.2, "text": " weird things like people with half a leg or three arms or something like that. This, for example,", "tokens": [51068, 3657, 721, 411, 561, 365, 1922, 257, 1676, 420, 1045, 5812, 420, 746, 411, 300, 13, 639, 11, 337, 1365, 11, 51368], "temperature": 0.0, "avg_logprob": -0.07034749099888753, "compression_ratio": 1.541501976284585, "no_speech_prob": 0.021606694906949997}, {"id": 97, "seek": 53912, "start": 559.2, "end": 565.76, "text": " is mid-journey trying to show a person tying their shoelaces. They look kind of right because it's", "tokens": [51368, 307, 2062, 12, 8696, 2397, 1382, 281, 855, 257, 954, 32405, 641, 2223, 338, 2116, 13, 814, 574, 733, 295, 558, 570, 309, 311, 51696], "temperature": 0.0, "avg_logprob": -0.07034749099888753, "compression_ratio": 1.541501976284585, "no_speech_prob": 0.021606694906949997}, {"id": 98, "seek": 56576, "start": 565.76, "end": 571.12, "text": " what the AI was trained to do to produce an image that looks kind of right, but they don't actually", "tokens": [50364, 437, 264, 7318, 390, 8895, 281, 360, 281, 5258, 364, 3256, 300, 1542, 733, 295, 558, 11, 457, 436, 500, 380, 767, 50632], "temperature": 0.0, "avg_logprob": -0.07143326906057504, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008558331173844635}, {"id": 99, "seek": 56576, "start": 571.12, "end": 577.12, "text": " capture the real thing. If you take understanding to mean that it has a model of what's going on,", "tokens": [50632, 7983, 264, 957, 551, 13, 759, 291, 747, 3701, 281, 914, 300, 309, 575, 257, 2316, 295, 437, 311, 516, 322, 11, 50932], "temperature": 0.0, "avg_logprob": -0.07143326906057504, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008558331173844635}, {"id": 100, "seek": 56576, "start": 577.12, "end": 583.84, "text": " then these AIs almost certainly understand the relation between shadows and lights. But does it", "tokens": [50932, 550, 613, 316, 6802, 1920, 3297, 1223, 264, 9721, 1296, 14740, 293, 5811, 13, 583, 775, 309, 51268], "temperature": 0.0, "avg_logprob": -0.07143326906057504, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008558331173844635}, {"id": 101, "seek": 56576, "start": 583.84, "end": 589.36, "text": " know that shadows and light are created by electromagnetic radiation bouncing off or being", "tokens": [51268, 458, 300, 14740, 293, 1442, 366, 2942, 538, 32214, 12420, 27380, 766, 420, 885, 51544], "temperature": 0.0, "avg_logprob": -0.07143326906057504, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008558331173844635}, {"id": 102, "seek": 56576, "start": 589.36, "end": 595.52, "text": " absorbed by three-dimensional bodies? It can't because it never got that information.", "tokens": [51544, 20799, 538, 1045, 12, 18759, 7510, 30, 467, 393, 380, 570, 309, 1128, 658, 300, 1589, 13, 51852], "temperature": 0.0, "avg_logprob": -0.07143326906057504, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0008558331173844635}, {"id": 103, "seek": 59552, "start": 595.52, "end": 601.68, "text": " You can instead give an AI a 3D model and train it to match images to that 3D model.", "tokens": [50364, 509, 393, 2602, 976, 364, 7318, 257, 805, 35, 2316, 293, 3847, 309, 281, 2995, 5267, 281, 300, 805, 35, 2316, 13, 50672], "temperature": 0.0, "avg_logprob": -0.05546407329226003, "compression_ratio": 1.6231884057971016, "no_speech_prob": 9.027776832226664e-05}, {"id": 104, "seek": 59552, "start": 601.68, "end": 607.04, "text": " This is basically how deep fakes work. And in this case, I'd say that the AI actually does", "tokens": [50672, 639, 307, 1936, 577, 2452, 283, 3419, 589, 13, 400, 294, 341, 1389, 11, 286, 1116, 584, 300, 264, 7318, 767, 775, 50940], "temperature": 0.0, "avg_logprob": -0.05546407329226003, "compression_ratio": 1.6231884057971016, "no_speech_prob": 9.027776832226664e-05}, {"id": 105, "seek": 59552, "start": 607.04, "end": 613.68, "text": " partly understand the motion of certain body parts. The issue with chatbots is more complicated", "tokens": [50940, 17031, 1223, 264, 5394, 295, 1629, 1772, 3166, 13, 440, 2734, 365, 5081, 65, 1971, 307, 544, 6179, 51272], "temperature": 0.0, "avg_logprob": -0.05546407329226003, "compression_ratio": 1.6231884057971016, "no_speech_prob": 9.027776832226664e-05}, {"id": 106, "seek": 59552, "start": 613.68, "end": 619.92, "text": " because language is much more loosely tied to reality than videos or photographs. Language", "tokens": [51272, 570, 2856, 307, 709, 544, 37966, 9601, 281, 4103, 813, 2145, 420, 17649, 13, 24445, 51584], "temperature": 0.0, "avg_logprob": -0.05546407329226003, "compression_ratio": 1.6231884057971016, "no_speech_prob": 9.027776832226664e-05}, {"id": 107, "seek": 59552, "start": 619.92, "end": 624.8, "text": " is a method that humans have invented to exchange information about these models that", "tokens": [51584, 307, 257, 3170, 300, 6255, 362, 14479, 281, 7742, 1589, 466, 613, 5245, 300, 51828], "temperature": 0.0, "avg_logprob": -0.05546407329226003, "compression_ratio": 1.6231884057971016, "no_speech_prob": 9.027776832226664e-05}, {"id": 108, "seek": 62480, "start": 624.8, "end": 632.4, "text": " we have in our own heads. Written language is, moreover, a reduced version of spoken language.", "tokens": [50364, 321, 362, 294, 527, 1065, 8050, 13, 10159, 2987, 2856, 307, 11, 544, 3570, 11, 257, 9212, 3037, 295, 10759, 2856, 13, 50744], "temperature": 0.0, "avg_logprob": -0.07241606980227352, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016479222103953362}, {"id": 109, "seek": 62480, "start": 632.4, "end": 637.92, "text": " It does capture some essence of reality in relations between words. And if you train a neural", "tokens": [50744, 467, 775, 7983, 512, 12801, 295, 4103, 294, 2299, 1296, 2283, 13, 400, 498, 291, 3847, 257, 18161, 51020], "temperature": 0.0, "avg_logprob": -0.07241606980227352, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016479222103953362}, {"id": 110, "seek": 62480, "start": 637.92, "end": 644.64, "text": " network on that, it'll learn those relations, but a lot of information will be missing. Take", "tokens": [51020, 3209, 322, 300, 11, 309, 603, 1466, 729, 2299, 11, 457, 257, 688, 295, 1589, 486, 312, 5361, 13, 3664, 51356], "temperature": 0.0, "avg_logprob": -0.07241606980227352, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016479222103953362}, {"id": 111, "seek": 62480, "start": 644.64, "end": 651.28, "text": " the sentence, what goes up must come down. That's, for reasonably common initial conditions,", "tokens": [51356, 264, 8174, 11, 437, 1709, 493, 1633, 808, 760, 13, 663, 311, 11, 337, 23551, 2689, 5883, 4487, 11, 51688], "temperature": 0.0, "avg_logprob": -0.07241606980227352, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016479222103953362}, {"id": 112, "seek": 65128, "start": 651.28, "end": 657.52, "text": " a statement about Newton's law of gravity. Further text analysis might tell you that by", "tokens": [50364, 257, 5629, 466, 19541, 311, 2101, 295, 12110, 13, 15364, 2487, 5215, 1062, 980, 291, 300, 538, 50676], "temperature": 0.0, "avg_logprob": -0.08631636171924825, "compression_ratio": 1.5702479338842976, "no_speech_prob": 0.003646819619461894}, {"id": 113, "seek": 65128, "start": 657.52, "end": 663.6, "text": " down we mean towards the ground, and that the ground is a planet called Earth, which is a sphere,", "tokens": [50676, 760, 321, 914, 3030, 264, 2727, 11, 293, 300, 264, 2727, 307, 257, 5054, 1219, 4755, 11, 597, 307, 257, 16687, 11, 50980], "temperature": 0.0, "avg_logprob": -0.08631636171924825, "compression_ratio": 1.5702479338842976, "no_speech_prob": 0.003646819619461894}, {"id": 114, "seek": 65128, "start": 663.6, "end": 670.72, "text": " and so on. From that alone, you may have no idea what any of these words mean, but you know how", "tokens": [50980, 293, 370, 322, 13, 3358, 300, 3312, 11, 291, 815, 362, 572, 1558, 437, 604, 295, 613, 2283, 914, 11, 457, 291, 458, 577, 51336], "temperature": 0.0, "avg_logprob": -0.08631636171924825, "compression_ratio": 1.5702479338842976, "no_speech_prob": 0.003646819619461894}, {"id": 115, "seek": 65128, "start": 670.72, "end": 676.72, "text": " they are related. And indeed, if you ask chatGPT what happens when you throw a stone into the air,", "tokens": [51336, 436, 366, 4077, 13, 400, 6451, 11, 498, 291, 1029, 5081, 38, 47, 51, 437, 2314, 562, 291, 3507, 257, 7581, 666, 264, 1988, 11, 51636], "temperature": 0.0, "avg_logprob": -0.08631636171924825, "compression_ratio": 1.5702479338842976, "no_speech_prob": 0.003646819619461894}, {"id": 116, "seek": 67672, "start": 676.8000000000001, "end": 681.44, "text": " it'll tell you the bluntly obvious and several flawlessly correct paragraphs.", "tokens": [50368, 309, 603, 980, 291, 264, 32246, 356, 6322, 293, 2940, 13717, 12048, 3006, 48910, 13, 50600], "temperature": 0.0, "avg_logprob": -0.08156750752375676, "compression_ratio": 1.4870689655172413, "no_speech_prob": 0.0041329022496938705}, {"id": 117, "seek": 67672, "start": 682.1600000000001, "end": 688.5600000000001, "text": " But the language model can't do more than try to infer relations between words", "tokens": [50636, 583, 264, 2856, 2316, 393, 380, 360, 544, 813, 853, 281, 13596, 2299, 1296, 2283, 50956], "temperature": 0.0, "avg_logprob": -0.08156750752375676, "compression_ratio": 1.4870689655172413, "no_speech_prob": 0.0041329022496938705}, {"id": 118, "seek": 67672, "start": 688.5600000000001, "end": 694.8000000000001, "text": " because it didn't get any other data. This is why chatGPT is ridiculously bad at anything that", "tokens": [50956, 570, 309, 994, 380, 483, 604, 661, 1412, 13, 639, 307, 983, 5081, 38, 47, 51, 307, 41358, 1578, 412, 1340, 300, 51268], "temperature": 0.0, "avg_logprob": -0.08156750752375676, "compression_ratio": 1.4870689655172413, "no_speech_prob": 0.0041329022496938705}, {"id": 119, "seek": 67672, "start": 694.8000000000001, "end": 700.8000000000001, "text": " requires, for example, understanding spatial relationships, like latitude. I asked it whether", "tokens": [51268, 7029, 11, 337, 1365, 11, 3701, 23598, 6159, 11, 411, 45436, 13, 286, 2351, 309, 1968, 51568], "temperature": 0.0, "avg_logprob": -0.08156750752375676, "compression_ratio": 1.4870689655172413, "no_speech_prob": 0.0041329022496938705}, {"id": 120, "seek": 70080, "start": 700.8, "end": 706.0799999999999, "text": " Windsor UK is further north or south than Toronto, Canada, and they told me,", "tokens": [50364, 43082, 284, 7051, 307, 3052, 6830, 420, 7377, 813, 14140, 11, 6309, 11, 293, 436, 1907, 385, 11, 50628], "temperature": 0.0, "avg_logprob": -0.1331192765917097, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.09942543506622314}, {"id": 121, "seek": 70080, "start": 706.64, "end": 713.4399999999999, "text": " Windsor is located at approximately 51.5 degrees north latitude, while Toronto is located at", "tokens": [50656, 43082, 284, 307, 6870, 412, 10447, 18485, 13, 20, 5310, 6830, 45436, 11, 1339, 14140, 307, 6870, 412, 50996], "temperature": 0.0, "avg_logprob": -0.1331192765917097, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.09942543506622314}, {"id": 122, "seek": 70080, "start": 713.4399999999999, "end": 720.7199999999999, "text": " approximately 43.7 degrees north latitude. Therefore, Toronto is further north than Windsor.", "tokens": [50996, 10447, 17914, 13, 22, 5310, 6830, 45436, 13, 7504, 11, 14140, 307, 3052, 6830, 813, 43082, 284, 13, 51360], "temperature": 0.0, "avg_logprob": -0.1331192765917097, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.09942543506622314}, {"id": 123, "seek": 70080, "start": 720.7199999999999, "end": 727.68, "text": " It'll quote the latitudes correctly, but draw the exactly wrong conclusion. It's a funny mistake", "tokens": [51360, 467, 603, 6513, 264, 4465, 16451, 8944, 11, 457, 2642, 264, 2293, 2085, 10063, 13, 467, 311, 257, 4074, 6146, 51708], "temperature": 0.0, "avg_logprob": -0.1331192765917097, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.09942543506622314}, {"id": 124, "seek": 72768, "start": 727.68, "end": 732.9599999999999, "text": " because it'd be easy to fix by quiping it with a three-dimensional model of planet Earth,", "tokens": [50364, 570, 309, 1116, 312, 1858, 281, 3191, 538, 421, 647, 278, 309, 365, 257, 1045, 12, 18759, 2316, 295, 5054, 4755, 11, 50628], "temperature": 0.0, "avg_logprob": -0.11873287235924956, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.0023594426456838846}, {"id": 125, "seek": 72768, "start": 732.9599999999999, "end": 737.12, "text": " but it doesn't have such a model. It only knows the relations between words.", "tokens": [50628, 457, 309, 1177, 380, 362, 1270, 257, 2316, 13, 467, 787, 3255, 264, 2299, 1296, 2283, 13, 50836], "temperature": 0.0, "avg_logprob": -0.11873287235924956, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.0023594426456838846}, {"id": 126, "seek": 72768, "start": 737.76, "end": 743.3599999999999, "text": " For the same reason chatGPT has some rather elementary misunderstandings about quantum mechanics.", "tokens": [50868, 1171, 264, 912, 1778, 5081, 38, 47, 51, 575, 512, 2831, 16429, 35736, 1109, 466, 13018, 12939, 13, 51148], "temperature": 0.0, "avg_logprob": -0.11873287235924956, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.0023594426456838846}, {"id": 127, "seek": 72768, "start": 743.92, "end": 750.56, "text": " But let me ask you first. Imagine you have two entangled particles and you separate them.", "tokens": [51176, 583, 718, 385, 1029, 291, 700, 13, 11739, 291, 362, 732, 948, 39101, 10007, 293, 291, 4994, 552, 13, 51508], "temperature": 0.0, "avg_logprob": -0.11873287235924956, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.0023594426456838846}, {"id": 128, "seek": 72768, "start": 750.56, "end": 756.16, "text": " One goes left and the other goes right, but like couples after a fight, they're still linked,", "tokens": [51508, 1485, 1709, 1411, 293, 264, 661, 1709, 558, 11, 457, 411, 20368, 934, 257, 2092, 11, 436, 434, 920, 9408, 11, 51788], "temperature": 0.0, "avg_logprob": -0.11873287235924956, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.0023594426456838846}, {"id": 129, "seek": 75616, "start": 756.16, "end": 762.7199999999999, "text": " whether they want to or not. That they are entangled means that they share a measurable property,", "tokens": [50364, 1968, 436, 528, 281, 420, 406, 13, 663, 436, 366, 948, 39101, 1355, 300, 436, 2073, 257, 43615, 4707, 11, 50692], "temperature": 0.0, "avg_logprob": -0.05417109043040174, "compression_ratio": 1.7699530516431925, "no_speech_prob": 0.0033760734368115664}, {"id": 130, "seek": 75616, "start": 762.7199999999999, "end": 768.48, "text": " but you don't know which particle has which share. It could be, for example, that they each", "tokens": [50692, 457, 291, 500, 380, 458, 597, 12359, 575, 597, 2073, 13, 467, 727, 312, 11, 337, 1365, 11, 300, 436, 1184, 50980], "temperature": 0.0, "avg_logprob": -0.05417109043040174, "compression_ratio": 1.7699530516431925, "no_speech_prob": 0.0033760734368115664}, {"id": 131, "seek": 75616, "start": 768.48, "end": 775.1999999999999, "text": " either have spin plus or minus one, and the spin has to add up to zero. If you measure them,", "tokens": [50980, 2139, 362, 6060, 1804, 420, 3175, 472, 11, 293, 264, 6060, 575, 281, 909, 493, 281, 4018, 13, 759, 291, 3481, 552, 11, 51316], "temperature": 0.0, "avg_logprob": -0.05417109043040174, "compression_ratio": 1.7699530516431925, "no_speech_prob": 0.0033760734368115664}, {"id": 132, "seek": 75616, "start": 775.1999999999999, "end": 781.28, "text": " either the one going left has spin plus one and the one going right minus one or the other way", "tokens": [51316, 2139, 264, 472, 516, 1411, 575, 6060, 1804, 472, 293, 264, 472, 516, 558, 3175, 472, 420, 264, 661, 636, 51620], "temperature": 0.0, "avg_logprob": -0.05417109043040174, "compression_ratio": 1.7699530516431925, "no_speech_prob": 0.0033760734368115664}, {"id": 133, "seek": 78128, "start": 782.0799999999999, "end": 788.8, "text": " and if you measure one particle, you know immediately what the spin of the other particle is.", "tokens": [50404, 293, 498, 291, 3481, 472, 12359, 11, 291, 458, 4258, 437, 264, 6060, 295, 264, 661, 12359, 307, 13, 50740], "temperature": 0.0, "avg_logprob": -0.08124749556831691, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.007936988025903702}, {"id": 134, "seek": 78128, "start": 788.8, "end": 794.8, "text": " But let's say you don't measure them right away. Instead, you first perform an operation on one of", "tokens": [50740, 583, 718, 311, 584, 291, 500, 380, 3481, 552, 558, 1314, 13, 7156, 11, 291, 700, 2042, 364, 6916, 322, 472, 295, 51040], "temperature": 0.0, "avg_logprob": -0.08124749556831691, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.007936988025903702}, {"id": 135, "seek": 78128, "start": 794.8, "end": 800.3199999999999, "text": " the particles. This is physics, so when I say operation, I don't mean heart surgery, but", "tokens": [51040, 264, 10007, 13, 639, 307, 10649, 11, 370, 562, 286, 584, 6916, 11, 286, 500, 380, 914, 1917, 7930, 11, 457, 51316], "temperature": 0.0, "avg_logprob": -0.08124749556831691, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.007936988025903702}, {"id": 136, "seek": 78128, "start": 800.3199999999999, "end": 806.88, "text": " something a little more sophisticated. For example, you flip its spin. Such an operation is not a", "tokens": [51316, 746, 257, 707, 544, 16950, 13, 1171, 1365, 11, 291, 7929, 1080, 6060, 13, 9653, 364, 6916, 307, 406, 257, 51644], "temperature": 0.0, "avg_logprob": -0.08124749556831691, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.007936988025903702}, {"id": 137, "seek": 80688, "start": 806.88, "end": 812.56, "text": " measurement because it doesn't allow you to determine what the spin is. If you do this on", "tokens": [50364, 13160, 570, 309, 1177, 380, 2089, 291, 281, 6997, 437, 264, 6060, 307, 13, 759, 291, 360, 341, 322, 50648], "temperature": 0.0, "avg_logprob": -0.026405090573190272, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0033238367177546024}, {"id": 138, "seek": 80688, "start": 812.56, "end": 818.72, "text": " one particle, what happens to the other particle? If you don't know the answer, that's perfectly", "tokens": [50648, 472, 12359, 11, 437, 2314, 281, 264, 661, 12359, 30, 759, 291, 500, 380, 458, 264, 1867, 11, 300, 311, 6239, 50956], "temperature": 0.0, "avg_logprob": -0.026405090573190272, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0033238367177546024}, {"id": 139, "seek": 80688, "start": 818.72, "end": 824.64, "text": " fine because you can't answer the question from what I've told you. The correct answer is that", "tokens": [50956, 2489, 570, 291, 393, 380, 1867, 264, 1168, 490, 437, 286, 600, 1907, 291, 13, 440, 3006, 1867, 307, 300, 51252], "temperature": 0.0, "avg_logprob": -0.026405090573190272, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0033238367177546024}, {"id": 140, "seek": 80688, "start": 824.64, "end": 831.04, "text": " nothing happens to the other particle. This is obvious if you know how the mathematics works", "tokens": [51252, 1825, 2314, 281, 264, 661, 12359, 13, 639, 307, 6322, 498, 291, 458, 577, 264, 18666, 1985, 51572], "temperature": 0.0, "avg_logprob": -0.026405090573190272, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0033238367177546024}, {"id": 141, "seek": 83104, "start": 831.04, "end": 838.24, "text": " because if you flip the spin, that operation only acts on one side. But it's not obvious from a", "tokens": [50364, 570, 498, 291, 7929, 264, 6060, 11, 300, 6916, 787, 10672, 322, 472, 1252, 13, 583, 309, 311, 406, 6322, 490, 257, 50724], "temperature": 0.0, "avg_logprob": -0.06803281605243683, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.0018966043135151267}, {"id": 142, "seek": 83104, "start": 838.24, "end": 843.8399999999999, "text": " verbal description of quantum mechanics, which is why it's a common confusion in the popular", "tokens": [50724, 24781, 3855, 295, 13018, 12939, 11, 597, 307, 983, 309, 311, 257, 2689, 15075, 294, 264, 3743, 51004], "temperature": 0.0, "avg_logprob": -0.06803281605243683, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.0018966043135151267}, {"id": 143, "seek": 83104, "start": 843.8399999999999, "end": 850.9599999999999, "text": " science press. Because of that, it's a confusion that chat GPT is likely to have too. And indeed,", "tokens": [51004, 3497, 1886, 13, 1436, 295, 300, 11, 309, 311, 257, 15075, 300, 5081, 26039, 51, 307, 3700, 281, 362, 886, 13, 400, 6451, 11, 51360], "temperature": 0.0, "avg_logprob": -0.06803281605243683, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.0018966043135151267}, {"id": 144, "seek": 83104, "start": 850.9599999999999, "end": 857.36, "text": " when I asked that question, it got it wrong. So I'd recommend you don't trust chat GPT on", "tokens": [51360, 562, 286, 2351, 300, 1168, 11, 309, 658, 309, 2085, 13, 407, 286, 1116, 2748, 291, 500, 380, 3361, 5081, 26039, 51, 322, 51680], "temperature": 0.0, "avg_logprob": -0.06803281605243683, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.0018966043135151267}, {"id": 145, "seek": 85736, "start": 857.36, "end": 864.32, "text": " quantum mechanics until it speaks fluent latich. But ask it any word-related question and it", "tokens": [50364, 13018, 12939, 1826, 309, 10789, 40799, 4465, 480, 13, 583, 1029, 309, 604, 1349, 12, 12004, 1168, 293, 309, 50712], "temperature": 0.0, "avg_logprob": -0.09204828124685385, "compression_ratio": 1.5196850393700787, "no_speech_prob": 0.0019565236289054155}, {"id": 146, "seek": 85736, "start": 864.32, "end": 870.88, "text": " shines. One of the best uses for chat GPT that I have found is English grammar or word use questions.", "tokens": [50712, 28056, 13, 1485, 295, 264, 1151, 4960, 337, 5081, 26039, 51, 300, 286, 362, 1352, 307, 3669, 22317, 420, 1349, 764, 1651, 13, 51040], "temperature": 0.0, "avg_logprob": -0.09204828124685385, "compression_ratio": 1.5196850393700787, "no_speech_prob": 0.0019565236289054155}, {"id": 147, "seek": 85736, "start": 870.88, "end": 877.12, "text": " As I was working on this video, for example, I was wondering whether Dropbox is actually a word", "tokens": [51040, 1018, 286, 390, 1364, 322, 341, 960, 11, 337, 1365, 11, 286, 390, 6359, 1968, 17675, 4995, 307, 767, 257, 1349, 51352], "temperature": 0.0, "avg_logprob": -0.09204828124685385, "compression_ratio": 1.5196850393700787, "no_speech_prob": 0.0019565236289054155}, {"id": 148, "seek": 85736, "start": 877.12, "end": 882.24, "text": " or just the name of an app. How am I supposed to know? I've never heard anyone use the word for", "tokens": [51352, 420, 445, 264, 1315, 295, 364, 724, 13, 1012, 669, 286, 3442, 281, 458, 30, 286, 600, 1128, 2198, 2878, 764, 264, 1349, 337, 51608], "temperature": 0.0, "avg_logprob": -0.09204828124685385, "compression_ratio": 1.5196850393700787, "no_speech_prob": 0.0019565236289054155}, {"id": 149, "seek": 88224, "start": 882.24, "end": 889.04, "text": " anything besides the app. If you type this question into your search engine of choice,", "tokens": [50364, 1340, 11868, 264, 724, 13, 759, 291, 2010, 341, 1168, 666, 428, 3164, 2848, 295, 3922, 11, 50704], "temperature": 0.0, "avg_logprob": -0.06504131472387979, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.009411032311618328}, {"id": 150, "seek": 88224, "start": 889.04, "end": 895.28, "text": " the only thing you get is a gazillion hits explaining how Dropbox the app works.", "tokens": [50704, 264, 787, 551, 291, 483, 307, 257, 26232, 11836, 8664, 13468, 577, 17675, 4995, 264, 724, 1985, 13, 51016], "temperature": 0.0, "avg_logprob": -0.06504131472387979, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.009411032311618328}, {"id": 151, "seek": 88224, "start": 896.08, "end": 902.0, "text": " Ask the question to chat GPT and it'll tell you that yes, Dropbox is a word that English", "tokens": [51056, 12320, 264, 1168, 281, 5081, 26039, 51, 293, 309, 603, 980, 291, 300, 2086, 11, 17675, 4995, 307, 257, 1349, 300, 3669, 51352], "temperature": 0.0, "avg_logprob": -0.06504131472387979, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.009411032311618328}, {"id": 152, "seek": 88224, "start": 902.0, "end": 908.88, "text": " native speakers will understand. For the same reason, chat GPT is really good at listing pros", "tokens": [51352, 8470, 9518, 486, 1223, 13, 1171, 264, 912, 1778, 11, 5081, 26039, 51, 307, 534, 665, 412, 22161, 6267, 51696], "temperature": 0.0, "avg_logprob": -0.06504131472387979, "compression_ratio": 1.5695067264573992, "no_speech_prob": 0.009411032311618328}, {"id": 153, "seek": 90888, "start": 908.88, "end": 915.6, "text": " and cons for certain arguments because those are words which stand in relation to the question.", "tokens": [50364, 293, 1014, 337, 1629, 12869, 570, 729, 366, 2283, 597, 1463, 294, 9721, 281, 264, 1168, 13, 50700], "temperature": 0.0, "avg_logprob": -0.10295493262154716, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.02194424532353878}, {"id": 154, "seek": 90888, "start": 915.6, "end": 921.2, "text": " It's also good at finding technical terms and keywords from rather vague verbal descriptions.", "tokens": [50700, 467, 311, 611, 665, 412, 5006, 6191, 2115, 293, 21009, 490, 2831, 24247, 24781, 24406, 13, 50980], "temperature": 0.0, "avg_logprob": -0.10295493262154716, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.02194424532353878}, {"id": 155, "seek": 90888, "start": 921.92, "end": 928.16, "text": " For example, I asked it, what's the name for this effect where things get shorter when you move at", "tokens": [51016, 1171, 1365, 11, 286, 2351, 309, 11, 437, 311, 264, 1315, 337, 341, 1802, 689, 721, 483, 11639, 562, 291, 1286, 412, 51328], "temperature": 0.0, "avg_logprob": -0.10295493262154716, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.02194424532353878}, {"id": 156, "seek": 90888, "start": 928.16, "end": 934.56, "text": " high speed? It explained, the name of the effect you are referring to is length contraction or", "tokens": [51328, 1090, 3073, 30, 467, 8825, 11, 264, 1315, 295, 264, 1802, 291, 366, 13761, 281, 307, 4641, 37372, 420, 51648], "temperature": 0.0, "avg_logprob": -0.10295493262154716, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.02194424532353878}, {"id": 157, "seek": 93456, "start": 934.56, "end": 939.1199999999999, "text": " Lawrence contraction. It is a consequence of the theory of special relativity.", "tokens": [50364, 22787, 37372, 13, 467, 307, 257, 18326, 295, 264, 5261, 295, 2121, 45675, 13, 50592], "temperature": 0.0, "avg_logprob": -0.07454582952683972, "compression_ratio": 1.5814814814814815, "no_speech_prob": 0.00041729744407348335}, {"id": 158, "seek": 93456, "start": 939.68, "end": 944.0799999999999, "text": " Which is perfectly correct. But don't ask it how English words are pronounced,", "tokens": [50620, 3013, 307, 6239, 3006, 13, 583, 500, 380, 1029, 309, 577, 3669, 2283, 366, 23155, 11, 50840], "temperature": 0.0, "avg_logprob": -0.07454582952683972, "compression_ratio": 1.5814814814814815, "no_speech_prob": 0.00041729744407348335}, {"id": 159, "seek": 93456, "start": 944.0799999999999, "end": 949.52, "text": " it makes even more mistakes than I do. What does this tell us about whether we", "tokens": [50840, 309, 1669, 754, 544, 8038, 813, 286, 360, 13, 708, 775, 341, 980, 505, 466, 1968, 321, 51112], "temperature": 0.0, "avg_logprob": -0.07454582952683972, "compression_ratio": 1.5814814814814815, "no_speech_prob": 0.00041729744407348335}, {"id": 160, "seek": 93456, "start": 949.52, "end": 956.0799999999999, "text": " understand quantum mechanics? I've argued that understanding can't be inferred from the relation", "tokens": [51112, 1223, 13018, 12939, 30, 286, 600, 20219, 300, 3701, 393, 380, 312, 13596, 986, 490, 264, 9721, 51440], "temperature": 0.0, "avg_logprob": -0.07454582952683972, "compression_ratio": 1.5814814814814815, "no_speech_prob": 0.00041729744407348335}, {"id": 161, "seek": 93456, "start": 956.0799999999999, "end": 962.2399999999999, "text": " between input and output alone. The relevant question is instead whether a system has a model", "tokens": [51440, 1296, 4846, 293, 5598, 3312, 13, 440, 7340, 1168, 307, 2602, 1968, 257, 1185, 575, 257, 2316, 51748], "temperature": 0.0, "avg_logprob": -0.07454582952683972, "compression_ratio": 1.5814814814814815, "no_speech_prob": 0.00041729744407348335}, {"id": 162, "seek": 96224, "start": 962.24, "end": 967.6800000000001, "text": " of what it's trying to understand, a model that it can use to explain what's going on.", "tokens": [50364, 295, 437, 309, 311, 1382, 281, 1223, 11, 257, 2316, 300, 309, 393, 764, 281, 2903, 437, 311, 516, 322, 13, 50636], "temperature": 0.0, "avg_logprob": -0.07412434660870096, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0022514699958264828}, {"id": 163, "seek": 96224, "start": 968.32, "end": 975.12, "text": " And I'd say this is definitely the case for physicists who use quantum mechanics. I have a", "tokens": [50668, 400, 286, 1116, 584, 341, 307, 2138, 264, 1389, 337, 48716, 567, 764, 13018, 12939, 13, 286, 362, 257, 51008], "temperature": 0.0, "avg_logprob": -0.07412434660870096, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0022514699958264828}, {"id": 164, "seek": 96224, "start": 975.12, "end": 980.96, "text": " model inside my head for how quantum mechanics works. It's a set of equations that I have used", "tokens": [51008, 2316, 1854, 452, 1378, 337, 577, 13018, 12939, 1985, 13, 467, 311, 257, 992, 295, 11787, 300, 286, 362, 1143, 51300], "temperature": 0.0, "avg_logprob": -0.07412434660870096, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0022514699958264828}, {"id": 165, "seek": 96224, "start": 980.96, "end": 986.8, "text": " many times that I know how to apply and use to answer questions. And I'm sure the same is the", "tokens": [51300, 867, 1413, 300, 286, 458, 577, 281, 3079, 293, 764, 281, 1867, 1651, 13, 400, 286, 478, 988, 264, 912, 307, 264, 51592], "temperature": 0.0, "avg_logprob": -0.07412434660870096, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0022514699958264828}, {"id": 166, "seek": 98680, "start": 986.8, "end": 992.4799999999999, "text": " case for other physicists. The problem with quantum mechanics is that those equations", "tokens": [50364, 1389, 337, 661, 48716, 13, 440, 1154, 365, 13018, 12939, 307, 300, 729, 11787, 50648], "temperature": 0.0, "avg_logprob": -0.060615533514867856, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.006588454358279705}, {"id": 167, "seek": 98680, "start": 992.4799999999999, "end": 999.1999999999999, "text": " do not correspond to words we use in everyday language. Most of the problems we see with", "tokens": [50648, 360, 406, 6805, 281, 2283, 321, 764, 294, 7429, 2856, 13, 4534, 295, 264, 2740, 321, 536, 365, 50984], "temperature": 0.0, "avg_logprob": -0.060615533514867856, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.006588454358279705}, {"id": 168, "seek": 98680, "start": 999.1999999999999, "end": 1005.52, "text": " understanding quantum mechanics come from the impossibility of expressing the equations in words.", "tokens": [50984, 3701, 13018, 12939, 808, 490, 264, 38802, 2841, 295, 22171, 264, 11787, 294, 2283, 13, 51300], "temperature": 0.0, "avg_logprob": -0.060615533514867856, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.006588454358279705}, {"id": 169, "seek": 98680, "start": 1006.16, "end": 1011.3599999999999, "text": " At least in English. For all I know, you can do it in Chinese. Maybe that explains why the", "tokens": [51332, 1711, 1935, 294, 3669, 13, 1171, 439, 286, 458, 11, 291, 393, 360, 309, 294, 4649, 13, 2704, 300, 13948, 983, 264, 51592], "temperature": 0.0, "avg_logprob": -0.060615533514867856, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.006588454358279705}, {"id": 170, "seek": 101136, "start": 1011.36, "end": 1017.44, "text": " Chinese are so good with quantum technologies. It is of course possible to just convert equations", "tokens": [50364, 4649, 366, 370, 665, 365, 13018, 7943, 13, 467, 307, 295, 1164, 1944, 281, 445, 7620, 11787, 50668], "temperature": 0.0, "avg_logprob": -0.05639056945114993, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.002081872196868062}, {"id": 171, "seek": 101136, "start": 1017.44, "end": 1023.76, "text": " into words by reading them out. But we normally don't do that. What we do in science communication", "tokens": [50668, 666, 2283, 538, 3760, 552, 484, 13, 583, 321, 5646, 500, 380, 360, 300, 13, 708, 321, 360, 294, 3497, 6101, 50984], "temperature": 0.0, "avg_logprob": -0.05639056945114993, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.002081872196868062}, {"id": 172, "seek": 101136, "start": 1023.76, "end": 1031.04, "text": " is kind of a mixture with metaphors and attempts to explain some of the maths. And that conveys", "tokens": [50984, 307, 733, 295, 257, 9925, 365, 30946, 830, 293, 15257, 281, 2903, 512, 295, 264, 36287, 13, 400, 300, 18053, 749, 51348], "temperature": 0.0, "avg_logprob": -0.05639056945114993, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.002081872196868062}, {"id": 173, "seek": 101136, "start": 1031.04, "end": 1038.08, "text": " some aspects of how the equations work. But if you take the words too literally, they stop making", "tokens": [51348, 512, 7270, 295, 577, 264, 11787, 589, 13, 583, 498, 291, 747, 264, 2283, 886, 3736, 11, 436, 1590, 1455, 51700], "temperature": 0.0, "avg_logprob": -0.05639056945114993, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.002081872196868062}, {"id": 174, "seek": 103808, "start": 1038.08, "end": 1044.32, "text": " sense. But equations aren't necessary for understanding. You can also gain understanding", "tokens": [50364, 2020, 13, 583, 11787, 3212, 380, 4818, 337, 3701, 13, 509, 393, 611, 6052, 3701, 50676], "temperature": 0.0, "avg_logprob": -0.04328683864923171, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.0013044195948168635}, {"id": 175, "seek": 103808, "start": 1044.32, "end": 1050.08, "text": " of quantum mechanics by games or apps that visualize the behavior of the equations,", "tokens": [50676, 295, 13018, 12939, 538, 2813, 420, 7733, 300, 23273, 264, 5223, 295, 264, 11787, 11, 50964], "temperature": 0.0, "avg_logprob": -0.04328683864923171, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.0013044195948168635}, {"id": 176, "seek": 103808, "start": 1050.08, "end": 1056.0, "text": " like those that I talked about in an earlier video. That too will allow you to build a model", "tokens": [50964, 411, 729, 300, 286, 2825, 466, 294, 364, 3071, 960, 13, 663, 886, 486, 2089, 291, 281, 1322, 257, 2316, 51260], "temperature": 0.0, "avg_logprob": -0.04328683864923171, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.0013044195948168635}, {"id": 177, "seek": 103808, "start": 1056.0, "end": 1062.48, "text": " inside your head for how quantum mechanics works. This is why I'd also say that if we use computer", "tokens": [51260, 1854, 428, 1378, 337, 577, 13018, 12939, 1985, 13, 639, 307, 983, 286, 1116, 611, 584, 300, 498, 321, 764, 3820, 51584], "temperature": 0.0, "avg_logprob": -0.04328683864923171, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.0013044195948168635}, {"id": 178, "seek": 106248, "start": 1062.56, "end": 1069.1200000000001, "text": " simulations and visualizations in science, especially for complex problems, that doesn't mean", "tokens": [50368, 35138, 293, 5056, 14455, 294, 3497, 11, 2318, 337, 3997, 2740, 11, 300, 1177, 380, 914, 50696], "temperature": 0.0, "avg_logprob": -0.06256631127110233, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.004754027351737022}, {"id": 179, "seek": 106248, "start": 1069.1200000000001, "end": 1075.3600000000001, "text": " we've given up on understanding. Visualizing the behavior of a system and probing it and seeing", "tokens": [50696, 321, 600, 2212, 493, 322, 3701, 13, 23187, 3319, 264, 5223, 295, 257, 1185, 293, 1239, 278, 309, 293, 2577, 51008], "temperature": 0.0, "avg_logprob": -0.06256631127110233, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.004754027351737022}, {"id": 180, "seek": 106248, "start": 1075.3600000000001, "end": 1082.08, "text": " what it does is another way of building a model in your head. There is another reason why physicists", "tokens": [51008, 437, 309, 775, 307, 1071, 636, 295, 2390, 257, 2316, 294, 428, 1378, 13, 821, 307, 1071, 1778, 983, 48716, 51344], "temperature": 0.0, "avg_logprob": -0.06256631127110233, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.004754027351737022}, {"id": 181, "seek": 106248, "start": 1082.08, "end": 1086.8, "text": " say they don't understand quantum mechanics, which is that it's internally inconsistent.", "tokens": [51344, 584, 436, 500, 380, 1223, 13018, 12939, 11, 597, 307, 300, 309, 311, 19501, 36891, 13, 51580], "temperature": 0.0, "avg_logprob": -0.06256631127110233, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.004754027351737022}, {"id": 182, "seek": 106248, "start": 1086.8, "end": 1091.68, "text": " I've talked about this a few times before and it's somewhat off topic here, so I don't want to get", "tokens": [51580, 286, 600, 2825, 466, 341, 257, 1326, 1413, 949, 293, 309, 311, 8344, 766, 4829, 510, 11, 370, 286, 500, 380, 528, 281, 483, 51824], "temperature": 0.0, "avg_logprob": -0.06256631127110233, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.004754027351737022}, {"id": 183, "seek": 109168, "start": 1091.68, "end": 1097.28, "text": " into this again. Let me just say that there are problems with quantum mechanics that go beyond", "tokens": [50364, 666, 341, 797, 13, 961, 385, 445, 584, 300, 456, 366, 2740, 365, 13018, 12939, 300, 352, 4399, 50644], "temperature": 0.0, "avg_logprob": -0.07177069865235496, "compression_ratio": 1.5635179153094463, "no_speech_prob": 0.00475189508870244}, {"id": 184, "seek": 109168, "start": 1097.28, "end": 1103.8400000000001, "text": " the difficulty of expressing it in words. So where will the AI boom leaders? First of all,", "tokens": [50644, 264, 10360, 295, 22171, 309, 294, 2283, 13, 407, 689, 486, 264, 7318, 9351, 3523, 30, 2386, 295, 439, 11, 50972], "temperature": 0.0, "avg_logprob": -0.07177069865235496, "compression_ratio": 1.5635179153094463, "no_speech_prob": 0.00475189508870244}, {"id": 185, "seek": 109168, "start": 1103.8400000000001, "end": 1109.44, "text": " it's rather foreseeable that before long, we'll all have a personalized AI that'll offer anything", "tokens": [50972, 309, 311, 2831, 38736, 712, 300, 949, 938, 11, 321, 603, 439, 362, 257, 28415, 7318, 300, 603, 2626, 1340, 51252], "temperature": 0.0, "avg_logprob": -0.07177069865235496, "compression_ratio": 1.5635179153094463, "no_speech_prob": 0.00475189508870244}, {"id": 186, "seek": 109168, "start": 1109.44, "end": 1115.52, "text": " from financial advice to relationship counseling. The more you can afford to pay, the better it'll", "tokens": [51252, 490, 4669, 5192, 281, 2480, 23889, 13, 440, 544, 291, 393, 6157, 281, 1689, 11, 264, 1101, 309, 603, 51556], "temperature": 0.0, "avg_logprob": -0.07177069865235496, "compression_ratio": 1.5635179153094463, "no_speech_prob": 0.00475189508870244}, {"id": 187, "seek": 109168, "start": 1115.52, "end": 1120.72, "text": " be and the free version will suggest you marry the Prince of Nigeria. Of course, people are going", "tokens": [51556, 312, 293, 264, 1737, 3037, 486, 3402, 291, 9747, 264, 9821, 295, 28828, 13, 2720, 1164, 11, 561, 366, 516, 51816], "temperature": 0.0, "avg_logprob": -0.07177069865235496, "compression_ratio": 1.5635179153094463, "no_speech_prob": 0.00475189508870244}, {"id": 188, "seek": 112072, "start": 1120.72, "end": 1125.84, "text": " to complain it'll destroy the world and all, but it'll happen anyway because when has the risk", "tokens": [50364, 281, 11024, 309, 603, 5293, 264, 1002, 293, 439, 11, 457, 309, 603, 1051, 4033, 570, 562, 575, 264, 3148, 50620], "temperature": 0.0, "avg_logprob": -0.07409866039569561, "compression_ratio": 1.6192170818505338, "no_speech_prob": 0.008841224014759064}, {"id": 189, "seek": 112072, "start": 1125.84, "end": 1130.88, "text": " of destroying the world ever stopped us from doing anything if there was money to make with it?", "tokens": [50620, 295, 19926, 264, 1002, 1562, 5936, 505, 490, 884, 1340, 498, 456, 390, 1460, 281, 652, 365, 309, 30, 50872], "temperature": 0.0, "avg_logprob": -0.07409866039569561, "compression_ratio": 1.6192170818505338, "no_speech_prob": 0.008841224014759064}, {"id": 190, "seek": 112072, "start": 1131.44, "end": 1136.4, "text": " The best and biggest AIs will be those of big companies and governments,", "tokens": [50900, 440, 1151, 293, 3880, 316, 6802, 486, 312, 729, 295, 955, 3431, 293, 11280, 11, 51148], "temperature": 0.0, "avg_logprob": -0.07409866039569561, "compression_ratio": 1.6192170818505338, "no_speech_prob": 0.008841224014759064}, {"id": 191, "seek": 112072, "start": 1136.4, "end": 1142.96, "text": " and that's almost guaranteed to increase wealth disparities. We're also going to see YouTube", "tokens": [51148, 293, 300, 311, 1920, 18031, 281, 3488, 7203, 32514, 13, 492, 434, 611, 516, 281, 536, 3088, 51476], "temperature": 0.0, "avg_logprob": -0.07409866039569561, "compression_ratio": 1.6192170818505338, "no_speech_prob": 0.008841224014759064}, {"id": 192, "seek": 112072, "start": 1142.96, "end": 1150.16, "text": " flooded by human avatars and other funky AI-generated visuals because it's much faster and cheaper", "tokens": [51476, 31594, 538, 1952, 1305, 267, 685, 293, 661, 33499, 7318, 12, 21848, 770, 26035, 570, 309, 311, 709, 4663, 293, 12284, 51836], "temperature": 0.0, "avg_logprob": -0.07409866039569561, "compression_ratio": 1.6192170818505338, "no_speech_prob": 0.008841224014759064}, {"id": 193, "seek": 115016, "start": 1150.16, "end": 1155.44, "text": " than getting a human to retext or go out and film that old-fashioned thing called reality.", "tokens": [50364, 813, 1242, 257, 1952, 281, 319, 25111, 420, 352, 484, 293, 2007, 300, 1331, 12, 37998, 551, 1219, 4103, 13, 50628], "temperature": 0.0, "avg_logprob": -0.06457585627489751, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.0006069797673262656}, {"id": 194, "seek": 115016, "start": 1156.0, "end": 1161.1200000000001, "text": " But I don't think this trend will last long because it'll be extremely difficult to make", "tokens": [50656, 583, 286, 500, 380, 519, 341, 6028, 486, 1036, 938, 570, 309, 603, 312, 4664, 2252, 281, 652, 50912], "temperature": 0.0, "avg_logprob": -0.06457585627489751, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.0006069797673262656}, {"id": 195, "seek": 115016, "start": 1161.1200000000001, "end": 1167.28, "text": " money with it. The easier it becomes to create artificial footage, the more people will look", "tokens": [50912, 1460, 365, 309, 13, 440, 3571, 309, 3643, 281, 1884, 11677, 9556, 11, 264, 544, 561, 486, 574, 51220], "temperature": 0.0, "avg_logprob": -0.06457585627489751, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.0006069797673262656}, {"id": 196, "seek": 115016, "start": 1167.28, "end": 1172.8000000000002, "text": " for authenticity, so that stupid German accent might eventually actually be good for something.", "tokens": [51220, 337, 34215, 11, 370, 300, 6631, 6521, 11982, 1062, 4728, 767, 312, 665, 337, 746, 13, 51496], "temperature": 0.0, "avg_logprob": -0.06457585627489751, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.0006069797673262656}, {"id": 197, "seek": 115016, "start": 1172.8000000000002, "end": 1178.24, "text": " If nothing else, it makes me difficult to simulate. Will AI eventually become conscious?", "tokens": [51496, 759, 1825, 1646, 11, 309, 1669, 385, 2252, 281, 27817, 13, 3099, 7318, 4728, 1813, 6648, 30, 51768], "temperature": 0.0, "avg_logprob": -0.06457585627489751, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.0006069797673262656}, {"id": 198, "seek": 117824, "start": 1178.8, "end": 1184.32, "text": " Of course. There's nothing magic about the human brain, it's just a lot of connections that process", "tokens": [50392, 2720, 1164, 13, 821, 311, 1825, 5585, 466, 264, 1952, 3567, 11, 309, 311, 445, 257, 688, 295, 9271, 300, 1399, 50668], "temperature": 0.0, "avg_logprob": -0.06804655476620322, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.001596723566763103}, {"id": 199, "seek": 117824, "start": 1184.32, "end": 1190.88, "text": " a lot of information. If we can be conscious, computers can do it too, and it will happen", "tokens": [50668, 257, 688, 295, 1589, 13, 759, 321, 393, 312, 6648, 11, 10807, 393, 360, 309, 886, 11, 293, 309, 486, 1051, 50996], "temperature": 0.0, "avg_logprob": -0.06804655476620322, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.001596723566763103}, {"id": 200, "seek": 117824, "start": 1190.88, "end": 1198.32, "text": " eventually. How will we know? Like understanding, you can't probe consciousness just by observing", "tokens": [50996, 4728, 13, 1012, 486, 321, 458, 30, 1743, 3701, 11, 291, 393, 380, 22715, 10081, 445, 538, 22107, 51368], "temperature": 0.0, "avg_logprob": -0.06804655476620322, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.001596723566763103}, {"id": 201, "seek": 117824, "start": 1198.32, "end": 1204.4, "text": " what goes in and comes out. If you'd really want to know, you'd have to look what's going on inside", "tokens": [51368, 437, 1709, 294, 293, 1487, 484, 13, 759, 291, 1116, 534, 528, 281, 458, 11, 291, 1116, 362, 281, 574, 437, 311, 516, 322, 1854, 51672], "temperature": 0.0, "avg_logprob": -0.06804655476620322, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.001596723566763103}, {"id": 202, "seek": 120440, "start": 1204.4, "end": 1210.16, "text": " and at the moment, that wouldn't help because we don't know how to identify consciousness in any", "tokens": [50364, 293, 412, 264, 1623, 11, 300, 2759, 380, 854, 570, 321, 500, 380, 458, 577, 281, 5876, 10081, 294, 604, 50652], "temperature": 0.0, "avg_logprob": -0.07872242494062943, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.0019259825348854065}, {"id": 203, "seek": 120440, "start": 1210.16, "end": 1216.88, "text": " case. Basically, we can't answer the question. But personally, I find this extremely interesting", "tokens": [50652, 1389, 13, 8537, 11, 321, 393, 380, 1867, 264, 1168, 13, 583, 5665, 11, 286, 915, 341, 4664, 1880, 50988], "temperature": 0.0, "avg_logprob": -0.07872242494062943, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.0019259825348854065}, {"id": 204, "seek": 120440, "start": 1216.88, "end": 1223.0400000000002, "text": " because we're about to create an intelligent species that'll be very different from our own.", "tokens": [50988, 570, 321, 434, 466, 281, 1884, 364, 13232, 6172, 300, 603, 312, 588, 819, 490, 527, 1065, 13, 51296], "temperature": 0.0, "avg_logprob": -0.07872242494062943, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.0019259825348854065}, {"id": 205, "seek": 120440, "start": 1223.0400000000002, "end": 1227.1200000000001, "text": " And if we're dumb enough to cause our own extinction this way, then I guess that's what", "tokens": [51296, 400, 498, 321, 434, 10316, 1547, 281, 3082, 527, 1065, 33163, 341, 636, 11, 550, 286, 2041, 300, 311, 437, 51500], "temperature": 0.0, "avg_logprob": -0.07872242494062943, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.0019259825348854065}, {"id": 206, "seek": 120440, "start": 1227.1200000000001, "end": 1233.2800000000002, "text": " we deserve. Meanwhile, enjoy the ride. At least for now, the best tool we have for understanding", "tokens": [51500, 321, 9948, 13, 13879, 11, 2103, 264, 5077, 13, 1711, 1935, 337, 586, 11, 264, 1151, 2290, 321, 362, 337, 3701, 51808], "temperature": 0.0, "avg_logprob": -0.07872242494062943, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.0019259825348854065}, {"id": 207, "seek": 123328, "start": 1233.28, "end": 1238.8, "text": " the world is the human brain. But if you really want to understand quantum mechanics or neural", "tokens": [50364, 264, 1002, 307, 264, 1952, 3567, 13, 583, 498, 291, 534, 528, 281, 1223, 13018, 12939, 420, 18161, 50640], "temperature": 0.0, "avg_logprob": -0.04519118819125863, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.047381091862916946}, {"id": 208, "seek": 123328, "start": 1238.8, "end": 1245.2, "text": " networks, then passively watching a video isn't enough. You have to actively engage with the", "tokens": [50640, 9590, 11, 550, 1320, 3413, 1976, 257, 960, 1943, 380, 1547, 13, 509, 362, 281, 13022, 4683, 365, 264, 50960], "temperature": 0.0, "avg_logprob": -0.04519118819125863, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.047381091862916946}, {"id": 209, "seek": 123328, "start": 1245.2, "end": 1251.6, "text": " material. Brilliant.org, who have been sponsoring this video, is a great place for that. Brilliant", "tokens": [50960, 2527, 13, 34007, 13, 4646, 11, 567, 362, 668, 30311, 341, 960, 11, 307, 257, 869, 1081, 337, 300, 13, 34007, 51280], "temperature": 0.0, "avg_logprob": -0.04519118819125863, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.047381091862916946}, {"id": 210, "seek": 123328, "start": 1251.6, "end": 1257.76, "text": " offers courses on a large variety of subjects in science and mathematics, and they add new content", "tokens": [51280, 7736, 7712, 322, 257, 2416, 5673, 295, 13066, 294, 3497, 293, 18666, 11, 293, 436, 909, 777, 2701, 51588], "temperature": 0.0, "avg_logprob": -0.04519118819125863, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.047381091862916946}, {"id": 211, "seek": 125776, "start": 1257.76, "end": 1263.92, "text": " every month. The great thing about their courses is that they're all interactive with visualizations", "tokens": [50364, 633, 1618, 13, 440, 869, 551, 466, 641, 7712, 307, 300, 436, 434, 439, 15141, 365, 5056, 14455, 50672], "temperature": 0.0, "avg_logprob": -0.07777287883143272, "compression_ratio": 1.5912698412698412, "no_speech_prob": 0.08147276937961578}, {"id": 212, "seek": 125776, "start": 1263.92, "end": 1270.0, "text": " and follow-up questions. So you can check right away whether you can apply what you've learned,", "tokens": [50672, 293, 1524, 12, 1010, 1651, 13, 407, 291, 393, 1520, 558, 1314, 1968, 291, 393, 3079, 437, 291, 600, 3264, 11, 50976], "temperature": 0.0, "avg_logprob": -0.07777287883143272, "compression_ratio": 1.5912698412698412, "no_speech_prob": 0.08147276937961578}, {"id": 213, "seek": 125776, "start": 1270.0, "end": 1276.0, "text": " and that's really what understanding is all about. What I need to freshen up my knowledge or want to", "tokens": [50976, 293, 300, 311, 534, 437, 3701, 307, 439, 466, 13, 708, 286, 643, 281, 25235, 2932, 493, 452, 3601, 420, 528, 281, 51276], "temperature": 0.0, "avg_logprob": -0.07777287883143272, "compression_ratio": 1.5912698412698412, "no_speech_prob": 0.08147276937961578}, {"id": 214, "seek": 125776, "start": 1276.0, "end": 1282.4, "text": " learn something new, first thing I do is look it up on Brilliant. To get some background on the physics", "tokens": [51276, 1466, 746, 777, 11, 700, 551, 286, 360, 307, 574, 309, 493, 322, 34007, 13, 1407, 483, 512, 3678, 322, 264, 10649, 51596], "temperature": 0.0, "avg_logprob": -0.07777287883143272, "compression_ratio": 1.5912698412698412, "no_speech_prob": 0.08147276937961578}, {"id": 215, "seek": 128240, "start": 1282.4, "end": 1288.24, "text": " in this video, check out, for example, their courses on neural networks and quantum objects,", "tokens": [50364, 294, 341, 960, 11, 1520, 484, 11, 337, 1365, 11, 641, 7712, 322, 18161, 9590, 293, 13018, 6565, 11, 50656], "temperature": 0.0, "avg_logprob": -0.08284485340118408, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.04601236805319786}, {"id": 216, "seek": 128240, "start": 1288.24, "end": 1295.2, "text": " or even better, check out my own course about quantum mechanics. My course gives you an introduction", "tokens": [50656, 420, 754, 1101, 11, 1520, 484, 452, 1065, 1164, 466, 13018, 12939, 13, 1222, 1164, 2709, 291, 364, 9339, 51004], "temperature": 0.0, "avg_logprob": -0.08284485340118408, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.04601236805319786}, {"id": 217, "seek": 128240, "start": 1295.2, "end": 1301.6000000000001, "text": " to interference, superpositions and entanglement, the uncertainty principle, and Bell's theorem.", "tokens": [51004, 281, 24497, 11, 1687, 30010, 2451, 293, 948, 656, 3054, 11, 264, 15697, 8665, 11, 293, 11485, 311, 20904, 13, 51324], "temperature": 0.0, "avg_logprob": -0.08284485340118408, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.04601236805319786}, {"id": 218, "seek": 128240, "start": 1302.24, "end": 1307.3600000000001, "text": " You don't need to be an expert to take this course. I've worked together with Brilliant so", "tokens": [51356, 509, 500, 380, 643, 281, 312, 364, 5844, 281, 747, 341, 1164, 13, 286, 600, 2732, 1214, 365, 34007, 370, 51612], "temperature": 0.0, "avg_logprob": -0.08284485340118408, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.04601236805319786}, {"id": 219, "seek": 130736, "start": 1307.36, "end": 1312.56, "text": " that you can start from the very basics. If you're interested in trying Brilliant out,", "tokens": [50364, 300, 291, 393, 722, 490, 264, 588, 14688, 13, 759, 291, 434, 3102, 294, 1382, 34007, 484, 11, 50624], "temperature": 0.0, "avg_logprob": -0.10991897139438363, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.10952350497245789}, {"id": 220, "seek": 130736, "start": 1312.56, "end": 1318.8, "text": " use our link brilliant.org slash Sabine and sign up for free trial, where you'll get to try out", "tokens": [50624, 764, 527, 2113, 10248, 13, 4646, 17330, 13915, 533, 293, 1465, 493, 337, 1737, 7308, 11, 689, 291, 603, 483, 281, 853, 484, 50936], "temperature": 0.0, "avg_logprob": -0.10991897139438363, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.10952350497245789}, {"id": 221, "seek": 130736, "start": 1318.8, "end": 1324.8, "text": " everything Brilliant has to offer for 30 days. The first 200 subscribers using this link will also", "tokens": [50936, 1203, 34007, 575, 281, 2626, 337, 2217, 1708, 13, 440, 700, 2331, 11092, 1228, 341, 2113, 486, 611, 51236], "temperature": 0.0, "avg_logprob": -0.10991897139438363, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.10952350497245789}, {"id": 222, "seek": 130736, "start": 1324.8, "end": 1333.6, "text": " get 20% off the annual premium subscription. Thanks for watching. See you next week.", "tokens": [51236, 483, 945, 4, 766, 264, 9784, 12049, 17231, 13, 2561, 337, 1976, 13, 3008, 291, 958, 1243, 13, 51676], "temperature": 0.0, "avg_logprob": -0.10991897139438363, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.10952350497245789}], "language": "en"}