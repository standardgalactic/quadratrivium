WEBVTT

00:00.000 --> 00:04.560
What should you do if you're spending too much time on social media, asking for a friend?

00:05.440 --> 00:10.320
Well, you try to convince yourself that social media is actually good for, well,

00:10.320 --> 00:15.680
something. It's gotta be good for something, right? But they say that social media increases

00:15.680 --> 00:20.400
polarization and gets you stuck in echo chambers full of fake news and so on.

00:21.200 --> 00:25.200
How bad is social media? That's what we'll talk about today.

00:26.000 --> 00:35.840
Social media has changed society profoundly. About 60% of the world's population now uses

00:35.840 --> 00:42.160
social media. It has made it vastly easier to find people all over the globe to connect with them

00:42.160 --> 00:50.240
and to get insulted by them. What does that do to society? It's complicated. American social

00:50.240 --> 00:56.240
psychologists Jonathan Haidt and sociologist Chris Bayer have compiled a public Google doc

00:56.240 --> 01:02.000
in which they collect references on questions such as, does social media make people angrier?

01:02.000 --> 01:07.920
And does social media create political echo chambers? The most relevant thing you learn

01:07.920 --> 01:13.200
from this document is that whatever your opinion, you can find a paper that supports it.

01:13.200 --> 01:18.400
Honestly, I began working on this video thinking it'll end up being one big shrug,

01:18.400 --> 01:24.160
because that's how sociology generally looks like to a physicist. But it turns out it isn't

01:24.160 --> 01:31.360
quite as bad. You just have to be really careful with phrasing the question. For example, you may

01:31.360 --> 01:37.280
remember the headlines claiming that fake news spreads faster than the truth. Then again,

01:37.280 --> 01:42.320
there were headlines saying that those headlines about fake news were themselves fake news.

01:42.880 --> 01:47.840
What is going on? Well, the original headlines were based on a 2018

01:47.840 --> 01:54.560
paper published in Science by researchers at MIT. The authors compared how true and false

01:54.560 --> 02:04.480
news stories spread on Twitter. They had a sample of about 126,000 news items from 2006 to 2017,

02:04.480 --> 02:11.520
tweeted by about 3 million people over 4.5 million times. So not a small study.

02:11.520 --> 02:16.560
These news items were classified as true or false according to certain fact-tracking

02:16.560 --> 02:22.000
organizations. The conclusion of the study was, in the authors' own words, that

02:22.000 --> 02:28.320
falsehood diffused significantly farther, faster, deeper and more broadly than the truth

02:28.320 --> 02:35.120
in all categories. The facts were most pronounced for false political news. And it wasn't a small

02:35.120 --> 02:41.440
difference. They found that it took two stories approximately six times as long as false stories

02:41.520 --> 02:49.280
to reach 1,500 people. But in 2021, other researchers pointed out that the 2018 paper

02:49.280 --> 02:55.280
looked at news that had been fact-checked by certain organizations, but that those organizations pay

02:55.280 --> 03:01.840
more attention to news that have already spread quite successfully. An article in Science then

03:01.840 --> 03:07.520
claimed that this means the original study had been debunked. This is why you've seen the

03:07.520 --> 03:13.760
headline saying news about fake news is fake news. That wasn't the end of the story. Because the

03:13.760 --> 03:19.600
authors of the original study then said they'd never claimed their study applies to all fake news,

03:19.600 --> 03:25.280
it had just been misreported. And the authors of the new study said they had never claimed the

03:25.280 --> 03:31.680
earlier study was wrong because they knew it had been misreported. Then the author of the Science

03:31.760 --> 03:37.360
News article who had claimed that the misreported fake news study was fake news

03:37.360 --> 03:43.920
apologized that his article had misreported this story. I hope that clarifies it. But wait,

03:43.920 --> 03:51.520
what does all of that mean now? Do fake news spread better or do they not? The answer is they do,

03:51.520 --> 03:57.680
but it turns out that the major difference between true news and fake news is that fake news spread

03:57.680 --> 04:03.840
to a larger audience. And since they appeal to a larger audience, they also spread faster.

04:04.400 --> 04:09.920
But if you compare true and false stories that have reached an audience of the same size,

04:09.920 --> 04:16.880
then the sharing pattern looks the same. This was the point of the 2021 paper. It's not like fake

04:16.880 --> 04:22.720
news networks have a different connectivity. The size of the audience that they attract is the

04:22.720 --> 04:28.480
major difference. And yes, that was strictly speaking only demonstrated for fake news stories

04:28.480 --> 04:34.400
that were fact-checked by certain organizations. One of the authors made this diagram to show

04:34.400 --> 04:40.000
the difference between what they said they did and what the headlines said they did. But the

04:40.000 --> 04:45.840
authors also say they're reasonably confident their finding will carry over to false news more

04:45.840 --> 04:52.480
generally, but that remains to be seen. I'm guessing there are people working on this as we speak.

04:52.480 --> 04:58.160
But the 2018 science paper made an interesting point that didn't spread widely. They found

04:58.160 --> 05:04.640
that bots accelerated the spread of true and false news equally. This means that if false news

05:04.640 --> 05:10.960
spreads better than the truth, that's because humans are more likely to spread false news.

05:10.960 --> 05:16.800
We can't blame it on the bots. The authors conjecture that the reason may be that people

05:16.800 --> 05:21.520
like novelty and it's easier to be original with something that's made up.

05:21.520 --> 05:27.680
Anecdotal evidence. I had my first encounter with fake news on Facebook in 2016 when Trump

05:27.680 --> 05:34.720
ran for president. It was a quote attributed to Trump from some anti-Republican Facebook page

05:34.720 --> 05:39.920
shared by an American friend. Several people pointed out that there was no evidence Trump

05:39.920 --> 05:46.240
actually said that. The guy who shared it reacted by saying it's funny even if it isn't true.

05:46.240 --> 05:51.440
And that's why false news spreads. We share it for reasons other than accuracy,

05:51.440 --> 05:56.720
because it's funny or upsetting or because it allows us to express our own opinion.

05:56.720 --> 06:02.000
Whether it's true doesn't really matter for that. The problem is that the next person who

06:02.000 --> 06:08.160
comes across shared fake news believes that the person who shared it believed it to be true

06:08.160 --> 06:13.600
and is therefore more likely to also believe it to be true. What can be done about it?

06:13.600 --> 06:19.040
It's easier than you think. Because most people agree that fake news is bad and they're actually

06:19.040 --> 06:25.200
quite good at spotting it. You just have to occasionally remind them to think before sharing.

06:25.200 --> 06:29.360
At least this was the conclusion of a paper published in Nature last year.

06:30.000 --> 06:36.720
The authors recruited about a thousand Americans and presented them with 36 actual news stories

06:36.720 --> 06:42.400
taken from social media. Half of the headlines were false and half were correct.

06:42.400 --> 06:47.360
Half favorable to Democrats and the other half favorable to Republicans.

06:47.360 --> 06:52.560
The participants were then asked to evaluate the accuracy of the news items.

06:53.200 --> 06:59.360
They quite reliably rated correct headlines as correct and false ones as false.

06:59.360 --> 07:05.360
And while they did rate headlines in favor of their own political orientation as correct more often

07:05.360 --> 07:11.440
than those in favor of the other camp, the partisan influence was much smaller than that of the

07:11.440 --> 07:17.920
actual accuracy of the headline. So the problem is not that we're just bad at spotting bad news.

07:18.640 --> 07:24.960
But the authors also found that whether the headlines were right or wrong had little effect

07:24.960 --> 07:32.080
on whether people intended to share a news item. They then encouraged people to consider the accuracy

07:32.080 --> 07:39.520
of the news item and afterwards asked again how likely they'd share it. This simple tactic led

07:39.520 --> 07:46.240
to a big reduction of the intention to share false news but didn't affect the intention to share real

07:46.240 --> 07:53.120
news. According to the paper, accuracy often has little effect on sharing because the social

07:53.120 --> 07:59.840
media context focuses users' attention on other factors such as the desire to attract and please

07:59.840 --> 08:05.440
followers and friends or to signal one's group membership. According to another paper that just

08:05.440 --> 08:12.000
appeared two months ago, the misinformation problem is particularly pronounced in the United States.

08:12.560 --> 08:18.400
The authors of the paper found that while people from the UK, Canada, Australia and New Zealand

08:18.400 --> 08:24.640
are exposed to misinformation on social media at about the same rate, Americans are three times

08:24.640 --> 08:30.480
more likely to share it. Earlier this year a review paper in the journal Nature Medicine

08:30.480 --> 08:35.760
looked at the spread of misinformation about public health in particular by reviewing one

08:35.760 --> 08:42.080
and a 23 papers. The two major conclusions that the author draws from his literature survey is that

08:42.080 --> 08:47.520
A. people are sometimes duped by misinformation just because they are distracted or not paying

08:47.520 --> 08:54.080
attention and B. some people believe in and share misinformation because it reinforces their beliefs.

08:54.640 --> 09:00.080
So that's consistent with what the other papers had found. As to what to do about it,

09:00.080 --> 09:05.440
one thing he suggests is also just reminding people to think about accuracy before sharing.

09:06.000 --> 09:11.440
Another interesting suggestion he has is to give people information about the tactics of

09:11.440 --> 09:17.840
misinformation spreaders with browser games. There are two of those games, one is called

09:17.840 --> 09:24.640
Go Viral and the other one Get Bad News. Studies have found that people who played these games

09:24.640 --> 09:30.400
were much better at spotting health-related misinformation. You can try them out yourself,

09:30.400 --> 09:36.080
links are in the info below. I think this is a really good idea and I'd like to have a game

09:36.080 --> 09:42.400
like this about physics please. So yes, social media spreads a lot of misinformation. The good

09:42.400 --> 09:49.040
side of social media is that it also seems to generally benefit information literacy. In 2018,

09:49.120 --> 09:55.840
a team of American researchers recruited almost 3,000 Americans. They offered half of them $20

09:55.840 --> 10:02.480
to deactivate their Facebook accounts for four weeks just prior to the 2018 midterm elections.

10:02.480 --> 10:07.520
Four weeks later, those who disconnected from Facebook were less able to correctly answer

10:07.520 --> 10:13.360
factual questions about recent news events. But they also reported increased well-being

10:13.360 --> 10:19.520
and less political polarization. Let's therefore look at what we know about polarization and

10:19.520 --> 10:25.760
echo chambers. If you watched a few of my videos on quantum mechanics, soon all your recommended

10:25.760 --> 10:32.480
videos will be about quantum mechanics. Suddenly, the whole world is quantum mechanics. Such an

10:32.480 --> 10:39.120
echo chamber seems to be an inevitable side effect of algorithms that want to help you find what

10:39.120 --> 10:46.240
you're interested in. And as a result, you get more of the same. This leads to the dreaded

10:46.240 --> 10:51.600
conspiracy rabbit holes that you fall into on YouTube as it happened to me when I was working

10:51.600 --> 10:57.920
on my video on flat earthers, though YouTube seems to have tweaked their algorithm since to prevent

10:57.920 --> 11:04.640
that from happening quite as easily. These more of the same algorithms help you make contact with

11:04.640 --> 11:10.800
people who think like yourself. So the idea that we live in echo chambers sounds plausible.

11:10.800 --> 11:18.080
But plausible ideas are the ones you should be most skeptical about. What does the data say?

11:18.080 --> 11:24.160
According to a 2021 paper by a group from the University of Oxford, echo chamber issues are

11:24.160 --> 11:30.240
real, but the problem has been hugely overstated. They looked at surveys from seven different

11:30.240 --> 11:36.560
countries in which people reported what news they typically consumed. Turns out, only about

11:36.560 --> 11:42.160
five percent of social media users are properly stuck in a political echo chamber in which they

11:42.160 --> 11:49.520
almost exclusively consume news from one political side. Though the numbers differ somewhat by country,

11:49.520 --> 11:55.440
the overall largest fraction of people in echo chambers is that of the American left.

11:55.520 --> 12:01.440
The previously mentioned Chris Bale is lead author of a 2018 paper about an experiment

12:01.440 --> 12:08.400
in which they try to get people out of their echo chambers. They surveyed about 1,500 Americans,

12:08.400 --> 12:14.640
about half Democrats and the other half Republicans, who visited Twitter at least three times each

12:14.640 --> 12:21.040
week. After one week of tracking, a randomly selected group of those people was offered

12:21.040 --> 12:27.200
$11 to follow a Twitter bot for one month, but they were not informed about the purpose of the study.

12:27.760 --> 12:33.280
The bot initially just tweeted landscape pictures, but then began tweeting opinions

12:33.280 --> 12:39.840
that promoted the participants' opposing political ideology. At the end of the month,

12:39.840 --> 12:47.200
the participants were surveyed again. Turned out that Republicans who followed a liberal Twitter bot

12:47.200 --> 12:54.000
became even more conservative. The more they were exposed, the larger the effect. For Democrats,

12:54.000 --> 12:59.600
the change was not statistically significant. By the way, Chris Bale is the director of an

12:59.600 --> 13:05.680
institute called the Polarization Lab that lets you check how deeply stuck in an echo chamber

13:05.680 --> 13:12.560
you are on Twitter. Turns out, rather unsurprisingly, I'm deeply stuck in a liberal camp. That's what

13:12.560 --> 13:18.400
you get when you mostly follow people with PhDs. The question whether social media increases

13:18.400 --> 13:24.000
polarization in society has been extensively studied, especially in the United States.

13:24.640 --> 13:29.840
The risk, sociologists say, is that social media makes it easier to find people whose

13:29.840 --> 13:36.640
opinions we like and we get encouraged by like-minded people to distance ourselves from the perceived

13:36.640 --> 13:44.560
enemy. Indeed, in 2018, a leaked internal presentation at Facebook warned senior executives

13:44.560 --> 13:50.880
that Facebook algorithms exploit the human brain's attraction to divisiveness and that,

13:50.880 --> 13:56.480
if left unchecked, the algorithm would feed users more and more divisive content

13:56.480 --> 14:01.600
in an effort to gain user attention and increase time on the platform.

14:01.600 --> 14:06.560
Now, it's quite well established among sociologists already that increased levels

14:06.560 --> 14:12.400
of polarization in society are associated with an erosion of constructive political debate,

14:12.400 --> 14:18.320
social trust and inter-party cooperation. The question we're interested in here is whether

14:18.320 --> 14:25.600
social media increases this polarization. In 2021, researchers from the UK and the US

14:25.600 --> 14:32.080
set out to answer the question whether out-group animosity drives engagement on social media.

14:32.640 --> 14:39.440
They analyzed almost 3 million Twitter posts by news media accounts and US congressional members.

14:39.440 --> 14:46.240
They found that posts about the political out-group were shared or retweeted about twice as often

14:46.240 --> 14:52.000
as posts about the in-group. And almost all of the posts about the other political camp were

14:52.000 --> 14:58.000
negative, leading to more negative engagement. Again, though, you have to be really careful

14:58.000 --> 15:03.680
to keep in mind what question a study was asking in the first place, because this finding doesn't

15:03.680 --> 15:10.720
necessarily mean that the negative engagement cost polarization to increase. It might just mean

15:10.720 --> 15:17.840
that social media is a good platform to live out your feelings. Just which way the causation goes

15:17.840 --> 15:24.000
is at the moment rather unclear. For one thing, a study from 2017 found that self-reported

15:24.000 --> 15:30.240
polarization is higher among elderly Americans who are less likely to be online to begin with.

15:30.240 --> 15:35.600
Another thing to keep in mind is that the USA isn't the only country in the world.

15:35.600 --> 15:42.000
A team of American researchers pointed out last year that while social media usage has increased

15:42.000 --> 15:48.400
worldwide, polarization has not. It has increased in the US, but in many other countries,

15:48.400 --> 15:56.320
polarization has in fact decreased. Indeed, a 2021 study among more than 3,000 Dutch citizens

15:56.320 --> 16:03.600
found that self-reported polarization correlates with social media use, but the causation goes from

16:03.600 --> 16:09.440
polarization to social media use and it depends on the platform you're using. At least in the

16:09.440 --> 16:15.360
Dutch sample, people who became politically more polarized spent more time on Facebook,

16:15.360 --> 16:20.800
but less time on Twitter. It's hard to interpret what this means, because God knows what's up

16:20.800 --> 16:27.200
with Dutch Twitter. But I think what we can take away from this is that the idea that more social

16:27.200 --> 16:34.000
media use increases polarization is almost certainly too simple to be correct. What happens

16:34.080 --> 16:40.400
depends both on cultural context and on platform design. So what do we learn from all this?

16:41.280 --> 16:47.200
Most obviously, we learn that this is a very active research area. I certainly hope that the

16:47.200 --> 16:54.160
results will eventually lead to better algorithms for social media. While we wait for that to happen,

16:54.160 --> 17:00.240
I think the best we can do is focus on the part that's in our hands, which is to decide what we

17:00.240 --> 17:06.640
pay attention to and what we share. What I have taken away from all those papers is that we have

17:06.640 --> 17:13.520
to be especially careful with headlines that upset us. Yes, they might get a lot of comments,

17:13.520 --> 17:19.040
but they might also spread misinformation and hate. So be careful out there.

17:19.040 --> 17:23.920
How has Sabina talking about nuclear power one day and about social media the other day?

17:23.920 --> 17:30.080
Is she omniscient? I'm afraid the answer is no. I work with several other people who help me

17:30.080 --> 17:35.840
sort through the scientific literature to bring the information to you. And the only reason this

17:35.840 --> 17:42.400
works is thanks to our sponsors. Today's episode was made possible by MelScience,

17:42.400 --> 17:47.840
which is a subscription service for science experiments. And I have to say, my family's

17:47.840 --> 17:52.960
having a lot of fun with their products. MelScience has experiments for children in

17:52.960 --> 17:58.000
different age categories and different scientific disciplines. And not only this,

17:58.080 --> 18:03.520
their experiments come together with AR in VR lessons and live online classes.

18:04.080 --> 18:09.600
The experiment I got this week is a gyroscope labelled for children aged five and up. It's

18:09.600 --> 18:15.760
a great opportunity to talk about color perception and the preservation of angular momentum and the

18:15.760 --> 18:21.600
solar system and why we always see the same side of the moon and why the length of the day depends

18:21.600 --> 18:27.440
on which way the wind blows. And well, I guess I got a little carried away there. I found the

18:27.440 --> 18:35.120
MelScience experiment extremely well designed and also high quality products. This is not

18:35.120 --> 18:41.040
cheap stuff that breaks when you touch it. It works like you expect it to work. And of course,

18:41.040 --> 18:46.720
we do have a special offer for viewers of this channel. You can get 50% off the first month

18:46.720 --> 18:52.880
for any MelScience subscription if you use our link in the info below or scan the QR code. So

18:52.880 --> 19:02.560
go check it out. Thanks for watching. See you next week.

