1
00:00:00,000 --> 00:00:05,280
You and I together are more than the sum of the parts, it's not just that I know some

2
00:00:05,280 --> 00:00:10,320
things you don't know and you know some things I don't know, like how to prevent hair from

3
00:00:10,320 --> 00:00:16,400
looking like sauerkraut. No, there's more to it. Maybe, hopefully, every once in a while,

4
00:00:16,400 --> 00:00:22,720
I point one of you into a new direction and you see something I couldn't see. Together,

5
00:00:22,720 --> 00:00:28,560
we're more intelligent than either of us alone. But collective intelligence has a flip side,

6
00:00:28,640 --> 00:00:34,240
collective stupidity. Sometimes, we're more stupid together than we are on our own.

7
00:00:34,960 --> 00:00:41,600
But what makes some groups of people intelligent and others stupid? That's what we'll talk about today.

8
00:00:45,920 --> 00:00:49,920
Everything around you is made of just a few types of elementary particles.

9
00:00:49,920 --> 00:00:55,200
Fundamentally, there's little difference between you and a cheesecracker. Both are made from up

10
00:00:55,200 --> 00:01:01,200
and down quarks with electrons held together by gluons and photons. If you combine many of those

11
00:01:01,200 --> 00:01:07,440
particles, you get increasingly complex systems. First, you get atoms, then molecules, and those

12
00:01:07,440 --> 00:01:14,000
molecules can combine to living beings, which can combine to societies. Each time you combine

13
00:01:14,000 --> 00:01:20,400
many constituents with their interactions, you can get completely new behavior. We call this behavior

14
00:01:20,400 --> 00:01:26,240
emergent. The difference between you and a cheesecracker isn't on the fundamental level.

15
00:01:26,240 --> 00:01:32,640
It's on the emergent level. You talk. A cheesecracker doesn't, or if it does,

16
00:01:32,640 --> 00:01:39,360
maybe cut back on those THC gummies. A simple example of emergent behavior is a wave. If you

17
00:01:39,360 --> 00:01:46,320
combine a lot of water molecules, you get waves. But waves don't exist for single molecules.

18
00:01:46,320 --> 00:01:51,920
This makes no sense. Waves only exist on the collective level. This is what it means for

19
00:01:51,920 --> 00:01:57,600
something to be emergent. It's a property that doesn't make sense on the fundamental level.

20
00:01:57,600 --> 00:02:03,680
It's not only particles that have emergent behavior. Living beings have, too. People can do

21
00:02:03,680 --> 00:02:10,480
a la-ola wave. Sheep flow like fluids through gates, and starlings do mesmerizing murmurations.

22
00:02:11,120 --> 00:02:17,040
And it's not only motion that living beings coordinate. They also coordinate the exchange

23
00:02:17,040 --> 00:02:23,120
of information. Fungi, for example, coordinate their growth to optimize the transport of

24
00:02:23,120 --> 00:02:29,840
nutrients. According to a study that was just published last year, they do this by using directional

25
00:02:29,840 --> 00:02:36,080
memory and collision-induced branching. This emergent behavior can even be exploited to get

26
00:02:36,080 --> 00:02:43,360
fungi to produce microscopic devices. For example, slime mold has been coaxed into growing a network

27
00:02:43,360 --> 00:02:49,680
that can transport dyes. In another experiment, the mold networks were connected to create

28
00:02:49,680 --> 00:02:56,160
logical circuits. While these are remarkable examples of collective behavior, scientists

29
00:02:56,160 --> 00:03:02,080
don't think that fungi are actually collectively intelligent, though I think that's exactly what

30
00:03:02,080 --> 00:03:07,280
the fungi want us to think. The animals that are probably best known for their collective

31
00:03:07,280 --> 00:03:13,760
intelligence are bees. They use motion, often called the bee dance, to share information about

32
00:03:13,760 --> 00:03:19,600
food sources, and they build hives together with a sophisticated division of labor to raise their

33
00:03:19,600 --> 00:03:26,400
young. We call them collectively intelligent not just because they are able to share information,

34
00:03:26,400 --> 00:03:32,880
but because they base decisions on their shared information they learn from each other.

35
00:03:32,880 --> 00:03:38,720
Ants show a similar intelligence in colonies. They communicate with each other by pheromones to

36
00:03:38,720 --> 00:03:45,600
signal where food can be found, and together they can defeat enemies much larger than themselves.

37
00:03:45,600 --> 00:03:51,760
But ants also show us the problems with collective intelligence. Ants try to follow each other's

38
00:03:51,840 --> 00:03:57,760
trails, and if some of them accidentally draw such a trail into a circle, they'll walk in

39
00:03:57,760 --> 00:04:04,960
circles until they die. This death spiral isn't collective intelligence, it's collective stupidity.

40
00:04:04,960 --> 00:04:10,480
It's what happens when a usually beneficial behavior goes badly wrong, and the same thing

41
00:04:10,480 --> 00:04:16,560
can happen for humans. Two heads are better than one, the saying goes, and sometimes it's true.

42
00:04:16,560 --> 00:04:21,840
Contestants of the quiz show, who wants to be a millionaire, can ask the audience to answer a

43
00:04:21,840 --> 00:04:28,240
limited number of questions for them. The audience then collectively votes on what they believe the

44
00:04:28,240 --> 00:04:34,000
correct answer to be. It doesn't always work, but statistically the audience gets it right

45
00:04:34,000 --> 00:04:40,800
91% of the time. But the wisdom of the crowds wasn't born with quiz shows. The idea is much

46
00:04:40,800 --> 00:04:47,120
older. It dates back to 1907 when Francis Galton went to a livestock exhibition in Plymouth

47
00:04:47,120 --> 00:04:53,040
and asked a group of about 800 people to guess the weight of an ox that was on display. He

48
00:04:53,040 --> 00:05:01,040
collected the results and calculated the average value of all estimates. The result was 1,207 pounds,

49
00:05:01,040 --> 00:05:07,760
almost exactly the right weight of 1,198 pounds. He published the results of this cutting-edge

50
00:05:08,000 --> 00:05:13,280
search in Nature. Those were the days, people. Broadly speaking, the reason large groups are

51
00:05:13,280 --> 00:05:18,800
better than individuals in answering simple questions is that some people in the group

52
00:05:18,800 --> 00:05:25,600
are knowledgeable about the topic, and the rest make errors that average out. This tells you

53
00:05:25,600 --> 00:05:30,720
that asking the audience isn't going to make you a millionaire if there are very few people who

54
00:05:30,720 --> 00:05:36,640
know the answer. But if you ever need to know the weight of an ox, then asking people at a 19th

55
00:05:36,640 --> 00:05:42,880
century farmer's market is a pretty good idea. There are more modern applications of this idea too.

56
00:05:42,880 --> 00:05:49,680
Average guesses of crowds are valuable information. It's why companies use crowdsourcing to collect

57
00:05:49,680 --> 00:05:56,000
feedback from some customers to make recommendations for others. It's why they solicit reviews to

58
00:05:56,000 --> 00:06:02,480
judge the quality of products and services. It's why YouTube wants to know how long you

59
00:06:02,480 --> 00:06:09,360
watch a video and whether you can be bothered to click like. This information is worth real money.

60
00:06:09,360 --> 00:06:15,040
Better still, people provide it for free. Another example of collective intelligence that you're

61
00:06:15,040 --> 00:06:20,800
all familiar with is Wikipedia. Yes, it has its problems, which is why I've given up

62
00:06:20,800 --> 00:06:26,960
correcting entries on quantum mechanics, but it's good enough to be useful just by collecting

63
00:06:26,960 --> 00:06:33,840
information. Indeed, Wikipedia usefully has an entry about the reliability of Wikipedia

64
00:06:33,840 --> 00:06:39,440
that collects studies on the subject. Results depend on topic, but by and large they found

65
00:06:39,440 --> 00:06:46,400
that Wikipedia tends to be as accurate as other encyclopedias, though it's frequently incomplete

66
00:06:46,400 --> 00:06:52,320
of completely omitting relevant information. A particularly impressive use of collective

67
00:06:52,320 --> 00:06:58,880
human intelligence are stock markets. Yes, they have a bad reputation, but that's because most

68
00:06:58,880 --> 00:07:04,880
of us only take note of the stock market when something goes wrong. Most of the time, however,

69
00:07:04,880 --> 00:07:11,440
the stock exchange guides investment to all our advantage. It works because the incentives of

70
00:07:11,440 --> 00:07:18,240
individual traders are aligned with the optimal distribution of resources, at least in theory.

71
00:07:18,240 --> 00:07:25,360
But this only works so long as we have appropriate regulations that govern trade. Like if you signed

72
00:07:25,360 --> 00:07:31,360
it, you're bound to it. If you agree to pay $44 billion for a social media platform and put your

73
00:07:31,360 --> 00:07:36,960
name on the paper, then you can't just change your mind the next day. You're also not allowed to

74
00:07:36,960 --> 00:07:43,120
trade insider information. Monopolies must be broken up, and there's a lot of other regulations on

75
00:07:43,120 --> 00:07:49,840
trade, because without them, the stock market wouldn't produce results that we want. It'd still

76
00:07:49,840 --> 00:07:56,560
collect information from individuals, but the outcome would no longer be what we desire, and that

77
00:07:56,560 --> 00:08:02,880
brings us to the problem. The problem is groups are only collectively intelligent when the mechanism

78
00:08:02,880 --> 00:08:09,200
to collect their information is carefully set up. If you crowdsource information from a group

79
00:08:09,200 --> 00:08:14,880
and want errors to average out, then the members of the group must put forward their private

80
00:08:14,880 --> 00:08:20,800
information independently of the others. This means most importantly, you shouldn't know what

81
00:08:20,800 --> 00:08:27,680
other people have said before you put forward your own guess. This is why they only show you poll

82
00:08:27,680 --> 00:08:35,360
results after you've voted yourself, and they ask the audience is set up that way too. But if that's

83
00:08:35,440 --> 00:08:41,280
not the case, if people know what others have said before making up their own mind, then the

84
00:08:41,280 --> 00:08:48,560
information can become systematically biased. This can lead to all kinds of trouble. The famous

85
00:08:48,560 --> 00:08:54,800
Ash experiment from the 1950s illustrates this. In this experiment, participants were assigned

86
00:08:54,800 --> 00:09:00,960
to groups with confederates of the experimenter and were asked to match the length of line zone

87
00:09:00,960 --> 00:09:08,160
cards with a comparison line. The confederates consistently gave obviously incorrect answers,

88
00:09:08,160 --> 00:09:13,760
but many participants agreed with them, even though their own perceptions told them that

89
00:09:13,760 --> 00:09:20,560
the answers were wrong. Now that the participants agreed with the wrong answers doesn't necessarily

90
00:09:20,560 --> 00:09:25,760
mean they believed them. If I was in a room with a group of people who insisted that the

91
00:09:25,760 --> 00:09:31,120
longer line is actually the shorter one, I'd also agree with them. I'd also keep my back

92
00:09:31,120 --> 00:09:37,760
to the wall and inch towards the exit. This is why, even though the Ash experiment has been

93
00:09:37,760 --> 00:09:44,160
reproduced in many different variants, just how to interpret it has remained somewhat controversial.

94
00:09:44,800 --> 00:09:50,720
People might have many reasons to agree with others, even if they don't believe them. But

95
00:09:50,800 --> 00:09:57,360
while the interpretation for why people act this way has remained unclear, there's little doubt

96
00:09:57,360 --> 00:10:04,080
that they do. And this is how information cascades work. An information cascade happens when

97
00:10:04,080 --> 00:10:10,080
individuals ignore the information that they privately hold and instead pass on the information

98
00:10:10,080 --> 00:10:16,960
they obtain from others for whatever reasons. Like all collective behaviors, this one isn't

99
00:10:16,960 --> 00:10:23,840
necessarily bad. In fact, it's usually beneficial. You've all seen information cascades on social

100
00:10:23,840 --> 00:10:30,080
media. In the early days of the Covid pandemic, information was sparse and we passed on what

101
00:10:30,080 --> 00:10:35,920
little we heard about symptoms and prevention. That's an example for how information cascades

102
00:10:35,920 --> 00:10:42,400
can be useful. But misinformation can spread the same way. This is, for example, how panic

103
00:10:42,400 --> 00:10:49,040
buying comes about. No one actually thinks they need 100 rolls of toilet paper. But if everyone

104
00:10:49,040 --> 00:10:55,760
else thinks they need it, maybe these people know something I don't know. Better safe than sorry.

105
00:10:55,760 --> 00:11:02,000
For crowd judgment to become systematically skewed, it isn't necessary that people completely

106
00:11:02,000 --> 00:11:07,760
ignore their information. It's sufficient already if they're influenced. And this happens

107
00:11:07,760 --> 00:11:13,120
everywhere around us. Multiple studies have found that information cascades happen for

108
00:11:13,120 --> 00:11:21,040
software adoption, online reading, product ratings and other everyday instances. Since I know you

109
00:11:21,040 --> 00:11:26,480
don't come here for the fluff, sociologists distinguish such information cascades from

110
00:11:26,480 --> 00:11:33,360
hurt behavior. Hurt behavior just means that individuals behave the same way, but not necessarily

111
00:11:33,360 --> 00:11:40,000
that in doing so they ignore their own information. In reality, we often see a mix of information

112
00:11:40,000 --> 00:11:46,320
cascades and hurt behavior. A particularly influential example of hurt behavior comes from

113
00:11:46,320 --> 00:11:52,880
an experiment by Milgram in the 1960s. Yes, that's the same Milgram who did the much discussed

114
00:11:52,880 --> 00:11:59,120
prisoner experiments. But this one was a little more innocent. He recruited a few people to stand

115
00:11:59,200 --> 00:12:05,840
in the street corner and point at nothing in the sky. Sure enough, other people came to join them

116
00:12:05,840 --> 00:12:12,320
to look at nothing. YouTube is basically built on this idea. If you come across a video that's

117
00:12:12,320 --> 00:12:18,560
been watched by a million people, you're more likely to watch it than if it had only 10 views.

118
00:12:18,560 --> 00:12:24,160
And most of the time, that's probably a good decision. But it also means that social media

119
00:12:24,160 --> 00:12:30,240
has a strong rich get richer trend, where you eventually end up with some people who are popular

120
00:12:30,240 --> 00:12:35,920
for being popular. They're interesting just because others think they're interesting.

121
00:12:35,920 --> 00:12:41,680
Many financial crashes are due to information cascades, and that's certainly nothing new.

122
00:12:41,680 --> 00:12:48,000
In the 18th century, the Scottish businessman John Law founded the Mississippi Company,

123
00:12:48,000 --> 00:12:53,200
whose purpose it was to develop the French territory near the Mississippi River.

124
00:12:53,200 --> 00:13:00,000
His stocks sold like warm bagels all over Europe, and Law was granted a monopoly on the trade.

125
00:13:00,800 --> 00:13:07,120
This was already a bad idea, but things got worse when the French government began printing

126
00:13:07,120 --> 00:13:14,000
more money so that everyone could buy more Mississippi stocks. Inevitably, eventually,

127
00:13:14,000 --> 00:13:19,600
investors realized there was no way the supposed wealth was ever to become real.

128
00:13:19,600 --> 00:13:23,840
They tried to get their money out of the bank, and the bubble collapsed,

129
00:13:23,840 --> 00:13:28,640
leaving many people bankrupt, economic growth seriously damaged,

130
00:13:28,640 --> 00:13:35,120
and trust in the financial system in shambles. The dot-com bubble of the late 1990s worked

131
00:13:35,120 --> 00:13:40,960
like that too. Everyone and their dog were investing into internet startups, even though

132
00:13:40,960 --> 00:13:46,720
no one really knew how those were supposed to eventually make money. The value of these

133
00:13:46,720 --> 00:13:52,880
stocks became incredibly overinflated, when those startups eventually went live,

134
00:13:52,880 --> 00:13:58,720
but created little to no revenue, the bubble burst. A more recent example of an information

135
00:13:58,720 --> 00:14:05,440
cascade was the 2008 financial crash. Banks were handing out mortgages to borrowers who couldn't

136
00:14:05,440 --> 00:14:11,680
reasonably be expected to pay them back. The banks then collected the mortgages and other loans

137
00:14:11,680 --> 00:14:18,720
into packages called securities that were sold to investors. When interest rates went up,

138
00:14:18,720 --> 00:14:24,960
it became clear that these mortgages and loans wouldn't be paid back. The value of the securities

139
00:14:24,960 --> 00:14:32,080
dropped rather suddenly and caused a big wave of bankruptcies. The 2008 financial crisis is

140
00:14:32,080 --> 00:14:38,640
particularly tragic in that it was preventable. Many people working in those banks knew that

141
00:14:38,640 --> 00:14:44,720
handing out those loans was a really bad idea that would eventually go wrong. But if they hadn't

142
00:14:44,720 --> 00:14:51,520
played along, they'd have lost their job, so the cascade rolled on. What do we learn from all that?

143
00:14:51,520 --> 00:14:57,840
Can we use some of this information to our own advantage? Well, yes. First of all, we learn that

144
00:14:57,840 --> 00:15:02,960
if you want to make good use of the collected intelligence of a group, you have to try and

145
00:15:02,960 --> 00:15:09,520
find a format in which everyone is comfortable coming forward with their information. And you

146
00:15:09,520 --> 00:15:17,120
need a way to prevent one person from being biased by another person to the extent possible. That is,

147
00:15:17,120 --> 00:15:22,000
of course, if coming to an intelligent decision is what you want in the first place. If you want

148
00:15:22,000 --> 00:15:27,760
the meeting to just be over quickly, then I suggest you ask the most aggressive dude for an opinion

149
00:15:27,840 --> 00:15:34,480
first and let him shout down anyone who dares disagree. Of course, making good use of collective

150
00:15:34,480 --> 00:15:42,400
intelligence is easier said than done. So let me mention two things that I found useful. First,

151
00:15:42,400 --> 00:15:48,880
we have a natural tendency to focus on the issues that come up more often. But those aren't

152
00:15:48,880 --> 00:15:55,200
necessarily the most important ones. This is why managers like to use tables to identify

153
00:15:55,200 --> 00:16:01,200
how important and urgent a problem is before spending time on it. I know that academics tend

154
00:16:01,200 --> 00:16:07,040
to find those tables somewhat silly, but it's indeed a way to prevent collective stupidity.

155
00:16:07,600 --> 00:16:12,800
The second useful thing to know is that just reminding people that their opinion might be

156
00:16:12,800 --> 00:16:19,920
biased can help to reduce the effect. That's for small teams whose decision making you can influence.

157
00:16:20,480 --> 00:16:26,400
But what about the large crowds that you find on social media? One thing that I mentioned already

158
00:16:26,400 --> 00:16:32,960
in my earlier video about social media is that just stepping back and thinking about what you're

159
00:16:32,960 --> 00:16:40,160
doing is a way to prevent regrets. And I formulate that so carefully because for some people maybe

160
00:16:40,160 --> 00:16:45,920
preventing a cascade of false information isn't what they want in the first place. But more

161
00:16:45,920 --> 00:16:52,560
interestingly, you can beat the crowd by being part of a group. I know this sounds somewhat

162
00:16:52,560 --> 00:16:59,680
contradictory, but let me explain. Numerous studies have found that small groups make better

163
00:16:59,680 --> 00:17:06,480
decisions than individuals on objective tasks. That is, tasks for which there is an answer that

164
00:17:06,480 --> 00:17:13,360
is either right or wrong, such as which way is the baggage claim left or right? Ask your family

165
00:17:13,360 --> 00:17:17,840
and you're less likely to find yourself next to a broken vending machine at the far end of the

166
00:17:17,840 --> 00:17:24,640
terminal. It's easy to see how this is useful on social media. Not sure whether that email is

167
00:17:24,640 --> 00:17:31,520
legit or a scam? Ask a few friends. Another useful thing to know is that the biggest problem for

168
00:17:31,520 --> 00:17:38,400
groups in getting things right is having members which are confident but often wrong. That's because

169
00:17:38,400 --> 00:17:44,960
the confident people make up their mind first and this then causes an information cascade which

170
00:17:44,960 --> 00:17:50,880
sways the less confident people. So be careful around confident people. It's not that they are

171
00:17:50,880 --> 00:17:58,080
necessarily wrong, but if they are, they amplify errors. Another issue is that we tend to overrate

172
00:17:58,080 --> 00:18:05,440
the relevance of our own opinion compared to that of others. It's called egocentric bias. A way to

173
00:18:05,440 --> 00:18:10,400
beat this issue is to hold back on forming an opinion, but of course that doesn't work if

174
00:18:10,400 --> 00:18:16,960
everyone does it. In summary, making intelligent decisions isn't easy and we're not naturally

175
00:18:16,960 --> 00:18:22,720
good at it. Whether a group of people makes intelligent or dumb decisions depends strongly on

176
00:18:22,720 --> 00:18:28,640
how the information is aggregated. Under certain circumstances, errors can amplify each other

177
00:18:28,640 --> 00:18:34,800
rather than canceling out. As you see, it isn't easy being sharper than a cheesecracker.

178
00:18:34,880 --> 00:18:40,640
Collective intelligence is all well and fine, but as they say, garbage in, garbage out. The most

179
00:18:40,640 --> 00:18:47,120
important thing you can do for your intelligence is to pick your input wisely and if you like quality

180
00:18:47,120 --> 00:18:53,440
input about scientific discoveries, I recommend you check out our sponsor Nautilus. Nautilus is a

181
00:18:53,440 --> 00:18:59,600
literary science magazine. They care about writing and they use it to its full potential.

182
00:18:59,600 --> 00:19:05,200
What I particularly like about them is that they cover all areas of science, from astronomy to

183
00:19:05,200 --> 00:19:11,600
economics, history, neuroscience, philosophy and physics. They'll keep you well informed on topics

184
00:19:11,600 --> 00:19:16,880
of current interest and it's always a pleasure to read. I've written several contributions for

185
00:19:16,880 --> 00:19:23,040
Nautilus myself about quantum gravity and black holes and other stuff that I normally write about,

186
00:19:23,040 --> 00:19:29,200
but what makes Nautilus so fun is the breadth of the topics they cover. A recent masterpiece is

187
00:19:29,200 --> 00:19:35,360
for example Amanda Gefter's essay What Plants Are Saying About Us. This will really make you

188
00:19:35,360 --> 00:19:40,880
question what you know about cognition. Or if you're interested in the intersection of art and

189
00:19:40,880 --> 00:19:47,280
science, they have a recent essay by Elena Kazamiya, The Enemy Made Visible, about a sculpture

190
00:19:47,280 --> 00:19:53,120
representing the COVID virus. The artist herself said it's by far the best she has read on the

191
00:19:53,120 --> 00:19:59,760
sculpture. So there you go, science, art, philosophy and great writing all in one place.

192
00:19:59,760 --> 00:20:05,040
You can join Nautilus as a digital only member or get a print subscription to receive six of

193
00:20:05,040 --> 00:20:11,120
their beautifully illustrated editions. In addition to full access to all the stories in Nautilus,

194
00:20:11,120 --> 00:20:17,840
members receive benefits like priority access to events, exclusive products and product discounts.

195
00:20:17,840 --> 00:20:22,800
For example, they recently partnered with the Australian streetwear company Jungles

196
00:20:22,800 --> 00:20:28,400
to draw attention to ocean science. The revenue goes to their Ocean Conservation Fund.

197
00:20:28,400 --> 00:20:34,480
Nautilus is really much more than just a magazine. And of course we do have a special offer for you.

198
00:20:34,480 --> 00:20:40,640
If you use our custom link Nautil.us slash Sabine, you'll get 15% off your membership

199
00:20:40,640 --> 00:20:45,680
subscription. So go and check this out. Thanks for watching. See you next week.

