1
00:00:00,000 --> 00:00:04,560
What should you do if you're spending too much time on social media, asking for a friend?

2
00:00:05,440 --> 00:00:10,320
Well, you try to convince yourself that social media is actually good for, well,

3
00:00:10,320 --> 00:00:15,680
something. It's gotta be good for something, right? But they say that social media increases

4
00:00:15,680 --> 00:00:20,400
polarization and gets you stuck in echo chambers full of fake news and so on.

5
00:00:21,200 --> 00:00:25,200
How bad is social media? That's what we'll talk about today.

6
00:00:26,000 --> 00:00:35,840
Social media has changed society profoundly. About 60% of the world's population now uses

7
00:00:35,840 --> 00:00:42,160
social media. It has made it vastly easier to find people all over the globe to connect with them

8
00:00:42,160 --> 00:00:50,240
and to get insulted by them. What does that do to society? It's complicated. American social

9
00:00:50,240 --> 00:00:56,240
psychologists Jonathan Haidt and sociologist Chris Bayer have compiled a public Google doc

10
00:00:56,240 --> 00:01:02,000
in which they collect references on questions such as, does social media make people angrier?

11
00:01:02,000 --> 00:01:07,920
And does social media create political echo chambers? The most relevant thing you learn

12
00:01:07,920 --> 00:01:13,200
from this document is that whatever your opinion, you can find a paper that supports it.

13
00:01:13,200 --> 00:01:18,400
Honestly, I began working on this video thinking it'll end up being one big shrug,

14
00:01:18,400 --> 00:01:24,160
because that's how sociology generally looks like to a physicist. But it turns out it isn't

15
00:01:24,160 --> 00:01:31,360
quite as bad. You just have to be really careful with phrasing the question. For example, you may

16
00:01:31,360 --> 00:01:37,280
remember the headlines claiming that fake news spreads faster than the truth. Then again,

17
00:01:37,280 --> 00:01:42,320
there were headlines saying that those headlines about fake news were themselves fake news.

18
00:01:42,880 --> 00:01:47,840
What is going on? Well, the original headlines were based on a 2018

19
00:01:47,840 --> 00:01:54,560
paper published in Science by researchers at MIT. The authors compared how true and false

20
00:01:54,560 --> 00:02:04,480
news stories spread on Twitter. They had a sample of about 126,000 news items from 2006 to 2017,

21
00:02:04,480 --> 00:02:11,520
tweeted by about 3 million people over 4.5 million times. So not a small study.

22
00:02:11,520 --> 00:02:16,560
These news items were classified as true or false according to certain fact-tracking

23
00:02:16,560 --> 00:02:22,000
organizations. The conclusion of the study was, in the authors' own words, that

24
00:02:22,000 --> 00:02:28,320
falsehood diffused significantly farther, faster, deeper and more broadly than the truth

25
00:02:28,320 --> 00:02:35,120
in all categories. The facts were most pronounced for false political news. And it wasn't a small

26
00:02:35,120 --> 00:02:41,440
difference. They found that it took two stories approximately six times as long as false stories

27
00:02:41,520 --> 00:02:49,280
to reach 1,500 people. But in 2021, other researchers pointed out that the 2018 paper

28
00:02:49,280 --> 00:02:55,280
looked at news that had been fact-checked by certain organizations, but that those organizations pay

29
00:02:55,280 --> 00:03:01,840
more attention to news that have already spread quite successfully. An article in Science then

30
00:03:01,840 --> 00:03:07,520
claimed that this means the original study had been debunked. This is why you've seen the

31
00:03:07,520 --> 00:03:13,760
headline saying news about fake news is fake news. That wasn't the end of the story. Because the

32
00:03:13,760 --> 00:03:19,600
authors of the original study then said they'd never claimed their study applies to all fake news,

33
00:03:19,600 --> 00:03:25,280
it had just been misreported. And the authors of the new study said they had never claimed the

34
00:03:25,280 --> 00:03:31,680
earlier study was wrong because they knew it had been misreported. Then the author of the Science

35
00:03:31,760 --> 00:03:37,360
News article who had claimed that the misreported fake news study was fake news

36
00:03:37,360 --> 00:03:43,920
apologized that his article had misreported this story. I hope that clarifies it. But wait,

37
00:03:43,920 --> 00:03:51,520
what does all of that mean now? Do fake news spread better or do they not? The answer is they do,

38
00:03:51,520 --> 00:03:57,680
but it turns out that the major difference between true news and fake news is that fake news spread

39
00:03:57,680 --> 00:04:03,840
to a larger audience. And since they appeal to a larger audience, they also spread faster.

40
00:04:04,400 --> 00:04:09,920
But if you compare true and false stories that have reached an audience of the same size,

41
00:04:09,920 --> 00:04:16,880
then the sharing pattern looks the same. This was the point of the 2021 paper. It's not like fake

42
00:04:16,880 --> 00:04:22,720
news networks have a different connectivity. The size of the audience that they attract is the

43
00:04:22,720 --> 00:04:28,480
major difference. And yes, that was strictly speaking only demonstrated for fake news stories

44
00:04:28,480 --> 00:04:34,400
that were fact-checked by certain organizations. One of the authors made this diagram to show

45
00:04:34,400 --> 00:04:40,000
the difference between what they said they did and what the headlines said they did. But the

46
00:04:40,000 --> 00:04:45,840
authors also say they're reasonably confident their finding will carry over to false news more

47
00:04:45,840 --> 00:04:52,480
generally, but that remains to be seen. I'm guessing there are people working on this as we speak.

48
00:04:52,480 --> 00:04:58,160
But the 2018 science paper made an interesting point that didn't spread widely. They found

49
00:04:58,160 --> 00:05:04,640
that bots accelerated the spread of true and false news equally. This means that if false news

50
00:05:04,640 --> 00:05:10,960
spreads better than the truth, that's because humans are more likely to spread false news.

51
00:05:10,960 --> 00:05:16,800
We can't blame it on the bots. The authors conjecture that the reason may be that people

52
00:05:16,800 --> 00:05:21,520
like novelty and it's easier to be original with something that's made up.

53
00:05:21,520 --> 00:05:27,680
Anecdotal evidence. I had my first encounter with fake news on Facebook in 2016 when Trump

54
00:05:27,680 --> 00:05:34,720
ran for president. It was a quote attributed to Trump from some anti-Republican Facebook page

55
00:05:34,720 --> 00:05:39,920
shared by an American friend. Several people pointed out that there was no evidence Trump

56
00:05:39,920 --> 00:05:46,240
actually said that. The guy who shared it reacted by saying it's funny even if it isn't true.

57
00:05:46,240 --> 00:05:51,440
And that's why false news spreads. We share it for reasons other than accuracy,

58
00:05:51,440 --> 00:05:56,720
because it's funny or upsetting or because it allows us to express our own opinion.

59
00:05:56,720 --> 00:06:02,000
Whether it's true doesn't really matter for that. The problem is that the next person who

60
00:06:02,000 --> 00:06:08,160
comes across shared fake news believes that the person who shared it believed it to be true

61
00:06:08,160 --> 00:06:13,600
and is therefore more likely to also believe it to be true. What can be done about it?

62
00:06:13,600 --> 00:06:19,040
It's easier than you think. Because most people agree that fake news is bad and they're actually

63
00:06:19,040 --> 00:06:25,200
quite good at spotting it. You just have to occasionally remind them to think before sharing.

64
00:06:25,200 --> 00:06:29,360
At least this was the conclusion of a paper published in Nature last year.

65
00:06:30,000 --> 00:06:36,720
The authors recruited about a thousand Americans and presented them with 36 actual news stories

66
00:06:36,720 --> 00:06:42,400
taken from social media. Half of the headlines were false and half were correct.

67
00:06:42,400 --> 00:06:47,360
Half favorable to Democrats and the other half favorable to Republicans.

68
00:06:47,360 --> 00:06:52,560
The participants were then asked to evaluate the accuracy of the news items.

69
00:06:53,200 --> 00:06:59,360
They quite reliably rated correct headlines as correct and false ones as false.

70
00:06:59,360 --> 00:07:05,360
And while they did rate headlines in favor of their own political orientation as correct more often

71
00:07:05,360 --> 00:07:11,440
than those in favor of the other camp, the partisan influence was much smaller than that of the

72
00:07:11,440 --> 00:07:17,920
actual accuracy of the headline. So the problem is not that we're just bad at spotting bad news.

73
00:07:18,640 --> 00:07:24,960
But the authors also found that whether the headlines were right or wrong had little effect

74
00:07:24,960 --> 00:07:32,080
on whether people intended to share a news item. They then encouraged people to consider the accuracy

75
00:07:32,080 --> 00:07:39,520
of the news item and afterwards asked again how likely they'd share it. This simple tactic led

76
00:07:39,520 --> 00:07:46,240
to a big reduction of the intention to share false news but didn't affect the intention to share real

77
00:07:46,240 --> 00:07:53,120
news. According to the paper, accuracy often has little effect on sharing because the social

78
00:07:53,120 --> 00:07:59,840
media context focuses users' attention on other factors such as the desire to attract and please

79
00:07:59,840 --> 00:08:05,440
followers and friends or to signal one's group membership. According to another paper that just

80
00:08:05,440 --> 00:08:12,000
appeared two months ago, the misinformation problem is particularly pronounced in the United States.

81
00:08:12,560 --> 00:08:18,400
The authors of the paper found that while people from the UK, Canada, Australia and New Zealand

82
00:08:18,400 --> 00:08:24,640
are exposed to misinformation on social media at about the same rate, Americans are three times

83
00:08:24,640 --> 00:08:30,480
more likely to share it. Earlier this year a review paper in the journal Nature Medicine

84
00:08:30,480 --> 00:08:35,760
looked at the spread of misinformation about public health in particular by reviewing one

85
00:08:35,760 --> 00:08:42,080
and a 23 papers. The two major conclusions that the author draws from his literature survey is that

86
00:08:42,080 --> 00:08:47,520
A. people are sometimes duped by misinformation just because they are distracted or not paying

87
00:08:47,520 --> 00:08:54,080
attention and B. some people believe in and share misinformation because it reinforces their beliefs.

88
00:08:54,640 --> 00:09:00,080
So that's consistent with what the other papers had found. As to what to do about it,

89
00:09:00,080 --> 00:09:05,440
one thing he suggests is also just reminding people to think about accuracy before sharing.

90
00:09:06,000 --> 00:09:11,440
Another interesting suggestion he has is to give people information about the tactics of

91
00:09:11,440 --> 00:09:17,840
misinformation spreaders with browser games. There are two of those games, one is called

92
00:09:17,840 --> 00:09:24,640
Go Viral and the other one Get Bad News. Studies have found that people who played these games

93
00:09:24,640 --> 00:09:30,400
were much better at spotting health-related misinformation. You can try them out yourself,

94
00:09:30,400 --> 00:09:36,080
links are in the info below. I think this is a really good idea and I'd like to have a game

95
00:09:36,080 --> 00:09:42,400
like this about physics please. So yes, social media spreads a lot of misinformation. The good

96
00:09:42,400 --> 00:09:49,040
side of social media is that it also seems to generally benefit information literacy. In 2018,

97
00:09:49,120 --> 00:09:55,840
a team of American researchers recruited almost 3,000 Americans. They offered half of them $20

98
00:09:55,840 --> 00:10:02,480
to deactivate their Facebook accounts for four weeks just prior to the 2018 midterm elections.

99
00:10:02,480 --> 00:10:07,520
Four weeks later, those who disconnected from Facebook were less able to correctly answer

100
00:10:07,520 --> 00:10:13,360
factual questions about recent news events. But they also reported increased well-being

101
00:10:13,360 --> 00:10:19,520
and less political polarization. Let's therefore look at what we know about polarization and

102
00:10:19,520 --> 00:10:25,760
echo chambers. If you watched a few of my videos on quantum mechanics, soon all your recommended

103
00:10:25,760 --> 00:10:32,480
videos will be about quantum mechanics. Suddenly, the whole world is quantum mechanics. Such an

104
00:10:32,480 --> 00:10:39,120
echo chamber seems to be an inevitable side effect of algorithms that want to help you find what

105
00:10:39,120 --> 00:10:46,240
you're interested in. And as a result, you get more of the same. This leads to the dreaded

106
00:10:46,240 --> 00:10:51,600
conspiracy rabbit holes that you fall into on YouTube as it happened to me when I was working

107
00:10:51,600 --> 00:10:57,920
on my video on flat earthers, though YouTube seems to have tweaked their algorithm since to prevent

108
00:10:57,920 --> 00:11:04,640
that from happening quite as easily. These more of the same algorithms help you make contact with

109
00:11:04,640 --> 00:11:10,800
people who think like yourself. So the idea that we live in echo chambers sounds plausible.

110
00:11:10,800 --> 00:11:18,080
But plausible ideas are the ones you should be most skeptical about. What does the data say?

111
00:11:18,080 --> 00:11:24,160
According to a 2021 paper by a group from the University of Oxford, echo chamber issues are

112
00:11:24,160 --> 00:11:30,240
real, but the problem has been hugely overstated. They looked at surveys from seven different

113
00:11:30,240 --> 00:11:36,560
countries in which people reported what news they typically consumed. Turns out, only about

114
00:11:36,560 --> 00:11:42,160
five percent of social media users are properly stuck in a political echo chamber in which they

115
00:11:42,160 --> 00:11:49,520
almost exclusively consume news from one political side. Though the numbers differ somewhat by country,

116
00:11:49,520 --> 00:11:55,440
the overall largest fraction of people in echo chambers is that of the American left.

117
00:11:55,520 --> 00:12:01,440
The previously mentioned Chris Bale is lead author of a 2018 paper about an experiment

118
00:12:01,440 --> 00:12:08,400
in which they try to get people out of their echo chambers. They surveyed about 1,500 Americans,

119
00:12:08,400 --> 00:12:14,640
about half Democrats and the other half Republicans, who visited Twitter at least three times each

120
00:12:14,640 --> 00:12:21,040
week. After one week of tracking, a randomly selected group of those people was offered

121
00:12:21,040 --> 00:12:27,200
$11 to follow a Twitter bot for one month, but they were not informed about the purpose of the study.

122
00:12:27,760 --> 00:12:33,280
The bot initially just tweeted landscape pictures, but then began tweeting opinions

123
00:12:33,280 --> 00:12:39,840
that promoted the participants' opposing political ideology. At the end of the month,

124
00:12:39,840 --> 00:12:47,200
the participants were surveyed again. Turned out that Republicans who followed a liberal Twitter bot

125
00:12:47,200 --> 00:12:54,000
became even more conservative. The more they were exposed, the larger the effect. For Democrats,

126
00:12:54,000 --> 00:12:59,600
the change was not statistically significant. By the way, Chris Bale is the director of an

127
00:12:59,600 --> 00:13:05,680
institute called the Polarization Lab that lets you check how deeply stuck in an echo chamber

128
00:13:05,680 --> 00:13:12,560
you are on Twitter. Turns out, rather unsurprisingly, I'm deeply stuck in a liberal camp. That's what

129
00:13:12,560 --> 00:13:18,400
you get when you mostly follow people with PhDs. The question whether social media increases

130
00:13:18,400 --> 00:13:24,000
polarization in society has been extensively studied, especially in the United States.

131
00:13:24,640 --> 00:13:29,840
The risk, sociologists say, is that social media makes it easier to find people whose

132
00:13:29,840 --> 00:13:36,640
opinions we like and we get encouraged by like-minded people to distance ourselves from the perceived

133
00:13:36,640 --> 00:13:44,560
enemy. Indeed, in 2018, a leaked internal presentation at Facebook warned senior executives

134
00:13:44,560 --> 00:13:50,880
that Facebook algorithms exploit the human brain's attraction to divisiveness and that,

135
00:13:50,880 --> 00:13:56,480
if left unchecked, the algorithm would feed users more and more divisive content

136
00:13:56,480 --> 00:14:01,600
in an effort to gain user attention and increase time on the platform.

137
00:14:01,600 --> 00:14:06,560
Now, it's quite well established among sociologists already that increased levels

138
00:14:06,560 --> 00:14:12,400
of polarization in society are associated with an erosion of constructive political debate,

139
00:14:12,400 --> 00:14:18,320
social trust and inter-party cooperation. The question we're interested in here is whether

140
00:14:18,320 --> 00:14:25,600
social media increases this polarization. In 2021, researchers from the UK and the US

141
00:14:25,600 --> 00:14:32,080
set out to answer the question whether out-group animosity drives engagement on social media.

142
00:14:32,640 --> 00:14:39,440
They analyzed almost 3 million Twitter posts by news media accounts and US congressional members.

143
00:14:39,440 --> 00:14:46,240
They found that posts about the political out-group were shared or retweeted about twice as often

144
00:14:46,240 --> 00:14:52,000
as posts about the in-group. And almost all of the posts about the other political camp were

145
00:14:52,000 --> 00:14:58,000
negative, leading to more negative engagement. Again, though, you have to be really careful

146
00:14:58,000 --> 00:15:03,680
to keep in mind what question a study was asking in the first place, because this finding doesn't

147
00:15:03,680 --> 00:15:10,720
necessarily mean that the negative engagement cost polarization to increase. It might just mean

148
00:15:10,720 --> 00:15:17,840
that social media is a good platform to live out your feelings. Just which way the causation goes

149
00:15:17,840 --> 00:15:24,000
is at the moment rather unclear. For one thing, a study from 2017 found that self-reported

150
00:15:24,000 --> 00:15:30,240
polarization is higher among elderly Americans who are less likely to be online to begin with.

151
00:15:30,240 --> 00:15:35,600
Another thing to keep in mind is that the USA isn't the only country in the world.

152
00:15:35,600 --> 00:15:42,000
A team of American researchers pointed out last year that while social media usage has increased

153
00:15:42,000 --> 00:15:48,400
worldwide, polarization has not. It has increased in the US, but in many other countries,

154
00:15:48,400 --> 00:15:56,320
polarization has in fact decreased. Indeed, a 2021 study among more than 3,000 Dutch citizens

155
00:15:56,320 --> 00:16:03,600
found that self-reported polarization correlates with social media use, but the causation goes from

156
00:16:03,600 --> 00:16:09,440
polarization to social media use and it depends on the platform you're using. At least in the

157
00:16:09,440 --> 00:16:15,360
Dutch sample, people who became politically more polarized spent more time on Facebook,

158
00:16:15,360 --> 00:16:20,800
but less time on Twitter. It's hard to interpret what this means, because God knows what's up

159
00:16:20,800 --> 00:16:27,200
with Dutch Twitter. But I think what we can take away from this is that the idea that more social

160
00:16:27,200 --> 00:16:34,000
media use increases polarization is almost certainly too simple to be correct. What happens

161
00:16:34,080 --> 00:16:40,400
depends both on cultural context and on platform design. So what do we learn from all this?

162
00:16:41,280 --> 00:16:47,200
Most obviously, we learn that this is a very active research area. I certainly hope that the

163
00:16:47,200 --> 00:16:54,160
results will eventually lead to better algorithms for social media. While we wait for that to happen,

164
00:16:54,160 --> 00:17:00,240
I think the best we can do is focus on the part that's in our hands, which is to decide what we

165
00:17:00,240 --> 00:17:06,640
pay attention to and what we share. What I have taken away from all those papers is that we have

166
00:17:06,640 --> 00:17:13,520
to be especially careful with headlines that upset us. Yes, they might get a lot of comments,

167
00:17:13,520 --> 00:17:19,040
but they might also spread misinformation and hate. So be careful out there.

168
00:17:19,040 --> 00:17:23,920
How has Sabina talking about nuclear power one day and about social media the other day?

169
00:17:23,920 --> 00:17:30,080
Is she omniscient? I'm afraid the answer is no. I work with several other people who help me

170
00:17:30,080 --> 00:17:35,840
sort through the scientific literature to bring the information to you. And the only reason this

171
00:17:35,840 --> 00:17:42,400
works is thanks to our sponsors. Today's episode was made possible by MelScience,

172
00:17:42,400 --> 00:17:47,840
which is a subscription service for science experiments. And I have to say, my family's

173
00:17:47,840 --> 00:17:52,960
having a lot of fun with their products. MelScience has experiments for children in

174
00:17:52,960 --> 00:17:58,000
different age categories and different scientific disciplines. And not only this,

175
00:17:58,080 --> 00:18:03,520
their experiments come together with AR in VR lessons and live online classes.

176
00:18:04,080 --> 00:18:09,600
The experiment I got this week is a gyroscope labelled for children aged five and up. It's

177
00:18:09,600 --> 00:18:15,760
a great opportunity to talk about color perception and the preservation of angular momentum and the

178
00:18:15,760 --> 00:18:21,600
solar system and why we always see the same side of the moon and why the length of the day depends

179
00:18:21,600 --> 00:18:27,440
on which way the wind blows. And well, I guess I got a little carried away there. I found the

180
00:18:27,440 --> 00:18:35,120
MelScience experiment extremely well designed and also high quality products. This is not

181
00:18:35,120 --> 00:18:41,040
cheap stuff that breaks when you touch it. It works like you expect it to work. And of course,

182
00:18:41,040 --> 00:18:46,720
we do have a special offer for viewers of this channel. You can get 50% off the first month

183
00:18:46,720 --> 00:18:52,880
for any MelScience subscription if you use our link in the info below or scan the QR code. So

184
00:18:52,880 --> 00:19:02,560
go check it out. Thanks for watching. See you next week.

