WEBVTT

00:00.000 --> 00:11.200
Yeah, so I'm John Rouser, I'm here to talk to you today about how humans see data.

00:11.200 --> 00:17.960
So this is a talk about visualization, which is taking raw data, numbers, and labels, and

00:17.960 --> 00:22.180
turning it into images for the purposes of communication.

00:22.180 --> 00:25.400
So that is a super important idea, so I will say it again.

00:25.400 --> 00:31.080
The act of creating a visualization is fundamentally an act of communication.

00:31.080 --> 00:38.080
Your goal with a visualization is to make new ideas appear in the heads of other people.

00:38.080 --> 00:41.600
More specifically, this is a talk about how to make better visualizations.

00:41.600 --> 00:47.320
I want to make you better at helping humans solve analytical problems quickly and accurately

00:47.320 --> 00:49.000
with visualization.

00:49.000 --> 00:54.560
I want to make you more effective at getting ideas into other people's heads.

00:54.560 --> 01:00.760
I feel like I should start by justifying visualization as a technique in the first place.

01:00.760 --> 01:03.760
Some people say, well, you don't really need visualizations because you can communicate

01:03.760 --> 01:11.200
just a tremendous amount of information using tables of data, like the stock listings page.

01:11.200 --> 01:13.000
So we'll do a little exercise.

01:13.000 --> 01:16.920
Here's some data, 24 points each with an X and a Y value.

01:16.920 --> 01:20.560
You are a very clever audience, so I'm certain that if I let you work at it for just a little

01:20.560 --> 01:25.400
bit, you'd figure out what the relationship is between X and Y, but if I just show you

01:25.400 --> 01:28.200
this, it's immediately obvious.

01:28.200 --> 01:34.880
So what just happened in your head happened in about 250 milliseconds without any conscious

01:34.880 --> 01:35.880
effort.

01:35.880 --> 01:39.760
In fact, there is no way that you can stop it from happening.

01:39.760 --> 01:45.280
Your eyes are a direct high bandwidth channel directly into your brain.

01:45.280 --> 01:50.920
You recognize this as a circle because of what's called pre-attentive processing, which

01:50.920 --> 01:56.240
is the subconscious accumulation of information from the environment.

01:56.240 --> 02:02.160
And the best visualizations, they make the information that you want to communicate available

02:02.160 --> 02:07.800
pre-attentively so that the user doesn't even have to think about what they are seeing.

02:07.800 --> 02:12.600
That's not always possible, but it's what you should be striving for.

02:12.600 --> 02:18.360
And the point, really, of this little exercise is that a graph is an encoding of data, which

02:18.360 --> 02:26.000
is to say that this table and this chart contain the exact same information, but in this display,

02:26.000 --> 02:31.200
the information is encoded in a way that makes it accessible to your incredibly powerful

02:31.200 --> 02:33.160
visual system.

02:33.160 --> 02:35.560
But not all encodings are equally good.

02:35.560 --> 02:40.560
So here I've mapped the X value to position on the horizontal axis and the Y value to

02:40.560 --> 02:42.320
position on the vertical axis.

02:42.320 --> 02:43.320
That choice has a name.

02:43.320 --> 02:45.600
This is called a scatter plot.

02:45.600 --> 02:47.360
But I could have made different choices.

02:47.360 --> 02:52.360
I could have taken the exact same data and treated it as an ordered sequence and then

02:52.360 --> 02:55.120
plotted two lines like this.

02:55.120 --> 03:00.280
So this is the same data, but now it's not at all obvious what the relationship is.

03:00.280 --> 03:06.040
So this is akin to a time series plot, which is ubiquitous, of course, in operational dashboards.

03:06.040 --> 03:11.880
The time series plot is incredibly useful if time is critical to the analysis, which

03:11.880 --> 03:14.360
may or may not be the case.

03:14.360 --> 03:16.800
It's certainly not the case here.

03:16.800 --> 03:22.800
So a good reason to visualize is that it can make salient features of the data available

03:22.800 --> 03:26.080
pre-attentively without any effort at all.

03:26.080 --> 03:29.600
Good visualizations optimize for the human visual system.

03:29.600 --> 03:33.640
If that's true, then it seems like it'd be really good to know how the human visual

03:33.640 --> 03:39.080
system works or more precisely how the human visual system decodes a graph.

03:39.080 --> 03:43.520
The good news is that there's been a lot of research into this question, and what I want

03:43.520 --> 03:48.040
to do now is I want to walk you through some of that research using a framework that comes

03:48.040 --> 03:49.600
from this book.

03:49.600 --> 03:54.600
So Bill Cleveland did some of the most important original research in this area, and this book

03:54.600 --> 03:58.440
summarizes that research into a coherent framework.

03:58.440 --> 04:05.000
One of the most important parts of that framework is what he calls the three visual operations

04:05.000 --> 04:06.760
of pattern perception.

04:06.760 --> 04:08.000
So I'll define them.

04:08.000 --> 04:09.600
First, detection.

04:09.600 --> 04:16.520
Detection is the visual recognition that a geometric object encodes a physical value.

04:16.520 --> 04:22.800
Assembly then is the grouping of detected graphical elements, and estimation is the visual

04:22.800 --> 04:28.920
assessment of the relative magnitude of two or more quantitative physical values.

04:28.920 --> 04:32.600
So I'm going to go through these, but I want to go through them in reverse order.

04:32.600 --> 04:38.480
I want to start with estimation, because that's where I see people making the most mistakes.

04:38.480 --> 04:43.880
So Cleveland lays out three different levels of estimation, where one level down in sort

04:43.880 --> 04:45.200
of this hierarchy of concepts.

04:45.200 --> 04:48.240
So we're going to talk about three different levels of estimation.

04:48.240 --> 04:51.720
The first is discrimination, where all you can do is just tell that two values are or

04:51.720 --> 04:53.040
aren't the same.

04:53.040 --> 04:56.000
Next is ranking, where you can impose some order on the values.

04:56.000 --> 05:00.840
And then the last one is ratioing, where you can say with some precision what the ratio

05:00.840 --> 05:03.920
between two values is.

05:03.920 --> 05:08.320
Notice that all of these involve comparison.

05:08.320 --> 05:15.640
Efficient comparison between different data points is nearly always the point of a visualization.

05:15.640 --> 05:21.520
If I just wanted to report the individual data without comparison or any further analysis,

05:21.520 --> 05:25.400
then a simple table would be completely fine or maybe better just like a USB key with the

05:25.400 --> 05:26.880
data.

05:26.880 --> 05:31.120
But when you're trying to think with data, you are always making comparisons.

05:31.120 --> 05:37.040
As Edward Tufty says, a famous writer about information visualization, he says at the

05:37.040 --> 05:42.600
heart of quantitative reasoning is a single question as compared to what.

05:42.600 --> 05:47.320
So when we're making a visualization, what we want to do is to get as far down this list

05:47.320 --> 05:52.280
as possible with efficiency and with accuracy.

05:52.280 --> 05:57.160
Some of the best research on this question is summarized in this amazing paper by Cleveland

05:57.160 --> 05:58.160
and McGill.

05:58.160 --> 06:01.000
You can get this online for free.

06:01.000 --> 06:07.040
This paper is a treasure trove of insight, but I want to focus on one little table.

06:07.040 --> 06:08.760
I see people like getting out their phones.

06:08.760 --> 06:09.760
They want to take a photo.

06:09.760 --> 06:12.720
I'm going to tweet slides afterwards, so don't worry about it.

06:12.720 --> 06:13.720
Sorry.

06:13.720 --> 06:15.480
Look, there it is.

06:15.480 --> 06:16.480
You want to take a photo?

06:16.480 --> 06:17.480
Awesome.

06:17.480 --> 06:23.120
So I want to focus on this one little table buried on the third page of the report because

06:23.120 --> 06:28.000
I want to claim that this table might be the single most important thing that you can possibly

06:28.000 --> 06:29.920
know about visualizing data.

06:29.920 --> 06:36.080
What this table shows is how accurate humans are at estimating quantities that are encoded

06:36.080 --> 06:37.080
in different ways.

06:37.080 --> 06:40.040
So here's the table in a more readable format.

06:40.040 --> 06:45.280
What Cleveland is doing here is he's giving us seven different ways to encode a quantitative

06:45.280 --> 06:50.440
value, and he ranks them from most effective to least effective.

06:50.440 --> 06:54.720
We are most accurate when decoding position on a common scale and least accurate when

06:54.720 --> 06:56.560
decoding hue.

06:56.560 --> 07:01.200
And what you should do when you're making a graphic is think, what is the most important

07:01.200 --> 07:04.560
comparison that I want the user to make?

07:04.560 --> 07:09.320
Whatever that is, you should encode that thing using position on a common scale.

07:09.320 --> 07:14.060
As you pack more and more different dimensions of data into the same chart, you have to go

07:14.060 --> 07:17.060
further down this list, you're just forced to.

07:17.060 --> 07:21.140
If you have several different comparisons, all of which are very important, and you might

07:21.140 --> 07:23.460
have to make multiple charts.

07:23.460 --> 07:27.660
What I want to do now is I want to take you through these in order, in order to give you

07:27.660 --> 07:30.180
an intuitive sense for how they work.

07:30.180 --> 07:33.340
So I'll start with hue, and we'll work our way up.

07:33.340 --> 07:38.460
Before I can even get started, I need to talk about how color works because human color

07:38.460 --> 07:41.220
perception is incredibly complicated.

07:41.220 --> 07:46.940
As Tamara Munzner says, the first rule of color is do not talk about color because color

07:46.940 --> 07:48.620
is not a single thing.

07:48.620 --> 07:50.620
Color is really three things.

07:50.620 --> 07:55.260
There are three channels that are encoded in any one color.

07:55.260 --> 07:59.860
The first is luminance, which is roughly how bright a color is.

07:59.860 --> 08:04.780
Contrast in luminance is really important for edge detection in complex scenes.

08:04.780 --> 08:10.020
The next is saturation, which is roughly how colorful the color is.

08:10.060 --> 08:14.380
Low saturation is like a muted pink, high saturation is a bright red.

08:14.380 --> 08:19.620
And finally, hue, which is what we mean when we say color in common speech.

08:19.620 --> 08:22.780
So here's a chart that encodes information using hue.

08:22.780 --> 08:29.420
So this is a data set of 32 cars that were tested in a 1974 issue of Motor Trend Magazine.

08:29.420 --> 08:36.940
What I've plotted here is the fuel efficiency in miles per gallon, or MPG, so a higher number

08:36.940 --> 08:39.660
is better, more miles per gallon.

08:39.700 --> 08:42.820
Let me say up front that this is a terrible plot.

08:42.820 --> 08:48.340
You would never make a plot like this in real life, but let's try to extract some value

08:48.340 --> 08:49.340
from it.

08:49.340 --> 08:52.700
So the first of Cleveland's estimation tasks is discrimination.

08:52.700 --> 08:57.380
So what do you think of these two, the Firebird versus the Merck 450 SLC, are they the same

08:57.380 --> 08:58.380
or different?

08:58.380 --> 08:59.380
Different.

08:59.380 --> 09:00.460
Very easy, right?

09:00.460 --> 09:01.660
Because they're right next to each other.

09:01.660 --> 09:02.780
What if I make it harder?

09:02.780 --> 09:07.660
The Merck 450 SLC versus the Dodge Challenger, same or different?

09:08.660 --> 09:12.180
Okay, I find it impossible to tell.

09:12.180 --> 09:19.060
People vary in their ability to perceive differences in hue, so I can't really tell.

09:19.060 --> 09:22.300
Maybe you can tell if you can, you're better at this than me.

09:22.300 --> 09:25.660
I think they're actually different, very, very slightly different.

09:25.660 --> 09:27.740
Okay, so much for discrimination.

09:27.740 --> 09:28.740
What about ranking?

09:28.740 --> 09:29.740
Here's an easy one.

09:29.740 --> 09:35.060
The Toyota Corolla versus the Chrysler Imperial, which has better fuel efficiency, right?

09:35.060 --> 09:36.460
You can do that just with domain knowledge.

09:36.460 --> 09:39.340
You don't even need the chart.

09:39.340 --> 09:45.260
You can do that accurately, but it's slow because it requires a table lookup.

09:45.260 --> 09:51.260
You have to refer to the legend to recall if red is at the high end of the scale or at

09:51.260 --> 09:52.740
the low end of the scale.

09:52.740 --> 09:57.420
And you have to refer to the scale because hue does not have a natural ordering.

09:57.420 --> 10:02.820
The process of table lookup is really expensive and you should seek to avoid it as much as

10:02.820 --> 10:03.820
possible.

10:04.580 --> 10:08.540
As soon as we talk about ranking, it becomes obvious that there is something else dreadfully

10:08.540 --> 10:15.540
wrong with this chart, aside from using hue as a scale to encode a continuous variable.

10:15.540 --> 10:19.860
How have I ordered the models on the Y-axis alphabetical?

10:19.860 --> 10:20.860
Why?

10:20.860 --> 10:21.980
Because that's the default.

10:21.980 --> 10:23.540
Is that a good encoding?

10:23.540 --> 10:24.540
No.

10:24.540 --> 10:27.180
It turns out that this is an important observation.

10:27.180 --> 10:33.220
The default ordering of a categorical variable is almost never the right ordering for a

10:33.260 --> 10:34.500
visualization.

10:34.500 --> 10:38.820
You almost always want to re-rank a categorical variable somehow.

10:38.820 --> 10:42.740
In our case, of course, the best way is by efficiency.

10:42.740 --> 10:47.900
Now, ranking any two is trivial, provided that you can discriminate between them, which

10:47.900 --> 10:49.780
still is not easy.

10:49.780 --> 10:54.420
For example, I believe all of these have slightly different efficiency, but I find it really

10:54.420 --> 10:56.420
hard to tell from this plot.

10:56.420 --> 10:58.940
We'll leave hue behind.

10:58.940 --> 11:00.180
We'll move up.

11:00.220 --> 11:07.060
Three-dimensional volume, density of points or objects more generally, and color saturation

11:07.060 --> 11:09.700
are all roughly tied at the next level.

11:09.700 --> 11:12.100
I'm going to talk about saturation.

11:12.100 --> 11:15.140
Saturation has two big advantages over hue.

11:15.140 --> 11:19.260
The first is that there is a natural ordering to saturation.

11:19.260 --> 11:24.140
If I gave you this, but then I hid the legend and I asked you to put these in order with

11:24.140 --> 11:29.540
no other information, you would naturally and easily arrive back at this.

11:29.580 --> 11:34.660
The other big advantage saturation has over hue is that your eye can give a quantitative

11:34.660 --> 11:36.900
estimate of saturation.

11:36.900 --> 11:43.500
If I extend the lower end of the scale down to zero, you can start to do the third of

11:43.500 --> 11:45.500
Cleveland's levels of estimation.

11:45.500 --> 11:47.860
You can do ratioing.

11:47.860 --> 11:53.700
I can ask you, how much more fuel efficient is the Toyota Corolla at the top compared

11:53.700 --> 11:55.500
to the Cadillac at the bottom?

11:55.500 --> 11:56.980
Is it two times more efficient?

11:56.980 --> 11:59.180
Is it three times more efficient?

11:59.180 --> 12:00.740
Maybe it's four.

12:00.740 --> 12:05.020
You should be able to come up with a guess without even referring to the legend.

12:05.020 --> 12:08.380
That guess will be reasonably accurate.

12:08.380 --> 12:13.140
Maybe you can get one significant figure out of that guess.

12:13.140 --> 12:16.180
Moving up one more, come to two-dimensional area.

12:16.180 --> 12:20.900
Here's the same data with fuel efficiency encoded by two-dimensional area.

12:20.900 --> 12:26.420
This is the first time where accurate ratioing becomes possible.

12:26.420 --> 12:30.500
If I ask again, how much more fuel efficient is the Corolla at the top compared to the

12:30.500 --> 12:35.460
Continental, expressed as a multiple of the Continental?

12:35.460 --> 12:39.260
This is the first time that you'll be able to give an answer with any confidence without

12:39.260 --> 12:42.140
referring to the legend.

12:42.140 --> 12:48.460
It's important to note that this is only possible because I have scaled the area of each point.

12:48.460 --> 12:52.420
Not the radius, not the diameter, but the area with efficiency.

12:52.420 --> 12:56.380
Humans perceive the size of something according to its area.

12:56.420 --> 13:00.580
Also, because the points are scaled so that a point representing zero miles per gallon

13:00.580 --> 13:06.300
would have zero area, I make these points because two-dimensional area is an extremely

13:06.300 --> 13:09.780
useful encoding for data.

13:09.780 --> 13:15.140
You can put another dimension of your data on area, but you have to do it carefully,

13:15.140 --> 13:17.940
otherwise you're going to mislead.

13:17.940 --> 13:20.340
This particular plot, again, is pretty strange.

13:20.340 --> 13:25.100
You'd never make a plot exactly like this in real life, but we use area to encode information

13:25.100 --> 13:29.660
all the time, like in a bubble chart, where the area of each point usually encodes the

13:29.660 --> 13:33.060
amount of data that's in that point.

13:33.060 --> 13:35.700
Moving up one more level, we come to angle.

13:35.700 --> 13:38.020
Here's the same efficiency data encoded with angle.

13:38.020 --> 13:43.900
Again, it is super important that I have set this plot up so that a horizontal line means

13:43.900 --> 13:48.700
zero miles per gallon and a vertical line is the maximum of the data set.

13:48.700 --> 13:53.300
Because I have done that, it's trivial for you to say that the Cadillac is about a third

13:53.300 --> 13:58.500
as efficient as the Corolla, and about half as efficient as the Dotson 710.

13:58.500 --> 14:00.660
Again, this is a silly chart.

14:00.660 --> 14:05.700
You would never make a chart using angle in this way, but we use angle to encode information

14:05.700 --> 14:10.580
all the time, like here's a chart of world population broken out by continent.

14:10.580 --> 14:15.940
Here because population is plotted directly on the y-axis, the rate of population growth

14:15.940 --> 14:22.060
is encoded as the angle of the line, and it's hard to see whether, say, Oceania at the bottom

14:22.060 --> 14:26.780
is growing faster or slower than Asia at the top.

14:26.780 --> 14:28.340
Next up, length.

14:28.340 --> 14:29.660
Here's data encoded as length.

14:29.660 --> 14:33.580
I had to jitter the bars so that you couldn't compare against a common baseline.

14:33.580 --> 14:35.780
That's the next level up in the hierarchy.

14:35.780 --> 14:41.540
With length, you're getting more accurate ratioing than with area or with angle.

14:41.540 --> 14:46.460
For example, the valiant here, you can estimate that it's one and a half times more efficient

14:46.460 --> 14:48.180
than the Cadillac.

14:48.180 --> 14:52.300
Not much precision would have been difficult with area or with slope.

14:52.300 --> 14:57.860
We still have not achieved very much improvement in discrimination or in ranking, so take,

14:57.860 --> 14:59.340
for example, this pair.

14:59.340 --> 15:03.220
Even though they are right next to each other, it is hard to tell if the Maserati Bora is

15:03.220 --> 15:07.660
better or worse than the Chrysler Imperial or if they're tied.

15:07.660 --> 15:10.940
I think they're different, but it's hard to tell.

15:10.940 --> 15:14.740
You might say that this is a weird plot, but we use length of unaligned segments to encode

15:14.740 --> 15:21.420
information all the time, like in this plot, where error bars use length to encode the

15:21.420 --> 15:24.340
size of a confidence interval.

15:24.340 --> 15:27.940
Moving on, we'll come to position on identical but nonaligned scales.

15:27.940 --> 15:32.020
Here's the fuel efficiency data encoded with identical but nonaligned scales.

15:32.020 --> 15:38.540
You can decode position much more accurately than anything that has come before.

15:38.540 --> 15:44.420
Taking the last example, you can just barely see that the Chrysler Imperial on the left

15:44.420 --> 15:47.900
is ever so slightly worse than the Maserati Bora.

15:47.900 --> 15:53.580
Again, this is a dumb chart that no one would ever make, but we use identical but nonaligned

15:53.580 --> 15:58.260
scales all the time, like in this pair of time series, where the time axis is identical

15:58.260 --> 16:01.460
between the panels, but the y-axis varies.

16:01.460 --> 16:06.140
Finally, we've arrived at the top, position on a common scale.

16:06.140 --> 16:10.060
This, of course, is the way I should have plotted the data from the very beginning.

16:10.060 --> 16:15.420
In this display, you can take the ratio of any two points to two significant digits with

16:15.420 --> 16:20.220
very little effort, and both ranking and discrimination are trivial.

16:20.220 --> 16:23.580
Here's the Maserati Bora versus the Chrysler Imperial comparison.

16:23.580 --> 16:27.220
You can easily read off that the Bora gets 15 miles per gallon and the Imperial gets

16:27.220 --> 16:30.140
about 14 and a half.

16:30.140 --> 16:34.420
Now that you understand these, and you have a little bit of intuition for how they work

16:34.420 --> 16:40.020
and what are the benefits of each, we can look at the implications of this research.

16:40.980 --> 16:43.380
Here's an observation.

16:43.380 --> 16:46.700
Stacked anything is nearly always a mistake.

16:46.700 --> 16:52.140
Here's a data set of, this is a chart from a data set of 54,000 diamonds.

16:52.140 --> 16:57.060
Each row in the data set encodes the cut, the color, the clarity, the price, other data,

16:57.060 --> 16:59.260
like the table size, and so on.

16:59.260 --> 17:05.820
This chart is just showing the count of diamonds in each combination of cut and clarity.

17:05.820 --> 17:12.460
Because we have only two dimensions of space, an X and a Y axis, we have to encode cut with

17:12.460 --> 17:13.580
another channel.

17:13.580 --> 17:14.580
We pick hue.

17:14.580 --> 17:17.940
It's totally fine to use hue to encode categorical data.

17:17.940 --> 17:20.060
It's very effective at that.

17:20.060 --> 17:24.700
Then we stack the bars for each type of cut on top of each other.

17:24.700 --> 17:26.860
This is called a stacked bar chart.

17:26.860 --> 17:29.860
This chart is showing two different sets of data.

17:29.860 --> 17:36.940
First it's showing the number of diamonds in each clarity class, so 13,000 in SI1, 12,000

17:36.940 --> 17:39.420
in SI2, and so on.

17:39.420 --> 17:44.020
That's encoded by the total length of the bars, which you can read off of a common scale,

17:44.020 --> 17:45.980
the Y axis.

17:45.980 --> 17:51.740
The second kind of information is the count of diamonds that are in each cut clarity combination.

17:51.740 --> 17:55.140
That is encoded by the length of each of the little colored bars.

17:55.140 --> 17:59.020
So tell me, the premium cut is the blue bars.

17:59.020 --> 18:04.060
Are there more SI1 premium diamonds or more SI2 premium diamonds?

18:04.060 --> 18:05.860
It's hard to tell.

18:05.860 --> 18:07.700
I can't tell.

18:07.700 --> 18:12.220
If you care about communicating the count in each combination of cut and clarity, then

18:12.220 --> 18:16.300
encode that information using position on a common scale.

18:16.300 --> 18:21.980
This is called a parallel coordinates plot, and it is nearly always preferable to a stacked

18:21.980 --> 18:23.460
bar chart.

18:23.460 --> 18:28.660
With this, I'll admit, you have lost some summary information about how many diamonds

18:28.660 --> 18:33.540
are in each clarity group, but if you want that, then just use two charts.

18:33.540 --> 18:37.700
So often what people want to do is they want to cram all of the information into a single

18:37.700 --> 18:42.380
chart, and they end up with one chart that doesn't do anything well.

18:42.380 --> 18:47.020
If you don't have space for two charts, then you just have to decide which is the more

18:47.020 --> 18:50.820
important comparison and leave out the other views.

18:50.820 --> 18:55.620
Editing is hard, but it's what your users want you to do.

18:55.780 --> 19:01.500
Stacking forces the reader to decode lengths, not position on a common scale.

19:01.500 --> 19:06.260
Decoding lengths is less efficient and less precise than position on a common scale, which

19:06.260 --> 19:10.260
is why stacked charts are almost always a mistake.

19:10.260 --> 19:12.380
There are exceptions to every rule.

19:12.380 --> 19:15.100
This graph has a point.

19:15.100 --> 19:22.460
Android growth is dominating everything else, including iOS, despite what you might think

19:22.460 --> 19:25.380
if you're wealthy and you work in tech.

19:25.380 --> 19:33.820
You could communicate this exact same information with a line chart, but this has more impact.

19:33.820 --> 19:37.020
Next observation, pie charts.

19:37.020 --> 19:38.700
Pie charts are always a mistake.

19:38.700 --> 19:40.620
Here's Coda Hale on the topic.

19:40.620 --> 19:46.260
The information visualization equivalent of a roofing hammer to the frontal lobe.

19:46.260 --> 19:53.860
They are as professional as a pair of assless chaps.

19:53.860 --> 19:54.860
I'm preaching to the choir.

19:54.860 --> 19:59.700
I suspect you have doubtless heard people rail against pie charts before telling you not

19:59.700 --> 20:02.380
to use them, but now you know why.

20:02.380 --> 20:04.340
You know why pie charts are bad.

20:04.340 --> 20:10.180
Pie charts encode quantitative data using angle, which is fourth on our list.

20:10.180 --> 20:12.380
You can do better than angle.

20:12.380 --> 20:16.620
Worse, most pie charts only show a tiny amount of data.

20:16.620 --> 20:23.020
Here's a topical chart from the US presidential election.

20:23.020 --> 20:28.780
This graph appeared on Politico, showing who won the third US presidential debate.

20:28.780 --> 20:32.820
This entire graphic encodes one number.

20:32.820 --> 20:33.820
One number.

20:33.820 --> 20:39.820
If you click on the little among Democrats tab at the top, you get another number.

20:39.820 --> 20:41.420
So exciting.

20:41.420 --> 20:48.060
For a solution, we will turn to Tufti, who says pie charts should never be used.

20:48.060 --> 20:52.900
Instead, he says, tables are preferable to graphics for small data sets.

20:52.900 --> 20:56.100
This is so much better than a pie chart.

20:56.100 --> 21:01.020
If you have more than a handful of categories, a dot plot is an excellent alternative.

21:01.020 --> 21:08.940
And if you have a lot of categories for the love of all that is holy, do not do this.

21:08.940 --> 21:14.980
Find some sensible way to summarize or model your data.

21:14.980 --> 21:17.020
All good pie charts are jokes.

21:17.020 --> 21:24.020
I will bring you my favorite from all time.

21:24.020 --> 21:30.860
Okay, onward.

21:30.860 --> 21:34.500
Next observation is that comparison is trivial on a common scale.

21:34.500 --> 21:40.100
So I showed this plot briefly a couple of minutes ago, but let's look at it more closely.

21:40.100 --> 21:45.300
I'm plotting two metrics here, CPU utilization and service latency separately in different

21:45.300 --> 21:46.540
panels.

21:46.540 --> 21:50.660
Mostly all operational dashboards do this, and they do it mainly for the convenience

21:50.660 --> 21:54.900
of the person who made the chart or the convenience of the person who wrote the operational dashboard

21:54.900 --> 21:58.980
software and the user is just forced into this choice.

21:58.980 --> 22:00.940
So what's it mean for the user?

22:00.940 --> 22:04.700
The two spikes, are they aligned in time or not?

22:04.700 --> 22:09.620
They're not, and it's easy for you to tell, but it did require some effort.

22:09.620 --> 22:14.740
Because I'm using nonaligned scales, it requires two table lookups and a slot in your working

22:14.740 --> 22:15.860
memory.

22:15.860 --> 22:16.900
So we can do better than that.

22:16.900 --> 22:21.420
If you put them on a common scale, the top of the hierarchy, it's trivial to see that

22:21.420 --> 22:26.340
the spikes don't line up, but now you see why people don't typically mix metrics on

22:26.340 --> 22:31.380
a common time scale because the magnitude of the two series is different.

22:31.380 --> 22:37.220
But there is an easy, nearly universal fix, which is to standardize, that's a technical

22:37.220 --> 22:43.500
term from statistics, to standardize each series by subtracting its mean and dividing

22:43.500 --> 22:45.380
by its standard deviation.

22:45.380 --> 22:50.780
And now we have this nice plot that lets us compare the anomalies in the two series.

22:50.780 --> 22:54.260
And it's true that we have lost some information here.

22:54.260 --> 22:57.820
We don't know the absolute magnitude of the spikes anymore.

22:57.820 --> 23:04.380
So if all we had was this view, we wouldn't know what they were in an absolute sense.

23:04.380 --> 23:09.900
And it matters if the spike in CPU is to 12% or to 100%.

23:09.900 --> 23:14.940
So the truth is that sometimes you want this plot and sometimes you want this one, and

23:14.940 --> 23:20.140
you can't know in advance, and so your monitoring software should make it easy for you to flip

23:20.140 --> 23:23.980
back and forth between these two different views.

23:23.980 --> 23:29.500
Which brings up an aside, which is that the dashboard metaphor is fundamentally flawed.

23:29.500 --> 23:36.580
The systems that we work with are so complex, so much more complex than operating a car,

23:36.580 --> 23:43.060
that no static set of metrics could possibly work, like even half the time.

23:43.060 --> 23:48.580
This is a truth that I think our industry really has yet to come to grips with.

23:48.580 --> 23:54.580
The affordances for transforming, rearranging, and replotting data are all clunky and slow

23:54.580 --> 23:58.100
in the monitoring software that I'm aware of anyway.

23:58.100 --> 24:03.780
I think this is a deep weakness in our tool chain, and it is a huge opportunity for people

24:03.780 --> 24:06.820
who are motivated to fix it.

24:06.820 --> 24:10.220
As long as we're talking about weaknesses in modern monitoring software, let's talk

24:10.220 --> 24:12.220
about time series plots.

24:12.220 --> 24:17.700
With most monitoring software, all you can do is plot time series like these, where time

24:17.700 --> 24:23.020
marches from left to right, and some metric is plotted as a line.

24:23.020 --> 24:26.500
Imagine that this is the latency of some service and its dependency.

24:26.500 --> 24:30.420
So all you can say about these two is that they go up and down together.

24:30.420 --> 24:32.220
But there's more to their relationship than that.

24:32.220 --> 24:36.900
If I put them together so that the metrics are on a common scale, you get a hint maybe

24:36.900 --> 24:37.900
at what's going on.

24:37.940 --> 24:43.140
But the observation here is that a scatter plot will show the relationship directly.

24:43.140 --> 24:49.740
If you want to show how two metrics move in relation to one another, there is no better

24:49.740 --> 24:51.940
tool than a scatter plot.

24:51.940 --> 24:56.380
So what I've done here is just take each point in time as an observation and put the service

24:56.380 --> 24:59.780
on the y-axis and its dependency on the x-axis.

24:59.780 --> 25:02.940
I can make it look nicer if I put a smoother over it.

25:02.940 --> 25:05.260
And now you see that the relationship isn't linear.

25:05.260 --> 25:10.500
The service's latency goes up faster than the dependency, which means that it's somehow

25:10.500 --> 25:12.300
bottlenecked on that dependency.

25:12.300 --> 25:16.420
And the dependency represents a key scaling risk for that service.

25:16.420 --> 25:21.300
I don't know of monitoring software that makes it easy, that helps you to make scatterplots

25:21.300 --> 25:22.300
like this.

25:22.300 --> 25:25.620
And again, this just feels like a missed opportunity.

25:25.620 --> 25:30.300
My last observation in this section is that growth charts usually aren't.

25:30.300 --> 25:34.180
So here's a plot of world population again by continent over time.

25:34.580 --> 25:39.300
The y-axis is on a log scale because population growth is exponential.

25:39.300 --> 25:45.020
You have almost certainly seen a chart similar to this showing daily active users or monthly

25:45.020 --> 25:48.860
active users or something like that relevant to your business.

25:48.860 --> 25:51.740
These charts are typically presented as growth charts.

25:51.740 --> 25:53.980
Look how we're growing, isn't it lovely?

25:53.980 --> 25:57.860
You can barely tell anything about growth from this chart.

25:57.860 --> 26:04.220
As I asked before, which is growing faster in the 1960s, Oceania or Asia?

26:04.220 --> 26:06.620
How about in the 1980s?

26:06.620 --> 26:09.060
It is impossible to tell.

26:09.060 --> 26:11.820
Growth is the derivative of this curve.

26:11.820 --> 26:16.660
If you care about growth, then plot growth directly on a common scale.

26:16.660 --> 26:20.180
So here's the same data as before, but now I've plotted the percentage growth directly

26:20.180 --> 26:21.780
on the y-axis.

26:21.780 --> 26:26.420
And now it's obvious that Asia in orange has been growing faster than Oceania in blue,

26:26.420 --> 26:29.860
except for a brief period in the early 1960s.

26:29.860 --> 26:36.540
I think more interesting, it's now obvious that growth is trending down across all continents,

26:36.540 --> 26:41.340
which is completely non-obvious from the original chart.

26:41.340 --> 26:43.780
So if growth is important, plot it directly.

26:43.780 --> 26:49.100
Okay, so that's all I wanted to say about this hierarchy and its implications for estimation.

26:49.100 --> 26:53.620
I want to move back one step in Cleveland's operations.

26:53.620 --> 26:56.100
So I want to talk about assembly.

26:56.100 --> 27:01.200
All that assembly is the grouping of detected graphical elements.

27:01.200 --> 27:04.900
The most useful research here comes from Gestalt psychology.

27:04.900 --> 27:06.700
The word Gestalt is German.

27:06.700 --> 27:10.180
It translates roughly to shape or form.

27:10.180 --> 27:13.180
The School of Thought goes back to the 1890s.

27:13.180 --> 27:18.660
And the central idea is that when your mind forms a Gestalt, a shape, the whole has a

27:18.660 --> 27:22.620
reality that is entirely separate from the parts.

27:22.620 --> 27:23.620
So what do I mean by that?

27:23.620 --> 27:26.020
So here's a concrete example.

27:26.020 --> 27:32.100
All I am showing you is some blobs of black color on a white background, but you perceive

27:32.100 --> 27:34.020
this as a panda bear.

27:34.020 --> 27:36.820
So this is an example of reification.

27:36.820 --> 27:42.700
The Gestalt principle of reification says that you experience more spatial information

27:42.700 --> 27:47.300
than is explicitly present in an image.

27:47.300 --> 27:53.820
Another Gestalt principle is emergence, which is best demonstrated by an example.

27:53.820 --> 27:55.780
So this is the famous dog picture.

27:55.780 --> 27:58.500
Does everybody see the dog?

27:58.500 --> 27:59.820
Does anyone not see the dog?

27:59.820 --> 28:00.820
I'm curious.

28:00.820 --> 28:01.820
Oh.

28:01.820 --> 28:02.820
OK, a couple of people.

28:02.820 --> 28:04.820
And you said it was a dog.

28:04.820 --> 28:05.820
Oh.

28:05.820 --> 28:06.820
OK, interesting.

28:06.820 --> 28:11.120
So this fellow here says they only saw it as a dog when I said that it was a dog.

28:11.120 --> 28:13.420
It's hard to see, I think.

28:13.420 --> 28:20.220
The point about emergence is that you don't recognize this as a dog by first identifying

28:20.220 --> 28:23.860
its feet, and then its tail, and then its head, and so on.

28:23.860 --> 28:28.900
The dog just appears all at once as a complete object.

28:28.900 --> 28:33.880
That's similar to the circle that we saw before, which just appears independent of the little

28:33.880 --> 28:36.940
dots that suggest its form.

28:36.940 --> 28:41.300
When we are thinking about making charts, the most useful idea from Gestalt psychology

28:41.300 --> 28:44.940
is prognons or pithiness.

28:44.940 --> 28:53.080
The principle of prognons says that we strongly prefer to interpret stimuli as regular and

28:53.080 --> 28:57.260
simple and orderly.

28:57.260 --> 29:03.260
Gestalt psychologists have identified a number of rules that will describe and predict how

29:03.260 --> 29:05.500
we will simplify visual input.

29:05.500 --> 29:10.980
So one we've seen before, the law of closure, which is what turns the dots into a circle.

29:10.980 --> 29:15.060
Another of the Gestalt laws of grouping is the law of continuity, which says that we

29:15.060 --> 29:19.740
will group together objects that follow an established direction.

29:19.740 --> 29:24.340
So it's hard for us to make sense of this, where the cars are sorted alphabetically.

29:24.340 --> 29:28.700
In fact, you will psychologically recoil from this chart.

29:28.700 --> 29:30.020
You're like, I have to do work.

29:30.020 --> 29:32.140
This is so unpleasant.

29:32.140 --> 29:38.140
But if we sort them by efficiency, it becomes effortless for us to do all of Cleveland's

29:38.140 --> 29:43.540
estimation operations, discrimination, ranking, and race showing.

29:43.540 --> 29:49.260
So that's another observation, that good plots organize the data to take advantage of continuity

29:49.260 --> 29:53.380
and to help the viewer assemble an overall Gestalt.

29:53.380 --> 29:57.980
This is also happening when we make a scatter plot, like this one from the car's data showing

29:57.980 --> 30:03.100
the relationship between the weight of a car and its efficiency as weight increases, efficiency

30:03.100 --> 30:04.100
decreases.

30:04.100 --> 30:06.940
Notice that the continuity does not have to be perfect.

30:06.940 --> 30:09.940
It just has to be suggestive.

30:09.940 --> 30:12.180
But we can add more information to the scatter plot.

30:12.180 --> 30:16.580
We can use Hue to encode the number of cylinders in the engine of each car.

30:16.580 --> 30:21.220
If we do that, you can easily pick out that there are three distinct groups of points.

30:21.220 --> 30:25.500
So what's happening here is we're leveraging the law of similarity, which says that we tend

30:25.500 --> 30:30.900
to see elements that are physically similar to each other as part of the same object,

30:30.900 --> 30:34.540
and then things that are different as parts of different objects.

30:34.540 --> 30:39.660
We use this all the time to put categorical information on charts, and there are lots

30:39.660 --> 30:43.340
of different ways to encode categorical information.

30:43.340 --> 30:45.180
Not all encodings are equally good.

30:45.180 --> 30:50.300
So here I'm just using shape, and this is pretty clearly worse than color.

30:50.300 --> 30:53.620
But if we use different shapes, it's much better, right?

30:53.620 --> 31:00.980
This works so well because your eye is really good at picking out differences in curvature.

31:00.980 --> 31:05.740
In fact, you can use degree of curvature to encode quantitative information.

31:05.740 --> 31:09.820
It's not in the list of seven that I showed earlier, but I think it's about tied with

31:09.820 --> 31:11.820
angle.

31:11.820 --> 31:15.820
So the circles, they really pop out, and as we learned before, we're also pretty good

31:15.820 --> 31:21.900
at detecting the slopes of lines, and so the triangles stand out from the little plus signs.

31:21.900 --> 31:25.660
To see what I mean about curvature, here's an encoding that you'd think would work really

31:25.660 --> 31:30.340
well, but it's terrible because the curvature of the sixes is too much like the curvature

31:30.340 --> 31:32.220
of the eights.

31:32.220 --> 31:36.740
And there's no good reason to, you can't use two channels to encode the same information

31:36.740 --> 31:37.740
redundantly.

31:37.740 --> 31:42.180
In fact, that's often a good choice, and we can even take this a step further.

31:42.180 --> 31:46.500
We can add a linear model to each subset of the data, and now we're taking advantage of

31:46.500 --> 31:50.660
another Gestalt grouping law by enclosing regions of space.

31:50.660 --> 31:55.580
Okay, the last grouping law I want to talk about is the law of proximity, which says

31:55.580 --> 32:01.340
that we tend to group elements that are near each other into a complete object.

32:01.340 --> 32:05.500
So proximity is a very strong grouping principle.

32:05.500 --> 32:09.700
This is the same diamonds data I was showing before as a stacked bar chart, but now instead

32:09.700 --> 32:16.300
of stacking the bars, I've dodged them from side to side so that they don't overlap.

32:16.300 --> 32:22.540
Proximity strongly encourages you to make eight objects from this image, one for each

32:22.540 --> 32:24.220
level of clarity.

32:24.220 --> 32:30.380
I personally find it very difficult to assemble the Gestalt of the blue bars and then compare

32:30.380 --> 32:33.100
them with the greenish bars.

32:33.100 --> 32:39.120
And again, the solution is to just use a parallel coordinates plot where you can very easily

32:39.120 --> 32:44.820
see the rise and fall of each cut, and you can compare the shape of each cut to the others.

32:44.820 --> 32:50.420
So another observation is that dodged bar charts are pretty much always a bad idea.

32:50.420 --> 32:53.420
They allow you to do two things, but neither of them well.

32:53.420 --> 33:01.020
Okay, now I want to move back up to the top, to the first of Cleveland's three operations

33:01.020 --> 33:02.380
of pattern perception.

33:02.380 --> 33:05.040
I want to talk about detection.

33:05.040 --> 33:09.300
Detection is the operation of recognizing that a geometric object encodes a physical

33:09.300 --> 33:10.300
value.

33:10.300 --> 33:16.060
And so most trivially, it's the recognition that each point in this scatter plot represents

33:16.060 --> 33:21.900
a car and encodes both the fuel efficiency and the weight of the car.

33:21.900 --> 33:26.740
So I say that detection is trivial, but it's not.

33:26.740 --> 33:31.780
Lots of people, the only charts they ever see are time series charts where a line moves

33:31.780 --> 33:34.580
from left to right, encoding a metric over time.

33:34.580 --> 33:38.780
If you show those people any other kind of chart, they are going to have to go to some

33:38.780 --> 33:43.860
effort to try to figure out what the different shapes mean and how the image is conveying

33:43.860 --> 33:45.420
information.

33:45.420 --> 33:48.340
It is very easy to make it harder than it needs to be.

33:48.340 --> 33:52.180
So most commonly, people give too little visual weight to the objects that are encoding the

33:52.180 --> 33:57.140
data, either by having too little luminance contrast between the foreground and the background,

33:57.140 --> 34:00.740
or by making the objects encoding the data too small.

34:00.740 --> 34:06.100
Another problem is introducing other objects that compete with the data, like the lines

34:06.100 --> 34:09.940
for a grid and a border around the plot area.

34:09.940 --> 34:14.520
So that's not so bad, but if I amp it up a little bit, it gets really bad really quickly.

34:14.520 --> 34:21.960
In the extreme, you start to introduce visual artifacts that aren't even real.

34:21.960 --> 34:28.800
So for example, do you see the faint gray circles in the intersections of the lines?

34:28.800 --> 34:31.760
That's an illusion, the name of which is escaping me right now.

34:31.760 --> 34:34.520
It's named after a German fellow.

34:34.520 --> 34:38.320
It's worth noting that in this regard, Excel's defaults are pretty bad.

34:38.320 --> 34:42.440
So the default is to have only horizontal grid lines, I don't know why, and for them

34:42.480 --> 34:47.440
to be fully saturated, and I get those same little gray dots where the blue line intersects

34:47.440 --> 34:48.840
the grid lines.

34:48.840 --> 34:54.120
So another observation is the detection is perhaps not as trivial as it might seem.

34:54.120 --> 34:58.720
Be sure that the data has a lot of contrast with the background, and if you include grid

34:58.720 --> 35:02.920
lines, make them muted so that they don't interfere with detection.

35:02.920 --> 35:06.360
As Tufti says, above all else, show the data.

35:06.360 --> 35:10.520
So talking about grid lines brings me to part five.

35:10.600 --> 35:13.520
This is actually my favorite part, so I'm glad you stayed to the end.

35:13.520 --> 35:17.720
In this part, I want to review a few more useful results from psychology research.

35:17.720 --> 35:22.920
And the first one, this is my favorite part, Weber's law, which says that the just noticeable

35:22.920 --> 35:28.960
difference, the smallest difference that you can detect between two things is proportional

35:28.960 --> 35:31.720
to the size or the intensity of those two things.

35:31.720 --> 35:34.240
That it's proportional is really surprising.

35:34.240 --> 35:38.320
So this statement is extremely abstract, I'll make it more concrete.

35:38.320 --> 35:42.520
If I gave you two weights, one of which was 10 kilograms, the other of which was 20 kilograms,

35:42.520 --> 35:45.960
you would very easily be able to tell me which one was heavier.

35:45.960 --> 35:49.760
But if I gave you two more weights, one of which was 100 kilograms and one of which was

35:49.760 --> 35:54.840
110, you would have a much harder time telling me if they were the same or if they were different.

35:54.840 --> 36:00.560
And so even though the absolute difference is the same, 10 kilograms in each case, the

36:00.560 --> 36:06.520
top comparison, the heavier one is double the weight, and in the bottom it's only 10%.

36:07.040 --> 36:10.680
10% is about the just noticeable difference.

36:10.680 --> 36:14.400
Another example are these two lines the same length, they're different.

36:14.400 --> 36:19.680
It's hard to tell, maybe if you work at it, they're different.

36:19.680 --> 36:25.240
What if I put these little blue boxes on, or the blue boxes different or the same?

36:25.240 --> 36:27.160
Super easy to tell that they're different.

36:27.160 --> 36:31.280
It turns out that the total height of each bar is the same, 12 units.

36:31.280 --> 36:35.840
And so that means that the difference between the two white bars is exactly the same as

36:35.840 --> 36:41.040
the two blue bars in absolute terms, but you can distinguish between the blue bars because

36:41.040 --> 36:45.520
the difference is large compared to their size.

36:45.520 --> 36:51.920
So now we have another observation, Weber's law is why grid lines are useful, excuse me.

36:51.920 --> 36:56.360
And Weber's law dictates the use of grid lines, how many you should have and whether or not

36:56.360 --> 36:57.720
you should have them at all.

36:57.720 --> 37:03.160
So here are eight curves all plotting the same class of function with different parameters.

37:03.160 --> 37:06.400
So of this pair, which one dips down lower?

37:06.400 --> 37:07.400
Hard to tell.

37:07.400 --> 37:08.400
Okay.

37:08.400 --> 37:09.400
How about now?

37:09.400 --> 37:11.720
It's trivial.

37:11.720 --> 37:18.120
It's easy because putting that dip in a much smaller box makes the just noticeable difference

37:18.120 --> 37:21.000
proportionately smaller.

37:21.000 --> 37:24.480
So here's another Tufti quote, erase non-data ink.

37:24.480 --> 37:27.440
People sometimes take this to mean that you should erase grid lines because grid lines

37:27.440 --> 37:29.360
aren't data, right?

37:29.360 --> 37:30.520
This is a misquote.

37:30.520 --> 37:34.320
What Tufti really wrote is erase non-data ink within reason.

37:34.320 --> 37:38.320
My version is erase non-data ink that interferes with detection or doesn't assist with assembly

37:38.320 --> 37:44.120
or estimation, which is not as pithy as Tufti, but perhaps a little more accurate.

37:44.120 --> 37:45.120
Okay.

37:45.120 --> 37:49.560
New topic, another interesting result is that you are best at detecting variation in slope

37:49.560 --> 37:52.240
near 45 degrees.

37:52.240 --> 37:58.280
So this is the same data plotted three different ways and the change in slope is most obvious

37:58.640 --> 38:04.280
when the line is near 45 degrees, which brings up a super useful idea called banking to 45.

38:04.280 --> 38:08.640
The idea of banking to 45 is that you should set the aspect ratio of your plot so that

38:08.640 --> 38:11.560
the average slope is 45 degrees.

38:11.560 --> 38:13.040
Here's a demonstration.

38:13.040 --> 38:16.320
This plot shows the monthly average sunspot number.

38:16.320 --> 38:21.360
The raw data is the partly saturated white points and a smooth series is then laid over

38:21.360 --> 38:22.680
in blue.

38:22.680 --> 38:27.160
From this data, about all you can tell is that sunspots have some kind of cycle and

38:27.160 --> 38:30.520
the peak of that cycle is varied over time.

38:30.520 --> 38:37.040
But if I break the series up into three 55-year chunks and then I stretch them out, then you

38:37.040 --> 38:41.840
can see a lot more nuance in the onset and the decay of each cycle.

38:41.840 --> 38:47.000
It looks like maybe the onset is more sharp and then the decline is more gradual.

38:47.000 --> 38:49.800
I'd never have been able to see that in the original chart.

38:49.800 --> 38:54.200
And so a good rule of thumb is to set the aspect ratio of your plots so that the average

38:54.200 --> 38:58.640
line segment is roughly 45 degrees.

38:58.640 --> 39:00.040
You don't have to get all scientific about it.

39:00.040 --> 39:02.920
You can just eyeball it and it'll be fine.

39:02.920 --> 39:08.280
This data brings up another aside, which is, should I include zero on my scale?

39:08.280 --> 39:10.200
This is the panel that was at the lower left.

39:10.200 --> 39:14.800
To make it so flat, all I did was extend the y-axis down to 800,000.

39:14.800 --> 39:18.520
If I extend the y-axis all the way down to zero, you have a really hard time seeing the

39:18.520 --> 39:19.520
variation.

39:19.960 --> 39:26.280
So the real answer to whether or not you should include zero on your scale is it depends.

39:26.280 --> 39:28.760
Here's the truth as compact as I can make it.

39:28.760 --> 39:35.760
If your encoding relies on intensity for accurate estimation, then you have to include zero

39:35.760 --> 39:38.200
in your scale.

39:38.200 --> 39:40.200
On the other hand, you're using position.

39:40.200 --> 39:41.200
It's up to you.

39:41.200 --> 39:48.520
So this is the saturation chart before, relies on pre-attentive judgment of intensity of

39:48.760 --> 39:50.360
position, so it has to go to zero.

39:50.360 --> 39:53.720
Same deal with area, with the dot plot, I'm using position.

39:53.720 --> 39:57.320
I'm free to clip the x-axis at 10.

39:57.320 --> 40:01.480
In fact, it's a good idea to do so because it gives you more resolution.

40:01.480 --> 40:06.480
With bars, people sometimes argue that what the viewer is doing with a bar chart is they're

40:06.480 --> 40:09.040
comparing the position of the end of the bars.

40:09.040 --> 40:10.560
I say that's a mistake.

40:10.560 --> 40:16.120
What you're actually encouraging the user to do with a bar chart is to compare lengths.

40:16.320 --> 40:22.520
In a bar chart, the axis has to start at zero because if you don't, you're going to mislead

40:22.520 --> 40:24.640
your users.

40:24.640 --> 40:25.640
Here's a bar chart.

40:25.640 --> 40:26.640
You can't see the variation.

40:26.640 --> 40:30.080
If you abandon the bars, then it's completely obvious.

40:30.080 --> 40:36.680
Above all else, show the data, or I say show the variation in the data.

40:36.680 --> 40:38.160
I have negative 30 seconds.

40:38.160 --> 40:41.760
I want to say just two words about tools.

40:41.800 --> 40:48.560
Every plot in this talk was made using the ggplot2 library in R, which is the best graphing

40:48.560 --> 40:52.040
toolset that I have ever used by far.

40:52.040 --> 40:57.040
If you want to learn more about it, I've published the source code to all these charts as our

40:57.040 --> 41:01.880
markdown document on GitHub, and I have put the rendered document up on our pubs.

41:01.880 --> 41:04.760
I'll also put this presentation up on SlideShare.

41:04.760 --> 41:09.560
I will tweet all these links under my username, Jay Rouser.

41:09.560 --> 41:10.560
That's all I have.

41:10.560 --> 41:11.560
Thank you.

41:11.560 --> 41:12.560
Thank you.

41:12.560 --> 41:13.560
Thank you.

41:13.560 --> 41:14.560
Thank you.

41:14.560 --> 41:15.560
Thank you.

41:15.560 --> 41:16.560
Thank you.

41:16.560 --> 41:17.560
Thank you.

41:17.560 --> 41:18.560
Thank you.

41:18.560 --> 41:19.560
Thank you.

41:19.560 --> 41:20.560
Thank you.

41:20.560 --> 41:21.560
Thank you.

41:21.560 --> 41:22.560
Thank you.

41:22.560 --> 41:23.560
Thank you.

41:23.560 --> 41:24.560
Thank you.

41:24.560 --> 41:25.560
Thank you.

41:25.560 --> 41:26.560
Thank you.

41:26.560 --> 41:27.560
Thank you.

41:27.560 --> 41:28.560
Thank you.

41:28.560 --> 41:29.560
Thank you.

41:29.560 --> 41:30.560
Thank you.

41:30.560 --> 41:31.560
Thank you.

41:31.560 --> 41:32.560
Thank you.

41:32.560 --> 41:33.560
Thank you.

41:33.560 --> 41:34.560
Thank you.

41:34.560 --> 41:35.560
Thank you.

41:35.560 --> 41:36.560
Thank you.

