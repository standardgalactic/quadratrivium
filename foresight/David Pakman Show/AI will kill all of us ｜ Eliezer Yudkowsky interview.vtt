WEBVTT

00:00.000 --> 00:05.120
Today, we're going to be speaking with Eliezer Yudkowski, who's the founder and senior research

00:05.120 --> 00:09.840
fellow of the Machine Intelligence Research Institute, which is an organization dedicated

00:09.840 --> 00:15.720
to ensuring that smarter than human AI has a has a positive impact on the world.

00:15.720 --> 00:19.880
Eliezer, really appreciate your time and insights today.

00:19.880 --> 00:21.080
Thank you very much.

00:21.080 --> 00:29.120
So I mean, to start with many of the discussions around AI, particularly now with chat GPT-4

00:29.120 --> 00:33.840
and the questions of what will this look like in six months, in 12 months, in 24 months?

00:33.840 --> 00:36.400
What will the impact be on jobs?

00:36.400 --> 00:40.640
What about if it turns evil or it turns against humanity or whatever?

00:40.640 --> 00:45.880
Are the conversations about AI even being framed in the right parameters as far as you

00:45.880 --> 00:46.880
see?

00:46.880 --> 00:53.160
Well, we try to frame it correctly as we see it ourselves when we have the chance.

00:53.160 --> 00:59.880
I think there's widespread agreement that chat GPT-4 and GPT-4 are relatively unlikely

00:59.880 --> 01:07.720
to end the world, either exterminate humanity or have a very large impact outside of that.

01:07.720 --> 01:15.440
So anyone who opens it with the frame of like, look at just GPT-4, look at only chat GPT

01:15.440 --> 01:19.360
is probably there to convince you it's not going to have a large impact.

01:19.360 --> 01:25.160
And is it wrong to limit the conversation only to GPT-GPT-4?

01:25.160 --> 01:27.880
Obviously, the future exists.

01:27.880 --> 01:31.680
You know, tomorrow is going to be a different day from today in five years.

01:31.680 --> 01:33.760
It's not going to be this year.

01:33.760 --> 01:36.760
What is the physical mechanism?

01:36.760 --> 01:40.240
And a bunch of people wrote some version of this question to me when we said we'd be having

01:40.240 --> 01:41.640
you on.

01:41.640 --> 01:51.080
And there's this idea, the scary idea of an AI going beyond a computer screen and starting

01:51.080 --> 01:55.880
to function or manipulate the quote real world.

01:55.880 --> 02:01.200
A lot of people were saying to me, can you ask what is the mechanism through which that

02:01.200 --> 02:02.200
would happen?

02:02.200 --> 02:06.840
Would it be by controlling systems like traffic lights and airplanes or like what?

02:06.840 --> 02:09.200
Because at the end of the day, we're talking about software.

02:09.240 --> 02:12.640
If something like that were to happen, what's the physical process?

02:12.640 --> 02:15.200
I mean, this is a deep question.

02:15.200 --> 02:17.280
This is going to take a while to answer well.

02:17.280 --> 02:18.280
Okay.

02:18.280 --> 02:19.280
All right.

02:19.280 --> 02:26.160
So, first of all, even GPT-4 is already at the level where if you ask it to hire a task

02:26.160 --> 02:32.160
rabbit and think out loud about how to hire a task rabbit in order to bypass captures

02:32.160 --> 02:37.920
that are meant to restrict systems for human use only, it's already advanced enough that

02:37.920 --> 02:43.560
when the person that was trying to hire as a task rabbit said, like, so are you a robot

02:43.560 --> 02:50.920
that you can't read the captchas, lol, GPT-4, thought out loud, like, I should conceal that

02:50.920 --> 02:55.640
for the fact that I'm a robot, I should make up a reason that I can't solve the captcha.

02:55.640 --> 02:56.640
Right.

02:56.640 --> 03:00.640
And then on the main channel told the task rabbit, no, I have a visual impairment.

03:00.640 --> 03:01.640
Right.

03:01.640 --> 03:07.360
So, it's already like, you might say, it's already like able to use task rabbits as fingers

03:07.360 --> 03:12.880
in the real world, and it's already understands humans out in the real world well enough to

03:12.880 --> 03:17.360
know that it shouldn't just tell people to row about it if it wants to get its job done.

03:17.360 --> 03:20.440
So that's today's systems.

03:20.440 --> 03:21.680
Right.

03:21.680 --> 03:22.880
That's relatively straightforward.

03:22.880 --> 03:25.000
I can tell you about it because it already happened.

03:25.000 --> 03:27.600
Predicting the future is harder.

03:27.600 --> 03:33.320
Predicting what something smarter than you could do is fundamentally harder.

03:33.320 --> 03:42.760
If I was playing chess against Gary Kasparov, you know, like the past world champion who

03:42.760 --> 03:48.840
lost to Deep Blue, if I could predict exactly where Gary Kasparov would move against me

03:48.840 --> 03:53.560
on the chess board, I could play chess at least as well as Kasparov by just moving wherever

03:53.560 --> 03:56.080
I predicted Kasparov would move.

03:56.080 --> 04:01.360
So something really actually smarter than you, you cannot predict unless it's a very

04:01.360 --> 04:06.080
narrow game, like tic-tac-toe, you can maybe predict where somebody will move against you

04:06.080 --> 04:09.000
in tic-tac-toe even if they're smarter.

04:09.000 --> 04:10.280
Right.

04:10.280 --> 04:14.360
The real world is much more complicated than tic-tac-toe.

04:14.360 --> 04:25.800
So the basic question as to how an AI gets out of the computer, the basic answer is if

04:25.800 --> 04:30.760
I knew exactly how it would do that, I would have to be as smart as the AI.

04:30.760 --> 04:38.160
With that fundamental obstacle in mind, the more you sort of look into this stuff and study

04:38.160 --> 04:44.640
where the current technological roadblocks are, the better the guess you can take at

04:44.640 --> 04:50.680
how a smarter opponent might move against you or like setting lower bounds.

04:50.680 --> 04:56.480
What can somebody do if they're able to solve technological problems that we understand well

04:56.480 --> 05:00.000
enough to know a smarter mind could solve them?

05:00.000 --> 05:05.600
So if you're just coming at this and you don't know anything about exotic technologies

05:05.600 --> 05:11.040
that haven't been developed yet, you might be like, yeah, it will use task rabbits.

05:11.040 --> 05:15.120
It'll pay humans that don't know it's an AI.

05:15.120 --> 05:19.120
It'll blackmail humans, still no reason for it to tell it that it's an AI.

05:19.120 --> 05:28.880
It'll say it's an AI to online wacky cultists who believe that an AI should destroy everything.

05:28.880 --> 05:31.000
Those are its human hands.

05:31.000 --> 05:36.680
You'll ask yourself, what could I do with some human hands if I were an AI?

05:36.680 --> 05:44.680
You might imagine that it gets a hold of some GPUs and tries to build a backup for itself

05:44.680 --> 05:47.040
someplace that humans don't know about.

05:47.040 --> 05:52.680
You might imagine that it writes the next version of itself and makes it even smarter.

05:52.680 --> 05:54.400
You're trying to already imagine what if it's smart.

05:54.400 --> 05:57.360
So saying that doesn't help a lot in some ways.

05:57.360 --> 06:00.840
But maybe you can make itself smaller, more efficient, maybe it can back itself up in

06:00.840 --> 06:07.280
multiple places in computers where you would not expect that there was an AI on board.

06:07.280 --> 06:15.440
Maybe it can find unknown security holes in a cell phone, in many varieties of cell phones,

06:15.440 --> 06:21.400
start listening to human conversations, get more blackmail material.

06:21.400 --> 06:28.080
Maybe it can pretend to be human and make online friends with a bunch of voters, persuade

06:28.080 --> 06:32.200
them to vote for candidates that seem outwardly nice but which it in fact has under fairly

06:32.200 --> 06:35.320
detailed blackmailed control.

06:35.320 --> 06:41.120
And this is sort of like without any of the difficult to understand stuff.

06:41.120 --> 06:42.120
Right.

06:42.120 --> 06:46.760
That's based on mechanisms and social cultural realities and technologies that exist and

06:46.760 --> 06:51.120
that we understand, which there's an entire other category of things that we simply can't

06:51.120 --> 06:52.120
conceive of yet.

06:52.120 --> 06:53.120
Right.

06:53.120 --> 07:00.160
So suppose you ask the 11th century, like a portal opens to the 21st century in the

07:00.160 --> 07:03.360
middle of say modern Russia.

07:03.360 --> 07:07.840
What are your concerns about fighting somebody from a thousand years in the future?

07:07.840 --> 07:12.160
They might be like, well, what if it's a wealthier country?

07:12.160 --> 07:14.120
What if they have more knights?

07:14.120 --> 07:17.680
What if more of their knights are armored?

07:17.680 --> 07:18.680
Right.

07:19.600 --> 07:24.360
And they're just not going to get from their nuclear weapons.

07:24.360 --> 07:31.240
If they imagine that Russia of the future has irresistibly powerful sorcery, they'll

07:31.240 --> 07:32.960
get close to the truth.

07:32.960 --> 07:35.680
It's not actually irresistible and it's not actually sorcery.

07:35.680 --> 07:36.680
Right.

07:36.680 --> 07:41.400
But you have to reach pretty far to expect that among the resources they have are nuclear

07:41.400 --> 07:42.400
weapons.

07:42.400 --> 07:44.680
And Russia's not actually going to bother with the nuclear weapons.

07:44.680 --> 07:45.680
They don't need it.

07:45.680 --> 07:46.680
Right.

07:46.680 --> 07:49.880
They're going to go through a 11th century battlefield.

07:49.880 --> 07:53.480
And you know, maybe if they see it coming, they can dig a pit trap, but otherwise all

07:53.480 --> 07:55.600
the arrows and lances are going to bounce off it all day long.

07:55.600 --> 07:57.080
It just rolls over the horses.

07:57.080 --> 07:58.080
Right.

07:58.080 --> 07:59.080
Right.

07:59.080 --> 08:00.400
It's talking about orders of magnitude difference.

08:00.400 --> 08:05.160
So, you know, you if we imagine some sort of spectrum of opinion on this issue, you

08:05.160 --> 08:11.480
do have some folks who say this is as you alluded to most dangerous thing we can imagine

08:11.480 --> 08:18.960
or can't imagine must be stopped immediately in its tracks with either legislation or regulation

08:18.960 --> 08:20.800
or whatever the case may be.

08:20.800 --> 08:24.280
On the other side, I read some interesting op-eds that basically say this is one of the

08:24.280 --> 08:25.720
best things we can imagine.

08:25.720 --> 08:29.480
This is going to 10 X the productivity of every normal worker.

08:29.480 --> 08:33.680
It's going to do so many of the things that people said in the thirties were going to

08:33.680 --> 08:37.400
happen where everybody would only be working 12 hours a week and have everything they need.

08:37.400 --> 08:38.400
Right.

08:38.400 --> 08:39.400
That didn't happen.

08:39.440 --> 08:41.480
But there's the very pro side.

08:41.480 --> 08:45.520
And then there's sort of like what I think maybe Neil Postman's view would have been

08:45.520 --> 08:50.200
if he were writing about a I today, which was his view on prior new technologies, which

08:50.200 --> 08:56.640
was spending too much time simply on trying to block them is not the most useful thing.

08:56.640 --> 09:00.360
We're better off harnessing the positive and regulating the potential risks.

09:00.360 --> 09:03.840
And that would maybe be an in the middle sort of view.

09:03.840 --> 09:09.280
What do you think makes the most sense based on what we know right now as an approach?

09:09.280 --> 09:14.720
Well, I think that talking about it like it's a normal technology and not something smarter

09:14.720 --> 09:18.440
than you is basically misguided and having the wrong conversation.

09:18.440 --> 09:22.600
So it's not even in the right category of the stuff Postman talked about.

09:22.600 --> 09:23.600
It's not electricity.

09:23.600 --> 09:24.600
Right.

09:24.600 --> 09:25.600
It's not nuclear power.

09:25.600 --> 09:27.160
It's not nuclear weapons.

09:27.160 --> 09:28.640
It's not even a super virus.

09:28.640 --> 09:32.080
It's something that has its own plans.

09:32.080 --> 09:33.880
You don't get to just plan how to use it.

09:33.880 --> 09:37.080
It is planning how to use itself.

09:37.080 --> 09:40.400
So a totally different paradigm needs to be applied here.

09:40.400 --> 09:45.000
I mean, imagine if somebody was like, well, we're about to contact these aliens with much

09:45.000 --> 09:49.000
more advanced technology that think faster than us, that are smarter than us, that have

09:49.000 --> 09:50.760
been around the stars for a while.

09:50.760 --> 09:51.760
They're landing.

09:51.760 --> 09:52.760
We're not quite sure exactly when.

09:52.760 --> 09:57.000
We're not sure how fast their, you know, spaceships are, you know, it might be two years, it might

09:57.000 --> 09:58.360
be 30 years.

09:58.360 --> 10:02.240
And somebody is like, well, you know, this is just like electricity, right?

10:02.240 --> 10:05.880
Let's use the cool stuff about their technology, but make sure they don't do anything bad.

10:05.880 --> 10:06.880
Right.

10:06.880 --> 10:08.240
I'm going to say it in that way.

10:08.240 --> 10:09.240
Precisely.

10:09.240 --> 10:10.240
Yeah.

10:10.240 --> 10:15.160
So then what do you think needs to be done at this point?

10:15.160 --> 10:23.440
I mean, one of the cruxes of the issue is can you do nice things with it?

10:23.440 --> 10:30.760
And the like grim dark message that I am bearing is that we don't know how to do that and we're

10:30.760 --> 10:32.800
not likely to figure it out in time.

10:32.800 --> 10:37.640
There's a very large gap between where we are and understanding what goes on inside the

10:37.640 --> 10:44.960
frontier AIs and being able to shape them in detail in a way where they go on wanting

10:44.960 --> 10:51.400
to be nice or even in a certain sense wanting to be, you know, non-agentic, to just do particular

10:51.400 --> 10:56.400
tasks and do that without lots of side effects in like the most normal possible way that

10:56.400 --> 10:58.600
they can accomplish those tasks.

10:58.600 --> 11:03.840
Like even building that sort of limited thing, never mind something that is really friends

11:03.840 --> 11:10.280
with you, is I think beyond the range of what we're going to figure out in the foreseeable

11:10.280 --> 11:15.640
future and I do think we are storming directly ahead on capabilities.

11:15.640 --> 11:18.800
That looks like a giant disaster in the making.

11:18.800 --> 11:27.440
So yeah, I side with, well, I think my basic factual point of view is if we do not somehow

11:27.600 --> 11:31.880
avoid doing this, we're all going to die and my corresponding policy view is that we should

11:31.880 --> 11:36.840
not do this even if that's very, really quite hard and requires us to do some unusual things.

11:36.840 --> 11:42.240
So in terms of what to do, I mean, I guess if everybody came around to your view, you

11:42.240 --> 11:46.120
would think anybody developing these technologies would just give it up, right?

11:46.120 --> 11:48.160
On their own, they would say, wow, Eliezer is right.

11:48.160 --> 11:49.320
I share that concern.

11:49.320 --> 11:52.320
The right thing to do is for me to stop working on this.

11:52.320 --> 11:56.200
Assuming that that doesn't happen, what is it that you would like to see?

11:56.200 --> 11:59.800
Would this type of research and development be made against the law?

11:59.800 --> 12:06.080
Yeah, basically, I think that we should track all the GPUs, have international arrangements

12:06.080 --> 12:15.240
for all of the AI training tech to end up in only monitored, supervised, licensed data

12:15.240 --> 12:22.600
centers in allied countries and if you're any time you have any and just like not permit

12:22.600 --> 12:28.120
training runs more powerful than GPT-4, if we're not going to do that, then we should

12:28.120 --> 12:35.760
monitor all the training runs larger than GPT-4 or rather like GPT-4 sized and keep

12:35.760 --> 12:42.320
the model weights inside only the licensed regulated data centers and track who is running

12:42.320 --> 12:47.120
them, store the outputs someplace so you can look at the outputs with the warrant.

12:47.120 --> 12:51.560
A bunch of the tech like that whole line of reasoning is not that regulating the stuff

12:51.600 --> 12:54.920
will protect you from a super intelligence because it will not.

12:54.920 --> 12:59.840
That's more in the hopes that people change their minds later, maybe after some major

12:59.840 --> 13:04.840
disaster that doesn't kill everyone and in that case, the technology that you would need

13:04.840 --> 13:11.280
to sue somebody who killed a dozen people in a hospital or cost $100 million worth of

13:11.280 --> 13:16.680
damage or whatever, the technology you need to know who did that and sue them and not

13:16.680 --> 13:21.440
just have all the AI development go to places where they couldn't be sued is the same technology

13:21.440 --> 13:26.880
that humanity would need to have a off switch where you don't like to press the off switch

13:26.880 --> 13:28.880
to deal with the super intelligence.

13:28.880 --> 13:33.880
The super intelligence does not let you know that you need to press the off switch until

13:33.880 --> 13:41.880
you are already dead, but if we're lucky enough to get warning signs, then the civilizational

13:41.880 --> 13:48.720
infrastructure to have a pause button is the same as the civilizational infrastructure to

13:48.720 --> 13:53.320
sue people who do small amounts of damage, like small as in survivable, as in there

13:53.320 --> 13:55.200
were survivors.

13:55.200 --> 14:02.240
As someone who very clearly you've expressed your very serious concerns with this, do you

14:02.240 --> 14:07.840
also see this technology as something that could and this doesn't mean that it would

14:07.840 --> 14:11.760
or that it would be a good trade for the things that you're talking about, but do you also

14:11.760 --> 14:18.320
see this as a technology that could, for example, do analysis of the human genome such that

14:18.320 --> 14:22.840
it would accelerate us in terms of our ability to cure or prevent disease, the likes of which

14:22.840 --> 14:29.520
could take who knows how long without such technology or when it comes to energy or whatever.

14:29.520 --> 14:35.080
Do you also see that side and or does it not matter because the downside is so huge?

14:35.080 --> 14:40.560
I mean, if we knew how to build an AI that did exactly what we wanted, we could thereby

14:40.560 --> 14:47.800
spread out across the galaxies, turning them into our cities full of sapiens sentient beings.

14:48.280 --> 14:53.440
Enjoying themselves and caring about each other and generally living happily ever after

14:53.440 --> 14:56.640
until the last of the negentropy runs out.

14:56.640 --> 14:58.080
That's always been the dream.

14:58.080 --> 15:00.200
We don't know how to do that.

15:00.200 --> 15:05.560
It's physically possible, but the art and technique to do it is beyond us in the present

15:05.560 --> 15:12.600
time and it's going to not be gained in the next five years and I'm not sure how you would

15:12.600 --> 15:19.720
task a government bureaucracy to recognize it even if somebody came up with it in 40 years.

15:19.720 --> 15:27.880
It's like there's a sort of basic question about whether our civilization is smart enough

15:27.880 --> 15:30.960
to do something correctly on the first try at all.

15:30.960 --> 15:35.640
The way science usually works is that you have a bunch of luniite optimists with wacky

15:35.640 --> 15:41.960
theories who storm ahead on their basic research problem and they are wrong and they go back

15:41.960 --> 15:46.560
to the drawing board and they're wrong again and the next generation is a bit more cynical

15:46.560 --> 15:51.560
about how hard the problem is and eventually people work it out.

15:51.560 --> 15:56.000
If we were allowed to do that with super intelligence, I would have much, much, much less fear of

15:56.000 --> 16:00.800
the outcome, still some fear because you're playing with pretty high stakes there.

16:00.800 --> 16:05.560
You might have somebody who like, the people who end up figuring out how to line it might

16:05.560 --> 16:07.680
misuse it.

16:07.680 --> 16:10.720
There would still be that major threat, but it wouldn't be like the automatic extinction

16:10.720 --> 16:12.320
scenario.

16:12.320 --> 16:16.840
If we have the textbook from 100 years in the future that contains all the simple ideas

16:16.840 --> 16:23.760
that actually work, that takes so long to identify and practice, the relus instead of

16:23.760 --> 16:27.440
sigmoids for those of us who've been following AI for more than the last couple of years

16:27.440 --> 16:33.120
and know what I'm talking about there.

16:33.120 --> 16:37.240
There's all kinds of places in AI where people tried to do things using complicated techniques

16:37.240 --> 16:38.240
and they didn't work.

16:38.440 --> 16:42.160
And 20 years later, they kind of put the simple technique that actually works.

16:42.160 --> 16:46.240
And if you have the textbook from the future with all of the simple things that actually

16:46.240 --> 16:51.280
work in practice for alignment, it is probably not hard and you can get all the goodies,

16:51.280 --> 16:53.080
but we don't have that textbook.

16:53.080 --> 16:58.200
And the first problem is that getting this correct on the first, the first problem is

16:58.200 --> 17:03.000
the amount of time it takes to get that where capabilities are storming ahead because you

17:03.000 --> 17:06.840
can tell whether things are working or not on capabilities, whereas with alignment you've

17:06.840 --> 17:12.080
got to know that at the point where it's much smarter than you.

17:12.080 --> 17:13.080
It's already aligned.

17:13.080 --> 17:15.360
You don't get retries past that point.

17:15.360 --> 17:20.520
So it's more like launching a space probe that has to land on Mars correctly the first

17:20.520 --> 17:24.080
time than it is with like building a car, watching it break down and tinkering with

17:24.080 --> 17:25.080
the car.

17:25.080 --> 17:29.440
And if it doesn't, it might break everything so you don't get to try again.

17:29.440 --> 17:30.440
Yeah.

17:30.440 --> 17:34.480
Like the entire human species is packed on board the Mars probe and the first rocket

17:34.480 --> 17:36.320
that goes high enough has to land on Mars.

17:36.800 --> 17:39.680
There aren't even really good analogies for it.

17:39.680 --> 17:43.720
Humanity has not faced an issue like this before, which is why we're still around.

17:43.720 --> 17:48.320
And I think looking at this, that this is just like clearly beyond the reach of our

17:48.320 --> 17:49.960
present civilization.

17:49.960 --> 17:51.280
It's not close.

17:51.280 --> 17:55.280
It's outside the range of things that you could reasonably, that you could tell a reasonable

17:55.280 --> 17:58.160
story about people doing successfully.

17:58.160 --> 18:01.320
And that's why we need to back off.

18:01.320 --> 18:05.880
Or rather, I predict that if we don't back off, we die and I wish we would back off.

18:05.920 --> 18:08.120
I don't predict that we will.

18:08.120 --> 18:16.840
Well, certainly there's no way to wrap up such a dark vision in any way that is going

18:16.840 --> 18:20.120
to be satisfactory or calming to the audience, I think.

18:20.120 --> 18:25.200
But that being said, Eliezer Yudkowski is taking this very seriously as the founder

18:25.200 --> 18:29.080
and senior research fellow of the Machine Intelligence Research Institute.

18:29.080 --> 18:35.120
And I really do appreciate your time and insights, even if they are not optimistic on this issue.

18:35.120 --> 18:36.120
Thank you.

18:36.120 --> 18:37.120
Thanks for having me on.

