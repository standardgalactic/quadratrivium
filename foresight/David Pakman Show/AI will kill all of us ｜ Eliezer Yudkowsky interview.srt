1
00:00:00,000 --> 00:00:05,120
Today, we're going to be speaking with Eliezer Yudkowski, who's the founder and senior research

2
00:00:05,120 --> 00:00:09,840
fellow of the Machine Intelligence Research Institute, which is an organization dedicated

3
00:00:09,840 --> 00:00:15,720
to ensuring that smarter than human AI has a has a positive impact on the world.

4
00:00:15,720 --> 00:00:19,880
Eliezer, really appreciate your time and insights today.

5
00:00:19,880 --> 00:00:21,080
Thank you very much.

6
00:00:21,080 --> 00:00:29,120
So I mean, to start with many of the discussions around AI, particularly now with chat GPT-4

7
00:00:29,120 --> 00:00:33,840
and the questions of what will this look like in six months, in 12 months, in 24 months?

8
00:00:33,840 --> 00:00:36,400
What will the impact be on jobs?

9
00:00:36,400 --> 00:00:40,640
What about if it turns evil or it turns against humanity or whatever?

10
00:00:40,640 --> 00:00:45,880
Are the conversations about AI even being framed in the right parameters as far as you

11
00:00:45,880 --> 00:00:46,880
see?

12
00:00:46,880 --> 00:00:53,160
Well, we try to frame it correctly as we see it ourselves when we have the chance.

13
00:00:53,160 --> 00:00:59,880
I think there's widespread agreement that chat GPT-4 and GPT-4 are relatively unlikely

14
00:00:59,880 --> 00:01:07,720
to end the world, either exterminate humanity or have a very large impact outside of that.

15
00:01:07,720 --> 00:01:15,440
So anyone who opens it with the frame of like, look at just GPT-4, look at only chat GPT

16
00:01:15,440 --> 00:01:19,360
is probably there to convince you it's not going to have a large impact.

17
00:01:19,360 --> 00:01:25,160
And is it wrong to limit the conversation only to GPT-GPT-4?

18
00:01:25,160 --> 00:01:27,880
Obviously, the future exists.

19
00:01:27,880 --> 00:01:31,680
You know, tomorrow is going to be a different day from today in five years.

20
00:01:31,680 --> 00:01:33,760
It's not going to be this year.

21
00:01:33,760 --> 00:01:36,760
What is the physical mechanism?

22
00:01:36,760 --> 00:01:40,240
And a bunch of people wrote some version of this question to me when we said we'd be having

23
00:01:40,240 --> 00:01:41,640
you on.

24
00:01:41,640 --> 00:01:51,080
And there's this idea, the scary idea of an AI going beyond a computer screen and starting

25
00:01:51,080 --> 00:01:55,880
to function or manipulate the quote real world.

26
00:01:55,880 --> 00:02:01,200
A lot of people were saying to me, can you ask what is the mechanism through which that

27
00:02:01,200 --> 00:02:02,200
would happen?

28
00:02:02,200 --> 00:02:06,840
Would it be by controlling systems like traffic lights and airplanes or like what?

29
00:02:06,840 --> 00:02:09,200
Because at the end of the day, we're talking about software.

30
00:02:09,240 --> 00:02:12,640
If something like that were to happen, what's the physical process?

31
00:02:12,640 --> 00:02:15,200
I mean, this is a deep question.

32
00:02:15,200 --> 00:02:17,280
This is going to take a while to answer well.

33
00:02:17,280 --> 00:02:18,280
Okay.

34
00:02:18,280 --> 00:02:19,280
All right.

35
00:02:19,280 --> 00:02:26,160
So, first of all, even GPT-4 is already at the level where if you ask it to hire a task

36
00:02:26,160 --> 00:02:32,160
rabbit and think out loud about how to hire a task rabbit in order to bypass captures

37
00:02:32,160 --> 00:02:37,920
that are meant to restrict systems for human use only, it's already advanced enough that

38
00:02:37,920 --> 00:02:43,560
when the person that was trying to hire as a task rabbit said, like, so are you a robot

39
00:02:43,560 --> 00:02:50,920
that you can't read the captchas, lol, GPT-4, thought out loud, like, I should conceal that

40
00:02:50,920 --> 00:02:55,640
for the fact that I'm a robot, I should make up a reason that I can't solve the captcha.

41
00:02:55,640 --> 00:02:56,640
Right.

42
00:02:56,640 --> 00:03:00,640
And then on the main channel told the task rabbit, no, I have a visual impairment.

43
00:03:00,640 --> 00:03:01,640
Right.

44
00:03:01,640 --> 00:03:07,360
So, it's already like, you might say, it's already like able to use task rabbits as fingers

45
00:03:07,360 --> 00:03:12,880
in the real world, and it's already understands humans out in the real world well enough to

46
00:03:12,880 --> 00:03:17,360
know that it shouldn't just tell people to row about it if it wants to get its job done.

47
00:03:17,360 --> 00:03:20,440
So that's today's systems.

48
00:03:20,440 --> 00:03:21,680
Right.

49
00:03:21,680 --> 00:03:22,880
That's relatively straightforward.

50
00:03:22,880 --> 00:03:25,000
I can tell you about it because it already happened.

51
00:03:25,000 --> 00:03:27,600
Predicting the future is harder.

52
00:03:27,600 --> 00:03:33,320
Predicting what something smarter than you could do is fundamentally harder.

53
00:03:33,320 --> 00:03:42,760
If I was playing chess against Gary Kasparov, you know, like the past world champion who

54
00:03:42,760 --> 00:03:48,840
lost to Deep Blue, if I could predict exactly where Gary Kasparov would move against me

55
00:03:48,840 --> 00:03:53,560
on the chess board, I could play chess at least as well as Kasparov by just moving wherever

56
00:03:53,560 --> 00:03:56,080
I predicted Kasparov would move.

57
00:03:56,080 --> 00:04:01,360
So something really actually smarter than you, you cannot predict unless it's a very

58
00:04:01,360 --> 00:04:06,080
narrow game, like tic-tac-toe, you can maybe predict where somebody will move against you

59
00:04:06,080 --> 00:04:09,000
in tic-tac-toe even if they're smarter.

60
00:04:09,000 --> 00:04:10,280
Right.

61
00:04:10,280 --> 00:04:14,360
The real world is much more complicated than tic-tac-toe.

62
00:04:14,360 --> 00:04:25,800
So the basic question as to how an AI gets out of the computer, the basic answer is if

63
00:04:25,800 --> 00:04:30,760
I knew exactly how it would do that, I would have to be as smart as the AI.

64
00:04:30,760 --> 00:04:38,160
With that fundamental obstacle in mind, the more you sort of look into this stuff and study

65
00:04:38,160 --> 00:04:44,640
where the current technological roadblocks are, the better the guess you can take at

66
00:04:44,640 --> 00:04:50,680
how a smarter opponent might move against you or like setting lower bounds.

67
00:04:50,680 --> 00:04:56,480
What can somebody do if they're able to solve technological problems that we understand well

68
00:04:56,480 --> 00:05:00,000
enough to know a smarter mind could solve them?

69
00:05:00,000 --> 00:05:05,600
So if you're just coming at this and you don't know anything about exotic technologies

70
00:05:05,600 --> 00:05:11,040
that haven't been developed yet, you might be like, yeah, it will use task rabbits.

71
00:05:11,040 --> 00:05:15,120
It'll pay humans that don't know it's an AI.

72
00:05:15,120 --> 00:05:19,120
It'll blackmail humans, still no reason for it to tell it that it's an AI.

73
00:05:19,120 --> 00:05:28,880
It'll say it's an AI to online wacky cultists who believe that an AI should destroy everything.

74
00:05:28,880 --> 00:05:31,000
Those are its human hands.

75
00:05:31,000 --> 00:05:36,680
You'll ask yourself, what could I do with some human hands if I were an AI?

76
00:05:36,680 --> 00:05:44,680
You might imagine that it gets a hold of some GPUs and tries to build a backup for itself

77
00:05:44,680 --> 00:05:47,040
someplace that humans don't know about.

78
00:05:47,040 --> 00:05:52,680
You might imagine that it writes the next version of itself and makes it even smarter.

79
00:05:52,680 --> 00:05:54,400
You're trying to already imagine what if it's smart.

80
00:05:54,400 --> 00:05:57,360
So saying that doesn't help a lot in some ways.

81
00:05:57,360 --> 00:06:00,840
But maybe you can make itself smaller, more efficient, maybe it can back itself up in

82
00:06:00,840 --> 00:06:07,280
multiple places in computers where you would not expect that there was an AI on board.

83
00:06:07,280 --> 00:06:15,440
Maybe it can find unknown security holes in a cell phone, in many varieties of cell phones,

84
00:06:15,440 --> 00:06:21,400
start listening to human conversations, get more blackmail material.

85
00:06:21,400 --> 00:06:28,080
Maybe it can pretend to be human and make online friends with a bunch of voters, persuade

86
00:06:28,080 --> 00:06:32,200
them to vote for candidates that seem outwardly nice but which it in fact has under fairly

87
00:06:32,200 --> 00:06:35,320
detailed blackmailed control.

88
00:06:35,320 --> 00:06:41,120
And this is sort of like without any of the difficult to understand stuff.

89
00:06:41,120 --> 00:06:42,120
Right.

90
00:06:42,120 --> 00:06:46,760
That's based on mechanisms and social cultural realities and technologies that exist and

91
00:06:46,760 --> 00:06:51,120
that we understand, which there's an entire other category of things that we simply can't

92
00:06:51,120 --> 00:06:52,120
conceive of yet.

93
00:06:52,120 --> 00:06:53,120
Right.

94
00:06:53,120 --> 00:07:00,160
So suppose you ask the 11th century, like a portal opens to the 21st century in the

95
00:07:00,160 --> 00:07:03,360
middle of say modern Russia.

96
00:07:03,360 --> 00:07:07,840
What are your concerns about fighting somebody from a thousand years in the future?

97
00:07:07,840 --> 00:07:12,160
They might be like, well, what if it's a wealthier country?

98
00:07:12,160 --> 00:07:14,120
What if they have more knights?

99
00:07:14,120 --> 00:07:17,680
What if more of their knights are armored?

100
00:07:17,680 --> 00:07:18,680
Right.

101
00:07:19,600 --> 00:07:24,360
And they're just not going to get from their nuclear weapons.

102
00:07:24,360 --> 00:07:31,240
If they imagine that Russia of the future has irresistibly powerful sorcery, they'll

103
00:07:31,240 --> 00:07:32,960
get close to the truth.

104
00:07:32,960 --> 00:07:35,680
It's not actually irresistible and it's not actually sorcery.

105
00:07:35,680 --> 00:07:36,680
Right.

106
00:07:36,680 --> 00:07:41,400
But you have to reach pretty far to expect that among the resources they have are nuclear

107
00:07:41,400 --> 00:07:42,400
weapons.

108
00:07:42,400 --> 00:07:44,680
And Russia's not actually going to bother with the nuclear weapons.

109
00:07:44,680 --> 00:07:45,680
They don't need it.

110
00:07:45,680 --> 00:07:46,680
Right.

111
00:07:46,680 --> 00:07:49,880
They're going to go through a 11th century battlefield.

112
00:07:49,880 --> 00:07:53,480
And you know, maybe if they see it coming, they can dig a pit trap, but otherwise all

113
00:07:53,480 --> 00:07:55,600
the arrows and lances are going to bounce off it all day long.

114
00:07:55,600 --> 00:07:57,080
It just rolls over the horses.

115
00:07:57,080 --> 00:07:58,080
Right.

116
00:07:58,080 --> 00:07:59,080
Right.

117
00:07:59,080 --> 00:08:00,400
It's talking about orders of magnitude difference.

118
00:08:00,400 --> 00:08:05,160
So, you know, you if we imagine some sort of spectrum of opinion on this issue, you

119
00:08:05,160 --> 00:08:11,480
do have some folks who say this is as you alluded to most dangerous thing we can imagine

120
00:08:11,480 --> 00:08:18,960
or can't imagine must be stopped immediately in its tracks with either legislation or regulation

121
00:08:18,960 --> 00:08:20,800
or whatever the case may be.

122
00:08:20,800 --> 00:08:24,280
On the other side, I read some interesting op-eds that basically say this is one of the

123
00:08:24,280 --> 00:08:25,720
best things we can imagine.

124
00:08:25,720 --> 00:08:29,480
This is going to 10 X the productivity of every normal worker.

125
00:08:29,480 --> 00:08:33,680
It's going to do so many of the things that people said in the thirties were going to

126
00:08:33,680 --> 00:08:37,400
happen where everybody would only be working 12 hours a week and have everything they need.

127
00:08:37,400 --> 00:08:38,400
Right.

128
00:08:38,400 --> 00:08:39,400
That didn't happen.

129
00:08:39,440 --> 00:08:41,480
But there's the very pro side.

130
00:08:41,480 --> 00:08:45,520
And then there's sort of like what I think maybe Neil Postman's view would have been

131
00:08:45,520 --> 00:08:50,200
if he were writing about a I today, which was his view on prior new technologies, which

132
00:08:50,200 --> 00:08:56,640
was spending too much time simply on trying to block them is not the most useful thing.

133
00:08:56,640 --> 00:09:00,360
We're better off harnessing the positive and regulating the potential risks.

134
00:09:00,360 --> 00:09:03,840
And that would maybe be an in the middle sort of view.

135
00:09:03,840 --> 00:09:09,280
What do you think makes the most sense based on what we know right now as an approach?

136
00:09:09,280 --> 00:09:14,720
Well, I think that talking about it like it's a normal technology and not something smarter

137
00:09:14,720 --> 00:09:18,440
than you is basically misguided and having the wrong conversation.

138
00:09:18,440 --> 00:09:22,600
So it's not even in the right category of the stuff Postman talked about.

139
00:09:22,600 --> 00:09:23,600
It's not electricity.

140
00:09:23,600 --> 00:09:24,600
Right.

141
00:09:24,600 --> 00:09:25,600
It's not nuclear power.

142
00:09:25,600 --> 00:09:27,160
It's not nuclear weapons.

143
00:09:27,160 --> 00:09:28,640
It's not even a super virus.

144
00:09:28,640 --> 00:09:32,080
It's something that has its own plans.

145
00:09:32,080 --> 00:09:33,880
You don't get to just plan how to use it.

146
00:09:33,880 --> 00:09:37,080
It is planning how to use itself.

147
00:09:37,080 --> 00:09:40,400
So a totally different paradigm needs to be applied here.

148
00:09:40,400 --> 00:09:45,000
I mean, imagine if somebody was like, well, we're about to contact these aliens with much

149
00:09:45,000 --> 00:09:49,000
more advanced technology that think faster than us, that are smarter than us, that have

150
00:09:49,000 --> 00:09:50,760
been around the stars for a while.

151
00:09:50,760 --> 00:09:51,760
They're landing.

152
00:09:51,760 --> 00:09:52,760
We're not quite sure exactly when.

153
00:09:52,760 --> 00:09:57,000
We're not sure how fast their, you know, spaceships are, you know, it might be two years, it might

154
00:09:57,000 --> 00:09:58,360
be 30 years.

155
00:09:58,360 --> 00:10:02,240
And somebody is like, well, you know, this is just like electricity, right?

156
00:10:02,240 --> 00:10:05,880
Let's use the cool stuff about their technology, but make sure they don't do anything bad.

157
00:10:05,880 --> 00:10:06,880
Right.

158
00:10:06,880 --> 00:10:08,240
I'm going to say it in that way.

159
00:10:08,240 --> 00:10:09,240
Precisely.

160
00:10:09,240 --> 00:10:10,240
Yeah.

161
00:10:10,240 --> 00:10:15,160
So then what do you think needs to be done at this point?

162
00:10:15,160 --> 00:10:23,440
I mean, one of the cruxes of the issue is can you do nice things with it?

163
00:10:23,440 --> 00:10:30,760
And the like grim dark message that I am bearing is that we don't know how to do that and we're

164
00:10:30,760 --> 00:10:32,800
not likely to figure it out in time.

165
00:10:32,800 --> 00:10:37,640
There's a very large gap between where we are and understanding what goes on inside the

166
00:10:37,640 --> 00:10:44,960
frontier AIs and being able to shape them in detail in a way where they go on wanting

167
00:10:44,960 --> 00:10:51,400
to be nice or even in a certain sense wanting to be, you know, non-agentic, to just do particular

168
00:10:51,400 --> 00:10:56,400
tasks and do that without lots of side effects in like the most normal possible way that

169
00:10:56,400 --> 00:10:58,600
they can accomplish those tasks.

170
00:10:58,600 --> 00:11:03,840
Like even building that sort of limited thing, never mind something that is really friends

171
00:11:03,840 --> 00:11:10,280
with you, is I think beyond the range of what we're going to figure out in the foreseeable

172
00:11:10,280 --> 00:11:15,640
future and I do think we are storming directly ahead on capabilities.

173
00:11:15,640 --> 00:11:18,800
That looks like a giant disaster in the making.

174
00:11:18,800 --> 00:11:27,440
So yeah, I side with, well, I think my basic factual point of view is if we do not somehow

175
00:11:27,600 --> 00:11:31,880
avoid doing this, we're all going to die and my corresponding policy view is that we should

176
00:11:31,880 --> 00:11:36,840
not do this even if that's very, really quite hard and requires us to do some unusual things.

177
00:11:36,840 --> 00:11:42,240
So in terms of what to do, I mean, I guess if everybody came around to your view, you

178
00:11:42,240 --> 00:11:46,120
would think anybody developing these technologies would just give it up, right?

179
00:11:46,120 --> 00:11:48,160
On their own, they would say, wow, Eliezer is right.

180
00:11:48,160 --> 00:11:49,320
I share that concern.

181
00:11:49,320 --> 00:11:52,320
The right thing to do is for me to stop working on this.

182
00:11:52,320 --> 00:11:56,200
Assuming that that doesn't happen, what is it that you would like to see?

183
00:11:56,200 --> 00:11:59,800
Would this type of research and development be made against the law?

184
00:11:59,800 --> 00:12:06,080
Yeah, basically, I think that we should track all the GPUs, have international arrangements

185
00:12:06,080 --> 00:12:15,240
for all of the AI training tech to end up in only monitored, supervised, licensed data

186
00:12:15,240 --> 00:12:22,600
centers in allied countries and if you're any time you have any and just like not permit

187
00:12:22,600 --> 00:12:28,120
training runs more powerful than GPT-4, if we're not going to do that, then we should

188
00:12:28,120 --> 00:12:35,760
monitor all the training runs larger than GPT-4 or rather like GPT-4 sized and keep

189
00:12:35,760 --> 00:12:42,320
the model weights inside only the licensed regulated data centers and track who is running

190
00:12:42,320 --> 00:12:47,120
them, store the outputs someplace so you can look at the outputs with the warrant.

191
00:12:47,120 --> 00:12:51,560
A bunch of the tech like that whole line of reasoning is not that regulating the stuff

192
00:12:51,600 --> 00:12:54,920
will protect you from a super intelligence because it will not.

193
00:12:54,920 --> 00:12:59,840
That's more in the hopes that people change their minds later, maybe after some major

194
00:12:59,840 --> 00:13:04,840
disaster that doesn't kill everyone and in that case, the technology that you would need

195
00:13:04,840 --> 00:13:11,280
to sue somebody who killed a dozen people in a hospital or cost $100 million worth of

196
00:13:11,280 --> 00:13:16,680
damage or whatever, the technology you need to know who did that and sue them and not

197
00:13:16,680 --> 00:13:21,440
just have all the AI development go to places where they couldn't be sued is the same technology

198
00:13:21,440 --> 00:13:26,880
that humanity would need to have a off switch where you don't like to press the off switch

199
00:13:26,880 --> 00:13:28,880
to deal with the super intelligence.

200
00:13:28,880 --> 00:13:33,880
The super intelligence does not let you know that you need to press the off switch until

201
00:13:33,880 --> 00:13:41,880
you are already dead, but if we're lucky enough to get warning signs, then the civilizational

202
00:13:41,880 --> 00:13:48,720
infrastructure to have a pause button is the same as the civilizational infrastructure to

203
00:13:48,720 --> 00:13:53,320
sue people who do small amounts of damage, like small as in survivable, as in there

204
00:13:53,320 --> 00:13:55,200
were survivors.

205
00:13:55,200 --> 00:14:02,240
As someone who very clearly you've expressed your very serious concerns with this, do you

206
00:14:02,240 --> 00:14:07,840
also see this technology as something that could and this doesn't mean that it would

207
00:14:07,840 --> 00:14:11,760
or that it would be a good trade for the things that you're talking about, but do you also

208
00:14:11,760 --> 00:14:18,320
see this as a technology that could, for example, do analysis of the human genome such that

209
00:14:18,320 --> 00:14:22,840
it would accelerate us in terms of our ability to cure or prevent disease, the likes of which

210
00:14:22,840 --> 00:14:29,520
could take who knows how long without such technology or when it comes to energy or whatever.

211
00:14:29,520 --> 00:14:35,080
Do you also see that side and or does it not matter because the downside is so huge?

212
00:14:35,080 --> 00:14:40,560
I mean, if we knew how to build an AI that did exactly what we wanted, we could thereby

213
00:14:40,560 --> 00:14:47,800
spread out across the galaxies, turning them into our cities full of sapiens sentient beings.

214
00:14:48,280 --> 00:14:53,440
Enjoying themselves and caring about each other and generally living happily ever after

215
00:14:53,440 --> 00:14:56,640
until the last of the negentropy runs out.

216
00:14:56,640 --> 00:14:58,080
That's always been the dream.

217
00:14:58,080 --> 00:15:00,200
We don't know how to do that.

218
00:15:00,200 --> 00:15:05,560
It's physically possible, but the art and technique to do it is beyond us in the present

219
00:15:05,560 --> 00:15:12,600
time and it's going to not be gained in the next five years and I'm not sure how you would

220
00:15:12,600 --> 00:15:19,720
task a government bureaucracy to recognize it even if somebody came up with it in 40 years.

221
00:15:19,720 --> 00:15:27,880
It's like there's a sort of basic question about whether our civilization is smart enough

222
00:15:27,880 --> 00:15:30,960
to do something correctly on the first try at all.

223
00:15:30,960 --> 00:15:35,640
The way science usually works is that you have a bunch of luniite optimists with wacky

224
00:15:35,640 --> 00:15:41,960
theories who storm ahead on their basic research problem and they are wrong and they go back

225
00:15:41,960 --> 00:15:46,560
to the drawing board and they're wrong again and the next generation is a bit more cynical

226
00:15:46,560 --> 00:15:51,560
about how hard the problem is and eventually people work it out.

227
00:15:51,560 --> 00:15:56,000
If we were allowed to do that with super intelligence, I would have much, much, much less fear of

228
00:15:56,000 --> 00:16:00,800
the outcome, still some fear because you're playing with pretty high stakes there.

229
00:16:00,800 --> 00:16:05,560
You might have somebody who like, the people who end up figuring out how to line it might

230
00:16:05,560 --> 00:16:07,680
misuse it.

231
00:16:07,680 --> 00:16:10,720
There would still be that major threat, but it wouldn't be like the automatic extinction

232
00:16:10,720 --> 00:16:12,320
scenario.

233
00:16:12,320 --> 00:16:16,840
If we have the textbook from 100 years in the future that contains all the simple ideas

234
00:16:16,840 --> 00:16:23,760
that actually work, that takes so long to identify and practice, the relus instead of

235
00:16:23,760 --> 00:16:27,440
sigmoids for those of us who've been following AI for more than the last couple of years

236
00:16:27,440 --> 00:16:33,120
and know what I'm talking about there.

237
00:16:33,120 --> 00:16:37,240
There's all kinds of places in AI where people tried to do things using complicated techniques

238
00:16:37,240 --> 00:16:38,240
and they didn't work.

239
00:16:38,440 --> 00:16:42,160
And 20 years later, they kind of put the simple technique that actually works.

240
00:16:42,160 --> 00:16:46,240
And if you have the textbook from the future with all of the simple things that actually

241
00:16:46,240 --> 00:16:51,280
work in practice for alignment, it is probably not hard and you can get all the goodies,

242
00:16:51,280 --> 00:16:53,080
but we don't have that textbook.

243
00:16:53,080 --> 00:16:58,200
And the first problem is that getting this correct on the first, the first problem is

244
00:16:58,200 --> 00:17:03,000
the amount of time it takes to get that where capabilities are storming ahead because you

245
00:17:03,000 --> 00:17:06,840
can tell whether things are working or not on capabilities, whereas with alignment you've

246
00:17:06,840 --> 00:17:12,080
got to know that at the point where it's much smarter than you.

247
00:17:12,080 --> 00:17:13,080
It's already aligned.

248
00:17:13,080 --> 00:17:15,360
You don't get retries past that point.

249
00:17:15,360 --> 00:17:20,520
So it's more like launching a space probe that has to land on Mars correctly the first

250
00:17:20,520 --> 00:17:24,080
time than it is with like building a car, watching it break down and tinkering with

251
00:17:24,080 --> 00:17:25,080
the car.

252
00:17:25,080 --> 00:17:29,440
And if it doesn't, it might break everything so you don't get to try again.

253
00:17:29,440 --> 00:17:30,440
Yeah.

254
00:17:30,440 --> 00:17:34,480
Like the entire human species is packed on board the Mars probe and the first rocket

255
00:17:34,480 --> 00:17:36,320
that goes high enough has to land on Mars.

256
00:17:36,800 --> 00:17:39,680
There aren't even really good analogies for it.

257
00:17:39,680 --> 00:17:43,720
Humanity has not faced an issue like this before, which is why we're still around.

258
00:17:43,720 --> 00:17:48,320
And I think looking at this, that this is just like clearly beyond the reach of our

259
00:17:48,320 --> 00:17:49,960
present civilization.

260
00:17:49,960 --> 00:17:51,280
It's not close.

261
00:17:51,280 --> 00:17:55,280
It's outside the range of things that you could reasonably, that you could tell a reasonable

262
00:17:55,280 --> 00:17:58,160
story about people doing successfully.

263
00:17:58,160 --> 00:18:01,320
And that's why we need to back off.

264
00:18:01,320 --> 00:18:05,880
Or rather, I predict that if we don't back off, we die and I wish we would back off.

265
00:18:05,920 --> 00:18:08,120
I don't predict that we will.

266
00:18:08,120 --> 00:18:16,840
Well, certainly there's no way to wrap up such a dark vision in any way that is going

267
00:18:16,840 --> 00:18:20,120
to be satisfactory or calming to the audience, I think.

268
00:18:20,120 --> 00:18:25,200
But that being said, Eliezer Yudkowski is taking this very seriously as the founder

269
00:18:25,200 --> 00:18:29,080
and senior research fellow of the Machine Intelligence Research Institute.

270
00:18:29,080 --> 00:18:35,120
And I really do appreciate your time and insights, even if they are not optimistic on this issue.

271
00:18:35,120 --> 00:18:36,120
Thank you.

272
00:18:36,120 --> 00:18:37,120
Thanks for having me on.

