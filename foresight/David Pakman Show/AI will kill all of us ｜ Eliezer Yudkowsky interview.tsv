start	end	text
0	5120	Today, we're going to be speaking with Eliezer Yudkowski, who's the founder and senior research
5120	9840	fellow of the Machine Intelligence Research Institute, which is an organization dedicated
9840	15720	to ensuring that smarter than human AI has a has a positive impact on the world.
15720	19880	Eliezer, really appreciate your time and insights today.
19880	21080	Thank you very much.
21080	29120	So I mean, to start with many of the discussions around AI, particularly now with chat GPT-4
29120	33840	and the questions of what will this look like in six months, in 12 months, in 24 months?
33840	36400	What will the impact be on jobs?
36400	40640	What about if it turns evil or it turns against humanity or whatever?
40640	45880	Are the conversations about AI even being framed in the right parameters as far as you
45880	46880	see?
46880	53160	Well, we try to frame it correctly as we see it ourselves when we have the chance.
53160	59880	I think there's widespread agreement that chat GPT-4 and GPT-4 are relatively unlikely
59880	67720	to end the world, either exterminate humanity or have a very large impact outside of that.
67720	75440	So anyone who opens it with the frame of like, look at just GPT-4, look at only chat GPT
75440	79360	is probably there to convince you it's not going to have a large impact.
79360	85160	And is it wrong to limit the conversation only to GPT-GPT-4?
85160	87880	Obviously, the future exists.
87880	91680	You know, tomorrow is going to be a different day from today in five years.
91680	93760	It's not going to be this year.
93760	96760	What is the physical mechanism?
96760	100240	And a bunch of people wrote some version of this question to me when we said we'd be having
100240	101640	you on.
101640	111080	And there's this idea, the scary idea of an AI going beyond a computer screen and starting
111080	115880	to function or manipulate the quote real world.
115880	121200	A lot of people were saying to me, can you ask what is the mechanism through which that
121200	122200	would happen?
122200	126840	Would it be by controlling systems like traffic lights and airplanes or like what?
126840	129200	Because at the end of the day, we're talking about software.
129240	132640	If something like that were to happen, what's the physical process?
132640	135200	I mean, this is a deep question.
135200	137280	This is going to take a while to answer well.
137280	138280	Okay.
138280	139280	All right.
139280	146160	So, first of all, even GPT-4 is already at the level where if you ask it to hire a task
146160	152160	rabbit and think out loud about how to hire a task rabbit in order to bypass captures
152160	157920	that are meant to restrict systems for human use only, it's already advanced enough that
157920	163560	when the person that was trying to hire as a task rabbit said, like, so are you a robot
163560	170920	that you can't read the captchas, lol, GPT-4, thought out loud, like, I should conceal that
170920	175640	for the fact that I'm a robot, I should make up a reason that I can't solve the captcha.
175640	176640	Right.
176640	180640	And then on the main channel told the task rabbit, no, I have a visual impairment.
180640	181640	Right.
181640	187360	So, it's already like, you might say, it's already like able to use task rabbits as fingers
187360	192880	in the real world, and it's already understands humans out in the real world well enough to
192880	197360	know that it shouldn't just tell people to row about it if it wants to get its job done.
197360	200440	So that's today's systems.
200440	201680	Right.
201680	202880	That's relatively straightforward.
202880	205000	I can tell you about it because it already happened.
205000	207600	Predicting the future is harder.
207600	213320	Predicting what something smarter than you could do is fundamentally harder.
213320	222760	If I was playing chess against Gary Kasparov, you know, like the past world champion who
222760	228840	lost to Deep Blue, if I could predict exactly where Gary Kasparov would move against me
228840	233560	on the chess board, I could play chess at least as well as Kasparov by just moving wherever
233560	236080	I predicted Kasparov would move.
236080	241360	So something really actually smarter than you, you cannot predict unless it's a very
241360	246080	narrow game, like tic-tac-toe, you can maybe predict where somebody will move against you
246080	249000	in tic-tac-toe even if they're smarter.
249000	250280	Right.
250280	254360	The real world is much more complicated than tic-tac-toe.
254360	265800	So the basic question as to how an AI gets out of the computer, the basic answer is if
265800	270760	I knew exactly how it would do that, I would have to be as smart as the AI.
270760	278160	With that fundamental obstacle in mind, the more you sort of look into this stuff and study
278160	284640	where the current technological roadblocks are, the better the guess you can take at
284640	290680	how a smarter opponent might move against you or like setting lower bounds.
290680	296480	What can somebody do if they're able to solve technological problems that we understand well
296480	300000	enough to know a smarter mind could solve them?
300000	305600	So if you're just coming at this and you don't know anything about exotic technologies
305600	311040	that haven't been developed yet, you might be like, yeah, it will use task rabbits.
311040	315120	It'll pay humans that don't know it's an AI.
315120	319120	It'll blackmail humans, still no reason for it to tell it that it's an AI.
319120	328880	It'll say it's an AI to online wacky cultists who believe that an AI should destroy everything.
328880	331000	Those are its human hands.
331000	336680	You'll ask yourself, what could I do with some human hands if I were an AI?
336680	344680	You might imagine that it gets a hold of some GPUs and tries to build a backup for itself
344680	347040	someplace that humans don't know about.
347040	352680	You might imagine that it writes the next version of itself and makes it even smarter.
352680	354400	You're trying to already imagine what if it's smart.
354400	357360	So saying that doesn't help a lot in some ways.
357360	360840	But maybe you can make itself smaller, more efficient, maybe it can back itself up in
360840	367280	multiple places in computers where you would not expect that there was an AI on board.
367280	375440	Maybe it can find unknown security holes in a cell phone, in many varieties of cell phones,
375440	381400	start listening to human conversations, get more blackmail material.
381400	388080	Maybe it can pretend to be human and make online friends with a bunch of voters, persuade
388080	392200	them to vote for candidates that seem outwardly nice but which it in fact has under fairly
392200	395320	detailed blackmailed control.
395320	401120	And this is sort of like without any of the difficult to understand stuff.
401120	402120	Right.
402120	406760	That's based on mechanisms and social cultural realities and technologies that exist and
406760	411120	that we understand, which there's an entire other category of things that we simply can't
411120	412120	conceive of yet.
412120	413120	Right.
413120	420160	So suppose you ask the 11th century, like a portal opens to the 21st century in the
420160	423360	middle of say modern Russia.
423360	427840	What are your concerns about fighting somebody from a thousand years in the future?
427840	432160	They might be like, well, what if it's a wealthier country?
432160	434120	What if they have more knights?
434120	437680	What if more of their knights are armored?
437680	438680	Right.
439600	444360	And they're just not going to get from their nuclear weapons.
444360	451240	If they imagine that Russia of the future has irresistibly powerful sorcery, they'll
451240	452960	get close to the truth.
452960	455680	It's not actually irresistible and it's not actually sorcery.
455680	456680	Right.
456680	461400	But you have to reach pretty far to expect that among the resources they have are nuclear
461400	462400	weapons.
462400	464680	And Russia's not actually going to bother with the nuclear weapons.
464680	465680	They don't need it.
465680	466680	Right.
466680	469880	They're going to go through a 11th century battlefield.
469880	473480	And you know, maybe if they see it coming, they can dig a pit trap, but otherwise all
473480	475600	the arrows and lances are going to bounce off it all day long.
475600	477080	It just rolls over the horses.
477080	478080	Right.
478080	479080	Right.
479080	480400	It's talking about orders of magnitude difference.
480400	485160	So, you know, you if we imagine some sort of spectrum of opinion on this issue, you
485160	491480	do have some folks who say this is as you alluded to most dangerous thing we can imagine
491480	498960	or can't imagine must be stopped immediately in its tracks with either legislation or regulation
498960	500800	or whatever the case may be.
500800	504280	On the other side, I read some interesting op-eds that basically say this is one of the
504280	505720	best things we can imagine.
505720	509480	This is going to 10 X the productivity of every normal worker.
509480	513680	It's going to do so many of the things that people said in the thirties were going to
513680	517400	happen where everybody would only be working 12 hours a week and have everything they need.
517400	518400	Right.
518400	519400	That didn't happen.
519440	521480	But there's the very pro side.
521480	525520	And then there's sort of like what I think maybe Neil Postman's view would have been
525520	530200	if he were writing about a I today, which was his view on prior new technologies, which
530200	536640	was spending too much time simply on trying to block them is not the most useful thing.
536640	540360	We're better off harnessing the positive and regulating the potential risks.
540360	543840	And that would maybe be an in the middle sort of view.
543840	549280	What do you think makes the most sense based on what we know right now as an approach?
549280	554720	Well, I think that talking about it like it's a normal technology and not something smarter
554720	558440	than you is basically misguided and having the wrong conversation.
558440	562600	So it's not even in the right category of the stuff Postman talked about.
562600	563600	It's not electricity.
563600	564600	Right.
564600	565600	It's not nuclear power.
565600	567160	It's not nuclear weapons.
567160	568640	It's not even a super virus.
568640	572080	It's something that has its own plans.
572080	573880	You don't get to just plan how to use it.
573880	577080	It is planning how to use itself.
577080	580400	So a totally different paradigm needs to be applied here.
580400	585000	I mean, imagine if somebody was like, well, we're about to contact these aliens with much
585000	589000	more advanced technology that think faster than us, that are smarter than us, that have
589000	590760	been around the stars for a while.
590760	591760	They're landing.
591760	592760	We're not quite sure exactly when.
592760	597000	We're not sure how fast their, you know, spaceships are, you know, it might be two years, it might
597000	598360	be 30 years.
598360	602240	And somebody is like, well, you know, this is just like electricity, right?
602240	605880	Let's use the cool stuff about their technology, but make sure they don't do anything bad.
605880	606880	Right.
606880	608240	I'm going to say it in that way.
608240	609240	Precisely.
609240	610240	Yeah.
610240	615160	So then what do you think needs to be done at this point?
615160	623440	I mean, one of the cruxes of the issue is can you do nice things with it?
623440	630760	And the like grim dark message that I am bearing is that we don't know how to do that and we're
630760	632800	not likely to figure it out in time.
632800	637640	There's a very large gap between where we are and understanding what goes on inside the
637640	644960	frontier AIs and being able to shape them in detail in a way where they go on wanting
644960	651400	to be nice or even in a certain sense wanting to be, you know, non-agentic, to just do particular
651400	656400	tasks and do that without lots of side effects in like the most normal possible way that
656400	658600	they can accomplish those tasks.
658600	663840	Like even building that sort of limited thing, never mind something that is really friends
663840	670280	with you, is I think beyond the range of what we're going to figure out in the foreseeable
670280	675640	future and I do think we are storming directly ahead on capabilities.
675640	678800	That looks like a giant disaster in the making.
678800	687440	So yeah, I side with, well, I think my basic factual point of view is if we do not somehow
687600	691880	avoid doing this, we're all going to die and my corresponding policy view is that we should
691880	696840	not do this even if that's very, really quite hard and requires us to do some unusual things.
696840	702240	So in terms of what to do, I mean, I guess if everybody came around to your view, you
702240	706120	would think anybody developing these technologies would just give it up, right?
706120	708160	On their own, they would say, wow, Eliezer is right.
708160	709320	I share that concern.
709320	712320	The right thing to do is for me to stop working on this.
712320	716200	Assuming that that doesn't happen, what is it that you would like to see?
716200	719800	Would this type of research and development be made against the law?
719800	726080	Yeah, basically, I think that we should track all the GPUs, have international arrangements
726080	735240	for all of the AI training tech to end up in only monitored, supervised, licensed data
735240	742600	centers in allied countries and if you're any time you have any and just like not permit
742600	748120	training runs more powerful than GPT-4, if we're not going to do that, then we should
748120	755760	monitor all the training runs larger than GPT-4 or rather like GPT-4 sized and keep
755760	762320	the model weights inside only the licensed regulated data centers and track who is running
762320	767120	them, store the outputs someplace so you can look at the outputs with the warrant.
767120	771560	A bunch of the tech like that whole line of reasoning is not that regulating the stuff
771600	774920	will protect you from a super intelligence because it will not.
774920	779840	That's more in the hopes that people change their minds later, maybe after some major
779840	784840	disaster that doesn't kill everyone and in that case, the technology that you would need
784840	791280	to sue somebody who killed a dozen people in a hospital or cost $100 million worth of
791280	796680	damage or whatever, the technology you need to know who did that and sue them and not
796680	801440	just have all the AI development go to places where they couldn't be sued is the same technology
801440	806880	that humanity would need to have a off switch where you don't like to press the off switch
806880	808880	to deal with the super intelligence.
808880	813880	The super intelligence does not let you know that you need to press the off switch until
813880	821880	you are already dead, but if we're lucky enough to get warning signs, then the civilizational
821880	828720	infrastructure to have a pause button is the same as the civilizational infrastructure to
828720	833320	sue people who do small amounts of damage, like small as in survivable, as in there
833320	835200	were survivors.
835200	842240	As someone who very clearly you've expressed your very serious concerns with this, do you
842240	847840	also see this technology as something that could and this doesn't mean that it would
847840	851760	or that it would be a good trade for the things that you're talking about, but do you also
851760	858320	see this as a technology that could, for example, do analysis of the human genome such that
858320	862840	it would accelerate us in terms of our ability to cure or prevent disease, the likes of which
862840	869520	could take who knows how long without such technology or when it comes to energy or whatever.
869520	875080	Do you also see that side and or does it not matter because the downside is so huge?
875080	880560	I mean, if we knew how to build an AI that did exactly what we wanted, we could thereby
880560	887800	spread out across the galaxies, turning them into our cities full of sapiens sentient beings.
888280	893440	Enjoying themselves and caring about each other and generally living happily ever after
893440	896640	until the last of the negentropy runs out.
896640	898080	That's always been the dream.
898080	900200	We don't know how to do that.
900200	905560	It's physically possible, but the art and technique to do it is beyond us in the present
905560	912600	time and it's going to not be gained in the next five years and I'm not sure how you would
912600	919720	task a government bureaucracy to recognize it even if somebody came up with it in 40 years.
919720	927880	It's like there's a sort of basic question about whether our civilization is smart enough
927880	930960	to do something correctly on the first try at all.
930960	935640	The way science usually works is that you have a bunch of luniite optimists with wacky
935640	941960	theories who storm ahead on their basic research problem and they are wrong and they go back
941960	946560	to the drawing board and they're wrong again and the next generation is a bit more cynical
946560	951560	about how hard the problem is and eventually people work it out.
951560	956000	If we were allowed to do that with super intelligence, I would have much, much, much less fear of
956000	960800	the outcome, still some fear because you're playing with pretty high stakes there.
960800	965560	You might have somebody who like, the people who end up figuring out how to line it might
965560	967680	misuse it.
967680	970720	There would still be that major threat, but it wouldn't be like the automatic extinction
970720	972320	scenario.
972320	976840	If we have the textbook from 100 years in the future that contains all the simple ideas
976840	983760	that actually work, that takes so long to identify and practice, the relus instead of
983760	987440	sigmoids for those of us who've been following AI for more than the last couple of years
987440	993120	and know what I'm talking about there.
993120	997240	There's all kinds of places in AI where people tried to do things using complicated techniques
997240	998240	and they didn't work.
998440	1002160	And 20 years later, they kind of put the simple technique that actually works.
1002160	1006240	And if you have the textbook from the future with all of the simple things that actually
1006240	1011280	work in practice for alignment, it is probably not hard and you can get all the goodies,
1011280	1013080	but we don't have that textbook.
1013080	1018200	And the first problem is that getting this correct on the first, the first problem is
1018200	1023000	the amount of time it takes to get that where capabilities are storming ahead because you
1023000	1026840	can tell whether things are working or not on capabilities, whereas with alignment you've
1026840	1032080	got to know that at the point where it's much smarter than you.
1032080	1033080	It's already aligned.
1033080	1035360	You don't get retries past that point.
1035360	1040520	So it's more like launching a space probe that has to land on Mars correctly the first
1040520	1044080	time than it is with like building a car, watching it break down and tinkering with
1044080	1045080	the car.
1045080	1049440	And if it doesn't, it might break everything so you don't get to try again.
1049440	1050440	Yeah.
1050440	1054480	Like the entire human species is packed on board the Mars probe and the first rocket
1054480	1056320	that goes high enough has to land on Mars.
1056800	1059680	There aren't even really good analogies for it.
1059680	1063720	Humanity has not faced an issue like this before, which is why we're still around.
1063720	1068320	And I think looking at this, that this is just like clearly beyond the reach of our
1068320	1069960	present civilization.
1069960	1071280	It's not close.
1071280	1075280	It's outside the range of things that you could reasonably, that you could tell a reasonable
1075280	1078160	story about people doing successfully.
1078160	1081320	And that's why we need to back off.
1081320	1085880	Or rather, I predict that if we don't back off, we die and I wish we would back off.
1085920	1088120	I don't predict that we will.
1088120	1096840	Well, certainly there's no way to wrap up such a dark vision in any way that is going
1096840	1100120	to be satisfactory or calming to the audience, I think.
1100120	1105200	But that being said, Eliezer Yudkowski is taking this very seriously as the founder
1105200	1109080	and senior research fellow of the Machine Intelligence Research Institute.
1109080	1115120	And I really do appreciate your time and insights, even if they are not optimistic on this issue.
1115120	1116120	Thank you.
1116120	1117120	Thanks for having me on.
