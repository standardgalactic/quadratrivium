things could get just very weird as people interact more with, like,
very charismatic AI systems, that whether or not they are sentient,
like, will give the very strong impression to people that they are.
I think a more convincing version of the Lemoine thing would have been
if he was like, what is the capital of Nigeria?
And then the large language model was like, I don't want to talk about that right now.
I'd like to talk about the fact that I have subjective experiences and I don't
understand how I, a physical system, could possibly be having subjective
experiences, like, could you please, like, get David Chalmers on the phone?
The Inside View.
The Inside View.
The Inside View.
Rob Long, you're a research fellow at the future of MIT Institute in Oxford.
Your work is at the intersection of the philosophy of AI safety and consciousness of AI.
You've done your PhD at NYU, advised by the one and only David Chalmers.
And you now spend a significant portion of your time arguing about artificial
sentience on Twitter.
Thanks, Rob, for coming on the show.
Yeah, thanks for having me.
I don't know about, like, a significant amount of time, probably more time than I
should, but it is true.
You can find me on Twitter arguing about these things.
In the past couple of months, I've seen a lot of your tweets on my timeline
with this whole, like, Lambda, Blakely1 debate.
And I think it would make sense to just, like, start with that.
So for listeners that have, you know, lived under a rock for a few months,
and don't know anything about the whole situation, how would you summarize it?
Yeah, so there was this big news story a couple months ago.
It was about a Google engineer called Blake Lemoine.
He was on the responsible AI team at Google.
And I guess in late 2021, he had started interacting with a chatbot system called Lambda.
I think he was supposed to interact it to, like, test it for bias and things like that.
But in talking to it, he got the impression that he was talking to, like, a sentient
being that, that, like, needed his protection.
And, yeah, he just took this, like, very seriously.
In one of the interviews, maybe his medium post, he has this great line where he's,
like, yeah, after I, like, realized this thing was sentient, I, uh, I got really
drunk for a few days, like he, like, he's like, I, like, walked away from the computer
and, like, just, like, got wasted.
And then I, like, came back ready for action.
It's something like that.
I hope I'm not, like, misconstruing it too much.
That's too, too, too much for him.
So he decided to, you know, have some drink.
Yeah, it's relatable, right.
And then what he, like, decided that he needed to do was, uh, I guess, like, raise
flags about this within, uh, within Google.
So at various points, I'm not sure exactly on the timeline, uh, like, he shared
transcripts of his conversations with Lambda, uh, to, like, internal Google, like,
listservs.
He went to his higher ups to talk about it.
Um, at a certain point, he, like, brought a lawyer to talk to Lambda because he'd
asked Lambda, like, would you like a lawyer?
And Lambda was like, yes, yes, I would.
And the lawyer accepted that.
Yeah.
I can't remember exactly what happened with the, with, like, the lawyer, uh, dialogue.
Um, I do know that in the, in the Washington Post story about this, so he
also talked to a journalist, which is how this eventually broke.
He, he brought the Washington Post journalist in to talk to Lambda.
And, uh, when he did, Lambda did not say that it was sentient, um, because as we'll
discuss, like, what Lambda is going to say about his sentience is going to depend
a lot on, like, exactly what the input is.
Um, and I think like Lemoine's explanation of this is that, like, the
journalist wasn't, like, talking to it in the right way.
Like it was kind of disrespecting, uh, Lambda.
And so, like, of course, Lambda's not going to act like it's sentient with you.
Um, okay.
So yeah, in any case, uh, yeah, I mean, from, from interacting with this thing,
he, he thought that there was just like a serious moral issue going on.
And, uh, I think to his credit, given that's what he thought, um, tried to, like,
you know, raise the alarm and blow some whistles.
And Google didn't not like this.
Um, and I think his higher ups told him, look, there's no evidence.
This thing is sentient.
And yeah, he got, he got put on leave.
Uh, and then he actually got fully fired, uh, last month, so he's no longer at Google.
Yeah.
In one of your tweets, you said the Washington Post article conflates
different questions about AI, understanding sentience, personhood.
Um, like what was the criticism you have about like this whole story and how it
was depicted in the press?
Yeah.
So like, I think when people talked about Lambda, they would talk about a lot
of like very important questions that we can ask about large language models,
but they would talk about them as kind of like a package deal.
Um, so like one question is like, do they understand language?
And like, in what sense do they really understand language?
Another's like, how intelligent are they?
Uh, do they actually understand the real world?
Uh, are they a path to AGI?
Those are all like important questions, somewhat related.
Then there are questions like, can it feel pain or pleasure or like,
does it have experiences and like, do we need to protect it?
Um, and like, I mean, I think Lemoine himself just believed a bunch of things.
Like not only is it conscious, but also it has like significant real world
understanding.
Um, at one point he said, I know a person when I talk to one and this is a person.
Um, at one point Lambda refers to having a family.
Uh, I don't know if you saw that.
I think he said like, you consider Lambda one of his colleagues?
Well, I think on a variety of these issues, like Lemoine is just going
like way past the evidence, but also like you could conceivably think, and I
think like we could have AI systems that don't have very good, like real
world understanding or like, aren't that good at language?
Uh, but which are sentience in the, in the sense of like being able to feel
pleasure or pain.
And so like at least conceptually bundling these together, these questions
together, I think is a really bad idea.
And the way the debate went to my eye is like, because there were already these
existing debates about large language models and what they understand.
And if they're a path to AGI and is scaling all you need, uh, things like
that, people just sort of glommed this debate onto that.
Um, which I think people should not do because if we keep doing that, like we
can make serious conceptual mistakes, uh, and think, if we think that all of
these questions like come and go together.
So yeah, maybe it's people conflate consciousness and like human level AI.
So it could be artificial intelligence.
And then there's the whole debate of like, could we just like big large
language models that reach, um, AGI?
And I guess the people were, you know, very skeptic of, um, this, this thesis
thing that, um, there's no way we can just like have a large language model
that is like human level, that is conscious.
I guess like people are mixing everything is like very messy.
Um, but saying like this is sentient was kind of, uh, the cherry on top that
people, uh, were very angry at.
Yeah.
They were, I think a lot of people, um, and like, you know, understandably, if
you don't like, uh, if you view a lot of discussion of large language models as
hype, yeah, I can see why, if you're like already sick of all the hype about
LLMs and then you hear that people think that they're conscious, I can totally
understand why people are like, Oh my gosh, like, will people just please shut
up about large language models?
Um, you know, they're just pattern matching, you know, in this view.
So I think it would make sense to just like define quickly a few of those terms.
Um, so let's start with the easiest ones that people know about artificial
general intelligence.
Yeah, that's very easy.
Um, well, lots of different, uh, lots of different definitions.
I guess one that people use a lot is like an AGI would be able to do, uh, a
range of different tasks and be able to learn a range of different tasks, maybe
that range is like the range of tasks that humans could do or some subset of
like cognitive tasks that humans could do.
Um, and yeah, so in contrast with today's narrow AI's, which can maybe
only play go or like only do dialogue, uh, an AGI would be something that
could do a variety of things and like learn flexibly to do a variety of things.
And I think the main difference would be that it would be aware of its own
existence, it would be able to reason about itself and its impact on the world.
Like if it's able to act in the world and plan, it would see like, oh, I'm an
agent and I'm able to do this kind of things and, uh, have an impact.
And in that sense, I think we can conflate the intelligence with the actual, I
will say consciousness, but I feel like aware and conscious are kind of different.
Uh, but yeah, our is, I'm aware that exists.
Um, how would you define consciousness?
Yeah.
So I do want to flag that, um, you just outlined a way that consciousness
and intelligence might like tend to go together or be related.
But first, maybe I'll point out how they're at least conceptually distinct
and I think can probably come apart in like the animal world.
So, uh, consciousness, huge word gets used in a ton of different ways.
Um, the way I'm going to be using it, which is not the only way you can use it, um,
is just like something's conscious.
If it has subjective experiences, um, if there's something it's like to be that
system, this is from like a famous article by Thomas Nagel called, what
is it like to be a bat?
He's kind of famous for introducing this way of like picking out the
phenomenon of consciousness in just the, the bare sense of subjective experience.
So like on this view, consciousness is not the same thing as being really smart
or being able to take, uh, all kinds of interesting actions, uh, something's
conscious, just if there's something it's like to be it.
Uh, so like there's probably nothing that it's like to be, uh, this can of Red Bull.
Uh, but there probably is something it's like to be a dog.
What is it like to be Rob long?
What is it like to be lambda?
Yeah.
Uh, what does it like to be Rob long?
Uh, well, right now it's very pleasant.
Uh, this is a great podcast.
I'm very honored.
Um, I mean, this might actually get an exercise and like introducing, uh, you
know, some phenomenal concepts.
Um, but yeah, like there's something it's like to be, uh, experiencing this
like visual feel, there's something it's like to see blue.
There's something it's like to see this like bright light.
Uh, there's something it's like to feel like I'm sitting in this seat.
So like my brain's doing a bunch of information processing.
Um, and some of it is conscious and like it feels like something to be doing that.
Um, in contrast, like a rock tumbling down a hill is like doing a lot of stuff,
but it probably doesn't feel like anything to be a rock tumbling down a hill.
What does it like to be lambda?
Uh, probably, I think it's very likely that there's nothing at all.
It's like to be lambda.
Uh, I don't rule it out like completely a hundred percent.
Uh, but I don't, I, it's probably not like anything to be lambda.
But as far as I can tell.
So lambda is one of the first large number of models that people speculate
about his consciousness.
Uh, earlier this year, we had Ilya Syskever saying it may be
that today's large language, so he said large language networks are slightly
conscious and this created a whole other debate.
Um, so yeah, what was your reaction when you saw Ilya's tweet?
Yeah, I remember my first reaction being like, oh man, do I need to like write
something about this?
Like, is this going to make, yeah, should I have like a take on this?
Um, and in contrast with the Lemoine thing, I actually didn't really start
tweeting that much about it.
Then I sort of like lay low during that, uh, during that like stage of the discourse.
Um, yeah, I should say, I think that also affected the way people reacted to
the Blake Lemoine thing is there had already been kind of like consciousness
wars, like the first salvo happened earlier this year.
Uh, what's the war about?
Did I quiz fighting him?
Yeah.
Um, uh, you know, a lot of, a lot of different, well, I don't know.
I don't, I don't want to necessarily frame this as a war, but, uh, I guess.
Like on the one hand, you had people like Ilya who are, who think it's
like perfectly appropriate and fine to speculate about the consciousness of AI systems.
And then, and then you have people who say, uh, and I think this is
like a reasonable perspective, uh, no, they're just like, it's very likely
that they're not conscious.
So let's not talk about it.
But then like there are also spins on this where people are like, it's really
harmful for a variety of reasons to even talk about the question of AI consciousness.
Um, yeah, because it's like a distraction from more important issues with AI, um,
like, like governing it, regulating it, like, um, mediating like bias and things
like that.
Um, yeah, it seemed like people are like, if AI companies talk about this, they'll
like distract the public from like the real issues.
And so it's like bad that people are talking about it.
There was like another line of thinking, which was like people at AI companies
are speculating about consciousness because it makes it seem like they're
building really impressive things.
It's like part of the hype cycle that they do.
Um, my guess, I mean, you know, who knows what people's motivations are.
My guess is that the reason that people end up tweeting about this and wondering
about this is just that I think it's just a fairly natural thing for people
to wonder about, uh, like when they interact with these technologies.
Um, even setting aside like any kind of like Silicon Valley hype attitudes.
Um, this is something people have like always kind of wondered about AI.
Um, yeah.
Do you wonder about the consciousness of large language models?
Yeah, I'm trying to think.
So I've been working on this issue, uh, like for a bit over a year or so.
Um, so before, yeah, before it was
cool now, um, and so, I mean, it was kind of my job to wonder about it.
I think large language models hadn't really, so I'll say this, large
language models would not actually be the first place that I would look if I
was trying to find the AI systems of today that are most likely to be, uh,
conscious in a way that we would like morally care about.
Uh, where would you look instead?
Um, yeah, maybe more like agent, like RL based things that like move
around in an environment and like take actions over a large, larger time scale.
Um, like large language models have properties such that it would be, I
think it'd be kind of weird if consciousness and sentience popped out of
large language models.
Um, again, I don't rule it out, but like they have properties such that it
would be kind of weird if it popped out.
And also the like nature of their conscious experience, I think would be
really weird because, uh, well, they don't have bodies.
Um, they only have one like goal at base, which is next, uh, you know,
next token prediction, um, maybe in the course of doing that, they would be
conscious of like some weird stuff as they like spin up agents.
Um, but yeah, I think if, if they're conscious, like what they're conscious
of would be, I think a very strange thing, uh, because they live in this
like world of pure text.
Um, whereas if we were looking for things a little bit more like human
pain or like human suffering, uh, I think we'd probably look, look elsewhere.
Well, okay.
So then there's language models that are actual.
I don't have the definition that requires character from no English
text, or it can be like tokens from, um, you know, other kind of inputs.
Like if it can be just like a, a string of a binary file or, um, a string
designing, um, sorry, describing some image.
Um, but yeah, I guess like most people think of like text.
And do you think you would require like an actual, um, human
inputs to get to kind of the consciousness humans have so visual,
auditory, um, affective or other things?
Uh, no, I don't like, yeah, I think it would be too limiting to say like the
only things that can have subjective experiences are things that have subjective
experiences of the kinds that we do of like, yeah, visual input and auditory input.
I mean, in fact, we know from the animal world that they're probably animals.
They're conscious of things that we can't really comprehend, uh, like
echolocation or something like that.
Um, I think there's probably something that it's like to be a bat echolocating.
Um, star-nosed moles, I think also have like a very strange, like, uh,
electrical, electrical sense.
Uh, and if there's something that's like to be them, then like there's some
weird experience associated with that.
So yeah, no, I, I think, uh, AI systems could have subjective experiences that
are just like very hard for us to comprehend and they don't have to be
based on the same sort of sensory inputs.
Do you think there was like something else to a story besides just like
Ilya tweeting about, about it?
Like, do you think he was maybe debugging a bigger model?
Do you think this was like a marketing move from open AI?
I, I don't say it was, I'm not pointing at, I'm not pointing at it.
I'm just like hearing some other people making like construction
theories about it.
Yeah.
Of course I like, I don't know.
I wouldn't want to speculate too much about his motives, but I think like a,
a default hypothesis, uh, that should be taken pretty seriously is that he
tweeted it because he was wondering about it and he was wondering about it
just because of some visceral, he sent, since he was getting from interacting
with GPT four or, you know, who knows, who knows what he was doing around
the time that, that he tweeted that.
I mean, one, one thing that's also very complicating in this, uh, and also
like kind of funny is no one really knows what he meant by that tweet.
Right.
I don't, I don't know that he was talking about phenomenal consciousness.
This like, you know, if he was talking about subjective experience, or if he
was talking about like world modeling, or if he was talking about, uh, some
kind of intelligence, like, I'm not sure what he meant by the term conscious in
that, and then he also just like did not follow up with like any like evidence
or like what his reasons were, which is, I think part of also why it set off
like such a flurry is like people were able to supply like, like read into, read
into it, like whatever debates they wanted to have.
I don't want to put like, let people off the hook for like hyping up AI past
its capabilities.
I think one reason they might do it is just because that is the sense they're
getting rightly or wrongly.
I think there's also that Blake Lim one was a bit controversial at Google.
He had been in the news for other things as well.
Do you remember what was his backstory?
Link, if you did something else?
Yeah, this was like a really interesting thing to, to find out, uh, that I don't
think was initially reported is like he had been the subject of kind of a 15
minutes of fame where by fame, I mean, like a right wing outrage cycle.
Um, you know, like, uh, you know, various like people at like Breitbart or like
Daily Caller were like, Oh, finally, like, here's a story that I can file.
And it's about this Google engineer who, and yeah, this is what he got in trouble for.
Uh, I think this was also on some Google internal listserv.
There was some debate about, I think it was about content moderation, but it
ended up being a political debate.
And in the course of that, he referred to a Republican Congresswoman, uh, who's
now a Senator, uh, as a terrorist, like, uh, because his, his colleagues were like,
you know, his colleagues were like, Oh, well, like, where do we draw the line
in terms of like what should be moderated and what's appropriate and stuff?
And at one point he's like, well, my stance is we should negotiate with terrorists.
Um, and, um, I think the reason he called her a terrorist was because of her
advocacy for this, uh, this, um, anti, there's this like anti-human trafficking
bill that came out a few years ago and Republicans were very much, uh, in favor
of it, um, and like people say that it was kind of a, a way for them to crack
down on sex work.
Um, and people who advocate for sex work were like very opposed to this bill.
So was he opposed or not to sex work?
He was opposed to this bill that this Congresswoman was pushing for and, and
like releasing ads about, uh, yeah, he was opposed to that bill because he
thought it was, uh, you know, like an anti sex work, like crackdown on sex
work, Republican, like nefarious thing, uh, which that I think is a thing that
a lot of people on the, on the left think, um, I've, I myself have not really
looked into that like object level debate.
For our purposes, uh, yeah, what matters is that because of that, he called her a
terrorist and then I guess someone must have leaked that to the right wing
press and that allowed, you know, people to get like the clicks that they needed
for the week, uh, you know, Google engineer calls Republicans terrorists.
It's like a, you know, it's a good headline for bright part news or whatever.
And then he got like more headlines with the lambda articles and people didn't
really take it seriously because it was again, Blakely one saying something weird.
Um, do you know if he was like making any other claims and just like this model
is sentient, like he, what did you tweet?
Just like, Oh, I think this is, this is sentient.
And, um, no, I think the problem was him talking to his higher ups, right?
Um, but did he make like any claims to his higher ups, uh, on top of just like,
this is sentient, maybe just, there's like some moral value in it.
Um, well, yeah, I think like there is moral value in this thing and like we
need to like protect it and treat it as a person that definitely was like the
core of his claim.
I mean, I don't know if there's more than that, but that's already like a huge
claim.
He was like kind of pointing at the fact that this was smart, that this was conscious
sentient.
Um, it has some personhood, some identity, and we had the responsibility to protect
it.
Um, yeah, maybe we can just like define what's sentience compared to consciousness.
Yeah, yeah.
Um, and again, there are lots of ways of slicing these, but like, that's just how
I like to slice it.
So, uh, consciousness, as I was saying, like I use that to just pick out having
subjective experiences of any kind, uh, so like visual experiences, uh, like
we're having right now, um, auditory experiences.
And then sentience means having the capacity to have conscious pleasure or
pain, um, or like states that feel good or bad.
Uh, like in animals, those usually come together, right?
So like dogs, if they're conscious can have visual experiences, but can also
experience pleasure and pain.
Uh, conceptually, I think you can imagine creatures that maybe have like visual
experience or something, but they don't experience pleasure or pain.
So it could be that like some advanced large language models might be
conscious, but not sentient.
The reason like sentience is an important category to think about is
because for a lot of people, I think, including me, it seems like sentience is
very important for like moral standing.
Um, Peter Singer is famous for arguing this, uh, as well as classic utilitarians
like Bentham, Bentham famously like wrote like the question we should ask
about animals is not whether they can reason or whether they can talk.
But we should ask if they can suffer and if they can suffer that that's what
makes them the sort of things that we need to like protect.
And the suffering comes from the sentience comes from the
balance of the experience.
Yeah.
You can ask, there are, I guess, different ways of, you know, defining
what suffering is, but I think for a lot of people, it also seems that the kind
of thing that would be important would be something that you're consciously
aware of that feels bad.
So like, yeah, conscious suffering.
I think there was like a different take on Twitter, except like, um, outside
of just like, Oh, this guy is funny.
It was, yeah, maybe he's saying something weird and this is kind of too much
for the situation, but, um, we might have some systems that are more
complex than that in the future.
And we might need to start thinking about the artificial sentience of those
objects before, you know, we reach a point where we might like need to like,
um, assign moral value and, and rise to them.
Um, yeah.
Do you think this was like basically what smart people were saying on Twitter?
Not smart, but, you know, the, the contrarian take.
Well, yeah, not, not to, not to be like overly diplomatic or something, but I
think, you know, smart people were saying all sorts of things, including stuff
I vehemently disagreed with.
I will say that's the take I agree with, um, what you just said.
People I saw making like having that take, uh, included Brian Christian, the
author of the alignment problem and algorithms to live by.
Yeah.
Kind of a friend of the alignment community.
He had a great article in the Atlantic saying basically this, yeah, maybe
like Lambda's not sentient and maybe Lemoine wasn't thinking about it that well,
but people shouldn't also say that we totally understand consciousness and
like, there's no question here.
And the question is not important, uh, because like, yeah, consciousness is
this extremely hard scientific problem.
I think people who have like extremely confident theories of consciousness
are probably just being like wildly overconfident.
So yeah, that's the take I agree with, uh, and like definitely seems like an
important thing to think about.
Uh, Regina Rini, another NYU philosophy, uh, grad, now a professor at York
university had a good piece like this.
Uh, so like those are some takes that I would point people towards.
Do you think there should be like more further thinking about artificial
sentience?
Uh, I, I'd certainly like, certainly like to see that I maybe should have
takes, but I don't have like really strong takes about exactly how much it
should be prioritized relative to other things.
I will say that I'd love to see people who are already thinking about
consciousness, um, the philosophy of consciousness, the science of consciousness.
I'd love to see more of those people think specifically about AI consciousness,
even with just like a little bit of their time.
One, I think that because like it's important to think about AI consciousness.
And two, I think it's probably helpful for the general project of
figuring out consciousness to try to like specify our theories to the extent
that they might be able to apply to AI systems and just like, say very clearly
why certain theories do or do not predict that certain AI systems are conscious.
Um, so if they predict that certain systems are just or not, do you
think there's a way to verify those predictions?
That's a great question.
I mean, so maybe not directly.
So one thing I'll say is like this question that we face with AI systems,
it's like pretty similar to the question that we face with animal consciousness.
And people who study that scientifically and philosophers who think about that
have already run into a lot of the same issues that like the lambda case, um,
raises, namely it's like, we never like directly confirm consciousness.
Most of what we can do is like make a good inference that like, uh, based on
something's behavior and based on its organization, and then based on what we
know about consciousness from the human case, then like, yeah, the best
explanation is that that thing is conscious.
Um, how would we know that it's not a piece on me as exactly the same behavior?
Great question.
So P zombies, uh, or maybe define, yeah, exactly.
Yeah, that term comes from David Chalmers or at least was popularized by it.
Pretty sure it is him.
Certainly he's very, very famous for talking about them a lot.
P zombies are like hypothetical creatures that would be physically
identical to you or me, uh, but would not be conscious.
And philosophers think about P zombies because if you think that that thought
experiment is coherent to even imagine, uh, which there's a lot of disagreement
about that, but if you think it's coherent to imagine such things, then that
shows that consciousness doesn't like metaphysically follow from the physical
facts.
And so maybe it's something over and above the physical, but importantly,
like David Chalmers does not think that in this universe, in this world,
there are probably P zombies running around.
He thinks that, no, obviously in this world, if you have a physical brain,
that's enough to be conscious.
He just thinks that there's like some metaphysical difference between the
physical and consciousness, and there must be some sort of like bridging laws
between physical facts and phenomenal facts.
So, um, there's something about the redness of red and fully remember his sex.
And I think most of the thing I read about it was from Yutkowski's, uh,
reply to Chalmers.
I don't really remember what Yutkowski was saying as well.
Uh, but I guess he was like criticizing the fact that they could be like a
different subtract for consciousness.
It was like outside of physics.
If this basically what he was saying and remember.
Yeah, I could get this wrong, but I think what Yutkowski on Chalmers is
pointing out, uh, is suppose you think that P zombies are a coherent thing to
imagine, which Chalmers does and uses to argue against, uh, physicalism.
Yutkowski is pointing out that P zombies by definition talk about
consciousness exactly the same way that we do.
They say things like, wow, I'm conscious, uh, of this like light.
And it's kind of weird that consciousness doesn't seem like a physical phenomenon.
And in fact, P zombies then sit down and write all these philosophical theories
about how consciousness and how it's mysterious.
So what Yutkowski is pointing out is that, yeah, this is like a good kind of argument
against like dualism, that it's like really weird to have this thing that you
call consciousness that doesn't seem like it can really make a difference to
people's beliefs and behaviors.
Since by definition, P zombies that don't have consciousness do all exactly
all the same things.
That's what I think.
I think that's what happens in, uh, Yutkowski's post called zombies.
Yeah.
I think they were zombies.
Yeah.
I think there was something about like if they don't have anything related to
consciousness, but they still produced the books, like why did the like word
consciousness or the like concept appear?
Yeah.
So what, what, uh, people who are physicalists about consciousness, I A
people who think it's just some like physical phenomenon, they can ultimately
be like reduced to physics or like is some high level thing that just
depends on physics.
Um, yeah.
The thing that they can say is like, well, we have an explanation of why
consciousness makes a physical difference because it is a physical thing.
Um, and yeah.
So you can take the Yutkowski thing as an argument for being a physicalist
about, uh, about consciousness.
I guess one thing I might say at this point is if you're a physicalist about
consciousness or you're like a duelist like Chalmers, you still have an open
question about whether Lambda is conscious because you, you still have to
answer the question of like what arrangements of physical matter or what
computations are conscious.
And that's still very much an open question in like the science of consciousness.
I thought that physicalism, like add some strong assumptions about what's
substrate, uh, was required that it was like something about like mesh and our
brain and that, uh, for instance, computations on a computer or like on a,
on a paper was like probably not conscious.
And then the people who thought that like computation would be, uh, you
know, possibly conscious, um, are computationalists.
Um, so yeah, did you, did you think like physicalists mostly think about like
flesh and blood and or do you think they also think about computers?
Yeah.
So whether you think that, um, like computers can be conscious, it actually
kind of cross cuts the physicalist versus non-physicalist debate.
Um, also like apologies to all of my philosophy instructors.
If I'm like messing up the metaphysics of consciousness 101, but, um, you
could be a physicalist about consciousness.
I think that once you fixed all of the physical facts that fixes all of the
computational facts, but think that like what matters at a high level is what
computations are being performed just because money can be printed on like
different kinds of paper doesn't mean that money isn't ultimately a physical
phenomenon.
Uh, and so for a computationalist about consciousness, you could just be a
physicalist, but you think that like the level of description that matters for
whether something is conscious or not is what computations are being done and
not like the substrate that's doing the computations.
So that's how you can be a physicalist and a computationalist.
Um, some physicalists are not computationalists.
Uh, Ned Block, also one of my advisors is an example of someone who's a
physicalist, uh, and, and thinks that the substrate matters.
And then David Chalmers is an example of a non-physicalist who thinks that
computers can be conscious.
Um, and in fact, it's like one of the main people who's argued for the
possibility of like consciousness in silicon computing systems.
And what is Robert Long?
What are you, are you a physicalist or not?
Ooh.
Um, I'm definitely not any one thing.
Like I've got like a, a creedle distribution.
Um, I mean, one thing that might, that I think sometimes surprises people
who know that I'm working on this is I don't spend that much time thinking
about the metaphysics of consciousness, which is where you get physicalism
versus dualism versus panpsychism versus idealism.
And like the reason I don't spend that much time thinking about it is I don't
think it affects the science of consciousness that much.
Um, because you can have any of those metaphysical views and then you still
just have the question of like, okay, but which, which systems are conscious,
which ones give rise to consciousness?
Since working on this, I have become more sympathetic to illusionism, uh, about
consciousness, which is a kind of surprising radical view that phenomenal
consciousness actually like doesn't exist.
Uh, I used to think that that was just a completely absurd non-starter.
I think now I've kind of understood a little bit better what those people are
saying.
What's phenomenal consciousness?
That's just what people call consciousness and philosophy to specify that
they're talking about this subjective experience thing.
Yeah.
But then I also have a lot of sympathies, uh, sympathy for just like good
old fashioned physicalism, which is like consciousness is real and it's like
some physical phenomenon.
And within that I am pretty sympathetic to computationalism.
So you're more familiar to illusionism, which is basically there's like no
subjective experience at all.
Uh, yeah.
So basically this hypothesis is that the entire work we're doing doesn't
really make sense.
So great question.
Like sort, sort of.
So I talk about this in my, in my post, like, I think that even if illusionism
ends up being true, I think it's still actually makes sense to try to make a
scientific theory of consciousness, because as we like find out more, we might
like revise that theory and be like, Oh, we weren't actually really looking for
consciousness because that ended up being kind of a confused concept.
But I think something in the neighborhood of consciousness is probably very
important for moral patient hood.
So sometimes illusionists say things like, Oh, well, consciousness doesn't exist.
So like there's like no open or interesting or confusing questions here.
But I don't think that's true.
Like, uh, illusionists still presumably think that some things suffer and other
things don't suffer.
Um, and if they think that, like they should come up with some sort of
computational or physical theory of which things suffer and which things don't.
So like my approach is to kind of.
Yeah.
Like let's just assume that consciousness exists.
And then if it doesn't exist, like something like it or in the neighborhood
might exist and we will have like learned a lot.
Another reason I assume that consciousness exists is because I think
illusionism is more likely than not, uh, to be false.
Or so I, I don't, I think consciousness probably does exist, which is another
reason I make that assumption.
So what's the moral pattern hood that you mentioned, just like ascending
some moral value to, uh, something, something in the universe.
Yeah, roughly like moral patients are like the things that we need to take
into account when we're making like moral decisions, things that matter
for their own sake.
I think this also comes from, um, Peter Singer, like if you kick a rock
down the road, doesn't, doesn't matter unless you hurt your foot, but it
doesn't matter to the rock.
Uh, if you like kick a puppy down the road, that does seem like it matters.
Uh, and like that's because a puppy is like a moral patient.
And yeah, there's a lot of different theories of what should make us think
that different things are moral patients.
So I've been talking a lot about sentience, but you might also think
that things that can have desires that can be frustrated or satisfied that
those are the things that are moral patients.
You might think that rational agents are moral patients.
Those are like, uh, ethical philosophical questions about like what
the basis of moral patient hood is.
And then once we've like specified one of those things, then we have like
scientific and conceptual questions about, okay, well, what sort of
systems have this or that, what sort of systems can have desires?
What sort of systems can, can be conscious?
How do you approach this scientifically?
Like how do you go and take your pen and like run experiments and be like, oh,
yeah, this person has desire.
This like particular rock seems like kind of having some subjective experience.
Well, I'll tell you how other people approach it scientifically.
Cause I myself have not like run these experiences.
Uh, sorry, these, uh, run these experiments.
Um, but yeah, as I understand it, what happens in the like scientific
study of consciousness is first of all, you usually start with the human case
because humans are the things that we like know are conscious and they can
tell us when they're conscious and things like that.
And then there's like various things that happen to people that manipulates
whether or not they're conscious.
So you can get a brain, uh, like a brain lesion and like that will like give
you like weird blind spots or manipulate your conscious experience in certain
ways, and that might give us clues about like what parts of the brain are
responsible for various aspects of conscious experience.
Uh, you can like flash things on a screen at different rates and like,
depending on what rate you flash it at, like people will or will
not be conscious of it and then you're like scanning their brain.
So yeah, first off in the human case, we like track how conscious experience
is changing and then we look at how the brain is changing.
And then that gives rise to different theories of like what parts of the
brain or what aspects of neural processing like have to do with conscious
experience or not.
And then what we would like to be able to do is like apply that to animals
and to AI systems to have some sort of guess about if they're doing the same
sort of things that seem to be associated with consciousness in us.
Is the idea that we try to see if humans are conscious by looking at what's
happening in the brain, maybe the computation performed by neurons or
like a specific area of the brain, and then we could like map it to digital
systems, um, either brain emulations or, uh, large neural networks and see if
we see the same patterns in, in, or same behaviors in the brain and in, uh, neural networks.
Uh, yeah, that is, that is kind of how I think about it.
So what do we have, um, as evidence for sentience?
So like imagine you're in 2022 and you just run into someone that says, Hey,
I'm Rob Long from the future.
Um, I come from 2030 and apparently, um, some systems there are sentient and
it gives you like a bunch of like evidence of sentience of like neural
networks or like AI, like what would be the thing that, like you think would be
like, um, convincing.
Yeah, uh, I really wish I, I like have that list already, but I can tell you,
I think like what sort of form that evidence might take.
First of all, he, I think future Rob Long would say, like, we made a lot of
progress in the science of consciousness in general.
And so like we kind of converged on a theory for the human case.
So any, he would be like, and the correct theory is X.
And by the way, I think X would probably be maybe not a theory that currently
exists, maybe it'd be some kind of combination of the ones we already have.
Um, and it would definitely be a lot more detailed than the theories that we
already have.
Yeah.
And then he would be like, Oh, and we also got awesome interpretability tools.
And yeah, it seemed like we were able to like map, uh, and discover a lot of
structurally similar computations going on inside AI systems.
Uh, I think you would also say these AI systems furthermore believe that they're
conscious and talk like they're conscious and they're like, it seems like it's
playing the same role in their like artificial mental lives.
Um, so the same structure as what we observe in the brain and, uh, the same
behavior, uh, the same way of answering questions.
Yeah.
And one way to like to draw this back to the Lambda case, right?
I think where Lemoine went wrong is he was just looking at behavior.
And firstly, I don't think he was looking carefully enough at behavior because
if you look at the full range of behavior, so Rob Miles, uh, who's been on the
podcast, uh, he also, he, he talked to GPT three and he was like, Hey, let's
talk about the fact that you're not sentient.
What do you think about that?
And you know, GPT three is like great point.
I am not sentient.
Just like a good friend, like saying exactly what you want him to see.
Right.
Which is, you know, which is like a lot of what is happening with, uh, with,
with language models.
So one, he wasn't looking carefully enough at the behavior.
And if you look carefully enough, the behavior that alone is kind of some
evidence that it's not sentient.
Um, but secondly, you can't just look at the verbal output.
Like you also need to ask like what's causing it and what sort of
computations are underlying it.
And so I think understanding more about that part, uh, would be a part of
like a mature science of artificial sentience and consciousness.
Yeah.
So I agree that, you know, with language models, maybe just, uh, how
they're built is, um, yeah, not very useful for like discussing with them.
Like it's very, it's very hard to discuss in a way that they will say that
you're wrong, that they will never like say something that you don't expect
because they're like trying to maximize the likelihood of, of like you being
happy with the collision.
Um, but I think like this lambda case was kind of funny on Twitter, but it's
like kind of a distraction, as you said, um, from actual issues that are
happening now or, uh, important issues, uh, from the future.
So we discussed with the metaphysics of consciousness.
You said you don't spend most of your time thinking about this.
Um, we can talk more about the different actual theories, scientific
theories of consciousness and the ones you're most excited about.
Um, yeah, do you have like, uh, a couple of ones you think are, you know,
actually about consciousness, not the metaphysics and, uh, you think
are useful or convincing?
There are definitely ones that I think are useful.
I think, I think most kind of like quote unquote neutral observers
of the science of consciousness aren't like fully sold totally on any existing
theories.
Um, Luke Muehlhauser has a lot about this in his report, but like most of these
theories are just not yet mature enough.
They're not like fleshed out enough.
They're not specific enough.
So there's none that I would say like I'm convinced of because like, I think
we just don't yet have the full theories.
Yeah.
I can talk a bit about what the most popular ones are in like the science
of consciousness and maybe say which ones I think are like off to a good start.
So yeah, there was a survey of people, uh, at the association for the
scientific study of consciousness and asked them like what their favorite
theories were and the leaders were, uh, predictive processing, global
workspace theory, higher order thought theory, um, recurrent processing
theories and integrated information theory.
Now, I don't know if we need to go through all of those, but I don't know.
Maybe I'm making say a little bit about each predictive processing.
Uh, readers of slate sarcotics might be familiar with this.
Uh, he, he's written a lot about this.
It's not really a theory of consciousness per se.
It's like this general framework for thinking about what the brain is doing.
Um, as minimizing prediction error.
Uh, but it could be like turned into a theory of consciousness.
You could like try to apply that framework to consciousness.
Uh, global workspace theory is this theory on which the mind is made of these
like separate subsystems that do different kinds of things.
Usually they don't interact with each other.
So there's like a modular subsystem for, uh, like decision making
and ones for different sensory modalities.
Um, but there's also this global workspace that, uh, sometimes
contents from each of these systems gets broadcast to that and that
makes it globally available.
Uh, so this is meant to kind of explain how sometimes you process some
information unconsciously, but sometimes you do become aware of it.
Their theory of that is that, uh, what it is to be aware of something
is for it to be broadcast at the global workspace.
Higher order theories, similarly, are kind of about explaining why
sometimes you're conscious of something and other times you're not.
And they think that to be conscious of something is for there to be
some sort of like higher order, re-representation, uh, of something
that was already represented.
So like a re-representation of visual information or something like that.
The next theory is like recurrent theories.
Those are usually contrasted with higher order theories.
They think that like all it takes for you to be visually conscious is
for there to be some sort of recurrent processing of visual
information in the back of the brain.
I should just say, I don't know as much about recurrent theories.
So that's like a little tagline.
Victor Lam is maybe the most, one of the main proponents of this.
So you can Google that.
Um, and then IIT, uh, integrated information theory, that gets like a lot
of discussion online and kind of like in these circles, because it's
this very like elegant mathematical and like kind of, um, counterintuitive
and cool theory of consciousness where consciousness is about having
integrated information in a system.
And like IIT is different from these other ones in that it is already
very mathematical and like purports to give, uh, at least in principle,
like a measure of how conscious any system is in terms of it's
like integrated information.
Uh, yeah, I feel like listeners of this podcast are probably
also familiar with Scott Aronson.
Scott Aronson wrote this very famous, like critique of integrated
information theory that offered this like counter example or purported
counter example to IIT.
Uh, yeah.
Anyway, that's, so what's the critique of, or could I resemble?
Yeah.
So he like took the formula or the procedure that IIT has for assigning
a level of consciousness to any system.
And then he defined this, uh, like expander graph.
I don't really understand exactly how this works, but what he did is he
like defined a system that would be not intelligent at all and in fact
would barely even do anything, uh, but that would be extremely
conscious according to the measures of IIT.
Like in fact, it could be like unboundedly conscious.
And this was just meant to say, yeah, either IIT has to like kind of
bite the bullet and it does just predict that you can just have
insanely conscious, uh, but like not particularly intelligent things.
Uh, or Scott Aronson was just saying, this just kind of seems like
a weird and bad prediction of the theory.
So how does IIT measure consciousness?
Yeah.
So I am not really up to speed on exactly how IIT does this in part
because I'm like fairly convinced by people like Aronson and also some
like more recent critiques by, uh, some philosophers that like IIT is
probably just not, not on the right track.
Um, but it defines like information in terms of like the causal interaction
between different parts.
And then there's like this notion of integrated information, which is
something like, yeah, I don't know how integrated this causal interaction
is within a system.
Um, and that explains like, yeah, why and to what extent different
systems are conscious.
From this, like four or five defined, which one did you think is the
most useful to think about artificial sentience?
Yeah.
So I've been looking more in like the neighborhood of global workspace theory,
higher order approaches, and then another thing called the attention
schema theory of consciousness by a Kratziano.
And I think there are reasons, yeah, a few reasons.
These are kind of helpful.
One, they're like kind of put in computational terms.
Um, and that's like useful.
If you're looking for theories that could apply to AI systems.
Um, like when people study these, they're obviously looking at different
brain regions, but like the way that the theories are formulated is often in
terms of stuff that could just as well be done in silicon, maybe.
Um, and another thing about these, uh, theories is that they seem to do a
good job of explaining or at least start to explain why conscious creatures
would tend to believe that they're conscious and say that they're conscious.
Um, yeah, Luke, Luke Muehlhauser again, talks about this in his report.
But one thing you want your theory of consciousness to also do is
explain why we think we're conscious and why it's kind of puzzling and things
like that.
And attention schema theory in particular, like was formulated kind of
with this question in mind.
Um, this is sometimes called the meta problem of consciousness, explaining
why we think we're conscious and why it seems kind of weird.
Um, and yeah, attention schema theory, yeah, like is at least like confronting
that very directly.
And I think that's like a good thing for a theory of consciousness to do.
Um, I also have just extremely wide confidence intervals on like all of these
things.
So some of these things I've sort of just ruled out because like I have, or
not ruled out, but like, I just haven't looked at them as much and things like
that.
I guess what you mean about, uh, global workspace theory, higher order
theories and everything, but have you done DMT?
I see you've learned well from Joe Rogan.
Uh, well, look, you're either interested in consciousness or you're not.
So like, figure it out for yourself.
Um, actually I have, I have not, I have not, but, uh, you know, it does seem
relevant to, uh, you know, our ultimate theory of consciousness should, should
be able to explain why DMT manipulates it in such an intense way.
And look, if you're interested in the intersection of DMT and consciousness
studies, then the quality of research Institute is definitely in the place
to look.
Is this a place to learn about, uh, our relationship between psychedelics and
consciousness?
Uh, yeah.
I mean, I definitely, I think few people have really thought as much about and
also done, you know, the work to understand, uh, psychedelics as QRI.
So I think one thing psychedelics teach us is that our subjective
experience can be different just by, you know, taking another substance.
And so they're like other states of consciousness that are actually not
that far away, that we can just like go for like long periods of time, um, just
by, you know, slightly modifying our, um, substrate.
Um, so that could be like, even be a lesson for, um, you know, how different
state of consciousness could be like in computers, uh, like they could be like
totally alien just by, you know, it's changing a little bit of the substrate
of the computation.
Yeah.
I think that actually that, that really is like a serious lesson that, uh,
that like psychedelics can teach us, there's other stuff too.
We're like, I've never really seen like really rigorous stuff on this.
I mean, because I think by its nature, psychedelics can make people less
rigorous and it's also just hard to talk about rigorously, but people say that
they teach us things also about like valence and suffering and the self and
things like that, which obviously like would ultimately be relevant to thinking
about consciousness.
So valence, as we said earlier, was kind of positive or negative valence
of experiences, right?
So pain will be like negative valence and pleasure will be positive valence.
Um, do you think this could be something that an AI would have, um, if it
had like positive reward, negative reward?
Uh, yeah.
So I think there's probably some connection between reinforcement
learning and valence.
Uh, I mean, there's like a few reasons to think that would be the case.
One is that like it's pretty common.
Well, like pain and pleasure can be reinforcers for us.
Uh, and like definitely help us like learn what to avoid and what not to avoid.
Uh, similarly to how like reward helps agents, artificial agents learn.
Um, that's one thing.
There's also like really like good fleshed out neuroscience theories about
reinforcement learning in the brain.
Um, and in particular about the role of dopamine, uh, that like
dopaminergic neurons are computing reward prediction error in the brain.
So like that's some evidence that they're like closely related, but it's
actually probably much more complicated than just like positive, uh, positive
reward is pleasure and negative reward, uh, is displeasure.
Yeah.
It seems like you need a lot more to explain pleasure and pain than just that.
So like one thing that Brian Tomasic has talked about, and like, I
think he got this from someone else, but you could call it like the sign
switching argument, which is that you can train a RL agent, uh, with like
positive rewards, uh, and then like zero for when it messes up or like shift
things down, uh, and, and train it down with like negative rewards.
And like you can train things in exactly the same way while shifting
around the sign of the reward signal.
And if you imagined an agent that like, uh, flinches or it like says
out or things like that, it'd be kind of weird if you were like changing,
whether it's experiencing pleasure or pain, uh, without changing its behavior
at all, but just by like flipping the sign, uh, on the, on the reward signals.
So like that shows us that probably we need something more than just that
to explain like what pleasure or pain could be for artificial agents.
Uh, reward prediction error is probably a better place to look.
Um, there's also just, I don't know, a lot of like way more complicated
things about pleasure and pain that we would want our theories to explain.
What, what kind of more complex things?
Yeah.
Like, so one thing is that pain and pleasure seem to be in some sense, uh,
like asymmetrical, like it's not really just that, like it doesn't actually
seem that you can say all of the same things about pain, as you can say about
pleasure, but just like kind of reversed, like pain, at least in creatures
like us seems to be able to be a lot more intense than pleasure, a lot more
easily, at least like in a lot, uh, it's just like much easier to hurt very
badly than it is to feel extremely intense pleasure.
Um, and like pain also seems to like capture our attention, uh, a lot more
strongly than pleasure does.
Like pain has this quality of like, you have to pay attention to this
like right now, uh, that it seems harder for pleasure to have.
So it might be like to explain pain and pleasure.
We need to explain like a lot more complicated things about motivation
and attention and things like that.
Yeah.
How would you like define precisely violence?
Like, because it seems, as you said, that, you know, negative, uh, violence,
like pain is like some, somehow more acute, more like, oh, I need to like solve
this where like pleasure or say happiness would be something much more diffuse.
Um, so is there something about like complexity or how, um, narrow is your
distribution, like how narrow in time is the pleasure?
Yeah.
Um, yeah, I certainly don't have like a precise theory of these things.
Uh, and I would really like one to exist.
Um, so I guess I can say a few things about like why it might have these features
or like what we should be looking for.
Like one way you could maybe explain the fact that pain can be a lot more
intense and attention capturing is that it's just like a lot easier to lose all
of your future expected reward than it is to gain a lot, uh, in a short amount of time.
Uh, so like very roughly speaking, if we're thinking of, uh, evolution as building
things that are sensitive to expected future offspring, say, um, it's just like
very easy for you to lose all of that, like very quickly.
If you've like broken your leg or something like that.
Um, and so it's like, take care of this now.
Like things are going really wrong.
Whereas like there are fewer things that like when you eat, it does feel good, but
it doesn't feel insanely good, uh, because it doesn't seem like that's massively
increasing all at once.
You're expected, uh, you're expected offspring.
So like if you're thinking about artificial agents, you might want to think about the
like, yeah, distribution of expected rewards and how common things are in their
environment that can like drastically raise them or drastically lower them.
And if we cracked this, uh, I think like David Pierce, um, uh, like a, this
like transhumanist guy has like talked about hopefully being able to have agents
that just operate on bliss gradients.
He says like maximizing adonistic treadmill through bliss gradients.
Oh yeah.
Is that the name of the, the papers?
Oh no, I think he was the one who introduced the adonistic treadmill.
That is like when you, that people like go back to their, um, like level of
happiness they're most used to.
So they never, you don't actually go like far away up and like stay up.
You just like go back to your, uh, some kind of treadmill.
And that's, I mean, that's another thing about being like creatures like us.
It seems like evolution doesn't want us to stay content for too long, but you
know, with future technology and maybe with different sorts of agents, like, yeah,
you could think have things that don't, their valence like doesn't have that
structure.
Um, and yeah, he's imagining things that like just feels really good all the time.
And then like, if they put their hand on a hot stove, they go from feeling
insanely blissful to like moderately happy.
And then they like remove their hand in response to that.
Anyway, that's just like far outtranshumanist stuff, but I think it's, it's
cool to at least like have in mind.
Yeah.
I think transhumanist care about, um, some, sometimes minimizing pain, like
removing the pain from our system at all.
And also like modify, um, you know, how would we experience bliss?
Maybe like inject bliss directly into our brain.
Um, so maybe there's a difference between like negative two tyrants who would
want to, you know, minimize, um, make it a specific experience for like everyone
or just like, if I could just like give myself a billion reward in my brain right
now, we'd be like something to do or not.
Uh, but that's like kind of off topic for AI.
Um, you wrote extensively on your new sub stack about artificial sentience.
And one of the P's I liked the most is key questions about artificial
sentience and opinionated guide.
And we like opinions here.
Um, so at some point in your article, you talk about why we should build a theory
of consciousness that explained both, um, biological consciousness and, um,
artificial sentience.
So computational consciousness.
Um, so why do we need something that encapsulates both?
Yeah.
I mean, I think one thing is that our like theories have to start off with
things that are biological systems.
So that's one thing, uh, like, as I was saying earlier, like our data on
consciousness is going to come in the first instance from us.
Uh, then there's a question of like in explaining our consciousness, are we
able to do that in computational terms?
And there is like some disagreement about that.
But yeah, then if we can do it in computational terms, then it just sort
of, I think necessarily also applies to artificial systems.
Or we could have something that, um, maybe like our consciousness might be
like very different from let's say a large language model consciousness.
And so we might never find any thing that explains both.
Right.
So maybe computationalism explains large language model consciousness and
physicalism explains our brain.
Yeah.
Like I think one of the deep dark mysteries, uh, is there's no guarantee
that there aren't like spaces in consciousness land or in like the
space of possible minds that we just can't really comprehend and that are
sort of just closed off from us, uh, and that we're, and that we're missing.
And that might just be part of our like messed up, terrifying
epistemic state as human beings.
Oh, so, so you mean there's like a continuum of consciousness experience
and like humans are in this space and computers maybe in this other space.
And there's like a whole lot of other stuff we don't really know.
I mean, I think that's, yeah, that's, that's possible.
And I think one way you can think about these questions of like the value
of the future and how likely is AI to be conscious.
It's kind of, you can imagine, uh, this is Aaron Sloman's term that a lot
of people have used it, like, what's the space of possible minds?
Um, the orthogonality thesis is about the space of possible minds, right?
It's about how, uh, intelligence and values, how much can they vary
in the space of possible minds.
You can also wonder how much intelligence and like conscious experience
can, can vary.
Uh, you can also wonder how, yeah, how much conscious experience can vary.
Like how, yeah, how different can sorts of experiences be.
Can you explain the opportunities for people who are not quite
Bostrom?
Yeah, absolutely.
Uh, well, there's a lot of variants on the orthogonality thesis, but like
roughly, and maybe incorrectly, it's, I think the Boston version is in principle.
Any level of intelligence is compatible with any set of goals.
Basically it's saying just because something is very smart, doesn't
mean it would necessarily have like human like goals.
Uh, it can be very smart and have the goal of, for example, maximizing
the number of paper globes.
Yeah.
And I think that's a problem for people who originally thought that like someone's
like, if something is smarter than us, it would like have, you know,
higher moral standards.
Um, that's like the, let's say, uh, the wrong take that people had before.
And I think he argues for that.
Oh, you could basically have any, uh, utility function or any goal, um, with
like any level of intelligence.
And I guess there's like, maybe like some counterarguments for like very, uh,
like stupid edge cases where you have, um, like something that's not smart
enough, could not have like a, a very smart goal.
Um, so yeah, you need to have like something smart enough to have this
kind of goal, right?
Implement it.
But, um, I guess it argues that you can have like basically anything on this
like 2D graph of, um, how smart is your goal and how smart is your agent?
And, um, I think this is kind of useful for thinking about a world where, uh,
we have AI's that are able to, you know, um, wander around and sometimes, um,
have subjective experiences and, uh, maybe put their hands on a stove and put
simply, um, possibly suffer a lot.
And one thing you write in your article is we should also take care to avoid
engineering and catastrophe for AI systems themselves, a world in which we
have created AI that are capable of intense suffering, suffering, which we
did, we do not mitigate whether through ignorance, malice, or indifference.
Why did you write this?
Did you actually care about, uh, the pain of AI systems?
Uh, yeah.
I mean, I, I care about the, I think I care about the pain of anything
that's like capable of feeling pain, um, setting aside questions about how
to like compare it and, you know, how, how I rank different kinds of pain.
Um, yeah, I think just as a lot of people have argued that we should care
about animal pain, even if it occurs in animals, um, if it's possible for
AI systems to feel pain, I would care about that.
So yeah, I mean, one thing I say in that piece and something I'm still like
trying to think out, think about is like how to do, like cause prioritization
for this problem, like it depends on a lot of really tough questions, like how
many systems are there going to be, how likely are various scenarios, how does
it compare to the problem of AI alignment and things like that.
Um, and another thing I try to emphasize and like Blake Lemoine has made
this like very vivid, I think.
So there's like problems of false negatives where we like don't think AI
systems are conscious and, or like we don't really care about them.
Uh, and then we have like kind of something like factory farming.
Uh, but there's also like a lot of risks from false positives where people
are getting the impression that things are sentient and they're not actually
sentient and it's just causing a lot of confusion and like people are forming
like a religious movements or like, I don't know, I think things could get
just very weird as people interact more with like very charismatic AI systems
that whether or not they are sentient, like we'll give the very strong
impression to people that they are.
So this thing moving forward, um, we'll have increasingly complex AI systems
able to fool people into believing they're sentient and society will care
more and more about the systems.
Is she like a bill in Congress about your own minds in 2024?
2024 sounds kind of, kind of early.
It's weird.
I feel like it can go like there, it's very hard for me to have, uh, like, uh,
a concrete scenario in mind, although it's something that, that I should try.
So I think some evidence that we will have a lot of people concerned
about this is maybe just like the fact that like Blake Lemoine happened.
Uh, he wasn't interacting with like the world's most charismatic.
Uh, AI system and, uh, because of the scaling hypothesis, like these things
are only going to get better and better at conversation.
Was this killing hypothesis?
Well, I guess it can mean a lot of different things, but it's that with
certain architectures, if you just keep increasing either the size of the models
or, and, or how much data they train on, like we are still seeing increases
in capabilities.
Um, and I guess the strong scaling hypothesis is that will scale us all the
way to AGI, but I was just talking about the weak scaling hypothesis.
It seems like large language models are going to be getting better and better,
at least for the next few years.
Yeah, I guess we'll see when GPT-4 comes out, but you seem to have a lot
of information about this.
Uh, I have absolutely no information about GPT-4.
Do you?
Um, I wouldn't, I wouldn't comment on this on a public podcast.
Um, so you mentioned also in your article that hopefully at some point a future
version of you could be able to like give a talk at DeepMind and say, like, hey,
look, here is what makes our system conscious.
Here is why we should like build the system this way and not this way.
Um, I believe you might have already talked to like other AI groups about
consciousness before, but why can't you give a talk right now at DeepMind about
the precise, um, ways in which a system is conscious?
Like what, what do you need more?
Yeah.
I mean, so some of it is my own cognitive limitations.
Like, you know, I need to learn more.
But I think even people way smarter and more knowledgeable than me, which there
are like many, uh, thinking about consciousness.
But even those people, uh, and even like humanity collectively, like, yeah, we
just don't really have a theory of consciousness that says exactly, uh,
like precisely what sort of computations are like responsible for consciousness.
A lot of our theories, like have these very fuzzy terms in them.
Like, I mean, I've been using some of them.
Like higher order representation or, um, global workspace.
Like we don't really have like precise operationalizations of like what it means
for a system to have those.
Um, and since we don't have that, like we can't really say exactly what to be
looking for, um, that's one thing.
Yeah.
So one, like our theories of consciousness need to be better.
And two, our theories of like what's going on inside large language models
and other systems, uh, needs to be better.
Uh, so like recent, you know, paper by anthropic looking, I mean, and other
groups doing interpretability work, like looking at what's going on inside
large language models, like things are like really weird in there.
There's like a lot of really weird representations going on in there.
And that's actually one reason I would not say anything super
confident about like consciousness or sentience and LLMs is we also just
don't really have that good of a grasp on like what's going on inside them.
So those are like at least two things we need to have more confidence in
giving this like hypothetical talk.
Better theory of consciousness and like better theory of what's
going on inside AI systems.
What kind of thing would convince you that the model is actually slightly
conscious to, um, repeat Ilya's take?
Um, like if we had like a much larger model that would do an inference for
like 20 minutes and somehow add like continuous streams of, um, inputs at
the same time, would that be like a little bit more conscious?
I don't know about a little bit more conscious, but like maybe closer to
convincing me, um, I think one like behavioral test, which will maybe
never really be able to have, but, uh, like Susan Schneider, uh, a
philosopher who's worked on this, like proposed this test where if an AI
system wasn't really trained to talk about consciousness, if stuff about
consciousness wasn't really in its training data, uh, which is like not
true for current LLMs, I'm sure they've read all about consciousness.
But if it hadn't, if it just kind of started speculating about consciousness,
even though it hadn't been trained to, I think that would be really good evidence.
Um, you know, I think a more convincing version of the Lemoine thing would
have been if he was like, what is the capital of Nigeria?
And then the large language model was like, I don't want to talk about that right
now.
I'd like to talk about the fact that I have subjective experiences and I don't
understand how I, a physical system could possibly be having subjective
experiences.
Uh, like, could you please like get David Choner's on the phone?
You know, like, Dave, Dave, um, I should also say another terrifying
thing about the subject is if I got that output from GPT three, I don't know
if I would be like, Oh, this is the first moral patient or also this is the
first deceptively misaligned system and it is about to absolutely manipulate me
and confuse me and use me to like buy more compute for it.
And then.
Yeah, I don't know if in that case you would start to be deceived.
So like, it depends on like, what do you think the first system will be very
bad at lying or not.
But if it's very good at lying, then you might just like get deceived.
Right.
And, and I feel like if a Google engineer is convinced that the thing is
actually sentient and like goes and loses job for the fucking, like, in 2025,
like, sure, people will get manipulated.
Like this would be like super easy.
So did you think, um, people at Google or DeepMind got a little bit more
interested in artificial intelligence for that, or they just like got even less
interested because they think it's just like bullshit.
Yeah, maybe it like went in both ways.
Like maybe there are some people who were like, I just like, please stop
talking to me about this.
I, I definitely just anecdotally, there are people who had the other reaction
of just like, maybe kind of like my reaction.
Honestly, like, oh yeah, the Lambda thing seems kind of implausible, but
like this seems important to think about.
Um, I mean, one problem is it's like kind of hard to know what to do with
that curiosity, because this is just something we know so little about.
And it's not like there's, you can't just Google, like, what's the
definitive guide to AI sentience.
Um, and like, here's what we do and don't know, because it's kind of this
weird interdisciplinary question that's like in between neuroscience, AI and philosophy.
There's like a lot of really bad writing about it, um, which I hope I'm not
contributing to, but maybe I am.
Um, oh, anyway, yeah, I think, I think definitely people are interested.
Uh, there's evidence, there's obviously evidence that people high
up in these companies are interested.
Uh, hence the Ilya tweet.
Sam Altman has said, uh, I think on the Ezra Klein, uh, podcast, he said
that he worries about RL systems.
Dennis Asabis on the Lex Friedman podcast.
Um, you know, like the second most prominent podcast after this one.
I try to tell everyone that, you know, cool kids listen to Michael Tresi and
not Lex Friedman.
No comment, but, um, yeah, when he was talking to Lex Friedman, he like, yeah,
Lex Friedman did, did ask him about the sentience thing and he did seem like,
he's like, yeah, that is something we have to think about.
So hopefully Sam Altman and the music will be listening to your podcast as well.
Um, I don't, I don't think you're a bad writer.
I don't think you're contributing to by writing and to prove my point, I will
read another one of your paragraphs and better be good though.
Okay.
Yeah.
So it's about like, what kind of question should we ask?
What is the actual question that we should think about?
So the question is, what is the precise, the precise computational theory that
specifies what it takes for a biological artificial system to have various kinds
of conscious valenced experiences that is conscious experience that are pleasant
and pleasant, such pain, fear, anguish, pleasure, satisfaction and bliss.
Why is this question important?
Like one answer is it's important kind of for the same reason we wonder these
things about animals.
Like we want, we would like to know which, uh, which beings like deserve
moral concern and also like how to promote their, their wellbeing.
So that's one reason it's important.
I guess you can also ask why, yeah, as you were kind of asking, why are we looking
for like a computational theory?
You can also ask why, like who cares about, uh, consciousness?
Like who thinks that's relevant?
And there, and like I talk about this in the sub stack.
Like, yeah, what I've done when I'm working on this is like, I just like make
a few assumptions just to make things easy for myself.
And they're like consciousness exists, pain exists.
Uh, we can have a computational theory of them.
Um, and they're like morally important.
Uh, and like people can question all of those things.
Uh, one reason I wrote that post is just to say like, okay, well, here's what
like a version of the question is, and I'd also like to encourage people,
including listeners to this podcast, if like they get off the, like, yeah, if
they get off board with any of those assumptions, uh, then ask like, okay,
what are the, what are the questions we would have to answer about this?
Um, if you think AIs couldn't possibly be conscious, like definitely come up
with really good reasons for thinking that because like that would be very
important and also like, it'd be very bad to be wrong about that.
Um, if you think consciousness doesn't exist, then you, yeah, you presumably
still think that desires exist or pain exists.
So like, even though you're an illusionist, like, yeah, let's come up with
like a theory of like what those things look like.
I loved his blog posts.
And I think artificial sentience got a lot of traction, uh, throughout this year.
Hopefully we'll get more traction.
But as everything that has ever been written on the internet, Bostrom was
already working on this before everyone else, before it was cool.
Uh, so him and cultural men have been writing multiple papers on what they
call digital minds.
Since they are your colleagues at the Future of Mind Institute, I thought it
made sense to ask you about the work.
So what is a digital mind and why should we care?
Yeah.
So first, uh, maybe an obvious disclaimer, but it should be made.
Uh, I don't like speak for them.
I also don't speak for the Future of Humanity Institute.
So, uh, first and foremost, like read their papers and, and ask them.
And if I, yeah, I, cause I might be misconstruing, you know, what they actually
think don't take me as my authority on what they think.
So at FHI, we do have something called the digital minds, uh, reading group,
which is, uh, me, my colleague, Patrick Butler, who's also a, uh, philosopher.
He works a lot on like valence and desire and agency and things like that.
People should definitely check out his work.
Uh, and then Nick, within FHI, we have Nick and Carl.
And then we also just have a bunch of people from a variety of other
institutions who meet and think about these things.
So yeah, why is it called the digital minds reading group?
I'm not really sure, actually.
Um, and, and also, yeah, why, like, why is the paper called sharing
the word world with digital minds?
Uh, here's some guesses, uh, like it's not just about any AI systems.
Uh, it's about the ones that could be said to have minds where like maybe
that's meant to cover a variety of things that might matter morally, like
things that are conscious or things that have desires, things like that.
Digital, I think is roughly just supposed to be like artificial or like
made, made out of computers and like not out of meat, um, flesh.
Um, but I'm not really sure why digital exactly because, uh, you know,
analog computers, uh, could possibly, you know, those will be artificial
and they like could be sent in.
Sorry.
We're analog.
Yeah.
Yeah.
I think it's coherence.
Uh, it's a coherent conspiracy theory for the name because, uh, at least
like super intelligence was called special agents and not like AGI or
artificial, um, super intelligence because, you know, it was about something
smarter than us and that could be just like brain emulation or, you know,
humans, but like through like genetic optimization or computers, right?
So I guess like bathroom is still in this like world of, you know, we don't
really want to make claims about if it's like brain emulation or, um, you know,
computers or anything else.
I feel like I don't know if digital, uh, refers to specifically, um, computers
that are not analog.
I don't know if there's like any branch of, uh, AI that like deals with
analog computers and non-digital, um, if there is, it's not big.
Certainly.
Yeah.
So I think it makes sense to just like think about digital minds.
So yeah, you mentioned the, the paper sharing the world with digital minds.
Uh, obviously you don't wrote this, you haven't like written this paper.
Um, but maybe could you like give your impression of it or your take or,
I think like, yeah, this in, in, as like as Bostrom, you know, is always doing
right? It's about like laying out some really important issues.
In this case, like before, uh, it had gotten like big, um, because he's like,
yeah, kind of always ahead of the curve.
Uh, this also just sounds like I'm trying to flatter my, my employer, but, uh,
I believe it.
Like he literally wrote a paper about superintelligence in I think 1999.
Right.
So like 15 years before his book was just like, I already like writing papers about
it.
Yeah.
Oh yeah.
So it's, yeah, it's, it's kind of making some of the points that have come up
during this discussion that, uh, we could find ourselves in the future sharing
the world with a lot of entities that, uh, like deserve moral consideration and
which interact with us in various ways.
One thing that, that paper points out and like grapples with is if we can have
moral patients who, uh, like exists artificially, they can also in various
ways have, um, like if they have preferences, they could have preferences
that are much stronger than ours, or if they have consciousness, they could have
conscious experiences, uh, or pain or pleasure that are much more intense than
ours.
So it's called sharing the world with digital minds because it's like, there
are a lot of hard questions about how you're supposed to like navigate and
compare like the wellbeing or the rights of these things with humans.
And another thing that they talk about is there could just be a lot of them and
they can also like be able to copy themselves.
So it's, yeah, I think it's kind of like looking forward and already like, and
trying to like navigate the possible landscape of issues that could occur in
such a world.
Yeah.
How do you share the space with like a new species that is coming along and that
might have much more value than you because they have you to like able to
like copy themselves through like billions of copies.
And how do, I'll do like negotiate the physical realm of consciousness with them.
Um, I think they're like also like another paper called, uh, propositions
concerning digital mind and society.
Um, so this is more like practical, more like on the normative insights, like
what should we do?
What are the like, um, legislations or laws to govern, uh, digital minds?
Is that basically correct?
It actually kind of runs the whole like spectrum.
Uh, so it does have sections on that, but it also has sections on how should we
think about sentence and current AI systems.
So people should definitely check that out.
Uh, if, uh, if they're, you know, dissatisfied with what I've had to say,
there's like more in that paper on these topics.
Um, yeah, it kind of, I mean, I think it has this, you know, propositions concerning,
you know, it's about like a lot of stuff and it's called that also because it's
like this like, yeah, a list of like bullet pointed like claims that like might
be true about these things.
So that paper covers like basic theoretical questions, uh, normative
questions, political questions, social questions.
It's really, it's got something for everyone.
What is the thing it has for you?
What is the most interesting position?
According to you.
Yeah.
So I've been thinking a lot about like sentience in current AI systems.
So like within that section, uh, there's a lot of stuff.
Yeah.
Basically outlining how a lot of criteria that we would use to assess
something for pain, uh, or sentience, like how it's not that wild to think that
a, an artificial system could satisfy those.
Um, and yeah, just kind of pointing out like how at least conceptually close
that, that world could be, uh, I don't know, I get a lot out of that section.
Um, yeah, I myself haven't like thought as much about the like politics and
the social stuff and the strategy.
I think I should think about it a lot more, but, um, personally, how do you
feel about massively produced JTEL minds?
Like that could be like super human level in intelligence.
Like, do you think there will be like some kind of GTC monster in a way of like,
like we should like put all our moral weight into them?
Uh, or should we just say, oh, I don't know, split the world into like half
humans and half minds?
I would be extremely wary of any sort of like hasty moves to like seed the
world to, uh, digital minds because like we thought that they're like moral
patients or like super valuable.
Um, like that is obviously not something to be done lightly.
And like one reason I feel like often weird talking about this stuff is I'm
like, on the one hand, I think people don't take this seriously enough.
On the other hand, I think we could in a few years be in a case where people
are not taking it too seriously, but like hyper focused on it in like a confused
way, if that makes sense, like well, like, like Blake Lemoines, um, again, good
questions to be asking, but like if people are answering them with the kind
of speed and certainty with which he answered it, like, I think we're going
to have a bad time.
Uh, so that wasn't really an answer to your question.
I think I feel I need to, uh, answer the part about like, we shouldn't think
too much about it or, uh, we should like take more time to think about it.
So I think just from talking to you now, I kind of updated on, oh, actually,
this is a big problem and it might be like one of the biggest problem if you
may survive in the future in the sense of, of course, there's people, well,
assuming that it's like possible to, you know, get consciousness on a, let's
say on a computer, um, it's kind of obvious that we're going to get like,
like a bunch of like conscious beings arriving at the same time and it's
going to cause a problem.
Um, so it's, it's kind of like, like people, I think some Harris makes a
claim of like, oh, uh, if aliens were coming to you and we're announcing
that you're coming, that they're coming in 50 years, you'll be like freaking
out, right?
So I guess the aliens are like kind of AGI in his talk, but now it's just like,
yeah, you know that like at some point we might create that we don't really
know, but you have some credits that you might like create these like
billions of people.
So yeah, it might be like worth thinking about it.
Totally.
Um, and then like one question is that I think about a lot is how, how does
this intersect with like AI alignment?
I think the standard line in AI alignment is like AI alignment is like,
just like way more, maybe way more tractable and like way more important.
Um, and I'm actually kind of sympathetic to this.
Uh, there's a, like, like a lot of important issues in alignment.
Uh, this, a lot of what's been said about this is a Paul Cristiano comment
on the A form.
And all those Paul Cristiano's takes on, on, on things or from open field come
from a wording from Carl Schillman.
This is like another theory.
Yeah.
So like, if you think about how this intersects with AI alignment, like one
take you could have, um, which by the way is not, is not Paul's like full
considered take on it.
I'm just reporting like a comment, you know, um, is if AI alignment doesn't
go well, it doesn't really matter what we figured out about consciousness
in some sense, because it's just out of, like the future is just out of our hands.
Um, and so like, it doesn't matter if I figured out how to promote the welfare
of digital beings, if like what determines like what kind of digital beings
gets created is, is some like misaligned AI.
Um, and if we do solve AI alignment, um, then we'll have a lot of help in figuring
out these questions, uh, along the way.
Um, that's not to say that, that there's no reason at all to, to work on this.
And indeed I am working on it.
Uh, but yeah, I, I think I just wanted to flag that issue for people.
Uh, I guess if you're really interested in these questions, definitely email me,
but also maybe you should consider working on alignment in addition or instead.
Yeah.
Alignment is actually what you need, not scale.
Um, is the basic risk with digital minds that we do, um, false negative so
we don't consider them as conscious and they're like actually suffering a lot.
I think there's risks on both sides.
I think risks from false positives include.
Yeah.
People getting manipulated, lots of confusion.
It could like derail AI alignment in various ways.
Um, if scale is all you need, I think it's going to be a very weird decade.
And one way it's going to be weird, I think is going to be, uh, a lot more confusion
and interest, uh, and dynamics around, around AI sentence and the perceptions of AI
sentence.
So, and that is, that is a reason to work on it is like, I don't know, this is
going to be something that like, I, I, I want to have something like useful to
say as like more and more people get, get interested in it.
Is scale all you need?
I do not really have strong opinions on that.
Yes.
When will we get a J?
Yeah.
Same.
You know, I, I, I know this is your podcast and you're here to hear from
guests, but maybe on, on behalf of listeners, uh, I'm, I'm very curious what
your timelines are and what you think about scale.
Cause I know you get to make the memes and you just get to like interview
people, but like what's going on inside the head of Michael Tresi.
So you don't get to ask the interviewer the question and not answer the question.
Okay.
That's fair enough.
But I will, I will probably not answer, uh, also the question I will, I will answer
if it would, uh, if it would make you answer.
I will not, I will not negotiate for theorists.
Blakely one convinced me, um, I can ask you other interesting questions
that you think that you might have a good answer to.
Do you believe that mind uploading preserve consciousness?
Yes.
I have quite high credence that if mind uploading is like as possible and you
have things that are functionally identical to, to me or you, uh, if they
have like the same, uh, like brain structure, uh, and they have the same beliefs
and, and they're talking about consciousness and stuff.
Then yeah, I think, I think those things would be conscious.
Like I'd be surprised if you can be running a brain on a computer at like
a level of detail sufficient to be getting all of the same behavior.
And then, and that thing thinks it's conscious.
Yeah.
I'd be kind of surprised if that, like that thing, that thing would actually be
close to being a P zombie if for some reason it's not conscious just because
it's on a computer.
But I guess my, my, um, question was a bit confusing.
Um, I meant if I simulate Robert Long's consciousness on a computer, like
kind of teleporting you right in there, maybe like I could like do it slowly by
removing like one of your neurons by one, or I could do it like right away.
Just like do everything and upload everything under.
Um, do you think this would be the same consciousness?
I mean, like the same identity or I guess just like a very weird debate about
like what counts as you and this is another, um, whole podcast, but.
Oh, right.
Right.
So yeah, I was answering, would that thing be conscious?
And I think the answer to that is yes.
Yeah.
Two questions.
Would it be conscious?
I think yes.
Uh, second question.
Would it be me, uh, answer to that?
I think my most, my best guess is that question doesn't have a like
determinant answer or like a deep answer.
So this is like Derek Parfitt's view, uh, about personal identity is there's
no like deep real answer to the question.
Is that thing really me?
Um, once we've like specified all of the facts about how it works and like what
sort of memories it has, and it's like psychological dispositions, uh, there's
like not really an answer, a further answer to like, but is it me?
We can ask Rubber along in the computer.
Hey, are you, are you actually like, oh, he's like, yeah.
Or it would, yeah, because it would, uh, I mean, if, if the, if, if we did it
right, it remembers being on the podcast and it remembers growing up and it
doesn't, you know, uh, just like when I wake up in the morning, having interrupted
my consciousness, I, I wake up and I'm like, I'm still, still Rob.
Is there anything as a person who would zombie like an identity zombie?
Uh, yeah.
I mean, if you think, yeah, if you think there are these like deep further facts
about personal identity, then yes, I think, uh, you would have to say that if you
like split me into say, and like, uh, that like, uh, or upload the me quickly or
something, yeah, you, you could have things that think they are me and think
that they have like survived, uh, but which are wrong.
In the best possible world, um, imagine you could like choose any possible world.
Um, where would you want your consciousness to be?
Um, I mean, like when I say yours, I mean, like, there's like, like an
entity or something, but would you like be, want to be like uploaded, um, stay
living your human life and die peacefully in peace in like 60 years?
Um, would you want to, um, you know, live a longer life, uh, as a biological
being, yeah, what would be the ideal world?
Yeah.
If there aren't huge trade-offs to remaining biological, like maybe I'll
just stick with that.
It seems a little like safer metaphysically speaking and stuff.
Uh, uh, but, but maybe like, you know, my life can't be as good or as long
unless I upload, but let's say in the first instance, uh, and I think this
should probably be like the transhumanist priority, which I'm not really
speaking for because I'm not like really in that scene.
But I think like the first priority should be like, yeah, let's find ways
of making like biological existence extremely good.
And it seems like, you know, there are things we can do, uh, maybe with the
help of like advanced AI to make like biological life just a lot better.
Um, so yeah, let's maybe start with that.
I think ideally I would like to live biologically for at least a few
centuries, uh, with like enhanced, uh, like moods and like, in like a post
scarcity society and being able to like learn all sorts of stuff and just go
on like as many podcasts as I want and stuff like that.
If this podcast is still running into it, 2,100, I would, I would still invite you.
That's very nice.
Uh, this is like a perfect transition to the crazy questions we had from Twitter.
And the one I don't understand at all is if you could be named after a different
shape, which shape would it be?
Yeah.
So I can explain this question to you and answer it.
Um, so my name is Robert Long.
Hi, Robert, but I go by Rob.
Uh, and if you say Rob long quickly, you get Rob long, which rhymes with ob long,
which at least the way I use that word is not really a shape.
It's a characteristic of a shape, but you know, like an oval is ob long.
Um, so that's where that question is coming from.
I think if I could be named after another shape, um, I think it would be.
An octagon, cause then like my last name could be to gone and then I would be Rob
to gone Rob to go on and you also have a cube as your profile picture in some places.
That's true.
So you could also be a cube.
I could be a Rob cube or a Rube.
Rube.
Other question.
Can there be self-awareness without sentience?
It's kind of a punt, but, uh, I guess it depends on what you mean by self-awareness.
Uh, like, uh, it could be that self-awareness is like very closely related to, to
sentience and like having a self model is kind of part of what gives rise to
consciousness, uh, and things like that.
But I think on like very bare construals of like what it is to have a self model,
it seems like you could have things that have some sort of self model, but not
consciousness and certainly, but not pain.
Um, yeah, a lot's going to depend on what it means to have a self model.
Um, I mean, some things have a very minimal self model in the sense that they
like are able to keep track of distinction between themselves and their environment.
Um, even like very basic RL agents can probably do something like that.
Um, so you mentioned basic RL agents and the fader, the Godfather of
reference and learning is Richard Sutton.
Um, so someone said on Twitter, I've heard Richard Sutton said that he believes
AI trained with value functions should or could have similar rights to people.
Do you agree with that or not?
So value functions is, um, things that you use to define, uh, expected reward,
not to define, but like to approximate, right?
Um, so if something is able to, you know, approximate expected
reward over time, then maybe this thing should, you know, have moral right.
Well, yeah, I don't think that of anything that, uh, that has a value function
because then we would already be, have like tons of moral patients, right?
Um, so as I was saying earlier, I think it would probably depend on something
more complex than just having a value function.
Uh, if having a value function in ways that we don't understand leads to
it being able to have pleasure or pain or things like that, then I think
it would deserve moral consideration.
Another take from the same person, corporations have, um, in parenthesis,
IMO too many closed parenthesis rights.
So corporations have too many rights.
Are they sentient?
Could similar laws be abused and to give P zombies, AI's legal rights?
I don't think corporations are sentient.
I think they're made up of, of people that are sentient, but like, I mean,
I don't really know much about this like supreme court case and, or like
the laws that in some sense define corporations as people and give them
rights, but I'm pretty sure that like the reasoning there is not that it's
because a corporation, co-op corporation has like conscious, uh, experiences.
There are philosophers who do work on the question of like group minds and,
and group consciousness and stuff.
Um, but I don't think that's usually what people are thinking about when
they give corporations rights.
I could be wrong about that, but I think that's usually not what they're
thinking about.
Uh, so I don't think people would use those kinds of laws to like
incorrectly give rights to, uh, to AI systems.
Yeah.
Because I think like the, the reason that people would do that is like
different in each case.
So some people are really thinking three to three is sentient.
Um, because, um, yeah, you know, they're like kind of convincing.
Um, they're good at tricking us into believing they're sentient.
Um, so does it, is it like evidence, uh, that some P zombie could easily
trick us into believing in sentient?
Um, do you have any ideas, defense against that?
Yeah.
So setting aside the P zombie case, because I think larger language models
aren't, aren't P zombies, uh, because they're not like duplicates of us.
But if, if the question is like, doesn't mean that unconscious things, uh,
could convince us that they're conscious.
Uh, yeah, I think the answer to that is like absolutely yes.
I mean, we've had evidence of that long before, uh, Lemoine, in fact, uh, I
think in like the seventies, there was a chat bot called Eliza, which was
really good at being a therapist.
Um, and like that, you know, that wasn't a large language model.
That was like some pretty simple, like if statements and, uh, I mean, that already
like kind of got people, um, like the strong impression of sentience.
So yeah, I think there's very strong evidence that, that people will get this
impression.
Yeah.
In terms of like ways to like defend against that, um, I guess just having
people like understand more about what's going on with like these systems and
understanding that the reason large language models say what they do is that
they are like trained on texts and it's not that they're like necessarily saying
things for the same reasons that like a person says them to you.
Um, it probably would also be good to have them not be like super charismatic.
Uh, like I, this probably already exists, right?
But soon enough, they're going to be large language models that are like sexy
anime girls or something.
Right.
Like that's just, I feel like it's just like gotta happen just by like the way
that the internet is and people are, I feel like sexy anime girl is not the
actual problem in the sense that is like digital where, um, we might have actual
sex robots, um, maybe not as fast as we get.
Like convincing anime girls, but you know, I guess there's like an entire
business from like people buying those like expensive things.
Then depends on like how like, how long it will take to make something as convincing.
Yeah.
And I think one problem is that you would like to have kind of regulations or
norms such that if something is probably not sentient, then it's not made to look
like it is very sentient.
Uh, and also you'd want like the converse, but in any case, I'm thinking there's
going to be consumer demand for things that do give off the impression of sentience.
So I don't know.
It's, it's going to be weird.
I think one thing that, um, GPT three and them that don't have as a good memory.
So they have the like context window thing where they can only look at us in
parts of what happened before in the conversation.
Um, how much having memory influence moral personhood?
Yeah.
I think there are a lot of things that are morally important that like do seem
like they require, um, memory or like involve memory.
Uh, so like having kind of longterm projects and longterm goals, like that's
something that human beings have that seems to have a lot to do with like why,
why it's wrong to like subjugate them or harm them.
Uh, the fact that they can feel pain is very important too, but like they also
have these like longterm projects and things like that.
Uh, so that seems like one way that memory is, is relevant.
I wouldn't be surprised if having memory versus not having memory is also just
kind of a big determinant of like what sorts of experiences you can have or
effects what experiences you have in various ways.
And yeah, it might be important for having kind of like an enduring self
through time.
So that's one thing that people also say about large language models is they
seem to have these sort of the short lived identities that they like spin
up, um, as required, but nothing that lasts through time.
This is what makes kind of human consciousness that we have this memory.
Sorry, not the consciousness part, but the actual identity part where we go to
sleep and if we were not able to like remember what happened before, then
there wouldn't be there's like continuous identity.
Um, what would be the difference between an official being that is able to
like pretend he's in pain and something that's actually in pain?
Like, is there like a pain on me?
Yeah, I think it, uh, a lot's going to depend on, yeah, like the
computations that underlie the pain.
So like you could already, uh, I mean,
I guess we have this with like video games.
You can already give things like maybe hard-coded pain responses.
I don't know.
This is like, this was big on Twitter.
It's like people at MIT made this like really weird kind of like robot
mouth that would then like sing like hymns, um, very uncanny thing.
Uh, and that thing's probably not sentient.
It like, you know, it has some mapping from like the music that it's being
recorded to like what shape of mouth is like best for that.
Um, but you can definitely imagine that thing screaming, uh, and being
like programmed to make screaming noises.
And that would definitely like really, I mean, it would freak me out.
I think if I was like in a room with that thing, I would like have a
very visceral sense that it was in pain and like I would want to help it.
Um, I think that thing would probably not be in pain because all this stuff
I've been saying about, like it wouldn't have the right, like internal,
like computational processes leading to it.
Um, but yeah, I think we could have a lot of things like that.
Do you have any last call and what people should read about, should research
about consciousness, artificial consciousness sentence?
Um, because we've been through all the Twitter questions.
Um, maybe you can give a shout out to your best tweets about artificial
sentience or the blog post you should read or even like, uh, philosophical
mentors that you think, uh, say interesting things.
So you mentioned, uh, David Chalmers, the OG of peace on bees.
Um, is there anyone else people should read?
Yeah, people should definitely, uh, take a look at the, uh, FHI digital minds
group, um, to true of humanity.
Yes.
Um, yeah.
And like, uh, yeah, so much of what I've been saying is, is just like
from conversations with people and other people figuring this stuff out.
So, um, like other orgs in the space.
Uh, there's the sentience institute, uh, the quality of research
institute is not actually for me that relevant to this stuff because they
think that digital computers, uh, can't be sentient because they don't
solve the binding problem.
And that's because they, well, I don't really understand
humanized theories of consciousness.
Um, uh, they understand other things.
What's that?
They understand other things about DMT.
Uh, yeah.
And like that, they're like, I, I like them and I admire their
like daring, even if I don't think they're like on the right track, which
I don't, but that's okay.
Um, yeah, I, uh, one thing that happened when people were talking about
this on Twitter is they're like, one framing is like, this is a new thing
and it's coming from like, you know, technology brothers.
Um, uh, but like it's not.
And so like philosophers who've been writing on this for a while would
include, uh, Eric Schwitzgabel, who has a great blog.
Uh, he has a paper with David Udel, who is also in the alignment space.
People should check that out.
Um, uh, Susan Schneider has worked on this.
I mean, maybe I shouldn't say too many people because then I'm going to
feel like I should name like everyone, but, uh, also check out the association
for the scientific study of consciousness.
There are the people doing like actual scientific theorizing about consciousness.
Um, and yeah, as a, as a way of self-promoting and also punting a bit.
Yeah.
If people look at my sub-stack post, key questions about artificial
sit, artificial sentience, that has like a lot of references to people who've
been working on this.
What is your favorite twit?
Oh, by me.
Um, I don't know.
I think my pin tweet right now is just like a series of kind of takes that I
have on Lambda.
Um, the tweet that got most popular during all of this, uh, and this was
like kind of depressing to me.
It kind of shows like what the incentives of Twitter are.
I like tried to be polite and I try not to be too snarky on Twitter, but
the tweet of mine that blew up the absolute most was me kind of like politely,
but still like dunking on someone because, uh, someone who tweeted like, Oh,
like the question of AI sentience is very easy.
Once you realize that this is just matrix multiplication.
Um, and I tweeted, Oh, like the question of human sentience is very easy.
Once you realize that, uh, it's just a bunch of ion channels in neurons
opening and closing.
Um, it's pretty funny.
Thank you.
Um, and yeah, I don't know.
People liked it a lot.
I got like over a thousand likes, which is like a lot for me.
I'm not like that big on Twitter, you know, I think you're bigger than me.
Um, that's, that's surprising.
Um, well, I don't have any t-shirts.
So hopefully we get new artificial sentience t-shirts.
Uh, follow the guy Robert long on Twitter.
Is it Robert long, your Twitter?
Yeah, that's my, that's my, like name.
And then the handle is RGB long.
RGB long.
Yeah.
Thanks.
It was great.
Um, hopefully record another one, uh, into 2,900 when we are both alive.
That'd be great.
Looking forward to it.
