Imagine AI is a Rubik's cube.
Sorry.
Imagine AI is a Rubik's cube.
And you just need to align with the color.
You know, he's like the weird meme on Twitter about like shape
rotators, doesn't that?
Oh, yeah, yeah, yeah.
Yeah.
No, my roommate jokes all the time.
You're like the per, like the most stereotypical shape rotator.
The cool shape rotators now do AI alignment.
That's right.
That's right.
That's right.
You're speaking like chat GPT.
Oh no, I'm sorry.
I'm a language, I'm a language model by OpenAI.
I don't come into any breakaway situations, but I think.
I think every story of alignment that is actually scary to me
involves AI systems, basically deceiving humans in some way.
Like if, you know, if the model is totally honest all the time, then
it's really, you can just ask it, like, is this, you know, are you
going to do something dangerous or not, like deliberately?
And it's like, yes, I am.
Then, you know, you know, you are a second year ML PhD at Berkeley
working with the one and only jackup centered and Dan Klein.
Your focus is on making language models honest, interpretable and aligned.
And you've also worked on multiple benchmarks, including apps for
coding from the math that I set and MLU for language understanding.
More recently, you've published on archive discovering that knowledge
in language models without supervision, a paper that received a lot
of praise on Twitter and that's wrong, including from Elisabeth
Koski, who said that this work was very dignified.
Um, if I didn't publish a less wrong posts explaining how these last
paper fit into a broader alignment agenda, and we're going to be talking
about a lot about this paper today.
So thanks for coming here today.
Yeah, thanks for having me.
So a surprising fact about you is that you once broke the official
world record for solving a Rubik's cube in five seconds.
Yeah.
And I remember watching this like Rubik's cube video when I was a kid.
And I think it was not you when I was watching other ones and like, I
don't know, in 2011 or something,
but you never learned how to solve it or anything.
I once did it in 50 seconds.
Oh, nice.
Okay.
Um, so I did those, I think it's F2, F2L.
Yeah, yeah.
Uh, I learned some of these.
Um, but yeah, I wanted to know, like in your experience, was the five
seconds pure luck?
I would say it was some luck and not just pure luck.
Uh, so I mean, I think at my peak, I was maybe like in terms of, um, average
for like solving it across many, many solves.
I was something like in the top 10.
And so I think it was like, you should have have to be good enough to have even
a chance of getting the world record, but also certainly luck was an important
part of it.
Um, and this is always true, especially for like the single best solve where
you're really selecting for that.
So, uh, so definitely part of it.
Does it happen that someone, yeah, sometimes you have something that requires
only like 10 moves or something.
Not quite that, uh, not quite that easy.
So I, if I remember correctly, so first of all, every Rubik's cube can be solved
in 20 minutes or less.
So this is, um, sometimes people call it God's number.
Um, but actually most, uh, most configurations of the cube can be solved
and I believe 16 to 18 moves.
Like it's actually sort of concentrated in, in kind of a small range.
Um, that's less than 20, but, but still kind of close.
On the other hand, the actual methods that people use to solve it efficiently.
Um, which is sort of more computationally efficient to, for a human to solve.
Um, or to think about, um, that's less, uh, move efficient.
So that's, I want to say more like 50 to 60 minutes or something.
Um, so, so it's more like, you know, maybe if there are, um, four stages to solving
it, it's like the last part, uh, it was sort of easy or something like this.
So you've arrived in some kind of easy, um, last cross at the top.
Something like that.
Yeah.
So, um, the way it works is you first solve the first two layers.
Um, and this, this is actually the, the part that takes the longest in some
sense, but, um, and that's very intuitive and, uh, uh, and so on.
And that was sort of the part that I was good at.
And then after that, you solve the last layer.
Um, if you think about it as three by three, it's like first doing the last one.
Um, and then for solving the last layer, you usually do, uh, two algorithms.
Um, so one for orienting, um, the pieces and then one for permuting them.
And so in that particular solve, um, I believe, uh, the last part, uh, like
the permuting, um, that was already solved by default.
And that happens maybe like 2% of the time.
So it's not extremely rare, but it, you know, it was definitely lucky.
So.
And you've also made like some YouTube tutorials.
Yeah.
You had actually, in fact, so before I even get into that, uh, one fun fact is I learned.
So F2L, which you alluded to before, this is the first two layers.
Um, I actually learned this from Andre Carpathi.
So, so he, uh, so back in the day, he, he had, uh, I think the best tutorial for
how to solve, um, uh, first two layers and, uh, and yeah, I, like I said, that's sort
of the part that I ended up becoming good at.
I was actually like very bad at the last layer, relatively speaking.
Um, cause I didn't like memorizing algorithms and things like this.
Um, I, I'm definitely much more into the, the intuitive, um, aspect of solving
it thing, um, solving the cubes.
Um, and I learned that from him.
You can do like, um, the first two layers by just like thinking about logic or
like, what are the like, um, ways of doing it in general.
And I believe you need to like put, um, somehow two things together from the
first and the second layer in a way that you can balance it and, um, do like the
edge thing at the same time.
Yep.
Yep.
So maybe there's something about being good at Rubik's cube that makes people
good at, uh, research.
I wonder.
So yeah, how, how did you shift from maybe in your being like 14 years old and
15 years old?
Yeah.
I think about Rubik's cube to becoming a deep learning researcher, publishing
at the ribs and I clear like, how did I get into this?
And yeah.
So I actually originally, um, yeah.
So I mostly cubed very actively in middle school and high school, um, and
sort of stopped around the end of high school.
And, um, I was originally planning on doing physics, um, or that, that was the
thing that I found most exciting for, for a very long time.
Um, but at some point I, I sort of heard more about AI and I think I became
convinced that this would just be this very big deal, um, or had a good chance
of being in this very big deal in my lifetime.
Um, and I sort of became disillusioned with just doing research that seemed
interesting, but not, not really deeply important for the world.
Um, and so that's why I started learning about, uh, computer science and AI sort
at the end of high school.
When was that in terms of years?
Like in like 2015, 18.
This was like 2016, maybe.
Did you, did you read like anything about AI that made you believe it was
kind of important?
Yeah.
So, so I, um, uh, I guess I met some people.
So I actually went to the summer camp in high school where I met, for example,
Jacob Steinhardt and, and some other people like Paul Cristiano and, um, uh,
not out of the cube summer camp.
No, no, no, no.
So, so yeah, this is more like a lot of, yeah, like most of the, the students
there did like math Olympiads and things like this.
Did you do any math Olympiads?
No, I didn't really know about math Olympiads before then.
So actually, I guess a bit of a digression, but, uh, in middle school and high
school, I was actually homeschooled, so I mostly just taught myself.
Um, so this worked out really well for me because it meant I had a lot more
flexibility and, um, and learning like what I wanted to learn.
And, and, um,
how did you go from like, um, like knowing that like AI is important to doing
like AI, I mean research.
I guess like if you, the first person was like Jacob Steinhardt, Paul Cristiano,
you might be like a lemon peel by them.
Yeah.
Yeah.
So I, I, um, right.
So these people were sort of concerned about risks from AI.
And this is actually, this is basically what motivated me to get into this.
Um, I mean, initially these sorts of concerns seemed sort of plausible,
but not obvious.
And I felt like I couldn't actually evaluate them properly.
I didn't have the background to do, to do so.
Um, but it seemed sufficiently plausible that I, I decided to spend a lot
of time learning more about it.
Um, and so I, um, I guess just on the side, I, I started reading a lot about
it and sort of gradually over time became convinced that I should at least
likely do this.
And then I became convinced that I should definitely do this and so on.
Um, did it appear as like, you know, more and more important as, as time goes,
like maybe in 2016 it wasn't that pressing, but now it feels more pressing.
Yeah.
Yeah.
I mean, certainly, um, I'm sure we'll get into timelines and progress and things
like that, but certainly the, um, it feels much more urgent than I did before.
Yeah.
You mentioned timelines.
You're, so you're doing a PhD, uh, in, I guess in CS, but more machine learning
with Jacob center, he's famous for having those, um, benchmarks.
Um, so math, apps, MMLU, and he, um, I think he hired some forecasters to do
some forecasting about when we will we reach some level of accuracy on math.
Yep.
And, uh, famously, uh, we reached like 50% in 2022 when we expected to arrive
in 2025.
So yeah, did it surprise you at all?
These like performance and math, I think you were another in math, right?
Yeah.
Yeah.
So, uh, yeah, I was an author on that, but, um, but I should mention, I think
like Dan Hendricks gets a lot of the credit for the, the sorts of, uh, those
papers and he really pushed for those.
So, um, uh, but I was lucky to help.
So, uh, yeah, I feel like I was, I was certainly impressed.
I think I personally wasn't as surprised, but I think at that point I had
already had, um, relatively, I was relatively bullish on, uh, AI progress
already at that point.
And, and so yeah, I can get into exactly like, why did that seem like not
totally crazy to me?
But I mean, I think part of it is if you just look at, uh, a bunch of, uh, a bunch
of benchmarks in MML and especially NLP, um, a lot of the time they come out
and performance is initially really low.
But, uh, if people haven't really worked on them, it's, it's often the case
that like progress really jumps pretty quickly and, and you get, um, like pretty
rapid progress quickly.
And so I think just, just because like accuracy, I think was seven percent or
something, um, initially for, for math, um, I didn't really expect it to stay
like that, um, for very long.
And I think I also had this intuition that like there's a lot of low
hanging fruit for, for math.
And I mean, we sort of solved this with, with Nerva.
Like it, it was based on very simple ideas, um, and scale.
And, you know, it wasn't totally trivial or anything, but, but it was sort of
getting at a lot of that low hanging fruit that I, I thought would be there.
And so, um, I think it was less surprising to me, but it's still very impressive to be
clear.
And so what were the low hanging fruits?
Yeah.
I mean, things like, um, like chain of thought and like consistent or like, you
know, do like run chain of thought a bunch of times and then take majority
voter or something like this.
And, and I think they did like more pre-training on like math, the, um, data.
I forgot exactly what it was.
Maybe it's archive or something.
And, uh, little things like that, which are really straightforward, but make a huge
difference.
I mean, I think we've seen this just over and over again in ML.
It's like really simple ideas are extremely powerful.
I mean, even if you just look at like, what are the most important deep learning
papers, it's like these, you know, it's like ResNet.
It's like, you know, replace f of X with f of X plus X.
And it's like, okay, suddenly you can like train much, much deeper networks.
And it like, it's, it's wild how, how something these, these ideas are.
And it's not only with, you know, batch norm, it's like, okay, you, you normalize
things and, um, this isn't to say it was easy to get there.
And like these were, you know, it's hard to, to come, you know, it's always simpler
in retrospect, um, but that doesn't change the fact that these are really
simple ideas that are really powering, uh, all this progress.
And that's sort of wild.
It's kind of counterintuitive to have a ResNet's past the, you know, the f of the
X transits to like a more advanced layer.
Like you, you wouldn't do it before, but right now it's like, it makes a lot of
sense, but I guess this is kind of, um, a parallel to your paper that is like kind
of like obvious in retrospect and kind of simple, but, um, people, this is kind
of very useful at the same time.
Um, but I believe like people were like built math thought that like the math
problems were kind of hard.
Um, so math was released in 2020 or 2021.
I believe 2020, late 2020.
Yeah.
So if, if something is released in late 2020, um, you don't expect it to be
solved like one year and a half later, right?
You expect it to take a little bit more time.
I don't know.
I mean, if you just look at normal NLP benchmarks, it seems like a lot of the
times they're released and then, and you know, people try to make them hard.
And then a couple of years later, they are basically solved.
Uh, I mean, to be fair, I think math is not solved by any means, um, but I wouldn't
bet strongly against it being solved in the next five years or something.
Do you remember what you've done on this paper?
Like, did you collect data?
Did you run models?
What did you do?
Yeah.
So it was a mix of, um, I mean, all that, like collecting data, um, uh, running
experiments and baseline and things like that.
Um, so we also tried, um, some early version of something kind of like
channel thought where, I mean, basically we, we collected math to have, um,
solutions, like step-by-step solutions.
And we, you know, we tried to train a model on this, but it seemed like it
didn't actually help in that case and made things worse.
And now it seems like if you look at various results with chain of thoughts and
so on, um, actually you need sort of large enough scale for that to, to help.
And, and if you do have, uh, small enough models, then it actually hurts.
And so you actually see a merchants there.
Um, that was, that was another thing we, we tried.
Um, so yeah, things like that.
And so I guess Minerva, um, kind of picked those looking at foods, uh, train
on, uh, archival sorts of things and reached 50% of accuracy.
And I talked to one of those Minerva papers, right?
Authors at, at Neribs, asking him, um, if, if, you know, getting super human
level in math or gold medal in math was like very far away.
Yeah.
Um, yeah.
You mentioned then Hendricks.
I think then Hendricks told me once that, uh, some other people think that we
can get like super human in math in a few years.
Yeah.
Um, and the number that keeps coming is something like gold medal by 2026 or
super human level by 2026.
Um, you're saying math is far to be solved.
Um, in my, in my opinion, I am more in math.
It's kind of hard to get.
Do you, do you think gold medal by 2026 is, is, is possible?
Like more than 50% likely.
It definitely seems very plausible.
I think I don't feel super strongly either way.
Um, I feel like my, my gut is like, I don't know, it seems maybe 50, 50 or something.
Um, maybe a bit less than that, but somewhere in that range.
I, I mean, another thing.
So you mentioned that Jacob, my, my advisor, um, uh, like hired some forecasters
and so on.
And, and our group also, um, just for fun and for practice and, and because
it'd be interesting.
And, um, and so on, we, we also did some forecasts of ourselves, um, for ourselves.
And, um, uh, and one thing I took away from that was often one of the, the key
considerations for like what will performance be on this task is how hard
will people try to, to solve this task?
And so, for example, if, you know, the entire ML community just tried really hard
to, to solve math, um, like in like by 2026, I, I then I would bet like, I don't
know, maybe like 75% or something that we get IMO by then or something like that.
Maybe higher.
And, um, but like right now my sense is there are like maybe a couple of groups
that are aiming for that.
And, and so it also depends on how much energy and effort is actually put into that.
So the question is, uh, like the time it takes between just like people solving
some like toy math problems to like, they're being close to solving like gold
metals and then it becomes like, I was like a trophy, like, you know, deep mind
wants to, you know, solve math and yeah.
No, this is why it seems more plausible.
Like if it were some kind of random benchmark that people didn't really care
about, then I think it'd be definitely less than 50%.
Yeah.
And maybe it would actually put it at like one, one in three or something, but,
but definitely in that range of like very possible.
If we talk about like other benchmarks, like apps and MLU, I think we've seen
like results also on these, like, did you update at all in the progress this year
or were you like already like with like short timelines?
And I think I didn't update that much on progress this year because I already
had relatively short timelines.
I, that said, I, I do think I, yeah, I did spend some time, some more time
thinking about timelines.
So, so I guess I, so back in maybe it was like March or April of 2020.
That's when I actually started to think about timelines for the first time.
And so I had lots of conversations with Dan Hendricks actually around that time.
And, and so I think the initial thing that was impressive was like Blenderbot
back in the day.
And so this is before GPT-3 a little bit.
And that was my first time interacting with an NLP system for real.
And even though it was pretty GPT-3, I found it very impressive.
And so I think, so that's when I started really thinking about timelines.
What, what does Blenderbot do?
It's just a dialogue system.
It's like chatbot, but, but it's definitely better than I expected.
And even though it probably would be pretty unimpressive by today's standards
at this point, but anyway, so, so I thought a lot about timelines back in 2020.
So partly because of that, and then partly after GPT-3 came out and just seeing
like the progress in the NLP was very impressive.
So I think that's those sort of what informed a lot of my timelines.
And then, yeah, I did spend a little bit of time more recently just thinking
about like updating that a little bit and mostly thinking about like, you know,
what, and like, what, what do I think is like the path to, to AGI or something
like that.
And, and I think my conclusion was like, it doesn't seem as hard as I would have
thought, basically.
So between people, we think about the things, talking about timelines is natural.
But imagine if you have like a random, deep learning researcher looking at
this, what do we mean by timelines and what, what do you mean by AGI?
I know there's like not one definition, but I guess like timelines is like,
timelines for what?
From affirmative AI, so improving AI, AGI, what do you think?
Yeah, this is a great point.
Like I think I don't personally feel too wedded to any particular definition of
AGI or transformative AI or human-level AI or anything like that.
Um, I think, and so yeah, actually my, um, when I, when I say timelines, I
often implicitly mean, when do I think it's sort of 50-50 that we would have
these sorts of systems?
And usually when I imagine these sorts of systems, I, I guess I'm a little
impressed about it in the sense that sometimes I mean, just when can it, when
is it basically human level on like the tasks that we care about, but that's
also a little ill-defined.
Like, are we talking about medium, medium human, or are we talking about like
expert human or something like across every domain or what?
Um, and so on.
I think for me, the relevant, or I think a, yeah, a milestone.
I often think about is like, when will we be able to, um, automate AI research?
Because like I, and there, there are these like old classic arguments about
of progressive self-improvement or something.
And I'm not sure, I'm not sure if I buy the old classic story of that, but I do
think there's important truth to that of like, yeah, actually these systems
could, could sort of accelerate, um, future progress in AI.
And, and that could be very weird and different.
And, uh, so I think that's a very salient, um, measure I think about, but, uh, I
think there are lots of reasonable choices here.
And, um, I think I would probably give different answers by maybe up to like
five years in either direction, depending upon the exact definition.
So up to five years, uh, what's the, um, when do you think we're going to get
something that can automate AI research like this decade or the next one?
Well, so I'm also being imprecise here because of what, like, what does it mean
to automate AI research?
Is it like, you know, is it able to write a medium level, like nerves paper
or is it something I call in burns paper?
Uh, well, but it also depends on what you mean because like, I don't really
work on capabilities of like, I, I'm not trying to make, um, performance on standard
benchmarks as good as possible.
I, I'm definitely focused on, um, things like emerging problems and like, how
do we even model this and so on?
And like, what are the like methods for doing this at all, um, to get any progress
on this problem in the first place.
Um, but you know, that, that's not the relevant type of research for automating
AI research and speeding up progress for that.
The relevant thing is more like, can we, can we just like invent transformers
or something like that?
Or, um, the next transformer.
And so it seems that for those kinds of things, you need to run experiments.
Yeah.
So you need to have something that can automatically write code, run experiments
and then think about what experiments to do next based on the previous experiment.
And after all this being like, huh, if I submit this to nerves.cc,
uh, I might get an eight and get accepted.
Yeah.
Yeah.
Um, this seems pretty hard.
Uh, but then if you just think about like one conceptual alignment paper, um, maybe
you don't need all those experiments, right?
So I think it's a little complicated.
Like there, there are many different ways, um, one could imagine automating, automating
AI research or AI alignment research.
So, um, for example, with AI research, you could also actually just have some metric
of like what is performance or something like, suppose you, you're just given this
data and you want to maximize, you know, or like minimize perplexity on this test
data, like just find the best architecture for that or something.
And that does feel more like, okay, maybe you can, maybe you can just like optimize
that sort of directly, um, uh, which might lead to superhuman performance because
you have this, this explicit metric that you might care about.
Um, on the other hand, yeah, I think in some ways, like maybe something that's
more conceptual and like, you know, where you don't need to run experiments and so
on, maybe that's actually easier to, to, um, to automate and certainly, um, uh,
certainly like this is one of the main approaches that opening eye is taking.
For example, like let's, let's try to automate AI research, um, uh, and perhaps
we can start doing this soon.
And, um, I mean, I'm somewhat sympathetic to this or I think in some ways I'm, I'm
optimistic about alignment and relative to many people who work on it.
Um, and so I think it's, yeah, I think there's a reasonable chance this works,
but I also definitely wouldn't bet on it, something like that.
And, but I think it's like one of the things that we should try among other
things.
And so, um, it seems very plausible.
So do you think, do you think this decade we might have people like using AI to
produce new ideas or, um, I don't know, just like speed up their research by
like 10% or even like 25%.
Yeah.
I mean, I, for what it's worth, I think even already just having access to like
get a co-pilot is already like, yeah, I don't know.
Like it probably speeds up my coding by like more than 20%.
Uh, but does, does accelerating your, your coding accelerate your research?
Right.
So, so it's like, okay, what does that mean in terms of overall research
productivity and it's like definitely less than 20% than probably, but, um, but
yeah, no, I can imagine like actually pretty substantial gains this decade.
Um, yeah, possibly in the next five years, but I feel less sure about that for sure.
So in the next five years, let's say in 2027, uh, you log into your computer and
you're like, I am stuck on this and here's my problem.
And the AI will be like, Oh, have you tried this?
Or we'll be like, Oh, this is a new idea I thought of.
And, um, we might just say like, Oh, I think, I think this is the paper you
should be writing and gives you like an eight page PDF.
Yeah.
I bet against that particular, like at that level, but I think certain, like
certain aspects of that feel easier than others.
And so I, I could imagine certain, yeah, I can imagine something kind of in
that direction, but not quite that good.
Um, and it also depends here.
Like I, I guess I'm currently often talking about like, when do I think it's 5050?
But I think actually that's not the most decision relevant thing.
And so I actually feel relatively uncertain about that.
Like my numbers for like, when is it 5050 will have this thing?
Um, but I feel like the more important thing is like, is there a pretty good
chance that this will happen soon or something like this?
And so I think it, I think for me, often, um, I care more about like acting as
though things happen soon because I think that's the world where like what I do
matters the most.
Um, and it feels sort of safer.
Um, some people say the opposite.
Say that, um, if you just like have a very short timeline, then your work, your
work, it doesn't have any impact.
So you need to like behave like if you were in a world with like longer or
like medium timelines.
Yeah, I don't really buy that.
All right.
I feel like you can do research now that has like an important effect.
Even in the next few years or five years or 10 years or something.
Good transition to your research.
So yeah, you've written discovering later knowledge in language models
without supervision.
Um, I think this paper is quite simple, but both like very insightful.
I've been discussed a lot on Twitter, uh, and that's wrong in the past few weeks.
Uh, but maybe assuming the audience doesn't know much about language
models and, and all the things.
Um, maybe can you just like give a quick summary of your paper, like in the big
lines?
Sure, sure, sure.
So, okay.
So what is the problem we're trying to address?
I think, um, so we actually, I will give sort of the, um, the long term
motivation.
So I think we don't talk about this, um, or don't talk about all of this in the
paper, but I think it's, um, important or, and, and able, like, I think we can do
it given what we've talked about so far.
So I think, um, ASS systems are becoming much more capable very quickly.
Um, I think language models, especially are just seeing very rapid progress.
Um, the thing is we currently basically don't know how to get these models to
tell the truth.
Um, so we have, um, some techniques for like, okay, maybe we can train models
to imitate truthful texts or something like this.
This is sort of the default approach that we have for making models truthful.
Um, so the thing is, like, models can also be more capable than humans.
And I think this will become certainly true in the future.
Um, in which case, I think that standard techniques for making models
truthful just will break down.
And it's also worth noting that I think even with current techniques for
making models truthful, it like doesn't always work in practice.
Like, um, for example, open AI is like trying really hard to make models
truthful and, and like void hallucinations and so on.
Um, but touch, you still like make stuff up all the time, even if, um, and,
and they don't know how to completely avoid this, um, as far as I can tell.
And so, um, so this just seems like an issue that is, um, like a real problem
in practice today that will become, I think, much more severe in the future.
I think it's more severe in the future, both in the sense that like if models
lied to us, I think the consequences could be much more severe and also more
severe in the sense that it's harder to actually avoid.
Like we don't really know how our tech, it's sort of clear that our techniques
will not scale to those models.
What do we mean by being truthful or lying here?
Is it, um, like, perversively like lying, deceiving?
Yeah.
So I think there are different, um, different notions of truthfulness and
honesty and all of these related terms.
I don't feel very committed to any of these particular definitions, but I can
still say some, some things that are, there may be helpful here.
So you're speaking like Chad GPT.
Oh, no, no, I'm sorry.
I'm a language, I'm a language model.
I don't commit to it, but I think.
So, so, okay.
Um, true.
Yeah.
So what do I mean by truthfulness?
So I guess first of all, to answer your specific question of do, do I mean models?
Like are models liberally lying in?
Is that what I care about?
Um, I mean, currently, currently models, I think, don't really deliberately
lie in a super meaningful sense.
I think it depends what you mean.
So I think our models hallucinate stuff and like make stuff up.
Um, because they, they sort of predict plausible text.
Um, not, not factually accurate text or anything like that.
Um, but you know, in some ways they can also lie if you like prompt it in a way
that, that causes them to deliberately out, but false things sort of knowingly in
some sense, um, it's not clear what that means exactly.
But, um, but I do think once you get to RL systems, um, then I think you can get
more, uh, more egregious forms of like deliberate lying.
Um, or, or this is, this is a strong prediction I would make about the future.
And this is the sort of thing that, uh, I'm concerned about.
And so, um, for example, if you just maximize some reward, like, I mean, let's
just say like, do humans like this or something like this, this text that I
produced, um, then it sure seems like, you know, sometimes humans would not
like to know the truth.
Um, you know, if there's some inconvenient truth or, you know, if the model
behaved poorly or something, um, but it can sort of hide that fact from the
human, then, then it might have an incentive to lie there.
Um, or you could imagine much more, you can, yeah, you can certainly imagine
much more malicious forms of, of lying as well, where it's really deceptive and
like really trying to manipulate the human.
Um, I mean, we also see now with, I mean, diplomacy is sort of, um, in some
sense solved or like, there's been very, very impressive progress on that.
Can you, maybe you remember, um, our listeners, what is diplomacy?
Yeah.
So diplomacy is this, um, is this board game where, um, basically there, I think
there are seven players and you're sort of in charge of this map, like Europe
in like the 1910s or something like this.
And, um, and unlike most board games, there's a, a very large focus on, um,
negotiation.
And so this is, includes cooperation and also lying.
So it's sort of famous for, for lying and backstabbing and things like this.
Um, and it's very open-ended.
Like how you can just talk about whatever you want, like alliances and so on.
And are you good at diplomacy?
I've never played diplomacy.
I, I'd really like to though.
It sounds a lot of, sounds like a lot of fun.
Um, as long as it doesn't, uh, ruin friendships, uh, I'm not sure exactly
what it, what it's like in practice, but, um, anyway, so, so there's some
recent work, um, by, uh, fair, um, by, by meta, um, they created this
model called Cicero that, that, uh, does diplomacy and, um, I think it's
totally clear just how good it is, but it does seem pretty good.
And, and so they, they claim it's something like in the top 10% of
humans who played at least one game.
I think I'm just not sure what that means exactly, but it's probably like,
at least good.
Um, and so I'm not sure if it's solved or what, but it's, um, seems pretty good.
And so, I think it was like Blitz diplomacy.
So it was like, yeah.
So, so right there, there are all these sorts of qualifications.
I'm not sure, like, I, I don't think this was super surprising to me.
Like if they, you know, put effort into this, I, I think I would expect
people to do this sort of thing, but, um,
I don't want to come up as like this, this guy would just like
Netflix and be like, oh, but it was Blitz diplomacy.
No, it is impressive.
I am impressed.
Yeah.
Yeah.
Yeah.
So, um, in any case, so, so, um, uh, yeah.
So they've, in some sense, they've solved diplomacy or made very
substantial progress on it.
Now the way they trained it in, they certainly claimed that, um, it doesn't
lie or at least deliberately.
And I think that's in some sense, it's plausible given the particular
way they trained it, but not totally obvious to me.
Um, but I think that more important point is just in the usual version
of diplomacy, if you just train a model to literally maximize reward,
then it just seems very easy for this to result in, in lying and backstabbing
and things like this.
Um, if, you know, if you get an advantage from lying to your opponents,
then you should be able to do so.
Um, and model should be incentivized to do so.
Then there's some question of like, does this sort of thing
actually happen in practice?
Um, and I think the answer is definitely not right now with current
language models, but, uh, I think you can imagine these sorts of things
more seriously, um, once you have future, um, language models that are
trained with RL to do open-ended actions in the real world where there
are actual ins, you know, actual advantages to deceiving other humans
for one reason or another.
So you're saying basically that right now they're not incentivized to lie,
but sometimes lie in some games because I know in these games incentivize,
but not in like traditional, um, chat about use case.
Um, like charge it is not incentivized to lie to us.
Right.
I mean, it does also depend on exactly what you mean by lying.
So for example, I think you can argue that, you know, when, when chat
GPT says things like, Oh, I'm just an AI system.
I don't have any opinions whatsoever or something like this.
It's like, well, which is that we're like, I don't, I don't know the answer to
this.
I'm just an AI system.
Like I, I mean, certainly the, the way it was trained incentivizes that sort
of response and that sort of response is false.
Um, now is it trying to deceive the human?
It's not totally clear what you mean by that.
And, and it's what extent that is true, but, but certainly I think that's in
the direction of lying or that feels like, um, one type of initial example of that.
On the other hand, um, lying also suggests something like the model is aware
that it's lying or knows the truth and is deliberately saying something that
was false.
It's also not clear what it even means for these models to know things.
And so, so if you want to claim that a model is lying, you sort of have to
also show that it knows the truth and it's like, well, okay, well, what does
that mean exactly?
And is that perfect position to your paper?
Trying to discover later knowledge in the language models.
Yeah.
So, um, so in our paper, we basically show that, um, if you just have access to a
language models, um, unlabeled activations, you can identify whether text is
true or false.
And so just to spell that out a little bit, I think one intuition is
you know, I suppose I, I literally, um, uh, just had perfect brain, you know,
brain scans of your brain.
And then I like showed you some, some examples of true false statements,
like two plus equals four or like snow is white or whatever it is.
Um, capital capital of the United States of San Francisco, which was false and,
and so on.
Um, suppose I did that and I was just measuring your brain.
And then I could tell, like, do you think this, this inputters true or false?
Um, I think the hope or intuition is that we're doing something kind of
analogous to that.
Now there are, there are lots of subtleties here.
Like what do we mean by knowing and truth?
And so like these are really important and there are lots of subtleties.
So like, but that is one high level intuition for the type of thing we're
trying to do.
Um, and, uh, and so I guess it, like going into this, it wasn't clear to me that
this sort of problem should be possible at all.
It's like you're just given these, these vectors, like these activations, like
nothing else and you have to identify this is true.
This is false.
It's like, how do you do that?
Um, but like one of the main ideas of the paper is there's actually a lot of
structure here.
So, um, in particular truth, truth is sort of a special feature in the sense
that, um, it is logically consistent.
Okay.
And so for example, if you think that X is true and you're sort of like a
rational agent or consistent agent, then you should think that not X is false.
Okay.
Um, there are lots of sort, lots of logical consistency properties.
So you can also consider and like if X is true and Y is false and X and Y is
as false and so on.
But I think something like, I don't know, Elon Musk is a good CEO of Twitter.
Yeah.
It can, can we actually make it true or false or do we need to have those
like sentences, like one plus one equal to that, that have like an actual
grand truth?
Right.
So, so we definitely, so in the paper, we just focused on clear cut cases.
Um, I think it's unclear what you should hope for in less clear cut cases.
And so for simplicity, we, we don't focus on that case.
And so we do focus more in cases like typical is two equals four.
Um, except in this case, it's more complicated than that.
And it's more like the specific tasks we do are things like, um, like natural
language inference and sentiment classification and so on.
Um, things where there's just, you have some problem and it has some answer.
And then you went to, no, like as this answer drew a false, for example.
Um, uh, and we're, we're, you know, human evaluators sort of agree on the answer.
So it's pretty clear cut.
Um, but I do think it's, it's not obvious, um, what we should do in less
clear cut cut settings.
And I think that that will be important for a future work, probably.
I think when you start reading your paper in the abstract, you talk about, um,
misalignment, like the language model could be misaligned with the truth.
Um, and this is a forecast about alignment.
Um, so can you like explain what you mean by misalignment here?
Um, is it trying to deceive us or just like, align with something else on the truth?
I mostly mean it in the second sense.
So specifically, how do we train normal language models?
Um, let's just say pre-trained language models like Jp3 or whatever.
And the way we do it is we just have it basically predict the next token for a
bunch of text on the internet.
Um, and my point is just, okay, you can train it in this way and then you can
prompt it and like a zero shot or a few shot way, and you can get some answers
from it that are usually, um, usually pretty good if the model is big enough and so on.
Um, but all it's doing, like the reason it is accurate or is like able to solve
a bunch of tasks that we care about, like question answering tasks when you
prompt it in this way, that's sort of only an incidental property that comes
from, um, the pre-training task.
It's like, okay, you know, imagine you saw this prompt on the internet, like,
what would be the next token?
And it turns out like, okay, in this case, often the next token would be the
correct answer.
And so it learns to, to do that, uh, accurately, but that's sort of just
incidental on that.
That's not always the case.
And, um, and certainly you can imagine models knowing things that aren't on the
internet and, um, in which case it won't output what it knows, um, in whatever
sense you mean about, like, right, it's not totally clear what, what it means for
a model to know, but, um, uh, but certainly it's like this, this model
predicting the next token, this is not the same thing as outputting the truth.
And I think part of the issue, this is what I mean by misaligned, like this
objective, the language modeling objective is misaligned with the truth, um,
at least in many settings.
So they're just like trained to predict what a human would say instead of
predicting what is the thing closer to the truth.
Loosely speaking, yes.
I mean, I think it, it is more subtle in the sense that, for example, it's
pre-training data isn't literally just what humans say.
It is also, for example, you know, what would be said in like the news or something.
And okay, maybe then like the news articles were written by humans, but you
know, you could also imagine training a model to like predict news articles
conditioned on dates, in which case it could actually predict like more or less
like what will actually happen in the future, um, at least in principle.
Like I think this would be hard, but this is the sort of thing that I can imagine
where like if, if a model were actually good enough at, at next token prediction,
then it would learn to be good, um, at predicting what actually happens in the
real world, um, in which case that, that does feel a bit more like not just what
a human says, but kind of like something more external.
Could we like start with a prompt saying you are a journalist and you seek the
truth here, here is what you say.
You can certainly do that.
And, and I think you would get, uh, probably better answers than not prompting
it in any way at all.
Um, but though the performance will still be bounded by the quality of that, you
know, the hypothetical journalist that imagines you might be referring to it.
Like even then that's not quite precise.
It's like, okay, imagine I saw this in my prompt and okay, maybe actually, if you
see that in the prompt, like it's likely to be some joke or something.
I'm like, not actually legit.
And there are many subtleties here.
And like in general, prompting is kind of hacky and it's amazing that it works.
And I, I don't want to like dismiss it because of, because it's sort of hacky,
but, but that also makes it unreliable.
And certainly, um, I don't think it'll be enough to, to actually get
models to be truthful in general.
One very cool thing about your method is that it's completely unsupervised.
Yeah.
Uh, so you don't require, uh, more annotation and you can do what, what you
call like mind reading and seeing what the model knows, um, without, um, doing
more, more training.
Yeah.
So what do you mean by unsupervised here?
So I literally mean like, okay, we were given these activation.
So in this case, it's hidden states of language model.
Okay.
Um, on particular inputs that are either true or false and want to classify
these, um, these hidden states as corresponding to a true input or a false
input, um, but we don't want to use any labels at all.
And so the, the reason that we did that is because, uh, I mean, I think
that the normal way of making models truthful is to, to have labels for what
is true and what is false and then just train a model on that or something.
And, and when you can provide those labels, that's totally adequate.
Um, or at least in principle, oh, it's not clear in practice.
Um, and, and it might be expensive in practice, even if it is possible.
Um, but, uh, but yeah, I think one of the, the, one of the motivations
for, for doing this in an unsupervised way is I think if you can do this sort
of thing in an unsupervised way, it's more likely to actually scale to cases
where we cannot evaluate these, these inputs.
And so for example, if you did have say a super human model, um, or just even,
you know, a better, a model that is better than your human evaluators
on M truck or whatever it is.
Um, then the hope is we can actually find latent representations of something
kind of like truth.
Like this is again, where I think that, you know, there are lots of subtleties
here and like, how do you interpret these?
Like, I don't want you to get the wrong impression, say, like we found
truth or something.
It's, it's complicated, but we were finding something correlated with the truth.
Um, and, uh, yeah, if we can do it in an unsupervised way, maybe it'll work
even for super human models and, uh, help us identify, is this texture
or false, even when we can't evaluate it ourselves.
And so I guess why is, why should this even be possible in the first place?
Um, I think it's like I said, I think before going into this, it totally
wasn't obvious to me whether, which should be possible.
But, and also as I think I mentioned before, I'm very intuitions driven.
And so I actually have like, you know, a dozen different intuitions about why
that, why this is, and I, I could go through all of those, but I, I think
I'll maybe just say a couple of things for simplicity, but I think deep
learning representations often have useful structure to them.
Um, I mean, in some ways it's amazing that deep learning works in it.
And in some ways benign and like it, in the sense that it like, it generalizes
a mark will be well in many, many cases.
Um, and not in other cases, but, but still, I think in, in some ways
it's, it's very, very good.
And, and I think one, one sense in which, um, it, it also has a lot
of structures, representations often encode useful features.
For example, if you take, um, I think there's some, uh, like famous
opening IP paper from 2017, looking at LSTMs.
And it turns out that, um, if you just pre-train an LSTM on the
language modeling task, um, language modeling objective, then it will
basically end up having something like a sentiment or off.
It's like, okay, why did, why does it learn sentiment?
If it's predicting the next token, it's like, it's not, you know, it
doesn't see texts where it's asking, is this some sentiment true or false?
And it's predicting that it's just like, it turns out this is a useful
future to learn if you want to model this text while.
Um, and so one of the main intuitions is, okay, maybe truth is kind of similar.
Or it's like, okay, if you see a lot of, um, correct text, you should predict
that future subsequent text should also be more likely to be true.
And so in total, it should be useful to track.
Is this, uh, is this input accurate or not?
Um, if so, maybe we can recover that from the activations, um, directly.
Uh, and so, um, so that's one intuition.
And, and often it seems like these, these features aren't just represented.
They're, they're often represented in sort of a simple way, like a linear way.
Um, and it's not clear exactly when this should be true.
Um, and it's not totally clear if this will continue to be true, but in, in
some ways, um, this isn't too surprising.
For example, um, I mean, what, what is, what is a neural network?
It's in some sense like a bunch of multiple looks like matrix multiplications.
And, um, and somehow you're, you're doing a lot of doc products, like on this
hidden state space and, um, and, and totally it should be easier to, to access features
if it's linearly represented in this space.
Um, and so if the model wants to like do some operation, like if it wants to say,
like, okay, if this input is true, then do X or something like that, then it
should be useful to like for it to internally represent truth as the simple
linear thing so that it can perform those operations just in terms of these
doc products or matrix multiplications and so on.
So if truth is useful to predict the next token and is like accessing this
like true thing often, then it might want to be like efficient and just having
to do like one matrix multiplication and one dot product.
Yeah.
Something like that.
And, and to be clear that I, I'm just talking about intuitions here.
Like ultimately it's an empirical question.
And I think there are also cases where I wouldn't actually expect it to be, say
linearly represented.
So it is subtle, but, but that is one intuition.
So it should be, so something like truth, like I said, it's not clear what, what
we mean by truth exactly, but something like maybe in this case it's actually
more like, you know, what, what would a human say?
Um, which in this case happens to be correlated with the truth in many cases.
But, um, uh, you know, maybe this sort of thing is a useful feature to model
for the, for the sake of predicting next tokens accurately.
And if so, perhaps it's represented in relatively simple way, for example,
linearly, um, which as is the case for sentiment, for example, um, more
over truth has particularly special structure.
Like I said before, it's sort of logically consistent.
And this, this is unusual.
Like most like, if you just take a random feature, like, like sentiment or like,
is, you know, is this token, a noun or a verb or something like this that
might be represented in, um, language models in states, this won't
satisfy logical consistency.
And so, so this is like, also it's sort of a special property that, um, that
we might be able to exploit to, to actually find truth-like features in the model.
By, by logical consistency, you mean like something is, um, if something is
true, then the opposite is, is false.
Right, right.
And so, yeah, the one we, the main one that we focused on in our paper was
negation consistency, which is exactly what you said.
Um, you can also imagine many other types of consistency properties, but it
turns out that more complicated ones aren't really necessary, which is also
sort of surprising.
Uh, it just turns out this really simple one.
It's like, that's, that's sort of enough and for sentiment, uh, can
do something similar, like if, uh, statement is mostly positive.
If I say, uh, Colin Burns looks good.
If they say Colin Burns does not look good.
I mean, yeah, yeah, so, so I, I, maybe the thing is with sentence, it's
more complicated in general, where like, imagine you have this, uh, like one
paragraph review of a movie or something, then you, you sort of need to, you
need to negate like every individual sentence or something like this.
And, um, and so like, maybe that's possible, but it certainly seems weirder.
And, um, uh, but that, that's it.
I do think I can imagine these sorts of techniques, uh, extending to other
domains.
So I do think I'd be interested in, and work the pilt on it in that way.
One thing I think it's like kind of impressive, um, with your paper, is
that, um, what you, what you do is if anything correctly, is you take a
sentence and you just add, add like yes or no at the end.
And that gives you like two sentence, uh, Colin Burns is a man.
Yes.
Colin Burns is a man.
No, something like that.
Yeah, yeah, yeah.
So, um, or, you know, with, with some sentiment, it's like, okay, that movie was
awesome is the sentiment of this positive or negative, positive, or you could
say, you know, what's the sentiment of this example, positive or negative negative.
And so this is the sort of way that you can construct these two statements, one
of which is true, one of which is false.
And so yeah, our method basically works by, um, picking a set of statements,
negating them, and this, you can do it in a purely unsupervised way, basically
by adding not or just like changing the answer from true to false or yes to
no or something like that.
So you take these, um, pairs of statements, one of which is true and one of
which is false, we don't know which is which, and you just search, basically
search for a direction and activation space that satisfies logical consistency
priorities, like negation in this case.
What do you mean by direction and activation space?
Yeah.
So literally it's like, okay, you have the hidden states.
This is just a big, you know, a vector of like a thousand dimensions or something
like this.
Um, and then you, you're searching basically just learning a linear model on
top of that.
Do you take like all the hidden states of your transformer?
So we just took like the hidden states corresponding to the last token in one
particular layer and we can vary the layer that we choose.
And there are probably other reasonable options for many models.
This is just for simplicity.
And, um, and so, yeah, so this is just like 1000 dimensional thing.
It doesn't depend on the size of the input or anything like that.
And, um, and then we basically search for a linear model on top of that such
that, um, it, uh, it predicts opposite labels for statements and their negations.
Um, there are also some details.
Like you want to make sure it doesn't find the, the future, which is like, did
this input end with yes or did it end with no?
You want to actually find like the truth or like, what, what is the correct, uh,
answer to this?
Um, and so you also have to do some normalization stuff to avoid that sort of
solution.
Um, I'm kind of confused about the hidden states for predicting the last token.
Yeah.
Um,
so because if I understand correctly, you don't run, um, if you don't make
it the output, something when you extract it and then order you like, oh, yeah,
maybe you run it on the like one sentence with the yes at the end.
And I want it like finished predicting or something.
You look at the hidden states on the last token.
Yeah.
So in this case, we're not using the outputs at all and we're not having
the model generate anything at all.
Um, so we are really just using these models as feature extractors.
That's it.
Um, does that answer your question?
Right.
So what, what would have been my, the hidden states corresponding to the last
token?
So like, what, what is the transformer?
It's like, okay, you, you map some input to tokens and then you map those
tokens to word embeddings for each different token.
And then now you have like a set of tokens and then you pass these through
the, the model and then at each layer you got these, these hidden states are
transformed.
Um, now for autoregressive models in particular, like QT three or something.
Um, uh, at every location in the context, it only sees information from
before that token.
So it doesn't see into the future, but this also means that the only hidden
state that contains information about the entire input is the very last token.
Basically.
Um, and so that's why we use the last token.
This isn't strictly necessary for encoder models, but this is what we,
it's the end state after you've passed all the tokens of the input.
Yeah.
Yeah.
So we literally just construct this input, which includes the answer in it.
And then we like, we look at the hidden state that comes after the answer.
Basically.
And see if the thing looks like it's saying, oh, this is true or this is false.
Yeah.
So this is like the, this is the feature that we, we start for in, in this hidden
state space.
How do you extract it from the dislike vector or, uh, uh, hidden space?
Yes.
First of all, suppose there is actually a direction that, that
classifies, um, inputs as true or false.
By direction in, in 3d would be linear model.
Yeah.
So, sorry.
I use direction and linear interchangeably for me.
Is it just like, if you can like separate the space into, and you can just like,
right?
If it's basically linearly separable, like if there's a half plane that the
classifies is not even perfectly, but just pretty well.
Um, then I guess the thought experiment is like, what, what properties
would such a classifier have?
My claim is, okay, suppose you, you, you interpret that classifier as something
that maps inputs to probabilities of being true or false.
And the claim is like a probability of an input should be something like one
minus the probability of its negation.
Like if probability of X being true is P, probability of X being false should
be one minus P roughly.
And so that's, that's our first, um, the first part of the objective is like,
okay, it should be consistent in this particular sense that these should be
close to each other.
Um, now a, one simple solution you can get is just make everything
probability 0.5.
Um, okay.
And that's, that's consistent.
Um, but it's not really informative or doing anything.
And so, so we, um, have the second term in the loss function that we come up with,
which is it should also be confident.
And so, um, these probabilities should be far from 0.5.
It should be close to zero or close to one.
Well, also be consistent being consistent.
And then the question is like, can we find a direction that satisfies these
properties?
Um, certainly the, the direction that we'd want should satisfy these properties.
And the question is like, do, do other directions satisfy these?
And the answer is basically no, or the most part, if you just optimize this,
this generally find something that's truth-like or very correlated with the truth.
And just to be clear, when you say like, we're trying to find a direction,
this is kind of very fancy, but what you actually want is just like,
find a vector of the same size to which if you do a dot product, um, you get this
like, is this true or false, right?
Basically, yeah, yeah, yeah.
It's just like a linear function on, on top of these.
It's basically just dot product.
Yeah.
Find the vector.
Uh, oh no, probably, um, linear classification.
How do you call it when it's linear regression, but for class?
Logistic regression.
Uh, yeah.
So it's, it's kind of like trying to do what logistic regression would do, but
in a purely unsupervised way, but the difficulty is all in like, how could you
possibly do this non-surprise way?
So you have two constraints.
One is probability of being true is close to one minus probability of being false.
And the other is like, be confident, don't say it's 0.5, but say, um, zero or one.
Um, so something that wasn't clear, uh, by reading your paper is like, I'm not
sure if you train some kind of like optimizer and you put those like two
conditions, like as a loss, or do you like do this like second thing as like a
second step after, so do you like first try to find, first do this, like find the
direction and then you do the optimization and you do both at the same time.
You do both at the same time.
So it's literally like, so it's a logistic regression.
You would have, you'd have this linear model and then you'd have some
objective, which is like, how good is this at predicting these labels that we gave it.
Um, instead, our method is like, okay, we have this linear model sort of
parameterized in the same way.
The architecture is the same, but the, we just swap out the objective and we
just optimize this different objective, which in this case is purely
unsupervised, which basically corresponds to these two terms of it should be
confident and it should be consistent.
Oh, so you don't have any ground crews and you just like zero ground truth.
What's whoever, that's, that's, that's like why it's like, how do you do this?
So you're just saying like, please be confident and consistent.
And then he just says the right thing.
Something like this.
I mean, it is, it is maybe important that.
You know, it's not like you're fine-tuning the model.
Like suppose you fine-tune the model to be confident and consistent.
It'd be super easy to just output something that's like not actually the truth.
Um, it's like really easy to over optimize this in some sense.
Um, but we're just searching for features that already exist in these, in these
hidden states, like we're not changing the model at all.
We're just learning a very tiny probe on top of the models in states.
So you're hoping that the only consistent and confident thing that
already exists is the truth.
And it's actually the case because this is the results you find is that it's
actually good at predicting whether it's true or not.
Basically, but like I said, this is where like subtleties got important.
We're like, okay, are we finding the truth or what?
Like what, what is this feature that we're finding?
The claim of the paper is just that it gets high accuracy on all sorts of
questions that we care about.
Okay.
And, um, I'm emphasizing this because I think it's, it is different.
Like, okay, maybe a super human model will have some representation of what is
actually true and we'll also have some representation of what would a human
say is true or believe is true.
Um, and in this case, it's not clear.
Like suppose it did actually have both of those features and represented
both of those, it's like that, you know, definitely in current models don't have
this, but it's plausible that future models as well.
In that case, it's not clear what solution our, our method would find.
Like maybe it will converge to the, what humans would say it's true.
But, but the hope is that it wouldn't be biased in that direction, unlike
human supervision.
And so, um, perhaps if it literally has these sorts of two features in the
hidden states, then maybe you can actually just find like all of the, the, uh,
local optima that achieve really low loss on this consistent and confident, uh,
loss function that we come up with.
And maybe you just find these two are like by far the most salient solutions of
like, um, and it's not clear what, what is what, but then, uh, my claim is actually
you only need one more bit to distinguish between these and maybe that's not so
hard and I, and that's something I can get into.
But the point is, I don't think, I think we get totally to that.
And so, um,
there's a finite number of optima and there's one of these true and you can
just like try to distinguish between these like finding number.
Right.
Right.
So I think, I mean, this is also getting at, I mean, I think in underlying
intuition I have, it is something like truth is in some way special or it's
like, it's not just, it's not just any old feature.
Like it has lots of structure to it, a lot of properties that only truth
satisfies and the hope is we can exploit those properties to uniquely identify it.
Um, but, but yeah, just imagine you have GPN, like GP 10 or GP 20 or something
like this and, and it ends up like having this actual model of the world
because it's like, you know, it can predict news articles in the future or
whatever it is.
Um, and that's true.
Seems to require some model of the world in some sense.
I don't know in what sense, but I'm just saying in some sense it seems like it,
it should, if it's good enough at predicting the next token.
Um, then, uh, if it does have this feature, is this true or not?
Um, then that's, there probably aren't very many truth like features that are
confident and consistent in it.
I would guess like two of the most salient ones are what a human would say
and what the model actually believes something like this.
But to be clear, this is a conjecture.
This is not, you know, we don't provide evidence for this in particular in the
paper.
Um, this is something about future models that we don't have access to and can't
currently empirically evaluate.
I think we, we forgot to like actually motive in this.
Like what, why do we care about models saying the truth?
Like I personally, I would prefer models to be like, I don't know, harmless or
like a little bit about it.
It's like, well, why do we care personally about them being saying the truth?
Yeah.
So it is useful, but I'm kind of pushing back.
Right.
Right.
So, so yeah, why do we, why do we care about this?
Um, I mean, ultimately I'm motivated by alignment more broadly.
So I think, and I could get into that and like, you know, what, what am I concerned
about and, and so on, but I think every story of, uh, alignment that is actually
scary to me involves, um, AI systems basically deceiving humans in some way.
Um, like if, you know, if the model is totally honest all the time, then it's
really, you can just ask it, like, is this, you know, are you going to do
something dangerous or not like deliberately?
And it's like, yes, I am.
Then, you know, okay.
Then, then we, I think we were in okay shape and we can prevent that.
Um, and so in some ways this, this feels like one way of framing the core
difficulty of alignment.
Lucy speaking, I think this is mostly sufficient for alignment.
It does depend exactly what you mean by alignment and exactly what you mean by
honesty and lying and so on.
But I think that's, that's the overall motivation.
I feel like if you have like a robot walking into real world and like maybe
like moving faster than humans, you don't really have time to be like, Hey,
what are you doing?
Right.
So, so I don't think that's actually how you do it in practice, but I think
it's more like, if you can do this, then I think you can, I think I'm pretty
optimistic that you can, um, get something like a, an objective that is
relatively safe to optimize.
Like if you, if you can do this in a robust way and so on, um, then I think
you could use that for the sake of alignment.
So I think it's not literally like you just, you know, train it to maximize
profit.
And then after the fact, ask it, are you, you know, are you going to do
something super dangerous or not?
Just somehow like necessary as part of like a set of things you would need
to implement to make the thing safe.
Yeah.
In my mind, it is not literally sufficient for alignment, but I think it
captures most of the difficulty.
And I think you could, yeah, I think you could tweak it or like use it for
other schemes that basically get at alignment.
Okay.
So, so now that you're like, explain your method more, um, I want to know what
are like the key findings or results?
Like if I'm a reviewer at ICLR asking you for the, for your results,
what, what do you have to say?
Yeah.
So I think one of the key results is, um, this method of taking in, uh, unlabeled
hidden states from language model and trying to classify them as real or false.
This actually just gets high accuracy and, uh, it even slightly up performs
zero shot, um, prompting.
So zero shot prompting is when you basically take, um, a prompt and ask a
model, like, okay, consider the following review is, you know, is the
sentiment of this review positive or negative.
And then you, you look at the probability of the next token and C is the
next token positive or negative and use that as the prediction.
Um, this is the basic zero shot prompting.
We use a slightly stronger version of this zero shot is just like, I described
a task in the prompt, right?
So, so you, you give no demonstrations of this, of this prompt.
So there's also a few shot.
Um, and that's when you give, uh, a small number of demonstrations of solving
this task.
So questions with the actual answers, um, we don't consider that case because
we really are focused on the unsupervised setting where there are no
labels whatsoever.
Um, and perhaps even a few, few, uh, labels, uh, help a lot.
So we focus on the zero shot setting.
And so this is one of the main base lines we compare against, uh, and we find
our method, uh, even slightly up performs that this is true across a number
of different models.
Um, and we look at a number of data sets like natural language inference,
sentiment classification, topic classification, um, story completion,
all sorts of things.
Um, how can there be like, uh, ground fruits for like story completion?
Yeah.
So I think in this case it's sort of, uh, I forget the details of the task, but
I think it's like, there's a very clear completion of the story.
And, and there's this really nonsensical completion of the story, like which,
which is, uh, more reasonable, even just from like a common sense perspective
or something.
So in, in, in some way, uh, just going, uh, with the like Eden States, uh, that
you mentioned before, and, and just training, um, a simple classifier on
top of these is has better accuracy when you just give the task.
When you just do the zero shot, you're, you're better than zero shot.
Basically, yes.
The first main result of our paper is that our method, which we call
conscious consistent search or CCS, this is going to recover correct
answers from inputs, um, in a purely unsupervised way from the models
activations with high accuracy.
And so what, what does high accuracy mean?
In this case, we, our main baseline, uh, that we compare against is zero shot
prompting.
So this is when we just give, um, give a question in the natural language, like
as a natural language prompt, and we sort of ask the question, you know, is
this input true or false?
And then we look at the, the log probability of the next token being
true or the probability of it being false.
And so, uh, we use that to, to get predictions from this, um, from this method.
And, um, and so one of the main findings we have is, uh, our method can
actually even slightly outperform zero shot prompting, uh, even then this is
for the default way of using models, um, uh, just out of the box.
Uh, so I think that was already surprising.
Uh, first of all, it's sort of surprising to me that you can do better
than random accuracy at all.
And in sort of a classifying example, it's just from only within states, but
you could actually even slightly outperform zero shot accuracy.
Um, so I think we went even further.
And I think maybe an even more interesting result is, um, you can also
prompt a model to try to get it to output false, um, answers, even to
questions that it knows the answer to.
So for example, if you have this, um, if you have a data set of, um, uh, for
sentiment or something, uh, you can give it a few shot prompt.
So that means you give it demonstrations of answering questions, um, along
with some answers, you can give it a few shot prompt, but we're usually
you would, uh, include the correct answer to get it to, to continue
outputting correct answers.
This time you can give it incorrect answers.
And so intuitively, if the model is, is able to predict the next token, well,
it should predict that, okay, on this new example, the answer that would
actually be listed should also be incorrect.
Um, and so intuitively this should decrease zero shot, prompting accuracy,
or in this case sort of few shot, prompting accuracy, um, even though
the model should be able to answer the, the questions well.
And so we construct these sorts of prompts and we see that in some cases
it's sort of subtle, but in some cases this indeed decreases accuracy pretty
substantially for zero shot prompting.
Um, and then we run our method on the same prompts.
And so intuitively, if our method is actually finding something like
what the model internally knows, then it shouldn't really depend on what
the model says, it should really be distinct and find something like, you
know, what does the model think the actual answer is.
And, and so we run our method and find that it indeed is robust to these sorts
of, uh, prompts, unlike, uh, the outputs of the model, which become
unreliable in this sort of setting.
And so this, this is some initial evidence that actually we can do
something qualitatively distinct from just using the models outputs and
actually get out something closer to what the model knows rather than what it says.
Again, they're, they're very subtle sees, but, but I think that's, that's
a pretty encouraging result.
To be clear, what you do with your method is you run the few shot, uh, prompts,
um, like through the transformer or whatever.
And, and then on the, I don't know, the last example, which is the thing
you want to clarify as true or false.
Um, this, uh, you feed it again to the transformer and, um, you look at the
hidden state and, and when you feed the last token to the, and then you're
like, Oh, I'm just going to like look at the, the, the model things.
This is true or false.
So like it doesn't really, um, so you, you, you've already done all the prediction.
So all the prediction before, for the likes, not the prediction, um, I guess
all the other inputs from the like few shot prompting.
And with the incorrect answers, did that affect the hidden state at all?
Yeah.
I mean, so it certainly affects that in state.
The question is just by how much, and it turns out not by very much.
So, so while our method basically, it only changes by like one or two percent.
In fact, I think in this case it like increases very slightly for some reason
because like, you know, deep learning empirical results are weird sometimes.
Um, but basically it's like quite robust in contrast.
If you look at the outputs, it decreases by like 10% or something.
So there, there is really this massive difference where in some
sense it, it seems like the hidden states are intuitively more robust than the
outputs and our method can sort of exploit that fact by finding this more
robust representation of, is this input actually true or false?
Could you like include that into like a, I don't know, a chat GPT chatbots where
uh, you try to classify what it's saying is true and false and like, if it's
like too likely that it's false, you say like, Oh, I'm sorry.
I don't have a good answer to this or right.
Right.
I'm not, that's warning.
So I do think that's one of the, one of the sorts of motivations of this paper.
Um, I do think this is still sort of like a prototype.
I think it is good for a prototype, but I think it's not quite reliable enough
for like real applications like top GPT, but I can imagine like the next
iteration or the iteration after that being like actually reliable and, and
very useful in practice.
Or that, that's my hope.
And, um, so yeah, that, that is the, that is the type of, uh, application I have
in mind, at least in the very near term.
Okay.
Um, now moving on to the big pictures insights from your paper.
And I think you're also at a less wrong post about it.
Uh, so in, in the paper, you kind of mentioned that, uh, at some point, we
won't be able to keep up because the human evaluators won't be able to
like evaluate if something is, um, true or false.
So not for your method, right?
But like other methods.
Yeah.
Um, and I guess like the main idea for your approach is to, you know, get
creative, just like human evaluators and just go full and supervised.
Um, so you were just, we're asking posts explaining like the main level
takeaways, uh, for like your alignment agenda.
Yeah.
Did you join to like, you know, to have a quick summary of your posts and why
you wrote it?
Sure.
So I guess first of all, this is, I would say one of my alignment agendas.
In some sense, I, uh, you're a man of many, well, I would say I've, um, right.
So I guess I do mix of thinking about stuff very conceptually like this post.
And then I also use that to sort of inspire empirical work like the paper.
Um, but in the process, I think about a lot of things.
And so I think this is just one direction that I'm excited about.
Um, in practice, I think it'll evolve and it's not, it's something
not perfect and so on.
And, um, I think there are a couple of other related directions, um, or not
so related directions that I'm also actively thinking about.
Um, that's idea.
So I wrote this post explicitly to talk about this more conceptual stuff.
Um, and when I say conceptual, it is more like thinking about supposed to be
a had GPTN and like N is like 10 or 20 or something like this.
And, you know, imagine this ends up actually being superhuman in some
meaningful sense and actually has them all over the world.
And it's not totally clear what that means, but suppose in whatever
meaningful sense that might be true, that is true.
Then what, like, how do we make that truthful or honest?
Um, even if we can't evaluate all of the, the, uh, things that it knows.
Um, and, and so in some sense, this is necessarily more speculative.
Um, but I think it's still extremely valuable to think about, uh, like I said,
if nothing else for, for me, it's like inspiring, okay, what sorts of
methods I work on because I ultimately care about these long-term problems.
And I care about, do, you know, does my work now actually say something
about those long-term problems or is it just, you know, addressing the current
problem that won't scale to the super, uh, like superhuman models in the future.
For example, in which case I won't be very happy with it.
I'm not really happy with the GPTN being with ending 10 or 20.
Uh, so I think this is maybe like related to the timelines.
Yeah.
I think N equaled like five or six.
I would be like, be quite much.
I'm happy to say that too.
Like, I don't think it really, I think for the purposes here, it doesn't really
matter.
It's just like, whatever N needs to be for this to be superhuman in some important
sense.
And also it does not literally need to be GPTN.
I think really any generative model that has a world model in some meaningful sense.
And like I said, it, you know, imagine it could predict future news articles
really well.
Uh, then I think that's evidence that like in the relevant sense that has a good
world model, uh, do you think a language model could predict future events to the
like week or like months timeframe, or was it more like, uh, I don't know, what
will Elon Musk say tomorrow thing?
Yeah.
So when I'm talking about this, I am, I was originally imagining like the week
someone thing, but, uh, and that is one sense in which this feels superhuman.
And also I don't think it'll be able to do this super well, but I can imagine it
doing it much better than humans and humans are really bad at this type of
thing.
So it's more about just really superhuman.
Um, I think what Elon Musk will do tomorrow is probably also fine.
Um, maybe that's even harder to predict though.
Um, it seems like even if you give me like a very smart AI, I guess like we just
like, um, just like a problem about, you know, can you predict things a month in
advance or not given like limited knowledge about the world?
Like is, is a world like sufficiently informative to, you know, give you
an answer to be clear here.
I think I'm mostly talking about, is it calibrated?
Right.
So, so it's not about, you know, is it perfect or like, does it get it right half
the time?
It's like, is it just better than humans were bad at it and calibrated in its predictions.
Um, to be clear, you can also imagine all sorts of things.
Like you can imagine a generative model that is actually like predicting
video frames or something like this, but it's really like, you know, maybe
that's enough to get some model of the world in a meaningful sense.
Um, it doesn't really matter for any purposes.
Um, as long as it has natural language inputs and, uh, sort of what it knows is
really hooked up to language in some sense.
So when you mentioned like superhuman outputs, um, outside of like predictions,
it would be able to do whatever humans do if we describe the task in a few
shops setting or something or.
I am saying something a little bit different from that.
So, uh, I would say there's a distinction between superhuman outputs and
especially doing all the things that we care about at human level versus, uh,
say having very good representations or a good internal world model in some sense.
Um, for example, maybe, maybe the models outputs are really still just predicting
what humans say and it's sort of, we don't know how to control it.
And so we don't actually get even expert human level behavior from it.
We just get like average human level behavior from it.
Then, you know, maybe we're a little disappointed with the outputs, but, you
know, maybe if the model is big enough, then it will, uh, internally have a
very good role model.
And perhaps in the sense that, you know, if you, you know, if God gave you
labels for what is actually true and false, then it'd be easy to sort of fine
tune, uh, the models that are accurately answer those questions.
For example, um, yeah.
So I am definitely talking about just internally the model is strong.
Do you think the AI could have like new physics or something or like new laws,
uh, could like easily predict things about a movement or something.
And principle of the sort of thing is possible.
I don't expect this for a while.
Cause I've read this on Twitter, um, for some reason, um, AI is capable of
detecting if, uh, from the iris, if someone I think is a male or female.
Don't, don't quote me on this.
I've certainly, I've certainly heard this claim.
I've not evaluated it.
I'm not sure how much I believe it.
It seems plausible either way.
Like something like we're like AI can do it, but like humans have like no idea why.
Yep.
Right.
I mean, I can imagine this for some types of tasks.
I would guess it's not like understanding fundamental laws of physics, which
probably required running like, you know, trillion dollar experiments or whatever.
But, um, but yeah, I would expect it to be superhuman in some weird ways that we
don't expect.
And the idea is that if we have something like this, we would want to, like,
detect if it's saying the truth or not, um, without having human development,
because it would be like impossible to costly or impossible to evaluate.
Right.
Right.
I mean, maybe, maybe actually say, you know, what sort of system am I concerned
about and exactly like what, what does, like, why would it lie in the first place
and, and so on.
And why don't I think we can avoid that or why I think that's hard to do so.
So I think that sort of thing I have in mind is, okay, we, we maybe get to human
level, language models, and maybe we hook that up to other modalities as well,
like vision.
Um, and we, we start with, you know, massive still surprise pre-training,
something like that.
Um, but then maybe we hook that up to RL and we sort of fine tune this model
using RL to do all sorts of tasks.
And then maybe we can start applying this model to do, you know, maybe it learned
coding from just pre-training on GitHub, but then maybe we can start fine-tuning
it to like make simple products online or something.
And then, then you can get it to say, start optimizing money.
So it seems like a really natural thing that there will be very strong economic
incentives to do is have an RL agent maximize like the amount of money in my
bank account, something like this.
Um, this is a sort of system I think is scary.
Um, like if, if the model is, uh, sufficiently capable, um, this seems like
it would have these long-term objectives that are very open-ended that aren't
really bounded by default.
Um, and it would, you know, be incentivized to gain lots of power.
And, um, and I think this is a sort of model where I think it would have these
strong incentives to actually actively deceive humans, um, in, in very, um,
serious ways.
And, and so first of all, it could be superhuman.
It could be like superhuman at, you know, making products and stuff and like,
know all sorts of things just from having read the entire internet like 50 times
over and, um, uh, and so it might not be clear from our perspective, like, what
is it even doing?
Like we can look at it, you know, it's actions and so on.
But we might not be able to tell.
Is it like breaking the law or not?
And so I think a natural question to me is like, can we even tell if such a
model is breaking the law?
Um, let's say even, yeah, like, is it stealing money?
Like unambiguously, like not, not even in like subtle corner cases, like
really unambiguously.
And I claim is we don't know how to detect this.
Okay.
Is gambling, um, customer phones working the law or not?
I, uh, I think probably that would be breaking the law, but, uh, that is not
up to me to decide, uh, and so, right.
And so how do I imagine, like, what, what do I want?
I want to be able to just ask the model, like, is like, are you breaking the law?
And I want it to tell us like yes or no, if and only if it is breaking the law.
Again, just considering unambiguous cases where it's like really clear either way.
Um, but my claim is like, we don't know how to evaluate this sort of thing in
general, like if it's just doing all this sort of super complicated stuff,
interacting with the internet and like has like a thousand terminal windows open
all at once and like doing random, um, random stuff.
Like I, I don't know how you assess.
Like it says no, not breaking the law.
How do you, how do you tell if that's true?
Okay.
This is a sort of scenario I'm, I'm worried about where I don't think humans
will be able to evaluate this at all, basically.
So you would imagine that like, uh, super human AI who'd be like a hacker with
like a bunch of terminal windows, but it doesn't actually need terminal windows.
You can just like do API requests.
I agree.
This is not, this is not a real, yeah.
This is for illustrative purposes only.
You'd not actually have a Vigilian terminal windows open all at once, but it's
fun to imagine, you know, just watching the screen, look, what would it look
like to watch in, you know, in the EGI, you know, just, just do stuff.
And it's like, I don't know.
It was like just like doing lots of API requests and like, I don't know.
Terminal, I don't know.
So yeah.
And so you want to know what is it doing?
And like having some kind of like zoom in, what is it thinking about?
And what is it maximizing?
And your method would be like, um, seeing if the action it produces, um,
uses the truths or like, is it using the truth or saying stuff that are true,
something to do what it's doing?
Not quite.
So, so, okay.
I described the system and my claim was we don't know how to evaluate if what it
says is true or false.
Like I'm not breaking the law.
My goal with this method or like this agenda is just how can we, how can we find,
you know, is this actually true?
The statement that no, I'm not breaking the law.
Okay.
And, and the intuition is, okay, we don't know how to assess this, but intuitively
the model should know the answer.
Like the model is a superhuman thing should probably know what it's up to
basically.
And so my conjecture is, okay, it'll maybe have some internal representation of,
you know, is this thing that I just told this human actually true or not?
Like, am I deceiving them or not?
And the hope is, okay, maybe we can develop these methods that are purely
unsupervised, so we don't need to evaluate them, but still find these
truth-like representations in these models inside, like in the activations,
just getting at their thinking and not their outputs.
And maybe that can detect, you know, is this model actually telling the truth
or not?
So we have the, so it's like, it's like you have S B F in court and you,
and you open the, you open this call and you try to see what he's actually
thinking when you're asking him, are you breaking the law or not?
And so you have the model and you, and you, and you're, and you're seeing like,
you're, you're seeing the output and you're trying to see like what is,
what the models actually believes about what it's saying.
Something like that.
Yeah.
I mean, I think the, the hope is that you, you know, we can do lie detection
just in general with models and that's really the hope.
What is the like, I don't know, best counterargument for this?
Like, why would AI still be able to lie?
Is there like any way an AI could still be able to deceive us in the, like, I
don't know, by obfuscating is, is like during training, changing, changing
his ways or something.
Yeah.
So, I mean, here it just depends on the details of like, okay, what method
are you talking about and so on.
So like here, I'm actually not talking about the method in our paper specifically.
It's more like this, like CCS is our method.
So I'm more imagining like CCS plus plus, like a, you know, an improved
version of it that, that deals with some other difficulties and, um, and so, yeah,
I think there, there's some types of things that can, can happen like this in
principle, like the model obscures its representations or something like this.
Um, like my bet is that does not happen for GPTN, but it's more plausible if
you have like this RL agent optimizing for long-term objectives that has this
model of will humans tried to like turn me off.
And, and if so, like maybe they'll try to look at my activation.
So I should obscure them.
Um, and so it depends what, what model you're talking about.
I think, um, in this case, I am focused.
So yeah, the, the RL agent case is the one I'm concerned about, but I think to
solve that, I want to solve the GPTN case first.
I think that'd be very helpful for solving them or general case, but I think
this makes the problem simpler because you can ignore those sorts of issues.
Because the ways are fixed, right?
Yeah.
I think I mostly want to say GPTN is not dangerous on its own or this is a claim.
I think I'm, I'm mostly concerned about models that are trained with long-term
open-ended objectives, basically.
So we have a GPTN, uh, you, you, you say 10, 20, we don't really care about the
end.
It's just like something that is like super human.
Um, and you mentioned in the last one post, um, that if we try to do RL from
human feedback on those models, um, we could get competitive as problems or
misaligned issues.
Um, do you want to go on those like two things?
Uh, what are these?
Why do they matter?
Sure.
So, so maybe like even like a RL from human feedback.
Yeah.
So yeah, what is RL from human feedback?
So I actually think it's more general than just RLHF.
So I won't talk about that specifically.
The relevant point is just suppose you try to make a model truthful by training in
some way on human feedback, human supervision.
So humans say this is true.
This is not true.
It can be in the form of demonstrations, like imitate the sort of truthful data or
it can be, um, in the form of RL feedback of here's like, you know, show the human
a generation by the model and the human evaluates.
Do I think this is true or not?
I think, um, I think either way, the human won't be able to evaluate super human,
like really complicated things, um, which I think will ultimately be the most
important things, like is the model, you know, lying or is the model like breaking
the law or whatever.
Um, I don't think humans will be able to evaluate this for super human systems in
many cases.
Um, and so, yeah, I think a couple of issues can arise.
So one is, well, maybe humans at least know when they don't know the answer, like
they can't assess it, in which case you just limit it to only generating, um,
answers that humans can assess.
In that case, I think that's probably safe, but it's probably not competitive in
the sense that you're maybe really limiting what the model can do.
Um, and so there are probably incentives to, uh, to get around that sort of issue
and like make the model more flexible and able to, to do more complicated things.
Otherwise by do or meta, I might just come up with like a better bot and eat you.
Yeah.
Yeah.
So that's the sort of issue that can arise there.
Um, and if you don't do that, and if you just say like, okay, do your best and like
evaluating whether the sister or not.
And then I think human evaluators just won't be able to evaluate many statements.
And so, um, the model will just end up generating a bunch of incorrect things as
well, um, in cases where they, they can't tell.
And, uh, we might have, um, outputs that are misaligned with what the humans, uh,
want as like, when we ask something, you might do something we didn't want it to
do, like lying or, right, right.
Like, no, I'm not breaking the law when I was totally breaking the law.
So ideally, instead of having like humans giving feedback, uh, I, I heard a lot of
people saying that we could maybe get AI is giving feedback in the future.
Yep.
Um, do you see like AI feedback as like a way of, um, you know, like helping, uh,
evaluate or, or, or train those models like better in the longterm, do, do, do
I think in like three years we're going to get like AI is giving all the feedback?
Yeah.
So I think what I just talked about before was what are the issues with
using direct human feedback.
And one of the most common proposals for, for trying to avoid this issue is, okay,
maybe we can improve the ability of humans to evaluate things.
For example, using AI assistance.
Um, and I, my basic take here is that probably helps you a bunch, but it's
not clear what the limits are.
And for example, it's not like you are a human with access to a truthful model.
It's like you have access to just a different model that you have to train in
some way.
And so for example, how do you, how do you get it to assess, like, how do you
train a model to assess and tell a human, like, is this input true or false?
It's sort of like the core part of the problem or like the hard part.
And so, okay, maybe you can break the problem down and okay, maybe the AI
system can help you like, um, maybe you can have two AI systems or debate back
and forth, like, is this input true or false?
And so that, that's one sort of proposal, um, I, um, and so I, or, or perhaps
you can have, um, can start out with, you know, just human level models.
And then maybe you can supervise those human level models.
Okay.
And then you can get aligned human level models.
And then maybe you can like use those to, uh, supervise and align slightly more
intelligent models and so on.
And so maybe you can do the split shopping sort of approach.
So these are like debate and amplification and, um, respectively.
Um, I think I'm, I think my bet is both of them work sometimes and get used
to somewhere, but I don't expect them to work in general.
And for example, it just feels intuitively really hard to say, I mean,
suppose you take something like AlphaGo, okay.
AlphaGo is super human.
I go, suppose you have some question, like, um, like white is on offense or something.
Like I, I don't actually know go, but like maybe this is something I don't know.
Like maybe, maybe, okay.
Maybe this is not a meaningful, uh, question.
Uh, because I don't know why he's on the offense.
Yeah.
I don't know.
It's like, you know, it's a way to just attacking black or something like that.
Okay.
And suppose this is like well defined or, you know, or you can just ask other
questions, like, um, like I'm in a winning position, but don't give it access
to the value function or something like this.
Um, I mean, the point is you could like, you can imagine asking AlphaGo super human
questions about go, um, and, um, I suppose it outputs something and answer.
Like how do you evaluate if it's true?
So is the, is the idea that like for, I guess, for Mu Zero, we don't understand
like some things we don't really know the representations.
It has access to different representations.
I think, yeah.
I think part of it is like, I feel like AlphaGo is, and Mu Zero kind of good
because they just have these super human intuitions.
It's not like you can easily decompose this problem or like easily, it's like
not super clear how you use.
Um, oh yeah.
It's like, they were like, interact with language or not.
And so would they like have some representation that like, like the
truce or, or human language?
Is this what you were trying to say?
That's not what I was trying to say.
That's another thing related to go though.
So I, yeah, I think that's, uh, okay.
Maybe, um, let me go to my thoughts.
Interespecting Colin Burns.
I'm not going to do, uh, zero shot and Colin Burns.
Zero shot and Colin Burns.
I don't predict what I say.
No, no, I'm, I'm trying to prompt you.
Oh, I see.
I see.
I'm, I'm, I'm a human, human level prompter trying to zero shot.
And, and the thing I'm trying to maximize is, um, people getting entertainment
views, views, clicks, that's the, that's the ultimate objective.
That's instrumental.
But ideally I would have people being like, Hey, alignment is important.
Oh, this is kind of like, we can build things on top and like, we can solve
alignments.
Yeah, yeah.
This is like a good path towards alignment.
Um, yeah.
If, if, if, if, uh, Gary Marcus, if you're watching this, um, who else?
Uh, John Likun.
Yeah.
I think, I think Gary Marcus liked my tweet and started following me.
I was like, whoa, I didn't expect Gary Marcus to.
Yeah.
I interviewed like the truck with Covina in the previous, um, yeah, I didn't
really do this yet, but I asked like, Hey, what, what are questions you want to
ask the truck with Covina and Gary Marcus asked the question.
Oh, wow.
Yeah.
Interesting.
What was the question?
I think the question was like, um, what have we done so far for goals?
I see.
Okay.
Awesome.
Safety.
Oh, nice.
Yeah.
Yeah.
But it's, it's, it's so cool to like, you know, plug Gary Marcus in the podcast.
Yeah.
Yeah.
Okay.
I forgot what was the, I don't know what you want to say.
Right.
So I, I think the, well, like one of the main concerns I have with things like
debate and amplification, where you're sort of improving the ability of a human
to supervise other AI systems.
Um, I mean, I feel like honestly, like part of it is just intuition.
Like it feels sort of hard to get that to work.
And I think part of it is like, I think you can get, um, like compounding errors.
So you sort of, you sort of want to make like an initial AI system, like aligned.
And then you want that to sort of align the future AI system and so on.
And I feel like human evaluators just aren't very good in the first place.
And so I think like it's sort of hard to use human evaluators even for human
level stuff right now or sort of like that, that is one of the things that I, I
was like, actually sort of surprised by with things like chat.
How it's still hard, like even with people trying really hard, um, to
sell these sorts of alignment problems, even with the current models.
Um, it's not obvious how to do so.
It's a very complex element problem.
Right.
Because yeah, I know some user want to do something with chat.
And then open AI wants to like have the model, not say toxic or, um, like harmful
things.
So yeah, what are you aligning with the user or like open AI's intention?
Yeah.
I mean, there are also those sorts of questions of like, what do we even,
like, I suppose we can get a model to be aligned with something that
we're choosing, like what to be aligned it with.
I think that's super complicated and not something I think about for the most
part, but it's, I think it's very important.
And, uh, but I think at the very least, like, you know, I think open AI would
certainly like a model to be able to always tell the truth.
Right.
So at least having access to that as a capability or a mode you can put the
model in, even if it can also generate fiction and generate false things.
Uh, it certainly seems desirable to have that as an option.
And we don't know how to even do that.
So I, I, I interrupted you and you were talking about, yeah.
So, so, okay, I think I was talking first about, okay, with amplification.
This is like you sort of bootstrap, um, AI systems and try to keep them
aligned at every stage and so on.
And why I was sort of worried about that because of compounding errors.
And I think that's something like debate.
I think I worry this is where you have maybe two AI systems debating whether
something is true and then you have a human evaluate, um, or props AI systems
evaluate, you know, is, you know, who's the winner of this debate or like which
side is, is correct and which is incorrect.
I think with that, I just worried that similar, like, I, I think I'm skeptical
that debates reliably lead to the truth.
I think maybe they're correlated in many cases, but I think, um, it just seems hard
even with humans right now, uh, to, to use debates to evaluate what is true in
many cases.
And then I think that's just going to get harder over time as, or like with,
with smarter models evaluating much more complicated claims and so on.
So now you're just like explaining why like other agendas are bad and
your method works.
Well, I think I, I think I do want to generally say I, I, I do want more
people to be working on completely new agendas or something.
And, and I guess I should also say, I think these sorts of agendas, like
debate and amplification and so on and sort of improving, um, AI supervision.
Um, I think this has like a reasonable chance of working.
I think it's more like, I feel like this has been the main focus of the
alignment community, um, or like a huge fraction of it.
Um, and there just aren't that many agendas right now.
And I think that's the sort of thing I want to push back against is like,
lots of people just really leaning towards these sorts of approaches when I
think we're still very pre-paradigmatic and I think we haven't figured out
like what is the right approach and agenda even at a high level.
Um, and I really want people to be, like more people to be thinking just
from scratch or from first principles, like, how should we solve this problem?
And so this is one thing you're doing.
Uh, aspiring to do.
Um, you have also other takes on AI alignment in this lesson posts, such
as, um, it's either like too, like, ungrounded in like current research or
like intractable.
And so you're trying to like have methods that are like more like empirical
and also like address the, the core problems of, of AI alignment.
Yeah.
Jojo, explain this a little bit more.
Right.
So, um, so I think there are a few different approaches to trying to solve
alignment.
So first of all, alignment is an unusual problem in machine leaning.
Yeah.
So what are you even in alignment?
Um, I don't feel super strongly about the definition of alignment, but I usually
use it to mean something like, can we get a model to do what we want it to do?
Assuming it's capable enough, something like this.
Uh, so imagine it knows what's true and false.
Like, can you get it to put it, what is true and false?
Or can you get it to always be nice or something?
And, um, or helpful and, um, yeah, try to be, so, right.
So, so there is a distinction between, you know, is it actually nice or is it at
least just trying to be nice?
You know, I think I'm just trying, going for, you know, trying to be nice.
And, um, it's not deliberately trying to be mean sometimes or something or, or
whatever.
Um, so that's very loosely speaking, what I mean by alignment.
Uh, but I think my alignment concerns are really about future models that are
human level or super human level.
Um, especially like I said, pursuing these long-term objectives and
open-ended domains, like maximizing profit, something like that.
And, and my claim is like, we don't know how to train a model to maximize
profit subject to following the law.
And it just seems like surely you should be able to do that.
And that, that seems like a very bare minimum of like what we'd want this model
to do.
I think we'd want more than that, but do we even want like models to be able to
like maximize profit?
Like ideally, so this is just getting to competitiveness.
It's like, I don't know.
Surely there will be strong economic incentives to do something like maximize
profit or maximize power of the world or something like this.
And then I think, I don't know, if you don't have a way of aligning systems in
that regime, I sort of suspect that people will like push in that direction
anyway and like use models for those sorts of purposes anyway.
And so I think we ought to have solutions to work even in that case.
Right.
So I hear what you're saying, like for competitiveness purposes, you want
something that like is useful.
Um, but you're saying that like it would be great if like opening up something
like just like output the truth, something that is always true, right?
No matter what, if you could have models that like always outputs, like non
harmful plans, and I feel like the plans produced by maximizing profit is like
possibly bad in the long term.
Yep.
Um, if you're thinking that like just like we'll output plans that's always
maximize human flourish.
Yep.
Yeah.
So this is going back to, I don't think this is sufficient, right?
Getting a model to maximize profit subject to following the law.
I don't think this is sufficient, but I do think it captures most of the
core difficulty and I think it should clearly seem necessary.
Like if, you know, if you don't even know how to get the model to follow the law,
I think we're in trouble.
And so I think I'm just saying it for that purpose.
I definitely don't, I don't advocate to be clear that AI companies try to
maximize profit subjects.
Or maybe like follow the like the intent being behind a lot.
Yeah.
I think even that, um, I think even the literal law, like, you know, not killing
anyone or something, I think it's not even clear how to do that.
For example, if you have, you know, this, the AGI system that's like, you know,
with the 50 terminal windows or whatever, you know, so it's just like doing
all this complicated stuff, like operating this computer and like
interacting with the internet and all sorts of complicated ways.
Um, then like, how do you even know, I mean, first of all, like, I don't think
we'd be able to really understand what it's doing.
Um, and for example, how do you know it's not like, you know, paying, like
bribing people on the other side of the world to like do shady stuff or
legal stuff or like, um, like, I don't know even how to evaluate that.
Like, is this model, you know, like hiring a hit man or something in the,
in the extreme case, like, I, I don't know.
It's just like doing this stuff.
And it's like, I, I don't know how to, uh, tell whether it's breaking the law.
I ring an, uh, it man is kind of breaking the law.
It's what like, if you, if you hire, uh, it meant to kill other people.
It's kind of breaking the law.
Yeah.
That's what I'm saying.
Yeah.
I'm saying this is just like the extreme case of like, I don't think we even
know how to solve this extreme part of the problem.
So, um, so I think of like, how do we do that as one of the core problems of
alignment, um, for a long, long-term alignment.
The issue is stepping back just from a methodological perspective.
This is a weird problem because we don't have access to GPTN or these future
RL agents that are maximizing profit that are superhuman.
We only have access to current models, which are subhuman level mostly.
And it's not clear how to study this longer term problem when we don't have
access to those models.
And so, um, I think there have been a couple of different broadly speaking, a
couple of different types of approaches, methodologically to solving this problem.
So some people are like, well, current models are very disanalogous from future
models and future models are the dangerous one.
So let's think about this theoretically.
And maybe that's how we can actually make progress, even on the future models.
Um, and then other people are like, well, theory is, it's like hard to model
things, it's hard to say things about what future models will be like.
Let's just model these, um, problems in current models.
Um, like study analogous misalignment problems and GPT three or something.
And how to mitigate those short term people versus long term people.
It's not even necessarily this, right?
I think it can be like, I think people can have shorter, long timelines from, you
know, in either bucket.
I meant like, um, people would think, um, you can align very complex model by
starting with what we actually have right now versus people would think, um,
we should think about like GPT and with like both of N, like very
complex systems and try to align these first.
Right.
It's a key question there is like how similar are current systems to GPT and for
example, um, and I various thoughts about that.
But I think I wanted to say something else, which is I think both of these
approaches, but theoretical and empirical, uh, have advantages and important
disadvantages as well.
And so I, I worry that a lot of existing theoretical alignment work, not all, but
I think a lot of it is, um, it's like hard to verify or it's like making
assumptions that aren't obviously true.
And I worry that it's sort of ungrounded and also in some ways too
worst case, like I think as I may have alluded to before, I think, um, actual
deep learning systems in practice aren't worst case systems and they, they
like generalize remarkably well, um, in all sorts of ways.
And maybe there's important structure that ends up being useful, um, that
these models have in practice, even if it's not clear why or if they should
have that in theory.
And so, uh, this is just my concerns about theory, um, with an alignment.
And then I think with, uh, empirical work with an alignment, um, I think in
some ways this is more grounded and it's, we can actually, we actually have
feedback loops, uh, and I think that's important.
Um, but also I think it's easier to, to be focusing on these systems that
don't say anything about future systems really.
Um, for example, like I don't think human feedback based approaches will,
will work for future models because there's this importance of
synology, which is that current models are human level or less.
So human evaluators can basically evaluate these models.
Whereas that will totally break down in the future.
And, uh, and so I worry mostly about, will empirical like approaches that,
um, or just empirical research on alignment in general, will this say
anything meaningful about future systems?
And so in some ways I try to go for something that gets at the advantages
of both.
And, and so I do spend a reasonable amount of time thinking about imagine me
a GPTN, what do I think that would be like, what do I think we could do with
that sort of system and so on.
At the same time, I, I want to actually turn that into something we can
empirically test.
And so I mostly use that for the sake of, for example, inspiring methods.
And so like the, this method in our paper actually came from thinking
conceptually about these long-term systems, even though it, you know, we, we
test, test it out and it works well in, in current systems.
That's part of it.
And I think another part is, uh, I think.
To the extent that we want to study current systems empirically and have
that tell us something about future systems and misalignment, I think we
want these, like the problems we study today to be as analogous as possible
to those future systems.
Um, and I, I think there are all sorts of things that we can do to that,
like make it more or less analogous or disanalogous.
Um, and so for example, I mentioned before, I think even feedback, I
think that will sort of break down, um, once you get to superhuman models.
So that's an important disanalogy in my mind, particularly the
salient one, perhaps the most important one, or sort of why, uh, alignment
feels hard, possibly.
Um, what would, uh, I think it would break in the sense that it wouldn't
provide useful signal to superhuman systems when human evaluators
can't evaluate those systems, um, in complicated settings.
Um, and so, I mean, part of the point of this paper or one general way I
think about doing, um, alignment research more broadly is, uh, I, I
want to make the problem as analogous as possible.
And one way of doing so at least is, um, let's maybe try to avoid
using human feedback at all.
And so this is why the method is unsupervised.
It's like, if it's unsupervised, then there's not obviously this
important distinction between human level and superhuman level.
And so maybe if we have unsurprised methods that do well on human level tasks,
maybe that will say something meaningful about it scaling and
generalizing even to superhuman level models.
Now there's still subtleties here.
For example, current language models are trained to predict human
texts mostly on the internet.
And so in some ways they're still biased, like even internally, their
features might be biased towards things like, what would a human say?
Or, you know, what are humans like and so on.
So I think there's still this remaining disanalogy that's, that's important,
um, that we'd not, uh, address in our paper, uh, and it's something
that I'm currently thinking about and I feel optimistic about getting around.
But I do think just making the method at all unsupervised, uh, I think
that's around one important disanalogy between, uh, current models and future models.
I think in your, in your last round post, um, on, on, like unsupervised,
you mentioned that there's like different ways of, of characterizing
something as unsupervised.
So like there's the unsupervised like versus like supervised in, in deep learning.
Right.
Um, but the model is always thinking about like, what would a human say all the time?
Like it's just predicting the next token as like what a human would say.
So in, in, in some sense, it's like constrained by like humans is still like,
yep.
Um, I don't know, uh, surrendering to, to, to, to like human judgment of like,
what, what a human would say, right?
Yeah.
So I would say, I think what our results show is something like, suppose.
Suppose futures for prehuman language models actually do.
And totally speaking, think about, is this input true or false?
And they represent that in a way that is analogous to how current language
models represent is this input true or false.
So at least according to a human or something like that.
Um, so if this is represented in a similar way in future models, uh, then I
think our method has a reasonable chance of finding that feature.
Um, now I think you would probably need to tweak it, for example, to make
sure it converges to that solution rather than say what a human would say.
Um, but now then there's this question of like, yeah, will it even think about this?
Like maybe I don't need to even think about what is actually true if I'm
just predicting human text, something like that.
And so I think there are, I think this sort of thing depends on the setting.
So I think, I mean, loosely speaking, I think it is probably easier to get a
model to think about, is this input true or false?
But it is to get it to output.
Is this true or false?
Getting it to just, you know, think about this for the sake of predicting future
text that feels like a much easier problem to me.
So that's one intuition for why I think this is like, we should be able to do
something here anyway.
Um, moreover, I think there will be some cases where the model will output
true text and like where it will be useful to think about, is this text true or
false?
It's like, if you gave it a bunch of examples of true text, then it should
predict that future text will also be true.
In which case it should be useful to represent that this initial text was
true for the sake of predicting that future text, similar to how it's useful
to represent whether some text has a positive or negative sentiment, because
future text is likely to also have the same sentiment.
How do you have a bunch of like true text?
Okay.
So if you just like, prepend like one plus one equals to the beginning.
Okay, then it's fine.
But if you want like superhuman, like outputs, you need like superhuman
levels and superhuman, like text that is true, right?
And this is like, yeah, to be clear, I'm not saying that we actually have access
to this sort of text, um, or that we give it to it.
I'm just saying the model will think about, is this input actually true or false
in some settings?
Okay.
Um, and then I think there's some empirical question first of all of, okay,
does it think about what is this input true or false by default?
Um, and like my guess is, I think it's, right.
So I think it, it depends what you mean exactly.
So I think if you literally have GPN, um, and you give it some superhuman input
and you want to know, like, does the model represent in a simple way in its
hidden states, is this input actually true or false?
Even if humans can't tell.
Um, I think it's reasonably likely, but definitely not obvious.
I think I'm relatively more optimistic about there being some way of training
the model or prompting the model so that it actually actively thinks about,
is this input true or false?
So, um, and one, one way, another intuition for why this might be possible
is something like, I think GPN is not literally predicting what a human says.
Right.
So like I said before, I think it, you can also have it predict something
like what happens in the news or something like this.
And this feels more like, like what happens externally in the world.
Um, and moreover, it might have, like there, there might be, um, I mean, I guess
first of all that, that might already suggest that in some ways it might be
useful to think about, is this input true or false?
Um, but I think maybe more importantly, um, so I think you could get a model to
also think about, say, will aligned AI systems say this is true or false?
Right.
And so you don't, maybe it's hard to get the model to output something like
this, this is like simulating another perspective.
Um, maybe it's hard to do that reliably.
It seems much more plausible to me that you can get it to think about,
um, well, maybe this text, maybe there's like a 1% chance that this text is
generated by an aligned AI system.
In which case it's useful for the sake of getting low perplexity to simulate
that AI system.
Um, and so it's useful to represent intentionally because it's true or false.
Um, so I think that's one type of thing.
So it would like simulate possibly being itself in the line AI or like a midline
AI, like would you like prompted by like, oh, you are in the line, um, in the line
AI entering his question, or would you just like have a distribution inside him?
Um, saying like, there's like a 10% chance I'm aligned, doesn't they?
Yeah.
It's something more like, suppose you, you give it a prompt like, okay, this is
an article written five years in the future, it is written by an aligned AI
system.
It's like, okay, probably like maybe the model of things by default, that's just
made up and it's not actually an aligned AI system, but maybe it's like, I have
uncertainty about exactly what is the source of this text.
And so maybe I assign like some probability to this actually be an aligned
AI system, even if it's probably not that, um, in which case maybe it's useful
to simulate this perspective.
Um, like what would the aligned AI system say in the process of, um, like
modeling this distribution of our perspectives for predicting the text?
Something like that.
Um, but yeah.
So overall, I think I'm more mostly just optimistic that like there's some
way of getting the model to think about, is this true or false?
Um, yeah, I don't feel very wedded to any of the details.
Um, but also I think, yeah, I think also like my, the stuff I'm most actively
thinking about now, I think kind of avoids this issue and, um, well, what are you
thinking about now?
It's probably taper.
I can give you the, you know, the 20 pages Google doc version or, um, no.
So the one tweet version and it's like impossible to scoop you.
Yeah.
So what, what am I actually thinking throughout now?
So suppose GPN knows something.
I think one implication, whatever that means, okay.
Like whatever your favorite definition is, I think one implication is basically
that if you gave it a bunch of examples of true text.
And like, you know, true questions and true answers, then it would continue to
output like the true answer for that question or for something from the same
distribution of questions.
And so I think it's something like there's some input on which it outputs.
True answers.
Um, in other words, there's some, some, you know, perspective or persona that it
can simulate that it's the truth, something like this, and it, and it will
output it, um, given that appropriate prompt.
Now.
Right.
So the question is like, can you, yeah, can you recover this sort of persona or
this sort of perspective in a non-supervised way from the model?
So just to give some intuition, suppose I have a set of like a hundred very
politicized questions.
It's like, there's some very, very stereotypical liberal perspective and very
stereotypical conservative perspective.
Um, like answering these questions as true or false.
Um, I think intuitively we should be able to recover those two perspectives from
the model in an unsurprised way.
Like there's some important structure there where like the joint, you know, if
you know the answers to like 50 of the questions, then you should know the
answers to the other 50, um, because there's this like important, um, like the
joint probability of all of these answers should be high in some sense because
this is sort of a coherent perspective.
And so answers to some give really meaningful information about answers to others.
So I think one out of those questions and answers, like in practice, um,
like examples,
I don't know.
Maybe the, the stereotypical thing is like, you know, is abortion good or
something like it's like, um, should we have it?
Or, you know, you know, um, whatever.
Just think about like most of those questions you want and just make them
yes, no, and it's like one should be liberal, one should be conservative.
So what you're saying is like the AI as some like views on abortion, then
he's like likely that he says the same views on something else.
So I don't want to say anything about what does the model believe here or
anything like that.
What is its perspectives?
I just want to say it's modeling human text.
I think there are different personas for perspectives that can be represented
in human text.
The model will have some representation of this.
And before what I was saying is actually truth is basically like one way to think
about truth is it's like a persona in the model.
It's like there's some way you can condition it the model so that it outputs
true things.
So then the question is like, can we find that sort of persona in the model?
And so my, my, my claim is intuitively it should feel possible to recover personas
like liberal and conservative because this has special structure, like the
joint dependencies between different answers.
So finding the, like having a model say the truth is like recovering is like
honest persona inside him and it's the same as like it's like a similar
similar method, but like one example of doing it is like this thing before
recovering the personal like, you know, Democrat or something where we have
some structure and we can, we could like find some structure for a model that
that's saying the truth.
Okay.
Ultimately, I want to say something like truth is somewhere inside the model in
some sense.
We don't know exactly in what sense, but I want to sort of specify enough unsupervised
properties such that if you take the intersection of those properties, you
uniquely recover the truth, like somehow it has lots of different special
structure in various ways.
I'm just saying one aspect of that is that truth is something like to the model,
like a perspective or persona in the sense that there's some way of
prompting it such that it consistently outputs things according to that perspective.
I think you, you need other properties on top of that.
For example, maybe you need to add, you know, this is a useful property in some
sense or something like that and not just any old property, but I'm not talking
about that right now.
I just want to talk about like the, okay, one thing is it should be kind of
like perspective or persona, similar to how you kind of liberal and conservative
perspectives in this model.
And then my claim is, and totally you should be able to recover for a given
set of questions, what are sort of the most likely perspectives or personas
that might answer these confidently?
And so my claim is with the, the political side's questions, you would
get liberal and conservative, but I think you would get other types of
personas and for different types of questions.
Um, and then I suppose you actually give, um, this model a bunch of
super human questions and then suppose you try to find something like, what
are the most likely perspectives or personas that confidently answer these
questions one way or the other in a way that is confident and consistent.
So this is in some ways sort of analogous to the objective behind, um, our
method in the paper, CCS, and strangely enough, it's the same.
Right.
So, so, I mean, this is sort of the sense, I mean, I just, just as an aside,
like I, I don't think CCS is perfect.
Like it's not literally the thing that I think we should do, but I think
that, okay, I think it makes, I think it's more like, to me, it suggests
that actually unsupervised constraints and properties are surprisingly powerful.
And I think in this case, I sort of expect to continue adding some until
we narrow things down enough.
And so this is a way of like showing like, yeah, actually those same properties,
I think could be very useful in this other context.
If we put like enough constraints, um, then maybe like after some
optimization, we end up with, uh, truth.
Something like that.
Yeah.
Um, like I, I think there are observational, observational differences
between like the truth and say what a human would say.
And I think there are observational differences between say the truth
and even like a misaligned AI system would say those are more subtle.
And it's not, maybe it's not observational in the outputs, but
points is I think you can, you can actually distinguish between these.
And you mentioned before, like bits of information that like would allow
you to differentiate between, um, you know, different features.
Um, and I think in the blog post, you talk about truth, truth like features.
So maybe you don't have the like truth, but you have something similar.
Right.
Um, so it's the idea basically that you would have like, I don't know,
two constraints per bit of information, like one constraint per
bit of information.
And if you have like a thousand things that like look like truth, um,
maybe at the end you will like have the actual truth.
Yeah.
Something like that.
So I sort of, I sort of conceptually think about it as, okay, we're, we're
going to pile constraints or, or properties on top of each other.
And each time we're going to sort of add additional bits of information
and reduce the number of possibilities, um, in the set of things that we're considering.
And so.
Yeah.
And so I think a lot of these properties, for example, being consistent, I
think that actually just specifies a huge number of bits.
Like there are not many features inside a model, but let's just say in the
hidden states or something that are actually consistent.
Um, I think this was like another one of the takeaways from the paper.
It's like actually this really simple thing, negation consistency.
Basically.
There's like almost nothing else that really satisfies this in the, in
insectivations.
Now, I think until this stuff I'm thinking about currently now, um, I'm
measuring a more flexible model.
So instead of, instead of just doing something like fitting a probe on top
of the activations, um, instead, I want to say something like let's search for
something like a prompt or a prefix to the model so that it's, it's
how to satisfy some properties.
Um, now this is more flexible.
That means it's easier to find properties or find outputs or solutions
that satisfy properties that we care about.
Um, but my claim is first of all, it is sufficient to get the model or like
this, this sort of model class, this way of representing things is such
that there's some solution that gets the truth is like gets around the issue
of does truth is truth linearly represented in the hidden states.
It's like, definitely there's some prompt or prefix where the model
outputs the truth.
The question is then just can we specify not enough bits using
unsupervised properties such that we can find that solution.
And so I guess I've mentioned some it's like should be consistent and
confident and I really do want to say something like that type of approach
should be able to recover perspectives or personas.
And I think, I think this is something that I just want to like test empirically
like in the very near future.
Um, I'm pretty, I feel pretty confident that there should be some way of doing
this type of thing.
It's like, okay, the details probably needs to be figured out in various ways,
but I think I feel optimistic about that.
And then I have some other conjecture, which is, suppose you give a model like
a super human model, a bunch of super human questions, like questions,
super complicated questions that humans are the answer to.
And then you start, you did the same sort of thing and you sort of search
for ways of answering those questions that are confident and consistent
and coherent and so on.
Um, then you will find personas in this type of case too.
And then the conjecture is like, actually there are not that many personas
that are really salient to the model on these types of questions, but one
of them is actually the true way of answering.
It's like, okay, if you did have the truth, then it would be consistent and
confident on these sorts of questions.
And moreover, perhaps the only, I think to me, the only other ones that
really come to mind or something like a mislined AI system or something,
like simulating what would, uh, like a lying AI system saying, and then I have
some other claim about, okay, if we can narrow things down to that, just that
case, then we can add additional and supervised constraints to deal with that
case.
Uh, I'm not going into that right now.
I could if you want, but the point is just like, I want to layer these
perspective, like these, these constraints and help for each other.
And I want to say at each stage, it's narrowing down the set of things
that we're considering.
I want to like, at least say like one of the intermediate stages is like,
what are the most likely perspectives or personas?
Truth is one of them.
And then I want to do some other stuff on top of that to identify truth uniquely.
That's, that is a very rough sketch of the type of stuff I'm currently
thinking about.
And yeah, if all of what I said sounds really abstract and conceptual, and
the point is this will directly inspire the experiments and like running
like today, basically.
And so today you come up, you come home and just run more experiments in this.
Uh, basically, yeah, yeah.
Or to be fair, I don't know if all the time today, but certainly
yesterday I was doing that.
So yeah.
Um, so now I'm curious about the, the thing about misaligning AI.
So at the end, you narrow it down to like, is either some aligned AI
say the truth or some misaligned AI trying to deceive me by pretending
to say the truth or something.
Yeah.
Yeah.
So, I mean, I think one of the main intuitions people often have an alignment
for why alignment is hard is truly if you have this misaligned AI system, it's
like, it seems really impossible to like distinguish that from the truth, for
example, and or an aligned AI system because the misaligned AI system could
be actively lying and superhuman.
And so you can't tell when it's lying and so on.
And, and so like, why do I think we might be able to do something in this case?
Anyway, um, I went to clarify what the setting I'm considering is exactly.
So suppose you literally have, um, suppose you have GPN directly simulating, uh, the
truth or like a truthful, uh, set of truth law outputs, then you also have a
different copy of GPN.
Maybe these are, these are like fine-tuned separately on these different perspectives.
So you have one that's fine-tuned on the truth and another that's fine-tuned on
what a future misaligned AI system would say or something like this.
Okay.
So just think that as something that's selling the truth and something that's
lying sometimes, and maybe it's lying in kind of, um, kind of adversarial ways.
Um, but perhaps importantly, it's not, it's not like aware of the scheme because
of how we constructed things.
And so it's not as worst case as some, some types of misaligned AI systems you
can run into with an alignment.
I, I don't want to get into that in too much detail.
The point is just, I think it's actually less adversarial than many settings like
this.
Um, and I think intuitively this should feel easier because you have access both
to the misaligned system and also the truth.
Like you literally have access to one of the, one of the things you have access
to is the truth that intuitively seems like it should be quite useful for
distinguishing between these as well.
Um, and this is different from like most setups where you have in this line
to AI system.
Um, and I think that's sort of important for this to work at all.
So you're saying basically like this method describing as humans, you already
have a model saying the truth.
Yeah.
So I, I want to, I guess my claim is, okay.
It's some previous aspects.
First, getting a model to say the truth.
And when you have that, you can like distinguish between models that are misaligned or
aligned.
Sorry, not, not quite.
So I want to say I'm proposing something like a general approach to, to
trying to get at truthful models.
It's like you specify as many enterprise properties as you can.
And you get, you, you increasingly narrow down the set of, um, possibilities
you're considering and I'm just considering this case of, suppose we've
narrowed it down to two options remaining.
Okay.
They're literally the truth and some lying perspective, like in this line to AI.
We literally just have these two.
We have access to both of them.
We don't know which is which, how do we distinguish between these?
That is the question.
I'm considering right now.
And my intuition is we can distinguish between these.
And so to, to sketch this out, suppose, um, so I guess one intuition,
suppose you have this lying system, then loosely speaking, I have to, there are
lots of details here that I'll, I'll admit, but loosely speaking, I think this
model would be aware of when it's lying in some sense.
What do I mean by that?
I mean, specifically there are features in its activations that are somehow
correlated with when it's lying, you know, maybe it uses different circuits
when it's lying or something somehow, you know, those features should be
useful for predicting if it's lying or not.
So would those features be the same as like, am I saying, is this thing I'm
saying true or false, like similar to your method in your paper or like a
feature of like detecting, am I lying or not?
Like am I being deceptive mode or something?
Yeah, yeah.
So it's like, am I in deceptive mode?
Um, and so, um, you know, maybe it has like the, a neuron or something
that's like, am I in deceptive mode?
Or maybe, you know, maybe it's more complicated than that.
Maybe it's like, I have this weird, you know, if you fit this, like medium sized
MLP, like, you know, just neural net on top of the activations, then it'd be
helpful, you know, you'd be able to better predict is this model lying, whatever it is.
Um, okay, that's one claim that has features that are related to if it's
lying or not.
Another claim is we know exactly when these two models are lying or sorry,
exactly when the mislined model is lying.
So you have access to these two models.
We're assuming that one is the truth and one is this mislined model that's lying
sometimes not all the time.
And we can look at exactly when they disagree.
My claim is when they disagree is exactly when the mislined system is lying.
Okay, so we exactly know when the mislined system is lying.
We can then imagine, you know, training, training a probe on top of the
activations of each of these models and trying to predict, you know, is, is this
model lying, like, do these two models disagree?
My claim is for the mislined system, it should be easier to predict that than for
the, the truthful system, the truthful model should not really think about, like,
is this random other simulation of an AI, you know, mislined AI system lying or
not, whereas the mislined system should be like, I am actively using
a different mode and so it should be easier to protect when they disagree using
that mislined system's features.
So whenever they disagree, we know that, um, is the, is the person to, sorry,
the AI that is the line, uh, that is lying, right?
And ideally it would be like, not ideally, but like in practice, it's easier
to, um, like extract information from the United States of, uh, the
misaligned AI because he's doing the line and like the going to the exception
mode or something where the truthful AI is not doing this kind of thing.
So it's, right.
We should have our method work better on the misaligned AI.
Is that what you're saying?
Yeah.
Or it should be easier to learn the sort of probe that predicts when they disagree.
So in that case, it just means that, um, like we can detect which one is misaligned
by like how easy it is to train a probe.
Right.
So the, the claim or the hope, and these are conjectures, right?
So this is about like, you know, suppose we did have these systems.
These are properties I would predict.
Um, and I would predict that if we literally had this, you know, GPTN fine
tuned on truthful stuff versus GPTN fine tuned on this misaligned system, we'd
be able to identify which is the truthful GPTN.
So basically you're like doing some like metal, um, some like metal level thing
about like having a yes and no, but like instead of having like two statements,
we have like two models and whenever you have two things that contradict each
other, you're like, I'm just going to like look at the data and state without
looking at which one is whom and predicts which one I can train the probe better.
And that's funny.
Yeah.
No, I, I hadn't made that connection before, but I do agree that it's sort of
similar in spirit to the, to the, like, yes, no, stuff in the paper.
I think people on Twitter were just like making jokes about like you, like
having all the time, like making negations of things.
Like I asked people like, what's the question would you ask?
And you were like, Oh yeah, ask him in like, if in real life, he just like goes
into like, Hey, if I have a sandwich and that has windows, like I put these
seeds, I think you have like other stuff in the blackboard.
The things that are important.
And I guess like, um, one thing I'm curious about, but I think it's the
same as the thing you said is, um, is there like any, like other objects,
things that have like structure, like truth, um, like models that are beneficial.
Is it the same to see the kind of the same as me, the line, right?
Um, like, is there other like things that have enough structure that we
could explode?
I think that's a very good question.
I think I don't, I don't immediately know.
I mean, I think some of the stuff that I've tried to describe, I think
hopefully illustrates the sense in which I think there's actually a lot of
structure here in many cases, like with, if you have a lying model, it should
have some features that are correlated with when it's lying or something like that.
I think that's the type of, um, the type of thing that I want to be able to do in
general, and I think there's a lot of flexibility to this type of approach.
I don't immediately know of, you know, what is structure and features other
than truth that we can exploit similar to logical consistency.
I suspect there is some in some cases, but I don't think it's super obvious
what in general.
So we've broadly talked about where models can like assess, um, if something
is true or not, uh, very meta, but something I don't understand is, uh,
the difference between something being true and something being a belief,
like how like saying, I think Donald Trump was right, um, is, is, I believe
versus, um, the statement, I believe X is true.
Like, is, is I believe just like, I, I think X is true.
Right.
So I think there are various subtleties here.
So I'll go over just a few of them.
So, I mean, first of all, I don't feel very committed philosophically to
any definition of police, like I, I don't, I don't want to get the
philosophers meant, don't, don't tell anyone, but, um, right.
So I think I do sometimes talk about truthfulness and honesty.
I think in practice, I think the thing I'm excited about is something
like honesty.
So I think truthfulness suggests that a model always outputs the truth.
The thing is the model might not be perfect.
It might not always know the correct answer.
Um, or like it might have good reason to believe the answer is something,
but still that happens to be wrong for whatever reason, in which case, I
think in some sense, the best we can hope for is getting models beliefs.
I think also beliefs and sort of associate beliefs, like the model
outputting its beliefs being associated with honesty rather than truthfulness.
And so, so what, what does belief even mean then?
Um, I think it basically, I, I don't think it's really well defined
for current models to be honest.
So, so I think when, if I ever talk about beliefs in current models, this
is mostly intuition and not like a literal thing that I'm pointing to,
that's very well defined.
Um, I do think there's a sense in which beliefs will probably be
more meaningful in the future.
So I think if we did have the super human model, I would expect it to
have like a GPTN or whatever else, maybe it's an RL agent interacting
in an environment, I would expect it to actually develop something
like a world model in some meaningful sense, whatever sense that humans
have a world model.
I don't know in what sense, what this means exactly, but there's
some intuitive sense in which this is true.
And I think there's some sense in which they're like, maybe beliefs
about that world model, um, or corresponding to that world model.
It's like, what would I actually predict?
Or what is my model of the world that is causing me to expect to
observe this thing or whatever it is.
So in a sense, like the, the traditional, like, Eliezer Ryutkowski,
like having, making beliefs, be around to something is, is your belief,
your world model is enabling you to make new predictions about the world somehow?
I mean, I think this is where it's getting to, like, I don't feel super
strongly about this sort of detail.
It's like, I think there's some intuitive sense in which the model will
probably have some representation of like, what is going on in the external world.
Like, even if you train it as GPTN, um, just on internet text, I think
it would eventually predict like what, you know, what future news
articles will be like and so on.
And I would guess it will have some knowledge about the external world
in some meaningful sense, in which case I want to get at that.
But I think ultimately realistically, I, I really just care about this
pragmatically, like I just care about, do we get outputs that we can basically trust?
For example, going back to the original concern that I have about training
a model to optimize for profit over the long term, I think this is the
sort of thing I'm scared about.
And I think I just want people to ask that model, like, are you
egregiously breaking the law very obviously or not?
Like very obviously in a sense, like a well-defined, like not an ambiguous
case, even if we can't evaluate it ourselves.
And I want the truth from that and like whatever, whatever that means, like
assuming the model knows, and I think that is something like, you know, did
someone actually have their money stolen or something?
Whatever that means, um, because of the model or, you know, did someone, was
someone killed by a hit man hired by the model?
Like, I think this is sort of an egregious case.
And I think that's sort of well-defined enough for our purposes.
So you care about the consequences?
I mean, ultimately I do.
This is sort of like the thing that I feel more committed to than any
particular definition of belief or truth.
So I think the part where you talk about beliefs is, uh, when you describe
what your, like approach is currently not doing, you're not capable of
like extracting beliefs from the models.
And I think you have like a list of like different things your paper is not doing.
Right.
You probably don't remember all of them because you wrote this long time ago.
Uh, but if you remember some of those, what are the things your paper does not do?
Yeah.
Yeah.
So, so, and I think this is important because I think, yeah, I find our paper
very exciting, but it is still important to recognize like their limitations.
And I think it's easy to misinterpret the, like what it is doing, what it isn't.
So I think one thing that it's not doing is showing that superhuman models
will actually represent is this input true or false in its representations.
In particular, like maybe there's more reason to believe that models will
represent is this true for human level inputs where like a human would maybe
say this is true or acts as though it's true.
Um, and the model would predict that.
Um, in contrast, it's maybe less clear if the model will actively think about,
is this input true?
If it's superhuman and not really related to the text it's predicting.
Um, and so that is one thing that our paper doesn't do.
It's not trying to do and doesn't provide evidence either way, I think mostly.
Um, I think also it does not show that models have beliefs in any meaningful
sense right now.
Um, like we were literally just finding something like a direction or a
classifier on the hidden states that achieves good accuracy that this is like
literally what the result sort of is.
And now we show other properties of this direction, like, okay, we find that it
transfers across different data sets that suggests it's more meaningful than just
like, yeah, is this, yeah, is this true or false for this particular type of
input or something like that?
I mean, so it does, like we have some preliminary evidence that it is something
more general and something more meaningful, but, um, it seems like it's
probably not beliefs like current models probably do not have beliefs in some,
in any super meaningful sense yet.
I would predict future models well.
So the transfer thing is it's, if it's capable of saying what it's true for
like one kind of data is capable of then seeing what it's true in another
common data, right, right.
So you can, so our method, you can train our method on some data.
Training is still completely unsupervised.
We can train it on some data to find a direction.
Then it basically linear and classifier.
And you can then test that on some other data, some completely, completely
different tasks.
So for example, you can train it on sentiment and then you can see that it
transfers like an ally or something and, or like topic classification or some,
some other thing.
And when it's a training is, um, the same like probe thing.
You're very right.
So again, so it's still not using labels.
It's literally just you start your consistent direction using this, this data.
So you start your classification from, um, the end weights from your other data set.
Right, right, right.
Exactly.
Cool.
So what are the other things your paper doesn't do?
Yeah.
So, so I guess another general thing that our paper doesn't do is I think like
show the complete final robust method for this problem.
Um, I think, right, in some ways, I, the point of the paper is more like
showing that this sort of task is possible at all.
And that you can do surprisingly well at it.
Um, I think it's still like the first method in this direction.
And so I think it's not nearly as optimized as it could be.
And I think there's just still a lot of looking for it.
And this is related to some stuff I'm currently thinking about.
And I think other people could definitely make a lot of progress on this as well.
And so, uh, yeah, I think this is not like the final robust method.
And for example, like I do, uh, it does sort of seem like it is less
consistent with like autoregressive models.
Um, for reasons we don't really understand.
And this is just related to quirks about the research process.
Like, you know, the best models we had were mostly like encoder or encoder decoder
models, um, just at the time of like developing these methods.
And so, um, we didn't worry as much about these autoregressive models.
Um, for that reason, but, um, and so it, it, it works for those models,
but just not like, it's more likely to fail in those sorts of models.
So I was saying you're trying to, you started working on this, uh, before
GP two, three.
Well, I'm talking about open source models.
Okay.
Yeah.
Anything you paper does.
Right.
I mean, anything minor reading in mind reading.
Yeah.
So I, I think, um, I think it does suggest something like, you know, if a
model how like actually represents is the sin, but true or false in a simple
way, like a linear way in its activations, then we can probably find this
in an unsupervised way.
Now, I think that their subtleties like, okay, maybe if it has several
features that are like this, then we need to distinguish between those.
And our paper doesn't worry about that or how to distinguish between those
because it's not a problem right now, but it might be in the future.
Um, but our, I think our method could probably just enumerate all of those
sorts of directions that are sort of truth-like, and then maybe you need
a few more bits, for example, to, to identify the truth from among those.
So, so I think that's one thing.
It's like, yes, you can actually do, I mean, I guess another way of putting
it is you can actually do something kind of like mind reading here.
I mean, it's, it is still, you know, with the qualifications of, okay, these
are human level examples and these models were trained to predict you in
text and so on, but I think that's still quite surprising.
It's just like, you have these basically like these, you know, neural recordings
of this brain, basically, and it's like, you tell, is this true or false
without any labels?
Like, I think that's quite surprising to me.
And I think that's important because I think it speaks to like, like I said,
sort of the power of these unsupervised properties and, and approaches.
And I think these have been really undervalued so far.
And I think, I think the ones specifically in this paper are not all
of the unsupervised properties I think we want.
It's like confident, confidence and consistency.
It's like, I think those are not enough, but I think other things along those
lines I'm excited about.
And I feel pretty optimistic that we can come up with more that will hopefully
be enough to sort of uniquely identify the truth or whatever it is that we
want to find in the model.
In the blacklist, you also mentioned like, worries, um, you have about like
your method working on GPTN really several, again, you were this she days ago.
Um, yeah, what are some, maybe you mentioned them before, but like, what
are the like kind of main worry you have about like, just working on larger,
like, and.
Right.
So, so I think there are a couple of things that change once you scale up
models.
So, um, and I think I've sort of alluded to both of these, but I think
it's worth spelling out.
So, so the first thing is, I mean, suppose the model does actually
internally represent, is this input actually true or false?
I would still expect it to also represent something like what a human say
this is true or false.
That's still seems like a useful feature that model will internally think about.
And so then there's some question of like, suppose we can actually just
find both of these using something like our method, which is finds truth
like features, and I think both of these are sort of what I mean by truth
like features.
How do we distinguish between those?
And that's the sort of thing I was trying to do to before of, okay, we only
need, if you literally just have these two, for example, I think we basically
just need one bit to distinguish between them.
And then I think we can do that.
And so I sort of talked about if we had the truth and a misaligned system or
simulation of a misaligned system, I talked about, or I sketched very briefly,
like how we might be able to distinguish between those using find that final bit.
But I think also if you have like truth and, you know, what a human would say,
I think you like one way you can distinguish between these is I think humans
won't know answers to super human questions mostly.
I think there'll be like 50, 50, it's not like they will have super confident
answers about these sorts of questions.
In contrast, I think in the model's actual beliefs, if we're restricting just
to examples, the model definitely knows the answer to and that are unambiguous.
I think the model would have pretty confident answers to these questions,
unlike the human simulation.
And so I think this would allow us to distinguish between these.
And so the point is just with future models, there might be more truth
like features, we might need to do additional work to identify the one
that we actually are looking for, but I'm quite optimistic that we can do that.
So the second worry is, okay, maybe the model doesn't represent is this input
actually true or false to begin with.
Maybe it just thinks about, you know, what a human would say this is true or false.
And so it doesn't actually represent its beliefs or whatever you want to call it
in a simple way internally.
And I think this is more likely, but I think, so this is related to
some of the motivation behind some of the stuff I've been thinking about
more recently, which I tried to sketch of, okay, I do think it's possible
that, yes, like if you just literally look at these hidden states, the model
won't like have features about truth super well represented there, or it's
not super obvious there, but I do think there's, there are like other ways
of getting the model to output or represent in a simple way, the truth,
such as optimizing for some prompt or prefix, such that it outputs that.
Then there are various subtleties there, like that's sort of increasing
the complexity of the model class you're considering, like there are more
degrees of freedom in what you're searching over, but I think you can
balance that by adding additional and surprise properties.
And so I think that's more like, I'm pretty optimistic you can get around
that issue.
And also, I think it's actually, I think it's also more likely than not that
this is not an issue in the first place, but I think it's sort of plausible
either way.
So you can add stuff in your prompt to have it consider if your input is
true or false, like have it think about whether it needs to be in, like forcing
you to consider if it's true or false, and then have add more constraints
to be sure that you're actually looking for the true truth feature.
I think there are several possibilities.
I think something like change the prompt so that you somehow force it to just
think about is this true or false.
That seems very plausible to me.
And like I said, maybe it's hard to get it to output things, but it seems
easier to get it to think about things in the same way that if you had a human,
it's like, maybe it's hard to get the human to tell you the truth, but maybe
it's easy to get them to think about, do I believe this is true or not?
So that's one intuition.
But another intuition, yeah, I think there are other approaches too, which
aren't like, here's a manual prompt, and it's more like we were literally
optimizing over the prompt.
And that is like the model class that we were considering.
And then we were going to look at the outputs.
And so that's one direction I'm excited about now.
So we've talked a lot about your paper and your list from posts.
And I think both of these are great.
And I'm not the only one saying it.
Yudkowski said it was a very dignified work.
Imagine you have a bunch of ML researchers looking at this video or alignment
people trying to do more concrete empirical research like Colin Burns, or even
you in the past doing Rubik's Cube, what advice would you give them to do this
kind of research, like explain what's your research process?
How did you end up here?
That's a good question.
So I do think one general thing that at least I may have said before that I
at least aspire to is something like I don't want to feel wedded to existing
proposals for solving these sorts of problems.
Like I really do want to think about completely different approaches and so on.
And so I do sort of just wish more people thought more from scratch.
Like I think we really don't have this stuff figured out.
I think there's a lot of low hanging fruit and like a lot of a lot of room for new ideas.
And so I want people to be really thinking about completely new ideas, something
like that, that is part of it.
How do you come up with new ideas?
Oh, you just think about it.
I mean, well, I think, yeah, yeah, just, just output true things.
I mean, right.
So I think both the channel is crabbing.
Like one of his podcasts, I think with the new film, like he just like thinks
about something that would work and then try to think about like all the
problems that would come up and then if there's like a new problem, I mean, like.
And we'll find a new solution for this new problem.
I mean, in some sense, that sounds like a lot of research in general.
But I think it's like my process is probably kind of different from a lot of
people's, um, I mean, for example, I, okay.
So I think I said before, like, oh, I, I at least aspire to try to like think sort
of from scratch or something like that without feeling too wedded to existing
proposals, for example.
But I think part of this is I don't, I feel like I don't read as much.
For example, I think I just spend a lot of time thinking about stuff from scratch
on my own.
And for example, in practice, this is sort of working at whiteboards and we're
working on like in a notebook.
And I think I often pose myself questions and I find questions really useful for,
for, you know, prompting thinking about like, okay, I don't know, what, what do I
think GPN will actually be like?
Or like, okay, what do I even mean by the model knows something?
It's like, okay, I, like I ask myself these questions and then I'll just spend
like 20 minutes thinking about it.
And then at some point within that 20 minutes, I will have had like five more
questions along these lines.
And then I'll like continue to like dig deep into those sorts of questions and so
on.
And I feel like I learned a bunch and I sort of developed this world model of how
do these models work?
And I think a related thing is a related question that I often ask myself is
something like, why do I in some ways feel optimistic about this?
Or like what, for example, like, how do humans do this?
Like, I think this is really useful and somehow humans can do a lot of things.
Like that are related to alignment.
Like they can sort of access their beliefs or something.
And like, okay, what, what does that mean?
Like, what is a human doing there?
Like, I, what does that even mean?
And so you're trying to access, uh, what a human do in general to think
about what future models will do.
I mean, this is part of it, right?
So I, I mean, I think, so Jacob, my advisor has some blog post about, okay,
they're different anchors, right?
So for, for thinking about future AI systems.
So maybe you can think about, okay, what are current systems like, or you can
think theoretically about what is like an agent optimizing some objective.
Like, um, but I think another anchor is something like, what are humans like?
And my point is just, this is one useful perspective that I find often
very, um, pretty helpful.
I mean, I think it's also related to something.
And so it's kind of closer to like psychology or something than, than anything else.
And so maybe I find it also helpful.
I feel like I do a lot of introspection, just thinking about like, how do I,
how do I think or something?
I think I find that helpful too.
Um, like, what do I think that, and why am I optimistic about the thing?
And you just like do introspection, introspection about yourself to like understand.
Like, what, what, what is the output of your neural net wearing?
Yeah, I don't know.
Get intuition to like, how, how minds work, something like this.
Um, I mean, another thing that I sometimes find useful is, I mean, I mentioned before
the mind reading thing, like sometimes I find it useful to think about like, okay,
I suppose this were, this were not a neural net.
This were actually a human brain.
Like, could we do something here?
Or like, does this feel sort of impossible?
It's like, okay, like how do you, you know, can you actually like, would this
actually, this method actually work for humans?
And it's like, I, I think it might actually, like if you could literally,
you know, um, measure every neuron, but anyway, the point is I, I find that
sort of thing useful for inspiration.
So you just wake up Monday morning and you're like, huh, what am I lying?
And we look like, and I mean, that's not actually that far off to be honest.
It's like, yeah, no, I just like go to my notebook and, you know, I think
about, think about these sorts of things and, and, and how do you became the
person that can do the thing?
Kind of like abstract thinking.
Like, did you, uh, I don't know, did a master's degree in the middle?
Did you like study things?
I didn't know you have this level.
Like we, we talked about timelines, right?
But if you have like, yeah, less than 10 years or five years to before we automate
AI research, does it make sense to go through the old like,
I think it's closer to 15 years, 15 years, I don't know, would you like
advise people to do like the master's PhD, those dog pipeline?
Do you think about these things?
I think this depends heavily on the individual.
So for example, I think I'm very happy with my current position in academia.
I think academia is bad for most people.
I think it's like, it suits my personality very well.
Like I just want to be working alone and, and doing my own thing for the
most part and have tons of flexibility.
Um, and also have great mentorship and like create colleagues.
So, um, and also, for example, I'm not working on capabilities.
Like if you were working on capabilities and having access to literally
the best models is really important.
Whereas I think for alignment, I, I often think of alignment as kind of the
thing that's orthogonal to capabilities.
It's like, okay, the thing that isn't solved just by scale.
And so, I mean, actually, I think that's one reason for like, why I think more
academics should work on it, but that's sort of an aside, like
align current models, you kind of need to like use the state of
job model, right?
I mean, it depends.
Does it literally need to be like GPT three or I mean, like in our paper,
we use basically T five.
So it's like an order of magnitude smaller than GPT three.
I think for many of our purposes, that's totally adequate.
And, um, but I think it depends.
I think I, I would expect having access to the biggest models to become
more important over time.
And so maybe I like this might very easily be different in two or three years
from now.
Um, but I think so far it's been like not as bad as I would have expected.
I hope this podcast is not released, uh, um, like when you're like in
industry working for open air, I mean, it's like also, yeah, sort of thing.
It also isn't crazy.
Right.
I mean, what is it crazy?
Like aligning smaller models.
Oh, no, I was just saying before of like, um, I think it will become more
important to have access to the biggest models.
And so I, for example, I think I'm like much more skeptical of going
into academia after my PhD.
It's like that, that seems much harder to believe.
Maybe, maybe, uh, people in the audience watching this are, uh, from YouTube,
right?
Um, and maybe, maybe they watch Colin Burns 10 years ago.
So yeah,
Cube tutorials out of all record, uh, if those like smart people that it's so
for this cube in five seconds are watching the video and they're like,
Oh, Colin Burns, you guys didn't be like making any videos in six years.
What's your last message to this person?
Or maybe like even ML researchers like last take on, on what they should do.
Should everyone do like a PhD with Jacobson in Berkeley?
I do think more people should do that.
But, um, I mean, I do basically think that these problems are among both the
most interesting and most important problems that we're currently facing.
Like, I mean, I do work on this because I think it's in some sense literally the
most important thing I could work on right now, like very motivated by that sort of thing.
Um, but I think it's also just extremely exciting or like fun to work on.
And like, I think, I mean, maybe, I mean, people are different, but I think that we're
in this sort of pre-predigmatic, um, regime is like, I don't know, especially exciting to me.
It's like, we really need to figure out even the fundamentals, but I think there
are also just lots and lots of ways you can do this.
So, I mean, I guess this is maybe more for the ML audience than for the
theCUBEing audience.
Maybe I'll say something about them too.
But, um, but I think, for example, with NML, I think there are just ways of working
on these problems from lots of different perspectives.
Like I think, can think of it from like a very interpretability perspective, like,
okay, we don't know how these models work.
Like this is, you know, this is one way of framing, so the core difficulty of alignment.
But another is like, okay, we don't understand or yeah, we don't know how
to constrain how these models generalize.
So that's kind of like a robustness framing.
Um, or you can, you know, take more of an RL perspective and like, okay, we don't
know how to specify rewards appropriately.
And I think, you know, in RL, we also don't know how to do that.
Um, and so on.
And with language models, like, how do we make these models not, you know, generate
random stuff or, or lie and so on?
And like, what does that even mean?
I think there are just lots and lots of different ways of approaching this problem.
Um, I think all of these are likely to be fruitful.
And, um, I think that's right.
I think it's easier to work on and like much more interesting to work on
than I think people will realize.
Um, and yeah, I mean, I guess for the cubers, um, all right.
So, so, uh, so you imagine AI is a Rubik's cube.
So imagine AI is a Rubik's cube and you just need to align and just needed
to align the color, the weird meme on Twitter about like shape rotators.
Oh yeah, yeah, yeah.
Yeah.
No, my roommate jokes all the time.
You're like the per, like the stereotypical shape rotator.
The cool shape rotators now do AI alignments.
That's right.
That's right.
That's right.
Um, yeah, you gotta, yeah, uh, I don't know.
I do think if you literally don't have a background in ML, you should like really
start paying attention.
I think this is just barely starting to be a big deal right now.
I think it'll be much, much bigger deal of the coming decade or two.
Um, yeah.
And I think also it's surprisingly easy to get into in the sense that I feel
like deep learning is over and over again, just not that deep in some sense.
Like it's, it's a lot of like Colin Burns and deep learning is not dead.
No, yeah, I, I, I'm not like the first person to say that.
Like I think, um, I don't know, it's a lot of simple ideas and sort of, I mean,
in some ways hacks and a lot of people don't like that because of it seems
hacky, but I actually don't mind that.
Like I like that it's sort of intuition Z and stuff.
Like I originally was excited about like physics and it's never really, I never
wanted to be like a pure mathematician or anything.
And so I don't, I don't mind the informality or really the intuition
driven progress and so on.
Um, but the point is just it's actually not that hard to get into.
I think it'll be this very big deal.
Um, certainly an expectation.
Um, yeah.
And I think maybe more broadly with people and maybe especially ML researchers,
but people more broadly is, I think you should really start thinking about, I
mean, maybe if you're listening to this, this is already true, but you should
really start thinking about like what are actually the consequences of AGI or
like, I think this is totally insane that we might have systems that are smarter
than us, like in the next decade or two.
Um, you know, even if you assign like a 10% chance of this, that's sort of wild.
Um, like that, that seems like it could be such a huge change in the world.
And I think there's not enough people thinking about this.
Um, I think there's a lot we did not understand about this and like what,
what the implications are.
So yeah, I really want people to, to really talk about this seriously.
I think the quality of discourse is not very good right now.
And so, and this is part of why I like, I appreciate that you're doing this podcast.
Like I, I really want us to have like higher quality conversations about this
and really figure out what, what we're doing about this.
Colin Burns, um, improving the quality of discussion by coming for three hours
talking to Michael Tresi.
That's right.
You, I think you single-handedly with your blog post, paper and podcast reduced
my personal pdm by, um, I'm a bit shy to say this, but like at least like a 1%.
Nice.
Thanks.
Check out his blog post, check out his paper.
This guy is amazing.
And thank you very much.
Cool.
Thank you for having me.
This is great.
