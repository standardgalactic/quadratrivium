start	end	text
0	9600	So, this is one of the most interesting conversations I've had all year, and it's with former
9600	12920	poker pro and scientist Liv Berea.
12920	17720	So Liv has a YouTube channel where she makes films about game theory, complexity, physics,
17720	22360	a lot of other really interesting topics, and we covered a lot of ground in this conversation,
22360	26480	including what she learned about intuition and logic playing poker.
26480	29440	Everyone in the world tells you, oh no, trust your gut when it's got a really strong feeling,
29440	31280	it knows, it knows something that you don't.
31280	37400	And it's like, well, this is clearly not true, because it's often wrong, and it seems to
37400	39920	be largely, am I having a good day or a bad day?
39920	45080	If I'm having a good day, my intuitions tend to be more optimistic, if I'm having a bad
45080	49080	day, my intuition is pessimistic, so I can't trust it that much.
49080	53520	We also talked about game theory and why it's such a useful tool for sense making.
53520	60400	It's just the mathematics of these competitive situations, effectively, looking to see what
60400	66320	optimal strategies are, suboptimal strategies, and the phenomena that arise out of them.
66320	71760	And we delved into a concept popularized by Scott Alexander called Molok, which took the
71760	74320	conversation in a really interesting direction.
74320	81280	And what he really did for the first time was he related Molok to game theory, and talked
81280	85880	about how it seems to be this sort of the force, so again, if there's this force of
85880	90880	something that's driving the emergence and complexity, there seems to be this opposing
90880	97080	force, which is a force of destruction that sort of uses competition for ill.
97080	103760	Molok is the god of unhealthy competition, of negative sum gains, so competitive interactions
103760	108120	that make the world worse off for their existence, as opposed to being neutral or better.
108120	112920	Liv is also one of the speakers at our free state of sense making event on the 25th and
112920	117800	26th of September, so you can sign up for that down in the show notes, and I hope you
117800	118800	enjoy the film.
118800	121080	So Liv, welcome to Rebel Wisdom.
121080	122080	Thanks for coming.
122080	123080	Thanks for having me.
123080	128240	So you're a former pro poker player, and you also have an interest in game theory, which
128240	132360	is something that we've covered on the channel and something we're always wanting to learn
132360	133840	a little bit more about.
133840	139520	But the first question I wanted to ask you is, can we play our way out of all the sort
139520	146600	of game theory traps we find ourselves in, by which I mean the broken information landscape,
146600	151640	institutional corruption, culture wars, polarization, obviously the list goes on, or are we just
151640	152640	completely fucked?
152640	159720	I mean, that's the quadrillion dollar question, right?
159720	163640	I certainly don't know the answer to that.
163640	169320	I sincerely hope that there is a way out of it.
169320	172240	I don't see what it is, in all honesty.
172240	178160	It's something that consumes my thoughts on pretty much a daily basis.
178160	185160	But there's something in me, call it dumb optimism, call it the belief.
185160	192760	If there's these sort of dangerous forces, sort of semi-entropic forces trying to break
192760	199120	down complexity, the complexity that is civilization in one direction, it almost seems like there's
199120	207960	some kind of force pro-complexity trying to hold everything together and keep this weird
207960	211920	world that we're in going, keeping things interesting.
211920	215840	So yeah, I mean, I don't know what the answer is.
215840	222800	All I know is that we are in an unbelievably critical time of the past decade and certainly
222800	225480	the coming decade.
225480	229240	It's definitely, we're definitely in the most interesting time in human history.
229240	236040	Okay, sure, I might be biased, seeing as I'm here, and we like to be the kings of our
236040	237040	own stories.
237040	241760	But yeah, shit's getting real.
241760	243880	Which leads me to a question I wanted to ask you about.
243880	248840	A lot of the people we've had on the channel, like Daniel Spaktenberger or Jamie Weill or
248840	254320	Jordan Hall, who've talked about existential risk in particular, seem to be quite pessimistic.
254320	259600	And then there's other people who have a kind of, there's a meme called Dumer Optimism,
259600	264360	where there's a sense of, yes, we're screwed, but within that we have some kind of hope
264360	266600	and a kind of optimism in spite of that.
266600	271240	And then we have people like, there's lots of different sort of tribes, like extinction.
271240	275800	So deep adaptation, which is a big influence on, for example, extinction rebellion, which
275800	281200	is this sort of environmental version of, we're all screwed, it's already done.
281200	284680	We need to go live in cabins and we need to be, it's all, I think it has kind of a religious
284680	286200	quality to it almost.
286200	292480	Yeah, I'm curious about how optimistic or pessimistic you are about the current state
292480	293480	of affairs.
293480	297240	It depends what day you're asking me.
297240	305400	Right now I'm in a more pessimistic mood simply because I read this article by Kaifuli yesterday
305400	310680	talking about the next generation of warfare that we're entering into with these, you know,
310680	314600	with autonomous weapons and particularly AI-driven ones.
314600	320320	It's not inconceivable within a few years that for $1,000 you can, and a little bit
320320	326440	of know-how basically build an entirely anonymous drone that can take out whoever you want.
326440	329880	And it will not be able to be traced back to you.
329880	334520	And that's just the beginning.
334520	338880	And you read that and you're just like, I don't see how we will ever make it.
338880	344560	So right now I'm in a pessimistic mood, but after the weekend I did a little Burning Man
344560	348680	ceremony and just felt the magic a little bit and I was like, no, we're going to make
348680	349680	it.
349680	350760	And it's weird.
350760	356440	It's like a, I mean, you could almost say, is it like a sort of left brain, right brain
356440	357440	thing?
357440	360560	You know, logically I don't see a way out of it, but there's like some intuition in
360560	362200	me that feels like we're going to make it.
362200	369720	But then sometimes my intuitions are just like, no, like also, like I don't see a solution.
369720	374160	So I don't know.
374160	380840	And I guess perhaps the thing to do in the face of that, you know, this constant uncertainty
380840	389000	and this oscillation between the two extremes is to, you know, which state of mind would
389000	391200	I rather live in more?
391200	394720	Given that I don't know which one's the right one and I seem to be able to go into both,
394720	402160	I might as well invest in what I can in myself to control, you know, am I more likely to
402160	404720	wake up in an optimistic mood or a pessimistic mood?
404720	411200	And I can control that to an extent by, by, you know, taking care of my information diet.
411200	412200	What do I read?
412200	414200	How much time do I spend on Twitter?
414200	418320	You know, definitely the pessimism is correlated to the amount, you know, my, what my, my phone
418320	421160	tells me I've been looking at Twitter, you know, how many minutes per day?
421160	428160	You know, part of the thing I hate about, you know, you go on to Netflix or whatever
428160	436560	right now, it's just the amount of dystopian art out there to utopian art is like a hundred
436560	437560	to one.
437560	438560	Probably worse even.
438560	446600	There's just, there's just so little utopian programming, films, books.
446600	450000	And part of the reason I think is because it's just much easier to imagine a dystopia.
450000	456640	You know, one of the reasons why utopia doesn't exist is because it's incredibly hard to build.
456640	460920	So again, if you have a more optimistic society, then it gives people the space to dream up
460920	464360	and think of more positive things and it gets the ball rolling in the right direction.
464360	465880	Yeah, I like that.
465880	471600	There's a bunch of stuff in there I'd love to pick up on the choice to be optimistic.
471600	474680	You know, I think that is a very interesting thing in the times we live in.
474680	478880	And it doesn't surprise me that it's actually something I've written about before the fascination
478880	480480	we have with dystopia.
480480	484800	I think is in, I think there's lots of reasons for it, but I think in part I've described
485800	489960	the world we're living in sort of culturally is like a noir story, like a noir detective
489960	496160	story where you have the detective going through the often like encountering different institutions
496160	500720	like the church and okay, it turns out the priest is in on it and corrupt and like corrupt
500720	504720	to the core and then the judges are corrupt and the police are corrupt and everyone's
504720	512000	corrupt except for the detective usually and plays this kind of chivalric role of the kind
512040	519160	of broken but all like roughly altogether pure of soul somewhere in there right and
519160	522840	they're like, but there's light in the darkness, but they're sort of like hitting the whiskey
522840	526720	because like just to be the light in the darkness is so much and I think I think there's something
526720	533760	in that as a kind of as an image and there are and also just kind of, you know, it we're
533760	541880	recording this not long after the 20th anniversary of 9 11 and the the sheer shaking foundational
541880	549040	shaking impact of things like 9 11 and the financial crash and many other things that
549040	556240	have happened since then in terms of institutional trust. I think we we have this sense of being
556240	561680	in or moving towards a dystopia, but I'm very interested as well in something you were
561680	567880	just talking about in the the optimism within that right the the kind of and even so there
567880	572360	is this kind of there is this hope so I wanted to talk a little bit about game theory because
572360	575680	this kind of points towards a critique I have of game theory, but I thought it'd be cool
575680	582040	to to just kind of get a bit of a definition of what game theory is before in case anyone's
582040	588720	not really familiar with it. All game theory is is a branch of economics basically, which
588720	597520	typically deals with competitive systems and it looks at the strategic, you know, it describes
597560	602480	people or decision makers within it as agents typically just so that it's not human centric,
602480	611760	you know, AIs could be decision makers or whatever, mice, rats and so on. And it's just the mathematics
611760	617600	of these competitive situations effectively looking looking to see what optimal strategies
617600	623200	are suboptimal strategies and the phenomena that arise out of them. You have an interesting
623240	628440	position in this because you're a former pro poker player. And so, you know, one could argue
628440	632440	maybe this is wrong, correct me, but is that you were sort of having to apply game theory
632440	638440	under tremendous pressure with high stakes. But I'm curious about on the table, how much
638440	643960	of that is game theory and how much of it is, obviously, experience and then intuition,
643960	647480	which is another thing I'd love to talk about. But, you know, the critique of game theory
647480	653280	is often just like with with modern economics is that it relies on on actors being rational.
653280	657280	I know not all game theory does, but this idea of rational actors looking after their
657280	660960	self interest, you know, it turns out we're not really rational actors, but I still think
660960	664920	it has a lot of value. How applicable was it to to your poker career?
665200	672400	So I think we need to step back a bit in terms of like describing what's going on at the poker
672400	678840	table. So you will have, obviously, seven or eight other people around the table. And
678840	685720	your job is to basically sift through all the different forms of information that you're
685720	690640	receiving in order to figure out what the optimal decision is. And you've got multiple decision
690640	697440	points. And there's a broad range of information that you're receiving, like from, you know,
697440	704800	the the the amount that the person bets, the the cards that you have relating to the cards
704800	711760	on the board on the table, the the the demeanor of the of the person, you know, like that
711760	716320	you know about their past experience, how much they've played. But then now their face
716320	719760	is doing something funny that you never noticed before, or they're breathing heavier or something
719760	725280	like that, something they say. So there's this, there's a lot of qualitative and quantitative
725320	733080	information coming in. And where game theory comes in, really, right, in this, in the case
733080	737920	of poker, typically game theory applies to the strictly quantitative stuff. So, you know,
737920	744720	the there will be certain probabilities with which other cards will come out. And you you
744720	748560	know that the sort of odds that the that are being offered to you based upon your bet and so
748560	753440	on. And what that means based upon that information, effectively in a vacuum, the map, the
753920	758240	quantifiable information is that there will be these mathematically optimal solutions to these
758240	763040	different situations, which are, because there's so many possible situations, they're very hard
763040	768240	to calculate. It doesn't, it sounds like, oh, so you just need to remember the math, no. But
769360	772800	what game theory will do is basically suggest that there are certain strategies
773360	778800	that you will want to employ in certain situations based upon this quantified information. But then,
778800	783760	of course, there's this, this like nebula of other stuff coming in, like, well, yeah, but they
783760	787840	were breathing funny, which they weren't doing before. How do you quantify that and so on.
789040	794480	And in terms of how much of that nebula sort of applies to your overall decision making,
795040	802160	I hate to put like, try and put a percentage on it, but it's by and large, like, 90% of the
802160	807440	quantified stuff. And then the the the like, these sort of fuzzy things around the edge will count
807440	813840	for 10% of your decision. A lot of people, so when I first started playing poker back in 2005,
813840	817840	no one really understood game theory, no one understood the mechanics of how the game worked.
818400	822880	And the best players in the world were basically these like, they were typically older,
824000	830560	kind of like hustler types, who had just spent decades in casinos, just seeing the gamut of
830560	835600	human behavior and developing really strong intuitions about people. And so often they'd
835600	840160	make these these these strange plays that would turn out to be correct. And they wouldn't even
840160	845120	be able to explain to you why they did it. It was purely this sort of automatic unconscious
845120	850000	intuitive process going on, which and their intuitions were basically better than everyone
850000	857360	else's. But then when online poker appeared, and we started getting like, data analysis software
857360	863680	and this kind of stuff, you now all of a sudden we had data that pros could look at and and use
863760	868640	to analyze where they were going wrong, you know, where are the leaks in their game. And this kind
868640	873520	of, you know, it lifted the lid lifted the veil on what's going on in poker. And now people to
873520	878640	realize that, okay, there's mathematical solutions to this. And what it meant was that the game
878640	882640	basically went through a sort of scientific revolution, away from this pure artistry of just
882640	887920	feeling and having a vibe of someone to to going, Well, actually, look, this is the mathematical
887920	893360	optimal solution. I'll stick to this until I get such strong overwhelming evidence from
893760	900960	something else that I might override it. And in that, you know, in some ways, I hate this, but
901680	905840	in reality, like you cannot be a top professional these days without having that mathematical
905840	912400	foundation. You just no one's intuitions will be able to surpass knowing if you're playing as
912400	916320	someone who's playing a game theory optimal style, even though in some ways it's kind of robotic,
917040	923520	you just can't beat it by pure intuition alone. But of course, you know, if someone is now playing
923520	930240	not quite perfectly, now you can have these other like these these other like fuzzy skills,
930240	934080	you can bring in these fuzzy skills to figure out how the how to exploit their mistakes.
935600	941920	But yeah, the very long winded way of answering basically, it's it's by and by and large a very
941920	947520	mathematical game over 80 90%. That's really interesting. Fascinating. I really like that
947520	952480	that kind of laying out. I got a nice image there of the various sort of data points that are
952480	956640	happening and many of them at the same time, which is a bit like, I mean, that's, you know,
956640	959360	trying to make sense of something online is a little bit like that as well.
960960	965680	Yeah, and and I think it's interesting with like the particular so the game of poker, I think
966560	971680	can we can look at also life in general through that lens in a lot of ways like we
972560	979920	like I also have this kind of yearning for being purely intuitive, right? But I also am aware that
980240	984320	intuition really does lead us to stray a lot of the time, right, especially when we're trying to
984320	988640	make sense of complexity. Something John Varvakey, we've had on the channel a lot talks about in
988640	994000	cognitive science terms, you know, how do we how do we make sure the frame that we're looking at
994000	997840	everything through like the like he uses the example of like the glasses we're wearing,
997840	1001760	how do we know that that frame is accurate? And of course, it's never 100% accurate, but
1002320	1007360	practices and techniques that help us take the frame off and go, Oh, yeah, shit, that's a crack,
1008080	1012320	that's smudged that I'm seeing everything wonky, clean it a little bit, put it back on. And that
1012320	1018000	process of continuously regenerating our frames, I think is kind of more important than ever.
1019040	1025040	At the same time, there is something, well, I would use the term kind of like transcendent or
1025040	1031920	magic about the power of intuition. And so when we're looking at something like, well,
1031920	1035280	actually to rewind a bit, I mean, I see this come up in culture a lot, I'm sure, you know,
1035280	1040480	this has been a real trend of people trusting their own intuition over, say, the opinion of
1040480	1045520	experts. And that happens a lot now. And there is good reason why we don't necessarily trust
1046400	1050880	experts. You know, I mean, I read an article recently, which was, you know, making the point
1050880	1055280	about America had access to the absolute best experts in international relations,
1055280	1062000	counterinsurgency, etc. For 20 years, when the and yet the pull out of Afghanistan was a
1062000	1067600	complete catastrophe, right? Unlimited budget almost unlimited expert credentialed experts.
1067600	1071440	A few experiments have been done where an educated so there's a caveat to this,
1072160	1078160	really someone who's decently educated on a topic, trying to predict the future based on that
1078160	1085200	information, it gets about the scores is roughly in line with a credentialed expert. So and I'll
1085200	1089920	put in the show notes probably the article that was from actually read it this morning, I think.
1090560	1095520	It raises an interesting question though, right? So we need to be able to have some
1096480	1102000	way of not getting completely deluded by our feelings and our intuition, which it which it
1102000	1106000	tends to do. And every time I notice it's happened to myself, I'm like, Oh, that's a bit embarrassing.
1106000	1109600	I really shouldn't have been so certain. But it keeps happening, obviously, because we're human.
1109600	1115520	But and then at the same time, we need to know how to trust it in some way. I think so I'm curious
1115520	1119680	to hear your your journey with this. I know you were kind of at one point sort of anti intuition,
1119680	1124080	then you've gone kind of in a bit of a journey with it. Yeah. I mean, I started out, you know,
1124080	1132800	prior to poker, my decision making process was just a delightful mix of deep overconfidence
1133600	1140160	and emotionality. Just I was a highly emotional, you know, it was 20 year old girl who
1140480	1145920	was the bee's knees and everything and hadn't learned about life yet. And then poker came along
1145920	1150160	and didn't always you know, the great thing about poker is that it's like got these like quite tight
1150160	1157920	feedback loops where well, just tight enough so that you will eventually realize that you're
1157920	1164080	clearly not doing something right because you're losing money. But loose enough where you can still
1164080	1168160	be deluded for a while about your relative skill level, you know, because there is this luck factor.
1168720	1173760	And so you part of the hardest thing is figuring out when you know, when things are going wrong,
1173760	1178240	is it because you're making bad decisions? Or is it because luck is not on your side?
1178240	1182080	Because it can be both or a mixture of the two. So you know, you've got your your system one,
1182080	1187680	which is like the classic like intuition, this unconscious process, and then system two, as I
1187680	1193520	call it, well, Kahneman calls it, which is like this, the voice in your head, you know, what's 471
1193520	1201840	plus 86, that'll be your system two at work. And so then I was like, okay, so really what poker
1201840	1206880	is about is about this linear system two stuff, this this thinking things through like solving
1206880	1213840	a math problem, and which it by enlarges. And, and then and I would sometimes try and you know,
1213840	1220480	I'd be playing, and I would, you know, facing a big, you know, big, typical decision, someone's
1220480	1225520	put me all in on the river, I'm facing my tournament life, you know, heart's pumping in my ears. And
1226960	1231920	my gut will be saying, oh, a fold, fold, fold, they've got it this time. And the maths will be
1231920	1236640	saying, cool, no, you've got to call you've got, you know, your hand is statistically good enough
1236640	1244000	to do so. And I would, for a while, I would be like, well, my, my, no, my gut is so strong,
1244000	1248880	I'm going to just listen to that. And on, well, I don't know whether it's more often than not,
1248880	1253040	but on a sufficient number of occasions, my gut was completely wrong. And I was like, huh,
1253040	1256880	this is interesting, because everyone's been telling me, you know, but everyone in the world
1256880	1260240	tells you, oh, no, trust your gut, when it's got a really strong feeling, it knows, it knows
1260240	1264480	something that you don't. And it's like, well, this is clearly not true, because it's often wrong.
1265280	1269920	There's bias, and it seems to be largely by large, you know, am I having a good day or a bad day?
1269920	1275280	If I'm having a good day, my, my intuitions tend to be more optimistic. If I'm having a bad day,
1275280	1281840	my intuition is pessimistic. So I can't trust it that much. And, you know, that was sort of,
1281840	1287520	then I did my TED Talk a few years after that where, and I still stand by the contents of it.
1287520	1294640	Basically, I, I, I shit on intuition a little bit saying, you know, if you Google it, the internet
1294640	1300720	tells you that it's, it's this perfect source of knowledge, you should never second guess it,
1300720	1304640	always trust it, and so on. And I think that's actually very dangerous advice, because
1305600	1309280	there are some things your intuition is really good for, and some things it's terrible for.
1310800	1315680	And the main, without going into the details of what all those things are, the main thing is,
1315680	1319440	basically, if it's something that, if it's a decision you've made many, many, many times,
1320000	1323600	then your intuition is going to be pretty good. Which is why our intuitions are all from quite
1323600	1327360	good around like social things, you know, you meet someone and you get like a weird vibe off them,
1327440	1330960	and you don't quite know why. You know, you've met a lot of people, by the time you're in your
1330960	1334720	thirties, you've met a lot of people. Chances are your intuition is fairly good, and you should
1334720	1340320	listen to it. But if it's, you know, if you've started a new job, and you're solving difficult
1340320	1345280	problems, that you haven't quite figured out how to do the like logic for yet, relying on your
1345280	1350400	intuition isn't a good idea either, because it, ultimately, it needs some data and experience
1350400	1355840	to be based off. As far as we know, it's not this purely magical thing. Might be sometimes,
1355840	1362080	but we'll get to that. So yeah, so then I sort of went into this, like, deep skepticism of using
1362080	1366640	intuition. Certainly, you know, and certainly was, my message was, don't overreel on it,
1366640	1374240	be very careful, because it can be biased. But then, more recently, funnily enough, after a
1374240	1379440	conversation with Daniel Schmaxenberger, who always manages to, like, anyone who has ever met
1379440	1385680	him, he just like shakes people's brains up. One of the things he said to me, we hadn't
1385680	1389440	had much of a conversation, but he pulled me aside. He's like, you need to get in touch with
1389440	1394240	your feminine side a bit more. And I was like, what? What does that mean? And I was initially,
1394240	1399360	like, annoyed me. Because I had, historically, sort of associated femininity a little bit with
1399360	1405280	weakness. I'd always, you know, I've always kind of been attracted to typically male pursuits,
1405280	1411280	you know, physics, heavy metal, poker, you know, it's all very predictable. To the point where
1411280	1419200	I then started to associate feminine things with weakness, and so on. But it was just that I didn't
1419200	1428880	quite understand what femininity is. And what it kind of, you know, if masculinity is kind of like
1428880	1436960	this outward seeking, looking for some strict type ways of defining and viewing the world,
1437680	1445760	femininity is this more sort of passive, inward facing, reflective form of thinking,
1446880	1450880	a form of wisdom. It's just like another form of wisdom. And that's what I think he was getting
1450880	1455200	at, basically. He's like, play around with your feminine side and it will enable you to
1455920	1461120	start listening to your intuition a bit more and respecting it. And he was absolutely right.
1461120	1467760	And since I started doing that, I don't know, life just became a bit easier. And
1470400	1476080	just, I don't know, I just felt like more of a whole person. It's hard to describe. It sounds
1476080	1480880	a bit weird, but it was kind of cool. I really like that explanation of when intuition is useful
1480880	1487840	and when it isn't, right? It reminds me of, so there's something I've been quite influenced
1487840	1492160	by is the work of a guy called William Duggan at Columbia Business School. And he talks about
1492160	1498400	strategic intuition. And strategic intuition is basically our, so he uses it, he talks about
1498400	1503040	creativity and innovation through it, but has these four different stages of how we come to new
1503040	1508000	ideas and how we kind of get new insights. And the first one is examples from history. So you've
1508000	1512880	done, like you said, you have lots of experience in the thing. It might not even just be in that
1512880	1518160	thing, you have a whole database in your brain of, let you use the example of social interactions.
1518160	1522080	So that we have all these different kind of social interactions there and are unconscious in the
1522080	1526400	library of our memory. But then we also have loads of other things connected to that, loads of
1526400	1531440	different frames, we have lots of different, so let's say kind of horizontal connections. So it
1531440	1535680	might be, okay, I also know about this person's culture, or I also know about the cultural
1535680	1539520	context of where we are, or I'm having this interaction with someone at a roller skating rink.
1539520	1542720	Well, I know how people are at roller skates, so all these different things are going on.
1542720	1548880	And of all that information, then unconscious, so the second stage of this innovation process is
1548880	1552400	presence of mind. So you don't try and find the answer, which goes into that receptivity you're
1552400	1559280	kind of talking about. It's, okay, well, I'm just going to be awake and aware and allow my unconscious
1559280	1563840	to do its thing. And then the third stage is the eureka moment, which he calls a kudwi, which is a
1563840	1567760	strike of the eye in French, which I don't exactly know where they got that phrase from. But it's
1567760	1572000	a kind of, and we've all had that experience of something hits you like a ton of bricks,
1572000	1576160	and it often happens when you're not trying to solve the problem. You were taking a shower,
1576160	1581040	you're walking the dog, whatever it might be. And then the fourth stage is the resolution to carry
1581040	1585680	it through. And something I used to do with companies and was on a kind of campaign to get
1585680	1590480	rid of brainstorming, because brainstorming is really awful for that creative innovation process.
1590480	1595760	Because what brainstorming is really good for is going, here's many ideas, let's as a group hone
1595760	1602960	down the ideas to one, what is terrible for is here's a blank space, make ideas, because that's
1602960	1607040	not how the process works, according to Duggan. And he goes into the kind of neuroscience of it.
1607680	1613200	But one example he uses in that is of a fireman going into a burning building and having this
1613200	1616960	intense feeling of get out, get everyone out. And this is a real life example. And I think
1616960	1622160	there's quite a lot of them goes in doesn't know why that can't see anything in particular that's
1622240	1627760	telling him this is incredibly dangerous. But has seen so many fires that somewhere is intuition
1627760	1631200	is like something's off, get out, obviously get out the building, the whole thing collapses like
1631200	1634800	five seconds later. That's what you're talking about. It sounds like you have the intuition.
1634800	1639440	Well, well, I mean, usually when people report this kind of thing, they can't quite identify
1639440	1644320	what it was. But there will be some element in the in all of the complexity of the environment
1644960	1649280	that is connecting with a preexisting element, which is Oh, that time where that happened. And
1649280	1654160	this I also noticed that slight offness to the smell or whatever it might be that you can't
1654160	1660960	consciously notice. So in those moments that kind of screaming intuition. But if I walked into a
1660960	1666880	burning building, I'd probably have that screaming intuition the whole time. Get out, get out. So
1666880	1670960	yeah, I really quite like that example. So one of the aspects of game theory that we've talked
1670960	1675280	about on the channel before. But I don't understand that well. So I'm going to ask you about it is
1675280	1680160	multipolar traps seems very relevant to the times you live in life. So what is a multipolar trap?
1680160	1686560	Yeah, a multipolar trap is is basically another word for race to the bottom type scenarios,
1688400	1693920	which involve typically coordination problems. So a group of people who are in a system where
1693920	1700720	there's some level of competition, you know, they're competing for thing x. And in order to get more
1700720	1708080	of x, it typically means that they have to trade off some kind of values. And inevitably,
1708080	1711760	that trade off will keep happening more and more because there's individual incentives on each
1711760	1717200	person to do that. And it results in everyone ending up in a worse state than they they would be
1717200	1721600	before. So an example that is very fresh to mind because I just made a video on it is
1723520	1727840	the these new face filters that I don't know if you've seen or if you spend any time on Instagram
1727840	1735120	don't. But if you do go on there, particularly for women, but men as well, there's this just these
1735120	1741840	unbelievably good AI driven filters that you can, you know, you put your photo in,
1742480	1749440	and you press it, and it will just make your face slightly more optimal. And it's often quite subtle.
1749440	1755280	I mean, there's ones that are very clear and blatant. But the most dangerous ones are these
1755280	1760400	really subtle ones where if I was to just show it to you and you hadn't really met me before,
1760400	1765680	or you didn't know me, you would have no idea that it's there. But for the user, you see it,
1765680	1768720	you know, you can do it before and after of what your natural face looks like. And then with this
1768720	1776000	thing, it makes you absolutely hate your face. Like, like, it's astonishing. And you can apply
1776000	1780080	this to like, Angelina Jolie, you name it, the most beautiful people on earth, and it will make them
1780080	1784400	look like trash. By comparison, because you know, we're such relative creatures, right, we always
1784400	1790240	just like, we're always comparing. And these things are super cheap, they're completely ubiquitous.
1791040	1795040	And, you know, I've done all right in the looks department, and I'm like a fairly well established,
1795040	1800400	you know, mentally, you know, chicken her like mid late 30s. And they're messing me up hard.
1800400	1804000	Like I like to the point I now I've used them on my pictures to try it out, like I'm like,
1804000	1809920	how do I ever not use this all the time. So what the hell it's doing to teenagers is just like,
1810000	1815840	I can't imagine because I mean, like, they're having to compare their faces to the best, you
1815840	1823040	know, like the Hollywood version of themselves. And so the way this relates to like a multipolar trap
1823040	1829680	is individually, even if someone knows that this is bad for them to use this, and they know it's
1829680	1834000	bad for like, their followers to, you know, to be posting these pictures, because it makes their
1834000	1839520	followers feel worse about themselves. If you're trying to make it as an influencer on Instagram,
1839920	1843920	how do you do that? Well, you, you, you want to post the best pictures of yourself possible,
1843920	1848560	like typically beauty and sex cells, ultimately. So individually, everyone is incentivized to
1848560	1856240	actually use one of these, these, these apps. And then even if, and even if people get together
1856240	1858880	and say like, this is bullshit, we shouldn't, we shouldn't be doing it, you know, it's bad,
1858880	1864000	it's bad for us and for everyone else. Ultimately, it's still such a competitive rat race that
1864000	1868720	there's such a pressure on everyone to quietly go and use one, particularly as no one can really
1868720	1873120	tell if you're using it. And then people will suspect that others are using it anyway. So then
1873120	1875920	they're like, well, I might as well. And then basically everything falls back down again. So
1875920	1882800	it's impossible to get like a, a reliable, you know, a, a solid pact going where no one uses
1882800	1888400	these things because of these incentive pressures. So it's turning beauty, because beauty was kind
1888400	1892720	of historically considered to be something that correlated with health, right? That's kind of
1892720	1898560	how it originally emerged back in, you know, presumably in prehistoric times, females wanted
1898560	1903680	to mate with males that showed signs of evolutionary fitness for their environment.
1904880	1908400	But then there are points like, like with peacock feathers and so on where
1909360	1914560	sexual selection and like what's good for your environment, you know, for your survival can
1914560	1920400	like decouple and diverge. And then you stop optimizing for this like secondary trait.
1921600	1927520	And, you know, so basically where beauty can get decoupled from health. And that's, to me,
1927520	1933840	these, these apps are like the ultimate example of that because you're like, we know they're
1933840	1937680	unhealthy for us mentally. Like there's tons of studies out there which are showing this just
1937680	1942480	actually really, really bad for teenagers in particular. And yet we, because they make us
1942480	1947600	look so good, we can't stop using them. So yeah, that's like a multipolar trap where you just can't
1947600	1953120	get everyone to agree to not use them. Yeah, that's a great and terrifying example, isn't it? It's
1953120	1956560	quite, yeah. I mean, it's a very mild example actually, you know, in the grand scheme of
1956560	1962000	multipolar traps, there are much, much more dangerous ones, you know, like AI arms races
1962000	1966480	and so on. But it's like a nice little example because it's, I think it's also a good example
1966480	1970000	because it's something that a lot of people who aren't typically exposed to these kind of ideas
1971040	1976000	can relate to. Another thing that we're kind of skirting around is the topic of complexity.
1976000	1979920	And you have a background in physics as well. And the kind of complexity theory,
1980880	1985440	what doesn't necessarily come from physics, but I think it's kind of the physics and yeah, I mean,
1985440	1991120	they're like brothers and sisters, they're not the home of complexity theory. So why are you
1991120	1996160	interested in it? What is it about complexity theory you find useful? I mean, it's just,
1997280	2002080	I mean, it's kind of the study of what is, right? Because, I mean, whether or not you believe in
2002080	2008320	aliens, like the earth, what's going on on earth right now is just so, from a computational
2008320	2012240	standpoint, it's probably the most complex thing within our, certainly within our pocket of the
2012240	2019760	universe, in my opinion, and probably the observable universe. And it's this weird,
2021280	2025360	you know, it's kind of poetic in that like complexity to describe something that is hard
2025360	2030080	to describe. And it's by, even by that, we struggle to even come to a definition of what
2030080	2035440	complexity is. So like, that's why I just find it so fascinating, because we just, it's like
2035440	2040160	really like the cutting edge. So it's very much a frontier of knowledge. You know, just like we,
2041840	2045600	there's certain things in fundamental physics we haven't figured out. We really haven't, we just
2045600	2051760	don't have like a solid theory of complexity yet as a civilization. We're getting there.
2053040	2060400	And it, I think will be, again, this is an intuition, but it, I feel like understanding
2060480	2066800	complexity is kind of essential, or at least having a firm grasp on it is essential for us to
2068560	2074480	make it through. You know, we are in a more complex stage of civilization than ever before,
2075040	2080080	and it's only getting more complex. And to an extent, we want it to get more complex, because,
2081360	2088640	you know, if we do blow ourselves up, that's a permanent reduction and curtailment of complexity
2088640	2096160	in the universe, which, as I mentioned to you before, I think is very bad. And so, yeah, I just
2096160	2099600	think it's, I just think it's an absolutely fascinating topic. And every time I speak to
2099600	2104000	someone about it, I always learn something new. That's the interesting thing as well. You speak to,
2104000	2107920	not even necessarily experts, just asking people to define it. It's like, wow, I didn't think of
2107920	2112960	that before, which is usually a sign that it's a really important topic. Yeah. And what are some
2112960	2117680	of the elements, I mean, maybe useful if we talk a little bit about the elements of a complex system
2117680	2124080	compared to, well, one useful way as well is the difference between complex and complicated.
2124080	2127360	Yes. So maybe we could start there, and then we can talk a little bit about what happens in
2127360	2134880	complex systems. So a nice definition of complex versus complicated. Complicated is the opposite
2134880	2140000	of simple. So something that has, you know, many, many different bits and so on, and is, you know,
2140880	2147680	many, many sort of constituent parts. Whereas complex is the opposite of independent.
2149360	2155280	So the, the, the, so the main way a complex system differs from a complicated one is a
2155280	2160720	complex one is sort of, it has this, this level of sort of self, self-referentialism,
2161440	2167280	and it has these feedback loops and so on. And also it evolves over time. So a complicated system
2167280	2174640	has lots of different parts, but is otherwise static in time by and large. And so it's, you know,
2174640	2178240	if you were trying to simulate it, you could actually simulate it quite easily and also explain
2178240	2183440	it. Whereas a complex system, because there's sort of so many more like levels of dimensionality to
2183440	2188240	it, which are like changing and feeding back into one another. And it's sort of like these
2188240	2194400	sweet spots between all these different dimensions of things. It's very, very hard to describe
2195360	2200480	and also predict, which is sort of relates to this idea of emergence as well, because
2202080	2208400	a complex system is basically something that has emerged from something of lower complexity.
2209680	2215120	And this process of emergence is often like kind of a black box. We don't understand how and why
2215120	2221760	it happens. You know, like no one could have predicted the internet, even in like, probably
2221760	2227840	like 1890, which, you know, in the grand scheme of the timeline of humanity is nothing. But it was
2227840	2233280	this unbelievably complex thing that came out of an already complex system, and yet it would have
2233280	2240640	been impossible to predict. And so, yeah, that's the main difference to me. Yeah, that's a great
2240640	2246480	explanation. I haven't heard that one before. The image that came to me was a complicated system.
2246480	2250640	It's like a grandfather clock, right? And a complex system is the Amazon rainforest. Yes.
2250640	2255200	Yeah. And I just got this image of a grandfather clock sitting in the Amazon rainforest, as this
2255200	2261200	kind of juxtaposition, but just how incredibly different they are. You know, it's really recent
2261200	2265760	that I've become more interested in complexity. And part of it, part of what's really exciting
2265760	2270800	me about it, as I'm on this kind of, what I feel like will be a very long journey of understanding,
2270800	2276320	is that the sort of enlightenment project that began perhaps in the 1700s, all the way up until
2276320	2282880	now, I would argue, was we can look at the world and figure out how it's like a grandfather clock,
2283440	2290320	given enough time. And then as scientists progress, we've been like, okay, actually, maybe our model
2290320	2295680	was wrong. Maybe maybe the universe isn't like a grandfather clock. Maybe it's a complex system,
2295680	2300720	which I think is pretty much the truth that we've kind of arrived at. But what I'm seeing as I kind
2300800	2306400	of delve into this is just how important complexity is as a framing for so many different things.
2306400	2310080	So many of the things we've talked about on Rebel Wisdom as well, like, you know, trying to make
2310080	2314320	sense of the information landscape, trying to look at the culture wars and figure out, okay, how the
2314320	2320720	hell do we come into some sense of coherence with one another? How do we revive the common so we
2320720	2325440	can actually have new type of conversation? All of those are complex problems, because they're all
2325440	2330480	feeding back on each other. And then there's this hope of emergence, because we don't know,
2330480	2334720	like you said, we don't know what's going to emerge from the interaction of all those different parts.
2335520	2342480	My sense is there's something like a new religion of sorts could emerge, which would completely
2342480	2348960	flip would be the next thing that completely flips our entire way of working and communicating with
2348960	2353600	each other because we cannot coordinate right now. And the thing that has helped us coordinate has
2353600	2360480	been that in the past, right? So I mean, yeah, a really appealing meme. Exactly. Something that is,
2360480	2363680	and I think we have no, I don't think it would look anything like the religions that have come
2363680	2369840	before. And I've argued that it's sort of brewing as we speak online, and then breaching, I've called
2369840	2373920	it the age of breach, because things are brewing online, and then breaching into consensus reality.
2373920	2378720	And so far, it's been quite nascent, like the like GameStop, or the capital riots, where it's like,
2378720	2381520	oh, God, that looks like a new thing. And then it's like, well, it all collapsed, but
2382400	2389040	who knows? Yes, there's like a boundary between this like this, this whatever is underneath
2389040	2392240	there, and then what's in our world, and that boundary is becoming more and more
2393040	2397040	looped in some way. Definitely. That's what you describe it. It's like a larger surface area or
2397040	2402640	something. And, you know, things will pop through. And then, yeah, no, I mean, I mean, the religion
2402640	2411600	topic is a whole other thing. I, I by and large agree that not only will we likely see some kind
2411600	2417680	of new religion, and it's almost seems like people want to need that. But I think we should have
2417680	2424000	something. And my main problem I've had with all the past religions is just they've been,
2424000	2429440	they're just really unfun. They just buy not all of them, but by and large, they're about
2430320	2436160	no sex, no drinking or whatever, like just they're the antithesis of partying.
2437600	2440480	And again, like, there are some good pockets within some of them. And I think those are the
2440480	2446720	ones that, you know, have flourished for a while and so on. But clearly, it seems to be some part
2446720	2452640	of the human spirit we want to worship. We want to think of something bigger than us. That to me
2452640	2458640	is some evidence that there might be something bigger than us. And but regardless, like, why,
2458640	2465760	you know, why not play into that? Why not use that? But at least, I mean, I've been thinking
2465760	2470240	about this for a little while, I think there's, there's value in actually brainstorming,
2470960	2474240	collectively, what would we want if we were to design a religion, what would it look like?
2475360	2480560	I think it's worth certainly the viewers of rebel wisdom and people thinking about like,
2480560	2484240	just spending some time to write down five things that they would like about religion or
2484240	2487440	something like that. And then we hive mind it and see what comes.
2487520	2492480	Yeah, I'll join right now. Straight up. Before it even gets developed. Yeah. Yeah,
2492480	2496080	that it's a very it's an interesting one. It touches on something we've talked about as well,
2496080	2501840	which is this sort of this strange relationship we now have with with rationality and reason and
2501840	2508560	our understanding of of what that is. And our conception of ourselves as rational actors when
2508560	2514480	we're we're sort of anything but and yet we do have this capacity to take a step back and be
2514480	2520480	reasonable. I'm fascinated by that dynamic in particular and how it's how it's showing up
2520480	2525360	culturally, you know, this sense of on every side of the political, let's say every tribe
2525360	2530240	in the political spectrum, there seems to be this sense of, I feel like this, therefore,
2530240	2537920	this is truth, whether it's in sort of successor ideology of progressives right now, or whether
2537920	2544960	it's in the the kind of rabid certainty of, you know, some people on the right about their
2544960	2552160	their own views. There's a sense of feeling an emotion overtaking our sensemaking. And I think
2552160	2559520	often religion has been the necessary place to pour that energy with other people as well.
2559520	2563760	Yeah. And so I think without it, we're, I mean, we've talked about this a lot, we're totally
2563840	2569520	adrift in a hurricane. Yes. Yeah. Yeah. So yeah, I'm I'm curious. And I think it'll be weird as hell,
2569520	2575200	whatever, whatever emerges will be weird. Yeah. Yeah. So you actually, when we were talking before
2575200	2578880	before this discussion, you actually introduced me to a concept I hadn't heard of before,
2578880	2584800	which comes from, well, Scott Alexander popularized it, but it comes from an Allen Ginsburg poem
2584800	2590480	originally. And before that is it, I think a believer Sumerian God, Canaanite God called
2590480	2596000	Moloch, right? Yes. And I got quite excited when I came across this, and I'll talk about why in a
2596000	2600480	bit first we should talk about what exactly it is it relates to complexity. And there's this
2600480	2605200	other concept called Moloch. And yeah, what is it? Yes, I mean, you summarized where it's where it
2605200	2610400	came from quite well, it was originally the, it's either the Canaanites or the Coffinogens,
2610400	2618080	I don't know, but it was a God of war, that they supposedly sacrificed their children to
2618080	2622560	by putting them into an oven and burning them so that Moloch would be happy and they'd win their
2622560	2629600	wars. So really, that was dark as it gets. And then it became more popular when, when Allen
2629600	2635600	Ginsburg wrote this amazing poem called, it's actually called Howl, talking about this thing
2635600	2641520	that sounds sort of analogous to capitalism, making people mad. And then Scott Alexander
2642480	2648800	really nailed it. But he wrote this unbelievable blog called Meditations on Moloch, which was
2648800	2652640	the first time I've seen, basically, he's trying to analyze what Allen Ginsburg is talking about,
2652640	2658080	this this sort of mechanistic thing. And what he really did for the first time was he related
2658080	2665760	Moloch to game theory, and talked about how it seems to be this sort of the force. So again,
2665760	2670960	if there's this force of something that's driving the, you know, emergence and complexity,
2670960	2677760	there seems to be this opposing force, which is a force of destruction that sort of uses competition
2677760	2684560	for ill. And, you know, the way the way I'm terming it, I'm doing a video series on this,
2684560	2691040	and I'm terming it as basically Moloch is the God of unhealthy competition of negative sum games.
2691040	2695920	So competitive interactions that make the world worse off for their existence, as opposed to
2695920	2702160	being neutral or better, because games can be good or bad, you know. And, and so Moloch is kind
2702160	2708160	of like this personification of that. But in reality, what it is is just this diet, there's
2708160	2714480	like dumb blind force of like, evolution and economics, where basically you'll have these
2716640	2722000	systems where individuals are incentivized to do sort of the selfish thing, kind of like a prisoner's
2722080	2728640	dilemma, like a multi-person prisoner's dilemma, where they, everyone is individually incentivized
2728640	2735120	to do the thing that will give them a short term gain. But if everyone does that, then everyone
2735120	2740000	overall ends up worse off. So from, basically, from a God's eye view, everyone should do the
2740000	2744560	cooperative thing. But in reality, because it's so hard to get so many people to coordinate,
2745600	2750000	there's no way of enforcing it, then everyone ends up in a bad place. And that's, you know,
2750000	2756080	it's called a multipolar trap, or a Moloch trap, as I like to call it. So yeah, that's kind of,
2756080	2761520	it's kind of an abstract concept. But for simplicity, think of it as like the God of unhealthy,
2761520	2767200	when competition goes wrong. Very cool. Yeah, that's a really cool explanation. And I like in
2767200	2772480	that essay that he points out, it just takes one person to be a dickhead for in many of those
2772480	2777200	multipolar traps, everyone's cooperating except for one person, and that can then... In the really
2777200	2782800	bad ones, in the worst designed ones, like a good example would be like, you're at a stadium
2784560	2789760	at a football game, and you're in a block, and everyone's sitting down at the start of the game.
2791040	2796560	But then the team comes on, and someone at the very front gets excited and just wants a slightly,
2796560	2799840	slightly better angle so they stand up, and then makes the person behind them stand up,
2799840	2805840	and then the next person ends on. And everyone has to stand up now. And they're just stuck there,
2806480	2812400	basically the system has fallen into this lower state where, yes, sure, you could quit the game
2812400	2816240	by being like, I refuse to stand, I'm going to sit down, but now you don't get to see. So
2816800	2822720	either way, you don't have a better strategy than the current one, which is now standing up.
2822720	2827200	But if you could poll everybody, they would much rather everyone be sitting and have the
2827200	2834480	same view, effectively. So yeah, that's just one example of where a poorly designed system,
2835120	2839360	due to competitive dynamics, can have this like cascading effect where everyone ends up
2839360	2841760	in this annoying situation where they'd rather not be.
2842560	2846400	Yeah, and I think what's, I really love the concept, because I think one of the things that's
2846400	2853600	useful about it is that in the various circles who are interested in changing the world in some way,
2854240	2859920	changing the system, finding ways to create a better system, which is something I really also kind
2860000	2866160	of identify with and I really care about a lot, there is often very little, it's often very meta,
2866160	2873120	it's often like, it gets very zoomed out, and doesn't necessarily look at the forces acting
2873120	2877520	again. Sometimes it does, sometimes it doesn't, right? And there's something about the concept.
2877520	2881040	So I got really excited when you, sorry, when you introduced me to the concept, because I looked at
2881040	2886880	the, you described it a bit, I read the Scott Alexander essay, and then I, for me, there was a
2886880	2890960	lot of crossover with the model I'm really familiar with from, from mysticism, from the
2890960	2896880	Gnostic Gospels. So the early Christians, well, arguably they weren't really that Christian, but
2896880	2900640	some of them were a little bit more of kind of Hellenistic mystery tradition. So we're talking
2900640	2910000	kind of maybe the third century, you know, second and third century. And they had this incredibly
2910000	2915920	sophisticated model of, I would say a model of human psychology, of the human psyche. And they had a
2916000	2924480	creation myth in which the earth is a living goddess, and the god of the Old Testament, Yahweh,
2924480	2930160	is this kind of, this false god, which is just pure ego, right? And he kind of, he gets created
2930880	2935840	by mistake by her. It's kind of complex why, but he's, they describe him as like an abortion,
2935840	2942160	like he's, so she is a goddess and has the all the kind of divine creativity of a goddess, right?
2942160	2946960	She's tapped into the sort of universal source of emergence, let's call it. And he comes along
2946960	2950720	and he says, well, I'm the god of everything there is. And she's like, well, no, you're not,
2950720	2954640	because you're not, you're not, you're almost like a cracked reflection. You're not even real,
2954640	2959280	really. And it's interesting because the neuroscience of our egos has been argued that
2960000	2968080	the, the narrating eye, our egos looking out for our own ends is a kind of conglomeration of many
2968080	2974160	different aspects of our brain. And we kind of cobbled together a self from our essence. And so
2974160	2980880	in a way, yeah, yeah, exactly. Yeah. And that's exactly a lower complexity self. And in a way,
2980880	2985040	it's real, but it's not real at the same time. So you have this dynamic there.
2985920	2992240	And then they argued that when we, so human beings have that divine intentionality and
2992240	2998960	that divine creativity. And also, though, we are very easily diluted by Yahweh and very easily
2998960	3005040	diluted by the Archons, his sort of weird, mech, super weird cosmology they had, but like this
3005040	3010800	kind of mechanistic self replicating almost like machine angels, right, really out there.
3011760	3017600	So, so for the Gnostics, their argument was like, look, we see Yahweh or we say see Moloch and,
3017600	3021920	you know, it's very similar in the systems that we're part of, we see it in the Romans,
3021920	3026960	we see it in the tax collectors, we see it in the way that people replace the kind of
3026960	3031440	hierarchies of nature, the natural hierarchies with fake human made hierarchies of society.
3032480	3036560	And so they were sort of outsiders. And their argument was you have to tap into
3037280	3043520	the deeper spiritual wisdom inside yourself, the Gnosis, in order to kind of liberate, they were
3043520	3047760	like quite, you know, into like liberating yourself kind of quite psychedelic as well.
3047840	3053280	And Carl Jung was a huge, like fan of the Gnostics, right? The two of the codices are called the
3053280	3057680	Jung codices, because he bought them basically, he found them after they were discovered, the
3057680	3061600	scrolls were discovered in a cave in the 1950s by two Egyptian farmers. It's crazy.
3061600	3066160	This is amazing. That's a crazy cool story, yeah. And then the final bit of it, for me the
3066160	3072960	synchronicity was that there's a book on Jung by Peter Kingsley, he's a classical scholar called,
3073040	3078320	it's kind of, he's also kind of a mystic, which is called Katafalk. And he talks a lot in that
3078320	3084480	book about this biblical, this kind of mystical tradition that he argues Jung was very familiar
3084480	3091680	with, which is about prophecy. And the prophecy, a part of knowing who the prophet was was that
3091680	3096240	they howled, right, which is the name of the Ginsburg poem. So I was like, oh, very cool.
3096240	3100640	There's a lot, I know, right? There's a lot of, there's a lot of crossovers like everywhere.
3100640	3107360	Yeah. And what I love about that is like, for me, I look at it like, I think they were, and Jung
3107360	3112720	thought they were incredibly sophisticated, mappers of the conscious of human consciousness.
3112720	3116640	But there's also some aspect to it, which can't be quite explained for me, which is that
3117360	3124800	there is this, it does seem there is this entropic force that humans create that we create
3124800	3129360	from our own, I don't know what your take on it is, is it do we create it from our own
3129360	3134240	bad incentives that are built into us or? I don't think humans in particular create it.
3134240	3141440	I think it's just, I mean, again, depending on sort of the mood of the day, like it almost
3141440	3148240	feels like it's just like there are these two opposing types of forces going on within the
3148240	3155360	universe, like deeply metaphysical forces, where, you know, it's not that it is entropy,
3156000	3160800	but it's like it uses entropy to its advantage. It's basically a thing trying to,
3163440	3167440	I don't know, but like, Mike is like, the way, the way I envision it, it's just like, you know,
3167440	3172480	that feeling where you are playing something and you want to win so badly. And I know this,
3172480	3177840	because I had this like pathologic, I was pathologically competitive as a kid. And it's
3177840	3184160	like eyes on the prize, but to this like all consuming extent, and you can't see anything
3184160	3189280	else, you can't see the externalities of what you do. And all you care about is this like,
3190080	3198720	optimizing for this one narrow reward. And the byproduct of that is that if you play that out
3198720	3202720	to its logical conclusion, it means that you will turn basically the everything into the
3202720	3206400	universe into this one thing. So the ultimate instantiation of that is actually like the
3206400	3210000	paperclip maximizer, right? Which is why when you mentioned the Gnostics talking about this
3210000	3212720	like mechanization of all these many, many things of like the same thing.
3213600	3215920	Could you describe what that is in case that someone isn't familiar with it?
3215920	3220880	Yeah. So the paperclip maximizer is this, this thought experiment, I think by Eliezer Yudkowski
3220880	3227760	of like, a way in a super intelligent AI could go wrong, whereby it's super intelligent in that
3227760	3235200	it's unbelievably good at getting whatever it wants done done. But it's stupid into the extent that
3235200	3239200	it was basically programmed to do this one narrow thing, which in this instance, you wanted to
3239280	3243520	make paperclips. I just wanted my AI to make some more paperclips better than what I can
3243520	3248880	currently do. I'm a paperclip maker. But because it's so unbelievably good, it turns everything
3248880	3253760	from, you know, the factory it's in, it figures out how to pull the constituent parts of the
3253760	3258960	atoms, the blood, you know, the hemoglobin in your blood, the iron extracts it and, you know,
3258960	3262960	dismantles everything until it can tie all the entire universe, anything it can in the universe
3262960	3270160	into paperclips. Why? Because basically, it was so laser focused eyes on the prize, winning
3270160	3275120	the goal of making as many paperclips, that you end up dismantling the universe into this
3275680	3282880	very low complexity state. So it's effectively like kind of a, it's analogous a little bit to,
3282880	3287440	you know, the heat death of the universe. Because what is that? It's actually a very low complexity
3287440	3292720	state. Where basically, it's just like, how would you describe it? Well, it's just homogenous
3292720	3298320	gray soup. You know, that's all it is. The universe started out very low complexity in that it was
3298320	3304000	kind of like this singularity of matter and energy. But you know, if we're talking in terms of
3304000	3310000	Korma-Garov complexity, which is like basically how many, how many bits of code do you need to
3310000	3316880	describe a thing? It started at the universe, started out pretty simple. And then time started
3317280	3321280	and things started unfolding. And suddenly we, you know, we've started seeing hydrogen and
3321280	3326560	then helium. And that would coalesce into stars, which created greater, heavier elements. And all
3326560	3331440	this beautiful complexity started emerging, patternicity. There's like dance between order and
3331440	3336960	disorder, you know, with like a bit of hierarchy, but a bit of anarchy and so on that creates this
3336960	3344480	like highly complex dynamic system that's very hard to describe. Like to write the piece of code,
3344560	3349840	to describe the universe, you basically have to just create the universe. That's what a highly
3349840	3357360	complex system is. And, but at some point, the stars will die out and so on, all this sort of
3357360	3361920	free energy that is used to create all this complexity will start dissipating. And then it'll
3361920	3368560	slowly, as far as we know, turn into this gray soup, which is low complexity. So it'll do this,
3369120	3380000	entropy will do this over time. And so, yeah, so this like this gray soup is kind of analogous to,
3380000	3385280	in my opinion, like what a paperclip maximizer would would do. It's not a very, basically,
3385280	3390160	it's permanently curtailing, you know, there's no more complexity to to arise. The universe has
3390160	3398160	reached this steady state. And that is, you know, it seems like a tragedy on enormous potential
3398240	3403040	because at least up until now, it seems like the universe is trying to emerge into greater and
3403040	3407760	greater states of complexity. So I hate to boil it down to like good and evil terms. But
3408960	3416000	to me, good is that which creates, you know, allows for greater emergence and complexity
3416000	3421600	to appear and thereby utility, you know, useful information that we can process and do and make
3421600	3427840	wonderful things with. And evil is that which does the opposite. In other words, like turns
3427840	3434640	things into this like low diversity, low, like high, you know, like very basic situation,
3434640	3439440	whether it's, you know, a cloud of hydrogen, which in the mountain, the, the, the howl poem he
3440320	3446720	and Ginsberg describes Moloch as Moloch, who is a cloud of sex as hydrogen. So it's like this,
3446720	3450400	yeah, it's like this kind of a force of entropy, but slightly different because entropy is actually
3450400	3456000	just kind of neutral. Entropy is just like time effectively. Whereas Moloch is a thing that turns
3456000	3461680	everything into this like one mono focus of sacrificing everything in order to win this one
3461680	3467520	thing. Hence the like child sacrifice. Brilliant. Yeah. It's so I think it's a really, really
3468320	3472480	excited to introduce this model into kind of, you know, into the channel and like our thinking
3472480	3477280	in general, because I think there's, there's something I don't quite know what, and I think
3477280	3480560	this is the journey you're on, right? Like there's something in it, right? There's something in it.
3480560	3485120	Well, so the current thing I'm trying to figure out right now. And I mean, a lot of this again is
3485120	3490000	like a sort of an offshoot from conversations with Daniel Schmacktenberger, Forest Landry as well.
3490960	3499440	But I don't think so like Moloch is tied to the competition, right? But competition gone wrong.
3500000	3504800	But competition can also be an enormous force for good. Because actually, like,
3506080	3510560	you know, the capitalistic model has risen the world to what it is right now. Like we would not
3510560	3516880	be living the cushy life with our, you know, nice cameras in this cool room and so on without the,
3517760	3523840	a lot of the luxuries that capitalism has provided. And in its best form, capitalism is,
3523840	3528000	you know, it's like a positive sum game. It's using competition in order to like
3528000	3534080	drive progress faster than it would without and creating these novel things that arguably wouldn't
3534080	3540720	exist without it. So it seems to be that there's like some kind of optimal
3542000	3547920	amount of competition, like zero sum type competition that we would want within the
3547920	3552480	universe, but kind of just like encapsulated in little pockets, you know, constrained in certain
3552480	3558720	ways, whereby Moloch can't get his dirty claws into it and like turn it, turn the whole, you know,
3558720	3565280	twist it into something bad. And again, like we, because a lot of, you know, a lot of people
3565280	3570400	are like, no, we just need pure, pure cooperation. Pure cooperation is all we ever want. No competition
3570400	3576320	whatsoever. But that's a lower complexity state than a universe that has mostly competition,
3576320	3580480	as I mostly cooperation, with little pockets of competition driving, driving it forward,
3580480	3587120	but in just like delicately constrained ways. So I'm trying to, it is very much just a working,
3587120	3591840	you know, a thing I'm just playing with, but there seems like there's something to that.
3592640	3599680	It's like a higher order level of complexity where you're using games in a beneficial way
3599680	3603840	without letting them get too out of control. Yeah, it's very interesting. I mean, what comes up
3603840	3611680	for me is something around intention and values, right, on the individual level, because so to
3612640	3622160	to use that Gnostic model again, another aspect of it was that the the issue is the ego disconnected
3622160	3628320	from the self in Jungian terms. So we all have an ego, we need one. That's great. I consider myself
3628320	3636800	very competitive, like, but there's also a question of priorities. And would I be competitive at the
3636880	3643280	expense of, say someone else's well being, or would I decide to be competitive instead of
3644480	3648000	loving my wife, right, if it was the trade off, which one of those priority, which one of those
3648000	3652400	are hiring my value system. So there's something about that going on, I think, which then applies
3652400	3657360	right back to the game theory examples, because in Scott Alexander's essay, you know, he ends each
3657360	3662560	of those examples with like, but from a God's eye view, if we all cooperated, better outcomes for
3662560	3668480	everyone. And what I find interesting about, yeah, so that Gnostic model saw saw the ego
3668480	3674880	disconnected from the self as unable to create anything truly novel, because all it can do
3674880	3681680	is replicate. And what it does, so we have that God and we have Mother Earth Gaia, or Sophia,
3681680	3687360	they called her. So that's kind of divine creativity that each human has. But when the ego
3687360	3692000	is disconnected from the self, it can't do anything except mimic that divine creativity. So and what
3692000	3698480	it ends up doing is creating like a Disneyland reality that looks real, but isn't real. Nothing's
3698480	3703280	authentic. Everything is skin deep. Think of kind of reality TV, for example, there's so many aspects
3703280	3708800	of our culture. I read a great essay, which I'll put in the show, it's about solar punk and the
3708800	3714160	author talked talked about, we've run out of authentic things to frack from our cultures, and
3714160	3718480	now we're fracking our own future, you know, fracking our idea of the future just to get some
3718960	3726160	that extractive thing. And then I guess it also goes to the idea of sociopathy and psychopathy
3726160	3731280	and Machiavellianism, which is this kind of dark triad in psychology. And, you know, I've
3731280	3736560	heard Daniel Schmackenberg and many others in this space talking about that being a major problem
3736560	3740080	when you're trying to create cohesion and cooperation is like the sociopath problem,
3740080	3746960	because a sociopath or a narcissist looks like they care about other people, but they do not
3746960	3753360	care at all. They don't have empathy, but they're mimicking empathy. And it is insanely destructive
3753360	3759440	because how do we cooperate when that's in the mix? I don't even know if it's that they don't
3759440	3763360	have empathy. I think often psychopaths do have empathy, because empathy is just being able to
3763360	3767520	put yourself and like, you know, like feel what someone else is feeling. And a truly powerful
3767520	3771360	psychopath is someone who would be able to actually really feel what you're feeling, but then just
3771360	3775760	not actually give a shit about what happens to you. And like, they're doing something that will
3775760	3784400	be ultimately self-serving at the expense of you. But yeah, that's probably a good distinction to make.
3785440	3790560	And an unsolved problem, I think, in terms of, okay, we want to find new ways to cooperate,
3790560	3795520	we want to find new ways to kind of build a sustainable future. How do we deal with that
3795520	3800160	problem? I mean, it's a really, on the game theory level, it's fascinating. I'm just on the kind of
3800160	3805280	like being a human being level. It's such a big question. I mean, yeah, it's about building robust
3805280	3810640	systems whereby a few defectors, you know, of people playing this multi-person prisoner's
3810640	3815440	dilemma, where the system can sustain, you know, having a few people do it, because I think it's
3815440	3821200	going to be next to impossible to ever have everyone working purely cooperatively, purely like,
3822080	3825600	and I'm speaking from really a personal level here, because I was on a game show, which was the
3825600	3829840	prisoner's dilemma to an extent. And it was, you know, it was a one time one. And I did the,
3830320	3836400	you know, I defected. I did the selfish thing by, you know, doing the clearly dominant strategy
3836400	3839760	in terms of winning money, which was my goal on the thing. I went in and I was like, I need to win
3839760	3845200	money. So I did the selfish thing. But man, like, I'm still getting hate messages from that today,
3845200	3850400	like 14 years later. What was the game show? It's called Golden Balls. It's on YouTube. I hate it.
3851360	3860000	And you're worth watching. Did you win? I won. Yeah. I mean, I played it. I played the game
3860000	3865040	perfectly from a, if you're optimizing for the value of winning money. But that's again,
3865040	3871840	it's this bigger question of like, what are the externalities to this? Overall, you know, I was
3871840	3877280	doing the sort of the, yeah, I was doing the strategically, I picked the game theory optimal
3877280	3881760	solution within the definitions of the game. But outside of that, there's other externalities
3881760	3885280	about the value of cooperation, the value of not looking someone in the eye and saying,
3885280	3888800	yeah, I'm going to split this with you. And then actually stealing it, you know, and
3890560	3895040	and it taught me like a really important lesson of thinking about the externalities of whatever
3895040	3900320	it is we do. And so, but what we need really is to build a system that is robust enough,
3900960	3907760	complex enough, but in the right way, where it minimized where the amount of externalities
3907760	3913280	any one individual can do, negative externalities, any one individual can do are contained and
3913280	3919200	constrained. And I don't know how we do that, but we have to find a way. Yeah, so it's a really
3919200	3925040	good point. I think an example I've used before is the, how the US Constitution was made, because
3925120	3931040	they did a quite good job. Obviously, not perfect, but quite good job of thinking. Yeah,
3931040	3935360	really credit at the time of creating an antifragile system with that in mind, because they were
3935360	3940320	focused on how do you prevent tyranny. And the process whereby they went through it was,
3940320	3943760	you know, in part the federalist papers, we're just writing to each other constantly being like,
3943760	3947760	well, what if someone does this, like, oh, shit, yeah, what if someone does that? Yeah,
3947760	3952080	stress testing constantly. And I think, I mean, that's probably a good place for us all to start
3952080	3956560	is just to kind of, yeah, start having that conversation, which I think a lot of people
3956560	3960720	already having and start to kind of stress testing. And now we have the luxury of being
3960720	3965120	able to actually make models and play them out. Like, we can, you know, there's like,
3965120	3969120	I think, you know, game designers, et cetera, a lot of people out there who can go, okay, well,
3969120	3973600	let's let's put it in a simulation and see what happens, you know, that's really exciting to me.
3973600	3979360	Yeah. Yeah. No, I mean, that's exactly as they're, the blessing and curse of technology is that
3979440	3986480	technology is particularly exponential ones are making Moloch's life much easier to,
3987360	3992400	to destroy the universe. But at the same time, we're also building technologies that
3992400	3996400	enable cooperation better coordinate, you know, the ability to coordinate with one another.
3997280	4001200	You know, like a simple internet forum, it's just such a valuable thing that like just never
4001200	4006720	existed before. You know, it's not only because it's a way of sharing information across multiple
4006720	4011920	minds, across time, not just in the moment, but also storing it so that new people can come in
4011920	4016400	and retroactively go back and learn and then like, and now add something new and so on.
4016400	4020560	So when we were prepping this interview, you mentioned like quite a few different things
4020560	4024560	you're doing, which sound very exciting. And I think our viewers will be quite interested in.
4024560	4028000	So, and you also have a YouTube channel, which I think is where they're going to be coming.
4028000	4032720	Yes. And as a reminder, you're appearing and our state of sense making free event at the end of
4032720	4036880	September. So people can check you out there. But we'll put your YouTube info below in the
4036880	4040080	show notes and maybe you could tell us a little bit about like what's coming up, like what you're
4040080	4046880	working on. Yeah. I've been down the rabbit hole on this like Molek concept and to an extent complexity.
4047680	4053920	So yeah, making a series, trying to pick apart what Molek is and also bring it to life. I do
4053920	4059280	some acting in it for the first time, which is weird. I dress up. It's a whole thing. But
4060240	4066000	a lot of it relates very tightly to what we've talked about today. So yeah, people enjoyed
4066000	4069600	this, then they should check it out. Yeah, I'm really looking forward to that. So yeah, subscribe
4069600	4075280	to live. We'll put the your different social handles down the show notes and thank you so much
4075280	4079840	for coming on. Thanks for having me. This is awesome. Our ability to make sense of the world
4079840	4085280	is breaking down. We're making more and more consequential choices with worse and worse sense
4085280	4092000	making to inform those choices, which is kind of running increasingly fast through the woods
4092000	4097280	increasingly blind. Over the last two years, Rebel Wisdom has interviewed some of the world's top
4097280	4104480	thinkers. Now we've brought them together for an eight week online course, Sense Making 101,
4105120	4112720	with Daniel Schmacktenberger, Diane Mushow-Hamilton, John Vivecchi, and more. Improve your sense making,
4112720	4120960	develop your sovereignty, and join a wider community looking to do the same.
