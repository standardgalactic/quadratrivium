1
00:00:00,000 --> 00:00:09,600
So, this is one of the most interesting conversations I've had all year, and it's with former

2
00:00:09,600 --> 00:00:12,920
poker pro and scientist Liv Berea.

3
00:00:12,920 --> 00:00:17,720
So Liv has a YouTube channel where she makes films about game theory, complexity, physics,

4
00:00:17,720 --> 00:00:22,360
a lot of other really interesting topics, and we covered a lot of ground in this conversation,

5
00:00:22,360 --> 00:00:26,480
including what she learned about intuition and logic playing poker.

6
00:00:26,480 --> 00:00:29,440
Everyone in the world tells you, oh no, trust your gut when it's got a really strong feeling,

7
00:00:29,440 --> 00:00:31,280
it knows, it knows something that you don't.

8
00:00:31,280 --> 00:00:37,400
And it's like, well, this is clearly not true, because it's often wrong, and it seems to

9
00:00:37,400 --> 00:00:39,920
be largely, am I having a good day or a bad day?

10
00:00:39,920 --> 00:00:45,080
If I'm having a good day, my intuitions tend to be more optimistic, if I'm having a bad

11
00:00:45,080 --> 00:00:49,080
day, my intuition is pessimistic, so I can't trust it that much.

12
00:00:49,080 --> 00:00:53,520
We also talked about game theory and why it's such a useful tool for sense making.

13
00:00:53,520 --> 00:01:00,400
It's just the mathematics of these competitive situations, effectively, looking to see what

14
00:01:00,400 --> 00:01:06,320
optimal strategies are, suboptimal strategies, and the phenomena that arise out of them.

15
00:01:06,320 --> 00:01:11,760
And we delved into a concept popularized by Scott Alexander called Molok, which took the

16
00:01:11,760 --> 00:01:14,320
conversation in a really interesting direction.

17
00:01:14,320 --> 00:01:21,280
And what he really did for the first time was he related Molok to game theory, and talked

18
00:01:21,280 --> 00:01:25,880
about how it seems to be this sort of the force, so again, if there's this force of

19
00:01:25,880 --> 00:01:30,880
something that's driving the emergence and complexity, there seems to be this opposing

20
00:01:30,880 --> 00:01:37,080
force, which is a force of destruction that sort of uses competition for ill.

21
00:01:37,080 --> 00:01:43,760
Molok is the god of unhealthy competition, of negative sum gains, so competitive interactions

22
00:01:43,760 --> 00:01:48,120
that make the world worse off for their existence, as opposed to being neutral or better.

23
00:01:48,120 --> 00:01:52,920
Liv is also one of the speakers at our free state of sense making event on the 25th and

24
00:01:52,920 --> 00:01:57,800
26th of September, so you can sign up for that down in the show notes, and I hope you

25
00:01:57,800 --> 00:01:58,800
enjoy the film.

26
00:01:58,800 --> 00:02:01,080
So Liv, welcome to Rebel Wisdom.

27
00:02:01,080 --> 00:02:02,080
Thanks for coming.

28
00:02:02,080 --> 00:02:03,080
Thanks for having me.

29
00:02:03,080 --> 00:02:08,240
So you're a former pro poker player, and you also have an interest in game theory, which

30
00:02:08,240 --> 00:02:12,360
is something that we've covered on the channel and something we're always wanting to learn

31
00:02:12,360 --> 00:02:13,840
a little bit more about.

32
00:02:13,840 --> 00:02:19,520
But the first question I wanted to ask you is, can we play our way out of all the sort

33
00:02:19,520 --> 00:02:26,600
of game theory traps we find ourselves in, by which I mean the broken information landscape,

34
00:02:26,600 --> 00:02:31,640
institutional corruption, culture wars, polarization, obviously the list goes on, or are we just

35
00:02:31,640 --> 00:02:32,640
completely fucked?

36
00:02:32,640 --> 00:02:39,720
I mean, that's the quadrillion dollar question, right?

37
00:02:39,720 --> 00:02:43,640
I certainly don't know the answer to that.

38
00:02:43,640 --> 00:02:49,320
I sincerely hope that there is a way out of it.

39
00:02:49,320 --> 00:02:52,240
I don't see what it is, in all honesty.

40
00:02:52,240 --> 00:02:58,160
It's something that consumes my thoughts on pretty much a daily basis.

41
00:02:58,160 --> 00:03:05,160
But there's something in me, call it dumb optimism, call it the belief.

42
00:03:05,160 --> 00:03:12,760
If there's these sort of dangerous forces, sort of semi-entropic forces trying to break

43
00:03:12,760 --> 00:03:19,120
down complexity, the complexity that is civilization in one direction, it almost seems like there's

44
00:03:19,120 --> 00:03:27,960
some kind of force pro-complexity trying to hold everything together and keep this weird

45
00:03:27,960 --> 00:03:31,920
world that we're in going, keeping things interesting.

46
00:03:31,920 --> 00:03:35,840
So yeah, I mean, I don't know what the answer is.

47
00:03:35,840 --> 00:03:42,800
All I know is that we are in an unbelievably critical time of the past decade and certainly

48
00:03:42,800 --> 00:03:45,480
the coming decade.

49
00:03:45,480 --> 00:03:49,240
It's definitely, we're definitely in the most interesting time in human history.

50
00:03:49,240 --> 00:03:56,040
Okay, sure, I might be biased, seeing as I'm here, and we like to be the kings of our

51
00:03:56,040 --> 00:03:57,040
own stories.

52
00:03:57,040 --> 00:04:01,760
But yeah, shit's getting real.

53
00:04:01,760 --> 00:04:03,880
Which leads me to a question I wanted to ask you about.

54
00:04:03,880 --> 00:04:08,840
A lot of the people we've had on the channel, like Daniel Spaktenberger or Jamie Weill or

55
00:04:08,840 --> 00:04:14,320
Jordan Hall, who've talked about existential risk in particular, seem to be quite pessimistic.

56
00:04:14,320 --> 00:04:19,600
And then there's other people who have a kind of, there's a meme called Dumer Optimism,

57
00:04:19,600 --> 00:04:24,360
where there's a sense of, yes, we're screwed, but within that we have some kind of hope

58
00:04:24,360 --> 00:04:26,600
and a kind of optimism in spite of that.

59
00:04:26,600 --> 00:04:31,240
And then we have people like, there's lots of different sort of tribes, like extinction.

60
00:04:31,240 --> 00:04:35,800
So deep adaptation, which is a big influence on, for example, extinction rebellion, which

61
00:04:35,800 --> 00:04:41,200
is this sort of environmental version of, we're all screwed, it's already done.

62
00:04:41,200 --> 00:04:44,680
We need to go live in cabins and we need to be, it's all, I think it has kind of a religious

63
00:04:44,680 --> 00:04:46,200
quality to it almost.

64
00:04:46,200 --> 00:04:52,480
Yeah, I'm curious about how optimistic or pessimistic you are about the current state

65
00:04:52,480 --> 00:04:53,480
of affairs.

66
00:04:53,480 --> 00:04:57,240
It depends what day you're asking me.

67
00:04:57,240 --> 00:05:05,400
Right now I'm in a more pessimistic mood simply because I read this article by Kaifuli yesterday

68
00:05:05,400 --> 00:05:10,680
talking about the next generation of warfare that we're entering into with these, you know,

69
00:05:10,680 --> 00:05:14,600
with autonomous weapons and particularly AI-driven ones.

70
00:05:14,600 --> 00:05:20,320
It's not inconceivable within a few years that for $1,000 you can, and a little bit

71
00:05:20,320 --> 00:05:26,440
of know-how basically build an entirely anonymous drone that can take out whoever you want.

72
00:05:26,440 --> 00:05:29,880
And it will not be able to be traced back to you.

73
00:05:29,880 --> 00:05:34,520
And that's just the beginning.

74
00:05:34,520 --> 00:05:38,880
And you read that and you're just like, I don't see how we will ever make it.

75
00:05:38,880 --> 00:05:44,560
So right now I'm in a pessimistic mood, but after the weekend I did a little Burning Man

76
00:05:44,560 --> 00:05:48,680
ceremony and just felt the magic a little bit and I was like, no, we're going to make

77
00:05:48,680 --> 00:05:49,680
it.

78
00:05:49,680 --> 00:05:50,760
And it's weird.

79
00:05:50,760 --> 00:05:56,440
It's like a, I mean, you could almost say, is it like a sort of left brain, right brain

80
00:05:56,440 --> 00:05:57,440
thing?

81
00:05:57,440 --> 00:06:00,560
You know, logically I don't see a way out of it, but there's like some intuition in

82
00:06:00,560 --> 00:06:02,200
me that feels like we're going to make it.

83
00:06:02,200 --> 00:06:09,720
But then sometimes my intuitions are just like, no, like also, like I don't see a solution.

84
00:06:09,720 --> 00:06:14,160
So I don't know.

85
00:06:14,160 --> 00:06:20,840
And I guess perhaps the thing to do in the face of that, you know, this constant uncertainty

86
00:06:20,840 --> 00:06:29,000
and this oscillation between the two extremes is to, you know, which state of mind would

87
00:06:29,000 --> 00:06:31,200
I rather live in more?

88
00:06:31,200 --> 00:06:34,720
Given that I don't know which one's the right one and I seem to be able to go into both,

89
00:06:34,720 --> 00:06:42,160
I might as well invest in what I can in myself to control, you know, am I more likely to

90
00:06:42,160 --> 00:06:44,720
wake up in an optimistic mood or a pessimistic mood?

91
00:06:44,720 --> 00:06:51,200
And I can control that to an extent by, by, you know, taking care of my information diet.

92
00:06:51,200 --> 00:06:52,200
What do I read?

93
00:06:52,200 --> 00:06:54,200
How much time do I spend on Twitter?

94
00:06:54,200 --> 00:06:58,320
You know, definitely the pessimism is correlated to the amount, you know, my, what my, my phone

95
00:06:58,320 --> 00:07:01,160
tells me I've been looking at Twitter, you know, how many minutes per day?

96
00:07:01,160 --> 00:07:08,160
You know, part of the thing I hate about, you know, you go on to Netflix or whatever

97
00:07:08,160 --> 00:07:16,560
right now, it's just the amount of dystopian art out there to utopian art is like a hundred

98
00:07:16,560 --> 00:07:17,560
to one.

99
00:07:17,560 --> 00:07:18,560
Probably worse even.

100
00:07:18,560 --> 00:07:26,600
There's just, there's just so little utopian programming, films, books.

101
00:07:26,600 --> 00:07:30,000
And part of the reason I think is because it's just much easier to imagine a dystopia.

102
00:07:30,000 --> 00:07:36,640
You know, one of the reasons why utopia doesn't exist is because it's incredibly hard to build.

103
00:07:36,640 --> 00:07:40,920
So again, if you have a more optimistic society, then it gives people the space to dream up

104
00:07:40,920 --> 00:07:44,360
and think of more positive things and it gets the ball rolling in the right direction.

105
00:07:44,360 --> 00:07:45,880
Yeah, I like that.

106
00:07:45,880 --> 00:07:51,600
There's a bunch of stuff in there I'd love to pick up on the choice to be optimistic.

107
00:07:51,600 --> 00:07:54,680
You know, I think that is a very interesting thing in the times we live in.

108
00:07:54,680 --> 00:07:58,880
And it doesn't surprise me that it's actually something I've written about before the fascination

109
00:07:58,880 --> 00:08:00,480
we have with dystopia.

110
00:08:00,480 --> 00:08:04,800
I think is in, I think there's lots of reasons for it, but I think in part I've described

111
00:08:05,800 --> 00:08:09,960
the world we're living in sort of culturally is like a noir story, like a noir detective

112
00:08:09,960 --> 00:08:16,160
story where you have the detective going through the often like encountering different institutions

113
00:08:16,160 --> 00:08:20,720
like the church and okay, it turns out the priest is in on it and corrupt and like corrupt

114
00:08:20,720 --> 00:08:24,720
to the core and then the judges are corrupt and the police are corrupt and everyone's

115
00:08:24,720 --> 00:08:32,000
corrupt except for the detective usually and plays this kind of chivalric role of the kind

116
00:08:32,040 --> 00:08:39,160
of broken but all like roughly altogether pure of soul somewhere in there right and

117
00:08:39,160 --> 00:08:42,840
they're like, but there's light in the darkness, but they're sort of like hitting the whiskey

118
00:08:42,840 --> 00:08:46,720
because like just to be the light in the darkness is so much and I think I think there's something

119
00:08:46,720 --> 00:08:53,760
in that as a kind of as an image and there are and also just kind of, you know, it we're

120
00:08:53,760 --> 00:09:01,880
recording this not long after the 20th anniversary of 9 11 and the the sheer shaking foundational

121
00:09:01,880 --> 00:09:09,040
shaking impact of things like 9 11 and the financial crash and many other things that

122
00:09:09,040 --> 00:09:16,240
have happened since then in terms of institutional trust. I think we we have this sense of being

123
00:09:16,240 --> 00:09:21,680
in or moving towards a dystopia, but I'm very interested as well in something you were

124
00:09:21,680 --> 00:09:27,880
just talking about in the the optimism within that right the the kind of and even so there

125
00:09:27,880 --> 00:09:32,360
is this kind of there is this hope so I wanted to talk a little bit about game theory because

126
00:09:32,360 --> 00:09:35,680
this kind of points towards a critique I have of game theory, but I thought it'd be cool

127
00:09:35,680 --> 00:09:42,040
to to just kind of get a bit of a definition of what game theory is before in case anyone's

128
00:09:42,040 --> 00:09:48,720
not really familiar with it. All game theory is is a branch of economics basically, which

129
00:09:48,720 --> 00:09:57,520
typically deals with competitive systems and it looks at the strategic, you know, it describes

130
00:09:57,560 --> 00:10:02,480
people or decision makers within it as agents typically just so that it's not human centric,

131
00:10:02,480 --> 00:10:11,760
you know, AIs could be decision makers or whatever, mice, rats and so on. And it's just the mathematics

132
00:10:11,760 --> 00:10:17,600
of these competitive situations effectively looking looking to see what optimal strategies

133
00:10:17,600 --> 00:10:23,200
are suboptimal strategies and the phenomena that arise out of them. You have an interesting

134
00:10:23,240 --> 00:10:28,440
position in this because you're a former pro poker player. And so, you know, one could argue

135
00:10:28,440 --> 00:10:32,440
maybe this is wrong, correct me, but is that you were sort of having to apply game theory

136
00:10:32,440 --> 00:10:38,440
under tremendous pressure with high stakes. But I'm curious about on the table, how much

137
00:10:38,440 --> 00:10:43,960
of that is game theory and how much of it is, obviously, experience and then intuition,

138
00:10:43,960 --> 00:10:47,480
which is another thing I'd love to talk about. But, you know, the critique of game theory

139
00:10:47,480 --> 00:10:53,280
is often just like with with modern economics is that it relies on on actors being rational.

140
00:10:53,280 --> 00:10:57,280
I know not all game theory does, but this idea of rational actors looking after their

141
00:10:57,280 --> 00:11:00,960
self interest, you know, it turns out we're not really rational actors, but I still think

142
00:11:00,960 --> 00:11:04,920
it has a lot of value. How applicable was it to to your poker career?

143
00:11:05,200 --> 00:11:12,400
So I think we need to step back a bit in terms of like describing what's going on at the poker

144
00:11:12,400 --> 00:11:18,840
table. So you will have, obviously, seven or eight other people around the table. And

145
00:11:18,840 --> 00:11:25,720
your job is to basically sift through all the different forms of information that you're

146
00:11:25,720 --> 00:11:30,640
receiving in order to figure out what the optimal decision is. And you've got multiple decision

147
00:11:30,640 --> 00:11:37,440
points. And there's a broad range of information that you're receiving, like from, you know,

148
00:11:37,440 --> 00:11:44,800
the the the amount that the person bets, the the cards that you have relating to the cards

149
00:11:44,800 --> 00:11:51,760
on the board on the table, the the the demeanor of the of the person, you know, like that

150
00:11:51,760 --> 00:11:56,320
you know about their past experience, how much they've played. But then now their face

151
00:11:56,320 --> 00:11:59,760
is doing something funny that you never noticed before, or they're breathing heavier or something

152
00:11:59,760 --> 00:12:05,280
like that, something they say. So there's this, there's a lot of qualitative and quantitative

153
00:12:05,320 --> 00:12:13,080
information coming in. And where game theory comes in, really, right, in this, in the case

154
00:12:13,080 --> 00:12:17,920
of poker, typically game theory applies to the strictly quantitative stuff. So, you know,

155
00:12:17,920 --> 00:12:24,720
the there will be certain probabilities with which other cards will come out. And you you

156
00:12:24,720 --> 00:12:28,560
know that the sort of odds that the that are being offered to you based upon your bet and so

157
00:12:28,560 --> 00:12:33,440
on. And what that means based upon that information, effectively in a vacuum, the map, the

158
00:12:33,920 --> 00:12:38,240
quantifiable information is that there will be these mathematically optimal solutions to these

159
00:12:38,240 --> 00:12:43,040
different situations, which are, because there's so many possible situations, they're very hard

160
00:12:43,040 --> 00:12:48,240
to calculate. It doesn't, it sounds like, oh, so you just need to remember the math, no. But

161
00:12:49,360 --> 00:12:52,800
what game theory will do is basically suggest that there are certain strategies

162
00:12:53,360 --> 00:12:58,800
that you will want to employ in certain situations based upon this quantified information. But then,

163
00:12:58,800 --> 00:13:03,760
of course, there's this, this like nebula of other stuff coming in, like, well, yeah, but they

164
00:13:03,760 --> 00:13:07,840
were breathing funny, which they weren't doing before. How do you quantify that and so on.

165
00:13:09,040 --> 00:13:14,480
And in terms of how much of that nebula sort of applies to your overall decision making,

166
00:13:15,040 --> 00:13:22,160
I hate to put like, try and put a percentage on it, but it's by and large, like, 90% of the

167
00:13:22,160 --> 00:13:27,440
quantified stuff. And then the the the like, these sort of fuzzy things around the edge will count

168
00:13:27,440 --> 00:13:33,840
for 10% of your decision. A lot of people, so when I first started playing poker back in 2005,

169
00:13:33,840 --> 00:13:37,840
no one really understood game theory, no one understood the mechanics of how the game worked.

170
00:13:38,400 --> 00:13:42,880
And the best players in the world were basically these like, they were typically older,

171
00:13:44,000 --> 00:13:50,560
kind of like hustler types, who had just spent decades in casinos, just seeing the gamut of

172
00:13:50,560 --> 00:13:55,600
human behavior and developing really strong intuitions about people. And so often they'd

173
00:13:55,600 --> 00:14:00,160
make these these these strange plays that would turn out to be correct. And they wouldn't even

174
00:14:00,160 --> 00:14:05,120
be able to explain to you why they did it. It was purely this sort of automatic unconscious

175
00:14:05,120 --> 00:14:10,000
intuitive process going on, which and their intuitions were basically better than everyone

176
00:14:10,000 --> 00:14:17,360
else's. But then when online poker appeared, and we started getting like, data analysis software

177
00:14:17,360 --> 00:14:23,680
and this kind of stuff, you now all of a sudden we had data that pros could look at and and use

178
00:14:23,760 --> 00:14:28,640
to analyze where they were going wrong, you know, where are the leaks in their game. And this kind

179
00:14:28,640 --> 00:14:33,520
of, you know, it lifted the lid lifted the veil on what's going on in poker. And now people to

180
00:14:33,520 --> 00:14:38,640
realize that, okay, there's mathematical solutions to this. And what it meant was that the game

181
00:14:38,640 --> 00:14:42,640
basically went through a sort of scientific revolution, away from this pure artistry of just

182
00:14:42,640 --> 00:14:47,920
feeling and having a vibe of someone to to going, Well, actually, look, this is the mathematical

183
00:14:47,920 --> 00:14:53,360
optimal solution. I'll stick to this until I get such strong overwhelming evidence from

184
00:14:53,760 --> 00:15:00,960
something else that I might override it. And in that, you know, in some ways, I hate this, but

185
00:15:01,680 --> 00:15:05,840
in reality, like you cannot be a top professional these days without having that mathematical

186
00:15:05,840 --> 00:15:12,400
foundation. You just no one's intuitions will be able to surpass knowing if you're playing as

187
00:15:12,400 --> 00:15:16,320
someone who's playing a game theory optimal style, even though in some ways it's kind of robotic,

188
00:15:17,040 --> 00:15:23,520
you just can't beat it by pure intuition alone. But of course, you know, if someone is now playing

189
00:15:23,520 --> 00:15:30,240
not quite perfectly, now you can have these other like these these other like fuzzy skills,

190
00:15:30,240 --> 00:15:34,080
you can bring in these fuzzy skills to figure out how the how to exploit their mistakes.

191
00:15:35,600 --> 00:15:41,920
But yeah, the very long winded way of answering basically, it's it's by and by and large a very

192
00:15:41,920 --> 00:15:47,520
mathematical game over 80 90%. That's really interesting. Fascinating. I really like that

193
00:15:47,520 --> 00:15:52,480
that kind of laying out. I got a nice image there of the various sort of data points that are

194
00:15:52,480 --> 00:15:56,640
happening and many of them at the same time, which is a bit like, I mean, that's, you know,

195
00:15:56,640 --> 00:15:59,360
trying to make sense of something online is a little bit like that as well.

196
00:16:00,960 --> 00:16:05,680
Yeah, and and I think it's interesting with like the particular so the game of poker, I think

197
00:16:06,560 --> 00:16:11,680
can we can look at also life in general through that lens in a lot of ways like we

198
00:16:12,560 --> 00:16:19,920
like I also have this kind of yearning for being purely intuitive, right? But I also am aware that

199
00:16:20,240 --> 00:16:24,320
intuition really does lead us to stray a lot of the time, right, especially when we're trying to

200
00:16:24,320 --> 00:16:28,640
make sense of complexity. Something John Varvakey, we've had on the channel a lot talks about in

201
00:16:28,640 --> 00:16:34,000
cognitive science terms, you know, how do we how do we make sure the frame that we're looking at

202
00:16:34,000 --> 00:16:37,840
everything through like the like he uses the example of like the glasses we're wearing,

203
00:16:37,840 --> 00:16:41,760
how do we know that that frame is accurate? And of course, it's never 100% accurate, but

204
00:16:42,320 --> 00:16:47,360
practices and techniques that help us take the frame off and go, Oh, yeah, shit, that's a crack,

205
00:16:48,080 --> 00:16:52,320
that's smudged that I'm seeing everything wonky, clean it a little bit, put it back on. And that

206
00:16:52,320 --> 00:16:58,000
process of continuously regenerating our frames, I think is kind of more important than ever.

207
00:16:59,040 --> 00:17:05,040
At the same time, there is something, well, I would use the term kind of like transcendent or

208
00:17:05,040 --> 00:17:11,920
magic about the power of intuition. And so when we're looking at something like, well,

209
00:17:11,920 --> 00:17:15,280
actually to rewind a bit, I mean, I see this come up in culture a lot, I'm sure, you know,

210
00:17:15,280 --> 00:17:20,480
this has been a real trend of people trusting their own intuition over, say, the opinion of

211
00:17:20,480 --> 00:17:25,520
experts. And that happens a lot now. And there is good reason why we don't necessarily trust

212
00:17:26,400 --> 00:17:30,880
experts. You know, I mean, I read an article recently, which was, you know, making the point

213
00:17:30,880 --> 00:17:35,280
about America had access to the absolute best experts in international relations,

214
00:17:35,280 --> 00:17:42,000
counterinsurgency, etc. For 20 years, when the and yet the pull out of Afghanistan was a

215
00:17:42,000 --> 00:17:47,600
complete catastrophe, right? Unlimited budget almost unlimited expert credentialed experts.

216
00:17:47,600 --> 00:17:51,440
A few experiments have been done where an educated so there's a caveat to this,

217
00:17:52,160 --> 00:17:58,160
really someone who's decently educated on a topic, trying to predict the future based on that

218
00:17:58,160 --> 00:18:05,200
information, it gets about the scores is roughly in line with a credentialed expert. So and I'll

219
00:18:05,200 --> 00:18:09,920
put in the show notes probably the article that was from actually read it this morning, I think.

220
00:18:10,560 --> 00:18:15,520
It raises an interesting question though, right? So we need to be able to have some

221
00:18:16,480 --> 00:18:22,000
way of not getting completely deluded by our feelings and our intuition, which it which it

222
00:18:22,000 --> 00:18:26,000
tends to do. And every time I notice it's happened to myself, I'm like, Oh, that's a bit embarrassing.

223
00:18:26,000 --> 00:18:29,600
I really shouldn't have been so certain. But it keeps happening, obviously, because we're human.

224
00:18:29,600 --> 00:18:35,520
But and then at the same time, we need to know how to trust it in some way. I think so I'm curious

225
00:18:35,520 --> 00:18:39,680
to hear your your journey with this. I know you were kind of at one point sort of anti intuition,

226
00:18:39,680 --> 00:18:44,080
then you've gone kind of in a bit of a journey with it. Yeah. I mean, I started out, you know,

227
00:18:44,080 --> 00:18:52,800
prior to poker, my decision making process was just a delightful mix of deep overconfidence

228
00:18:53,600 --> 00:19:00,160
and emotionality. Just I was a highly emotional, you know, it was 20 year old girl who

229
00:19:00,480 --> 00:19:05,920
was the bee's knees and everything and hadn't learned about life yet. And then poker came along

230
00:19:05,920 --> 00:19:10,160
and didn't always you know, the great thing about poker is that it's like got these like quite tight

231
00:19:10,160 --> 00:19:17,920
feedback loops where well, just tight enough so that you will eventually realize that you're

232
00:19:17,920 --> 00:19:24,080
clearly not doing something right because you're losing money. But loose enough where you can still

233
00:19:24,080 --> 00:19:28,160
be deluded for a while about your relative skill level, you know, because there is this luck factor.

234
00:19:28,720 --> 00:19:33,760
And so you part of the hardest thing is figuring out when you know, when things are going wrong,

235
00:19:33,760 --> 00:19:38,240
is it because you're making bad decisions? Or is it because luck is not on your side?

236
00:19:38,240 --> 00:19:42,080
Because it can be both or a mixture of the two. So you know, you've got your your system one,

237
00:19:42,080 --> 00:19:47,680
which is like the classic like intuition, this unconscious process, and then system two, as I

238
00:19:47,680 --> 00:19:53,520
call it, well, Kahneman calls it, which is like this, the voice in your head, you know, what's 471

239
00:19:53,520 --> 00:20:01,840
plus 86, that'll be your system two at work. And so then I was like, okay, so really what poker

240
00:20:01,840 --> 00:20:06,880
is about is about this linear system two stuff, this this thinking things through like solving

241
00:20:06,880 --> 00:20:13,840
a math problem, and which it by enlarges. And, and then and I would sometimes try and you know,

242
00:20:13,840 --> 00:20:20,480
I'd be playing, and I would, you know, facing a big, you know, big, typical decision, someone's

243
00:20:20,480 --> 00:20:25,520
put me all in on the river, I'm facing my tournament life, you know, heart's pumping in my ears. And

244
00:20:26,960 --> 00:20:31,920
my gut will be saying, oh, a fold, fold, fold, they've got it this time. And the maths will be

245
00:20:31,920 --> 00:20:36,640
saying, cool, no, you've got to call you've got, you know, your hand is statistically good enough

246
00:20:36,640 --> 00:20:44,000
to do so. And I would, for a while, I would be like, well, my, my, no, my gut is so strong,

247
00:20:44,000 --> 00:20:48,880
I'm going to just listen to that. And on, well, I don't know whether it's more often than not,

248
00:20:48,880 --> 00:20:53,040
but on a sufficient number of occasions, my gut was completely wrong. And I was like, huh,

249
00:20:53,040 --> 00:20:56,880
this is interesting, because everyone's been telling me, you know, but everyone in the world

250
00:20:56,880 --> 00:21:00,240
tells you, oh, no, trust your gut, when it's got a really strong feeling, it knows, it knows

251
00:21:00,240 --> 00:21:04,480
something that you don't. And it's like, well, this is clearly not true, because it's often wrong.

252
00:21:05,280 --> 00:21:09,920
There's bias, and it seems to be largely by large, you know, am I having a good day or a bad day?

253
00:21:09,920 --> 00:21:15,280
If I'm having a good day, my, my intuitions tend to be more optimistic. If I'm having a bad day,

254
00:21:15,280 --> 00:21:21,840
my intuition is pessimistic. So I can't trust it that much. And, you know, that was sort of,

255
00:21:21,840 --> 00:21:27,520
then I did my TED Talk a few years after that where, and I still stand by the contents of it.

256
00:21:27,520 --> 00:21:34,640
Basically, I, I, I shit on intuition a little bit saying, you know, if you Google it, the internet

257
00:21:34,640 --> 00:21:40,720
tells you that it's, it's this perfect source of knowledge, you should never second guess it,

258
00:21:40,720 --> 00:21:44,640
always trust it, and so on. And I think that's actually very dangerous advice, because

259
00:21:45,600 --> 00:21:49,280
there are some things your intuition is really good for, and some things it's terrible for.

260
00:21:50,800 --> 00:21:55,680
And the main, without going into the details of what all those things are, the main thing is,

261
00:21:55,680 --> 00:21:59,440
basically, if it's something that, if it's a decision you've made many, many, many times,

262
00:22:00,000 --> 00:22:03,600
then your intuition is going to be pretty good. Which is why our intuitions are all from quite

263
00:22:03,600 --> 00:22:07,360
good around like social things, you know, you meet someone and you get like a weird vibe off them,

264
00:22:07,440 --> 00:22:10,960
and you don't quite know why. You know, you've met a lot of people, by the time you're in your

265
00:22:10,960 --> 00:22:14,720
thirties, you've met a lot of people. Chances are your intuition is fairly good, and you should

266
00:22:14,720 --> 00:22:20,320
listen to it. But if it's, you know, if you've started a new job, and you're solving difficult

267
00:22:20,320 --> 00:22:25,280
problems, that you haven't quite figured out how to do the like logic for yet, relying on your

268
00:22:25,280 --> 00:22:30,400
intuition isn't a good idea either, because it, ultimately, it needs some data and experience

269
00:22:30,400 --> 00:22:35,840
to be based off. As far as we know, it's not this purely magical thing. Might be sometimes,

270
00:22:35,840 --> 00:22:42,080
but we'll get to that. So yeah, so then I sort of went into this, like, deep skepticism of using

271
00:22:42,080 --> 00:22:46,640
intuition. Certainly, you know, and certainly was, my message was, don't overreel on it,

272
00:22:46,640 --> 00:22:54,240
be very careful, because it can be biased. But then, more recently, funnily enough, after a

273
00:22:54,240 --> 00:22:59,440
conversation with Daniel Schmaxenberger, who always manages to, like, anyone who has ever met

274
00:22:59,440 --> 00:23:05,680
him, he just like shakes people's brains up. One of the things he said to me, we hadn't

275
00:23:05,680 --> 00:23:09,440
had much of a conversation, but he pulled me aside. He's like, you need to get in touch with

276
00:23:09,440 --> 00:23:14,240
your feminine side a bit more. And I was like, what? What does that mean? And I was initially,

277
00:23:14,240 --> 00:23:19,360
like, annoyed me. Because I had, historically, sort of associated femininity a little bit with

278
00:23:19,360 --> 00:23:25,280
weakness. I'd always, you know, I've always kind of been attracted to typically male pursuits,

279
00:23:25,280 --> 00:23:31,280
you know, physics, heavy metal, poker, you know, it's all very predictable. To the point where

280
00:23:31,280 --> 00:23:39,200
I then started to associate feminine things with weakness, and so on. But it was just that I didn't

281
00:23:39,200 --> 00:23:48,880
quite understand what femininity is. And what it kind of, you know, if masculinity is kind of like

282
00:23:48,880 --> 00:23:56,960
this outward seeking, looking for some strict type ways of defining and viewing the world,

283
00:23:57,680 --> 00:24:05,760
femininity is this more sort of passive, inward facing, reflective form of thinking,

284
00:24:06,880 --> 00:24:10,880
a form of wisdom. It's just like another form of wisdom. And that's what I think he was getting

285
00:24:10,880 --> 00:24:15,200
at, basically. He's like, play around with your feminine side and it will enable you to

286
00:24:15,920 --> 00:24:21,120
start listening to your intuition a bit more and respecting it. And he was absolutely right.

287
00:24:21,120 --> 00:24:27,760
And since I started doing that, I don't know, life just became a bit easier. And

288
00:24:30,400 --> 00:24:36,080
just, I don't know, I just felt like more of a whole person. It's hard to describe. It sounds

289
00:24:36,080 --> 00:24:40,880
a bit weird, but it was kind of cool. I really like that explanation of when intuition is useful

290
00:24:40,880 --> 00:24:47,840
and when it isn't, right? It reminds me of, so there's something I've been quite influenced

291
00:24:47,840 --> 00:24:52,160
by is the work of a guy called William Duggan at Columbia Business School. And he talks about

292
00:24:52,160 --> 00:24:58,400
strategic intuition. And strategic intuition is basically our, so he uses it, he talks about

293
00:24:58,400 --> 00:25:03,040
creativity and innovation through it, but has these four different stages of how we come to new

294
00:25:03,040 --> 00:25:08,000
ideas and how we kind of get new insights. And the first one is examples from history. So you've

295
00:25:08,000 --> 00:25:12,880
done, like you said, you have lots of experience in the thing. It might not even just be in that

296
00:25:12,880 --> 00:25:18,160
thing, you have a whole database in your brain of, let you use the example of social interactions.

297
00:25:18,160 --> 00:25:22,080
So that we have all these different kind of social interactions there and are unconscious in the

298
00:25:22,080 --> 00:25:26,400
library of our memory. But then we also have loads of other things connected to that, loads of

299
00:25:26,400 --> 00:25:31,440
different frames, we have lots of different, so let's say kind of horizontal connections. So it

300
00:25:31,440 --> 00:25:35,680
might be, okay, I also know about this person's culture, or I also know about the cultural

301
00:25:35,680 --> 00:25:39,520
context of where we are, or I'm having this interaction with someone at a roller skating rink.

302
00:25:39,520 --> 00:25:42,720
Well, I know how people are at roller skates, so all these different things are going on.

303
00:25:42,720 --> 00:25:48,880
And of all that information, then unconscious, so the second stage of this innovation process is

304
00:25:48,880 --> 00:25:52,400
presence of mind. So you don't try and find the answer, which goes into that receptivity you're

305
00:25:52,400 --> 00:25:59,280
kind of talking about. It's, okay, well, I'm just going to be awake and aware and allow my unconscious

306
00:25:59,280 --> 00:26:03,840
to do its thing. And then the third stage is the eureka moment, which he calls a kudwi, which is a

307
00:26:03,840 --> 00:26:07,760
strike of the eye in French, which I don't exactly know where they got that phrase from. But it's

308
00:26:07,760 --> 00:26:12,000
a kind of, and we've all had that experience of something hits you like a ton of bricks,

309
00:26:12,000 --> 00:26:16,160
and it often happens when you're not trying to solve the problem. You were taking a shower,

310
00:26:16,160 --> 00:26:21,040
you're walking the dog, whatever it might be. And then the fourth stage is the resolution to carry

311
00:26:21,040 --> 00:26:25,680
it through. And something I used to do with companies and was on a kind of campaign to get

312
00:26:25,680 --> 00:26:30,480
rid of brainstorming, because brainstorming is really awful for that creative innovation process.

313
00:26:30,480 --> 00:26:35,760
Because what brainstorming is really good for is going, here's many ideas, let's as a group hone

314
00:26:35,760 --> 00:26:42,960
down the ideas to one, what is terrible for is here's a blank space, make ideas, because that's

315
00:26:42,960 --> 00:26:47,040
not how the process works, according to Duggan. And he goes into the kind of neuroscience of it.

316
00:26:47,680 --> 00:26:53,200
But one example he uses in that is of a fireman going into a burning building and having this

317
00:26:53,200 --> 00:26:56,960
intense feeling of get out, get everyone out. And this is a real life example. And I think

318
00:26:56,960 --> 00:27:02,160
there's quite a lot of them goes in doesn't know why that can't see anything in particular that's

319
00:27:02,240 --> 00:27:07,760
telling him this is incredibly dangerous. But has seen so many fires that somewhere is intuition

320
00:27:07,760 --> 00:27:11,200
is like something's off, get out, obviously get out the building, the whole thing collapses like

321
00:27:11,200 --> 00:27:14,800
five seconds later. That's what you're talking about. It sounds like you have the intuition.

322
00:27:14,800 --> 00:27:19,440
Well, well, I mean, usually when people report this kind of thing, they can't quite identify

323
00:27:19,440 --> 00:27:24,320
what it was. But there will be some element in the in all of the complexity of the environment

324
00:27:24,960 --> 00:27:29,280
that is connecting with a preexisting element, which is Oh, that time where that happened. And

325
00:27:29,280 --> 00:27:34,160
this I also noticed that slight offness to the smell or whatever it might be that you can't

326
00:27:34,160 --> 00:27:40,960
consciously notice. So in those moments that kind of screaming intuition. But if I walked into a

327
00:27:40,960 --> 00:27:46,880
burning building, I'd probably have that screaming intuition the whole time. Get out, get out. So

328
00:27:46,880 --> 00:27:50,960
yeah, I really quite like that example. So one of the aspects of game theory that we've talked

329
00:27:50,960 --> 00:27:55,280
about on the channel before. But I don't understand that well. So I'm going to ask you about it is

330
00:27:55,280 --> 00:28:00,160
multipolar traps seems very relevant to the times you live in life. So what is a multipolar trap?

331
00:28:00,160 --> 00:28:06,560
Yeah, a multipolar trap is is basically another word for race to the bottom type scenarios,

332
00:28:08,400 --> 00:28:13,920
which involve typically coordination problems. So a group of people who are in a system where

333
00:28:13,920 --> 00:28:20,720
there's some level of competition, you know, they're competing for thing x. And in order to get more

334
00:28:20,720 --> 00:28:28,080
of x, it typically means that they have to trade off some kind of values. And inevitably,

335
00:28:28,080 --> 00:28:31,760
that trade off will keep happening more and more because there's individual incentives on each

336
00:28:31,760 --> 00:28:37,200
person to do that. And it results in everyone ending up in a worse state than they they would be

337
00:28:37,200 --> 00:28:41,600
before. So an example that is very fresh to mind because I just made a video on it is

338
00:28:43,520 --> 00:28:47,840
the these new face filters that I don't know if you've seen or if you spend any time on Instagram

339
00:28:47,840 --> 00:28:55,120
don't. But if you do go on there, particularly for women, but men as well, there's this just these

340
00:28:55,120 --> 00:29:01,840
unbelievably good AI driven filters that you can, you know, you put your photo in,

341
00:29:02,480 --> 00:29:09,440
and you press it, and it will just make your face slightly more optimal. And it's often quite subtle.

342
00:29:09,440 --> 00:29:15,280
I mean, there's ones that are very clear and blatant. But the most dangerous ones are these

343
00:29:15,280 --> 00:29:20,400
really subtle ones where if I was to just show it to you and you hadn't really met me before,

344
00:29:20,400 --> 00:29:25,680
or you didn't know me, you would have no idea that it's there. But for the user, you see it,

345
00:29:25,680 --> 00:29:28,720
you know, you can do it before and after of what your natural face looks like. And then with this

346
00:29:28,720 --> 00:29:36,000
thing, it makes you absolutely hate your face. Like, like, it's astonishing. And you can apply

347
00:29:36,000 --> 00:29:40,080
this to like, Angelina Jolie, you name it, the most beautiful people on earth, and it will make them

348
00:29:40,080 --> 00:29:44,400
look like trash. By comparison, because you know, we're such relative creatures, right, we always

349
00:29:44,400 --> 00:29:50,240
just like, we're always comparing. And these things are super cheap, they're completely ubiquitous.

350
00:29:51,040 --> 00:29:55,040
And, you know, I've done all right in the looks department, and I'm like a fairly well established,

351
00:29:55,040 --> 00:30:00,400
you know, mentally, you know, chicken her like mid late 30s. And they're messing me up hard.

352
00:30:00,400 --> 00:30:04,000
Like I like to the point I now I've used them on my pictures to try it out, like I'm like,

353
00:30:04,000 --> 00:30:09,920
how do I ever not use this all the time. So what the hell it's doing to teenagers is just like,

354
00:30:10,000 --> 00:30:15,840
I can't imagine because I mean, like, they're having to compare their faces to the best, you

355
00:30:15,840 --> 00:30:23,040
know, like the Hollywood version of themselves. And so the way this relates to like a multipolar trap

356
00:30:23,040 --> 00:30:29,680
is individually, even if someone knows that this is bad for them to use this, and they know it's

357
00:30:29,680 --> 00:30:34,000
bad for like, their followers to, you know, to be posting these pictures, because it makes their

358
00:30:34,000 --> 00:30:39,520
followers feel worse about themselves. If you're trying to make it as an influencer on Instagram,

359
00:30:39,920 --> 00:30:43,920
how do you do that? Well, you, you, you want to post the best pictures of yourself possible,

360
00:30:43,920 --> 00:30:48,560
like typically beauty and sex cells, ultimately. So individually, everyone is incentivized to

361
00:30:48,560 --> 00:30:56,240
actually use one of these, these, these apps. And then even if, and even if people get together

362
00:30:56,240 --> 00:30:58,880
and say like, this is bullshit, we shouldn't, we shouldn't be doing it, you know, it's bad,

363
00:30:58,880 --> 00:31:04,000
it's bad for us and for everyone else. Ultimately, it's still such a competitive rat race that

364
00:31:04,000 --> 00:31:08,720
there's such a pressure on everyone to quietly go and use one, particularly as no one can really

365
00:31:08,720 --> 00:31:13,120
tell if you're using it. And then people will suspect that others are using it anyway. So then

366
00:31:13,120 --> 00:31:15,920
they're like, well, I might as well. And then basically everything falls back down again. So

367
00:31:15,920 --> 00:31:22,800
it's impossible to get like a, a reliable, you know, a, a solid pact going where no one uses

368
00:31:22,800 --> 00:31:28,400
these things because of these incentive pressures. So it's turning beauty, because beauty was kind

369
00:31:28,400 --> 00:31:32,720
of historically considered to be something that correlated with health, right? That's kind of

370
00:31:32,720 --> 00:31:38,560
how it originally emerged back in, you know, presumably in prehistoric times, females wanted

371
00:31:38,560 --> 00:31:43,680
to mate with males that showed signs of evolutionary fitness for their environment.

372
00:31:44,880 --> 00:31:48,400
But then there are points like, like with peacock feathers and so on where

373
00:31:49,360 --> 00:31:54,560
sexual selection and like what's good for your environment, you know, for your survival can

374
00:31:54,560 --> 00:32:00,400
like decouple and diverge. And then you stop optimizing for this like secondary trait.

375
00:32:01,600 --> 00:32:07,520
And, you know, so basically where beauty can get decoupled from health. And that's, to me,

376
00:32:07,520 --> 00:32:13,840
these, these apps are like the ultimate example of that because you're like, we know they're

377
00:32:13,840 --> 00:32:17,680
unhealthy for us mentally. Like there's tons of studies out there which are showing this just

378
00:32:17,680 --> 00:32:22,480
actually really, really bad for teenagers in particular. And yet we, because they make us

379
00:32:22,480 --> 00:32:27,600
look so good, we can't stop using them. So yeah, that's like a multipolar trap where you just can't

380
00:32:27,600 --> 00:32:33,120
get everyone to agree to not use them. Yeah, that's a great and terrifying example, isn't it? It's

381
00:32:33,120 --> 00:32:36,560
quite, yeah. I mean, it's a very mild example actually, you know, in the grand scheme of

382
00:32:36,560 --> 00:32:42,000
multipolar traps, there are much, much more dangerous ones, you know, like AI arms races

383
00:32:42,000 --> 00:32:46,480
and so on. But it's like a nice little example because it's, I think it's also a good example

384
00:32:46,480 --> 00:32:50,000
because it's something that a lot of people who aren't typically exposed to these kind of ideas

385
00:32:51,040 --> 00:32:56,000
can relate to. Another thing that we're kind of skirting around is the topic of complexity.

386
00:32:56,000 --> 00:32:59,920
And you have a background in physics as well. And the kind of complexity theory,

387
00:33:00,880 --> 00:33:05,440
what doesn't necessarily come from physics, but I think it's kind of the physics and yeah, I mean,

388
00:33:05,440 --> 00:33:11,120
they're like brothers and sisters, they're not the home of complexity theory. So why are you

389
00:33:11,120 --> 00:33:16,160
interested in it? What is it about complexity theory you find useful? I mean, it's just,

390
00:33:17,280 --> 00:33:22,080
I mean, it's kind of the study of what is, right? Because, I mean, whether or not you believe in

391
00:33:22,080 --> 00:33:28,320
aliens, like the earth, what's going on on earth right now is just so, from a computational

392
00:33:28,320 --> 00:33:32,240
standpoint, it's probably the most complex thing within our, certainly within our pocket of the

393
00:33:32,240 --> 00:33:39,760
universe, in my opinion, and probably the observable universe. And it's this weird,

394
00:33:41,280 --> 00:33:45,360
you know, it's kind of poetic in that like complexity to describe something that is hard

395
00:33:45,360 --> 00:33:50,080
to describe. And it's by, even by that, we struggle to even come to a definition of what

396
00:33:50,080 --> 00:33:55,440
complexity is. So like, that's why I just find it so fascinating, because we just, it's like

397
00:33:55,440 --> 00:34:00,160
really like the cutting edge. So it's very much a frontier of knowledge. You know, just like we,

398
00:34:01,840 --> 00:34:05,600
there's certain things in fundamental physics we haven't figured out. We really haven't, we just

399
00:34:05,600 --> 00:34:11,760
don't have like a solid theory of complexity yet as a civilization. We're getting there.

400
00:34:13,040 --> 00:34:20,400
And it, I think will be, again, this is an intuition, but it, I feel like understanding

401
00:34:20,480 --> 00:34:26,800
complexity is kind of essential, or at least having a firm grasp on it is essential for us to

402
00:34:28,560 --> 00:34:34,480
make it through. You know, we are in a more complex stage of civilization than ever before,

403
00:34:35,040 --> 00:34:40,080
and it's only getting more complex. And to an extent, we want it to get more complex, because,

404
00:34:41,360 --> 00:34:48,640
you know, if we do blow ourselves up, that's a permanent reduction and curtailment of complexity

405
00:34:48,640 --> 00:34:56,160
in the universe, which, as I mentioned to you before, I think is very bad. And so, yeah, I just

406
00:34:56,160 --> 00:34:59,600
think it's, I just think it's an absolutely fascinating topic. And every time I speak to

407
00:34:59,600 --> 00:35:04,000
someone about it, I always learn something new. That's the interesting thing as well. You speak to,

408
00:35:04,000 --> 00:35:07,920
not even necessarily experts, just asking people to define it. It's like, wow, I didn't think of

409
00:35:07,920 --> 00:35:12,960
that before, which is usually a sign that it's a really important topic. Yeah. And what are some

410
00:35:12,960 --> 00:35:17,680
of the elements, I mean, maybe useful if we talk a little bit about the elements of a complex system

411
00:35:17,680 --> 00:35:24,080
compared to, well, one useful way as well is the difference between complex and complicated.

412
00:35:24,080 --> 00:35:27,360
Yes. So maybe we could start there, and then we can talk a little bit about what happens in

413
00:35:27,360 --> 00:35:34,880
complex systems. So a nice definition of complex versus complicated. Complicated is the opposite

414
00:35:34,880 --> 00:35:40,000
of simple. So something that has, you know, many, many different bits and so on, and is, you know,

415
00:35:40,880 --> 00:35:47,680
many, many sort of constituent parts. Whereas complex is the opposite of independent.

416
00:35:49,360 --> 00:35:55,280
So the, the, the, so the main way a complex system differs from a complicated one is a

417
00:35:55,280 --> 00:36:00,720
complex one is sort of, it has this, this level of sort of self, self-referentialism,

418
00:36:01,440 --> 00:36:07,280
and it has these feedback loops and so on. And also it evolves over time. So a complicated system

419
00:36:07,280 --> 00:36:14,640
has lots of different parts, but is otherwise static in time by and large. And so it's, you know,

420
00:36:14,640 --> 00:36:18,240
if you were trying to simulate it, you could actually simulate it quite easily and also explain

421
00:36:18,240 --> 00:36:23,440
it. Whereas a complex system, because there's sort of so many more like levels of dimensionality to

422
00:36:23,440 --> 00:36:28,240
it, which are like changing and feeding back into one another. And it's sort of like these

423
00:36:28,240 --> 00:36:34,400
sweet spots between all these different dimensions of things. It's very, very hard to describe

424
00:36:35,360 --> 00:36:40,480
and also predict, which is sort of relates to this idea of emergence as well, because

425
00:36:42,080 --> 00:36:48,400
a complex system is basically something that has emerged from something of lower complexity.

426
00:36:49,680 --> 00:36:55,120
And this process of emergence is often like kind of a black box. We don't understand how and why

427
00:36:55,120 --> 00:37:01,760
it happens. You know, like no one could have predicted the internet, even in like, probably

428
00:37:01,760 --> 00:37:07,840
like 1890, which, you know, in the grand scheme of the timeline of humanity is nothing. But it was

429
00:37:07,840 --> 00:37:13,280
this unbelievably complex thing that came out of an already complex system, and yet it would have

430
00:37:13,280 --> 00:37:20,640
been impossible to predict. And so, yeah, that's the main difference to me. Yeah, that's a great

431
00:37:20,640 --> 00:37:26,480
explanation. I haven't heard that one before. The image that came to me was a complicated system.

432
00:37:26,480 --> 00:37:30,640
It's like a grandfather clock, right? And a complex system is the Amazon rainforest. Yes.

433
00:37:30,640 --> 00:37:35,200
Yeah. And I just got this image of a grandfather clock sitting in the Amazon rainforest, as this

434
00:37:35,200 --> 00:37:41,200
kind of juxtaposition, but just how incredibly different they are. You know, it's really recent

435
00:37:41,200 --> 00:37:45,760
that I've become more interested in complexity. And part of it, part of what's really exciting

436
00:37:45,760 --> 00:37:50,800
me about it, as I'm on this kind of, what I feel like will be a very long journey of understanding,

437
00:37:50,800 --> 00:37:56,320
is that the sort of enlightenment project that began perhaps in the 1700s, all the way up until

438
00:37:56,320 --> 00:38:02,880
now, I would argue, was we can look at the world and figure out how it's like a grandfather clock,

439
00:38:03,440 --> 00:38:10,320
given enough time. And then as scientists progress, we've been like, okay, actually, maybe our model

440
00:38:10,320 --> 00:38:15,680
was wrong. Maybe maybe the universe isn't like a grandfather clock. Maybe it's a complex system,

441
00:38:15,680 --> 00:38:20,720
which I think is pretty much the truth that we've kind of arrived at. But what I'm seeing as I kind

442
00:38:20,800 --> 00:38:26,400
of delve into this is just how important complexity is as a framing for so many different things.

443
00:38:26,400 --> 00:38:30,080
So many of the things we've talked about on Rebel Wisdom as well, like, you know, trying to make

444
00:38:30,080 --> 00:38:34,320
sense of the information landscape, trying to look at the culture wars and figure out, okay, how the

445
00:38:34,320 --> 00:38:40,720
hell do we come into some sense of coherence with one another? How do we revive the common so we

446
00:38:40,720 --> 00:38:45,440
can actually have new type of conversation? All of those are complex problems, because they're all

447
00:38:45,440 --> 00:38:50,480
feeding back on each other. And then there's this hope of emergence, because we don't know,

448
00:38:50,480 --> 00:38:54,720
like you said, we don't know what's going to emerge from the interaction of all those different parts.

449
00:38:55,520 --> 00:39:02,480
My sense is there's something like a new religion of sorts could emerge, which would completely

450
00:39:02,480 --> 00:39:08,960
flip would be the next thing that completely flips our entire way of working and communicating with

451
00:39:08,960 --> 00:39:13,600
each other because we cannot coordinate right now. And the thing that has helped us coordinate has

452
00:39:13,600 --> 00:39:20,480
been that in the past, right? So I mean, yeah, a really appealing meme. Exactly. Something that is,

453
00:39:20,480 --> 00:39:23,680
and I think we have no, I don't think it would look anything like the religions that have come

454
00:39:23,680 --> 00:39:29,840
before. And I've argued that it's sort of brewing as we speak online, and then breaching, I've called

455
00:39:29,840 --> 00:39:33,920
it the age of breach, because things are brewing online, and then breaching into consensus reality.

456
00:39:33,920 --> 00:39:38,720
And so far, it's been quite nascent, like the like GameStop, or the capital riots, where it's like,

457
00:39:38,720 --> 00:39:41,520
oh, God, that looks like a new thing. And then it's like, well, it all collapsed, but

458
00:39:42,400 --> 00:39:49,040
who knows? Yes, there's like a boundary between this like this, this whatever is underneath

459
00:39:49,040 --> 00:39:52,240
there, and then what's in our world, and that boundary is becoming more and more

460
00:39:53,040 --> 00:39:57,040
looped in some way. Definitely. That's what you describe it. It's like a larger surface area or

461
00:39:57,040 --> 00:40:02,640
something. And, you know, things will pop through. And then, yeah, no, I mean, I mean, the religion

462
00:40:02,640 --> 00:40:11,600
topic is a whole other thing. I, I by and large agree that not only will we likely see some kind

463
00:40:11,600 --> 00:40:17,680
of new religion, and it's almost seems like people want to need that. But I think we should have

464
00:40:17,680 --> 00:40:24,000
something. And my main problem I've had with all the past religions is just they've been,

465
00:40:24,000 --> 00:40:29,440
they're just really unfun. They just buy not all of them, but by and large, they're about

466
00:40:30,320 --> 00:40:36,160
no sex, no drinking or whatever, like just they're the antithesis of partying.

467
00:40:37,600 --> 00:40:40,480
And again, like, there are some good pockets within some of them. And I think those are the

468
00:40:40,480 --> 00:40:46,720
ones that, you know, have flourished for a while and so on. But clearly, it seems to be some part

469
00:40:46,720 --> 00:40:52,640
of the human spirit we want to worship. We want to think of something bigger than us. That to me

470
00:40:52,640 --> 00:40:58,640
is some evidence that there might be something bigger than us. And but regardless, like, why,

471
00:40:58,640 --> 00:41:05,760
you know, why not play into that? Why not use that? But at least, I mean, I've been thinking

472
00:41:05,760 --> 00:41:10,240
about this for a little while, I think there's, there's value in actually brainstorming,

473
00:41:10,960 --> 00:41:14,240
collectively, what would we want if we were to design a religion, what would it look like?

474
00:41:15,360 --> 00:41:20,560
I think it's worth certainly the viewers of rebel wisdom and people thinking about like,

475
00:41:20,560 --> 00:41:24,240
just spending some time to write down five things that they would like about religion or

476
00:41:24,240 --> 00:41:27,440
something like that. And then we hive mind it and see what comes.

477
00:41:27,520 --> 00:41:32,480
Yeah, I'll join right now. Straight up. Before it even gets developed. Yeah. Yeah,

478
00:41:32,480 --> 00:41:36,080
that it's a very it's an interesting one. It touches on something we've talked about as well,

479
00:41:36,080 --> 00:41:41,840
which is this sort of this strange relationship we now have with with rationality and reason and

480
00:41:41,840 --> 00:41:48,560
our understanding of of what that is. And our conception of ourselves as rational actors when

481
00:41:48,560 --> 00:41:54,480
we're we're sort of anything but and yet we do have this capacity to take a step back and be

482
00:41:54,480 --> 00:42:00,480
reasonable. I'm fascinated by that dynamic in particular and how it's how it's showing up

483
00:42:00,480 --> 00:42:05,360
culturally, you know, this sense of on every side of the political, let's say every tribe

484
00:42:05,360 --> 00:42:10,240
in the political spectrum, there seems to be this sense of, I feel like this, therefore,

485
00:42:10,240 --> 00:42:17,920
this is truth, whether it's in sort of successor ideology of progressives right now, or whether

486
00:42:17,920 --> 00:42:24,960
it's in the the kind of rabid certainty of, you know, some people on the right about their

487
00:42:24,960 --> 00:42:32,160
their own views. There's a sense of feeling an emotion overtaking our sensemaking. And I think

488
00:42:32,160 --> 00:42:39,520
often religion has been the necessary place to pour that energy with other people as well.

489
00:42:39,520 --> 00:42:43,760
Yeah. And so I think without it, we're, I mean, we've talked about this a lot, we're totally

490
00:42:43,840 --> 00:42:49,520
adrift in a hurricane. Yes. Yeah. Yeah. So yeah, I'm I'm curious. And I think it'll be weird as hell,

491
00:42:49,520 --> 00:42:55,200
whatever, whatever emerges will be weird. Yeah. Yeah. So you actually, when we were talking before

492
00:42:55,200 --> 00:42:58,880
before this discussion, you actually introduced me to a concept I hadn't heard of before,

493
00:42:58,880 --> 00:43:04,800
which comes from, well, Scott Alexander popularized it, but it comes from an Allen Ginsburg poem

494
00:43:04,800 --> 00:43:10,480
originally. And before that is it, I think a believer Sumerian God, Canaanite God called

495
00:43:10,480 --> 00:43:16,000
Moloch, right? Yes. And I got quite excited when I came across this, and I'll talk about why in a

496
00:43:16,000 --> 00:43:20,480
bit first we should talk about what exactly it is it relates to complexity. And there's this

497
00:43:20,480 --> 00:43:25,200
other concept called Moloch. And yeah, what is it? Yes, I mean, you summarized where it's where it

498
00:43:25,200 --> 00:43:30,400
came from quite well, it was originally the, it's either the Canaanites or the Coffinogens,

499
00:43:30,400 --> 00:43:38,080
I don't know, but it was a God of war, that they supposedly sacrificed their children to

500
00:43:38,080 --> 00:43:42,560
by putting them into an oven and burning them so that Moloch would be happy and they'd win their

501
00:43:42,560 --> 00:43:49,600
wars. So really, that was dark as it gets. And then it became more popular when, when Allen

502
00:43:49,600 --> 00:43:55,600
Ginsburg wrote this amazing poem called, it's actually called Howl, talking about this thing

503
00:43:55,600 --> 00:44:01,520
that sounds sort of analogous to capitalism, making people mad. And then Scott Alexander

504
00:44:02,480 --> 00:44:08,800
really nailed it. But he wrote this unbelievable blog called Meditations on Moloch, which was

505
00:44:08,800 --> 00:44:12,640
the first time I've seen, basically, he's trying to analyze what Allen Ginsburg is talking about,

506
00:44:12,640 --> 00:44:18,080
this this sort of mechanistic thing. And what he really did for the first time was he related

507
00:44:18,080 --> 00:44:25,760
Moloch to game theory, and talked about how it seems to be this sort of the force. So again,

508
00:44:25,760 --> 00:44:30,960
if there's this force of something that's driving the, you know, emergence and complexity,

509
00:44:30,960 --> 00:44:37,760
there seems to be this opposing force, which is a force of destruction that sort of uses competition

510
00:44:37,760 --> 00:44:44,560
for ill. And, you know, the way the way I'm terming it, I'm doing a video series on this,

511
00:44:44,560 --> 00:44:51,040
and I'm terming it as basically Moloch is the God of unhealthy competition of negative sum games.

512
00:44:51,040 --> 00:44:55,920
So competitive interactions that make the world worse off for their existence, as opposed to

513
00:44:55,920 --> 00:45:02,160
being neutral or better, because games can be good or bad, you know. And, and so Moloch is kind

514
00:45:02,160 --> 00:45:08,160
of like this personification of that. But in reality, what it is is just this diet, there's

515
00:45:08,160 --> 00:45:14,480
like dumb blind force of like, evolution and economics, where basically you'll have these

516
00:45:16,640 --> 00:45:22,000
systems where individuals are incentivized to do sort of the selfish thing, kind of like a prisoner's

517
00:45:22,080 --> 00:45:28,640
dilemma, like a multi-person prisoner's dilemma, where they, everyone is individually incentivized

518
00:45:28,640 --> 00:45:35,120
to do the thing that will give them a short term gain. But if everyone does that, then everyone

519
00:45:35,120 --> 00:45:40,000
overall ends up worse off. So from, basically, from a God's eye view, everyone should do the

520
00:45:40,000 --> 00:45:44,560
cooperative thing. But in reality, because it's so hard to get so many people to coordinate,

521
00:45:45,600 --> 00:45:50,000
there's no way of enforcing it, then everyone ends up in a bad place. And that's, you know,

522
00:45:50,000 --> 00:45:56,080
it's called a multipolar trap, or a Moloch trap, as I like to call it. So yeah, that's kind of,

523
00:45:56,080 --> 00:46:01,520
it's kind of an abstract concept. But for simplicity, think of it as like the God of unhealthy,

524
00:46:01,520 --> 00:46:07,200
when competition goes wrong. Very cool. Yeah, that's a really cool explanation. And I like in

525
00:46:07,200 --> 00:46:12,480
that essay that he points out, it just takes one person to be a dickhead for in many of those

526
00:46:12,480 --> 00:46:17,200
multipolar traps, everyone's cooperating except for one person, and that can then... In the really

527
00:46:17,200 --> 00:46:22,800
bad ones, in the worst designed ones, like a good example would be like, you're at a stadium

528
00:46:24,560 --> 00:46:29,760
at a football game, and you're in a block, and everyone's sitting down at the start of the game.

529
00:46:31,040 --> 00:46:36,560
But then the team comes on, and someone at the very front gets excited and just wants a slightly,

530
00:46:36,560 --> 00:46:39,840
slightly better angle so they stand up, and then makes the person behind them stand up,

531
00:46:39,840 --> 00:46:45,840
and then the next person ends on. And everyone has to stand up now. And they're just stuck there,

532
00:46:46,480 --> 00:46:52,400
basically the system has fallen into this lower state where, yes, sure, you could quit the game

533
00:46:52,400 --> 00:46:56,240
by being like, I refuse to stand, I'm going to sit down, but now you don't get to see. So

534
00:46:56,800 --> 00:47:02,720
either way, you don't have a better strategy than the current one, which is now standing up.

535
00:47:02,720 --> 00:47:07,200
But if you could poll everybody, they would much rather everyone be sitting and have the

536
00:47:07,200 --> 00:47:14,480
same view, effectively. So yeah, that's just one example of where a poorly designed system,

537
00:47:15,120 --> 00:47:19,360
due to competitive dynamics, can have this like cascading effect where everyone ends up

538
00:47:19,360 --> 00:47:21,760
in this annoying situation where they'd rather not be.

539
00:47:22,560 --> 00:47:26,400
Yeah, and I think what's, I really love the concept, because I think one of the things that's

540
00:47:26,400 --> 00:47:33,600
useful about it is that in the various circles who are interested in changing the world in some way,

541
00:47:34,240 --> 00:47:39,920
changing the system, finding ways to create a better system, which is something I really also kind

542
00:47:40,000 --> 00:47:46,160
of identify with and I really care about a lot, there is often very little, it's often very meta,

543
00:47:46,160 --> 00:47:53,120
it's often like, it gets very zoomed out, and doesn't necessarily look at the forces acting

544
00:47:53,120 --> 00:47:57,520
again. Sometimes it does, sometimes it doesn't, right? And there's something about the concept.

545
00:47:57,520 --> 00:48:01,040
So I got really excited when you, sorry, when you introduced me to the concept, because I looked at

546
00:48:01,040 --> 00:48:06,880
the, you described it a bit, I read the Scott Alexander essay, and then I, for me, there was a

547
00:48:06,880 --> 00:48:10,960
lot of crossover with the model I'm really familiar with from, from mysticism, from the

548
00:48:10,960 --> 00:48:16,880
Gnostic Gospels. So the early Christians, well, arguably they weren't really that Christian, but

549
00:48:16,880 --> 00:48:20,640
some of them were a little bit more of kind of Hellenistic mystery tradition. So we're talking

550
00:48:20,640 --> 00:48:30,000
kind of maybe the third century, you know, second and third century. And they had this incredibly

551
00:48:30,000 --> 00:48:35,920
sophisticated model of, I would say a model of human psychology, of the human psyche. And they had a

552
00:48:36,000 --> 00:48:44,480
creation myth in which the earth is a living goddess, and the god of the Old Testament, Yahweh,

553
00:48:44,480 --> 00:48:50,160
is this kind of, this false god, which is just pure ego, right? And he kind of, he gets created

554
00:48:50,880 --> 00:48:55,840
by mistake by her. It's kind of complex why, but he's, they describe him as like an abortion,

555
00:48:55,840 --> 00:49:02,160
like he's, so she is a goddess and has the all the kind of divine creativity of a goddess, right?

556
00:49:02,160 --> 00:49:06,960
She's tapped into the sort of universal source of emergence, let's call it. And he comes along

557
00:49:06,960 --> 00:49:10,720
and he says, well, I'm the god of everything there is. And she's like, well, no, you're not,

558
00:49:10,720 --> 00:49:14,640
because you're not, you're not, you're almost like a cracked reflection. You're not even real,

559
00:49:14,640 --> 00:49:19,280
really. And it's interesting because the neuroscience of our egos has been argued that

560
00:49:20,000 --> 00:49:28,080
the, the narrating eye, our egos looking out for our own ends is a kind of conglomeration of many

561
00:49:28,080 --> 00:49:34,160
different aspects of our brain. And we kind of cobbled together a self from our essence. And so

562
00:49:34,160 --> 00:49:40,880
in a way, yeah, yeah, exactly. Yeah. And that's exactly a lower complexity self. And in a way,

563
00:49:40,880 --> 00:49:45,040
it's real, but it's not real at the same time. So you have this dynamic there.

564
00:49:45,920 --> 00:49:52,240
And then they argued that when we, so human beings have that divine intentionality and

565
00:49:52,240 --> 00:49:58,960
that divine creativity. And also, though, we are very easily diluted by Yahweh and very easily

566
00:49:58,960 --> 00:50:05,040
diluted by the Archons, his sort of weird, mech, super weird cosmology they had, but like this

567
00:50:05,040 --> 00:50:10,800
kind of mechanistic self replicating almost like machine angels, right, really out there.

568
00:50:11,760 --> 00:50:17,600
So, so for the Gnostics, their argument was like, look, we see Yahweh or we say see Moloch and,

569
00:50:17,600 --> 00:50:21,920
you know, it's very similar in the systems that we're part of, we see it in the Romans,

570
00:50:21,920 --> 00:50:26,960
we see it in the tax collectors, we see it in the way that people replace the kind of

571
00:50:26,960 --> 00:50:31,440
hierarchies of nature, the natural hierarchies with fake human made hierarchies of society.

572
00:50:32,480 --> 00:50:36,560
And so they were sort of outsiders. And their argument was you have to tap into

573
00:50:37,280 --> 00:50:43,520
the deeper spiritual wisdom inside yourself, the Gnosis, in order to kind of liberate, they were

574
00:50:43,520 --> 00:50:47,760
like quite, you know, into like liberating yourself kind of quite psychedelic as well.

575
00:50:47,840 --> 00:50:53,280
And Carl Jung was a huge, like fan of the Gnostics, right? The two of the codices are called the

576
00:50:53,280 --> 00:50:57,680
Jung codices, because he bought them basically, he found them after they were discovered, the

577
00:50:57,680 --> 00:51:01,600
scrolls were discovered in a cave in the 1950s by two Egyptian farmers. It's crazy.

578
00:51:01,600 --> 00:51:06,160
This is amazing. That's a crazy cool story, yeah. And then the final bit of it, for me the

579
00:51:06,160 --> 00:51:12,960
synchronicity was that there's a book on Jung by Peter Kingsley, he's a classical scholar called,

580
00:51:13,040 --> 00:51:18,320
it's kind of, he's also kind of a mystic, which is called Katafalk. And he talks a lot in that

581
00:51:18,320 --> 00:51:24,480
book about this biblical, this kind of mystical tradition that he argues Jung was very familiar

582
00:51:24,480 --> 00:51:31,680
with, which is about prophecy. And the prophecy, a part of knowing who the prophet was was that

583
00:51:31,680 --> 00:51:36,240
they howled, right, which is the name of the Ginsburg poem. So I was like, oh, very cool.

584
00:51:36,240 --> 00:51:40,640
There's a lot, I know, right? There's a lot of, there's a lot of crossovers like everywhere.

585
00:51:40,640 --> 00:51:47,360
Yeah. And what I love about that is like, for me, I look at it like, I think they were, and Jung

586
00:51:47,360 --> 00:51:52,720
thought they were incredibly sophisticated, mappers of the conscious of human consciousness.

587
00:51:52,720 --> 00:51:56,640
But there's also some aspect to it, which can't be quite explained for me, which is that

588
00:51:57,360 --> 00:52:04,800
there is this, it does seem there is this entropic force that humans create that we create

589
00:52:04,800 --> 00:52:09,360
from our own, I don't know what your take on it is, is it do we create it from our own

590
00:52:09,360 --> 00:52:14,240
bad incentives that are built into us or? I don't think humans in particular create it.

591
00:52:14,240 --> 00:52:21,440
I think it's just, I mean, again, depending on sort of the mood of the day, like it almost

592
00:52:21,440 --> 00:52:28,240
feels like it's just like there are these two opposing types of forces going on within the

593
00:52:28,240 --> 00:52:35,360
universe, like deeply metaphysical forces, where, you know, it's not that it is entropy,

594
00:52:36,000 --> 00:52:40,800
but it's like it uses entropy to its advantage. It's basically a thing trying to,

595
00:52:43,440 --> 00:52:47,440
I don't know, but like, Mike is like, the way, the way I envision it, it's just like, you know,

596
00:52:47,440 --> 00:52:52,480
that feeling where you are playing something and you want to win so badly. And I know this,

597
00:52:52,480 --> 00:52:57,840
because I had this like pathologic, I was pathologically competitive as a kid. And it's

598
00:52:57,840 --> 00:53:04,160
like eyes on the prize, but to this like all consuming extent, and you can't see anything

599
00:53:04,160 --> 00:53:09,280
else, you can't see the externalities of what you do. And all you care about is this like,

600
00:53:10,080 --> 00:53:18,720
optimizing for this one narrow reward. And the byproduct of that is that if you play that out

601
00:53:18,720 --> 00:53:22,720
to its logical conclusion, it means that you will turn basically the everything into the

602
00:53:22,720 --> 00:53:26,400
universe into this one thing. So the ultimate instantiation of that is actually like the

603
00:53:26,400 --> 00:53:30,000
paperclip maximizer, right? Which is why when you mentioned the Gnostics talking about this

604
00:53:30,000 --> 00:53:32,720
like mechanization of all these many, many things of like the same thing.

605
00:53:33,600 --> 00:53:35,920
Could you describe what that is in case that someone isn't familiar with it?

606
00:53:35,920 --> 00:53:40,880
Yeah. So the paperclip maximizer is this, this thought experiment, I think by Eliezer Yudkowski

607
00:53:40,880 --> 00:53:47,760
of like, a way in a super intelligent AI could go wrong, whereby it's super intelligent in that

608
00:53:47,760 --> 00:53:55,200
it's unbelievably good at getting whatever it wants done done. But it's stupid into the extent that

609
00:53:55,200 --> 00:53:59,200
it was basically programmed to do this one narrow thing, which in this instance, you wanted to

610
00:53:59,280 --> 00:54:03,520
make paperclips. I just wanted my AI to make some more paperclips better than what I can

611
00:54:03,520 --> 00:54:08,880
currently do. I'm a paperclip maker. But because it's so unbelievably good, it turns everything

612
00:54:08,880 --> 00:54:13,760
from, you know, the factory it's in, it figures out how to pull the constituent parts of the

613
00:54:13,760 --> 00:54:18,960
atoms, the blood, you know, the hemoglobin in your blood, the iron extracts it and, you know,

614
00:54:18,960 --> 00:54:22,960
dismantles everything until it can tie all the entire universe, anything it can in the universe

615
00:54:22,960 --> 00:54:30,160
into paperclips. Why? Because basically, it was so laser focused eyes on the prize, winning

616
00:54:30,160 --> 00:54:35,120
the goal of making as many paperclips, that you end up dismantling the universe into this

617
00:54:35,680 --> 00:54:42,880
very low complexity state. So it's effectively like kind of a, it's analogous a little bit to,

618
00:54:42,880 --> 00:54:47,440
you know, the heat death of the universe. Because what is that? It's actually a very low complexity

619
00:54:47,440 --> 00:54:52,720
state. Where basically, it's just like, how would you describe it? Well, it's just homogenous

620
00:54:52,720 --> 00:54:58,320
gray soup. You know, that's all it is. The universe started out very low complexity in that it was

621
00:54:58,320 --> 00:55:04,000
kind of like this singularity of matter and energy. But you know, if we're talking in terms of

622
00:55:04,000 --> 00:55:10,000
Korma-Garov complexity, which is like basically how many, how many bits of code do you need to

623
00:55:10,000 --> 00:55:16,880
describe a thing? It started at the universe, started out pretty simple. And then time started

624
00:55:17,280 --> 00:55:21,280
and things started unfolding. And suddenly we, you know, we've started seeing hydrogen and

625
00:55:21,280 --> 00:55:26,560
then helium. And that would coalesce into stars, which created greater, heavier elements. And all

626
00:55:26,560 --> 00:55:31,440
this beautiful complexity started emerging, patternicity. There's like dance between order and

627
00:55:31,440 --> 00:55:36,960
disorder, you know, with like a bit of hierarchy, but a bit of anarchy and so on that creates this

628
00:55:36,960 --> 00:55:44,480
like highly complex dynamic system that's very hard to describe. Like to write the piece of code,

629
00:55:44,560 --> 00:55:49,840
to describe the universe, you basically have to just create the universe. That's what a highly

630
00:55:49,840 --> 00:55:57,360
complex system is. And, but at some point, the stars will die out and so on, all this sort of

631
00:55:57,360 --> 00:56:01,920
free energy that is used to create all this complexity will start dissipating. And then it'll

632
00:56:01,920 --> 00:56:08,560
slowly, as far as we know, turn into this gray soup, which is low complexity. So it'll do this,

633
00:56:09,120 --> 00:56:20,000
entropy will do this over time. And so, yeah, so this like this gray soup is kind of analogous to,

634
00:56:20,000 --> 00:56:25,280
in my opinion, like what a paperclip maximizer would would do. It's not a very, basically,

635
00:56:25,280 --> 00:56:30,160
it's permanently curtailing, you know, there's no more complexity to to arise. The universe has

636
00:56:30,160 --> 00:56:38,160
reached this steady state. And that is, you know, it seems like a tragedy on enormous potential

637
00:56:38,240 --> 00:56:43,040
because at least up until now, it seems like the universe is trying to emerge into greater and

638
00:56:43,040 --> 00:56:47,760
greater states of complexity. So I hate to boil it down to like good and evil terms. But

639
00:56:48,960 --> 00:56:56,000
to me, good is that which creates, you know, allows for greater emergence and complexity

640
00:56:56,000 --> 00:57:01,600
to appear and thereby utility, you know, useful information that we can process and do and make

641
00:57:01,600 --> 00:57:07,840
wonderful things with. And evil is that which does the opposite. In other words, like turns

642
00:57:07,840 --> 00:57:14,640
things into this like low diversity, low, like high, you know, like very basic situation,

643
00:57:14,640 --> 00:57:19,440
whether it's, you know, a cloud of hydrogen, which in the mountain, the, the, the howl poem he

644
00:57:20,320 --> 00:57:26,720
and Ginsberg describes Moloch as Moloch, who is a cloud of sex as hydrogen. So it's like this,

645
00:57:26,720 --> 00:57:30,400
yeah, it's like this kind of a force of entropy, but slightly different because entropy is actually

646
00:57:30,400 --> 00:57:36,000
just kind of neutral. Entropy is just like time effectively. Whereas Moloch is a thing that turns

647
00:57:36,000 --> 00:57:41,680
everything into this like one mono focus of sacrificing everything in order to win this one

648
00:57:41,680 --> 00:57:47,520
thing. Hence the like child sacrifice. Brilliant. Yeah. It's so I think it's a really, really

649
00:57:48,320 --> 00:57:52,480
excited to introduce this model into kind of, you know, into the channel and like our thinking

650
00:57:52,480 --> 00:57:57,280
in general, because I think there's, there's something I don't quite know what, and I think

651
00:57:57,280 --> 00:58:00,560
this is the journey you're on, right? Like there's something in it, right? There's something in it.

652
00:58:00,560 --> 00:58:05,120
Well, so the current thing I'm trying to figure out right now. And I mean, a lot of this again is

653
00:58:05,120 --> 00:58:10,000
like a sort of an offshoot from conversations with Daniel Schmacktenberger, Forest Landry as well.

654
00:58:10,960 --> 00:58:19,440
But I don't think so like Moloch is tied to the competition, right? But competition gone wrong.

655
00:58:20,000 --> 00:58:24,800
But competition can also be an enormous force for good. Because actually, like,

656
00:58:26,080 --> 00:58:30,560
you know, the capitalistic model has risen the world to what it is right now. Like we would not

657
00:58:30,560 --> 00:58:36,880
be living the cushy life with our, you know, nice cameras in this cool room and so on without the,

658
00:58:37,760 --> 00:58:43,840
a lot of the luxuries that capitalism has provided. And in its best form, capitalism is,

659
00:58:43,840 --> 00:58:48,000
you know, it's like a positive sum game. It's using competition in order to like

660
00:58:48,000 --> 00:58:54,080
drive progress faster than it would without and creating these novel things that arguably wouldn't

661
00:58:54,080 --> 00:59:00,720
exist without it. So it seems to be that there's like some kind of optimal

662
00:59:02,000 --> 00:59:07,920
amount of competition, like zero sum type competition that we would want within the

663
00:59:07,920 --> 00:59:12,480
universe, but kind of just like encapsulated in little pockets, you know, constrained in certain

664
00:59:12,480 --> 00:59:18,720
ways, whereby Moloch can't get his dirty claws into it and like turn it, turn the whole, you know,

665
00:59:18,720 --> 00:59:25,280
twist it into something bad. And again, like we, because a lot of, you know, a lot of people

666
00:59:25,280 --> 00:59:30,400
are like, no, we just need pure, pure cooperation. Pure cooperation is all we ever want. No competition

667
00:59:30,400 --> 00:59:36,320
whatsoever. But that's a lower complexity state than a universe that has mostly competition,

668
00:59:36,320 --> 00:59:40,480
as I mostly cooperation, with little pockets of competition driving, driving it forward,

669
00:59:40,480 --> 00:59:47,120
but in just like delicately constrained ways. So I'm trying to, it is very much just a working,

670
00:59:47,120 --> 00:59:51,840
you know, a thing I'm just playing with, but there seems like there's something to that.

671
00:59:52,640 --> 00:59:59,680
It's like a higher order level of complexity where you're using games in a beneficial way

672
00:59:59,680 --> 01:00:03,840
without letting them get too out of control. Yeah, it's very interesting. I mean, what comes up

673
01:00:03,840 --> 01:00:11,680
for me is something around intention and values, right, on the individual level, because so to

674
01:00:12,640 --> 01:00:22,160
to use that Gnostic model again, another aspect of it was that the the issue is the ego disconnected

675
01:00:22,160 --> 01:00:28,320
from the self in Jungian terms. So we all have an ego, we need one. That's great. I consider myself

676
01:00:28,320 --> 01:00:36,800
very competitive, like, but there's also a question of priorities. And would I be competitive at the

677
01:00:36,880 --> 01:00:43,280
expense of, say someone else's well being, or would I decide to be competitive instead of

678
01:00:44,480 --> 01:00:48,000
loving my wife, right, if it was the trade off, which one of those priority, which one of those

679
01:00:48,000 --> 01:00:52,400
are hiring my value system. So there's something about that going on, I think, which then applies

680
01:00:52,400 --> 01:00:57,360
right back to the game theory examples, because in Scott Alexander's essay, you know, he ends each

681
01:00:57,360 --> 01:01:02,560
of those examples with like, but from a God's eye view, if we all cooperated, better outcomes for

682
01:01:02,560 --> 01:01:08,480
everyone. And what I find interesting about, yeah, so that Gnostic model saw saw the ego

683
01:01:08,480 --> 01:01:14,880
disconnected from the self as unable to create anything truly novel, because all it can do

684
01:01:14,880 --> 01:01:21,680
is replicate. And what it does, so we have that God and we have Mother Earth Gaia, or Sophia,

685
01:01:21,680 --> 01:01:27,360
they called her. So that's kind of divine creativity that each human has. But when the ego

686
01:01:27,360 --> 01:01:32,000
is disconnected from the self, it can't do anything except mimic that divine creativity. So and what

687
01:01:32,000 --> 01:01:38,480
it ends up doing is creating like a Disneyland reality that looks real, but isn't real. Nothing's

688
01:01:38,480 --> 01:01:43,280
authentic. Everything is skin deep. Think of kind of reality TV, for example, there's so many aspects

689
01:01:43,280 --> 01:01:48,800
of our culture. I read a great essay, which I'll put in the show, it's about solar punk and the

690
01:01:48,800 --> 01:01:54,160
author talked talked about, we've run out of authentic things to frack from our cultures, and

691
01:01:54,160 --> 01:01:58,480
now we're fracking our own future, you know, fracking our idea of the future just to get some

692
01:01:58,960 --> 01:02:06,160
that extractive thing. And then I guess it also goes to the idea of sociopathy and psychopathy

693
01:02:06,160 --> 01:02:11,280
and Machiavellianism, which is this kind of dark triad in psychology. And, you know, I've

694
01:02:11,280 --> 01:02:16,560
heard Daniel Schmackenberg and many others in this space talking about that being a major problem

695
01:02:16,560 --> 01:02:20,080
when you're trying to create cohesion and cooperation is like the sociopath problem,

696
01:02:20,080 --> 01:02:26,960
because a sociopath or a narcissist looks like they care about other people, but they do not

697
01:02:26,960 --> 01:02:33,360
care at all. They don't have empathy, but they're mimicking empathy. And it is insanely destructive

698
01:02:33,360 --> 01:02:39,440
because how do we cooperate when that's in the mix? I don't even know if it's that they don't

699
01:02:39,440 --> 01:02:43,360
have empathy. I think often psychopaths do have empathy, because empathy is just being able to

700
01:02:43,360 --> 01:02:47,520
put yourself and like, you know, like feel what someone else is feeling. And a truly powerful

701
01:02:47,520 --> 01:02:51,360
psychopath is someone who would be able to actually really feel what you're feeling, but then just

702
01:02:51,360 --> 01:02:55,760
not actually give a shit about what happens to you. And like, they're doing something that will

703
01:02:55,760 --> 01:03:04,400
be ultimately self-serving at the expense of you. But yeah, that's probably a good distinction to make.

704
01:03:05,440 --> 01:03:10,560
And an unsolved problem, I think, in terms of, okay, we want to find new ways to cooperate,

705
01:03:10,560 --> 01:03:15,520
we want to find new ways to kind of build a sustainable future. How do we deal with that

706
01:03:15,520 --> 01:03:20,160
problem? I mean, it's a really, on the game theory level, it's fascinating. I'm just on the kind of

707
01:03:20,160 --> 01:03:25,280
like being a human being level. It's such a big question. I mean, yeah, it's about building robust

708
01:03:25,280 --> 01:03:30,640
systems whereby a few defectors, you know, of people playing this multi-person prisoner's

709
01:03:30,640 --> 01:03:35,440
dilemma, where the system can sustain, you know, having a few people do it, because I think it's

710
01:03:35,440 --> 01:03:41,200
going to be next to impossible to ever have everyone working purely cooperatively, purely like,

711
01:03:42,080 --> 01:03:45,600
and I'm speaking from really a personal level here, because I was on a game show, which was the

712
01:03:45,600 --> 01:03:49,840
prisoner's dilemma to an extent. And it was, you know, it was a one time one. And I did the,

713
01:03:50,320 --> 01:03:56,400
you know, I defected. I did the selfish thing by, you know, doing the clearly dominant strategy

714
01:03:56,400 --> 01:03:59,760
in terms of winning money, which was my goal on the thing. I went in and I was like, I need to win

715
01:03:59,760 --> 01:04:05,200
money. So I did the selfish thing. But man, like, I'm still getting hate messages from that today,

716
01:04:05,200 --> 01:04:10,400
like 14 years later. What was the game show? It's called Golden Balls. It's on YouTube. I hate it.

717
01:04:11,360 --> 01:04:20,000
And you're worth watching. Did you win? I won. Yeah. I mean, I played it. I played the game

718
01:04:20,000 --> 01:04:25,040
perfectly from a, if you're optimizing for the value of winning money. But that's again,

719
01:04:25,040 --> 01:04:31,840
it's this bigger question of like, what are the externalities to this? Overall, you know, I was

720
01:04:31,840 --> 01:04:37,280
doing the sort of the, yeah, I was doing the strategically, I picked the game theory optimal

721
01:04:37,280 --> 01:04:41,760
solution within the definitions of the game. But outside of that, there's other externalities

722
01:04:41,760 --> 01:04:45,280
about the value of cooperation, the value of not looking someone in the eye and saying,

723
01:04:45,280 --> 01:04:48,800
yeah, I'm going to split this with you. And then actually stealing it, you know, and

724
01:04:50,560 --> 01:04:55,040
and it taught me like a really important lesson of thinking about the externalities of whatever

725
01:04:55,040 --> 01:05:00,320
it is we do. And so, but what we need really is to build a system that is robust enough,

726
01:05:00,960 --> 01:05:07,760
complex enough, but in the right way, where it minimized where the amount of externalities

727
01:05:07,760 --> 01:05:13,280
any one individual can do, negative externalities, any one individual can do are contained and

728
01:05:13,280 --> 01:05:19,200
constrained. And I don't know how we do that, but we have to find a way. Yeah, so it's a really

729
01:05:19,200 --> 01:05:25,040
good point. I think an example I've used before is the, how the US Constitution was made, because

730
01:05:25,120 --> 01:05:31,040
they did a quite good job. Obviously, not perfect, but quite good job of thinking. Yeah,

731
01:05:31,040 --> 01:05:35,360
really credit at the time of creating an antifragile system with that in mind, because they were

732
01:05:35,360 --> 01:05:40,320
focused on how do you prevent tyranny. And the process whereby they went through it was,

733
01:05:40,320 --> 01:05:43,760
you know, in part the federalist papers, we're just writing to each other constantly being like,

734
01:05:43,760 --> 01:05:47,760
well, what if someone does this, like, oh, shit, yeah, what if someone does that? Yeah,

735
01:05:47,760 --> 01:05:52,080
stress testing constantly. And I think, I mean, that's probably a good place for us all to start

736
01:05:52,080 --> 01:05:56,560
is just to kind of, yeah, start having that conversation, which I think a lot of people

737
01:05:56,560 --> 01:06:00,720
already having and start to kind of stress testing. And now we have the luxury of being

738
01:06:00,720 --> 01:06:05,120
able to actually make models and play them out. Like, we can, you know, there's like,

739
01:06:05,120 --> 01:06:09,120
I think, you know, game designers, et cetera, a lot of people out there who can go, okay, well,

740
01:06:09,120 --> 01:06:13,600
let's let's put it in a simulation and see what happens, you know, that's really exciting to me.

741
01:06:13,600 --> 01:06:19,360
Yeah. Yeah. No, I mean, that's exactly as they're, the blessing and curse of technology is that

742
01:06:19,440 --> 01:06:26,480
technology is particularly exponential ones are making Moloch's life much easier to,

743
01:06:27,360 --> 01:06:32,400
to destroy the universe. But at the same time, we're also building technologies that

744
01:06:32,400 --> 01:06:36,400
enable cooperation better coordinate, you know, the ability to coordinate with one another.

745
01:06:37,280 --> 01:06:41,200
You know, like a simple internet forum, it's just such a valuable thing that like just never

746
01:06:41,200 --> 01:06:46,720
existed before. You know, it's not only because it's a way of sharing information across multiple

747
01:06:46,720 --> 01:06:51,920
minds, across time, not just in the moment, but also storing it so that new people can come in

748
01:06:51,920 --> 01:06:56,400
and retroactively go back and learn and then like, and now add something new and so on.

749
01:06:56,400 --> 01:07:00,560
So when we were prepping this interview, you mentioned like quite a few different things

750
01:07:00,560 --> 01:07:04,560
you're doing, which sound very exciting. And I think our viewers will be quite interested in.

751
01:07:04,560 --> 01:07:08,000
So, and you also have a YouTube channel, which I think is where they're going to be coming.

752
01:07:08,000 --> 01:07:12,720
Yes. And as a reminder, you're appearing and our state of sense making free event at the end of

753
01:07:12,720 --> 01:07:16,880
September. So people can check you out there. But we'll put your YouTube info below in the

754
01:07:16,880 --> 01:07:20,080
show notes and maybe you could tell us a little bit about like what's coming up, like what you're

755
01:07:20,080 --> 01:07:26,880
working on. Yeah. I've been down the rabbit hole on this like Molek concept and to an extent complexity.

756
01:07:27,680 --> 01:07:33,920
So yeah, making a series, trying to pick apart what Molek is and also bring it to life. I do

757
01:07:33,920 --> 01:07:39,280
some acting in it for the first time, which is weird. I dress up. It's a whole thing. But

758
01:07:40,240 --> 01:07:46,000
a lot of it relates very tightly to what we've talked about today. So yeah, people enjoyed

759
01:07:46,000 --> 01:07:49,600
this, then they should check it out. Yeah, I'm really looking forward to that. So yeah, subscribe

760
01:07:49,600 --> 01:07:55,280
to live. We'll put the your different social handles down the show notes and thank you so much

761
01:07:55,280 --> 01:07:59,840
for coming on. Thanks for having me. This is awesome. Our ability to make sense of the world

762
01:07:59,840 --> 01:08:05,280
is breaking down. We're making more and more consequential choices with worse and worse sense

763
01:08:05,280 --> 01:08:12,000
making to inform those choices, which is kind of running increasingly fast through the woods

764
01:08:12,000 --> 01:08:17,280
increasingly blind. Over the last two years, Rebel Wisdom has interviewed some of the world's top

765
01:08:17,280 --> 01:08:24,480
thinkers. Now we've brought them together for an eight week online course, Sense Making 101,

766
01:08:25,120 --> 01:08:32,720
with Daniel Schmacktenberger, Diane Mushow-Hamilton, John Vivecchi, and more. Improve your sense making,

767
01:08:32,720 --> 01:08:40,960
develop your sovereignty, and join a wider community looking to do the same.

