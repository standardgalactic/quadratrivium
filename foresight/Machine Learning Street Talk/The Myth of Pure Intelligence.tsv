start	end	text
0	2280	and I should look into your eyes.
2280	4320	A bit of a bit of a combo.
4320	7680	So yeah, like that camera vaguely in my direction.
7680	9120	I don't think it matters.
9120	10880	OK, just don't look down.
10880	15600	OK, don't look down because of the glasses and the light.
15600	18600	Oh, yeah, let me check that.
18600	20840	Tiny bit sort of, but when I say source,
20840	24360	the only light source.
24360	28120	So I'm director of product for R&D Adversus.
28200	32560	And I'm also a student in PhD for cognitive computing
32560	35560	at the University of Quebec in Montreal.
35560	36760	Fantastic.
36760	39000	So what have we just been discussing?
39000	41760	God, we've been discussing a lot of things.
41760	44960	I think we discussed panpsychism most recently.
44960	46720	We discussed computationalism.
46720	51080	We discussed whether we should be focused on materiality
51080	53400	or whether it's about the processes themselves,
53400	57360	the hard problem, IIT, lots of things, lots of things.
57400	58880	Yeah, so why don't we start with the beginning?
58880	60480	So that's quite a good segue, actually.
60480	64880	So recently there was this letter of opposition
64880	67600	around integrated information theory.
67600	70960	What's your take on that?
70960	73360	I think the reaction was to be expected.
73360	75840	I think you can't ask people that aren't
75840	78840	involved in a process which is aimed at determining
78840	83520	whether they are right or wrong just to accept
83520	86520	the outcomes of something in which they had no say.
86520	87120	So there's that.
87120	88440	It's like saying, well, you're going
88440	92560	to have to accept the outcome of elections, but you can't vote.
92560	97440	I think one of the issues is the people who created the study
97440	100520	determined a process that left too much room, too much wiggle
100520	102560	room for interpretation and critique,
102560	104400	which means there's a problem in the study design
104400	106320	to begin with.
106320	110720	I think when you start attacking through adversarial means
110720	114720	a field, it is highly likely the field is going to retaliate.
114760	119880	So this open letter, I think, was a retaliatory move.
119880	123040	I think it was a very strong response
123040	127840	to perhaps coverage that should have been more nuanced.
127840	129960	I definitely don't think IIT is the leading
129960	131520	consciousness theory.
131520	132960	I think it's an interesting one.
132960	134160	I think it has promise.
134160	136400	I think it's definitely formal.
136400	137880	There are many others.
137880	141080	I think this one was just one of the ones that was selected
141080	142560	in the Templeton study.
142560	146680	And so I think we need to be more careful about how we comport
146680	149040	ourselves towards other scientists.
149040	153280	When we call an entire field pseudoscience,
153280	154920	I don't think it helps.
154920	157440	I think we can critique the parts of the field
157440	162640	we think are egregious or problematic.
162640	166040	I think that's helpful because it builds towards something
166040	169560	rather than just destroying something for which a lot of people
169560	171640	have given their lives.
172280	175040	Not through death, but they've given a lot of their time.
175040	176040	They've studied.
176040	180040	They've understood what would constitute the right kind
180040	181840	of methodology, and they've applied it.
181840	183680	So I don't think we can just disqualify it
183680	186560	as something that has no value offhand.
186560	188720	Now, it's not to say that they necessarily all
188720	194280	had an offhand opinion, but the way it was presented
194280	200240	basically suggested because of tabloid style reporting,
200760	202600	we're leaning towards something dangerous,
202600	206800	and to do so, they used tabloid-styled methods.
206800	208840	That's my take.
208840	211360	Do you think there is any bright line between what
211360	213840	is legitimate science and what isn't?
213840	215680	I think there might be, but I don't think
215680	217360	it's where people think it is.
217360	224400	I think, for instance, some people called science something
224400	225760	that used tools.
225760	228800	And I think, to me, that's insufficient to qualify as science.
228800	233640	So it's not because you use a little pinny machine
233640	235760	to go like this on somebody's head,
235760	238160	that phrenology is effectively a science.
238160	239280	That's not.
239280	243640	I think you need to measure invariance
243640	246160	and measure how those invariance allow you
246160	247680	some degree of predictability.
247680	253840	To me, that's the essence of science.
253880	258560	But I also think that some things that people
258560	262640	can discard as evidence still can qualify as evidence.
262640	265800	To me, phenomenology can become a part of evidence.
265800	269480	To me, intersectional data, which
269480	272320	means it's not necessarily data that will easily reproduce,
272320	276600	can qualify as data under the caveats,
276600	278800	that it is not something that would reproduce,
278800	281680	that it is something that is specific to a perspective
281680	284440	and therefore cannot qualify as a generalizer.
284440	288000	I think a lot of qualitative methods
288000	292280	have developed entire frameworks to explain how
292280	295280	to interpret what they measured as data.
295280	298360	And oftentimes, because people in the STEM fields
298360	301960	are not aware of how this interpretation is
301960	305200	meant to be taken, they just see something which
305200	306520	doesn't seem very systematic.
306520	308080	And they're like, well, that's not data.
308080	311440	I was like, well, OK, to your measurement,
311440	312480	that's not data.
312480	314200	Because there is no way for what you're
314200	317880	using to interpret that in any sort of systematized way.
317880	318880	Sure.
318880	323880	But for someone who understands how to generate that data,
323880	327160	how to potentially return to the space which
327160	332160	has similar partitions, and how to turn this
332160	335160	into something which becomes a little more systematic,
335160	338560	like thematic analysis.
338600	341960	And let's say, interjudge agreement.
341960	345000	Then we get to something that resembles systematicity,
345000	347600	and therefore we can derive patterns from there.
347600	351720	So I think the line is not as bright as people would like
351720	353680	to think, but it does exist.
353680	357280	Yeah, I like your notion of systematicity.
357280	359640	Because I suppose you could just argue
359640	362160	on some kind of just basic utility.
362160	363800	You mentioned phrenology.
363800	367120	And there was this horrific gaydar paper a few years ago.
367120	370000	I don't know if you remember, where they ingested
370000	372680	a whole bunch of images from dating websites.
372680	376000	And they trained it to predict whether you were gay or not.
376000	378200	And it was from Stanford.
378200	380600	And the same Stanford researcher recently
380600	384520	published a paper about theory of mind and language models.
384520	385880	That's an interesting aside.
385880	388560	But anyway, that's something which is ethically ridiculous.
388560	391120	And there are all sorts of confounding factors and so on.
391120	393600	But they might argue, well, this is science
393600	395080	because it has predictive power.
397400	399520	Well, what part of it was science?
399520	401320	Like, what model did it allow us?
401320	402960	What understanding did it allow us?
402960	407640	Did the AI have predictive power, or did we?
407640	409680	And under what circumstances?
409680	412400	Like, did it predict very accurately
412400	413680	whether someone was gay or not?
413680	414280	I don't know.
414280	416400	But you could tell me whether, yes, it did or not.
416400	418280	But what does that tell us about the world?
418280	421320	Does it tell us that there's a context under which certain sets
421320	424800	of features are common for a given population?
424840	427240	Possibly, but I still don't know what those features are.
427240	428960	It didn't really help me.
428960	432840	And then the idea that we can turn certain kinds of studies
432840	437600	in practice, I think, is not beyond the scope of science.
437600	438520	That's not really it.
438520	443920	But I think it requires the scientists
443920	446800	to take a stance before the scientific experiment
446800	449600	to say, these are the outcomes I believe
449600	451600	should be put out into the world.
451600	455880	And therefore, this is what I choose to reify with my science.
455880	457920	And also, the kinds of methods I'm
457920	460800	going to use are themselves anchored
460800	462920	in some kind of contextuality, right?
462920	466960	Like, the idea of breaking apart the world
466960	470200	into certain kinds of categories which are laden.
470200	472800	And I'm going to measure those categories
472800	475000	with certain kinds of things, like IQ tests.
475000	478280	It's quite controversial, right?
478280	480800	It's like, do they measure intelligence?
480800	484040	Or do they measure a certain kind of aptitude
484040	486680	given a certain kind of context?
486680	488920	Like, maybe?
488920	492600	And also, what do we mean when we say intelligence?
492600	495160	Like, are we saying some people, therefore, are not,
495160	498440	and therefore, X, Y, Z?
498440	503760	So I think that doesn't make it phrenology, though.
503760	506040	That still could become science.
506040	509080	And in the case of the AI model,
509080	512120	if it had accuracy given that, it probably did.
512120	518160	But I don't think that tells me anything about a causal factor
518160	521360	that I could then derive and build upon, right?
521360	523680	Like, it doesn't really help.
523680	524720	Yeah, no, it's interesting.
524720	527880	I mean, the whole endeavor of science
527880	529920	is something that fascinates me.
529920	532240	And once you get past the question, of course, of, you know,
532240	534160	well, what makes good science?
534160	537400	Chomsky, by the way, says that language models are not
537400	541360	the theory of language, because they basically, you know,
541360	542720	they can recognize anything.
542720	544640	You can just put noise into a language model
544640	545640	and it will recognize it.
545640	549000	So it doesn't really say anything about the why
549000	551840	and the what would happen if not question.
551840	555080	It just, you know, it just models absolutely everything.
555080	558760	So science needs to kind of create clear dividing lines
558760	561040	between what things are and what things are not
561040	562000	and why they happen.
563000	566800	Yeah, so I think I just think we have to be careful when we say
566800	568440	what things are and what things are not.
568440	570880	I think it's always relative to something else.
570880	574720	Like, I think it's true that you can put language
574720	577160	in a language model and it will do something.
577160	579840	But will it do what you wanted it to do?
579840	580960	I'm not sure.
580960	582960	So in the case of Chomsky, I don't think he's wrong.
582960	585960	I just think that in general, what we mean by language,
585960	588680	we don't just mean a way to transmit
588720	592160	semantics, I think we mean a way to transmit semantics
592160	595000	which are anchored into something for which we expect
595000	600000	a certain kind of outcome to also follow.
600360	603400	Because if both you and I mean blue butterfly,
603400	607400	but we don't mean the same like referent,
608240	609080	then it's useless.
609080	610920	That's not really language.
610920	614920	We're not really like converging over anything.
615400	616800	So I think that's the thing.
616800	620000	Like the language models don't necessarily converge with us
620000	622160	where we want them to converge.
622160	624360	So in that sense, they're disconnected,
624360	628840	but they do, to me and you, carry some semantics.
628840	631680	And that's how we can tell that like,
631680	633840	they're not doing what we thought they were gonna do there.
633840	636080	They can get disaligned with us.
636080	638280	So yeah, so again, things that they,
638280	640080	what things are and what things are not
640080	642880	is always relative to what we're doing.
642880	645080	What we're not is always relative
645080	648760	to some kind of perspective that frames it
648760	650900	and constrains it for it to be at all.
651880	654080	Yeah, I mean, because obviously we can get to the question
654080	657960	of whether you think language is a system of thought
657960	659640	or a system of communication.
659640	660920	But I kind of agree with what you're saying
660920	664520	about language models, which is that essentially
664520	667560	they are spitting out words, which means something to us.
667560	670240	And we can discuss where that intrinsic meaning comes from
670240	673000	whether it's pragmatics or whether it's just
673000	676440	some kind of colloquial usage of the word or whatever.
676440	679040	I mean, what's your take on the first question,
679040	681160	which is, do you think language evolved
681160	683280	as a communication or thinking mechanism?
686560	688120	I think we're baking in the definition
688120	691200	when we ask for either like, is language something
691200	695720	that is simply a reference between some kind of cluster
695720	698440	and some kind of real thing, is that language
698440	702640	or does it have to be something which is externalized
702640	707640	as a symbol, in which case, then it's purely communication?
707640	711080	So whether I think we have to be more clear
711080	713200	about what we mean, because I do think
713200	717360	that you don't need to externalize what you understood
717360	718960	in order to cluster it.
718960	722240	It's better, it allows you to confirm and act on it.
722240	725000	And especially if there are more agents around you,
725040	728760	I think it makes thinking much more efficient.
728760	732480	So I think if we went all the way down,
732480	735680	I think we'd find that maybe the egg or chicken problem,
735680	737440	like we could solve it sort of,
737440	740880	I think at the layers at which we would recognize something
740880	742760	that resembles thought or resembles language,
742760	744200	they're no longer separate.
745600	750600	But if we cast language as a system of reference
751600	756600	and reference, then I don't think you need communication
758240	762800	for that, you just need one entity to recognize something
762800	766800	and be able to keep that as something
766800	768320	it can refer back to later.
768320	770080	So it it towards itself,
770080	772840	but that also could be qualified as communication,
772840	775920	communication over time with yourself, right?
775920	777560	You cash something out,
777560	779920	which you will pull back again afterwards.
779920	784120	So whether it's spatial or temporal or spatiotemporal,
784120	786960	I think, is the real question we're asking here.
786960	788080	Okay, okay, interesting.
788080	792800	But I think that you can embed thinking inside language.
792800	796600	I mean, for example, like if you're writing software,
796600	798480	there are design patterns
798480	800960	and there are patterns for doing lots of different things
800960	802240	and we give them names
802240	804760	and then the developers recognize those patterns
804760	807080	in the software and they say, oh, I understand now.
807080	810040	And loads and loads of these thinking primitives,
810040	813280	I think are kind of culturally embedded in our language
813280	815200	and they're like memes
815200	817600	and they get passed through generation
817600	819360	and we just point to those things
819360	820720	and then we link them
820720	823960	to those kind of cognitive programs that we have.
823960	825640	The fidelity of that mapping, of course,
825640	828000	whether it's mediated or whether it's a simulacrum,
828000	829240	we can discuss that.
830760	832440	Okay, I don't disagree.
832440	833960	Yeah, I think you're right.
834000	838600	I think you tapped into this idea of whether
839640	843240	this is thought or does it require a name?
843240	845640	Would the pattern exist without the name
845640	848000	and would it be referable without the name?
848000	849520	Could we still point to it?
849520	854520	And then if we think of sign language,
856200	858800	what qualifies as a symbol in sign language
858800	862040	is it a reliable action
862080	864680	that follows something else?
864680	866400	Is it this intermediate?
866400	870680	And in this case, what qualifies as an intermediate
870680	874120	or intermediary when you're pointing at something?
874120	877800	So is it the fact that I look at you, you look at me,
877800	879280	I know you're waiting for something,
879280	880560	so I'm gonna point to the thing
880560	882120	and you're gonna make that connection as well.
882120	883320	Is that language?
883320	885480	I would argue, yeah, it is language.
885480	887560	And that's why I think animals, to some extent,
887560	889040	a lot of them have language.
889040	891360	If they're capable of transmitting information
891400	893280	through themselves as intermediaries.
894720	898400	And again, do you do this with yourself
898400	903080	without having to use what we would qualify as words?
903080	907720	I think words are just reliable intermediate clusters
907720	908920	and whatever those are.
908920	910240	Yeah, no, I completely agree.
910240	913000	So yeah, first of all, I think when we say language,
913000	915840	people assume that we're talking about words.
915840	918240	And as you rightly said,
918240	920080	just imagine a hypothetical situation,
920080	922400	you were born unable to make any sounds,
922400	923720	you didn't have a mouth
923720	926480	and you could only write with a pen.
926480	928880	You would still be every bit as capable as any other human
928880	931120	or perhaps you could only use sign language.
931120	932760	And then you could say, well,
932760	934720	does sign language have the representation
934720	936480	or fidelity or cardinality of language?
936480	938040	Yes, it does, absolutely it does.
938040	941880	So language isn't so much about the cardinality of,
941880	944480	I mean, language is just basically a set of strings,
944480	947080	by the way, so how much can you represent?
947080	950000	But anyway, I think it's about
950000	952440	kind of social complexification in our brains.
953440	955120	And even the ability to write
955120	956560	is quite an interesting example of that.
956560	958840	So this is a proto-ability that we've learned
958840	963120	and you mentioned animals and animals don't have the ability
963120	966600	to, you know, they don't have the flexibility that we have.
966600	969400	So they don't have the ability to map symbols
969400	971280	to a variety of different phenomena.
971280	973040	They do map some symbols to phenomena,
973040	975200	but that seems to be much more hardwired.
975200	978760	Whereas with humans, it's very, very flexible.
978760	980800	And it's a real mystery and unfortunately,
980800	982880	we don't have the phylogenetic record,
982880	984960	the intermediate record of how and why
984960	987920	that capability developed, but it did develop in humans.
990960	993480	So I can't speak as to why it develops
993480	995280	in humans and not in animals.
996480	999640	I think what we can talk about is
999640	1002520	what qualifies something as a language
1002520	1005760	if it's not the words, the sounds, the writing
1005760	1010160	or the signs, I think we're talking about a process
1010160	1014440	which is it's degree of rigidity on a certain scale.
1015400	1018640	So when you and I use words,
1018640	1021680	there's a very high likelihood that we refer to
1021680	1026160	some similar abstractness that can be contained, right?
1026160	1027960	Under some kind of market blanket.
1028000	1033000	Whereas the porosity of me kind of pointing over there,
1033200	1034560	that could mean a lot of things.
1034560	1036400	Like I haven't really reduced entropy.
1036400	1039320	Like given a context, there's a possibility
1039320	1040720	that we would reduce it, right?
1040720	1043640	And that's precisely the sociality you're talking about.
1043640	1045040	We're talking about these scripts
1045040	1048320	that basically constrain, constrain, constrain, constrain.
1048320	1049600	And then within those constraints,
1049600	1054480	there are a lot of different kinds of signals
1054520	1058960	that have come to take on a reliable meaning.
1058960	1061000	And we're so expressive
1061000	1062760	because we have so many of these symbols
1062760	1064080	and we can compose them.
1064080	1065360	So there's different scales, right?
1065360	1068680	So if I say the word trout,
1068680	1070320	I'm not sure you know what I mean here.
1070320	1071480	Like why trout?
1071480	1072400	What are you trying to say?
1072400	1073520	But if I add more words,
1073520	1076200	then suddenly I've also added more context to trout.
1076200	1078320	So I've contained, constrained
1078320	1080760	and now suddenly scales of meaning arise.
1080760	1082280	We have more priors, more priors
1082280	1085040	and now trout takes on a whole other meaning.
1085040	1086800	But if I kept pointing,
1086800	1088880	I don't think there'd be lots of information
1088880	1092080	that's contained here until maybe I do the thing for you
1092080	1094520	and now you'll know maybe that if I point over there,
1094520	1095720	that's what I mean.
1095720	1097480	Yeah, yeah, fascinating.
1097480	1100280	You mentioned intelligence before, by the way.
1100280	1103000	So what's your take on intelligence?
1103000	1106200	To me, intelligence doesn't start
1106200	1107520	as fundamentally human, right?
1107520	1108400	So we start for that.
1108400	1111080	Like to me, intelligence is a type of process.
1111120	1116120	It's the capacity to derive as many paths
1116120	1118200	between two points as possible.
1119800	1123360	Because it entails that you can compute those paths, right?
1123360	1126260	So your capacity to compute them is one thing.
1127600	1130120	And the precision with which you'll be able
1130120	1130960	to reach the point, right?
1130960	1132840	You could think you're computing the paths,
1132840	1134160	but you could be wrong.
1134160	1136240	You could actually get to a way different point.
1136240	1139040	So I don't think that would qualify as intelligence.
1139080	1141520	Intelligence is both your capacity
1141520	1143480	to reach accurately the point,
1143480	1146040	but also to reach it through a variety of means
1146040	1148320	that you can entertain at a given time.
1150280	1153680	So on that, and we'll talk about goals in a minute,
1153680	1155560	but let's say a goal is an end state.
1155560	1156960	So there's a state action space
1156960	1159040	and we're traversing through the space.
1159040	1162520	And then we look at the cone pattern of traversal
1162520	1165960	and the shape of that cone tells us a lot about intelligence.
1165960	1170040	So the cone might incorporate future affordances
1170040	1172360	which are not yet available to us.
1173480	1177240	They might include, maybe there's some kind of a cost function
1177240	1179960	because some affordances are harder to take than others,
1179960	1182120	both because of physical difficulty
1182120	1184120	and intellectual difficulty.
1184120	1186200	So you're saying there's some kind of utility function
1186200	1187880	that we can sum over all of those trajectories
1187880	1189560	to tell us how intelligent we are.
1190760	1191720	Yeah, I think so.
1191720	1195040	I think it's not necessarily also your capacity
1195120	1196360	to enact them, right?
1196360	1199560	I think, because if it was your capacity to enact them,
1199560	1201840	anybody disempowered would not be very intelligent
1201840	1202800	and that's not true.
1202800	1203760	That's not true.
1203760	1205320	They could compute it and be like,
1205320	1208480	well, I mean, I can see that if I were in that position
1208480	1213080	I would be able to or I can see that this is a path
1213080	1215600	that exists, but there's something in the way.
1215600	1217440	Like I can't go there.
1217440	1219080	That path exists nonetheless.
1219080	1221160	Like I know I can go outside right now
1221160	1222840	but all the doors are closed.
1222880	1225840	So I know I could try to find a way to open the door
1225840	1229320	but eventually the cost for me to get outside
1229320	1231560	would be so high I would effectively dissolve
1231560	1232720	so there's no point.
1232720	1234080	So I might as well just not do it.
1234080	1236200	The likelihood of me actually taking that path
1236200	1237840	is really low because I'm disempowered.
1237840	1238960	I'm not less intelligent.
1238960	1240200	I found a way.
1240200	1242640	I found a highly improbable way.
1242640	1244840	I just don't think I should do it.
1244840	1249840	So I think intelligence generally correlates
1250360	1252080	with your capacity to reach those goals
1252080	1254040	and therefore accrue more resources
1254040	1258760	because you'll be able to find ways to get towards places
1258760	1260560	which are self-evidencing.
1260560	1263680	And that also has a self-reinforcing cycle
1263680	1267000	where the more likely you are to accrue to resources,
1267000	1268800	the more likely you are to find more paths
1268800	1270480	that lead you to more goals, et cetera.
1270480	1274240	And then maybe if you can stop thinking
1274240	1277200	about the lower level paths you have to take
1277200	1278800	then you could compute larger scales
1278800	1282000	and you can become more effective on a, again,
1282000	1284080	grander scale so you can start influencing more people.
1284080	1288200	You can start thinking in terms of longer time depth
1289080	1291880	whereas if you're constantly trying to survive,
1291880	1296560	like everything's constantly hitting you from everywhere
1296560	1298360	then you have to compute all the paths
1298360	1299840	to avoid all those things.
1299840	1302560	So again, privilege plays into the degree to which
1302560	1304680	your intellect can expand, right?
1304680	1307880	Or at least expand in terms of empowerment.
1307920	1310200	But that's not to say you're not intelligence.
1310200	1312040	It just means that your intelligence
1312040	1314320	gets carried and echoed in the world.
1314320	1316760	And then there's all the social stuff, right?
1316760	1318760	Because you and I are constrained by socialness
1318760	1323640	or by some degree of social scripts, we can compute more.
1323640	1327320	We don't have to wonder how we should use that table.
1327320	1329280	We don't have to wonder how I should use this mic.
1329280	1330120	I know.
1330120	1333080	So the more we have these,
1333920	1335240	should you hold me to stop or?
1335240	1336080	Carry on.
1336120	1337880	The more we have these social signals
1337880	1339720	that allow us to compute more,
1340840	1342800	the more we can think on larger scales
1342800	1344920	or deeper time depth.
1344920	1348520	But sometimes I also think that some degree of constraints
1348520	1352680	also sometimes preclude you from thinking
1352680	1355840	that a potential path is probable.
1355840	1357240	Yeah, yeah, I mean, there's a few things there.
1357240	1360160	I mean, first of all, I quite like this idea
1360160	1365160	that the length of the prediction horizon
1365640	1367440	is a good indication of intelligence.
1367440	1371040	But the only way that we can increase the prediction horizon
1371040	1374440	is through creating very low resolution abstractions
1374440	1376400	to represent our sense world.
1376400	1380160	So that has a kind of brittleness associated with it.
1380160	1381320	You said something interesting before,
1381320	1382840	which is that you could think of it
1382840	1386320	almost as overcoming adversity or overcoming cost,
1386320	1390400	which like so some trajectories are going against the grain,
1390400	1393240	but some choose to pursue those trajectories.
1393280	1396200	And therefore you can think of them as more intelligent.
1396200	1398160	Yeah, so it's quite interesting.
1398160	1399480	I mean, I also like to zoom out
1399480	1402280	and think of the actual goals themselves.
1402280	1407000	Now, first of all, I think that goals are a system property.
1407000	1409240	I don't think individual agents have goals.
1409240	1412520	So I'm not sure it makes sense for an agent
1412520	1416360	to pursue a goal, but let's just as a best faith argument,
1416360	1418680	take a goal as a future state,
1418680	1421120	which an agent is aware of.
1421120	1424600	I would also factor in the utility of those goals
1424600	1425920	and the selection of those goals
1425920	1427640	as being parts of intelligence.
1427640	1430360	Yeah, so again, I mean, you said something interesting.
1430360	1431560	It's a property of the system.
1431560	1436120	And I think if we take what I said at faith,
1436120	1437720	intelligence is a property of the system,
1437720	1439320	not just the individual, right?
1439320	1441200	Because again, you get carried
1441200	1443720	given that the system props you somehow
1443720	1447760	and gives you capacities to think farther.
1447760	1450080	So given that you and I are in the same system
1450120	1453200	and have the same relationship to the system,
1453200	1455160	we could quantify or intellect,
1455160	1458760	given how far mine would go relative to yours.
1458760	1461600	But that's only given that we have the same relationship
1461600	1464800	to the system, which is again, highly unlikely.
1464800	1466480	You would have to basically be a twin
1466480	1468200	with practically in similar life
1468200	1470400	to have the same relationship to the system.
1471480	1474800	And then again, I mean, this idea of goal with utility,
1474800	1478000	it's always relative to a scale, right?
1478000	1482240	So given this scale, there's this gate,
1482240	1484560	which is highly likely and this gate,
1484560	1485920	which is slightly likely.
1485920	1488240	And so I'm probably gonna go from here to here
1488240	1491880	and how I get from here to here is the real question
1491880	1494000	and the degree to which I minimize free energy
1494000	1497920	by going from point to point and improve my model also
1497920	1499040	by going from point to point,
1499040	1502000	which is gonna allow me to reach the next goal
1502000	1503600	without too much energy.
1503600	1507200	So I don't think that someone taking a path
1507200	1509520	against the grain is necessarily more,
1509520	1511280	means that they're more intelligent.
1511280	1514800	I think that if they can take an unlikely path,
1514800	1518800	which also allows them to reach a goal
1518800	1520320	that others would have wanted to reach
1520320	1522560	and maybe couldn't or would do so
1522560	1526680	with having to spend more energy at a given scale,
1526680	1529180	means they're probably more intelligent.
1529180	1530800	But it doesn't mean that if you follow the path,
1530800	1531960	you're not more intelligent.
1531960	1534760	Maybe this is the smoothest ride.
1534760	1537960	Maybe there's no point in doing something different.
1537960	1542560	Maybe to you, the goal is that simple.
1542560	1545600	So you took the smoothest ride, that's very smart.
1545600	1549280	You just also were able to compute many other paths
1549280	1550760	to go there, you know?
1550760	1553520	Yeah, but I think a goal in,
1553520	1557320	because we were talking earlier about complex categories
1557320	1560800	like a chair, it's very, very difficult to define a chair,
1560800	1562800	and using the affordance is the way
1562800	1563960	that most ontologists use.
1564160	1566080	It's a thing you sit on or whatever.
1566080	1571080	And a goal, we're not actually talking about a state,
1571280	1575200	because if we represent the world in this systematic way
1575200	1577680	where we discretize everything,
1577680	1579680	then if you think about it,
1579680	1581940	there would be an infinitude of goals.
1581940	1584760	So we come up with an abstract notion of a goal,
1584760	1587080	and now we've just got this big goal thing
1587080	1588620	that we want to get to.
1588620	1590960	And still there's an infinite number of trajectories
1590960	1592680	that would reach an estate
1592680	1594640	which satisfied that goal.
1594640	1596960	So it just, it feels, I don't know,
1596960	1600360	it feels kind of vague to me.
1600360	1601360	To say goal?
1602320	1604360	Yeah, well, I guess like,
1604360	1606240	because we were talking earlier about science
1606240	1607320	and the benefit of science,
1607320	1609880	and I think like, you know,
1609880	1612600	abduction is a key part of science,
1612600	1616640	and abduction is about finding a reasonable explanation.
1616640	1619280	So, you know, selecting out of the infinite set
1619320	1622880	of hypotheses a reasonable set and mapping to them.
1622880	1625280	And it's a similar thing with a goal, right?
1625280	1628360	So what you're actually doing is you're,
1628360	1629320	it's very creative.
1629320	1633360	You're basically creating something from nothing.
1633360	1636000	And you're saying, these three things
1636000	1637640	seem very valuable to me.
1637640	1641100	Now we know that we're talking about a multi-agent system.
1641100	1642360	And the intelligence,
1642360	1644440	and what you said really resonated with me,
1644440	1646600	the intelligence arises at the system level.
1646600	1648760	So the individual agent,
1648760	1651000	I think the goals are not arbitrary,
1651000	1653160	but not a million miles from arbitrary.
1653160	1655920	But when you multiply that with many, many agents
1655920	1657640	and a large collective intelligence,
1657640	1659440	and you kind of average over the top,
1659440	1660760	then you get these emergent goals
1660760	1662320	and you get intelligent behavior.
1662320	1666000	But it doesn't make sense to me to think of an individual agent
1666000	1669560	as having any meaningful form of planning and reasoning.
1671080	1674440	Well, I guess we're putting a lot in meaningful here,
1674440	1676920	but as a thought experiment,
1676920	1681920	I like to think of someone really, really, really smart
1683960	1687160	who would compute like 3,000 years into the future.
1689400	1691320	And you would think, like, well,
1691320	1694600	that person would try, you know, solve world hunger
1694600	1697360	and they would like, I don't know,
1697360	1700880	they wouldn't try to take over the government.
1700880	1703560	And, you know, they would find the path
1703560	1708200	that makes sure that somehow they reach immortality
1708200	1710960	and their children and something like that.
1710960	1715360	And I'm like, I don't think so.
1715360	1718680	I think someone really, really, really intelligent
1719760	1724320	would probably be able to compute all the paths
1724320	1727720	and find that they're already on the free energy
1727720	1730200	minimization path and they would just, you know,
1730200	1732240	live a very normal life.
1732240	1736160	Like they would live the life they seem to have been set on
1736160	1738320	and they would continue in that
1738320	1740400	because they're part of a manifold
1740400	1744160	and they're already part of a pool of negentropy
1744160	1747880	and they're already like minimizing somehow.
1747880	1752600	As Friston says, he argues there's something fundamental
1752600	1755200	about, you know, if we talk about utility,
1755200	1757280	for him, utility is existence.
1757280	1759760	And there is an argument that some things
1759800	1763960	are so fundamental, so platonic that they go about saying.
1763960	1765840	So maybe it's existence.
1765840	1767320	I'm not so sure about that.
1767320	1769880	I still think that inside Friston's framework,
1769880	1773680	there are still many potential ways to act and live.
1773680	1776120	And the ex-risk people, they say,
1776120	1779040	oh, obviously these agents are going to seek
1779040	1781000	to maximize power.
1781000	1782680	And well, why is it obviously?
1782680	1785000	I'm not entirely sure.
1785000	1787360	The agents as in the AIs?
1787360	1789400	Yeah, the AIs.
1789440	1792560	First of all, it's interesting that we're saying plural, right?
1792560	1794120	Like in general, the...
1794120	1797080	Well, for them actually, it's the one, right?
1797080	1800360	They're still at the AI stage of there being one.
1800360	1801200	Yeah, right?
1801200	1802040	I know, it's so typical.
1802040	1806880	That's only because they haven't got to the multi-scale yet.
1806880	1809280	No, I know, but I think it speaks to something.
1809280	1813080	It speaks to this notion of individuality
1813080	1814400	and disconnection from the others.
1814400	1817440	Whereas, no, intelligence is a function of your group,
1817440	1818440	not just of yourself.
1818440	1820000	The genius myth is a myth.
1820000	1823480	They all collaborate and prosociality
1823480	1824560	leads to something better.
1824560	1828120	So I do think that should AIs take over,
1828120	1830040	it won't be one, it'll be several,
1830040	1832800	it'll be a group, like coalition or something.
1832800	1836520	I'm interested in exploring various limiting factors
1836520	1838920	or scaling laws for intelligence.
1838920	1841360	And one of the big ones is knowing.
1841360	1842760	And in a certain environment,
1842760	1846960	there are only a finite number of things to know.
1847000	1849280	And what you know might be related to the agents
1849280	1851520	that you cohabit your environment with.
1851520	1853800	But you did say earlier when we were having a discussion
1853800	1855720	that you thought in principle
1855720	1858480	that you could scale intelligence.
1858480	1862200	And I actually like to rephrase what you just said.
1862200	1864080	So you said when you zoom out
1864080	1867680	and you're looking not at individual trajectory volumes,
1867680	1870520	but you're looking at kind of like the adaptability
1870520	1873880	of the entire super organism.
1873880	1876680	Because that jives with the common definition
1876720	1879200	of intelligence that is about adaptation, efficiency.
1879200	1880240	And I really like that.
1880240	1882240	But I guess I still want to say though
1882240	1885520	that the X-Risk people think that there's a pure intelligence.
1885520	1887000	So it's pure reason.
1887000	1891120	It's not in any way constrained by material reality
1891120	1893560	or like what we know.
1893560	1896440	And I think, well, let's look at an ecosystem.
1896440	1901440	And even if the agents had a perfect model of each other,
1901880	1905080	and so there was a lot of complexification
1905080	1907400	of the models of every single agent,
1907400	1909520	surely there's a hard limit
1909520	1911880	of how many time steps they could predict ahead.
1912880	1914720	If that intelligence is pure,
1914720	1917320	like not limited by materiality,
1917320	1918480	like say it's not,
1918480	1921080	it doesn't relate to land hours principle.
1921080	1922800	It's not bound by quantum physics.
1922800	1924480	Like is that what you mean?
1925440	1929240	Well, let's say it were possible for an agent
1929240	1933840	in a collective to predict many, many, many, many steps ahead.
1933880	1935080	In order for that to happen,
1935080	1937000	the agent would essentially need
1937000	1940880	to have an internal simulation of the entire system.
1941840	1944280	And maybe not the entire system
1944280	1947080	because we're speaking about mediation and interfaces
1947080	1948240	and affordances and so on.
1948240	1950200	So there's still a lot of information hiding there.
1950200	1953280	But let's say the agent had some holistic understanding
1953280	1954400	of the entire system
1954400	1957160	and could roll trajectories far into the future.
1957160	1959440	So, but I guess my point though is that
1959440	1961320	the agent wouldn't be intelligent.
1961320	1963360	It would have memorized everything there is
1963360	1964960	and know about that ecosystem.
1967120	1969800	I don't think this time limit thing,
1969800	1973680	I think you would just lose granularity.
1973680	1977840	I think we have hierarchical models
1977840	1981040	and if you extend this to infinity,
1981040	1986040	there's a really sort of high level,
1986360	1989000	practically near infinity state,
1989000	1990520	which you can predict, right?
1990520	1992480	There are constants to the universe.
1992600	1995160	And so, while I can't tell what you're gonna be doing
1995160	1997280	in 10 years with precision,
1997280	1999640	I know there's a day you're gonna die.
1999640	2002040	Like there's some constants that I can predict.
2004400	2007520	You know, I know that likes it's gonna keep traveling
2007520	2009160	until it no longer, you know,
2009160	2013720	like there's a degree of granularity
2013720	2015560	that can maintain accuracy,
2015560	2018840	even if it loses out on precision.
2018840	2021720	So that's probably one of the solutions.
2021760	2023800	You would start losing out on granularity.
2023800	2025000	So if you're asking me,
2025000	2028480	can you continue predicting with the same level
2028480	2032340	of both precision and accuracy across infinite time steps?
2034160	2036660	I think if you're stable enough, yeah.
2036660	2039100	So the question is really about the degree
2039100	2041640	to which you have volatility in your system, right?
2041640	2044480	Because if you and I just freeze in time,
2044480	2045960	I mean, we're freezing time, that's it.
2045960	2048160	I've predicted you for infinity.
2048160	2051400	So it's really a degree to which you can,
2052560	2056960	predict emergence I think relative to volatility.
2056960	2059040	And I think we'll be able to,
2059040	2061840	not necessarily that soon, but I think it's coming.
2061840	2063400	We're gonna be able to predict emergence.
2063400	2065960	So if we do, if we can predict emergence,
2065960	2069360	and we can have models that can get to,
2069360	2071380	let's say minimal description length,
2071380	2073440	and they can improve their models
2073440	2076400	such that they could always reach a higher level,
2076400	2079600	a shorter description length,
2079600	2083160	then I think we can predict infinity.
2083160	2084360	I mean, there's a few things on that.
2084360	2089360	So first of all, I think that when we have ecosystems
2090080	2093560	of life, there is a degree of irreducible complexity.
2093560	2095680	And I think there's a kind of power law distribution
2095680	2098640	where some of it is reducible complexity
2098640	2101080	and quite regular, and then a lot of it is so chaotic
2101080	2104920	on the long tail that it's essentially irreducible.
2104920	2106600	But I do think, what you said is very interesting
2106600	2108940	about there being certain trade-offs about predictions.
2108940	2112060	So yes, we could create low-resolution models
2112060	2113980	that could predict far into the future.
2113980	2116020	And we were having an interesting conversation
2116020	2118700	about AlphaGo, because I think what that does
2118700	2120780	is it's still a high precision.
2120780	2122700	It's a high-resolution model,
2124100	2126900	and even in respect of time steps.
2126900	2129940	But what it does is it's not a robust model.
2129940	2131060	So it's a point-alistic model.
2131060	2132420	So what it's saying is like, okay,
2132420	2135300	well, Go has a structure to it,
2135300	2138700	because the state space is combinatorially large.
2138700	2139660	It has a structure to it,
2139660	2141580	and most of the humans traverse the structure
2141580	2145340	along these joints, and I'm gonna just fill in the thing
2145340	2146540	around the structure.
2146540	2148060	And that's kind of what you're saying.
2148060	2151220	So I've got this model, which represents trajectories
2151220	2154620	on the structure, there's holes everywhere, right?
2154620	2156160	No, I mean, I think that's exactly it.
2156160	2158580	I think you're talking about the degree to which I can predict
2158580	2161340	like say a black swan, right?
2161340	2162900	What happens if there's a black swan?
2162900	2165700	I can't predict that, like that's outside of the bounce.
2165700	2170700	So there's always a limit to given
2171300	2174660	a sort of relatively static model, what I can predict,
2174660	2176660	no matter how good that model is.
2176660	2179860	Well, we're talking about models that continuously improve.
2179860	2184860	So if I continue grabbing complexity
2185100	2188340	and I continue improving, and I think
2190820	2195660	you'd continue rolling out, say, your predictions.
2195660	2200620	I think you'd, it would become increasingly possible
2200620	2203580	for you to predict farther and farther ahead,
2203580	2207740	especially if you have this capacity to predict
2207740	2209500	something that qualifies as a black swan,
2209500	2211620	which is just a phase shift, right?
2211620	2213460	It's a phase shift that you can't see at the scale
2213460	2217260	at which you're generally operating your predictions.
2217260	2219100	So...
2219100	2222180	Are you defining black swan to mean a rare single event
2222180	2224380	or like a transition to a new regime?
2226060	2228180	I mean, I think it's the same.
2228180	2231380	I think a rare single event, effectively,
2231380	2236380	if you do not have the capacity to adapt to it afterwards,
2237460	2239380	is effectively a new regime.
2240420	2245420	So black swans can either trigger your resilience,
2246300	2251100	but that effectively means you had the capacity
2251100	2255100	to deal with policies that could take you across.
2255100	2257180	So while you couldn't predict the event itself,
2257180	2259860	you had policies that were capable of dealing with it.
2260780	2262260	And if you can't deal with it,
2262260	2265180	and you had no policies that could account
2265180	2268420	for this degree of error, like this is too much.
2268420	2270820	So I think that's effectively, at that point,
2270820	2272060	you become in a new regime.
2272060	2272900	Interesting.
2272900	2273740	Okay, okay.
2273740	2277100	Let's move over to general phenomenology
2277100	2278140	and an activism type stuff.
2278140	2281220	So the early anactivists,
2281220	2284700	they thought that agents kind of construct
2284700	2286300	their own environment
2286300	2289820	and that they have a phenomenal experience.
2289820	2292100	We were discussing earlier, like this,
2292100	2294940	the extent to which the phenomenal component
2294940	2297580	adds something to an agent.
2297580	2298580	What's your take on that?
2298580	2299500	Oh, no, I think it does.
2299500	2304500	I think it forces the agent
2305060	2307740	to deal with that information.
2308260	2313260	Let's say, through a little more constraints,
2315100	2316860	you can understand something,
2318340	2322980	but it will have very low impact on your behavior
2322980	2324540	if you don't feel it.
2324540	2327460	You can have rules about things,
2327460	2331620	but what if I take away the rules?
2331620	2334940	Well, there's nothing then.
2334940	2339940	Like we're just understanding doesn't entail anything.
2339980	2343100	So effectively phenomenology
2344020	2347660	sort of forces you into some constraints, right?
2347660	2350260	It's phenomenology only arises
2350260	2354540	in relation to your self-model.
2354540	2357740	So what I'm gonna focus on
2357740	2361380	relates to what I need to focus on,
2361380	2363060	what will give me more information,
2363060	2365300	what relates to my utility function,
2365300	2369460	what I, the intensity of what I feel
2369460	2373540	relates to how closely or how deeply
2373540	2375220	it affects my internal states.
2375220	2377340	Like there's a whole set of things like this
2377340	2379700	that relate to internal external dynamics.
2379700	2382660	So I do think it adds something.
2382660	2385540	Effectively, it adds a degree of precision
2385540	2387620	relative to the policies you're gonna have to take
2387620	2390380	to maintain, to self-organize.
2390380	2395380	Without these, you just have some sort of blank picture
2397180	2399660	on which you're not really capable of focusing.
2400580	2403460	We're really good at this reflexive self-attention
2403460	2406340	and also knowing what to pay attention to around us
2406340	2408420	and knowing what level of resolution,
2408420	2409900	how much can we discard?
2409900	2411500	So for example, we don't need to know
2411500	2413620	what the title of all of these books are around us
2413620	2415260	at the moment because it's not relevant.
2415260	2417740	So we have this great ability to just discard
2417740	2419040	all relevant information.
2419040	2422120	But I don't think phenomenology adds anything
2422120	2425000	to that discussion, which is to say like,
2425000	2427800	what is it about phenomenology?
2427800	2430040	What value does it add in of itself
2430040	2433200	that can't be explained by just the basic agent framework?
2434720	2436080	Well, the basic agent framework,
2436080	2440640	does it have a reflexive model of itself?
2440640	2445640	Like is it capable of determining how it functions
2446040	2447560	and how that functioning relates
2447560	2449800	to specific elements outside of itself?
2449800	2451520	Well, that's a great question actually.
2451520	2456080	So, because we were saying earlier that to be an agent,
2456080	2458960	you need to have this like reflexive model of yourself.
2458960	2460760	And in order for that to happen,
2460760	2462760	you have to be around other agents
2462760	2464240	because you have to pass information out
2464240	2465480	and it comes in again.
2465480	2469480	So, but this self-awareness I think you would argue
2469480	2471720	only happens when you have a sufficient level
2471720	2473760	of nesting or complexity.
2473760	2475680	Sure, yeah, absolutely.
2475720	2477840	But then how does it emerge there?
2477840	2479840	So, you add the complexity,
2479840	2483000	but what is it that makes this reflexive self-model emerge?
2487360	2492360	So, we're trying to suppose right now,
2493760	2498760	like we're trying to make a self-model emerge.
2500920	2504080	Like most of the time when you create an agent
2504080	2506880	that has a self-model, you just endow it with it.
2506880	2510920	So, if you're asking me how does the self-model emerge?
2510920	2512320	I think, I mean, I could explain it to you
2512320	2514240	in terms of like evolutionary demands,
2514240	2518240	the kinds of systems that we're able to represent
2520240	2522880	through some kind of tracking mechanism,
2522880	2527880	their own functioning on top of the functioning itself.
2528920	2533320	And that through that tracking of their own functioning,
2533320	2535280	they were able to modulate the precision
2535280	2536400	on their functioning.
2536400	2538080	I mean, that's an explanation of how it emerges, right?
2538080	2540160	On a purely evolutionary stance.
2541560	2544680	Now, how does that come about?
2546040	2547560	I'm not sure I can tell you exactly,
2547560	2549960	but we are suggesting that potentially
2552520	2554600	a system can track anything, right?
2554600	2556120	Can track the outside,
2556120	2558640	can track the inside to some extent.
2558640	2562560	So, now that it tracks something,
2562560	2564640	it makes predictions and these predictions
2564640	2568040	are either good or they carry error.
2568040	2570320	Given that they carry error too often,
2570320	2573680	it could choose, it has the possibility
2573680	2575280	to create a new state.
2575280	2578160	Like just change its model, create a new state space
2578160	2579800	and that would be structure learning.
2579800	2583720	And so, the supposition would be
2583720	2587520	that let's say a system that has as an action,
2587520	2590200	as a possible action to let's say,
2590200	2592120	create a bunch of cells over here
2592160	2594160	that can track that stuff,
2594160	2597320	it will survive longer than the system
2597320	2600040	that doesn't push a bunch of cells over here
2600040	2601440	that happen to have the capacity
2601440	2604200	to track what they're doing.
2604200	2608480	So, we were gonna test this with say,
2609800	2611920	an agent that does a simple, silly task,
2611920	2614440	like find this thing over there.
2614440	2617960	Or now find it, but in a graph that's like 1000 node long.
2617960	2619960	Are you better at it if you have a self model
2619960	2621760	or are you worse at it if you have self model?
2621800	2623480	And we're not gonna give it the self model.
2623480	2626600	We're gonna try and see if it can,
2626600	2628800	through an action of modeling anything
2628800	2631560	or changing anything, it can start modeling
2631560	2635120	its own processes and see if that allows it
2635120	2637080	to eventually, through structure learning,
2637080	2639400	grow into something that has a self model.
2639400	2641000	Yeah, I mean, I can believe,
2641000	2642440	because it sounds very principal to me.
2642440	2644640	So, I can believe that it would emerge
2644640	2648400	and it would be useful for survival to emerge.
2648400	2649560	It makes a lot of sense to me.
2649560	2651200	And also, it reminds me of like,
2651200	2652800	Daniel Dennett's intentional stance.
2652800	2657560	So, like whether or not other actors have mental states,
2657560	2659520	treating them as if they did,
2659520	2661280	so that you can make better predictions
2661280	2662520	about what they do.
2662520	2664760	That's also very, very useful.
2664760	2668000	But also, this kind of projection can be misleading
2668000	2669560	because people look at language models
2669560	2671760	and they say they have a theory of mind
2671760	2674360	and they're regurgitating things
2674360	2675960	from the psychology literature and they're saying,
2675960	2678000	oh, look, let's do this theory of mind test
2678000	2679160	on a language model.
2679200	2681840	Oh, it seems to have a theory of mind.
2681840	2684680	Well, I don't think it does have a theory of mind.
2684680	2688240	So, like, is it just another form of anthropomorphization?
2689120	2689960	So, that's the thing.
2689960	2692120	So, when we say it, what do we mean?
2693080	2694880	And I think it's possible
2696920	2699400	that in the embedding,
2700440	2703120	because it encodes something again,
2703120	2706000	that could be sort of touring complete,
2706040	2711040	there could be something which tracks theory of mind.
2711720	2713960	But I don't think it,
2713960	2717280	as in it's having awareness of,
2717280	2721080	allowing it a causal model of theory of mind
2721080	2722560	has theory of mind.
2722560	2724080	That, to me, is different.
2724080	2726760	So, can it regurgitate information
2726760	2729640	that would act like theory of mind
2729640	2731680	and be as accurate a theory of mind?
2731680	2732520	Pretty sure it would.
2732520	2735680	Yeah, it's possible with enough data thrown at it.
2735680	2736520	Probably would.
2738240	2739080	But,
2740480	2744840	I don't think it would be able
2744840	2746720	to tweak its own model
2746720	2748280	such that it can
2750200	2755200	really identify why and how
2755200	2757880	or where it has theory of mind.
2757880	2761480	Like, I don't think it could self-model in that way.
2761480	2766160	And I don't think it could tweak its own model like that.
2766160	2767000	Whereas,
2768800	2770200	I'm pretty sure I can.
2770200	2774040	I'm pretty sure I have the capacity to know
2774040	2775080	that's what I'm doing.
2775080	2776880	I'm putting that into you.
2776880	2778880	And therefore,
2778880	2782480	through trying to understand the causal chains
2782480	2786560	that lead me to getting theory of mind about you,
2786560	2790840	I can maybe change how that manifests
2790840	2794280	or increment over it or add more precision
2794280	2796760	or even transfer it to something else
2796760	2799800	to which now I will give some degree of theory of mind
2799800	2804640	because I've made that conscious tweak in my model.
2804640	2805680	Yeah, yeah.
2805680	2808400	Okay, we had an interesting discussion earlier
2808400	2810040	about mediation.
2810040	2813920	So, and it's related to semantics, right?
2813920	2815520	So, we understand each other.
2815520	2817120	And in this conversation,
2818080	2822680	let's say we're creating this temporary construction.
2822680	2825600	So, we're implementing the mediation pattern.
2825600	2828360	And we've selected some categories
2828360	2830040	that we kind of agree on the semantics
2830040	2831320	and we're kind of like mediating
2831320	2833360	through that shared structure.
2833360	2834720	And that to me,
2834720	2838000	I mean, I guess that's like a kind of bespoke theory of mind,
2838000	2840200	which we've just created now.
2841440	2844600	The fact that we're capable of mediating through language?
2844680	2846160	Yeah, not that well, I mean,
2846160	2848960	as we're using language in a very broad term,
2848960	2849800	as we said earlier,
2849800	2851800	so we're communicating using language,
2851800	2855520	but we are using a certain vernacular,
2855520	2857280	we're talking about certain categories and so on
2857280	2858120	that that we agree with.
2858120	2860560	But I guess like the reason I'm bringing up mediation
2860560	2864680	is perhaps we are communicating
2864680	2868440	in a slightly adapted way than we naturally would.
2868440	2871800	Yeah, so to me, the reason that this allows me to derive
2871800	2873280	that we have theory of mind
2873280	2875520	is because through this,
2875520	2880520	I can sort of push away a specialization to you
2881680	2884640	that allows me no longer to compute
2884640	2885680	what you're gonna compute
2885680	2888440	and still together reach our goal.
2889840	2891600	What this means?
2891600	2893240	I think you can model me
2894240	2896380	and I think I can model you.
2898040	2899000	That's it.
2899000	2900440	To me, that's the extent of it.
2900440	2902240	Like I can put myself in your shoes
2902280	2903280	and know what you're gonna do
2903280	2904800	and you can put yourself in my shoes
2904800	2906840	and know what I'm gonna do.
2906840	2909080	And that's just shared pretensions.
2909080	2911240	It's shared coordination
2911240	2913880	because I've outsourced something to you
2913880	2916920	that I think reliably you're going to push
2916920	2918360	towards a similar outcome.
2920000	2921360	I don't think it's that complicated.
2921360	2925160	I think definitely language is one of our main mediums
2925160	2927960	through which we understand how we reach that coordination,
2927960	2929840	but there's so much more to it, right?
2929840	2934840	Like institutions embedded material reality,
2936600	2940040	physical constraints, phenotype, time.
2940040	2945040	Like there's so much to these shared flows
2946680	2951280	that we are part of that we both know how to interact with
2951280	2953200	and I know you know how to interact with them
2953200	2954400	and how can I recognize that?
2954400	2956800	Well, I see you interacting with them.
2956800	2958840	I know that were I in your position,
2958840	2961880	I would do very much the same thing.
2961880	2964920	And I can tell you can tell I'm doing the same thing
2964920	2966320	because you put the chair there.
2966320	2970040	I'm pretty sure you intended me to sit and talk in that thing.
2970040	2972200	You didn't tell me that.
2972200	2974040	We understood through the scripts
2974040	2977880	that we were able to reliably predict that this happened.
2977880	2982880	So that's to me the medium is quite broad,
2983360	2987200	but it manifests through its reliability.
2987200	2988840	Yes, yeah, interesting.
2988840	2991280	And in some sense, our generative models,
2991280	2993480	even though they are very, very different,
2993480	2996000	there is a significant intersection,
2996000	2998000	although I don't think it makes sense to talk about
2998000	3000040	an intersection of the high resolution generative model.
3000040	3003280	It's more about the language or not only the language,
3003280	3005040	you were talking about the performativity
3005040	3006040	of sitting down in a chair
3006040	3009240	and like the various things that we do,
3009240	3013800	they can create intersections to bridge understanding.
3013800	3014640	Absolutely.
3014640	3017400	That's why poetry to me is mind-boggling.
3017400	3022320	Like poetry to me is precisely those intersections
3022320	3024960	which are generally inexistent.
3024960	3025800	They're novel.
3025800	3028320	Like we're literally creating images
3028320	3030600	nobody else has created before, creating connections
3030600	3032520	nobody else has created before.
3032520	3035040	And yet, or at least it's the hope,
3035040	3037280	people understand poetry.
3037280	3042280	You have created an entirely new constraint system
3042400	3046560	which normally doesn't point to anything.
3046560	3049960	And somehow, because there's all these webs
3049960	3052880	of other constraints that I can sort of pull from
3052880	3056840	in reliable ways, you and I will create
3056840	3061840	the new series of intersections across all these dimensions
3062680	3066240	and we still are capable of communicating
3066240	3067520	and coordinating across that.
3067520	3069960	To me, that's mind-boggling.
3070040	3070880	Yeah, yeah.
3070880	3073080	And so subjectivity fascinates me
3073080	3076480	and poetry we spoke about going to a music festival earlier.
3076480	3080640	And at the music festival, the shared experience
3080640	3084640	is that weird sense of being with other people.
3084640	3087320	Clearly not, what does it mean to understand music?
3087320	3089600	What does it mean to understand poetry?
3089600	3090440	I'm not sure.
3090440	3093640	Like it's this moment of, whoop,
3093640	3095360	something new is happening here
3095360	3098600	and we can both reliably feel there's something new.
3098640	3101120	And now what we make of that something new,
3101120	3104680	I think is why poetry is a terrible way to communicate.
3104680	3106880	Like I mean, great, you've made something clever.
3106880	3109160	We both felt something, something new here
3109160	3112800	and we communicated the fact that we understand this gap
3112800	3115720	and there's the possibility for our feeling of this gap
3115720	3120720	to be similar and if it's, to me, a good poem
3121200	3124320	will have reliably similar experiences
3124320	3127280	in the sense that some poem means to say something to you,
3127280	3128960	not just create an experience.
3128960	3132680	Like I was just watching this video on YouTube
3132680	3134200	the other day about Rupi Kaur
3134200	3137920	and how she with her Insta poetry kind of ruined poetry
3137920	3140120	because a lot of her poems are like,
3140120	3142480	her poems are like completely like first level.
3142480	3144000	They're just, they're just,
3144000	3146040	she just said, this is a book and I have a book.
3146040	3148040	Like there's nothing there, right?
3149800	3154800	But sometimes through the clever ways that her words
3155800	3160800	mix in a new and powerful, surprising way,
3162040	3165840	you will feel this intensity and connect this intensity
3165840	3169120	with a term you wouldn't have normally.
3170120	3174760	And that's a new phenomenology that has given you a meaning.
3174760	3178560	You now understand what it's like to be a woman.
3178560	3180560	You would never have put that intensity
3180560	3182960	on this connection of concepts
3183120	3185080	that doesn't mean anything to you.
3185080	3187160	It's like there's no valence there.
3187160	3190760	And now there is and suddenly you've experienced something
3190760	3192720	that she meant to make you experience.
3192720	3195840	And so she has communicated what she meant to communicate.
3195840	3200120	There's very low uncertainty there.
3200120	3203160	But when you read a poem by some abstract poet
3203160	3208160	who says something that, I mean, it's surprising
3208160	3210840	but I don't understand what he's trying to say to me.
3210840	3212800	I feel like it's not going as far.
3212800	3215160	Like, yeah, okay, I felt the surprise
3215160	3219600	but now it has no, it doesn't help with my model.
3219600	3222000	It doesn't go any deeper.
3222000	3223640	And perhaps that poetry is not for me.
3223640	3226360	It doesn't give me a new phenomenology
3226360	3229320	other than the surprise of the words.
3229320	3231400	Yeah, that really resonates with me.
3231400	3234880	And I think where I was getting stuck before is,
3234880	3237160	I thought, well, and I think of meaning
3237160	3239400	and understanding as being quite related.
3239480	3244480	And you could have a very impactful phenomenal experience
3244800	3247600	but I was worried it might stop with you, but it doesn't.
3247600	3252360	So as long as the information can propagate, right?
3252360	3254880	It has this memetic quality to it
3254880	3258080	because there is no meaning without grounding
3258080	3259760	but everything is grounded.
3259760	3263120	So you're reading a poem, you're sitting in this location.
3263120	3265880	It's now been grounded to this location.
3265880	3269640	You now create a yearly pilgrimage
3269640	3271920	where you come back to this location with your friends
3271920	3273520	and you read the same poem
3273520	3277120	and then this memetic energy builds
3277120	3278800	and builds and builds.
3278800	3281400	So, and maybe it doesn't always work that way
3281400	3286040	but I think you can interpret that as still forming meaning.
3286040	3289840	No, I agree, but I think I would find interesting
3289840	3291200	the thought experiment of,
3291200	3293040	because you and I, we exist in this world, right?
3293040	3295080	So obviously our meaning is in this world.
3295120	3297480	So we can create emergent structures
3297480	3300160	but in the end they always bottom out to this world.
3300160	3304040	But what if we were, to your point earlier,
3304040	3308960	entirely systems that only exist in words?
3308960	3312920	Like we, it's just words that like through some kind
3312920	3314840	of functor becomes other words
3314840	3316520	and then connect to other words.
3316520	3320080	And it's just like large graph of words to words to words.
3320080	3325080	And anytime a path through words is enacted,
3326520	3328960	it triggers other paths through words.
3330520	3334280	I feel like you could have something that resembles
3336160	3337880	phenomenology.
3337880	3341040	It just wouldn't be this.
3341040	3343600	Like there's a possibility for a system
3343600	3348600	to fully be contained and self-organized
3349200	3354200	within that so long as its maintenance
3356400	3360520	is conditioned upon it having a boundary
3360520	3362680	which it has to maintain.
3362680	3364280	Otherwise it disappears.
3364280	3367360	So I could totally see some kind of simulation
3367360	3370800	of like graphs that try to stay graphs
3370800	3373720	and become so complex that they maintain graphs
3373720	3377400	and it wouldn't really relate to their capacity
3377400	3378520	to exchange bits, right?
3378880	3380040	Again, it would have nothing to do
3380040	3382560	with the physical embodiment of it.
3382560	3384600	It's just that I can't conceive of anything
3384600	3388240	that wouldn't have a physical embodiment to some degree.
3388240	3390240	So, but I don't think that's impossible.
3392120	3395440	Before we finish, I'm supposed to ask you about resiliency.
3395440	3398520	Collective intelligences, they are grown.
3398520	3401080	They're not designed by definition.
3401080	3403200	And they have some interesting properties.
3403200	3405800	They can kind of heal themselves, repair themselves.
3405800	3407200	But there's a lot of complexity.
3407200	3409880	We don't understand what they're doing entirely.
3409880	3412800	But I guess like it is intuitive to me
3412800	3415440	how, I mean, Michael Levin talks about this,
3415440	3418320	you know, a collective intelligence is more resilient
3418320	3419520	because of self-organization.
3419520	3421600	But can you kind of explain from your perspective
3421600	3422680	like why you think that is?
3422680	3424320	Yeah, I mean, exactly why?
3424320	3428160	Why is self-organization of a group more resilient?
3429200	3430720	So we talked about it earlier,
3430720	3434000	like how do we define resilience?
3434000	3436800	We define resilience as the capacity for a system
3436800	3441280	given a perturbation to either remain the same
3442800	3447800	or live the perturbation and return to what it was before
3449080	3454080	or learn something new and adapt and become better, right?
3455480	3458120	So generally we think of black swans
3458120	3460200	as these highly improbable things
3460200	3463880	that you kind of, you can't really deal with.
3463920	3467040	And so generally what'll happen with a black swan
3467040	3470520	is it'll vastly perturb your system.
3470520	3473160	And if you have the seed of the system
3473160	3475840	and might grow again and return to what it was,
3475840	3480840	so from that point already, it doesn't have inertia.
3481400	3484280	Inertia is like the wind on the cup.
3484280	3486240	The cup, the wind is a disruption,
3486240	3489440	but it's so, it has such inertia in its policies
3489440	3491320	that it's like it's gonna stay a cup.
3491320	3493800	Now, a black swan is me grabbing the cup
3493800	3494920	and dropping it on the floor.
3494920	3496520	It can't deal with that.
3496520	3500200	Now, a plastic cup or like say a rubber cup,
3500200	3502240	it'll get on the floor, it'll bounce a little,
3502240	3504880	it'll deform and it'll come back.
3504880	3506760	A smart plastic cup.
3508440	3513000	Now that cup will not only, you know, it'll fall,
3513000	3515440	it'll learn what happened in the fall
3515440	3519160	and it'll be like, what was the cause of the fall?
3519160	3520120	Somebody grab me.
3520120	3522000	So next time something comes to grab me,
3522000	3523000	I'll move a little bit.
3523000	3524720	So that would be a plastic system.
3524720	3529560	So the reason systems that are groups
3529560	3531080	have the capacity for resilience
3531080	3535760	is because they have the capacity for one,
3535760	3538760	creating some core of inertia.
3538760	3541600	Something of the group will be maintained.
3541600	3544680	Like they have a very strong inside,
3544680	3546840	it's very clear about what it is
3546840	3549320	and it'll be able to continue pushing
3549320	3550800	what it is into the world.
3550800	3553120	Then it has like structures that are a bit more,
3553120	3555280	you know, like elastic.
3555280	3558840	They'll take disturbances, but they'll come back
3558840	3560160	and they'll be able to do that
3560160	3562920	because they can receive again the signal from the inside
3562920	3564400	that's like, this is what we are.
3564400	3566280	Okay, all right, let's go back.
3566280	3568440	And through this, they understand
3568440	3570480	that they have predictability,
3570480	3572280	potentially even redundancy.
3572280	3573880	So now what happens with redundancy?
3573880	3576480	Well, maybe it becomes the generacy.
3576480	3578760	So while this is doing that
3578760	3581080	and I have a certainty that it's gonna keep doing that
3581080	3583000	because this is super certain
3583000	3584960	and this is likely to return,
3584960	3589720	now I can maybe venture off and try something different
3589720	3592880	because even if I die, even if part of me dies,
3592880	3595720	this is pretty certain to continue.
3595720	3598560	So now I have the capacity to not only
3598560	3601800	should this die continue having the same function
3601800	3604920	but should this die return to that function, right?
3604920	3607880	We both have the capacity to maintain that function
3607880	3610880	where we have high certainty over this path
3610880	3612080	which we know is a good path,
3612080	3614280	but we also have the capacity to learn more
3614280	3617560	and become more fit relative to a landscape
3617560	3619760	and expand such that one day
3619760	3623920	when relative to this system, the black swan arrives,
3625280	3628560	we are so big now, we have so many paths.
3628560	3630560	The black swan is no longer gonna be a black swan,
3630560	3633480	the likelihood of a black swan decreases
3633480	3636640	as you increase your fitness landscape.
3638000	3640120	And have you studied,
3640120	3641560	redundancy is quite an interesting one
3641560	3644120	because presumably the system would learn
3644120	3646440	its own level of redundancy
3646440	3648680	and that must surely be related to things
3648680	3650160	like its behavioral complexity
3650160	3653640	and like its sense world environment and so on.
3653640	3656760	So Alex Kieffer, so we wrote a paper on this specifically
3656760	3661400	and Alex Kieffer wrote a beautiful formalism
3661440	3663960	that basically qualified redundancy
3663960	3668160	as a degree of useless complexity.
3668160	3672400	Anything that is not useless complexity is degeneracy.
3672400	3676560	So I have two hands, but if I use one hand,
3676560	3677760	the other isn't useless, right?
3677760	3678680	It can do other things.
3678680	3680480	We can play the piano, et cetera.
3680480	3682920	Like it has the capacity to have different functions
3682920	3684880	even if it can do the same function.
3684880	3689520	But if I had five hands and they could only do two things,
3689520	3692840	I'd have like three hands too many.
3692840	3695120	Like it wouldn't be able,
3695120	3697480	unless I could create some emergent patterns
3697480	3699080	through the repetition of certain,
3699080	3702520	at some point I would expend a lot of energy
3702520	3706240	in maintaining things which are most often useless.
3706240	3709320	Interesting, but so I agree that
3709320	3712040	we have to be really economical with function
3712040	3713560	because function is our interface.
3713560	3716800	But I was talking earlier about,
3716800	3718560	in this active inference framework,
3718560	3721320	we could potentially swap out agents
3721320	3723320	with different skill programs.
3723320	3726240	So we could have like a marketplace of intelligence.
3726240	3729080	But then it's kind of degenerate
3729080	3732240	to have lots of agents kicking about that we're not using
3732240	3734440	because we're holding onto them on the basis
3734440	3736160	that they might be useful in the future.
3736160	3737720	So we come into a new regime.
3737720	3739040	Oh, let's bring this guy back in
3739040	3741120	because it's started making good predictions again.
3741120	3744840	But I don't wanna be holding these guys around for too long.
3744840	3746600	So I think you mean redundant.
3746600	3747560	They would be redundant
3747560	3749400	because they wouldn't be doing any other function.
3749400	3751000	If they were degenerate,
3751000	3754240	they would be trying to expand the fitness landscape
3754240	3756080	and therefore they would never be useless.
3756080	3758120	Well, I would say that they are harming
3758120	3761320	the intelligence of the overall collective
3761320	3764080	because they are essentially,
3764080	3767040	they're creating predictions which are not useful.
3767040	3769680	They're polluting the predictive apparatus.
3772200	3774600	But no, I think it's an interesting point.
3774600	3776960	But so come back to what you were saying before.
3777160	3779600	So you said if they are hanging around,
3779600	3781440	they are degeneracy.
3781440	3784960	So what's the difference between redundancy and degeneracy?
3784960	3785800	So that's the thing.
3785800	3787920	I think if they're just hanging around
3787920	3792000	and not doing anything, then they're just redundant.
3792000	3793080	But if they're doing stuff
3793080	3795760	which is rowing in the wrong direction,
3795760	3796720	then it's degeneracy.
3796720	3799200	Yeah, I mean, degeneracy doesn't entail
3799200	3802520	that you're gonna necessarily go in the right direction,
3802520	3804360	but it gives you the possibility to,
3804360	3807160	which means you can increase your fitness landscape.
3807160	3809680	So increasing your fitness landscape
3809680	3811240	also means accruing error.
3811240	3813440	Like you're going to make some errors
3813440	3815560	and some parts of you are gonna die.
3815560	3819600	So it's sometimes okay to continue accruing error.
3819600	3821320	This is why I think that the distinction
3821320	3823880	between degeneracy and redundancy is important.
3823880	3827480	If you're just expanding energy for something
3827480	3830160	which literally doesn't help you right now
3830160	3833320	but is doing the same thing as something else
3833320	3835840	which would potentially help you,
3835840	3838920	and it's not, like there's so much redundancy
3838920	3842600	that let's say given some error,
3843520	3845960	you still wouldn't need it,
3845960	3848960	then it's probably best to pull it away.
3848960	3850760	But if it's doing something else in the meantime,
3850760	3853400	even if it's rowing in the wrong direction,
3853400	3855600	it's still doing something.
3855600	3860040	It's still trying to compute some error
3860040	3861800	and it's possible that over time,
3861800	3865080	you will find that this is just not a thing you ever need.
3865080	3867280	You will have accrued so much error
3867280	3868680	that you'll just push it away.
3868680	3871360	And this is how your system changes over time as well.
3871360	3875760	You don't just necessarily like grow as a blob, right?
3875760	3878440	You sometimes you just prune and you push it away.
3878440	3880680	And it is good for you to learn
3880680	3882520	that you had to get rid of it.
3882520	3884400	You wouldn't necessarily know otherwise.
3884400	3887680	Yeah, yeah, it's stimulating some thoughts actually
3887680	3889280	because I guess it's very similar
3889280	3891200	to Friston's idea of entropy.
3891200	3893960	So, you know, like maintaining behavioral complexity
3893960	3895640	because it might be useful.
3895640	3898880	But there's this thing with gradient optimization.
3898880	3901440	When you do monotonic gradient optimization,
3901440	3905280	you kind of expect to just strip away anything
3905280	3907000	that's not helping me.
3907000	3910680	And sometimes you need to like maintain the degeneracy
3910680	3912000	for a significant amount of time
3912000	3913720	before it becomes useful later.
3913720	3915000	So like in Friston's framework,
3915000	3917320	I think it's like it's keeping it around,
3917320	3918600	but for quite a long time.
3919480	3924480	Yeah, so I think it's all about the amount of energy
3926560	3928960	you have as say capital.
3933800	3935800	And I think it touches into something really interesting
3935800	3937920	about psychology, like hoarders.
3937920	3940080	Why do hoarders exist?
3940080	3942080	Like if you ask them, what's the likelihood
3942080	3946480	of you ever using that thing, like ever at all?
3946480	3948160	And they're like, well, maybe I'll need it.
3948160	3950440	And then you point out to them a situation,
3950440	3952840	well, the situation happened where you would have needed it
3952840	3954320	and you still didn't use it.
3954320	3955280	Now, what do you think?
3955280	3958120	And it just, it doesn't update.
3958120	3962520	So to them, it's like they don't have the right capacity
3962520	3965560	to update their model such that over time,
3965560	3968840	even given that probability and the fact that it happened,
3968840	3970360	et cetera, they can't prune it.
3971720	3973000	But if it doesn't cost you anything,
3973000	3975880	like you just have a, you have an adapter at home
3975880	3978000	and you never go to the US,
3978000	3980200	but you still have it and it's like, whatever,
3980200	3982200	you know, it doesn't cause that much error.
3983200	3988080	Maybe it's worth maintaining the possibility of a policy
3988080	3990120	which would use it.
3990120	3991600	It's more about
3994160	3997360	given the pool of policies you're maintaining,
3997360	3999760	how much does it cost you to maintain a policy
3999760	4001320	that goes all the way over there,
4001320	4004400	even though you can tell your viability is over here.
4004400	4007720	You could choose to do so and maybe it'll be adaptive,
4007720	4010520	but if it isn't, effectively the kind of system
4010520	4013800	that maintains the wrong balance
4013800	4016800	over the type of redundancy and degeneracy
4016800	4018800	that it maintains is just gonna,
4018800	4020960	yeah, it's gonna perish faster than the others.
4020960	4022600	Yeah, yeah.
4022600	4024400	Well, this has been amazing.
4024400	4026840	Thank you so much for chatting with me.
4026840	4028200	You're welcome, it was fun.
