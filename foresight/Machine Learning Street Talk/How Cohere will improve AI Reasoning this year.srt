1
00:00:00,000 --> 00:00:03,680
Previous generations of the model have been weak reasoners,

2
00:00:03,680 --> 00:00:04,560
but they do reason.

3
00:00:04,560 --> 00:00:06,280
In the same way that hallucination

4
00:00:06,280 --> 00:00:08,840
was an existential threat to this technology,

5
00:00:08,840 --> 00:00:11,480
no, we'll never be able to trust this stuff.

6
00:00:11,480 --> 00:00:13,120
There are hundreds of millions of people

7
00:00:13,120 --> 00:00:14,800
using this techno, and they trust it.

8
00:00:14,800 --> 00:00:16,000
It's actually useful for them.

9
00:00:16,000 --> 00:00:18,360
We're making very good progress on the hallucination problem.

10
00:00:18,360 --> 00:00:20,480
I think we'll make very good progress

11
00:00:20,480 --> 00:00:22,360
this year and next on Reasoning.

12
00:00:22,360 --> 00:00:25,360
Here we are again, another episode of MLST.

13
00:00:25,360 --> 00:00:28,600
Today with Aidan Gomez, the CEO of Coheir.

14
00:00:28,600 --> 00:00:31,360
Now, I interviewed Aidan in London a couple of weeks ago

15
00:00:31,360 --> 00:00:32,840
just after their build event

16
00:00:32,840 --> 00:00:35,440
and after Aidan did his presentation,

17
00:00:35,440 --> 00:00:38,840
I sat him down for an hour and I gave him a grilling.

18
00:00:38,840 --> 00:00:40,360
And he was such a good sport

19
00:00:40,360 --> 00:00:42,960
for being so transparent and authentic.

20
00:00:42,960 --> 00:00:45,000
This is the difference with Coheir.

21
00:00:45,000 --> 00:00:46,840
They say it like it is.

22
00:00:46,840 --> 00:00:48,160
There's no bullshit.

23
00:00:48,160 --> 00:00:49,720
There's no digital gods.

24
00:00:49,720 --> 00:00:51,520
There's no super intelligence.

25
00:00:51,520 --> 00:00:53,360
They're just a bunch of folks

26
00:00:53,360 --> 00:00:55,560
solving real world business problems.

27
00:00:55,560 --> 00:00:57,160
Their models are incredibly good

28
00:00:57,160 --> 00:00:59,800
for a true vlogmented generation, for multilingual.

29
00:00:59,800 --> 00:01:02,080
But they also have some serious challenges

30
00:01:02,080 --> 00:01:03,400
that they need to overcome.

31
00:01:03,400 --> 00:01:05,960
We spoke about the AI risk discussion,

32
00:01:05,960 --> 00:01:08,400
where the language models take away our agency,

33
00:01:08,400 --> 00:01:10,120
how he's dealing with policy, right?

34
00:01:10,120 --> 00:01:12,840
So on the one hand, he's talking with governments,

35
00:01:12,840 --> 00:01:15,760
trying to get them to allow startups

36
00:01:15,760 --> 00:01:18,520
to be more competitive, to innovate.

37
00:01:18,520 --> 00:01:21,040
But at the same token, he's also very concerned

38
00:01:21,040 --> 00:01:23,440
about some of the societal risks of AI.

39
00:01:23,440 --> 00:01:25,000
So that's quite an interesting burden

40
00:01:25,000 --> 00:01:26,880
and juxtaposition that he has to hold

41
00:01:27,080 --> 00:01:28,280
in his mind.

42
00:01:28,280 --> 00:01:30,600
We spoke a lot about the company culture at Coheir.

43
00:01:30,600 --> 00:01:32,040
Now, I'm very impressed with Coheir.

44
00:01:32,040 --> 00:01:34,480
All of the people I've spoken to have been very smart,

45
00:01:34,480 --> 00:01:36,280
just really nice people.

46
00:01:36,280 --> 00:01:39,440
And how has he cultivated that culture?

47
00:01:39,440 --> 00:01:41,080
He was very frank and transparent

48
00:01:41,080 --> 00:01:44,360
about some of the mistakes he made early on as a CEO.

49
00:01:44,360 --> 00:01:46,480
So yeah, plenty to get your teeth into.

50
00:01:46,480 --> 00:01:48,280
I hope you enjoy the conversation.

51
00:01:49,440 --> 00:01:51,200
For Coheir, I think we're a little bit different

52
00:01:51,200 --> 00:01:53,160
than some of the other companies in the space,

53
00:01:53,160 --> 00:01:56,520
in the sense that we're not really here to build AGI.

54
00:01:56,520 --> 00:01:59,280
What we're here to do is create value for the world.

55
00:01:59,280 --> 00:02:00,840
And the way that we think we can do that

56
00:02:00,840 --> 00:02:05,000
is by putting this tech into the hands of enterprises

57
00:02:05,000 --> 00:02:07,240
so that they can integrate it into their products,

58
00:02:07,240 --> 00:02:10,640
they can augment their workforce with it.

59
00:02:10,640 --> 00:02:13,440
And so it's all about driving value

60
00:02:13,440 --> 00:02:15,600
and really putting this technology

61
00:02:15,600 --> 00:02:17,880
into the hands of more people

62
00:02:17,880 --> 00:02:21,000
and driving productivity for humanity.

63
00:02:21,840 --> 00:02:23,160
Aiden, welcome to MLSD.

64
00:02:23,160 --> 00:02:25,160
It's an absolute honor to have you on.

65
00:02:25,200 --> 00:02:27,200
Thank you so much. Appreciate it.

66
00:02:27,200 --> 00:02:28,640
There's a bit of a last mile problem

67
00:02:28,640 --> 00:02:29,800
with large language models,

68
00:02:29,800 --> 00:02:33,680
so you folks have created this incredible general technology.

69
00:02:33,680 --> 00:02:36,080
But when enterprises implement it,

70
00:02:36,080 --> 00:02:39,560
they have a whole bunch of legislative constraints,

71
00:02:39,560 --> 00:02:41,280
security constraints.

72
00:02:41,280 --> 00:02:45,600
Yeah, yeah, I think there's loads of barriers to access.

73
00:02:45,600 --> 00:02:50,440
Privacy to policy to just the familiarity of the teams

74
00:02:50,440 --> 00:02:52,560
with the tech, it's brand new.

75
00:02:52,560 --> 00:02:54,920
And so they haven't built with this technology before

76
00:02:54,920 --> 00:02:56,960
and they're still getting up to speed

77
00:02:56,960 --> 00:03:00,800
with the opportunity space, what they can do with it.

78
00:03:00,800 --> 00:03:05,240
That being said, people are so excited about AI

79
00:03:05,240 --> 00:03:08,240
and its opportunity that the motivation and the will

80
00:03:08,240 --> 00:03:10,760
is there to overcome a lot of these hurdles.

81
00:03:10,760 --> 00:03:13,520
So we're trying to help with that as much as we can,

82
00:03:13,520 --> 00:03:17,320
whether it's like our LMU education course

83
00:03:17,320 --> 00:03:19,720
to help general developers get up to speed

84
00:03:19,720 --> 00:03:21,280
on how to build with this stuff

85
00:03:21,280 --> 00:03:23,480
or us engaging at the policy level

86
00:03:23,480 --> 00:03:26,120
to make sure that we have sensible policy

87
00:03:26,120 --> 00:03:28,880
and not over-regulation or regulation

88
00:03:28,880 --> 00:03:33,880
that hurts startups or encumbers industry in adopting it.

89
00:03:34,040 --> 00:03:35,560
So we're trying to pull the levers that we can

90
00:03:35,560 --> 00:03:37,280
to help accelerate the adoption

91
00:03:37,280 --> 00:03:40,560
and make sure that it gets adopted in the right way.

92
00:03:40,560 --> 00:03:42,960
But yeah, no, it's definitely the past two years

93
00:03:42,960 --> 00:03:46,840
have been a push.

94
00:03:46,840 --> 00:03:49,520
There's a lot of stuff slowing down adoption

95
00:03:49,520 --> 00:03:51,360
that I would love to see eased.

96
00:03:51,400 --> 00:03:53,720
I think the tools need to get better, easier to use,

97
00:03:53,720 --> 00:03:55,720
more intuitive, more robust.

98
00:03:55,720 --> 00:03:57,960
Prompt engineering is still a thing.

99
00:03:57,960 --> 00:03:59,520
It shouldn't be right.

100
00:03:59,520 --> 00:04:02,200
It shouldn't matter how you phrase something specifically.

101
00:04:02,200 --> 00:04:04,560
It should be, the model should be smart enough

102
00:04:04,560 --> 00:04:06,680
to generally understand your intent

103
00:04:06,680 --> 00:04:09,520
and take action on your behalf reliably.

104
00:04:09,520 --> 00:04:11,680
So even at the technological layer,

105
00:04:11,680 --> 00:04:14,320
I think us as model builders, we have a lot to do

106
00:04:14,320 --> 00:04:16,480
to bring the barriers down.

107
00:04:16,480 --> 00:04:20,680
But I'm optimistic, like the pace of progress is fantastic.

108
00:04:20,680 --> 00:04:21,760
Yes.

109
00:04:21,760 --> 00:04:25,000
On that kind of prompt Britonist thing,

110
00:04:25,000 --> 00:04:28,080
I wonder whether you think that we are on a path to have it.

111
00:04:28,080 --> 00:04:30,000
I mean, in an ideal world,

112
00:04:30,000 --> 00:04:33,240
the model and the application would be completely decoupled.

113
00:04:33,240 --> 00:04:34,600
So you could swap the model out

114
00:04:34,600 --> 00:04:36,080
or when you folks bring out a new model,

115
00:04:36,080 --> 00:04:38,600
we can just swap it out and nothing breaks.

116
00:04:38,600 --> 00:04:40,360
But at the moment, that's not the case.

117
00:04:40,360 --> 00:04:43,640
But as the models become increasingly better,

118
00:04:43,640 --> 00:04:45,960
do you think they will be robust in that way?

119
00:04:45,960 --> 00:04:47,160
They should be, right?

120
00:04:48,160 --> 00:04:50,840
There will always be quirks to different models

121
00:04:50,840 --> 00:04:52,800
because we're all training on, you know,

122
00:04:52,800 --> 00:04:56,680
we hope different data that focuses on different aspects

123
00:04:56,680 --> 00:05:00,680
or elicits different behavior in the model.

124
00:05:00,680 --> 00:05:05,440
So there will always be quirks to the behavior of models,

125
00:05:05,440 --> 00:05:07,440
the personalities of models,

126
00:05:07,440 --> 00:05:09,200
what they're good at and bad at.

127
00:05:09,200 --> 00:05:13,440
But in general, in terms of like following an instruction,

128
00:05:13,440 --> 00:05:16,680
we should be quite robust to that universally.

129
00:05:18,160 --> 00:05:19,680
And so the ideal is that, yeah,

130
00:05:19,680 --> 00:05:22,680
you can just take a prompt and drop it into any system

131
00:05:22,680 --> 00:05:24,200
and see which one performs best

132
00:05:24,200 --> 00:05:26,320
and then move forward with that one.

133
00:05:26,320 --> 00:05:29,080
In reality, the status quo is a prompt

134
00:05:29,080 --> 00:05:30,160
that works on one system,

135
00:05:30,160 --> 00:05:32,920
fundamentally does not work on another.

136
00:05:32,920 --> 00:05:37,920
And so there's this rift or these walls in between systems

137
00:05:38,240 --> 00:05:40,320
that make them very, very different.

138
00:05:40,320 --> 00:05:42,840
Hopefully that'll start to lift.

139
00:05:42,840 --> 00:05:46,280
There's a lot of effort going into data augmentation

140
00:05:46,280 --> 00:05:47,800
that makes these models more robust

141
00:05:47,800 --> 00:05:50,320
to changes in prompt space.

142
00:05:50,320 --> 00:05:51,720
We're doing a lot of work on that.

143
00:05:51,720 --> 00:05:55,120
It's driven a lot by synthetic data

144
00:05:55,120 --> 00:05:59,560
and finding, doing search to basically find the prompts

145
00:05:59,560 --> 00:06:02,320
or the augmentations that changes to prompts

146
00:06:02,320 --> 00:06:06,920
that break the model and then training to fix that break.

147
00:06:07,880 --> 00:06:09,960
So I'm optimistic that sort of brittleness

148
00:06:09,960 --> 00:06:10,800
is gonna go away.

149
00:06:10,800 --> 00:06:11,800
Interesting.

150
00:06:11,800 --> 00:06:14,400
So kind of finding problems with the model

151
00:06:14,400 --> 00:06:17,400
and then robustifying and robustifying.

152
00:06:17,400 --> 00:06:19,240
In doing so, how does that change

153
00:06:19,240 --> 00:06:21,280
the characteristics of the model?

154
00:06:21,280 --> 00:06:24,480
Does it make it less creative or less capable in some sense?

155
00:06:24,480 --> 00:06:26,440
I mean, what do you ever feel for the trade-offs there?

156
00:06:26,440 --> 00:06:29,240
Yeah, I mean, I don't think so.

157
00:06:29,240 --> 00:06:32,680
I think that that's orthogonal.

158
00:06:32,680 --> 00:06:34,280
The process of making it more robust

159
00:06:34,280 --> 00:06:35,720
is orthogonal to creativity.

160
00:06:35,720 --> 00:06:37,840
There are aspects of the post-training procedure

161
00:06:37,840 --> 00:06:41,920
that do reduce creativity or, you know,

162
00:06:41,960 --> 00:06:44,720
some people like to say like lobotomize the model.

163
00:06:46,000 --> 00:06:47,680
So it's definitely a problem.

164
00:06:47,680 --> 00:06:50,320
It's something that we watch for

165
00:06:50,320 --> 00:06:54,040
and we're trying to prevent.

166
00:06:54,040 --> 00:06:59,040
I would say that one of the most disappointing aspects

167
00:06:59,040 --> 00:07:00,960
of the current regime of building these models

168
00:07:00,960 --> 00:07:04,440
is that a lot of people train on synthetic data

169
00:07:04,440 --> 00:07:05,760
from one source, right?

170
00:07:05,760 --> 00:07:07,720
Like just from the GPT models.

171
00:07:07,720 --> 00:07:09,680
And so all of them, all of the models

172
00:07:09,680 --> 00:07:11,760
that are being created,

173
00:07:11,760 --> 00:07:13,400
they sort of speak the same.

174
00:07:13,400 --> 00:07:15,680
They kind of have the same personality

175
00:07:15,680 --> 00:07:18,560
and it leads to this collapse

176
00:07:18,560 --> 00:07:21,160
into a lot of different models

177
00:07:21,160 --> 00:07:22,640
looking and feeling the same.

178
00:07:25,160 --> 00:07:26,200
And that makes them boring

179
00:07:27,280 --> 00:07:31,800
because like you have the same shortcomings across models

180
00:07:31,800 --> 00:07:34,520
rather than if you have a diverse set of models

181
00:07:34,520 --> 00:07:36,840
that have different failures in different places,

182
00:07:36,840 --> 00:07:39,760
you can much better address, you know,

183
00:07:39,760 --> 00:07:42,800
the preferences of much more people.

184
00:07:44,480 --> 00:07:48,320
I've noticed due to synthetic data taking off,

185
00:07:49,280 --> 00:07:51,600
just a total collapse in terms of

186
00:07:52,600 --> 00:07:57,400
the different types of behavior models exhibit

187
00:07:57,400 --> 00:08:01,280
and that cohere because our customers are enterprises.

188
00:08:01,280 --> 00:08:02,360
Like that's who we sell to.

189
00:08:02,360 --> 00:08:04,080
It's not consumers, it's not, you know,

190
00:08:04,080 --> 00:08:06,240
anything other than enterprises

191
00:08:06,240 --> 00:08:08,600
who want to adopt this tech.

192
00:08:08,600 --> 00:08:10,040
And they're very, very sensitive

193
00:08:10,040 --> 00:08:11,840
to what data went into the model.

194
00:08:11,840 --> 00:08:14,800
And so we exclude other model providers,

195
00:08:14,800 --> 00:08:18,040
data, you know, very aggressively.

196
00:08:18,040 --> 00:08:19,320
Of course, some will slip in

197
00:08:19,320 --> 00:08:21,120
as we're scraping the web, et cetera.

198
00:08:21,120 --> 00:08:23,000
But we make a very concerted effort

199
00:08:23,000 --> 00:08:26,280
to avoid other model outputs.

200
00:08:26,280 --> 00:08:28,320
And so if you talk to our model,

201
00:08:28,320 --> 00:08:30,280
when we release command R and R plus,

202
00:08:30,280 --> 00:08:33,280
one of the things I kept reading on Reddit and Twitter

203
00:08:33,280 --> 00:08:34,600
was it feels different.

204
00:08:34,600 --> 00:08:37,680
Like something feels special about this model.

205
00:08:37,680 --> 00:08:40,360
I don't think that's any like magic at cohere

206
00:08:40,360 --> 00:08:41,800
other than the fact that we didn't do

207
00:08:41,800 --> 00:08:43,240
what the other guys are doing,

208
00:08:43,240 --> 00:08:46,360
which is training on the model outputs of OpenAI.

209
00:08:46,360 --> 00:08:47,200
I agree with you.

210
00:08:47,200 --> 00:08:50,040
So when people, you know,

211
00:08:50,040 --> 00:08:52,000
see chat GPT or whatever for the first time,

212
00:08:52,000 --> 00:08:53,760
that they're blown away by it.

213
00:08:53,760 --> 00:08:57,200
But there are motifs that come up again and again

214
00:08:57,200 --> 00:09:00,040
and again, unraveling the mysteries, you know,

215
00:09:00,040 --> 00:09:02,640
delving into the intricate complexities

216
00:09:02,640 --> 00:09:04,000
or blah, blah, blah, blah, blah.

217
00:09:04,000 --> 00:09:06,320
And when you start to see these patterns

218
00:09:06,320 --> 00:09:09,080
and these constructions, you just start to think,

219
00:09:09,080 --> 00:09:10,720
oh, I don't like this very much

220
00:09:10,720 --> 00:09:12,080
because you start to see through it.

221
00:09:12,080 --> 00:09:13,160
It's a little bit like, you know,

222
00:09:13,160 --> 00:09:14,800
when you start to see through someone,

223
00:09:14,800 --> 00:09:16,520
they're not interesting anymore.

224
00:09:16,520 --> 00:09:18,720
And I haven't seen that with cohere,

225
00:09:18,720 --> 00:09:21,520
but I have seen it with many of the other models.

226
00:09:21,520 --> 00:09:24,040
Now, my intuition was always,

227
00:09:24,040 --> 00:09:25,840
I don't, I haven't really formed this very well,

228
00:09:25,840 --> 00:09:28,400
but I thought that maybe it could come from

229
00:09:28,400 --> 00:09:30,280
just the kind of data sets that we're using,

230
00:09:30,280 --> 00:09:34,040
or maybe it could come from the preference fine tuning.

231
00:09:34,400 --> 00:09:36,960
Are you saying that that monolithic effect

232
00:09:36,960 --> 00:09:40,080
is because they're kind of eating each other's poop?

233
00:09:40,080 --> 00:09:41,640
Yeah, no, yeah, yeah.

234
00:09:41,640 --> 00:09:44,440
It's some sort of like human centipede effect.

235
00:09:44,440 --> 00:09:48,960
I think, yeah, they're training on the outputs

236
00:09:48,960 --> 00:09:50,360
of a single model.

237
00:09:50,360 --> 00:09:52,960
And so it's all collapsing into that model's

238
00:09:52,960 --> 00:09:54,000
output distribution.

239
00:09:54,000 --> 00:09:56,080
And so if that output distribution has quirks

240
00:09:56,080 --> 00:09:59,440
like saying the word delve a lot,

241
00:09:59,440 --> 00:10:01,400
then it's gonna just pop up all over the place.

242
00:10:01,400 --> 00:10:03,880
And people will take it for granted that,

243
00:10:03,880 --> 00:10:06,280
oh, I guess LLMs just behave like this,

244
00:10:06,280 --> 00:10:07,640
but they don't have to.

245
00:10:07,640 --> 00:10:08,480
They don't have to.

246
00:10:08,480 --> 00:10:10,760
It's interesting how subjective creativity is as well,

247
00:10:10,760 --> 00:10:11,840
because a lot of people thought

248
00:10:11,840 --> 00:10:13,880
that it was creative a couple of years ago.

249
00:10:13,880 --> 00:10:14,840
And then when you see it everywhere,

250
00:10:14,840 --> 00:10:15,800
it's not creative anymore.

251
00:10:15,800 --> 00:10:18,280
So it needs to be novel to be creative.

252
00:10:18,280 --> 00:10:21,160
But I mean, you folks have just released

253
00:10:21,160 --> 00:10:22,720
the command R series of models

254
00:10:22,720 --> 00:10:25,480
and you've blown everyone away.

255
00:10:25,480 --> 00:10:26,360
Tell me about them.

256
00:10:26,360 --> 00:10:28,080
But if you wouldn't mind also,

257
00:10:28,080 --> 00:10:30,400
why did it take you a while to catch up

258
00:10:30,400 --> 00:10:31,800
and get state of the art performance?

259
00:10:31,840 --> 00:10:36,280
Yeah, we spent a lot of 2023 lagging.

260
00:10:36,280 --> 00:10:38,520
I think that that is accurate to say.

261
00:10:39,400 --> 00:10:43,880
What we were doing was sort of reorganizing internally.

262
00:10:43,880 --> 00:10:45,200
We were rebuilding the company,

263
00:10:45,200 --> 00:10:48,480
rebuilding the modeling team, the tech strategy

264
00:10:48,480 --> 00:10:52,800
and preparing for the runs that led to command R.

265
00:10:52,800 --> 00:10:54,120
It was clear to us that the process

266
00:10:54,120 --> 00:10:56,280
that we had used to build the first command

267
00:10:56,280 --> 00:10:58,960
and generations before that, it wasn't working.

268
00:10:58,960 --> 00:11:00,280
It wasn't gonna scale.

269
00:11:00,280 --> 00:11:03,520
And so we just rethought the entire pipeline.

270
00:11:03,520 --> 00:11:06,520
And it took us a while to rebuild things,

271
00:11:07,840 --> 00:11:10,360
run the experiments that we needed to run

272
00:11:10,360 --> 00:11:12,440
in order to make decisions on what the design

273
00:11:12,440 --> 00:11:17,160
of this new model building engine would look like.

274
00:11:19,040 --> 00:11:21,040
And then it takes time to do the runs.

275
00:11:21,040 --> 00:11:23,080
We spent a lot of last year doing that.

276
00:11:23,080 --> 00:11:25,880
But I think the results speak for themselves.

277
00:11:25,880 --> 00:11:28,120
And also with command R and R plus,

278
00:11:28,120 --> 00:11:31,880
it's just the first step in a series of new models

279
00:11:31,880 --> 00:11:33,200
that we wanna produce.

280
00:11:33,200 --> 00:11:37,320
We're very excited to lean into specific capabilities.

281
00:11:37,320 --> 00:11:41,720
And so while the general language model improvements,

282
00:11:41,720 --> 00:11:42,760
they're super important.

283
00:11:42,760 --> 00:11:43,600
They're crucial, right?

284
00:11:43,600 --> 00:11:44,720
Like the models have to get smarter.

285
00:11:44,720 --> 00:11:47,120
They have to get more capable.

286
00:11:47,120 --> 00:11:49,820
And we'll continue to press on that direction.

287
00:11:50,760 --> 00:11:54,080
We care about narrowing our focus a bit more.

288
00:11:54,120 --> 00:11:58,360
And so for 2024, even with the command R series,

289
00:11:58,360 --> 00:12:00,680
we focused in on rag and tool use.

290
00:12:00,680 --> 00:12:03,080
I think you're gonna see a continuation

291
00:12:03,080 --> 00:12:05,520
and extension of that focus

292
00:12:05,520 --> 00:12:07,600
really making these models robust

293
00:12:07,600 --> 00:12:10,600
at the key capabilities that enterprise cares about,

294
00:12:10,600 --> 00:12:12,000
that will drive productivity,

295
00:12:12,000 --> 00:12:14,720
that will automate really sophisticated processes

296
00:12:14,720 --> 00:12:19,160
that today, as humanity knows it,

297
00:12:19,160 --> 00:12:21,400
is only the domain of humans.

298
00:12:21,400 --> 00:12:22,920
We really wanna go after that.

299
00:12:23,920 --> 00:12:28,920
And give our models the ability to help in those spaces.

300
00:12:29,120 --> 00:12:30,680
Is it fair to say that,

301
00:12:30,680 --> 00:12:31,840
I don't know whether you feel

302
00:12:31,840 --> 00:12:35,000
that the general large language models are saturating.

303
00:12:35,000 --> 00:12:36,240
Maybe you could comment on that first,

304
00:12:36,240 --> 00:12:38,000
but if you do think that,

305
00:12:39,000 --> 00:12:40,440
does that give me a bit of a read

306
00:12:40,440 --> 00:12:43,800
that there's a move towards specialization of the models?

307
00:12:43,800 --> 00:12:45,560
I don't think they're saturating.

308
00:12:45,560 --> 00:12:46,960
I think they're getting so good

309
00:12:46,960 --> 00:12:49,720
that it's hard to see the incremental improvement.

310
00:12:50,720 --> 00:12:55,480
But that incremental improvement is extremely important.

311
00:12:55,480 --> 00:12:58,880
So once the models are smarter than you,

312
00:12:58,880 --> 00:13:02,120
it's really hard to, in a domain, in medicine,

313
00:13:02,120 --> 00:13:04,200
like Coher's model,

314
00:13:04,200 --> 00:13:07,960
it knows more than I do about medicine, for sure.

315
00:13:07,960 --> 00:13:09,160
Just absolutely.

316
00:13:09,160 --> 00:13:11,280
And so I can't really effectively assess

317
00:13:11,280 --> 00:13:14,000
whether we're improving in that dimension.

318
00:13:14,000 --> 00:13:15,240
I can't tell anyone.

319
00:13:15,240 --> 00:13:16,640
It's smarter than me.

320
00:13:16,640 --> 00:13:18,600
I trust it more than I trust myself

321
00:13:18,600 --> 00:13:23,440
to diagnose symptoms or process medical data.

322
00:13:23,440 --> 00:13:24,720
And so I'm not equipped to do that.

323
00:13:24,720 --> 00:13:27,040
Instead, what we need to do is create data sets

324
00:13:27,040 --> 00:13:30,000
or go out and find people who are still better

325
00:13:30,000 --> 00:13:32,560
than the model at those domains.

326
00:13:32,560 --> 00:13:35,560
And they can tell me whether it's improving.

327
00:13:35,560 --> 00:13:38,480
But for us, like the general population,

328
00:13:39,600 --> 00:13:43,920
at some point, we kind of stop seeing improvement

329
00:13:43,920 --> 00:13:45,040
between model versions.

330
00:13:45,040 --> 00:13:46,640
It's harder to feel.

331
00:13:46,960 --> 00:13:49,400
And you need to really zoom in

332
00:13:49,400 --> 00:13:51,080
to a place that you're an expert

333
00:13:51,080 --> 00:13:53,760
and that you know previous generations failed

334
00:13:53,760 --> 00:13:55,160
to see the progress.

335
00:13:56,200 --> 00:13:58,000
I think about it sometimes as like

336
00:13:59,360 --> 00:14:03,040
painting in a canvas of knowledge.

337
00:14:03,040 --> 00:14:06,560
And at some point, the holes in the canvas become so small.

338
00:14:06,560 --> 00:14:08,000
You have to take out a microscope

339
00:14:08,000 --> 00:14:10,200
to actually see it and paint it in.

340
00:14:10,200 --> 00:14:13,640
We're sort of in that part of the space for these models.

341
00:14:13,640 --> 00:14:15,560
And so improvement becomes much harder

342
00:14:15,560 --> 00:14:17,440
for us, the model builders,

343
00:14:17,440 --> 00:14:20,520
but it's much harder to feel and see for users

344
00:14:20,520 --> 00:14:25,520
who aren't diving in very close to analyze performance.

345
00:14:26,080 --> 00:14:27,160
I don't think it's saturating.

346
00:14:27,160 --> 00:14:29,040
I think it's still making,

347
00:14:29,040 --> 00:14:32,000
we're still making very significant progress.

348
00:14:32,000 --> 00:14:35,040
I do think the past 18 months,

349
00:14:36,280 --> 00:14:37,960
maybe a little bit less than 18 months.

350
00:14:37,960 --> 00:14:40,760
Yeah, the past 18 months, 12 months,

351
00:14:40,760 --> 00:14:43,920
we've been compressing.

352
00:14:44,920 --> 00:14:47,360
So we built these massive, giant,

353
00:14:47,360 --> 00:14:49,680
multi-trillion parameter models,

354
00:14:49,680 --> 00:14:53,200
which were just extraordinary artifacts

355
00:14:53,200 --> 00:14:56,240
of intelligence and capability.

356
00:14:56,240 --> 00:14:59,960
And we realized it's impractical.

357
00:14:59,960 --> 00:15:02,440
You can't actually put this thing into production, right?

358
00:15:02,440 --> 00:15:05,520
Like it takes 60, a 100s to certainly,

359
00:15:05,520 --> 00:15:08,880
it just, we could not productionize this.

360
00:15:08,880 --> 00:15:10,760
The economics don't work out.

361
00:15:10,760 --> 00:15:13,280
And so then we spent the year compressing

362
00:15:13,280 --> 00:15:16,800
those massive models down into much smaller form factors.

363
00:15:18,680 --> 00:15:23,680
There's very likely going to be a series of re-expansion

364
00:15:23,960 --> 00:15:28,960
and scale, both on the model scale

365
00:15:30,320 --> 00:15:34,800
in terms of parameters, but also data scale and data quality.

366
00:15:34,800 --> 00:15:37,960
And that's being supported by much better

367
00:15:37,960 --> 00:15:40,680
synthetic data methods that find much more useful

368
00:15:40,680 --> 00:15:44,520
synthetic data that are quite compelling at search

369
00:15:44,520 --> 00:15:48,560
to discover, to automatically discover weak points of models

370
00:15:48,560 --> 00:15:50,680
and then close those gaps.

371
00:15:51,760 --> 00:15:54,320
So I think we've, over the past year,

372
00:15:54,320 --> 00:15:58,320
gotten very good at making models more efficient.

373
00:15:58,320 --> 00:16:03,080
And we've created new methods that let us

374
00:16:03,080 --> 00:16:06,320
sort of just like plug in compute and data

375
00:16:06,320 --> 00:16:09,280
and have the model continuously improve.

376
00:16:09,280 --> 00:16:13,240
You've used words like smart and capabilities.

377
00:16:13,240 --> 00:16:16,760
And if you think of smart as knowledge,

378
00:16:16,760 --> 00:16:17,720
I completely agree with you.

379
00:16:17,720 --> 00:16:20,640
I think knowledge is a living thing.

380
00:16:20,640 --> 00:16:23,480
We're all improvising and we are generating

381
00:16:23,480 --> 00:16:24,760
new knowledge all the time.

382
00:16:24,760 --> 00:16:27,400
And it just increases exponentially.

383
00:16:27,400 --> 00:16:29,440
And there's no reason why language models

384
00:16:29,440 --> 00:16:31,200
can't become more and more and more knowledgeable

385
00:16:31,200 --> 00:16:33,960
because we just acquire more and more data.

386
00:16:33,960 --> 00:16:37,520
And in that sense, a medical doctor is smarter than me

387
00:16:37,520 --> 00:16:39,240
in that domain because they have the knowledge

388
00:16:39,440 --> 00:16:40,680
that I don't have.

389
00:16:40,680 --> 00:16:42,920
But some people could say that intelligence

390
00:16:42,920 --> 00:16:45,640
is something a little bit more abstract than that.

391
00:16:45,640 --> 00:16:48,600
It might be the ability to build models.

392
00:16:49,640 --> 00:16:51,200
It might be the ability to reason.

393
00:16:51,200 --> 00:16:53,240
It might be the ability to plan.

394
00:16:53,240 --> 00:16:56,040
And this is when we get into the kind of the age your own

395
00:16:56,040 --> 00:16:57,400
thing.

396
00:16:57,400 --> 00:17:00,240
So how do you demarcate those things?

397
00:17:00,240 --> 00:17:02,480
Are you saying that the models are becoming more knowledgeable

398
00:17:02,480 --> 00:17:04,600
but they're not necessarily becoming more intelligent

399
00:17:04,600 --> 00:17:05,520
like we are?

400
00:17:05,520 --> 00:17:10,520
I think reasoning is crucial to intelligence.

401
00:17:13,120 --> 00:17:16,200
I think that these models can reason

402
00:17:16,200 --> 00:17:18,080
and that's a controversial claim.

403
00:17:18,080 --> 00:17:20,160
I think a lot of people would debate that.

404
00:17:20,160 --> 00:17:21,320
A lot of people would make the claim

405
00:17:21,320 --> 00:17:22,960
that the architectures we're using

406
00:17:22,960 --> 00:17:25,240
or the methods we're using don't support

407
00:17:26,600 --> 00:17:27,920
that sort of behavior.

408
00:17:27,920 --> 00:17:31,400
I think that previous generations of the model

409
00:17:31,400 --> 00:17:34,320
have been weak reasoners, but they do reason.

410
00:17:35,560 --> 00:17:38,280
And it's not a discreet,

411
00:17:38,280 --> 00:17:40,040
does it have this capability or not?

412
00:17:40,040 --> 00:17:42,840
It's a continuum of how robust the reasoning engine

413
00:17:42,840 --> 00:17:44,480
inside these models is.

414
00:17:45,440 --> 00:17:47,240
We're getting much better methods

415
00:17:47,240 --> 00:17:50,320
for improving reasoning generally.

416
00:17:50,320 --> 00:17:51,640
We're getting much better methods

417
00:17:51,640 --> 00:17:54,120
of eliciting that behavior from the models

418
00:17:54,120 --> 00:17:55,920
and teaching them how to do it

419
00:17:55,920 --> 00:17:57,880
and apply it to many different domains,

420
00:17:57,880 --> 00:18:01,960
whether it's math, whether it's decision-making tasks,

421
00:18:01,960 --> 00:18:04,860
breaking down tasks, planning how to execute them.

422
00:18:06,440 --> 00:18:09,720
Those were key missing capabilities

423
00:18:09,720 --> 00:18:13,080
that were quite weak in previous generations of models,

424
00:18:13,080 --> 00:18:15,560
which are now starting to emerge

425
00:18:16,880 --> 00:18:19,960
in a significantly more robust fashion.

426
00:18:19,960 --> 00:18:22,760
And so in the same way that hallucination

427
00:18:22,760 --> 00:18:26,280
was it used to be an existential threat to this technology,

428
00:18:26,280 --> 00:18:28,520
no, we'll never be able to trust this stuff.

429
00:18:29,520 --> 00:18:32,160
There are hundreds of millions of people using this techno

430
00:18:32,160 --> 00:18:33,000
and they trust it.

431
00:18:33,000 --> 00:18:33,960
It's actually useful for them.

432
00:18:33,960 --> 00:18:36,280
They use it because it's useful to their job.

433
00:18:36,280 --> 00:18:38,680
We're making very good progress on the hallucination problem.

434
00:18:38,680 --> 00:18:40,840
I think we'll make very good progress

435
00:18:40,840 --> 00:18:43,160
this year and next on reasoning.

436
00:18:43,160 --> 00:18:46,640
I think it's just a capability,

437
00:18:46,640 --> 00:18:51,640
a skill that the model needs to be taught.

438
00:18:51,800 --> 00:18:54,560
And we're building the methods and data

439
00:18:54,560 --> 00:18:59,560
and techniques to support teaching, teaching these models.

440
00:18:59,600 --> 00:19:01,440
Yeah, it is interesting how you can kind of break

441
00:19:01,440 --> 00:19:03,240
intelligence down to all of these things

442
00:19:03,240 --> 00:19:05,240
and some you might argue are missing now,

443
00:19:05,240 --> 00:19:09,680
like planning, creativity is an interesting one.

444
00:19:09,680 --> 00:19:11,720
Agency is quite an interesting one.

445
00:19:11,720 --> 00:19:15,000
And presumably as a thing has more understanding

446
00:19:15,000 --> 00:19:16,280
and it has more autonomy,

447
00:19:16,280 --> 00:19:19,000
it could in principle develop agency

448
00:19:19,000 --> 00:19:19,960
at some point in the future.

449
00:19:19,960 --> 00:19:22,440
But you think of these things as skills.

450
00:19:22,440 --> 00:19:25,600
Could you give any hints to how you've moved the needle

451
00:19:25,600 --> 00:19:26,440
on this?

452
00:19:26,440 --> 00:19:27,880
So the knowledge thing, it seems to me

453
00:19:27,880 --> 00:19:29,840
that you would just get more data

454
00:19:29,840 --> 00:19:31,440
and curate and refine the data.

455
00:19:31,440 --> 00:19:32,880
But could you give any hints

456
00:19:32,880 --> 00:19:35,080
on how you've made it better at reasoning, for example?

457
00:19:35,080 --> 00:19:35,920
Yeah, with knowledge,

458
00:19:35,920 --> 00:19:38,720
I think it's about augmentation with RAG

459
00:19:38,720 --> 00:19:42,040
and better modeling techniques, cleaner data sets

460
00:19:42,040 --> 00:19:44,720
so that you remember the right stuff

461
00:19:44,720 --> 00:19:48,200
and don't retain the less relevant stuff.

462
00:19:49,560 --> 00:19:52,400
Those are the techniques that move the needle there.

463
00:19:52,400 --> 00:19:55,360
With reasoning, there are like circuits

464
00:19:55,360 --> 00:19:57,640
that you really need to bake in to the model.

465
00:19:57,640 --> 00:19:59,560
You need to show it and demonstrate it,

466
00:19:59,600 --> 00:20:02,160
how to break down tasks at a very low level,

467
00:20:03,120 --> 00:20:04,680
think through them.

468
00:20:04,680 --> 00:20:06,760
And that's stuff that's not actually

469
00:20:06,760 --> 00:20:08,880
that abundant on the internet.

470
00:20:08,880 --> 00:20:12,160
So it doesn't come for free using our previous techniques

471
00:20:12,160 --> 00:20:15,040
of just scrape the web and train the model and scale up.

472
00:20:15,040 --> 00:20:18,520
People don't usually write out their inner monologue, right?

473
00:20:18,520 --> 00:20:20,720
They usually write the results of that inner monologue.

474
00:20:20,720 --> 00:20:23,520
And so it's something that the model

475
00:20:23,520 --> 00:20:26,560
has been missing a view into.

476
00:20:26,560 --> 00:20:28,800
I think synthetic data will go a long way

477
00:20:28,800 --> 00:20:31,760
in closing that gap and supporting building

478
00:20:32,760 --> 00:20:36,400
multi-trillion token data sets

479
00:20:36,400 --> 00:20:40,280
that actually demonstrate how to have an inner monologue,

480
00:20:40,280 --> 00:20:41,480
how to reason through things,

481
00:20:41,480 --> 00:20:44,440
how to think through problems, make mistakes,

482
00:20:44,440 --> 00:20:47,680
identify mistakes, correct them and retry.

483
00:20:47,680 --> 00:20:50,440
That sort of long thought process data

484
00:20:50,440 --> 00:20:53,680
is actually extremely scarce.

485
00:20:53,680 --> 00:20:54,520
It's very rare.

486
00:20:54,520 --> 00:20:55,360
It's very rare.

487
00:20:55,360 --> 00:20:56,200
It's really hard to find.

488
00:20:56,200 --> 00:20:57,040
You can find it on the internet, of course.

489
00:20:57,240 --> 00:21:02,240
Stuff like forums where people help each other

490
00:21:02,280 --> 00:21:03,880
with homework and sort of break down.

491
00:21:03,880 --> 00:21:06,560
This is how I arrived at this answer.

492
00:21:06,560 --> 00:21:09,120
But when you look at the internet in totality,

493
00:21:09,120 --> 00:21:12,560
those are like pinpricks on the surface of this thing.

494
00:21:12,560 --> 00:21:15,040
And so pulling that forward, emphasizing it,

495
00:21:15,040 --> 00:21:18,240
augmenting it, producing more of that data

496
00:21:18,240 --> 00:21:22,240
should be a key priority if you're gonna actually

497
00:21:22,240 --> 00:21:24,920
teach these models to exhibit that behavior.

498
00:21:24,920 --> 00:21:26,560
Is there a trade-off between,

499
00:21:26,600 --> 00:21:29,800
I mean, for example, we could use the Unreal Engine

500
00:21:29,800 --> 00:21:32,320
to generate lots of visual training data

501
00:21:32,320 --> 00:21:34,680
for a Vision Foundation model.

502
00:21:34,680 --> 00:21:36,680
Or an alternative would be we could have

503
00:21:36,680 --> 00:21:39,400
like some kind of hybrid prediction architecture

504
00:21:39,400 --> 00:21:42,520
where we somehow encode naive physics

505
00:21:42,520 --> 00:21:43,960
into the architecture itself,

506
00:21:43,960 --> 00:21:47,320
which means rather than memorizing lots of generated data,

507
00:21:47,320 --> 00:21:50,920
we just kind of build a hybrid architecture.

508
00:21:50,920 --> 00:21:53,760
Is that a trade-off that you're kind of thinking about?

509
00:21:53,760 --> 00:21:56,280
Like specifically with the video side of things

510
00:21:56,320 --> 00:21:58,560
where physics is relevant, I think that's

511
00:22:00,040 --> 00:22:01,600
a totally fine strategy.

512
00:22:01,600 --> 00:22:05,080
I think that, yeah, a lot of the physics engines

513
00:22:05,080 --> 00:22:09,440
that people have built are, they're flawed, right?

514
00:22:09,440 --> 00:22:12,600
Like video games still don't look like reality.

515
00:22:12,600 --> 00:22:14,640
They still don't behave like reality.

516
00:22:14,640 --> 00:22:17,040
And so training off of that data,

517
00:22:17,040 --> 00:22:21,840
I think will leave you in a really unsatisfying place.

518
00:22:21,880 --> 00:22:26,480
Like there's just still some Uncanny Valley weirdness to it.

519
00:22:28,080 --> 00:22:30,680
I think like we have tons of actual video data

520
00:22:30,680 --> 00:22:34,120
of the real world where physics is definitely implemented

521
00:22:34,120 --> 00:22:37,000
and being represented completely accurately.

522
00:22:37,000 --> 00:22:40,480
And so that should be our go-to source.

523
00:22:40,480 --> 00:22:45,480
I think trying to use simulators at this stage

524
00:22:45,920 --> 00:22:46,760
is the wrong approach.

525
00:22:46,760 --> 00:22:48,680
I think you should take as much data

526
00:22:48,680 --> 00:22:50,000
from the real world as you can

527
00:22:50,000 --> 00:22:51,600
and use that as a bootstrap

528
00:22:51,600 --> 00:22:53,720
to then build synthetic data engines

529
00:22:53,720 --> 00:22:56,320
that help you iteratively improve.

530
00:22:56,320 --> 00:22:58,120
It's what happened in language as well, right?

531
00:22:58,120 --> 00:23:01,640
Like we didn't go to synthetic language,

532
00:23:01,640 --> 00:23:04,160
rules-based synthetic language generators

533
00:23:04,160 --> 00:23:05,480
to teach our language models,

534
00:23:05,480 --> 00:23:07,520
the basic principles of language

535
00:23:07,520 --> 00:23:11,360
using our linguistic models that we've built.

536
00:23:11,360 --> 00:23:13,200
No, we threw all that away.

537
00:23:13,200 --> 00:23:17,360
We took actual language data from humans, trained on that,

538
00:23:17,360 --> 00:23:20,280
and then used the models that were the output of that

539
00:23:20,320 --> 00:23:25,320
to improve iteratively and via experimentation.

540
00:23:27,040 --> 00:23:29,600
I think the same will be true in vision.

541
00:23:29,600 --> 00:23:32,160
That's a really interesting point, actually.

542
00:23:32,160 --> 00:23:34,400
Because with the SORA model from OpenAI,

543
00:23:34,400 --> 00:23:35,720
it does look a bit weird.

544
00:23:35,720 --> 00:23:37,160
It looks like it's always flying

545
00:23:37,160 --> 00:23:39,760
and it looks very game engine-like.

546
00:23:39,760 --> 00:23:42,320
And the language example is beautiful.

547
00:23:42,320 --> 00:23:44,960
But what about something like mathematics?

548
00:23:44,960 --> 00:23:48,560
Are there examples where rather than kind of, you know,

549
00:23:48,560 --> 00:23:51,360
perturbing or mutating what already exists,

550
00:23:51,360 --> 00:23:54,280
you might just start from first principles and rules?

551
00:23:54,280 --> 00:23:55,280
Totally, yeah.

552
00:23:55,280 --> 00:24:00,280
Like mathematics is so explicitly rule-driven

553
00:24:01,040 --> 00:24:04,000
and so explicitly verifiable.

554
00:24:04,000 --> 00:24:08,600
It's like the perfect example of synthetic data generation.

555
00:24:08,600 --> 00:24:13,600
I think it's definitely one of the domains

556
00:24:13,760 --> 00:24:15,240
that will crack first.

557
00:24:15,440 --> 00:24:19,240
And on top of that, code, right?

558
00:24:19,240 --> 00:24:22,280
You can completely, synthetically generate code, verify it.

559
00:24:22,280 --> 00:24:23,120
Does it run?

560
00:24:23,120 --> 00:24:26,560
Does it produce the outputs that you want on a test set?

561
00:24:27,960 --> 00:24:31,240
So when it is that explicit and verifiable,

562
00:24:31,240 --> 00:24:33,680
it's perfect for synthetic data generation,

563
00:24:33,680 --> 00:24:34,880
like just ideal.

564
00:24:34,880 --> 00:24:37,840
Yeah, but I guess this is kind of what I'm thinking about.

565
00:24:37,840 --> 00:24:40,680
That with code, you can actually constrain it way more

566
00:24:40,680 --> 00:24:41,880
than language.

567
00:24:41,880 --> 00:24:45,520
So you could, rather than using an existing self-attention

568
00:24:45,520 --> 00:24:47,680
transformer, you know, you might want to have something

569
00:24:47,680 --> 00:24:50,080
that only works on trees or whatever.

570
00:24:50,080 --> 00:24:52,960
And maybe that would work better for that particular thing.

571
00:24:52,960 --> 00:24:55,240
But then I guess we'd have to have some kind of mixture

572
00:24:55,240 --> 00:24:58,640
of experts and not have a single model.

573
00:24:58,640 --> 00:25:02,640
Yeah, I think that's behind the scenes actually,

574
00:25:02,640 --> 00:25:04,040
a lot of the strategy.

575
00:25:04,040 --> 00:25:07,440
You'll likely have an MOE where one component,

576
00:25:07,440 --> 00:25:10,000
one of those experts is gonna be an expert in code,

577
00:25:10,000 --> 00:25:11,800
very highly specialized to that.

578
00:25:11,840 --> 00:25:14,600
Heavily upsampled on synthetic code data

579
00:25:14,600 --> 00:25:17,480
and real code data, math, et cetera.

580
00:25:18,760 --> 00:25:22,120
And that expert will act as a general reasoning engine

581
00:25:22,120 --> 00:25:23,680
and will be very good at logic

582
00:25:23,680 --> 00:25:25,760
and those sorts of components.

583
00:25:25,760 --> 00:25:28,320
You might have a medical expert,

584
00:25:28,320 --> 00:25:31,960
which has dramatic upsampling along that axis.

585
00:25:34,200 --> 00:25:39,200
Yeah, I think that's a very effective path towards

586
00:25:39,440 --> 00:25:41,580
even more efficient models.

587
00:25:41,580 --> 00:25:43,740
So if you're in the medical domain

588
00:25:43,740 --> 00:25:45,940
or the math or code domain,

589
00:25:45,940 --> 00:25:47,980
you can then pull out that expert

590
00:25:47,980 --> 00:25:49,420
and use it independently.

591
00:25:49,420 --> 00:25:54,420
You don't need to keep around this huge monolithic model.

592
00:25:54,660 --> 00:25:57,460
You can just take out a sub-component and deploy that.

593
00:25:59,380 --> 00:26:02,180
Yeah, I think that architecture already exists.

594
00:26:02,180 --> 00:26:03,820
Yeah, I wonder if you can talk to that a little bit

595
00:26:03,820 --> 00:26:05,180
because I'm very excited about that

596
00:26:05,180 --> 00:26:08,060
because it now seems that maybe we could call

597
00:26:08,060 --> 00:26:12,420
what you just described an agentic distributed AI system

598
00:26:12,420 --> 00:26:15,820
where the agents can pass messages to each other

599
00:26:15,820 --> 00:26:18,500
and one of them might be an expert in mathematics,

600
00:26:18,500 --> 00:26:20,860
one of them might be an expert in coding or so on.

601
00:26:20,860 --> 00:26:21,860
But then you've got this problem

602
00:26:21,860 --> 00:26:24,300
that you kind of send a message into the nexus

603
00:26:24,300 --> 00:26:26,860
and all of the models are kind of passing messages

604
00:26:26,860 --> 00:26:30,460
to each other and it's kind of unbounded in runtime

605
00:26:30,460 --> 00:26:31,860
as opposed to one of the great things

606
00:26:31,860 --> 00:26:33,340
with the language model is

607
00:26:33,340 --> 00:26:35,940
it just does a fixed amount of compute per iteration.

608
00:26:36,020 --> 00:26:38,020
You just put some prompt in

609
00:26:38,020 --> 00:26:40,180
and you get the answer straight back out.

610
00:26:40,180 --> 00:26:44,020
So does that kind of unboundedness introduce problems?

611
00:26:45,060 --> 00:26:47,940
I mean, a language model could just generate infinitely

612
00:26:47,940 --> 00:26:49,580
and not produce a stop token

613
00:26:49,580 --> 00:26:51,580
and you would go on forever.

614
00:26:51,580 --> 00:26:53,660
So I think the problem already exists

615
00:26:53,660 --> 00:26:56,020
and models are quite well-behaved in terms of,

616
00:26:58,700 --> 00:27:02,020
if you train them to give up

617
00:27:02,020 --> 00:27:05,220
and to say, I need to respond, they will.

618
00:27:05,940 --> 00:27:06,940
They tend to.

619
00:27:07,820 --> 00:27:09,900
So I'm not too concerned about like,

620
00:27:09,900 --> 00:27:13,180
runaway processes that would just not be useful

621
00:27:13,180 --> 00:27:16,180
as well, hugely computationally expensive.

622
00:27:16,180 --> 00:27:19,060
And yeah, it seems like models can produce stop tokens.

623
00:27:19,060 --> 00:27:22,700
And I think that even in a multi-agent scenario,

624
00:27:24,340 --> 00:27:27,380
discourse between agents will conclude itself

625
00:27:27,380 --> 00:27:28,540
in a reasonable amount of time.

626
00:27:28,540 --> 00:27:29,380
Yeah, interesting.

627
00:27:29,380 --> 00:27:32,340
And I think even now with your multi-step tool use,

628
00:27:32,340 --> 00:27:33,340
that's basically what you've done.

629
00:27:33,340 --> 00:27:35,420
You could in principle do that recursively

630
00:27:35,420 --> 00:27:37,700
and you could constrain the computation graph

631
00:27:37,700 --> 00:27:39,060
so that there's no cycles

632
00:27:39,060 --> 00:27:41,340
and it comes back in a fixed amount of time.

633
00:27:41,340 --> 00:27:44,060
Yeah, we terminate execution

634
00:27:44,060 --> 00:27:46,620
after some number of failed attempts.

635
00:27:46,620 --> 00:27:48,740
So it's easy to solve that way.

636
00:27:48,740 --> 00:27:51,060
It's a little bit unsatisfying.

637
00:27:51,060 --> 00:27:53,620
I think our multi-optimal use right now,

638
00:27:53,620 --> 00:27:55,060
it's our very first pass.

639
00:27:55,060 --> 00:27:57,180
It's like the negative one.

640
00:27:57,180 --> 00:28:00,860
And so it's not that good at catching

641
00:28:00,860 --> 00:28:02,060
when it's made mistakes.

642
00:28:02,060 --> 00:28:04,200
It's not that good at correcting its mistakes,

643
00:28:04,200 --> 00:28:06,320
even if it's caught that it fucked up.

644
00:28:07,360 --> 00:28:10,720
And so I think we're still very early there,

645
00:28:10,720 --> 00:28:12,520
but those systems are gonna start to become

646
00:28:12,520 --> 00:28:16,040
extremely robust and reliable.

647
00:28:16,040 --> 00:28:18,160
And I'm very excited for that.

648
00:28:18,160 --> 00:28:19,000
Amazing.

649
00:28:19,000 --> 00:28:24,000
So I'm interested to know from, in your own words,

650
00:28:24,000 --> 00:28:27,320
how, I mean, we're just talking to you folks

651
00:28:27,320 --> 00:28:29,680
who've got this forward engineering team,

652
00:28:29,680 --> 00:28:31,360
your enterprise focused,

653
00:28:31,360 --> 00:28:33,560
you're helping bridge the last mile problem

654
00:28:33,560 --> 00:28:35,920
and really embedding yourselves into large enterprise,

655
00:28:35,920 --> 00:28:38,280
which is an amazing differentiator.

656
00:28:38,280 --> 00:28:40,680
But other than that, there's always the question,

657
00:28:40,680 --> 00:28:44,200
lots of people say these models are just kind of interchangeable

658
00:28:44,200 --> 00:28:48,000
and you're just kind of playing the token game at some point.

659
00:28:48,000 --> 00:28:50,760
And I just wondered like, what's your plan there?

660
00:28:50,760 --> 00:28:52,760
I agree with that sentiment.

661
00:28:52,760 --> 00:28:54,880
Models are way too similar.

662
00:28:54,880 --> 00:28:57,720
I think there's going to start to be differentiation

663
00:28:57,720 --> 00:28:58,560
between models.

664
00:28:58,560 --> 00:29:02,720
Like I was talking about before with command R and R plus,

665
00:29:03,920 --> 00:29:08,480
we're going to start really focusing in on key capabilities.

666
00:29:08,480 --> 00:29:12,040
The general language model game is,

667
00:29:13,000 --> 00:29:16,680
there's a lot of players and it's pretty saturated.

668
00:29:18,880 --> 00:29:21,480
I think people are gonna start to have to branch out.

669
00:29:21,480 --> 00:29:24,240
I think that consumer language models

670
00:29:24,240 --> 00:29:27,000
are going to separate away from enterprise language models.

671
00:29:27,000 --> 00:29:29,320
Within enterprise, there's gonna be a lot of specialization

672
00:29:29,320 --> 00:29:31,100
into specific domains.

673
00:29:31,660 --> 00:29:35,500
And so for Cohere, what I want to see us do

674
00:29:35,500 --> 00:29:40,500
is in product space, push into more tailored capabilities

675
00:29:41,340 --> 00:29:42,740
for particular problems.

676
00:29:44,740 --> 00:29:46,980
We want to drive value for enterprise

677
00:29:46,980 --> 00:29:49,340
and different enterprises operated in different spaces

678
00:29:49,340 --> 00:29:50,340
and they have different needs.

679
00:29:50,340 --> 00:29:52,260
The tools that their models might need to use

680
00:29:52,260 --> 00:29:54,460
look very different from one another.

681
00:29:54,460 --> 00:29:56,180
And we want to make sure that we're serving

682
00:29:56,180 --> 00:30:00,140
each of those niches particularly well or uniquely well.

683
00:30:00,140 --> 00:30:01,820
And that will be our value proposition

684
00:30:01,820 --> 00:30:03,380
differentiated from others.

685
00:30:05,100 --> 00:30:09,180
So that notion of specialization

686
00:30:09,180 --> 00:30:12,420
or enhanced capability in particular domains

687
00:30:12,420 --> 00:30:14,660
is something that we definitely want to explore

688
00:30:14,660 --> 00:30:17,460
at the product level and start to offer.

689
00:30:17,460 --> 00:30:20,580
Because like you say, the dollar per token space,

690
00:30:20,580 --> 00:30:23,100
it's super, we're not gonna stop that.

691
00:30:23,100 --> 00:30:24,820
It's important for the community, right?

692
00:30:24,820 --> 00:30:26,620
Like folks need to build on top of this.

693
00:30:26,620 --> 00:30:29,740
They need access to good models at fair prices.

694
00:30:29,980 --> 00:30:33,340
So we're gonna continue to give that to the world.

695
00:30:34,780 --> 00:30:36,940
But we want to create differentiated value.

696
00:30:36,940 --> 00:30:39,220
And I think that's gonna come from focusing on

697
00:30:39,220 --> 00:30:42,220
the actual problems that enterprises want to tackle

698
00:30:42,220 --> 00:30:45,140
and getting extremely, extremely good at them.

699
00:30:45,140 --> 00:30:45,980
Interesting.

700
00:30:45,980 --> 00:30:50,900
On that, are you planning kind of horizontal products

701
00:30:50,900 --> 00:30:52,340
or vertical products?

702
00:30:52,340 --> 00:30:56,820
And the reason I say horizontal is I know a few startups now

703
00:30:56,860 --> 00:31:00,060
that are building kind of low code,

704
00:31:00,060 --> 00:31:02,420
app dev platforms with large language models

705
00:31:02,420 --> 00:31:04,580
and they're making it incredibly easy in the enterprise

706
00:31:04,580 --> 00:31:06,740
to compose together different models

707
00:31:06,740 --> 00:31:08,900
and to deploy applications on phones.

708
00:31:08,900 --> 00:31:12,660
And it's really democratized because it's so much easier

709
00:31:12,660 --> 00:31:14,580
now for people to do artificial intelligence.

710
00:31:14,580 --> 00:31:17,820
That would be a good example of like a horizontal one, I guess.

711
00:31:17,820 --> 00:31:20,820
So I think our product right now is super horizontal, right?

712
00:31:20,820 --> 00:31:22,820
It's like general language models, embedding models,

713
00:31:22,820 --> 00:31:23,660
re-rank models.

714
00:31:23,660 --> 00:31:26,020
It's a platform that's deployable privately

715
00:31:26,020 --> 00:31:27,980
on every single cloud.

716
00:31:27,980 --> 00:31:31,460
You can deploy the model against any sort of data,

717
00:31:31,460 --> 00:31:35,540
whether it's medical, finance, legal, it doesn't matter.

718
00:31:35,540 --> 00:31:40,540
It's the most horizontal product and platform you can build.

719
00:31:41,660 --> 00:31:44,020
What we're gonna start to do is more

720
00:31:44,020 --> 00:31:47,820
towards verticalization and so specializing models

721
00:31:47,820 --> 00:31:51,060
at particular problems or objectives

722
00:31:51,060 --> 00:31:55,100
that exist in the world and offering a product

723
00:31:55,100 --> 00:31:57,060
that solves that for the enterprise.

724
00:31:57,060 --> 00:31:59,220
Would you ever go beyond the model

725
00:31:59,220 --> 00:32:01,580
and kind of plug a little bit deeper

726
00:32:01,580 --> 00:32:03,180
into the platform in the enterprise?

727
00:32:03,180 --> 00:32:06,580
So for example, building operating models

728
00:32:06,580 --> 00:32:10,780
or one approach would be to just fine-tune the model

729
00:32:10,780 --> 00:32:12,620
on lots of data from a particular domain,

730
00:32:12,620 --> 00:32:14,260
but it's still a language model.

731
00:32:14,260 --> 00:32:16,180
The interface with Coheir is the same

732
00:32:16,180 --> 00:32:19,900
or another one would be to let's say build

733
00:32:19,900 --> 00:32:22,460
something a little bit like Databricks or Snowflake

734
00:32:22,500 --> 00:32:25,820
or something like an enterprise-wide suite

735
00:32:25,820 --> 00:32:29,900
that allows you to deploy, discover, create, share

736
00:32:29,900 --> 00:32:32,060
artificial intelligence in an enterprise.

737
00:32:32,060 --> 00:32:35,780
The only reason I say that is as you're an AWS,

738
00:32:35,780 --> 00:32:37,780
they give you free credits.

739
00:32:37,780 --> 00:32:39,900
They want you to get on their platform

740
00:32:39,900 --> 00:32:41,780
because they know you're never leaving

741
00:32:41,780 --> 00:32:44,180
because you've got something there

742
00:32:44,180 --> 00:32:46,100
which is not easily replaceable.

743
00:32:46,100 --> 00:32:48,540
People learn how to use it, they love it.

744
00:32:48,540 --> 00:32:49,980
Would that be a potential future?

745
00:32:49,980 --> 00:32:54,460
Yeah, it's definitely still going to be a platform,

746
00:32:54,460 --> 00:32:57,060
customizable, something that the user,

747
00:32:57,060 --> 00:32:59,220
which for us as an enterprise can adopt

748
00:32:59,220 --> 00:33:01,460
and sort of bring into their environment,

749
00:33:01,460 --> 00:33:04,660
hook in their data, their tools,

750
00:33:04,660 --> 00:33:07,820
their whatever they want to plug in.

751
00:33:07,820 --> 00:33:10,900
The verticalization is gonna come from

752
00:33:10,900 --> 00:33:12,780
investing in the model to be good

753
00:33:12,780 --> 00:33:14,300
within a particular domain.

754
00:33:14,300 --> 00:33:16,900
That might mean fine-tuning on data within that domain.

755
00:33:16,900 --> 00:33:20,500
That might mean making sure the model is very good

756
00:33:20,500 --> 00:33:23,700
at using the tools that employees operating

757
00:33:23,700 --> 00:33:25,140
in that domain would use.

758
00:33:25,140 --> 00:33:25,980
But that's our focus.

759
00:33:25,980 --> 00:33:27,660
It's starting to get more specific

760
00:33:27,660 --> 00:33:30,420
and focused on the actual use cases

761
00:33:30,420 --> 00:33:31,780
that enterprises care about

762
00:33:31,780 --> 00:33:34,420
and not just doing, you know,

763
00:33:34,420 --> 00:33:39,420
version 345 of the same general model.

764
00:33:42,060 --> 00:33:45,220
So I saw you tweeted about Nick Bostrom's

765
00:33:45,260 --> 00:33:48,340
Future of Humanity Institute shutting down.

766
00:33:48,340 --> 00:33:50,060
Do you have any thoughts on that?

767
00:33:51,500 --> 00:33:54,340
Yeah, I think it sucks to see

768
00:33:54,340 --> 00:33:59,340
any sort of academic institute collapse.

769
00:34:00,780 --> 00:34:03,340
To be honest, I know nothing more than what's public there.

770
00:34:03,340 --> 00:34:06,220
So I don't know if there were some internal issues

771
00:34:06,220 --> 00:34:10,180
that caused the philosophy department to pull funding.

772
00:34:11,180 --> 00:34:16,180
But I've been a pretty vocal critic of ex-risk

773
00:34:17,820 --> 00:34:22,180
and the idea that language models are going to

774
00:34:22,180 --> 00:34:24,260
take over the world and kill everyone.

775
00:34:25,700 --> 00:34:28,820
But despite that, I still want people thinking about that.

776
00:34:28,820 --> 00:34:30,460
I still want academics thinking about that.

777
00:34:30,460 --> 00:34:34,100
I don't think that regulators and policy folks

778
00:34:34,100 --> 00:34:35,500
should be thinking about it yet

779
00:34:35,500 --> 00:34:37,420
because it's so far away and remote

780
00:34:38,300 --> 00:34:40,180
and potentially completely irrelevant.

781
00:34:40,180 --> 00:34:42,540
But that's the domain of academia,

782
00:34:42,540 --> 00:34:46,060
is to pursue those long horizon, high risk projects

783
00:34:46,060 --> 00:34:47,060
and make progress on them.

784
00:34:47,060 --> 00:34:50,220
And so I certainly don't want to see

785
00:34:50,220 --> 00:34:55,220
the academic front of that effort get defunded.

786
00:34:57,180 --> 00:34:59,740
That being said, I think that those organizations

787
00:34:59,740 --> 00:35:03,820
have really been trying to get their hands into policy

788
00:35:03,820 --> 00:35:07,100
and impact private sector, public sector

789
00:35:07,100 --> 00:35:10,740
in a way that is threatening to progress,

790
00:35:11,900 --> 00:35:16,900
misleading and so I think that we're starting

791
00:35:19,260 --> 00:35:20,860
to have within our community,

792
00:35:20,860 --> 00:35:23,300
like the AI machine learning community,

793
00:35:23,300 --> 00:35:24,540
a bit of a correction.

794
00:35:25,460 --> 00:35:27,580
Those people were kind of given a lot of power,

795
00:35:27,580 --> 00:35:28,860
were listened to a lot

796
00:35:30,780 --> 00:35:34,140
and developed what I think we all

797
00:35:34,140 --> 00:35:36,180
recognize as too much influence.

798
00:35:37,180 --> 00:35:40,740
And it started to produce bills

799
00:35:40,740 --> 00:35:45,740
and talks about policy that would totally collapse progress

800
00:35:46,460 --> 00:35:49,180
in the space, very, very prematurely

801
00:35:49,180 --> 00:35:54,180
about theoretical long-term risks that might be an issue.

802
00:35:54,540 --> 00:35:55,380
And so fortunately,

803
00:35:55,380 --> 00:35:57,380
I think there's a cultural correction happening

804
00:35:57,380 --> 00:35:59,500
where even the legislators and policymakers

805
00:35:59,500 --> 00:36:02,620
are starting to say, you know, this is not,

806
00:36:03,620 --> 00:36:07,300
it's not appropriate the level of influence

807
00:36:07,300 --> 00:36:09,580
that this one group is having

808
00:36:09,580 --> 00:36:11,620
and we should listen to a much more broad

809
00:36:11,620 --> 00:36:14,860
and diverse set of opinions.

810
00:36:14,860 --> 00:36:16,260
So I'm still concerned about that

811
00:36:16,260 --> 00:36:19,100
and I'll continue to speak out against that when I see it,

812
00:36:19,100 --> 00:36:21,420
but at the academic level,

813
00:36:21,420 --> 00:36:26,420
I don't want to see, you know, professors lose their funding.

814
00:36:26,540 --> 00:36:30,060
I think that they should continue to pursue those ideas.

815
00:36:30,060 --> 00:36:31,580
Yeah, I think Bostrom blogged that.

816
00:36:31,580 --> 00:36:34,180
He was trying to resist the entropic forces

817
00:36:34,180 --> 00:36:36,180
of the philosophy department for several years

818
00:36:36,180 --> 00:36:38,180
and eventually he lost.

819
00:36:38,180 --> 00:36:39,980
But I'm in two minds as well.

820
00:36:39,980 --> 00:36:42,020
So as you know, I've hosted many debates

821
00:36:42,020 --> 00:36:43,260
with, you know, like Connolly,

822
00:36:43,260 --> 00:36:45,020
he for example, and Beth Jezos

823
00:36:45,020 --> 00:36:46,700
and a bunch of different people.

824
00:36:46,700 --> 00:36:49,500
And one thing that strikes me is how ideological it is.

825
00:36:49,500 --> 00:36:51,180
I really thought as a podcaster,

826
00:36:51,180 --> 00:36:54,140
I could have an honest and open conversation

827
00:36:54,140 --> 00:36:56,260
and it's never gone well.

828
00:36:56,260 --> 00:36:57,780
And I've put a lot of thought

829
00:36:57,780 --> 00:37:00,060
into trying to understand why that is.

830
00:37:00,060 --> 00:37:02,500
And I think philosophically you can trace it back

831
00:37:02,500 --> 00:37:05,860
to things like paternalism and safetyism

832
00:37:05,860 --> 00:37:08,620
and utilitarianism and consequentialism

833
00:37:08,620 --> 00:37:10,620
and long-termism and, you know,

834
00:37:10,620 --> 00:37:15,060
these are ideologies that make one believe

835
00:37:15,060 --> 00:37:17,820
that even though it's just a subjective probability

836
00:37:17,820 --> 00:37:20,020
that I know better than you,

837
00:37:20,020 --> 00:37:22,180
I can predict the future better than you.

838
00:37:22,180 --> 00:37:24,980
And they've become much more pragmatic in recent years.

839
00:37:24,980 --> 00:37:26,700
So rather than talking about

840
00:37:26,700 --> 00:37:28,860
the old school Bostrom superintelligence,

841
00:37:28,860 --> 00:37:30,780
we're now talking about, you know,

842
00:37:30,780 --> 00:37:33,380
memetic risks and bio risks and things

843
00:37:33,380 --> 00:37:36,460
that I think are designed to get more people on board with it.

844
00:37:36,460 --> 00:37:39,620
And I agree with you that they've had a lot of influence.

845
00:37:39,620 --> 00:37:41,580
But why is it so difficult

846
00:37:41,580 --> 00:37:43,340
to have a rational conversation?

847
00:37:43,340 --> 00:37:44,780
Yeah, no, I think it's what you say.

848
00:37:44,780 --> 00:37:46,980
It's very ideological.

849
00:37:46,980 --> 00:37:49,140
There are camps and positions

850
00:37:49,140 --> 00:37:52,500
and it's, for some reason,

851
00:37:52,500 --> 00:37:55,420
it's become very cult-like on both sides.

852
00:37:56,420 --> 00:37:58,820
Obviously, there's the EA movement,

853
00:37:58,820 --> 00:38:02,220
which formed a cult-like environment

854
00:38:02,220 --> 00:38:05,100
of adherence to those principles

855
00:38:05,100 --> 00:38:09,820
and their recommended behaviors and actions,

856
00:38:09,820 --> 00:38:13,100
you know, what you should work on in your life.

857
00:38:13,100 --> 00:38:14,860
They have dating apps for you.

858
00:38:14,860 --> 00:38:16,540
Like, it's very insular.

859
00:38:16,540 --> 00:38:21,140
And then there was an ironic, I think,

860
00:38:21,140 --> 00:38:23,820
although it's increasingly not clear,

861
00:38:23,940 --> 00:38:26,540
an ironic counter-movement, which was EAAC.

862
00:38:26,540 --> 00:38:28,260
Yeah.

863
00:38:28,260 --> 00:38:29,700
And that has spun out into something

864
00:38:29,700 --> 00:38:31,300
that is very not ironic.

865
00:38:31,300 --> 00:38:35,620
It's very libertarian, accelerationist,

866
00:38:35,620 --> 00:38:38,380
which are ideals that I don't hold either.

867
00:38:38,380 --> 00:38:42,980
And so both of these camps I find really unappealing.

868
00:38:42,980 --> 00:38:44,700
I don't want to be associated with either of them.

869
00:38:44,700 --> 00:38:49,420
Yeah, I found, you know, EA was very dominant

870
00:38:49,420 --> 00:38:50,500
for a long time.

871
00:38:50,540 --> 00:38:54,300
And so when EAAC came out, it was, like, refreshing.

872
00:38:54,300 --> 00:38:57,540
Finally, someone's, like, calling them on their bullshit.

873
00:38:57,540 --> 00:38:59,420
But at this point, it's just mind-numbing

874
00:38:59,420 --> 00:39:03,700
and, like, completely not of interest to me.

875
00:39:03,700 --> 00:39:04,980
Yeah, we've had Beth on the show.

876
00:39:04,980 --> 00:39:06,420
He's a really nice guy, actually.

877
00:39:06,420 --> 00:39:08,660
I invested in Guillaume's company.

878
00:39:08,660 --> 00:39:09,700
Oh, did he? Yeah, yeah, yeah.

879
00:39:09,700 --> 00:39:10,780
Oh, it's not that...

880
00:39:10,780 --> 00:39:14,420
He's brilliant. Like, he's a really, really nice person.

881
00:39:15,620 --> 00:39:18,700
I'm proud to see Canadians doing great things.

882
00:39:19,700 --> 00:39:21,580
The best thing I found super funny.

883
00:39:21,580 --> 00:39:24,260
And I think EAAC was necessary.

884
00:39:24,260 --> 00:39:28,180
I now believe that both EAAC and EAAC need to be dissolved.

885
00:39:28,180 --> 00:39:33,380
We've seen them through to their logical conclusion,

886
00:39:33,380 --> 00:39:35,100
and now we're starting to get into territory

887
00:39:35,100 --> 00:39:38,060
that's very strange.

888
00:39:38,060 --> 00:39:39,420
From a philosophical perspective,

889
00:39:39,420 --> 00:39:43,020
how do you kind of see the role of AI in society?

890
00:39:43,020 --> 00:39:47,060
And I'm quite interested in how it's affecting our reality,

891
00:39:47,060 --> 00:39:49,980
and how we interface with technology

892
00:39:49,980 --> 00:39:51,740
is really dramatically changing over time.

893
00:39:51,740 --> 00:39:52,900
I mean, what do you think about that?

894
00:39:52,900 --> 00:39:54,940
Yeah, completely true.

895
00:39:54,940 --> 00:40:00,020
I think I view it in the same way I view the computer

896
00:40:00,020 --> 00:40:01,660
or the CPU.

897
00:40:01,660 --> 00:40:02,620
It's a tool.

898
00:40:02,620 --> 00:40:04,020
It's something that we're going to leverage,

899
00:40:04,020 --> 00:40:06,580
that we're going to build on top of

900
00:40:06,580 --> 00:40:08,660
and use to make our lives better,

901
00:40:08,660 --> 00:40:11,060
to make us more productive,

902
00:40:11,060 --> 00:40:13,700
to make things cheaper, more accessible.

903
00:40:14,700 --> 00:40:18,180
I think all the good that came from the computer

904
00:40:18,180 --> 00:40:21,700
and the internet is going to be dwarfed by this,

905
00:40:21,700 --> 00:40:25,700
the democratization of intelligence

906
00:40:25,700 --> 00:40:29,180
and having that always at your disposal at any time.

907
00:40:29,180 --> 00:40:32,700
That's something that, you know, 50 years ago,

908
00:40:32,700 --> 00:40:35,980
you couldn't even dream of it, right?

909
00:40:35,980 --> 00:40:38,900
It's surreal, the amount of progress

910
00:40:38,900 --> 00:40:40,580
that's been made in half a century.

911
00:40:40,580 --> 00:40:41,940
And so I'm really excited for that.

912
00:40:42,380 --> 00:40:47,380
I think it will do a lot of good and alleviate a lot of ills.

913
00:40:49,140 --> 00:40:52,420
I think the human experience, our lives,

914
00:40:52,420 --> 00:40:55,900
will be dramatically improved by having access

915
00:40:55,900 --> 00:40:59,300
to much more intelligence in our lives.

916
00:40:59,300 --> 00:41:00,380
I'm really excited as well,

917
00:41:00,380 --> 00:41:04,420
but are there particular things that you are concerned about?

918
00:41:04,420 --> 00:41:07,940
I mean, for example, people say that language models

919
00:41:07,940 --> 00:41:09,580
might enfee us,

920
00:41:09,580 --> 00:41:13,020
they might lead to mass manipulation and persuasion.

921
00:41:13,020 --> 00:41:15,220
I mean, Jan Lacoon tweeted the other day,

922
00:41:15,220 --> 00:41:18,140
he said, where's the mass manipulation?

923
00:41:18,140 --> 00:41:19,180
Where's the persuasion?

924
00:41:19,180 --> 00:41:21,580
There might be something that just happens gradually

925
00:41:21,580 --> 00:41:23,140
over time, but are there things

926
00:41:23,140 --> 00:41:25,260
that you do worry about?

927
00:41:25,260 --> 00:41:26,580
Of course, yeah, of course.

928
00:41:26,580 --> 00:41:28,180
It's a general technology,

929
00:41:28,180 --> 00:41:31,340
and so it can be used in a lot of different ways,

930
00:41:31,340 --> 00:41:33,500
many of which are, I think,

931
00:41:33,500 --> 00:41:35,620
abhorrent and ones that we should avoid

932
00:41:35,620 --> 00:41:37,340
and make very difficult to do.

933
00:41:38,940 --> 00:41:42,700
I'm much more of an optimist than I am a pessimist,

934
00:41:42,700 --> 00:41:46,660
but on the side of things that are risky,

935
00:41:46,660 --> 00:41:49,980
I think that misinformation is high up on the list.

936
00:41:49,980 --> 00:41:52,180
I think that we're already seeing

937
00:41:53,540 --> 00:41:56,700
social media platforms start to build in the mitigations.

938
00:41:56,700 --> 00:41:58,500
I think things like human verification

939
00:41:58,500 --> 00:41:59,780
are gonna become crucial.

940
00:42:00,740 --> 00:42:04,020
If I'm reading a poster talking about

941
00:42:04,940 --> 00:42:08,020
whatever Canadian elections or politicians,

942
00:42:08,020 --> 00:42:11,620
I wanna know that that's a voting Canadian citizen.

943
00:42:11,620 --> 00:42:14,100
I wanna know, because I want to know

944
00:42:14,100 --> 00:42:17,060
what my compatriots think, right?

945
00:42:17,060 --> 00:42:19,820
Even if they're on the opposite side of the fence to me,

946
00:42:19,820 --> 00:42:20,860
like that's fine.

947
00:42:20,860 --> 00:42:24,060
I wanna hear what they think,

948
00:42:25,140 --> 00:42:28,300
but I don't wanna hear what some foreign adversary

949
00:42:28,300 --> 00:42:32,500
has spun up a bot to push into the discourse.

950
00:42:32,500 --> 00:42:36,460
And so human verification, I think, is crucial.

951
00:42:36,460 --> 00:42:39,060
That's the one that's top of mind for me.

952
00:42:39,060 --> 00:42:41,140
I think some of the more remote risks,

953
00:42:41,140 --> 00:42:46,060
like bio weapons and this sort of stuff,

954
00:42:46,060 --> 00:42:47,500
I'm less concerned about.

955
00:42:47,500 --> 00:42:51,100
In feeblement and becoming dependent on the technology,

956
00:42:51,100 --> 00:42:53,340
I think folks said that about calculators

957
00:42:53,340 --> 00:42:55,940
and we wouldn't learn how to do basic math.

958
00:42:55,940 --> 00:42:59,660
Humans are intrinsically curious.

959
00:42:59,660 --> 00:43:00,740
We want to know things,

960
00:43:00,740 --> 00:43:04,100
and we can't ask the right questions of machines

961
00:43:04,100 --> 00:43:05,620
without knowing things.

962
00:43:05,620 --> 00:43:08,980
And so we'll continue to be really well educated,

963
00:43:08,980 --> 00:43:12,140
better educated, more knowledgeable than we were before

964
00:43:12,140 --> 00:43:13,260
without that technology.

965
00:43:13,260 --> 00:43:15,820
Yeah, because if you look at the enfeeblement pie chart,

966
00:43:15,820 --> 00:43:17,500
a calculator is a very small part

967
00:43:17,500 --> 00:43:19,940
and a general AI is quite a large part,

968
00:43:19,940 --> 00:43:21,940
which is a little bit concerning, I guess,

969
00:43:21,940 --> 00:43:26,940
but I agree with you that maybe the jobs one has spoken about.

970
00:43:26,940 --> 00:43:28,500
I've not seen a lot of evidence of that yet,

971
00:43:28,500 --> 00:43:31,460
but it's so pernicious, it might happen slowly over time.

972
00:43:31,460 --> 00:43:33,220
Daniel Dennett wrote an interesting article,

973
00:43:33,220 --> 00:43:35,180
and rest in peace, by the way,

974
00:43:35,180 --> 00:43:37,740
Daniel Dennett called Counterfeit People,

975
00:43:37,740 --> 00:43:39,020
which he published in The Atlantic,

976
00:43:39,020 --> 00:43:41,460
and he was kind of saying that when we have all of these bots

977
00:43:41,460 --> 00:43:44,780
and generative video models and so on,

978
00:43:44,780 --> 00:43:47,860
at some point they'll become indistinguishable from reality,

979
00:43:47,860 --> 00:43:50,060
and that will lead to a kind of acquiescence

980
00:43:50,060 --> 00:43:52,260
where we don't trust anything we see.

981
00:43:52,260 --> 00:43:55,020
And I think that's quite interesting,

982
00:43:55,020 --> 00:43:57,300
and I also think that these models

983
00:43:57,300 --> 00:44:00,940
might kind of affect our agency in quite a weird way,

984
00:44:00,940 --> 00:44:02,980
but it's so difficult to understand now

985
00:44:02,980 --> 00:44:05,780
how that's going to affect society.

986
00:44:05,780 --> 00:44:08,700
Yeah, I think even now,

987
00:44:08,700 --> 00:44:11,420
people have been taught to be extremely skeptical

988
00:44:11,420 --> 00:44:14,220
of what they read and see.

989
00:44:14,220 --> 00:44:17,100
There's a very strong prior inside of us,

990
00:44:17,100 --> 00:44:18,380
for any media that we consume,

991
00:44:18,380 --> 00:44:21,100
that it's been skewed or manipulated

992
00:44:21,100 --> 00:44:24,420
or produced to propagate an idea,

993
00:44:24,420 --> 00:44:26,780
and I think it's good to have a skeptical populace.

994
00:44:26,780 --> 00:44:28,500
I think it's good to be skeptical

995
00:44:28,500 --> 00:44:32,020
about what you read, regardless of the medium.

996
00:44:32,020 --> 00:44:34,020
And I think people will do what they've always done,

997
00:44:34,020 --> 00:44:36,260
which is filter towards sources

998
00:44:36,260 --> 00:44:38,140
they find trustworthy and objective.

999
00:44:40,220 --> 00:44:43,820
That'll happen even with ML and the loop,

1000
00:44:43,820 --> 00:44:45,940
disinformation and misinformation campaigns,

1001
00:44:45,940 --> 00:44:47,260
manipulation campaigns,

1002
00:44:47,260 --> 00:44:50,500
they existed well before models existed.

1003
00:44:51,780 --> 00:44:54,700
And so it's not like a novel concept,

1004
00:44:54,980 --> 00:44:56,460
and it's always been a risk.

1005
00:44:57,660 --> 00:45:00,860
And the question is how much more prevalent

1006
00:45:00,860 --> 00:45:03,900
does the technology make that risk?

1007
00:45:05,620 --> 00:45:09,260
I'm optimistic that we're quite robust

1008
00:45:09,260 --> 00:45:13,100
and that we'll find ways to make it very hard

1009
00:45:13,100 --> 00:45:15,220
for bad actors to exploit the technology.

1010
00:45:15,220 --> 00:45:18,740
My rough take is that the more agency the AI has,

1011
00:45:18,740 --> 00:45:20,020
the more of a risk it is,

1012
00:45:20,020 --> 00:45:22,780
because if it is just doing supervised things,

1013
00:45:22,820 --> 00:45:25,340
then every step of the process,

1014
00:45:25,340 --> 00:45:29,060
it's being aligned and constrained and steered by humans.

1015
00:45:29,060 --> 00:45:31,420
If we ever did create a gentile AI,

1016
00:45:31,420 --> 00:45:34,260
then there's this kind of weird divergence

1017
00:45:34,260 --> 00:45:36,780
and all sorts of scary things might happen.

1018
00:45:36,780 --> 00:45:38,340
But I wanted to talk a little bit

1019
00:45:38,340 --> 00:45:39,900
about policy and regulation.

1020
00:45:39,900 --> 00:45:41,100
So you spoke to that earlier,

1021
00:45:41,100 --> 00:45:43,300
you said that potentially there are some

1022
00:45:43,300 --> 00:45:46,700
quite damaging policy changes being considered.

1023
00:45:46,700 --> 00:45:47,540
Could you speak to that?

1024
00:45:47,540 --> 00:45:50,180
Yeah, I've seen ideas floated.

1025
00:45:50,180 --> 00:45:53,380
I don't think any seriously damaging policy

1026
00:45:53,380 --> 00:45:55,860
has actually passed, fortunately.

1027
00:45:55,860 --> 00:45:57,940
But within what's being considered,

1028
00:45:57,940 --> 00:46:02,300
there are ideas that they will destroy innovation,

1029
00:46:02,300 --> 00:46:03,620
they will destroy startups.

1030
00:46:05,020 --> 00:46:06,860
And so you'll just entrench power

1031
00:46:06,860 --> 00:46:08,580
with the existing incumbents.

1032
00:46:09,500 --> 00:46:13,340
Some of those examples might be fines,

1033
00:46:14,340 --> 00:46:17,220
which if they're a $100 million fine,

1034
00:46:17,260 --> 00:46:20,620
that's gonna wipe out and stamp out a startup.

1035
00:46:20,620 --> 00:46:24,700
But for a large, you know, big tech company,

1036
00:46:24,700 --> 00:46:27,140
it's like 10 minutes of revenue.

1037
00:46:27,140 --> 00:46:28,140
It just doesn't matter.

1038
00:46:28,140 --> 00:46:29,780
It fundamentally is irrelevant.

1039
00:46:29,780 --> 00:46:32,020
And certainly a cost they're willing to take

1040
00:46:32,020 --> 00:46:33,180
to capture a market.

1041
00:46:34,820 --> 00:46:37,820
And so very disproportionate consequences

1042
00:46:37,820 --> 00:46:41,420
for the same punishment.

1043
00:46:41,420 --> 00:46:45,180
Over-regulation in that way that it's thoughtless

1044
00:46:45,180 --> 00:46:47,020
will have the exact opposite effect

1045
00:46:47,020 --> 00:46:49,860
of what I think all of us in the public

1046
00:46:49,860 --> 00:46:51,340
and in government want.

1047
00:46:51,340 --> 00:46:52,980
We want competitive markets.

1048
00:46:52,980 --> 00:46:54,900
We don't want oligopolies.

1049
00:46:55,740 --> 00:46:58,580
And we're starting to see oligopolies emerge.

1050
00:46:58,580 --> 00:47:02,540
And so there needs to be fairly strong action

1051
00:47:02,540 --> 00:47:05,500
pushing against the entrenchment of those oligopolies.

1052
00:47:05,500 --> 00:47:09,860
And we need to preserve the ability to self-disrupt.

1053
00:47:09,860 --> 00:47:12,100
Because if you can't, if you have an oligopoly

1054
00:47:12,100 --> 00:47:13,900
and you have entrenched incumbents,

1055
00:47:14,620 --> 00:47:18,500
the likelihood of self-disrupting,

1056
00:47:18,500 --> 00:47:21,900
of the new winner emerging within your market,

1057
00:47:21,900 --> 00:47:24,480
being one of your players, goes down.

1058
00:47:24,480 --> 00:47:27,300
And so you're gonna be disrupted from outside.

1059
00:47:27,300 --> 00:47:28,380
That's a huge risk.

1060
00:47:28,380 --> 00:47:31,820
And so you need competitive, self-disrupting markets.

1061
00:47:31,820 --> 00:47:35,620
And it seems like some of the policy folks

1062
00:47:35,620 --> 00:47:37,540
are just acting non-strategically

1063
00:47:37,540 --> 00:47:40,140
and not considering that.

1064
00:47:41,100 --> 00:47:45,980
But fortunately, what has been passed seems sensible.

1065
00:47:45,980 --> 00:47:47,740
Can you comment in particular

1066
00:47:47,740 --> 00:47:52,740
on the EU AI legislation and the Canadian?

1067
00:47:52,820 --> 00:47:55,180
I probably can't say anything too specific.

1068
00:47:55,180 --> 00:48:00,100
I think the Canadian legislation hasn't gone through yet.

1069
00:48:00,100 --> 00:48:03,740
The EU AI Act has, but fortunately,

1070
00:48:03,740 --> 00:48:06,420
it was reigned quite far back

1071
00:48:06,420 --> 00:48:09,260
from its initial position.

1072
00:48:09,260 --> 00:48:11,300
I think all of those regulators were in conversation

1073
00:48:11,300 --> 00:48:15,020
with all of them when you talk to the folks,

1074
00:48:15,020 --> 00:48:16,740
they wanna do the right thing.

1075
00:48:16,740 --> 00:48:19,060
They're under a lot of pressure from different parties

1076
00:48:19,060 --> 00:48:22,340
with conflicting interests,

1077
00:48:22,340 --> 00:48:23,300
but they're trying to do the right thing.

1078
00:48:23,300 --> 00:48:24,540
They're trying to make sure this technology

1079
00:48:24,540 --> 00:48:26,140
gets out into the world in a safe way,

1080
00:48:26,140 --> 00:48:28,140
that there's oversight,

1081
00:48:28,140 --> 00:48:31,300
that we don't entrench the incumbents.

1082
00:48:31,300 --> 00:48:36,300
And we ideally actually sort of bias towards disruption

1083
00:48:36,460 --> 00:48:41,460
and the creation of new value and innovation, new players.

1084
00:48:45,060 --> 00:48:48,740
So I think they all want that, but it's a tightrope.

1085
00:48:48,740 --> 00:48:52,660
It's a very difficult line to walk.

1086
00:48:52,660 --> 00:48:54,860
I think one of the issues is that

1087
00:48:54,860 --> 00:48:57,500
not a lot of people certainly in the government

1088
00:48:57,500 --> 00:48:59,340
understand how this technology works.

1089
00:48:59,340 --> 00:49:00,860
It seems like magic.

1090
00:49:00,860 --> 00:49:04,660
And many people, I mean, even in the AI space,

1091
00:49:04,700 --> 00:49:06,860
I mean, Lacoon and Hinton, for example,

1092
00:49:06,860 --> 00:49:08,980
people have very different opinions about it,

1093
00:49:08,980 --> 00:49:10,860
but I'm also interested in your views

1094
00:49:10,860 --> 00:49:12,940
on the kind of health of the startup scene.

1095
00:49:12,940 --> 00:49:14,860
So we're in a bit of a downturn at the moment.

1096
00:49:14,860 --> 00:49:17,580
It doesn't seem to have affected the LLM space,

1097
00:49:17,580 --> 00:49:19,700
but even in the LLM space, I've noticed a trend

1098
00:49:19,700 --> 00:49:23,060
that many people started kind of wrapper companies

1099
00:49:23,060 --> 00:49:26,420
where they did an LLM, but it didn't really do anything

1100
00:49:26,420 --> 00:49:28,460
that couldn't easily be replicated.

1101
00:49:28,460 --> 00:49:30,180
And what are your thoughts there?

1102
00:49:30,180 --> 00:49:33,020
Do you think we're gonna see a trend towards startups

1103
00:49:33,060 --> 00:49:35,740
doing something that is very differentiated?

1104
00:49:35,740 --> 00:49:38,700
Yeah, I mean, I think we're in a moment of churn.

1105
00:49:38,700 --> 00:49:43,700
So I think there are gonna be some players

1106
00:49:43,700 --> 00:49:48,700
who started a while ago who fold or go into other companies,

1107
00:49:48,700 --> 00:49:51,180
get acquired, that type of thing,

1108
00:49:51,180 --> 00:49:53,100
but there's a whole new generation emerging.

1109
00:49:53,100 --> 00:49:57,660
I know a bunch of people starting up, Ivan and I,

1110
00:49:57,660 --> 00:50:01,100
we invest in startups and we're seeing an uptick

1111
00:50:01,100 --> 00:50:03,500
in the number of AI startups that are coming out.

1112
00:50:04,980 --> 00:50:06,820
It's sort of like a reformatting.

1113
00:50:06,820 --> 00:50:09,500
There was a bunch of folks building at one layer,

1114
00:50:09,500 --> 00:50:13,240
like the LLM layer or the one layer above that,

1115
00:50:14,500 --> 00:50:15,820
tooling, et cetera.

1116
00:50:16,740 --> 00:50:19,140
The players have kind of been set in that space,

1117
00:50:19,140 --> 00:50:20,980
it seems, yeah, of course,

1118
00:50:20,980 --> 00:50:23,240
I'd be happy to see new players emerge.

1119
00:50:24,500 --> 00:50:29,500
But we now need a set of ideas and products and companies

1120
00:50:30,100 --> 00:50:31,900
building up the stack.

1121
00:50:31,900 --> 00:50:36,900
So more abstract concepts, stuff like end user products

1122
00:50:37,300 --> 00:50:40,140
and agent companies, they're all starting to pop up

1123
00:50:40,140 --> 00:50:43,940
and create really interesting new ideas.

1124
00:50:43,940 --> 00:50:45,500
And then that will sort of settle

1125
00:50:45,500 --> 00:50:47,820
and we'll have our players at that layer.

1126
00:50:47,820 --> 00:50:50,980
So it's a continuous cycle.

1127
00:50:52,820 --> 00:50:55,220
Yeah, I'm really excited about the AI startup space.

1128
00:50:55,220 --> 00:50:57,900
It feels like we're finally starting to get our feet

1129
00:50:57,900 --> 00:50:58,740
on the ground a little bit.

1130
00:50:59,020 --> 00:51:01,460
For example, with the tool use, with the RAG,

1131
00:51:01,460 --> 00:51:03,740
it's starting to look a lot more like traditional

1132
00:51:03,740 --> 00:51:04,820
software engineering.

1133
00:51:04,820 --> 00:51:06,980
So what we're seeing now is people kind of rolling up

1134
00:51:06,980 --> 00:51:08,380
their sleeves and actually building out

1135
00:51:08,380 --> 00:51:10,660
these very sophisticated software architectures

1136
00:51:10,660 --> 00:51:13,180
that compose LLMs in interesting ways.

1137
00:51:13,180 --> 00:51:15,460
And they're not just kind of, you know,

1138
00:51:15,460 --> 00:51:18,020
just building a simple LLM with a prompt on the top.

1139
00:51:19,020 --> 00:51:21,100
Yeah, it's definitely getting more sophisticated.

1140
00:51:21,100 --> 00:51:25,140
And as the tools get more robust and reliable,

1141
00:51:25,140 --> 00:51:28,980
it's unlocking totally new applications.

1142
00:51:28,980 --> 00:51:32,060
And the utility is starting to be seen and felt

1143
00:51:32,060 --> 00:51:33,100
in the real world.

1144
00:51:33,100 --> 00:51:35,420
I think last year was very much like the year

1145
00:51:35,420 --> 00:51:38,260
the world woke up to the technology

1146
00:51:38,260 --> 00:51:40,820
and got their footing with it.

1147
00:51:40,820 --> 00:51:42,380
So it got familiarity.

1148
00:51:44,260 --> 00:51:49,260
This year is when things are gonna start hitting production.

1149
00:51:49,260 --> 00:51:51,220
They're gonna actually start to hit our hands

1150
00:51:51,220 --> 00:51:54,260
and we're gonna be able to use this as part of our work,

1151
00:51:54,260 --> 00:51:58,340
part of our play, the products that we use.

1152
00:51:59,380 --> 00:52:02,260
It's gonna become a much more fundamental part

1153
00:52:02,260 --> 00:52:03,420
of our daily life.

1154
00:52:05,820 --> 00:52:06,780
So it's very gratifying.

1155
00:52:06,780 --> 00:52:11,020
Like we've been building Cohere for four and a half years now.

1156
00:52:11,020 --> 00:52:12,080
We're in our fifth year.

1157
00:52:15,580 --> 00:52:17,660
And I think for a long time,

1158
00:52:17,660 --> 00:52:19,660
we were out there sort of preaching,

1159
00:52:19,660 --> 00:52:22,100
this is really cool, please care about this.

1160
00:52:22,420 --> 00:52:25,460
Like this is gonna be an important thing.

1161
00:52:25,460 --> 00:52:26,780
And folks would pat us on the back

1162
00:52:26,780 --> 00:52:29,180
and say, nice science project.

1163
00:52:32,180 --> 00:52:35,260
But finally, we're actually starting to see

1164
00:52:35,260 --> 00:52:37,820
the fruits of all that labor.

1165
00:52:37,820 --> 00:52:41,540
And so it's really gratifying to see real world impact.

1166
00:52:41,540 --> 00:52:43,180
And I think that's what we exist for

1167
00:52:43,180 --> 00:52:45,100
is really trying to accelerate that

1168
00:52:45,100 --> 00:52:48,420
and make more of it happen faster

1169
00:52:48,420 --> 00:52:52,060
and in the best way possible.

1170
00:52:52,060 --> 00:52:53,660
And what was your biggest mistake?

1171
00:52:53,660 --> 00:52:56,540
I mean, do you have any advice for other startup founders?

1172
00:52:56,540 --> 00:52:59,860
What did you do that perhaps they should avoid?

1173
00:53:01,100 --> 00:53:06,100
I fucked up constantly at every stage of the company.

1174
00:53:09,220 --> 00:53:12,980
I think, I guess just like admitting that you've messed up

1175
00:53:14,060 --> 00:53:18,420
and trying not to be in denial about it

1176
00:53:18,420 --> 00:53:20,260
and fixing it as quickly as possible

1177
00:53:20,300 --> 00:53:23,620
has been the most important thing

1178
00:53:23,620 --> 00:53:27,740
to continue to thrive and exist.

1179
00:53:29,020 --> 00:53:32,340
But yeah, this is the first company I started.

1180
00:53:32,340 --> 00:53:33,700
Same for Nick and Ivan.

1181
00:53:33,700 --> 00:53:37,260
And so the whole founding team, we were fresh into it

1182
00:53:37,260 --> 00:53:40,040
and we made potentially every mistake

1183
00:53:40,040 --> 00:53:41,400
you could possibly make.

1184
00:53:43,300 --> 00:53:48,060
Fortunately, we were good at listening to others

1185
00:53:48,100 --> 00:53:49,580
who had done it before

1186
00:53:49,580 --> 00:53:52,180
and seen a lot more than we'd seen.

1187
00:53:52,180 --> 00:53:54,740
And so I'm sure we've dodged some mistakes,

1188
00:53:54,740 --> 00:53:57,940
but it feels like we've made them all.

1189
00:53:57,940 --> 00:53:59,500
Yeah, if you don't make mistakes you're not learning,

1190
00:53:59,500 --> 00:54:01,060
I guess, but just final question.

1191
00:54:01,060 --> 00:54:04,700
How do you, because it's such a large organization now

1192
00:54:04,700 --> 00:54:06,980
and there's this problem of vertical information flow.

1193
00:54:06,980 --> 00:54:08,140
So there might be a problem

1194
00:54:08,140 --> 00:54:09,860
that some of your folks have discovered now

1195
00:54:09,860 --> 00:54:12,580
and it takes a while to filter through to you.

1196
00:54:12,580 --> 00:54:14,700
But obviously it needs to be scalable

1197
00:54:14,700 --> 00:54:15,540
so you need to delegate.

1198
00:54:15,540 --> 00:54:16,860
How do you deal with that?

1199
00:54:16,860 --> 00:54:21,580
Yeah, I'm very close to like the ICs.

1200
00:54:21,580 --> 00:54:22,420
I'm very close.

1201
00:54:22,420 --> 00:54:26,580
Like I'm not someone who works through their reports

1202
00:54:26,580 --> 00:54:28,420
or follows the chain of command.

1203
00:54:28,420 --> 00:54:32,540
I just talk to the people who are actually doing the work.

1204
00:54:34,020 --> 00:54:37,540
And so information flows quite freely.

1205
00:54:39,140 --> 00:54:42,020
I'm sometimes, I think I'm mostly annoying people

1206
00:54:42,020 --> 00:54:43,700
at this point because I'm like pinging them every day.

1207
00:54:43,700 --> 00:54:44,580
How's that run going?

1208
00:54:44,580 --> 00:54:46,660
You know, have we tried this experiment?

1209
00:54:46,660 --> 00:54:48,740
I'm very deeply involved in stuff.

1210
00:54:49,700 --> 00:54:51,380
So we haven't had too much.

1211
00:54:52,740 --> 00:54:57,740
I don't feel like there's an information flow issue

1212
00:54:58,100 --> 00:55:03,100
echo here with scaling, collaboration between teams,

1213
00:55:03,380 --> 00:55:05,120
especially when you're a global company

1214
00:55:05,120 --> 00:55:08,120
and you're not sitting in the same office as a person.

1215
00:55:09,860 --> 00:55:10,980
That's very difficult.

1216
00:55:10,980 --> 00:55:13,220
I think remote work is really, really hard.

1217
00:55:13,220 --> 00:55:14,380
It's not easy.

1218
00:55:14,380 --> 00:55:15,220
It's not easy.

1219
00:55:15,220 --> 00:55:20,220
And I think that concentrating teams to geographical areas

1220
00:55:20,820 --> 00:55:24,900
or at least time zones is very important

1221
00:55:24,900 --> 00:55:28,780
and leads to a lot more productivity and effectiveness.

1222
00:55:28,780 --> 00:55:31,780
It's part of the reason I moved here to London

1223
00:55:31,780 --> 00:55:34,220
is to be closer to a good chunk

1224
00:55:34,220 --> 00:55:35,460
of our machine learning team.

1225
00:55:35,460 --> 00:55:38,180
Phil's here, Patrick is here.

1226
00:55:40,660 --> 00:55:43,620
Like I want to be present and involved in the ML

1227
00:55:43,620 --> 00:55:46,180
component of the company as much as possible.

1228
00:55:49,340 --> 00:55:51,140
Yeah, I think as we've scaled,

1229
00:55:51,140 --> 00:55:54,140
there's been systems that we've used

1230
00:55:54,140 --> 00:55:56,100
that have broken down at each phase,

1231
00:55:56,100 --> 00:55:58,940
stuff that worked for the first 10 of us,

1232
00:55:58,940 --> 00:56:01,220
didn't work for the next 20,

1233
00:56:02,220 --> 00:56:04,180
didn't work for the next 100.

1234
00:56:04,180 --> 00:56:09,180
Now we're pushing 350, I think.

1235
00:56:09,740 --> 00:56:12,260
And there are people at the company who I don't know,

1236
00:56:12,260 --> 00:56:13,380
which is insane.

1237
00:56:14,620 --> 00:56:18,780
A super weird experience.

1238
00:56:21,060 --> 00:56:23,260
But we've hired really fantastic people

1239
00:56:23,260 --> 00:56:24,740
and we continue to do so.

1240
00:56:24,740 --> 00:56:28,780
And I think you just trust that people will still

1241
00:56:28,780 --> 00:56:30,620
make the right decisions going forward

1242
00:56:30,620 --> 00:56:33,460
and that you've set standards high enough

1243
00:56:35,740 --> 00:56:38,140
that you don't need to approve every single hire.

1244
00:56:38,140 --> 00:56:39,740
You don't need to know what every single person

1245
00:56:39,740 --> 00:56:40,820
is working on.

1246
00:56:41,820 --> 00:56:45,180
And you just trust your colleagues.

1247
00:56:45,180 --> 00:56:47,900
Yeah, I can attest that you hire very well.

1248
00:56:47,900 --> 00:56:49,820
It's probably the best culture I've ever seen

1249
00:56:49,820 --> 00:56:51,700
in any company, actually.

1250
00:56:51,700 --> 00:56:53,020
Thank you so much.

1251
00:56:53,020 --> 00:56:55,020
Final question, I mean, just out of interest,

1252
00:56:55,020 --> 00:56:57,540
do you get like microcosms in the different offices?

1253
00:56:57,540 --> 00:57:01,220
I mean, do you see like different mini cultures?

1254
00:57:01,220 --> 00:57:02,940
Oh yeah, totally, 100%.

1255
00:57:02,940 --> 00:57:06,140
Like the London office compared to the Toronto office

1256
00:57:06,140 --> 00:57:08,980
compared to SF, New York.

1257
00:57:10,900 --> 00:57:13,300
The vibes are very, very different.

1258
00:57:13,300 --> 00:57:14,620
Like so different.

1259
00:57:16,180 --> 00:57:18,180
London is so nice.

1260
00:57:18,180 --> 00:57:21,020
It feels really tight-knit.

1261
00:57:21,020 --> 00:57:22,820
Like it still feels like a startup,

1262
00:57:24,420 --> 00:57:25,340
which we are a startup,

1263
00:57:25,340 --> 00:57:27,900
but it still feels like a tiny 30-person startup

1264
00:57:27,900 --> 00:57:31,420
where you go out for beers with your colleagues

1265
00:57:31,420 --> 00:57:34,700
after work at the pub like regularly.

1266
00:57:34,700 --> 00:57:36,620
Everyone knows what everyone else is working on

1267
00:57:36,620 --> 00:57:40,700
inside the office, calls on each other for help.

1268
00:57:40,740 --> 00:57:41,980
London is super tight-knit.

1269
00:57:41,980 --> 00:57:43,620
I think the culture here is like,

1270
00:57:45,380 --> 00:57:46,900
I can't pick favorites,

1271
00:57:46,900 --> 00:57:48,580
but I really like the culture here.

1272
00:57:50,060 --> 00:57:52,380
In Toronto, it's our biggest office,

1273
00:57:52,380 --> 00:57:54,940
and so it's much broader,

1274
00:57:56,780 --> 00:57:58,020
but the culture there is amazing too.

1275
00:57:58,020 --> 00:58:00,700
Super hard-working, stay late,

1276
00:58:00,700 --> 00:58:03,220
and like grind very passionate.

1277
00:58:03,220 --> 00:58:04,820
There's like different groups

1278
00:58:04,820 --> 00:58:06,620
that are close with each other there.

1279
00:58:07,620 --> 00:58:10,900
New York is new, but that city is just so much fun.

1280
00:58:10,900 --> 00:58:13,500
It's just like such an incredible city,

1281
00:58:13,500 --> 00:58:16,140
so much energy, always awake,

1282
00:58:16,140 --> 00:58:17,780
work hard, play hard.

1283
00:58:19,580 --> 00:58:21,340
SF, I spend the least time in.

1284
00:58:21,340 --> 00:58:25,420
I'm not a huge SF fan, to be completely honest.

1285
00:58:25,420 --> 00:58:26,860
It's our second HQ,

1286
00:58:27,740 --> 00:58:31,820
but I just haven't gotten into the city,

1287
00:58:31,820 --> 00:58:33,980
I think, in the way that others have.

1288
00:58:34,820 --> 00:58:38,620
I think SF compared to like New York, Toronto, London,

1289
00:58:39,540 --> 00:58:41,580
I feel like New York, Toronto, and London

1290
00:58:41,580 --> 00:58:44,220
are real cities.

1291
00:58:45,220 --> 00:58:48,580
There are artists, there are, you know,

1292
00:58:48,580 --> 00:58:52,780
people just doing a very diverse set of things,

1293
00:58:52,780 --> 00:58:56,380
and across all the different fields

1294
00:58:56,380 --> 00:58:58,060
that are going on there,

1295
00:58:58,060 --> 00:59:00,220
you have some of the best people in the world.

1296
00:59:01,220 --> 00:59:04,100
SF is much more, it feels more homogenous to me.

1297
00:59:04,100 --> 00:59:05,540
It's a lot of people doing the same stuff.

1298
00:59:05,540 --> 00:59:07,500
There's sort of one topic of conversation.

1299
00:59:07,500 --> 00:59:09,860
You don't bump into someone

1300
00:59:09,860 --> 00:59:14,060
with a categorically different worldview than you,

1301
00:59:14,060 --> 00:59:16,060
or perspective, or experience.

1302
00:59:18,060 --> 00:59:19,500
And so I like visiting,

1303
00:59:19,500 --> 00:59:23,500
because I meet like brilliant people in our field, in tech.

1304
00:59:25,460 --> 00:59:28,460
But to live there would be really cool.

1305
00:59:28,460 --> 00:59:32,420
To live there would be really difficult for me.

1306
00:59:32,420 --> 00:59:37,420
I would feel like I'm sacrificing whole pieces of my life.

1307
00:59:41,340 --> 00:59:42,420
But I love visiting.

1308
00:59:42,420 --> 00:59:43,260
It's a great place.

1309
00:59:43,260 --> 00:59:44,900
And I love the folks who are there,

1310
00:59:44,900 --> 00:59:47,460
and most of our investors are there.

1311
00:59:47,460 --> 00:59:51,300
And so it's a really cool environment,

1312
00:59:51,300 --> 00:59:53,700
very intense and like competitive,

1313
00:59:53,700 --> 00:59:55,940
and those are really good things.

1314
00:59:55,940 --> 00:59:58,260
It's very motivating to be there.

1315
00:59:58,260 --> 01:00:00,260
But I think I can get that just by visiting.

1316
01:00:00,260 --> 01:00:03,140
I don't need to commit myself full time.

1317
01:00:03,980 --> 01:00:06,300
Aidan Gomez, it's been an honor and a pleasure.

1318
01:00:06,300 --> 01:00:07,580
Thank you so much for joining us.

1319
01:00:07,580 --> 01:00:09,220
Thank you so much for having me.

