1
00:00:00,000 --> 00:00:01,080
To achieve your highest goals,

2
00:00:01,080 --> 00:00:02,840
you have to be willing to abandon them.

3
00:00:02,840 --> 00:00:04,880
Today, we're publishing the first

4
00:00:04,880 --> 00:00:07,280
in a small series of conversations

5
00:00:07,280 --> 00:00:10,040
with Professor Ken Stanley.

6
00:00:10,040 --> 00:00:13,440
Reading Ken's book, Why Greatness Cannot Be Planned,

7
00:00:13,440 --> 00:00:16,680
was one of the most intellectually awakening moments

8
00:00:16,680 --> 00:00:19,000
of the latter part of my life.

9
00:00:19,000 --> 00:00:21,640
It really turned my thinking upside down,

10
00:00:21,640 --> 00:00:24,680
making me question many deeply held beliefs

11
00:00:24,680 --> 00:00:26,040
which I had previously.

12
00:00:27,000 --> 00:00:28,280
The episode that we did with him

13
00:00:28,280 --> 00:00:30,440
was many months in the making,

14
00:00:30,440 --> 00:00:35,440
and perhaps the greatest ever episode of MLST.

15
00:00:35,440 --> 00:00:37,720
Ken argued in his book that our world

16
00:00:37,720 --> 00:00:41,440
has become saturated with objectives.

17
00:00:41,440 --> 00:00:44,200
The process of setting an objective,

18
00:00:44,200 --> 00:00:46,040
attempting to achieve it,

19
00:00:46,040 --> 00:00:48,440
and measuring progress along the way,

20
00:00:48,440 --> 00:00:52,560
has become the primary route to achievement in our culture.

21
00:00:52,560 --> 00:00:56,040
It's not like he's saying that objectives are bad, per se,

22
00:00:56,040 --> 00:00:57,720
especially if they're modest.

23
00:00:57,720 --> 00:01:00,440
But what he thinks are that when goals are ambitious,

24
00:01:00,440 --> 00:01:05,360
which is to say they are unknowable, complex, or abstract,

25
00:01:05,360 --> 00:01:09,960
or put more simply, that they entail discovery, creativity,

26
00:01:09,960 --> 00:01:13,680
invention, innovation, or happiness,

27
00:01:13,680 --> 00:01:17,040
then the search base becomes deceptive,

28
00:01:17,040 --> 00:01:22,040
which is to say the measure itself becomes a false compass

29
00:01:22,120 --> 00:01:24,200
which blinds us to the stepping stones

30
00:01:24,200 --> 00:01:26,200
which we should actually take.

31
00:01:26,200 --> 00:01:29,520
I mean, is the key to artificial intelligence

32
00:01:29,520 --> 00:01:32,160
really related to intelligence?

33
00:01:32,160 --> 00:01:35,040
Does taking a job with a higher salary

34
00:01:35,040 --> 00:01:38,360
really bring you closer to being a millionaire?

35
00:01:38,360 --> 00:01:40,520
The problem is that the stepping stones

36
00:01:40,520 --> 00:01:42,880
which lead to ambitious objectives

37
00:01:42,880 --> 00:01:45,000
tend to be pretty strange.

38
00:01:45,000 --> 00:01:48,520
They don't resemble the final end state.

39
00:01:48,520 --> 00:01:52,360
Vacuum tubes led to computers, for example,

40
00:01:52,360 --> 00:01:56,560
and YouTube started as a video dating website.

41
00:01:56,560 --> 00:02:01,560
In a sense, creativity itself is actually a search problem.

42
00:02:02,040 --> 00:02:06,560
Is it possible to explore a search space intelligently

43
00:02:06,560 --> 00:02:10,560
without using an objective to align towards discovery

44
00:02:10,560 --> 00:02:14,340
and away from the trap of preconceived results?

45
00:02:15,400 --> 00:02:18,880
Greatness is possible if you're willing to stop demanding

46
00:02:18,880 --> 00:02:21,560
what greatness should be.

47
00:02:21,600 --> 00:02:24,760
The greatest moments and epiphanies in our life

48
00:02:24,760 --> 00:02:28,760
are so often unexpected and unplanned.

49
00:02:28,760 --> 00:02:32,680
Serendipity can play an outsize role in our lives.

50
00:02:34,000 --> 00:02:37,520
At the end of Joel Lemon and Ken Stanley's 2011 paper,

51
00:02:37,520 --> 00:02:39,200
Abandoning Objectives,

52
00:02:39,200 --> 00:02:42,720
they concluded that it was almost like a riddle.

53
00:02:42,720 --> 00:02:46,600
Novelty search suggests a surprising new perspective

54
00:02:46,600 --> 00:02:49,320
on achievement, which is to say,

55
00:02:49,320 --> 00:02:51,520
to achieve your highest goals,

56
00:02:51,520 --> 00:02:53,880
you must be willing to abandon them.

57
00:02:54,800 --> 00:02:57,960
I always get engineers on my team to read this paper

58
00:02:57,960 --> 00:03:01,120
and it's illuminating in so many ways.

59
00:03:01,120 --> 00:03:05,080
I love using the visual analogy of a maze

60
00:03:05,080 --> 00:03:07,800
to represent the search problem of life

61
00:03:07,800 --> 00:03:10,400
and stepping stones in that maze

62
00:03:10,400 --> 00:03:15,080
as being either potential objectives or end states.

63
00:03:15,080 --> 00:03:17,640
Of course, we don't know about the existence

64
00:03:17,640 --> 00:03:20,080
of most of the objectives and the fog of war

65
00:03:20,080 --> 00:03:23,520
blinds us from seeing far ahead into the maze.

66
00:03:23,520 --> 00:03:27,640
The fundamental problem is the missing information problem.

67
00:03:27,640 --> 00:03:29,120
But as we'll soon find out,

68
00:03:29,120 --> 00:03:31,640
not just the information itself,

69
00:03:31,640 --> 00:03:35,000
but how we represent it, understand it,

70
00:03:35,000 --> 00:03:37,600
experience it and even know it.

71
00:03:38,640 --> 00:03:41,880
The fascinating thing is that if we were an oracle

72
00:03:41,880 --> 00:03:45,080
and we knew exactly which steps to take in life,

73
00:03:45,080 --> 00:03:47,680
we could become billionaires within months

74
00:03:47,680 --> 00:03:50,840
or we could achieve anything we wanted to.

75
00:03:50,840 --> 00:03:52,800
The only inconvenient thing stopping us

76
00:03:52,800 --> 00:03:55,080
from realizing our dreams

77
00:03:55,080 --> 00:03:58,680
is that the space of possibilities is exponentially large.

78
00:03:58,680 --> 00:04:01,960
It's very expensive to sample many trajectories

79
00:04:01,960 --> 00:04:04,720
in that space, so we tend to get stuck

80
00:04:04,720 --> 00:04:08,320
in certain sections of the maze for long stretches of time.

81
00:04:09,360 --> 00:04:13,040
Ken thinks that the most valuable commodity in search

82
00:04:13,040 --> 00:04:14,200
is information.

83
00:04:15,080 --> 00:04:18,360
We must accumulate as much of it as possible.

84
00:04:18,360 --> 00:04:21,160
There's an arrow of informational complexity

85
00:04:21,160 --> 00:04:22,840
and natural evolution.

86
00:04:22,840 --> 00:04:25,560
And Ken was the first machine learning researcher

87
00:04:25,560 --> 00:04:30,000
to take seriously the growing yet controversial view

88
00:04:30,000 --> 00:04:32,760
in biology that adaptive selection

89
00:04:32,760 --> 00:04:35,760
does not explain the arrow of complexity in nature.

90
00:04:36,680 --> 00:04:39,080
We're standing in a maze right now.

91
00:04:39,080 --> 00:04:42,000
There are many doors within walking distance

92
00:04:42,000 --> 00:04:45,480
which lead to unthinkable greatness.

93
00:04:45,480 --> 00:04:49,200
We simply haven't walked over and open them.

94
00:04:49,200 --> 00:04:52,960
We are existence proofs of general intelligence.

95
00:04:52,960 --> 00:04:54,960
Every single one of us has a brain,

96
00:04:54,960 --> 00:04:57,000
but just like the infinite number of doors

97
00:04:57,000 --> 00:04:59,320
we could open in the maze of life,

98
00:04:59,320 --> 00:05:02,120
it took evolution billions of years

99
00:05:02,120 --> 00:05:04,640
and gazillions of individual life forms

100
00:05:04,640 --> 00:05:06,720
to create our brains.

101
00:05:06,720 --> 00:05:10,040
Now, Ken is known for his pioneering research

102
00:05:10,040 --> 00:05:12,040
in the open-endedness space.

103
00:05:12,040 --> 00:05:13,640
You might recall the poet paper,

104
00:05:13,640 --> 00:05:16,640
the pairwise open-ended trailblazer.

105
00:05:16,640 --> 00:05:19,760
It was the first ever episode of MLST that we spoke about that,

106
00:05:19,760 --> 00:05:23,720
but that's an example of this line of research.

107
00:05:23,720 --> 00:05:27,640
Open-endedness might seem like a nebulous term,

108
00:05:27,640 --> 00:05:29,520
but you can think of it as being an AI system

109
00:05:29,520 --> 00:05:33,400
which doesn't have a boundary in its state space

110
00:05:33,400 --> 00:05:36,280
and doesn't ever finish accumulating information.

111
00:05:40,320 --> 00:05:43,360
Sir Arthur Stanley Eddington,

112
00:05:43,360 --> 00:05:45,360
the notorious astronomer and physicist

113
00:05:45,360 --> 00:05:47,480
born in the late 19th century,

114
00:05:47,480 --> 00:05:51,120
spoke about the subjectivity in science.

115
00:05:51,120 --> 00:05:54,800
His point is that science only tells us a sliver

116
00:05:54,800 --> 00:05:57,800
of what's really happening in the world around us,

117
00:05:57,800 --> 00:06:00,480
and we should be a lot less arrogant in our claims

118
00:06:00,480 --> 00:06:04,760
to understand it and our attempts to formalize it.

119
00:06:04,760 --> 00:06:06,360
Physics, he argued,

120
00:06:06,360 --> 00:06:09,240
can never reveal the true nature of things,

121
00:06:09,280 --> 00:06:12,760
rather it deals with relations between observables

122
00:06:12,760 --> 00:06:16,880
which are subjectively selected by the human mind.

123
00:06:16,880 --> 00:06:19,720
He says that he was inclined to attribute

124
00:06:19,720 --> 00:06:22,960
the whole responsibility for the laws of mechanics

125
00:06:22,960 --> 00:06:25,320
and gravitation to the mind

126
00:06:25,320 --> 00:06:29,440
and deny the external world any share in them.

127
00:06:29,440 --> 00:06:31,000
He went on to say,

128
00:06:31,000 --> 00:06:34,680
the laws which we have hitherto been unable to fit

129
00:06:34,680 --> 00:06:38,400
into a rational scheme are the true natural laws

130
00:06:38,400 --> 00:06:40,880
inherent in the external world,

131
00:06:40,880 --> 00:06:43,840
and mind has no chance of molding them

132
00:06:43,840 --> 00:06:46,600
in accordance with its own outlook.

133
00:06:48,120 --> 00:06:50,760
Subjectivity is everywhere,

134
00:06:50,760 --> 00:06:53,320
and yet in so many ways we delude ourselves

135
00:06:53,320 --> 00:06:58,200
that we are transforming subjectivity into objectivity.

136
00:06:58,200 --> 00:07:01,600
Imagine a fisherman catching fish in a small ocean.

137
00:07:01,600 --> 00:07:03,600
Depending on the size of his net,

138
00:07:03,600 --> 00:07:07,080
he might reason that there's nothing very small in the ocean

139
00:07:07,080 --> 00:07:09,680
because those objects are slipping through his net.

140
00:07:10,720 --> 00:07:13,040
What we can agree on is that our understanding

141
00:07:13,040 --> 00:07:16,280
of physics breaks if we zoom in too much

142
00:07:16,280 --> 00:07:17,960
or zoom out too much,

143
00:07:17,960 --> 00:07:21,160
so at best we have a frame of reference

144
00:07:21,160 --> 00:07:25,200
which Eddington would argue is observer relative.

145
00:07:25,200 --> 00:07:28,680
Surprisingly little in our world can actually be objectified

146
00:07:28,680 --> 00:07:31,560
without using abstract motifs.

147
00:07:31,560 --> 00:07:32,880
Have you ever asked a philosopher

148
00:07:32,880 --> 00:07:35,840
to define what it means to be real,

149
00:07:35,840 --> 00:07:38,560
or what it means to exist?

150
00:07:38,560 --> 00:07:42,080
I can guarantee you that they'll be reaching.

151
00:07:42,080 --> 00:07:45,200
They'll say things like something exists

152
00:07:45,200 --> 00:07:47,560
if it has a causal effect on the world,

153
00:07:47,560 --> 00:07:50,160
or if it can be measured,

154
00:07:50,160 --> 00:07:52,800
or if it's not illusory,

155
00:07:52,800 --> 00:07:56,200
which is to say it is as it appears,

156
00:07:56,200 --> 00:08:00,600
or perhaps that it's genuine in big air quotes.

157
00:08:01,640 --> 00:08:04,720
What does it mean to be intelligent?

158
00:08:04,720 --> 00:08:06,320
What is life?

159
00:08:06,320 --> 00:08:09,160
Describe an ecosystem.

160
00:08:09,160 --> 00:08:11,360
What is British culture?

161
00:08:11,360 --> 00:08:13,880
Describe your mind.

162
00:08:13,880 --> 00:08:17,280
Describe your conscious experience.

163
00:08:17,280 --> 00:08:18,640
Have you heard of the parable

164
00:08:18,640 --> 00:08:20,720
of the blind men and the elephant?

165
00:08:20,720 --> 00:08:23,440
Any attempt to formalize a complex phenomena

166
00:08:23,440 --> 00:08:26,480
lead to excluding large parts of the truth.

167
00:08:26,480 --> 00:08:28,520
What fascinated me about this conversation

168
00:08:28,520 --> 00:08:29,600
with Ken this evening

169
00:08:29,600 --> 00:08:32,920
is I got a much deeper understanding of his philosophy.

170
00:08:32,920 --> 00:08:34,760
He led by saying that he thought

171
00:08:34,760 --> 00:08:36,160
it's worth questioning

172
00:08:36,160 --> 00:08:37,600
whether artificial intelligence

173
00:08:37,600 --> 00:08:40,280
is even a science or not.

174
00:08:40,280 --> 00:08:43,480
Just let that sink in for a moment.

175
00:08:43,480 --> 00:08:46,160
I'd been too focused on my mental framework

176
00:08:46,160 --> 00:08:50,320
of objectives, behavior, and actions.

177
00:08:50,320 --> 00:08:52,240
The broader story is that Ken thinks

178
00:08:52,240 --> 00:08:54,600
society and institutions

179
00:08:54,600 --> 00:08:57,840
are scared of any subjectivity.

180
00:08:57,840 --> 00:09:00,360
Subjectivity in general.

181
00:09:00,360 --> 00:09:03,200
He thinks that attempts to formalize complex,

182
00:09:03,200 --> 00:09:05,840
emergent, and integrative phenomena

183
00:09:05,840 --> 00:09:09,840
like intelligence, consciousness, life, society,

184
00:09:09,840 --> 00:09:12,240
the mind, or anything else for that matter,

185
00:09:12,240 --> 00:09:14,800
only deludes us and blocks us

186
00:09:14,800 --> 00:09:18,760
from potentially discovering a deeper reality later.

187
00:09:18,760 --> 00:09:21,200
It's a very human trait to seek to understand

188
00:09:21,200 --> 00:09:22,280
the world around us

189
00:09:22,280 --> 00:09:24,640
to varying degrees of self-delusion.

190
00:09:24,640 --> 00:09:26,960
Before we understand a particular phenomena,

191
00:09:26,960 --> 00:09:28,960
it's almost impossible to come up

192
00:09:28,960 --> 00:09:32,280
with a good scientific and formal definition.

193
00:09:32,280 --> 00:09:34,080
How could we possibly define something

194
00:09:34,080 --> 00:09:36,000
which we don't understand well?

195
00:09:36,000 --> 00:09:39,320
This is the paradox in computer science and philosophy.

196
00:09:39,320 --> 00:09:40,920
The more we seek something,

197
00:09:40,920 --> 00:09:42,920
the more it eludes us.

198
00:09:44,000 --> 00:09:46,440
It sounds almost anti-intellectual, doesn't it?

199
00:09:46,440 --> 00:09:50,080
This idea that we should reject formalization.

200
00:09:50,080 --> 00:09:52,440
But Ken thinks that many phenomena

201
00:09:52,440 --> 00:09:55,920
will only be trivialized by vacant attempts

202
00:09:56,000 --> 00:09:59,680
to formalize them through oversimplification.

203
00:09:59,680 --> 00:10:02,920
Because inevitably, we'll chop off many aspects

204
00:10:02,920 --> 00:10:05,720
which might not be captured by the formalization.

205
00:10:05,720 --> 00:10:08,000
I had always associated Ken's philosophy

206
00:10:08,000 --> 00:10:10,960
with the missing information problem

207
00:10:10,960 --> 00:10:14,080
rather than the representation problem.

208
00:10:14,080 --> 00:10:15,440
The missing information problem

209
00:10:15,440 --> 00:10:17,480
is that we don't know something

210
00:10:17,480 --> 00:10:20,400
or we only know part of the truth.

211
00:10:20,400 --> 00:10:23,760
The representation problem is that we simply don't understand

212
00:10:23,760 --> 00:10:27,480
or at least we can't verbalize what we're experiencing.

213
00:10:27,480 --> 00:10:30,080
Ken says that the corporate world is dominated

214
00:10:30,080 --> 00:10:34,360
by the attempted veneer of objectivity.

215
00:10:34,360 --> 00:10:37,600
If you're trying to land an engineering role in Fang,

216
00:10:37,600 --> 00:10:42,080
your tolerance for ambiguity and subjectivity

217
00:10:42,080 --> 00:10:43,960
is the single most selective feature

218
00:10:43,960 --> 00:10:46,760
of the level which you get hired at.

219
00:10:46,760 --> 00:10:50,720
If you can solve clearly defined problems, you're a level four.

220
00:10:50,720 --> 00:10:52,880
If you can find the problems,

221
00:10:52,880 --> 00:10:54,640
you're a level six.

222
00:10:54,640 --> 00:10:58,800
If you can find the areas, you're a level seven.

223
00:10:58,800 --> 00:11:02,320
And if you can find people who can find the areas,

224
00:11:02,320 --> 00:11:05,400
you're a level eight or a level nine.

225
00:11:05,400 --> 00:11:08,520
I give loads of open-ended tasks to my team,

226
00:11:08,520 --> 00:11:10,360
discover people in the organization

227
00:11:10,360 --> 00:11:13,560
who are subject matter experts in domain X,

228
00:11:13,560 --> 00:11:15,880
build an operating model for knowledge sharing

229
00:11:15,880 --> 00:11:18,160
and standardization.

230
00:11:18,160 --> 00:11:20,320
What does good look like?

231
00:11:20,320 --> 00:11:22,560
It's possible we don't know it yet,

232
00:11:22,560 --> 00:11:24,640
which is the missing information problem.

233
00:11:24,640 --> 00:11:27,920
But it's also possible that we know it,

234
00:11:27,920 --> 00:11:30,040
but we're not able to verbalize it,

235
00:11:30,040 --> 00:11:31,920
which is the representation problem,

236
00:11:31,920 --> 00:11:35,080
inherent with all complex phenomena.

237
00:11:35,080 --> 00:11:36,080
It's quite interesting, actually.

238
00:11:36,080 --> 00:11:38,800
I'm building a code review platform called Merge.

239
00:11:38,800 --> 00:11:41,840
And it's easy to objectify all of the low-level metrics

240
00:11:41,840 --> 00:11:44,920
such as how many customer engineers we support

241
00:11:44,920 --> 00:11:47,560
or how efficiently we're sharing knowledge.

242
00:11:47,560 --> 00:11:49,000
All of the high-level outcomes

243
00:11:49,000 --> 00:11:51,760
like a better engineering culture.

244
00:11:51,760 --> 00:11:54,000
They're emergent properties, right?

245
00:11:54,000 --> 00:11:55,400
It's a bit like pornography.

246
00:11:55,400 --> 00:11:56,800
You know it when you see it.

247
00:11:58,960 --> 00:12:03,160
So now, I give you Professor Ken Stanley.

248
00:12:03,160 --> 00:12:04,800
We have a nose for the interesting.

249
00:12:04,800 --> 00:12:05,960
That's how we got this far.

250
00:12:05,960 --> 00:12:08,040
That's how civilization came out.

251
00:12:08,040 --> 00:12:11,000
That's why the history of innovation is so amazing.

252
00:12:11,000 --> 00:12:14,040
Everything washes out when we start ruling by committee.

253
00:12:14,040 --> 00:12:15,480
Like, we have to allow people

254
00:12:15,480 --> 00:12:17,480
to follow their passions to their extremes.

255
00:12:17,480 --> 00:12:20,480
And yet we run society as if this actually

256
00:12:20,480 --> 00:12:22,320
makes any sense at all.

257
00:12:22,320 --> 00:12:24,760
I think the gradient of interestingness

258
00:12:24,760 --> 00:12:26,760
is probably the best expression

259
00:12:26,760 --> 00:12:29,320
of the ideal divergent search.

260
00:12:29,320 --> 00:12:30,440
You get to this problem that, like,

261
00:12:30,440 --> 00:12:32,600
I don't know how to formalize interestingness,

262
00:12:32,600 --> 00:12:35,480
which you get to then are proxies for interestingness.

263
00:12:35,480 --> 00:12:37,680
That not everything that's novel is interesting,

264
00:12:37,680 --> 00:12:40,320
but just about everything that's interesting is novel.

265
00:12:40,320 --> 00:12:42,720
It is in my personality and nature

266
00:12:42,720 --> 00:12:45,600
to want to overthrow this,

267
00:12:45,600 --> 00:12:47,840
I guess we could say, tyranny of objectives.

268
00:12:51,480 --> 00:12:58,480
So today, we have an incredibly special guest.

269
00:12:58,480 --> 00:13:02,480
Indeed, my hero in AI, Professor Kenneth Stanley.

270
00:13:02,480 --> 00:13:04,920
Now, the Kenneth Stanley show that we filmed,

271
00:13:04,920 --> 00:13:07,320
which is to say episode 38,

272
00:13:07,320 --> 00:13:10,120
was my favorite episode that we've ever done.

273
00:13:10,120 --> 00:13:11,160
Reading Kenneth's book,

274
00:13:11,160 --> 00:13:12,480
Why Greatness Cannot Be Planned,

275
00:13:12,480 --> 00:13:13,760
and preparing for that show,

276
00:13:13,760 --> 00:13:17,160
triggered an incredible amount of intellectual growth for me.

277
00:13:17,160 --> 00:13:18,560
And a hallmark of that, you know,

278
00:13:18,560 --> 00:13:20,360
when you learn something profound,

279
00:13:20,440 --> 00:13:22,200
is that you start recognizing it

280
00:13:22,200 --> 00:13:23,760
in many other domains of your life.

281
00:13:23,760 --> 00:13:25,840
I mean, you remember when you learned probability theory

282
00:13:25,840 --> 00:13:26,760
for the first time,

283
00:13:26,760 --> 00:13:29,320
and you started seeing exponential distributions

284
00:13:29,320 --> 00:13:31,000
absolutely everywhere.

285
00:13:31,000 --> 00:13:32,720
So I was just saying to Kenneth

286
00:13:32,720 --> 00:13:35,120
that when we listened back to the show last night,

287
00:13:35,120 --> 00:13:37,360
it was probably the best example ever of a show

288
00:13:37,360 --> 00:13:38,640
we overprepared for,

289
00:13:38,640 --> 00:13:40,560
which is to say that during the interview,

290
00:13:40,560 --> 00:13:42,120
we were just bamboozling Kenneth.

291
00:13:42,120 --> 00:13:43,160
We were so excited,

292
00:13:43,160 --> 00:13:44,920
we almost couldn't control ourselves.

293
00:13:44,920 --> 00:13:46,840
And I think in particular,

294
00:13:46,840 --> 00:13:49,400
Keith and Kenneth reached a common ground, actually,

295
00:13:49,400 --> 00:13:51,680
in the sense that divergence and convergence

296
00:13:51,680 --> 00:13:53,400
don't have to be hyperbolic

297
00:13:53,400 --> 00:13:55,520
or entirely mutually exclusive.

298
00:13:55,520 --> 00:13:58,800
Now, Kenneth thinks that we have a nose for the interesting.

299
00:13:58,800 --> 00:14:00,480
That's how we got this far.

300
00:14:00,480 --> 00:14:02,760
It's the basis of all innovation

301
00:14:02,760 --> 00:14:05,000
and the secret of our society.

302
00:14:05,000 --> 00:14:08,640
Kenneth believes that the concept of deception in search,

303
00:14:08,640 --> 00:14:10,960
which is to say getting stuck in local optimans

304
00:14:10,960 --> 00:14:12,840
and indeed deluding ourselves

305
00:14:12,840 --> 00:14:15,560
that we even know what good looks like in the first place,

306
00:14:15,560 --> 00:14:18,680
is what regularly sends us into brick walls.

307
00:14:18,680 --> 00:14:22,320
Kenneth thinks that institutions are full of gatekeepers

308
00:14:22,320 --> 00:14:24,480
and the gatekeepers only want to see your objectives

309
00:14:24,480 --> 00:14:25,600
and metrics.

310
00:14:25,600 --> 00:14:29,400
Any expert on search would know that it's completely naive

311
00:14:29,400 --> 00:14:31,200
and yet we still use this approach

312
00:14:31,200 --> 00:14:33,600
for the most complex problems in our society.

313
00:14:33,600 --> 00:14:35,320
Committees wash out everything.

314
00:14:35,320 --> 00:14:38,240
We need to let people follow their interests to the extremes

315
00:14:38,240 --> 00:14:39,800
and risk has to be tolerated

316
00:14:39,800 --> 00:14:41,880
in order to make the greatest discoveries.

317
00:14:41,880 --> 00:14:43,640
Anyway, Professor Kenneth Stanley,

318
00:14:43,640 --> 00:14:46,400
it's an absolute honor to have you back on the show.

319
00:14:46,440 --> 00:14:48,160
Thank you for having me here.

320
00:14:48,160 --> 00:14:49,000
So it's been great.

321
00:14:49,000 --> 00:14:51,840
I really enjoyed being on it and I'm glad to be back here.

322
00:14:51,840 --> 00:14:54,160
I mean, even the thing you made in front of that show

323
00:14:54,160 --> 00:14:55,000
was awesome.

324
00:14:55,000 --> 00:14:57,640
That was, I showed that to my son and he was like,

325
00:14:57,640 --> 00:14:58,760
what the heck is that?

326
00:14:58,760 --> 00:15:00,280
He's like, that's not my dad.

327
00:15:00,280 --> 00:15:01,520
And I was like, that is, he's like,

328
00:15:01,520 --> 00:15:04,280
you've never seen me in that kind of environment,

329
00:15:04,280 --> 00:15:05,440
but it was pretty cool.

330
00:15:06,480 --> 00:15:09,120
Oh yeah, so we painted a cafe in behind you,

331
00:15:09,120 --> 00:15:11,600
but we did that first thing in Blender.

332
00:15:11,600 --> 00:15:13,240
But I had to learn Blender just for that.

333
00:15:13,240 --> 00:15:14,920
It was the first thing I ever did.

334
00:15:14,920 --> 00:15:16,680
And what I started doing afterwards,

335
00:15:16,680 --> 00:15:19,040
I wished that we did your show a tiny bit later.

336
00:15:19,040 --> 00:15:21,360
I started using my virtual reality headset.

337
00:15:21,360 --> 00:15:25,560
So I've got an Oculus Quest and then I use Google tilt brush

338
00:15:25,560 --> 00:15:28,040
and then I can kind of create a beautiful 3D environment.

339
00:15:28,040 --> 00:15:30,520
I can put slides and videos and all kinds of stuff up there.

340
00:15:30,520 --> 00:15:31,760
And that would have been perfect for that

341
00:15:31,760 --> 00:15:33,560
because I wanted to show all of the stepping stones

342
00:15:33,560 --> 00:15:36,840
in the divergent search in a 3D environment.

343
00:15:36,840 --> 00:15:39,560
But that's a cool thing is we're getting more sophisticated

344
00:15:39,560 --> 00:15:40,840
and we're starting to use, you know,

345
00:15:40,840 --> 00:15:42,400
manum to do animations

346
00:15:42,400 --> 00:15:44,480
because we want to be able to really kind of,

347
00:15:44,480 --> 00:15:45,920
at least for some of the episodes,

348
00:15:45,920 --> 00:15:49,600
tell the story in a very visual and educational way.

349
00:15:49,600 --> 00:15:51,520
So we're kind of going back now and revisiting

350
00:15:51,520 --> 00:15:54,360
some of our favorite, you know, favorite episodes

351
00:15:54,360 --> 00:15:56,040
and can we tell parts of it again

352
00:15:56,040 --> 00:15:58,800
with this new technology, if you will,

353
00:15:58,800 --> 00:16:00,960
or new techniques that we have skills at.

354
00:16:00,960 --> 00:16:02,520
That's cool, yeah.

355
00:16:02,520 --> 00:16:04,800
I got to tell you, just on a personal note,

356
00:16:04,800 --> 00:16:06,360
because you mentioned, you know, your son saying,

357
00:16:06,360 --> 00:16:09,200
that's not my dad, part of my motivation

358
00:16:09,200 --> 00:16:12,560
for doing these YouTube, you know, shows

359
00:16:12,600 --> 00:16:14,720
is that I want there to be something

360
00:16:14,720 --> 00:16:17,160
where my kids can go maybe when I'm gone or whatever

361
00:16:17,160 --> 00:16:19,040
and look back and say, oh yeah, that was my dad.

362
00:16:19,040 --> 00:16:20,080
That's what he sounded like.

363
00:16:20,080 --> 00:16:21,040
That's what he looked like.

364
00:16:21,040 --> 00:16:22,960
You know, that's how we talk.

365
00:16:22,960 --> 00:16:24,320
It's a really nice thing.

366
00:16:24,320 --> 00:16:25,400
Yeah.

367
00:16:25,400 --> 00:16:26,240
Yeah, yeah.

368
00:16:26,240 --> 00:16:27,160
The problem is no one understands

369
00:16:27,160 --> 00:16:28,120
what we're talking about.

370
00:16:28,120 --> 00:16:28,960
Well, maybe they will.

371
00:16:28,960 --> 00:16:30,160
One of my friends and family,

372
00:16:30,160 --> 00:16:32,160
they say, oh, I tried to watch it,

373
00:16:32,160 --> 00:16:34,960
but I just couldn't understand what you were talking about.

374
00:16:34,960 --> 00:16:37,200
It's so funny because from my perspective,

375
00:16:37,200 --> 00:16:39,960
we're really making things accessible

376
00:16:39,960 --> 00:16:41,440
and dumbing things down as much as possible.

377
00:16:41,840 --> 00:16:44,000
And I still think of myself as completely clueless,

378
00:16:44,000 --> 00:16:45,640
but it's quite deceptive, isn't it,

379
00:16:45,640 --> 00:16:47,800
that you don't realize you're talking another language.

380
00:16:47,800 --> 00:16:48,640
Yeah, it's funny.

381
00:16:48,640 --> 00:16:51,200
I mean, my mom, I said, here's this video,

382
00:16:51,200 --> 00:16:52,800
and she has no idea what this is,

383
00:16:52,800 --> 00:16:55,200
and but she showed her sister who's like eight years old

384
00:16:55,200 --> 00:16:56,800
and didn't know anything.

385
00:16:56,800 --> 00:16:59,600
And her sister was just like, you know,

386
00:16:59,600 --> 00:17:01,560
it seemed like you were really winning in that discussion,

387
00:17:01,560 --> 00:17:03,840
even though I have no idea what you're talking about.

388
00:17:03,840 --> 00:17:04,680
That's cool.

389
00:17:04,680 --> 00:17:05,520
Well, that's good.

390
00:17:05,520 --> 00:17:07,800
I mean, at least the tone sounded like I'm winning.

391
00:17:08,760 --> 00:17:10,120
Well, yeah, it's kind of funny.

392
00:17:10,120 --> 00:17:11,600
Well, speaking of winning,

393
00:17:11,600 --> 00:17:14,560
and that's kind of the question I wanted to ask you is that,

394
00:17:14,560 --> 00:17:16,040
you know, it's one of those ideas,

395
00:17:16,040 --> 00:17:17,560
Tim mentioned these kind of ideas

396
00:17:17,560 --> 00:17:19,840
that the first time you hear them,

397
00:17:19,840 --> 00:17:21,680
and after you kind of digested them a bit,

398
00:17:21,680 --> 00:17:23,320
then you start seeing them everywhere,

399
00:17:23,320 --> 00:17:25,560
and you start seeing, you know, connections,

400
00:17:25,560 --> 00:17:26,680
connections to it everywhere.

401
00:17:26,680 --> 00:17:29,880
And I think your ideas are like that.

402
00:17:29,880 --> 00:17:31,480
At least they were for me.

403
00:17:31,480 --> 00:17:34,760
And I started come to this line of thinking

404
00:17:34,760 --> 00:17:35,960
that I just wanted to ask you about,

405
00:17:35,960 --> 00:17:37,800
just to see if it's completely off base,

406
00:17:37,800 --> 00:17:40,280
or if there's any, maybe any kind of truth to it,

407
00:17:40,280 --> 00:17:43,360
which is, and I know I keep repeating this, Tim,

408
00:17:43,360 --> 00:17:44,200
and I apologize,

409
00:17:44,200 --> 00:17:47,120
but one of the most beautiful quotes I ever heard

410
00:17:47,120 --> 00:17:50,520
was from Claude Shannon many decades ago.

411
00:17:50,520 --> 00:17:53,760
I heard this, and he said, you know,

412
00:17:53,760 --> 00:17:58,280
we have knowledge of the past, but we cannot change it.

413
00:17:58,280 --> 00:18:02,360
We have no knowledge of the future, but we control it.

414
00:18:02,360 --> 00:18:05,080
And I realized back then that there's this duality,

415
00:18:05,080 --> 00:18:07,760
and it's reflected there in that asymmetry of time,

416
00:18:07,760 --> 00:18:11,280
but also in the way I think about science and engineering.

417
00:18:11,280 --> 00:18:13,720
So there are the two sides of a coin

418
00:18:13,720 --> 00:18:18,080
where scientists use engineering to control

419
00:18:18,080 --> 00:18:20,640
in order to gain new knowledge.

420
00:18:20,640 --> 00:18:23,240
Whereas engineers take the knowledge that we have,

421
00:18:23,240 --> 00:18:25,200
and they use it to gain control, you know,

422
00:18:25,200 --> 00:18:26,960
by building a better building,

423
00:18:26,960 --> 00:18:29,200
crossing a water stream or whatever.

424
00:18:29,200 --> 00:18:31,200
And it's also similar to the exploration

425
00:18:31,200 --> 00:18:34,480
versus exploitation, you know, trade-off.

426
00:18:34,480 --> 00:18:37,120
And I'm coming around to this way of thinking that,

427
00:18:37,120 --> 00:18:41,080
yeah, you know, objectives can be quite harmful

428
00:18:41,080 --> 00:18:44,280
when you're in that exploration, you know, phase,

429
00:18:44,280 --> 00:18:45,720
trying to learn new knowledge,

430
00:18:45,720 --> 00:18:47,640
because just as you point out,

431
00:18:47,640 --> 00:18:50,480
who knows what the stepping stones are gonna be like?

432
00:18:50,480 --> 00:18:51,560
And all the kind of, let's say,

433
00:18:51,560 --> 00:18:53,080
quote-unquote counter examples

434
00:18:53,080 --> 00:18:54,240
that I were thinking of before,

435
00:18:54,240 --> 00:18:56,760
it's because they fall more in the engineering side of things,

436
00:18:56,760 --> 00:18:59,040
which is, okay, look, we have some knowledge,

437
00:18:59,040 --> 00:19:00,840
and we do have a very specific goal

438
00:19:00,840 --> 00:19:02,840
we're trying to do with that knowledge,

439
00:19:02,840 --> 00:19:05,240
you know, build a faster rocket or whatever.

440
00:19:05,240 --> 00:19:07,480
And so, yeah, we just go down this kind of very

441
00:19:07,480 --> 00:19:09,960
refinement, objective-based kind of goal,

442
00:19:09,960 --> 00:19:11,960
but it's really not gonna lead us necessarily

443
00:19:11,960 --> 00:19:14,840
to new knowledge or new insight.

444
00:19:14,840 --> 00:19:17,780
Is this kind of a fair dichotomy or not?

445
00:19:18,880 --> 00:19:21,320
Yeah, I mean, you know, it's nice to keep to hear you're

446
00:19:21,320 --> 00:19:23,640
coming around to that way of thinking somewhat.

447
00:19:23,640 --> 00:19:25,640
It's like a while, isn't it?

448
00:19:25,640 --> 00:19:26,800
Hey, I'm open-minded.

449
00:19:28,120 --> 00:19:33,000
It's, yeah, it's interesting to connect to Shannon.

450
00:19:33,040 --> 00:19:35,320
That I appreciate and is an interesting connection.

451
00:19:35,320 --> 00:19:39,840
And I think the dichotomy is pretty fair,

452
00:19:39,840 --> 00:19:43,880
that like in search of discovery,

453
00:19:43,880 --> 00:19:45,720
yeah, a lot of engineering principles

454
00:19:45,720 --> 00:19:50,720
are not really the best tools for innovation necessarily,

455
00:19:51,200 --> 00:19:53,820
especially basic, like basic exploration.

456
00:19:55,120 --> 00:19:59,400
And it's true also that it seems like engineering

457
00:19:59,400 --> 00:20:02,400
culturally kind of pervades scientific education,

458
00:20:02,440 --> 00:20:03,440
at least in some sciences.

459
00:20:03,440 --> 00:20:06,120
I don't know, like I only know about my own experience

460
00:20:06,120 --> 00:20:08,600
in computer science, but like I was in

461
00:20:08,600 --> 00:20:10,960
computer science engineering major at U Penn

462
00:20:10,960 --> 00:20:14,520
that was like computer science engineering is like one thing.

463
00:20:14,520 --> 00:20:17,120
And so, yeah, a lot of engineering philosophy

464
00:20:17,120 --> 00:20:19,000
is like stuck in with the science.

465
00:20:19,880 --> 00:20:24,880
And maybe there is like some conflation there,

466
00:20:26,560 --> 00:20:29,520
but I also think like another thing to consider,

467
00:20:29,520 --> 00:20:31,440
which might be a little bit more out there,

468
00:20:31,440 --> 00:20:35,720
and controversial is that I think it's worth

469
00:20:35,720 --> 00:20:38,200
questioning whether artificial intelligence

470
00:20:38,200 --> 00:20:39,840
really is only science.

471
00:20:41,040 --> 00:20:43,200
And is that actually what we're doing?

472
00:20:43,200 --> 00:20:48,200
Like a lot of the discussion about AI will come back to,

473
00:20:49,680 --> 00:20:50,880
you know, that's not scientific,

474
00:20:50,880 --> 00:20:53,800
or does it kind of attack or a way of looking at like

475
00:20:53,800 --> 00:20:57,640
an idea where like we can sort of invoke science

476
00:20:57,640 --> 00:21:00,240
and slavery to put something down.

477
00:21:01,160 --> 00:21:03,440
And, you know, one interpretation is just that,

478
00:21:04,360 --> 00:21:07,680
you know, maybe there, it was a good scientific insight,

479
00:21:07,680 --> 00:21:09,560
but somebody just missed it or something.

480
00:21:09,560 --> 00:21:11,040
They're not seeing the big picture.

481
00:21:11,040 --> 00:21:12,520
But another interpretation is that actually

482
00:21:12,520 --> 00:21:15,480
this is not science, and it shouldn't be,

483
00:21:15,480 --> 00:21:18,240
it shouldn't be caged inside of that,

484
00:21:18,240 --> 00:21:19,640
only that way of thinking.

485
00:21:21,160 --> 00:21:23,400
And I've actually written about this,

486
00:21:23,400 --> 00:21:25,720
was much less received, much less attention

487
00:21:25,720 --> 00:21:27,600
when I wrote about it, but I thought a lot about it.

488
00:21:27,680 --> 00:21:29,960
Like that in some ways I think AI is,

489
00:21:29,960 --> 00:21:32,200
has a lot of connection to art.

490
00:21:32,200 --> 00:21:33,800
And I don't mean just in the sense that like

491
00:21:33,800 --> 00:21:35,120
you can use AI to generate art.

492
00:21:35,120 --> 00:21:36,960
I mean, you can do that, obviously.

493
00:21:36,960 --> 00:21:38,520
But I mean, in the sense that like,

494
00:21:38,520 --> 00:21:43,040
art is about the reproduction often, not always,

495
00:21:43,040 --> 00:21:45,800
but often it's about the reproduction of natural artifacts

496
00:21:45,800 --> 00:21:47,120
in an artificial way.

497
00:21:47,120 --> 00:21:48,880
Like, I mean, when you paint a picture

498
00:21:48,880 --> 00:21:51,440
of like an apple or something, like a still life,

499
00:21:51,440 --> 00:21:54,080
and nobody's expecting it to be a real apple.

500
00:21:54,080 --> 00:21:55,800
Like that's not the question.

501
00:21:55,800 --> 00:21:58,400
And nobody even expects there's necessarily

502
00:21:58,400 --> 00:22:00,240
a faithful reproduction of an apple.

503
00:22:00,240 --> 00:22:02,560
Or let's say like, you know, Van Gogh's Starry Night,

504
00:22:02,560 --> 00:22:04,840
it doesn't look anything like a real Starry Night

505
00:22:04,840 --> 00:22:06,720
that I bet anybody on earth has ever seen,

506
00:22:06,720 --> 00:22:08,800
even if they're on drugs.

507
00:22:08,800 --> 00:22:10,720
And yet, what does it do?

508
00:22:10,720 --> 00:22:12,560
What is the value of Starry Night then?

509
00:22:12,560 --> 00:22:14,160
Since it's not accurate.

510
00:22:14,160 --> 00:22:16,960
Scientifically, it's not a very good job.

511
00:22:16,960 --> 00:22:19,120
Well, it actually gives you new insight

512
00:22:19,120 --> 00:22:20,680
into something about Starry Night

513
00:22:20,680 --> 00:22:22,320
that you hadn't really thought about.

514
00:22:22,320 --> 00:22:24,240
And that's what's thrilling about it,

515
00:22:24,240 --> 00:22:26,000
is that like it gives you a new perspective

516
00:22:26,000 --> 00:22:28,240
on something that you're familiar with.

517
00:22:28,240 --> 00:22:30,360
AI is also concerned with the reproduction

518
00:22:30,360 --> 00:22:33,040
of natural phenomena in artifice.

519
00:22:33,040 --> 00:22:34,320
It's the same thing.

520
00:22:34,320 --> 00:22:37,840
And in a similar way, like an algorithm is,

521
00:22:37,840 --> 00:22:39,640
I think it's just another way,

522
00:22:39,640 --> 00:22:42,560
it's kind of like another painting.

523
00:22:42,560 --> 00:22:44,520
It's just a painting that's expressed

524
00:22:44,520 --> 00:22:45,880
rather than through brush strokes,

525
00:22:45,880 --> 00:22:47,920
it's expressed through code.

526
00:22:47,920 --> 00:22:49,200
But if you look at it that way,

527
00:22:49,200 --> 00:22:51,120
you can understand that like,

528
00:22:51,120 --> 00:22:53,000
it can be an interpretation of nature

529
00:22:53,000 --> 00:22:54,440
without being accurate.

530
00:22:54,440 --> 00:22:56,040
And that can still be valuable

531
00:22:56,040 --> 00:22:58,120
just as Starry Night is valuable.

532
00:22:58,120 --> 00:23:00,600
And the reason it can be valuable back to science

533
00:23:00,600 --> 00:23:02,160
is because some of the thoughts it gives you

534
00:23:02,160 --> 00:23:03,160
will be stepping stones

535
00:23:03,160 --> 00:23:05,400
that then lead back to scientific insights.

536
00:23:05,400 --> 00:23:07,960
So we're straddling between science and art here.

537
00:23:07,960 --> 00:23:08,800
But the thing is,

538
00:23:08,800 --> 00:23:11,320
it's not necessarily a harmful thing to do.

539
00:23:11,320 --> 00:23:15,120
And so we should actually consider whether,

540
00:23:15,120 --> 00:23:17,200
like to the extent that an algorithm actually

541
00:23:17,200 --> 00:23:18,560
is an artistic inspiration,

542
00:23:18,560 --> 00:23:20,920
which I think a lot of what I was doing was,

543
00:23:20,920 --> 00:23:22,640
like when I was building things like Neat,

544
00:23:22,640 --> 00:23:24,040
I was inspired by like,

545
00:23:24,040 --> 00:23:26,040
the increases in complexity in nature.

546
00:23:26,040 --> 00:23:27,640
I was never under the delusion

547
00:23:27,640 --> 00:23:29,560
that Neat is an accurate depiction

548
00:23:29,560 --> 00:23:31,640
of how evolution works in nature.

549
00:23:31,640 --> 00:23:33,560
But it's kind of an ode to nature in a way,

550
00:23:33,560 --> 00:23:35,480
like the way a painting might be.

551
00:23:35,480 --> 00:23:38,000
But it's never discussed in that way.

552
00:23:38,000 --> 00:23:40,960
You said in your presentation last time

553
00:23:40,960 --> 00:23:44,720
that you were becoming a little bit more radicalized

554
00:23:44,720 --> 00:23:47,240
because you were doing it to not computer scientists,

555
00:23:47,240 --> 00:23:50,080
artists, and all of a sudden they were saying to you,

556
00:23:50,080 --> 00:23:51,480
oh my God, Kenneth, like,

557
00:23:51,480 --> 00:23:53,000
finally, my parents keep saying to me,

558
00:23:53,000 --> 00:23:53,920
why are you doing art?

559
00:23:53,920 --> 00:23:55,200
Why are you wasting your time?

560
00:23:55,200 --> 00:23:56,400
And now I understand,

561
00:23:56,400 --> 00:23:57,880
now I actually have a sense of purpose,

562
00:23:57,880 --> 00:23:59,200
it means something.

563
00:23:59,200 --> 00:24:00,680
And I think what you were saying a second ago,

564
00:24:00,680 --> 00:24:01,680
because it's quite interesting you said,

565
00:24:01,680 --> 00:24:02,880
or a starry night.

566
00:24:02,880 --> 00:24:05,400
And you could say that's a kind of model

567
00:24:05,400 --> 00:24:06,680
or a kind of representation.

568
00:24:06,680 --> 00:24:09,000
But is it fair to say that you're actually more interested

569
00:24:09,000 --> 00:24:12,120
in analogizing the process itself?

570
00:24:12,120 --> 00:24:15,760
Like analogizing the process of discovery in AI with art.

571
00:24:15,760 --> 00:24:17,160
Is that what you mean?

572
00:24:17,160 --> 00:24:18,000
Yeah.

573
00:24:18,000 --> 00:24:19,800
I think there's an analogy between art and AI.

574
00:24:19,800 --> 00:24:21,040
It's more than just process.

575
00:24:21,040 --> 00:24:22,800
It's like, it is a branch of art.

576
00:24:22,800 --> 00:24:26,720
Like it's literally about like reproducing natural phenomena

577
00:24:26,720 --> 00:24:28,800
in artifice, which is what artists do.

578
00:24:29,880 --> 00:24:31,880
The difference is that the,

579
00:24:33,400 --> 00:24:36,440
the, I mean, art is, you know, you might say it's like,

580
00:24:36,440 --> 00:24:37,800
there's more of an aesthetic emphasis

581
00:24:37,800 --> 00:24:41,040
or something in art, but I mean, that's just a choice.

582
00:24:41,040 --> 00:24:43,520
I mean, that's what we're doing in AI.

583
00:24:43,520 --> 00:24:44,720
We're interested in,

584
00:24:44,720 --> 00:24:46,640
so one exception I should say for the artists

585
00:24:46,640 --> 00:24:49,160
who are listening, which are probably few,

586
00:24:49,360 --> 00:24:51,320
but I want to acknowledge that that's not the only thing

587
00:24:51,320 --> 00:24:52,520
that art is concerned with.

588
00:24:52,520 --> 00:24:54,200
When I wrote about this, I actually,

589
00:24:54,200 --> 00:24:55,320
I wrote a paper on this.

590
00:24:55,320 --> 00:24:57,560
It was one of the weirdest things I did

591
00:24:57,560 --> 00:24:59,600
because I actually went to like a real library

592
00:24:59,600 --> 00:25:01,560
and was researching art history and things like that.

593
00:25:01,560 --> 00:25:04,640
I felt like a real like scholar or something in the library.

594
00:25:04,640 --> 00:25:07,920
But so I got a lot of, I got some criticism from artists.

595
00:25:07,920 --> 00:25:09,400
And like one of the things they were pointing out was that

596
00:25:09,400 --> 00:25:11,120
it's not always about the reproduction of nature.

597
00:25:11,120 --> 00:25:13,360
Like there's some modern art that has nothing to do

598
00:25:13,360 --> 00:25:14,400
with reproducing anything.

599
00:25:14,400 --> 00:25:16,160
It's just about like the pure aesthetics.

600
00:25:16,160 --> 00:25:18,960
So let's just acknowledge that that's not all art is about.

601
00:25:18,960 --> 00:25:22,240
But much of art throughout art history has been about that.

602
00:25:22,240 --> 00:25:24,560
It's certainly a part of art is to reproduce

603
00:25:24,560 --> 00:25:25,920
natural phenomena and artifice.

604
00:25:25,920 --> 00:25:27,800
And that's what art AI is.

605
00:25:27,800 --> 00:25:29,120
So it's not just a connection.

606
00:25:29,120 --> 00:25:30,640
I think it is art.

607
00:25:30,640 --> 00:25:32,560
It's just that we're not willing to discuss that side of it

608
00:25:32,560 --> 00:25:35,400
because we're very proud of ourselves for being scientists.

609
00:25:35,400 --> 00:25:37,120
It makes us feel good about ourselves.

610
00:25:37,120 --> 00:25:38,800
But the reason I started thinking about this

611
00:25:38,800 --> 00:25:42,200
is because I was getting in some arguments with people

612
00:25:42,200 --> 00:25:45,360
where the kind of the crux, the breaking point

613
00:25:45,360 --> 00:25:46,840
where they tried to basically stop it

614
00:25:46,840 --> 00:25:48,600
and say this is the end of the argument

615
00:25:48,600 --> 00:25:52,600
was basically to say that what I was saying was not scientific.

616
00:25:52,600 --> 00:25:54,160
Or like this is not a scientific question.

617
00:25:54,160 --> 00:25:56,640
It can't be objectively analyzed.

618
00:25:56,640 --> 00:25:58,080
You can't get an objective,

619
00:25:58,080 --> 00:26:00,560
you can't get falsification on this question.

620
00:26:00,560 --> 00:26:02,400
So it's basically not subject to debate.

621
00:26:02,400 --> 00:26:04,640
Like let's not talk about it because it's pointless.

622
00:26:04,640 --> 00:26:06,800
And I felt like that is just pure cowardice.

623
00:26:06,800 --> 00:26:10,160
Like we are, you are just saying you are afraid to inquire

624
00:26:10,160 --> 00:26:12,920
in directions where you don't know how to falsify.

625
00:26:12,920 --> 00:26:14,360
To me that's just cowardice.

626
00:26:14,360 --> 00:26:17,160
And I do not like associating science with cowardice.

627
00:26:17,160 --> 00:26:21,320
And I started thinking about, am I really doing science?

628
00:26:21,320 --> 00:26:23,320
Like maybe there's another few here.

629
00:26:23,320 --> 00:26:25,680
I don't know that I was like fully just trying

630
00:26:25,680 --> 00:26:30,160
to create generate hypotheses and validate or falsify them.

631
00:26:30,160 --> 00:26:33,560
There's an artistic side to I think creating these algorithms.

632
00:26:33,560 --> 00:26:35,520
And I think it might be healthy to acknowledge that.

633
00:26:35,520 --> 00:26:37,480
It's interesting to think about.

634
00:26:37,480 --> 00:26:41,440
I think your frame of reference doesn't seem to fit

635
00:26:41,440 --> 00:26:43,400
into the paradigm of science in so many ways.

636
00:26:43,400 --> 00:26:45,280
And I remember you said on the show last time

637
00:26:45,280 --> 00:26:48,720
that you were exasperated that pickbreeder was not recognized

638
00:26:48,720 --> 00:26:50,560
as being scientifically useful.

639
00:26:50,560 --> 00:26:54,160
And there was another convergent committee version

640
00:26:54,160 --> 00:26:56,080
of pickbreeder and then the images just looked

641
00:26:56,080 --> 00:26:57,840
like wallpaper backgrounds.

642
00:26:57,840 --> 00:27:01,560
But to your point before, I kind of agree with you

643
00:27:01,560 --> 00:27:05,080
that when you look at what happens in the process of art,

644
00:27:05,080 --> 00:27:07,160
it seems very intelligent.

645
00:27:07,160 --> 00:27:10,560
But I'm trying to understand where the intelligence lies.

646
00:27:10,560 --> 00:27:13,880
Because I'm interested in the idea of it being embodied,

647
00:27:13,880 --> 00:27:16,760
for example, and an emergent phenomenon.

648
00:27:16,760 --> 00:27:20,000
So clearly when human beings collaborate together

649
00:27:20,000 --> 00:27:22,680
in this divergent way, that's something very interesting.

650
00:27:22,680 --> 00:27:26,800
But what would happen if the agents producing the art

651
00:27:26,800 --> 00:27:30,000
were much less intelligent than humans?

652
00:27:30,000 --> 00:27:34,280
Do you think that could still lead to a kind of intelligence

653
00:27:34,280 --> 00:27:35,880
at a larger scale?

654
00:27:36,960 --> 00:27:39,840
Yeah, and I just will also like preface

655
00:27:39,840 --> 00:27:42,040
by just saying that I'm not against science.

656
00:27:42,160 --> 00:27:44,160
I do like being associated with science.

657
00:27:44,160 --> 00:27:46,440
I don't want to be the out-characterized as an artist

658
00:27:46,440 --> 00:27:47,720
only.

659
00:27:47,720 --> 00:27:50,200
I just think that we're straddling both here.

660
00:27:50,200 --> 00:27:52,800
And the mechanisms of science clearly

661
00:27:52,800 --> 00:27:54,200
have served us well in many cases.

662
00:27:54,200 --> 00:27:56,560
So I don't want to be associated with kooky views

663
00:27:56,560 --> 00:27:59,000
that we should get rid of science or something like that.

664
00:27:59,000 --> 00:28:00,480
I just think it expands our horizons

665
00:28:00,480 --> 00:28:03,720
to understand what we're doing and how it relates to art.

666
00:28:03,720 --> 00:28:06,240
And also just before to answer the question,

667
00:28:06,240 --> 00:28:07,680
I think it's also important to note

668
00:28:07,680 --> 00:28:09,520
that I'm not just making the generic point

669
00:28:09,520 --> 00:28:12,560
that there's an aesthetic aspect to science, which

670
00:28:12,560 --> 00:28:14,600
people have written entire books about.

671
00:28:14,600 --> 00:28:16,680
People have pointed out before, not me,

672
00:28:16,680 --> 00:28:18,840
but there's famous books that have

673
00:28:18,840 --> 00:28:23,360
been written about how science has an artistic side,

674
00:28:23,360 --> 00:28:26,120
like an artistic inclination can help you as a scientist.

675
00:28:26,120 --> 00:28:29,120
I mean, this is not new to point these things out.

676
00:28:29,120 --> 00:28:31,160
But I'm trying to make a much more literal point

677
00:28:31,160 --> 00:28:34,680
that AI itself, specifically AI, with the word artificial

678
00:28:34,680 --> 00:28:38,920
in front of it, is really about art in a strong way.

679
00:28:38,960 --> 00:28:41,400
And this is what has not been acknowledged.

680
00:28:41,400 --> 00:28:43,600
This point has also come up in mathematics.

681
00:28:43,600 --> 00:28:46,440
And for the life of me, I can't think of whether it was

682
00:28:46,440 --> 00:28:48,720
Penrose or maybe even Gauss or somebody,

683
00:28:48,720 --> 00:28:51,920
but a legendary mathematician made this point.

684
00:28:51,920 --> 00:28:54,200
They were bemoaning the fact that there wasn't

685
00:28:54,200 --> 00:28:57,120
more creativity in mathematics.

686
00:28:57,120 --> 00:28:59,920
Because if you look at some of the greatest achievements

687
00:28:59,920 --> 00:29:02,520
of mathematics, they were these things that were created

688
00:29:02,520 --> 00:29:08,800
that were entirely new, like calculus from Leibniz

689
00:29:09,480 --> 00:29:10,320
or Newton, right?

690
00:29:10,320 --> 00:29:12,800
I mean, just these new creations, new ways of thinking

691
00:29:12,800 --> 00:29:15,240
about things that were really inspired

692
00:29:15,240 --> 00:29:17,320
from a more artistic sense.

693
00:29:17,320 --> 00:29:20,080
And he was making the same point that like,

694
00:29:20,080 --> 00:29:21,320
that's mathematics.

695
00:29:21,320 --> 00:29:23,040
And to kind of say like, well, no,

696
00:29:23,040 --> 00:29:26,080
if it doesn't have a certain level of rigor,

697
00:29:26,080 --> 00:29:27,400
it doesn't approach mathematics.

698
00:29:27,400 --> 00:29:30,440
Well, in order to get to those forms of rigor,

699
00:29:30,440 --> 00:29:32,280
you first have to have that stepping stone,

700
00:29:32,280 --> 00:29:36,600
that inspiration that generates something brand new.

701
00:29:36,600 --> 00:29:38,640
And I gotta figure this out,

702
00:29:38,640 --> 00:29:41,160
but I forget who it was and they made this point.

703
00:29:41,160 --> 00:29:44,000
So if it applies even to mathematics,

704
00:29:44,000 --> 00:29:45,720
it has to apply also to things

705
00:29:45,720 --> 00:29:47,760
that are more grounded in reality.

706
00:29:47,760 --> 00:29:50,200
So I think your point completely stands.

707
00:29:50,200 --> 00:29:55,200
And it is sad that people tried to dismiss it as,

708
00:29:55,760 --> 00:29:58,480
well, that's not science or that's not mathematics.

709
00:30:00,920 --> 00:30:02,200
There's an interesting response.

710
00:30:02,200 --> 00:30:03,680
There's a good, it's not science.

711
00:30:03,680 --> 00:30:05,720
Like, I wonder where that leads exactly.

712
00:30:05,720 --> 00:30:07,880
It's like, I'm not pretending to be talking about science.

713
00:30:07,880 --> 00:30:09,280
Like, it's still so we should discuss.

714
00:30:09,280 --> 00:30:12,520
Good, it's not already known, I'm doing something new.

715
00:30:12,520 --> 00:30:13,360
Yeah.

716
00:30:14,760 --> 00:30:17,480
So where does the intelligence lie?

717
00:30:17,480 --> 00:30:19,720
I think was where I was getting at.

718
00:30:19,720 --> 00:30:21,720
I'm really fascinated by this idea

719
00:30:21,720 --> 00:30:25,200
that it's not in the brain that it's in the process.

720
00:30:25,200 --> 00:30:27,640
And I think your ideas are going in that direction.

721
00:30:27,640 --> 00:30:29,080
Yeah, I remember your question.

722
00:30:29,080 --> 00:30:34,080
Yeah, yeah, so like, if there was a less intelligent

723
00:30:34,760 --> 00:30:38,680
kind of move towards our less intelligent type of agent

724
00:30:38,680 --> 00:30:41,800
that was involved in an artistic evolution,

725
00:30:41,800 --> 00:30:44,600
like what would be the quality of that?

726
00:30:45,920 --> 00:30:50,720
Yeah, I think, I do think it's possible for there to be

727
00:30:50,720 --> 00:30:55,720
artistic, it's complicated because like art is subjective.

728
00:30:55,720 --> 00:30:58,080
I mean, we're making it for ourselves.

729
00:30:58,080 --> 00:31:01,400
So like, if there were things that aren't us

730
00:31:01,400 --> 00:31:02,880
that are doing something artistic,

731
00:31:03,280 --> 00:31:04,880
presumably they're making it for themselves.

732
00:31:04,880 --> 00:31:07,400
So it might not be interesting to us.

733
00:31:07,400 --> 00:31:10,360
I'm not sure it would, it might be interesting to us,

734
00:31:10,360 --> 00:31:13,480
but it would be a process worth paying attention to

735
00:31:13,480 --> 00:31:15,920
in some way, I do believe that,

736
00:31:15,920 --> 00:31:20,920
because it would have these properties of trends

737
00:31:21,440 --> 00:31:25,200
and stepping stones and like an evolutionary process,

738
00:31:25,200 --> 00:31:27,960
like a phylogeny would result from that.

739
00:31:27,960 --> 00:31:29,680
Whether it's of interest to us is,

740
00:31:29,680 --> 00:31:31,560
I mean, just the artifacts themselves,

741
00:31:31,560 --> 00:31:34,480
I'm not totally sure that they would be.

742
00:31:34,480 --> 00:31:37,280
Cause a lot of, I mean, art does often reference things,

743
00:31:37,280 --> 00:31:39,840
like I said, in nature, cause that's what our experience is.

744
00:31:39,840 --> 00:31:42,520
I mean, if you have no experience of nature,

745
00:31:42,520 --> 00:31:44,560
art can't be about that then.

746
00:31:44,560 --> 00:31:45,520
So what is it about?

747
00:31:45,520 --> 00:31:47,280
And it could be about other things,

748
00:31:47,280 --> 00:31:48,480
just mathematical relationships.

749
00:31:48,480 --> 00:31:51,480
Some small subset of art is, like I said,

750
00:31:51,480 --> 00:31:53,400
like I mean, some modern art has nothing to do

751
00:31:53,400 --> 00:31:55,400
with referencing anything in the world.

752
00:31:58,000 --> 00:32:00,440
So maybe some people would appreciate that,

753
00:32:00,440 --> 00:32:01,760
but even those things seem to relate

754
00:32:01,760 --> 00:32:03,920
to some emotional resonance or something like that.

755
00:32:03,920 --> 00:32:07,200
Like these beings may have no emotion.

756
00:32:07,200 --> 00:32:08,640
So I don't know where you'd enjoy this,

757
00:32:08,640 --> 00:32:11,960
but from a kind of like just like analyzing

758
00:32:11,960 --> 00:32:13,320
the process point of view,

759
00:32:13,320 --> 00:32:14,480
I think it might be interesting

760
00:32:14,480 --> 00:32:16,720
cause there would be such a process.

761
00:32:16,720 --> 00:32:20,080
It's so interesting because the rubber meets the road.

762
00:32:20,080 --> 00:32:23,440
We're talking about this, all of these brains

763
00:32:23,440 --> 00:32:26,000
operating together in this divergent process.

764
00:32:26,000 --> 00:32:28,240
And you were just saying that an artist possibly

765
00:32:28,240 --> 00:32:30,440
has some kind of phenomenological resonance

766
00:32:30,440 --> 00:32:31,760
with a situation.

767
00:32:31,760 --> 00:32:35,000
And I'm interested in the kind of continuum

768
00:32:35,000 --> 00:32:37,320
between brains and the kind of algorithms

769
00:32:37,320 --> 00:32:38,760
we produce in computer science.

770
00:32:38,760 --> 00:32:40,520
Because I think we could all agree

771
00:32:40,520 --> 00:32:43,160
that neural networks and optimization algorithms,

772
00:32:43,160 --> 00:32:44,800
they don't seem very human-like

773
00:32:44,800 --> 00:32:47,120
and they certainly don't think the way humans do,

774
00:32:47,120 --> 00:32:50,360
even if they can produce intelligent senior behavior.

775
00:32:50,360 --> 00:32:52,080
So you really lean into this idea

776
00:32:52,080 --> 00:32:54,520
of trusting our instincts and subjectivity.

777
00:32:54,520 --> 00:32:57,120
And a cynical reading is that our subjectivity

778
00:32:57,120 --> 00:32:58,560
is essentially random.

779
00:32:58,560 --> 00:33:01,800
And a random search would be the most divergent search,

780
00:33:01,800 --> 00:33:03,760
but that would clearly be rubbish, right?

781
00:33:03,760 --> 00:33:05,440
So there must be some kind of continuum

782
00:33:05,440 --> 00:33:07,480
between a totally random search

783
00:33:07,480 --> 00:33:09,960
and a principled divergent search.

784
00:33:09,960 --> 00:33:13,060
So how would you kind of articulate and reason about that?

785
00:33:14,080 --> 00:33:19,080
Yeah, it's true that any kind of divergence

786
00:33:19,080 --> 00:33:21,640
that pushes too far towards a randomness,

787
00:33:21,640 --> 00:33:23,700
I don't think would be interesting.

788
00:33:24,560 --> 00:33:27,000
And I don't think that's what artists

789
00:33:27,000 --> 00:33:28,960
I don't think that's what human artists are doing

790
00:33:28,960 --> 00:33:31,600
because I don't think it would be interesting.

791
00:33:31,600 --> 00:33:34,360
It's just that art is,

792
00:33:35,920 --> 00:33:39,840
they are really concerned with what's interesting, I think,

793
00:33:39,840 --> 00:33:41,520
but without the constraints of science.

794
00:33:41,520 --> 00:33:44,760
So it's not supposed to prove something that you're trying

795
00:33:44,760 --> 00:33:46,920
to figure out whether it's true or not.

796
00:33:46,920 --> 00:33:49,520
It's just supposed to lead to some kind of insight

797
00:33:49,520 --> 00:33:52,200
or feeling or something, it depends on the artist.

798
00:33:52,200 --> 00:33:55,560
And of course that then points back to things we care about

799
00:33:55,560 --> 00:33:57,760
because we're humans and we care about having insights.

800
00:33:57,760 --> 00:34:00,560
And of course the things we want to have insights about vary

801
00:34:00,560 --> 00:34:02,760
but like generally art that we appreciate

802
00:34:02,760 --> 00:34:05,000
like leads you to having some realization

803
00:34:05,000 --> 00:34:07,400
that is generally commonly held

804
00:34:07,400 --> 00:34:10,280
like that people would agree is interesting.

805
00:34:10,280 --> 00:34:14,720
And so artists are exploring that

806
00:34:14,720 --> 00:34:19,240
and that leads to other ideas that might not be art.

807
00:34:19,240 --> 00:34:20,680
You know, I think, yeah, like a scientist

808
00:34:20,680 --> 00:34:24,560
can be inspired by art to think about a phenomenon

809
00:34:25,400 --> 00:34:26,960
or somebody else can be inspired architect,

810
00:34:26,960 --> 00:34:28,480
obviously many times inspired by arts.

811
00:34:28,480 --> 00:34:30,560
There's lots of inspiration that comes out of art

812
00:34:30,560 --> 00:34:31,840
if you feel philosophical as well

813
00:34:31,840 --> 00:34:35,000
like about like how the world works and what matters.

814
00:34:35,000 --> 00:34:36,680
There's plenty of topics to talk about

815
00:34:36,680 --> 00:34:39,800
that aren't falsifiable in a scientific sense.

816
00:34:40,840 --> 00:34:43,480
And, you know, with AI algorithms,

817
00:34:43,480 --> 00:34:47,320
they, I think secretly a lot of the explanation

818
00:34:47,320 --> 00:34:51,440
for what has risen and fallen within AI is that,

819
00:34:51,440 --> 00:34:54,480
not the actual scientific results.

820
00:34:54,480 --> 00:34:56,080
Is that people have resonated.

821
00:34:56,080 --> 00:34:58,040
I like that word because that's really about art.

822
00:34:58,040 --> 00:35:00,640
It's not, it's not about correctness or accuracy.

823
00:35:00,640 --> 00:35:02,000
It's about resonance.

824
00:35:02,000 --> 00:35:04,240
And people have resonated with certain algorithms.

825
00:35:04,240 --> 00:35:05,880
It's like they just felt it.

826
00:35:05,880 --> 00:35:07,400
It got to a point.

827
00:35:07,400 --> 00:35:09,360
Well, this is like an artistic realization,

828
00:35:09,360 --> 00:35:11,280
not a scientific realization.

829
00:35:11,280 --> 00:35:12,880
Like where it resonated with some sense

830
00:35:12,880 --> 00:35:14,680
of what intelligence is for you.

831
00:35:15,680 --> 00:35:16,920
And it's not like the whole thing.

832
00:35:16,920 --> 00:35:19,200
Nobody got the whole deal of intelligence obviously

833
00:35:19,200 --> 00:35:21,360
but some part of it like resonates.

834
00:35:21,360 --> 00:35:23,840
And that can be extremely inspiring.

835
00:35:23,840 --> 00:35:26,160
And I think explains certain like inflection points

836
00:35:26,160 --> 00:35:28,360
in the history of machine learning

837
00:35:28,360 --> 00:35:31,040
where I think it was resonance really that explains it.

838
00:35:31,040 --> 00:35:31,880
Yeah.

839
00:35:31,880 --> 00:35:33,440
And I think it's really interesting what you're saying.

840
00:35:33,440 --> 00:35:36,520
And I think there's a tendency of some

841
00:35:37,400 --> 00:35:41,640
to again dismiss this type of thing as not science,

842
00:35:41,640 --> 00:35:43,400
woo woo, you know, whatever.

843
00:35:43,400 --> 00:35:48,400
But I think that stems from a being insufficiently Darwinian

844
00:35:48,920 --> 00:35:52,400
in the sense that look, whatever's up in our brain,

845
00:35:52,400 --> 00:35:56,160
okay, it's the beneficiary of a billion years

846
00:35:56,160 --> 00:35:58,000
of evolution, okay.

847
00:35:58,000 --> 00:36:01,120
And these insights and intuitions that we have,

848
00:36:01,120 --> 00:36:02,680
even if we're not conscious of them,

849
00:36:02,680 --> 00:36:04,200
maybe they're not happening at a level

850
00:36:04,200 --> 00:36:07,840
that we can analytically break down consciously

851
00:36:07,840 --> 00:36:11,320
and think about could still be extremely useful

852
00:36:11,320 --> 00:36:12,480
and extremely valuable.

853
00:36:12,480 --> 00:36:14,720
And so I have no doubt believing that

854
00:36:14,720 --> 00:36:18,040
when a human mind sees an algorithm even,

855
00:36:18,040 --> 00:36:20,560
it can perceive some connections

856
00:36:20,600 --> 00:36:23,200
to some abstract concepts

857
00:36:23,200 --> 00:36:25,600
that have been proven out through evolution

858
00:36:25,600 --> 00:36:27,760
as being highly, highly useful.

859
00:36:27,760 --> 00:36:29,680
And so you may be seeing those connections.

860
00:36:29,680 --> 00:36:33,120
Is that, I mean, does that capture potentially a fair

861
00:36:33,120 --> 00:36:36,320
and scientific justification of why we should pay attention

862
00:36:36,320 --> 00:36:39,180
to intuition and artistic intuition?

863
00:36:40,360 --> 00:36:42,120
Yeah, I think it's fair.

864
00:36:42,120 --> 00:36:45,440
I think though I wouldn't only couch it

865
00:36:45,440 --> 00:36:47,080
in terms of evolution, I think it's broader

866
00:36:47,080 --> 00:36:48,760
than just from evolution.

867
00:36:48,800 --> 00:36:50,480
Like it's from our experience also,

868
00:36:50,480 --> 00:36:52,240
like experience since you were born,

869
00:36:53,360 --> 00:36:55,880
and that's your memories and the feelings

870
00:36:55,880 --> 00:36:57,520
that you've had over the course of your life

871
00:36:57,520 --> 00:36:59,560
that I think enter into.

872
00:36:59,560 --> 00:37:01,120
Of course, just some evolution explanation

873
00:37:01,120 --> 00:37:02,840
for how you process those experiences,

874
00:37:02,840 --> 00:37:05,120
but the experiences are also part of the background

875
00:37:05,120 --> 00:37:07,960
for what you appreciate and find interesting in your life.

876
00:37:07,960 --> 00:37:12,000
And yeah, I think that that is, as you say,

877
00:37:13,640 --> 00:37:17,560
an important part of the history of ideas,

878
00:37:17,560 --> 00:37:19,000
even in science.

879
00:37:19,000 --> 00:37:22,560
And what I guess, what's actionable about that though,

880
00:37:22,560 --> 00:37:26,960
is that it's interesting to think about the extent to it,

881
00:37:26,960 --> 00:37:31,840
we should actually allow or facilitate discussion

882
00:37:31,840 --> 00:37:33,840
on this level, not at this meta level

883
00:37:33,840 --> 00:37:35,680
that we're talking about, should we do this,

884
00:37:35,680 --> 00:37:38,680
but at the level of here is what resonates to me,

885
00:37:38,680 --> 00:37:40,400
like about the specific thing.

886
00:37:40,400 --> 00:37:42,440
This is why it's interesting, like as a reviewer,

887
00:37:42,440 --> 00:37:46,080
like I don't care if it gets like 5% less accuracy

888
00:37:46,080 --> 00:37:47,320
on this set, it's super interesting

889
00:37:47,320 --> 00:37:50,840
because XYZ, it reminds me of something,

890
00:37:50,840 --> 00:37:54,360
powerfully reminds me, like can we have discussions like this?

891
00:37:54,360 --> 00:37:55,440
We can't right now.

892
00:37:55,440 --> 00:37:57,720
And so it's interesting just to think about that,

893
00:37:57,720 --> 00:38:01,440
like would it help to facilitate progress?

894
00:38:01,440 --> 00:38:02,720
Would it stymie progress?

895
00:38:02,720 --> 00:38:03,880
Because I think most people,

896
00:38:03,880 --> 00:38:06,080
their first good instinct is it's bad for progress,

897
00:38:06,080 --> 00:38:09,320
because it opens the floodgates of sort of like unregulated,

898
00:38:09,320 --> 00:38:11,600
like unempirical type of speculation.

899
00:38:11,600 --> 00:38:13,200
We're all afraid of that.

900
00:38:13,200 --> 00:38:15,880
But I think we should be cautious,

901
00:38:15,920 --> 00:38:17,760
I think we're too afraid of it,

902
00:38:17,760 --> 00:38:19,920
and that like we can handle this,

903
00:38:19,920 --> 00:38:21,840
because like we actually know about

904
00:38:21,840 --> 00:38:22,840
what we're talking about.

905
00:38:22,840 --> 00:38:24,960
Like that's the thing that makes this valid.

906
00:38:24,960 --> 00:38:27,400
Like it's again, like if it was just some random person

907
00:38:27,400 --> 00:38:28,840
on the street, I wouldn't want it to have

908
00:38:28,840 --> 00:38:31,400
an aesthetic discussion of algorithms.

909
00:38:31,400 --> 00:38:33,680
But if it's experts, I don't understand why we're not allowed

910
00:38:33,680 --> 00:38:36,440
to have aesthetic feelings and relating things

911
00:38:36,440 --> 00:38:40,080
in like analogizing, things like that seem perfectly fine.

912
00:38:40,080 --> 00:38:41,320
We should be able to do, I would say,

913
00:38:41,320 --> 00:38:43,480
even if you can't do that, there's a problem.

914
00:38:43,480 --> 00:38:45,560
Like why are you an expert?

915
00:38:45,680 --> 00:38:47,120
Like if you can't make analogies

916
00:38:47,120 --> 00:38:48,680
and actually talk about what's interesting

917
00:38:48,680 --> 00:38:50,480
or inspiring about the work.

918
00:38:50,480 --> 00:38:51,320
Okay.

919
00:38:51,320 --> 00:38:54,080
One thing that I'm wrestling with a little bit here is,

920
00:38:54,080 --> 00:38:56,640
I mean, Douglas Hofstadter, the famous Douglas Hofstadter,

921
00:38:56,640 --> 00:38:59,600
he once said that he was terrified

922
00:38:59,600 --> 00:39:03,280
that AI might be disappointingly simple to mechanize.

923
00:39:03,280 --> 00:39:05,840
And I think a lot of the stuff that we're talking about here

924
00:39:05,840 --> 00:39:07,560
is, I mean, we're talking about subjectivity,

925
00:39:07,560 --> 00:39:10,040
but also we're talking about the externalization

926
00:39:10,040 --> 00:39:11,360
of intelligence.

927
00:39:11,360 --> 00:39:14,280
So rather than it being encapsulated

928
00:39:14,360 --> 00:39:17,440
in an individual brain, a lot of it is emergent

929
00:39:17,440 --> 00:39:21,520
and can be thought of as something completely different.

930
00:39:23,720 --> 00:39:26,840
I'm concerned about the lack of free will

931
00:39:26,840 --> 00:39:29,960
for want of a better, so we've been dealt

932
00:39:29,960 --> 00:39:32,680
with the experiences and the environment

933
00:39:32,680 --> 00:39:34,040
that we have in life.

934
00:39:34,040 --> 00:39:37,240
And to a certain extent, is it still intelligent

935
00:39:37,240 --> 00:39:39,840
if our cards are marked?

936
00:39:40,680 --> 00:39:43,360
If everything has been mapped out in my life

937
00:39:43,400 --> 00:39:46,680
as a function of the environment that I'm in

938
00:39:46,680 --> 00:39:48,760
and my life experiences.

939
00:39:48,760 --> 00:39:52,160
I guess I have this, just as Douglas Hofstadter did,

940
00:39:52,160 --> 00:39:55,120
I have this very fanciful idea in my mind

941
00:39:55,120 --> 00:39:57,960
of what intelligence is, that it's infinitely nuanced.

942
00:39:57,960 --> 00:40:00,720
And we have this phenomenological experience.

943
00:40:00,720 --> 00:40:05,720
And, you know, for example, Hofstadter spoke about Chopin,

944
00:40:05,720 --> 00:40:07,520
this beautiful piece of music

945
00:40:07,520 --> 00:40:10,040
and what the infinite nuance and subtlety

946
00:40:10,040 --> 00:40:12,320
that must have gone through his mind when he created it.

947
00:40:12,360 --> 00:40:16,120
And wouldn't it be horrible if that was just the result

948
00:40:16,120 --> 00:40:20,640
of quite a simple process that you could define using code?

949
00:40:22,160 --> 00:40:23,960
This is an interesting question.

950
00:40:25,120 --> 00:40:28,640
I think, yeah, it's clear that there's a movement

951
00:40:28,640 --> 00:40:32,200
in machine learning towards that kind of perspective

952
00:40:32,200 --> 00:40:34,080
of basically simplicity.

953
00:40:34,080 --> 00:40:35,800
And it goes back before deep learning.

954
00:40:35,800 --> 00:40:39,440
I mean, people were observing that they say, well, like,

955
00:40:39,440 --> 00:40:41,440
you know, cortical circuits like in the brain

956
00:40:41,480 --> 00:40:43,880
all share like a huge amount of similarity.

957
00:40:43,880 --> 00:40:46,280
It's like, it is possible it's all the same algorithm

958
00:40:46,280 --> 00:40:49,840
all throughout and there's just some simple explanation.

959
00:40:49,840 --> 00:40:52,680
And then, like, when we see, like,

960
00:40:52,680 --> 00:40:55,360
things like really large language models

961
00:40:55,360 --> 00:40:59,160
that are basically like uniform architectural structures

962
00:40:59,160 --> 00:41:02,200
that just get bigger, it seems to,

963
00:41:02,200 --> 00:41:03,800
it seems to point in that direction

964
00:41:03,800 --> 00:41:06,680
that like it's not like a bunch of really complex,

965
00:41:06,680 --> 00:41:09,040
rich subtlety like going on through the system.

966
00:41:09,440 --> 00:41:12,280
We don't know, though, yet, we don't know.

967
00:41:12,280 --> 00:41:14,080
I mean, the jury's still out.

968
00:41:14,080 --> 00:41:16,400
We haven't actually gotten to human level.

969
00:41:16,400 --> 00:41:20,200
And I think that, yeah, and I mean,

970
00:41:20,200 --> 00:41:22,520
you can also point to evolutionary processes too.

971
00:41:22,520 --> 00:41:24,360
And they're also simple, like there's a simple thing

972
00:41:24,360 --> 00:41:26,480
and this explains everything,

973
00:41:26,480 --> 00:41:28,560
like it's not really that interesting of a thing

974
00:41:28,560 --> 00:41:30,480
in and of itself.

975
00:41:30,480 --> 00:41:32,200
I don't, I guess to me it's just,

976
00:41:32,200 --> 00:41:33,840
we would like to know the answer to this.

977
00:41:33,840 --> 00:41:35,920
Like, can you actually get these things to work

978
00:41:35,920 --> 00:41:37,200
through very simple processes?

979
00:41:37,200 --> 00:41:40,040
That's probably really important to know,

980
00:41:40,040 --> 00:41:43,120
like just in terms of being able to do machine learning.

981
00:41:43,120 --> 00:41:45,200
But I don't think for me it would be

982
00:41:46,560 --> 00:41:49,440
that disappointing one way or another, I think.

983
00:41:49,440 --> 00:41:52,760
Cause I think the subtlety is still in there.

984
00:41:52,760 --> 00:41:54,840
It just came in through a different channel.

985
00:41:54,840 --> 00:41:57,760
Like, okay, maybe the subtlety is not in the architecture.

986
00:41:57,760 --> 00:42:00,040
I personally think there is subtlety in the architecture.

987
00:42:00,040 --> 00:42:03,440
That's my guess, like it's not gonna be super simple.

988
00:42:03,440 --> 00:42:05,040
But let's say it doesn't have to be,

989
00:42:05,040 --> 00:42:07,120
it could be all like uniform.

990
00:42:07,120 --> 00:42:11,000
But then, like what you do and what you care about

991
00:42:11,000 --> 00:42:14,440
can still be like the constellation of stuff

992
00:42:14,440 --> 00:42:17,480
that you learned, which you learned over your lifetime.

993
00:42:17,480 --> 00:42:20,720
Like what that amount to, you know, in aggregate

994
00:42:20,720 --> 00:42:24,200
can still be, I think, highly rich and subtle

995
00:42:24,200 --> 00:42:26,000
in its connectivity.

996
00:42:26,000 --> 00:42:28,440
It's just a structure that emerged from a simple process

997
00:42:28,440 --> 00:42:29,480
which allowed it to emerge,

998
00:42:29,480 --> 00:42:32,140
but then the structure itself is complex.

999
00:42:32,140 --> 00:42:33,760
So I don't think it would diminish sort of

1000
00:42:33,800 --> 00:42:38,400
like the grandiosity of like what we are to me.

1001
00:42:38,400 --> 00:42:40,160
I could see other people might think otherwise,

1002
00:42:40,160 --> 00:42:41,640
but it's okay.

1003
00:42:41,640 --> 00:42:42,960
I don't know what the actual truth is,

1004
00:42:42,960 --> 00:42:44,640
but that wouldn't necessarily bother me.

1005
00:42:44,640 --> 00:42:45,960
Yeah, I agree with you.

1006
00:42:45,960 --> 00:42:49,080
It doesn't decrease the grandiosity of what we are.

1007
00:42:49,080 --> 00:42:50,400
Consciousness you're talking about?

1008
00:42:50,400 --> 00:42:54,320
I was surprised that the guy has so much conversation.

1009
00:42:55,440 --> 00:42:57,520
I didn't realize it struck a chord.

1010
00:42:57,520 --> 00:42:59,720
I find that I think we don't know

1011
00:42:59,720 --> 00:43:01,960
what the motivation is behind that tweet.

1012
00:43:01,960 --> 00:43:06,960
It's, I'm assuming it was meant to provoke conversation.

1013
00:43:07,240 --> 00:43:09,080
There's no depth in that tweet at all,

1014
00:43:09,080 --> 00:43:10,480
but it's not because it's dumb.

1015
00:43:10,480 --> 00:43:12,520
It's because like it's a tweet and there's no room

1016
00:43:12,520 --> 00:43:15,440
to actually talk about all the complexity of the issue.

1017
00:43:15,440 --> 00:43:17,720
And so I'm assuming that he's not actually making an argument

1018
00:43:17,720 --> 00:43:20,560
that he even thinks is like persuasive one way or another.

1019
00:43:20,560 --> 00:43:22,620
He's just provoking discussion.

1020
00:43:22,620 --> 00:43:24,320
And as such, I think it's effective.

1021
00:43:24,320 --> 00:43:26,320
It provoked discussion certainly.

1022
00:43:26,320 --> 00:43:27,600
It got a lot of discussion going

1023
00:43:27,600 --> 00:43:30,280
and allowed people to show their cards on this.

1024
00:43:30,280 --> 00:43:32,400
And I'm actually curious about what people's cards are

1025
00:43:32,400 --> 00:43:34,880
on this, like people don't talk about this that much.

1026
00:43:34,880 --> 00:43:37,000
I think it's not, it's not immediately germane

1027
00:43:37,000 --> 00:43:38,240
to making progress in machine learning.

1028
00:43:38,240 --> 00:43:40,360
So it's in some ways you might think it was a waste of time

1029
00:43:40,360 --> 00:43:43,120
because we can't really use this discussion to get anywhere

1030
00:43:43,120 --> 00:43:45,480
and people are busy trying to publish papers and stuff.

1031
00:43:45,480 --> 00:43:47,800
But I'm just just personally curious like

1032
00:43:47,800 --> 00:43:49,640
about what people think in this field

1033
00:43:49,640 --> 00:43:51,200
because it's obviously relevant

1034
00:43:51,200 --> 00:43:52,840
like to what we're trying to do.

1035
00:43:52,840 --> 00:43:54,840
I mean, maybe we connected into quite a lot

1036
00:43:54,840 --> 00:43:56,400
of the symbolic community.

1037
00:43:56,400 --> 00:43:59,000
And on LinkedIn, everyone was just posting saying,

1038
00:43:59,000 --> 00:44:02,120
oh my God, that this hype is getting out of control.

1039
00:44:02,120 --> 00:44:04,000
The runaway train of deep learning.

1040
00:44:04,000 --> 00:44:05,880
No, it's like, oh, you're basically just like

1041
00:44:05,880 --> 00:44:06,720
feeding into the hype.

1042
00:44:06,720 --> 00:44:08,800
But I could interpret that to be completely independent

1043
00:44:08,800 --> 00:44:10,440
from deep learning height.

1044
00:44:10,440 --> 00:44:11,280
Well.

1045
00:44:11,280 --> 00:44:12,120
It's just basically saying like,

1046
00:44:12,120 --> 00:44:14,320
is there a threshold that's crossed

1047
00:44:14,320 --> 00:44:17,740
where there actually is like conscious phenomenon happening?

1048
00:44:18,960 --> 00:44:20,640
And it doesn't mean that what we're doing

1049
00:44:20,640 --> 00:44:21,960
is right at all right now.

1050
00:44:21,960 --> 00:44:23,160
It's still an interesting question.

1051
00:44:23,160 --> 00:44:27,320
I love it because it is provoking this conversation

1052
00:44:27,320 --> 00:44:30,920
that we need to be a bit more defined

1053
00:44:30,920 --> 00:44:32,440
in what we mean by consciousness

1054
00:44:32,440 --> 00:44:34,040
or at least to think about it.

1055
00:44:34,040 --> 00:44:36,920
So I thought it was for the purpose that can things

1056
00:44:36,920 --> 00:44:38,680
which is to provoke conversation.

1057
00:44:40,600 --> 00:44:42,800
And I will admit it provoked me to tweet something.

1058
00:44:42,800 --> 00:44:45,280
I actually don't tweet much

1059
00:44:45,280 --> 00:44:47,000
but I actually tweeted something last night

1060
00:44:47,000 --> 00:44:49,920
because I just couldn't resist it unconsciousness.

1061
00:44:49,920 --> 00:44:50,960
What did you tweet?

1062
00:44:50,960 --> 00:44:52,560
I don't know if it'll upset you maybe.

1063
00:44:52,560 --> 00:44:53,400
I don't know.

1064
00:44:53,400 --> 00:44:54,240
Yeah, tell us.

1065
00:44:54,240 --> 00:44:55,080
Tell us what did you tweet.

1066
00:44:55,080 --> 00:44:55,920
What's upset him?

1067
00:44:55,920 --> 00:44:56,760
What did I tweet?

1068
00:44:56,760 --> 00:44:58,480
I'd have to look at my phone to remember that

1069
00:44:58,480 --> 00:44:59,600
what I said exactly.

1070
00:44:59,600 --> 00:45:00,960
Trigger warning, Tim.

1071
00:45:00,960 --> 00:45:01,800
Be careful.

1072
00:45:03,040 --> 00:45:05,560
Yeah, so this is actually connected to our discussion

1073
00:45:05,560 --> 00:45:07,520
in some subtle way here that we've been having

1074
00:45:07,520 --> 00:45:09,480
because like basically my tweet on consciousness

1075
00:45:09,480 --> 00:45:12,600
I was pointing out that I noticed

1076
00:45:12,600 --> 00:45:14,880
like since the original consciousness tweet

1077
00:45:14,880 --> 00:45:18,160
like a lot of people making off-handed comments

1078
00:45:18,160 --> 00:45:21,200
basically dismissing consciousness is not a good topic

1079
00:45:21,200 --> 00:45:23,800
because it lacks an objective measure.

1080
00:45:23,800 --> 00:45:26,080
This is an easy way to get out of this.

1081
00:45:27,040 --> 00:45:28,800
And the thing that's being missed here

1082
00:45:28,800 --> 00:45:30,680
is that is precisely why it's fascinating.

1083
00:45:30,680 --> 00:45:32,960
Like it is the phenomenon of subjectivity.

1084
00:45:32,960 --> 00:45:35,120
It cannot have an objective measure

1085
00:45:35,120 --> 00:45:37,600
unless we're talking about a superficial aspect of it

1086
00:45:37,600 --> 00:45:38,920
but the interesting aspect of it

1087
00:45:38,920 --> 00:45:40,840
is the part that's hard to talk about.

1088
00:45:40,840 --> 00:45:43,640
And so it's literally what it's like from the inside.

1089
00:45:43,640 --> 00:45:46,200
And so the idea that we cannot discuss that

1090
00:45:46,200 --> 00:45:48,800
is an interesting idea is exactly the kind of cowardice

1091
00:45:48,800 --> 00:45:49,800
that I'm talking about.

1092
00:45:49,800 --> 00:45:51,760
We're using science to block us

1093
00:45:51,760 --> 00:45:54,600
from exploring something that's uncomfortable.

1094
00:45:54,600 --> 00:45:56,600
And if science lacks the tools,

1095
00:45:56,600 --> 00:45:57,720
like if that's what we're saying,

1096
00:45:57,720 --> 00:46:01,000
it lacks the tools to address consciousness

1097
00:46:01,000 --> 00:46:02,520
because it is subjective.

1098
00:46:02,520 --> 00:46:04,680
That's not an indictment of consciousness as a concept,

1099
00:46:04,680 --> 00:46:05,840
it's an indictment of science.

1100
00:46:05,840 --> 00:46:06,680
That's right.

1101
00:46:06,680 --> 00:46:07,520
You've got me there, I must admit

1102
00:46:07,520 --> 00:46:09,040
that's a very clever response.

1103
00:46:09,040 --> 00:46:10,160
There's my tweet.

1104
00:46:10,160 --> 00:46:11,880
Yeah, I don't know how to respond to that

1105
00:46:11,880 --> 00:46:14,600
but I mean, I personally like to think of consciousness

1106
00:46:14,600 --> 00:46:16,640
as being the like qualia

1107
00:46:16,640 --> 00:46:20,160
and the subjective phenomenological experience.

1108
00:46:20,160 --> 00:46:22,160
And I thought it was a stretch to say

1109
00:46:22,160 --> 00:46:24,680
that something like GPT-3

1110
00:46:24,680 --> 00:46:27,920
could possibly have any kind of subjective experience.

1111
00:46:27,920 --> 00:46:31,080
I find it a stretch that did you have subjective experience?

1112
00:46:31,080 --> 00:46:32,080
I mean, or did I do?

1113
00:46:32,080 --> 00:46:35,600
I mean, why a bag of atoms has a subjective experience

1114
00:46:35,600 --> 00:46:36,440
like qualia?

1115
00:46:36,440 --> 00:46:37,640
I have no idea why that would be.

1116
00:46:37,640 --> 00:46:38,760
This is the problem, right?

1117
00:46:38,760 --> 00:46:43,280
So we know quite a few people and they are so cynical

1118
00:46:43,280 --> 00:46:47,880
and they argue that intelligence is just a parlor trick

1119
00:46:47,880 --> 00:46:49,680
that GPT-3 is intelligent

1120
00:46:49,680 --> 00:46:51,000
and we're not really intelligent.

1121
00:46:51,000 --> 00:46:54,800
And when we think we're thinking,

1122
00:46:54,800 --> 00:46:56,720
we're just doing some hash table lookup

1123
00:46:56,720 --> 00:46:58,240
and it's all a trick.

1124
00:46:58,240 --> 00:47:01,880
Yeah, I mean, there are people that have argued

1125
00:47:01,880 --> 00:47:03,840
that consciousness is just like a trick.

1126
00:47:03,840 --> 00:47:07,640
And we're just confused when we think we're conscious.

1127
00:47:07,640 --> 00:47:09,480
It's really nothing special going on.

1128
00:47:09,480 --> 00:47:10,640
Yeah, I mean, if you take that view,

1129
00:47:10,640 --> 00:47:12,360
then none of this is very interesting.

1130
00:47:12,360 --> 00:47:13,280
I don't take that view

1131
00:47:13,280 --> 00:47:15,240
because I believe that there are qualia

1132
00:47:15,240 --> 00:47:17,760
but that's just a belief I can't prove anything.

1133
00:47:17,760 --> 00:47:19,080
But then again, how could I?

1134
00:47:19,080 --> 00:47:20,880
It's a subjective discussion.

1135
00:47:20,880 --> 00:47:24,440
And so like the real issue at hand here

1136
00:47:24,440 --> 00:47:28,600
is like whether we think that subjective phenomena are real,

1137
00:47:28,600 --> 00:47:30,280
like do they actually exist?

1138
00:47:32,280 --> 00:47:33,880
To me, it's worth discussing

1139
00:47:33,880 --> 00:47:36,560
even though it's actually outside of the bounds

1140
00:47:36,560 --> 00:47:38,200
of current science.

1141
00:47:38,200 --> 00:47:41,040
I don't know any way that we can look at this empirically

1142
00:47:41,040 --> 00:47:43,400
but I don't find that ambiguity uncomfortable.

1143
00:47:43,400 --> 00:47:44,240
I think it's interesting.

1144
00:47:44,240 --> 00:47:45,080
I like things that are ambiguous.

1145
00:47:45,080 --> 00:47:46,160
That's where we start learning things.

1146
00:47:46,160 --> 00:47:47,280
I have to hand it to you.

1147
00:47:47,280 --> 00:47:48,640
You've really got me there.

1148
00:47:50,320 --> 00:47:53,280
Okay, well, that's good.

1149
00:47:53,280 --> 00:47:54,120
Yeah.

1150
00:47:54,120 --> 00:47:54,960
Made a point.

1151
00:47:54,960 --> 00:47:56,400
Yeah, I mean, part of this,

1152
00:47:56,400 --> 00:47:59,000
and some of those points are somewhat old.

1153
00:47:59,000 --> 00:48:01,280
Like the point has been made, I think many times

1154
00:48:01,280 --> 00:48:05,280
that science is lack of ability to describe consciousness

1155
00:48:05,280 --> 00:48:07,560
is not an indictment of consciousness.

1156
00:48:07,560 --> 00:48:09,280
It's an indictment of science.

1157
00:48:09,280 --> 00:48:10,800
I mean, there's things missing from it.

1158
00:48:10,800 --> 00:48:12,600
We need to expand it a bit.

1159
00:48:12,600 --> 00:48:13,920
That you're quoting a tweet there

1160
00:48:13,920 --> 00:48:14,760
but someone probably said it before.

1161
00:48:15,520 --> 00:48:18,000
Yeah, it's kind of an old indictment of it

1162
00:48:18,000 --> 00:48:23,080
but I think what I often see is that people often cling

1163
00:48:23,080 --> 00:48:26,280
to kind of extreme definitions of things

1164
00:48:26,280 --> 00:48:29,120
because if they're confronted with a middle ground

1165
00:48:29,120 --> 00:48:32,080
that's completely reasonable, it's just boring.

1166
00:48:32,080 --> 00:48:35,480
Like they almost just can't accept that that's the answer.

1167
00:48:35,480 --> 00:48:39,880
For example, for me, consciousness, from my perspective,

1168
00:48:39,880 --> 00:48:43,880
it's definitely a pattern of neural activity in the brain

1169
00:48:43,920 --> 00:48:45,880
and it's probably one that's doing something

1170
00:48:45,880 --> 00:48:48,000
like analyzing the neural activity

1171
00:48:48,000 --> 00:48:51,760
of other parts of the brain and or itself and that's it.

1172
00:48:51,760 --> 00:48:54,440
Like it's, you know, what's the big mystery here

1173
00:48:54,440 --> 00:48:57,840
but that definition is almost like too easy

1174
00:48:57,840 --> 00:49:00,040
and too reasonable and then we have to start talking about,

1175
00:49:00,040 --> 00:49:03,000
yeah, but what does it feel like to be that,

1176
00:49:03,000 --> 00:49:04,520
you know, that pattern of neurons?

1177
00:49:04,520 --> 00:49:06,160
And I saw this happen like in a debate

1178
00:49:06,160 --> 00:49:11,160
between Daniel Dennett and Sam Harris about free will, okay?

1179
00:49:11,600 --> 00:49:14,320
Where Daniel Dennett is saying, look, to me,

1180
00:49:14,320 --> 00:49:18,720
free will is the fact that you can evaluate options

1181
00:49:18,720 --> 00:49:21,360
and you evaluate those options

1182
00:49:21,360 --> 00:49:24,680
and one path is taken based on that evaluation.

1183
00:49:24,680 --> 00:49:26,840
So for example, a chess program,

1184
00:49:26,840 --> 00:49:29,320
if it's evaluating the board possibility

1185
00:49:29,320 --> 00:49:31,400
and doing a Monte Carlo tree search

1186
00:49:31,400 --> 00:49:34,760
and it evaluates one as being the best option

1187
00:49:34,760 --> 00:49:37,640
and it takes that path, that's free will.

1188
00:49:37,640 --> 00:49:40,800
In other words, it's freedom of parameter space.

1189
00:49:41,040 --> 00:49:43,360
It's, you know, freedom of options.

1190
00:49:43,360 --> 00:49:47,120
And Sam Harris' only response to that is like, well, okay,

1191
00:49:47,120 --> 00:49:49,000
but that's not what people think.

1192
00:49:49,000 --> 00:49:52,200
Like that's not what somebody on the street says is free will.

1193
00:49:52,200 --> 00:49:55,880
They think it's this magical thing that, well, who cares?

1194
00:49:55,880 --> 00:49:59,640
I mean, who cares what people think is surprising?

1195
00:49:59,640 --> 00:50:01,680
Like there's people that believe all kinds of things

1196
00:50:01,680 --> 00:50:04,840
that don't have any type of scientific basis

1197
00:50:04,840 --> 00:50:06,120
or mathematical basis.

1198
00:50:06,120 --> 00:50:09,800
We find a very reasonable definition of free will

1199
00:50:09,800 --> 00:50:13,000
that's totally compatible with the reality

1200
00:50:13,000 --> 00:50:15,560
and the pragmatic experience of free will.

1201
00:50:15,560 --> 00:50:18,400
And yet because it's surprising to some people

1202
00:50:18,400 --> 00:50:21,480
or because it's too boring, we just refuse to accept it.

1203
00:50:23,560 --> 00:50:27,960
Yeah, well, you noted that like people tend to gravitate

1204
00:50:27,960 --> 00:50:31,120
to extreme positions, it's pretty clear.

1205
00:50:31,120 --> 00:50:33,120
Politics too, for a lot of reasons.

1206
00:50:34,280 --> 00:50:37,040
But I also think there's something about human nature

1207
00:50:37,080 --> 00:50:39,400
where people don't like to say, I don't know.

1208
00:50:40,600 --> 00:50:43,600
And that's like, it's really interesting, I think,

1209
00:50:43,600 --> 00:50:44,880
that we don't, we don't admit, we don't know.

1210
00:50:44,880 --> 00:50:46,880
People respond with certitude to things

1211
00:50:46,880 --> 00:50:49,040
that we have absolutely no idea about.

1212
00:50:49,040 --> 00:50:51,600
And I feel like that is much more of the issue

1213
00:50:51,600 --> 00:50:54,480
with consciousness is that we really don't know.

1214
00:50:54,480 --> 00:50:58,520
And I disagree with like sort of Dennis' position

1215
00:50:58,520 --> 00:51:01,240
because it's about, he's claiming to know.

1216
00:51:01,240 --> 00:51:04,240
And I think that it's actually most courageous

1217
00:51:04,240 --> 00:51:06,120
just to say, I don't really know what's going on here.

1218
00:51:06,520 --> 00:51:08,800
The reason I think it's totally reasonable to say

1219
00:51:08,800 --> 00:51:12,480
we don't know is because look at polarizes all of us.

1220
00:51:12,480 --> 00:51:14,480
Like this is one of those issues where experts

1221
00:51:14,480 --> 00:51:16,920
can come out of completely different extremes.

1222
00:51:16,920 --> 00:51:18,960
And there's no consensus.

1223
00:51:18,960 --> 00:51:20,080
And like when that's happening,

1224
00:51:20,080 --> 00:51:22,520
probably nobody knows what's going on.

1225
00:51:22,520 --> 00:51:24,280
We have not come to consensus yet.

1226
00:51:24,280 --> 00:51:27,000
And so I think there is something deeper going on here

1227
00:51:27,000 --> 00:51:29,680
that needs to be addressed and it's not simple

1228
00:51:29,680 --> 00:51:32,200
and certitude is not the right response.

1229
00:51:32,200 --> 00:51:34,880
So to me, it's just, I would say I don't really know,

1230
00:51:34,880 --> 00:51:37,560
but I find interesting to delve into what it is

1231
00:51:37,560 --> 00:51:40,200
that I don't know, like the details of what we don't know.

1232
00:51:40,200 --> 00:51:41,320
Cause that's where it gets interesting.

1233
00:51:41,320 --> 00:51:42,760
There's lots of things we do know,

1234
00:51:42,760 --> 00:51:44,200
which is what tends to get rehashed

1235
00:51:44,200 --> 00:51:45,760
when we respond with certitude.

1236
00:51:45,760 --> 00:51:47,520
Like I know what I know, but I don't,

1237
00:51:47,520 --> 00:51:48,840
I'm more interested in what I don't know.

1238
00:51:48,840 --> 00:51:49,800
Well, let me just follow up there.

1239
00:51:49,800 --> 00:51:52,600
So this was a debate between two extremists.

1240
00:51:52,600 --> 00:51:55,200
One extremist saying there's no such thing as very well,

1241
00:51:55,200 --> 00:51:56,520
it's an illusion.

1242
00:51:56,520 --> 00:51:58,560
And the other one, actually a middle ground,

1243
00:51:58,560 --> 00:52:01,840
but at least extreme from a certainty perspective,

1244
00:52:01,840 --> 00:52:03,440
which is saying here's the definition.

1245
00:52:03,440 --> 00:52:06,520
But the reason why I gravitate towards that position

1246
00:52:06,520 --> 00:52:08,280
is because it at least provides us

1247
00:52:08,280 --> 00:52:10,000
with an operational paradigm

1248
00:52:10,000 --> 00:52:12,640
by which we can do exactly what you're suggesting,

1249
00:52:12,640 --> 00:52:14,760
which is explore what we don't know.

1250
00:52:14,760 --> 00:52:16,280
So if we take it as like, okay,

1251
00:52:16,280 --> 00:52:18,800
here's a working definition of free will.

1252
00:52:18,800 --> 00:52:21,240
Now let's find all the areas where it breaks down,

1253
00:52:21,240 --> 00:52:22,880
explore them scientifically.

1254
00:52:22,880 --> 00:52:24,360
It's at least useful, right?

1255
00:52:24,360 --> 00:52:26,640
Whereas an extreme position saying,

1256
00:52:26,640 --> 00:52:29,760
no, nothing is free will, it's an illusion, it's not useful.

1257
00:52:29,760 --> 00:52:31,360
I can't do anything with that.

1258
00:52:32,000 --> 00:52:34,640
I think it's quite hard to have a definition.

1259
00:52:34,640 --> 00:52:37,000
I was challenging one of my friends yesterday, Kenneth,

1260
00:52:37,000 --> 00:52:40,560
about imagine you wanted to become a billionaire,

1261
00:52:40,560 --> 00:52:42,800
give me an objective to optimize.

1262
00:52:42,800 --> 00:52:45,200
And it's really, really difficult

1263
00:52:45,200 --> 00:52:47,200
because you can start to scratch around

1264
00:52:47,200 --> 00:52:50,560
and talk about diversity and information, accumulation,

1265
00:52:50,560 --> 00:52:52,280
all the stuff, novelty, interestingness,

1266
00:52:52,280 --> 00:52:54,320
but you're really scratching around.

1267
00:52:54,320 --> 00:52:55,920
And it's the same thing here.

1268
00:52:55,920 --> 00:52:59,560
We're talking about consciousness and free will.

1269
00:52:59,640 --> 00:53:02,080
And these are very subjective things.

1270
00:53:02,080 --> 00:53:03,480
And when Keith was talking about

1271
00:53:03,480 --> 00:53:05,160
the Dennett-Sam Harris debate,

1272
00:53:06,240 --> 00:53:07,720
because it's not like free will

1273
00:53:07,720 --> 00:53:12,000
is about maximizing the expected reward,

1274
00:53:12,000 --> 00:53:13,920
although perhaps if you created an agent

1275
00:53:13,920 --> 00:53:15,560
to do such a thing it would,

1276
00:53:15,560 --> 00:53:18,000
maybe you could tune it to behave in such a way

1277
00:53:18,000 --> 00:53:19,760
as humans behave.

1278
00:53:19,760 --> 00:53:24,760
But from my perspective, free will is about the subjectivity

1279
00:53:25,080 --> 00:53:28,680
and about the agency, those two things.

1280
00:53:28,680 --> 00:53:30,640
And I can't really describe those two things

1281
00:53:30,640 --> 00:53:32,920
in any more detail than that.

1282
00:53:32,920 --> 00:53:33,840
Yeah.

1283
00:53:33,840 --> 00:53:37,760
Well, one distinction that I think I want to make is that

1284
00:53:37,760 --> 00:53:40,360
I would separate in my kind of like questioning

1285
00:53:40,360 --> 00:53:42,880
and thinking free will from consciousness.

1286
00:53:42,880 --> 00:53:45,360
Like I think it's two different questions for me.

1287
00:53:45,360 --> 00:53:47,080
I could see why you might want to combine them,

1288
00:53:47,080 --> 00:53:48,840
but I just think they're different.

1289
00:53:48,840 --> 00:53:51,160
I've thought much more about consciousness than free will.

1290
00:53:51,160 --> 00:53:53,360
So on that I think I'm not,

1291
00:53:53,360 --> 00:53:54,640
I haven't really thought through

1292
00:53:54,640 --> 00:53:56,760
how to address free will very well.

1293
00:53:57,760 --> 00:53:59,600
But my consciousness I think,

1294
00:53:59,600 --> 00:54:04,040
to me it's about the issue that's really problematic

1295
00:54:04,040 --> 00:54:06,520
is when it comes to like quality and things like that.

1296
00:54:06,520 --> 00:54:07,640
Like there are other aspects of consciousness

1297
00:54:07,640 --> 00:54:09,560
we might talk about that I think they're less problematic,

1298
00:54:09,560 --> 00:54:13,000
but that is very mysterious and I feel unresolved.

1299
00:54:13,000 --> 00:54:15,080
But in case we're making a general point here

1300
00:54:15,080 --> 00:54:17,960
about these kinds of discussions and yeah,

1301
00:54:17,960 --> 00:54:21,280
I think the general points that he's making are reasonable.

1302
00:54:22,160 --> 00:54:26,720
And so we have David Chalmers coming on the show next month.

1303
00:54:26,720 --> 00:54:28,840
Oh, I was gonna say, did you read his book?

1304
00:54:28,840 --> 00:54:30,720
I mean, that's like, now that was a book.

1305
00:54:30,720 --> 00:54:34,400
I really liked that book because it is about not knowing.

1306
00:54:34,400 --> 00:54:36,240
The book is basically trying to tell,

1307
00:54:36,240 --> 00:54:37,080
that's how I interpret it.

1308
00:54:37,080 --> 00:54:37,920
I'm no philosopher,

1309
00:54:37,920 --> 00:54:39,480
so maybe I don't even understand what I'm reading,

1310
00:54:39,480 --> 00:54:41,960
but my interpretation was basically a big argument

1311
00:54:41,960 --> 00:54:43,960
about why we should admit

1312
00:54:43,960 --> 00:54:46,080
that we really don't know what's going on.

1313
00:54:46,080 --> 00:54:47,480
There's very few books like that.

1314
00:54:47,480 --> 00:54:50,000
I love a good book about not knowing things.

1315
00:54:50,040 --> 00:54:51,400
I don't know if you guys know that

1316
00:54:51,400 --> 00:54:54,560
like one of the very first neuroevolution experiments

1317
00:54:54,560 --> 00:54:56,720
cause I was in the field of neuroevolution was Chalmers.

1318
00:54:56,720 --> 00:54:59,960
He actually did it long before all this stuff

1319
00:54:59,960 --> 00:55:00,800
he's famous for.

1320
00:55:01,920 --> 00:55:04,400
He re-evolved the rules of back propagation.

1321
00:55:05,480 --> 00:55:06,320
Amazing.

1322
00:55:07,560 --> 00:55:08,760
Sighted him many times for that.

1323
00:55:08,760 --> 00:55:11,040
Amazing, he comes up absolutely everywhere.

1324
00:55:11,040 --> 00:55:12,600
He's such an interesting guy.

1325
00:55:13,640 --> 00:55:15,880
I wanted to talk a little bit about some of that stuff

1326
00:55:15,880 --> 00:55:18,520
because there's not a lot of new work

1327
00:55:18,520 --> 00:55:19,960
in your space at the moment.

1328
00:55:19,960 --> 00:55:21,760
And I suppose like one way to frame the question

1329
00:55:21,760 --> 00:55:25,120
is clearly poet and enhanced poet are fascinating.

1330
00:55:25,120 --> 00:55:30,120
And they are much more divergent than many other algorithms.

1331
00:55:30,480 --> 00:55:32,600
But are you aware of anything

1332
00:55:32,600 --> 00:55:34,280
which has been artificially created

1333
00:55:34,280 --> 00:55:36,240
which is extremely divergent?

1334
00:55:36,240 --> 00:55:37,680
I mean, more so than poet even

1335
00:55:37,680 --> 00:55:39,520
or is that currently the state of the art?

1336
00:55:40,880 --> 00:55:42,960
Well, there are still things going on.

1337
00:55:42,960 --> 00:55:46,640
Like one place to look is under the name quality diversity.

1338
00:55:46,640 --> 00:55:48,280
Like you can find a website.

1339
00:55:48,720 --> 00:55:50,400
I think it's called Quality Diversity Optimization.

1340
00:55:50,400 --> 00:55:51,560
Unfortunately, the word optimization

1341
00:55:51,560 --> 00:55:52,360
I wouldn't have put in there,

1342
00:55:52,360 --> 00:55:54,640
but that's what they're calling it.

1343
00:55:54,640 --> 00:55:56,160
And that's basically about,

1344
00:55:56,160 --> 00:55:58,440
QD algorithms are basically about like novelty,

1345
00:55:58,440 --> 00:55:59,880
like novelty seeking things combined

1346
00:55:59,880 --> 00:56:01,440
with the notion of quality.

1347
00:56:01,440 --> 00:56:03,640
And so you can see like the latest there.

1348
00:56:03,640 --> 00:56:05,800
Like those are divergent at some level.

1349
00:56:05,800 --> 00:56:07,760
Like almost every paper that there's like 150,

1350
00:56:07,760 --> 00:56:09,320
I think the last time I looked.

1351
00:56:10,200 --> 00:56:13,240
But the thing about it is it is not yet connected

1352
00:56:13,240 --> 00:56:14,440
to the mainstream of machine learning,

1353
00:56:14,440 --> 00:56:15,960
which is why you're not hearing about it

1354
00:56:15,960 --> 00:56:17,480
or noticing it as much.

1355
00:56:17,480 --> 00:56:18,960
That's always been a problem historically.

1356
00:56:18,960 --> 00:56:21,200
Going back to neuro evolution,

1357
00:56:21,200 --> 00:56:23,320
it's often evolutionary,

1358
00:56:23,320 --> 00:56:24,480
but there are some,

1359
00:56:24,480 --> 00:56:26,720
there is some drift out of that I think recently.

1360
00:56:26,720 --> 00:56:29,520
Like people have sent me preprints.

1361
00:56:29,520 --> 00:56:31,640
I think something's gonna come out soon, for example,

1362
00:56:31,640 --> 00:56:34,080
trying to build on like the idea of poet.

1363
00:56:34,080 --> 00:56:35,360
And like that are much more kind of

1364
00:56:35,360 --> 00:56:38,440
machine learning, reinforcement learning oriented.

1365
00:56:38,440 --> 00:56:39,840
And so there will be things.

1366
00:56:39,840 --> 00:56:42,280
There's a trickle coming out in that direction.

1367
00:56:42,280 --> 00:56:43,840
And then now we're seeing like

1368
00:56:43,840 --> 00:56:46,320
there's open-ended learning symposium workshops,

1369
00:56:46,320 --> 00:56:48,000
like popping up at mainstream conferences.

1370
00:56:48,000 --> 00:56:51,520
I know I'm speaking at, I guess, is it iClear, I think?

1371
00:56:51,520 --> 00:56:53,600
There's gonna be a workshop on open-endedness.

1372
00:56:53,600 --> 00:56:57,640
And so there's definitely some momentum.

1373
00:56:57,640 --> 00:57:00,680
I think it's still early and could fizzle out,

1374
00:57:00,680 --> 00:57:02,280
but you're seeing stuff.

1375
00:57:03,440 --> 00:57:05,440
But in any case, to the question,

1376
00:57:05,440 --> 00:57:09,160
do we actually see something more open-ended than poet?

1377
00:57:09,160 --> 00:57:10,800
I think the answer is no currently.

1378
00:57:10,800 --> 00:57:13,800
I'm not aware of everything going on, but that's a pretty,

1379
00:57:14,800 --> 00:57:17,720
like maybe like there's some things

1380
00:57:17,720 --> 00:57:18,960
that improve on it in one way or another,

1381
00:57:18,960 --> 00:57:20,680
but I wouldn't call it more open-ended.

1382
00:57:20,680 --> 00:57:22,880
No, I think that that's a pretty high bar.

1383
00:57:22,880 --> 00:57:26,880
Like, and there is headroom, I think,

1384
00:57:26,880 --> 00:57:29,040
to be more open-ended than poet,

1385
00:57:29,040 --> 00:57:31,560
but it's a high bar, it's really hard.

1386
00:57:31,560 --> 00:57:36,560
And I think when people see poet, they focus,

1387
00:57:37,400 --> 00:57:38,360
from machine learning perspective,

1388
00:57:38,360 --> 00:57:41,840
they focus more on the curriculum learning aspect of it.

1389
00:57:42,480 --> 00:57:43,560
Like the curriculum learning aspect

1390
00:57:43,560 --> 00:57:45,160
is like a certain perspective you could have,

1391
00:57:45,160 --> 00:57:47,280
and you could think about it as like,

1392
00:57:47,280 --> 00:57:50,160
how do we get something really intelligent

1393
00:57:50,160 --> 00:57:53,320
for a certain kind of problem that has generality?

1394
00:57:53,320 --> 00:57:55,160
Like I think about something like that,

1395
00:57:55,160 --> 00:57:58,200
and this is like give some clues in that direction.

1396
00:57:58,200 --> 00:58:00,600
But that's not really going towards the part

1397
00:58:00,600 --> 00:58:03,480
that like inspires me to the open-endedness side of it.

1398
00:58:03,480 --> 00:58:05,360
You know, what I really wanna see is that

1399
00:58:05,360 --> 00:58:08,000
it just continues to invent like totally out

1400
00:58:08,000 --> 00:58:10,120
of the blue crazy stuff forever.

1401
00:58:10,120 --> 00:58:12,560
And that like seems to get less mind-share,

1402
00:58:12,560 --> 00:58:14,440
like that kind of question,

1403
00:58:14,440 --> 00:58:15,960
maybe because it's not very practical,

1404
00:58:15,960 --> 00:58:18,200
or nobody's really sure what we're even talking about,

1405
00:58:18,200 --> 00:58:21,240
like what crazy things you actually want to see.

1406
00:58:21,240 --> 00:58:23,200
But I would, that's the kind of thing

1407
00:58:23,200 --> 00:58:26,720
where I don't think we're seeing a lot of push or progress.

1408
00:58:26,720 --> 00:58:29,720
On the curriculum learning, I do think we see things,

1409
00:58:29,720 --> 00:58:31,800
and they are interesting within that context.

1410
00:58:31,800 --> 00:58:33,720
Yeah, the curriculum learning thing fascinates me,

1411
00:58:33,720 --> 00:58:37,600
because I remember talking about, I think, ICML 2019,

1412
00:58:37,600 --> 00:58:39,200
and he was saying, look on poet,

1413
00:58:39,240 --> 00:58:42,200
there's an example of an agent,

1414
00:58:42,200 --> 00:58:43,640
and we have this curriculum,

1415
00:58:43,640 --> 00:58:45,680
and sometimes we need to kind of shift

1416
00:58:45,680 --> 00:58:48,880
between a very kind of complex environment,

1417
00:58:48,880 --> 00:58:50,440
and then back to a simple environment

1418
00:58:50,440 --> 00:58:52,240
in order to solve this particular problem.

1419
00:58:52,240 --> 00:58:54,120
But you spoke about generality,

1420
00:58:54,120 --> 00:58:57,880
and I would still argue that the kind of program

1421
00:58:57,880 --> 00:59:00,360
learned by poet doesn't have generality,

1422
00:59:00,360 --> 00:59:03,480
but the process which produced it does.

1423
00:59:03,480 --> 00:59:05,200
So Francois Choulet has this measure

1424
00:59:05,200 --> 00:59:07,280
of intelligence conception,

1425
00:59:07,280 --> 00:59:10,600
and he has this idea of intelligence being a process.

1426
00:59:10,600 --> 00:59:12,720
So there's like a meta-learning process,

1427
00:59:12,720 --> 00:59:14,760
and then it can produce skill programs,

1428
00:59:14,760 --> 00:59:17,080
which can then work in any particular situation.

1429
00:59:17,080 --> 00:59:19,760
So I mean, is that similar to your mindset?

1430
00:59:21,080 --> 00:59:23,520
So I agree that it doesn't have generality.

1431
00:59:23,520 --> 00:59:24,720
That's totally true.

1432
00:59:24,720 --> 00:59:27,360
It's totally about hyper-specialization.

1433
00:59:27,360 --> 00:59:28,800
This gets to actually the art aspect,

1434
00:59:28,800 --> 00:59:31,040
the art discussion we were having before.

1435
00:59:31,040 --> 00:59:35,480
To me, that is artistically appealing,

1436
00:59:35,480 --> 00:59:37,720
because it's evocative of nature,

1437
00:59:37,720 --> 00:59:41,440
where you're not going for a super-generalist in general.

1438
00:59:41,440 --> 00:59:45,840
Each niche is basically a hyper-specialized niche,

1439
00:59:45,840 --> 00:59:50,680
which interestingly eventually led to extreme generality,

1440
00:59:50,680 --> 00:59:53,520
like us, like we have an extreme level of generality

1441
00:59:53,520 --> 00:59:54,360
in certain ways.

1442
00:59:54,360 --> 00:59:55,880
Like in certain ways, we're not photosynthetics,

1443
00:59:55,880 --> 00:59:57,520
we don't have that kind of generality,

1444
00:59:57,520 --> 00:59:59,600
but we have intelligent generality,

1445
00:59:59,600 --> 01:00:01,640
but it went through hyper-specialization.

1446
01:00:02,760 --> 01:00:04,360
If you go back through the ancestry,

1447
01:00:04,360 --> 01:00:05,760
you're looking at hyper-specialists,

1448
01:00:05,760 --> 01:00:06,920
not generalists that are trying to become

1449
01:00:06,920 --> 01:00:07,920
more and more intelligent,

1450
01:00:07,920 --> 01:00:09,520
like you're looking at things like flatworms again,

1451
01:00:09,520 --> 01:00:12,240
or not generalists in any sense.

1452
01:00:12,240 --> 01:00:14,640
Like it's a new reorganization of the body plan.

1453
01:00:15,400 --> 01:00:17,000
And so I find, first of all,

1454
01:00:17,000 --> 01:00:18,280
just from an artistic perspective,

1455
01:00:18,280 --> 01:00:23,280
find the depiction of something that is about,

1456
01:00:24,120 --> 01:00:27,600
like continually branching and just interesting aesthetically,

1457
01:00:27,600 --> 01:00:29,240
and something that we should create.

1458
01:00:29,240 --> 01:00:31,440
Like we should create things that do that.

1459
01:00:31,440 --> 01:00:33,040
But what I notice is that always,

1460
01:00:33,040 --> 01:00:34,760
people point to that as a weakness,

1461
01:00:34,760 --> 01:00:36,160
and say, well, there's a caveat here,

1462
01:00:36,160 --> 01:00:38,800
it's very specialist-oriented, you know?

1463
01:00:38,800 --> 01:00:39,920
Why not go for generality?

1464
01:00:39,920 --> 01:00:41,440
And actually you could.

1465
01:00:41,440 --> 01:00:43,760
This is like a fairly intuitive notion,

1466
01:00:43,760 --> 01:00:45,960
that like, yeah, we can get divergent curricula,

1467
01:00:45,960 --> 01:00:49,480
but try to focus it back down to a centralized point

1468
01:00:49,480 --> 01:00:50,520
where we're trying to get generality.

1469
01:00:50,520 --> 01:00:52,560
I mean, I'm not gonna give it exact way you can do that,

1470
01:00:52,560 --> 01:00:55,120
but this is like an intuitive concept, I think,

1471
01:00:55,120 --> 01:00:57,080
to think about doing that.

1472
01:00:57,080 --> 01:00:59,200
But the point I wanna make though is that,

1473
01:00:59,200 --> 01:01:02,120
look, like some really great kinds of generality,

1474
01:01:02,120 --> 01:01:04,160
the stepping stones are through specialists.

1475
01:01:04,160 --> 01:01:05,840
What are we gonna do about that?

1476
01:01:05,840 --> 01:01:07,360
Like, especially like us.

1477
01:01:07,360 --> 01:01:08,600
Like you could claim that like,

1478
01:01:08,600 --> 01:01:11,440
we're just gonna go straight to hypergeneralization.

1479
01:01:11,440 --> 01:01:13,440
Like that's where we're trying to get our super generalists

1480
01:01:13,440 --> 01:01:14,600
or something like that,

1481
01:01:14,600 --> 01:01:16,440
by getting more and more and more and more general.

1482
01:01:16,440 --> 01:01:18,520
Maybe you're right, like deep learning is magic,

1483
01:01:18,520 --> 01:01:20,000
like we just add more data.

1484
01:01:20,000 --> 01:01:21,000
It's not that simple though,

1485
01:01:21,000 --> 01:01:22,320
because the fact that we're admitting

1486
01:01:22,320 --> 01:01:24,200
we need a curriculum means we don't have the data,

1487
01:01:24,200 --> 01:01:26,360
so we have to get it somehow.

1488
01:01:26,360 --> 01:01:28,960
But you also have to just, there's something interesting,

1489
01:01:28,960 --> 01:01:30,800
you have to admit there's something interesting

1490
01:01:30,840 --> 01:01:34,480
about when hyper-specialization actually leads to generalization

1491
01:01:34,480 --> 01:01:37,080
and this kind of paradoxical stepping stone principle,

1492
01:01:37,080 --> 01:01:39,200
that the things that don't resemble what you want

1493
01:01:39,200 --> 01:01:42,160
ultimately are the stepping stones that get you to it.

1494
01:01:42,160 --> 01:01:45,320
And hyper-specialization is like a really powerful thing,

1495
01:01:45,320 --> 01:01:46,680
because it allows you to drop,

1496
01:01:46,680 --> 01:01:48,960
it allows you to make assumptions.

1497
01:01:48,960 --> 01:01:50,960
Like you can assume something about the environment

1498
01:01:50,960 --> 01:01:52,320
you're entering before you entered

1499
01:01:52,320 --> 01:01:54,320
because you're a specialist in that environment.

1500
01:01:54,320 --> 01:01:57,400
And I think that it can be a disability or a liability,

1501
01:01:57,400 --> 01:01:59,440
like if you actually go into environments,

1502
01:01:59,440 --> 01:02:01,720
having no assumptions whatsoever,

1503
01:02:01,720 --> 01:02:04,200
so you have to be ready for all possible contingencies

1504
01:02:04,200 --> 01:02:06,560
under the sun, that's what generalization means

1505
01:02:06,560 --> 01:02:08,160
in a super general sense.

1506
01:02:08,160 --> 01:02:10,200
Like would you want, you know, airline pilots

1507
01:02:10,200 --> 01:02:12,560
to like not be sure whether they're flying a stunt jet

1508
01:02:12,560 --> 01:02:13,760
or a passenger jet?

1509
01:02:13,760 --> 01:02:15,480
Like they've got to do some checks upfront

1510
01:02:15,480 --> 01:02:17,520
to see which scenario they're in.

1511
01:02:17,520 --> 01:02:19,040
Like if you're just a passenger pilot,

1512
01:02:19,040 --> 01:02:20,680
you don't do those checks.

1513
01:02:20,680 --> 01:02:22,600
Like you know what you're doing.

1514
01:02:22,600 --> 01:02:25,480
And so I think there's reason to talk more

1515
01:02:25,480 --> 01:02:28,000
about this issue of like the specialization of poet

1516
01:02:28,000 --> 01:02:30,240
is actually an interesting facet of it

1517
01:02:30,240 --> 01:02:32,280
and not necessarily just like a liability

1518
01:02:32,280 --> 01:02:33,600
that we have to get around.

1519
01:02:33,600 --> 01:02:35,680
I've just thought of an interesting connection

1520
01:02:35,680 --> 01:02:37,480
that hadn't occurred to me before,

1521
01:02:37,480 --> 01:02:41,360
but there is a link between specialization and divergence.

1522
01:02:41,360 --> 01:02:42,800
Because if you think about it at a general age,

1523
01:02:42,800 --> 01:02:44,400
and that's the equivalent of the committee

1524
01:02:44,400 --> 01:02:45,720
that you hate so much.

1525
01:02:45,720 --> 01:02:48,560
And what you were just saying with evolution,

1526
01:02:48,560 --> 01:02:51,600
starting with specialization actually allows you

1527
01:02:51,600 --> 01:02:54,640
to explore many more interesting stepping stones.

1528
01:02:54,640 --> 01:02:56,360
But the thing I want to get to you though

1529
01:02:56,360 --> 01:02:59,080
is intelligence must be specialized.

1530
01:02:59,080 --> 01:03:02,360
I mean certainly even in conceptions like AIXI,

1531
01:03:02,360 --> 01:03:05,200
it's framed in terms of being able to perform tasks

1532
01:03:05,200 --> 01:03:06,480
in certain environments.

1533
01:03:06,480 --> 01:03:08,360
There's no such thing as general intelligence.

1534
01:03:08,360 --> 01:03:09,760
So if you were an alien being

1535
01:03:09,760 --> 01:03:11,480
and you came down to planet Earth,

1536
01:03:11,480 --> 01:03:13,440
would you really see that much of a difference

1537
01:03:13,440 --> 01:03:16,720
between our kind of intelligence and photosynthesis?

1538
01:03:18,160 --> 01:03:20,760
So that is really interesting.

1539
01:03:21,960 --> 01:03:24,840
It clearly originates from specialization.

1540
01:03:25,040 --> 01:03:27,120
I don't think you can deny that.

1541
01:03:27,120 --> 01:03:31,000
The explanatory apparatus are through specialization.

1542
01:03:31,000 --> 01:03:32,320
Why is it what it is?

1543
01:03:33,320 --> 01:03:34,920
It's related to the environment we're in.

1544
01:03:34,920 --> 01:03:36,720
I mean, that must be true, obviously.

1545
01:03:36,720 --> 01:03:40,600
So it has to do with optimizing within that environment.

1546
01:03:40,600 --> 01:03:44,280
But I feel like what's going on is something to do with,

1547
01:03:44,280 --> 01:03:49,040
from that specialization has emerged real generality.

1548
01:03:49,040 --> 01:03:53,240
I feel like our intelligence is sufficiently general

1549
01:03:53,240 --> 01:03:58,000
to move outside of anything in our environment at all.

1550
01:03:58,000 --> 01:04:00,320
It gets to this question where people sometimes say

1551
01:04:00,320 --> 01:04:02,280
that there are certain things we cannot understand.

1552
01:04:02,280 --> 01:04:04,800
Like it's impossible, like as human beings.

1553
01:04:04,800 --> 01:04:06,040
It's usually like, well, why can't we,

1554
01:04:06,040 --> 01:04:07,480
why would there be things we can't understand?

1555
01:04:07,480 --> 01:04:08,920
Well, it's like they're just so far outside

1556
01:04:08,920 --> 01:04:09,760
of our environment.

1557
01:04:09,760 --> 01:04:10,960
They have nothing to do with anything in the experience.

1558
01:04:10,960 --> 01:04:13,840
I think, I don't really fall into that yet, but I think,

1559
01:04:13,840 --> 01:04:16,560
I think we have the capacity to understand literally anything.

1560
01:04:16,560 --> 01:04:18,640
Well, given enough information,

1561
01:04:18,640 --> 01:04:20,240
like obviously we can't know about things

1562
01:04:20,240 --> 01:04:23,040
that we can't actually observe at all.

1563
01:04:23,040 --> 01:04:25,120
So we don't know those things.

1564
01:04:25,120 --> 01:04:26,560
But like if I was given information,

1565
01:04:26,560 --> 01:04:27,880
I believe I could understand the concept.

1566
01:04:27,880 --> 01:04:30,480
Like I could understand where did the universe come from?

1567
01:04:30,480 --> 01:04:34,000
If you told me what happened, like I think there is some,

1568
01:04:34,000 --> 01:04:36,880
it has nothing to do with the kind of situation I come from.

1569
01:04:36,880 --> 01:04:39,120
But I think I have the capacity to generality.

1570
01:04:39,120 --> 01:04:41,440
So I think it's really interesting that,

1571
01:04:41,440 --> 01:04:44,480
that like somehow a degree of generality

1572
01:04:44,480 --> 01:04:47,760
emerged from this specialization, which goes beyond

1573
01:04:47,760 --> 01:04:49,680
just being good in this environment.

1574
01:04:49,680 --> 01:04:50,800
Could I distinguish though,

1575
01:04:50,800 --> 01:04:52,960
because Jeff Hawkins made this point in his book as well,

1576
01:04:53,040 --> 01:04:55,400
what's interesting about humans is for the first time,

1577
01:04:55,400 --> 01:04:57,520
knowledge and genes have been separated.

1578
01:04:57,520 --> 01:04:59,720
So I completely appreciate what you're saying,

1579
01:04:59,720 --> 01:05:02,440
that we can understand the universe and everything in it,

1580
01:05:02,440 --> 01:05:04,120
but our behavioral intelligence

1581
01:05:04,120 --> 01:05:06,200
is still very much tied to our environment.

1582
01:05:06,200 --> 01:05:08,400
I don't know whether you're familiar with James Lovelock

1583
01:05:08,400 --> 01:05:09,880
and his Gaia theory.

1584
01:05:09,880 --> 01:05:13,560
And he essentially thinks of all life on the planet

1585
01:05:13,560 --> 01:05:17,240
as being kind of like an ecosystem or a meta ecosystem.

1586
01:05:18,240 --> 01:05:21,040
So could you think of humans as just being a product

1587
01:05:21,040 --> 01:05:23,040
of our environment in that same way?

1588
01:05:24,320 --> 01:05:28,120
Is our intelligence limited by the environment we're in?

1589
01:05:28,920 --> 01:05:33,760
I mean, we are, I do, our intelligence is,

1590
01:05:33,760 --> 01:05:38,760
does, it has like some areas where it's more elastic

1591
01:05:39,440 --> 01:05:41,720
than others, I guess, because of our environment.

1592
01:05:41,720 --> 01:05:42,560
I think that would be true.

1593
01:05:42,560 --> 01:05:44,040
Like there's some things that are easier to grok

1594
01:05:44,040 --> 01:05:46,720
than other things, because the things that are easier

1595
01:05:46,720 --> 01:05:50,040
to grok are more aligned with where we come from.

1596
01:05:50,480 --> 01:05:53,440
And so that's like what conceptually we're adapted for.

1597
01:05:53,440 --> 01:05:55,320
Like things like the difference between the third dimension

1598
01:05:55,320 --> 01:05:56,720
and the fourth dimension.

1599
01:05:56,720 --> 01:05:58,640
Like it's easy to reason in three dimensions.

1600
01:05:58,640 --> 01:06:00,840
It's quite hard to reason in four dimensions.

1601
01:06:00,840 --> 01:06:03,560
So we just aren't really adapted to that.

1602
01:06:03,560 --> 01:06:05,240
Does it mean that I can't understand?

1603
01:06:05,240 --> 01:06:06,760
I don't think it means I can't understand.

1604
01:06:06,760 --> 01:06:09,960
But it's just not as flexible, it's not as elastic.

1605
01:06:09,960 --> 01:06:12,040
And so I guess it's going somewhere in the middle.

1606
01:06:12,040 --> 01:06:13,720
Maybe just a little bit of Keith's going to the middle.

1607
01:06:13,720 --> 01:06:16,160
I don't, like, yeah, I don't think either extreme,

1608
01:06:16,160 --> 01:06:18,720
like we were completely specialized.

1609
01:06:18,760 --> 01:06:20,080
I don't think so.

1610
01:06:20,080 --> 01:06:22,400
We're like absolutely like generalist

1611
01:06:22,400 --> 01:06:24,280
in the most like flexible sense.

1612
01:06:24,280 --> 01:06:26,640
No, we're somewhere in the middle.

1613
01:06:26,640 --> 01:06:29,200
But I think that the toolbox we have is sufficient,

1614
01:06:29,200 --> 01:06:32,640
I do believe, to ultimately capture anything.

1615
01:06:32,640 --> 01:06:34,480
I do think we could do that.

1616
01:06:34,480 --> 01:06:38,000
Yeah, so I would argue it's probably an open question.

1617
01:06:38,000 --> 01:06:41,520
I think this may be a case where maybe you're being too certain

1618
01:06:41,520 --> 01:06:44,800
because when you talk about like these dimensionality things,

1619
01:06:44,800 --> 01:06:47,840
you know, I know there are many mathematical structures

1620
01:06:48,000 --> 01:06:51,120
that exhibit very different, you know,

1621
01:06:51,120 --> 01:06:54,080
fundamentally different behavior and say five dimensions

1622
01:06:54,080 --> 01:06:56,240
versus six dimensions versus eight.

1623
01:06:56,240 --> 01:06:59,360
And I think, sure, people have figured that out.

1624
01:06:59,360 --> 01:07:02,080
And we did that by externalizing that intelligence,

1625
01:07:02,080 --> 01:07:04,880
writing down symbolics, doing a bunch of equations.

1626
01:07:04,880 --> 01:07:07,840
But I think it would be fair to say that no human being

1627
01:07:07,840 --> 01:07:11,280
has ever claimed that they could grok that in their mind.

1628
01:07:11,280 --> 01:07:14,640
Like they can do it by virtue of this externalized intelligence,

1629
01:07:14,640 --> 01:07:18,320
but to really hold it in their brain and kind of intuit over it.

1630
01:07:19,040 --> 01:07:22,800
You know, my guess would be there probably are limitations

1631
01:07:22,800 --> 01:07:23,760
to what we can do.

1632
01:07:23,760 --> 01:07:25,200
And that's one of the things that excites me

1633
01:07:25,200 --> 01:07:27,040
about the potential for HEI.

1634
01:07:27,840 --> 01:07:29,280
I don't know if we're going to get to it,

1635
01:07:29,280 --> 01:07:32,480
but if we ever do get to it, it would be really interesting

1636
01:07:32,480 --> 01:07:34,240
to see what it's capable of.

1637
01:07:35,440 --> 01:07:38,080
Unshackled by the fact that it, you know,

1638
01:07:38,080 --> 01:07:42,800
that we evolved in a three plus one dimensional environment,

1639
01:07:42,800 --> 01:07:43,680
you know, to survive.

1640
01:07:44,720 --> 01:07:46,320
Yeah, good points.

1641
01:07:46,320 --> 01:07:47,760
Those are really good points.

1642
01:07:47,760 --> 01:07:48,880
That's interesting to think about.

1643
01:07:48,880 --> 01:07:49,600
Yeah, you're right.

1644
01:07:49,600 --> 01:07:52,880
I hadn't thought of that angle about the AI has this potential

1645
01:07:52,880 --> 01:07:53,840
to break out of that box.

1646
01:07:53,840 --> 01:07:57,200
And that is an interesting thing about AGI.

1647
01:07:58,320 --> 01:07:59,840
So yeah, okay, point taken.

1648
01:08:01,520 --> 01:08:04,080
Yeah, so I am saying something extreme

1649
01:08:04,080 --> 01:08:06,080
if I claim that we can understand everything.

1650
01:08:06,080 --> 01:08:06,800
That is extreme.

1651
01:08:07,760 --> 01:08:08,000
Right.

1652
01:08:08,000 --> 01:08:09,520
I guess I'll still stick with my claim,

1653
01:08:09,520 --> 01:08:12,320
but I might take a lot of effort, I guess.

1654
01:08:12,560 --> 01:08:17,360
Well, the way I see it, and I think you may be right about this,

1655
01:08:17,360 --> 01:08:18,640
at least from this perspective,

1656
01:08:18,640 --> 01:08:22,400
is that it may well be the case that something like,

1657
01:08:23,600 --> 01:08:27,360
you know, second order logic or category theory

1658
01:08:27,360 --> 01:08:30,240
or these sorts of logics that we've already discovered,

1659
01:08:30,240 --> 01:08:31,600
it may turn out to be the case

1660
01:08:31,600 --> 01:08:33,520
that they're mathematically sufficient

1661
01:08:34,080 --> 01:08:37,040
to describe any conceivable phenomenon

1662
01:08:37,040 --> 01:08:39,200
that we'll observe in this universe.

1663
01:08:39,200 --> 01:08:41,440
And so I guess I would say you could be right

1664
01:08:41,440 --> 01:08:45,200
that our languages and our methods that we developed

1665
01:08:45,920 --> 01:08:47,600
kind of externalized from us

1666
01:08:47,600 --> 01:08:49,600
and something that we participate in

1667
01:08:49,600 --> 01:08:52,640
have reached this kind of ultimate level of generality.

1668
01:08:52,640 --> 01:08:54,240
I just think it's a little bit beyond

1669
01:08:54,240 --> 01:08:56,240
what a single human mind,

1670
01:08:56,240 --> 01:08:58,160
at least at this phase of our evolution,

1671
01:08:58,160 --> 01:08:58,960
can comprehend.

1672
01:09:00,160 --> 01:09:00,720
That's interesting,

1673
01:09:00,720 --> 01:09:03,760
because that actually starts to go into this issue

1674
01:09:03,760 --> 01:09:06,000
of what it means to understand.

1675
01:09:06,000 --> 01:09:10,640
And this debate about do these AIs really understand

1676
01:09:10,640 --> 01:09:14,000
and yeah, there's like different kind of levels of that.

1677
01:09:14,000 --> 01:09:16,160
So it's true that when I say like,

1678
01:09:16,160 --> 01:09:17,120
we can understand everything,

1679
01:09:17,120 --> 01:09:18,800
it's a little unclear what I mean by understand.

1680
01:09:18,800 --> 01:09:21,520
Like does it mean just apply the right logical language

1681
01:09:21,520 --> 01:09:22,960
to describe the phenomenon,

1682
01:09:22,960 --> 01:09:25,680
even though we don't really get that like flash feeling

1683
01:09:25,680 --> 01:09:26,800
of like, wow, I really get it.

1684
01:09:26,800 --> 01:09:28,880
And maybe that, maybe they're right.

1685
01:09:28,880 --> 01:09:29,920
Maybe that is out of reach.

1686
01:09:30,640 --> 01:09:31,920
Keith, why don't you just go and look up

1687
01:09:31,920 --> 01:09:33,360
our definition of understanding.

1688
01:09:33,360 --> 01:09:35,120
I can give you the definition of reasoning

1689
01:09:35,120 --> 01:09:35,840
that we came up with,

1690
01:09:35,840 --> 01:09:38,480
which is the ability to derive new knowledge

1691
01:09:38,480 --> 01:09:40,240
from existing knowledge and experience.

1692
01:09:41,600 --> 01:09:43,600
Right, but this reasoning thing is a big thing.

1693
01:09:43,600 --> 01:09:45,840
Do you know when we say neural networks don't reason?

1694
01:09:45,840 --> 01:09:49,360
A lot of it has to do with this notion of extrapolation.

1695
01:09:49,360 --> 01:09:52,080
And people talk about the very geometric notion

1696
01:09:52,080 --> 01:09:52,880
of extrapolation,

1697
01:09:52,880 --> 01:09:56,640
but we're talking about being able to execute a function

1698
01:09:56,640 --> 01:10:00,160
in some logical discrete space.

1699
01:10:00,880 --> 01:10:03,440
So to be able to take something we know

1700
01:10:03,440 --> 01:10:05,520
and extrapolate it into a new situation,

1701
01:10:05,520 --> 01:10:06,720
spoken like Gary Marcus,

1702
01:10:06,720 --> 01:10:09,200
and I'm sure you've had this conversation with Gary many times.

1703
01:10:11,600 --> 01:10:13,360
Yeah, Gary has very strong feelings

1704
01:10:13,360 --> 01:10:14,960
about understanding this, true.

1705
01:10:16,320 --> 01:10:19,200
Yeah, very interesting feelings about that.

1706
01:10:19,200 --> 01:10:21,040
So it just took me a bit to go look up

1707
01:10:21,040 --> 01:10:23,440
because we did have some episode

1708
01:10:23,440 --> 01:10:25,760
where we were really getting into defining

1709
01:10:25,760 --> 01:10:28,400
some of these concepts with Gary.

1710
01:10:28,400 --> 01:10:30,640
Right, and at least here's how I define these.

1711
01:10:30,640 --> 01:10:32,720
So maybe we get your take on, it could be fun.

1712
01:10:32,720 --> 01:10:36,080
So it's that reasoning is the act

1713
01:10:36,080 --> 01:10:39,200
of deriving new knowledge from prior knowledge

1714
01:10:39,280 --> 01:10:40,480
plus new information.

1715
01:10:42,320 --> 01:10:45,200
Semantics, a mapping from structures,

1716
01:10:45,200 --> 01:10:47,520
whether mathematical, logical, symbolic,

1717
01:10:47,520 --> 01:10:50,720
or other structures to physical reality.

1718
01:10:51,920 --> 01:10:55,520
And understanding we had as the act of deriving

1719
01:10:55,520 --> 01:10:58,880
new semantic mappings from prior semantics

1720
01:10:58,880 --> 01:10:59,760
plus new knowledge.

1721
01:11:00,960 --> 01:11:03,040
That's esoteric, but that's how we defined it.

1722
01:11:04,240 --> 01:11:07,280
So understanding is really was the ability to like,

1723
01:11:07,280 --> 01:11:08,720
okay, if I have a world model

1724
01:11:08,720 --> 01:11:10,720
that understands something about physics,

1725
01:11:11,840 --> 01:11:16,800
that like gravity exists and balls roll and whatever,

1726
01:11:16,800 --> 01:11:19,280
and somebody gives me some new knowledge,

1727
01:11:19,280 --> 01:11:23,200
which says, hey, this ball is actually hollow.

1728
01:11:24,240 --> 01:11:26,000
From kind of my understanding of physics,

1729
01:11:26,000 --> 01:11:29,120
I can now, and I just use the word,

1730
01:11:29,120 --> 01:11:32,080
because I understand this semantic model

1731
01:11:32,080 --> 01:11:33,600
and you give me this new knowledge,

1732
01:11:33,600 --> 01:11:36,320
I can now derive new semantics.

1733
01:11:36,320 --> 01:11:39,440
I can say, well, the ball is gonna behave now in this way.

1734
01:11:39,440 --> 01:11:42,400
In other words, I have a new mapping to this physical world

1735
01:11:42,400 --> 01:11:43,680
because I've gained new knowledge.

1736
01:11:44,560 --> 01:11:47,440
That's kind of the way in which we perceived understanding.

1737
01:11:47,440 --> 01:11:50,320
So with like, this was in the context of natural language,

1738
01:11:52,000 --> 01:11:53,920
you know, processing, let's say,

1739
01:11:53,920 --> 01:11:56,400
or systems that do that, GPT-3 or whatever,

1740
01:11:56,960 --> 01:11:59,600
because it doesn't have this semantic model of the world.

1741
01:11:59,600 --> 01:12:02,880
If you say something like, the beer fell off the table,

1742
01:12:03,600 --> 01:12:07,040
it may not be able to derive that now the floor is wet

1743
01:12:07,040 --> 01:12:09,360
and somebody might slip if they fall,

1744
01:12:10,160 --> 01:12:12,800
that that requires this kind of extra level of understanding.

1745
01:12:14,160 --> 01:12:18,480
Yeah, and that's filling in the gaps as well.

1746
01:12:18,480 --> 01:12:22,240
So a lot of NLU people say that the one thing

1747
01:12:22,240 --> 01:12:25,200
neural networks can't do is extrapolate

1748
01:12:25,200 --> 01:12:26,880
over the missing information.

1749
01:12:26,880 --> 01:12:28,000
And that's a great example.

1750
01:12:28,000 --> 01:12:30,800
So you could reason that we've just knocked the beer off the table,

1751
01:12:30,800 --> 01:12:31,600
now the floor is wet.

1752
01:12:32,320 --> 01:12:35,920
So there's a kind of exponential space of missing information

1753
01:12:35,920 --> 01:12:37,360
that we need reasoning to fill in.

1754
01:12:38,000 --> 01:12:43,920
Okay, I have to admit that I, it's probably disappointing you

1755
01:12:43,920 --> 01:12:46,320
because I'm not, I don't really like definitions.

1756
01:12:48,880 --> 01:12:52,080
I just never find definition discussions engaging

1757
01:12:52,080 --> 01:12:53,680
are really helpful to me.

1758
01:12:53,680 --> 01:12:58,160
I really, again, often find that appeal to definition

1759
01:12:58,160 --> 01:13:01,280
is often just a way of escaping an uncomfortable situation.

1760
01:13:01,280 --> 01:13:03,520
And I want to go towards the uncomfortable situation.

1761
01:13:03,520 --> 01:13:04,960
So it's like, we often will say,

1762
01:13:04,960 --> 01:13:06,960
well, no one's really clearly defined consciousness.

1763
01:13:06,960 --> 01:13:09,040
Like, first, before I'm going to discuss it,

1764
01:13:09,040 --> 01:13:10,960
you need to define it to my satisfaction.

1765
01:13:11,760 --> 01:13:13,840
Well, okay, we're obviously not going to discuss it then.

1766
01:13:13,840 --> 01:13:15,440
Like, I'll never satisfy you.

1767
01:13:15,440 --> 01:13:16,320
Right, right.

1768
01:13:16,320 --> 01:13:20,400
And so it's often in the AI, decades of discussion,

1769
01:13:20,400 --> 01:13:21,280
what is intelligence?

1770
01:13:22,480 --> 01:13:25,280
And we see it in open-endedness,

1771
01:13:25,280 --> 01:13:27,680
we start having a problem, like in this small field,

1772
01:13:27,680 --> 01:13:29,840
like there's open-endedness workshops that come

1773
01:13:29,840 --> 01:13:31,360
and like half the papers were just like,

1774
01:13:31,360 --> 01:13:35,440
what is long pages and pages of definition in terms.

1775
01:13:35,440 --> 01:13:37,040
It's like, are we going to ever do anything?

1776
01:13:37,040 --> 01:13:39,280
Or just we're going to argue about this for the next decade.

1777
01:13:39,280 --> 01:13:40,960
Like, what are we even talking about

1778
01:13:40,960 --> 01:13:41,920
is what we're going to talk about.

1779
01:13:43,040 --> 01:13:46,240
And I feel like a lot of this is not necessary.

1780
01:13:46,240 --> 01:13:48,400
I feel like I can talk about consciousness.

1781
01:13:48,400 --> 01:13:50,160
I can't define it for you to your satisfaction.

1782
01:13:50,160 --> 01:13:50,960
I can talk about intelligence.

1783
01:13:50,960 --> 01:13:52,720
I can make progress on intelligence.

1784
01:13:52,720 --> 01:13:54,080
I can talk about open-endedness.

1785
01:13:54,080 --> 01:13:56,000
I don't care really what the definition is.

1786
01:13:56,000 --> 01:13:57,840
It's just you're going to use it to stop me from talking.

1787
01:13:57,840 --> 01:14:00,640
So the only thing I'll defend there is that, I mean,

1788
01:14:01,680 --> 01:14:03,600
I, again, I'm a pragmatist.

1789
01:14:03,600 --> 01:14:05,760
I believe in defining things to the extent

1790
01:14:05,760 --> 01:14:07,760
necessary to communicate.

1791
01:14:07,760 --> 01:14:10,400
And so we have to have, it's kind of like going back

1792
01:14:10,400 --> 01:14:15,120
to the whole hyper-specialization leading to innovation.

1793
01:14:15,120 --> 01:14:17,760
If you just have a divergent thing,

1794
01:14:17,760 --> 01:14:20,160
I think I made this point in our first video,

1795
01:14:20,160 --> 01:14:23,040
you're just going to end up with a universe of gray goo.

1796
01:14:23,040 --> 01:14:25,040
Like the really fascinating thing is that

1797
01:14:25,040 --> 01:14:27,440
because there are these constraints of you

1798
01:14:27,520 --> 01:14:30,400
need to survive in order to pass on your information,

1799
01:14:30,400 --> 01:14:32,880
you wind up with this kind of beautiful tapestry

1800
01:14:32,880 --> 01:14:35,520
of hyper-specialized things that recombine

1801
01:14:35,520 --> 01:14:37,280
to become more general.

1802
01:14:37,280 --> 01:14:39,600
And, you know, it's far more interesting than either extreme,

1803
01:14:39,600 --> 01:14:44,000
like either the gray goo or like the, you know, the nothing, you know.

1804
01:14:44,000 --> 01:14:49,120
And so as far as definitions go, I think that I don't like

1805
01:14:49,120 --> 01:14:52,080
to sit there and pedantically argue forever

1806
01:14:52,080 --> 01:14:54,160
about what the definition of intelligence is,

1807
01:14:54,160 --> 01:14:56,880
but we need to have enough of a definition

1808
01:14:56,880 --> 01:14:58,960
that we can make progress in learning

1809
01:14:58,960 --> 01:15:01,360
and kind of doing scientific discoveries.

1810
01:15:01,360 --> 01:15:04,720
So things like the beer falls off the table and it's wet.

1811
01:15:04,720 --> 01:15:08,560
Well, if a system can't figure that out,

1812
01:15:09,600 --> 01:15:12,640
we notice that it can't figure out kind of a class of things.

1813
01:15:12,640 --> 01:15:15,040
And that class of things has something in common

1814
01:15:15,040 --> 01:15:17,360
and then we give it a name and maybe it's understanding

1815
01:15:17,360 --> 01:15:18,800
or whatever, I don't care.

1816
01:15:18,800 --> 01:15:22,400
But it's just a way of talking about that class of problems

1817
01:15:22,400 --> 01:15:24,160
of things that it's not able to achieve

1818
01:15:24,160 --> 01:15:26,560
because then we can try and figure out how to do that.

1819
01:15:27,760 --> 01:15:31,040
Yeah, I think that the symbolists, though,

1820
01:15:31,040 --> 01:15:35,200
that when they come up with formal arguments,

1821
01:15:36,000 --> 01:15:38,400
it's not, that doesn't come first.

1822
01:15:38,400 --> 01:15:41,680
They notice that neural networks can't do something,

1823
01:15:41,680 --> 01:15:45,040
which is to say they can't fill in the missing gaps.

1824
01:15:45,040 --> 01:15:48,080
And then they come up with a formalism to express why that is.

1825
01:15:48,080 --> 01:15:50,000
And also, many of these symbolists believe

1826
01:15:50,000 --> 01:15:53,520
that there are kind of platonic abstractions

1827
01:15:53,520 --> 01:15:54,640
that exist in the universe.

1828
01:15:54,640 --> 01:15:57,120
They think that mathematics is discovered, not invented.

1829
01:15:57,920 --> 01:16:02,000
So that kind of formal apparatus

1830
01:16:02,000 --> 01:16:03,600
is how they understand the world.

1831
01:16:04,800 --> 01:16:07,520
Well, I feel like I should try to solidify my attack

1832
01:16:07,520 --> 01:16:09,200
on definitions since it's obviously

1833
01:16:09,200 --> 01:16:11,280
fairly, again, radical thing to say.

1834
01:16:13,760 --> 01:16:16,560
I would acknowledge, like, in the sense that Keith is saying that,

1835
01:16:17,440 --> 01:16:18,960
again, like, I don't want to be a prank.

1836
01:16:18,960 --> 01:16:21,040
Like, obviously, you need to define some terms sometimes.

1837
01:16:21,040 --> 01:16:22,240
Like, that's obviously clear.

1838
01:16:22,240 --> 01:16:23,280
That should be completely clear.

1839
01:16:23,280 --> 01:16:24,320
I'm not against that.

1840
01:16:24,400 --> 01:16:28,160
Like, you know, especially, like, you're gonna derive something

1841
01:16:28,160 --> 01:16:30,720
and you're writing a paper or you have a certain set of assumptions

1842
01:16:30,720 --> 01:16:32,480
you need to know what they are, like, in order to prove

1843
01:16:32,480 --> 01:16:33,760
that it actually is true and not true.

1844
01:16:33,760 --> 01:16:36,720
If that's what you're trying to do, like, that makes total sense to me.

1845
01:16:37,680 --> 01:16:40,560
So I'm not a blanket saying we shouldn't have definitions.

1846
01:16:40,560 --> 01:16:42,640
But I think the thing about definitions that's interesting,

1847
01:16:42,640 --> 01:16:45,680
like, a lot of things is that there just isn't solid ground.

1848
01:16:45,680 --> 01:16:48,080
Like, in terms of, like, they're just generally good for you.

1849
01:16:48,080 --> 01:16:50,480
Like, they can be good for you or they can be bad for you.

1850
01:16:50,480 --> 01:16:52,320
They can be a tool of clarification

1851
01:16:52,320 --> 01:16:54,080
or they can be a tool of obfuscation.

1852
01:16:54,400 --> 01:16:56,160
And it depends how you use them.

1853
01:16:56,160 --> 01:16:59,040
And I often find them to be tools of obfuscation.

1854
01:16:59,040 --> 01:17:01,120
Like, especially when we're talking about things we don't know,

1855
01:17:01,120 --> 01:17:03,040
which is, once again, the problem, which is what I'm interested in.

1856
01:17:03,040 --> 01:17:05,120
I want to talk about things that we don't know.

1857
01:17:05,120 --> 01:17:08,640
And that seems where people get really passionate about definitions.

1858
01:17:08,640 --> 01:17:10,560
So it's like, what does it mean to understand?

1859
01:17:10,560 --> 01:17:11,840
We don't know. I don't know.

1860
01:17:11,840 --> 01:17:14,320
What I do know, though, I'm confident there's such a thing.

1861
01:17:14,320 --> 01:17:15,280
There is understanding.

1862
01:17:15,280 --> 01:17:18,160
It might be a continuum, maybe it's not just a binary concept.

1863
01:17:18,160 --> 01:17:19,760
Beyond that, I don't really know what it means.

1864
01:17:20,880 --> 01:17:23,120
And so I would be interested to talk in depth,

1865
01:17:23,120 --> 01:17:24,320
like, what does this really mean?

1866
01:17:24,320 --> 01:17:25,120
Like, let's look at this.

1867
01:17:26,080 --> 01:17:28,880
But no, we have to, like, we just end up in this, like,

1868
01:17:28,880 --> 01:17:30,480
a big argument about the definition.

1869
01:17:31,440 --> 01:17:33,680
And I find that in that case, it's obfuscation,

1870
01:17:33,680 --> 01:17:34,720
because it's fear.

1871
01:17:34,720 --> 01:17:37,760
Because really, like, if your whole pitch is,

1872
01:17:37,760 --> 01:17:40,720
like, your thing that you get a lot of traction on

1873
01:17:40,720 --> 01:17:43,680
is basically attacking the fact that things don't understand,

1874
01:17:44,560 --> 01:17:46,240
then you might be a little uncomfortable

1875
01:17:46,240 --> 01:17:48,640
if we really start dissecting what you mean.

1876
01:17:48,640 --> 01:17:50,880
And so you should just comment us and just tell us,

1877
01:17:50,880 --> 01:17:53,040
like, hey, you're not even being clear in your terms.

1878
01:17:53,600 --> 01:17:55,520
And just stop the argument in its tracks.

1879
01:17:56,400 --> 01:17:58,240
And that's the kind of definition I don't like.

1880
01:17:58,240 --> 01:18:00,560
I mean, I'd rather just, like, look, we agree there's,

1881
01:18:00,560 --> 01:18:01,600
I don't know exactly what it is.

1882
01:18:01,600 --> 01:18:02,720
You don't know exactly what it is.

1883
01:18:02,720 --> 01:18:03,680
Let's talk about it anyway.

1884
01:18:03,680 --> 01:18:04,400
It's uncomfortable.

1885
01:18:04,960 --> 01:18:06,080
But there is such a thing.

1886
01:18:06,080 --> 01:18:06,960
That's what we should agree on.

1887
01:18:06,960 --> 01:18:08,160
Do we agree there's understanding?

1888
01:18:08,160 --> 01:18:09,680
Like, is there anything that exists?

1889
01:18:10,400 --> 01:18:11,760
Well, see, that's a problem is...

1890
01:18:13,520 --> 01:18:16,000
Right, but that's a problem is you can run into some folks

1891
01:18:16,000 --> 01:18:17,360
that will go that extreme and say,

1892
01:18:17,360 --> 01:18:19,280
there's no such thing as intelligence.

1893
01:18:19,280 --> 01:18:20,800
It's all just, yeah.

1894
01:18:20,800 --> 01:18:22,560
So, but look, I agree with you in principle,

1895
01:18:22,560 --> 01:18:25,200
which is, again, I like definitions

1896
01:18:25,200 --> 01:18:28,720
insofar as they're necessary to enable communication.

1897
01:18:28,720 --> 01:18:30,640
So I'm totally on board with the idea

1898
01:18:30,640 --> 01:18:32,560
that we need to be talking about things.

1899
01:18:32,560 --> 01:18:34,640
And that's why I kind of like say

1900
01:18:34,640 --> 01:18:36,320
the coherence theory of knowledge

1901
01:18:36,320 --> 01:18:38,000
because it acknowledges, look,

1902
01:18:38,000 --> 01:18:40,800
we're never going to get to the foundation

1903
01:18:40,800 --> 01:18:42,880
beyond which there's no other foundation

1904
01:18:42,880 --> 01:18:43,680
that we can imagine.

1905
01:18:43,680 --> 01:18:46,240
We just need to understand far enough

1906
01:18:46,240 --> 01:18:48,720
and define things far enough that we can make progress.

1907
01:18:49,600 --> 01:18:51,440
So, I'm a little bit...

1908
01:18:51,440 --> 01:18:54,160
I can completely understand where you're coming from, Kenneth.

1909
01:18:54,160 --> 01:18:58,480
You think that we have a fundamental fear of the unknown,

1910
01:18:58,480 --> 01:19:00,720
and we're hiding behind our formalisms.

1911
01:19:00,720 --> 01:19:04,640
And my only worry is that it seems

1912
01:19:04,640 --> 01:19:06,400
a little bit anti-intellectual

1913
01:19:06,400 --> 01:19:09,200
because you can make an observation

1914
01:19:09,200 --> 01:19:13,520
that an AI model is not behaving the way we are.

1915
01:19:13,520 --> 01:19:14,960
There's loads of assumptions there.

1916
01:19:14,960 --> 01:19:16,960
We assume that we are behaving in a way

1917
01:19:17,200 --> 01:19:21,200
and we are doing things the right way.

1918
01:19:21,200 --> 01:19:23,520
But then you can say,

1919
01:19:23,520 --> 01:19:26,240
oh, let's resist any formalism

1920
01:19:26,240 --> 01:19:28,080
to try and break this down analytically.

1921
01:19:29,600 --> 01:19:30,560
Do you see the conflict there?

1922
01:19:31,360 --> 01:19:32,240
Yeah, absolutely.

1923
01:19:32,240 --> 01:19:35,440
I mean, it's a dangerous position

1924
01:19:35,440 --> 01:19:38,320
because it's clear that some formalism is necessary,

1925
01:19:38,320 --> 01:19:39,440
like I tried to concede.

1926
01:19:39,440 --> 01:19:41,840
I mean, I'm walking a tightrope.

1927
01:19:41,840 --> 01:19:44,560
So it's not...

1928
01:19:44,560 --> 01:19:45,760
Yeah, it can't be...

1929
01:19:46,480 --> 01:19:48,160
I can't make this blanket claim

1930
01:19:48,160 --> 01:19:51,280
that formalism needs to be completely thrown out the window.

1931
01:19:51,840 --> 01:19:54,960
That would just destroy my credibility.

1932
01:19:56,480 --> 01:19:58,320
But the point is...

1933
01:19:58,320 --> 01:19:59,440
These are just pendulum.

1934
01:19:59,440 --> 01:20:00,240
These are pendulum?

1935
01:20:00,240 --> 01:20:00,960
What is the plural?

1936
01:20:00,960 --> 01:20:01,520
Pendulums.

1937
01:20:02,320 --> 01:20:03,040
These are pendulum.

1938
01:20:03,040 --> 01:20:04,480
They swing in different directions.

1939
01:20:04,480 --> 01:20:07,680
So we become so enamored with this,

1940
01:20:07,680 --> 01:20:09,120
which is a useful tool,

1941
01:20:09,120 --> 01:20:10,800
but to the point where it actually becomes

1942
01:20:10,800 --> 01:20:12,480
like a form of obfuscation.

1943
01:20:12,480 --> 01:20:14,400
And I do believe definition is like that.

1944
01:20:14,480 --> 01:20:17,840
Definition is not always and it's not everybody.

1945
01:20:17,840 --> 01:20:21,120
But a lot of the time when there's an uncomfortable issue,

1946
01:20:21,120 --> 01:20:24,000
we immediately jump to definition to obfuscate.

1947
01:20:24,000 --> 01:20:26,640
I think consciousness is one of the greatest examples of that.

1948
01:20:26,640 --> 01:20:28,720
It's like there's clearly a mystery here.

1949
01:20:29,920 --> 01:20:30,880
And I agree there are a few people

1950
01:20:30,880 --> 01:20:32,560
who would disagree there's any mystery at all.

1951
01:20:33,200 --> 01:20:35,120
But to me, it's clear there's a mystery.

1952
01:20:35,120 --> 01:20:37,840
Don't need to have a definition to know there's a mystery.

1953
01:20:37,840 --> 01:20:39,360
We could get into why it's mysterious.

1954
01:20:39,360 --> 01:20:42,240
That's more interesting to me than what the definition is.

1955
01:20:42,240 --> 01:20:44,320
But if you jump at me with this definition stuff,

1956
01:20:44,640 --> 01:20:46,880
you're going to stop me from getting into that.

1957
01:20:46,880 --> 01:20:48,320
I feel like that's pure obfuscation.

1958
01:20:48,320 --> 01:20:50,080
This is one of the greatest unknown things

1959
01:20:50,080 --> 01:20:53,040
in the entire scientific world, like consciousness.

1960
01:20:53,040 --> 01:20:54,720
It's one of the greatest mysteries of all time.

1961
01:20:55,840 --> 01:20:58,000
And like so, are you about definitions?

1962
01:20:58,000 --> 01:20:59,440
Is that really where we're going to go with this?

1963
01:21:00,320 --> 01:21:01,680
Maybe we don't have a good definition yet

1964
01:21:01,680 --> 01:21:03,440
because we don't even know what we're talking about.

1965
01:21:03,440 --> 01:21:04,960
That's part of why it's so mysterious.

1966
01:21:06,000 --> 01:21:07,280
I think this is quite interesting though

1967
01:21:07,280 --> 01:21:08,320
because with consciousness,

1968
01:21:08,320 --> 01:21:10,480
we clearly don't have a mental apparatus.

1969
01:21:11,360 --> 01:21:13,440
You know, like for example,

1970
01:21:13,440 --> 01:21:15,520
if someone just took some hallucinogenics

1971
01:21:15,520 --> 01:21:19,360
and they just had a completely crazy visual experience,

1972
01:21:19,360 --> 01:21:21,120
they wouldn't have any words

1973
01:21:21,120 --> 01:21:23,680
or any mental framework to hang this off.

1974
01:21:23,680 --> 01:21:25,680
Whereas when we're talking about an apparatus

1975
01:21:25,680 --> 01:21:27,440
to describe intelligent behavior,

1976
01:21:27,440 --> 01:21:30,720
we absolutely do have an apparatus to hang things off.

1977
01:21:30,720 --> 01:21:34,640
So I guess, is that a spectrum in your mind?

1978
01:21:37,440 --> 01:21:38,640
Yeah, there is a spectrum.

1979
01:21:39,040 --> 01:21:45,280
I would also concede that with consciousness,

1980
01:21:46,320 --> 01:21:49,280
yeah, it's true that it's much worse for us.

1981
01:21:49,280 --> 01:21:50,960
It's true because it's ineffable,

1982
01:21:50,960 --> 01:21:53,200
which is another way of saying you can't put it into words.

1983
01:21:53,200 --> 01:21:55,520
Like all the things we're discussing have no words.

1984
01:21:56,480 --> 01:21:57,920
Like the blueness of blue.

1985
01:21:57,920 --> 01:21:59,440
Like I can't actually describe it.

1986
01:21:59,440 --> 01:22:01,520
You can't break it down into parts or say what it is.

1987
01:22:02,560 --> 01:22:04,240
And so that makes it extremely difficult

1988
01:22:04,240 --> 01:22:06,320
like to actually get into anything formal about it.

1989
01:22:07,040 --> 01:22:10,080
Whereas intelligence, I would grant that you can,

1990
01:22:10,960 --> 01:22:13,120
to some extent, like you can actually point to things

1991
01:22:13,120 --> 01:22:17,760
that can be reduced to words or symbols or formalism.

1992
01:22:18,640 --> 01:22:21,760
And so it's a little bit better on that slippery slope.

1993
01:22:21,760 --> 01:22:24,080
It's higher up and less dangerous.

1994
01:22:25,040 --> 01:22:27,920
But still, I think it's still on a slippery slope.

1995
01:22:27,920 --> 01:22:29,840
Like there's, I don't think we,

1996
01:22:29,840 --> 01:22:32,000
although we can talk about intelligence

1997
01:22:32,000 --> 01:22:33,680
more easily than consciousness,

1998
01:22:33,680 --> 01:22:36,800
I still don't think we really fully grasp this either

1999
01:22:37,520 --> 01:22:40,240
or really have the words to really get at what we're talking about.

2000
01:22:40,240 --> 01:22:41,600
We don't understand ourselves well enough

2001
01:22:41,600 --> 01:22:42,880
to understand what we're talking about.

2002
01:22:43,840 --> 01:22:47,600
And so it's, there's still a little bit of this room for obfuscation

2003
01:22:48,480 --> 01:22:51,600
where we fail to address the mystery itself

2004
01:22:51,600 --> 01:22:53,920
by just deciding to focus on the definition.

2005
01:22:56,160 --> 01:22:59,200
Yeah, I mean, I think if we did understand it

2006
01:22:59,760 --> 01:23:00,960
or had a clear definition,

2007
01:23:00,960 --> 01:23:04,240
you know, we wouldn't be having so many interesting papers coming out,

2008
01:23:04,240 --> 01:23:06,320
like on the measure of intelligence.

2009
01:23:06,320 --> 01:23:09,680
And that was the thing that I liked about Chile's paper is,

2010
01:23:10,560 --> 01:23:14,240
hey, as long as it's an operationally useful measure,

2011
01:23:14,960 --> 01:23:16,160
then that helps us.

2012
01:23:16,720 --> 01:23:19,520
It turns out, while it's kind of a nice framework

2013
01:23:19,520 --> 01:23:22,080
to think about things, there isn't yet a measure of it,

2014
01:23:22,640 --> 01:23:25,520
you know, kind of working on that and he's working on that.

2015
01:23:25,520 --> 01:23:27,840
But again, I love the idea that we just,

2016
01:23:28,720 --> 01:23:31,040
you know, the goal here, like you said earlier,

2017
01:23:31,040 --> 01:23:32,800
is really to talk about things.

2018
01:23:32,800 --> 01:23:35,040
You know, it's to communicate, it's to learn,

2019
01:23:35,040 --> 01:23:39,440
it's to make progress, it's to explore and to some degree,

2020
01:23:39,440 --> 01:23:41,280
to make life better, to exploit.

2021
01:23:41,280 --> 01:23:45,600
But it's all about just doing almost the minimal necessary

2022
01:23:45,600 --> 01:23:49,840
to enable communication and exploration, I think.

2023
01:23:52,000 --> 01:23:53,600
Yeah, yeah, yeah.

2024
01:23:53,600 --> 01:23:54,880
So I totally agree.

2025
01:23:55,440 --> 01:23:57,440
Like the ultimate point is to explore.

2026
01:23:58,240 --> 01:24:03,120
And yeah, I just like to go to places that are ambiguous,

2027
01:24:03,120 --> 01:24:04,800
like on purpose for something.

2028
01:24:04,800 --> 01:24:06,320
I feel like that's what we're supposed to do,

2029
01:24:06,320 --> 01:24:08,320
like if we're talking about science or art,

2030
01:24:09,440 --> 01:24:11,840
those are like the really interesting uncomfortable places

2031
01:24:11,840 --> 01:24:12,720
where you're going to learn something.

2032
01:24:12,720 --> 01:24:14,320
It's almost like how in physics,

2033
01:24:14,320 --> 01:24:19,040
physicists always want to go to where two great theories collide

2034
01:24:19,040 --> 01:24:20,800
and nobody knows what happens.

2035
01:24:20,800 --> 01:24:23,040
Like what happens at the surface of a black hole

2036
01:24:23,040 --> 01:24:26,320
where quantum mechanics and general relativity collide.

2037
01:24:26,320 --> 01:24:28,560
There's a lot of unknown and ambiguity there.

2038
01:24:28,560 --> 01:24:30,480
That's where the real progress is going to come from.

2039
01:24:31,120 --> 01:24:34,480
You're sitting right on the boundary of chaos and order.

2040
01:24:34,480 --> 01:24:37,200
You're trying to straddle that straddle line.

2041
01:24:37,200 --> 01:24:38,320
Exactly, exactly.

2042
01:24:39,280 --> 01:24:42,560
Well, Professor Kenestan, it's always an honor.

2043
01:24:42,560 --> 01:24:44,000
Thank you so much for joining us.

2044
01:24:45,040 --> 01:24:45,520
Thank you.

2045
01:24:45,520 --> 01:24:46,080
That was super fun.

2046
01:24:46,080 --> 01:24:46,560
Thank you.

2047
01:24:46,560 --> 01:24:47,200
Thank you so much.

2048
01:24:47,200 --> 01:24:47,840
Thank you so much.

2049
01:24:47,840 --> 01:24:48,240
Amazing.

