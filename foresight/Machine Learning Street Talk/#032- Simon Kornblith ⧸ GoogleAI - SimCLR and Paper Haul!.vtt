WEBVTT

00:00.000 --> 00:14.600
Welcome back. Today we're talking with Dr Simon Cornblith, a research scientist in the

00:14.600 --> 00:20.640
Google Brain team. Simon is most famous for being one of the authors on Simclear, the computer

00:20.640 --> 00:25.360
vision paper that used self-supervised learning and the contrastive loss with loads of cool

00:25.360 --> 00:30.760
image augmentations. Simon also used to be a neuroscientist.

00:30.760 --> 00:35.920
When I was pretty young, I was interested in consciousness and how we create this kind

00:35.920 --> 00:41.640
of impression of the external world inside our heads. And so I guess it's pretty obvious

00:41.640 --> 00:46.560
how that translates into an interest in brains and how the brain works.

00:46.560 --> 00:52.240
Turns out the neuroscience is really difficult. Progress is really slow and tedious. Simon's

00:52.320 --> 00:58.160
goal is to understand the inner workings of neural networks, both in meat space and

00:58.160 --> 01:05.640
in silicon. He initially thought that the artificial variety might be easier to understand.

01:05.640 --> 01:07.360
He was in for a rude awakening.

01:07.360 --> 01:11.200
So in a neural network, we can record all the neurons, which is extremely challenging

01:11.200 --> 01:17.160
in a biological organism. And we can also manipulate the system in any kind of way that

01:17.160 --> 01:22.080
we can imagine. But it still seems really hard to understand neural networks. I think there

01:22.080 --> 01:28.800
are a lot of ideas from machine learning that will ultimately help us understand brains.

01:28.800 --> 01:33.920
Maybe we could make some headway that might eventually translate back to brains. And so

01:33.920 --> 01:35.800
that's how I ended up in machine learning.

01:35.800 --> 01:39.360
People often try and anthropomorphise neural networks.

01:39.360 --> 01:43.720
People try to relate whatever neural network they've built back to a brain and they say

01:43.720 --> 01:46.800
that it works like the brain, but it doesn't work like the brain.

01:46.800 --> 01:51.780
So Simon was involved in this paper, do wide and deep networks learn the same things, uncovering

01:51.780 --> 01:55.460
how neural network representations vary with width and depth.

01:55.460 --> 02:01.300
Simon pioneered this really fascinating way of comparing representations by comparing

02:01.300 --> 02:06.820
features. And what this essentially amounts to is we need to have a similarity function

02:06.820 --> 02:13.420
so that we can compare the representations in layers to themselves in different parts

02:13.420 --> 02:15.780
of the network or indeed to other networks.

02:15.780 --> 02:19.540
And so for this similarity measure to work well, the first thing Simon did was take two

02:19.540 --> 02:24.580
architecturally identical networks, A and B, trained from different random initialisations

02:24.580 --> 02:32.180
and just ensure that the third convolution layer is more self-similar to its counterpart

02:32.180 --> 02:34.460
than any of the other layers.

02:34.460 --> 02:38.420
If that works, then you're onto something. Turns out that's not super simple to do, but

02:38.420 --> 02:41.740
Simon came up with this concept called the centred kernel alignment, which we'll talk

02:41.740 --> 02:43.180
about on the call.

02:43.180 --> 02:49.300
But this is actually super fascinating. We're talking about this idea here of using self-similarity

02:49.300 --> 02:55.020
to reason about the evolution of representations throughout successive layers in the neural

02:55.020 --> 03:00.940
network. And what Simon found is that you get this kind of characteristic blockiness.

03:00.940 --> 03:06.380
So when you see these large blocks, what it means is that the representations are no longer

03:06.380 --> 03:08.460
evolving in respect of time.

03:08.460 --> 03:13.940
So it's showing here the representational similarity of all of the layers against themselves

03:13.940 --> 03:16.140
and against all of the other layers.

03:16.140 --> 03:20.260
So clearly there's this characteristic diagonal down the matrix, as you would see with any

03:20.260 --> 03:22.260
self-similarity matrix.

03:22.260 --> 03:27.700
And because this blockiness appears, it means that nothing is happening.

03:27.700 --> 03:31.620
And what Simon realised is you can actually delete these layers from the neural network

03:31.620 --> 03:35.100
and it wouldn't make any difference because it hasn't learned anything new.

03:35.100 --> 03:39.900
But it's also a really interesting way of reasoning about a kind of pathology, a weird

03:39.900 --> 03:43.060
thing that happens when you saturate a neural network.

03:43.060 --> 03:47.100
So he said that this presence of this block structure is an indicator of the halting of

03:47.100 --> 03:51.420
evolution and a strong indicator of over-parameterisation.

03:51.420 --> 03:56.020
And he actually shows that this blockiness appears on deeper networks and wider networks.

03:56.020 --> 03:59.220
But this concept of self-similarity analysis is not new to me.

03:59.220 --> 04:06.060
On my PhD, I was fascinated in segmenting DJ-mixed music shows and I actually used the same techniques

04:06.060 --> 04:10.260
for learning regimes in financial datasets later on.

04:10.620 --> 04:13.900
This is an example of a DJ-mix which I segmented.

04:13.900 --> 04:19.260
I came up with a dynamic programming algorithm which would essentially sum up all of the

04:19.260 --> 04:25.260
tiles along this diagonal and compute the lowest costs contiguous segmentation.

04:25.260 --> 04:26.580
And it's super interesting.

04:26.580 --> 04:31.260
So here are two music tracks and you can see that they are more self-similar to each other

04:31.260 --> 04:35.820
than they are any of the other tracks just because of the tone of the colour here.

04:35.820 --> 04:39.700
And if you zoom into a track, you can even see that there are symmetries.

04:39.700 --> 04:44.420
This part of the track here is a repetition from this part of the track here.

04:44.420 --> 04:49.420
And you can tell that from this kind of symmetry pattern on the diagonal.

04:49.420 --> 04:52.780
And you can see that there's a little bit in the track in the middle here which is not

04:52.780 --> 04:55.740
similar to any other part of the track.

04:55.740 --> 05:00.660
You see some really interesting stuff here and essentially I'm a huge fan of anyone using

05:00.660 --> 05:05.420
self-similarity matrices for reasoning about the evolution of representations.

05:05.420 --> 05:07.140
I think it's a fascinating idea.

05:07.140 --> 05:09.980
So how did Simon come up with this measure of similarity?

05:09.980 --> 05:12.580
The centred kernel alignment.

05:12.580 --> 05:17.900
Jeff Hinton had another idea and I tried the idea that it worked but then we wondered is

05:17.900 --> 05:20.540
there a simpler thing that worked.

05:20.540 --> 05:23.140
And that's how we ended up with centred kernel alignment.

05:23.140 --> 05:27.940
The blockiness in these matrices is absolutely fascinating but how much can we read into

05:27.940 --> 05:28.940
it?

05:28.940 --> 05:33.460
It's not clear what we should really expect in terms of how a neural network representation

05:33.460 --> 05:35.060
evolves through the layers.

05:35.060 --> 05:40.260
I think there's kind of some theory on what we should expect if all the layers are linear.

05:40.260 --> 05:44.740
But like obviously the neural networks that we train are nonlinear and it's really important

05:44.740 --> 05:47.340
to have a nonlinearity in between the layers.

05:47.340 --> 05:52.780
If we see that nothing is changing from one layer to the next that's a really bad sign.

05:52.780 --> 05:56.860
If the neural network representation isn't changing then obviously nothing's happening.

05:56.860 --> 06:01.540
We couldn't have predicted this ahead of time based on what we know about neural network

06:01.620 --> 06:06.940
theory and we couldn't have predicted it ahead of time based on the accuracy of the network.

06:06.940 --> 06:08.740
Does this apply to ResNets though?

06:08.740 --> 06:11.460
I thought that they could learn their own capacity.

06:11.460 --> 06:15.780
You can either look at networks without residual connections where you do actually find that

06:15.780 --> 06:21.580
at some depth the accuracy will start going down and in networks without residual connections

06:21.580 --> 06:28.140
we find that the depth where accuracy starts to go down is like around the same depth where

06:28.180 --> 06:33.020
you begin seeing this kind of block structure where many successive layers have similar

06:33.020 --> 06:37.780
representations and it looks like the representation is no longer getting refined through the network.

06:37.780 --> 06:42.420
Once you start getting these blocks making the network deeper, making the network wider

06:42.420 --> 06:45.660
no longer really gives you any improvement in accuracy.

06:45.660 --> 06:50.220
So it seems like this is basically telling you that the network has fit the data as much

06:50.220 --> 06:56.820
as it can and there's no real advantage to using something bigger.

06:56.820 --> 07:02.020
Next we move on to Simon's paper about using different loss functions on image classifiers

07:02.020 --> 07:05.980
and he made some really interesting findings actually so the loss functions only really

07:05.980 --> 07:10.780
seem to affect the penultimate layers in the neural network.

07:10.780 --> 07:14.580
This also gives us some pretty useful insight into transfer learning.

07:14.580 --> 07:19.500
The last third of the network is setting up the penultimate layer representation in a

07:19.500 --> 07:24.700
way that is good for your loss function but the first two thirds of the network are somehow

07:24.860 --> 07:27.420
just learning general features.

07:27.420 --> 07:32.180
I think this also corresponds with the success of transfer learning where we can take features

07:32.180 --> 07:36.260
that we've learned on one task and transfer them to some other task.

07:36.260 --> 07:37.980
What's the implication though?

07:37.980 --> 07:44.460
It seems, is the implication that the loss function is not having any impact on the representations

07:44.460 --> 07:45.660
early on in the network?

07:45.660 --> 07:49.340
That seems like quite a big implication.

07:49.340 --> 07:52.460
Ultimately we're asking the network to do the same thing just in a slightly different

07:52.460 --> 07:53.460
way.

07:53.500 --> 07:59.940
Like some inverse correlation between the gains you get from a loss function and how

07:59.940 --> 08:01.900
good it is for transfer learning.

08:01.900 --> 08:09.900
If you use loss functions that give you higher accuracy on ImageNet, you tend to learn representations

08:09.900 --> 08:14.060
that transfer substantially worse in that setting.

08:14.060 --> 08:19.100
The loss functions that perform better lead classes to become more separated in the penultimate

08:19.100 --> 08:20.100
layers.

08:20.140 --> 08:25.340
To standard softmax loss, actually the classes are not that separated from each other in

08:25.340 --> 08:28.180
the penultimate layer representation.

08:28.180 --> 08:33.780
Right now on whatever TensorFlow Hub or Hugging Face repositories and so on, we have these

08:33.780 --> 08:38.660
pre-trend models and the pre-trend models, they're like full stack models and people

08:38.660 --> 08:46.820
usually take some sort of last or next to last hidden layer but maybe we should much

08:46.900 --> 08:52.820
more focus on actually providing like half of a network to share, like determining which

08:52.820 --> 08:58.220
are actually the best, good or general representations from a data set and so on.

08:58.220 --> 09:00.060
It's a really interesting question.

09:00.060 --> 09:04.740
If we just want to turn an image into a vector that we could then train a linear classifier

09:04.740 --> 09:07.900
on top of, what is the best way of doing that?

09:07.900 --> 09:12.340
Self-supervised pre-training, just like word vectors, gives us a really great starting

09:12.340 --> 09:17.780
point to vectorize an image into a semantically relevant geometric space.

09:17.780 --> 09:21.820
It's been a real game changer in the computer vision world since about 2018.

09:21.820 --> 09:25.900
We want that neural network to learn a representation such that when we then just train a linear

09:25.900 --> 09:30.860
classifier on top of that representation to classify image net, it's going to do well.

09:30.860 --> 09:35.780
But we want to learn the initial representation without using any kind of labels.

09:35.780 --> 09:38.780
So what is self-supervised pre-training for vision?

09:38.780 --> 09:42.460
People came up with these kinds of tasks that you could try to train a neural network to

09:42.460 --> 09:46.260
do so that it would learn some kind of good representation.

09:46.260 --> 09:53.140
You're trying to learn some kind of representation space where you've got different patches from

09:53.140 --> 09:57.340
an image or different augmentations from an image, just different representations of the

09:57.340 --> 10:02.940
same image and you want to learn a representation space where these representations of the same

10:02.940 --> 10:08.740
image are all close together in that representation space and they're far apart from the representations

10:09.380 --> 10:11.340
of other images.

10:11.340 --> 10:16.220
This surprisingly seems to lead to very good representations.

10:16.220 --> 10:23.060
I was very fascinated by all of these different tricks that you apparently have to get and

10:23.060 --> 10:28.140
so big kudos to figuring all of this out for the rest of us.

10:28.140 --> 10:31.500
Data augmentation is absolutely key to making this work.

10:31.500 --> 10:35.300
The important part of the recipe is data augmentation.

10:35.300 --> 10:40.380
There are really only two super important data augmentations that we need.

10:40.380 --> 10:45.860
So we have to take two different crops from the same image and then we have to do some

10:45.860 --> 10:48.020
kind of color distortion.

10:48.020 --> 10:50.380
Turns out though the architecture isn't that important.

10:50.380 --> 10:56.780
You don't have to worry about architecture, engineering specifically or contrastive learning.

10:56.780 --> 10:58.660
What was new in the CIM CLR paper?

10:58.660 --> 11:04.060
We introduced the idea of this projection head in CIM Clear and we also spend a lot

11:04.060 --> 11:06.220
of time studying the augmentation.

11:06.220 --> 11:08.340
And what about the bring your own latent paper?

11:08.340 --> 11:18.380
I don't really have any insight into how either BYOL or the more recent papers actually

11:18.380 --> 11:21.860
are learning a representation that doesn't end up collapsing.

11:21.860 --> 11:26.700
Why it doesn't happen relates to some mysteries about neural network training dynamics that

11:26.700 --> 11:28.940
we still don't entirely understand.

11:28.940 --> 11:31.620
We dive deep into data augmentation in general.

11:31.620 --> 11:35.940
The data augmentation that you need for contrastive learning is different from the data augmentation

11:35.940 --> 11:38.980
that you need for supervised learning because the task is different.

11:38.980 --> 11:44.220
When you have contrastive learning, you have this problem that if there's just one feature

11:44.220 --> 11:50.580
in your data that can be used to do the contrastive task to get images of the same example or

11:50.580 --> 11:55.020
views of the same example close together and far apart from views of all the other examples.

11:55.020 --> 11:58.980
If you could do that with one feature, that would be the only feature the network would

11:58.980 --> 12:02.660
ever learn or it might be the only feature the network would ever learn.

12:02.660 --> 12:06.780
And so with the augmentation, you're making the task harder so that the network actually

12:06.780 --> 12:10.420
has to learn many different kinds of features.

12:10.420 --> 12:16.580
We find that this color distortion actually is very important for self-supervised learning,

12:16.580 --> 12:21.300
for contrastive learning, or as it doesn't really matter for supervised learning.

12:21.300 --> 12:26.420
There seems to be this fascinating universality of representations, especially in vision.

12:26.420 --> 12:30.740
I'm not trying to be flippant when I say this because practitioners have used ImageNet

12:30.740 --> 12:32.740
on a variety of downstream tasks.

12:32.740 --> 12:36.580
For example, they might use it for classifying circuit boards or something.

12:36.580 --> 12:39.300
And the miraculous thing is it just seems to work quite well.

12:39.300 --> 12:43.500
So do you think in your opinion that there is some kind of universality?

12:43.500 --> 12:49.100
I'm very skeptical about universality of ImageNet for different tasks.

12:49.100 --> 12:52.980
Even though there are lots of cars in ImageNet, if you pre-train on ImageNet and you fine

12:52.980 --> 12:58.500
tune on that data set, you will learn to classify it faster and fewer steps than if you had

12:58.500 --> 13:03.100
trained from scratch on the Stanford cars data set.

13:03.100 --> 13:06.940
But you won't actually perform any better at the end.

13:06.940 --> 13:09.860
Representations of images are not that universal.

13:09.860 --> 13:16.140
And at least what works for natural images like those in ImageNet may not work on other

13:16.140 --> 13:17.780
data sets.

13:17.780 --> 13:25.860
Taking a bit from the universality of representations to the universality of augmentations, since

13:25.860 --> 13:32.620
this is such a crucial part, do you think that there is a systematic way how we can

13:32.620 --> 13:35.060
discover augmentations?

13:35.060 --> 13:37.340
Right now, it seems to be kind of a whack-a-mole, right?

13:37.340 --> 13:38.340
It's okay.

13:38.340 --> 13:40.300
We just feed images and say, no, that's too easy.

13:40.300 --> 13:41.300
We crop them.

13:41.300 --> 13:42.580
Oh, no, it's the color histogram.

13:42.580 --> 13:45.420
So we like whack on the color and then it works better.

13:45.500 --> 13:50.740
Maybe someone finds out, oh, there is still this easy feature that the network, every

13:50.740 --> 13:56.340
noun, then pays attention to, so we design a method to whack on that a bit.

13:56.340 --> 14:01.700
Do you think there is a systematic way or will this kind of philosophically always rely

14:01.700 --> 14:07.580
on us humans having a higher level inside of what we want to do with the data set?

14:07.580 --> 14:10.460
So what comes next after data augmentation?

14:10.460 --> 14:12.460
So would the next step be some simulation?

14:13.180 --> 14:14.180
Do you know what I mean?

14:14.180 --> 14:17.580
Where we impute physics and we impute some world knowledge and then, I don't know, whether

14:17.580 --> 14:19.660
we train a machine learning model from that?

14:19.660 --> 14:26.100
Yeah, I think there are definitely shortcomings in our current machine learning models, understandings

14:26.100 --> 14:27.100
of the world.

14:27.100 --> 14:32.340
There are probably things that we can't just solve by throwing more static images at them.

14:32.340 --> 14:38.100
I think maybe the next step, rather than trying to immediately situate the machine

14:38.100 --> 14:43.420
learning model in a simulated world, we could just think about video.

14:43.420 --> 14:49.180
I think probably representation learning from video is going to be a big thing next year

14:49.180 --> 14:53.420
or the year after, something sometime in the near future.

14:53.420 --> 14:55.740
Finally, we talk about Simon's paper.

14:55.740 --> 15:00.460
Big self-supervised models are strong semi-supervised learners.

15:00.460 --> 15:05.980
What is a practical problem is the situation where you have a lot of unlabeled data and

15:05.980 --> 15:09.340
then a very small amount of labeled data.

15:09.340 --> 15:15.180
What I find fascinating is how many ideas come together in this paper.

15:15.180 --> 15:20.580
You probably didn't sit down after a SimClear one and be like, all right, what do we do

15:20.580 --> 15:21.580
for SimClear two?

15:21.580 --> 15:22.580
Okay, let's do this.

15:22.580 --> 15:24.220
So it tells me there was this process.

15:24.220 --> 15:30.340
Could you, if you can, maybe elaborate a bit on how did you going to build up the system

15:30.340 --> 15:32.820
towards the final output?

15:32.820 --> 15:37.540
We also tried the approach of first fine-tuning the big network and then distilling it.

15:37.540 --> 15:40.900
It turned out that worked a lot better.

15:40.900 --> 15:46.540
What we found was this approach of pre-training, then fine-tuning, then distilling works a

15:46.540 --> 15:50.660
lot better than pre-training, then distilling, then fine-tuning.

15:50.660 --> 15:56.100
We probably shouldn't expect distillation of the kind that we do in SimClear v2 to work

15:56.100 --> 16:02.660
substantially better than supervised distillation, which has been around for quite a while now.

16:02.660 --> 16:09.820
I think what's impressive is that in the self-supervised case, in the contrastive case, distillation

16:09.820 --> 16:14.460
basically allows you to recover the same accuracy that you would get from training supervised

16:14.460 --> 16:18.980
from scratch, whereas without it, the accuracy is a lot worse.

16:18.980 --> 16:23.940
So it seems like it maybe matters more in this contrastive case.

16:23.940 --> 16:29.300
But I think generally when you do distillation in the supervised case, you can get maybe

16:29.300 --> 16:33.580
a percentage point gain, maybe a couple of percentage points.

16:33.580 --> 16:38.180
And I think that's probably about the limit in terms of the improvement that you could

16:38.180 --> 16:44.820
get from any kind of distillation-based approach over supervised training from scratch.

16:44.820 --> 16:50.580
Can you use GANs, Generative Adversarial Neural Networks, to do data augmentation?

16:50.580 --> 16:52.780
Or is that just a myth?

16:52.780 --> 16:55.100
Simon certainly seems to think so.

16:55.100 --> 16:59.260
Using a GAN to do data augmentation, you have this problem that you still don't actually

16:59.260 --> 17:01.140
have more data.

17:01.140 --> 17:03.620
You have a GAN that's trained on the same data.

17:03.620 --> 17:08.460
And so it might help you because your way of encoding inductive bias into the GAN is

17:08.460 --> 17:12.500
different from your way of encoding inductive bias into the neural network.

17:12.500 --> 17:17.540
And maybe by having more inductive bias, you can learn a better function.

17:17.540 --> 17:21.660
You still don't have more data, and it seems like without having more data, there's no

17:21.660 --> 17:26.460
reason to expect a priority that you will be able to learn a better function.

17:26.460 --> 17:32.820
Ironically, when you do the simple data augmentation, you do have more data because you put all

17:32.820 --> 17:38.620
the knowledge in there as a human of what makes two images dissimilar visually, but

17:38.620 --> 17:44.500
still equivalent semantically, which, again, is exactly the opposite.

17:44.500 --> 17:51.180
It gives you images that are visually similar, but it has no intuition of what the semantic

17:51.180 --> 17:52.980
similarity is.

17:52.980 --> 17:57.180
We round off the show by talking about Simon's love of the Julia language.

17:57.180 --> 18:02.380
Julia is a much better programming language than Python in many ways.

18:02.380 --> 18:07.180
Julia is designed for these situations where maybe beyond just matrices, you have these

18:07.180 --> 18:11.500
funny types of structured matrices, you have sparse matrices, and you can define special

18:11.500 --> 18:16.700
methods for the product of a sparse matrix in a vector, or all sorts of things where

18:16.700 --> 18:20.500
you might want different methods depending on the types.

18:20.500 --> 18:22.300
I really hope you've enjoyed the show today.

18:22.300 --> 18:24.380
We've had so much fun making it.

18:24.380 --> 18:27.940
Remember to like, comment, and subscribe.

18:27.940 --> 18:33.820
We love reading your comments, every single one of them, and we'll see you back next

18:33.820 --> 18:34.820
week.

18:34.820 --> 18:39.820
Welcome back to the Machine Learning Street Talk YouTube channel and podcast with my

18:39.820 --> 18:46.100
two compadre, Syac, the neural network pruner, Paul, and Yannick, the Lightspeed protein

18:46.100 --> 18:49.180
folder, Kiltcher.

18:49.180 --> 18:53.780
Today we have an incredibly special guest, Simon Cornblith, and Simon got his PhD in

18:53.780 --> 18:56.980
brain and cognitive sciences from MIT.

18:56.980 --> 19:00.740
His undergrad was from Caltech, and he's a research scientist at Google Brain.

19:00.740 --> 19:03.660
He's been there since about 2017.

19:03.660 --> 19:08.900
He's been cited nearly 2,000 times, which for someone quite early in career is seriously

19:08.900 --> 19:09.900
impressive.

19:09.900 --> 19:16.500
He's got a keen interest in the digital humanities, in philosophy, computer science, machine learning,

19:16.500 --> 19:19.140
computer vision, and neuroscience.

19:19.140 --> 19:22.900
He used to be a neuroscientist before we started doing machine learning, and he tells us that

19:22.900 --> 19:27.180
he's got some very strong opinions about neuroscience and machine learning, which we certainly will

19:27.180 --> 19:28.860
be getting on to later.

19:28.860 --> 19:33.300
He's a huge lover of the Julia language, so if you Google Simon's name, you'll see him

19:33.300 --> 19:38.820
talking at about a million Julia conferences, so definitely check that out as well.

19:38.820 --> 19:43.740
Simon pioneered the use of centered kernel alignment as a way of analyzing the evolution

19:43.740 --> 19:49.180
of representations in layers, in network, and between networks of different architectures.

19:49.180 --> 19:53.980
Now Simon, like me, is a lover of similarity matrices, and what can be gleaned from them?

19:53.980 --> 19:59.220
On my own PhD, I worked with them a lot for music segmentation, and also for detecting

19:59.220 --> 20:01.780
regimes in financial data sets.

20:01.780 --> 20:07.220
When a block in a ResNet is no longer self-similar to previous layers early on, you might intuit

20:07.220 --> 20:12.700
that it's moving into a new representational regime, or maybe it's just started hallucinating.

20:12.780 --> 20:16.540
All of this stuff was covered in his paper, Do Wide and Deep Neural Networks Learn the

20:16.540 --> 20:21.900
Same Things, and I find it fascinating that representation of self-similarity can reveal

20:21.900 --> 20:23.540
network pathology.

20:23.540 --> 20:27.580
Now in his paper, What's in a Loss Function for Image Classification, he noted that different

20:27.580 --> 20:33.500
losses and regularizers have similar accuracies on several data sets, but using the same representational

20:33.500 --> 20:39.500
evolution analysis, Simon gleaned that these losses and regularizers only affected the

20:39.500 --> 20:43.340
penultimate layers in the neural network, revealing inherent limitations in what can

20:43.340 --> 20:46.500
be achieved in manipulating the loss on a network.

20:46.500 --> 20:50.140
Now next in the session today, we're going to talk about the Simclear paper, and this

20:50.140 --> 20:55.940
was an incredibly exciting paper for unsupervised contrastive image learning with augmentations.

20:55.940 --> 21:00.740
It introduced a learnable nonlinear transformation between the representations and the contrastive

21:00.740 --> 21:04.100
loss, which massively improved the representations.

21:04.100 --> 21:08.740
The composition of augmentations is super important, and whenever anyone asks me about

21:08.980 --> 21:12.100
what are the different data augmentations in computer vision, I always point them to

21:12.100 --> 21:16.940
the SimCLR paper because it's got this wonderful matrix, and in that matrix it was shown that

21:16.940 --> 21:21.740
the crop and the color I think were the most effective augmentations, but Simon also noted

21:21.740 --> 21:25.580
that the batch sizes were super important, and the paper improved over the state of the

21:25.580 --> 21:30.020
art on the ImageNet top one, and actually matched unsupervised methods for the first

21:30.020 --> 21:32.420
time, albeit with many more parameters.

21:32.420 --> 21:35.820
But the final paper we're going to talk about today is Big Self-Supervised Models, a strong

21:35.900 --> 21:40.060
semi-supervised learners, and this is where you can learn from fewer labeled examples while

21:40.060 --> 21:44.940
making use of a large amount of unlabeled data, and with unsupervised pre-training

21:44.940 --> 21:50.380
on SimCLR v2, supervised fine-tuning on a few labeled examples, and then distillation

21:50.380 --> 21:54.900
with unlabeled examples, this approach improved the label efficiency over previous state-of-the-art

21:54.900 --> 21:55.900
methods.

21:55.900 --> 22:00.140
I remember Yannick Lightspeed Kilcher made a video on this one, which I watched a few

22:00.140 --> 22:03.900
months ago, so Yannick will have all of that completely fresh in his mind.

22:03.980 --> 22:06.980
Anyway, Simon, it's an absolute pleasure to welcome you to the show.

22:06.980 --> 22:07.980
Thank you so much for coming.

22:07.980 --> 22:08.980
It's great to be here.

22:08.980 --> 22:09.980
Amazing.

22:09.980 --> 22:11.780
How did you get into machine learning?

22:11.780 --> 22:18.540
So I guess first I got into neuroscience, and then I got disillusioned with neuroscience.

22:18.540 --> 22:23.700
When I was pretty young, I was interested in consciousness and how we create this kind

22:23.700 --> 22:28.060
of impression of the external world inside our heads.

22:28.060 --> 22:32.500
And so I guess it's pretty obvious how that translates into an interest in brains and

22:32.500 --> 22:34.300
how the brain works.

22:34.300 --> 22:40.460
So I spent both four years as an undergraduate doing neuroscience research, and then seven

22:40.460 --> 22:46.900
years working with monkeys at MIT trying to figure out how monkey brains work.

22:46.900 --> 22:53.460
And then after that, I felt like we weren't getting very far by trying to record from

22:53.460 --> 22:58.100
neurons in monkeys' brains and figure out how those neurons work.

22:58.100 --> 23:03.780
So I thought about what other ways are there approaching this problem?

23:03.780 --> 23:10.380
How could we think about how to understand how the brain is doing tasks?

23:10.380 --> 23:16.500
And it seemed like maybe by building systems that can do those tasks well that are not

23:16.500 --> 23:18.740
biological, we could learn more.

23:18.740 --> 23:21.060
So that's how I got into machine learning.

23:21.060 --> 23:25.820
I joined the Google AI residency program, which is like this great program that Google

23:25.820 --> 23:32.660
has to take people who have extensive background in some field that is not machine learning

23:32.660 --> 23:35.580
and train them to do machine learning.

23:35.580 --> 23:40.660
And I ended up at Google, and initially I thought I'm going to spend a year here learning

23:40.660 --> 23:45.140
about machine learning related stuff, and then maybe I'll go back to neuroscience and

23:45.140 --> 23:49.820
I'll decide the tools for machine learning could be applied back to brains, and maybe

23:49.820 --> 23:54.580
we can learn more about brains by applying the tools of machine learning there.

23:54.580 --> 23:59.260
But ultimately I decided I was more interested in just looking at how the neural networks

23:59.260 --> 24:04.500
work and also in the engineering challenges of building better neural networks, which

24:04.500 --> 24:06.500
I actually think are fun.

24:06.500 --> 24:11.020
One of the thoughts that came to my mind is it's fascinating looking at the kind of introspective

24:11.020 --> 24:15.380
analysis that you've been conducting with neural networks, but could you contrast that

24:15.380 --> 24:16.380
with neuroscience?

24:16.380 --> 24:22.300
Because as I understand, you have MRI scans and you have different ways of trying to visualize

24:22.300 --> 24:26.620
and reason about the behavior of a brain, but you can't really tweak the architecture

24:26.620 --> 24:30.780
and tweak all of the knobs and the levers in quite the same way you do in machine learning.

24:30.780 --> 24:37.020
Yeah, so like in neuroscience people also use this analysis across different individuals

24:37.020 --> 24:39.740
or different organisms or whatever.

24:39.740 --> 24:44.620
It is a tool that people use in neuroscience as well, but I guess they're limited in the

24:44.620 --> 24:50.060
ways in which they could manipulate the systems that are providing these representations.

24:50.060 --> 24:55.900
So in neuroscience, you're always constrained by data, so you can compare representations

24:55.900 --> 25:00.340
of images across individuals by doing MRI scans.

25:00.340 --> 25:05.020
But first of all, you might not get a very good idea of how the brain is representing

25:05.020 --> 25:08.580
those images because there's a lot of noise in the MRI scan and there's a limit to how

25:08.580 --> 25:13.980
long you can scan each person, whereas I guess in a neural network, noise is not a problem.

25:13.980 --> 25:18.340
The entire system's deterministic, we just pass in the image and we get the representation

25:18.340 --> 25:19.340
vector.

25:19.460 --> 25:24.020
And you also have these kinds of limits of, like, we can't see what happens if people

25:24.020 --> 25:29.500
have bigger brains, like we can't manipulate the architecture in those kinds of ways.

25:29.500 --> 25:34.660
So even though we can look at how intact brains are working, we can't see how representations

25:34.660 --> 25:38.380
change when we manipulate them all that easily.

25:38.380 --> 25:42.340
And I guess, again, in machine learning, like we can do all of those things.

25:42.340 --> 25:45.220
We can look at what happens when we change the loss function.

25:45.220 --> 25:48.700
We can look at what happens when we make the network deeper or wider.

25:48.700 --> 25:54.140
So I think there are, like, some really cool ways that even the same techniques can be

25:54.140 --> 25:59.420
applied in machine learning that they couldn't be applied in neuroscience.

25:59.420 --> 26:05.700
I felt like we weren't getting very far by trying to record from neurons in monkey's

26:05.700 --> 26:09.140
brains and figure out how those neurons work.

26:09.140 --> 26:15.020
Like it didn't really seem like a very effective way of figuring out how the brain constructs

26:15.220 --> 26:18.900
this kind of internal representation of the world.

26:18.900 --> 26:24.460
So from there, I thought about what could we actually do to understand this?

26:24.460 --> 26:29.940
And it seemed like the most promising thing to do was to look at what happens in simpler

26:29.940 --> 26:35.540
systems that we can construct ourselves and where we can analyze the behavior of everything

26:35.540 --> 26:36.980
inside the system.

26:36.980 --> 26:41.020
So in a neural network, we can record all the neurons, which is extremely challenging

26:41.020 --> 26:42.900
in a biological organism.

26:42.940 --> 26:48.100
And we can also manipulate the system in any kind of way that we can imagine.

26:48.100 --> 26:51.700
But it still seems like really hard to understand neural networks.

26:51.700 --> 26:57.500
So it seemed like maybe this was a more tractable challenge and a challenge where maybe we could

26:57.500 --> 27:01.740
make some headway that might eventually translate back to brains.

27:01.740 --> 27:03.780
And so that's how I ended up in machine learning.

27:03.780 --> 27:06.740
I guess there are, like, other great things about machine learning.

27:06.740 --> 27:10.940
I guess the pay is much better than in, like, academic neuroscience.

27:10.980 --> 27:16.180
But really, I think, like, it's a logical progression based on the ideas that I was

27:16.180 --> 27:17.020
interested in.

27:17.020 --> 27:21.540
And I am still interested in the same sorts of ideas.

27:21.540 --> 27:27.700
Do you still think now that you're in machine learning and have made some progress here that

27:27.700 --> 27:33.660
there is a good chance that we're going to map our knowledge that we gain back to the brain?

27:33.660 --> 27:38.380
Or do you think there is a bit of a disconnect?

27:38.380 --> 27:40.740
I think that's a really good question.

27:40.740 --> 27:45.380
I think there is definitely some knowledge that we're going to get from machine learning

27:45.380 --> 27:47.020
that will map back to the brain.

27:47.020 --> 27:52.380
I think, like, in terms of general principles and ways of looking at how, like, information

27:52.380 --> 27:58.220
processing systems work, I think there are a lot of ideas from machine learning that

27:58.220 --> 28:01.420
will ultimately help us understand brains.

28:01.420 --> 28:06.780
I'm a little less sure whether we're going to build, like, a machine learning system

28:06.780 --> 28:07.780
that is a brain.

28:07.780 --> 28:12.380
I think there's a disconnect between the way that the systems that we build work and

28:12.380 --> 28:13.820
the way that biology works.

28:13.820 --> 28:17.980
And I think that's insurmountable just because there's differences between what you can

28:17.980 --> 28:23.220
build efficiently with cells and what you can build efficiently in silicon.

28:23.220 --> 28:28.860
But in terms of approaches to understanding, in terms of building tools to understand things,

28:28.860 --> 28:34.660
the tools that we build in machine learning, I think will eventually be useful in neuroscience.

28:34.660 --> 28:40.460
So people make a lot of analogies and they make a lot of claims about neuroscience in

28:40.460 --> 28:43.540
connections with neural networks.

28:43.540 --> 28:48.100
Is there a statement or a bunch of statements that you hear over and over again where you

28:48.100 --> 28:51.580
just cringe because they're so wrong?

28:51.580 --> 28:53.380
Is that something that happens to you?

28:53.380 --> 28:55.300
I can imagine it would.

28:55.300 --> 28:56.300
Yeah.

28:56.300 --> 28:57.300
Yeah.

28:57.300 --> 29:01.140
So I think there's this, like, kind of basic fact that neural networks are inspired by

29:01.140 --> 29:03.060
brains, which is true.

29:03.060 --> 29:07.940
Then there's all this other stuff where people try to relate whatever neural network they've

29:07.940 --> 29:11.700
built back to a brain and they say that it works like the brain, but it doesn't work

29:11.700 --> 29:12.700
like the brain.

29:12.700 --> 29:17.940
There's still this huge kind of disconnect in how the system is actually operating.

29:17.940 --> 29:20.500
The brain is not literally doing back prop.

29:20.500 --> 29:22.820
It might be doing something that's like back prop.

29:22.820 --> 29:29.100
We still don't really know, but it's not literally computing gradients by automatic differentiation.

29:29.100 --> 29:33.340
And I'm fascinated to talk about this line of reasoning that you have because you're

29:33.340 --> 29:38.020
clearly the kind of guy that you want to reason about the behavior of models and in particular

29:38.020 --> 29:40.220
the evolution of representations.

29:40.220 --> 29:43.380
And I watched one of your presentations on YouTube where you were talking about how you

29:43.380 --> 29:46.620
can compare the representations by comparing features.

29:46.620 --> 29:50.620
And of course, the naive way of doing is the dot product or some variations of that.

29:50.620 --> 29:52.100
Turns out that doesn't work very well.

29:52.100 --> 29:57.420
And you came up with this wonderful metric called the centered kernel alignment.

29:57.420 --> 30:00.020
So how did that all come about?

30:00.020 --> 30:06.420
The way we came up with that idea was that Jeff Hinton had another idea and I tried the

30:06.420 --> 30:11.860
idea and it worked, but then we wondered, is there a simpler thing that worked?

30:11.860 --> 30:14.300
And that's how we ended up with centered kernel alignment.

30:14.300 --> 30:19.620
I guess the problem that we had in trying to come up with a way of comparing similarity

30:19.620 --> 30:24.820
of neural network representations is that it's really hard to know what is a good way.

30:24.820 --> 30:28.780
Like it's not something where you can really develop a good benchmark.

30:28.780 --> 30:34.300
So like in the paper, we came up with this simple sanity check where the idea is basically

30:34.300 --> 30:39.460
we've got two architecturally identical neural networks and we just train them from different

30:39.460 --> 30:41.260
random initializations.

30:41.260 --> 30:45.700
And so we want it to be the case that if you measure like the similarity between a layer

30:45.700 --> 30:51.100
from network A and all the layers from network B, that the most similar layer in network

30:51.100 --> 30:53.820
B is going to be the architecturally corresponding layer.

30:53.820 --> 30:59.540
So if we have layer two from network A, it should be more similar to layer two from network

30:59.540 --> 31:02.420
B than layer three or layer four.

31:02.420 --> 31:08.860
And so like basically we found that what people had been doing before didn't always pass that

31:08.860 --> 31:10.380
sanity check.

31:10.380 --> 31:14.620
And we basically tried to come up with the simplest way of building a similarity index

31:14.620 --> 31:17.580
that did actually pass that sanity check.

31:17.580 --> 31:20.020
And that's how we ended up with centered kernel alignment.

31:21.020 --> 31:25.260
Yeah, because I think you showed that the canonical correlation analysis only worked

31:25.260 --> 31:27.940
about, I think at an accuracy of about 1.4%.

31:27.940 --> 31:30.220
So it's complete apples and oranges.

31:30.220 --> 31:34.180
But this absolutely fascinates me though, because when you plot this thing in this kind

31:34.180 --> 31:40.100
of self similarity matrix, you can glean so much about the evolution as a function of

31:40.100 --> 31:41.100
time.

31:41.100 --> 31:44.260
And because you talk about this in one of your other papers as well, that there's this

31:44.260 --> 31:46.700
characteristic blockiness.

31:46.700 --> 31:52.780
And when you see blockiness, that successive layers are similar to versions of themselves

31:52.780 --> 31:54.140
in the past.

31:54.140 --> 31:57.700
And that kind of means that they're not evolving anymore.

31:57.700 --> 32:02.260
And you then made the intuition in your paper that, well, essentially it's redundant information.

32:02.260 --> 32:05.380
If it's not learning anything new, I can just delete that block.

32:05.380 --> 32:08.740
I can just delete those layers from the neural network and it won't make any difference.

32:08.740 --> 32:10.540
And indeed it didn't.

32:10.540 --> 32:13.540
Yeah.

32:13.540 --> 32:21.100
Could you, for people listening, explain the similarity measure you came up with in principle,

32:21.100 --> 32:25.180
just so we can imagine something, how that should even work?

32:25.180 --> 32:26.180
Yeah.

32:26.180 --> 32:31.620
So I guess the idea is you've got a neural network and you feed some set of examples,

32:31.620 --> 32:33.900
like multiple examples through the neural network.

32:33.900 --> 32:40.100
And now you've got some matrix where the rows of the matrix are different examples and the

32:40.100 --> 32:42.820
columns are different neurons.

32:42.820 --> 32:47.940
So yeah, you can imagine this as if you have vectors of activations for each example, you've

32:47.940 --> 32:50.460
stacked them real wise.

32:50.460 --> 32:54.900
So now what do we do with that to compare two neural networks trained from different

32:54.900 --> 32:56.500
random initializations?

32:56.500 --> 33:01.140
The problem is if we were to just take the square difference between those matrices,

33:01.140 --> 33:06.780
we have this problem that the neurons between these two different networks aren't necessarily

33:06.780 --> 33:10.500
aligned in any way if they're trained from different random initializations.

33:10.500 --> 33:14.660
Even if we had exactly the same neurons, we shouldn't expect that neuron one would be

33:14.660 --> 33:18.420
the same, representing the same thing in both networks.

33:18.420 --> 33:21.180
So we need some way to get around that problem.

33:21.180 --> 33:28.860
One way around this problem is instead of comparing these original matrices, we're going

33:28.860 --> 33:34.820
to make matrices that measure the similarity of each example to each other example for

33:34.820 --> 33:37.420
one particular network.

33:37.420 --> 33:42.740
So if we've got example A and example B, we can measure their similarity very simply

33:42.740 --> 33:46.260
just by taking the dot product between those two vectors.

33:46.260 --> 33:50.060
And now because we're measuring similarity from the same network, we don't have to worry

33:50.060 --> 33:52.180
about this alignment problem.

33:52.180 --> 33:57.860
And we get some idea of how similar different examples are to each other according to the

33:57.860 --> 34:03.100
representation in network A. So if we do that for all the examples, we get some examples

34:03.100 --> 34:05.300
by examples matrix.

34:05.300 --> 34:09.660
And then we can do that both for our first network and for our second network.

34:09.660 --> 34:14.620
So after we've done that, we've got these two examples by examples matrices.

34:14.620 --> 34:19.780
And then the easy way to compare those matrices is we just reshape them to vectors and we

34:19.780 --> 34:22.620
take the dot product again between those vectors.

34:22.620 --> 34:28.660
So now we've measured the similarities between the similarities of the examples.

34:28.660 --> 34:33.660
And this doesn't have this problem of aligning the neurons because instead of measuring similarities

34:33.660 --> 34:37.780
of neurons, we're measuring similarities of examples and then we're comparing those

34:37.780 --> 34:38.780
similarities.

34:38.780 --> 34:42.620
So ultimately, we do that, we take that dot product and then we normalize it in a way

34:42.620 --> 34:45.020
that makes it invariant to scaling.

34:45.020 --> 34:50.420
So if you just took the dot product, you'd have this problem that scaling all of the

34:50.420 --> 34:55.420
features by some number, if you scale everything by a factor of two, the dot product will go

34:55.420 --> 34:57.380
up by a factor of two.

34:57.380 --> 35:01.860
And so we just apply some normalization so that kind of scaling will not affect the similarity

35:01.860 --> 35:08.420
index and we get centered kernel alignment, which gives us a similarity score between

35:08.420 --> 35:10.060
zero and one.

35:10.060 --> 35:13.460
The fascinating thing is that you can replace that dot product with a kernel because it's

35:13.460 --> 35:14.580
a gram matrix.

35:14.580 --> 35:15.580
Yeah.

35:15.580 --> 35:19.900
So did you find that it made a difference if you use, let's say, the RBF kernel?

35:19.900 --> 35:20.900
Yeah.

35:20.900 --> 35:21.900
Yeah.

35:21.900 --> 35:25.700
So, yeah, basically when we're measuring the similarities between examples, we can just

35:25.700 --> 35:29.180
instead of taking the dot product between the representations of the different examples,

35:29.180 --> 35:33.900
we can take the kernel between one example and another example because the kernel is

35:33.900 --> 35:36.460
also a way of measuring similarity.

35:36.460 --> 35:38.620
And so we tried that.

35:38.620 --> 35:41.940
It turns out that like for CNNs, it didn't really make a difference.

35:41.940 --> 35:46.340
Like the RBF kernel worked, but sort of just taking a regular dot product.

35:46.340 --> 35:52.260
But we did find in the appendix of that paper that if you instead use an RBF kernel with

35:52.260 --> 35:57.260
a transformer, it actually does work better than taking a normal dot product.

35:57.260 --> 36:02.300
And I think like part of what's going on is that sometimes you want it to be the case

36:02.300 --> 36:08.300
that when you're measuring similarity, you care more about the distances between the

36:08.300 --> 36:13.540
examples that you're close to than the distances to the examples that you're far away from.

36:13.540 --> 36:18.260
Like once you're really far away from something, maybe it doesn't matter so much if you're

36:18.260 --> 36:24.900
10 times as far away because like you're already so far, you're already not going to...

36:24.900 --> 36:30.380
You don't really care how far away something is once you're far enough.

36:30.380 --> 36:34.980
And the RBF kernel takes that into account in a way that a linear dot product wouldn't.

36:34.980 --> 36:43.660
The linear dot product is like very sensitive to the global distances in the space.

36:43.660 --> 36:47.020
What I find fascinating is that you can glean so much from the blockiness, right?

36:47.020 --> 36:51.380
So you are saying that as it becomes blockier, it might be an indication that it's become

36:51.380 --> 36:53.700
saturated in some sense.

36:53.700 --> 36:59.940
And I'm also interested in a way, we already know that the representations in neural networks

36:59.940 --> 37:03.500
are increasingly abstract, so they don't necessarily bear any resemblance to the beginning.

37:03.500 --> 37:08.260
So when we're looking at the cell similarity matrix, we don't necessarily want the representations

37:08.260 --> 37:12.340
on the final penultimate layers to be similar to the ones at the beginning.

37:12.340 --> 37:15.260
We want there to be a continuous evolution.

37:15.260 --> 37:20.180
We don't want to have a stalled evolution because that would correspond to this blockiness.

37:20.180 --> 37:25.940
But is it when you've stalled for a long time, is that when it becomes pathological?

37:25.940 --> 37:29.380
Because we want it to evolve in stops and starts, don't we?

37:29.380 --> 37:30.380
Yeah.

37:30.380 --> 37:35.500
I think it's not clear what we should really expect in terms of how a neural network representation

37:35.500 --> 37:37.060
evolves through the layers.

37:37.060 --> 37:42.340
I think there's kind of some theory on what we should expect if all the layers are linear.

37:42.340 --> 37:46.780
But obviously, the neural networks that we train are nonlinear, and it's really important

37:46.780 --> 37:49.380
to have a nonlinearity in between the layers.

37:49.380 --> 37:54.180
And so at that point, it's really hard to reason about what the optimal thing for a

37:54.180 --> 37:56.700
neural network to do actually is.

37:56.700 --> 38:00.180
I think it's something that we can really only study empirically.

38:00.180 --> 38:04.220
On the other hand, I do think if we see that nothing is changing from one layer to the

38:04.220 --> 38:06.820
next, that's a really bad sign.

38:06.820 --> 38:10.700
If the neural network representation isn't changing, then obviously nothing's happening.

38:10.700 --> 38:15.540
But I guess it's unclear whether we should expect abrupt shifts or we want things to

38:15.540 --> 38:17.380
happen slowly between the layers.

38:17.380 --> 38:22.700
I'm not sure whether we really have the theoretical knowledge to say what is best.

38:22.700 --> 38:23.700
Yeah.

38:23.700 --> 38:27.940
I'd love to see this as a kind of tool in our toolbox that we could use on different network

38:27.940 --> 38:28.940
architectures.

38:28.940 --> 38:32.780
But you said that the other learn features are shared across different initializations

38:32.780 --> 38:36.300
and architectures, particularly across the depths of the network.

38:36.300 --> 38:42.380
So it almost seems as if this blockiness is separate to your work in wide and deep neural

38:42.380 --> 38:45.900
networks because you showed that the width and the depth have got different effects on

38:45.900 --> 38:49.620
network predictions at the example level or at the class level.

38:49.620 --> 38:52.140
But the blockiness almost seems to be an orthogonal thing.

38:52.140 --> 38:56.420
That's just when you have this kind of saturation of the network, you see the blockiness.

38:56.420 --> 38:57.420
Yeah.

38:57.420 --> 38:58.420
Yeah.

38:58.420 --> 39:02.980
So initially we had hoped that we could look at other similarities between wide networks

39:02.980 --> 39:06.180
and deep networks in their representations.

39:06.180 --> 39:09.620
But like when we did those experiments, we actually just found that if you make the network

39:09.620 --> 39:14.140
really wide, you get this kind of blockiness in the representations and those blocks are

39:14.140 --> 39:17.220
like dissimilar across different initializations.

39:17.220 --> 39:19.700
And then the same thing happens if you make the network really deep.

39:19.700 --> 39:23.200
We see these like big blocks in the representations.

39:23.200 --> 39:29.380
So that made it hard to study these very wide and very deep networks from the representational

39:29.380 --> 39:31.020
similarity perspective.

39:31.060 --> 39:34.380
But at the same time, I think it's like a really interesting observation.

39:34.380 --> 39:39.260
Like it's something where we couldn't have predicted this ahead of time based on what

39:39.260 --> 39:43.980
we know about neural network theory and we couldn't have predicted it ahead of time based

39:43.980 --> 39:46.100
on the accuracy of the network.

39:46.100 --> 39:50.860
It's something where we really needed like these techniques for looking at the internal

39:50.860 --> 39:55.620
representations of neural networks to see what was happening inside of them.

39:55.700 --> 40:00.820
There's this whole literature that takes a look at a network's expressibility with

40:00.820 --> 40:03.100
regards to its depth and width.

40:03.100 --> 40:07.980
So could you just explain it to us whether or not we should be able to meaningfully quantify

40:07.980 --> 40:13.220
or formulate the expressibility of a neural network with regards to your analysis made

40:13.220 --> 40:14.220
on that?

40:14.220 --> 40:15.220
Yeah.

40:15.220 --> 40:18.780
So there's this work that looks at like kind of the functions that can be expressed by

40:18.780 --> 40:22.100
wide networks and the functions that can be expressed by deep networks.

40:22.100 --> 40:28.180
And I guess like the neural networks seem to become exponentially more expressive as

40:28.180 --> 40:29.900
you make them deeper.

40:29.900 --> 40:33.460
So it seems like in that sense depth is more important than width.

40:33.460 --> 40:38.100
But on the other hand, like the neural networks that we actually train in this paper, like

40:38.100 --> 40:42.220
both the wide networks and the deep networks are big enough that they can overfit the entire

40:42.220 --> 40:43.540
training set.

40:43.540 --> 40:48.460
So in this case, like the expressibility of the network is not really important.

40:48.660 --> 40:52.260
Important is the function that the network actually ends up learning.

40:52.260 --> 40:57.740
So I guess even though networks could express more functions when they're deep, what we're

40:57.740 --> 41:03.140
really studying is the function that you actually get when you train the neural network by gradient

41:03.140 --> 41:07.260
descent on some data, what the optimization process actually finds.

41:07.260 --> 41:12.060
One thing as well in that paper, you talked about the network pathology, right?

41:12.060 --> 41:18.260
You said that two times depth accuracy 95%, four times 93.2, eight times 91.9.

41:18.260 --> 41:21.460
So because this is this runs counter to what a lot of us would intuit.

41:21.460 --> 41:23.460
We think that you can have as much depth as you want.

41:23.460 --> 41:27.260
And architectures like ResNet in some sense, they learn their own capacity.

41:27.260 --> 41:29.340
There is a pathology there happening clearly.

41:29.340 --> 41:33.780
And how would you determine that from this visualization?

41:33.780 --> 41:36.500
Yeah, so I guess there are two kind of results.

41:36.500 --> 41:42.020
So like you can either look at networks without residual connections, where you do actually

41:42.100 --> 41:45.780
find that at some depth, the accuracy will start going down.

41:45.780 --> 41:51.820
And in networks without residual connections, we find that like that the depth where accuracy

41:51.820 --> 41:56.860
starts to go down is like around the same depth where you begin seeing this kind of block

41:56.860 --> 42:00.820
structure where many successive layers have similar representations.

42:00.820 --> 42:04.740
And it looks like the representation is no longer getting refined through the network.

42:04.740 --> 42:07.740
Yeah, I mean, with ResNets, you can make them much deeper.

42:07.740 --> 42:12.860
And it seems like it doesn't hurt accuracy as much even once you start getting these blocks.

42:13.100 --> 42:17.260
But it also seems once you start getting these blocks, making the network deeper, making

42:17.260 --> 42:21.580
that work wider, no longer really gives you any improvement in accuracy.

42:21.580 --> 42:27.020
So it seems like this is basically telling you that the network has fit the data as much as it can.

42:27.020 --> 42:31.940
And and there's no real advantage to using something bigger.

42:33.180 --> 42:37.340
Fascinating. Let's move on to another paper that you've done, which is quite related in

42:37.340 --> 42:40.020
terms of you've used the same analysis to reason about it.

42:40.020 --> 42:43.260
But you had a paper called What's in a Loss Function for Image Classification.

42:43.420 --> 42:47.820
And you looked at a whole bunch of different label smoothing and regularizers, which are

42:47.820 --> 42:49.580
things that you do on the end of the network.

42:49.580 --> 42:54.820
And you identified differences in accuracy and calibration and out of domain distribution.

42:55.020 --> 42:57.620
And you made some really interesting observations.

42:57.620 --> 43:00.900
So by the way, we're talking about things like do we use the softmax or the squared

43:00.900 --> 43:03.780
area or dropout or label smoothing or logic penalty.

43:03.940 --> 43:07.540
But you noticed using the same analysis technique that only affected the

43:07.540 --> 43:10.700
representations on the penultimate layers of the neural network.

43:10.900 --> 43:12.580
What's going on there?

43:12.580 --> 43:15.020
Yeah, so it's not just the penultimate layer.

43:15.020 --> 43:20.500
It's like the the last maybe third of the network is affected by the loss function.

43:20.500 --> 43:25.460
But then the first two thirds of the network, it seems like you learn the same representation

43:25.460 --> 43:27.460
no matter what loss function you use.

43:27.460 --> 43:29.860
So it doesn't change if you use label smoothing.

43:30.100 --> 43:35.300
It doesn't even change if you use mean squared error instead of using softmax cross entropy.

43:35.300 --> 43:40.260
You still basically learn the same representation for the first two thirds of the network.

43:40.260 --> 43:44.020
And I think it's still it's a bit of a puzzle to us why this happens.

43:44.020 --> 43:47.540
Clearly, it matters that you're training the network with the loss function.

43:47.540 --> 43:52.820
There's those layers in the first two thirds of the network do change from the initialization.

43:52.820 --> 43:58.260
But I guess it seems that the last third of the network is setting up the penultimate

43:58.260 --> 44:02.180
layer representation in a way that is good for your loss function.

44:02.180 --> 44:07.540
But the first two thirds of the network are somehow just learning general features.

44:07.540 --> 44:11.300
I think this also like corresponds with the success of transfer learning,

44:11.300 --> 44:15.700
where we can take features that we've learned on one task and transfer them to some other task.

44:16.500 --> 44:21.940
What's the implication that it seems is the implication that the loss function

44:21.940 --> 44:25.780
is not having any impact on the representations early on in the network?

44:26.500 --> 44:28.180
That seems like quite a big implication.

44:29.140 --> 44:34.500
Yeah, I think the loss function must have some impact because if you don't train

44:34.500 --> 44:40.180
the network, if you don't have any loss function at all, then the representation

44:40.180 --> 44:44.820
in that first two thirds of the network is actually quite different.

44:44.820 --> 44:48.980
I think what's really happening is there are these differences among the loss functions,

44:48.980 --> 44:52.660
which don't really matter except later in the network.

44:52.740 --> 44:58.500
Although they will give you a slight change in accuracy and slight changes in robustness,

44:58.500 --> 45:01.940
they don't matter for this general feature learning process.

45:01.940 --> 45:07.540
I guess maybe it's what we should expect because ultimately we're asking the network

45:07.540 --> 45:09.700
to do the same thing just in a slightly different way.

45:09.700 --> 45:12.660
We're still asking the network to classify images.

45:12.660 --> 45:17.060
We're just asking it to provide slightly different outputs to produce a slightly

45:17.060 --> 45:20.020
different representation at the penultimate layer.

45:20.100 --> 45:26.900
Maybe we should expect that those earlier features that are just trying to

45:27.540 --> 45:33.540
represent general things about images, those will be the same no matter what loss function we pick.

45:34.820 --> 45:39.380
In your experiments, did you find whether or not model capacity has anything to do with it?

45:39.380 --> 45:44.580
Yeah, so we didn't really investigate different model capacities in that paper.

45:44.580 --> 45:49.380
I would expect that the same thing holds for a wide range of model capacities.

45:49.700 --> 45:53.860
There's no indication from the experiments that if you use a bigger network or a slightly

45:53.860 --> 45:55.860
smaller network that things would change all that much.

45:57.060 --> 46:01.300
Yeah, I think it's still an open question how model capacity changes things.

46:01.300 --> 46:06.180
I guess in the Sinclair paper, we found that model capacity can matter quite a bit.

46:06.180 --> 46:09.300
Yeah, so the general hypothesis there should also hold,

46:09.300 --> 46:14.180
even though your model is bigger or smaller, no matter how big or smaller your network is,

46:14.180 --> 46:19.060
the general feature learning regime or paradigm should still hold no matter what

46:19.060 --> 46:21.140
loss function you would end up using.

46:21.140 --> 46:25.860
Yeah, that would be my guess. I think if you're in a regime where it's really hard for you to fit

46:25.860 --> 46:30.500
the training data, if you have a very small network, it might be the case that you see more

46:30.500 --> 46:36.740
differences in the earlier layers because it might be that the loss function really affects

46:36.740 --> 46:42.100
what features are best there in a way that it wouldn't if the network is a bit bigger and if

46:42.100 --> 46:47.940
it's more capable of fitting your training data. But I don't really know. I think this

46:47.940 --> 46:51.540
is something that is probably worth looking at in some follow-up work.

46:52.420 --> 46:57.220
And you found also there's implications for transfer learning with respect to the loss

46:57.220 --> 47:03.540
function. There seems to be some inverse correlation between the gains you get from a loss

47:03.540 --> 47:09.220
function and how good it is for transfer learning. Or is there a connection between

47:09.220 --> 47:11.940
loss functions and regularizers and all of that?

47:12.660 --> 47:17.380
Yeah, we look at just linear transfer in the paper. So if we take the features from the

47:17.380 --> 47:22.420
penultimate layer and we try to use them directly for some other task, how good are those features

47:22.420 --> 47:30.900
going to be? And what we found was that if you use loss functions that give you higher accuracy

47:30.900 --> 47:37.940
on ImageNet, you tend to learn representations that transfer substantially worse in that setting.

47:38.900 --> 47:44.340
And our intuition is you could learn many different kinds of representations in that

47:44.340 --> 47:50.980
penultimate layer and still do a reasonable job of classifying ImageNet. But what seems to happen

47:50.980 --> 47:56.820
is that the loss functions that perform better lead classes to become more separated in the

47:56.820 --> 48:03.220
penultimate layer. So like class A and class B will be farther apart relative to the variability

48:03.220 --> 48:08.820
within the class. And when you have a situation like that, you have this penultimate layer

48:08.820 --> 48:14.260
representation that's specialized for the classes in ImageNet. Like you've got a thousand clusters

48:14.260 --> 48:20.180
corresponding to the thousand classes in ImageNet. And so then if you want to use like those kinds

48:20.180 --> 48:26.100
of representations for some other task, it will only really work well if you have exactly the

48:26.100 --> 48:32.260
same classes that are in ImageNet because they're already organized by the ImageNet classes.

48:32.340 --> 48:35.780
On the other hand, what we found is if you just use standard softmax loss,

48:35.780 --> 48:40.500
actually the classes are not that separated from each other in the penultimate layer

48:40.500 --> 48:45.860
representation. And because they're not that separated, there are these features that you

48:45.860 --> 48:51.620
could use to classify things that are not ImageNet that still convey some kind of

48:51.620 --> 48:57.060
useful information about the images that are not just their ImageNet class labels.

48:57.940 --> 49:03.700
It hints at a bit of a future where, you know, like right now on whatever TensorFlow Hub or

49:03.700 --> 49:09.220
HuggingFace repositories and so on, we have these pre-trend models. And the pre-trend models,

49:09.220 --> 49:16.100
they're like full stack models and people usually take some sort of last or next to last hidden layer.

49:16.820 --> 49:24.500
But maybe we should much more focus on actually providing like half of a network to share. Like

49:24.500 --> 49:30.580
determining which are actually the best good or general representations from a data set and so on.

49:30.580 --> 49:33.380
Do you have any of this in mind when you do work like this?

49:34.500 --> 49:41.060
Yeah, at Google, what is generally best for us to do is just to fine-tune the whole network.

49:41.060 --> 49:44.660
And if you fine-tune the whole network, it eliminates some of these issues with

49:45.300 --> 49:50.260
the actual form of the representation in the penultimate layer. Because even if you have this

49:50.260 --> 49:54.900
kind of highly specialized penultimate layer, when if you're allowed to change all the other

49:54.900 --> 49:58.580
weights in the network, you can fix that and you can specialize the rest of the network

49:59.140 --> 50:04.900
for some other task. But yeah, I think like it's a really interesting question. If we just want

50:04.900 --> 50:09.780
to turn an image into a vector that we could then train a linear classifier on top of,

50:09.780 --> 50:14.100
what is the best way of doing that? How should we approach that problem? And how should we approach

50:14.100 --> 50:19.620
that problem if we want this very like general universal vector representation of an image that

50:19.620 --> 50:24.020
would work well for a lot of different tasks? And I think we don't really have good ways of

50:24.020 --> 50:30.820
doing that because basically this is all empirical, right? Like, we don't know what makes a good

50:30.820 --> 50:35.620
universal representation of an image. We've just got to try a bunch of things and figure out what

50:35.620 --> 50:40.420
works best. And I guess, yeah, the insight from this paper is like actually the loss function

50:40.420 --> 50:45.940
that you use to train the network can make a huge difference there. Fascinating. I guess without

50:45.940 --> 50:52.180
any further ado, we should move on to SIMCLEAR, a simple framework for contrastive learning of

50:52.180 --> 50:57.540
visual representations. We've been absolutely fascinated by this concept of unsupervised

50:57.540 --> 51:02.820
contrastive image representation learning algorithms. We've seen such a huge kind of step

51:02.820 --> 51:09.060
forward, haven't we, over the last couple of years in this area? Yeah, it's pretty amazing to me.

51:09.620 --> 51:13.860
Could you just go back to real basics? Imagine that people out there have been living in a

51:13.860 --> 51:18.580
cave. They don't know what contrastive learning is. They don't know about image augmentation.

51:18.580 --> 51:23.940
How would you frame the whole thing up? The self-supervised learning setup is we've got a

51:23.940 --> 51:31.540
bunch of images and at least the initial like historical self-supervised learning setup is

51:31.540 --> 51:36.980
we've got a bunch of images. We want to train some kind of neural network on it. And we want

51:36.980 --> 51:40.980
that neural network to learn a representation such that when we then just train a linear

51:40.980 --> 51:45.780
classifier on top of that representation to classify ImageNet, it's going to do well. But

51:45.780 --> 51:51.780
we want to learn the initial representation without using any kind of labels. And yeah,

51:51.780 --> 51:56.980
I guess there are a lot of different approaches that people tried for this problem. Like people

51:56.980 --> 52:04.660
tried things like let's train a neural network so that we can cut up the image into just a grid

52:04.660 --> 52:09.860
and shuffle the grid. And then the neural network has to figure out how to assemble these puzzle

52:09.860 --> 52:14.900
pieces back into the original image. And maybe that'll give us a good representation. Or let's try

52:15.540 --> 52:21.460
just rotating the images so we can have images that are rotated 90, 180, 270 degrees. And then

52:21.460 --> 52:27.380
we'll have the neural network try to classify what rotation we fed into it. And so people came up

52:27.380 --> 52:32.340
with these kinds of tasks that you could try to train a neural network to do so that it would learn

52:32.340 --> 52:37.620
some kind of good representation. They were defined in this ad hoc way. Let's come up with some kind

52:37.620 --> 52:43.300
of funny thing where you don't need a label. You can have the neural network trained to do

52:43.300 --> 52:49.700
this kind of thing. And maybe it'll learn something about images. Starting in around 2018, there are

52:49.700 --> 52:57.380
a few papers that basically suggested this alternative approach where you're trying to learn

52:57.380 --> 53:03.780
some kind of representation space where you've got different patches from an image or different

53:03.860 --> 53:08.420
augmentations from an image, just different representations of the same image. And you want

53:08.420 --> 53:14.180
to learn a representation space where these representations of the same image are all close

53:14.180 --> 53:19.860
together in that representation space. And they're far apart from the representations of other images.

53:20.580 --> 53:27.220
This surprisingly seems to lead to very good representations. But it turns out there are a

53:27.220 --> 53:32.980
lot of very important details to get this to work well. So it's really like a situation where the

53:32.980 --> 53:39.300
basic idea is very simple. Let's create multiple views of an image and try to get them close to

53:39.300 --> 53:45.140
each other and far away from everything else. But things like augmentation and things like the

53:45.140 --> 53:50.820
exact way we set up the network end up being very important to learning a good representation with

53:50.820 --> 53:55.780
this kind of technique. How does the negative sampling work? People have done this in different

53:55.780 --> 54:01.620
ways. So in Simclear, our way of doing negative sampling is very simple. So basically,

54:02.340 --> 54:11.140
we are attracting two views of the same image. And then we have a mini batch that has 4,096

54:11.140 --> 54:18.420
images in it and two augmentations of each image. And so we are repelling using a softmax from all

54:18.420 --> 54:27.460
of the other 8,190 views in that mini batch. Basically, we want our two augmentations of

54:27.460 --> 54:33.140
the same image close and we want them to be far from the other 8,190 images.

54:34.420 --> 54:41.140
Yeah, it's a bit of a throwback to work to VEC. I think it's pretty cool how these ideas just come

54:41.140 --> 54:49.060
up through the eras and through the different models and so on. And there is seemingly always

54:49.060 --> 54:57.540
another layer on top of these ideas. Pretty cool. Yeah. So if you only consider, you know,

54:57.540 --> 55:02.900
two views that are coming out of the same image as the positive pair, so to speak, and all the other

55:02.900 --> 55:08.740
views are coming out of the different images located in the same mini batch, wouldn't this hurt

55:08.740 --> 55:15.780
the representation space to some extent? Let's say you have multiple images of dogs in a mini batch

55:16.180 --> 55:23.220
of 4,096 samples. We would essentially want the representations of different dogs to map together

55:23.220 --> 55:28.500
as closer as possible in the representation space while representations of cats from dogs would

55:28.500 --> 55:34.580
get further away. Wouldn't we expect this? But how does Simclear ensure this rigorously? I guess

55:34.580 --> 55:39.620
it's because of the larger batch sizes you use, but I still wanted to know from you.

55:39.620 --> 55:46.420
Yeah, one thing is, even if we've got other kinds of images that we want to be close together in the

55:46.420 --> 55:52.100
mini batch, even if we've got like a dog image and then another dog image and ultimately we want to

55:52.100 --> 55:58.500
learn a representation space where maybe they're not so far apart, like on average, most of the

55:58.500 --> 56:04.340
images in the mini batch are things that we want to be really far apart from. So maybe it doesn't

56:04.340 --> 56:12.180
hurt that much if we're repelling from everything as opposed to just repelling from images that are

56:12.180 --> 56:18.020
part of other classes. I think this actually is something that hurts current self-supervised

56:18.020 --> 56:22.260
learning techniques and hurts contrastive techniques because we also know when you do the

56:22.260 --> 56:29.220
contrastive loss, if you don't contrast against examples that are very close to you, that actually

56:29.300 --> 56:34.420
improves things a little bit. So if you don't contrast against the very hard negatives,

56:34.420 --> 56:39.860
we've found that gives you slightly higher accuracy when you do this linear evaluation.

56:39.860 --> 56:44.340
That kind of suggests that this really is a problem with these techniques that maybe sometimes you

56:44.340 --> 56:49.860
don't want to be as far apart from other images as the losses encouraging you to be. Now there's

56:49.860 --> 56:55.300
one other aspect which is that in Simclear, we don't actually use the representation that's

56:55.300 --> 57:01.780
feeding into the loss function. Like we have this projection head, an MLP on top of the network,

57:01.780 --> 57:06.900
and instead of using that representation at the end of the network, we use a representation

57:06.900 --> 57:14.100
that's two layers back. And so by using a representation that's two layers back, even if

57:14.100 --> 57:19.300
in the final layer we're pushing things apart, we kind of figure that this earlier representation

57:19.300 --> 57:24.020
might not have pushed apart the things that really are semantically similar. And indeed,

57:24.020 --> 57:29.860
we find that using this earlier representation in the network leads to higher linear evaluation

57:29.860 --> 57:37.140
accuracy. So it works better. I was very fascinated by all of these different tricks that you apparently

57:37.140 --> 57:44.500
have to get. And so big kudos to figuring all of this out for the rest of us. There has been a

57:44.500 --> 57:52.180
lot of follow-up work on this idea. A lot of modifications. There is this bootstrap your own

57:52.180 --> 57:59.060
latent where they completely leave out the negative sampling. Then I think just like one or two weeks

57:59.060 --> 58:07.460
ago, there was a paper saying if you build in a stop gradient into the contrastive loss,

58:07.460 --> 58:13.220
you also apparently don't need a negative and so on. Do you have maybe from your own work or

58:13.220 --> 58:21.300
from work of others, do you have any sort of current? If I were to build a self-supervised

58:21.380 --> 58:28.740
contrastive representation learner today, what is the top things I should do? What is my recipe?

58:28.740 --> 58:35.380
How do I go about it? The most important part of the recipe is data augmentation. So we're

58:35.380 --> 58:40.420
going to use two views from the same image and it's very important how those two views are

58:40.420 --> 58:47.460
constructed. But they're really only two super important data augmentations that we need. So

58:47.460 --> 58:53.540
we have to take two different crops from the same image and then we have to do some kind of color

58:53.540 --> 59:01.220
distortion. So in SimClear we use very aggressive color distortion. So that is probably the most

59:01.220 --> 59:06.900
important part of the recipe. Then I guess we feed that representation into a neural network

59:06.900 --> 59:12.420
and fortunately we found that you can just use a regular ResNet 50 for this part. You don't have

59:12.420 --> 59:19.060
to worry about architecture, engineering, specifically for contrastive learning. Then

59:19.060 --> 59:24.500
I think all of the work since SimClear also uses this idea of putting an MLP on top of the end of

59:24.500 --> 59:31.060
the network and then using that to get whatever representation goes into the loss function,

59:31.060 --> 59:37.300
but then discarding part of the MLP when we later just want the representation for a downstream

59:37.300 --> 59:45.540
task. All of those pieces are pieces that are shared by all of these modern self-supervised

59:45.540 --> 59:51.780
learning techniques. So like we introduced the idea of this projection head in SimClear and we

59:51.780 --> 59:56.660
also spend a lot of time studying the augmentation although we were not the first people to

59:57.620 --> 01:00:02.660
come up with the idea that the augmentation was important. Yeah, in terms of what the loss function

01:00:02.660 --> 01:00:08.180
is, I guess it's surprising that there are so many things that work that we use this contrastive

01:00:08.180 --> 01:00:13.060
loss in SimClear because it was what previous work had done and it's like intuitive that you might

01:00:13.060 --> 01:00:18.740
want to learn a space where you're explicitly pushing away representations of other examples,

01:00:19.540 --> 01:00:26.020
but I guess like in BYOL they aren't explicitly contrasting against representations of other

01:00:26.020 --> 01:00:32.740
examples. So instead they have a network where they're taking a moving average of

01:00:33.620 --> 01:00:38.580
the weights that they've been learning and they try to match the representation that's coming

01:00:38.580 --> 01:00:43.860
out of the network that they're training to this representation of this moving average network

01:00:44.420 --> 01:00:50.740
and somehow magically that works and I guess it doesn't even have to be a moving average. I think

01:00:50.740 --> 01:00:57.140
you were referring to earlier like you can just match the representation of one network to

01:00:57.860 --> 01:01:01.940
stop gradient of the same network as long as you're matching the representation in an earlier

01:01:01.940 --> 01:01:09.700
layer and I think like it's still like mysterious why that should work. I don't really have any

01:01:09.700 --> 01:01:19.700
insight into how either BYOL or the more recent papers actually are learning a representation

01:01:19.700 --> 01:01:24.580
that doesn't end up collapsing. The problem is if you're trying to match some earlier representation

01:01:25.140 --> 01:01:29.140
you could just collapse to the point where all of your representations are the same and then

01:01:29.700 --> 01:01:36.180
like you would trivially be matching the earlier representation, but this doesn't happen and I

01:01:36.180 --> 01:01:41.300
think why it doesn't happen relates to some mysteries about neural network training dynamics

01:01:41.300 --> 01:01:46.820
that we still don't entirely understand. I'm absolutely fascinated by this concept of data

01:01:46.820 --> 01:01:51.220
augmentation. Early on in my neural network career I just imagined it as being a way of

01:01:51.220 --> 01:01:56.820
increasing the size of your training set, but in a sense you're not really adding new information.

01:01:57.460 --> 01:02:03.380
You are creating semantically equivalent noise perturbations or examples similar to how BERT

01:02:03.380 --> 01:02:08.340
works the NLP model it's like a denoising autoencoder and you're creating noise diversions of the

01:02:08.340 --> 01:02:12.580
same thing and pushing the examples off the manifold. So there seems to be a dichotomy between

01:02:12.580 --> 01:02:16.260
on the one hand augmenting your data and it's almost like you're stopping the neural network

01:02:16.260 --> 01:02:21.780
from overfitting on things like the color or some specific feature you don't want to. You want to

01:02:21.780 --> 01:02:27.140
have a bit of generalization, but at the same time you are saying those things over there it's

01:02:27.140 --> 01:02:31.700
definitely nothing like that. The data augmentation that you need for contrastive learning is

01:02:31.700 --> 01:02:35.780
different from the data augmentation that you need for supervised learning because the task is

01:02:35.780 --> 01:02:41.220
different. When you have contrastive learning you have this problem that if there's just one

01:02:41.220 --> 01:02:47.780
feature in your data that can be used to do the contrastive task to get images of the same example

01:02:47.780 --> 01:02:52.260
or views of the same example close together and far apart from views of all the other examples.

01:02:52.260 --> 01:02:56.340
If you could do that with one feature that would be the only feature the network would

01:02:56.340 --> 01:03:01.460
ever learn or it might be the only feature the network would ever learn. And so with the augmentation

01:03:01.460 --> 01:03:04.820
you're making the task harder so that the network actually has to learn

01:03:05.780 --> 01:03:11.220
many different kinds of features. So I guess we find that this color distortion

01:03:11.220 --> 01:03:15.780
actually is very important for self-supervised learning, for contrastive learning,

01:03:15.780 --> 01:03:21.700
whereas it doesn't really matter for supervised learning. And what we think is going on is that

01:03:21.700 --> 01:03:27.700
if you have two crops from the same image, generally their color histograms are surprisingly

01:03:27.700 --> 01:03:33.940
similar. If you just plot out the intensity histogram of the image you can see that the

01:03:33.940 --> 01:03:39.540
crops came from the same image. And that's a trick that the network is very good at doing

01:03:39.540 --> 01:03:44.100
because I guess if you have ReLU activations they're very good at computing histograms.

01:03:44.820 --> 01:03:50.500
And so by doing the color distortion we basically we don't let the network just learn

01:03:51.140 --> 01:03:55.620
the color histograms in order to do the contrastive task. We force the network

01:03:55.620 --> 01:04:00.660
to actually use other sorts of information and that ends up being like critical to the

01:04:00.660 --> 01:04:04.980
performance of these contrastive methods. Like it basically doesn't work unless you do

01:04:05.700 --> 01:04:10.180
that kind of aggressive color distortion. Because that seems to be the key thing then. So you're

01:04:10.180 --> 01:04:16.580
not telling it to learn things, you're telling it not to learn things. We're telling it to learn

01:04:16.580 --> 01:04:22.180
one thing. We're telling it to learn figure out which views came from the same image. But then,

01:04:22.180 --> 01:04:28.740
yeah, we have to make sure that it learns to do that with a diverse set of features instead

01:04:28.740 --> 01:04:34.180
of just doing it in one way. Because I guess it's like a task that's actually pretty easy to do if

01:04:34.180 --> 01:04:41.060
you don't have this kind of aggressive augmentation. Yeah, I think in a way it helps the network to

01:04:41.060 --> 01:04:46.580
also differentiate what actually what is the thing that differentiates two images. I think

01:04:47.300 --> 01:04:53.380
it helps the network to learn, you know, pick up on that signal. To that end, I also wanted to ask

01:04:53.380 --> 01:04:59.780
for a custom dataset, if I wanted to, you know, apply a sim clear, what pointers should I take

01:04:59.780 --> 01:05:04.820
into consideration while designing my augmentation policy? I'm sure you have been asked about this

01:05:04.820 --> 01:05:09.700
question quite a few times. But yeah, I think it's a good question. Like I think like we actually

01:05:09.700 --> 01:05:14.740
still don't really know how generalizable these contrastive learning techniques are beyond image

01:05:14.740 --> 01:05:20.900
net. Like we know they work super well on image net. But like image net is like a boring dataset to

01:05:20.900 --> 01:05:24.980
apply contrastive learning to because we actually already have all the labels and we could just be

01:05:24.980 --> 01:05:32.660
doing supervised learning. But I think starting with the crop and color augmentation is definitely a

01:05:32.660 --> 01:05:37.780
good idea, at least like for datasets that have color, I guess if you don't have color, then maybe

01:05:37.780 --> 01:05:45.780
think about distorting intensities instead of colors. But beyond that, I think it depends on the

01:05:45.860 --> 01:05:51.460
specific task and what you really want the neural network to pick up out of the dataset.

01:05:52.020 --> 01:05:56.820
I feel like there are probably some sorts of data where I wouldn't really expect

01:05:57.620 --> 01:06:04.340
contrastive learning to work well. So for example, like if you try to do contrastive learning on a

01:06:04.340 --> 01:06:11.620
dataset of medical images where you've just got healthy patients, and then you want to translate

01:06:11.620 --> 01:06:17.620
that to like some sort of dataset of people with some kind of pathology, you might never pick up the

01:06:17.620 --> 01:06:22.900
features that are important for detecting the pathology. But yeah, I think this question of how

01:06:22.900 --> 01:06:27.860
do you design the augmentation, what augmentation works well for datasets that maybe aren't natural

01:06:27.860 --> 01:06:34.020
images like these kinds of medical images or maybe like satellite images. That's an important

01:06:34.020 --> 01:06:39.300
question that we haven't addressed yet. There seems to be this fascinating universality of

01:06:39.300 --> 01:06:43.860
representations, especially in vision. This is exactly the kind of thing you can test with your

01:06:43.860 --> 01:06:49.700
wonderful similarity matrix idea. I'm not trying to be flippant when I say this because

01:06:49.700 --> 01:06:55.300
practitioners have used ImageNet on a variety of downstream tasks. For example, they might use it for

01:06:55.300 --> 01:07:00.020
classifying circuit boards or something. And the miraculous thing is it just seems to work quite

01:07:00.020 --> 01:07:05.780
well. So do you think in your opinion that there is some kind of universality? I'm very skeptical

01:07:05.780 --> 01:07:12.980
about like universality of ImageNet for different tasks. Like in the past, we did some work where

01:07:12.980 --> 01:07:19.380
we looked at how well ImageNet networks transfer to other tasks. And it seems like

01:07:20.180 --> 01:07:27.060
there are actually some tasks which are just like datasets of natural images where pre-training an

01:07:27.060 --> 01:07:33.380
ImageNet doesn't really help at all. And those datasets just seem to be too different from ImageNet.

01:07:33.380 --> 01:07:40.100
They're things like this Stanford cars dataset where you have to like classify different cars

01:07:40.100 --> 01:07:45.140
according to their make, model, and year. It turns out even though there are lots of cars in ImageNet,

01:07:45.140 --> 01:07:50.420
if you pre-train on ImageNet and you fine-tune on that dataset, you will learn to classify it

01:07:50.420 --> 01:07:56.100
faster in fewer steps than if you had trained from scratch on the Stanford cars dataset.

01:07:56.820 --> 01:08:01.700
But you won't actually perform any better at the end. And that's true even though the

01:08:01.780 --> 01:08:08.660
Stanford cars dataset has 10,000 images. So it's tiny compared to ImageNet. So I think like actually

01:08:08.660 --> 01:08:16.020
representations of images are not that universal. And at least what works for natural images for

01:08:16.020 --> 01:08:22.820
images like those in ImageNet may not work on other datasets. I think there's also like limited

01:08:22.820 --> 01:08:29.220
evidence for transfer from ImageNet to medical datasets. It seems if you don't like work really

01:08:29.220 --> 01:08:35.380
hard at tuning hyperparameters or if you don't train for long enough, you will get better accuracy

01:08:35.380 --> 01:08:40.180
by starting with an ImageNet pre-trained network. But if you do very thorough experiments and you

01:08:40.180 --> 01:08:44.820
train for long enough, you try different learning rates and weight decay parameters, like actually

01:08:44.820 --> 01:08:50.500
it seems like training from scratch on most medical datasets will give you the same accuracy as if

01:08:50.500 --> 01:08:56.260
you started from a network that's pre-trained on some other giant dataset. Maybe this makes sense

01:08:56.260 --> 01:09:02.660
because if you think about radiologists, like it's not like a radiologist can just like at the

01:09:02.660 --> 01:09:08.100
beginning of their education, they can't just look at an MRI or an X-ray image and say this is

01:09:08.100 --> 01:09:13.860
where the tumor is. It's something that takes them years of training to learn how to do. And so maybe

01:09:13.860 --> 01:09:19.540
it also makes sense that like our neural networks can't just immediately easily without lots of

01:09:19.540 --> 01:09:27.220
training pick up on very different image distributions. It does seem to make sense,

01:09:27.220 --> 01:09:34.340
going a bit from the universality of representations to the universality of

01:09:34.340 --> 01:09:41.780
augmentation, since this is such a crucial part. Do you think that there is a systematic way how

01:09:41.780 --> 01:09:47.300
we can discover augmentations? Because it seems right now, it seems to be kind of a whack-a-mole,

01:09:47.300 --> 01:09:52.180
right? It's okay, we just feed images and it's no, that's too easy. We crop them. Oh no, it's the

01:09:52.180 --> 01:09:57.620
color histogram. So we like whack on the color and then it works better. But maybe someone finds out

01:09:57.620 --> 01:10:02.900
oh, there is still this easy feature that the network every now and then pays attention to. So

01:10:02.900 --> 01:10:10.020
we design a method to whack on that a bit. Do you think there is a systematic way or will this

01:10:10.020 --> 01:10:16.340
kind of philosophically always rely on us humans having a higher level inside of what we want to

01:10:16.340 --> 01:10:24.100
do with the dataset? Yeah, so I think actually I'm hopeful that at least for natural images,

01:10:24.100 --> 01:10:29.780
just like crops and color distortions are enough, because I guess like what we found is

01:10:29.780 --> 01:10:35.540
you combine those two augmentations and if you do that, like that gets you most of the way to

01:10:35.540 --> 01:10:41.140
supervised accuracy. So maybe we shouldn't expect huge gains from adding additional augmentations on

01:10:41.140 --> 01:10:46.260
top of that, even though there are like in the Sinclair paper, we add like Gaussian blur, which

01:10:46.260 --> 01:10:52.100
gives slight gains on top of that. I guess in the BYL paper, they add even more augmentations on top

01:10:52.100 --> 01:10:58.180
of that. So you can get small gains, but it seems like the gains are much smaller once you've got

01:10:58.180 --> 01:11:03.860
the crops and the color distortions there. In terms of systematic ways of discovering

01:11:04.420 --> 01:11:09.620
what set of augmentations we should be using, I guess there's a paper that I saw where

01:11:10.420 --> 01:11:16.420
they basically use linear evaluation on the rotation prediction task to see whether the

01:11:16.420 --> 01:11:21.860
augmentations are good and they claim that actually works for evaluating the augmentations.

01:11:21.860 --> 01:11:28.340
So maybe that's one way, I don't know. There are all sorts of ways of designing augmentations

01:11:28.340 --> 01:11:35.540
for supervised learning that could conceivably be applied to the self-supervised learning setting,

01:11:35.620 --> 01:11:41.060
to the contrastive setting. There are these meta-learning based approaches for learning

01:11:41.060 --> 01:11:47.700
data augmentation. I'm not sure, those techniques tend to be pretty complicated. I'm not sure whether

01:11:47.700 --> 01:11:53.140
it's actually easier to deal with those techniques than just like trying a bunch of things, but I

01:11:53.140 --> 01:11:59.460
think that maybe it doesn't matter that much. Maybe like just, at least if you're dealing with natural

01:11:59.460 --> 01:12:07.300
images, maybe crop and color distortion is enough. I guess if you think about other images,

01:12:07.300 --> 01:12:11.220
I don't really have any idea. I guess it depends on what the images look like.

01:12:11.940 --> 01:12:17.300
There are lots of things that you could be expressing as images like a spectrogram or

01:12:17.300 --> 01:12:22.420
like some kind of chart or whatever, where you could be applying a neural network to it,

01:12:22.420 --> 01:12:27.380
but the further you get from natural images, the less clear it is what kind of augmentations

01:12:27.460 --> 01:12:32.740
you should be working with. It is a fascinating thought though, this universality of augmentations.

01:12:32.740 --> 01:12:39.780
When you said cropping and color, that made me think it seems to be related to the inductive

01:12:39.780 --> 01:12:45.940
priors in the CNN architecture that we use and also to things like its regularly sampled, gridded,

01:12:46.500 --> 01:12:50.580
discrete image data, because we're speaking with Max Welling the other week and as he's

01:12:50.580 --> 01:12:56.100
created lots of other interesting inductive priors for computer vision models. It does set

01:12:56.180 --> 01:13:00.100
my mind racing a little bit because presumably there's a continuum. On the one hand, we don't do

01:13:00.100 --> 01:13:04.740
any augmentation and we just learn from examples. In the middle, we do the augmentation and then

01:13:04.740 --> 01:13:08.580
maybe in the future, because some people have said that computer vision systems don't have seen

01:13:08.580 --> 01:13:12.900
understanding. They don't understand physics. The ball might be on the table, but we don't know that

01:13:12.900 --> 01:13:17.780
it's not falling and so on. There's a lot of missing information. Would the next step be some

01:13:17.780 --> 01:13:22.420
simulation? Do you know what I mean? Where we impute physics and we impute some world knowledge

01:13:22.420 --> 01:13:24.980
and then I don't know whether we train a machine learning model from that?

01:13:25.620 --> 01:13:31.380
Yeah, I think there are definitely shortcomings in our current machine learning models,

01:13:31.380 --> 01:13:36.660
understandings of the world. There are probably things that we can't just solve by throwing more

01:13:36.660 --> 01:13:43.380
static images at them. I think maybe the next step, rather than trying to immediately situate

01:13:43.380 --> 01:13:49.780
the machine learning model in a simulated world, we could just think about video. I think there's

01:13:49.780 --> 01:13:55.380
already a lot of additional information in video that a neural network could use to learn

01:13:55.380 --> 01:14:02.100
interesting representations. It seems like if you just see static images, it's hard to learn

01:14:02.100 --> 01:14:08.500
how to segment objects. It's hard to learn where the object's boundaries are, but once you have

01:14:08.500 --> 01:14:13.300
video, it's like the stuff that's moving together is an object and you can tell that because it's

01:14:13.300 --> 01:14:18.980
moving together. I think there's a lot of potential for learning better visual representations

01:14:20.740 --> 01:14:26.020
and maybe eventually from these kinds of interactions in simulated environments. I

01:14:26.020 --> 01:14:32.020
think ultimately it becomes a computational headache. Even video is a computational headache

01:14:32.020 --> 01:14:36.820
because suddenly you've got all of these frames that you have to deal with. You probably want to be

01:14:38.260 --> 01:14:45.460
thinking about how representations change over time and video data is just huge. It's especially

01:14:45.460 --> 01:14:53.620
huge if you have to process many frames at once on your accelerators. I think that's why this hasn't

01:14:53.620 --> 01:15:00.100
taken off yet, but I think probably representation learning from video is going to be a big thing

01:15:00.100 --> 01:15:07.780
next year or the year after or sometime in the near future. We would love to talk about your big

01:15:07.780 --> 01:15:12.180
self-supervised models, our strong semi-supervised learners. This is super interesting because

01:15:12.180 --> 01:15:16.020
you're combining the unsupervised stuff that we've been talking about in Simclear, but now

01:15:16.020 --> 01:15:21.540
we're in the semi-supervised domain where the label efficiency becomes super important. What's

01:15:21.540 --> 01:15:28.580
the deal there? Yeah. I guess in Simclear we focus on this question of linear evaluation

01:15:28.580 --> 01:15:34.100
accuracy. We're just learning a representation without any labels and then training a linear

01:15:34.100 --> 01:15:39.700
classifier on top of that representation on the same data, but now with all the labels. It turns

01:15:39.780 --> 01:15:46.260
out that's not really a very practical problem if you have all the labels. There's not necessarily any

01:15:46.260 --> 01:15:52.580
reason in practice that you would want to first learn this representation and then train the

01:15:52.580 --> 01:15:58.660
classifier versus just doing standard supervised end-to-end training. What is a practical problem

01:15:58.660 --> 01:16:05.460
is the situation where you have a lot of unlabeled data and then a very small amount of labeled

01:16:05.460 --> 01:16:14.260
data. That's the situation that we look at in Simclear v2 in that paper. What we find there is

01:16:14.260 --> 01:16:22.180
that you can train this network fully unsupervised without using the labels on all the data and then

01:16:22.180 --> 01:16:27.780
you can fine-tune it on just the subset where you've got the labels. If you do that, it's possible to

01:16:27.780 --> 01:16:35.140
get very high accuracy, especially if the network is very big. Basically, we find if you have a

01:16:35.140 --> 01:16:40.100
really big ResNet, if you have ResNet 152 and then you make the layers three times wider,

01:16:40.660 --> 01:16:47.940
when you do that, you can get accuracy when you fine-tune on 10% of the labels that's substantially

01:16:47.940 --> 01:16:54.020
better than if you trained ResNet 50 from scratch with all the labels. Once you have that really big

01:16:54.020 --> 01:16:58.180
network, it turns out you don't have to put the really big network into production. You can take

01:16:58.180 --> 01:17:04.900
the really big network and you can then distill it back into a standard ResNet 50 and you can retain

01:17:04.900 --> 01:17:09.540
almost all of the accuracy when you do that. I guess what's important about this distillation

01:17:09.540 --> 01:17:14.980
process is we're not just going to distill on the labeled dataset. We're going to also use

01:17:15.860 --> 01:17:22.740
the labels that this giant network, which we fine-tuned on a small subset of the data,

01:17:22.740 --> 01:17:28.180
we're going to use the labels that it gives on all of our unlabeled data. We're going to use it to

01:17:28.180 --> 01:17:33.460
generate labels and then we're just going to use those labels to train a much smaller network. If

01:17:33.460 --> 01:17:39.700
we do that, we get accuracy that's similar to or maybe even slightly better than standard supervised

01:17:39.700 --> 01:17:46.980
training from scratch. This becomes a highly practically relatable approach toward doing

01:17:47.540 --> 01:17:54.420
computer vision related things. We see all the times that folks have a huge corpus of unlabeled

01:17:54.420 --> 01:18:02.020
images, but they only have maybe 5% or 10% labeled images. This immediately becomes a practically

01:18:02.100 --> 01:18:07.460
applicable recipe for them. I definitely am looking forward to seeing this thing

01:18:07.460 --> 01:18:13.300
implemented at scale at different companies. That's there. If I understood it correctly,

01:18:13.300 --> 01:18:20.100
just for the viewers, you folks used a variant of distillation here, which is more popularly

01:18:20.100 --> 01:18:24.340
referred to as self-training. I don't think there's really a difference between

01:18:25.140 --> 01:18:31.380
what we call distillation and what other people call self-training. I guess the idea is basically

01:18:31.460 --> 01:18:37.540
we will pass information into the network. We get its output probabilities and then we train

01:18:37.540 --> 01:18:41.380
another neural network with those output probabilities as the targets.

01:18:42.180 --> 01:18:48.660
What I find fascinating is how many ideas come together in this paper. There's first,

01:18:49.380 --> 01:18:56.100
there's this, let's do representation learning and then we have these just small labels. We find

01:18:56.100 --> 01:19:02.580
tune and then there's big networks, small networks. Then you label, but you also apply

01:19:02.580 --> 01:19:09.060
some noise, if I understand correctly, in the process of transferring. Maybe I'm misremembering

01:19:09.060 --> 01:19:15.060
that, but there's a lot of ideas that come together. Yannick, you probably confused it with

01:19:15.060 --> 01:19:20.260
noisy student training, where they impose noise during the student training.

01:19:20.820 --> 01:19:27.060
Sorry, maybe not, but there is a lot of ideas that come together. Something tells me that

01:19:27.060 --> 01:19:35.300
there was a process behind going, you probably didn't sit down after SimClear1 and be like,

01:19:35.300 --> 01:19:40.100
all right, what do we do for SimClear2? Okay, let's do this. It tells me there was this process.

01:19:40.740 --> 01:19:48.020
If you can maybe elaborate a bit on how did you going to build up the system towards the final

01:19:48.020 --> 01:19:54.420
output? Was there dead ends or was it like, let's build up until we can no longer make it better?

01:19:54.420 --> 01:20:03.460
How was that? Yeah, I guess there was a bit of a process. After the original SimClear paper,

01:20:03.460 --> 01:20:08.340
I guess it's clear from the SimClear paper that when we have this bigger network, we get much

01:20:08.340 --> 01:20:16.020
higher linear evaluation accuracy than we do if we just train SimClear with a ResNet50. Then the

01:20:16.020 --> 01:20:22.740
question was, is there some way that we can somehow eliminate this dependence on this giant

01:20:22.740 --> 01:20:27.140
network? Because the giant network is annoying to work with, it's computationally expensive,

01:20:27.140 --> 01:20:35.220
it's big. We first tried what happens if you distill the unsupervised network. We basically

01:20:35.220 --> 01:20:42.020
have this task that is set up as a form of cross entropy loss when we're doing the contrast of

01:20:42.020 --> 01:20:50.420
learning. You can also think about distilling on that task where you have a probability distribution

01:20:50.420 --> 01:20:55.060
that corresponds to the similarity between an image and all the other images. You could use

01:20:55.060 --> 01:21:00.900
those kinds of targets to distill. We tried that and that kind of worked. Then we also tried the

01:21:00.900 --> 01:21:06.180
approach of first fine-tuning the big network and then distilling it. It turned out that worked

01:21:06.260 --> 01:21:14.580
a lot better. I guess we jumped straight to distillation because we knew that we could get

01:21:14.580 --> 01:21:21.620
much better results by using a giant network with SimClear. Then once you realize that distillation

01:21:21.620 --> 01:21:26.740
is going to be important, the only thing you've got to figure out is what kind of distillation

01:21:26.740 --> 01:21:31.940
should you be doing. What we found was this approach of pre-training, then fine-tuning,

01:21:31.940 --> 01:21:37.060
then distilling works a lot better than pre-training, then distilling, then fine-tuning.

01:21:37.620 --> 01:21:45.860
How far down do you think one can go with this final distillation step? Is this something that is

01:21:45.860 --> 01:21:51.860
conceivably going to be available on, let's say, edge devices at some point, like that

01:21:52.980 --> 01:21:59.140
our glasses or something run with similar accuracies to these giant networks? Or

01:22:00.020 --> 01:22:03.700
is it more a factor of four, a factor of 10 kind of stuff?

01:22:04.660 --> 01:22:10.980
I think there are clearly some limits to distillation. I guess we probably shouldn't

01:22:10.980 --> 01:22:16.980
expect distillation of the kind that we do in SimClear v2 to work substantially better than

01:22:17.620 --> 01:22:23.780
supervised distillation, which has been around for quite a while now. I think what's impressive is

01:22:23.860 --> 01:22:30.260
that in the self-supervised case, in the contrastive case, distillation basically allows you to

01:22:30.260 --> 01:22:35.220
recover the same accuracy that you would get from training supervised from scratch, whereas without

01:22:35.220 --> 01:22:42.340
it, the accuracy is a lot worse. It seems like it maybe matters more in this contrastive case,

01:22:42.980 --> 01:22:48.820
but I think generally when you do distillation in the supervised case, you can get maybe a

01:22:48.820 --> 01:22:54.980
percentage point gain, maybe a couple of percentage points. I think that's probably about the limit

01:22:54.980 --> 01:23:00.900
in terms of the improvement that you could get from any kind of distillation-based approach

01:23:00.900 --> 01:23:06.900
over supervised training from scratch. Fascinating. I don't know if you know that we've been playing

01:23:06.900 --> 01:23:11.860
with GPT-3 and you said something quite interesting just now. You said that they're deterministic,

01:23:11.860 --> 01:23:16.020
but in GPT-3, that's not really the case. If you sample from it deterministically,

01:23:16.020 --> 01:23:23.220
it gets stuck in cycles. You have to do some kind of trickery, some kind of random sampling from

01:23:23.220 --> 01:23:28.740
this distribution. It might be the case in future computer vision models as well, that we have to

01:23:28.740 --> 01:23:32.740
randomly sample from them in some way, because otherwise it would get into some pathological

01:23:32.740 --> 01:23:37.780
behavior. Maybe to do that, we need to have some kind of controller on the top. I suppose my question

01:23:37.780 --> 01:23:42.260
is in the future, maybe we will be in the stochastic regime. What do you think about that?

01:23:43.140 --> 01:23:49.620
With GPT-3, you're trying to generate data. When you generate data, there has to be some

01:23:49.620 --> 01:23:54.500
kind of noise that's coming from somewhere. The process of generating data is like turning

01:23:54.500 --> 01:24:00.980
noise into data. For image classification, we have the data and we just want to turn it into

01:24:00.980 --> 01:24:07.380
a label. Maybe there's this implicit notion of stochasticity in that the network gives some

01:24:07.860 --> 01:24:13.540
output distribution. I think we still want everything to be deterministic if it can be

01:24:13.540 --> 01:24:19.620
deterministic. We basically want the network to say, this is a dog with probability x.

01:24:20.340 --> 01:24:25.860
If there's some way to improve it with stochasticity, I don't know. I guess dropout used to be very

01:24:25.860 --> 01:24:30.980
popular, but it seems like it's not so popular anymore and it doesn't really help us very much

01:24:30.980 --> 01:24:37.860
with vision models. Also, even dropout is generally only used when we train the neural

01:24:37.860 --> 01:24:46.260
networks. On the other hand, the brain is very stochastic. The brain has lots of noise. I guess

01:24:46.260 --> 01:24:50.900
that suggests that maybe there's some way to leverage noise to learn better representations

01:24:50.900 --> 01:24:54.340
in neural networks as well and we just don't quite know the right way yet.

01:24:54.900 --> 01:24:59.140
That's right. Max Welling said to us that he thinks that the future of AI will have a

01:24:59.140 --> 01:25:04.660
generative component. He thinks that we have the matrix in our minds. We have these simulations

01:25:04.660 --> 01:25:11.220
going on all the time and we're generating new scenarios. It's related to the data

01:25:11.220 --> 01:25:16.340
augmentation thing as well. Some people have said to me in the past that using a GAN might be a way

01:25:16.340 --> 01:25:21.940
of doing data augmentation. Presumably, that would require some kind of stochastic sampling as well.

01:25:21.940 --> 01:25:25.380
I suppose it's just quite interesting to see where these two things might meet in the middle at

01:25:25.380 --> 01:25:30.900
some point. Max Yeah. I don't know. I guess like with a GAN, using a GAN to do data augmentation,

01:25:30.900 --> 01:25:35.940
you have this problem that you still don't actually have more data. You have a GAN that's

01:25:35.940 --> 01:25:42.340
trained on the same data. It might help you because your way of encoding inductive bias into the GAN

01:25:42.340 --> 01:25:48.100
is different from your way of encoding inductive bias into the neural network. Maybe by having

01:25:48.100 --> 01:25:54.500
more inductive bias, you can learn a better function. You still don't have more data at it.

01:25:55.300 --> 01:26:01.300
Without having more data, there's no reason to expect a priority that you will be able to learn

01:26:01.300 --> 01:26:06.340
a better function. Paul I'm so glad you said that. It was always my intuition. The amount of people

01:26:06.340 --> 01:26:09.700
that have said to me that you should use a GAN for data augmentation. Anyway.

01:26:09.700 --> 01:26:14.660
Max What's the first thing you think about if you're like, oh, what could I use a GAN for?

01:26:14.660 --> 01:26:20.020
And then you learn about data, and they're like, wait, this is so much more data. Yeah,

01:26:20.020 --> 01:26:26.020
but conceptually, yes, you don't have more data. And ironically, when you do the simple

01:26:26.020 --> 01:26:31.780
data augmentation, you do have more data because you put all the knowledge in there

01:26:31.780 --> 01:26:39.060
as a human of what makes two images dissimilar visually, but still equivalent semantically,

01:26:39.060 --> 01:26:45.140
which again, is exactly the opposite. It gives you images that are visually similar,

01:26:45.140 --> 01:26:51.700
but it has no intuition of what the semantic similarity is. For my last question, I actually

01:26:51.700 --> 01:26:57.460
want to switch topics just a little bit to what Tim said at the beginning, namely your love of Julia.

01:26:58.260 --> 01:27:05.540
So I have seen a number of especially data scientists be strong advocates for Julia as a

01:27:05.540 --> 01:27:12.180
language and so on. Do you want to give anyone sort of the pitch? Why should we even consider this?

01:27:13.060 --> 01:27:19.300
Yeah, so I think Julia is a much better programming language than Python in many ways.

01:27:19.940 --> 01:27:24.980
I guess one thing and I guess the thing that first attracted me to Julia is that it's really

01:27:24.980 --> 01:27:30.820
fast, like you can write Julia code and with very little work, you will end up running as

01:27:30.820 --> 01:27:36.820
fast as equivalent C code. So that's something that you can't get out of standard Python code.

01:27:36.820 --> 01:27:41.300
If you're just writing a for loop in Python, it's going to be super slow. But in Julia,

01:27:41.300 --> 01:27:44.900
you don't have to worry about all of that. And you don't have to worry about like Cython or

01:27:44.900 --> 01:27:49.860
number all of this other stuff that people have hacked on top of Python. Julia is just

01:27:49.860 --> 01:27:57.620
designed to be fast and it works. I think there are other advantages to Julia as a language

01:27:57.620 --> 01:28:04.820
beyond that. I guess it's built on this idea of generic functions where you have a function that

01:28:05.860 --> 01:28:10.100
can take multiple types and you can define the function differently for the different types.

01:28:10.660 --> 01:28:15.220
And this is something that we do all the time when we're doing like machine learning, like we

01:28:15.220 --> 01:28:20.500
have matrix multiplication, which is a different form of multiplication that takes matrices and

01:28:20.500 --> 01:28:28.020
produces something. And I guess like in Python, it's so it used to be that you had to type dot dot

01:28:29.300 --> 01:28:34.980
to multiply things, but now it's like they have this add symbol that does matrix multiplication.

01:28:34.980 --> 01:28:39.700
But Julia is designed for these situations where maybe beyond just matrices,

01:28:39.700 --> 01:28:44.180
you have these funny types of structured matrices, you have sparse matrices, and you can define

01:28:44.180 --> 01:28:49.860
special methods for the product of a sparse matrix and a vector or all sorts of things where you

01:28:49.860 --> 01:28:55.940
might want different methods depending on the types. And even though it seems like this is

01:28:55.940 --> 01:29:00.900
complicated and you might have some trouble picking which version of the function is going to be called

01:29:00.900 --> 01:29:08.980
at runtime, because Julia is ultimately compiling everything when you call it. And because it has

01:29:08.980 --> 01:29:16.580
this kind of strong type system, it can ultimately pick which method is going to be used and compile

01:29:16.580 --> 01:29:21.620
that method call in and you don't have to worry about picking which one. And so it still ends up

01:29:21.620 --> 01:29:29.620
being fast. I also think like there's this question of whether like the object oriented Python or

01:29:29.620 --> 01:29:36.900
the object oriented paradigm and Python is really like the best paradigm for machine learning.

01:29:36.900 --> 01:29:42.980
Because I guess like it's like we have data and then we have functions that operate on the data.

01:29:42.980 --> 01:29:46.740
But in the object oriented paradigm, you want the functions that operate on the data to be

01:29:46.740 --> 01:29:52.820
attached to the data, which is like a weird way of setting things up. And that Julia is not set up

01:29:52.820 --> 01:29:58.820
that way. You have these data structures. And then because you are able to create functions that

01:29:58.820 --> 01:30:04.260
specialize on the data structures, you don't have to worry about attaching those functions to the

01:30:04.340 --> 01:30:10.660
data structures themselves. Amazing. Dr. Simon Cornblith. Thank you very much for joining us

01:30:10.660 --> 01:30:15.780
this evening. It's been an absolute pleasure. Thanks for having me. Thank you. I really hope

01:30:15.780 --> 01:30:21.140
you've enjoyed the show today. We've had so much fun making it. Remember to like, comment,

01:30:21.140 --> 01:30:26.180
and subscribe. We love reading your comments, every single one of them. And we'll see you back

01:30:26.180 --> 01:30:29.060
next week.

