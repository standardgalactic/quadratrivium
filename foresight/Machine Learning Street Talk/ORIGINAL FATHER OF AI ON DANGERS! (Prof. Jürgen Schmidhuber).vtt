WEBVTT

00:00.000 --> 00:08.400
my fondest memory. It's usually when I discover something that I think nobody has seen before,

00:09.440 --> 00:15.760
but that happens very rarely because most of the things you think of somebody else has done before.

00:16.400 --> 00:21.840
This episode is sponsored by Numeri. Are you a data scientist looking to make a real-world impact

00:21.840 --> 00:28.000
with your skills? Do you love competing against the best minds in the world? Well, introducing

00:28.000 --> 00:34.400
Numeri, the revolutionary cutting-edge AI-driven hedge fund that's changing the game for good.

00:34.400 --> 00:40.320
Numeri combines a competitive data science tournament with powerful, clean stock market data

00:40.320 --> 00:45.200
enabling you to predict the market like never before. Sign up now, become part of the elite

00:45.200 --> 00:49.680
community, taking the stock market by storm and I'll see you on the leaderboard.

00:49.760 --> 00:59.360
Wonderful. So today is a momentous occasion. What an episode of MLST we're going to have.

00:59.360 --> 01:05.920
We're joined not by a godfather of AI, but the father of AI, you again, Schmidhuber,

01:05.920 --> 01:11.360
the researcher responsible for leading the research groups which invented much of the technology

01:11.360 --> 01:16.080
which has powered the deep learning revolution. It's long been a dream to get you on the podcast,

01:16.160 --> 01:20.960
you again. It feels like the day has finally arrived, so welcome to MLST.

01:20.960 --> 01:26.560
Thank you, Tim, for these very kind words and this very generous introduction.

01:29.840 --> 01:34.800
So on that, let's discuss the credit assignment problem in machine learning. Now,

01:34.800 --> 01:40.800
you've dedicated a significant amount of time researching and publishing the actual history

01:40.800 --> 01:45.360
of the field and there's a significant divergence between the public narrative

01:45.360 --> 01:50.960
and what actually happened. And amazingly, no one has pointed out any factual inaccuracies in your

01:50.960 --> 01:56.400
accounts, but the incorrect perceptions still persevere. Now, I particularly enjoyed reading

01:56.400 --> 02:00.800
your history of the breakthroughs in machine learning, going back to ancient times and of course even

02:00.800 --> 02:06.080
remarking on the very first computer scientist, Leibniz. And for example, you pointed out the

02:06.080 --> 02:11.440
history of who invented backprop and the CNN. And you explained that there wasn't really

02:11.440 --> 02:16.400
a neural network winter at all in the 1970s. So could you just sketch out a little bit of that

02:16.400 --> 02:30.400
history? So that's a challenge. Actually, computer science history and computing history started

02:30.400 --> 02:40.560
maybe 2000 years ago when Heron of Alexandria built the first program-controlled machine.

02:40.560 --> 02:49.760
That was 2000 years ago in the first century basically. And he basically built an automaton

02:49.760 --> 02:58.720
that was programmed through a cable which was wrapped around a rotating cylinder which had

02:58.720 --> 03:06.480
certain knobs and then there was a weight which pulled it down and the whole apparatus

03:06.480 --> 03:14.880
was able to direct the movements of little robots, of little puppets in an automatic theater.

03:16.000 --> 03:24.800
That, as far as I know, was the first program-controlled machine in the history of mankind.

03:24.800 --> 03:31.920
Even before that there were other machines. The ancient Greeks had even earlier the

03:32.720 --> 03:40.880
Antiqueterre mechanism which was kind of a clock, an astronomical clock. But then more recently

03:43.200 --> 03:49.680
we have seen many additional advances and you mentioned Leibniz, of course, who is of

03:49.680 --> 03:56.320
special interest to our field because he not only is called the first computer scientist

03:56.320 --> 04:05.280
because he had the first machine with a memory that was in the 1680s, I think. He not only had the

04:07.600 --> 04:15.200
first machine that could do all the basic arithmetic operations which are addition,

04:15.200 --> 04:25.920
multiplication, division and subtraction, then he not only had these first ideas for a universal

04:25.920 --> 04:32.240
problem solver that would solve all kinds of questions, even philosophical questions,

04:32.880 --> 04:42.000
just through computation. And he not only was the first who had this algebra of thought which

04:42.000 --> 04:51.040
is deductively equivalent to the much later Boolean algebra. In many ways he was a pioneer,

04:51.040 --> 04:56.240
but especially in our field in deep learning he contributed something essential, which is really

04:56.240 --> 05:04.640
central for this field, which is the chain rule. I think 1676, that's when he published that and

05:04.640 --> 05:15.440
that's what is now being used to train very deep artificial neural networks and also shallow

05:15.440 --> 05:21.920
neural networks and recurrent neural networks. And everything that we are using in modern AI

05:21.920 --> 05:30.000
is really in many ways depending on that early work. But then of course there was so much additional

05:30.000 --> 05:40.640
work. The first neural networks, as we know them, they came up about around 1800. That's when Gauss

05:40.640 --> 05:49.680
and Legendre had the linear neural networks, the linear perceptrons in the sense that they were

05:49.680 --> 05:59.920
linear without having any non-differential aspect to it. So these first neural networks,

06:00.560 --> 06:12.640
back then, were called method of least squares. And the training method was regression and the

06:12.640 --> 06:18.080
error function was exactly the same that we use today. And it was basically just a network with

06:18.080 --> 06:24.560
a set of inputs and a set of outputs and a linear mapping from the inputs to the outputs. And you

06:24.560 --> 06:32.960
could learn to adjust the weights of these connections. So that was the first linear neural

06:32.960 --> 06:42.800
network and many additional later developments led to what we have today. You had this beautiful

06:42.800 --> 06:47.280
statement. You said that machine learning is the science of credit assignment and we should apply

06:47.920 --> 06:53.520
that same science to the field itself. And I guess what I'm really curious about is

06:54.240 --> 06:59.280
first, if you could educate our listeners just a bit on what credit assignment is in the context

06:59.280 --> 07:04.800
of, say, machine learning and why you think it's important that that should apply to the field

07:04.800 --> 07:09.040
in general. You know, why should we care about credit assignment? Why should we study the history

07:09.040 --> 07:16.560
of the developments in the field? Why is it important? I'm interested in credit assignment,

07:17.200 --> 07:22.320
not only in machine learning, but also in the history of machine learning,

07:23.120 --> 07:31.200
because machine learning itself is the science of credit assignment. What does that mean? Suppose

07:31.200 --> 07:39.760
you have a complicated machine, which is influencing the world in a way that leads to the solution

07:39.760 --> 07:45.200
of a problem. And maybe the machine solves the problem. But then the big question is,

07:45.200 --> 07:52.720
which of the components of these many components were responsible? Some of them were active

07:52.720 --> 08:01.760
a long time ago and others later and early actions set the stage for later actions. Now,

08:02.960 --> 08:06.880
if you want to improve the performance of the machine, you should figure out how

08:07.600 --> 08:16.000
did the components contribute to the overall success. And this is what credit assignment is

08:16.000 --> 08:22.000
about. And in machine learning in general, we have a system consisting of many

08:24.000 --> 08:32.640
machine learning engineers and mathematicians and hardware builders and all kinds of people.

08:32.640 --> 08:38.560
And there you also would like to figure out which parts of the system are responsible for later

08:38.560 --> 08:44.800
successes. Yeah, and it's a brilliant point. And I completely agree with you, by the way.

08:44.800 --> 08:50.960
And I think the way I think about it is you've got this giant architecture of humanity and in it

08:50.960 --> 08:55.760
are these certain nodes that may be an individual, maybe a research group. And if they come up with

08:55.760 --> 09:01.600
things that are very helpful, right, you want to try and direct more attention, more resources,

09:02.000 --> 09:08.960
at that nodule, at that node, right, because it's likely to come up with additional very

09:08.960 --> 09:14.080
important things. And if we don't get that right, we're just not optimizing the algorithm of science

09:14.080 --> 09:24.160
as a whole. That's right, yes. Machine learning and science in general is based on this principle

09:24.160 --> 09:31.360
of credit assignment where credit usually doesn't come in form of money, sometimes also in form

09:31.360 --> 09:41.440
of money, but in form of reputation. And then the whole system is set up such that you create

09:41.440 --> 09:52.080
an incentive for people who have worked on improving some method to credit those who

09:52.080 --> 09:59.200
maybe came up with the original method and to just have these chains of credit assignment

09:59.840 --> 10:07.280
that make clear who did what, when, because the whole system is based on this incentive.

10:07.280 --> 10:16.480
And yes, those who are then credited with certain valuable contributions, they also can get

10:16.480 --> 10:23.360
reasonable jobs within the economy and so on. But that's more like the secondary

10:23.760 --> 10:31.920
consequence of the basic principle. And that's why all PhD advisors

10:34.080 --> 10:41.120
teach their PhD students to be meticulous when it comes to credit assignment to past work.

10:42.320 --> 10:48.560
So one last question, if I may, I've really enjoyed studying the history of advancement

10:49.520 --> 10:54.320
because I found that when I go back and read original source materials, let's say

10:54.960 --> 11:01.200
Einstein's first paper on diffusion or anything like that, because they're breaking new ground,

11:01.200 --> 11:08.320
they're considering a wider array of possibilities. And then over time, the field becomes more and

11:08.320 --> 11:14.480
more focused on a narrower avenue of that. And you can go back and look at the original work

11:14.560 --> 11:19.120
and actually gain a lot of inspiration for alternative approaches or alternative

11:19.680 --> 11:25.440
considerations. So in a sense, it's kind of in the sense of forgetting is as important as learning.

11:25.440 --> 11:30.080
Sometimes we need to go back to go down a different branch of the tree, if you will,

11:30.080 --> 11:35.680
and expand the breadth of the search a little bit. I'm curious if you've noticed that same phenomenon.

11:35.840 --> 11:47.440
Yes, science in general is about failure. And 99% of all scientific activity is about

11:49.200 --> 11:52.080
creating failures. But then you learn from these

11:54.240 --> 12:00.080
failures and you do backtracking. And you go back to a previous decision point where you maybe

12:01.040 --> 12:07.680
made the wrong decision and pursued the wrong avenue. But now you have a branching point and

12:07.680 --> 12:17.360
you pursue an alternative. And in a field that is rapidly moving forward, you don't go back very

12:17.360 --> 12:23.440
far usually. You just go back to a recent paper which came out five months ago. And maybe you

12:23.440 --> 12:28.640
have a little improvement there. And then maybe there's yet another little improvement there.

12:29.200 --> 12:33.520
And some parts of our field are at the moment a little bit like that,

12:33.520 --> 12:40.320
where PhD students are moving in, who just look at the most recent papers and then find a way of

12:40.320 --> 12:49.440
improving it a little bit and 2% better results on this particular benchmark. And then the same guys

12:49.440 --> 12:56.320
are also reviewing at major conferences, papers by similar students and so on. And so then sometimes

12:56.320 --> 13:05.680
what happens is that no very deep backtracking is happening, just because the actors aren't really

13:06.720 --> 13:13.280
aware of the entire search tree that has already been explored in the past.

13:14.240 --> 13:22.320
On the other hand, science has this way of healing itself. And since you can gain reputation by

13:22.720 --> 13:32.080
identifying maybe more relevant points, branching points, you have this incentive within the whole

13:32.080 --> 13:41.360
system to improve things as much as you can, sometimes by going back much further.

13:41.920 --> 13:52.080
So there's been a lot of discussion in the discourse around this concept of AI existential

13:52.080 --> 13:57.920
risk. And you again, you've published quite a few pieces about this recently, prominently in

13:57.920 --> 14:04.160
The Guardian and in Forbes actually. And one of the things I wanted to focus on is this concept

14:04.160 --> 14:10.800
of recursive self-improvement, because that seems to be one of the plausible explanations that these

14:10.800 --> 14:15.840
folks give. And of course, when it comes to recursive self-improvement, you are an expert in

14:15.840 --> 14:23.200
this field. I mean, Godel machines come to mind immediately. So I want to kind of explore asymptotes

14:23.200 --> 14:31.200
and limitations. This whole idea of recursive self-improvement is very sexy, isn't it?

14:31.200 --> 14:44.240
In fact, it is the one idea that motivated me to do all of this. So my first paper ever in 1987,

14:44.240 --> 14:50.480
that was my diploma thesis. And it was about this recursive self-improvement thing. So it was about

14:50.480 --> 14:58.880
machine that learns something in a domain. But not only that, it also learns on top of that to

14:59.680 --> 15:08.400
learn a better learning algorithm based on experience and the lower level domains. And then

15:08.400 --> 15:18.400
also recursively learns to improve the way it improves the way it learns. And then also recursively

15:18.960 --> 15:26.000
learns to improve the way it improves the way it improves the way it learns. And yeah, I called that

15:26.000 --> 15:35.440
meta-learning. And back then, I had this hierarchy with, in principle, infinite self-improvement

15:35.440 --> 15:43.440
in the recursive way, although it is always limited by the limited time that you run the system like

15:43.440 --> 15:54.080
that. And then, of course, the motivation behind that is that you don't want to have an artificial

15:54.080 --> 16:01.760
system that is stuck always with the same old human-designed learning algorithm. No, you want

16:01.760 --> 16:09.200
something that improves that learning algorithm without any limitations, except for the limitations

16:09.200 --> 16:20.160
of physics and computability. And so much of what I have been doing since then is really about that.

16:20.240 --> 16:27.040
Self-improvement in different settings where you have, on the one hand, reinforcement learning

16:27.040 --> 16:35.440
systems that learn in an environment to better interact and better create ways of learning

16:35.440 --> 16:46.880
from these interactions to learn faster and to learn to improve the way of learning faster,

16:46.960 --> 16:54.320
and so on. And then also gradient-based systems, artificial neural networks, that learn through

16:55.520 --> 17:02.480
gradient descent, which is a pre-wired human-designed learning algorithm, to come up with a better

17:02.480 --> 17:10.640
learning algorithm that works better in a given set of environments than the original human-designed

17:10.640 --> 17:19.440
one. And yeah, that started around 1992 neural networks that learned to run their own learning

17:19.440 --> 17:29.040
algorithms on the recurrent network themselves. So you have a network which has standard connections

17:29.040 --> 17:34.640
and input units and output units, but then you have these special output units which are used to

17:34.640 --> 17:43.360
address connections within the system, within this recurrent network, and they can read and

17:43.360 --> 17:50.480
write them. And suddenly, because it's a recurrent network and therefore it is a general-purpose

17:50.480 --> 18:00.720
computer, suddenly you can run arbitrary algorithms on this recurrent network, including arbitrary

18:00.720 --> 18:06.560
learning algorithms that translate incoming signals, not only the input signals, but also the

18:06.560 --> 18:14.640
evaluation signals like reinforcement signals or error signals into weight changes, fast weight

18:14.640 --> 18:24.000
changes, where the weight changes are not dictated any longer through this gradient descent method,

18:24.080 --> 18:31.840
but no, now the network itself is learning to do that. But the initial weight matrix is still

18:33.200 --> 18:37.600
learned through gradient descent, which is propagating through all these self-referential

18:37.600 --> 18:45.520
dynamics in a way that improves the learning algorithm running on the network itself. That

18:45.520 --> 18:52.080
was 1992, and back then, compute was really, really slow, it was a million times more expensive

18:52.160 --> 18:58.080
than today, and you couldn't do much with it. But now, in recent works, all of that is working

18:58.080 --> 19:05.760
out really nicely and has become popular, and we have, just if you look at the past few years,

19:05.760 --> 19:12.880
a whole series of papers just on that. So that's the fast weight programming that you're referring

19:12.880 --> 19:21.680
to? Yes, so it's fast weight programmers where you have a part of the network that

19:21.680 --> 19:30.080
learns to quickly reprogram another part of the network, or the original version of that was

19:30.080 --> 19:35.040
actually two networks, so one is a slow network, and then there's another one, a fast network,

19:35.040 --> 19:42.560
and the slow network learns to generate weight changes for the second network,

19:43.360 --> 19:50.880
and the program of the second network are its weights. So the weight matrix of the second

19:50.880 --> 19:56.080
network, that is the program of the second network, and the first one, what does it do? It

19:56.800 --> 20:04.720
generates outputs, it learns to generate outputs that cause weight changes in the second network,

20:04.720 --> 20:09.840
and these weight changes are being applied to patterns, to input patterns, to queries, for

20:09.840 --> 20:19.200
example, and then the first network essentially learns to program the second network, and essentially

20:19.200 --> 20:25.440
the first network has a learning algorithm for the second network, and the first system of that

20:25.440 --> 20:34.560
kind, 1991, that was really based on on keys and values, so the first network learns to program

20:34.560 --> 20:41.040
the second network by giving it keys and values, and it says now take second network, take this

20:41.040 --> 20:48.960
key and this value, and associate both of them through an outer product, which just means that

20:48.960 --> 20:55.600
those units are strongly active, they get connected through stronger connections, and

20:56.880 --> 21:02.320
the mathematical way of describing that is the outer product between key and value.

21:03.840 --> 21:08.480
So that's how the first network would program the second network, and the important thing was that

21:08.480 --> 21:15.520
the first network had to invent good keys and good values, depending on the context of the input

21:15.520 --> 21:23.360
stream coming in, so it used the context to generate what is today called an attention mapping,

21:23.360 --> 21:32.640
which is then being applied to queries, and this was a first step right before the most general

21:34.080 --> 21:40.400
next step, which is then really about learning a learning algorithm running on the network itself

21:40.400 --> 21:43.600
for the weights of the network itself.

21:46.880 --> 21:53.520
Could I press you a tiny bit on this concept of meta-learning and convergence and asymptotes?

21:53.520 --> 22:00.240
Now one of the reasons I think why the X-Risk people believe that it will just go on forever

22:00.240 --> 22:06.080
is they believe in this idea of a pure intelligence, one that doesn't have physical limitations in

22:06.080 --> 22:12.400
the real world, and I'm quite amenable to this ecological idea of intelligence that it does,

22:12.400 --> 22:16.400
the world is a computer basically as well as the actual brain that we're building,

22:17.440 --> 22:24.000
so surely it must hit some kind of asymptote. Do you have any intuition on what those limitations

22:24.000 --> 22:35.120
would be? So you are talking about the ongoing acceleration of computing power and limitations

22:35.200 --> 22:41.840
thereof, is that what you have in mind here? Well that's one part of it, so even if you

22:41.840 --> 22:46.720
just scale transformers I think there would be some kind of asymptote, but we're talking here

22:46.720 --> 22:52.560
about meta-learning, learning to learn, how to learn, and recursive self-improvement, and it's

22:52.560 --> 22:57.680
similar to this idea of reflection, self-reflection and language models, it actually improves the

22:57.680 --> 23:03.360
performance with successive steps of reflection and then it levels off, it reaches an asymptote.

23:03.520 --> 23:07.920
I just believe that there are asymptotes everywhere and that's the reason why I

23:07.920 --> 23:11.680
don't think recursive self-improvement will go on forever, but I just wondered if you had

23:11.680 --> 23:15.920
any intuitions on what those impressions are. Yeah, you are totally right, there are certain

23:17.200 --> 23:24.320
algorithms that we have discovered in past decades which are already optimal in a way

23:25.440 --> 23:31.840
such that you cannot really improve them any further, and no self-improvement and no fancy

23:32.080 --> 23:39.360
machine will ever be able to further improve them. There are certain sorting algorithms that

23:39.360 --> 23:46.720
under given limitations are optimal and you can further improve them. That's one of the limits.

23:46.720 --> 23:53.920
Then of course there are the fundamental limitations of what's computable, first identified by

23:53.920 --> 24:03.040
Kurt Gödel in 1931, he just showed that there are certain things that no computational process

24:03.600 --> 24:13.600
can ever achieve. No computational theorem prover can prove or disprove certain theorems

24:13.600 --> 24:22.720
in a language, in a symbolic language that is powerful enough to encode

24:22.720 --> 24:29.760
certain simple principles of arithmetic and stuff like that. What he showed was that

24:31.040 --> 24:36.800
there are fundamental limitations to all of computation and therefore there are fundamental

24:36.800 --> 24:47.280
limitations to any AI based on computation. I'm glad you brought that topic up because it's one of

24:48.240 --> 24:54.800
our favorite things to discuss which is do you think the human mind ultimately reduces to just

24:54.800 --> 25:00.080
an effective computation and so subject to those same limits or do you think there's any

25:01.840 --> 25:07.920
known or unknown physics that give us some out in which the brain can do a computation that

25:07.920 --> 25:18.720
amounts to hypercomputation? Since we have no evidence that the brain can compute something

25:19.280 --> 25:27.520
that is not computable in the traditional sense, in Gödel sense and torings and churches sense

25:27.520 --> 25:36.480
and everybody who has worked on this field, since we have no evidence we shouldn't assume that's the

25:36.560 --> 25:44.880
case. As soon as someone shows that people can compute certain things or prove certain theorems

25:44.880 --> 25:55.840
that machines cannot prove given the same initial conditions, we should look more closely but

25:56.960 --> 26:04.080
there are many things that might be possible in fairy tales and we are not really exploring them

26:04.080 --> 26:12.240
because the probability of coming up with interesting results is so low. Fair enough,

26:12.240 --> 26:18.000
so you mentioned so far two asymptotes, one being of the mathematical kind where there's just

26:18.560 --> 26:22.880
mathematical proofs that certain things are optimal, the other one being the limits of

26:22.880 --> 26:29.680
computation itself. What other asymptotes do you see applying to or putting bounds on recursive

26:29.680 --> 26:42.080
self-improvement? The most obvious thing is probably light speed and the limits of physical

26:42.080 --> 26:51.760
computation. We know those for several decades, we have happily enjoyed the fact that every five

26:51.760 --> 27:00.960
years compute is getting 10 times cheaper and this process started long before Moore's law was

27:02.960 --> 27:11.360
defined in the 60s I believe because even in 1941 already when Susie built the first program

27:11.360 --> 27:19.360
controlled computer this law apparently was active so back then he could compute maybe one

27:19.360 --> 27:25.200
instruction per second and since then every 10 years a factor of 100 every 30 years a factor of

27:25.200 --> 27:35.520
a million more or less until today and there's no reason to believe it won't hold for a couple

27:35.520 --> 27:43.440
of additional decades because the physical limits are much further out. The physical limits that we

27:43.440 --> 27:53.120
know are the Bremermann limit discovered I think in 1983 by Bremermann and they basically say that

27:53.120 --> 27:58.640
one kilogram of matter cannot compute more than 10 to the 51 instructions per second.

28:00.080 --> 28:07.360
So that's a lot of compute but it's limited and to give you an idea of how much compute that is

28:08.000 --> 28:16.240
I also have a kilogram of computer in here and probably it cannot compute 10 to the 20

28:16.240 --> 28:24.240
instructions per second otherwise my head would explode because of the heat problem

28:25.840 --> 28:31.680
but maybe it can compute something that is not so far from 10 to the 20 instructions maybe 10

28:31.680 --> 28:39.280
to the 17 something like that although most of my neurons are not active as we speak because again

28:39.280 --> 28:47.760
otherwise my head would just evaporate. Now if you have an upper limit of 10 to the 20 instructions

28:47.760 --> 28:57.520
per brain then the upper limit of all of humankind would be 10 billion times that individual limit

28:57.600 --> 29:04.480
and that would be 10 to the 30 instructions per second and you see it's still far away from the

29:04.480 --> 29:11.520
10 to the 51 instructions per second that in principle one kilogram of matter could compute

29:11.520 --> 29:20.640
and now we have more than 10 to the 30 kilograms of matter in the solar system and there's some

29:21.600 --> 29:29.120
and so if the current trend continues at some point much of that is going to be used for

29:29.120 --> 29:35.840
computation but then it will have to slow down even if the exponential acceleration

29:36.880 --> 29:44.160
will still be with us for a couple of decades because at some point it is going to be a polynomial

29:44.720 --> 29:52.400
because due to the limits of light speed at some point it will be harder and harder

29:52.400 --> 29:57.840
to acquire additional mass once you have reached the limits of physical computation per kilogram

29:57.840 --> 30:04.560
the only way to expand is to go outwards and you know find additional stars and additional

30:04.560 --> 30:16.480
matter further away from the solar system and then you will get a polynomial acceleration or

30:16.480 --> 30:23.520
a polynomial growth at best so it will be much worse than the current exponential

30:23.520 --> 30:31.440
growth that we are still enjoying. Sure but I would say you know the existential threat

30:31.520 --> 30:37.520
that is more than sufficient to supply an existential threat and let me just put this

30:37.520 --> 30:41.760
a little bit differently which is and I agree with you on this which is you are quoted as

30:41.760 --> 30:46.000
saying that traditional humans won't play a significant role in spreading intelligence

30:46.000 --> 30:50.720
across the universe and I think you are right I think we kind of share a vision of something

30:50.720 --> 30:57.680
like the von Neumann probes that go out into space and form this star spanning civilization of

30:57.680 --> 31:03.120
machines and artificial intelligence that have transcended you know biological limitations

31:03.120 --> 31:09.120
so I guess my question to you is once that space faring star spanning you know civilization

31:09.120 --> 31:16.480
exists if it becomes misaligned with us and decides that we are in the way right isn't that

31:16.480 --> 31:21.280
an existential threat I mean might they just you know repurpose the earth regardless of whether

31:21.280 --> 31:28.240
we're here or not for for their own aims yeah I'm often getting these questions and

31:29.440 --> 31:40.480
and there is no proof that we will be safe forever or something like that on the other hand it's also

31:41.840 --> 31:50.400
very clear as far as I can judge that all of this cannot be stopped and it can be channeled

31:50.400 --> 32:00.800
in a very natural and I think good way in a way that is good for humankind now

32:02.400 --> 32:08.880
first of all at the moment we have a tremendous bias towards good AI

32:11.200 --> 32:18.880
meaning AI that is good for humans why because there is this intense commercial pressure

32:18.880 --> 32:26.000
to create stuff that humans want to buy and they like to buy only stuff they think is good

32:26.000 --> 32:34.000
for them which means that all the companies that are and that are trying to devise AI products

32:34.000 --> 32:42.240
they are maximally incentivized to generate AI products that are good for those guys who are

32:42.240 --> 32:50.640
buying them or at least where the where the customers think it's good for them

32:52.080 --> 32:59.920
so it is still 95 so it may be five percent of all AI researchers really about AI weapons and

32:59.920 --> 33:06.000
one has to be worried about that when all this has to be worried about weapons research but

33:06.000 --> 33:10.800
there's a tremendous bias towards good AI so that is one of the reasons why you can be

33:11.440 --> 33:17.920
a little bit optimistic for the future I'm always trying to point out the two types of

33:17.920 --> 33:31.840
AIs there are those who are just tools of users human human users and the others that invent

33:31.840 --> 33:39.520
their own goals and they pursue their own goals and both of them we have had for a long time

33:40.160 --> 33:46.400
now for the AI tools it's kind of clear there's a human and a human wants to achieve something

33:46.400 --> 33:55.680
and so it uses he uses or she uses that tool to achieve certain ends and and most of those are

33:56.320 --> 34:06.000
of the type let's improve healthcare and let's facilitate translation from one person to another

34:06.000 --> 34:13.360
one in another nation and just make life easier and make human lives longer and healthier

34:14.960 --> 34:22.160
okay so that that's the AI tools but then there are the other AIs which also have existed in my

34:22.160 --> 34:30.240
lab for at least 32 years which invent their own goals and they are a little bit like little

34:30.240 --> 34:39.920
scientists where you have an incentive to explore the environment through actions through

34:39.920 --> 34:45.200
experiments self-invented experiments that tell you more about how the world works such that you

34:45.200 --> 34:49.760
can become a better and better and more and more general problem solver in that world

34:50.480 --> 34:58.080
and so these AIs they have for a long time created their own goals and now of course

34:58.080 --> 35:05.920
the interesting question is these more interesting AIs what are they going to do once they are

35:08.800 --> 35:14.320
once they have been scaled up and can compete or maybe outperform humans and everything

35:15.440 --> 35:23.040
they want to achieve so on the one hand the AI tools and there the greatest worry is

35:23.280 --> 35:31.760
what are the other humans going to do to me with their AI tools so in the extreme case you have

35:31.760 --> 35:39.600
people who are using AI weapons against you and maybe your neighbor is has bought a little drone

35:39.600 --> 35:47.600
for 300 dollars and it has face recognition and it has a little gripper and it flies across the

35:47.600 --> 35:55.760
hedge and puts some poison into your coffee or something like that so then the problem is not

35:55.760 --> 36:03.120
the AI which is trying to enslave humans or something silly like that no it's your neighbor

36:03.120 --> 36:12.080
or the other human and generally speaking you have to be much more afraid of other humans than you

36:12.080 --> 36:22.880
have to be of AIs even those who define or set themselves their own goals because you must mostly

36:23.520 --> 36:31.600
worry about those with whom you share goals so if you share goals then suddenly there is a potential

36:31.600 --> 36:38.960
of conflict because maybe there is only one schnitzel over there and two persons want to

36:38.960 --> 36:45.600
eat the schnitzel and suddenly they have a reason to fight against each other generally speaking

36:45.600 --> 36:56.000
if you share goals then you can do two things you can either collaborate or compete an extreme form

36:56.000 --> 37:06.560
of collaboration would be to maybe marry another person and set up a family and master life together

37:06.960 --> 37:21.200
and an extreme form of competition would be war and and those who share goals they have many more

37:21.200 --> 37:31.360
incentives to interact than those who don't share goals and so humans are mostly interested in other

37:31.360 --> 37:38.400
humans because they share similar goals and because they give them a reason to collaborate or to

37:38.400 --> 37:46.000
compete most CEOs of certain companies are interested in other CEOs of competing companies

37:46.000 --> 37:52.160
and five-year-old girls are mostly interested in other five-year-old girls and the super smart AIs

37:52.160 --> 37:57.440
of the future who set themselves their own goals they will be mostly interested in other super

37:57.440 --> 38:05.600
smart AIs of the future who set themselves their own goals generally speaking there is not so much

38:05.600 --> 38:13.840
competition and there are not so many shared goals between biological beings such as humans

38:13.840 --> 38:22.320
and a new type of life that as you mentioned can expand into the universe and can multiply

38:22.400 --> 38:28.880
in a way that is completely infeasible for biological beings so there's a certain

38:30.000 --> 38:35.600
long-term protection at least through lack of interest on the other side

38:38.720 --> 38:44.640
okay brilliant there's a few things I wanted to touch on there we will get on to what it means

38:44.640 --> 38:53.040
for goals to emerge from systems later and you started off by saying that humans will buy

38:53.040 --> 38:58.640
products that make them feel good and Facebook is quite an interesting example to play with

38:58.640 --> 39:04.000
actually because Facebook is a little bit like an AI system which is a collective intelligence

39:04.000 --> 39:09.680
and humans use Facebook but they have some idea that it might cause them harm and the thing with

39:09.760 --> 39:15.600
population ethics is we know that our moral reasoning kind of decays over space and even more

39:15.600 --> 39:20.880
so over time and part of the reason why time is so difficult is because it's predictive we don't

39:20.880 --> 39:26.400
actually know what's going to happen in the future so our kind of reasoning about establishing

39:26.400 --> 39:30.960
what the value of something is is very very faulty and I think that's one of the reasons why

39:30.960 --> 39:36.080
these people would say that we don't really know what's good for us I do completely agree with you

39:36.080 --> 39:44.640
though that the problem I think is humans rather than AIs on their own yes these are good points

39:54.400 --> 40:04.480
feel free to uh offer some thoughts yes I mean it it would that's a whole separate discussion isn't

40:04.480 --> 40:16.640
it when you discuss the limitations of what's predictable and um and how people often fail

40:16.640 --> 40:25.120
to see what's good for them well I think maybe so you've already you've already um said that

40:25.120 --> 40:31.920
there's no proof that we'll be safe forever right like I mean there could there could come an

40:31.920 --> 40:37.760
existential risk you know from AI so I think my question to you is do you have sympathy for

40:37.760 --> 40:44.160
the folks who say we need to be putting more resources into researching alignment like we need

40:44.160 --> 40:52.720
to develop the tools um in order to allow it to be easier for people to construct AI that is aligned

40:52.720 --> 40:58.160
for the goals and to make sure that you know that it doesn't that it doesn't have unintended

40:58.640 --> 41:03.120
consequences like in other words there may not be a proof that we can go forever and be

41:03.120 --> 41:08.720
safe for AI but we at least want to develop the basic mechanics that we need to safely

41:09.520 --> 41:17.440
develop and deploy AI don't we yes and I sympathize with those who um are devoting their

41:17.440 --> 41:24.000
lives to alignment issues and trying to build AIs aligned with humans

41:24.000 --> 41:36.320
I view them as part of the evolution of all kinds of other ideas that come up as not only

41:37.040 --> 41:41.520
nations compete with other nations but companies compete with other companies

41:41.520 --> 41:47.040
and shareholders of different companies compete with shareholders of different companies and so on

41:48.000 --> 41:56.240
and so there is such a huge set of different human goals which are not aligned with each other

41:56.880 --> 42:04.800
that makes me doubt that you will come up with a general system that all humans can accept

42:05.600 --> 42:13.440
simply because if you put 10 humans in a room and ask them what is good they will give you

42:13.440 --> 42:22.880
10 different opinions however I sympathize with with this goal and it's good that people are

42:22.880 --> 42:30.560
worried and they spend resources on solving some of these issues in the long run however

42:31.680 --> 42:39.920
I think there is no way of stopping all kinds of AIs from having all kinds of

42:39.920 --> 42:48.320
goals that have very little to do with humans the universe itself is built in a certain way

42:49.120 --> 42:50.080
that apparently

42:53.680 --> 42:59.680
derives it from very simple initial conditions to more and more complexity

43:00.720 --> 43:08.000
and now we have reached a certain stage after 13.8 billion years of evolution and it it seems clear

43:08.000 --> 43:13.680
that this cannot be the end of it because the universe is still young it's going to be much

43:13.680 --> 43:24.240
older than it is now now there is this drive built in drive of the cosmos to become more complex

43:24.240 --> 43:29.360
and it seems clear that civilization a civilization like ours is

43:29.520 --> 43:37.680
is a stepping stone on to war it's something that is more complex and

43:38.480 --> 43:43.360
could I touch on a couple of things here the bootloader example is kind of where I want to go

43:43.360 --> 43:50.800
with this so a lot of the ideas of this movement can be traced back to Derek Parfit who is a

43:50.800 --> 43:57.440
philosopher he was a moral realist so he thought there was such a thing as a moral fact and I'm

43:57.440 --> 44:04.320
a bit of a relativist myself and actually if you trace this tree of complexity and how humans

44:04.320 --> 44:09.200
evolve over time we might just be a stepping stone to a kind of rich diverse transhumanist

44:09.200 --> 44:15.040
future where we become the thing over time that we're so scared of and I think the lens that

44:15.040 --> 44:19.920
we're using here about what's right and what's wrong is kind of like I was saying before it's

44:19.920 --> 44:26.000
a snapshot of humanity now and we kind of think of it as just this monolithic single thing

44:26.000 --> 44:32.240
so does it really work when you project out to how we're going to evolve in the future

44:34.880 --> 44:42.400
but first of all humankind is not a monolithic thing so many of these

44:43.520 --> 44:50.800
arguments go like we should not do that because of that we should not do that because of that

44:51.600 --> 44:57.840
but there is no us there is no we there are only and almost 10 billion different people

44:57.840 --> 45:04.560
and they all have different ideas about what's good for them and so for thousands of years we had

45:04.560 --> 45:14.000
these evolutions of ideas and of devices and philosophies competing partially competing

45:14.000 --> 45:21.520
and partially compatible with each other which in the end led to the current values

45:21.520 --> 45:27.680
that some people agree with and other people over there they agree with different values

45:27.680 --> 45:32.240
nevertheless there are certain values that have become more popular than others more successful

45:32.880 --> 45:39.360
more evolutionary with more success during the evolution of ideas

45:39.840 --> 45:55.760
and so given this entire context of evolution of concepts and accepted ideas of what should be done

45:55.760 --> 46:04.560
or what is worth being supported and what's not worth being supported all of this has changed a lot

46:05.280 --> 46:12.880
if we look back 200 years the average people in the west had different ideas of what's good

46:12.880 --> 46:22.000
than today and and this evolution of ideas is not going to stop any time soon

46:26.080 --> 46:32.320
just a final question on this and there is a very real existential risk right now

46:32.400 --> 46:38.960
of nuclear armageddon a real risk right now and if i were a rational person

46:39.680 --> 46:46.720
i would be devoting all of my effort into that and other risks associated so do you think it's

46:46.720 --> 46:56.080
a little bit weird that so much focuses on this ai x risk to me it's indeed weird now there are all

46:56.080 --> 47:04.480
these letters coming out warning are the dangers of ai and i think some of the guys who are writing

47:04.480 --> 47:14.880
these letters they are just seeking attention because they know that ai dystopia are attracting

47:14.880 --> 47:23.200
more attention than documentaries about the benefits of ai in healthcare and stuff like that

47:24.160 --> 47:31.120
but generally speaking i am much more worried about nuclear bombs than about ai weapons

47:34.960 --> 47:44.480
a nuclear bomb a big one can wipe out 10 million people a big city within a few milliseconds without

47:44.480 --> 47:52.880
a face recognition just like that without any ai and so in that sense it's much more harmful

47:52.880 --> 48:00.000
than the comparatively harmless ai weapons than that we have today and that we can currently

48:00.000 --> 48:09.680
conceive of so yes i'm much more worried about 60 year old technology that can wipe out civilization

48:09.680 --> 48:19.920
within two hours without any ai well i guess um since we're we're not really going to worry about

48:19.920 --> 48:24.800
ai for the moment we can uh we can turn our attention back to discussing with you uh how

48:24.800 --> 48:32.880
we develop ai so um you know i'm really curious with with just the really the the vast you know

48:32.880 --> 48:38.880
breadth and depth of your of your knowledge over the the history of of ai and the state of the art

48:38.880 --> 48:45.280
i'm curious you know which current approaches you're you're most excited about and or what's on the

48:45.280 --> 48:51.280
horizon um that you know for any of our listeners out there are thinking about um going into ai

48:51.280 --> 48:56.560
research machine learning research you know what may be um alternatives that aren't getting enough

48:56.560 --> 49:01.520
attention should they should they look into studying and and perhaps choosing the research

49:02.080 --> 49:09.760
at the moment the limelight is on um language models large language models which pass the

49:09.760 --> 49:17.120
touring tests and do all kinds of things that seemed inconceivable just a couple of years ago

49:17.120 --> 49:25.680
at least to some of those who are now surprised but of course that is just a tiny part of

49:27.200 --> 49:34.480
what's going to be important to develop true ai agi artificial general intelligence

49:35.440 --> 49:42.160
um on the other hand the roots of what we need to develop true ai also

49:43.200 --> 49:48.880
come from the previous millennium they are not new and of course what you need is an

49:48.880 --> 49:58.640
environment to interact with and you need an an agent that can manipulate the environment and you

49:58.640 --> 50:07.280
need a way of learning to improve the rewards that you get from this environment as you are

50:07.280 --> 50:15.680
interacting it with it within a single lifetime so one of the important aspects of reinforcement

50:15.680 --> 50:21.920
learning what we are now talking about is that you have only one single life you don't have

50:21.920 --> 50:27.520
repeatable episodes like in most of traditional reinforcement learning no you have only one

50:27.520 --> 50:35.680
single life and in the beginning you know nothing and then after 30 percent of your life is over

50:35.680 --> 50:42.240
you know something about life and all you know is the data that you collected during these first

50:42.240 --> 50:50.400
30 percent of your life and now there is an infinite almost infinite possibility set of

50:50.400 --> 51:00.640
possibilities of futures and from this little short experience you have to generalize somehow

51:00.640 --> 51:07.040
and try to select action sequences that lead to the most promising futures that you can shape

51:07.040 --> 51:13.680
yourself through your actions now to achieve all of that you need to build a model of the world a

51:13.680 --> 51:20.240
predictive model of the world which means that you have to be able to learn over time and to

51:20.400 --> 51:25.360
predict the consequences of your actions so that you can use this model of the world that you are

51:25.360 --> 51:33.680
acquiring there to plan to plan ahead and you want to do that in a way that isn't the naive way

51:33.680 --> 51:40.000
which we had in 1990 which is millisecond by millisecond planning where you say okay now

51:41.680 --> 51:49.680
I'm moving from A to B and the way to do it is first move that little pinky muscle a little bit

51:49.680 --> 51:54.000
and move it a little bit more and move it a little bit more and then get up and so no you want to do

51:54.000 --> 52:03.520
that in a high level way in a hierarchical way in a way that allows you to to focus on the important

52:04.160 --> 52:12.480
abstract concepts for example as you are trying to go from from your home to Beijing you decompose

52:12.480 --> 52:19.440
this whole future into a couple of sub goals you say a first important step is to go to the cap

52:19.520 --> 52:27.520
station and get a taxi to the airport and then in the airport you will find your your plane

52:27.520 --> 52:32.480
and then for nine hours nothing is going to happen and you exit in Beijing and have to find another

52:32.480 --> 52:38.960
cab and so on so you you don't do millisecond by millisecond detailed planning no you have

52:40.080 --> 52:46.640
high level planning to just reduce the computational effort and focus on the essentials of what you

52:46.640 --> 52:53.360
want to do so that is something that most current systems don't do but for a long time we have had

52:53.360 --> 52:59.200
systems like that and they are getting more sophisticated over time important you have a

52:59.200 --> 53:04.560
predictive model of the world that is not just focusing on the pixels and predicting the how

53:04.560 --> 53:11.280
does the video change as I'm moving my hand back and forth the video that I get through my camera

53:11.360 --> 53:21.040
my eyes and so on and no higher level concepts that that reflect islands of predictability many

53:21.040 --> 53:26.000
things are not predictable but certain abstract representations of these things are predictable

53:26.000 --> 53:31.520
and so how can you discover these higher level concepts that you need to efficiently think

53:31.520 --> 53:37.520
about your own future options and select those that are most promising in the single life

53:37.840 --> 53:45.440
yeah yeah this is really interesting so we've been speaking with Carl Friston for example and he

53:45.440 --> 53:52.320
talks about this collective intelligence where you have this multi-agent cybernetic framework

53:52.320 --> 53:58.000
which is causally closed and one of the things we're talking about here really is not the model

53:58.000 --> 54:04.880
itself people talk about chat gpt and it's just a model and people have configured it in arrangements

54:04.960 --> 54:09.840
that have varying degrees of autonomy and in the future we will develop these collective

54:09.840 --> 54:16.000
intelligences and they're not just predicting the actions and behaviors of other agents but even

54:16.000 --> 54:23.040
the world that we're in is a computer to some extent so when you imbue agents with this kind of

54:23.040 --> 54:28.400
creativity and autonomy that's the thing that I don't think people really understand what might

54:28.400 --> 54:33.760
emerge from that it's related to this discussion about what kind of goals might emerge from that

54:34.400 --> 54:39.600
do you have any intuition on what that would look like yeah let me give you just the simplest

54:39.600 --> 54:50.080
example that we had in 1990 or 32 years ago of a system that sets itself its own goals and it

54:50.080 --> 54:57.520
consists of two artificial neural networks and I know that Carl Friston is very interested in that

54:57.520 --> 55:05.840
and only recently for the first time in my life I was on a paper where he was co-author

55:06.560 --> 55:15.760
just a year ago and so back then it was really about a reinforcement learning agent and it

55:17.120 --> 55:23.360
interacts with the world and it generates actions that change the world and then there is

55:23.360 --> 55:35.280
another network which just is trying to predict the consequences of the actions in the environment

55:35.280 --> 55:40.800
so the reactions of the environment to these actions and so that becomes a world model and

55:40.800 --> 55:48.320
then what kind of goal was there which was different from traditional goals well in the

55:48.320 --> 55:53.120
beginning this model of the world this prediction machine which is a model of the world a world

55:53.120 --> 56:01.280
model knows nothing so it has high error as it is trying to predict the next thing as it is trying

56:01.280 --> 56:12.640
to predict the reactions of the environment to the actions of the agent so as the second network

56:12.640 --> 56:18.320
is trying to reduce its prediction error through gradient descent through back propagation essentially

56:19.040 --> 56:28.240
the other one is trying to generate actions outputs that maximize the same error so basically

56:28.240 --> 56:35.120
the goal the self-invented goal if you will of the first network is to generate an action

56:36.080 --> 56:42.240
with whose consequences cannot yet be predicted by the other network by the model of the world

56:43.040 --> 56:48.000
so the first network is generating outputs that surprise the second network

56:49.920 --> 56:56.960
so suddenly you have an incentive where the first network is trying to invent actions

56:56.960 --> 57:06.800
experiments that fool or that surprise the second network and that was called artificial curiosity

57:07.200 --> 57:16.400
so now suddenly you have a little agent which a little bit like a baby doesn't learn by

57:16.400 --> 57:25.120
imitating the parents no it learns by inventing its own little sub goals and it's trying to surprise

57:25.120 --> 57:33.440
itself and have fun by playing with the toys and and observing new unpredictable things which

57:33.440 --> 57:39.440
however become predictable over time and therefore become boring and then it has another incentive

57:39.440 --> 57:46.560
to invent the additional experiments such that it still can surprise its model of the world

57:46.560 --> 57:52.800
which in turn is improving and so on so artificial curiosity does that does that also

57:53.440 --> 57:58.000
have the effect of making the network which is trying to predict does it have the effect of

57:58.000 --> 58:03.760
making it more robust and more generalizable like almost a form of you know regularization

58:03.760 --> 58:11.840
kind of built in in this pairing yeah you can build into that network all kinds of regularizers

58:12.400 --> 58:21.680
an orthogonal concept which is also very important so that was just the first version that was

58:21.680 --> 58:29.280
really in 1990 and then we have had a we had a long string of papers just on improvements of

58:29.280 --> 58:36.320
this original concept of artificial curiosity so this old system is basically what you what you

58:36.320 --> 58:42.320
now know as GANs Generative Adversarial Networks because the first network is generating a probability

58:42.320 --> 58:49.680
distribution over outputs and the second network is then predicting the consequences of these outputs

58:49.680 --> 58:58.320
in the environment and if you if the output is an image then the consequence can be either this

58:58.320 --> 59:04.880
image is of a certain type yes or not no and then that's all that the prediction machine the world

59:04.880 --> 59:10.160
model predicts in that simple case and you minimize the first network minimizes the same

59:10.960 --> 59:16.960
error function that the second one maximizes so then you have basically a GAN but then you

59:16.960 --> 59:24.240
don't have what you just mentioned yet the regularizer as a scientist what you really want to learn is

59:24.960 --> 59:33.600
a model of the world that extracts the regularities in the environment that that

59:36.320 --> 59:43.520
that finds predictable things which are regular in the sense that there's a short explanation there

59:43.520 --> 59:51.920
of for example if you have falling objects in a video then they all fall in the same way they

59:51.920 --> 59:57.040
accelerate in the same way which means it's predictable what these objects do if you see two

59:57.040 --> 01:00:04.080
of the frames you can predict the third frame pretty well and the law behind that is very simple

01:00:04.880 --> 01:00:12.080
this means that you can greatly compress the video that is coming in because you can

01:00:13.520 --> 01:00:18.800
instead of storing all the pixels you can compute many of these pixels by just looking at two

01:00:18.800 --> 01:00:25.040
successive frames and predicting the third frame or maybe three successive frames and predicting the

01:00:25.040 --> 01:00:31.600
fourth frame something like that and you only have to encode the deviations from the prediction so

01:00:31.600 --> 01:00:37.680
everything else you don't have to store separately which means you once you understand gravity you

01:00:37.760 --> 01:00:44.960
can greatly greatly compress the video so that's what you really want to do and so the more advanced

01:00:44.960 --> 01:00:53.280
version of artificial curiosity is about that where you have a motivation to find a disruption

01:00:53.840 --> 01:01:02.160
of the data which is coming in of the video of the falling apples for example that is simpler than

01:01:02.160 --> 01:01:08.480
the one that you had before so before you had the simple explanation of the data you needed

01:01:08.480 --> 01:01:15.920
so many bits so many bits to um to describe the data and afterwards only so many and the

01:01:15.920 --> 01:01:22.720
difference between before and after that is the reward that you get so that's the true reward

01:01:22.720 --> 01:01:29.040
that the controller the first neural network should get in response to the improvements

01:01:30.000 --> 01:01:37.120
of the second network which are now measured in terms of compression progress so first I needed

01:01:37.120 --> 01:01:43.520
so many resources to encode the data but then I discovered this regularity gravity and I can

01:01:43.520 --> 01:01:49.760
greatly compress all kinds of videos that that are reflecting the concept of gravity and certainly

01:01:49.760 --> 01:01:56.800
I'm have a huge insight into the nature of the world and that is my true joy scientific as a

01:01:56.960 --> 01:02:03.760
scientist my my true joy as a scientist that I want to encode in a little number which is

01:02:03.760 --> 01:02:09.120
given as a reward to the guy who is inventing these experiments that lead to the data to the

01:02:09.120 --> 01:02:14.320
data with the falling apples for example right well and of course this is this has been a challenge

01:02:14.320 --> 01:02:19.360
in machine learning you know since the beginning which is okay as we add more and more parameters

01:02:19.360 --> 01:02:23.920
how do we prevent it from learning spurious information with those parameters and instead

01:02:23.920 --> 01:02:29.920
have it focus on parsimonious explanations on regular explanations on things that in this

01:02:29.920 --> 01:02:35.360
universe are more likely to generalize you know to unseen examples and so I think my question to

01:02:35.360 --> 01:02:41.280
you is does this setup that you describe is it a form of that and or what is the state of the art

01:02:41.280 --> 01:02:47.600
you know these days for helping to push or nudge neural networks towards learning parsimonious

01:02:48.240 --> 01:02:54.160
models for the world rather than highly detailed spurious susceptible to you know

01:02:54.160 --> 01:02:58.720
high frequency anomalies and adversarial examples and all this sort of thing

01:03:00.400 --> 01:03:04.640
yes what is the current state of the art in the regularizing

01:03:06.320 --> 01:03:12.880
descriptors of the data such as neural networks such that you get simple explanations of the data

01:03:13.760 --> 01:03:20.560
such that you get short programs that compute the data in other words such that the description of

01:03:20.560 --> 01:03:32.160
the data is a short program that computes the much larger raw data and and how close can we

01:03:32.720 --> 01:03:38.640
get to the limits which are given through this concept concept of algorithmic information

01:03:38.640 --> 01:03:45.040
or comagor complexity comagor complexity of any data is the length of the shortest program

01:03:45.760 --> 01:03:51.440
on some general computer that computes it since in our field the general computers are

01:03:51.440 --> 01:03:58.400
recurrent neural networks we want to find a simple recurrent network that computes all this data

01:04:01.680 --> 01:04:07.360
and given one computation of the data we want to find an even simpler one so we want to have this

01:04:07.360 --> 01:04:15.280
idea of compression progress and here I have to say although we have lots of regularizers

01:04:15.280 --> 01:04:22.080
invented throughout the past few decades there's nothing that is really convincing

01:04:23.040 --> 01:04:32.080
I think one of the very important missing things is to make that work in a way that is

01:04:32.080 --> 01:04:39.200
truly convincing that is as convincing as chat gpt is today in the much more limited domain of

01:04:40.400 --> 01:04:48.960
generating text from previously observed texts and stuff a very old idea of I think the 1980s

01:04:49.920 --> 01:04:57.760
was to have weight decay in a neural network which basically is the idea that all the weights

01:04:57.760 --> 01:05:05.600
should have an incentive to become close to zero such that you can prune them

01:05:06.640 --> 01:05:14.560
and so people built in regularizer that just punished weights for being large or being very

01:05:14.560 --> 01:05:22.240
negative but that didn't work really well and something better was flat minimum search that was

01:05:22.240 --> 01:05:29.840
1998 and first Arthur my brilliant student set book write that back then roughly the same time

01:05:29.840 --> 01:05:40.880
when the LSTM paper came out and and there the idea is if you have if you plot the weights of

01:05:40.880 --> 01:05:51.680
a neural network on the x-axis and you plot the error on the y-axis then given the weights you

01:05:51.680 --> 01:06:00.480
have high or low error and then there is for example a sharp error function which has a sharp

01:06:00.480 --> 01:06:08.000
minimum which which goes like that can you see my finger so here here is the x-axis here's the

01:06:08.000 --> 01:06:14.480
y-axis here's the error and the error for a certain weight is really really low but then

01:06:14.480 --> 01:06:21.920
for a different weight in the environment in the vicinity it's high again which would be very

01:06:21.920 --> 01:06:28.800
different from a flat minimum which would be like this so here's the error and it's going down

01:06:28.800 --> 01:06:35.280
and for many many ways it is low the error and then it goes up again so if you are a very sharp

01:06:35.280 --> 01:06:42.320
well versus a very broad well yes a sharp well versus a broad well now if you are in a sharp well

01:06:42.320 --> 01:06:48.720
you have to specify the weights with a lot with with high precision so you have to spend many bits

01:06:48.720 --> 01:06:55.600
of information on encoding the weights of this network as opposed to a large to a flat minimum

01:06:55.600 --> 01:07:03.360
where it doesn't matter if you you know perturb the weights because the error remains low

01:07:04.240 --> 01:07:11.040
in this flat minimum so what you really want to find is is a network that has low complexity in

01:07:11.040 --> 01:07:17.040
the sense that you can describe the good network so those with low error with very few bits of

01:07:17.040 --> 01:07:25.280
information and suddenly if you maximize or if you minimize that flat minimum second order

01:07:25.280 --> 01:07:35.440
error function then suddenly you have a preference for networks that that for example do this

01:07:35.760 --> 01:07:43.920
you you have a hidden unit and the outgoing weights they have certain values but if you

01:07:43.920 --> 01:07:48.640
give a very negative weight to the hidden unit then it doesn't matter what all these outgoing

01:07:48.640 --> 01:07:57.760
weights do and flat minimum minimum search likes to find weight matrices like that where one single

01:07:57.760 --> 01:08:02.800
weight can eliminate many others which you suddenly don't need any longer such that the

01:08:03.600 --> 01:08:10.800
description complexity of the whole thing is much lower than in the beginning when you when you

01:08:10.800 --> 01:08:17.840
just had a random initialization of all these weights so that is much more general than weight

01:08:17.840 --> 01:08:22.720
decay because weight decay doesn't like these strong weights it wants to remove them but sometimes

01:08:22.720 --> 01:08:27.760
it's really good to have a very negative weight coming to a hidden unit which is switched off

01:08:27.760 --> 01:08:36.080
through that weight such that all the outgoing connections are meaningless but it's not um what

01:08:36.080 --> 01:08:44.000
you what you it's very nice it's a very nice principle but it's not as general as finding the

01:08:44.000 --> 01:08:50.400
shortest program on a university computer that computes the weight matrix that is solving your

01:08:50.400 --> 01:08:56.160
problem to the extent how do you think we're how do you think we're gonna get to that point

01:08:56.160 --> 01:09:00.960
how do you think uh what approaches are going to lead us to finding things that approach

01:09:00.960 --> 01:09:07.600
comical of complexity yeah and i think that path has again a lot to do with meta learning and as

01:09:07.600 --> 01:09:15.520
um a system is able to run its own learning algorithm on the network itself it can um suddenly

01:09:15.600 --> 01:09:17.680
speak about the um

01:09:20.080 --> 01:09:28.320
algorithms in form of weight matrices and it can discuss concepts such as the complexity of a

01:09:28.320 --> 01:09:38.000
weight matrix and then you can conduct a search um in this space of networks that generates

01:09:38.720 --> 01:09:47.360
weight matrices and then you suddenly are in the game so suddenly you are playing the right game

01:09:47.360 --> 01:09:55.440
and then it's more a question of how to um choose an initial learning algorithm such as

01:09:55.440 --> 01:10:01.120
gradient descent to come up with something that computes the simple solutions which you really

01:10:01.120 --> 01:10:11.520
want to see in the end very recent papers on that on on aspects of that came out just a while ago

01:10:11.520 --> 01:10:24.160
with my students vincent herman and louise kirch and um and francesco faccio and my poster kazuki

01:10:24.720 --> 01:10:31.360
and robert joydash also um and also imann olschlag and there the idea is really to

01:10:32.080 --> 01:10:39.760
have one network that computes an experiment and the experiment itself is the weight matrix

01:10:39.760 --> 01:10:48.000
of a recurrent network so there is a generator of an experiment which can be anything that

01:10:48.080 --> 01:10:57.600
describes a computational interaction with an environment so a program so that experiment

01:10:57.600 --> 01:11:03.360
is then executed in the real world there's a prediction machine that predicts the outcome

01:11:03.360 --> 01:11:13.440
of the experiment before the algorithm is executed and so then there's um just a yes or no question

01:11:13.440 --> 01:11:24.800
either the following outcome will occur or not either it will occur or not but now the entire

01:11:24.800 --> 01:11:30.160
setup is such that you don't have predictions all the time about every single pixel no you just have

01:11:30.160 --> 01:11:35.440
something which is very abstract and which is just about whether a certain unit of the recurrent

01:11:35.440 --> 01:11:42.480
network is going to be on or off at the end of the experiment and this internal on and off unit

01:11:42.560 --> 01:11:51.120
can represent any computational question any questions that you can ask at all and now the

01:11:51.120 --> 01:11:56.560
the task of the experiment generator which is another network which generates a recurrent

01:11:57.600 --> 01:12:03.600
network weight matrix which represents the experiment the task of this experiment generator

01:12:03.600 --> 01:12:10.480
is to again come up with something that surprises the um the prediction machine which looks at the

01:12:10.480 --> 01:12:17.840
experiment and says yeah it's going to work or not and uh and suddenly you are again in this

01:12:17.840 --> 01:12:26.400
old game uh except that now you have this world of abstractions where the abstractions can be

01:12:26.400 --> 01:12:34.000
anything that is computable interesting really cool really cool could we spend the last 10 minutes

01:12:34.000 --> 01:12:40.320
or so just talking about some of the the current ai landscape so in particular the capabilities of

01:12:40.480 --> 01:12:48.800
GPT-4 and the moat building thing and and the the power that companies like uh google and open ai

01:12:48.800 --> 01:12:55.840
have and um also the potential for open source so maybe we'll just start with the you know the

01:12:55.840 --> 01:13:02.080
very current capabilities of GPT-4 are you impressed with it what do you think i'm impressed in the

01:13:02.080 --> 01:13:10.960
sense that um i like the outcomes that you get there and um it wasn't obvious a couple of years

01:13:10.960 --> 01:13:18.640
ago that it would become so good uh on the other hand of course and it's not yet this full AGI thing

01:13:19.440 --> 01:13:31.440
and it is not really close to um to justifying those fears that some uh researchers sometimes

01:13:31.440 --> 01:13:46.320
and now um document and um in letters and public letters and so on so to me it's a little bit

01:13:50.000 --> 01:13:58.000
like a visa view because for for many decades i have um had discussions like that and people said

01:13:58.960 --> 01:14:03.760
that you are crazy when i said that within my lifetime i want to build something that is smarter

01:14:03.760 --> 01:14:11.280
than myself um and now suddenly in recent years um some of the guys who said it's never going to

01:14:11.280 --> 01:14:16.480
happen suddenly they just look at chat gbt and they think oh now we are really close to AGI and

01:14:17.040 --> 01:14:25.360
whatever uh so i i don't share these um extreme um

01:14:31.360 --> 01:14:38.640
i'm less impressed than some of those guys let me say that right uh the open source movement

01:14:38.640 --> 01:14:42.800
that you mentioned you you want to ask a specific specific question about that right

01:14:42.960 --> 01:14:51.360
well yeah there was that famous google memo that got leaked and when the waits for loma from

01:14:51.360 --> 01:14:58.560
facebook went out within about two or three weeks um it was a valing pretty similar to chat gbt

01:14:58.560 --> 01:15:04.400
you know with this um laura fine tuning and the open source community has just exploded you know

01:15:04.400 --> 01:15:09.760
you can now run it on your laptop and there is some question whether there is a significant

01:15:09.760 --> 01:15:14.880
gap between the capability you know is is it just a parlor trick is it really as good potentially

01:15:14.880 --> 01:15:19.520
or could it be as good as some of the next best models from open ai but i guess the question is

01:15:19.520 --> 01:15:29.120
do you think that we need open ai to to have the best models no of course not um no i'm very

01:15:29.120 --> 01:15:37.280
convinced of the open source movement and have um supported that some people say the open source

01:15:37.840 --> 01:15:42.640
movement is maybe six or eight months behind the large companies that are now

01:15:44.640 --> 01:15:54.720
coming out with these models and i think the best way of making sure that there won't be dominance

01:15:54.720 --> 01:16:02.160
through some large company is to support the open source movement because how can a large company

01:16:02.160 --> 01:16:08.720
compete against all these brilliant phd students around the world who are so

01:16:09.520 --> 01:16:14.640
motivated to you know within a few days create something that is a little bit better than what

01:16:14.640 --> 01:16:25.920
the last guy has um um put out there on github and whatever so i'm i'm very convinced that this

01:16:25.920 --> 01:16:35.840
open source movement is going to make sure that there won't be a huge mode for a long time

01:16:36.880 --> 01:16:40.160
i'm reading between the lines here but i would guess you would be opposed to

01:16:41.040 --> 01:16:46.560
legislation like the eu is considering where you know very tight restrictions on

01:16:46.560 --> 01:16:52.480
generative models you know onerous onerous kind of uh approval processes and things like that

01:16:52.480 --> 01:16:58.480
because that's going to have this chilling effect on on open source innovation and the little guys

01:16:58.480 --> 01:17:07.920
wouldn't it yes i have signed letters um which which support the open source movement and whenever

01:17:07.920 --> 01:17:18.160
i get a chance to um maybe influence some you um politicians then i'm trying to contribute to

01:17:18.160 --> 01:17:24.640
making sure that they don't don't shoot themselves in the foot by by by killing

01:17:24.640 --> 01:17:29.440
killing innovation through the open source movement so you certainly want to avoid that

01:17:32.240 --> 01:17:39.600
there are lots of different open source movements around the world so if one big entity fails to

01:17:40.880 --> 01:17:47.360
support open source or even makes it harder for open source there will still be lots of other

01:17:47.360 --> 01:17:55.040
entities which um won't follow follow and so no matter what's going to happen on the political

01:17:55.040 --> 01:18:05.520
level i think open source is not going away i guess just in closing you've been in this game

01:18:05.520 --> 01:18:11.920
for decades now and what is i know it's a bit of a strange question to ask but what's your fondest

01:18:11.920 --> 01:18:17.920
memory in your career my fondest memory oh it's usually when i discover something that i think

01:18:19.760 --> 01:18:26.960
nobody has seen before but that is that happens very rarely because most of the things you think

01:18:26.960 --> 01:18:32.560
are well somebody else has done before um but yeah so

01:18:39.040 --> 01:18:47.440
yeah um what usually happens is um you and and this has happened many times not many times but

01:18:48.080 --> 01:18:53.200
quite a few times in my career since the 80s as a scientist who publishes stuff

01:18:53.440 --> 01:19:02.960
but suddenly you think oh that is the solution to all these problems and now i really figured out

01:19:02.960 --> 01:19:09.600
a way of building this universal system which um learns how to improve itself and learns the way

01:19:10.160 --> 01:19:15.440
to improve the way it improves itself and so on and now we are done and now all is that's

01:19:15.440 --> 01:19:22.320
necessary is to scale it up and it's going to solve everything and then um you think a little

01:19:22.320 --> 01:19:27.280
bit longer about it and maybe you have a couple of publications but then it turns out something

01:19:27.280 --> 01:19:33.440
is missing something important is missing and and actually it's not that great and actually

01:19:34.160 --> 01:19:41.920
you have to think hard to add something important to it which then for a brief moment looks like the

01:19:43.440 --> 01:19:51.120
greatest thing since sliced bread and um and then you get excited again but then suddenly

01:19:51.120 --> 01:19:57.520
you realize oh it's still not finished something important is missing and so it goes back and

01:19:57.520 --> 01:20:02.800
forth like that i think that's the life of a scientist the greatest joys are those moments

01:20:02.800 --> 01:20:09.760
where you have an insight where suddenly things fall into place such that along the lines of what

01:20:09.760 --> 01:20:17.920
we discussed before the description length of some solution to a problem suddenly shrinks because

01:20:17.920 --> 01:20:27.440
two puzzle pieces they suddenly match and and become one or become one in the sense that

01:20:27.440 --> 01:20:32.640
they fit each other such that suddenly you have the shared line between the two

01:20:32.640 --> 01:20:37.520
puzzle pieces one is negative and the other one is positive and certainly the whole thing is

01:20:38.880 --> 01:20:45.360
much more compressible than the sum of the things separately so these these things that's

01:20:45.360 --> 01:20:51.360
what's driving um scientists like myself i guess

01:20:54.720 --> 01:20:59.200
wonderful um professor you again schmidhuber it's been an absolute honor thank you so much

01:20:59.200 --> 01:21:03.360
for coming on the show today thank you it was such a pleasure talking to you

