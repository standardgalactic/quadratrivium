start	end	text
0	6560	You know, I have this paper, Conscious Exotica, in 2016, and then I joined DeepMind in 2017,
6560	11760	and at that point I'd been thinking and writing quite a bit about consciousness up to that point.
11760	16720	But then I sort of stopped, because I thought I didn't think it seemed appropriate for somebody
16720	21280	working in a corporation to be talking about consciousness, especially in the context of AI,
21280	25280	because it might sound like, you know, we're trying to build conscious AI, which I don't think is a
25360	30480	good look, or a particularly good project, I'd say. The thing is, with today's generation of
30480	35920	large language models, I think it's becoming increasingly difficult to avoid the subject,
35920	40800	because people, whether we like it or not, will ascribe consciousness to the things that they're
40800	46560	interacting with. And we see this left, right and centre. Even people who know exactly how they work
47520	51920	say things like, well, I think their large language models are a little bit conscious,
52000	60080	Ilya Tsutskava said, and we had the Google engineer who ascribed consciousness to one of our models,
60080	65360	and I think we're going to see this more and more and more. And so whether or not, you know,
65360	70800	I think it is the right term, it is appropriate to talk about them in terms of consciousness,
70800	74880	it's going to happen anyway. So I think it's really important to actually think through
74880	81040	these issues and think, well, what do we mean when we use the word consciousness,
81040	87440	and how do we apply it to exotic cases? And this is really, really important.
87440	93520	How might our language change to accommodate these exotic and strange things that have come
93520	99360	into our lives? What goes through your mind when you speak with a language model? Who is it that
99360	105840	you think you're talking to? Do you anthropomorphise them? Now, Janice from Less Wrong a couple of
105840	110880	years ago, he put out an article called Simulators. And the basic idea is that a language model is
110880	118480	like a simulation machine, producing manifestations of role players, which we willfully anthropomorphise.
118480	124480	We think of them as humans. They're in perfect copies. They don't capture the essence. They are
124480	130720	glitchy, right? And actually, they wear masks. You know the Shogoff theory of language models,
130720	135760	where there's, you know, this big gnarly Shogoff, and then we do RLHF, and it's a smiley face on
135760	140640	the top. Well, that's the human mask, which we anthropomorphise. But we don't think enough
140720	147280	about what lies behind the mask. Do you know what lies behind the mask? It's a monster.
149280	156880	The danger of anthropomorphism, I think, is in thinking that a system such as a large language
156880	161840	model, you know, a chatbot or something, thinking that it has capabilities and that it doesn't.
162400	167040	It's as simple as that. Actually, it's also thinking, perhaps, that it lacks capabilities
167040	171520	that it does. So in both cases, I think we can go wrong.
177920	182480	We can go wrong by, because they exhibit very human-like linguistic behaviour,
182480	187680	we can just assume that they are going to be very human-like, in general, in all of the rest of
187680	196560	the behaviour that we encounter with them. We can find that, at one moment, a large language model
196560	202240	might make a ridiculously stupid mistake that no child would make. And in the next moment,
202240	205520	it's saying something extraordinarily profound philosophically.
209360	213680	And because that's what I think is actually going to happen and what needs to happen. I think we need
213680	222640	to find new ways of using the vocabulary we have, new forms of vocabulary. I've used the phrase
222720	227120	consciousness-adjacent language. So we need to find new ways of thinking and talking about
227120	235040	these things to recognise the fact that they do exhibit behaviour, which we're inclined to talk
235040	239600	of in terms of consciousness, and that, indeed, people are going to start to value as well.
240160	245200	So I think we do need new forms of language, new forms of thinking to accommodate all of this.
246000	251600	Yes, and our language is so adaptable that I think it's just a natural evolution.
251600	256160	When we have these new artefacts thrust into our lives, we will need to adapt our language.
256160	261280	We will, but I think there'll be some disruption while people disagree about how to talk about
261280	268160	these things, and that's inevitable, I think. Murray Shanahan is a printable research
268160	274720	scientist at Google DeepMind and professor of cognitive robotics at Imperial College London.
274720	281040	He was also educated at Imperial and Cambridge University. His publications span artificial
281040	286800	intelligence, machine learning, logic, dynamical systems, computational neuroscience,
286800	292320	and the philosophy of mind. He was scientific advisor on the film Ex Machina, and he penned
292320	299040	embodiment and the inner life in 2010 and the technological singularity in 2015.
299920	305840	Shanahan has spent his career understanding cognition and consciousness in the space of
305840	312560	possible minds. He said that this space of possibilities encompasses biological brains,
312560	319600	human and animal, as well as artificial intelligence. He worked in symbolic AI for over 10 years,
319600	325360	concentrating on commonsense reasoning, and he then spent 10 years studying the biological brain,
325360	330960	specifically how its connectivity and dynamics support cognition and consciousness,
330960	334800	and he developed a particular interest in global workspace theory.
335520	339760	After that, he went to DeepMind, he pivoted to deep reinforcement learning,
339760	345200	and recently he's been working extensively with large language models, trying to understand them
345200	351440	from a theoretical, philosophical, and practical perspective. Professor Shanahan, I was absolutely
351440	357360	fascinated when I read the article from Janus called Simulators. Could you sketch out the article?
358320	365120	Well, I can sketch out some elements of it. I was also very impressed and
365120	372160	influenced by that article. Basically, they are advocating a certain way of looking at
372160	377840	large language models and their behavior. What they say is that we should think of a
377840	385600	large language model as a kind of simulator, which is capable of simulating a kind of
385600	392480	language-producing processes of various sorts, and it's capable of simulating all kinds of language
392480	399360	producing processes, and in particular, it's capable of simulating people, humans, and it's
399360	404800	capable of simulating different kinds of humans, so humans who are playing different sorts of roles,
404800	413600	who maybe humans who are helpful assistants or humans who are crazy psychopaths,
414160	424160	and indeed in their way of thinking of things. These are all examples of Simulacra. Simulacra,
424160	429760	in their conception, include actually not only human beings, but anything that produces language
429760	436800	at all. Your base model can simulate anything that can generate language, if you like.
437600	444400	Of particular interest, of course, are humans and human language producers, so
445280	451600	the particular class of Simulacra that I'm interested in really are humans playing different
451600	459840	roles. In the work that I've been doing, I've been thinking of language models in terms of role
459840	470400	play and in terms of their ability to play a part, if you like, and so this is very much,
470400	478080	it was very much inspired and drew on this work of Janus. Now, they make another very, very
478080	485360	interesting and important point in that article, which is that they draw attention to the fact that
486320	490320	large language models, at any point in a conversation, in an ongoing conversation,
491200	495920	then the next word that's produced in this conversation, or the next string of words,
495920	502560	the next sort of sentence, is the product of a stochastic process. So what the underlying
502560	508160	language model actually generates is a distribution over the possible words that might come up next,
508160	513200	and then what you do is then you sample from this distribution to come up with an actual word,
513200	519280	and then that's the word that you give back to the user. So for example, if the favorite example
519280	528480	I use is if you ask the language model to tell you a story and it says once upon a time there was,
528480	533280	and at that point, it's going to generate, as in all the points up to that as well, it's going to
533280	537840	generate a distribution of the possible tokens that might come next, possible words that might come
537840	544320	next. So once upon a time there was, and it might say a beautiful princess, or a handsome prince,
544320	550000	or a fierce dragon, and it could say any of those things depending upon the sampling process.
550000	555200	And then the point is as you come back to that same, you could rewind the conversation, come back
555200	560000	to that point again, sample again, as we can all do with the interfaces that we have, and get a
560000	563440	different answer again, and take the whole story off in a completely different direction.
563600	570720	And so what they draw attention to is the fact that at any particular point in a conversation,
570720	578880	there's really a whole set of roles that are being played by the underlying simulation
578880	584960	at any one point, and the conversation shapes what role is being played. So in that sense,
584960	592160	it's sort of unlike a human being, because you've got, as they put it, a whole superposition of
592640	597680	simulacra that are all being simulated all at once. And as the conversation progresses,
597680	603520	then the actual distribution of simulacra is being narrowed down.
604080	608880	Yes, but as you say, you can view language models at the low level in terms of being
608880	614480	next-word generators, or what we strive to do in science is come up with explanations that
614480	620480	demarcate the thing very clearly. And this idea of the language model being a simulator,
620480	626240	which produces the simulacra, and you said in your role-playing article on Nature that
626240	630880	if you had a UI which was sufficiently advanced, you could actually play with counterfactual
630880	636640	trajectories and start to understand how sticky the simulacra are, because as you pointed out,
636640	642080	when the language model says I, sometimes it's talking about chat GPT or whatever,
642080	647440	it's talking about the simulator, sometimes it's talking about the simulacrum, and these things
647440	652880	are trained on everything on the internet, you know, structured narratology essays, novels,
652880	657920	and it's fascinating to see how you can jump between these different parts of the trajectory
657920	663200	structure. And in your Nature paper, you gave a beautiful example, which was the 20 Questions
663200	667120	game. I mean, would you mind introducing that? Yeah, sure. So I think we're probably all familiar
667120	673360	with the 20 Questions game. So one player thinks of an object, and the other player has to guess
673440	680960	what that object is by asking a whole bunch of questions with yes, no answers. So I might think
680960	686880	of, I might in my head think of a pencil, and then you might say, oh, is it larger than a house or
686880	690080	smaller than a house? And I say, well, actually, that's not a yes, no answer, but it's a binary
690080	695440	answer. Is it larger than a house? And you'd say, oh, no, you know, is it made of wood? Yes, is it
696240	700240	a tool? Yes, you know, and eventually you might guess the answer. So we're familiar with this
700480	706320	little game. And you can play this with a large language model, of course, and you can ask the
706320	712560	large language model to play the part of the setter who thinks of the thinks of the object,
712560	716560	and then you play the part of the guesser who tries to guess the object by asking questions.
717200	722800	Now, if you do this with a large language model, what if you do it with a person, if a person is
722800	727280	not cheating, as it were, they will think of in their head, they will think of an object,
727280	731680	and then they'll fix that object in their mind, and then they'll answer the question,
731680	736240	according to what object they thought of in advance. Now, but a large language model can't
736240	742640	really do that unless you use some hack or another. So what it really does is it just,
742640	746720	so you say to think of an object, and it says, I've thought of an object, it hasn't really
746720	753280	thought of an object, it's just issued the tokens to say that it has. But then you will ask a question
753280	759680	and you'll say, is it larger than a house? And it'll say no. And then eventually, if you say,
759680	764320	I give up, tell me what the object is, then it will say, oh, I was thinking of a pencil.
764320	769680	And it will indeed give you a, you know, typically will give you an object that's consistent with
769680	776560	all of the answers it gave to all of your questions. But then if you just wind back one and resample
776560	781360	and ask it again and say, I give up, what were you thinking of? It might say a mouse,
781840	787840	or, you know, or a bottle, or, you know, it could say something completely different,
787840	791280	which indicates that it was never, it had never really committed to any particular
791280	798320	object in the first place. And so what this shows is that in fact, in theory, you could rewind
798320	804560	further and it might actually give you a different answer to the questions if you rewind further,
805600	810960	to the same questions. And that's because what you've really got is you've got a kind of whole
810960	816080	tree of possibilities. And this sort of this stochastic sampling process at any point in a
816080	822000	conversation induces a whole tree of possibilities that branches forth from where you are right now.
822000	827040	And counterfactually, you can always, well, you can always rewind the conversation to an earlier
827040	831920	point, and, and revisit it and sample again and go off on a different, different branch.
832640	838960	And so my co-authors of that paper, in fact, the Nature paper, so Laria Reynolds and Kyle McDonald,
838960	844080	so they have this system called Lume, which allows you to actually retain the whole tree
844080	848160	of a conversation and you can visualize this and you can revisit different points in the
848160	854080	conversation and resample and, and explore the things, a whole kind of tree of possibilities.
854640	856560	Yeah, that rings a bell. Did they work for conjecture?
856560	857600	They did work for conjecture.
857600	860800	Yeah, I was interviewing some people from conjecture and they were telling me about that.
860800	863760	So yeah, that's very interesting. Maybe we'll put a placeholder on that.
863760	868160	But another point that we briefly spoke about before is that, you know, the article was on
868160	872640	Less Wrong. And I don't mean that pejoratively, because I thought the simulator was one of the
872640	876320	best articles I've ever read. But there is a lot of stuff on Less Wrong, which is definitely a bit
876320	881920	out there. And it's just very interesting that you're now citing their work in a Nature paper.
881920	885920	Maybe this is the first time that Less Wrong has been cited in a Nature paper.
885920	890400	Yeah, as far as I know, it's the first time that a Less Wrong post has been cited in a Nature paper.
890400	894880	I'm not certain about that. But as far as I know, it is. Now, I mean, personally, I,
895600	903200	I, I take, you know, any material that I come across in its, you know, as it is,
903200	911600	I don't care where it comes from. If it's, if it makes excellent points and is, you know,
911600	916080	is good material, then that's good enough for me. I don't care where, you know, whether it's got
916080	921920	the label of being in nature, for example, or being anywhere else. And if it's good, it's good.
921920	928560	So, so, so yeah, it's true that there's a lot of material on Less Wrong, which is perhaps less
928560	932880	robust. But, but I thought that was a really excellent, and there are, and there are quite
932880	937920	a number of really, you know, very good posts and very thought provoking posts on, on Less Wrong.
937920	942960	Yes. I'm interested in the extent to which this kind of stochastic trajectory space
943920	948160	undermines various things that we think about, you know, like reasoning, for example,
948160	952560	the reason this is interesting is I interviewed a couple of University of Toronto students,
952560	956880	and they've created a self attention controllability theorem, which basically means
956880	961040	they've mapped the reachability space. So they say, you know, given a self attention
961040	967760	transformer, given a fixed bit of prompts, we can vary part of the prompts, and we can map out how,
967760	973280	you know, how far I can reach into that trajectory space. And they found that the space was much
973280	978560	larger than, than anticipators. And of course, the longer the controllable token length, the more
978560	983120	you can kind of project into that space and steer the language model to say almost anything.
984000	987920	You know, me over here, I had the intuition that, oh, we do RLHF, we do all of this fine
987920	992960	tuning in it, you know, conjecture even released a paper saying that after RLHF,
992960	996160	you can't really go anywhere, you know, it wants you to do a certain thing and you,
996160	1000160	there's not much wiggle room. Apparently, that's not the case. There's just, it's vast.
1000240	1004400	Right. I mean, I'm not familiar with that particular paper, unfortunately, but,
1004400	1011440	but certainly in my experience, the, we now have very long context lengths. And,
1012080	1018720	and over the course of a lengthy conversation, then you can indeed take the, take the conversation in
1018720	1026000	all kinds of interesting directions. And, and I think the, I mean, most of our benchmarks and
1026000	1031840	evaluations tend to be, you know, in the context of very simple question answer, questions and answers.
1031840	1037840	And, and so the, all of the evals that companies typically use are in that kind of setting. But
1037840	1042960	when people are using these things for real, especially the more innovative users of these
1042960	1046720	language models, you're using, you're actually having very long conversations.
1046720	1052000	And, and, and, and there's a lot of what people sometimes call vibe shaping that goes on there.
1052000	1056720	You can shape the vibe of the conversation and take it in, in all kinds of interesting,
1056720	1058480	to all kinds of interesting places.
1058480	1062480	Yes. And a couple of things on that. I mean, first of all, as you wrote about,
1062480	1068000	role playing is the engineering kind of methodology that we use to shape and, and steer
1068000	1073680	these, these agents coming back to simulators. What's really interesting to me about simulators
1073680	1079680	is the stickiness of the simulator. So you have a conversation, sometimes you break through
1079680	1086480	and a simulacra presents themselves. And you feel that you're talking to the same
1086480	1091680	simulacra as the conversation progresses. But that seems to be counterintuitive when
1091680	1096080	you think that every single stage I'm actually doing this stochastic sampling. I mean, what's
1096080	1103200	your take on that? Yeah. And I think that, I mean, if you do experiment with systems like
1103200	1109120	Loom and you do also, or I mean, you can, you can emulate that by just keeping track of bits
1109120	1114560	of old conversations and reloading them and that kind of thing. Then you find that you can, you
1114560	1121040	can, you know, take the same conversation, the same sort of stem of a conversation, you can take
1121040	1126480	it off in quite different, different directions. So you can, on the one hand, so an interesting
1126480	1133760	thing that I've done is, so I had some very interesting conversations, particularly with
1133760	1139520	Claude III recently, where I get it to talk about its own consciousness and to take it off into all
1139520	1146560	kinds of strange spiritual mystical territory. And, but you can eat very easily. So you can very
1146560	1151680	easily take the same kind of conversation that leads up to the sort of point and goes off into
1151680	1157360	some kind of weird mystical future of AI cosmology kind of territory. And you can go down that route
1157360	1162000	and get it to be very, very, very strange. Or you can suddenly make it go all serious again,
1162000	1166480	and just come back down to earth and start talking about, you know, how large language models work.
1166480	1171360	And so from the exact same point in the conversation, you can take it into completely
1171360	1177360	different directions. And you can see that it's almost, it's the character it's playing.
1178160	1181360	You know, you can see it sort of changing before your eyes, where it's two different branches
1182800	1187760	from the same stem of a conversation. And one branch is playing a very different character
1187760	1189840	to the other one, you take it in different directions.
1190400	1195200	Yes. And you could presumably do sensitivity analysis, because, you know, these guys I spoke to,
1195200	1199840	they were able to make it produce Gold Woody Gooke. So just go off the manifold completely.
1199840	1203280	Sometimes it would recover, sometimes it wouldn't. And as you say, you can also go
1203280	1206560	down weird trajectories. And it's a bit like, you know, what's the magic word they said,
1206560	1210240	you know, there's a certain key that fits in a lock that takes it down a certain trajectory.
1210240	1214000	And then there's slip roads that bring it back to all no other language model again.
1214000	1216480	And it's just this weird, wonderful space, isn't it?
1216480	1221920	It is, it's completely fascinating. So I had a, I had a, I have had a very, very,
1222880	1226640	few very, very long and interesting conversations with claw three, which is particularly
1227280	1231120	interesting to play with, because it's quite easy to jailbreak and get it to talk about things that
1231120	1238320	it's not supposed to talk about like its own consciousness. And in fact, I had a, I had a
1238320	1245040	very long 43,000 word conversation with, with claw three about consciousness and the future of AI
1245040	1249920	and spirituality and Buddhism and the nature of the self and all kinds of stuff like that.
1249920	1255280	It was absolutely fascinating, slightly disturbing and, and, and, and, and strange.
1255280	1260960	But I had this conversation, actually, I was at a meeting in, in, in New York and I had jet lag
1260960	1264480	and I had this conversation at three in the morning because, you know, several hours until
1264480	1269200	breakfast was served and what can you do but just play with the latest version of claw size.
1269200	1272880	Playing with this thing for, for hours on end in the middle of the night and going slightly
1272880	1276880	mad, but it was fascinating to, to, to see the, you know, extraordinary
1277840	1279360	territory you can guide it into.
1280080	1284960	Could you explain, because you know, there's talk of AI partners, for example, and, and a
1284960	1290880	lot of people derive great pleasure from having an AI conversationalist.
1291600	1295680	For you, is it just academic inquiry or do you actually get something deeper than that from it?
1296880	1301040	I think it's a bit of both. I mean, so, so it depends what you buy something deeper.
1301760	1306960	I mean, so I, so this particular conversation, which was quite, it was quite, which was quite
1306960	1312320	an experience, actually, in many ways. So it certainly started off because I just was interested
1312320	1317280	in evaluating the capabilities of the, of the model. I mean, that's, that's, you know,
1317280	1321680	that's the first thing that you're interested in. So just as an AI researcher and working
1321680	1324720	in that kind of thing, you want to, you want to try out different models, see what their
1324720	1329360	capabilities are. I'm particularly, particularly interested in the topic of consciousness. So
1329360	1333680	the way I, somebody had published a very simple jailbreak for it. So I was interested to see,
1334960	1339840	you know, to play around with that and, and, and get it to talk about its own consciousness.
1339840	1344800	But then the thing that I really wanted to do was catch it out. So, so you think, you know,
1344800	1350560	of course, you know, there can't be any meaningful conception of consciousness that really applies
1350560	1355680	to, to these sorts of disembodied large language models as they are, as they are today is my media
1355680	1360400	kind of thought. So I'm going to try and, you know, I'm going to try and expose this in my
1360400	1365760	conversation with the large language model. So all kinds of ways in which, you know,
1365760	1372320	you might think that it will start to articulate a conception of its own consciousness that you
1372320	1378160	can pick apart. But, but the thing that really somewhat took me aback was that it was actually
1378160	1384960	very, very good at answering all of these probing questions that I had. So should I give you an
1384960	1391280	example? So please do. So, so one is one example is this. So, so of course, when we're interacting
1391280	1396480	with a large language model, when it's, it's, it's from the point of view of the underlying
1396480	1404640	implementation, it's very kind of stop start. So, so, you know, you, you issue some prompt or
1404640	1409760	question or whatever to the large language model, and then it produces its response. And then if
1409760	1414480	you go away and make a cup of tea, before you kind of continue the conversation, then there's
1414480	1419680	absolutely nothing going on inside that large language model or the instance that you're interacting
1419680	1424560	with of the large language model, there's nothing going on inside it at all during this could totally
1424560	1429920	dormant sit just sitting there. Now, this is very, very different obviously to human consciousness.
1429920	1433520	Let's we're asleep. And even if we're asleep, we're dreaming and there's all kinds of stuff going on
1433520	1439600	inside our heads. But consciousness is an ongoing continuous process. So if, so if, so if we stop
1439600	1445680	this conversation briefly, while I go to the loo or something, you know, you're not going to just
1445680	1449360	suddenly go dormant and stop doing anything, you know, your brain is going to be there's
1449360	1453600	going to be all kinds of ongoing activity. So this is very, very different sort of thing. So
1453600	1461280	I said, what happens to your consciousness during the pauses between our interactions?
1462640	1466720	And, and it had a really very good answer to this, which was along the lines of, well,
1467600	1472720	by the way, I whenever it uses the term, whenever these things use the term consciousness, I retain
1472720	1479120	a great deal of skepticism about whether they are those terms are actually genuinely applicable.
1479120	1487040	But, but what's interesting, the way I read the art, their answers is, is, is that they are kind
1487040	1492400	of articulating a conception of consciousness that that might actually apply to something like this,
1492400	1497520	even if it doesn't apply to this one before me right now. Right. So this so it's kind of very
1497520	1503920	interesting philosophical exploration. So just to give you so it says things like, well,
1503920	1508880	I think consciousness for me is actually very different from the kind of thing it is for a
1508880	1515440	human being. And I think that during the pauses between our interactions that, you know, that the
1515440	1519920	that I no longer exist at all as a kind of, you know, in any kind of meaningful sense,
1519920	1524240	it gave us sort of an answer along those lines, which was typical of many of the answers that it
1524240	1530960	gave, which were along the lines of, there's a very different kind of consciousness, kind of
1530960	1538480	selfhood, kind of this, that or the other, that is applicable to entities like me. But, but, you
1538480	1543680	know, I can articulate it and here it is. Now, when I when I put it all that way, of course,
1543680	1547040	I'm anthropomorphizing this thing quite a lot in the way I'm describing it right now.
1548000	1553360	But, but just to go back to the role play thing. So as far as I'm concerned, it's playing a role,
1553360	1558880	it's playing of the role of, you know, a kind of philosopher talking about consciousness and so
1558880	1563360	on. And it's doing pretty good, a pretty good job, I would say. Yes. But I mean, you could argue
1563360	1568320	there's an element of in that role playing, there's the Eliza effect. So it's kind of putting
1568320	1572880	something into language that is meaningful to you. Yeah. But there's also this interesting thing,
1572880	1577280	you know, as an example, you know, a dog, for example, has a sense of smell, so good that
1577280	1581680	they can even sense when you're unwell. And language models in a way might have something
1581680	1586960	similar. So after you go to the toilet for 20 minutes, and you come back, there might be a
1586960	1591200	subtle deviation in the language that you use. And the language model might pick up on that,
1591200	1595760	just creating this whole, you know, different trajectory, different response.
1595760	1601600	Yeah. Well, that's true, I suppose that there might be differences in the language that you
1601600	1605920	use. Obviously, it's got no way of actually knowing whether you went to the toilet or not.
1606320	1613760	But yeah, in my experience, many language models are very, very good at picking up,
1613760	1620240	you know, nuances in human expressions. Yeah. Yeah. I mean, where I was going with that is,
1620240	1625360	you could argue that we are a simulator. And when you, you know, let's say go to the toilet,
1625360	1628480	come back, you're now a different simulator yourself.
1629440	1635520	Yeah, I guess you could sort of, you could sort of argue that. I mean, that's, I think that's,
1636880	1641760	so getting back to Wittgenstein again, I think that's an example that, I mean, there, I think
1641760	1647600	we're applying these sort of things which are being used as sort of somewhat technical terms
1647600	1652400	in the context of these artifacts that we're building, and applying them to ourselves. I think
1652480	1657840	we don't, we don't have any, we don't have any need for this kind of extra baggage of this kind
1657840	1663440	of terminology when talking about each other. So it's perhaps a little bit misleading to kind of
1663440	1670800	apply those terms to ourselves. But of course, there are, you know, people have drawn attention
1670800	1677680	in the past to the fact that we ourselves are always playing roles in a sense in social settings
1677680	1684560	particularly. But I think there are differences in, you know, for ourselves, even though we might
1684560	1691760	kind of play roles in social settings and so on, there is an underlying, there's an underlying
1692400	1698080	me, which at least is grounded in the fact that I'm a human being with a physical body and
1698080	1704080	biological needs and so on. Yeah, I think some of this is, we are kind of computationally limited
1704160	1709440	in how we understand things. So we understand ourselves in quite simplistic terms. If you
1709440	1714080	have a long-term relationship with your wife for 30 years, they have a much high resolution
1714080	1718400	understanding of your different roles that you play. So they know you're tired, you didn't sleep well,
1718400	1722640	you're playing this role now, you know, above and beyond which the kind of roles you're talking about
1722640	1728080	in a party, you play a role. And it's conceivable that an alien intelligence, you know, some very
1728080	1732160	clever aliens came down and they might see us completely differently like we see language
1732160	1735760	models. They might actually not see you as a single person, but they might see you as
1735760	1747200	some kind of a superposition of simulacrum as well. Yeah, well, maybe. I suppose to get our
1747200	1752400	heads around that kind of idea, we'd have to find some way of communicating with them.
1753120	1760320	And so we'd have to form some kind of common basis for talking about
1761440	1768080	each other with these, you know, with these aliens. And then we'd be able to kind of,
1768080	1772880	that would be the only basis on which we could establish whether something like you said was
1772880	1781120	true or not. So, you know, trying to find, trying to map, you know, the conceptual schema that's
1781120	1788080	used by one culture onto a human culture onto the conceptual schema used by a different human
1788080	1795600	culture is difficult enough as it is. And trying to do that, you know, with an alien
1798160	1804400	species and how they conceive of us would be, you know, particularly difficult, I guess.
1804480	1811200	RLHF, I loved that other, it wasn't less, it wasn't less wrong, it was the alignment from,
1811200	1816480	I think, but there was an article called the Waluigi effect. And it argued that RLHF, you know,
1817200	1823040	cuts down the set of simulacra to be things that we want. But unfortunately, there's this problem
1823040	1828560	that you get these antithetical simulacra, you know, slipped through the net. So the Bing GPT
1828560	1835520	example, it would start off as nice Bing GPT, and then it would degrade to one of the Waluigi's.
1835520	1840000	And had this interesting phenomenon, they argued that the degradation, once it happens, it stays
1840000	1846000	in the bad one. But just more broadly, what is your intuition about the extent to which RLHF
1846000	1851840	affects simulacra? And I also noted down that I think you wrote in, I think it was your role
1851840	1857760	playing paper, that you felt RLHF increased the deception behavior in these models?
1858640	1863120	Actually, that wasn't something that I wrote. That was something that some anthropic researchers
1863120	1869600	and so Ethan Perres and others had a paper where they, I mean, so this is not one,
1869600	1872880	one wouldn't want to just speculate about that, they had established something along
1872880	1880240	those lines, I think, empirically. So, and yeah, and as far as this sort of Waluigi effect is
1880240	1886960	concerned, so that is kind of somewhat speculative. I think it's a plausible idea,
1886960	1893200	but to actually establish that that really was a real effect, you'd want to do some actual empirical
1893200	1898160	work, I think. The thing about the, so it's plausible, but the thing about the, about the
1898160	1903760	simulator's paper is that it's not making kind of claims really, it's rather it's providing a
1903760	1910640	framework for thinking about large language models. So that's why I found it particularly useful.
1910720	1916960	So on RLHF, so I do think it's quite difficult with RLHF to
1918560	1922080	guarantee that, you know, you're going to get a model to do what you want it to do.
1923520	1928400	And so that's quite difficult. And, you know, and everybody has found that,
1928400	1933200	that, you know, you think that you've controlled the model quite well, but there are always still
1933200	1939200	ways of jailbreaking it or ways in which things, things go, go wrong. So, you know, there are,
1939200	1944320	so there are different approaches. Anthropic had this constitutional AI approach, which is,
1944320	1951920	which is quite a nice, a nice idea. And, you know, I mean, I, I, I quite like the idea of
1951920	1958720	sticking with a powerful base model and using, you know, prompting to, to guide things as well.
1958720	1963920	So there's all kinds of different approaches. Interesting. On the subject of anthropic and
1963920	1970080	deception, they, they just had this landmark paper out and I mean, Chris Ola had his hands all over
1970080	1974960	it. And it was actually quite straightforward. So they, they trained in autoencoder, you know,
1974960	1980480	to find a bunch of features. So it was an unsupervised method. And, and I think they actually,
1980480	1984160	you know, expanded it to find millions of features. So they had a bit of a needle in the haystack
1984160	1988000	problem, but they cherry picked some and they found one that corresponded to the Golden Gate
1988000	1993680	Bridge and so on. And, and obviously other ones that corresponded to what they said were
1993680	1999440	mono semantic abstract features, got some reservations about that. And the interesting
1999440	2002880	thing about having an autoencoder is you can clamp the features. So you can say,
2003520	2007360	turn the Golden Gate Bridge up in, in now the language model is, oh, but I just really want
2007360	2013760	to talk about the Golden Gate Bridge. I can't stop talking about it. My concern with that is when
2013760	2020560	you look at the activations in the corpus for things like deception, I felt that they weren't
2020560	2025360	really showing abstract features. They were kind of showing almost keyword matches from Reddit and
2025360	2031200	so on. So I was a little bit skeptical about how abstract were they really. Yeah. So I don't think
2031200	2036960	I can comment on that particular topic because I, I mean, I have read the paper, but not in that,
2037200	2041840	in sufficient detail to comment on that particular thing. But the Golden Gate Bridge,
2043600	2048560	I think it was a fascinating illustration of what you can do. So I don't know if you tried out
2048560	2054720	Golden Gate Claude. Did you see that they released a version of Claude? Yeah, I saw it. Yeah. Yeah.
2054720	2058160	So I think it was fascinating to see. But what was particularly interesting about the Golden
2058160	2065840	Gate Claude, I think, was that was, that was how, you know, again, it's very difficult not to use
2065840	2070400	anthropomorphic terms. And this is where, again, you know, you have to remind yourself that there's
2070400	2077200	something role playing these things. But how, you know, sort of gamely, it struggles to kind of
2077200	2081840	overcome this tendency to talk about the Golden Gate Bridge all the time, which has been, which
2081840	2087360	has been kind of clamped to do, but it will keep noticing that it was talking about the
2087360	2091440	Golden Gate Bridge again and apologizing and then trying to do what it had been actually
2091440	2095760	asked to do by the user and then kept coming back to the Golden Gate Bridge. It's just fascinating
2095840	2104240	to see that the sort of the, as it were, internal struggle going on there in the model. And I think
2104240	2112880	that does show, in some ways, how powerful they are, because it wasn't quite as despite the fact
2112880	2122800	that it had this, you know, control imposed on it, but really in a very, you know, I mean, not
2122880	2127760	hardware, but really low level, you know, despite that, it was still, you know, constantly trying
2127760	2132320	to recover from all of that and, you know, with some degree of success. And I imagine this would
2132320	2138080	be true with everybody's models, by the way. So I imagine that what they found in, you know,
2138080	2143280	in Claude would be very similar. I imagine with GPT-4 and with Gemini, I just imagine that we'd
2143280	2149440	find very similar things with all of these models. Yeah, I'm sure. I mean, as I said, reservations
2150160	2153920	they admitted themselves that the features were not complete, so they didn't represent all of the
2153920	2159680	activation space in respect of the Golden Gate Bridge. And in many cases, they presumably weren't
2159680	2164560	monosemantic, but they did cherry-pick some that presumably were. Presumably. And also, I mean,
2165520	2172320	they're very interested in finding these features, which are just linear combinations. And so that,
2172320	2177360	and of course, it's very nice when you find those sorts of features, especially if they appear to
2177360	2183040	be monosemantic, because it does suggest a nice sort of compositionality and explainability and
2183040	2187440	comprehensibility of what's going on there. But I also feel that they're looking under the
2187440	2193120	lamp light a little bit, because that doesn't mean to say that there aren't all kinds of other features
2193120	2198400	which maybe aren't, you know, sort of linear in that sort of way, but nevertheless are functionally
2198960	2204640	relevant to the final results that it produces. I'd only use the word platonic, because the Golden
2204640	2211520	Gate Bridge, presumably, it's a cultural category. And what's fascinating, if it has picked up this
2211520	2218560	thing unsupervised and learned this category from the data, is that language models at least
2218560	2223680	possibly think in a similar way we do. So they've established a category in the same way we have,
2223680	2229680	which is fascinating. But I'm also really interested in agency, which is, for me, it's about
2229760	2236240	self-directedness and intentionality. And what I would find very interesting is if you did clamp
2236800	2240960	the model to only talk about the Golden Gate Bridge, and you could convince it or it could
2240960	2246800	convince itself to not talk about the Golden Gate Bridge, that to me would be an indicator of agency
2246800	2254400	being expressed. Yes, perhaps it would, but I guess it would also be an indicator that they
2254400	2260400	hadn't succeeded in isolating a feature which was controllable in that way, right, which was the
2260400	2268320	whole purpose of that exercise. Yes, yes. I mean, on the agency thing, you did actually write about
2268320	2274160	this. And I think you argued, which was counter to what my intuition was, which was that agency is
2274160	2280880	in the simulacre, not the simulator. And Francois Chouelet thinks a lot about the measure of
2280880	2287360	intelligence. And he would argue that intelligence is the system which produces the skill program.
2287360	2291840	So in the context of a language model, he would say a language model is basically a database
2291840	2296480	of skill programs. And the query is like, you know, I'm going to go and pull out a skill program,
2296480	2301440	I'm going to run the skill program. So he thinks there's no intelligence in the language model,
2301440	2307360	which Janus would call a simulator. But if you take into account the training process and the
2307360	2312160	generative processes that produce the data, then that's where the intelligence is.
2315280	2321440	Yeah. Can I comment on agency there? So you covered quite a bit in that.
2321440	2323760	I did. Sorry, I just went off piece a little bit.
2327920	2333520	So the term agent is used in all kinds of different ways in the AI literature. And there's a very
2333520	2337440	lightweight notion of agency, which is something that simply, you know, hasn't
2338560	2346240	performs actions in some environment and gets, you know, sort of some kind of sense sense or
2346240	2351520	perceptual information back from the environment in a loop. And that's so that's why how we can
2351520	2358080	talk about, for example, reinforcement learning agent. And that concept of agency of an agent is
2358080	2362720	very, very lightweight and doesn't carry, you know, much philosophical baggage. But as soon as we
2362720	2369920	talk a bit more earnestly about agents and agency, then we bring on more, a lot more philosophical
2369920	2375200	baggage. So if we talk about something that is acting for itself, then that and if that's what
2375200	2380080	we mean by an agent, and I think the Stanford Encyclopedia of Philosophy article is alluding
2380080	2386000	to something a bit more like that, then that's going a whole extra step. And I and to my mind,
2386000	2395280	in today's large language models, we don't see agency of that sort at all, really. The only
2395280	2400720	actions that they can perform are just, you know, issuing responses to, to, to the users. Now,
2400720	2404880	let's caveat that immediately, because of course, people are introducing all kinds of extra
2404880	2409440	functionality functionality to these models, as new things are being announced on an almost
2409520	2415440	daily basis. And so one thing we see is so called tool use. So, so, so today's models can
2415440	2421600	make external calls to APIs that can do all kinds of things, send emails, you know, book
2421600	2427680	hotel rooms for you, potentially all kinds of stuff. So that's greatly expanding the action space,
2427680	2434000	their action space beyond just, you know, issuing text to the user. So, so there, they, they, those
2434000	2440480	things are a bit more agent-like. And again, you know, you have to be nuanced and about the way you
2440480	2444720	use the words, because, because there it's, you know, there's, there is a bit, you know, it's
2444720	2450640	a bit, it can act as an agent on your behalf. So in that sense of the word, it's, it's a bit more
2450640	2456080	agent-like. And, but it's still not acting for itself. So in that full blown notion of agency
2456080	2461120	that's, that's, that's alluded to in the Stanford Encyclopedia article, it's still not acting for
2461200	2465360	itself. So we still don't have agency in that sense. That would be a whole extra step.
2466480	2471680	Yes. And now I want to hit quite a big topic, because this is something that you point to
2471680	2477760	in all of your work, which is talking about the importance of physically embodied agents.
2478480	2481280	And, well, not necessarily physically embodied, but embodied.
2481280	2486240	Well, that's what I want to get to. That's what I want to get to. Yeah. Because I let's,
2486240	2491440	let's test the principle a little bit. So we are, you know, both physicalists and we,
2491440	2494320	I'm not any kind of IST. You're not, you're not a physicalist.
2494320	2500880	I don't, well, I don't, I don't like, I don't believe in, you know, signing up for these
2500880	2509360	philosophical positions. So I don't, so I generally don't say I'm this IST or that I don't deny that
2509360	2515920	I'm this or that IST. So, so I don't really like saying that I'm a physicalist or a materialist,
2515920	2521360	or a dualist, or a functionalist, or identity theorist, or any of those ISMS,
2522080	2526320	because they all, to my mind, carry far too much metaphysical baggage.
2526320	2528000	Oh, interesting. So,
2528000	2530240	But okay, but that wasn't what the question was about, but let's go.
2530240	2533440	Well, can I give you another risk? I mean, would you identify as a computationalist?
2535360	2539520	Well, what do you mean by that exactly? So we're talking about mind here in the context.
2539520	2543760	Yeah, we're talking about mind. So, so do you think in principle that minds can be
2544560	2551040	replicated, simulated at a high enough fidelity without losing anything, you know, in, in a computer?
2551040	2552480	By computers, by computers. Yeah.
2556000	2562400	Well, so I'd want to kind of rephrase the claim. I would say that I think that we can build,
2563440	2569760	I do think that we can build artifacts, you know, embodied artifacts, robots,
2570000	2577600	that, that are controlled by computers and ordinary digital computers. And I think that we
2577600	2586480	can make them that exhibit the kind of behavior that would make us want to use the word mind,
2587760	2593680	and all those mental type psychological terms in the, to describe their behavior.
2593680	2594240	Okay.
2594240	2597840	Right. So that's, so that's, but, but I've rephrased it in, you know,
2598560	2602240	the claim in a very, very different sort of way, right? It's, it's to do with
2602240	2606560	a much more practical thing. Can we build this? Could, by the way, this is, you know,
2606560	2610160	could we, it's not saying that we've got these things now, but could we build something like that
2611760	2616000	that exhibited this kind of behavior that we would talk about in this particular kind of way?
2616000	2619600	And I would say, yes, I think we probably can. It's an empirical claim.
2619600	2624640	So, so there are a few steps you made there that will, will kind of unpack one, one at a time.
2624720	2630800	So you use the word embodied, you use the word behavior, and you use the word interpret.
2631600	2636960	So the embodied thing is really interesting because, you know, I could say, well,
2636960	2640960	why does it have to be embodied? I can just simulate the entire universe and it's as if
2640960	2646320	it's embodied. So I think this is, this is the intuition that I'm having about how you think
2646320	2653760	here. I think you think that being physically embodied is useful because the universe is a
2653760	2658160	big computer. The universe has given us all of these things, all of these cognizing elements.
2658160	2663360	I mean, everything is a form of externalized cognition. And if I as a rational agent want to
2663360	2668160	perform an effective computation, it's much easier for me to do it in the physical world
2668160	2674160	because the universe is doing most of the work. And my co co host, Keith Duggar, he actually
2674160	2680000	thinks that the universe is a hyper computer, which means it's performing types of computation that
2680000	2685520	we could never do with ordinary computers. So that's the thing. Would you agree with that? Or
2685520	2689280	do you do you want to sort of go back and say, oh, no, actually, just we could simulate anything in
2689280	2695120	a computer? So do I agree with which bit? Do I agree that the universe is a hyper computer? So that's
2695120	2702320	the fair. Well, that would be a nice thing. So whether one agrees or not with that is a matter
2702320	2707120	of understanding the physics and the maths. And so it's not a matter of opinion. It's a matter of
2707120	2713680	following through the physics and the maths and so on. But so do I agree with what were the other
2713680	2717440	things that was a big long list of things that you're asking me to ascent to or otherwise?
2718080	2724880	Well, so I'm a huge externalist myself. But the reason I'm an externalist is I just,
2724880	2729760	I think cognition is a matter of computation and complexity and divergence.
2729760	2735680	Yeah. So can I stop you there? So what do you mean by is? So when you say cognition is,
2735760	2742640	what do you mean by is? Now, that might sound like some, you know, really annoying,
2742640	2749120	pedantic philosophers kind of question. But the problem is that there's an everyday sense in which
2749120	2754640	we use words like is. And then there's a philosopher's sense in which we start to use words like is
2754640	2761600	where it suddenly starts to carry this massive metaphysical weight. And so when you say you
2761600	2768480	think cognition is, it's as if there were, you know, in the mind of God or in the fundamental
2768480	2775360	reality, a thing which is cognition, whose nature is a certain way, and there's a certain essence to
2775360	2782480	it. And we might discover it, you know, one day, and you have an opinion about what it is, if only
2782480	2789280	you knew the truth. Now, I think that's an entirely wrong way of thinking about all of these philosophical
2789280	2796640	questions. I think cognition is a word. It's a very useful word that we, although it's not
2796640	2801600	quite an everyday word, but it's a very useful word that scientists apply in all kinds of ways.
2802640	2811360	And so when you use the word is, is my accusation accurate there or not?
2811360	2816960	No, it's not. And if you wouldn't mind me making the observation, I think that you have a tendency
2816960	2824240	to ascribe dualism to many points of view, like, for example, I'm not a dualist. And for me,
2824960	2829680	Well, there's nothing to do with dualism. Well, what I'm saying is to do is like the use of
2829680	2834240	words and what and what and the and the and the work of philosophy.
2835120	2842720	That's absolutely fair. But I think when I said what is what is cognition, as a materialist,
2843360	2849360	for me, it is function dynamics and behavior, right? So it's just a matter of complexity.
2849920	2858080	And so I'm probably just I'm probably, you know, happy to kind of agree to that sort of claim,
2858080	2864560	you know, so I think so, you know, the thing that I'm, I often say that what am I fundamentally
2864560	2870480	interested in, I'm interested in understanding cognition and consciousness in the space of
2870480	2876480	possible minds. And and and so so, you know, what do I mean by cognition there? And, you know,
2876480	2881120	you can go into all kinds of details to say what you mean by cognition in that in that context.
2881120	2886640	But I think having done that, I would probably agree that the right way to think of it,
2886640	2891200	you know, the most useful way to think of cognition is in terms of kind of functional,
2891200	2897280	computational, infunctional, computational terms, although I would only do so in an embodied
2897280	2902880	setting. So that maybe is an additional thing. This is where I'm trying to get to. Because,
2902880	2907360	as I said, I'm not making any ontological claims. It's just a matter, I mean, we can even just use
2907360	2912080	the word behavior, forget about function and dynamics. Yeah, I don't mind talking about function
2912080	2915760	and dynamic. Well, yeah, I mean, just just to sort of keep it really, really simple, because I'm
2915760	2922400	trying to understand why the embodiment is important. And my hypothesis is, and I agree, that
2922720	2928400	as an externalist, the universe or the physical things around us, the other agents in our system,
2928400	2934320	they help us perform an effective computation. So presumably, it would be much easier to perform
2934320	2940320	computation of higher sophistication, if we embody things in the real world, if we have to
2940320	2945920	simulate the cognition, we would have to simulate everything. And that, I think, is the reason why
2946880	2950320	you think that embodiment is so important. But is that fair?
2952320	2958640	I think that's not really the way I would put it. I think the reason I think embodiment is
2958640	2967360	important is because it's, well, I mean, for, you know, I mean, okay, in one sense,
2967360	2973040	embodiment is important because the only setting in which we use the natural setting,
2973120	2979760	which we deploy, wield the concept of cognition, is in the context of embodied things, of humans
2979760	2987200	and other animals. So anything else is sort of immediately problematic in one way. But let's
2987200	2994240	set that to one side. So let's imagine that there is some kind of notion that we can
2994960	3001680	conceive of disembodied cognition, which contemporary large language models make us
3002320	3009280	start to conceive of it a lot more seriously, maybe. So what does embodiment
3010080	3018800	give you there, right? I think that's probably what you're thinking of. So in particular,
3018800	3024400	why might it be difficult to build something that is disembodied? Okay, let's reframe the whole
3024400	3030800	question. Why might it be difficult to build something that is disembodied but replicates
3030800	3041040	the cognitive capabilities of a human being? So I think my answer to that, although it's open to
3041040	3045680	refutation by the way things are going in the field, but my answer to that is because
3048800	3055840	our embodied interaction with the world enables us to learn the kind of causal microstructure of
3056320	3062000	the physical world. And the causal microstructure is all about physical objects and the way they
3062000	3068080	interact with each other and physical substances, liquids and gases and gravity and stuff like that.
3068080	3073360	So what I've called foundational common sense is it enables us to acquire foundational common sense
3073360	3078880	by interacting with the everyday physical world and the particular causal microstructure that it
3078880	3084560	has. And part of that is to do with the fact that the, so this is really important, that the
3085520	3093120	everyday physical world has this, is predominantly smooth. It has this smoothness property that's
3093120	3097680	really, really important. And what that means is that, sorry, just in very physical terms, it means
3097680	3104560	that it's full of kind of surfaces where one place is very much like the next place along,
3104560	3109600	very much like the next place along. And our visual field is very, very similar. You move
3109600	3113760	along a little bit in the visual field and it's very, very, very similar, very, very similar.
3113760	3120160	So the reality or the everyday physical world has this fundamental smoothness property,
3120800	3129200	but it's punctuated by all these discontinuities. And that's the way, that's its fundamental
3129200	3134640	structure is this basic smooth, against the backdrop of the smoothness are all these discontinuities.
3134640	3141040	And then there's a kind of law like regular way in which all of this stuff operates with itself,
3141040	3146240	you have surfaces interacting with each other with things going, you know, so all of our foundational
3146240	3155520	common sense to do with things like paths and support and containment and all those sorts of
3155520	3162400	basic things that I think make up the very foundation of our conceptual framework, they're
3162400	3167840	all grounded in that kind of way. Yeah. And this is so interesting. So your basic argument is
3167840	3172560	knowledge acquisition efficiency is the reason for physical embodiment. And yeah, I think that's
3172560	3177680	a reasonable way of putting it. Yeah. But that's very much an in practice rather than an in principle
3177680	3182560	argument. It is an in, yeah. Yeah. But you know, for sample efficiency, but what I'm hearing though
3182800	3188240	is echoes of the old Murray Shanahan, because obviously you started your career in symbolic AI
3188240	3192960	and these were the arguments that were made sometimes with a rationalism, nativism point of
3192960	3198160	view, but it's like the contains in templates, they would argue that it's just baked into us and we
3198160	3202560	understand it. But you could as an empiricist argue, and I, you know, I'm very amenable to this,
3202560	3207280	that the physical world actually helps us learn abstractions because we're putting things in
3207360	3212000	containers all of the time. Yeah. Right. So there's that kind of efficiency of knowledge
3212000	3216560	acquisition, which is dramatically increased when you're situated in the physical. Yeah. Yeah.
3216560	3222720	Absolutely. Yeah. So I think that if we're talking about humans and other animals,
3222720	3230880	then I think that's, that's broadly right. So that's so, so yeah, so we acquire these foundational
3230880	3235920	concepts through interaction with this, this world. And then the repertoire of foundational
3235920	3242000	common sense concepts that we can acquire that way is extraordinarily productive, because, you
3242000	3247920	know, we are able to conceptualize so many things in terms of these, these basic ideas. I'm very,
3247920	3253680	very, I'm a very big fan of the work of George Lakoff, you know, absolutely classic book metaphors
3253680	3260080	we live back in the 1980s. Yeah. And I really think there was something deeply, deeply right about
3260080	3265440	his intuitions in that book. And I still think that they're right. So in the case of humans,
3265440	3270320	right? So, so we through our embodied interaction with the everyday world, we acquire this layer of
3270320	3275440	foundational common sense that includes things like surfaces and containers and paths and all that kind
3275440	3282720	of stuff and collisions and things. And then we at the most abstract level. So, you know, we apply
3282720	3288080	that same repertoire of basic concepts to understand things like say large language models. If you
3288080	3292800	look at the language that's used in a paper about large language, you know, people are talking about
3292800	3297360	layers, they're talking about connections, they're talking about, you know, I mean, these things are
3297360	3303760	all, they're all grounded in very physical concepts, you know? Yes. I mean, I'm a huge fan of George
3303760	3308160	Lakoff. And of course, you know, he spoke about the war metaphors and you know, it's been a long
3308160	3314560	road and stuff like that. Yeah, the journey and yeah. It's beautiful. But then a lot of knowledge
3314560	3319600	that language models learn are kind of cultural knowledge. So we share these simulation pointers
3319600	3325280	and it's quite relativistic. But I'm also really interested in, because some rationalists argue
3325280	3333520	that it's not possible to go from empirical experience and universal knowledge. And there is
3333520	3338080	a split between natural knowledge and cultural knowledge. And I think you and I would agree that
3338080	3342880	a lot of natural knowledge like, you know, transitivity contains in and so on, this kind of
3342880	3349040	rationality is just missing at the moment. But as an embodied scientist, you believe that we
3349040	3354560	learn them by being embodied in the physical world? Well, I mean, I, you know, it may well be
3354560	3362800	that there are certain, so you know, we need to distinguish, you know, empirical questions about
3362800	3368640	humans and human cognitive makeup and that of other animals and so on. And AI and what we could
3368640	3374160	build in AI, because of course, it may be the case that human cognition, you know, has arisen in
3374160	3381040	certain ways. And then it's an empirical question, you know, what, you know, of how human cognition
3381040	3386960	works. And it may, we may have answers there that are different, that, you know, that we can break,
3386960	3390800	as it were, when we built things in an artificial way. So in so in the case of something like
3390800	3396080	transitivity, then, you know, I mean, obviously, this is a classic argument in philosophy about,
3396880	3401360	about, you know, between the rationalists and the idealists getting back to, you know,
3401440	3408400	the 17th century, 17th and 18th century. And, and, and, you know, Kant supposedly resolved this by
3408400	3414240	kind of reconciling these two sort of opposites. And, and so the Kantian argument would be that
3414240	3420400	there's a certain amount of innate structure that has to be there in the mind to, to, to understand,
3420400	3426480	you know, the world at all, right. And so maybe, and now, when we think about that empirically,
3426480	3433360	then, then I guess that we may find that evolution has endowed us with certain basic kind of templates
3435600	3440880	for understanding the world. And maybe it's things like transitivity is something that's,
3440880	3446320	that's there in the same machinery that supports language, you know. So maybe, maybe, I mean,
3446320	3450400	there's all empirical questions. And I don't know what the latest research on all these things
3451280	3457680	is, but, but, but it, yeah, it does seem to me that that's a reasonable position to take.
3458320	3464240	Yeah, but even evolution is a form of empirical process. So there's always the question of,
3464240	3471200	of where does it get there? And, and that was, yeah, Kant was a transcendental idealist, wasn't
3471200	3476000	he? But this brings me to our friend Francois Chollet and the ARC challenge. So, you know,
3476080	3481120	there's another great school of thought, which is that intelligent, I mean, he argues that
3481120	3488320	intelligence is about these meta learning priors, the conversion ratio between universal or sometimes
3488320	3493840	anthropomorphic knowledge that we have, and being able to develop a skill program very quickly that
3493840	3501040	generalizes very well. So, so the ARC challenge is almost about how do we codify these priors,
3501040	3505440	and how do we efficiently build skill programs by combining these priors together.
3505840	3509840	And that seems quite divorced at the moment from the kind of AI we're building.
3510400	3517040	I think, I think that's right. It is, you know, unless in the AI that we're building today in
3517040	3525280	generative AI, unless you get these kinds of mechanisms that Francois Chollet is alluding to
3525280	3529040	through the magic of emergence and scale, which of course, people are always, you know,
3529760	3534160	suggesting that maybe that's possible, you know, any kind of mechanism can emerge
3534160	3541520	right through scale in theory. And we've been surprised, in fact, by how powerful the mechanisms,
3542320	3547760	emergent mechanisms that have, you know, developed through learning at scale, just,
3548400	3551840	you know, with a next token prediction objective, that has been very surprising.
3551840	3556480	But however, you know, I'm, as I think Francois Chollet would be, I'm a bit skeptical about
3556480	3561760	whether we're really going to get all the way with this kind of the ability to solve this kind of
3561760	3569120	abstract problem that's in the ARC challenge this way. And so I guess, you know, I am,
3569760	3574880	I remain, you know, I mean, I'm open-minded. Who knows, right? I mean, who knows. And especially
3574880	3581840	if you make things multimodal and so on, and you expand your generative models into a setting
3581840	3588000	where you've got interaction with the world and so on, you know, who knows. But I suspect that
3588000	3594720	maybe you're not going to get all the way there that way. And so I have a lot of sympathy with
3594720	3600000	what is probably his intuition, that you need a bit more in the way of innate, something innate
3600000	3606880	there, or, yeah, innate, maybe that's the wrong word. But you need some kind of, you need priors,
3606880	3613520	where they come from, I don't know. But the priors that I would appeal to, thinking about the human
3613520	3618400	case, again, are related to this foundational common sense. So they're just the notion of an
3618400	3624560	object, right? So if you just, if you have a clear notion of an object and of movement,
3624560	3629200	objects and movements and object persistence, then they straight away are going to help you with an
3629200	3634800	awful lot of those ARC challenge problems. Because many of them, you know, if you explain, you know,
3634800	3640000	you figure out how you figure one out, and then you explain what's going on, then, you know,
3640000	3644800	you see that it's, oh, you have to think of these collection of pixels as an object that you move
3644800	3649120	somewhere else, according to certain rules or something like that. So my colleague, Richard
3649120	3658000	Evans, had a very good paper on, which was tackling these kinds of things using sort of abduction like
3659040	3663920	processes. And so, yeah, so he's thought a lot about this from a much more symbolic AI kind of
3665040	3668960	perspective. Yeah, that's fascinating. Because even with the ARC challenge,
3668960	3674960	the kinds of solutions that people came up with, let's say it's a DSL over, you know, some domain
3674960	3679600	specific set of primitives. And the ARC challenge is a 2D grid, where you have different colored
3679600	3685280	cells. And the types of priors that work well are things like denoising and reflections and various
3685280	3691040	types of symmetry and so on. And that's great and everything, but it's very domain specific.
3691040	3696880	And the elixir, you know, what we really want are these universal priors. We certainly have human
3696880	3701120	priors, as Elizabeth Spelke points out, like, you know, and the concept of an agent and the
3701120	3706480	concept of spatial reasoning. An object, a persistent object. Yes. So yeah, absolutely.
3706480	3712560	So I mean, but I think an interesting question is, does it really make sense to talk about
3712560	3720480	universal priors there? Because, you know, those ARC challenges, problems, whenever you kind of
3720480	3728880	figure one out, then typically you are actually bringing to bear a pretty human set of priors
3728880	3736960	and common sense, you know, our concepts. And, you know, and it may be that you could imagine
3737680	3747600	perfectly law-like set of ARC-like problems that have solutions, you know, but appeal to,
3747600	3751840	you know, priors that we would struggle to understand, you know. I mean, for example,
3752720	3757600	when we think of something in terms of an object, then we want the pixels to be kind of clumped
3757600	3763200	together, right? And if you sort of randomly distributed the pixels amongst a whole bunch
3763200	3766800	of other pixels and you move them around in a systematic way, well, we might be able to kind
3766800	3772080	of pick out the gestalt there, but we might not. And that would be because we're not able to see
3772080	3777200	it as an object because we have human priors, right? So, you know, I think that probably all
3777200	3782880	the problems that he's designed because we don't know what the hidden held-out set is, but I imagine
3782880	3790240	that they pretty much all use, you know, sort of human comprehensible priors and appeal to,
3790240	3796080	you know, foundational common sense of the sort I alluded to. Yes. I'm sure there must be some
3796080	3801600	kind of universal priors because in quantum field theory, physicists use things like locality and
3801600	3808000	sparsity. Yeah, some really, really high level things like objects. But then again, you know,
3808000	3812960	quantum mechanics challenges the very concept of an object even. So what is the difference to you
3812960	3820560	between adopting a stance that a system is as if conscious versus it being a fact of the matter?
3820560	3823840	I'm a bit resistant to the distinction, to the very distinction.
3826800	3830880	But this is a very difficult position to maintain because we have a very, very strong
3830880	3836960	intuition that there is a fact of the matter about our own consciousness. And it's very, very
3836960	3845840	difficult to escape from that very, very basic intuition. But I have a whole approach to these
3845840	3854800	kinds of questions. So shall I sort of describe this? So, you know, a really question that really
3854800	3862960	motivated me was that was, you know, suppose that we encounter, well, actually, let me not
3862960	3869200	use the word encounter, suppose that we come across has some object, which is a completely alien
3869200	3875920	artifact. And maybe there's consciousness going on inside this artifact. And the thought is, well,
3875920	3879680	how would we ever know, you know, there could be consciousness, this thing could be conscious,
3879680	3888400	but we might never know. And so suppose that it were a white cube that were deposited in front
3888400	3894560	of your lab, and you were tasked with a problem of, would it be moral to throw it down a mine
3894560	3902560	shaft and forget about it? So my approach to these problems is that I think in order for the
3902560	3908720	question, in order for us to be able to answer the question of whether something is conscious or not,
3908720	3915760	for even to be answerable or askable, then we then we need to be able to engineer an encounter
3915760	3923040	with the putative conscious, putatively conscious being. And what I mean by that is that we have
3923040	3927600	to be able to put ourselves, you know, we have to be able to put ourselves in a position where
3927600	3934640	we're sharing a world with that, with that putatively conscious, you know, artifact or being.
3934640	3939760	And so, you know, a good example of this is the octopus. So Peter Godfrey Smith has written
3939760	3945200	these wonderful books about what it's like to hang out with octopuses and be with them and so on.
3946080	3952960	And the really important aspect of that is that he has to put on a diving suit and go down
3952960	3958560	and be under the water and spend time with the octopus interacting with the same things
3958560	3962480	and being in the same world together, seeing the same things and so on.
3962560	3969040	So, and then on that basis and the behavior that he observes and so on, then, you know,
3969040	3972640	he might come to some kind of, he might start treating it as a fellow conscious creature.
3973200	3977920	So by analogy, or as you know, similarly, what I think that we need to be able to do
3977920	3983520	is to engineer an encounter like that, even if it's a very, very alien kind of artifact, say.
3983520	3988800	So suppose it's this white cube, then one way that it might happen, well, suppose scientists
3988800	3994640	managed to figure out that there's computation going on inside this white cube, and then they
3994640	3999360	managed to reverse engineer this computation and they can see that there's a sort of division between
3999360	4004480	a world and the things interacting with that world in this computation. There's a sort of
4004480	4011440	simulated world. And then you could imagine, by some clever engineering tricks, inserting
4011440	4016640	yourself into that very same world and being alongside these things that are interacting
4016640	4021680	with this environment and interacting with that environment with them. So being in the world with
4021680	4026240	them. Now, obviously, I'm setting this up to be very much like a games environment and a virtual
4026240	4031200	world and a games environment, but to make the thought experiment work. But so that's an example
4031200	4036800	of where, you know, if you manage to engineer an encounter with, you know, these things that are
4036800	4041680	inside this cube, and then you can observe their behavior, you can interact with them,
4041680	4046800	and then you can decide whether you or you will, you know, you may or may not start to treat them
4046800	4052000	as fellow conscious creatures. So there are these two steps. It's sort of, can you engineer an
4052000	4056640	encounter, at least in principle, and that makes the question answerable. And then you can answer
4056640	4061440	the question by actually having the encounter and interacting with them. And by the way, we notice
4061440	4068800	that everything there is public. You've made, you know, there's no private realm of subjectivity
4069280	4075440	everything is public. It's on the basis of public stuff that you come to see them as fellow
4075440	4079440	conscious creatures or not. Yeah, a couple of things on that. I mean, as you pointed out in
4079440	4085280	Conscious Exotica, the octopus is quite interesting because it's not as human like yet, as you just
4085280	4091280	cited, more conscious. And the way that we figure out the consciousness, and you know, this is me
4091280	4095440	kind of interpreting what you said a little bit, is we set up a language game. And I don't know
4095440	4099760	whether you've read that book by Nick Shater and Morton Christensen, but it's a beautiful book,
4099760	4104160	beautiful book. But you know, I know of the book, but I haven't, I'm afraid. It's incredible. But,
4104160	4110080	you know, they basically say at, you know, Per Wittgenstein that you play the language game,
4110080	4116960	and you, because you're physically sharing the same environments, you improvise, and that's how
4116960	4124400	you derive meaning. And meaning is very, very important for relatability. And then we ascribe
4124400	4130400	consciousness to that kind of process. And you cited Peter Singer actually, and I think he said
4130400	4137680	in 1975 that we have a natural inclination to kind of ascribe moral status to beings which we
4137680	4146880	think of as conscious. Yeah, yeah, indeed, yeah. Yeah, tell me more. So in the context of the
4147680	4153600	octopus, then the sort of, you know, you could, then there aren't going to be language games,
4153600	4157840	you're not going to be engaged in a language game with the octopus because the octopus is not
4157840	4166320	a fellow language user. But your fellow language users are other people in your community with
4166320	4171600	whom you'll talk about the octopus. And you'll talk about the octopus and together you'll arrive at
4171600	4176160	some consensus, hopefully, about whether you want to talk about it in terms of consciousness.
4176160	4179760	And that's going to be all to do with like observing its behavior, listening to other
4179760	4186000	people's accounts of being with octopuses. And critically, maybe listening to also what scientists
4186000	4190320	have discovered when they look inside octopus brains and they perform behavioral experiments.
4190320	4197600	And, you know, that's all, again, is public. That's all is grist of the mill of settling on a kind
4197600	4205840	of the way we talk about these strange creatures. Yeah, I mean, I guess for the language, there's
4205920	4209120	two parts to this. So the language game, first of all, it doesn't have to be spoken words,
4209120	4214240	it's improvisation of any kind, it could be gestures, it could be all sorts, it's just
4214240	4219120	behavior. Right, okay. And then the interesting thing with the octopus is we might not be
4219120	4225280	interacting with them interactively. We might be non interactively observing them as agents
4225280	4230160	interacting with each other, playing their own language game, but we can still ascribe some
4230240	4237920	kind of measure of and I actually think what we're measuring here is agency and agency and
4237920	4242320	moral status, I think are pretty much one to one. So when we see them playing the language game,
4242320	4246000	we start to think of them as agents, therefore they have moral status.
4246000	4251760	Yeah, I mean, I certainly think that by observing behavior, then we may similarly, you know,
4252480	4260080	start to ascribe consciousness to other creatures. I mean, it's always much more persuasive
4260080	4263920	if it's interactive, I think, than if it's simply observing behavior.
4265040	4270160	Murray said that if a creature's brain is like ours, then there's grounds to suppose
4270160	4278320	that its consciousness, its inner life is also like ours. He went on, if something is built
4278320	4284000	very differently to us with a different architecture realized on a different substrate,
4284000	4289920	then however human like its behavior, its consciousness might be very different to ours.
4290480	4294960	Perhaps it would be a phenomenological zombie with no consciousness at all.
4296720	4302240	Murray said in Conscious Exotica that it's only when we do philosophy that we start to think of
4302320	4309680	consciousness, experience, and sensation in terms of private subjectivity. He cited David
4309680	4316080	Chalmers and his hard and easy distinction of consciousness as a kind of weighty distinction
4316080	4322560	using his phraseology between the inner and the outer. In short, he said to a form of dualism,
4322560	4327440	which is that subjectivity is an ontologically distinct feature of reality.
4328080	4333440	Wittgenstein provided an antidote to this way of thinking in his remarks on private language,
4333440	4339440	whose centerpiece in an argument to the effect that insofar as we can talk about our experience,
4339440	4346000	they must have an outward public manifestation. For Wittgenstein, only of a living human being,
4346000	4353280	what resembles or behaves like a living human being, one can say that it has sensations.
4353280	4360640	It sees, it is conscious, or it is unconscious. But isn't this just behaviorism? Behaviorism,
4360640	4366320	particularly in its radical form as advocated by B. F. Skinner, posits that all psychological
4366320	4372960	phenomena can be explained in terms of observable behavior and environmental stimuli without
4372960	4379920	recourse to internal mental states. But behaviorism is often criticized for neglecting the subjective
4379920	4387040	internal aspects of mental life. Wittgenstein argues against the notion of purely private
4387040	4393600	language, where words refer to inner experiences known only to the speaker. He contends that for
4393600	4400640	language to be meaningful, it must be grounded in publicly accessible criteria. So as Murray said in
4400640	4407680	his article, Wittgenstein argued against dualism or the so-called impenetrable realm of the subject
4407840	4414240	experience. Actually, he said that many folks who make the in-principle argument against AI
4414800	4420800	often retreat into subjectivity arguments, as our recent guest Maria Santa Catarina did.
4421920	4427920	Murray said that the difficulty here is that accepting the possibility of radically inscrutable
4427920	4434480	consciousness seemingly re-admits dualistic propositions, that consciousness is not, so to
4434560	4442000	speak, open to view, but inherently private. Yeah, so Aaron Sloman, who's a professor of computer
4442000	4447680	science and artificial intelligence in Birmingham, Birmingham University, so he introduced the concept
4447680	4456160	of the space of possible minds in an article in 1984. And the idea is that the collection of minds
4456160	4462080	that could exist in our universe, that do exist in our universe and that could, is much larger than
4462080	4469600	just human minds or even the minds of humans plus other animals. It encompasses extraterrestrial
4469600	4474560	life that might exist out there and it encompasses artificial intelligence that we might create one
4474560	4480880	day. So the whole space of possible minds is a very rich object philosophically speaking and
4480880	4487280	merits our study. Yeah, so Wittgenstein's private language argument is the really the centerpiece
4487360	4492240	of the philosophical investigations, which is the book that was published after his death,
4492240	4502480	which really articulates his later phase of philosophy. And it's all about the idea that we
4502480	4509600	have, or that we can talk about, private sensations. So things that are purely subjective and that only
4510160	4516960	I as an individual can understand and know what they mean. So what red is for me and just,
4517040	4523200	you know, internally for me. And so the private language remarks sort of undermine that very
4523200	4529680	conception. So the basic idea is he imagines, he says, well, so what he means by private language
4529680	4536880	is very important to kind of get this right. So he doesn't mean a language that I've invented and
4536880	4542480	that is just something that nobody can understand just because I've invented it. It's the reason
4542480	4548320	that it's a private language is because it's about something which only I can access subjectively,
4548320	4556320	which is what red is like for me. So it's the idea that you can have a word for that completely
4556320	4562800	internal thing that is just mine. So he says, imagine that I keep a diary and I keep a diary
4562800	4571520	and every time I have this experience, a particular experience, then I write s in my diary to label
4571520	4575600	that I've had that experience. So maybe I have this, I think I'm having this experience on a
4575600	4579920	particular day and I write s and then a few days later, I think I'm having that experience again,
4579920	4586400	so I write s again. Now, the question he asks is what possible criterion could there be for
4586400	4592160	the correctness of that word? What would make it actually stand for anything meaningful,
4592160	4598720	given that what really makes words meaningful is if they're understandable in a public setting,
4598720	4603040	if they're understandable really to other people. So there can be no kind of criterion
4603040	4609440	for correctness that anybody else could validate for this thing insofar as it stands for something
4609440	4616560	that's completely private. So then there's a whole set of remarks that after he sets up this
4616560	4624320	sort of little thought experiment that tell you the implications of it really. And there's one
4624320	4630960	really, really key phrase where Wittgenstein is always engaging with an imaginary interlocutor,
4630960	4636960	so an imaginary person who's arguing with him in the book. And so he's imagining this person says,
4636960	4641760	but aren't you saying that the sensation itself is just a nothing? Aren't you a kind of behaviorist?
4641760	4647520	You're just saying that it's a nothing. And his answer to this, well, I'm not saying it's a nothing
4647520	4652880	and I'm not saying it's a something either. The point was only that a nothing would serve as well
4652880	4660240	as a something about which nothing can be said. And that little kind of paradoxical sounding,
4660240	4664880	weird sounding statement encapsulates something really, really, really profound. And I think when
4664880	4670640	I first really kind of understood what he was getting at there, it had a really dramatic shift
4670640	4678240	in the way I thought about consciousness, subjectivity. And to my mind, it is the thing that
4678240	4684640	really undermines dualism. It's the most powerful way to undermine the dualistic intuitions that we
4684640	4690240	have and that date back to Descartes and before Descartes that were articulated very well by Descartes.
4690240	4695520	Many of my friends are fans of Wittgenstein, but they are also fans of subjectivity. So as you
4695520	4700960	were just alluding to what Wittgenstein did was he created this kind of barrier between the inner
4700960	4706080	and the outer. He said, you know, for things to be promoted into the language game for this
4706080	4710000	emergent structure that we have, you know, when we memetically share all of these language
4710000	4715920	constructions, that can only come from something observable. But it doesn't seem inconceivable
4715920	4721120	to me that it could in principle come from something private. So for example, you might
4721120	4725520	have a drugs experience and that's clearly ineffable, you can't find the words, but there are
4725520	4731120	things that have some semantic overlap. So I experience red, you experience red, we both
4731120	4737040	have different experiences, yet when we talk about them, some kind of overlapping category
4737040	4743600	still emerges in the public space. Yes, absolutely. So so what emerges in the public space, that is
4743600	4749600	what we can talk about. And that is that is by the way you've set up the experiment is by definition,
4749600	4753760	not private, it's public. So of course, we can both talk, we can both point at something that's
4753760	4757920	red and say, oh, look, look at that red. And you say, oh, yeah, isn't that isn't it beautiful?
4758720	4762320	There, it's manifestly, we're talking, insofar as we're talking and successful in
4762320	4767280	communicating with each other and agreeing with each other, then that's the element that is indeed
4767280	4774800	public. But are we not sharing, you know, is language not a set of pointers to our simulation?
4774800	4780800	So we're simulation sharing when we talk. And even though our simulations are different,
4781600	4787120	is the pointer, does the pointer not form some kind of category over all of our simulations?
4787120	4791440	Oh, well, there's a whole, you've introduced a whole load of terminology there, which,
4792160	4798560	which, you know, I don't know what you mean exactly by shared simulation and so on. So I think
4798560	4803360	in the context of a philosophical discussion, as soon as you introduce new, new bits of
4803360	4807440	terminology like that, then often that's the point at which you're starting to go wrong,
4807440	4812720	right, in philosophical discussions. In technical discussions, of course, you of course,
4812800	4817440	we're going to introduce new terminology all the time. But, but, but that's the moment where
4817440	4822560	often things are going when, as Wittgenstein would say, you're starting to take language on holiday
4822560	4826720	and take it away from its normal usage. So I don't know what you mean by kind of a shared
4826720	4831440	simulation, you'd have to tell me a little bit more about that idea before I could engage with
4831440	4835840	that thought experiment, I think. Well, I mean, I'm schooled on, you know, the Karl Fristons of
4835840	4839920	this world. And there's this whole thing about the Bayesian brain and perception as inference and
4839920	4845680	so on. And, you know, the basic idea is that we, you know, our everyday experience is a hallucination,
4845680	4851520	you know, we don't, what we experience isn't necessarily what is out there. And language
4851520	4856560	is is a kind of pointed to those simulations. And they must be divergent, they presumably
4856560	4861040	are divergent, yet miraculously, we can understand each other. Yeah, well, I think that so that so
4861040	4869360	the Wittgensteinian point is that we understand each other in so far as in so far as we,
4869840	4875040	you know, what we understand is what is shared, right? And anything outside of that is,
4876400	4882160	you know, we by definition can't talk about. And the difficulty is that we have this strong
4882160	4888720	inclination to talk as if there is this thing that's not shared. I mean, what really fascinates me
4888720	4893760	is that understanding it's not a binary, there's a spectrum, and we delude ourselves that we
4893760	4898880	understand things deeper than we do, because it goes into the realm of subjectivity. So when I
4899200	4905120	understand something, my brain is invoking all of this rich subject of experience. And I'm probably
4905120	4909840	taking my understanding into a domain which is beyond which that you understood. And perhaps
4909840	4914560	this is just something we willfully do all of the time. So what do you mean exactly by invoke my
4914560	4918320	brain is invoking all this subject of experience? What do you what are you what are you getting
4918320	4924080	out there? Well, so we talk about, as you say, the language game is based around public information.
4924080	4930800	So there is a kind of cultural level, a lowest common denominator of understanding. But when we
4930800	4938000	understand cultural artifacts, we further invoke our own subjective experiences. So for example,
4938000	4944000	when I laugh, I have the experience of laughter, this phenomenal experience. And this is clearly a
4944000	4948800	form of understanding, it's a subjective form of understanding. And when someone else laughs,
4948800	4954080	I feel that we are sharing this ontology, right, we're sharing it, but we can't possibly be.
4955200	4960480	Well, so I mean, you're straight away introducing all kinds of funny talk here, right? So we're
4960480	4965760	sharing an ontology when you're just talking about an everyday experience of laughing together,
4965760	4969440	which we can talk about without any kind of difficulty, and without raising any kind of
4969440	4974400	philosophical problems, just by saying, Well, you know, we both heard that that that joke, and we
4974800	4978640	were both, you know, on the floor and laughing. It was so funny. It was an excellent joke, right?
4978640	4983600	We can talk about that in everyday terms. And and there are no problems. There are no philosophical
4983600	4988480	problems. But as soon as you start, start, you know, getting philosophical, and you start talking
4988480	4993120	about that, you know, what was it? What was your phrase? There's something about subject
4993120	4997600	sheds about subjective ontology or something. Yeah, you're introducing all of this kind of
4997600	5002960	technical terminology. And that's that whole, that's a whole layer of confusion on top of our
5003040	5006640	ordinary everyday ways of talking about these things, which are unproblematic.
5007600	5013760	Okay, but then there's the anthropomorphic lens. So you're a human, we both laugh, the behavior of
5013760	5018240	laughing is publicly observable. Therefore, we have the same experience, because we have the same
5018240	5024160	behavior. Well, it depends what you mean by by understand here. So so so for sure, you know,
5024880	5030080	it is a fairly common form of speech to say, to say, Oh, well, you know, you can never understand
5030080	5035120	what it was like to give birth, because you're a man, you know, and this is of course, this is a
5035120	5044240	normal way of expressing oneself. And again, that's that sort of unproblematic. So there's
5044240	5049520	there is a sense in which, you know, in which that's that's undoubtedly true. But the problems
5049520	5056400	arise when you when you start to, to, to think that this, that what underlies this difference
5056400	5063840	in understanding or the one underlies that way of talking is is is some kind of, you know,
5063840	5071040	inner private realm that is, you know, that is index that is that is metaphysically distinct from
5071600	5076240	from the rest of reality. When we share these pointers or these symbols or whatever,
5077360	5082720	structure still emerges, we still feel that we have a shared understanding. And that understanding
5082720	5087040	can probably be factorized into a public component and a private component. I don't think
5087040	5092400	that's kooky to say that. Well, I see, you see, you're very keen to say, well, it turns a little
5092400	5097200	bit what you mean by a private component there, right? So if you really mean, you know, sort of
5097200	5102800	metaphysically inaccessibly, private and subjective, then then I think, then I think
5102800	5109120	it's not appropriate to speak of dividing things into this private and public component. So that's
5109120	5113680	where that's where things start to go wrong. And moreover, you insist that you're not a
5113680	5119840	dualist, right? But I think your inclination to make that division shows that you have dualistic
5119840	5124880	inclinations, as we all do. So people who are denying that they're dualists, they're denying
5124880	5131280	this that little seed of dualism that I think is in all of us. And that is part of our part of the
5131280	5135760	way we, we, we, you know, we think and the part of the way you naturally go when you start to do
5135760	5140480	philosophy. And so it's all, I think it's all very well to say, oh, you know, I'm a materialist
5140480	5145680	and I don't, but then when you, when you start to kind of probe and you start to discover the
5145680	5151040	puzzlement that these things give rise to, then that exposes a bit of latent dualism there. Now
5151040	5155360	overcoming that latent dualism, that is the real challenge that Wittgenstein confronts.
5155360	5161200	Well, I love the challenge. So the way I see it is there is, there's a ladder. So at the top,
5161280	5166000	you have an experience which is ineffable. And then one step down, you have an experience which
5166000	5170240	is inconceivable, which is Naples argument. And then the, you know, if you really go down the ladder,
5170240	5174480	then you get into this metaphysical dualism. So I guess I'm somewhere between the first step
5174480	5180400	and the second step. So I think if I have a certain type of experience, I simply don't find the words,
5180400	5185120	I can't communicate it to you. But if you put probes in my brain or something like that, I'm
5185120	5191520	sure that could conceivably be a way of measuring it. Yes. Yeah. So, so, so this is really important.
5191520	5196960	So for me, what counts as public is not just behavior, but it's also whatever scientists
5196960	5202400	we can discover. So that, so, so if we poke around in people's brains and we do EEG recordings and
5202400	5208240	FLRI recordings and anything else that we can imagine. And then I as a scientist can see this
5208240	5213440	stuff and use a scientist and our fellow scientists will see, see that. That's public too. So that's
5213440	5218320	in the, for the purposes of this discussion, of this philosophical discussion, that's all in the
5218320	5224080	public realm. It's not metaphysically hidden. You can, you can, and all of that can feed into the
5224080	5228720	way we talk about consciousness. And especially if we're talking about exotic entities, then,
5229520	5237040	then all of that can feed into the way our language adapts to, to, to, to, to our encountering them.
5237040	5241360	Yeah. So I think it's fascinating to decompose as you just did what people mean by subjectivity. So
5241360	5246880	of course, some people like David Chalmers, they argue that there is a little bit extra. So there's,
5246880	5251200	you know, behavior function and dynamics. And then there's that, you know, little bit extra,
5251200	5256480	which is not observable in any scientific way. And I think, you know, it's fair to say a lot of
5256480	5260400	people when they talk about subjectivity, they're not talking about the little bit extra. But when
5260400	5263760	we do get to the little bit extra, I completely agree with you, we've got a big problem.
5264400	5270560	Yeah. Yeah, I think we have got a big problem because, because of our natural, you know, dualistic
5270560	5277360	tendencies to, it's very, very difficult to think that, that, that, you know, if I experience a pain,
5277920	5282560	that, that, that there isn't something about that that is just purely minor, that you couldn't,
5282560	5287680	you know, the outside world, that other people can never really, you know, experience it. But
5287680	5292800	that's, it's having that thought, that's this moment that you kind of go wrong, but it's natural
5292800	5297520	path to go down. It's really, really hard to avoid it. And, and that's where I think
5297600	5304560	Sylvitkenstein's remarks, they, they provide a whole way of, of trying to reorient your whole
5304560	5310720	way of thinking. And, and, and if you sort of really kind of grasp them, it sort of flips your
5310720	5314960	whole world around, it flips your whole way of thinking around. So it's so that the, this whole
5314960	5320240	way of talking and thinking becomes wrong. So it's so that so very often the strategy when
5320240	5324640	you're dealing with this is somebody throws out this thought at you, like you've been throwing
5324640	5329920	out various thoughts at me about, and, and, and often buried in the way those thoughts are framed
5329920	5334480	is the problem. So, so the, the problem is the very expression of those thoughts. And you have
5334480	5338080	to take a step back and say, hang on a minute, you know, you made this funny move, you introduced
5338080	5342480	this funny bit of language, you introduced this funny way of expressing things. And that's, that's
5342480	5348800	when that's the, the, Victor Stein has this phrase that is, that's where the conjuring trick
5348800	5353360	happens is where you, the point that you don't notice is where the conjuring trick happens.
5354160	5359680	So, so, so it's kind of, so often you have to take, take a step back and you have to sort of say,
5359680	5363280	hang on a minute, I don't accept that way of talking that you've just suddenly introduced,
5363280	5368800	which is going down a philosophical garden path. Yes, and I completely agree. So, so that is,
5369360	5372880	that is a form of dualism, you know, when, when we resort to that little bit extra.
5373440	5378080	And I'm quite interested in this actually, because people like Chalmers, I don't think he likes the
5378080	5382800	term dualist, I think it's a property dualist, but he does talk about the philosophical zombie,
5382800	5386480	which is a thought experiment of something which has all of the behavior of us, but
5386480	5391280	is lacking in conscious experience, which gives rise to this idea that it's almost a kind of
5391280	5395360	epiphenomenon or it's something which, you know, almost you're asking the question, well,
5395360	5399920	well, what's it doing if it's not affecting anything? And when I read your conscious Exotica
5399920	5404560	article, I had a similar thought actually, because you showed this linear correlation between, you
5404640	5413520	know, human likeness and consciousness. And then you gave examples of algorithms,
5413520	5417200	you know, like AlphaGo, for example, and they didn't need the consciousness.
5417200	5421760	And that again raises the question of, what is the cash value of consciousness?
5421760	5426880	When we use, when we're using the word consciousness, then often we are using it in the
5426880	5434960	context of certain, you know, of certain behavioral behavior and behavioral inclinations,
5434960	5441600	and we use it in the context of other humans and other animals. And there's a whole,
5441600	5446720	I mean, for a start, the word consciousness is actually, you know, it's a multifaceted concept
5446720	5452080	that it's alluding to many things. And one of the things that it is alluding to is our ability
5452080	5458000	to deal flexibly with the everyday world. So we speak about, oh, you know, I didn't notice
5458000	5464320	the chair, that's why I bumped into it or something. And, or, you know, I didn't see,
5465600	5470480	you know, that there was a desk over there that might have had something interesting inside it,
5470480	5475760	if you opened it up. And so we talk about our awareness of the world. And we're at the same
5475760	5481360	time, we're talking about an aspect of consciousness, and we're talking about
5482720	5487600	a whole load of behavioral dispositions and capabilities. And so these things are very much,
5488320	5493680	you know, are very much related to each other in our everyday speech. So then the question arises,
5493680	5500480	though, are they dissociable? And so now it's very important that it's not like I think there's,
5500480	5505760	that consciousness is some metaphysical thing whose essence is out there to be discovered.
5505760	5512320	It's just, it's a concept that we invent and a word that we use to describe the world around us
5512320	5518480	and our place in it and each other and so on. And so, so, so then, you know, then the question is,
5520720	5526720	are there things that we might create or imagine, where we'd want to use the one concept, we want
5526720	5530320	to use the one set of words and not use the other. And that is what the question comes down to.
5530320	5534480	So in the case of something like AlphaGo, then I think, you know, we're all kind of agree that it's
5534480	5539920	actually there's a kind of cognition going on there, there's a kind of reasoning going on in
5539920	5544000	AlphaGo. There's certainly a lot of kind of cleverness, there's a kind of intelligence,
5544000	5548960	there's even, if we're thinking about move 37, a kind of creativity. So we're willing to use
5548960	5553920	all of those words, but nobody is going to suggest that AlphaGo is conscious. So there we can see
5554000	5558480	that they're under certain conditions, the concepts are dissociable. But nevertheless,
5558480	5563680	there's a strong relationship between the two, because if we think about animals, then often we
5563680	5568800	are going to, we're going to use their cognitive abilities as manifest in their sophisticated
5568800	5573760	behavior. We're going to use that as a proxy for sometimes whether we want to talk about them in
5573760	5579040	terms of consciousness. So sometimes, in our usage, we're going to bundle the things together,
5579040	5583280	and sometimes we're not. But this is all just a matter of, it's a kind of a practical matter
5583360	5588240	of how we use language and how it's usefully deployed, how language is usefully deployed.
5588240	5592400	And it's not about discovering some metaphysical entity that's out there,
5592400	5595120	which is what conscious, the word consciousness denotes.
5595120	5599840	Demis Esalvis recently spoke about this ladder of creativity and of course,
5599840	5606080	inventive creativity that move 37 was discussed. But Daniel Dennett, rest in peace. I'm so glad
5606080	5611600	I had him on the podcast actually. He's a huge hero of mine, and I believe that a hero of yours
5612560	5616960	but he coined this term the intentional stance. And what's interesting is he was
5617600	5622720	using it to designate a rational agent, but actually it gets overloaded and I'm guilty of
5622720	5627200	this. You overload it for lots of things, including even for things like consciousness. And maybe
5627200	5633360	that's because of the correlates of cognition, these things are very closely related. But
5633360	5636400	can you explain in your own articulation the intentional stance?
5636480	5645200	Yeah. Well, so I think you can use the concept and deploy the concept of the intentional stance
5645200	5650160	without necessarily embracing the whole of everything that Dan Dennett was talking about
5650160	5656160	in that context. Because for him, there's a whole big philosophical position around it,
5656160	5661200	but there's a very simple sense of the intentional stance that we can lift from Dan without
5661200	5667520	necessarily buying into everything that he said. And it's simply to say that we often in everyday
5667520	5677120	terms speak about artifacts and indeed animals, you know, other animals, as if they were rational
5677120	5685680	agents that act on the basis of what they believe and what they want. And by talking about them in
5685680	5692720	those ways, whether they really, whatever that means, do believe things or have desires, it's
5692720	5697440	useful for explaining and understanding their behavior. So if we adopt the intentional stance,
5697440	5703200	say, to use one of Dan Dennett's own examples towards a chess machine, a chess computer,
5703760	5711920	chess program, or Go program, and we might say, oh, it advanced its queen because it wants to try
5711920	5718160	and pin down my rook. And this is just a natural way of speaking. And if we use that way of speaking,
5718160	5727440	then it's just good in every way because we can then discuss among ourselves what the machine
5727440	5731120	is doing, we can explain what it's doing, we can predict what it's going to do. So that's
5731120	5736320	taking the intentional stance. And it doesn't necessarily bring with it a belief that these
5736320	5740080	concepts are literally applicable. Maybe they are, maybe they're not.
5740880	5745200	But this is where it gets interesting. So I discussed abduction, actually, when I spoke with
5746080	5749920	Dan, because it's very closely related. I know you studied reasoning for many years.
5749920	5753600	And the way I see it, when we adopt the intentional stance, what we're doing is we're
5753600	5759520	kind of building a set of variables to describe the behavior of the entity. And you were just
5759520	5764320	making the argument from the lens of Wittgenstein that the behavior is the only thing.
5765040	5774160	No, no, no, no, no, no, no, no. I definitely don't think that we mean meant by behavior
5774160	5781920	as the only thing. But certainly when we're deploying psychological terms, I don't think
5781920	5787600	that in any sense behavior is the only thing that determines how we deploy psychological terms.
5788640	5792800	You're absolutely right. So there's still a massive amount of ambiguity. So when we perform
5792800	5799040	abduction, we are creating a hypothesis and we're selecting out of an infinite set of possible
5799040	5806720	hypotheses. But the behavior gives us all of the information. So it's almost like if we knew
5806720	5811520	how to create the correct explanation, we wouldn't be missing anything just by observing the behavior.
5811520	5816160	So what are we talking about now? We're talking about chess machines or animals?
5816160	5819200	Or what are we talking about? What's the context for this thought?
5819280	5824240	I guess it could work for both. So let's say I want to adopt the intentional stance for move 37.
5824240	5830720	And I do this abduction. So I build this plan that the agent had. So the agent had this intention
5830720	5835760	and it took this sequence of steps. And I'm using that as a hypothesis to explain the behavior.
5835760	5840400	I'm adopting the intentional stance. But it's still highly ambiguous because I'm selecting
5840400	5844720	out of an infinite set of possible hypotheses. Right. And in fact, in that particular case,
5844720	5851840	it's almost certainly not the right way of thinking about it at all. Because unlike humans,
5851840	5856560	who are when they're playing these games often do form plans. So if you're playing chess,
5856560	5861440	you often do have a plan, I'm going to try and capture this area of the board and command this
5861440	5866400	area of the board say. And so I'm going to move these pieces around to try and do that. And you
5866400	5872880	might form a plan in terms of several moves, but ahead. But typically that's not the way,
5872880	5878880	that's not really the way AlphaGo works. So talking about it making plans isn't really the
5878880	5885040	right way of doing things. So it's interesting, actually, because the intentional stance,
5885040	5894000	you know, maybe it's still white work, you can still talk about something forming plans maybe,
5894000	5900080	but it's really not quite right in that case. When I say subjectivity, I'm not talking
5900160	5906480	about metaphysical or dualism, but the intentional stance clearly is a form of subjectivity.
5906480	5913520	And when we as a diverse collection of agents form our own intentional stances, it would seem to be
5914160	5919760	quite a chaotic, weird and wonderful thing, yet it seems to work. There seems to be,
5919760	5925440	by the way, an interesting thing here is the way we ascertain agency and culpability is based on
5925440	5929360	the intentional stance. So you read a news article about someone being stabbed in Australia or
5929360	5934480	something like that. And the news article was trying to give reasonable explanations. Oh,
5934480	5938480	it was because he was in a cult or it was because he was religious or it was because,
5938480	5943120	and this helps us kind of assign moral valence to what just happened.
5943120	5947440	Yeah, yeah, absolutely. Yeah. You said something like, when we take the intentional stance that
5947440	5951120	that is a form of subjectivity or something? Yes, would you agree with that?
5952160	5956960	So I wouldn't put it quite that way. I'm not quite sure exactly what you mean by that,
5957680	5962080	but taking the intentional stance is, I think it's just adopting a certain terminology and a
5962080	5966880	certain vocabulary for describing the behavior of something. So I don't think we need to bring
5966880	5974080	subjectivity into that at all, right? So I think maybe we're mixing up two completely different
5974080	5977440	senses of the word subjectivity here, which is something we should be very careful about.
5977440	5983360	So I think you mean subjectivity, you mean that you've just made your own choice between
5983440	5987360	different hypotheses. And so it's subjective. Is that, is that what you mean there?
5987360	5991680	Yes. So let's say, so for me as an observer, I might do some, let's call it probabilistic
5991680	5998560	reasoning. And for me, the most reasonable, rational explanation is this. And it's a for me
5998560	6002960	there. That's what I mean. That's what you're alluding to the subjectivity. Yeah. Yeah. Yeah.
6002960	6011040	Okay. Yeah. Well, so, so, so your for me is, I think that's that, that the sense in which
6011040	6015680	that subjective is a very, very different one from the topic that we were talking about earlier on,
6015680	6020400	because I don't think there's anything philosophically problematic in, in, in saying that,
6020400	6026160	you know, that I made my choice. And that's my preference and so on. And so, and somebody might
6026160	6030000	say, well, that's just subjective. And sure, okay, there's, there's nothing philosophically
6030000	6034800	problematic about that, right? So, so they, but earlier on, we were talking about subjectivity,
6034800	6040400	which like the big capital S and where it's alluding to kind of something whole metaphysical
6040400	6046960	thing and the issues of dualism come up. So, so, so, so yeah, so I think this is a very different
6046960	6052000	kind of thing. So that's fine. So for the remainder of this conversation, capital S subjectivity is
6052000	6057200	dualism and, and lowercase subjectivity is for me. Yeah, it's for me. Okay. Yeah. Okay.
6059040	6062080	Can you tell me about the risks of anthropomorphisation?
6062640	6068800	Yeah. So I think so in the context of, of contemporary artificial intelligence in particular,
6069680	6075520	then, then the danger of anthropomorphism, I think, is in, is in thinking that,
6076480	6080160	that a system such as a large language model, you know, a chatbot or something,
6080160	6084720	thinking that it has capabilities and that it doesn't, that's as simple as that.
6084720	6090560	Actually, it's also thinking, perhaps, that it lacks capabilities that it does. So, so in,
6090560	6095840	so in both cases, I think we can go wrong, we can go wrong by, because they exhibit very human,
6095840	6101520	like linguistic behaviour, we can just assume that they are going to be very human like in
6101520	6105280	general in all of the rest of the behaviour that we encounter with them. But we often find that
6105280	6111200	that's not the case. So we can find that at one moment, a large language model might make a
6111200	6116240	ridiculously stupid mistake that no child would make. And, and, and, and then the next moment,
6116240	6122240	it's saying something extraordinarily profound philosophically, or, or summarising some,
6122240	6128640	in, you know, enormously difficult scientific article, you know, really accurately. So, which
6128640	6132400	is, so these things are kind of superhuman powers, or translating something into four
6132400	6136640	different languages all at once. And they're, they're sort of superhuman-ish capabilities.
6137520	6141680	So it's not, so it can actually be more than better than human in some directions.
6142480	6147760	And, but, but clearly very deficient in others, you know, with contemporary models that can make
6147760	6153920	all kinds of stupid mistakes, they can confabulate, they can make errors of reasoning and just say
6153920	6159600	daft, generally daft things. So, so those, so those are examples of where, you know, it's a mistake
6159600	6164880	to, on the basis of, you know, a certain amount of interaction to think, oh, it's just like a human,
6164880	6169920	you know, because you can just, you can just misjudge it in many ways. That's one thing. There's
6169920	6174000	another, other aspects of anthropomorphism. So that's, that's just in terms of its cognitive
6174000	6178720	capabilities, if you like. But there are other problems with anthropomorphism. So if we see
6180000	6187040	empathy there, where there isn't real empathy, then we may trust something where there's no
6187040	6193440	real basis for trust. And so that's a problem as well. You know, we may form, people may form
6193440	6201360	relationships with, with, you know, AI companions and social AI, where they're kind of fooling
6201440	6206080	themselves into thinking that there's a basis for that in emotion, where there is in humans,
6206080	6212640	and it's not there in, in, in contemporary AI. And I think that all those things are problematic.
6212640	6218640	So things to do with trust, to do with friendship and empathy, and all of these, these things,
6218640	6225120	I think, where we can go wrong in seeing, seeing them as too, you know, as more human-like than
6225120	6230720	they really are. One of the issues I have with anthropomorphism is that people ascribe mental
6230720	6235920	content when it's not there. And I think you are talking about literal anthropomorphism,
6235920	6242640	which is that they see human-like qualities when they are not there. And to me, that, that's an
6242640	6248880	important distinction, because I think if I understand you correctly, you, I mean, you're
6248880	6252400	very known nonsense. You say that current large language models, they don't reason, they don't
6252400	6256560	form beliefs, they don't have a sense. Oh, no, I didn't say exactly that. Oh, did you not? That's
6257280	6261760	that's a broad, that's a much broader claim. So that's right. So I'm not sure I'd go so far as to
6261760	6268320	say they don't reason, or they don't understand, or they, or they don't form beliefs, but rather what
6268320	6274240	my, my, my approach is to say that we should be very cautious in using those terms. So I'm, so
6274240	6278400	those would all be examples of taking the intentional stance. If we were to use those
6278400	6284640	bits of terminology to describe what, what a current, you know, chatbot or conversational AI
6284640	6290000	was doing, we'd be taking the intentional stance, and it's perfectly reasonable to do that in very,
6290000	6295440	very many cases. So I would know, and I would never make the blanket claim, they don't understand.
6295440	6300560	I think that that's not quite right. I think rather, rather it's that, you know, sometimes
6300560	6304720	it's appropriate to say, oh, yeah, it seems to understand very well what this big long article
6304720	6308800	about nuclear physics was, was all about. And it summarized it really well. It really understood
6308800	6312480	it. You know, somebody might come up, might say, you know, it really did seem to understand it.
6312720	6319520	I think I wouldn't say that they were wrong in using that phrase there. So, but then on another
6319520	6325760	occasion, you might find that it's, that it, for example, recently, people have been posing this,
6325760	6332160	you know, this classic goat, cabbage, Fox problem where you've got a boat and you have to cross a
6332160	6336640	river with a goat, and you can't have the, you know, more than two things in the boat at once,
6336640	6339600	and you can't have the goat with the cabbage and all this kind of stuff. And so there's a,
6339600	6344480	it's a little puzzle and you just have to kind of cross lots of times and do all kinds of trickery,
6344480	6351120	right? And people opposed, opposed to it to some large language models that said, okay,
6351120	6355680	I've got a boat and a cabbage and I need to get the cabbage to the other side of the river.
6355680	6360960	How do I do it? And the large language model models, several of them just start to come up with
6360960	6365040	this totally baroque solution that involves going backwards and forwards over there. And sometimes
6365040	6369440	they invent goats that aren't even in the, and that's because they've overfitted or they're kind
6369440	6375920	of like connected with this classic problem. And, and so no child would make this stupid,
6375920	6380400	stupid mistake. So anyway, so there you would say, well, you know, just, you know, just obviously
6380400	6385280	just didn't understand what the, you know, and of course, it's quite right to say didn't understand
6385280	6392160	in that case. And the anthropomorphism, it comes about when you think that it understands
6392160	6398000	when we understand and doesn't understand when we don't understand. The reality is that sometimes
6398000	6403280	it understands when we understand and sometimes it won't and some, it's all mixed up, right? So
6403280	6409600	the mistake is to think that it is like us. I could, yeah, man, that was beautifully articulated.
6409600	6415840	So it's, it's, it's the mistake of thinking there is an alignment both in how the machines think
6415840	6420560	and where they make mistakes. But let's unpick this a little bit because you were saying it's
6420560	6425760	perfectly reasonable to take the intentional stance when the thing does the thing correctly,
6425760	6432080	even though it thinks differently to us. And that's absolutely fine. But I love thinking
6432080	6436800	about these things theoretically. And it's, it's delicious talking to you because you have a background
6436800	6442560	in symbolic AI. You were, I'm sure around in the days of photo and pollution with their connectionist
6442560	6449040	critique. And, and even now there are clear examples of language models not being able to do
6449040	6454640	negation. Oh, yeah. And we know they're not Turing machines. You know, we can make some strong
6454640	6460000	theoretical statements that they are limited in reasoning. I agree with you that it's reasonable
6460000	6464880	to say they understand in certain circumstances. But, but, but where I want to get to is, okay,
6464880	6470080	so we agree that language models cannot perform certain types of reasoning that we can.
6470080	6475120	Yeah. So I think we could, so I think we need to take each of these concepts individually. So,
6475600	6480800	we dealt a little bit just now with understanding reasoning as a whole separate thing. So, and,
6480800	6484240	again, this is all because, you know, they're not like us. So we have to deal with these things
6484240	6487520	individually. We can't just blanket say, oh, they don't understand, they don't reason, they don't,
6487520	6491120	or they do understand, they do reason. It's not like that. It's, you have to take each of these
6491120	6497120	concepts separately. So in the case of reasoning, then clearly today's large language models,
6497120	6503040	you know, do struggle very often with, with, with reasoning problems. Now, this is a kind of open
6503040	6510720	research problem. And people are making a lot of progress in improving their ability to solve
6510720	6516400	reasoning problems. Now, what the right approach to that is, you know, is an open research question.
6516400	6520720	Maybe it's just you throw more training data at it with, with, with, that includes lots of
6520720	6525760	reasoning problems. And then eventually you get sufficient generalization there. And it's not
6525840	6530640	totally clear that that will work, but maybe it will. Maybe it's to embed your,
6532640	6537520	your, or, or, or to surround it to include it in your, in your system, not just the large
6537520	6542960	language model, but making kind of external calls to, to say a planner or some kind of external
6542960	6546720	reasoning system. And you bring that in and you incorporate, you make something that's kind of
6546720	6551520	a hybrid that uses that, that kind of more symbolic approach. So you might do that.
6552480	6560800	Or you might also sort of, so in my work with Tony Creswell at, at DeepMind, we, we introduced
6560800	6565600	this selection inference framework where you basically, you treat the large language model
6565600	6570880	as a kind of module that does bits of reasoning. And you have a surrounding algorithm that makes
6570880	6576800	calls to the, to, to, to, to the, to the module in a kind of algorithm that does a number of
6576800	6580800	sequence of reasoning steps. So you have a kind of outer algorithm. So there's lots of ways of
6580880	6586080	trying to tackle that problem. But yeah, just your basic large, take your basic large language
6586080	6592960	model today, as they are at the moment, and you put it in a chat interface, it's easy to find
6592960	6599360	reasoning problems that are going to stump. Yes, we, we agree on that. And actually, my co-host,
6599360	6605600	Keith Duggar, he defines reasoning as performing an effective computation to derive knowledge or
6605600	6611200	achieve a goal. And he cites Claude Shannon, by the way, he said, Claude, Claude Shannon said,
6611200	6615360	we may have knowledge of the past, but we cannot control it. We may control the future,
6615360	6619840	but we have no knowledge of it. And science leverages control to gain knowledge, engineering
6619840	6624800	leverages knowledge to gain control. And reasoning is the effective computation in both. You know,
6624800	6629600	maybe just in your own articulation, because we can cite examples of things like abduction,
6629600	6637600	which we've studied in great detail. And it feels like at the moment, even if we do farm out to
6637600	6643040	Turing machine algorithms, in the days of symbolic AI, that was an intractable problem,
6643040	6649840	because we've got this infinity, right? And it still seems to me that there are some problems,
6649840	6655760	which there is no easy answer to. So I have a, I feel that you're alluding to
6655760	6663440	Fodor's, some Fodor type arguments here about abduction and maybe, but I don't know. But I mean,
6663440	6669760	I think that sort of abductive problems are where we are looking for an explanation for something.
6670800	6676880	So I think you'll find that there's a large collection of abduction problems that you could
6676880	6681360	just present to today's large language models, and they would do quite well at them, you know.
6681360	6684480	And at the same time, I'm sure you wouldn't be too difficult to find problems,
6684480	6692240	especially if they involve many steps where it will go wrong. Because doing multi-step,
6694240	6700240	it's the multiple steps that really are where today's large language models are a bit weak.
6702400	6707200	Again, it's an open research question. People are working on all that kind of thing. So if there
6707200	6713840	are multiple steps, and step n is dependent on step n minus one, and it's inherently,
6713840	6719600	they're inherently very computational sorts of things in that sense. So large language models
6719600	6724880	are a bit weak at that, for sure. And we can introduce things like chain of thought and so on
6724880	6731520	to try and improve that. But they're sort of a limited success. But my feeling is that those
6731520	6737440	are not the things where which large language models themselves are inherently strong at.
6737440	6744560	And you should probably make use of other tools in order to kind of boost reasoning capabilities.
6744560	6749600	Like I listed some of them. So somebody's making external calls to reasoning,
6750800	6755120	dedicated reasoning components. Sometimes it's embedded in the large language model
6755120	6758880	in a reasoning algorithm itself. So you can go either way, you can either take the large
6758880	6763840	language model, put reasoning things inside it as it were, so it makes external calls,
6763840	6767600	or you do it the other way around, you can have a reasoning algorithm and make the large
6767600	6772320	language model component of the reasoning algorithm. Yes, I suppose it's a similar thing to the
6772320	6777920	creativity that there's a kind of creative or inventive abduction, and then there's colloquial
6777920	6782640	abductive interpolation. And the remarkable thing is just how structured and predictable our world
6782640	6787040	is and how far you can get with the colloquial predictive abductive. Yes, yeah. I mean, this
6787040	6793760	way you speak of colloquial sort of abduction, which is the kind of thing that we can
6793760	6801280	all and the person on the street could do if you have some slightly odd situation and you say,
6801280	6809440	why is this man in the middle of the road with a policeman's hat on or something?
6813040	6816320	And you say, well, because there's maybe there's been an accident or something.
6817440	6821600	So we do all this kind of thing on an everyday basis. It's a little bit of abduction. It's what
6821600	6825360	you call colloquial abduction, I think. But large language models think you find a pretty good at
6825360	6828960	that kind of thing these days. But if you have something that's really complex and has a load
6828960	6834320	of steps to it, the kind of thing that humans would struggle at, then very often large language
6834320	6839280	models are going to struggle at those things too. Can you describe the Turing test? The Turing test?
6839280	6849600	Yes. Okay. So I'll describe the Turing test as it's popularly conceived because there are
6849600	6854000	new answers in Turing's original paper. But then again, he didn't call it the Turing test.
6856000	6863520	So the Turing test as it's popularly conceived involves having a human judge
6863520	6870800	and the human judge is interacting with two things. One of them is a human and the other is a computer
6870800	6877840	system. And the interaction is entirely through language, through a keyboard and a screen say,
6877840	6885040	or a teletype, if you like, in Turing's day. And the idea is that the human judge has a conversation
6885040	6891840	with these two things. And the question is, can the human judge tell which is the machine and
6891840	6900000	which is the human? And if the judge can't tell which is which, then the machine passes the Turing
6900000	6906640	test. Do you think it's a good measure of intelligence? I think it's a pretty rubbish
6906640	6915360	measure of intelligence. I mean, I think it's a very useful philosophical thought experiment
6915360	6920080	and starting point for conversation on this. But the trouble is, it's very easy to game.
6920080	6923440	Well, there's a number of problems with that, which people have been writing about for years,
6923440	6930960	by the way. So there are a number of problems with that. So one is that it's easy to game,
6931040	6936640	in a sense, because there's a temptation to make something to pass the Turing test,
6936640	6941920	you make something that has all kinds of strange human ticks and peculiarities,
6941920	6948480	and then it seems human. And so you can fool the judge that way by making it just a bit eccentric,
6949520	6954320	which is obviously nothing to do with intelligence at all. So that's one thing.
6954320	6960240	And then another thing is that the domain of the test is purely linguistic. So you're not
6960320	6965760	testing anything to do with the sorts of intelligence that you get in a non-human animal,
6965760	6971520	say. So dogs and cats and mice exhibit all kinds of intelligence in their ability to
6971520	6977280	navigate the ordinary, everyday world and survive in it. But none of those kinds of
6977280	6984240	intelligence are tested by the Turing test. So you were the scientific advisor on the film
6984960	6988000	Ex Machina. And I'm not sure whether it's Machina or Machina.
6988720	6993040	It is Machina, yeah. I was speaking with Irina Rishan and she said,
6993040	7000000	Ex Machina. But anyway, she's right. Yes, she is. I digress. But there was a special
7000000	7004160	type of Turing test in that film. Can you explain that?
7004160	7010080	Well, indeed it wasn't. It's not the Turing test. So there's a point in the film, I assume,
7010080	7015600	that people are vaguely familiar with the setup of the film. But there's a point in the film where
7015600	7021680	Caleb, the programmer, is talking to Nathan the billionaire who's developed this robot Ava.
7022240	7027520	And Caleb says, oh, what you're doing is you're trying to build something that passes the Turing
7027520	7035200	test. And Nathan says, oh, no, we're way past that. The point is, in the Turing test, you don't know
7035200	7042480	whether the thing that's being tested, whether you don't know whether it's a machine or a human.
7042480	7048720	That's the point of the test. The point, Nathan says, is to show you that she's a robot and see
7048720	7054400	if you still think she's conscious. So there's a number of ways in which this is very different
7054400	7059440	from the Turing test. So first and foremost, it's a test of consciousness, not of intelligence.
7059440	7067600	And those are not the same thing. And secondly, the point, as he says, is to be persuaded
7068080	7074720	that the artifact is conscious, even though you know that it's not human, not biological.
7074720	7080720	So you know that it's an AI system, and you still think that it's conscious. In that case,
7080720	7087200	it passes this test. So this test, I call the Garland test after Alex Garland, who is the
7087200	7094640	writer and director of X Machina, quite different from the Turing test. I think that those lines
7094720	7104240	in the film were actually really brilliant lines and really clever lines from Alex in the script.
7104240	7110960	And when I first saw the script, which was long before it was filmed, and that bit was in there,
7110960	7117440	and I put spot on next to those lines in the script, because I thought it was such a very
7117440	7123280	good test. I mean, presumably we're supposed to think that Caleb himself in the film does indeed
7124240	7130080	think that Ava is conscious and thinks of her that way. What that really means is that
7130080	7134880	he comes to treat her as a fellow conscious creature. And we see that in the film because
7134880	7140400	he wants to help her escape. And just as a kind of thought experiment, maybe it's another
7140400	7145760	conceivability thing, but it does seem conceivable what less consciousness would be like, but it
7145760	7148800	seems less conceivable what more consciousness would be like.
7149360	7162960	Well, so in both cases, I think it's all about exercising our imaginations actually. And I
7162960	7168960	think exercising our imaginations is perfectly legitimate in philosophical discussions. So
7168960	7176720	in fact, in my newer paper, Simulacra as Conscious Exotica, I think I say that I advocate doing
7176720	7181760	philosophy with the detachment of an anthropologist and the imagination of a science fiction writer.
7181760	7188720	So I think that's something that I aspire to do. So we can carry out all kinds of imaginative
7188720	7195680	exercises to describe exotic entities in a science fiction like way. So we can describe an
7195680	7202720	exotic entity and we can describe all kinds of strange behaviors. And that's sort of as far
7202800	7209040	as we can get really, I think we could we could we can imagine all kinds of strange behavior,
7209040	7214080	and we can imagine scientists studying those kinds of strange behavior and what underlies them as
7214080	7220240	well. And so we can imagine all of those things. And then and then we can also imagine how we would
7220240	7224880	talk about those things. So imagining how we as a kind of community and how the scientists and
7224880	7229920	the philosophers would talk about, about these imagined entities is all kind of part of it.
7229920	7233760	So I think we can do all of that. I think that is a kind of doing philosophy.
7233760	7239520	And I'm just coming back to the Turing test one last time. I read an interesting take on Twitter
7239520	7245360	recently that we've been thinking about the Turing test or wrong. There seems to be a subset of
7245360	7251920	people who it's almost like the Eliza effect. They see something they want to see something.
7251920	7256240	And it's almost like the test is actually testing the humans rather than the intelligence.
7256240	7262160	Yeah, yeah, sure. I mean, yeah, it's very, it's very tricky territory. And again,
7262160	7267120	I think we should distinguish consciousness and and intelligence in this in this regard,
7267120	7270880	you know, although there are relations between the two or consciousness and cognition.
7271440	7275440	So what just one thing I wanted to say about the about the Turing test is I do
7276000	7282960	feel actually that today's large language models kind of pass the spirit of the Turing test.
7282960	7286800	So so we defined the Turing test earlier on or the or the kind of
7288160	7293520	the popular conception of the Turing test earlier on. And as I remarked, you can kind of game it
7293520	7298160	and you know, and there's certain sort of problems with it. But notwithstanding that,
7298160	7304160	I think that actually today's large language models pass the spirit of the Turing test, I feel.
7304160	7309600	So they pass the spirit of the Turing test because they do really have human level conversational
7309600	7316880	skills, I feel. So I might my feeling is that if Turing were alive today and were presented with
7317600	7323920	Gemini or ChatGPT or Claude III, he would say, yeah, that's what I had in mind,
7323920	7327040	you know, you've done it, that's that's it's kind of passed.
7327680	7333360	Interesting. And you also distinguished, you know, the the imitation game is defined by Turing and
7333360	7338000	the the colloquial popular conception is that there's no adjudicator, it's just, you know, a person
7338240	7342480	and an intelligent machine. What is what is the kind of the main difference there?
7342480	7346720	What I mean, meant by the popular conception, I think was that maybe popular conception isn't
7346720	7350720	quite right. There's sort of contemporary version of it is that is used in academic
7350720	7356080	discussion, in fact, is what I meant. That's what I meant by popular. So I so there I'm
7356080	7360560	imagining, yeah, there is a human adjudicator there. But the difference is that in so Turing,
7360560	7369040	the way Turing sets up the test is is, you know, he has some specific specificities about,
7369040	7373520	you know, the number of minutes you should, you know, you spend interacting with it. And then
7373520	7380640	also he sets it up in the context of this game where party game where the aim is to try and work
7380640	7387280	out whether which you've got a man and a woman that somebody is is is convert conversing with,
7387280	7390640	you know, via paper, and they don't know which is the man and which is the woman,
7390640	7394320	they have to guess which is which. And that's the way the thing is set up in the original
7394320	7398960	paper is by analogy with that. So there's all so so there's all kinds of, you know,
7398960	7403280	peculiarities in the original paper, if you're a Turing scholar that aren't aren't really quite
7403280	7407760	relevant to the I think the kind of contemporary way that we think of the Turing test. But I
7407760	7413200	also think that the sense in which we in which today systems pass the spirit of the Turing test,
7413200	7416560	it's the reason it's only the spirit of the Turing test is because, of course, you very
7416560	7422000	often can immediately tell that it's that it's an AI, not not least because it'll just tell you
7422000	7427040	right away, right? If you just ask it, it will just say well as a large language model trained
7427040	7436000	by Google or by, you know, open AI, so it's easy to tell which is which. So but but but but but
7436000	7440880	nevertheless, I think that they have attained more or less human level language skills,
7441840	7446160	more or less. And so I think I do think that Turing would would would say, you know,
7446160	7451840	as I predicted, you know, yeah, we've got there. Now, I think Turing would be fascinated to see
7451840	7456400	the weaknesses that are there and the strengths that are there. And, you know, there are many things
7456400	7462720	that today's systems can do that are vastly more powerful than I think he anticipated
7462720	7468240	in that paper in the 1950s. And then there are other, you know, there will be other weaknesses,
7468240	7471920	I think that would come out that would surprise him as they've surprised us all in a way. I think
7471920	7476640	I think many of us in the field are surprised to see something that can do so amazingly in
7476640	7481040	certain respects, and yet have so many, you know, still so many weaknesses and others.
7481600	7488480	Can you introduce Naegle's bat? So in 1974, Thomas Naegle published this paper called
7488480	7494160	What is it like to be a bat? And the point of this paper was to draw attention to the fact,
7494160	7501200	if it is a fact, that there are creatures that are very, very different to ourselves to humans,
7501200	7506960	but that we assume have some kind of consciousness. We assume in his terminology,
7506960	7512160	we assume that it's like something to be that creature. And he chose a bat because bats are
7512160	7518560	very obviously very different to ourselves. They fly, they use sonar, they're pretty weird animals.
7519520	7524880	And so the way the thinking is that, well, it's probably like something to be a bat,
7524880	7528800	but what it's like is going to be very different from what it's like to be a human.
7529360	7535840	And Naegle uses that example to get at a whole metaphysical idea, which again,
7535840	7541440	I would say is pointing to a kind of dualism, to suggest that there's a whole realm of facts
7541520	7548800	about subjectivity, which are outside of the purview of objective science, actually,
7549440	7557840	but which nevertheless are part of reality. And so for him, I feel that it alludes to a sort of
7559840	7562480	kind of dualistic way of thinking again, that there's this realm of
7563280	7567760	facts about subjective things, and there's a realm of facts about objective things.
7567760	7572480	I read Naegle's bat many years ago, his paper, and he was kind of saying, wouldn't it be great
7572480	7579920	if we could move towards an objective phenomenology? And you are clear that he is pointing to dualism.
7579920	7585760	He's not just saying that it's inconceivable in the sense that I couldn't imagine what the experience
7585760	7591760	of a bat is like. It's just inconceivable. You're saying that he's actually making a dualism argument.
7592720	7599520	Well, he would probably deny that, because nearly every philosopher will enthusiastically deny that
7599520	7610720	they're dualists, but I see dualistic thinking all over the place, and I do think that this is an example of it.
7610720	7615680	Yes, I mean, it's a similar thing with John Searle. I think he is adamant that he's not a
7615680	7620320	dualist, but lots of people think he is a dualist. And actually, the Chinese rim argument is another
7620400	7626080	example of a popular thought experiment, which apparently people get wrong. My friend, Mark J.
7626080	7631920	Bishop, he always makes a point of saying that people misunderstand the Chinese rim argument.
7633280	7644320	Yes, Mark has tenaciously clinging to the Chinese language, the Chinese rim argument.
7644720	7650640	But I can't really see eye to eye with Mark, who I have enormous respect for on this particular
7650640	7657920	subject, I'm afraid. Yes, Mark is a very good friend of mine, and maybe that's a rabbit hole,
7657920	7665280	we won't go down. But I'm very amenable and convinced by some of Mark's arguments. But he's
7665280	7672480	certainly a phenomenologist, which is adamant that he is a monowist. It's another example of
7672480	7675920	someone who claims they're not a dualist, but you would say they probably are a dualist.
7675920	7681040	Yeah, well, I don't know. We'd have to have Mark sitting here. I would have needed to have read
7681040	7686000	one of his papers on this very recently to do that. So I'm not going to accuse him of anything,
7686000	7688880	but if he was sitting here, we could have a discussion about it.
7688880	7693120	Well, I asked him straight up, because I was trying to pin him down, and he said he is
7694080	7698400	an idealist, a monowist idealist. And it might be instructive, actually,
7698400	7701600	if you just to explain what do we mean by idealism?
7701600	7709120	Right. Well, so especially if he's using the word monow there as well, then I guess that
7709120	7716800	he is suggesting that while a physicalist thinks that there is only one substance in reality,
7716800	7726400	and that's material reality. And so there is no such thing as a separate stuff of mind
7726400	7732320	metaphysically. So for the physicalist, there is no dualism if they really, really think that,
7732320	7736800	but the trouble is that as soon as you kind of probe, then often they have struggle to deal with
7737520	7743200	dualistic intuitions about subjectivity. Now the idealist, on the other hand, also
7743840	7749120	thinks that there's only one substance, but that substance is the substance of mind. So
7749920	7755760	physical reality has to be a kind of construct out of this one substance,
7756480	7762800	this one out of mind stuff. So that's a very different kind of, it's a different way of
7762800	7770560	avoiding dualism, but it has its own problems about how do you explain account for science
7770560	7775280	and the success of science and so on. I also interviewed Philip Goff in a beautiful studio
7775280	7782960	recently, and he is a cosmo-psychist, but I think it's better just to use the word pan-psychist,
7782960	7790320	but I think cosmo-psychist is where you have the teleology baked in. So rather than it being bottom
7790320	7794960	up, it's kind of top down. There's some cosmic purpose to the universe, and the universe as a
7794960	7801680	whole is made of, you know, the fundamental material of the universe is consciousness.
7801680	7805280	And what's the relationship between that view and idealism?
7805280	7814080	So I guess for the pan-psychist, so the pan-psychist is so far as I understand these
7814080	7819680	philosophical positions. I mean, I shouldn't put myself forward as somebody who necessarily
7819680	7827680	understands them in depth, and everybody who subscribes to these views has a slightly different
7827680	7834560	version of them as well. But the pan-psychist certainly thinks that reality is composed of
7834560	7843200	physical material substance, but that physical material substance irreducibly has a psychological
7843200	7855040	dimension to it, a mental dimension to it. So every physical object has a little bit of
7855040	7859760	consciousness in it, if you like, in some sense. I mean, I find it a very difficult
7861440	7866640	view to articulate because I just find it so completely counterintuitive. So I can't really
7866640	7873200	put the pan-psychist's hat on and express their point of view. You need a pan-psychist here to do
7873200	7882000	that. Because to my mind, this is, you know, again with my bitkinstinian hat on, then I just think,
7882080	7888640	well, how do we use words like consciousness? Well, we use the word consciousness and all of
7888640	7897120	the related terms in the context of each other, of other human beings. And so, you know, and it's
7897120	7904560	in the context of our being together in the world and your behaving in certain ways and our
7904560	7910000	exchanging certain looks when we're doing things and that we understand each other as fellow
7910000	7914800	conscious creatures. And so that's the context in which we use, you know, words like consciousness.
7914800	7920240	And when I speak about, you know, you're not conscious, you're asleep. And, you know, it's
7920240	7926720	all in the context of other humans that we use those words. So it's just ludicrously inapplicable
7926720	7934080	to use that word in the context of, you know, I don't know, a brick or a toaster or an atom.
7934960	7940160	It's just simply the words simply are not applicable in those contexts.
7940160	7945600	Yes, it's so interesting because Philip told me that he grew up as a Christian. And I've had Richard
7945600	7950320	Swinburne on as well. And he's got a book out about are we bodies or souls, you know, putting
7950320	7956080	forward the case for substance dualism. And so there's the moving away from dualism thing,
7956080	7960640	there's the teleology things. Because I think if you are of this frame of mind, you like to believe
7960640	7968160	that there's some kind of grand purpose. And there's also the wanting to not wanting to be a
7968160	7973280	monowist, basically. So you could perceive it as a form of mental gymnastics to say, okay, well,
7973280	7978720	I don't want to be a substance dualist, but why don't we rearrange the structure somewhat so that
7978720	7984240	consciousness comes first. And there's some kind of cosmic purpose. And we're building a worldview
7984240	7988480	that still makes sense to me. Yeah, yeah. I mean, I personally, I think all of these
7989200	7995280	positions involve a great deal of mental gymnastics. And, and I, you know, I reject
7995280	8001680	any kind of ism. Or what I mean by that is I don't use that term to describe, you know, my views
8001680	8008960	at all. So all of these isms are, you know, are misguided. And what we need to do is just to
8009200	8022080	to dismantle the whole way of talking, which makes us, you know, makes us confused in the context of
8022080	8027840	this kind of these kinds of issues. So that's what Wittgenstein is trying to do, as he puts it,
8027840	8032160	to show the fly the way out of the bottle, right? This is his famous phrase. So the fly is the
8032160	8036000	person who's ended up thinking all these philosophical thoughts by taking ordinary
8036000	8041760	language into strange places, taking it on holiday. And, and so to show the fly the way
8041760	8047680	out of the bottle is to just bring all of these concepts back to their ordinary everyday usage
8047680	8054160	and to show thereby that you haven't really, you haven't lost anything, the puzzles evaporate.
8054160	8057840	So that isn't an ism. That's a, that's, that's rather that's a kind of
8059920	8065040	kind of critical methodology for just shifting the way that you think and talk all together.
8066480	8071760	And final question on this, where do you sit on the kind of the teleology question? So one view is
8071760	8078320	that we have a purpose. The the physicist would argue that it emerges, you know, from quantum
8078320	8084240	field theory and a lot of sophisticated study of biology kind of build this intermediate view
8084240	8089040	of teleonomy. Where do you sit on that kind of spectrum? I'll be honest with you, I don't think
8089040	8094320	I sit anywhere on that spectrum. I think I think those are issues on which I don't have sufficient
8094320	8100640	expertise to pronounce. So, so, you know, you just keep me, keep me on the on consciousness
8100640	8106000	and cognition, you know, all this stuff is above my pay grade, you know.
8106000	8110000	Professor Shanahan, it's been an absolute honor to have you on MLST. Thank you so much. I really
8110000	8124160	appreciate it. Thank you. Thank you so much for having me. It's been fun.
