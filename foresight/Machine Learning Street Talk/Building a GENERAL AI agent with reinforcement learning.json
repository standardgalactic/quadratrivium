{"text": " Open-endedness is essentially, you know, we're studying systems that can generate their own data in an infinite capacity And so it's systems that essentially if you run it for longer and longer they get more and more complex They generate more and more quote-unquote interestingness or interesting data And so if we can actually, you know, crack this nut of how do we actually come up with a Self-improving system in the sense that it keeps generating interesting data We can then use that data to train further train our models But of course you get into this perpetual data machine type of idea where obviously, you know There's how do you generate more data? If you know the data is ultimately coming from a model that you probably trained on previous data How do you get net new information from that? Well, I think a lot of this is actually just resolved purely again going back to this idea of the reward function Right or a preference function where there is outside information coming in through some sort of filtering criteria For example human designers in the loop or designers designing some sort of preference model that could essentially automatically rate the kinds of automatic Data that's being generated by these open-ended systems. What does waker stand for? Right, so waker stands for a weighted acquisition of knowledge across environments for robustness Fantastic, and what was the title of the paper? Oh, right. Yeah reward free curricula. Oh girl. What was the title? reward free curricula for training robust world models, that was it. Okay, so Give us the elevator pitch. Yeah, totally. So basically like the overarching Question that we're trying to answer with this paper is like how should we go about training like very general agents? So in the context of the paper, we think of a general agent as being one that's able to perform a lot of different tasks So we might think of these as different reward functions or for thinking of it from a reinforcement learning perspective But also be able to perform those tasks in lots of different environments So, you know, we don't want a robot to just be able to do you know pick up tasks do tasks in my like my kitchen specifically we want the robot to be able to go into like arbitrary Apartments and also be able to do those tasks in like arbitrary environments. And so we kind of thought about like, yeah How do we want to create an agent that can do such a thing? And we argue in the paper that a good way of doing it would be to have an agent that has a very general world model So a world model meaning that it can predict the outcome of sequences of actions and predict what will happen if it does certain actions And so we argue if we have a very general world model that can lead to a very general agent That's able to perform, you know, a variety of tasks in different environments And so then, you know, once we've established that we kind of ask the question of how do we get a very general world model? And what does it mean to have a good world model that works? While in a very general setting across different environments and different tasks like how do we define that and how should we gather data to do that? Beautiful. So I really enjoyed reading the paper and it reminded me a lot of Kenneth Stanley's poet paper So he was doing this thing called curriculum learning and it's really related to machine teaching as well There's quite a few things in machine learning where you say well if we had a really principled way of Selecting the best training data and presenting it to the learner in the best possible order Could the learner be better and in that poet paper Stanley was kind of generating a diverse set of Environments and like training a learner on those things and you're doing something very similar and you're using this mini max regret Which is a concept from decision theory. Can you bring that in? Yeah, absolutely. So So I guess we have this notion of like wanting to be To perform well across a wide range of scenarios, right? So scenarios in our context mean like different environments and different tasks and kind of like the most Standard way of thinking about that, especially in reinforcement learning or a machine learning in general as you think about like the average performance So so how do I optimize like the expected reward across all of these different scenarios? and a lot of the work that that munchies done as well kind of argues that Just just optimizing for expectation isn't necessarily the best The best objective so, you know, we can imagine in the real world We don't really know like the distribution over possible tasks or or anything Well, you know in most situations, we don't know things like that and so maybe a better objective is to try and be robust instead and robust basically we can think of that as meaning like we should do Reasonably well in every situation we could be in and that that's kind of what a robust objective is And one of the ways that you can define a robust objective is via mini max regret And so regret means like suboptimality like how well that I do relative to the best I could have possibly done So that means basically the same thing as it does in normal English And so the mini max regret objective basically says across all possible situations. I want to try and do Minimize the regret across all possible situations minimize the maximum regret I should say so that means in all possible situations We should do almost as well as the best we could have possibly done And I guess just to contrast this against the standard objective for robustness So the more common objective for robustness at least traditionally is like a maximum performance. That means maximize the performance While the environments like minimizing and choosing the most adversarial environment or the most adversarial scenario But but the problem with kind of the maximum objective is that in some environments you just can't do anything Let's say it's like some such situations is too hard You're doomed and so if in some situations you're doomed and you always get like zero reward or negative infinity reward That means there's no incentive to try and do better in any other environment because your maximum reward is always going to be zero And so therefore I think like minty argues as well as Michael Dennis and a lot of these recent papers argue that mini max regret So minimizing the maximum self-optimality is actually like a better objective for a general agent. That's robust fascinating. So If I understand correctly is it a way of saying I want to have the best case worst Expected regret. Yes. So basically mini max regret is saying that if you assume that you know The environment is adversarial to you in some way like when you're training or at inference time when you're actually Testing your policy out in the real world Mini max regret is saying the agent should behave the model should behave in a way that minimizes its worst case possible regret Over all the possible conditions of the world that this adversary could choose What's really interesting about this paper is we are talking about the reward free Exploration phase and we're also talking about the domain of model based reinforcement learning as opposed to You know, let's say value based reinforcement learning where You get this entanglement, right? So the dynamics the model of the world It's still in there, but it's kind of in meshed with this with this value model Whereas in model based reinforcement learning in a principal way We kind of separate out the parts so that we can do explicit planning and imagination and simulations and stuff like that So we're very much in this model based domain, right? Yeah, absolutely Yes, we focused on yeah model based reinforcement learning or some people like to call this like the world model setting more recently But yeah, like you said we you know in typical like model free reinforcement learning We we typically aim to learn a policy and a value function and yeah As you said like that value function is kind of implicitly encoding the dynamics through the fact that we learn the value function using the bellman equation So so the bellman equation kind of propagates the information between like transition and the environments through the value function So so the value function will like implicitly have the dynamics in it But in model based reinforcement learning we want to very explicitly model the dynamics of the environment And so what I mean by that is we want to be able to take some previous sequence of observations Perhaps those are images and then also condition on the next action We want to take in the environment and then be able to predict the distribution over the next observation or states We're very explicitly modeling the dynamics of the environment Okay, now this is really interesting because you know people think about reinforcement learning and in reinforcement learning You don't so much care about having a model of the world You care about building trajectories that lead to some you know task or goal or whatever that you're interested in so like I mean just just in broader terms. What what what do we get from explicitly modeling the world? So there are there are a few arguments for why we would want to explicitly model the environment So so one of which is um a lot of people would argue that you get better sample efficiency by modeling the environment And the argument for this is you know the reward function might be quite sparse And so if you're just relying on like the propagation of rewards backwards to try and learn the optimal behavior That might not be as efficient as actually learning the dynamics because the dynamics can be learned from every single transition that you have It's kind of like a standard supervised or unsupervised learning problem So so you kind of have like a richer signal to learn from which might arguably lead to better sample efficiency um, but I think like More concrete arguments that I would argue for or that if you have a model of the environment It's it's some kind of more general thing that you can then use to develop better decision making later on So so if you just learn a value function You're kind of only learning how to optimally do that specific reward function or optimize that specific reward function Um, but if we have a model of the environment, we can kind of arbitrarily be given some task later down Whether it be a reward function or a goal state or something like that and we can then plan to optimize that task later down the road so I would think that um You know, it's kind of a much more general way of having a powerful decision making agent Rather than just specifying like one task and learning the optimal kind of policy for one task and I guess another thing that I'll add to that is um Rather than only learning like a feedforward policy like you wouldn't reinforcement learning So something that maps directly to actions The other thing that a world model allows you to do is also to do online planning So you can imagine at test time we're trying to deploy it in the environment But we can actually do a bit more further planning through the world model to then work out what the best action is Rather than relying on just a neural network to immediately output an action And there's kind of a lot of work showing that if you can do this like planning at test time You can kind of get a lot of a better performance on a lot of environments, especially things that That really rely on um search to do well things like go and like these kind of games We do have to think explicitly ahead in the environment And so I would think those are the main reasons you would want to consider um a learning a world model And maybe a last point I'll just add is that I think this is kind of a um Again, like unclear whether this is true necessarily But but I think some people would argue that a world model will generalize better than learning a value function So you can imagine like a world model is learning things like you know state transitions So you can imagine if you if you're training on straight transitions The model is kind of implicitly being forced to learn something like physics or something like that And so if you're like very explicitly forcing the model to learn something like physics You could argue, you know We'll go to some new state and the rules of physics will still hold and therefore the world model will still be Quite good at the new state potentially whereas if you learn a value function I guess it's a little bit less clear as to whether you're putting a new situation Will the same kind of structure of that value hold as it would a model Anyway, so that was a bit of a long answer, but no, no, it's fascinating I mean when I was reading the paper that one of the reads I got is um in machine learning We are often overcoming the curse of sparsity. So of course like in trajectories and reinforcement learning that that's quite intuitive But even in learning the world model itself the model Just because of the way they're trained it tends to compress the world into small little motifs and Actually, the world is quite complicated and we need to combine the motifs together in lots of interesting and rich ways And by exploring through the world model, we're almost kind of like make it we're forcing it to make those connections Yeah, and I think um, you know to follow up on mark's um Mark's point I think it's also interesting because especially in the waker paper, uh, the world model setting We're looking at specifically reward free world models And so essentially there's this explicit decision to separate separate out the two components of world model Which is essentially the dynamics function, which tells you how things transition from state to state How does a state transition state of the world transition to the next state of the world? Given an action that the model or the agent is taking in that world and the reward that it receives So the slider part the reward is defined by the reward function And so, uh, you know, I think mark was uh to follow up on his point a lot of the benefits of the world model is In this design arrangement is that you can compositionally separate out this dynamics aspect from the reward aspect So the general idea would be why shouldn't agent train in such a world model be able to generalize to a new setting? Well, maybe if that setting shares a lot of the underlying dynamics in that version of the world for example rules of physics and the agent has learned how to exploit those to accomplish, um Navigation around that environment or reach different types of tasks Achieve different kinds of tasks in that environment Then you can um sort of superimpose a different reward function That essentially defines a different task because the reward function defines what task success is so you can essentially superimpose different tasks on top of that dynamics model and you would You know, you could expect that the agent could learn more quickly because it's already mastered sort of the foundational skills of navigating or manipulating different aspects of the dynamics of that world We've been on a bit of a journey here. I think over the last few years in the literature of um We we want to have robust models and we're doing that by kind of perturbing and you know making a bunch of manipulations to the environment And there there was this domain randomization and there's like unsupervised environment design And of course your your iteration now is doing this in in the domain of Reward-free exploration, but can you take us on on that journey sort of maybe starting with um domain randomization? kind of just to uh elaborate on something that mark was previously talking about which is that the typical you know standard setup in machine learning is to Uh, essentially optimize a model's performance uh over a uniform distribution Over the data points and so this is really just randomly sampling data points And we try to minimize the loss over those data points for whatever objective We're trying to minimize or maximize in reinforcement learning. Um We want to train agents that can perform well in lots of different versions of the environment and so You can think of each environment Almost as a bundle of data points, right? It's kind of the set of trajectories that the agent can Can encounter within that version of the world and we essentially in reinforcement learning we want to learn to maximize The reward of the agent uh in that set of trajectories So we want to specifically start to actively pursue those trajectories that give us the highest reward and we learn from the reward signal as the feedback signal for Figuring out, you know, which actions and therefore which trajectories will lead to maximizing that reward and so typically um when we operate in the multitask setting, uh, we essentially randomly sample different versions of the environment And essentially have the agent try to maximize its performance its reward on that random sample of environments uniformly Sampled from, you know, the set of possible environments And this is essentially Causing the agent it'll cause the agent to learn a policy that's optimal for essentially uniform distribution over those environments Um, but of course this is kind of a naive assumption because we essentially are assuming that every possible version of the environment is equally likely Which is obviously not true because some versions of the world will not be as likely as other as others Uh, for example, like if you walk outside the sky is usually blue and not green And so, you know, when the sky is orange, maybe that happens if you're in california There's a wildfire, but that's not usually the case And so instead what we can do is we can turn to decision theory and think of Sort of more sensible approaches to what it means to act optimally When you're uncertain about uh, what state of the world the world will be in and so the thing that we focus on in this paper Is this idea of minimax regret where it is this idea again of Having the agent act in a way that essentially minimizes its worst case regret In any possible, uh, state of the world So largely, you know, this is a shift from randomly sample what it means in practice is you want to shift from randomly sampling environments during training to essentially, uh, sampling environments that maximize the agent's regret And what this means is you're now actively sampling for those environment settings where the agents, um, Experiencing the most regret and here regret is defined just simply as what does the optimal agent do in that version of the environment? And what did this current agent that's learning do in that environment? And so there's this gap in performance and you want to actively find those environments where that gap is maximal And if you view this as this adversarial game now between, you know, uh, an adversary like nature That's choosing the environment and the agent that's learning to solve the environment. Um, you can think of the adversary as, you know, having a Payoff function in that game or it's rewarded for the Based on the regret that the agent experiences and the agent is trying to shrink that regret So the agent you can think of as being rewarded for, you know, um, the the negative of that reward So the agent's reward signal is you can think of as the negative of the regret And so now you have the setting where you can essentially view this training process this active sampling process as a two player zero sum game where the Adversary is, you know, rewarded for the regret of the agent in each environment it chooses and the agent is rewarded based on the The agent receives the negative regret as its payoff. And so, um, we know that into player zero sum games There's always a this there's always a solution called a Nash equilibrium and so this is an idea in game theory where basically this is um, a choice of behaviors on both parties or a choice of strategies on both parties in the game such that, um No player can do better unless the other player changes their strategy And so you can think of this as a situation where, you know, I'm not Neither player is incentivized to deviate from their behavior. Uh, once they reach this choice of mutual strategies And so we know that all two player zero sum games have a Nash equilibrium A set of strategies between the two players and in this case We know there's additional theorem called the mini max theorem Which says that when in a two player zero sum game specifically two players and zero sum when, um, You are at the Nash equilibrium setting then each player must be playing what's called the mini max The mini max strategy, which means that each player is minimizing the maximum Minimizing the maximum reward for the other player And so here the reward again is the regret and therefore just based on this known, you know, theorem about two player zero sum games We know that, um, the agent which is, you know, receiving the payoff of negative regret It's the min player. It must be implementing the min and max regret strategy And so this is how we essentially can shape the training process to essentially, um, arrive at an agent that performs mini max regret decision making Rather than decision making that optimizes, um, just a uniform sample of environments Okay, so kind of play back, um, some of those things as I understand it So, um, essentially we we are we're building a model which will learn to select the environments where we perform badly on And then we fine-tune on those environments because we're leaning into the gaps. We're saying where where do I perform badly? Let's fine-tune on that and then you're saying that if we continue to do this as a kind of adversarial sampling game That we will reach a Nash equilibrium. So it will converge in a good place But help me understand that why would it You know, it seems to me intuitively that it might be unstable or it might not quite why does it converge? So there's no guarantees around convergence And so I think this is an area where there's a lot of room for innovation Uh around these methods a lot of this is um, this is more I would say like theoretical motivation around why we think actively sampling environment settings based on, um, estimates of regret is a good idea and another point related to that Around sort of this gap between the theory. I I just um explained and in practice is that Regret itself is a pretty hard quantity to actually Measure in practice because you know knowing regrets defined as what's optimal performance minus my agents performance So you kind of have to know what optimal performance is and in general you don't know the optimal behavior Therefore you don't really know the optimal performance on any environment unless it's like a very toy setting and so In practice, we also use approximations for the regret in order to do this kind of active sampling and so There's a lot of deviations between theory and practice So There's no guarantees, you know that different forms of gradient based optimization For rl training would actually lead to converging to Nash equilibria A lot of the theory is just stating that if you were to run the system the learning system for a long time if we make the assumption that the optimization algorithm is fairly good at producing, you know an improved response to the Other player in this type of zero sum game you if you're assuming that if the successive sort of series of best responses That the optimization algorithm is generating Continues to improve over the previous ones you could make the assumption that maybe eventually it does get to that equilibrium But there is no mathematical guarantee that this actually happens what we want to do is You know build this latent dynamics You know a predictive model which is a simulacrum of what the idealized version is But we don't have a way of directly computing the regrets. So we kind of perform You know, we learn a proxy for that regret. How does that work? So we think of regret in the following way. So so there's kind of this um Old school result from like um mdp theory or maybe it's not that old but like 20 years ago or something like that called the simulation lemma and that basically says that you know If we let's assume for now that we we have like an optimal planner So we can give our like model of the world to this optimal planner and end some reward function Let's say later down the road we get given some reward function And so we give the model and the reward function to our optimal planner and we assume that this planner can return The optimal policy in our model So we kind of have this, you know planning oracle And if we assume that we can do that Then we can think about the difference between like how good the policy would be from Our planning oracle in the model versus the truly optimal policy in the real world And so what the simulation lemma tells us is that you know the difference between these two policies So the one found by acting optimally in the model versus the truly optimal one Is bounded essentially by the error between the model and the real world Under the distribution of states that the policy would generate So so, you know, it only it only matters that we have low error where the policy would go essentially because you know If there are some states that are just completely irrelevant what the policy is going to do It's not really going to matter if the if the model is not Accurate and there So we kind of use this result to think about the regret So that that gives us like, you know, if we have like one One true mdp and one model of an mdp and one reward function The simulation lemma can tell us, you know What would kind of be the regret if we did this optimal planning within this one model of the Of the mdp But then in our work, we're not really interested in the setting of like one mdp one reward function Um, so we start to think about, you know, what happens if we have arbitrarily many environments as well as arbitrarily many reward functions Which we don't know in advance And then I guess the other thing that I should say like you you alluded to like latent dynamics is You know, these existing results are assuming that we have an mdp. That's fully observable meaning, you know exactly what the state of the environment is Um, but usually when we think about like world models or even or just maybe more modern reinforcement learning We're really interested in learning from like quite high dimensional signals. So images or maybe Probably images, but maybe there are the high high dimensional signals we want to reason about And because we're just using image observations, this means that the world is like partially observable Like we can't infer everything we need to know about the world just from one image, you know For basically any physical task like the velocity of objects is important, but you can't infer that just from one image Um, so in this partially observable environments, we really want to take A sequence of observations because we need to to use those sequence of observations to infer what the state is So, you know viewing a sequence of images will help me to infer what the um The velocities are for example, and so we can think of this as inferring like a belief A belief over what the state is and a partially observable mdp Um, so we need this full sequence of images and we need to use the full sequence of images to then to be able to predict ahead What the next observation will be and that's kind of what you know, most world models are attempting to do Um, but if we just like taken a bunch of images and then try and directly predict images again, that's like quite a hard problem Um to just like just predict straight an image space And so the most common thing to do is kind of to take your previous sequence of images And then try and get like some compressed representation of the history of images into like the latent state And then predict the dynamics in the latent state So yeah, so I have my sequence of images I kind of compress these somehow into some vector And then I give it a new new action and I try and predict what the next kind of latent vector will be given this new action And this now represents my prediction of the dynamics in the world And then if I want to um, you know predict what the next observation would be an image space Then I can also decode that back to an image Um, but then a lot of works also argue that maybe we don't want to actually learn to predict the entire image So maybe you don't want to actually decode the entire image But that's that's another aspect that we might want to get into but there's this whole broad story of of um working in the latent space And um in reinforcement learning there was that paper called world models by you know, david haran and schmidhuber And it also I think has a relationship with you know, what lakoon's doing with jepper and these like you know joint embedding prediction architecture So there seems to be something magical about working in in the latent space And also you were talking about um, you know partially observable markoff decision processors And you know, that seems to be this idea that we need to have a modeling framework for the world And I guess like the ideal situation would be is that like we just we we knew exactly what would happen You know every single time step in every single state Um, but we don't you know, so so we model it as a partially observable Markov decision process and the markov bit is quite interesting as well I mean maybe and you guys can just sort of introduce what why do we use that as a model? So markovian basically just means you only need to look at like the current state to be able to infer all the information about the system um, so so in a markov decision process We have some state and then we assume that we're able to take some actions and given some state in some action We get some distribution over next states of the system And then the the system will transition according to that distribution to the next state And this is just like kind of a general framework for modeling like systems that we might want to control So, you know, it kind of dates back to like early work and control theory But then it's also the main framework used in reinforcement learning um Yeah, and the reinforcement learning setting because it's the decision process We we also add an reward function which tells us how good it is to be in a certain state or to execute a certain state action pair Um, but yeah, as you said with relating to like partial observability and a lot of like systems We we don't actually know what the true like state of the world is So so you can imagine, you know, if we want to think of the entire world as a partially observable mdp We can't just have some vector telling us exactly what the true configuration of the world is or maybe that exists But we can't we definitely can't just know that and so we usually think of it as being a partially observable system Um, so this means that like given given the state, um, you know at each step We'll basically get some distribution over observations and we just get to observe that observation So, you know, the state of the world could be what it currently is in here and maybe my um observation is like a camera image so I only get some Camera image of the world that allows me to infer a bit of information about the state Um, and because it only allows me to infer a bit of information about the state It doesn't tell me the whole state It really you need to keep track of all of the observations you have to be able to keep track of all the information You have about the world So, you know, you can imagine um, if the task is for me to remember how to get out the door a while ago Um, you know, I don't just need to be able to like look at my current image of the world to be able to infer that information I need to have kept track of like all my previous information as well Um, so that's kind of why we think about often want to think about like partially observable environments as opposed to fully observable ones Amazing amazing. So so minci, maybe you can um bring in this this latent idea And and sort of contrast that to what lacuna is doing as well Sure, I mean, so I think in machine learning and deep learning, uh, there's this general paradigm that's been around You know since the inception which is learning latent latent representations of data and one of the benefits of learning latent representation is that You know, ideally your objective, uh, that leads to learning these latent representations Is that you are ultimately learning a lower dimensional representation of the data or dynamics that you're modeling like in our case with the world model Um, that captures just what is necessary. It's a more compact representation of just the information that's necessary to predict The task you're trying to predict and so um with uh with our case Or latent space world models a lot of the benefit of working in the latent space Is that if as opposed to working in the full image space? for example If your observations are images like in a video game is that there could be a lot of spurious features Or you know a lot of additional information that you could be expending lots of compute and um, you know gradient updates Just to learn those patterns when they don't actually impact the ultimate um transition dynamics Or reward dynamics that you need to learn in order to do well in that environment So one example is if you have a game where, you know, maybe the background is different Uh, because it's daytime or nighttime or it's close to sunset. Um, but ultimately, you know, the background doesn't really impact How the player moves around in the environment or whether they've reached the end goal of the task and so If you're training a model where it needs to compress a lot of this information First into a smaller dimensional latent vector or latent representation Um, you don't really need you would expect that latent representation not to actually capture It would start to ignore the background color and it might only capture certain features of the environment that can Essentially if you were to decode it back out it might only capture certain information about the environment That's predictive of the actual task that you want to solve Um, so maybe if the task is to say reach a coin at the end of a level Then maybe the latent representation would capture the presence of the coin or whether the the proximity of the character You're controlling to the coin and so With the jeppa related work I think a lot of this is also, you know, motivated with this idea where if we can learn a better latent space representation Um of images or videos or whatever modality we're trying to model Um, it's a much lower dimensional computationally efficient representation. Uh that you can You can effectively use for downstream tasks. Um I'm not I'm actually not super familiar with exactly, you know, the the visual jeppa Objective so you don't think I can say too much about that. Oh, that's okay. Yeah. I mean, but yeah, I mean you pretty much nailed it so, um, I mean Lacune even gives the example of like, um, you know in self-driving cars You might not be interested in the leaves on on on the road, you know So like with increasing levels of of nesting you kind of like learn to Ignore the things that are not relevant and focus on the things that that are relevant But we're almost getting to the center of the bulls I hear so intelligence to me is all about model building And and that's what these abstractions are. They're they're models that kind of are predictive about the thing that that that's relevant and kind of like ignoring what is not relevant and We build better models when we have a curriculum. I mean apparently this happens in nature as well. Max Bennett I was talking to him the other day and he said, you know, our genome doesn't encode all of our skills Um explicitly because it would be too inefficient to do so, but they do encode a kind of curriculum So we teach babies. Yeah, we babble with babies and we teach babies how to talk and stuff like that So so the curricula is is really important and then we're getting to the center of the bull's eye Which is intelligence in in general now I think Lacune thinks that it's specialized and and what that means is that there are there are motifs That's statistically generalized and what that means is that You do need environments you need to find motifs that are present in In as many environments as possible and those are the generalizing features. Do would you agree with that? Yeah, definitely. I think that a lot of um So a lot of really powerful machine learning methods, for example, uh are trained in simulation And when you're training in simulation, there's a concept in control from control literature Called the sim 2 real gap and essentially this is essentially quantifying a performance difference between Well, it's quantifying a few things one is just how different is the are the actual physical or other other kinds of dynamics captured by your simulator Compared to reality. So if you have a physics simulator, how accurate are for example the friction dynamics or different kinds of contact dynamics? In your robotic simulator compared to those actual dynamics in the real world with a real robot Um, and this also leads to a sim 2 real gap in terms of performance So if you train in the simulator, you know, a lot of times what machine learning is really good at is it's really good at learning to exploit Whatever system you're training this the model in and so it's fairly um common for You know systems that are models that are trained within a simulator to learn to eventually exploit the simulator and so actually like one big area of um Games ai is using is actually leveraging this idea where they essentially use ml models They optimize ml models to within a certain game environment to try to find bugs within that environment to look for exploits automatically Um, so ml systems are very good at finding exploits in whatever system you have But then the issue is those exploits are usually where exactly where the gap between your simulator and reality resides And so you actually don't want your model to learn to exploit these differences between the simulator and reality to get a high performance Uh, because that kind of defeats the purpose of then later transferring your model That's trained in simulation to reality because now in reality, obviously the model can't exploit those same those same glitches within the simulator Um, yeah, so yeah Yeah, I mean because the reason this is really interesting is is that the the premise of your paper is that It is possible to build a generalist agent Which means it's an agent that can be fine tuned and work really well on a whole bunch of downstream tasks And to me that implies that at least in our physical world in any situation You might use this agent that there are general motifs that it could have learned during free training that it could like, you know Become activated in any situation. Um, does that is that fair? Yeah, maybe I can say something about um Just the way that we should could think about like the different like latent dynamic subjective So so I think I agree that like at least when I try and think about how I think or how people think I think I agree that like You know a truly intelligent system should kind of think through the world and like a very compressed representation of the world Like if I'm trying to like think through how to go to the airport Like I'm definitely not like predicting ahead in terms of like the raw image space of trying to predict every image I might observe on the way the airport and things like this. And so I think we have this kind of like trade-off between, you know Um, like we said with the bgf of paper like should should be just try and like Kind of basically model like the minimum information we need about the world to try and you know Do the do the relevant task in the world? I think what you're saying. I think that probably is Maybe more what we think about when we think about like human intelligence or something like that Um, but then there's also this other way where we just say we're going to just like Enforce the model to be able to predict ahead every single image And so in our paper, we do actually enforce that the model has to predict the next image um, and so um Basically what this might mean is yeah, like maybe the model does You know, hopefully it does like like you said like kind of capture the underlying like true things that matter in the environment But it might also mean like what we're saying with like the leaves example Like this might force the model to kind of capture a lot of irrelevant details That don't really matter like the leaves on the ground and things like this And so, you know, maybe that means it isn't actually capturing the underlying motifs It's actually just getting good at image generation. Um, but then I've or image prediction I should say Um, but then I've also heard arguments kind of saying, you know, so what if people don't really think in terms of like image prediction You know, I you know, we think in terms of like more like these high level motifs But people have other people would argue that you know Kind of the machine learning machinery is there to do really good image prediction So so if if we if we can get a model that can actually just like predict images ahead really well Um, and not really worry so much about whether it's reasoning about these like high level features You know, if you can predict images ahead really well, you know, that's enough to make to do good decision making a lot of context So I think there's this kind of like Contrasting ways of thinking about, you know, image prediction is good enough We'll just predict like really visually good scenes and that will be good enough for decision making Or do we want to force the model to try and reason about like more abstract features of the environment? And that's kind of a more intelligent way of reasoning about the world. Um, and yeah, I think that's a very interesting trade-off Yeah, yeah, I mean like it's um Like the biggest problem in machine learning is overfitting, you know So as you say like that, there are all of these statistically generalizing features But they generalize within the domain and the domain might be like your your simulator or like, you know How you're training it rather than how it's being used in in production And then as you say that there's also this um almost human chauvinistic or puritanical view on this which is that well You know, it does the right thing for the wrong reasons or I use different motifs to do the reasoning So that thing must be doing it wrong. Do you know what I mean? And um, I was talking with chris bishop at msr the other day and and you know, he's um big on Symmetries and yeah, you know, the kind of stuff that like max welling and takako hen and bronstein and um The deep mind has done loads of cool stuff on on this But it's this idea that like we know the world um has a certain geometry. It has certain physical priors So like we can deliberately um, you know, kind of construct the approximation class in machine learning Methods so so that like we make it an easier problem, right? Because we because we know we know the thing is in there Yeah, so I mean, I guess sort of the uh Slight tangent I went into around the sim to real gap I guess part of the point I wanted to make there is that um, you know One way around the sim to real gap is you could try to train You could try to parametrize a very large space of possible versions of reality And this is kind of the motivation behind this method of domain randomization where you sort of say this is the you know This is the specific task domain I care about I can parametrize the different Uh versions of the task with a few parameters And I basically want to search over the space of parameters and train my model or my agent on all possible variations of this world But obviously that's not very sample efficient because that design space could be huge could be massive And so instead we like these active sampling strategies like we were talking about earlier around mini max regret style Active sampling where you sample those environments that maximize your regret or some other type of objective Maybe like uncertainty uh similar to what we do in the waker paper But ultimately these things these active sampling process it leads to What we like to call an auto curriculum automatic curriculum And this is in contrast to prior curriculum learning works because here This is an automatically generated curriculum. So you you can kind of not have any pre-defined notion of what is Easy or hard it's purely fixed to what is easier or hard for the model in terms of how good the model is at Performing at those tasks. And so it's nice. It's an automatic curriculum So you can think of it as almost like weaving a path through this high-dimensional design space automatically such that if the Agent or model were to train on data along this path of environments through its experiences in this path of environments during the training curriculum It'll basically be maximizing some sort of information gain objective um Because you know, for example regret if there's a high regret that's that means there's a high Ceiling there's a high gap in terms of how much the agent can improve Which implies that there's a lot more for the agent to learn in those environments So it's sort of this like optimal you want to find this optimal path weaving through the high-dimensional design space of environments Now the danger here is that as you do this, uh, auto curriculum the auto curriculum, uh, could also go haywire very easily because The design space is so big if you're training in simulation, which we have to do because these methods are so sample inefficient We need so much data to train them. Um, you want to train in simulation But if you're doing the auto curriculum in the simulation design space It could start to veer very easily and quickly into different corners or niches of the design space where You know the parameters no longer really make sense in terms of mapping to a physical reality or a real world scenario That we as human users Uh actually care about and so kind of it would be you know It would defeat the purpose of spending all this compute to train this model that could then help us in the real world Because now it's veering off into parts of the design space that don't really matter for humans It's kind of noisy parts of the design space. And so this kind of leads us to this question of grounding How do we ground curricula? How do we align the curricula such that you know? They can still do their exploration through this active sampling type of procedure over the environment design space But at the same still at the same time maintain at least some proximity to the parts of that design space that are relevant to What humans care about in terms of the actual tasks they represent I mean i've been speaking with kenneth stanley a lot recently and we're talking about open-endedness And in general i've been trying to come at this problem from multiple angles And i've been using the lens of agency because i think agency is something that happens in the real world And that's why we have this divergent process because we have multiple agents, you know, kind of like You know undirected following their own gradient of interestingness. So in in evolution That's a great example that it is this divergent process, but it's also grounded It's physically grounded, you know, so it's like the physical world creates some kind of constraints on on the things that are found And i mean, you know clune called this ai generating algorithms. There's quite a few different takes on this But the idea is that um to search this complex search space We we need to have a divergent search and that's like we actually need to create the problems and the solutions So like in the real world The the you know the giraffes had the problem of like eating the leaves from from from the trees And the problems and the solutions get generated in tandem And this whole thing just kind of grows and grows and grows And that seems to be the most important feature that is missing in current ai systems and the grounding or the Stanley calls it the gradient of interestingness. I'm not sure whether you'd agree with that But um, i mean what mark what what what do you think about the importance of like this divergence in ai? kind of the current paradigm of machine learning Of kind of like, you know gathering some data set beforehand or specifying some simulated beforehand if it's reinforcement learning Is kind of good enough to do like a lot of reasonable tasks that we might care about um, you know Like obviously like predicting language or generating simulated language or performing very well at some simulated task in rl But it definitely seems like the next step towards like very general agents that are kind of You know, I guess maybe I don't know if we want to use the term agi But there's something something more along the lines of a general agent that's kind of you know able to kind of self improve and learn in more diverse environments Um, it definitely seems like that's kind of the next step of where machine learning will go And if we're going to get to that point, I kind of agree with the idea that You know, it certainly doesn't make sense to have some agent that just randomly trying to gather completely random new knowledge Like it certainly seems to make sense that you know, you know, even as a human To improve your intelligence you kind of selectively try and find out the areas in which like you can gather more More information or more knowledge and things like this and this is kind of what you know Leads to this kind of I guess branching or you know, like you said like the diverse set of things um That you might want to learn more about and so yeah I think like it clearly seems to make sense that like this kind of more open-ended this thinking is probably going to be like The next paradigm of how we think about these kinds of systems But I'll I think mentally we'll have more to say about this I think the reason open-endedness is so interesting now is I think we're uh, there's there's a few reasons why I think it's like newly relevant to this current era of machine learning because these ideas have been around for quite a while like, um, Ken Stanley, Joe Lehmann um, Jeff Klune, uh Lisa Soros these a lot of these researchers, they've They've been thinking about open-endedness and novelty based search divergent search for decades. Um I think it's really interesting to think about why there's sort of this resurgence of these ideas now and I think a lot of it is because It is again, you know, it's it's sort of following the same um Sort of uh tailwinds that have been driving a lot of the ml industry Which is just like much better compute much larger datasets And I think what we're seeing now is that we know that Modern deep learning methods work best when we can scale up the compute and the data. That's how you get them to work Um to the to their maximal capabilities. Um at some point We're going to run out of data and a lot of people are now starting to talk about You know this as sort of a pending issue on the horizon Which is you know at the current rate of consuming data for training our foundation models at some point We're going to run out of data. We're going to where are we going to get the next trillion tokens from? Um, and so I think a lot of this uh now points a lot of the interest to open-endedness because open-endedness is essentially, you know We're studying systems that can generate their own data in an infinite Capacity and so it's systems that essentially if you run it for longer and longer They get more and more complex. They generate more and more quote-unquote interestingness or interesting data um And so if we can actually, you know crack this nut of how do we actually Come up with a self-improving system in the sense that it keeps generating interesting data We can then use that data to train further train our models But of course you get into this perpetual data machine type of Idea where obviously, you know, there's how do you generate more data? If you know the data is ultimately coming from a model that you probably trained on previous data How do you get net new information from that? Well, I think a lot of this is actually just resolved purely again going back to this idea of the reward function Right or a preference function where there is outside information coming in through some sort of filtering criteria For example human designers in the loop Or designers designing some sort of preference model that could essentially automatically rate the kinds of automatic Data that's being generated by these open-ended systems And if we can do this kind of filtering we can essentially automatically find start to automatically find Useful net new data net new trajectories net new even, you know, maybe Sentences like tokens or net new content to train our models on I've been thinking a lot about creativity recently and I think creativity is is is the other half of the coin of intelligence So in the world we live in I think that the intelligent process is is us We are a divergent search and we are And basically tackling a complex search space and we are building knowledge And we are memetically sharing them in our society We're embedding them in our language and then language models come and like acquire all of that knowledge So the cynical take is that ai today doesn't you know generalize and It doesn't it doesn't creatively find new knowledge It just is a representation of the knowledge that we have found But it's not black and white is it so the the work that you're doing is a great example of no no no You can generate new knowledge by exploring these complex search spaces and even though you're exploring existing models You're discovering interesting and novel combinations of those models that have not been found before So it's creating a novel margin on something that was not there before But I suppose the ideal future we want to get into is that we really can just From a far deeper level generate new knowledge Yeah, I think one interesting thing that I've been thinking about more recently, you know is that um sort of the you know The high level question is just Right now all of the state of the rai systems from chat gbt to stable diffusion style models for text image generation All of these systems they're they're amazing very impressive, you know Like five years ago. I would not have believed that these systems could exist at this level of performance today But uh, ultimately, uh, what they do is they're in the they're they're in the q&a business So I basically ask these systems a question or I give them a command and they give me an answer Um, and so I think the next frontier of ai is really how do we design systems that don't just Answer questions, but they actually are the ones that start to ask the questions And I think once we can have ai systems that start to ask interesting questions Um, that's when we start to get closer to I think traditional notions of what uh strong agi might be Okay, so so again really really interesting now So we're getting into agency and and people think that oh you could give a language model agency You just like you know run it in a loop and interesting things will happen Well, well, that's not true because the whole point of open-endedness is to prove that Existing systems converge so they don't diverge so they don't accumulate information So we would need to create a kind of agent that like, you know, it would just keep running And it would just keep doing interesting and novel things that would keep accumulating information And I think that the reason why language models don't have agency is because they are essentially A low entropy model and what that means is during training a lot of the the sort of like the unnecessary Um, you know complexity was snipped off. So the models only know about relevant things in the next step What's the next best token and it feels like we would need to have not only a higher entropy search But we would also need to have um a diverse set of models that are actively Continually learning and and diverging from from each other, but that's just my take. I mean, what do you guys think about that? Yeah, I think that So I guess this relates quite a lot to this idea of like intrinsic motivation, which is something that we utilize in our paper and I guess I guess the idea with that is like You know, if we're trying to like gather new data in the environment, like we shouldn't necessarily be constrained to just try and Gather new data that's like good for a specific task And so I guess this kind of you know, so intrinsic motivation basically says I should just gather new information because it's novel And things like this and so we can basically like specifically try and gather information that you know Reduces our uncertainty about the environment and and or similar objectives that that don't rely on some external reward signal And I think we when you get to the situation where the model is able to like self-improve in the absence of an external reward signal So intrinsic meaning that the the signal for what you should get is just purely generated by the model So it's purely intrinsic to the model Um, so I think the situation where you know, you have the model that's able to self-improve without any external signal without a human Having to define what the reward is or what the objective is or this was good data. This was bad data Um, I feel like that does feel like a lot closer to the notion of agency because of the fact you don't have kind of some External person defining what's good and what's bad? And so yeah, I think like this the like and you also mentioned the word like creativity because I think At least in the context of things that I've done in terms of machine learning or reinforcement learning. I think like intrinsic motivation feels like the closest thing related to creativity So you're basically like trying to gather information because it's novel or because you think it's or the model thinks it's interesting rather than because You know, it satisfies some objective And so I think we could maybe say like intrinsic motivation is in some sense like an objective for being creative as well Um, I don't know if you have any thoughts about this. Yeah, I think I think that uh It's I think there's definitely a hugely deep connection between intrinsic motivation and creativity. Um In the literature intrinsic motivations also sometimes called artificial curiosity So this is a term that was coined by Juergen Schmidt-Huber Could you could you explain it just what it is? Yeah, so oh, yeah So taking a step back intrinsic motivation is essentially um in reinforcement learning We train on reward signals and as mark was saying, um, we typically train on external reward signal by external We mean that this is a task based reward. So this is um external in the sense that something outside of the agent That's learning like the human system designer decided that this is what the reward signal is for the task Uh intrinsic means that we want to we don't design directly the reward signal But we're actually using some aspect of the model itself In order to drive the models learning forward. And so one example of this could be prediction error So if the model, uh has a large prediction error on a certain task like averaged over each time step We can use that as a reward signal and say, hey, you want to visit more parts of the environment where you're bad at predicting Um, how the state will transition when you act in that part of the environment. And so Uh, as you can see, this is very similar to maybe like intuitive notions of what curiosity is Curiosity and different forms of play Um in the psychology literature, a lot of people actually argue that, you know, different forms of play In curiosity really they they amount to you can model these behaviors as essentially a person trying to Engage in activities where, you know, they're not very good at predicting the outcome And that's kind of what makes you could argue That's kind of what makes certain kinds of entertainment fun because or entertaining because you can't actually predict what will happen um You know in a few frames of the movie like a movie wouldn't be very interesting or a book would not be very interesting If you can predict what will happen in the rest of the book just by reading the first few pages Uh, and so intrinsic motivation is really saying let's guide the model towards parts of the environment or the world Or experiences where it's similarly unpredictable Stanley speaks about this this concept of deception or we call it the false compass Which is this idea that any objective and even you you could say exploring all of the search space is an objective So he said every every objective has deception and if you monotonically Optimize any objective you will always lead into you know, like a Deceptive part of the search space, but then like the counter argument is okay. Well, let's let's not um, let's not have any principles for doing the You know the the exploration. Let's just do something completely random and that doesn't seem very good So so then, you know, there's this concept of well, how do I how do I imbue some concept of what's interesting without falling victim to deception? Yes, so ken stanley, uh has a famous essay in the realm of open-endedness where he points out That this notion of interestingness Is uh, ultimately a subjective concept and so even in the case of intrinsic motivation Which I think is you know in practice we can get a lot of mileage out of this And we've seen this in a lot of domains where exploration helps a lot like even in the wakeer paper it's largely founded on this idea on how we exploit intrinsic motivation for learning world models, but Ultimately, you know, these these model based Measures of intrinsic motivation. They are by definition based on the particular model at play and so At some point, you know, you're you're starting to over fit to what that specific model finds interesting And of course what that model finds interesting if your measure of interestingness is something like a prediction error Is going to be a function of you know, the specific architecture of the model the actual inductive biases of that model The capacity of that model to learn and so you could imagine a model where you know At the beginning it's looking for lots of interesting parts of a particular video game environment But at some point, you know, it might saturate what it can represent And what it can learn and at some point it might start to find things It's explored before interesting just because it's starting to forget those parts of the environment You know if you have like a very rich stream of different kinds of environments that it's exploring So ultimately this is like an example of deception because now it's like I I think that my model is the model thinks it's exploring Parts of the environment that it finds interesting based on this prediction error But ultimately it might actually start to go back to other parts of the environment because of issues of model capacity And another really famous example of this issue would be like the noisy tv So like if your environment has you know, this this uh Noisy tv where it's just showing random noise random rgb pixels. Um, you know, that's You know, that's not something you can actually predict because it's just noise And so the model if your intrinsic motivation is really just to search for novelty in the form of prediction error It might just start staring at this tv forever because it's something that it just can't predict and I know just by looking at that tv It'll be maximizing its prediction error. Yeah. Yeah, it's so interesting. Um, so so just coming into rich stuff in a little bit So he had this idea called um reward is enough and and essentially that doesn't make in the case that you know Just using um implicit uh motivation all the stuff that that you've just been speaking about using this trajectory You know optimization process that we can do everything we need to do And in in your paper, you're kind of making an argument similar to what lakuna has been making for years about self supervised image learning That what we should do guys is let's let's kind of pre-train a base model So this model um understands environmental dynamics really well And then we stick a reward in there and and we build um agents after that So does it in any way reinforce or pun intended uh satan or or do you think it's still complimentary? I think it's still complimentary at least if I understand the the meaning of the reward is enough paper because my understanding of that Um line of thought is basically saying that you know, we can kind of specify You know any tasks that we might want an intelligent agent to do as optimizing a reward in some like mdp or promdp So market decision process or something like that and I think our work isn't contrary to that in the sense of like You know, I do think that that probably is a sufficient framework to be able to model any any kind of behavior that we might want an agent to Do but I think when it comes to actually like practically implementing that idea. There's a lot of difficulties So the first one might be um, you know, how do we even specify that reward function? so, you know, if the reward function is to um Have a good life or something like this like there's obviously like You know, maybe there is some like numerical way of defining that in terms of an mdp But there's like not actually a good way of writing down that function that maps what I do to whether I'm getting good rewards And so I think there's this kind of like, you know, I think that's a good framework for like thinking about any problem But then you have these kind of like practical issues of how do you actually define rewards? And how do you how do you say like? Were there an agents doing well and not doing well and things like this? Um, and so I think that's still um Even with the world models lines of work. I think that's still like kind of quite a difficult issue So so so the world models lines of work kind of, you know, allow you to model, you know, predicting ahead in the environment Which is a very useful thing for doing a lot of tasks Um, but then if you actually want to optimize some specific task You still have this problem of like, how do you define the reward? And so we eventually want to get to this point of being able to like inject a reward into the world model So we're kind of in agreement with that kind of line of thinking in a sense We're eventually going to use a reward to derive the the the desired intelligent behavior So I don't think there's any conflict in that sense But we still have this kind of problem of how do we inject that reward into the the world model? How do we define what that reward should be? um and the case of um You know one of the easiest things to do for example Would just be to label each image with reward and then you can kind of Encode that image into the latent space of the world model and then use that to define how good a certain thing is And that's kind of the style of thinking what we think of in our work Um, but I don't think that overcomes this like overarching issue of in general It's you know rewards can define everything, but how do you in practice like get that function is pretty hard Yeah, I mean in a sense reward is enough is sort of a tontology because once you know the reward um If you know the reward function for your environment You can essentially compute the value function, which gives you the optimal policy um And so reward has to be enough if you know the reward function and so Uh, I think the more interesting question is definitely like what is enough for the reward? What is enough to actually have a system automatically figure out what are interesting new rewards for us to train new agents? We're new models on or continue training existing models on and I think this goes back to the question of environment design This is largely the motivation of that line of work this auto curricula environment design where essentially if we can automatically Weave through this path of possible environments of the design space of the environments The design space clearly will encompass like a big part of the design space is also encompassing the reward for those tasks And so essentially we want to find a curriculum automatic curriculum or path through the possible reward functions In which we can start to train a more and more general agent But then the interesting question is again Like what exactly is the right notion of interestingness in order to drive that curriculum that path through the design space of possible things we could be training our model or agent on and um, and that's I think One of the most interesting open questions and it relates to the question as well of how do we get the model to ask the questions? Because really what drives humans in terms of asking further questions Is our own implicit notion of interestingness which is informed by things like the scientific method and you know being able to create explanations about the world And we find things interesting when we can't actually explain some phenomenon about the world Based on existing theories or explanations And so I think what's really missing for a well-grounded, you know, human interpretable version of Interestingness is having models that can essentially come up with their own theories about the world and start to probe those theories For where there's mismatch between, you know, the their learned theory of the world And evidence that new evidence that they find from experiences in the world Yeah, it's so interesting and and um, I mean when I make the argument that agent should be physically and socially embedded It's it's actually quite a simple argument, which is just the guardrails. It's that interesting this thing I think that that is how, you know, having um, uh agency but with the guardrails of our physical and social embedding So, you know, we're we're sampling things that make sense because they're already there That, you know, but but but obviously we can go off piece to little bit as individual agents I I feel that that's what helps that process just coming back to Sutton It's entirely possible that I've misunderstood Sutton, by the way So my my interpretation of reward is enough and it might be true as you say that it's tautological given that if you already knew The reward function for a particular environment then it could do everything that it needed to do But my interpretation of reward is enough is that it would lead to um, a general intelligence and you know General in in the kind of magical sense that it would work in in any possible situation But if it is specialized in the way that we agreed earlier that there exists a reward function Which would inco you know codify Motifs and things that you know, you need to know or optimize in a particular environment or set of environments Then to me that's still specialized intelligence and I would agree. Yeah. Yeah, yeah That's that I think that aligns with my take as well where I think if you have a reward function It's already sort of applying Largely applies to at least the examples in that position paper about reward is enough It seems like most of the reward functions they discuss are largely Grounded in a specific task And I think that if you have the reward function for a specific task Then it definitely seems that you can have some optimization or learning algorithm That essentially learns to optimize that reward and therefore achieve that task So I do think sort of the open question that Uh, I think same reward is enough I think it kind of passes the buck up further one level to the question of where that reward comes from And I do think that having systems that can automatically design interesting new rewards. That seems like the frontier Yeah, I agree and and you know because to me intelligence is about discovering the knowledge and the knowledge is the reward function So if it was like kind of baking the knowledge in into the system, um, okay so another sort of galaxy brain take is um, I was talking to bishop about this the other day and um Do you think of like deep learning models as one model? Or do you think of them as a sort of like intrinsic ensemble of models because they they get they behave differently in an input sensitive way So, you know, like depending on the prompts you put into language into a language model You might find that like a different part of the weight space gets activated and and essentially it's like retrieving a mini program And that program is being run, but it's not it's not model building. It's like model Retrieving, but we would would you agree of that? Hmm I guess i'm not sure about that like Like within like subsets of a single homogeneous model But I guess the thing that I like to think about that's I think quite related to this is this idea of like And I think yamakun also kind of well a lot of people have laid out like a similar architecture It's like, you know, should we think of intelligent agents as having kind of like separate subsystems that can maybe Like be thought of as different neural networks. And so, you know, we could have like, you know Um, the standard notion of a policy which is like outputting actions And maybe we also want to have the notion of like a prediction model more like a world model that predicts What might go ahead in the world as well as maybe like a planner that is somehow good at like optimizing in that model And so we could kind of think of all these things as like separate subcomponents that we assume an intelligent You know an intelligent Thing would have like an intelligent thing should be able to predict ahead in the world It should also be able to output actions It should hopefully maybe be able to infer like why other things happened and things like this And so I guess as to whether we think that should you know be just like one homogeneous model For which maybe you query it and maybe you know different aspects of that model are kind of um You know handle different aspects of the query or whether we should think of those as separate components I'm not I'm not really sure as to whether it matters whether they're separate components or not because yeah I agree that you probably could just have like one massive model that does all these things And I think at least from the the trend that I've been seeing In kind of the world models literature and and also just like I guess the rl literature Or maybe just we should call it the foundation model literature Is you kind of don't want to have like a separate model that does the prediction for actions and a separate model That does the prediction observations like why not just have one massive model That's jointly trained to predict everything you might want to query and then depending on the different query You know it will just either predict an action or a predictive video sequence or it can be conditioned on actions or conditioned on language So I think in this sense like this kind of model like you said is more like just one massive model But it kind of has like a lots of different sub tasks that it's able to do um And so maybe this is actually like the more effective way of training a model because then you kind of get generalization Across these different sub tasks as well Well, yeah, and the reason I'm asking the question is um It seemed I mean like you know for for an outsider coming in it looks like statistics has broken You know in the olden days we used to talk about the no free lunch theorem used to say like you know You need to have specialized models for different situations And now the narrative is that we have generalist models We have foundation models and and they are better than the specialized models in a strong sense And you know and I like to sort of push on this a little bit and see well When when does it break because we know that there are like these physics inspired models with inductive priors that you know Know about invariance of you know like molecules and drug discovery and stuff like that And surely they would be better than a language model But no no no now they're training language models on mathematical conjecturing and like you know like Drug formulation using tokens and and so on so you know as an outsider you might just think well We can just use a big transformers model for everything I I think a lot of this does come from um well, so I think the attention-based transformer architecture is Proven empirically to just be highly scalable highly effective at learning lots of different kinds of data distributions But I think also part of it is just that we're just starting to enter this regime When we're just training these models on an insanely large amount of data, and I think that a lot of times We need to sort of take a step back and really consider the amazing performances on different tasks And really think about you know how much information was actually leaked into This task in the training data because Right now. We're really just training these huge models on I think I would say that we're largely training them on the test distribution in many cases I do there I have seen like lots of examples of Truly impressive behaviors from these models that that do seem like Truly novel like zero shot generalization to unseen tasks Like there was a recent example. I saw on twitter or someone Had like a very low resource like rare language and they gave it a few They gave I think the cloud 3 model a few examples and it was able to essentially perfectly reproduce new utterances in that language So that does seem very impressive But it does seem at the same time, you know a lot of the performances for example on elsat or like ap biology exams I imagine a lot of that is really a function of just Literally giving the model the test domain in terms of information during the training step Okay, okay So there are like two schools of thought on this when we talk about world models You know people are talking about sorrow and is it building a world model and and it certainly seems to be it seems to be doing I mean, obviously it's not doing navier stokes It's not doing like fluid dynamics, but it seems to be doing something like that So like one extreme view is that it is just a hash table And you know, it's it's kind of doing some diffused approximate retrieval or whatever another school of thought is that it's like a simulator And you know people talk about the simulator's view of large language models and you know, like it's like it's modeling Not only, you know, just to just just the words and the language But it's also implicitly learned to model the world and the people and all of us So that's the spectrum. I mean like uh mark, where do you think these things are on that spectrum? Yeah, I think we have like it would be great to be able to play around with it and kind of see what we can get out of it But I think I think if you can for example You know after each kind of you know, so it's a language condition model So if after each kind of frame you could for example put in a different language language kind of conditioning and say like You know, what happens here if you know, the mug was pushed off the table instead of whatever else was originally happening in the video And so if you can basically do this kind of like counterfactual like interventional predictions where you kind of Give some new action and then you're able to see like the alternative outcome of that new action I think if the model is able to do that then I would think that it does have a pretty good understanding of how the world works in the sense of You know, I really think like if you can predict the outcome of any action given some sequence of observations I do think that's a pretty good proxy for being able to say if you can do that you really do understand how the world works And so I think if the model can do that I would be kind of inclined to say that it does have a kind of world model in the sense of understanding The underlying world but then there might also be a chance that you know You know these models aren't like you said it's more just like a diffuse retrieval and perhaps if you try and do like a very Fine grain conditioning on a slightly different outcome different like conditioning Maybe it won't actually give you the correct kind of counterfactual prediction And so I think maybe we'd have to see how good these models are at generalizing to slightly different inputs and things like that To really see if it understands things well, or it is just like kind of generating some arbitrary video Yeah, I think it's a double whammy because our colloquial use of language and like you know use of models and intelligence It's so static that like, you know, we we um, we think of that as being intelligence But but we're still going like we're now create we're creating knowledge right now We're creating models because we're exploring we're doing exactly what you said Minshew We're like we're exploring the search space and we're building models and we're combining them together And you know, presumably we would diverge quite quickly from from from the language models But I mean what what's your take on on this idea that they are, you know, potentially world simulators? Yeah um, so just regarding the the sort of lookup analogy for these large models, I think it's So my mental model is similar to that. Um, although I think it's it's very close to um, I think a really good write-up of of the of this alternative take Which is more like There's an alternative take which is that it is kind of like a lookup table But the prompt itself is a key that maps not to a specific sort of response, but to potentially like a function Yeah, and a vast space of functions and france wash relay had a really good Sort of blog post where he kind of goes more into the details of this viewpoint But I think that that really, you know resonates with my intuition of how these things behave where it's not literally looking up like A key value in a hash table It seems more like it's these models have learned over tremendous amounts of data to compress that data They have to learn, I think more abstract functions that help to explain that data and therefore they're learning functions So they're approximating some kind of function Or a vast family of functions And I think the prompt really acts like as a key that essentially activates a particular function And so you can kind of think of you know in the classical world where one neural network equals one function like basically it's mapping from Images to image net labels now like foundation model in the foundation model regime It's like one foundation model is essentially kind of like a giant database of lots and lots of different functions That's basically activated selectively based on the input with prompt Um, and I do think that you know based on this I think it's definitely possible that with enough data from the world enough experiential data That these foundation models can learn sort of a basis set of dynamics and transitions that explain how the world works And essentially if it does learn these transitions, um, for example in like the massive amount of video data that swore is trained on Um, I would say that yeah, I would agree that they are essentially starting to approximate, uh world models Sure. Yeah, so yeah, these are two um separate papers. So So the first one being dreamer led by like Dan and jar Haffner. So this is um, you know example of work in the space of world models and so Basically what dreamer involves doing is like a way of training a world model And then also showing that you can just generate synthetic data in the small model and then optimize decision making like purely using the synthetic data um So we talked a little bit earlier about like partially observable mdps. So we want to like take kind of the sequence of observations Um, and then be able to predict like the next a distribution of the next observation given some action and so we also talked about how you might want to like compress this into like a um More compressed representation of your of the previous observation So basically what dreamer proposes to do and a lot of works on world modeling is to take your previous sequence of observations And then you map them to some compressed representation And then could predict ahead in this latent space. Um, the next uh, latent Latent state condition on the action and then yeah, the really interesting thing about this is that now, um, you know We can in general predict what's going to happen to condition on different actions So now if you want to get like interesting behavior out of something like dreamer You can then go ahead and generate a lot of synthetic data using dreamer Or the dreamer world model and then use that to optimize behavior And so in dreamer basically the way it's done is by doing like on policy reinforcement learning in the world model So a lot of people call this like reinforcement learning and imagination So it's basically, you know, you're imagining a bunch of synthetic data then using that to like use some standard reinforcement learning algorithm and then optimize behavior in some sense And then you could also do other things like Monte Carlo tree search Which is like closer to like the works on on mu zero and things like this Creativity is a little bit like a cloud and all the creativity only happens on the surface of the cloud So there's this interesting thing that like creative discovery depends on the history of all the things that I discovered before And typically like new discovery only happens at the end of the chain not back in in the middle Exactly and and there's also this notion that creativity happens through knowledge So like knowledge new knowledge doesn't come from the ether. It's kind of There's some creative component to it, but it's it's on the The the trodden path of existing knowledge that we already have. Yeah, that wasn't a very good question But you see I mean so so when we talk about imagination through like, you know Like reinforcement learning policies and and so on what we're saying is like, you know I'm I'm imagining all of these like possible, you know worlds and so on But I'm using the cognitive primitives of all of the stuff that I already know Yeah, I think knowledge is definitely a compounding compounding artifact That's basically like the culmination of everything all the experiences that we That we encounter like throughout our whole life and through also like beyond, you know Going backwards beyond like even our individual lives into like the cultural knowledge that's shared and what's really cool about language models is that they are essentially a codification of cultural knowledge and so Jeff Klune has this concept of AI generating AI and so he's got multiple pillars of essentially what it takes for You to have AI systems that generate General AI systems and he recently added actually like as a fundamental piece of this in in his framework This idea of building on top of foundation models And so he says he calls it like standing on the shoulders of giant foundation models Which is I think really just sort of the ml equivalent of building on top of cultural knowledge There's there's a real shift recently towards talking about um synthetic data And as we were just saying like, you know synthetic data, it doesn't come from the ether So we already know stuff about the world. We we build simulators and we kind of generate new Information but in the neighborhood of things that we already know and then we kind of like iterate and fine-tune on the generated data um What what do you think about that process? Yeah, no, I think yeah, maybe I'll bring it back to this like the plan to explore line of work. So, yeah um, so so basically like the motivation of that kind of work is like Kind of saying, you know, we might have some like previous data set or something And we've trained our world model on that data set But we really want to go out and like gather more data and then like improve the world model um By gathering more data And so we can use things like intrinsic motivation to then give us like a reward signal within the world model So in the sense of something like prediction error, which mentioned earlier So now we can basically like train a policy in the world model that's now not trained for a specific task But it's trained to go out and gather information in the world So basically now, you know, you do this imagining in the world model to imagine ahead But instead of imagining ahead, how do I do a task? Well, you're imagining ahead How do I get to states that I don't know what happens and therefore we'll learn more and that's basically like the motivation behind plan to explore um, and then and our um Paper waker it's it's kind of like inspired by plan to explore as well as works on like auto-curricular and so basically what we're trying to say is You know plan to explore is good for for getting an agent to go out and gather data Um within a single environment and you know and presumably once you've gathered enough data within a single environment Then you can generate a bunch of synthetic data in that single environment And then do what we discuss with dreamer in terms of like optimizing a policy for that very specific environment um, but what we're really interested in is saying, you know Let's not assume that we have like one specific environment beforehand Let's assume that you know, there's some space of you know broad range of scenarios Like we want a very like general agents There might be a bunch of different environments and then within that what those different environments We kind of want to be able to to handle absolutely any task And so in the waker paper, we're basically saying like, you know, how should we gather the data within um Within this like broad space of possible environments and tasks such that we can train a very good world model And then once we have that world model, that's kind of like capable across environments and tasks You know the assumption is that we can then use that to generate good synthetic data, which we can then Um use to optimize behavior And so maybe to talk a little bit about like how we formalize this problem Um, so, you know, we mentioned earlier this idea of like the simulation lemma So we basically say that or an existing work that says like in a single environment We can bound the gap between the optimal policy That's trained in the world model so trained in the synthetic data to the to the truly optimal policy By the error in the world model and the distribution of states generated by that policy So it's kind of intuitive like the world model should have, you know, low error and then we will get a good policy out of it But then what we're trying to say is like now, let's assume we don't know what the environment is beforehand And we also don't know what the task is beforehand So how do we get like a good world model that can handle like all of those situations? When we later want to go ahead and optimize some task um And so the way that we do this is we basically yeah We then use this notion of mini max regret to say that the policy should have like low maximum regret across this hot entire space of environments And then using the simulation lemma we can basically say now Now the um the world model has to have low error across all environments Under the distribution of states generated by the optimal policy for any future task Um, so we're going to say like yeah, the world model has to be good for any environment and Under, you know in any area that the policy might go to that's relevant to the future tasks And then what we kind of say in the paper is, you know, if we want a truly general agent We're not going to know what the distribution of tasks is beforehand So we don't know we don't know what the reward function is. We don't have a set of reward functions Um, you know, we're just going to kind of assume the agent has to do anything later down the line And this is kind of like related to this idea of like open-endedness that we've talked a lot about And so if we don't know what the task is going to be like later down the line Then the best assumption we can do is say that, you know, it could be any reward function later down the line Which is maybe not the best assumption because as we talked a bit earlier if you're just kind of You know, we talked about a bit about intrinsic motivation and interestingness And if you kind of assume the task can be absolutely anything later down the line You're kind of assuming that, you know, the agent might want to do something completely ridiculous later like it If you do this in robotics, that might mean the task is just to do like backflips later or something like that But you have no interest in doing that. So it's it's not clear if that's really a good assumption about How we should think about what tasks might be interesting later, but that's the assumption we make So we assume the task can be absolutely anything later down the line So so now we have to get a to the point where we have the world model Which is good for any environment and under the distribution of states generated for any task or any optimal reward function And to do this we basically like leverage two different techniques So to generate this state um So to handle the aspect that we don't know what the task is later down the line. We assumed that um We have an intrinsically motivated policy that's basically seeking out the maximum uncertainty in any single environment And so basically if if this um If this intrinsically motivated policy is seeking out the maximum uncertainty in every environment Um, it's kind of like estimating for us what the maximum uncertainty is in every environment because it's like actively finding uncertainty in every environment So now we have a policy that's finding like the maximum uncertainty in every environment And then if we want to optimize this like mini max criterion across environments We kind of need the maximum uncertainty to be low across all environments. So So we kind of have to have like um, you know, this policy isn't able to find like lots of big errors across all different environments um And so Basically, you know, what we could think like what what happened in practice is, you know You can imagine there are a bunch of different environments Some which are like a low complexity and some of which are high complexity And if we just kind of naively sample from those two different environments data, you know Our world model is going to very quickly get good at the low complexity environment And then it's going to leave a lot more data from that high complexity environment to eventually get the errors low in the high complexity environment So to bring it back to the title of the paper, which is weighted acquisition of knowledge across environments for a bussiness So the idea here is that we're basically going to Change how we sample that distribution of data across environments to make sure that maximum uncertainty stays low across environments So what this ends up looking like is, you know, we're going to sample less data from the environment that has lower complexity And then we're going to actively sample more data from the environment that has higher complexity Such that we we bring those errors down on the higher complexity environments And I guess it's a little bit different to existing works on curricula because normally in curricula like automatic curriculum learning You kind of assume that you have some reward function Which is telling you how well the policy is doing in each environment and use use that specific like metric of how well the policy is doing To determine, um, you know, where the policy has more potential to learn But because we're making this assumption that, you know, we don't know what the reward function is We're trying to get a general agent that can kind of do any task any reward function Um, we don't assume that we know that reward function beforehand So we can't use reward as a metric of saying like I need more data from here or I need more data from here But then kind of the main argument of the paper is showing that, you know If we just think about this in terms of prediction error in the world model Like we can actually use that as like an intrinsic motivation signal to say, you know Does the agent need to gather more data from this environment or from this environment without access to reward function and so we could kind of think of um This work as kind of a more general approach to automatic curriculum learning in the sense of like we're not assuming that you have a reward function beforehand We're kind of agnostic to what the task is And because and to kind of distill that knowledge that's that's gathered without the reward function We use the world model as a mechanism to like distill that knowledge Because if you just like naively have an agent gathering information with no reward function You know, how do you how do you kind of put that knowledge into the agent? And we kind of argue the best way of doing that is the world model So that's kind of a summary of like the waker paper and what like what the ultimate algorithm ends up doing So I mean essentially you're doing a high entropy search. So you're you're leaning into Areas of complexity and you're building a higher complexity model Which goes against the grain of the intuition of like Occam's razor that should have simple models So you're you're almost deliberately saying no, I want I want to model the the complexity and have more of that And then the other interesting thing is like from from a curriculum learning point of view I think traditionally we did explicit curriculum learning and you know, we might have some Principles around having a monotonically increasing curriculum of complexity Whereas here by leaning into Environments where we do worse on so we're selecting them based on prediction error We're actually implicitly getting a kind of monotonically increasing complexity, which just happens to work really well Yeah, I guess actually it actually almost ends up being in the opposite direction So so by leaning into the the the higher complexity environments more We're kind of saying let's prioritize the harder environments more to begin with So let's like gather more data in the higher complexity environments Um, you know, because I guess intuitively if you kind of want to be good across all environments You kind of need more data from the higher complexity environments And we don't really explicitly think about an ordering of going first from easy to hard Um, I guess that maybe there is a something to look into there because You know Like a lot of these works go from low complexity to high complexity because it's kind of easier to learn An initial policy that can kind of do something in the low complexity environment and then you build up the complexity Gradually, um, but I think that that idea is most useful when you know what the task is So you could imagine if the task is like low commotion if it's walking You kind of want to first learn a policy that's able to walk on flat ground And then maybe gradually build up the complexity like add and bumps and then eventually it can walk on like a very Complicated terrain so it kind of makes sense to go from low to high complexity um, but in this work we're focusing on purely intrinsic motivation meaning that the policy is not trying to learn a specific task It's trying to just seek out um uncertainty and like reduce uncertainty And so we don't really have the notion of you know You first need to be able to learn how to do something on an easy An easy environment and then move towards harder environments because there is no specific task that we're trying to learn And so I think for this reason, you know, we wouldn't didn't really focus on this notion of moving from easier to harder environments So that actually, you know, we're consistently something more data from the hard environments And I guess I think this relates or I think this is something that you brought up when we when we worked on this is like You know, I think we can really relate this idea to like a lot of different contexts including things like like language models, for example um So, you know, you can imagine if I'm training an llm. I don't really necessarily have this, you know Not really a reward function in some sense. You're just trying to Do like unsupervised prediction And so, you know, we could for example take the prediction error of like a language model and a bunch of different domains and say, you know, the language model is Not very good at predicting a language about some certain task or something like that And you know, we could say, you know, and intuitively the same thing kinds of holds if it's not very good at predicting, you know What the next token is in french like we should presumably gather more data in french And that so that kind of gives us a way of like actively gathering the appropriate data Um, and so yeah, I think this idea of like gathering more data based on certainty Obviously is a very general idea like the idea of like active learning Um, but we kind of like Specialized that into thinking about how do we think about this in terms of the reinforcement learning setting? It might be interesting to talk about as well like sort of because we looked at some of the metrics as well, right? The environment complexity metrics. Yeah, we don't have the external notion of difficulty But we we also did look at sort of the emergent, uh, curriculum. Yeah. Yeah. Yeah. Gotcha. Yeah, so I guess um So it kind of depended on the environment So in some environments, you just kind of got this like very straightforward behavior of like, you know Consistently gather more data in the more complex environment um, but because we're we're actively trying to gather data, um Of the the environments for which the uncertainty is the highest kind of this curriculum could change over over the course of training So so what happened in some of the other environments? For example, is that initially all the environments are just like high uncertainty Like there's like all environments are kind of misunderstood therefore like sample all environments like equally more or less To just get a rough understanding And then you know as as the model would improve on the simplest environments Then we would see like more and more emphasis towards sampling the highest complexity environments So I guess in that sense we would get something to more like kind of what you said in terms of like a standard curriculum But a bit different in the sense of like initially everything is uncertain So we're just going to sample everything uniformly Um, but then we kind of get a better understanding of which of the environments You know the uncertainty remains high on these higher complexity ones and those are the ones we need to like go out and gather more data Yeah I mean I can see this both ways I mean certainly from like a Bayesian optimization point of view that there's something to be said for Um, you know, this is where I'm uncertain going gather more data where where I have highest uncertainty And uh, as you say like traditionally in curriculum learning We are told that we need to have monotonic increasing complexity But as you just said that's when we have a particular task in mind now neural networks They're a little bit like a block of clay aren't they so you know, it starts off with Abject complexity and then we do stand, you know, we do um stochastic gradient descent and we chip away at the clay And we kind of build we sculpt a statue that that that we want to build and I'm just trying to get an intuition here So like with this maximum entropy Search, you know like high entropy search What we're doing is is we're saying okay Well, here are some complex models and these models must contain motifs that tell us a lot of information It's a little bit like the elo algorithm in chess You know, you actually get information gain when something surprising happened So here's a big block of complexity and I'm going to try and infer What the motifs are in that complexity that that explain the information that I'm missing I think that a lot of this ultimately traces back to sort of there's like this like fundamental pattern towards uh, I think that like ties a lot of these ideas around active Um active experiment design or like active sampling, which is and all these autocurricular methods, which is you essentially want to devise Uh, what you know nowadays we call a self supervised objective or self self supervised training algorithm Um, where essentially you have the system essentially use signals. It produces itself Um during the training or evaluation process in order to drive itself forward in terms of deciding what future data to train on And so, you know, we sometimes call these kinds of systems autocurricular as well because it's automatically generating this curriculum of Tasks to train on and I think the sort of like the fundamental connecting Uh pattern here is just that this the signal that we use to drive the training It's always going to be based on something like, uh, an uncertainty signal or, um Going back to the open-endedness literature something like a classic notion of interestingness And I think there's just a lot of different possible choices for this metric and so One for example, we talked a lot about Minimax regret So regret could be one of these driving signals because it measures the existence of a performance gap and therefore Probably an information gap as well in terms of learning to master those tasks with high regret But also uncertainty is also another one it ties back to novelty because novel environments you will be more uncertain within And so there's fundamentally lots of different sort of branches of these autocurricular that you could use Depending on this search objective that you use to drive this exploration process Can we contrast this to you know, like, um, large language models that they are self-supervised learning So, you know, we do this self-supervised objective, you know, which is like, you know, typically predict in the next word And it's a similar thing with, um, self-supervised, um image Learning now the difference is with that is you're talking about a principled way of, you know, seeking specific information You know with, um, let's say high entropy and that would lead to an implicit curricula Whereas with language modeling language modeling, there is no implicit curricula But I might argue that there kind of is because the way the model does this continual learning, um, it might regularize itself So if you give it sort of surprising and weird information, the language model might just kind of brush it off And if you reinforce things that it already knows then it's almost like a stream of channels, you know It'll say, okay, you know go and go and pay attention to that. So it's almost like it's implicit Yeah, and I would say that in some ways it's almost explicit in terms of how we design these systems A lot of times like if you look at, for example, open ai's job listings They're actually hiring specifically for experts in different domains to essentially create the next Batch of supervised data to train or instruction tune their models on For example, they hire biologists or they hire people with legal expertise to generate this data And you can think of this essentially as a human steered or human driven version of this active sampling process, right? Because essentially they know that the model tends to get high perplexity or they don't it doesn't perform as well on this domain of tasks It doesn't get as high of an LSAT score as it could and so you can essentially, you know, it's it's beyond an algorithm at this point Right, it's kind of the super algorithm where you have the system designers now also being part of the data collection process and in a way supervised learning is really just sort of one point in a continual learning process where, you know Classically, we just looked at one step of this which is here's a batch of data train on that but really Building machine learning systems, especially nowadays. Everything's in production. These are all live systems You have to keep it up to date. You have to keep it continually generalizing to new knowledge Like chat gpt or clod or gemini and so really it's sort of this pattern over and over again in sequence where you collect a batch of data Train your model on that collect the next batch of data You know continue training your model on that And really you want to be selective about what the next batch of data is because obviously if you just retrain it on the previous batch of data It's going to overfit to that data Beyond a few epochs or it's not going to you know get as much novel information from it just because it's already trained on it So you do want to selectively actively collect the data And so I think we kind of almost explicitly already do this at a systems level And I think the next frontier is really just having systems that self-improve in this way where they can start to guide More of their own active data collection. I love this way of thinking about it. You know like gbt4 is a memetic intelligence It's not just like you know a bunch of weights on on a on a server somewhere And so you could argue, you know, there's this concept called graduate student descent Which is what happens in academia or even as you just articulated with open ai It's a little bit like an epic mechanical turk right where you know They are monitoring the logs They know when things go go badly and then they lean into it in the same way you are they go in higher experts And they kind of like add more and more data in all of the holes And eventually there are no more pockets of like abject failure It just it just appears to work really well for everyone and people start to say that it's you know, generally intelligent So yeah, so there's this interesting systems view of of intelligence Yeah, it kind of starts to mimic just the scientific process in a way Where we're sort of we were putting a lot of hope in the model to basically be able to distill information from sort of the net news batch of data that we collect You know that we know the model currently doesn't explain well And we we we put a lot of faith and gradient descent in order to basically be able to come up with updates to the weights That better explain that data. So we're kind of we're kind of already treating the system as almost like an automated Scientist or an automated version of this like continual process of creating theories and explanations about the world But of course, you know Humans are still much better at language models at doing this or large models at doing this So I do think there clearly seems like a huge gap in terms of Well, we still have work that needs to be done in order to build systems that can actually build much more robust theories Based on like net do new data and even seeking that out as humans do Interesting and certainly, you know in in this broader memetic intelligence. We are still the sources of agency But um, we were just sort of talking a minute ago about there being two types of ai You know, there's there's an ai where we are the generating sources of agency But there might potentially be another ai in the future where that that is the generating source of agency Yeah, I so I think that um This kind of ties into my my the framework I personally used to think about open-ended systems as well Where I think that you know at a high level you can you can study ai sort of in silico You can study it in systems that you control that you design and that you try to like have the ai model self-improve within And so you can try to build Systems that self-improve within silico and that's going to lead to potentially some issues around like the grounding problem Where essentially it starts to the auto the auto curricular exploratory process starts to veer into parts pockets of the design space That are not relevant to tasks you care about Um, and so this is kind of the danger of like generating open-ended systems in silico And I think it's very similar to potential dangers of generating agi in silico um And I think the alternative is really just what are existing intelligent systems And how do we actually amplify the efficiency the efficacy of those systems the intelligence within those systems? And so you can kind of think of like sort of the entire enterprise of ai research as do we want to generate like ai or intelligence from scratch Or do we want to build tools? You know motivated or inspired by human intelligence and other intelligent systems and use that to further amplify existing intelligence like human creativity human intelligence Could you argue because if intelligence is a divergent search process? You might be tempted to think that well if we had loads of tools to help us share The models and help other people discover the models that i've created that that will help us generally be more intelligent But could you make the counter argument that i'm actually sequestering agency or stealing agency from other people because rather than thinking for themselves And discovering novel models. They're just going to use my model Yeah, I mean I think that in the best case scenario you're building systems that essentially, you know Not you know to to think about how you know as existing systems nowadays can build on the shoulders of foundation models You really want the to build models where even humans can stand on their shoulders where the humans can basically leverage the existing expertise or Automative capabilities of those models to then like move further beyond what they're naturally capable of doing And really that pushes the frontier of the knowledge that we can create as a civilization And so you're already starting to see this where there's some recent studies that show for example like junior software engineers that use systems like Chat gpt to help them with coding at work They actually now are starting to match the performance of more senior engineers Because it sort of levels the playing field But that also translates into just like net more productivity per software engineer. And so I think that it's more just unlocking sort of Existing bottleneck and how productive each individual can be and really just means that each individual can create a lot more value Can discover a lot more knowledge Than before Okay, but I mean do you think that it creates a tendency towards boilerplate though So we're more we're more efficient at doing things that exist But you know like on on the frontier we might have a slowdown There's definitely the danger that it can lock you in to certain patterns Right. So basically if chat gpt always returns a certain boilerplate that might have an anti pattern in it Um, if that stays around it could self-amplify and then future generations of programmers might just adopt that by default because it's what's already Generated by autocomplete. So I think that that's also another really interesting realm of questions Which is basically how do you um, how do you avoid these kinds of uh, these local optima? When you start to train a model on its own outputs And I think again like sort of the solution will start to look like some form of novelty search or exploration Makes sense. Okay. Um, what do you guys think about like, um, you know academic academia versus industry and Some say there's a bit of a brain drain from academia. Totally. Yeah, I think there's like a very Very clear trade-off between the two and they said they both have like fantastic things going for them And I guess the trade-off being you know academic freedom An academia and be able to like individually pursue ideas like purely for curiosity's sake And um, you know, that's something I've really loved about academia But I guess you know, I guess the general trend and machine learning research at the moment is kind of towards like larger scale projects especially You know a lot of the properties that we might want to see kind of only emerge when you expend a lot of compute and therefore You know a lot of interesting research can kind of Maybe not only be done in an industry, but it's a lot easier to do some kinds of research in industry And so I think this kind of leads this trade-off of do you want freedom? Or do you want to be on these like larger projects that are potentially more impactful? And so yeah, I've really struggled with that trade-off. I think they they both Have big pros and cons. I don't know what you think minty. Yeah, I I think that um Industry is I I think I like at a very like first word rough approximation would be to say that industry focuses much more on um exploitation and academia is where you know in principle you should get a lot more exploration But I do think that currently both Systems are kind of like entwined in the same sort of reward function at a high level where essentially You know if if if you're if you care a lot about Citations and a short-term greedy algorithm for maximizing citations would be to focus your research efforts on sort of whatever topic is Trendy or hyped at the current time And so like I think you see tons of people obviously Working on language models partly because it really is a fascinating subject And it really is like the most powerful form of deep learning we have so I understand why everyone's working on it but I also think that um A lot of it is kind of you do get this sort of rich gets richer effect around Different topics that people tend to gravitate towards and you lose a lot of the exploration that you should otherwise have um And that's partly because you know like both industry and academia are at some level optimizing for a similar Sort of reputational status or citation count sort of metric And so I think that's an issue, but I also think that in some ways Uh industry you could say has Additional benefit where I do think that from like a short-term point of view industry is better poised to make certain Higher impact research not just because of the resources available to industry, but also partly because um Sort of industry, uh, you know Rides or dies based on whether the actual research artifact you produce Is useful and so I think that's like a very powerful reward function that is not necessarily true for academia Um, and then sort of on the to take the counter position I think academia obviously, you know, you have a lot more freedom to just explore ideas that don't need to be on that critical path For value creation immediately and so it gives you a lot more scope to potentially find like the next big thing And so I think really it's about like if you want to if you want to take the bet that you can You know play a part in discovering the next big thing Then and that's that's suited to your taste for research then academia makes more sense but if you know, um, you want to You want to maximize the probability you'll have a higher impact in sort of like a near horizon line of work Then industry is definitely I think a better bet rich Sutton, you know, he had this bitter lesson essay And he made the argument that it's just all computation and there are no shortcuts and you can even think of you know Maybe we're not very intelligent Evolution has just been running for a very very long time and we are the result of that So in in a sense, do you think that we could make strides in intelligence? You know just through ingenuity or are we always going to need loads of computer power? This definitely like makes me think of like the recent trend that we've been seeing even in like kind of the reinforcement learning Literature lately, which is like these kind of large scale Like mostly industry projects that are kind of they're even ditching the idea of doing like sequential decision making so You know you have all these algorithms that are like, you know optimal planning and so forth But we're kind of seeing A trend towards you know, even ditching that complexity of algorithm and just going straight to just copy what the human did and so kind of reducing the problem to you know essentially no real algorithmic Innovation and more just like can you gather enough expert data? And I think yeah, I guess the reason why that trend is occurring is Is I guess like you said there's kind of been You know the bit lesson kind of said that you know Just being able to scale with more data and more compute is kind of the most important thing And a lot of the more complex algorithms, especially around like reinforcement learning are actually like quite challenging to scale up especially like online reinforcement learning if you want to go out and like Actually have an agent like actively collecting data in a bunch of different environments And updating itself online like that's so much like engineering infrastructure to set up And so I think there's this this trend towards just like the simplest algorithm possible Which is like not even reinforcement learning not even planning just copy an expert but I think that That's like you kind of said earlier with like this kind of like short-term exploitation I think this is you know, it it kind of makes sense to exploit this now and push it as far as possible because You know, it's very easy to just train a large transformer and then gather as much data as possible And I think in areas like robotics, we haven't really seen like how far can that go like Can you actually get a generally useful robotics platform just by gathering more expert demonstrations and training a larger and larger transformer? And so I think it does kind of make sense that why like a lot of industry projects are pursuing that because we don't really know You know, will will that actually hit a bottleneck or or if you just gather enough data Will that will that kind of be sufficient? And I guess like You know, you could argue that I think it's probably true that there must be a better algorithm out there That can and principle do this in a more efficient way But I guess if it's just easier to just gather more data and just do imitation learning I can see that there's at least a business case for trying that So I guess I'm on the the opinion of like, you know, there must be a more efficient way of getting to like a more intelligent system But it's not necessarily clear that just scaling like raw supervised learning or unsupervised learning Like won't get you there and so it does make sense to pursue that first But kind of what I hope and expect to see is that eventually pure imitation learning or pure unsupervised learning will kind of Run out of steam and everything will plateau and I think at that point You know, then these like more complicated algorithms about gathering more data reinforcement learning planning, etc Will really come into their own And so I guess this again relates back to like the academia industry trade-off like, you know A lot of the projects in the industry are just going to kind of be exploiting gathering data right now Whereas maybe there's a lot of scope to do these kind of more Exploratory exploratory projects where maybe that will get you to like the next frontier a few years down the line I don't know what you think about this Yeah, I definitely think that um Yeah, just like treating everything as just supervised learning it does tend to work because we have large data sets But um, I think again like the challenge is just at some point we will run out of tokens We'll run out of data to train on and so that's why the self-improving more self exploratory systems will be more and more I think paramount to like driving performance even further So if we want to sort of break beyond sort of the token limit of like the data that's available now We actually need these systems to generate their own tokens their own synthetic data And that's that's where like the self play auto curricula exploration types of algorithms will start to Become more and more prominent and obviously you need an environment in which to do that exploration And that's where the world model Line of research is going to be very powerful just because that allows you to really sort of milk all of the value within The existing previous data you have seen by creating these role models where you might be able to do like counterfactual trajectories and really learn much more Um, amplify the existing data you had Yeah, I mean, I think one of one of the key things for me, um is modeling dynamics. So, um It's quite interesting actually with the human knowledge things or even looking at the innovations from from deep mind You know early versions of alpha go were bootstrapped with human knowledge And then there was the alpha zero. So it was actually doing what we're talking about It was actually discovering knowledge on its own and um in principle. That's a great idea But of course like an irrestricted domain, it's tractable but in the real world it isn't and I'm not sure whether it makes sense to use The the computation and you know information metaphor for the real world and humans and so on but but the basic idea is that We are all real agents the universe is a massive computer We're discovering all of this knowledge and then we're bootstrapping that into a machine learning algorithm And then the question is well, if you kind of just capture the thing now without the dynamics that produced it Um, will the system be robust and could you still um, you know, kind of Carry on as we were in the real world if that makes sense. So um, but yeah The interesting thing with the work you've done is is that you are modeling agential systems You are modeling dynamics, but could that be used for you know, much more complex tasks like the real world Like simulating much more complex systems in the real world. Yeah, I think that if you if you So I think that just purely imitation learning alone is not really going to get you there But I think that if you can if you can imitate So one is sort of finding the set of tasks I think that if you find the set of tasks or reward functions that could be relevant Then you can start to simulate things that are otherwise really hard to capture by just purely imitating historical trajectories So for example strategic adaptation type of behaviors are really hard because those are sort of an open-ended space of behaviors where If you basically have like a stock market, for example, that's a really good example Where if you have a stock market, that's a very open-ended system And like different traders will have different strategies that are best responses to each other And then over time the set of strategies evolves over time in an open-ended way Um, you know trading strategies that worked 10 years ago probably won't work very well today because people have sort of um They've sort of figured out those strategies. And so they won't be very competitive. And so um I don't see an an imitation learning system being able to sort of Um, generalize to that level of complexity just because by definition it's imitating previous Uh trajectories and therefore strategies. So I think you need some notion of like a um a more uh More interactive trial and error learning that allows for strategic adaptation And that requires some notion of a payoff or a reward. And so you kind of need to have this this idea of um You you can't just purely I think learn Uh A model of something like the stock market just based on previous data You really need to have more inductive biases around uh, sort of you know What creates a payoff or what the actual reward function is for each of the traders? Uh, but that might be something that you could um You could learn over time, but I But maybe not in the yeah, so this is kind of like this is not very coherent But I feel like uh, you might need something that looks more like learning over space of programs That starts to encompass different kinds of uh tasks And then you can basically simulate those tasks to completion with agents that can essentially Uh try to self-improve against other agents The stock market I think is a wonderful metaphor for we're talking about and for for two reasons first of all from the grounding reason Because you know like the the the the the memetic world is very ungrounded And that's why we develop as humans lots of weird shared delusions about things because it's actually like you know It can go in it can go in almost any direction And also the concept of alpha I think is really important because a trading strategy works really well today And then when other people learn about it it no longer provides an advantage because everyone else knows about it And I feel it's the same with language models. So, you know, like gbt4 pros was really novel and cool It was great to you know, have like a ted talk speech when it came out And now it doesn't seem cool anymore because everyone's using it on linkedin So it's almost like that we need to have this in like continuous creative evolving process producing new sources of alpha And the paradox is that if everyone has access to the same model it can't be a source of alpha by definition Yeah, I guess on that like topic because we kind of talked about like synthetic data earlier And you kind of said like you know one one mechanism towards getting like a kind of self-improving system that is able to kind of You know Continue to improve is to kind of like filter the synthetic data for example So you might kind of you know have the the new system and then we generate some more data And then we kind of have some like filtering mechanism to say that you know in the current stock market This is this is good data or what you know, whatever system we're thinking about and then we can kind of like Use that to enable the model to improve You know and adapt to the new system But something I've always like why like thought about is like well I guess one is is it really trivial to be able to like filter that you know new synthetic data And then two it feels like if you're just relying on like filtering existing synthetic data Like isn't that a never to go into kind of plateau and so I guess eventually You know we talked about how you kind of said that you do actually actively not need to go out and get real more real data But I guess I'm kind of asking you do you think this idea of just like filtering synthetic data from a model is kind of Sufficient to always be able to adapt and improve or is it always going to be a mixture of like more real data Plus synthetic data filtering. I think it's the latter just because um at some point you would expect that The synthetic data you do generate it'll start to sort of saturate like what's already in the model Just because the model is trained on a finite amount of information. So at some point you're just going to start to see more and more Especially like the more likely trajectories or sequences of samples. You'll start to see that More and more and so you're not really going to be very sample efficient in terms of searching for the synthetic data So can you can you tell us about the results of the paper? Totally. Yeah, so basically we evaluate this um This algorithm on a bunch of like synthetic simulated domains kind of like robotics related tasks Um and kind of yet environments where there's like varying levels of complexity So, you know, you might have a robot pushing around a variable number of like objects Or maybe you have different terrain that the robot might want to um Learn to kind of you know do locomotion over and things like this Um and so kind of you know, the main comparison we make is like how well does waker work relative to like naive domain randomization So how well does it work if you just like uniformly sample the space of environments versus if you do actively seek out The environments that have this like higher uncertainty Um and so basically what we show is that you know, if we do the waker approach We still do like very well on average, but we consistently do better in terms of robustness And so robustness by robustness. I mean here that that we do better in terms of the worst environment That the agent is evaluated under and so this kind of means, you know If the agent is able to do well in the worst environments that it that it is evaluated under That kind of shows that it's able to do well across all environments because Its worst performance is still good. Um, so we kind of this this shows that we we achieve this like robustness property Which we talked about in terms of like mini max regret But we evaluate we don't evaluate it in terms of like the true notion of mini max regret because as we talked about earlier Actually evaluating regret exactly as difficult because that that requires knowing the exact true optimal performance Which isn't something we can really know. So instead we we just show that you know The agent performs well across all environments more so than if you just like naively sampled the environments uniformly And in terms of decomposing the performance across the spectrum of possible environments So like, you know the ideal situation is that we have a very simple model which just generalizes So we happen to have found the golden motif, you know, there's a spectrum of correlations almost all of them are spurious But we've just you know, just by through some sheer magic We found the best motif to work in all situations. Probably that's not quite true Probably there are some good generalizing motifs and the model has also kind of like memorized the long tail And that there's some degree of like, you know, it works really well on on the test set that might not out of domain distribution Do you have any like way of reasoning about what that is? um so Yeah, I agree. I guess there's like not necessarily it's not necessarily the case that by like Focusing more on these like long tail examples That's necessarily the best way of training the best model because like you said like maybe it happens to be the case that If the model is trained on some certain subset of the tasks like that will actually generalize better but but I think in practice, that's not something we can really um Really know how to you know, like optimally select the best kind of set of tasks that will generalize well And so we do focus more on on like, you know, these these kind of long tail tasks Or like the ones that we might see rarely and therefore have high uncertainty about um In terms of like the the out of distribution generalization So so we do also do some experiments like looking at how well does the model generalize out of distribution? And basically what we show is that if we train the model in this way And then we give it some more environments that hasn't seen at test time Um, if the environments are more complex then then we've seen sorry hasn't seen at training time Basically like this model then generalizes better to out of distribution environments that are like more complex Which is kind of what you'd expect because we've kind of biased something towards more complexity We're able to generalize better to out of distribution environments that have higher complexity And then I guess the question is like do we care about out of distribution environments that have higher complexity? Like what about the out of distribution environments that have lower complexity? and I would argue that you know Basically the lower environment out of distribution distribution environments that have lower complexity Like we would already expect that the model is able to do very well at so so there's not really much of a difference there because you know Almost any reasonably trained model can handle the very simplest environment So what we really care about is can we generalize out of distribution to like higher complexity environments? And so by biasing the something towards the higher complexity environments We do show that we're able to generalize further out of distribution to even higher complexity environments Okay, but is there any way of knowing whether it's kind of like Memorizing the high complexity instances or whether it's still learning abstract motifs and generalizing between them Yeah, that's a great question. I think that's a really interesting question generally for ml as a field right now Which is better evaluation benchmarks for Generalization within different kinds of models Um, and like we we alluded to earlier. There's kind of this issue of data leakage between training and test set Which is um, which is definitely an issue that is currently happening with large language models Um, it doesn't take away from the impressiveness of these models because clearly there is a strong generalization aspect to their behavior, but I do think that in terms of measuring performance on specific benchmarks Um, we really need to solve this problem. How do we have these clean data sets? That allow us to To truly test on inputs that the model hasn't seen at training. Um, I think in the case of reinforcement learning That's a bit more difficult just because usually we focus on a particular task domain And so there's always going to be some shared similarities within the task But obviously, uh, we didn't do this in this paper, but we could try things where we have more um more controlled Settings where we you know change one aspect of the environment and really see if it's learning specific causal relationships between Things that have to be accomplished in that task But we didn't do that. Um, that I actually think would be a really interesting idea for A new evaluation environment for rl Yeah, I mean the benchmarks thing is just a huge challenge in in machine learning In general, but just just to kind of round off off the interview I mean minchie you you were talking about you're doing some work with um edgreff instead and it is amazing I'm getting edg back on and um, you said that um, you've been looking into this kind of the interface Between humans and machine learning. Can you tell me about that? Yeah, so just to not say too much about it because um, it's related to current work that's happening at DeepMind Um is just that you know, I think from personally from a high level point of view I'm very interested, you know talking about this divide sort of this fork in the road in terms of What's the path to open studying open-endedness studying it in silico or studying it in situ in the setting of an actual open-ended system like a user App interaction or you know the interaction between a user and a piece of software on the web Or potentially with many other users. There are such rich Existing systems online that are already open-ended because they amplify or connect the creativity and knowledge of humans To create more knowledge and more creative artifacts. And so I think what's really uh, interesting in my mind now is sort of studying Systems or algorithms that allow us to better steer the creativity of humans As they are mediated by software And basically allow us to essentially amplify Existing intelligent or creative systems that are open-ended so amplify existing open open-endedness rather than try to build it from scratch Amazing guys. It's been an honor to have you on MLS T. Thank you so much. Thanks. Thank you. Yeah Great cool. Yeah, we're done", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.88, "text": " Open-endedness is essentially, you know, we're studying systems that can generate their own data in an infinite capacity", "tokens": [50364, 7238, 12, 3502, 1287, 307, 4476, 11, 291, 458, 11, 321, 434, 7601, 3652, 300, 393, 8460, 641, 1065, 1412, 294, 364, 13785, 6042, 50658], "temperature": 0.0, "avg_logprob": -0.1880773012755347, "compression_ratio": 1.9061371841155235, "no_speech_prob": 0.04013543948531151}, {"id": 1, "seek": 0, "start": 5.88, "end": 10.52, "text": " And so it's systems that essentially if you run it for longer and longer they get more and more complex", "tokens": [50658, 400, 370, 309, 311, 3652, 300, 4476, 498, 291, 1190, 309, 337, 2854, 293, 2854, 436, 483, 544, 293, 544, 3997, 50890], "temperature": 0.0, "avg_logprob": -0.1880773012755347, "compression_ratio": 1.9061371841155235, "no_speech_prob": 0.04013543948531151}, {"id": 2, "seek": 0, "start": 10.52, "end": 13.76, "text": " They generate more and more quote-unquote interestingness or interesting data", "tokens": [50890, 814, 8460, 544, 293, 544, 6513, 12, 409, 25016, 1880, 1287, 420, 1880, 1412, 51052], "temperature": 0.0, "avg_logprob": -0.1880773012755347, "compression_ratio": 1.9061371841155235, "no_speech_prob": 0.04013543948531151}, {"id": 3, "seek": 0, "start": 14.24, "end": 19.04, "text": " And so if we can actually, you know, crack this nut of how do we actually come up with a", "tokens": [51076, 400, 370, 498, 321, 393, 767, 11, 291, 458, 11, 6226, 341, 5393, 295, 577, 360, 321, 767, 808, 493, 365, 257, 51316], "temperature": 0.0, "avg_logprob": -0.1880773012755347, "compression_ratio": 1.9061371841155235, "no_speech_prob": 0.04013543948531151}, {"id": 4, "seek": 0, "start": 19.56, "end": 22.52, "text": " Self-improving system in the sense that it keeps generating interesting data", "tokens": [51342, 16348, 12, 332, 4318, 798, 1185, 294, 264, 2020, 300, 309, 5965, 17746, 1880, 1412, 51490], "temperature": 0.0, "avg_logprob": -0.1880773012755347, "compression_ratio": 1.9061371841155235, "no_speech_prob": 0.04013543948531151}, {"id": 5, "seek": 0, "start": 23.04, "end": 28.1, "text": " We can then use that data to train further train our models", "tokens": [51516, 492, 393, 550, 764, 300, 1412, 281, 3847, 3052, 3847, 527, 5245, 51769], "temperature": 0.0, "avg_logprob": -0.1880773012755347, "compression_ratio": 1.9061371841155235, "no_speech_prob": 0.04013543948531151}, {"id": 6, "seek": 2810, "start": 28.14, "end": 34.22, "text": " But of course you get into this perpetual data machine type of idea where obviously, you know", "tokens": [50366, 583, 295, 1164, 291, 483, 666, 341, 48216, 1412, 3479, 2010, 295, 1558, 689, 2745, 11, 291, 458, 50670], "temperature": 0.0, "avg_logprob": -0.1369190909645774, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.013216252438724041}, {"id": 7, "seek": 2810, "start": 34.22, "end": 36.22, "text": " There's how do you generate more data?", "tokens": [50670, 821, 311, 577, 360, 291, 8460, 544, 1412, 30, 50770], "temperature": 0.0, "avg_logprob": -0.1369190909645774, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.013216252438724041}, {"id": 8, "seek": 2810, "start": 36.38, "end": 40.54, "text": " If you know the data is ultimately coming from a model that you probably trained on previous data", "tokens": [50778, 759, 291, 458, 264, 1412, 307, 6284, 1348, 490, 257, 2316, 300, 291, 1391, 8895, 322, 3894, 1412, 50986], "temperature": 0.0, "avg_logprob": -0.1369190909645774, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.013216252438724041}, {"id": 9, "seek": 2810, "start": 40.54, "end": 42.18, "text": " How do you get net new information from that?", "tokens": [50986, 1012, 360, 291, 483, 2533, 777, 1589, 490, 300, 30, 51068], "temperature": 0.0, "avg_logprob": -0.1369190909645774, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.013216252438724041}, {"id": 10, "seek": 2810, "start": 42.18, "end": 47.42, "text": " Well, I think a lot of this is actually just resolved purely again going back to this idea of the reward function", "tokens": [51068, 1042, 11, 286, 519, 257, 688, 295, 341, 307, 767, 445, 20772, 17491, 797, 516, 646, 281, 341, 1558, 295, 264, 7782, 2445, 51330], "temperature": 0.0, "avg_logprob": -0.1369190909645774, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.013216252438724041}, {"id": 11, "seek": 2810, "start": 47.42, "end": 53.58, "text": " Right or a preference function where there is outside information coming in through some sort of filtering criteria", "tokens": [51330, 1779, 420, 257, 17502, 2445, 689, 456, 307, 2380, 1589, 1348, 294, 807, 512, 1333, 295, 30822, 11101, 51638], "temperature": 0.0, "avg_logprob": -0.1369190909645774, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.013216252438724041}, {"id": 12, "seek": 5358, "start": 53.58, "end": 61.82, "text": " For example human designers in the loop or designers designing some sort of preference model that could essentially automatically rate the kinds of automatic", "tokens": [50364, 1171, 1365, 1952, 16196, 294, 264, 6367, 420, 16196, 14685, 512, 1333, 295, 17502, 2316, 300, 727, 4476, 6772, 3314, 264, 3685, 295, 12509, 50776], "temperature": 0.0, "avg_logprob": -0.27712682214113743, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.04598667845129967}, {"id": 13, "seek": 5358, "start": 62.46, "end": 66.98, "text": " Data that's being generated by these open-ended systems. What does waker stand for?", "tokens": [50808, 11888, 300, 311, 885, 10833, 538, 613, 1269, 12, 3502, 3652, 13, 708, 775, 261, 4003, 1463, 337, 30, 51034], "temperature": 0.0, "avg_logprob": -0.27712682214113743, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.04598667845129967}, {"id": 14, "seek": 5358, "start": 67.06, "end": 72.46, "text": " Right, so waker stands for a weighted acquisition of knowledge across environments for robustness", "tokens": [51038, 1779, 11, 370, 261, 4003, 7382, 337, 257, 32807, 21668, 295, 3601, 2108, 12388, 337, 13956, 1287, 51308], "temperature": 0.0, "avg_logprob": -0.27712682214113743, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.04598667845129967}, {"id": 15, "seek": 5358, "start": 73.42, "end": 75.42, "text": " Fantastic, and what was the title of the paper?", "tokens": [51356, 21320, 11, 293, 437, 390, 264, 4876, 295, 264, 3035, 30, 51456], "temperature": 0.0, "avg_logprob": -0.27712682214113743, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.04598667845129967}, {"id": 16, "seek": 5358, "start": 76.42, "end": 79.42, "text": " Oh, right. Yeah reward free curricula. Oh girl. What was the title?", "tokens": [51506, 876, 11, 558, 13, 865, 7782, 1737, 13179, 3780, 13, 876, 2013, 13, 708, 390, 264, 4876, 30, 51656], "temperature": 0.0, "avg_logprob": -0.27712682214113743, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.04598667845129967}, {"id": 17, "seek": 7942, "start": 80.42, "end": 85.14, "text": " reward free curricula for training robust world models, that was it. Okay, so", "tokens": [50414, 7782, 1737, 13179, 3780, 337, 3097, 13956, 1002, 5245, 11, 300, 390, 309, 13, 1033, 11, 370, 50650], "temperature": 0.0, "avg_logprob": -0.2092041383709824, "compression_ratio": 1.7281879194630871, "no_speech_prob": 0.0008290899568237364}, {"id": 18, "seek": 7942, "start": 86.86, "end": 91.46000000000001, "text": " Give us the elevator pitch. Yeah, totally. So basically like the overarching", "tokens": [50736, 5303, 505, 264, 18782, 7293, 13, 865, 11, 3879, 13, 407, 1936, 411, 264, 45501, 50966], "temperature": 0.0, "avg_logprob": -0.2092041383709824, "compression_ratio": 1.7281879194630871, "no_speech_prob": 0.0008290899568237364}, {"id": 19, "seek": 7942, "start": 92.62, "end": 97.62, "text": " Question that we're trying to answer with this paper is like how should we go about training like very general agents?", "tokens": [51024, 14464, 300, 321, 434, 1382, 281, 1867, 365, 341, 3035, 307, 411, 577, 820, 321, 352, 466, 3097, 411, 588, 2674, 12554, 30, 51274], "temperature": 0.0, "avg_logprob": -0.2092041383709824, "compression_ratio": 1.7281879194630871, "no_speech_prob": 0.0008290899568237364}, {"id": 20, "seek": 7942, "start": 97.82000000000001, "end": 103.3, "text": " So in the context of the paper, we think of a general agent as being one that's able to perform a lot of different tasks", "tokens": [51284, 407, 294, 264, 4319, 295, 264, 3035, 11, 321, 519, 295, 257, 2674, 9461, 382, 885, 472, 300, 311, 1075, 281, 2042, 257, 688, 295, 819, 9608, 51558], "temperature": 0.0, "avg_logprob": -0.2092041383709824, "compression_ratio": 1.7281879194630871, "no_speech_prob": 0.0008290899568237364}, {"id": 21, "seek": 7942, "start": 103.38, "end": 107.86, "text": " So we might think of these as different reward functions or for thinking of it from a reinforcement learning perspective", "tokens": [51562, 407, 321, 1062, 519, 295, 613, 382, 819, 7782, 6828, 420, 337, 1953, 295, 309, 490, 257, 29280, 2539, 4585, 51786], "temperature": 0.0, "avg_logprob": -0.2092041383709824, "compression_ratio": 1.7281879194630871, "no_speech_prob": 0.0008290899568237364}, {"id": 22, "seek": 10786, "start": 108.1, "end": 110.94, "text": " But also be able to perform those tasks in lots of different environments", "tokens": [50376, 583, 611, 312, 1075, 281, 2042, 729, 9608, 294, 3195, 295, 819, 12388, 50518], "temperature": 0.0, "avg_logprob": -0.15420574612087673, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.0032721241004765034}, {"id": 23, "seek": 10786, "start": 110.94, "end": 116.86, "text": " So, you know, we don't want a robot to just be able to do you know pick up tasks do tasks in my like my kitchen", "tokens": [50518, 407, 11, 291, 458, 11, 321, 500, 380, 528, 257, 7881, 281, 445, 312, 1075, 281, 360, 291, 458, 1888, 493, 9608, 360, 9608, 294, 452, 411, 452, 6525, 50814], "temperature": 0.0, "avg_logprob": -0.15420574612087673, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.0032721241004765034}, {"id": 24, "seek": 10786, "start": 116.86, "end": 118.86, "text": " specifically we want the robot to be able to go into like arbitrary", "tokens": [50814, 4682, 321, 528, 264, 7881, 281, 312, 1075, 281, 352, 666, 411, 23211, 50914], "temperature": 0.0, "avg_logprob": -0.15420574612087673, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.0032721241004765034}, {"id": 25, "seek": 10786, "start": 119.42, "end": 124.32, "text": " Apartments and also be able to do those tasks in like arbitrary environments. And so we kind of thought about like, yeah", "tokens": [50942, 24111, 1117, 293, 611, 312, 1075, 281, 360, 729, 9608, 294, 411, 23211, 12388, 13, 400, 370, 321, 733, 295, 1194, 466, 411, 11, 1338, 51187], "temperature": 0.0, "avg_logprob": -0.15420574612087673, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.0032721241004765034}, {"id": 26, "seek": 10786, "start": 124.34, "end": 127.1, "text": " How do we want to create an agent that can do such a thing?", "tokens": [51188, 1012, 360, 321, 528, 281, 1884, 364, 9461, 300, 393, 360, 1270, 257, 551, 30, 51326], "temperature": 0.0, "avg_logprob": -0.15420574612087673, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.0032721241004765034}, {"id": 27, "seek": 10786, "start": 127.1, "end": 132.54, "text": " And we argue in the paper that a good way of doing it would be to have an agent that has a very general world model", "tokens": [51326, 400, 321, 9695, 294, 264, 3035, 300, 257, 665, 636, 295, 884, 309, 576, 312, 281, 362, 364, 9461, 300, 575, 257, 588, 2674, 1002, 2316, 51598], "temperature": 0.0, "avg_logprob": -0.15420574612087673, "compression_ratio": 1.896551724137931, "no_speech_prob": 0.0032721241004765034}, {"id": 28, "seek": 13254, "start": 132.9, "end": 138.73999999999998, "text": " So a world model meaning that it can predict the outcome of sequences of actions and predict what will happen if it does certain actions", "tokens": [50382, 407, 257, 1002, 2316, 3620, 300, 309, 393, 6069, 264, 9700, 295, 22978, 295, 5909, 293, 6069, 437, 486, 1051, 498, 309, 775, 1629, 5909, 50674], "temperature": 0.0, "avg_logprob": -0.16034087089643087, "compression_ratio": 2.0220125786163523, "no_speech_prob": 0.017145833000540733}, {"id": 29, "seek": 13254, "start": 138.73999999999998, "end": 142.73999999999998, "text": " And so we argue if we have a very general world model that can lead to a very general agent", "tokens": [50674, 400, 370, 321, 9695, 498, 321, 362, 257, 588, 2674, 1002, 2316, 300, 393, 1477, 281, 257, 588, 2674, 9461, 50874], "temperature": 0.0, "avg_logprob": -0.16034087089643087, "compression_ratio": 2.0220125786163523, "no_speech_prob": 0.017145833000540733}, {"id": 30, "seek": 13254, "start": 142.73999999999998, "end": 146.45999999999998, "text": " That's able to perform, you know, a variety of tasks in different environments", "tokens": [50874, 663, 311, 1075, 281, 2042, 11, 291, 458, 11, 257, 5673, 295, 9608, 294, 819, 12388, 51060], "temperature": 0.0, "avg_logprob": -0.16034087089643087, "compression_ratio": 2.0220125786163523, "no_speech_prob": 0.017145833000540733}, {"id": 31, "seek": 13254, "start": 146.45999999999998, "end": 151.62, "text": " And so then, you know, once we've established that we kind of ask the question of how do we get a very general world model?", "tokens": [51060, 400, 370, 550, 11, 291, 458, 11, 1564, 321, 600, 7545, 300, 321, 733, 295, 1029, 264, 1168, 295, 577, 360, 321, 483, 257, 588, 2674, 1002, 2316, 30, 51318], "temperature": 0.0, "avg_logprob": -0.16034087089643087, "compression_ratio": 2.0220125786163523, "no_speech_prob": 0.017145833000540733}, {"id": 32, "seek": 13254, "start": 151.62, "end": 153.85999999999999, "text": " And what does it mean to have a good world model that works?", "tokens": [51318, 400, 437, 775, 309, 914, 281, 362, 257, 665, 1002, 2316, 300, 1985, 30, 51430], "temperature": 0.0, "avg_logprob": -0.16034087089643087, "compression_ratio": 2.0220125786163523, "no_speech_prob": 0.017145833000540733}, {"id": 33, "seek": 13254, "start": 154.26, "end": 160.14, "text": " While in a very general setting across different environments and different tasks like how do we define that and how should we gather data to do that?", "tokens": [51450, 3987, 294, 257, 588, 2674, 3287, 2108, 819, 12388, 293, 819, 9608, 411, 577, 360, 321, 6964, 300, 293, 577, 820, 321, 5448, 1412, 281, 360, 300, 30, 51744], "temperature": 0.0, "avg_logprob": -0.16034087089643087, "compression_ratio": 2.0220125786163523, "no_speech_prob": 0.017145833000540733}, {"id": 34, "seek": 16014, "start": 161.1, "end": 166.7, "text": " Beautiful. So I really enjoyed reading the paper and it reminded me a lot of Kenneth Stanley's poet paper", "tokens": [50412, 14724, 13, 407, 286, 534, 4626, 3760, 264, 3035, 293, 309, 15920, 385, 257, 688, 295, 33735, 28329, 311, 20874, 3035, 50692], "temperature": 0.0, "avg_logprob": -0.18714647614554072, "compression_ratio": 1.7245762711864407, "no_speech_prob": 0.0032527754083275795}, {"id": 35, "seek": 16014, "start": 166.85999999999999, "end": 173.22, "text": " So he was doing this thing called curriculum learning and it's really related to machine teaching as well", "tokens": [50700, 407, 415, 390, 884, 341, 551, 1219, 14302, 2539, 293, 309, 311, 534, 4077, 281, 3479, 4571, 382, 731, 51018], "temperature": 0.0, "avg_logprob": -0.18714647614554072, "compression_ratio": 1.7245762711864407, "no_speech_prob": 0.0032527754083275795}, {"id": 36, "seek": 16014, "start": 173.22, "end": 178.29999999999998, "text": " There's quite a few things in machine learning where you say well if we had a really principled way of", "tokens": [51018, 821, 311, 1596, 257, 1326, 721, 294, 3479, 2539, 689, 291, 584, 731, 498, 321, 632, 257, 534, 3681, 15551, 636, 295, 51272], "temperature": 0.0, "avg_logprob": -0.18714647614554072, "compression_ratio": 1.7245762711864407, "no_speech_prob": 0.0032527754083275795}, {"id": 37, "seek": 16014, "start": 178.7, "end": 183.29999999999998, "text": " Selecting the best training data and presenting it to the learner in the best possible order", "tokens": [51292, 13638, 278, 264, 1151, 3097, 1412, 293, 15578, 309, 281, 264, 33347, 294, 264, 1151, 1944, 1668, 51522], "temperature": 0.0, "avg_logprob": -0.18714647614554072, "compression_ratio": 1.7245762711864407, "no_speech_prob": 0.0032527754083275795}, {"id": 38, "seek": 18330, "start": 183.34, "end": 190.06, "text": " Could the learner be better and in that poet paper Stanley was kind of generating a diverse set of", "tokens": [50366, 7497, 264, 33347, 312, 1101, 293, 294, 300, 20874, 3035, 28329, 390, 733, 295, 17746, 257, 9521, 992, 295, 50702], "temperature": 0.0, "avg_logprob": -0.1893681034897313, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.026287533342838287}, {"id": 39, "seek": 18330, "start": 190.3, "end": 197.42000000000002, "text": " Environments and like training a learner on those things and you're doing something very similar and you're using this mini max regret", "tokens": [50714, 19286, 1117, 293, 411, 3097, 257, 33347, 322, 729, 721, 293, 291, 434, 884, 746, 588, 2531, 293, 291, 434, 1228, 341, 8382, 11469, 10879, 51070], "temperature": 0.0, "avg_logprob": -0.1893681034897313, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.026287533342838287}, {"id": 40, "seek": 18330, "start": 197.42000000000002, "end": 201.62, "text": " Which is a concept from decision theory. Can you bring that in? Yeah, absolutely. So", "tokens": [51070, 3013, 307, 257, 3410, 490, 3537, 5261, 13, 1664, 291, 1565, 300, 294, 30, 865, 11, 3122, 13, 407, 51280], "temperature": 0.0, "avg_logprob": -0.1893681034897313, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.026287533342838287}, {"id": 41, "seek": 18330, "start": 202.62, "end": 205.26000000000002, "text": " So I guess we have this notion of like wanting to be", "tokens": [51330, 407, 286, 2041, 321, 362, 341, 10710, 295, 411, 7935, 281, 312, 51462], "temperature": 0.0, "avg_logprob": -0.1893681034897313, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.026287533342838287}, {"id": 42, "seek": 18330, "start": 205.74, "end": 208.46, "text": " To perform well across a wide range of scenarios, right?", "tokens": [51486, 1407, 2042, 731, 2108, 257, 4874, 3613, 295, 15077, 11, 558, 30, 51622], "temperature": 0.0, "avg_logprob": -0.1893681034897313, "compression_ratio": 1.621212121212121, "no_speech_prob": 0.026287533342838287}, {"id": 43, "seek": 20846, "start": 208.5, "end": 213.18, "text": " So scenarios in our context mean like different environments and different tasks and kind of like the most", "tokens": [50366, 407, 15077, 294, 527, 4319, 914, 411, 819, 12388, 293, 819, 9608, 293, 733, 295, 411, 264, 881, 50600], "temperature": 0.0, "avg_logprob": -0.20020246913290432, "compression_ratio": 1.801948051948052, "no_speech_prob": 0.003272414207458496}, {"id": 44, "seek": 20846, "start": 213.78, "end": 218.82, "text": " Standard way of thinking about that, especially in reinforcement learning or a machine learning in general as you think about like the average performance", "tokens": [50630, 21298, 636, 295, 1953, 466, 300, 11, 2318, 294, 29280, 2539, 420, 257, 3479, 2539, 294, 2674, 382, 291, 519, 466, 411, 264, 4274, 3389, 50882], "temperature": 0.0, "avg_logprob": -0.20020246913290432, "compression_ratio": 1.801948051948052, "no_speech_prob": 0.003272414207458496}, {"id": 45, "seek": 20846, "start": 218.82, "end": 223.22, "text": " So so how do I optimize like the expected reward across all of these different scenarios?", "tokens": [50882, 407, 370, 577, 360, 286, 19719, 411, 264, 5176, 7782, 2108, 439, 295, 613, 819, 15077, 30, 51102], "temperature": 0.0, "avg_logprob": -0.20020246913290432, "compression_ratio": 1.801948051948052, "no_speech_prob": 0.003272414207458496}, {"id": 46, "seek": 20846, "start": 223.78, "end": 227.10000000000002, "text": " and a lot of the work that that munchies done as well kind of argues that", "tokens": [51130, 293, 257, 688, 295, 264, 589, 300, 300, 275, 1680, 530, 1096, 382, 731, 733, 295, 38218, 300, 51296], "temperature": 0.0, "avg_logprob": -0.20020246913290432, "compression_ratio": 1.801948051948052, "no_speech_prob": 0.003272414207458496}, {"id": 47, "seek": 20846, "start": 227.66, "end": 230.9, "text": " Just just optimizing for expectation isn't necessarily the best", "tokens": [51324, 1449, 445, 40425, 337, 14334, 1943, 380, 4725, 264, 1151, 51486], "temperature": 0.0, "avg_logprob": -0.20020246913290432, "compression_ratio": 1.801948051948052, "no_speech_prob": 0.003272414207458496}, {"id": 48, "seek": 20846, "start": 231.70000000000002, "end": 235.18, "text": " The best objective so, you know, we can imagine in the real world", "tokens": [51526, 440, 1151, 10024, 370, 11, 291, 458, 11, 321, 393, 3811, 294, 264, 957, 1002, 51700], "temperature": 0.0, "avg_logprob": -0.20020246913290432, "compression_ratio": 1.801948051948052, "no_speech_prob": 0.003272414207458496}, {"id": 49, "seek": 23518, "start": 235.22, "end": 238.62, "text": " We don't really know like the distribution over possible tasks or or anything", "tokens": [50366, 492, 500, 380, 534, 458, 411, 264, 7316, 670, 1944, 9608, 420, 420, 1340, 50536], "temperature": 0.0, "avg_logprob": -0.16368189725008878, "compression_ratio": 1.919463087248322, "no_speech_prob": 0.0008039900567382574}, {"id": 50, "seek": 23518, "start": 238.62, "end": 242.86, "text": " Well, you know in most situations, we don't know things like that and so maybe a better objective is to try and be", "tokens": [50536, 1042, 11, 291, 458, 294, 881, 6851, 11, 321, 500, 380, 458, 721, 411, 300, 293, 370, 1310, 257, 1101, 10024, 307, 281, 853, 293, 312, 50748], "temperature": 0.0, "avg_logprob": -0.16368189725008878, "compression_ratio": 1.919463087248322, "no_speech_prob": 0.0008039900567382574}, {"id": 51, "seek": 23518, "start": 243.22, "end": 247.22, "text": " robust instead and robust basically we can think of that as meaning like we should do", "tokens": [50766, 13956, 2602, 293, 13956, 1936, 321, 393, 519, 295, 300, 382, 3620, 411, 321, 820, 360, 50966], "temperature": 0.0, "avg_logprob": -0.16368189725008878, "compression_ratio": 1.919463087248322, "no_speech_prob": 0.0008039900567382574}, {"id": 52, "seek": 23518, "start": 247.70000000000002, "end": 252.22, "text": " Reasonably well in every situation we could be in and that that's kind of what a robust objective is", "tokens": [50990, 39693, 1188, 731, 294, 633, 2590, 321, 727, 312, 294, 293, 300, 300, 311, 733, 295, 437, 257, 13956, 10024, 307, 51216], "temperature": 0.0, "avg_logprob": -0.16368189725008878, "compression_ratio": 1.919463087248322, "no_speech_prob": 0.0008039900567382574}, {"id": 53, "seek": 23518, "start": 252.94, "end": 256.66, "text": " And one of the ways that you can define a robust objective is via mini max regret", "tokens": [51252, 400, 472, 295, 264, 2098, 300, 291, 393, 6964, 257, 13956, 10024, 307, 5766, 8382, 11469, 10879, 51438], "temperature": 0.0, "avg_logprob": -0.16368189725008878, "compression_ratio": 1.919463087248322, "no_speech_prob": 0.0008039900567382574}, {"id": 54, "seek": 23518, "start": 256.66, "end": 262.3, "text": " And so regret means like suboptimality like how well that I do relative to the best I could have possibly done", "tokens": [51438, 400, 370, 10879, 1355, 411, 1422, 5747, 332, 1860, 411, 577, 731, 300, 286, 360, 4972, 281, 264, 1151, 286, 727, 362, 6264, 1096, 51720], "temperature": 0.0, "avg_logprob": -0.16368189725008878, "compression_ratio": 1.919463087248322, "no_speech_prob": 0.0008039900567382574}, {"id": 55, "seek": 26230, "start": 262.34000000000003, "end": 265.22, "text": " So that means basically the same thing as it does in normal English", "tokens": [50366, 407, 300, 1355, 1936, 264, 912, 551, 382, 309, 775, 294, 2710, 3669, 50510], "temperature": 0.0, "avg_logprob": -0.20130420332195378, "compression_ratio": 2.0381944444444446, "no_speech_prob": 0.0005526197492145002}, {"id": 56, "seek": 26230, "start": 265.62, "end": 270.66, "text": " And so the mini max regret objective basically says across all possible situations. I want to try and do", "tokens": [50530, 400, 370, 264, 8382, 11469, 10879, 10024, 1936, 1619, 2108, 439, 1944, 6851, 13, 286, 528, 281, 853, 293, 360, 50782], "temperature": 0.0, "avg_logprob": -0.20130420332195378, "compression_ratio": 2.0381944444444446, "no_speech_prob": 0.0005526197492145002}, {"id": 57, "seek": 26230, "start": 271.7, "end": 277.02000000000004, "text": " Minimize the regret across all possible situations minimize the maximum regret I should say so that means in all possible situations", "tokens": [50834, 2829, 43890, 264, 10879, 2108, 439, 1944, 6851, 17522, 264, 6674, 10879, 286, 820, 584, 370, 300, 1355, 294, 439, 1944, 6851, 51100], "temperature": 0.0, "avg_logprob": -0.20130420332195378, "compression_ratio": 2.0381944444444446, "no_speech_prob": 0.0005526197492145002}, {"id": 58, "seek": 26230, "start": 277.02000000000004, "end": 280.38, "text": " We should do almost as well as the best we could have possibly done", "tokens": [51100, 492, 820, 360, 1920, 382, 731, 382, 264, 1151, 321, 727, 362, 6264, 1096, 51268], "temperature": 0.0, "avg_logprob": -0.20130420332195378, "compression_ratio": 2.0381944444444446, "no_speech_prob": 0.0005526197492145002}, {"id": 59, "seek": 26230, "start": 280.90000000000003, "end": 284.3, "text": " And I guess just to contrast this against the standard objective for robustness", "tokens": [51294, 400, 286, 2041, 445, 281, 8712, 341, 1970, 264, 3832, 10024, 337, 13956, 1287, 51464], "temperature": 0.0, "avg_logprob": -0.20130420332195378, "compression_ratio": 2.0381944444444446, "no_speech_prob": 0.0005526197492145002}, {"id": 60, "seek": 26230, "start": 284.3, "end": 291.3, "text": " So the more common objective for robustness at least traditionally is like a maximum performance. That means maximize the performance", "tokens": [51464, 407, 264, 544, 2689, 10024, 337, 13956, 1287, 412, 1935, 19067, 307, 411, 257, 6674, 3389, 13, 663, 1355, 19874, 264, 3389, 51814], "temperature": 0.0, "avg_logprob": -0.20130420332195378, "compression_ratio": 2.0381944444444446, "no_speech_prob": 0.0005526197492145002}, {"id": 61, "seek": 29130, "start": 291.82, "end": 296.90000000000003, "text": " While the environments like minimizing and choosing the most adversarial environment or the most adversarial scenario", "tokens": [50390, 3987, 264, 12388, 411, 46608, 293, 10875, 264, 881, 17641, 44745, 2823, 420, 264, 881, 17641, 44745, 9005, 50644], "temperature": 0.0, "avg_logprob": -0.17641981657560882, "compression_ratio": 1.8971631205673758, "no_speech_prob": 0.001926300348713994}, {"id": 62, "seek": 29130, "start": 297.82, "end": 302.86, "text": " But but the problem with kind of the maximum objective is that in some environments you just can't do anything", "tokens": [50690, 583, 457, 264, 1154, 365, 733, 295, 264, 6674, 10024, 307, 300, 294, 512, 12388, 291, 445, 393, 380, 360, 1340, 50942], "temperature": 0.0, "avg_logprob": -0.17641981657560882, "compression_ratio": 1.8971631205673758, "no_speech_prob": 0.001926300348713994}, {"id": 63, "seek": 29130, "start": 302.86, "end": 305.06, "text": " Let's say it's like some such situations is too hard", "tokens": [50942, 961, 311, 584, 309, 311, 411, 512, 1270, 6851, 307, 886, 1152, 51052], "temperature": 0.0, "avg_logprob": -0.17641981657560882, "compression_ratio": 1.8971631205673758, "no_speech_prob": 0.001926300348713994}, {"id": 64, "seek": 29130, "start": 305.06, "end": 310.62, "text": " You're doomed and so if in some situations you're doomed and you always get like zero reward or negative infinity reward", "tokens": [51052, 509, 434, 33847, 293, 370, 498, 294, 512, 6851, 291, 434, 33847, 293, 291, 1009, 483, 411, 4018, 7782, 420, 3671, 13202, 7782, 51330], "temperature": 0.0, "avg_logprob": -0.17641981657560882, "compression_ratio": 1.8971631205673758, "no_speech_prob": 0.001926300348713994}, {"id": 65, "seek": 29130, "start": 310.7, "end": 316.42, "text": " That means there's no incentive to try and do better in any other environment because your maximum reward is always going to be zero", "tokens": [51334, 663, 1355, 456, 311, 572, 22346, 281, 853, 293, 360, 1101, 294, 604, 661, 2823, 570, 428, 6674, 7782, 307, 1009, 516, 281, 312, 4018, 51620], "temperature": 0.0, "avg_logprob": -0.17641981657560882, "compression_ratio": 1.8971631205673758, "no_speech_prob": 0.001926300348713994}, {"id": 66, "seek": 31642, "start": 316.42, "end": 322.14000000000004, "text": " And so therefore I think like minty argues as well as Michael Dennis and a lot of these recent papers argue that mini max regret", "tokens": [50364, 400, 370, 4412, 286, 519, 411, 18189, 88, 38218, 382, 731, 382, 5116, 23376, 293, 257, 688, 295, 613, 5162, 10577, 9695, 300, 8382, 11469, 10879, 50650], "temperature": 0.0, "avg_logprob": -0.21132927314907896, "compression_ratio": 1.6133828996282529, "no_speech_prob": 0.00375555083155632}, {"id": 67, "seek": 31642, "start": 322.14000000000004, "end": 328.1, "text": " So minimizing the maximum self-optimality is actually like a better objective for a general agent. That's robust fascinating. So", "tokens": [50650, 407, 46608, 264, 6674, 2698, 12, 5747, 332, 1860, 307, 767, 411, 257, 1101, 10024, 337, 257, 2674, 9461, 13, 663, 311, 13956, 10343, 13, 407, 50948], "temperature": 0.0, "avg_logprob": -0.21132927314907896, "compression_ratio": 1.6133828996282529, "no_speech_prob": 0.00375555083155632}, {"id": 68, "seek": 31642, "start": 328.90000000000003, "end": 332.98, "text": " If I understand correctly is it a way of saying I want to have the best case", "tokens": [50988, 759, 286, 1223, 8944, 307, 309, 257, 636, 295, 1566, 286, 528, 281, 362, 264, 1151, 1389, 51192], "temperature": 0.0, "avg_logprob": -0.21132927314907896, "compression_ratio": 1.6133828996282529, "no_speech_prob": 0.00375555083155632}, {"id": 69, "seek": 31642, "start": 333.54, "end": 335.26, "text": " worst", "tokens": [51220, 5855, 51306], "temperature": 0.0, "avg_logprob": -0.21132927314907896, "compression_ratio": 1.6133828996282529, "no_speech_prob": 0.00375555083155632}, {"id": 70, "seek": 31642, "start": 335.26, "end": 341.14, "text": " Expected regret. Yes. So basically mini max regret is saying that if you assume that you know", "tokens": [51306, 2111, 10729, 10879, 13, 1079, 13, 407, 1936, 8382, 11469, 10879, 307, 1566, 300, 498, 291, 6552, 300, 291, 458, 51600], "temperature": 0.0, "avg_logprob": -0.21132927314907896, "compression_ratio": 1.6133828996282529, "no_speech_prob": 0.00375555083155632}, {"id": 71, "seek": 34114, "start": 341.14, "end": 346.41999999999996, "text": " The environment is adversarial to you in some way like when you're training or at inference time when you're actually", "tokens": [50364, 440, 2823, 307, 17641, 44745, 281, 291, 294, 512, 636, 411, 562, 291, 434, 3097, 420, 412, 38253, 565, 562, 291, 434, 767, 50628], "temperature": 0.0, "avg_logprob": -0.20436471637926604, "compression_ratio": 1.7786561264822134, "no_speech_prob": 0.0050499155186116695}, {"id": 72, "seek": 34114, "start": 346.97999999999996, "end": 348.97999999999996, "text": " Testing your policy out in the real world", "tokens": [50656, 45517, 428, 3897, 484, 294, 264, 957, 1002, 50756], "temperature": 0.0, "avg_logprob": -0.20436471637926604, "compression_ratio": 1.7786561264822134, "no_speech_prob": 0.0050499155186116695}, {"id": 73, "seek": 34114, "start": 349.46, "end": 356.38, "text": " Mini max regret is saying the agent should behave the model should behave in a way that minimizes its worst case possible regret", "tokens": [50780, 18239, 11469, 10879, 307, 1566, 264, 9461, 820, 15158, 264, 2316, 820, 15158, 294, 257, 636, 300, 4464, 5660, 1080, 5855, 1389, 1944, 10879, 51126], "temperature": 0.0, "avg_logprob": -0.20436471637926604, "compression_ratio": 1.7786561264822134, "no_speech_prob": 0.0050499155186116695}, {"id": 74, "seek": 34114, "start": 356.65999999999997, "end": 360.7, "text": " Over all the possible conditions of the world that this adversary could choose", "tokens": [51140, 4886, 439, 264, 1944, 4487, 295, 264, 1002, 300, 341, 48222, 727, 2826, 51342], "temperature": 0.0, "avg_logprob": -0.20436471637926604, "compression_ratio": 1.7786561264822134, "no_speech_prob": 0.0050499155186116695}, {"id": 75, "seek": 34114, "start": 360.9, "end": 364.74, "text": " What's really interesting about this paper is we are talking about the reward free", "tokens": [51352, 708, 311, 534, 1880, 466, 341, 3035, 307, 321, 366, 1417, 466, 264, 7782, 1737, 51544], "temperature": 0.0, "avg_logprob": -0.20436471637926604, "compression_ratio": 1.7786561264822134, "no_speech_prob": 0.0050499155186116695}, {"id": 76, "seek": 36474, "start": 365.3, "end": 371.42, "text": " Exploration phase and we're also talking about the domain of model based reinforcement learning as opposed to", "tokens": [50392, 12514, 9357, 5574, 293, 321, 434, 611, 1417, 466, 264, 9274, 295, 2316, 2361, 29280, 2539, 382, 8851, 281, 50698], "temperature": 0.0, "avg_logprob": -0.18704449813977808, "compression_ratio": 1.881918819188192, "no_speech_prob": 0.06067928299307823}, {"id": 77, "seek": 36474, "start": 371.98, "end": 374.66, "text": " You know, let's say value based reinforcement learning where", "tokens": [50726, 509, 458, 11, 718, 311, 584, 2158, 2361, 29280, 2539, 689, 50860], "temperature": 0.0, "avg_logprob": -0.18704449813977808, "compression_ratio": 1.881918819188192, "no_speech_prob": 0.06067928299307823}, {"id": 78, "seek": 36474, "start": 376.22, "end": 379.86, "text": " You get this entanglement, right? So the dynamics the model of the world", "tokens": [50938, 509, 483, 341, 948, 656, 3054, 11, 558, 30, 407, 264, 15679, 264, 2316, 295, 264, 1002, 51120], "temperature": 0.0, "avg_logprob": -0.18704449813977808, "compression_ratio": 1.881918819188192, "no_speech_prob": 0.06067928299307823}, {"id": 79, "seek": 36474, "start": 379.86, "end": 383.38, "text": " It's still in there, but it's kind of in meshed with this with this value model", "tokens": [51120, 467, 311, 920, 294, 456, 11, 457, 309, 311, 733, 295, 294, 17407, 292, 365, 341, 365, 341, 2158, 2316, 51296], "temperature": 0.0, "avg_logprob": -0.18704449813977808, "compression_ratio": 1.881918819188192, "no_speech_prob": 0.06067928299307823}, {"id": 80, "seek": 36474, "start": 383.42, "end": 385.90000000000003, "text": " Whereas in model based reinforcement learning in a principal way", "tokens": [51298, 13813, 294, 2316, 2361, 29280, 2539, 294, 257, 9716, 636, 51422], "temperature": 0.0, "avg_logprob": -0.18704449813977808, "compression_ratio": 1.881918819188192, "no_speech_prob": 0.06067928299307823}, {"id": 81, "seek": 36474, "start": 385.90000000000003, "end": 391.22, "text": " We kind of separate out the parts so that we can do explicit planning and imagination and simulations and stuff like that", "tokens": [51422, 492, 733, 295, 4994, 484, 264, 3166, 370, 300, 321, 393, 360, 13691, 5038, 293, 12938, 293, 35138, 293, 1507, 411, 300, 51688], "temperature": 0.0, "avg_logprob": -0.18704449813977808, "compression_ratio": 1.881918819188192, "no_speech_prob": 0.06067928299307823}, {"id": 82, "seek": 39122, "start": 391.3, "end": 394.34000000000003, "text": " So we're very much in this model based domain, right? Yeah, absolutely", "tokens": [50368, 407, 321, 434, 588, 709, 294, 341, 2316, 2361, 9274, 11, 558, 30, 865, 11, 3122, 50520], "temperature": 0.0, "avg_logprob": -0.1384572488290292, "compression_ratio": 2.02803738317757, "no_speech_prob": 0.0022482441272586584}, {"id": 83, "seek": 39122, "start": 394.34000000000003, "end": 400.1, "text": " Yes, we focused on yeah model based reinforcement learning or some people like to call this like the world model setting more recently", "tokens": [50520, 1079, 11, 321, 5178, 322, 1338, 2316, 2361, 29280, 2539, 420, 512, 561, 411, 281, 818, 341, 411, 264, 1002, 2316, 3287, 544, 3938, 50808], "temperature": 0.0, "avg_logprob": -0.1384572488290292, "compression_ratio": 2.02803738317757, "no_speech_prob": 0.0022482441272586584}, {"id": 84, "seek": 39122, "start": 400.34000000000003, "end": 403.70000000000005, "text": " But yeah, like you said we you know in typical like model free reinforcement learning", "tokens": [50820, 583, 1338, 11, 411, 291, 848, 321, 291, 458, 294, 7476, 411, 2316, 1737, 29280, 2539, 50988], "temperature": 0.0, "avg_logprob": -0.1384572488290292, "compression_ratio": 2.02803738317757, "no_speech_prob": 0.0022482441272586584}, {"id": 85, "seek": 39122, "start": 403.78000000000003, "end": 406.74, "text": " We we typically aim to learn a policy and a value function and yeah", "tokens": [50992, 492, 321, 5850, 5939, 281, 1466, 257, 3897, 293, 257, 2158, 2445, 293, 1338, 51140], "temperature": 0.0, "avg_logprob": -0.1384572488290292, "compression_ratio": 2.02803738317757, "no_speech_prob": 0.0022482441272586584}, {"id": 86, "seek": 39122, "start": 406.74, "end": 412.90000000000003, "text": " As you said like that value function is kind of implicitly encoding the dynamics through the fact that we learn the value function using the bellman equation", "tokens": [51140, 1018, 291, 848, 411, 300, 2158, 2445, 307, 733, 295, 26947, 356, 43430, 264, 15679, 807, 264, 1186, 300, 321, 1466, 264, 2158, 2445, 1228, 264, 4549, 1601, 5367, 51448], "temperature": 0.0, "avg_logprob": -0.1384572488290292, "compression_ratio": 2.02803738317757, "no_speech_prob": 0.0022482441272586584}, {"id": 87, "seek": 39122, "start": 412.98, "end": 418.90000000000003, "text": " So so the bellman equation kind of propagates the information between like transition and the environments through the value function", "tokens": [51452, 407, 370, 264, 4549, 1601, 5367, 733, 295, 12425, 1024, 264, 1589, 1296, 411, 6034, 293, 264, 12388, 807, 264, 2158, 2445, 51748], "temperature": 0.0, "avg_logprob": -0.1384572488290292, "compression_ratio": 2.02803738317757, "no_speech_prob": 0.0022482441272586584}, {"id": 88, "seek": 41890, "start": 418.9, "end": 421.78, "text": " So so the value function will like implicitly have the dynamics in it", "tokens": [50364, 407, 370, 264, 2158, 2445, 486, 411, 26947, 356, 362, 264, 15679, 294, 309, 50508], "temperature": 0.0, "avg_logprob": -0.08938929092052371, "compression_ratio": 2.034700315457413, "no_speech_prob": 0.0003337731468491256}, {"id": 89, "seek": 41890, "start": 422.58, "end": 426.41999999999996, "text": " But in model based reinforcement learning we want to very explicitly model the dynamics of the environment", "tokens": [50548, 583, 294, 2316, 2361, 29280, 2539, 321, 528, 281, 588, 20803, 2316, 264, 15679, 295, 264, 2823, 50740], "temperature": 0.0, "avg_logprob": -0.08938929092052371, "compression_ratio": 2.034700315457413, "no_speech_prob": 0.0003337731468491256}, {"id": 90, "seek": 41890, "start": 426.82, "end": 430.58, "text": " And so what I mean by that is we want to be able to take some previous sequence of observations", "tokens": [50760, 400, 370, 437, 286, 914, 538, 300, 307, 321, 528, 281, 312, 1075, 281, 747, 512, 3894, 8310, 295, 18163, 50948], "temperature": 0.0, "avg_logprob": -0.08938929092052371, "compression_ratio": 2.034700315457413, "no_speech_prob": 0.0003337731468491256}, {"id": 91, "seek": 41890, "start": 430.82, "end": 433.94, "text": " Perhaps those are images and then also condition on the next action", "tokens": [50960, 10517, 729, 366, 5267, 293, 550, 611, 4188, 322, 264, 958, 3069, 51116], "temperature": 0.0, "avg_logprob": -0.08938929092052371, "compression_ratio": 2.034700315457413, "no_speech_prob": 0.0003337731468491256}, {"id": 92, "seek": 41890, "start": 433.94, "end": 438.41999999999996, "text": " We want to take in the environment and then be able to predict the distribution over the next observation or states", "tokens": [51116, 492, 528, 281, 747, 294, 264, 2823, 293, 550, 312, 1075, 281, 6069, 264, 7316, 670, 264, 958, 14816, 420, 4368, 51340], "temperature": 0.0, "avg_logprob": -0.08938929092052371, "compression_ratio": 2.034700315457413, "no_speech_prob": 0.0003337731468491256}, {"id": 93, "seek": 41890, "start": 438.41999999999996, "end": 440.73999999999995, "text": " We're very explicitly modeling the dynamics of the environment", "tokens": [51340, 492, 434, 588, 20803, 15983, 264, 15679, 295, 264, 2823, 51456], "temperature": 0.0, "avg_logprob": -0.08938929092052371, "compression_ratio": 2.034700315457413, "no_speech_prob": 0.0003337731468491256}, {"id": 94, "seek": 41890, "start": 441.06, "end": 445.38, "text": " Okay, now this is really interesting because you know people think about reinforcement learning and in reinforcement learning", "tokens": [51472, 1033, 11, 586, 341, 307, 534, 1880, 570, 291, 458, 561, 519, 466, 29280, 2539, 293, 294, 29280, 2539, 51688], "temperature": 0.0, "avg_logprob": -0.08938929092052371, "compression_ratio": 2.034700315457413, "no_speech_prob": 0.0003337731468491256}, {"id": 95, "seek": 44538, "start": 445.38, "end": 448.74, "text": " You don't so much care about having a model of the world", "tokens": [50364, 509, 500, 380, 370, 709, 1127, 466, 1419, 257, 2316, 295, 264, 1002, 50532], "temperature": 0.0, "avg_logprob": -0.09667379409074783, "compression_ratio": 1.8419354838709678, "no_speech_prob": 0.001322415191680193}, {"id": 96, "seek": 44538, "start": 449.06, "end": 455.21999999999997, "text": " You care about building trajectories that lead to some you know task or goal or whatever that you're interested in so like", "tokens": [50548, 509, 1127, 466, 2390, 18257, 2083, 300, 1477, 281, 512, 291, 458, 5633, 420, 3387, 420, 2035, 300, 291, 434, 3102, 294, 370, 411, 50856], "temperature": 0.0, "avg_logprob": -0.09667379409074783, "compression_ratio": 1.8419354838709678, "no_speech_prob": 0.001322415191680193}, {"id": 97, "seek": 44538, "start": 455.94, "end": 459.86, "text": " I mean just just in broader terms. What what what do we get from explicitly modeling the world?", "tokens": [50892, 286, 914, 445, 445, 294, 13227, 2115, 13, 708, 437, 437, 360, 321, 483, 490, 20803, 15983, 264, 1002, 30, 51088], "temperature": 0.0, "avg_logprob": -0.09667379409074783, "compression_ratio": 1.8419354838709678, "no_speech_prob": 0.001322415191680193}, {"id": 98, "seek": 44538, "start": 460.26, "end": 463.38, "text": " So there are there are a few arguments for why we would want to explicitly model the environment", "tokens": [51108, 407, 456, 366, 456, 366, 257, 1326, 12869, 337, 983, 321, 576, 528, 281, 20803, 2316, 264, 2823, 51264], "temperature": 0.0, "avg_logprob": -0.09667379409074783, "compression_ratio": 1.8419354838709678, "no_speech_prob": 0.001322415191680193}, {"id": 99, "seek": 44538, "start": 463.38, "end": 468.74, "text": " So so one of which is um a lot of people would argue that you get better sample efficiency by modeling the environment", "tokens": [51264, 407, 370, 472, 295, 597, 307, 1105, 257, 688, 295, 561, 576, 9695, 300, 291, 483, 1101, 6889, 10493, 538, 15983, 264, 2823, 51532], "temperature": 0.0, "avg_logprob": -0.09667379409074783, "compression_ratio": 1.8419354838709678, "no_speech_prob": 0.001322415191680193}, {"id": 100, "seek": 44538, "start": 468.98, "end": 472.18, "text": " And the argument for this is you know the reward function might be quite sparse", "tokens": [51544, 400, 264, 6770, 337, 341, 307, 291, 458, 264, 7782, 2445, 1062, 312, 1596, 637, 11668, 51704], "temperature": 0.0, "avg_logprob": -0.09667379409074783, "compression_ratio": 1.8419354838709678, "no_speech_prob": 0.001322415191680193}, {"id": 101, "seek": 47218, "start": 472.5, "end": 476.90000000000003, "text": " And so if you're just relying on like the propagation of rewards backwards to try and learn the optimal behavior", "tokens": [50380, 400, 370, 498, 291, 434, 445, 24140, 322, 411, 264, 38377, 295, 17203, 12204, 281, 853, 293, 1466, 264, 16252, 5223, 50600], "temperature": 0.0, "avg_logprob": -0.09571701793347374, "compression_ratio": 1.765079365079365, "no_speech_prob": 0.0053837778978049755}, {"id": 102, "seek": 47218, "start": 477.14, "end": 482.82, "text": " That might not be as efficient as actually learning the dynamics because the dynamics can be learned from every single transition that you have", "tokens": [50612, 663, 1062, 406, 312, 382, 7148, 382, 767, 2539, 264, 15679, 570, 264, 15679, 393, 312, 3264, 490, 633, 2167, 6034, 300, 291, 362, 50896], "temperature": 0.0, "avg_logprob": -0.09571701793347374, "compression_ratio": 1.765079365079365, "no_speech_prob": 0.0053837778978049755}, {"id": 103, "seek": 47218, "start": 482.90000000000003, "end": 485.46000000000004, "text": " It's kind of like a standard supervised or unsupervised learning problem", "tokens": [50900, 467, 311, 733, 295, 411, 257, 3832, 46533, 420, 2693, 12879, 24420, 2539, 1154, 51028], "temperature": 0.0, "avg_logprob": -0.09571701793347374, "compression_ratio": 1.765079365079365, "no_speech_prob": 0.0053837778978049755}, {"id": 104, "seek": 47218, "start": 485.46000000000004, "end": 490.9, "text": " So so you kind of have like a richer signal to learn from which might arguably lead to better sample efficiency", "tokens": [51028, 407, 370, 291, 733, 295, 362, 411, 257, 29021, 6358, 281, 1466, 490, 597, 1062, 26771, 1477, 281, 1101, 6889, 10493, 51300], "temperature": 0.0, "avg_logprob": -0.09571701793347374, "compression_ratio": 1.765079365079365, "no_speech_prob": 0.0053837778978049755}, {"id": 105, "seek": 47218, "start": 491.46000000000004, "end": 492.9, "text": " um, but I think like", "tokens": [51328, 1105, 11, 457, 286, 519, 411, 51400], "temperature": 0.0, "avg_logprob": -0.09571701793347374, "compression_ratio": 1.765079365079365, "no_speech_prob": 0.0053837778978049755}, {"id": 106, "seek": 47218, "start": 492.9, "end": 497.06, "text": " More concrete arguments that I would argue for or that if you have a model of the environment", "tokens": [51400, 5048, 9859, 12869, 300, 286, 576, 9695, 337, 420, 300, 498, 291, 362, 257, 2316, 295, 264, 2823, 51608], "temperature": 0.0, "avg_logprob": -0.09571701793347374, "compression_ratio": 1.765079365079365, "no_speech_prob": 0.0053837778978049755}, {"id": 107, "seek": 49706, "start": 497.14, "end": 502.26, "text": " It's it's some kind of more general thing that you can then use to develop better decision making later on", "tokens": [50368, 467, 311, 309, 311, 512, 733, 295, 544, 2674, 551, 300, 291, 393, 550, 764, 281, 1499, 1101, 3537, 1455, 1780, 322, 50624], "temperature": 0.0, "avg_logprob": -0.0847609904077318, "compression_ratio": 1.9840255591054312, "no_speech_prob": 0.024419032037258148}, {"id": 108, "seek": 49706, "start": 502.26, "end": 504.26, "text": " So so if you just learn a value function", "tokens": [50624, 407, 370, 498, 291, 445, 1466, 257, 2158, 2445, 50724], "temperature": 0.0, "avg_logprob": -0.0847609904077318, "compression_ratio": 1.9840255591054312, "no_speech_prob": 0.024419032037258148}, {"id": 109, "seek": 49706, "start": 504.9, "end": 510.18, "text": " You're kind of only learning how to optimally do that specific reward function or optimize that specific reward function", "tokens": [50756, 509, 434, 733, 295, 787, 2539, 577, 281, 5028, 379, 360, 300, 2685, 7782, 2445, 420, 19719, 300, 2685, 7782, 2445, 51020], "temperature": 0.0, "avg_logprob": -0.0847609904077318, "compression_ratio": 1.9840255591054312, "no_speech_prob": 0.024419032037258148}, {"id": 110, "seek": 49706, "start": 510.66, "end": 514.82, "text": " Um, but if we have a model of the environment, we can kind of arbitrarily be given some task later down", "tokens": [51044, 3301, 11, 457, 498, 321, 362, 257, 2316, 295, 264, 2823, 11, 321, 393, 733, 295, 19071, 3289, 312, 2212, 512, 5633, 1780, 760, 51252], "temperature": 0.0, "avg_logprob": -0.0847609904077318, "compression_ratio": 1.9840255591054312, "no_speech_prob": 0.024419032037258148}, {"id": 111, "seek": 49706, "start": 514.82, "end": 520.34, "text": " Whether it be a reward function or a goal state or something like that and we can then plan to optimize that task later down the road", "tokens": [51252, 8503, 309, 312, 257, 7782, 2445, 420, 257, 3387, 1785, 420, 746, 411, 300, 293, 321, 393, 550, 1393, 281, 19719, 300, 5633, 1780, 760, 264, 3060, 51528], "temperature": 0.0, "avg_logprob": -0.0847609904077318, "compression_ratio": 1.9840255591054312, "no_speech_prob": 0.024419032037258148}, {"id": 112, "seek": 49706, "start": 520.66, "end": 522.5, "text": " so I would think that um", "tokens": [51544, 370, 286, 576, 519, 300, 1105, 51636], "temperature": 0.0, "avg_logprob": -0.0847609904077318, "compression_ratio": 1.9840255591054312, "no_speech_prob": 0.024419032037258148}, {"id": 113, "seek": 49706, "start": 522.5, "end": 525.94, "text": " You know, it's kind of a much more general way of having a powerful decision making agent", "tokens": [51636, 509, 458, 11, 309, 311, 733, 295, 257, 709, 544, 2674, 636, 295, 1419, 257, 4005, 3537, 1455, 9461, 51808], "temperature": 0.0, "avg_logprob": -0.0847609904077318, "compression_ratio": 1.9840255591054312, "no_speech_prob": 0.024419032037258148}, {"id": 114, "seek": 52594, "start": 526.2600000000001, "end": 530.34, "text": " Rather than just specifying like one task and learning the optimal kind of policy for one task", "tokens": [50380, 16571, 813, 445, 1608, 5489, 411, 472, 5633, 293, 2539, 264, 16252, 733, 295, 3897, 337, 472, 5633, 50584], "temperature": 0.0, "avg_logprob": -0.08069358553205218, "compression_ratio": 1.9331306990881458, "no_speech_prob": 0.004467344842851162}, {"id": 115, "seek": 52594, "start": 530.6600000000001, "end": 532.6600000000001, "text": " and I guess another thing that I'll add to that is um", "tokens": [50600, 293, 286, 2041, 1071, 551, 300, 286, 603, 909, 281, 300, 307, 1105, 50700], "temperature": 0.0, "avg_logprob": -0.08069358553205218, "compression_ratio": 1.9331306990881458, "no_speech_prob": 0.004467344842851162}, {"id": 116, "seek": 52594, "start": 533.46, "end": 537.46, "text": " Rather than only learning like a feedforward policy like you wouldn't reinforcement learning", "tokens": [50740, 16571, 813, 787, 2539, 411, 257, 3154, 13305, 3897, 411, 291, 2759, 380, 29280, 2539, 50940], "temperature": 0.0, "avg_logprob": -0.08069358553205218, "compression_ratio": 1.9331306990881458, "no_speech_prob": 0.004467344842851162}, {"id": 117, "seek": 52594, "start": 537.5400000000001, "end": 539.5400000000001, "text": " So something that maps directly to actions", "tokens": [50944, 407, 746, 300, 11317, 3838, 281, 5909, 51044], "temperature": 0.0, "avg_logprob": -0.08069358553205218, "compression_ratio": 1.9331306990881458, "no_speech_prob": 0.004467344842851162}, {"id": 118, "seek": 52594, "start": 539.62, "end": 542.58, "text": " The other thing that a world model allows you to do is also to do online planning", "tokens": [51048, 440, 661, 551, 300, 257, 1002, 2316, 4045, 291, 281, 360, 307, 611, 281, 360, 2950, 5038, 51196], "temperature": 0.0, "avg_logprob": -0.08069358553205218, "compression_ratio": 1.9331306990881458, "no_speech_prob": 0.004467344842851162}, {"id": 119, "seek": 52594, "start": 542.6600000000001, "end": 546.1, "text": " So you can imagine at test time we're trying to deploy it in the environment", "tokens": [51200, 407, 291, 393, 3811, 412, 1500, 565, 321, 434, 1382, 281, 7274, 309, 294, 264, 2823, 51372], "temperature": 0.0, "avg_logprob": -0.08069358553205218, "compression_ratio": 1.9331306990881458, "no_speech_prob": 0.004467344842851162}, {"id": 120, "seek": 52594, "start": 546.2600000000001, "end": 550.34, "text": " But we can actually do a bit more further planning through the world model to then work out what the best action is", "tokens": [51380, 583, 321, 393, 767, 360, 257, 857, 544, 3052, 5038, 807, 264, 1002, 2316, 281, 550, 589, 484, 437, 264, 1151, 3069, 307, 51584], "temperature": 0.0, "avg_logprob": -0.08069358553205218, "compression_ratio": 1.9331306990881458, "no_speech_prob": 0.004467344842851162}, {"id": 121, "seek": 52594, "start": 550.74, "end": 553.7800000000001, "text": " Rather than relying on just a neural network to immediately output an action", "tokens": [51604, 16571, 813, 24140, 322, 445, 257, 18161, 3209, 281, 4258, 5598, 364, 3069, 51756], "temperature": 0.0, "avg_logprob": -0.08069358553205218, "compression_ratio": 1.9331306990881458, "no_speech_prob": 0.004467344842851162}, {"id": 122, "seek": 55378, "start": 554.18, "end": 557.38, "text": " And there's kind of a lot of work showing that if you can do this like planning at test time", "tokens": [50384, 400, 456, 311, 733, 295, 257, 688, 295, 589, 4099, 300, 498, 291, 393, 360, 341, 411, 5038, 412, 1500, 565, 50544], "temperature": 0.0, "avg_logprob": -0.08985716218401672, "compression_ratio": 1.896067415730337, "no_speech_prob": 0.0060961805284023285}, {"id": 123, "seek": 55378, "start": 557.6999999999999, "end": 561.3, "text": " You can kind of get a lot of a better performance on a lot of environments, especially things that", "tokens": [50560, 509, 393, 733, 295, 483, 257, 688, 295, 257, 1101, 3389, 322, 257, 688, 295, 12388, 11, 2318, 721, 300, 50740], "temperature": 0.0, "avg_logprob": -0.08985716218401672, "compression_ratio": 1.896067415730337, "no_speech_prob": 0.0060961805284023285}, {"id": 124, "seek": 55378, "start": 561.6999999999999, "end": 565.4599999999999, "text": " That really rely on um search to do well things like go and like these kind of games", "tokens": [50760, 663, 534, 10687, 322, 1105, 3164, 281, 360, 731, 721, 411, 352, 293, 411, 613, 733, 295, 2813, 50948], "temperature": 0.0, "avg_logprob": -0.08985716218401672, "compression_ratio": 1.896067415730337, "no_speech_prob": 0.0060961805284023285}, {"id": 125, "seek": 55378, "start": 565.4599999999999, "end": 568.1, "text": " We do have to think explicitly ahead in the environment", "tokens": [50948, 492, 360, 362, 281, 519, 20803, 2286, 294, 264, 2823, 51080], "temperature": 0.0, "avg_logprob": -0.08985716218401672, "compression_ratio": 1.896067415730337, "no_speech_prob": 0.0060961805284023285}, {"id": 126, "seek": 55378, "start": 568.5, "end": 572.5799999999999, "text": " And so I would think those are the main reasons you would want to consider um a learning a world model", "tokens": [51100, 400, 370, 286, 576, 519, 729, 366, 264, 2135, 4112, 291, 576, 528, 281, 1949, 1105, 257, 2539, 257, 1002, 2316, 51304], "temperature": 0.0, "avg_logprob": -0.08985716218401672, "compression_ratio": 1.896067415730337, "no_speech_prob": 0.0060961805284023285}, {"id": 127, "seek": 55378, "start": 572.8199999999999, "end": 575.6999999999999, "text": " And maybe a last point I'll just add is that I think this is kind of a um", "tokens": [51316, 400, 1310, 257, 1036, 935, 286, 603, 445, 909, 307, 300, 286, 519, 341, 307, 733, 295, 257, 1105, 51460], "temperature": 0.0, "avg_logprob": -0.08985716218401672, "compression_ratio": 1.896067415730337, "no_speech_prob": 0.0060961805284023285}, {"id": 128, "seek": 55378, "start": 576.3399999999999, "end": 578.42, "text": " Again, like unclear whether this is true necessarily", "tokens": [51492, 3764, 11, 411, 25636, 1968, 341, 307, 2074, 4725, 51596], "temperature": 0.0, "avg_logprob": -0.08985716218401672, "compression_ratio": 1.896067415730337, "no_speech_prob": 0.0060961805284023285}, {"id": 129, "seek": 55378, "start": 578.42, "end": 583.22, "text": " But but I think some people would argue that a world model will generalize better than learning a value function", "tokens": [51596, 583, 457, 286, 519, 512, 561, 576, 9695, 300, 257, 1002, 2316, 486, 2674, 1125, 1101, 813, 2539, 257, 2158, 2445, 51836], "temperature": 0.0, "avg_logprob": -0.08985716218401672, "compression_ratio": 1.896067415730337, "no_speech_prob": 0.0060961805284023285}, {"id": 130, "seek": 58322, "start": 583.5400000000001, "end": 587.38, "text": " So you can imagine like a world model is learning things like you know state transitions", "tokens": [50380, 407, 291, 393, 3811, 411, 257, 1002, 2316, 307, 2539, 721, 411, 291, 458, 1785, 23767, 50572], "temperature": 0.0, "avg_logprob": -0.08266803453553398, "compression_ratio": 2.1124260355029585, "no_speech_prob": 0.002250720513984561}, {"id": 131, "seek": 58322, "start": 587.5400000000001, "end": 589.94, "text": " So you can imagine if you if you're training on straight transitions", "tokens": [50580, 407, 291, 393, 3811, 498, 291, 498, 291, 434, 3097, 322, 2997, 23767, 50700], "temperature": 0.0, "avg_logprob": -0.08266803453553398, "compression_ratio": 2.1124260355029585, "no_speech_prob": 0.002250720513984561}, {"id": 132, "seek": 58322, "start": 590.02, "end": 593.38, "text": " The model is kind of implicitly being forced to learn something like physics or something like that", "tokens": [50704, 440, 2316, 307, 733, 295, 26947, 356, 885, 7579, 281, 1466, 746, 411, 10649, 420, 746, 411, 300, 50872], "temperature": 0.0, "avg_logprob": -0.08266803453553398, "compression_ratio": 2.1124260355029585, "no_speech_prob": 0.002250720513984561}, {"id": 133, "seek": 58322, "start": 593.62, "end": 596.5, "text": " And so if you're like very explicitly forcing the model to learn something like physics", "tokens": [50884, 400, 370, 498, 291, 434, 411, 588, 20803, 19030, 264, 2316, 281, 1466, 746, 411, 10649, 51028], "temperature": 0.0, "avg_logprob": -0.08266803453553398, "compression_ratio": 2.1124260355029585, "no_speech_prob": 0.002250720513984561}, {"id": 134, "seek": 58322, "start": 596.82, "end": 597.62, "text": " You could argue, you know", "tokens": [51044, 509, 727, 9695, 11, 291, 458, 51084], "temperature": 0.0, "avg_logprob": -0.08266803453553398, "compression_ratio": 2.1124260355029585, "no_speech_prob": 0.002250720513984561}, {"id": 135, "seek": 58322, "start": 597.62, "end": 601.3000000000001, "text": " We'll go to some new state and the rules of physics will still hold and therefore the world model will still be", "tokens": [51084, 492, 603, 352, 281, 512, 777, 1785, 293, 264, 4474, 295, 10649, 486, 920, 1797, 293, 4412, 264, 1002, 2316, 486, 920, 312, 51268], "temperature": 0.0, "avg_logprob": -0.08266803453553398, "compression_ratio": 2.1124260355029585, "no_speech_prob": 0.002250720513984561}, {"id": 136, "seek": 58322, "start": 601.62, "end": 604.82, "text": " Quite good at the new state potentially whereas if you learn a value function", "tokens": [51284, 20464, 665, 412, 264, 777, 1785, 7263, 9735, 498, 291, 1466, 257, 2158, 2445, 51444], "temperature": 0.0, "avg_logprob": -0.08266803453553398, "compression_ratio": 2.1124260355029585, "no_speech_prob": 0.002250720513984561}, {"id": 137, "seek": 58322, "start": 605.22, "end": 608.58, "text": " I guess it's a little bit less clear as to whether you're putting a new situation", "tokens": [51464, 286, 2041, 309, 311, 257, 707, 857, 1570, 1850, 382, 281, 1968, 291, 434, 3372, 257, 777, 2590, 51632], "temperature": 0.0, "avg_logprob": -0.08266803453553398, "compression_ratio": 2.1124260355029585, "no_speech_prob": 0.002250720513984561}, {"id": 138, "seek": 58322, "start": 608.74, "end": 612.1, "text": " Will the same kind of structure of that value hold as it would a model", "tokens": [51640, 3099, 264, 912, 733, 295, 3877, 295, 300, 2158, 1797, 382, 309, 576, 257, 2316, 51808], "temperature": 0.0, "avg_logprob": -0.08266803453553398, "compression_ratio": 2.1124260355029585, "no_speech_prob": 0.002250720513984561}, {"id": 139, "seek": 61210, "start": 612.34, "end": 615.0600000000001, "text": " Anyway, so that was a bit of a long answer, but no, no, it's fascinating", "tokens": [50376, 5684, 11, 370, 300, 390, 257, 857, 295, 257, 938, 1867, 11, 457, 572, 11, 572, 11, 309, 311, 10343, 50512], "temperature": 0.0, "avg_logprob": -0.11733281702027284, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.001281473319977522}, {"id": 140, "seek": 61210, "start": 615.0600000000001, "end": 620.02, "text": " I mean when I was reading the paper that one of the reads I got is um in machine learning", "tokens": [50512, 286, 914, 562, 286, 390, 3760, 264, 3035, 300, 472, 295, 264, 15700, 286, 658, 307, 1105, 294, 3479, 2539, 50760], "temperature": 0.0, "avg_logprob": -0.11733281702027284, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.001281473319977522}, {"id": 141, "seek": 61210, "start": 620.02, "end": 625.46, "text": " We are often overcoming the curse of sparsity. So of course like in trajectories and reinforcement learning that that's quite intuitive", "tokens": [50760, 492, 366, 2049, 38047, 264, 17139, 295, 637, 685, 507, 13, 407, 295, 1164, 411, 294, 18257, 2083, 293, 29280, 2539, 300, 300, 311, 1596, 21769, 51032], "temperature": 0.0, "avg_logprob": -0.11733281702027284, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.001281473319977522}, {"id": 142, "seek": 61210, "start": 625.7, "end": 628.74, "text": " But even in learning the world model itself the model", "tokens": [51044, 583, 754, 294, 2539, 264, 1002, 2316, 2564, 264, 2316, 51196], "temperature": 0.0, "avg_logprob": -0.11733281702027284, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.001281473319977522}, {"id": 143, "seek": 61210, "start": 629.3000000000001, "end": 634.26, "text": " Just because of the way they're trained it tends to compress the world into small little motifs and", "tokens": [51224, 1449, 570, 295, 264, 636, 436, 434, 8895, 309, 12258, 281, 14778, 264, 1002, 666, 1359, 707, 2184, 18290, 293, 51472], "temperature": 0.0, "avg_logprob": -0.11733281702027284, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.001281473319977522}, {"id": 144, "seek": 61210, "start": 634.9, "end": 640.1, "text": " Actually, the world is quite complicated and we need to combine the motifs together in lots of interesting and rich ways", "tokens": [51504, 5135, 11, 264, 1002, 307, 1596, 6179, 293, 321, 643, 281, 10432, 264, 2184, 18290, 1214, 294, 3195, 295, 1880, 293, 4593, 2098, 51764], "temperature": 0.0, "avg_logprob": -0.11733281702027284, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.001281473319977522}, {"id": 145, "seek": 64010, "start": 640.5, "end": 645.86, "text": " And by exploring through the world model, we're almost kind of like make it we're forcing it to make those connections", "tokens": [50384, 400, 538, 12736, 807, 264, 1002, 2316, 11, 321, 434, 1920, 733, 295, 411, 652, 309, 321, 434, 19030, 309, 281, 652, 729, 9271, 50652], "temperature": 0.0, "avg_logprob": -0.1077930171315263, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.004824430216103792}, {"id": 146, "seek": 64010, "start": 646.02, "end": 648.74, "text": " Yeah, and I think um, you know to follow up on mark's um", "tokens": [50660, 865, 11, 293, 286, 519, 1105, 11, 291, 458, 281, 1524, 493, 322, 1491, 311, 1105, 50796], "temperature": 0.0, "avg_logprob": -0.1077930171315263, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.004824430216103792}, {"id": 147, "seek": 64010, "start": 649.5400000000001, "end": 650.26, "text": " Mark's point", "tokens": [50836, 3934, 311, 935, 50872], "temperature": 0.0, "avg_logprob": -0.1077930171315263, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.004824430216103792}, {"id": 148, "seek": 64010, "start": 650.26, "end": 654.58, "text": " I think it's also interesting because especially in the waker paper, uh, the world model setting", "tokens": [50872, 286, 519, 309, 311, 611, 1880, 570, 2318, 294, 264, 261, 4003, 3035, 11, 2232, 11, 264, 1002, 2316, 3287, 51088], "temperature": 0.0, "avg_logprob": -0.1077930171315263, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.004824430216103792}, {"id": 149, "seek": 64010, "start": 654.58, "end": 657.0600000000001, "text": " We're looking at specifically reward free world models", "tokens": [51088, 492, 434, 1237, 412, 4682, 7782, 1737, 1002, 5245, 51212], "temperature": 0.0, "avg_logprob": -0.1077930171315263, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.004824430216103792}, {"id": 150, "seek": 64010, "start": 657.3000000000001, "end": 662.66, "text": " And so essentially there's this explicit decision to separate separate out the two components of world model", "tokens": [51224, 400, 370, 4476, 456, 311, 341, 13691, 3537, 281, 4994, 4994, 484, 264, 732, 6677, 295, 1002, 2316, 51492], "temperature": 0.0, "avg_logprob": -0.1077930171315263, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.004824430216103792}, {"id": 151, "seek": 64010, "start": 662.9, "end": 667.38, "text": " Which is essentially the dynamics function, which tells you how things transition from state to state", "tokens": [51504, 3013, 307, 4476, 264, 15679, 2445, 11, 597, 5112, 291, 577, 721, 6034, 490, 1785, 281, 1785, 51728], "temperature": 0.0, "avg_logprob": -0.1077930171315263, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.004824430216103792}, {"id": 152, "seek": 66738, "start": 667.38, "end": 671.46, "text": " How does a state transition state of the world transition to the next state of the world?", "tokens": [50364, 1012, 775, 257, 1785, 6034, 1785, 295, 264, 1002, 6034, 281, 264, 958, 1785, 295, 264, 1002, 30, 50568], "temperature": 0.0, "avg_logprob": -0.1136344586099897, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.007343655917793512}, {"id": 153, "seek": 66738, "start": 671.7, "end": 676.5, "text": " Given an action that the model or the agent is taking in that world and the reward that it receives", "tokens": [50580, 18600, 364, 3069, 300, 264, 2316, 420, 264, 9461, 307, 1940, 294, 300, 1002, 293, 264, 7782, 300, 309, 20717, 50820], "temperature": 0.0, "avg_logprob": -0.1136344586099897, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.007343655917793512}, {"id": 154, "seek": 66738, "start": 676.58, "end": 679.46, "text": " So the slider part the reward is defined by the reward function", "tokens": [50824, 407, 264, 26046, 644, 264, 7782, 307, 7642, 538, 264, 7782, 2445, 50968], "temperature": 0.0, "avg_logprob": -0.1136344586099897, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.007343655917793512}, {"id": 155, "seek": 66738, "start": 679.78, "end": 685.9399999999999, "text": " And so, uh, you know, I think mark was uh to follow up on his point a lot of the benefits of the world model is", "tokens": [50984, 400, 370, 11, 2232, 11, 291, 458, 11, 286, 519, 1491, 390, 2232, 281, 1524, 493, 322, 702, 935, 257, 688, 295, 264, 5311, 295, 264, 1002, 2316, 307, 51292], "temperature": 0.0, "avg_logprob": -0.1136344586099897, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.007343655917793512}, {"id": 156, "seek": 66738, "start": 686.26, "end": 688.66, "text": " In this design arrangement is that you can", "tokens": [51308, 682, 341, 1715, 17620, 307, 300, 291, 393, 51428], "temperature": 0.0, "avg_logprob": -0.1136344586099897, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.007343655917793512}, {"id": 157, "seek": 66738, "start": 689.3, "end": 693.22, "text": " compositionally separate out this dynamics aspect from the reward aspect", "tokens": [51460, 12686, 379, 4994, 484, 341, 15679, 4171, 490, 264, 7782, 4171, 51656], "temperature": 0.0, "avg_logprob": -0.1136344586099897, "compression_ratio": 1.8862745098039215, "no_speech_prob": 0.007343655917793512}, {"id": 158, "seek": 69322, "start": 693.46, "end": 698.9, "text": " So the general idea would be why shouldn't agent train in such a world model be able to generalize to a new setting?", "tokens": [50376, 407, 264, 2674, 1558, 576, 312, 983, 4659, 380, 9461, 3847, 294, 1270, 257, 1002, 2316, 312, 1075, 281, 2674, 1125, 281, 257, 777, 3287, 30, 50648], "temperature": 0.0, "avg_logprob": -0.11734304428100586, "compression_ratio": 1.7544483985765125, "no_speech_prob": 0.004330980591475964}, {"id": 159, "seek": 69322, "start": 699.0600000000001, "end": 703.86, "text": " Well, maybe if that setting shares a lot of the underlying dynamics in that version of the world", "tokens": [50656, 1042, 11, 1310, 498, 300, 3287, 12182, 257, 688, 295, 264, 14217, 15679, 294, 300, 3037, 295, 264, 1002, 50896], "temperature": 0.0, "avg_logprob": -0.11734304428100586, "compression_ratio": 1.7544483985765125, "no_speech_prob": 0.004330980591475964}, {"id": 160, "seek": 69322, "start": 703.94, "end": 708.82, "text": " for example rules of physics and the agent has learned how to exploit those to accomplish, um", "tokens": [50900, 337, 1365, 4474, 295, 10649, 293, 264, 9461, 575, 3264, 577, 281, 25924, 729, 281, 9021, 11, 1105, 51144], "temperature": 0.0, "avg_logprob": -0.11734304428100586, "compression_ratio": 1.7544483985765125, "no_speech_prob": 0.004330980591475964}, {"id": 161, "seek": 69322, "start": 709.46, "end": 712.5, "text": " Navigation around that environment or reach different types of tasks", "tokens": [51176, 9219, 21494, 926, 300, 2823, 420, 2524, 819, 3467, 295, 9608, 51328], "temperature": 0.0, "avg_logprob": -0.11734304428100586, "compression_ratio": 1.7544483985765125, "no_speech_prob": 0.004330980591475964}, {"id": 162, "seek": 69322, "start": 712.9, "end": 714.9, "text": " Achieve different kinds of tasks in that environment", "tokens": [51348, 15847, 11108, 819, 3685, 295, 9608, 294, 300, 2823, 51448], "temperature": 0.0, "avg_logprob": -0.11734304428100586, "compression_ratio": 1.7544483985765125, "no_speech_prob": 0.004330980591475964}, {"id": 163, "seek": 69322, "start": 715.0600000000001, "end": 718.26, "text": " Then you can um sort of superimpose a different reward function", "tokens": [51456, 1396, 291, 393, 1105, 1333, 295, 1687, 8814, 541, 257, 819, 7782, 2445, 51616], "temperature": 0.0, "avg_logprob": -0.11734304428100586, "compression_ratio": 1.7544483985765125, "no_speech_prob": 0.004330980591475964}, {"id": 164, "seek": 71826, "start": 718.58, "end": 723.7, "text": " That essentially defines a different task because the reward function defines what task success is", "tokens": [50380, 663, 4476, 23122, 257, 819, 5633, 570, 264, 7782, 2445, 23122, 437, 5633, 2245, 307, 50636], "temperature": 0.0, "avg_logprob": -0.11350081013698204, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.009112413041293621}, {"id": 165, "seek": 71826, "start": 723.9399999999999, "end": 729.22, "text": " so you can essentially superimpose different tasks on top of that dynamics model and you would", "tokens": [50648, 370, 291, 393, 4476, 1687, 8814, 541, 819, 9608, 322, 1192, 295, 300, 15679, 2316, 293, 291, 576, 50912], "temperature": 0.0, "avg_logprob": -0.11350081013698204, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.009112413041293621}, {"id": 166, "seek": 71826, "start": 729.54, "end": 734.98, "text": " You know, you could expect that the agent could learn more quickly because it's already mastered sort of the foundational skills of", "tokens": [50928, 509, 458, 11, 291, 727, 2066, 300, 264, 9461, 727, 1466, 544, 2661, 570, 309, 311, 1217, 38686, 1333, 295, 264, 32195, 3942, 295, 51200], "temperature": 0.0, "avg_logprob": -0.11350081013698204, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.009112413041293621}, {"id": 167, "seek": 71826, "start": 735.22, "end": 739.46, "text": " navigating or manipulating different aspects of the dynamics of that world", "tokens": [51212, 32054, 420, 40805, 819, 7270, 295, 264, 15679, 295, 300, 1002, 51424], "temperature": 0.0, "avg_logprob": -0.11350081013698204, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.009112413041293621}, {"id": 168, "seek": 71826, "start": 739.78, "end": 744.02, "text": " We've been on a bit of a journey here. I think over the last few years in the literature of", "tokens": [51440, 492, 600, 668, 322, 257, 857, 295, 257, 4671, 510, 13, 286, 519, 670, 264, 1036, 1326, 924, 294, 264, 10394, 295, 51652], "temperature": 0.0, "avg_logprob": -0.11350081013698204, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.009112413041293621}, {"id": 169, "seek": 74402, "start": 744.1, "end": 745.22, "text": " um", "tokens": [50368, 1105, 50424], "temperature": 0.0, "avg_logprob": -0.14518211301693246, "compression_ratio": 1.7649006622516556, "no_speech_prob": 0.005523834377527237}, {"id": 170, "seek": 74402, "start": 745.22, "end": 752.8199999999999, "text": " We we want to have robust models and we're doing that by kind of perturbing and you know making a bunch of manipulations to the environment", "tokens": [50424, 492, 321, 528, 281, 362, 13956, 5245, 293, 321, 434, 884, 300, 538, 733, 295, 13269, 374, 4324, 293, 291, 458, 1455, 257, 3840, 295, 9258, 4136, 281, 264, 2823, 50804], "temperature": 0.0, "avg_logprob": -0.14518211301693246, "compression_ratio": 1.7649006622516556, "no_speech_prob": 0.005523834377527237}, {"id": 171, "seek": 74402, "start": 753.06, "end": 757.3, "text": " And there there was this domain randomization and there's like unsupervised environment design", "tokens": [50816, 400, 456, 456, 390, 341, 9274, 4974, 2144, 293, 456, 311, 411, 2693, 12879, 24420, 2823, 1715, 51028], "temperature": 0.0, "avg_logprob": -0.14518211301693246, "compression_ratio": 1.7649006622516556, "no_speech_prob": 0.005523834377527237}, {"id": 172, "seek": 74402, "start": 757.3, "end": 760.66, "text": " And of course your your iteration now is doing this in in the domain of", "tokens": [51028, 400, 295, 1164, 428, 428, 24784, 586, 307, 884, 341, 294, 294, 264, 9274, 295, 51196], "temperature": 0.0, "avg_logprob": -0.14518211301693246, "compression_ratio": 1.7649006622516556, "no_speech_prob": 0.005523834377527237}, {"id": 173, "seek": 74402, "start": 761.38, "end": 766.9, "text": " Reward-free exploration, but can you take us on on that journey sort of maybe starting with um domain randomization?", "tokens": [51232, 1300, 1007, 12, 10792, 16197, 11, 457, 393, 291, 747, 505, 322, 322, 300, 4671, 1333, 295, 1310, 2891, 365, 1105, 9274, 4974, 2144, 30, 51508], "temperature": 0.0, "avg_logprob": -0.14518211301693246, "compression_ratio": 1.7649006622516556, "no_speech_prob": 0.005523834377527237}, {"id": 174, "seek": 74402, "start": 767.14, "end": 772.34, "text": " kind of just to uh elaborate on something that mark was previously talking about which is that the typical", "tokens": [51520, 733, 295, 445, 281, 2232, 20945, 322, 746, 300, 1491, 390, 8046, 1417, 466, 597, 307, 300, 264, 7476, 51780], "temperature": 0.0, "avg_logprob": -0.14518211301693246, "compression_ratio": 1.7649006622516556, "no_speech_prob": 0.005523834377527237}, {"id": 175, "seek": 77234, "start": 772.6600000000001, "end": 775.62, "text": " you know standard setup in machine learning is to", "tokens": [50380, 291, 458, 3832, 8657, 294, 3479, 2539, 307, 281, 50528], "temperature": 0.0, "avg_logprob": -0.16451781558007308, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0018099797889590263}, {"id": 176, "seek": 77234, "start": 776.1800000000001, "end": 780.5, "text": " Uh, essentially optimize a model's performance uh over a uniform distribution", "tokens": [50556, 4019, 11, 4476, 19719, 257, 2316, 311, 3389, 2232, 670, 257, 9452, 7316, 50772], "temperature": 0.0, "avg_logprob": -0.16451781558007308, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0018099797889590263}, {"id": 177, "seek": 77234, "start": 780.98, "end": 784.34, "text": " Over the data points and so this is really just randomly sampling data points", "tokens": [50796, 4886, 264, 1412, 2793, 293, 370, 341, 307, 534, 445, 16979, 21179, 1412, 2793, 50964], "temperature": 0.0, "avg_logprob": -0.16451781558007308, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0018099797889590263}, {"id": 178, "seek": 77234, "start": 784.34, "end": 788.34, "text": " And we try to minimize the loss over those data points for whatever objective", "tokens": [50964, 400, 321, 853, 281, 17522, 264, 4470, 670, 729, 1412, 2793, 337, 2035, 10024, 51164], "temperature": 0.0, "avg_logprob": -0.16451781558007308, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0018099797889590263}, {"id": 179, "seek": 77234, "start": 788.4200000000001, "end": 792.1800000000001, "text": " We're trying to minimize or maximize in reinforcement learning. Um", "tokens": [51168, 492, 434, 1382, 281, 17522, 420, 19874, 294, 29280, 2539, 13, 3301, 51356], "temperature": 0.0, "avg_logprob": -0.16451781558007308, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0018099797889590263}, {"id": 180, "seek": 77234, "start": 792.74, "end": 795.62, "text": " We want to train agents that can perform well in lots of different", "tokens": [51384, 492, 528, 281, 3847, 12554, 300, 393, 2042, 731, 294, 3195, 295, 819, 51528], "temperature": 0.0, "avg_logprob": -0.16451781558007308, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0018099797889590263}, {"id": 181, "seek": 77234, "start": 796.1800000000001, "end": 798.1800000000001, "text": " versions of the environment and so", "tokens": [51556, 9606, 295, 264, 2823, 293, 370, 51656], "temperature": 0.0, "avg_logprob": -0.16451781558007308, "compression_ratio": 1.7121212121212122, "no_speech_prob": 0.0018099797889590263}, {"id": 182, "seek": 79818, "start": 798.5799999999999, "end": 800.5799999999999, "text": " You can think of each environment", "tokens": [50384, 509, 393, 519, 295, 1184, 2823, 50484], "temperature": 0.0, "avg_logprob": -0.1163283211844308, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0037065804935991764}, {"id": 183, "seek": 79818, "start": 800.5799999999999, "end": 804.5799999999999, "text": " Almost as a bundle of data points, right? It's kind of the set of trajectories that the agent can", "tokens": [50484, 12627, 382, 257, 24438, 295, 1412, 2793, 11, 558, 30, 467, 311, 733, 295, 264, 992, 295, 18257, 2083, 300, 264, 9461, 393, 50684], "temperature": 0.0, "avg_logprob": -0.1163283211844308, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0037065804935991764}, {"id": 184, "seek": 79818, "start": 805.8599999999999, "end": 811.4599999999999, "text": " Can encounter within that version of the world and we essentially in reinforcement learning we want to learn to maximize", "tokens": [50748, 1664, 8593, 1951, 300, 3037, 295, 264, 1002, 293, 321, 4476, 294, 29280, 2539, 321, 528, 281, 1466, 281, 19874, 51028], "temperature": 0.0, "avg_logprob": -0.1163283211844308, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0037065804935991764}, {"id": 185, "seek": 79818, "start": 812.0999999999999, "end": 816.26, "text": " The reward of the agent uh in that set of trajectories", "tokens": [51060, 440, 7782, 295, 264, 9461, 2232, 294, 300, 992, 295, 18257, 2083, 51268], "temperature": 0.0, "avg_logprob": -0.1163283211844308, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0037065804935991764}, {"id": 186, "seek": 79818, "start": 816.26, "end": 818.26, "text": " So we want to specifically start to", "tokens": [51268, 407, 321, 528, 281, 4682, 722, 281, 51368], "temperature": 0.0, "avg_logprob": -0.1163283211844308, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0037065804935991764}, {"id": 187, "seek": 79818, "start": 819.06, "end": 825.06, "text": " actively pursue those trajectories that give us the highest reward and we learn from the reward signal as the feedback signal for", "tokens": [51408, 13022, 12392, 729, 18257, 2083, 300, 976, 505, 264, 6343, 7782, 293, 321, 1466, 490, 264, 7782, 6358, 382, 264, 5824, 6358, 337, 51708], "temperature": 0.0, "avg_logprob": -0.1163283211844308, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0037065804935991764}, {"id": 188, "seek": 82506, "start": 825.38, "end": 829.9399999999999, "text": " Figuring out, you know, which actions and therefore which trajectories will lead to maximizing that reward", "tokens": [50380, 22443, 1345, 484, 11, 291, 458, 11, 597, 5909, 293, 4412, 597, 18257, 2083, 486, 1477, 281, 5138, 3319, 300, 7782, 50608], "temperature": 0.0, "avg_logprob": -0.1638661952728921, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.006096127908676863}, {"id": 189, "seek": 82506, "start": 830.3399999999999, "end": 837.6199999999999, "text": " and so typically um when we operate in the multitask setting, uh, we essentially randomly sample different versions of the environment", "tokens": [50628, 293, 370, 5850, 1105, 562, 321, 9651, 294, 264, 42338, 3863, 3287, 11, 2232, 11, 321, 4476, 16979, 6889, 819, 9606, 295, 264, 2823, 50992], "temperature": 0.0, "avg_logprob": -0.1638661952728921, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.006096127908676863}, {"id": 190, "seek": 82506, "start": 837.8599999999999, "end": 844.0999999999999, "text": " And essentially have the agent try to maximize its performance its reward on that random sample of environments", "tokens": [51004, 400, 4476, 362, 264, 9461, 853, 281, 19874, 1080, 3389, 1080, 7782, 322, 300, 4974, 6889, 295, 12388, 51316], "temperature": 0.0, "avg_logprob": -0.1638661952728921, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.006096127908676863}, {"id": 191, "seek": 82506, "start": 844.7399999999999, "end": 845.78, "text": " uniformly", "tokens": [51348, 48806, 51400], "temperature": 0.0, "avg_logprob": -0.1638661952728921, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.006096127908676863}, {"id": 192, "seek": 82506, "start": 845.78, "end": 848.18, "text": " Sampled from, you know, the set of possible environments", "tokens": [51400, 4832, 15551, 490, 11, 291, 458, 11, 264, 992, 295, 1944, 12388, 51520], "temperature": 0.0, "avg_logprob": -0.1638661952728921, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.006096127908676863}, {"id": 193, "seek": 82506, "start": 848.9799999999999, "end": 850.9799999999999, "text": " And this is essentially", "tokens": [51560, 400, 341, 307, 4476, 51660], "temperature": 0.0, "avg_logprob": -0.1638661952728921, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.006096127908676863}, {"id": 194, "seek": 85098, "start": 850.98, "end": 858.5, "text": " Causing the agent it'll cause the agent to learn a policy that's optimal for essentially uniform distribution over those environments", "tokens": [50364, 7544, 7981, 264, 9461, 309, 603, 3082, 264, 9461, 281, 1466, 257, 3897, 300, 311, 16252, 337, 4476, 9452, 7316, 670, 729, 12388, 50740], "temperature": 0.0, "avg_logprob": -0.11150559440988009, "compression_ratio": 1.8169934640522876, "no_speech_prob": 0.005729398690164089}, {"id": 195, "seek": 85098, "start": 858.9, "end": 865.78, "text": " Um, but of course this is kind of a naive assumption because we essentially are assuming that every possible version of the environment is equally likely", "tokens": [50760, 3301, 11, 457, 295, 1164, 341, 307, 733, 295, 257, 29052, 15302, 570, 321, 4476, 366, 11926, 300, 633, 1944, 3037, 295, 264, 2823, 307, 12309, 3700, 51104], "temperature": 0.0, "avg_logprob": -0.11150559440988009, "compression_ratio": 1.8169934640522876, "no_speech_prob": 0.005729398690164089}, {"id": 196, "seek": 85098, "start": 866.02, "end": 870.9, "text": " Which is obviously not true because some versions of the world will not be as likely as other as others", "tokens": [51116, 3013, 307, 2745, 406, 2074, 570, 512, 9606, 295, 264, 1002, 486, 406, 312, 382, 3700, 382, 661, 382, 2357, 51360], "temperature": 0.0, "avg_logprob": -0.11150559440988009, "compression_ratio": 1.8169934640522876, "no_speech_prob": 0.005729398690164089}, {"id": 197, "seek": 85098, "start": 871.14, "end": 874.74, "text": " Uh, for example, like if you walk outside the sky is usually blue and not green", "tokens": [51372, 4019, 11, 337, 1365, 11, 411, 498, 291, 1792, 2380, 264, 5443, 307, 2673, 3344, 293, 406, 3092, 51552], "temperature": 0.0, "avg_logprob": -0.11150559440988009, "compression_ratio": 1.8169934640522876, "no_speech_prob": 0.005729398690164089}, {"id": 198, "seek": 85098, "start": 874.98, "end": 879.22, "text": " And so, you know, when the sky is orange, maybe that happens if you're in california", "tokens": [51564, 400, 370, 11, 291, 458, 11, 562, 264, 5443, 307, 7671, 11, 1310, 300, 2314, 498, 291, 434, 294, 2104, 5203, 654, 51776], "temperature": 0.0, "avg_logprob": -0.11150559440988009, "compression_ratio": 1.8169934640522876, "no_speech_prob": 0.005729398690164089}, {"id": 199, "seek": 87922, "start": 879.3000000000001, "end": 881.46, "text": " There's a wildfire, but that's not usually the case", "tokens": [50368, 821, 311, 257, 4868, 12037, 11, 457, 300, 311, 406, 2673, 264, 1389, 50476], "temperature": 0.0, "avg_logprob": -0.11497761717939799, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.003944281954318285}, {"id": 200, "seek": 87922, "start": 881.78, "end": 886.58, "text": " And so instead what we can do is we can turn to decision theory and think of", "tokens": [50492, 400, 370, 2602, 437, 321, 393, 360, 307, 321, 393, 1261, 281, 3537, 5261, 293, 519, 295, 50732], "temperature": 0.0, "avg_logprob": -0.11497761717939799, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.003944281954318285}, {"id": 201, "seek": 87922, "start": 887.22, "end": 890.82, "text": " Sort of more sensible approaches to what it means to act optimally", "tokens": [50764, 26149, 295, 544, 25380, 11587, 281, 437, 309, 1355, 281, 605, 5028, 379, 50944], "temperature": 0.0, "avg_logprob": -0.11497761717939799, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.003944281954318285}, {"id": 202, "seek": 87922, "start": 891.3000000000001, "end": 898.4200000000001, "text": " When you're uncertain about uh, what state of the world the world will be in and so the thing that we focus on in this paper", "tokens": [50968, 1133, 291, 434, 11308, 466, 2232, 11, 437, 1785, 295, 264, 1002, 264, 1002, 486, 312, 294, 293, 370, 264, 551, 300, 321, 1879, 322, 294, 341, 3035, 51324], "temperature": 0.0, "avg_logprob": -0.11497761717939799, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.003944281954318285}, {"id": 203, "seek": 87922, "start": 899.38, "end": 903.14, "text": " Is this idea of minimax regret where it is this idea again of", "tokens": [51372, 1119, 341, 1558, 295, 4464, 2797, 10879, 689, 309, 307, 341, 1558, 797, 295, 51560], "temperature": 0.0, "avg_logprob": -0.11497761717939799, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.003944281954318285}, {"id": 204, "seek": 87922, "start": 903.78, "end": 908.34, "text": " Having the agent act in a way that essentially minimizes its worst case regret", "tokens": [51592, 10222, 264, 9461, 605, 294, 257, 636, 300, 4476, 4464, 5660, 1080, 5855, 1389, 10879, 51820], "temperature": 0.0, "avg_logprob": -0.11497761717939799, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.003944281954318285}, {"id": 205, "seek": 90922, "start": 909.86, "end": 911.86, "text": " In any possible, uh, state of the world", "tokens": [50396, 682, 604, 1944, 11, 2232, 11, 1785, 295, 264, 1002, 50496], "temperature": 0.0, "avg_logprob": -0.1691881699995561, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.004330690950155258}, {"id": 206, "seek": 90922, "start": 912.1, "end": 918.02, "text": " So largely, you know, this is a shift from randomly sample what it means in practice is you want to shift from randomly sampling", "tokens": [50508, 407, 11611, 11, 291, 458, 11, 341, 307, 257, 5513, 490, 16979, 6889, 437, 309, 1355, 294, 3124, 307, 291, 528, 281, 5513, 490, 16979, 21179, 50804], "temperature": 0.0, "avg_logprob": -0.1691881699995561, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.004330690950155258}, {"id": 207, "seek": 90922, "start": 918.4200000000001, "end": 924.6600000000001, "text": " environments during training to essentially, uh, sampling environments that maximize the agent's regret", "tokens": [50824, 12388, 1830, 3097, 281, 4476, 11, 2232, 11, 21179, 12388, 300, 19874, 264, 9461, 311, 10879, 51136], "temperature": 0.0, "avg_logprob": -0.1691881699995561, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.004330690950155258}, {"id": 208, "seek": 90922, "start": 925.22, "end": 931.0600000000001, "text": " And what this means is you're now actively sampling for those environment settings where the agents, um,", "tokens": [51164, 400, 437, 341, 1355, 307, 291, 434, 586, 13022, 21179, 337, 729, 2823, 6257, 689, 264, 12554, 11, 1105, 11, 51456], "temperature": 0.0, "avg_logprob": -0.1691881699995561, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.004330690950155258}, {"id": 209, "seek": 90922, "start": 932.26, "end": 938.98, "text": " Experiencing the most regret and here regret is defined just simply as what does the optimal agent do in that version of the environment?", "tokens": [51516, 12522, 1053, 2175, 264, 881, 10879, 293, 510, 10879, 307, 7642, 445, 2935, 382, 437, 775, 264, 16252, 9461, 360, 294, 300, 3037, 295, 264, 2823, 30, 51852], "temperature": 0.0, "avg_logprob": -0.1691881699995561, "compression_ratio": 1.8795620437956204, "no_speech_prob": 0.004330690950155258}, {"id": 210, "seek": 93922, "start": 939.3000000000001, "end": 942.26, "text": " And what did this current agent that's learning do in that environment?", "tokens": [50368, 400, 437, 630, 341, 2190, 9461, 300, 311, 2539, 360, 294, 300, 2823, 30, 50516], "temperature": 0.0, "avg_logprob": -0.10013652699334281, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.0008039124659262598}, {"id": 211, "seek": 93922, "start": 942.26, "end": 947.38, "text": " And so there's this gap in performance and you want to actively find those environments where that gap is maximal", "tokens": [50516, 400, 370, 456, 311, 341, 7417, 294, 3389, 293, 291, 528, 281, 13022, 915, 729, 12388, 689, 300, 7417, 307, 49336, 50772], "temperature": 0.0, "avg_logprob": -0.10013652699334281, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.0008039124659262598}, {"id": 212, "seek": 93922, "start": 947.78, "end": 953.94, "text": " And if you view this as this adversarial game now between, you know, uh, an adversary like nature", "tokens": [50792, 400, 498, 291, 1910, 341, 382, 341, 17641, 44745, 1216, 586, 1296, 11, 291, 458, 11, 2232, 11, 364, 48222, 411, 3687, 51100], "temperature": 0.0, "avg_logprob": -0.10013652699334281, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.0008039124659262598}, {"id": 213, "seek": 93922, "start": 953.94, "end": 960.98, "text": " That's choosing the environment and the agent that's learning to solve the environment. Um, you can think of the adversary as, you know, having a", "tokens": [51100, 663, 311, 10875, 264, 2823, 293, 264, 9461, 300, 311, 2539, 281, 5039, 264, 2823, 13, 3301, 11, 291, 393, 519, 295, 264, 48222, 382, 11, 291, 458, 11, 1419, 257, 51452], "temperature": 0.0, "avg_logprob": -0.10013652699334281, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.0008039124659262598}, {"id": 214, "seek": 93922, "start": 961.78, "end": 965.22, "text": " Payoff function in that game or it's rewarded for the", "tokens": [51492, 11431, 4506, 2445, 294, 300, 1216, 420, 309, 311, 29105, 337, 264, 51664], "temperature": 0.0, "avg_logprob": -0.10013652699334281, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.0008039124659262598}, {"id": 215, "seek": 96522, "start": 965.7, "end": 970.26, "text": " Based on the regret that the agent experiences and the agent is trying to shrink that regret", "tokens": [50388, 18785, 322, 264, 10879, 300, 264, 9461, 5235, 293, 264, 9461, 307, 1382, 281, 23060, 300, 10879, 50616], "temperature": 0.0, "avg_logprob": -0.13542206754389496, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.0007792856777086854}, {"id": 216, "seek": 96522, "start": 970.26, "end": 976.1, "text": " So the agent you can think of as being rewarded for, you know, um, the the negative of that reward", "tokens": [50616, 407, 264, 9461, 291, 393, 519, 295, 382, 885, 29105, 337, 11, 291, 458, 11, 1105, 11, 264, 264, 3671, 295, 300, 7782, 50908], "temperature": 0.0, "avg_logprob": -0.13542206754389496, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.0007792856777086854}, {"id": 217, "seek": 96522, "start": 976.1, "end": 979.78, "text": " So the agent's reward signal is you can think of as the negative of the regret", "tokens": [50908, 407, 264, 9461, 311, 7782, 6358, 307, 291, 393, 519, 295, 382, 264, 3671, 295, 264, 10879, 51092], "temperature": 0.0, "avg_logprob": -0.13542206754389496, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.0007792856777086854}, {"id": 218, "seek": 96522, "start": 980.1800000000001, "end": 984.1, "text": " And so now you have the setting where you can essentially view this training process", "tokens": [51112, 400, 370, 586, 291, 362, 264, 3287, 689, 291, 393, 4476, 1910, 341, 3097, 1399, 51308], "temperature": 0.0, "avg_logprob": -0.13542206754389496, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.0007792856777086854}, {"id": 219, "seek": 96522, "start": 984.5, "end": 989.0600000000001, "text": " this active sampling process as a two player zero sum game where the", "tokens": [51328, 341, 4967, 21179, 1399, 382, 257, 732, 4256, 4018, 2408, 1216, 689, 264, 51556], "temperature": 0.0, "avg_logprob": -0.13542206754389496, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.0007792856777086854}, {"id": 220, "seek": 98906, "start": 989.4599999999999, "end": 996.3399999999999, "text": " Adversary is, you know, rewarded for the regret of the agent in each environment it chooses and the agent is rewarded based on the", "tokens": [50384, 1999, 840, 822, 307, 11, 291, 458, 11, 29105, 337, 264, 10879, 295, 264, 9461, 294, 1184, 2823, 309, 25963, 293, 264, 9461, 307, 29105, 2361, 322, 264, 50728], "temperature": 0.0, "avg_logprob": -0.14434402867367394, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.004467882681638002}, {"id": 221, "seek": 98906, "start": 997.06, "end": 1003.38, "text": " The agent receives the negative regret as its payoff. And so, um, we know that into player zero sum games", "tokens": [50764, 440, 9461, 20717, 264, 3671, 10879, 382, 1080, 46547, 13, 400, 370, 11, 1105, 11, 321, 458, 300, 666, 4256, 4018, 2408, 2813, 51080], "temperature": 0.0, "avg_logprob": -0.14434402867367394, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.004467882681638002}, {"id": 222, "seek": 98906, "start": 1003.38, "end": 1007.4599999999999, "text": " There's always a this there's always a solution called a Nash equilibrium", "tokens": [51080, 821, 311, 1009, 257, 341, 456, 311, 1009, 257, 3827, 1219, 257, 25012, 15625, 51284], "temperature": 0.0, "avg_logprob": -0.14434402867367394, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.004467882681638002}, {"id": 223, "seek": 98906, "start": 1007.54, "end": 1010.7399999999999, "text": " and so this is an idea in game theory where basically this is", "tokens": [51288, 293, 370, 341, 307, 364, 1558, 294, 1216, 5261, 689, 1936, 341, 307, 51448], "temperature": 0.0, "avg_logprob": -0.14434402867367394, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.004467882681638002}, {"id": 224, "seek": 98906, "start": 1011.14, "end": 1017.6199999999999, "text": " um, a choice of behaviors on both parties or a choice of strategies on both parties in the game such that, um", "tokens": [51468, 1105, 11, 257, 3922, 295, 15501, 322, 1293, 8265, 420, 257, 3922, 295, 9029, 322, 1293, 8265, 294, 264, 1216, 1270, 300, 11, 1105, 51792], "temperature": 0.0, "avg_logprob": -0.14434402867367394, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.004467882681638002}, {"id": 225, "seek": 101762, "start": 1018.5, "end": 1021.62, "text": " No player can do better unless the other player changes their strategy", "tokens": [50408, 883, 4256, 393, 360, 1101, 5969, 264, 661, 4256, 2962, 641, 5206, 50564], "temperature": 0.0, "avg_logprob": -0.12908383516164926, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.0010003454517573118}, {"id": 226, "seek": 101762, "start": 1021.78, "end": 1025.06, "text": " And so you can think of this as a situation where, you know, I'm not", "tokens": [50572, 400, 370, 291, 393, 519, 295, 341, 382, 257, 2590, 689, 11, 291, 458, 11, 286, 478, 406, 50736], "temperature": 0.0, "avg_logprob": -0.12908383516164926, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.0010003454517573118}, {"id": 227, "seek": 101762, "start": 1025.62, "end": 1031.46, "text": " Neither player is incentivized to deviate from their behavior. Uh, once they reach this choice of mutual strategies", "tokens": [50764, 23956, 4256, 307, 35328, 1602, 281, 1905, 13024, 490, 641, 5223, 13, 4019, 11, 1564, 436, 2524, 341, 3922, 295, 16917, 9029, 51056], "temperature": 0.0, "avg_logprob": -0.12908383516164926, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.0010003454517573118}, {"id": 228, "seek": 101762, "start": 1031.94, "end": 1036.98, "text": " And so we know that all two player zero sum games have a Nash equilibrium", "tokens": [51080, 400, 370, 321, 458, 300, 439, 732, 4256, 4018, 2408, 2813, 362, 257, 25012, 15625, 51332], "temperature": 0.0, "avg_logprob": -0.12908383516164926, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.0010003454517573118}, {"id": 229, "seek": 101762, "start": 1037.38, "end": 1040.82, "text": " A set of strategies between the two players and in this case", "tokens": [51352, 316, 992, 295, 9029, 1296, 264, 732, 4150, 293, 294, 341, 1389, 51524], "temperature": 0.0, "avg_logprob": -0.12908383516164926, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.0010003454517573118}, {"id": 230, "seek": 101762, "start": 1041.3, "end": 1043.86, "text": " We know there's additional theorem called the mini max theorem", "tokens": [51548, 492, 458, 456, 311, 4497, 20904, 1219, 264, 8382, 11469, 20904, 51676], "temperature": 0.0, "avg_logprob": -0.12908383516164926, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.0010003454517573118}, {"id": 231, "seek": 104386, "start": 1044.1, "end": 1049.62, "text": " Which says that when in a two player zero sum game specifically two players and zero sum when, um,", "tokens": [50376, 3013, 1619, 300, 562, 294, 257, 732, 4256, 4018, 2408, 1216, 4682, 732, 4150, 293, 4018, 2408, 562, 11, 1105, 11, 50652], "temperature": 0.0, "avg_logprob": -0.126877766211056, "compression_ratio": 1.8852459016393444, "no_speech_prob": 0.0064871348440647125}, {"id": 232, "seek": 104386, "start": 1050.02, "end": 1055.86, "text": " You are at the Nash equilibrium setting then each player must be playing what's called the mini max", "tokens": [50672, 509, 366, 412, 264, 25012, 15625, 3287, 550, 1184, 4256, 1633, 312, 2433, 437, 311, 1219, 264, 8382, 11469, 50964], "temperature": 0.0, "avg_logprob": -0.126877766211056, "compression_ratio": 1.8852459016393444, "no_speech_prob": 0.0064871348440647125}, {"id": 233, "seek": 104386, "start": 1056.74, "end": 1061.06, "text": " The mini max strategy, which means that each player is minimizing the maximum", "tokens": [51008, 440, 8382, 11469, 5206, 11, 597, 1355, 300, 1184, 4256, 307, 46608, 264, 6674, 51224], "temperature": 0.0, "avg_logprob": -0.126877766211056, "compression_ratio": 1.8852459016393444, "no_speech_prob": 0.0064871348440647125}, {"id": 234, "seek": 104386, "start": 1062.4199999999998, "end": 1065.54, "text": " Minimizing the maximum reward for the other player", "tokens": [51292, 2829, 332, 3319, 264, 6674, 7782, 337, 264, 661, 4256, 51448], "temperature": 0.0, "avg_logprob": -0.126877766211056, "compression_ratio": 1.8852459016393444, "no_speech_prob": 0.0064871348440647125}, {"id": 235, "seek": 104386, "start": 1065.86, "end": 1072.4199999999998, "text": " And so here the reward again is the regret and therefore just based on this known, you know, theorem about two player zero sum games", "tokens": [51464, 400, 370, 510, 264, 7782, 797, 307, 264, 10879, 293, 4412, 445, 2361, 322, 341, 2570, 11, 291, 458, 11, 20904, 466, 732, 4256, 4018, 2408, 2813, 51792], "temperature": 0.0, "avg_logprob": -0.126877766211056, "compression_ratio": 1.8852459016393444, "no_speech_prob": 0.0064871348440647125}, {"id": 236, "seek": 107242, "start": 1072.66, "end": 1077.14, "text": " We know that, um, the agent which is, you know, receiving the payoff of negative regret", "tokens": [50376, 492, 458, 300, 11, 1105, 11, 264, 9461, 597, 307, 11, 291, 458, 11, 10040, 264, 46547, 295, 3671, 10879, 50600], "temperature": 0.0, "avg_logprob": -0.13479753841053355, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0035874415189027786}, {"id": 237, "seek": 107242, "start": 1077.22, "end": 1081.7, "text": " It's the min player. It must be implementing the min and max regret strategy", "tokens": [50604, 467, 311, 264, 923, 4256, 13, 467, 1633, 312, 18114, 264, 923, 293, 11469, 10879, 5206, 50828], "temperature": 0.0, "avg_logprob": -0.13479753841053355, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0035874415189027786}, {"id": 238, "seek": 107242, "start": 1082.02, "end": 1091.54, "text": " And so this is how we essentially can shape the training process to essentially, um, arrive at an agent that performs mini max regret decision making", "tokens": [50844, 400, 370, 341, 307, 577, 321, 4476, 393, 3909, 264, 3097, 1399, 281, 4476, 11, 1105, 11, 8881, 412, 364, 9461, 300, 26213, 8382, 11469, 10879, 3537, 1455, 51320], "temperature": 0.0, "avg_logprob": -0.13479753841053355, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0035874415189027786}, {"id": 239, "seek": 107242, "start": 1091.8600000000001, "end": 1096.5, "text": " Rather than decision making that optimizes, um, just a uniform sample of environments", "tokens": [51336, 16571, 813, 3537, 1455, 300, 5028, 5660, 11, 1105, 11, 445, 257, 9452, 6889, 295, 12388, 51568], "temperature": 0.0, "avg_logprob": -0.13479753841053355, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0035874415189027786}, {"id": 240, "seek": 107242, "start": 1096.8200000000002, "end": 1100.42, "text": " Okay, so kind of play back, um, some of those things as I understand it", "tokens": [51584, 1033, 11, 370, 733, 295, 862, 646, 11, 1105, 11, 512, 295, 729, 721, 382, 286, 1223, 309, 51764], "temperature": 0.0, "avg_logprob": -0.13479753841053355, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.0035874415189027786}, {"id": 241, "seek": 110042, "start": 1100.74, "end": 1107.46, "text": " So, um, essentially we we are we're building a model which will learn to select the environments where we perform badly on", "tokens": [50380, 407, 11, 1105, 11, 4476, 321, 321, 366, 321, 434, 2390, 257, 2316, 597, 486, 1466, 281, 3048, 264, 12388, 689, 321, 2042, 13425, 322, 50716], "temperature": 0.0, "avg_logprob": -0.09075542189117171, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0023858360946178436}, {"id": 242, "seek": 110042, "start": 1107.78, "end": 1112.98, "text": " And then we fine-tune on those environments because we're leaning into the gaps. We're saying where where do I perform badly?", "tokens": [50732, 400, 550, 321, 2489, 12, 83, 2613, 322, 729, 12388, 570, 321, 434, 23390, 666, 264, 15031, 13, 492, 434, 1566, 689, 689, 360, 286, 2042, 13425, 30, 50992], "temperature": 0.0, "avg_logprob": -0.09075542189117171, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0023858360946178436}, {"id": 243, "seek": 110042, "start": 1113.14, "end": 1119.38, "text": " Let's fine-tune on that and then you're saying that if we continue to do this as a kind of adversarial sampling game", "tokens": [51000, 961, 311, 2489, 12, 83, 2613, 322, 300, 293, 550, 291, 434, 1566, 300, 498, 321, 2354, 281, 360, 341, 382, 257, 733, 295, 17641, 44745, 21179, 1216, 51312], "temperature": 0.0, "avg_logprob": -0.09075542189117171, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0023858360946178436}, {"id": 244, "seek": 110042, "start": 1119.6200000000001, "end": 1123.22, "text": " That we will reach a Nash equilibrium. So it will converge in a good place", "tokens": [51324, 663, 321, 486, 2524, 257, 25012, 15625, 13, 407, 309, 486, 41881, 294, 257, 665, 1081, 51504], "temperature": 0.0, "avg_logprob": -0.09075542189117171, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0023858360946178436}, {"id": 245, "seek": 110042, "start": 1123.54, "end": 1126.02, "text": " But help me understand that why would it", "tokens": [51520, 583, 854, 385, 1223, 300, 983, 576, 309, 51644], "temperature": 0.0, "avg_logprob": -0.09075542189117171, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0023858360946178436}, {"id": 246, "seek": 112602, "start": 1126.74, "end": 1132.02, "text": " You know, it seems to me intuitively that it might be unstable or it might not quite why does it converge?", "tokens": [50400, 509, 458, 11, 309, 2544, 281, 385, 46506, 300, 309, 1062, 312, 23742, 420, 309, 1062, 406, 1596, 983, 775, 309, 41881, 30, 50664], "temperature": 0.0, "avg_logprob": -0.18239079429989768, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.002510382328182459}, {"id": 247, "seek": 112602, "start": 1132.5, "end": 1134.5, "text": " So there's no guarantees around convergence", "tokens": [50688, 407, 456, 311, 572, 32567, 926, 32181, 50788], "temperature": 0.0, "avg_logprob": -0.18239079429989768, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.002510382328182459}, {"id": 248, "seek": 112602, "start": 1135.1399999999999, "end": 1138.82, "text": " And so I think this is an area where there's a lot of room for innovation", "tokens": [50820, 400, 370, 286, 519, 341, 307, 364, 1859, 689, 456, 311, 257, 688, 295, 1808, 337, 8504, 51004], "temperature": 0.0, "avg_logprob": -0.18239079429989768, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.002510382328182459}, {"id": 249, "seek": 112602, "start": 1139.22, "end": 1145.06, "text": " Uh around these methods a lot of this is um, this is more I would say like theoretical motivation around why we think", "tokens": [51024, 4019, 926, 613, 7150, 257, 688, 295, 341, 307, 1105, 11, 341, 307, 544, 286, 576, 584, 411, 20864, 12335, 926, 983, 321, 519, 51316], "temperature": 0.0, "avg_logprob": -0.18239079429989768, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.002510382328182459}, {"id": 250, "seek": 112602, "start": 1145.46, "end": 1152.26, "text": " actively sampling environment settings based on, um, estimates of regret is a good idea and another point related to that", "tokens": [51336, 13022, 21179, 2823, 6257, 2361, 322, 11, 1105, 11, 20561, 295, 10879, 307, 257, 665, 1558, 293, 1071, 935, 4077, 281, 300, 51676], "temperature": 0.0, "avg_logprob": -0.18239079429989768, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.002510382328182459}, {"id": 251, "seek": 115226, "start": 1152.5, "end": 1158.74, "text": " Around sort of this gap between the theory. I I just um explained and in practice is that", "tokens": [50376, 17633, 1333, 295, 341, 7417, 1296, 264, 5261, 13, 286, 286, 445, 1105, 8825, 293, 294, 3124, 307, 300, 50688], "temperature": 0.0, "avg_logprob": -0.13218136017139143, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.005641169846057892}, {"id": 252, "seek": 115226, "start": 1159.46, "end": 1161.94, "text": " Regret itself is a pretty hard quantity to actually", "tokens": [50724, 4791, 1505, 2564, 307, 257, 1238, 1152, 11275, 281, 767, 50848], "temperature": 0.0, "avg_logprob": -0.13218136017139143, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.005641169846057892}, {"id": 253, "seek": 115226, "start": 1162.5, "end": 1166.9, "text": " Measure in practice because you know knowing regrets defined as what's optimal performance", "tokens": [50876, 41436, 294, 3124, 570, 291, 458, 5276, 31214, 7642, 382, 437, 311, 16252, 3389, 51096], "temperature": 0.0, "avg_logprob": -0.13218136017139143, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.005641169846057892}, {"id": 254, "seek": 115226, "start": 1167.78, "end": 1169.7, "text": " minus my agents performance", "tokens": [51140, 3175, 452, 12554, 3389, 51236], "temperature": 0.0, "avg_logprob": -0.13218136017139143, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.005641169846057892}, {"id": 255, "seek": 115226, "start": 1169.7, "end": 1174.5, "text": " So you kind of have to know what optimal performance is and in general you don't know the optimal behavior", "tokens": [51236, 407, 291, 733, 295, 362, 281, 458, 437, 16252, 3389, 307, 293, 294, 2674, 291, 500, 380, 458, 264, 16252, 5223, 51476], "temperature": 0.0, "avg_logprob": -0.13218136017139143, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.005641169846057892}, {"id": 256, "seek": 115226, "start": 1174.5, "end": 1179.94, "text": " Therefore you don't really know the optimal performance on any environment unless it's like a very toy setting and so", "tokens": [51476, 7504, 291, 500, 380, 534, 458, 264, 16252, 3389, 322, 604, 2823, 5969, 309, 311, 411, 257, 588, 12058, 3287, 293, 370, 51748], "temperature": 0.0, "avg_logprob": -0.13218136017139143, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.005641169846057892}, {"id": 257, "seek": 117994, "start": 1180.8200000000002, "end": 1183.3, "text": " In practice, we also use approximations for the regret", "tokens": [50408, 682, 3124, 11, 321, 611, 764, 8542, 763, 337, 264, 10879, 50532], "temperature": 0.0, "avg_logprob": -0.11452495200293404, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.3551729100290686e-05}, {"id": 258, "seek": 117994, "start": 1183.94, "end": 1186.9, "text": " in order to do this kind of active sampling and so", "tokens": [50564, 294, 1668, 281, 360, 341, 733, 295, 4967, 21179, 293, 370, 50712], "temperature": 0.0, "avg_logprob": -0.11452495200293404, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.3551729100290686e-05}, {"id": 259, "seek": 117994, "start": 1187.94, "end": 1190.3400000000001, "text": " There's a lot of deviations between theory and practice", "tokens": [50764, 821, 311, 257, 688, 295, 31219, 763, 1296, 5261, 293, 3124, 50884], "temperature": 0.0, "avg_logprob": -0.11452495200293404, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.3551729100290686e-05}, {"id": 260, "seek": 117994, "start": 1191.38, "end": 1192.3400000000001, "text": " So", "tokens": [50936, 407, 50984], "temperature": 0.0, "avg_logprob": -0.11452495200293404, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.3551729100290686e-05}, {"id": 261, "seek": 117994, "start": 1192.3400000000001, "end": 1196.1000000000001, "text": " There's no guarantees, you know that different forms of gradient based optimization", "tokens": [50984, 821, 311, 572, 32567, 11, 291, 458, 300, 819, 6422, 295, 16235, 2361, 19618, 51172], "temperature": 0.0, "avg_logprob": -0.11452495200293404, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.3551729100290686e-05}, {"id": 262, "seek": 117994, "start": 1196.66, "end": 1200.18, "text": " For rl training would actually lead to converging to Nash equilibria", "tokens": [51200, 1171, 367, 75, 3097, 576, 767, 1477, 281, 9652, 3249, 281, 25012, 14204, 4668, 51376], "temperature": 0.0, "avg_logprob": -0.11452495200293404, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.3551729100290686e-05}, {"id": 263, "seek": 117994, "start": 1200.5800000000002, "end": 1206.18, "text": " A lot of the theory is just stating that if you were to run the system the learning system for a long time if we make the assumption that", "tokens": [51396, 316, 688, 295, 264, 5261, 307, 445, 26688, 300, 498, 291, 645, 281, 1190, 264, 1185, 264, 2539, 1185, 337, 257, 938, 565, 498, 321, 652, 264, 15302, 300, 51676], "temperature": 0.0, "avg_logprob": -0.11452495200293404, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.3551729100290686e-05}, {"id": 264, "seek": 117994, "start": 1207.38, "end": 1209.38, "text": " the optimization algorithm is", "tokens": [51736, 264, 19618, 9284, 307, 51836], "temperature": 0.0, "avg_logprob": -0.11452495200293404, "compression_ratio": 1.7077464788732395, "no_speech_prob": 2.3551729100290686e-05}, {"id": 265, "seek": 120994, "start": 1210.02, "end": 1213.78, "text": " fairly good at producing, you know an improved response to the", "tokens": [50368, 6457, 665, 412, 10501, 11, 291, 458, 364, 9689, 4134, 281, 264, 50556], "temperature": 0.0, "avg_logprob": -0.12733918047965842, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.0014509577304124832}, {"id": 266, "seek": 120994, "start": 1214.5, "end": 1220.98, "text": " Other player in this type of zero sum game you if you're assuming that if the successive sort of series of best responses", "tokens": [50592, 5358, 4256, 294, 341, 2010, 295, 4018, 2408, 1216, 291, 498, 291, 434, 11926, 300, 498, 264, 48043, 1333, 295, 2638, 295, 1151, 13019, 50916], "temperature": 0.0, "avg_logprob": -0.12733918047965842, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.0014509577304124832}, {"id": 267, "seek": 120994, "start": 1221.6200000000001, "end": 1224.02, "text": " That the optimization algorithm is generating", "tokens": [50948, 663, 264, 19618, 9284, 307, 17746, 51068], "temperature": 0.0, "avg_logprob": -0.12733918047965842, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.0014509577304124832}, {"id": 268, "seek": 120994, "start": 1224.9, "end": 1231.3, "text": " Continues to improve over the previous ones you could make the assumption that maybe eventually it does get to that equilibrium", "tokens": [51112, 14674, 1247, 281, 3470, 670, 264, 3894, 2306, 291, 727, 652, 264, 15302, 300, 1310, 4728, 309, 775, 483, 281, 300, 15625, 51432], "temperature": 0.0, "avg_logprob": -0.12733918047965842, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.0014509577304124832}, {"id": 269, "seek": 120994, "start": 1231.38, "end": 1234.5800000000002, "text": " But there is no mathematical guarantee that this actually happens", "tokens": [51436, 583, 456, 307, 572, 18894, 10815, 300, 341, 767, 2314, 51596], "temperature": 0.0, "avg_logprob": -0.12733918047965842, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.0014509577304124832}, {"id": 270, "seek": 120994, "start": 1235.38, "end": 1237.38, "text": " what we want to do is", "tokens": [51636, 437, 321, 528, 281, 360, 307, 51736], "temperature": 0.0, "avg_logprob": -0.12733918047965842, "compression_ratio": 1.664179104477612, "no_speech_prob": 0.0014509577304124832}, {"id": 271, "seek": 123738, "start": 1237.7800000000002, "end": 1240.18, "text": " You know build this latent dynamics", "tokens": [50384, 509, 458, 1322, 341, 48994, 15679, 50504], "temperature": 0.0, "avg_logprob": -0.13272471828315094, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.00419223727658391}, {"id": 272, "seek": 123738, "start": 1241.14, "end": 1245.6200000000001, "text": " You know a predictive model which is a simulacrum of what the idealized version is", "tokens": [50552, 509, 458, 257, 35521, 2316, 597, 307, 257, 1034, 425, 326, 6247, 295, 437, 264, 7157, 1602, 3037, 307, 50776], "temperature": 0.0, "avg_logprob": -0.13272471828315094, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.00419223727658391}, {"id": 273, "seek": 123738, "start": 1245.94, "end": 1249.6200000000001, "text": " But we don't have a way of directly computing the regrets. So we kind of perform", "tokens": [50792, 583, 321, 500, 380, 362, 257, 636, 295, 3838, 15866, 264, 31214, 13, 407, 321, 733, 295, 2042, 50976], "temperature": 0.0, "avg_logprob": -0.13272471828315094, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.00419223727658391}, {"id": 274, "seek": 123738, "start": 1250.42, "end": 1253.46, "text": " You know, we learn a proxy for that regret. How does that work?", "tokens": [51016, 509, 458, 11, 321, 1466, 257, 29690, 337, 300, 10879, 13, 1012, 775, 300, 589, 30, 51168], "temperature": 0.0, "avg_logprob": -0.13272471828315094, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.00419223727658391}, {"id": 275, "seek": 123738, "start": 1253.7, "end": 1257.14, "text": " So we think of regret in the following way. So so there's kind of this um", "tokens": [51180, 407, 321, 519, 295, 10879, 294, 264, 3480, 636, 13, 407, 370, 456, 311, 733, 295, 341, 1105, 51352], "temperature": 0.0, "avg_logprob": -0.13272471828315094, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.00419223727658391}, {"id": 276, "seek": 123738, "start": 1257.7800000000002, "end": 1264.18, "text": " Old school result from like um mdp theory or maybe it's not that old but like 20 years ago or something like that called the simulation lemma", "tokens": [51384, 8633, 1395, 1874, 490, 411, 1105, 275, 67, 79, 5261, 420, 1310, 309, 311, 406, 300, 1331, 457, 411, 945, 924, 2057, 420, 746, 411, 300, 1219, 264, 16575, 7495, 1696, 51704], "temperature": 0.0, "avg_logprob": -0.13272471828315094, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.00419223727658391}, {"id": 277, "seek": 123738, "start": 1264.5, "end": 1266.5, "text": " and that basically says that you know", "tokens": [51720, 293, 300, 1936, 1619, 300, 291, 458, 51820], "temperature": 0.0, "avg_logprob": -0.13272471828315094, "compression_ratio": 1.6950819672131148, "no_speech_prob": 0.00419223727658391}, {"id": 278, "seek": 126650, "start": 1266.74, "end": 1269.7, "text": " If we let's assume for now that we we have like an optimal planner", "tokens": [50376, 759, 321, 718, 311, 6552, 337, 586, 300, 321, 321, 362, 411, 364, 16252, 31268, 50524], "temperature": 0.0, "avg_logprob": -0.09787951982938327, "compression_ratio": 2.0, "no_speech_prob": 0.0011334348237141967}, {"id": 279, "seek": 126650, "start": 1269.78, "end": 1273.86, "text": " So we can give our like model of the world to this optimal planner and end some reward function", "tokens": [50528, 407, 321, 393, 976, 527, 411, 2316, 295, 264, 1002, 281, 341, 16252, 31268, 293, 917, 512, 7782, 2445, 50732], "temperature": 0.0, "avg_logprob": -0.09787951982938327, "compression_ratio": 2.0, "no_speech_prob": 0.0011334348237141967}, {"id": 280, "seek": 126650, "start": 1273.94, "end": 1276.26, "text": " Let's say later down the road we get given some reward function", "tokens": [50736, 961, 311, 584, 1780, 760, 264, 3060, 321, 483, 2212, 512, 7782, 2445, 50852], "temperature": 0.0, "avg_logprob": -0.09787951982938327, "compression_ratio": 2.0, "no_speech_prob": 0.0011334348237141967}, {"id": 281, "seek": 126650, "start": 1276.74, "end": 1281.3, "text": " And so we give the model and the reward function to our optimal planner and we assume that this planner can return", "tokens": [50876, 400, 370, 321, 976, 264, 2316, 293, 264, 7782, 2445, 281, 527, 16252, 31268, 293, 321, 6552, 300, 341, 31268, 393, 2736, 51104], "temperature": 0.0, "avg_logprob": -0.09787951982938327, "compression_ratio": 2.0, "no_speech_prob": 0.0011334348237141967}, {"id": 282, "seek": 126650, "start": 1281.7, "end": 1283.7, "text": " The optimal policy in our model", "tokens": [51124, 440, 16252, 3897, 294, 527, 2316, 51224], "temperature": 0.0, "avg_logprob": -0.09787951982938327, "compression_ratio": 2.0, "no_speech_prob": 0.0011334348237141967}, {"id": 283, "seek": 126650, "start": 1284.26, "end": 1286.26, "text": " So we kind of have this, you know planning oracle", "tokens": [51252, 407, 321, 733, 295, 362, 341, 11, 291, 458, 5038, 420, 7041, 51352], "temperature": 0.0, "avg_logprob": -0.09787951982938327, "compression_ratio": 2.0, "no_speech_prob": 0.0011334348237141967}, {"id": 284, "seek": 126650, "start": 1286.98, "end": 1288.82, "text": " And if we assume that we can do that", "tokens": [51388, 400, 498, 321, 6552, 300, 321, 393, 360, 300, 51480], "temperature": 0.0, "avg_logprob": -0.09787951982938327, "compression_ratio": 2.0, "no_speech_prob": 0.0011334348237141967}, {"id": 285, "seek": 126650, "start": 1288.82, "end": 1292.18, "text": " Then we can think about the difference between like how good the policy would be from", "tokens": [51480, 1396, 321, 393, 519, 466, 264, 2649, 1296, 411, 577, 665, 264, 3897, 576, 312, 490, 51648], "temperature": 0.0, "avg_logprob": -0.09787951982938327, "compression_ratio": 2.0, "no_speech_prob": 0.0011334348237141967}, {"id": 286, "seek": 129218, "start": 1292.74, "end": 1296.66, "text": " Our planning oracle in the model versus the truly optimal policy in the real world", "tokens": [50392, 2621, 5038, 420, 7041, 294, 264, 2316, 5717, 264, 4908, 16252, 3897, 294, 264, 957, 1002, 50588], "temperature": 0.0, "avg_logprob": -0.08863678845492276, "compression_ratio": 2.033333333333333, "no_speech_prob": 0.029305577278137207}, {"id": 287, "seek": 129218, "start": 1297.46, "end": 1301.94, "text": " And so what the simulation lemma tells us is that you know the difference between these two policies", "tokens": [50628, 400, 370, 437, 264, 16575, 7495, 1696, 5112, 505, 307, 300, 291, 458, 264, 2649, 1296, 613, 732, 7657, 50852], "temperature": 0.0, "avg_logprob": -0.08863678845492276, "compression_ratio": 2.033333333333333, "no_speech_prob": 0.029305577278137207}, {"id": 288, "seek": 129218, "start": 1301.94, "end": 1304.98, "text": " So the one found by acting optimally in the model versus the truly optimal one", "tokens": [50852, 407, 264, 472, 1352, 538, 6577, 5028, 379, 294, 264, 2316, 5717, 264, 4908, 16252, 472, 51004], "temperature": 0.0, "avg_logprob": -0.08863678845492276, "compression_ratio": 2.033333333333333, "no_speech_prob": 0.029305577278137207}, {"id": 289, "seek": 129218, "start": 1305.46, "end": 1309.0600000000002, "text": " Is bounded essentially by the error between the model and the real world", "tokens": [51028, 1119, 37498, 4476, 538, 264, 6713, 1296, 264, 2316, 293, 264, 957, 1002, 51208], "temperature": 0.0, "avg_logprob": -0.08863678845492276, "compression_ratio": 2.033333333333333, "no_speech_prob": 0.029305577278137207}, {"id": 290, "seek": 129218, "start": 1309.78, "end": 1313.22, "text": " Under the distribution of states that the policy would generate", "tokens": [51244, 6974, 264, 7316, 295, 4368, 300, 264, 3897, 576, 8460, 51416], "temperature": 0.0, "avg_logprob": -0.08863678845492276, "compression_ratio": 2.033333333333333, "no_speech_prob": 0.029305577278137207}, {"id": 291, "seek": 129218, "start": 1313.46, "end": 1318.3400000000001, "text": " So so, you know, it only it only matters that we have low error where the policy would go essentially because you know", "tokens": [51428, 407, 370, 11, 291, 458, 11, 309, 787, 309, 787, 7001, 300, 321, 362, 2295, 6713, 689, 264, 3897, 576, 352, 4476, 570, 291, 458, 51672], "temperature": 0.0, "avg_logprob": -0.08863678845492276, "compression_ratio": 2.033333333333333, "no_speech_prob": 0.029305577278137207}, {"id": 292, "seek": 129218, "start": 1318.5800000000002, "end": 1321.0600000000002, "text": " If there are some states that are just completely irrelevant what the policy is going to do", "tokens": [51684, 759, 456, 366, 512, 4368, 300, 366, 445, 2584, 28682, 437, 264, 3897, 307, 516, 281, 360, 51808], "temperature": 0.0, "avg_logprob": -0.08863678845492276, "compression_ratio": 2.033333333333333, "no_speech_prob": 0.029305577278137207}, {"id": 293, "seek": 132106, "start": 1321.06, "end": 1323.06, "text": " It's not really going to matter if the if the model is not", "tokens": [50364, 467, 311, 406, 534, 516, 281, 1871, 498, 264, 498, 264, 2316, 307, 406, 50464], "temperature": 0.0, "avg_logprob": -0.11629472362051765, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.004197929985821247}, {"id": 294, "seek": 132106, "start": 1323.54, "end": 1324.82, "text": " Accurate and there", "tokens": [50488, 5725, 33144, 293, 456, 50552], "temperature": 0.0, "avg_logprob": -0.11629472362051765, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.004197929985821247}, {"id": 295, "seek": 132106, "start": 1324.82, "end": 1326.82, "text": " So we kind of use this result to think about the regret", "tokens": [50552, 407, 321, 733, 295, 764, 341, 1874, 281, 519, 466, 264, 10879, 50652], "temperature": 0.0, "avg_logprob": -0.11629472362051765, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.004197929985821247}, {"id": 296, "seek": 132106, "start": 1326.8999999999999, "end": 1329.54, "text": " So that that gives us like, you know, if we have like one", "tokens": [50656, 407, 300, 300, 2709, 505, 411, 11, 291, 458, 11, 498, 321, 362, 411, 472, 50788], "temperature": 0.0, "avg_logprob": -0.11629472362051765, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.004197929985821247}, {"id": 297, "seek": 132106, "start": 1330.34, "end": 1333.7, "text": " One true mdp and one model of an mdp and one reward function", "tokens": [50828, 1485, 2074, 275, 67, 79, 293, 472, 2316, 295, 364, 275, 67, 79, 293, 472, 7782, 2445, 50996], "temperature": 0.0, "avg_logprob": -0.11629472362051765, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.004197929985821247}, {"id": 298, "seek": 132106, "start": 1334.34, "end": 1336.1799999999998, "text": " The simulation lemma can tell us, you know", "tokens": [51028, 440, 16575, 7495, 1696, 393, 980, 505, 11, 291, 458, 51120], "temperature": 0.0, "avg_logprob": -0.11629472362051765, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.004197929985821247}, {"id": 299, "seek": 132106, "start": 1336.1799999999998, "end": 1340.1, "text": " What would kind of be the regret if we did this optimal planning within this one model of the", "tokens": [51120, 708, 576, 733, 295, 312, 264, 10879, 498, 321, 630, 341, 16252, 5038, 1951, 341, 472, 2316, 295, 264, 51316], "temperature": 0.0, "avg_logprob": -0.11629472362051765, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.004197929985821247}, {"id": 300, "seek": 132106, "start": 1340.98, "end": 1342.26, "text": " Of the mdp", "tokens": [51360, 2720, 264, 275, 67, 79, 51424], "temperature": 0.0, "avg_logprob": -0.11629472362051765, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.004197929985821247}, {"id": 301, "seek": 132106, "start": 1342.26, "end": 1346.1799999999998, "text": " But then in our work, we're not really interested in the setting of like one mdp one reward function", "tokens": [51424, 583, 550, 294, 527, 589, 11, 321, 434, 406, 534, 3102, 294, 264, 3287, 295, 411, 472, 275, 67, 79, 472, 7782, 2445, 51620], "temperature": 0.0, "avg_logprob": -0.11629472362051765, "compression_ratio": 1.848708487084871, "no_speech_prob": 0.004197929985821247}, {"id": 302, "seek": 134618, "start": 1346.9, "end": 1352.8200000000002, "text": " Um, so we start to think about, you know, what happens if we have arbitrarily many environments as well as arbitrarily many reward functions", "tokens": [50400, 3301, 11, 370, 321, 722, 281, 519, 466, 11, 291, 458, 11, 437, 2314, 498, 321, 362, 19071, 3289, 867, 12388, 382, 731, 382, 19071, 3289, 867, 7782, 6828, 50696], "temperature": 0.0, "avg_logprob": -0.12290524764799736, "compression_ratio": 1.7506925207756232, "no_speech_prob": 0.002980803372338414}, {"id": 303, "seek": 134618, "start": 1352.98, "end": 1354.5, "text": " Which we don't know in advance", "tokens": [50704, 3013, 321, 500, 380, 458, 294, 7295, 50780], "temperature": 0.0, "avg_logprob": -0.12290524764799736, "compression_ratio": 1.7506925207756232, "no_speech_prob": 0.002980803372338414}, {"id": 304, "seek": 134618, "start": 1354.5, "end": 1358.42, "text": " And then I guess the other thing that I should say like you you alluded to like latent dynamics is", "tokens": [50780, 400, 550, 286, 2041, 264, 661, 551, 300, 286, 820, 584, 411, 291, 291, 33919, 281, 411, 48994, 15679, 307, 50976], "temperature": 0.0, "avg_logprob": -0.12290524764799736, "compression_ratio": 1.7506925207756232, "no_speech_prob": 0.002980803372338414}, {"id": 305, "seek": 134618, "start": 1358.66, "end": 1364.1000000000001, "text": " You know, these existing results are assuming that we have an mdp. That's fully observable meaning, you know exactly what the state of the environment is", "tokens": [50988, 509, 458, 11, 613, 6741, 3542, 366, 11926, 300, 321, 362, 364, 275, 67, 79, 13, 663, 311, 4498, 9951, 712, 3620, 11, 291, 458, 2293, 437, 264, 1785, 295, 264, 2823, 307, 51260], "temperature": 0.0, "avg_logprob": -0.12290524764799736, "compression_ratio": 1.7506925207756232, "no_speech_prob": 0.002980803372338414}, {"id": 306, "seek": 134618, "start": 1364.66, "end": 1368.98, "text": " Um, but usually when we think about like world models or even or just maybe more modern reinforcement learning", "tokens": [51288, 3301, 11, 457, 2673, 562, 321, 519, 466, 411, 1002, 5245, 420, 754, 420, 445, 1310, 544, 4363, 29280, 2539, 51504], "temperature": 0.0, "avg_logprob": -0.12290524764799736, "compression_ratio": 1.7506925207756232, "no_speech_prob": 0.002980803372338414}, {"id": 307, "seek": 134618, "start": 1369.6200000000001, "end": 1372.9, "text": " We're really interested in learning from like quite high dimensional signals. So", "tokens": [51536, 492, 434, 534, 3102, 294, 2539, 490, 411, 1596, 1090, 18795, 12354, 13, 407, 51700], "temperature": 0.0, "avg_logprob": -0.12290524764799736, "compression_ratio": 1.7506925207756232, "no_speech_prob": 0.002980803372338414}, {"id": 308, "seek": 134618, "start": 1373.46, "end": 1375.22, "text": " images or maybe", "tokens": [51728, 5267, 420, 1310, 51816], "temperature": 0.0, "avg_logprob": -0.12290524764799736, "compression_ratio": 1.7506925207756232, "no_speech_prob": 0.002980803372338414}, {"id": 309, "seek": 137522, "start": 1375.22, "end": 1378.5, "text": " Probably images, but maybe there are the high high dimensional signals we want to reason about", "tokens": [50364, 9210, 5267, 11, 457, 1310, 456, 366, 264, 1090, 1090, 18795, 12354, 321, 528, 281, 1778, 466, 50528], "temperature": 0.0, "avg_logprob": -0.11261513084173203, "compression_ratio": 1.9444444444444444, "no_speech_prob": 0.0012063306057825685}, {"id": 310, "seek": 137522, "start": 1379.3, "end": 1383.78, "text": " And because we're just using image observations, this means that the world is like partially observable", "tokens": [50568, 400, 570, 321, 434, 445, 1228, 3256, 18163, 11, 341, 1355, 300, 264, 1002, 307, 411, 18886, 9951, 712, 50792], "temperature": 0.0, "avg_logprob": -0.11261513084173203, "compression_ratio": 1.9444444444444444, "no_speech_prob": 0.0012063306057825685}, {"id": 311, "seek": 137522, "start": 1383.78, "end": 1387.3, "text": " Like we can't infer everything we need to know about the world just from one image, you know", "tokens": [50792, 1743, 321, 393, 380, 13596, 1203, 321, 643, 281, 458, 466, 264, 1002, 445, 490, 472, 3256, 11, 291, 458, 50968], "temperature": 0.0, "avg_logprob": -0.11261513084173203, "compression_ratio": 1.9444444444444444, "no_speech_prob": 0.0012063306057825685}, {"id": 312, "seek": 137522, "start": 1388.42, "end": 1393.22, "text": " For basically any physical task like the velocity of objects is important, but you can't infer that just from one image", "tokens": [51024, 1171, 1936, 604, 4001, 5633, 411, 264, 9269, 295, 6565, 307, 1021, 11, 457, 291, 393, 380, 13596, 300, 445, 490, 472, 3256, 51264], "temperature": 0.0, "avg_logprob": -0.11261513084173203, "compression_ratio": 1.9444444444444444, "no_speech_prob": 0.0012063306057825685}, {"id": 313, "seek": 137522, "start": 1393.94, "end": 1397.14, "text": " Um, so in this partially observable environments, we really want to take", "tokens": [51300, 3301, 11, 370, 294, 341, 18886, 9951, 712, 12388, 11, 321, 534, 528, 281, 747, 51460], "temperature": 0.0, "avg_logprob": -0.11261513084173203, "compression_ratio": 1.9444444444444444, "no_speech_prob": 0.0012063306057825685}, {"id": 314, "seek": 137522, "start": 1397.7, "end": 1402.66, "text": " A sequence of observations because we need to to use those sequence of observations to infer what the state is", "tokens": [51488, 316, 8310, 295, 18163, 570, 321, 643, 281, 281, 764, 729, 8310, 295, 18163, 281, 13596, 437, 264, 1785, 307, 51736], "temperature": 0.0, "avg_logprob": -0.11261513084173203, "compression_ratio": 1.9444444444444444, "no_speech_prob": 0.0012063306057825685}, {"id": 315, "seek": 140266, "start": 1402.98, "end": 1406.1000000000001, "text": " So, you know viewing a sequence of images will help me to infer what the um", "tokens": [50380, 407, 11, 291, 458, 17480, 257, 8310, 295, 5267, 486, 854, 385, 281, 13596, 437, 264, 1105, 50536], "temperature": 0.0, "avg_logprob": -0.11203879780239528, "compression_ratio": 1.9029126213592233, "no_speech_prob": 0.001098626060411334}, {"id": 316, "seek": 140266, "start": 1406.8200000000002, "end": 1410.3400000000001, "text": " The velocities are for example, and so we can think of this as inferring like a belief", "tokens": [50572, 440, 7806, 1088, 366, 337, 1365, 11, 293, 370, 321, 393, 519, 295, 341, 382, 13596, 2937, 411, 257, 7107, 50748], "temperature": 0.0, "avg_logprob": -0.11203879780239528, "compression_ratio": 1.9029126213592233, "no_speech_prob": 0.001098626060411334}, {"id": 317, "seek": 140266, "start": 1411.0600000000002, "end": 1413.94, "text": " A belief over what the state is and a partially observable mdp", "tokens": [50784, 316, 7107, 670, 437, 264, 1785, 307, 293, 257, 18886, 9951, 712, 275, 67, 79, 50928], "temperature": 0.0, "avg_logprob": -0.11203879780239528, "compression_ratio": 1.9029126213592233, "no_speech_prob": 0.001098626060411334}, {"id": 318, "seek": 140266, "start": 1414.66, "end": 1419.3000000000002, "text": " Um, so we need this full sequence of images and we need to use the full sequence of images to then to be able to predict ahead", "tokens": [50964, 3301, 11, 370, 321, 643, 341, 1577, 8310, 295, 5267, 293, 321, 643, 281, 764, 264, 1577, 8310, 295, 5267, 281, 550, 281, 312, 1075, 281, 6069, 2286, 51196], "temperature": 0.0, "avg_logprob": -0.11203879780239528, "compression_ratio": 1.9029126213592233, "no_speech_prob": 0.001098626060411334}, {"id": 319, "seek": 140266, "start": 1419.3000000000002, "end": 1423.7, "text": " What the next observation will be and that's kind of what you know, most world models are attempting to do", "tokens": [51196, 708, 264, 958, 14816, 486, 312, 293, 300, 311, 733, 295, 437, 291, 458, 11, 881, 1002, 5245, 366, 22001, 281, 360, 51416], "temperature": 0.0, "avg_logprob": -0.11203879780239528, "compression_ratio": 1.9029126213592233, "no_speech_prob": 0.001098626060411334}, {"id": 320, "seek": 140266, "start": 1424.26, "end": 1429.5400000000002, "text": " Um, but if we just like taken a bunch of images and then try and directly predict images again, that's like quite a hard problem", "tokens": [51444, 3301, 11, 457, 498, 321, 445, 411, 2726, 257, 3840, 295, 5267, 293, 550, 853, 293, 3838, 6069, 5267, 797, 11, 300, 311, 411, 1596, 257, 1152, 1154, 51708], "temperature": 0.0, "avg_logprob": -0.11203879780239528, "compression_ratio": 1.9029126213592233, "no_speech_prob": 0.001098626060411334}, {"id": 321, "seek": 142954, "start": 1430.26, "end": 1432.98, "text": " Um to just like just predict straight an image space", "tokens": [50400, 3301, 281, 445, 411, 445, 6069, 2997, 364, 3256, 1901, 50536], "temperature": 0.0, "avg_logprob": -0.09231263500148967, "compression_ratio": 1.9806949806949807, "no_speech_prob": 0.007120059337466955}, {"id": 322, "seek": 142954, "start": 1433.46, "end": 1436.8999999999999, "text": " And so the most common thing to do is kind of to take your previous sequence of images", "tokens": [50560, 400, 370, 264, 881, 2689, 551, 281, 360, 307, 733, 295, 281, 747, 428, 3894, 8310, 295, 5267, 50732], "temperature": 0.0, "avg_logprob": -0.09231263500148967, "compression_ratio": 1.9806949806949807, "no_speech_prob": 0.007120059337466955}, {"id": 323, "seek": 142954, "start": 1437.3, "end": 1441.94, "text": " And then try and get like some compressed representation of the history of images into like the latent state", "tokens": [50752, 400, 550, 853, 293, 483, 411, 512, 30353, 10290, 295, 264, 2503, 295, 5267, 666, 411, 264, 48994, 1785, 50984], "temperature": 0.0, "avg_logprob": -0.09231263500148967, "compression_ratio": 1.9806949806949807, "no_speech_prob": 0.007120059337466955}, {"id": 324, "seek": 142954, "start": 1442.6599999999999, "end": 1444.6599999999999, "text": " And then predict the dynamics in the latent state", "tokens": [51020, 400, 550, 6069, 264, 15679, 294, 264, 48994, 1785, 51120], "temperature": 0.0, "avg_logprob": -0.09231263500148967, "compression_ratio": 1.9806949806949807, "no_speech_prob": 0.007120059337466955}, {"id": 325, "seek": 142954, "start": 1445.3799999999999, "end": 1447.3, "text": " So yeah, so I have my sequence of images", "tokens": [51156, 407, 1338, 11, 370, 286, 362, 452, 8310, 295, 5267, 51252], "temperature": 0.0, "avg_logprob": -0.09231263500148967, "compression_ratio": 1.9806949806949807, "no_speech_prob": 0.007120059337466955}, {"id": 326, "seek": 142954, "start": 1447.3799999999999, "end": 1449.94, "text": " I kind of compress these somehow into some vector", "tokens": [51256, 286, 733, 295, 14778, 613, 6063, 666, 512, 8062, 51384], "temperature": 0.0, "avg_logprob": -0.09231263500148967, "compression_ratio": 1.9806949806949807, "no_speech_prob": 0.007120059337466955}, {"id": 327, "seek": 142954, "start": 1450.42, "end": 1455.54, "text": " And then I give it a new new action and I try and predict what the next kind of latent vector will be given this new action", "tokens": [51408, 400, 550, 286, 976, 309, 257, 777, 777, 3069, 293, 286, 853, 293, 6069, 437, 264, 958, 733, 295, 48994, 8062, 486, 312, 2212, 341, 777, 3069, 51664], "temperature": 0.0, "avg_logprob": -0.09231263500148967, "compression_ratio": 1.9806949806949807, "no_speech_prob": 0.007120059337466955}, {"id": 328, "seek": 145554, "start": 1455.86, "end": 1458.5, "text": " And this now represents my prediction of the dynamics in the world", "tokens": [50380, 400, 341, 586, 8855, 452, 17630, 295, 264, 15679, 294, 264, 1002, 50512], "temperature": 0.0, "avg_logprob": -0.10832082112630208, "compression_ratio": 1.9113149847094801, "no_speech_prob": 0.003000300144776702}, {"id": 329, "seek": 145554, "start": 1458.8999999999999, "end": 1462.42, "text": " And then if I want to um, you know predict what the next observation would be an image space", "tokens": [50532, 400, 550, 498, 286, 528, 281, 1105, 11, 291, 458, 6069, 437, 264, 958, 14816, 576, 312, 364, 3256, 1901, 50708], "temperature": 0.0, "avg_logprob": -0.10832082112630208, "compression_ratio": 1.9113149847094801, "no_speech_prob": 0.003000300144776702}, {"id": 330, "seek": 145554, "start": 1462.42, "end": 1464.1, "text": " Then I can also decode that back to an image", "tokens": [50708, 1396, 286, 393, 611, 979, 1429, 300, 646, 281, 364, 3256, 50792], "temperature": 0.0, "avg_logprob": -0.10832082112630208, "compression_ratio": 1.9113149847094801, "no_speech_prob": 0.003000300144776702}, {"id": 331, "seek": 145554, "start": 1464.6599999999999, "end": 1469.06, "text": " Um, but then a lot of works also argue that maybe we don't want to actually learn to predict the entire image", "tokens": [50820, 3301, 11, 457, 550, 257, 688, 295, 1985, 611, 9695, 300, 1310, 321, 500, 380, 528, 281, 767, 1466, 281, 6069, 264, 2302, 3256, 51040], "temperature": 0.0, "avg_logprob": -0.10832082112630208, "compression_ratio": 1.9113149847094801, "no_speech_prob": 0.003000300144776702}, {"id": 332, "seek": 145554, "start": 1469.06, "end": 1471.3, "text": " So maybe you don't want to actually decode the entire image", "tokens": [51040, 407, 1310, 291, 500, 380, 528, 281, 767, 979, 1429, 264, 2302, 3256, 51152], "temperature": 0.0, "avg_logprob": -0.10832082112630208, "compression_ratio": 1.9113149847094801, "no_speech_prob": 0.003000300144776702}, {"id": 333, "seek": 145554, "start": 1471.3, "end": 1477.1399999999999, "text": " But that's that's another aspect that we might want to get into but there's this whole broad story of of um working in the latent space", "tokens": [51152, 583, 300, 311, 300, 311, 1071, 4171, 300, 321, 1062, 528, 281, 483, 666, 457, 456, 311, 341, 1379, 4152, 1657, 295, 295, 1105, 1364, 294, 264, 48994, 1901, 51444], "temperature": 0.0, "avg_logprob": -0.10832082112630208, "compression_ratio": 1.9113149847094801, "no_speech_prob": 0.003000300144776702}, {"id": 334, "seek": 145554, "start": 1477.54, "end": 1481.86, "text": " And um in reinforcement learning there was that paper called world models by you know, david haran and schmidhuber", "tokens": [51464, 400, 1105, 294, 29280, 2539, 456, 390, 300, 3035, 1219, 1002, 5245, 538, 291, 458, 11, 11753, 327, 2233, 282, 293, 956, 25394, 71, 10261, 51680], "temperature": 0.0, "avg_logprob": -0.10832082112630208, "compression_ratio": 1.9113149847094801, "no_speech_prob": 0.003000300144776702}, {"id": 335, "seek": 148186, "start": 1482.34, "end": 1488.5, "text": " And it also I think has a relationship with you know, what lakoon's doing with jepper and these like you know joint embedding prediction architecture", "tokens": [50388, 400, 309, 611, 286, 519, 575, 257, 2480, 365, 291, 458, 11, 437, 287, 514, 4106, 311, 884, 365, 1506, 3717, 293, 613, 411, 291, 458, 7225, 12240, 3584, 17630, 9482, 50696], "temperature": 0.0, "avg_logprob": -0.15022602677345276, "compression_ratio": 1.7770897832817338, "no_speech_prob": 0.012284720316529274}, {"id": 336, "seek": 148186, "start": 1488.5, "end": 1491.06, "text": " So there seems to be something magical about working in in the latent space", "tokens": [50696, 407, 456, 2544, 281, 312, 746, 12066, 466, 1364, 294, 294, 264, 48994, 1901, 50824], "temperature": 0.0, "avg_logprob": -0.15022602677345276, "compression_ratio": 1.7770897832817338, "no_speech_prob": 0.012284720316529274}, {"id": 337, "seek": 148186, "start": 1491.3799999999999, "end": 1495.4599999999998, "text": " And also you were talking about um, you know partially observable markoff decision processors", "tokens": [50840, 400, 611, 291, 645, 1417, 466, 1105, 11, 291, 458, 18886, 9951, 712, 1491, 4506, 3537, 27751, 51044], "temperature": 0.0, "avg_logprob": -0.15022602677345276, "compression_ratio": 1.7770897832817338, "no_speech_prob": 0.012284720316529274}, {"id": 338, "seek": 148186, "start": 1495.78, "end": 1499.1399999999999, "text": " And you know, that seems to be this idea that we need to have a modeling framework for the world", "tokens": [51060, 400, 291, 458, 11, 300, 2544, 281, 312, 341, 1558, 300, 321, 643, 281, 362, 257, 15983, 8388, 337, 264, 1002, 51228], "temperature": 0.0, "avg_logprob": -0.15022602677345276, "compression_ratio": 1.7770897832817338, "no_speech_prob": 0.012284720316529274}, {"id": 339, "seek": 148186, "start": 1499.54, "end": 1504.26, "text": " And I guess like the ideal situation would be is that like we just we we knew exactly what would happen", "tokens": [51248, 400, 286, 2041, 411, 264, 7157, 2590, 576, 312, 307, 300, 411, 321, 445, 321, 321, 2586, 2293, 437, 576, 1051, 51484], "temperature": 0.0, "avg_logprob": -0.15022602677345276, "compression_ratio": 1.7770897832817338, "no_speech_prob": 0.012284720316529274}, {"id": 340, "seek": 148186, "start": 1504.6599999999999, "end": 1507.1399999999999, "text": " You know every single time step in every single state", "tokens": [51504, 509, 458, 633, 2167, 565, 1823, 294, 633, 2167, 1785, 51628], "temperature": 0.0, "avg_logprob": -0.15022602677345276, "compression_ratio": 1.7770897832817338, "no_speech_prob": 0.012284720316529274}, {"id": 341, "seek": 150714, "start": 1507.7, "end": 1510.8200000000002, "text": " Um, but we don't you know, so so we model it as a partially observable", "tokens": [50392, 3301, 11, 457, 321, 500, 380, 291, 458, 11, 370, 370, 321, 2316, 309, 382, 257, 18886, 9951, 712, 50548], "temperature": 0.0, "avg_logprob": -0.1314132365774601, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.007967174053192139}, {"id": 342, "seek": 150714, "start": 1511.0600000000002, "end": 1513.94, "text": " Markov decision process and the markov bit is quite interesting as well", "tokens": [50560, 3934, 5179, 3537, 1399, 293, 264, 1491, 5179, 857, 307, 1596, 1880, 382, 731, 50704], "temperature": 0.0, "avg_logprob": -0.1314132365774601, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.007967174053192139}, {"id": 343, "seek": 150714, "start": 1513.94, "end": 1517.8600000000001, "text": " I mean maybe and you guys can just sort of introduce what why do we use that as a model?", "tokens": [50704, 286, 914, 1310, 293, 291, 1074, 393, 445, 1333, 295, 5366, 437, 983, 360, 321, 764, 300, 382, 257, 2316, 30, 50900], "temperature": 0.0, "avg_logprob": -0.1314132365774601, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.007967174053192139}, {"id": 344, "seek": 150714, "start": 1518.26, "end": 1523.6200000000001, "text": " So markovian basically just means you only need to look at like the current state to be able to infer all the information about the system", "tokens": [50920, 407, 1491, 5179, 952, 1936, 445, 1355, 291, 787, 643, 281, 574, 412, 411, 264, 2190, 1785, 281, 312, 1075, 281, 13596, 439, 264, 1589, 466, 264, 1185, 51188], "temperature": 0.0, "avg_logprob": -0.1314132365774601, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.007967174053192139}, {"id": 345, "seek": 150714, "start": 1524.3400000000001, "end": 1526.42, "text": " um, so so in a markov decision process", "tokens": [51224, 1105, 11, 370, 370, 294, 257, 1491, 5179, 3537, 1399, 51328], "temperature": 0.0, "avg_logprob": -0.1314132365774601, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.007967174053192139}, {"id": 346, "seek": 150714, "start": 1526.5800000000002, "end": 1531.6200000000001, "text": " We have some state and then we assume that we're able to take some actions and given some state in some action", "tokens": [51336, 492, 362, 512, 1785, 293, 550, 321, 6552, 300, 321, 434, 1075, 281, 747, 512, 5909, 293, 2212, 512, 1785, 294, 512, 3069, 51588], "temperature": 0.0, "avg_logprob": -0.1314132365774601, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.007967174053192139}, {"id": 347, "seek": 150714, "start": 1531.6200000000001, "end": 1533.7800000000002, "text": " We get some distribution over next states of the system", "tokens": [51588, 492, 483, 512, 7316, 670, 958, 4368, 295, 264, 1185, 51696], "temperature": 0.0, "avg_logprob": -0.1314132365774601, "compression_ratio": 1.8343949044585988, "no_speech_prob": 0.007967174053192139}, {"id": 348, "seek": 153378, "start": 1534.1, "end": 1537.3799999999999, "text": " And then the the system will transition according to that distribution to the next state", "tokens": [50380, 400, 550, 264, 264, 1185, 486, 6034, 4650, 281, 300, 7316, 281, 264, 958, 1785, 50544], "temperature": 0.0, "avg_logprob": -0.10542383522822939, "compression_ratio": 1.8300283286118981, "no_speech_prob": 0.005909041501581669}, {"id": 349, "seek": 153378, "start": 1537.62, "end": 1541.78, "text": " And this is just like kind of a general framework for modeling like systems that we might want to control", "tokens": [50556, 400, 341, 307, 445, 411, 733, 295, 257, 2674, 8388, 337, 15983, 411, 3652, 300, 321, 1062, 528, 281, 1969, 50764], "temperature": 0.0, "avg_logprob": -0.10542383522822939, "compression_ratio": 1.8300283286118981, "no_speech_prob": 0.005909041501581669}, {"id": 350, "seek": 153378, "start": 1541.94, "end": 1545.06, "text": " So, you know, it kind of dates back to like early work and control theory", "tokens": [50772, 407, 11, 291, 458, 11, 309, 733, 295, 11691, 646, 281, 411, 2440, 589, 293, 1969, 5261, 50928], "temperature": 0.0, "avg_logprob": -0.10542383522822939, "compression_ratio": 1.8300283286118981, "no_speech_prob": 0.005909041501581669}, {"id": 351, "seek": 153378, "start": 1545.06, "end": 1547.3799999999999, "text": " But then it's also the main framework used in reinforcement learning", "tokens": [50928, 583, 550, 309, 311, 611, 264, 2135, 8388, 1143, 294, 29280, 2539, 51044], "temperature": 0.0, "avg_logprob": -0.10542383522822939, "compression_ratio": 1.8300283286118981, "no_speech_prob": 0.005909041501581669}, {"id": 352, "seek": 153378, "start": 1547.86, "end": 1548.02, "text": " um", "tokens": [51068, 1105, 51076], "temperature": 0.0, "avg_logprob": -0.10542383522822939, "compression_ratio": 1.8300283286118981, "no_speech_prob": 0.005909041501581669}, {"id": 353, "seek": 153378, "start": 1548.02, "end": 1550.74, "text": " Yeah, and the reinforcement learning setting because it's the decision process", "tokens": [51076, 865, 11, 293, 264, 29280, 2539, 3287, 570, 309, 311, 264, 3537, 1399, 51212], "temperature": 0.0, "avg_logprob": -0.10542383522822939, "compression_ratio": 1.8300283286118981, "no_speech_prob": 0.005909041501581669}, {"id": 354, "seek": 153378, "start": 1550.82, "end": 1556.66, "text": " We we also add an reward function which tells us how good it is to be in a certain state or to execute a certain state action pair", "tokens": [51216, 492, 321, 611, 909, 364, 7782, 2445, 597, 5112, 505, 577, 665, 309, 307, 281, 312, 294, 257, 1629, 1785, 420, 281, 14483, 257, 1629, 1785, 3069, 6119, 51508], "temperature": 0.0, "avg_logprob": -0.10542383522822939, "compression_ratio": 1.8300283286118981, "no_speech_prob": 0.005909041501581669}, {"id": 355, "seek": 153378, "start": 1557.3, "end": 1560.8999999999999, "text": " Um, but yeah, as you said with relating to like partial observability and a lot of like systems", "tokens": [51540, 3301, 11, 457, 1338, 11, 382, 291, 848, 365, 23968, 281, 411, 14641, 9951, 2310, 293, 257, 688, 295, 411, 3652, 51720], "temperature": 0.0, "avg_logprob": -0.10542383522822939, "compression_ratio": 1.8300283286118981, "no_speech_prob": 0.005909041501581669}, {"id": 356, "seek": 156090, "start": 1560.98, "end": 1563.46, "text": " We we don't actually know what the true like state of the world is", "tokens": [50368, 492, 321, 500, 380, 767, 458, 437, 264, 2074, 411, 1785, 295, 264, 1002, 307, 50492], "temperature": 0.0, "avg_logprob": -0.0760657804725814, "compression_ratio": 1.9266666666666667, "no_speech_prob": 0.0037065958604216576}, {"id": 357, "seek": 156090, "start": 1563.46, "end": 1568.42, "text": " So so you can imagine, you know, if we want to think of the entire world as a partially observable mdp", "tokens": [50492, 407, 370, 291, 393, 3811, 11, 291, 458, 11, 498, 321, 528, 281, 519, 295, 264, 2302, 1002, 382, 257, 18886, 9951, 712, 275, 67, 79, 50740], "temperature": 0.0, "avg_logprob": -0.0760657804725814, "compression_ratio": 1.9266666666666667, "no_speech_prob": 0.0037065958604216576}, {"id": 358, "seek": 156090, "start": 1569.22, "end": 1574.9, "text": " We can't just have some vector telling us exactly what the true configuration of the world is or maybe that exists", "tokens": [50780, 492, 393, 380, 445, 362, 512, 8062, 3585, 505, 2293, 437, 264, 2074, 11694, 295, 264, 1002, 307, 420, 1310, 300, 8198, 51064], "temperature": 0.0, "avg_logprob": -0.0760657804725814, "compression_ratio": 1.9266666666666667, "no_speech_prob": 0.0037065958604216576}, {"id": 359, "seek": 156090, "start": 1574.9, "end": 1579.22, "text": " But we can't we definitely can't just know that and so we usually think of it as being a partially observable system", "tokens": [51064, 583, 321, 393, 380, 321, 2138, 393, 380, 445, 458, 300, 293, 370, 321, 2673, 519, 295, 309, 382, 885, 257, 18886, 9951, 712, 1185, 51280], "temperature": 0.0, "avg_logprob": -0.0760657804725814, "compression_ratio": 1.9266666666666667, "no_speech_prob": 0.0037065958604216576}, {"id": 360, "seek": 156090, "start": 1579.94, "end": 1584.66, "text": " Um, so this means that like given given the state, um, you know at each step", "tokens": [51316, 3301, 11, 370, 341, 1355, 300, 411, 2212, 2212, 264, 1785, 11, 1105, 11, 291, 458, 412, 1184, 1823, 51552], "temperature": 0.0, "avg_logprob": -0.0760657804725814, "compression_ratio": 1.9266666666666667, "no_speech_prob": 0.0037065958604216576}, {"id": 361, "seek": 156090, "start": 1584.66, "end": 1588.5800000000002, "text": " We'll basically get some distribution over observations and we just get to observe that observation", "tokens": [51552, 492, 603, 1936, 483, 512, 7316, 670, 18163, 293, 321, 445, 483, 281, 11441, 300, 14816, 51748], "temperature": 0.0, "avg_logprob": -0.0760657804725814, "compression_ratio": 1.9266666666666667, "no_speech_prob": 0.0037065958604216576}, {"id": 362, "seek": 158858, "start": 1588.98, "end": 1594.74, "text": " So, you know, the state of the world could be what it currently is in here and maybe my um observation is like a camera image", "tokens": [50384, 407, 11, 291, 458, 11, 264, 1785, 295, 264, 1002, 727, 312, 437, 309, 4362, 307, 294, 510, 293, 1310, 452, 1105, 14816, 307, 411, 257, 2799, 3256, 50672], "temperature": 0.0, "avg_logprob": -0.09909744390705288, "compression_ratio": 2.1142857142857143, "no_speech_prob": 0.0027139524463564157}, {"id": 363, "seek": 158858, "start": 1594.82, "end": 1596.58, "text": " so I only get some", "tokens": [50676, 370, 286, 787, 483, 512, 50764], "temperature": 0.0, "avg_logprob": -0.09909744390705288, "compression_ratio": 2.1142857142857143, "no_speech_prob": 0.0027139524463564157}, {"id": 364, "seek": 158858, "start": 1596.58, "end": 1599.6999999999998, "text": " Camera image of the world that allows me to infer a bit of information about the state", "tokens": [50764, 23734, 3256, 295, 264, 1002, 300, 4045, 385, 281, 13596, 257, 857, 295, 1589, 466, 264, 1785, 50920], "temperature": 0.0, "avg_logprob": -0.09909744390705288, "compression_ratio": 2.1142857142857143, "no_speech_prob": 0.0027139524463564157}, {"id": 365, "seek": 158858, "start": 1600.34, "end": 1602.82, "text": " Um, and because it only allows me to infer a bit of information about the state", "tokens": [50952, 3301, 11, 293, 570, 309, 787, 4045, 385, 281, 13596, 257, 857, 295, 1589, 466, 264, 1785, 51076], "temperature": 0.0, "avg_logprob": -0.09909744390705288, "compression_ratio": 2.1142857142857143, "no_speech_prob": 0.0027139524463564157}, {"id": 366, "seek": 158858, "start": 1602.82, "end": 1604.26, "text": " It doesn't tell me the whole state", "tokens": [51076, 467, 1177, 380, 980, 385, 264, 1379, 1785, 51148], "temperature": 0.0, "avg_logprob": -0.09909744390705288, "compression_ratio": 2.1142857142857143, "no_speech_prob": 0.0027139524463564157}, {"id": 367, "seek": 158858, "start": 1604.26, "end": 1608.74, "text": " It really you need to keep track of all of the observations you have to be able to keep track of all the information", "tokens": [51148, 467, 534, 291, 643, 281, 1066, 2837, 295, 439, 295, 264, 18163, 291, 362, 281, 312, 1075, 281, 1066, 2837, 295, 439, 264, 1589, 51372], "temperature": 0.0, "avg_logprob": -0.09909744390705288, "compression_ratio": 2.1142857142857143, "no_speech_prob": 0.0027139524463564157}, {"id": 368, "seek": 158858, "start": 1608.74, "end": 1609.6999999999998, "text": " You have about the world", "tokens": [51372, 509, 362, 466, 264, 1002, 51420], "temperature": 0.0, "avg_logprob": -0.09909744390705288, "compression_ratio": 2.1142857142857143, "no_speech_prob": 0.0027139524463564157}, {"id": 369, "seek": 158858, "start": 1609.6999999999998, "end": 1615.6999999999998, "text": " So, you know, you can imagine um, if the task is for me to remember how to get out the door a while ago", "tokens": [51420, 407, 11, 291, 458, 11, 291, 393, 3811, 1105, 11, 498, 264, 5633, 307, 337, 385, 281, 1604, 577, 281, 483, 484, 264, 2853, 257, 1339, 2057, 51720], "temperature": 0.0, "avg_logprob": -0.09909744390705288, "compression_ratio": 2.1142857142857143, "no_speech_prob": 0.0027139524463564157}, {"id": 370, "seek": 161570, "start": 1615.94, "end": 1621.46, "text": " Um, you know, I don't just need to be able to like look at my current image of the world to be able to infer that information", "tokens": [50376, 3301, 11, 291, 458, 11, 286, 500, 380, 445, 643, 281, 312, 1075, 281, 411, 574, 412, 452, 2190, 3256, 295, 264, 1002, 281, 312, 1075, 281, 13596, 300, 1589, 50652], "temperature": 0.0, "avg_logprob": -0.17882629235585532, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.000852749974001199}, {"id": 371, "seek": 161570, "start": 1621.46, "end": 1624.02, "text": " I need to have kept track of like all my previous information as well", "tokens": [50652, 286, 643, 281, 362, 4305, 2837, 295, 411, 439, 452, 3894, 1589, 382, 731, 50780], "temperature": 0.0, "avg_logprob": -0.17882629235585532, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.000852749974001199}, {"id": 372, "seek": 161570, "start": 1624.5800000000002, "end": 1630.1000000000001, "text": " Um, so that's kind of why we think about often want to think about like partially observable environments as opposed to fully observable ones", "tokens": [50808, 3301, 11, 370, 300, 311, 733, 295, 983, 321, 519, 466, 2049, 528, 281, 519, 466, 411, 18886, 9951, 712, 12388, 382, 8851, 281, 4498, 9951, 712, 2306, 51084], "temperature": 0.0, "avg_logprob": -0.17882629235585532, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.000852749974001199}, {"id": 373, "seek": 161570, "start": 1630.3400000000001, "end": 1635.14, "text": " Amazing amazing. So so minci, maybe you can um bring in this this latent idea", "tokens": [51096, 14165, 2243, 13, 407, 370, 275, 21961, 11, 1310, 291, 393, 1105, 1565, 294, 341, 341, 48994, 1558, 51336], "temperature": 0.0, "avg_logprob": -0.17882629235585532, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.000852749974001199}, {"id": 374, "seek": 161570, "start": 1635.8600000000001, "end": 1638.66, "text": " And and sort of contrast that to what lacuna is doing as well", "tokens": [51372, 400, 293, 1333, 295, 8712, 300, 281, 437, 28027, 5051, 307, 884, 382, 731, 51512], "temperature": 0.0, "avg_logprob": -0.17882629235585532, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.000852749974001199}, {"id": 375, "seek": 161570, "start": 1639.06, "end": 1644.74, "text": " Sure, I mean, so I think in machine learning and deep learning, uh, there's this general paradigm that's been around", "tokens": [51532, 4894, 11, 286, 914, 11, 370, 286, 519, 294, 3479, 2539, 293, 2452, 2539, 11, 2232, 11, 456, 311, 341, 2674, 24709, 300, 311, 668, 926, 51816], "temperature": 0.0, "avg_logprob": -0.17882629235585532, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.000852749974001199}, {"id": 376, "seek": 164474, "start": 1644.98, "end": 1646.98, "text": " You know since the inception which is learning", "tokens": [50376, 509, 458, 1670, 264, 49834, 597, 307, 2539, 50476], "temperature": 0.0, "avg_logprob": -0.14263049375663683, "compression_ratio": 1.9885931558935361, "no_speech_prob": 0.0038237550761550665}, {"id": 377, "seek": 164474, "start": 1647.94, "end": 1653.54, "text": " latent latent representations of data and one of the benefits of learning latent representation is that", "tokens": [50524, 48994, 48994, 33358, 295, 1412, 293, 472, 295, 264, 5311, 295, 2539, 48994, 10290, 307, 300, 50804], "temperature": 0.0, "avg_logprob": -0.14263049375663683, "compression_ratio": 1.9885931558935361, "no_speech_prob": 0.0038237550761550665}, {"id": 378, "seek": 164474, "start": 1654.34, "end": 1658.66, "text": " You know, ideally your objective, uh, that leads to learning these latent representations", "tokens": [50844, 509, 458, 11, 22915, 428, 10024, 11, 2232, 11, 300, 6689, 281, 2539, 613, 48994, 33358, 51060], "temperature": 0.0, "avg_logprob": -0.14263049375663683, "compression_ratio": 1.9885931558935361, "no_speech_prob": 0.0038237550761550665}, {"id": 379, "seek": 164474, "start": 1658.82, "end": 1665.54, "text": " Is that you are ultimately learning a lower dimensional representation of the data or dynamics that you're modeling like in our case with the world model", "tokens": [51068, 1119, 300, 291, 366, 6284, 2539, 257, 3126, 18795, 10290, 295, 264, 1412, 420, 15679, 300, 291, 434, 15983, 411, 294, 527, 1389, 365, 264, 1002, 2316, 51404], "temperature": 0.0, "avg_logprob": -0.14263049375663683, "compression_ratio": 1.9885931558935361, "no_speech_prob": 0.0038237550761550665}, {"id": 380, "seek": 164474, "start": 1665.86, "end": 1672.9, "text": " Um, that captures just what is necessary. It's a more compact representation of just the information that's necessary to predict", "tokens": [51420, 3301, 11, 300, 27986, 445, 437, 307, 4818, 13, 467, 311, 257, 544, 14679, 10290, 295, 445, 264, 1589, 300, 311, 4818, 281, 6069, 51772], "temperature": 0.0, "avg_logprob": -0.14263049375663683, "compression_ratio": 1.9885931558935361, "no_speech_prob": 0.0038237550761550665}, {"id": 381, "seek": 167290, "start": 1673.3000000000002, "end": 1677.8600000000001, "text": " The task you're trying to predict and so um with uh with our case", "tokens": [50384, 440, 5633, 291, 434, 1382, 281, 6069, 293, 370, 1105, 365, 2232, 365, 527, 1389, 50612], "temperature": 0.0, "avg_logprob": -0.1347333095112785, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.017706917598843575}, {"id": 382, "seek": 167290, "start": 1678.3400000000001, "end": 1681.94, "text": " Or latent space world models a lot of the benefit of working in the latent space", "tokens": [50636, 1610, 48994, 1901, 1002, 5245, 257, 688, 295, 264, 5121, 295, 1364, 294, 264, 48994, 1901, 50816], "temperature": 0.0, "avg_logprob": -0.1347333095112785, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.017706917598843575}, {"id": 383, "seek": 167290, "start": 1682.02, "end": 1684.98, "text": " Is that if as opposed to working in the full image space?", "tokens": [50820, 1119, 300, 498, 382, 8851, 281, 1364, 294, 264, 1577, 3256, 1901, 30, 50968], "temperature": 0.0, "avg_logprob": -0.1347333095112785, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.017706917598843575}, {"id": 384, "seek": 167290, "start": 1685.0600000000002, "end": 1685.5400000000002, "text": " for example", "tokens": [50972, 337, 1365, 50996], "temperature": 0.0, "avg_logprob": -0.1347333095112785, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.017706917598843575}, {"id": 385, "seek": 167290, "start": 1685.5400000000002, "end": 1690.5800000000002, "text": " If your observations are images like in a video game is that there could be a lot of spurious features", "tokens": [50996, 759, 428, 18163, 366, 5267, 411, 294, 257, 960, 1216, 307, 300, 456, 727, 312, 257, 688, 295, 637, 24274, 4122, 51248], "temperature": 0.0, "avg_logprob": -0.1347333095112785, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.017706917598843575}, {"id": 386, "seek": 167290, "start": 1690.8200000000002, "end": 1697.22, "text": " Or you know a lot of additional information that you could be expending lots of compute and um, you know gradient updates", "tokens": [51260, 1610, 291, 458, 257, 688, 295, 4497, 1589, 300, 291, 727, 312, 1278, 2029, 3195, 295, 14722, 293, 1105, 11, 291, 458, 16235, 9205, 51580], "temperature": 0.0, "avg_logprob": -0.1347333095112785, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.017706917598843575}, {"id": 387, "seek": 167290, "start": 1697.46, "end": 1702.8200000000002, "text": " Just to learn those patterns when they don't actually impact the ultimate um transition dynamics", "tokens": [51592, 1449, 281, 1466, 729, 8294, 562, 436, 500, 380, 767, 2712, 264, 9705, 1105, 6034, 15679, 51860], "temperature": 0.0, "avg_logprob": -0.1347333095112785, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.017706917598843575}, {"id": 388, "seek": 170290, "start": 1702.9, "end": 1706.74, "text": " Or reward dynamics that you need to learn in order to do well in that environment", "tokens": [50364, 1610, 7782, 15679, 300, 291, 643, 281, 1466, 294, 1668, 281, 360, 731, 294, 300, 2823, 50556], "temperature": 0.0, "avg_logprob": -0.1189902927564538, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0006262610550038517}, {"id": 389, "seek": 170290, "start": 1706.9, "end": 1710.8200000000002, "text": " So one example is if you have a game where, you know, maybe the background is different", "tokens": [50564, 407, 472, 1365, 307, 498, 291, 362, 257, 1216, 689, 11, 291, 458, 11, 1310, 264, 3678, 307, 819, 50760], "temperature": 0.0, "avg_logprob": -0.1189902927564538, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0006262610550038517}, {"id": 390, "seek": 170290, "start": 1711.14, "end": 1716.8200000000002, "text": " Uh, because it's daytime or nighttime or it's close to sunset. Um, but ultimately, you know, the background", "tokens": [50776, 4019, 11, 570, 309, 311, 31908, 420, 38595, 420, 309, 311, 1998, 281, 20142, 13, 3301, 11, 457, 6284, 11, 291, 458, 11, 264, 3678, 51060], "temperature": 0.0, "avg_logprob": -0.1189902927564538, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0006262610550038517}, {"id": 391, "seek": 170290, "start": 1717.38, "end": 1719.38, "text": " doesn't really impact", "tokens": [51088, 1177, 380, 534, 2712, 51188], "temperature": 0.0, "avg_logprob": -0.1189902927564538, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0006262610550038517}, {"id": 392, "seek": 170290, "start": 1719.46, "end": 1725.14, "text": " How the player moves around in the environment or whether they've reached the end goal of the task and so", "tokens": [51192, 1012, 264, 4256, 6067, 926, 294, 264, 2823, 420, 1968, 436, 600, 6488, 264, 917, 3387, 295, 264, 5633, 293, 370, 51476], "temperature": 0.0, "avg_logprob": -0.1189902927564538, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0006262610550038517}, {"id": 393, "seek": 170290, "start": 1725.7800000000002, "end": 1729.7800000000002, "text": " If you're training a model where it needs to compress a lot of this information", "tokens": [51508, 759, 291, 434, 3097, 257, 2316, 689, 309, 2203, 281, 14778, 257, 688, 295, 341, 1589, 51708], "temperature": 0.0, "avg_logprob": -0.1189902927564538, "compression_ratio": 1.7077464788732395, "no_speech_prob": 0.0006262610550038517}, {"id": 394, "seek": 172978, "start": 1729.86, "end": 1733.54, "text": " First into a smaller dimensional latent vector or latent representation", "tokens": [50368, 2386, 666, 257, 4356, 18795, 48994, 8062, 420, 48994, 10290, 50552], "temperature": 0.0, "avg_logprob": -0.10735873020056522, "compression_ratio": 2.050955414012739, "no_speech_prob": 0.020960325375199318}, {"id": 395, "seek": 172978, "start": 1733.94, "end": 1738.26, "text": " Um, you don't really need you would expect that latent representation not to actually capture", "tokens": [50572, 3301, 11, 291, 500, 380, 534, 643, 291, 576, 2066, 300, 48994, 10290, 406, 281, 767, 7983, 50788], "temperature": 0.0, "avg_logprob": -0.10735873020056522, "compression_ratio": 2.050955414012739, "no_speech_prob": 0.020960325375199318}, {"id": 396, "seek": 172978, "start": 1738.5, "end": 1743.3, "text": " It would start to ignore the background color and it might only capture certain features of the environment that can", "tokens": [50800, 467, 576, 722, 281, 11200, 264, 3678, 2017, 293, 309, 1062, 787, 7983, 1629, 4122, 295, 264, 2823, 300, 393, 51040], "temperature": 0.0, "avg_logprob": -0.10735873020056522, "compression_ratio": 2.050955414012739, "no_speech_prob": 0.020960325375199318}, {"id": 397, "seek": 172978, "start": 1743.86, "end": 1748.26, "text": " Essentially if you were to decode it back out it might only capture certain information about the environment", "tokens": [51068, 23596, 498, 291, 645, 281, 979, 1429, 309, 646, 484, 309, 1062, 787, 7983, 1629, 1589, 466, 264, 2823, 51288], "temperature": 0.0, "avg_logprob": -0.10735873020056522, "compression_ratio": 2.050955414012739, "no_speech_prob": 0.020960325375199318}, {"id": 398, "seek": 172978, "start": 1748.34, "end": 1750.8999999999999, "text": " That's predictive of the actual task that you want to solve", "tokens": [51292, 663, 311, 35521, 295, 264, 3539, 5633, 300, 291, 528, 281, 5039, 51420], "temperature": 0.0, "avg_logprob": -0.10735873020056522, "compression_ratio": 2.050955414012739, "no_speech_prob": 0.020960325375199318}, {"id": 399, "seek": 172978, "start": 1751.3, "end": 1754.5, "text": " Um, so maybe if the task is to say reach a coin at the end of a level", "tokens": [51440, 3301, 11, 370, 1310, 498, 264, 5633, 307, 281, 584, 2524, 257, 11464, 412, 264, 917, 295, 257, 1496, 51600], "temperature": 0.0, "avg_logprob": -0.10735873020056522, "compression_ratio": 2.050955414012739, "no_speech_prob": 0.020960325375199318}, {"id": 400, "seek": 172978, "start": 1754.74, "end": 1759.62, "text": " Then maybe the latent representation would capture the presence of the coin or whether the the proximity of the character", "tokens": [51612, 1396, 1310, 264, 48994, 10290, 576, 7983, 264, 6814, 295, 264, 11464, 420, 1968, 264, 264, 27632, 295, 264, 2517, 51856], "temperature": 0.0, "avg_logprob": -0.10735873020056522, "compression_ratio": 2.050955414012739, "no_speech_prob": 0.020960325375199318}, {"id": 401, "seek": 175978, "start": 1759.78, "end": 1761.78, "text": " You're controlling to the coin", "tokens": [50364, 509, 434, 14905, 281, 264, 11464, 50464], "temperature": 0.0, "avg_logprob": -0.1581569573818109, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.0012445099418982863}, {"id": 402, "seek": 175978, "start": 1762.18, "end": 1763.54, "text": " and so", "tokens": [50484, 293, 370, 50552], "temperature": 0.0, "avg_logprob": -0.1581569573818109, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.0012445099418982863}, {"id": 403, "seek": 175978, "start": 1763.54, "end": 1764.98, "text": " With the jeppa related work", "tokens": [50552, 2022, 264, 1506, 34827, 4077, 589, 50624], "temperature": 0.0, "avg_logprob": -0.1581569573818109, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.0012445099418982863}, {"id": 404, "seek": 175978, "start": 1764.98, "end": 1770.66, "text": " I think a lot of this is also, you know, motivated with this idea where if we can learn a better latent space representation", "tokens": [50624, 286, 519, 257, 688, 295, 341, 307, 611, 11, 291, 458, 11, 14515, 365, 341, 1558, 689, 498, 321, 393, 1466, 257, 1101, 48994, 1901, 10290, 50908], "temperature": 0.0, "avg_logprob": -0.1581569573818109, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.0012445099418982863}, {"id": 405, "seek": 175978, "start": 1770.98, "end": 1774.26, "text": " Um of images or videos or whatever modality we're trying to model", "tokens": [50924, 3301, 295, 5267, 420, 2145, 420, 2035, 1072, 1860, 321, 434, 1382, 281, 2316, 51088], "temperature": 0.0, "avg_logprob": -0.1581569573818109, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.0012445099418982863}, {"id": 406, "seek": 175978, "start": 1774.5, "end": 1779.3799999999999, "text": " Um, it's a much lower dimensional computationally efficient representation. Uh that you can", "tokens": [51100, 3301, 11, 309, 311, 257, 709, 3126, 18795, 24903, 379, 7148, 10290, 13, 4019, 300, 291, 393, 51344], "temperature": 0.0, "avg_logprob": -0.1581569573818109, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.0012445099418982863}, {"id": 407, "seek": 175978, "start": 1780.26, "end": 1783.22, "text": " You can effectively use for downstream tasks. Um", "tokens": [51388, 509, 393, 8659, 764, 337, 30621, 9608, 13, 3301, 51536], "temperature": 0.0, "avg_logprob": -0.1581569573818109, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.0012445099418982863}, {"id": 408, "seek": 175978, "start": 1784.18, "end": 1788.74, "text": " I'm not I'm actually not super familiar with exactly, you know, the the visual jeppa", "tokens": [51584, 286, 478, 406, 286, 478, 767, 406, 1687, 4963, 365, 2293, 11, 291, 458, 11, 264, 264, 5056, 1506, 34827, 51812], "temperature": 0.0, "avg_logprob": -0.1581569573818109, "compression_ratio": 1.6853146853146854, "no_speech_prob": 0.0012445099418982863}, {"id": 409, "seek": 178978, "start": 1790.74, "end": 1796.74, "text": " Objective so you don't think I can say too much about that. Oh, that's okay. Yeah. I mean, but yeah, I mean you pretty much nailed it", "tokens": [50412, 24753, 488, 370, 291, 500, 380, 519, 286, 393, 584, 886, 709, 466, 300, 13, 876, 11, 300, 311, 1392, 13, 865, 13, 286, 914, 11, 457, 1338, 11, 286, 914, 291, 1238, 709, 30790, 309, 50712], "temperature": 0.0, "avg_logprob": -0.14418361949271896, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.003135683946311474}, {"id": 410, "seek": 178978, "start": 1796.98, "end": 1798.34, "text": " so, um, I mean", "tokens": [50724, 370, 11, 1105, 11, 286, 914, 50792], "temperature": 0.0, "avg_logprob": -0.14418361949271896, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.003135683946311474}, {"id": 411, "seek": 178978, "start": 1798.34, "end": 1801.46, "text": " Lacune even gives the example of like, um, you know in self-driving cars", "tokens": [50792, 40113, 2613, 754, 2709, 264, 1365, 295, 411, 11, 1105, 11, 291, 458, 294, 2698, 12, 47094, 5163, 50948], "temperature": 0.0, "avg_logprob": -0.14418361949271896, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.003135683946311474}, {"id": 412, "seek": 178978, "start": 1801.78, "end": 1804.58, "text": " You might not be interested in the leaves on on on the road, you know", "tokens": [50964, 509, 1062, 406, 312, 3102, 294, 264, 5510, 322, 322, 322, 264, 3060, 11, 291, 458, 51104], "temperature": 0.0, "avg_logprob": -0.14418361949271896, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.003135683946311474}, {"id": 413, "seek": 178978, "start": 1804.58, "end": 1807.7, "text": " So like with increasing levels of of nesting you kind of like learn to", "tokens": [51104, 407, 411, 365, 5662, 4358, 295, 295, 297, 8714, 291, 733, 295, 411, 1466, 281, 51260], "temperature": 0.0, "avg_logprob": -0.14418361949271896, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.003135683946311474}, {"id": 414, "seek": 178978, "start": 1808.1, "end": 1811.62, "text": " Ignore the things that are not relevant and focus on the things that that are relevant", "tokens": [51280, 24754, 418, 264, 721, 300, 366, 406, 7340, 293, 1879, 322, 264, 721, 300, 300, 366, 7340, 51456], "temperature": 0.0, "avg_logprob": -0.14418361949271896, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.003135683946311474}, {"id": 415, "seek": 178978, "start": 1812.02, "end": 1817.22, "text": " But we're almost getting to the center of the bulls I hear so intelligence to me is all about model building", "tokens": [51476, 583, 321, 434, 1920, 1242, 281, 264, 3056, 295, 264, 4693, 82, 286, 1568, 370, 7599, 281, 385, 307, 439, 466, 2316, 2390, 51736], "temperature": 0.0, "avg_logprob": -0.14418361949271896, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.003135683946311474}, {"id": 416, "seek": 181722, "start": 1817.38, "end": 1822.18, "text": " And and that's what these abstractions are. They're they're models that kind of are predictive about the thing that that that's relevant", "tokens": [50372, 400, 293, 300, 311, 437, 613, 12649, 626, 366, 13, 814, 434, 436, 434, 5245, 300, 733, 295, 366, 35521, 466, 264, 551, 300, 300, 300, 311, 7340, 50612], "temperature": 0.0, "avg_logprob": -0.12511278699328016, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0024107820354402065}, {"id": 417, "seek": 181722, "start": 1822.26, "end": 1824.9, "text": " and kind of like ignoring what is not relevant and", "tokens": [50616, 293, 733, 295, 411, 26258, 437, 307, 406, 7340, 293, 50748], "temperature": 0.0, "avg_logprob": -0.12511278699328016, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0024107820354402065}, {"id": 418, "seek": 181722, "start": 1825.38, "end": 1830.66, "text": " We build better models when we have a curriculum. I mean apparently this happens in nature as well. Max Bennett", "tokens": [50772, 492, 1322, 1101, 5245, 562, 321, 362, 257, 14302, 13, 286, 914, 7970, 341, 2314, 294, 3687, 382, 731, 13, 7402, 40620, 51036], "temperature": 0.0, "avg_logprob": -0.12511278699328016, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0024107820354402065}, {"id": 419, "seek": 181722, "start": 1830.66, "end": 1834.34, "text": " I was talking to him the other day and he said, you know, our genome doesn't encode all of our skills", "tokens": [51036, 286, 390, 1417, 281, 796, 264, 661, 786, 293, 415, 848, 11, 291, 458, 11, 527, 21953, 1177, 380, 2058, 1429, 439, 295, 527, 3942, 51220], "temperature": 0.0, "avg_logprob": -0.12511278699328016, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0024107820354402065}, {"id": 420, "seek": 181722, "start": 1834.58, "end": 1839.06, "text": " Um explicitly because it would be too inefficient to do so, but they do encode a kind of curriculum", "tokens": [51232, 3301, 20803, 570, 309, 576, 312, 886, 43495, 281, 360, 370, 11, 457, 436, 360, 2058, 1429, 257, 733, 295, 14302, 51456], "temperature": 0.0, "avg_logprob": -0.12511278699328016, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0024107820354402065}, {"id": 421, "seek": 181722, "start": 1839.3, "end": 1843.7, "text": " So we teach babies. Yeah, we babble with babies and we teach babies how to talk and stuff like that", "tokens": [51468, 407, 321, 2924, 10917, 13, 865, 11, 321, 7564, 638, 365, 10917, 293, 321, 2924, 10917, 577, 281, 751, 293, 1507, 411, 300, 51688], "temperature": 0.0, "avg_logprob": -0.12511278699328016, "compression_ratio": 1.7781065088757397, "no_speech_prob": 0.0024107820354402065}, {"id": 422, "seek": 184370, "start": 1843.94, "end": 1849.38, "text": " So so the curricula is is really important and then we're getting to the center of the bull's eye", "tokens": [50376, 407, 370, 264, 13179, 3780, 307, 307, 534, 1021, 293, 550, 321, 434, 1242, 281, 264, 3056, 295, 264, 4693, 311, 3313, 50648], "temperature": 0.0, "avg_logprob": -0.14617219993046351, "compression_ratio": 1.8206106870229009, "no_speech_prob": 0.002390973037108779}, {"id": 423, "seek": 184370, "start": 1849.46, "end": 1851.7, "text": " Which is intelligence in in general now", "tokens": [50652, 3013, 307, 7599, 294, 294, 2674, 586, 50764], "temperature": 0.0, "avg_logprob": -0.14617219993046351, "compression_ratio": 1.8206106870229009, "no_speech_prob": 0.002390973037108779}, {"id": 424, "seek": 184370, "start": 1852.26, "end": 1858.26, "text": " I think Lacune thinks that it's specialized and and what that means is that there are there are motifs", "tokens": [50792, 286, 519, 40113, 2613, 7309, 300, 309, 311, 19813, 293, 293, 437, 300, 1355, 307, 300, 456, 366, 456, 366, 2184, 18290, 51092], "temperature": 0.0, "avg_logprob": -0.14617219993046351, "compression_ratio": 1.8206106870229009, "no_speech_prob": 0.002390973037108779}, {"id": 425, "seek": 184370, "start": 1858.74, "end": 1861.7, "text": " That's statistically generalized and what that means is that", "tokens": [51116, 663, 311, 36478, 44498, 293, 437, 300, 1355, 307, 300, 51264], "temperature": 0.0, "avg_logprob": -0.14617219993046351, "compression_ratio": 1.8206106870229009, "no_speech_prob": 0.002390973037108779}, {"id": 426, "seek": 184370, "start": 1862.5800000000002, "end": 1863.78, "text": " You do need", "tokens": [51308, 509, 360, 643, 51368], "temperature": 0.0, "avg_logprob": -0.14617219993046351, "compression_ratio": 1.8206106870229009, "no_speech_prob": 0.002390973037108779}, {"id": 427, "seek": 184370, "start": 1863.78, "end": 1867.6200000000001, "text": " environments you need to find motifs that are present in", "tokens": [51368, 12388, 291, 643, 281, 915, 2184, 18290, 300, 366, 1974, 294, 51560], "temperature": 0.0, "avg_logprob": -0.14617219993046351, "compression_ratio": 1.8206106870229009, "no_speech_prob": 0.002390973037108779}, {"id": 428, "seek": 184370, "start": 1868.26, "end": 1872.9, "text": " In as many environments as possible and those are the generalizing features. Do would you agree with that?", "tokens": [51592, 682, 382, 867, 12388, 382, 1944, 293, 729, 366, 264, 2674, 3319, 4122, 13, 1144, 576, 291, 3986, 365, 300, 30, 51824], "temperature": 0.0, "avg_logprob": -0.14617219993046351, "compression_ratio": 1.8206106870229009, "no_speech_prob": 0.002390973037108779}, {"id": 429, "seek": 187290, "start": 1873.38, "end": 1875.38, "text": " Yeah, definitely. I think that a lot of um", "tokens": [50388, 865, 11, 2138, 13, 286, 519, 300, 257, 688, 295, 1105, 50488], "temperature": 0.0, "avg_logprob": -0.1334772473289853, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.0005357122281566262}, {"id": 430, "seek": 187290, "start": 1876.02, "end": 1880.26, "text": " So a lot of really powerful machine learning methods, for example, uh are trained in simulation", "tokens": [50520, 407, 257, 688, 295, 534, 4005, 3479, 2539, 7150, 11, 337, 1365, 11, 2232, 366, 8895, 294, 16575, 50732], "temperature": 0.0, "avg_logprob": -0.1334772473289853, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.0005357122281566262}, {"id": 431, "seek": 187290, "start": 1880.66, "end": 1884.5, "text": " And when you're training in simulation, there's a concept in control from control literature", "tokens": [50752, 400, 562, 291, 434, 3097, 294, 16575, 11, 456, 311, 257, 3410, 294, 1969, 490, 1969, 10394, 50944], "temperature": 0.0, "avg_logprob": -0.1334772473289853, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.0005357122281566262}, {"id": 432, "seek": 187290, "start": 1884.9, "end": 1889.46, "text": " Called the sim 2 real gap and essentially this is essentially quantifying a performance difference between", "tokens": [50964, 45001, 264, 1034, 568, 957, 7417, 293, 4476, 341, 307, 4476, 4426, 5489, 257, 3389, 2649, 1296, 51192], "temperature": 0.0, "avg_logprob": -0.1334772473289853, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.0005357122281566262}, {"id": 433, "seek": 187290, "start": 1890.02, "end": 1897.46, "text": " Well, it's quantifying a few things one is just how different is the are the actual physical or other other kinds of dynamics captured by your simulator", "tokens": [51220, 1042, 11, 309, 311, 4426, 5489, 257, 1326, 721, 472, 307, 445, 577, 819, 307, 264, 366, 264, 3539, 4001, 420, 661, 661, 3685, 295, 15679, 11828, 538, 428, 32974, 51592], "temperature": 0.0, "avg_logprob": -0.1334772473289853, "compression_ratio": 1.710801393728223, "no_speech_prob": 0.0005357122281566262}, {"id": 434, "seek": 189746, "start": 1897.54, "end": 1904.18, "text": " Compared to reality. So if you have a physics simulator, how accurate are for example the friction dynamics or different kinds of contact dynamics?", "tokens": [50368, 30539, 281, 4103, 13, 407, 498, 291, 362, 257, 10649, 32974, 11, 577, 8559, 366, 337, 1365, 264, 17710, 15679, 420, 819, 3685, 295, 3385, 15679, 30, 50700], "temperature": 0.0, "avg_logprob": -0.1268250306447347, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.09803156554698944}, {"id": 435, "seek": 189746, "start": 1904.66, "end": 1909.38, "text": " In your robotic simulator compared to those actual dynamics in the real world with a real robot", "tokens": [50724, 682, 428, 30468, 32974, 5347, 281, 729, 3539, 15679, 294, 264, 957, 1002, 365, 257, 957, 7881, 50960], "temperature": 0.0, "avg_logprob": -0.1268250306447347, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.09803156554698944}, {"id": 436, "seek": 189746, "start": 1909.7, "end": 1912.9, "text": " Um, and this also leads to a sim 2 real gap in terms of performance", "tokens": [50976, 3301, 11, 293, 341, 611, 6689, 281, 257, 1034, 568, 957, 7417, 294, 2115, 295, 3389, 51136], "temperature": 0.0, "avg_logprob": -0.1268250306447347, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.09803156554698944}, {"id": 437, "seek": 189746, "start": 1913.06, "end": 1919.46, "text": " So if you train in the simulator, you know, a lot of times what machine learning is really good at is it's really good at learning to exploit", "tokens": [51144, 407, 498, 291, 3847, 294, 264, 32974, 11, 291, 458, 11, 257, 688, 295, 1413, 437, 3479, 2539, 307, 534, 665, 412, 307, 309, 311, 534, 665, 412, 2539, 281, 25924, 51464], "temperature": 0.0, "avg_logprob": -0.1268250306447347, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.09803156554698944}, {"id": 438, "seek": 189746, "start": 1919.6200000000001, "end": 1925.22, "text": " Whatever system you're training this the model in and so it's fairly um common for", "tokens": [51472, 8541, 1185, 291, 434, 3097, 341, 264, 2316, 294, 293, 370, 309, 311, 6457, 1105, 2689, 337, 51752], "temperature": 0.0, "avg_logprob": -0.1268250306447347, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.09803156554698944}, {"id": 439, "seek": 192522, "start": 1925.6200000000001, "end": 1930.9, "text": " You know systems that are models that are trained within a simulator to learn to eventually exploit the simulator", "tokens": [50384, 509, 458, 3652, 300, 366, 5245, 300, 366, 8895, 1951, 257, 32974, 281, 1466, 281, 4728, 25924, 264, 32974, 50648], "temperature": 0.0, "avg_logprob": -0.09585892653264919, "compression_ratio": 1.930232558139535, "no_speech_prob": 0.004608947783708572}, {"id": 440, "seek": 192522, "start": 1931.14, "end": 1933.14, "text": " and so actually like one big area of um", "tokens": [50660, 293, 370, 767, 411, 472, 955, 1859, 295, 1105, 50760], "temperature": 0.0, "avg_logprob": -0.09585892653264919, "compression_ratio": 1.930232558139535, "no_speech_prob": 0.004608947783708572}, {"id": 441, "seek": 192522, "start": 1933.3, "end": 1938.42, "text": " Games ai is using is actually leveraging this idea where they essentially use ml models", "tokens": [50768, 12761, 9783, 307, 1228, 307, 767, 32666, 341, 1558, 689, 436, 4476, 764, 23271, 5245, 51024], "temperature": 0.0, "avg_logprob": -0.09585892653264919, "compression_ratio": 1.930232558139535, "no_speech_prob": 0.004608947783708572}, {"id": 442, "seek": 192522, "start": 1938.58, "end": 1945.06, "text": " They optimize ml models to within a certain game environment to try to find bugs within that environment to look for exploits automatically", "tokens": [51032, 814, 19719, 23271, 5245, 281, 1951, 257, 1629, 1216, 2823, 281, 853, 281, 915, 15120, 1951, 300, 2823, 281, 574, 337, 12382, 1208, 6772, 51356], "temperature": 0.0, "avg_logprob": -0.09585892653264919, "compression_ratio": 1.930232558139535, "no_speech_prob": 0.004608947783708572}, {"id": 443, "seek": 192522, "start": 1945.3, "end": 1948.5, "text": " Um, so ml systems are very good at finding exploits in whatever system you have", "tokens": [51368, 3301, 11, 370, 23271, 3652, 366, 588, 665, 412, 5006, 12382, 1208, 294, 2035, 1185, 291, 362, 51528], "temperature": 0.0, "avg_logprob": -0.09585892653264919, "compression_ratio": 1.930232558139535, "no_speech_prob": 0.004608947783708572}, {"id": 444, "seek": 192522, "start": 1948.74, "end": 1954.58, "text": " But then the issue is those exploits are usually where exactly where the gap between your simulator and reality resides", "tokens": [51540, 583, 550, 264, 2734, 307, 729, 12382, 1208, 366, 2673, 689, 2293, 689, 264, 7417, 1296, 428, 32974, 293, 4103, 47157, 51832], "temperature": 0.0, "avg_logprob": -0.09585892653264919, "compression_ratio": 1.930232558139535, "no_speech_prob": 0.004608947783708572}, {"id": 445, "seek": 195458, "start": 1954.82, "end": 1960.98, "text": " And so you actually don't want your model to learn to exploit these differences between the simulator and reality to get a high performance", "tokens": [50376, 400, 370, 291, 767, 500, 380, 528, 428, 2316, 281, 1466, 281, 25924, 613, 7300, 1296, 264, 32974, 293, 4103, 281, 483, 257, 1090, 3389, 50684], "temperature": 0.0, "avg_logprob": -0.11419994742782028, "compression_ratio": 1.8241758241758241, "no_speech_prob": 0.0012407852336764336}, {"id": 446, "seek": 195458, "start": 1961.3, "end": 1965.22, "text": " Uh, because that kind of defeats the purpose of then later transferring your model", "tokens": [50700, 4019, 11, 570, 300, 733, 295, 7486, 1720, 264, 4334, 295, 550, 1780, 31437, 428, 2316, 50896], "temperature": 0.0, "avg_logprob": -0.11419994742782028, "compression_ratio": 1.8241758241758241, "no_speech_prob": 0.0012407852336764336}, {"id": 447, "seek": 195458, "start": 1965.54, "end": 1972.5, "text": " That's trained in simulation to reality because now in reality, obviously the model can't exploit those same those same glitches within the simulator", "tokens": [50912, 663, 311, 8895, 294, 16575, 281, 4103, 570, 586, 294, 4103, 11, 2745, 264, 2316, 393, 380, 25924, 729, 912, 729, 912, 23552, 279, 1951, 264, 32974, 51260], "temperature": 0.0, "avg_logprob": -0.11419994742782028, "compression_ratio": 1.8241758241758241, "no_speech_prob": 0.0012407852336764336}, {"id": 448, "seek": 195458, "start": 1972.8999999999999, "end": 1974.8999999999999, "text": " Um, yeah, so yeah", "tokens": [51280, 3301, 11, 1338, 11, 370, 1338, 51380], "temperature": 0.0, "avg_logprob": -0.11419994742782028, "compression_ratio": 1.8241758241758241, "no_speech_prob": 0.0012407852336764336}, {"id": 449, "seek": 195458, "start": 1974.8999999999999, "end": 1980.1799999999998, "text": " Yeah, I mean because the reason this is really interesting is is that the the premise of your paper is that", "tokens": [51380, 865, 11, 286, 914, 570, 264, 1778, 341, 307, 534, 1880, 307, 307, 300, 264, 264, 22045, 295, 428, 3035, 307, 300, 51644], "temperature": 0.0, "avg_logprob": -0.11419994742782028, "compression_ratio": 1.8241758241758241, "no_speech_prob": 0.0012407852336764336}, {"id": 450, "seek": 198018, "start": 1980.9, "end": 1984.02, "text": " It is possible to build a generalist agent", "tokens": [50400, 467, 307, 1944, 281, 1322, 257, 2674, 468, 9461, 50556], "temperature": 0.0, "avg_logprob": -0.11567071219471013, "compression_ratio": 1.6763636363636363, "no_speech_prob": 0.027264639735221863}, {"id": 451, "seek": 198018, "start": 1984.42, "end": 1989.94, "text": " Which means it's an agent that can be fine tuned and work really well on a whole bunch of downstream tasks", "tokens": [50576, 3013, 1355, 309, 311, 364, 9461, 300, 393, 312, 2489, 10870, 293, 589, 534, 731, 322, 257, 1379, 3840, 295, 30621, 9608, 50852], "temperature": 0.0, "avg_logprob": -0.11567071219471013, "compression_ratio": 1.6763636363636363, "no_speech_prob": 0.027264639735221863}, {"id": 452, "seek": 198018, "start": 1990.26, "end": 1995.3, "text": " And to me that implies that at least in our physical world in any situation", "tokens": [50868, 400, 281, 385, 300, 18779, 300, 412, 1935, 294, 527, 4001, 1002, 294, 604, 2590, 51120], "temperature": 0.0, "avg_logprob": -0.11567071219471013, "compression_ratio": 1.6763636363636363, "no_speech_prob": 0.027264639735221863}, {"id": 453, "seek": 198018, "start": 1995.38, "end": 2001.78, "text": " You might use this agent that there are general motifs that it could have learned during free training that it could like, you know", "tokens": [51124, 509, 1062, 764, 341, 9461, 300, 456, 366, 2674, 2184, 18290, 300, 309, 727, 362, 3264, 1830, 1737, 3097, 300, 309, 727, 411, 11, 291, 458, 51444], "temperature": 0.0, "avg_logprob": -0.11567071219471013, "compression_ratio": 1.6763636363636363, "no_speech_prob": 0.027264639735221863}, {"id": 454, "seek": 198018, "start": 2002.02, "end": 2008.8200000000002, "text": " Become activated in any situation. Um, does that is that fair? Yeah, maybe I can say something about um", "tokens": [51456, 44308, 18157, 294, 604, 2590, 13, 3301, 11, 775, 300, 307, 300, 3143, 30, 865, 11, 1310, 286, 393, 584, 746, 466, 1105, 51796], "temperature": 0.0, "avg_logprob": -0.11567071219471013, "compression_ratio": 1.6763636363636363, "no_speech_prob": 0.027264639735221863}, {"id": 455, "seek": 200882, "start": 2009.1399999999999, "end": 2012.8999999999999, "text": " Just the way that we should could think about like the different like latent dynamic subjective", "tokens": [50380, 1449, 264, 636, 300, 321, 820, 727, 519, 466, 411, 264, 819, 411, 48994, 8546, 25972, 50568], "temperature": 0.0, "avg_logprob": -0.08404861450195313, "compression_ratio": 2.0090909090909093, "no_speech_prob": 0.0069008320569992065}, {"id": 456, "seek": 200882, "start": 2012.98, "end": 2017.54, "text": " So so I think I agree that like at least when I try and think about how I think or how people think", "tokens": [50572, 407, 370, 286, 519, 286, 3986, 300, 411, 412, 1935, 562, 286, 853, 293, 519, 466, 577, 286, 519, 420, 577, 561, 519, 50800], "temperature": 0.0, "avg_logprob": -0.08404861450195313, "compression_ratio": 2.0090909090909093, "no_speech_prob": 0.0069008320569992065}, {"id": 457, "seek": 200882, "start": 2017.62, "end": 2018.82, "text": " I think I agree that like", "tokens": [50804, 286, 519, 286, 3986, 300, 411, 50864], "temperature": 0.0, "avg_logprob": -0.08404861450195313, "compression_ratio": 2.0090909090909093, "no_speech_prob": 0.0069008320569992065}, {"id": 458, "seek": 200882, "start": 2018.82, "end": 2023.3799999999999, "text": " You know a truly intelligent system should kind of think through the world and like a very compressed representation of the world", "tokens": [50864, 509, 458, 257, 4908, 13232, 1185, 820, 733, 295, 519, 807, 264, 1002, 293, 411, 257, 588, 30353, 10290, 295, 264, 1002, 51092], "temperature": 0.0, "avg_logprob": -0.08404861450195313, "compression_ratio": 2.0090909090909093, "no_speech_prob": 0.0069008320569992065}, {"id": 459, "seek": 200882, "start": 2023.3799999999999, "end": 2025.9399999999998, "text": " Like if I'm trying to like think through how to go to the airport", "tokens": [51092, 1743, 498, 286, 478, 1382, 281, 411, 519, 807, 577, 281, 352, 281, 264, 10155, 51220], "temperature": 0.0, "avg_logprob": -0.08404861450195313, "compression_ratio": 2.0090909090909093, "no_speech_prob": 0.0069008320569992065}, {"id": 460, "seek": 200882, "start": 2026.02, "end": 2031.1399999999999, "text": " Like I'm definitely not like predicting ahead in terms of like the raw image space of trying to predict every image", "tokens": [51224, 1743, 286, 478, 2138, 406, 411, 32884, 2286, 294, 2115, 295, 411, 264, 8936, 3256, 1901, 295, 1382, 281, 6069, 633, 3256, 51480], "temperature": 0.0, "avg_logprob": -0.08404861450195313, "compression_ratio": 2.0090909090909093, "no_speech_prob": 0.0069008320569992065}, {"id": 461, "seek": 200882, "start": 2031.1399999999999, "end": 2035.7, "text": " I might observe on the way the airport and things like this. And so I think we have this kind of like trade-off between, you know", "tokens": [51480, 286, 1062, 11441, 322, 264, 636, 264, 10155, 293, 721, 411, 341, 13, 400, 370, 286, 519, 321, 362, 341, 733, 295, 411, 4923, 12, 4506, 1296, 11, 291, 458, 51708], "temperature": 0.0, "avg_logprob": -0.08404861450195313, "compression_ratio": 2.0090909090909093, "no_speech_prob": 0.0069008320569992065}, {"id": 462, "seek": 203570, "start": 2036.5, "end": 2040.02, "text": " Um, like we said with the bgf of paper like should should be just try and like", "tokens": [50404, 3301, 11, 411, 321, 848, 365, 264, 272, 70, 69, 295, 3035, 411, 820, 820, 312, 445, 853, 293, 411, 50580], "temperature": 0.0, "avg_logprob": -0.17137484057196256, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0026309837121516466}, {"id": 463, "seek": 203570, "start": 2040.5, "end": 2043.94, "text": " Kind of basically model like the minimum information we need about the world to try and you know", "tokens": [50604, 9242, 295, 1936, 2316, 411, 264, 7285, 1589, 321, 643, 466, 264, 1002, 281, 853, 293, 291, 458, 50776], "temperature": 0.0, "avg_logprob": -0.17137484057196256, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0026309837121516466}, {"id": 464, "seek": 203570, "start": 2044.02, "end": 2048.34, "text": " Do the do the relevant task in the world? I think what you're saying. I think that probably is", "tokens": [50780, 1144, 264, 360, 264, 7340, 5633, 294, 264, 1002, 30, 286, 519, 437, 291, 434, 1566, 13, 286, 519, 300, 1391, 307, 50996], "temperature": 0.0, "avg_logprob": -0.17137484057196256, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0026309837121516466}, {"id": 465, "seek": 203570, "start": 2049.14, "end": 2052.5, "text": " Maybe more what we think about when we think about like human intelligence or something like that", "tokens": [51036, 2704, 544, 437, 321, 519, 466, 562, 321, 519, 466, 411, 1952, 7599, 420, 746, 411, 300, 51204], "temperature": 0.0, "avg_logprob": -0.17137484057196256, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0026309837121516466}, {"id": 466, "seek": 203570, "start": 2053.06, "end": 2055.78, "text": " Um, but then there's also this other way where we just say we're going to just like", "tokens": [51232, 3301, 11, 457, 550, 456, 311, 611, 341, 661, 636, 689, 321, 445, 584, 321, 434, 516, 281, 445, 411, 51368], "temperature": 0.0, "avg_logprob": -0.17137484057196256, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0026309837121516466}, {"id": 467, "seek": 203570, "start": 2056.5, "end": 2058.82, "text": " Enforce the model to be able to predict ahead every single image", "tokens": [51404, 2193, 5156, 264, 2316, 281, 312, 1075, 281, 6069, 2286, 633, 2167, 3256, 51520], "temperature": 0.0, "avg_logprob": -0.17137484057196256, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0026309837121516466}, {"id": 468, "seek": 203570, "start": 2059.2200000000003, "end": 2063.14, "text": " And so in our paper, we do actually enforce that the model has to predict the next image", "tokens": [51540, 400, 370, 294, 527, 3035, 11, 321, 360, 767, 24825, 300, 264, 2316, 575, 281, 6069, 264, 958, 3256, 51736], "temperature": 0.0, "avg_logprob": -0.17137484057196256, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0026309837121516466}, {"id": 469, "seek": 206314, "start": 2064.02, "end": 2066.02, "text": " um, and so um", "tokens": [50408, 1105, 11, 293, 370, 1105, 50508], "temperature": 0.0, "avg_logprob": -0.12444689520474138, "compression_ratio": 1.894578313253012, "no_speech_prob": 0.0014548798790201545}, {"id": 470, "seek": 206314, "start": 2066.3399999999997, "end": 2068.74, "text": " Basically what this might mean is yeah, like maybe the model does", "tokens": [50524, 8537, 437, 341, 1062, 914, 307, 1338, 11, 411, 1310, 264, 2316, 775, 50644], "temperature": 0.0, "avg_logprob": -0.12444689520474138, "compression_ratio": 1.894578313253012, "no_speech_prob": 0.0014548798790201545}, {"id": 471, "seek": 206314, "start": 2069.2999999999997, "end": 2074.58, "text": " You know, hopefully it does like like you said like kind of capture the underlying like true things that matter in the environment", "tokens": [50672, 509, 458, 11, 4696, 309, 775, 411, 411, 291, 848, 411, 733, 295, 7983, 264, 14217, 411, 2074, 721, 300, 1871, 294, 264, 2823, 50936], "temperature": 0.0, "avg_logprob": -0.12444689520474138, "compression_ratio": 1.894578313253012, "no_speech_prob": 0.0014548798790201545}, {"id": 472, "seek": 206314, "start": 2074.8199999999997, "end": 2077.3799999999997, "text": " But it might also mean like what we're saying with like the leaves example", "tokens": [50948, 583, 309, 1062, 611, 914, 411, 437, 321, 434, 1566, 365, 411, 264, 5510, 1365, 51076], "temperature": 0.0, "avg_logprob": -0.12444689520474138, "compression_ratio": 1.894578313253012, "no_speech_prob": 0.0014548798790201545}, {"id": 473, "seek": 206314, "start": 2077.3799999999997, "end": 2080.58, "text": " Like this might force the model to kind of capture a lot of irrelevant details", "tokens": [51076, 1743, 341, 1062, 3464, 264, 2316, 281, 733, 295, 7983, 257, 688, 295, 28682, 4365, 51236], "temperature": 0.0, "avg_logprob": -0.12444689520474138, "compression_ratio": 1.894578313253012, "no_speech_prob": 0.0014548798790201545}, {"id": 474, "seek": 206314, "start": 2080.58, "end": 2082.8199999999997, "text": " That don't really matter like the leaves on the ground and things like this", "tokens": [51236, 663, 500, 380, 534, 1871, 411, 264, 5510, 322, 264, 2727, 293, 721, 411, 341, 51348], "temperature": 0.0, "avg_logprob": -0.12444689520474138, "compression_ratio": 1.894578313253012, "no_speech_prob": 0.0014548798790201545}, {"id": 475, "seek": 206314, "start": 2083.22, "end": 2086.1, "text": " And so, you know, maybe that means it isn't actually capturing the underlying motifs", "tokens": [51368, 400, 370, 11, 291, 458, 11, 1310, 300, 1355, 309, 1943, 380, 767, 23384, 264, 14217, 2184, 18290, 51512], "temperature": 0.0, "avg_logprob": -0.12444689520474138, "compression_ratio": 1.894578313253012, "no_speech_prob": 0.0014548798790201545}, {"id": 476, "seek": 206314, "start": 2086.1, "end": 2090.18, "text": " It's actually just getting good at image generation. Um, but then I've or image prediction I should say", "tokens": [51512, 467, 311, 767, 445, 1242, 665, 412, 3256, 5125, 13, 3301, 11, 457, 550, 286, 600, 420, 3256, 17630, 286, 820, 584, 51716], "temperature": 0.0, "avg_logprob": -0.12444689520474138, "compression_ratio": 1.894578313253012, "no_speech_prob": 0.0014548798790201545}, {"id": 477, "seek": 209018, "start": 2090.98, "end": 2095.94, "text": " Um, but then I've also heard arguments kind of saying, you know, so what if people don't really think in terms of like image prediction", "tokens": [50404, 3301, 11, 457, 550, 286, 600, 611, 2198, 12869, 733, 295, 1566, 11, 291, 458, 11, 370, 437, 498, 561, 500, 380, 534, 519, 294, 2115, 295, 411, 3256, 17630, 50652], "temperature": 0.0, "avg_logprob": -0.09999411361022566, "compression_ratio": 2.0088235294117647, "no_speech_prob": 0.0017003518296405673}, {"id": 478, "seek": 209018, "start": 2096.1, "end": 2098.98, "text": " You know, I you know, we think in terms of like more like these high level motifs", "tokens": [50660, 509, 458, 11, 286, 291, 458, 11, 321, 519, 294, 2115, 295, 411, 544, 411, 613, 1090, 1496, 2184, 18290, 50804], "temperature": 0.0, "avg_logprob": -0.09999411361022566, "compression_ratio": 2.0088235294117647, "no_speech_prob": 0.0017003518296405673}, {"id": 479, "seek": 209018, "start": 2099.2999999999997, "end": 2101.3799999999997, "text": " But people have other people would argue that you know", "tokens": [50820, 583, 561, 362, 661, 561, 576, 9695, 300, 291, 458, 50924], "temperature": 0.0, "avg_logprob": -0.09999411361022566, "compression_ratio": 2.0088235294117647, "no_speech_prob": 0.0017003518296405673}, {"id": 480, "seek": 209018, "start": 2101.62, "end": 2104.66, "text": " Kind of the machine learning machinery is there to do really good image prediction", "tokens": [50936, 9242, 295, 264, 3479, 2539, 27302, 307, 456, 281, 360, 534, 665, 3256, 17630, 51088], "temperature": 0.0, "avg_logprob": -0.09999411361022566, "compression_ratio": 2.0088235294117647, "no_speech_prob": 0.0017003518296405673}, {"id": 481, "seek": 209018, "start": 2105.06, "end": 2109.06, "text": " So so if if we if we can get a model that can actually just like predict images ahead really well", "tokens": [51108, 407, 370, 498, 498, 321, 498, 321, 393, 483, 257, 2316, 300, 393, 767, 445, 411, 6069, 5267, 2286, 534, 731, 51308], "temperature": 0.0, "avg_logprob": -0.09999411361022566, "compression_ratio": 2.0088235294117647, "no_speech_prob": 0.0017003518296405673}, {"id": 482, "seek": 209018, "start": 2109.54, "end": 2112.8999999999996, "text": " Um, and not really worry so much about whether it's reasoning about these like high level features", "tokens": [51332, 3301, 11, 293, 406, 534, 3292, 370, 709, 466, 1968, 309, 311, 21577, 466, 613, 411, 1090, 1496, 4122, 51500], "temperature": 0.0, "avg_logprob": -0.09999411361022566, "compression_ratio": 2.0088235294117647, "no_speech_prob": 0.0017003518296405673}, {"id": 483, "seek": 209018, "start": 2113.22, "end": 2117.94, "text": " You know, if you can predict images ahead really well, you know, that's enough to make to do good decision making a lot of context", "tokens": [51516, 509, 458, 11, 498, 291, 393, 6069, 5267, 2286, 534, 731, 11, 291, 458, 11, 300, 311, 1547, 281, 652, 281, 360, 665, 3537, 1455, 257, 688, 295, 4319, 51752], "temperature": 0.0, "avg_logprob": -0.09999411361022566, "compression_ratio": 2.0088235294117647, "no_speech_prob": 0.0017003518296405673}, {"id": 484, "seek": 211794, "start": 2118.18, "end": 2119.94, "text": " So I think there's this kind of like", "tokens": [50376, 407, 286, 519, 456, 311, 341, 733, 295, 411, 50464], "temperature": 0.0, "avg_logprob": -0.10273789104662444, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0017502402188256383}, {"id": 485, "seek": 211794, "start": 2119.94, "end": 2122.7400000000002, "text": " Contrasting ways of thinking about, you know, image prediction is good enough", "tokens": [50464, 4839, 4148, 278, 2098, 295, 1953, 466, 11, 291, 458, 11, 3256, 17630, 307, 665, 1547, 50604], "temperature": 0.0, "avg_logprob": -0.10273789104662444, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0017502402188256383}, {"id": 486, "seek": 211794, "start": 2122.7400000000002, "end": 2126.58, "text": " We'll just predict like really visually good scenes and that will be good enough for decision making", "tokens": [50604, 492, 603, 445, 6069, 411, 534, 19622, 665, 8026, 293, 300, 486, 312, 665, 1547, 337, 3537, 1455, 50796], "temperature": 0.0, "avg_logprob": -0.10273789104662444, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0017502402188256383}, {"id": 487, "seek": 211794, "start": 2126.9, "end": 2130.34, "text": " Or do we want to force the model to try and reason about like more abstract features of the environment?", "tokens": [50812, 1610, 360, 321, 528, 281, 3464, 264, 2316, 281, 853, 293, 1778, 466, 411, 544, 12649, 4122, 295, 264, 2823, 30, 50984], "temperature": 0.0, "avg_logprob": -0.10273789104662444, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0017502402188256383}, {"id": 488, "seek": 211794, "start": 2130.34, "end": 2135.38, "text": " And that's kind of a more intelligent way of reasoning about the world. Um, and yeah, I think that's a very interesting trade-off", "tokens": [50984, 400, 300, 311, 733, 295, 257, 544, 13232, 636, 295, 21577, 466, 264, 1002, 13, 3301, 11, 293, 1338, 11, 286, 519, 300, 311, 257, 588, 1880, 4923, 12, 4506, 51236], "temperature": 0.0, "avg_logprob": -0.10273789104662444, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0017502402188256383}, {"id": 489, "seek": 211794, "start": 2136.42, "end": 2138.42, "text": " Yeah, yeah, I mean like it's um", "tokens": [51288, 865, 11, 1338, 11, 286, 914, 411, 309, 311, 1105, 51388], "temperature": 0.0, "avg_logprob": -0.10273789104662444, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0017502402188256383}, {"id": 490, "seek": 211794, "start": 2139.14, "end": 2141.2200000000003, "text": " Like the biggest problem in machine learning is overfitting, you know", "tokens": [51424, 1743, 264, 3880, 1154, 294, 3479, 2539, 307, 670, 69, 2414, 11, 291, 458, 51528], "temperature": 0.0, "avg_logprob": -0.10273789104662444, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0017502402188256383}, {"id": 491, "seek": 211794, "start": 2141.2200000000003, "end": 2143.94, "text": " So as you say like that, there are all of these statistically generalizing features", "tokens": [51528, 407, 382, 291, 584, 411, 300, 11, 456, 366, 439, 295, 613, 36478, 2674, 3319, 4122, 51664], "temperature": 0.0, "avg_logprob": -0.10273789104662444, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0017502402188256383}, {"id": 492, "seek": 214394, "start": 2143.94, "end": 2148.1, "text": " But they generalize within the domain and the domain might be like your your simulator or like, you know", "tokens": [50364, 583, 436, 2674, 1125, 1951, 264, 9274, 293, 264, 9274, 1062, 312, 411, 428, 428, 32974, 420, 411, 11, 291, 458, 50572], "temperature": 0.0, "avg_logprob": -0.13799671359829135, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.016548391431570053}, {"id": 493, "seek": 214394, "start": 2148.1, "end": 2150.82, "text": " How you're training it rather than how it's being used in in production", "tokens": [50572, 1012, 291, 434, 3097, 309, 2831, 813, 577, 309, 311, 885, 1143, 294, 294, 4265, 50708], "temperature": 0.0, "avg_logprob": -0.13799671359829135, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.016548391431570053}, {"id": 494, "seek": 214394, "start": 2151.06, "end": 2155.78, "text": " And then as you say that there's also this um almost human chauvinistic or puritanical view on this which is that well", "tokens": [50720, 400, 550, 382, 291, 584, 300, 456, 311, 611, 341, 1105, 1920, 1952, 417, 1459, 4796, 3142, 420, 1864, 9670, 804, 1910, 322, 341, 597, 307, 300, 731, 50956], "temperature": 0.0, "avg_logprob": -0.13799671359829135, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.016548391431570053}, {"id": 495, "seek": 214394, "start": 2156.34, "end": 2160.42, "text": " You know, it does the right thing for the wrong reasons or I use different motifs to do the reasoning", "tokens": [50984, 509, 458, 11, 309, 775, 264, 558, 551, 337, 264, 2085, 4112, 420, 286, 764, 819, 2184, 18290, 281, 360, 264, 21577, 51188], "temperature": 0.0, "avg_logprob": -0.13799671359829135, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.016548391431570053}, {"id": 496, "seek": 214394, "start": 2160.42, "end": 2162.9, "text": " So that thing must be doing it wrong. Do you know what I mean?", "tokens": [51188, 407, 300, 551, 1633, 312, 884, 309, 2085, 13, 1144, 291, 458, 437, 286, 914, 30, 51312], "temperature": 0.0, "avg_logprob": -0.13799671359829135, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.016548391431570053}, {"id": 497, "seek": 214394, "start": 2163.2200000000003, "end": 2167.86, "text": " And um, I was talking with chris bishop at msr the other day and and you know, he's um big on", "tokens": [51328, 400, 1105, 11, 286, 390, 1417, 365, 417, 5714, 34470, 412, 275, 82, 81, 264, 661, 786, 293, 293, 291, 458, 11, 415, 311, 1105, 955, 322, 51560], "temperature": 0.0, "avg_logprob": -0.13799671359829135, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.016548391431570053}, {"id": 498, "seek": 216786, "start": 2168.1, "end": 2173.46, "text": " Symmetries and yeah, you know, the kind of stuff that like max welling and takako hen and bronstein and um", "tokens": [50376, 3902, 2174, 302, 2244, 293, 1338, 11, 291, 458, 11, 264, 733, 295, 1507, 300, 411, 11469, 731, 278, 293, 991, 18501, 22253, 293, 16586, 9089, 293, 1105, 50644], "temperature": 0.0, "avg_logprob": -0.1634686929208261, "compression_ratio": 1.7804054054054055, "no_speech_prob": 0.10038682818412781}, {"id": 499, "seek": 216786, "start": 2173.46, "end": 2175.38, "text": " The deep mind has done loads of cool stuff on on this", "tokens": [50644, 440, 2452, 1575, 575, 1096, 12668, 295, 1627, 1507, 322, 322, 341, 50740], "temperature": 0.0, "avg_logprob": -0.1634686929208261, "compression_ratio": 1.7804054054054055, "no_speech_prob": 0.10038682818412781}, {"id": 500, "seek": 216786, "start": 2175.6200000000003, "end": 2180.9, "text": " But it's this idea that like we know the world um has a certain geometry. It has certain physical priors", "tokens": [50752, 583, 309, 311, 341, 1558, 300, 411, 321, 458, 264, 1002, 1105, 575, 257, 1629, 18426, 13, 467, 575, 1629, 4001, 1790, 830, 51016], "temperature": 0.0, "avg_logprob": -0.1634686929208261, "compression_ratio": 1.7804054054054055, "no_speech_prob": 0.10038682818412781}, {"id": 501, "seek": 216786, "start": 2180.9, "end": 2185.78, "text": " So like we can deliberately um, you know, kind of construct the approximation class in machine learning", "tokens": [51016, 407, 411, 321, 393, 23506, 1105, 11, 291, 458, 11, 733, 295, 7690, 264, 28023, 1508, 294, 3479, 2539, 51260], "temperature": 0.0, "avg_logprob": -0.1634686929208261, "compression_ratio": 1.7804054054054055, "no_speech_prob": 0.10038682818412781}, {"id": 502, "seek": 216786, "start": 2186.26, "end": 2190.58, "text": " Methods so so that like we make it an easier problem, right? Because we because we know we know the thing is in there", "tokens": [51284, 25285, 82, 370, 370, 300, 411, 321, 652, 309, 364, 3571, 1154, 11, 558, 30, 1436, 321, 570, 321, 458, 321, 458, 264, 551, 307, 294, 456, 51500], "temperature": 0.0, "avg_logprob": -0.1634686929208261, "compression_ratio": 1.7804054054054055, "no_speech_prob": 0.10038682818412781}, {"id": 503, "seek": 216786, "start": 2191.7000000000003, "end": 2193.7000000000003, "text": " Yeah, so I mean, I guess sort of the uh", "tokens": [51556, 865, 11, 370, 286, 914, 11, 286, 2041, 1333, 295, 264, 2232, 51656], "temperature": 0.0, "avg_logprob": -0.1634686929208261, "compression_ratio": 1.7804054054054055, "no_speech_prob": 0.10038682818412781}, {"id": 504, "seek": 219370, "start": 2194.2599999999998, "end": 2196.2599999999998, "text": " Slight tangent I went into around the sim to real gap", "tokens": [50392, 318, 2764, 27747, 286, 1437, 666, 926, 264, 1034, 281, 957, 7417, 50492], "temperature": 0.0, "avg_logprob": -0.13729171147422184, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.016910752281546593}, {"id": 505, "seek": 219370, "start": 2196.4199999999996, "end": 2199.54, "text": " I guess part of the point I wanted to make there is that um, you know", "tokens": [50500, 286, 2041, 644, 295, 264, 935, 286, 1415, 281, 652, 456, 307, 300, 1105, 11, 291, 458, 50656], "temperature": 0.0, "avg_logprob": -0.13729171147422184, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.016910752281546593}, {"id": 506, "seek": 219370, "start": 2199.54, "end": 2202.4199999999996, "text": " One way around the sim to real gap is you could try to train", "tokens": [50656, 1485, 636, 926, 264, 1034, 281, 957, 7417, 307, 291, 727, 853, 281, 3847, 50800], "temperature": 0.0, "avg_logprob": -0.13729171147422184, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.016910752281546593}, {"id": 507, "seek": 219370, "start": 2203.06, "end": 2207.06, "text": " You could try to parametrize a very large space of possible versions of reality", "tokens": [50832, 509, 727, 853, 281, 6220, 302, 470, 1381, 257, 588, 2416, 1901, 295, 1944, 9606, 295, 4103, 51032], "temperature": 0.0, "avg_logprob": -0.13729171147422184, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.016910752281546593}, {"id": 508, "seek": 219370, "start": 2207.2999999999997, "end": 2213.3799999999997, "text": " And this is kind of the motivation behind this method of domain randomization where you sort of say this is the you know", "tokens": [51044, 400, 341, 307, 733, 295, 264, 12335, 2261, 341, 3170, 295, 9274, 4974, 2144, 689, 291, 1333, 295, 584, 341, 307, 264, 291, 458, 51348], "temperature": 0.0, "avg_logprob": -0.13729171147422184, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.016910752281546593}, {"id": 509, "seek": 219370, "start": 2213.3799999999997, "end": 2216.66, "text": " This is the specific task domain I care about I can parametrize the different", "tokens": [51348, 639, 307, 264, 2685, 5633, 9274, 286, 1127, 466, 286, 393, 6220, 302, 470, 1381, 264, 819, 51512], "temperature": 0.0, "avg_logprob": -0.13729171147422184, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.016910752281546593}, {"id": 510, "seek": 219370, "start": 2216.98, "end": 2219.3799999999997, "text": " Uh versions of the task with a few parameters", "tokens": [51528, 4019, 9606, 295, 264, 5633, 365, 257, 1326, 9834, 51648], "temperature": 0.0, "avg_logprob": -0.13729171147422184, "compression_ratio": 1.8921933085501859, "no_speech_prob": 0.016910752281546593}, {"id": 511, "seek": 221938, "start": 2219.62, "end": 2226.02, "text": " And I basically want to search over the space of parameters and train my model or my agent on all possible variations of this world", "tokens": [50376, 400, 286, 1936, 528, 281, 3164, 670, 264, 1901, 295, 9834, 293, 3847, 452, 2316, 420, 452, 9461, 322, 439, 1944, 17840, 295, 341, 1002, 50696], "temperature": 0.0, "avg_logprob": -0.11117161644829644, "compression_ratio": 1.6996805111821087, "no_speech_prob": 0.06006426364183426}, {"id": 512, "seek": 221938, "start": 2226.1800000000003, "end": 2230.82, "text": " But obviously that's not very sample efficient because that design space could be huge could be massive", "tokens": [50704, 583, 2745, 300, 311, 406, 588, 6889, 7148, 570, 300, 1715, 1901, 727, 312, 2603, 727, 312, 5994, 50936], "temperature": 0.0, "avg_logprob": -0.11117161644829644, "compression_ratio": 1.6996805111821087, "no_speech_prob": 0.06006426364183426}, {"id": 513, "seek": 221938, "start": 2231.06, "end": 2234.82, "text": " And so instead we like these active sampling strategies like we were talking about earlier", "tokens": [50948, 400, 370, 2602, 321, 411, 613, 4967, 21179, 9029, 411, 321, 645, 1417, 466, 3071, 51136], "temperature": 0.0, "avg_logprob": -0.11117161644829644, "compression_ratio": 1.6996805111821087, "no_speech_prob": 0.06006426364183426}, {"id": 514, "seek": 221938, "start": 2235.38, "end": 2237.38, "text": " around mini max regret style", "tokens": [51164, 926, 8382, 11469, 10879, 3758, 51264], "temperature": 0.0, "avg_logprob": -0.11117161644829644, "compression_ratio": 1.6996805111821087, "no_speech_prob": 0.06006426364183426}, {"id": 515, "seek": 221938, "start": 2237.54, "end": 2242.34, "text": " Active sampling where you sample those environments that maximize your regret or some other type of objective", "tokens": [51272, 26635, 21179, 689, 291, 6889, 729, 12388, 300, 19874, 428, 10879, 420, 512, 661, 2010, 295, 10024, 51512], "temperature": 0.0, "avg_logprob": -0.11117161644829644, "compression_ratio": 1.6996805111821087, "no_speech_prob": 0.06006426364183426}, {"id": 516, "seek": 221938, "start": 2242.42, "end": 2245.94, "text": " Maybe like uncertainty uh similar to what we do in the waker paper", "tokens": [51516, 2704, 411, 15697, 2232, 2531, 281, 437, 321, 360, 294, 264, 261, 4003, 3035, 51692], "temperature": 0.0, "avg_logprob": -0.11117161644829644, "compression_ratio": 1.6996805111821087, "no_speech_prob": 0.06006426364183426}, {"id": 517, "seek": 224594, "start": 2246.5, "end": 2249.86, "text": " But ultimately these things these active sampling process it leads to", "tokens": [50392, 583, 6284, 613, 721, 613, 4967, 21179, 1399, 309, 6689, 281, 50560], "temperature": 0.0, "avg_logprob": -0.10514932870864868, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.013633567839860916}, {"id": 518, "seek": 224594, "start": 2250.66, "end": 2253.3, "text": " What we like to call an auto curriculum automatic curriculum", "tokens": [50600, 708, 321, 411, 281, 818, 364, 8399, 14302, 12509, 14302, 50732], "temperature": 0.0, "avg_logprob": -0.10514932870864868, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.013633567839860916}, {"id": 519, "seek": 224594, "start": 2253.94, "end": 2257.06, "text": " And this is in contrast to prior curriculum learning works because here", "tokens": [50764, 400, 341, 307, 294, 8712, 281, 4059, 14302, 2539, 1985, 570, 510, 50920], "temperature": 0.0, "avg_logprob": -0.10514932870864868, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.013633567839860916}, {"id": 520, "seek": 224594, "start": 2257.38, "end": 2262.5, "text": " This is an automatically generated curriculum. So you you can kind of not have any pre-defined notion of what is", "tokens": [50936, 639, 307, 364, 6772, 10833, 14302, 13, 407, 291, 291, 393, 733, 295, 406, 362, 604, 659, 12, 37716, 10710, 295, 437, 307, 51192], "temperature": 0.0, "avg_logprob": -0.10514932870864868, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.013633567839860916}, {"id": 521, "seek": 224594, "start": 2262.9, "end": 2268.9, "text": " Easy or hard it's purely fixed to what is easier or hard for the model in terms of how good the model is at", "tokens": [51212, 16002, 420, 1152, 309, 311, 17491, 6806, 281, 437, 307, 3571, 420, 1152, 337, 264, 2316, 294, 2115, 295, 577, 665, 264, 2316, 307, 412, 51512], "temperature": 0.0, "avg_logprob": -0.10514932870864868, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.013633567839860916}, {"id": 522, "seek": 224594, "start": 2269.06, "end": 2271.78, "text": " Performing at those tasks. And so it's nice. It's an automatic curriculum", "tokens": [51520, 19351, 278, 412, 729, 9608, 13, 400, 370, 309, 311, 1481, 13, 467, 311, 364, 12509, 14302, 51656], "temperature": 0.0, "avg_logprob": -0.10514932870864868, "compression_ratio": 1.7877697841726619, "no_speech_prob": 0.013633567839860916}, {"id": 523, "seek": 227178, "start": 2271.94, "end": 2276.6600000000003, "text": " So you can think of it as almost like weaving a path through this high-dimensional design space", "tokens": [50372, 407, 291, 393, 519, 295, 309, 382, 1920, 411, 40028, 257, 3100, 807, 341, 1090, 12, 18759, 1715, 1901, 50608], "temperature": 0.0, "avg_logprob": -0.13538343180780826, "compression_ratio": 1.782758620689655, "no_speech_prob": 0.02930588275194168}, {"id": 524, "seek": 227178, "start": 2277.38, "end": 2279.38, "text": " automatically such that if the", "tokens": [50644, 6772, 1270, 300, 498, 264, 50744], "temperature": 0.0, "avg_logprob": -0.13538343180780826, "compression_ratio": 1.782758620689655, "no_speech_prob": 0.02930588275194168}, {"id": 525, "seek": 227178, "start": 2279.38, "end": 2285.6200000000003, "text": " Agent or model were to train on data along this path of environments through its experiences in this path of environments during the training curriculum", "tokens": [50744, 27174, 420, 2316, 645, 281, 3847, 322, 1412, 2051, 341, 3100, 295, 12388, 807, 1080, 5235, 294, 341, 3100, 295, 12388, 1830, 264, 3097, 14302, 51056], "temperature": 0.0, "avg_logprob": -0.13538343180780826, "compression_ratio": 1.782758620689655, "no_speech_prob": 0.02930588275194168}, {"id": 526, "seek": 227178, "start": 2285.86, "end": 2289.0600000000004, "text": " It'll basically be maximizing some sort of information gain objective", "tokens": [51068, 467, 603, 1936, 312, 5138, 3319, 512, 1333, 295, 1589, 6052, 10024, 51228], "temperature": 0.0, "avg_logprob": -0.13538343180780826, "compression_ratio": 1.782758620689655, "no_speech_prob": 0.02930588275194168}, {"id": 527, "seek": 227178, "start": 2289.38, "end": 2290.1000000000004, "text": " um", "tokens": [51244, 1105, 51280], "temperature": 0.0, "avg_logprob": -0.13538343180780826, "compression_ratio": 1.782758620689655, "no_speech_prob": 0.02930588275194168}, {"id": 528, "seek": 227178, "start": 2290.1000000000004, "end": 2294.5, "text": " Because you know, for example regret if there's a high regret that's that means there's a high", "tokens": [51280, 1436, 291, 458, 11, 337, 1365, 10879, 498, 456, 311, 257, 1090, 10879, 300, 311, 300, 1355, 456, 311, 257, 1090, 51500], "temperature": 0.0, "avg_logprob": -0.13538343180780826, "compression_ratio": 1.782758620689655, "no_speech_prob": 0.02930588275194168}, {"id": 529, "seek": 227178, "start": 2295.3, "end": 2298.42, "text": " Ceiling there's a high gap in terms of how much the agent can improve", "tokens": [51540, 8257, 4883, 456, 311, 257, 1090, 7417, 294, 2115, 295, 577, 709, 264, 9461, 393, 3470, 51696], "temperature": 0.0, "avg_logprob": -0.13538343180780826, "compression_ratio": 1.782758620689655, "no_speech_prob": 0.02930588275194168}, {"id": 530, "seek": 229842, "start": 2298.66, "end": 2301.14, "text": " Which implies that there's a lot more for the agent to learn in those environments", "tokens": [50376, 3013, 18779, 300, 456, 311, 257, 688, 544, 337, 264, 9461, 281, 1466, 294, 729, 12388, 50500], "temperature": 0.0, "avg_logprob": -0.09812216604909589, "compression_ratio": 1.805921052631579, "no_speech_prob": 0.01132998988032341}, {"id": 531, "seek": 229842, "start": 2301.3, "end": 2306.7400000000002, "text": " So it's sort of this like optimal you want to find this optimal path weaving through the high-dimensional design space of environments", "tokens": [50508, 407, 309, 311, 1333, 295, 341, 411, 16252, 291, 528, 281, 915, 341, 16252, 3100, 40028, 807, 264, 1090, 12, 18759, 1715, 1901, 295, 12388, 50780], "temperature": 0.0, "avg_logprob": -0.09812216604909589, "compression_ratio": 1.805921052631579, "no_speech_prob": 0.01132998988032341}, {"id": 532, "seek": 229842, "start": 2307.06, "end": 2314.82, "text": " Now the danger here is that as you do this, uh, auto curriculum the auto curriculum, uh, could also go haywire very easily because", "tokens": [50796, 823, 264, 4330, 510, 307, 300, 382, 291, 360, 341, 11, 2232, 11, 8399, 14302, 264, 8399, 14302, 11, 2232, 11, 727, 611, 352, 4842, 42689, 588, 3612, 570, 51184], "temperature": 0.0, "avg_logprob": -0.09812216604909589, "compression_ratio": 1.805921052631579, "no_speech_prob": 0.01132998988032341}, {"id": 533, "seek": 229842, "start": 2315.06, "end": 2320.58, "text": " The design space is so big if you're training in simulation, which we have to do because these methods are so sample inefficient", "tokens": [51196, 440, 1715, 1901, 307, 370, 955, 498, 291, 434, 3097, 294, 16575, 11, 597, 321, 362, 281, 360, 570, 613, 7150, 366, 370, 6889, 43495, 51472], "temperature": 0.0, "avg_logprob": -0.09812216604909589, "compression_ratio": 1.805921052631579, "no_speech_prob": 0.01132998988032341}, {"id": 534, "seek": 229842, "start": 2320.58, "end": 2323.78, "text": " We need so much data to train them. Um, you want to train in simulation", "tokens": [51472, 492, 643, 370, 709, 1412, 281, 3847, 552, 13, 3301, 11, 291, 528, 281, 3847, 294, 16575, 51632], "temperature": 0.0, "avg_logprob": -0.09812216604909589, "compression_ratio": 1.805921052631579, "no_speech_prob": 0.01132998988032341}, {"id": 535, "seek": 232378, "start": 2323.78, "end": 2326.82, "text": " But if you're doing the auto curriculum in the simulation design space", "tokens": [50364, 583, 498, 291, 434, 884, 264, 8399, 14302, 294, 264, 16575, 1715, 1901, 50516], "temperature": 0.0, "avg_logprob": -0.10324320062860712, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.07155022770166397}, {"id": 536, "seek": 232378, "start": 2327.2200000000003, "end": 2333.38, "text": " It could start to veer very easily and quickly into different corners or niches of the design space where", "tokens": [50536, 467, 727, 722, 281, 1241, 260, 588, 3612, 293, 2661, 666, 819, 12413, 420, 25570, 279, 295, 264, 1715, 1901, 689, 50844], "temperature": 0.0, "avg_logprob": -0.10324320062860712, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.07155022770166397}, {"id": 537, "seek": 232378, "start": 2333.94, "end": 2340.26, "text": " You know the parameters no longer really make sense in terms of mapping to a physical reality or a real world scenario", "tokens": [50872, 509, 458, 264, 9834, 572, 2854, 534, 652, 2020, 294, 2115, 295, 18350, 281, 257, 4001, 4103, 420, 257, 957, 1002, 9005, 51188], "temperature": 0.0, "avg_logprob": -0.10324320062860712, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.07155022770166397}, {"id": 538, "seek": 232378, "start": 2340.5800000000004, "end": 2342.5800000000004, "text": " That we as human users", "tokens": [51204, 663, 321, 382, 1952, 5022, 51304], "temperature": 0.0, "avg_logprob": -0.10324320062860712, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.07155022770166397}, {"id": 539, "seek": 232378, "start": 2342.6600000000003, "end": 2345.6200000000003, "text": " Uh actually care about and so kind of it would be you know", "tokens": [51308, 4019, 767, 1127, 466, 293, 370, 733, 295, 309, 576, 312, 291, 458, 51456], "temperature": 0.0, "avg_logprob": -0.10324320062860712, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.07155022770166397}, {"id": 540, "seek": 232378, "start": 2345.78, "end": 2349.86, "text": " It would defeat the purpose of spending all this compute to train this model that could then help us in the real world", "tokens": [51464, 467, 576, 11785, 264, 4334, 295, 6434, 439, 341, 14722, 281, 3847, 341, 2316, 300, 727, 550, 854, 505, 294, 264, 957, 1002, 51668], "temperature": 0.0, "avg_logprob": -0.10324320062860712, "compression_ratio": 1.7282229965156795, "no_speech_prob": 0.07155022770166397}, {"id": 541, "seek": 234986, "start": 2349.94, "end": 2354.42, "text": " Because now it's veering off into parts of the design space that don't really matter for humans", "tokens": [50368, 1436, 586, 309, 311, 1241, 1794, 766, 666, 3166, 295, 264, 1715, 1901, 300, 500, 380, 534, 1871, 337, 6255, 50592], "temperature": 0.0, "avg_logprob": -0.09601127591907468, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.07149077206850052}, {"id": 542, "seek": 234986, "start": 2354.42, "end": 2360.02, "text": " It's kind of noisy parts of the design space. And so this kind of leads us to this question of grounding", "tokens": [50592, 467, 311, 733, 295, 24518, 3166, 295, 264, 1715, 1901, 13, 400, 370, 341, 733, 295, 6689, 505, 281, 341, 1168, 295, 46727, 50872], "temperature": 0.0, "avg_logprob": -0.09601127591907468, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.07149077206850052}, {"id": 543, "seek": 234986, "start": 2360.02, "end": 2363.6200000000003, "text": " How do we ground curricula? How do we align the curricula such that you know?", "tokens": [50872, 1012, 360, 321, 2727, 13179, 3780, 30, 1012, 360, 321, 7975, 264, 13179, 3780, 1270, 300, 291, 458, 30, 51052], "temperature": 0.0, "avg_logprob": -0.09601127591907468, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.07149077206850052}, {"id": 544, "seek": 234986, "start": 2363.6200000000003, "end": 2368.42, "text": " They can still do their exploration through this active sampling type of procedure over the environment design space", "tokens": [51052, 814, 393, 920, 360, 641, 16197, 807, 341, 4967, 21179, 2010, 295, 10747, 670, 264, 2823, 1715, 1901, 51292], "temperature": 0.0, "avg_logprob": -0.09601127591907468, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.07149077206850052}, {"id": 545, "seek": 234986, "start": 2368.58, "end": 2375.2200000000003, "text": " But at the same still at the same time maintain at least some proximity to the parts of that design space that are relevant to", "tokens": [51300, 583, 412, 264, 912, 920, 412, 264, 912, 565, 6909, 412, 1935, 512, 27632, 281, 264, 3166, 295, 300, 1715, 1901, 300, 366, 7340, 281, 51632], "temperature": 0.0, "avg_logprob": -0.09601127591907468, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.07149077206850052}, {"id": 546, "seek": 237522, "start": 2375.7, "end": 2378.3399999999997, "text": " What humans care about in terms of the actual tasks they represent", "tokens": [50388, 708, 6255, 1127, 466, 294, 2115, 295, 264, 3539, 9608, 436, 2906, 50520], "temperature": 0.0, "avg_logprob": -0.10460103133629109, "compression_ratio": 1.7947976878612717, "no_speech_prob": 0.03657615929841995}, {"id": 547, "seek": 237522, "start": 2378.5, "end": 2381.9399999999996, "text": " I mean i've been speaking with kenneth stanley a lot recently and we're talking about open-endedness", "tokens": [50528, 286, 914, 741, 600, 668, 4124, 365, 350, 1857, 3293, 27984, 3420, 257, 688, 3938, 293, 321, 434, 1417, 466, 1269, 12, 3502, 1287, 50700], "temperature": 0.0, "avg_logprob": -0.10460103133629109, "compression_ratio": 1.7947976878612717, "no_speech_prob": 0.03657615929841995}, {"id": 548, "seek": 237522, "start": 2382.3399999999997, "end": 2385.8599999999997, "text": " And in general i've been trying to come at this problem from multiple angles", "tokens": [50720, 400, 294, 2674, 741, 600, 668, 1382, 281, 808, 412, 341, 1154, 490, 3866, 14708, 50896], "temperature": 0.0, "avg_logprob": -0.10460103133629109, "compression_ratio": 1.7947976878612717, "no_speech_prob": 0.03657615929841995}, {"id": 549, "seek": 237522, "start": 2385.9399999999996, "end": 2391.06, "text": " And i've been using the lens of agency because i think agency is something that happens in the real world", "tokens": [50900, 400, 741, 600, 668, 1228, 264, 6765, 295, 7934, 570, 741, 519, 7934, 307, 746, 300, 2314, 294, 264, 957, 1002, 51156], "temperature": 0.0, "avg_logprob": -0.10460103133629109, "compression_ratio": 1.7947976878612717, "no_speech_prob": 0.03657615929841995}, {"id": 550, "seek": 237522, "start": 2391.06, "end": 2395.14, "text": " And that's why we have this divergent process because we have multiple agents, you know, kind of like", "tokens": [51156, 400, 300, 311, 983, 321, 362, 341, 18558, 6930, 1399, 570, 321, 362, 3866, 12554, 11, 291, 458, 11, 733, 295, 411, 51360], "temperature": 0.0, "avg_logprob": -0.10460103133629109, "compression_ratio": 1.7947976878612717, "no_speech_prob": 0.03657615929841995}, {"id": 551, "seek": 237522, "start": 2395.54, "end": 2399.54, "text": " You know undirected following their own gradient of interestingness. So in in evolution", "tokens": [51380, 509, 458, 674, 11890, 292, 3480, 641, 1065, 16235, 295, 1880, 1287, 13, 407, 294, 294, 9303, 51580], "temperature": 0.0, "avg_logprob": -0.10460103133629109, "compression_ratio": 1.7947976878612717, "no_speech_prob": 0.03657615929841995}, {"id": 552, "seek": 237522, "start": 2399.54, "end": 2402.98, "text": " That's a great example that it is this divergent process, but it's also grounded", "tokens": [51580, 663, 311, 257, 869, 1365, 300, 309, 307, 341, 18558, 6930, 1399, 11, 457, 309, 311, 611, 23535, 51752], "temperature": 0.0, "avg_logprob": -0.10460103133629109, "compression_ratio": 1.7947976878612717, "no_speech_prob": 0.03657615929841995}, {"id": 553, "seek": 240298, "start": 2402.98, "end": 2408.34, "text": " It's physically grounded, you know, so it's like the physical world creates some kind of constraints on on the things that are found", "tokens": [50364, 467, 311, 9762, 23535, 11, 291, 458, 11, 370, 309, 311, 411, 264, 4001, 1002, 7829, 512, 733, 295, 18491, 322, 322, 264, 721, 300, 366, 1352, 50632], "temperature": 0.0, "avg_logprob": -0.1196653393731601, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.006555888336151838}, {"id": 554, "seek": 240298, "start": 2408.66, "end": 2412.9, "text": " And i mean, you know clune called this ai generating algorithms. There's quite a few different takes on this", "tokens": [50648, 400, 741, 914, 11, 291, 458, 596, 2613, 1219, 341, 9783, 17746, 14642, 13, 821, 311, 1596, 257, 1326, 819, 2516, 322, 341, 50860], "temperature": 0.0, "avg_logprob": -0.1196653393731601, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.006555888336151838}, {"id": 555, "seek": 240298, "start": 2412.9, "end": 2416.02, "text": " But the idea is that um to search this complex search space", "tokens": [50860, 583, 264, 1558, 307, 300, 1105, 281, 3164, 341, 3997, 3164, 1901, 51016], "temperature": 0.0, "avg_logprob": -0.1196653393731601, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.006555888336151838}, {"id": 556, "seek": 240298, "start": 2416.1, "end": 2421.94, "text": " We we need to have a divergent search and that's like we actually need to create the problems and the solutions", "tokens": [51020, 492, 321, 643, 281, 362, 257, 18558, 6930, 3164, 293, 300, 311, 411, 321, 767, 643, 281, 1884, 264, 2740, 293, 264, 6547, 51312], "temperature": 0.0, "avg_logprob": -0.1196653393731601, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.006555888336151838}, {"id": 557, "seek": 240298, "start": 2421.94, "end": 2423.22, "text": " So like in the real world", "tokens": [51312, 407, 411, 294, 264, 957, 1002, 51376], "temperature": 0.0, "avg_logprob": -0.1196653393731601, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.006555888336151838}, {"id": 558, "seek": 240298, "start": 2423.22, "end": 2427.62, "text": " The the you know the giraffes had the problem of like eating the leaves from from from the trees", "tokens": [51376, 440, 264, 291, 458, 264, 14703, 2518, 279, 632, 264, 1154, 295, 411, 3936, 264, 5510, 490, 490, 490, 264, 5852, 51596], "temperature": 0.0, "avg_logprob": -0.1196653393731601, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.006555888336151838}, {"id": 559, "seek": 240298, "start": 2427.78, "end": 2430.26, "text": " And the problems and the solutions get generated in tandem", "tokens": [51604, 400, 264, 2740, 293, 264, 6547, 483, 10833, 294, 48120, 51728], "temperature": 0.0, "avg_logprob": -0.1196653393731601, "compression_ratio": 1.9193548387096775, "no_speech_prob": 0.006555888336151838}, {"id": 560, "seek": 243026, "start": 2430.42, "end": 2432.5, "text": " And this whole thing just kind of grows and grows and grows", "tokens": [50372, 400, 341, 1379, 551, 445, 733, 295, 13156, 293, 13156, 293, 13156, 50476], "temperature": 0.0, "avg_logprob": -0.154482634862264, "compression_ratio": 1.8178807947019868, "no_speech_prob": 0.0012049833312630653}, {"id": 561, "seek": 243026, "start": 2432.6600000000003, "end": 2439.46, "text": " And that seems to be the most important feature that is missing in current ai systems and the grounding or the", "tokens": [50484, 400, 300, 2544, 281, 312, 264, 881, 1021, 4111, 300, 307, 5361, 294, 2190, 9783, 3652, 293, 264, 46727, 420, 264, 50824], "temperature": 0.0, "avg_logprob": -0.154482634862264, "compression_ratio": 1.8178807947019868, "no_speech_prob": 0.0012049833312630653}, {"id": 562, "seek": 243026, "start": 2440.6600000000003, "end": 2443.94, "text": " Stanley calls it the gradient of interestingness. I'm not sure whether you'd agree with that", "tokens": [50884, 28329, 5498, 309, 264, 16235, 295, 1880, 1287, 13, 286, 478, 406, 988, 1968, 291, 1116, 3986, 365, 300, 51048], "temperature": 0.0, "avg_logprob": -0.154482634862264, "compression_ratio": 1.8178807947019868, "no_speech_prob": 0.0012049833312630653}, {"id": 563, "seek": 243026, "start": 2444.0200000000004, "end": 2449.0600000000004, "text": " But um, i mean what mark what what what do you think about the importance of like this divergence in ai?", "tokens": [51052, 583, 1105, 11, 741, 914, 437, 1491, 437, 437, 437, 360, 291, 519, 466, 264, 7379, 295, 411, 341, 47387, 294, 9783, 30, 51304], "temperature": 0.0, "avg_logprob": -0.154482634862264, "compression_ratio": 1.8178807947019868, "no_speech_prob": 0.0012049833312630653}, {"id": 564, "seek": 243026, "start": 2449.7000000000003, "end": 2452.0200000000004, "text": " kind of the current paradigm of machine learning", "tokens": [51336, 733, 295, 264, 2190, 24709, 295, 3479, 2539, 51452], "temperature": 0.0, "avg_logprob": -0.154482634862264, "compression_ratio": 1.8178807947019868, "no_speech_prob": 0.0012049833312630653}, {"id": 565, "seek": 243026, "start": 2452.6600000000003, "end": 2458.1000000000004, "text": " Of kind of like, you know gathering some data set beforehand or specifying some simulated beforehand if it's reinforcement learning", "tokens": [51484, 2720, 733, 295, 411, 11, 291, 458, 13519, 512, 1412, 992, 22893, 420, 1608, 5489, 512, 41713, 22893, 498, 309, 311, 29280, 2539, 51756], "temperature": 0.0, "avg_logprob": -0.154482634862264, "compression_ratio": 1.8178807947019868, "no_speech_prob": 0.0012049833312630653}, {"id": 566, "seek": 245810, "start": 2458.58, "end": 2462.58, "text": " Is kind of good enough to do like a lot of reasonable tasks that we might care about", "tokens": [50388, 1119, 733, 295, 665, 1547, 281, 360, 411, 257, 688, 295, 10585, 9608, 300, 321, 1062, 1127, 466, 50588], "temperature": 0.0, "avg_logprob": -0.12361127919164197, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0038230465725064278}, {"id": 567, "seek": 245810, "start": 2463.06, "end": 2463.86, "text": " um, you know", "tokens": [50612, 1105, 11, 291, 458, 50652], "temperature": 0.0, "avg_logprob": -0.12361127919164197, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0038230465725064278}, {"id": 568, "seek": 245810, "start": 2463.86, "end": 2468.9, "text": " Like obviously like predicting language or generating simulated language or performing very well at some simulated task in rl", "tokens": [50652, 1743, 2745, 411, 32884, 2856, 420, 17746, 41713, 2856, 420, 10205, 588, 731, 412, 512, 41713, 5633, 294, 367, 75, 50904], "temperature": 0.0, "avg_logprob": -0.12361127919164197, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0038230465725064278}, {"id": 569, "seek": 245810, "start": 2469.14, "end": 2472.66, "text": " But it definitely seems like the next step towards like very general agents that are kind of", "tokens": [50916, 583, 309, 2138, 2544, 411, 264, 958, 1823, 3030, 411, 588, 2674, 12554, 300, 366, 733, 295, 51092], "temperature": 0.0, "avg_logprob": -0.12361127919164197, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0038230465725064278}, {"id": 570, "seek": 245810, "start": 2473.46, "end": 2476.1, "text": " You know, I guess maybe I don't know if we want to use the term agi", "tokens": [51132, 509, 458, 11, 286, 2041, 1310, 286, 500, 380, 458, 498, 321, 528, 281, 764, 264, 1433, 623, 72, 51264], "temperature": 0.0, "avg_logprob": -0.12361127919164197, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0038230465725064278}, {"id": 571, "seek": 245810, "start": 2476.1, "end": 2479.54, "text": " But there's something something more along the lines of a general agent that's kind of you know", "tokens": [51264, 583, 456, 311, 746, 746, 544, 2051, 264, 3876, 295, 257, 2674, 9461, 300, 311, 733, 295, 291, 458, 51436], "temperature": 0.0, "avg_logprob": -0.12361127919164197, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0038230465725064278}, {"id": 572, "seek": 245810, "start": 2479.86, "end": 2483.14, "text": " able to kind of self improve and learn in more diverse environments", "tokens": [51452, 1075, 281, 733, 295, 2698, 3470, 293, 1466, 294, 544, 9521, 12388, 51616], "temperature": 0.0, "avg_logprob": -0.12361127919164197, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0038230465725064278}, {"id": 573, "seek": 245810, "start": 2483.62, "end": 2487.62, "text": " Um, it definitely seems like that's kind of the next step of where machine learning will go", "tokens": [51640, 3301, 11, 309, 2138, 2544, 411, 300, 311, 733, 295, 264, 958, 1823, 295, 689, 3479, 2539, 486, 352, 51840], "temperature": 0.0, "avg_logprob": -0.12361127919164197, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.0038230465725064278}, {"id": 574, "seek": 248810, "start": 2488.18, "end": 2490.8199999999997, "text": " And if we're going to get to that point, I kind of agree with the idea that", "tokens": [50368, 400, 498, 321, 434, 516, 281, 483, 281, 300, 935, 11, 286, 733, 295, 3986, 365, 264, 1558, 300, 50500], "temperature": 0.0, "avg_logprob": -0.09714339597381814, "compression_ratio": 1.9342105263157894, "no_speech_prob": 0.000779075373429805}, {"id": 575, "seek": 248810, "start": 2491.54, "end": 2496.9, "text": " You know, it certainly doesn't make sense to have some agent that just randomly trying to gather completely random new knowledge", "tokens": [50536, 509, 458, 11, 309, 3297, 1177, 380, 652, 2020, 281, 362, 512, 9461, 300, 445, 16979, 1382, 281, 5448, 2584, 4974, 777, 3601, 50804], "temperature": 0.0, "avg_logprob": -0.09714339597381814, "compression_ratio": 1.9342105263157894, "no_speech_prob": 0.000779075373429805}, {"id": 576, "seek": 248810, "start": 2497.22, "end": 2500.58, "text": " Like it certainly seems to make sense that you know, you know, even as a human", "tokens": [50820, 1743, 309, 3297, 2544, 281, 652, 2020, 300, 291, 458, 11, 291, 458, 11, 754, 382, 257, 1952, 50988], "temperature": 0.0, "avg_logprob": -0.09714339597381814, "compression_ratio": 1.9342105263157894, "no_speech_prob": 0.000779075373429805}, {"id": 577, "seek": 248810, "start": 2501.22, "end": 2506.1, "text": " To improve your intelligence you kind of selectively try and find out the areas in which like you can gather more", "tokens": [51020, 1407, 3470, 428, 7599, 291, 733, 295, 3048, 3413, 853, 293, 915, 484, 264, 3179, 294, 597, 411, 291, 393, 5448, 544, 51264], "temperature": 0.0, "avg_logprob": -0.09714339597381814, "compression_ratio": 1.9342105263157894, "no_speech_prob": 0.000779075373429805}, {"id": 578, "seek": 248810, "start": 2506.5, "end": 2509.54, "text": " More information or more knowledge and things like this and this is kind of what you know", "tokens": [51284, 5048, 1589, 420, 544, 3601, 293, 721, 411, 341, 293, 341, 307, 733, 295, 437, 291, 458, 51436], "temperature": 0.0, "avg_logprob": -0.09714339597381814, "compression_ratio": 1.9342105263157894, "no_speech_prob": 0.000779075373429805}, {"id": 579, "seek": 248810, "start": 2509.7, "end": 2514.3399999999997, "text": " Leads to this kind of I guess branching or you know, like you said like the diverse set of things um", "tokens": [51444, 1456, 5834, 281, 341, 733, 295, 286, 2041, 9819, 278, 420, 291, 458, 11, 411, 291, 848, 411, 264, 9521, 992, 295, 721, 1105, 51676], "temperature": 0.0, "avg_logprob": -0.09714339597381814, "compression_ratio": 1.9342105263157894, "no_speech_prob": 0.000779075373429805}, {"id": 580, "seek": 251434, "start": 2515.1400000000003, "end": 2517.54, "text": " That you might want to learn more about and so yeah", "tokens": [50404, 663, 291, 1062, 528, 281, 1466, 544, 466, 293, 370, 1338, 50524], "temperature": 0.0, "avg_logprob": -0.1577563543577452, "compression_ratio": 1.8024316109422491, "no_speech_prob": 0.006487417500466108}, {"id": 581, "seek": 251434, "start": 2517.54, "end": 2521.6200000000003, "text": " I think like it clearly seems to make sense that like this kind of more open-ended this thinking is probably going to be like", "tokens": [50524, 286, 519, 411, 309, 4448, 2544, 281, 652, 2020, 300, 411, 341, 733, 295, 544, 1269, 12, 3502, 341, 1953, 307, 1391, 516, 281, 312, 411, 50728], "temperature": 0.0, "avg_logprob": -0.1577563543577452, "compression_ratio": 1.8024316109422491, "no_speech_prob": 0.006487417500466108}, {"id": 582, "seek": 251434, "start": 2522.02, "end": 2524.34, "text": " The next paradigm of how we think about these kinds of systems", "tokens": [50748, 440, 958, 24709, 295, 577, 321, 519, 466, 613, 3685, 295, 3652, 50864], "temperature": 0.0, "avg_logprob": -0.1577563543577452, "compression_ratio": 1.8024316109422491, "no_speech_prob": 0.006487417500466108}, {"id": 583, "seek": 251434, "start": 2524.58, "end": 2526.58, "text": " But I'll I think mentally we'll have more to say about this", "tokens": [50876, 583, 286, 603, 286, 519, 17072, 321, 603, 362, 544, 281, 584, 466, 341, 50976], "temperature": 0.0, "avg_logprob": -0.1577563543577452, "compression_ratio": 1.8024316109422491, "no_speech_prob": 0.006487417500466108}, {"id": 584, "seek": 251434, "start": 2526.98, "end": 2532.82, "text": " I think the reason open-endedness is so interesting now is I think we're uh, there's there's a few reasons why I think it's like", "tokens": [50996, 286, 519, 264, 1778, 1269, 12, 3502, 1287, 307, 370, 1880, 586, 307, 286, 519, 321, 434, 2232, 11, 456, 311, 456, 311, 257, 1326, 4112, 983, 286, 519, 309, 311, 411, 51288], "temperature": 0.0, "avg_logprob": -0.1577563543577452, "compression_ratio": 1.8024316109422491, "no_speech_prob": 0.006487417500466108}, {"id": 585, "seek": 251434, "start": 2532.9, "end": 2539.7000000000003, "text": " newly relevant to this current era of machine learning because these ideas have been around for quite a while like, um, Ken Stanley, Joe Lehmann", "tokens": [51292, 15109, 7340, 281, 341, 2190, 4249, 295, 3479, 2539, 570, 613, 3487, 362, 668, 926, 337, 1596, 257, 1339, 411, 11, 1105, 11, 8273, 28329, 11, 6807, 1456, 8587, 969, 51632], "temperature": 0.0, "avg_logprob": -0.1577563543577452, "compression_ratio": 1.8024316109422491, "no_speech_prob": 0.006487417500466108}, {"id": 586, "seek": 251434, "start": 2540.02, "end": 2542.02, "text": " um, Jeff Klune, uh", "tokens": [51648, 1105, 11, 7506, 16053, 2613, 11, 2232, 51748], "temperature": 0.0, "avg_logprob": -0.1577563543577452, "compression_ratio": 1.8024316109422491, "no_speech_prob": 0.006487417500466108}, {"id": 587, "seek": 254202, "start": 2542.02, "end": 2544.18, "text": " Lisa Soros these a lot of these researchers, they've", "tokens": [50364, 12252, 21421, 329, 613, 257, 688, 295, 613, 10309, 11, 436, 600, 50472], "temperature": 0.0, "avg_logprob": -0.13401189057723337, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.005818693432956934}, {"id": 588, "seek": 254202, "start": 2544.9, "end": 2550.18, "text": " They've been thinking about open-endedness and novelty based search divergent search for decades. Um", "tokens": [50508, 814, 600, 668, 1953, 466, 1269, 12, 3502, 1287, 293, 44805, 2361, 3164, 18558, 6930, 3164, 337, 7878, 13, 3301, 50772], "temperature": 0.0, "avg_logprob": -0.13401189057723337, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.005818693432956934}, {"id": 589, "seek": 254202, "start": 2550.74, "end": 2555.06, "text": " I think it's really interesting to think about why there's sort of this resurgence of these ideas now", "tokens": [50800, 286, 519, 309, 311, 534, 1880, 281, 519, 466, 983, 456, 311, 1333, 295, 341, 725, 44607, 295, 613, 3487, 586, 51016], "temperature": 0.0, "avg_logprob": -0.13401189057723337, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.005818693432956934}, {"id": 590, "seek": 254202, "start": 2555.3, "end": 2557.3, "text": " and I think a lot of it is because", "tokens": [51028, 293, 286, 519, 257, 688, 295, 309, 307, 570, 51128], "temperature": 0.0, "avg_logprob": -0.13401189057723337, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.005818693432956934}, {"id": 591, "seek": 254202, "start": 2557.38, "end": 2559.7, "text": " It is again, you know, it's it's sort of following the same", "tokens": [51132, 467, 307, 797, 11, 291, 458, 11, 309, 311, 309, 311, 1333, 295, 3480, 264, 912, 51248], "temperature": 0.0, "avg_logprob": -0.13401189057723337, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.005818693432956934}, {"id": 592, "seek": 254202, "start": 2560.1, "end": 2560.82, "text": " um", "tokens": [51268, 1105, 51304], "temperature": 0.0, "avg_logprob": -0.13401189057723337, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.005818693432956934}, {"id": 593, "seek": 254202, "start": 2560.82, "end": 2563.7, "text": " Sort of uh tailwinds that have been driving a lot of the ml industry", "tokens": [51304, 26149, 295, 2232, 6838, 12199, 82, 300, 362, 668, 4840, 257, 688, 295, 264, 23271, 3518, 51448], "temperature": 0.0, "avg_logprob": -0.13401189057723337, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.005818693432956934}, {"id": 594, "seek": 254202, "start": 2563.7, "end": 2566.98, "text": " Which is just like much better compute much larger datasets", "tokens": [51448, 3013, 307, 445, 411, 709, 1101, 14722, 709, 4833, 42856, 51612], "temperature": 0.0, "avg_logprob": -0.13401189057723337, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.005818693432956934}, {"id": 595, "seek": 254202, "start": 2567.3, "end": 2569.78, "text": " And I think what we're seeing now is that we know that", "tokens": [51628, 400, 286, 519, 437, 321, 434, 2577, 586, 307, 300, 321, 458, 300, 51752], "temperature": 0.0, "avg_logprob": -0.13401189057723337, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.005818693432956934}, {"id": 596, "seek": 256978, "start": 2570.26, "end": 2575.1400000000003, "text": " Modern deep learning methods work best when we can scale up the compute and the data. That's how you get them to work", "tokens": [50388, 19814, 2452, 2539, 7150, 589, 1151, 562, 321, 393, 4373, 493, 264, 14722, 293, 264, 1412, 13, 663, 311, 577, 291, 483, 552, 281, 589, 50632], "temperature": 0.0, "avg_logprob": -0.11491971697126116, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.001032073050737381}, {"id": 597, "seek": 256978, "start": 2575.5400000000004, "end": 2579.38, "text": " Um to the to their maximal capabilities. Um at some point", "tokens": [50652, 3301, 281, 264, 281, 641, 49336, 10862, 13, 3301, 412, 512, 935, 50844], "temperature": 0.0, "avg_logprob": -0.11491971697126116, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.001032073050737381}, {"id": 598, "seek": 256978, "start": 2579.38, "end": 2582.42, "text": " We're going to run out of data and a lot of people are now starting to talk about", "tokens": [50844, 492, 434, 516, 281, 1190, 484, 295, 1412, 293, 257, 688, 295, 561, 366, 586, 2891, 281, 751, 466, 50996], "temperature": 0.0, "avg_logprob": -0.11491971697126116, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.001032073050737381}, {"id": 599, "seek": 256978, "start": 2582.6600000000003, "end": 2584.7400000000002, "text": " You know this as sort of a pending issue on the horizon", "tokens": [51008, 509, 458, 341, 382, 1333, 295, 257, 32110, 2734, 322, 264, 18046, 51112], "temperature": 0.0, "avg_logprob": -0.11491971697126116, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.001032073050737381}, {"id": 600, "seek": 256978, "start": 2584.7400000000002, "end": 2589.1400000000003, "text": " Which is you know at the current rate of consuming data for training our foundation models at some point", "tokens": [51112, 3013, 307, 291, 458, 412, 264, 2190, 3314, 295, 19867, 1412, 337, 3097, 527, 7030, 5245, 412, 512, 935, 51332], "temperature": 0.0, "avg_logprob": -0.11491971697126116, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.001032073050737381}, {"id": 601, "seek": 256978, "start": 2589.1400000000003, "end": 2592.5, "text": " We're going to run out of data. We're going to where are we going to get the next trillion tokens from?", "tokens": [51332, 492, 434, 516, 281, 1190, 484, 295, 1412, 13, 492, 434, 516, 281, 689, 366, 321, 516, 281, 483, 264, 958, 18723, 22667, 490, 30, 51500], "temperature": 0.0, "avg_logprob": -0.11491971697126116, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.001032073050737381}, {"id": 602, "seek": 256978, "start": 2592.9, "end": 2595.6200000000003, "text": " Um, and so I think a lot of this uh now", "tokens": [51520, 3301, 11, 293, 370, 286, 519, 257, 688, 295, 341, 2232, 586, 51656], "temperature": 0.0, "avg_logprob": -0.11491971697126116, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.001032073050737381}, {"id": 603, "seek": 259562, "start": 2596.02, "end": 2600.3399999999997, "text": " points a lot of the interest to open-endedness because open-endedness is essentially, you know", "tokens": [50384, 2793, 257, 688, 295, 264, 1179, 281, 1269, 12, 3502, 1287, 570, 1269, 12, 3502, 1287, 307, 4476, 11, 291, 458, 50600], "temperature": 0.0, "avg_logprob": -0.12782247924804688, "compression_ratio": 1.9301470588235294, "no_speech_prob": 0.001809897948987782}, {"id": 604, "seek": 259562, "start": 2600.58, "end": 2603.46, "text": " We're studying systems that can generate their own data in an infinite", "tokens": [50612, 492, 434, 7601, 3652, 300, 393, 8460, 641, 1065, 1412, 294, 364, 13785, 50756], "temperature": 0.0, "avg_logprob": -0.12782247924804688, "compression_ratio": 1.9301470588235294, "no_speech_prob": 0.001809897948987782}, {"id": 605, "seek": 259562, "start": 2604.02, "end": 2607.7, "text": " Capacity and so it's systems that essentially if you run it for longer and longer", "tokens": [50784, 8363, 19008, 293, 370, 309, 311, 3652, 300, 4476, 498, 291, 1190, 309, 337, 2854, 293, 2854, 50968], "temperature": 0.0, "avg_logprob": -0.12782247924804688, "compression_ratio": 1.9301470588235294, "no_speech_prob": 0.001809897948987782}, {"id": 606, "seek": 259562, "start": 2608.02, "end": 2612.42, "text": " They get more and more complex. They generate more and more quote-unquote interestingness or interesting data", "tokens": [50984, 814, 483, 544, 293, 544, 3997, 13, 814, 8460, 544, 293, 544, 6513, 12, 409, 25016, 1880, 1287, 420, 1880, 1412, 51204], "temperature": 0.0, "avg_logprob": -0.12782247924804688, "compression_ratio": 1.9301470588235294, "no_speech_prob": 0.001809897948987782}, {"id": 607, "seek": 259562, "start": 2612.8199999999997, "end": 2613.22, "text": " um", "tokens": [51224, 1105, 51244], "temperature": 0.0, "avg_logprob": -0.12782247924804688, "compression_ratio": 1.9301470588235294, "no_speech_prob": 0.001809897948987782}, {"id": 608, "seek": 259562, "start": 2613.22, "end": 2616.58, "text": " And so if we can actually, you know crack this nut of how do we actually", "tokens": [51244, 400, 370, 498, 321, 393, 767, 11, 291, 458, 6226, 341, 5393, 295, 577, 360, 321, 767, 51412], "temperature": 0.0, "avg_logprob": -0.12782247924804688, "compression_ratio": 1.9301470588235294, "no_speech_prob": 0.001809897948987782}, {"id": 609, "seek": 259562, "start": 2616.98, "end": 2621.22, "text": " Come up with a self-improving system in the sense that it keeps generating interesting data", "tokens": [51432, 2492, 493, 365, 257, 2698, 12, 332, 4318, 798, 1185, 294, 264, 2020, 300, 309, 5965, 17746, 1880, 1412, 51644], "temperature": 0.0, "avg_logprob": -0.12782247924804688, "compression_ratio": 1.9301470588235294, "no_speech_prob": 0.001809897948987782}, {"id": 610, "seek": 262122, "start": 2621.9399999999996, "end": 2626.74, "text": " We can then use that data to train further train our models", "tokens": [50400, 492, 393, 550, 764, 300, 1412, 281, 3847, 3052, 3847, 527, 5245, 50640], "temperature": 0.0, "avg_logprob": -0.10444915408179874, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.028863579034805298}, {"id": 611, "seek": 262122, "start": 2626.98, "end": 2630.8999999999996, "text": " But of course you get into this perpetual data machine type of", "tokens": [50652, 583, 295, 1164, 291, 483, 666, 341, 48216, 1412, 3479, 2010, 295, 50848], "temperature": 0.0, "avg_logprob": -0.10444915408179874, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.028863579034805298}, {"id": 612, "seek": 262122, "start": 2631.4599999999996, "end": 2634.5, "text": " Idea where obviously, you know, there's how do you generate more data?", "tokens": [50876, 47245, 689, 2745, 11, 291, 458, 11, 456, 311, 577, 360, 291, 8460, 544, 1412, 30, 51028], "temperature": 0.0, "avg_logprob": -0.10444915408179874, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.028863579034805298}, {"id": 613, "seek": 262122, "start": 2635.3799999999997, "end": 2639.14, "text": " If you know the data is ultimately coming from a model that you probably trained on previous data", "tokens": [51072, 759, 291, 458, 264, 1412, 307, 6284, 1348, 490, 257, 2316, 300, 291, 1391, 8895, 322, 3894, 1412, 51260], "temperature": 0.0, "avg_logprob": -0.10444915408179874, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.028863579034805298}, {"id": 614, "seek": 262122, "start": 2639.22, "end": 2640.98, "text": " How do you get net new information from that?", "tokens": [51264, 1012, 360, 291, 483, 2533, 777, 1589, 490, 300, 30, 51352], "temperature": 0.0, "avg_logprob": -0.10444915408179874, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.028863579034805298}, {"id": 615, "seek": 262122, "start": 2640.98, "end": 2645.9399999999996, "text": " Well, I think a lot of this is actually just resolved purely again going back to this idea of the reward function", "tokens": [51352, 1042, 11, 286, 519, 257, 688, 295, 341, 307, 767, 445, 20772, 17491, 797, 516, 646, 281, 341, 1558, 295, 264, 7782, 2445, 51600], "temperature": 0.0, "avg_logprob": -0.10444915408179874, "compression_ratio": 1.7018867924528303, "no_speech_prob": 0.028863579034805298}, {"id": 616, "seek": 264594, "start": 2646.1, "end": 2652.18, "text": " Right or a preference function where there is outside information coming in through some sort of filtering criteria", "tokens": [50372, 1779, 420, 257, 17502, 2445, 689, 456, 307, 2380, 1589, 1348, 294, 807, 512, 1333, 295, 30822, 11101, 50676], "temperature": 0.0, "avg_logprob": -0.11934684304630055, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.2391989380121231}, {"id": 617, "seek": 264594, "start": 2652.42, "end": 2654.42, "text": " For example human designers in the loop", "tokens": [50688, 1171, 1365, 1952, 16196, 294, 264, 6367, 50788], "temperature": 0.0, "avg_logprob": -0.11934684304630055, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.2391989380121231}, {"id": 618, "seek": 264594, "start": 2654.82, "end": 2660.42, "text": " Or designers designing some sort of preference model that could essentially automatically rate the kinds of automatic", "tokens": [50808, 1610, 16196, 14685, 512, 1333, 295, 17502, 2316, 300, 727, 4476, 6772, 3314, 264, 3685, 295, 12509, 51088], "temperature": 0.0, "avg_logprob": -0.11934684304630055, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.2391989380121231}, {"id": 619, "seek": 264594, "start": 2661.3, "end": 2663.54, "text": " Data that's being generated by these open-ended systems", "tokens": [51132, 11888, 300, 311, 885, 10833, 538, 613, 1269, 12, 3502, 3652, 51244], "temperature": 0.0, "avg_logprob": -0.11934684304630055, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.2391989380121231}, {"id": 620, "seek": 264594, "start": 2663.7000000000003, "end": 2668.58, "text": " And if we can do this kind of filtering we can essentially automatically find start to automatically find", "tokens": [51252, 400, 498, 321, 393, 360, 341, 733, 295, 30822, 321, 393, 4476, 6772, 915, 722, 281, 6772, 915, 51496], "temperature": 0.0, "avg_logprob": -0.11934684304630055, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.2391989380121231}, {"id": 621, "seek": 264594, "start": 2669.14, "end": 2673.06, "text": " Useful net new data net new trajectories net new even, you know, maybe", "tokens": [51524, 8278, 906, 2533, 777, 1412, 2533, 777, 18257, 2083, 2533, 777, 754, 11, 291, 458, 11, 1310, 51720], "temperature": 0.0, "avg_logprob": -0.11934684304630055, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.2391989380121231}, {"id": 622, "seek": 267306, "start": 2673.94, "end": 2678.02, "text": " Sentences like tokens or net new content to train our models on", "tokens": [50408, 23652, 2667, 411, 22667, 420, 2533, 777, 2701, 281, 3847, 527, 5245, 322, 50612], "temperature": 0.0, "avg_logprob": -0.1050019105275472, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.003218868048861623}, {"id": 623, "seek": 267306, "start": 2678.34, "end": 2684.82, "text": " I've been thinking a lot about creativity recently and I think creativity is is is the other half of the coin of intelligence", "tokens": [50628, 286, 600, 668, 1953, 257, 688, 466, 12915, 3938, 293, 286, 519, 12915, 307, 307, 307, 264, 661, 1922, 295, 264, 11464, 295, 7599, 50952], "temperature": 0.0, "avg_logprob": -0.1050019105275472, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.003218868048861623}, {"id": 624, "seek": 267306, "start": 2685.22, "end": 2689.7, "text": " So in the world we live in I think that the intelligent process is is us", "tokens": [50972, 407, 294, 264, 1002, 321, 1621, 294, 286, 519, 300, 264, 13232, 1399, 307, 307, 505, 51196], "temperature": 0.0, "avg_logprob": -0.1050019105275472, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.003218868048861623}, {"id": 625, "seek": 267306, "start": 2690.1, "end": 2692.1, "text": " We are a divergent search and we are", "tokens": [51216, 492, 366, 257, 18558, 6930, 3164, 293, 321, 366, 51316], "temperature": 0.0, "avg_logprob": -0.1050019105275472, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.003218868048861623}, {"id": 626, "seek": 267306, "start": 2692.58, "end": 2696.42, "text": " And basically tackling a complex search space and we are building knowledge", "tokens": [51340, 400, 1936, 34415, 257, 3997, 3164, 1901, 293, 321, 366, 2390, 3601, 51532], "temperature": 0.0, "avg_logprob": -0.1050019105275472, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.003218868048861623}, {"id": 627, "seek": 267306, "start": 2696.74, "end": 2698.82, "text": " And we are memetically sharing them in our society", "tokens": [51548, 400, 321, 366, 1334, 22652, 5414, 552, 294, 527, 4086, 51652], "temperature": 0.0, "avg_logprob": -0.1050019105275472, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.003218868048861623}, {"id": 628, "seek": 267306, "start": 2698.82, "end": 2702.66, "text": " We're embedding them in our language and then language models come and like acquire all of that knowledge", "tokens": [51652, 492, 434, 12240, 3584, 552, 294, 527, 2856, 293, 550, 2856, 5245, 808, 293, 411, 20001, 439, 295, 300, 3601, 51844], "temperature": 0.0, "avg_logprob": -0.1050019105275472, "compression_ratio": 1.893238434163701, "no_speech_prob": 0.003218868048861623}, {"id": 629, "seek": 270266, "start": 2702.8999999999996, "end": 2706.8999999999996, "text": " So the cynical take is that ai today doesn't you know generalize and", "tokens": [50376, 407, 264, 46345, 747, 307, 300, 9783, 965, 1177, 380, 291, 458, 2674, 1125, 293, 50576], "temperature": 0.0, "avg_logprob": -0.10007592794057485, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0014050703030079603}, {"id": 630, "seek": 270266, "start": 2707.7, "end": 2710.2599999999998, "text": " It doesn't it doesn't creatively find new knowledge", "tokens": [50616, 467, 1177, 380, 309, 1177, 380, 43750, 915, 777, 3601, 50744], "temperature": 0.0, "avg_logprob": -0.10007592794057485, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0014050703030079603}, {"id": 631, "seek": 270266, "start": 2710.5, "end": 2713.2999999999997, "text": " It just is a representation of the knowledge that we have found", "tokens": [50756, 467, 445, 307, 257, 10290, 295, 264, 3601, 300, 321, 362, 1352, 50896], "temperature": 0.0, "avg_logprob": -0.10007592794057485, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0014050703030079603}, {"id": 632, "seek": 270266, "start": 2713.62, "end": 2718.2599999999998, "text": " But it's not black and white is it so the the work that you're doing is a great example of no no no", "tokens": [50912, 583, 309, 311, 406, 2211, 293, 2418, 307, 309, 370, 264, 264, 589, 300, 291, 434, 884, 307, 257, 869, 1365, 295, 572, 572, 572, 51144], "temperature": 0.0, "avg_logprob": -0.10007592794057485, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0014050703030079603}, {"id": 633, "seek": 270266, "start": 2718.66, "end": 2726.66, "text": " You can generate new knowledge by exploring these complex search spaces and even though you're exploring existing models", "tokens": [51164, 509, 393, 8460, 777, 3601, 538, 12736, 613, 3997, 3164, 7673, 293, 754, 1673, 291, 434, 12736, 6741, 5245, 51564], "temperature": 0.0, "avg_logprob": -0.10007592794057485, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0014050703030079603}, {"id": 634, "seek": 270266, "start": 2727.06, "end": 2731.8599999999997, "text": " You're discovering interesting and novel combinations of those models that have not been found before", "tokens": [51584, 509, 434, 24773, 1880, 293, 7613, 21267, 295, 729, 5245, 300, 362, 406, 668, 1352, 949, 51824], "temperature": 0.0, "avg_logprob": -0.10007592794057485, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0014050703030079603}, {"id": 635, "seek": 273186, "start": 2731.86, "end": 2736.1, "text": " So it's creating a novel margin on something that was not there before", "tokens": [50364, 407, 309, 311, 4084, 257, 7613, 10270, 322, 746, 300, 390, 406, 456, 949, 50576], "temperature": 0.0, "avg_logprob": -0.12777905945384174, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.0004721881414297968}, {"id": 636, "seek": 273186, "start": 2736.42, "end": 2740.6600000000003, "text": " But I suppose the ideal future we want to get into is that we really can just", "tokens": [50592, 583, 286, 7297, 264, 7157, 2027, 321, 528, 281, 483, 666, 307, 300, 321, 534, 393, 445, 50804], "temperature": 0.0, "avg_logprob": -0.12777905945384174, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.0004721881414297968}, {"id": 637, "seek": 273186, "start": 2741.46, "end": 2744.34, "text": " From a far deeper level generate new knowledge", "tokens": [50844, 3358, 257, 1400, 7731, 1496, 8460, 777, 3601, 50988], "temperature": 0.0, "avg_logprob": -0.12777905945384174, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.0004721881414297968}, {"id": 638, "seek": 273186, "start": 2745.1400000000003, "end": 2750.58, "text": " Yeah, I think one interesting thing that I've been thinking about more recently, you know is that um sort of the you know", "tokens": [51028, 865, 11, 286, 519, 472, 1880, 551, 300, 286, 600, 668, 1953, 466, 544, 3938, 11, 291, 458, 307, 300, 1105, 1333, 295, 264, 291, 458, 51300], "temperature": 0.0, "avg_logprob": -0.12777905945384174, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.0004721881414297968}, {"id": 639, "seek": 273186, "start": 2750.58, "end": 2752.34, "text": " The high level question is just", "tokens": [51300, 440, 1090, 1496, 1168, 307, 445, 51388], "temperature": 0.0, "avg_logprob": -0.12777905945384174, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.0004721881414297968}, {"id": 640, "seek": 273186, "start": 2752.34, "end": 2759.2200000000003, "text": " Right now all of the state of the rai systems from chat gbt to stable diffusion style models for text image generation", "tokens": [51388, 1779, 586, 439, 295, 264, 1785, 295, 264, 367, 1301, 3652, 490, 5081, 290, 4517, 281, 8351, 25242, 3758, 5245, 337, 2487, 3256, 5125, 51732], "temperature": 0.0, "avg_logprob": -0.12777905945384174, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.0004721881414297968}, {"id": 641, "seek": 275922, "start": 2759.3799999999997, "end": 2763.7, "text": " All of these systems they're they're amazing very impressive, you know", "tokens": [50372, 1057, 295, 613, 3652, 436, 434, 436, 434, 2243, 588, 8992, 11, 291, 458, 50588], "temperature": 0.0, "avg_logprob": -0.10610297254023661, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.004980398342013359}, {"id": 642, "seek": 275922, "start": 2763.7799999999997, "end": 2768.2599999999998, "text": " Like five years ago. I would not have believed that these systems could exist at this level of performance today", "tokens": [50592, 1743, 1732, 924, 2057, 13, 286, 576, 406, 362, 7847, 300, 613, 3652, 727, 2514, 412, 341, 1496, 295, 3389, 965, 50816], "temperature": 0.0, "avg_logprob": -0.10610297254023661, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.004980398342013359}, {"id": 643, "seek": 275922, "start": 2768.66, "end": 2773.4599999999996, "text": " But uh, ultimately, uh, what they do is they're in the they're they're in the q&a business", "tokens": [50836, 583, 2232, 11, 6284, 11, 2232, 11, 437, 436, 360, 307, 436, 434, 294, 264, 436, 434, 436, 434, 294, 264, 9505, 5, 64, 1606, 51076], "temperature": 0.0, "avg_logprob": -0.10610297254023661, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.004980398342013359}, {"id": 644, "seek": 275922, "start": 2773.62, "end": 2778.1, "text": " So I basically ask these systems a question or I give them a command and they give me an answer", "tokens": [51084, 407, 286, 1936, 1029, 613, 3652, 257, 1168, 420, 286, 976, 552, 257, 5622, 293, 436, 976, 385, 364, 1867, 51308], "temperature": 0.0, "avg_logprob": -0.10610297254023661, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.004980398342013359}, {"id": 645, "seek": 275922, "start": 2778.74, "end": 2783.2999999999997, "text": " Um, and so I think the next frontier of ai is really how do we design systems that don't just", "tokens": [51340, 3301, 11, 293, 370, 286, 519, 264, 958, 35853, 295, 9783, 307, 534, 577, 360, 321, 1715, 3652, 300, 500, 380, 445, 51568], "temperature": 0.0, "avg_logprob": -0.10610297254023661, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.004980398342013359}, {"id": 646, "seek": 275922, "start": 2783.9399999999996, "end": 2787.4599999999996, "text": " Answer questions, but they actually are the ones that start to ask the questions", "tokens": [51600, 24545, 1651, 11, 457, 436, 767, 366, 264, 2306, 300, 722, 281, 1029, 264, 1651, 51776], "temperature": 0.0, "avg_logprob": -0.10610297254023661, "compression_ratio": 1.8166666666666667, "no_speech_prob": 0.004980398342013359}, {"id": 647, "seek": 278746, "start": 2788.02, "end": 2792.1, "text": " And I think once we can have ai systems that start to ask interesting questions", "tokens": [50392, 400, 286, 519, 1564, 321, 393, 362, 9783, 3652, 300, 722, 281, 1029, 1880, 1651, 50596], "temperature": 0.0, "avg_logprob": -0.13917794404206452, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.0008035708451643586}, {"id": 648, "seek": 278746, "start": 2792.7400000000002, "end": 2798.82, "text": " Um, that's when we start to get closer to I think traditional notions of what uh strong agi might be", "tokens": [50628, 3301, 11, 300, 311, 562, 321, 722, 281, 483, 4966, 281, 286, 519, 5164, 35799, 295, 437, 2232, 2068, 623, 72, 1062, 312, 50932], "temperature": 0.0, "avg_logprob": -0.13917794404206452, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.0008035708451643586}, {"id": 649, "seek": 278746, "start": 2799.14, "end": 2801.14, "text": " Okay, so so again really really interesting now", "tokens": [50948, 1033, 11, 370, 370, 797, 534, 534, 1880, 586, 51048], "temperature": 0.0, "avg_logprob": -0.13917794404206452, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.0008035708451643586}, {"id": 650, "seek": 278746, "start": 2801.46, "end": 2805.46, "text": " So we're getting into agency and and people think that oh you could give a language model agency", "tokens": [51064, 407, 321, 434, 1242, 666, 7934, 293, 293, 561, 519, 300, 1954, 291, 727, 976, 257, 2856, 2316, 7934, 51264], "temperature": 0.0, "avg_logprob": -0.13917794404206452, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.0008035708451643586}, {"id": 651, "seek": 278746, "start": 2805.46, "end": 2808.02, "text": " You just like you know run it in a loop and interesting things will happen", "tokens": [51264, 509, 445, 411, 291, 458, 1190, 309, 294, 257, 6367, 293, 1880, 721, 486, 1051, 51392], "temperature": 0.0, "avg_logprob": -0.13917794404206452, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.0008035708451643586}, {"id": 652, "seek": 278746, "start": 2808.18, "end": 2811.54, "text": " Well, well, that's not true because the whole point of open-endedness is to prove that", "tokens": [51400, 1042, 11, 731, 11, 300, 311, 406, 2074, 570, 264, 1379, 935, 295, 1269, 12, 3502, 1287, 307, 281, 7081, 300, 51568], "temperature": 0.0, "avg_logprob": -0.13917794404206452, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.0008035708451643586}, {"id": 653, "seek": 278746, "start": 2811.86, "end": 2815.46, "text": " Existing systems converge so they don't diverge so they don't accumulate information", "tokens": [51584, 2111, 468, 278, 3652, 41881, 370, 436, 500, 380, 18558, 432, 370, 436, 500, 380, 33384, 1589, 51764], "temperature": 0.0, "avg_logprob": -0.13917794404206452, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.0008035708451643586}, {"id": 654, "seek": 281546, "start": 2815.7, "end": 2819.06, "text": " So we would need to create a kind of agent that like, you know, it would just keep running", "tokens": [50376, 407, 321, 576, 643, 281, 1884, 257, 733, 295, 9461, 300, 411, 11, 291, 458, 11, 309, 576, 445, 1066, 2614, 50544], "temperature": 0.0, "avg_logprob": -0.11843570362437855, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.001978798769414425}, {"id": 655, "seek": 281546, "start": 2819.06, "end": 2823.06, "text": " And it would just keep doing interesting and novel things that would keep accumulating information", "tokens": [50544, 400, 309, 576, 445, 1066, 884, 1880, 293, 7613, 721, 300, 576, 1066, 12989, 12162, 1589, 50744], "temperature": 0.0, "avg_logprob": -0.11843570362437855, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.001978798769414425}, {"id": 656, "seek": 281546, "start": 2823.46, "end": 2828.18, "text": " And I think that the reason why language models don't have agency is because they are essentially", "tokens": [50764, 400, 286, 519, 300, 264, 1778, 983, 2856, 5245, 500, 380, 362, 7934, 307, 570, 436, 366, 4476, 51000], "temperature": 0.0, "avg_logprob": -0.11843570362437855, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.001978798769414425}, {"id": 657, "seek": 281546, "start": 2828.7400000000002, "end": 2834.42, "text": " A low entropy model and what that means is during training a lot of the the sort of like the unnecessary", "tokens": [51028, 316, 2295, 30867, 2316, 293, 437, 300, 1355, 307, 1830, 3097, 257, 688, 295, 264, 264, 1333, 295, 411, 264, 19350, 51312], "temperature": 0.0, "avg_logprob": -0.11843570362437855, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.001978798769414425}, {"id": 658, "seek": 281546, "start": 2834.98, "end": 2840.1, "text": " Um, you know complexity was snipped off. So the models only know about relevant things in the next step", "tokens": [51340, 3301, 11, 291, 458, 14024, 390, 2406, 5529, 766, 13, 407, 264, 5245, 787, 458, 466, 7340, 721, 294, 264, 958, 1823, 51596], "temperature": 0.0, "avg_logprob": -0.11843570362437855, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.001978798769414425}, {"id": 659, "seek": 284010, "start": 2840.1, "end": 2846.18, "text": " What's the next best token and it feels like we would need to have not only a higher entropy search", "tokens": [50364, 708, 311, 264, 958, 1151, 14862, 293, 309, 3417, 411, 321, 576, 643, 281, 362, 406, 787, 257, 2946, 30867, 3164, 50668], "temperature": 0.0, "avg_logprob": -0.1144035448793505, "compression_ratio": 1.7197231833910034, "no_speech_prob": 0.03403059393167496}, {"id": 660, "seek": 284010, "start": 2846.42, "end": 2850.9, "text": " But we would also need to have um a diverse set of models that are actively", "tokens": [50680, 583, 321, 576, 611, 643, 281, 362, 1105, 257, 9521, 992, 295, 5245, 300, 366, 13022, 50904], "temperature": 0.0, "avg_logprob": -0.1144035448793505, "compression_ratio": 1.7197231833910034, "no_speech_prob": 0.03403059393167496}, {"id": 661, "seek": 284010, "start": 2851.62, "end": 2856.74, "text": " Continually learning and and diverging from from each other, but that's just my take. I mean, what do you guys think about that?", "tokens": [50940, 14674, 671, 2539, 293, 293, 18558, 3249, 490, 490, 1184, 661, 11, 457, 300, 311, 445, 452, 747, 13, 286, 914, 11, 437, 360, 291, 1074, 519, 466, 300, 30, 51196], "temperature": 0.0, "avg_logprob": -0.1144035448793505, "compression_ratio": 1.7197231833910034, "no_speech_prob": 0.03403059393167496}, {"id": 662, "seek": 284010, "start": 2857.06, "end": 2858.74, "text": " Yeah, I think that", "tokens": [51212, 865, 11, 286, 519, 300, 51296], "temperature": 0.0, "avg_logprob": -0.1144035448793505, "compression_ratio": 1.7197231833910034, "no_speech_prob": 0.03403059393167496}, {"id": 663, "seek": 284010, "start": 2858.74, "end": 2863.54, "text": " So I guess this relates quite a lot to this idea of like intrinsic motivation, which is something that we utilize in our paper and I guess", "tokens": [51296, 407, 286, 2041, 341, 16155, 1596, 257, 688, 281, 341, 1558, 295, 411, 35698, 12335, 11, 597, 307, 746, 300, 321, 16117, 294, 527, 3035, 293, 286, 2041, 51536], "temperature": 0.0, "avg_logprob": -0.1144035448793505, "compression_ratio": 1.7197231833910034, "no_speech_prob": 0.03403059393167496}, {"id": 664, "seek": 284010, "start": 2864.3399999999997, "end": 2866.3399999999997, "text": " I guess the idea with that is like", "tokens": [51576, 286, 2041, 264, 1558, 365, 300, 307, 411, 51676], "temperature": 0.0, "avg_logprob": -0.1144035448793505, "compression_ratio": 1.7197231833910034, "no_speech_prob": 0.03403059393167496}, {"id": 665, "seek": 286634, "start": 2866.82, "end": 2871.7000000000003, "text": " You know, if we're trying to like gather new data in the environment, like we shouldn't necessarily be constrained to just try and", "tokens": [50388, 509, 458, 11, 498, 321, 434, 1382, 281, 411, 5448, 777, 1412, 294, 264, 2823, 11, 411, 321, 4659, 380, 4725, 312, 38901, 281, 445, 853, 293, 50632], "temperature": 0.0, "avg_logprob": -0.14212751388549805, "compression_ratio": 1.870307167235495, "no_speech_prob": 0.0038234787061810493}, {"id": 666, "seek": 286634, "start": 2871.94, "end": 2874.58, "text": " Gather new data that's like good for a specific task", "tokens": [50644, 39841, 777, 1412, 300, 311, 411, 665, 337, 257, 2685, 5633, 50776], "temperature": 0.0, "avg_logprob": -0.14212751388549805, "compression_ratio": 1.870307167235495, "no_speech_prob": 0.0038234787061810493}, {"id": 667, "seek": 286634, "start": 2876.1800000000003, "end": 2882.02, "text": " And so I guess this kind of you know, so intrinsic motivation basically says I should just gather new information because it's novel", "tokens": [50856, 400, 370, 286, 2041, 341, 733, 295, 291, 458, 11, 370, 35698, 12335, 1936, 1619, 286, 820, 445, 5448, 777, 1589, 570, 309, 311, 7613, 51148], "temperature": 0.0, "avg_logprob": -0.14212751388549805, "compression_ratio": 1.870307167235495, "no_speech_prob": 0.0038234787061810493}, {"id": 668, "seek": 286634, "start": 2883.46, "end": 2887.54, "text": " And things like this and so we can basically like specifically try and gather information that you know", "tokens": [51220, 400, 721, 411, 341, 293, 370, 321, 393, 1936, 411, 4682, 853, 293, 5448, 1589, 300, 291, 458, 51424], "temperature": 0.0, "avg_logprob": -0.14212751388549805, "compression_ratio": 1.870307167235495, "no_speech_prob": 0.0038234787061810493}, {"id": 669, "seek": 286634, "start": 2888.02, "end": 2894.34, "text": " Reduces our uncertainty about the environment and and or similar objectives that that don't rely on some external reward signal", "tokens": [51448, 4477, 84, 887, 527, 15697, 466, 264, 2823, 293, 293, 420, 2531, 15961, 300, 300, 500, 380, 10687, 322, 512, 8320, 7782, 6358, 51764], "temperature": 0.0, "avg_logprob": -0.14212751388549805, "compression_ratio": 1.870307167235495, "no_speech_prob": 0.0038234787061810493}, {"id": 670, "seek": 289434, "start": 2894.6600000000003, "end": 2899.94, "text": " And I think we when you get to the situation where the model is able to like self-improve in the absence of an external reward signal", "tokens": [50380, 400, 286, 519, 321, 562, 291, 483, 281, 264, 2590, 689, 264, 2316, 307, 1075, 281, 411, 2698, 12, 332, 46955, 294, 264, 17145, 295, 364, 8320, 7782, 6358, 50644], "temperature": 0.0, "avg_logprob": -0.09918896993001303, "compression_ratio": 1.9597523219814241, "no_speech_prob": 0.002472385996952653}, {"id": 671, "seek": 289434, "start": 2900.02, "end": 2904.42, "text": " So intrinsic meaning that the the signal for what you should get is just purely generated by the model", "tokens": [50648, 407, 35698, 3620, 300, 264, 264, 6358, 337, 437, 291, 820, 483, 307, 445, 17491, 10833, 538, 264, 2316, 50868], "temperature": 0.0, "avg_logprob": -0.09918896993001303, "compression_ratio": 1.9597523219814241, "no_speech_prob": 0.002472385996952653}, {"id": 672, "seek": 289434, "start": 2904.42, "end": 2906.42, "text": " So it's purely intrinsic to the model", "tokens": [50868, 407, 309, 311, 17491, 35698, 281, 264, 2316, 50968], "temperature": 0.0, "avg_logprob": -0.09918896993001303, "compression_ratio": 1.9597523219814241, "no_speech_prob": 0.002472385996952653}, {"id": 673, "seek": 289434, "start": 2906.6600000000003, "end": 2912.6600000000003, "text": " Um, so I think the situation where you know, you have the model that's able to self-improve without any external signal without a human", "tokens": [50980, 3301, 11, 370, 286, 519, 264, 2590, 689, 291, 458, 11, 291, 362, 264, 2316, 300, 311, 1075, 281, 2698, 12, 332, 46955, 1553, 604, 8320, 6358, 1553, 257, 1952, 51280], "temperature": 0.0, "avg_logprob": -0.09918896993001303, "compression_ratio": 1.9597523219814241, "no_speech_prob": 0.002472385996952653}, {"id": 674, "seek": 289434, "start": 2912.6600000000003, "end": 2916.58, "text": " Having to define what the reward is or what the objective is or this was good data. This was bad data", "tokens": [51280, 10222, 281, 6964, 437, 264, 7782, 307, 420, 437, 264, 10024, 307, 420, 341, 390, 665, 1412, 13, 639, 390, 1578, 1412, 51476], "temperature": 0.0, "avg_logprob": -0.09918896993001303, "compression_ratio": 1.9597523219814241, "no_speech_prob": 0.002472385996952653}, {"id": 675, "seek": 289434, "start": 2917.1400000000003, "end": 2922.02, "text": " Um, I feel like that does feel like a lot closer to the notion of agency because of the fact you don't have kind of some", "tokens": [51504, 3301, 11, 286, 841, 411, 300, 775, 841, 411, 257, 688, 4966, 281, 264, 10710, 295, 7934, 570, 295, 264, 1186, 291, 500, 380, 362, 733, 295, 512, 51748], "temperature": 0.0, "avg_logprob": -0.09918896993001303, "compression_ratio": 1.9597523219814241, "no_speech_prob": 0.002472385996952653}, {"id": 676, "seek": 292202, "start": 2922.34, "end": 2924.58, "text": " External person defining what's good and what's bad?", "tokens": [50380, 48277, 954, 17827, 437, 311, 665, 293, 437, 311, 1578, 30, 50492], "temperature": 0.0, "avg_logprob": -0.13078159756130642, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.0032725229393690825}, {"id": 677, "seek": 292202, "start": 2925.14, "end": 2929.78, "text": " And so yeah, I think like this the like and you also mentioned the word like creativity because I think", "tokens": [50520, 400, 370, 1338, 11, 286, 519, 411, 341, 264, 411, 293, 291, 611, 2835, 264, 1349, 411, 12915, 570, 286, 519, 50752], "temperature": 0.0, "avg_logprob": -0.13078159756130642, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.0032725229393690825}, {"id": 678, "seek": 292202, "start": 2930.18, "end": 2932.18, "text": " At least in the context of things that", "tokens": [50772, 1711, 1935, 294, 264, 4319, 295, 721, 300, 50872], "temperature": 0.0, "avg_logprob": -0.13078159756130642, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.0032725229393690825}, {"id": 679, "seek": 292202, "start": 2932.2599999999998, "end": 2938.2599999999998, "text": " I've done in terms of machine learning or reinforcement learning. I think like intrinsic motivation feels like the closest thing related to creativity", "tokens": [50876, 286, 600, 1096, 294, 2115, 295, 3479, 2539, 420, 29280, 2539, 13, 286, 519, 411, 35698, 12335, 3417, 411, 264, 13699, 551, 4077, 281, 12915, 51176], "temperature": 0.0, "avg_logprob": -0.13078159756130642, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.0032725229393690825}, {"id": 680, "seek": 292202, "start": 2938.58, "end": 2943.94, "text": " So you're basically like trying to gather information because it's novel or because you think it's or the model thinks it's interesting", "tokens": [51192, 407, 291, 434, 1936, 411, 1382, 281, 5448, 1589, 570, 309, 311, 7613, 420, 570, 291, 519, 309, 311, 420, 264, 2316, 7309, 309, 311, 1880, 51460], "temperature": 0.0, "avg_logprob": -0.13078159756130642, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.0032725229393690825}, {"id": 681, "seek": 292202, "start": 2944.34, "end": 2945.78, "text": " rather than because", "tokens": [51480, 2831, 813, 570, 51552], "temperature": 0.0, "avg_logprob": -0.13078159756130642, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.0032725229393690825}, {"id": 682, "seek": 292202, "start": 2945.78, "end": 2947.78, "text": " You know, it satisfies some objective", "tokens": [51552, 509, 458, 11, 309, 44271, 512, 10024, 51652], "temperature": 0.0, "avg_logprob": -0.13078159756130642, "compression_ratio": 1.8305084745762712, "no_speech_prob": 0.0032725229393690825}, {"id": 683, "seek": 294778, "start": 2948.02, "end": 2953.5400000000004, "text": " And so I think we could maybe say like intrinsic motivation is in some sense like an objective for being creative as well", "tokens": [50376, 400, 370, 286, 519, 321, 727, 1310, 584, 411, 35698, 12335, 307, 294, 512, 2020, 411, 364, 10024, 337, 885, 5880, 382, 731, 50652], "temperature": 0.0, "avg_logprob": -0.1377539316813151, "compression_ratio": 1.7152317880794703, "no_speech_prob": 0.008183822967112064}, {"id": 684, "seek": 294778, "start": 2954.1800000000003, "end": 2957.5400000000004, "text": " Um, I don't know if you have any thoughts about this. Yeah, I think I think that uh", "tokens": [50684, 3301, 11, 286, 500, 380, 458, 498, 291, 362, 604, 4598, 466, 341, 13, 865, 11, 286, 519, 286, 519, 300, 2232, 50852], "temperature": 0.0, "avg_logprob": -0.1377539316813151, "compression_ratio": 1.7152317880794703, "no_speech_prob": 0.008183822967112064}, {"id": 685, "seek": 294778, "start": 2958.26, "end": 2963.94, "text": " It's I think there's definitely a hugely deep connection between intrinsic motivation and creativity. Um", "tokens": [50888, 467, 311, 286, 519, 456, 311, 2138, 257, 27417, 2452, 4984, 1296, 35698, 12335, 293, 12915, 13, 3301, 51172], "temperature": 0.0, "avg_logprob": -0.1377539316813151, "compression_ratio": 1.7152317880794703, "no_speech_prob": 0.008183822967112064}, {"id": 686, "seek": 294778, "start": 2964.5, "end": 2967.86, "text": " In the literature intrinsic motivations also sometimes called artificial curiosity", "tokens": [51200, 682, 264, 10394, 35698, 39034, 611, 2171, 1219, 11677, 18769, 51368], "temperature": 0.0, "avg_logprob": -0.1377539316813151, "compression_ratio": 1.7152317880794703, "no_speech_prob": 0.008183822967112064}, {"id": 687, "seek": 294778, "start": 2967.94, "end": 2970.42, "text": " So this is a term that was coined by Juergen Schmidt-Huber", "tokens": [51372, 407, 341, 307, 257, 1433, 300, 390, 45222, 538, 508, 5486, 1766, 42621, 12, 39, 10261, 51496], "temperature": 0.0, "avg_logprob": -0.1377539316813151, "compression_ratio": 1.7152317880794703, "no_speech_prob": 0.008183822967112064}, {"id": 688, "seek": 294778, "start": 2970.9, "end": 2973.5400000000004, "text": " Could you could you explain it just what it is? Yeah, so oh, yeah", "tokens": [51520, 7497, 291, 727, 291, 2903, 309, 445, 437, 309, 307, 30, 865, 11, 370, 1954, 11, 1338, 51652], "temperature": 0.0, "avg_logprob": -0.1377539316813151, "compression_ratio": 1.7152317880794703, "no_speech_prob": 0.008183822967112064}, {"id": 689, "seek": 297354, "start": 2973.54, "end": 2977.94, "text": " So taking a step back intrinsic motivation is essentially um in reinforcement learning", "tokens": [50364, 407, 1940, 257, 1823, 646, 35698, 12335, 307, 4476, 1105, 294, 29280, 2539, 50584], "temperature": 0.0, "avg_logprob": -0.11314826641442641, "compression_ratio": 1.926356589147287, "no_speech_prob": 0.00348248821683228}, {"id": 690, "seek": 297354, "start": 2977.94, "end": 2983.3, "text": " We train on reward signals and as mark was saying, um, we typically train on external reward signal by external", "tokens": [50584, 492, 3847, 322, 7782, 12354, 293, 382, 1491, 390, 1566, 11, 1105, 11, 321, 5850, 3847, 322, 8320, 7782, 6358, 538, 8320, 50852], "temperature": 0.0, "avg_logprob": -0.11314826641442641, "compression_ratio": 1.926356589147287, "no_speech_prob": 0.00348248821683228}, {"id": 691, "seek": 297354, "start": 2983.3, "end": 2989.14, "text": " We mean that this is a task based reward. So this is um external in the sense that something outside of the agent", "tokens": [50852, 492, 914, 300, 341, 307, 257, 5633, 2361, 7782, 13, 407, 341, 307, 1105, 8320, 294, 264, 2020, 300, 746, 2380, 295, 264, 9461, 51144], "temperature": 0.0, "avg_logprob": -0.11314826641442641, "compression_ratio": 1.926356589147287, "no_speech_prob": 0.00348248821683228}, {"id": 692, "seek": 297354, "start": 2989.14, "end": 2993.7799999999997, "text": " That's learning like the human system designer decided that this is what the reward signal is for the task", "tokens": [51144, 663, 311, 2539, 411, 264, 1952, 1185, 11795, 3047, 300, 341, 307, 437, 264, 7782, 6358, 307, 337, 264, 5633, 51376], "temperature": 0.0, "avg_logprob": -0.11314826641442641, "compression_ratio": 1.926356589147287, "no_speech_prob": 0.00348248821683228}, {"id": 693, "seek": 297354, "start": 2994.1, "end": 2998.74, "text": " Uh intrinsic means that we want to we don't design directly the reward signal", "tokens": [51392, 4019, 35698, 1355, 300, 321, 528, 281, 321, 500, 380, 1715, 3838, 264, 7782, 6358, 51624], "temperature": 0.0, "avg_logprob": -0.11314826641442641, "compression_ratio": 1.926356589147287, "no_speech_prob": 0.00348248821683228}, {"id": 694, "seek": 299874, "start": 2998.8199999999997, "end": 3001.7799999999997, "text": " But we're actually using some aspect of the model itself", "tokens": [50368, 583, 321, 434, 767, 1228, 512, 4171, 295, 264, 2316, 2564, 50516], "temperature": 0.0, "avg_logprob": -0.1043738957607385, "compression_ratio": 1.76875, "no_speech_prob": 0.05918312817811966}, {"id": 695, "seek": 299874, "start": 3002.18, "end": 3006.8999999999996, "text": " In order to drive the models learning forward. And so one example of this could be prediction error", "tokens": [50536, 682, 1668, 281, 3332, 264, 5245, 2539, 2128, 13, 400, 370, 472, 1365, 295, 341, 727, 312, 17630, 6713, 50772], "temperature": 0.0, "avg_logprob": -0.1043738957607385, "compression_ratio": 1.76875, "no_speech_prob": 0.05918312817811966}, {"id": 696, "seek": 299874, "start": 3007.06, "end": 3012.4199999999996, "text": " So if the model, uh has a large prediction error on a certain task like averaged over each time step", "tokens": [50780, 407, 498, 264, 2316, 11, 2232, 575, 257, 2416, 17630, 6713, 322, 257, 1629, 5633, 411, 18247, 2980, 670, 1184, 565, 1823, 51048], "temperature": 0.0, "avg_logprob": -0.1043738957607385, "compression_ratio": 1.76875, "no_speech_prob": 0.05918312817811966}, {"id": 697, "seek": 299874, "start": 3012.66, "end": 3019.2999999999997, "text": " We can use that as a reward signal and say, hey, you want to visit more parts of the environment where you're bad at predicting", "tokens": [51060, 492, 393, 764, 300, 382, 257, 7782, 6358, 293, 584, 11, 4177, 11, 291, 528, 281, 3441, 544, 3166, 295, 264, 2823, 689, 291, 434, 1578, 412, 32884, 51392], "temperature": 0.0, "avg_logprob": -0.1043738957607385, "compression_ratio": 1.76875, "no_speech_prob": 0.05918312817811966}, {"id": 698, "seek": 299874, "start": 3019.62, "end": 3023.7, "text": " Um, how the state will transition when you act in that part of the environment. And so", "tokens": [51408, 3301, 11, 577, 264, 1785, 486, 6034, 562, 291, 605, 294, 300, 644, 295, 264, 2823, 13, 400, 370, 51612], "temperature": 0.0, "avg_logprob": -0.1043738957607385, "compression_ratio": 1.76875, "no_speech_prob": 0.05918312817811966}, {"id": 699, "seek": 299874, "start": 3024.2599999999998, "end": 3028.2599999999998, "text": " Uh, as you can see, this is very similar to maybe like intuitive notions of what curiosity is", "tokens": [51640, 4019, 11, 382, 291, 393, 536, 11, 341, 307, 588, 2531, 281, 1310, 411, 21769, 35799, 295, 437, 18769, 307, 51840], "temperature": 0.0, "avg_logprob": -0.1043738957607385, "compression_ratio": 1.76875, "no_speech_prob": 0.05918312817811966}, {"id": 700, "seek": 302874, "start": 3028.8999999999996, "end": 3030.8999999999996, "text": " Curiosity and different forms of play", "tokens": [50372, 48998, 293, 819, 6422, 295, 862, 50472], "temperature": 0.0, "avg_logprob": -0.12586176711901098, "compression_ratio": 1.853046594982079, "no_speech_prob": 0.0024721829686313868}, {"id": 701, "seek": 302874, "start": 3030.9799999999996, "end": 3035.4599999999996, "text": " Um in the psychology literature, a lot of people actually argue that, you know, different forms of play", "tokens": [50476, 3301, 294, 264, 15105, 10394, 11, 257, 688, 295, 561, 767, 9695, 300, 11, 291, 458, 11, 819, 6422, 295, 862, 50700], "temperature": 0.0, "avg_logprob": -0.12586176711901098, "compression_ratio": 1.853046594982079, "no_speech_prob": 0.0024721829686313868}, {"id": 702, "seek": 302874, "start": 3035.9399999999996, "end": 3041.8599999999997, "text": " In curiosity really they they amount to you can model these behaviors as essentially a person trying to", "tokens": [50724, 682, 18769, 534, 436, 436, 2372, 281, 291, 393, 2316, 613, 15501, 382, 4476, 257, 954, 1382, 281, 51020], "temperature": 0.0, "avg_logprob": -0.12586176711901098, "compression_ratio": 1.853046594982079, "no_speech_prob": 0.0024721829686313868}, {"id": 703, "seek": 302874, "start": 3042.5, "end": 3045.7799999999997, "text": " Engage in activities where, you know, they're not very good at predicting the outcome", "tokens": [51052, 2469, 609, 294, 5354, 689, 11, 291, 458, 11, 436, 434, 406, 588, 665, 412, 32884, 264, 9700, 51216], "temperature": 0.0, "avg_logprob": -0.12586176711901098, "compression_ratio": 1.853046594982079, "no_speech_prob": 0.0024721829686313868}, {"id": 704, "seek": 302874, "start": 3045.9399999999996, "end": 3047.54, "text": " And that's kind of what makes you could argue", "tokens": [51224, 400, 300, 311, 733, 295, 437, 1669, 291, 727, 9695, 51304], "temperature": 0.0, "avg_logprob": -0.12586176711901098, "compression_ratio": 1.853046594982079, "no_speech_prob": 0.0024721829686313868}, {"id": 705, "seek": 302874, "start": 3047.54, "end": 3053.8599999999997, "text": " That's kind of what makes certain kinds of entertainment fun because or entertaining because you can't actually predict what will happen", "tokens": [51304, 663, 311, 733, 295, 437, 1669, 1629, 3685, 295, 12393, 1019, 570, 420, 20402, 570, 291, 393, 380, 767, 6069, 437, 486, 1051, 51620], "temperature": 0.0, "avg_logprob": -0.12586176711901098, "compression_ratio": 1.853046594982079, "no_speech_prob": 0.0024721829686313868}, {"id": 706, "seek": 302874, "start": 3054.18, "end": 3055.14, "text": " um", "tokens": [51636, 1105, 51684], "temperature": 0.0, "avg_logprob": -0.12586176711901098, "compression_ratio": 1.853046594982079, "no_speech_prob": 0.0024721829686313868}, {"id": 707, "seek": 305514, "start": 3055.14, "end": 3059.2999999999997, "text": " You know in a few frames of the movie like a movie wouldn't be very interesting or a book would not be very interesting", "tokens": [50364, 509, 458, 294, 257, 1326, 12083, 295, 264, 3169, 411, 257, 3169, 2759, 380, 312, 588, 1880, 420, 257, 1446, 576, 406, 312, 588, 1880, 50572], "temperature": 0.0, "avg_logprob": -0.09997834014892579, "compression_ratio": 1.755287009063444, "no_speech_prob": 0.0019858272280544043}, {"id": 708, "seek": 305514, "start": 3059.54, "end": 3063.46, "text": " If you can predict what will happen in the rest of the book just by reading the first few pages", "tokens": [50584, 759, 291, 393, 6069, 437, 486, 1051, 294, 264, 1472, 295, 264, 1446, 445, 538, 3760, 264, 700, 1326, 7183, 50780], "temperature": 0.0, "avg_logprob": -0.09997834014892579, "compression_ratio": 1.755287009063444, "no_speech_prob": 0.0019858272280544043}, {"id": 709, "seek": 305514, "start": 3063.8599999999997, "end": 3068.3399999999997, "text": " Uh, and so intrinsic motivation is really saying let's guide the model towards parts of the environment or the world", "tokens": [50800, 4019, 11, 293, 370, 35698, 12335, 307, 534, 1566, 718, 311, 5934, 264, 2316, 3030, 3166, 295, 264, 2823, 420, 264, 1002, 51024], "temperature": 0.0, "avg_logprob": -0.09997834014892579, "compression_ratio": 1.755287009063444, "no_speech_prob": 0.0019858272280544043}, {"id": 710, "seek": 305514, "start": 3068.58, "end": 3071.2999999999997, "text": " Or experiences where it's similarly unpredictable", "tokens": [51036, 1610, 5235, 689, 309, 311, 14138, 31160, 51172], "temperature": 0.0, "avg_logprob": -0.09997834014892579, "compression_ratio": 1.755287009063444, "no_speech_prob": 0.0019858272280544043}, {"id": 711, "seek": 305514, "start": 3071.7799999999997, "end": 3076.18, "text": " Stanley speaks about this this concept of deception or we call it the false compass", "tokens": [51196, 28329, 10789, 466, 341, 341, 3410, 295, 40451, 420, 321, 818, 309, 264, 7908, 10707, 51416], "temperature": 0.0, "avg_logprob": -0.09997834014892579, "compression_ratio": 1.755287009063444, "no_speech_prob": 0.0019858272280544043}, {"id": 712, "seek": 305514, "start": 3076.5, "end": 3082.8199999999997, "text": " Which is this idea that any objective and even you you could say exploring all of the search space is an objective", "tokens": [51432, 3013, 307, 341, 1558, 300, 604, 10024, 293, 754, 291, 291, 727, 584, 12736, 439, 295, 264, 3164, 1901, 307, 364, 10024, 51748], "temperature": 0.0, "avg_logprob": -0.09997834014892579, "compression_ratio": 1.755287009063444, "no_speech_prob": 0.0019858272280544043}, {"id": 713, "seek": 308282, "start": 3082.82, "end": 3086.26, "text": " So he said every every objective has deception and if you monotonically", "tokens": [50364, 407, 415, 848, 633, 633, 10024, 575, 40451, 293, 498, 291, 1108, 27794, 984, 50536], "temperature": 0.0, "avg_logprob": -0.18486921295864892, "compression_ratio": 1.738709677419355, "no_speech_prob": 0.006780064664781094}, {"id": 714, "seek": 308282, "start": 3086.6600000000003, "end": 3089.7000000000003, "text": " Optimize any objective you will always lead into you know, like a", "tokens": [50556, 35013, 1125, 604, 10024, 291, 486, 1009, 1477, 666, 291, 458, 11, 411, 257, 50708], "temperature": 0.0, "avg_logprob": -0.18486921295864892, "compression_ratio": 1.738709677419355, "no_speech_prob": 0.006780064664781094}, {"id": 715, "seek": 308282, "start": 3090.34, "end": 3097.3, "text": " Deceptive part of the search space, but then like the counter argument is okay. Well, let's let's not um, let's not have any principles for doing the", "tokens": [50740, 1346, 1336, 488, 644, 295, 264, 3164, 1901, 11, 457, 550, 411, 264, 5682, 6770, 307, 1392, 13, 1042, 11, 718, 311, 718, 311, 406, 1105, 11, 718, 311, 406, 362, 604, 9156, 337, 884, 264, 51088], "temperature": 0.0, "avg_logprob": -0.18486921295864892, "compression_ratio": 1.738709677419355, "no_speech_prob": 0.006780064664781094}, {"id": 716, "seek": 308282, "start": 3098.02, "end": 3103.2200000000003, "text": " You know the the exploration. Let's just do something completely random and that doesn't seem very good", "tokens": [51124, 509, 458, 264, 264, 16197, 13, 961, 311, 445, 360, 746, 2584, 4974, 293, 300, 1177, 380, 1643, 588, 665, 51384], "temperature": 0.0, "avg_logprob": -0.18486921295864892, "compression_ratio": 1.738709677419355, "no_speech_prob": 0.006780064664781094}, {"id": 717, "seek": 308282, "start": 3103.38, "end": 3111.2200000000003, "text": " So so then, you know, there's this concept of well, how do I how do I imbue some concept of what's interesting without falling victim to deception?", "tokens": [51392, 407, 370, 550, 11, 291, 458, 11, 456, 311, 341, 3410, 295, 731, 11, 577, 360, 286, 577, 360, 286, 566, 65, 622, 512, 3410, 295, 437, 311, 1880, 1553, 7440, 6760, 281, 40451, 30, 51784], "temperature": 0.0, "avg_logprob": -0.18486921295864892, "compression_ratio": 1.738709677419355, "no_speech_prob": 0.006780064664781094}, {"id": 718, "seek": 311122, "start": 3111.2999999999997, "end": 3117.22, "text": " Yes, so ken stanley, uh has a famous essay in the realm of open-endedness where he points out", "tokens": [50368, 1079, 11, 370, 18787, 27984, 3420, 11, 2232, 575, 257, 4618, 16238, 294, 264, 15355, 295, 1269, 12, 3502, 1287, 689, 415, 2793, 484, 50664], "temperature": 0.0, "avg_logprob": -0.1530195765134667, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.0008828635327517986}, {"id": 719, "seek": 311122, "start": 3118.58, "end": 3120.58, "text": " That this notion of interestingness", "tokens": [50732, 663, 341, 10710, 295, 1880, 1287, 50832], "temperature": 0.0, "avg_logprob": -0.1530195765134667, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.0008828635327517986}, {"id": 720, "seek": 311122, "start": 3121.22, "end": 3126.1, "text": " Is uh, ultimately a subjective concept and so even in the case of intrinsic motivation", "tokens": [50864, 1119, 2232, 11, 6284, 257, 25972, 3410, 293, 370, 754, 294, 264, 1389, 295, 35698, 12335, 51108], "temperature": 0.0, "avg_logprob": -0.1530195765134667, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.0008828635327517986}, {"id": 721, "seek": 311122, "start": 3126.1, "end": 3129.2999999999997, "text": " Which I think is you know in practice we can get a lot of mileage out of this", "tokens": [51108, 3013, 286, 519, 307, 291, 458, 294, 3124, 321, 393, 483, 257, 688, 295, 43121, 484, 295, 341, 51268], "temperature": 0.0, "avg_logprob": -0.1530195765134667, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.0008828635327517986}, {"id": 722, "seek": 311122, "start": 3129.9399999999996, "end": 3134.5, "text": " And we've seen this in a lot of domains where exploration helps a lot like even in the wakeer paper", "tokens": [51300, 400, 321, 600, 1612, 341, 294, 257, 688, 295, 25514, 689, 16197, 3665, 257, 688, 411, 754, 294, 264, 6634, 260, 3035, 51528], "temperature": 0.0, "avg_logprob": -0.1530195765134667, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.0008828635327517986}, {"id": 723, "seek": 311122, "start": 3134.5, "end": 3139.62, "text": " it's largely founded on this idea on how we exploit intrinsic motivation for learning world models, but", "tokens": [51528, 309, 311, 11611, 13234, 322, 341, 1558, 322, 577, 321, 25924, 35698, 12335, 337, 2539, 1002, 5245, 11, 457, 51784], "temperature": 0.0, "avg_logprob": -0.1530195765134667, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.0008828635327517986}, {"id": 724, "seek": 313962, "start": 3140.42, "end": 3143.14, "text": " Ultimately, you know, these these model based", "tokens": [50404, 23921, 11, 291, 458, 11, 613, 613, 2316, 2361, 50540], "temperature": 0.0, "avg_logprob": -0.1529717036655971, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0006666365079581738}, {"id": 725, "seek": 313962, "start": 3143.7799999999997, "end": 3150.3399999999997, "text": " Measures of intrinsic motivation. They are by definition based on the particular model at play and so", "tokens": [50572, 1923, 20044, 295, 35698, 12335, 13, 814, 366, 538, 7123, 2361, 322, 264, 1729, 2316, 412, 862, 293, 370, 50900], "temperature": 0.0, "avg_logprob": -0.1529717036655971, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0006666365079581738}, {"id": 726, "seek": 313962, "start": 3151.22, "end": 3156.74, "text": " At some point, you know, you're you're starting to over fit to what that specific model finds interesting", "tokens": [50944, 1711, 512, 935, 11, 291, 458, 11, 291, 434, 291, 434, 2891, 281, 670, 3318, 281, 437, 300, 2685, 2316, 10704, 1880, 51220], "temperature": 0.0, "avg_logprob": -0.1529717036655971, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0006666365079581738}, {"id": 727, "seek": 313962, "start": 3157.22, "end": 3162.02, "text": " And of course what that model finds interesting if your measure of interestingness is something like a prediction error", "tokens": [51244, 400, 295, 1164, 437, 300, 2316, 10704, 1880, 498, 428, 3481, 295, 1880, 1287, 307, 746, 411, 257, 17630, 6713, 51484], "temperature": 0.0, "avg_logprob": -0.1529717036655971, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0006666365079581738}, {"id": 728, "seek": 313962, "start": 3163.14, "end": 3169.46, "text": " Is going to be a function of you know, the specific architecture of the model the actual inductive biases of that model", "tokens": [51540, 1119, 516, 281, 312, 257, 2445, 295, 291, 458, 11, 264, 2685, 9482, 295, 264, 2316, 264, 3539, 31612, 488, 32152, 295, 300, 2316, 51856], "temperature": 0.0, "avg_logprob": -0.1529717036655971, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0006666365079581738}, {"id": 729, "seek": 316962, "start": 3170.1, "end": 3173.8599999999997, "text": " The capacity of that model to learn and so you could imagine a model where you know", "tokens": [50388, 440, 6042, 295, 300, 2316, 281, 1466, 293, 370, 291, 727, 3811, 257, 2316, 689, 291, 458, 50576], "temperature": 0.0, "avg_logprob": -0.09095195929209392, "compression_ratio": 1.9580838323353293, "no_speech_prob": 0.0018672042060643435}, {"id": 730, "seek": 316962, "start": 3174.2599999999998, "end": 3177.7799999999997, "text": " At the beginning it's looking for lots of interesting parts of a particular video game environment", "tokens": [50596, 1711, 264, 2863, 309, 311, 1237, 337, 3195, 295, 1880, 3166, 295, 257, 1729, 960, 1216, 2823, 50772], "temperature": 0.0, "avg_logprob": -0.09095195929209392, "compression_ratio": 1.9580838323353293, "no_speech_prob": 0.0018672042060643435}, {"id": 731, "seek": 316962, "start": 3177.8599999999997, "end": 3180.8199999999997, "text": " But at some point, you know, it might saturate what it can represent", "tokens": [50776, 583, 412, 512, 935, 11, 291, 458, 11, 309, 1062, 21160, 473, 437, 309, 393, 2906, 50924], "temperature": 0.0, "avg_logprob": -0.09095195929209392, "compression_ratio": 1.9580838323353293, "no_speech_prob": 0.0018672042060643435}, {"id": 732, "seek": 316962, "start": 3181.06, "end": 3184.02, "text": " And what it can learn and at some point it might start to find things", "tokens": [50936, 400, 437, 309, 393, 1466, 293, 412, 512, 935, 309, 1062, 722, 281, 915, 721, 51084], "temperature": 0.0, "avg_logprob": -0.09095195929209392, "compression_ratio": 1.9580838323353293, "no_speech_prob": 0.0018672042060643435}, {"id": 733, "seek": 316962, "start": 3184.1, "end": 3187.94, "text": " It's explored before interesting just because it's starting to forget those parts of the environment", "tokens": [51088, 467, 311, 24016, 949, 1880, 445, 570, 309, 311, 2891, 281, 2870, 729, 3166, 295, 264, 2823, 51280], "temperature": 0.0, "avg_logprob": -0.09095195929209392, "compression_ratio": 1.9580838323353293, "no_speech_prob": 0.0018672042060643435}, {"id": 734, "seek": 316962, "start": 3188.1, "end": 3192.02, "text": " You know if you have like a very rich stream of different kinds of environments that it's exploring", "tokens": [51288, 509, 458, 498, 291, 362, 411, 257, 588, 4593, 4309, 295, 819, 3685, 295, 12388, 300, 309, 311, 12736, 51484], "temperature": 0.0, "avg_logprob": -0.09095195929209392, "compression_ratio": 1.9580838323353293, "no_speech_prob": 0.0018672042060643435}, {"id": 735, "seek": 316962, "start": 3192.18, "end": 3198.18, "text": " So ultimately this is like an example of deception because now it's like I I think that my model is the model thinks it's exploring", "tokens": [51492, 407, 6284, 341, 307, 411, 364, 1365, 295, 40451, 570, 586, 309, 311, 411, 286, 286, 519, 300, 452, 2316, 307, 264, 2316, 7309, 309, 311, 12736, 51792], "temperature": 0.0, "avg_logprob": -0.09095195929209392, "compression_ratio": 1.9580838323353293, "no_speech_prob": 0.0018672042060643435}, {"id": 736, "seek": 319818, "start": 3198.2599999999998, "end": 3201.2999999999997, "text": " Parts of the environment that it finds interesting based on this prediction error", "tokens": [50368, 4100, 82, 295, 264, 2823, 300, 309, 10704, 1880, 2361, 322, 341, 17630, 6713, 50520], "temperature": 0.0, "avg_logprob": -0.1098269830670273, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.008845659904181957}, {"id": 737, "seek": 319818, "start": 3201.46, "end": 3206.74, "text": " But ultimately it might actually start to go back to other parts of the environment because of issues of model capacity", "tokens": [50528, 583, 6284, 309, 1062, 767, 722, 281, 352, 646, 281, 661, 3166, 295, 264, 2823, 570, 295, 2663, 295, 2316, 6042, 50792], "temperature": 0.0, "avg_logprob": -0.1098269830670273, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.008845659904181957}, {"id": 738, "seek": 319818, "start": 3206.8199999999997, "end": 3209.8599999999997, "text": " And another really famous example of this issue would be like the noisy tv", "tokens": [50796, 400, 1071, 534, 4618, 1365, 295, 341, 2734, 576, 312, 411, 264, 24518, 16364, 50948], "temperature": 0.0, "avg_logprob": -0.1098269830670273, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.008845659904181957}, {"id": 739, "seek": 319818, "start": 3210.02, "end": 3213.22, "text": " So like if your environment has you know, this this uh", "tokens": [50956, 407, 411, 498, 428, 2823, 575, 291, 458, 11, 341, 341, 2232, 51116], "temperature": 0.0, "avg_logprob": -0.1098269830670273, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.008845659904181957}, {"id": 740, "seek": 319818, "start": 3213.8599999999997, "end": 3218.8199999999997, "text": " Noisy tv where it's just showing random noise random rgb pixels. Um, you know, that's", "tokens": [51148, 883, 14169, 16364, 689, 309, 311, 445, 4099, 4974, 5658, 4974, 367, 70, 65, 18668, 13, 3301, 11, 291, 458, 11, 300, 311, 51396], "temperature": 0.0, "avg_logprob": -0.1098269830670273, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.008845659904181957}, {"id": 741, "seek": 319818, "start": 3219.46, "end": 3222.2599999999998, "text": " You know, that's not something you can actually predict because it's just noise", "tokens": [51428, 509, 458, 11, 300, 311, 406, 746, 291, 393, 767, 6069, 570, 309, 311, 445, 5658, 51568], "temperature": 0.0, "avg_logprob": -0.1098269830670273, "compression_ratio": 1.8272058823529411, "no_speech_prob": 0.008845659904181957}, {"id": 742, "seek": 322226, "start": 3222.5, "end": 3228.1000000000004, "text": " And so the model if your intrinsic motivation is really just to search for novelty in the form of prediction error", "tokens": [50376, 400, 370, 264, 2316, 498, 428, 35698, 12335, 307, 534, 445, 281, 3164, 337, 44805, 294, 264, 1254, 295, 17630, 6713, 50656], "temperature": 0.0, "avg_logprob": -0.1252961617928964, "compression_ratio": 1.8083832335329342, "no_speech_prob": 0.03953762352466583}, {"id": 743, "seek": 322226, "start": 3228.26, "end": 3233.86, "text": " It might just start staring at this tv forever because it's something that it just can't predict and I know just by looking at that tv", "tokens": [50664, 467, 1062, 445, 722, 18043, 412, 341, 16364, 5680, 570, 309, 311, 746, 300, 309, 445, 393, 380, 6069, 293, 286, 458, 445, 538, 1237, 412, 300, 16364, 50944], "temperature": 0.0, "avg_logprob": -0.1252961617928964, "compression_ratio": 1.8083832335329342, "no_speech_prob": 0.03953762352466583}, {"id": 744, "seek": 322226, "start": 3233.94, "end": 3240.5, "text": " It'll be maximizing its prediction error. Yeah. Yeah, it's so interesting. Um, so so just coming into rich stuff in a little bit", "tokens": [50948, 467, 603, 312, 5138, 3319, 1080, 17630, 6713, 13, 865, 13, 865, 11, 309, 311, 370, 1880, 13, 3301, 11, 370, 370, 445, 1348, 666, 4593, 1507, 294, 257, 707, 857, 51276], "temperature": 0.0, "avg_logprob": -0.1252961617928964, "compression_ratio": 1.8083832335329342, "no_speech_prob": 0.03953762352466583}, {"id": 745, "seek": 322226, "start": 3240.5, "end": 3246.42, "text": " So he had this idea called um reward is enough and and essentially that doesn't make in the case that you know", "tokens": [51276, 407, 415, 632, 341, 1558, 1219, 1105, 7782, 307, 1547, 293, 293, 4476, 300, 1177, 380, 652, 294, 264, 1389, 300, 291, 458, 51572], "temperature": 0.0, "avg_logprob": -0.1252961617928964, "compression_ratio": 1.8083832335329342, "no_speech_prob": 0.03953762352466583}, {"id": 746, "seek": 322226, "start": 3246.5, "end": 3251.7000000000003, "text": " Just using um implicit uh motivation all the stuff that that you've just been speaking about using this trajectory", "tokens": [51576, 1449, 1228, 1105, 26947, 2232, 12335, 439, 264, 1507, 300, 300, 291, 600, 445, 668, 4124, 466, 1228, 341, 21512, 51836], "temperature": 0.0, "avg_logprob": -0.1252961617928964, "compression_ratio": 1.8083832335329342, "no_speech_prob": 0.03953762352466583}, {"id": 747, "seek": 325170, "start": 3252.18, "end": 3255.9399999999996, "text": " You know optimization process that we can do everything we need to do", "tokens": [50388, 509, 458, 19618, 1399, 300, 321, 393, 360, 1203, 321, 643, 281, 360, 50576], "temperature": 0.0, "avg_logprob": -0.1484211798637144, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002348117297515273}, {"id": 748, "seek": 325170, "start": 3256.2599999999998, "end": 3262.02, "text": " And in in your paper, you're kind of making an argument similar to what lakuna has been making for years about self supervised image learning", "tokens": [50592, 400, 294, 294, 428, 3035, 11, 291, 434, 733, 295, 1455, 364, 6770, 2531, 281, 437, 287, 514, 5051, 575, 668, 1455, 337, 924, 466, 2698, 46533, 3256, 2539, 50880], "temperature": 0.0, "avg_logprob": -0.1484211798637144, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002348117297515273}, {"id": 749, "seek": 325170, "start": 3262.02, "end": 3265.7, "text": " That what we should do guys is let's let's kind of pre-train a base model", "tokens": [50880, 663, 437, 321, 820, 360, 1074, 307, 718, 311, 718, 311, 733, 295, 659, 12, 83, 7146, 257, 3096, 2316, 51064], "temperature": 0.0, "avg_logprob": -0.1484211798637144, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002348117297515273}, {"id": 750, "seek": 325170, "start": 3266.02, "end": 3269.06, "text": " So this model um understands environmental dynamics really well", "tokens": [51080, 407, 341, 2316, 1105, 15146, 8303, 15679, 534, 731, 51232], "temperature": 0.0, "avg_logprob": -0.1484211798637144, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002348117297515273}, {"id": 751, "seek": 325170, "start": 3269.3799999999997, "end": 3272.8199999999997, "text": " And then we stick a reward in there and and we build um agents after that", "tokens": [51248, 400, 550, 321, 2897, 257, 7782, 294, 456, 293, 293, 321, 1322, 1105, 12554, 934, 300, 51420], "temperature": 0.0, "avg_logprob": -0.1484211798637144, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002348117297515273}, {"id": 752, "seek": 325170, "start": 3272.8999999999996, "end": 3278.5, "text": " So does it in any way reinforce or pun intended uh satan or or do you think it's still complimentary?", "tokens": [51424, 407, 775, 309, 294, 604, 636, 22634, 420, 4468, 10226, 2232, 3227, 282, 420, 420, 360, 291, 519, 309, 311, 920, 47162, 30, 51704], "temperature": 0.0, "avg_logprob": -0.1484211798637144, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002348117297515273}, {"id": 753, "seek": 327850, "start": 3279.14, "end": 3284.5, "text": " I think it's still complimentary at least if I understand the the meaning of the reward is enough paper because my understanding of that", "tokens": [50396, 286, 519, 309, 311, 920, 47162, 412, 1935, 498, 286, 1223, 264, 264, 3620, 295, 264, 7782, 307, 1547, 3035, 570, 452, 3701, 295, 300, 50664], "temperature": 0.0, "avg_logprob": -0.11267079805072985, "compression_ratio": 1.8454258675078865, "no_speech_prob": 0.0012447218177840114}, {"id": 754, "seek": 327850, "start": 3285.06, "end": 3288.58, "text": " Um line of thought is basically saying that you know, we can kind of specify", "tokens": [50692, 3301, 1622, 295, 1194, 307, 1936, 1566, 300, 291, 458, 11, 321, 393, 733, 295, 16500, 50868], "temperature": 0.0, "avg_logprob": -0.11267079805072985, "compression_ratio": 1.8454258675078865, "no_speech_prob": 0.0012447218177840114}, {"id": 755, "seek": 327850, "start": 3289.14, "end": 3294.5, "text": " You know any tasks that we might want an intelligent agent to do as optimizing a reward in some like mdp or promdp", "tokens": [50896, 509, 458, 604, 9608, 300, 321, 1062, 528, 364, 13232, 9461, 281, 360, 382, 40425, 257, 7782, 294, 512, 411, 275, 67, 79, 420, 2234, 67, 79, 51164], "temperature": 0.0, "avg_logprob": -0.11267079805072985, "compression_ratio": 1.8454258675078865, "no_speech_prob": 0.0012447218177840114}, {"id": 756, "seek": 327850, "start": 3294.58, "end": 3298.74, "text": " So market decision process or something like that and I think our work isn't contrary to that in the sense of like", "tokens": [51168, 407, 2142, 3537, 1399, 420, 746, 411, 300, 293, 286, 519, 527, 589, 1943, 380, 19506, 281, 300, 294, 264, 2020, 295, 411, 51376], "temperature": 0.0, "avg_logprob": -0.11267079805072985, "compression_ratio": 1.8454258675078865, "no_speech_prob": 0.0012447218177840114}, {"id": 757, "seek": 327850, "start": 3299.54, "end": 3305.62, "text": " You know, I do think that that probably is a sufficient framework to be able to model any any kind of behavior that we might want an agent to", "tokens": [51416, 509, 458, 11, 286, 360, 519, 300, 300, 1391, 307, 257, 11563, 8388, 281, 312, 1075, 281, 2316, 604, 604, 733, 295, 5223, 300, 321, 1062, 528, 364, 9461, 281, 51720], "temperature": 0.0, "avg_logprob": -0.11267079805072985, "compression_ratio": 1.8454258675078865, "no_speech_prob": 0.0012447218177840114}, {"id": 758, "seek": 330562, "start": 3305.62, "end": 3310.1, "text": " Do but I think when it comes to actually like practically implementing that idea. There's a lot of difficulties", "tokens": [50364, 1144, 457, 286, 519, 562, 309, 1487, 281, 767, 411, 15667, 18114, 300, 1558, 13, 821, 311, 257, 688, 295, 14399, 50588], "temperature": 0.0, "avg_logprob": -0.08563769052899073, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.014955279417335987}, {"id": 759, "seek": 330562, "start": 3310.5, "end": 3315.54, "text": " So the first one might be um, you know, how do we even specify that reward function?", "tokens": [50608, 407, 264, 700, 472, 1062, 312, 1105, 11, 291, 458, 11, 577, 360, 321, 754, 16500, 300, 7782, 2445, 30, 50860], "temperature": 0.0, "avg_logprob": -0.08563769052899073, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.014955279417335987}, {"id": 760, "seek": 330562, "start": 3315.8599999999997, "end": 3318.18, "text": " so, you know, if the reward function is to um", "tokens": [50876, 370, 11, 291, 458, 11, 498, 264, 7782, 2445, 307, 281, 1105, 50992], "temperature": 0.0, "avg_logprob": -0.08563769052899073, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.014955279417335987}, {"id": 761, "seek": 330562, "start": 3318.8199999999997, "end": 3321.38, "text": " Have a good life or something like this like there's obviously like", "tokens": [51024, 3560, 257, 665, 993, 420, 746, 411, 341, 411, 456, 311, 2745, 411, 51152], "temperature": 0.0, "avg_logprob": -0.08563769052899073, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.014955279417335987}, {"id": 762, "seek": 330562, "start": 3321.94, "end": 3325.7799999999997, "text": " You know, maybe there is some like numerical way of defining that in terms of an mdp", "tokens": [51180, 509, 458, 11, 1310, 456, 307, 512, 411, 29054, 636, 295, 17827, 300, 294, 2115, 295, 364, 275, 67, 79, 51372], "temperature": 0.0, "avg_logprob": -0.08563769052899073, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.014955279417335987}, {"id": 763, "seek": 330562, "start": 3326.58, "end": 3332.1, "text": " But there's like not actually a good way of writing down that function that maps what I do to whether I'm getting good rewards", "tokens": [51412, 583, 456, 311, 411, 406, 767, 257, 665, 636, 295, 3579, 760, 300, 2445, 300, 11317, 437, 286, 360, 281, 1968, 286, 478, 1242, 665, 17203, 51688], "temperature": 0.0, "avg_logprob": -0.08563769052899073, "compression_ratio": 1.806228373702422, "no_speech_prob": 0.014955279417335987}, {"id": 764, "seek": 333210, "start": 3332.42, "end": 3336.5, "text": " And so I think there's this kind of like, you know, I think that's a good framework for like thinking about any problem", "tokens": [50380, 400, 370, 286, 519, 456, 311, 341, 733, 295, 411, 11, 291, 458, 11, 286, 519, 300, 311, 257, 665, 8388, 337, 411, 1953, 466, 604, 1154, 50584], "temperature": 0.0, "avg_logprob": -0.09832642792136806, "compression_ratio": 1.9905956112852665, "no_speech_prob": 0.005383939947932959}, {"id": 765, "seek": 333210, "start": 3336.74, "end": 3340.3399999999997, "text": " But then you have these kind of like practical issues of how do you actually define rewards?", "tokens": [50596, 583, 550, 291, 362, 613, 733, 295, 411, 8496, 2663, 295, 577, 360, 291, 767, 6964, 17203, 30, 50776], "temperature": 0.0, "avg_logprob": -0.09832642792136806, "compression_ratio": 1.9905956112852665, "no_speech_prob": 0.005383939947932959}, {"id": 766, "seek": 333210, "start": 3340.3399999999997, "end": 3342.3399999999997, "text": " And how do you how do you say like?", "tokens": [50776, 400, 577, 360, 291, 577, 360, 291, 584, 411, 30, 50876], "temperature": 0.0, "avg_logprob": -0.09832642792136806, "compression_ratio": 1.9905956112852665, "no_speech_prob": 0.005383939947932959}, {"id": 767, "seek": 333210, "start": 3342.58, "end": 3347.46, "text": " Were there an agents doing well and not doing well and things like this? Um, and so I think that's still um", "tokens": [50888, 12448, 456, 364, 12554, 884, 731, 293, 406, 884, 731, 293, 721, 411, 341, 30, 3301, 11, 293, 370, 286, 519, 300, 311, 920, 1105, 51132], "temperature": 0.0, "avg_logprob": -0.09832642792136806, "compression_ratio": 1.9905956112852665, "no_speech_prob": 0.005383939947932959}, {"id": 768, "seek": 333210, "start": 3348.02, "end": 3351.54, "text": " Even with the world models lines of work. I think that's still like kind of quite a difficult issue", "tokens": [51160, 2754, 365, 264, 1002, 5245, 3876, 295, 589, 13, 286, 519, 300, 311, 920, 411, 733, 295, 1596, 257, 2252, 2734, 51336], "temperature": 0.0, "avg_logprob": -0.09832642792136806, "compression_ratio": 1.9905956112852665, "no_speech_prob": 0.005383939947932959}, {"id": 769, "seek": 333210, "start": 3351.94, "end": 3356.9, "text": " So so so the world models lines of work kind of, you know, allow you to model, you know, predicting ahead in the environment", "tokens": [51356, 407, 370, 370, 264, 1002, 5245, 3876, 295, 589, 733, 295, 11, 291, 458, 11, 2089, 291, 281, 2316, 11, 291, 458, 11, 32884, 2286, 294, 264, 2823, 51604], "temperature": 0.0, "avg_logprob": -0.09832642792136806, "compression_ratio": 1.9905956112852665, "no_speech_prob": 0.005383939947932959}, {"id": 770, "seek": 333210, "start": 3357.46, "end": 3359.7, "text": " Which is a very useful thing for doing a lot of tasks", "tokens": [51632, 3013, 307, 257, 588, 4420, 551, 337, 884, 257, 688, 295, 9608, 51744], "temperature": 0.0, "avg_logprob": -0.09832642792136806, "compression_ratio": 1.9905956112852665, "no_speech_prob": 0.005383939947932959}, {"id": 771, "seek": 335970, "start": 3360.3399999999997, "end": 3362.98, "text": " Um, but then if you actually want to optimize some specific task", "tokens": [50396, 3301, 11, 457, 550, 498, 291, 767, 528, 281, 19719, 512, 2685, 5633, 50528], "temperature": 0.0, "avg_logprob": -0.09212360884013929, "compression_ratio": 1.9526813880126184, "no_speech_prob": 0.0038238794077187777}, {"id": 772, "seek": 335970, "start": 3363.3799999999997, "end": 3365.7799999999997, "text": " You still have this problem of like, how do you define the reward?", "tokens": [50548, 509, 920, 362, 341, 1154, 295, 411, 11, 577, 360, 291, 6964, 264, 7782, 30, 50668], "temperature": 0.0, "avg_logprob": -0.09212360884013929, "compression_ratio": 1.9526813880126184, "no_speech_prob": 0.0038238794077187777}, {"id": 773, "seek": 335970, "start": 3366.02, "end": 3369.9399999999996, "text": " And so we eventually want to get to this point of being able to like inject a reward into the world model", "tokens": [50680, 400, 370, 321, 4728, 528, 281, 483, 281, 341, 935, 295, 885, 1075, 281, 411, 10711, 257, 7782, 666, 264, 1002, 2316, 50876], "temperature": 0.0, "avg_logprob": -0.09212360884013929, "compression_ratio": 1.9526813880126184, "no_speech_prob": 0.0038238794077187777}, {"id": 774, "seek": 335970, "start": 3370.02, "end": 3372.98, "text": " So we're kind of in agreement with that kind of line of thinking in a sense", "tokens": [50880, 407, 321, 434, 733, 295, 294, 8106, 365, 300, 733, 295, 1622, 295, 1953, 294, 257, 2020, 51028], "temperature": 0.0, "avg_logprob": -0.09212360884013929, "compression_ratio": 1.9526813880126184, "no_speech_prob": 0.0038238794077187777}, {"id": 775, "seek": 335970, "start": 3373.06, "end": 3376.98, "text": " We're eventually going to use a reward to derive the the the desired intelligent behavior", "tokens": [51032, 492, 434, 4728, 516, 281, 764, 257, 7782, 281, 28446, 264, 264, 264, 14721, 13232, 5223, 51228], "temperature": 0.0, "avg_logprob": -0.09212360884013929, "compression_ratio": 1.9526813880126184, "no_speech_prob": 0.0038238794077187777}, {"id": 776, "seek": 335970, "start": 3376.98, "end": 3378.66, "text": " So I don't think there's any conflict in that sense", "tokens": [51228, 407, 286, 500, 380, 519, 456, 311, 604, 6596, 294, 300, 2020, 51312], "temperature": 0.0, "avg_logprob": -0.09212360884013929, "compression_ratio": 1.9526813880126184, "no_speech_prob": 0.0038238794077187777}, {"id": 777, "seek": 335970, "start": 3378.8999999999996, "end": 3382.74, "text": " But we still have this kind of problem of how do we inject that reward into the the world model?", "tokens": [51324, 583, 321, 920, 362, 341, 733, 295, 1154, 295, 577, 360, 321, 10711, 300, 7782, 666, 264, 264, 1002, 2316, 30, 51516], "temperature": 0.0, "avg_logprob": -0.09212360884013929, "compression_ratio": 1.9526813880126184, "no_speech_prob": 0.0038238794077187777}, {"id": 778, "seek": 335970, "start": 3382.74, "end": 3384.74, "text": " How do we define what that reward should be?", "tokens": [51516, 1012, 360, 321, 6964, 437, 300, 7782, 820, 312, 30, 51616], "temperature": 0.0, "avg_logprob": -0.09212360884013929, "compression_ratio": 1.9526813880126184, "no_speech_prob": 0.0038238794077187777}, {"id": 779, "seek": 335970, "start": 3384.98, "end": 3386.98, "text": " um and the case of um", "tokens": [51628, 1105, 293, 264, 1389, 295, 1105, 51728], "temperature": 0.0, "avg_logprob": -0.09212360884013929, "compression_ratio": 1.9526813880126184, "no_speech_prob": 0.0038238794077187777}, {"id": 780, "seek": 338698, "start": 3387.38, "end": 3389.38, "text": " You know one of the easiest things to do for example", "tokens": [50384, 509, 458, 472, 295, 264, 12889, 721, 281, 360, 337, 1365, 50484], "temperature": 0.0, "avg_logprob": -0.10963987641864353, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0006877692067064345}, {"id": 781, "seek": 338698, "start": 3389.46, "end": 3392.34, "text": " Would just be to label each image with reward and then you can kind of", "tokens": [50488, 6068, 445, 312, 281, 7645, 1184, 3256, 365, 7782, 293, 550, 291, 393, 733, 295, 50632], "temperature": 0.0, "avg_logprob": -0.10963987641864353, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0006877692067064345}, {"id": 782, "seek": 338698, "start": 3392.66, "end": 3397.3, "text": " Encode that image into the latent space of the world model and then use that to define how good a certain thing is", "tokens": [50648, 29584, 1429, 300, 3256, 666, 264, 48994, 1901, 295, 264, 1002, 2316, 293, 550, 764, 300, 281, 6964, 577, 665, 257, 1629, 551, 307, 50880], "temperature": 0.0, "avg_logprob": -0.10963987641864353, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0006877692067064345}, {"id": 783, "seek": 338698, "start": 3397.62, "end": 3400.02, "text": " And that's kind of the style of thinking what we think of in our work", "tokens": [50896, 400, 300, 311, 733, 295, 264, 3758, 295, 1953, 437, 321, 519, 295, 294, 527, 589, 51016], "temperature": 0.0, "avg_logprob": -0.10963987641864353, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0006877692067064345}, {"id": 784, "seek": 338698, "start": 3400.5, "end": 3404.18, "text": " Um, but I don't think that overcomes this like overarching issue of in general", "tokens": [51040, 3301, 11, 457, 286, 500, 380, 519, 300, 670, 9055, 341, 411, 45501, 2734, 295, 294, 2674, 51224], "temperature": 0.0, "avg_logprob": -0.10963987641864353, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0006877692067064345}, {"id": 785, "seek": 338698, "start": 3404.26, "end": 3409.06, "text": " It's you know rewards can define everything, but how do you in practice like get that function is pretty hard", "tokens": [51228, 467, 311, 291, 458, 17203, 393, 6964, 1203, 11, 457, 577, 360, 291, 294, 3124, 411, 483, 300, 2445, 307, 1238, 1152, 51468], "temperature": 0.0, "avg_logprob": -0.10963987641864353, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0006877692067064345}, {"id": 786, "seek": 338698, "start": 3410.18, "end": 3414.5, "text": " Yeah, I mean in a sense reward is enough is sort of a tontology because once you know the reward", "tokens": [51524, 865, 11, 286, 914, 294, 257, 2020, 7782, 307, 1547, 307, 1333, 295, 257, 256, 896, 1793, 570, 1564, 291, 458, 264, 7782, 51740], "temperature": 0.0, "avg_logprob": -0.10963987641864353, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0006877692067064345}, {"id": 787, "seek": 341450, "start": 3414.58, "end": 3415.7, "text": " um", "tokens": [50368, 1105, 50424], "temperature": 0.0, "avg_logprob": -0.13444115685634925, "compression_ratio": 1.9137931034482758, "no_speech_prob": 0.002714707050472498}, {"id": 788, "seek": 341450, "start": 3415.7, "end": 3417.7, "text": " If you know the reward function for your environment", "tokens": [50424, 759, 291, 458, 264, 7782, 2445, 337, 428, 2823, 50524], "temperature": 0.0, "avg_logprob": -0.13444115685634925, "compression_ratio": 1.9137931034482758, "no_speech_prob": 0.002714707050472498}, {"id": 789, "seek": 341450, "start": 3417.86, "end": 3420.98, "text": " You can essentially compute the value function, which gives you the optimal policy", "tokens": [50532, 509, 393, 4476, 14722, 264, 2158, 2445, 11, 597, 2709, 291, 264, 16252, 3897, 50688], "temperature": 0.0, "avg_logprob": -0.13444115685634925, "compression_ratio": 1.9137931034482758, "no_speech_prob": 0.002714707050472498}, {"id": 790, "seek": 341450, "start": 3421.22, "end": 3421.54, "text": " um", "tokens": [50700, 1105, 50716], "temperature": 0.0, "avg_logprob": -0.13444115685634925, "compression_ratio": 1.9137931034482758, "no_speech_prob": 0.002714707050472498}, {"id": 791, "seek": 341450, "start": 3421.54, "end": 3424.82, "text": " And so reward has to be enough if you know the reward function and so", "tokens": [50716, 400, 370, 7782, 575, 281, 312, 1547, 498, 291, 458, 264, 7782, 2445, 293, 370, 50880], "temperature": 0.0, "avg_logprob": -0.13444115685634925, "compression_ratio": 1.9137931034482758, "no_speech_prob": 0.002714707050472498}, {"id": 792, "seek": 341450, "start": 3425.46, "end": 3428.9, "text": " Uh, I think the more interesting question is definitely like what is enough for the reward?", "tokens": [50912, 4019, 11, 286, 519, 264, 544, 1880, 1168, 307, 2138, 411, 437, 307, 1547, 337, 264, 7782, 30, 51084], "temperature": 0.0, "avg_logprob": -0.13444115685634925, "compression_ratio": 1.9137931034482758, "no_speech_prob": 0.002714707050472498}, {"id": 793, "seek": 341450, "start": 3429.14, "end": 3436.34, "text": " What is enough to actually have a system automatically figure out what are interesting new rewards for us to train new agents?", "tokens": [51096, 708, 307, 1547, 281, 767, 362, 257, 1185, 6772, 2573, 484, 437, 366, 1880, 777, 17203, 337, 505, 281, 3847, 777, 12554, 30, 51456], "temperature": 0.0, "avg_logprob": -0.13444115685634925, "compression_ratio": 1.9137931034482758, "no_speech_prob": 0.002714707050472498}, {"id": 794, "seek": 341450, "start": 3436.34, "end": 3442.02, "text": " We're new models on or continue training existing models on and I think this goes back to the question of environment design", "tokens": [51456, 492, 434, 777, 5245, 322, 420, 2354, 3097, 6741, 5245, 322, 293, 286, 519, 341, 1709, 646, 281, 264, 1168, 295, 2823, 1715, 51740], "temperature": 0.0, "avg_logprob": -0.13444115685634925, "compression_ratio": 1.9137931034482758, "no_speech_prob": 0.002714707050472498}, {"id": 795, "seek": 344202, "start": 3442.02, "end": 3447.7, "text": " This is largely the motivation of that line of work this auto curricula environment design where essentially if we can automatically", "tokens": [50364, 639, 307, 11611, 264, 12335, 295, 300, 1622, 295, 589, 341, 8399, 13179, 3780, 2823, 1715, 689, 4476, 498, 321, 393, 6772, 50648], "temperature": 0.0, "avg_logprob": -0.11692047119140625, "compression_ratio": 2.003846153846154, "no_speech_prob": 0.015420831739902496}, {"id": 796, "seek": 344202, "start": 3447.78, "end": 3451.78, "text": " Weave through this path of possible environments of the design space of the environments", "tokens": [50652, 492, 946, 807, 341, 3100, 295, 1944, 12388, 295, 264, 1715, 1901, 295, 264, 12388, 50852], "temperature": 0.0, "avg_logprob": -0.11692047119140625, "compression_ratio": 2.003846153846154, "no_speech_prob": 0.015420831739902496}, {"id": 797, "seek": 344202, "start": 3452.18, "end": 3458.66, "text": " The design space clearly will encompass like a big part of the design space is also encompassing the reward for those tasks", "tokens": [50872, 440, 1715, 1901, 4448, 486, 28268, 411, 257, 955, 644, 295, 264, 1715, 1901, 307, 611, 28268, 278, 264, 7782, 337, 729, 9608, 51196], "temperature": 0.0, "avg_logprob": -0.11692047119140625, "compression_ratio": 2.003846153846154, "no_speech_prob": 0.015420831739902496}, {"id": 798, "seek": 344202, "start": 3458.98, "end": 3464.5, "text": " And so essentially we want to find a curriculum automatic curriculum or path through the possible reward functions", "tokens": [51212, 400, 370, 4476, 321, 528, 281, 915, 257, 14302, 12509, 14302, 420, 3100, 807, 264, 1944, 7782, 6828, 51488], "temperature": 0.0, "avg_logprob": -0.11692047119140625, "compression_ratio": 2.003846153846154, "no_speech_prob": 0.015420831739902496}, {"id": 799, "seek": 344202, "start": 3464.82, "end": 3467.46, "text": " In which we can start to train a more and more general agent", "tokens": [51504, 682, 597, 321, 393, 722, 281, 3847, 257, 544, 293, 544, 2674, 9461, 51636], "temperature": 0.0, "avg_logprob": -0.11692047119140625, "compression_ratio": 2.003846153846154, "no_speech_prob": 0.015420831739902496}, {"id": 800, "seek": 346746, "start": 3467.7, "end": 3469.7, "text": " But then the interesting question is again", "tokens": [50376, 583, 550, 264, 1880, 1168, 307, 797, 50476], "temperature": 0.0, "avg_logprob": -0.12811213879545857, "compression_ratio": 1.854368932038835, "no_speech_prob": 0.05831972882151604}, {"id": 801, "seek": 346746, "start": 3469.78, "end": 3475.94, "text": " Like what exactly is the right notion of interestingness in order to drive that curriculum that path through the design space", "tokens": [50480, 1743, 437, 2293, 307, 264, 558, 10710, 295, 1880, 1287, 294, 1668, 281, 3332, 300, 14302, 300, 3100, 807, 264, 1715, 1901, 50788], "temperature": 0.0, "avg_logprob": -0.12811213879545857, "compression_ratio": 1.854368932038835, "no_speech_prob": 0.05831972882151604}, {"id": 802, "seek": 346746, "start": 3476.1, "end": 3481.3, "text": " of possible things we could be training our model or agent on and um, and that's I think", "tokens": [50796, 295, 1944, 721, 321, 727, 312, 3097, 527, 2316, 420, 9461, 322, 293, 1105, 11, 293, 300, 311, 286, 519, 51056], "temperature": 0.0, "avg_logprob": -0.12811213879545857, "compression_ratio": 1.854368932038835, "no_speech_prob": 0.05831972882151604}, {"id": 803, "seek": 346746, "start": 3481.7, "end": 3487.06, "text": " One of the most interesting open questions and it relates to the question as well of how do we get the model to ask the questions?", "tokens": [51076, 1485, 295, 264, 881, 1880, 1269, 1651, 293, 309, 16155, 281, 264, 1168, 382, 731, 295, 577, 360, 321, 483, 264, 2316, 281, 1029, 264, 1651, 30, 51344], "temperature": 0.0, "avg_logprob": -0.12811213879545857, "compression_ratio": 1.854368932038835, "no_speech_prob": 0.05831972882151604}, {"id": 804, "seek": 346746, "start": 3487.7, "end": 3491.2200000000003, "text": " Because really what drives humans in terms of asking further questions", "tokens": [51376, 1436, 534, 437, 11754, 6255, 294, 2115, 295, 3365, 3052, 1651, 51552], "temperature": 0.0, "avg_logprob": -0.12811213879545857, "compression_ratio": 1.854368932038835, "no_speech_prob": 0.05831972882151604}, {"id": 805, "seek": 346746, "start": 3492.02, "end": 3497.2200000000003, "text": " Is our own implicit notion of interestingness which is informed by things like the scientific method and you know", "tokens": [51592, 1119, 527, 1065, 26947, 10710, 295, 1880, 1287, 597, 307, 11740, 538, 721, 411, 264, 8134, 3170, 293, 291, 458, 51852], "temperature": 0.0, "avg_logprob": -0.12811213879545857, "compression_ratio": 1.854368932038835, "no_speech_prob": 0.05831972882151604}, {"id": 806, "seek": 349722, "start": 3497.2999999999997, "end": 3500.3399999999997, "text": " being able to create explanations about the world", "tokens": [50368, 885, 1075, 281, 1884, 28708, 466, 264, 1002, 50520], "temperature": 0.0, "avg_logprob": -0.1281813868769893, "compression_ratio": 1.8430656934306568, "no_speech_prob": 0.0015007182955741882}, {"id": 807, "seek": 349722, "start": 3500.74, "end": 3505.4599999999996, "text": " And we find things interesting when we can't actually explain some phenomenon about the world", "tokens": [50540, 400, 321, 915, 721, 1880, 562, 321, 393, 380, 767, 2903, 512, 14029, 466, 264, 1002, 50776], "temperature": 0.0, "avg_logprob": -0.1281813868769893, "compression_ratio": 1.8430656934306568, "no_speech_prob": 0.0015007182955741882}, {"id": 808, "seek": 349722, "start": 3505.9399999999996, "end": 3507.9399999999996, "text": " Based on existing theories or explanations", "tokens": [50800, 18785, 322, 6741, 13667, 420, 28708, 50900], "temperature": 0.0, "avg_logprob": -0.1281813868769893, "compression_ratio": 1.8430656934306568, "no_speech_prob": 0.0015007182955741882}, {"id": 809, "seek": 349722, "start": 3508.2599999999998, "end": 3513.7, "text": " And so I think what's really missing for a well-grounded, you know, human interpretable version of", "tokens": [50916, 400, 370, 286, 519, 437, 311, 534, 5361, 337, 257, 731, 12, 2921, 292, 11, 291, 458, 11, 1952, 7302, 712, 3037, 295, 51188], "temperature": 0.0, "avg_logprob": -0.1281813868769893, "compression_ratio": 1.8430656934306568, "no_speech_prob": 0.0015007182955741882}, {"id": 810, "seek": 349722, "start": 3514.18, "end": 3520.1, "text": " Interestingness is having models that can essentially come up with their own theories about the world and start to probe those theories", "tokens": [51212, 14711, 1287, 307, 1419, 5245, 300, 393, 4476, 808, 493, 365, 641, 1065, 13667, 466, 264, 1002, 293, 722, 281, 22715, 729, 13667, 51508], "temperature": 0.0, "avg_logprob": -0.1281813868769893, "compression_ratio": 1.8430656934306568, "no_speech_prob": 0.0015007182955741882}, {"id": 811, "seek": 349722, "start": 3520.2599999999998, "end": 3523.62, "text": " For where there's mismatch between, you know, the their learned theory of the world", "tokens": [51516, 1171, 689, 456, 311, 23220, 852, 1296, 11, 291, 458, 11, 264, 641, 3264, 5261, 295, 264, 1002, 51684], "temperature": 0.0, "avg_logprob": -0.1281813868769893, "compression_ratio": 1.8430656934306568, "no_speech_prob": 0.0015007182955741882}, {"id": 812, "seek": 352362, "start": 3524.02, "end": 3527.7, "text": " And evidence that new evidence that they find from experiences in the world", "tokens": [50384, 400, 4467, 300, 777, 4467, 300, 436, 915, 490, 5235, 294, 264, 1002, 50568], "temperature": 0.0, "avg_logprob": -0.1580117596082451, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.003626168007031083}, {"id": 813, "seek": 352362, "start": 3528.02, "end": 3532.9, "text": " Yeah, it's so interesting and and um, I mean when I make the argument that agent should be physically and socially embedded", "tokens": [50584, 865, 11, 309, 311, 370, 1880, 293, 293, 1105, 11, 286, 914, 562, 286, 652, 264, 6770, 300, 9461, 820, 312, 9762, 293, 21397, 16741, 50828], "temperature": 0.0, "avg_logprob": -0.1580117596082451, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.003626168007031083}, {"id": 814, "seek": 352362, "start": 3532.98, "end": 3538.1, "text": " It's it's actually quite a simple argument, which is just the guardrails. It's that interesting this thing", "tokens": [50832, 467, 311, 309, 311, 767, 1596, 257, 2199, 6770, 11, 597, 307, 445, 264, 6290, 424, 4174, 13, 467, 311, 300, 1880, 341, 551, 51088], "temperature": 0.0, "avg_logprob": -0.1580117596082451, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.003626168007031083}, {"id": 815, "seek": 352362, "start": 3538.18, "end": 3545.06, "text": " I think that that is how, you know, having um, uh agency but with the guardrails of our physical and social embedding", "tokens": [51092, 286, 519, 300, 300, 307, 577, 11, 291, 458, 11, 1419, 1105, 11, 2232, 7934, 457, 365, 264, 6290, 424, 4174, 295, 527, 4001, 293, 2093, 12240, 3584, 51436], "temperature": 0.0, "avg_logprob": -0.1580117596082451, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.003626168007031083}, {"id": 816, "seek": 352362, "start": 3545.14, "end": 3548.18, "text": " So, you know, we're we're sampling things that make sense because they're already there", "tokens": [51440, 407, 11, 291, 458, 11, 321, 434, 321, 434, 21179, 721, 300, 652, 2020, 570, 436, 434, 1217, 456, 51592], "temperature": 0.0, "avg_logprob": -0.1580117596082451, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.003626168007031083}, {"id": 817, "seek": 354818, "start": 3548.4199999999996, "end": 3552.1, "text": " That, you know, but but but obviously we can go off piece to little bit as individual agents", "tokens": [50376, 663, 11, 291, 458, 11, 457, 457, 457, 2745, 321, 393, 352, 766, 2522, 281, 707, 857, 382, 2609, 12554, 50560], "temperature": 0.0, "avg_logprob": -0.13409202282245342, "compression_ratio": 1.7823343848580442, "no_speech_prob": 0.03839115798473358}, {"id": 818, "seek": 354818, "start": 3552.3399999999997, "end": 3555.8599999999997, "text": " I I feel that that's what helps that process just coming back to Sutton", "tokens": [50572, 286, 286, 841, 300, 300, 311, 437, 3665, 300, 1399, 445, 1348, 646, 281, 40492, 1756, 50748], "temperature": 0.0, "avg_logprob": -0.13409202282245342, "compression_ratio": 1.7823343848580442, "no_speech_prob": 0.03839115798473358}, {"id": 819, "seek": 354818, "start": 3555.94, "end": 3558.1, "text": " It's entirely possible that I've misunderstood Sutton, by the way", "tokens": [50752, 467, 311, 7696, 1944, 300, 286, 600, 33870, 40492, 1756, 11, 538, 264, 636, 50860], "temperature": 0.0, "avg_logprob": -0.13409202282245342, "compression_ratio": 1.7823343848580442, "no_speech_prob": 0.03839115798473358}, {"id": 820, "seek": 354818, "start": 3558.1, "end": 3564.4199999999996, "text": " So my my interpretation of reward is enough and it might be true as you say that it's tautological given that if you already knew", "tokens": [50860, 407, 452, 452, 14174, 295, 7782, 307, 1547, 293, 309, 1062, 312, 2074, 382, 291, 584, 300, 309, 311, 256, 1375, 4383, 2212, 300, 498, 291, 1217, 2586, 51176], "temperature": 0.0, "avg_logprob": -0.13409202282245342, "compression_ratio": 1.7823343848580442, "no_speech_prob": 0.03839115798473358}, {"id": 821, "seek": 354818, "start": 3564.74, "end": 3568.74, "text": " The reward function for a particular environment then it could do everything that it needed to do", "tokens": [51192, 440, 7782, 2445, 337, 257, 1729, 2823, 550, 309, 727, 360, 1203, 300, 309, 2978, 281, 360, 51392], "temperature": 0.0, "avg_logprob": -0.13409202282245342, "compression_ratio": 1.7823343848580442, "no_speech_prob": 0.03839115798473358}, {"id": 822, "seek": 354818, "start": 3568.8199999999997, "end": 3574.8999999999996, "text": " But my interpretation of reward is enough is that it would lead to um, a general intelligence and you know", "tokens": [51396, 583, 452, 14174, 295, 7782, 307, 1547, 307, 300, 309, 576, 1477, 281, 1105, 11, 257, 2674, 7599, 293, 291, 458, 51700], "temperature": 0.0, "avg_logprob": -0.13409202282245342, "compression_ratio": 1.7823343848580442, "no_speech_prob": 0.03839115798473358}, {"id": 823, "seek": 357490, "start": 3574.98, "end": 3579.38, "text": " General in in the kind of magical sense that it would work in in any possible situation", "tokens": [50368, 6996, 294, 294, 264, 733, 295, 12066, 2020, 300, 309, 576, 589, 294, 294, 604, 1944, 2590, 50588], "temperature": 0.0, "avg_logprob": -0.16061032719972754, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.0017210044898092747}, {"id": 824, "seek": 357490, "start": 3579.7000000000003, "end": 3584.9, "text": " But if it is specialized in the way that we agreed earlier that there exists a reward function", "tokens": [50604, 583, 498, 309, 307, 19813, 294, 264, 636, 300, 321, 9166, 3071, 300, 456, 8198, 257, 7782, 2445, 50864], "temperature": 0.0, "avg_logprob": -0.16061032719972754, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.0017210044898092747}, {"id": 825, "seek": 357490, "start": 3584.98, "end": 3586.98, "text": " Which would inco you know codify", "tokens": [50868, 3013, 576, 834, 78, 291, 458, 17656, 2505, 50968], "temperature": 0.0, "avg_logprob": -0.16061032719972754, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.0017210044898092747}, {"id": 826, "seek": 357490, "start": 3587.14, "end": 3592.7400000000002, "text": " Motifs and things that you know, you need to know or optimize in a particular environment or set of environments", "tokens": [50976, 8956, 18290, 293, 721, 300, 291, 458, 11, 291, 643, 281, 458, 420, 19719, 294, 257, 1729, 2823, 420, 992, 295, 12388, 51256], "temperature": 0.0, "avg_logprob": -0.16061032719972754, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.0017210044898092747}, {"id": 827, "seek": 357490, "start": 3593.14, "end": 3597.2200000000003, "text": " Then to me that's still specialized intelligence and I would agree. Yeah. Yeah, yeah", "tokens": [51276, 1396, 281, 385, 300, 311, 920, 19813, 7599, 293, 286, 576, 3986, 13, 865, 13, 865, 11, 1338, 51480], "temperature": 0.0, "avg_logprob": -0.16061032719972754, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.0017210044898092747}, {"id": 828, "seek": 357490, "start": 3597.3, "end": 3602.26, "text": " That's that I think that aligns with my take as well where I think if you have a reward function", "tokens": [51484, 663, 311, 300, 286, 519, 300, 7975, 82, 365, 452, 747, 382, 731, 689, 286, 519, 498, 291, 362, 257, 7782, 2445, 51732], "temperature": 0.0, "avg_logprob": -0.16061032719972754, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.0017210044898092747}, {"id": 829, "seek": 360226, "start": 3603.1400000000003, "end": 3605.1400000000003, "text": " It's already sort of applying", "tokens": [50408, 467, 311, 1217, 1333, 295, 9275, 50508], "temperature": 0.0, "avg_logprob": -0.12262929810418023, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.0035930632147938013}, {"id": 830, "seek": 360226, "start": 3605.6200000000003, "end": 3609.2200000000003, "text": " Largely applies to at least the examples in that position paper about reward is enough", "tokens": [50532, 11569, 70, 736, 13165, 281, 412, 1935, 264, 5110, 294, 300, 2535, 3035, 466, 7782, 307, 1547, 50712], "temperature": 0.0, "avg_logprob": -0.12262929810418023, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.0035930632147938013}, {"id": 831, "seek": 360226, "start": 3609.2200000000003, "end": 3612.1000000000004, "text": " It seems like most of the reward functions they discuss are largely", "tokens": [50712, 467, 2544, 411, 881, 295, 264, 7782, 6828, 436, 2248, 366, 11611, 50856], "temperature": 0.0, "avg_logprob": -0.12262929810418023, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.0035930632147938013}, {"id": 832, "seek": 360226, "start": 3612.98, "end": 3614.98, "text": " Grounded in a specific task", "tokens": [50900, 28371, 292, 294, 257, 2685, 5633, 51000], "temperature": 0.0, "avg_logprob": -0.12262929810418023, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.0035930632147938013}, {"id": 833, "seek": 360226, "start": 3615.0600000000004, "end": 3617.6200000000003, "text": " And I think that if you have the reward function for a specific task", "tokens": [51004, 400, 286, 519, 300, 498, 291, 362, 264, 7782, 2445, 337, 257, 2685, 5633, 51132], "temperature": 0.0, "avg_logprob": -0.12262929810418023, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.0035930632147938013}, {"id": 834, "seek": 360226, "start": 3617.94, "end": 3622.0200000000004, "text": " Then it definitely seems that you can have some optimization or learning algorithm", "tokens": [51148, 1396, 309, 2138, 2544, 300, 291, 393, 362, 512, 19618, 420, 2539, 9284, 51352], "temperature": 0.0, "avg_logprob": -0.12262929810418023, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.0035930632147938013}, {"id": 835, "seek": 360226, "start": 3622.26, "end": 3625.5400000000004, "text": " That essentially learns to optimize that reward and therefore achieve that task", "tokens": [51364, 663, 4476, 27152, 281, 19719, 300, 7782, 293, 4412, 4584, 300, 5633, 51528], "temperature": 0.0, "avg_logprob": -0.12262929810418023, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.0035930632147938013}, {"id": 836, "seek": 360226, "start": 3626.42, "end": 3628.5, "text": " So I do think sort of the open question that", "tokens": [51572, 407, 286, 360, 519, 1333, 295, 264, 1269, 1168, 300, 51676], "temperature": 0.0, "avg_logprob": -0.12262929810418023, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.0035930632147938013}, {"id": 837, "seek": 362850, "start": 3628.98, "end": 3631.7, "text": " Uh, I think same reward is enough", "tokens": [50388, 4019, 11, 286, 519, 912, 7782, 307, 1547, 50524], "temperature": 0.0, "avg_logprob": -0.13540045670636996, "compression_ratio": 1.8376623376623376, "no_speech_prob": 0.001144541660323739}, {"id": 838, "seek": 362850, "start": 3631.78, "end": 3636.5, "text": " I think it kind of passes the buck up further one level to the question of where that reward comes from", "tokens": [50528, 286, 519, 309, 733, 295, 11335, 264, 14894, 493, 3052, 472, 1496, 281, 264, 1168, 295, 689, 300, 7782, 1487, 490, 50764], "temperature": 0.0, "avg_logprob": -0.13540045670636996, "compression_ratio": 1.8376623376623376, "no_speech_prob": 0.001144541660323739}, {"id": 839, "seek": 362850, "start": 3636.66, "end": 3641.94, "text": " And I do think that having systems that can automatically design interesting new rewards. That seems like the frontier", "tokens": [50772, 400, 286, 360, 519, 300, 1419, 3652, 300, 393, 6772, 1715, 1880, 777, 17203, 13, 663, 2544, 411, 264, 35853, 51036], "temperature": 0.0, "avg_logprob": -0.13540045670636996, "compression_ratio": 1.8376623376623376, "no_speech_prob": 0.001144541660323739}, {"id": 840, "seek": 362850, "start": 3642.18, "end": 3647.22, "text": " Yeah, I agree and and you know because to me intelligence is about discovering the knowledge and the knowledge is the reward function", "tokens": [51048, 865, 11, 286, 3986, 293, 293, 291, 458, 570, 281, 385, 7599, 307, 466, 24773, 264, 3601, 293, 264, 3601, 307, 264, 7782, 2445, 51300], "temperature": 0.0, "avg_logprob": -0.13540045670636996, "compression_ratio": 1.8376623376623376, "no_speech_prob": 0.001144541660323739}, {"id": 841, "seek": 362850, "start": 3647.22, "end": 3651.06, "text": " So if it was like kind of baking the knowledge in into the system, um, okay", "tokens": [51300, 407, 498, 309, 390, 411, 733, 295, 12102, 264, 3601, 294, 666, 264, 1185, 11, 1105, 11, 1392, 51492], "temperature": 0.0, "avg_logprob": -0.13540045670636996, "compression_ratio": 1.8376623376623376, "no_speech_prob": 0.001144541660323739}, {"id": 842, "seek": 362850, "start": 3651.06, "end": 3656.9, "text": " so another sort of galaxy brain take is um, I was talking to bishop about this the other day and um", "tokens": [51492, 370, 1071, 1333, 295, 17639, 3567, 747, 307, 1105, 11, 286, 390, 1417, 281, 34470, 466, 341, 264, 661, 786, 293, 1105, 51784], "temperature": 0.0, "avg_logprob": -0.13540045670636996, "compression_ratio": 1.8376623376623376, "no_speech_prob": 0.001144541660323739}, {"id": 843, "seek": 365690, "start": 3657.62, "end": 3660.1800000000003, "text": " Do you think of like deep learning models as one model?", "tokens": [50400, 1144, 291, 519, 295, 411, 2452, 2539, 5245, 382, 472, 2316, 30, 50528], "temperature": 0.0, "avg_logprob": -0.14038438063401443, "compression_ratio": 1.8779661016949152, "no_speech_prob": 0.0009690657607279718}, {"id": 844, "seek": 365690, "start": 3660.1800000000003, "end": 3666.42, "text": " Or do you think of them as a sort of like intrinsic ensemble of models because they they get they behave differently in an input sensitive way", "tokens": [50528, 1610, 360, 291, 519, 295, 552, 382, 257, 1333, 295, 411, 35698, 19492, 295, 5245, 570, 436, 436, 483, 436, 15158, 7614, 294, 364, 4846, 9477, 636, 50840], "temperature": 0.0, "avg_logprob": -0.14038438063401443, "compression_ratio": 1.8779661016949152, "no_speech_prob": 0.0009690657607279718}, {"id": 845, "seek": 365690, "start": 3666.7400000000002, "end": 3670.34, "text": " So, you know, like depending on the prompts you put into language into a language model", "tokens": [50856, 407, 11, 291, 458, 11, 411, 5413, 322, 264, 41095, 291, 829, 666, 2856, 666, 257, 2856, 2316, 51036], "temperature": 0.0, "avg_logprob": -0.14038438063401443, "compression_ratio": 1.8779661016949152, "no_speech_prob": 0.0009690657607279718}, {"id": 846, "seek": 365690, "start": 3670.58, "end": 3675.62, "text": " You might find that like a different part of the weight space gets activated and and essentially it's like retrieving a mini program", "tokens": [51048, 509, 1062, 915, 300, 411, 257, 819, 644, 295, 264, 3364, 1901, 2170, 18157, 293, 293, 4476, 309, 311, 411, 19817, 798, 257, 8382, 1461, 51300], "temperature": 0.0, "avg_logprob": -0.14038438063401443, "compression_ratio": 1.8779661016949152, "no_speech_prob": 0.0009690657607279718}, {"id": 847, "seek": 365690, "start": 3675.86, "end": 3680.1800000000003, "text": " And that program is being run, but it's not it's not model building. It's like model", "tokens": [51312, 400, 300, 1461, 307, 885, 1190, 11, 457, 309, 311, 406, 309, 311, 406, 2316, 2390, 13, 467, 311, 411, 2316, 51528], "temperature": 0.0, "avg_logprob": -0.14038438063401443, "compression_ratio": 1.8779661016949152, "no_speech_prob": 0.0009690657607279718}, {"id": 848, "seek": 365690, "start": 3680.9, "end": 3682.9, "text": " Retrieving, but we would would you agree of that?", "tokens": [51564, 11495, 5469, 798, 11, 457, 321, 576, 576, 291, 3986, 295, 300, 30, 51664], "temperature": 0.0, "avg_logprob": -0.14038438063401443, "compression_ratio": 1.8779661016949152, "no_speech_prob": 0.0009690657607279718}, {"id": 849, "seek": 368290, "start": 3683.14, "end": 3684.1800000000003, "text": " Hmm", "tokens": [50376, 8239, 50428], "temperature": 0.0, "avg_logprob": -0.13559955728465112, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0021824028808623552}, {"id": 850, "seek": 368290, "start": 3684.1800000000003, "end": 3686.1800000000003, "text": " I guess i'm not sure about that like", "tokens": [50428, 286, 2041, 741, 478, 406, 988, 466, 300, 411, 50528], "temperature": 0.0, "avg_logprob": -0.13559955728465112, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0021824028808623552}, {"id": 851, "seek": 368290, "start": 3686.1800000000003, "end": 3688.5, "text": " Like within like subsets of a single homogeneous model", "tokens": [50528, 1743, 1951, 411, 2090, 1385, 295, 257, 2167, 42632, 2316, 50644], "temperature": 0.0, "avg_logprob": -0.13559955728465112, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0021824028808623552}, {"id": 852, "seek": 368290, "start": 3688.5, "end": 3692.5, "text": " But I guess the thing that I like to think about that's I think quite related to this is this idea of like", "tokens": [50644, 583, 286, 2041, 264, 551, 300, 286, 411, 281, 519, 466, 300, 311, 286, 519, 1596, 4077, 281, 341, 307, 341, 1558, 295, 411, 50844], "temperature": 0.0, "avg_logprob": -0.13559955728465112, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0021824028808623552}, {"id": 853, "seek": 368290, "start": 3693.14, "end": 3696.7400000000002, "text": " And I think yamakun also kind of well a lot of people have laid out like a similar architecture", "tokens": [50876, 400, 286, 519, 288, 335, 514, 409, 611, 733, 295, 731, 257, 688, 295, 561, 362, 9897, 484, 411, 257, 2531, 9482, 51056], "temperature": 0.0, "avg_logprob": -0.13559955728465112, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0021824028808623552}, {"id": 854, "seek": 368290, "start": 3696.7400000000002, "end": 3701.3, "text": " It's like, you know, should we think of intelligent agents as having kind of like separate subsystems that can maybe", "tokens": [51056, 467, 311, 411, 11, 291, 458, 11, 820, 321, 519, 295, 13232, 12554, 382, 1419, 733, 295, 411, 4994, 2090, 9321, 82, 300, 393, 1310, 51284], "temperature": 0.0, "avg_logprob": -0.13559955728465112, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0021824028808623552}, {"id": 855, "seek": 368290, "start": 3701.86, "end": 3705.06, "text": " Like be thought of as different neural networks. And so, you know, we could have like, you know", "tokens": [51312, 1743, 312, 1194, 295, 382, 819, 18161, 9590, 13, 400, 370, 11, 291, 458, 11, 321, 727, 362, 411, 11, 291, 458, 51472], "temperature": 0.0, "avg_logprob": -0.13559955728465112, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0021824028808623552}, {"id": 856, "seek": 368290, "start": 3705.86, "end": 3709.06, "text": " Um, the standard notion of a policy which is like outputting actions", "tokens": [51512, 3301, 11, 264, 3832, 10710, 295, 257, 3897, 597, 307, 411, 5598, 783, 5909, 51672], "temperature": 0.0, "avg_logprob": -0.13559955728465112, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0021824028808623552}, {"id": 857, "seek": 370906, "start": 3709.38, "end": 3712.82, "text": " And maybe we also want to have the notion of like a prediction model more like a world model that predicts", "tokens": [50380, 400, 1310, 321, 611, 528, 281, 362, 264, 10710, 295, 411, 257, 17630, 2316, 544, 411, 257, 1002, 2316, 300, 6069, 82, 50552], "temperature": 0.0, "avg_logprob": -0.09916641235351563, "compression_ratio": 2.067073170731707, "no_speech_prob": 0.02032514475286007}, {"id": 858, "seek": 370906, "start": 3712.82, "end": 3717.54, "text": " What might go ahead in the world as well as maybe like a planner that is somehow good at like optimizing in that model", "tokens": [50552, 708, 1062, 352, 2286, 294, 264, 1002, 382, 731, 382, 1310, 411, 257, 31268, 300, 307, 6063, 665, 412, 411, 40425, 294, 300, 2316, 50788], "temperature": 0.0, "avg_logprob": -0.09916641235351563, "compression_ratio": 2.067073170731707, "no_speech_prob": 0.02032514475286007}, {"id": 859, "seek": 370906, "start": 3717.54, "end": 3721.86, "text": " And so we could kind of think of all these things as like separate subcomponents that we assume an intelligent", "tokens": [50788, 400, 370, 321, 727, 733, 295, 519, 295, 439, 613, 721, 382, 411, 4994, 1422, 21541, 40496, 300, 321, 6552, 364, 13232, 51004], "temperature": 0.0, "avg_logprob": -0.09916641235351563, "compression_ratio": 2.067073170731707, "no_speech_prob": 0.02032514475286007}, {"id": 860, "seek": 370906, "start": 3722.1, "end": 3723.7, "text": " You know an intelligent", "tokens": [51016, 509, 458, 364, 13232, 51096], "temperature": 0.0, "avg_logprob": -0.09916641235351563, "compression_ratio": 2.067073170731707, "no_speech_prob": 0.02032514475286007}, {"id": 861, "seek": 370906, "start": 3723.7, "end": 3726.66, "text": " Thing would have like an intelligent thing should be able to predict ahead in the world", "tokens": [51096, 30902, 576, 362, 411, 364, 13232, 551, 820, 312, 1075, 281, 6069, 2286, 294, 264, 1002, 51244], "temperature": 0.0, "avg_logprob": -0.09916641235351563, "compression_ratio": 2.067073170731707, "no_speech_prob": 0.02032514475286007}, {"id": 862, "seek": 370906, "start": 3726.66, "end": 3728.66, "text": " It should also be able to output actions", "tokens": [51244, 467, 820, 611, 312, 1075, 281, 5598, 5909, 51344], "temperature": 0.0, "avg_logprob": -0.09916641235351563, "compression_ratio": 2.067073170731707, "no_speech_prob": 0.02032514475286007}, {"id": 863, "seek": 370906, "start": 3728.66, "end": 3732.2599999999998, "text": " It should hopefully maybe be able to infer like why other things happened and things like this", "tokens": [51344, 467, 820, 4696, 1310, 312, 1075, 281, 13596, 411, 983, 661, 721, 2011, 293, 721, 411, 341, 51524], "temperature": 0.0, "avg_logprob": -0.09916641235351563, "compression_ratio": 2.067073170731707, "no_speech_prob": 0.02032514475286007}, {"id": 864, "seek": 370906, "start": 3732.74, "end": 3736.98, "text": " And so I guess as to whether we think that should you know be just like one homogeneous model", "tokens": [51548, 400, 370, 286, 2041, 382, 281, 1968, 321, 519, 300, 820, 291, 458, 312, 445, 411, 472, 42632, 2316, 51760], "temperature": 0.0, "avg_logprob": -0.09916641235351563, "compression_ratio": 2.067073170731707, "no_speech_prob": 0.02032514475286007}, {"id": 865, "seek": 373698, "start": 3737.78, "end": 3741.38, "text": " For which maybe you query it and maybe you know different aspects of that model are kind of um", "tokens": [50404, 1171, 597, 1310, 291, 14581, 309, 293, 1310, 291, 458, 819, 7270, 295, 300, 2316, 366, 733, 295, 1105, 50584], "temperature": 0.0, "avg_logprob": -0.11830450693766276, "compression_ratio": 1.9902912621359223, "no_speech_prob": 0.002800524001941085}, {"id": 866, "seek": 373698, "start": 3742.02, "end": 3745.7, "text": " You know handle different aspects of the query or whether we should think of those as separate components", "tokens": [50616, 509, 458, 4813, 819, 7270, 295, 264, 14581, 420, 1968, 321, 820, 519, 295, 729, 382, 4994, 6677, 50800], "temperature": 0.0, "avg_logprob": -0.11830450693766276, "compression_ratio": 1.9902912621359223, "no_speech_prob": 0.002800524001941085}, {"id": 867, "seek": 373698, "start": 3745.7, "end": 3749.62, "text": " I'm not I'm not really sure as to whether it matters whether they're separate components or not because yeah", "tokens": [50800, 286, 478, 406, 286, 478, 406, 534, 988, 382, 281, 1968, 309, 7001, 1968, 436, 434, 4994, 6677, 420, 406, 570, 1338, 50996], "temperature": 0.0, "avg_logprob": -0.11830450693766276, "compression_ratio": 1.9902912621359223, "no_speech_prob": 0.002800524001941085}, {"id": 868, "seek": 373698, "start": 3749.62, "end": 3752.58, "text": " I agree that you probably could just have like one massive model that does all these things", "tokens": [50996, 286, 3986, 300, 291, 1391, 727, 445, 362, 411, 472, 5994, 2316, 300, 775, 439, 613, 721, 51144], "temperature": 0.0, "avg_logprob": -0.11830450693766276, "compression_ratio": 1.9902912621359223, "no_speech_prob": 0.002800524001941085}, {"id": 869, "seek": 373698, "start": 3752.98, "end": 3755.14, "text": " And I think at least from the the trend that I've been seeing", "tokens": [51164, 400, 286, 519, 412, 1935, 490, 264, 264, 6028, 300, 286, 600, 668, 2577, 51272], "temperature": 0.0, "avg_logprob": -0.11830450693766276, "compression_ratio": 1.9902912621359223, "no_speech_prob": 0.002800524001941085}, {"id": 870, "seek": 373698, "start": 3756.34, "end": 3760.58, "text": " In kind of the world models literature and and also just like I guess the rl literature", "tokens": [51332, 682, 733, 295, 264, 1002, 5245, 10394, 293, 293, 611, 445, 411, 286, 2041, 264, 367, 75, 10394, 51544], "temperature": 0.0, "avg_logprob": -0.11830450693766276, "compression_ratio": 1.9902912621359223, "no_speech_prob": 0.002800524001941085}, {"id": 871, "seek": 373698, "start": 3760.58, "end": 3762.58, "text": " Or maybe just we should call it the foundation model literature", "tokens": [51544, 1610, 1310, 445, 321, 820, 818, 309, 264, 7030, 2316, 10394, 51644], "temperature": 0.0, "avg_logprob": -0.11830450693766276, "compression_ratio": 1.9902912621359223, "no_speech_prob": 0.002800524001941085}, {"id": 872, "seek": 376258, "start": 3762.74, "end": 3766.8199999999997, "text": " Is you kind of don't want to have like a separate model that does the prediction for actions and a separate model", "tokens": [50372, 1119, 291, 733, 295, 500, 380, 528, 281, 362, 411, 257, 4994, 2316, 300, 775, 264, 17630, 337, 5909, 293, 257, 4994, 2316, 50576], "temperature": 0.0, "avg_logprob": -0.11262095196646933, "compression_ratio": 2.052486187845304, "no_speech_prob": 0.030196934938430786}, {"id": 873, "seek": 376258, "start": 3766.8199999999997, "end": 3770.02, "text": " That does the prediction observations like why not just have one massive model", "tokens": [50576, 663, 775, 264, 17630, 18163, 411, 983, 406, 445, 362, 472, 5994, 2316, 50736], "temperature": 0.0, "avg_logprob": -0.11262095196646933, "compression_ratio": 2.052486187845304, "no_speech_prob": 0.030196934938430786}, {"id": 874, "seek": 376258, "start": 3770.2599999999998, "end": 3774.02, "text": " That's jointly trained to predict everything you might want to query and then depending on the different query", "tokens": [50748, 663, 311, 46557, 8895, 281, 6069, 1203, 291, 1062, 528, 281, 14581, 293, 550, 5413, 322, 264, 819, 14581, 50936], "temperature": 0.0, "avg_logprob": -0.11262095196646933, "compression_ratio": 2.052486187845304, "no_speech_prob": 0.030196934938430786}, {"id": 875, "seek": 376258, "start": 3774.1, "end": 3779.7799999999997, "text": " You know it will just either predict an action or a predictive video sequence or it can be conditioned on actions or conditioned on language", "tokens": [50940, 509, 458, 309, 486, 445, 2139, 6069, 364, 3069, 420, 257, 35521, 960, 8310, 420, 309, 393, 312, 35833, 322, 5909, 420, 35833, 322, 2856, 51224], "temperature": 0.0, "avg_logprob": -0.11262095196646933, "compression_ratio": 2.052486187845304, "no_speech_prob": 0.030196934938430786}, {"id": 876, "seek": 376258, "start": 3780.02, "end": 3784.34, "text": " So I think in this sense like this kind of model like you said is more like just one massive model", "tokens": [51236, 407, 286, 519, 294, 341, 2020, 411, 341, 733, 295, 2316, 411, 291, 848, 307, 544, 411, 445, 472, 5994, 2316, 51452], "temperature": 0.0, "avg_logprob": -0.11262095196646933, "compression_ratio": 2.052486187845304, "no_speech_prob": 0.030196934938430786}, {"id": 877, "seek": 376258, "start": 3784.34, "end": 3787.22, "text": " But it kind of has like a lots of different sub tasks that it's able to do", "tokens": [51452, 583, 309, 733, 295, 575, 411, 257, 3195, 295, 819, 1422, 9608, 300, 309, 311, 1075, 281, 360, 51596], "temperature": 0.0, "avg_logprob": -0.11262095196646933, "compression_ratio": 2.052486187845304, "no_speech_prob": 0.030196934938430786}, {"id": 878, "seek": 376258, "start": 3787.7799999999997, "end": 3788.34, "text": " um", "tokens": [51624, 1105, 51652], "temperature": 0.0, "avg_logprob": -0.11262095196646933, "compression_ratio": 2.052486187845304, "no_speech_prob": 0.030196934938430786}, {"id": 879, "seek": 376258, "start": 3788.34, "end": 3792.34, "text": " And so maybe this is actually like the more effective way of training a model because then you kind of get generalization", "tokens": [51652, 400, 370, 1310, 341, 307, 767, 411, 264, 544, 4942, 636, 295, 3097, 257, 2316, 570, 550, 291, 733, 295, 483, 2674, 2144, 51852], "temperature": 0.0, "avg_logprob": -0.11262095196646933, "compression_ratio": 2.052486187845304, "no_speech_prob": 0.030196934938430786}, {"id": 880, "seek": 379234, "start": 3792.34, "end": 3794.34, "text": " Across these different sub tasks as well", "tokens": [50364, 34527, 613, 819, 1422, 9608, 382, 731, 50464], "temperature": 0.0, "avg_logprob": -0.12270735291873708, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0004541777598205954}, {"id": 881, "seek": 379234, "start": 3794.5, "end": 3796.5, "text": " Well, yeah, and the reason I'm asking the question is um", "tokens": [50472, 1042, 11, 1338, 11, 293, 264, 1778, 286, 478, 3365, 264, 1168, 307, 1105, 50572], "temperature": 0.0, "avg_logprob": -0.12270735291873708, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0004541777598205954}, {"id": 882, "seek": 379234, "start": 3797.1400000000003, "end": 3801.86, "text": " It seemed I mean like you know for for an outsider coming in it looks like statistics has broken", "tokens": [50604, 467, 6576, 286, 914, 411, 291, 458, 337, 337, 364, 40484, 1348, 294, 309, 1542, 411, 12523, 575, 5463, 50840], "temperature": 0.0, "avg_logprob": -0.12270735291873708, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0004541777598205954}, {"id": 883, "seek": 379234, "start": 3802.1000000000004, "end": 3805.1400000000003, "text": " You know in the olden days we used to talk about the no free lunch theorem used to say like you know", "tokens": [50852, 509, 458, 294, 264, 1331, 268, 1708, 321, 1143, 281, 751, 466, 264, 572, 1737, 6349, 20904, 1143, 281, 584, 411, 291, 458, 51004], "temperature": 0.0, "avg_logprob": -0.12270735291873708, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0004541777598205954}, {"id": 884, "seek": 379234, "start": 3805.1400000000003, "end": 3807.6200000000003, "text": " You need to have specialized models for different situations", "tokens": [51004, 509, 643, 281, 362, 19813, 5245, 337, 819, 6851, 51128], "temperature": 0.0, "avg_logprob": -0.12270735291873708, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0004541777598205954}, {"id": 885, "seek": 379234, "start": 3807.86, "end": 3810.1000000000004, "text": " And now the narrative is that we have generalist models", "tokens": [51140, 400, 586, 264, 9977, 307, 300, 321, 362, 2674, 468, 5245, 51252], "temperature": 0.0, "avg_logprob": -0.12270735291873708, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0004541777598205954}, {"id": 886, "seek": 379234, "start": 3810.1000000000004, "end": 3814.26, "text": " We have foundation models and and they are better than the specialized models in a strong sense", "tokens": [51252, 492, 362, 7030, 5245, 293, 293, 436, 366, 1101, 813, 264, 19813, 5245, 294, 257, 2068, 2020, 51460], "temperature": 0.0, "avg_logprob": -0.12270735291873708, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0004541777598205954}, {"id": 887, "seek": 379234, "start": 3814.6600000000003, "end": 3817.78, "text": " And you know and I like to sort of push on this a little bit and see well", "tokens": [51480, 400, 291, 458, 293, 286, 411, 281, 1333, 295, 2944, 322, 341, 257, 707, 857, 293, 536, 731, 51636], "temperature": 0.0, "avg_logprob": -0.12270735291873708, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0004541777598205954}, {"id": 888, "seek": 381778, "start": 3817.86, "end": 3823.7000000000003, "text": " When when does it break because we know that there are like these physics inspired models with inductive priors that you know", "tokens": [50368, 1133, 562, 775, 309, 1821, 570, 321, 458, 300, 456, 366, 411, 613, 10649, 7547, 5245, 365, 31612, 488, 1790, 830, 300, 291, 458, 50660], "temperature": 0.0, "avg_logprob": -0.13665125920222357, "compression_ratio": 1.8018867924528301, "no_speech_prob": 0.0025490091647952795}, {"id": 889, "seek": 381778, "start": 3823.78, "end": 3827.94, "text": " Know about invariance of you know like molecules and drug discovery and stuff like that", "tokens": [50664, 10265, 466, 33270, 719, 295, 291, 458, 411, 13093, 293, 4110, 12114, 293, 1507, 411, 300, 50872], "temperature": 0.0, "avg_logprob": -0.13665125920222357, "compression_ratio": 1.8018867924528301, "no_speech_prob": 0.0025490091647952795}, {"id": 890, "seek": 381778, "start": 3827.94, "end": 3830.26, "text": " And surely they would be better than a language model", "tokens": [50872, 400, 11468, 436, 576, 312, 1101, 813, 257, 2856, 2316, 50988], "temperature": 0.0, "avg_logprob": -0.13665125920222357, "compression_ratio": 1.8018867924528301, "no_speech_prob": 0.0025490091647952795}, {"id": 891, "seek": 381778, "start": 3830.26, "end": 3834.42, "text": " But no no no now they're training language models on mathematical conjecturing and like you know like", "tokens": [50988, 583, 572, 572, 572, 586, 436, 434, 3097, 2856, 5245, 322, 18894, 416, 1020, 1345, 293, 411, 291, 458, 411, 51196], "temperature": 0.0, "avg_logprob": -0.13665125920222357, "compression_ratio": 1.8018867924528301, "no_speech_prob": 0.0025490091647952795}, {"id": 892, "seek": 381778, "start": 3835.0600000000004, "end": 3839.7000000000003, "text": " Drug formulation using tokens and and so on so you know as an outsider you might just think well", "tokens": [51228, 35806, 37642, 1228, 22667, 293, 293, 370, 322, 370, 291, 458, 382, 364, 40484, 291, 1062, 445, 519, 731, 51460], "temperature": 0.0, "avg_logprob": -0.13665125920222357, "compression_ratio": 1.8018867924528301, "no_speech_prob": 0.0025490091647952795}, {"id": 893, "seek": 381778, "start": 3839.7000000000003, "end": 3841.86, "text": " We can just use a big transformers model for everything", "tokens": [51460, 492, 393, 445, 764, 257, 955, 4088, 433, 2316, 337, 1203, 51568], "temperature": 0.0, "avg_logprob": -0.13665125920222357, "compression_ratio": 1.8018867924528301, "no_speech_prob": 0.0025490091647952795}, {"id": 894, "seek": 381778, "start": 3842.82, "end": 3846.26, "text": " I I think a lot of this does come from um well, so", "tokens": [51616, 286, 286, 519, 257, 688, 295, 341, 775, 808, 490, 1105, 731, 11, 370, 51788], "temperature": 0.0, "avg_logprob": -0.13665125920222357, "compression_ratio": 1.8018867924528301, "no_speech_prob": 0.0025490091647952795}, {"id": 895, "seek": 384626, "start": 3847.2200000000003, "end": 3849.5400000000004, "text": " I think the attention-based transformer architecture is", "tokens": [50412, 286, 519, 264, 3202, 12, 6032, 31782, 9482, 307, 50528], "temperature": 0.0, "avg_logprob": -0.1108237888502038, "compression_ratio": 1.7516129032258065, "no_speech_prob": 0.0002959414850920439}, {"id": 896, "seek": 384626, "start": 3850.1000000000004, "end": 3855.1400000000003, "text": " Proven empirically to just be highly scalable highly effective at learning lots of different kinds of data distributions", "tokens": [50556, 1705, 553, 25790, 984, 281, 445, 312, 5405, 38481, 5405, 4942, 412, 2539, 3195, 295, 819, 3685, 295, 1412, 37870, 50808], "temperature": 0.0, "avg_logprob": -0.1108237888502038, "compression_ratio": 1.7516129032258065, "no_speech_prob": 0.0002959414850920439}, {"id": 897, "seek": 384626, "start": 3855.78, "end": 3859.3, "text": " But I think also part of it is just that we're just starting to enter this regime", "tokens": [50840, 583, 286, 519, 611, 644, 295, 309, 307, 445, 300, 321, 434, 445, 2891, 281, 3242, 341, 13120, 51016], "temperature": 0.0, "avg_logprob": -0.1108237888502038, "compression_ratio": 1.7516129032258065, "no_speech_prob": 0.0002959414850920439}, {"id": 898, "seek": 384626, "start": 3859.3, "end": 3864.5, "text": " When we're just training these models on an insanely large amount of data, and I think that a lot of times", "tokens": [51016, 1133, 321, 434, 445, 3097, 613, 5245, 322, 364, 40965, 2416, 2372, 295, 1412, 11, 293, 286, 519, 300, 257, 688, 295, 1413, 51276], "temperature": 0.0, "avg_logprob": -0.1108237888502038, "compression_ratio": 1.7516129032258065, "no_speech_prob": 0.0002959414850920439}, {"id": 899, "seek": 384626, "start": 3865.38, "end": 3869.78, "text": " We need to sort of take a step back and really consider the amazing performances on different tasks", "tokens": [51320, 492, 643, 281, 1333, 295, 747, 257, 1823, 646, 293, 534, 1949, 264, 2243, 16087, 322, 819, 9608, 51540], "temperature": 0.0, "avg_logprob": -0.1108237888502038, "compression_ratio": 1.7516129032258065, "no_speech_prob": 0.0002959414850920439}, {"id": 900, "seek": 384626, "start": 3870.1800000000003, "end": 3873.6200000000003, "text": " And really think about you know how much information was actually leaked into", "tokens": [51560, 400, 534, 519, 466, 291, 458, 577, 709, 1589, 390, 767, 31779, 666, 51732], "temperature": 0.0, "avg_logprob": -0.1108237888502038, "compression_ratio": 1.7516129032258065, "no_speech_prob": 0.0002959414850920439}, {"id": 901, "seek": 387362, "start": 3874.3399999999997, "end": 3876.3399999999997, "text": " This task in the training data because", "tokens": [50400, 639, 5633, 294, 264, 3097, 1412, 570, 50500], "temperature": 0.0, "avg_logprob": -0.12535470008850097, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0055541363544762135}, {"id": 902, "seek": 387362, "start": 3877.2999999999997, "end": 3879.2999999999997, "text": " Right now. We're really just training", "tokens": [50548, 1779, 586, 13, 492, 434, 534, 445, 3097, 50648], "temperature": 0.0, "avg_logprob": -0.12535470008850097, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0055541363544762135}, {"id": 903, "seek": 387362, "start": 3880.02, "end": 3882.02, "text": " these huge models on", "tokens": [50684, 613, 2603, 5245, 322, 50784], "temperature": 0.0, "avg_logprob": -0.12535470008850097, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0055541363544762135}, {"id": 904, "seek": 387362, "start": 3882.02, "end": 3885.7, "text": " I think I would say that we're largely training them on the test distribution in many cases", "tokens": [50784, 286, 519, 286, 576, 584, 300, 321, 434, 11611, 3097, 552, 322, 264, 1500, 7316, 294, 867, 3331, 50968], "temperature": 0.0, "avg_logprob": -0.12535470008850097, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0055541363544762135}, {"id": 905, "seek": 387362, "start": 3886.42, "end": 3889.7, "text": " I do there I have seen like lots of examples of", "tokens": [51004, 286, 360, 456, 286, 362, 1612, 411, 3195, 295, 5110, 295, 51168], "temperature": 0.0, "avg_logprob": -0.12535470008850097, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0055541363544762135}, {"id": 906, "seek": 387362, "start": 3890.18, "end": 3893.54, "text": " Truly impressive behaviors from these models that that do seem like", "tokens": [51192, 43548, 8992, 15501, 490, 613, 5245, 300, 300, 360, 1643, 411, 51360], "temperature": 0.0, "avg_logprob": -0.12535470008850097, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0055541363544762135}, {"id": 907, "seek": 387362, "start": 3894.3399999999997, "end": 3897.22, "text": " Truly novel like zero shot generalization to unseen tasks", "tokens": [51400, 43548, 7613, 411, 4018, 3347, 2674, 2144, 281, 40608, 9608, 51544], "temperature": 0.0, "avg_logprob": -0.12535470008850097, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0055541363544762135}, {"id": 908, "seek": 387362, "start": 3897.7, "end": 3900.8199999999997, "text": " Like there was a recent example. I saw on twitter or someone", "tokens": [51568, 1743, 456, 390, 257, 5162, 1365, 13, 286, 1866, 322, 21439, 420, 1580, 51724], "temperature": 0.0, "avg_logprob": -0.12535470008850097, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0055541363544762135}, {"id": 909, "seek": 390082, "start": 3901.38, "end": 3904.5800000000004, "text": " Had like a very low resource like rare language and they gave it a few", "tokens": [50392, 12298, 411, 257, 588, 2295, 7684, 411, 5892, 2856, 293, 436, 2729, 309, 257, 1326, 50552], "temperature": 0.0, "avg_logprob": -0.10718748210805708, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.0020489308517426252}, {"id": 910, "seek": 390082, "start": 3904.98, "end": 3910.98, "text": " They gave I think the cloud 3 model a few examples and it was able to essentially perfectly reproduce new utterances in that language", "tokens": [50572, 814, 2729, 286, 519, 264, 4588, 805, 2316, 257, 1326, 5110, 293, 309, 390, 1075, 281, 4476, 6239, 29501, 777, 17567, 2676, 294, 300, 2856, 50872], "temperature": 0.0, "avg_logprob": -0.10718748210805708, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.0020489308517426252}, {"id": 911, "seek": 390082, "start": 3911.54, "end": 3913.54, "text": " So that does seem very impressive", "tokens": [50900, 407, 300, 775, 1643, 588, 8992, 51000], "temperature": 0.0, "avg_logprob": -0.10718748210805708, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.0020489308517426252}, {"id": 912, "seek": 390082, "start": 3913.78, "end": 3919.1400000000003, "text": " But it does seem at the same time, you know a lot of the performances for example on elsat or like ap biology exams", "tokens": [51012, 583, 309, 775, 1643, 412, 264, 912, 565, 11, 291, 458, 257, 688, 295, 264, 16087, 337, 1365, 322, 10302, 267, 420, 411, 1882, 14956, 20514, 51280], "temperature": 0.0, "avg_logprob": -0.10718748210805708, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.0020489308517426252}, {"id": 913, "seek": 390082, "start": 3919.3, "end": 3921.38, "text": " I imagine a lot of that is really a function of just", "tokens": [51288, 286, 3811, 257, 688, 295, 300, 307, 534, 257, 2445, 295, 445, 51392], "temperature": 0.0, "avg_logprob": -0.10718748210805708, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.0020489308517426252}, {"id": 914, "seek": 390082, "start": 3922.34, "end": 3927.78, "text": " Literally giving the model the test domain in terms of information during the training step", "tokens": [51440, 23768, 2902, 264, 2316, 264, 1500, 9274, 294, 2115, 295, 1589, 1830, 264, 3097, 1823, 51712], "temperature": 0.0, "avg_logprob": -0.10718748210805708, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.0020489308517426252}, {"id": 915, "seek": 392778, "start": 3928.1800000000003, "end": 3929.2200000000003, "text": " Okay, okay", "tokens": [50384, 1033, 11, 1392, 50436], "temperature": 0.0, "avg_logprob": -0.10782488584518432, "compression_ratio": 1.9880597014925374, "no_speech_prob": 0.01532794814556837}, {"id": 916, "seek": 392778, "start": 3929.2200000000003, "end": 3932.26, "text": " So there are like two schools of thought on this when we talk about world models", "tokens": [50436, 407, 456, 366, 411, 732, 4656, 295, 1194, 322, 341, 562, 321, 751, 466, 1002, 5245, 50588], "temperature": 0.0, "avg_logprob": -0.10782488584518432, "compression_ratio": 1.9880597014925374, "no_speech_prob": 0.01532794814556837}, {"id": 917, "seek": 392778, "start": 3932.26, "end": 3936.98, "text": " You know people are talking about sorrow and is it building a world model and and it certainly seems to be it seems to be doing", "tokens": [50588, 509, 458, 561, 366, 1417, 466, 23027, 293, 307, 309, 2390, 257, 1002, 2316, 293, 293, 309, 3297, 2544, 281, 312, 309, 2544, 281, 312, 884, 50824], "temperature": 0.0, "avg_logprob": -0.10782488584518432, "compression_ratio": 1.9880597014925374, "no_speech_prob": 0.01532794814556837}, {"id": 918, "seek": 392778, "start": 3937.0600000000004, "end": 3938.42, "text": " I mean, obviously it's not doing navier stokes", "tokens": [50828, 286, 914, 11, 2745, 309, 311, 406, 884, 5947, 811, 342, 8606, 50896], "temperature": 0.0, "avg_logprob": -0.10782488584518432, "compression_ratio": 1.9880597014925374, "no_speech_prob": 0.01532794814556837}, {"id": 919, "seek": 392778, "start": 3938.42, "end": 3941.2200000000003, "text": " It's not doing like fluid dynamics, but it seems to be doing something like that", "tokens": [50896, 467, 311, 406, 884, 411, 9113, 15679, 11, 457, 309, 2544, 281, 312, 884, 746, 411, 300, 51036], "temperature": 0.0, "avg_logprob": -0.10782488584518432, "compression_ratio": 1.9880597014925374, "no_speech_prob": 0.01532794814556837}, {"id": 920, "seek": 392778, "start": 3941.46, "end": 3945.1400000000003, "text": " So like one extreme view is that it is just a hash table", "tokens": [51048, 407, 411, 472, 8084, 1910, 307, 300, 309, 307, 445, 257, 22019, 3199, 51232], "temperature": 0.0, "avg_logprob": -0.10782488584518432, "compression_ratio": 1.9880597014925374, "no_speech_prob": 0.01532794814556837}, {"id": 921, "seek": 392778, "start": 3945.5400000000004, "end": 3952.1800000000003, "text": " And you know, it's it's kind of doing some diffused approximate retrieval or whatever another school of thought is that it's like a simulator", "tokens": [51252, 400, 291, 458, 11, 309, 311, 309, 311, 733, 295, 884, 512, 7593, 4717, 30874, 19817, 3337, 420, 2035, 1071, 1395, 295, 1194, 307, 300, 309, 311, 411, 257, 32974, 51584], "temperature": 0.0, "avg_logprob": -0.10782488584518432, "compression_ratio": 1.9880597014925374, "no_speech_prob": 0.01532794814556837}, {"id": 922, "seek": 392778, "start": 3952.42, "end": 3957.2200000000003, "text": " And you know people talk about the simulator's view of large language models and you know, like it's like it's modeling", "tokens": [51596, 400, 291, 458, 561, 751, 466, 264, 32974, 311, 1910, 295, 2416, 2856, 5245, 293, 291, 458, 11, 411, 309, 311, 411, 309, 311, 15983, 51836], "temperature": 0.0, "avg_logprob": -0.10782488584518432, "compression_ratio": 1.9880597014925374, "no_speech_prob": 0.01532794814556837}, {"id": 923, "seek": 395722, "start": 3957.54, "end": 3960.66, "text": " Not only, you know, just to just just the words and the language", "tokens": [50380, 1726, 787, 11, 291, 458, 11, 445, 281, 445, 445, 264, 2283, 293, 264, 2856, 50536], "temperature": 0.0, "avg_logprob": -0.11745365142822266, "compression_ratio": 1.9085173501577286, "no_speech_prob": 0.0015222327783703804}, {"id": 924, "seek": 395722, "start": 3960.74, "end": 3964.5, "text": " But it's also implicitly learned to model the world and the people and all of us", "tokens": [50540, 583, 309, 311, 611, 26947, 356, 3264, 281, 2316, 264, 1002, 293, 264, 561, 293, 439, 295, 505, 50728], "temperature": 0.0, "avg_logprob": -0.11745365142822266, "compression_ratio": 1.9085173501577286, "no_speech_prob": 0.0015222327783703804}, {"id": 925, "seek": 395722, "start": 3964.98, "end": 3969.7, "text": " So that's the spectrum. I mean like uh mark, where do you think these things are on that spectrum?", "tokens": [50752, 407, 300, 311, 264, 11143, 13, 286, 914, 411, 2232, 1491, 11, 689, 360, 291, 519, 613, 721, 366, 322, 300, 11143, 30, 50988], "temperature": 0.0, "avg_logprob": -0.11745365142822266, "compression_ratio": 1.9085173501577286, "no_speech_prob": 0.0015222327783703804}, {"id": 926, "seek": 395722, "start": 3969.9399999999996, "end": 3974.3399999999997, "text": " Yeah, I think we have like it would be great to be able to play around with it and kind of see what we can get out of it", "tokens": [51000, 865, 11, 286, 519, 321, 362, 411, 309, 576, 312, 869, 281, 312, 1075, 281, 862, 926, 365, 309, 293, 733, 295, 536, 437, 321, 393, 483, 484, 295, 309, 51220], "temperature": 0.0, "avg_logprob": -0.11745365142822266, "compression_ratio": 1.9085173501577286, "no_speech_prob": 0.0015222327783703804}, {"id": 927, "seek": 395722, "start": 3974.4199999999996, "end": 3976.8999999999996, "text": " But I think I think if you can for example", "tokens": [51224, 583, 286, 519, 286, 519, 498, 291, 393, 337, 1365, 51348], "temperature": 0.0, "avg_logprob": -0.11745365142822266, "compression_ratio": 1.9085173501577286, "no_speech_prob": 0.0015222327783703804}, {"id": 928, "seek": 395722, "start": 3977.8599999999997, "end": 3980.8999999999996, "text": " You know after each kind of you know, so it's a language condition model", "tokens": [51396, 509, 458, 934, 1184, 733, 295, 291, 458, 11, 370, 309, 311, 257, 2856, 4188, 2316, 51548], "temperature": 0.0, "avg_logprob": -0.11745365142822266, "compression_ratio": 1.9085173501577286, "no_speech_prob": 0.0015222327783703804}, {"id": 929, "seek": 395722, "start": 3980.98, "end": 3986.74, "text": " So if after each kind of frame you could for example put in a different language language kind of conditioning and say like", "tokens": [51552, 407, 498, 934, 1184, 733, 295, 3920, 291, 727, 337, 1365, 829, 294, 257, 819, 2856, 2856, 733, 295, 21901, 293, 584, 411, 51840], "temperature": 0.0, "avg_logprob": -0.11745365142822266, "compression_ratio": 1.9085173501577286, "no_speech_prob": 0.0015222327783703804}, {"id": 930, "seek": 398722, "start": 3987.22, "end": 3992.8199999999997, "text": " You know, what happens here if you know, the mug was pushed off the table instead of whatever else was originally happening in the video", "tokens": [50364, 509, 458, 11, 437, 2314, 510, 498, 291, 458, 11, 264, 23610, 390, 9152, 766, 264, 3199, 2602, 295, 2035, 1646, 390, 7993, 2737, 294, 264, 960, 50644], "temperature": 0.0, "avg_logprob": -0.10841004053751628, "compression_ratio": 1.9233226837060702, "no_speech_prob": 0.0010002620983868837}, {"id": 931, "seek": 398722, "start": 3993.06, "end": 3997.3799999999997, "text": " And so if you can basically do this kind of like counterfactual like interventional predictions where you kind of", "tokens": [50656, 400, 370, 498, 291, 393, 1936, 360, 341, 733, 295, 411, 5682, 44919, 901, 411, 13176, 304, 21264, 689, 291, 733, 295, 50872], "temperature": 0.0, "avg_logprob": -0.10841004053751628, "compression_ratio": 1.9233226837060702, "no_speech_prob": 0.0010002620983868837}, {"id": 932, "seek": 398722, "start": 3997.8599999999997, "end": 4001.9399999999996, "text": " Give some new action and then you're able to see like the alternative outcome of that new action", "tokens": [50896, 5303, 512, 777, 3069, 293, 550, 291, 434, 1075, 281, 536, 411, 264, 8535, 9700, 295, 300, 777, 3069, 51100], "temperature": 0.0, "avg_logprob": -0.10841004053751628, "compression_ratio": 1.9233226837060702, "no_speech_prob": 0.0010002620983868837}, {"id": 933, "seek": 398722, "start": 4002.5, "end": 4008.3399999999997, "text": " I think if the model is able to do that then I would think that it does have a pretty good understanding of how the world works in the sense of", "tokens": [51128, 286, 519, 498, 264, 2316, 307, 1075, 281, 360, 300, 550, 286, 576, 519, 300, 309, 775, 362, 257, 1238, 665, 3701, 295, 577, 264, 1002, 1985, 294, 264, 2020, 295, 51420], "temperature": 0.0, "avg_logprob": -0.10841004053751628, "compression_ratio": 1.9233226837060702, "no_speech_prob": 0.0010002620983868837}, {"id": 934, "seek": 398722, "start": 4008.74, "end": 4014.1, "text": " You know, I really think like if you can predict the outcome of any action given some sequence of observations", "tokens": [51440, 509, 458, 11, 286, 534, 519, 411, 498, 291, 393, 6069, 264, 9700, 295, 604, 3069, 2212, 512, 8310, 295, 18163, 51708], "temperature": 0.0, "avg_logprob": -0.10841004053751628, "compression_ratio": 1.9233226837060702, "no_speech_prob": 0.0010002620983868837}, {"id": 935, "seek": 401410, "start": 4014.1, "end": 4018.58, "text": " I do think that's a pretty good proxy for being able to say if you can do that you really do understand how the world works", "tokens": [50364, 286, 360, 519, 300, 311, 257, 1238, 665, 29690, 337, 885, 1075, 281, 584, 498, 291, 393, 360, 300, 291, 534, 360, 1223, 577, 264, 1002, 1985, 50588], "temperature": 0.0, "avg_logprob": -0.09736972088580365, "compression_ratio": 1.8832335329341316, "no_speech_prob": 0.0018672379665076733}, {"id": 936, "seek": 401410, "start": 4020.98, "end": 4022.66, "text": " And so I think if the model can do that", "tokens": [50708, 400, 370, 286, 519, 498, 264, 2316, 393, 360, 300, 50792], "temperature": 0.0, "avg_logprob": -0.09736972088580365, "compression_ratio": 1.8832335329341316, "no_speech_prob": 0.0018672379665076733}, {"id": 937, "seek": 401410, "start": 4022.66, "end": 4026.8199999999997, "text": " I would be kind of inclined to say that it does have a kind of world model in the sense of understanding", "tokens": [50792, 286, 576, 312, 733, 295, 28173, 281, 584, 300, 309, 775, 362, 257, 733, 295, 1002, 2316, 294, 264, 2020, 295, 3701, 51000], "temperature": 0.0, "avg_logprob": -0.09736972088580365, "compression_ratio": 1.8832335329341316, "no_speech_prob": 0.0018672379665076733}, {"id": 938, "seek": 401410, "start": 4027.22, "end": 4029.54, "text": " The underlying world but then there might also be a chance that you know", "tokens": [51020, 440, 14217, 1002, 457, 550, 456, 1062, 611, 312, 257, 2931, 300, 291, 458, 51136], "temperature": 0.0, "avg_logprob": -0.09736972088580365, "compression_ratio": 1.8832335329341316, "no_speech_prob": 0.0018672379665076733}, {"id": 939, "seek": 401410, "start": 4030.3399999999997, "end": 4035.7, "text": " You know these models aren't like you said it's more just like a diffuse retrieval and perhaps if you try and do like a very", "tokens": [51176, 509, 458, 613, 5245, 3212, 380, 411, 291, 848, 309, 311, 544, 445, 411, 257, 42165, 19817, 3337, 293, 4317, 498, 291, 853, 293, 360, 411, 257, 588, 51444], "temperature": 0.0, "avg_logprob": -0.09736972088580365, "compression_ratio": 1.8832335329341316, "no_speech_prob": 0.0018672379665076733}, {"id": 940, "seek": 401410, "start": 4036.1, "end": 4039.54, "text": " Fine grain conditioning on a slightly different outcome different like conditioning", "tokens": [51464, 12024, 12837, 21901, 322, 257, 4748, 819, 9700, 819, 411, 21901, 51636], "temperature": 0.0, "avg_logprob": -0.09736972088580365, "compression_ratio": 1.8832335329341316, "no_speech_prob": 0.0018672379665076733}, {"id": 941, "seek": 401410, "start": 4039.54, "end": 4042.2599999999998, "text": " Maybe it won't actually give you the correct kind of counterfactual prediction", "tokens": [51636, 2704, 309, 1582, 380, 767, 976, 291, 264, 3006, 733, 295, 5682, 44919, 901, 17630, 51772], "temperature": 0.0, "avg_logprob": -0.09736972088580365, "compression_ratio": 1.8832335329341316, "no_speech_prob": 0.0018672379665076733}, {"id": 942, "seek": 404226, "start": 4042.6600000000003, "end": 4047.46, "text": " And so I think maybe we'd have to see how good these models are at generalizing to slightly different inputs and things like that", "tokens": [50384, 400, 370, 286, 519, 1310, 321, 1116, 362, 281, 536, 577, 665, 613, 5245, 366, 412, 2674, 3319, 281, 4748, 819, 15743, 293, 721, 411, 300, 50624], "temperature": 0.0, "avg_logprob": -0.12880852172424742, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.0014521073317155242}, {"id": 943, "seek": 404226, "start": 4047.7000000000003, "end": 4052.5, "text": " To really see if it understands things well, or it is just like kind of generating some arbitrary video", "tokens": [50636, 1407, 534, 536, 498, 309, 15146, 721, 731, 11, 420, 309, 307, 445, 411, 733, 295, 17746, 512, 23211, 960, 50876], "temperature": 0.0, "avg_logprob": -0.12880852172424742, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.0014521073317155242}, {"id": 944, "seek": 404226, "start": 4052.7400000000002, "end": 4057.7000000000003, "text": " Yeah, I think it's a double whammy because our colloquial use of language and like you know use of models and intelligence", "tokens": [50888, 865, 11, 286, 519, 309, 311, 257, 3834, 315, 335, 2226, 570, 527, 1263, 29826, 831, 764, 295, 2856, 293, 411, 291, 458, 764, 295, 5245, 293, 7599, 51136], "temperature": 0.0, "avg_logprob": -0.12880852172424742, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.0014521073317155242}, {"id": 945, "seek": 404226, "start": 4057.7000000000003, "end": 4062.98, "text": " It's so static that like, you know, we we um, we think of that as being intelligence", "tokens": [51136, 467, 311, 370, 13437, 300, 411, 11, 291, 458, 11, 321, 321, 1105, 11, 321, 519, 295, 300, 382, 885, 7599, 51400], "temperature": 0.0, "avg_logprob": -0.12880852172424742, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.0014521073317155242}, {"id": 946, "seek": 404226, "start": 4063.38, "end": 4066.98, "text": " But but we're still going like we're now create we're creating knowledge right now", "tokens": [51420, 583, 457, 321, 434, 920, 516, 411, 321, 434, 586, 1884, 321, 434, 4084, 3601, 558, 586, 51600], "temperature": 0.0, "avg_logprob": -0.12880852172424742, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.0014521073317155242}, {"id": 947, "seek": 404226, "start": 4066.98, "end": 4069.94, "text": " We're creating models because we're exploring we're doing exactly what you said Minshew", "tokens": [51600, 492, 434, 4084, 5245, 570, 321, 434, 12736, 321, 434, 884, 2293, 437, 291, 848, 49239, 17418, 51748], "temperature": 0.0, "avg_logprob": -0.12880852172424742, "compression_ratio": 1.8106508875739644, "no_speech_prob": 0.0014521073317155242}, {"id": 948, "seek": 406994, "start": 4069.94, "end": 4073.78, "text": " We're like we're exploring the search space and we're building models and we're combining them together", "tokens": [50364, 492, 434, 411, 321, 434, 12736, 264, 3164, 1901, 293, 321, 434, 2390, 5245, 293, 321, 434, 21928, 552, 1214, 50556], "temperature": 0.0, "avg_logprob": -0.12406387329101562, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.005286871921271086}, {"id": 949, "seek": 406994, "start": 4074.1, "end": 4077.78, "text": " And you know, presumably we would diverge quite quickly from from from the language models", "tokens": [50572, 400, 291, 458, 11, 26742, 321, 576, 18558, 432, 1596, 2661, 490, 490, 490, 264, 2856, 5245, 50756], "temperature": 0.0, "avg_logprob": -0.12406387329101562, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.005286871921271086}, {"id": 950, "seek": 406994, "start": 4077.78, "end": 4082.9, "text": " But I mean what what's your take on on this idea that they are, you know, potentially world simulators? Yeah", "tokens": [50756, 583, 286, 914, 437, 437, 311, 428, 747, 322, 322, 341, 1558, 300, 436, 366, 11, 291, 458, 11, 7263, 1002, 1034, 39265, 30, 865, 51012], "temperature": 0.0, "avg_logprob": -0.12406387329101562, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.005286871921271086}, {"id": 951, "seek": 406994, "start": 4083.3, "end": 4088.7400000000002, "text": " um, so just regarding the the sort of lookup analogy for these large models, I think it's", "tokens": [51032, 1105, 11, 370, 445, 8595, 264, 264, 1333, 295, 574, 1010, 21663, 337, 613, 2416, 5245, 11, 286, 519, 309, 311, 51304], "temperature": 0.0, "avg_logprob": -0.12406387329101562, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.005286871921271086}, {"id": 952, "seek": 406994, "start": 4089.46, "end": 4095.06, "text": " So my mental model is similar to that. Um, although I think it's it's very close to um, I think a really good write-up of", "tokens": [51340, 407, 452, 4973, 2316, 307, 2531, 281, 300, 13, 3301, 11, 4878, 286, 519, 309, 311, 309, 311, 588, 1998, 281, 1105, 11, 286, 519, 257, 534, 665, 2464, 12, 1010, 295, 51620], "temperature": 0.0, "avg_logprob": -0.12406387329101562, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.005286871921271086}, {"id": 953, "seek": 406994, "start": 4095.62, "end": 4097.62, "text": " of the of this alternative take", "tokens": [51648, 295, 264, 295, 341, 8535, 747, 51748], "temperature": 0.0, "avg_logprob": -0.12406387329101562, "compression_ratio": 1.7588424437299035, "no_speech_prob": 0.005286871921271086}, {"id": 954, "seek": 409762, "start": 4098.099999999999, "end": 4099.38, "text": " Which is more like", "tokens": [50388, 3013, 307, 544, 411, 50452], "temperature": 0.0, "avg_logprob": -0.15190929836697048, "compression_ratio": 1.7377622377622377, "no_speech_prob": 0.0046081324107944965}, {"id": 955, "seek": 409762, "start": 4099.38, "end": 4101.94, "text": " There's an alternative take which is that it is kind of like a lookup table", "tokens": [50452, 821, 311, 364, 8535, 747, 597, 307, 300, 309, 307, 733, 295, 411, 257, 574, 1010, 3199, 50580], "temperature": 0.0, "avg_logprob": -0.15190929836697048, "compression_ratio": 1.7377622377622377, "no_speech_prob": 0.0046081324107944965}, {"id": 956, "seek": 409762, "start": 4102.18, "end": 4108.42, "text": " But the prompt itself is a key that maps not to a specific sort of response, but to potentially like a function", "tokens": [50592, 583, 264, 12391, 2564, 307, 257, 2141, 300, 11317, 406, 281, 257, 2685, 1333, 295, 4134, 11, 457, 281, 7263, 411, 257, 2445, 50904], "temperature": 0.0, "avg_logprob": -0.15190929836697048, "compression_ratio": 1.7377622377622377, "no_speech_prob": 0.0046081324107944965}, {"id": 957, "seek": 409762, "start": 4108.74, "end": 4112.66, "text": " Yeah, and a vast space of functions and france wash relay had a really good", "tokens": [50920, 865, 11, 293, 257, 8369, 1901, 295, 6828, 293, 431, 719, 5675, 24214, 632, 257, 534, 665, 51116], "temperature": 0.0, "avg_logprob": -0.15190929836697048, "compression_ratio": 1.7377622377622377, "no_speech_prob": 0.0046081324107944965}, {"id": 958, "seek": 409762, "start": 4113.22, "end": 4116.18, "text": " Sort of blog post where he kind of goes more into the details of this viewpoint", "tokens": [51144, 26149, 295, 6968, 2183, 689, 415, 733, 295, 1709, 544, 666, 264, 4365, 295, 341, 35248, 51292], "temperature": 0.0, "avg_logprob": -0.15190929836697048, "compression_ratio": 1.7377622377622377, "no_speech_prob": 0.0046081324107944965}, {"id": 959, "seek": 409762, "start": 4116.34, "end": 4122.42, "text": " But I think that that really, you know resonates with my intuition of how these things behave where it's not literally looking up like", "tokens": [51300, 583, 286, 519, 300, 300, 534, 11, 291, 458, 41051, 365, 452, 24002, 295, 577, 613, 721, 15158, 689, 309, 311, 406, 3736, 1237, 493, 411, 51604], "temperature": 0.0, "avg_logprob": -0.15190929836697048, "compression_ratio": 1.7377622377622377, "no_speech_prob": 0.0046081324107944965}, {"id": 960, "seek": 412242, "start": 4122.66, "end": 4124.66, "text": " A key value in a hash table", "tokens": [50376, 316, 2141, 2158, 294, 257, 22019, 3199, 50476], "temperature": 0.0, "avg_logprob": -0.1015274544072345, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.08629316091537476}, {"id": 961, "seek": 412242, "start": 4124.74, "end": 4129.14, "text": " It seems more like it's these models have learned over tremendous amounts of data to compress that data", "tokens": [50480, 467, 2544, 544, 411, 309, 311, 613, 5245, 362, 3264, 670, 10048, 11663, 295, 1412, 281, 14778, 300, 1412, 50700], "temperature": 0.0, "avg_logprob": -0.1015274544072345, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.08629316091537476}, {"id": 962, "seek": 412242, "start": 4129.14, "end": 4134.26, "text": " They have to learn, I think more abstract functions that help to explain that data and therefore they're learning functions", "tokens": [50700, 814, 362, 281, 1466, 11, 286, 519, 544, 12649, 6828, 300, 854, 281, 2903, 300, 1412, 293, 4412, 436, 434, 2539, 6828, 50956], "temperature": 0.0, "avg_logprob": -0.1015274544072345, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.08629316091537476}, {"id": 963, "seek": 412242, "start": 4134.42, "end": 4136.42, "text": " So they're approximating some kind of function", "tokens": [50964, 407, 436, 434, 8542, 990, 512, 733, 295, 2445, 51064], "temperature": 0.0, "avg_logprob": -0.1015274544072345, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.08629316091537476}, {"id": 964, "seek": 412242, "start": 4136.66, "end": 4138.18, "text": " Or a vast family of functions", "tokens": [51076, 1610, 257, 8369, 1605, 295, 6828, 51152], "temperature": 0.0, "avg_logprob": -0.1015274544072345, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.08629316091537476}, {"id": 965, "seek": 412242, "start": 4138.5, "end": 4142.26, "text": " And I think the prompt really acts like as a key that essentially activates a particular function", "tokens": [51168, 400, 286, 519, 264, 12391, 534, 10672, 411, 382, 257, 2141, 300, 4476, 43869, 257, 1729, 2445, 51356], "temperature": 0.0, "avg_logprob": -0.1015274544072345, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.08629316091537476}, {"id": 966, "seek": 412242, "start": 4142.5, "end": 4148.1, "text": " And so you can kind of think of you know in the classical world where one neural network equals one function like basically it's mapping from", "tokens": [51368, 400, 370, 291, 393, 733, 295, 519, 295, 291, 458, 294, 264, 13735, 1002, 689, 472, 18161, 3209, 6915, 472, 2445, 411, 1936, 309, 311, 18350, 490, 51648], "temperature": 0.0, "avg_logprob": -0.1015274544072345, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.08629316091537476}, {"id": 967, "seek": 414810, "start": 4148.5, "end": 4153.06, "text": " Images to image net labels now like foundation model in the foundation model regime", "tokens": [50384, 4331, 1660, 281, 3256, 2533, 16949, 586, 411, 7030, 2316, 294, 264, 7030, 2316, 13120, 50612], "temperature": 0.0, "avg_logprob": -0.10431075096130371, "compression_ratio": 1.923913043478261, "no_speech_prob": 0.006690823473036289}, {"id": 968, "seek": 414810, "start": 4153.14, "end": 4158.26, "text": " It's like one foundation model is essentially kind of like a giant database of lots and lots of different functions", "tokens": [50616, 467, 311, 411, 472, 7030, 2316, 307, 4476, 733, 295, 411, 257, 7410, 8149, 295, 3195, 293, 3195, 295, 819, 6828, 50872], "temperature": 0.0, "avg_logprob": -0.10431075096130371, "compression_ratio": 1.923913043478261, "no_speech_prob": 0.006690823473036289}, {"id": 969, "seek": 414810, "start": 4159.3, "end": 4162.34, "text": " That's basically activated selectively based on the input with prompt", "tokens": [50924, 663, 311, 1936, 18157, 3048, 3413, 2361, 322, 264, 4846, 365, 12391, 51076], "temperature": 0.0, "avg_logprob": -0.10431075096130371, "compression_ratio": 1.923913043478261, "no_speech_prob": 0.006690823473036289}, {"id": 970, "seek": 414810, "start": 4162.820000000001, "end": 4165.38, "text": " Um, and I do think that you know based on this", "tokens": [51100, 3301, 11, 293, 286, 360, 519, 300, 291, 458, 2361, 322, 341, 51228], "temperature": 0.0, "avg_logprob": -0.10431075096130371, "compression_ratio": 1.923913043478261, "no_speech_prob": 0.006690823473036289}, {"id": 971, "seek": 414810, "start": 4165.46, "end": 4169.780000000001, "text": " I think it's definitely possible that with enough data from the world enough experiential data", "tokens": [51232, 286, 519, 309, 311, 2138, 1944, 300, 365, 1547, 1412, 490, 264, 1002, 1547, 49611, 831, 1412, 51448], "temperature": 0.0, "avg_logprob": -0.10431075096130371, "compression_ratio": 1.923913043478261, "no_speech_prob": 0.006690823473036289}, {"id": 972, "seek": 414810, "start": 4170.02, "end": 4176.42, "text": " That these foundation models can learn sort of a basis set of dynamics and transitions that explain how the world works", "tokens": [51460, 663, 613, 7030, 5245, 393, 1466, 1333, 295, 257, 5143, 992, 295, 15679, 293, 23767, 300, 2903, 577, 264, 1002, 1985, 51780], "temperature": 0.0, "avg_logprob": -0.10431075096130371, "compression_ratio": 1.923913043478261, "no_speech_prob": 0.006690823473036289}, {"id": 973, "seek": 417642, "start": 4177.06, "end": 4182.9800000000005, "text": " And essentially if it does learn these transitions, um, for example in like the massive amount of video data that swore is trained on", "tokens": [50396, 400, 4476, 498, 309, 775, 1466, 613, 23767, 11, 1105, 11, 337, 1365, 294, 411, 264, 5994, 2372, 295, 960, 1412, 300, 1693, 418, 307, 8895, 322, 50692], "temperature": 0.0, "avg_logprob": -0.16274496108766587, "compression_ratio": 1.7466216216216217, "no_speech_prob": 0.0013668552273884416}, {"id": 974, "seek": 417642, "start": 4183.22, "end": 4188.66, "text": " Um, I would say that yeah, I would agree that they are essentially starting to approximate, uh world models", "tokens": [50704, 3301, 11, 286, 576, 584, 300, 1338, 11, 286, 576, 3986, 300, 436, 366, 4476, 2891, 281, 30874, 11, 2232, 1002, 5245, 50976], "temperature": 0.0, "avg_logprob": -0.16274496108766587, "compression_ratio": 1.7466216216216217, "no_speech_prob": 0.0013668552273884416}, {"id": 975, "seek": 417642, "start": 4189.06, "end": 4191.86, "text": " Sure. Yeah, so yeah, these are two um separate papers. So", "tokens": [50996, 4894, 13, 865, 11, 370, 1338, 11, 613, 366, 732, 1105, 4994, 10577, 13, 407, 51136], "temperature": 0.0, "avg_logprob": -0.16274496108766587, "compression_ratio": 1.7466216216216217, "no_speech_prob": 0.0013668552273884416}, {"id": 976, "seek": 417642, "start": 4192.42, "end": 4199.62, "text": " So the first one being dreamer led by like Dan and jar Haffner. So this is um, you know example of work in the space of world models and so", "tokens": [51164, 407, 264, 700, 472, 885, 3055, 260, 4684, 538, 411, 3394, 293, 15181, 389, 2518, 1193, 13, 407, 341, 307, 1105, 11, 291, 458, 1365, 295, 589, 294, 264, 1901, 295, 1002, 5245, 293, 370, 51524], "temperature": 0.0, "avg_logprob": -0.16274496108766587, "compression_ratio": 1.7466216216216217, "no_speech_prob": 0.0013668552273884416}, {"id": 977, "seek": 417642, "start": 4200.58, "end": 4203.62, "text": " Basically what dreamer involves doing is like a way of training a world model", "tokens": [51572, 8537, 437, 3055, 260, 11626, 884, 307, 411, 257, 636, 295, 3097, 257, 1002, 2316, 51724], "temperature": 0.0, "avg_logprob": -0.16274496108766587, "compression_ratio": 1.7466216216216217, "no_speech_prob": 0.0013668552273884416}, {"id": 978, "seek": 420362, "start": 4203.86, "end": 4210.82, "text": " And then also showing that you can just generate synthetic data in the small model and then optimize decision making like purely using the synthetic data", "tokens": [50376, 400, 550, 611, 4099, 300, 291, 393, 445, 8460, 23420, 1412, 294, 264, 1359, 2316, 293, 550, 19719, 3537, 1455, 411, 17491, 1228, 264, 23420, 1412, 50724], "temperature": 0.0, "avg_logprob": -0.12066325233096169, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.007691806182265282}, {"id": 979, "seek": 420362, "start": 4211.54, "end": 4212.74, "text": " um", "tokens": [50760, 1105, 50820], "temperature": 0.0, "avg_logprob": -0.12066325233096169, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.007691806182265282}, {"id": 980, "seek": 420362, "start": 4212.74, "end": 4218.58, "text": " So we talked a little bit earlier about like partially observable mdps. So we want to like take kind of the sequence of observations", "tokens": [50820, 407, 321, 2825, 257, 707, 857, 3071, 466, 411, 18886, 9951, 712, 275, 67, 1878, 13, 407, 321, 528, 281, 411, 747, 733, 295, 264, 8310, 295, 18163, 51112], "temperature": 0.0, "avg_logprob": -0.12066325233096169, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.007691806182265282}, {"id": 981, "seek": 420362, "start": 4219.38, "end": 4224.099999999999, "text": " Um, and then be able to predict like the next a distribution of the next observation given some action", "tokens": [51152, 3301, 11, 293, 550, 312, 1075, 281, 6069, 411, 264, 958, 257, 7316, 295, 264, 958, 14816, 2212, 512, 3069, 51388], "temperature": 0.0, "avg_logprob": -0.12066325233096169, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.007691806182265282}, {"id": 982, "seek": 420362, "start": 4224.9, "end": 4228.5, "text": " and so we also talked about how you might want to like compress this into like a um", "tokens": [51428, 293, 370, 321, 611, 2825, 466, 577, 291, 1062, 528, 281, 411, 14778, 341, 666, 411, 257, 1105, 51608], "temperature": 0.0, "avg_logprob": -0.12066325233096169, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.007691806182265282}, {"id": 983, "seek": 422850, "start": 4229.3, "end": 4231.86, "text": " More compressed representation of your of the previous observation", "tokens": [50404, 5048, 30353, 10290, 295, 428, 295, 264, 3894, 14816, 50532], "temperature": 0.0, "avg_logprob": -0.1315164649695681, "compression_ratio": 1.8652482269503545, "no_speech_prob": 0.012818312272429466}, {"id": 984, "seek": 422850, "start": 4231.94, "end": 4237.46, "text": " So basically what dreamer proposes to do and a lot of works on world modeling is to take your previous sequence of observations", "tokens": [50536, 407, 1936, 437, 3055, 260, 2365, 4201, 281, 360, 293, 257, 688, 295, 1985, 322, 1002, 15983, 307, 281, 747, 428, 3894, 8310, 295, 18163, 50812], "temperature": 0.0, "avg_logprob": -0.1315164649695681, "compression_ratio": 1.8652482269503545, "no_speech_prob": 0.012818312272429466}, {"id": 985, "seek": 422850, "start": 4237.86, "end": 4240.1, "text": " And then you map them to some compressed representation", "tokens": [50832, 400, 550, 291, 4471, 552, 281, 512, 30353, 10290, 50944], "temperature": 0.0, "avg_logprob": -0.1315164649695681, "compression_ratio": 1.8652482269503545, "no_speech_prob": 0.012818312272429466}, {"id": 986, "seek": 422850, "start": 4240.9, "end": 4244.74, "text": " And then could predict ahead in this latent space. Um, the next uh, latent", "tokens": [50984, 400, 550, 727, 6069, 2286, 294, 341, 48994, 1901, 13, 3301, 11, 264, 958, 2232, 11, 48994, 51176], "temperature": 0.0, "avg_logprob": -0.1315164649695681, "compression_ratio": 1.8652482269503545, "no_speech_prob": 0.012818312272429466}, {"id": 987, "seek": 422850, "start": 4245.54, "end": 4250.74, "text": " Latent state condition on the action and then yeah, the really interesting thing about this is that now, um, you know", "tokens": [51216, 7354, 317, 1785, 4188, 322, 264, 3069, 293, 550, 1338, 11, 264, 534, 1880, 551, 466, 341, 307, 300, 586, 11, 1105, 11, 291, 458, 51476], "temperature": 0.0, "avg_logprob": -0.1315164649695681, "compression_ratio": 1.8652482269503545, "no_speech_prob": 0.012818312272429466}, {"id": 988, "seek": 422850, "start": 4251.22, "end": 4254.42, "text": " We can in general predict what's going to happen to condition on different actions", "tokens": [51500, 492, 393, 294, 2674, 6069, 437, 311, 516, 281, 1051, 281, 4188, 322, 819, 5909, 51660], "temperature": 0.0, "avg_logprob": -0.1315164649695681, "compression_ratio": 1.8652482269503545, "no_speech_prob": 0.012818312272429466}, {"id": 989, "seek": 425442, "start": 4254.66, "end": 4257.7, "text": " So now if you want to get like interesting behavior out of something like dreamer", "tokens": [50376, 407, 586, 498, 291, 528, 281, 483, 411, 1880, 5223, 484, 295, 746, 411, 3055, 260, 50528], "temperature": 0.0, "avg_logprob": -0.09531043296636538, "compression_ratio": 2.0694444444444446, "no_speech_prob": 0.016911517828702927}, {"id": 990, "seek": 425442, "start": 4257.78, "end": 4261.54, "text": " You can then go ahead and generate a lot of synthetic data using dreamer", "tokens": [50532, 509, 393, 550, 352, 2286, 293, 8460, 257, 688, 295, 23420, 1412, 1228, 3055, 260, 50720], "temperature": 0.0, "avg_logprob": -0.09531043296636538, "compression_ratio": 2.0694444444444446, "no_speech_prob": 0.016911517828702927}, {"id": 991, "seek": 425442, "start": 4262.26, "end": 4265.38, "text": " Or the dreamer world model and then use that to optimize behavior", "tokens": [50756, 1610, 264, 3055, 260, 1002, 2316, 293, 550, 764, 300, 281, 19719, 5223, 50912], "temperature": 0.0, "avg_logprob": -0.09531043296636538, "compression_ratio": 2.0694444444444446, "no_speech_prob": 0.016911517828702927}, {"id": 992, "seek": 425442, "start": 4265.78, "end": 4270.42, "text": " And so in dreamer basically the way it's done is by doing like on policy reinforcement learning in the world model", "tokens": [50932, 400, 370, 294, 3055, 260, 1936, 264, 636, 309, 311, 1096, 307, 538, 884, 411, 322, 3897, 29280, 2539, 294, 264, 1002, 2316, 51164], "temperature": 0.0, "avg_logprob": -0.09531043296636538, "compression_ratio": 2.0694444444444446, "no_speech_prob": 0.016911517828702927}, {"id": 993, "seek": 425442, "start": 4270.5, "end": 4273.38, "text": " So a lot of people call this like reinforcement learning and imagination", "tokens": [51168, 407, 257, 688, 295, 561, 818, 341, 411, 29280, 2539, 293, 12938, 51312], "temperature": 0.0, "avg_logprob": -0.09531043296636538, "compression_ratio": 2.0694444444444446, "no_speech_prob": 0.016911517828702927}, {"id": 994, "seek": 425442, "start": 4273.7, "end": 4279.9400000000005, "text": " So it's basically, you know, you're imagining a bunch of synthetic data then using that to like use some standard reinforcement learning algorithm and then optimize", "tokens": [51328, 407, 309, 311, 1936, 11, 291, 458, 11, 291, 434, 27798, 257, 3840, 295, 23420, 1412, 550, 1228, 300, 281, 411, 764, 512, 3832, 29280, 2539, 9284, 293, 550, 19719, 51640], "temperature": 0.0, "avg_logprob": -0.09531043296636538, "compression_ratio": 2.0694444444444446, "no_speech_prob": 0.016911517828702927}, {"id": 995, "seek": 425442, "start": 4281.06, "end": 4282.58, "text": " behavior in some sense", "tokens": [51696, 5223, 294, 512, 2020, 51772], "temperature": 0.0, "avg_logprob": -0.09531043296636538, "compression_ratio": 2.0694444444444446, "no_speech_prob": 0.016911517828702927}, {"id": 996, "seek": 428258, "start": 4282.98, "end": 4285.86, "text": " And then you could also do other things like Monte Carlo tree search", "tokens": [50384, 400, 550, 291, 727, 611, 360, 661, 721, 411, 38105, 45112, 4230, 3164, 50528], "temperature": 0.0, "avg_logprob": -0.12212664394055382, "compression_ratio": 1.910344827586207, "no_speech_prob": 0.0018358713714405894}, {"id": 997, "seek": 428258, "start": 4285.94, "end": 4288.98, "text": " Which is like closer to like the works on on mu zero and things like this", "tokens": [50532, 3013, 307, 411, 4966, 281, 411, 264, 1985, 322, 322, 2992, 4018, 293, 721, 411, 341, 50684], "temperature": 0.0, "avg_logprob": -0.12212664394055382, "compression_ratio": 1.910344827586207, "no_speech_prob": 0.0018358713714405894}, {"id": 998, "seek": 428258, "start": 4289.54, "end": 4295.3, "text": " Creativity is a little bit like a cloud and all the creativity only happens on the surface of the cloud", "tokens": [50712, 11972, 4253, 307, 257, 707, 857, 411, 257, 4588, 293, 439, 264, 12915, 787, 2314, 322, 264, 3753, 295, 264, 4588, 51000], "temperature": 0.0, "avg_logprob": -0.12212664394055382, "compression_ratio": 1.910344827586207, "no_speech_prob": 0.0018358713714405894}, {"id": 999, "seek": 428258, "start": 4295.54, "end": 4299.94, "text": " So there's this interesting thing that like creative discovery depends on the history of all the things that I discovered before", "tokens": [51012, 407, 456, 311, 341, 1880, 551, 300, 411, 5880, 12114, 5946, 322, 264, 2503, 295, 439, 264, 721, 300, 286, 6941, 949, 51232], "temperature": 0.0, "avg_logprob": -0.12212664394055382, "compression_ratio": 1.910344827586207, "no_speech_prob": 0.0018358713714405894}, {"id": 1000, "seek": 428258, "start": 4300.18, "end": 4304.0199999999995, "text": " And typically like new discovery only happens at the end of the chain not back in in the middle", "tokens": [51244, 400, 5850, 411, 777, 12114, 787, 2314, 412, 264, 917, 295, 264, 5021, 406, 646, 294, 294, 264, 2808, 51436], "temperature": 0.0, "avg_logprob": -0.12212664394055382, "compression_ratio": 1.910344827586207, "no_speech_prob": 0.0018358713714405894}, {"id": 1001, "seek": 428258, "start": 4305.0599999999995, "end": 4308.98, "text": " Exactly and and there's also this notion that creativity happens through knowledge", "tokens": [51488, 7587, 293, 293, 456, 311, 611, 341, 10710, 300, 12915, 2314, 807, 3601, 51684], "temperature": 0.0, "avg_logprob": -0.12212664394055382, "compression_ratio": 1.910344827586207, "no_speech_prob": 0.0018358713714405894}, {"id": 1002, "seek": 430898, "start": 4309.0599999999995, "end": 4312.419999999999, "text": " So like knowledge new knowledge doesn't come from the ether. It's kind of", "tokens": [50368, 407, 411, 3601, 777, 3601, 1177, 380, 808, 490, 264, 37096, 13, 467, 311, 733, 295, 50536], "temperature": 0.0, "avg_logprob": -0.1301444233327672, "compression_ratio": 1.8045602605863191, "no_speech_prob": 0.004528481047600508}, {"id": 1003, "seek": 430898, "start": 4313.379999999999, "end": 4316.0199999999995, "text": " There's some creative component to it, but it's it's on the", "tokens": [50584, 821, 311, 512, 5880, 6542, 281, 309, 11, 457, 309, 311, 309, 311, 322, 264, 50716], "temperature": 0.0, "avg_logprob": -0.1301444233327672, "compression_ratio": 1.8045602605863191, "no_speech_prob": 0.004528481047600508}, {"id": 1004, "seek": 430898, "start": 4316.9, "end": 4322.179999999999, "text": " The the trodden path of existing knowledge that we already have. Yeah, that wasn't a very good question", "tokens": [50760, 440, 264, 4495, 67, 1556, 3100, 295, 6741, 3601, 300, 321, 1217, 362, 13, 865, 11, 300, 2067, 380, 257, 588, 665, 1168, 51024], "temperature": 0.0, "avg_logprob": -0.1301444233327672, "compression_ratio": 1.8045602605863191, "no_speech_prob": 0.004528481047600508}, {"id": 1005, "seek": 430898, "start": 4322.5, "end": 4326.0199999999995, "text": " But you see I mean so so when we talk about imagination through like, you know", "tokens": [51040, 583, 291, 536, 286, 914, 370, 370, 562, 321, 751, 466, 12938, 807, 411, 11, 291, 458, 51216], "temperature": 0.0, "avg_logprob": -0.1301444233327672, "compression_ratio": 1.8045602605863191, "no_speech_prob": 0.004528481047600508}, {"id": 1006, "seek": 430898, "start": 4326.099999999999, "end": 4329.139999999999, "text": " Like reinforcement learning policies and and so on what we're saying is like, you know", "tokens": [51220, 1743, 29280, 2539, 7657, 293, 293, 370, 322, 437, 321, 434, 1566, 307, 411, 11, 291, 458, 51372], "temperature": 0.0, "avg_logprob": -0.1301444233327672, "compression_ratio": 1.8045602605863191, "no_speech_prob": 0.004528481047600508}, {"id": 1007, "seek": 430898, "start": 4329.219999999999, "end": 4332.82, "text": " I'm I'm imagining all of these like possible, you know worlds and so on", "tokens": [51376, 286, 478, 286, 478, 27798, 439, 295, 613, 411, 1944, 11, 291, 458, 13401, 293, 370, 322, 51556], "temperature": 0.0, "avg_logprob": -0.1301444233327672, "compression_ratio": 1.8045602605863191, "no_speech_prob": 0.004528481047600508}, {"id": 1008, "seek": 430898, "start": 4332.98, "end": 4337.139999999999, "text": " But I'm using the cognitive primitives of all of the stuff that I already know", "tokens": [51564, 583, 286, 478, 1228, 264, 15605, 2886, 38970, 295, 439, 295, 264, 1507, 300, 286, 1217, 458, 51772], "temperature": 0.0, "avg_logprob": -0.1301444233327672, "compression_ratio": 1.8045602605863191, "no_speech_prob": 0.004528481047600508}, {"id": 1009, "seek": 433714, "start": 4338.02, "end": 4340.820000000001, "text": " Yeah, I think knowledge is definitely a compounding", "tokens": [50408, 865, 11, 286, 519, 3601, 307, 2138, 257, 14154, 278, 50548], "temperature": 0.0, "avg_logprob": -0.17758177639393324, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0007553010364063084}, {"id": 1010, "seek": 433714, "start": 4341.780000000001, "end": 4343.780000000001, "text": " compounding", "tokens": [50596, 14154, 278, 50696], "temperature": 0.0, "avg_logprob": -0.17758177639393324, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0007553010364063084}, {"id": 1011, "seek": 433714, "start": 4343.780000000001, "end": 4345.62, "text": " artifact", "tokens": [50696, 34806, 50788], "temperature": 0.0, "avg_logprob": -0.17758177639393324, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0007553010364063084}, {"id": 1012, "seek": 433714, "start": 4345.62, "end": 4349.700000000001, "text": " That's basically like the culmination of everything all the experiences that we", "tokens": [50788, 663, 311, 1936, 411, 264, 28583, 399, 295, 1203, 439, 264, 5235, 300, 321, 50992], "temperature": 0.0, "avg_logprob": -0.17758177639393324, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0007553010364063084}, {"id": 1013, "seek": 433714, "start": 4350.660000000001, "end": 4354.42, "text": " That we encounter like throughout our whole life and through also like beyond, you know", "tokens": [51040, 663, 321, 8593, 411, 3710, 527, 1379, 993, 293, 807, 611, 411, 4399, 11, 291, 458, 51228], "temperature": 0.0, "avg_logprob": -0.17758177639393324, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0007553010364063084}, {"id": 1014, "seek": 433714, "start": 4354.5, "end": 4362.18, "text": " Going backwards beyond like even our individual lives into like the cultural knowledge that's shared and what's really cool about language models", "tokens": [51232, 10963, 12204, 4399, 411, 754, 527, 2609, 2909, 666, 411, 264, 6988, 3601, 300, 311, 5507, 293, 437, 311, 534, 1627, 466, 2856, 5245, 51616], "temperature": 0.0, "avg_logprob": -0.17758177639393324, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0007553010364063084}, {"id": 1015, "seek": 433714, "start": 4362.900000000001, "end": 4364.900000000001, "text": " is that they are essentially a", "tokens": [51652, 307, 300, 436, 366, 4476, 257, 51752], "temperature": 0.0, "avg_logprob": -0.17758177639393324, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0007553010364063084}, {"id": 1016, "seek": 436490, "start": 4364.9, "end": 4367.219999999999, "text": " codification of cultural knowledge and so", "tokens": [50364, 17656, 3774, 295, 6988, 3601, 293, 370, 50480], "temperature": 0.0, "avg_logprob": -0.13693031642747963, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.006901250220835209}, {"id": 1017, "seek": 436490, "start": 4367.62, "end": 4373.299999999999, "text": " Jeff Klune has this concept of AI generating AI and so he's got multiple pillars of essentially what it takes for", "tokens": [50500, 7506, 16053, 2613, 575, 341, 3410, 295, 7318, 17746, 7318, 293, 370, 415, 311, 658, 3866, 26729, 295, 4476, 437, 309, 2516, 337, 50784], "temperature": 0.0, "avg_logprob": -0.13693031642747963, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.006901250220835209}, {"id": 1018, "seek": 436490, "start": 4373.94, "end": 4375.94, "text": " You to have AI systems that generate", "tokens": [50816, 509, 281, 362, 7318, 3652, 300, 8460, 50916], "temperature": 0.0, "avg_logprob": -0.13693031642747963, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.006901250220835209}, {"id": 1019, "seek": 436490, "start": 4376.0199999999995, "end": 4381.0599999999995, "text": " General AI systems and he recently added actually like as a fundamental piece of this in in his framework", "tokens": [50920, 6996, 7318, 3652, 293, 415, 3938, 3869, 767, 411, 382, 257, 8088, 2522, 295, 341, 294, 294, 702, 8388, 51172], "temperature": 0.0, "avg_logprob": -0.13693031642747963, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.006901250220835209}, {"id": 1020, "seek": 436490, "start": 4381.379999999999, "end": 4383.54, "text": " This idea of building on top of foundation models", "tokens": [51188, 639, 1558, 295, 2390, 322, 1192, 295, 7030, 5245, 51296], "temperature": 0.0, "avg_logprob": -0.13693031642747963, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.006901250220835209}, {"id": 1021, "seek": 436490, "start": 4383.94, "end": 4387.46, "text": " And so he says he calls it like standing on the shoulders of giant foundation models", "tokens": [51316, 400, 370, 415, 1619, 415, 5498, 309, 411, 4877, 322, 264, 10245, 295, 7410, 7030, 5245, 51492], "temperature": 0.0, "avg_logprob": -0.13693031642747963, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.006901250220835209}, {"id": 1022, "seek": 436490, "start": 4388.099999999999, "end": 4392.98, "text": " Which is I think really just sort of the ml equivalent of building on top of cultural knowledge", "tokens": [51524, 3013, 307, 286, 519, 534, 445, 1333, 295, 264, 23271, 10344, 295, 2390, 322, 1192, 295, 6988, 3601, 51768], "temperature": 0.0, "avg_logprob": -0.13693031642747963, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.006901250220835209}, {"id": 1023, "seek": 439298, "start": 4393.459999999999, "end": 4395.86, "text": " There's there's a real shift recently towards talking about", "tokens": [50388, 821, 311, 456, 311, 257, 957, 5513, 3938, 3030, 1417, 466, 50508], "temperature": 0.0, "avg_logprob": -0.1688188392425252, "compression_ratio": 1.75, "no_speech_prob": 0.002319017192348838}, {"id": 1024, "seek": 439298, "start": 4396.339999999999, "end": 4398.0199999999995, "text": " um synthetic data", "tokens": [50532, 1105, 23420, 1412, 50616], "temperature": 0.0, "avg_logprob": -0.1688188392425252, "compression_ratio": 1.75, "no_speech_prob": 0.002319017192348838}, {"id": 1025, "seek": 439298, "start": 4398.0199999999995, "end": 4401.379999999999, "text": " And as we were just saying like, you know synthetic data, it doesn't come from the ether", "tokens": [50616, 400, 382, 321, 645, 445, 1566, 411, 11, 291, 458, 23420, 1412, 11, 309, 1177, 380, 808, 490, 264, 37096, 50784], "temperature": 0.0, "avg_logprob": -0.1688188392425252, "compression_ratio": 1.75, "no_speech_prob": 0.002319017192348838}, {"id": 1026, "seek": 439298, "start": 4401.7, "end": 4408.74, "text": " So we already know stuff about the world. We we build simulators and we kind of generate new", "tokens": [50800, 407, 321, 1217, 458, 1507, 466, 264, 1002, 13, 492, 321, 1322, 1034, 39265, 293, 321, 733, 295, 8460, 777, 51152], "temperature": 0.0, "avg_logprob": -0.1688188392425252, "compression_ratio": 1.75, "no_speech_prob": 0.002319017192348838}, {"id": 1027, "seek": 439298, "start": 4409.299999999999, "end": 4415.86, "text": " Information but in the neighborhood of things that we already know and then we kind of like iterate and fine-tune on the generated data", "tokens": [51180, 15357, 457, 294, 264, 7630, 295, 721, 300, 321, 1217, 458, 293, 550, 321, 733, 295, 411, 44497, 293, 2489, 12, 83, 2613, 322, 264, 10833, 1412, 51508], "temperature": 0.0, "avg_logprob": -0.1688188392425252, "compression_ratio": 1.75, "no_speech_prob": 0.002319017192348838}, {"id": 1028, "seek": 439298, "start": 4416.339999999999, "end": 4417.7, "text": " um", "tokens": [51532, 1105, 51600], "temperature": 0.0, "avg_logprob": -0.1688188392425252, "compression_ratio": 1.75, "no_speech_prob": 0.002319017192348838}, {"id": 1029, "seek": 439298, "start": 4417.7, "end": 4419.54, "text": " What what do you think about that process?", "tokens": [51600, 708, 437, 360, 291, 519, 466, 300, 1399, 30, 51692], "temperature": 0.0, "avg_logprob": -0.1688188392425252, "compression_ratio": 1.75, "no_speech_prob": 0.002319017192348838}, {"id": 1030, "seek": 441954, "start": 4419.54, "end": 4423.94, "text": " Yeah, no, I think yeah, maybe I'll bring it back to this like the plan to explore line of work. So, yeah", "tokens": [50364, 865, 11, 572, 11, 286, 519, 1338, 11, 1310, 286, 603, 1565, 309, 646, 281, 341, 411, 264, 1393, 281, 6839, 1622, 295, 589, 13, 407, 11, 1338, 50584], "temperature": 0.0, "avg_logprob": -0.09974478251898466, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0013248412869870663}, {"id": 1031, "seek": 441954, "start": 4424.9, "end": 4428.18, "text": " um, so so basically like the motivation of that kind of work is like", "tokens": [50632, 1105, 11, 370, 370, 1936, 411, 264, 12335, 295, 300, 733, 295, 589, 307, 411, 50796], "temperature": 0.0, "avg_logprob": -0.09974478251898466, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0013248412869870663}, {"id": 1032, "seek": 441954, "start": 4428.74, "end": 4431.7, "text": " Kind of saying, you know, we might have some like previous data set or something", "tokens": [50824, 9242, 295, 1566, 11, 291, 458, 11, 321, 1062, 362, 512, 411, 3894, 1412, 992, 420, 746, 50972], "temperature": 0.0, "avg_logprob": -0.09974478251898466, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0013248412869870663}, {"id": 1033, "seek": 441954, "start": 4431.7, "end": 4433.62, "text": " And we've trained our world model on that data set", "tokens": [50972, 400, 321, 600, 8895, 527, 1002, 2316, 322, 300, 1412, 992, 51068], "temperature": 0.0, "avg_logprob": -0.09974478251898466, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0013248412869870663}, {"id": 1034, "seek": 441954, "start": 4433.78, "end": 4437.7, "text": " But we really want to go out and like gather more data and then like improve the world model", "tokens": [51076, 583, 321, 534, 528, 281, 352, 484, 293, 411, 5448, 544, 1412, 293, 550, 411, 3470, 264, 1002, 2316, 51272], "temperature": 0.0, "avg_logprob": -0.09974478251898466, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0013248412869870663}, {"id": 1035, "seek": 441954, "start": 4438.58, "end": 4439.54, "text": " um", "tokens": [51316, 1105, 51364], "temperature": 0.0, "avg_logprob": -0.09974478251898466, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0013248412869870663}, {"id": 1036, "seek": 441954, "start": 4439.54, "end": 4441.54, "text": " By gathering more data", "tokens": [51364, 3146, 13519, 544, 1412, 51464], "temperature": 0.0, "avg_logprob": -0.09974478251898466, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0013248412869870663}, {"id": 1037, "seek": 441954, "start": 4441.54, "end": 4446.0199999999995, "text": " And so we can use things like intrinsic motivation to then give us like a reward signal within the world model", "tokens": [51464, 400, 370, 321, 393, 764, 721, 411, 35698, 12335, 281, 550, 976, 505, 411, 257, 7782, 6358, 1951, 264, 1002, 2316, 51688], "temperature": 0.0, "avg_logprob": -0.09974478251898466, "compression_ratio": 1.8197278911564625, "no_speech_prob": 0.0013248412869870663}, {"id": 1038, "seek": 444602, "start": 4446.34, "end": 4449.620000000001, "text": " So in the sense of something like prediction error, which mentioned earlier", "tokens": [50380, 407, 294, 264, 2020, 295, 746, 411, 17630, 6713, 11, 597, 2835, 3071, 50544], "temperature": 0.0, "avg_logprob": -0.09986448287963867, "compression_ratio": 1.908496732026144, "no_speech_prob": 0.015903117135167122}, {"id": 1039, "seek": 444602, "start": 4449.620000000001, "end": 4454.1, "text": " So now we can basically like train a policy in the world model that's now not trained for a specific task", "tokens": [50544, 407, 586, 321, 393, 1936, 411, 3847, 257, 3897, 294, 264, 1002, 2316, 300, 311, 586, 406, 8895, 337, 257, 2685, 5633, 50768], "temperature": 0.0, "avg_logprob": -0.09986448287963867, "compression_ratio": 1.908496732026144, "no_speech_prob": 0.015903117135167122}, {"id": 1040, "seek": 444602, "start": 4454.42, "end": 4457.38, "text": " But it's trained to go out and gather information in the world", "tokens": [50784, 583, 309, 311, 8895, 281, 352, 484, 293, 5448, 1589, 294, 264, 1002, 50932], "temperature": 0.0, "avg_logprob": -0.09986448287963867, "compression_ratio": 1.908496732026144, "no_speech_prob": 0.015903117135167122}, {"id": 1041, "seek": 444602, "start": 4458.1, "end": 4461.700000000001, "text": " So basically now, you know, you do this imagining in the world model to imagine ahead", "tokens": [50968, 407, 1936, 586, 11, 291, 458, 11, 291, 360, 341, 27798, 294, 264, 1002, 2316, 281, 3811, 2286, 51148], "temperature": 0.0, "avg_logprob": -0.09986448287963867, "compression_ratio": 1.908496732026144, "no_speech_prob": 0.015903117135167122}, {"id": 1042, "seek": 444602, "start": 4461.860000000001, "end": 4464.900000000001, "text": " But instead of imagining ahead, how do I do a task? Well, you're imagining ahead", "tokens": [51156, 583, 2602, 295, 27798, 2286, 11, 577, 360, 286, 360, 257, 5633, 30, 1042, 11, 291, 434, 27798, 2286, 51308], "temperature": 0.0, "avg_logprob": -0.09986448287963867, "compression_ratio": 1.908496732026144, "no_speech_prob": 0.015903117135167122}, {"id": 1043, "seek": 444602, "start": 4465.14, "end": 4471.540000000001, "text": " How do I get to states that I don't know what happens and therefore we'll learn more and that's basically like the motivation behind plan to explore", "tokens": [51320, 1012, 360, 286, 483, 281, 4368, 300, 286, 500, 380, 458, 437, 2314, 293, 4412, 321, 603, 1466, 544, 293, 300, 311, 1936, 411, 264, 12335, 2261, 1393, 281, 6839, 51640], "temperature": 0.0, "avg_logprob": -0.09986448287963867, "compression_ratio": 1.908496732026144, "no_speech_prob": 0.015903117135167122}, {"id": 1044, "seek": 444602, "start": 4472.580000000001, "end": 4474.580000000001, "text": " um, and then and our um", "tokens": [51692, 1105, 11, 293, 550, 293, 527, 1105, 51792], "temperature": 0.0, "avg_logprob": -0.09986448287963867, "compression_ratio": 1.908496732026144, "no_speech_prob": 0.015903117135167122}, {"id": 1045, "seek": 447458, "start": 4474.74, "end": 4479.78, "text": " Paper waker it's it's kind of like inspired by plan to explore as well as works on like auto-curricular", "tokens": [50372, 24990, 261, 4003, 309, 311, 309, 311, 733, 295, 411, 7547, 538, 1393, 281, 6839, 382, 731, 382, 1985, 322, 411, 8399, 12, 14112, 42750, 50624], "temperature": 0.0, "avg_logprob": -0.12074931930093204, "compression_ratio": 1.8633540372670807, "no_speech_prob": 0.0021819511894136667}, {"id": 1046, "seek": 447458, "start": 4480.58, "end": 4482.58, "text": " and so basically what we're trying to say is", "tokens": [50664, 293, 370, 1936, 437, 321, 434, 1382, 281, 584, 307, 50764], "temperature": 0.0, "avg_logprob": -0.12074931930093204, "compression_ratio": 1.8633540372670807, "no_speech_prob": 0.0021819511894136667}, {"id": 1047, "seek": 447458, "start": 4483.22, "end": 4487.0599999999995, "text": " You know plan to explore is good for for getting an agent to go out and gather data", "tokens": [50796, 509, 458, 1393, 281, 6839, 307, 665, 337, 337, 1242, 364, 9461, 281, 352, 484, 293, 5448, 1412, 50988], "temperature": 0.0, "avg_logprob": -0.12074931930093204, "compression_ratio": 1.8633540372670807, "no_speech_prob": 0.0021819511894136667}, {"id": 1048, "seek": 447458, "start": 4487.3, "end": 4492.5, "text": " Um within a single environment and you know and presumably once you've gathered enough data within a single environment", "tokens": [51000, 3301, 1951, 257, 2167, 2823, 293, 291, 458, 293, 26742, 1564, 291, 600, 13032, 1547, 1412, 1951, 257, 2167, 2823, 51260], "temperature": 0.0, "avg_logprob": -0.12074931930093204, "compression_ratio": 1.8633540372670807, "no_speech_prob": 0.0021819511894136667}, {"id": 1049, "seek": 447458, "start": 4492.5, "end": 4495.62, "text": " Then you can generate a bunch of synthetic data in that single environment", "tokens": [51260, 1396, 291, 393, 8460, 257, 3840, 295, 23420, 1412, 294, 300, 2167, 2823, 51416], "temperature": 0.0, "avg_logprob": -0.12074931930093204, "compression_ratio": 1.8633540372670807, "no_speech_prob": 0.0021819511894136667}, {"id": 1050, "seek": 447458, "start": 4496.0199999999995, "end": 4500.9, "text": " And then do what we discuss with dreamer in terms of like optimizing a policy for that very specific environment", "tokens": [51436, 400, 550, 360, 437, 321, 2248, 365, 3055, 260, 294, 2115, 295, 411, 40425, 257, 3897, 337, 300, 588, 2685, 2823, 51680], "temperature": 0.0, "avg_logprob": -0.12074931930093204, "compression_ratio": 1.8633540372670807, "no_speech_prob": 0.0021819511894136667}, {"id": 1051, "seek": 447458, "start": 4501.78, "end": 4504.0199999999995, "text": " um, but what we're really interested in is saying, you know", "tokens": [51724, 1105, 11, 457, 437, 321, 434, 534, 3102, 294, 307, 1566, 11, 291, 458, 51836], "temperature": 0.0, "avg_logprob": -0.12074931930093204, "compression_ratio": 1.8633540372670807, "no_speech_prob": 0.0021819511894136667}, {"id": 1052, "seek": 450458, "start": 4504.66, "end": 4507.62, "text": " Let's not assume that we have like one specific environment beforehand", "tokens": [50368, 961, 311, 406, 6552, 300, 321, 362, 411, 472, 2685, 2823, 22893, 50516], "temperature": 0.0, "avg_logprob": -0.08383305370807648, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0006461397861130536}, {"id": 1053, "seek": 450458, "start": 4507.86, "end": 4511.7, "text": " Let's assume that you know, there's some space of you know broad range of scenarios", "tokens": [50528, 961, 311, 6552, 300, 291, 458, 11, 456, 311, 512, 1901, 295, 291, 458, 4152, 3613, 295, 15077, 50720], "temperature": 0.0, "avg_logprob": -0.08383305370807648, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0006461397861130536}, {"id": 1054, "seek": 450458, "start": 4511.78, "end": 4513.38, "text": " Like we want a very like general agents", "tokens": [50724, 1743, 321, 528, 257, 588, 411, 2674, 12554, 50804], "temperature": 0.0, "avg_logprob": -0.08383305370807648, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0006461397861130536}, {"id": 1055, "seek": 450458, "start": 4513.38, "end": 4517.3, "text": " There might be a bunch of different environments and then within that what those different environments", "tokens": [50804, 821, 1062, 312, 257, 3840, 295, 819, 12388, 293, 550, 1951, 300, 437, 729, 819, 12388, 51000], "temperature": 0.0, "avg_logprob": -0.08383305370807648, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0006461397861130536}, {"id": 1056, "seek": 450458, "start": 4517.3, "end": 4520.26, "text": " We kind of want to be able to to handle absolutely any task", "tokens": [51000, 492, 733, 295, 528, 281, 312, 1075, 281, 281, 4813, 3122, 604, 5633, 51148], "temperature": 0.0, "avg_logprob": -0.08383305370807648, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0006461397861130536}, {"id": 1057, "seek": 450458, "start": 4521.14, "end": 4526.58, "text": " And so in the waker paper, we're basically saying like, you know, how should we gather the data within um", "tokens": [51192, 400, 370, 294, 264, 261, 4003, 3035, 11, 321, 434, 1936, 1566, 411, 11, 291, 458, 11, 577, 820, 321, 5448, 264, 1412, 1951, 1105, 51464], "temperature": 0.0, "avg_logprob": -0.08383305370807648, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0006461397861130536}, {"id": 1058, "seek": 450458, "start": 4527.3, "end": 4532.42, "text": " Within this like broad space of possible environments and tasks such that we can train a very good world model", "tokens": [51500, 15996, 341, 411, 4152, 1901, 295, 1944, 12388, 293, 9608, 1270, 300, 321, 393, 3847, 257, 588, 665, 1002, 2316, 51756], "temperature": 0.0, "avg_logprob": -0.08383305370807648, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.0006461397861130536}, {"id": 1059, "seek": 453242, "start": 4532.82, "end": 4536.9, "text": " And then once we have that world model, that's kind of like capable across environments and tasks", "tokens": [50384, 400, 550, 1564, 321, 362, 300, 1002, 2316, 11, 300, 311, 733, 295, 411, 8189, 2108, 12388, 293, 9608, 50588], "temperature": 0.0, "avg_logprob": -0.10097312132517497, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.001133427955210209}, {"id": 1060, "seek": 453242, "start": 4537.22, "end": 4541.3, "text": " You know the assumption is that we can then use that to generate good synthetic data, which we can then", "tokens": [50604, 509, 458, 264, 15302, 307, 300, 321, 393, 550, 764, 300, 281, 8460, 665, 23420, 1412, 11, 597, 321, 393, 550, 50808], "temperature": 0.0, "avg_logprob": -0.10097312132517497, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.001133427955210209}, {"id": 1061, "seek": 453242, "start": 4541.7, "end": 4543.7, "text": " Um use to optimize behavior", "tokens": [50828, 3301, 764, 281, 19719, 5223, 50928], "temperature": 0.0, "avg_logprob": -0.10097312132517497, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.001133427955210209}, {"id": 1062, "seek": 453242, "start": 4544.58, "end": 4547.62, "text": " And so maybe to talk a little bit about like how we formalize this problem", "tokens": [50972, 400, 370, 1310, 281, 751, 257, 707, 857, 466, 411, 577, 321, 9860, 1125, 341, 1154, 51124], "temperature": 0.0, "avg_logprob": -0.10097312132517497, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.001133427955210209}, {"id": 1063, "seek": 453242, "start": 4548.02, "end": 4551.78, "text": " Um, so, you know, we mentioned earlier this idea of like the simulation lemma", "tokens": [51144, 3301, 11, 370, 11, 291, 458, 11, 321, 2835, 3071, 341, 1558, 295, 411, 264, 16575, 7495, 1696, 51332], "temperature": 0.0, "avg_logprob": -0.10097312132517497, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.001133427955210209}, {"id": 1064, "seek": 453242, "start": 4551.86, "end": 4556.26, "text": " So we basically say that or an existing work that says like in a single environment", "tokens": [51336, 407, 321, 1936, 584, 300, 420, 364, 6741, 589, 300, 1619, 411, 294, 257, 2167, 2823, 51556], "temperature": 0.0, "avg_logprob": -0.10097312132517497, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.001133427955210209}, {"id": 1065, "seek": 453242, "start": 4556.58, "end": 4559.38, "text": " We can bound the gap between the optimal policy", "tokens": [51572, 492, 393, 5472, 264, 7417, 1296, 264, 16252, 3897, 51712], "temperature": 0.0, "avg_logprob": -0.10097312132517497, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.001133427955210209}, {"id": 1066, "seek": 455938, "start": 4559.86, "end": 4564.58, "text": " That's trained in the world model so trained in the synthetic data to the to the truly optimal policy", "tokens": [50388, 663, 311, 8895, 294, 264, 1002, 2316, 370, 8895, 294, 264, 23420, 1412, 281, 264, 281, 264, 4908, 16252, 3897, 50624], "temperature": 0.0, "avg_logprob": -0.08767222042741446, "compression_ratio": 1.964856230031949, "no_speech_prob": 0.0015977666480466723}, {"id": 1067, "seek": 455938, "start": 4564.9800000000005, "end": 4568.82, "text": " By the error in the world model and the distribution of states generated by that policy", "tokens": [50644, 3146, 264, 6713, 294, 264, 1002, 2316, 293, 264, 7316, 295, 4368, 10833, 538, 300, 3897, 50836], "temperature": 0.0, "avg_logprob": -0.08767222042741446, "compression_ratio": 1.964856230031949, "no_speech_prob": 0.0015977666480466723}, {"id": 1068, "seek": 455938, "start": 4569.46, "end": 4574.5, "text": " So it's kind of intuitive like the world model should have, you know, low error and then we will get a good policy out of it", "tokens": [50868, 407, 309, 311, 733, 295, 21769, 411, 264, 1002, 2316, 820, 362, 11, 291, 458, 11, 2295, 6713, 293, 550, 321, 486, 483, 257, 665, 3897, 484, 295, 309, 51120], "temperature": 0.0, "avg_logprob": -0.08767222042741446, "compression_ratio": 1.964856230031949, "no_speech_prob": 0.0015977666480466723}, {"id": 1069, "seek": 455938, "start": 4575.22, "end": 4579.7, "text": " But then what we're trying to say is like now, let's assume we don't know what the environment is beforehand", "tokens": [51156, 583, 550, 437, 321, 434, 1382, 281, 584, 307, 411, 586, 11, 718, 311, 6552, 321, 500, 380, 458, 437, 264, 2823, 307, 22893, 51380], "temperature": 0.0, "avg_logprob": -0.08767222042741446, "compression_ratio": 1.964856230031949, "no_speech_prob": 0.0015977666480466723}, {"id": 1070, "seek": 455938, "start": 4579.7, "end": 4582.1, "text": " And we also don't know what the task is beforehand", "tokens": [51380, 400, 321, 611, 500, 380, 458, 437, 264, 5633, 307, 22893, 51500], "temperature": 0.0, "avg_logprob": -0.08767222042741446, "compression_ratio": 1.964856230031949, "no_speech_prob": 0.0015977666480466723}, {"id": 1071, "seek": 455938, "start": 4582.34, "end": 4585.9400000000005, "text": " So how do we get like a good world model that can handle like all of those situations?", "tokens": [51512, 407, 577, 360, 321, 483, 411, 257, 665, 1002, 2316, 300, 393, 4813, 411, 439, 295, 729, 6851, 30, 51692], "temperature": 0.0, "avg_logprob": -0.08767222042741446, "compression_ratio": 1.964856230031949, "no_speech_prob": 0.0015977666480466723}, {"id": 1072, "seek": 455938, "start": 4586.66, "end": 4588.900000000001, "text": " When we later want to go ahead and optimize some task", "tokens": [51728, 1133, 321, 1780, 528, 281, 352, 2286, 293, 19719, 512, 5633, 51840], "temperature": 0.0, "avg_logprob": -0.08767222042741446, "compression_ratio": 1.964856230031949, "no_speech_prob": 0.0015977666480466723}, {"id": 1073, "seek": 458938, "start": 4589.62, "end": 4590.74, "text": " um", "tokens": [50376, 1105, 50432], "temperature": 0.0, "avg_logprob": -0.12045678445848368, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.00020986248273402452}, {"id": 1074, "seek": 458938, "start": 4590.74, "end": 4592.74, "text": " And so the way that we do this is we basically yeah", "tokens": [50432, 400, 370, 264, 636, 300, 321, 360, 341, 307, 321, 1936, 1338, 50532], "temperature": 0.0, "avg_logprob": -0.12045678445848368, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.00020986248273402452}, {"id": 1075, "seek": 458938, "start": 4593.22, "end": 4599.54, "text": " We then use this notion of mini max regret to say that the policy should have like low maximum regret across this hot entire space of environments", "tokens": [50556, 492, 550, 764, 341, 10710, 295, 8382, 11469, 10879, 281, 584, 300, 264, 3897, 820, 362, 411, 2295, 6674, 10879, 2108, 341, 2368, 2302, 1901, 295, 12388, 50872], "temperature": 0.0, "avg_logprob": -0.12045678445848368, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.00020986248273402452}, {"id": 1076, "seek": 458938, "start": 4600.1, "end": 4602.58, "text": " And then using the simulation lemma we can basically say now", "tokens": [50900, 400, 550, 1228, 264, 16575, 7495, 1696, 321, 393, 1936, 584, 586, 51024], "temperature": 0.0, "avg_logprob": -0.12045678445848368, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.00020986248273402452}, {"id": 1077, "seek": 458938, "start": 4603.22, "end": 4607.3, "text": " Now the um the world model has to have low error across all environments", "tokens": [51056, 823, 264, 1105, 264, 1002, 2316, 575, 281, 362, 2295, 6713, 2108, 439, 12388, 51260], "temperature": 0.0, "avg_logprob": -0.12045678445848368, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.00020986248273402452}, {"id": 1078, "seek": 458938, "start": 4607.9400000000005, "end": 4613.22, "text": " Under the distribution of states generated by the optimal policy for any future task", "tokens": [51292, 6974, 264, 7316, 295, 4368, 10833, 538, 264, 16252, 3897, 337, 604, 2027, 5633, 51556], "temperature": 0.0, "avg_logprob": -0.12045678445848368, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.00020986248273402452}, {"id": 1079, "seek": 458938, "start": 4613.86, "end": 4617.7, "text": " Um, so we're going to say like yeah, the world model has to be good for any environment and", "tokens": [51588, 3301, 11, 370, 321, 434, 516, 281, 584, 411, 1338, 11, 264, 1002, 2316, 575, 281, 312, 665, 337, 604, 2823, 293, 51780], "temperature": 0.0, "avg_logprob": -0.12045678445848368, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.00020986248273402452}, {"id": 1080, "seek": 461770, "start": 4618.26, "end": 4622.099999999999, "text": " Under, you know in any area that the policy might go to that's relevant to the future tasks", "tokens": [50392, 6974, 11, 291, 458, 294, 604, 1859, 300, 264, 3897, 1062, 352, 281, 300, 311, 7340, 281, 264, 2027, 9608, 50584], "temperature": 0.0, "avg_logprob": -0.07870082259178161, "compression_ratio": 2.0129032258064514, "no_speech_prob": 0.0018100377637892962}, {"id": 1081, "seek": 461770, "start": 4623.139999999999, "end": 4626.74, "text": " And then what we kind of say in the paper is, you know, if we want a truly general agent", "tokens": [50636, 400, 550, 437, 321, 733, 295, 584, 294, 264, 3035, 307, 11, 291, 458, 11, 498, 321, 528, 257, 4908, 2674, 9461, 50816], "temperature": 0.0, "avg_logprob": -0.07870082259178161, "compression_ratio": 2.0129032258064514, "no_speech_prob": 0.0018100377637892962}, {"id": 1082, "seek": 461770, "start": 4626.98, "end": 4629.3, "text": " We're not going to know what the distribution of tasks is beforehand", "tokens": [50828, 492, 434, 406, 516, 281, 458, 437, 264, 7316, 295, 9608, 307, 22893, 50944], "temperature": 0.0, "avg_logprob": -0.07870082259178161, "compression_ratio": 2.0129032258064514, "no_speech_prob": 0.0018100377637892962}, {"id": 1083, "seek": 461770, "start": 4629.3, "end": 4632.9, "text": " So we don't know we don't know what the reward function is. We don't have a set of reward functions", "tokens": [50944, 407, 321, 500, 380, 458, 321, 500, 380, 458, 437, 264, 7782, 2445, 307, 13, 492, 500, 380, 362, 257, 992, 295, 7782, 6828, 51124], "temperature": 0.0, "avg_logprob": -0.07870082259178161, "compression_ratio": 2.0129032258064514, "no_speech_prob": 0.0018100377637892962}, {"id": 1084, "seek": 461770, "start": 4633.62, "end": 4637.38, "text": " Um, you know, we're just going to kind of assume the agent has to do anything later down the line", "tokens": [51160, 3301, 11, 291, 458, 11, 321, 434, 445, 516, 281, 733, 295, 6552, 264, 9461, 575, 281, 360, 1340, 1780, 760, 264, 1622, 51348], "temperature": 0.0, "avg_logprob": -0.07870082259178161, "compression_ratio": 2.0129032258064514, "no_speech_prob": 0.0018100377637892962}, {"id": 1085, "seek": 461770, "start": 4637.38, "end": 4640.9, "text": " And this is kind of like related to this idea of like open-endedness that we've talked a lot about", "tokens": [51348, 400, 341, 307, 733, 295, 411, 4077, 281, 341, 1558, 295, 411, 1269, 12, 3502, 1287, 300, 321, 600, 2825, 257, 688, 466, 51524], "temperature": 0.0, "avg_logprob": -0.07870082259178161, "compression_ratio": 2.0129032258064514, "no_speech_prob": 0.0018100377637892962}, {"id": 1086, "seek": 461770, "start": 4641.78, "end": 4645.0599999999995, "text": " And so if we don't know what the task is going to be like later down the line", "tokens": [51568, 400, 370, 498, 321, 500, 380, 458, 437, 264, 5633, 307, 516, 281, 312, 411, 1780, 760, 264, 1622, 51732], "temperature": 0.0, "avg_logprob": -0.07870082259178161, "compression_ratio": 2.0129032258064514, "no_speech_prob": 0.0018100377637892962}, {"id": 1087, "seek": 464506, "start": 4646.02, "end": 4650.740000000001, "text": " Then the best assumption we can do is say that, you know, it could be any reward function later down the line", "tokens": [50412, 1396, 264, 1151, 15302, 321, 393, 360, 307, 584, 300, 11, 291, 458, 11, 309, 727, 312, 604, 7782, 2445, 1780, 760, 264, 1622, 50648], "temperature": 0.0, "avg_logprob": -0.08328834109836154, "compression_ratio": 1.97, "no_speech_prob": 0.006289131473749876}, {"id": 1088, "seek": 464506, "start": 4651.38, "end": 4655.54, "text": " Which is maybe not the best assumption because as we talked a bit earlier if you're just kind of", "tokens": [50680, 3013, 307, 1310, 406, 264, 1151, 15302, 570, 382, 321, 2825, 257, 857, 3071, 498, 291, 434, 445, 733, 295, 50888], "temperature": 0.0, "avg_logprob": -0.08328834109836154, "compression_ratio": 1.97, "no_speech_prob": 0.006289131473749876}, {"id": 1089, "seek": 464506, "start": 4656.18, "end": 4659.3, "text": " You know, we talked about a bit about intrinsic motivation and interestingness", "tokens": [50920, 509, 458, 11, 321, 2825, 466, 257, 857, 466, 35698, 12335, 293, 1880, 1287, 51076], "temperature": 0.0, "avg_logprob": -0.08328834109836154, "compression_ratio": 1.97, "no_speech_prob": 0.006289131473749876}, {"id": 1090, "seek": 464506, "start": 4659.620000000001, "end": 4662.660000000001, "text": " And if you kind of assume the task can be absolutely anything later down the line", "tokens": [51092, 400, 498, 291, 733, 295, 6552, 264, 5633, 393, 312, 3122, 1340, 1780, 760, 264, 1622, 51244], "temperature": 0.0, "avg_logprob": -0.08328834109836154, "compression_ratio": 1.97, "no_speech_prob": 0.006289131473749876}, {"id": 1091, "seek": 464506, "start": 4663.06, "end": 4667.3, "text": " You're kind of assuming that, you know, the agent might want to do something completely ridiculous later like it", "tokens": [51264, 509, 434, 733, 295, 11926, 300, 11, 291, 458, 11, 264, 9461, 1062, 528, 281, 360, 746, 2584, 11083, 1780, 411, 309, 51476], "temperature": 0.0, "avg_logprob": -0.08328834109836154, "compression_ratio": 1.97, "no_speech_prob": 0.006289131473749876}, {"id": 1092, "seek": 464506, "start": 4668.1, "end": 4672.34, "text": " If you do this in robotics, that might mean the task is just to do like backflips later or something like that", "tokens": [51516, 759, 291, 360, 341, 294, 34145, 11, 300, 1062, 914, 264, 5633, 307, 445, 281, 360, 411, 646, 3423, 2600, 1780, 420, 746, 411, 300, 51728], "temperature": 0.0, "avg_logprob": -0.08328834109836154, "compression_ratio": 1.97, "no_speech_prob": 0.006289131473749876}, {"id": 1093, "seek": 467234, "start": 4672.34, "end": 4676.34, "text": " But you have no interest in doing that. So it's it's not clear if that's really a good assumption about", "tokens": [50364, 583, 291, 362, 572, 1179, 294, 884, 300, 13, 407, 309, 311, 309, 311, 406, 1850, 498, 300, 311, 534, 257, 665, 15302, 466, 50564], "temperature": 0.0, "avg_logprob": -0.0957596562489742, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0001739957951940596}, {"id": 1094, "seek": 467234, "start": 4676.74, "end": 4680.18, "text": " How we should think about what tasks might be interesting later, but that's the assumption we make", "tokens": [50584, 1012, 321, 820, 519, 466, 437, 9608, 1062, 312, 1880, 1780, 11, 457, 300, 311, 264, 15302, 321, 652, 50756], "temperature": 0.0, "avg_logprob": -0.0957596562489742, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0001739957951940596}, {"id": 1095, "seek": 467234, "start": 4680.18, "end": 4682.58, "text": " So we assume the task can be absolutely anything later down the line", "tokens": [50756, 407, 321, 6552, 264, 5633, 393, 312, 3122, 1340, 1780, 760, 264, 1622, 50876], "temperature": 0.0, "avg_logprob": -0.0957596562489742, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0001739957951940596}, {"id": 1096, "seek": 467234, "start": 4684.34, "end": 4687.22, "text": " So so now we have to get a to the point where we have the world model", "tokens": [50964, 407, 370, 586, 321, 362, 281, 483, 257, 281, 264, 935, 689, 321, 362, 264, 1002, 2316, 51108], "temperature": 0.0, "avg_logprob": -0.0957596562489742, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0001739957951940596}, {"id": 1097, "seek": 467234, "start": 4687.7, "end": 4693.14, "text": " Which is good for any environment and under the distribution of states generated for any task or any optimal reward function", "tokens": [51132, 3013, 307, 665, 337, 604, 2823, 293, 833, 264, 7316, 295, 4368, 10833, 337, 604, 5633, 420, 604, 16252, 7782, 2445, 51404], "temperature": 0.0, "avg_logprob": -0.0957596562489742, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0001739957951940596}, {"id": 1098, "seek": 467234, "start": 4695.3, "end": 4698.34, "text": " And to do this we basically like leverage two different techniques", "tokens": [51512, 400, 281, 360, 341, 321, 1936, 411, 13982, 732, 819, 7512, 51664], "temperature": 0.0, "avg_logprob": -0.0957596562489742, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0001739957951940596}, {"id": 1099, "seek": 469834, "start": 4699.3, "end": 4701.3, "text": " So to generate this state um", "tokens": [50412, 407, 281, 8460, 341, 1785, 1105, 50512], "temperature": 0.0, "avg_logprob": -0.10292584073227064, "compression_ratio": 2.142857142857143, "no_speech_prob": 0.0007096317131072283}, {"id": 1100, "seek": 469834, "start": 4701.62, "end": 4705.9400000000005, "text": " So to handle the aspect that we don't know what the task is later down the line. We assumed that um", "tokens": [50528, 407, 281, 4813, 264, 4171, 300, 321, 500, 380, 458, 437, 264, 5633, 307, 1780, 760, 264, 1622, 13, 492, 15895, 300, 1105, 50744], "temperature": 0.0, "avg_logprob": -0.10292584073227064, "compression_ratio": 2.142857142857143, "no_speech_prob": 0.0007096317131072283}, {"id": 1101, "seek": 469834, "start": 4706.82, "end": 4711.9400000000005, "text": " We have an intrinsically motivated policy that's basically seeking out the maximum uncertainty in any single environment", "tokens": [50788, 492, 362, 364, 28621, 984, 14515, 3897, 300, 311, 1936, 11670, 484, 264, 6674, 15697, 294, 604, 2167, 2823, 51044], "temperature": 0.0, "avg_logprob": -0.10292584073227064, "compression_ratio": 2.142857142857143, "no_speech_prob": 0.0007096317131072283}, {"id": 1102, "seek": 469834, "start": 4712.5, "end": 4714.5, "text": " And so basically if if this um", "tokens": [51072, 400, 370, 1936, 498, 498, 341, 1105, 51172], "temperature": 0.0, "avg_logprob": -0.10292584073227064, "compression_ratio": 2.142857142857143, "no_speech_prob": 0.0007096317131072283}, {"id": 1103, "seek": 469834, "start": 4714.9800000000005, "end": 4718.900000000001, "text": " If this intrinsically motivated policy is seeking out the maximum uncertainty in every environment", "tokens": [51196, 759, 341, 28621, 984, 14515, 3897, 307, 11670, 484, 264, 6674, 15697, 294, 633, 2823, 51392], "temperature": 0.0, "avg_logprob": -0.10292584073227064, "compression_ratio": 2.142857142857143, "no_speech_prob": 0.0007096317131072283}, {"id": 1104, "seek": 469834, "start": 4719.38, "end": 4726.02, "text": " Um, it's kind of like estimating for us what the maximum uncertainty is in every environment because it's like actively finding uncertainty in every environment", "tokens": [51416, 3301, 11, 309, 311, 733, 295, 411, 8017, 990, 337, 505, 437, 264, 6674, 15697, 307, 294, 633, 2823, 570, 309, 311, 411, 13022, 5006, 15697, 294, 633, 2823, 51748], "temperature": 0.0, "avg_logprob": -0.10292584073227064, "compression_ratio": 2.142857142857143, "no_speech_prob": 0.0007096317131072283}, {"id": 1105, "seek": 472602, "start": 4726.820000000001, "end": 4730.18, "text": " So now we have a policy that's finding like the maximum uncertainty in every environment", "tokens": [50404, 407, 586, 321, 362, 257, 3897, 300, 311, 5006, 411, 264, 6674, 15697, 294, 633, 2823, 50572], "temperature": 0.0, "avg_logprob": -0.12369338158638246, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.00047275921679101884}, {"id": 1106, "seek": 472602, "start": 4730.820000000001, "end": 4734.42, "text": " And then if we want to optimize this like mini max criterion across environments", "tokens": [50604, 400, 550, 498, 321, 528, 281, 19719, 341, 411, 8382, 11469, 46691, 2108, 12388, 50784], "temperature": 0.0, "avg_logprob": -0.12369338158638246, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.00047275921679101884}, {"id": 1107, "seek": 472602, "start": 4734.820000000001, "end": 4738.26, "text": " We kind of need the maximum uncertainty to be low across all environments. So", "tokens": [50804, 492, 733, 295, 643, 264, 6674, 15697, 281, 312, 2295, 2108, 439, 12388, 13, 407, 50976], "temperature": 0.0, "avg_logprob": -0.12369338158638246, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.00047275921679101884}, {"id": 1108, "seek": 472602, "start": 4738.9800000000005, "end": 4744.34, "text": " So we kind of have to have like um, you know, this policy isn't able to find like lots of big errors across all different environments", "tokens": [51012, 407, 321, 733, 295, 362, 281, 362, 411, 1105, 11, 291, 458, 11, 341, 3897, 1943, 380, 1075, 281, 915, 411, 3195, 295, 955, 13603, 2108, 439, 819, 12388, 51280], "temperature": 0.0, "avg_logprob": -0.12369338158638246, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.00047275921679101884}, {"id": 1109, "seek": 472602, "start": 4745.3, "end": 4746.26, "text": " um", "tokens": [51328, 1105, 51376], "temperature": 0.0, "avg_logprob": -0.12369338158638246, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.00047275921679101884}, {"id": 1110, "seek": 472602, "start": 4746.26, "end": 4747.22, "text": " And so", "tokens": [51376, 400, 370, 51424], "temperature": 0.0, "avg_logprob": -0.12369338158638246, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.00047275921679101884}, {"id": 1111, "seek": 472602, "start": 4747.22, "end": 4751.540000000001, "text": " Basically, you know, what we could think like what what happened in practice is, you know", "tokens": [51424, 8537, 11, 291, 458, 11, 437, 321, 727, 519, 411, 437, 437, 2011, 294, 3124, 307, 11, 291, 458, 51640], "temperature": 0.0, "avg_logprob": -0.12369338158638246, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.00047275921679101884}, {"id": 1112, "seek": 472602, "start": 4751.540000000001, "end": 4753.14, "text": " You can imagine there are a bunch of different environments", "tokens": [51640, 509, 393, 3811, 456, 366, 257, 3840, 295, 819, 12388, 51720], "temperature": 0.0, "avg_logprob": -0.12369338158638246, "compression_ratio": 1.9151943462897527, "no_speech_prob": 0.00047275921679101884}, {"id": 1113, "seek": 475314, "start": 4753.14, "end": 4756.02, "text": " Some which are like a low complexity and some of which are high complexity", "tokens": [50364, 2188, 597, 366, 411, 257, 2295, 14024, 293, 512, 295, 597, 366, 1090, 14024, 50508], "temperature": 0.0, "avg_logprob": -0.09433820170740928, "compression_ratio": 1.9591836734693877, "no_speech_prob": 0.0008558083791285753}, {"id": 1114, "seek": 475314, "start": 4756.900000000001, "end": 4760.42, "text": " And if we just kind of naively sample from those two different environments data, you know", "tokens": [50552, 400, 498, 321, 445, 733, 295, 1667, 3413, 6889, 490, 729, 732, 819, 12388, 1412, 11, 291, 458, 50728], "temperature": 0.0, "avg_logprob": -0.09433820170740928, "compression_ratio": 1.9591836734693877, "no_speech_prob": 0.0008558083791285753}, {"id": 1115, "seek": 475314, "start": 4760.5, "end": 4763.3, "text": " Our world model is going to very quickly get good at the low complexity environment", "tokens": [50732, 2621, 1002, 2316, 307, 516, 281, 588, 2661, 483, 665, 412, 264, 2295, 14024, 2823, 50872], "temperature": 0.0, "avg_logprob": -0.09433820170740928, "compression_ratio": 1.9591836734693877, "no_speech_prob": 0.0008558083791285753}, {"id": 1116, "seek": 475314, "start": 4763.9400000000005, "end": 4769.06, "text": " And then it's going to leave a lot more data from that high complexity environment to eventually get the errors low in the high complexity environment", "tokens": [50904, 400, 550, 309, 311, 516, 281, 1856, 257, 688, 544, 1412, 490, 300, 1090, 14024, 2823, 281, 4728, 483, 264, 13603, 2295, 294, 264, 1090, 14024, 2823, 51160], "temperature": 0.0, "avg_logprob": -0.09433820170740928, "compression_ratio": 1.9591836734693877, "no_speech_prob": 0.0008558083791285753}, {"id": 1117, "seek": 475314, "start": 4769.9400000000005, "end": 4774.660000000001, "text": " So to bring it back to the title of the paper, which is weighted acquisition of knowledge across environments for a bussiness", "tokens": [51204, 407, 281, 1565, 309, 646, 281, 264, 4876, 295, 264, 3035, 11, 597, 307, 32807, 21668, 295, 3601, 2108, 12388, 337, 257, 1255, 82, 1324, 51440], "temperature": 0.0, "avg_logprob": -0.09433820170740928, "compression_ratio": 1.9591836734693877, "no_speech_prob": 0.0008558083791285753}, {"id": 1118, "seek": 475314, "start": 4774.9800000000005, "end": 4777.14, "text": " So the idea here is that we're basically going to", "tokens": [51456, 407, 264, 1558, 510, 307, 300, 321, 434, 1936, 516, 281, 51564], "temperature": 0.0, "avg_logprob": -0.09433820170740928, "compression_ratio": 1.9591836734693877, "no_speech_prob": 0.0008558083791285753}, {"id": 1119, "seek": 477714, "start": 4778.1, "end": 4784.18, "text": " Change how we sample that distribution of data across environments to make sure that maximum uncertainty stays low across environments", "tokens": [50412, 15060, 577, 321, 6889, 300, 7316, 295, 1412, 2108, 12388, 281, 652, 988, 300, 6674, 15697, 10834, 2295, 2108, 12388, 50716], "temperature": 0.0, "avg_logprob": -0.08059793710708618, "compression_ratio": 1.9895833333333333, "no_speech_prob": 0.031138917431235313}, {"id": 1120, "seek": 477714, "start": 4784.58, "end": 4789.860000000001, "text": " So what this ends up looking like is, you know, we're going to sample less data from the environment that has lower complexity", "tokens": [50736, 407, 437, 341, 5314, 493, 1237, 411, 307, 11, 291, 458, 11, 321, 434, 516, 281, 6889, 1570, 1412, 490, 264, 2823, 300, 575, 3126, 14024, 51000], "temperature": 0.0, "avg_logprob": -0.08059793710708618, "compression_ratio": 1.9895833333333333, "no_speech_prob": 0.031138917431235313}, {"id": 1121, "seek": 477714, "start": 4790.5, "end": 4794.34, "text": " And then we're going to actively sample more data from the environment that has higher complexity", "tokens": [51032, 400, 550, 321, 434, 516, 281, 13022, 6889, 544, 1412, 490, 264, 2823, 300, 575, 2946, 14024, 51224], "temperature": 0.0, "avg_logprob": -0.08059793710708618, "compression_ratio": 1.9895833333333333, "no_speech_prob": 0.031138917431235313}, {"id": 1122, "seek": 477714, "start": 4794.660000000001, "end": 4797.62, "text": " Such that we we bring those errors down on the higher complexity environments", "tokens": [51240, 9653, 300, 321, 321, 1565, 729, 13603, 760, 322, 264, 2946, 14024, 12388, 51388], "temperature": 0.0, "avg_logprob": -0.08059793710708618, "compression_ratio": 1.9895833333333333, "no_speech_prob": 0.031138917431235313}, {"id": 1123, "seek": 477714, "start": 4798.18, "end": 4803.54, "text": " And I guess it's a little bit different to existing works on curricula because normally in curricula like automatic curriculum learning", "tokens": [51416, 400, 286, 2041, 309, 311, 257, 707, 857, 819, 281, 6741, 1985, 322, 13179, 3780, 570, 5646, 294, 13179, 3780, 411, 12509, 14302, 2539, 51684], "temperature": 0.0, "avg_logprob": -0.08059793710708618, "compression_ratio": 1.9895833333333333, "no_speech_prob": 0.031138917431235313}, {"id": 1124, "seek": 480354, "start": 4804.18, "end": 4806.1, "text": " You kind of assume that you have some reward function", "tokens": [50396, 509, 733, 295, 6552, 300, 291, 362, 512, 7782, 2445, 50492], "temperature": 0.0, "avg_logprob": -0.0688483421116659, "compression_ratio": 2.0664451827242525, "no_speech_prob": 0.01281712856143713}, {"id": 1125, "seek": 480354, "start": 4806.1, "end": 4812.74, "text": " Which is telling you how well the policy is doing in each environment and use use that specific like metric of how well the policy is doing", "tokens": [50492, 3013, 307, 3585, 291, 577, 731, 264, 3897, 307, 884, 294, 1184, 2823, 293, 764, 764, 300, 2685, 411, 20678, 295, 577, 731, 264, 3897, 307, 884, 50824], "temperature": 0.0, "avg_logprob": -0.0688483421116659, "compression_ratio": 2.0664451827242525, "no_speech_prob": 0.01281712856143713}, {"id": 1126, "seek": 480354, "start": 4813.54, "end": 4817.06, "text": " To determine, um, you know, where the policy has more potential to learn", "tokens": [50864, 1407, 6997, 11, 1105, 11, 291, 458, 11, 689, 264, 3897, 575, 544, 3995, 281, 1466, 51040], "temperature": 0.0, "avg_logprob": -0.0688483421116659, "compression_ratio": 2.0664451827242525, "no_speech_prob": 0.01281712856143713}, {"id": 1127, "seek": 480354, "start": 4817.38, "end": 4820.58, "text": " But because we're making this assumption that, you know, we don't know what the reward function is", "tokens": [51056, 583, 570, 321, 434, 1455, 341, 15302, 300, 11, 291, 458, 11, 321, 500, 380, 458, 437, 264, 7782, 2445, 307, 51216], "temperature": 0.0, "avg_logprob": -0.0688483421116659, "compression_ratio": 2.0664451827242525, "no_speech_prob": 0.01281712856143713}, {"id": 1128, "seek": 480354, "start": 4820.74, "end": 4823.86, "text": " We're trying to get a general agent that can kind of do any task any reward function", "tokens": [51224, 492, 434, 1382, 281, 483, 257, 2674, 9461, 300, 393, 733, 295, 360, 604, 5633, 604, 7782, 2445, 51380], "temperature": 0.0, "avg_logprob": -0.0688483421116659, "compression_ratio": 2.0664451827242525, "no_speech_prob": 0.01281712856143713}, {"id": 1129, "seek": 480354, "start": 4824.74, "end": 4827.7, "text": " Um, we don't assume that we know that reward function beforehand", "tokens": [51424, 3301, 11, 321, 500, 380, 6552, 300, 321, 458, 300, 7782, 2445, 22893, 51572], "temperature": 0.0, "avg_logprob": -0.0688483421116659, "compression_ratio": 2.0664451827242525, "no_speech_prob": 0.01281712856143713}, {"id": 1130, "seek": 480354, "start": 4827.7, "end": 4831.62, "text": " So we can't use reward as a metric of saying like I need more data from here or I need more data from here", "tokens": [51572, 407, 321, 393, 380, 764, 7782, 382, 257, 20678, 295, 1566, 411, 286, 643, 544, 1412, 490, 510, 420, 286, 643, 544, 1412, 490, 510, 51768], "temperature": 0.0, "avg_logprob": -0.0688483421116659, "compression_ratio": 2.0664451827242525, "no_speech_prob": 0.01281712856143713}, {"id": 1131, "seek": 483162, "start": 4832.42, "end": 4835.54, "text": " But then kind of the main argument of the paper is showing that, you know", "tokens": [50404, 583, 550, 733, 295, 264, 2135, 6770, 295, 264, 3035, 307, 4099, 300, 11, 291, 458, 50560], "temperature": 0.0, "avg_logprob": -0.08328863400131908, "compression_ratio": 1.8395061728395061, "no_speech_prob": 0.00030531350057572126}, {"id": 1132, "seek": 483162, "start": 4835.62, "end": 4838.82, "text": " If we just think about this in terms of prediction error in the world model", "tokens": [50564, 759, 321, 445, 519, 466, 341, 294, 2115, 295, 17630, 6713, 294, 264, 1002, 2316, 50724], "temperature": 0.0, "avg_logprob": -0.08328863400131908, "compression_ratio": 1.8395061728395061, "no_speech_prob": 0.00030531350057572126}, {"id": 1133, "seek": 483162, "start": 4839.54, "end": 4843.14, "text": " Like we can actually use that as like an intrinsic motivation signal to say, you know", "tokens": [50760, 1743, 321, 393, 767, 764, 300, 382, 411, 364, 35698, 12335, 6358, 281, 584, 11, 291, 458, 50940], "temperature": 0.0, "avg_logprob": -0.08328863400131908, "compression_ratio": 1.8395061728395061, "no_speech_prob": 0.00030531350057572126}, {"id": 1134, "seek": 483162, "start": 4843.22, "end": 4848.18, "text": " Does the agent need to gather more data from this environment or from this environment without access to reward function", "tokens": [50944, 4402, 264, 9461, 643, 281, 5448, 544, 1412, 490, 341, 2823, 420, 490, 341, 2823, 1553, 2105, 281, 7782, 2445, 51192], "temperature": 0.0, "avg_logprob": -0.08328863400131908, "compression_ratio": 1.8395061728395061, "no_speech_prob": 0.00030531350057572126}, {"id": 1135, "seek": 483162, "start": 4848.74, "end": 4850.74, "text": " and so we could kind of think of um", "tokens": [51220, 293, 370, 321, 727, 733, 295, 519, 295, 1105, 51320], "temperature": 0.0, "avg_logprob": -0.08328863400131908, "compression_ratio": 1.8395061728395061, "no_speech_prob": 0.00030531350057572126}, {"id": 1136, "seek": 483162, "start": 4851.0599999999995, "end": 4857.3, "text": " This work as kind of a more general approach to automatic curriculum learning in the sense of like we're not assuming that you have a reward function beforehand", "tokens": [51336, 639, 589, 382, 733, 295, 257, 544, 2674, 3109, 281, 12509, 14302, 2539, 294, 264, 2020, 295, 411, 321, 434, 406, 11926, 300, 291, 362, 257, 7782, 2445, 22893, 51648], "temperature": 0.0, "avg_logprob": -0.08328863400131908, "compression_ratio": 1.8395061728395061, "no_speech_prob": 0.00030531350057572126}, {"id": 1137, "seek": 483162, "start": 4857.3, "end": 4859.3, "text": " We're kind of agnostic to what the task is", "tokens": [51648, 492, 434, 733, 295, 623, 77, 19634, 281, 437, 264, 5633, 307, 51748], "temperature": 0.0, "avg_logprob": -0.08328863400131908, "compression_ratio": 1.8395061728395061, "no_speech_prob": 0.00030531350057572126}, {"id": 1138, "seek": 485930, "start": 4860.1, "end": 4864.1, "text": " And because and to kind of distill that knowledge that's that's gathered without the reward function", "tokens": [50404, 400, 570, 293, 281, 733, 295, 42923, 300, 3601, 300, 311, 300, 311, 13032, 1553, 264, 7782, 2445, 50604], "temperature": 0.0, "avg_logprob": -0.09459421600120654, "compression_ratio": 1.9543973941368078, "no_speech_prob": 0.0008549626218155026}, {"id": 1139, "seek": 485930, "start": 4864.18, "end": 4866.9800000000005, "text": " We use the world model as a mechanism to like distill that knowledge", "tokens": [50608, 492, 764, 264, 1002, 2316, 382, 257, 7513, 281, 411, 42923, 300, 3601, 50748], "temperature": 0.0, "avg_logprob": -0.09459421600120654, "compression_ratio": 1.9543973941368078, "no_speech_prob": 0.0008549626218155026}, {"id": 1140, "seek": 485930, "start": 4867.3, "end": 4871.3, "text": " Because if you just like naively have an agent gathering information with no reward function", "tokens": [50764, 1436, 498, 291, 445, 411, 1667, 3413, 362, 364, 9461, 13519, 1589, 365, 572, 7782, 2445, 50964], "temperature": 0.0, "avg_logprob": -0.09459421600120654, "compression_ratio": 1.9543973941368078, "no_speech_prob": 0.0008549626218155026}, {"id": 1141, "seek": 485930, "start": 4872.34, "end": 4874.820000000001, "text": " You know, how do you how do you kind of put that knowledge into the agent?", "tokens": [51016, 509, 458, 11, 577, 360, 291, 577, 360, 291, 733, 295, 829, 300, 3601, 666, 264, 9461, 30, 51140], "temperature": 0.0, "avg_logprob": -0.09459421600120654, "compression_ratio": 1.9543973941368078, "no_speech_prob": 0.0008549626218155026}, {"id": 1142, "seek": 485930, "start": 4874.820000000001, "end": 4877.14, "text": " And we kind of argue the best way of doing that is the world model", "tokens": [51140, 400, 321, 733, 295, 9695, 264, 1151, 636, 295, 884, 300, 307, 264, 1002, 2316, 51256], "temperature": 0.0, "avg_logprob": -0.09459421600120654, "compression_ratio": 1.9543973941368078, "no_speech_prob": 0.0008549626218155026}, {"id": 1143, "seek": 485930, "start": 4878.26, "end": 4882.9800000000005, "text": " So that's kind of a summary of like the waker paper and what like what the ultimate algorithm ends up doing", "tokens": [51312, 407, 300, 311, 733, 295, 257, 12691, 295, 411, 264, 261, 4003, 3035, 293, 437, 411, 437, 264, 9705, 9284, 5314, 493, 884, 51548], "temperature": 0.0, "avg_logprob": -0.09459421600120654, "compression_ratio": 1.9543973941368078, "no_speech_prob": 0.0008549626218155026}, {"id": 1144, "seek": 485930, "start": 4883.54, "end": 4888.34, "text": " So I mean essentially you're doing a high entropy search. So you're you're leaning into", "tokens": [51576, 407, 286, 914, 4476, 291, 434, 884, 257, 1090, 30867, 3164, 13, 407, 291, 434, 291, 434, 23390, 666, 51816], "temperature": 0.0, "avg_logprob": -0.09459421600120654, "compression_ratio": 1.9543973941368078, "no_speech_prob": 0.0008549626218155026}, {"id": 1145, "seek": 488834, "start": 4889.14, "end": 4892.02, "text": " Areas of complexity and you're building a higher complexity model", "tokens": [50404, 2014, 296, 295, 14024, 293, 291, 434, 2390, 257, 2946, 14024, 2316, 50548], "temperature": 0.0, "avg_logprob": -0.12050388133631343, "compression_ratio": 1.8448275862068966, "no_speech_prob": 0.0007051164284348488}, {"id": 1146, "seek": 488834, "start": 4892.02, "end": 4896.5, "text": " Which goes against the grain of the intuition of like Occam's razor that should have simple models", "tokens": [50548, 3013, 1709, 1970, 264, 12837, 295, 264, 24002, 295, 411, 26191, 335, 311, 30478, 300, 820, 362, 2199, 5245, 50772], "temperature": 0.0, "avg_logprob": -0.12050388133631343, "compression_ratio": 1.8448275862068966, "no_speech_prob": 0.0007051164284348488}, {"id": 1147, "seek": 488834, "start": 4896.5, "end": 4902.02, "text": " So you're you're almost deliberately saying no, I want I want to model the the complexity and have more of that", "tokens": [50772, 407, 291, 434, 291, 434, 1920, 23506, 1566, 572, 11, 286, 528, 286, 528, 281, 2316, 264, 264, 14024, 293, 362, 544, 295, 300, 51048], "temperature": 0.0, "avg_logprob": -0.12050388133631343, "compression_ratio": 1.8448275862068966, "no_speech_prob": 0.0007051164284348488}, {"id": 1148, "seek": 488834, "start": 4902.34, "end": 4905.62, "text": " And then the other interesting thing is like from from a curriculum learning point of view", "tokens": [51064, 400, 550, 264, 661, 1880, 551, 307, 411, 490, 490, 257, 14302, 2539, 935, 295, 1910, 51228], "temperature": 0.0, "avg_logprob": -0.12050388133631343, "compression_ratio": 1.8448275862068966, "no_speech_prob": 0.0007051164284348488}, {"id": 1149, "seek": 488834, "start": 4905.62, "end": 4910.42, "text": " I think traditionally we did explicit curriculum learning and you know, we might have some", "tokens": [51228, 286, 519, 19067, 321, 630, 13691, 14302, 2539, 293, 291, 458, 11, 321, 1062, 362, 512, 51468], "temperature": 0.0, "avg_logprob": -0.12050388133631343, "compression_ratio": 1.8448275862068966, "no_speech_prob": 0.0007051164284348488}, {"id": 1150, "seek": 488834, "start": 4910.9800000000005, "end": 4914.58, "text": " Principles around having a monotonically increasing curriculum of complexity", "tokens": [51496, 38372, 2622, 926, 1419, 257, 1108, 27794, 984, 5662, 14302, 295, 14024, 51676], "temperature": 0.0, "avg_logprob": -0.12050388133631343, "compression_ratio": 1.8448275862068966, "no_speech_prob": 0.0007051164284348488}, {"id": 1151, "seek": 491458, "start": 4914.98, "end": 4916.98, "text": " Whereas here by leaning into", "tokens": [50384, 13813, 510, 538, 23390, 666, 50484], "temperature": 0.0, "avg_logprob": -0.11015789065740805, "compression_ratio": 1.9277978339350181, "no_speech_prob": 0.0026689735241234303}, {"id": 1152, "seek": 491458, "start": 4917.94, "end": 4921.3, "text": " Environments where we do worse on so we're selecting them based on prediction error", "tokens": [50532, 19286, 1117, 689, 321, 360, 5324, 322, 370, 321, 434, 18182, 552, 2361, 322, 17630, 6713, 50700], "temperature": 0.0, "avg_logprob": -0.11015789065740805, "compression_ratio": 1.9277978339350181, "no_speech_prob": 0.0026689735241234303}, {"id": 1153, "seek": 491458, "start": 4921.46, "end": 4926.74, "text": " We're actually implicitly getting a kind of monotonically increasing complexity, which just happens to work really well", "tokens": [50708, 492, 434, 767, 26947, 356, 1242, 257, 733, 295, 1108, 27794, 984, 5662, 14024, 11, 597, 445, 2314, 281, 589, 534, 731, 50972], "temperature": 0.0, "avg_logprob": -0.11015789065740805, "compression_ratio": 1.9277978339350181, "no_speech_prob": 0.0026689735241234303}, {"id": 1154, "seek": 491458, "start": 4927.54, "end": 4931.46, "text": " Yeah, I guess actually it actually almost ends up being in the opposite direction", "tokens": [51012, 865, 11, 286, 2041, 767, 309, 767, 1920, 5314, 493, 885, 294, 264, 6182, 3513, 51208], "temperature": 0.0, "avg_logprob": -0.11015789065740805, "compression_ratio": 1.9277978339350181, "no_speech_prob": 0.0026689735241234303}, {"id": 1155, "seek": 491458, "start": 4931.46, "end": 4935.14, "text": " So so by leaning into the the the higher complexity environments more", "tokens": [51208, 407, 370, 538, 23390, 666, 264, 264, 264, 2946, 14024, 12388, 544, 51392], "temperature": 0.0, "avg_logprob": -0.11015789065740805, "compression_ratio": 1.9277978339350181, "no_speech_prob": 0.0026689735241234303}, {"id": 1156, "seek": 491458, "start": 4935.14, "end": 4938.42, "text": " We're kind of saying let's prioritize the harder environments more to begin with", "tokens": [51392, 492, 434, 733, 295, 1566, 718, 311, 25164, 264, 6081, 12388, 544, 281, 1841, 365, 51556], "temperature": 0.0, "avg_logprob": -0.11015789065740805, "compression_ratio": 1.9277978339350181, "no_speech_prob": 0.0026689735241234303}, {"id": 1157, "seek": 491458, "start": 4938.42, "end": 4941.7, "text": " So let's like gather more data in the higher complexity environments", "tokens": [51556, 407, 718, 311, 411, 5448, 544, 1412, 294, 264, 2946, 14024, 12388, 51720], "temperature": 0.0, "avg_logprob": -0.11015789065740805, "compression_ratio": 1.9277978339350181, "no_speech_prob": 0.0026689735241234303}, {"id": 1158, "seek": 494170, "start": 4942.66, "end": 4947.139999999999, "text": " Um, you know, because I guess intuitively if you kind of want to be good across all environments", "tokens": [50412, 3301, 11, 291, 458, 11, 570, 286, 2041, 46506, 498, 291, 733, 295, 528, 281, 312, 665, 2108, 439, 12388, 50636], "temperature": 0.0, "avg_logprob": -0.10939065421499856, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.0030745575204491615}, {"id": 1159, "seek": 494170, "start": 4947.62, "end": 4950.179999999999, "text": " You kind of need more data from the higher complexity environments", "tokens": [50660, 509, 733, 295, 643, 544, 1412, 490, 264, 2946, 14024, 12388, 50788], "temperature": 0.0, "avg_logprob": -0.10939065421499856, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.0030745575204491615}, {"id": 1160, "seek": 494170, "start": 4950.5, "end": 4954.26, "text": " And we don't really explicitly think about an ordering of going first from easy to hard", "tokens": [50804, 400, 321, 500, 380, 534, 20803, 519, 466, 364, 21739, 295, 516, 700, 490, 1858, 281, 1152, 50992], "temperature": 0.0, "avg_logprob": -0.10939065421499856, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.0030745575204491615}, {"id": 1161, "seek": 494170, "start": 4954.74, "end": 4957.54, "text": " Um, I guess that maybe there is a something to look into there because", "tokens": [51016, 3301, 11, 286, 2041, 300, 1310, 456, 307, 257, 746, 281, 574, 666, 456, 570, 51156], "temperature": 0.0, "avg_logprob": -0.10939065421499856, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.0030745575204491615}, {"id": 1162, "seek": 494170, "start": 4958.179999999999, "end": 4959.0599999999995, "text": " You know", "tokens": [51188, 509, 458, 51232], "temperature": 0.0, "avg_logprob": -0.10939065421499856, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.0030745575204491615}, {"id": 1163, "seek": 494170, "start": 4959.0599999999995, "end": 4962.66, "text": " Like a lot of these works go from low complexity to high complexity because it's kind of easier to learn", "tokens": [51232, 1743, 257, 688, 295, 613, 1985, 352, 490, 2295, 14024, 281, 1090, 14024, 570, 309, 311, 733, 295, 3571, 281, 1466, 51412], "temperature": 0.0, "avg_logprob": -0.10939065421499856, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.0030745575204491615}, {"id": 1164, "seek": 494170, "start": 4962.66, "end": 4967.54, "text": " An initial policy that can kind of do something in the low complexity environment and then you build up the complexity", "tokens": [51412, 1107, 5883, 3897, 300, 393, 733, 295, 360, 746, 294, 264, 2295, 14024, 2823, 293, 550, 291, 1322, 493, 264, 14024, 51656], "temperature": 0.0, "avg_logprob": -0.10939065421499856, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.0030745575204491615}, {"id": 1165, "seek": 496754, "start": 4967.94, "end": 4972.1, "text": " Gradually, um, but I think that that idea is most useful when you know what the task is", "tokens": [50384, 16710, 671, 11, 1105, 11, 457, 286, 519, 300, 300, 1558, 307, 881, 4420, 562, 291, 458, 437, 264, 5633, 307, 50592], "temperature": 0.0, "avg_logprob": -0.12392414064335644, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.023683762177824974}, {"id": 1166, "seek": 496754, "start": 4972.1, "end": 4974.98, "text": " So you could imagine if the task is like low commotion if it's walking", "tokens": [50592, 407, 291, 727, 3811, 498, 264, 5633, 307, 411, 2295, 800, 19228, 498, 309, 311, 4494, 50736], "temperature": 0.0, "avg_logprob": -0.12392414064335644, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.023683762177824974}, {"id": 1167, "seek": 496754, "start": 4975.38, "end": 4978.18, "text": " You kind of want to first learn a policy that's able to walk on flat ground", "tokens": [50756, 509, 733, 295, 528, 281, 700, 1466, 257, 3897, 300, 311, 1075, 281, 1792, 322, 4962, 2727, 50896], "temperature": 0.0, "avg_logprob": -0.12392414064335644, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.023683762177824974}, {"id": 1168, "seek": 496754, "start": 4978.26, "end": 4982.42, "text": " And then maybe gradually build up the complexity like add and bumps and then eventually it can walk on like a very", "tokens": [50900, 400, 550, 1310, 13145, 1322, 493, 264, 14024, 411, 909, 293, 27719, 293, 550, 4728, 309, 393, 1792, 322, 411, 257, 588, 51108], "temperature": 0.0, "avg_logprob": -0.12392414064335644, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.023683762177824974}, {"id": 1169, "seek": 496754, "start": 4983.06, "end": 4985.94, "text": " Complicated terrain so it kind of makes sense to go from low to high complexity", "tokens": [51140, 33736, 3587, 17674, 370, 309, 733, 295, 1669, 2020, 281, 352, 490, 2295, 281, 1090, 14024, 51284], "temperature": 0.0, "avg_logprob": -0.12392414064335644, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.023683762177824974}, {"id": 1170, "seek": 496754, "start": 4986.5, "end": 4988.5, "text": " um, but in this work we're focusing on", "tokens": [51312, 1105, 11, 457, 294, 341, 589, 321, 434, 8416, 322, 51412], "temperature": 0.0, "avg_logprob": -0.12392414064335644, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.023683762177824974}, {"id": 1171, "seek": 496754, "start": 4989.3, "end": 4993.54, "text": " purely intrinsic motivation meaning that the policy is not trying to learn a specific task", "tokens": [51452, 17491, 35698, 12335, 3620, 300, 264, 3897, 307, 406, 1382, 281, 1466, 257, 2685, 5633, 51664], "temperature": 0.0, "avg_logprob": -0.12392414064335644, "compression_ratio": 1.768987341772152, "no_speech_prob": 0.023683762177824974}, {"id": 1172, "seek": 499354, "start": 4993.86, "end": 4997.86, "text": " It's trying to just seek out um uncertainty and like reduce uncertainty", "tokens": [50380, 467, 311, 1382, 281, 445, 8075, 484, 1105, 15697, 293, 411, 5407, 15697, 50580], "temperature": 0.0, "avg_logprob": -0.09528152362720387, "compression_ratio": 2.0, "no_speech_prob": 0.02095591463148594}, {"id": 1173, "seek": 499354, "start": 4998.26, "end": 5000.18, "text": " And so we don't really have the notion of you know", "tokens": [50600, 400, 370, 321, 500, 380, 534, 362, 264, 10710, 295, 291, 458, 50696], "temperature": 0.0, "avg_logprob": -0.09528152362720387, "compression_ratio": 2.0, "no_speech_prob": 0.02095591463148594}, {"id": 1174, "seek": 499354, "start": 5000.18, "end": 5002.82, "text": " You first need to be able to learn how to do something on an easy", "tokens": [50696, 509, 700, 643, 281, 312, 1075, 281, 1466, 577, 281, 360, 746, 322, 364, 1858, 50828], "temperature": 0.0, "avg_logprob": -0.09528152362720387, "compression_ratio": 2.0, "no_speech_prob": 0.02095591463148594}, {"id": 1175, "seek": 499354, "start": 5003.38, "end": 5008.34, "text": " An easy environment and then move towards harder environments because there is no specific task that we're trying to learn", "tokens": [50856, 1107, 1858, 2823, 293, 550, 1286, 3030, 6081, 12388, 570, 456, 307, 572, 2685, 5633, 300, 321, 434, 1382, 281, 1466, 51104], "temperature": 0.0, "avg_logprob": -0.09528152362720387, "compression_ratio": 2.0, "no_speech_prob": 0.02095591463148594}, {"id": 1176, "seek": 499354, "start": 5008.66, "end": 5013.86, "text": " And so I think for this reason, you know, we wouldn't didn't really focus on this notion of moving from easier to harder environments", "tokens": [51120, 400, 370, 286, 519, 337, 341, 1778, 11, 291, 458, 11, 321, 2759, 380, 994, 380, 534, 1879, 322, 341, 10710, 295, 2684, 490, 3571, 281, 6081, 12388, 51380], "temperature": 0.0, "avg_logprob": -0.09528152362720387, "compression_ratio": 2.0, "no_speech_prob": 0.02095591463148594}, {"id": 1177, "seek": 499354, "start": 5013.86, "end": 5017.14, "text": " So that actually, you know, we're consistently something more data from the hard environments", "tokens": [51380, 407, 300, 767, 11, 291, 458, 11, 321, 434, 14961, 746, 544, 1412, 490, 264, 1152, 12388, 51544], "temperature": 0.0, "avg_logprob": -0.09528152362720387, "compression_ratio": 2.0, "no_speech_prob": 0.02095591463148594}, {"id": 1178, "seek": 499354, "start": 5017.78, "end": 5021.7, "text": " And I guess I think this relates or I think this is something that you brought up when we when we worked on this is like", "tokens": [51576, 400, 286, 2041, 286, 519, 341, 16155, 420, 286, 519, 341, 307, 746, 300, 291, 3038, 493, 562, 321, 562, 321, 2732, 322, 341, 307, 411, 51772], "temperature": 0.0, "avg_logprob": -0.09528152362720387, "compression_ratio": 2.0, "no_speech_prob": 0.02095591463148594}, {"id": 1179, "seek": 502170, "start": 5022.42, "end": 5028.26, "text": " You know, I think we can really relate this idea to like a lot of different contexts including things like like language models, for example", "tokens": [50400, 509, 458, 11, 286, 519, 321, 393, 534, 10961, 341, 1558, 281, 411, 257, 688, 295, 819, 30628, 3009, 721, 411, 411, 2856, 5245, 11, 337, 1365, 50692], "temperature": 0.0, "avg_logprob": -0.13856791087559292, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.0015007848851382732}, {"id": 1180, "seek": 502170, "start": 5029.139999999999, "end": 5030.0199999999995, "text": " um", "tokens": [50736, 1105, 50780], "temperature": 0.0, "avg_logprob": -0.13856791087559292, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.0015007848851382732}, {"id": 1181, "seek": 502170, "start": 5030.0199999999995, "end": 5034.74, "text": " So, you know, you can imagine if I'm training an llm. I don't really necessarily have this, you know", "tokens": [50780, 407, 11, 291, 458, 11, 291, 393, 3811, 498, 286, 478, 3097, 364, 287, 75, 76, 13, 286, 500, 380, 534, 4725, 362, 341, 11, 291, 458, 51016], "temperature": 0.0, "avg_logprob": -0.13856791087559292, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.0015007848851382732}, {"id": 1182, "seek": 502170, "start": 5034.82, "end": 5037.3, "text": " Not really a reward function in some sense. You're just trying to", "tokens": [51020, 1726, 534, 257, 7782, 2445, 294, 512, 2020, 13, 509, 434, 445, 1382, 281, 51144], "temperature": 0.0, "avg_logprob": -0.13856791087559292, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.0015007848851382732}, {"id": 1183, "seek": 502170, "start": 5038.099999999999, "end": 5040.099999999999, "text": " Do like unsupervised prediction", "tokens": [51184, 1144, 411, 2693, 12879, 24420, 17630, 51284], "temperature": 0.0, "avg_logprob": -0.13856791087559292, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.0015007848851382732}, {"id": 1184, "seek": 502170, "start": 5040.34, "end": 5047.0599999999995, "text": " And so, you know, we could for example take the prediction error of like a language model and a bunch of different domains and say, you know, the language model is", "tokens": [51296, 400, 370, 11, 291, 458, 11, 321, 727, 337, 1365, 747, 264, 17630, 6713, 295, 411, 257, 2856, 2316, 293, 257, 3840, 295, 819, 25514, 293, 584, 11, 291, 458, 11, 264, 2856, 2316, 307, 51632], "temperature": 0.0, "avg_logprob": -0.13856791087559292, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.0015007848851382732}, {"id": 1185, "seek": 502170, "start": 5048.0199999999995, "end": 5051.46, "text": " Not very good at predicting a language about some certain task or something like that", "tokens": [51680, 1726, 588, 665, 412, 32884, 257, 2856, 466, 512, 1629, 5633, 420, 746, 411, 300, 51852], "temperature": 0.0, "avg_logprob": -0.13856791087559292, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.0015007848851382732}, {"id": 1186, "seek": 505170, "start": 5051.94, "end": 5056.34, "text": " And you know, we could say, you know, and intuitively the same thing kinds of holds if it's not very good at predicting, you know", "tokens": [50376, 400, 291, 458, 11, 321, 727, 584, 11, 291, 458, 11, 293, 46506, 264, 912, 551, 3685, 295, 9190, 498, 309, 311, 406, 588, 665, 412, 32884, 11, 291, 458, 50596], "temperature": 0.0, "avg_logprob": -0.13837683461878422, "compression_ratio": 1.8793650793650793, "no_speech_prob": 0.00015839695697650313}, {"id": 1187, "seek": 505170, "start": 5057.38, "end": 5060.9, "text": " What the next token is in french like we should presumably gather more data in french", "tokens": [50648, 708, 264, 958, 14862, 307, 294, 27598, 411, 321, 820, 26742, 5448, 544, 1412, 294, 27598, 50824], "temperature": 0.0, "avg_logprob": -0.13837683461878422, "compression_ratio": 1.8793650793650793, "no_speech_prob": 0.00015839695697650313}, {"id": 1188, "seek": 505170, "start": 5061.46, "end": 5065.139999999999, "text": " And that so that kind of gives us a way of like actively gathering the appropriate data", "tokens": [50852, 400, 300, 370, 300, 733, 295, 2709, 505, 257, 636, 295, 411, 13022, 13519, 264, 6854, 1412, 51036], "temperature": 0.0, "avg_logprob": -0.13837683461878422, "compression_ratio": 1.8793650793650793, "no_speech_prob": 0.00015839695697650313}, {"id": 1189, "seek": 505170, "start": 5065.62, "end": 5069.62, "text": " Um, and so yeah, I think this idea of like gathering more data based on certainty", "tokens": [51060, 3301, 11, 293, 370, 1338, 11, 286, 519, 341, 1558, 295, 411, 13519, 544, 1412, 2361, 322, 27022, 51260], "temperature": 0.0, "avg_logprob": -0.13837683461878422, "compression_ratio": 1.8793650793650793, "no_speech_prob": 0.00015839695697650313}, {"id": 1190, "seek": 505170, "start": 5069.62, "end": 5072.34, "text": " Obviously is a very general idea like the idea of like active learning", "tokens": [51260, 7580, 307, 257, 588, 2674, 1558, 411, 264, 1558, 295, 411, 4967, 2539, 51396], "temperature": 0.0, "avg_logprob": -0.13837683461878422, "compression_ratio": 1.8793650793650793, "no_speech_prob": 0.00015839695697650313}, {"id": 1191, "seek": 505170, "start": 5072.9, "end": 5074.5, "text": " Um, but we kind of like", "tokens": [51424, 3301, 11, 457, 321, 733, 295, 411, 51504], "temperature": 0.0, "avg_logprob": -0.13837683461878422, "compression_ratio": 1.8793650793650793, "no_speech_prob": 0.00015839695697650313}, {"id": 1192, "seek": 505170, "start": 5074.5, "end": 5078.26, "text": " Specialized that into thinking about how do we think about this in terms of the reinforcement learning setting?", "tokens": [51504, 11863, 1602, 300, 666, 1953, 466, 577, 360, 321, 519, 466, 341, 294, 2115, 295, 264, 29280, 2539, 3287, 30, 51692], "temperature": 0.0, "avg_logprob": -0.13837683461878422, "compression_ratio": 1.8793650793650793, "no_speech_prob": 0.00015839695697650313}, {"id": 1193, "seek": 507826, "start": 5078.74, "end": 5082.900000000001, "text": " It might be interesting to talk about as well like sort of because we looked at some of the metrics as well, right?", "tokens": [50388, 467, 1062, 312, 1880, 281, 751, 466, 382, 731, 411, 1333, 295, 570, 321, 2956, 412, 512, 295, 264, 16367, 382, 731, 11, 558, 30, 50596], "temperature": 0.0, "avg_logprob": -0.14537313768080065, "compression_ratio": 1.8286604361370717, "no_speech_prob": 0.002714394824579358}, {"id": 1194, "seek": 507826, "start": 5082.900000000001, "end": 5086.58, "text": " The environment complexity metrics. Yeah, we don't have the external notion of difficulty", "tokens": [50596, 440, 2823, 14024, 16367, 13, 865, 11, 321, 500, 380, 362, 264, 8320, 10710, 295, 10360, 50780], "temperature": 0.0, "avg_logprob": -0.14537313768080065, "compression_ratio": 1.8286604361370717, "no_speech_prob": 0.002714394824579358}, {"id": 1195, "seek": 507826, "start": 5086.66, "end": 5093.14, "text": " But we we also did look at sort of the emergent, uh, curriculum. Yeah. Yeah. Yeah. Gotcha. Yeah, so I guess um", "tokens": [50784, 583, 321, 321, 611, 630, 574, 412, 1333, 295, 264, 4345, 6930, 11, 2232, 11, 14302, 13, 865, 13, 865, 13, 865, 13, 42109, 13, 865, 11, 370, 286, 2041, 1105, 51108], "temperature": 0.0, "avg_logprob": -0.14537313768080065, "compression_ratio": 1.8286604361370717, "no_speech_prob": 0.002714394824579358}, {"id": 1196, "seek": 507826, "start": 5093.780000000001, "end": 5095.22, "text": " So it kind of depended on the environment", "tokens": [51140, 407, 309, 733, 295, 1367, 3502, 322, 264, 2823, 51212], "temperature": 0.0, "avg_logprob": -0.14537313768080065, "compression_ratio": 1.8286604361370717, "no_speech_prob": 0.002714394824579358}, {"id": 1197, "seek": 507826, "start": 5095.22, "end": 5099.22, "text": " So in some environments, you just kind of got this like very straightforward behavior of like, you know", "tokens": [51212, 407, 294, 512, 12388, 11, 291, 445, 733, 295, 658, 341, 411, 588, 15325, 5223, 295, 411, 11, 291, 458, 51412], "temperature": 0.0, "avg_logprob": -0.14537313768080065, "compression_ratio": 1.8286604361370717, "no_speech_prob": 0.002714394824579358}, {"id": 1198, "seek": 507826, "start": 5099.3, "end": 5101.860000000001, "text": " Consistently gather more data in the more complex environment", "tokens": [51416, 6923, 468, 2276, 5448, 544, 1412, 294, 264, 544, 3997, 2823, 51544], "temperature": 0.0, "avg_logprob": -0.14537313768080065, "compression_ratio": 1.8286604361370717, "no_speech_prob": 0.002714394824579358}, {"id": 1199, "seek": 507826, "start": 5102.66, "end": 5106.18, "text": " um, but because we're we're actively trying to gather data, um", "tokens": [51584, 1105, 11, 457, 570, 321, 434, 321, 434, 13022, 1382, 281, 5448, 1412, 11, 1105, 51760], "temperature": 0.0, "avg_logprob": -0.14537313768080065, "compression_ratio": 1.8286604361370717, "no_speech_prob": 0.002714394824579358}, {"id": 1200, "seek": 510618, "start": 5106.900000000001, "end": 5112.900000000001, "text": " Of the the environments for which the uncertainty is the highest kind of this curriculum could change over over the course of training", "tokens": [50400, 2720, 264, 264, 12388, 337, 597, 264, 15697, 307, 264, 6343, 733, 295, 341, 14302, 727, 1319, 670, 670, 264, 1164, 295, 3097, 50700], "temperature": 0.0, "avg_logprob": -0.09913514241450976, "compression_ratio": 2.0, "no_speech_prob": 0.002979730488732457}, {"id": 1201, "seek": 510618, "start": 5112.9800000000005, "end": 5115.22, "text": " So so what happened in some of the other environments?", "tokens": [50704, 407, 370, 437, 2011, 294, 512, 295, 264, 661, 12388, 30, 50816], "temperature": 0.0, "avg_logprob": -0.09913514241450976, "compression_ratio": 2.0, "no_speech_prob": 0.002979730488732457}, {"id": 1202, "seek": 510618, "start": 5115.22, "end": 5118.740000000001, "text": " For example, is that initially all the environments are just like high uncertainty", "tokens": [50816, 1171, 1365, 11, 307, 300, 9105, 439, 264, 12388, 366, 445, 411, 1090, 15697, 50992], "temperature": 0.0, "avg_logprob": -0.09913514241450976, "compression_ratio": 2.0, "no_speech_prob": 0.002979730488732457}, {"id": 1203, "seek": 510618, "start": 5119.06, "end": 5124.42, "text": " Like there's like all environments are kind of misunderstood therefore like sample all environments like equally more or less", "tokens": [51008, 1743, 456, 311, 411, 439, 12388, 366, 733, 295, 33870, 4412, 411, 6889, 439, 12388, 411, 12309, 544, 420, 1570, 51276], "temperature": 0.0, "avg_logprob": -0.09913514241450976, "compression_ratio": 2.0, "no_speech_prob": 0.002979730488732457}, {"id": 1204, "seek": 510618, "start": 5124.58, "end": 5126.58, "text": " To just get a rough understanding", "tokens": [51284, 1407, 445, 483, 257, 5903, 3701, 51384], "temperature": 0.0, "avg_logprob": -0.09913514241450976, "compression_ratio": 2.0, "no_speech_prob": 0.002979730488732457}, {"id": 1205, "seek": 510618, "start": 5126.740000000001, "end": 5129.54, "text": " And then you know as as the model would improve on the simplest environments", "tokens": [51392, 400, 550, 291, 458, 382, 382, 264, 2316, 576, 3470, 322, 264, 22811, 12388, 51532], "temperature": 0.0, "avg_logprob": -0.09913514241450976, "compression_ratio": 2.0, "no_speech_prob": 0.002979730488732457}, {"id": 1206, "seek": 510618, "start": 5129.54, "end": 5133.38, "text": " Then we would see like more and more emphasis towards sampling the highest complexity environments", "tokens": [51532, 1396, 321, 576, 536, 411, 544, 293, 544, 16271, 3030, 21179, 264, 6343, 14024, 12388, 51724], "temperature": 0.0, "avg_logprob": -0.09913514241450976, "compression_ratio": 2.0, "no_speech_prob": 0.002979730488732457}, {"id": 1207, "seek": 513338, "start": 5133.7, "end": 5137.78, "text": " So I guess in that sense we would get something to more like kind of what you said in terms of like a standard curriculum", "tokens": [50380, 407, 286, 2041, 294, 300, 2020, 321, 576, 483, 746, 281, 544, 411, 733, 295, 437, 291, 848, 294, 2115, 295, 411, 257, 3832, 14302, 50584], "temperature": 0.0, "avg_logprob": -0.10473509254695484, "compression_ratio": 1.893048128342246, "no_speech_prob": 0.0016283371951431036}, {"id": 1208, "seek": 513338, "start": 5138.02, "end": 5141.14, "text": " But a bit different in the sense of like initially everything is uncertain", "tokens": [50596, 583, 257, 857, 819, 294, 264, 2020, 295, 411, 9105, 1203, 307, 11308, 50752], "temperature": 0.0, "avg_logprob": -0.10473509254695484, "compression_ratio": 1.893048128342246, "no_speech_prob": 0.0016283371951431036}, {"id": 1209, "seek": 513338, "start": 5141.3, "end": 5143.3, "text": " So we're just going to sample everything uniformly", "tokens": [50760, 407, 321, 434, 445, 516, 281, 6889, 1203, 48806, 50860], "temperature": 0.0, "avg_logprob": -0.10473509254695484, "compression_ratio": 1.893048128342246, "no_speech_prob": 0.0016283371951431036}, {"id": 1210, "seek": 513338, "start": 5143.78, "end": 5146.34, "text": " Um, but then we kind of get a better understanding of which of the environments", "tokens": [50884, 3301, 11, 457, 550, 321, 733, 295, 483, 257, 1101, 3701, 295, 597, 295, 264, 12388, 51012], "temperature": 0.0, "avg_logprob": -0.10473509254695484, "compression_ratio": 1.893048128342246, "no_speech_prob": 0.0016283371951431036}, {"id": 1211, "seek": 513338, "start": 5146.42, "end": 5151.78, "text": " You know the uncertainty remains high on these higher complexity ones and those are the ones we need to like go out and gather more data", "tokens": [51016, 509, 458, 264, 15697, 7023, 1090, 322, 613, 2946, 14024, 2306, 293, 729, 366, 264, 2306, 321, 643, 281, 411, 352, 484, 293, 5448, 544, 1412, 51284], "temperature": 0.0, "avg_logprob": -0.10473509254695484, "compression_ratio": 1.893048128342246, "no_speech_prob": 0.0016283371951431036}, {"id": 1212, "seek": 513338, "start": 5152.1, "end": 5152.82, "text": " Yeah", "tokens": [51300, 865, 51336], "temperature": 0.0, "avg_logprob": -0.10473509254695484, "compression_ratio": 1.893048128342246, "no_speech_prob": 0.0016283371951431036}, {"id": 1213, "seek": 513338, "start": 5152.82, "end": 5154.1, "text": " I mean I can see this both ways", "tokens": [51336, 286, 914, 286, 393, 536, 341, 1293, 2098, 51400], "temperature": 0.0, "avg_logprob": -0.10473509254695484, "compression_ratio": 1.893048128342246, "no_speech_prob": 0.0016283371951431036}, {"id": 1214, "seek": 513338, "start": 5154.1, "end": 5157.38, "text": " I mean certainly from like a Bayesian optimization point of view that there's something to be said for", "tokens": [51400, 286, 914, 3297, 490, 411, 257, 7840, 42434, 19618, 935, 295, 1910, 300, 456, 311, 746, 281, 312, 848, 337, 51564], "temperature": 0.0, "avg_logprob": -0.10473509254695484, "compression_ratio": 1.893048128342246, "no_speech_prob": 0.0016283371951431036}, {"id": 1215, "seek": 513338, "start": 5157.54, "end": 5162.34, "text": " Um, you know, this is where I'm uncertain going gather more data where where I have highest uncertainty", "tokens": [51572, 3301, 11, 291, 458, 11, 341, 307, 689, 286, 478, 11308, 516, 5448, 544, 1412, 689, 689, 286, 362, 6343, 15697, 51812], "temperature": 0.0, "avg_logprob": -0.10473509254695484, "compression_ratio": 1.893048128342246, "no_speech_prob": 0.0016283371951431036}, {"id": 1216, "seek": 516234, "start": 5162.82, "end": 5165.3, "text": " And uh, as you say like traditionally in curriculum learning", "tokens": [50388, 400, 2232, 11, 382, 291, 584, 411, 19067, 294, 14302, 2539, 50512], "temperature": 0.0, "avg_logprob": -0.14167919711790222, "compression_ratio": 1.7492260061919505, "no_speech_prob": 0.0036669080145657063}, {"id": 1217, "seek": 516234, "start": 5165.54, "end": 5168.5, "text": " We are told that we need to have monotonic increasing complexity", "tokens": [50524, 492, 366, 1907, 300, 321, 643, 281, 362, 1108, 310, 11630, 5662, 14024, 50672], "temperature": 0.0, "avg_logprob": -0.14167919711790222, "compression_ratio": 1.7492260061919505, "no_speech_prob": 0.0036669080145657063}, {"id": 1218, "seek": 516234, "start": 5168.5, "end": 5172.5, "text": " But as you just said that's when we have a particular task in mind now neural networks", "tokens": [50672, 583, 382, 291, 445, 848, 300, 311, 562, 321, 362, 257, 1729, 5633, 294, 1575, 586, 18161, 9590, 50872], "temperature": 0.0, "avg_logprob": -0.14167919711790222, "compression_ratio": 1.7492260061919505, "no_speech_prob": 0.0036669080145657063}, {"id": 1219, "seek": 516234, "start": 5172.5, "end": 5175.78, "text": " They're a little bit like a block of clay aren't they so you know, it starts off with", "tokens": [50872, 814, 434, 257, 707, 857, 411, 257, 3461, 295, 13517, 3212, 380, 436, 370, 291, 458, 11, 309, 3719, 766, 365, 51036], "temperature": 0.0, "avg_logprob": -0.14167919711790222, "compression_ratio": 1.7492260061919505, "no_speech_prob": 0.0036669080145657063}, {"id": 1220, "seek": 516234, "start": 5176.26, "end": 5181.46, "text": " Abject complexity and then we do stand, you know, we do um stochastic gradient descent and we chip away at the clay", "tokens": [51060, 2847, 1020, 14024, 293, 550, 321, 360, 1463, 11, 291, 458, 11, 321, 360, 1105, 342, 8997, 2750, 16235, 23475, 293, 321, 11409, 1314, 412, 264, 13517, 51320], "temperature": 0.0, "avg_logprob": -0.14167919711790222, "compression_ratio": 1.7492260061919505, "no_speech_prob": 0.0036669080145657063}, {"id": 1221, "seek": 516234, "start": 5181.54, "end": 5187.3, "text": " And we kind of build we sculpt a statue that that that we want to build and I'm just trying to get an intuition here", "tokens": [51324, 400, 321, 733, 295, 1322, 321, 12613, 257, 17385, 300, 300, 300, 321, 528, 281, 1322, 293, 286, 478, 445, 1382, 281, 483, 364, 24002, 510, 51612], "temperature": 0.0, "avg_logprob": -0.14167919711790222, "compression_ratio": 1.7492260061919505, "no_speech_prob": 0.0036669080145657063}, {"id": 1222, "seek": 516234, "start": 5187.3, "end": 5189.3, "text": " So like with this maximum entropy", "tokens": [51612, 407, 411, 365, 341, 6674, 30867, 51712], "temperature": 0.0, "avg_logprob": -0.14167919711790222, "compression_ratio": 1.7492260061919505, "no_speech_prob": 0.0036669080145657063}, {"id": 1223, "seek": 518930, "start": 5189.54, "end": 5191.54, "text": " Search, you know like high entropy search", "tokens": [50376, 17180, 11, 291, 458, 411, 1090, 30867, 3164, 50476], "temperature": 0.0, "avg_logprob": -0.11230626250758316, "compression_ratio": 1.8525641025641026, "no_speech_prob": 0.014940707013010979}, {"id": 1224, "seek": 518930, "start": 5191.62, "end": 5193.06, "text": " What we're doing is is we're saying okay", "tokens": [50480, 708, 321, 434, 884, 307, 307, 321, 434, 1566, 1392, 50552], "temperature": 0.0, "avg_logprob": -0.11230626250758316, "compression_ratio": 1.8525641025641026, "no_speech_prob": 0.014940707013010979}, {"id": 1225, "seek": 518930, "start": 5193.06, "end": 5198.66, "text": " Well, here are some complex models and these models must contain motifs that tell us a lot of information", "tokens": [50552, 1042, 11, 510, 366, 512, 3997, 5245, 293, 613, 5245, 1633, 5304, 2184, 18290, 300, 980, 505, 257, 688, 295, 1589, 50832], "temperature": 0.0, "avg_logprob": -0.11230626250758316, "compression_ratio": 1.8525641025641026, "no_speech_prob": 0.014940707013010979}, {"id": 1226, "seek": 518930, "start": 5199.06, "end": 5200.900000000001, "text": " It's a little bit like the elo algorithm in chess", "tokens": [50852, 467, 311, 257, 707, 857, 411, 264, 38682, 9284, 294, 24122, 50944], "temperature": 0.0, "avg_logprob": -0.11230626250758316, "compression_ratio": 1.8525641025641026, "no_speech_prob": 0.014940707013010979}, {"id": 1227, "seek": 518930, "start": 5200.9800000000005, "end": 5204.74, "text": " You know, you actually get information gain when something surprising happened", "tokens": [50948, 509, 458, 11, 291, 767, 483, 1589, 6052, 562, 746, 8830, 2011, 51136], "temperature": 0.0, "avg_logprob": -0.11230626250758316, "compression_ratio": 1.8525641025641026, "no_speech_prob": 0.014940707013010979}, {"id": 1228, "seek": 518930, "start": 5205.06, "end": 5209.7, "text": " So here's a big block of complexity and I'm going to try and infer", "tokens": [51152, 407, 510, 311, 257, 955, 3461, 295, 14024, 293, 286, 478, 516, 281, 853, 293, 13596, 51384], "temperature": 0.0, "avg_logprob": -0.11230626250758316, "compression_ratio": 1.8525641025641026, "no_speech_prob": 0.014940707013010979}, {"id": 1229, "seek": 518930, "start": 5210.26, "end": 5213.9400000000005, "text": " What the motifs are in that complexity that that explain the information that I'm missing", "tokens": [51412, 708, 264, 2184, 18290, 366, 294, 300, 14024, 300, 300, 2903, 264, 1589, 300, 286, 478, 5361, 51596], "temperature": 0.0, "avg_logprob": -0.11230626250758316, "compression_ratio": 1.8525641025641026, "no_speech_prob": 0.014940707013010979}, {"id": 1230, "seek": 518930, "start": 5214.18, "end": 5219.22, "text": " I think that a lot of this ultimately traces back to sort of there's like this like fundamental pattern", "tokens": [51608, 286, 519, 300, 257, 688, 295, 341, 6284, 26076, 646, 281, 1333, 295, 456, 311, 411, 341, 411, 8088, 5102, 51860], "temperature": 0.0, "avg_logprob": -0.11230626250758316, "compression_ratio": 1.8525641025641026, "no_speech_prob": 0.014940707013010979}, {"id": 1231, "seek": 521930, "start": 5219.62, "end": 5223.14, "text": " towards uh, I think that like ties a lot of these ideas around active", "tokens": [50380, 3030, 2232, 11, 286, 519, 300, 411, 14039, 257, 688, 295, 613, 3487, 926, 4967, 50556], "temperature": 0.0, "avg_logprob": -0.17761348377574573, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.0004878164909314364}, {"id": 1232, "seek": 521930, "start": 5223.62, "end": 5230.34, "text": " Um active experiment design or like active sampling, which is and all these autocurricular methods, which is you essentially want to devise", "tokens": [50580, 3301, 4967, 5120, 1715, 420, 411, 4967, 21179, 11, 597, 307, 293, 439, 613, 45833, 374, 42750, 7150, 11, 597, 307, 291, 4476, 528, 281, 1905, 908, 50916], "temperature": 0.0, "avg_logprob": -0.17761348377574573, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.0004878164909314364}, {"id": 1233, "seek": 521930, "start": 5230.900000000001, "end": 5236.34, "text": " Uh, what you know nowadays we call a self supervised objective or self self supervised training algorithm", "tokens": [50944, 4019, 11, 437, 291, 458, 13434, 321, 818, 257, 2698, 46533, 10024, 420, 2698, 2698, 46533, 3097, 9284, 51216], "temperature": 0.0, "avg_logprob": -0.17761348377574573, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.0004878164909314364}, {"id": 1234, "seek": 521930, "start": 5236.74, "end": 5241.860000000001, "text": " Um, where essentially you have the system essentially use signals. It produces itself", "tokens": [51236, 3301, 11, 689, 4476, 291, 362, 264, 1185, 4476, 764, 12354, 13, 467, 14725, 2564, 51492], "temperature": 0.0, "avg_logprob": -0.17761348377574573, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.0004878164909314364}, {"id": 1235, "seek": 521930, "start": 5242.34, "end": 5248.42, "text": " Um during the training or evaluation process in order to drive itself forward in terms of deciding what future data to train on", "tokens": [51516, 3301, 1830, 264, 3097, 420, 13344, 1399, 294, 1668, 281, 3332, 2564, 2128, 294, 2115, 295, 17990, 437, 2027, 1412, 281, 3847, 322, 51820], "temperature": 0.0, "avg_logprob": -0.17761348377574573, "compression_ratio": 1.8241379310344827, "no_speech_prob": 0.0004878164909314364}, {"id": 1236, "seek": 524842, "start": 5248.74, "end": 5255.06, "text": " And so, you know, we sometimes call these kinds of systems autocurricular as well because it's automatically generating this curriculum of", "tokens": [50380, 400, 370, 11, 291, 458, 11, 321, 2171, 818, 613, 3685, 295, 3652, 45833, 374, 42750, 382, 731, 570, 309, 311, 6772, 17746, 341, 14302, 295, 50696], "temperature": 0.0, "avg_logprob": -0.1539303915841239, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0008558081462979317}, {"id": 1237, "seek": 524842, "start": 5255.38, "end": 5259.22, "text": " Tasks to train on and I think the sort of like the fundamental connecting", "tokens": [50712, 27293, 1694, 281, 3847, 322, 293, 286, 519, 264, 1333, 295, 411, 264, 8088, 11015, 50904], "temperature": 0.0, "avg_logprob": -0.1539303915841239, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0008558081462979317}, {"id": 1238, "seek": 524842, "start": 5260.26, "end": 5264.02, "text": " Uh pattern here is just that this the signal that we use to drive the training", "tokens": [50956, 4019, 5102, 510, 307, 445, 300, 341, 264, 6358, 300, 321, 764, 281, 3332, 264, 3097, 51144], "temperature": 0.0, "avg_logprob": -0.1539303915841239, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0008558081462979317}, {"id": 1239, "seek": 524842, "start": 5264.18, "end": 5268.66, "text": " It's always going to be based on something like, uh, an uncertainty signal or, um", "tokens": [51152, 467, 311, 1009, 516, 281, 312, 2361, 322, 746, 411, 11, 2232, 11, 364, 15697, 6358, 420, 11, 1105, 51376], "temperature": 0.0, "avg_logprob": -0.1539303915841239, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0008558081462979317}, {"id": 1240, "seek": 524842, "start": 5269.06, "end": 5273.3, "text": " Going back to the open-endedness literature something like a classic notion of interestingness", "tokens": [51396, 10963, 646, 281, 264, 1269, 12, 3502, 1287, 10394, 746, 411, 257, 7230, 10710, 295, 1880, 1287, 51608], "temperature": 0.0, "avg_logprob": -0.1539303915841239, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0008558081462979317}, {"id": 1241, "seek": 527330, "start": 5273.78, "end": 5278.58, "text": " And I think there's just a lot of different possible choices for this metric and so", "tokens": [50388, 400, 286, 519, 456, 311, 445, 257, 688, 295, 819, 1944, 7994, 337, 341, 20678, 293, 370, 50628], "temperature": 0.0, "avg_logprob": -0.11126484590418198, "compression_ratio": 1.6701754385964913, "no_speech_prob": 0.04467069357633591}, {"id": 1242, "seek": 527330, "start": 5279.38, "end": 5281.38, "text": " One for example, we talked a lot about", "tokens": [50668, 1485, 337, 1365, 11, 321, 2825, 257, 688, 466, 50768], "temperature": 0.0, "avg_logprob": -0.11126484590418198, "compression_ratio": 1.6701754385964913, "no_speech_prob": 0.04467069357633591}, {"id": 1243, "seek": 527330, "start": 5281.38, "end": 5282.26, "text": " Minimax regret", "tokens": [50768, 2829, 332, 2797, 10879, 50812], "temperature": 0.0, "avg_logprob": -0.11126484590418198, "compression_ratio": 1.6701754385964913, "no_speech_prob": 0.04467069357633591}, {"id": 1244, "seek": 527330, "start": 5282.26, "end": 5287.62, "text": " So regret could be one of these driving signals because it measures the existence of a performance gap and therefore", "tokens": [50812, 407, 10879, 727, 312, 472, 295, 613, 4840, 12354, 570, 309, 8000, 264, 9123, 295, 257, 3389, 7417, 293, 4412, 51080], "temperature": 0.0, "avg_logprob": -0.11126484590418198, "compression_ratio": 1.6701754385964913, "no_speech_prob": 0.04467069357633591}, {"id": 1245, "seek": 527330, "start": 5287.7, "end": 5291.78, "text": " Probably an information gap as well in terms of learning to master those tasks with high regret", "tokens": [51084, 9210, 364, 1589, 7417, 382, 731, 294, 2115, 295, 2539, 281, 4505, 729, 9608, 365, 1090, 10879, 51288], "temperature": 0.0, "avg_logprob": -0.11126484590418198, "compression_ratio": 1.6701754385964913, "no_speech_prob": 0.04467069357633591}, {"id": 1246, "seek": 527330, "start": 5292.26, "end": 5298.34, "text": " But also uncertainty is also another one it ties back to novelty because novel environments you will be more uncertain within", "tokens": [51312, 583, 611, 15697, 307, 611, 1071, 472, 309, 14039, 646, 281, 44805, 570, 7613, 12388, 291, 486, 312, 544, 11308, 1951, 51616], "temperature": 0.0, "avg_logprob": -0.11126484590418198, "compression_ratio": 1.6701754385964913, "no_speech_prob": 0.04467069357633591}, {"id": 1247, "seek": 529834, "start": 5298.74, "end": 5303.54, "text": " And so there's fundamentally lots of different sort of branches of these autocurricular that you could use", "tokens": [50384, 400, 370, 456, 311, 17879, 3195, 295, 819, 1333, 295, 14770, 295, 613, 45833, 374, 42750, 300, 291, 727, 764, 50624], "temperature": 0.0, "avg_logprob": -0.14578580441682235, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.004523290321230888}, {"id": 1248, "seek": 529834, "start": 5303.54, "end": 5307.78, "text": " Depending on this search objective that you use to drive this exploration process", "tokens": [50624, 22539, 322, 341, 3164, 10024, 300, 291, 764, 281, 3332, 341, 16197, 1399, 50836], "temperature": 0.0, "avg_logprob": -0.14578580441682235, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.004523290321230888}, {"id": 1249, "seek": 529834, "start": 5308.900000000001, "end": 5312.82, "text": " Can we contrast this to you know, like, um, large language models that they are self-supervised learning", "tokens": [50892, 1664, 321, 8712, 341, 281, 291, 458, 11, 411, 11, 1105, 11, 2416, 2856, 5245, 300, 436, 366, 2698, 12, 48172, 24420, 2539, 51088], "temperature": 0.0, "avg_logprob": -0.14578580441682235, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.004523290321230888}, {"id": 1250, "seek": 529834, "start": 5312.9800000000005, "end": 5318.34, "text": " So, you know, we do this self-supervised objective, you know, which is like, you know, typically predict in the next word", "tokens": [51096, 407, 11, 291, 458, 11, 321, 360, 341, 2698, 12, 48172, 24420, 10024, 11, 291, 458, 11, 597, 307, 411, 11, 291, 458, 11, 5850, 6069, 294, 264, 958, 1349, 51364], "temperature": 0.0, "avg_logprob": -0.14578580441682235, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.004523290321230888}, {"id": 1251, "seek": 529834, "start": 5318.58, "end": 5321.46, "text": " And it's a similar thing with, um, self-supervised, um image", "tokens": [51376, 400, 309, 311, 257, 2531, 551, 365, 11, 1105, 11, 2698, 12, 48172, 24420, 11, 1105, 3256, 51520], "temperature": 0.0, "avg_logprob": -0.14578580441682235, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.004523290321230888}, {"id": 1252, "seek": 532146, "start": 5322.26, "end": 5329.3, "text": " Learning now the difference is with that is you're talking about a principled way of, you know, seeking specific information", "tokens": [50404, 15205, 586, 264, 2649, 307, 365, 300, 307, 291, 434, 1417, 466, 257, 3681, 15551, 636, 295, 11, 291, 458, 11, 11670, 2685, 1589, 50756], "temperature": 0.0, "avg_logprob": -0.10924723188755876, "compression_ratio": 1.8088737201365188, "no_speech_prob": 0.04921736568212509}, {"id": 1253, "seek": 532146, "start": 5329.62, "end": 5334.9, "text": " You know with, um, let's say high entropy and that would lead to an implicit curricula", "tokens": [50772, 509, 458, 365, 11, 1105, 11, 718, 311, 584, 1090, 30867, 293, 300, 576, 1477, 281, 364, 26947, 13179, 3780, 51036], "temperature": 0.0, "avg_logprob": -0.10924723188755876, "compression_ratio": 1.8088737201365188, "no_speech_prob": 0.04921736568212509}, {"id": 1254, "seek": 532146, "start": 5335.22, "end": 5338.9800000000005, "text": " Whereas with language modeling language modeling, there is no implicit curricula", "tokens": [51052, 13813, 365, 2856, 15983, 2856, 15983, 11, 456, 307, 572, 26947, 13179, 3780, 51240], "temperature": 0.0, "avg_logprob": -0.10924723188755876, "compression_ratio": 1.8088737201365188, "no_speech_prob": 0.04921736568212509}, {"id": 1255, "seek": 532146, "start": 5339.22, "end": 5345.78, "text": " But I might argue that there kind of is because the way the model does this continual learning, um, it might regularize itself", "tokens": [51252, 583, 286, 1062, 9695, 300, 456, 733, 295, 307, 570, 264, 636, 264, 2316, 775, 341, 1421, 901, 2539, 11, 1105, 11, 309, 1062, 3890, 1125, 2564, 51580], "temperature": 0.0, "avg_logprob": -0.10924723188755876, "compression_ratio": 1.8088737201365188, "no_speech_prob": 0.04921736568212509}, {"id": 1256, "seek": 532146, "start": 5345.78, "end": 5349.86, "text": " So if you give it sort of surprising and weird information, the language model might just kind of brush it off", "tokens": [51580, 407, 498, 291, 976, 309, 1333, 295, 8830, 293, 3657, 1589, 11, 264, 2856, 2316, 1062, 445, 733, 295, 5287, 309, 766, 51784], "temperature": 0.0, "avg_logprob": -0.10924723188755876, "compression_ratio": 1.8088737201365188, "no_speech_prob": 0.04921736568212509}, {"id": 1257, "seek": 534986, "start": 5350.0199999999995, "end": 5354.5, "text": " And if you reinforce things that it already knows then it's almost like a stream of channels, you know", "tokens": [50372, 400, 498, 291, 22634, 721, 300, 309, 1217, 3255, 550, 309, 311, 1920, 411, 257, 4309, 295, 9235, 11, 291, 458, 50596], "temperature": 0.0, "avg_logprob": -0.11254319548606873, "compression_ratio": 1.7348242811501597, "no_speech_prob": 0.004896217957139015}, {"id": 1258, "seek": 534986, "start": 5354.5, "end": 5358.42, "text": " It'll say, okay, you know go and go and pay attention to that. So it's almost like it's implicit", "tokens": [50596, 467, 603, 584, 11, 1392, 11, 291, 458, 352, 293, 352, 293, 1689, 3202, 281, 300, 13, 407, 309, 311, 1920, 411, 309, 311, 26947, 50792], "temperature": 0.0, "avg_logprob": -0.11254319548606873, "compression_ratio": 1.7348242811501597, "no_speech_prob": 0.004896217957139015}, {"id": 1259, "seek": 534986, "start": 5358.66, "end": 5362.66, "text": " Yeah, and I would say that in some ways it's almost explicit in terms of how we design these systems", "tokens": [50804, 865, 11, 293, 286, 576, 584, 300, 294, 512, 2098, 309, 311, 1920, 13691, 294, 2115, 295, 577, 321, 1715, 613, 3652, 51004], "temperature": 0.0, "avg_logprob": -0.11254319548606873, "compression_ratio": 1.7348242811501597, "no_speech_prob": 0.004896217957139015}, {"id": 1260, "seek": 534986, "start": 5363.219999999999, "end": 5366.58, "text": " A lot of times like if you look at, for example, open ai's job listings", "tokens": [51032, 316, 688, 295, 1413, 411, 498, 291, 574, 412, 11, 337, 1365, 11, 1269, 9783, 311, 1691, 45615, 51200], "temperature": 0.0, "avg_logprob": -0.11254319548606873, "compression_ratio": 1.7348242811501597, "no_speech_prob": 0.004896217957139015}, {"id": 1261, "seek": 534986, "start": 5366.74, "end": 5371.86, "text": " They're actually hiring specifically for experts in different domains to essentially create the next", "tokens": [51208, 814, 434, 767, 15335, 4682, 337, 8572, 294, 819, 25514, 281, 4476, 1884, 264, 958, 51464], "temperature": 0.0, "avg_logprob": -0.11254319548606873, "compression_ratio": 1.7348242811501597, "no_speech_prob": 0.004896217957139015}, {"id": 1262, "seek": 534986, "start": 5372.5, "end": 5376.5, "text": " Batch of supervised data to train or instruction tune their models on", "tokens": [51496, 363, 852, 295, 46533, 1412, 281, 3847, 420, 10951, 10864, 641, 5245, 322, 51696], "temperature": 0.0, "avg_logprob": -0.11254319548606873, "compression_ratio": 1.7348242811501597, "no_speech_prob": 0.004896217957139015}, {"id": 1263, "seek": 537650, "start": 5376.9, "end": 5381.06, "text": " For example, they hire biologists or they hire people with legal expertise to generate this data", "tokens": [50384, 1171, 1365, 11, 436, 11158, 3228, 12256, 420, 436, 11158, 561, 365, 5089, 11769, 281, 8460, 341, 1412, 50592], "temperature": 0.0, "avg_logprob": -0.09546891219324345, "compression_ratio": 1.8200589970501475, "no_speech_prob": 0.014059895649552345}, {"id": 1264, "seek": 537650, "start": 5381.38, "end": 5387.22, "text": " And you can think of this essentially as a human steered or human driven version of this active sampling process, right?", "tokens": [50608, 400, 291, 393, 519, 295, 341, 4476, 382, 257, 1952, 2126, 4073, 420, 1952, 9555, 3037, 295, 341, 4967, 21179, 1399, 11, 558, 30, 50900], "temperature": 0.0, "avg_logprob": -0.09546891219324345, "compression_ratio": 1.8200589970501475, "no_speech_prob": 0.014059895649552345}, {"id": 1265, "seek": 537650, "start": 5387.46, "end": 5394.34, "text": " Because essentially they know that the model tends to get high perplexity or they don't it doesn't perform as well on this domain of tasks", "tokens": [50912, 1436, 4476, 436, 458, 300, 264, 2316, 12258, 281, 483, 1090, 680, 18945, 507, 420, 436, 500, 380, 309, 1177, 380, 2042, 382, 731, 322, 341, 9274, 295, 9608, 51256], "temperature": 0.0, "avg_logprob": -0.09546891219324345, "compression_ratio": 1.8200589970501475, "no_speech_prob": 0.014059895649552345}, {"id": 1266, "seek": 537650, "start": 5394.5, "end": 5400.74, "text": " It doesn't get as high of an LSAT score as it could and so you can essentially, you know, it's it's beyond an algorithm at this point", "tokens": [51264, 467, 1177, 380, 483, 382, 1090, 295, 364, 36657, 2218, 6175, 382, 309, 727, 293, 370, 291, 393, 4476, 11, 291, 458, 11, 309, 311, 309, 311, 4399, 364, 9284, 412, 341, 935, 51576], "temperature": 0.0, "avg_logprob": -0.09546891219324345, "compression_ratio": 1.8200589970501475, "no_speech_prob": 0.014059895649552345}, {"id": 1267, "seek": 537650, "start": 5400.82, "end": 5406.1, "text": " Right, it's kind of the super algorithm where you have the system designers now also being part of the data collection process", "tokens": [51580, 1779, 11, 309, 311, 733, 295, 264, 1687, 9284, 689, 291, 362, 264, 1185, 16196, 586, 611, 885, 644, 295, 264, 1412, 5765, 1399, 51844], "temperature": 0.0, "avg_logprob": -0.09546891219324345, "compression_ratio": 1.8200589970501475, "no_speech_prob": 0.014059895649552345}, {"id": 1268, "seek": 540650, "start": 5406.58, "end": 5408.58, "text": " and in a way", "tokens": [50368, 293, 294, 257, 636, 50468], "temperature": 0.0, "avg_logprob": -0.12304626611562876, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.0007094976026564837}, {"id": 1269, "seek": 540650, "start": 5408.58, "end": 5413.54, "text": " supervised learning is really just sort of one point in a continual learning process where, you know", "tokens": [50468, 46533, 2539, 307, 534, 445, 1333, 295, 472, 935, 294, 257, 1421, 901, 2539, 1399, 689, 11, 291, 458, 50716], "temperature": 0.0, "avg_logprob": -0.12304626611562876, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.0007094976026564837}, {"id": 1270, "seek": 540650, "start": 5414.1, "end": 5418.5, "text": " Classically, we just looked at one step of this which is here's a batch of data train on that but really", "tokens": [50744, 9471, 984, 11, 321, 445, 2956, 412, 472, 1823, 295, 341, 597, 307, 510, 311, 257, 15245, 295, 1412, 3847, 322, 300, 457, 534, 50964], "temperature": 0.0, "avg_logprob": -0.12304626611562876, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.0007094976026564837}, {"id": 1271, "seek": 540650, "start": 5419.38, "end": 5423.46, "text": " Building machine learning systems, especially nowadays. Everything's in production. These are all live systems", "tokens": [51008, 18974, 3479, 2539, 3652, 11, 2318, 13434, 13, 5471, 311, 294, 4265, 13, 1981, 366, 439, 1621, 3652, 51212], "temperature": 0.0, "avg_logprob": -0.12304626611562876, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.0007094976026564837}, {"id": 1272, "seek": 540650, "start": 5423.7, "end": 5427.54, "text": " You have to keep it up to date. You have to keep it continually generalizing to new knowledge", "tokens": [51224, 509, 362, 281, 1066, 309, 493, 281, 4002, 13, 509, 362, 281, 1066, 309, 22277, 2674, 3319, 281, 777, 3601, 51416], "temperature": 0.0, "avg_logprob": -0.12304626611562876, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.0007094976026564837}, {"id": 1273, "seek": 540650, "start": 5428.82, "end": 5436.1, "text": " Like chat gpt or clod or gemini and so really it's sort of this pattern over and over again in sequence where you collect a batch of data", "tokens": [51480, 1743, 5081, 290, 662, 420, 596, 378, 420, 7173, 3812, 293, 370, 534, 309, 311, 1333, 295, 341, 5102, 670, 293, 670, 797, 294, 8310, 689, 291, 2500, 257, 15245, 295, 1412, 51844], "temperature": 0.0, "avg_logprob": -0.12304626611562876, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.0007094976026564837}, {"id": 1274, "seek": 543650, "start": 5436.5, "end": 5438.82, "text": " Train your model on that collect the next batch of data", "tokens": [50364, 28029, 428, 2316, 322, 300, 2500, 264, 958, 15245, 295, 1412, 50480], "temperature": 0.0, "avg_logprob": -0.10272215295025683, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0018098175060003996}, {"id": 1275, "seek": 543650, "start": 5439.62, "end": 5441.62, "text": " You know continue training your model on that", "tokens": [50520, 509, 458, 2354, 3097, 428, 2316, 322, 300, 50620], "temperature": 0.0, "avg_logprob": -0.10272215295025683, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0018098175060003996}, {"id": 1276, "seek": 543650, "start": 5442.18, "end": 5448.66, "text": " And really you want to be selective about what the next batch of data is because obviously if you just retrain it on the previous batch of data", "tokens": [50648, 400, 534, 291, 528, 281, 312, 33930, 466, 437, 264, 958, 15245, 295, 1412, 307, 570, 2745, 498, 291, 445, 1533, 7146, 309, 322, 264, 3894, 15245, 295, 1412, 50972], "temperature": 0.0, "avg_logprob": -0.10272215295025683, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0018098175060003996}, {"id": 1277, "seek": 543650, "start": 5449.22, "end": 5451.06, "text": " It's going to overfit to that data", "tokens": [51000, 467, 311, 516, 281, 670, 6845, 281, 300, 1412, 51092], "temperature": 0.0, "avg_logprob": -0.10272215295025683, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0018098175060003996}, {"id": 1278, "seek": 543650, "start": 5451.06, "end": 5456.98, "text": " Beyond a few epochs or it's not going to you know get as much novel information from it just because it's already trained on it", "tokens": [51092, 19707, 257, 1326, 30992, 28346, 420, 309, 311, 406, 516, 281, 291, 458, 483, 382, 709, 7613, 1589, 490, 309, 445, 570, 309, 311, 1217, 8895, 322, 309, 51388], "temperature": 0.0, "avg_logprob": -0.10272215295025683, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0018098175060003996}, {"id": 1279, "seek": 543650, "start": 5457.22, "end": 5461.14, "text": " So you do want to selectively actively collect the data", "tokens": [51400, 407, 291, 360, 528, 281, 3048, 3413, 13022, 2500, 264, 1412, 51596], "temperature": 0.0, "avg_logprob": -0.10272215295025683, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0018098175060003996}, {"id": 1280, "seek": 543650, "start": 5461.54, "end": 5466.02, "text": " And so I think we kind of almost explicitly already do this at a systems level", "tokens": [51616, 400, 370, 286, 519, 321, 733, 295, 1920, 20803, 1217, 360, 341, 412, 257, 3652, 1496, 51840], "temperature": 0.0, "avg_logprob": -0.10272215295025683, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0018098175060003996}, {"id": 1281, "seek": 546650, "start": 5466.82, "end": 5472.34, "text": " And I think the next frontier is really just having systems that self-improve in this way where they can start to guide", "tokens": [50380, 400, 286, 519, 264, 958, 35853, 307, 534, 445, 1419, 3652, 300, 2698, 12, 332, 46955, 294, 341, 636, 689, 436, 393, 722, 281, 5934, 50656], "temperature": 0.0, "avg_logprob": -0.13124577204386392, "compression_ratio": 1.6767371601208458, "no_speech_prob": 0.00021247818949632347}, {"id": 1282, "seek": 546650, "start": 5472.66, "end": 5478.66, "text": " More of their own active data collection. I love this way of thinking about it. You know like gbt4 is a memetic intelligence", "tokens": [50672, 5048, 295, 641, 1065, 4967, 1412, 5765, 13, 286, 959, 341, 636, 295, 1953, 466, 309, 13, 509, 458, 411, 290, 4517, 19, 307, 257, 1334, 3532, 7599, 50972], "temperature": 0.0, "avg_logprob": -0.13124577204386392, "compression_ratio": 1.6767371601208458, "no_speech_prob": 0.00021247818949632347}, {"id": 1283, "seek": 546650, "start": 5478.66, "end": 5482.02, "text": " It's not just like you know a bunch of weights on on a on a server somewhere", "tokens": [50972, 467, 311, 406, 445, 411, 291, 458, 257, 3840, 295, 17443, 322, 322, 257, 322, 257, 7154, 4079, 51140], "temperature": 0.0, "avg_logprob": -0.13124577204386392, "compression_ratio": 1.6767371601208458, "no_speech_prob": 0.00021247818949632347}, {"id": 1284, "seek": 546650, "start": 5482.42, "end": 5486.02, "text": " And so you could argue, you know, there's this concept called graduate student descent", "tokens": [51160, 400, 370, 291, 727, 9695, 11, 291, 458, 11, 456, 311, 341, 3410, 1219, 8080, 3107, 23475, 51340], "temperature": 0.0, "avg_logprob": -0.13124577204386392, "compression_ratio": 1.6767371601208458, "no_speech_prob": 0.00021247818949632347}, {"id": 1285, "seek": 546650, "start": 5486.02, "end": 5489.94, "text": " Which is what happens in academia or even as you just articulated with open ai", "tokens": [51340, 3013, 307, 437, 2314, 294, 28937, 420, 754, 382, 291, 445, 43322, 365, 1269, 9783, 51536], "temperature": 0.0, "avg_logprob": -0.13124577204386392, "compression_ratio": 1.6767371601208458, "no_speech_prob": 0.00021247818949632347}, {"id": 1286, "seek": 546650, "start": 5489.94, "end": 5493.06, "text": " It's a little bit like an epic mechanical turk right where you know", "tokens": [51536, 467, 311, 257, 707, 857, 411, 364, 13581, 12070, 3243, 74, 558, 689, 291, 458, 51692], "temperature": 0.0, "avg_logprob": -0.13124577204386392, "compression_ratio": 1.6767371601208458, "no_speech_prob": 0.00021247818949632347}, {"id": 1287, "seek": 549306, "start": 5493.780000000001, "end": 5494.900000000001, "text": " They are monitoring the logs", "tokens": [50400, 814, 366, 11028, 264, 20820, 50456], "temperature": 0.0, "avg_logprob": -0.103697056673011, "compression_ratio": 1.80635838150289, "no_speech_prob": 0.0025866725482046604}, {"id": 1288, "seek": 549306, "start": 5494.900000000001, "end": 5500.1, "text": " They know when things go go badly and then they lean into it in the same way you are they go in higher experts", "tokens": [50456, 814, 458, 562, 721, 352, 352, 13425, 293, 550, 436, 11659, 666, 309, 294, 264, 912, 636, 291, 366, 436, 352, 294, 2946, 8572, 50716], "temperature": 0.0, "avg_logprob": -0.103697056673011, "compression_ratio": 1.80635838150289, "no_speech_prob": 0.0025866725482046604}, {"id": 1289, "seek": 549306, "start": 5500.1, "end": 5502.9800000000005, "text": " And they kind of like add more and more data in all of the holes", "tokens": [50716, 400, 436, 733, 295, 411, 909, 544, 293, 544, 1412, 294, 439, 295, 264, 8118, 50860], "temperature": 0.0, "avg_logprob": -0.103697056673011, "compression_ratio": 1.80635838150289, "no_speech_prob": 0.0025866725482046604}, {"id": 1290, "seek": 549306, "start": 5503.14, "end": 5506.18, "text": " And eventually there are no more pockets of like abject failure", "tokens": [50868, 400, 4728, 456, 366, 572, 544, 16491, 295, 411, 410, 1020, 7763, 51020], "temperature": 0.0, "avg_logprob": -0.103697056673011, "compression_ratio": 1.80635838150289, "no_speech_prob": 0.0025866725482046604}, {"id": 1291, "seek": 549306, "start": 5506.18, "end": 5511.46, "text": " It just it just appears to work really well for everyone and people start to say that it's you know, generally intelligent", "tokens": [51020, 467, 445, 309, 445, 7038, 281, 589, 534, 731, 337, 1518, 293, 561, 722, 281, 584, 300, 309, 311, 291, 458, 11, 5101, 13232, 51284], "temperature": 0.0, "avg_logprob": -0.103697056673011, "compression_ratio": 1.80635838150289, "no_speech_prob": 0.0025866725482046604}, {"id": 1292, "seek": 549306, "start": 5511.700000000001, "end": 5514.9800000000005, "text": " So yeah, so there's this interesting systems view of of intelligence", "tokens": [51296, 407, 1338, 11, 370, 456, 311, 341, 1880, 3652, 1910, 295, 295, 7599, 51460], "temperature": 0.0, "avg_logprob": -0.103697056673011, "compression_ratio": 1.80635838150289, "no_speech_prob": 0.0025866725482046604}, {"id": 1293, "seek": 549306, "start": 5515.22, "end": 5517.860000000001, "text": " Yeah, it kind of starts to mimic just the scientific process in a way", "tokens": [51472, 865, 11, 309, 733, 295, 3719, 281, 31075, 445, 264, 8134, 1399, 294, 257, 636, 51604], "temperature": 0.0, "avg_logprob": -0.103697056673011, "compression_ratio": 1.80635838150289, "no_speech_prob": 0.0025866725482046604}, {"id": 1294, "seek": 549306, "start": 5518.42, "end": 5522.26, "text": " Where we're sort of we were putting a lot of hope in the model to basically be able to distill", "tokens": [51632, 2305, 321, 434, 1333, 295, 321, 645, 3372, 257, 688, 295, 1454, 294, 264, 2316, 281, 1936, 312, 1075, 281, 42923, 51824], "temperature": 0.0, "avg_logprob": -0.103697056673011, "compression_ratio": 1.80635838150289, "no_speech_prob": 0.0025866725482046604}, {"id": 1295, "seek": 552226, "start": 5522.820000000001, "end": 5525.7, "text": " information from sort of the net news batch of data that we collect", "tokens": [50392, 1589, 490, 1333, 295, 264, 2533, 2583, 15245, 295, 1412, 300, 321, 2500, 50536], "temperature": 0.0, "avg_logprob": -0.12309146146161841, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0020502672996371984}, {"id": 1296, "seek": 552226, "start": 5526.58, "end": 5528.820000000001, "text": " You know that we know the model currently doesn't explain well", "tokens": [50580, 509, 458, 300, 321, 458, 264, 2316, 4362, 1177, 380, 2903, 731, 50692], "temperature": 0.0, "avg_logprob": -0.12309146146161841, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0020502672996371984}, {"id": 1297, "seek": 552226, "start": 5529.06, "end": 5535.14, "text": " And we we we put a lot of faith and gradient descent in order to basically be able to come up with updates to the weights", "tokens": [50704, 400, 321, 321, 321, 829, 257, 688, 295, 4522, 293, 16235, 23475, 294, 1668, 281, 1936, 312, 1075, 281, 808, 493, 365, 9205, 281, 264, 17443, 51008], "temperature": 0.0, "avg_logprob": -0.12309146146161841, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0020502672996371984}, {"id": 1298, "seek": 552226, "start": 5535.3, "end": 5541.14, "text": " That better explain that data. So we're kind of we're kind of already treating the system as almost like an automated", "tokens": [51016, 663, 1101, 2903, 300, 1412, 13, 407, 321, 434, 733, 295, 321, 434, 733, 295, 1217, 15083, 264, 1185, 382, 1920, 411, 364, 18473, 51308], "temperature": 0.0, "avg_logprob": -0.12309146146161841, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0020502672996371984}, {"id": 1299, "seek": 552226, "start": 5541.860000000001, "end": 5545.14, "text": " Scientist or an automated version of this like continual", "tokens": [51344, 18944, 468, 420, 364, 18473, 3037, 295, 341, 411, 1421, 901, 51508], "temperature": 0.0, "avg_logprob": -0.12309146146161841, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0020502672996371984}, {"id": 1300, "seek": 552226, "start": 5545.860000000001, "end": 5548.42, "text": " process of creating theories and explanations about the world", "tokens": [51544, 1399, 295, 4084, 13667, 293, 28708, 466, 264, 1002, 51672], "temperature": 0.0, "avg_logprob": -0.12309146146161841, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0020502672996371984}, {"id": 1301, "seek": 554842, "start": 5549.3, "end": 5551.22, "text": " But of course, you know", "tokens": [50408, 583, 295, 1164, 11, 291, 458, 50504], "temperature": 0.0, "avg_logprob": -0.1295325133177611, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0014689229428768158}, {"id": 1302, "seek": 554842, "start": 5551.22, "end": 5555.86, "text": " Humans are still much better at language models at doing this or large models at doing this", "tokens": [50504, 35809, 366, 920, 709, 1101, 412, 2856, 5245, 412, 884, 341, 420, 2416, 5245, 412, 884, 341, 50736], "temperature": 0.0, "avg_logprob": -0.1295325133177611, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0014689229428768158}, {"id": 1303, "seek": 554842, "start": 5555.86, "end": 5559.22, "text": " So I do think there clearly seems like a huge gap in terms of", "tokens": [50736, 407, 286, 360, 519, 456, 4448, 2544, 411, 257, 2603, 7417, 294, 2115, 295, 50904], "temperature": 0.0, "avg_logprob": -0.1295325133177611, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0014689229428768158}, {"id": 1304, "seek": 554842, "start": 5559.54, "end": 5564.26, "text": " Well, we still have work that needs to be done in order to build systems that can actually build much more robust theories", "tokens": [50920, 1042, 11, 321, 920, 362, 589, 300, 2203, 281, 312, 1096, 294, 1668, 281, 1322, 3652, 300, 393, 767, 1322, 709, 544, 13956, 13667, 51156], "temperature": 0.0, "avg_logprob": -0.1295325133177611, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0014689229428768158}, {"id": 1305, "seek": 554842, "start": 5565.14, "end": 5568.18, "text": " Based on like net do new data and even seeking that out as humans do", "tokens": [51200, 18785, 322, 411, 2533, 360, 777, 1412, 293, 754, 11670, 300, 484, 382, 6255, 360, 51352], "temperature": 0.0, "avg_logprob": -0.1295325133177611, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0014689229428768158}, {"id": 1306, "seek": 554842, "start": 5568.58, "end": 5574.1, "text": " Interesting and certainly, you know in in this broader memetic intelligence. We are still the sources of agency", "tokens": [51372, 14711, 293, 3297, 11, 291, 458, 294, 294, 341, 13227, 1334, 3532, 7599, 13, 492, 366, 920, 264, 7139, 295, 7934, 51648], "temperature": 0.0, "avg_logprob": -0.1295325133177611, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0014689229428768158}, {"id": 1307, "seek": 557410, "start": 5574.820000000001, "end": 5579.3, "text": " But um, we were just sort of talking a minute ago about there being two types of ai", "tokens": [50400, 583, 1105, 11, 321, 645, 445, 1333, 295, 1417, 257, 3456, 2057, 466, 456, 885, 732, 3467, 295, 9783, 50624], "temperature": 0.0, "avg_logprob": -0.09399904298388269, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0012615870218724012}, {"id": 1308, "seek": 557410, "start": 5579.38, "end": 5582.9800000000005, "text": " You know, there's there's an ai where we are the generating sources of agency", "tokens": [50628, 509, 458, 11, 456, 311, 456, 311, 364, 9783, 689, 321, 366, 264, 17746, 7139, 295, 7934, 50808], "temperature": 0.0, "avg_logprob": -0.09399904298388269, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0012615870218724012}, {"id": 1309, "seek": 557410, "start": 5582.9800000000005, "end": 5587.620000000001, "text": " But there might potentially be another ai in the future where that that is the generating source of agency", "tokens": [50808, 583, 456, 1062, 7263, 312, 1071, 9783, 294, 264, 2027, 689, 300, 300, 307, 264, 17746, 4009, 295, 7934, 51040], "temperature": 0.0, "avg_logprob": -0.09399904298388269, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0012615870218724012}, {"id": 1310, "seek": 557410, "start": 5589.22, "end": 5591.700000000001, "text": " Yeah, I so I think that um", "tokens": [51120, 865, 11, 286, 370, 286, 519, 300, 1105, 51244], "temperature": 0.0, "avg_logprob": -0.09399904298388269, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0012615870218724012}, {"id": 1311, "seek": 557410, "start": 5592.34, "end": 5594.820000000001, "text": " This kind of ties into my my the framework", "tokens": [51276, 639, 733, 295, 14039, 666, 452, 452, 264, 8388, 51400], "temperature": 0.0, "avg_logprob": -0.09399904298388269, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0012615870218724012}, {"id": 1312, "seek": 557410, "start": 5594.820000000001, "end": 5597.38, "text": " I personally used to think about open-ended systems as well", "tokens": [51400, 286, 5665, 1143, 281, 519, 466, 1269, 12, 3502, 3652, 382, 731, 51528], "temperature": 0.0, "avg_logprob": -0.09399904298388269, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0012615870218724012}, {"id": 1313, "seek": 557410, "start": 5598.02, "end": 5603.780000000001, "text": " Where I think that you know at a high level you can you can study ai sort of in silico", "tokens": [51560, 2305, 286, 519, 300, 291, 458, 412, 257, 1090, 1496, 291, 393, 291, 393, 2979, 9783, 1333, 295, 294, 3425, 2789, 51848], "temperature": 0.0, "avg_logprob": -0.09399904298388269, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.0012615870218724012}, {"id": 1314, "seek": 560378, "start": 5603.86, "end": 5610.42, "text": " You can study it in systems that you control that you design and that you try to like have the ai model self-improve within", "tokens": [50368, 509, 393, 2979, 309, 294, 3652, 300, 291, 1969, 300, 291, 1715, 293, 300, 291, 853, 281, 411, 362, 264, 9783, 2316, 2698, 12, 332, 46955, 1951, 50696], "temperature": 0.0, "avg_logprob": -0.1265898659115746, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0012840301496908069}, {"id": 1315, "seek": 560378, "start": 5610.74, "end": 5612.74, "text": " And so you can try to build", "tokens": [50712, 400, 370, 291, 393, 853, 281, 1322, 50812], "temperature": 0.0, "avg_logprob": -0.1265898659115746, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0012840301496908069}, {"id": 1316, "seek": 560378, "start": 5612.98, "end": 5618.099999999999, "text": " Systems that self-improve within silico and that's going to lead to potentially some issues around like the grounding problem", "tokens": [50824, 27059, 300, 2698, 12, 332, 46955, 1951, 3425, 2789, 293, 300, 311, 516, 281, 1477, 281, 7263, 512, 2663, 926, 411, 264, 46727, 1154, 51080], "temperature": 0.0, "avg_logprob": -0.1265898659115746, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0012840301496908069}, {"id": 1317, "seek": 560378, "start": 5618.259999999999, "end": 5624.9, "text": " Where essentially it starts to the auto the auto curricular exploratory process starts to veer into parts pockets of the design space", "tokens": [51088, 2305, 4476, 309, 3719, 281, 264, 8399, 264, 8399, 13179, 1040, 24765, 4745, 1399, 3719, 281, 1241, 260, 666, 3166, 16491, 295, 264, 1715, 1901, 51420], "temperature": 0.0, "avg_logprob": -0.1265898659115746, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0012840301496908069}, {"id": 1318, "seek": 560378, "start": 5624.9, "end": 5627.0599999999995, "text": " That are not relevant to tasks you care about", "tokens": [51420, 663, 366, 406, 7340, 281, 9608, 291, 1127, 466, 51528], "temperature": 0.0, "avg_logprob": -0.1265898659115746, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0012840301496908069}, {"id": 1319, "seek": 560378, "start": 5627.46, "end": 5632.0199999999995, "text": " Um, and so this is kind of the danger of like generating open-ended systems in silico", "tokens": [51548, 3301, 11, 293, 370, 341, 307, 733, 295, 264, 4330, 295, 411, 17746, 1269, 12, 3502, 3652, 294, 3425, 2789, 51776], "temperature": 0.0, "avg_logprob": -0.1265898659115746, "compression_ratio": 1.9052631578947368, "no_speech_prob": 0.0012840301496908069}, {"id": 1320, "seek": 563202, "start": 5632.1, "end": 5635.860000000001, "text": " And I think it's very similar to potential dangers of generating agi in silico", "tokens": [50368, 400, 286, 519, 309, 311, 588, 2531, 281, 3995, 27701, 295, 17746, 623, 72, 294, 3425, 2789, 50556], "temperature": 0.0, "avg_logprob": -0.11660429268829094, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.0035372613929212093}, {"id": 1321, "seek": 563202, "start": 5636.26, "end": 5637.3, "text": " um", "tokens": [50576, 1105, 50628], "temperature": 0.0, "avg_logprob": -0.11660429268829094, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.0035372613929212093}, {"id": 1322, "seek": 563202, "start": 5637.3, "end": 5641.700000000001, "text": " And I think the alternative is really just what are existing intelligent systems", "tokens": [50628, 400, 286, 519, 264, 8535, 307, 534, 445, 437, 366, 6741, 13232, 3652, 50848], "temperature": 0.0, "avg_logprob": -0.11660429268829094, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.0035372613929212093}, {"id": 1323, "seek": 563202, "start": 5642.1, "end": 5647.860000000001, "text": " And how do we actually amplify the efficiency the efficacy of those systems the intelligence within those systems?", "tokens": [50868, 400, 577, 360, 321, 767, 41174, 264, 10493, 264, 33492, 295, 729, 3652, 264, 7599, 1951, 729, 3652, 30, 51156], "temperature": 0.0, "avg_logprob": -0.11660429268829094, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.0035372613929212093}, {"id": 1324, "seek": 563202, "start": 5648.1, "end": 5654.18, "text": " And so you can kind of think of like sort of the entire enterprise of ai research as do we want to generate like ai or intelligence from scratch", "tokens": [51168, 400, 370, 291, 393, 733, 295, 519, 295, 411, 1333, 295, 264, 2302, 14132, 295, 9783, 2132, 382, 360, 321, 528, 281, 8460, 411, 9783, 420, 7599, 490, 8459, 51472], "temperature": 0.0, "avg_logprob": -0.11660429268829094, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.0035372613929212093}, {"id": 1325, "seek": 563202, "start": 5654.5, "end": 5656.5, "text": " Or do we want to build tools?", "tokens": [51488, 1610, 360, 321, 528, 281, 1322, 3873, 30, 51588], "temperature": 0.0, "avg_logprob": -0.11660429268829094, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.0035372613929212093}, {"id": 1326, "seek": 563202, "start": 5656.5, "end": 5661.3, "text": " You know motivated or inspired by human intelligence and other intelligent systems and use that to further amplify", "tokens": [51588, 509, 458, 14515, 420, 7547, 538, 1952, 7599, 293, 661, 13232, 3652, 293, 764, 300, 281, 3052, 41174, 51828], "temperature": 0.0, "avg_logprob": -0.11660429268829094, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.0035372613929212093}, {"id": 1327, "seek": 566130, "start": 5661.62, "end": 5664.26, "text": " existing intelligence like human creativity human intelligence", "tokens": [50380, 6741, 7599, 411, 1952, 12915, 1952, 7599, 50512], "temperature": 0.0, "avg_logprob": -0.09207444441945929, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.00039189725066535175}, {"id": 1328, "seek": 566130, "start": 5664.74, "end": 5668.18, "text": " Could you argue because if intelligence is a divergent search process?", "tokens": [50536, 7497, 291, 9695, 570, 498, 7599, 307, 257, 18558, 6930, 3164, 1399, 30, 50708], "temperature": 0.0, "avg_logprob": -0.09207444441945929, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.00039189725066535175}, {"id": 1329, "seek": 566130, "start": 5668.900000000001, "end": 5672.900000000001, "text": " You might be tempted to think that well if we had loads of tools to help us share", "tokens": [50744, 509, 1062, 312, 29941, 281, 519, 300, 731, 498, 321, 632, 12668, 295, 3873, 281, 854, 505, 2073, 50944], "temperature": 0.0, "avg_logprob": -0.09207444441945929, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.00039189725066535175}, {"id": 1330, "seek": 566130, "start": 5673.22, "end": 5678.5, "text": " The models and help other people discover the models that i've created that that will help us generally be more intelligent", "tokens": [50960, 440, 5245, 293, 854, 661, 561, 4411, 264, 5245, 300, 741, 600, 2942, 300, 300, 486, 854, 505, 5101, 312, 544, 13232, 51224], "temperature": 0.0, "avg_logprob": -0.09207444441945929, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.00039189725066535175}, {"id": 1331, "seek": 566130, "start": 5678.66, "end": 5685.38, "text": " But could you make the counter argument that i'm actually sequestering agency or stealing agency from other people because rather than thinking for themselves", "tokens": [51232, 583, 727, 291, 652, 264, 5682, 6770, 300, 741, 478, 767, 5123, 377, 1794, 7934, 420, 19757, 7934, 490, 661, 561, 570, 2831, 813, 1953, 337, 2969, 51568], "temperature": 0.0, "avg_logprob": -0.09207444441945929, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.00039189725066535175}, {"id": 1332, "seek": 566130, "start": 5685.62, "end": 5688.02, "text": " And discovering novel models. They're just going to use my model", "tokens": [51580, 400, 24773, 7613, 5245, 13, 814, 434, 445, 516, 281, 764, 452, 2316, 51700], "temperature": 0.0, "avg_logprob": -0.09207444441945929, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.00039189725066535175}, {"id": 1333, "seek": 568802, "start": 5688.42, "end": 5693.3, "text": " Yeah, I mean I think that in the best case scenario you're building systems that essentially, you know", "tokens": [50384, 865, 11, 286, 914, 286, 519, 300, 294, 264, 1151, 1389, 9005, 291, 434, 2390, 3652, 300, 4476, 11, 291, 458, 50628], "temperature": 0.0, "avg_logprob": -0.14243218393036813, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.004329885356128216}, {"id": 1334, "seek": 568802, "start": 5694.1, "end": 5700.34, "text": " Not you know to to think about how you know as existing systems nowadays can build on the shoulders of foundation models", "tokens": [50668, 1726, 291, 458, 281, 281, 519, 466, 577, 291, 458, 382, 6741, 3652, 13434, 393, 1322, 322, 264, 10245, 295, 7030, 5245, 50980], "temperature": 0.0, "avg_logprob": -0.14243218393036813, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.004329885356128216}, {"id": 1335, "seek": 568802, "start": 5700.580000000001, "end": 5706.740000000001, "text": " You really want the to build models where even humans can stand on their shoulders where the humans can basically leverage the", "tokens": [50992, 509, 534, 528, 264, 281, 1322, 5245, 689, 754, 6255, 393, 1463, 322, 641, 10245, 689, 264, 6255, 393, 1936, 13982, 264, 51300], "temperature": 0.0, "avg_logprob": -0.14243218393036813, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.004329885356128216}, {"id": 1336, "seek": 568802, "start": 5707.06, "end": 5708.42, "text": " existing expertise or", "tokens": [51316, 6741, 11769, 420, 51384], "temperature": 0.0, "avg_logprob": -0.14243218393036813, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.004329885356128216}, {"id": 1337, "seek": 568802, "start": 5708.42, "end": 5714.18, "text": " Automative capabilities of those models to then like move further beyond what they're naturally capable of doing", "tokens": [51384, 24619, 1166, 10862, 295, 729, 5245, 281, 550, 411, 1286, 3052, 4399, 437, 436, 434, 8195, 8189, 295, 884, 51672], "temperature": 0.0, "avg_logprob": -0.14243218393036813, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.004329885356128216}, {"id": 1338, "seek": 571418, "start": 5714.5, "end": 5718.740000000001, "text": " And really that pushes the frontier of the knowledge that we can create as a civilization", "tokens": [50380, 400, 534, 300, 21020, 264, 35853, 295, 264, 3601, 300, 321, 393, 1884, 382, 257, 18036, 50592], "temperature": 0.0, "avg_logprob": -0.1081239076761099, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0356607548892498}, {"id": 1339, "seek": 571418, "start": 5718.9800000000005, "end": 5726.02, "text": " And so you're already starting to see this where there's some recent studies that show for example like junior software engineers that use systems like", "tokens": [50604, 400, 370, 291, 434, 1217, 2891, 281, 536, 341, 689, 456, 311, 512, 5162, 5313, 300, 855, 337, 1365, 411, 16195, 4722, 11955, 300, 764, 3652, 411, 50956], "temperature": 0.0, "avg_logprob": -0.1081239076761099, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0356607548892498}, {"id": 1340, "seek": 571418, "start": 5726.58, "end": 5728.900000000001, "text": " Chat gpt to help them with coding at work", "tokens": [50984, 27503, 290, 662, 281, 854, 552, 365, 17720, 412, 589, 51100], "temperature": 0.0, "avg_logprob": -0.1081239076761099, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0356607548892498}, {"id": 1341, "seek": 571418, "start": 5729.14, "end": 5732.740000000001, "text": " They actually now are starting to match the performance of more senior engineers", "tokens": [51112, 814, 767, 586, 366, 2891, 281, 2995, 264, 3389, 295, 544, 7965, 11955, 51292], "temperature": 0.0, "avg_logprob": -0.1081239076761099, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0356607548892498}, {"id": 1342, "seek": 571418, "start": 5733.3, "end": 5734.900000000001, "text": " Because it sort of levels the playing field", "tokens": [51320, 1436, 309, 1333, 295, 4358, 264, 2433, 2519, 51400], "temperature": 0.0, "avg_logprob": -0.1081239076761099, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0356607548892498}, {"id": 1343, "seek": 571418, "start": 5735.06, "end": 5740.9800000000005, "text": " But that also translates into just like net more productivity per software engineer. And so", "tokens": [51408, 583, 300, 611, 28468, 666, 445, 411, 2533, 544, 15604, 680, 4722, 11403, 13, 400, 370, 51704], "temperature": 0.0, "avg_logprob": -0.1081239076761099, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0356607548892498}, {"id": 1344, "seek": 574098, "start": 5741.94, "end": 5744.099999999999, "text": " I think that it's more just unlocking sort of", "tokens": [50412, 286, 519, 300, 309, 311, 544, 445, 49620, 1333, 295, 50520], "temperature": 0.0, "avg_logprob": -0.09949561527797154, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.00023410285939462483}, {"id": 1345, "seek": 574098, "start": 5744.74, "end": 5749.94, "text": " Existing bottleneck and how productive each individual can be and really just means that each individual can create a lot more value", "tokens": [50552, 2111, 468, 278, 44641, 547, 293, 577, 13304, 1184, 2609, 393, 312, 293, 534, 445, 1355, 300, 1184, 2609, 393, 1884, 257, 688, 544, 2158, 50812], "temperature": 0.0, "avg_logprob": -0.09949561527797154, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.00023410285939462483}, {"id": 1346, "seek": 574098, "start": 5750.0199999999995, "end": 5752.0199999999995, "text": " Can discover a lot more knowledge", "tokens": [50816, 1664, 4411, 257, 688, 544, 3601, 50916], "temperature": 0.0, "avg_logprob": -0.09949561527797154, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.00023410285939462483}, {"id": 1347, "seek": 574098, "start": 5752.339999999999, "end": 5753.54, "text": " Than before", "tokens": [50932, 18289, 949, 50992], "temperature": 0.0, "avg_logprob": -0.09949561527797154, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.00023410285939462483}, {"id": 1348, "seek": 574098, "start": 5753.54, "end": 5757.78, "text": " Okay, but I mean do you think that it creates a tendency towards boilerplate though", "tokens": [50992, 1033, 11, 457, 286, 914, 360, 291, 519, 300, 309, 7829, 257, 18187, 3030, 39228, 37008, 1673, 51204], "temperature": 0.0, "avg_logprob": -0.09949561527797154, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.00023410285939462483}, {"id": 1349, "seek": 574098, "start": 5757.78, "end": 5760.66, "text": " So we're more we're more efficient at doing things that exist", "tokens": [51204, 407, 321, 434, 544, 321, 434, 544, 7148, 412, 884, 721, 300, 2514, 51348], "temperature": 0.0, "avg_logprob": -0.09949561527797154, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.00023410285939462483}, {"id": 1350, "seek": 574098, "start": 5761.0599999999995, "end": 5764.419999999999, "text": " But you know like on on the frontier we might have a slowdown", "tokens": [51368, 583, 291, 458, 411, 322, 322, 264, 35853, 321, 1062, 362, 257, 2964, 5093, 51536], "temperature": 0.0, "avg_logprob": -0.09949561527797154, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.00023410285939462483}, {"id": 1351, "seek": 574098, "start": 5764.58, "end": 5768.0199999999995, "text": " There's definitely the danger that it can lock you in to certain patterns", "tokens": [51544, 821, 311, 2138, 264, 4330, 300, 309, 393, 4017, 291, 294, 281, 1629, 8294, 51716], "temperature": 0.0, "avg_logprob": -0.09949561527797154, "compression_ratio": 1.738831615120275, "no_speech_prob": 0.00023410285939462483}, {"id": 1352, "seek": 576802, "start": 5768.18, "end": 5772.5, "text": " Right. So basically if chat gpt always returns a certain boilerplate that might have an anti pattern in it", "tokens": [50372, 1779, 13, 407, 1936, 498, 5081, 290, 662, 1009, 11247, 257, 1629, 39228, 37008, 300, 1062, 362, 364, 6061, 5102, 294, 309, 50588], "temperature": 0.0, "avg_logprob": -0.14489542427709548, "compression_ratio": 1.6523178807947019, "no_speech_prob": 0.014953226782381535}, {"id": 1353, "seek": 576802, "start": 5772.820000000001, "end": 5779.700000000001, "text": " Um, if that stays around it could self-amplify and then future generations of programmers might just adopt that by default because it's what's already", "tokens": [50604, 3301, 11, 498, 300, 10834, 926, 309, 727, 2698, 12, 335, 564, 2505, 293, 550, 2027, 10593, 295, 41504, 1062, 445, 6878, 300, 538, 7576, 570, 309, 311, 437, 311, 1217, 50948], "temperature": 0.0, "avg_logprob": -0.14489542427709548, "compression_ratio": 1.6523178807947019, "no_speech_prob": 0.014953226782381535}, {"id": 1354, "seek": 576802, "start": 5780.1, "end": 5784.42, "text": " Generated by autocomplete. So I think that that's also another really interesting realm of questions", "tokens": [50968, 15409, 770, 538, 45833, 298, 17220, 13, 407, 286, 519, 300, 300, 311, 611, 1071, 534, 1880, 15355, 295, 1651, 51184], "temperature": 0.0, "avg_logprob": -0.14489542427709548, "compression_ratio": 1.6523178807947019, "no_speech_prob": 0.014953226782381535}, {"id": 1355, "seek": 576802, "start": 5784.5, "end": 5789.9400000000005, "text": " Which is basically how do you um, how do you avoid these kinds of uh, these local optima?", "tokens": [51188, 3013, 307, 1936, 577, 360, 291, 1105, 11, 577, 360, 291, 5042, 613, 3685, 295, 2232, 11, 613, 2654, 2427, 4775, 30, 51460], "temperature": 0.0, "avg_logprob": -0.14489542427709548, "compression_ratio": 1.6523178807947019, "no_speech_prob": 0.014953226782381535}, {"id": 1356, "seek": 576802, "start": 5790.34, "end": 5793.3, "text": " When you start to train a model on its own outputs", "tokens": [51480, 1133, 291, 722, 281, 3847, 257, 2316, 322, 1080, 1065, 23930, 51628], "temperature": 0.0, "avg_logprob": -0.14489542427709548, "compression_ratio": 1.6523178807947019, "no_speech_prob": 0.014953226782381535}, {"id": 1357, "seek": 579330, "start": 5793.62, "end": 5798.42, "text": " And I think again like sort of the solution will start to look like some form of novelty search or exploration", "tokens": [50380, 400, 286, 519, 797, 411, 1333, 295, 264, 3827, 486, 722, 281, 574, 411, 512, 1254, 295, 44805, 3164, 420, 16197, 50620], "temperature": 0.0, "avg_logprob": -0.11724764700154312, "compression_ratio": 1.778125, "no_speech_prob": 0.010010852478444576}, {"id": 1358, "seek": 579330, "start": 5798.9800000000005, "end": 5803.860000000001, "text": " Makes sense. Okay. Um, what do you guys think about like, um, you know academic academia versus industry and", "tokens": [50648, 25245, 2020, 13, 1033, 13, 3301, 11, 437, 360, 291, 1074, 519, 466, 411, 11, 1105, 11, 291, 458, 7778, 28937, 5717, 3518, 293, 50892], "temperature": 0.0, "avg_logprob": -0.11724764700154312, "compression_ratio": 1.778125, "no_speech_prob": 0.010010852478444576}, {"id": 1359, "seek": 579330, "start": 5804.58, "end": 5808.9800000000005, "text": " Some say there's a bit of a brain drain from academia. Totally. Yeah, I think there's like a very", "tokens": [50928, 2188, 584, 456, 311, 257, 857, 295, 257, 3567, 12339, 490, 28937, 13, 22837, 13, 865, 11, 286, 519, 456, 311, 411, 257, 588, 51148], "temperature": 0.0, "avg_logprob": -0.11724764700154312, "compression_ratio": 1.778125, "no_speech_prob": 0.010010852478444576}, {"id": 1360, "seek": 579330, "start": 5809.7, "end": 5813.78, "text": " Very clear trade-off between the two and they said they both have like fantastic things going for them", "tokens": [51184, 4372, 1850, 4923, 12, 4506, 1296, 264, 732, 293, 436, 848, 436, 1293, 362, 411, 5456, 721, 516, 337, 552, 51388], "temperature": 0.0, "avg_logprob": -0.11724764700154312, "compression_ratio": 1.778125, "no_speech_prob": 0.010010852478444576}, {"id": 1361, "seek": 579330, "start": 5814.26, "end": 5817.06, "text": " And I guess the trade-off being you know academic freedom", "tokens": [51412, 400, 286, 2041, 264, 4923, 12, 4506, 885, 291, 458, 7778, 5645, 51552], "temperature": 0.0, "avg_logprob": -0.11724764700154312, "compression_ratio": 1.778125, "no_speech_prob": 0.010010852478444576}, {"id": 1362, "seek": 579330, "start": 5817.78, "end": 5822.26, "text": " An academia and be able to like individually pursue ideas like purely for curiosity's sake", "tokens": [51588, 1107, 28937, 293, 312, 1075, 281, 411, 16652, 12392, 3487, 411, 17491, 337, 18769, 311, 9717, 51812], "temperature": 0.0, "avg_logprob": -0.11724764700154312, "compression_ratio": 1.778125, "no_speech_prob": 0.010010852478444576}, {"id": 1363, "seek": 582226, "start": 5823.14, "end": 5826.02, "text": " And um, you know, that's something I've really loved about academia", "tokens": [50408, 400, 1105, 11, 291, 458, 11, 300, 311, 746, 286, 600, 534, 4333, 466, 28937, 50552], "temperature": 0.0, "avg_logprob": -0.09449619717068142, "compression_ratio": 1.8146964856230032, "no_speech_prob": 0.0009109780075959861}, {"id": 1364, "seek": 582226, "start": 5826.02, "end": 5831.62, "text": " But I guess you know, I guess the general trend and machine learning research at the moment is kind of towards like larger scale projects", "tokens": [50552, 583, 286, 2041, 291, 458, 11, 286, 2041, 264, 2674, 6028, 293, 3479, 2539, 2132, 412, 264, 1623, 307, 733, 295, 3030, 411, 4833, 4373, 4455, 50832], "temperature": 0.0, "avg_logprob": -0.09449619717068142, "compression_ratio": 1.8146964856230032, "no_speech_prob": 0.0009109780075959861}, {"id": 1365, "seek": 582226, "start": 5831.7, "end": 5832.66, "text": " especially", "tokens": [50836, 2318, 50884], "temperature": 0.0, "avg_logprob": -0.09449619717068142, "compression_ratio": 1.8146964856230032, "no_speech_prob": 0.0009109780075959861}, {"id": 1366, "seek": 582226, "start": 5832.66, "end": 5838.02, "text": " You know a lot of the properties that we might want to see kind of only emerge when you expend a lot of compute and therefore", "tokens": [50884, 509, 458, 257, 688, 295, 264, 7221, 300, 321, 1062, 528, 281, 536, 733, 295, 787, 21511, 562, 291, 24439, 257, 688, 295, 14722, 293, 4412, 51152], "temperature": 0.0, "avg_logprob": -0.09449619717068142, "compression_ratio": 1.8146964856230032, "no_speech_prob": 0.0009109780075959861}, {"id": 1367, "seek": 582226, "start": 5838.900000000001, "end": 5840.820000000001, "text": " You know a lot of interesting research can kind of", "tokens": [51196, 509, 458, 257, 688, 295, 1880, 2132, 393, 733, 295, 51292], "temperature": 0.0, "avg_logprob": -0.09449619717068142, "compression_ratio": 1.8146964856230032, "no_speech_prob": 0.0009109780075959861}, {"id": 1368, "seek": 582226, "start": 5842.18, "end": 5846.18, "text": " Maybe not only be done in an industry, but it's a lot easier to do some kinds of research in industry", "tokens": [51360, 2704, 406, 787, 312, 1096, 294, 364, 3518, 11, 457, 309, 311, 257, 688, 3571, 281, 360, 512, 3685, 295, 2132, 294, 3518, 51560], "temperature": 0.0, "avg_logprob": -0.09449619717068142, "compression_ratio": 1.8146964856230032, "no_speech_prob": 0.0009109780075959861}, {"id": 1369, "seek": 582226, "start": 5846.66, "end": 5849.54, "text": " And so I think this kind of leads this trade-off of do you want freedom?", "tokens": [51584, 400, 370, 286, 519, 341, 733, 295, 6689, 341, 4923, 12, 4506, 295, 360, 291, 528, 5645, 30, 51728], "temperature": 0.0, "avg_logprob": -0.09449619717068142, "compression_ratio": 1.8146964856230032, "no_speech_prob": 0.0009109780075959861}, {"id": 1370, "seek": 584954, "start": 5849.54, "end": 5852.58, "text": " Or do you want to be on these like larger projects that are potentially more impactful?", "tokens": [50364, 1610, 360, 291, 528, 281, 312, 322, 613, 411, 4833, 4455, 300, 366, 7263, 544, 30842, 30, 50516], "temperature": 0.0, "avg_logprob": -0.19647299725076425, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.001410272903740406}, {"id": 1371, "seek": 584954, "start": 5852.98, "end": 5855.7, "text": " And so yeah, I've really struggled with that trade-off. I think they they both", "tokens": [50536, 400, 370, 1338, 11, 286, 600, 534, 19023, 365, 300, 4923, 12, 4506, 13, 286, 519, 436, 436, 1293, 50672], "temperature": 0.0, "avg_logprob": -0.19647299725076425, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.001410272903740406}, {"id": 1372, "seek": 584954, "start": 5856.18, "end": 5861.06, "text": " Have big pros and cons. I don't know what you think minty. Yeah, I I think that um", "tokens": [50696, 3560, 955, 6267, 293, 1014, 13, 286, 500, 380, 458, 437, 291, 519, 18189, 88, 13, 865, 11, 286, 286, 519, 300, 1105, 50940], "temperature": 0.0, "avg_logprob": -0.19647299725076425, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.001410272903740406}, {"id": 1373, "seek": 584954, "start": 5862.18, "end": 5869.7, "text": " Industry is I I think I like at a very like first word rough approximation would be to say that industry focuses much more on", "tokens": [50996, 38178, 307, 286, 286, 519, 286, 411, 412, 257, 588, 411, 700, 1349, 5903, 28023, 576, 312, 281, 584, 300, 3518, 16109, 709, 544, 322, 51372], "temperature": 0.0, "avg_logprob": -0.19647299725076425, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.001410272903740406}, {"id": 1374, "seek": 584954, "start": 5870.18, "end": 5872.18, "text": " um exploitation and", "tokens": [51396, 1105, 33122, 293, 51496], "temperature": 0.0, "avg_logprob": -0.19647299725076425, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.001410272903740406}, {"id": 1375, "seek": 584954, "start": 5872.66, "end": 5876.5, "text": " academia is where you know in principle you should get a lot more exploration", "tokens": [51520, 28937, 307, 689, 291, 458, 294, 8665, 291, 820, 483, 257, 688, 544, 16197, 51712], "temperature": 0.0, "avg_logprob": -0.19647299725076425, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.001410272903740406}, {"id": 1376, "seek": 587650, "start": 5877.46, "end": 5879.46, "text": " But I do think that currently", "tokens": [50412, 583, 286, 360, 519, 300, 4362, 50512], "temperature": 0.0, "avg_logprob": -0.18367580745531165, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0009109512320719659}, {"id": 1377, "seek": 587650, "start": 5879.78, "end": 5880.66, "text": " both", "tokens": [50528, 1293, 50572], "temperature": 0.0, "avg_logprob": -0.18367580745531165, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0009109512320719659}, {"id": 1378, "seek": 587650, "start": 5880.66, "end": 5885.46, "text": " Systems are kind of like entwined in the same sort of reward function at a high level where essentially", "tokens": [50572, 27059, 366, 733, 295, 411, 948, 86, 2001, 294, 264, 912, 1333, 295, 7782, 2445, 412, 257, 1090, 1496, 689, 4476, 50812], "temperature": 0.0, "avg_logprob": -0.18367580745531165, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0009109512320719659}, {"id": 1379, "seek": 587650, "start": 5886.9, "end": 5889.7, "text": " You know if if if you're if you care a lot about", "tokens": [50884, 509, 458, 498, 498, 498, 291, 434, 498, 291, 1127, 257, 688, 466, 51024], "temperature": 0.0, "avg_logprob": -0.18367580745531165, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0009109512320719659}, {"id": 1380, "seek": 587650, "start": 5890.34, "end": 5896.82, "text": " Citations and a short-term greedy algorithm for maximizing citations would be to focus your research efforts on", "tokens": [51056, 18435, 763, 293, 257, 2099, 12, 7039, 28228, 9284, 337, 5138, 3319, 4814, 763, 576, 312, 281, 1879, 428, 2132, 6484, 322, 51380], "temperature": 0.0, "avg_logprob": -0.18367580745531165, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0009109512320719659}, {"id": 1381, "seek": 587650, "start": 5897.54, "end": 5899.54, "text": " sort of whatever topic is", "tokens": [51416, 1333, 295, 2035, 4829, 307, 51516], "temperature": 0.0, "avg_logprob": -0.18367580745531165, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0009109512320719659}, {"id": 1382, "seek": 587650, "start": 5899.86, "end": 5902.18, "text": " Trendy or hyped at the current time", "tokens": [51532, 37417, 88, 420, 43172, 412, 264, 2190, 565, 51648], "temperature": 0.0, "avg_logprob": -0.18367580745531165, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0009109512320719659}, {"id": 1383, "seek": 590218, "start": 5902.26, "end": 5904.900000000001, "text": " And so like I think you see tons of people obviously", "tokens": [50368, 400, 370, 411, 286, 519, 291, 536, 9131, 295, 561, 2745, 50500], "temperature": 0.0, "avg_logprob": -0.10997150529105708, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.007814140059053898}, {"id": 1384, "seek": 590218, "start": 5905.38, "end": 5908.66, "text": " Working on language models partly because it really is a fascinating subject", "tokens": [50524, 18337, 322, 2856, 5245, 17031, 570, 309, 534, 307, 257, 10343, 3983, 50688], "temperature": 0.0, "avg_logprob": -0.10997150529105708, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.007814140059053898}, {"id": 1385, "seek": 590218, "start": 5908.740000000001, "end": 5914.26, "text": " And it really is like the most powerful form of deep learning we have so I understand why everyone's working on it", "tokens": [50692, 400, 309, 534, 307, 411, 264, 881, 4005, 1254, 295, 2452, 2539, 321, 362, 370, 286, 1223, 983, 1518, 311, 1364, 322, 309, 50968], "temperature": 0.0, "avg_logprob": -0.10997150529105708, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.007814140059053898}, {"id": 1386, "seek": 590218, "start": 5914.42, "end": 5916.26, "text": " but I also think that um", "tokens": [50976, 457, 286, 611, 519, 300, 1105, 51068], "temperature": 0.0, "avg_logprob": -0.10997150529105708, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.007814140059053898}, {"id": 1387, "seek": 590218, "start": 5916.26, "end": 5920.18, "text": " A lot of it is kind of you do get this sort of rich gets richer effect around", "tokens": [51068, 316, 688, 295, 309, 307, 733, 295, 291, 360, 483, 341, 1333, 295, 4593, 2170, 29021, 1802, 926, 51264], "temperature": 0.0, "avg_logprob": -0.10997150529105708, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.007814140059053898}, {"id": 1388, "seek": 590218, "start": 5920.58, "end": 5927.14, "text": " Different topics that people tend to gravitate towards and you lose a lot of the exploration that you should otherwise have", "tokens": [51284, 20825, 8378, 300, 561, 3928, 281, 7427, 8086, 3030, 293, 291, 3624, 257, 688, 295, 264, 16197, 300, 291, 820, 5911, 362, 51612], "temperature": 0.0, "avg_logprob": -0.10997150529105708, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.007814140059053898}, {"id": 1389, "seek": 592714, "start": 5927.62, "end": 5928.34, "text": " um", "tokens": [50388, 1105, 50424], "temperature": 0.0, "avg_logprob": -0.1754956841468811, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008710486814379692}, {"id": 1390, "seek": 592714, "start": 5928.34, "end": 5933.860000000001, "text": " And that's partly because you know like both industry and academia are at some level optimizing for a similar", "tokens": [50424, 400, 300, 311, 17031, 570, 291, 458, 411, 1293, 3518, 293, 28937, 366, 412, 512, 1496, 40425, 337, 257, 2531, 50700], "temperature": 0.0, "avg_logprob": -0.1754956841468811, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008710486814379692}, {"id": 1391, "seek": 592714, "start": 5935.46, "end": 5938.02, "text": " Sort of reputational status or citation count sort of metric", "tokens": [50780, 26149, 295, 1085, 325, 1478, 6558, 420, 45590, 1207, 1333, 295, 20678, 50908], "temperature": 0.0, "avg_logprob": -0.1754956841468811, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008710486814379692}, {"id": 1392, "seek": 592714, "start": 5939.06, "end": 5942.740000000001, "text": " And so I think that's an issue, but I also think that in some ways", "tokens": [50960, 400, 370, 286, 519, 300, 311, 364, 2734, 11, 457, 286, 611, 519, 300, 294, 512, 2098, 51144], "temperature": 0.0, "avg_logprob": -0.1754956841468811, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008710486814379692}, {"id": 1393, "seek": 592714, "start": 5943.14, "end": 5945.62, "text": " Uh industry you could say has", "tokens": [51164, 4019, 3518, 291, 727, 584, 575, 51288], "temperature": 0.0, "avg_logprob": -0.1754956841468811, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008710486814379692}, {"id": 1394, "seek": 592714, "start": 5946.34, "end": 5951.3, "text": " Additional benefit where I do think that from like a short-term point of view industry is better poised to", "tokens": [51324, 44272, 5121, 689, 286, 360, 519, 300, 490, 411, 257, 2099, 12, 7039, 935, 295, 1910, 3518, 307, 1101, 714, 2640, 281, 51572], "temperature": 0.0, "avg_logprob": -0.1754956841468811, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008710486814379692}, {"id": 1395, "seek": 592714, "start": 5952.02, "end": 5953.700000000001, "text": " make certain", "tokens": [51608, 652, 1629, 51692], "temperature": 0.0, "avg_logprob": -0.1754956841468811, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.008710486814379692}, {"id": 1396, "seek": 595370, "start": 5953.7, "end": 5959.22, "text": " Higher impact research not just because of the resources available to industry, but also partly because", "tokens": [50364, 31997, 2712, 2132, 406, 445, 570, 295, 264, 3593, 2435, 281, 3518, 11, 457, 611, 17031, 570, 50640], "temperature": 0.0, "avg_logprob": -0.1578324472802317, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0034826688934117556}, {"id": 1397, "seek": 595370, "start": 5959.78, "end": 5961.139999999999, "text": " um", "tokens": [50668, 1105, 50736], "temperature": 0.0, "avg_logprob": -0.1578324472802317, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0034826688934117556}, {"id": 1398, "seek": 595370, "start": 5961.139999999999, "end": 5963.46, "text": " Sort of industry, uh, you know", "tokens": [50736, 26149, 295, 3518, 11, 2232, 11, 291, 458, 50852], "temperature": 0.0, "avg_logprob": -0.1578324472802317, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0034826688934117556}, {"id": 1399, "seek": 595370, "start": 5963.46, "end": 5966.66, "text": " Rides or dies based on whether the actual research artifact you produce", "tokens": [50852, 497, 1875, 420, 2714, 2361, 322, 1968, 264, 3539, 2132, 34806, 291, 5258, 51012], "temperature": 0.0, "avg_logprob": -0.1578324472802317, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0034826688934117556}, {"id": 1400, "seek": 595370, "start": 5967.38, "end": 5973.3, "text": " Is useful and so I think that's like a very powerful reward function that is not necessarily true for academia", "tokens": [51048, 1119, 4420, 293, 370, 286, 519, 300, 311, 411, 257, 588, 4005, 7782, 2445, 300, 307, 406, 4725, 2074, 337, 28937, 51344], "temperature": 0.0, "avg_logprob": -0.1578324472802317, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0034826688934117556}, {"id": 1401, "seek": 595370, "start": 5973.54, "end": 5976.34, "text": " Um, and then sort of on the to take the counter position", "tokens": [51356, 3301, 11, 293, 550, 1333, 295, 322, 264, 281, 747, 264, 5682, 2535, 51496], "temperature": 0.0, "avg_logprob": -0.1578324472802317, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0034826688934117556}, {"id": 1402, "seek": 595370, "start": 5976.34, "end": 5982.0199999999995, "text": " I think academia obviously, you know, you have a lot more freedom to just explore ideas that don't need to be on that critical path", "tokens": [51496, 286, 519, 28937, 2745, 11, 291, 458, 11, 291, 362, 257, 688, 544, 5645, 281, 445, 6839, 3487, 300, 500, 380, 643, 281, 312, 322, 300, 4924, 3100, 51780], "temperature": 0.0, "avg_logprob": -0.1578324472802317, "compression_ratio": 1.7254237288135594, "no_speech_prob": 0.0034826688934117556}, {"id": 1403, "seek": 598202, "start": 5982.02, "end": 5986.660000000001, "text": " For value creation immediately and so it gives you a lot more scope to potentially find like the next big thing", "tokens": [50364, 1171, 2158, 8016, 4258, 293, 370, 309, 2709, 291, 257, 688, 544, 11923, 281, 7263, 915, 411, 264, 958, 955, 551, 50596], "temperature": 0.0, "avg_logprob": -0.12221221382736314, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.010005774907767773}, {"id": 1404, "seek": 598202, "start": 5987.06, "end": 5991.540000000001, "text": " And so I think really it's about like if you want to if you want to take the bet that you can", "tokens": [50616, 400, 370, 286, 519, 534, 309, 311, 466, 411, 498, 291, 528, 281, 498, 291, 528, 281, 747, 264, 778, 300, 291, 393, 50840], "temperature": 0.0, "avg_logprob": -0.12221221382736314, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.010005774907767773}, {"id": 1405, "seek": 598202, "start": 5992.18, "end": 5994.26, "text": " You know play a part in discovering the next big thing", "tokens": [50872, 509, 458, 862, 257, 644, 294, 24773, 264, 958, 955, 551, 50976], "temperature": 0.0, "avg_logprob": -0.12221221382736314, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.010005774907767773}, {"id": 1406, "seek": 598202, "start": 5994.740000000001, "end": 5998.660000000001, "text": " Then and that's that's suited to your taste for research then academia makes more sense", "tokens": [51000, 1396, 293, 300, 311, 300, 311, 24736, 281, 428, 3939, 337, 2132, 550, 28937, 1669, 544, 2020, 51196], "temperature": 0.0, "avg_logprob": -0.12221221382736314, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.010005774907767773}, {"id": 1407, "seek": 598202, "start": 5999.06, "end": 6001.06, "text": " but if you know, um, you want to", "tokens": [51216, 457, 498, 291, 458, 11, 1105, 11, 291, 528, 281, 51316], "temperature": 0.0, "avg_logprob": -0.12221221382736314, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.010005774907767773}, {"id": 1408, "seek": 598202, "start": 6001.620000000001, "end": 6006.580000000001, "text": " You want to maximize the probability you'll have a higher impact in sort of like a near horizon line of work", "tokens": [51344, 509, 528, 281, 19874, 264, 8482, 291, 603, 362, 257, 2946, 2712, 294, 1333, 295, 411, 257, 2651, 18046, 1622, 295, 589, 51592], "temperature": 0.0, "avg_logprob": -0.12221221382736314, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.010005774907767773}, {"id": 1409, "seek": 598202, "start": 6006.820000000001, "end": 6011.780000000001, "text": " Then industry is definitely I think a better bet rich Sutton, you know, he had this bitter lesson essay", "tokens": [51604, 1396, 3518, 307, 2138, 286, 519, 257, 1101, 778, 4593, 40492, 1756, 11, 291, 458, 11, 415, 632, 341, 13871, 6898, 16238, 51852], "temperature": 0.0, "avg_logprob": -0.12221221382736314, "compression_ratio": 1.8109756097560976, "no_speech_prob": 0.010005774907767773}, {"id": 1410, "seek": 601202, "start": 6012.1, "end": 6017.22, "text": " And he made the argument that it's just all computation and there are no shortcuts and you can even think of you know", "tokens": [50368, 400, 415, 1027, 264, 6770, 300, 309, 311, 445, 439, 24903, 293, 456, 366, 572, 34620, 293, 291, 393, 754, 519, 295, 291, 458, 50624], "temperature": 0.0, "avg_logprob": -0.11123557950629563, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.0008362349472008646}, {"id": 1411, "seek": 601202, "start": 6017.780000000001, "end": 6019.780000000001, "text": " Maybe we're not very intelligent", "tokens": [50652, 2704, 321, 434, 406, 588, 13232, 50752], "temperature": 0.0, "avg_logprob": -0.11123557950629563, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.0008362349472008646}, {"id": 1412, "seek": 601202, "start": 6019.860000000001, "end": 6024.820000000001, "text": " Evolution has just been running for a very very long time and we are the result of that", "tokens": [50756, 40800, 575, 445, 668, 2614, 337, 257, 588, 588, 938, 565, 293, 321, 366, 264, 1874, 295, 300, 51004], "temperature": 0.0, "avg_logprob": -0.11123557950629563, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.0008362349472008646}, {"id": 1413, "seek": 601202, "start": 6025.06, "end": 6030.18, "text": " So in in a sense, do you think that we could make strides in intelligence?", "tokens": [51016, 407, 294, 294, 257, 2020, 11, 360, 291, 519, 300, 321, 727, 652, 1056, 1875, 294, 7599, 30, 51272], "temperature": 0.0, "avg_logprob": -0.11123557950629563, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.0008362349472008646}, {"id": 1414, "seek": 601202, "start": 6030.580000000001, "end": 6034.900000000001, "text": " You know just through ingenuity or are we always going to need loads of computer power?", "tokens": [51292, 509, 458, 445, 807, 21600, 21757, 420, 366, 321, 1009, 516, 281, 643, 12668, 295, 3820, 1347, 30, 51508], "temperature": 0.0, "avg_logprob": -0.11123557950629563, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.0008362349472008646}, {"id": 1415, "seek": 601202, "start": 6036.26, "end": 6041.22, "text": " This definitely like makes me think of like the recent trend that we've been seeing even in like kind of the reinforcement learning", "tokens": [51576, 639, 2138, 411, 1669, 385, 519, 295, 411, 264, 5162, 6028, 300, 321, 600, 668, 2577, 754, 294, 411, 733, 295, 264, 29280, 2539, 51824], "temperature": 0.0, "avg_logprob": -0.11123557950629563, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.0008362349472008646}, {"id": 1416, "seek": 604122, "start": 6041.22, "end": 6043.780000000001, "text": " Literature lately, which is like these kind of large scale", "tokens": [50364, 16090, 1503, 12881, 11, 597, 307, 411, 613, 733, 295, 2416, 4373, 50492], "temperature": 0.0, "avg_logprob": -0.1454287841256741, "compression_ratio": 1.815884476534296, "no_speech_prob": 0.0012062618043273687}, {"id": 1417, "seek": 604122, "start": 6044.900000000001, "end": 6050.740000000001, "text": " Like mostly industry projects that are kind of they're even ditching the idea of doing like sequential decision making so", "tokens": [50548, 1743, 5240, 3518, 4455, 300, 366, 733, 295, 436, 434, 754, 25325, 278, 264, 1558, 295, 884, 411, 42881, 3537, 1455, 370, 50840], "temperature": 0.0, "avg_logprob": -0.1454287841256741, "compression_ratio": 1.815884476534296, "no_speech_prob": 0.0012062618043273687}, {"id": 1418, "seek": 604122, "start": 6051.860000000001, "end": 6055.7, "text": " You know you have all these algorithms that are like, you know optimal planning and so forth", "tokens": [50896, 509, 458, 291, 362, 439, 613, 14642, 300, 366, 411, 11, 291, 458, 16252, 5038, 293, 370, 5220, 51088], "temperature": 0.0, "avg_logprob": -0.1454287841256741, "compression_ratio": 1.815884476534296, "no_speech_prob": 0.0012062618043273687}, {"id": 1419, "seek": 604122, "start": 6056.02, "end": 6058.02, "text": " But we're kind of seeing", "tokens": [51104, 583, 321, 434, 733, 295, 2577, 51204], "temperature": 0.0, "avg_logprob": -0.1454287841256741, "compression_ratio": 1.815884476534296, "no_speech_prob": 0.0012062618043273687}, {"id": 1420, "seek": 604122, "start": 6058.02, "end": 6063.9400000000005, "text": " A trend towards you know, even ditching that complexity of algorithm and just going straight to just copy what the human did", "tokens": [51204, 316, 6028, 3030, 291, 458, 11, 754, 25325, 278, 300, 14024, 295, 9284, 293, 445, 516, 2997, 281, 445, 5055, 437, 264, 1952, 630, 51500], "temperature": 0.0, "avg_logprob": -0.1454287841256741, "compression_ratio": 1.815884476534296, "no_speech_prob": 0.0012062618043273687}, {"id": 1421, "seek": 604122, "start": 6064.26, "end": 6066.740000000001, "text": " and so kind of reducing the problem to you know", "tokens": [51516, 293, 370, 733, 295, 12245, 264, 1154, 281, 291, 458, 51640], "temperature": 0.0, "avg_logprob": -0.1454287841256741, "compression_ratio": 1.815884476534296, "no_speech_prob": 0.0012062618043273687}, {"id": 1422, "seek": 604122, "start": 6067.62, "end": 6069.62, "text": " essentially no real algorithmic", "tokens": [51684, 4476, 572, 957, 9284, 299, 51784], "temperature": 0.0, "avg_logprob": -0.1454287841256741, "compression_ratio": 1.815884476534296, "no_speech_prob": 0.0012062618043273687}, {"id": 1423, "seek": 606962, "start": 6070.34, "end": 6073.22, "text": " Innovation and more just like can you gather enough expert data?", "tokens": [50400, 27092, 293, 544, 445, 411, 393, 291, 5448, 1547, 5844, 1412, 30, 50544], "temperature": 0.0, "avg_logprob": -0.10088494666536948, "compression_ratio": 1.8882175226586102, "no_speech_prob": 0.0017542990390211344}, {"id": 1424, "seek": 606962, "start": 6073.7, "end": 6076.58, "text": " And I think yeah, I guess the reason why that trend is occurring is", "tokens": [50568, 400, 286, 519, 1338, 11, 286, 2041, 264, 1778, 983, 300, 6028, 307, 18386, 307, 50712], "temperature": 0.0, "avg_logprob": -0.10088494666536948, "compression_ratio": 1.8882175226586102, "no_speech_prob": 0.0017542990390211344}, {"id": 1425, "seek": 606962, "start": 6077.14, "end": 6079.14, "text": " Is I guess like you said there's kind of been", "tokens": [50740, 1119, 286, 2041, 411, 291, 848, 456, 311, 733, 295, 668, 50840], "temperature": 0.0, "avg_logprob": -0.10088494666536948, "compression_ratio": 1.8882175226586102, "no_speech_prob": 0.0017542990390211344}, {"id": 1426, "seek": 606962, "start": 6079.7, "end": 6082.0199999999995, "text": " You know the bit lesson kind of said that you know", "tokens": [50868, 509, 458, 264, 857, 6898, 733, 295, 848, 300, 291, 458, 50984], "temperature": 0.0, "avg_logprob": -0.10088494666536948, "compression_ratio": 1.8882175226586102, "no_speech_prob": 0.0017542990390211344}, {"id": 1427, "seek": 606962, "start": 6082.42, "end": 6086.18, "text": " Just being able to scale with more data and more compute is kind of the most important thing", "tokens": [51004, 1449, 885, 1075, 281, 4373, 365, 544, 1412, 293, 544, 14722, 307, 733, 295, 264, 881, 1021, 551, 51192], "temperature": 0.0, "avg_logprob": -0.10088494666536948, "compression_ratio": 1.8882175226586102, "no_speech_prob": 0.0017542990390211344}, {"id": 1428, "seek": 606962, "start": 6086.66, "end": 6091.54, "text": " And a lot of the more complex algorithms, especially around like reinforcement learning are actually like quite challenging to scale up", "tokens": [51216, 400, 257, 688, 295, 264, 544, 3997, 14642, 11, 2318, 926, 411, 29280, 2539, 366, 767, 411, 1596, 7595, 281, 4373, 493, 51460], "temperature": 0.0, "avg_logprob": -0.10088494666536948, "compression_ratio": 1.8882175226586102, "no_speech_prob": 0.0017542990390211344}, {"id": 1429, "seek": 606962, "start": 6092.0199999999995, "end": 6094.98, "text": " especially like online reinforcement learning if you want to go out and like", "tokens": [51484, 2318, 411, 2950, 29280, 2539, 498, 291, 528, 281, 352, 484, 293, 411, 51632], "temperature": 0.0, "avg_logprob": -0.10088494666536948, "compression_ratio": 1.8882175226586102, "no_speech_prob": 0.0017542990390211344}, {"id": 1430, "seek": 606962, "start": 6095.78, "end": 6099.22, "text": " Actually have an agent like actively collecting data in a bunch of different environments", "tokens": [51672, 5135, 362, 364, 9461, 411, 13022, 12510, 1412, 294, 257, 3840, 295, 819, 12388, 51844], "temperature": 0.0, "avg_logprob": -0.10088494666536948, "compression_ratio": 1.8882175226586102, "no_speech_prob": 0.0017542990390211344}, {"id": 1431, "seek": 609962, "start": 6099.62, "end": 6103.14, "text": " And updating itself online like that's so much like engineering infrastructure to set up", "tokens": [50364, 400, 25113, 2564, 2950, 411, 300, 311, 370, 709, 411, 7043, 6896, 281, 992, 493, 50540], "temperature": 0.0, "avg_logprob": -0.07898195399794468, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.00010229476902168244}, {"id": 1432, "seek": 609962, "start": 6103.54, "end": 6106.9, "text": " And so I think there's this this trend towards just like the simplest algorithm possible", "tokens": [50560, 400, 370, 286, 519, 456, 311, 341, 341, 6028, 3030, 445, 411, 264, 22811, 9284, 1944, 50728], "temperature": 0.0, "avg_logprob": -0.07898195399794468, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.00010229476902168244}, {"id": 1433, "seek": 609962, "start": 6106.9, "end": 6111.22, "text": " Which is like not even reinforcement learning not even planning just copy an expert", "tokens": [50728, 3013, 307, 411, 406, 754, 29280, 2539, 406, 754, 5038, 445, 5055, 364, 5844, 50944], "temperature": 0.0, "avg_logprob": -0.07898195399794468, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.00010229476902168244}, {"id": 1434, "seek": 609962, "start": 6111.7, "end": 6113.46, "text": " but I think that", "tokens": [50968, 457, 286, 519, 300, 51056], "temperature": 0.0, "avg_logprob": -0.07898195399794468, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.00010229476902168244}, {"id": 1435, "seek": 609962, "start": 6113.46, "end": 6117.46, "text": " That's like you kind of said earlier with like this kind of like short-term exploitation", "tokens": [51056, 663, 311, 411, 291, 733, 295, 848, 3071, 365, 411, 341, 733, 295, 411, 2099, 12, 7039, 33122, 51256], "temperature": 0.0, "avg_logprob": -0.07898195399794468, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.00010229476902168244}, {"id": 1436, "seek": 609962, "start": 6117.94, "end": 6122.34, "text": " I think this is you know, it it kind of makes sense to exploit this now and push it as far as possible because", "tokens": [51280, 286, 519, 341, 307, 291, 458, 11, 309, 309, 733, 295, 1669, 2020, 281, 25924, 341, 586, 293, 2944, 309, 382, 1400, 382, 1944, 570, 51500], "temperature": 0.0, "avg_logprob": -0.07898195399794468, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.00010229476902168244}, {"id": 1437, "seek": 609962, "start": 6122.66, "end": 6126.9, "text": " You know, it's very easy to just train a large transformer and then gather as much data as possible", "tokens": [51516, 509, 458, 11, 309, 311, 588, 1858, 281, 445, 3847, 257, 2416, 31782, 293, 550, 5448, 382, 709, 1412, 382, 1944, 51728], "temperature": 0.0, "avg_logprob": -0.07898195399794468, "compression_ratio": 1.8407643312101911, "no_speech_prob": 0.00010229476902168244}, {"id": 1438, "seek": 612690, "start": 6127.219999999999, "end": 6130.98, "text": " And I think in areas like robotics, we haven't really seen like how far can that go like", "tokens": [50380, 400, 286, 519, 294, 3179, 411, 34145, 11, 321, 2378, 380, 534, 1612, 411, 577, 1400, 393, 300, 352, 411, 50568], "temperature": 0.0, "avg_logprob": -0.08933210372924805, "compression_ratio": 1.853731343283582, "no_speech_prob": 0.0032727099023759365}, {"id": 1439, "seek": 612690, "start": 6131.299999999999, "end": 6137.78, "text": " Can you actually get a generally useful robotics platform just by gathering more expert demonstrations and training a larger and larger transformer?", "tokens": [50584, 1664, 291, 767, 483, 257, 5101, 4420, 34145, 3663, 445, 538, 13519, 544, 5844, 34714, 293, 3097, 257, 4833, 293, 4833, 31782, 30, 50908], "temperature": 0.0, "avg_logprob": -0.08933210372924805, "compression_ratio": 1.853731343283582, "no_speech_prob": 0.0032727099023759365}, {"id": 1440, "seek": 612690, "start": 6138.099999999999, "end": 6142.98, "text": " And so I think it does kind of make sense that why like a lot of industry projects are pursuing that because we don't really know", "tokens": [50924, 400, 370, 286, 519, 309, 775, 733, 295, 652, 2020, 300, 983, 411, 257, 688, 295, 3518, 4455, 366, 20222, 300, 570, 321, 500, 380, 534, 458, 51168], "temperature": 0.0, "avg_logprob": -0.08933210372924805, "compression_ratio": 1.853731343283582, "no_speech_prob": 0.0032727099023759365}, {"id": 1441, "seek": 612690, "start": 6143.379999999999, "end": 6147.379999999999, "text": " You know, will will that actually hit a bottleneck or or if you just gather enough data", "tokens": [51188, 509, 458, 11, 486, 486, 300, 767, 2045, 257, 44641, 547, 420, 420, 498, 291, 445, 5448, 1547, 1412, 51388], "temperature": 0.0, "avg_logprob": -0.08933210372924805, "compression_ratio": 1.853731343283582, "no_speech_prob": 0.0032727099023759365}, {"id": 1442, "seek": 612690, "start": 6147.78, "end": 6149.54, "text": " Will that will that kind of be sufficient?", "tokens": [51408, 3099, 300, 486, 300, 733, 295, 312, 11563, 30, 51496], "temperature": 0.0, "avg_logprob": -0.08933210372924805, "compression_ratio": 1.853731343283582, "no_speech_prob": 0.0032727099023759365}, {"id": 1443, "seek": 612690, "start": 6150.099999999999, "end": 6151.299999999999, "text": " And I guess like", "tokens": [51524, 400, 286, 2041, 411, 51584], "temperature": 0.0, "avg_logprob": -0.08933210372924805, "compression_ratio": 1.853731343283582, "no_speech_prob": 0.0032727099023759365}, {"id": 1444, "seek": 612690, "start": 6151.299999999999, "end": 6155.46, "text": " You know, you could argue that I think it's probably true that there must be a better algorithm out there", "tokens": [51584, 509, 458, 11, 291, 727, 9695, 300, 286, 519, 309, 311, 1391, 2074, 300, 456, 1633, 312, 257, 1101, 9284, 484, 456, 51792], "temperature": 0.0, "avg_logprob": -0.08933210372924805, "compression_ratio": 1.853731343283582, "no_speech_prob": 0.0032727099023759365}, {"id": 1445, "seek": 615546, "start": 6155.7, "end": 6157.86, "text": " That can and principle do this in a more efficient way", "tokens": [50376, 663, 393, 293, 8665, 360, 341, 294, 257, 544, 7148, 636, 50484], "temperature": 0.0, "avg_logprob": -0.09233603989782412, "compression_ratio": 1.8489208633093526, "no_speech_prob": 0.0016483016079291701}, {"id": 1446, "seek": 615546, "start": 6158.34, "end": 6162.1, "text": " But I guess if it's just easier to just gather more data and just do imitation learning", "tokens": [50508, 583, 286, 2041, 498, 309, 311, 445, 3571, 281, 445, 5448, 544, 1412, 293, 445, 360, 47624, 2539, 50696], "temperature": 0.0, "avg_logprob": -0.09233603989782412, "compression_ratio": 1.8489208633093526, "no_speech_prob": 0.0016483016079291701}, {"id": 1447, "seek": 615546, "start": 6162.18, "end": 6164.66, "text": " I can see that there's at least a business case for trying that", "tokens": [50700, 286, 393, 536, 300, 456, 311, 412, 1935, 257, 1606, 1389, 337, 1382, 300, 50824], "temperature": 0.0, "avg_logprob": -0.09233603989782412, "compression_ratio": 1.8489208633093526, "no_speech_prob": 0.0016483016079291701}, {"id": 1448, "seek": 615546, "start": 6165.54, "end": 6171.46, "text": " So I guess I'm on the the opinion of like, you know, there must be a more efficient way of getting to like a more intelligent system", "tokens": [50868, 407, 286, 2041, 286, 478, 322, 264, 264, 4800, 295, 411, 11, 291, 458, 11, 456, 1633, 312, 257, 544, 7148, 636, 295, 1242, 281, 411, 257, 544, 13232, 1185, 51164], "temperature": 0.0, "avg_logprob": -0.09233603989782412, "compression_ratio": 1.8489208633093526, "no_speech_prob": 0.0016483016079291701}, {"id": 1449, "seek": 615546, "start": 6171.78, "end": 6176.66, "text": " But it's not necessarily clear that just scaling like raw supervised learning or unsupervised learning", "tokens": [51180, 583, 309, 311, 406, 4725, 1850, 300, 445, 21589, 411, 8936, 46533, 2539, 420, 2693, 12879, 24420, 2539, 51424], "temperature": 0.0, "avg_logprob": -0.09233603989782412, "compression_ratio": 1.8489208633093526, "no_speech_prob": 0.0016483016079291701}, {"id": 1450, "seek": 615546, "start": 6176.9800000000005, "end": 6180.18, "text": " Like won't get you there and so it does make sense to pursue that first", "tokens": [51440, 1743, 1582, 380, 483, 291, 456, 293, 370, 309, 775, 652, 2020, 281, 12392, 300, 700, 51600], "temperature": 0.0, "avg_logprob": -0.09233603989782412, "compression_ratio": 1.8489208633093526, "no_speech_prob": 0.0016483016079291701}, {"id": 1451, "seek": 618018, "start": 6180.42, "end": 6185.780000000001, "text": " But kind of what I hope and expect to see is that eventually pure imitation learning or pure unsupervised learning will kind of", "tokens": [50376, 583, 733, 295, 437, 286, 1454, 293, 2066, 281, 536, 307, 300, 4728, 6075, 47624, 2539, 420, 6075, 2693, 12879, 24420, 2539, 486, 733, 295, 50644], "temperature": 0.0, "avg_logprob": -0.10193493117147417, "compression_ratio": 1.7928994082840237, "no_speech_prob": 0.050293441861867905}, {"id": 1452, "seek": 618018, "start": 6186.18, "end": 6188.9800000000005, "text": " Run out of steam and everything will plateau and I think at that point", "tokens": [50664, 8950, 484, 295, 11952, 293, 1203, 486, 39885, 293, 286, 519, 412, 300, 935, 50804], "temperature": 0.0, "avg_logprob": -0.10193493117147417, "compression_ratio": 1.7928994082840237, "no_speech_prob": 0.050293441861867905}, {"id": 1453, "seek": 618018, "start": 6189.46, "end": 6194.02, "text": " You know, then these like more complicated algorithms about gathering more data reinforcement learning planning, etc", "tokens": [50828, 509, 458, 11, 550, 613, 411, 544, 6179, 14642, 466, 13519, 544, 1412, 29280, 2539, 5038, 11, 5183, 51056], "temperature": 0.0, "avg_logprob": -0.10193493117147417, "compression_ratio": 1.7928994082840237, "no_speech_prob": 0.050293441861867905}, {"id": 1454, "seek": 618018, "start": 6194.02, "end": 6195.62, "text": " Will really come into their own", "tokens": [51056, 3099, 534, 808, 666, 641, 1065, 51136], "temperature": 0.0, "avg_logprob": -0.10193493117147417, "compression_ratio": 1.7928994082840237, "no_speech_prob": 0.050293441861867905}, {"id": 1455, "seek": 618018, "start": 6195.62, "end": 6198.820000000001, "text": " And so I guess this again relates back to like the academia industry trade-off like, you know", "tokens": [51136, 400, 370, 286, 2041, 341, 797, 16155, 646, 281, 411, 264, 28937, 3518, 4923, 12, 4506, 411, 11, 291, 458, 51296], "temperature": 0.0, "avg_logprob": -0.10193493117147417, "compression_ratio": 1.7928994082840237, "no_speech_prob": 0.050293441861867905}, {"id": 1456, "seek": 618018, "start": 6198.820000000001, "end": 6202.26, "text": " A lot of the projects in the industry are just going to kind of be exploiting gathering data right now", "tokens": [51296, 316, 688, 295, 264, 4455, 294, 264, 3518, 366, 445, 516, 281, 733, 295, 312, 12382, 1748, 13519, 1412, 558, 586, 51468], "temperature": 0.0, "avg_logprob": -0.10193493117147417, "compression_ratio": 1.7928994082840237, "no_speech_prob": 0.050293441861867905}, {"id": 1457, "seek": 618018, "start": 6202.740000000001, "end": 6205.14, "text": " Whereas maybe there's a lot of scope to do these kind of more", "tokens": [51492, 13813, 1310, 456, 311, 257, 688, 295, 11923, 281, 360, 613, 733, 295, 544, 51612], "temperature": 0.0, "avg_logprob": -0.10193493117147417, "compression_ratio": 1.7928994082840237, "no_speech_prob": 0.050293441861867905}, {"id": 1458, "seek": 620514, "start": 6205.62, "end": 6210.660000000001, "text": " Exploratory exploratory projects where maybe that will get you to like the next frontier a few years down the line", "tokens": [50388, 12514, 284, 4745, 24765, 4745, 4455, 689, 1310, 300, 486, 483, 291, 281, 411, 264, 958, 35853, 257, 1326, 924, 760, 264, 1622, 50640], "temperature": 0.0, "avg_logprob": -0.13257974193942162, "compression_ratio": 1.7697594501718212, "no_speech_prob": 0.009706933051347733}, {"id": 1459, "seek": 620514, "start": 6211.14, "end": 6212.740000000001, "text": " I don't know what you think about this", "tokens": [50664, 286, 500, 380, 458, 437, 291, 519, 466, 341, 50744], "temperature": 0.0, "avg_logprob": -0.13257974193942162, "compression_ratio": 1.7697594501718212, "no_speech_prob": 0.009706933051347733}, {"id": 1460, "seek": 620514, "start": 6212.740000000001, "end": 6214.820000000001, "text": " Yeah, I definitely think that um", "tokens": [50744, 865, 11, 286, 2138, 519, 300, 1105, 50848], "temperature": 0.0, "avg_logprob": -0.13257974193942162, "compression_ratio": 1.7697594501718212, "no_speech_prob": 0.009706933051347733}, {"id": 1461, "seek": 620514, "start": 6214.820000000001, "end": 6219.46, "text": " Yeah, just like treating everything as just supervised learning it does tend to work because we have large data sets", "tokens": [50848, 865, 11, 445, 411, 15083, 1203, 382, 445, 46533, 2539, 309, 775, 3928, 281, 589, 570, 321, 362, 2416, 1412, 6352, 51080], "temperature": 0.0, "avg_logprob": -0.13257974193942162, "compression_ratio": 1.7697594501718212, "no_speech_prob": 0.009706933051347733}, {"id": 1462, "seek": 620514, "start": 6219.54, "end": 6223.9400000000005, "text": " But um, I think again like the challenge is just at some point we will run out of tokens", "tokens": [51084, 583, 1105, 11, 286, 519, 797, 411, 264, 3430, 307, 445, 412, 512, 935, 321, 486, 1190, 484, 295, 22667, 51304], "temperature": 0.0, "avg_logprob": -0.13257974193942162, "compression_ratio": 1.7697594501718212, "no_speech_prob": 0.009706933051347733}, {"id": 1463, "seek": 620514, "start": 6223.9400000000005, "end": 6230.42, "text": " We'll run out of data to train on and so that's why the self-improving more self exploratory systems will be more and more", "tokens": [51304, 492, 603, 1190, 484, 295, 1412, 281, 3847, 322, 293, 370, 300, 311, 983, 264, 2698, 12, 332, 4318, 798, 544, 2698, 24765, 4745, 3652, 486, 312, 544, 293, 544, 51628], "temperature": 0.0, "avg_logprob": -0.13257974193942162, "compression_ratio": 1.7697594501718212, "no_speech_prob": 0.009706933051347733}, {"id": 1464, "seek": 623042, "start": 6230.74, "end": 6233.46, "text": " I think paramount to like driving performance even further", "tokens": [50380, 286, 519, 6220, 792, 281, 411, 4840, 3389, 754, 3052, 50516], "temperature": 0.0, "avg_logprob": -0.0834259875985079, "compression_ratio": 1.7946428571428572, "no_speech_prob": 0.008846402168273926}, {"id": 1465, "seek": 623042, "start": 6233.62, "end": 6237.78, "text": " So if we want to sort of break beyond sort of the token limit of like the data that's available now", "tokens": [50524, 407, 498, 321, 528, 281, 1333, 295, 1821, 4399, 1333, 295, 264, 14862, 4948, 295, 411, 264, 1412, 300, 311, 2435, 586, 50732], "temperature": 0.0, "avg_logprob": -0.0834259875985079, "compression_ratio": 1.7946428571428572, "no_speech_prob": 0.008846402168273926}, {"id": 1466, "seek": 623042, "start": 6238.02, "end": 6241.06, "text": " We actually need these systems to generate their own tokens their own synthetic data", "tokens": [50744, 492, 767, 643, 613, 3652, 281, 8460, 641, 1065, 22667, 641, 1065, 23420, 1412, 50896], "temperature": 0.0, "avg_logprob": -0.0834259875985079, "compression_ratio": 1.7946428571428572, "no_speech_prob": 0.008846402168273926}, {"id": 1467, "seek": 623042, "start": 6241.7, "end": 6246.42, "text": " And that's that's where like the self play auto curricula exploration types of algorithms will start to", "tokens": [50928, 400, 300, 311, 300, 311, 689, 411, 264, 2698, 862, 8399, 13179, 3780, 16197, 3467, 295, 14642, 486, 722, 281, 51164], "temperature": 0.0, "avg_logprob": -0.0834259875985079, "compression_ratio": 1.7946428571428572, "no_speech_prob": 0.008846402168273926}, {"id": 1468, "seek": 623042, "start": 6247.62, "end": 6251.86, "text": " Become more and more prominent and obviously you need an environment in which to do that exploration", "tokens": [51224, 44308, 544, 293, 544, 17034, 293, 2745, 291, 643, 364, 2823, 294, 597, 281, 360, 300, 16197, 51436], "temperature": 0.0, "avg_logprob": -0.0834259875985079, "compression_ratio": 1.7946428571428572, "no_speech_prob": 0.008846402168273926}, {"id": 1469, "seek": 623042, "start": 6252.18, "end": 6254.18, "text": " And that's where the world model", "tokens": [51452, 400, 300, 311, 689, 264, 1002, 2316, 51552], "temperature": 0.0, "avg_logprob": -0.0834259875985079, "compression_ratio": 1.7946428571428572, "no_speech_prob": 0.008846402168273926}, {"id": 1470, "seek": 623042, "start": 6254.58, "end": 6260.02, "text": " Line of research is going to be very powerful just because that allows you to really sort of milk all of the value within", "tokens": [51572, 14670, 295, 2132, 307, 516, 281, 312, 588, 4005, 445, 570, 300, 4045, 291, 281, 534, 1333, 295, 5392, 439, 295, 264, 2158, 1951, 51844], "temperature": 0.0, "avg_logprob": -0.0834259875985079, "compression_ratio": 1.7946428571428572, "no_speech_prob": 0.008846402168273926}, {"id": 1471, "seek": 626042, "start": 6260.58, "end": 6267.06, "text": " The existing previous data you have seen by creating these role models where you might be able to do like counterfactual trajectories and really learn much more", "tokens": [50372, 440, 6741, 3894, 1412, 291, 362, 1612, 538, 4084, 613, 3090, 5245, 689, 291, 1062, 312, 1075, 281, 360, 411, 5682, 44919, 901, 18257, 2083, 293, 534, 1466, 709, 544, 50696], "temperature": 0.0, "avg_logprob": -0.15472506188057564, "compression_ratio": 1.8027397260273972, "no_speech_prob": 0.0009056637063622475}, {"id": 1472, "seek": 626042, "start": 6267.46, "end": 6269.46, "text": " Um, amplify the existing data you had", "tokens": [50716, 3301, 11, 41174, 264, 6741, 1412, 291, 632, 50816], "temperature": 0.0, "avg_logprob": -0.15472506188057564, "compression_ratio": 1.8027397260273972, "no_speech_prob": 0.0009056637063622475}, {"id": 1473, "seek": 626042, "start": 6270.02, "end": 6275.46, "text": " Yeah, I mean, I think one of one of the key things for me, um is modeling dynamics. So, um", "tokens": [50844, 865, 11, 286, 914, 11, 286, 519, 472, 295, 472, 295, 264, 2141, 721, 337, 385, 11, 1105, 307, 15983, 15679, 13, 407, 11, 1105, 51116], "temperature": 0.0, "avg_logprob": -0.15472506188057564, "compression_ratio": 1.8027397260273972, "no_speech_prob": 0.0009056637063622475}, {"id": 1474, "seek": 626042, "start": 6276.34, "end": 6280.42, "text": " It's quite interesting actually with the human knowledge things or even looking at the innovations from from deep mind", "tokens": [51160, 467, 311, 1596, 1880, 767, 365, 264, 1952, 3601, 721, 420, 754, 1237, 412, 264, 24283, 490, 490, 2452, 1575, 51364], "temperature": 0.0, "avg_logprob": -0.15472506188057564, "compression_ratio": 1.8027397260273972, "no_speech_prob": 0.0009056637063622475}, {"id": 1475, "seek": 626042, "start": 6280.42, "end": 6283.54, "text": " You know early versions of alpha go were bootstrapped with human knowledge", "tokens": [51364, 509, 458, 2440, 9606, 295, 8961, 352, 645, 11450, 19639, 3320, 365, 1952, 3601, 51520], "temperature": 0.0, "avg_logprob": -0.15472506188057564, "compression_ratio": 1.8027397260273972, "no_speech_prob": 0.0009056637063622475}, {"id": 1476, "seek": 626042, "start": 6283.78, "end": 6286.26, "text": " And then there was the alpha zero. So it was actually doing what we're talking about", "tokens": [51532, 400, 550, 456, 390, 264, 8961, 4018, 13, 407, 309, 390, 767, 884, 437, 321, 434, 1417, 466, 51656], "temperature": 0.0, "avg_logprob": -0.15472506188057564, "compression_ratio": 1.8027397260273972, "no_speech_prob": 0.0009056637063622475}, {"id": 1477, "seek": 626042, "start": 6286.26, "end": 6290.34, "text": " It was actually discovering knowledge on its own and um in principle. That's a great idea", "tokens": [51656, 467, 390, 767, 24773, 3601, 322, 1080, 1065, 293, 1105, 294, 8665, 13, 663, 311, 257, 869, 1558, 51860], "temperature": 0.0, "avg_logprob": -0.15472506188057564, "compression_ratio": 1.8027397260273972, "no_speech_prob": 0.0009056637063622475}, {"id": 1478, "seek": 629042, "start": 6290.42, "end": 6296.42, "text": " But of course like an irrestricted domain, it's tractable but in the real world it isn't and I'm not sure whether it makes sense to use", "tokens": [50364, 583, 295, 1164, 411, 364, 3418, 4149, 3740, 292, 9274, 11, 309, 311, 24207, 712, 457, 294, 264, 957, 1002, 309, 1943, 380, 293, 286, 478, 406, 988, 1968, 309, 1669, 2020, 281, 764, 50664], "temperature": 0.0, "avg_logprob": -0.15791078047318893, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.001189376343972981}, {"id": 1479, "seek": 629042, "start": 6296.5, "end": 6302.26, "text": " The the computation and you know information metaphor for the real world and humans and so on but but the basic idea is that", "tokens": [50668, 440, 264, 24903, 293, 291, 458, 1589, 19157, 337, 264, 957, 1002, 293, 6255, 293, 370, 322, 457, 457, 264, 3875, 1558, 307, 300, 50956], "temperature": 0.0, "avg_logprob": -0.15791078047318893, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.001189376343972981}, {"id": 1480, "seek": 629042, "start": 6302.42, "end": 6305.22, "text": " We are all real agents the universe is a massive computer", "tokens": [50964, 492, 366, 439, 957, 12554, 264, 6445, 307, 257, 5994, 3820, 51104], "temperature": 0.0, "avg_logprob": -0.15791078047318893, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.001189376343972981}, {"id": 1481, "seek": 629042, "start": 6305.54, "end": 6310.34, "text": " We're discovering all of this knowledge and then we're bootstrapping that into a machine learning algorithm", "tokens": [51120, 492, 434, 24773, 439, 295, 341, 3601, 293, 550, 321, 434, 11450, 19639, 3759, 300, 666, 257, 3479, 2539, 9284, 51360], "temperature": 0.0, "avg_logprob": -0.15791078047318893, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.001189376343972981}, {"id": 1482, "seek": 629042, "start": 6310.66, "end": 6316.18, "text": " And then the question is well, if you kind of just capture the thing now without the dynamics that produced it", "tokens": [51376, 400, 550, 264, 1168, 307, 731, 11, 498, 291, 733, 295, 445, 7983, 264, 551, 586, 1553, 264, 15679, 300, 7126, 309, 51652], "temperature": 0.0, "avg_logprob": -0.15791078047318893, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.001189376343972981}, {"id": 1483, "seek": 629042, "start": 6316.42, "end": 6320.18, "text": " Um, will the system be robust and could you still um, you know, kind of", "tokens": [51664, 3301, 11, 486, 264, 1185, 312, 13956, 293, 727, 291, 920, 1105, 11, 291, 458, 11, 733, 295, 51852], "temperature": 0.0, "avg_logprob": -0.15791078047318893, "compression_ratio": 1.760115606936416, "no_speech_prob": 0.001189376343972981}, {"id": 1484, "seek": 632018, "start": 6320.26, "end": 6324.66, "text": " Carry on as we were in the real world if that makes sense. So um, but yeah", "tokens": [50368, 44168, 322, 382, 321, 645, 294, 264, 957, 1002, 498, 300, 1669, 2020, 13, 407, 1105, 11, 457, 1338, 50588], "temperature": 0.0, "avg_logprob": -0.1074901808292494, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0005023120902478695}, {"id": 1485, "seek": 632018, "start": 6324.66, "end": 6327.860000000001, "text": " The interesting thing with the work you've done is is that you are modeling agential systems", "tokens": [50588, 440, 1880, 551, 365, 264, 589, 291, 600, 1096, 307, 307, 300, 291, 366, 15983, 623, 2549, 3652, 50748], "temperature": 0.0, "avg_logprob": -0.1074901808292494, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0005023120902478695}, {"id": 1486, "seek": 632018, "start": 6327.860000000001, "end": 6334.5, "text": " You are modeling dynamics, but could that be used for you know, much more complex tasks like the real world", "tokens": [50748, 509, 366, 15983, 15679, 11, 457, 727, 300, 312, 1143, 337, 291, 458, 11, 709, 544, 3997, 9608, 411, 264, 957, 1002, 51080], "temperature": 0.0, "avg_logprob": -0.1074901808292494, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0005023120902478695}, {"id": 1487, "seek": 632018, "start": 6335.14, "end": 6340.26, "text": " Like simulating much more complex systems in the real world. Yeah, I think that if you if you", "tokens": [51112, 1743, 1034, 12162, 709, 544, 3997, 3652, 294, 264, 957, 1002, 13, 865, 11, 286, 519, 300, 498, 291, 498, 291, 51368], "temperature": 0.0, "avg_logprob": -0.1074901808292494, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0005023120902478695}, {"id": 1488, "seek": 632018, "start": 6341.38, "end": 6346.02, "text": " So I think that just purely imitation learning alone is not really going to get you there", "tokens": [51424, 407, 286, 519, 300, 445, 17491, 47624, 2539, 3312, 307, 406, 534, 516, 281, 483, 291, 456, 51656], "temperature": 0.0, "avg_logprob": -0.1074901808292494, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0005023120902478695}, {"id": 1489, "seek": 634602, "start": 6346.900000000001, "end": 6350.26, "text": " But I think that if you can if you can imitate", "tokens": [50408, 583, 286, 519, 300, 498, 291, 393, 498, 291, 393, 35556, 50576], "temperature": 0.0, "avg_logprob": -0.07557015120983124, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.010650536976754665}, {"id": 1490, "seek": 634602, "start": 6351.38, "end": 6354.02, "text": " So one is sort of finding the set of tasks", "tokens": [50632, 407, 472, 307, 1333, 295, 5006, 264, 992, 295, 9608, 50764], "temperature": 0.0, "avg_logprob": -0.07557015120983124, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.010650536976754665}, {"id": 1491, "seek": 634602, "start": 6354.1, "end": 6358.660000000001, "text": " I think that if you find the set of tasks or reward functions that could be relevant", "tokens": [50768, 286, 519, 300, 498, 291, 915, 264, 992, 295, 9608, 420, 7782, 6828, 300, 727, 312, 7340, 50996], "temperature": 0.0, "avg_logprob": -0.07557015120983124, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.010650536976754665}, {"id": 1492, "seek": 634602, "start": 6358.820000000001, "end": 6364.900000000001, "text": " Then you can start to simulate things that are otherwise really hard to capture by just purely imitating historical trajectories", "tokens": [51004, 1396, 291, 393, 722, 281, 27817, 721, 300, 366, 5911, 534, 1152, 281, 7983, 538, 445, 17491, 566, 16350, 8584, 18257, 2083, 51308], "temperature": 0.0, "avg_logprob": -0.07557015120983124, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.010650536976754665}, {"id": 1493, "seek": 634602, "start": 6365.38, "end": 6372.740000000001, "text": " So for example strategic adaptation type of behaviors are really hard because those are sort of an open-ended space of behaviors where", "tokens": [51332, 407, 337, 1365, 10924, 21549, 2010, 295, 15501, 366, 534, 1152, 570, 729, 366, 1333, 295, 364, 1269, 12, 3502, 1901, 295, 15501, 689, 51700], "temperature": 0.0, "avg_logprob": -0.07557015120983124, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.010650536976754665}, {"id": 1494, "seek": 637274, "start": 6373.46, "end": 6376.74, "text": " If you basically have like a stock market, for example, that's a really good example", "tokens": [50400, 759, 291, 1936, 362, 411, 257, 4127, 2142, 11, 337, 1365, 11, 300, 311, 257, 534, 665, 1365, 50564], "temperature": 0.0, "avg_logprob": -0.10102320098876953, "compression_ratio": 1.872852233676976, "no_speech_prob": 0.003593253903090954}, {"id": 1495, "seek": 637274, "start": 6377.0599999999995, "end": 6379.62, "text": " Where if you have a stock market, that's a very open-ended system", "tokens": [50580, 2305, 498, 291, 362, 257, 4127, 2142, 11, 300, 311, 257, 588, 1269, 12, 3502, 1185, 50708], "temperature": 0.0, "avg_logprob": -0.10102320098876953, "compression_ratio": 1.872852233676976, "no_speech_prob": 0.003593253903090954}, {"id": 1496, "seek": 637274, "start": 6379.78, "end": 6383.38, "text": " And like different traders will have different strategies that are best responses to each other", "tokens": [50716, 400, 411, 819, 26014, 486, 362, 819, 9029, 300, 366, 1151, 13019, 281, 1184, 661, 50896], "temperature": 0.0, "avg_logprob": -0.10102320098876953, "compression_ratio": 1.872852233676976, "no_speech_prob": 0.003593253903090954}, {"id": 1497, "seek": 637274, "start": 6383.62, "end": 6387.3, "text": " And then over time the set of strategies evolves over time in an open-ended way", "tokens": [50908, 400, 550, 670, 565, 264, 992, 295, 9029, 43737, 670, 565, 294, 364, 1269, 12, 3502, 636, 51092], "temperature": 0.0, "avg_logprob": -0.10102320098876953, "compression_ratio": 1.872852233676976, "no_speech_prob": 0.003593253903090954}, {"id": 1498, "seek": 637274, "start": 6387.62, "end": 6393.54, "text": " Um, you know trading strategies that worked 10 years ago probably won't work very well today because people have sort of um", "tokens": [51108, 3301, 11, 291, 458, 9529, 9029, 300, 2732, 1266, 924, 2057, 1391, 1582, 380, 589, 588, 731, 965, 570, 561, 362, 1333, 295, 1105, 51404], "temperature": 0.0, "avg_logprob": -0.10102320098876953, "compression_ratio": 1.872852233676976, "no_speech_prob": 0.003593253903090954}, {"id": 1499, "seek": 637274, "start": 6394.34, "end": 6399.7, "text": " They've sort of figured out those strategies. And so they won't be very competitive. And so um", "tokens": [51444, 814, 600, 1333, 295, 8932, 484, 729, 9029, 13, 400, 370, 436, 1582, 380, 312, 588, 10043, 13, 400, 370, 1105, 51712], "temperature": 0.0, "avg_logprob": -0.10102320098876953, "compression_ratio": 1.872852233676976, "no_speech_prob": 0.003593253903090954}, {"id": 1500, "seek": 639970, "start": 6400.34, "end": 6403.46, "text": " I don't see an an imitation learning system being able to sort of", "tokens": [50396, 286, 500, 380, 536, 364, 364, 47624, 2539, 1185, 885, 1075, 281, 1333, 295, 50552], "temperature": 0.0, "avg_logprob": -0.1588836227144514, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.0006877794512547553}, {"id": 1501, "seek": 639970, "start": 6404.099999999999, "end": 6408.74, "text": " Um, generalize to that level of complexity just because by definition it's imitating previous", "tokens": [50584, 3301, 11, 2674, 1125, 281, 300, 1496, 295, 14024, 445, 570, 538, 7123, 309, 311, 566, 16350, 3894, 50816], "temperature": 0.0, "avg_logprob": -0.1588836227144514, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.0006877794512547553}, {"id": 1502, "seek": 639970, "start": 6409.22, "end": 6415.0599999999995, "text": " Uh trajectories and therefore strategies. So I think you need some notion of like a um a more uh", "tokens": [50840, 4019, 18257, 2083, 293, 4412, 9029, 13, 407, 286, 519, 291, 643, 512, 10710, 295, 411, 257, 1105, 257, 544, 2232, 51132], "temperature": 0.0, "avg_logprob": -0.1588836227144514, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.0006877794512547553}, {"id": 1503, "seek": 639970, "start": 6415.54, "end": 6419.3, "text": " More interactive trial and error learning that allows for strategic adaptation", "tokens": [51156, 5048, 15141, 7308, 293, 6713, 2539, 300, 4045, 337, 10924, 21549, 51344], "temperature": 0.0, "avg_logprob": -0.1588836227144514, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.0006877794512547553}, {"id": 1504, "seek": 639970, "start": 6419.38, "end": 6425.46, "text": " And that requires some notion of a payoff or a reward. And so you kind of need to have this this idea of um", "tokens": [51348, 400, 300, 7029, 512, 10710, 295, 257, 46547, 420, 257, 7782, 13, 400, 370, 291, 733, 295, 643, 281, 362, 341, 341, 1558, 295, 1105, 51652], "temperature": 0.0, "avg_logprob": -0.1588836227144514, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.0006877794512547553}, {"id": 1505, "seek": 639970, "start": 6426.9, "end": 6428.9, "text": " You you can't just purely I think learn", "tokens": [51724, 509, 291, 393, 380, 445, 17491, 286, 519, 1466, 51824], "temperature": 0.0, "avg_logprob": -0.1588836227144514, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.0006877794512547553}, {"id": 1506, "seek": 642890, "start": 6428.98, "end": 6430.179999999999, "text": " Uh", "tokens": [50368, 4019, 50428], "temperature": 0.0, "avg_logprob": -0.12182350001059288, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004044459492433816}, {"id": 1507, "seek": 642890, "start": 6430.179999999999, "end": 6433.0599999999995, "text": " A model of something like the stock market just based on previous data", "tokens": [50428, 316, 2316, 295, 746, 411, 264, 4127, 2142, 445, 2361, 322, 3894, 1412, 50572], "temperature": 0.0, "avg_logprob": -0.12182350001059288, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004044459492433816}, {"id": 1508, "seek": 642890, "start": 6433.0599999999995, "end": 6437.0599999999995, "text": " You really need to have more inductive biases around uh, sort of you know", "tokens": [50572, 509, 534, 643, 281, 362, 544, 31612, 488, 32152, 926, 2232, 11, 1333, 295, 291, 458, 50772], "temperature": 0.0, "avg_logprob": -0.12182350001059288, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004044459492433816}, {"id": 1509, "seek": 642890, "start": 6437.379999999999, "end": 6443.62, "text": " What creates a payoff or what the actual reward function is for each of the traders? Uh, but that might be something that you could", "tokens": [50788, 708, 7829, 257, 46547, 420, 437, 264, 3539, 7782, 2445, 307, 337, 1184, 295, 264, 26014, 30, 4019, 11, 457, 300, 1062, 312, 746, 300, 291, 727, 51100], "temperature": 0.0, "avg_logprob": -0.12182350001059288, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004044459492433816}, {"id": 1510, "seek": 642890, "start": 6444.099999999999, "end": 6445.299999999999, "text": " um", "tokens": [51124, 1105, 51184], "temperature": 0.0, "avg_logprob": -0.12182350001059288, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004044459492433816}, {"id": 1511, "seek": 642890, "start": 6445.299999999999, "end": 6447.299999999999, "text": " You could learn over time, but I", "tokens": [51184, 509, 727, 1466, 670, 565, 11, 457, 286, 51284], "temperature": 0.0, "avg_logprob": -0.12182350001059288, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004044459492433816}, {"id": 1512, "seek": 642890, "start": 6447.7, "end": 6451.94, "text": " But maybe not in the yeah, so this is kind of like this is not very coherent", "tokens": [51304, 583, 1310, 406, 294, 264, 1338, 11, 370, 341, 307, 733, 295, 411, 341, 307, 406, 588, 36239, 51516], "temperature": 0.0, "avg_logprob": -0.12182350001059288, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004044459492433816}, {"id": 1513, "seek": 642890, "start": 6451.94, "end": 6456.259999999999, "text": " But I feel like uh, you might need something that looks more like learning over space of programs", "tokens": [51516, 583, 286, 841, 411, 2232, 11, 291, 1062, 643, 746, 300, 1542, 544, 411, 2539, 670, 1901, 295, 4268, 51732], "temperature": 0.0, "avg_logprob": -0.12182350001059288, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0004044459492433816}, {"id": 1514, "seek": 645626, "start": 6456.5, "end": 6459.860000000001, "text": " That starts to encompass different kinds of uh tasks", "tokens": [50376, 663, 3719, 281, 28268, 819, 3685, 295, 2232, 9608, 50544], "temperature": 0.0, "avg_logprob": -0.12497143996389289, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0029762345366179943}, {"id": 1515, "seek": 645626, "start": 6460.02, "end": 6464.66, "text": " And then you can basically simulate those tasks to completion with agents that can essentially", "tokens": [50552, 400, 550, 291, 393, 1936, 27817, 729, 9608, 281, 19372, 365, 12554, 300, 393, 4476, 50784], "temperature": 0.0, "avg_logprob": -0.12497143996389289, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0029762345366179943}, {"id": 1516, "seek": 645626, "start": 6465.06, "end": 6467.14, "text": " Uh try to self-improve against other agents", "tokens": [50804, 4019, 853, 281, 2698, 12, 332, 46955, 1970, 661, 12554, 50908], "temperature": 0.0, "avg_logprob": -0.12497143996389289, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0029762345366179943}, {"id": 1517, "seek": 645626, "start": 6467.860000000001, "end": 6473.7, "text": " The stock market I think is a wonderful metaphor for we're talking about and for for two reasons first of all from the grounding reason", "tokens": [50944, 440, 4127, 2142, 286, 519, 307, 257, 3715, 19157, 337, 321, 434, 1417, 466, 293, 337, 337, 732, 4112, 700, 295, 439, 490, 264, 46727, 1778, 51236], "temperature": 0.0, "avg_logprob": -0.12497143996389289, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0029762345366179943}, {"id": 1518, "seek": 645626, "start": 6473.9400000000005, "end": 6476.9800000000005, "text": " Because you know like the the the the the memetic world is very ungrounded", "tokens": [51248, 1436, 291, 458, 411, 264, 264, 264, 264, 264, 1334, 3532, 1002, 307, 588, 517, 2921, 292, 51400], "temperature": 0.0, "avg_logprob": -0.12497143996389289, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0029762345366179943}, {"id": 1519, "seek": 645626, "start": 6477.06, "end": 6481.46, "text": " And that's why we develop as humans lots of weird shared delusions about things because it's actually like you know", "tokens": [51404, 400, 300, 311, 983, 321, 1499, 382, 6255, 3195, 295, 3657, 5507, 1103, 27255, 466, 721, 570, 309, 311, 767, 411, 291, 458, 51624], "temperature": 0.0, "avg_logprob": -0.12497143996389289, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0029762345366179943}, {"id": 1520, "seek": 648146, "start": 6481.7, "end": 6483.78, "text": " It can go in it can go in almost any direction", "tokens": [50376, 467, 393, 352, 294, 309, 393, 352, 294, 1920, 604, 3513, 50480], "temperature": 0.0, "avg_logprob": -0.11921450388517311, "compression_ratio": 1.7859327217125383, "no_speech_prob": 0.03705118969082832}, {"id": 1521, "seek": 648146, "start": 6484.02, "end": 6489.3, "text": " And also the concept of alpha I think is really important because a trading strategy works really well today", "tokens": [50492, 400, 611, 264, 3410, 295, 8961, 286, 519, 307, 534, 1021, 570, 257, 9529, 5206, 1985, 534, 731, 965, 50756], "temperature": 0.0, "avg_logprob": -0.11921450388517311, "compression_ratio": 1.7859327217125383, "no_speech_prob": 0.03705118969082832}, {"id": 1522, "seek": 648146, "start": 6489.62, "end": 6494.74, "text": " And then when other people learn about it it no longer provides an advantage because everyone else knows about it", "tokens": [50772, 400, 550, 562, 661, 561, 1466, 466, 309, 309, 572, 2854, 6417, 364, 5002, 570, 1518, 1646, 3255, 466, 309, 51028], "temperature": 0.0, "avg_logprob": -0.11921450388517311, "compression_ratio": 1.7859327217125383, "no_speech_prob": 0.03705118969082832}, {"id": 1523, "seek": 648146, "start": 6494.9, "end": 6499.54, "text": " And I feel it's the same with language models. So, you know, like gbt4 pros was really novel and cool", "tokens": [51036, 400, 286, 841, 309, 311, 264, 912, 365, 2856, 5245, 13, 407, 11, 291, 458, 11, 411, 290, 4517, 19, 6267, 390, 534, 7613, 293, 1627, 51268], "temperature": 0.0, "avg_logprob": -0.11921450388517311, "compression_ratio": 1.7859327217125383, "no_speech_prob": 0.03705118969082832}, {"id": 1524, "seek": 648146, "start": 6499.86, "end": 6503.3, "text": " It was great to you know, have like a ted talk speech when it came out", "tokens": [51284, 467, 390, 869, 281, 291, 458, 11, 362, 411, 257, 22337, 751, 6218, 562, 309, 1361, 484, 51456], "temperature": 0.0, "avg_logprob": -0.11921450388517311, "compression_ratio": 1.7859327217125383, "no_speech_prob": 0.03705118969082832}, {"id": 1525, "seek": 648146, "start": 6503.62, "end": 6506.5, "text": " And now it doesn't seem cool anymore because everyone's using it on linkedin", "tokens": [51472, 400, 586, 309, 1177, 380, 1643, 1627, 3602, 570, 1518, 311, 1228, 309, 322, 9408, 259, 51616], "temperature": 0.0, "avg_logprob": -0.11921450388517311, "compression_ratio": 1.7859327217125383, "no_speech_prob": 0.03705118969082832}, {"id": 1526, "seek": 648146, "start": 6506.74, "end": 6510.34, "text": " So it's almost like that we need to have this in like continuous", "tokens": [51628, 407, 309, 311, 1920, 411, 300, 321, 643, 281, 362, 341, 294, 411, 10957, 51808], "temperature": 0.0, "avg_logprob": -0.11921450388517311, "compression_ratio": 1.7859327217125383, "no_speech_prob": 0.03705118969082832}, {"id": 1527, "seek": 651034, "start": 6510.900000000001, "end": 6514.42, "text": " creative evolving process producing new sources of alpha", "tokens": [50392, 5880, 21085, 1399, 10501, 777, 7139, 295, 8961, 50568], "temperature": 0.0, "avg_logprob": -0.09668766435726668, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.0023185235913842916}, {"id": 1528, "seek": 651034, "start": 6514.82, "end": 6520.18, "text": " And the paradox is that if everyone has access to the same model it can't be a source of alpha by definition", "tokens": [50588, 400, 264, 26221, 307, 300, 498, 1518, 575, 2105, 281, 264, 912, 2316, 309, 393, 380, 312, 257, 4009, 295, 8961, 538, 7123, 50856], "temperature": 0.0, "avg_logprob": -0.09668766435726668, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.0023185235913842916}, {"id": 1529, "seek": 651034, "start": 6520.34, "end": 6524.5, "text": " Yeah, I guess on that like topic because we kind of talked about like synthetic data earlier", "tokens": [50864, 865, 11, 286, 2041, 322, 300, 411, 4829, 570, 321, 733, 295, 2825, 466, 411, 23420, 1412, 3071, 51072], "temperature": 0.0, "avg_logprob": -0.09668766435726668, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.0023185235913842916}, {"id": 1530, "seek": 651034, "start": 6524.58, "end": 6530.1, "text": " And you kind of said like you know one one mechanism towards getting like a kind of self-improving system that is able to kind of", "tokens": [51076, 400, 291, 733, 295, 848, 411, 291, 458, 472, 472, 7513, 3030, 1242, 411, 257, 733, 295, 2698, 12, 332, 4318, 798, 1185, 300, 307, 1075, 281, 733, 295, 51352], "temperature": 0.0, "avg_logprob": -0.09668766435726668, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.0023185235913842916}, {"id": 1531, "seek": 651034, "start": 6530.26, "end": 6531.06, "text": " You know", "tokens": [51360, 509, 458, 51400], "temperature": 0.0, "avg_logprob": -0.09668766435726668, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.0023185235913842916}, {"id": 1532, "seek": 651034, "start": 6531.06, "end": 6534.26, "text": " Continue to improve is to kind of like filter the synthetic data for example", "tokens": [51400, 24472, 281, 3470, 307, 281, 733, 295, 411, 6608, 264, 23420, 1412, 337, 1365, 51560], "temperature": 0.0, "avg_logprob": -0.09668766435726668, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.0023185235913842916}, {"id": 1533, "seek": 651034, "start": 6534.34, "end": 6538.02, "text": " So you might kind of you know have the the new system and then we generate some more data", "tokens": [51564, 407, 291, 1062, 733, 295, 291, 458, 362, 264, 264, 777, 1185, 293, 550, 321, 8460, 512, 544, 1412, 51748], "temperature": 0.0, "avg_logprob": -0.09668766435726668, "compression_ratio": 1.855263157894737, "no_speech_prob": 0.0023185235913842916}, {"id": 1534, "seek": 653802, "start": 6538.02, "end": 6541.9400000000005, "text": " And then we kind of have some like filtering mechanism to say that you know in the current stock market", "tokens": [50364, 400, 550, 321, 733, 295, 362, 512, 411, 30822, 7513, 281, 584, 300, 291, 458, 294, 264, 2190, 4127, 2142, 50560], "temperature": 0.0, "avg_logprob": -0.10614451151045542, "compression_ratio": 1.9087719298245613, "no_speech_prob": 0.0011334463488310575}, {"id": 1535, "seek": 653802, "start": 6541.9400000000005, "end": 6545.540000000001, "text": " This is this is good data or what you know, whatever system we're thinking about and then we can kind of like", "tokens": [50560, 639, 307, 341, 307, 665, 1412, 420, 437, 291, 458, 11, 2035, 1185, 321, 434, 1953, 466, 293, 550, 321, 393, 733, 295, 411, 50740], "temperature": 0.0, "avg_logprob": -0.10614451151045542, "compression_ratio": 1.9087719298245613, "no_speech_prob": 0.0011334463488310575}, {"id": 1536, "seek": 653802, "start": 6546.34, "end": 6548.34, "text": " Use that to enable the model to improve", "tokens": [50780, 8278, 300, 281, 9528, 264, 2316, 281, 3470, 50880], "temperature": 0.0, "avg_logprob": -0.10614451151045542, "compression_ratio": 1.9087719298245613, "no_speech_prob": 0.0011334463488310575}, {"id": 1537, "seek": 653802, "start": 6548.9800000000005, "end": 6550.9800000000005, "text": " You know and adapt to the new system", "tokens": [50912, 509, 458, 293, 6231, 281, 264, 777, 1185, 51012], "temperature": 0.0, "avg_logprob": -0.10614451151045542, "compression_ratio": 1.9087719298245613, "no_speech_prob": 0.0011334463488310575}, {"id": 1538, "seek": 653802, "start": 6551.06, "end": 6554.02, "text": " But something I've always like why like thought about is like well", "tokens": [51016, 583, 746, 286, 600, 1009, 411, 983, 411, 1194, 466, 307, 411, 731, 51164], "temperature": 0.0, "avg_logprob": -0.10614451151045542, "compression_ratio": 1.9087719298245613, "no_speech_prob": 0.0011334463488310575}, {"id": 1539, "seek": 653802, "start": 6554.02, "end": 6558.660000000001, "text": " I guess one is is it really trivial to be able to like filter that you know new synthetic data", "tokens": [51164, 286, 2041, 472, 307, 307, 309, 534, 26703, 281, 312, 1075, 281, 411, 6608, 300, 291, 458, 777, 23420, 1412, 51396], "temperature": 0.0, "avg_logprob": -0.10614451151045542, "compression_ratio": 1.9087719298245613, "no_speech_prob": 0.0011334463488310575}, {"id": 1540, "seek": 653802, "start": 6559.14, "end": 6564.34, "text": " And then two it feels like if you're just relying on like filtering existing synthetic data", "tokens": [51420, 400, 550, 732, 309, 3417, 411, 498, 291, 434, 445, 24140, 322, 411, 30822, 6741, 23420, 1412, 51680], "temperature": 0.0, "avg_logprob": -0.10614451151045542, "compression_ratio": 1.9087719298245613, "no_speech_prob": 0.0011334463488310575}, {"id": 1541, "seek": 656434, "start": 6564.900000000001, "end": 6569.22, "text": " Like isn't that a never to go into kind of plateau and so I guess eventually", "tokens": [50392, 1743, 1943, 380, 300, 257, 1128, 281, 352, 666, 733, 295, 39885, 293, 370, 286, 2041, 4728, 50608], "temperature": 0.0, "avg_logprob": -0.14631537785605778, "compression_ratio": 1.8141891891891893, "no_speech_prob": 0.0008040128159336746}, {"id": 1542, "seek": 656434, "start": 6569.22, "end": 6574.26, "text": " You know we talked about how you kind of said that you do actually actively not need to go out and get real more real data", "tokens": [50608, 509, 458, 321, 2825, 466, 577, 291, 733, 295, 848, 300, 291, 360, 767, 13022, 406, 643, 281, 352, 484, 293, 483, 957, 544, 957, 1412, 50860], "temperature": 0.0, "avg_logprob": -0.14631537785605778, "compression_ratio": 1.8141891891891893, "no_speech_prob": 0.0008040128159336746}, {"id": 1543, "seek": 656434, "start": 6574.58, "end": 6579.46, "text": " But I guess I'm kind of asking you do you think this idea of just like filtering synthetic data from a model is kind of", "tokens": [50876, 583, 286, 2041, 286, 478, 733, 295, 3365, 291, 360, 291, 519, 341, 1558, 295, 445, 411, 30822, 23420, 1412, 490, 257, 2316, 307, 733, 295, 51120], "temperature": 0.0, "avg_logprob": -0.14631537785605778, "compression_ratio": 1.8141891891891893, "no_speech_prob": 0.0008040128159336746}, {"id": 1544, "seek": 656434, "start": 6580.1, "end": 6584.900000000001, "text": " Sufficient to always be able to adapt and improve or is it always going to be a mixture of like more real data", "tokens": [51152, 318, 30664, 281, 1009, 312, 1075, 281, 6231, 293, 3470, 420, 307, 309, 1009, 516, 281, 312, 257, 9925, 295, 411, 544, 957, 1412, 51392], "temperature": 0.0, "avg_logprob": -0.14631537785605778, "compression_ratio": 1.8141891891891893, "no_speech_prob": 0.0008040128159336746}, {"id": 1545, "seek": 656434, "start": 6585.3, "end": 6590.02, "text": " Plus synthetic data filtering. I think it's the latter just because um at some point you would expect that", "tokens": [51412, 7721, 23420, 1412, 30822, 13, 286, 519, 309, 311, 264, 18481, 445, 570, 1105, 412, 512, 935, 291, 576, 2066, 300, 51648], "temperature": 0.0, "avg_logprob": -0.14631537785605778, "compression_ratio": 1.8141891891891893, "no_speech_prob": 0.0008040128159336746}, {"id": 1546, "seek": 659002, "start": 6590.820000000001, "end": 6595.620000000001, "text": " The synthetic data you do generate it'll start to sort of saturate like what's already in the model", "tokens": [50404, 440, 23420, 1412, 291, 360, 8460, 309, 603, 722, 281, 1333, 295, 21160, 473, 411, 437, 311, 1217, 294, 264, 2316, 50644], "temperature": 0.0, "avg_logprob": -0.08365826379685175, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.006095369812101126}, {"id": 1547, "seek": 659002, "start": 6596.18, "end": 6601.22, "text": " Just because the model is trained on a finite amount of information. So at some point you're just going to start to see more and more", "tokens": [50672, 1449, 570, 264, 2316, 307, 8895, 322, 257, 19362, 2372, 295, 1589, 13, 407, 412, 512, 935, 291, 434, 445, 516, 281, 722, 281, 536, 544, 293, 544, 50924], "temperature": 0.0, "avg_logprob": -0.08365826379685175, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.006095369812101126}, {"id": 1548, "seek": 659002, "start": 6602.34, "end": 6606.900000000001, "text": " Especially like the more likely trajectories or sequences of samples. You'll start to see that", "tokens": [50980, 8545, 411, 264, 544, 3700, 18257, 2083, 420, 22978, 295, 10938, 13, 509, 603, 722, 281, 536, 300, 51208], "temperature": 0.0, "avg_logprob": -0.08365826379685175, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.006095369812101126}, {"id": 1549, "seek": 659002, "start": 6607.620000000001, "end": 6612.820000000001, "text": " More and more and so you're not really going to be very sample efficient in terms of searching for the synthetic data", "tokens": [51244, 5048, 293, 544, 293, 370, 291, 434, 406, 534, 516, 281, 312, 588, 6889, 7148, 294, 2115, 295, 10808, 337, 264, 23420, 1412, 51504], "temperature": 0.0, "avg_logprob": -0.08365826379685175, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.006095369812101126}, {"id": 1550, "seek": 659002, "start": 6613.540000000001, "end": 6618.900000000001, "text": " So can you can you tell us about the results of the paper? Totally. Yeah, so basically we evaluate this um", "tokens": [51540, 407, 393, 291, 393, 291, 980, 505, 466, 264, 3542, 295, 264, 3035, 30, 22837, 13, 865, 11, 370, 1936, 321, 13059, 341, 1105, 51808], "temperature": 0.0, "avg_logprob": -0.08365826379685175, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.006095369812101126}, {"id": 1551, "seek": 661890, "start": 6619.62, "end": 6624.66, "text": " This algorithm on a bunch of like synthetic simulated domains kind of like robotics related tasks", "tokens": [50400, 639, 9284, 322, 257, 3840, 295, 411, 23420, 41713, 25514, 733, 295, 411, 34145, 4077, 9608, 50652], "temperature": 0.0, "avg_logprob": -0.13664925404084036, "compression_ratio": 1.7845117845117846, "no_speech_prob": 0.0006262600654736161}, {"id": 1552, "seek": 661890, "start": 6625.139999999999, "end": 6628.339999999999, "text": " Um and kind of yet environments where there's like varying levels of complexity", "tokens": [50676, 3301, 293, 733, 295, 1939, 12388, 689, 456, 311, 411, 22984, 4358, 295, 14024, 50836], "temperature": 0.0, "avg_logprob": -0.13664925404084036, "compression_ratio": 1.7845117845117846, "no_speech_prob": 0.0006262600654736161}, {"id": 1553, "seek": 661890, "start": 6628.42, "end": 6632.259999999999, "text": " So, you know, you might have a robot pushing around a variable number of like objects", "tokens": [50840, 407, 11, 291, 458, 11, 291, 1062, 362, 257, 7881, 7380, 926, 257, 7006, 1230, 295, 411, 6565, 51032], "temperature": 0.0, "avg_logprob": -0.13664925404084036, "compression_ratio": 1.7845117845117846, "no_speech_prob": 0.0006262600654736161}, {"id": 1554, "seek": 661890, "start": 6632.339999999999, "end": 6635.46, "text": " Or maybe you have different terrain that the robot might want to um", "tokens": [51036, 1610, 1310, 291, 362, 819, 17674, 300, 264, 7881, 1062, 528, 281, 1105, 51192], "temperature": 0.0, "avg_logprob": -0.13664925404084036, "compression_ratio": 1.7845117845117846, "no_speech_prob": 0.0006262600654736161}, {"id": 1555, "seek": 661890, "start": 6636.58, "end": 6639.54, "text": " Learn to kind of you know do locomotion over and things like this", "tokens": [51248, 17216, 281, 733, 295, 291, 458, 360, 36369, 19228, 670, 293, 721, 411, 341, 51396], "temperature": 0.0, "avg_logprob": -0.13664925404084036, "compression_ratio": 1.7845117845117846, "no_speech_prob": 0.0006262600654736161}, {"id": 1556, "seek": 661890, "start": 6640.179999999999, "end": 6645.86, "text": " Um and so kind of you know, the main comparison we make is like how well does waker work relative to like naive domain randomization", "tokens": [51428, 3301, 293, 370, 733, 295, 291, 458, 11, 264, 2135, 9660, 321, 652, 307, 411, 577, 731, 775, 261, 4003, 589, 4972, 281, 411, 29052, 9274, 4974, 2144, 51712], "temperature": 0.0, "avg_logprob": -0.13664925404084036, "compression_ratio": 1.7845117845117846, "no_speech_prob": 0.0006262600654736161}, {"id": 1557, "seek": 664586, "start": 6645.86, "end": 6651.62, "text": " So how well does it work if you just like uniformly sample the space of environments versus if you do actively seek out", "tokens": [50364, 407, 577, 731, 775, 309, 589, 498, 291, 445, 411, 48806, 6889, 264, 1901, 295, 12388, 5717, 498, 291, 360, 13022, 8075, 484, 50652], "temperature": 0.0, "avg_logprob": -0.11833127339680989, "compression_ratio": 1.823943661971831, "no_speech_prob": 0.0006461422890424728}, {"id": 1558, "seek": 664586, "start": 6651.94, "end": 6654.259999999999, "text": " The environments that have this like higher uncertainty", "tokens": [50668, 440, 12388, 300, 362, 341, 411, 2946, 15697, 50784], "temperature": 0.0, "avg_logprob": -0.11833127339680989, "compression_ratio": 1.823943661971831, "no_speech_prob": 0.0006461422890424728}, {"id": 1559, "seek": 664586, "start": 6654.98, "end": 6658.74, "text": " Um and so basically what we show is that you know, if we do the waker approach", "tokens": [50820, 3301, 293, 370, 1936, 437, 321, 855, 307, 300, 291, 458, 11, 498, 321, 360, 264, 261, 4003, 3109, 51008], "temperature": 0.0, "avg_logprob": -0.11833127339680989, "compression_ratio": 1.823943661971831, "no_speech_prob": 0.0006461422890424728}, {"id": 1560, "seek": 664586, "start": 6659.0599999999995, "end": 6663.54, "text": " We still do like very well on average, but we consistently do better in terms of robustness", "tokens": [51024, 492, 920, 360, 411, 588, 731, 322, 4274, 11, 457, 321, 14961, 360, 1101, 294, 2115, 295, 13956, 1287, 51248], "temperature": 0.0, "avg_logprob": -0.11833127339680989, "compression_ratio": 1.823943661971831, "no_speech_prob": 0.0006461422890424728}, {"id": 1561, "seek": 664586, "start": 6663.94, "end": 6668.42, "text": " And so robustness by robustness. I mean here that that we do better in terms of the worst environment", "tokens": [51268, 400, 370, 13956, 1287, 538, 13956, 1287, 13, 286, 914, 510, 300, 300, 321, 360, 1101, 294, 2115, 295, 264, 5855, 2823, 51492], "temperature": 0.0, "avg_logprob": -0.11833127339680989, "compression_ratio": 1.823943661971831, "no_speech_prob": 0.0006461422890424728}, {"id": 1562, "seek": 664586, "start": 6668.98, "end": 6671.78, "text": " That the agent is evaluated under and so this kind of means, you know", "tokens": [51520, 663, 264, 9461, 307, 25509, 833, 293, 370, 341, 733, 295, 1355, 11, 291, 458, 51660], "temperature": 0.0, "avg_logprob": -0.11833127339680989, "compression_ratio": 1.823943661971831, "no_speech_prob": 0.0006461422890424728}, {"id": 1563, "seek": 667178, "start": 6671.78, "end": 6676.259999999999, "text": " If the agent is able to do well in the worst environments that it that it is evaluated under", "tokens": [50364, 759, 264, 9461, 307, 1075, 281, 360, 731, 294, 264, 5855, 12388, 300, 309, 300, 309, 307, 25509, 833, 50588], "temperature": 0.0, "avg_logprob": -0.1334120635986328, "compression_ratio": 2.013745704467354, "no_speech_prob": 0.0021823688875883818}, {"id": 1564, "seek": 667178, "start": 6676.42, "end": 6679.3, "text": " That kind of shows that it's able to do well across all environments because", "tokens": [50596, 663, 733, 295, 3110, 300, 309, 311, 1075, 281, 360, 731, 2108, 439, 12388, 570, 50740], "temperature": 0.0, "avg_logprob": -0.1334120635986328, "compression_ratio": 2.013745704467354, "no_speech_prob": 0.0021823688875883818}, {"id": 1565, "seek": 667178, "start": 6679.7, "end": 6684.9, "text": " Its worst performance is still good. Um, so we kind of this this shows that we we achieve this like robustness property", "tokens": [50760, 6953, 5855, 3389, 307, 920, 665, 13, 3301, 11, 370, 321, 733, 295, 341, 341, 3110, 300, 321, 321, 4584, 341, 411, 13956, 1287, 4707, 51020], "temperature": 0.0, "avg_logprob": -0.1334120635986328, "compression_ratio": 2.013745704467354, "no_speech_prob": 0.0021823688875883818}, {"id": 1566, "seek": 667178, "start": 6685.46, "end": 6687.46, "text": " Which we talked about in terms of like mini max regret", "tokens": [51048, 3013, 321, 2825, 466, 294, 2115, 295, 411, 8382, 11469, 10879, 51148], "temperature": 0.0, "avg_logprob": -0.1334120635986328, "compression_ratio": 2.013745704467354, "no_speech_prob": 0.0021823688875883818}, {"id": 1567, "seek": 667178, "start": 6688.0199999999995, "end": 6693.38, "text": " But we evaluate we don't evaluate it in terms of like the true notion of mini max regret because as we talked about earlier", "tokens": [51176, 583, 321, 13059, 321, 500, 380, 13059, 309, 294, 2115, 295, 411, 264, 2074, 10710, 295, 8382, 11469, 10879, 570, 382, 321, 2825, 466, 3071, 51444], "temperature": 0.0, "avg_logprob": -0.1334120635986328, "compression_ratio": 2.013745704467354, "no_speech_prob": 0.0021823688875883818}, {"id": 1568, "seek": 667178, "start": 6693.7, "end": 6699.38, "text": " Actually evaluating regret exactly as difficult because that that requires knowing the exact true optimal performance", "tokens": [51460, 5135, 27479, 10879, 2293, 382, 2252, 570, 300, 300, 7029, 5276, 264, 1900, 2074, 16252, 3389, 51744], "temperature": 0.0, "avg_logprob": -0.1334120635986328, "compression_ratio": 2.013745704467354, "no_speech_prob": 0.0021823688875883818}, {"id": 1569, "seek": 669938, "start": 6699.9400000000005, "end": 6702.9800000000005, "text": " Which isn't something we can really know. So instead we we just show that you know", "tokens": [50392, 3013, 1943, 380, 746, 321, 393, 534, 458, 13, 407, 2602, 321, 321, 445, 855, 300, 291, 458, 50544], "temperature": 0.0, "avg_logprob": -0.11297601461410522, "compression_ratio": 1.792982456140351, "no_speech_prob": 0.0006855290848761797}, {"id": 1570, "seek": 669938, "start": 6703.06, "end": 6708.9800000000005, "text": " The agent performs well across all environments more so than if you just like naively sampled the environments uniformly", "tokens": [50548, 440, 9461, 26213, 731, 2108, 439, 12388, 544, 370, 813, 498, 291, 445, 411, 1667, 3413, 3247, 15551, 264, 12388, 48806, 50844], "temperature": 0.0, "avg_logprob": -0.11297601461410522, "compression_ratio": 1.792982456140351, "no_speech_prob": 0.0006855290848761797}, {"id": 1571, "seek": 669938, "start": 6709.78, "end": 6714.58, "text": " And in terms of decomposing the performance across the spectrum of possible environments", "tokens": [50884, 400, 294, 2115, 295, 22867, 6110, 264, 3389, 2108, 264, 11143, 295, 1944, 12388, 51124], "temperature": 0.0, "avg_logprob": -0.11297601461410522, "compression_ratio": 1.792982456140351, "no_speech_prob": 0.0006855290848761797}, {"id": 1572, "seek": 669938, "start": 6714.58, "end": 6719.54, "text": " So like, you know the ideal situation is that we have a very simple model which just generalizes", "tokens": [51124, 407, 411, 11, 291, 458, 264, 7157, 2590, 307, 300, 321, 362, 257, 588, 2199, 2316, 597, 445, 2674, 5660, 51372], "temperature": 0.0, "avg_logprob": -0.11297601461410522, "compression_ratio": 1.792982456140351, "no_speech_prob": 0.0006855290848761797}, {"id": 1573, "seek": 669938, "start": 6719.54, "end": 6724.9800000000005, "text": " So we happen to have found the golden motif, you know, there's a spectrum of correlations almost all of them are spurious", "tokens": [51372, 407, 321, 1051, 281, 362, 1352, 264, 9729, 39478, 11, 291, 458, 11, 456, 311, 257, 11143, 295, 13983, 763, 1920, 439, 295, 552, 366, 637, 24274, 51644], "temperature": 0.0, "avg_logprob": -0.11297601461410522, "compression_ratio": 1.792982456140351, "no_speech_prob": 0.0006855290848761797}, {"id": 1574, "seek": 672498, "start": 6725.0599999999995, "end": 6728.419999999999, "text": " But we've just you know, just by through some sheer magic", "tokens": [50368, 583, 321, 600, 445, 291, 458, 11, 445, 538, 807, 512, 23061, 5585, 50536], "temperature": 0.0, "avg_logprob": -0.09167022991897468, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.01131702121347189}, {"id": 1575, "seek": 672498, "start": 6728.66, "end": 6732.339999999999, "text": " We found the best motif to work in all situations. Probably that's not quite true", "tokens": [50548, 492, 1352, 264, 1151, 39478, 281, 589, 294, 439, 6851, 13, 9210, 300, 311, 406, 1596, 2074, 50732], "temperature": 0.0, "avg_logprob": -0.09167022991897468, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.01131702121347189}, {"id": 1576, "seek": 672498, "start": 6732.419999999999, "end": 6737.459999999999, "text": " Probably there are some good generalizing motifs and the model has also kind of like memorized the long tail", "tokens": [50736, 9210, 456, 366, 512, 665, 2674, 3319, 2184, 18290, 293, 264, 2316, 575, 611, 733, 295, 411, 46677, 264, 938, 6838, 50988], "temperature": 0.0, "avg_logprob": -0.09167022991897468, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.01131702121347189}, {"id": 1577, "seek": 672498, "start": 6737.94, "end": 6742.98, "text": " And that there's some degree of like, you know, it works really well on on the test set that might not out of domain distribution", "tokens": [51012, 400, 300, 456, 311, 512, 4314, 295, 411, 11, 291, 458, 11, 309, 1985, 534, 731, 322, 322, 264, 1500, 992, 300, 1062, 406, 484, 295, 9274, 7316, 51264], "temperature": 0.0, "avg_logprob": -0.09167022991897468, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.01131702121347189}, {"id": 1578, "seek": 672498, "start": 6743.139999999999, "end": 6746.099999999999, "text": " Do you have any like way of reasoning about what that is?", "tokens": [51272, 1144, 291, 362, 604, 411, 636, 295, 21577, 466, 437, 300, 307, 30, 51420], "temperature": 0.0, "avg_logprob": -0.09167022991897468, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.01131702121347189}, {"id": 1579, "seek": 672498, "start": 6746.98, "end": 6748.259999999999, "text": " um", "tokens": [51464, 1105, 51528], "temperature": 0.0, "avg_logprob": -0.09167022991897468, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.01131702121347189}, {"id": 1580, "seek": 672498, "start": 6748.259999999999, "end": 6749.54, "text": " so", "tokens": [51528, 370, 51592], "temperature": 0.0, "avg_logprob": -0.09167022991897468, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.01131702121347189}, {"id": 1581, "seek": 672498, "start": 6749.54, "end": 6753.62, "text": " Yeah, I agree. I guess there's like not necessarily it's not necessarily the case that by like", "tokens": [51592, 865, 11, 286, 3986, 13, 286, 2041, 456, 311, 411, 406, 4725, 309, 311, 406, 4725, 264, 1389, 300, 538, 411, 51796], "temperature": 0.0, "avg_logprob": -0.09167022991897468, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.01131702121347189}, {"id": 1582, "seek": 675362, "start": 6754.099999999999, "end": 6756.42, "text": " Focusing more on these like long tail examples", "tokens": [50388, 479, 905, 7981, 544, 322, 613, 411, 938, 6838, 5110, 50504], "temperature": 0.0, "avg_logprob": -0.09964785777347189, "compression_ratio": 1.909375, "no_speech_prob": 0.0012062789173796773}, {"id": 1583, "seek": 675362, "start": 6756.5, "end": 6761.62, "text": " That's necessarily the best way of training the best model because like you said like maybe it happens to be the case that", "tokens": [50508, 663, 311, 4725, 264, 1151, 636, 295, 3097, 264, 1151, 2316, 570, 411, 291, 848, 411, 1310, 309, 2314, 281, 312, 264, 1389, 300, 50764], "temperature": 0.0, "avg_logprob": -0.09964785777347189, "compression_ratio": 1.909375, "no_speech_prob": 0.0012062789173796773}, {"id": 1584, "seek": 675362, "start": 6762.0199999999995, "end": 6766.0199999999995, "text": " If the model is trained on some certain subset of the tasks like that will actually generalize better", "tokens": [50784, 759, 264, 2316, 307, 8895, 322, 512, 1629, 25993, 295, 264, 9608, 411, 300, 486, 767, 2674, 1125, 1101, 50984], "temperature": 0.0, "avg_logprob": -0.09964785777347189, "compression_ratio": 1.909375, "no_speech_prob": 0.0012062789173796773}, {"id": 1585, "seek": 675362, "start": 6766.099999999999, "end": 6768.82, "text": " but but I think in practice, that's not something we can really um", "tokens": [50988, 457, 457, 286, 519, 294, 3124, 11, 300, 311, 406, 746, 321, 393, 534, 1105, 51124], "temperature": 0.0, "avg_logprob": -0.09964785777347189, "compression_ratio": 1.909375, "no_speech_prob": 0.0012062789173796773}, {"id": 1586, "seek": 675362, "start": 6769.78, "end": 6775.22, "text": " Really know how to you know, like optimally select the best kind of set of tasks that will generalize well", "tokens": [51172, 4083, 458, 577, 281, 291, 458, 11, 411, 5028, 379, 3048, 264, 1151, 733, 295, 992, 295, 9608, 300, 486, 2674, 1125, 731, 51444], "temperature": 0.0, "avg_logprob": -0.09964785777347189, "compression_ratio": 1.909375, "no_speech_prob": 0.0012062789173796773}, {"id": 1587, "seek": 675362, "start": 6775.54, "end": 6779.0599999999995, "text": " And so we do focus more on on like, you know, these these kind of long tail tasks", "tokens": [51460, 400, 370, 321, 360, 1879, 544, 322, 322, 411, 11, 291, 458, 11, 613, 613, 733, 295, 938, 6838, 9608, 51636], "temperature": 0.0, "avg_logprob": -0.09964785777347189, "compression_ratio": 1.909375, "no_speech_prob": 0.0012062789173796773}, {"id": 1588, "seek": 675362, "start": 6779.0599999999995, "end": 6782.26, "text": " Or like the ones that we might see rarely and therefore have high uncertainty about", "tokens": [51636, 1610, 411, 264, 2306, 300, 321, 1062, 536, 13752, 293, 4412, 362, 1090, 15697, 466, 51796], "temperature": 0.0, "avg_logprob": -0.09964785777347189, "compression_ratio": 1.909375, "no_speech_prob": 0.0012062789173796773}, {"id": 1589, "seek": 678226, "start": 6782.9800000000005, "end": 6783.780000000001, "text": " um", "tokens": [50400, 1105, 50440], "temperature": 0.0, "avg_logprob": -0.1292135755894548, "compression_ratio": 2.0509803921568626, "no_speech_prob": 0.0010985784465447068}, {"id": 1590, "seek": 678226, "start": 6783.780000000001, "end": 6786.5, "text": " In terms of like the the out of distribution generalization", "tokens": [50440, 682, 2115, 295, 411, 264, 264, 484, 295, 7316, 2674, 2144, 50576], "temperature": 0.0, "avg_logprob": -0.1292135755894548, "compression_ratio": 2.0509803921568626, "no_speech_prob": 0.0010985784465447068}, {"id": 1591, "seek": 678226, "start": 6786.58, "end": 6790.9800000000005, "text": " So so we do also do some experiments like looking at how well does the model generalize out of distribution?", "tokens": [50580, 407, 370, 321, 360, 611, 360, 512, 12050, 411, 1237, 412, 577, 731, 775, 264, 2316, 2674, 1125, 484, 295, 7316, 30, 50800], "temperature": 0.0, "avg_logprob": -0.1292135755894548, "compression_ratio": 2.0509803921568626, "no_speech_prob": 0.0010985784465447068}, {"id": 1592, "seek": 678226, "start": 6791.860000000001, "end": 6794.900000000001, "text": " And basically what we show is that if we train the model in this way", "tokens": [50844, 400, 1936, 437, 321, 855, 307, 300, 498, 321, 3847, 264, 2316, 294, 341, 636, 50996], "temperature": 0.0, "avg_logprob": -0.1292135755894548, "compression_ratio": 2.0509803921568626, "no_speech_prob": 0.0010985784465447068}, {"id": 1593, "seek": 678226, "start": 6795.22, "end": 6798.42, "text": " And then we give it some more environments that hasn't seen at test time", "tokens": [51012, 400, 550, 321, 976, 309, 512, 544, 12388, 300, 6132, 380, 1612, 412, 1500, 565, 51172], "temperature": 0.0, "avg_logprob": -0.1292135755894548, "compression_ratio": 2.0509803921568626, "no_speech_prob": 0.0010985784465447068}, {"id": 1594, "seek": 678226, "start": 6798.900000000001, "end": 6803.3, "text": " Um, if the environments are more complex then then we've seen sorry hasn't seen at training time", "tokens": [51196, 3301, 11, 498, 264, 12388, 366, 544, 3997, 550, 550, 321, 600, 1612, 2597, 6132, 380, 1612, 412, 3097, 565, 51416], "temperature": 0.0, "avg_logprob": -0.1292135755894548, "compression_ratio": 2.0509803921568626, "no_speech_prob": 0.0010985784465447068}, {"id": 1595, "seek": 678226, "start": 6804.18, "end": 6808.5, "text": " Basically like this model then generalizes better to out of distribution environments that are like more complex", "tokens": [51460, 8537, 411, 341, 2316, 550, 2674, 5660, 1101, 281, 484, 295, 7316, 12388, 300, 366, 411, 544, 3997, 51676], "temperature": 0.0, "avg_logprob": -0.1292135755894548, "compression_ratio": 2.0509803921568626, "no_speech_prob": 0.0010985784465447068}, {"id": 1596, "seek": 680850, "start": 6808.5, "end": 6812.26, "text": " Which is kind of what you'd expect because we've kind of biased something towards more complexity", "tokens": [50364, 3013, 307, 733, 295, 437, 291, 1116, 2066, 570, 321, 600, 733, 295, 28035, 746, 3030, 544, 14024, 50552], "temperature": 0.0, "avg_logprob": -0.10059293589197603, "compression_ratio": 2.2644067796610168, "no_speech_prob": 0.004330774769186974}, {"id": 1597, "seek": 680850, "start": 6812.42, "end": 6815.54, "text": " We're able to generalize better to out of distribution environments that have higher complexity", "tokens": [50560, 492, 434, 1075, 281, 2674, 1125, 1101, 281, 484, 295, 7316, 12388, 300, 362, 2946, 14024, 50716], "temperature": 0.0, "avg_logprob": -0.10059293589197603, "compression_ratio": 2.2644067796610168, "no_speech_prob": 0.004330774769186974}, {"id": 1598, "seek": 680850, "start": 6815.94, "end": 6820.42, "text": " And then I guess the question is like do we care about out of distribution environments that have higher complexity?", "tokens": [50736, 400, 550, 286, 2041, 264, 1168, 307, 411, 360, 321, 1127, 466, 484, 295, 7316, 12388, 300, 362, 2946, 14024, 30, 50960], "temperature": 0.0, "avg_logprob": -0.10059293589197603, "compression_ratio": 2.2644067796610168, "no_speech_prob": 0.004330774769186974}, {"id": 1599, "seek": 680850, "start": 6820.42, "end": 6822.9, "text": " Like what about the out of distribution environments that have lower complexity?", "tokens": [50960, 1743, 437, 466, 264, 484, 295, 7316, 12388, 300, 362, 3126, 14024, 30, 51084], "temperature": 0.0, "avg_logprob": -0.10059293589197603, "compression_ratio": 2.2644067796610168, "no_speech_prob": 0.004330774769186974}, {"id": 1600, "seek": 680850, "start": 6823.54, "end": 6825.54, "text": " and I would argue that you know", "tokens": [51116, 293, 286, 576, 9695, 300, 291, 458, 51216], "temperature": 0.0, "avg_logprob": -0.10059293589197603, "compression_ratio": 2.2644067796610168, "no_speech_prob": 0.004330774769186974}, {"id": 1601, "seek": 680850, "start": 6825.94, "end": 6830.18, "text": " Basically the lower environment out of distribution distribution environments that have lower complexity", "tokens": [51236, 8537, 264, 3126, 2823, 484, 295, 7316, 7316, 12388, 300, 362, 3126, 14024, 51448], "temperature": 0.0, "avg_logprob": -0.10059293589197603, "compression_ratio": 2.2644067796610168, "no_speech_prob": 0.004330774769186974}, {"id": 1602, "seek": 680850, "start": 6830.18, "end": 6835.54, "text": " Like we would already expect that the model is able to do very well at so so there's not really much of a difference there because you know", "tokens": [51448, 1743, 321, 576, 1217, 2066, 300, 264, 2316, 307, 1075, 281, 360, 588, 731, 412, 370, 370, 456, 311, 406, 534, 709, 295, 257, 2649, 456, 570, 291, 458, 51716], "temperature": 0.0, "avg_logprob": -0.10059293589197603, "compression_ratio": 2.2644067796610168, "no_speech_prob": 0.004330774769186974}, {"id": 1603, "seek": 683554, "start": 6836.0199999999995, "end": 6839.3, "text": " Almost any reasonably trained model can handle the very simplest environment", "tokens": [50388, 12627, 604, 23551, 8895, 2316, 393, 4813, 264, 588, 22811, 2823, 50552], "temperature": 0.0, "avg_logprob": -0.08234587737492152, "compression_ratio": 1.974910394265233, "no_speech_prob": 0.0019250386394560337}, {"id": 1604, "seek": 683554, "start": 6839.46, "end": 6843.78, "text": " So what we really care about is can we generalize out of distribution to like higher complexity environments?", "tokens": [50560, 407, 437, 321, 534, 1127, 466, 307, 393, 321, 2674, 1125, 484, 295, 7316, 281, 411, 2946, 14024, 12388, 30, 50776], "temperature": 0.0, "avg_logprob": -0.08234587737492152, "compression_ratio": 1.974910394265233, "no_speech_prob": 0.0019250386394560337}, {"id": 1605, "seek": 683554, "start": 6844.18, "end": 6847.14, "text": " And so by biasing the something towards the higher complexity environments", "tokens": [50796, 400, 370, 538, 3228, 3349, 264, 746, 3030, 264, 2946, 14024, 12388, 50944], "temperature": 0.0, "avg_logprob": -0.08234587737492152, "compression_ratio": 1.974910394265233, "no_speech_prob": 0.0019250386394560337}, {"id": 1606, "seek": 683554, "start": 6847.14, "end": 6851.06, "text": " We do show that we're able to generalize further out of distribution to even higher complexity environments", "tokens": [50944, 492, 360, 855, 300, 321, 434, 1075, 281, 2674, 1125, 3052, 484, 295, 7316, 281, 754, 2946, 14024, 12388, 51140], "temperature": 0.0, "avg_logprob": -0.08234587737492152, "compression_ratio": 1.974910394265233, "no_speech_prob": 0.0019250386394560337}, {"id": 1607, "seek": 683554, "start": 6851.3, "end": 6854.42, "text": " Okay, but is there any way of knowing whether it's kind of like", "tokens": [51152, 1033, 11, 457, 307, 456, 604, 636, 295, 5276, 1968, 309, 311, 733, 295, 411, 51308], "temperature": 0.0, "avg_logprob": -0.08234587737492152, "compression_ratio": 1.974910394265233, "no_speech_prob": 0.0019250386394560337}, {"id": 1608, "seek": 683554, "start": 6854.82, "end": 6859.86, "text": " Memorizing the high complexity instances or whether it's still learning abstract motifs and generalizing between them", "tokens": [51328, 8731, 284, 3319, 264, 1090, 14024, 14519, 420, 1968, 309, 311, 920, 2539, 12649, 2184, 18290, 293, 2674, 3319, 1296, 552, 51580], "temperature": 0.0, "avg_logprob": -0.08234587737492152, "compression_ratio": 1.974910394265233, "no_speech_prob": 0.0019250386394560337}, {"id": 1609, "seek": 685986, "start": 6860.179999999999, "end": 6866.259999999999, "text": " Yeah, that's a great question. I think that's a really interesting question generally for ml as a field right now", "tokens": [50380, 865, 11, 300, 311, 257, 869, 1168, 13, 286, 519, 300, 311, 257, 534, 1880, 1168, 5101, 337, 23271, 382, 257, 2519, 558, 586, 50684], "temperature": 0.0, "avg_logprob": -0.1873904445715118, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.005219330545514822}, {"id": 1610, "seek": 685986, "start": 6866.259999999999, "end": 6868.259999999999, "text": " Which is better evaluation benchmarks for", "tokens": [50684, 3013, 307, 1101, 13344, 43751, 337, 50784], "temperature": 0.0, "avg_logprob": -0.1873904445715118, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.005219330545514822}, {"id": 1611, "seek": 685986, "start": 6868.9, "end": 6871.46, "text": " Generalization within different kinds of models", "tokens": [50816, 6996, 2144, 1951, 819, 3685, 295, 5245, 50944], "temperature": 0.0, "avg_logprob": -0.1873904445715118, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.005219330545514822}, {"id": 1612, "seek": 685986, "start": 6871.78, "end": 6877.94, "text": " Um, and like we we alluded to earlier. There's kind of this issue of data leakage between training and test set", "tokens": [50960, 3301, 11, 293, 411, 321, 321, 33919, 281, 3071, 13, 821, 311, 733, 295, 341, 2734, 295, 1412, 47799, 1296, 3097, 293, 1500, 992, 51268], "temperature": 0.0, "avg_logprob": -0.1873904445715118, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.005219330545514822}, {"id": 1613, "seek": 685986, "start": 6878.099999999999, "end": 6883.54, "text": " Which is um, which is definitely an issue that is currently happening with large language models", "tokens": [51276, 3013, 307, 1105, 11, 597, 307, 2138, 364, 2734, 300, 307, 4362, 2737, 365, 2416, 2856, 5245, 51548], "temperature": 0.0, "avg_logprob": -0.1873904445715118, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.005219330545514822}, {"id": 1614, "seek": 685986, "start": 6883.94, "end": 6888.82, "text": " Um, it doesn't take away from the impressiveness of these models because clearly there is a strong generalization", "tokens": [51568, 3301, 11, 309, 1177, 380, 747, 1314, 490, 264, 6729, 8477, 295, 613, 5245, 570, 4448, 456, 307, 257, 2068, 2674, 2144, 51812], "temperature": 0.0, "avg_logprob": -0.1873904445715118, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.005219330545514822}, {"id": 1615, "seek": 688882, "start": 6889.139999999999, "end": 6894.099999999999, "text": " aspect to their behavior, but I do think that in terms of measuring performance on specific benchmarks", "tokens": [50380, 4171, 281, 641, 5223, 11, 457, 286, 360, 519, 300, 294, 2115, 295, 13389, 3389, 322, 2685, 43751, 50628], "temperature": 0.0, "avg_logprob": -0.1381519162977064, "compression_ratio": 1.6344827586206896, "no_speech_prob": 0.0003859403950627893}, {"id": 1616, "seek": 688882, "start": 6894.42, "end": 6897.94, "text": " Um, we really need to solve this problem. How do we have these clean data sets?", "tokens": [50644, 3301, 11, 321, 534, 643, 281, 5039, 341, 1154, 13, 1012, 360, 321, 362, 613, 2541, 1412, 6352, 30, 50820], "temperature": 0.0, "avg_logprob": -0.1381519162977064, "compression_ratio": 1.6344827586206896, "no_speech_prob": 0.0003859403950627893}, {"id": 1617, "seek": 688882, "start": 6898.5, "end": 6900.259999999999, "text": " That allow us to", "tokens": [50848, 663, 2089, 505, 281, 50936], "temperature": 0.0, "avg_logprob": -0.1381519162977064, "compression_ratio": 1.6344827586206896, "no_speech_prob": 0.0003859403950627893}, {"id": 1618, "seek": 688882, "start": 6900.259999999999, "end": 6905.62, "text": " To truly test on inputs that the model hasn't seen at training. Um, I think in the case of", "tokens": [50936, 1407, 4908, 1500, 322, 15743, 300, 264, 2316, 6132, 380, 1612, 412, 3097, 13, 3301, 11, 286, 519, 294, 264, 1389, 295, 51204], "temperature": 0.0, "avg_logprob": -0.1381519162977064, "compression_ratio": 1.6344827586206896, "no_speech_prob": 0.0003859403950627893}, {"id": 1619, "seek": 688882, "start": 6906.42, "end": 6908.42, "text": " reinforcement learning", "tokens": [51244, 29280, 2539, 51344], "temperature": 0.0, "avg_logprob": -0.1381519162977064, "compression_ratio": 1.6344827586206896, "no_speech_prob": 0.0003859403950627893}, {"id": 1620, "seek": 688882, "start": 6908.42, "end": 6912.5, "text": " That's a bit more difficult just because usually we focus on a particular task domain", "tokens": [51344, 663, 311, 257, 857, 544, 2252, 445, 570, 2673, 321, 1879, 322, 257, 1729, 5633, 9274, 51548], "temperature": 0.0, "avg_logprob": -0.1381519162977064, "compression_ratio": 1.6344827586206896, "no_speech_prob": 0.0003859403950627893}, {"id": 1621, "seek": 688882, "start": 6912.66, "end": 6915.0599999999995, "text": " And so there's always going to be some shared similarities within the task", "tokens": [51556, 400, 370, 456, 311, 1009, 516, 281, 312, 512, 5507, 24197, 1951, 264, 5633, 51676], "temperature": 0.0, "avg_logprob": -0.1381519162977064, "compression_ratio": 1.6344827586206896, "no_speech_prob": 0.0003859403950627893}, {"id": 1622, "seek": 691506, "start": 6915.06, "end": 6920.9800000000005, "text": " But obviously, uh, we didn't do this in this paper, but we could try things where we have more um more controlled", "tokens": [50364, 583, 2745, 11, 2232, 11, 321, 994, 380, 360, 341, 294, 341, 3035, 11, 457, 321, 727, 853, 721, 689, 321, 362, 544, 1105, 544, 10164, 50660], "temperature": 0.0, "avg_logprob": -0.148498300920453, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.007299841847270727}, {"id": 1623, "seek": 691506, "start": 6921.620000000001, "end": 6927.9400000000005, "text": " Settings where we you know change one aspect of the environment and really see if it's learning specific causal relationships between", "tokens": [50692, 27286, 689, 321, 291, 458, 1319, 472, 4171, 295, 264, 2823, 293, 534, 536, 498, 309, 311, 2539, 2685, 38755, 6159, 1296, 51008], "temperature": 0.0, "avg_logprob": -0.148498300920453, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.007299841847270727}, {"id": 1624, "seek": 691506, "start": 6928.580000000001, "end": 6930.580000000001, "text": " Things that have to be accomplished in that task", "tokens": [51040, 9514, 300, 362, 281, 312, 15419, 294, 300, 5633, 51140], "temperature": 0.0, "avg_logprob": -0.148498300920453, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.007299841847270727}, {"id": 1625, "seek": 691506, "start": 6930.900000000001, "end": 6934.820000000001, "text": " But we didn't do that. Um, that I actually think would be a really interesting idea for", "tokens": [51156, 583, 321, 994, 380, 360, 300, 13, 3301, 11, 300, 286, 767, 519, 576, 312, 257, 534, 1880, 1558, 337, 51352], "temperature": 0.0, "avg_logprob": -0.148498300920453, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.007299841847270727}, {"id": 1626, "seek": 691506, "start": 6935.46, "end": 6937.46, "text": " A new evaluation environment for rl", "tokens": [51384, 316, 777, 13344, 2823, 337, 367, 75, 51484], "temperature": 0.0, "avg_logprob": -0.148498300920453, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.007299841847270727}, {"id": 1627, "seek": 691506, "start": 6937.9400000000005, "end": 6941.46, "text": " Yeah, I mean the benchmarks thing is just a huge challenge in in machine learning", "tokens": [51508, 865, 11, 286, 914, 264, 43751, 551, 307, 445, 257, 2603, 3430, 294, 294, 3479, 2539, 51684], "temperature": 0.0, "avg_logprob": -0.148498300920453, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.007299841847270727}, {"id": 1628, "seek": 694146, "start": 6941.94, "end": 6945.22, "text": " In general, but just just to kind of round off off the interview", "tokens": [50388, 682, 2674, 11, 457, 445, 445, 281, 733, 295, 3098, 766, 766, 264, 4049, 50552], "temperature": 0.0, "avg_logprob": -0.17882094383239747, "compression_ratio": 1.7576601671309193, "no_speech_prob": 0.03295578807592392}, {"id": 1629, "seek": 694146, "start": 6945.22, "end": 6949.14, "text": " I mean minchie you you were talking about you're doing some work with um edgreff instead and it is amazing", "tokens": [50552, 286, 914, 923, 339, 414, 291, 291, 645, 1417, 466, 291, 434, 884, 512, 589, 365, 1105, 1257, 70, 265, 602, 2602, 293, 309, 307, 2243, 50748], "temperature": 0.0, "avg_logprob": -0.17882094383239747, "compression_ratio": 1.7576601671309193, "no_speech_prob": 0.03295578807592392}, {"id": 1630, "seek": 694146, "start": 6949.14, "end": 6954.26, "text": " I'm getting edg back on and um, you said that um, you've been looking into this kind of the interface", "tokens": [50748, 286, 478, 1242, 1257, 70, 646, 322, 293, 1105, 11, 291, 848, 300, 1105, 11, 291, 600, 668, 1237, 666, 341, 733, 295, 264, 9226, 51004], "temperature": 0.0, "avg_logprob": -0.17882094383239747, "compression_ratio": 1.7576601671309193, "no_speech_prob": 0.03295578807592392}, {"id": 1631, "seek": 694146, "start": 6954.58, "end": 6956.74, "text": " Between humans and machine learning. Can you tell me about that?", "tokens": [51020, 18967, 6255, 293, 3479, 2539, 13, 1664, 291, 980, 385, 466, 300, 30, 51128], "temperature": 0.0, "avg_logprob": -0.17882094383239747, "compression_ratio": 1.7576601671309193, "no_speech_prob": 0.03295578807592392}, {"id": 1632, "seek": 694146, "start": 6956.9800000000005, "end": 6962.1, "text": " Yeah, so just to not say too much about it because um, it's related to current work that's happening at DeepMind", "tokens": [51140, 865, 11, 370, 445, 281, 406, 584, 886, 709, 466, 309, 570, 1105, 11, 309, 311, 4077, 281, 2190, 589, 300, 311, 2737, 412, 14895, 44, 471, 51396], "temperature": 0.0, "avg_logprob": -0.17882094383239747, "compression_ratio": 1.7576601671309193, "no_speech_prob": 0.03295578807592392}, {"id": 1633, "seek": 694146, "start": 6962.5, "end": 6966.34, "text": " Um is just that you know, I think from personally from a high level point of view", "tokens": [51416, 3301, 307, 445, 300, 291, 458, 11, 286, 519, 490, 5665, 490, 257, 1090, 1496, 935, 295, 1910, 51608], "temperature": 0.0, "avg_logprob": -0.17882094383239747, "compression_ratio": 1.7576601671309193, "no_speech_prob": 0.03295578807592392}, {"id": 1634, "seek": 694146, "start": 6966.66, "end": 6970.66, "text": " I'm very interested, you know talking about this divide sort of this fork in the road in terms of", "tokens": [51624, 286, 478, 588, 3102, 11, 291, 458, 1417, 466, 341, 9845, 1333, 295, 341, 17716, 294, 264, 3060, 294, 2115, 295, 51824], "temperature": 0.0, "avg_logprob": -0.17882094383239747, "compression_ratio": 1.7576601671309193, "no_speech_prob": 0.03295578807592392}, {"id": 1635, "seek": 697066, "start": 6970.98, "end": 6976.099999999999, "text": " What's the path to open studying open-endedness studying it in silico or studying it in", "tokens": [50380, 708, 311, 264, 3100, 281, 1269, 7601, 1269, 12, 3502, 1287, 7601, 309, 294, 3425, 2789, 420, 7601, 309, 294, 50636], "temperature": 0.0, "avg_logprob": -0.14895901237566447, "compression_ratio": 1.756198347107438, "no_speech_prob": 0.003592662513256073}, {"id": 1636, "seek": 697066, "start": 6976.66, "end": 6980.58, "text": " situ in the setting of an actual open-ended system like a user", "tokens": [50664, 2054, 294, 264, 3287, 295, 364, 3539, 1269, 12, 3502, 1185, 411, 257, 4195, 50860], "temperature": 0.0, "avg_logprob": -0.14895901237566447, "compression_ratio": 1.756198347107438, "no_speech_prob": 0.003592662513256073}, {"id": 1637, "seek": 697066, "start": 6981.46, "end": 6986.5, "text": " App interaction or you know the interaction between a user and a piece of software on the web", "tokens": [50904, 3132, 9285, 420, 291, 458, 264, 9285, 1296, 257, 4195, 293, 257, 2522, 295, 4722, 322, 264, 3670, 51156], "temperature": 0.0, "avg_logprob": -0.14895901237566447, "compression_ratio": 1.756198347107438, "no_speech_prob": 0.003592662513256073}, {"id": 1638, "seek": 697066, "start": 6987.38, "end": 6990.099999999999, "text": " Or potentially with many other users. There are such rich", "tokens": [51200, 1610, 7263, 365, 867, 661, 5022, 13, 821, 366, 1270, 4593, 51336], "temperature": 0.0, "avg_logprob": -0.14895901237566447, "compression_ratio": 1.756198347107438, "no_speech_prob": 0.003592662513256073}, {"id": 1639, "seek": 697066, "start": 6990.9, "end": 6997.54, "text": " Existing systems online that are already open-ended because they amplify or connect the creativity and knowledge of humans", "tokens": [51376, 2111, 468, 278, 3652, 2950, 300, 366, 1217, 1269, 12, 3502, 570, 436, 41174, 420, 1745, 264, 12915, 293, 3601, 295, 6255, 51708], "temperature": 0.0, "avg_logprob": -0.14895901237566447, "compression_ratio": 1.756198347107438, "no_speech_prob": 0.003592662513256073}, {"id": 1640, "seek": 699754, "start": 6998.18, "end": 7004.18, "text": " To create more knowledge and more creative artifacts. And so I think what's really uh, interesting in my mind now is sort of studying", "tokens": [50396, 1407, 1884, 544, 3601, 293, 544, 5880, 24617, 13, 400, 370, 286, 519, 437, 311, 534, 2232, 11, 1880, 294, 452, 1575, 586, 307, 1333, 295, 7601, 50696], "temperature": 0.0, "avg_logprob": -0.135727344675267, "compression_ratio": 1.724, "no_speech_prob": 0.007112640421837568}, {"id": 1641, "seek": 699754, "start": 7004.9, "end": 7009.06, "text": " Systems or algorithms that allow us to better steer the creativity of humans", "tokens": [50732, 27059, 420, 14642, 300, 2089, 505, 281, 1101, 30814, 264, 12915, 295, 6255, 50940], "temperature": 0.0, "avg_logprob": -0.135727344675267, "compression_ratio": 1.724, "no_speech_prob": 0.007112640421837568}, {"id": 1642, "seek": 699754, "start": 7009.7, "end": 7011.7, "text": " As they are mediated by software", "tokens": [50972, 1018, 436, 366, 17269, 770, 538, 4722, 51072], "temperature": 0.0, "avg_logprob": -0.135727344675267, "compression_ratio": 1.724, "no_speech_prob": 0.007112640421837568}, {"id": 1643, "seek": 699754, "start": 7012.42, "end": 7015.38, "text": " And basically allow us to essentially amplify", "tokens": [51108, 400, 1936, 2089, 505, 281, 4476, 41174, 51256], "temperature": 0.0, "avg_logprob": -0.135727344675267, "compression_ratio": 1.724, "no_speech_prob": 0.007112640421837568}, {"id": 1644, "seek": 699754, "start": 7015.94, "end": 7023.06, "text": " Existing intelligent or creative systems that are open-ended so amplify existing open open-endedness rather than try to build it from scratch", "tokens": [51284, 2111, 468, 278, 13232, 420, 5880, 3652, 300, 366, 1269, 12, 3502, 370, 41174, 6741, 1269, 1269, 12, 3502, 1287, 2831, 813, 853, 281, 1322, 309, 490, 8459, 51640], "temperature": 0.0, "avg_logprob": -0.135727344675267, "compression_ratio": 1.724, "no_speech_prob": 0.007112640421837568}, {"id": 1645, "seek": 702306, "start": 7024.02, "end": 7028.580000000001, "text": " Amazing guys. It's been an honor to have you on MLS T. Thank you so much. Thanks. Thank you. Yeah", "tokens": [50412, 14165, 1074, 13, 467, 311, 668, 364, 5968, 281, 362, 291, 322, 376, 19198, 314, 13, 1044, 291, 370, 709, 13, 2561, 13, 1044, 291, 13, 865, 50640], "temperature": 0.0, "avg_logprob": -0.1654767632484436, "compression_ratio": 1.2115384615384615, "no_speech_prob": 0.023602576926350594}, {"id": 1646, "seek": 702306, "start": 7029.38, "end": 7031.38, "text": " Great cool. Yeah, we're done", "tokens": [50680, 3769, 1627, 13, 865, 11, 321, 434, 1096, 50780], "temperature": 0.0, "avg_logprob": -0.1654767632484436, "compression_ratio": 1.2115384615384615, "no_speech_prob": 0.023602576926350594}], "language": "en"}