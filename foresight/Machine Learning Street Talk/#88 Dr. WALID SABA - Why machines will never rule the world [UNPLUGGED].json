{"text": " Welcome back to Machine Learning Street Talk. I'm your host, Tim Scarf. Now, today on the show, we're joined by Dr. Walid Saber, who's just written a review of the book Machines Will Never Rule the World, Artificial Intelligence Without Fear, written by Jops Langreb and Barry Smith. Now, Dr. Saber will be discussing his review, which provides a detailed analysis of the book's arguments that strong AI is impossible. In his review, Dr. Saber acknowledges the argument made by Langreb and Smith that anything we engineer is ultimately a system which can be mathematically modelled and described. He then goes on to discuss the complexity of modelling mental processes, which the authors argue are dynamic, adaptive, continuously evolving, and constitutes systems whose behaviour affects and is affected by the environment they function in. He also touches on the notion of granularity, arguing that complex systems are all the way up from specific components of the mind to the mind itself and that no known mathematics can model them. Dr. Saber then delves into the complexities of language and open interactive dialogues, asserting that language is a prerequisite for any artificial general intelligence, but that linguistic communication itself is a complex system that no mathematics can model. He doesn't subscribe to the argument that interactive bots can be built in narrow domains, since responses and the overall context cannot be predicted in any meaningful way. Dr. Saber has two reservations as to the conclusions made by Langreb and Smith. He questions their use of the word never and suggests there could be a new mathematics that mental processes require that is yet to be discovered. He also doesn't believe that the fact that complex behaviour cannot be mathematically modelled precludes the possibility of building such systems, as evidenced by the intentional programming language LISP, and he also considers the possibility of hypercomputation in validating the church-touring hypothesis. We'll talk about that a bit later. Finally, Dr. Saber expresses his regret that the book didn't go into further detail on the frame problem in AI. He calls for further research into belief revision in complex systems. Join us today as we speak with Dr. Wallid Saber about his review of this book that we've just been speaking about. Machines will never rule the world. And by the way, I think very highly of Wallid. I think he is one of our most loved guests. He is a polymath. He has an incredible breadth of knowledge across so many fields, you know, from AI and computer science, to mathematics, to philosophy, to linguistics. He really is a rare breed, and he also brings a very interesting contrarian view, I would say, to the current kind of modus operandi, or the zeitgeist in the community at the moment. He's a breath of fresh air. Anyway, if you haven't already, consider subscribing to our YouTube channel, or indeed rating our podcast on your favourite podcasting platform if you happen to be listening to us. Anyway, without any further delay, I give you Dr. Wallid Saber. Welcome back to MLST, folks. We have the unmistakable Wallid Saber, the legend that is Wallid Saber, but we also have Mark from our Discord community. Mark, would you like to introduce yourself? Hi, guys. I'm Mark Aguil, a philosopher, cognitive scientist, and software engineer at MLST. Awesome. Welcome, Mark. You know, things have been a bit of a blur, but I think this is right, Tim. I think Wallid was the first, or one of the first, really big names that we had come on the show, right? I don't think so, but the most controversial, let's put it this way, the one that made probably, it was so predictable, what was, I mean, probably the first one that broke that predictive model. Well, I just remember, I mean, I remember Tim and I being like super, because we didn't know you, right, before then, and I remember us being like so excited that you agreed to come on the show. You know, of course, now that we know you, we're kind of like, ah, whatever, it's just Wallid. We're just through, by the way. We were following all over ourselves that you were coming on the show, we're like, oh my god, this is so awesome. To be honest with you, I never thought I would, I would, yeah, I never thought I would have, my opinion would matter that much, to be honest with you, and it all started by creating this medium blog, and I started spitting out stuff that, hey, do you guys know that there's dissonance in that? And I was surprised how much it, even by people that are living off and making a living, and they preach and write papers on what I'm attacking, and they would say, don't mention my name, by the way, it's all private messages. But apparently, I said a couple of things that touched people, but you know. Yeah. Yeah, I was at New York's last week, and the amount of people that came up to me and said, I love the show, Tim, Wallid's my favorite guest that you've had on, because Wallid just provides a completely different perspective, because we're bred on empiricism and neural networks. And part of the reason I want to get you back on, Wallid, is to counteract some of the, I mean, we've been speaking to a lot of deep learning people recently, so we need to counteract that a little bit. Yeah, that one shocked the hell out of me. I mean, I have people like myself at this event. I mean, if you said ACL, maybe, yeah, okay. But I mean, this is the deep learning meeting, right? I mean, so I was shocked. But yeah. Yeah, indeed. Although I have some more positive views of DL now. Oh, go on. Yeah, I mean, look, I breaking news, breaking news. Positive, although I still have my reservations as to AGI and all that stuff. But I have been completely impressed with the developments in large language models. I have to admit that sometimes I say, what the hell is this? Now, technically, let me tell you what's happening. And I'm working on something that probably would quantify where can this go? How much can you, how much will scale? The bottom line is this, these guys have impressed the hell out of me, and they have proven that scale does matter. I mean, now these large language models, if you take language from lexical to syntactic to semantic to pragmatic levels, they have definitely mastered syntax. And this is not a small feat. I mean, this is huge. They have proven that if I read tons of texts written by humans, I can figure out the grammar of language, and they have done that. That's huge. Okay, so and I don't like here where I don't like people. I don't want to mention names, but people that supposedly are in my camp, right, insisting on refusing to see the elephant in the room. No, large language models have proven that if I ingest terabytes of text, I will figure out syntactic rules. They have done that. Now, okay, and here's where technically, and you have to admit, I mean, they, as a matter of fact, they probably know syntax now more than many college graduates. Okay, these are from data. That's a huge experiment in cognitive science that no matter how, how, I mean, you can't be religious in this, you have to be scientific. I see the proof that these large language models by ingesting tons of text written by humans, they have figured out the syntax of language. End of story. I have, there's an existential proof. Go on, open AI, try DaVinci 2 or DaVinci 3 or whatever you like to try. Their syntactic competency is beyond belief. I'm shocked every time I use it. Okay. Now, that's not the end of language understanding. There's semantics, and then there's pragmatics. Wow. I mean, so I'm working on something to quantify. So now we have, I don't know, we're up to a trillion parameter that allowed me to master syntax. Now, let's see semantics. And semantic can be broken down to, have you guys figured out reference resolution? Have you guys figured out scope resolution, prepositional phrase attachments? There are so many pain points and semantic processing. Can we quantify how many more parameters we need to conquer semantics? And then pragmatics, there are things like, the teenager shot a policeman and he immediately fled away. Now, possibly both can, the he can be the policeman. He fled away to escape further injuries. I mean, that can happen. But most likely the one that's led away is the teenager. That's pragmatics because we know in the world we live in, if you shoot someone, they're going to try to capture you and you try to flee, right? That's not semantics. That's way beyond. How many parameters beyond semantics do you need to capture that? If you can put a, if you can come up with a rough number, I mean, it could be a number that's manageable, that's doable by more scaling, which would be an interesting result. But it could be that it's a number beyond the universe we live in, which means guys, except that you can master syntax and a little bit of lexical semantics, you can figure out the meaning of some words, but to do full understanding with pragmatics, we're talking about numbers that we might have to wait 2000 years. Yes, in theory, it works. So basically, I'm trying to work now on which is going to be very difficult to quantify because they have proven that scale did improve syntax, no doubt, not improve it. They've almost mastered syntax. But how far can this go? I mean, can you quantify how far can this go scientifically without saying, let's try with more, let's try with more? Which is, which is not, it's not going to be easy to do. Anyway. So I'm curious, could I push back a little bit on the syntax you said, if they've mastered syntax, and I'm kind of, okay, I mean, I guess you have an existing proof, and there's like a behavioral kind of proof that it does seem to have very syntactic sentences. And obviously, if you're in something with whatever billion of parameters, but would you say what those rules are? Could we write them down? Probably not, right? Because it's a billion different numbers of weights. No, no. And you don't have the old school kind of generative grammar approach. And also, it's not the way humans have learned language. And could you reverse engineer or tweak it? We don't know what it is, it's a black box. So, okay, there's a behavioral tense in which it knows. But isn't it the way humans do learn language? I mean, I think it's more, though, it's more related to the way humans learn language than, I mean, I was 20 until I knew grammar. I mean, we use language without, without knowing grammatical rules. So there is an argument that humans don't learn grammar, that it's, you know, pretty native. And all we do is tweak a few parameters. And then we add vocabulary over the years, tweaks or whatever. That's the, you know, the Chomsky and generative grammar approach. So I don't think that we have a billion parameters that we tweak over 20 years. I don't think that's how we work. And we have, I mean, there's amazing competency of children at two years of age, you know, with language they have, you've said, you've, you're writing, you pointed this out, actually, children know, is absolutely amazing, you know, straight out of, you know, it is amazing, but it is amazing. And I, and I didn't change the way I think about and we have innate stuff. But here's the change that these guys have made me go through. It's a minor change, but it's, it's, it's not that subtle, actually. Here's the thing. I was told before these new results are coming out that look, we do have innate stuff, which took us three, four hundred thousand years of evolution. All we're doing by ingesting all this text is we're simulating, right, these 300,000 years. So give us a chance to simulate this innateness if you want, in a way, in a way. Okay, I, that argument was said long time ago, and I thought, come on, you're chasing infinity. What happened with the real difference in my mind now is they have proven that they conquered one beast in language. Nobody can dispute that. Can I have a go at disputing it? So, in the Polition and Foda connectionism critique, they spoke about productivity, you know, the infinite cardinality of language. There was recently a deep mind paper talking about the Chomsky hierarchy and deep neural network. Well, I mean, RNNs are regular languages, but, you know, I think transformers and the rest of them are at the bottom of the hierarchy. So quantitatively, we know we haven't conquered infinity. So why with such a shallow horizon, are they doing so well? I agree. Here's the thing, language use, languages infinite, but probably the long tail of probably 90% of ordinary language use, right, can be figured out from the stuff that we write. So they will never capture all of language. Yes, but they might reach the level of a competent educated man like us in language competency. So, all I'm saying is what they have achieved is a huge result in terms of the big question of scale and big data. They have definitely proved that if I see enough data, I will learn something and something that's not trivial. Look, you know where I'm coming from. You're talking Foda and Polition. I mean, you're preaching to the choir, right? But I have to be a scientist too. I mean, I don't like, I'm following Gary Marcus, and he's like, I don't like people that minimize what happened. I'm a scientist, right? I see a big result. I say, wow, right? And look, we're talking, nobody bashed deep learning more than me, especially large language models. I mean, I'm like, I was saying this is silly, right? But I have to say they have proven something to me at least, which is huge because I know how difficult language is. I am impressed equally. Wouldn't you say it's an engineering, an engineering triumph rather than a scientific? It's an engineering triumph. But here's the point, Mark. I think it's a little bit more. That's the only thing I'm trying to, I'm not saying, look, I didn't give up on, I can get to the criticism later. So don't put me on in that camp yet, right? Or, or ever, right? Because I know, I know theoretically, theoretically, mathematically, you cannot understand language this way. All I'm saying is, in terms of cognitive science, what happened and what is happening as we speak is not nothing. It's a huge, for example, if I can ingest a lot, again, what they prove is, is that well, the two are related. So it's one thing scale from tons and tons of data. I can learn something that is not trivial. That to me has been proven. The point I'm making is not the point I'm making is not that they solve the language problem. Sorry. Yeah, I want to jump in here a little bit. Because from my perspective, I think part of why you're saying it's huge is because I think it was a huge step for you personally. Because I know, you know, from the past, like talking to you, like you've had a much more extreme view, you know, on the capabilities of large language models than, for example, myself. Because for me, I don't see anything new here. It's kind of like, I'll give you an example outside of syntax just for a moment. So just transcription, because that's what Tim and I happen to be working on quite a bit right now. In other words, transcribing audio into text. All the state-of-the-art models are pretty much sitting around each other at about 90% you know, accuracy right of transcription. But here's the thing is that's for people speaking relatively common languages with a relatively standard accent. Okay, as soon as you bring someone in the room that has an accent or speaks a, you know, with maybe like some type of a challenge, like a speech challenge, or this side of the other thing, it becomes garbage again. And like for Tim and I, or there's noise in the background, music playing in the background. And as Tim and I have probably hammered, you know, to death and beaten a dead horse on our channel like so many times, we've never doubted that machine learning can learn like the bulk of the curve, where it really, really struggles is in all these edge cases, and the corner cases, and the periphery where it can easily, it's very brittle, right, in those kind of areas. Like this is the point we've been making out for a long, long time. And so the fact that like massive trillions of parameters and terabytes of data was able to learn 90% or more, 95% or whatever is syntax. Okay, I get it. Like from a linguist perspective, that was a, you know, maybe a big triumph or something. But I'm still always about that other like 5%. And the problem with the approach of deep neural networks is to get that other 5% is like 100 times as many more parameters, whereas like using, whereas using more generalized, abstracted, you know, methods that we haven't yet really discovered. You're hitting it on the nail. And that's why I'm working out on quantifying this because now we are doing exponential growth in the number of parameters for not even linear growth in the accuracy, even logarithmic, I agree with you 100%. So that other 10% might require 2000 years of data that we don't even have. That's what I'm working on. How far can this go because the function is against them now? Like, I mean, we're increasing GPU power and the number of data that we're ingesting exponentially for a minute increase in accuracy, which is, right, that's the end of the logarithmic. And this is, this is part of the announcers that we have to go through. So look, all my reservations that I had before apply. So I'm being misunderstood. All I'm saying is simple. These guys, what they have done is not as trivial as I thought initially. Okay, so let me, let me really be very careful in what I'm saying because now I have a following. I don't want to lose it. No, I'm not, I'm not changing scientifically where I was. I mean, science is science. And I know theoretically, I don't get into things like intentionality and these models understand nothing about the word. I'm talking about syntax only, by the way, syntax on and some coherence when they patch things together. The coherence is amazing. They're not patching things together that don't relate at all. So I'm talking about syntax and, and coherence and syntax. Okay. And a touch of semantics, right? My point, let me repeat it so that I'm not misunderstood. They have proven something that many cognitive scientists would never accept, never ever. But this existential proof has told many cognitive scientists, don't dismiss learning from data only, blind, no labeling, some aspects of language, actually very impressive aspects of language. These guys have proven that. And me as a cognitive scientist, I have to admit because I see it. I see from data alone, these systems have learned non-trivial aspects of language. Now, how do you interpret that? Where do you take it? What do you conclude from it? We can, we can debate that. But all I'm saying is, I have seen something that I never thought I would see, that just ingesting text in these deep networks, you can actually figure something not trivial about language. That has been done. I mean, you can, you can say there are pigs, pigs that fly. Okay. Prove me wrong. I saw them prove they don't exist. Well, I can. But existential proofs are the most powerful proofs. It's an existential proof, proof by doing. I'm showing you language competency by ingesting text on it. So this dismissive all these are, what is the phrase that Melanie uses? Not Melanie. Stochastic parents. No. Oh, Bender. Emily Bender. Emily Bender. No, these are not stochastic parents anymore for me. I am seeing, look, if I go through the tests on conducting, I have 20 pages of tests on every aspect. And they get better. I mean, I am seeing things that lexical ambiguity, they've almost resolved it like, we were at the baseball stadium last night, we had a ball. They knew that ball is not the baseball. I'm seeing things like, what the hell is this? And if anybody can test these systems, I can with all humility. I'm trying my best now to make them fail, which was not the case just a month ago. All I'm saying is I'm seeing something that I never thought I would see as a cognitive scientist, as a computation linguist. Let me put it this way. To see this capability now, you have to bring back Montague, Frigge, Marvin Minsky, John McCarthy, all the pioneers of logic and AI, put them together and give them a thousand bright engineers. And they will not do this. In a minute, we're going to get onto your book review, but you are just alluding to the problem of semantics and pragmatics. And also I want to bring in symbol grounding as being the next potential brick walls. Could you just talk to that a little bit more? Well, look, symbol grounding was an issue in symbolic systems. You're using symbolic systems. So you're saying cat, CAT, it's reference based semantics. So I'm going to use CAT to refer to a concept called CAT. And then the concept called CAT is a frame in most systems, in frame based systems with properties. It's a mammal. It's a thing that has this and this kind of fair whiskers, blah, blah, blah. It's the intention of what a CAT is. And then symbol grounding came like, okay, you're defining CAT as symbol in terms of symbols. Like, so where do we go? It's like a dictionary to read the definition of a word. I have to know all the words. So I might go and so it's a cyclical representational system. It's not grounded in anything in the end. It's a closed. Basically, it's a system that defines itself, like what the hell's going on here, right? Symbol grounding was CAT has to be associated with something real outside. That's a real CAT. In symbolic systems, we don't have that, right? We can get into symbol grounding. It's a huge subject on its own, like where do meanings, where do words get their meaning from? Is it embodied? Is it experiential? Does it have to be can a deaf and a blind person ever understand the meaning of something? So that's a huge... I mean, we spoke to Andrew Lampinen and he was getting into Pierce's triad semiotics, this embodied relativistic notion of grounding, which actually I'm developing a bit of a taste for personally, but you're very skeptical about that. Could you just sketch that out? I don't think that's the issue grounding. I mean, people make a lot of it and like our common friend, Bishop, Mark Bishop, that you will never understand the meaning of something if you don't live in the environment and it's... That has never been... I don't believe so. That's why we call it artificial intelligence, right? I mean, we're never going to have the intelligence of a human being. We're never going to have a robot that really chokes when they see their nephew after six years, right? I mean... And that's... That was never the one. That's why we're building artificial intelligence, not human intelligence. So this whole argument about grounding and embodiment and I will never understand what pain is because a robot will never really feel pain. That to me, that's besides the point. I'm not building artificial life. I'm building an artificially intelligent machine that will do things in a way that you would say, what the hell was that? Probably that's how AR should be defined. That's it. What the... Who did this, right? That's it. It feels pain or it doesn't feel pain or it will never know what crying is, like so. So at least I come from this angle. I'm not into building artificial humans. I'm an engineer. I'm into building artificial intelligence systems. Systems that can reason, right? In the environment we live in, solve problems intelligently and problems that usually require human intelligence. Like I'm into... I would like to see a world where we don't have accountants. Come on. We don't have doctors. I open my mobile. I have a doctor. I converse with them intelligently and they tell me exactly what to do. Done. Nobody goes to medical school anymore. Everybody should write poetry and play music and enjoy the beach. That's it. That's the AI I'm interested in. We will never build robots that will understand love. So to me, these are arguments that they're irrelevant. We're building artificial intelligence. When we did calculators, we never gave a damn how we do it in the mind. And we have calculators that can beat any mathematician in doing a division of two prime numbers, each of which is 20 digits. Yeah, I think it's more where your area of interest is. I mean, so yours is in the engineering. And what's coming to mind right now is our conversation with Professor Chomsky where he said, yeah, these are great feats of engineering. I mean, I like bulldozers too. They just don't have anything to do with science. Sure, or philosophy for that matter. I mean, I think some of these questions, yeah, maybe they don't have a lot to do with building AIs that do a bunch of useful things. But they have a lot to do with philosophy or science or whatever. Mathematics for that matter. This is a bridge to the book because the authors were misunderstood from their title, and I told them that privately. No, not privately. I want to tell them that because I got comments from people privately that the title is misleading. The title assumes they are anti AI, not really. They're saying roughly what I'm saying. I'm not interested in building an artificial human. We can never do that problem. Right. So all this, and this is important because people are trivializing. I mean, you have people talking about AGI from five years ago. And all we had was something that can do amazing pattern recognition. That's it. So it's important for us to say, Hey guys, cool it down. Do you know what you mean when you talk about machines that surpass human intelligence? This is not just a word you throw out, because you're impressed with a system that can recognize cats from dogs. Come on, take it easy. Slow down, right? And this book is about that. It's like, do you know what it means to have a system that can feel and react instantly real time to changing situations around them? And do you know what you're talking about? So yeah, I'm interested in the engineering side of AI. And that's what makes me impressed by something like a DaVinci 2 or DaVinci 3 as an AI enthusiast. I look at this and I say, wow, we've never been able to reach this milestone. This is a huge milestone. That's how I look at will it be, will it be the solution for the language understanding problem? No, because language understanding in the full sense of the word understanding, the way we speak now, the way we were speaking now involves a lot more than mastering syntax, but they did master a big aspect of language. And I can see it. I can try it. And I'm trying to make it fail now in syntax and even some coherence, some mild, let's call them mild semantics. And it's very impressive. So the question now becomes for AI researchers, not just engineers, is this scalability scalable? Is this scalable? Is this approach scalable? So much data and so much compute power, they mastered syntax more or less. I think they did at least as much as a competent language user. So my first question is like, do you see natural language as essentially computable as in like cheering machine computer? Or do we need some other kind of new mathematics to describe it? For example, hypercoputation, whatever that might be. In other words, is there a generative grammar or algorithm or set of rules that generate our valid sentences? When it comes to language itself, I think language is a formal language. There is a compiler for natural language that can be built, like we have built one for Java, C sharp. So this is Montague, yeah. Yeah, I believe Montague was right, although Montague was was attacking the semantics part. Okay, he touched a little bit on intention and then not much on pragmatics, but Montague and it took me years. And I had to be advised by a very smart philosopher, logician, that stop saying Montague didn't deal with this. Montague was never in the business of reference resolution. That's pragmatics. Montague was trying to prove there is a formal system and algebraic system. And he used lambda calculus, strongly typed system that I can use to compose language like I do with arithmetic or calculus or anything. There's a logic that are mathematics for language, which is a huge thing. Montague was not a trivial semanticist in the history of language. He was huge. So would you say that Montague is doing for language semantics of language, what chance he did for syntax of language? Exactly, exactly. And the common denominator interesting between them is someone that Chomsky himself admires a lot Barbara Partee, who was he did her PhD with Montague. She's a Montegoian, Montague semantics. But she and she, she did say almost the same phrase. He said what Montague did for semantics is equivalent to what Chomsky did for syntax. Yes. Okay, exact, almost exact phrase. You think he was right about semantics? Yes. So for example, there's a computable definition of what is a pile of sand. Right. No, no, no, no, no, no, no. I'm not sure you're right about that. No, no, no, no. Hold on. Let's not get Montague was not a psychologist or an ontologist or he said whatever your meaning for something is. Okay. He didn't even care. Montague never did ontology and conceptual and like, what, how do you define the meaning of what is a book? Look, let's, and took me, I'm telling you, took me three years to appreciate what Montague was doing. And I, and my thesis was on Montague semantics, the masters before the PhD. Here's what Montague did, Keith, and you'll appreciate Montague said, whatever your meaning for the individual words, the lexical meaning. So cat means see, okay, you go with your psychologist and cognitive scientist and ontologist and disagree about the meaning of a cat. Finally, you come to me and you say, we have a meaning for cat and it's see. Follow me. Montague never cared about what is the nature of things outside. Right. Whatever your meaning for these individual concepts are. Right. Here's how you make, you get the meaning of a whole mathematically. I'll give you a simple example that will make you appreciate what I'm talking about. John refers to a person. Right. The neighbor next door refers to a person. The neighbor next door that just moved from California refers to a person. The neighbor next door that knows John very well and drives for the LTD is a person. All of how can you have this phrase and John refer to a person and have the same semantic type composition in a way that never fails. Like you do in arithmetic, he wanted to prove that natural language is a formal language. He developed a semantic algebra that makes this long phrase referred to the in the end to an object that has the same semantic type as John mathematically. If you do it, it never fails. The details of this were genius. Okay. So Montague then made the big claim natural language is a formal language. Give me some time. I'll work out the full algebra. You go then and decide what the individual meanings are. I don't care. Montague never gave a damn about cognitive science and knowledge and he was a logician. He wanted to prove there's a calculus underneath natural language. Calculus of meanings. You decide on the meaning. I'm telling you, it took me a while. I thought he's doing semantics. What is the meaning of this and Montague? He said he never cared. He was doing an algebra of meanings, regardless of what the meanings are. Okay. But his project was huge. Montague was trying to prove there's an algebraic system behind language, like any other formal language. Like you can get an arithmetic expression and build a tree for that, evaluate it, and get the final meaning. Natural language works the same way. Except it's not that simple. That's all. So his project was huge and he was misunderstood. So he was really doing semantics. That's semantics. Pragmatics is a different thing. What do you think is the core unique property of natural human language? Would you agree with Chomsky on it being digital infinity? Yeah. Okay. The infinite thing in the productivity. To me, no, it's I'm half Chomsky and half something else. To me, no, the real, real unique thing about language. And that's why even if Montague succeeded, that's half the battle. It's not in the semantics, although that's huge. To me, it's the pragmatic side, the abductive inference. I mean, we use induction and we use deduction and we always ignore abduction. Abduction is the unique, is the humanly unique reasoning capability. I mean, rats do inductive reasoning. They, to a certain extent, that's how they learn a few things, inductively, really. All the lower species do inductive reasoning to a certain extent. And some of them do some deductive reasoning, if there's then this, but at a very shallow level, of course. Abductive reasoning is uniquely human. And that's the part of language understanding, which means reasoning to the best explanation. Abductive reasoning is I reach a conclusion, not inductively by induction or, and not deductively, I deduced it. But I reached this conclusion because it's possible, it can happen. And it is the best conclusion I can come up with, given everything else I know. Abductive reasoning is the real reasoning methodology that makes us unique as human. We reason to the best, we reason, it's called reasoning to the best explanation. Right? So that's, that's, Pierce and others, I mean, Pierce was the pioneer of abductive reasoning or abduction. But I'm talking about an abduction has come to have two sort of tracks. And there's abductive reasoning in the traditional philosophical charts, Pierce. But there's abductive reasoning as it used to be called in the 80s, when case-based reasoning came out and expert systems who, there was something called EBL, explanation-based learning. And it was even a learning technique, which is really reasoning to the best explanation. Basically, I have to make a decision. Actually, Jerry Hobbs, you guys heard me mention his name several times before, who's, I think, huge in semantics, has a paper when he was at SRI with other luminaries too. The title is interpretation as abduction or understanding as abduction. And basically, he shows how all the difficult, all the challenges in language understanding beyond semantics. So we're done with Montague. Now I'm doing the final understanding of what makes sense given, because every expression has several meanings. Even if I did the semantics perfectly, I have to choose the most plausible meaning from all the possible meanings. That's pragmatics. And the way you do that very well is in language. We do abductive reasoning. We say, I'm left with three meanings, three possible meanings, syntax excluded, 200 syntax trees, semantics excluded, few invalid semantic expressions. And I'm left with three still, three possible meanings. They can all happen in the world we live in. Which one is the most plausible? We do this abductively. Which meaning is the most likely meaning given the context and what I know? That's the last challenge in language. So we need to, we need to add the abductive model, which we humans do. I go back to the teenager shot of policemen, both meanings, both interpretation can happen, right? Either one can flee, right? But most likely it's the teenager that fled away, given what I know and given that's abductive reasoning. But semantically both can happen. Yes, I do want to emphasize something that Wally like briefly mentioned, but I think it's very important to mention is that there's two senses of abduction. And they differ in the following way, which is kind of the more modern sense, which is what Wally's been talking about like pretty much this whole time, is abduction used to justify hypotheses. But the older and original sense of it and still an equally important one is abduction for generating hypotheses. And this ability to generate hypotheses is something that's extremely powerful and so far uniquely human. But generate from... Hold on, let me just finish here. Which is this, this is like something where Einstein is just sitting there pontificating on how the heck can light be the same no matter how the earth is moving and blah, blah, blah, and comes up without a thin air, like this hypothesis that relativity applies, right? That the physical laws are the same no matter what your reference frame is. So this ability to almost... That people talk about sort of pull from thin air, this kind of intuitive leap to something that ends up being like a grand new theory, that's also abduction. Right. But in both cases, Keith, and I agree with you, that's the old view of what abductive reasoning was to scientists. But in both cases, you're choosing from possible... Oh, no, no, no, just a minute because this is where I think I probably quite disagree with you, which is the modern sense of abduction to me is much more similar to just inference like to a Bayesian. So in other words, you give me a whole slew of hypotheses and I can tell you which hypotheses should be preferred just on the basis of marginalization and strict, like Bayesian theory, no problem with that. It's not actually abduction, it's just inference, right? Just rules of inference. Whereas just a minute, generating that space in the first place is unique and very different from inference, like the ability to produce from nothing models to consider, that's the core of abduction from my point of view. But okay, so we're saying the same thing, but indifferent. These possibilities that you generate are valid possibilities. So abductive reasoning... I don't know if they're valid until I do the inference. No, you're generating a pool of possibilities. That's the step, generating a pool of possibilities. Fine, fine, fine, fine, but in the end... How do you do that? Keith, I think we're saying the same thing, it's just a terminology. In the end, you're choosing from a set of possible valid hypotheses. Induction is, you don't know where you're going until you get there. In abductive reasoning, you are, whether it's the old way or the modern way, in the end, what's common between them is, I have a set of possibilities. I will use abductive reasoning to decide which is the most plausible. In a sense, you're scoring them, and you're saying, from all these possibilities, this is the most plausible. Yeah, but see, you keep assuming the... You keep positing that you have a bunch of possibilities, and I'm saying those possibilities have to come from somewhere, and where they come from is abduction. Oh, okay, it depends on the domain and language. They come from what we know is true. Okay, I see your point. Where they come from depends on the domain of reasoning. In many cases, they come from what we know is true, or they come from evidence. Yeah, I guess it's just important to know there are these two senses of abduction, and don't forget about both of them, because they're both... Right, and that's why abduction, like induction, as opposed to deduction, abduction and induction are both approximate. You can never have 100%, because in the end, you're assigning a score, you're saying. So, both of them are probabilistic in a way, or they have a certain uncertainty. So, when you're doing abductive reasoning, even in language, I make a decision as this is the right interpretation given the context, but it's what we call... Could be wrong. You might have eaten a ball out of this all year. Exactly, and that's why when I read further, I change my first interpretation. In language, it's not monotonic, actually. We do non-monotonic reasoning in the sense that I might override my first decision. But all of that is pragmatics, and we do this in conversation. Two, three sentences after, I understand really fully what you said before, because I remade the interpretation. And Waleed, do you have any thoughts on where this came from, or basically the evolution of language, or if you like the evolution of this abductive athlete? Do you have any ideas, or is it unique to humans? It seems it is. Unique to humans, definitely. I mean, animal language, animal symbolic languages have been studied thoroughly. And two things, here's where the genius of photo comes in, productivity. I mean, language have a finite set of symbols, and they're not productive. They don't do compositions. And this ties to... Is it animals? Your time up? No animal, no non-human animal has a productive language. In other words, I have a set of symbols, and if I can compose them, I can make a new symbol. Language, animals don't compose things, because they don't decompose them when they're done. They have a finite... It's a hash table. If I make this symbol, I mean this. If I make this... Okay, no matter how sophisticated it is, because they don't have recursion, they don't have infinity, they can't deal at that level with complexity. Some of them have a larger lexicon than others. Okay, but that's still the same paradigm. So, productivity, in other words, this capacity to learn, we were just talking about John, or the neighbor next door, or the neighbor next door that just came from California. I can productively make a person out of three sentences, and in the end, they collapse to a John, right? That productivity doesn't exist in any species except humans, which means compositionality, which means systematicity, which means all of that. So, it's unique to human, definitely. This has been established, and it came with thought. That's the if and only if. That's why we're the only species that really reason. I mean, okay, I have people insist that animals think and they reason. They're not really reasoning, okay? Only humans reason, and thought and language came together. It's sort of like a phenomenon. There are some, there's some proof, even anthropologists, and they say it looks like language was detected when tools and some basic machinery was detected first. So, the human mind at some point had this capacity to think and language came with it. It was like almost at the same time. So, it's uniquely human, definitely now. Where did it come from? Wow. I think it was the need really to express thoughts. Like at some point, we started having thoughts that we want to communicate. So, the external artifact we see outside, whether it's English or ancient Greek or Latin, languages evolve for societal reason and all that. But the external artifact that we use to communicate thoughts came out of the need of the internal language that started to develop. What Fodor calls it, the language of thought, mental ease. And we, so we had that thing going on inside and then we had to communicate. We started with weird sounds and then we scribbled things on the wall to communicate. And then that thing developed until we started making symbols like, okay, if I say this, that means this. I don't know the exact process. I'm not a biologist or evolutionary linguist or, but I think thought is the key here. So, there's a language of thought. And these external things are because linguistic research has also shown that there are many universals in language, regardless of what the language is, even if they are completely different systems like Asian languages and Latin-based languages. They all have a verb, an action. They all have objects and agents of the action. They all have events and events have duration, time and place. So, there are a set of cognitive, I call them universal cognitive primitives, right? There's always an object there somewhere, or an agent of an activity. Now, how you express it in different languages, these are universals. That's the language of thought. That's the internal language, which has to be the same. And objects have properties and all that. So, there are universal primitives. And we instantiate them in different languages differently, but that's to me secondary. Okay. Okay. That's great. William, could you talk a little bit about your recent overview? A colleague that I never worked with, but a colleague in the field. To review this book, and I looked at it and I said, oh, I have enough on my plate. This is not an easy book. But then I, because I liked it, I said, yeah, I'd like to write it. And in the end, it turned out to be not as technically involved as I thought. It's sort of, and I'm saying that not to be negative, but it's sort of the same argument over and over. The gist of the argument is quite simple, actually. And they try to prove it from different vantage points than in the book, from a biological, sociological, psychological, mathematical. But the gist of the book is any talk of AGI is wishful thinking. And it's beyond anything we can ever develop mathematically, so as to engineer it in any, in any realistic way. They make good arguments throughout. There are many examples of the basic idea is that all the mathematics we know, right, mathematics available to us, cannot model not just the entire mind, but even subsystems in the mind, language being one of them. And so it's all complex systems within complex systems in a complex environment, the system around us that we interact with. And none of it can be modeled mathematically, none of it even at any level. So forget doing AGI that can interact with us in an intelligent way. Now, you can do controlled narrow AI, right? You can build very intelligent machines that can do amazing stuff. But any talk of AGI, strong AI, is just talk until, unless, and they admit that, unless we come up with a new mathematics that we never even knew at the scale of Leibniz calculus or Newton, like we're talking about a new mathematics that we never conceived of, right? Which they say most likely all evidence says that's not going to happen, right? So now you can get into why. So that's their claim. And why? They say that all these systems are complex systems. And in complex systems, the idea is that these are, first of all, dynamic systems. They work in a dynamic environment. They are continuously evolving and adapting, right? They are self feeding systems. These are not systems that only take input output. These systems change their behavior. And I gave an example from list. These systems are systems that change their behavior, their algorithms, if you want, they change their mind from a stimulus. So I might, and that's why I said they, I would have liked to see a discussion on the frame problem, because the frame problem in the AI is about this. How can I reason in a dynamic and uncertain environment and react dynamically, although what I do in the environment might affect what I believe about the environment in real time. And they're right. There is no mathematics we know of now. That's why we don't have a solution for the frame problem. So this kind of cyclical cause and effect cannot be modeled by anything we know on mathematics. And this I agree with them. They give an example. I made just an example in language, for example. Language, we know. If I have a dialogue, okay, we all agree that the interpretation of any occurrence requires having the context in mind as part of the, part of the input to the evaluation of the meaning is the context as an extra parameter, right? Now, the context is changing based on something I cannot predict, which is the response of some participant in the dialogue. There is no meaningful way of predicting how someone might respond. So in other words, the context is mathematically not defined, but I need it in the interpretation. Thus, no language understanding, no language understanding, no AGI, because they believe language understanding is a prerequisite. So the, their conclusion, I mean, you can question every step in this inference they come to, but they give language as an example, but we have social behavior, I can give an example. They have a nice example in social behavior. Here's an example of a complex system that cannot, we don't have any mathematics that can model. We're staying in a queue, in a clinic, an emergency room. What do they call them? These ER. So, but there's a queue because they all have emergencies, right? Now, the social behavior, then the social norm is that in the queue, okay, we all have, we all have urgent issues. But in the end, I came first, right? Okay, so that's a social norm. And, but can a robot understand that if someone fainted, really, I mean, it's almost gone, right? Our social norm accepts that this person violates the queue order, right? This is something dynamic that happens, like the queue is this way. And how can a robot update the rules and not kill someone because they violated the order of the queue? In other words, these interactions, these cyclical cause and effect are very complex, that no mathematical model. Or the example I said in language, they prove this cannot be done. Context is needed to interpret everything. I cannot predict what the context will be because I cannot predict you respond to my, so it's unpredictable, they call it erratic, almost random. So there is no mathematics that can model it. And there are many aspects to the mind, whether it's social reasoning, language, and then they conclude there cannot be a system that we can model on volume and machines, because we don't have the mathematics to model it. And these, they go into deep learning. And they give examples even like deep learning, no matter how much data you ingest, you can never predict the future. You're lucky if you can do a good job on the past and even forget the future. And definitely forget the, sorry, the present. So definitely forget. Can I jump in for a minute because I have a couple of comments. So one is, would you agree that this is quite synonymous with, you know, Douglas Hofstadter's strange, strange loops and the whole like random reference, the self-referential self systems, because I mean, complex systems, a big part of them is they usually are, they do have feedback loops. And at some scale, they become so they will involve self reference. Yeah. Okay. My other, my other point I want to make is this, you know, I have quite a bit of sympathy towards the viewpoint, right, of this, of this book that you're talking about, with one exception, which is I'm still optimistic that we can discover a mathematics that may help us out. And so I always think to the foundation series by Isaac Asimov, because in there, they discover a science in a mathematics called psycho history, which at least allows them to predict complex systems of a certain scale and larger. So in the book, it's sort of like planet scale and larger, they're able to actually predict, you know, these complex sociological systems and human behaviors, and how they're going to interact like beyond, beyond that scale. And it's really fascinating. I make that point. I highly recommend that series to anybody, because it's very fascinating because, you know, they talk a lot about sort of what if you had the science, what might it look like, etc. And in there, there's like this little tiny microscopic thing that's beyond the predictability of psycho history that comes in and kind of mucks up the works and creates anomalies that they have to constantly keep combating against. So I think if anybody wants a fictional take on a possible mathematics of this, like, I would recommend Yeah, I make this point. I say, I agree with their argument. We're trying to model complex systems in the sense of cyclical cause and effect that we don't have anything that can model them intelligently. And I give an example in this, in this, I can write a program that changes itself at runtime. Because this is intentional, I can, the whole program can be a parameter, which I can look at it. Well, code is data. That's why I can manipulate the program itself and go look at it after execution and see different program than the one I wrote. It's, it's amazing list. So if I, so I can write programs in this that no one can understand and model and do program verification. So I make the argument that, okay, I can see your point. But like Keith said, never is a long time. Why say we cannot come up with a new mathematics? I can see you at one point, someone discovering, yeah, at the level of Newton differential calculus, why not? Which could happen. So the word never for me, it's hard to digest. Maybe an AGI will discover the mathematics to create itself. And the other point is, the other point is, which is another point that John McCarthy wants. Who said we have to understand what we built? Here's what I mean. Do we understand ourselves? We don't. So why not build a scary intelligent machine that we don't really understand, like my list program? So what I'm saying is I had, I had an issue with them saying, that precludes AGI. No, it doesn't. In theory, I can build a complex intelligent machine like us in many respects. Doesn't feel pain. Hey, who cares? But it's scary intelligent. And we don't understand how it works. So what? This can happen. I can build something I don't understand. So in theory, I have two issues with their book, that this never and this absolute decision that we're done, we can never get there. No, we might build something we don't understand by discovering some new weird mathematics. So, okay, I agree with you that it's a, it's a complex thing that we will never understand. But so what? But what I loved about the book is it's a sobering book. I mean, it really is a balancing book compared to the hype and the simplicity you see out there. I mean, you, you recommend it or highly because I mean, I didn't need that much sobering. I know that any talk of AGI is like, Hey, take a break. Enjoy your paycheck, but don't make silly statements like this, right? Although I felt I thought you might have fallen off that wagon at the beginning of this conversation. No, I'm a defender of the faith. But, but so it's, I recommend it to people that need it. Like me, I needed it too. It's a sobering book. Like, this is how complex what you're trying to do is. Okay, guys. So before you go out and say, language understand. And the nice thing is they took aspects of the mind, just language itself is a beast that we cannot conquer. So imagine the whole mind and the granular thing they go through it. I mean, it's all, it's complex systems all the way down or all the way up if you want. So language is a complex system on its own part of the mind, which is a complex system on its own part of the human living organism, which is a complex system on its own. So and at every level, the complexity, we don't have a mathematics for that's the gist of their argument. So people that make these big claims about AI need to read it. Guys, cool down, cool down. You have, you have not solved problems that occupy the most penetrating minds in the history of enemy from Emmanuel Khan to you have not solved these problems, cool it down. You can build narrow AI, very narrow AI. And all this transferability, transferability. I mean, if you're good at chess, I know people that are good at chess and they're almost good at nothing else, not okay. So forget this. If I'm good at chess, I can be a smart doctor. No. So we are a very complex machine. So this book is a good sobering book, mathematically speaking, philosophically speaking, so that people will tone down what they're saying and start speaking science instead of media gibberish, right? Deep learning will soon be able to do everything. I mean, from a scientist. Well, it seems like a council of despair almost. Is there any optimistic or positive hopeful aspects to it? No, I that part I don't like this never, right? I mean, I am a believer that we can do AGI, but not a human like AI. We might do a very powerful AI that in many ways is more powerful. I mean, we've done that now. I mean, machines are now superior to us in many respects and respects even that they require intelligence, not a bulldozer that can lift more than me, that will have to do cognitive tasks better than us. We have go or finding patterns and data at the scale that no human can do. So we are building intelligent machines, but can we conquer things like language like autonomous driving was a failure. It's a big upset for AI because they trivialize the problem that we can go. Well, and that's kind of what I want to get to, you know, Mark kind of in response to you, which is I take these kind of sobering, these sobering things and look, I mean, the book sounds great, and I'm definitely going to get it and read it. But some of these thoughts, you know, many people have had, you know, many times over the years, right? And I've recognized that there are these limitations. But I think like part of part of why I think books like this are actually have an optimistic kind of side to them is I hope, I hope they encourage people to get more creative. Okay, like, like stop just trying to dump every single dollar you have into yet another parameter, you know, into yet another thousand or billion parameters in a model, like, let's take some of our resources, like, sure, let's keep doing that engineering, but let's take some portion of our resources here and invest it in, like, crazy ideas. And I know Tim's smile here because it's like, like, can it's can a Stanley kind of type thing, right? Like, just go out there and try to do something crazy to find that mathematics that we need, right? Which is let's get creative, let's work on crazy things, let's have crazy ideas, let's work on hybrid systems, let's not give up on, you know, neuromorphic, you know, systems and computer, whatever, like, let's spread out, let's spread out a bit, because of the fact that if we just keep going down this direction of ever larger Turing machines, like, that may not be the solution. Actually, they make this point exactly in different ways that if anything, their goal is to let people widen their horizon. So many aspects of this problem that guys, if we keep going, this is not going to get us there. And that's why they argued mathematically, philosophically. And I think they have a good argument. This is not going to take us there. But let's explore that. So that and being so religious about this will will hinder any other possibility. So overall, their argument is a good argument. I think everybody should read this book that's interested in AGI as a goal. What we have cannot ever take us there. They prove this mathematically. I mean, to me, they proved it in language on. So we need something new. And if you want to do something new, we can't just stay in this corner and with this, that's not going to get us there. So in a way, it's not a negative book. It's a sobering book, I will use the term sober. Well, and encouraging of more variety and more daring and more creativity and right, they don't push too much on that. But but indirectly, the indirect net result, if people appreciate the argument will be to look and explore other ways. So in a way, it's more positive than negative. And by the way, stating a mathematical fact is never negative. Oh, don't be so sure about that. If they're saying we're always on the edge of being canceled for stating like mathematical facts. So yeah, but I mean, if they're saying that what we're doing now, we're not we'll never get us to AGI. That's not negative. You're saying we need something else, we need something more. Or yours, yours, look, it could have saved us, they use this phrase, money down the drain. Autonomous driving is a case, is a good case. Billions, we're talking non trivial money guys, we're talking more than the budgets of some European countries. Just imagine the scale. And those guys went bust, right? Why? Because they trivialized the autonomous truck. An autonomous car is an autonomous agent, guys. It's an it's an agent trying to reason in a dynamic and uncertain environment and has on the fly to change to do belief revision, change its strategy, because of something new that came up. All of that is from seeing the tree and the stop sign. It's all vision. Yeah, so this is actually encouraging because now for all the Uber drivers out there and long hauled truck drivers, your job is safe. Like it's not going to be replaced anytime soon. Yeah, it's amazing. And a few years back when I was still in the valley, yeah, I was in Mecca in Silicon Valley. And I would talk to superb PhDs in neuroscience and big AI engineers at top companies. They were so excited that we're at level four in a year or two. And this was six years ago. I say, guys, this will not happen. They say, why are you so negative? I said, you cannot have an autonomous agent on the road without solving the frame problem. How do I revise everything I know, because of this new event? And this has to happen real time. We don't have a solution for the frame problem. All you're doing is you have cars on a railway. We have autonomous cars now. It's called the train. We have autonomous flight. If I'm not in a dynamic and uncertain environment reasoning, yeah, I can have autonomous anything. We call it the railway. I mean, we call it Amtrak. It's autonomous. You press a button and it goes. If we're talking about reasoning in the streets of San Francisco, you have to face the frame problem or you will kill people. Anyway, on that sobering note, I think a lot of stuff goes unnoticed in San Francisco, so I'm not sure about that. Probably that's the least of them. No, but the scale of money that went the scale. This is the value of this book, the scale of investment. I mean, if 10% of that was put on another approach, hey, you weird guy with this weird idea, take 10% of what we're throwing down the drain and explore something else. Show me. That's where I'm at, too. Diversify the effort. There's a huge impact here, societal impact. We're wasting billions of dollars just because I don't want to listen to anyone else. It happened in the chatbot industry, which I'm more familiar with than autonomous driving. Chatbot this, chatbot that, and there was an explosion. It was like a blob, like the internet thing. Now we can't get away from them. Every website we go to, it's like, leave me alone. Nobody wants to use them because we know how they work. These are stochastic parents. Yeah, literally just going to a FAC and hitting control F is more effective for me than trying to interact with a chatbot. The search engines by key phrases you put and they bring you a link and they say, read this. This is your answer. They are search engines basically. But again, the amount of money, because I lived in that industry, the amount of money spent on chatbots will scare the hell out of anybody. You combine that with autonomous driving, both dead, almost zero. We're talking billions and billions and billions. And you talk to any one of them in the highest, in the middle of the fever. They won't listen to you. I have people now calling me back and saying, you were right. Yeah, after $200 billion. So, science is important. Engineering is important, but science is important too. That's where the value of this book is. Guys, hacking alone will not do the whole thing. You're a bright engineer, you can hack your way through what we know is true. That's the space you can play with. You cannot hack your way in a bigger set of possibilities that are. You didn't verify that you can go there. An engineer can be creative within a Venn diagram that the scientists drew for them. That's the difference between science and engineering. The scientist draws the Venn diagram and that's the value of philosophers, at least the analytic philosophers, that know logic and metaphysics and quantum mechanics and philosophers that are on the technical side. They know how to draw the Venn diagram. You, as an engineer, if you're wasting your time here, that's called money down the drain. Play inside the Venn diagram. Otherwise, you're just an over enthused engineer who should go back and study computability. It's kind of like how patent examiners can easily reject anything that comes in that claims to violate the second law of thermodynamics, right? Well, listen, Wally, I think we sincerely appreciate your time today. And also, Mark, thank you for joining us and asking great questions. We should do this again. Thanks, Wally. Yeah, I really appreciate it. So thanks, always fun, guys. I see. Peace.", "segments": [{"id": 0, "seek": 0, "start": 0.96, "end": 5.36, "text": " Welcome back to Machine Learning Street Talk. I'm your host, Tim Scarf.", "tokens": [50412, 4027, 646, 281, 22155, 15205, 7638, 8780, 13, 286, 478, 428, 3975, 11, 7172, 23181, 69, 13, 50632], "temperature": 0.0, "avg_logprob": -0.19679609599866366, "compression_ratio": 1.461847389558233, "no_speech_prob": 0.1749933362007141}, {"id": 1, "seek": 0, "start": 6.24, "end": 12.88, "text": " Now, today on the show, we're joined by Dr. Walid Saber, who's just written a review of the book", "tokens": [50676, 823, 11, 965, 322, 264, 855, 11, 321, 434, 6869, 538, 2491, 13, 9707, 327, 13915, 260, 11, 567, 311, 445, 3720, 257, 3131, 295, 264, 1446, 51008], "temperature": 0.0, "avg_logprob": -0.19679609599866366, "compression_ratio": 1.461847389558233, "no_speech_prob": 0.1749933362007141}, {"id": 2, "seek": 0, "start": 12.88, "end": 19.6, "text": " Machines Will Never Rule the World, Artificial Intelligence Without Fear, written by Jops Langreb", "tokens": [51008, 12089, 1652, 3099, 7344, 27533, 264, 3937, 11, 5735, 10371, 27274, 9129, 28054, 11, 3720, 538, 508, 3370, 13313, 22692, 51344], "temperature": 0.0, "avg_logprob": -0.19679609599866366, "compression_ratio": 1.461847389558233, "no_speech_prob": 0.1749933362007141}, {"id": 3, "seek": 0, "start": 19.6, "end": 26.0, "text": " and Barry Smith. Now, Dr. Saber will be discussing his review, which provides a detailed analysis", "tokens": [51344, 293, 21639, 8538, 13, 823, 11, 2491, 13, 13915, 260, 486, 312, 10850, 702, 3131, 11, 597, 6417, 257, 9942, 5215, 51664], "temperature": 0.0, "avg_logprob": -0.19679609599866366, "compression_ratio": 1.461847389558233, "no_speech_prob": 0.1749933362007141}, {"id": 4, "seek": 2600, "start": 26.0, "end": 33.76, "text": " of the book's arguments that strong AI is impossible. In his review, Dr. Saber acknowledges", "tokens": [50364, 295, 264, 1446, 311, 12869, 300, 2068, 7318, 307, 6243, 13, 682, 702, 3131, 11, 2491, 13, 13915, 260, 15195, 2880, 50752], "temperature": 0.0, "avg_logprob": -0.07665069603625639, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.01472330093383789}, {"id": 5, "seek": 2600, "start": 33.76, "end": 39.36, "text": " the argument made by Langreb and Smith that anything we engineer is ultimately a system", "tokens": [50752, 264, 6770, 1027, 538, 13313, 22692, 293, 8538, 300, 1340, 321, 11403, 307, 6284, 257, 1185, 51032], "temperature": 0.0, "avg_logprob": -0.07665069603625639, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.01472330093383789}, {"id": 6, "seek": 2600, "start": 39.36, "end": 45.120000000000005, "text": " which can be mathematically modelled and described. He then goes on to discuss the complexity of", "tokens": [51032, 597, 393, 312, 44003, 1072, 41307, 293, 7619, 13, 634, 550, 1709, 322, 281, 2248, 264, 14024, 295, 51320], "temperature": 0.0, "avg_logprob": -0.07665069603625639, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.01472330093383789}, {"id": 7, "seek": 2600, "start": 45.120000000000005, "end": 53.04, "text": " modelling mental processes, which the authors argue are dynamic, adaptive, continuously evolving,", "tokens": [51320, 42253, 4973, 7555, 11, 597, 264, 16552, 9695, 366, 8546, 11, 27912, 11, 15684, 21085, 11, 51716], "temperature": 0.0, "avg_logprob": -0.07665069603625639, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.01472330093383789}, {"id": 8, "seek": 5304, "start": 53.04, "end": 59.92, "text": " and constitutes systems whose behaviour affects and is affected by the environment they function in.", "tokens": [50364, 293, 44204, 3652, 6104, 17229, 11807, 293, 307, 8028, 538, 264, 2823, 436, 2445, 294, 13, 50708], "temperature": 0.0, "avg_logprob": -0.08084862462935909, "compression_ratio": 1.5828877005347595, "no_speech_prob": 0.0018096254207193851}, {"id": 9, "seek": 5304, "start": 61.92, "end": 67.92, "text": " He also touches on the notion of granularity, arguing that complex systems are all the way up", "tokens": [50808, 634, 611, 17431, 322, 264, 10710, 295, 39962, 507, 11, 19697, 300, 3997, 3652, 366, 439, 264, 636, 493, 51108], "temperature": 0.0, "avg_logprob": -0.08084862462935909, "compression_ratio": 1.5828877005347595, "no_speech_prob": 0.0018096254207193851}, {"id": 10, "seek": 5304, "start": 68.48, "end": 76.4, "text": " from specific components of the mind to the mind itself and that no known mathematics can model them.", "tokens": [51136, 490, 2685, 6677, 295, 264, 1575, 281, 264, 1575, 2564, 293, 300, 572, 2570, 18666, 393, 2316, 552, 13, 51532], "temperature": 0.0, "avg_logprob": -0.08084862462935909, "compression_ratio": 1.5828877005347595, "no_speech_prob": 0.0018096254207193851}, {"id": 11, "seek": 7640, "start": 76.88000000000001, "end": 82.56, "text": " Dr. Saber then delves into the complexities of language and open interactive dialogues,", "tokens": [50388, 2491, 13, 13915, 260, 550, 1103, 977, 666, 264, 48705, 295, 2856, 293, 1269, 15141, 45551, 11, 50672], "temperature": 0.0, "avg_logprob": -0.148935370249291, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.0029796715825796127}, {"id": 12, "seek": 7640, "start": 82.56, "end": 88.16000000000001, "text": " asserting that language is a prerequisite for any artificial general intelligence,", "tokens": [50672, 1256, 27187, 300, 2856, 307, 257, 38333, 34152, 337, 604, 11677, 2674, 7599, 11, 50952], "temperature": 0.0, "avg_logprob": -0.148935370249291, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.0029796715825796127}, {"id": 13, "seek": 7640, "start": 88.16000000000001, "end": 94.72, "text": " but that linguistic communication itself is a complex system that no mathematics can model.", "tokens": [50952, 457, 300, 43002, 6101, 2564, 307, 257, 3997, 1185, 300, 572, 18666, 393, 2316, 13, 51280], "temperature": 0.0, "avg_logprob": -0.148935370249291, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.0029796715825796127}, {"id": 14, "seek": 7640, "start": 95.52000000000001, "end": 100.80000000000001, "text": " He doesn't subscribe to the argument that interactive bots can be built in narrow domains,", "tokens": [51320, 634, 1177, 380, 3022, 281, 264, 6770, 300, 15141, 35410, 393, 312, 3094, 294, 9432, 25514, 11, 51584], "temperature": 0.0, "avg_logprob": -0.148935370249291, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.0029796715825796127}, {"id": 15, "seek": 10080, "start": 101.52, "end": 106.88, "text": " since responses and the overall context cannot be predicted in any meaningful way.", "tokens": [50400, 1670, 13019, 293, 264, 4787, 4319, 2644, 312, 19147, 294, 604, 10995, 636, 13, 50668], "temperature": 0.0, "avg_logprob": -0.19058262218128552, "compression_ratio": 1.5772727272727274, "no_speech_prob": 0.018491540104150772}, {"id": 16, "seek": 10080, "start": 107.92, "end": 114.16, "text": " Dr. Saber has two reservations as to the conclusions made by Langreb and Smith.", "tokens": [50720, 2491, 13, 13915, 260, 575, 732, 40222, 382, 281, 264, 22865, 1027, 538, 13313, 22692, 293, 8538, 13, 51032], "temperature": 0.0, "avg_logprob": -0.19058262218128552, "compression_ratio": 1.5772727272727274, "no_speech_prob": 0.018491540104150772}, {"id": 17, "seek": 10080, "start": 114.16, "end": 120.24, "text": " He questions their use of the word never and suggests there could be a new mathematics", "tokens": [51032, 634, 1651, 641, 764, 295, 264, 1349, 1128, 293, 13409, 456, 727, 312, 257, 777, 18666, 51336], "temperature": 0.0, "avg_logprob": -0.19058262218128552, "compression_ratio": 1.5772727272727274, "no_speech_prob": 0.018491540104150772}, {"id": 18, "seek": 10080, "start": 120.24, "end": 126.47999999999999, "text": " that mental processes require that is yet to be discovered. He also doesn't believe that the fact", "tokens": [51336, 300, 4973, 7555, 3651, 300, 307, 1939, 281, 312, 6941, 13, 634, 611, 1177, 380, 1697, 300, 264, 1186, 51648], "temperature": 0.0, "avg_logprob": -0.19058262218128552, "compression_ratio": 1.5772727272727274, "no_speech_prob": 0.018491540104150772}, {"id": 19, "seek": 12648, "start": 126.56, "end": 132.56, "text": " that complex behaviour cannot be mathematically modelled precludes the possibility of building", "tokens": [50368, 300, 3997, 17229, 2644, 312, 44003, 1072, 41307, 4346, 1471, 279, 264, 7959, 295, 2390, 50668], "temperature": 0.0, "avg_logprob": -0.25871706008911133, "compression_ratio": 1.550185873605948, "no_speech_prob": 0.0036469053011387587}, {"id": 20, "seek": 12648, "start": 132.56, "end": 139.6, "text": " such systems, as evidenced by the intentional programming language LISP, and he also considers", "tokens": [50668, 1270, 3652, 11, 382, 43699, 1232, 538, 264, 21935, 9410, 2856, 441, 2343, 47, 11, 293, 415, 611, 33095, 51020], "temperature": 0.0, "avg_logprob": -0.25871706008911133, "compression_ratio": 1.550185873605948, "no_speech_prob": 0.0036469053011387587}, {"id": 21, "seek": 12648, "start": 139.6, "end": 145.12, "text": " the possibility of hypercomputation in validating the church-touring hypothesis.", "tokens": [51020, 264, 7959, 295, 9848, 1112, 2582, 399, 294, 7363, 990, 264, 4128, 12, 83, 40510, 17291, 13, 51296], "temperature": 0.0, "avg_logprob": -0.25871706008911133, "compression_ratio": 1.550185873605948, "no_speech_prob": 0.0036469053011387587}, {"id": 22, "seek": 12648, "start": 145.12, "end": 149.84, "text": " We'll talk about that a bit later. Finally, Dr. Saber expresses his regret", "tokens": [51296, 492, 603, 751, 466, 300, 257, 857, 1780, 13, 6288, 11, 2491, 13, 13915, 260, 39204, 702, 10879, 51532], "temperature": 0.0, "avg_logprob": -0.25871706008911133, "compression_ratio": 1.550185873605948, "no_speech_prob": 0.0036469053011387587}, {"id": 23, "seek": 12648, "start": 149.84, "end": 154.8, "text": " that the book didn't go into further detail on the frame problem in AI.", "tokens": [51532, 300, 264, 1446, 994, 380, 352, 666, 3052, 2607, 322, 264, 3920, 1154, 294, 7318, 13, 51780], "temperature": 0.0, "avg_logprob": -0.25871706008911133, "compression_ratio": 1.550185873605948, "no_speech_prob": 0.0036469053011387587}, {"id": 24, "seek": 15648, "start": 156.48, "end": 160.79999999999998, "text": " He calls for further research into belief revision in complex systems.", "tokens": [50364, 634, 5498, 337, 3052, 2132, 666, 7107, 34218, 294, 3997, 3652, 13, 50580], "temperature": 0.0, "avg_logprob": -0.14819230166348543, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.027084633708000183}, {"id": 25, "seek": 15648, "start": 162.23999999999998, "end": 166.88, "text": " Join us today as we speak with Dr. Wallid Saber about his review of this book that we've just", "tokens": [50652, 19642, 505, 965, 382, 321, 1710, 365, 2491, 13, 9551, 327, 13915, 260, 466, 702, 3131, 295, 341, 1446, 300, 321, 600, 445, 50884], "temperature": 0.0, "avg_logprob": -0.14819230166348543, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.027084633708000183}, {"id": 26, "seek": 15648, "start": 166.88, "end": 172.48, "text": " been speaking about. Machines will never rule the world. And by the way, I think very highly", "tokens": [50884, 668, 4124, 466, 13, 12089, 1652, 486, 1128, 4978, 264, 1002, 13, 400, 538, 264, 636, 11, 286, 519, 588, 5405, 51164], "temperature": 0.0, "avg_logprob": -0.14819230166348543, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.027084633708000183}, {"id": 27, "seek": 15648, "start": 172.48, "end": 178.95999999999998, "text": " of Wallid. I think he is one of our most loved guests. He is a polymath. He has an incredible", "tokens": [51164, 295, 9551, 327, 13, 286, 519, 415, 307, 472, 295, 527, 881, 4333, 9804, 13, 634, 307, 257, 6754, 24761, 13, 634, 575, 364, 4651, 51488], "temperature": 0.0, "avg_logprob": -0.14819230166348543, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.027084633708000183}, {"id": 28, "seek": 15648, "start": 178.95999999999998, "end": 184.16, "text": " breadth of knowledge across so many fields, you know, from AI and computer science,", "tokens": [51488, 35862, 295, 3601, 2108, 370, 867, 7909, 11, 291, 458, 11, 490, 7318, 293, 3820, 3497, 11, 51748], "temperature": 0.0, "avg_logprob": -0.14819230166348543, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.027084633708000183}, {"id": 29, "seek": 18416, "start": 184.16, "end": 191.12, "text": " to mathematics, to philosophy, to linguistics. He really is a rare breed, and he also brings", "tokens": [50364, 281, 18666, 11, 281, 10675, 11, 281, 21766, 6006, 13, 634, 534, 307, 257, 5892, 18971, 11, 293, 415, 611, 5607, 50712], "temperature": 0.0, "avg_logprob": -0.10143379771381343, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.15133945643901825}, {"id": 30, "seek": 18416, "start": 191.12, "end": 198.24, "text": " a very interesting contrarian view, I would say, to the current kind of modus operandi,", "tokens": [50712, 257, 588, 1880, 660, 5352, 952, 1910, 11, 286, 576, 584, 11, 281, 264, 2190, 733, 295, 1072, 301, 2208, 49460, 11, 51068], "temperature": 0.0, "avg_logprob": -0.10143379771381343, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.15133945643901825}, {"id": 31, "seek": 18416, "start": 198.24, "end": 202.24, "text": " or the zeitgeist in the community at the moment. He's a breath of fresh air.", "tokens": [51068, 420, 264, 49367, 432, 468, 294, 264, 1768, 412, 264, 1623, 13, 634, 311, 257, 6045, 295, 4451, 1988, 13, 51268], "temperature": 0.0, "avg_logprob": -0.10143379771381343, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.15133945643901825}, {"id": 32, "seek": 18416, "start": 202.96, "end": 208.72, "text": " Anyway, if you haven't already, consider subscribing to our YouTube channel, or indeed", "tokens": [51304, 5684, 11, 498, 291, 2378, 380, 1217, 11, 1949, 19981, 281, 527, 3088, 2269, 11, 420, 6451, 51592], "temperature": 0.0, "avg_logprob": -0.10143379771381343, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.15133945643901825}, {"id": 33, "seek": 18416, "start": 208.72, "end": 213.51999999999998, "text": " rating our podcast on your favourite podcasting platform if you happen to be listening to us.", "tokens": [51592, 10990, 527, 7367, 322, 428, 10696, 7367, 278, 3663, 498, 291, 1051, 281, 312, 4764, 281, 505, 13, 51832], "temperature": 0.0, "avg_logprob": -0.10143379771381343, "compression_ratio": 1.6102941176470589, "no_speech_prob": 0.15133945643901825}, {"id": 34, "seek": 21416, "start": 214.16, "end": 217.92, "text": " Anyway, without any further delay, I give you Dr. Wallid Saber.", "tokens": [50364, 5684, 11, 1553, 604, 3052, 8577, 11, 286, 976, 291, 2491, 13, 9551, 327, 13915, 260, 13, 50552], "temperature": 0.0, "avg_logprob": -0.13844041566591006, "compression_ratio": 1.449438202247191, "no_speech_prob": 0.0004838532186113298}, {"id": 35, "seek": 21416, "start": 224.4, "end": 233.51999999999998, "text": " Welcome back to MLST, folks. We have the unmistakable Wallid Saber, the legend that is Wallid Saber,", "tokens": [50876, 4027, 646, 281, 376, 19198, 51, 11, 4024, 13, 492, 362, 264, 19334, 468, 514, 712, 9551, 327, 13915, 260, 11, 264, 9451, 300, 307, 9551, 327, 13915, 260, 11, 51332], "temperature": 0.0, "avg_logprob": -0.13844041566591006, "compression_ratio": 1.449438202247191, "no_speech_prob": 0.0004838532186113298}, {"id": 36, "seek": 21416, "start": 233.51999999999998, "end": 238.16, "text": " but we also have Mark from our Discord community. Mark, would you like to introduce yourself?", "tokens": [51332, 457, 321, 611, 362, 3934, 490, 527, 32623, 1768, 13, 3934, 11, 576, 291, 411, 281, 5366, 1803, 30, 51564], "temperature": 0.0, "avg_logprob": -0.13844041566591006, "compression_ratio": 1.449438202247191, "no_speech_prob": 0.0004838532186113298}, {"id": 37, "seek": 23816, "start": 239.04, "end": 246.0, "text": " Hi, guys. I'm Mark Aguil, a philosopher, cognitive scientist, and software engineer at", "tokens": [50408, 2421, 11, 1074, 13, 286, 478, 3934, 2725, 84, 388, 11, 257, 29805, 11, 15605, 12662, 11, 293, 4722, 11403, 412, 50756], "temperature": 0.0, "avg_logprob": -0.17227455942254316, "compression_ratio": 1.4788732394366197, "no_speech_prob": 0.02436697855591774}, {"id": 38, "seek": 23816, "start": 246.0, "end": 252.96, "text": " MLST. Awesome. Welcome, Mark. You know, things have been a bit of a blur,", "tokens": [50756, 376, 19198, 51, 13, 10391, 13, 4027, 11, 3934, 13, 509, 458, 11, 721, 362, 668, 257, 857, 295, 257, 14257, 11, 51104], "temperature": 0.0, "avg_logprob": -0.17227455942254316, "compression_ratio": 1.4788732394366197, "no_speech_prob": 0.02436697855591774}, {"id": 39, "seek": 23816, "start": 252.96, "end": 258.4, "text": " but I think this is right, Tim. I think Wallid was the first, or one of the first,", "tokens": [51104, 457, 286, 519, 341, 307, 558, 11, 7172, 13, 286, 519, 9551, 327, 390, 264, 700, 11, 420, 472, 295, 264, 700, 11, 51376], "temperature": 0.0, "avg_logprob": -0.17227455942254316, "compression_ratio": 1.4788732394366197, "no_speech_prob": 0.02436697855591774}, {"id": 40, "seek": 23816, "start": 258.4, "end": 265.2, "text": " really big names that we had come on the show, right? I don't think so,", "tokens": [51376, 534, 955, 5288, 300, 321, 632, 808, 322, 264, 855, 11, 558, 30, 286, 500, 380, 519, 370, 11, 51716], "temperature": 0.0, "avg_logprob": -0.17227455942254316, "compression_ratio": 1.4788732394366197, "no_speech_prob": 0.02436697855591774}, {"id": 41, "seek": 26520, "start": 265.2, "end": 271.12, "text": " but the most controversial, let's put it this way, the one that made probably,", "tokens": [50364, 457, 264, 881, 17323, 11, 718, 311, 829, 309, 341, 636, 11, 264, 472, 300, 1027, 1391, 11, 50660], "temperature": 0.0, "avg_logprob": -0.1386218017406678, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.005638040602207184}, {"id": 42, "seek": 26520, "start": 272.4, "end": 280.48, "text": " it was so predictable, what was, I mean, probably the first one that broke that", "tokens": [50724, 309, 390, 370, 27737, 11, 437, 390, 11, 286, 914, 11, 1391, 264, 700, 472, 300, 6902, 300, 51128], "temperature": 0.0, "avg_logprob": -0.1386218017406678, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.005638040602207184}, {"id": 43, "seek": 26520, "start": 283.59999999999997, "end": 289.2, "text": " predictive model. Well, I just remember, I mean, I remember Tim and I being like super,", "tokens": [51284, 35521, 2316, 13, 1042, 11, 286, 445, 1604, 11, 286, 914, 11, 286, 1604, 7172, 293, 286, 885, 411, 1687, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1386218017406678, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.005638040602207184}, {"id": 44, "seek": 26520, "start": 289.2, "end": 294.0, "text": " because we didn't know you, right, before then, and I remember us being like so excited that you", "tokens": [51564, 570, 321, 994, 380, 458, 291, 11, 558, 11, 949, 550, 11, 293, 286, 1604, 505, 885, 411, 370, 2919, 300, 291, 51804], "temperature": 0.0, "avg_logprob": -0.1386218017406678, "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.005638040602207184}, {"id": 45, "seek": 29400, "start": 294.0, "end": 298.4, "text": " agreed to come on the show. You know, of course, now that we know you, we're kind of like,", "tokens": [50364, 9166, 281, 808, 322, 264, 855, 13, 509, 458, 11, 295, 1164, 11, 586, 300, 321, 458, 291, 11, 321, 434, 733, 295, 411, 11, 50584], "temperature": 0.0, "avg_logprob": -0.15421371459960936, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0040067448280751705}, {"id": 46, "seek": 29400, "start": 298.4, "end": 304.16, "text": " ah, whatever, it's just Wallid. We're just through, by the way.", "tokens": [50584, 3716, 11, 2035, 11, 309, 311, 445, 9551, 327, 13, 492, 434, 445, 807, 11, 538, 264, 636, 13, 50872], "temperature": 0.0, "avg_logprob": -0.15421371459960936, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0040067448280751705}, {"id": 47, "seek": 29400, "start": 304.16, "end": 308.0, "text": " We were following all over ourselves that you were coming on the show, we're like, oh my god,", "tokens": [50872, 492, 645, 3480, 439, 670, 4175, 300, 291, 645, 1348, 322, 264, 855, 11, 321, 434, 411, 11, 1954, 452, 3044, 11, 51064], "temperature": 0.0, "avg_logprob": -0.15421371459960936, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0040067448280751705}, {"id": 48, "seek": 29400, "start": 308.0, "end": 313.76, "text": " this is so awesome. To be honest with you, I never thought I would, I would,", "tokens": [51064, 341, 307, 370, 3476, 13, 1407, 312, 3245, 365, 291, 11, 286, 1128, 1194, 286, 576, 11, 286, 576, 11, 51352], "temperature": 0.0, "avg_logprob": -0.15421371459960936, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0040067448280751705}, {"id": 49, "seek": 29400, "start": 315.28, "end": 319.68, "text": " yeah, I never thought I would have, my opinion would matter that much, to be honest with you,", "tokens": [51428, 1338, 11, 286, 1128, 1194, 286, 576, 362, 11, 452, 4800, 576, 1871, 300, 709, 11, 281, 312, 3245, 365, 291, 11, 51648], "temperature": 0.0, "avg_logprob": -0.15421371459960936, "compression_ratio": 1.8060344827586208, "no_speech_prob": 0.0040067448280751705}, {"id": 50, "seek": 31968, "start": 319.68, "end": 326.48, "text": " and it all started by creating this medium blog, and I started spitting out stuff that,", "tokens": [50364, 293, 309, 439, 1409, 538, 4084, 341, 6399, 6968, 11, 293, 286, 1409, 637, 2414, 484, 1507, 300, 11, 50704], "temperature": 0.0, "avg_logprob": -0.14678387019945227, "compression_ratio": 1.5170454545454546, "no_speech_prob": 0.00870311725884676}, {"id": 51, "seek": 31968, "start": 327.6, "end": 334.08, "text": " hey, do you guys know that there's dissonance in that? And I was surprised how much it,", "tokens": [50760, 4177, 11, 360, 291, 1074, 458, 300, 456, 311, 717, 3015, 719, 294, 300, 30, 400, 286, 390, 6100, 577, 709, 309, 11, 51084], "temperature": 0.0, "avg_logprob": -0.14678387019945227, "compression_ratio": 1.5170454545454546, "no_speech_prob": 0.00870311725884676}, {"id": 52, "seek": 31968, "start": 336.4, "end": 345.84000000000003, "text": " even by people that are living off and making a living, and they preach and write papers on", "tokens": [51200, 754, 538, 561, 300, 366, 2647, 766, 293, 1455, 257, 2647, 11, 293, 436, 21552, 293, 2464, 10577, 322, 51672], "temperature": 0.0, "avg_logprob": -0.14678387019945227, "compression_ratio": 1.5170454545454546, "no_speech_prob": 0.00870311725884676}, {"id": 53, "seek": 34584, "start": 346.56, "end": 351.59999999999997, "text": " what I'm attacking, and they would say, don't mention my name, by the way, it's all private", "tokens": [50400, 437, 286, 478, 15010, 11, 293, 436, 576, 584, 11, 500, 380, 2152, 452, 1315, 11, 538, 264, 636, 11, 309, 311, 439, 4551, 50652], "temperature": 0.0, "avg_logprob": -0.1374184876969717, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.021198995411396027}, {"id": 54, "seek": 34584, "start": 351.59999999999997, "end": 360.79999999999995, "text": " messages. But apparently, I said a couple of things that touched people, but you know.", "tokens": [50652, 7897, 13, 583, 7970, 11, 286, 848, 257, 1916, 295, 721, 300, 9828, 561, 11, 457, 291, 458, 13, 51112], "temperature": 0.0, "avg_logprob": -0.1374184876969717, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.021198995411396027}, {"id": 55, "seek": 34584, "start": 362.55999999999995, "end": 367.84, "text": " Yeah. Yeah, I was at New York's last week, and the amount of people that came up to me and said,", "tokens": [51200, 865, 13, 865, 11, 286, 390, 412, 1873, 3609, 311, 1036, 1243, 11, 293, 264, 2372, 295, 561, 300, 1361, 493, 281, 385, 293, 848, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1374184876969717, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.021198995411396027}, {"id": 56, "seek": 34584, "start": 367.84, "end": 372.71999999999997, "text": " I love the show, Tim, Wallid's my favorite guest that you've had on, because Wallid just provides", "tokens": [51464, 286, 959, 264, 855, 11, 7172, 11, 9551, 327, 311, 452, 2954, 8341, 300, 291, 600, 632, 322, 11, 570, 9551, 327, 445, 6417, 51708], "temperature": 0.0, "avg_logprob": -0.1374184876969717, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.021198995411396027}, {"id": 57, "seek": 37272, "start": 372.72, "end": 379.36, "text": " a completely different perspective, because we're bred on empiricism and neural networks. And part", "tokens": [50364, 257, 2584, 819, 4585, 11, 570, 321, 434, 34133, 322, 25790, 26356, 293, 18161, 9590, 13, 400, 644, 50696], "temperature": 0.0, "avg_logprob": -0.09978162895128564, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.007192907389253378}, {"id": 58, "seek": 37272, "start": 379.36, "end": 384.08000000000004, "text": " of the reason I want to get you back on, Wallid, is to counteract some of the, I mean, we've been", "tokens": [50696, 295, 264, 1778, 286, 528, 281, 483, 291, 646, 322, 11, 9551, 327, 11, 307, 281, 5682, 578, 512, 295, 264, 11, 286, 914, 11, 321, 600, 668, 50932], "temperature": 0.0, "avg_logprob": -0.09978162895128564, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.007192907389253378}, {"id": 59, "seek": 37272, "start": 384.08000000000004, "end": 388.96000000000004, "text": " speaking to a lot of deep learning people recently, so we need to counteract that a little bit.", "tokens": [50932, 4124, 281, 257, 688, 295, 2452, 2539, 561, 3938, 11, 370, 321, 643, 281, 5682, 578, 300, 257, 707, 857, 13, 51176], "temperature": 0.0, "avg_logprob": -0.09978162895128564, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.007192907389253378}, {"id": 60, "seek": 37272, "start": 389.52000000000004, "end": 398.32000000000005, "text": " Yeah, that one shocked the hell out of me. I mean, I have people like myself at this event. I mean,", "tokens": [51204, 865, 11, 300, 472, 12763, 264, 4921, 484, 295, 385, 13, 286, 914, 11, 286, 362, 561, 411, 2059, 412, 341, 2280, 13, 286, 914, 11, 51644], "temperature": 0.0, "avg_logprob": -0.09978162895128564, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.007192907389253378}, {"id": 61, "seek": 39832, "start": 398.32, "end": 409.04, "text": " if you said ACL, maybe, yeah, okay. But I mean, this is the deep learning meeting, right? I mean,", "tokens": [50364, 498, 291, 848, 43873, 11, 1310, 11, 1338, 11, 1392, 13, 583, 286, 914, 11, 341, 307, 264, 2452, 2539, 3440, 11, 558, 30, 286, 914, 11, 50900], "temperature": 0.0, "avg_logprob": -0.22633714843214603, "compression_ratio": 1.3082191780821917, "no_speech_prob": 0.01714802347123623}, {"id": 62, "seek": 39832, "start": 409.04, "end": 420.24, "text": " so I was shocked. But yeah. Yeah, indeed. Although I have some more positive views of DL now.", "tokens": [50900, 370, 286, 390, 12763, 13, 583, 1338, 13, 865, 11, 6451, 13, 5780, 286, 362, 512, 544, 3353, 6809, 295, 413, 43, 586, 13, 51460], "temperature": 0.0, "avg_logprob": -0.22633714843214603, "compression_ratio": 1.3082191780821917, "no_speech_prob": 0.01714802347123623}, {"id": 63, "seek": 42024, "start": 420.24, "end": 428.48, "text": " Oh, go on. Yeah, I mean, look, I breaking news, breaking news.", "tokens": [50364, 876, 11, 352, 322, 13, 865, 11, 286, 914, 11, 574, 11, 286, 7697, 2583, 11, 7697, 2583, 13, 50776], "temperature": 0.0, "avg_logprob": -0.21826668391152035, "compression_ratio": 1.4219653179190752, "no_speech_prob": 0.016378358006477356}, {"id": 64, "seek": 42024, "start": 430.48, "end": 438.16, "text": " Positive, although I still have my reservations as to AGI and all that stuff. But I have been", "tokens": [50876, 46326, 11, 4878, 286, 920, 362, 452, 40222, 382, 281, 316, 26252, 293, 439, 300, 1507, 13, 583, 286, 362, 668, 51260], "temperature": 0.0, "avg_logprob": -0.21826668391152035, "compression_ratio": 1.4219653179190752, "no_speech_prob": 0.016378358006477356}, {"id": 65, "seek": 42024, "start": 440.16, "end": 447.12, "text": " completely impressed with the developments in large language models. I have to admit that", "tokens": [51360, 2584, 11679, 365, 264, 20862, 294, 2416, 2856, 5245, 13, 286, 362, 281, 9796, 300, 51708], "temperature": 0.0, "avg_logprob": -0.21826668391152035, "compression_ratio": 1.4219653179190752, "no_speech_prob": 0.016378358006477356}, {"id": 66, "seek": 44712, "start": 448.0, "end": 453.92, "text": " sometimes I say, what the hell is this? Now, technically, let me tell you what's happening.", "tokens": [50408, 2171, 286, 584, 11, 437, 264, 4921, 307, 341, 30, 823, 11, 12120, 11, 718, 385, 980, 291, 437, 311, 2737, 13, 50704], "temperature": 0.0, "avg_logprob": -0.10634073397008384, "compression_ratio": 1.5235849056603774, "no_speech_prob": 0.0010645411675795913}, {"id": 67, "seek": 44712, "start": 456.56, "end": 459.84000000000003, "text": " And I'm working on something that probably would quantify", "tokens": [50836, 400, 286, 478, 1364, 322, 746, 300, 1391, 576, 40421, 51000], "temperature": 0.0, "avg_logprob": -0.10634073397008384, "compression_ratio": 1.5235849056603774, "no_speech_prob": 0.0010645411675795913}, {"id": 68, "seek": 44712, "start": 461.44, "end": 468.08, "text": " where can this go? How much can you, how much will scale? The bottom line is this,", "tokens": [51080, 689, 393, 341, 352, 30, 1012, 709, 393, 291, 11, 577, 709, 486, 4373, 30, 440, 2767, 1622, 307, 341, 11, 51412], "temperature": 0.0, "avg_logprob": -0.10634073397008384, "compression_ratio": 1.5235849056603774, "no_speech_prob": 0.0010645411675795913}, {"id": 69, "seek": 44712, "start": 468.72, "end": 474.48, "text": " these guys have impressed the hell out of me, and they have proven that scale does matter.", "tokens": [51444, 613, 1074, 362, 11679, 264, 4921, 484, 295, 385, 11, 293, 436, 362, 12785, 300, 4373, 775, 1871, 13, 51732], "temperature": 0.0, "avg_logprob": -0.10634073397008384, "compression_ratio": 1.5235849056603774, "no_speech_prob": 0.0010645411675795913}, {"id": 70, "seek": 47448, "start": 474.48, "end": 481.44, "text": " I mean, now these large language models, if you take language from lexical to", "tokens": [50364, 286, 914, 11, 586, 613, 2416, 2856, 5245, 11, 498, 291, 747, 2856, 490, 476, 87, 804, 281, 50712], "temperature": 0.0, "avg_logprob": -0.14755022910333448, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.000719990988727659}, {"id": 71, "seek": 47448, "start": 482.48, "end": 489.20000000000005, "text": " syntactic to semantic to pragmatic levels, they have definitely mastered syntax.", "tokens": [50764, 23980, 19892, 281, 47982, 281, 46904, 4358, 11, 436, 362, 2138, 38686, 28431, 13, 51100], "temperature": 0.0, "avg_logprob": -0.14755022910333448, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.000719990988727659}, {"id": 72, "seek": 47448, "start": 490.64000000000004, "end": 499.20000000000005, "text": " And this is not a small feat. I mean, this is huge. They have proven that if I read", "tokens": [51172, 400, 341, 307, 406, 257, 1359, 15425, 13, 286, 914, 11, 341, 307, 2603, 13, 814, 362, 12785, 300, 498, 286, 1401, 51600], "temperature": 0.0, "avg_logprob": -0.14755022910333448, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.000719990988727659}, {"id": 73, "seek": 49920, "start": 500.15999999999997, "end": 506.4, "text": " tons of texts written by humans, I can figure out the grammar of language,", "tokens": [50412, 9131, 295, 15765, 3720, 538, 6255, 11, 286, 393, 2573, 484, 264, 22317, 295, 2856, 11, 50724], "temperature": 0.0, "avg_logprob": -0.14775287241175555, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.003374071093276143}, {"id": 74, "seek": 49920, "start": 506.96, "end": 515.84, "text": " and they have done that. That's huge. Okay, so and I don't like here where I don't like people.", "tokens": [50752, 293, 436, 362, 1096, 300, 13, 663, 311, 2603, 13, 1033, 11, 370, 293, 286, 500, 380, 411, 510, 689, 286, 500, 380, 411, 561, 13, 51196], "temperature": 0.0, "avg_logprob": -0.14775287241175555, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.003374071093276143}, {"id": 75, "seek": 49920, "start": 515.84, "end": 520.24, "text": " I don't want to mention names, but people that supposedly are in my camp, right,", "tokens": [51196, 286, 500, 380, 528, 281, 2152, 5288, 11, 457, 561, 300, 20581, 366, 294, 452, 2255, 11, 558, 11, 51416], "temperature": 0.0, "avg_logprob": -0.14775287241175555, "compression_ratio": 1.485207100591716, "no_speech_prob": 0.003374071093276143}, {"id": 76, "seek": 52024, "start": 521.2, "end": 529.84, "text": " insisting on refusing to see the elephant in the room. No, large language models have proven", "tokens": [50412, 13466, 278, 322, 37289, 281, 536, 264, 19791, 294, 264, 1808, 13, 883, 11, 2416, 2856, 5245, 362, 12785, 50844], "temperature": 0.0, "avg_logprob": -0.10516392191251119, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.0030721593648195267}, {"id": 77, "seek": 52024, "start": 530.4, "end": 538.72, "text": " that if I ingest terabytes of text, I will figure out syntactic rules. They have done that.", "tokens": [50872, 300, 498, 286, 3957, 377, 1796, 24538, 295, 2487, 11, 286, 486, 2573, 484, 23980, 19892, 4474, 13, 814, 362, 1096, 300, 13, 51288], "temperature": 0.0, "avg_logprob": -0.10516392191251119, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.0030721593648195267}, {"id": 78, "seek": 53872, "start": 539.36, "end": 547.84, "text": " Now, okay, and here's where technically, and you have to admit, I mean, they,", "tokens": [50396, 823, 11, 1392, 11, 293, 510, 311, 689, 12120, 11, 293, 291, 362, 281, 9796, 11, 286, 914, 11, 436, 11, 50820], "temperature": 0.0, "avg_logprob": -0.1660671080312421, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.0007790662348270416}, {"id": 79, "seek": 53872, "start": 549.2, "end": 554.5600000000001, "text": " as a matter of fact, they probably know syntax now more than many college graduates.", "tokens": [50888, 382, 257, 1871, 295, 1186, 11, 436, 1391, 458, 28431, 586, 544, 813, 867, 3859, 13577, 13, 51156], "temperature": 0.0, "avg_logprob": -0.1660671080312421, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.0007790662348270416}, {"id": 80, "seek": 53872, "start": 556.48, "end": 563.0400000000001, "text": " Okay, these are from data. That's a huge experiment in cognitive science that", "tokens": [51252, 1033, 11, 613, 366, 490, 1412, 13, 663, 311, 257, 2603, 5120, 294, 15605, 3497, 300, 51580], "temperature": 0.0, "avg_logprob": -0.1660671080312421, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.0007790662348270416}, {"id": 81, "seek": 56304, "start": 563.36, "end": 568.64, "text": " no matter how, how, I mean, you can't be religious in this, you have to be scientific.", "tokens": [50380, 572, 1871, 577, 11, 577, 11, 286, 914, 11, 291, 393, 380, 312, 7185, 294, 341, 11, 291, 362, 281, 312, 8134, 13, 50644], "temperature": 0.0, "avg_logprob": -0.20356783916040794, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0004169922904111445}, {"id": 82, "seek": 56304, "start": 569.5999999999999, "end": 576.9599999999999, "text": " I see the proof that these large language models by ingesting tons of text written by humans,", "tokens": [50692, 286, 536, 264, 8177, 300, 613, 2416, 2856, 5245, 538, 3957, 8714, 9131, 295, 2487, 3720, 538, 6255, 11, 51060], "temperature": 0.0, "avg_logprob": -0.20356783916040794, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0004169922904111445}, {"id": 83, "seek": 56304, "start": 576.9599999999999, "end": 583.8399999999999, "text": " they have figured out the syntax of language. End of story. I have, there's an existential proof.", "tokens": [51060, 436, 362, 8932, 484, 264, 28431, 295, 2856, 13, 6967, 295, 1657, 13, 286, 362, 11, 456, 311, 364, 37133, 8177, 13, 51404], "temperature": 0.0, "avg_logprob": -0.20356783916040794, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0004169922904111445}, {"id": 84, "seek": 56304, "start": 584.4, "end": 588.64, "text": " Go on, open AI, try DaVinci 2 or DaVinci 3 or whatever you like to try.", "tokens": [51432, 1037, 322, 11, 1269, 7318, 11, 853, 3933, 53, 21961, 568, 420, 3933, 53, 21961, 805, 420, 2035, 291, 411, 281, 853, 13, 51644], "temperature": 0.0, "avg_logprob": -0.20356783916040794, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0004169922904111445}, {"id": 85, "seek": 58864, "start": 589.1999999999999, "end": 597.4399999999999, "text": " Their syntactic competency is beyond belief. I'm shocked every time I use it. Okay.", "tokens": [50392, 6710, 23980, 19892, 50097, 307, 4399, 7107, 13, 286, 478, 12763, 633, 565, 286, 764, 309, 13, 1033, 13, 50804], "temperature": 0.0, "avg_logprob": -0.14930583491469873, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.0006770562031306326}, {"id": 86, "seek": 58864, "start": 598.72, "end": 604.24, "text": " Now, that's not the end of language understanding. There's semantics, and then there's pragmatics.", "tokens": [50868, 823, 11, 300, 311, 406, 264, 917, 295, 2856, 3701, 13, 821, 311, 4361, 45298, 11, 293, 550, 456, 311, 33394, 15677, 1167, 13, 51144], "temperature": 0.0, "avg_logprob": -0.14930583491469873, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.0006770562031306326}, {"id": 87, "seek": 58864, "start": 604.24, "end": 611.52, "text": " Wow. I mean, so I'm working on something to quantify. So now we have, I don't know,", "tokens": [51144, 3153, 13, 286, 914, 11, 370, 286, 478, 1364, 322, 746, 281, 40421, 13, 407, 586, 321, 362, 11, 286, 500, 380, 458, 11, 51508], "temperature": 0.0, "avg_logprob": -0.14930583491469873, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.0006770562031306326}, {"id": 88, "seek": 58864, "start": 611.52, "end": 618.48, "text": " we're up to a trillion parameter that allowed me to master syntax. Now, let's see semantics.", "tokens": [51508, 321, 434, 493, 281, 257, 18723, 13075, 300, 4350, 385, 281, 4505, 28431, 13, 823, 11, 718, 311, 536, 4361, 45298, 13, 51856], "temperature": 0.0, "avg_logprob": -0.14930583491469873, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.0006770562031306326}, {"id": 89, "seek": 61848, "start": 618.48, "end": 623.6, "text": " And semantic can be broken down to, have you guys figured out reference resolution? Have you", "tokens": [50364, 400, 47982, 393, 312, 5463, 760, 281, 11, 362, 291, 1074, 8932, 484, 6408, 8669, 30, 3560, 291, 50620], "temperature": 0.0, "avg_logprob": -0.12875872644884834, "compression_ratio": 1.567251461988304, "no_speech_prob": 0.00043710440513677895}, {"id": 90, "seek": 61848, "start": 623.6, "end": 629.2, "text": " guys figured out scope resolution, prepositional phrase attachments? There are so many", "tokens": [50620, 1074, 8932, 484, 11923, 8669, 11, 2666, 329, 2628, 9535, 37987, 30, 821, 366, 370, 867, 50900], "temperature": 0.0, "avg_logprob": -0.12875872644884834, "compression_ratio": 1.567251461988304, "no_speech_prob": 0.00043710440513677895}, {"id": 91, "seek": 61848, "start": 631.52, "end": 639.76, "text": " pain points and semantic processing. Can we quantify how many more parameters we need to", "tokens": [51016, 1822, 2793, 293, 47982, 9007, 13, 1664, 321, 40421, 577, 867, 544, 9834, 321, 643, 281, 51428], "temperature": 0.0, "avg_logprob": -0.12875872644884834, "compression_ratio": 1.567251461988304, "no_speech_prob": 0.00043710440513677895}, {"id": 92, "seek": 63976, "start": 639.84, "end": 644.16, "text": " conquer semantics? And then pragmatics, there are things like,", "tokens": [50368, 24136, 4361, 45298, 30, 400, 550, 33394, 15677, 1167, 11, 456, 366, 721, 411, 11, 50584], "temperature": 0.0, "avg_logprob": -0.17108151220506237, "compression_ratio": 1.4968944099378882, "no_speech_prob": 0.0032205169554799795}, {"id": 93, "seek": 63976, "start": 649.84, "end": 656.64, "text": " the teenager shot a policeman and he immediately fled away. Now, possibly both can,", "tokens": [50868, 264, 21440, 3347, 257, 42658, 293, 415, 4258, 24114, 1314, 13, 823, 11, 6264, 1293, 393, 11, 51208], "temperature": 0.0, "avg_logprob": -0.17108151220506237, "compression_ratio": 1.4968944099378882, "no_speech_prob": 0.0032205169554799795}, {"id": 94, "seek": 63976, "start": 657.36, "end": 663.4399999999999, "text": " the he can be the policeman. He fled away to escape further injuries. I mean, that can happen.", "tokens": [51244, 264, 415, 393, 312, 264, 42658, 13, 634, 24114, 1314, 281, 7615, 3052, 14799, 13, 286, 914, 11, 300, 393, 1051, 13, 51548], "temperature": 0.0, "avg_logprob": -0.17108151220506237, "compression_ratio": 1.4968944099378882, "no_speech_prob": 0.0032205169554799795}, {"id": 95, "seek": 66344, "start": 664.24, "end": 672.8800000000001, "text": " But most likely the one that's led away is the teenager. That's pragmatics because we know in", "tokens": [50404, 583, 881, 3700, 264, 472, 300, 311, 4684, 1314, 307, 264, 21440, 13, 663, 311, 33394, 15677, 1167, 570, 321, 458, 294, 50836], "temperature": 0.0, "avg_logprob": -0.16051992193444983, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.0002232984552392736}, {"id": 96, "seek": 66344, "start": 672.8800000000001, "end": 680.24, "text": " the world we live in, if you shoot someone, they're going to try to capture you and you try to flee,", "tokens": [50836, 264, 1002, 321, 1621, 294, 11, 498, 291, 3076, 1580, 11, 436, 434, 516, 281, 853, 281, 7983, 291, 293, 291, 853, 281, 25146, 11, 51204], "temperature": 0.0, "avg_logprob": -0.16051992193444983, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.0002232984552392736}, {"id": 97, "seek": 66344, "start": 680.24, "end": 686.8800000000001, "text": " right? That's not semantics. That's way beyond. How many parameters beyond semantics do you need", "tokens": [51204, 558, 30, 663, 311, 406, 4361, 45298, 13, 663, 311, 636, 4399, 13, 1012, 867, 9834, 4399, 4361, 45298, 360, 291, 643, 51536], "temperature": 0.0, "avg_logprob": -0.16051992193444983, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.0002232984552392736}, {"id": 98, "seek": 68688, "start": 686.88, "end": 699.36, "text": " to capture that? If you can put a, if you can come up with a rough number, I mean, it could be", "tokens": [50364, 281, 7983, 300, 30, 759, 291, 393, 829, 257, 11, 498, 291, 393, 808, 493, 365, 257, 5903, 1230, 11, 286, 914, 11, 309, 727, 312, 50988], "temperature": 0.0, "avg_logprob": -0.11552923019618204, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.0029300348833203316}, {"id": 99, "seek": 68688, "start": 699.36, "end": 708.4, "text": " a number that's manageable, that's doable by more scaling, which would be an interesting result.", "tokens": [50988, 257, 1230, 300, 311, 38798, 11, 300, 311, 41183, 538, 544, 21589, 11, 597, 576, 312, 364, 1880, 1874, 13, 51440], "temperature": 0.0, "avg_logprob": -0.11552923019618204, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.0029300348833203316}, {"id": 100, "seek": 68688, "start": 708.4, "end": 715.92, "text": " But it could be that it's a number beyond the universe we live in, which means guys,", "tokens": [51440, 583, 309, 727, 312, 300, 309, 311, 257, 1230, 4399, 264, 6445, 321, 1621, 294, 11, 597, 1355, 1074, 11, 51816], "temperature": 0.0, "avg_logprob": -0.11552923019618204, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.0029300348833203316}, {"id": 101, "seek": 71688, "start": 717.04, "end": 721.92, "text": " except that you can master syntax and a little bit of lexical semantics, you can figure out the", "tokens": [50372, 3993, 300, 291, 393, 4505, 28431, 293, 257, 707, 857, 295, 476, 87, 804, 4361, 45298, 11, 291, 393, 2573, 484, 264, 50616], "temperature": 0.0, "avg_logprob": -0.09897457559903462, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0018641242058947682}, {"id": 102, "seek": 71688, "start": 721.92, "end": 729.04, "text": " meaning of some words, but to do full understanding with pragmatics, we're talking about numbers that", "tokens": [50616, 3620, 295, 512, 2283, 11, 457, 281, 360, 1577, 3701, 365, 33394, 15677, 1167, 11, 321, 434, 1417, 466, 3547, 300, 50972], "temperature": 0.0, "avg_logprob": -0.09897457559903462, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0018641242058947682}, {"id": 103, "seek": 71688, "start": 730.0, "end": 737.04, "text": " we might have to wait 2000 years. Yes, in theory, it works. So basically, I'm trying to work now on", "tokens": [51020, 321, 1062, 362, 281, 1699, 8132, 924, 13, 1079, 11, 294, 5261, 11, 309, 1985, 13, 407, 1936, 11, 286, 478, 1382, 281, 589, 586, 322, 51372], "temperature": 0.0, "avg_logprob": -0.09897457559903462, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0018641242058947682}, {"id": 104, "seek": 71688, "start": 737.04, "end": 744.64, "text": " which is going to be very difficult to quantify because they have proven that scale did improve", "tokens": [51372, 597, 307, 516, 281, 312, 588, 2252, 281, 40421, 570, 436, 362, 12785, 300, 4373, 630, 3470, 51752], "temperature": 0.0, "avg_logprob": -0.09897457559903462, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0018641242058947682}, {"id": 105, "seek": 74464, "start": 744.72, "end": 754.16, "text": " syntax, no doubt, not improve it. They've almost mastered syntax. But how far can this go? I mean,", "tokens": [50368, 28431, 11, 572, 6385, 11, 406, 3470, 309, 13, 814, 600, 1920, 38686, 28431, 13, 583, 577, 1400, 393, 341, 352, 30, 286, 914, 11, 50840], "temperature": 0.0, "avg_logprob": -0.17011455253318505, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0013880881015211344}, {"id": 106, "seek": 74464, "start": 754.16, "end": 760.88, "text": " can you quantify how far can this go scientifically without saying, let's try with more, let's try", "tokens": [50840, 393, 291, 40421, 577, 1400, 393, 341, 352, 39719, 1553, 1566, 11, 718, 311, 853, 365, 544, 11, 718, 311, 853, 51176], "temperature": 0.0, "avg_logprob": -0.17011455253318505, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0013880881015211344}, {"id": 107, "seek": 74464, "start": 760.88, "end": 768.0, "text": " with more? Which is, which is not, it's not going to be easy to do. Anyway. So I'm curious, could I", "tokens": [51176, 365, 544, 30, 3013, 307, 11, 597, 307, 406, 11, 309, 311, 406, 516, 281, 312, 1858, 281, 360, 13, 5684, 13, 407, 286, 478, 6369, 11, 727, 286, 51532], "temperature": 0.0, "avg_logprob": -0.17011455253318505, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0013880881015211344}, {"id": 108, "seek": 74464, "start": 768.0, "end": 773.04, "text": " push back a little bit on the syntax you said, if they've mastered syntax, and I'm kind of,", "tokens": [51532, 2944, 646, 257, 707, 857, 322, 264, 28431, 291, 848, 11, 498, 436, 600, 38686, 28431, 11, 293, 286, 478, 733, 295, 11, 51784], "temperature": 0.0, "avg_logprob": -0.17011455253318505, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0013880881015211344}, {"id": 109, "seek": 77304, "start": 773.04, "end": 777.1999999999999, "text": " okay, I mean, I guess you have an existing proof, and there's like a behavioral kind of", "tokens": [50364, 1392, 11, 286, 914, 11, 286, 2041, 291, 362, 364, 6741, 8177, 11, 293, 456, 311, 411, 257, 19124, 733, 295, 50572], "temperature": 0.0, "avg_logprob": -0.16310830797467912, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.0006163671496324241}, {"id": 110, "seek": 77304, "start": 777.92, "end": 782.8, "text": " proof that it does seem to have very syntactic sentences. And obviously, if you're in something", "tokens": [50608, 8177, 300, 309, 775, 1643, 281, 362, 588, 23980, 19892, 16579, 13, 400, 2745, 11, 498, 291, 434, 294, 746, 50852], "temperature": 0.0, "avg_logprob": -0.16310830797467912, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.0006163671496324241}, {"id": 111, "seek": 77304, "start": 782.8, "end": 788.16, "text": " with whatever billion of parameters, but would you say what those rules are? Could we write them", "tokens": [50852, 365, 2035, 5218, 295, 9834, 11, 457, 576, 291, 584, 437, 729, 4474, 366, 30, 7497, 321, 2464, 552, 51120], "temperature": 0.0, "avg_logprob": -0.16310830797467912, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.0006163671496324241}, {"id": 112, "seek": 77304, "start": 788.16, "end": 793.28, "text": " down? Probably not, right? Because it's a billion different numbers of weights. No, no. And you", "tokens": [51120, 760, 30, 9210, 406, 11, 558, 30, 1436, 309, 311, 257, 5218, 819, 3547, 295, 17443, 13, 883, 11, 572, 13, 400, 291, 51376], "temperature": 0.0, "avg_logprob": -0.16310830797467912, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.0006163671496324241}, {"id": 113, "seek": 77304, "start": 793.28, "end": 797.68, "text": " don't have the old school kind of generative grammar approach. And also, it's not the way humans", "tokens": [51376, 500, 380, 362, 264, 1331, 1395, 733, 295, 1337, 1166, 22317, 3109, 13, 400, 611, 11, 309, 311, 406, 264, 636, 6255, 51596], "temperature": 0.0, "avg_logprob": -0.16310830797467912, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.0006163671496324241}, {"id": 114, "seek": 77304, "start": 797.68, "end": 801.4399999999999, "text": " have learned language. And could you reverse engineer or tweak it? We don't know what it is,", "tokens": [51596, 362, 3264, 2856, 13, 400, 727, 291, 9943, 11403, 420, 29879, 309, 30, 492, 500, 380, 458, 437, 309, 307, 11, 51784], "temperature": 0.0, "avg_logprob": -0.16310830797467912, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.0006163671496324241}, {"id": 115, "seek": 80144, "start": 801.44, "end": 809.6800000000001, "text": " it's a black box. So, okay, there's a behavioral tense in which it knows. But isn't it the way", "tokens": [50364, 309, 311, 257, 2211, 2424, 13, 407, 11, 1392, 11, 456, 311, 257, 19124, 18760, 294, 597, 309, 3255, 13, 583, 1943, 380, 309, 264, 636, 50776], "temperature": 0.0, "avg_logprob": -0.13944643619013766, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.001726649352349341}, {"id": 116, "seek": 80144, "start": 809.6800000000001, "end": 816.24, "text": " humans do learn language? I mean, I think it's more, though, it's more related to the way humans", "tokens": [50776, 6255, 360, 1466, 2856, 30, 286, 914, 11, 286, 519, 309, 311, 544, 11, 1673, 11, 309, 311, 544, 4077, 281, 264, 636, 6255, 51104], "temperature": 0.0, "avg_logprob": -0.13944643619013766, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.001726649352349341}, {"id": 117, "seek": 80144, "start": 816.24, "end": 824.96, "text": " learn language than, I mean, I was 20 until I knew grammar. I mean, we use language without,", "tokens": [51104, 1466, 2856, 813, 11, 286, 914, 11, 286, 390, 945, 1826, 286, 2586, 22317, 13, 286, 914, 11, 321, 764, 2856, 1553, 11, 51540], "temperature": 0.0, "avg_logprob": -0.13944643619013766, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.001726649352349341}, {"id": 118, "seek": 80144, "start": 824.96, "end": 830.6400000000001, "text": " without knowing grammatical rules. So there is an argument that humans don't learn grammar,", "tokens": [51540, 1553, 5276, 17570, 267, 804, 4474, 13, 407, 456, 307, 364, 6770, 300, 6255, 500, 380, 1466, 22317, 11, 51824], "temperature": 0.0, "avg_logprob": -0.13944643619013766, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.001726649352349341}, {"id": 119, "seek": 83064, "start": 830.64, "end": 835.04, "text": " that it's, you know, pretty native. And all we do is tweak a few parameters. And then we add", "tokens": [50364, 300, 309, 311, 11, 291, 458, 11, 1238, 8470, 13, 400, 439, 321, 360, 307, 29879, 257, 1326, 9834, 13, 400, 550, 321, 909, 50584], "temperature": 0.0, "avg_logprob": -0.14716793596744537, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.000766781740821898}, {"id": 120, "seek": 83064, "start": 835.04, "end": 840.4, "text": " vocabulary over the years, tweaks or whatever. That's the, you know, the Chomsky and generative", "tokens": [50584, 19864, 670, 264, 924, 11, 46664, 420, 2035, 13, 663, 311, 264, 11, 291, 458, 11, 264, 761, 4785, 4133, 293, 1337, 1166, 50852], "temperature": 0.0, "avg_logprob": -0.14716793596744537, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.000766781740821898}, {"id": 121, "seek": 83064, "start": 840.4, "end": 848.16, "text": " grammar approach. So I don't think that we have a billion parameters that we tweak over 20 years.", "tokens": [50852, 22317, 3109, 13, 407, 286, 500, 380, 519, 300, 321, 362, 257, 5218, 9834, 300, 321, 29879, 670, 945, 924, 13, 51240], "temperature": 0.0, "avg_logprob": -0.14716793596744537, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.000766781740821898}, {"id": 122, "seek": 83064, "start": 848.16, "end": 853.12, "text": " I don't think that's how we work. And we have, I mean, there's amazing competency of children", "tokens": [51240, 286, 500, 380, 519, 300, 311, 577, 321, 589, 13, 400, 321, 362, 11, 286, 914, 11, 456, 311, 2243, 50097, 295, 2227, 51488], "temperature": 0.0, "avg_logprob": -0.14716793596744537, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.000766781740821898}, {"id": 123, "seek": 83064, "start": 853.12, "end": 857.2, "text": " at two years of age, you know, with language they have, you've said, you've, you're writing,", "tokens": [51488, 412, 732, 924, 295, 3205, 11, 291, 458, 11, 365, 2856, 436, 362, 11, 291, 600, 848, 11, 291, 600, 11, 291, 434, 3579, 11, 51692], "temperature": 0.0, "avg_logprob": -0.14716793596744537, "compression_ratio": 1.7453874538745386, "no_speech_prob": 0.000766781740821898}, {"id": 124, "seek": 85720, "start": 857.2, "end": 862.08, "text": " you pointed this out, actually, children know, is absolutely amazing, you know, straight out of,", "tokens": [50364, 291, 10932, 341, 484, 11, 767, 11, 2227, 458, 11, 307, 3122, 2243, 11, 291, 458, 11, 2997, 484, 295, 11, 50608], "temperature": 0.0, "avg_logprob": -0.1398453899458343, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.0073395296931266785}, {"id": 125, "seek": 85720, "start": 862.08, "end": 868.88, "text": " you know, it is amazing, but it is amazing. And I, and I didn't change the way I think about", "tokens": [50608, 291, 458, 11, 309, 307, 2243, 11, 457, 309, 307, 2243, 13, 400, 286, 11, 293, 286, 994, 380, 1319, 264, 636, 286, 519, 466, 50948], "temperature": 0.0, "avg_logprob": -0.1398453899458343, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.0073395296931266785}, {"id": 126, "seek": 85720, "start": 869.6800000000001, "end": 876.72, "text": " and we have innate stuff. But here's the change that these guys have made me", "tokens": [50988, 293, 321, 362, 41766, 1507, 13, 583, 510, 311, 264, 1319, 300, 613, 1074, 362, 1027, 385, 51340], "temperature": 0.0, "avg_logprob": -0.1398453899458343, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.0073395296931266785}, {"id": 127, "seek": 85720, "start": 879.44, "end": 885.2, "text": " go through. It's a minor change, but it's, it's, it's not that subtle, actually. Here's the thing.", "tokens": [51476, 352, 807, 13, 467, 311, 257, 6696, 1319, 11, 457, 309, 311, 11, 309, 311, 11, 309, 311, 406, 300, 13743, 11, 767, 13, 1692, 311, 264, 551, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1398453899458343, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.0073395296931266785}, {"id": 128, "seek": 88520, "start": 885.2, "end": 893.2, "text": " I was told before these new results are coming out that look, we do have innate stuff, which", "tokens": [50364, 286, 390, 1907, 949, 613, 777, 3542, 366, 1348, 484, 300, 574, 11, 321, 360, 362, 41766, 1507, 11, 597, 50764], "temperature": 0.0, "avg_logprob": -0.13744125366210938, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0011333373840898275}, {"id": 129, "seek": 88520, "start": 894.1600000000001, "end": 900.72, "text": " took us three, four hundred thousand years of evolution. All we're doing by ingesting all", "tokens": [50812, 1890, 505, 1045, 11, 1451, 3262, 4714, 924, 295, 9303, 13, 1057, 321, 434, 884, 538, 3957, 8714, 439, 51140], "temperature": 0.0, "avg_logprob": -0.13744125366210938, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0011333373840898275}, {"id": 130, "seek": 88520, "start": 900.72, "end": 911.36, "text": " this text is we're simulating, right, these 300,000 years. So give us a chance to simulate", "tokens": [51140, 341, 2487, 307, 321, 434, 1034, 12162, 11, 558, 11, 613, 6641, 11, 1360, 924, 13, 407, 976, 505, 257, 2931, 281, 27817, 51672], "temperature": 0.0, "avg_logprob": -0.13744125366210938, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0011333373840898275}, {"id": 131, "seek": 91136, "start": 911.36, "end": 921.04, "text": " this innateness if you want, in a way, in a way. Okay, I, that argument was said long time ago,", "tokens": [50364, 341, 41766, 1287, 498, 291, 528, 11, 294, 257, 636, 11, 294, 257, 636, 13, 1033, 11, 286, 11, 300, 6770, 390, 848, 938, 565, 2057, 11, 50848], "temperature": 0.0, "avg_logprob": -0.15721025982418577, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.006138497963547707}, {"id": 132, "seek": 91136, "start": 921.04, "end": 928.5600000000001, "text": " and I thought, come on, you're chasing infinity. What happened with the real difference in my", "tokens": [50848, 293, 286, 1194, 11, 808, 322, 11, 291, 434, 17876, 13202, 13, 708, 2011, 365, 264, 957, 2649, 294, 452, 51224], "temperature": 0.0, "avg_logprob": -0.15721025982418577, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.006138497963547707}, {"id": 133, "seek": 91136, "start": 928.5600000000001, "end": 936.48, "text": " mind now is they have proven that they conquered one beast in language. Nobody can dispute that.", "tokens": [51224, 1575, 586, 307, 436, 362, 12785, 300, 436, 32695, 472, 13464, 294, 2856, 13, 9297, 393, 25379, 300, 13, 51620], "temperature": 0.0, "avg_logprob": -0.15721025982418577, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.006138497963547707}, {"id": 134, "seek": 93648, "start": 937.12, "end": 944.24, "text": " Can I have a go at disputing it? So, in the Polition and Foda connectionism critique,", "tokens": [50396, 1664, 286, 362, 257, 352, 412, 37669, 278, 309, 30, 407, 11, 294, 264, 3635, 849, 293, 479, 13449, 4984, 1434, 25673, 11, 50752], "temperature": 0.0, "avg_logprob": -0.1674403336088536, "compression_ratio": 1.5986159169550174, "no_speech_prob": 0.004750208929181099}, {"id": 135, "seek": 93648, "start": 944.24, "end": 948.72, "text": " they spoke about productivity, you know, the infinite cardinality of language. There was", "tokens": [50752, 436, 7179, 466, 15604, 11, 291, 458, 11, 264, 13785, 2920, 259, 1860, 295, 2856, 13, 821, 390, 50976], "temperature": 0.0, "avg_logprob": -0.1674403336088536, "compression_ratio": 1.5986159169550174, "no_speech_prob": 0.004750208929181099}, {"id": 136, "seek": 93648, "start": 948.72, "end": 954.16, "text": " recently a deep mind paper talking about the Chomsky hierarchy and deep neural network. Well,", "tokens": [50976, 3938, 257, 2452, 1575, 3035, 1417, 466, 264, 761, 4785, 4133, 22333, 293, 2452, 18161, 3209, 13, 1042, 11, 51248], "temperature": 0.0, "avg_logprob": -0.1674403336088536, "compression_ratio": 1.5986159169550174, "no_speech_prob": 0.004750208929181099}, {"id": 137, "seek": 93648, "start": 954.16, "end": 958.88, "text": " I mean, RNNs are regular languages, but, you know, I think transformers and the rest of them are", "tokens": [51248, 286, 914, 11, 45702, 45, 82, 366, 3890, 8650, 11, 457, 11, 291, 458, 11, 286, 519, 4088, 433, 293, 264, 1472, 295, 552, 366, 51484], "temperature": 0.0, "avg_logprob": -0.1674403336088536, "compression_ratio": 1.5986159169550174, "no_speech_prob": 0.004750208929181099}, {"id": 138, "seek": 93648, "start": 958.88, "end": 966.08, "text": " at the bottom of the hierarchy. So quantitatively, we know we haven't conquered infinity. So why", "tokens": [51484, 412, 264, 2767, 295, 264, 22333, 13, 407, 27778, 356, 11, 321, 458, 321, 2378, 380, 32695, 13202, 13, 407, 983, 51844], "temperature": 0.0, "avg_logprob": -0.1674403336088536, "compression_ratio": 1.5986159169550174, "no_speech_prob": 0.004750208929181099}, {"id": 139, "seek": 96608, "start": 966.08, "end": 973.36, "text": " with such a shallow horizon, are they doing so well? I agree. Here's the thing, language use,", "tokens": [50364, 365, 1270, 257, 20488, 18046, 11, 366, 436, 884, 370, 731, 30, 286, 3986, 13, 1692, 311, 264, 551, 11, 2856, 764, 11, 50728], "temperature": 0.0, "avg_logprob": -0.1594104766845703, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.0009083301411010325}, {"id": 140, "seek": 96608, "start": 974.5600000000001, "end": 983.6800000000001, "text": " languages infinite, but probably the long tail of probably 90% of ordinary language use,", "tokens": [50788, 8650, 13785, 11, 457, 1391, 264, 938, 6838, 295, 1391, 4289, 4, 295, 10547, 2856, 764, 11, 51244], "temperature": 0.0, "avg_logprob": -0.1594104766845703, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.0009083301411010325}, {"id": 141, "seek": 96608, "start": 984.4000000000001, "end": 992.5600000000001, "text": " right, can be figured out from the stuff that we write. So they will never capture all of language.", "tokens": [51280, 558, 11, 393, 312, 8932, 484, 490, 264, 1507, 300, 321, 2464, 13, 407, 436, 486, 1128, 7983, 439, 295, 2856, 13, 51688], "temperature": 0.0, "avg_logprob": -0.1594104766845703, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.0009083301411010325}, {"id": 142, "seek": 99256, "start": 993.52, "end": 1002.8, "text": " Yes, but they might reach the level of a competent educated man like us in language competency.", "tokens": [50412, 1079, 11, 457, 436, 1062, 2524, 264, 1496, 295, 257, 29998, 15872, 587, 411, 505, 294, 2856, 50097, 13, 50876], "temperature": 0.0, "avg_logprob": -0.20031071931887895, "compression_ratio": 1.2796610169491525, "no_speech_prob": 0.0018348968587815762}, {"id": 143, "seek": 99256, "start": 1004.0799999999999, "end": 1010.2399999999999, "text": " So, all I'm saying is what they have achieved is a huge", "tokens": [50940, 407, 11, 439, 286, 478, 1566, 307, 437, 436, 362, 11042, 307, 257, 2603, 51248], "temperature": 0.0, "avg_logprob": -0.20031071931887895, "compression_ratio": 1.2796610169491525, "no_speech_prob": 0.0018348968587815762}, {"id": 144, "seek": 101024, "start": 1011.2, "end": 1024.32, "text": " result in terms of the big question of scale and big data. They have definitely proved that", "tokens": [50412, 1874, 294, 2115, 295, 264, 955, 1168, 295, 4373, 293, 955, 1412, 13, 814, 362, 2138, 14617, 300, 51068], "temperature": 0.0, "avg_logprob": -0.1558640846839318, "compression_ratio": 1.4124293785310735, "no_speech_prob": 0.006579392123967409}, {"id": 145, "seek": 101024, "start": 1024.96, "end": 1029.52, "text": " if I see enough data, I will learn something and something that's not trivial.", "tokens": [51100, 498, 286, 536, 1547, 1412, 11, 286, 486, 1466, 746, 293, 746, 300, 311, 406, 26703, 13, 51328], "temperature": 0.0, "avg_logprob": -0.1558640846839318, "compression_ratio": 1.4124293785310735, "no_speech_prob": 0.006579392123967409}, {"id": 146, "seek": 101024, "start": 1031.2, "end": 1035.04, "text": " Look, you know where I'm coming from. You're talking Foda and Polition. I mean,", "tokens": [51412, 2053, 11, 291, 458, 689, 286, 478, 1348, 490, 13, 509, 434, 1417, 479, 13449, 293, 3635, 849, 13, 286, 914, 11, 51604], "temperature": 0.0, "avg_logprob": -0.1558640846839318, "compression_ratio": 1.4124293785310735, "no_speech_prob": 0.006579392123967409}, {"id": 147, "seek": 103504, "start": 1035.04, "end": 1040.6399999999999, "text": " you're preaching to the choir, right? But I have to be a scientist too. I mean, I don't like,", "tokens": [50364, 291, 434, 25381, 281, 264, 31244, 11, 558, 30, 583, 286, 362, 281, 312, 257, 12662, 886, 13, 286, 914, 11, 286, 500, 380, 411, 11, 50644], "temperature": 0.0, "avg_logprob": -0.18796032496861048, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.036711689084768295}, {"id": 148, "seek": 103504, "start": 1040.6399999999999, "end": 1046.8, "text": " I'm following Gary Marcus, and he's like, I don't like people that minimize what happened.", "tokens": [50644, 286, 478, 3480, 13788, 26574, 11, 293, 415, 311, 411, 11, 286, 500, 380, 411, 561, 300, 17522, 437, 2011, 13, 50952], "temperature": 0.0, "avg_logprob": -0.18796032496861048, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.036711689084768295}, {"id": 149, "seek": 103504, "start": 1048.24, "end": 1055.12, "text": " I'm a scientist, right? I see a big result. I say, wow, right? And look, we're talking, nobody", "tokens": [51024, 286, 478, 257, 12662, 11, 558, 30, 286, 536, 257, 955, 1874, 13, 286, 584, 11, 6076, 11, 558, 30, 400, 574, 11, 321, 434, 1417, 11, 5079, 51368], "temperature": 0.0, "avg_logprob": -0.18796032496861048, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.036711689084768295}, {"id": 150, "seek": 103504, "start": 1055.12, "end": 1058.72, "text": " bashed deep learning more than me, especially large language models. I mean, I'm like,", "tokens": [51368, 987, 27096, 2452, 2539, 544, 813, 385, 11, 2318, 2416, 2856, 5245, 13, 286, 914, 11, 286, 478, 411, 11, 51548], "temperature": 0.0, "avg_logprob": -0.18796032496861048, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.036711689084768295}, {"id": 151, "seek": 105872, "start": 1059.3600000000001, "end": 1069.28, "text": " I was saying this is silly, right? But I have to say they have proven something to me at least,", "tokens": [50396, 286, 390, 1566, 341, 307, 11774, 11, 558, 30, 583, 286, 362, 281, 584, 436, 362, 12785, 746, 281, 385, 412, 1935, 11, 50892], "temperature": 0.0, "avg_logprob": -0.13828556189376318, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.002182228956371546}, {"id": 152, "seek": 105872, "start": 1070.4, "end": 1076.24, "text": " which is huge because I know how difficult language is. I am impressed equally.", "tokens": [50948, 597, 307, 2603, 570, 286, 458, 577, 2252, 2856, 307, 13, 286, 669, 11679, 12309, 13, 51240], "temperature": 0.0, "avg_logprob": -0.13828556189376318, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.002182228956371546}, {"id": 153, "seek": 105872, "start": 1077.52, "end": 1081.2, "text": " Wouldn't you say it's an engineering, an engineering triumph rather than a scientific?", "tokens": [51304, 26291, 380, 291, 584, 309, 311, 364, 7043, 11, 364, 7043, 29156, 2831, 813, 257, 8134, 30, 51488], "temperature": 0.0, "avg_logprob": -0.13828556189376318, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.002182228956371546}, {"id": 154, "seek": 105872, "start": 1082.16, "end": 1087.04, "text": " It's an engineering triumph. But here's the point, Mark. I think it's a little bit more.", "tokens": [51536, 467, 311, 364, 7043, 29156, 13, 583, 510, 311, 264, 935, 11, 3934, 13, 286, 519, 309, 311, 257, 707, 857, 544, 13, 51780], "temperature": 0.0, "avg_logprob": -0.13828556189376318, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.002182228956371546}, {"id": 155, "seek": 108704, "start": 1087.84, "end": 1092.8, "text": " That's the only thing I'm trying to, I'm not saying, look, I didn't give up on, I can get to", "tokens": [50404, 663, 311, 264, 787, 551, 286, 478, 1382, 281, 11, 286, 478, 406, 1566, 11, 574, 11, 286, 994, 380, 976, 493, 322, 11, 286, 393, 483, 281, 50652], "temperature": 0.0, "avg_logprob": -0.12423600402532839, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.0006459372816607356}, {"id": 156, "seek": 108704, "start": 1092.8, "end": 1099.52, "text": " the criticism later. So don't put me on in that camp yet, right? Or, or ever, right? Because I know,", "tokens": [50652, 264, 15835, 1780, 13, 407, 500, 380, 829, 385, 322, 294, 300, 2255, 1939, 11, 558, 30, 1610, 11, 420, 1562, 11, 558, 30, 1436, 286, 458, 11, 50988], "temperature": 0.0, "avg_logprob": -0.12423600402532839, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.0006459372816607356}, {"id": 157, "seek": 108704, "start": 1099.52, "end": 1105.84, "text": " I know theoretically, theoretically, mathematically, you cannot understand language this way.", "tokens": [50988, 286, 458, 29400, 11, 29400, 11, 44003, 11, 291, 2644, 1223, 2856, 341, 636, 13, 51304], "temperature": 0.0, "avg_logprob": -0.12423600402532839, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.0006459372816607356}, {"id": 158, "seek": 108704, "start": 1105.84, "end": 1113.2, "text": " All I'm saying is, in terms of cognitive science, what happened and what is happening as we speak", "tokens": [51304, 1057, 286, 478, 1566, 307, 11, 294, 2115, 295, 15605, 3497, 11, 437, 2011, 293, 437, 307, 2737, 382, 321, 1710, 51672], "temperature": 0.0, "avg_logprob": -0.12423600402532839, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.0006459372816607356}, {"id": 159, "seek": 111320, "start": 1113.76, "end": 1124.16, "text": " is not nothing. It's a huge, for example, if I can ingest a lot, again, what they prove is,", "tokens": [50392, 307, 406, 1825, 13, 467, 311, 257, 2603, 11, 337, 1365, 11, 498, 286, 393, 3957, 377, 257, 688, 11, 797, 11, 437, 436, 7081, 307, 11, 50912], "temperature": 0.0, "avg_logprob": -0.2054536372055242, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.0034248882438987494}, {"id": 160, "seek": 111320, "start": 1124.16, "end": 1132.48, "text": " is that well, the two are related. So it's one thing scale from tons and tons of data. I can learn", "tokens": [50912, 307, 300, 731, 11, 264, 732, 366, 4077, 13, 407, 309, 311, 472, 551, 4373, 490, 9131, 293, 9131, 295, 1412, 13, 286, 393, 1466, 51328], "temperature": 0.0, "avg_logprob": -0.2054536372055242, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.0034248882438987494}, {"id": 161, "seek": 111320, "start": 1132.48, "end": 1140.72, "text": " something that is not trivial. That to me has been proven. The point I'm making is not the", "tokens": [51328, 746, 300, 307, 406, 26703, 13, 663, 281, 385, 575, 668, 12785, 13, 440, 935, 286, 478, 1455, 307, 406, 264, 51740], "temperature": 0.0, "avg_logprob": -0.2054536372055242, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.0034248882438987494}, {"id": 162, "seek": 114072, "start": 1140.8, "end": 1144.24, "text": " point I'm making is not that they solve the language problem. Sorry.", "tokens": [50368, 935, 286, 478, 1455, 307, 406, 300, 436, 5039, 264, 2856, 1154, 13, 4919, 13, 50540], "temperature": 0.0, "avg_logprob": -0.14332463719823338, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0022509838454425335}, {"id": 163, "seek": 114072, "start": 1144.24, "end": 1148.72, "text": " Yeah, I want to jump in here a little bit. Because from my perspective, I think", "tokens": [50540, 865, 11, 286, 528, 281, 3012, 294, 510, 257, 707, 857, 13, 1436, 490, 452, 4585, 11, 286, 519, 50764], "temperature": 0.0, "avg_logprob": -0.14332463719823338, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0022509838454425335}, {"id": 164, "seek": 114072, "start": 1149.6000000000001, "end": 1156.24, "text": " part of why you're saying it's huge is because I think it was a huge step for you personally.", "tokens": [50808, 644, 295, 983, 291, 434, 1566, 309, 311, 2603, 307, 570, 286, 519, 309, 390, 257, 2603, 1823, 337, 291, 5665, 13, 51140], "temperature": 0.0, "avg_logprob": -0.14332463719823338, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0022509838454425335}, {"id": 165, "seek": 114072, "start": 1156.24, "end": 1160.16, "text": " Because I know, you know, from the past, like talking to you, like you've had a much more", "tokens": [51140, 1436, 286, 458, 11, 291, 458, 11, 490, 264, 1791, 11, 411, 1417, 281, 291, 11, 411, 291, 600, 632, 257, 709, 544, 51336], "temperature": 0.0, "avg_logprob": -0.14332463719823338, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0022509838454425335}, {"id": 166, "seek": 114072, "start": 1160.16, "end": 1166.72, "text": " extreme view, you know, on the capabilities of large language models than, for example, myself.", "tokens": [51336, 8084, 1910, 11, 291, 458, 11, 322, 264, 10862, 295, 2416, 2856, 5245, 813, 11, 337, 1365, 11, 2059, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14332463719823338, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.0022509838454425335}, {"id": 167, "seek": 116672, "start": 1166.72, "end": 1172.24, "text": " Because for me, I don't see anything new here. It's kind of like, I'll give you an example", "tokens": [50364, 1436, 337, 385, 11, 286, 500, 380, 536, 1340, 777, 510, 13, 467, 311, 733, 295, 411, 11, 286, 603, 976, 291, 364, 1365, 50640], "temperature": 0.0, "avg_logprob": -0.11216211820903578, "compression_ratio": 1.5, "no_speech_prob": 0.010326663963496685}, {"id": 168, "seek": 116672, "start": 1172.24, "end": 1178.56, "text": " outside of syntax just for a moment. So just transcription, because that's what Tim and I", "tokens": [50640, 2380, 295, 28431, 445, 337, 257, 1623, 13, 407, 445, 35288, 11, 570, 300, 311, 437, 7172, 293, 286, 50956], "temperature": 0.0, "avg_logprob": -0.11216211820903578, "compression_ratio": 1.5, "no_speech_prob": 0.010326663963496685}, {"id": 169, "seek": 116672, "start": 1178.56, "end": 1184.4, "text": " happen to be working on quite a bit right now. In other words, transcribing audio into text.", "tokens": [50956, 1051, 281, 312, 1364, 322, 1596, 257, 857, 558, 586, 13, 682, 661, 2283, 11, 1145, 39541, 6278, 666, 2487, 13, 51248], "temperature": 0.0, "avg_logprob": -0.11216211820903578, "compression_ratio": 1.5, "no_speech_prob": 0.010326663963496685}, {"id": 170, "seek": 116672, "start": 1186.48, "end": 1191.52, "text": " All the state-of-the-art models are pretty much sitting around each other at about 90%", "tokens": [51352, 1057, 264, 1785, 12, 2670, 12, 3322, 12, 446, 5245, 366, 1238, 709, 3798, 926, 1184, 661, 412, 466, 4289, 4, 51604], "temperature": 0.0, "avg_logprob": -0.11216211820903578, "compression_ratio": 1.5, "no_speech_prob": 0.010326663963496685}, {"id": 171, "seek": 119152, "start": 1192.16, "end": 1198.56, "text": " you know, accuracy right of transcription. But here's the thing is that's for people speaking", "tokens": [50396, 291, 458, 11, 14170, 558, 295, 35288, 13, 583, 510, 311, 264, 551, 307, 300, 311, 337, 561, 4124, 50716], "temperature": 0.0, "avg_logprob": -0.1586416069118456, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0826696902513504}, {"id": 172, "seek": 119152, "start": 1199.52, "end": 1206.48, "text": " relatively common languages with a relatively standard accent. Okay, as soon as you bring", "tokens": [50764, 7226, 2689, 8650, 365, 257, 7226, 3832, 11982, 13, 1033, 11, 382, 2321, 382, 291, 1565, 51112], "temperature": 0.0, "avg_logprob": -0.1586416069118456, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0826696902513504}, {"id": 173, "seek": 119152, "start": 1206.48, "end": 1213.52, "text": " someone in the room that has an accent or speaks a, you know, with maybe like some type of a", "tokens": [51112, 1580, 294, 264, 1808, 300, 575, 364, 11982, 420, 10789, 257, 11, 291, 458, 11, 365, 1310, 411, 512, 2010, 295, 257, 51464], "temperature": 0.0, "avg_logprob": -0.1586416069118456, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0826696902513504}, {"id": 174, "seek": 119152, "start": 1213.52, "end": 1218.6399999999999, "text": " challenge, like a speech challenge, or this side of the other thing, it becomes garbage again.", "tokens": [51464, 3430, 11, 411, 257, 6218, 3430, 11, 420, 341, 1252, 295, 264, 661, 551, 11, 309, 3643, 14150, 797, 13, 51720], "temperature": 0.0, "avg_logprob": -0.1586416069118456, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0826696902513504}, {"id": 175, "seek": 121864, "start": 1218.72, "end": 1226.0, "text": " And like for Tim and I, or there's noise in the background, music playing in the", "tokens": [50368, 400, 411, 337, 7172, 293, 286, 11, 420, 456, 311, 5658, 294, 264, 3678, 11, 1318, 2433, 294, 264, 50732], "temperature": 0.0, "avg_logprob": -0.15536125054520167, "compression_ratio": 1.6045454545454545, "no_speech_prob": 0.07260280102491379}, {"id": 176, "seek": 121864, "start": 1226.0, "end": 1230.8000000000002, "text": " background. And as Tim and I have probably hammered, you know, to death and beaten a", "tokens": [50732, 3678, 13, 400, 382, 7172, 293, 286, 362, 1391, 13017, 292, 11, 291, 458, 11, 281, 2966, 293, 17909, 257, 50972], "temperature": 0.0, "avg_logprob": -0.15536125054520167, "compression_ratio": 1.6045454545454545, "no_speech_prob": 0.07260280102491379}, {"id": 177, "seek": 121864, "start": 1230.8000000000002, "end": 1236.3200000000002, "text": " dead horse on our channel like so many times, we've never doubted that machine learning can learn", "tokens": [50972, 3116, 6832, 322, 527, 2269, 411, 370, 867, 1413, 11, 321, 600, 1128, 6385, 292, 300, 3479, 2539, 393, 1466, 51248], "temperature": 0.0, "avg_logprob": -0.15536125054520167, "compression_ratio": 1.6045454545454545, "no_speech_prob": 0.07260280102491379}, {"id": 178, "seek": 121864, "start": 1236.3200000000002, "end": 1242.48, "text": " like the bulk of the curve, where it really, really struggles is in all these edge cases,", "tokens": [51248, 411, 264, 16139, 295, 264, 7605, 11, 689, 309, 534, 11, 534, 17592, 307, 294, 439, 613, 4691, 3331, 11, 51556], "temperature": 0.0, "avg_logprob": -0.15536125054520167, "compression_ratio": 1.6045454545454545, "no_speech_prob": 0.07260280102491379}, {"id": 179, "seek": 124248, "start": 1242.56, "end": 1249.76, "text": " and the corner cases, and the periphery where it can easily, it's very brittle, right, in those", "tokens": [50368, 293, 264, 4538, 3331, 11, 293, 264, 26807, 88, 689, 309, 393, 3612, 11, 309, 311, 588, 49325, 11, 558, 11, 294, 729, 50728], "temperature": 0.0, "avg_logprob": -0.13863523051423846, "compression_ratio": 1.545816733067729, "no_speech_prob": 0.07054402679204941}, {"id": 180, "seek": 124248, "start": 1249.76, "end": 1253.84, "text": " kind of areas. Like this is the point we've been making out for a long, long time. And so the fact", "tokens": [50728, 733, 295, 3179, 13, 1743, 341, 307, 264, 935, 321, 600, 668, 1455, 484, 337, 257, 938, 11, 938, 565, 13, 400, 370, 264, 1186, 50932], "temperature": 0.0, "avg_logprob": -0.13863523051423846, "compression_ratio": 1.545816733067729, "no_speech_prob": 0.07054402679204941}, {"id": 181, "seek": 124248, "start": 1253.84, "end": 1260.48, "text": " that like massive trillions of parameters and terabytes of data was able to learn 90% or more,", "tokens": [50932, 300, 411, 5994, 504, 46279, 295, 9834, 293, 1796, 24538, 295, 1412, 390, 1075, 281, 1466, 4289, 4, 420, 544, 11, 51264], "temperature": 0.0, "avg_logprob": -0.13863523051423846, "compression_ratio": 1.545816733067729, "no_speech_prob": 0.07054402679204941}, {"id": 182, "seek": 124248, "start": 1260.48, "end": 1267.68, "text": " 95% or whatever is syntax. Okay, I get it. Like from a linguist perspective, that was a, you know,", "tokens": [51264, 13420, 4, 420, 2035, 307, 28431, 13, 1033, 11, 286, 483, 309, 13, 1743, 490, 257, 21766, 468, 4585, 11, 300, 390, 257, 11, 291, 458, 11, 51624], "temperature": 0.0, "avg_logprob": -0.13863523051423846, "compression_ratio": 1.545816733067729, "no_speech_prob": 0.07054402679204941}, {"id": 183, "seek": 126768, "start": 1267.68, "end": 1272.3200000000002, "text": " maybe a big triumph or something. But I'm still always about that other like 5%.", "tokens": [50364, 1310, 257, 955, 29156, 420, 746, 13, 583, 286, 478, 920, 1009, 466, 300, 661, 411, 1025, 6856, 50596], "temperature": 0.0, "avg_logprob": -0.10604344736231436, "compression_ratio": 1.5877862595419847, "no_speech_prob": 0.0046803830191493034}, {"id": 184, "seek": 126768, "start": 1273.2, "end": 1277.44, "text": " And the problem with the approach of deep neural networks is to get that other 5%", "tokens": [50640, 400, 264, 1154, 365, 264, 3109, 295, 2452, 18161, 9590, 307, 281, 483, 300, 661, 1025, 4, 50852], "temperature": 0.0, "avg_logprob": -0.10604344736231436, "compression_ratio": 1.5877862595419847, "no_speech_prob": 0.0046803830191493034}, {"id": 185, "seek": 126768, "start": 1278.48, "end": 1285.2, "text": " is like 100 times as many more parameters, whereas like using, whereas using more generalized,", "tokens": [50904, 307, 411, 2319, 1413, 382, 867, 544, 9834, 11, 9735, 411, 1228, 11, 9735, 1228, 544, 44498, 11, 51240], "temperature": 0.0, "avg_logprob": -0.10604344736231436, "compression_ratio": 1.5877862595419847, "no_speech_prob": 0.0046803830191493034}, {"id": 186, "seek": 126768, "start": 1285.2, "end": 1289.28, "text": " abstracted, you know, methods that we haven't yet really discovered.", "tokens": [51240, 12649, 292, 11, 291, 458, 11, 7150, 300, 321, 2378, 380, 1939, 534, 6941, 13, 51444], "temperature": 0.0, "avg_logprob": -0.10604344736231436, "compression_ratio": 1.5877862595419847, "no_speech_prob": 0.0046803830191493034}, {"id": 187, "seek": 126768, "start": 1289.28, "end": 1293.28, "text": " You're hitting it on the nail. And that's why I'm working out on quantifying this because", "tokens": [51444, 509, 434, 8850, 309, 322, 264, 10173, 13, 400, 300, 311, 983, 286, 478, 1364, 484, 322, 4426, 5489, 341, 570, 51644], "temperature": 0.0, "avg_logprob": -0.10604344736231436, "compression_ratio": 1.5877862595419847, "no_speech_prob": 0.0046803830191493034}, {"id": 188, "seek": 129328, "start": 1294.16, "end": 1302.32, "text": " now we are doing exponential growth in the number of parameters for not even linear growth in the", "tokens": [50408, 586, 321, 366, 884, 21510, 4599, 294, 264, 1230, 295, 9834, 337, 406, 754, 8213, 4599, 294, 264, 50816], "temperature": 0.0, "avg_logprob": -0.11309678587194992, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.00016088710981421173}, {"id": 189, "seek": 129328, "start": 1302.32, "end": 1313.52, "text": " accuracy, even logarithmic, I agree with you 100%. So that other 10% might require 2000 years of data", "tokens": [50816, 14170, 11, 754, 41473, 355, 13195, 11, 286, 3986, 365, 291, 2319, 6856, 407, 300, 661, 1266, 4, 1062, 3651, 8132, 924, 295, 1412, 51376], "temperature": 0.0, "avg_logprob": -0.11309678587194992, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.00016088710981421173}, {"id": 190, "seek": 129328, "start": 1313.52, "end": 1318.16, "text": " that we don't even have. That's what I'm working on. How far can this go because the function", "tokens": [51376, 300, 321, 500, 380, 754, 362, 13, 663, 311, 437, 286, 478, 1364, 322, 13, 1012, 1400, 393, 341, 352, 570, 264, 2445, 51608], "temperature": 0.0, "avg_logprob": -0.11309678587194992, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.00016088710981421173}, {"id": 191, "seek": 131816, "start": 1319.0400000000002, "end": 1325.3600000000001, "text": " is against them now? Like, I mean, we're increasing GPU power and the number of data", "tokens": [50408, 307, 1970, 552, 586, 30, 1743, 11, 286, 914, 11, 321, 434, 5662, 18407, 1347, 293, 264, 1230, 295, 1412, 50724], "temperature": 0.0, "avg_logprob": -0.15062661881142475, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.002358600264415145}, {"id": 192, "seek": 131816, "start": 1325.3600000000001, "end": 1332.96, "text": " that we're ingesting exponentially for a minute increase in accuracy, which is,", "tokens": [50724, 300, 321, 434, 3957, 8714, 37330, 337, 257, 3456, 3488, 294, 14170, 11, 597, 307, 11, 51104], "temperature": 0.0, "avg_logprob": -0.15062661881142475, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.002358600264415145}, {"id": 193, "seek": 131816, "start": 1332.96, "end": 1337.6000000000001, "text": " right, that's the end of the logarithmic. And this is, this is part of the announcers that we", "tokens": [51104, 558, 11, 300, 311, 264, 917, 295, 264, 41473, 355, 13195, 13, 400, 341, 307, 11, 341, 307, 644, 295, 264, 4262, 8530, 300, 321, 51336], "temperature": 0.0, "avg_logprob": -0.15062661881142475, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.002358600264415145}, {"id": 194, "seek": 131816, "start": 1337.6000000000001, "end": 1346.3200000000002, "text": " have to go through. So look, all my reservations that I had before apply. So I'm being misunderstood.", "tokens": [51336, 362, 281, 352, 807, 13, 407, 574, 11, 439, 452, 40222, 300, 286, 632, 949, 3079, 13, 407, 286, 478, 885, 33870, 13, 51772], "temperature": 0.0, "avg_logprob": -0.15062661881142475, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.002358600264415145}, {"id": 195, "seek": 134632, "start": 1346.32, "end": 1353.36, "text": " All I'm saying is simple. These guys, what they have done is not as trivial as I thought initially.", "tokens": [50364, 1057, 286, 478, 1566, 307, 2199, 13, 1981, 1074, 11, 437, 436, 362, 1096, 307, 406, 382, 26703, 382, 286, 1194, 9105, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1048898790396896, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0014766240492463112}, {"id": 196, "seek": 134632, "start": 1354.08, "end": 1360.3999999999999, "text": " Okay, so let me, let me really be very careful in what I'm saying because now I have a following.", "tokens": [50752, 1033, 11, 370, 718, 385, 11, 718, 385, 534, 312, 588, 5026, 294, 437, 286, 478, 1566, 570, 586, 286, 362, 257, 3480, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1048898790396896, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0014766240492463112}, {"id": 197, "seek": 134632, "start": 1360.3999999999999, "end": 1368.96, "text": " I don't want to lose it. No, I'm not, I'm not changing scientifically where I was. I mean,", "tokens": [51068, 286, 500, 380, 528, 281, 3624, 309, 13, 883, 11, 286, 478, 406, 11, 286, 478, 406, 4473, 39719, 689, 286, 390, 13, 286, 914, 11, 51496], "temperature": 0.0, "avg_logprob": -0.1048898790396896, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0014766240492463112}, {"id": 198, "seek": 134632, "start": 1368.96, "end": 1373.6799999999998, "text": " science is science. And I know theoretically, I don't get into things like intentionality and", "tokens": [51496, 3497, 307, 3497, 13, 400, 286, 458, 29400, 11, 286, 500, 380, 483, 666, 721, 411, 7789, 1860, 293, 51732], "temperature": 0.0, "avg_logprob": -0.1048898790396896, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0014766240492463112}, {"id": 199, "seek": 137368, "start": 1374.4, "end": 1378.64, "text": " these models understand nothing about the word. I'm talking about syntax only, by the way,", "tokens": [50400, 613, 5245, 1223, 1825, 466, 264, 1349, 13, 286, 478, 1417, 466, 28431, 787, 11, 538, 264, 636, 11, 50612], "temperature": 0.0, "avg_logprob": -0.14338165105775344, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0006873231031931937}, {"id": 200, "seek": 137368, "start": 1379.44, "end": 1386.16, "text": " syntax on and some coherence when they patch things together. The coherence is amazing.", "tokens": [50652, 28431, 322, 293, 512, 26528, 655, 562, 436, 9972, 721, 1214, 13, 440, 26528, 655, 307, 2243, 13, 50988], "temperature": 0.0, "avg_logprob": -0.14338165105775344, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0006873231031931937}, {"id": 201, "seek": 137368, "start": 1386.96, "end": 1394.0800000000002, "text": " They're not patching things together that don't relate at all. So I'm talking about syntax and,", "tokens": [51028, 814, 434, 406, 9972, 278, 721, 1214, 300, 500, 380, 10961, 412, 439, 13, 407, 286, 478, 1417, 466, 28431, 293, 11, 51384], "temperature": 0.0, "avg_logprob": -0.14338165105775344, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0006873231031931937}, {"id": 202, "seek": 137368, "start": 1394.0800000000002, "end": 1399.04, "text": " and coherence and syntax. Okay. And a touch of semantics, right?", "tokens": [51384, 293, 26528, 655, 293, 28431, 13, 1033, 13, 400, 257, 2557, 295, 4361, 45298, 11, 558, 30, 51632], "temperature": 0.0, "avg_logprob": -0.14338165105775344, "compression_ratio": 1.7295918367346939, "no_speech_prob": 0.0006873231031931937}, {"id": 203, "seek": 139904, "start": 1399.44, "end": 1404.24, "text": " My point, let me repeat it so that I'm not misunderstood.", "tokens": [50384, 1222, 935, 11, 718, 385, 7149, 309, 370, 300, 286, 478, 406, 33870, 13, 50624], "temperature": 0.0, "avg_logprob": -0.15950522703282974, "compression_ratio": 1.5129870129870129, "no_speech_prob": 0.0014511169865727425}, {"id": 204, "seek": 139904, "start": 1406.24, "end": 1414.6399999999999, "text": " They have proven something that many cognitive scientists would never accept, never ever.", "tokens": [50724, 814, 362, 12785, 746, 300, 867, 15605, 7708, 576, 1128, 3241, 11, 1128, 1562, 13, 51144], "temperature": 0.0, "avg_logprob": -0.15950522703282974, "compression_ratio": 1.5129870129870129, "no_speech_prob": 0.0014511169865727425}, {"id": 205, "seek": 139904, "start": 1416.8, "end": 1425.36, "text": " But this existential proof has told many cognitive scientists, don't dismiss learning", "tokens": [51252, 583, 341, 37133, 8177, 575, 1907, 867, 15605, 7708, 11, 500, 380, 16974, 2539, 51680], "temperature": 0.0, "avg_logprob": -0.15950522703282974, "compression_ratio": 1.5129870129870129, "no_speech_prob": 0.0014511169865727425}, {"id": 206, "seek": 142536, "start": 1425.36, "end": 1430.7199999999998, "text": " from data only, blind, no labeling, some aspects of language,", "tokens": [50364, 490, 1412, 787, 11, 6865, 11, 572, 40244, 11, 512, 7270, 295, 2856, 11, 50632], "temperature": 0.0, "avg_logprob": -0.13121145301394993, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.0014981611166149378}, {"id": 207, "seek": 142536, "start": 1432.7199999999998, "end": 1437.76, "text": " actually very impressive aspects of language. These guys have proven that.", "tokens": [50732, 767, 588, 8992, 7270, 295, 2856, 13, 1981, 1074, 362, 12785, 300, 13, 50984], "temperature": 0.0, "avg_logprob": -0.13121145301394993, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.0014981611166149378}, {"id": 208, "seek": 142536, "start": 1438.9599999999998, "end": 1446.3999999999999, "text": " And me as a cognitive scientist, I have to admit because I see it. I see from data alone,", "tokens": [51044, 400, 385, 382, 257, 15605, 12662, 11, 286, 362, 281, 9796, 570, 286, 536, 309, 13, 286, 536, 490, 1412, 3312, 11, 51416], "temperature": 0.0, "avg_logprob": -0.13121145301394993, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.0014981611166149378}, {"id": 209, "seek": 142536, "start": 1447.04, "end": 1450.7199999999998, "text": " these systems have learned non-trivial aspects of language.", "tokens": [51448, 613, 3652, 362, 3264, 2107, 12, 83, 470, 22640, 7270, 295, 2856, 13, 51632], "temperature": 0.0, "avg_logprob": -0.13121145301394993, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.0014981611166149378}, {"id": 210, "seek": 145072, "start": 1451.6000000000001, "end": 1457.92, "text": " Now, how do you interpret that? Where do you take it? What do you conclude from it?", "tokens": [50408, 823, 11, 577, 360, 291, 7302, 300, 30, 2305, 360, 291, 747, 309, 30, 708, 360, 291, 16886, 490, 309, 30, 50724], "temperature": 0.0, "avg_logprob": -0.189021970184756, "compression_ratio": 1.5082872928176796, "no_speech_prob": 0.001243982813321054}, {"id": 211, "seek": 145072, "start": 1457.92, "end": 1465.6000000000001, "text": " We can, we can debate that. But all I'm saying is, I have seen something that I never thought I would", "tokens": [50724, 492, 393, 11, 321, 393, 7958, 300, 13, 583, 439, 286, 478, 1566, 307, 11, 286, 362, 1612, 746, 300, 286, 1128, 1194, 286, 576, 51108], "temperature": 0.0, "avg_logprob": -0.189021970184756, "compression_ratio": 1.5082872928176796, "no_speech_prob": 0.001243982813321054}, {"id": 212, "seek": 145072, "start": 1465.6000000000001, "end": 1475.2, "text": " see, that just ingesting text in these deep networks, you can actually figure something", "tokens": [51108, 536, 11, 300, 445, 3957, 8714, 2487, 294, 613, 2452, 9590, 11, 291, 393, 767, 2573, 746, 51588], "temperature": 0.0, "avg_logprob": -0.189021970184756, "compression_ratio": 1.5082872928176796, "no_speech_prob": 0.001243982813321054}, {"id": 213, "seek": 147520, "start": 1476.16, "end": 1484.24, "text": " not trivial about language. That has been done. I mean, you can, you can say there are pigs,", "tokens": [50412, 406, 26703, 466, 2856, 13, 663, 575, 668, 1096, 13, 286, 914, 11, 291, 393, 11, 291, 393, 584, 456, 366, 24380, 11, 50816], "temperature": 0.0, "avg_logprob": -0.18164000573096337, "compression_ratio": 1.56, "no_speech_prob": 0.0021097890567034483}, {"id": 214, "seek": 147520, "start": 1484.24, "end": 1491.3600000000001, "text": " pigs that fly. Okay. Prove me wrong. I saw them prove they don't exist. Well, I can.", "tokens": [50816, 24380, 300, 3603, 13, 1033, 13, 1705, 303, 385, 2085, 13, 286, 1866, 552, 7081, 436, 500, 380, 2514, 13, 1042, 11, 286, 393, 13, 51172], "temperature": 0.0, "avg_logprob": -0.18164000573096337, "compression_ratio": 1.56, "no_speech_prob": 0.0021097890567034483}, {"id": 215, "seek": 147520, "start": 1492.0800000000002, "end": 1501.76, "text": " But existential proofs are the most powerful proofs. It's an existential proof, proof by doing.", "tokens": [51208, 583, 37133, 8177, 82, 366, 264, 881, 4005, 8177, 82, 13, 467, 311, 364, 37133, 8177, 11, 8177, 538, 884, 13, 51692], "temperature": 0.0, "avg_logprob": -0.18164000573096337, "compression_ratio": 1.56, "no_speech_prob": 0.0021097890567034483}, {"id": 216, "seek": 150176, "start": 1501.84, "end": 1506.4, "text": " I'm showing you language competency by ingesting text on it.", "tokens": [50368, 286, 478, 4099, 291, 2856, 50097, 538, 3957, 8714, 2487, 322, 309, 13, 50596], "temperature": 0.0, "avg_logprob": -0.2793304180276805, "compression_ratio": 1.3356164383561644, "no_speech_prob": 0.0024244976229965687}, {"id": 217, "seek": 150176, "start": 1509.12, "end": 1511.28, "text": " So this dismissive", "tokens": [50732, 407, 341, 16974, 488, 50840], "temperature": 0.0, "avg_logprob": -0.2793304180276805, "compression_ratio": 1.3356164383561644, "no_speech_prob": 0.0024244976229965687}, {"id": 218, "seek": 150176, "start": 1517.36, "end": 1527.92, "text": " all these are, what is the phrase that Melanie uses? Not Melanie. Stochastic parents. No.", "tokens": [51144, 439, 613, 366, 11, 437, 307, 264, 9535, 300, 42798, 4960, 30, 1726, 42798, 13, 745, 8997, 2750, 3152, 13, 883, 13, 51672], "temperature": 0.0, "avg_logprob": -0.2793304180276805, "compression_ratio": 1.3356164383561644, "no_speech_prob": 0.0024244976229965687}, {"id": 219, "seek": 150176, "start": 1527.92, "end": 1529.44, "text": " Oh, Bender. Emily Bender.", "tokens": [51672, 876, 11, 363, 3216, 13, 15034, 363, 3216, 13, 51748], "temperature": 0.0, "avg_logprob": -0.2793304180276805, "compression_ratio": 1.3356164383561644, "no_speech_prob": 0.0024244976229965687}, {"id": 220, "seek": 152944, "start": 1529.44, "end": 1535.6000000000001, "text": " Emily Bender. No, these are not stochastic parents anymore for me. I am seeing,", "tokens": [50364, 15034, 363, 3216, 13, 883, 11, 613, 366, 406, 342, 8997, 2750, 3152, 3602, 337, 385, 13, 286, 669, 2577, 11, 50672], "temperature": 0.0, "avg_logprob": -0.17336981637137278, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.0003566083323676139}, {"id": 221, "seek": 152944, "start": 1536.16, "end": 1542.72, "text": " look, if I go through the tests on conducting, I have 20 pages of tests on every aspect.", "tokens": [50700, 574, 11, 498, 286, 352, 807, 264, 6921, 322, 21749, 11, 286, 362, 945, 7183, 295, 6921, 322, 633, 4171, 13, 51028], "temperature": 0.0, "avg_logprob": -0.17336981637137278, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.0003566083323676139}, {"id": 222, "seek": 152944, "start": 1545.28, "end": 1548.72, "text": " And they get better. I mean, I am seeing things that", "tokens": [51156, 400, 436, 483, 1101, 13, 286, 914, 11, 286, 669, 2577, 721, 300, 51328], "temperature": 0.0, "avg_logprob": -0.17336981637137278, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.0003566083323676139}, {"id": 223, "seek": 152944, "start": 1550.88, "end": 1553.76, "text": " lexical ambiguity, they've almost resolved it like,", "tokens": [51436, 476, 87, 804, 46519, 11, 436, 600, 1920, 20772, 309, 411, 11, 51580], "temperature": 0.0, "avg_logprob": -0.17336981637137278, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.0003566083323676139}, {"id": 224, "seek": 155376, "start": 1554.64, "end": 1564.16, "text": " we were at the baseball stadium last night, we had a ball. They knew that ball is not the baseball.", "tokens": [50408, 321, 645, 412, 264, 14323, 18585, 1036, 1818, 11, 321, 632, 257, 2594, 13, 814, 2586, 300, 2594, 307, 406, 264, 14323, 13, 50884], "temperature": 0.0, "avg_logprob": -0.11890492568144927, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.0005612460081465542}, {"id": 225, "seek": 155376, "start": 1565.52, "end": 1572.24, "text": " I'm seeing things like, what the hell is this? And if anybody can test these systems,", "tokens": [50952, 286, 478, 2577, 721, 411, 11, 437, 264, 4921, 307, 341, 30, 400, 498, 4472, 393, 1500, 613, 3652, 11, 51288], "temperature": 0.0, "avg_logprob": -0.11890492568144927, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.0005612460081465542}, {"id": 226, "seek": 155376, "start": 1572.24, "end": 1578.32, "text": " I can with all humility. I'm trying my best now to make them fail, which was not the case just", "tokens": [51288, 286, 393, 365, 439, 27106, 13, 286, 478, 1382, 452, 1151, 586, 281, 652, 552, 3061, 11, 597, 390, 406, 264, 1389, 445, 51592], "temperature": 0.0, "avg_logprob": -0.11890492568144927, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.0005612460081465542}, {"id": 227, "seek": 157832, "start": 1578.3999999999999, "end": 1584.48, "text": " a month ago. All I'm saying is I'm seeing something that I never thought I would see", "tokens": [50368, 257, 1618, 2057, 13, 1057, 286, 478, 1566, 307, 286, 478, 2577, 746, 300, 286, 1128, 1194, 286, 576, 536, 50672], "temperature": 0.0, "avg_logprob": -0.13418533007303873, "compression_ratio": 1.4050632911392404, "no_speech_prob": 0.0053822072222828865}, {"id": 228, "seek": 157832, "start": 1585.84, "end": 1588.56, "text": " as a cognitive scientist, as a computation linguist.", "tokens": [50740, 382, 257, 15605, 12662, 11, 382, 257, 24903, 21766, 468, 13, 50876], "temperature": 0.0, "avg_logprob": -0.13418533007303873, "compression_ratio": 1.4050632911392404, "no_speech_prob": 0.0053822072222828865}, {"id": 229, "seek": 157832, "start": 1590.72, "end": 1597.4399999999998, "text": " Let me put it this way. To see this capability now, you have to bring back Montague,", "tokens": [50984, 961, 385, 829, 309, 341, 636, 13, 1407, 536, 341, 13759, 586, 11, 291, 362, 281, 1565, 646, 7947, 4918, 11, 51320], "temperature": 0.0, "avg_logprob": -0.13418533007303873, "compression_ratio": 1.4050632911392404, "no_speech_prob": 0.0053822072222828865}, {"id": 230, "seek": 159744, "start": 1598.24, "end": 1607.8400000000001, "text": " Frigge, Marvin Minsky, John McCarthy, all the pioneers of logic and AI, put them together", "tokens": [50404, 1526, 328, 432, 11, 48722, 376, 44153, 11, 2619, 44085, 11, 439, 264, 47381, 295, 9952, 293, 7318, 11, 829, 552, 1214, 50884], "temperature": 0.0, "avg_logprob": -0.1474998809479095, "compression_ratio": 1.5353982300884956, "no_speech_prob": 0.037173159420490265}, {"id": 231, "seek": 159744, "start": 1608.48, "end": 1613.68, "text": " and give them a thousand bright engineers. And they will not do this.", "tokens": [50916, 293, 976, 552, 257, 4714, 4730, 11955, 13, 400, 436, 486, 406, 360, 341, 13, 51176], "temperature": 0.0, "avg_logprob": -0.1474998809479095, "compression_ratio": 1.5353982300884956, "no_speech_prob": 0.037173159420490265}, {"id": 232, "seek": 159744, "start": 1614.88, "end": 1619.52, "text": " In a minute, we're going to get onto your book review, but you are just alluding to the problem", "tokens": [51236, 682, 257, 3456, 11, 321, 434, 516, 281, 483, 3911, 428, 1446, 3131, 11, 457, 291, 366, 445, 439, 33703, 281, 264, 1154, 51468], "temperature": 0.0, "avg_logprob": -0.1474998809479095, "compression_ratio": 1.5353982300884956, "no_speech_prob": 0.037173159420490265}, {"id": 233, "seek": 159744, "start": 1619.52, "end": 1625.04, "text": " of semantics and pragmatics. And also I want to bring in symbol grounding as being the next", "tokens": [51468, 295, 4361, 45298, 293, 33394, 15677, 1167, 13, 400, 611, 286, 528, 281, 1565, 294, 5986, 46727, 382, 885, 264, 958, 51744], "temperature": 0.0, "avg_logprob": -0.1474998809479095, "compression_ratio": 1.5353982300884956, "no_speech_prob": 0.037173159420490265}, {"id": 234, "seek": 162504, "start": 1625.04, "end": 1628.6399999999999, "text": " potential brick walls. Could you just talk to that a little bit more?", "tokens": [50364, 3995, 16725, 7920, 13, 7497, 291, 445, 751, 281, 300, 257, 707, 857, 544, 30, 50544], "temperature": 0.0, "avg_logprob": -0.15562503017596344, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.0030202525667846203}, {"id": 235, "seek": 162504, "start": 1629.36, "end": 1638.3999999999999, "text": " Well, look, symbol grounding was an issue in symbolic systems. You're using symbolic systems.", "tokens": [50580, 1042, 11, 574, 11, 5986, 46727, 390, 364, 2734, 294, 25755, 3652, 13, 509, 434, 1228, 25755, 3652, 13, 51032], "temperature": 0.0, "avg_logprob": -0.15562503017596344, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.0030202525667846203}, {"id": 236, "seek": 162504, "start": 1638.3999999999999, "end": 1648.48, "text": " So you're saying cat, CAT, it's reference based semantics. So I'm going to use CAT to refer to", "tokens": [51032, 407, 291, 434, 1566, 3857, 11, 41192, 11, 309, 311, 6408, 2361, 4361, 45298, 13, 407, 286, 478, 516, 281, 764, 41192, 281, 2864, 281, 51536], "temperature": 0.0, "avg_logprob": -0.15562503017596344, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.0030202525667846203}, {"id": 237, "seek": 164848, "start": 1648.48, "end": 1656.88, "text": " a concept called CAT. And then the concept called CAT is a frame in most systems, in frame based", "tokens": [50364, 257, 3410, 1219, 41192, 13, 400, 550, 264, 3410, 1219, 41192, 307, 257, 3920, 294, 881, 3652, 11, 294, 3920, 2361, 50784], "temperature": 0.0, "avg_logprob": -0.16181036569539783, "compression_ratio": 1.7477064220183487, "no_speech_prob": 0.06654294580221176}, {"id": 238, "seek": 164848, "start": 1656.88, "end": 1663.44, "text": " systems with properties. It's a mammal. It's a thing that has this and this kind of fair whiskers,", "tokens": [50784, 3652, 365, 7221, 13, 467, 311, 257, 49312, 13, 467, 311, 257, 551, 300, 575, 341, 293, 341, 733, 295, 3143, 24485, 433, 11, 51112], "temperature": 0.0, "avg_logprob": -0.16181036569539783, "compression_ratio": 1.7477064220183487, "no_speech_prob": 0.06654294580221176}, {"id": 239, "seek": 164848, "start": 1663.44, "end": 1670.48, "text": " blah, blah, blah. It's the intention of what a CAT is. And then symbol grounding came like, okay,", "tokens": [51112, 12288, 11, 12288, 11, 12288, 13, 467, 311, 264, 7789, 295, 437, 257, 41192, 307, 13, 400, 550, 5986, 46727, 1361, 411, 11, 1392, 11, 51464], "temperature": 0.0, "avg_logprob": -0.16181036569539783, "compression_ratio": 1.7477064220183487, "no_speech_prob": 0.06654294580221176}, {"id": 240, "seek": 164848, "start": 1670.48, "end": 1677.6, "text": " you're defining CAT as symbol in terms of symbols. Like, so where do we go? It's like a", "tokens": [51464, 291, 434, 17827, 41192, 382, 5986, 294, 2115, 295, 16944, 13, 1743, 11, 370, 689, 360, 321, 352, 30, 467, 311, 411, 257, 51820], "temperature": 0.0, "avg_logprob": -0.16181036569539783, "compression_ratio": 1.7477064220183487, "no_speech_prob": 0.06654294580221176}, {"id": 241, "seek": 167760, "start": 1677.6, "end": 1684.24, "text": " dictionary to read the definition of a word. I have to know all the words. So I might go and", "tokens": [50364, 25890, 281, 1401, 264, 7123, 295, 257, 1349, 13, 286, 362, 281, 458, 439, 264, 2283, 13, 407, 286, 1062, 352, 293, 50696], "temperature": 0.0, "avg_logprob": -0.15368257398190704, "compression_ratio": 1.59375, "no_speech_prob": 0.000362428865628317}, {"id": 242, "seek": 167760, "start": 1684.24, "end": 1690.56, "text": " so it's a cyclical representational system. It's not grounded in anything in the end.", "tokens": [50696, 370, 309, 311, 257, 19474, 804, 2906, 1478, 1185, 13, 467, 311, 406, 23535, 294, 1340, 294, 264, 917, 13, 51012], "temperature": 0.0, "avg_logprob": -0.15368257398190704, "compression_ratio": 1.59375, "no_speech_prob": 0.000362428865628317}, {"id": 243, "seek": 167760, "start": 1690.56, "end": 1695.52, "text": " It's a closed. Basically, it's a system that defines itself, like what the hell's going on here,", "tokens": [51012, 467, 311, 257, 5395, 13, 8537, 11, 309, 311, 257, 1185, 300, 23122, 2564, 11, 411, 437, 264, 4921, 311, 516, 322, 510, 11, 51260], "temperature": 0.0, "avg_logprob": -0.15368257398190704, "compression_ratio": 1.59375, "no_speech_prob": 0.000362428865628317}, {"id": 244, "seek": 167760, "start": 1695.52, "end": 1702.3999999999999, "text": " right? Symbol grounding was CAT has to be associated with something real outside.", "tokens": [51260, 558, 30, 3902, 5612, 46727, 390, 41192, 575, 281, 312, 6615, 365, 746, 957, 2380, 13, 51604], "temperature": 0.0, "avg_logprob": -0.15368257398190704, "compression_ratio": 1.59375, "no_speech_prob": 0.000362428865628317}, {"id": 245, "seek": 170240, "start": 1702.4, "end": 1705.92, "text": " That's a real CAT. In symbolic systems, we don't have that, right?", "tokens": [50364, 663, 311, 257, 957, 41192, 13, 682, 25755, 3652, 11, 321, 500, 380, 362, 300, 11, 558, 30, 50540], "temperature": 0.0, "avg_logprob": -0.14154264201288638, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.0009536081925034523}, {"id": 246, "seek": 170240, "start": 1708.72, "end": 1715.1200000000001, "text": " We can get into symbol grounding. It's a huge subject on its own, like where do meanings,", "tokens": [50680, 492, 393, 483, 666, 5986, 46727, 13, 467, 311, 257, 2603, 3983, 322, 1080, 1065, 11, 411, 689, 360, 28138, 11, 51000], "temperature": 0.0, "avg_logprob": -0.14154264201288638, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.0009536081925034523}, {"id": 247, "seek": 170240, "start": 1716.3200000000002, "end": 1724.16, "text": " where do words get their meaning from? Is it embodied? Is it experiential? Does it have to be", "tokens": [51060, 689, 360, 2283, 483, 641, 3620, 490, 30, 1119, 309, 42046, 30, 1119, 309, 49611, 831, 30, 4402, 309, 362, 281, 312, 51452], "temperature": 0.0, "avg_logprob": -0.14154264201288638, "compression_ratio": 1.4534883720930232, "no_speech_prob": 0.0009536081925034523}, {"id": 248, "seek": 172416, "start": 1725.0400000000002, "end": 1734.48, "text": " can a deaf and a blind person ever understand the meaning of something? So that's a huge...", "tokens": [50408, 393, 257, 15559, 293, 257, 6865, 954, 1562, 1223, 264, 3620, 295, 746, 30, 407, 300, 311, 257, 2603, 485, 50880], "temperature": 0.0, "avg_logprob": -0.20277118682861328, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.017655055969953537}, {"id": 249, "seek": 172416, "start": 1737.52, "end": 1742.64, "text": " I mean, we spoke to Andrew Lampinen and he was getting into Pierce's triad semiotics,", "tokens": [51032, 286, 914, 11, 321, 7179, 281, 10110, 441, 1215, 5636, 293, 415, 390, 1242, 666, 45432, 311, 1376, 345, 12909, 42131, 11, 51288], "temperature": 0.0, "avg_logprob": -0.20277118682861328, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.017655055969953537}, {"id": 250, "seek": 172416, "start": 1742.64, "end": 1748.24, "text": " this embodied relativistic notion of grounding, which actually I'm developing a bit of a taste", "tokens": [51288, 341, 42046, 21960, 3142, 10710, 295, 46727, 11, 597, 767, 286, 478, 6416, 257, 857, 295, 257, 3939, 51568], "temperature": 0.0, "avg_logprob": -0.20277118682861328, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.017655055969953537}, {"id": 251, "seek": 172416, "start": 1748.24, "end": 1752.64, "text": " for personally, but you're very skeptical about that. Could you just sketch that out?", "tokens": [51568, 337, 5665, 11, 457, 291, 434, 588, 28601, 466, 300, 13, 7497, 291, 445, 12325, 300, 484, 30, 51788], "temperature": 0.0, "avg_logprob": -0.20277118682861328, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.017655055969953537}, {"id": 252, "seek": 175264, "start": 1752.8000000000002, "end": 1758.0800000000002, "text": " I don't think that's the issue grounding. I mean, people make a lot of it and like our", "tokens": [50372, 286, 500, 380, 519, 300, 311, 264, 2734, 46727, 13, 286, 914, 11, 561, 652, 257, 688, 295, 309, 293, 411, 527, 50636], "temperature": 0.0, "avg_logprob": -0.12455246004007631, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.0004651541239582002}, {"id": 253, "seek": 175264, "start": 1758.0800000000002, "end": 1764.88, "text": " common friend, Bishop, Mark Bishop, that you will never understand the meaning of something if you", "tokens": [50636, 2689, 1277, 11, 30113, 11, 3934, 30113, 11, 300, 291, 486, 1128, 1223, 264, 3620, 295, 746, 498, 291, 50976], "temperature": 0.0, "avg_logprob": -0.12455246004007631, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.0004651541239582002}, {"id": 254, "seek": 175264, "start": 1764.88, "end": 1770.96, "text": " don't live in the environment and it's... That has never been... I don't believe so. That's why we", "tokens": [50976, 500, 380, 1621, 294, 264, 2823, 293, 309, 311, 485, 663, 575, 1128, 668, 485, 286, 500, 380, 1697, 370, 13, 663, 311, 983, 321, 51280], "temperature": 0.0, "avg_logprob": -0.12455246004007631, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.0004651541239582002}, {"id": 255, "seek": 175264, "start": 1770.96, "end": 1775.6000000000001, "text": " call it artificial intelligence, right? I mean, we're never going to have the intelligence of a", "tokens": [51280, 818, 309, 11677, 7599, 11, 558, 30, 286, 914, 11, 321, 434, 1128, 516, 281, 362, 264, 7599, 295, 257, 51512], "temperature": 0.0, "avg_logprob": -0.12455246004007631, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.0004651541239582002}, {"id": 256, "seek": 175264, "start": 1775.6000000000001, "end": 1782.0, "text": " human being. We're never going to have a robot that really chokes when they see their nephew", "tokens": [51512, 1952, 885, 13, 492, 434, 1128, 516, 281, 362, 257, 7881, 300, 534, 417, 8606, 562, 436, 536, 641, 30799, 51832], "temperature": 0.0, "avg_logprob": -0.12455246004007631, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.0004651541239582002}, {"id": 257, "seek": 178200, "start": 1782.0, "end": 1787.04, "text": " after six years, right? I mean... And that's... That was never the one. That's why we're building", "tokens": [50364, 934, 2309, 924, 11, 558, 30, 286, 914, 485, 400, 300, 311, 485, 663, 390, 1128, 264, 472, 13, 663, 311, 983, 321, 434, 2390, 50616], "temperature": 0.0, "avg_logprob": -0.12468331076882103, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.0005790902068838477}, {"id": 258, "seek": 178200, "start": 1787.04, "end": 1793.52, "text": " artificial intelligence, not human intelligence. So this whole argument about grounding and", "tokens": [50616, 11677, 7599, 11, 406, 1952, 7599, 13, 407, 341, 1379, 6770, 466, 46727, 293, 50940], "temperature": 0.0, "avg_logprob": -0.12468331076882103, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.0005790902068838477}, {"id": 259, "seek": 178200, "start": 1793.52, "end": 1799.44, "text": " embodiment and I will never understand what pain is because a robot will never really feel pain.", "tokens": [50940, 28935, 2328, 293, 286, 486, 1128, 1223, 437, 1822, 307, 570, 257, 7881, 486, 1128, 534, 841, 1822, 13, 51236], "temperature": 0.0, "avg_logprob": -0.12468331076882103, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.0005790902068838477}, {"id": 260, "seek": 178200, "start": 1799.44, "end": 1804.32, "text": " That to me, that's besides the point. I'm not building artificial life. I'm building an", "tokens": [51236, 663, 281, 385, 11, 300, 311, 11868, 264, 935, 13, 286, 478, 406, 2390, 11677, 993, 13, 286, 478, 2390, 364, 51480], "temperature": 0.0, "avg_logprob": -0.12468331076882103, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.0005790902068838477}, {"id": 261, "seek": 178200, "start": 1804.32, "end": 1810.16, "text": " artificially intelligent machine that will do things in a way that you would say, what the hell", "tokens": [51480, 39905, 2270, 13232, 3479, 300, 486, 360, 721, 294, 257, 636, 300, 291, 576, 584, 11, 437, 264, 4921, 51772], "temperature": 0.0, "avg_logprob": -0.12468331076882103, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.0005790902068838477}, {"id": 262, "seek": 181016, "start": 1810.16, "end": 1817.2, "text": " was that? Probably that's how AR should be defined. That's it. What the... Who did this, right? That's it.", "tokens": [50364, 390, 300, 30, 9210, 300, 311, 577, 8943, 820, 312, 7642, 13, 663, 311, 309, 13, 708, 264, 485, 2102, 630, 341, 11, 558, 30, 663, 311, 309, 13, 50716], "temperature": 0.0, "avg_logprob": -0.15750871896743773, "compression_ratio": 1.5, "no_speech_prob": 0.0005440820241346955}, {"id": 263, "seek": 181016, "start": 1817.8400000000001, "end": 1824.0800000000002, "text": " It feels pain or it doesn't feel pain or it will never know what crying is, like so.", "tokens": [50748, 467, 3417, 1822, 420, 309, 1177, 380, 841, 1822, 420, 309, 486, 1128, 458, 437, 8554, 307, 11, 411, 370, 13, 51060], "temperature": 0.0, "avg_logprob": -0.15750871896743773, "compression_ratio": 1.5, "no_speech_prob": 0.0005440820241346955}, {"id": 264, "seek": 181016, "start": 1826.3200000000002, "end": 1835.76, "text": " So at least I come from this angle. I'm not into building artificial humans. I'm an engineer.", "tokens": [51172, 407, 412, 1935, 286, 808, 490, 341, 5802, 13, 286, 478, 406, 666, 2390, 11677, 6255, 13, 286, 478, 364, 11403, 13, 51644], "temperature": 0.0, "avg_logprob": -0.15750871896743773, "compression_ratio": 1.5, "no_speech_prob": 0.0005440820241346955}, {"id": 265, "seek": 183576, "start": 1835.84, "end": 1842.96, "text": " I'm into building artificial intelligence systems. Systems that can reason, right? In the environment", "tokens": [50368, 286, 478, 666, 2390, 11677, 7599, 3652, 13, 27059, 300, 393, 1778, 11, 558, 30, 682, 264, 2823, 50724], "temperature": 0.0, "avg_logprob": -0.11510644933228852, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.0022861328907310963}, {"id": 266, "seek": 183576, "start": 1842.96, "end": 1849.04, "text": " we live in, solve problems intelligently and problems that usually require human intelligence.", "tokens": [50724, 321, 1621, 294, 11, 5039, 2740, 5613, 2276, 293, 2740, 300, 2673, 3651, 1952, 7599, 13, 51028], "temperature": 0.0, "avg_logprob": -0.11510644933228852, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.0022861328907310963}, {"id": 267, "seek": 183576, "start": 1849.04, "end": 1854.72, "text": " Like I'm into... I would like to see a world where we don't have accountants. Come on. We don't have", "tokens": [51028, 1743, 286, 478, 666, 485, 286, 576, 411, 281, 536, 257, 1002, 689, 321, 500, 380, 362, 2696, 1719, 13, 2492, 322, 13, 492, 500, 380, 362, 51312], "temperature": 0.0, "avg_logprob": -0.11510644933228852, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.0022861328907310963}, {"id": 268, "seek": 183576, "start": 1854.72, "end": 1862.56, "text": " doctors. I open my mobile. I have a doctor. I converse with them intelligently and they tell", "tokens": [51312, 8778, 13, 286, 1269, 452, 6013, 13, 286, 362, 257, 4631, 13, 286, 416, 4308, 365, 552, 5613, 2276, 293, 436, 980, 51704], "temperature": 0.0, "avg_logprob": -0.11510644933228852, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.0022861328907310963}, {"id": 269, "seek": 186256, "start": 1862.56, "end": 1867.9199999999998, "text": " me exactly what to do. Done. Nobody goes to medical school anymore. Everybody should write", "tokens": [50364, 385, 2293, 437, 281, 360, 13, 18658, 13, 9297, 1709, 281, 4625, 1395, 3602, 13, 7646, 820, 2464, 50632], "temperature": 0.0, "avg_logprob": -0.09868904601695926, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.0009843427687883377}, {"id": 270, "seek": 186256, "start": 1867.9199999999998, "end": 1874.3999999999999, "text": " poetry and play music and enjoy the beach. That's it. That's the AI I'm interested in. We will never", "tokens": [50632, 15155, 293, 862, 1318, 293, 2103, 264, 7534, 13, 663, 311, 309, 13, 663, 311, 264, 7318, 286, 478, 3102, 294, 13, 492, 486, 1128, 50956], "temperature": 0.0, "avg_logprob": -0.09868904601695926, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.0009843427687883377}, {"id": 271, "seek": 186256, "start": 1874.3999999999999, "end": 1883.6799999999998, "text": " build robots that will understand love. So to me, these are arguments that", "tokens": [50956, 1322, 14733, 300, 486, 1223, 959, 13, 407, 281, 385, 11, 613, 366, 12869, 300, 51420], "temperature": 0.0, "avg_logprob": -0.09868904601695926, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.0009843427687883377}, {"id": 272, "seek": 186256, "start": 1885.52, "end": 1890.1599999999999, "text": " they're irrelevant. We're building artificial intelligence. When we did calculators, we never", "tokens": [51512, 436, 434, 28682, 13, 492, 434, 2390, 11677, 7599, 13, 1133, 321, 630, 4322, 3391, 11, 321, 1128, 51744], "temperature": 0.0, "avg_logprob": -0.09868904601695926, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.0009843427687883377}, {"id": 273, "seek": 189016, "start": 1890.16, "end": 1897.0400000000002, "text": " gave a damn how we do it in the mind. And we have calculators that can beat any mathematician", "tokens": [50364, 2729, 257, 8151, 577, 321, 360, 309, 294, 264, 1575, 13, 400, 321, 362, 4322, 3391, 300, 393, 4224, 604, 48281, 50708], "temperature": 0.0, "avg_logprob": -0.14518032471338907, "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.0035360443871468306}, {"id": 274, "seek": 189016, "start": 1897.0400000000002, "end": 1905.44, "text": " in doing a division of two prime numbers, each of which is 20 digits. Yeah, I think it's more", "tokens": [50708, 294, 884, 257, 10044, 295, 732, 5835, 3547, 11, 1184, 295, 597, 307, 945, 27011, 13, 865, 11, 286, 519, 309, 311, 544, 51128], "temperature": 0.0, "avg_logprob": -0.14518032471338907, "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.0035360443871468306}, {"id": 275, "seek": 189016, "start": 1905.44, "end": 1911.52, "text": " where your area of interest is. I mean, so yours is in the engineering. And what's coming to mind", "tokens": [51128, 689, 428, 1859, 295, 1179, 307, 13, 286, 914, 11, 370, 6342, 307, 294, 264, 7043, 13, 400, 437, 311, 1348, 281, 1575, 51432], "temperature": 0.0, "avg_logprob": -0.14518032471338907, "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.0035360443871468306}, {"id": 276, "seek": 189016, "start": 1911.52, "end": 1918.64, "text": " right now is our conversation with Professor Chomsky where he said, yeah, these are great", "tokens": [51432, 558, 586, 307, 527, 3761, 365, 8419, 761, 4785, 4133, 689, 415, 848, 11, 1338, 11, 613, 366, 869, 51788], "temperature": 0.0, "avg_logprob": -0.14518032471338907, "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.0035360443871468306}, {"id": 277, "seek": 191864, "start": 1918.64, "end": 1923.3600000000001, "text": " feats of engineering. I mean, I like bulldozers too. They just don't have anything to do with", "tokens": [50364, 579, 1720, 295, 7043, 13, 286, 914, 11, 286, 411, 4693, 2595, 41698, 886, 13, 814, 445, 500, 380, 362, 1340, 281, 360, 365, 50600], "temperature": 0.0, "avg_logprob": -0.13632541341879933, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.04203399270772934}, {"id": 278, "seek": 191864, "start": 1923.3600000000001, "end": 1930.72, "text": " science. Sure, or philosophy for that matter. I mean, I think some of these questions,", "tokens": [50600, 3497, 13, 4894, 11, 420, 10675, 337, 300, 1871, 13, 286, 914, 11, 286, 519, 512, 295, 613, 1651, 11, 50968], "temperature": 0.0, "avg_logprob": -0.13632541341879933, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.04203399270772934}, {"id": 279, "seek": 191864, "start": 1931.5200000000002, "end": 1936.0800000000002, "text": " yeah, maybe they don't have a lot to do with building AIs that do a bunch of useful things.", "tokens": [51008, 1338, 11, 1310, 436, 500, 380, 362, 257, 688, 281, 360, 365, 2390, 316, 6802, 300, 360, 257, 3840, 295, 4420, 721, 13, 51236], "temperature": 0.0, "avg_logprob": -0.13632541341879933, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.04203399270772934}, {"id": 280, "seek": 191864, "start": 1937.2, "end": 1944.0800000000002, "text": " But they have a lot to do with philosophy or science or whatever. Mathematics for that matter.", "tokens": [51292, 583, 436, 362, 257, 688, 281, 360, 365, 10675, 420, 3497, 420, 2035, 13, 15776, 37541, 337, 300, 1871, 13, 51636], "temperature": 0.0, "avg_logprob": -0.13632541341879933, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.04203399270772934}, {"id": 281, "seek": 194408, "start": 1944.08, "end": 1952.6399999999999, "text": " This is a bridge to the book because the authors were misunderstood from their title,", "tokens": [50364, 639, 307, 257, 7283, 281, 264, 1446, 570, 264, 16552, 645, 33870, 490, 641, 4876, 11, 50792], "temperature": 0.0, "avg_logprob": -0.11952321869986397, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0032711157109588385}, {"id": 282, "seek": 194408, "start": 1952.6399999999999, "end": 1958.1599999999999, "text": " and I told them that privately. No, not privately. I want to tell them that because", "tokens": [50792, 293, 286, 1907, 552, 300, 31919, 13, 883, 11, 406, 31919, 13, 286, 528, 281, 980, 552, 300, 570, 51068], "temperature": 0.0, "avg_logprob": -0.11952321869986397, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0032711157109588385}, {"id": 283, "seek": 194408, "start": 1959.36, "end": 1965.12, "text": " I got comments from people privately that the title is misleading. The title assumes they are", "tokens": [51128, 286, 658, 3053, 490, 561, 31919, 300, 264, 4876, 307, 36429, 13, 440, 4876, 37808, 436, 366, 51416], "temperature": 0.0, "avg_logprob": -0.11952321869986397, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0032711157109588385}, {"id": 284, "seek": 194408, "start": 1965.12, "end": 1970.8799999999999, "text": " anti AI, not really. They're saying roughly what I'm saying. I'm not interested in building an", "tokens": [51416, 6061, 7318, 11, 406, 534, 13, 814, 434, 1566, 9810, 437, 286, 478, 1566, 13, 286, 478, 406, 3102, 294, 2390, 364, 51704], "temperature": 0.0, "avg_logprob": -0.11952321869986397, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0032711157109588385}, {"id": 285, "seek": 197088, "start": 1970.88, "end": 1977.8400000000001, "text": " artificial human. We can never do that problem. Right. So all this, and this is important because", "tokens": [50364, 11677, 1952, 13, 492, 393, 1128, 360, 300, 1154, 13, 1779, 13, 407, 439, 341, 11, 293, 341, 307, 1021, 570, 50712], "temperature": 0.0, "avg_logprob": -0.12595731439724775, "compression_ratio": 1.4619289340101522, "no_speech_prob": 0.0018096297280862927}, {"id": 286, "seek": 197088, "start": 1977.8400000000001, "end": 1986.48, "text": " people are trivializing. I mean, you have people talking about AGI from five years ago. And all", "tokens": [50712, 561, 366, 26703, 3319, 13, 286, 914, 11, 291, 362, 561, 1417, 466, 316, 26252, 490, 1732, 924, 2057, 13, 400, 439, 51144], "temperature": 0.0, "avg_logprob": -0.12595731439724775, "compression_ratio": 1.4619289340101522, "no_speech_prob": 0.0018096297280862927}, {"id": 287, "seek": 197088, "start": 1986.48, "end": 1992.88, "text": " we had was something that can do amazing pattern recognition. That's it. So it's important for", "tokens": [51144, 321, 632, 390, 746, 300, 393, 360, 2243, 5102, 11150, 13, 663, 311, 309, 13, 407, 309, 311, 1021, 337, 51464], "temperature": 0.0, "avg_logprob": -0.12595731439724775, "compression_ratio": 1.4619289340101522, "no_speech_prob": 0.0018096297280862927}, {"id": 288, "seek": 199288, "start": 1992.88, "end": 2000.48, "text": " us to say, Hey guys, cool it down. Do you know what you mean when you talk about machines that", "tokens": [50364, 505, 281, 584, 11, 1911, 1074, 11, 1627, 309, 760, 13, 1144, 291, 458, 437, 291, 914, 562, 291, 751, 466, 8379, 300, 50744], "temperature": 0.0, "avg_logprob": -0.1649561443844357, "compression_ratio": 1.4215686274509804, "no_speech_prob": 0.008844963274896145}, {"id": 289, "seek": 199288, "start": 2001.5200000000002, "end": 2009.2, "text": " surpass human intelligence? This is not just a word you throw out, because you're impressed with", "tokens": [50796, 27650, 1952, 7599, 30, 639, 307, 406, 445, 257, 1349, 291, 3507, 484, 11, 570, 291, 434, 11679, 365, 51180], "temperature": 0.0, "avg_logprob": -0.1649561443844357, "compression_ratio": 1.4215686274509804, "no_speech_prob": 0.008844963274896145}, {"id": 290, "seek": 199288, "start": 2010.24, "end": 2016.96, "text": " a system that can recognize cats from dogs. Come on, take it easy. Slow down, right? And this book", "tokens": [51232, 257, 1185, 300, 393, 5521, 11111, 490, 7197, 13, 2492, 322, 11, 747, 309, 1858, 13, 17703, 760, 11, 558, 30, 400, 341, 1446, 51568], "temperature": 0.0, "avg_logprob": -0.1649561443844357, "compression_ratio": 1.4215686274509804, "no_speech_prob": 0.008844963274896145}, {"id": 291, "seek": 201696, "start": 2016.96, "end": 2023.44, "text": " is about that. It's like, do you know what it means to have a system that can feel and", "tokens": [50364, 307, 466, 300, 13, 467, 311, 411, 11, 360, 291, 458, 437, 309, 1355, 281, 362, 257, 1185, 300, 393, 841, 293, 50688], "temperature": 0.0, "avg_logprob": -0.1309096200125558, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.0016227365704253316}, {"id": 292, "seek": 201696, "start": 2026.16, "end": 2032.72, "text": " react instantly real time to changing situations around them? And do you know what you're talking", "tokens": [50824, 4515, 13518, 957, 565, 281, 4473, 6851, 926, 552, 30, 400, 360, 291, 458, 437, 291, 434, 1417, 51152], "temperature": 0.0, "avg_logprob": -0.1309096200125558, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.0016227365704253316}, {"id": 293, "seek": 201696, "start": 2032.72, "end": 2040.32, "text": " about? So yeah, I'm interested in the engineering side of AI. And that's what makes me impressed", "tokens": [51152, 466, 30, 407, 1338, 11, 286, 478, 3102, 294, 264, 7043, 1252, 295, 7318, 13, 400, 300, 311, 437, 1669, 385, 11679, 51532], "temperature": 0.0, "avg_logprob": -0.1309096200125558, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.0016227365704253316}, {"id": 294, "seek": 204032, "start": 2040.8799999999999, "end": 2053.04, "text": " by something like a DaVinci 2 or DaVinci 3 as an AI enthusiast. I look at this and I say,", "tokens": [50392, 538, 746, 411, 257, 3933, 53, 21961, 568, 420, 3933, 53, 21961, 805, 382, 364, 7318, 18076, 525, 13, 286, 574, 412, 341, 293, 286, 584, 11, 51000], "temperature": 0.0, "avg_logprob": -0.19109025754426656, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.006688286084681749}, {"id": 295, "seek": 204032, "start": 2053.04, "end": 2059.68, "text": " wow, we've never been able to reach this milestone. This is a huge milestone. That's how I look at", "tokens": [51000, 6076, 11, 321, 600, 1128, 668, 1075, 281, 2524, 341, 28048, 13, 639, 307, 257, 2603, 28048, 13, 663, 311, 577, 286, 574, 412, 51332], "temperature": 0.0, "avg_logprob": -0.19109025754426656, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.006688286084681749}, {"id": 296, "seek": 204032, "start": 2061.2799999999997, "end": 2066.0, "text": " will it be, will it be the solution for the language understanding problem? No,", "tokens": [51412, 486, 309, 312, 11, 486, 309, 312, 264, 3827, 337, 264, 2856, 3701, 1154, 30, 883, 11, 51648], "temperature": 0.0, "avg_logprob": -0.19109025754426656, "compression_ratio": 1.488888888888889, "no_speech_prob": 0.006688286084681749}, {"id": 297, "seek": 206600, "start": 2066.96, "end": 2072.0, "text": " because language understanding in the full sense of the word understanding,", "tokens": [50412, 570, 2856, 3701, 294, 264, 1577, 2020, 295, 264, 1349, 3701, 11, 50664], "temperature": 0.0, "avg_logprob": -0.12457159793738162, "compression_ratio": 1.65625, "no_speech_prob": 0.0027538605500012636}, {"id": 298, "seek": 206600, "start": 2073.04, "end": 2078.08, "text": " the way we speak now, the way we were speaking now involves a lot more than mastering syntax,", "tokens": [50716, 264, 636, 321, 1710, 586, 11, 264, 636, 321, 645, 4124, 586, 11626, 257, 688, 544, 813, 49382, 28431, 11, 50968], "temperature": 0.0, "avg_logprob": -0.12457159793738162, "compression_ratio": 1.65625, "no_speech_prob": 0.0027538605500012636}, {"id": 299, "seek": 206600, "start": 2078.08, "end": 2089.36, "text": " but they did master a big aspect of language. And I can see it. I can try it. And I'm trying to", "tokens": [50968, 457, 436, 630, 4505, 257, 955, 4171, 295, 2856, 13, 400, 286, 393, 536, 309, 13, 286, 393, 853, 309, 13, 400, 286, 478, 1382, 281, 51532], "temperature": 0.0, "avg_logprob": -0.12457159793738162, "compression_ratio": 1.65625, "no_speech_prob": 0.0027538605500012636}, {"id": 300, "seek": 208936, "start": 2089.36, "end": 2097.36, "text": " make it fail now in syntax and even some coherence, some mild, let's call them mild semantics.", "tokens": [50364, 652, 309, 3061, 586, 294, 28431, 293, 754, 512, 26528, 655, 11, 512, 15154, 11, 718, 311, 818, 552, 15154, 4361, 45298, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1369620926526128, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.0013428641250357032}, {"id": 301, "seek": 208936, "start": 2099.6, "end": 2110.56, "text": " And it's very impressive. So the question now becomes for AI researchers, not just engineers, is", "tokens": [50876, 400, 309, 311, 588, 8992, 13, 407, 264, 1168, 586, 3643, 337, 7318, 10309, 11, 406, 445, 11955, 11, 307, 51424], "temperature": 0.0, "avg_logprob": -0.1369620926526128, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.0013428641250357032}, {"id": 302, "seek": 211056, "start": 2110.88, "end": 2122.0, "text": " this scalability scalable? Is this scalable? Is this approach scalable?", "tokens": [50380, 341, 15664, 2310, 38481, 30, 1119, 341, 38481, 30, 1119, 341, 3109, 38481, 30, 50936], "temperature": 0.0, "avg_logprob": -0.253582763671875, "compression_ratio": 1.4424778761061947, "no_speech_prob": 0.011149698868393898}, {"id": 303, "seek": 211056, "start": 2125.12, "end": 2132.88, "text": " So much data and so much compute power, they mastered syntax more or less. I think they did", "tokens": [51092, 407, 709, 1412, 293, 370, 709, 14722, 1347, 11, 436, 38686, 28431, 544, 420, 1570, 13, 286, 519, 436, 630, 51480], "temperature": 0.0, "avg_logprob": -0.253582763671875, "compression_ratio": 1.4424778761061947, "no_speech_prob": 0.011149698868393898}, {"id": 304, "seek": 213288, "start": 2132.88, "end": 2140.32, "text": " at least as much as a competent language user. So my first question is like,", "tokens": [50364, 412, 1935, 382, 709, 382, 257, 29998, 2856, 4195, 13, 407, 452, 700, 1168, 307, 411, 11, 50736], "temperature": 0.0, "avg_logprob": -0.1588590580929992, "compression_ratio": 1.5816733067729083, "no_speech_prob": 0.003535785712301731}, {"id": 305, "seek": 213288, "start": 2140.32, "end": 2145.12, "text": " do you see natural language as essentially computable as in like cheering machine computer?", "tokens": [50736, 360, 291, 536, 3303, 2856, 382, 4476, 2807, 712, 382, 294, 411, 11060, 3479, 3820, 30, 50976], "temperature": 0.0, "avg_logprob": -0.1588590580929992, "compression_ratio": 1.5816733067729083, "no_speech_prob": 0.003535785712301731}, {"id": 306, "seek": 213288, "start": 2145.12, "end": 2148.7200000000003, "text": " Or do we need some other kind of new mathematics to describe it? For example,", "tokens": [50976, 1610, 360, 321, 643, 512, 661, 733, 295, 777, 18666, 281, 6786, 309, 30, 1171, 1365, 11, 51156], "temperature": 0.0, "avg_logprob": -0.1588590580929992, "compression_ratio": 1.5816733067729083, "no_speech_prob": 0.003535785712301731}, {"id": 307, "seek": 213288, "start": 2148.7200000000003, "end": 2153.04, "text": " hypercoputation, whatever that might be. In other words, is there a generative grammar or", "tokens": [51156, 9848, 13084, 11380, 11, 2035, 300, 1062, 312, 13, 682, 661, 2283, 11, 307, 456, 257, 1337, 1166, 22317, 420, 51372], "temperature": 0.0, "avg_logprob": -0.1588590580929992, "compression_ratio": 1.5816733067729083, "no_speech_prob": 0.003535785712301731}, {"id": 308, "seek": 213288, "start": 2153.04, "end": 2156.2400000000002, "text": " algorithm or set of rules that generate our valid sentences?", "tokens": [51372, 9284, 420, 992, 295, 4474, 300, 8460, 527, 7363, 16579, 30, 51532], "temperature": 0.0, "avg_logprob": -0.1588590580929992, "compression_ratio": 1.5816733067729083, "no_speech_prob": 0.003535785712301731}, {"id": 309, "seek": 215624, "start": 2156.24, "end": 2163.6, "text": " When it comes to language itself, I think language is a formal language.", "tokens": [50364, 1133, 309, 1487, 281, 2856, 2564, 11, 286, 519, 2856, 307, 257, 9860, 2856, 13, 50732], "temperature": 0.0, "avg_logprob": -0.20849750155494326, "compression_ratio": 1.5, "no_speech_prob": 0.002433421788737178}, {"id": 310, "seek": 215624, "start": 2164.72, "end": 2173.52, "text": " There is a compiler for natural language that can be built, like we have built one for Java,", "tokens": [50788, 821, 307, 257, 31958, 337, 3303, 2856, 300, 393, 312, 3094, 11, 411, 321, 362, 3094, 472, 337, 10745, 11, 51228], "temperature": 0.0, "avg_logprob": -0.20849750155494326, "compression_ratio": 1.5, "no_speech_prob": 0.002433421788737178}, {"id": 311, "seek": 215624, "start": 2173.52, "end": 2181.68, "text": " C sharp. So this is Montague, yeah. Yeah, I believe Montague was right,", "tokens": [51228, 383, 8199, 13, 407, 341, 307, 7947, 4918, 11, 1338, 13, 865, 11, 286, 1697, 7947, 4918, 390, 558, 11, 51636], "temperature": 0.0, "avg_logprob": -0.20849750155494326, "compression_ratio": 1.5, "no_speech_prob": 0.002433421788737178}, {"id": 312, "seek": 218168, "start": 2181.68, "end": 2188.7999999999997, "text": " although Montague was was attacking the semantics part. Okay, he touched a little bit on intention", "tokens": [50364, 4878, 7947, 4918, 390, 390, 15010, 264, 4361, 45298, 644, 13, 1033, 11, 415, 9828, 257, 707, 857, 322, 7789, 50720], "temperature": 0.0, "avg_logprob": -0.13035741786366886, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.005726377479732037}, {"id": 313, "seek": 218168, "start": 2188.7999999999997, "end": 2196.96, "text": " and then not much on pragmatics, but Montague and it took me years. And I had to be advised by a very", "tokens": [50720, 293, 550, 406, 709, 322, 33394, 15677, 1167, 11, 457, 7947, 4918, 293, 309, 1890, 385, 924, 13, 400, 286, 632, 281, 312, 26269, 538, 257, 588, 51128], "temperature": 0.0, "avg_logprob": -0.13035741786366886, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.005726377479732037}, {"id": 314, "seek": 218168, "start": 2196.96, "end": 2202.96, "text": " smart philosopher, logician, that stop saying Montague didn't deal with this. Montague was never", "tokens": [51128, 4069, 29805, 11, 3565, 9027, 11, 300, 1590, 1566, 7947, 4918, 994, 380, 2028, 365, 341, 13, 7947, 4918, 390, 1128, 51428], "temperature": 0.0, "avg_logprob": -0.13035741786366886, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.005726377479732037}, {"id": 315, "seek": 218168, "start": 2202.96, "end": 2208.7999999999997, "text": " in the business of reference resolution. That's pragmatics. Montague was trying to prove", "tokens": [51428, 294, 264, 1606, 295, 6408, 8669, 13, 663, 311, 33394, 15677, 1167, 13, 7947, 4918, 390, 1382, 281, 7081, 51720], "temperature": 0.0, "avg_logprob": -0.13035741786366886, "compression_ratio": 1.628691983122363, "no_speech_prob": 0.005726377479732037}, {"id": 316, "seek": 220880, "start": 2209.76, "end": 2215.92, "text": " there is a formal system and algebraic system. And he used lambda calculus, strongly typed system", "tokens": [50412, 456, 307, 257, 9860, 1185, 293, 21989, 299, 1185, 13, 400, 415, 1143, 13607, 33400, 11, 10613, 33941, 1185, 50720], "temperature": 0.0, "avg_logprob": -0.1502421498298645, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0027112066745758057}, {"id": 317, "seek": 220880, "start": 2216.48, "end": 2223.52, "text": " that I can use to compose language like I do with arithmetic or calculus or anything. There's", "tokens": [50748, 300, 286, 393, 764, 281, 35925, 2856, 411, 286, 360, 365, 42973, 420, 33400, 420, 1340, 13, 821, 311, 51100], "temperature": 0.0, "avg_logprob": -0.1502421498298645, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0027112066745758057}, {"id": 318, "seek": 220880, "start": 2223.52, "end": 2229.92, "text": " a logic that are mathematics for language, which is a huge thing. Montague was not a trivial", "tokens": [51100, 257, 9952, 300, 366, 18666, 337, 2856, 11, 597, 307, 257, 2603, 551, 13, 7947, 4918, 390, 406, 257, 26703, 51420], "temperature": 0.0, "avg_logprob": -0.1502421498298645, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0027112066745758057}, {"id": 319, "seek": 220880, "start": 2231.52, "end": 2234.1600000000003, "text": " semanticist in the history of language. He was huge.", "tokens": [51500, 47982, 468, 294, 264, 2503, 295, 2856, 13, 634, 390, 2603, 13, 51632], "temperature": 0.0, "avg_logprob": -0.1502421498298645, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0027112066745758057}, {"id": 320, "seek": 223416, "start": 2234.8799999999997, "end": 2239.68, "text": " So would you say that Montague is doing for language semantics of language,", "tokens": [50400, 407, 576, 291, 584, 300, 7947, 4918, 307, 884, 337, 2856, 4361, 45298, 295, 2856, 11, 50640], "temperature": 0.0, "avg_logprob": -0.25098003659929546, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.00537171820178628}, {"id": 321, "seek": 223416, "start": 2239.68, "end": 2245.7599999999998, "text": " what chance he did for syntax of language? Exactly, exactly. And the common denominator", "tokens": [50640, 437, 2931, 415, 630, 337, 28431, 295, 2856, 30, 7587, 11, 2293, 13, 400, 264, 2689, 20687, 50944], "temperature": 0.0, "avg_logprob": -0.25098003659929546, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.00537171820178628}, {"id": 322, "seek": 223416, "start": 2245.7599999999998, "end": 2250.72, "text": " interesting between them is someone that Chomsky himself admires a lot Barbara Partee,", "tokens": [50944, 1880, 1296, 552, 307, 1580, 300, 761, 4785, 4133, 3647, 614, 3057, 495, 257, 688, 19214, 4100, 1653, 11, 51192], "temperature": 0.0, "avg_logprob": -0.25098003659929546, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.00537171820178628}, {"id": 323, "seek": 223416, "start": 2251.44, "end": 2257.3599999999997, "text": " who was he did her PhD with Montague. She's a Montegoian, Montague semantics.", "tokens": [51228, 567, 390, 415, 630, 720, 14476, 365, 7947, 4918, 13, 1240, 311, 257, 7947, 6308, 952, 11, 7947, 4918, 4361, 45298, 13, 51524], "temperature": 0.0, "avg_logprob": -0.25098003659929546, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.00537171820178628}, {"id": 324, "seek": 225736, "start": 2258.1600000000003, "end": 2264.2400000000002, "text": " But she and she, she did say almost the same phrase. He said what Montague did for semantics", "tokens": [50404, 583, 750, 293, 750, 11, 750, 630, 584, 1920, 264, 912, 9535, 13, 634, 848, 437, 7947, 4918, 630, 337, 4361, 45298, 50708], "temperature": 0.0, "avg_logprob": -0.2126906359637225, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.017707539722323418}, {"id": 325, "seek": 225736, "start": 2264.2400000000002, "end": 2269.76, "text": " is equivalent to what Chomsky did for syntax. Yes. Okay, exact, almost exact phrase.", "tokens": [50708, 307, 10344, 281, 437, 761, 4785, 4133, 630, 337, 28431, 13, 1079, 13, 1033, 11, 1900, 11, 1920, 1900, 9535, 13, 50984], "temperature": 0.0, "avg_logprob": -0.2126906359637225, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.017707539722323418}, {"id": 326, "seek": 225736, "start": 2271.52, "end": 2276.7200000000003, "text": " You think he was right about semantics? Yes. So for example, there's a computable definition", "tokens": [51072, 509, 519, 415, 390, 558, 466, 4361, 45298, 30, 1079, 13, 407, 337, 1365, 11, 456, 311, 257, 2807, 712, 7123, 51332], "temperature": 0.0, "avg_logprob": -0.2126906359637225, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.017707539722323418}, {"id": 327, "seek": 225736, "start": 2276.7200000000003, "end": 2282.48, "text": " of what is a pile of sand. Right. No, no, no, no, no, no, no. I'm not sure you're right about that.", "tokens": [51332, 295, 437, 307, 257, 14375, 295, 4932, 13, 1779, 13, 883, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 13, 286, 478, 406, 988, 291, 434, 558, 466, 300, 13, 51620], "temperature": 0.0, "avg_logprob": -0.2126906359637225, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.017707539722323418}, {"id": 328, "seek": 228248, "start": 2282.48, "end": 2288.64, "text": " No, no, no, no. Hold on. Let's not get Montague was not a psychologist or an ontologist or", "tokens": [50364, 883, 11, 572, 11, 572, 11, 572, 13, 6962, 322, 13, 961, 311, 406, 483, 7947, 4918, 390, 406, 257, 29514, 420, 364, 6592, 9201, 420, 50672], "temperature": 0.0, "avg_logprob": -0.2021319632436715, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0013022127095609903}, {"id": 329, "seek": 228248, "start": 2289.44, "end": 2296.56, "text": " he said whatever your meaning for something is. Okay. He didn't even care. Montague never", "tokens": [50712, 415, 848, 2035, 428, 3620, 337, 746, 307, 13, 1033, 13, 634, 994, 380, 754, 1127, 13, 7947, 4918, 1128, 51068], "temperature": 0.0, "avg_logprob": -0.2021319632436715, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0013022127095609903}, {"id": 330, "seek": 228248, "start": 2296.56, "end": 2302.08, "text": " did ontology and conceptual and like, what, how do you define the meaning of what is a book?", "tokens": [51068, 630, 6592, 1793, 293, 24106, 293, 411, 11, 437, 11, 577, 360, 291, 6964, 264, 3620, 295, 437, 307, 257, 1446, 30, 51344], "temperature": 0.0, "avg_logprob": -0.2021319632436715, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0013022127095609903}, {"id": 331, "seek": 228248, "start": 2303.68, "end": 2309.04, "text": " Look, let's, and took me, I'm telling you, took me three years to appreciate what Montague was", "tokens": [51424, 2053, 11, 718, 311, 11, 293, 1890, 385, 11, 286, 478, 3585, 291, 11, 1890, 385, 1045, 924, 281, 4449, 437, 7947, 4918, 390, 51692], "temperature": 0.0, "avg_logprob": -0.2021319632436715, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0013022127095609903}, {"id": 332, "seek": 230904, "start": 2309.04, "end": 2314.4, "text": " doing. And I, and my thesis was on Montague semantics, the masters before the PhD. Here's", "tokens": [50364, 884, 13, 400, 286, 11, 293, 452, 22288, 390, 322, 7947, 4918, 4361, 45298, 11, 264, 19294, 949, 264, 14476, 13, 1692, 311, 50632], "temperature": 0.0, "avg_logprob": -0.14252815889508536, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0014756780583411455}, {"id": 333, "seek": 230904, "start": 2314.4, "end": 2320.88, "text": " what Montague did, Keith, and you'll appreciate Montague said, whatever your meaning for the", "tokens": [50632, 437, 7947, 4918, 630, 11, 20613, 11, 293, 291, 603, 4449, 7947, 4918, 848, 11, 2035, 428, 3620, 337, 264, 50956], "temperature": 0.0, "avg_logprob": -0.14252815889508536, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0014756780583411455}, {"id": 334, "seek": 230904, "start": 2320.88, "end": 2330.0, "text": " individual words, the lexical meaning. So cat means see, okay, you go with your psychologist and", "tokens": [50956, 2609, 2283, 11, 264, 476, 87, 804, 3620, 13, 407, 3857, 1355, 536, 11, 1392, 11, 291, 352, 365, 428, 29514, 293, 51412], "temperature": 0.0, "avg_logprob": -0.14252815889508536, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0014756780583411455}, {"id": 335, "seek": 230904, "start": 2330.72, "end": 2334.56, "text": " cognitive scientist and ontologist and disagree about the meaning of a cat.", "tokens": [51448, 15605, 12662, 293, 6592, 9201, 293, 14091, 466, 264, 3620, 295, 257, 3857, 13, 51640], "temperature": 0.0, "avg_logprob": -0.14252815889508536, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0014756780583411455}, {"id": 336, "seek": 233456, "start": 2335.52, "end": 2338.88, "text": " Finally, you come to me and you say, we have a meaning for cat and it's see.", "tokens": [50412, 6288, 11, 291, 808, 281, 385, 293, 291, 584, 11, 321, 362, 257, 3620, 337, 3857, 293, 309, 311, 536, 13, 50580], "temperature": 0.0, "avg_logprob": -0.1871708252850701, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.00022689990873914212}, {"id": 337, "seek": 233456, "start": 2340.72, "end": 2346.08, "text": " Follow me. Montague never cared about what is the nature of things outside. Right.", "tokens": [50672, 9876, 385, 13, 7947, 4918, 1128, 19779, 466, 437, 307, 264, 3687, 295, 721, 2380, 13, 1779, 13, 50940], "temperature": 0.0, "avg_logprob": -0.1871708252850701, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.00022689990873914212}, {"id": 338, "seek": 233456, "start": 2346.88, "end": 2354.32, "text": " Whatever your meaning for these individual concepts are. Right. Here's how you make,", "tokens": [50980, 8541, 428, 3620, 337, 613, 2609, 10392, 366, 13, 1779, 13, 1692, 311, 577, 291, 652, 11, 51352], "temperature": 0.0, "avg_logprob": -0.1871708252850701, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.00022689990873914212}, {"id": 339, "seek": 233456, "start": 2354.32, "end": 2361.84, "text": " you get the meaning of a whole mathematically. I'll give you a simple example that will make", "tokens": [51352, 291, 483, 264, 3620, 295, 257, 1379, 44003, 13, 286, 603, 976, 291, 257, 2199, 1365, 300, 486, 652, 51728], "temperature": 0.0, "avg_logprob": -0.1871708252850701, "compression_ratio": 1.5821596244131455, "no_speech_prob": 0.00022689990873914212}, {"id": 340, "seek": 236184, "start": 2361.92, "end": 2371.36, "text": " you appreciate what I'm talking about. John refers to a person. Right. The neighbor next door", "tokens": [50368, 291, 4449, 437, 286, 478, 1417, 466, 13, 2619, 14942, 281, 257, 954, 13, 1779, 13, 440, 5987, 958, 2853, 50840], "temperature": 0.0, "avg_logprob": -0.13499449238632666, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0009086067439056933}, {"id": 341, "seek": 236184, "start": 2372.56, "end": 2378.7200000000003, "text": " refers to a person. The neighbor next door that just moved from California refers to a person.", "tokens": [50900, 14942, 281, 257, 954, 13, 440, 5987, 958, 2853, 300, 445, 4259, 490, 5384, 14942, 281, 257, 954, 13, 51208], "temperature": 0.0, "avg_logprob": -0.13499449238632666, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0009086067439056933}, {"id": 342, "seek": 236184, "start": 2380.32, "end": 2388.0, "text": " The neighbor next door that knows John very well and drives for the LTD is a person.", "tokens": [51288, 440, 5987, 958, 2853, 300, 3255, 2619, 588, 731, 293, 11754, 337, 264, 42671, 35, 307, 257, 954, 13, 51672], "temperature": 0.0, "avg_logprob": -0.13499449238632666, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0009086067439056933}, {"id": 343, "seek": 238800, "start": 2388.8, "end": 2396.08, "text": " All of how can you have this phrase and John refer to a person and have the same semantic type", "tokens": [50404, 1057, 295, 577, 393, 291, 362, 341, 9535, 293, 2619, 2864, 281, 257, 954, 293, 362, 264, 912, 47982, 2010, 50768], "temperature": 0.0, "avg_logprob": -0.11614947599523208, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.0012624310329556465}, {"id": 344, "seek": 238800, "start": 2396.64, "end": 2403.36, "text": " composition in a way that never fails. Like you do in arithmetic, he wanted to prove that", "tokens": [50796, 12686, 294, 257, 636, 300, 1128, 18199, 13, 1743, 291, 360, 294, 42973, 11, 415, 1415, 281, 7081, 300, 51132], "temperature": 0.0, "avg_logprob": -0.11614947599523208, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.0012624310329556465}, {"id": 345, "seek": 238800, "start": 2403.36, "end": 2410.64, "text": " natural language is a formal language. He developed a semantic algebra that makes this long phrase", "tokens": [51132, 3303, 2856, 307, 257, 9860, 2856, 13, 634, 4743, 257, 47982, 21989, 300, 1669, 341, 938, 9535, 51496], "temperature": 0.0, "avg_logprob": -0.11614947599523208, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.0012624310329556465}, {"id": 346, "seek": 238800, "start": 2410.64, "end": 2416.0, "text": " referred to the in the end to an object that has the same semantic type as John mathematically.", "tokens": [51496, 10839, 281, 264, 294, 264, 917, 281, 364, 2657, 300, 575, 264, 912, 47982, 2010, 382, 2619, 44003, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11614947599523208, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.0012624310329556465}, {"id": 347, "seek": 241600, "start": 2416.0, "end": 2426.32, "text": " If you do it, it never fails. The details of this were genius. Okay. So Montague then made the big", "tokens": [50364, 759, 291, 360, 309, 11, 309, 1128, 18199, 13, 440, 4365, 295, 341, 645, 14017, 13, 1033, 13, 407, 7947, 4918, 550, 1027, 264, 955, 50880], "temperature": 0.0, "avg_logprob": -0.10544593710648387, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.00029102450935170054}, {"id": 348, "seek": 241600, "start": 2426.32, "end": 2432.88, "text": " claim natural language is a formal language. Give me some time. I'll work out the full algebra.", "tokens": [50880, 3932, 3303, 2856, 307, 257, 9860, 2856, 13, 5303, 385, 512, 565, 13, 286, 603, 589, 484, 264, 1577, 21989, 13, 51208], "temperature": 0.0, "avg_logprob": -0.10544593710648387, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.00029102450935170054}, {"id": 349, "seek": 241600, "start": 2434.08, "end": 2439.04, "text": " You go then and decide what the individual meanings are. I don't care. Montague never gave a damn", "tokens": [51268, 509, 352, 550, 293, 4536, 437, 264, 2609, 28138, 366, 13, 286, 500, 380, 1127, 13, 7947, 4918, 1128, 2729, 257, 8151, 51516], "temperature": 0.0, "avg_logprob": -0.10544593710648387, "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.00029102450935170054}, {"id": 350, "seek": 243904, "start": 2439.04, "end": 2445.04, "text": " about cognitive science and knowledge and he was a logician. He wanted to prove", "tokens": [50364, 466, 15605, 3497, 293, 3601, 293, 415, 390, 257, 3565, 9027, 13, 634, 1415, 281, 7081, 50664], "temperature": 0.0, "avg_logprob": -0.12583854924077573, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.02125605382025242}, {"id": 351, "seek": 243904, "start": 2445.68, "end": 2451.44, "text": " there's a calculus underneath natural language. Calculus of meanings. You decide on the meaning.", "tokens": [50696, 456, 311, 257, 33400, 7223, 3303, 2856, 13, 3511, 36002, 295, 28138, 13, 509, 4536, 322, 264, 3620, 13, 50984], "temperature": 0.0, "avg_logprob": -0.12583854924077573, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.02125605382025242}, {"id": 352, "seek": 243904, "start": 2452.0, "end": 2456.16, "text": " I'm telling you, it took me a while. I thought he's doing semantics. What is the meaning of this", "tokens": [51012, 286, 478, 3585, 291, 11, 309, 1890, 385, 257, 1339, 13, 286, 1194, 415, 311, 884, 4361, 45298, 13, 708, 307, 264, 3620, 295, 341, 51220], "temperature": 0.0, "avg_logprob": -0.12583854924077573, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.02125605382025242}, {"id": 353, "seek": 243904, "start": 2456.16, "end": 2462.72, "text": " and Montague? He said he never cared. He was doing an algebra of meanings, regardless of what the", "tokens": [51220, 293, 7947, 4918, 30, 634, 848, 415, 1128, 19779, 13, 634, 390, 884, 364, 21989, 295, 28138, 11, 10060, 295, 437, 264, 51548], "temperature": 0.0, "avg_logprob": -0.12583854924077573, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.02125605382025242}, {"id": 354, "seek": 246272, "start": 2462.72, "end": 2477.4399999999996, "text": " meanings are. Okay. But his project was huge. Montague was trying to prove there's an algebraic", "tokens": [50364, 28138, 366, 13, 1033, 13, 583, 702, 1716, 390, 2603, 13, 7947, 4918, 390, 1382, 281, 7081, 456, 311, 364, 21989, 299, 51100], "temperature": 0.0, "avg_logprob": -0.1431435416726505, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.0034813694655895233}, {"id": 355, "seek": 246272, "start": 2477.4399999999996, "end": 2483.4399999999996, "text": " system behind language, like any other formal language. Like you can get an arithmetic expression", "tokens": [51100, 1185, 2261, 2856, 11, 411, 604, 661, 9860, 2856, 13, 1743, 291, 393, 483, 364, 42973, 6114, 51400], "temperature": 0.0, "avg_logprob": -0.1431435416726505, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.0034813694655895233}, {"id": 356, "seek": 246272, "start": 2483.4399999999996, "end": 2488.8799999999997, "text": " and build a tree for that, evaluate it, and get the final meaning. Natural language works the same", "tokens": [51400, 293, 1322, 257, 4230, 337, 300, 11, 13059, 309, 11, 293, 483, 264, 2572, 3620, 13, 20137, 2856, 1985, 264, 912, 51672], "temperature": 0.0, "avg_logprob": -0.1431435416726505, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.0034813694655895233}, {"id": 357, "seek": 248888, "start": 2488.88, "end": 2496.7200000000003, "text": " way. Except it's not that simple. That's all. So his project was huge and he was misunderstood.", "tokens": [50364, 636, 13, 16192, 309, 311, 406, 300, 2199, 13, 663, 311, 439, 13, 407, 702, 1716, 390, 2603, 293, 415, 390, 33870, 13, 50756], "temperature": 0.0, "avg_logprob": -0.08826941762651716, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.0012836756650358438}, {"id": 358, "seek": 248888, "start": 2498.48, "end": 2504.32, "text": " So he was really doing semantics. That's semantics. Pragmatics is a different thing.", "tokens": [50844, 407, 415, 390, 534, 884, 4361, 45298, 13, 663, 311, 4361, 45298, 13, 40067, 15677, 1167, 307, 257, 819, 551, 13, 51136], "temperature": 0.0, "avg_logprob": -0.08826941762651716, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.0012836756650358438}, {"id": 359, "seek": 248888, "start": 2505.44, "end": 2511.84, "text": " What do you think is the core unique property of natural human language? Would you agree with", "tokens": [51192, 708, 360, 291, 519, 307, 264, 4965, 3845, 4707, 295, 3303, 1952, 2856, 30, 6068, 291, 3986, 365, 51512], "temperature": 0.0, "avg_logprob": -0.08826941762651716, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.0012836756650358438}, {"id": 360, "seek": 251184, "start": 2511.84, "end": 2521.44, "text": " Chomsky on it being digital infinity? Yeah. Okay. The infinite thing in the productivity.", "tokens": [50364, 761, 4785, 4133, 322, 309, 885, 4562, 13202, 30, 865, 13, 1033, 13, 440, 13785, 551, 294, 264, 15604, 13, 50844], "temperature": 0.0, "avg_logprob": -0.12633067683169716, "compression_ratio": 1.5027932960893855, "no_speech_prob": 0.003317327704280615}, {"id": 361, "seek": 251184, "start": 2524.0, "end": 2533.36, "text": " To me, no, it's I'm half Chomsky and half something else. To me, no, the real,", "tokens": [50972, 1407, 385, 11, 572, 11, 309, 311, 286, 478, 1922, 761, 4785, 4133, 293, 1922, 746, 1646, 13, 1407, 385, 11, 572, 11, 264, 957, 11, 51440], "temperature": 0.0, "avg_logprob": -0.12633067683169716, "compression_ratio": 1.5027932960893855, "no_speech_prob": 0.003317327704280615}, {"id": 362, "seek": 251184, "start": 2533.36, "end": 2540.0, "text": " real unique thing about language. And that's why even if Montague succeeded, that's half the battle.", "tokens": [51440, 957, 3845, 551, 466, 2856, 13, 400, 300, 311, 983, 754, 498, 7947, 4918, 20263, 11, 300, 311, 1922, 264, 4635, 13, 51772], "temperature": 0.0, "avg_logprob": -0.12633067683169716, "compression_ratio": 1.5027932960893855, "no_speech_prob": 0.003317327704280615}, {"id": 363, "seek": 254000, "start": 2540.96, "end": 2549.28, "text": " It's not in the semantics, although that's huge. To me, it's the pragmatic side, the abductive", "tokens": [50412, 467, 311, 406, 294, 264, 4361, 45298, 11, 4878, 300, 311, 2603, 13, 1407, 385, 11, 309, 311, 264, 46904, 1252, 11, 264, 46465, 488, 50828], "temperature": 0.0, "avg_logprob": -0.10971228717124626, "compression_ratio": 1.6449704142011834, "no_speech_prob": 0.00034033547854050994}, {"id": 364, "seek": 254000, "start": 2549.28, "end": 2555.92, "text": " inference. I mean, we use induction and we use deduction and we always ignore abduction.", "tokens": [50828, 38253, 13, 286, 914, 11, 321, 764, 33371, 293, 321, 764, 46385, 293, 321, 1009, 11200, 410, 40335, 13, 51160], "temperature": 0.0, "avg_logprob": -0.10971228717124626, "compression_ratio": 1.6449704142011834, "no_speech_prob": 0.00034033547854050994}, {"id": 365, "seek": 254000, "start": 2555.92, "end": 2564.56, "text": " Abduction is the unique, is the humanly unique reasoning capability. I mean, rats do inductive", "tokens": [51160, 2847, 40335, 307, 264, 3845, 11, 307, 264, 1952, 356, 3845, 21577, 13759, 13, 286, 914, 11, 25691, 360, 31612, 488, 51592], "temperature": 0.0, "avg_logprob": -0.10971228717124626, "compression_ratio": 1.6449704142011834, "no_speech_prob": 0.00034033547854050994}, {"id": 366, "seek": 256456, "start": 2564.64, "end": 2570.88, "text": " reasoning. They, to a certain extent, that's how they learn a few things, inductively, really.", "tokens": [50368, 21577, 13, 814, 11, 281, 257, 1629, 8396, 11, 300, 311, 577, 436, 1466, 257, 1326, 721, 11, 31612, 3413, 11, 534, 13, 50680], "temperature": 0.0, "avg_logprob": -0.18450577704461066, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.0016197930090129375}, {"id": 367, "seek": 256456, "start": 2572.72, "end": 2578.64, "text": " All the lower species do inductive reasoning to a certain extent. And some of them do some", "tokens": [50772, 1057, 264, 3126, 6172, 360, 31612, 488, 21577, 281, 257, 1629, 8396, 13, 400, 512, 295, 552, 360, 512, 51068], "temperature": 0.0, "avg_logprob": -0.18450577704461066, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.0016197930090129375}, {"id": 368, "seek": 256456, "start": 2578.64, "end": 2584.4, "text": " deductive reasoning, if there's then this, but at a very shallow level, of course. Abductive", "tokens": [51068, 31513, 488, 21577, 11, 498, 456, 311, 550, 341, 11, 457, 412, 257, 588, 20488, 1496, 11, 295, 1164, 13, 2847, 5020, 488, 51356], "temperature": 0.0, "avg_logprob": -0.18450577704461066, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.0016197930090129375}, {"id": 369, "seek": 256456, "start": 2584.4, "end": 2592.08, "text": " reasoning is uniquely human. And that's the part of language understanding, which means", "tokens": [51356, 21577, 307, 31474, 1952, 13, 400, 300, 311, 264, 644, 295, 2856, 3701, 11, 597, 1355, 51740], "temperature": 0.0, "avg_logprob": -0.18450577704461066, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.0016197930090129375}, {"id": 370, "seek": 259208, "start": 2592.08, "end": 2599.12, "text": " reasoning to the best explanation. Abductive reasoning is I reach a conclusion, not inductively", "tokens": [50364, 21577, 281, 264, 1151, 10835, 13, 2847, 5020, 488, 21577, 307, 286, 2524, 257, 10063, 11, 406, 31612, 3413, 50716], "temperature": 0.0, "avg_logprob": -0.11839149309241254, "compression_ratio": 1.668639053254438, "no_speech_prob": 0.0008824883843772113}, {"id": 371, "seek": 259208, "start": 2600.0, "end": 2608.08, "text": " by induction or, and not deductively, I deduced it. But I reached this conclusion because it's", "tokens": [50760, 538, 33371, 420, 11, 293, 406, 31513, 3413, 11, 286, 4172, 41209, 309, 13, 583, 286, 6488, 341, 10063, 570, 309, 311, 51164], "temperature": 0.0, "avg_logprob": -0.11839149309241254, "compression_ratio": 1.668639053254438, "no_speech_prob": 0.0008824883843772113}, {"id": 372, "seek": 259208, "start": 2608.08, "end": 2617.52, "text": " possible, it can happen. And it is the best conclusion I can come up with, given everything", "tokens": [51164, 1944, 11, 309, 393, 1051, 13, 400, 309, 307, 264, 1151, 10063, 286, 393, 808, 493, 365, 11, 2212, 1203, 51636], "temperature": 0.0, "avg_logprob": -0.11839149309241254, "compression_ratio": 1.668639053254438, "no_speech_prob": 0.0008824883843772113}, {"id": 373, "seek": 261752, "start": 2617.52, "end": 2624.56, "text": " else I know. Abductive reasoning is the real reasoning methodology that makes us unique as", "tokens": [50364, 1646, 286, 458, 13, 2847, 5020, 488, 21577, 307, 264, 957, 21577, 24850, 300, 1669, 505, 3845, 382, 50716], "temperature": 0.0, "avg_logprob": -0.13462877945161203, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.001047795289196074}, {"id": 374, "seek": 261752, "start": 2624.56, "end": 2631.6, "text": " human. We reason to the best, we reason, it's called reasoning to the best explanation. Right? So", "tokens": [50716, 1952, 13, 492, 1778, 281, 264, 1151, 11, 321, 1778, 11, 309, 311, 1219, 21577, 281, 264, 1151, 10835, 13, 1779, 30, 407, 51068], "temperature": 0.0, "avg_logprob": -0.13462877945161203, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.001047795289196074}, {"id": 375, "seek": 261752, "start": 2632.24, "end": 2640.4, "text": " that's, that's, Pierce and others, I mean, Pierce was the pioneer of abductive reasoning or", "tokens": [51100, 300, 311, 11, 300, 311, 11, 45432, 293, 2357, 11, 286, 914, 11, 45432, 390, 264, 37668, 295, 46465, 488, 21577, 420, 51508], "temperature": 0.0, "avg_logprob": -0.13462877945161203, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.001047795289196074}, {"id": 376, "seek": 264040, "start": 2640.4, "end": 2648.2400000000002, "text": " abduction. But I'm talking about an abduction has come to have two sort of tracks. And there's", "tokens": [50364, 410, 40335, 13, 583, 286, 478, 1417, 466, 364, 410, 40335, 575, 808, 281, 362, 732, 1333, 295, 10218, 13, 400, 456, 311, 50756], "temperature": 0.0, "avg_logprob": -0.16803094031105578, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.001015214598737657}, {"id": 377, "seek": 264040, "start": 2648.2400000000002, "end": 2654.56, "text": " abductive reasoning in the traditional philosophical charts, Pierce. But there's abductive reasoning", "tokens": [50756, 46465, 488, 21577, 294, 264, 5164, 25066, 17767, 11, 45432, 13, 583, 456, 311, 46465, 488, 21577, 51072], "temperature": 0.0, "avg_logprob": -0.16803094031105578, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.001015214598737657}, {"id": 378, "seek": 264040, "start": 2654.56, "end": 2662.8, "text": " as it used to be called in the 80s, when case-based reasoning came out and expert systems who,", "tokens": [51072, 382, 309, 1143, 281, 312, 1219, 294, 264, 4688, 82, 11, 562, 1389, 12, 6032, 21577, 1361, 484, 293, 5844, 3652, 567, 11, 51484], "temperature": 0.0, "avg_logprob": -0.16803094031105578, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.001015214598737657}, {"id": 379, "seek": 266280, "start": 2663.76, "end": 2670.5600000000004, "text": " there was something called EBL, explanation-based learning. And it was even a learning technique,", "tokens": [50412, 456, 390, 746, 1219, 462, 17624, 11, 10835, 12, 6032, 2539, 13, 400, 309, 390, 754, 257, 2539, 6532, 11, 50752], "temperature": 0.0, "avg_logprob": -0.13964588541380116, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.002285779919475317}, {"id": 380, "seek": 266280, "start": 2671.1200000000003, "end": 2679.36, "text": " which is really reasoning to the best explanation. Basically, I have to make a decision. Actually,", "tokens": [50780, 597, 307, 534, 21577, 281, 264, 1151, 10835, 13, 8537, 11, 286, 362, 281, 652, 257, 3537, 13, 5135, 11, 51192], "temperature": 0.0, "avg_logprob": -0.13964588541380116, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.002285779919475317}, {"id": 381, "seek": 266280, "start": 2679.36, "end": 2685.6800000000003, "text": " Jerry Hobbs, you guys heard me mention his name several times before, who's, I think, huge in", "tokens": [51192, 17454, 22966, 929, 11, 291, 1074, 2198, 385, 2152, 702, 1315, 2940, 1413, 949, 11, 567, 311, 11, 286, 519, 11, 2603, 294, 51508], "temperature": 0.0, "avg_logprob": -0.13964588541380116, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.002285779919475317}, {"id": 382, "seek": 268568, "start": 2685.68, "end": 2694.72, "text": " semantics, has a paper when he was at SRI with other luminaries too. The title is interpretation", "tokens": [50364, 4361, 45298, 11, 575, 257, 3035, 562, 415, 390, 412, 318, 5577, 365, 661, 32476, 4889, 886, 13, 440, 4876, 307, 14174, 50816], "temperature": 0.0, "avg_logprob": -0.1113771726918775, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.019700530916452408}, {"id": 383, "seek": 268568, "start": 2694.72, "end": 2700.96, "text": " as abduction or understanding as abduction. And basically, he shows how all the difficult,", "tokens": [50816, 382, 410, 40335, 420, 3701, 382, 410, 40335, 13, 400, 1936, 11, 415, 3110, 577, 439, 264, 2252, 11, 51128], "temperature": 0.0, "avg_logprob": -0.1113771726918775, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.019700530916452408}, {"id": 384, "seek": 268568, "start": 2702.24, "end": 2706.72, "text": " all the challenges in language understanding beyond semantics. So we're done with Montague.", "tokens": [51192, 439, 264, 4759, 294, 2856, 3701, 4399, 4361, 45298, 13, 407, 321, 434, 1096, 365, 7947, 4918, 13, 51416], "temperature": 0.0, "avg_logprob": -0.1113771726918775, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.019700530916452408}, {"id": 385, "seek": 268568, "start": 2706.72, "end": 2713.12, "text": " Now I'm doing the final understanding of what makes sense given, because every expression has", "tokens": [51416, 823, 286, 478, 884, 264, 2572, 3701, 295, 437, 1669, 2020, 2212, 11, 570, 633, 6114, 575, 51736], "temperature": 0.0, "avg_logprob": -0.1113771726918775, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.019700530916452408}, {"id": 386, "seek": 271312, "start": 2713.12, "end": 2719.6, "text": " several meanings. Even if I did the semantics perfectly, I have to choose the most plausible", "tokens": [50364, 2940, 28138, 13, 2754, 498, 286, 630, 264, 4361, 45298, 6239, 11, 286, 362, 281, 2826, 264, 881, 39925, 50688], "temperature": 0.0, "avg_logprob": -0.11558888753255209, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.002978656440973282}, {"id": 387, "seek": 271312, "start": 2719.6, "end": 2725.2799999999997, "text": " meaning from all the possible meanings. That's pragmatics. And the way you do that very well", "tokens": [50688, 3620, 490, 439, 264, 1944, 28138, 13, 663, 311, 33394, 15677, 1167, 13, 400, 264, 636, 291, 360, 300, 588, 731, 50972], "temperature": 0.0, "avg_logprob": -0.11558888753255209, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.002978656440973282}, {"id": 388, "seek": 271312, "start": 2725.2799999999997, "end": 2733.04, "text": " is in language. We do abductive reasoning. We say, I'm left with three meanings, three possible", "tokens": [50972, 307, 294, 2856, 13, 492, 360, 46465, 488, 21577, 13, 492, 584, 11, 286, 478, 1411, 365, 1045, 28138, 11, 1045, 1944, 51360], "temperature": 0.0, "avg_logprob": -0.11558888753255209, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.002978656440973282}, {"id": 389, "seek": 271312, "start": 2733.04, "end": 2740.96, "text": " meanings, syntax excluded, 200 syntax trees, semantics excluded, few invalid semantic expressions.", "tokens": [51360, 28138, 11, 28431, 29486, 11, 2331, 28431, 5852, 11, 4361, 45298, 29486, 11, 1326, 34702, 47982, 15277, 13, 51756], "temperature": 0.0, "avg_logprob": -0.11558888753255209, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.002978656440973282}, {"id": 390, "seek": 274096, "start": 2740.96, "end": 2746.16, "text": " And I'm left with three still, three possible meanings. They can all happen in the world we", "tokens": [50364, 400, 286, 478, 1411, 365, 1045, 920, 11, 1045, 1944, 28138, 13, 814, 393, 439, 1051, 294, 264, 1002, 321, 50624], "temperature": 0.0, "avg_logprob": -0.06544297228577317, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0009389171027578413}, {"id": 391, "seek": 274096, "start": 2746.16, "end": 2753.28, "text": " live in. Which one is the most plausible? We do this abductively. Which meaning is the most", "tokens": [50624, 1621, 294, 13, 3013, 472, 307, 264, 881, 39925, 30, 492, 360, 341, 46465, 3413, 13, 3013, 3620, 307, 264, 881, 50980], "temperature": 0.0, "avg_logprob": -0.06544297228577317, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0009389171027578413}, {"id": 392, "seek": 274096, "start": 2753.28, "end": 2759.2, "text": " likely meaning given the context and what I know? That's the last challenge in language.", "tokens": [50980, 3700, 3620, 2212, 264, 4319, 293, 437, 286, 458, 30, 663, 311, 264, 1036, 3430, 294, 2856, 13, 51276], "temperature": 0.0, "avg_logprob": -0.06544297228577317, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0009389171027578413}, {"id": 393, "seek": 274096, "start": 2759.92, "end": 2765.52, "text": " So we need to, we need to add the abductive model, which we humans do. I go back to the", "tokens": [51312, 407, 321, 643, 281, 11, 321, 643, 281, 909, 264, 46465, 488, 2316, 11, 597, 321, 6255, 360, 13, 286, 352, 646, 281, 264, 51592], "temperature": 0.0, "avg_logprob": -0.06544297228577317, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0009389171027578413}, {"id": 394, "seek": 276552, "start": 2765.52, "end": 2771.28, "text": " teenager shot of policemen, both meanings, both interpretation can happen, right?", "tokens": [50364, 21440, 3347, 295, 6285, 14071, 11, 1293, 28138, 11, 1293, 14174, 393, 1051, 11, 558, 30, 50652], "temperature": 0.0, "avg_logprob": -0.1621715227762858, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0018380660330876708}, {"id": 395, "seek": 276552, "start": 2772.64, "end": 2780.32, "text": " Either one can flee, right? But most likely it's the teenager that fled away, given what I know", "tokens": [50720, 13746, 472, 393, 25146, 11, 558, 30, 583, 881, 3700, 309, 311, 264, 21440, 300, 24114, 1314, 11, 2212, 437, 286, 458, 51104], "temperature": 0.0, "avg_logprob": -0.1621715227762858, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0018380660330876708}, {"id": 396, "seek": 276552, "start": 2780.32, "end": 2784.8, "text": " and given that's abductive reasoning. But semantically both can happen.", "tokens": [51104, 293, 2212, 300, 311, 46465, 488, 21577, 13, 583, 4361, 49505, 1293, 393, 1051, 13, 51328], "temperature": 0.0, "avg_logprob": -0.1621715227762858, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0018380660330876708}, {"id": 397, "seek": 276552, "start": 2785.7599999999998, "end": 2791.28, "text": " Yes, I do want to emphasize something that Wally like briefly mentioned, but I think it's very", "tokens": [51376, 1079, 11, 286, 360, 528, 281, 16078, 746, 300, 343, 379, 411, 10515, 2835, 11, 457, 286, 519, 309, 311, 588, 51652], "temperature": 0.0, "avg_logprob": -0.1621715227762858, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0018380660330876708}, {"id": 398, "seek": 279128, "start": 2791.36, "end": 2798.0, "text": " important to mention is that there's two senses of abduction. And they differ in the", "tokens": [50368, 1021, 281, 2152, 307, 300, 456, 311, 732, 17057, 295, 410, 40335, 13, 400, 436, 743, 294, 264, 50700], "temperature": 0.0, "avg_logprob": -0.09880385341414485, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.04333530366420746}, {"id": 399, "seek": 279128, "start": 2798.0, "end": 2802.32, "text": " following way, which is kind of the more modern sense, which is what Wally's been talking about", "tokens": [50700, 3480, 636, 11, 597, 307, 733, 295, 264, 544, 4363, 2020, 11, 597, 307, 437, 343, 379, 311, 668, 1417, 466, 50916], "temperature": 0.0, "avg_logprob": -0.09880385341414485, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.04333530366420746}, {"id": 400, "seek": 279128, "start": 2802.32, "end": 2809.92, "text": " like pretty much this whole time, is abduction used to justify hypotheses. But the older and", "tokens": [50916, 411, 1238, 709, 341, 1379, 565, 11, 307, 410, 40335, 1143, 281, 20833, 49969, 13, 583, 264, 4906, 293, 51296], "temperature": 0.0, "avg_logprob": -0.09880385341414485, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.04333530366420746}, {"id": 401, "seek": 279128, "start": 2809.92, "end": 2816.0800000000004, "text": " original sense of it and still an equally important one is abduction for generating", "tokens": [51296, 3380, 2020, 295, 309, 293, 920, 364, 12309, 1021, 472, 307, 410, 40335, 337, 17746, 51604], "temperature": 0.0, "avg_logprob": -0.09880385341414485, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.04333530366420746}, {"id": 402, "seek": 281608, "start": 2816.7999999999997, "end": 2823.44, "text": " hypotheses. And this ability to generate hypotheses is something that's extremely", "tokens": [50400, 49969, 13, 400, 341, 3485, 281, 8460, 49969, 307, 746, 300, 311, 4664, 50732], "temperature": 0.0, "avg_logprob": -0.17962638441338596, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.05498630926012993}, {"id": 403, "seek": 281608, "start": 2823.44, "end": 2829.7599999999998, "text": " powerful and so far uniquely human. But generate from... Hold on, let me just finish here.", "tokens": [50732, 4005, 293, 370, 1400, 31474, 1952, 13, 583, 8460, 490, 485, 6962, 322, 11, 718, 385, 445, 2413, 510, 13, 51048], "temperature": 0.0, "avg_logprob": -0.17962638441338596, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.05498630926012993}, {"id": 404, "seek": 281608, "start": 2829.7599999999998, "end": 2835.12, "text": " Which is this, this is like something where Einstein is just sitting there pontificating on", "tokens": [51048, 3013, 307, 341, 11, 341, 307, 411, 746, 689, 23486, 307, 445, 3798, 456, 18770, 1089, 990, 322, 51316], "temperature": 0.0, "avg_logprob": -0.17962638441338596, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.05498630926012993}, {"id": 405, "seek": 281608, "start": 2836.72, "end": 2841.7599999999998, "text": " how the heck can light be the same no matter how the earth is moving and blah, blah, blah,", "tokens": [51396, 577, 264, 12872, 393, 1442, 312, 264, 912, 572, 1871, 577, 264, 4120, 307, 2684, 293, 12288, 11, 12288, 11, 12288, 11, 51648], "temperature": 0.0, "avg_logprob": -0.17962638441338596, "compression_ratio": 1.6359447004608294, "no_speech_prob": 0.05498630926012993}, {"id": 406, "seek": 284176, "start": 2841.76, "end": 2849.84, "text": " and comes up without a thin air, like this hypothesis that relativity applies, right?", "tokens": [50364, 293, 1487, 493, 1553, 257, 5862, 1988, 11, 411, 341, 17291, 300, 45675, 13165, 11, 558, 30, 50768], "temperature": 0.0, "avg_logprob": -0.15721724463290856, "compression_ratio": 1.5, "no_speech_prob": 0.018257657065987587}, {"id": 407, "seek": 284176, "start": 2849.84, "end": 2856.2400000000002, "text": " That the physical laws are the same no matter what your reference frame is. So this ability", "tokens": [50768, 663, 264, 4001, 6064, 366, 264, 912, 572, 1871, 437, 428, 6408, 3920, 307, 13, 407, 341, 3485, 51088], "temperature": 0.0, "avg_logprob": -0.15721724463290856, "compression_ratio": 1.5, "no_speech_prob": 0.018257657065987587}, {"id": 408, "seek": 284176, "start": 2856.2400000000002, "end": 2860.96, "text": " to almost... That people talk about sort of pull from thin air, this kind of intuitive", "tokens": [51088, 281, 1920, 485, 663, 561, 751, 466, 1333, 295, 2235, 490, 5862, 1988, 11, 341, 733, 295, 21769, 51324], "temperature": 0.0, "avg_logprob": -0.15721724463290856, "compression_ratio": 1.5, "no_speech_prob": 0.018257657065987587}, {"id": 409, "seek": 286096, "start": 2861.6, "end": 2870.2400000000002, "text": " leap to something that ends up being like a grand new theory, that's also abduction.", "tokens": [50396, 19438, 281, 746, 300, 5314, 493, 885, 411, 257, 2697, 777, 5261, 11, 300, 311, 611, 410, 40335, 13, 50828], "temperature": 0.0, "avg_logprob": -0.1954106092453003, "compression_ratio": 1.5157232704402517, "no_speech_prob": 0.058320265263319016}, {"id": 410, "seek": 286096, "start": 2870.2400000000002, "end": 2874.08, "text": " Right. But in both cases, Keith, and I agree with you, that's", "tokens": [50828, 1779, 13, 583, 294, 1293, 3331, 11, 20613, 11, 293, 286, 3986, 365, 291, 11, 300, 311, 51020], "temperature": 0.0, "avg_logprob": -0.1954106092453003, "compression_ratio": 1.5157232704402517, "no_speech_prob": 0.058320265263319016}, {"id": 411, "seek": 286096, "start": 2876.4, "end": 2883.76, "text": " the old view of what abductive reasoning was to scientists. But in both cases, you're choosing", "tokens": [51136, 264, 1331, 1910, 295, 437, 46465, 488, 21577, 390, 281, 7708, 13, 583, 294, 1293, 3331, 11, 291, 434, 10875, 51504], "temperature": 0.0, "avg_logprob": -0.1954106092453003, "compression_ratio": 1.5157232704402517, "no_speech_prob": 0.058320265263319016}, {"id": 412, "seek": 288376, "start": 2883.76, "end": 2890.96, "text": " from possible... Oh, no, no, no, just a minute because this is where I think I probably quite", "tokens": [50364, 490, 1944, 485, 876, 11, 572, 11, 572, 11, 572, 11, 445, 257, 3456, 570, 341, 307, 689, 286, 519, 286, 1391, 1596, 50724], "temperature": 0.0, "avg_logprob": -0.12834512581259516, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.04602083936333656}, {"id": 413, "seek": 288376, "start": 2890.96, "end": 2896.1600000000003, "text": " disagree with you, which is the modern sense of abduction to me is much more similar to just", "tokens": [50724, 14091, 365, 291, 11, 597, 307, 264, 4363, 2020, 295, 410, 40335, 281, 385, 307, 709, 544, 2531, 281, 445, 50984], "temperature": 0.0, "avg_logprob": -0.12834512581259516, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.04602083936333656}, {"id": 414, "seek": 288376, "start": 2896.1600000000003, "end": 2901.28, "text": " inference like to a Bayesian. So in other words, you give me a whole slew of hypotheses and I can", "tokens": [50984, 38253, 411, 281, 257, 7840, 42434, 13, 407, 294, 661, 2283, 11, 291, 976, 385, 257, 1379, 2426, 86, 295, 49969, 293, 286, 393, 51240], "temperature": 0.0, "avg_logprob": -0.12834512581259516, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.04602083936333656}, {"id": 415, "seek": 288376, "start": 2901.28, "end": 2907.6000000000004, "text": " tell you which hypotheses should be preferred just on the basis of marginalization and strict,", "tokens": [51240, 980, 291, 597, 49969, 820, 312, 16494, 445, 322, 264, 5143, 295, 16885, 2144, 293, 10910, 11, 51556], "temperature": 0.0, "avg_logprob": -0.12834512581259516, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.04602083936333656}, {"id": 416, "seek": 288376, "start": 2907.6000000000004, "end": 2912.5600000000004, "text": " like Bayesian theory, no problem with that. It's not actually abduction, it's just inference,", "tokens": [51556, 411, 7840, 42434, 5261, 11, 572, 1154, 365, 300, 13, 467, 311, 406, 767, 410, 40335, 11, 309, 311, 445, 38253, 11, 51804], "temperature": 0.0, "avg_logprob": -0.12834512581259516, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.04602083936333656}, {"id": 417, "seek": 291256, "start": 2912.56, "end": 2919.92, "text": " right? Just rules of inference. Whereas just a minute, generating that space", "tokens": [50364, 558, 30, 1449, 4474, 295, 38253, 13, 13813, 445, 257, 3456, 11, 17746, 300, 1901, 50732], "temperature": 0.0, "avg_logprob": -0.1342939019203186, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.00031996844336390495}, {"id": 418, "seek": 291256, "start": 2919.92, "end": 2925.7599999999998, "text": " in the first place is unique and very different from inference, like the ability to produce", "tokens": [50732, 294, 264, 700, 1081, 307, 3845, 293, 588, 819, 490, 38253, 11, 411, 264, 3485, 281, 5258, 51024], "temperature": 0.0, "avg_logprob": -0.1342939019203186, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.00031996844336390495}, {"id": 419, "seek": 291256, "start": 2926.88, "end": 2932.4, "text": " from nothing models to consider, that's the core of abduction from my point of view.", "tokens": [51080, 490, 1825, 5245, 281, 1949, 11, 300, 311, 264, 4965, 295, 410, 40335, 490, 452, 935, 295, 1910, 13, 51356], "temperature": 0.0, "avg_logprob": -0.1342939019203186, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.00031996844336390495}, {"id": 420, "seek": 291256, "start": 2934.16, "end": 2939.68, "text": " But okay, so we're saying the same thing, but indifferent. These possibilities that you generate", "tokens": [51444, 583, 1392, 11, 370, 321, 434, 1566, 264, 912, 551, 11, 457, 48502, 13, 1981, 12178, 300, 291, 8460, 51720], "temperature": 0.0, "avg_logprob": -0.1342939019203186, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.00031996844336390495}, {"id": 421, "seek": 293968, "start": 2940.48, "end": 2945.2799999999997, "text": " are valid possibilities. So abductive reasoning... I don't know if they're valid until I do the", "tokens": [50404, 366, 7363, 12178, 13, 407, 46465, 488, 21577, 485, 286, 500, 380, 458, 498, 436, 434, 7363, 1826, 286, 360, 264, 50644], "temperature": 0.0, "avg_logprob": -0.15610220853020162, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.0005974211380816996}, {"id": 422, "seek": 293968, "start": 2945.2799999999997, "end": 2952.8799999999997, "text": " inference. No, you're generating a pool of possibilities. That's the step, generating a pool", "tokens": [50644, 38253, 13, 883, 11, 291, 434, 17746, 257, 7005, 295, 12178, 13, 663, 311, 264, 1823, 11, 17746, 257, 7005, 51024], "temperature": 0.0, "avg_logprob": -0.15610220853020162, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.0005974211380816996}, {"id": 423, "seek": 293968, "start": 2952.8799999999997, "end": 2958.0, "text": " of possibilities. Fine, fine, fine, fine, but in the end... How do you do that? Keith, I think we're", "tokens": [51024, 295, 12178, 13, 12024, 11, 2489, 11, 2489, 11, 2489, 11, 457, 294, 264, 917, 485, 1012, 360, 291, 360, 300, 30, 20613, 11, 286, 519, 321, 434, 51280], "temperature": 0.0, "avg_logprob": -0.15610220853020162, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.0005974211380816996}, {"id": 424, "seek": 293968, "start": 2958.0, "end": 2964.16, "text": " saying the same thing, it's just a terminology. In the end, you're choosing from a set of possible", "tokens": [51280, 1566, 264, 912, 551, 11, 309, 311, 445, 257, 27575, 13, 682, 264, 917, 11, 291, 434, 10875, 490, 257, 992, 295, 1944, 51588], "temperature": 0.0, "avg_logprob": -0.15610220853020162, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.0005974211380816996}, {"id": 425, "seek": 296416, "start": 2965.04, "end": 2974.96, "text": " valid hypotheses. Induction is, you don't know where you're going until you get there. In abductive", "tokens": [50408, 7363, 49969, 13, 2333, 27549, 307, 11, 291, 500, 380, 458, 689, 291, 434, 516, 1826, 291, 483, 456, 13, 682, 46465, 488, 50904], "temperature": 0.0, "avg_logprob": -0.10206287557428534, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.001150812371633947}, {"id": 426, "seek": 296416, "start": 2974.96, "end": 2981.7599999999998, "text": " reasoning, you are, whether it's the old way or the modern way, in the end, what's common between", "tokens": [50904, 21577, 11, 291, 366, 11, 1968, 309, 311, 264, 1331, 636, 420, 264, 4363, 636, 11, 294, 264, 917, 11, 437, 311, 2689, 1296, 51244], "temperature": 0.0, "avg_logprob": -0.10206287557428534, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.001150812371633947}, {"id": 427, "seek": 296416, "start": 2981.7599999999998, "end": 2990.16, "text": " them is, I have a set of possibilities. I will use abductive reasoning to decide which is the most", "tokens": [51244, 552, 307, 11, 286, 362, 257, 992, 295, 12178, 13, 286, 486, 764, 46465, 488, 21577, 281, 4536, 597, 307, 264, 881, 51664], "temperature": 0.0, "avg_logprob": -0.10206287557428534, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.001150812371633947}, {"id": 428, "seek": 299016, "start": 2990.16, "end": 2995.3599999999997, "text": " plausible. In a sense, you're scoring them, and you're saying, from all these possibilities, this", "tokens": [50364, 39925, 13, 682, 257, 2020, 11, 291, 434, 22358, 552, 11, 293, 291, 434, 1566, 11, 490, 439, 613, 12178, 11, 341, 50624], "temperature": 0.0, "avg_logprob": -0.13672690832314371, "compression_ratio": 1.9311740890688258, "no_speech_prob": 0.022257443517446518}, {"id": 429, "seek": 299016, "start": 2995.3599999999997, "end": 3000.8799999999997, "text": " is the most plausible. Yeah, but see, you keep assuming the... You keep positing that you have", "tokens": [50624, 307, 264, 881, 39925, 13, 865, 11, 457, 536, 11, 291, 1066, 11926, 264, 485, 509, 1066, 1366, 1748, 300, 291, 362, 50900], "temperature": 0.0, "avg_logprob": -0.13672690832314371, "compression_ratio": 1.9311740890688258, "no_speech_prob": 0.022257443517446518}, {"id": 430, "seek": 299016, "start": 3000.8799999999997, "end": 3005.2799999999997, "text": " a bunch of possibilities, and I'm saying those possibilities have to come from somewhere,", "tokens": [50900, 257, 3840, 295, 12178, 11, 293, 286, 478, 1566, 729, 12178, 362, 281, 808, 490, 4079, 11, 51120], "temperature": 0.0, "avg_logprob": -0.13672690832314371, "compression_ratio": 1.9311740890688258, "no_speech_prob": 0.022257443517446518}, {"id": 431, "seek": 299016, "start": 3005.2799999999997, "end": 3010.56, "text": " and where they come from is abduction. Oh, okay, it depends on the domain and language. They come", "tokens": [51120, 293, 689, 436, 808, 490, 307, 410, 40335, 13, 876, 11, 1392, 11, 309, 5946, 322, 264, 9274, 293, 2856, 13, 814, 808, 51384], "temperature": 0.0, "avg_logprob": -0.13672690832314371, "compression_ratio": 1.9311740890688258, "no_speech_prob": 0.022257443517446518}, {"id": 432, "seek": 299016, "start": 3010.56, "end": 3016.3999999999996, "text": " from what we know is true. Okay, I see your point. Where they come from depends on the domain of", "tokens": [51384, 490, 437, 321, 458, 307, 2074, 13, 1033, 11, 286, 536, 428, 935, 13, 2305, 436, 808, 490, 5946, 322, 264, 9274, 295, 51676], "temperature": 0.0, "avg_logprob": -0.13672690832314371, "compression_ratio": 1.9311740890688258, "no_speech_prob": 0.022257443517446518}, {"id": 433, "seek": 301640, "start": 3016.48, "end": 3023.44, "text": " reasoning. In many cases, they come from what we know is true, or they come from evidence.", "tokens": [50368, 21577, 13, 682, 867, 3331, 11, 436, 808, 490, 437, 321, 458, 307, 2074, 11, 420, 436, 808, 490, 4467, 13, 50716], "temperature": 0.0, "avg_logprob": -0.146649440129598, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.005727675277739763}, {"id": 434, "seek": 301640, "start": 3024.56, "end": 3028.64, "text": " Yeah, I guess it's just important to know there are these two senses of abduction,", "tokens": [50772, 865, 11, 286, 2041, 309, 311, 445, 1021, 281, 458, 456, 366, 613, 732, 17057, 295, 410, 40335, 11, 50976], "temperature": 0.0, "avg_logprob": -0.146649440129598, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.005727675277739763}, {"id": 435, "seek": 301640, "start": 3029.44, "end": 3032.1600000000003, "text": " and don't forget about both of them, because they're both...", "tokens": [51016, 293, 500, 380, 2870, 466, 1293, 295, 552, 11, 570, 436, 434, 1293, 485, 51152], "temperature": 0.0, "avg_logprob": -0.146649440129598, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.005727675277739763}, {"id": 436, "seek": 301640, "start": 3032.96, "end": 3040.32, "text": " Right, and that's why abduction, like induction, as opposed to deduction, abduction and induction", "tokens": [51192, 1779, 11, 293, 300, 311, 983, 410, 40335, 11, 411, 33371, 11, 382, 8851, 281, 46385, 11, 410, 40335, 293, 33371, 51560], "temperature": 0.0, "avg_logprob": -0.146649440129598, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.005727675277739763}, {"id": 437, "seek": 301640, "start": 3040.32, "end": 3045.28, "text": " are both approximate. You can never have 100%, because in the end, you're assigning a score,", "tokens": [51560, 366, 1293, 30874, 13, 509, 393, 1128, 362, 2319, 8923, 570, 294, 264, 917, 11, 291, 434, 49602, 257, 6175, 11, 51808], "temperature": 0.0, "avg_logprob": -0.146649440129598, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.005727675277739763}, {"id": 438, "seek": 304528, "start": 3045.28, "end": 3052.0, "text": " you're saying. So, both of them are probabilistic in a way, or they have a certain uncertainty.", "tokens": [50364, 291, 434, 1566, 13, 407, 11, 1293, 295, 552, 366, 31959, 3142, 294, 257, 636, 11, 420, 436, 362, 257, 1629, 15697, 13, 50700], "temperature": 0.0, "avg_logprob": -0.18834780683421126, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.0007668304024264216}, {"id": 439, "seek": 304528, "start": 3052.96, "end": 3058.5600000000004, "text": " So, when you're doing abductive reasoning, even in language, I make a decision as this is the right", "tokens": [50748, 407, 11, 562, 291, 434, 884, 46465, 488, 21577, 11, 754, 294, 2856, 11, 286, 652, 257, 3537, 382, 341, 307, 264, 558, 51028], "temperature": 0.0, "avg_logprob": -0.18834780683421126, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.0007668304024264216}, {"id": 440, "seek": 304528, "start": 3058.5600000000004, "end": 3063.92, "text": " interpretation given the context, but it's what we call... Could be wrong. You might have eaten", "tokens": [51028, 14174, 2212, 264, 4319, 11, 457, 309, 311, 437, 321, 818, 485, 7497, 312, 2085, 13, 509, 1062, 362, 12158, 51296], "temperature": 0.0, "avg_logprob": -0.18834780683421126, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.0007668304024264216}, {"id": 441, "seek": 304528, "start": 3063.92, "end": 3068.4, "text": " a ball out of this all year. Exactly, and that's why when I read further, I change my first", "tokens": [51296, 257, 2594, 484, 295, 341, 439, 1064, 13, 7587, 11, 293, 300, 311, 983, 562, 286, 1401, 3052, 11, 286, 1319, 452, 700, 51520], "temperature": 0.0, "avg_logprob": -0.18834780683421126, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.0007668304024264216}, {"id": 442, "seek": 306840, "start": 3068.4, "end": 3077.6, "text": " interpretation. In language, it's not monotonic, actually. We do non-monotonic reasoning in the", "tokens": [50364, 14174, 13, 682, 2856, 11, 309, 311, 406, 1108, 310, 11630, 11, 767, 13, 492, 360, 2107, 12, 3317, 310, 11630, 21577, 294, 264, 50824], "temperature": 0.0, "avg_logprob": -0.10755472713046604, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.09900844842195511}, {"id": 443, "seek": 306840, "start": 3077.6, "end": 3087.28, "text": " sense that I might override my first decision. But all of that is pragmatics, and we do this", "tokens": [50824, 2020, 300, 286, 1062, 42321, 452, 700, 3537, 13, 583, 439, 295, 300, 307, 33394, 15677, 1167, 11, 293, 321, 360, 341, 51308], "temperature": 0.0, "avg_logprob": -0.10755472713046604, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.09900844842195511}, {"id": 444, "seek": 306840, "start": 3087.28, "end": 3092.88, "text": " in conversation. Two, three sentences after, I understand really fully what you said before,", "tokens": [51308, 294, 3761, 13, 4453, 11, 1045, 16579, 934, 11, 286, 1223, 534, 4498, 437, 291, 848, 949, 11, 51588], "temperature": 0.0, "avg_logprob": -0.10755472713046604, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.09900844842195511}, {"id": 445, "seek": 309288, "start": 3092.88, "end": 3098.7200000000003, "text": " because I remade the interpretation. And Waleed, do you have any thoughts on where", "tokens": [50364, 570, 286, 890, 762, 264, 14174, 13, 400, 343, 1220, 292, 11, 360, 291, 362, 604, 4598, 322, 689, 50656], "temperature": 0.0, "avg_logprob": -0.24874606999483975, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.002142261015251279}, {"id": 446, "seek": 309288, "start": 3099.76, "end": 3104.48, "text": " this came from, or basically the evolution of language, or if you like the evolution of this", "tokens": [50708, 341, 1361, 490, 11, 420, 1936, 264, 9303, 295, 2856, 11, 420, 498, 291, 411, 264, 9303, 295, 341, 50944], "temperature": 0.0, "avg_logprob": -0.24874606999483975, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.002142261015251279}, {"id": 447, "seek": 309288, "start": 3104.48, "end": 3111.84, "text": " abductive athlete? Do you have any ideas, or is it unique to humans? It seems it is.", "tokens": [50944, 46465, 488, 18002, 30, 1144, 291, 362, 604, 3487, 11, 420, 307, 309, 3845, 281, 6255, 30, 467, 2544, 309, 307, 13, 51312], "temperature": 0.0, "avg_logprob": -0.24874606999483975, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.002142261015251279}, {"id": 448, "seek": 309288, "start": 3113.84, "end": 3119.92, "text": " Unique to humans, definitely. I mean, animal language, animal symbolic languages have been", "tokens": [51412, 1156, 1925, 281, 6255, 11, 2138, 13, 286, 914, 11, 5496, 2856, 11, 5496, 25755, 8650, 362, 668, 51716], "temperature": 0.0, "avg_logprob": -0.24874606999483975, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.002142261015251279}, {"id": 449, "seek": 311992, "start": 3119.92, "end": 3126.16, "text": " studied thoroughly. And two things, here's where the genius of photo comes in, productivity. I mean,", "tokens": [50364, 9454, 17987, 13, 400, 732, 721, 11, 510, 311, 689, 264, 14017, 295, 5052, 1487, 294, 11, 15604, 13, 286, 914, 11, 50676], "temperature": 0.0, "avg_logprob": -0.163061956341347, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.008696393109858036}, {"id": 450, "seek": 311992, "start": 3126.7200000000003, "end": 3131.52, "text": " language have a finite set of symbols, and they're not productive. They don't do compositions.", "tokens": [50704, 2856, 362, 257, 19362, 992, 295, 16944, 11, 293, 436, 434, 406, 13304, 13, 814, 500, 380, 360, 43401, 13, 50944], "temperature": 0.0, "avg_logprob": -0.163061956341347, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.008696393109858036}, {"id": 451, "seek": 311992, "start": 3133.92, "end": 3137.36, "text": " And this ties to... Is it animals? Your time up?", "tokens": [51064, 400, 341, 14039, 281, 485, 1119, 309, 4882, 30, 2260, 565, 493, 30, 51236], "temperature": 0.0, "avg_logprob": -0.163061956341347, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.008696393109858036}, {"id": 452, "seek": 311992, "start": 3139.84, "end": 3147.6800000000003, "text": " No animal, no non-human animal has a productive language. In other words, I have a set of symbols,", "tokens": [51360, 883, 5496, 11, 572, 2107, 12, 18796, 5496, 575, 257, 13304, 2856, 13, 682, 661, 2283, 11, 286, 362, 257, 992, 295, 16944, 11, 51752], "temperature": 0.0, "avg_logprob": -0.163061956341347, "compression_ratio": 1.6103286384976525, "no_speech_prob": 0.008696393109858036}, {"id": 453, "seek": 314768, "start": 3148.3199999999997, "end": 3154.72, "text": " and if I can compose them, I can make a new symbol. Language, animals don't compose things,", "tokens": [50396, 293, 498, 286, 393, 35925, 552, 11, 286, 393, 652, 257, 777, 5986, 13, 24445, 11, 4882, 500, 380, 35925, 721, 11, 50716], "temperature": 0.0, "avg_logprob": -0.11136610412597656, "compression_ratio": 1.849802371541502, "no_speech_prob": 0.0026712510734796524}, {"id": 454, "seek": 314768, "start": 3154.72, "end": 3159.04, "text": " because they don't decompose them when they're done. They have a finite... It's a hash table.", "tokens": [50716, 570, 436, 500, 380, 22867, 541, 552, 562, 436, 434, 1096, 13, 814, 362, 257, 19362, 485, 467, 311, 257, 22019, 3199, 13, 50932], "temperature": 0.0, "avg_logprob": -0.11136610412597656, "compression_ratio": 1.849802371541502, "no_speech_prob": 0.0026712510734796524}, {"id": 455, "seek": 314768, "start": 3159.04, "end": 3163.7599999999998, "text": " If I make this symbol, I mean this. If I make this... Okay, no matter how sophisticated it is,", "tokens": [50932, 759, 286, 652, 341, 5986, 11, 286, 914, 341, 13, 759, 286, 652, 341, 485, 1033, 11, 572, 1871, 577, 16950, 309, 307, 11, 51168], "temperature": 0.0, "avg_logprob": -0.11136610412597656, "compression_ratio": 1.849802371541502, "no_speech_prob": 0.0026712510734796524}, {"id": 456, "seek": 314768, "start": 3164.56, "end": 3168.8799999999997, "text": " because they don't have recursion, they don't have infinity, they can't deal at that level", "tokens": [51208, 570, 436, 500, 380, 362, 20560, 313, 11, 436, 500, 380, 362, 13202, 11, 436, 393, 380, 2028, 412, 300, 1496, 51424], "temperature": 0.0, "avg_logprob": -0.11136610412597656, "compression_ratio": 1.849802371541502, "no_speech_prob": 0.0026712510734796524}, {"id": 457, "seek": 314768, "start": 3168.8799999999997, "end": 3174.96, "text": " with complexity. Some of them have a larger lexicon than others. Okay, but that's still the same", "tokens": [51424, 365, 14024, 13, 2188, 295, 552, 362, 257, 4833, 476, 87, 11911, 813, 2357, 13, 1033, 11, 457, 300, 311, 920, 264, 912, 51728], "temperature": 0.0, "avg_logprob": -0.11136610412597656, "compression_ratio": 1.849802371541502, "no_speech_prob": 0.0026712510734796524}, {"id": 458, "seek": 317496, "start": 3174.96, "end": 3182.7200000000003, "text": " paradigm. So, productivity, in other words, this capacity to learn, we were just talking about John,", "tokens": [50364, 24709, 13, 407, 11, 15604, 11, 294, 661, 2283, 11, 341, 6042, 281, 1466, 11, 321, 645, 445, 1417, 466, 2619, 11, 50752], "temperature": 0.0, "avg_logprob": -0.12456035614013672, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0008423473336733878}, {"id": 459, "seek": 317496, "start": 3182.7200000000003, "end": 3189.84, "text": " or the neighbor next door, or the neighbor next door that just came from California. I can", "tokens": [50752, 420, 264, 5987, 958, 2853, 11, 420, 264, 5987, 958, 2853, 300, 445, 1361, 490, 5384, 13, 286, 393, 51108], "temperature": 0.0, "avg_logprob": -0.12456035614013672, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0008423473336733878}, {"id": 460, "seek": 317496, "start": 3189.84, "end": 3197.12, "text": " productively make a person out of three sentences, and in the end, they collapse to a John, right?", "tokens": [51108, 1674, 3413, 652, 257, 954, 484, 295, 1045, 16579, 11, 293, 294, 264, 917, 11, 436, 15584, 281, 257, 2619, 11, 558, 30, 51472], "temperature": 0.0, "avg_logprob": -0.12456035614013672, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0008423473336733878}, {"id": 461, "seek": 317496, "start": 3197.12, "end": 3203.6, "text": " That productivity doesn't exist in any species except humans, which means compositionality,", "tokens": [51472, 663, 15604, 1177, 380, 2514, 294, 604, 6172, 3993, 6255, 11, 597, 1355, 12686, 1860, 11, 51796], "temperature": 0.0, "avg_logprob": -0.12456035614013672, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0008423473336733878}, {"id": 462, "seek": 320360, "start": 3203.6, "end": 3210.64, "text": " which means systematicity, which means all of that. So, it's unique to human, definitely. This", "tokens": [50364, 597, 1355, 27249, 507, 11, 597, 1355, 439, 295, 300, 13, 407, 11, 309, 311, 3845, 281, 1952, 11, 2138, 13, 639, 50716], "temperature": 0.0, "avg_logprob": -0.10233190436112254, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0002736802853178233}, {"id": 463, "seek": 320360, "start": 3210.64, "end": 3216.72, "text": " has been established, and it came with thought. That's the if and only if. That's why we're the", "tokens": [50716, 575, 668, 7545, 11, 293, 309, 1361, 365, 1194, 13, 663, 311, 264, 498, 293, 787, 498, 13, 663, 311, 983, 321, 434, 264, 51020], "temperature": 0.0, "avg_logprob": -0.10233190436112254, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0002736802853178233}, {"id": 464, "seek": 320360, "start": 3216.72, "end": 3221.7599999999998, "text": " only species that really reason. I mean, okay, I have people insist that animals think and they", "tokens": [51020, 787, 6172, 300, 534, 1778, 13, 286, 914, 11, 1392, 11, 286, 362, 561, 13466, 300, 4882, 519, 293, 436, 51272], "temperature": 0.0, "avg_logprob": -0.10233190436112254, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0002736802853178233}, {"id": 465, "seek": 320360, "start": 3221.7599999999998, "end": 3228.48, "text": " reason. They're not really reasoning, okay? Only humans reason, and thought and language came", "tokens": [51272, 1778, 13, 814, 434, 406, 534, 21577, 11, 1392, 30, 5686, 6255, 1778, 11, 293, 1194, 293, 2856, 1361, 51608], "temperature": 0.0, "avg_logprob": -0.10233190436112254, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0002736802853178233}, {"id": 466, "seek": 322848, "start": 3228.56, "end": 3234.96, "text": " together. It's sort of like a phenomenon. There are some, there's some proof, even anthropologists,", "tokens": [50368, 1214, 13, 467, 311, 1333, 295, 411, 257, 14029, 13, 821, 366, 512, 11, 456, 311, 512, 8177, 11, 754, 22727, 12256, 11, 50688], "temperature": 0.0, "avg_logprob": -0.12662417548043386, "compression_ratio": 1.572192513368984, "no_speech_prob": 0.0032634632661938667}, {"id": 467, "seek": 322848, "start": 3234.96, "end": 3246.48, "text": " and they say it looks like language was detected when tools and some basic machinery was detected", "tokens": [50688, 293, 436, 584, 309, 1542, 411, 2856, 390, 21896, 562, 3873, 293, 512, 3875, 27302, 390, 21896, 51264], "temperature": 0.0, "avg_logprob": -0.12662417548043386, "compression_ratio": 1.572192513368984, "no_speech_prob": 0.0032634632661938667}, {"id": 468, "seek": 322848, "start": 3246.48, "end": 3255.2, "text": " first. So, the human mind at some point had this capacity to think and language came with it. It", "tokens": [51264, 700, 13, 407, 11, 264, 1952, 1575, 412, 512, 935, 632, 341, 6042, 281, 519, 293, 2856, 1361, 365, 309, 13, 467, 51700], "temperature": 0.0, "avg_logprob": -0.12662417548043386, "compression_ratio": 1.572192513368984, "no_speech_prob": 0.0032634632661938667}, {"id": 469, "seek": 325520, "start": 3255.2, "end": 3261.9199999999996, "text": " was like almost at the same time. So, it's uniquely human, definitely now. Where did it come from?", "tokens": [50364, 390, 411, 1920, 412, 264, 912, 565, 13, 407, 11, 309, 311, 31474, 1952, 11, 2138, 586, 13, 2305, 630, 309, 808, 490, 30, 50700], "temperature": 0.0, "avg_logprob": -0.09560096263885498, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.0003458465216681361}, {"id": 470, "seek": 325520, "start": 3263.4399999999996, "end": 3277.4399999999996, "text": " Wow. I think it was the need really to express thoughts. Like at some point, we started having", "tokens": [50776, 3153, 13, 286, 519, 309, 390, 264, 643, 534, 281, 5109, 4598, 13, 1743, 412, 512, 935, 11, 321, 1409, 1419, 51476], "temperature": 0.0, "avg_logprob": -0.09560096263885498, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.0003458465216681361}, {"id": 471, "seek": 325520, "start": 3277.4399999999996, "end": 3284.3199999999997, "text": " thoughts that we want to communicate. So, the external artifact we see outside, whether it's", "tokens": [51476, 4598, 300, 321, 528, 281, 7890, 13, 407, 11, 264, 8320, 34806, 321, 536, 2380, 11, 1968, 309, 311, 51820], "temperature": 0.0, "avg_logprob": -0.09560096263885498, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.0003458465216681361}, {"id": 472, "seek": 328432, "start": 3284.4, "end": 3293.84, "text": " English or ancient Greek or Latin, languages evolve for societal reason and all that. But the", "tokens": [50368, 3669, 420, 7832, 10281, 420, 10803, 11, 8650, 16693, 337, 33472, 1778, 293, 439, 300, 13, 583, 264, 50840], "temperature": 0.0, "avg_logprob": -0.18514204025268555, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.0004709700879175216}, {"id": 473, "seek": 328432, "start": 3293.84, "end": 3301.84, "text": " external artifact that we use to communicate thoughts came out of the need of the internal", "tokens": [50840, 8320, 34806, 300, 321, 764, 281, 7890, 4598, 1361, 484, 295, 264, 643, 295, 264, 6920, 51240], "temperature": 0.0, "avg_logprob": -0.18514204025268555, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.0004709700879175216}, {"id": 474, "seek": 328432, "start": 3301.84, "end": 3308.6400000000003, "text": " language that started to develop. What Fodor calls it, the language of thought, mental ease.", "tokens": [51240, 2856, 300, 1409, 281, 1499, 13, 708, 479, 34024, 5498, 309, 11, 264, 2856, 295, 1194, 11, 4973, 12708, 13, 51580], "temperature": 0.0, "avg_logprob": -0.18514204025268555, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.0004709700879175216}, {"id": 475, "seek": 330864, "start": 3309.6, "end": 3317.92, "text": " And we, so we had that thing going on inside and then we had to communicate. We started with", "tokens": [50412, 400, 321, 11, 370, 321, 632, 300, 551, 516, 322, 1854, 293, 550, 321, 632, 281, 7890, 13, 492, 1409, 365, 50828], "temperature": 0.0, "avg_logprob": -0.15484784354626294, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009847591863945127}, {"id": 476, "seek": 330864, "start": 3317.92, "end": 3324.48, "text": " weird sounds and then we scribbled things on the wall to communicate. And then that thing developed", "tokens": [50828, 3657, 3263, 293, 550, 321, 39435, 18320, 721, 322, 264, 2929, 281, 7890, 13, 400, 550, 300, 551, 4743, 51156], "temperature": 0.0, "avg_logprob": -0.15484784354626294, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009847591863945127}, {"id": 477, "seek": 330864, "start": 3324.48, "end": 3332.08, "text": " until we started making symbols like, okay, if I say this, that means this. I don't know the", "tokens": [51156, 1826, 321, 1409, 1455, 16944, 411, 11, 1392, 11, 498, 286, 584, 341, 11, 300, 1355, 341, 13, 286, 500, 380, 458, 264, 51536], "temperature": 0.0, "avg_logprob": -0.15484784354626294, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009847591863945127}, {"id": 478, "seek": 333208, "start": 3332.08, "end": 3340.16, "text": " exact process. I'm not a biologist or evolutionary linguist or, but I think thought is the key here.", "tokens": [50364, 1900, 1399, 13, 286, 478, 406, 257, 3228, 9201, 420, 27567, 21766, 468, 420, 11, 457, 286, 519, 1194, 307, 264, 2141, 510, 13, 50768], "temperature": 0.0, "avg_logprob": -0.11696120195610579, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.003645456861704588}, {"id": 479, "seek": 333208, "start": 3340.16, "end": 3346.88, "text": " So, there's a language of thought. And these external things are because linguistic research", "tokens": [50768, 407, 11, 456, 311, 257, 2856, 295, 1194, 13, 400, 613, 8320, 721, 366, 570, 43002, 2132, 51104], "temperature": 0.0, "avg_logprob": -0.11696120195610579, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.003645456861704588}, {"id": 480, "seek": 333208, "start": 3346.88, "end": 3352.64, "text": " has also shown that there are many universals in language, regardless of what the language is,", "tokens": [51104, 575, 611, 4898, 300, 456, 366, 867, 5950, 1124, 294, 2856, 11, 10060, 295, 437, 264, 2856, 307, 11, 51392], "temperature": 0.0, "avg_logprob": -0.11696120195610579, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.003645456861704588}, {"id": 481, "seek": 333208, "start": 3352.64, "end": 3358.96, "text": " even if they are completely different systems like Asian languages and Latin-based languages.", "tokens": [51392, 754, 498, 436, 366, 2584, 819, 3652, 411, 10645, 8650, 293, 10803, 12, 6032, 8650, 13, 51708], "temperature": 0.0, "avg_logprob": -0.11696120195610579, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.003645456861704588}, {"id": 482, "seek": 335896, "start": 3359.52, "end": 3367.04, "text": " They all have a verb, an action. They all have objects and agents of the action.", "tokens": [50392, 814, 439, 362, 257, 9595, 11, 364, 3069, 13, 814, 439, 362, 6565, 293, 12554, 295, 264, 3069, 13, 50768], "temperature": 0.0, "avg_logprob": -0.11461827921312909, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.0011502340203151107}, {"id": 483, "seek": 335896, "start": 3367.84, "end": 3374.8, "text": " They all have events and events have duration, time and place. So, there are a set of cognitive,", "tokens": [50808, 814, 439, 362, 3931, 293, 3931, 362, 16365, 11, 565, 293, 1081, 13, 407, 11, 456, 366, 257, 992, 295, 15605, 11, 51156], "temperature": 0.0, "avg_logprob": -0.11461827921312909, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.0011502340203151107}, {"id": 484, "seek": 335896, "start": 3374.8, "end": 3381.76, "text": " I call them universal cognitive primitives, right? There's always an object there somewhere,", "tokens": [51156, 286, 818, 552, 11455, 15605, 2886, 38970, 11, 558, 30, 821, 311, 1009, 364, 2657, 456, 4079, 11, 51504], "temperature": 0.0, "avg_logprob": -0.11461827921312909, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.0011502340203151107}, {"id": 485, "seek": 335896, "start": 3381.76, "end": 3386.2400000000002, "text": " or an agent of an activity. Now, how you express it in different languages,", "tokens": [51504, 420, 364, 9461, 295, 364, 5191, 13, 823, 11, 577, 291, 5109, 309, 294, 819, 8650, 11, 51728], "temperature": 0.0, "avg_logprob": -0.11461827921312909, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.0011502340203151107}, {"id": 486, "seek": 338624, "start": 3386.72, "end": 3393.04, "text": " these are universals. That's the language of thought. That's the internal language,", "tokens": [50388, 613, 366, 5950, 1124, 13, 663, 311, 264, 2856, 295, 1194, 13, 663, 311, 264, 6920, 2856, 11, 50704], "temperature": 0.0, "avg_logprob": -0.1677888956936923, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.001752167008817196}, {"id": 487, "seek": 338624, "start": 3393.04, "end": 3400.24, "text": " which has to be the same. And objects have properties and all that. So, there are universal", "tokens": [50704, 597, 575, 281, 312, 264, 912, 13, 400, 6565, 362, 7221, 293, 439, 300, 13, 407, 11, 456, 366, 11455, 51064], "temperature": 0.0, "avg_logprob": -0.1677888956936923, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.001752167008817196}, {"id": 488, "seek": 338624, "start": 3400.24, "end": 3409.12, "text": " primitives. And we instantiate them in different languages differently, but that's to me secondary.", "tokens": [51064, 2886, 38970, 13, 400, 321, 9836, 13024, 552, 294, 819, 8650, 7614, 11, 457, 300, 311, 281, 385, 11396, 13, 51508], "temperature": 0.0, "avg_logprob": -0.1677888956936923, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.001752167008817196}, {"id": 489, "seek": 338624, "start": 3409.8399999999997, "end": 3414.64, "text": " Okay. Okay. That's great. William, could you talk a little bit about your recent overview?", "tokens": [51544, 1033, 13, 1033, 13, 663, 311, 869, 13, 6740, 11, 727, 291, 751, 257, 707, 857, 466, 428, 5162, 12492, 30, 51784], "temperature": 0.0, "avg_logprob": -0.1677888956936923, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.001752167008817196}, {"id": 490, "seek": 341624, "start": 3416.56, "end": 3425.8399999999997, "text": " A colleague that I never worked with, but a colleague in the field. To review this book,", "tokens": [50380, 316, 13532, 300, 286, 1128, 2732, 365, 11, 457, 257, 13532, 294, 264, 2519, 13, 1407, 3131, 341, 1446, 11, 50844], "temperature": 0.0, "avg_logprob": -0.11881374407418166, "compression_ratio": 1.5028901734104045, "no_speech_prob": 0.025143027305603027}, {"id": 491, "seek": 341624, "start": 3425.8399999999997, "end": 3431.04, "text": " and I looked at it and I said, oh, I have enough on my plate. This is not an easy book.", "tokens": [50844, 293, 286, 2956, 412, 309, 293, 286, 848, 11, 1954, 11, 286, 362, 1547, 322, 452, 5924, 13, 639, 307, 406, 364, 1858, 1446, 13, 51104], "temperature": 0.0, "avg_logprob": -0.11881374407418166, "compression_ratio": 1.5028901734104045, "no_speech_prob": 0.025143027305603027}, {"id": 492, "seek": 341624, "start": 3433.2799999999997, "end": 3438.8799999999997, "text": " But then I, because I liked it, I said, yeah, I'd like to write it. And in the end,", "tokens": [51216, 583, 550, 286, 11, 570, 286, 4501, 309, 11, 286, 848, 11, 1338, 11, 286, 1116, 411, 281, 2464, 309, 13, 400, 294, 264, 917, 11, 51496], "temperature": 0.0, "avg_logprob": -0.11881374407418166, "compression_ratio": 1.5028901734104045, "no_speech_prob": 0.025143027305603027}, {"id": 493, "seek": 343888, "start": 3438.88, "end": 3445.92, "text": " it turned out to be not as technically involved as I thought. It's sort of,", "tokens": [50364, 309, 3574, 484, 281, 312, 406, 382, 12120, 3288, 382, 286, 1194, 13, 467, 311, 1333, 295, 11, 50716], "temperature": 0.0, "avg_logprob": -0.10129803960973566, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.0013040528865531087}, {"id": 494, "seek": 343888, "start": 3447.2000000000003, "end": 3452.4, "text": " and I'm saying that not to be negative, but it's sort of the same argument over and over.", "tokens": [50780, 293, 286, 478, 1566, 300, 406, 281, 312, 3671, 11, 457, 309, 311, 1333, 295, 264, 912, 6770, 670, 293, 670, 13, 51040], "temperature": 0.0, "avg_logprob": -0.10129803960973566, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.0013040528865531087}, {"id": 495, "seek": 343888, "start": 3452.4, "end": 3458.96, "text": " The gist of the argument is quite simple, actually. And they try to prove it from different vantage", "tokens": [51040, 440, 290, 468, 295, 264, 6770, 307, 1596, 2199, 11, 767, 13, 400, 436, 853, 281, 7081, 309, 490, 819, 46206, 51368], "temperature": 0.0, "avg_logprob": -0.10129803960973566, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.0013040528865531087}, {"id": 496, "seek": 343888, "start": 3458.96, "end": 3463.92, "text": " points than in the book, from a biological, sociological, psychological, mathematical.", "tokens": [51368, 2793, 813, 294, 264, 1446, 11, 490, 257, 13910, 11, 3075, 4383, 11, 14346, 11, 18894, 13, 51616], "temperature": 0.0, "avg_logprob": -0.10129803960973566, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.0013040528865531087}, {"id": 497, "seek": 346392, "start": 3463.92, "end": 3474.64, "text": " But the gist of the book is any talk of AGI is wishful thinking. And it's beyond anything we", "tokens": [50364, 583, 264, 290, 468, 295, 264, 1446, 307, 604, 751, 295, 316, 26252, 307, 3172, 906, 1953, 13, 400, 309, 311, 4399, 1340, 321, 50900], "temperature": 0.0, "avg_logprob": -0.12598926837627705, "compression_ratio": 1.445054945054945, "no_speech_prob": 0.0008551097707822919}, {"id": 498, "seek": 346392, "start": 3474.64, "end": 3484.8, "text": " can ever develop mathematically, so as to engineer it in any, in any realistic way.", "tokens": [50900, 393, 1562, 1499, 44003, 11, 370, 382, 281, 11403, 309, 294, 604, 11, 294, 604, 12465, 636, 13, 51408], "temperature": 0.0, "avg_logprob": -0.12598926837627705, "compression_ratio": 1.445054945054945, "no_speech_prob": 0.0008551097707822919}, {"id": 499, "seek": 346392, "start": 3486.16, "end": 3493.12, "text": " They make good arguments throughout. There are many examples of the basic idea is that", "tokens": [51476, 814, 652, 665, 12869, 3710, 13, 821, 366, 867, 5110, 295, 264, 3875, 1558, 307, 300, 51824], "temperature": 0.0, "avg_logprob": -0.12598926837627705, "compression_ratio": 1.445054945054945, "no_speech_prob": 0.0008551097707822919}, {"id": 500, "seek": 349392, "start": 3494.88, "end": 3502.48, "text": " all the mathematics we know, right, mathematics available to us, cannot model", "tokens": [50412, 439, 264, 18666, 321, 458, 11, 558, 11, 18666, 2435, 281, 505, 11, 2644, 2316, 50792], "temperature": 0.0, "avg_logprob": -0.16660237716416182, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0002610496594570577}, {"id": 501, "seek": 349392, "start": 3504.48, "end": 3511.2000000000003, "text": " not just the entire mind, but even subsystems in the mind, language being one of them.", "tokens": [50892, 406, 445, 264, 2302, 1575, 11, 457, 754, 2090, 9321, 82, 294, 264, 1575, 11, 2856, 885, 472, 295, 552, 13, 51228], "temperature": 0.0, "avg_logprob": -0.16660237716416182, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0002610496594570577}, {"id": 502, "seek": 349392, "start": 3513.36, "end": 3518.8, "text": " And so it's all complex systems within complex systems in a complex environment,", "tokens": [51336, 400, 370, 309, 311, 439, 3997, 3652, 1951, 3997, 3652, 294, 257, 3997, 2823, 11, 51608], "temperature": 0.0, "avg_logprob": -0.16660237716416182, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0002610496594570577}, {"id": 503, "seek": 351880, "start": 3519.28, "end": 3525.36, "text": " the system around us that we interact with. And none of it can be modeled mathematically,", "tokens": [50388, 264, 1185, 926, 505, 300, 321, 4648, 365, 13, 400, 6022, 295, 309, 393, 312, 37140, 44003, 11, 50692], "temperature": 0.0, "avg_logprob": -0.10463093329167021, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0008412786992266774}, {"id": 504, "seek": 351880, "start": 3525.36, "end": 3532.32, "text": " none of it even at any level. So forget doing AGI that can interact with us in an intelligent way.", "tokens": [50692, 6022, 295, 309, 754, 412, 604, 1496, 13, 407, 2870, 884, 316, 26252, 300, 393, 4648, 365, 505, 294, 364, 13232, 636, 13, 51040], "temperature": 0.0, "avg_logprob": -0.10463093329167021, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0008412786992266774}, {"id": 505, "seek": 351880, "start": 3533.28, "end": 3540.8, "text": " Now, you can do controlled narrow AI, right? You can build very intelligent machines that can do", "tokens": [51088, 823, 11, 291, 393, 360, 10164, 9432, 7318, 11, 558, 30, 509, 393, 1322, 588, 13232, 8379, 300, 393, 360, 51464], "temperature": 0.0, "avg_logprob": -0.10463093329167021, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0008412786992266774}, {"id": 506, "seek": 354080, "start": 3540.88, "end": 3550.48, "text": " amazing stuff. But any talk of AGI, strong AI, is just talk until, unless, and they admit that,", "tokens": [50368, 2243, 1507, 13, 583, 604, 751, 295, 316, 26252, 11, 2068, 7318, 11, 307, 445, 751, 1826, 11, 5969, 11, 293, 436, 9796, 300, 11, 50848], "temperature": 0.0, "avg_logprob": -0.13838873682795344, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0034153193701058626}, {"id": 507, "seek": 354080, "start": 3550.48, "end": 3559.1200000000003, "text": " unless we come up with a new mathematics that we never even knew at the scale of Leibniz calculus", "tokens": [50848, 5969, 321, 808, 493, 365, 257, 777, 18666, 300, 321, 1128, 754, 2586, 412, 264, 4373, 295, 1456, 897, 77, 590, 33400, 51280], "temperature": 0.0, "avg_logprob": -0.13838873682795344, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0034153193701058626}, {"id": 508, "seek": 354080, "start": 3559.1200000000003, "end": 3565.28, "text": " or Newton, like we're talking about a new mathematics that we never conceived of, right?", "tokens": [51280, 420, 19541, 11, 411, 321, 434, 1417, 466, 257, 777, 18666, 300, 321, 1128, 34898, 295, 11, 558, 30, 51588], "temperature": 0.0, "avg_logprob": -0.13838873682795344, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0034153193701058626}, {"id": 509, "seek": 356528, "start": 3565.6000000000004, "end": 3573.2000000000003, "text": " Which they say most likely all evidence says that's not going to happen, right? So", "tokens": [50380, 3013, 436, 584, 881, 3700, 439, 4467, 1619, 300, 311, 406, 516, 281, 1051, 11, 558, 30, 407, 50760], "temperature": 0.0, "avg_logprob": -0.19096060202155316, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.00037400983273983}, {"id": 510, "seek": 356528, "start": 3574.5600000000004, "end": 3583.44, "text": " now you can get into why. So that's their claim. And why? They say that all these systems are", "tokens": [50828, 586, 291, 393, 483, 666, 983, 13, 407, 300, 311, 641, 3932, 13, 400, 983, 30, 814, 584, 300, 439, 613, 3652, 366, 51272], "temperature": 0.0, "avg_logprob": -0.19096060202155316, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.00037400983273983}, {"id": 511, "seek": 356528, "start": 3583.44, "end": 3593.52, "text": " complex systems. And in complex systems, the idea is that these are, first of all, dynamic systems.", "tokens": [51272, 3997, 3652, 13, 400, 294, 3997, 3652, 11, 264, 1558, 307, 300, 613, 366, 11, 700, 295, 439, 11, 8546, 3652, 13, 51776], "temperature": 0.0, "avg_logprob": -0.19096060202155316, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.00037400983273983}, {"id": 512, "seek": 359352, "start": 3593.52, "end": 3601.68, "text": " They work in a dynamic environment. They are continuously evolving and adapting, right? They", "tokens": [50364, 814, 589, 294, 257, 8546, 2823, 13, 814, 366, 15684, 21085, 293, 34942, 11, 558, 30, 814, 50772], "temperature": 0.0, "avg_logprob": -0.1064556288340735, "compression_ratio": 1.7034883720930232, "no_speech_prob": 0.000310008559608832}, {"id": 513, "seek": 359352, "start": 3601.68, "end": 3609.6, "text": " are self feeding systems. These are not systems that only take input output. These systems change", "tokens": [50772, 366, 2698, 12919, 3652, 13, 1981, 366, 406, 3652, 300, 787, 747, 4846, 5598, 13, 1981, 3652, 1319, 51168], "temperature": 0.0, "avg_logprob": -0.1064556288340735, "compression_ratio": 1.7034883720930232, "no_speech_prob": 0.000310008559608832}, {"id": 514, "seek": 359352, "start": 3609.6, "end": 3616.08, "text": " their behavior. And I gave an example from list. These systems are systems that change their behavior,", "tokens": [51168, 641, 5223, 13, 400, 286, 2729, 364, 1365, 490, 1329, 13, 1981, 3652, 366, 3652, 300, 1319, 641, 5223, 11, 51492], "temperature": 0.0, "avg_logprob": -0.1064556288340735, "compression_ratio": 1.7034883720930232, "no_speech_prob": 0.000310008559608832}, {"id": 515, "seek": 361608, "start": 3616.08, "end": 3624.64, "text": " their algorithms, if you want, they change their mind from a stimulus. So I might, and that's why", "tokens": [50364, 641, 14642, 11, 498, 291, 528, 11, 436, 1319, 641, 1575, 490, 257, 21366, 13, 407, 286, 1062, 11, 293, 300, 311, 983, 50792], "temperature": 0.0, "avg_logprob": -0.1088576528761122, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.004067697562277317}, {"id": 516, "seek": 361608, "start": 3624.64, "end": 3629.6, "text": " I said they, I would have liked to see a discussion on the frame problem, because the frame problem", "tokens": [50792, 286, 848, 436, 11, 286, 576, 362, 4501, 281, 536, 257, 5017, 322, 264, 3920, 1154, 11, 570, 264, 3920, 1154, 51040], "temperature": 0.0, "avg_logprob": -0.1088576528761122, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.004067697562277317}, {"id": 517, "seek": 361608, "start": 3629.6, "end": 3638.48, "text": " in the AI is about this. How can I reason in a dynamic and uncertain environment and react", "tokens": [51040, 294, 264, 7318, 307, 466, 341, 13, 1012, 393, 286, 1778, 294, 257, 8546, 293, 11308, 2823, 293, 4515, 51484], "temperature": 0.0, "avg_logprob": -0.1088576528761122, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.004067697562277317}, {"id": 518, "seek": 361608, "start": 3639.04, "end": 3645.12, "text": " dynamically, although what I do in the environment might affect what I believe about the environment", "tokens": [51512, 43492, 11, 4878, 437, 286, 360, 294, 264, 2823, 1062, 3345, 437, 286, 1697, 466, 264, 2823, 51816], "temperature": 0.0, "avg_logprob": -0.1088576528761122, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.004067697562277317}, {"id": 519, "seek": 364512, "start": 3645.12, "end": 3650.48, "text": " in real time. And they're right. There is no mathematics we know of now. That's why we don't", "tokens": [50364, 294, 957, 565, 13, 400, 436, 434, 558, 13, 821, 307, 572, 18666, 321, 458, 295, 586, 13, 663, 311, 983, 321, 500, 380, 50632], "temperature": 0.0, "avg_logprob": -0.13772417990009436, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.00026932937907986343}, {"id": 520, "seek": 364512, "start": 3650.48, "end": 3657.44, "text": " have a solution for the frame problem. So this kind of cyclical cause and effect", "tokens": [50632, 362, 257, 3827, 337, 264, 3920, 1154, 13, 407, 341, 733, 295, 19474, 804, 3082, 293, 1802, 50980], "temperature": 0.0, "avg_logprob": -0.13772417990009436, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.00026932937907986343}, {"id": 521, "seek": 364512, "start": 3658.64, "end": 3662.64, "text": " cannot be modeled by anything we know on mathematics. And this I agree with them.", "tokens": [51040, 2644, 312, 37140, 538, 1340, 321, 458, 322, 18666, 13, 400, 341, 286, 3986, 365, 552, 13, 51240], "temperature": 0.0, "avg_logprob": -0.13772417990009436, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.00026932937907986343}, {"id": 522, "seek": 364512, "start": 3664.88, "end": 3670.16, "text": " They give an example. I made just an example in language, for example. Language, we know.", "tokens": [51352, 814, 976, 364, 1365, 13, 286, 1027, 445, 364, 1365, 294, 2856, 11, 337, 1365, 13, 24445, 11, 321, 458, 13, 51616], "temperature": 0.0, "avg_logprob": -0.13772417990009436, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.00026932937907986343}, {"id": 523, "seek": 367016, "start": 3670.72, "end": 3677.92, "text": " If I have a dialogue, okay, we all agree that the interpretation of any occurrence requires", "tokens": [50392, 759, 286, 362, 257, 10221, 11, 1392, 11, 321, 439, 3986, 300, 264, 14174, 295, 604, 36122, 7029, 50752], "temperature": 0.0, "avg_logprob": -0.1159416979009455, "compression_ratio": 1.6374269005847952, "no_speech_prob": 0.0004372153780423105}, {"id": 524, "seek": 367016, "start": 3677.92, "end": 3684.16, "text": " having the context in mind as part of the, part of the input to the evaluation of the meaning", "tokens": [50752, 1419, 264, 4319, 294, 1575, 382, 644, 295, 264, 11, 644, 295, 264, 4846, 281, 264, 13344, 295, 264, 3620, 51064], "temperature": 0.0, "avg_logprob": -0.1159416979009455, "compression_ratio": 1.6374269005847952, "no_speech_prob": 0.0004372153780423105}, {"id": 525, "seek": 367016, "start": 3684.16, "end": 3692.0, "text": " is the context as an extra parameter, right? Now, the context is changing based on something I", "tokens": [51064, 307, 264, 4319, 382, 364, 2857, 13075, 11, 558, 30, 823, 11, 264, 4319, 307, 4473, 2361, 322, 746, 286, 51456], "temperature": 0.0, "avg_logprob": -0.1159416979009455, "compression_ratio": 1.6374269005847952, "no_speech_prob": 0.0004372153780423105}, {"id": 526, "seek": 369200, "start": 3692.0, "end": 3700.48, "text": " cannot predict, which is the response of some participant in the dialogue. There is no meaningful", "tokens": [50364, 2644, 6069, 11, 597, 307, 264, 4134, 295, 512, 24950, 294, 264, 10221, 13, 821, 307, 572, 10995, 50788], "temperature": 0.0, "avg_logprob": -0.09736241490007883, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.005055475048720837}, {"id": 527, "seek": 369200, "start": 3700.48, "end": 3707.36, "text": " way of predicting how someone might respond. So in other words, the context is mathematically", "tokens": [50788, 636, 295, 32884, 577, 1580, 1062, 4196, 13, 407, 294, 661, 2283, 11, 264, 4319, 307, 44003, 51132], "temperature": 0.0, "avg_logprob": -0.09736241490007883, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.005055475048720837}, {"id": 528, "seek": 369200, "start": 3707.36, "end": 3712.56, "text": " not defined, but I need it in the interpretation. Thus, no language understanding, no language", "tokens": [51132, 406, 7642, 11, 457, 286, 643, 309, 294, 264, 14174, 13, 13827, 11, 572, 2856, 3701, 11, 572, 2856, 51392], "temperature": 0.0, "avg_logprob": -0.09736241490007883, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.005055475048720837}, {"id": 529, "seek": 369200, "start": 3712.56, "end": 3718.08, "text": " understanding, no AGI, because they believe language understanding is a prerequisite. So the,", "tokens": [51392, 3701, 11, 572, 316, 26252, 11, 570, 436, 1697, 2856, 3701, 307, 257, 38333, 34152, 13, 407, 264, 11, 51668], "temperature": 0.0, "avg_logprob": -0.09736241490007883, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.005055475048720837}, {"id": 530, "seek": 371808, "start": 3719.04, "end": 3724.96, "text": " their conclusion, I mean, you can question every step in this inference they come to,", "tokens": [50412, 641, 10063, 11, 286, 914, 11, 291, 393, 1168, 633, 1823, 294, 341, 38253, 436, 808, 281, 11, 50708], "temperature": 0.0, "avg_logprob": -0.12274269481281658, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0006354981451295316}, {"id": 531, "seek": 371808, "start": 3724.96, "end": 3729.6, "text": " but they give language as an example, but we have social behavior, I can give an example.", "tokens": [50708, 457, 436, 976, 2856, 382, 364, 1365, 11, 457, 321, 362, 2093, 5223, 11, 286, 393, 976, 364, 1365, 13, 50940], "temperature": 0.0, "avg_logprob": -0.12274269481281658, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0006354981451295316}, {"id": 532, "seek": 371808, "start": 3729.6, "end": 3735.44, "text": " They have a nice example in social behavior. Here's an example of a complex system that cannot,", "tokens": [50940, 814, 362, 257, 1481, 1365, 294, 2093, 5223, 13, 1692, 311, 364, 1365, 295, 257, 3997, 1185, 300, 2644, 11, 51232], "temperature": 0.0, "avg_logprob": -0.12274269481281658, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0006354981451295316}, {"id": 533, "seek": 371808, "start": 3736.16, "end": 3743.84, "text": " we don't have any mathematics that can model. We're staying in a queue, in a clinic, an emergency", "tokens": [51268, 321, 500, 380, 362, 604, 18666, 300, 393, 2316, 13, 492, 434, 7939, 294, 257, 18639, 11, 294, 257, 14947, 11, 364, 7473, 51652], "temperature": 0.0, "avg_logprob": -0.12274269481281658, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0006354981451295316}, {"id": 534, "seek": 374384, "start": 3744.8, "end": 3751.52, "text": " room. What do they call them? These ER. So, but there's a queue because they all have", "tokens": [50412, 1808, 13, 708, 360, 436, 818, 552, 30, 1981, 14929, 13, 407, 11, 457, 456, 311, 257, 18639, 570, 436, 439, 362, 50748], "temperature": 0.0, "avg_logprob": -0.1427594076229047, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0010480644414201379}, {"id": 535, "seek": 374384, "start": 3751.52, "end": 3759.28, "text": " emergencies, right? Now, the social behavior, then the social norm is that in the queue,", "tokens": [50748, 43483, 11, 558, 30, 823, 11, 264, 2093, 5223, 11, 550, 264, 2093, 2026, 307, 300, 294, 264, 18639, 11, 51136], "temperature": 0.0, "avg_logprob": -0.1427594076229047, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0010480644414201379}, {"id": 536, "seek": 374384, "start": 3759.28, "end": 3765.04, "text": " okay, we all have, we all have urgent issues. But in the end, I came first, right? Okay, so that's a", "tokens": [51136, 1392, 11, 321, 439, 362, 11, 321, 439, 362, 19022, 2663, 13, 583, 294, 264, 917, 11, 286, 1361, 700, 11, 558, 30, 1033, 11, 370, 300, 311, 257, 51424], "temperature": 0.0, "avg_logprob": -0.1427594076229047, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0010480644414201379}, {"id": 537, "seek": 376504, "start": 3765.04, "end": 3776.88, "text": " social norm. And, but can a robot understand that if someone fainted, really, I mean, it's almost", "tokens": [50364, 2093, 2026, 13, 400, 11, 457, 393, 257, 7881, 1223, 300, 498, 1580, 21104, 292, 11, 534, 11, 286, 914, 11, 309, 311, 1920, 50956], "temperature": 0.0, "avg_logprob": -0.1468657138300877, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.003941382747143507}, {"id": 538, "seek": 376504, "start": 3776.88, "end": 3788.16, "text": " gone, right? Our social norm accepts that this person violates the queue order, right? This is", "tokens": [50956, 2780, 11, 558, 30, 2621, 2093, 2026, 33538, 300, 341, 954, 3448, 1024, 264, 18639, 1668, 11, 558, 30, 639, 307, 51520], "temperature": 0.0, "avg_logprob": -0.1468657138300877, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.003941382747143507}, {"id": 539, "seek": 378816, "start": 3788.16, "end": 3796.64, "text": " something dynamic that happens, like the queue is this way. And how can a robot update the rules", "tokens": [50364, 746, 8546, 300, 2314, 11, 411, 264, 18639, 307, 341, 636, 13, 400, 577, 393, 257, 7881, 5623, 264, 4474, 50788], "temperature": 0.0, "avg_logprob": -0.14753027712361197, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.002842990215867758}, {"id": 540, "seek": 378816, "start": 3796.64, "end": 3802.72, "text": " and not kill someone because they violated the order of the queue? In other words, these interactions,", "tokens": [50788, 293, 406, 1961, 1580, 570, 436, 33239, 264, 1668, 295, 264, 18639, 30, 682, 661, 2283, 11, 613, 13280, 11, 51092], "temperature": 0.0, "avg_logprob": -0.14753027712361197, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.002842990215867758}, {"id": 541, "seek": 378816, "start": 3802.72, "end": 3810.64, "text": " these cyclical cause and effect are very complex, that no mathematical model. Or the example I said", "tokens": [51092, 613, 19474, 804, 3082, 293, 1802, 366, 588, 3997, 11, 300, 572, 18894, 2316, 13, 1610, 264, 1365, 286, 848, 51488], "temperature": 0.0, "avg_logprob": -0.14753027712361197, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.002842990215867758}, {"id": 542, "seek": 378816, "start": 3810.64, "end": 3818.0, "text": " in language, they prove this cannot be done. Context is needed to interpret everything. I cannot", "tokens": [51488, 294, 2856, 11, 436, 7081, 341, 2644, 312, 1096, 13, 4839, 3828, 307, 2978, 281, 7302, 1203, 13, 286, 2644, 51856], "temperature": 0.0, "avg_logprob": -0.14753027712361197, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.002842990215867758}, {"id": 543, "seek": 381816, "start": 3818.16, "end": 3824.96, "text": " predict what the context will be because I cannot predict you respond to my, so it's unpredictable,", "tokens": [50364, 6069, 437, 264, 4319, 486, 312, 570, 286, 2644, 6069, 291, 4196, 281, 452, 11, 370, 309, 311, 31160, 11, 50704], "temperature": 0.0, "avg_logprob": -0.1450218829241666, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.0020174155943095684}, {"id": 544, "seek": 381816, "start": 3824.96, "end": 3830.3199999999997, "text": " they call it erratic, almost random. So there is no mathematics that can model it.", "tokens": [50704, 436, 818, 309, 1189, 25198, 11, 1920, 4974, 13, 407, 456, 307, 572, 18666, 300, 393, 2316, 309, 13, 50972], "temperature": 0.0, "avg_logprob": -0.1450218829241666, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.0020174155943095684}, {"id": 545, "seek": 381816, "start": 3832.16, "end": 3836.64, "text": " And there are many aspects to the mind, whether it's social reasoning, language, and then they", "tokens": [51064, 400, 456, 366, 867, 7270, 281, 264, 1575, 11, 1968, 309, 311, 2093, 21577, 11, 2856, 11, 293, 550, 436, 51288], "temperature": 0.0, "avg_logprob": -0.1450218829241666, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.0020174155943095684}, {"id": 546, "seek": 381816, "start": 3836.64, "end": 3846.08, "text": " conclude there cannot be a system that we can model on volume and machines, because we don't", "tokens": [51288, 16886, 456, 2644, 312, 257, 1185, 300, 321, 393, 2316, 322, 5523, 293, 8379, 11, 570, 321, 500, 380, 51760], "temperature": 0.0, "avg_logprob": -0.1450218829241666, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.0020174155943095684}, {"id": 547, "seek": 384608, "start": 3846.08, "end": 3852.24, "text": " have the mathematics to model it. And these, they go into deep learning. And they give examples", "tokens": [50364, 362, 264, 18666, 281, 2316, 309, 13, 400, 613, 11, 436, 352, 666, 2452, 2539, 13, 400, 436, 976, 5110, 50672], "temperature": 0.0, "avg_logprob": -0.13034444214195334, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.004826459567993879}, {"id": 548, "seek": 384608, "start": 3852.24, "end": 3857.36, "text": " even like deep learning, no matter how much data you ingest, you can never predict the future.", "tokens": [50672, 754, 411, 2452, 2539, 11, 572, 1871, 577, 709, 1412, 291, 3957, 377, 11, 291, 393, 1128, 6069, 264, 2027, 13, 50928], "temperature": 0.0, "avg_logprob": -0.13034444214195334, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.004826459567993879}, {"id": 549, "seek": 384608, "start": 3858.64, "end": 3866.3199999999997, "text": " You're lucky if you can do a good job on the past and even forget the future. And definitely", "tokens": [50992, 509, 434, 6356, 498, 291, 393, 360, 257, 665, 1691, 322, 264, 1791, 293, 754, 2870, 264, 2027, 13, 400, 2138, 51376], "temperature": 0.0, "avg_logprob": -0.13034444214195334, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.004826459567993879}, {"id": 550, "seek": 384608, "start": 3866.3199999999997, "end": 3872.88, "text": " forget the, sorry, the present. So definitely forget. Can I jump in for a minute because I", "tokens": [51376, 2870, 264, 11, 2597, 11, 264, 1974, 13, 407, 2138, 2870, 13, 1664, 286, 3012, 294, 337, 257, 3456, 570, 286, 51704], "temperature": 0.0, "avg_logprob": -0.13034444214195334, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.004826459567993879}, {"id": 551, "seek": 387288, "start": 3872.88, "end": 3879.6800000000003, "text": " have a couple of comments. So one is, would you agree that this is quite synonymous with,", "tokens": [50364, 362, 257, 1916, 295, 3053, 13, 407, 472, 307, 11, 576, 291, 3986, 300, 341, 307, 1596, 5451, 18092, 365, 11, 50704], "temperature": 0.0, "avg_logprob": -0.18577855428059895, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.011148619465529919}, {"id": 552, "seek": 387288, "start": 3879.6800000000003, "end": 3884.96, "text": " you know, Douglas Hofstadter's strange, strange loops and the whole like random reference, the", "tokens": [50704, 291, 458, 11, 23010, 37379, 48299, 391, 311, 5861, 11, 5861, 16121, 293, 264, 1379, 411, 4974, 6408, 11, 264, 50968], "temperature": 0.0, "avg_logprob": -0.18577855428059895, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.011148619465529919}, {"id": 553, "seek": 387288, "start": 3884.96, "end": 3889.6, "text": " self-referential self systems, because I mean, complex systems, a big part of them is they", "tokens": [50968, 2698, 12, 265, 612, 2549, 2698, 3652, 11, 570, 286, 914, 11, 3997, 3652, 11, 257, 955, 644, 295, 552, 307, 436, 51200], "temperature": 0.0, "avg_logprob": -0.18577855428059895, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.011148619465529919}, {"id": 554, "seek": 387288, "start": 3889.6, "end": 3895.28, "text": " usually are, they do have feedback loops. And at some scale, they become so they will involve", "tokens": [51200, 2673, 366, 11, 436, 360, 362, 5824, 16121, 13, 400, 412, 512, 4373, 11, 436, 1813, 370, 436, 486, 9494, 51484], "temperature": 0.0, "avg_logprob": -0.18577855428059895, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.011148619465529919}, {"id": 555, "seek": 387288, "start": 3895.28, "end": 3901.84, "text": " self reference. Yeah. Okay. My other, my other point I want to make is this, you know, I have", "tokens": [51484, 2698, 6408, 13, 865, 13, 1033, 13, 1222, 661, 11, 452, 661, 935, 286, 528, 281, 652, 307, 341, 11, 291, 458, 11, 286, 362, 51812], "temperature": 0.0, "avg_logprob": -0.18577855428059895, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.011148619465529919}, {"id": 556, "seek": 390184, "start": 3901.84, "end": 3907.6800000000003, "text": " quite a bit of sympathy towards the viewpoint, right, of this, of this book that you're talking about,", "tokens": [50364, 1596, 257, 857, 295, 33240, 3030, 264, 35248, 11, 558, 11, 295, 341, 11, 295, 341, 1446, 300, 291, 434, 1417, 466, 11, 50656], "temperature": 0.0, "avg_logprob": -0.10102249228436014, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.001244648126885295}, {"id": 557, "seek": 390184, "start": 3907.6800000000003, "end": 3913.44, "text": " with one exception, which is I'm still optimistic that we can discover a mathematics that may help", "tokens": [50656, 365, 472, 11183, 11, 597, 307, 286, 478, 920, 19397, 300, 321, 393, 4411, 257, 18666, 300, 815, 854, 50944], "temperature": 0.0, "avg_logprob": -0.10102249228436014, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.001244648126885295}, {"id": 558, "seek": 390184, "start": 3913.44, "end": 3920.0, "text": " us out. And so I always think to the foundation series by Isaac Asimov, because in there, they", "tokens": [50944, 505, 484, 13, 400, 370, 286, 1009, 519, 281, 264, 7030, 2638, 538, 22505, 1018, 332, 5179, 11, 570, 294, 456, 11, 436, 51272], "temperature": 0.0, "avg_logprob": -0.10102249228436014, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.001244648126885295}, {"id": 559, "seek": 390184, "start": 3920.0, "end": 3926.08, "text": " discover a science in a mathematics called psycho history, which at least allows them to predict", "tokens": [51272, 4411, 257, 3497, 294, 257, 18666, 1219, 33355, 2503, 11, 597, 412, 1935, 4045, 552, 281, 6069, 51576], "temperature": 0.0, "avg_logprob": -0.10102249228436014, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.001244648126885295}, {"id": 560, "seek": 392608, "start": 3926.16, "end": 3932.08, "text": " complex systems of a certain scale and larger. So in the book, it's sort of like planet scale", "tokens": [50368, 3997, 3652, 295, 257, 1629, 4373, 293, 4833, 13, 407, 294, 264, 1446, 11, 309, 311, 1333, 295, 411, 5054, 4373, 50664], "temperature": 0.0, "avg_logprob": -0.1211252999961923, "compression_ratio": 1.8134920634920635, "no_speech_prob": 0.18235816061496735}, {"id": 561, "seek": 392608, "start": 3932.08, "end": 3937.68, "text": " and larger, they're able to actually predict, you know, these complex sociological systems", "tokens": [50664, 293, 4833, 11, 436, 434, 1075, 281, 767, 6069, 11, 291, 458, 11, 613, 3997, 3075, 4383, 3652, 50944], "temperature": 0.0, "avg_logprob": -0.1211252999961923, "compression_ratio": 1.8134920634920635, "no_speech_prob": 0.18235816061496735}, {"id": 562, "seek": 392608, "start": 3937.68, "end": 3943.2799999999997, "text": " and human behaviors, and how they're going to interact like beyond, beyond that scale. And", "tokens": [50944, 293, 1952, 15501, 11, 293, 577, 436, 434, 516, 281, 4648, 411, 4399, 11, 4399, 300, 4373, 13, 400, 51224], "temperature": 0.0, "avg_logprob": -0.1211252999961923, "compression_ratio": 1.8134920634920635, "no_speech_prob": 0.18235816061496735}, {"id": 563, "seek": 392608, "start": 3943.2799999999997, "end": 3947.6, "text": " it's really fascinating. I make that point. I highly recommend that series to anybody,", "tokens": [51224, 309, 311, 534, 10343, 13, 286, 652, 300, 935, 13, 286, 5405, 2748, 300, 2638, 281, 4472, 11, 51440], "temperature": 0.0, "avg_logprob": -0.1211252999961923, "compression_ratio": 1.8134920634920635, "no_speech_prob": 0.18235816061496735}, {"id": 564, "seek": 392608, "start": 3947.6, "end": 3952.96, "text": " because it's very fascinating because, you know, they talk a lot about sort of what if you had", "tokens": [51440, 570, 309, 311, 588, 10343, 570, 11, 291, 458, 11, 436, 751, 257, 688, 466, 1333, 295, 437, 498, 291, 632, 51708], "temperature": 0.0, "avg_logprob": -0.1211252999961923, "compression_ratio": 1.8134920634920635, "no_speech_prob": 0.18235816061496735}, {"id": 565, "seek": 395296, "start": 3952.96, "end": 3958.4, "text": " the science, what might it look like, etc. And in there, there's like this little tiny microscopic", "tokens": [50364, 264, 3497, 11, 437, 1062, 309, 574, 411, 11, 5183, 13, 400, 294, 456, 11, 456, 311, 411, 341, 707, 5870, 47897, 50636], "temperature": 0.0, "avg_logprob": -0.10408206534596671, "compression_ratio": 1.6426116838487972, "no_speech_prob": 0.0020499799866229296}, {"id": 566, "seek": 395296, "start": 3958.4, "end": 3964.0, "text": " thing that's beyond the predictability of psycho history that comes in and kind of mucks up the", "tokens": [50636, 551, 300, 311, 4399, 264, 6069, 2310, 295, 33355, 2503, 300, 1487, 294, 293, 733, 295, 275, 15493, 493, 264, 50916], "temperature": 0.0, "avg_logprob": -0.10408206534596671, "compression_ratio": 1.6426116838487972, "no_speech_prob": 0.0020499799866229296}, {"id": 567, "seek": 395296, "start": 3964.0, "end": 3969.68, "text": " works and creates anomalies that they have to constantly keep combating against. So I think", "tokens": [50916, 1985, 293, 7829, 24769, 48872, 300, 436, 362, 281, 6460, 1066, 2512, 990, 1970, 13, 407, 286, 519, 51200], "temperature": 0.0, "avg_logprob": -0.10408206534596671, "compression_ratio": 1.6426116838487972, "no_speech_prob": 0.0020499799866229296}, {"id": 568, "seek": 395296, "start": 3969.68, "end": 3974.8, "text": " if anybody wants a fictional take on a possible mathematics of this, like, I would recommend", "tokens": [51200, 498, 4472, 2738, 257, 28911, 747, 322, 257, 1944, 18666, 295, 341, 11, 411, 11, 286, 576, 2748, 51456], "temperature": 0.0, "avg_logprob": -0.10408206534596671, "compression_ratio": 1.6426116838487972, "no_speech_prob": 0.0020499799866229296}, {"id": 569, "seek": 395296, "start": 3974.8, "end": 3981.44, "text": " Yeah, I make this point. I say, I agree with their argument. We're trying to model complex systems", "tokens": [51456, 865, 11, 286, 652, 341, 935, 13, 286, 584, 11, 286, 3986, 365, 641, 6770, 13, 492, 434, 1382, 281, 2316, 3997, 3652, 51788], "temperature": 0.0, "avg_logprob": -0.10408206534596671, "compression_ratio": 1.6426116838487972, "no_speech_prob": 0.0020499799866229296}, {"id": 570, "seek": 398144, "start": 3981.44, "end": 3986.96, "text": " in the sense of cyclical cause and effect that we don't have anything that can model them", "tokens": [50364, 294, 264, 2020, 295, 19474, 804, 3082, 293, 1802, 300, 321, 500, 380, 362, 1340, 300, 393, 2316, 552, 50640], "temperature": 0.0, "avg_logprob": -0.17977177469353928, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.0017802537186071277}, {"id": 571, "seek": 398144, "start": 3986.96, "end": 3992.16, "text": " intelligently. And I give an example in this, in this, I can write a program that changes itself", "tokens": [50640, 5613, 2276, 13, 400, 286, 976, 364, 1365, 294, 341, 11, 294, 341, 11, 286, 393, 2464, 257, 1461, 300, 2962, 2564, 50900], "temperature": 0.0, "avg_logprob": -0.17977177469353928, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.0017802537186071277}, {"id": 572, "seek": 398144, "start": 3992.16, "end": 3997.6, "text": " at runtime. Because this is intentional, I can, the whole program can be a parameter,", "tokens": [50900, 412, 34474, 13, 1436, 341, 307, 21935, 11, 286, 393, 11, 264, 1379, 1461, 393, 312, 257, 13075, 11, 51172], "temperature": 0.0, "avg_logprob": -0.17977177469353928, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.0017802537186071277}, {"id": 573, "seek": 398144, "start": 3998.2400000000002, "end": 4003.52, "text": " which I can look at it. Well, code is data. That's why I can manipulate the program itself", "tokens": [51204, 597, 286, 393, 574, 412, 309, 13, 1042, 11, 3089, 307, 1412, 13, 663, 311, 983, 286, 393, 20459, 264, 1461, 2564, 51468], "temperature": 0.0, "avg_logprob": -0.17977177469353928, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.0017802537186071277}, {"id": 574, "seek": 398144, "start": 4003.52, "end": 4007.68, "text": " and go look at it after execution and see different program than the one I wrote. It's,", "tokens": [51468, 293, 352, 574, 412, 309, 934, 15058, 293, 536, 819, 1461, 813, 264, 472, 286, 4114, 13, 467, 311, 11, 51676], "temperature": 0.0, "avg_logprob": -0.17977177469353928, "compression_ratio": 1.7346153846153847, "no_speech_prob": 0.0017802537186071277}, {"id": 575, "seek": 400768, "start": 4007.68, "end": 4013.8399999999997, "text": " it's amazing list. So if I, so I can write programs in this that no one can understand", "tokens": [50364, 309, 311, 2243, 1329, 13, 407, 498, 286, 11, 370, 286, 393, 2464, 4268, 294, 341, 300, 572, 472, 393, 1223, 50672], "temperature": 0.0, "avg_logprob": -0.13583017388979593, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.0014094843063503504}, {"id": 576, "seek": 400768, "start": 4014.8799999999997, "end": 4021.2, "text": " and model and do program verification. So I make the argument that, okay, I can see your point.", "tokens": [50724, 293, 2316, 293, 360, 1461, 30206, 13, 407, 286, 652, 264, 6770, 300, 11, 1392, 11, 286, 393, 536, 428, 935, 13, 51040], "temperature": 0.0, "avg_logprob": -0.13583017388979593, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.0014094843063503504}, {"id": 577, "seek": 400768, "start": 4022.16, "end": 4029.12, "text": " But like Keith said, never is a long time. Why say we cannot come up with a new mathematics?", "tokens": [51088, 583, 411, 20613, 848, 11, 1128, 307, 257, 938, 565, 13, 1545, 584, 321, 2644, 808, 493, 365, 257, 777, 18666, 30, 51436], "temperature": 0.0, "avg_logprob": -0.13583017388979593, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.0014094843063503504}, {"id": 578, "seek": 400768, "start": 4029.12, "end": 4035.44, "text": " I can see you at one point, someone discovering, yeah, at the level of Newton differential calculus,", "tokens": [51436, 286, 393, 536, 291, 412, 472, 935, 11, 1580, 24773, 11, 1338, 11, 412, 264, 1496, 295, 19541, 15756, 33400, 11, 51752], "temperature": 0.0, "avg_logprob": -0.13583017388979593, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.0014094843063503504}, {"id": 579, "seek": 403544, "start": 4035.44, "end": 4041.76, "text": " why not? Which could happen. So the word never for me, it's hard to digest.", "tokens": [50364, 983, 406, 30, 3013, 727, 1051, 13, 407, 264, 1349, 1128, 337, 385, 11, 309, 311, 1152, 281, 13884, 13, 50680], "temperature": 0.0, "avg_logprob": -0.15299508657800145, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0005526038585230708}, {"id": 580, "seek": 403544, "start": 4041.76, "end": 4045.04, "text": " Maybe an AGI will discover the mathematics to create itself.", "tokens": [50680, 2704, 364, 316, 26252, 486, 4411, 264, 18666, 281, 1884, 2564, 13, 50844], "temperature": 0.0, "avg_logprob": -0.15299508657800145, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0005526038585230708}, {"id": 581, "seek": 403544, "start": 4046.48, "end": 4052.56, "text": " And the other point is, the other point is, which is another point that John McCarthy wants.", "tokens": [50916, 400, 264, 661, 935, 307, 11, 264, 661, 935, 307, 11, 597, 307, 1071, 935, 300, 2619, 44085, 2738, 13, 51220], "temperature": 0.0, "avg_logprob": -0.15299508657800145, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0005526038585230708}, {"id": 582, "seek": 403544, "start": 4053.36, "end": 4060.8, "text": " Who said we have to understand what we built? Here's what I mean. Do we understand ourselves?", "tokens": [51260, 2102, 848, 321, 362, 281, 1223, 437, 321, 3094, 30, 1692, 311, 437, 286, 914, 13, 1144, 321, 1223, 4175, 30, 51632], "temperature": 0.0, "avg_logprob": -0.15299508657800145, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0005526038585230708}, {"id": 583, "seek": 406080, "start": 4060.8, "end": 4067.28, "text": " We don't. So why not build a scary intelligent machine that we don't really understand,", "tokens": [50364, 492, 500, 380, 13, 407, 983, 406, 1322, 257, 6958, 13232, 3479, 300, 321, 500, 380, 534, 1223, 11, 50688], "temperature": 0.0, "avg_logprob": -0.12354067961374919, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.0010986009147018194}, {"id": 584, "seek": 406080, "start": 4067.28, "end": 4073.04, "text": " like my list program? So what I'm saying is I had, I had an issue with them saying,", "tokens": [50688, 411, 452, 1329, 1461, 30, 407, 437, 286, 478, 1566, 307, 286, 632, 11, 286, 632, 364, 2734, 365, 552, 1566, 11, 50976], "temperature": 0.0, "avg_logprob": -0.12354067961374919, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.0010986009147018194}, {"id": 585, "seek": 406080, "start": 4073.76, "end": 4082.32, "text": " that precludes AGI. No, it doesn't. In theory, I can build a complex intelligent machine like us", "tokens": [51012, 300, 4346, 1471, 279, 316, 26252, 13, 883, 11, 309, 1177, 380, 13, 682, 5261, 11, 286, 393, 1322, 257, 3997, 13232, 3479, 411, 505, 51440], "temperature": 0.0, "avg_logprob": -0.12354067961374919, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.0010986009147018194}, {"id": 586, "seek": 406080, "start": 4083.04, "end": 4088.6400000000003, "text": " in many respects. Doesn't feel pain. Hey, who cares? But it's scary intelligent.", "tokens": [51476, 294, 867, 24126, 13, 12955, 380, 841, 1822, 13, 1911, 11, 567, 12310, 30, 583, 309, 311, 6958, 13232, 13, 51756], "temperature": 0.0, "avg_logprob": -0.12354067961374919, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.0010986009147018194}, {"id": 587, "seek": 408864, "start": 4089.44, "end": 4094.3199999999997, "text": " And we don't understand how it works. So what? This can happen. I can build something I don't", "tokens": [50404, 400, 321, 500, 380, 1223, 577, 309, 1985, 13, 407, 437, 30, 639, 393, 1051, 13, 286, 393, 1322, 746, 286, 500, 380, 50648], "temperature": 0.0, "avg_logprob": -0.09871998855045863, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.0022863822523504496}, {"id": 588, "seek": 408864, "start": 4094.3199999999997, "end": 4102.72, "text": " understand. So in theory, I have two issues with their book, that this never and this absolute", "tokens": [50648, 1223, 13, 407, 294, 5261, 11, 286, 362, 732, 2663, 365, 641, 1446, 11, 300, 341, 1128, 293, 341, 8236, 51068], "temperature": 0.0, "avg_logprob": -0.09871998855045863, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.0022863822523504496}, {"id": 589, "seek": 408864, "start": 4102.72, "end": 4106.48, "text": " decision that we're done, we can never get there. No, we might build something we don't understand", "tokens": [51068, 3537, 300, 321, 434, 1096, 11, 321, 393, 1128, 483, 456, 13, 883, 11, 321, 1062, 1322, 746, 321, 500, 380, 1223, 51256], "temperature": 0.0, "avg_logprob": -0.09871998855045863, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.0022863822523504496}, {"id": 590, "seek": 408864, "start": 4107.2, "end": 4111.76, "text": " by discovering some new weird mathematics. So, okay, I agree with you that it's a,", "tokens": [51292, 538, 24773, 512, 777, 3657, 18666, 13, 407, 11, 1392, 11, 286, 3986, 365, 291, 300, 309, 311, 257, 11, 51520], "temperature": 0.0, "avg_logprob": -0.09871998855045863, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.0022863822523504496}, {"id": 591, "seek": 408864, "start": 4112.48, "end": 4115.12, "text": " it's a complex thing that we will never understand. But so what?", "tokens": [51556, 309, 311, 257, 3997, 551, 300, 321, 486, 1128, 1223, 13, 583, 370, 437, 30, 51688], "temperature": 0.0, "avg_logprob": -0.09871998855045863, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.0022863822523504496}, {"id": 592, "seek": 411512, "start": 4115.12, "end": 4121.12, "text": " But what I loved about the book is it's a sobering book. I mean,", "tokens": [50364, 583, 437, 286, 4333, 466, 264, 1446, 307, 309, 311, 257, 26212, 278, 1446, 13, 286, 914, 11, 50664], "temperature": 0.0, "avg_logprob": -0.17468639214833578, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.004978352226316929}, {"id": 593, "seek": 411512, "start": 4122.24, "end": 4128.4, "text": " it really is a balancing book compared to the hype and the simplicity you see out there. I mean,", "tokens": [50720, 309, 534, 307, 257, 22495, 1446, 5347, 281, 264, 24144, 293, 264, 25632, 291, 536, 484, 456, 13, 286, 914, 11, 51028], "temperature": 0.0, "avg_logprob": -0.17468639214833578, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.004978352226316929}, {"id": 594, "seek": 411512, "start": 4128.4, "end": 4136.0, "text": " you, you recommend it or highly because I mean, I didn't need that much sobering. I know that", "tokens": [51028, 291, 11, 291, 2748, 309, 420, 5405, 570, 286, 914, 11, 286, 994, 380, 643, 300, 709, 26212, 278, 13, 286, 458, 300, 51408], "temperature": 0.0, "avg_logprob": -0.17468639214833578, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.004978352226316929}, {"id": 595, "seek": 411512, "start": 4136.0, "end": 4143.28, "text": " any talk of AGI is like, Hey, take a break. Enjoy your paycheck, but don't make silly statements", "tokens": [51408, 604, 751, 295, 316, 26252, 307, 411, 11, 1911, 11, 747, 257, 1821, 13, 15411, 428, 35639, 11, 457, 500, 380, 652, 11774, 12363, 51772], "temperature": 0.0, "avg_logprob": -0.17468639214833578, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.004978352226316929}, {"id": 596, "seek": 414328, "start": 4143.28, "end": 4147.679999999999, "text": " like this, right? Although I felt I thought you might have fallen off that wagon at the beginning", "tokens": [50364, 411, 341, 11, 558, 30, 5780, 286, 2762, 286, 1194, 291, 1062, 362, 11547, 766, 300, 34453, 412, 264, 2863, 50584], "temperature": 0.0, "avg_logprob": -0.2024139548247715, "compression_ratio": 1.5506072874493928, "no_speech_prob": 0.000779283232986927}, {"id": 597, "seek": 414328, "start": 4147.679999999999, "end": 4156.8, "text": " of this conversation. No, I'm a defender of the faith. But, but so it's, I recommend it to people", "tokens": [50584, 295, 341, 3761, 13, 883, 11, 286, 478, 257, 26537, 295, 264, 4522, 13, 583, 11, 457, 370, 309, 311, 11, 286, 2748, 309, 281, 561, 51040], "temperature": 0.0, "avg_logprob": -0.2024139548247715, "compression_ratio": 1.5506072874493928, "no_speech_prob": 0.000779283232986927}, {"id": 598, "seek": 414328, "start": 4156.8, "end": 4162.8, "text": " that need it. Like me, I needed it too. It's a sobering book. Like, this is how complex what", "tokens": [51040, 300, 643, 309, 13, 1743, 385, 11, 286, 2978, 309, 886, 13, 467, 311, 257, 26212, 278, 1446, 13, 1743, 11, 341, 307, 577, 3997, 437, 51340], "temperature": 0.0, "avg_logprob": -0.2024139548247715, "compression_ratio": 1.5506072874493928, "no_speech_prob": 0.000779283232986927}, {"id": 599, "seek": 414328, "start": 4162.8, "end": 4170.08, "text": " you're trying to do is. Okay, guys. So before you go out and say, language understand. And the", "tokens": [51340, 291, 434, 1382, 281, 360, 307, 13, 1033, 11, 1074, 13, 407, 949, 291, 352, 484, 293, 584, 11, 2856, 1223, 13, 400, 264, 51704], "temperature": 0.0, "avg_logprob": -0.2024139548247715, "compression_ratio": 1.5506072874493928, "no_speech_prob": 0.000779283232986927}, {"id": 600, "seek": 417008, "start": 4170.08, "end": 4176.24, "text": " nice thing is they took aspects of the mind, just language itself is a beast that we cannot conquer.", "tokens": [50364, 1481, 551, 307, 436, 1890, 7270, 295, 264, 1575, 11, 445, 2856, 2564, 307, 257, 13464, 300, 321, 2644, 24136, 13, 50672], "temperature": 0.0, "avg_logprob": -0.09449753968612007, "compression_ratio": 2.027027027027027, "no_speech_prob": 0.0004954033065587282}, {"id": 601, "seek": 417008, "start": 4176.96, "end": 4181.2, "text": " So imagine the whole mind and the granular thing they go through it. I mean, it's all,", "tokens": [50708, 407, 3811, 264, 1379, 1575, 293, 264, 39962, 551, 436, 352, 807, 309, 13, 286, 914, 11, 309, 311, 439, 11, 50920], "temperature": 0.0, "avg_logprob": -0.09449753968612007, "compression_ratio": 2.027027027027027, "no_speech_prob": 0.0004954033065587282}, {"id": 602, "seek": 417008, "start": 4181.2, "end": 4184.8, "text": " it's complex systems all the way down or all the way up if you want. So", "tokens": [50920, 309, 311, 3997, 3652, 439, 264, 636, 760, 420, 439, 264, 636, 493, 498, 291, 528, 13, 407, 51100], "temperature": 0.0, "avg_logprob": -0.09449753968612007, "compression_ratio": 2.027027027027027, "no_speech_prob": 0.0004954033065587282}, {"id": 603, "seek": 417008, "start": 4186.72, "end": 4191.5199999999995, "text": " language is a complex system on its own part of the mind, which is a complex system on its own", "tokens": [51196, 2856, 307, 257, 3997, 1185, 322, 1080, 1065, 644, 295, 264, 1575, 11, 597, 307, 257, 3997, 1185, 322, 1080, 1065, 51436], "temperature": 0.0, "avg_logprob": -0.09449753968612007, "compression_ratio": 2.027027027027027, "no_speech_prob": 0.0004954033065587282}, {"id": 604, "seek": 417008, "start": 4191.5199999999995, "end": 4197.44, "text": " part of the human living organism, which is a complex system on its own. So and at every level,", "tokens": [51436, 644, 295, 264, 1952, 2647, 24128, 11, 597, 307, 257, 3997, 1185, 322, 1080, 1065, 13, 407, 293, 412, 633, 1496, 11, 51732], "temperature": 0.0, "avg_logprob": -0.09449753968612007, "compression_ratio": 2.027027027027027, "no_speech_prob": 0.0004954033065587282}, {"id": 605, "seek": 419744, "start": 4197.44, "end": 4204.24, "text": " the complexity, we don't have a mathematics for that's the gist of their argument. So people that", "tokens": [50364, 264, 14024, 11, 321, 500, 380, 362, 257, 18666, 337, 300, 311, 264, 290, 468, 295, 641, 6770, 13, 407, 561, 300, 50704], "temperature": 0.0, "avg_logprob": -0.1848289966583252, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.0007907110848464072}, {"id": 606, "seek": 419744, "start": 4204.24, "end": 4211.04, "text": " make these big claims about AI need to read it. Guys, cool down, cool down. You have, you have not", "tokens": [50704, 652, 613, 955, 9441, 466, 7318, 643, 281, 1401, 309, 13, 7855, 11, 1627, 760, 11, 1627, 760, 13, 509, 362, 11, 291, 362, 406, 51044], "temperature": 0.0, "avg_logprob": -0.1848289966583252, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.0007907110848464072}, {"id": 607, "seek": 419744, "start": 4211.04, "end": 4217.679999999999, "text": " solved problems that occupy the most penetrating minds in the history of enemy from Emmanuel", "tokens": [51044, 13041, 2740, 300, 30645, 264, 881, 16183, 8754, 9634, 294, 264, 2503, 295, 5945, 490, 44421, 51376], "temperature": 0.0, "avg_logprob": -0.1848289966583252, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.0007907110848464072}, {"id": 608, "seek": 419744, "start": 4217.679999999999, "end": 4225.28, "text": " Khan to you have not solved these problems, cool it down. You can build narrow AI, very narrow AI.", "tokens": [51376, 18136, 281, 291, 362, 406, 13041, 613, 2740, 11, 1627, 309, 760, 13, 509, 393, 1322, 9432, 7318, 11, 588, 9432, 7318, 13, 51756], "temperature": 0.0, "avg_logprob": -0.1848289966583252, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.0007907110848464072}, {"id": 609, "seek": 422528, "start": 4226.24, "end": 4232.639999999999, "text": " And all this transferability, transferability. I mean, if you're good at chess, I know people", "tokens": [50412, 400, 439, 341, 5003, 2310, 11, 5003, 2310, 13, 286, 914, 11, 498, 291, 434, 665, 412, 24122, 11, 286, 458, 561, 50732], "temperature": 0.0, "avg_logprob": -0.13909755706787108, "compression_ratio": 1.751131221719457, "no_speech_prob": 0.0007431873236782849}, {"id": 610, "seek": 422528, "start": 4232.639999999999, "end": 4239.84, "text": " that are good at chess and they're almost good at nothing else, not okay. So forget this. If I'm", "tokens": [50732, 300, 366, 665, 412, 24122, 293, 436, 434, 1920, 665, 412, 1825, 1646, 11, 406, 1392, 13, 407, 2870, 341, 13, 759, 286, 478, 51092], "temperature": 0.0, "avg_logprob": -0.13909755706787108, "compression_ratio": 1.751131221719457, "no_speech_prob": 0.0007431873236782849}, {"id": 611, "seek": 422528, "start": 4239.84, "end": 4247.759999999999, "text": " good at chess, I can be a smart doctor. No. So we are a very complex machine. So this book is a good", "tokens": [51092, 665, 412, 24122, 11, 286, 393, 312, 257, 4069, 4631, 13, 883, 13, 407, 321, 366, 257, 588, 3997, 3479, 13, 407, 341, 1446, 307, 257, 665, 51488], "temperature": 0.0, "avg_logprob": -0.13909755706787108, "compression_ratio": 1.751131221719457, "no_speech_prob": 0.0007431873236782849}, {"id": 612, "seek": 422528, "start": 4247.759999999999, "end": 4254.719999999999, "text": " sobering book, mathematically speaking, philosophically speaking, so that people will tone down", "tokens": [51488, 26212, 278, 1446, 11, 44003, 4124, 11, 14529, 984, 4124, 11, 370, 300, 561, 486, 8027, 760, 51836], "temperature": 0.0, "avg_logprob": -0.13909755706787108, "compression_ratio": 1.751131221719457, "no_speech_prob": 0.0007431873236782849}, {"id": 613, "seek": 425472, "start": 4254.72, "end": 4263.360000000001, "text": " what they're saying and start speaking science instead of media gibberish, right? Deep learning", "tokens": [50364, 437, 436, 434, 1566, 293, 722, 4124, 3497, 2602, 295, 3021, 4553, 43189, 11, 558, 30, 14895, 2539, 50796], "temperature": 0.0, "avg_logprob": -0.15731475722621863, "compression_ratio": 1.4378109452736318, "no_speech_prob": 0.003478115424513817}, {"id": 614, "seek": 425472, "start": 4263.360000000001, "end": 4271.04, "text": " will soon be able to do everything. I mean, from a scientist. Well, it seems like a council of", "tokens": [50796, 486, 2321, 312, 1075, 281, 360, 1203, 13, 286, 914, 11, 490, 257, 12662, 13, 1042, 11, 309, 2544, 411, 257, 9209, 295, 51180], "temperature": 0.0, "avg_logprob": -0.15731475722621863, "compression_ratio": 1.4378109452736318, "no_speech_prob": 0.003478115424513817}, {"id": 615, "seek": 425472, "start": 4271.04, "end": 4281.04, "text": " despair almost. Is there any optimistic or positive hopeful aspects to it? No, I that part I don't", "tokens": [51180, 25763, 1920, 13, 1119, 456, 604, 19397, 420, 3353, 20531, 7270, 281, 309, 30, 883, 11, 286, 300, 644, 286, 500, 380, 51680], "temperature": 0.0, "avg_logprob": -0.15731475722621863, "compression_ratio": 1.4378109452736318, "no_speech_prob": 0.003478115424513817}, {"id": 616, "seek": 428104, "start": 4281.12, "end": 4291.84, "text": " like this never, right? I mean, I am a believer that we can do AGI, but not a human like AI.", "tokens": [50368, 411, 341, 1128, 11, 558, 30, 286, 914, 11, 286, 669, 257, 23892, 300, 321, 393, 360, 316, 26252, 11, 457, 406, 257, 1952, 411, 7318, 13, 50904], "temperature": 0.0, "avg_logprob": -0.154717397086228, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0038809548132121563}, {"id": 617, "seek": 428104, "start": 4292.72, "end": 4302.08, "text": " We might do a very powerful AI that in many ways is more powerful. I mean, we've done that now. I mean,", "tokens": [50948, 492, 1062, 360, 257, 588, 4005, 7318, 300, 294, 867, 2098, 307, 544, 4005, 13, 286, 914, 11, 321, 600, 1096, 300, 586, 13, 286, 914, 11, 51416], "temperature": 0.0, "avg_logprob": -0.154717397086228, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0038809548132121563}, {"id": 618, "seek": 428104, "start": 4302.08, "end": 4308.16, "text": " machines are now superior to us in many respects and respects even that they require intelligence,", "tokens": [51416, 8379, 366, 586, 13028, 281, 505, 294, 867, 24126, 293, 24126, 754, 300, 436, 3651, 7599, 11, 51720], "temperature": 0.0, "avg_logprob": -0.154717397086228, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.0038809548132121563}, {"id": 619, "seek": 430816, "start": 4308.16, "end": 4315.36, "text": " not a bulldozer that can lift more than me, that will have to do cognitive tasks better than us.", "tokens": [50364, 406, 257, 4693, 2595, 4527, 300, 393, 5533, 544, 813, 385, 11, 300, 486, 362, 281, 360, 15605, 9608, 1101, 813, 505, 13, 50724], "temperature": 0.0, "avg_logprob": -0.16051038106282553, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.001546800252981484}, {"id": 620, "seek": 430816, "start": 4315.36, "end": 4326.4, "text": " We have go or finding patterns and data at the scale that no human can do. So we are building", "tokens": [50724, 492, 362, 352, 420, 5006, 8294, 293, 1412, 412, 264, 4373, 300, 572, 1952, 393, 360, 13, 407, 321, 366, 2390, 51276], "temperature": 0.0, "avg_logprob": -0.16051038106282553, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.001546800252981484}, {"id": 621, "seek": 430816, "start": 4326.4, "end": 4333.12, "text": " intelligent machines, but can we conquer things like language like autonomous driving was a failure.", "tokens": [51276, 13232, 8379, 11, 457, 393, 321, 24136, 721, 411, 2856, 411, 23797, 4840, 390, 257, 7763, 13, 51612], "temperature": 0.0, "avg_logprob": -0.16051038106282553, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.001546800252981484}, {"id": 622, "seek": 433312, "start": 4333.12, "end": 4337.68, "text": " It's a big upset for AI because they trivialize the problem that we can go.", "tokens": [50364, 467, 311, 257, 955, 8340, 337, 7318, 570, 436, 26703, 1125, 264, 1154, 300, 321, 393, 352, 13, 50592], "temperature": 0.0, "avg_logprob": -0.159580987347059, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.018534671515226364}, {"id": 623, "seek": 433312, "start": 4338.5599999999995, "end": 4342.88, "text": " Well, and that's kind of what I want to get to, you know, Mark kind of in response to you, which is", "tokens": [50636, 1042, 11, 293, 300, 311, 733, 295, 437, 286, 528, 281, 483, 281, 11, 291, 458, 11, 3934, 733, 295, 294, 4134, 281, 291, 11, 597, 307, 50852], "temperature": 0.0, "avg_logprob": -0.159580987347059, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.018534671515226364}, {"id": 624, "seek": 433312, "start": 4344.24, "end": 4348.08, "text": " I take these kind of sobering, these sobering things and look, I mean,", "tokens": [50920, 286, 747, 613, 733, 295, 26212, 278, 11, 613, 26212, 278, 721, 293, 574, 11, 286, 914, 11, 51112], "temperature": 0.0, "avg_logprob": -0.159580987347059, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.018534671515226364}, {"id": 625, "seek": 433312, "start": 4350.16, "end": 4355.599999999999, "text": " the book sounds great, and I'm definitely going to get it and read it. But some of these thoughts,", "tokens": [51216, 264, 1446, 3263, 869, 11, 293, 286, 478, 2138, 516, 281, 483, 309, 293, 1401, 309, 13, 583, 512, 295, 613, 4598, 11, 51488], "temperature": 0.0, "avg_logprob": -0.159580987347059, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.018534671515226364}, {"id": 626, "seek": 433312, "start": 4355.599999999999, "end": 4361.5199999999995, "text": " you know, many people have had, you know, many times over the years, right? And I've recognized", "tokens": [51488, 291, 458, 11, 867, 561, 362, 632, 11, 291, 458, 11, 867, 1413, 670, 264, 924, 11, 558, 30, 400, 286, 600, 9823, 51784], "temperature": 0.0, "avg_logprob": -0.159580987347059, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.018534671515226364}, {"id": 627, "seek": 436152, "start": 4361.6, "end": 4366.0, "text": " that there are these limitations. But I think like part of part of why I think", "tokens": [50368, 300, 456, 366, 613, 15705, 13, 583, 286, 519, 411, 644, 295, 644, 295, 983, 286, 519, 50588], "temperature": 0.0, "avg_logprob": -0.10778900524517437, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.02095918543636799}, {"id": 628, "seek": 436152, "start": 4366.0, "end": 4371.360000000001, "text": " books like this are actually have an optimistic kind of side to them is I hope, I hope they", "tokens": [50588, 3642, 411, 341, 366, 767, 362, 364, 19397, 733, 295, 1252, 281, 552, 307, 286, 1454, 11, 286, 1454, 436, 50856], "temperature": 0.0, "avg_logprob": -0.10778900524517437, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.02095918543636799}, {"id": 629, "seek": 436152, "start": 4371.360000000001, "end": 4379.68, "text": " encourage people to get more creative. Okay, like, like stop just trying to dump every single dollar", "tokens": [50856, 5373, 561, 281, 483, 544, 5880, 13, 1033, 11, 411, 11, 411, 1590, 445, 1382, 281, 11430, 633, 2167, 7241, 51272], "temperature": 0.0, "avg_logprob": -0.10778900524517437, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.02095918543636799}, {"id": 630, "seek": 436152, "start": 4379.68, "end": 4385.76, "text": " you have into yet another parameter, you know, into yet another thousand or billion parameters", "tokens": [51272, 291, 362, 666, 1939, 1071, 13075, 11, 291, 458, 11, 666, 1939, 1071, 4714, 420, 5218, 9834, 51576], "temperature": 0.0, "avg_logprob": -0.10778900524517437, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.02095918543636799}, {"id": 631, "seek": 436152, "start": 4385.76, "end": 4390.240000000001, "text": " in a model, like, let's take some of our resources, like, sure, let's keep doing that engineering,", "tokens": [51576, 294, 257, 2316, 11, 411, 11, 718, 311, 747, 512, 295, 527, 3593, 11, 411, 11, 988, 11, 718, 311, 1066, 884, 300, 7043, 11, 51800], "temperature": 0.0, "avg_logprob": -0.10778900524517437, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.02095918543636799}, {"id": 632, "seek": 439024, "start": 4390.24, "end": 4395.679999999999, "text": " but let's take some portion of our resources here and invest it in, like, crazy ideas. And I know", "tokens": [50364, 457, 718, 311, 747, 512, 8044, 295, 527, 3593, 510, 293, 1963, 309, 294, 11, 411, 11, 3219, 3487, 13, 400, 286, 458, 50636], "temperature": 0.0, "avg_logprob": -0.15478262534508339, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.0038241692818701267}, {"id": 633, "seek": 439024, "start": 4395.679999999999, "end": 4401.28, "text": " Tim's smile here because it's like, like, can it's can a Stanley kind of type thing, right? Like, just", "tokens": [50636, 7172, 311, 7563, 510, 570, 309, 311, 411, 11, 411, 11, 393, 309, 311, 393, 257, 28329, 733, 295, 2010, 551, 11, 558, 30, 1743, 11, 445, 50916], "temperature": 0.0, "avg_logprob": -0.15478262534508339, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.0038241692818701267}, {"id": 634, "seek": 439024, "start": 4402.08, "end": 4408.639999999999, "text": " go out there and try to do something crazy to find that mathematics that we need, right? Which is", "tokens": [50956, 352, 484, 456, 293, 853, 281, 360, 746, 3219, 281, 915, 300, 18666, 300, 321, 643, 11, 558, 30, 3013, 307, 51284], "temperature": 0.0, "avg_logprob": -0.15478262534508339, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.0038241692818701267}, {"id": 635, "seek": 439024, "start": 4408.639999999999, "end": 4414.5599999999995, "text": " let's get creative, let's work on crazy things, let's have crazy ideas, let's work on hybrid", "tokens": [51284, 718, 311, 483, 5880, 11, 718, 311, 589, 322, 3219, 721, 11, 718, 311, 362, 3219, 3487, 11, 718, 311, 589, 322, 13051, 51580], "temperature": 0.0, "avg_logprob": -0.15478262534508339, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.0038241692818701267}, {"id": 636, "seek": 441456, "start": 4414.56, "end": 4420.8, "text": " systems, let's not give up on, you know, neuromorphic, you know, systems and computer,", "tokens": [50364, 3652, 11, 718, 311, 406, 976, 493, 322, 11, 291, 458, 11, 12087, 32702, 299, 11, 291, 458, 11, 3652, 293, 3820, 11, 50676], "temperature": 0.0, "avg_logprob": -0.13453732704629703, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.00898430310189724}, {"id": 637, "seek": 441456, "start": 4420.8, "end": 4425.84, "text": " whatever, like, let's spread out, let's spread out a bit, because of the fact that if we just", "tokens": [50676, 2035, 11, 411, 11, 718, 311, 3974, 484, 11, 718, 311, 3974, 484, 257, 857, 11, 570, 295, 264, 1186, 300, 498, 321, 445, 50928], "temperature": 0.0, "avg_logprob": -0.13453732704629703, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.00898430310189724}, {"id": 638, "seek": 441456, "start": 4425.84, "end": 4433.68, "text": " keep going down this direction of ever larger Turing machines, like, that may not be the solution.", "tokens": [50928, 1066, 516, 760, 341, 3513, 295, 1562, 4833, 314, 1345, 8379, 11, 411, 11, 300, 815, 406, 312, 264, 3827, 13, 51320], "temperature": 0.0, "avg_logprob": -0.13453732704629703, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.00898430310189724}, {"id": 639, "seek": 441456, "start": 4434.240000000001, "end": 4441.68, "text": " Actually, they make this point exactly in different ways that if anything, their goal is to let people", "tokens": [51348, 5135, 11, 436, 652, 341, 935, 2293, 294, 819, 2098, 300, 498, 1340, 11, 641, 3387, 307, 281, 718, 561, 51720], "temperature": 0.0, "avg_logprob": -0.13453732704629703, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.00898430310189724}, {"id": 640, "seek": 444168, "start": 4441.76, "end": 4447.92, "text": " widen their horizon. So many aspects of this problem that guys, if we keep going,", "tokens": [50368, 32552, 641, 18046, 13, 407, 867, 7270, 295, 341, 1154, 300, 1074, 11, 498, 321, 1066, 516, 11, 50676], "temperature": 0.0, "avg_logprob": -0.13881023540053256, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.006186273414641619}, {"id": 641, "seek": 444168, "start": 4447.92, "end": 4451.68, "text": " this is not going to get us there. And that's why they argued mathematically,", "tokens": [50676, 341, 307, 406, 516, 281, 483, 505, 456, 13, 400, 300, 311, 983, 436, 20219, 44003, 11, 50864], "temperature": 0.0, "avg_logprob": -0.13881023540053256, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.006186273414641619}, {"id": 642, "seek": 444168, "start": 4451.68, "end": 4455.92, "text": " philosophically. And I think they have a good argument. This is not going to take us there.", "tokens": [50864, 14529, 984, 13, 400, 286, 519, 436, 362, 257, 665, 6770, 13, 639, 307, 406, 516, 281, 747, 505, 456, 13, 51076], "temperature": 0.0, "avg_logprob": -0.13881023540053256, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.006186273414641619}, {"id": 643, "seek": 444168, "start": 4456.4800000000005, "end": 4464.320000000001, "text": " But let's explore that. So that and being so religious about this will will hinder any other", "tokens": [51104, 583, 718, 311, 6839, 300, 13, 407, 300, 293, 885, 370, 7185, 466, 341, 486, 486, 276, 5669, 604, 661, 51496], "temperature": 0.0, "avg_logprob": -0.13881023540053256, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.006186273414641619}, {"id": 644, "seek": 446432, "start": 4464.88, "end": 4473.04, "text": " possibility. So overall, their argument is a good argument. I think everybody should read this book", "tokens": [50392, 7959, 13, 407, 4787, 11, 641, 6770, 307, 257, 665, 6770, 13, 286, 519, 2201, 820, 1401, 341, 1446, 50800], "temperature": 0.0, "avg_logprob": -0.09873884374445135, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.003121084999293089}, {"id": 645, "seek": 446432, "start": 4473.04, "end": 4482.4, "text": " that's interested in AGI as a goal. What we have cannot ever take us there. They prove this", "tokens": [50800, 300, 311, 3102, 294, 316, 26252, 382, 257, 3387, 13, 708, 321, 362, 2644, 1562, 747, 505, 456, 13, 814, 7081, 341, 51268], "temperature": 0.0, "avg_logprob": -0.09873884374445135, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.003121084999293089}, {"id": 646, "seek": 446432, "start": 4482.4, "end": 4488.639999999999, "text": " mathematically. I mean, to me, they proved it in language on. So we need something new. And if you", "tokens": [51268, 44003, 13, 286, 914, 11, 281, 385, 11, 436, 14617, 309, 294, 2856, 322, 13, 407, 321, 643, 746, 777, 13, 400, 498, 291, 51580], "temperature": 0.0, "avg_logprob": -0.09873884374445135, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.003121084999293089}, {"id": 647, "seek": 446432, "start": 4488.639999999999, "end": 4493.679999999999, "text": " want to do something new, we can't just stay in this corner and with this, that's not going to get", "tokens": [51580, 528, 281, 360, 746, 777, 11, 321, 393, 380, 445, 1754, 294, 341, 4538, 293, 365, 341, 11, 300, 311, 406, 516, 281, 483, 51832], "temperature": 0.0, "avg_logprob": -0.09873884374445135, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.003121084999293089}, {"id": 648, "seek": 449368, "start": 4493.68, "end": 4501.280000000001, "text": " us there. So in a way, it's not a negative book. It's a sobering book, I will use the term sober.", "tokens": [50364, 505, 456, 13, 407, 294, 257, 636, 11, 309, 311, 406, 257, 3671, 1446, 13, 467, 311, 257, 26212, 278, 1446, 11, 286, 486, 764, 264, 1433, 26212, 13, 50744], "temperature": 0.0, "avg_logprob": -0.1297048872167414, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0016732118092477322}, {"id": 649, "seek": 449368, "start": 4502.08, "end": 4507.200000000001, "text": " Well, and encouraging of more variety and more daring and more creativity and", "tokens": [50784, 1042, 11, 293, 14580, 295, 544, 5673, 293, 544, 43128, 293, 544, 12915, 293, 51040], "temperature": 0.0, "avg_logprob": -0.1297048872167414, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0016732118092477322}, {"id": 650, "seek": 449368, "start": 4507.76, "end": 4512.64, "text": " right, they don't push too much on that. But but indirectly, the indirect", "tokens": [51068, 558, 11, 436, 500, 380, 2944, 886, 709, 322, 300, 13, 583, 457, 37779, 11, 264, 19523, 51312], "temperature": 0.0, "avg_logprob": -0.1297048872167414, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0016732118092477322}, {"id": 651, "seek": 449368, "start": 4513.76, "end": 4520.72, "text": " net result, if people appreciate the argument will be to look and explore other ways. So", "tokens": [51368, 2533, 1874, 11, 498, 561, 4449, 264, 6770, 486, 312, 281, 574, 293, 6839, 661, 2098, 13, 407, 51716], "temperature": 0.0, "avg_logprob": -0.1297048872167414, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0016732118092477322}, {"id": 652, "seek": 452072, "start": 4521.280000000001, "end": 4527.4400000000005, "text": " in a way, it's more positive than negative. And by the way, stating a mathematical fact is never", "tokens": [50392, 294, 257, 636, 11, 309, 311, 544, 3353, 813, 3671, 13, 400, 538, 264, 636, 11, 26688, 257, 18894, 1186, 307, 1128, 50700], "temperature": 0.0, "avg_logprob": -0.2691830279780369, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.0008164509781636298}, {"id": 653, "seek": 452072, "start": 4527.4400000000005, "end": 4535.12, "text": " negative. Oh, don't be so sure about that. If they're saying we're always on the edge of being", "tokens": [50700, 3671, 13, 876, 11, 500, 380, 312, 370, 988, 466, 300, 13, 759, 436, 434, 1566, 321, 434, 1009, 322, 264, 4691, 295, 885, 51084], "temperature": 0.0, "avg_logprob": -0.2691830279780369, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.0008164509781636298}, {"id": 654, "seek": 452072, "start": 4535.12, "end": 4541.04, "text": " canceled for stating like mathematical facts. So yeah, but I mean, if they're saying that", "tokens": [51084, 24839, 337, 26688, 411, 18894, 9130, 13, 407, 1338, 11, 457, 286, 914, 11, 498, 436, 434, 1566, 300, 51380], "temperature": 0.0, "avg_logprob": -0.2691830279780369, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.0008164509781636298}, {"id": 655, "seek": 452072, "start": 4542.16, "end": 4548.56, "text": " what we're doing now, we're not we'll never get us to AGI. That's not negative. You're saying we", "tokens": [51436, 437, 321, 434, 884, 586, 11, 321, 434, 406, 321, 603, 1128, 483, 505, 281, 316, 26252, 13, 663, 311, 406, 3671, 13, 509, 434, 1566, 321, 51756], "temperature": 0.0, "avg_logprob": -0.2691830279780369, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.0008164509781636298}, {"id": 656, "seek": 454856, "start": 4549.360000000001, "end": 4554.8, "text": " need something else, we need something more. Or yours, yours, look, it could have saved us,", "tokens": [50404, 643, 746, 1646, 11, 321, 643, 746, 544, 13, 1610, 6342, 11, 6342, 11, 574, 11, 309, 727, 362, 6624, 505, 11, 50676], "temperature": 0.0, "avg_logprob": -0.18626604688928483, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.0062832823023200035}, {"id": 657, "seek": 454856, "start": 4554.8, "end": 4561.04, "text": " they use this phrase, money down the drain. Autonomous driving is a case, is a good case.", "tokens": [50676, 436, 764, 341, 9535, 11, 1460, 760, 264, 12339, 13, 6049, 12481, 563, 4840, 307, 257, 1389, 11, 307, 257, 665, 1389, 13, 50988], "temperature": 0.0, "avg_logprob": -0.18626604688928483, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.0062832823023200035}, {"id": 658, "seek": 454856, "start": 4562.4800000000005, "end": 4567.200000000001, "text": " Billions, we're talking non trivial money guys, we're talking more than the budgets of some", "tokens": [51060, 5477, 626, 11, 321, 434, 1417, 2107, 26703, 1460, 1074, 11, 321, 434, 1417, 544, 813, 264, 26708, 295, 512, 51296], "temperature": 0.0, "avg_logprob": -0.18626604688928483, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.0062832823023200035}, {"id": 659, "seek": 454856, "start": 4567.200000000001, "end": 4575.360000000001, "text": " European countries. Just imagine the scale. And those guys went bust, right? Why? Because", "tokens": [51296, 6473, 3517, 13, 1449, 3811, 264, 4373, 13, 400, 729, 1074, 1437, 19432, 11, 558, 30, 1545, 30, 1436, 51704], "temperature": 0.0, "avg_logprob": -0.18626604688928483, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.0062832823023200035}, {"id": 660, "seek": 457536, "start": 4575.44, "end": 4580.88, "text": " they trivialized the autonomous truck. An autonomous car is an autonomous agent, guys.", "tokens": [50368, 436, 26703, 1602, 264, 23797, 5898, 13, 1107, 23797, 1032, 307, 364, 23797, 9461, 11, 1074, 13, 50640], "temperature": 0.0, "avg_logprob": -0.12841996585621554, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.0034289094619452953}, {"id": 661, "seek": 457536, "start": 4581.839999999999, "end": 4588.639999999999, "text": " It's an it's an agent trying to reason in a dynamic and uncertain environment and has on the", "tokens": [50688, 467, 311, 364, 309, 311, 364, 9461, 1382, 281, 1778, 294, 257, 8546, 293, 11308, 2823, 293, 575, 322, 264, 51028], "temperature": 0.0, "avg_logprob": -0.12841996585621554, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.0034289094619452953}, {"id": 662, "seek": 457536, "start": 4588.639999999999, "end": 4594.32, "text": " fly to change to do belief revision, change its strategy, because of something new that came up.", "tokens": [51028, 3603, 281, 1319, 281, 360, 7107, 34218, 11, 1319, 1080, 5206, 11, 570, 295, 746, 777, 300, 1361, 493, 13, 51312], "temperature": 0.0, "avg_logprob": -0.12841996585621554, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.0034289094619452953}, {"id": 663, "seek": 457536, "start": 4595.04, "end": 4600.16, "text": " All of that is from seeing the tree and the stop sign. It's all vision.", "tokens": [51348, 1057, 295, 300, 307, 490, 2577, 264, 4230, 293, 264, 1590, 1465, 13, 467, 311, 439, 5201, 13, 51604], "temperature": 0.0, "avg_logprob": -0.12841996585621554, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.0034289094619452953}, {"id": 664, "seek": 460016, "start": 4600.96, "end": 4605.28, "text": " Yeah, so this is actually encouraging because now for all the Uber drivers out there and", "tokens": [50404, 865, 11, 370, 341, 307, 767, 14580, 570, 586, 337, 439, 264, 21839, 11590, 484, 456, 293, 50620], "temperature": 0.0, "avg_logprob": -0.15797059800889757, "compression_ratio": 1.4892703862660943, "no_speech_prob": 0.024416621774435043}, {"id": 665, "seek": 460016, "start": 4605.28, "end": 4612.32, "text": " long hauled truck drivers, your job is safe. Like it's not going to be replaced anytime soon.", "tokens": [50620, 938, 21167, 292, 5898, 11590, 11, 428, 1691, 307, 3273, 13, 1743, 309, 311, 406, 516, 281, 312, 10772, 13038, 2321, 13, 50972], "temperature": 0.0, "avg_logprob": -0.15797059800889757, "compression_ratio": 1.4892703862660943, "no_speech_prob": 0.024416621774435043}, {"id": 666, "seek": 460016, "start": 4612.32, "end": 4616.88, "text": " Yeah, it's amazing. And a few years back when I was still in the valley,", "tokens": [50972, 865, 11, 309, 311, 2243, 13, 400, 257, 1326, 924, 646, 562, 286, 390, 920, 294, 264, 17636, 11, 51200], "temperature": 0.0, "avg_logprob": -0.15797059800889757, "compression_ratio": 1.4892703862660943, "no_speech_prob": 0.024416621774435043}, {"id": 667, "seek": 460016, "start": 4618.16, "end": 4625.2, "text": " yeah, I was in Mecca in Silicon Valley. And I would talk to superb PhDs in neuroscience and", "tokens": [51264, 1338, 11, 286, 390, 294, 1923, 22394, 294, 25351, 10666, 13, 400, 286, 576, 751, 281, 36617, 14476, 82, 294, 42762, 293, 51616], "temperature": 0.0, "avg_logprob": -0.15797059800889757, "compression_ratio": 1.4892703862660943, "no_speech_prob": 0.024416621774435043}, {"id": 668, "seek": 462520, "start": 4625.92, "end": 4632.08, "text": " big AI engineers at top companies. They were so excited that we're at level four", "tokens": [50400, 955, 7318, 11955, 412, 1192, 3431, 13, 814, 645, 370, 2919, 300, 321, 434, 412, 1496, 1451, 50708], "temperature": 0.0, "avg_logprob": -0.09098294803074428, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.00546798063442111}, {"id": 669, "seek": 462520, "start": 4632.08, "end": 4636.639999999999, "text": " in a year or two. And this was six years ago. I say, guys, this will not happen. They say,", "tokens": [50708, 294, 257, 1064, 420, 732, 13, 400, 341, 390, 2309, 924, 2057, 13, 286, 584, 11, 1074, 11, 341, 486, 406, 1051, 13, 814, 584, 11, 50936], "temperature": 0.0, "avg_logprob": -0.09098294803074428, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.00546798063442111}, {"id": 670, "seek": 462520, "start": 4636.639999999999, "end": 4642.08, "text": " why are you so negative? I said, you cannot have an autonomous agent on the road without", "tokens": [50936, 983, 366, 291, 370, 3671, 30, 286, 848, 11, 291, 2644, 362, 364, 23797, 9461, 322, 264, 3060, 1553, 51208], "temperature": 0.0, "avg_logprob": -0.09098294803074428, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.00546798063442111}, {"id": 671, "seek": 462520, "start": 4642.08, "end": 4646.96, "text": " solving the frame problem. How do I revise everything I know, because of this new event?", "tokens": [51208, 12606, 264, 3920, 1154, 13, 1012, 360, 286, 44252, 1203, 286, 458, 11, 570, 295, 341, 777, 2280, 30, 51452], "temperature": 0.0, "avg_logprob": -0.09098294803074428, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.00546798063442111}, {"id": 672, "seek": 462520, "start": 4648.24, "end": 4652.88, "text": " And this has to happen real time. We don't have a solution for the frame problem.", "tokens": [51516, 400, 341, 575, 281, 1051, 957, 565, 13, 492, 500, 380, 362, 257, 3827, 337, 264, 3920, 1154, 13, 51748], "temperature": 0.0, "avg_logprob": -0.09098294803074428, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.00546798063442111}, {"id": 673, "seek": 465288, "start": 4652.88, "end": 4659.2, "text": " All you're doing is you have cars on a railway. We have autonomous cars now. It's called the train.", "tokens": [50364, 1057, 291, 434, 884, 307, 291, 362, 5163, 322, 257, 25812, 13, 492, 362, 23797, 5163, 586, 13, 467, 311, 1219, 264, 3847, 13, 50680], "temperature": 0.0, "avg_logprob": -0.14290057653668284, "compression_ratio": 1.675257731958763, "no_speech_prob": 0.008306448347866535}, {"id": 674, "seek": 465288, "start": 4662.0, "end": 4668.32, "text": " We have autonomous flight. If I'm not in a dynamic and uncertain environment reasoning,", "tokens": [50820, 492, 362, 23797, 7018, 13, 759, 286, 478, 406, 294, 257, 8546, 293, 11308, 2823, 21577, 11, 51136], "temperature": 0.0, "avg_logprob": -0.14290057653668284, "compression_ratio": 1.675257731958763, "no_speech_prob": 0.008306448347866535}, {"id": 675, "seek": 465288, "start": 4668.96, "end": 4674.08, "text": " yeah, I can have autonomous anything. We call it the railway. I mean,", "tokens": [51168, 1338, 11, 286, 393, 362, 23797, 1340, 13, 492, 818, 309, 264, 25812, 13, 286, 914, 11, 51424], "temperature": 0.0, "avg_logprob": -0.14290057653668284, "compression_ratio": 1.675257731958763, "no_speech_prob": 0.008306448347866535}, {"id": 676, "seek": 465288, "start": 4675.2, "end": 4678.8, "text": " we call it Amtrak. It's autonomous. You press a button and it goes.", "tokens": [51480, 321, 818, 309, 2012, 83, 11272, 13, 467, 311, 23797, 13, 509, 1886, 257, 2960, 293, 309, 1709, 13, 51660], "temperature": 0.0, "avg_logprob": -0.14290057653668284, "compression_ratio": 1.675257731958763, "no_speech_prob": 0.008306448347866535}, {"id": 677, "seek": 467880, "start": 4679.76, "end": 4683.2, "text": " If we're talking about reasoning in the streets of San Francisco,", "tokens": [50412, 759, 321, 434, 1417, 466, 21577, 294, 264, 8481, 295, 5271, 12279, 11, 50584], "temperature": 0.0, "avg_logprob": -0.14388382144090606, "compression_ratio": 1.535, "no_speech_prob": 0.003171942662447691}, {"id": 678, "seek": 467880, "start": 4684.400000000001, "end": 4687.360000000001, "text": " you have to face the frame problem or you will kill people.", "tokens": [50644, 291, 362, 281, 1851, 264, 3920, 1154, 420, 291, 486, 1961, 561, 13, 50792], "temperature": 0.0, "avg_logprob": -0.14388382144090606, "compression_ratio": 1.535, "no_speech_prob": 0.003171942662447691}, {"id": 679, "seek": 467880, "start": 4689.84, "end": 4696.72, "text": " Anyway, on that sobering note, I think a lot of stuff goes unnoticed in San Francisco,", "tokens": [50916, 5684, 11, 322, 300, 26212, 278, 3637, 11, 286, 519, 257, 688, 295, 1507, 1709, 517, 2247, 4233, 294, 5271, 12279, 11, 51260], "temperature": 0.0, "avg_logprob": -0.14388382144090606, "compression_ratio": 1.535, "no_speech_prob": 0.003171942662447691}, {"id": 680, "seek": 467880, "start": 4696.72, "end": 4702.24, "text": " so I'm not sure about that. Probably that's the least of them. No, but the scale of money that", "tokens": [51260, 370, 286, 478, 406, 988, 466, 300, 13, 9210, 300, 311, 264, 1935, 295, 552, 13, 883, 11, 457, 264, 4373, 295, 1460, 300, 51536], "temperature": 0.0, "avg_logprob": -0.14388382144090606, "compression_ratio": 1.535, "no_speech_prob": 0.003171942662447691}, {"id": 681, "seek": 470224, "start": 4702.32, "end": 4709.36, "text": " went the scale. This is the value of this book, the scale of investment. I mean, if 10% of that", "tokens": [50368, 1437, 264, 4373, 13, 639, 307, 264, 2158, 295, 341, 1446, 11, 264, 4373, 295, 6078, 13, 286, 914, 11, 498, 1266, 4, 295, 300, 50720], "temperature": 0.0, "avg_logprob": -0.14737416257952698, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.14989061653614044}, {"id": 682, "seek": 470224, "start": 4709.36, "end": 4715.36, "text": " was put on another approach, hey, you weird guy with this weird idea, take 10% of what we're", "tokens": [50720, 390, 829, 322, 1071, 3109, 11, 4177, 11, 291, 3657, 2146, 365, 341, 3657, 1558, 11, 747, 1266, 4, 295, 437, 321, 434, 51020], "temperature": 0.0, "avg_logprob": -0.14737416257952698, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.14989061653614044}, {"id": 683, "seek": 470224, "start": 4715.36, "end": 4723.679999999999, "text": " throwing down the drain and explore something else. Show me. That's where I'm at, too.", "tokens": [51020, 10238, 760, 264, 12339, 293, 6839, 746, 1646, 13, 6895, 385, 13, 663, 311, 689, 286, 478, 412, 11, 886, 13, 51436], "temperature": 0.0, "avg_logprob": -0.14737416257952698, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.14989061653614044}, {"id": 684, "seek": 470224, "start": 4724.96, "end": 4731.28, "text": " Diversify the effort. There's a huge impact here, societal impact. We're wasting billions of dollars", "tokens": [51500, 413, 1762, 2505, 264, 4630, 13, 821, 311, 257, 2603, 2712, 510, 11, 33472, 2712, 13, 492, 434, 20457, 17375, 295, 3808, 51816], "temperature": 0.0, "avg_logprob": -0.14737416257952698, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.14989061653614044}, {"id": 685, "seek": 473128, "start": 4731.28, "end": 4737.2, "text": " just because I don't want to listen to anyone else. It happened in the chatbot industry, which", "tokens": [50364, 445, 570, 286, 500, 380, 528, 281, 2140, 281, 2878, 1646, 13, 467, 2011, 294, 264, 5081, 18870, 3518, 11, 597, 50660], "temperature": 0.0, "avg_logprob": -0.14635932568422297, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0030746632255613804}, {"id": 686, "seek": 473128, "start": 4737.2, "end": 4743.04, "text": " I'm more familiar with than autonomous driving. Chatbot this, chatbot that, and there was an", "tokens": [50660, 286, 478, 544, 4963, 365, 813, 23797, 4840, 13, 27503, 18870, 341, 11, 5081, 18870, 300, 11, 293, 456, 390, 364, 50952], "temperature": 0.0, "avg_logprob": -0.14635932568422297, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0030746632255613804}, {"id": 687, "seek": 473128, "start": 4743.04, "end": 4750.88, "text": " explosion. It was like a blob, like the internet thing. Now we can't get away from them. Every", "tokens": [50952, 15673, 13, 467, 390, 411, 257, 46115, 11, 411, 264, 4705, 551, 13, 823, 321, 393, 380, 483, 1314, 490, 552, 13, 2048, 51344], "temperature": 0.0, "avg_logprob": -0.14635932568422297, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0030746632255613804}, {"id": 688, "seek": 473128, "start": 4750.88, "end": 4759.44, "text": " website we go to, it's like, leave me alone. Nobody wants to use them because we know how", "tokens": [51344, 3144, 321, 352, 281, 11, 309, 311, 411, 11, 1856, 385, 3312, 13, 9297, 2738, 281, 764, 552, 570, 321, 458, 577, 51772], "temperature": 0.0, "avg_logprob": -0.14635932568422297, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0030746632255613804}, {"id": 689, "seek": 475944, "start": 4759.44, "end": 4768.16, "text": " they work. These are stochastic parents. Yeah, literally just going to a FAC and hitting control", "tokens": [50364, 436, 589, 13, 1981, 366, 342, 8997, 2750, 3152, 13, 865, 11, 3736, 445, 516, 281, 257, 479, 4378, 293, 8850, 1969, 50800], "temperature": 0.0, "avg_logprob": -0.20130390734285922, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.0028884068597108126}, {"id": 690, "seek": 475944, "start": 4768.16, "end": 4776.5599999999995, "text": " F is more effective for me than trying to interact with a chatbot. The search engines by key phrases", "tokens": [50800, 479, 307, 544, 4942, 337, 385, 813, 1382, 281, 4648, 365, 257, 5081, 18870, 13, 440, 3164, 12982, 538, 2141, 20312, 51220], "temperature": 0.0, "avg_logprob": -0.20130390734285922, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.0028884068597108126}, {"id": 691, "seek": 475944, "start": 4776.5599999999995, "end": 4780.879999999999, "text": " you put and they bring you a link and they say, read this. This is your answer. They are search", "tokens": [51220, 291, 829, 293, 436, 1565, 291, 257, 2113, 293, 436, 584, 11, 1401, 341, 13, 639, 307, 428, 1867, 13, 814, 366, 3164, 51436], "temperature": 0.0, "avg_logprob": -0.20130390734285922, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.0028884068597108126}, {"id": 692, "seek": 478088, "start": 4780.88, "end": 4787.04, "text": " engines basically. But again, the amount of money, because I lived in that industry,", "tokens": [50364, 12982, 1936, 13, 583, 797, 11, 264, 2372, 295, 1460, 11, 570, 286, 5152, 294, 300, 3518, 11, 50672], "temperature": 0.0, "avg_logprob": -0.11733664235761089, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.009699695743620396}, {"id": 693, "seek": 478088, "start": 4788.32, "end": 4796.56, "text": " the amount of money spent on chatbots will scare the hell out of anybody. You combine that with", "tokens": [50736, 264, 2372, 295, 1460, 4418, 322, 5081, 65, 1971, 486, 17185, 264, 4921, 484, 295, 4472, 13, 509, 10432, 300, 365, 51148], "temperature": 0.0, "avg_logprob": -0.11733664235761089, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.009699695743620396}, {"id": 694, "seek": 478088, "start": 4796.56, "end": 4804.400000000001, "text": " autonomous driving, both dead, almost zero. We're talking billions and billions and billions. And", "tokens": [51148, 23797, 4840, 11, 1293, 3116, 11, 1920, 4018, 13, 492, 434, 1417, 17375, 293, 17375, 293, 17375, 13, 400, 51540], "temperature": 0.0, "avg_logprob": -0.11733664235761089, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.009699695743620396}, {"id": 695, "seek": 478088, "start": 4804.400000000001, "end": 4810.0, "text": " you talk to any one of them in the highest, in the middle of the fever. They won't listen to you.", "tokens": [51540, 291, 751, 281, 604, 472, 295, 552, 294, 264, 6343, 11, 294, 264, 2808, 295, 264, 18277, 13, 814, 1582, 380, 2140, 281, 291, 13, 51820], "temperature": 0.0, "avg_logprob": -0.11733664235761089, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.009699695743620396}, {"id": 696, "seek": 481000, "start": 4810.0, "end": 4817.84, "text": " I have people now calling me back and saying, you were right. Yeah, after $200 billion. So,", "tokens": [50364, 286, 362, 561, 586, 5141, 385, 646, 293, 1566, 11, 291, 645, 558, 13, 865, 11, 934, 1848, 7629, 5218, 13, 407, 11, 50756], "temperature": 0.0, "avg_logprob": -0.11293130171926398, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0006161745404824615}, {"id": 697, "seek": 481000, "start": 4817.84, "end": 4824.08, "text": " science is important. Engineering is important, but science is important too. That's where the", "tokens": [50756, 3497, 307, 1021, 13, 16215, 307, 1021, 11, 457, 3497, 307, 1021, 886, 13, 663, 311, 689, 264, 51068], "temperature": 0.0, "avg_logprob": -0.11293130171926398, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0006161745404824615}, {"id": 698, "seek": 481000, "start": 4824.08, "end": 4831.68, "text": " value of this book is. Guys, hacking alone will not do the whole thing. You're a bright engineer,", "tokens": [51068, 2158, 295, 341, 1446, 307, 13, 7855, 11, 31422, 3312, 486, 406, 360, 264, 1379, 551, 13, 509, 434, 257, 4730, 11403, 11, 51448], "temperature": 0.0, "avg_logprob": -0.11293130171926398, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0006161745404824615}, {"id": 699, "seek": 481000, "start": 4831.68, "end": 4836.08, "text": " you can hack your way through what we know is true. That's the space you can play with.", "tokens": [51448, 291, 393, 10339, 428, 636, 807, 437, 321, 458, 307, 2074, 13, 663, 311, 264, 1901, 291, 393, 862, 365, 13, 51668], "temperature": 0.0, "avg_logprob": -0.11293130171926398, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0006161745404824615}, {"id": 700, "seek": 483608, "start": 4836.32, "end": 4845.92, "text": " You cannot hack your way in a bigger set of possibilities that are. You didn't verify that", "tokens": [50376, 509, 2644, 10339, 428, 636, 294, 257, 3801, 992, 295, 12178, 300, 366, 13, 509, 994, 380, 16888, 300, 50856], "temperature": 0.0, "avg_logprob": -0.12633433796110607, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0006560911424458027}, {"id": 701, "seek": 483608, "start": 4845.92, "end": 4853.6, "text": " you can go there. An engineer can be creative within a Venn diagram that the scientists drew", "tokens": [50856, 291, 393, 352, 456, 13, 1107, 11403, 393, 312, 5880, 1951, 257, 691, 1857, 10686, 300, 264, 7708, 12804, 51240], "temperature": 0.0, "avg_logprob": -0.12633433796110607, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0006560911424458027}, {"id": 702, "seek": 483608, "start": 4853.6, "end": 4860.4, "text": " for them. That's the difference between science and engineering. The scientist draws the Venn", "tokens": [51240, 337, 552, 13, 663, 311, 264, 2649, 1296, 3497, 293, 7043, 13, 440, 12662, 20045, 264, 691, 1857, 51580], "temperature": 0.0, "avg_logprob": -0.12633433796110607, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0006560911424458027}, {"id": 703, "seek": 483608, "start": 4860.4, "end": 4865.84, "text": " diagram and that's the value of philosophers, at least the analytic philosophers, that know logic", "tokens": [51580, 10686, 293, 300, 311, 264, 2158, 295, 36839, 11, 412, 1935, 264, 40358, 36839, 11, 300, 458, 9952, 51852], "temperature": 0.0, "avg_logprob": -0.12633433796110607, "compression_ratio": 1.7201834862385321, "no_speech_prob": 0.0006560911424458027}, {"id": 704, "seek": 486584, "start": 4865.84, "end": 4871.92, "text": " and metaphysics and quantum mechanics and philosophers that are on the technical side.", "tokens": [50364, 293, 30946, 41732, 293, 13018, 12939, 293, 36839, 300, 366, 322, 264, 6191, 1252, 13, 50668], "temperature": 0.0, "avg_logprob": -0.11921217328026182, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.0003919769369531423}, {"id": 705, "seek": 486584, "start": 4872.72, "end": 4878.400000000001, "text": " They know how to draw the Venn diagram. You, as an engineer, if you're wasting your time here,", "tokens": [50708, 814, 458, 577, 281, 2642, 264, 691, 1857, 10686, 13, 509, 11, 382, 364, 11403, 11, 498, 291, 434, 20457, 428, 565, 510, 11, 50992], "temperature": 0.0, "avg_logprob": -0.11921217328026182, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.0003919769369531423}, {"id": 706, "seek": 486584, "start": 4879.28, "end": 4885.12, "text": " that's called money down the drain. Play inside the Venn diagram. Otherwise,", "tokens": [51036, 300, 311, 1219, 1460, 760, 264, 12339, 13, 5506, 1854, 264, 691, 1857, 10686, 13, 10328, 11, 51328], "temperature": 0.0, "avg_logprob": -0.11921217328026182, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.0003919769369531423}, {"id": 707, "seek": 486584, "start": 4885.92, "end": 4890.64, "text": " you're just an over enthused engineer who should go back and study computability.", "tokens": [51368, 291, 434, 445, 364, 670, 948, 71, 4717, 11403, 567, 820, 352, 646, 293, 2979, 2807, 2310, 13, 51604], "temperature": 0.0, "avg_logprob": -0.11921217328026182, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.0003919769369531423}, {"id": 708, "seek": 489064, "start": 4891.6, "end": 4895.76, "text": " It's kind of like how patent examiners can easily reject anything that comes in that", "tokens": [50412, 467, 311, 733, 295, 411, 577, 20495, 1139, 259, 433, 393, 3612, 8248, 1340, 300, 1487, 294, 300, 50620], "temperature": 0.0, "avg_logprob": -0.21080398559570312, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.013843359425663948}, {"id": 709, "seek": 489064, "start": 4895.76, "end": 4902.4800000000005, "text": " claims to violate the second law of thermodynamics, right? Well, listen, Wally, I think we", "tokens": [50620, 9441, 281, 37478, 264, 1150, 2101, 295, 8810, 35483, 11, 558, 30, 1042, 11, 2140, 11, 343, 379, 11, 286, 519, 321, 50956], "temperature": 0.0, "avg_logprob": -0.21080398559570312, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.013843359425663948}, {"id": 710, "seek": 489064, "start": 4902.4800000000005, "end": 4908.72, "text": " sincerely appreciate your time today. And also, Mark, thank you for joining us and asking great", "tokens": [50956, 30694, 4449, 428, 565, 965, 13, 400, 611, 11, 3934, 11, 1309, 291, 337, 5549, 505, 293, 3365, 869, 51268], "temperature": 0.0, "avg_logprob": -0.21080398559570312, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.013843359425663948}, {"id": 711, "seek": 489064, "start": 4908.72, "end": 4916.160000000001, "text": " questions. We should do this again. Thanks, Wally. Yeah, I really appreciate it. So thanks,", "tokens": [51268, 1651, 13, 492, 820, 360, 341, 797, 13, 2561, 11, 343, 379, 13, 865, 11, 286, 534, 4449, 309, 13, 407, 3231, 11, 51640], "temperature": 0.0, "avg_logprob": -0.21080398559570312, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.013843359425663948}, {"id": 712, "seek": 491616, "start": 4916.16, "end": 4921.76, "text": " always fun, guys. I see. Peace.", "tokens": [50364, 1009, 1019, 11, 1074, 13, 286, 536, 13, 13204, 13, 50644], "temperature": 0.0, "avg_logprob": -0.5784741181593674, "compression_ratio": 0.7948717948717948, "no_speech_prob": 0.11880725622177124}], "language": "en"}