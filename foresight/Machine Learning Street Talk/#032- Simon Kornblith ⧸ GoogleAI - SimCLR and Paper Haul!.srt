1
00:00:00,000 --> 00:00:14,600
Welcome back. Today we're talking with Dr Simon Cornblith, a research scientist in the

2
00:00:14,600 --> 00:00:20,640
Google Brain team. Simon is most famous for being one of the authors on Simclear, the computer

3
00:00:20,640 --> 00:00:25,360
vision paper that used self-supervised learning and the contrastive loss with loads of cool

4
00:00:25,360 --> 00:00:30,760
image augmentations. Simon also used to be a neuroscientist.

5
00:00:30,760 --> 00:00:35,920
When I was pretty young, I was interested in consciousness and how we create this kind

6
00:00:35,920 --> 00:00:41,640
of impression of the external world inside our heads. And so I guess it's pretty obvious

7
00:00:41,640 --> 00:00:46,560
how that translates into an interest in brains and how the brain works.

8
00:00:46,560 --> 00:00:52,240
Turns out the neuroscience is really difficult. Progress is really slow and tedious. Simon's

9
00:00:52,320 --> 00:00:58,160
goal is to understand the inner workings of neural networks, both in meat space and

10
00:00:58,160 --> 00:01:05,640
in silicon. He initially thought that the artificial variety might be easier to understand.

11
00:01:05,640 --> 00:01:07,360
He was in for a rude awakening.

12
00:01:07,360 --> 00:01:11,200
So in a neural network, we can record all the neurons, which is extremely challenging

13
00:01:11,200 --> 00:01:17,160
in a biological organism. And we can also manipulate the system in any kind of way that

14
00:01:17,160 --> 00:01:22,080
we can imagine. But it still seems really hard to understand neural networks. I think there

15
00:01:22,080 --> 00:01:28,800
are a lot of ideas from machine learning that will ultimately help us understand brains.

16
00:01:28,800 --> 00:01:33,920
Maybe we could make some headway that might eventually translate back to brains. And so

17
00:01:33,920 --> 00:01:35,800
that's how I ended up in machine learning.

18
00:01:35,800 --> 00:01:39,360
People often try and anthropomorphise neural networks.

19
00:01:39,360 --> 00:01:43,720
People try to relate whatever neural network they've built back to a brain and they say

20
00:01:43,720 --> 00:01:46,800
that it works like the brain, but it doesn't work like the brain.

21
00:01:46,800 --> 00:01:51,780
So Simon was involved in this paper, do wide and deep networks learn the same things, uncovering

22
00:01:51,780 --> 00:01:55,460
how neural network representations vary with width and depth.

23
00:01:55,460 --> 00:02:01,300
Simon pioneered this really fascinating way of comparing representations by comparing

24
00:02:01,300 --> 00:02:06,820
features. And what this essentially amounts to is we need to have a similarity function

25
00:02:06,820 --> 00:02:13,420
so that we can compare the representations in layers to themselves in different parts

26
00:02:13,420 --> 00:02:15,780
of the network or indeed to other networks.

27
00:02:15,780 --> 00:02:19,540
And so for this similarity measure to work well, the first thing Simon did was take two

28
00:02:19,540 --> 00:02:24,580
architecturally identical networks, A and B, trained from different random initialisations

29
00:02:24,580 --> 00:02:32,180
and just ensure that the third convolution layer is more self-similar to its counterpart

30
00:02:32,180 --> 00:02:34,460
than any of the other layers.

31
00:02:34,460 --> 00:02:38,420
If that works, then you're onto something. Turns out that's not super simple to do, but

32
00:02:38,420 --> 00:02:41,740
Simon came up with this concept called the centred kernel alignment, which we'll talk

33
00:02:41,740 --> 00:02:43,180
about on the call.

34
00:02:43,180 --> 00:02:49,300
But this is actually super fascinating. We're talking about this idea here of using self-similarity

35
00:02:49,300 --> 00:02:55,020
to reason about the evolution of representations throughout successive layers in the neural

36
00:02:55,020 --> 00:03:00,940
network. And what Simon found is that you get this kind of characteristic blockiness.

37
00:03:00,940 --> 00:03:06,380
So when you see these large blocks, what it means is that the representations are no longer

38
00:03:06,380 --> 00:03:08,460
evolving in respect of time.

39
00:03:08,460 --> 00:03:13,940
So it's showing here the representational similarity of all of the layers against themselves

40
00:03:13,940 --> 00:03:16,140
and against all of the other layers.

41
00:03:16,140 --> 00:03:20,260
So clearly there's this characteristic diagonal down the matrix, as you would see with any

42
00:03:20,260 --> 00:03:22,260
self-similarity matrix.

43
00:03:22,260 --> 00:03:27,700
And because this blockiness appears, it means that nothing is happening.

44
00:03:27,700 --> 00:03:31,620
And what Simon realised is you can actually delete these layers from the neural network

45
00:03:31,620 --> 00:03:35,100
and it wouldn't make any difference because it hasn't learned anything new.

46
00:03:35,100 --> 00:03:39,900
But it's also a really interesting way of reasoning about a kind of pathology, a weird

47
00:03:39,900 --> 00:03:43,060
thing that happens when you saturate a neural network.

48
00:03:43,060 --> 00:03:47,100
So he said that this presence of this block structure is an indicator of the halting of

49
00:03:47,100 --> 00:03:51,420
evolution and a strong indicator of over-parameterisation.

50
00:03:51,420 --> 00:03:56,020
And he actually shows that this blockiness appears on deeper networks and wider networks.

51
00:03:56,020 --> 00:03:59,220
But this concept of self-similarity analysis is not new to me.

52
00:03:59,220 --> 00:04:06,060
On my PhD, I was fascinated in segmenting DJ-mixed music shows and I actually used the same techniques

53
00:04:06,060 --> 00:04:10,260
for learning regimes in financial datasets later on.

54
00:04:10,620 --> 00:04:13,900
This is an example of a DJ-mix which I segmented.

55
00:04:13,900 --> 00:04:19,260
I came up with a dynamic programming algorithm which would essentially sum up all of the

56
00:04:19,260 --> 00:04:25,260
tiles along this diagonal and compute the lowest costs contiguous segmentation.

57
00:04:25,260 --> 00:04:26,580
And it's super interesting.

58
00:04:26,580 --> 00:04:31,260
So here are two music tracks and you can see that they are more self-similar to each other

59
00:04:31,260 --> 00:04:35,820
than they are any of the other tracks just because of the tone of the colour here.

60
00:04:35,820 --> 00:04:39,700
And if you zoom into a track, you can even see that there are symmetries.

61
00:04:39,700 --> 00:04:44,420
This part of the track here is a repetition from this part of the track here.

62
00:04:44,420 --> 00:04:49,420
And you can tell that from this kind of symmetry pattern on the diagonal.

63
00:04:49,420 --> 00:04:52,780
And you can see that there's a little bit in the track in the middle here which is not

64
00:04:52,780 --> 00:04:55,740
similar to any other part of the track.

65
00:04:55,740 --> 00:05:00,660
You see some really interesting stuff here and essentially I'm a huge fan of anyone using

66
00:05:00,660 --> 00:05:05,420
self-similarity matrices for reasoning about the evolution of representations.

67
00:05:05,420 --> 00:05:07,140
I think it's a fascinating idea.

68
00:05:07,140 --> 00:05:09,980
So how did Simon come up with this measure of similarity?

69
00:05:09,980 --> 00:05:12,580
The centred kernel alignment.

70
00:05:12,580 --> 00:05:17,900
Jeff Hinton had another idea and I tried the idea that it worked but then we wondered is

71
00:05:17,900 --> 00:05:20,540
there a simpler thing that worked.

72
00:05:20,540 --> 00:05:23,140
And that's how we ended up with centred kernel alignment.

73
00:05:23,140 --> 00:05:27,940
The blockiness in these matrices is absolutely fascinating but how much can we read into

74
00:05:27,940 --> 00:05:28,940
it?

75
00:05:28,940 --> 00:05:33,460
It's not clear what we should really expect in terms of how a neural network representation

76
00:05:33,460 --> 00:05:35,060
evolves through the layers.

77
00:05:35,060 --> 00:05:40,260
I think there's kind of some theory on what we should expect if all the layers are linear.

78
00:05:40,260 --> 00:05:44,740
But like obviously the neural networks that we train are nonlinear and it's really important

79
00:05:44,740 --> 00:05:47,340
to have a nonlinearity in between the layers.

80
00:05:47,340 --> 00:05:52,780
If we see that nothing is changing from one layer to the next that's a really bad sign.

81
00:05:52,780 --> 00:05:56,860
If the neural network representation isn't changing then obviously nothing's happening.

82
00:05:56,860 --> 00:06:01,540
We couldn't have predicted this ahead of time based on what we know about neural network

83
00:06:01,620 --> 00:06:06,940
theory and we couldn't have predicted it ahead of time based on the accuracy of the network.

84
00:06:06,940 --> 00:06:08,740
Does this apply to ResNets though?

85
00:06:08,740 --> 00:06:11,460
I thought that they could learn their own capacity.

86
00:06:11,460 --> 00:06:15,780
You can either look at networks without residual connections where you do actually find that

87
00:06:15,780 --> 00:06:21,580
at some depth the accuracy will start going down and in networks without residual connections

88
00:06:21,580 --> 00:06:28,140
we find that the depth where accuracy starts to go down is like around the same depth where

89
00:06:28,180 --> 00:06:33,020
you begin seeing this kind of block structure where many successive layers have similar

90
00:06:33,020 --> 00:06:37,780
representations and it looks like the representation is no longer getting refined through the network.

91
00:06:37,780 --> 00:06:42,420
Once you start getting these blocks making the network deeper, making the network wider

92
00:06:42,420 --> 00:06:45,660
no longer really gives you any improvement in accuracy.

93
00:06:45,660 --> 00:06:50,220
So it seems like this is basically telling you that the network has fit the data as much

94
00:06:50,220 --> 00:06:56,820
as it can and there's no real advantage to using something bigger.

95
00:06:56,820 --> 00:07:02,020
Next we move on to Simon's paper about using different loss functions on image classifiers

96
00:07:02,020 --> 00:07:05,980
and he made some really interesting findings actually so the loss functions only really

97
00:07:05,980 --> 00:07:10,780
seem to affect the penultimate layers in the neural network.

98
00:07:10,780 --> 00:07:14,580
This also gives us some pretty useful insight into transfer learning.

99
00:07:14,580 --> 00:07:19,500
The last third of the network is setting up the penultimate layer representation in a

100
00:07:19,500 --> 00:07:24,700
way that is good for your loss function but the first two thirds of the network are somehow

101
00:07:24,860 --> 00:07:27,420
just learning general features.

102
00:07:27,420 --> 00:07:32,180
I think this also corresponds with the success of transfer learning where we can take features

103
00:07:32,180 --> 00:07:36,260
that we've learned on one task and transfer them to some other task.

104
00:07:36,260 --> 00:07:37,980
What's the implication though?

105
00:07:37,980 --> 00:07:44,460
It seems, is the implication that the loss function is not having any impact on the representations

106
00:07:44,460 --> 00:07:45,660
early on in the network?

107
00:07:45,660 --> 00:07:49,340
That seems like quite a big implication.

108
00:07:49,340 --> 00:07:52,460
Ultimately we're asking the network to do the same thing just in a slightly different

109
00:07:52,460 --> 00:07:53,460
way.

110
00:07:53,500 --> 00:07:59,940
Like some inverse correlation between the gains you get from a loss function and how

111
00:07:59,940 --> 00:08:01,900
good it is for transfer learning.

112
00:08:01,900 --> 00:08:09,900
If you use loss functions that give you higher accuracy on ImageNet, you tend to learn representations

113
00:08:09,900 --> 00:08:14,060
that transfer substantially worse in that setting.

114
00:08:14,060 --> 00:08:19,100
The loss functions that perform better lead classes to become more separated in the penultimate

115
00:08:19,100 --> 00:08:20,100
layers.

116
00:08:20,140 --> 00:08:25,340
To standard softmax loss, actually the classes are not that separated from each other in

117
00:08:25,340 --> 00:08:28,180
the penultimate layer representation.

118
00:08:28,180 --> 00:08:33,780
Right now on whatever TensorFlow Hub or Hugging Face repositories and so on, we have these

119
00:08:33,780 --> 00:08:38,660
pre-trend models and the pre-trend models, they're like full stack models and people

120
00:08:38,660 --> 00:08:46,820
usually take some sort of last or next to last hidden layer but maybe we should much

121
00:08:46,900 --> 00:08:52,820
more focus on actually providing like half of a network to share, like determining which

122
00:08:52,820 --> 00:08:58,220
are actually the best, good or general representations from a data set and so on.

123
00:08:58,220 --> 00:09:00,060
It's a really interesting question.

124
00:09:00,060 --> 00:09:04,740
If we just want to turn an image into a vector that we could then train a linear classifier

125
00:09:04,740 --> 00:09:07,900
on top of, what is the best way of doing that?

126
00:09:07,900 --> 00:09:12,340
Self-supervised pre-training, just like word vectors, gives us a really great starting

127
00:09:12,340 --> 00:09:17,780
point to vectorize an image into a semantically relevant geometric space.

128
00:09:17,780 --> 00:09:21,820
It's been a real game changer in the computer vision world since about 2018.

129
00:09:21,820 --> 00:09:25,900
We want that neural network to learn a representation such that when we then just train a linear

130
00:09:25,900 --> 00:09:30,860
classifier on top of that representation to classify image net, it's going to do well.

131
00:09:30,860 --> 00:09:35,780
But we want to learn the initial representation without using any kind of labels.

132
00:09:35,780 --> 00:09:38,780
So what is self-supervised pre-training for vision?

133
00:09:38,780 --> 00:09:42,460
People came up with these kinds of tasks that you could try to train a neural network to

134
00:09:42,460 --> 00:09:46,260
do so that it would learn some kind of good representation.

135
00:09:46,260 --> 00:09:53,140
You're trying to learn some kind of representation space where you've got different patches from

136
00:09:53,140 --> 00:09:57,340
an image or different augmentations from an image, just different representations of the

137
00:09:57,340 --> 00:10:02,940
same image and you want to learn a representation space where these representations of the same

138
00:10:02,940 --> 00:10:08,740
image are all close together in that representation space and they're far apart from the representations

139
00:10:09,380 --> 00:10:11,340
of other images.

140
00:10:11,340 --> 00:10:16,220
This surprisingly seems to lead to very good representations.

141
00:10:16,220 --> 00:10:23,060
I was very fascinated by all of these different tricks that you apparently have to get and

142
00:10:23,060 --> 00:10:28,140
so big kudos to figuring all of this out for the rest of us.

143
00:10:28,140 --> 00:10:31,500
Data augmentation is absolutely key to making this work.

144
00:10:31,500 --> 00:10:35,300
The important part of the recipe is data augmentation.

145
00:10:35,300 --> 00:10:40,380
There are really only two super important data augmentations that we need.

146
00:10:40,380 --> 00:10:45,860
So we have to take two different crops from the same image and then we have to do some

147
00:10:45,860 --> 00:10:48,020
kind of color distortion.

148
00:10:48,020 --> 00:10:50,380
Turns out though the architecture isn't that important.

149
00:10:50,380 --> 00:10:56,780
You don't have to worry about architecture, engineering specifically or contrastive learning.

150
00:10:56,780 --> 00:10:58,660
What was new in the CIM CLR paper?

151
00:10:58,660 --> 00:11:04,060
We introduced the idea of this projection head in CIM Clear and we also spend a lot

152
00:11:04,060 --> 00:11:06,220
of time studying the augmentation.

153
00:11:06,220 --> 00:11:08,340
And what about the bring your own latent paper?

154
00:11:08,340 --> 00:11:18,380
I don't really have any insight into how either BYOL or the more recent papers actually

155
00:11:18,380 --> 00:11:21,860
are learning a representation that doesn't end up collapsing.

156
00:11:21,860 --> 00:11:26,700
Why it doesn't happen relates to some mysteries about neural network training dynamics that

157
00:11:26,700 --> 00:11:28,940
we still don't entirely understand.

158
00:11:28,940 --> 00:11:31,620
We dive deep into data augmentation in general.

159
00:11:31,620 --> 00:11:35,940
The data augmentation that you need for contrastive learning is different from the data augmentation

160
00:11:35,940 --> 00:11:38,980
that you need for supervised learning because the task is different.

161
00:11:38,980 --> 00:11:44,220
When you have contrastive learning, you have this problem that if there's just one feature

162
00:11:44,220 --> 00:11:50,580
in your data that can be used to do the contrastive task to get images of the same example or

163
00:11:50,580 --> 00:11:55,020
views of the same example close together and far apart from views of all the other examples.

164
00:11:55,020 --> 00:11:58,980
If you could do that with one feature, that would be the only feature the network would

165
00:11:58,980 --> 00:12:02,660
ever learn or it might be the only feature the network would ever learn.

166
00:12:02,660 --> 00:12:06,780
And so with the augmentation, you're making the task harder so that the network actually

167
00:12:06,780 --> 00:12:10,420
has to learn many different kinds of features.

168
00:12:10,420 --> 00:12:16,580
We find that this color distortion actually is very important for self-supervised learning,

169
00:12:16,580 --> 00:12:21,300
for contrastive learning, or as it doesn't really matter for supervised learning.

170
00:12:21,300 --> 00:12:26,420
There seems to be this fascinating universality of representations, especially in vision.

171
00:12:26,420 --> 00:12:30,740
I'm not trying to be flippant when I say this because practitioners have used ImageNet

172
00:12:30,740 --> 00:12:32,740
on a variety of downstream tasks.

173
00:12:32,740 --> 00:12:36,580
For example, they might use it for classifying circuit boards or something.

174
00:12:36,580 --> 00:12:39,300
And the miraculous thing is it just seems to work quite well.

175
00:12:39,300 --> 00:12:43,500
So do you think in your opinion that there is some kind of universality?

176
00:12:43,500 --> 00:12:49,100
I'm very skeptical about universality of ImageNet for different tasks.

177
00:12:49,100 --> 00:12:52,980
Even though there are lots of cars in ImageNet, if you pre-train on ImageNet and you fine

178
00:12:52,980 --> 00:12:58,500
tune on that data set, you will learn to classify it faster and fewer steps than if you had

179
00:12:58,500 --> 00:13:03,100
trained from scratch on the Stanford cars data set.

180
00:13:03,100 --> 00:13:06,940
But you won't actually perform any better at the end.

181
00:13:06,940 --> 00:13:09,860
Representations of images are not that universal.

182
00:13:09,860 --> 00:13:16,140
And at least what works for natural images like those in ImageNet may not work on other

183
00:13:16,140 --> 00:13:17,780
data sets.

184
00:13:17,780 --> 00:13:25,860
Taking a bit from the universality of representations to the universality of augmentations, since

185
00:13:25,860 --> 00:13:32,620
this is such a crucial part, do you think that there is a systematic way how we can

186
00:13:32,620 --> 00:13:35,060
discover augmentations?

187
00:13:35,060 --> 00:13:37,340
Right now, it seems to be kind of a whack-a-mole, right?

188
00:13:37,340 --> 00:13:38,340
It's okay.

189
00:13:38,340 --> 00:13:40,300
We just feed images and say, no, that's too easy.

190
00:13:40,300 --> 00:13:41,300
We crop them.

191
00:13:41,300 --> 00:13:42,580
Oh, no, it's the color histogram.

192
00:13:42,580 --> 00:13:45,420
So we like whack on the color and then it works better.

193
00:13:45,500 --> 00:13:50,740
Maybe someone finds out, oh, there is still this easy feature that the network, every

194
00:13:50,740 --> 00:13:56,340
noun, then pays attention to, so we design a method to whack on that a bit.

195
00:13:56,340 --> 00:14:01,700
Do you think there is a systematic way or will this kind of philosophically always rely

196
00:14:01,700 --> 00:14:07,580
on us humans having a higher level inside of what we want to do with the data set?

197
00:14:07,580 --> 00:14:10,460
So what comes next after data augmentation?

198
00:14:10,460 --> 00:14:12,460
So would the next step be some simulation?

199
00:14:13,180 --> 00:14:14,180
Do you know what I mean?

200
00:14:14,180 --> 00:14:17,580
Where we impute physics and we impute some world knowledge and then, I don't know, whether

201
00:14:17,580 --> 00:14:19,660
we train a machine learning model from that?

202
00:14:19,660 --> 00:14:26,100
Yeah, I think there are definitely shortcomings in our current machine learning models, understandings

203
00:14:26,100 --> 00:14:27,100
of the world.

204
00:14:27,100 --> 00:14:32,340
There are probably things that we can't just solve by throwing more static images at them.

205
00:14:32,340 --> 00:14:38,100
I think maybe the next step, rather than trying to immediately situate the machine

206
00:14:38,100 --> 00:14:43,420
learning model in a simulated world, we could just think about video.

207
00:14:43,420 --> 00:14:49,180
I think probably representation learning from video is going to be a big thing next year

208
00:14:49,180 --> 00:14:53,420
or the year after, something sometime in the near future.

209
00:14:53,420 --> 00:14:55,740
Finally, we talk about Simon's paper.

210
00:14:55,740 --> 00:15:00,460
Big self-supervised models are strong semi-supervised learners.

211
00:15:00,460 --> 00:15:05,980
What is a practical problem is the situation where you have a lot of unlabeled data and

212
00:15:05,980 --> 00:15:09,340
then a very small amount of labeled data.

213
00:15:09,340 --> 00:15:15,180
What I find fascinating is how many ideas come together in this paper.

214
00:15:15,180 --> 00:15:20,580
You probably didn't sit down after a SimClear one and be like, all right, what do we do

215
00:15:20,580 --> 00:15:21,580
for SimClear two?

216
00:15:21,580 --> 00:15:22,580
Okay, let's do this.

217
00:15:22,580 --> 00:15:24,220
So it tells me there was this process.

218
00:15:24,220 --> 00:15:30,340
Could you, if you can, maybe elaborate a bit on how did you going to build up the system

219
00:15:30,340 --> 00:15:32,820
towards the final output?

220
00:15:32,820 --> 00:15:37,540
We also tried the approach of first fine-tuning the big network and then distilling it.

221
00:15:37,540 --> 00:15:40,900
It turned out that worked a lot better.

222
00:15:40,900 --> 00:15:46,540
What we found was this approach of pre-training, then fine-tuning, then distilling works a

223
00:15:46,540 --> 00:15:50,660
lot better than pre-training, then distilling, then fine-tuning.

224
00:15:50,660 --> 00:15:56,100
We probably shouldn't expect distillation of the kind that we do in SimClear v2 to work

225
00:15:56,100 --> 00:16:02,660
substantially better than supervised distillation, which has been around for quite a while now.

226
00:16:02,660 --> 00:16:09,820
I think what's impressive is that in the self-supervised case, in the contrastive case, distillation

227
00:16:09,820 --> 00:16:14,460
basically allows you to recover the same accuracy that you would get from training supervised

228
00:16:14,460 --> 00:16:18,980
from scratch, whereas without it, the accuracy is a lot worse.

229
00:16:18,980 --> 00:16:23,940
So it seems like it maybe matters more in this contrastive case.

230
00:16:23,940 --> 00:16:29,300
But I think generally when you do distillation in the supervised case, you can get maybe

231
00:16:29,300 --> 00:16:33,580
a percentage point gain, maybe a couple of percentage points.

232
00:16:33,580 --> 00:16:38,180
And I think that's probably about the limit in terms of the improvement that you could

233
00:16:38,180 --> 00:16:44,820
get from any kind of distillation-based approach over supervised training from scratch.

234
00:16:44,820 --> 00:16:50,580
Can you use GANs, Generative Adversarial Neural Networks, to do data augmentation?

235
00:16:50,580 --> 00:16:52,780
Or is that just a myth?

236
00:16:52,780 --> 00:16:55,100
Simon certainly seems to think so.

237
00:16:55,100 --> 00:16:59,260
Using a GAN to do data augmentation, you have this problem that you still don't actually

238
00:16:59,260 --> 00:17:01,140
have more data.

239
00:17:01,140 --> 00:17:03,620
You have a GAN that's trained on the same data.

240
00:17:03,620 --> 00:17:08,460
And so it might help you because your way of encoding inductive bias into the GAN is

241
00:17:08,460 --> 00:17:12,500
different from your way of encoding inductive bias into the neural network.

242
00:17:12,500 --> 00:17:17,540
And maybe by having more inductive bias, you can learn a better function.

243
00:17:17,540 --> 00:17:21,660
You still don't have more data, and it seems like without having more data, there's no

244
00:17:21,660 --> 00:17:26,460
reason to expect a priority that you will be able to learn a better function.

245
00:17:26,460 --> 00:17:32,820
Ironically, when you do the simple data augmentation, you do have more data because you put all

246
00:17:32,820 --> 00:17:38,620
the knowledge in there as a human of what makes two images dissimilar visually, but

247
00:17:38,620 --> 00:17:44,500
still equivalent semantically, which, again, is exactly the opposite.

248
00:17:44,500 --> 00:17:51,180
It gives you images that are visually similar, but it has no intuition of what the semantic

249
00:17:51,180 --> 00:17:52,980
similarity is.

250
00:17:52,980 --> 00:17:57,180
We round off the show by talking about Simon's love of the Julia language.

251
00:17:57,180 --> 00:18:02,380
Julia is a much better programming language than Python in many ways.

252
00:18:02,380 --> 00:18:07,180
Julia is designed for these situations where maybe beyond just matrices, you have these

253
00:18:07,180 --> 00:18:11,500
funny types of structured matrices, you have sparse matrices, and you can define special

254
00:18:11,500 --> 00:18:16,700
methods for the product of a sparse matrix in a vector, or all sorts of things where

255
00:18:16,700 --> 00:18:20,500
you might want different methods depending on the types.

256
00:18:20,500 --> 00:18:22,300
I really hope you've enjoyed the show today.

257
00:18:22,300 --> 00:18:24,380
We've had so much fun making it.

258
00:18:24,380 --> 00:18:27,940
Remember to like, comment, and subscribe.

259
00:18:27,940 --> 00:18:33,820
We love reading your comments, every single one of them, and we'll see you back next

260
00:18:33,820 --> 00:18:34,820
week.

261
00:18:34,820 --> 00:18:39,820
Welcome back to the Machine Learning Street Talk YouTube channel and podcast with my

262
00:18:39,820 --> 00:18:46,100
two compadre, Syac, the neural network pruner, Paul, and Yannick, the Lightspeed protein

263
00:18:46,100 --> 00:18:49,180
folder, Kiltcher.

264
00:18:49,180 --> 00:18:53,780
Today we have an incredibly special guest, Simon Cornblith, and Simon got his PhD in

265
00:18:53,780 --> 00:18:56,980
brain and cognitive sciences from MIT.

266
00:18:56,980 --> 00:19:00,740
His undergrad was from Caltech, and he's a research scientist at Google Brain.

267
00:19:00,740 --> 00:19:03,660
He's been there since about 2017.

268
00:19:03,660 --> 00:19:08,900
He's been cited nearly 2,000 times, which for someone quite early in career is seriously

269
00:19:08,900 --> 00:19:09,900
impressive.

270
00:19:09,900 --> 00:19:16,500
He's got a keen interest in the digital humanities, in philosophy, computer science, machine learning,

271
00:19:16,500 --> 00:19:19,140
computer vision, and neuroscience.

272
00:19:19,140 --> 00:19:22,900
He used to be a neuroscientist before we started doing machine learning, and he tells us that

273
00:19:22,900 --> 00:19:27,180
he's got some very strong opinions about neuroscience and machine learning, which we certainly will

274
00:19:27,180 --> 00:19:28,860
be getting on to later.

275
00:19:28,860 --> 00:19:33,300
He's a huge lover of the Julia language, so if you Google Simon's name, you'll see him

276
00:19:33,300 --> 00:19:38,820
talking at about a million Julia conferences, so definitely check that out as well.

277
00:19:38,820 --> 00:19:43,740
Simon pioneered the use of centered kernel alignment as a way of analyzing the evolution

278
00:19:43,740 --> 00:19:49,180
of representations in layers, in network, and between networks of different architectures.

279
00:19:49,180 --> 00:19:53,980
Now Simon, like me, is a lover of similarity matrices, and what can be gleaned from them?

280
00:19:53,980 --> 00:19:59,220
On my own PhD, I worked with them a lot for music segmentation, and also for detecting

281
00:19:59,220 --> 00:20:01,780
regimes in financial data sets.

282
00:20:01,780 --> 00:20:07,220
When a block in a ResNet is no longer self-similar to previous layers early on, you might intuit

283
00:20:07,220 --> 00:20:12,700
that it's moving into a new representational regime, or maybe it's just started hallucinating.

284
00:20:12,780 --> 00:20:16,540
All of this stuff was covered in his paper, Do Wide and Deep Neural Networks Learn the

285
00:20:16,540 --> 00:20:21,900
Same Things, and I find it fascinating that representation of self-similarity can reveal

286
00:20:21,900 --> 00:20:23,540
network pathology.

287
00:20:23,540 --> 00:20:27,580
Now in his paper, What's in a Loss Function for Image Classification, he noted that different

288
00:20:27,580 --> 00:20:33,500
losses and regularizers have similar accuracies on several data sets, but using the same representational

289
00:20:33,500 --> 00:20:39,500
evolution analysis, Simon gleaned that these losses and regularizers only affected the

290
00:20:39,500 --> 00:20:43,340
penultimate layers in the neural network, revealing inherent limitations in what can

291
00:20:43,340 --> 00:20:46,500
be achieved in manipulating the loss on a network.

292
00:20:46,500 --> 00:20:50,140
Now next in the session today, we're going to talk about the Simclear paper, and this

293
00:20:50,140 --> 00:20:55,940
was an incredibly exciting paper for unsupervised contrastive image learning with augmentations.

294
00:20:55,940 --> 00:21:00,740
It introduced a learnable nonlinear transformation between the representations and the contrastive

295
00:21:00,740 --> 00:21:04,100
loss, which massively improved the representations.

296
00:21:04,100 --> 00:21:08,740
The composition of augmentations is super important, and whenever anyone asks me about

297
00:21:08,980 --> 00:21:12,100
what are the different data augmentations in computer vision, I always point them to

298
00:21:12,100 --> 00:21:16,940
the SimCLR paper because it's got this wonderful matrix, and in that matrix it was shown that

299
00:21:16,940 --> 00:21:21,740
the crop and the color I think were the most effective augmentations, but Simon also noted

300
00:21:21,740 --> 00:21:25,580
that the batch sizes were super important, and the paper improved over the state of the

301
00:21:25,580 --> 00:21:30,020
art on the ImageNet top one, and actually matched unsupervised methods for the first

302
00:21:30,020 --> 00:21:32,420
time, albeit with many more parameters.

303
00:21:32,420 --> 00:21:35,820
But the final paper we're going to talk about today is Big Self-Supervised Models, a strong

304
00:21:35,900 --> 00:21:40,060
semi-supervised learners, and this is where you can learn from fewer labeled examples while

305
00:21:40,060 --> 00:21:44,940
making use of a large amount of unlabeled data, and with unsupervised pre-training

306
00:21:44,940 --> 00:21:50,380
on SimCLR v2, supervised fine-tuning on a few labeled examples, and then distillation

307
00:21:50,380 --> 00:21:54,900
with unlabeled examples, this approach improved the label efficiency over previous state-of-the-art

308
00:21:54,900 --> 00:21:55,900
methods.

309
00:21:55,900 --> 00:22:00,140
I remember Yannick Lightspeed Kilcher made a video on this one, which I watched a few

310
00:22:00,140 --> 00:22:03,900
months ago, so Yannick will have all of that completely fresh in his mind.

311
00:22:03,980 --> 00:22:06,980
Anyway, Simon, it's an absolute pleasure to welcome you to the show.

312
00:22:06,980 --> 00:22:07,980
Thank you so much for coming.

313
00:22:07,980 --> 00:22:08,980
It's great to be here.

314
00:22:08,980 --> 00:22:09,980
Amazing.

315
00:22:09,980 --> 00:22:11,780
How did you get into machine learning?

316
00:22:11,780 --> 00:22:18,540
So I guess first I got into neuroscience, and then I got disillusioned with neuroscience.

317
00:22:18,540 --> 00:22:23,700
When I was pretty young, I was interested in consciousness and how we create this kind

318
00:22:23,700 --> 00:22:28,060
of impression of the external world inside our heads.

319
00:22:28,060 --> 00:22:32,500
And so I guess it's pretty obvious how that translates into an interest in brains and

320
00:22:32,500 --> 00:22:34,300
how the brain works.

321
00:22:34,300 --> 00:22:40,460
So I spent both four years as an undergraduate doing neuroscience research, and then seven

322
00:22:40,460 --> 00:22:46,900
years working with monkeys at MIT trying to figure out how monkey brains work.

323
00:22:46,900 --> 00:22:53,460
And then after that, I felt like we weren't getting very far by trying to record from

324
00:22:53,460 --> 00:22:58,100
neurons in monkeys' brains and figure out how those neurons work.

325
00:22:58,100 --> 00:23:03,780
So I thought about what other ways are there approaching this problem?

326
00:23:03,780 --> 00:23:10,380
How could we think about how to understand how the brain is doing tasks?

327
00:23:10,380 --> 00:23:16,500
And it seemed like maybe by building systems that can do those tasks well that are not

328
00:23:16,500 --> 00:23:18,740
biological, we could learn more.

329
00:23:18,740 --> 00:23:21,060
So that's how I got into machine learning.

330
00:23:21,060 --> 00:23:25,820
I joined the Google AI residency program, which is like this great program that Google

331
00:23:25,820 --> 00:23:32,660
has to take people who have extensive background in some field that is not machine learning

332
00:23:32,660 --> 00:23:35,580
and train them to do machine learning.

333
00:23:35,580 --> 00:23:40,660
And I ended up at Google, and initially I thought I'm going to spend a year here learning

334
00:23:40,660 --> 00:23:45,140
about machine learning related stuff, and then maybe I'll go back to neuroscience and

335
00:23:45,140 --> 00:23:49,820
I'll decide the tools for machine learning could be applied back to brains, and maybe

336
00:23:49,820 --> 00:23:54,580
we can learn more about brains by applying the tools of machine learning there.

337
00:23:54,580 --> 00:23:59,260
But ultimately I decided I was more interested in just looking at how the neural networks

338
00:23:59,260 --> 00:24:04,500
work and also in the engineering challenges of building better neural networks, which

339
00:24:04,500 --> 00:24:06,500
I actually think are fun.

340
00:24:06,500 --> 00:24:11,020
One of the thoughts that came to my mind is it's fascinating looking at the kind of introspective

341
00:24:11,020 --> 00:24:15,380
analysis that you've been conducting with neural networks, but could you contrast that

342
00:24:15,380 --> 00:24:16,380
with neuroscience?

343
00:24:16,380 --> 00:24:22,300
Because as I understand, you have MRI scans and you have different ways of trying to visualize

344
00:24:22,300 --> 00:24:26,620
and reason about the behavior of a brain, but you can't really tweak the architecture

345
00:24:26,620 --> 00:24:30,780
and tweak all of the knobs and the levers in quite the same way you do in machine learning.

346
00:24:30,780 --> 00:24:37,020
Yeah, so like in neuroscience people also use this analysis across different individuals

347
00:24:37,020 --> 00:24:39,740
or different organisms or whatever.

348
00:24:39,740 --> 00:24:44,620
It is a tool that people use in neuroscience as well, but I guess they're limited in the

349
00:24:44,620 --> 00:24:50,060
ways in which they could manipulate the systems that are providing these representations.

350
00:24:50,060 --> 00:24:55,900
So in neuroscience, you're always constrained by data, so you can compare representations

351
00:24:55,900 --> 00:25:00,340
of images across individuals by doing MRI scans.

352
00:25:00,340 --> 00:25:05,020
But first of all, you might not get a very good idea of how the brain is representing

353
00:25:05,020 --> 00:25:08,580
those images because there's a lot of noise in the MRI scan and there's a limit to how

354
00:25:08,580 --> 00:25:13,980
long you can scan each person, whereas I guess in a neural network, noise is not a problem.

355
00:25:13,980 --> 00:25:18,340
The entire system's deterministic, we just pass in the image and we get the representation

356
00:25:18,340 --> 00:25:19,340
vector.

357
00:25:19,460 --> 00:25:24,020
And you also have these kinds of limits of, like, we can't see what happens if people

358
00:25:24,020 --> 00:25:29,500
have bigger brains, like we can't manipulate the architecture in those kinds of ways.

359
00:25:29,500 --> 00:25:34,660
So even though we can look at how intact brains are working, we can't see how representations

360
00:25:34,660 --> 00:25:38,380
change when we manipulate them all that easily.

361
00:25:38,380 --> 00:25:42,340
And I guess, again, in machine learning, like we can do all of those things.

362
00:25:42,340 --> 00:25:45,220
We can look at what happens when we change the loss function.

363
00:25:45,220 --> 00:25:48,700
We can look at what happens when we make the network deeper or wider.

364
00:25:48,700 --> 00:25:54,140
So I think there are, like, some really cool ways that even the same techniques can be

365
00:25:54,140 --> 00:25:59,420
applied in machine learning that they couldn't be applied in neuroscience.

366
00:25:59,420 --> 00:26:05,700
I felt like we weren't getting very far by trying to record from neurons in monkey's

367
00:26:05,700 --> 00:26:09,140
brains and figure out how those neurons work.

368
00:26:09,140 --> 00:26:15,020
Like it didn't really seem like a very effective way of figuring out how the brain constructs

369
00:26:15,220 --> 00:26:18,900
this kind of internal representation of the world.

370
00:26:18,900 --> 00:26:24,460
So from there, I thought about what could we actually do to understand this?

371
00:26:24,460 --> 00:26:29,940
And it seemed like the most promising thing to do was to look at what happens in simpler

372
00:26:29,940 --> 00:26:35,540
systems that we can construct ourselves and where we can analyze the behavior of everything

373
00:26:35,540 --> 00:26:36,980
inside the system.

374
00:26:36,980 --> 00:26:41,020
So in a neural network, we can record all the neurons, which is extremely challenging

375
00:26:41,020 --> 00:26:42,900
in a biological organism.

376
00:26:42,940 --> 00:26:48,100
And we can also manipulate the system in any kind of way that we can imagine.

377
00:26:48,100 --> 00:26:51,700
But it still seems like really hard to understand neural networks.

378
00:26:51,700 --> 00:26:57,500
So it seemed like maybe this was a more tractable challenge and a challenge where maybe we could

379
00:26:57,500 --> 00:27:01,740
make some headway that might eventually translate back to brains.

380
00:27:01,740 --> 00:27:03,780
And so that's how I ended up in machine learning.

381
00:27:03,780 --> 00:27:06,740
I guess there are, like, other great things about machine learning.

382
00:27:06,740 --> 00:27:10,940
I guess the pay is much better than in, like, academic neuroscience.

383
00:27:10,980 --> 00:27:16,180
But really, I think, like, it's a logical progression based on the ideas that I was

384
00:27:16,180 --> 00:27:17,020
interested in.

385
00:27:17,020 --> 00:27:21,540
And I am still interested in the same sorts of ideas.

386
00:27:21,540 --> 00:27:27,700
Do you still think now that you're in machine learning and have made some progress here that

387
00:27:27,700 --> 00:27:33,660
there is a good chance that we're going to map our knowledge that we gain back to the brain?

388
00:27:33,660 --> 00:27:38,380
Or do you think there is a bit of a disconnect?

389
00:27:38,380 --> 00:27:40,740
I think that's a really good question.

390
00:27:40,740 --> 00:27:45,380
I think there is definitely some knowledge that we're going to get from machine learning

391
00:27:45,380 --> 00:27:47,020
that will map back to the brain.

392
00:27:47,020 --> 00:27:52,380
I think, like, in terms of general principles and ways of looking at how, like, information

393
00:27:52,380 --> 00:27:58,220
processing systems work, I think there are a lot of ideas from machine learning that

394
00:27:58,220 --> 00:28:01,420
will ultimately help us understand brains.

395
00:28:01,420 --> 00:28:06,780
I'm a little less sure whether we're going to build, like, a machine learning system

396
00:28:06,780 --> 00:28:07,780
that is a brain.

397
00:28:07,780 --> 00:28:12,380
I think there's a disconnect between the way that the systems that we build work and

398
00:28:12,380 --> 00:28:13,820
the way that biology works.

399
00:28:13,820 --> 00:28:17,980
And I think that's insurmountable just because there's differences between what you can

400
00:28:17,980 --> 00:28:23,220
build efficiently with cells and what you can build efficiently in silicon.

401
00:28:23,220 --> 00:28:28,860
But in terms of approaches to understanding, in terms of building tools to understand things,

402
00:28:28,860 --> 00:28:34,660
the tools that we build in machine learning, I think will eventually be useful in neuroscience.

403
00:28:34,660 --> 00:28:40,460
So people make a lot of analogies and they make a lot of claims about neuroscience in

404
00:28:40,460 --> 00:28:43,540
connections with neural networks.

405
00:28:43,540 --> 00:28:48,100
Is there a statement or a bunch of statements that you hear over and over again where you

406
00:28:48,100 --> 00:28:51,580
just cringe because they're so wrong?

407
00:28:51,580 --> 00:28:53,380
Is that something that happens to you?

408
00:28:53,380 --> 00:28:55,300
I can imagine it would.

409
00:28:55,300 --> 00:28:56,300
Yeah.

410
00:28:56,300 --> 00:28:57,300
Yeah.

411
00:28:57,300 --> 00:29:01,140
So I think there's this, like, kind of basic fact that neural networks are inspired by

412
00:29:01,140 --> 00:29:03,060
brains, which is true.

413
00:29:03,060 --> 00:29:07,940
Then there's all this other stuff where people try to relate whatever neural network they've

414
00:29:07,940 --> 00:29:11,700
built back to a brain and they say that it works like the brain, but it doesn't work

415
00:29:11,700 --> 00:29:12,700
like the brain.

416
00:29:12,700 --> 00:29:17,940
There's still this huge kind of disconnect in how the system is actually operating.

417
00:29:17,940 --> 00:29:20,500
The brain is not literally doing back prop.

418
00:29:20,500 --> 00:29:22,820
It might be doing something that's like back prop.

419
00:29:22,820 --> 00:29:29,100
We still don't really know, but it's not literally computing gradients by automatic differentiation.

420
00:29:29,100 --> 00:29:33,340
And I'm fascinated to talk about this line of reasoning that you have because you're

421
00:29:33,340 --> 00:29:38,020
clearly the kind of guy that you want to reason about the behavior of models and in particular

422
00:29:38,020 --> 00:29:40,220
the evolution of representations.

423
00:29:40,220 --> 00:29:43,380
And I watched one of your presentations on YouTube where you were talking about how you

424
00:29:43,380 --> 00:29:46,620
can compare the representations by comparing features.

425
00:29:46,620 --> 00:29:50,620
And of course, the naive way of doing is the dot product or some variations of that.

426
00:29:50,620 --> 00:29:52,100
Turns out that doesn't work very well.

427
00:29:52,100 --> 00:29:57,420
And you came up with this wonderful metric called the centered kernel alignment.

428
00:29:57,420 --> 00:30:00,020
So how did that all come about?

429
00:30:00,020 --> 00:30:06,420
The way we came up with that idea was that Jeff Hinton had another idea and I tried the

430
00:30:06,420 --> 00:30:11,860
idea and it worked, but then we wondered, is there a simpler thing that worked?

431
00:30:11,860 --> 00:30:14,300
And that's how we ended up with centered kernel alignment.

432
00:30:14,300 --> 00:30:19,620
I guess the problem that we had in trying to come up with a way of comparing similarity

433
00:30:19,620 --> 00:30:24,820
of neural network representations is that it's really hard to know what is a good way.

434
00:30:24,820 --> 00:30:28,780
Like it's not something where you can really develop a good benchmark.

435
00:30:28,780 --> 00:30:34,300
So like in the paper, we came up with this simple sanity check where the idea is basically

436
00:30:34,300 --> 00:30:39,460
we've got two architecturally identical neural networks and we just train them from different

437
00:30:39,460 --> 00:30:41,260
random initializations.

438
00:30:41,260 --> 00:30:45,700
And so we want it to be the case that if you measure like the similarity between a layer

439
00:30:45,700 --> 00:30:51,100
from network A and all the layers from network B, that the most similar layer in network

440
00:30:51,100 --> 00:30:53,820
B is going to be the architecturally corresponding layer.

441
00:30:53,820 --> 00:30:59,540
So if we have layer two from network A, it should be more similar to layer two from network

442
00:30:59,540 --> 00:31:02,420
B than layer three or layer four.

443
00:31:02,420 --> 00:31:08,860
And so like basically we found that what people had been doing before didn't always pass that

444
00:31:08,860 --> 00:31:10,380
sanity check.

445
00:31:10,380 --> 00:31:14,620
And we basically tried to come up with the simplest way of building a similarity index

446
00:31:14,620 --> 00:31:17,580
that did actually pass that sanity check.

447
00:31:17,580 --> 00:31:20,020
And that's how we ended up with centered kernel alignment.

448
00:31:21,020 --> 00:31:25,260
Yeah, because I think you showed that the canonical correlation analysis only worked

449
00:31:25,260 --> 00:31:27,940
about, I think at an accuracy of about 1.4%.

450
00:31:27,940 --> 00:31:30,220
So it's complete apples and oranges.

451
00:31:30,220 --> 00:31:34,180
But this absolutely fascinates me though, because when you plot this thing in this kind

452
00:31:34,180 --> 00:31:40,100
of self similarity matrix, you can glean so much about the evolution as a function of

453
00:31:40,100 --> 00:31:41,100
time.

454
00:31:41,100 --> 00:31:44,260
And because you talk about this in one of your other papers as well, that there's this

455
00:31:44,260 --> 00:31:46,700
characteristic blockiness.

456
00:31:46,700 --> 00:31:52,780
And when you see blockiness, that successive layers are similar to versions of themselves

457
00:31:52,780 --> 00:31:54,140
in the past.

458
00:31:54,140 --> 00:31:57,700
And that kind of means that they're not evolving anymore.

459
00:31:57,700 --> 00:32:02,260
And you then made the intuition in your paper that, well, essentially it's redundant information.

460
00:32:02,260 --> 00:32:05,380
If it's not learning anything new, I can just delete that block.

461
00:32:05,380 --> 00:32:08,740
I can just delete those layers from the neural network and it won't make any difference.

462
00:32:08,740 --> 00:32:10,540
And indeed it didn't.

463
00:32:10,540 --> 00:32:13,540
Yeah.

464
00:32:13,540 --> 00:32:21,100
Could you, for people listening, explain the similarity measure you came up with in principle,

465
00:32:21,100 --> 00:32:25,180
just so we can imagine something, how that should even work?

466
00:32:25,180 --> 00:32:26,180
Yeah.

467
00:32:26,180 --> 00:32:31,620
So I guess the idea is you've got a neural network and you feed some set of examples,

468
00:32:31,620 --> 00:32:33,900
like multiple examples through the neural network.

469
00:32:33,900 --> 00:32:40,100
And now you've got some matrix where the rows of the matrix are different examples and the

470
00:32:40,100 --> 00:32:42,820
columns are different neurons.

471
00:32:42,820 --> 00:32:47,940
So yeah, you can imagine this as if you have vectors of activations for each example, you've

472
00:32:47,940 --> 00:32:50,460
stacked them real wise.

473
00:32:50,460 --> 00:32:54,900
So now what do we do with that to compare two neural networks trained from different

474
00:32:54,900 --> 00:32:56,500
random initializations?

475
00:32:56,500 --> 00:33:01,140
The problem is if we were to just take the square difference between those matrices,

476
00:33:01,140 --> 00:33:06,780
we have this problem that the neurons between these two different networks aren't necessarily

477
00:33:06,780 --> 00:33:10,500
aligned in any way if they're trained from different random initializations.

478
00:33:10,500 --> 00:33:14,660
Even if we had exactly the same neurons, we shouldn't expect that neuron one would be

479
00:33:14,660 --> 00:33:18,420
the same, representing the same thing in both networks.

480
00:33:18,420 --> 00:33:21,180
So we need some way to get around that problem.

481
00:33:21,180 --> 00:33:28,860
One way around this problem is instead of comparing these original matrices, we're going

482
00:33:28,860 --> 00:33:34,820
to make matrices that measure the similarity of each example to each other example for

483
00:33:34,820 --> 00:33:37,420
one particular network.

484
00:33:37,420 --> 00:33:42,740
So if we've got example A and example B, we can measure their similarity very simply

485
00:33:42,740 --> 00:33:46,260
just by taking the dot product between those two vectors.

486
00:33:46,260 --> 00:33:50,060
And now because we're measuring similarity from the same network, we don't have to worry

487
00:33:50,060 --> 00:33:52,180
about this alignment problem.

488
00:33:52,180 --> 00:33:57,860
And we get some idea of how similar different examples are to each other according to the

489
00:33:57,860 --> 00:34:03,100
representation in network A. So if we do that for all the examples, we get some examples

490
00:34:03,100 --> 00:34:05,300
by examples matrix.

491
00:34:05,300 --> 00:34:09,660
And then we can do that both for our first network and for our second network.

492
00:34:09,660 --> 00:34:14,620
So after we've done that, we've got these two examples by examples matrices.

493
00:34:14,620 --> 00:34:19,780
And then the easy way to compare those matrices is we just reshape them to vectors and we

494
00:34:19,780 --> 00:34:22,620
take the dot product again between those vectors.

495
00:34:22,620 --> 00:34:28,660
So now we've measured the similarities between the similarities of the examples.

496
00:34:28,660 --> 00:34:33,660
And this doesn't have this problem of aligning the neurons because instead of measuring similarities

497
00:34:33,660 --> 00:34:37,780
of neurons, we're measuring similarities of examples and then we're comparing those

498
00:34:37,780 --> 00:34:38,780
similarities.

499
00:34:38,780 --> 00:34:42,620
So ultimately, we do that, we take that dot product and then we normalize it in a way

500
00:34:42,620 --> 00:34:45,020
that makes it invariant to scaling.

501
00:34:45,020 --> 00:34:50,420
So if you just took the dot product, you'd have this problem that scaling all of the

502
00:34:50,420 --> 00:34:55,420
features by some number, if you scale everything by a factor of two, the dot product will go

503
00:34:55,420 --> 00:34:57,380
up by a factor of two.

504
00:34:57,380 --> 00:35:01,860
And so we just apply some normalization so that kind of scaling will not affect the similarity

505
00:35:01,860 --> 00:35:08,420
index and we get centered kernel alignment, which gives us a similarity score between

506
00:35:08,420 --> 00:35:10,060
zero and one.

507
00:35:10,060 --> 00:35:13,460
The fascinating thing is that you can replace that dot product with a kernel because it's

508
00:35:13,460 --> 00:35:14,580
a gram matrix.

509
00:35:14,580 --> 00:35:15,580
Yeah.

510
00:35:15,580 --> 00:35:19,900
So did you find that it made a difference if you use, let's say, the RBF kernel?

511
00:35:19,900 --> 00:35:20,900
Yeah.

512
00:35:20,900 --> 00:35:21,900
Yeah.

513
00:35:21,900 --> 00:35:25,700
So, yeah, basically when we're measuring the similarities between examples, we can just

514
00:35:25,700 --> 00:35:29,180
instead of taking the dot product between the representations of the different examples,

515
00:35:29,180 --> 00:35:33,900
we can take the kernel between one example and another example because the kernel is

516
00:35:33,900 --> 00:35:36,460
also a way of measuring similarity.

517
00:35:36,460 --> 00:35:38,620
And so we tried that.

518
00:35:38,620 --> 00:35:41,940
It turns out that like for CNNs, it didn't really make a difference.

519
00:35:41,940 --> 00:35:46,340
Like the RBF kernel worked, but sort of just taking a regular dot product.

520
00:35:46,340 --> 00:35:52,260
But we did find in the appendix of that paper that if you instead use an RBF kernel with

521
00:35:52,260 --> 00:35:57,260
a transformer, it actually does work better than taking a normal dot product.

522
00:35:57,260 --> 00:36:02,300
And I think like part of what's going on is that sometimes you want it to be the case

523
00:36:02,300 --> 00:36:08,300
that when you're measuring similarity, you care more about the distances between the

524
00:36:08,300 --> 00:36:13,540
examples that you're close to than the distances to the examples that you're far away from.

525
00:36:13,540 --> 00:36:18,260
Like once you're really far away from something, maybe it doesn't matter so much if you're

526
00:36:18,260 --> 00:36:24,900
10 times as far away because like you're already so far, you're already not going to...

527
00:36:24,900 --> 00:36:30,380
You don't really care how far away something is once you're far enough.

528
00:36:30,380 --> 00:36:34,980
And the RBF kernel takes that into account in a way that a linear dot product wouldn't.

529
00:36:34,980 --> 00:36:43,660
The linear dot product is like very sensitive to the global distances in the space.

530
00:36:43,660 --> 00:36:47,020
What I find fascinating is that you can glean so much from the blockiness, right?

531
00:36:47,020 --> 00:36:51,380
So you are saying that as it becomes blockier, it might be an indication that it's become

532
00:36:51,380 --> 00:36:53,700
saturated in some sense.

533
00:36:53,700 --> 00:36:59,940
And I'm also interested in a way, we already know that the representations in neural networks

534
00:36:59,940 --> 00:37:03,500
are increasingly abstract, so they don't necessarily bear any resemblance to the beginning.

535
00:37:03,500 --> 00:37:08,260
So when we're looking at the cell similarity matrix, we don't necessarily want the representations

536
00:37:08,260 --> 00:37:12,340
on the final penultimate layers to be similar to the ones at the beginning.

537
00:37:12,340 --> 00:37:15,260
We want there to be a continuous evolution.

538
00:37:15,260 --> 00:37:20,180
We don't want to have a stalled evolution because that would correspond to this blockiness.

539
00:37:20,180 --> 00:37:25,940
But is it when you've stalled for a long time, is that when it becomes pathological?

540
00:37:25,940 --> 00:37:29,380
Because we want it to evolve in stops and starts, don't we?

541
00:37:29,380 --> 00:37:30,380
Yeah.

542
00:37:30,380 --> 00:37:35,500
I think it's not clear what we should really expect in terms of how a neural network representation

543
00:37:35,500 --> 00:37:37,060
evolves through the layers.

544
00:37:37,060 --> 00:37:42,340
I think there's kind of some theory on what we should expect if all the layers are linear.

545
00:37:42,340 --> 00:37:46,780
But obviously, the neural networks that we train are nonlinear, and it's really important

546
00:37:46,780 --> 00:37:49,380
to have a nonlinearity in between the layers.

547
00:37:49,380 --> 00:37:54,180
And so at that point, it's really hard to reason about what the optimal thing for a

548
00:37:54,180 --> 00:37:56,700
neural network to do actually is.

549
00:37:56,700 --> 00:38:00,180
I think it's something that we can really only study empirically.

550
00:38:00,180 --> 00:38:04,220
On the other hand, I do think if we see that nothing is changing from one layer to the

551
00:38:04,220 --> 00:38:06,820
next, that's a really bad sign.

552
00:38:06,820 --> 00:38:10,700
If the neural network representation isn't changing, then obviously nothing's happening.

553
00:38:10,700 --> 00:38:15,540
But I guess it's unclear whether we should expect abrupt shifts or we want things to

554
00:38:15,540 --> 00:38:17,380
happen slowly between the layers.

555
00:38:17,380 --> 00:38:22,700
I'm not sure whether we really have the theoretical knowledge to say what is best.

556
00:38:22,700 --> 00:38:23,700
Yeah.

557
00:38:23,700 --> 00:38:27,940
I'd love to see this as a kind of tool in our toolbox that we could use on different network

558
00:38:27,940 --> 00:38:28,940
architectures.

559
00:38:28,940 --> 00:38:32,780
But you said that the other learn features are shared across different initializations

560
00:38:32,780 --> 00:38:36,300
and architectures, particularly across the depths of the network.

561
00:38:36,300 --> 00:38:42,380
So it almost seems as if this blockiness is separate to your work in wide and deep neural

562
00:38:42,380 --> 00:38:45,900
networks because you showed that the width and the depth have got different effects on

563
00:38:45,900 --> 00:38:49,620
network predictions at the example level or at the class level.

564
00:38:49,620 --> 00:38:52,140
But the blockiness almost seems to be an orthogonal thing.

565
00:38:52,140 --> 00:38:56,420
That's just when you have this kind of saturation of the network, you see the blockiness.

566
00:38:56,420 --> 00:38:57,420
Yeah.

567
00:38:57,420 --> 00:38:58,420
Yeah.

568
00:38:58,420 --> 00:39:02,980
So initially we had hoped that we could look at other similarities between wide networks

569
00:39:02,980 --> 00:39:06,180
and deep networks in their representations.

570
00:39:06,180 --> 00:39:09,620
But like when we did those experiments, we actually just found that if you make the network

571
00:39:09,620 --> 00:39:14,140
really wide, you get this kind of blockiness in the representations and those blocks are

572
00:39:14,140 --> 00:39:17,220
like dissimilar across different initializations.

573
00:39:17,220 --> 00:39:19,700
And then the same thing happens if you make the network really deep.

574
00:39:19,700 --> 00:39:23,200
We see these like big blocks in the representations.

575
00:39:23,200 --> 00:39:29,380
So that made it hard to study these very wide and very deep networks from the representational

576
00:39:29,380 --> 00:39:31,020
similarity perspective.

577
00:39:31,060 --> 00:39:34,380
But at the same time, I think it's like a really interesting observation.

578
00:39:34,380 --> 00:39:39,260
Like it's something where we couldn't have predicted this ahead of time based on what

579
00:39:39,260 --> 00:39:43,980
we know about neural network theory and we couldn't have predicted it ahead of time based

580
00:39:43,980 --> 00:39:46,100
on the accuracy of the network.

581
00:39:46,100 --> 00:39:50,860
It's something where we really needed like these techniques for looking at the internal

582
00:39:50,860 --> 00:39:55,620
representations of neural networks to see what was happening inside of them.

583
00:39:55,700 --> 00:40:00,820
There's this whole literature that takes a look at a network's expressibility with

584
00:40:00,820 --> 00:40:03,100
regards to its depth and width.

585
00:40:03,100 --> 00:40:07,980
So could you just explain it to us whether or not we should be able to meaningfully quantify

586
00:40:07,980 --> 00:40:13,220
or formulate the expressibility of a neural network with regards to your analysis made

587
00:40:13,220 --> 00:40:14,220
on that?

588
00:40:14,220 --> 00:40:15,220
Yeah.

589
00:40:15,220 --> 00:40:18,780
So there's this work that looks at like kind of the functions that can be expressed by

590
00:40:18,780 --> 00:40:22,100
wide networks and the functions that can be expressed by deep networks.

591
00:40:22,100 --> 00:40:28,180
And I guess like the neural networks seem to become exponentially more expressive as

592
00:40:28,180 --> 00:40:29,900
you make them deeper.

593
00:40:29,900 --> 00:40:33,460
So it seems like in that sense depth is more important than width.

594
00:40:33,460 --> 00:40:38,100
But on the other hand, like the neural networks that we actually train in this paper, like

595
00:40:38,100 --> 00:40:42,220
both the wide networks and the deep networks are big enough that they can overfit the entire

596
00:40:42,220 --> 00:40:43,540
training set.

597
00:40:43,540 --> 00:40:48,460
So in this case, like the expressibility of the network is not really important.

598
00:40:48,660 --> 00:40:52,260
Important is the function that the network actually ends up learning.

599
00:40:52,260 --> 00:40:57,740
So I guess even though networks could express more functions when they're deep, what we're

600
00:40:57,740 --> 00:41:03,140
really studying is the function that you actually get when you train the neural network by gradient

601
00:41:03,140 --> 00:41:07,260
descent on some data, what the optimization process actually finds.

602
00:41:07,260 --> 00:41:12,060
One thing as well in that paper, you talked about the network pathology, right?

603
00:41:12,060 --> 00:41:18,260
You said that two times depth accuracy 95%, four times 93.2, eight times 91.9.

604
00:41:18,260 --> 00:41:21,460
So because this is this runs counter to what a lot of us would intuit.

605
00:41:21,460 --> 00:41:23,460
We think that you can have as much depth as you want.

606
00:41:23,460 --> 00:41:27,260
And architectures like ResNet in some sense, they learn their own capacity.

607
00:41:27,260 --> 00:41:29,340
There is a pathology there happening clearly.

608
00:41:29,340 --> 00:41:33,780
And how would you determine that from this visualization?

609
00:41:33,780 --> 00:41:36,500
Yeah, so I guess there are two kind of results.

610
00:41:36,500 --> 00:41:42,020
So like you can either look at networks without residual connections, where you do actually

611
00:41:42,100 --> 00:41:45,780
find that at some depth, the accuracy will start going down.

612
00:41:45,780 --> 00:41:51,820
And in networks without residual connections, we find that like that the depth where accuracy

613
00:41:51,820 --> 00:41:56,860
starts to go down is like around the same depth where you begin seeing this kind of block

614
00:41:56,860 --> 00:42:00,820
structure where many successive layers have similar representations.

615
00:42:00,820 --> 00:42:04,740
And it looks like the representation is no longer getting refined through the network.

616
00:42:04,740 --> 00:42:07,740
Yeah, I mean, with ResNets, you can make them much deeper.

617
00:42:07,740 --> 00:42:12,860
And it seems like it doesn't hurt accuracy as much even once you start getting these blocks.

618
00:42:13,100 --> 00:42:17,260
But it also seems once you start getting these blocks, making the network deeper, making

619
00:42:17,260 --> 00:42:21,580
that work wider, no longer really gives you any improvement in accuracy.

620
00:42:21,580 --> 00:42:27,020
So it seems like this is basically telling you that the network has fit the data as much as it can.

621
00:42:27,020 --> 00:42:31,940
And and there's no real advantage to using something bigger.

622
00:42:33,180 --> 00:42:37,340
Fascinating. Let's move on to another paper that you've done, which is quite related in

623
00:42:37,340 --> 00:42:40,020
terms of you've used the same analysis to reason about it.

624
00:42:40,020 --> 00:42:43,260
But you had a paper called What's in a Loss Function for Image Classification.

625
00:42:43,420 --> 00:42:47,820
And you looked at a whole bunch of different label smoothing and regularizers, which are

626
00:42:47,820 --> 00:42:49,580
things that you do on the end of the network.

627
00:42:49,580 --> 00:42:54,820
And you identified differences in accuracy and calibration and out of domain distribution.

628
00:42:55,020 --> 00:42:57,620
And you made some really interesting observations.

629
00:42:57,620 --> 00:43:00,900
So by the way, we're talking about things like do we use the softmax or the squared

630
00:43:00,900 --> 00:43:03,780
area or dropout or label smoothing or logic penalty.

631
00:43:03,940 --> 00:43:07,540
But you noticed using the same analysis technique that only affected the

632
00:43:07,540 --> 00:43:10,700
representations on the penultimate layers of the neural network.

633
00:43:10,900 --> 00:43:12,580
What's going on there?

634
00:43:12,580 --> 00:43:15,020
Yeah, so it's not just the penultimate layer.

635
00:43:15,020 --> 00:43:20,500
It's like the the last maybe third of the network is affected by the loss function.

636
00:43:20,500 --> 00:43:25,460
But then the first two thirds of the network, it seems like you learn the same representation

637
00:43:25,460 --> 00:43:27,460
no matter what loss function you use.

638
00:43:27,460 --> 00:43:29,860
So it doesn't change if you use label smoothing.

639
00:43:30,100 --> 00:43:35,300
It doesn't even change if you use mean squared error instead of using softmax cross entropy.

640
00:43:35,300 --> 00:43:40,260
You still basically learn the same representation for the first two thirds of the network.

641
00:43:40,260 --> 00:43:44,020
And I think it's still it's a bit of a puzzle to us why this happens.

642
00:43:44,020 --> 00:43:47,540
Clearly, it matters that you're training the network with the loss function.

643
00:43:47,540 --> 00:43:52,820
There's those layers in the first two thirds of the network do change from the initialization.

644
00:43:52,820 --> 00:43:58,260
But I guess it seems that the last third of the network is setting up the penultimate

645
00:43:58,260 --> 00:44:02,180
layer representation in a way that is good for your loss function.

646
00:44:02,180 --> 00:44:07,540
But the first two thirds of the network are somehow just learning general features.

647
00:44:07,540 --> 00:44:11,300
I think this also like corresponds with the success of transfer learning,

648
00:44:11,300 --> 00:44:15,700
where we can take features that we've learned on one task and transfer them to some other task.

649
00:44:16,500 --> 00:44:21,940
What's the implication that it seems is the implication that the loss function

650
00:44:21,940 --> 00:44:25,780
is not having any impact on the representations early on in the network?

651
00:44:26,500 --> 00:44:28,180
That seems like quite a big implication.

652
00:44:29,140 --> 00:44:34,500
Yeah, I think the loss function must have some impact because if you don't train

653
00:44:34,500 --> 00:44:40,180
the network, if you don't have any loss function at all, then the representation

654
00:44:40,180 --> 00:44:44,820
in that first two thirds of the network is actually quite different.

655
00:44:44,820 --> 00:44:48,980
I think what's really happening is there are these differences among the loss functions,

656
00:44:48,980 --> 00:44:52,660
which don't really matter except later in the network.

657
00:44:52,740 --> 00:44:58,500
Although they will give you a slight change in accuracy and slight changes in robustness,

658
00:44:58,500 --> 00:45:01,940
they don't matter for this general feature learning process.

659
00:45:01,940 --> 00:45:07,540
I guess maybe it's what we should expect because ultimately we're asking the network

660
00:45:07,540 --> 00:45:09,700
to do the same thing just in a slightly different way.

661
00:45:09,700 --> 00:45:12,660
We're still asking the network to classify images.

662
00:45:12,660 --> 00:45:17,060
We're just asking it to provide slightly different outputs to produce a slightly

663
00:45:17,060 --> 00:45:20,020
different representation at the penultimate layer.

664
00:45:20,100 --> 00:45:26,900
Maybe we should expect that those earlier features that are just trying to

665
00:45:27,540 --> 00:45:33,540
represent general things about images, those will be the same no matter what loss function we pick.

666
00:45:34,820 --> 00:45:39,380
In your experiments, did you find whether or not model capacity has anything to do with it?

667
00:45:39,380 --> 00:45:44,580
Yeah, so we didn't really investigate different model capacities in that paper.

668
00:45:44,580 --> 00:45:49,380
I would expect that the same thing holds for a wide range of model capacities.

669
00:45:49,700 --> 00:45:53,860
There's no indication from the experiments that if you use a bigger network or a slightly

670
00:45:53,860 --> 00:45:55,860
smaller network that things would change all that much.

671
00:45:57,060 --> 00:46:01,300
Yeah, I think it's still an open question how model capacity changes things.

672
00:46:01,300 --> 00:46:06,180
I guess in the Sinclair paper, we found that model capacity can matter quite a bit.

673
00:46:06,180 --> 00:46:09,300
Yeah, so the general hypothesis there should also hold,

674
00:46:09,300 --> 00:46:14,180
even though your model is bigger or smaller, no matter how big or smaller your network is,

675
00:46:14,180 --> 00:46:19,060
the general feature learning regime or paradigm should still hold no matter what

676
00:46:19,060 --> 00:46:21,140
loss function you would end up using.

677
00:46:21,140 --> 00:46:25,860
Yeah, that would be my guess. I think if you're in a regime where it's really hard for you to fit

678
00:46:25,860 --> 00:46:30,500
the training data, if you have a very small network, it might be the case that you see more

679
00:46:30,500 --> 00:46:36,740
differences in the earlier layers because it might be that the loss function really affects

680
00:46:36,740 --> 00:46:42,100
what features are best there in a way that it wouldn't if the network is a bit bigger and if

681
00:46:42,100 --> 00:46:47,940
it's more capable of fitting your training data. But I don't really know. I think this

682
00:46:47,940 --> 00:46:51,540
is something that is probably worth looking at in some follow-up work.

683
00:46:52,420 --> 00:46:57,220
And you found also there's implications for transfer learning with respect to the loss

684
00:46:57,220 --> 00:47:03,540
function. There seems to be some inverse correlation between the gains you get from a loss

685
00:47:03,540 --> 00:47:09,220
function and how good it is for transfer learning. Or is there a connection between

686
00:47:09,220 --> 00:47:11,940
loss functions and regularizers and all of that?

687
00:47:12,660 --> 00:47:17,380
Yeah, we look at just linear transfer in the paper. So if we take the features from the

688
00:47:17,380 --> 00:47:22,420
penultimate layer and we try to use them directly for some other task, how good are those features

689
00:47:22,420 --> 00:47:30,900
going to be? And what we found was that if you use loss functions that give you higher accuracy

690
00:47:30,900 --> 00:47:37,940
on ImageNet, you tend to learn representations that transfer substantially worse in that setting.

691
00:47:38,900 --> 00:47:44,340
And our intuition is you could learn many different kinds of representations in that

692
00:47:44,340 --> 00:47:50,980
penultimate layer and still do a reasonable job of classifying ImageNet. But what seems to happen

693
00:47:50,980 --> 00:47:56,820
is that the loss functions that perform better lead classes to become more separated in the

694
00:47:56,820 --> 00:48:03,220
penultimate layer. So like class A and class B will be farther apart relative to the variability

695
00:48:03,220 --> 00:48:08,820
within the class. And when you have a situation like that, you have this penultimate layer

696
00:48:08,820 --> 00:48:14,260
representation that's specialized for the classes in ImageNet. Like you've got a thousand clusters

697
00:48:14,260 --> 00:48:20,180
corresponding to the thousand classes in ImageNet. And so then if you want to use like those kinds

698
00:48:20,180 --> 00:48:26,100
of representations for some other task, it will only really work well if you have exactly the

699
00:48:26,100 --> 00:48:32,260
same classes that are in ImageNet because they're already organized by the ImageNet classes.

700
00:48:32,340 --> 00:48:35,780
On the other hand, what we found is if you just use standard softmax loss,

701
00:48:35,780 --> 00:48:40,500
actually the classes are not that separated from each other in the penultimate layer

702
00:48:40,500 --> 00:48:45,860
representation. And because they're not that separated, there are these features that you

703
00:48:45,860 --> 00:48:51,620
could use to classify things that are not ImageNet that still convey some kind of

704
00:48:51,620 --> 00:48:57,060
useful information about the images that are not just their ImageNet class labels.

705
00:48:57,940 --> 00:49:03,700
It hints at a bit of a future where, you know, like right now on whatever TensorFlow Hub or

706
00:49:03,700 --> 00:49:09,220
HuggingFace repositories and so on, we have these pre-trend models. And the pre-trend models,

707
00:49:09,220 --> 00:49:16,100
they're like full stack models and people usually take some sort of last or next to last hidden layer.

708
00:49:16,820 --> 00:49:24,500
But maybe we should much more focus on actually providing like half of a network to share. Like

709
00:49:24,500 --> 00:49:30,580
determining which are actually the best good or general representations from a data set and so on.

710
00:49:30,580 --> 00:49:33,380
Do you have any of this in mind when you do work like this?

711
00:49:34,500 --> 00:49:41,060
Yeah, at Google, what is generally best for us to do is just to fine-tune the whole network.

712
00:49:41,060 --> 00:49:44,660
And if you fine-tune the whole network, it eliminates some of these issues with

713
00:49:45,300 --> 00:49:50,260
the actual form of the representation in the penultimate layer. Because even if you have this

714
00:49:50,260 --> 00:49:54,900
kind of highly specialized penultimate layer, when if you're allowed to change all the other

715
00:49:54,900 --> 00:49:58,580
weights in the network, you can fix that and you can specialize the rest of the network

716
00:49:59,140 --> 00:50:04,900
for some other task. But yeah, I think like it's a really interesting question. If we just want

717
00:50:04,900 --> 00:50:09,780
to turn an image into a vector that we could then train a linear classifier on top of,

718
00:50:09,780 --> 00:50:14,100
what is the best way of doing that? How should we approach that problem? And how should we approach

719
00:50:14,100 --> 00:50:19,620
that problem if we want this very like general universal vector representation of an image that

720
00:50:19,620 --> 00:50:24,020
would work well for a lot of different tasks? And I think we don't really have good ways of

721
00:50:24,020 --> 00:50:30,820
doing that because basically this is all empirical, right? Like, we don't know what makes a good

722
00:50:30,820 --> 00:50:35,620
universal representation of an image. We've just got to try a bunch of things and figure out what

723
00:50:35,620 --> 00:50:40,420
works best. And I guess, yeah, the insight from this paper is like actually the loss function

724
00:50:40,420 --> 00:50:45,940
that you use to train the network can make a huge difference there. Fascinating. I guess without

725
00:50:45,940 --> 00:50:52,180
any further ado, we should move on to SIMCLEAR, a simple framework for contrastive learning of

726
00:50:52,180 --> 00:50:57,540
visual representations. We've been absolutely fascinated by this concept of unsupervised

727
00:50:57,540 --> 00:51:02,820
contrastive image representation learning algorithms. We've seen such a huge kind of step

728
00:51:02,820 --> 00:51:09,060
forward, haven't we, over the last couple of years in this area? Yeah, it's pretty amazing to me.

729
00:51:09,620 --> 00:51:13,860
Could you just go back to real basics? Imagine that people out there have been living in a

730
00:51:13,860 --> 00:51:18,580
cave. They don't know what contrastive learning is. They don't know about image augmentation.

731
00:51:18,580 --> 00:51:23,940
How would you frame the whole thing up? The self-supervised learning setup is we've got a

732
00:51:23,940 --> 00:51:31,540
bunch of images and at least the initial like historical self-supervised learning setup is

733
00:51:31,540 --> 00:51:36,980
we've got a bunch of images. We want to train some kind of neural network on it. And we want

734
00:51:36,980 --> 00:51:40,980
that neural network to learn a representation such that when we then just train a linear

735
00:51:40,980 --> 00:51:45,780
classifier on top of that representation to classify ImageNet, it's going to do well. But

736
00:51:45,780 --> 00:51:51,780
we want to learn the initial representation without using any kind of labels. And yeah,

737
00:51:51,780 --> 00:51:56,980
I guess there are a lot of different approaches that people tried for this problem. Like people

738
00:51:56,980 --> 00:52:04,660
tried things like let's train a neural network so that we can cut up the image into just a grid

739
00:52:04,660 --> 00:52:09,860
and shuffle the grid. And then the neural network has to figure out how to assemble these puzzle

740
00:52:09,860 --> 00:52:14,900
pieces back into the original image. And maybe that'll give us a good representation. Or let's try

741
00:52:15,540 --> 00:52:21,460
just rotating the images so we can have images that are rotated 90, 180, 270 degrees. And then

742
00:52:21,460 --> 00:52:27,380
we'll have the neural network try to classify what rotation we fed into it. And so people came up

743
00:52:27,380 --> 00:52:32,340
with these kinds of tasks that you could try to train a neural network to do so that it would learn

744
00:52:32,340 --> 00:52:37,620
some kind of good representation. They were defined in this ad hoc way. Let's come up with some kind

745
00:52:37,620 --> 00:52:43,300
of funny thing where you don't need a label. You can have the neural network trained to do

746
00:52:43,300 --> 00:52:49,700
this kind of thing. And maybe it'll learn something about images. Starting in around 2018, there are

747
00:52:49,700 --> 00:52:57,380
a few papers that basically suggested this alternative approach where you're trying to learn

748
00:52:57,380 --> 00:53:03,780
some kind of representation space where you've got different patches from an image or different

749
00:53:03,860 --> 00:53:08,420
augmentations from an image, just different representations of the same image. And you want

750
00:53:08,420 --> 00:53:14,180
to learn a representation space where these representations of the same image are all close

751
00:53:14,180 --> 00:53:19,860
together in that representation space. And they're far apart from the representations of other images.

752
00:53:20,580 --> 00:53:27,220
This surprisingly seems to lead to very good representations. But it turns out there are a

753
00:53:27,220 --> 00:53:32,980
lot of very important details to get this to work well. So it's really like a situation where the

754
00:53:32,980 --> 00:53:39,300
basic idea is very simple. Let's create multiple views of an image and try to get them close to

755
00:53:39,300 --> 00:53:45,140
each other and far away from everything else. But things like augmentation and things like the

756
00:53:45,140 --> 00:53:50,820
exact way we set up the network end up being very important to learning a good representation with

757
00:53:50,820 --> 00:53:55,780
this kind of technique. How does the negative sampling work? People have done this in different

758
00:53:55,780 --> 00:54:01,620
ways. So in Simclear, our way of doing negative sampling is very simple. So basically,

759
00:54:02,340 --> 00:54:11,140
we are attracting two views of the same image. And then we have a mini batch that has 4,096

760
00:54:11,140 --> 00:54:18,420
images in it and two augmentations of each image. And so we are repelling using a softmax from all

761
00:54:18,420 --> 00:54:27,460
of the other 8,190 views in that mini batch. Basically, we want our two augmentations of

762
00:54:27,460 --> 00:54:33,140
the same image close and we want them to be far from the other 8,190 images.

763
00:54:34,420 --> 00:54:41,140
Yeah, it's a bit of a throwback to work to VEC. I think it's pretty cool how these ideas just come

764
00:54:41,140 --> 00:54:49,060
up through the eras and through the different models and so on. And there is seemingly always

765
00:54:49,060 --> 00:54:57,540
another layer on top of these ideas. Pretty cool. Yeah. So if you only consider, you know,

766
00:54:57,540 --> 00:55:02,900
two views that are coming out of the same image as the positive pair, so to speak, and all the other

767
00:55:02,900 --> 00:55:08,740
views are coming out of the different images located in the same mini batch, wouldn't this hurt

768
00:55:08,740 --> 00:55:15,780
the representation space to some extent? Let's say you have multiple images of dogs in a mini batch

769
00:55:16,180 --> 00:55:23,220
of 4,096 samples. We would essentially want the representations of different dogs to map together

770
00:55:23,220 --> 00:55:28,500
as closer as possible in the representation space while representations of cats from dogs would

771
00:55:28,500 --> 00:55:34,580
get further away. Wouldn't we expect this? But how does Simclear ensure this rigorously? I guess

772
00:55:34,580 --> 00:55:39,620
it's because of the larger batch sizes you use, but I still wanted to know from you.

773
00:55:39,620 --> 00:55:46,420
Yeah, one thing is, even if we've got other kinds of images that we want to be close together in the

774
00:55:46,420 --> 00:55:52,100
mini batch, even if we've got like a dog image and then another dog image and ultimately we want to

775
00:55:52,100 --> 00:55:58,500
learn a representation space where maybe they're not so far apart, like on average, most of the

776
00:55:58,500 --> 00:56:04,340
images in the mini batch are things that we want to be really far apart from. So maybe it doesn't

777
00:56:04,340 --> 00:56:12,180
hurt that much if we're repelling from everything as opposed to just repelling from images that are

778
00:56:12,180 --> 00:56:18,020
part of other classes. I think this actually is something that hurts current self-supervised

779
00:56:18,020 --> 00:56:22,260
learning techniques and hurts contrastive techniques because we also know when you do the

780
00:56:22,260 --> 00:56:29,220
contrastive loss, if you don't contrast against examples that are very close to you, that actually

781
00:56:29,300 --> 00:56:34,420
improves things a little bit. So if you don't contrast against the very hard negatives,

782
00:56:34,420 --> 00:56:39,860
we've found that gives you slightly higher accuracy when you do this linear evaluation.

783
00:56:39,860 --> 00:56:44,340
That kind of suggests that this really is a problem with these techniques that maybe sometimes you

784
00:56:44,340 --> 00:56:49,860
don't want to be as far apart from other images as the losses encouraging you to be. Now there's

785
00:56:49,860 --> 00:56:55,300
one other aspect which is that in Simclear, we don't actually use the representation that's

786
00:56:55,300 --> 00:57:01,780
feeding into the loss function. Like we have this projection head, an MLP on top of the network,

787
00:57:01,780 --> 00:57:06,900
and instead of using that representation at the end of the network, we use a representation

788
00:57:06,900 --> 00:57:14,100
that's two layers back. And so by using a representation that's two layers back, even if

789
00:57:14,100 --> 00:57:19,300
in the final layer we're pushing things apart, we kind of figure that this earlier representation

790
00:57:19,300 --> 00:57:24,020
might not have pushed apart the things that really are semantically similar. And indeed,

791
00:57:24,020 --> 00:57:29,860
we find that using this earlier representation in the network leads to higher linear evaluation

792
00:57:29,860 --> 00:57:37,140
accuracy. So it works better. I was very fascinated by all of these different tricks that you apparently

793
00:57:37,140 --> 00:57:44,500
have to get. And so big kudos to figuring all of this out for the rest of us. There has been a

794
00:57:44,500 --> 00:57:52,180
lot of follow-up work on this idea. A lot of modifications. There is this bootstrap your own

795
00:57:52,180 --> 00:57:59,060
latent where they completely leave out the negative sampling. Then I think just like one or two weeks

796
00:57:59,060 --> 00:58:07,460
ago, there was a paper saying if you build in a stop gradient into the contrastive loss,

797
00:58:07,460 --> 00:58:13,220
you also apparently don't need a negative and so on. Do you have maybe from your own work or

798
00:58:13,220 --> 00:58:21,300
from work of others, do you have any sort of current? If I were to build a self-supervised

799
00:58:21,380 --> 00:58:28,740
contrastive representation learner today, what is the top things I should do? What is my recipe?

800
00:58:28,740 --> 00:58:35,380
How do I go about it? The most important part of the recipe is data augmentation. So we're

801
00:58:35,380 --> 00:58:40,420
going to use two views from the same image and it's very important how those two views are

802
00:58:40,420 --> 00:58:47,460
constructed. But they're really only two super important data augmentations that we need. So

803
00:58:47,460 --> 00:58:53,540
we have to take two different crops from the same image and then we have to do some kind of color

804
00:58:53,540 --> 00:59:01,220
distortion. So in SimClear we use very aggressive color distortion. So that is probably the most

805
00:59:01,220 --> 00:59:06,900
important part of the recipe. Then I guess we feed that representation into a neural network

806
00:59:06,900 --> 00:59:12,420
and fortunately we found that you can just use a regular ResNet 50 for this part. You don't have

807
00:59:12,420 --> 00:59:19,060
to worry about architecture, engineering, specifically for contrastive learning. Then

808
00:59:19,060 --> 00:59:24,500
I think all of the work since SimClear also uses this idea of putting an MLP on top of the end of

809
00:59:24,500 --> 00:59:31,060
the network and then using that to get whatever representation goes into the loss function,

810
00:59:31,060 --> 00:59:37,300
but then discarding part of the MLP when we later just want the representation for a downstream

811
00:59:37,300 --> 00:59:45,540
task. All of those pieces are pieces that are shared by all of these modern self-supervised

812
00:59:45,540 --> 00:59:51,780
learning techniques. So like we introduced the idea of this projection head in SimClear and we

813
00:59:51,780 --> 00:59:56,660
also spend a lot of time studying the augmentation although we were not the first people to

814
00:59:57,620 --> 01:00:02,660
come up with the idea that the augmentation was important. Yeah, in terms of what the loss function

815
01:00:02,660 --> 01:00:08,180
is, I guess it's surprising that there are so many things that work that we use this contrastive

816
01:00:08,180 --> 01:00:13,060
loss in SimClear because it was what previous work had done and it's like intuitive that you might

817
01:00:13,060 --> 01:00:18,740
want to learn a space where you're explicitly pushing away representations of other examples,

818
01:00:19,540 --> 01:00:26,020
but I guess like in BYOL they aren't explicitly contrasting against representations of other

819
01:00:26,020 --> 01:00:32,740
examples. So instead they have a network where they're taking a moving average of

820
01:00:33,620 --> 01:00:38,580
the weights that they've been learning and they try to match the representation that's coming

821
01:00:38,580 --> 01:00:43,860
out of the network that they're training to this representation of this moving average network

822
01:00:44,420 --> 01:00:50,740
and somehow magically that works and I guess it doesn't even have to be a moving average. I think

823
01:00:50,740 --> 01:00:57,140
you were referring to earlier like you can just match the representation of one network to

824
01:00:57,860 --> 01:01:01,940
stop gradient of the same network as long as you're matching the representation in an earlier

825
01:01:01,940 --> 01:01:09,700
layer and I think like it's still like mysterious why that should work. I don't really have any

826
01:01:09,700 --> 01:01:19,700
insight into how either BYOL or the more recent papers actually are learning a representation

827
01:01:19,700 --> 01:01:24,580
that doesn't end up collapsing. The problem is if you're trying to match some earlier representation

828
01:01:25,140 --> 01:01:29,140
you could just collapse to the point where all of your representations are the same and then

829
01:01:29,700 --> 01:01:36,180
like you would trivially be matching the earlier representation, but this doesn't happen and I

830
01:01:36,180 --> 01:01:41,300
think why it doesn't happen relates to some mysteries about neural network training dynamics

831
01:01:41,300 --> 01:01:46,820
that we still don't entirely understand. I'm absolutely fascinated by this concept of data

832
01:01:46,820 --> 01:01:51,220
augmentation. Early on in my neural network career I just imagined it as being a way of

833
01:01:51,220 --> 01:01:56,820
increasing the size of your training set, but in a sense you're not really adding new information.

834
01:01:57,460 --> 01:02:03,380
You are creating semantically equivalent noise perturbations or examples similar to how BERT

835
01:02:03,380 --> 01:02:08,340
works the NLP model it's like a denoising autoencoder and you're creating noise diversions of the

836
01:02:08,340 --> 01:02:12,580
same thing and pushing the examples off the manifold. So there seems to be a dichotomy between

837
01:02:12,580 --> 01:02:16,260
on the one hand augmenting your data and it's almost like you're stopping the neural network

838
01:02:16,260 --> 01:02:21,780
from overfitting on things like the color or some specific feature you don't want to. You want to

839
01:02:21,780 --> 01:02:27,140
have a bit of generalization, but at the same time you are saying those things over there it's

840
01:02:27,140 --> 01:02:31,700
definitely nothing like that. The data augmentation that you need for contrastive learning is

841
01:02:31,700 --> 01:02:35,780
different from the data augmentation that you need for supervised learning because the task is

842
01:02:35,780 --> 01:02:41,220
different. When you have contrastive learning you have this problem that if there's just one

843
01:02:41,220 --> 01:02:47,780
feature in your data that can be used to do the contrastive task to get images of the same example

844
01:02:47,780 --> 01:02:52,260
or views of the same example close together and far apart from views of all the other examples.

845
01:02:52,260 --> 01:02:56,340
If you could do that with one feature that would be the only feature the network would

846
01:02:56,340 --> 01:03:01,460
ever learn or it might be the only feature the network would ever learn. And so with the augmentation

847
01:03:01,460 --> 01:03:04,820
you're making the task harder so that the network actually has to learn

848
01:03:05,780 --> 01:03:11,220
many different kinds of features. So I guess we find that this color distortion

849
01:03:11,220 --> 01:03:15,780
actually is very important for self-supervised learning, for contrastive learning,

850
01:03:15,780 --> 01:03:21,700
whereas it doesn't really matter for supervised learning. And what we think is going on is that

851
01:03:21,700 --> 01:03:27,700
if you have two crops from the same image, generally their color histograms are surprisingly

852
01:03:27,700 --> 01:03:33,940
similar. If you just plot out the intensity histogram of the image you can see that the

853
01:03:33,940 --> 01:03:39,540
crops came from the same image. And that's a trick that the network is very good at doing

854
01:03:39,540 --> 01:03:44,100
because I guess if you have ReLU activations they're very good at computing histograms.

855
01:03:44,820 --> 01:03:50,500
And so by doing the color distortion we basically we don't let the network just learn

856
01:03:51,140 --> 01:03:55,620
the color histograms in order to do the contrastive task. We force the network

857
01:03:55,620 --> 01:04:00,660
to actually use other sorts of information and that ends up being like critical to the

858
01:04:00,660 --> 01:04:04,980
performance of these contrastive methods. Like it basically doesn't work unless you do

859
01:04:05,700 --> 01:04:10,180
that kind of aggressive color distortion. Because that seems to be the key thing then. So you're

860
01:04:10,180 --> 01:04:16,580
not telling it to learn things, you're telling it not to learn things. We're telling it to learn

861
01:04:16,580 --> 01:04:22,180
one thing. We're telling it to learn figure out which views came from the same image. But then,

862
01:04:22,180 --> 01:04:28,740
yeah, we have to make sure that it learns to do that with a diverse set of features instead

863
01:04:28,740 --> 01:04:34,180
of just doing it in one way. Because I guess it's like a task that's actually pretty easy to do if

864
01:04:34,180 --> 01:04:41,060
you don't have this kind of aggressive augmentation. Yeah, I think in a way it helps the network to

865
01:04:41,060 --> 01:04:46,580
also differentiate what actually what is the thing that differentiates two images. I think

866
01:04:47,300 --> 01:04:53,380
it helps the network to learn, you know, pick up on that signal. To that end, I also wanted to ask

867
01:04:53,380 --> 01:04:59,780
for a custom dataset, if I wanted to, you know, apply a sim clear, what pointers should I take

868
01:04:59,780 --> 01:05:04,820
into consideration while designing my augmentation policy? I'm sure you have been asked about this

869
01:05:04,820 --> 01:05:09,700
question quite a few times. But yeah, I think it's a good question. Like I think like we actually

870
01:05:09,700 --> 01:05:14,740
still don't really know how generalizable these contrastive learning techniques are beyond image

871
01:05:14,740 --> 01:05:20,900
net. Like we know they work super well on image net. But like image net is like a boring dataset to

872
01:05:20,900 --> 01:05:24,980
apply contrastive learning to because we actually already have all the labels and we could just be

873
01:05:24,980 --> 01:05:32,660
doing supervised learning. But I think starting with the crop and color augmentation is definitely a

874
01:05:32,660 --> 01:05:37,780
good idea, at least like for datasets that have color, I guess if you don't have color, then maybe

875
01:05:37,780 --> 01:05:45,780
think about distorting intensities instead of colors. But beyond that, I think it depends on the

876
01:05:45,860 --> 01:05:51,460
specific task and what you really want the neural network to pick up out of the dataset.

877
01:05:52,020 --> 01:05:56,820
I feel like there are probably some sorts of data where I wouldn't really expect

878
01:05:57,620 --> 01:06:04,340
contrastive learning to work well. So for example, like if you try to do contrastive learning on a

879
01:06:04,340 --> 01:06:11,620
dataset of medical images where you've just got healthy patients, and then you want to translate

880
01:06:11,620 --> 01:06:17,620
that to like some sort of dataset of people with some kind of pathology, you might never pick up the

881
01:06:17,620 --> 01:06:22,900
features that are important for detecting the pathology. But yeah, I think this question of how

882
01:06:22,900 --> 01:06:27,860
do you design the augmentation, what augmentation works well for datasets that maybe aren't natural

883
01:06:27,860 --> 01:06:34,020
images like these kinds of medical images or maybe like satellite images. That's an important

884
01:06:34,020 --> 01:06:39,300
question that we haven't addressed yet. There seems to be this fascinating universality of

885
01:06:39,300 --> 01:06:43,860
representations, especially in vision. This is exactly the kind of thing you can test with your

886
01:06:43,860 --> 01:06:49,700
wonderful similarity matrix idea. I'm not trying to be flippant when I say this because

887
01:06:49,700 --> 01:06:55,300
practitioners have used ImageNet on a variety of downstream tasks. For example, they might use it for

888
01:06:55,300 --> 01:07:00,020
classifying circuit boards or something. And the miraculous thing is it just seems to work quite

889
01:07:00,020 --> 01:07:05,780
well. So do you think in your opinion that there is some kind of universality? I'm very skeptical

890
01:07:05,780 --> 01:07:12,980
about like universality of ImageNet for different tasks. Like in the past, we did some work where

891
01:07:12,980 --> 01:07:19,380
we looked at how well ImageNet networks transfer to other tasks. And it seems like

892
01:07:20,180 --> 01:07:27,060
there are actually some tasks which are just like datasets of natural images where pre-training an

893
01:07:27,060 --> 01:07:33,380
ImageNet doesn't really help at all. And those datasets just seem to be too different from ImageNet.

894
01:07:33,380 --> 01:07:40,100
They're things like this Stanford cars dataset where you have to like classify different cars

895
01:07:40,100 --> 01:07:45,140
according to their make, model, and year. It turns out even though there are lots of cars in ImageNet,

896
01:07:45,140 --> 01:07:50,420
if you pre-train on ImageNet and you fine-tune on that dataset, you will learn to classify it

897
01:07:50,420 --> 01:07:56,100
faster in fewer steps than if you had trained from scratch on the Stanford cars dataset.

898
01:07:56,820 --> 01:08:01,700
But you won't actually perform any better at the end. And that's true even though the

899
01:08:01,780 --> 01:08:08,660
Stanford cars dataset has 10,000 images. So it's tiny compared to ImageNet. So I think like actually

900
01:08:08,660 --> 01:08:16,020
representations of images are not that universal. And at least what works for natural images for

901
01:08:16,020 --> 01:08:22,820
images like those in ImageNet may not work on other datasets. I think there's also like limited

902
01:08:22,820 --> 01:08:29,220
evidence for transfer from ImageNet to medical datasets. It seems if you don't like work really

903
01:08:29,220 --> 01:08:35,380
hard at tuning hyperparameters or if you don't train for long enough, you will get better accuracy

904
01:08:35,380 --> 01:08:40,180
by starting with an ImageNet pre-trained network. But if you do very thorough experiments and you

905
01:08:40,180 --> 01:08:44,820
train for long enough, you try different learning rates and weight decay parameters, like actually

906
01:08:44,820 --> 01:08:50,500
it seems like training from scratch on most medical datasets will give you the same accuracy as if

907
01:08:50,500 --> 01:08:56,260
you started from a network that's pre-trained on some other giant dataset. Maybe this makes sense

908
01:08:56,260 --> 01:09:02,660
because if you think about radiologists, like it's not like a radiologist can just like at the

909
01:09:02,660 --> 01:09:08,100
beginning of their education, they can't just look at an MRI or an X-ray image and say this is

910
01:09:08,100 --> 01:09:13,860
where the tumor is. It's something that takes them years of training to learn how to do. And so maybe

911
01:09:13,860 --> 01:09:19,540
it also makes sense that like our neural networks can't just immediately easily without lots of

912
01:09:19,540 --> 01:09:27,220
training pick up on very different image distributions. It does seem to make sense,

913
01:09:27,220 --> 01:09:34,340
going a bit from the universality of representations to the universality of

914
01:09:34,340 --> 01:09:41,780
augmentation, since this is such a crucial part. Do you think that there is a systematic way how

915
01:09:41,780 --> 01:09:47,300
we can discover augmentations? Because it seems right now, it seems to be kind of a whack-a-mole,

916
01:09:47,300 --> 01:09:52,180
right? It's okay, we just feed images and it's no, that's too easy. We crop them. Oh no, it's the

917
01:09:52,180 --> 01:09:57,620
color histogram. So we like whack on the color and then it works better. But maybe someone finds out

918
01:09:57,620 --> 01:10:02,900
oh, there is still this easy feature that the network every now and then pays attention to. So

919
01:10:02,900 --> 01:10:10,020
we design a method to whack on that a bit. Do you think there is a systematic way or will this

920
01:10:10,020 --> 01:10:16,340
kind of philosophically always rely on us humans having a higher level inside of what we want to

921
01:10:16,340 --> 01:10:24,100
do with the dataset? Yeah, so I think actually I'm hopeful that at least for natural images,

922
01:10:24,100 --> 01:10:29,780
just like crops and color distortions are enough, because I guess like what we found is

923
01:10:29,780 --> 01:10:35,540
you combine those two augmentations and if you do that, like that gets you most of the way to

924
01:10:35,540 --> 01:10:41,140
supervised accuracy. So maybe we shouldn't expect huge gains from adding additional augmentations on

925
01:10:41,140 --> 01:10:46,260
top of that, even though there are like in the Sinclair paper, we add like Gaussian blur, which

926
01:10:46,260 --> 01:10:52,100
gives slight gains on top of that. I guess in the BYL paper, they add even more augmentations on top

927
01:10:52,100 --> 01:10:58,180
of that. So you can get small gains, but it seems like the gains are much smaller once you've got

928
01:10:58,180 --> 01:11:03,860
the crops and the color distortions there. In terms of systematic ways of discovering

929
01:11:04,420 --> 01:11:09,620
what set of augmentations we should be using, I guess there's a paper that I saw where

930
01:11:10,420 --> 01:11:16,420
they basically use linear evaluation on the rotation prediction task to see whether the

931
01:11:16,420 --> 01:11:21,860
augmentations are good and they claim that actually works for evaluating the augmentations.

932
01:11:21,860 --> 01:11:28,340
So maybe that's one way, I don't know. There are all sorts of ways of designing augmentations

933
01:11:28,340 --> 01:11:35,540
for supervised learning that could conceivably be applied to the self-supervised learning setting,

934
01:11:35,620 --> 01:11:41,060
to the contrastive setting. There are these meta-learning based approaches for learning

935
01:11:41,060 --> 01:11:47,700
data augmentation. I'm not sure, those techniques tend to be pretty complicated. I'm not sure whether

936
01:11:47,700 --> 01:11:53,140
it's actually easier to deal with those techniques than just like trying a bunch of things, but I

937
01:11:53,140 --> 01:11:59,460
think that maybe it doesn't matter that much. Maybe like just, at least if you're dealing with natural

938
01:11:59,460 --> 01:12:07,300
images, maybe crop and color distortion is enough. I guess if you think about other images,

939
01:12:07,300 --> 01:12:11,220
I don't really have any idea. I guess it depends on what the images look like.

940
01:12:11,940 --> 01:12:17,300
There are lots of things that you could be expressing as images like a spectrogram or

941
01:12:17,300 --> 01:12:22,420
like some kind of chart or whatever, where you could be applying a neural network to it,

942
01:12:22,420 --> 01:12:27,380
but the further you get from natural images, the less clear it is what kind of augmentations

943
01:12:27,460 --> 01:12:32,740
you should be working with. It is a fascinating thought though, this universality of augmentations.

944
01:12:32,740 --> 01:12:39,780
When you said cropping and color, that made me think it seems to be related to the inductive

945
01:12:39,780 --> 01:12:45,940
priors in the CNN architecture that we use and also to things like its regularly sampled, gridded,

946
01:12:46,500 --> 01:12:50,580
discrete image data, because we're speaking with Max Welling the other week and as he's

947
01:12:50,580 --> 01:12:56,100
created lots of other interesting inductive priors for computer vision models. It does set

948
01:12:56,180 --> 01:13:00,100
my mind racing a little bit because presumably there's a continuum. On the one hand, we don't do

949
01:13:00,100 --> 01:13:04,740
any augmentation and we just learn from examples. In the middle, we do the augmentation and then

950
01:13:04,740 --> 01:13:08,580
maybe in the future, because some people have said that computer vision systems don't have seen

951
01:13:08,580 --> 01:13:12,900
understanding. They don't understand physics. The ball might be on the table, but we don't know that

952
01:13:12,900 --> 01:13:17,780
it's not falling and so on. There's a lot of missing information. Would the next step be some

953
01:13:17,780 --> 01:13:22,420
simulation? Do you know what I mean? Where we impute physics and we impute some world knowledge

954
01:13:22,420 --> 01:13:24,980
and then I don't know whether we train a machine learning model from that?

955
01:13:25,620 --> 01:13:31,380
Yeah, I think there are definitely shortcomings in our current machine learning models,

956
01:13:31,380 --> 01:13:36,660
understandings of the world. There are probably things that we can't just solve by throwing more

957
01:13:36,660 --> 01:13:43,380
static images at them. I think maybe the next step, rather than trying to immediately situate

958
01:13:43,380 --> 01:13:49,780
the machine learning model in a simulated world, we could just think about video. I think there's

959
01:13:49,780 --> 01:13:55,380
already a lot of additional information in video that a neural network could use to learn

960
01:13:55,380 --> 01:14:02,100
interesting representations. It seems like if you just see static images, it's hard to learn

961
01:14:02,100 --> 01:14:08,500
how to segment objects. It's hard to learn where the object's boundaries are, but once you have

962
01:14:08,500 --> 01:14:13,300
video, it's like the stuff that's moving together is an object and you can tell that because it's

963
01:14:13,300 --> 01:14:18,980
moving together. I think there's a lot of potential for learning better visual representations

964
01:14:20,740 --> 01:14:26,020
and maybe eventually from these kinds of interactions in simulated environments. I

965
01:14:26,020 --> 01:14:32,020
think ultimately it becomes a computational headache. Even video is a computational headache

966
01:14:32,020 --> 01:14:36,820
because suddenly you've got all of these frames that you have to deal with. You probably want to be

967
01:14:38,260 --> 01:14:45,460
thinking about how representations change over time and video data is just huge. It's especially

968
01:14:45,460 --> 01:14:53,620
huge if you have to process many frames at once on your accelerators. I think that's why this hasn't

969
01:14:53,620 --> 01:15:00,100
taken off yet, but I think probably representation learning from video is going to be a big thing

970
01:15:00,100 --> 01:15:07,780
next year or the year after or sometime in the near future. We would love to talk about your big

971
01:15:07,780 --> 01:15:12,180
self-supervised models, our strong semi-supervised learners. This is super interesting because

972
01:15:12,180 --> 01:15:16,020
you're combining the unsupervised stuff that we've been talking about in Simclear, but now

973
01:15:16,020 --> 01:15:21,540
we're in the semi-supervised domain where the label efficiency becomes super important. What's

974
01:15:21,540 --> 01:15:28,580
the deal there? Yeah. I guess in Simclear we focus on this question of linear evaluation

975
01:15:28,580 --> 01:15:34,100
accuracy. We're just learning a representation without any labels and then training a linear

976
01:15:34,100 --> 01:15:39,700
classifier on top of that representation on the same data, but now with all the labels. It turns

977
01:15:39,780 --> 01:15:46,260
out that's not really a very practical problem if you have all the labels. There's not necessarily any

978
01:15:46,260 --> 01:15:52,580
reason in practice that you would want to first learn this representation and then train the

979
01:15:52,580 --> 01:15:58,660
classifier versus just doing standard supervised end-to-end training. What is a practical problem

980
01:15:58,660 --> 01:16:05,460
is the situation where you have a lot of unlabeled data and then a very small amount of labeled

981
01:16:05,460 --> 01:16:14,260
data. That's the situation that we look at in Simclear v2 in that paper. What we find there is

982
01:16:14,260 --> 01:16:22,180
that you can train this network fully unsupervised without using the labels on all the data and then

983
01:16:22,180 --> 01:16:27,780
you can fine-tune it on just the subset where you've got the labels. If you do that, it's possible to

984
01:16:27,780 --> 01:16:35,140
get very high accuracy, especially if the network is very big. Basically, we find if you have a

985
01:16:35,140 --> 01:16:40,100
really big ResNet, if you have ResNet 152 and then you make the layers three times wider,

986
01:16:40,660 --> 01:16:47,940
when you do that, you can get accuracy when you fine-tune on 10% of the labels that's substantially

987
01:16:47,940 --> 01:16:54,020
better than if you trained ResNet 50 from scratch with all the labels. Once you have that really big

988
01:16:54,020 --> 01:16:58,180
network, it turns out you don't have to put the really big network into production. You can take

989
01:16:58,180 --> 01:17:04,900
the really big network and you can then distill it back into a standard ResNet 50 and you can retain

990
01:17:04,900 --> 01:17:09,540
almost all of the accuracy when you do that. I guess what's important about this distillation

991
01:17:09,540 --> 01:17:14,980
process is we're not just going to distill on the labeled dataset. We're going to also use

992
01:17:15,860 --> 01:17:22,740
the labels that this giant network, which we fine-tuned on a small subset of the data,

993
01:17:22,740 --> 01:17:28,180
we're going to use the labels that it gives on all of our unlabeled data. We're going to use it to

994
01:17:28,180 --> 01:17:33,460
generate labels and then we're just going to use those labels to train a much smaller network. If

995
01:17:33,460 --> 01:17:39,700
we do that, we get accuracy that's similar to or maybe even slightly better than standard supervised

996
01:17:39,700 --> 01:17:46,980
training from scratch. This becomes a highly practically relatable approach toward doing

997
01:17:47,540 --> 01:17:54,420
computer vision related things. We see all the times that folks have a huge corpus of unlabeled

998
01:17:54,420 --> 01:18:02,020
images, but they only have maybe 5% or 10% labeled images. This immediately becomes a practically

999
01:18:02,100 --> 01:18:07,460
applicable recipe for them. I definitely am looking forward to seeing this thing

1000
01:18:07,460 --> 01:18:13,300
implemented at scale at different companies. That's there. If I understood it correctly,

1001
01:18:13,300 --> 01:18:20,100
just for the viewers, you folks used a variant of distillation here, which is more popularly

1002
01:18:20,100 --> 01:18:24,340
referred to as self-training. I don't think there's really a difference between

1003
01:18:25,140 --> 01:18:31,380
what we call distillation and what other people call self-training. I guess the idea is basically

1004
01:18:31,460 --> 01:18:37,540
we will pass information into the network. We get its output probabilities and then we train

1005
01:18:37,540 --> 01:18:41,380
another neural network with those output probabilities as the targets.

1006
01:18:42,180 --> 01:18:48,660
What I find fascinating is how many ideas come together in this paper. There's first,

1007
01:18:49,380 --> 01:18:56,100
there's this, let's do representation learning and then we have these just small labels. We find

1008
01:18:56,100 --> 01:19:02,580
tune and then there's big networks, small networks. Then you label, but you also apply

1009
01:19:02,580 --> 01:19:09,060
some noise, if I understand correctly, in the process of transferring. Maybe I'm misremembering

1010
01:19:09,060 --> 01:19:15,060
that, but there's a lot of ideas that come together. Yannick, you probably confused it with

1011
01:19:15,060 --> 01:19:20,260
noisy student training, where they impose noise during the student training.

1012
01:19:20,820 --> 01:19:27,060
Sorry, maybe not, but there is a lot of ideas that come together. Something tells me that

1013
01:19:27,060 --> 01:19:35,300
there was a process behind going, you probably didn't sit down after SimClear1 and be like,

1014
01:19:35,300 --> 01:19:40,100
all right, what do we do for SimClear2? Okay, let's do this. It tells me there was this process.

1015
01:19:40,740 --> 01:19:48,020
If you can maybe elaborate a bit on how did you going to build up the system towards the final

1016
01:19:48,020 --> 01:19:54,420
output? Was there dead ends or was it like, let's build up until we can no longer make it better?

1017
01:19:54,420 --> 01:20:03,460
How was that? Yeah, I guess there was a bit of a process. After the original SimClear paper,

1018
01:20:03,460 --> 01:20:08,340
I guess it's clear from the SimClear paper that when we have this bigger network, we get much

1019
01:20:08,340 --> 01:20:16,020
higher linear evaluation accuracy than we do if we just train SimClear with a ResNet50. Then the

1020
01:20:16,020 --> 01:20:22,740
question was, is there some way that we can somehow eliminate this dependence on this giant

1021
01:20:22,740 --> 01:20:27,140
network? Because the giant network is annoying to work with, it's computationally expensive,

1022
01:20:27,140 --> 01:20:35,220
it's big. We first tried what happens if you distill the unsupervised network. We basically

1023
01:20:35,220 --> 01:20:42,020
have this task that is set up as a form of cross entropy loss when we're doing the contrast of

1024
01:20:42,020 --> 01:20:50,420
learning. You can also think about distilling on that task where you have a probability distribution

1025
01:20:50,420 --> 01:20:55,060
that corresponds to the similarity between an image and all the other images. You could use

1026
01:20:55,060 --> 01:21:00,900
those kinds of targets to distill. We tried that and that kind of worked. Then we also tried the

1027
01:21:00,900 --> 01:21:06,180
approach of first fine-tuning the big network and then distilling it. It turned out that worked

1028
01:21:06,260 --> 01:21:14,580
a lot better. I guess we jumped straight to distillation because we knew that we could get

1029
01:21:14,580 --> 01:21:21,620
much better results by using a giant network with SimClear. Then once you realize that distillation

1030
01:21:21,620 --> 01:21:26,740
is going to be important, the only thing you've got to figure out is what kind of distillation

1031
01:21:26,740 --> 01:21:31,940
should you be doing. What we found was this approach of pre-training, then fine-tuning,

1032
01:21:31,940 --> 01:21:37,060
then distilling works a lot better than pre-training, then distilling, then fine-tuning.

1033
01:21:37,620 --> 01:21:45,860
How far down do you think one can go with this final distillation step? Is this something that is

1034
01:21:45,860 --> 01:21:51,860
conceivably going to be available on, let's say, edge devices at some point, like that

1035
01:21:52,980 --> 01:21:59,140
our glasses or something run with similar accuracies to these giant networks? Or

1036
01:22:00,020 --> 01:22:03,700
is it more a factor of four, a factor of 10 kind of stuff?

1037
01:22:04,660 --> 01:22:10,980
I think there are clearly some limits to distillation. I guess we probably shouldn't

1038
01:22:10,980 --> 01:22:16,980
expect distillation of the kind that we do in SimClear v2 to work substantially better than

1039
01:22:17,620 --> 01:22:23,780
supervised distillation, which has been around for quite a while now. I think what's impressive is

1040
01:22:23,860 --> 01:22:30,260
that in the self-supervised case, in the contrastive case, distillation basically allows you to

1041
01:22:30,260 --> 01:22:35,220
recover the same accuracy that you would get from training supervised from scratch, whereas without

1042
01:22:35,220 --> 01:22:42,340
it, the accuracy is a lot worse. It seems like it maybe matters more in this contrastive case,

1043
01:22:42,980 --> 01:22:48,820
but I think generally when you do distillation in the supervised case, you can get maybe a

1044
01:22:48,820 --> 01:22:54,980
percentage point gain, maybe a couple of percentage points. I think that's probably about the limit

1045
01:22:54,980 --> 01:23:00,900
in terms of the improvement that you could get from any kind of distillation-based approach

1046
01:23:00,900 --> 01:23:06,900
over supervised training from scratch. Fascinating. I don't know if you know that we've been playing

1047
01:23:06,900 --> 01:23:11,860
with GPT-3 and you said something quite interesting just now. You said that they're deterministic,

1048
01:23:11,860 --> 01:23:16,020
but in GPT-3, that's not really the case. If you sample from it deterministically,

1049
01:23:16,020 --> 01:23:23,220
it gets stuck in cycles. You have to do some kind of trickery, some kind of random sampling from

1050
01:23:23,220 --> 01:23:28,740
this distribution. It might be the case in future computer vision models as well, that we have to

1051
01:23:28,740 --> 01:23:32,740
randomly sample from them in some way, because otherwise it would get into some pathological

1052
01:23:32,740 --> 01:23:37,780
behavior. Maybe to do that, we need to have some kind of controller on the top. I suppose my question

1053
01:23:37,780 --> 01:23:42,260
is in the future, maybe we will be in the stochastic regime. What do you think about that?

1054
01:23:43,140 --> 01:23:49,620
With GPT-3, you're trying to generate data. When you generate data, there has to be some

1055
01:23:49,620 --> 01:23:54,500
kind of noise that's coming from somewhere. The process of generating data is like turning

1056
01:23:54,500 --> 01:24:00,980
noise into data. For image classification, we have the data and we just want to turn it into

1057
01:24:00,980 --> 01:24:07,380
a label. Maybe there's this implicit notion of stochasticity in that the network gives some

1058
01:24:07,860 --> 01:24:13,540
output distribution. I think we still want everything to be deterministic if it can be

1059
01:24:13,540 --> 01:24:19,620
deterministic. We basically want the network to say, this is a dog with probability x.

1060
01:24:20,340 --> 01:24:25,860
If there's some way to improve it with stochasticity, I don't know. I guess dropout used to be very

1061
01:24:25,860 --> 01:24:30,980
popular, but it seems like it's not so popular anymore and it doesn't really help us very much

1062
01:24:30,980 --> 01:24:37,860
with vision models. Also, even dropout is generally only used when we train the neural

1063
01:24:37,860 --> 01:24:46,260
networks. On the other hand, the brain is very stochastic. The brain has lots of noise. I guess

1064
01:24:46,260 --> 01:24:50,900
that suggests that maybe there's some way to leverage noise to learn better representations

1065
01:24:50,900 --> 01:24:54,340
in neural networks as well and we just don't quite know the right way yet.

1066
01:24:54,900 --> 01:24:59,140
That's right. Max Welling said to us that he thinks that the future of AI will have a

1067
01:24:59,140 --> 01:25:04,660
generative component. He thinks that we have the matrix in our minds. We have these simulations

1068
01:25:04,660 --> 01:25:11,220
going on all the time and we're generating new scenarios. It's related to the data

1069
01:25:11,220 --> 01:25:16,340
augmentation thing as well. Some people have said to me in the past that using a GAN might be a way

1070
01:25:16,340 --> 01:25:21,940
of doing data augmentation. Presumably, that would require some kind of stochastic sampling as well.

1071
01:25:21,940 --> 01:25:25,380
I suppose it's just quite interesting to see where these two things might meet in the middle at

1072
01:25:25,380 --> 01:25:30,900
some point. Max Yeah. I don't know. I guess like with a GAN, using a GAN to do data augmentation,

1073
01:25:30,900 --> 01:25:35,940
you have this problem that you still don't actually have more data. You have a GAN that's

1074
01:25:35,940 --> 01:25:42,340
trained on the same data. It might help you because your way of encoding inductive bias into the GAN

1075
01:25:42,340 --> 01:25:48,100
is different from your way of encoding inductive bias into the neural network. Maybe by having

1076
01:25:48,100 --> 01:25:54,500
more inductive bias, you can learn a better function. You still don't have more data at it.

1077
01:25:55,300 --> 01:26:01,300
Without having more data, there's no reason to expect a priority that you will be able to learn

1078
01:26:01,300 --> 01:26:06,340
a better function. Paul I'm so glad you said that. It was always my intuition. The amount of people

1079
01:26:06,340 --> 01:26:09,700
that have said to me that you should use a GAN for data augmentation. Anyway.

1080
01:26:09,700 --> 01:26:14,660
Max What's the first thing you think about if you're like, oh, what could I use a GAN for?

1081
01:26:14,660 --> 01:26:20,020
And then you learn about data, and they're like, wait, this is so much more data. Yeah,

1082
01:26:20,020 --> 01:26:26,020
but conceptually, yes, you don't have more data. And ironically, when you do the simple

1083
01:26:26,020 --> 01:26:31,780
data augmentation, you do have more data because you put all the knowledge in there

1084
01:26:31,780 --> 01:26:39,060
as a human of what makes two images dissimilar visually, but still equivalent semantically,

1085
01:26:39,060 --> 01:26:45,140
which again, is exactly the opposite. It gives you images that are visually similar,

1086
01:26:45,140 --> 01:26:51,700
but it has no intuition of what the semantic similarity is. For my last question, I actually

1087
01:26:51,700 --> 01:26:57,460
want to switch topics just a little bit to what Tim said at the beginning, namely your love of Julia.

1088
01:26:58,260 --> 01:27:05,540
So I have seen a number of especially data scientists be strong advocates for Julia as a

1089
01:27:05,540 --> 01:27:12,180
language and so on. Do you want to give anyone sort of the pitch? Why should we even consider this?

1090
01:27:13,060 --> 01:27:19,300
Yeah, so I think Julia is a much better programming language than Python in many ways.

1091
01:27:19,940 --> 01:27:24,980
I guess one thing and I guess the thing that first attracted me to Julia is that it's really

1092
01:27:24,980 --> 01:27:30,820
fast, like you can write Julia code and with very little work, you will end up running as

1093
01:27:30,820 --> 01:27:36,820
fast as equivalent C code. So that's something that you can't get out of standard Python code.

1094
01:27:36,820 --> 01:27:41,300
If you're just writing a for loop in Python, it's going to be super slow. But in Julia,

1095
01:27:41,300 --> 01:27:44,900
you don't have to worry about all of that. And you don't have to worry about like Cython or

1096
01:27:44,900 --> 01:27:49,860
number all of this other stuff that people have hacked on top of Python. Julia is just

1097
01:27:49,860 --> 01:27:57,620
designed to be fast and it works. I think there are other advantages to Julia as a language

1098
01:27:57,620 --> 01:28:04,820
beyond that. I guess it's built on this idea of generic functions where you have a function that

1099
01:28:05,860 --> 01:28:10,100
can take multiple types and you can define the function differently for the different types.

1100
01:28:10,660 --> 01:28:15,220
And this is something that we do all the time when we're doing like machine learning, like we

1101
01:28:15,220 --> 01:28:20,500
have matrix multiplication, which is a different form of multiplication that takes matrices and

1102
01:28:20,500 --> 01:28:28,020
produces something. And I guess like in Python, it's so it used to be that you had to type dot dot

1103
01:28:29,300 --> 01:28:34,980
to multiply things, but now it's like they have this add symbol that does matrix multiplication.

1104
01:28:34,980 --> 01:28:39,700
But Julia is designed for these situations where maybe beyond just matrices,

1105
01:28:39,700 --> 01:28:44,180
you have these funny types of structured matrices, you have sparse matrices, and you can define

1106
01:28:44,180 --> 01:28:49,860
special methods for the product of a sparse matrix and a vector or all sorts of things where you

1107
01:28:49,860 --> 01:28:55,940
might want different methods depending on the types. And even though it seems like this is

1108
01:28:55,940 --> 01:29:00,900
complicated and you might have some trouble picking which version of the function is going to be called

1109
01:29:00,900 --> 01:29:08,980
at runtime, because Julia is ultimately compiling everything when you call it. And because it has

1110
01:29:08,980 --> 01:29:16,580
this kind of strong type system, it can ultimately pick which method is going to be used and compile

1111
01:29:16,580 --> 01:29:21,620
that method call in and you don't have to worry about picking which one. And so it still ends up

1112
01:29:21,620 --> 01:29:29,620
being fast. I also think like there's this question of whether like the object oriented Python or

1113
01:29:29,620 --> 01:29:36,900
the object oriented paradigm and Python is really like the best paradigm for machine learning.

1114
01:29:36,900 --> 01:29:42,980
Because I guess like it's like we have data and then we have functions that operate on the data.

1115
01:29:42,980 --> 01:29:46,740
But in the object oriented paradigm, you want the functions that operate on the data to be

1116
01:29:46,740 --> 01:29:52,820
attached to the data, which is like a weird way of setting things up. And that Julia is not set up

1117
01:29:52,820 --> 01:29:58,820
that way. You have these data structures. And then because you are able to create functions that

1118
01:29:58,820 --> 01:30:04,260
specialize on the data structures, you don't have to worry about attaching those functions to the

1119
01:30:04,340 --> 01:30:10,660
data structures themselves. Amazing. Dr. Simon Cornblith. Thank you very much for joining us

1120
01:30:10,660 --> 01:30:15,780
this evening. It's been an absolute pleasure. Thanks for having me. Thank you. I really hope

1121
01:30:15,780 --> 01:30:21,140
you've enjoyed the show today. We've had so much fun making it. Remember to like, comment,

1122
01:30:21,140 --> 01:30:26,180
and subscribe. We love reading your comments, every single one of them. And we'll see you back

1123
01:30:26,180 --> 01:30:29,060
next week.

