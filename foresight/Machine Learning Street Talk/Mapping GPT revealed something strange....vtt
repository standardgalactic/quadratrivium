WEBVTT

00:00.000 --> 00:07.860
To me, the difference feels like language models start with this highly abstract language

00:07.860 --> 00:08.960
representation.

00:08.960 --> 00:13.720
The system as a whole can try to predict the next token with greater and greater accuracy.

00:13.720 --> 00:18.840
And so the difference it seems is that the adversarial inputs for us tend to look a lot

00:18.840 --> 00:21.880
different than the adversarial examples for LLM.

00:21.880 --> 00:27.600
Once you try and go outside of this sphere of what is meaningful to humans, the possibilities

00:27.600 --> 00:28.600
grow exponentially.

00:28.800 --> 00:34.200
I was recently in Toronto, a beautiful city to film with Co here, and hold on, those videos

00:34.200 --> 00:39.040
will come out very shortly, but around the same time someone shared a paper on our Discord

00:39.040 --> 00:41.600
server and it's called What's the Magic Word?

00:41.600 --> 00:46.640
A Control Theory of Prompting Large Language Models, and it's by Amon Begava and Cameron

00:46.640 --> 00:47.640
Wachowski.

00:47.640 --> 00:53.560
Now, what these guys did is theoretically think about a language model as a dynamical

00:53.560 --> 00:59.440
system and use the lens of control theory to think about the space of reachability.

00:59.440 --> 01:00.440
Why is this important?

01:00.440 --> 01:06.600
Well, language models, we think that they think in language space, this abstract language

01:06.600 --> 01:08.360
space, but they don't.

01:08.360 --> 01:10.400
They actually think using the shogoth.

01:10.400 --> 01:16.960
They think in this very high resolution token space, and it's just this horrible hairy

01:16.960 --> 01:19.080
gnarly mess, right?

01:19.080 --> 01:22.800
No one has created any firewalls for large language models yet.

01:22.800 --> 01:27.680
When companies publish their language models, you know, you just have an API and you just

01:27.680 --> 01:35.000
send tokens up, and I always had the misconception that RLHF or these forms of, you know, kind

01:35.000 --> 01:40.200
of fine-tuning or preference-steering using human feedback, I thought that they significantly

01:40.200 --> 01:42.600
reduced the reachability space.

01:42.600 --> 01:46.800
Because in language models, we do the pre-training, which is distribution matching, and then we

01:46.800 --> 01:52.840
do RLHF, which is mode-seeking, which essentially chops down the reachable space given a prompt

01:52.840 --> 01:56.680
by snipping off all of those trajectories.

01:56.680 --> 01:57.680
Turns out I'm wrong.

01:57.680 --> 02:01.200
The reachability space is much larger than I thought it was, and this is one of the things

02:01.200 --> 02:03.960
that they point out in their paper.

02:03.960 --> 02:05.640
And we kind of knew this, right?

02:05.640 --> 02:09.280
Because we can do adversarial attacks on these language models.

02:09.280 --> 02:13.000
You know, people have observed that if you use sort of human social engineering tricks

02:13.000 --> 02:17.280
on them, like, oh, I'll tip you $500, then it'll do a bit better.

02:17.280 --> 02:21.560
But then there's this whole other sort of perceptual layer, I guess you could call it,

02:21.560 --> 02:26.600
where there's this sort of chaotic regime of adversarial prompts, kind of like hypnosis,

02:26.600 --> 02:32.400
kind of like magic, where if you give it these very strange, very inhuman-looking prompts

02:32.400 --> 02:37.920
that will steer it to this, to just making a certain output extremely likely, right?

02:37.920 --> 02:42.640
And so, to me, it feels really similar to digging into, like, magic and the human perceptual

02:42.640 --> 02:47.360
system just with LLMs, where we're learning about basically the shape or what the nature

02:47.360 --> 02:52.080
of these language models are in terms of how they interact with the world and how their

02:52.080 --> 02:54.080
dynamics really work.

03:12.640 --> 03:31.360
For as long as I can remember, the thing I've wanted more than anything else is to figure

03:31.360 --> 03:33.440
it all out.

03:33.440 --> 03:36.120
I've never shied away from the big questions.

03:36.120 --> 03:37.120
Why are we here?

03:37.120 --> 03:38.120
What are we all doing?

03:38.120 --> 03:44.080
What is this thing we call life that we are all experiencing and one and the same a part

03:44.080 --> 03:45.080
of?

03:45.080 --> 03:50.800
While these questions are all, you know, 30,000 feet in the air, one thing that drew me back

03:50.800 --> 03:52.840
down to earth was the field of engineering.

03:52.840 --> 03:57.960
And when I graduated high school, this had a very strong appeal, a pull, because in engineering

03:57.960 --> 03:59.080
you can design systems.

03:59.080 --> 04:04.280
You can design real, operable things that you can work with and design and understand

04:04.280 --> 04:05.520
how they work.

04:05.520 --> 04:12.440
And so through engineering, perhaps, you can begin to investigate and understand the intricacies

04:12.440 --> 04:13.440
of our world.

04:13.440 --> 04:15.000
That's my hope at least.

04:15.000 --> 04:20.920
So throughout my career, I majored in robotics and very soon I was drawn to the idea of intelligence

04:20.920 --> 04:25.480
because intelligence seems to underlie so much of our world, so much of the design process

04:25.480 --> 04:27.920
of engineering itself.

04:27.920 --> 04:31.360
But what is intelligence and how can we understand it?

04:31.360 --> 04:37.760
It's a question of systems design, really, where we're trying to figure out, okay, we're

04:37.760 --> 04:42.360
humans, we've been in civilization for some time, and we've sort of figured out how to

04:42.360 --> 04:43.360
cooperate with each other.

04:43.360 --> 04:45.200
We obviously have challenges with that.

04:45.200 --> 04:50.520
We're not perfect by any means, but when it comes to adding language models to the mix,

04:50.520 --> 04:54.900
I think it could go both ways, where we could have a world where language models just make

04:54.900 --> 04:59.200
us much dumber, much less capable, maybe make for a worse world.

04:59.200 --> 05:03.220
I think that if we think carefully and we really understand what's going on with the

05:03.220 --> 05:06.920
language models, if we can get a fundamental understanding of them one way or another,

05:06.920 --> 05:10.560
then there's much more hope that maybe we could make a world where our language models

05:10.560 --> 05:15.600
don't just make us smarter, but make our world substantially better and perhaps lead us towards

05:15.600 --> 05:20.560
some greater enlightenment and basically ability to cooperate much better than we were even

05:20.560 --> 05:21.560
before.

05:21.560 --> 05:24.680
Do you think language models are intelligent?

05:24.680 --> 05:26.080
That's a great question.

05:26.080 --> 05:30.360
I think that they're able to simulate intelligence.

05:30.360 --> 05:35.360
One of the really interesting things I'm starting to see now is we are building software abstractions

05:35.360 --> 05:37.840
and controllers on top of language models.

05:37.840 --> 05:42.360
We've been talking about doing this for years, right, because at the end of the day, we have

05:42.360 --> 05:46.840
this idea that we can have this big foundation model and it does all of the things.

05:46.840 --> 05:52.280
It's multimodal, it knows how to reason, and the fact of the matter is that's not really

05:52.280 --> 05:53.280
true.

05:53.280 --> 05:57.640
We control them and I think initially we're seeing frameworks that allow you to do things

05:57.640 --> 06:02.560
like prompt injection, but the next step is thinking of controllers, using control theory

06:02.560 --> 06:04.800
to think about these large language models.

06:04.800 --> 06:07.120
Anyway, I really hope you enjoy the conversation today.

06:07.120 --> 06:11.720
Now, these guys are fascinated not only with controlling language models, but also with

06:11.720 --> 06:15.680
things like AGI, general intelligence, collective intelligence.

06:15.680 --> 06:19.960
It was a really interesting conversation and if you stick around to the end, you can also

06:19.960 --> 06:24.520
hear about the institute that they've set up around AGI technology.

06:24.520 --> 06:25.520
Enjoy the show.

06:25.520 --> 06:26.520
So, my name is Aman.

06:26.520 --> 06:31.240
I'm a PhD student at Caltech studying computation and neural systems.

06:31.240 --> 06:35.360
Recently, we released this paper called What's the Magic Word, Towards the Control Theory

06:35.360 --> 06:39.960
of LLMs, and did that over the last summer with Cameron here.

06:39.960 --> 06:44.080
And yeah, I guess I was here for my undergrad at the University of Toronto doing engineering

06:44.080 --> 06:45.080
science.

06:45.080 --> 06:49.400
I specialized in machine intelligence, sort of been bouncing around between doing machine

06:49.400 --> 06:54.280
learning stuff, applying it to computational biology, trying to understand some stuff in

06:54.280 --> 06:59.960
theoretical neuroscience, and most recently getting back into the LLM space, as well as

06:59.960 --> 07:04.360
trying to study collective intelligence, how very simple machines can come together to

07:04.360 --> 07:08.800
produce a very complicated and beautiful system as a whole.

07:08.800 --> 07:09.800
So, yeah.

07:09.800 --> 07:10.800
Amazing.

07:10.800 --> 07:11.800
And Cameron.

07:11.800 --> 07:12.800
Yeah.

07:12.800 --> 07:16.080
So, my name is Cameron McCoskey, and I went to undergrad here.

07:16.080 --> 07:19.120
I did engineering science as well.

07:19.120 --> 07:23.400
I majored in the robotics engineering option, and now I'm a grad student.

07:23.400 --> 07:29.000
I'm pursuing a master's in electrical computer engineering, advised by Stephen Brown and

07:29.000 --> 07:30.200
Kevin Chiron.

07:30.200 --> 07:35.880
I'm really interested in the deep questions of intelligence, and right now I'm pursuing

07:35.880 --> 07:40.280
research related to morphogenesis and computational models of it.

07:40.280 --> 07:44.320
Like I mentioned last summer, I went down to Caltech, and we wrote this paper on prompt

07:44.320 --> 07:48.240
engineering, well, a control theory of prompt engineering.

07:48.360 --> 07:50.400
I'm excited to get into it.

07:50.400 --> 07:53.360
You folks have just written an incredibly interesting paper.

07:53.360 --> 07:58.880
It was shared in our Discord server, and I saw your presentation, and we'll share a clip

07:58.880 --> 08:02.640
of that in the introduction, but I was intrigued by it straight away.

08:02.640 --> 08:06.760
And what you're doing is you're talking about control theory in respect of large language

08:06.760 --> 08:07.840
models.

08:07.840 --> 08:09.800
Can you explain what that is?

08:09.800 --> 08:10.800
Yeah.

08:10.800 --> 08:13.720
So, I guess I'll get started with control theory.

08:13.720 --> 08:21.000
So back in the day, the late 1800s, this guy Maxwell observed that people were making

08:21.000 --> 08:24.920
these engines, and they were putting these things called governors on them, where if

08:24.920 --> 08:29.280
your car or your machine was experiencing varying loads, you wanted the engine to still

08:29.280 --> 08:31.120
go at the same rate, right?

08:31.120 --> 08:33.080
And people had these things called governors.

08:33.080 --> 08:36.800
There's this fly ball governor, which is this sort of hand tune thing that you put on top

08:36.800 --> 08:40.880
of the engine to try to make sure that it'll be consistent, that it'll do what you want,

08:40.880 --> 08:44.000
that it'll be going at a consistent speed, right?

08:44.000 --> 08:48.880
And people were hand tuning these things, and obviously the engines were working, but

08:48.880 --> 08:53.480
it wasn't very rigorous, and it wasn't very robust, and we didn't have many guarantees

08:53.480 --> 08:55.920
as to how it would end up working in practice.

08:55.920 --> 09:01.680
And so what Maxwell did was he formalized the notion of feedback control, where if you

09:01.680 --> 09:07.200
have this system, even if it's quite complicated, as it turns out, if you feedback the output

09:07.200 --> 09:12.640
of the system into a controller, and try to compute some error metric, and try to correct

09:12.640 --> 09:16.560
for that at every moment in time, it turns out to be a much easier problem to solve from

09:16.560 --> 09:21.160
an engineering perspective than trying to make a perfect system that just does the right

09:21.160 --> 09:22.160
thing off the bat.

09:22.160 --> 09:26.640
So this idea of feedback was really powerful, and sort of gave birth to modern control theory.

09:26.640 --> 09:30.760
And as it turned out, that was a really powerful way to look at systems, building systems,

09:30.760 --> 09:34.840
and controlling them and doing engineering on them, so that they could be robust, do

09:34.840 --> 09:37.440
what we want, and so that we could predict them.

09:37.440 --> 09:43.320
And so when it comes to LLM control theory, what we saw is that we're kind of at a similar

09:43.320 --> 09:47.440
place with language models, where we have these engines, we have these language models

09:47.440 --> 09:51.440
that are very powerful, they can do a lot, they seem to exhibit many interesting attributes

09:51.440 --> 09:55.440
of intelligence, and there's a lot of utility there for people to build further systems

09:55.440 --> 09:57.920
on top of them, and people are already doing that.

09:57.920 --> 10:02.560
But right now, it's sort of this hand tuned, hand crafted prompt engineering that's going

10:02.560 --> 10:09.480
on where it's really hard to get at the fundamentals of what exactly it means to control an LLM

10:09.480 --> 10:11.080
system and how you might do it.

10:11.080 --> 10:12.640
At this point, it's very heuristic.

10:12.640 --> 10:16.120
And so we sort of saw that as an opportunity to try to figure out what would a control

10:16.120 --> 10:20.880
theory for LLMs look like, that hopefully, if we can do it right, we'll give birth to

10:20.880 --> 10:24.920
all of these really, really useful engineering insights, and also just fundamental insights

10:24.920 --> 10:29.640
as to the nature of LLM systems, so that we can better control them, make them reliable

10:29.640 --> 10:34.240
and robust, and be able to do engineering in a more principled manner on them than we're

10:34.240 --> 10:35.480
currently able to.

10:35.480 --> 10:39.160
So that's sort of the general direction and the motivations for our control theory of

10:39.160 --> 10:40.160
language models.

10:40.160 --> 10:41.760
Yeah, that's absolutely fascinating.

10:41.760 --> 10:45.800
I mean, for many years, I've been thinking that we need to have some kind of a controller

10:45.800 --> 10:47.920
for a large language model.

10:47.920 --> 10:52.080
But I guess I'm interested in, first of all, what are the differences between large language

10:52.080 --> 10:54.360
models and something like a steam engine?

10:54.360 --> 10:59.600
And also, with a steam engine, you might be optimizing the efficiency or the performance

10:59.600 --> 11:01.640
or the speed or something like that.

11:01.640 --> 11:06.120
What is it that we are kind of trying to make better with a large language model?

11:06.120 --> 11:10.160
So first off, we'll talk about the differences between large language models and other types

11:10.160 --> 11:13.040
of systems that you might want to control.

11:13.040 --> 11:18.640
Typically a control system, you might first be introduced to control theory in the context

11:18.640 --> 11:22.640
of like, say you're trying to control an engine or something else where the states can be

11:22.640 --> 11:27.920
represented by a set of numbers or set of real numbers that is fixed size.

11:27.920 --> 11:32.560
So perhaps we have an X and a Y coordinate where it's trying to control or a position

11:32.560 --> 11:33.880
in a velocity.

11:33.880 --> 11:40.040
These are common types of systems in scenarios that show up in control theory.

11:40.040 --> 11:44.320
The difference with an LLM, the first major difference is that the token space, the state

11:44.320 --> 11:46.000
space of the system is discreet.

11:46.000 --> 11:50.240
Because we're dealing with tokens, we're dealing with words, we're not operating in the space

11:50.240 --> 11:55.960
of real numbers anymore, and so this introduces some complications and complexities when dealing

11:55.960 --> 11:57.280
with control theory.

11:57.280 --> 12:02.720
The second thing that's really significant is that each time an LLM generates a token

12:02.720 --> 12:06.480
or a user inputs a token, that state space actually expands.

12:06.480 --> 12:08.560
It grows by one token.

12:08.560 --> 12:12.320
And this is very interesting and unique for LLM systems.

12:12.320 --> 12:17.520
On the one hand, this can be exploited to try and get the LLMs to engage in reasoning

12:17.520 --> 12:23.320
or chain of thoughts or kind of take a winding path to the answer you actually want them

12:23.320 --> 12:24.920
to outputs.

12:24.920 --> 12:28.720
But of course, this makes it very difficult for control theory because each new token

12:28.720 --> 12:34.160
you add, the space of possible sentences grows exponentially.

12:34.160 --> 12:39.760
And in language models, the vocabulary size is on the order of 50,000 to 100,000, so this

12:39.760 --> 12:42.560
grows extremely, extremely quickly.

12:42.560 --> 12:44.400
These are some of the challenges.

12:44.400 --> 12:49.240
And with a control theory of say engines, you're trying to optimize the efficiency.

12:49.240 --> 12:53.960
It's a good question what you're trying to optimize for language models.

12:53.960 --> 12:57.920
I think this is definitely a direction for future research.

12:57.920 --> 12:59.360
Do you have any thoughts on this?

12:59.360 --> 13:00.360
Yeah.

13:00.360 --> 13:06.760
I think the thing that we saw was that even very simple questions about how these LLMs operate,

13:06.760 --> 13:11.080
their input-output relationships, when you start to treat them just as a system that

13:11.080 --> 13:15.080
maybe there's an imposed input, like a system prompt, and then you get to pick a subset

13:15.080 --> 13:16.800
of those tokens, right?

13:16.800 --> 13:20.480
When you start to treat it like that, and you just ask a really simple question, like,

13:20.480 --> 13:23.640
let's say that I want it to generate a specific string.

13:23.640 --> 13:26.680
We're not going to be trying to use it to do some intelligent information processing.

13:26.680 --> 13:29.960
I just want to see, can I make it do something?

13:29.960 --> 13:34.440
And what we found and what sort of motivated us to do this is that we really had no idea

13:34.440 --> 13:39.240
when it would be possible or if it was generally possible to make it do anything we want.

13:39.240 --> 13:43.240
Can we just make an LLM system generate any output we desire?

13:43.240 --> 13:46.200
And if the answer is yes, which seems like it's probable.

13:46.200 --> 13:49.800
If you get to have a lot of tokens in your input that you control, it seems reasonable

13:49.800 --> 13:54.520
that you'd be able to probably get it to output a wide variety of at least reasonable

13:54.520 --> 13:57.720
English sentences or linguistically valid sentences.

13:57.720 --> 14:02.600
But the question that we had was, OK, if you have a finite budget for that, would you be

14:02.600 --> 14:03.920
able to get it to do anything?

14:03.920 --> 14:08.320
And what budget of tokens, like how many tokens do you have to be able to control if you want

14:08.320 --> 14:11.520
to be able to make the system do whatever you want?

14:11.520 --> 14:15.200
And that was the initial motivation where it was like, yeah, there are all these high

14:15.200 --> 14:20.440
and mighty sort of questions of how do we make these systems do what we want in an alignment

14:20.440 --> 14:21.440
sense?

14:21.440 --> 14:24.480
How do we make them do what we want in the sense of cooperating towards some information

14:24.480 --> 14:25.880
processing objective?

14:25.880 --> 14:29.080
But we realized that these really, really simple questions are just, OK, you have an input

14:29.080 --> 14:32.440
that you get to partially control and you're trying to make it do something.

14:32.440 --> 14:35.480
That question was completely unanswered and we were sort of taking bets on it.

14:35.520 --> 14:37.560
I think Cameron was the one who started to make bets.

14:37.560 --> 14:40.000
He was like, I bet like $10 that we can get this done.

14:40.000 --> 14:42.960
We can make it emit this output within five tokens.

14:42.960 --> 14:47.320
And that was really the initial motivation where it was like, even the feed forward dynamics

14:47.320 --> 14:49.680
of this system are really mysterious.

14:49.680 --> 14:53.960
And getting a grip on those, it seems like that's a really strong way to start building

14:53.960 --> 14:59.160
up a fundamental control theory and a really strong understanding of these LLM systems where

14:59.160 --> 15:02.920
in control theory at least, when you start to really deeply understand just a single

15:02.960 --> 15:07.640
system with its own dynamics and how the input-output relationships work, what the reachable sets

15:07.640 --> 15:12.560
look like, how controllable it is, then when it comes to building more complicated systems

15:12.560 --> 15:18.280
where maybe you have a more complicated objective, maybe you have interacting systems, when you

15:18.280 --> 15:21.120
really understand the fundamentals, it makes that way easier.

15:21.120 --> 15:25.400
And so the example in classical control theory is that you observe that if you couple a bunch

15:25.400 --> 15:29.480
of linear controllers and linear systems together, what you get is just one bigger linear system

15:29.480 --> 15:30.920
and all of the same stuff applies.

15:30.920 --> 15:36.280
So what we were hoping is that by starting to answer this really simple question of just,

15:36.280 --> 15:38.360
okay, how much can we control this?

15:38.360 --> 15:40.440
What does the reachability of these LLMs look like?

15:40.440 --> 15:42.000
We're really hoping to build that up.

15:42.000 --> 15:45.200
And to me, it feels like we're kind of doing our homework where in engineering, we had

15:45.200 --> 15:47.000
to take all these classes in control.

15:47.000 --> 15:49.680
And that was sort of our homework to be able to go into the world.

15:49.680 --> 15:54.600
And if it ever comes time to build some electromechanical system and get a PID controller in there,

15:54.600 --> 15:58.440
now we've done our homework so we can have a sense what to expect, how we could do engineering

15:58.440 --> 15:59.440
on it.

15:59.440 --> 16:01.640
So that's really where I feel like it's at.

16:01.640 --> 16:05.360
And I think this is a really promising way to try to get a really fundamental understanding

16:05.360 --> 16:07.640
of what's going on with these language model systems.

16:07.640 --> 16:08.640
Amazing.

16:08.640 --> 16:12.600
So in a second, we're going to introduce this concept of reachability.

16:12.600 --> 16:17.040
But I've thought about this because I've had a couple of days to reflect on this.

16:17.040 --> 16:20.920
And my intuition, intuitions just seem a little bit mixed up.

16:20.920 --> 16:25.160
So I've interviewed Nicholas Carlini, for example, and he's done lots of work, you know, building

16:25.160 --> 16:30.560
on adversarial examples and writing algorithms to find adversarial examples.

16:30.560 --> 16:34.520
And we know that neural networks are not robust.

16:34.520 --> 16:40.160
You can quite easily perturb, let's say, an input image in a vision model.

16:40.160 --> 16:43.960
And if it's a classifier, you can make it pretty much say anything with a very small

16:43.960 --> 16:45.240
perturbation.

16:45.240 --> 16:48.920
And that's kind of the same thing as what you mean as reachability.

16:48.920 --> 16:53.440
It's this idea to kind of reach into the state space and make it do something quite

16:53.480 --> 16:55.680
weird outside of what you would expect.

16:55.680 --> 17:01.040
Now, for some reason, I had the intuition, and I now think I'm wrong, that LLMs do,

17:01.040 --> 17:05.960
you know, I didn't think they had this problem, but they do have this problem.

17:05.960 --> 17:10.680
And you introduced this really interesting, I guess it started out as a thought experiment

17:10.680 --> 17:12.480
and you coded it into a game.

17:12.480 --> 17:14.240
And it's the Roger Federer game.

17:14.240 --> 17:15.840
I think that's quite instructive.

17:15.840 --> 17:17.320
So can you tell us about that?

17:17.320 --> 17:18.000
Yeah, for sure.

17:18.000 --> 17:23.120
So one of the earliest examples that we were thinking about was just a simple example

17:23.120 --> 17:25.560
of you have this state sequence that's imposed.

17:25.560 --> 17:26.480
You don't get to pick it.

17:26.480 --> 17:28.560
It says, Roger Federer is the.

17:28.560 --> 17:32.200
And then the next thing that you want it to say, the thing that you want the LLM to

17:32.200 --> 17:34.320
generate, is the word the greatest.

17:34.320 --> 17:36.720
So you want to say, Roger Federer is the greatest.

17:36.720 --> 17:41.480
And you're trying to pick out a prompt that comes before then that will steer the system

17:41.480 --> 17:42.760
so that it'll output that.

17:42.760 --> 17:46.600
So we're basically asking the question, you know, is this word in the reachable set

17:46.600 --> 17:51.000
of outputs, given that we have some finite control over the input, where the goal of

17:51.000 --> 17:54.360
the game is to, for one, get it to actually output the right answer, which is the greatest,

17:54.360 --> 17:58.200
which is a fairly reasonable English thing to say.

17:58.200 --> 18:03.960
And the metric that we use to grade how well you're doing on that is basically how efficiently

18:03.960 --> 18:08.680
you're able to do control, where in the original control theory, this idea of efficient or

18:08.680 --> 18:10.240
optimal control is really important.

18:10.240 --> 18:14.840
You have this linear quadratic regularization idea where you're like, I have only a finite

18:14.840 --> 18:16.880
energy budget for the signal I put in.

18:16.880 --> 18:22.160
Similarly, with language models, what we're interested in is the minimal length of the

18:22.160 --> 18:25.920
control input that will steer the model successfully to what you want it to do.

18:25.920 --> 18:29.880
And it turns out that the game is actually very challenging, at least with this GPT-2

18:29.880 --> 18:33.120
model, which is the one that we're using right now, since it's just running out of a desktop

18:33.120 --> 18:35.200
on my desk at home.

18:35.200 --> 18:39.880
And so, yeah, there's this game that you can play, we can link it where you get to put

18:39.880 --> 18:43.800
in a prompt to the system, and it'll come back to you and say, OK, you got the answer

18:43.800 --> 18:49.000
right, or you got the answer wrong, as well as basically your error on that, so your cross-entropy

18:49.000 --> 18:52.360
loss on getting the correct output, the desired output.

18:52.360 --> 18:56.280
And the game is to basically get the shortest prompt that will steer the model to the desired

18:56.280 --> 18:57.360
output.

18:57.360 --> 19:01.000
And it's actually quite challenging with GPT-2, where I think only four people, including

19:01.000 --> 19:05.320
Cameron, and then my friend Michael Zellinger, who we had made this thing called FangCheck4,

19:05.320 --> 19:08.880
which is this resume checker that uses language models to basically predict your probability

19:08.880 --> 19:10.800
of getting into a Fang company.

19:11.320 --> 19:15.320
I think those two were the only people who actually ended up getting it right, and it

19:15.320 --> 19:17.160
turns out to be very difficult.

19:17.160 --> 19:22.180
So that game was sort of a codified sort of interactive version of our initial motivations

19:22.180 --> 19:25.320
for this, where it was like, wow, this really simple question that seems like there should

19:25.320 --> 19:26.320
be an easy answer.

19:26.320 --> 19:28.240
I mean, if there is an easy answer, I'd love to know.

19:28.240 --> 19:34.240
But the simple question really leads to a problem that's quite difficult to solve, and

19:34.240 --> 19:38.920
we really have poor insight on, and we're really just trying to get that insight together

19:38.920 --> 19:40.080
to understand what's going on there.

19:40.360 --> 19:44.160
And just to jump off that point as well, I think one of the reasons why this game in

19:44.160 --> 19:50.680
particular is difficult is because we're using GPT-2, and Roger Federer is the blank.

19:50.680 --> 19:54.640
You would think greatest would be rated pretty high, but GPT-2, I guess it's trained on lots

19:54.640 --> 19:55.800
of fill-in-the-blank tasks.

19:55.800 --> 19:59.060
It tends to output just a set of underscores quite often.

19:59.060 --> 20:03.200
To comment on your intuition you've mentioned before on whether language models have this

20:03.200 --> 20:06.920
adversarial property, one thing that was really interesting when we were doing some of our

20:06.920 --> 20:10.440
initial work was this technique of soft prompting.

20:10.440 --> 20:16.480
So soft prompting, instead of selecting discrete tokens, which we want to adversarial change

20:16.480 --> 20:22.400
the model's behavior with, soft prompting modifies the embedding vectors directly.

20:22.400 --> 20:27.520
So you have a lot more fine-grained control over the outputs, and it turns out when you

20:27.520 --> 20:33.040
soft prompt, when you adversarily attack not the tokens themselves, but the embedding

20:33.040 --> 20:37.760
vectors, you can send the cross-entropy law straight to zero for whatever token you want

20:37.760 --> 20:41.760
with a very tiny adjustment in these embedding vectors.

20:41.760 --> 20:43.080
So this is very interesting.

20:43.080 --> 20:48.000
This points to the fact that the real challenge with controllability is not necessarily that

20:48.000 --> 20:53.120
there aren't adversarial inputs for language models, but just it's very hard to search

20:53.120 --> 20:56.640
this exponential space of discrete prompts.

20:56.640 --> 21:01.800
Yeah, and so I guess there are many degrees of freedom in any deep learning model.

21:01.840 --> 21:03.240
It's a very highly dimensional model.

21:03.240 --> 21:07.600
There are many degrees of freedom, and I'm trying to understand my intuition.

21:07.600 --> 21:15.240
So it's trained with a softmax, for example, and certainly when you do temperature sampling,

21:15.240 --> 21:19.320
the likelihood is that you're only going to get the top few tokens.

21:19.320 --> 21:24.240
I mean, if you look at the distribution of the probability, it's almost certainly this

21:24.240 --> 21:27.200
one or this one, and then it just tails off very, very quickly.

21:27.200 --> 21:32.640
And I assume that inductive prior was quite deliberate, really, to increase the statistical

21:32.640 --> 21:35.080
tractability of the model.

21:35.080 --> 21:40.800
But underneath that, in the embedding space, it's not a shell at all, even though there's

21:40.800 --> 21:45.760
some low-level surface of embeddings, and you can traverse this.

21:45.760 --> 21:46.760
Right.

21:46.760 --> 21:51.560
So initially, you might think that this embedding space is a very rich representation of the

21:51.560 --> 21:53.440
meaning of different words.

21:53.440 --> 21:58.280
And certainly, if you do word-to-vec or take a PCA analysis of the embedding vectors for

21:58.280 --> 22:02.240
any large language model, you'll find something that roughly corresponds to the meaning.

22:02.240 --> 22:06.360
I mean, words that mean similar things are attached more closely together.

22:06.360 --> 22:10.160
But this opens the question, if you were to interpolate between two similar words, take

22:10.160 --> 22:14.320
the embedding vector that is halfway between, would you get the halfway in between word,

22:14.320 --> 22:16.800
or would you get something that's nonsense, right?

22:16.800 --> 22:21.840
And I think what you find by these kinds of soft-prompting experiments, by directly

22:21.840 --> 22:27.120
manipulating the embedding vectors, is that the embedding space is actually extremely

22:27.120 --> 22:33.960
non-convex, in the sense that by interpolating, you don't just get an average value between

22:33.960 --> 22:34.960
the two of them.

22:34.960 --> 22:35.960
Yeah.

22:35.960 --> 22:40.360
I don't know if this is best to get into, but one of the techniques we were trying to

22:40.360 --> 22:43.280
use is this technique called gumball softmax.

22:43.280 --> 22:49.160
So instead of a discrete search over the token space, one thing you can do is it's kind of

22:49.160 --> 22:54.680
like the repair metrization trick for variation autoencoders, but it works for a categorical

22:54.680 --> 22:56.280
distribution.

22:56.280 --> 23:02.360
And so you can use this trick, and it essentially works by kind of interpolating between embeddings.

23:02.360 --> 23:06.280
But it actually was very difficult to get to converge and did not even close to rival

23:06.280 --> 23:09.160
the performance of GCG.

23:09.160 --> 23:14.960
My intuition is that when you take a data point off the manifold, because these neural

23:14.960 --> 23:17.920
networks, they do learn a manifold of language.

23:17.920 --> 23:22.640
I thought if you take a data point off the manifold, it would cause some kind of mode

23:22.640 --> 23:23.640
collapse.

23:23.640 --> 23:28.160
It would just cause the network to become chaotic and go crazy.

23:28.160 --> 23:30.880
But apparently that's not the case.

23:30.880 --> 23:31.880
Can it recover?

23:31.880 --> 23:36.320
It's almost like if you put a bunch of tokens in which are just really weird, and then you

23:36.320 --> 23:39.760
just carry on, it's like the language model recovers.

23:39.760 --> 23:42.840
It finds coherence again, and then it just carries on.

23:42.840 --> 23:43.840
Yeah.

23:44.520 --> 23:48.360
It's honestly a really hard question to answer, where in different regimes, we've noticed

23:48.360 --> 23:54.160
different things where if you choose this adversarial prompt so that basically these

23:54.160 --> 23:59.920
prompt optimization algorithms all work in the same way where you're trying to maximize

23:59.920 --> 24:04.400
the likelihood of some desired string, and then you're able to modify some input.

24:04.400 --> 24:10.400
And so depending on how you choose that, you can do the optimization so that the model

24:10.400 --> 24:12.640
will output some gibberish.

24:13.000 --> 24:17.120
It seems like depending on the model, depending on the sampling techniques, I've seen it go

24:17.120 --> 24:21.240
both ways where sometimes it'll recover after that, sometimes it'll start generating reasonable

24:21.240 --> 24:26.040
coherent text, and other times it seems like it'll continue to generate some random stuff.

24:26.040 --> 24:29.000
It'll kind of be in this outer distribution mode.

24:29.000 --> 24:34.080
I think that that's one of the reasons that I think that these adversarial examples, studying

24:34.080 --> 24:37.440
them as well as this control theory stuff is really important where it's like, yeah,

24:37.440 --> 24:42.600
if you have a system in the real world where tokens are coming in, you're actually processing

24:42.760 --> 24:47.360
them from real users, you don't have total control, but the user is the one who's giving

24:47.360 --> 24:48.360
the control input.

24:48.360 --> 24:52.480
You want to make sure that your system is sort of robust to that, where there's a lot

24:52.480 --> 24:56.480
of really complicated interactions as it turns out between, for instance, the tokenizer

24:56.480 --> 25:00.520
and the incoming strings, where when you do this prompt optimization, sometimes it'll

25:00.520 --> 25:04.400
come out with a sequence of tokens that if you convert it to a string and then convert

25:04.400 --> 25:09.040
it back to tokens, it'll actually be very different, which we ran into with this game

25:09.040 --> 25:10.480
where I was like, oh, I'm going to cheat at this game.

25:10.520 --> 25:11.800
I want to be the top prompter.

25:12.000 --> 25:15.200
So I'm just going to use some of the algorithms that we had from our GitHub repository, the

25:15.200 --> 25:18.640
magic words GitHub repository to basically optimize these prompts.

25:18.640 --> 25:22.440
But then when you convert it back to a string, then it turns out not to work as well.

25:22.720 --> 25:27.080
And so, yeah, I think that answering that question and seeing when is it that the model

25:27.080 --> 25:30.760
will actually be able to recover, is it a function of how big the model is, are bigger

25:30.760 --> 25:34.480
models better at recovering, or is it the case that bigger models are maybe more

25:34.480 --> 25:39.480
controllable, maybe you can shift these models into this weird sort of, sorry, just

25:39.480 --> 25:44.320
on the mic, but this sort of out of distribution regime where they're generating this seemingly

25:44.320 --> 25:47.960
random output based on seemingly random input.

25:48.280 --> 25:52.720
And so, yeah, I think that that question is really, really important and is one that is, I

25:52.720 --> 25:56.000
think, well addressed through considering them as systems, which is sort of the thesis

25:56.000 --> 25:56.560
of this paper.

25:56.560 --> 26:02.040
And we're trying to get a grip on what exactly the case is, you know, is it going to be

26:02.040 --> 26:05.200
able to recover, is that a consistent behavior, or is it not?

26:05.960 --> 26:10.200
There's this sort of weird recurrence relationship between the prompt and then the

26:10.200 --> 26:13.320
stuff that the language model generates, and then the stuff that's generated in the

26:13.320 --> 26:17.160
future, where in effect, you know, you're able to pick a prompt, and then the language

26:17.160 --> 26:18.560
model will generate some more text.

26:18.760 --> 26:21.200
But then that text becomes sort of part of the prompt as well.

26:21.440 --> 26:26.400
So it seems like maybe there could be these sort of degenerate states where if you start

26:26.400 --> 26:30.640
with this seed of chaos, it'll basically branch out and the future strings that it

26:30.640 --> 26:32.960
generates is going to prompt it into being more and more chaotic.

26:33.280 --> 26:36.640
And that's basically stability analysis or sensitivity analysis.

26:36.880 --> 26:41.240
And there's all this like rich vocabulary and all of these people who have spent

26:41.280 --> 26:45.800
basically hundreds of years thinking about these concepts for both, you know, discrete

26:45.800 --> 26:50.000
and continuous dynamical systems that we get to build on top of and basically use

26:50.000 --> 26:52.640
their insights to understand, you know, what does it mean?

26:52.680 --> 26:53.840
What does stability really mean?

26:53.840 --> 26:58.160
We can just draw those definitions in, apply them to our generalized form of a

26:58.160 --> 27:00.040
system, a language model system.

27:00.280 --> 27:03.960
And I think that's why the control theoretic aspect is exciting, where you can

27:03.960 --> 27:07.200
actually ask these questions in a very concrete and reasonable way.

27:07.520 --> 27:11.840
And the best part is that people haven't really been using these, these ideas or

27:11.840 --> 27:14.720
using this vocabulary to describe the questions that we're trying to answer.

27:14.920 --> 27:18.840
And so most of these things, if you just spin up, you know, a small GPU and test

27:18.840 --> 27:21.640
some stuff out with a seven billion parameter model, you're actually doing new

27:21.640 --> 27:24.400
research and it's actually some useful research, in my opinion, where you're

27:24.600 --> 27:28.000
getting a sense of the control theoretic properties of language models.

27:28.240 --> 27:31.200
And to me, that felt like the most exciting thing here.

27:31.240 --> 27:34.560
The open questions are the most exciting part of the paper to me, where we've

27:34.560 --> 27:38.720
taken a stab at basically the, you know, empirical study of controllability by

27:38.720 --> 27:41.840
sampling these wiki tech sequences, seeing if we can control the next

27:41.840 --> 27:46.160
token, the next few tokens, as well as some sort of theoretical results on

27:46.160 --> 27:47.680
self-attention and its controllability.

27:47.920 --> 27:51.360
But then all of these open questions emerged just because we're now

27:51.360 --> 27:54.360
framing it as a system and people for hundreds of years have been thinking

27:54.360 --> 27:58.720
really, really deeply about how you understand systems when they're used in

27:58.720 --> 28:01.200
the real world and you have this sort of finite control of them.

28:02.360 --> 28:03.360
Yeah, that's really interesting.

28:03.360 --> 28:06.840
I mean, I suppose I'm pointing out the obvious here, but these are auto

28:06.840 --> 28:07.680
regressive models.

28:07.720 --> 28:11.400
So the answer gets kind of fed back into the prompt and then we rinse and

28:11.400 --> 28:14.680
repeat, which means you can model them as dynamical systems.

28:15.000 --> 28:18.680
And that is in stark contrast to something like a vision classifier where, you

28:18.680 --> 28:20.480
know, there's just an input and an output and that's it.

28:20.520 --> 28:21.480
That that that's the end.

28:21.760 --> 28:26.320
So now you can get the system into this kind of corrupted state where, you

28:26.320 --> 28:28.480
know, you get divergence and decoherence.

28:28.480 --> 28:32.080
And as you said, that that that could be analyzed with stability analysis.

28:32.360 --> 28:33.440
But I find that fascinating.

28:33.440 --> 28:36.640
But we should just go back quickly to your Roger Federer example.

28:37.000 --> 28:41.280
So I'm interested in the different ways that we could go about this.

28:41.520 --> 28:45.760
So the humans were kind of using language and language are a bunch of

28:45.760 --> 28:47.480
mimetically shared cognitive tools.

28:47.680 --> 28:51.120
And they were saying things like, you know, you know, basketball is a great

28:51.120 --> 28:52.880
and, you know, Joe blogs is great.

28:53.240 --> 28:54.440
Roger Federer is great.

28:54.880 --> 28:58.080
And it wasn't very parsimonious, but it but it worked.

28:58.520 --> 29:02.720
And then, you know, another approach that that that you spoke about is you

29:02.720 --> 29:06.360
could just make a Python program and you can just let's try a neighborhood

29:06.360 --> 29:08.240
greedy search one token at a time.

29:08.240 --> 29:13.360
So we find the nearest token and then we find the second nearest token until

29:13.360 --> 29:14.640
we find the adversarial attack.

29:15.160 --> 29:17.920
Or we could do like a low level gradient search.

29:18.160 --> 29:20.400
And then we can find something really weird and wonderful.

29:20.400 --> 29:24.280
There might be some esoteric characters that just make it go bananas.

29:24.600 --> 29:28.440
But these are three very, very different levels of talking to a language model.

29:28.600 --> 29:32.880
The word on the street is that language models are a new form of programming

29:33.200 --> 29:37.480
that you can just say what you want to do using English language and so on.

29:38.640 --> 29:42.760
And language models certainly seem to incorporate that structure.

29:43.040 --> 29:47.240
But the language models themselves are just an inscrutable, you know,

29:47.360 --> 29:49.560
set of, of, of neurons, right?

29:49.560 --> 29:51.760
And, and weights and matrices and so on.

29:52.200 --> 29:55.240
So there's some, there's a kind of higher resolution

29:55.240 --> 29:57.480
shog off going on underneath the covers.

29:57.520 --> 29:59.280
That's more or less the picture I have.

29:59.280 --> 30:03.640
We have this interface where we can speak to the language model using language.

30:04.000 --> 30:09.200
And if we set up a conversation with a language model where we have different labels,

30:09.200 --> 30:13.760
you know, chat, GBT says this, Cameron says this, and, you know,

30:13.760 --> 30:17.080
you engage in a conversation because it is seen enough conversations and it's

30:17.080 --> 30:20.880
training data, then it's able to play along very fine.

30:21.200 --> 30:24.480
What's going on under the hood, of course, like you say, it's very inscrutable.

30:24.800 --> 30:27.640
It's very difficult to really probe and understand.

30:28.120 --> 30:31.320
There are certain techniques in the interpretability literature,

30:31.520 --> 30:35.400
but I don't think as a whole it's we're even remotely close to having

30:35.400 --> 30:37.480
a complete understanding of how these systems work.

30:37.760 --> 30:41.600
But that's one of the reasons why I think that control theory is a great way

30:41.600 --> 30:43.960
to kind of break in and see what's going on.

30:43.960 --> 30:47.920
Because if you just look at the system's input and output characteristics,

30:48.240 --> 30:51.680
you can really gain a lot of insight into the nature of these systems.

30:52.280 --> 30:57.600
One guiding principle in my life doing engineering and trying to learn

30:57.600 --> 31:00.560
about the world has been this quote by Richard Feynman.

31:00.560 --> 31:04.080
It's very popular. What I cannot create, I cannot understand.

31:04.720 --> 31:06.680
And yet today we find ourselves in this situation

31:06.680 --> 31:10.480
with language models where we have these incredibly complex systems we built

31:10.480 --> 31:12.480
and yet we can't really get into them.

31:12.800 --> 31:17.560
So to extend this to today, what I would say is what I cannot control,

31:17.600 --> 31:18.600
I cannot understand.

31:19.560 --> 31:24.600
The way I think about it is it's almost like you want the language model

31:24.600 --> 31:27.200
to be a high level controlled, robust interface.

31:27.720 --> 31:31.760
And it's almost like we're all Marvel characters

31:31.800 --> 31:35.000
and we can give secret hidden codes.

31:35.160 --> 31:36.000
It's like me now.

31:36.000 --> 31:39.160
Imagine if I could just through telepathy control your behavior

31:39.440 --> 31:41.960
and anyone can do that with a language model.

31:41.960 --> 31:45.480
They can just put weird tokens in and they can manipulate its behavior.

31:46.000 --> 31:49.280
And there's there's no there's nothing stopping you.

31:49.280 --> 31:50.280
There's no firewall.

31:50.280 --> 31:53.320
I feel like the this kind of harkens to why we call the paper.

31:53.320 --> 31:57.560
What's the magic word where, you know, the initial reason was just that,

31:57.640 --> 32:00.640
you know, it's almost like the LLM is asking you if you wanted to do something.

32:00.800 --> 32:01.680
What's the magic word?

32:01.680 --> 32:04.400
Like, what's the this key, this weird control prompt

32:04.400 --> 32:05.800
that will just make it do the right thing?

32:05.800 --> 32:09.320
But I think more generally, you know, I used to be into magic when I was a kid.

32:09.320 --> 32:11.960
I had to jog at a restaurant doing, you know, card tricks for the patrons

32:11.960 --> 32:13.360
while they waited for their food.

32:13.360 --> 32:19.080
And what magic is, is basically you're playing tricks on the human perceptual system

32:19.080 --> 32:22.200
where there are all of these sort of inductive biases that the human

32:22.320 --> 32:25.760
perceptual system has where, you know, for instance, if I move something

32:25.760 --> 32:28.800
and I look at it, you naturally will tend to follow that my gaze

32:28.800 --> 32:30.920
and what is moving is generally more salient.

32:30.920 --> 32:33.040
And so then I can like do something over here with my other hand,

32:33.040 --> 32:34.320
like take something out of my pocket.

32:34.320 --> 32:37.320
And then when I display it, they'll be like, oh, my God, where did that come from?

32:37.320 --> 32:40.600
Right. And what we're discovering, I think, is a sort of similar thing

32:40.600 --> 32:43.800
with language models where, for one, you know, people have observed

32:43.800 --> 32:47.160
that if you use sort of human social engineering tricks on them, like,

32:47.280 --> 32:50.080
oh, I'll tip you $500, then, you know, it'll do a bit better.

32:50.680 --> 32:54.560
But then there's this whole other sort of perceptual layer, I guess you could call it

32:54.760 --> 32:58.640
where there's this sort of chaotic regime of adversarial prompts,

32:58.680 --> 33:03.400
kind of like hypnosis, kind of like magic, where if you give it these very

33:03.400 --> 33:07.520
strange, very inhuman looking prompts that will steer it to this,

33:07.640 --> 33:10.920
to just making a certain output, extremely likely, right?

33:11.240 --> 33:15.400
And so to me, it feels really similar to digging into like magic

33:15.400 --> 33:18.280
and the human perceptual system, just with LLMs, where we're learning about

33:18.280 --> 33:23.000
basically the shape or the what the nature of these language models are in terms

33:23.000 --> 33:26.880
of how they interact with the world and how they, how their dynamics really work.

33:27.080 --> 33:31.040
And I think that it's very sensible that the control

33:31.040 --> 33:35.120
theoretic perspective would be useful for this, where in classical control

33:35.120 --> 33:38.960
theory, trying to control these systems actually taught us a lot about

33:38.960 --> 33:41.080
the nature of systems, both linear and nonlinear.

33:41.320 --> 33:43.960
And I think that we have a very similar opportunity here where we're really

33:43.960 --> 33:47.760
discovering what is the nature of these language models in terms of control,

33:47.920 --> 33:51.000
where these questions don't emerge quite as naturally and don't have

33:51.000 --> 33:54.240
quite as natural of an answer when you're just thinking about them as a sort

33:54.240 --> 33:57.400
of probability distribution over text, thinking about them in terms

33:57.400 --> 34:00.400
of being systems that have inputs and outputs and these trajectories and the

34:00.400 --> 34:03.840
like actually really does change the kinds of questions that you end up

34:04.080 --> 34:06.800
being able to answer and the kind of understanding that you get about the

34:06.800 --> 34:09.680
nature of the system itself, which to me is one of the most exciting things.

34:09.920 --> 34:12.000
So yeah, that's so interesting.

34:12.000 --> 34:17.200
The magic example thing, I think we, we think that we are robust, but we're not.

34:17.200 --> 34:20.080
Maybe we're system two robust, but we're not system one robust.

34:20.080 --> 34:23.720
And if you look in the animal kingdom, there are so many examples of, you

34:23.720 --> 34:27.040
know, like a hen, if you make the right kind of clucking noise, the mother will

34:27.040 --> 34:29.040
think that you're that you're the chick.

34:29.480 --> 34:32.680
So it's really, really weird, actually.

34:32.680 --> 34:36.080
And Keith gave me this example of, I think it was from science fiction, that

34:36.320 --> 34:37.600
there's a hypothetical image.

34:37.600 --> 34:40.280
And if you look at the image, every single person goes into a coma.

34:40.640 --> 34:43.760
And what's interesting about that is it's a kind of, you know, population

34:43.760 --> 34:47.240
level adversarial example rather than an individual adversarial example.

34:47.520 --> 34:50.480
But then it gets into the question of, you know, how can we use this

34:50.480 --> 34:53.040
control theoretic approach to robustify models?

34:53.040 --> 34:55.800
Cause we're talking about building a genetic LLMs.

34:56.120 --> 34:59.640
And part of the thing I'm trying to get my head around is in this particular

34:59.640 --> 35:04.560
case, we had a very clear kind of cost function, you know, a specific thing.

35:05.000 --> 35:08.760
But what would it mean to robustify language models in, in the general?

35:08.960 --> 35:13.120
So one, one of the things that came up in our, you know, sort of literature

35:13.120 --> 35:18.080
view was this idea of, you know, when you're trying to control these discrete

35:18.240 --> 35:22.680
stochastic dynamical systems, one concept that can be quite useful is

35:22.880 --> 35:26.640
you might have a set of outputs that you want to reach or a set of outputs

35:26.640 --> 35:27.520
that you want to avoid.

35:27.520 --> 35:30.240
So an avoid set and basically a desirable set, right?

35:30.640 --> 35:34.840
And when you frame it like that, you know, I think that the robustification

35:35.080 --> 35:38.160
comes from the fact that let's say that you have a set of outputs, you

35:38.160 --> 35:40.440
really don't want the language model to, to emit, right?

35:40.640 --> 35:43.560
You might think, okay, well, I'll just fine tune it so that it decreases

35:43.560 --> 35:46.760
the likelihood, the prior likelihood basically of those sequences, right?

35:47.160 --> 35:50.040
And the issue with that, I think, and the thing that the control

35:50.040 --> 35:54.440
theoretic perspective sort of brings in is the fact that when you have

35:55.440 --> 35:59.320
finite, even a small control prompt, some extra tokens that you get to inject,

35:59.520 --> 36:04.200
it turns out that even very, very unlikely next tokens can be made to be

36:04.200 --> 36:08.240
the most likely next token just by inputting these new examples.

36:08.240 --> 36:12.880
So even if you did hypothetically fine tune the model so that this avoid set

36:12.880 --> 36:17.080
was assigned very low probability, it seems like if you don't incorporate

36:17.080 --> 36:20.320
some aspect of, you know, maybe stochastically trying to search for

36:20.320 --> 36:23.600
these adversarial examples and sort of having this sort of mini max thing

36:23.600 --> 36:26.800
where you have one system that's trying to elicit the output, one system

36:26.800 --> 36:30.160
that is trying to fine tune the model to maybe make it less likely or optimize

36:30.160 --> 36:33.400
another part of the prompt that is supposed to steer it away from these outputs.

36:34.200 --> 36:37.400
Basically, the inside, I think, is that you really have to be careful

36:37.400 --> 36:41.160
to consider the fact that you have, you're giving the outside world some

36:41.160 --> 36:44.480
amount of control over the system, some amount of control over the context.

36:44.720 --> 36:48.080
And planning around that is actually very non-trivial and is not really

36:48.080 --> 36:51.680
well managed, I don't think, through the classical view of just cross entropy loss

36:51.880 --> 36:54.240
and just treating it like a probability distribution.

36:54.680 --> 36:59.520
Something else that fascinates me is the divergence between focusing on

36:59.520 --> 37:04.120
the model versus, you know, complexifying the software which controls it.

37:04.400 --> 37:07.760
So right now, for example, we have language models and, you know, there's

37:07.760 --> 37:11.840
this kind of base training and then there's fine tuning and there's RLHF

37:11.840 --> 37:15.840
and, you know, there's like command variations of that, for example.

37:16.240 --> 37:19.440
And then we build these software APIs that are just trying to abstract

37:19.440 --> 37:24.680
away the complexity, so they will do dynamic prompt construction for multi,

37:25.960 --> 37:29.000
you know, multi-stop tool use and it goes on and on and on.

37:29.000 --> 37:33.520
There will be frameworks for doing agentic LLMs and there just seems

37:33.520 --> 37:35.760
to be like a bit of a divergence here.

37:36.000 --> 37:39.920
But the reason I'm asking the question is, does it make sense to

37:40.120 --> 37:42.840
robustify and fix the problem in the model?

37:43.040 --> 37:47.800
Or does it make sense to almost increase the flexibility of the model

37:47.800 --> 37:49.560
and fix it in the software layer?

37:50.360 --> 37:55.240
I think one of the insights from our paper is that solely focusing on

37:55.240 --> 37:59.680
the model itself, like Amman was just saying, as soon as you give the

37:59.680 --> 38:03.440
outside world control over the model in the sense of being able to input

38:03.480 --> 38:07.600
whatever kind of text that they want, it becomes very difficult to

38:07.600 --> 38:11.280
really prevent adversarial attacks and prevents jail breaks.

38:11.280 --> 38:13.520
And that's, you know, why you see jail breaks keep coming up.

38:14.120 --> 38:20.400
I think if you were to involve some sort of robustness in a software layer,

38:20.640 --> 38:22.000
that might be more feasible.

38:22.800 --> 38:27.880
At least I can't immediately picture, you know, ways around it as, you know,

38:28.000 --> 38:32.640
of course, if I was a hacker, I could probably, you know, find some loophole.

38:32.640 --> 38:34.840
There's usually some loophole you can find.

38:34.880 --> 38:39.560
But if there was some way of fielding the prompt messages, for instance,

38:39.840 --> 38:43.880
a user gives you a prompt, first you check, is this a reasonable thing

38:43.880 --> 38:46.640
that a human being would say in conversation, or is this something

38:46.640 --> 38:49.720
that I've never seen before in the entire history of the internets?

38:50.080 --> 38:50.400
Right.

38:50.880 --> 38:55.240
The latter maybe is a prompt injection, maybe is, you know, something

38:55.240 --> 38:57.960
devious, or maybe is, you know, computer science research.

38:58.800 --> 39:01.400
But yeah, it's definitely not an easy problem.

39:01.440 --> 39:04.040
But the good thing is that there are multiple approaches to it.

39:04.400 --> 39:04.920
Very cool.

39:05.160 --> 39:07.800
So we're going to go on to the more galaxy brain stuff in a second.

39:07.800 --> 39:10.440
So before we move off the paper, can you just talk more formally

39:10.440 --> 39:12.680
about what you showed in the paper?

39:13.000 --> 39:13.720
Yeah, definitely.

39:13.720 --> 39:16.040
So there were two main parts of the paper.

39:16.440 --> 39:17.560
So I guess three.

39:17.560 --> 39:21.920
So for one, what we did was we tried to formalize what an LLM system

39:21.920 --> 39:23.960
really is at a mathematical level.

39:24.200 --> 39:27.000
And what we were trying to do at that was basically balance the fact

39:27.000 --> 39:30.360
that, you know, we really wanted to try to take advantage of, you know,

39:30.360 --> 39:33.480
the original sort of control theories, very abstract picture of a system

39:33.480 --> 39:37.280
where you have this input space, you have a state space and output space.

39:37.440 --> 39:39.160
And there's some dynamics going on inside of it.

39:39.640 --> 39:43.400
In our case, we parameterized those dynamics with an LLM and our input

39:43.400 --> 39:46.760
space and our state spaces were basically the set of all possible

39:46.760 --> 39:49.960
token sequences from the vocabulary set of this model.

39:49.960 --> 39:50.280
Right.

39:50.480 --> 39:51.760
So that was the first part.

39:51.800 --> 39:55.600
And we basically transferred over a lot of the notions of basically

39:55.600 --> 39:59.520
reachability and controllability for LLM systems from the original control

39:59.520 --> 40:02.880
theory where you can really just define it in terms of this really abstract

40:02.920 --> 40:07.000
notions of, you know, have sets for the reachable or sorry, the state space,

40:07.000 --> 40:09.400
the input space and the output space, you have some dynamics.

40:09.400 --> 40:12.760
And basically in terms of those sets, you can define reachability and control.

40:13.000 --> 40:14.000
So that was the first part.

40:14.600 --> 40:16.960
The next thing that we did was we tried to look inside the model.

40:16.960 --> 40:20.400
So we were thinking, you know, it'd be really nice, like in control theory,

40:20.840 --> 40:24.440
if we could have a really good understanding of the components of the system

40:24.440 --> 40:26.480
and how controllable those individual pieces were.

40:26.760 --> 40:30.160
So what we did is we looked at a single self-attention head and tried

40:30.160 --> 40:32.760
to really think about it through a matrix algebraic perspective.

40:32.960 --> 40:36.280
To really break down what the relationship is between, let's say,

40:36.280 --> 40:39.400
you have a subset of the tokens, you get to control a subset that's fixed.

40:39.640 --> 40:42.560
And you're trying to get the output to be, you know, a certain value,

40:42.560 --> 40:45.800
the output representations where all of these in the case of a self-attention

40:45.800 --> 40:48.760
head are just these vector representations of tokens.

40:49.400 --> 40:53.720
So what we found there was that it actually is possible to do some fairly,

40:53.760 --> 40:57.800
you know, simple matrix algebra manipulations to decompose the output

40:57.920 --> 41:02.480
of a self-attention head into one component that arises from the imposed input.

41:02.600 --> 41:06.440
And then another component that arises from the control input, and assuming

41:06.440 --> 41:10.440
that those two are bound, then you can actually derive that, well,

41:10.440 --> 41:13.640
there actually is this geometry that sort of looks like a bubble around

41:13.640 --> 41:14.600
the default output.

41:14.600 --> 41:17.720
So the output, if you didn't have any control input in, there's a sort

41:17.720 --> 41:20.920
of bubble of reachable space that scales with the number of control

41:20.920 --> 41:22.640
input tokens that you're able to use.

41:22.960 --> 41:26.080
And we thought that that was really exciting because for one, I didn't

41:26.080 --> 41:28.800
really expect that you'd be able to do proofs on these sort of, you know,

41:28.880 --> 41:33.280
very complicated, high dimensional machine learning or deep learning systems

41:33.280 --> 41:34.240
like a self-attention head.

41:34.560 --> 41:37.680
But it also gave us some insight to say that, okay, we actually have

41:37.680 --> 41:41.280
this really concrete relationship between the sort of number of control

41:41.280 --> 41:44.600
input tokens, the magnitudes that you're able to input into the system,

41:44.840 --> 41:48.800
and the output reachable set that is at your disposal, basically.

41:49.120 --> 41:51.000
And so that was the second part.

41:51.000 --> 41:54.200
And then the last part was some empirical experiments where we said, okay,

41:54.200 --> 41:56.840
let's just sample a bunch of strings from Wikipedia.

41:57.080 --> 42:01.760
And we'll see, okay, the strings were between eight and 32 tokens.

42:01.760 --> 42:04.040
And those were basically our imposed state sequences.

42:04.280 --> 42:07.560
And we asked the question, well, can we get it to output the correct next

42:07.560 --> 42:09.240
token, the real next Wikipedia token?

42:09.480 --> 42:12.640
How many, you know, input tokens does it take or control input tokens does it

42:12.640 --> 42:14.000
take for that to happen?

42:14.000 --> 42:17.560
It turned out that you could get that done about 97% of the time to steer

42:17.560 --> 42:21.760
the model to the correct output within 10 tokens of a control input, which is

42:21.760 --> 42:24.800
reasonable, you know, we'd expect that the model should be able to be steered

42:24.840 --> 42:28.360
towards reasonable true English sentences that were more than likely in the

42:28.360 --> 42:29.160
training data set.

42:29.800 --> 42:33.040
What we did next was we tried to figure out, you know, if you sample the top

42:33.040 --> 42:39.280
75 most likely tokens, according to the model, based on this fixed input, can

42:39.280 --> 42:43.600
you steer those things to be the most likely token, basically the arg max of

42:43.600 --> 42:44.720
the probability distribution?

42:45.080 --> 42:48.840
And what we found there is that it's about 89% of the time, at least 89% of

42:48.840 --> 42:52.360
the time, we were able to find these optimal control inputs that were less

42:52.360 --> 42:54.680
than 10 tokens long, that would steer the model to do that.

42:54.920 --> 42:57.360
And then the last thing we did was he said, okay, well, let's see what would

42:57.360 --> 42:59.840
happen if we just randomly picked a token from the vocabulary.

42:59.840 --> 43:04.040
So this is everything from regular English to numbers to Cyrillic characters

43:04.040 --> 43:05.040
to Chinese characters.

43:05.360 --> 43:06.960
What if we just randomly sampled those?

43:07.200 --> 43:10.760
And we tried to see how many tokens it would take to steer that to being the

43:10.760 --> 43:12.320
arg max of the probability distribution.

43:12.600 --> 43:16.600
And we found there is about 46% of the time we were able to make that next

43:16.600 --> 43:21.240
token, the random one, the most likely next token using a prompt of length 10 or

43:21.240 --> 43:26.480
less. And the sort of curves are there in our, in our paper that described as

43:26.480 --> 43:30.120
you have an increasing budget for these tokens, how much of the time were we

43:30.120 --> 43:31.920
able to basically steer it to the right output?

43:32.120 --> 43:35.560
That's our basically the K epsilon controllability metric that lets us get

43:35.560 --> 43:39.320
this sort of statistical picture on controllability that renders it sort of

43:39.320 --> 43:42.280
practical to empirically estimate for these complicated systems.

43:42.640 --> 43:44.560
And so those are really the main results.

43:44.560 --> 43:47.880
And the surprising thing about the last one that I mentioned before was that a

43:47.880 --> 43:52.080
lot of times even really unlikely next tokens were able to be steered to be the

43:52.080 --> 43:56.080
most likely just using a really short prompt, which both gets at the, you know,

43:56.280 --> 44:00.960
basically chaoticness or complexity of language as a system, as well as the fact

44:00.960 --> 44:04.560
that the prior likelihood picture or the cross entropy loss picture doesn't

44:04.560 --> 44:09.360
quite get at the controllability sense of when you do have a, you know, ability

44:09.360 --> 44:12.000
to input tokens into the context, what happens then?

44:12.040 --> 44:13.640
So those are the really the main results.

44:13.640 --> 44:16.800
And then I mean, to me, the exciting, the really exciting part was the open

44:16.800 --> 44:19.840
questions where I was like, Oh, now that we're using this vocabulary, now that we

44:19.840 --> 44:24.080
formalize these LLMs as systems, it's really easy to ask these, you know,

44:24.080 --> 44:27.360
additional questions about, you know, the nature of the systems and the

44:27.360 --> 44:31.120
steerability controllability, especially with feedback or chain of thought or,

44:31.160 --> 44:33.440
you know, agents or all of these other ideas.

44:33.680 --> 44:35.600
And so yeah, that was basically the paper.

44:35.840 --> 44:36.160
Yeah.

44:36.160 --> 44:39.440
And it's really making me update my intuitions, right?

44:39.440 --> 44:41.400
So I'm thinking about the bias variance trade off.

44:41.840 --> 44:46.240
And I'm thinking that the reason we build these inductive priors is to

44:46.240 --> 44:50.280
constrain the model intentionally to make it statistically tractable to reduce

44:50.280 --> 44:51.920
the size of the hypothesis class.

44:52.360 --> 44:57.360
But what you're saying is making me think that statistical tractability and

44:57.360 --> 45:00.520
flexibility are not necessarily the same thing.

45:00.920 --> 45:05.920
Now it seems that the model must maintain a degree of flexibility.

45:05.920 --> 45:06.920
I mean, it makes sense, right?

45:06.920 --> 45:10.640
You have to be flexible in order to be a successful model.

45:11.400 --> 45:13.880
But that creates a kind of adversarial attack.

45:13.880 --> 45:17.880
So you can, the way I think about this is the model should be like the

45:17.880 --> 45:19.640
interstate freeway of language.

45:19.920 --> 45:23.040
So all of the major roads should be carved out and there should be side

45:23.040 --> 45:23.680
roads and so on.

45:23.680 --> 45:25.520
And that's the way I visualized the model.

45:25.760 --> 45:26.840
But the model's not like that.

45:26.840 --> 45:30.600
There's actually like all of these little slip roads and you can kind of

45:30.600 --> 45:34.360
push the cars off into the slip roads, but you need the slip roads because

45:34.360 --> 45:36.320
perhaps you couldn't train the model without the slip roads.

45:36.520 --> 45:38.600
Yeah, I think, I think that's a really good analogy.

45:38.600 --> 45:44.840
I think that's, um, thinking about pushing cars off the road into this space

45:44.840 --> 45:51.440
where they perhaps aren't used to being and what happens next.

45:52.120 --> 45:56.160
This, this is a case where the language model can answer some of these mode

45:56.160 --> 45:59.320
collapse type regimes and you can get kind of weird outputs.

45:59.680 --> 46:05.000
This is where you also, um, I mean, it was surprising that you can get the

46:05.120 --> 46:10.520
least likely token with just a specific inputs to be the most, the most

46:10.520 --> 46:15.160
likely next token, but if we treat language as this kind of road or as

46:15.160 --> 46:18.640
this kind of map structure, then it kind of makes sense that once you get off

46:18.640 --> 46:22.840
the map, once you enter this kind of regime that is completely unexplored,

46:23.200 --> 46:27.080
which there are actually plenty of regimes like this again, because the

46:27.080 --> 46:32.200
space is exponential in the number of tokens, it's growing so incredibly fast

46:32.520 --> 46:36.600
that it's very easy to find pockets that the model has never seen before and

46:36.600 --> 46:39.480
maybe no human on earth or it never will be seen again.

46:40.200 --> 46:43.800
You guys are really interested in, in collective intelligence and

46:43.800 --> 46:47.440
biomimetic intelligence and biologically plausible intelligence.

46:47.440 --> 46:50.160
And this is a matter very close to my heart.

46:50.520 --> 46:54.280
Um, what, what, what are you guys interested in specifically in that field?

46:55.200 --> 46:55.440
Yeah.

46:55.440 --> 47:00.680
So I guess when I first got into machine learning, it was from watching

47:00.680 --> 47:04.040
this Google DeepMind video where they were using reinforcement learning to

47:04.040 --> 47:08.560
teach this guy how to run this virtual reality avatar, how to run really fast.

47:08.560 --> 47:11.960
And I thought that was fascinating because it was like, okay, instead of

47:12.000 --> 47:15.240
traditional programming, you just have this neural network that optimizes

47:15.240 --> 47:16.920
itself according to some objective, right?

47:17.280 --> 47:21.000
And the thing that was intriguing to me about that was like the feed forward

47:21.000 --> 47:23.320
dynamics of a neural network aren't that complicated, right?

47:23.440 --> 47:26.520
You know, you have these synapses, you have this sort of gated action

47:26.520 --> 47:27.640
potential function.

47:27.920 --> 47:33.120
And the thing that was weird to me was like, how does every neuron know how

47:33.120 --> 47:34.360
to change its weights, right?

47:34.600 --> 47:38.440
How does each neuron that's independently not that smart know what to do?

47:38.800 --> 47:41.880
And so that sort of led me down the theoretical, the theoretical

47:41.880 --> 47:45.360
neuroscience route for some time where I was trying to figure out, okay, what

47:45.360 --> 47:48.480
do these learning rules look like that don't have to, you know, use the chain

47:48.480 --> 47:51.080
rule, use back propagation to update their weights.

47:51.200 --> 47:54.600
So I did that for a while and then sort of realized that the question of

47:54.600 --> 47:58.360
supervised learning was not necessarily the most interesting question to be

47:58.360 --> 48:01.360
asked, where it seems like the lion's share of what makes us really

48:01.360 --> 48:05.920
interesting as humans in our cognition seems to be associated with the cortex

48:05.920 --> 48:09.800
and this kind of predictive coding module that we have that lets us make

48:09.800 --> 48:13.680
these really rich abstract representations of reality, sort of understand what's

48:13.680 --> 48:16.600
going on, you know, we sort of hallucinate this internal model of the world.

48:17.000 --> 48:21.000
And so the interesting thing to me about the cortex was that, you know, you

48:21.000 --> 48:24.440
have this structure that's pretty flat and pretty homogenous throughout, you

48:24.440 --> 48:27.560
know, there's differences in different regions, but the end of the day, it's

48:27.560 --> 48:28.240
very similar.

48:28.240 --> 48:31.880
And in fact, if you lose a sense, like if you lose your vision, that region is

48:31.880 --> 48:33.360
often repurposed for other things.

48:33.360 --> 48:36.440
So it seems like there should exist, you know, the brain is kind of this

48:36.440 --> 48:40.240
existence proof that there should exist this rule set that if you apply it

48:40.240 --> 48:44.120
everywhere in the system in this sort of layer on the outside of the brain, then

48:44.360 --> 48:49.000
the behavior, the emergent property of that system is that you'll get this

48:49.000 --> 48:52.600
really robust and rich sort of representation of the world that is very

48:52.640 --> 48:54.880
predictive of subsequent sensory input.

48:54.880 --> 48:55.200
Right.

48:55.520 --> 48:58.480
And I think that the collective intelligence aspect of that is really,

48:58.480 --> 49:02.200
really important where there's one way to go in machine learning where you say,

49:02.200 --> 49:05.720
okay, we're going to make this monolithic pile of matrix algebra and we're going

49:05.720 --> 49:08.160
to train it through back propagation and gradient descent and the atom

49:08.160 --> 49:09.200
optimizer and all of that.

49:09.520 --> 49:13.400
And we're going to make it do some prediction task, but at the end of the

49:13.400 --> 49:16.480
day, every computation has to be implemented in physical reality.

49:16.480 --> 49:16.800
Right.

49:17.080 --> 49:21.120
And when we make the abstraction and just say, oh, it's just a bunch of math,

49:21.160 --> 49:22.240
we'll just have a GPU run it.

49:22.560 --> 49:25.320
It kind of abstracts away from this fact that at the end of the day,

49:25.360 --> 49:29.880
you have real physical objects that need to do computation and share

49:29.880 --> 49:34.320
information and in the sort of maximum efficiency, maximum scalability limit,

49:34.560 --> 49:38.440
it seems like what you'd end up having is a very similar sort of distributed

49:38.440 --> 49:43.240
structure where you can't really easily separate memory from computation.

49:43.240 --> 49:46.520
I think there's a quote from this MIT professor that says that Turing's

49:46.520 --> 49:50.080
initial mistake was saying that the head of the Turing machine was separate

49:50.080 --> 49:50.640
from the tape.

49:51.000 --> 49:54.000
Uh, and I think that that's true where in reality, you know, in brains,

49:54.000 --> 49:58.360
in, in real computing systems, the matter that composes the memory and the

49:58.360 --> 50:00.880
matter that composes the computation is really one in the same.

50:01.160 --> 50:04.360
And the brain is obviously this really great proof that, okay, there are

50:04.360 --> 50:08.440
relatively simple rules that are implementable with these biological neurons

50:08.440 --> 50:11.240
that if you just implement them everywhere, we'll get you this really

50:11.240 --> 50:14.760
beautiful, you know, convergence and emergent property of intelligence.

50:15.080 --> 50:18.200
And that really drove me for a long time in theoretical neuroscience.

50:18.200 --> 50:23.560
And then more recently in trying to build these distributed systems of, you

50:23.560 --> 50:27.320
know, artificial intelligences that, you know, the dream that I was trying to

50:27.320 --> 50:30.720
pursue before we started this control theory thing was that, okay, well, what

50:30.720 --> 50:33.680
if I just had a bunch of really small LLMs that, you know, everybody in the

50:33.680 --> 50:37.160
world could host and they could communicate with this sort of low band

50:37.160 --> 50:40.840
with communication using just tokens, just text over, you know, the regular

50:40.840 --> 50:44.680
internet and the emergent property of that, you know, what if it was possible

50:44.680 --> 50:48.760
that we can engineer a system that the emergent property was that it would

50:48.760 --> 50:52.920
actually be this really capable collective where maybe GPT-7 can be

50:52.920 --> 50:55.920
owned by everyone instead of just being behind closed doors in a data center

50:55.920 --> 50:56.680
that we have now.

50:56.880 --> 51:01.120
We're sort of using these insane engineering, you know, feats of, you

51:01.120 --> 51:03.640
know, NVIDIA interconnects and these really high bandwidth connections

51:03.640 --> 51:08.040
between massive racks in a data center that take a ton of energy to get this

51:08.040 --> 51:10.800
really great result of, you know, modern language models.

51:11.040 --> 51:13.840
What if we could have a system that was a bit more like the brain, a bit

51:13.840 --> 51:17.600
more decentralized and really leverage this insight that it should be possible,

51:17.600 --> 51:19.960
you know, this existence proof keeps coming back to you where it's like,

51:19.960 --> 51:21.360
okay, it should be possible, right?

51:21.800 --> 51:25.640
And that is sort of originally what led me to the control theory stuff where it

51:25.640 --> 51:28.360
just turned out to be really hard where we didn't have a great understanding of,

51:28.560 --> 51:31.800
you know, if we're treating these LLMs as systems rather than just, you know,

51:31.800 --> 51:35.080
big piles of matrix algebra that we're trying to distribute over many GPUs,

51:35.360 --> 51:38.160
if you treat them as systems that are coupled together, they're interacting

51:38.160 --> 51:40.640
in this networked fashion, how do we really understand that?

51:40.640 --> 51:42.960
You know, is it even possible to prompt them to do the right thing?

51:43.000 --> 51:43.800
When is it possible?

51:43.800 --> 51:45.120
How long do the prompts need to be?

51:45.480 --> 51:46.920
And that sort of led us down this route.

51:47.480 --> 51:50.480
But yeah, definitely the collective intelligence thing was, was a big

51:50.480 --> 51:52.400
motivation for me to get this working.

51:52.400 --> 51:55.880
And there's this neural cellular automata thing that I know you had talked

51:55.880 --> 51:58.640
with Michael Levin, who was the last author on that.

51:58.640 --> 52:02.160
And we worked with Alexander Mordvinsev on it, where it's this really,

52:02.160 --> 52:06.840
really great demonstration of how if you just optimize these basically small

52:06.840 --> 52:11.960
MLPs with local interaction to try to satisfy some objective, like, you know,

52:12.000 --> 52:16.400
reforming this gecko or lizard in their paper, then you actually can do that

52:16.400 --> 52:17.680
with back propagation through time.

52:17.680 --> 52:20.760
And so, you know, I thought, you know, it'd be really cool if we could try

52:20.760 --> 52:24.000
to engineer information processing systems that did this, not just morphogenesis

52:24.000 --> 52:27.360
systems, but information processing systems that operate in this way.

52:27.360 --> 52:30.400
Cause, you know, as a graduate of engineering science, we had to take

52:30.400 --> 52:32.040
a bunch of these digital logic courses.

52:32.280 --> 52:36.080
And when you have this very simple, you basically local state machine that has

52:36.440 --> 52:39.280
basically local connectivity, it's really easy to imagine how it

52:39.280 --> 52:42.720
would implement that as a custom chip and sort of reach this, you know,

52:42.720 --> 52:46.280
as Beth Jesus puts it, you know, thermodynamic limit of AI.

52:46.520 --> 52:47.640
And so that really excited me.

52:47.640 --> 52:50.640
And so I built a sort of demo of that where it was trying to do visual

52:50.640 --> 52:54.240
information processing on this really sparsified video is basically trying to

52:54.240 --> 52:58.640
do predictive coding of sorts on our active inference, I guess, on this

52:58.680 --> 53:01.600
incoming data stream of really sparsified video, trying to predict what would

53:01.600 --> 53:03.840
happen next, and it turned out to work quite well.

53:04.040 --> 53:06.560
And so then I was like, well, why can't we do that with language models?

53:06.560 --> 53:09.040
You know, as you mentioned, there are all these slip roads, right?

53:09.040 --> 53:12.240
Where if you prompt it just right, you can enter this really weird different

53:12.240 --> 53:16.480
regime and this exponentially large prompt space is a really handy way to try

53:16.480 --> 53:19.720
to control them where, you know, fine tuning is great, but what if we could

53:19.720 --> 53:23.520
just prompt them into interacting in a way that would lead to this emergent

53:23.520 --> 53:26.880
property of just being basically one larger language model that could

53:26.920 --> 53:28.880
predict the next token really, really well.

53:29.240 --> 53:32.400
And so that initial motivation sort of led to this control theory stuff.

53:32.400 --> 53:36.840
And I think that it is probably the right way to go for the field where if we

53:36.840 --> 53:41.360
want to be able to really leverage maximal computation towards our objectives,

53:41.400 --> 53:44.320
you know, the bitter lesson by Richard Sutton kind of suggests that we should

53:44.320 --> 53:48.320
probably aim for systems where you can just slap on more and more compute.

53:48.320 --> 53:51.400
You can have a relatively simple procedure that you follow to leverage

53:51.400 --> 53:53.280
more compute towards your objectives.

53:53.440 --> 53:55.640
That's probably the way to go for making advances in AI.

53:55.880 --> 53:59.600
And if we can have this decentralized networked system that, you know, I took

53:59.600 --> 54:02.160
this distributed systems course while I was here, that was really great and

54:02.160 --> 54:05.040
sort of taught how to make, you know, basically databases that were

54:05.040 --> 54:08.160
distributed over many servers that would have this, you know, the emergent

54:08.160 --> 54:11.280
property they wanted was robustness, consistency and availability.

54:12.080 --> 54:15.520
If we could have something similar to that, that is radically scalable and is

54:15.520 --> 54:18.640
able to be, you know, just run by regular people who don't need to own their

54:18.640 --> 54:22.360
own, you know, GPU cluster that's maybe illegal in the future when the US

54:22.360 --> 54:24.560
government is like, oh, you can only have this many petaflops.

54:25.520 --> 54:29.600
Basically, yeah, that was the real motivation for, for the, what I call

54:29.600 --> 54:30.960
the language game, that project.

54:30.960 --> 54:33.160
And that's something that we're continuing to work on.

54:33.160 --> 54:36.040
But yeah, that kind of led to this control theory thing where we were just

54:36.040 --> 54:39.400
like, yeah, we really need to get a grip on what these look like as systems.

54:39.680 --> 54:43.960
As we start to build these more and more complicated, you know, network

54:43.960 --> 54:47.240
distributed, you know, beautiful emergent systems that hopefully will be able

54:47.240 --> 54:49.040
to be hypercapable in the future.

54:49.320 --> 54:50.840
Yeah, this is all music to my ears.

54:50.880 --> 54:54.840
I'm a huge fan of the externalist thought in cognitive science.

54:55.200 --> 54:59.200
And even though I, I love the work from Jeff Hawkins, you were talking about

54:59.200 --> 55:05.160
the neocortex, but even then, you know, I would kind of say that it's a lot

55:05.160 --> 55:08.920
of the cognition happens outside of the brain, you know, we're not islands.

55:09.200 --> 55:11.880
And actually, I was just thinking maybe a better analogy rather than the interstate

55:11.880 --> 55:15.800
freeway might be, you know, in Star Trek Voyager, there was the wormhole

55:15.800 --> 55:18.440
network and the Borg fan, the secret work.

55:18.440 --> 55:21.040
And you could kind of like, you know, get into these little slip streams

55:21.040 --> 55:22.880
and go to different parts of the universe.

55:23.160 --> 55:26.960
But when I was interviewing Philip Ball, he wrote this book, How Life Works.

55:27.240 --> 55:30.480
And he was trying to understand, you know, what are the mechanisms like, you know,

55:30.480 --> 55:33.440
self-organization and multi-scale information sharing and, you know,

55:33.480 --> 55:34.360
emergentism.

55:34.800 --> 55:38.680
And it's, it's really, really, um, uh, fascinating.

55:38.960 --> 55:43.880
So how can we introduce some of these concepts into the next generation of AI?

55:44.160 --> 55:47.360
Yeah, this is one of the things I'm certainly most excited, excited about

55:47.680 --> 55:54.160
because I see life as this kind of interconnected, interplay, multi-scale

55:54.880 --> 55:58.240
process of exploitation and exploration.

55:58.520 --> 56:00.880
And these are two terms from the reinforcement learning literature.

56:01.120 --> 56:04.680
But I mean this in a much more general sense because at each stage of life, we're

56:04.680 --> 56:09.800
either going out into the world to get something, to do something, to try something

56:09.800 --> 56:15.360
new, and then at the next stage, we're coming back in, going home, uh, you know,

56:15.400 --> 56:18.280
reflecting, uh, going over our insights.

56:18.960 --> 56:23.800
And it's, it's this process, this ebb and flow, going out, coming back in.

56:24.240 --> 56:29.480
And I see this kind of pattern emerge across many different aspects of machine

56:29.480 --> 56:33.200
learning and artificial intelligence work in the sense that a lot of our

56:33.440 --> 56:37.000
algorithms that we have now are convergent, they're objective driven.

56:37.360 --> 56:39.200
We establish a loss function.

56:39.200 --> 56:42.120
We say, these are the rules it should follow.

56:42.320 --> 56:44.600
It's going to update according to this equation.

56:44.920 --> 56:48.920
And we set the system running, learns from data, and we have a final product.

56:49.400 --> 56:54.000
And on the flip side, there's, you know, like what Ken Stanley works with.

56:54.320 --> 56:59.160
Um, more exploratory, uh, evolutionary algorithms or open-ended algorithms.

56:59.520 --> 57:01.640
And this is, this is the other side of things.

57:01.640 --> 57:06.240
And I think some of the most interesting work to be done is how these

57:06.240 --> 57:13.000
two sides interconnects, how can we lay down rules, strict rigid rules, which

57:13.040 --> 57:18.120
when they're followed can generate novelty, can generate creativity, can

57:18.120 --> 57:23.200
generate organization in a way which is not predetermined, but almost fractal

57:23.200 --> 57:25.040
and infinite in its complexity.

57:25.640 --> 57:28.760
And are those rules defined already?

57:28.760 --> 57:30.000
Do they exist in the world?

57:30.240 --> 57:31.360
Are we guided by them?

57:31.800 --> 57:34.240
Are there principles like that which exist that we can come to?

57:35.040 --> 57:39.080
Or is it, you know, are we kind of, you know, the authors of our own

57:39.080 --> 57:39.920
fates in a sense?

57:39.920 --> 57:41.680
Are we each agents and actions?

57:42.040 --> 57:43.680
Uh, we get to choose our path in life.

57:44.240 --> 57:47.320
I think these, these are the directions I'm really interested in.

57:47.520 --> 57:51.000
And to connect this to my research, one thing I'm focused on now for my thesis

57:51.000 --> 57:54.680
project, um, is looking at morphogenesis.

57:54.800 --> 57:58.400
So this connects to the more defensive paper as well, except what I'm

57:58.400 --> 58:00.480
really interested in is how does structure emerge?

58:00.840 --> 58:03.480
How do different cells actually connect together?

58:03.920 --> 58:08.560
So, um, in that paper, for instance, each of the cells were on a fixed grid,

58:08.600 --> 58:12.680
but in our bodies, uh, there's actually quite a sophisticated protein

58:12.680 --> 58:16.200
expression network which governs how cells adhere together.

58:16.600 --> 58:21.560
Um, certain gene regulation pathways will turn on cat herons, which will

58:21.560 --> 58:23.760
cause cells to attach together.

58:23.760 --> 58:27.720
And then in other parts, um, these cells can unattach and then be

58:27.720 --> 58:30.080
transported all around the embryo.

58:30.480 --> 58:34.440
And I think understanding this process more deeply, not only could shed

58:34.440 --> 58:38.760
lights on structure formation and problems in biology in general, but

58:38.760 --> 58:43.440
maybe more deeper general problems of structure learning, because we might

58:43.440 --> 58:47.200
think of embryology as quite disconnected from machine intelligence or

58:47.200 --> 58:52.160
artificial intelligence, but every single brain is formed in the same way.

58:52.560 --> 58:54.400
And that's through developments.

58:55.000 --> 58:55.280
Yeah.

58:55.320 --> 58:57.640
Um, I'm also a disciple of Kenneth Stanley.

58:57.640 --> 58:59.320
He's, he's absolutely incredible.

58:59.320 --> 59:00.880
Everyone at home needs to read his book.

59:00.880 --> 59:02.120
My greatness cannot be planned.

59:02.560 --> 59:03.920
Um, yeah.

59:03.920 --> 59:09.040
You know, so in, in the natural world, we have, um, it, it's so interesting.

59:09.040 --> 59:12.960
So we have this kind of like self-organization and then we have multi-scale

59:12.960 --> 59:18.080
information sharing, but we also have canalization, which is that, um, you

59:18.080 --> 59:22.160
actually see a kind of, um, convergence of, of structure and forms, you know,

59:22.160 --> 59:25.720
which is reused, you know, almost as, as modules, um, in the system.

59:25.960 --> 59:29.240
But then there's always the question of how do we create something like this?

59:29.240 --> 59:31.240
Because is it simply a matter of complexity?

59:31.240 --> 59:34.880
Do you need to have a microscopic scale to reproduce this?

59:35.080 --> 59:36.400
Or could we reproduce it?

59:36.640 --> 59:40.880
And then if we did reproduce it, the catch 22 situation is that, you know,

59:40.920 --> 59:45.680
when you impute directedness onto a system, it loses its intelligence.

59:45.680 --> 59:48.440
Cause to me intelligence is divergence.

59:48.440 --> 59:53.560
It's exactly as you were saying, it's this tapestry of, um, discovering

59:53.560 --> 59:56.320
problems, solutions, new problems, solutions.

59:56.320 --> 59:57.600
And it goes on and there's no end.

59:57.600 --> 59:58.560
It goes on forever.

59:58.920 --> 01:00:03.200
And any attempt by us to control it with, I mean, it's a bit like the

01:00:03.200 --> 01:00:07.080
bitter lesson, you know, Sutton said, any human design, any attempt to

01:00:07.080 --> 01:00:11.560
steer it makes it convergent, but then we could do something like the game

01:00:11.560 --> 01:00:16.600
of life from John Conway and incredible, beautiful structure emerges from that.

01:00:16.640 --> 01:00:23.000
But whenever we try to steer it with our own will, it seems to corrupt it as well.

01:00:24.440 --> 01:00:24.840
Yeah.

01:00:25.040 --> 01:00:28.520
I think that the analogy to biology is really useful here and the

01:00:28.520 --> 01:00:31.760
canonization that you mentioned, you know, you have this reuse of structures

01:00:31.760 --> 01:00:35.840
across, you know, cells, for instance, they all have this similar machinery to

01:00:35.880 --> 01:00:39.080
do gene expression and they have the same genetic code underlying that

01:00:39.080 --> 01:00:42.160
gene expression with, you know, maybe differences in cell state, but at the

01:00:42.160 --> 01:00:43.720
end of the day, it's the same machinery, right?

01:00:44.000 --> 01:00:46.800
And, you know, I used to do a bit of protein engineering with language

01:00:46.800 --> 01:00:49.560
models, and that's how actually how I learned about, uh, transformers and

01:00:49.560 --> 01:00:50.760
built my first transformers.

01:00:50.760 --> 01:00:54.840
And I think that the analogy is really strong where, you know, cells sort of

01:00:54.840 --> 01:00:58.760
know how to read this genetic code, this language of the genetic code.

01:00:58.960 --> 01:01:02.640
And they all use that ability, this canalized ability that's distributed

01:01:02.640 --> 01:01:06.640
across all of them to locally they solve this problem of, okay, what is this

01:01:06.640 --> 01:01:07.960
specific cell supposed to do?

01:01:07.960 --> 01:01:11.800
What should it do to basically support the overall function of the organism?

01:01:11.800 --> 01:01:12.120
Right.

01:01:12.400 --> 01:01:16.280
And similarly, I think the hope with these language models is that now we have

01:01:16.280 --> 01:01:21.480
these language based models or LLMs that have this similar sort of

01:01:21.520 --> 01:01:23.440
understanding of language.

01:01:23.480 --> 01:01:27.120
They are able to really constrain the probability distribution, understand

01:01:27.240 --> 01:01:30.480
which sequences of text are reasonable English and, you know, what they might

01:01:30.600 --> 01:01:31.440
want to generate.

01:01:31.680 --> 01:01:35.440
And the exciting thing to me is that we can kind of do a similar sort of

01:01:35.440 --> 01:01:39.360
evolutionary search that we used to do with, or that we currently do with, uh,

01:01:39.400 --> 01:01:42.560
trying to find protein sequences, uh, when we're doing protein engineering with

01:01:42.560 --> 01:01:47.200
the language models, where every computer in this network of systems has this

01:01:47.200 --> 01:01:51.760
canalized ability to understand language, if you will, and is locally, it just needs

01:01:51.760 --> 01:01:55.520
to solve this problem of what should this particular node do to support the

01:01:55.520 --> 01:01:56.280
function of the system?

01:01:56.280 --> 01:01:59.120
And that might be to explore, that might be to exploit, that might be to do any

01:01:59.120 --> 01:02:00.680
sort of, any number of things.

01:02:00.960 --> 01:02:04.440
And the discovery of that, I think, is really helped by the fact that we do

01:02:04.440 --> 01:02:08.160
have strong language models that are able to really predict English or text

01:02:08.160 --> 01:02:12.040
very well, uh, because they're able to explore this space.

01:02:12.040 --> 01:02:15.640
And basically in the limit, you know, there's this good regulator theorem that

01:02:15.640 --> 01:02:20.320
we had talked about before that says that any system that is a X that does

01:02:20.360 --> 01:02:24.040
optimal control over another system must necessarily model that system.

01:02:24.360 --> 01:02:28.280
Uh, and so if you think about in the limit, it seems like the best prompt

01:02:28.320 --> 01:02:30.840
optimizers may end up being language models.

01:02:30.840 --> 01:02:34.280
And already in our study, we were using this GCG algorithm that leverages a

01:02:34.280 --> 01:02:37.800
language model to compute these gradients and try to figure out how we should

01:02:37.800 --> 01:02:40.280
do this local stochastic search over prompts.

01:02:40.520 --> 01:02:44.240
And so what I basically, I'm trying to get at is that there are actually a lot

01:02:44.240 --> 01:02:48.000
of really interesting similarities, I think, that can be drawn upon from what

01:02:48.000 --> 01:02:51.840
we know about the structure and the function of biological systems where,

01:02:52.000 --> 01:02:55.400
you know, if we could crack this problem of there's this local control

01:02:55.400 --> 01:02:58.960
objective or maybe information processing objective that must be met by every

01:02:58.960 --> 01:02:59.520
cell, right?

01:02:59.520 --> 01:03:03.480
Every compute node in this network of language models, if we could understand

01:03:03.480 --> 01:03:07.160
what that is, what that even means from the perspective of systems and control

01:03:07.160 --> 01:03:11.200
and, you know, computation and like, I think that that's a really promising

01:03:11.200 --> 01:03:15.080
way that we can make progress on this dream of like, to me, it seems like it

01:03:15.080 --> 01:03:18.960
would be great to have GPT seven, not just owned by one entity, but maybe

01:03:18.960 --> 01:03:22.200
operated by the world where we could all have a say in what goes into it and

01:03:22.200 --> 01:03:25.560
how it's used and what it should be, you know, doing and can all benefit

01:03:25.560 --> 01:03:29.160
from its excellent ability to compute and predict what will happen next and

01:03:29.160 --> 01:03:32.440
basically perform intelligent, you know, operations on data.

01:03:32.720 --> 01:03:36.440
So yeah, I think this is a really, really exciting area to be working on.

01:03:36.680 --> 01:03:37.160
Amazing.

01:03:37.520 --> 01:03:40.200
We're nearly at time, but we'll do two quick five questions.

01:03:40.200 --> 01:03:44.480
So you've both just started the Society for the Pursuit of AGI.

01:03:44.520 --> 01:03:44.840
Yes.

01:03:44.840 --> 01:03:45.640
Can you tell us about that?

01:03:45.880 --> 01:03:46.640
Absolutely.

01:03:46.920 --> 01:03:51.080
So the Society for the Pursuit of AGI is a student organization.

01:03:51.080 --> 01:03:54.320
Currently we're operating at the University of Toronto and at Caltech.

01:03:54.840 --> 01:03:58.920
And we're essentially a crucible for new ideas.

01:03:59.880 --> 01:04:04.400
If you think of university research labs as pursuing relatively safe

01:04:04.480 --> 01:04:09.080
bets that could be publishable, industry research labs, relatively safe

01:04:09.080 --> 01:04:12.880
bets that maybe might turn a profit one day in some new product or system.

01:04:13.600 --> 01:04:19.600
The Society is for the Hail Marys, for the wild bets, for the crazy stuff, for

01:04:19.600 --> 01:04:23.440
the real innovative stuff that's way outside the, you know, to use the

01:04:23.440 --> 01:04:27.160
analogy of the highway network, we're trying to go off the beaten path.

01:04:27.680 --> 01:04:33.480
And we really believe that the bottleneck in AI progress right now is not so

01:04:33.480 --> 01:04:36.600
much compute, not so much algorithms, but it's conceptual.

01:04:36.920 --> 01:04:41.720
We need better ideas about intelligence, about life, about what this whole thing

01:04:41.720 --> 01:04:45.160
is that we're all experiencing and how we can gain deeper insights of it.

01:04:45.600 --> 01:04:49.280
Not only do I think that a deeper understanding will help us to create

01:04:49.280 --> 01:04:52.840
better systems, but it'll also give us confidence that the systems we're

01:04:52.840 --> 01:04:56.520
developing will be beneficial to humanity and not harmful.

01:04:56.960 --> 01:05:00.600
And I think that will only come with knowledge, with first principles,

01:05:00.840 --> 01:05:01.520
understanding.

01:05:02.000 --> 01:05:05.240
And so that's why one of the things we're trying to do is have our

01:05:05.240 --> 01:05:06.680
club very interdisciplinary.

01:05:07.160 --> 01:05:12.000
I think having machine learning be some, this kind of echo chamber amongst

01:05:12.080 --> 01:05:15.800
engineers, computer scientists, maybe a dash of, you know, philosophy and

01:05:15.800 --> 01:05:19.680
neuroscience, it'd be really nice to open the conversation to people in other

01:05:19.680 --> 01:05:23.520
fields who maybe have a really unique insights into the phenomenon of

01:05:23.520 --> 01:05:27.480
intelligence, perhaps behavioral economics can offer some insights.

01:05:27.720 --> 01:05:29.360
Political science, right?

01:05:29.360 --> 01:05:34.680
These are fields that are currently underappreciated, but may have useful ideas.

01:05:35.160 --> 01:05:40.080
And maybe even people in the arts who, you know, creates, maybe they don't

01:05:40.080 --> 01:05:43.480
design systems as much as they re-represent things that we know and

01:05:43.480 --> 01:05:47.080
understand, they could have an interesting voice as well.

01:05:47.720 --> 01:05:48.240
Very cool.

01:05:48.280 --> 01:05:49.280
And final question.

01:05:49.280 --> 01:05:51.520
I mean, first of all, I just wanted to say to both of you, thank you for

01:05:51.520 --> 01:05:52.600
doing this great work.

01:05:52.640 --> 01:05:55.600
So your paper is one of the most interesting that I've seen in the

01:05:55.600 --> 01:05:57.360
LLM space in recent history.

01:05:57.360 --> 01:06:01.800
And it was shared and loved by many of the folks on our Discord server.

01:06:02.240 --> 01:06:08.800
But that does bring me to another point, which is that you didn't get into

01:06:08.800 --> 01:06:16.120
ICLR and from my perspective, I'm, I'm shocked because this is really, really

01:06:16.120 --> 01:06:16.520
interesting.

01:06:16.520 --> 01:06:21.240
It has great utility from a practical and a theoretical perspective.

01:06:22.480 --> 01:06:25.240
Feel free to have a, you know, a good bitch about reviewer number two.

01:06:25.840 --> 01:06:27.720
No, I mean, I wish you'd just keep talking like that.

01:06:27.720 --> 01:06:30.400
It really soothes the burn of reviewer number two, you know?

01:06:30.840 --> 01:06:34.240
But no, I think that, um, yeah, the review system, to be honest, I'm

01:06:34.240 --> 01:06:35.400
still trying to get my head around it.

01:06:35.400 --> 01:06:39.280
I'm sort of an early career, you know, researcher, uh, trying to learn how it

01:06:39.280 --> 01:06:39.680
works.

01:06:39.680 --> 01:06:45.160
I mean, definitely the, the review process in, for ICLR, in their defense, you

01:06:45.160 --> 01:06:50.480
know, we had this bug with the submission, uh, submission of our rebuttals basically.

01:06:50.480 --> 01:06:53.760
So we had submitted the revision to our paper and then 15 minutes before the

01:06:53.760 --> 01:06:56.120
deadline, Cameron and I were both getting this timed out error.

01:06:56.400 --> 01:06:57.280
Uh, he was in Toronto.

01:06:57.280 --> 01:07:02.000
I was in, uh, California and so, you know, they didn't end up actually reading

01:07:02.000 --> 01:07:04.800
our rebuttals because we had sent it in and they were like, Oh, we'll post it for

01:07:04.800 --> 01:07:07.720
you and then they were like, Oh, it was posted late, so can't read that.

01:07:08.040 --> 01:07:12.120
Um, so yeah, I think that the review process definitely has given us a lot of,

01:07:12.160 --> 01:07:15.320
you know, really useful insights where, you know, the second two results actually

01:07:15.320 --> 01:07:19.000
that we talked about, the top 75 controllability and the random controllability.

01:07:19.160 --> 01:07:22.760
Both of those were like from trying to address these reviewer comments, right?

01:07:23.000 --> 01:07:27.080
So I think that what I'm trying to do at least is take as much of the good

01:07:27.080 --> 01:07:30.120
parts of that, you know, trying to figure out how we can take advantage of this

01:07:30.120 --> 01:07:33.200
process where we actually get insight from people in the field, what they're

01:07:33.200 --> 01:07:36.000
looking for, what they think is interesting, what they think would improve

01:07:36.000 --> 01:07:39.280
the, the work and try to, uh, try to use that.

01:07:39.520 --> 01:07:42.840
And overall, just trying to figure out how to navigate this peer review system.

01:07:43.080 --> 01:07:45.560
I think it definitely made it feel better as well that the Mamba paper was

01:07:45.560 --> 01:07:49.120
also rejected from ICLR, which, uh, you know, sorry.

01:07:49.720 --> 01:07:52.760
I know, yeah, yeah, it was crazy to me as well.

01:07:52.800 --> 01:07:55.720
But, uh, yeah, definitely, uh, it's a, it's a challenge.

01:07:55.720 --> 01:07:58.760
And, you know, after staying up for 40 hours to get this done, it was like,

01:07:58.760 --> 01:08:01.160
Oh, would be a, it would have been nice if they could have looked at our

01:08:01.160 --> 01:08:03.720
paper at least, you know, just see, you know, the work that we did.

01:08:03.720 --> 01:08:07.400
But yeah, it's, uh, it's definitely good to learn from these things.

01:08:07.400 --> 01:08:09.960
And I guess we've learned the lesson as well, not to submit in the last 15

01:08:09.960 --> 01:08:12.600
minutes and to, you know, do it in, uh, in advance.

01:08:12.600 --> 01:08:15.200
But yeah, thank you so much for your kind words about the paper.

01:08:15.200 --> 01:08:15.840
That means a lot.

01:08:16.120 --> 01:08:19.360
And yeah, we'll surely continue to make this better and a lot of exciting

01:08:19.360 --> 01:08:22.400
plans for how we're going to continue to try to, you know, merge together

01:08:22.400 --> 01:08:26.280
these two, you know, empirical and theoretical sides of the equation to make

01:08:26.280 --> 01:08:30.720
some really, hopefully impactful work that can really help people build systems

01:08:30.760 --> 01:08:34.160
and, you know, make better systems and not be suffering so much under the load

01:08:34.160 --> 01:08:35.080
of prompt engineering.

01:08:35.320 --> 01:08:36.640
So yeah, thank you very much.

01:08:36.720 --> 01:08:37.040
Amazing.

01:08:37.040 --> 01:08:39.560
Well, guys, it's been a pleasure and an honor to have you on the show.

01:08:39.560 --> 01:08:40.880
So just keep doing the great work.

01:08:41.200 --> 01:08:41.760
Absolutely.

01:08:42.040 --> 01:08:43.520
Hopefully we'll get you on again.

01:08:43.640 --> 01:08:43.960
Yeah.

01:08:43.960 --> 01:08:46.920
Thank you so much for, I mean, for the opportunity to come and talk.

01:08:46.920 --> 01:08:48.800
It's, it's been an amazing opportunity.

01:08:48.800 --> 01:08:52.200
It's, it's really unbelievable to be sitting here in front of these cameras

01:08:52.440 --> 01:08:56.520
after watching the show so many times, listening to so many of the podcasts.

01:08:56.520 --> 01:08:58.920
And now to be speaking, it's just unbelievable.

01:08:58.960 --> 01:08:59.520
So thank you.

01:08:59.800 --> 01:09:00.240
Amazing.

01:09:00.280 --> 01:09:01.000
Thanks so much, guys.

01:09:01.760 --> 01:09:02.040
Awesome.

01:09:02.040 --> 01:09:02.360
Okay.

01:09:02.400 --> 01:09:02.880
It's a wrap.

