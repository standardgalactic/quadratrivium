start	end	text
0	6000	I had the great pleasure of catching up with Professor Larissa Soldatova from Goldsmiths University.
6000	10000	She's really interested in automating the process of science.
10000	17000	And in order to do that, you need to know about logic, abduction, knowledge and semantics.
17000	24000	Biology is so complex, we still scratch the surface trying to understand.
24000	30000	And there are not enough scientists in the world to do it.
30000	36000	And if we can have some AI help, that would be probably a good thing.
36000	45000	So my presentation about reflecting on automation of science, history of it, where we are with it now,
45000	52000	how much we can automate now and where we're probably heading towards.
52000	56000	So I'm really interested in the philosophy of science, which is to say,
56000	60000	what is the meta process that scientists go through when they do science?
60000	69000	So science is about generating theories, explanations about how the world is this way and also how it's not that way.
69000	79000	But science is based on empiricism, which is basically following the data, doing experiments and trying to discount hypotheses.
79000	84000	But we still have this chicken and egg problem, which is where do the hypotheses come from?
84000	88000	And this is very related to the bias variance trade-off in machine learning.
88000	93000	Generally, we can't start from a blank slate. We can't start with nothing.
93000	98000	We need to have some world knowledge to structure our search process.
98000	105000	And then we run into trouble. How much should we seed the system with priors and world knowledge?
105000	110000	There are so many things in our physical world that it just feels like we should encode it into our systems.
110000	116000	It'd be a waste of time not to. But then there are some things that we feel that the system should learn.
116000	122000	But if we bias the system too much, then we might accrue a kind of approximation error
122000	127000	because there are things that our system just won't be able to search because we've biased it too much.
127000	134000	So there's always this trade-off between having the flexibility to find new things versus cutting down the search base.
134000	142000	The next big step will be when we will be able to consume more knowledge.
142000	150000	So not just data, but if we start using what we know.
150000	160000	Specific knowledge is a major source of machine and human intelligence and will suffice even with simple inference.
160000	166000	So if you take an AI system, it's mostly taking lots of data.
166000	177000	In the best case, it will take lots of different data, put it together, and then it outputs some predictive model or something else or piece of art.
177000	180000	But knowledge is still hugely underused.
180000	185000	No, one reason there is not much available in forms that computers can consume.
185000	194000	But just imagine if even these new models were immediately fed back and could be an input, so it's called closed loop systems,
194000	197000	then probably it would be more intelligent.
197000	198000	So this is quite interesting.
198000	205000	So going back to the 80s, the modus operandi in artificial intelligence was encoding knowledge into expert systems
205000	212000	because we knew that if you could encode knowledge, you can deduce new knowledge and new facts from existing knowledge.
212000	218000	So knowledge isn't just data, knowledge is one level of abstraction higher where you actually know the relations
218000	222000	and you can apply some kind of functions between data.
222000	224000	The problem is it's very brittle.
224000	229000	Famously, Doug Lenitz created this project called The Psych Project,
229000	233000	which had millions and millions of facts and relations and things in it.
233000	236000	And it was almost a complete waste of time.
236000	239000	It was too brittle, it wasn't particularly useful.
239000	242000	But empirically, though, we seem to be wasting our time with this blank slate approach
242000	246000	because machine learning moved away from really any explicit domain knowledge
246000	249000	and it just became these meta priors.
249000	257000	So things like symmetries and, I don't know, learning schedules and weight decay and stuff like that.
257000	263000	And the models only had very abstract priors in the sense of how to learn things statistically.
263000	265000	They didn't have any domain knowledge at all.
265000	269000	But there are so many things in our physical world that we just understand
269000	272000	and we should be able to explicitly code into our system.
272000	278000	But we have this paradox which is that every time we try and encode high level knowledge into systems,
278000	283000	it just introduces brittleness and this is what Rich Sutton warned against in his bitter lesson essay.
283000	287000	What's the difference for you between data and knowledge?
287000	295000	Yes, it's a good question and in different textbooks you can actually find different definitions.
295000	300000	But people who are working in knowledge representations, they have very clear ideas.
300000	302000	So data is facts.
302000	305000	Knowledge is when you go level up.
305000	311000	It's like rules, relations, ultimately better if you have models.
311000	314000	Better if you have executable models.
314000	319000	So you can put something in, get something out.
319000	321000	So this is knowledge.
321000	324000	A lot of knowledge you can discover directly from data.
324000	329000	If you have a lot of data, you can discover these rules, you can construct models.
329000	335000	But why don't you represent it immediately as knowledge if you already know it?
335000	338000	Why? It's very inefficient way.
338000	343000	It takes a lot of energy, these neural networks, they are destroying the planet.
343000	351000	You actually can encode a lot what we know in quite compact way and you can reuse it.
351000	354000	A lot what we know actually is not in the data.
354000	360000	Just an example, my collaborator gave this example.
360000	364000	They were modeling ecological systems and you can have a lot of data
364000	369000	and you can discover from data that bear has between two and five cups
369000	371000	and if it's more north it's less cup.
371000	377000	But why bother if we know you just encode it as a knowledge so you can extract it
377000	382000	but you also can just encode immediately and you can encode a lot additionally.
382000	384000	What is not in the data?
384000	387000	That is why it's more powerful and promising.
387000	395000	Larissa spoke about a project about 60 years ago to automate the scientific discovery of chemical compounds.
395000	400000	Not too dissimilar actually to Alpha Fold which came out recently from DeepMind
400000	405000	but anyway she's kind of saying that if you look at how these systems work today
405000	408000	it's not dramatically different than it was 60 years ago.
408000	410000	What chemists would do?
410000	415000	And they put in place a whole pipeline, knowledge acquisition
415000	424000	also giving output in forms that is good for users in this case chemists.
424000	429000	It really produced very interesting promising chemical structure.
429000	432000	Moreover this system actually was multi-agent.
432000	436000	It was heuristic dendral, metadendral and it was closed loop.
436000	443000	So whatever it managed to discover it immediately was going back and improved over time.
443000	450000	So it was 60 years ago and I would claim we haven't progressed much further.
450000	454000	Of course there were other fantastic systems
454000	461000	but the underlying architecture and principle how it operates is still the same.
462000	471000	So it's called the robot scientist and this was introduced again 15-20 years ago now.
471000	481000	So the idea is to develop a system, AI system that is capable of generating hypothesis, designing experiments
481000	489000	having real robotic labs to carry out these experiments, analyze results, do it in cycle and hopefully discover something new.
489000	495000	So it's a lot of data, external data, whatever data sets are relevant,
495000	500000	plus internally produced because it's a robotic lab, very powerful,
500000	506000	it can run in parallel, like 1000 biologists working in parallel in lab.
506000	510000	But it also has this knowledge, formalized knowledge.
510000	516000	It has domain knowledge, the first system worked with yeast biology
516000	524000	and it's also had this knowledge about what scientific discovery is, what key elements, how they're connected,
524000	529000	what you need to do, just like our experimental practices, how to design experiments.
529000	532000	You need to explain to machine at all.
532000	535000	So in science we need to do abduction.
535000	540000	This is absolutely delicious because you've heard us speak about abduction ad nauseam on MLST
540000	544000	and Larissa is out there talking about it, which is wonderful.
544000	550000	But yeah, you have to start with a hypothesis or a set of hypotheses in science before you do inference.
550000	555000	So inference is when you kind of learn mappings from hypotheses,
555000	560000	but abduction is how you first select the relevant hypotheses,
560000	566000	hypotheses that could generate powerful explanations for some phenomena.
566000	569000	And it's very creative, it's very mysterious.
569000	572000	So what is the logician 101 definition of abduction?
572000	576000	Well, it's reasoning to the best explanation.
576000	578000	Is that helpful?
578000	580000	Maybe, maybe.
580000	585000	I love the kind of cognitive science view on things, which is, you know, an explanation.
585000	590000	It's a causal model, which helps us understand the world because it gives explanatory power.
590000	593000	It carves the world up by the joints.
593000	594000	Why is the world this way?
594000	596000	Why is the world not that way?
596000	602000	And the raw building materials of these theories are cognitive priors,
602000	606000	and they're the same kind of priors that we imbue into machine learning models.
606000	610000	Noam Chomsky says that some of these priors are built into our brains.
610000	614000	I'm a big fan that many of the priors are socially embedded.
614000	617000	They're like software that float around mimetically.
617000	622000	But anyway, abduction is about you have a bunch of priors in the system,
622000	627000	and we as humans have this magical ability to grab the relevant priors,
627000	632000	stick them together into small models that give explanatory power,
632000	637000	and selecting a few of those models, and that's what we do when we do abduction.
637000	642000	So the key question is basically, can we automate that in a machine?
642000	647000	I will focus on hypothesis because in philosophy of science for a long time,
647000	653000	we considered that it's something that you cannot automate, that only humans can do it.
653000	658000	And to enable these hypotheses, this generation,
658000	662000	so you need to go out of the system, out of what you already know.
662000	666000	So you cannot use conventional logic like abduction.
666000	668000	You need to use something else.
668000	671000	And I'm used abduction, so there are other logic that you can use,
671000	676000	but you need to go outside of the system to have these guesses,
676000	681000	and then try to see if some of it makes sense or not.
681000	682000	It's not trivial.
682000	691000	Focusing on hypothesis, like you want to test hypothesis that this particular gene has that function,
691000	696000	but you actually cannot put gene into well, or you cannot measure function.
696000	699000	So you start doing some inferences.
699000	702000	What if we replace it to use some proxy?
702000	708000	If we use something with this gene knocked out, and if we put metabolites,
708000	711000	and so you go deeper, deeper, deeper.
711000	715000	So biologists do it just normal practices,
715000	720000	but when you try to explain it to computer how to do it, you really need to think.
720000	723000	So what steps we're taking,
723000	728000	we actually very rarely experiment with something that we make conclusions about.
728000	732000	So our conclusions are not always accurate, so then you have to go back.
732000	736000	So it was very interesting for us to try to automate it,
736000	741000	and then also reflect on how humans are doing science, especially experimental science.
741000	744000	There are many levels to this as well.
744000	746000	So if you want to go and get a job at Google now,
746000	752000	they already have a bit of a meritocracy which is based on kind of what we're talking about here.
752000	756000	To be very senior at Google, you need to find people,
756000	760000	and to be one step below that, you need to find the areas,
760000	762000	and below that you need to solve abstract problems,
762000	765000	and then below that you need to solve specific problems.
765000	771000	So there's a kind of hierarchy in the amount of creativity and ambiguity that you can tolerate,
771000	775000	and that's what scientists do, so they're always traversing these levels.
775000	778000	Abduction is about finding the areas,
778000	782000	and then once you perform inference on these hypotheses that you have abducted,
782000	785000	then that's solving a much more specific problem,
785000	787000	and it gets more and more specific.
787000	793000	The key point is that science is combinatorially, exponentially expensive,
793000	795000	just to run experiments.
795000	799000	And there isn't enough time in the world, there aren't enough scientists in the world.
799000	802000	Yes, having AI scientists can help us,
802000	807000	but we still need to always reflexively and abstractly go one level up.
807000	810000	We always need to be asking ourselves the question, can we do this better?
810000	814000	If we had a different perspective or a different view, could we make this more efficient?
814000	817000	And then could we make this more efficient?
817000	819000	And that's what we humans can do,
819000	822000	and that's what we need to imbue into AI robot scientists.
822000	828000	Well, you need to test like five, six drugs, work on these combinations.
828000	831000	So yes, it's combinatorical problem.
831000	838000	If you do it in a lab, we just don't have enough cells in our blood to run all these experiments.
838000	842000	So this is a justification when you need such system.
842000	846000	You first do reasoning based on all what we know,
846000	851000	with all these gaps, contradictions, but the best what you can do.
851000	856000	Output some plausible hypothesis is what should work in this situation.
856000	861000	Then you can do some simulation, again, the computational,
861000	866000	come up with some hundreds, several hundred hypothesis,
866000	873000	that then you can automatically test so that there's a cheaper way of doing it using robotics.
873000	877000	Yes, so I'm really interested in this idea of human cyborgs,
877000	880000	basically where humans and AIs work together.
880000	888000	The problem is it's not easy to say that combining humans and AIs together produces more productivity.
888000	891000	It probably does, but we don't know for sure.
891000	896000	There was a piece out by ThoughtWorks recently where they were talking about generative AI or co-pilot,
896000	899000	making developers two times more productive.
899000	901000	How do they measure that?
901000	903000	Software engineering is not a reducible activity.
903000	905000	It's a very complex phenomenon.
905000	909000	People have tried to measure the productivity of software engineers for many years.
909000	915000	How many lines of code do they write and the storyboard and issue tracking and stuff like that.
915000	920000	It's all rubbish, and the reason for that is it's a very complex, irreducible phenomenon.
920000	922000	Therein lies the problem.
922000	927000	There's a meme going around on LinkedIn at the moment talking about how with co-pilot,
927000	933000	you can generate the code ten times faster, but you then spend ten times more time debugging the code,
933000	938000	and that's because there's a technical debt, a new type of technical debt, understanding debt.
938000	944000	You generated all of this code and it works, and then in order to fix a problem, you need to actually understand it.
944000	947000	The mental model is not in the code.
947000	951000	Actually, in any software engineering project, the mental model is a memetic social thing.
951000	956000	It's in the brains of the developers, and it kind of floats around in the ether.
956000	958000	It's not in the code.
958000	960000	So, yeah, it's a similar thing here.
960000	965000	If we had an AI robotic scientist, how would we know that it's making us go faster?
965000	969000	I don't think such systems will replace humans.
969000	980000	As you see, at the best, they can automate some science, some experimental-driven science,
980000	988000	but it's an important part of science, and if it can be automated, it will be of great help.
988000	994000	Humans still have distinct advantages in many areas.
994000	996000	Hopefully, we will retain it.
996000	1004000	Ultimately, the best way is to work together, take advantage, and really, because it's him.
1004000	1009000	Then maybe we can make a faster progress.
1009000	1010000	Welcome to MLST.
1010000	1013000	We are here with Professor Larissa Soldatova.
1013000	1021000	Larissa joined Goldsmiths in November 2017 as director of the Online Masters in Data Science program.
1021000	1025000	She's an internationally recognized expert in artificial intelligence,
1025000	1032000	particularly in discovery science, reasoning, knowledge representation, and semantic technologies.
1032000	1036000	Larissa has also been working on the Robot Scientist project,
1036000	1040000	which investigates which processes of scientific discovery can be automated
1040000	1044000	and how robotic and human scientists can work together.
1044000	1051000	The Robot Scientist, Adam, was the first system which made an autonomous scientific discovery.
1051000	1056000	She's also involved in a number of international projects in the development of semantic standards,
1056000	1061000	for example, the machine learning schema, the robotics task ontology standard,
1061000	1065000	and also the laboratory protocols exact.
1065000	1068000	It's a pleasure to have you here, Larissa.
1068000	1072000	Maybe you could just start off by telling us about the events and your talk today.
1072000	1078000	My talk was about where we are with automating science.
1078000	1084000	Yes, it's very creative endeavor to do science,
1084000	1091000	but there are parts of it that scientists try to automate for a long time.
1091000	1099000	So the history of scientific discovery as a subject of computer science
1099000	1103000	is actually more than half a century old.
1103000	1111000	The first systems were developed in the 1960s in Stanford.
1111000	1118000	The first system was inspired by the need to go to other planets,
1118000	1123000	collect samples, like if you go to Mars.
1123000	1128000	You cannot send many samples back.
1128000	1133000	If you send data through what you managed to analyze,
1133000	1140000	it will take so long to send it to human scientists to get some instructions what to do next.
1140000	1146000	So that would stimulate the development of autonomous intelligence systems
1146000	1154000	that need to collect something, do something, decide what to do next, what it means.
1155000	1161000	Since then, it was a slow but steady progress.
1161000	1166000	So the systems that we are working on, it's already over 20 years.
1166000	1168000	Yes, it's called Robot Scientist.
1168000	1170000	And yes, the first was Adam.
1170000	1178000	It's actually stand for adaptive machines, but yeah, Adam, because it was the first.
1178000	1183000	Not to be confused with the other Adam for training neural networks?
1183000	1185000	Yes, it's a good point.
1185000	1191000	So the principle of such system is that it's very knowledge-intensive.
1191000	1199000	So we hear a lot about progress in AI, but it's mostly data-driven.
1199000	1203000	So now we have so much data and also computational power,
1203000	1207000	so it can be processed and something useful output.
1207000	1214000	And I believe that the next step will be when a lot of knowledge will be available
1214000	1216000	in the form that computers can take.
1216000	1218000	So right now they are taking data.
1218000	1227000	If we can give our knowledge and let them to reason with it, so that will be just...
1227000	1231000	And to do scientific discovery, of course, you need to have knowledge.
1231000	1241000	Scientists still insure supply because it takes like 25 years to produce.
1241000	1243000	Wow, you need to educate.
1243000	1246000	It's many, many years of studying.
1246000	1256000	And if we can create systems and also knowledge models starting from some domain knowledge
1256000	1260000	in some particular areas, so we are working with biological systems.
1260000	1267000	So if you can encode in machine-processable form, what we know,
1267000	1271000	and also encode how we do science.
1271000	1274000	So how we formulate hypothesis.
1274000	1277000	Of course, not any, but at least some.
1277000	1280000	It's actually quite algorithmic.
1280000	1282000	Yes, I'd love to know more about that.
1282000	1284000	So you said some really interesting things.
1284000	1287000	I mean, first of all, as you say, if we want to learn about Mars,
1287000	1289000	it's very sample inefficient because it's very expensive.
1289000	1292000	And then we get to the purpose of science.
1292000	1298000	And in my opinion, the purpose of science is about generating intelligible explanations.
1298000	1300000	And then we get to knowledge.
1300000	1305000	And I think you were just alluding to data and information is not knowledge.
1305000	1308000	And I know you're an expert in knowledge representation.
1308000	1316000	So perhaps we could just touch on, does knowledge have some primacy over just normal information?
1317000	1323000	Okay, so it's very true that different textbooks will give different definition.
1323000	1326000	And sometimes there is no distinction between data and information.
1326000	1328000	It's like synonym.
1328000	1333000	But yes, in my area of research, we are very clear about that.
1333000	1334000	Naturally.
1334000	1337000	So what we can see the data is facts.
1337000	1338000	Yes.
1338000	1341000	So this gene has this function.
1341000	1345000	And duck is birds.
1345000	1349000	Knowledge is when you go further.
1349000	1358000	So when you have rules, when you have, like, even connections or links between facts,
1358000	1360000	you can go further.
1360000	1363000	Yes, more knowledge model.
1364000	1375000	If you have a lot of data, how's prices in this area, how it was affected by flooding,
1375000	1380000	or so they dropped how much, then you can detect this patterns.
1380000	1382000	So this is knowledge.
1382000	1387000	So if there is flooding in the area, prices will drop.
1387000	1389000	So this is the rule.
1389000	1394000	And you can represent it in various forms.
1394000	1402000	So if we can represent it in forms that you can put it into memory of computers,
1402000	1404000	then it can be used.
1404000	1409000	And if you can be used together with data, if you have all these components,
1409000	1411000	it will be even more intelligent.
1411000	1412000	Interesting.
1412000	1418000	I guess where I was going with the question a little bit was also, can knowledge be probabilistic?
1418000	1419000	Of course.
1419000	1421000	All our knowledge is probabilistic.
1421000	1424000	We are not certain about anything.
1424000	1428000	Even if we think, yes, this is the state of the art.
1428000	1433000	So much knowledge was changed, was refuted.
1433000	1437000	So we always walk in this probabilistic framework.
1437000	1440000	We not always reflect on it.
1440000	1446000	But if you want to build a system that really outputs something,
1446000	1450000	it is underlying.
1450000	1454000	So the other side of working on such a system,
1454000	1460000	it gives you a chance to understand better about what science is.
1460000	1464000	There are no guaranteed truths.
1464000	1467000	It's just what we believe in at this point,
1467000	1471000	given all data we have and the other series,
1471000	1473000	we just walk together.
1473000	1475000	But it can change in the future.
1475000	1477000	There are a couple of things.
1477000	1481000	Rationalists think that there is only certainty.
1481000	1483000	You either know it or you don't.
1483000	1491000	I believe that Bayesian reasoning is an extension of logical reasoning in the domain of uncertainty.
1491000	1493000	Absolutely correct.
1493000	1495000	And it's important to remember about it.
1495000	1502000	It's important to remember even when we read newspapers.
1502000	1506000	But even scientific articles,
1506000	1511000	scientists actually tend to overgeneralize.
1511000	1515000	They are very excited about what they discovered.
1515000	1519000	It's important to step back and think that,
1519000	1521000	yes, it actually can be refuted.
1521000	1524000	It can be only one explanation.
1524000	1529000	And if you look at history of science,
1529000	1533000	in Newtonian physics,
1533000	1538000	it looked like the ultimate truth and said no.
1538000	1542000	The theory of relativity turned it all upside down.
1542000	1545000	It's only some fraction of it.
1545000	1546000	Yes.
1546000	1551000	To what extent should knowledge be grounded in the physical world?
1551000	1557000	So, the whole science, it has to be verifiable.
1557000	1564000	You can imagine many things plausible,
1564000	1571000	but unless you really show that this corresponds to reality,
1571000	1573000	we cannot trust it.
1573000	1577000	Of course, there can be maybe another explanation,
1577000	1581000	but at least this is connected to what we observe or even better
1581000	1586000	if we can design an experiment to show there is something behind it.
1586000	1590000	So, this is how science always worked.
1590000	1594000	Of course, I referred to experimental science
1594000	1597000	like biology, physics, where you actually can choose.
1597000	1601000	There are sort of experiments where we will not go astray
1601000	1607000	as we cannot build an automated system to do that.
1607000	1608000	Interesting.
1608000	1611000	I wondered whether you would define yourself as a neat or a scruffy
1611000	1613000	when it comes to knowledge representation.
1613000	1615000	I mean, for example, if I define...
1615000	1618000	And by the way, a neat is like a puritanical,
1618000	1621000	simple underlying principle, parsimony.
1621000	1623000	So, if I wanted to define a chair,
1623000	1628000	I could think of thousands of different analogies to describe a chair.
1628000	1632000	Is it collapsible or is it just very complicated?
1632000	1637000	I am a very neat and I can give you a perfect example
1637000	1639000	how to model a chair.
1639000	1640000	Go on.
1640000	1644000	We are very neat because we work with these reasoning systems
1644000	1647000	and they need sharp, clean logic.
1647000	1650000	And if you complicate matters, it complicates reasoning.
1650000	1653000	A chance at something will be really discovered
1653000	1656000	and we want to connect different bits of knowledge, different data.
1656000	1659000	So, the more streamlined it is, the better.
1659000	1664000	And if you use like minimum of relations
1664000	1669000	and so over years, I just learn to think that way
1669000	1671000	and about chair.
1671000	1674000	It's actually examples that I used in my lectures
1674000	1676000	when I was explaining to students.
1676000	1677000	Wonderful.
1677000	1679000	So, I was showing them just examples.
1679000	1681000	So, these are chairs.
1681000	1683000	They can look very differently.
1683000	1685000	Some don't have any legs.
1685000	1687000	Some have three legs.
1687000	1689000	So, how to explain computers?
1689000	1690000	This is a chair.
1690000	1691000	Yes.
1691000	1692000	So, you can...
1692000	1695000	Yes, you can represent many, many, many ways.
1695000	1699000	But a good knowledge model
1699000	1703000	will focus what we call intrinsic property.
1703000	1705000	What makes it a chair?
1708000	1709000	Give me an example.
1709000	1711000	What makes it a chair?
1711000	1714000	It's function to be seated on.
1714000	1717000	So, it's a relational model.
1717000	1720000	So, it was designed.
1720000	1723000	It was produced for it.
1723000	1724000	Yeah.
1724000	1725000	This is so interesting.
1725000	1727000	It can take many shapes, many colors,
1727000	1730000	but this is what defines a chair.
1730000	1732000	And because we produced it human,
1732000	1737000	so we can say this is function.
1737000	1739000	And all other...
1739000	1741000	Yes, it can have other properties
1741000	1743000	like what color it is, what shape.
1743000	1746000	But ultimately, this would make it a chair.
1746000	1747000	Yes.
1747000	1750000	Because when I hear philosophers talking about semantics,
1750000	1752000	they have to come up with some kind of a relational model.
1752000	1755000	And they will choose the view of function
1755000	1758000	or purpose is another good one.
1758000	1762000	And I guess what I'm saying is that it's a little bit anthropocentric
1762000	1766000	and like function.
1766000	1768000	For example, a hospital.
1768000	1770000	That building used to be a hospital,
1770000	1772000	but it's not a hospital anymore.
1772000	1778000	But again, why it is so centric?
1778000	1781000	Because it's us who made it.
1781000	1783000	So we have control over it.
1783000	1786000	Yes, it was hospital, now it's no more hospital.
1786000	1789000	Some social convention, some decision was made,
1789000	1791000	it was repurposed.
1791000	1793000	Now it is not hospital, it's something else.
1793000	1797000	But if you take biological systems like gene function,
1797000	1800000	we have nothing to do with that.
1800000	1802000	It evolved to be there.
1802000	1805000	It's still intrinsic property of it,
1805000	1808000	what function it has, what it's for.
1808000	1811000	Otherwise you can be very far.
1811000	1814000	Again, going back to a chair,
1814000	1816000	I can sit on a table.
1816000	1818000	It doesn't make it a chair
1818000	1820000	because it's not primarily function.
1820000	1823000	This is not what this was made for.
1823000	1824000	Yes, yes.
1824000	1826000	So if you agreed on it,
1826000	1829000	you need to have this clarity,
1829000	1832000	also agreement that this is how you model things.
1832000	1834000	Then you start to have consistency.
1834000	1836000	What is a chair? What is a table?
1836000	1839000	Because if you want to have intelligences,
1839000	1842000	then to design your office, for example.
1842000	1844000	So how will it do?
1844000	1847000	How it will distinguish what is what?
1847000	1851000	So this is quite similar in principle to Wittgenstein
1851000	1854000	said that the meaning of a word is in its use.
1854000	1857000	And I'm friends with a colleague of yours, Mark Bishop,
1857000	1860000	who says the meaning of computation is in its use.
1860000	1863000	But that's a very kind of relativistic,
1863000	1866000	social emergentist kind of view.
1866000	1870000	I guess what I'm saying is do you think there's a platonic meaning?
1870000	1874000	So for me, it's very easy to answer on these questions
1874000	1878000	because I actually work in the development of real systems
1878000	1881000	that need to take it, reason with it and produce something.
1881000	1885000	Justification of what we are doing and how we are doing it works.
1885000	1888000	You can do it differently.
1888000	1890000	You can take different points of view.
1890000	1892000	Let's design a different system based on it
1892000	1894000	and then compare what is better.
1894000	1896000	No one has done it.
1896000	1900000	So this is what is done, how it's done, it's working.
1900000	1902000	Okay, so that's very pragmatic.
1902000	1907000	Yes, I would say it's not relativistic.
1907000	1913000	It's a pragmatic approach to producing knowledge
1913000	1915000	in machine-consumable form.
1915000	1917000	It's not optimal, it's not the best,
1917000	1922000	but it's something good that computers can work with
1922000	1924000	and produce something.
1924000	1926000	I think you might be a closet scruffy.
1926000	1934000	Okay, are you familiar with the Psyche project from Doug Lennet?
1934000	1936000	I think so.
1936000	1940000	It's almost like preaching to the choir.
1940000	1945000	What kind of retrospective comments do you have about that project?
1945000	1950000	I think they just attempted to do too much.
1950000	1953000	So it's like in AI.
1953000	1964000	So this was a constant genetic AI versus something practical, fragmented.
1964000	1968000	So of course it's wonderful to have a system that can solve any task
1968000	1971000	for that you need such knowledge models.
1971000	1973000	But just not say yet.
1974000	1976000	Maybe in the future.
1976000	1982000	Right now we're trying to model some fragments, some domains of knowledge.
1982000	1987000	And if we have enough such fragments, hopefully we can combine.
1987000	1994000	But ultimately, if you want to have general AI, you will need such models.
1994000	1996000	Interesting.
1996000	2001000	One contrast people make is, I have friends who are in the domain of certainty.
2001000	2004000	So I think knowledge is either you know it or you didn't.
2004000	2007000	And the reason that they give and the reason why they don't like language models
2007000	2011000	is because they say, well, when you have actual knowledge, so you actually know,
2011000	2014000	you can deduce new facts and new knowledge.
2014000	2017000	So you can create more knowledge.
2017000	2021000	And they would say with a language model, because it's only pretending to reason,
2021000	2024000	you can't create any new knowledge with it.
2024000	2030000	It's actually an advantage we already started using it to generate hypothesis.
2030000	2038000	Because it's not rigid deduction, it is how you can actually make new guesses.
2038000	2040000	And some of them might be true.
2040000	2043000	You just need to experiment to show it.
2043000	2046000	So we already started.
2046000	2051000	We generated some very interesting hypothesis.
2051000	2054000	We're already organizing experiments to test them.
2054000	2056000	Yes.
2056000	2060000	I believe that science is about abduction.
2060000	2063000	So like creating this plausible set of hypotheses.
2063000	2066000	And we're at an event all about creativity.
2066000	2069000	And humans have that spark of inspiration, don't they?
2069000	2073000	We just have an idea about plausible hypotheses.
2073000	2079000	And maybe the world is quite sclerotic and predictable and boring.
2079000	2084000	But do you think that there's some spark that we won't get from Ayo?
2084000	2086000	I completely agree with you.
2086000	2088000	And it's exactly what we use.
2088000	2093000	We use abduction in our reasoning system to generate hypothesis.
2093000	2094000	Yes.
2094000	2100000	So in some simple application areas like in drug design, you use abduction.
2100000	2106000	You just have many, many, many examples of chemical structures, short activities.
2106000	2110000	So you can, if you see similar structure, you induce it.
2110000	2113000	It might show similar activity again.
2113000	2114000	It's not always.
2114000	2118000	So still, you cannot infer ultimate truth.
2118000	2120000	That is why you need experiments.
2120000	2126000	But abduction, yes, this is something that takes you outside of what you already know.
2126000	2127000	Yes.
2127000	2128000	I completely agree.
2128000	2133000	I think it's the most exciting part.
2133000	2140000	But again, this is what is technological.
2140000	2144000	So there are reasoners that can do abduction.
2144000	2149000	There are languages that can handle it.
2149000	2153000	So we have many building blocks to build such system.
2153000	2154000	Yes.
2154000	2155000	Yes.
2155000	2159000	And are you excited about the prospect of neurosymbolic architectures?
2159000	2166000	Or are you still very old school and you like these explicit knowledge representation?
2166000	2170000	That is a difficult question.
2170000	2174000	Yes, I'm very excited about what is happening.
2174000	2181000	I was skeptical about all these natural language processing.
2181000	2184000	We tried to work with NLP people.
2184000	2190000	And the results were quite disappointing how much useful information they actually could extract.
2190000	2194000	And then such progress.
2194000	2196000	So of course, it is exciting.
2196000	2202000	And we already started using it in our scientific work.
2202000	2219000	So at the same time, I believe there is a place for this very clean, crisp model just because you need something as simple as possible just to enable reasoning.
2219000	2227000	Just because all these reasoning engines are powerful.
2227000	2235000	But if they can reason over simpler structures, there is more chances they will do it better.
2235000	2245000	So for practical purposes, from all my experiences, the cleaner you can get, the better it works.
2245000	2256000	And ultimately, when we have more technological progress, we probably will be able to more reflect how it is in our brain.
2256000	2257000	Yes.
2257000	2260000	But there is also robustness and reliability and trustworthiness.
2260000	2261000	Precisely.
2261000	2267000	So you actually then can explain how it was done through this explainability.
2267000	2269000	It's all trustworthiness.
2269000	2273000	So if people understand how it works, if we can explain, this is how it happened.
2273000	2276000	This is what was taken as input.
2276000	2278000	This is how it was done.
2278000	2280000	And people will trust it more.
2280000	2286000	And especially like systems that we are working on, applications for cancer research.
2286000	2296000	What doctor would recommend these drugs if they don't understand how this system came up with the suggestions?
2296000	2297000	Yes.
2297000	2303000	And again, the cleaner your representation, the easier to explain it.
2303000	2304000	I know.
2304000	2305000	I really agree with you.
2305000	2319000	The only intuition I have is that if we're designing a novel molecule, for example, even if we knew or could understand how it was created, would we really understand it?
2319000	2321000	It seems so complicated, doesn't it?
2321000	2323000	It is.
2323000	2325000	But again, there are different logics.
2325000	2330000	So for example, physiologic, you can build system around that.
2330000	2341000	So ultimately, let's try to build more such systems and then you just force to understand.
2341000	2344000	So if you can build it, then you understand how it's working.
2344000	2345000	Yes.
2345000	2349000	Inevitably.
2349000	2356000	So I still believe that some portion and put in portion of this can be modeled.
2356000	2360000	Something probably not in any near future.
2360000	2364000	It's just too complex, too fast.
2364000	2366000	Louisa has been an absolute honor.
2366000	2370000	How can people find out more about you and what you're up to?
2370000	2376000	We're actually very bad with telling about what we are doing.
2376000	2380000	That is why I'm so glad that you invited me to talk about it.
2380000	2387000	We just buried in our work and the best we will publish our scientific papers and it is there with top.
2387000	2392000	Yes, you teach students, you tell students, it's based on your research, you use examples.
2392000	2400000	But apart from that, we don't do much and we are really quite keen to change this.
2400000	2409000	I think a lot of people are intimidated by papers and science and if only people knew how many fascinating ideas there are in there,
2409000	2413000	I think they just need a hook and it's so interesting.
2413000	2415000	No, I blame scientists.
2415000	2420000	They keep this complicated language even if sometimes they don't need it.
2420000	2425000	So they just keep people away from it.
2425000	2426000	Yes, they do.
2426000	2432000	No, I think scientists should do more about explaining their work, how they are doing it.
2432000	2438000	I always thought that we are doing our work, someone else will.
2438000	2451000	It's so full that AI has created problems, but no, it's actually us who have to do it and explain and tell and also warn.
2451000	2454000	Yeah, well now people can ask GPT for.
2454000	2459000	But what it generates, it's not all of us correct.
2459000	2462000	That's true, it's not, but it's quite good.
2462000	2464000	It's good, it's very impressive.
2464000	2466000	Yeah, it's quite good.
2466000	2467000	Larissa has been an honor.
2467000	2468000	Thank you so much.
2468000	2469000	Thank you.
2469000	2470000	Thank you very much.
2470000	2471000	My pleasure.
2471000	2475000	I'm so impressed by how much you know and understand.
2475000	2476000	Oh really?
2476000	2477000	Wow.
2477000	2480000	So many people around know abduction.
2480000	2481000	Come on.
2481000	2483000	We are a computer science podcast.
2483000	2486000	I think it's one of the most fundamental things.
2486000	2492000	Because you know, going back to Aristotle, you know, with the weak syllogisms and the strong syllogisms.
2492000	2499000	And, you know, I think of induction is actually quite interesting because most people use it to include abduction.
2499000	2506000	So not only creating the map to the hypothesis space, but actually, you know, abduction is just creating the plausible.
2506000	2513000	I completely agree because if you clear about it, so you clear about logic, so you can have fuzzy concept.
2513000	2517000	All this probabilistic, but underlying logic must be crisp.
2517000	2518000	You need to understand.
2518000	2524000	And if it's abduction, precisely, it's only with some probabilities that it's correct.
