Welcome to MLS T.
Today we are extremely excited to have two distinguished guests join us.
The first guest is Ellie Pavlik.
She is an assistant professor of computer science at Brown University
and a research scientist at Google AI.
Her work focuses on building better computational models of natural language semantics and pragmatics,
aiming to help computers understand language the way humans do.
Our second guest, the legendary Raphael Millier, is the 2020 Robert A. Burt Presidential Scholar
in Science and Neuroscience in the Center for Science and Society
and a lecturer in the Philosophy Department at Columbia University.
Raphael completed his D-Phil in Philosophy from the University of Oxford,
where his work centered on self-consciousness and his main interests lie in the philosophy of artificial intelligence,
cognitive science and the mind.
Now, this is an interesting experiment for us.
We've decided to get these two heavy weights, just one-on-one having a conversation with each other
and we're hosting it here on MLS T.
They spoke about compositionality and grounding,
compositional generalization benchmarks, mechanistic understanding in language models,
variable binding in transformers, language and vision models,
compositional behavior in humans,
compositional reasoning and negation in language models,
variable binding in reinforcement learning and transformers,
the difference between instruction tuning and RLHF
and the benefits of RLHF, referential grounding and language models.
And yeah, the Chomsky skepticism, of course, you know, our friend Stephen Piantodosi's paper,
inductive biases in language learning, language models in different languages
and indeed the future of academic work in language models.
Now, the audio from Raphael in particular wasn't as good as it could be.
I've done my very best to process it and improve it.
Just to help folks follow along, I've kind of generated some subtitles.
The subtitles on Google are absolutely rubbish
and speech recognition technologies come along so far in the last few years.
So, I've generated some better subtitles using another service
and I've superimposed it on the top.
I've also superimposed some descriptive titles on the top
just to help you folks follow along at home.
So, anyway, without any further ado, I give you Ellie Pavlik and Raphael Billier.
Enjoy.
Hi, Ellie.
I know Raphael. Hi, how's it going?
I guess you talked to Tim before,
so maybe you have a kind of more of a context of the previous conversation that started this one.
So, I'll let you decide where we're beginning, what the first topic is.
Yes, so we had a chat back a few weeks or half a year ago
when I came on the podcast and we had to get it short and you thought,
maybe if I come back home, we should do it as a discussion.
And I think you had to chat with you.
So, Tim thought that maybe some of the topics we could discuss
would be equal positionality and brown links with both versus your dads.
Seems natural, yeah.
Yeah.
So, I don't know whether we should try to disagree more than actually do
because I think we were aligned with a lot of the topics,
but I'm sure there are some topics.
Yeah, but I mean, I also feel like for both of them, I know what I currently think,
but I also am pretty prepared to just have to renege in a couple of years
and be like, I was wrong.
So, I feel like we can see both sides.
So, we can definitely simulate some disagree or we don't have to disagree,
but we can argue both sides of whichever issue.
I don't know where do you want to start?
Do you want to talk grounding or do you want to talk compositionality?
I think I have more immediate questions with respect to compositionality.
So, one thing I was wondering is how do you see the progress
on compositional generalization benchmarks?
So, just for the listener of yours,
it's really hard to assess whether large language models
on huge corpora actually acquire the capacity to generalize compositionally properly
because you can never really know what's in the training data with your models.
Before, you can never really know whether they're the memorized structures and so on.
So, what strategies to use, synthetic datasets, such that you try them on a test set
and on a trans set and then when you test them,
the only way that you can achieve this goal is generalizing perfectly.
And so, there has been some, the initial results from some of these datasets and benchmarks,
like Scars, Cards and others, were a little bit mixed with LSTMs and formers
and lately they have been a part of the steady improvements
past due to tweaks in the architecture and I remember having a discussion with Tal,
Linsen, when he organized this composition of the workshop that he was speaking of.
And one of the, one of the contentious points is whether we could call these tweaks
or whether these are significant changes.
And that's always the crux of the debate, I think, including also with things like
Paul Smolensky, Paul Smolensky's approach that has an explicit transfer product of presentations is
how much of a hand engineer tweak you make in the architecture
to solve this composition generalization problem and how much do we need.
So, I was just wondering where you step on that.
That's a super interesting, I mean, so, yeah, so on the composition generalization test,
I mean, I probably have like an unsatisfying middle ground opinion.
I'm curious where your thoughts are.
So I think, I think we're both pretty interested in the kind of mechanistic stuff right now.
So I guess for the audience, this is this idea that like this idea of trying to kind of understand
what the models are doing kind of under the hood.
So when we think about the, I guess, compositional generalization tasks,
it's like we have some inputs, what are the training, like what is the training model it gets
and then what are the outputs that it produces.
And that's really the data that we're basing our claim about whether it's compositional or not on.
And the kind of mechanistic or the other approaches to try to characterize
what actually is the process it used to get from the inputs to the outputs.
I really like Chris Ola, who I think coined the term mechanistic,
uses the phrase of kind of trying to understand the source code of the model.
So it's like you have, you want that kind of something like a kind of human understandable description
of the algorithm that it's running under the hood.
And so like I've just, I've been super hung up on that.
Like I think all of my projects, all of like what my students are working on are some flavor of that
because to me, I think the reason it's so, I think it's interesting,
but it also just feels like the questions about things like compositionality are almost stuck right now
without that level of description.
So if we're just looking at what are the inputs and what are the outputs,
I just feel like we're, it's just going around in circles with people like,
like we're not really making progress on the issue.
There's kind of people who are inclined to agree and inclined to disagree.
And so from my perspective, like, okay, so if we have the model that's doing this kind of, I guess,
quasi generalization, it's like succeeding on some cases,
not perfectly compositionally generalizing in the kind of really abstract case
that those data sets tend to be going for.
But it's doing something in between and we're trying to figure out whether that counts as compositional or not.
It seems like that just hinges on what it's actually doing under the hood and how it's doing it.
So I guess I'm like, I'm like basically like neutral on or not paying attention
to compositional generalization data sets right now.
And I recognize like a very fair criticism.
And I feel like when I've talked to people like tall, probably have this word.
It's like you can't just sidestep the issue.
It feels like being overly generous to the neural networks, right?
It's like they're not doing that well.
And then you like change the game a little bit, right?
You're like, oh, well, that's not even the metric we care about.
That's not really what I would see as the goal.
It's just like in the immediate term, it seems like first we want to characterize what's happening under the hood.
And then we can come back to those data sets and understand them much more in depth.
And then when we see how they're solving it or not solving it, we can like,
it gives us a much more concrete thing to analyze and try to ask whether that counts as compositional or whether that's at all human like,
if that's what we care about.
Like it, like, I guess to me it feels like a dead end if we're not allowed to comment on the procedure that happened in between.
And right now we can't comment on the procedure that happened in between inputs and outputs because we just don't know what's happening there.
So I don't know, I guess that's my current take on compositional generalization data sets is I'm like,
now is not the right time for them.
We'll come back to them later, which I recognize seems like a dodge, but it's not meant as a dodge.
It's like, basically, we'll come back to this later.
Yeah, but I'm curious what you think about them.
Yeah, I mean, I'm very sympathetic to that view.
And as you know, I'm super interesting to mechanistic that visibility just to looking at positionality.
And I guess in the background, there is lurks this debate about whether humans themselves have something like a perfectly compositional language of thoughts or something else.
Right.
Absolutely.
And so perhaps, you know, we might, we might learn some things about computations implementing a human cognition by looking at this imperfect completion of systems.
Absolutely. I mean, I think that's, that would be really my, we just like so right now, and this is a gross oversimplification of where the two viewpoints are, but like, the two really concrete like
kind of options on the table for like what a system can be is like this pure symbolic language of thought and that the language of thought it would be something like humans in their heads have something like a Python program language,
like a perfect kind of formal system for reasoning over symbols in this compositional way, or it's like this loose, or like these associations, these idiomatic kind of just things back together.
And like, I think most people would assume or something in between and there's not a really good proposal for what the in between is right like there's all kinds of ways of being in between those two things.
And so I feel like whatever we find in the neural networks gives us some kind of concrete proposal like here's an example of an in between.
It's not the language of thought thing.
They're often also obviously not just giant lookup tables right they're doing something more than that.
So whatever they're doing in between it's at least like a candidate for what could be happening right that's pretty exciting.
And it might not be the right one but at least it's something because I haven't seen like a really satisfying candidate for what the in between is right.
Like a lot of the debate still seems to kind of put these two strawmen up against each other.
Yeah, I agree.
And I think some of you with some of the people who are pushing back against this kind of really symbolic thing with the thoughts architecture that Paul Smolensky, the self-faith that you need to build into your connections model, some more explicit compositional structure with these
kinds of products, but these these vectors, rodent field of vectors that are never formal, but you can combine with transfer part of operations and there are other vector symbolic architectures like this that do it like that.
And my qualm is always with with this and I have to talk about this.
I think, you know, he's open to the possibility that perhaps transformers, you're handling compositionality in the way transformers handle these might turn out to be enough perhaps but my calm is that if you if you build, you know, if you kind of hard code.
Into your network architecture, or you do some future engineering to the input vectors being your first and all that seems just to be ad hoc to me right so you need to specify what are the rules what are the fillers and that's that seems like an
Inclosable model for how it works unless you want to say that they are in its academic concepts and we just do a lot.
I mean, a lot of people do.
Yeah.
Right.
That's like a claim that I think a large fraction of cognitive science is very happy to say is the case is that there's an inventory of innate atomic concepts.
Right.
But but yes, I agree.
I think maybe it's the computer scientists got or something something about that is unsatisfying or at least you want a story of where those came from, which still seems like you somehow it needs to emerge or come from data or come from some kind of pressure other than just we got lucky and they were there.
I changed the right inventory of concepts.
I mean, there's some daylight between the core knowledge knew that, you know, there are some some basic concepts and the four orient view that all atomic concepts must be in it because right.
Yes, which is born with this.
That's part of it.
And they're on the ball.
Yeah.
Right.
And it seems that if you if you want to handle all compositionality with these ad hoc world and filler vectors or something along these lines, then.
It seems that you're going in that direction. And what I'm excited about which forms formers is that it looks like they can do something that in my mind, if you look at the kind of secretive research researcher looks quite like variable binding by reading and writing information to sub spaces of the main embedding
space and using that as a virtual content principle memory.
But it's fuzzy right it's not just implementing a classical symbolic.
That's always the charge that I think that the classes are never against the connections is that oh if you're if your model is working well and if it's if it's a good model of human behavior and then presumably is just implementing classical symbolic architecture and it's right, right, right, right, right.
I mean, almost I, I'm teaching a class this spring on language processing of humans and machines and we were talking about this question of implementing a classical architecture and it's actually quite hard to tell even in the historic debates on this like what what
people are even claiming about this right like there's like simultaneously claims that like oh it'll like sure it can implement it but also nothing it ever does whatever count as an implementation like I just can't even tell what the what actually the consensus is.
But I agree I think we'll get something that the ideal is that when we look into the transformers will find something that preserves the really necessary pieces of it.
But is different enough to be interesting I mean, I don't know I guess it in some ways if what we did if what we ended up finding was something that was identical to the classical architecture that would be like, I think a huge win for classical architecture.
People right that the only way these transformers a were able to solve it was by learning to implement the thing they said that you would need all along.
But I think it's unlikely that we would find that right like I don't really know exactly how that would even work on so we'll get something like this fuzzy version, which, which I think would just be fascinating.
I mean, I'm curious so the variable binding stuff so I'm generally take the position that the transformers these other logical networks will be able to implement like these core kind of symbolic operations will be able to replicate this kind of behavior.
And then when we kind of dig under the hood will be able to find these clever implementations of these things.
But variable binding is one of these things where we've actually really struggled to find good evidence of it.
And I guess particularly so in language models it almost seems like you have to say it's there by like by Fiat because of some of these the ability of the models to do this compositional generalization stuff like they, they, I think Tom McCoy had a really cool paper where
I was showing that they could generate some like syntactic structures that were unattested in training. So for something like that you're like well I guess to do that you would have to have something like a abstract filler and roll binding.
But especially what we've played around the language and vision models and maybe it's just that clip kind of sucks.
I guess clip is the one of the pre trained link image and text models that kind of is trained to map images to to text captions.
But we've had several projects of trying to show, I guess clip and then the image generation models based on top of it like Dolly and state things like that.
Like we just can't get any evidence that it's doing anything clever or abstracting away from the from the structure so that's where we'll look for stuff like a red cube in front of a blue cube and then blue cube in front of a red cube or things like the, I guess the famous on Twitter like
astronaut riding a horse a horse riding an astronaut. And like, even with really controlled cases and doing it in large days that like we just can't get it to do anything that gives us any data to point to to be like look it's doing okay.
But maybe it's just a clip problem maybe just sex.
I think it's, I think it's a clip problem so that's that's, I wanted to talk about this because you have this really interesting paper about it and I think we can't generalize any finding about vision language models based on contrastive learning clip to say, language models train or to
say, there's this recent paper from Stanford configurably first author is like which which confirms my suspicions basically the ideas that click treats ends up treating text like just by the bags of words, because in order to just to fulfill the contrastive learning objectives of bringing
the captions and measures that go together and further away in the space the ones that don't go together. You don't need to actually induce much about syntax, right, just keywords is sufficient and and generally also in the captions themselves you don't have a ton of information about, say, like their, their relative positioning of different objects or how many of, you know, it's not like you say, they are key forks and one and one knife or just say
something in the caption. Right, right, right. And if you look at all the models like party from Google, other image nourishment models that is a pre trained as a text and currently is a pre thin language model, they actually do way better at things like much better.
Yeah, yeah, yeah. Yeah. Yeah.
Yeah. And I think that surprised me in trying to do so we did the red cube blue cube kind of stuff because that's like super abstract and it's easy to do. But one of my other students Charlie Lovering is working on a project with some of the image generation models to do to try to kind of more systematically look at the, you know, horse riding an astronaut kind of example.
And one thing that struck me with it is just how like how not uniformly distributed the world is right like it's just like finding examples of relations, like where you want to have like argument one relation argument to that can actually exist in both directions.
And that's a thing that a human could visualize. We like, it's really, really hard to do like we're restricted to a really simple set of relations because we like spit out a whole ton of things and tried flipping the order and you're like, yeah, this isn't a thing I could actually imagine or would expect a human to be able to
to realize in any kind of way. And so that I think getting at kind of revisiting our assumptions about how compositional humans are I think that's like kind of relevant data to have is like how often are we actually forced to combine completely
novel elements in a purely abstract way without anything that we can relate it to. Like what we think we're our kind of working theory for what the model is doing for these kinds of things is like if you ask for a horse riding an astronaut, it's like finding the most similar thing it's seen.
Right. It's like, well, here's an example of like, you know, a teddy bear riding a puppy dog or something and then morphs it into a horse ride, like some other type of thing.
And it's like, that's not actually an absurd model to assume might be underlying some kind of compositional behavior in humans too.
Yeah, it might be of the this might be for with respect to language models that friends all these kinds of effects on reasoning where you know you test them on syllogism solving the Western selection tasks you get you get these effects that are similar.
You know, also fun in humans where if you're feeling the story of the task with plausible details that could apply to realize that they did much better.
Right.
You just use this like farfetched, you know, abstracted novels.
Right.
Right. Yeah, and that's that's been shown like I know this is, I don't remember the authors but there's pretty classic study about the kind of logical syllogisms right like if you ask people to reason about these just like, you know, famous modus ponens or whatever.
When you ask them like a not a like people suck. Yeah, they don't know how to do it but if you put it in realistic real world scenario like you're at a party you can have ice cream and cake or whatever then people do totally fine and I think it.
It wouldn't be surprising that we might have these that we would have evolved to be able to reason about realistic scenes and not abstract ones.
Yeah, and the, the, I think the skill to be able to reason over the most, the more abstract examples of including with images and those just in a possible way is something probably require through training and people of, you know, who have two children,
for example, are probably familiar with that you kind of need to some prompt engineering as a word to, to get them to actually, you know, draft certain concepts or how to, you know, teaching logic, even to the grads.
Yeah, you get a lot of resistance from this content effects and to be prime institutions in the right way. And I would I wonder whether you know with the astronauts running a horse examples, whether it would be interesting to try that with with small children,
and see whether, you know, whether they actually do well at that test.
Yeah. Yeah, I mean, I would have to assume they would right so that's where like my intuitions are strongly like but humans are quite compositional like you would.
You would have to assume that they would know how to.
They wouldn't just draw a horse riding, or if you asked for a horse, they wouldn't just draw astronaut riding a horse, right. They might like giggle and be like that's silly courses don't write something right but.
Yeah, I mean, I guess so I guess some counter examples to both of our intuitions so that there's some of this variable binding stuff but with the image models and I'm kind of like I'm willing to give the models a pass at least in the immediate term while we figure out what's going on because
and like yeah I think some of this perfectly abstract compositionality might be like a tall order and not something we would have to do.
But the other thing that's always been weird to me so like I'm channeling Roman Feynman my colleague here who's like much more language of thought tradition and you know he'll say that you know of course humans are like you know better at like he's aware of all the data that people
aren't as good at logical reasoning in some settings but they can do this other sense and things like that but it's like that doesn't undermine that humans have logical capacities like we use negation freely all the time like we don't really struggle with that.
But the language models actually still kind of suck at negation like it's pretty easy to just write something in a slightly odd way and get them to ignore that you negated something I think even fairly recently I guess I haven't tried this in the past month so
maybe it's fixed but like I asked something like asked GPT or chat GPT I think for like a recipe that like uses tofu and nutritional yeast but isn't vegetarian or something like that and it just spits out some vegetarian recipes or something
like you know you sugar and lemon but not a dessert and it's like have you tried lemon bars and stuff so it's like it just kind of ignores it and that's weird to me that seems like for a language model doing a language modeling task like that's relevant like
this isn't like a super trick out of distribution thing so that's kind of like I guess a thorn that like I feel like that's where I say like I have a bit of a caveat where I wouldn't be surprised if a couple years from now I have to be like yeah I was wrong the models are not at all like human like or something
I'm hoping that's not the case but these kind of data points are like yeah that's it's frustrating how bad they are with negation or other kind of basic things like that.
Yeah I mean it's always a moving target of course because people maybe said GPT-4 can handle that for example but I agree it's definitely very unsatisfactory to still his failures as some compositing of the problems.
Yeah like we have some I guess so on one of these projects with Roman in this undergrad we're working with Alyssa we and this is just super frustrating data for me although actually it is getting better with the bigger models so maybe it is.
But for GPT-3 it was like we have this very basic task that they had run on humans so you say something like it gives a little scenario and then you say you know it's like some scenario about scientists running experiments on rats and it's like the scientists saw that none of the rats liked the food or something.
And then it's like now that they knew that some of the rats liked the food and they like did human reading time and saw that people showed a slow down and were surprised by the word some in that case because it's a blatant contradiction to what was just said.
And when we use like GPT-3 and predicts surprise there's like no surprise whatsoever no suggestion of any up being at all bothered by the word some in that context which is weird because that's just a straight up language modeling task it's just what is the probability of this word in context which the human data was very clearly like it's low and the model was like it's fine.
And then but then we had we tried with GPT-4 and then the numbers it looked better but it was kind of messy because we can't get perplexities of GPT-4 so you had to ask it to fill in the blank is a little bit different so it's hard to read.
API was just the Chesapeake version.
The with GPT-3 we had the API.
Because now GPT-4 is available by the API as well.
Yeah but it doesn't we need to check.
Okay.
Yeah we need to check last we checked we couldn't get perplexities out like we had the API but.
I see.
Yeah but yeah it's one of these weird things.
Yeah and so like maybe GPT-4 is better but it wasn't as clean of us comparison to the humans and and it was even for a model like GPT-3 like it was surprising that it was so bad at that and like I don't know.
So I feel like there's a few of these data points that are like this story isn't a slam dunk like there's some of these things that really should be easy for a model with basic structure.
You know.
Yeah it's surprising to me because there are you know this that is been telling others right that's even way smaller models.
Can exhibit the right behavior in terms of surprise all when you look at things like subject development agreement field of gap dependencies you know I don't stress on these and.
You know the right range of some tactics and they exhibit the same patterns of humans in terms of being surprised when the.
Right.
Subject things like that.
Even if you're very not structured because of the structures to.
Right.
So why is negation right.
Totally it seems much much simpler right like it's not like if anything I feel like negation plus these quantifier terms like you could just enumerate a table and say these things.
Can go together and can't go together like it's I mean there's like a little more semantic cushion around it you have to know who the who's being modified or whatever but like.
I'm quite sure models can do all of that so I was very surprised it might be just that it's a really.
Infrequent thing in the training data in the input but then you have this poverty of the stimulus argument that you have to account for right is like.
I mean maybe it is that this kind of entailment relation is just not frequent on the Internet but it is in kids input or I you know I don't know or maybe it is that the models are just not the right models for this task I don't want to believe that though I think there's
I think there's must be some other reason.
Yeah, I mean there was this paper by Will Merrill that shows that entailment semantics can be induced by an idealized ideal language model on synthetic data up to a certain sentence length.
In real in real world scenarios there's only given the size of the three.
I think perfect in terms of that it's going to be induced for me to this formal truth to send this is the four words.
But if you want perfect absolutely you know perfect.
Right, right, right.
You exactly know whether or not sometimes in the next intelligent right which I don't know that I believe in entailment.
It's like the logic stuff you're like okay we can come up with these like toy domains in which we all agree about entailment.
But like during my PhD I did a lot of work on entailment and just trying to collect entailment judgments on humans is a nightmare right like they do not behave to the point that you have like that's what I feel like that was like a switching point for me where I was like okay maybe maybe I should accept that we have to trust them.
But entailment is entailed.
And as you idealize Gressian speakers as well humans are not.
Yeah, no, no.
But it's it's intriguing because it does suggest that I mean you can learn something about entailment semantics from distributional information.
Even you know entailments in a loser sense like in the non perfect right sense.
So again, why is negation so hard is is.
Yeah, yeah, I guess I haven't seen a really good study on just.
The distribution of negation in the like in a models training corpus like how is it used in what context because I don't.
Um.
Yeah, like it might just be that it's actually it's just not distributed the way we kind of think it's distributed it just functions very differently in written text in general.
Yeah, like I do think that perhaps kids would learn it like kids get a very different input of negation than what I would imagine is on the internet.
Right.
Like I in academic writing I use negation only in the most convoluted ways like.
Like it with like these triple negations that are right you know like while it is not unreasonable to assume that such is not the case right like that's the way I would use negation I wouldn't say like that's not a dog.
Yeah, no one writes that.
Yeah, and also it's interesting because in terms of the text that's actually generated language models.
I don't think you know even with you three in my experience you see negation error is like I can even remember when when they generate text they use negation properly it's more when the policy comes somehow.
Sometimes they know of negation as if they try to maximize the relevance of every word.
So if you mentioned if you say I want you to give me recipes but not in the paprika.
They see the word paprika and then there's like I need to maximize the you know.
Yeah, yeah, but right.
And it might be.
I so when I was playing around with the recipes and stuff it did seem to change a lot I mean it was anecdotal but on on the wording so like if you marked it a lot more right like.
Like if you said something like a recipe with these things but include me it does fine right so if you say not vegetarian it gets confused also if you'd say like and not vegetarian versus but not vegetarian and like where you put like.
Like where you whether you front loaded or back loaded certain information it made a difference and so you could imagine there's these distributional signals where it's like when people are saying don't do this thing they market in a few ways right they don't just like.
Slip in the negation but otherwise have the sentence read exactly like it would in the positive case.
Like this probably a reason we have but as a conjunction and not and right is just to like help emphasize so that people don't miss that negation piece of it.
Right.
So like you could imagine something like that that it's like the model has very little incentive to emphasize the negation unless there's other signals that you really are right.
So you wouldn't like be deathly allergic to Paprika and then just like slip it in.
Give me a recipe for chicken Paprikash without Paprika or something and then just like yeah.
Yeah I know that.
So so so one other thing that this article was writing about and viable binding that I wanted to just keep up there's this.
Preprints that I don't think I've got published.
And on our size that's an alleges that you know they looked at viable binding interest formers.
And I can't exactly remember the methods, but the alleged that they found that transformers can't really do viable binding unless it's by using the output as an external memory.
And it gives to some of the discussion that we had at the conference that I co-organized with Dave Cheneze and a few signals.
Nick Shea made a somewhat similar claim that you know the kinds of what he calls non content specific reasoning that transformers can do.
He's always propped up by reasoning on the output.
So using using the generated work, like saying China China China from saying you just reason step by step and use use the steps in the generator steps to crutch to solve problems.
And maybe there is also this paper is very you've you've written structural and sub routine.
Yeah, I haven't had chance to read that one but but I just wanted to ask you because I was striking to me that you can do some things zero shots with kind of language models that seem to fly in the face of that kind of thing.
So for example, would you be for you can you can tell it's behave like a python shell.
And then define a function with a bunch of variables and then a little bit later in the interaction just, you know, call that function for specific values and and say, you know, and because it's been having like a python shell it just has to give the answer zero shot it's not able to do some
and it can do this pretty reliably and there's some family modes but the fact that you can do it that's all just me that you can has to be able to internally right make variables.
Absolutely. Yeah, I yeah I love that example that you gave in the workshop.
So at this philosophy of deep learning workshop because you had that like you had it behave like a python shell and do this Fibonacci sequence.
The language model like we just ask the language model to predict or to tell you the, you know, seven hundredth Fibonacci number it messed it up right.
Yeah, that was like super interesting and yeah I guess the thing we like fight with with these models is that they're, you know, they're like we do know that they have that they've trained over the whole Internet and so they've learned to like kind of sub spaces right and they're like drawing from these different
domains and it's makes it really hard to interact with them because they can always from anthropomorphizing but they can always pull this like oh sorry I didn't understand the question game right like I thought I was being my red
itself but it turns out you wanted me to be my New York time self or something like and it could always pull that so it could be like oh yeah no I know how but you have to ask me to act like a python shell because people on the Internet don't know the Fibonacci number.
And this is a little bit of like for the for the critics of large language models this is a frustrating game to play because like like this is always a move that the proponents of language models can make is like oh you didn't ask it right like it knows how but it
which is really why I feel like the mechanistic stuff is so important because if we know more about it then this becomes like less of it feels less like you're sidestepping the criticism so we can just say what actually happened but I do think that right now that is the case right like it could be that they
have the ability to find variables and do this stuff but just they
have deemed that that's not the right way to solve the task in the average case or in a typical case like maybe negation is not important for the typical thing but if it if they're acting as a python shell then of course negation is important right.
Yeah, and I'm really interested in the role of our life chef fun tuning in that context so we have put money for feedback because it looks like it's it's vast including the zero shot capacity of the model.
And I think there's some evidence that it ends up you know, condensing the probability mass of, you know the distribution of over over the next word for to much more narrow range, such that you know, just a few words will have a high probability for certain for some problems,
because it's essentially enabling the model to to latch on to the right task right away right instead of having to do this. I mean still, as I said, it's still resolved.
Oh, sorry, I didn't get your question right. But if you ask a question zero shot to valley lad, jpd3 without the life chef, it will weigh more often just have no clue what you're really asking it and you have to like
totally. Yeah, yeah. Yeah, yeah, I like to use I like that.
Open AI recently re released their like the old school jpd3 because you can see what a big difference even the instruction tuning makes like my favorite is if you ask something like write a report on the war in Ukraine or something.
It'll like follow it up as though it's like in an email and it'll be like, please include additional like details on budgets and blah, blah, blah. Please get it to me by Monday regards or something like rather than writing a report it like gives you a list of other tasks to do.
But yeah, so there's clearly like, like the RLHF clearly improves your shower at least in even instruction tuning something I'm, I don't know, maybe you've thought about this more.
I know we talked about this like briefly before like just I guess I'm, I'm still getting my head around what RLHF is or in particular how it's like, if it is profoundly different from other types of training because I feel like in some cases like I've heard a lot
of people kind of crediting RLHF as like possibly like, oh, we have all these problems with, with our language models, but maybe RLHF alleviates them.
And I can never tell if there's like a genuine feeling that it does or if it's kind of a this is a new kid on the block and we're not sure so to kind of hedge or future proof whatever claims we're making we add this caveat that's like maybe RLHF alleviates it.
Or maybe it's that like RL in general RL is associated with like grounded and maybe more cognitive like possible learning in certain domains and so people hear the RL part and think that it's somehow better.
But I'm not quite sure if RLHF is like deeply different or if you could induce the exact same behavior through something more like an instruction tuning setup.
Like I genuinely don't know which of those things is going on and I haven't heard like I know there are people trying to come up with a fine tuning variant that otherwise behaves the same as RLHF because RLHF is unstable and people don't like it.
Brown has a lot of people who work on RL and they look at RLHF and they're like this isn't even really RL because it's weird and it just seems like you kind of like folded more language models on top of each other.
So, so yeah I just I don't know how I feel about it and my gut is to be like no it's not special it's like the same stuff in a new package but that's based on absolutely nothing except vibes so I don't know it.
Yeah, I mean I do show my intuition with respect to the difference between RLHF and instruction tuning because it seems you know if you look at what people did with the Lamar model from the time I.
There's a generated a bunch of of you know input out compares with GP4 or something to create what is it? Vecunia or a pack out.
Yeah, so yeah, so that this camelid animals.
So they yeah so they generated essentially the instruction tuning.
Transat from models that have benefited from RLHF and then you kind of get the benefits of RLHF for free right or for $300 and so it seems that actually this works pretty well right so right so maybe I don't know I mean it could be.
I'm just pure speculation but like it that could be the case and RLHF is special it could be that RLHF allows you to optimize for a function that you can't directly optimize for with next word prediction but if you have an RLHF.
RLHF trained model you can then distill that function like the know and then directly something like that.
I want to say that RLHF doesn't even optimize for a different or special or function that like you could just take the data you get from RLHF and just.
Use it differently and like fine tune on it or something.
I mean that must not be the case so I guess I'm really purely speculating but that I think that's the intuition that I'm just like deeply wanting right now is like is there something special going on or is it just.
We found a different way of getting somewhere like we kind of stumbled upon it and we can actually get the same effect.
And it's I feel like it matters it matters from an engineer standpoint but it also matters because I just hear RLHF.
Actually I would like to talk to you about that because you like mentioned it in your grounding paper or like I've heard it from a lot of people.
Like at Federico had a paper on kind of disassociating language and thought and had all these different criticisms of large language models and the things they can't do and then at the end it's like but maybe RLHF solves all of this and I was like whoa that's like a huge.
But I don't know and so and I and I've seen similar things elsewhere and so I think to me I'm like.
I'm just deeply curious if that's the case like I think I'm lacking that intuition.
Yeah, no absolutely I mean that's first of all let's see best transitions through the topic.
Yeah, so nicely done.
Yeah.
So, just to follow from the audience, the, so if we're going to call them we wrote this paper and essentially we said well people use the notion of grounding in different ways in the literature.
And then of this goes back to how not simple grounding problem from the 1990.
90 and in which he argues that symbolic AI models lack the capacity to intrinsically many forms of patients and out this.
Because you know that the semantic interpretation of their representations is provided externally by the programmers so CHRT and you for example it's capable school program that could manipulate locks in the box world.
If you can link can connect its linguistics presentations to virtual objects but that connections provided as I thought can externally by physically programmers.
And the problem is how do we get models that actually he should be grounded.
Presentations of me because he guidance where here the key notion of grounding is referential so how do we get representations that are actually making reference to the the objects of the other worlds that they are about.
And that program has created your marriage with recent connections wills and so on and I need them a lot of with that support back and so we, we were trying to pick about different notions of grounding said the referential national
grounding is the most important one and then say well in light of that, can we say that mangos will also in a text only achieve some form of referential grounding, and we do mention our other chef because we've basically the argument is going from the perhaps the most plausible and convincing
for the most people to something that's a bit more speculative so with our own chef.
So the premise and she is that we, we've next for prediction that's a that's an intro linguistic function right so you could eat in the next word this is trained.
And that doesn't seem to give you quite the, at least intuitively, the right kind of, of normativity for for representations of the world the reference of music items, such that you could have the possibility of misrepresenting something.
So, in other words, you don't have anything to give you the right kind of world involving function is just, you know, whether you write a role about the next talking.
And our chef, on the other hand, because you get this explicit feedback from humans, including feedback about truthfulness honesty, the three h's of our chef is helpfulness, homelessness and honesty right so that is one of these which is a novelive component that's about that's an epistemic in all to
whether you're, if you're, you know, answering questions about capitals of the world, whether you're rather long about the state of the world when you say that Paris is the capital of France.
So that seems to be well involving in the right way, but then, you know, we had this conversation a few weeks back in, and as you rightly pointed out, it seems that, you know, you could get that the right kind of for the public function without this explicit feedback from humans.
And we do actually think that that's the case. So, and going down from slightly less consensual claim, we think that you can get in context learning and in context learning you have a fusion, when you have a fuchsia swamp where you have several examples of successful
problems that you can ask, if it's a future prompt on worldly facts, you also get this implicit feedback in the prompt about what's right of log in the world and if you're seeing projects learning as, you know, inducing a function, optimizing for a function that's not just
the next for prediction, then you can also see that as providing a well developed function, but then you can go even further and say, well, so why not look at pre training and I've told the awesome context windows in there that will include discussions of what the facts where, in order to do the next
for prediction, which is the proximate function, the model might have to induce a more complicated ultimate function that's about the real world. So I think that's where your intuition is.
Yeah, right. Yeah, I think exactly like an, and this is this. Yeah, I love, I mean, I love the paper and I like this idea of the world evolving function because I think that is the, that's kind of the intuition that we all have.
Right, like there does seem to be something that's not like I think most people's gut instinct is like next word prediction over text isn't enough, right. And then the challenge is figuring out like what like where is the line like what is the what actually is the problem with it right and that's
and that's where I'm kind of stuck because I think my like in my heart I kind of don't want language model next word prediction to count like I don't want that to be the whole thing.
And then I just like and a lot of our own work though we seem to be arguing the opposite like I'm not quite sure where I fall on this issue and so it's like yeah you want something like this like this world of our function or something about the learning mechanism right the learning mechanism doesn't
feel like enough you're like no obviously people do more right then predict the thing that happens next but then like formalizing what that difference is I just I haven't been able to convince myself like I haven't been able to come up with a thing that I'm like yeah that's the thing I totally buy it like sometimes it gets on this issue of like goals and I guess this
actually came up a bit in the philosophy of deep learning debate like you know and when I talked to a lot of cognitive scientists and people who don't like the next word patient language while it's like you know people have goals but that feels like a weird because you can say that the language
has a goal and the goal is to predict the next word and like so now are we just like making a judgment about what goals count as good ones and not good ones there's a similar yet like this kind of having the person give the feedback who is sufficiently tied to the world and now it seems
like we've like outsourced the question of whether the model is grounded to something about the trainer and the goals of which maybe actually is fine maybe philosophically that's fine you're like to fall along this causal chain you have to have kind of inherited from somebody who's also on the causal chain so I mean I don't know because like yeah if you had like somebody giving
RLHF feedback who is like intentionally misleading right or just confused and misinformed like any of these types of things like I don't know how that muddies the analysis of whether the model is now world involved or grounded
right
referentially grounded
yeah totally agree on the last point I think if the whole argument hinges also on the assumption that the crowd workers actually know how to run the outputs or do it right to a large extent
right
and there's this and I guess this is a classic philosophical qualm too right like cause we can like humans could be totally wrong about stuff right so like science progresses right so right now we might be seeing
like oh the I forgot one of the classic examples the like a lot of things about like viruses and diseases right like we had theories about what this different diseases were back in the day and then we learned stuff and it's like an entirely new thing now right
but you wouldn't say that the people historically had like an ungrounded meaningless notion of that thing right so we so like you need to account for the fact that we could be teaching the model something that is
ultimately wrong because we haven't learned that in fact you know that I don't I can't yeah but I can't think of a good example
the thing is theories of representations of representation will account for the fact that you can misrepresent things right so just because you have a you know a linguistic like lexical concept that is
not refraction be grounded doesn't mean that this precludes the possibility I have you know something going on right misrepresenting right so like thinking
yeah and I guess this is kind of it's like so if the model thinks that the capital of France is Berlin quote thinks that and produces that as output I guess we need to differentiate between the case where it's just wrong and ungrounded and
learning loose associations versus it was quote mistaken right but like that because I could not know the capital of the city and like that it's true that I don't have quite the right concept of that thing but it's not the same as me being a parrot that's
just spitting out it's right and so I don't think we quite quite right now I don't know how we're drawing that line within the models of them just producing wrong stuff versus
yeah I mean certainly with I think perhaps with things like early Jeff you can draw that line potentially but with retraining I mean and I show you intuition that you know since it seems you know we don't really want to do the line at early
Jeff it just it just seems like the low hunting fruit because that's what's going to come in last people but that's already you know it's already shouldn't become a racial statement to say like not going to do it all trained and
text only can achieve the first one but then we would want to go further like you and but then it's tricky so so here is the interesting that I have and I've been maybe overly impressed by these recent papers that look at income
learning and show that it's this wonderful Google and others that show the same thing essentially that they functionally equivalent to right fine tuning with gradient descent, even though you're not actually addressing the weights and what's
tuning on what what's what's the what's the during gradient descent virtually on is not next for prediction, it's whatever function is connected to the specified by the future, that the future prompt.
And so that really is the thing that can be okay so so so you could get the work involving stuff from that and if you get if you can get it at different time within complex learning, then presumably you could also get it at pre training time.
If you say you have a window context window that's a bunch of capital questions that was a good front.
Yeah, you have some future like stuff in the training data that presumably would allow the model when it's not at the beginning when it's totally random but when it starts learning everything last at some point.
Right, you know, you might not get that.
Yeah, yeah, and I think this is where.
Like I agree with that and I agree with kind of calling out RLHF as possibly the a different point because it's like it's a good thing to ground to as or not ground to to overloading the word ground it's good like data point right to use and then trying to peel away like what is the minimal
thing like what about RLHF gives it that and but like that's exactly that logic you just laid out is what I.
I think I accept right now or feel somewhat forced to accept right based on this because you're like yeah that seems correct.
But then I have to go back to putting on the hat of somebody somebody who does not believe these language models because and I think it is important to point out that when we're talking about language models being grounded or having meaning it's not the same as saying they are.
conscious and intelligent right like but sometimes but that's that's this kind of elephant in the room where it's like where are we going next and so I think when people are looking at.
These language models and they don't want to acknowledge that they can be referentially grounded because that seems like a step along the way to claiming that they are like human level cognition and all of these other ways.
It's so deeply unsatisfying it's like wait no like you've missed the point like now we're saying that just having a few examples during pre training of someone listing off countries who wrote that down.
In you know good faith just listing the capitals of countries that's enough and now the language model counts even though it's just doing next word prediction whether like that seems insane right and so I kind of like I feel like I just go between these two positions of being like.
Like right now based on everything I feel kind of forced to accept like no the language models I would say they're referentially grounded I can't find a good case to make for why they're not.
And at the same time I'm like do do we really want to say that that seems bizarre right like.
But when we get our for our hand forced by this the kind of all of nothing's thinking that you see sometimes in these debates where it's like.
At the high level it's like either the stochastic power so it's like human like.
You know AGI with with.
Understanding consciousness and for just whatever you would be like yeah yeah yeah.
And there's this huge you know.
Space in between possibilities that we could explore where you look at different capacities in the case by case based and say that.
And then within each capacity that grounding.
It's also spectrum right it's not like.
You know we want to say you get a few examples of question answers about capitals and then.
That's it you're you're you have human like referential grinding on everything.
You check that box.
You're good yeah right so.
So presumably you know you could say well you know that that gets you you know your foot on the lighter of a personal grounding in a tiny tiny tiny domain and.
And that's still you know 30 feet.
Interesting yeah right that said I mean there's something that I think is really interesting from your work on.
You know isomorphisms between say in color terms and the color space and stuff you've been working on with grounding that.
Maybe suggests that sometimes when you get a toe hold on grounding in a specific domain.
You could leverage the isomorphism between language in the world to get a little bit more for free.
Right right right right right you definitely could.
Imagine that and I guess this is the project that you and I start working together is the kind of what kind of power of analogy.
Reasoning do these models have because yeah you could imagine something like this like.
With the toe hold and really strong reasoning by analogy capability you could get a lot.
Out of that but I also agree I think there's just this big middle ground like it's not like you could.
Like.
Learning that the.
The meaning of the capital cities or something shouldn't.
Be enough that now by reasoning by analogy you can infer like.
Everything the whole world like that that seems.
I would if somebody could spell out a mechanism via which that would happen sure but I can't imagine what that would be.
Yeah yeah.
I was I was going to make a complete.
Detour but I remembered we didn't talk about the Chomsky stuff when we're talking about.
Yeah so I wanted to hear your thoughts on that.
The thoughts but I would like to hear your thoughts on yeah so.
So just for context but Chomsky.
Steve has been.
Writing about statistical models of language learning for decades but he recently.
Let on the records saying as you would expect that he's totally skeptical you can anything whatsoever from language models about human language acquisition human condition in general.
And he co wrote this op-ed in New York Times making that claim.
Even though he told me he would have he signed on it but would have made a point of all the differently.
Because it goes in different directions I think you know I have some process about how the op-ed is believe no different directions I think his point is more.
His corporate is simpler is just.
You know like with bottles.
And then he makes analogy with with a theory of physics that would say anything goes and.
And that wouldn't be a good theory of physics and he's he's he's very impressed by this particular paper that.
From Bowers colleagues I think that that's a big step.
Learning supposedly possible languages and showing that they have some visual scan.
Learn such languages and things that's that's that's it all months on and forth.
Yeah.
And then there's tip that I don't see.
I wrote this paper.
That's saying, you know, taking the completely opposite stance saying language models refused the whole program and programming linguistics.
And I, I mean, so I started between.
As often in these discussions that's the running thread but I think that there's always room for positions in between the two extremes.
Yes.
So, I think there was this interesting, interesting discussion that talent in others.
In a trader where there are two versions of the cover to the stimulus arguments that is used to justify the claim that there is something like the next year universal grandma in humans.
So there's a strong version that jumps key himself did defend in the past and that's people still defend neighboring very popular grandma.
You know, generally grandma textbooks, which is syntax is just unlearnable from data.
Period. Like no more of the devil will get you to learn to structure because having discovered cursive infinity productive system is not something you can learn from detail.
Interesting.
So, so I think there is a good argument and see it makes a good argument for that being somewhat refuted by language models.
And they discover in so far as they can, you know, there is a lot of work showing that it can induce that structure.
Whether it's, you know, no, no, it's, it's the, it's the verb agreements to get dependencies.
You can even decodes proxies from nations of baritone.
So, so that's a strong graph.
And then there is the more in my mind more reasonable developmental version, which is, well, with children can do this constraint generalizations on certain syntactic phenomena and if they get dependencies from few examples from this improvisate most.
And it seems like this is hard to come for if they don't have, you know, strong enough in that device to make this deductive inferences.
And one way that you could have this kind of device would be to have a nice grammatical stripes.
And there I think the evidence with language models is way less clear and a way more tentative.
There, there is a little bit of, so the problem being that the large number of the giant ones that actually give you for that and from order of orders of magnitude more words than children's children do.
There are a few papers in one from from Jengen colleagues from 231 and there is more recent one for the office.
Training models with between 100 million and sorry, 10 million and 100 billion words, which is what a child would get by the age of six, eight, 10 years old.
And they show that actually you can get a lot of paper with a title like this. It was like even when trained on a normal amount of data language model still replicate.
Yeah. Yeah. And so, and so what one of these papers shows that what you get when you train them, you keep training them on X for words is mostly he mostly pertains to things like common sense knowledge about the world and all that semantic stuff, but not so much the syntax stuff.
Maybe negation would be an exception.
But yeah, that's what I said. Yeah.
And there is this this also this project and I'm very excited to see the results of the baby a length challenge from Alex. Yeah, yeah.
And so this is the same kind of challenge they use this the credit is copies that's copies of child directed speech that's actually recorded from real real life.
And so this is the same kind of child's the kind of word stick here and sentence stick here. And there's a hard version of the challenge where you have to train language model on only 10 million words, I think at an easier version of 100 million words.
And people will submit their contributions and I think the results will be more supportive of the store.
But yeah, I think, I think that kind of projects could, in my opinion, constrain type of this in theoretical linguistics and maybe go against the week version of the progressives to us and say, look, like maybe you don't need
to think that's syntactic in a knowledge.
But even if that's the case, I think something, you know,
that doesn't mean that there is no amount of of in its structure or inductive bias whatsoever because after all, you know, language models are not typically rising they have inductive bias is just almost having the two biases.
And even if these are not language specific, I think that's a kitty front before Chemsky would say we say we need this language specific knowledge.
There is a sense in which the moderates empiricists meets halfway with the moderates nativists to say there is some like there are some inductive biases that not that they're like general that don't in general and then language specific and we can be happy with that so.
Right. Yeah, that's a really, yeah, that's a great point and I think I agree with that like I um yeah I mean I also just in general probably just a general of sciences like the putting something up as a choice between two extremes is like always detrimental right
like and sometimes they can prevent people from working on the problem because they're like afraid of pissing off half of the field.
But yeah, I so I had a, I like this I like one thing I was thinking about or was thought about with reading Steve's responses, again the kind of just him on the mechanistic stuff like to me, it seems like the answer kind of hinges I guess this is a different version of
question but it seems like a lot of it hinges on what the model is doing internally to process these to process the language like whether it represents something that looks like Trump skin syntax or not.
But I guess this is kind of a different question so there's there's the kind of what you were describing which I think is definitely like one of the important ways to be think about this problem from kind of a linguistics perspective which is like take the blank slate language model and say how much data doesn't need to learn because it's definitely a
super significant finding if you could replicate if like a transformer randomly initialized transformer trained on a realistic amount of child directed speech learn syntax like that's a huge finding right.
There's also this kind of like take the giant pre trained language model.
trained on a way more data than human has but then you can kind of use that as the starting point like that's the innate structure that a human has right, which could be a ton of language specific and syntactic structure, and then that's kind of what.
And then from that point on then it's quite efficient it's the problem so that's kind of what my first time I read Steve's papers like well we can't really say because we don't know whether the model has solved the language problem by learning some like nice looking
formal syntactic abstract syntactic structure under the hood or not although I think a lot of data suggests that it has.
But I guess it's it's kind of hard question to answer because you can't do something like you can't then take that as the starting point of a language learner because it's already learned a lot of language like you would want some other way of like inducing that structure, but then stripping out the ability to actually generate
like you would want to like find a way of like pre training the model getting all the innate syntactic structure but then somehow re initializing a large part of it so that.
Child would have to relearn and then use that as to ask whether what the language while has learned is something akin to the innate structure that Chomsky would have wanted I also.
I mean I guess something you could do because you mentioned like learning on learnable languages is like try to transfer a pre trained language model to one of these.
Unlearnable languages because I like I always think of the I've been thinking of the pre trained language models as kind of the like that's whatever is in doubt by evolution or something that's the starting yeah yeah right and so but yeah I recognize that it's kind of impossible to use as a model of a.
Pre linguistic child because it has way too much language to do any interesting experiments on its language learning but you could ask whether you could transfer to one of these unlearnable languages and whether it basically.
Needs to unlearn and relearn as basically like if it can transfer too well to one of these other languages that I guess would be a data point against.
That structure being the kind of structure that kids have born in whereas if it like their models are very powerful but what it might need to do is basically unlearn everything and then relearn the new language from scratch.
And then that would suggest that it's like not yeah it's not actually very learnable to the models once they've been pre trained with this strong bias for other structures.
Yeah although you need to show that humans that this this made of languages are actually unknowable.
And that's how to do because you can try with adults and then it's like an L2 problem like second language but if you want to try with the first language.
If that's how you said it I'm the you know if you think of pre training as like evolution.
And then you could say well whatever is going to be pre training could be the innate structure of Chomsky and yeah so then you would need to actually you know it's an impossible experiment to do you need to be very unethical.
You have to have a child grow up in this.
So because like what is the data that those languages unlearnable is that no language has the features that this language has.
I think so they have this weird yeah I haven't done a deep dive and I think it's very controversial so a lot of people were convinced by that paper.
Chomsky really latched onto that as you know hard evidence that we can learn nothing from language models.
Right.
Yeah I'm speaking out of my domain but I so I would defer to someone who studies this but my I see why it's controversial right because there's like perhaps lots of explanations for wide languages share common features other than those are the only possible features for the human mind to.
Yeah.
Yeah.
Interesting.
Yeah.
And also you know there is some evidence I think you know that some people who are not very changing the mistakes always point to evidence across languages where it seems that some of the features of generative grammar seem to work better for some dominant languages than others that some languages.
Right.
And then the standard move sometimes is to just add some kind of ethnic thing to the German grammar to compensate for that but then it becomes a little less elegant and.
Yeah.
You know.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah I mean I think it's kind of an un.
Transiential point but I guess there isn't as much work on these large language models in other languages right like I mean they're there are multi-lingual models and stuff but that's definitely some data points that would be good to know like we talk about transformers having some they do have inductive biases and things like I don't know how well they work for.
Other languages I've never seen a good controlled experiment because just the quantity and quality of data we have in different languages is like so varied that you just can't say anything when the models are better or worse.
But that definitely seems like from the perspective of asking what do these models mean for the Chomsky and program or universal grammar and stuff like it seems like we need data on how they work in.
Cross-lingually.
Yeah.
There is this work by Stephanie Chan goes from deep mind that we said that how the data distributional properties that probably sort of the trading data influences.
I think in contact starting in transformers but so they look at that we've made up better sense that different distributional distributional which is.
And they find that the typical distributional properties of languages that seem to actually be there is work stream that is you know there's a zip.
Zips will apply across basically every human language and it seems that there is something in common in the distributional properties of all human languages that transformers can watch on.
Yeah.
Yeah.
Interesting.
Interesting.
Yeah.
Yeah.
Luckily there's still lots of stuff to think about like sometimes it's like oh all the problems are being solved these models keep coming on doing all the tests and it's like oh god no there's so many.
Like I think we still have careers ahead of us that's good.
Yeah.
In the way because you know people freak out about the fact that all the big problems now are being tried by industry and before it's taken away from academia but I think all of the a lot of interesting problems actually things you can only do on small models anyway.
Like we can.
Oh yeah totally.
And so in a way there is a nice division of labor.
Yeah I actually yeah I was like I work at Google 20% and I've said a couple times like all my most interesting projects are the ones at Brown because like I actually think the small toy data things like that's where I feel like I'm learning stuff and like it makes sense.
That's how things work like it's so exciting.
So yeah I feel good about academic work on this stuff right now.
With the synoptimistic notes to end on.
Yeah thanks a lot.
This is fun.
This is a lot of fun.
Yeah we killed I guess I have my timer going almost 75 minutes.
It's pretty impressive.
Perfect.
Hi Tim.
Hi Tim.
