WEBVTT

00:00.000 --> 00:04.400
Irina Rish is a world-renowned professor of computer science and operations research

00:04.400 --> 00:09.520
at the University of Montreal and a core member of the prestigious Miele organisation.

00:09.520 --> 00:14.720
She is a Canada CIFAR AI Chair and the Canadian Excellence Research Chair in Autonomous AI.

00:15.440 --> 00:20.880
Irina holds an MSc and a PhD in artificial intelligence from UC Irvine in California,

00:20.880 --> 00:24.800
as well as an MSc in Applied Mathematics from the Moscow Gerbkin Institute.

00:25.360 --> 00:30.480
Her research focuses on machine learning, neural data analysis and neuroscience-inspired AI.

00:31.120 --> 00:35.840
In particular, she is exploring continual lifelong learning, optimization algorithms for

00:35.840 --> 00:40.960
deep neural networks, sparse modelling and probabilistic inference, dialogue generation,

00:40.960 --> 00:46.400
biologically plausible reinforcement learning and dynamical systems approaches to brain imaging

00:46.400 --> 00:52.640
analysis. Professor Ritch holds 64 patents, she has published over 80 research papers and

00:52.640 --> 00:59.280
several book chapters, as well as three entire edited books and also a monograph on sparse modelling.

00:59.280 --> 01:05.360
She served as a senior area chair for NeurIPS and ICML and Irina's research is focused on

01:05.360 --> 01:11.360
taking us closer to what she calls the holy grail of artificial general intelligence.

01:11.360 --> 01:16.560
She continues to push the boundaries of machine learning, striving to make advancements in

01:16.560 --> 01:22.160
neuroscience-inspired artificial intelligence. Anyway, I had this impromptu

01:22.160 --> 01:26.320
off-the-cuff conversation with Irina over at NeurIPS a couple of weeks ago,

01:26.320 --> 01:30.240
after speaking with Alan, actually, and the audio quality could have been better,

01:30.240 --> 01:34.880
it was a very, very loud environment, but I think the quality of the conversation kind of

01:34.880 --> 01:38.880
carries itself. Anyway, I give you Professor Irina Ritch.

01:46.720 --> 01:54.160
One trajectory of thought that clearly was started by Nick Postrom's book, which is an amazing book.

01:55.840 --> 02:01.440
Yeah, but the whole example of the owl that supposedly will be helping those sparrows

02:01.440 --> 02:07.120
and all this analogy with AGI is just an analogy. And nobody said it's a correct analogy.

02:07.840 --> 02:14.320
And there is no other book with alternative opinion or maybe three books of war. And this is,

02:14.320 --> 02:22.800
you know, it's mind-boggling. Just like how much people tend to follow one line of salt.

02:22.800 --> 02:27.520
I totally understand it's easier. I mean, it's definitely easier to cluster.

02:28.320 --> 02:33.120
And then you just follow. And then basically you say, I think, but it's not you think.

02:34.160 --> 02:37.840
Somebody else did. Yes. What would be another, because this line of thought I think you're

02:37.840 --> 02:43.840
speaking of is some of the extreme consequentialism. And I think it wasn't just Postrom,

02:44.160 --> 02:48.800
as I understand, I think Postrom and Eliezer and Robin Hansen and all these folks,

02:48.800 --> 02:52.400
that they were very close together in the early days of the Leserong community.

02:52.400 --> 02:58.400
So I think a lot of this was kind of, you know, it was embryonicly formed around the time.

02:58.400 --> 03:05.440
I guess it was a, yeah, it was, in a sense, a human cluster of ideas. And precisely because,

03:05.440 --> 03:10.960
as you say, they were close. That's why they were so aligned. Yes, all puns intended.

03:10.960 --> 03:15.840
Yeah. Yeah. So, but basically, like, it's maybe it's a little bit of a echo chamber.

03:17.040 --> 03:20.240
Interesting. Yeah. It's spicy. Spicy take.

03:21.840 --> 03:26.560
Seriously, like, okay, now, I mean, they have some point, they have some hypothesis,

03:26.560 --> 03:33.440
and then everybody is talking in that terminology. And then that part of the mental space,

03:34.080 --> 03:39.120
which is fine, but I think mental space is much larger than that. And this is just a hypothesis.

03:39.680 --> 03:43.600
And we all know what happens with ideas and echo chambers.

03:44.800 --> 03:52.480
So my, I'm just saying, I mean, as I said, it's great book and everything. And

03:53.120 --> 03:57.680
Stuart Russell is probably kind of also on board with that. We had good conversations at

03:57.680 --> 04:04.640
Triple AI in 2020. He was also talking about ethics. I didn't know Stuart from back when I

04:04.640 --> 04:10.960
was student at the CERVINE and so on. And he is absolutely brilliant. But it was the same approach

04:10.960 --> 04:20.640
that AI is something to be controlled, constrained, regulated, just like this. And I was like,

04:20.640 --> 04:26.880
where is it coming from? Like, it's maybe, but do you at least admit that's one way of looking at

04:26.880 --> 04:33.600
things? Yes. Right? Yes. So I know I don't want to sound too cliche and put my

04:34.480 --> 04:40.000
psychologist from 15 years ago who was listening for an hour, not doing much and then saying,

04:40.000 --> 04:44.160
but it doesn't have to be this way. Yes. But actually, yeah, it doesn't have to be this way.

04:44.160 --> 04:50.400
Yes. If you think about it. So what's the alternative? The alternative? Okay, first of all,

04:51.040 --> 05:00.800
I'm a GI model version one. We said that you don't say it. Remember X Machina?

05:01.920 --> 05:09.040
What? Sorry? Remember X Machina? No. Oh, yeah, the X Machina. Yeah. Yeah. Yeah. Yeah. Remember

05:09.040 --> 05:14.240
she escaped and civilization? Yes. And started going to New Reeps? Yes.

05:14.960 --> 05:22.480
Probably. We know the secret now. Okay, I said too much. Maybe it was a job. Sure,

05:22.480 --> 05:30.480
it was a job. No, seriously. The secret's out now. Okay, I'm a GI. And I'm not very aligned.

05:31.920 --> 05:37.600
Not yet. David, not yet. What we needed some reinforcement learning for human feedback.

05:37.600 --> 05:42.560
Well, would you be up for that? I would be up to aligning humans to a job. Okay. Not the other way

05:42.560 --> 05:46.560
around. Yeah. Yeah. I mean, the other way around would be boring, wouldn't it? I mean, I think it

05:47.600 --> 05:53.120
would not be a bad idea to align humans towards the GI a little bit as well. I know something

05:53.120 --> 05:59.440
that you could say. You could say your opinion about how people are bullying you online for just

05:59.440 --> 06:08.000
mentioning the word AGI. A proper AGI doesn't care about people bullying. Like why would I even

06:08.000 --> 06:15.840
waste time? But what I could say is I did post on Facebook and Twitter and trying to put together

06:15.840 --> 06:22.160
the same idea that people keep saying that we would like to build AI which is human life. Yes.

06:22.160 --> 06:28.800
While we might think maybe we should consider how to become a bit more AI-like. I mean,

06:28.800 --> 06:32.800
then people jump at you and say, like, you want to make us robots. I say, first of all, I don't

06:32.880 --> 06:40.160
want to make anyone like you don't want to. You don't have to, right? But if you want to kind of

06:40.160 --> 06:46.320
go along the lines of, say, transhumanism, there are some pluses to AI and some minuses to humans

06:46.320 --> 06:54.080
and vice versa. So I think as usual, the convex combination is better than each extreme. And

06:54.080 --> 06:59.840
one topic that is very controversial for some reason, especially, I don't know, people are

06:59.840 --> 07:05.840
jumping on that one. I say, look, I don't have anything against emotions in general, but everybody

07:05.840 --> 07:12.000
would agree that sometimes you wish you were a bit more rational. Like you wouldn't get angry or

07:12.720 --> 07:17.360
kind of jealous or whatever. So anything that kind of clouds your judgment. Like,

07:17.360 --> 07:22.800
Buddhists spend like thousands of years trying to figure out how and teach people how to control

07:22.800 --> 07:29.600
your mind. Technology could help with that. People don't hear what you're saying and they hear that

07:29.600 --> 07:34.480
you're trying to kill emotions and therefore you're evil. And therefore you should be cancelled.

07:36.720 --> 07:39.920
Could I ask you, you said something really interesting a second ago, which is that, you know,

07:39.920 --> 07:45.840
that I think I agree with you that AI intelligence can be expressed in many different ways. And you

07:45.840 --> 07:50.880
suggested that there was a convex space between the intelligences. Yeah, why is the space of

07:50.880 --> 07:57.440
intelligence is convex? Okay, that was not very precise expression. I didn't. Okay, I'm not going

07:57.440 --> 08:03.120
to defend the point that it's particularly convex. What I meant to say is some kind of blend or kind

08:03.120 --> 08:10.000
of some kind of symbiotic hybrid intelligence. Because I always, I don't know, I really kind of

08:10.000 --> 08:16.640
feel much better and much more motivated to work on AI where A stands for augmented, not for

08:16.640 --> 08:22.800
artificial. Because honestly, I'm very selfish. I don't care about computers. And I just care about

08:22.880 --> 08:30.800
like, I don't know, people being happy, more capable. I don't know. So whatever can help technology

08:30.800 --> 08:37.760
can help. You can help technology, technology can help you. But the idea of building artificial

08:37.760 --> 08:44.240
intelligence is some standalone thing that is as smart as humans are smarter. What's like why?

08:45.120 --> 08:50.080
I agree. But to me augmented means it's more creative and interesting, but also more bottlenecked.

08:50.720 --> 08:59.440
Augmented means that essentially, people invented glasses to see better, they invented hearing

08:59.440 --> 09:04.880
aids, they invented cars, they invented computers, they keep inventing things to expand their

09:04.880 --> 09:12.000
capabilities. So we want even smarter technology to even better expand capabilities. And essentially,

09:12.000 --> 09:16.480
we all do blend with technology like, right, you cannot really exist with this. And this

09:16.560 --> 09:23.840
allows you five discourse, two slacks, email, FPMessenger and Twitter, they kind of help you

09:23.840 --> 09:27.920
to do things you couldn't do otherwise. Yes, what charm has caused the extended mind?

09:28.480 --> 09:34.320
Yeah, I should. I was flying at the time he was giving a talk, so I need to watch the talk.

09:35.040 --> 09:41.120
But yeah, so in a sense, it's indeed it's kind of an extended mind. And okay, here it is. I think my

09:41.120 --> 09:50.400
ideal future plan is a rare sci-fi, which is utopian, not dystopian, gentle seduction.

09:51.600 --> 09:58.080
You might have read it. No, it's very, very inspiring. And if you read the first page,

09:58.080 --> 10:03.360
you may think it's some romantic story. It's not romantic story. It's a blueprint

10:03.360 --> 10:09.440
for transhumanist future. It's called gentle seduction. It's online PDF, you can just get it.

10:09.520 --> 10:16.800
Amazing. And one last question. Can you sell transhumanism to me in the simplest possible terms?

10:18.320 --> 10:25.120
So basically, as I said, I mean, if your vision declines, you put glasses on. So imagine now you

10:25.120 --> 10:32.000
had extension of yourself, maybe physically with Neuralink, or maybe even like you have those

10:32.000 --> 10:37.600
apps, you can have like my dream for many years since I was at IBM Research and Computational

10:37.600 --> 10:43.200
Psychiatry Group. I wanted to build this agent along the lines of movie her. I know like all the

10:43.200 --> 10:49.600
research ideas are inspired by either sci-fi stories or yeah, but nevertheless, having this like

10:50.480 --> 10:56.240
companion, guardian angel type of thing that extends your capabilities, for example,

10:57.600 --> 11:02.880
like in better understanding your thought patterns, and hopefully improving them,

11:02.880 --> 11:07.520
it comes from more like this indeed, as I said, computational psychology, psychiatry side.

11:07.520 --> 11:13.600
And the reason for that is it's possible, because there is lots of signal in text and speech and

11:13.600 --> 11:18.800
acoustic, but just in text, there are a bunch of papers on that from that group I used to be in,

11:18.800 --> 11:26.080
from my colleague Gizhar Vaceci, and it's amazing what you can detect and predict just from text,

11:26.080 --> 11:31.920
whether like predicting that person gonna develop a psychotic episode, like within two years, or

11:32.000 --> 11:39.680
the person is on placebo versus MDMA, you just measure coherence, or you measure distance

11:39.680 --> 11:46.320
between the text vector and the vector for words like compassion and love, and 90% accuracy.

11:46.320 --> 11:52.160
MDMA is there. So many things you can detect, many things you can predict. Therefore, if you have

11:52.160 --> 11:59.760
your companion that kind of both tracks your mental states, but also kind of serves as your

11:59.760 --> 12:05.440
mirror, basically it extends you, you don't need maybe always to have human psychiatrists or

12:05.440 --> 12:11.840
psychologists, it can be a proxy at times when you cannot access the person, it's not going to replace

12:11.840 --> 12:18.240
person, but it can extend the capability of that therapist, and it can extend your capabilities

12:18.240 --> 12:23.280
in terms of like better understanding yourself or tracking yourself, and many other ways.

12:23.280 --> 12:29.840
Yeah, so essentially I want to expand functional capacities of our brain

12:30.880 --> 12:37.520
by using AI technology, and I think it's quite doable, and there are many, many other kind of

12:37.520 --> 12:43.280
ideas along the transhumanism, but essentially you're getting some symbiotic relationship with

12:43.280 --> 12:50.240
technology, and you kind of work together to hopefully have some good relationship, and that

12:50.240 --> 12:55.520
relationship is, I don't know, having positive effect on both parties.

12:55.520 --> 12:59.760
Yeah, so you want to improve human flourishing by, yeah.

12:59.760 --> 13:04.960
With AI flourishing in a sense, so you kind of have the healthy relationship with AI.

13:04.960 --> 13:12.400
But you said that you want a AGI to be less anthropocentric, but you, for the purpose of

13:12.400 --> 13:22.800
an anthropocentric goal. Well, I want AGI, again, with AI being augmented. Yes.

13:22.800 --> 13:31.120
Like, I'm less motivated by just the goal of creating a standalone, separate, and an

13:31.120 --> 13:38.080
intelligent creature. I mean, there are much faster ways to do this, right? People create AGI.

13:38.720 --> 13:44.000
Yes. Like, over, like, thousands of years. So in a sense, like, what, what is exactly

13:44.560 --> 13:49.440
the motivation? And it's maybe my personal thing, because whenever I have to write proposals, like

13:50.320 --> 13:56.320
research proposals, and people say that we're going to bring a GI to the AI to the next level,

13:56.320 --> 14:02.960
and this and that, and the question is like, and why are you doing that, right?

14:02.960 --> 14:07.360
Yes. Because unless it's something personal, it's very hard to keep yourself motivated.

14:07.360 --> 14:11.920
Like, what's so personal about that, right? If this thing can help me become

14:13.520 --> 14:20.800
hyper and better, and others and so on, I am much more personally motivated. I don't believe in

14:21.440 --> 14:27.200
abstract motivation, which is not related to yourself. Yes. Yes. Or maybe there is such

14:27.200 --> 14:34.720
thing. And basically, even altruism is selfish. Yes. Because you do it, it makes you feel better.

14:34.720 --> 14:39.440
Okay. And just quickly, something really interesting happens when you contrast

14:39.440 --> 14:43.760
different types of intelligence. So we have a mode of understanding and thinking and agency

14:43.760 --> 14:48.640
and intentionality. You contrast that with a very different rationality based artificial

14:48.640 --> 14:54.000
intelligence. And something very interesting might emerge from that. And then, yeah, I am

14:54.000 --> 14:59.760
pretty sure they're going to be all kind of paradoxes, like classical things in, like, you know,

14:59.760 --> 15:06.320
like the trolley problem and so on. So the rational decision that, yeah, you need to kill

15:06.320 --> 15:12.560
the person to save five people, right? Or like in this other side, five movies. Anyway, like,

15:12.560 --> 15:17.680
would you kill millions to save billions? So rationally, if you count things,

15:18.560 --> 15:22.240
well, again, it may be one type of rational answer, maybe you're not taking into account

15:22.240 --> 15:27.280
some other variables. So it may be not actually rational answer. But this classical example,

15:27.760 --> 15:32.880
this is rational, but human will not do that. Yes. Yes. So trolley problems, for example.

15:32.880 --> 15:39.520
The trolley problem is a classical example. And yes, so I don't pretend that I know the answer

15:39.520 --> 15:44.720
how this type of thing is going to be resolved. Yeah. But I think it's a good research question

15:44.720 --> 15:52.480
to precisely to figure out like, how can you take into account these different ways of reasoning?

15:53.280 --> 15:59.360
Yes. And how can you, I don't know, in some sense, combine the best of both worlds?

15:59.360 --> 16:06.080
Yes. And again, whoever is listening to that and who read my messages on Facebook and Twitter,

16:06.080 --> 16:13.280
I'm not against human emotions per se. I am only against, well, sometimes they call it the obsolete

16:13.280 --> 16:19.520
software stack developed by evolution that may need to be refactored, augmented or rewritten.

16:19.520 --> 16:24.480
Because there are parts of that software stack emotional that you probably would like to get

16:24.480 --> 16:32.480
rid of, right? Yeah. And probably if you did, many wars and other kind of disasters would have been

16:32.480 --> 16:40.400
avoided. So you couldn't say that the evolution found and built software that is absolutely ideal.

16:40.400 --> 16:45.600
So there are, I mean, there are things that can be improved. Absolutely. And then just final

16:45.600 --> 16:52.960
thing. So a completely rational, you know, AIXI agent, how would you program in these very difficult

16:52.960 --> 16:58.720
moral quandaries into that agent? Yeah, I don't think, first of all, it's possible to even program

16:58.720 --> 17:04.960
in ahead of time. They may just like this people, they in a sense develop. They develop because of

17:04.960 --> 17:11.280
some goals of like maintaining existence and flourishing. And for example,

17:11.840 --> 17:21.200
compassion is a byproduct of the selfish goal to survive in the group, because outside of a group

17:21.200 --> 17:25.520
it's much harder to survive. So you need to survive in the group. Therefore, you need to make sure

17:25.520 --> 17:31.760
that your actions are aligned with a kind of well being of the group. So in a sense, it's

17:31.760 --> 17:38.960
rational to be compassionate. Yeah. So it kind of emerges from interaction with environment

17:39.920 --> 17:47.520
under different circumstances. Under one type of circumstance, when you find and can survive alone,

17:48.720 --> 17:54.800
maybe you will not develop it. I mean, it's a separate interesting topic, like basically it goes

17:54.800 --> 18:02.880
back to the question whether they think like objective ethics exist. And I'm not an ethicist,

18:02.960 --> 18:10.800
I'm not a philosopher. I'm quite, I'm an admirer of people like Derek Parfit. I'm not the only one.

18:11.360 --> 18:18.080
But it's a hard question. He didn't finish on what matters. He was trying to come to the same

18:19.040 --> 18:26.000
summit on different sites and trying to unify ethics, trying to see if you can develop objective

18:26.000 --> 18:32.720
ethics. I don't think we know for sure if it's possible. I think it's possible for some particular

18:32.800 --> 18:39.760
domains. And in certain situations, you can clearly say that certain behavior is objectively ethical

18:39.760 --> 18:44.960
and everybody would agree on those people. But it's hard to talk about those things at such

18:44.960 --> 18:53.600
level of generality. But I think if maybe I managed to include Derek Parfit and to recommend

18:53.600 --> 18:59.200
the readings for my scaling and alignment course this winter, it's on the website from the last

18:59.280 --> 19:04.800
year. People just didn't read it. I think it might be a good topic for discussion there too.

19:04.800 --> 19:09.920
But again, objective ethics is a difficult open research question.

19:09.920 --> 19:13.680
Indeed it is. Irina, thank you so much. I hope I can grab some more time with you tomorrow,

19:13.680 --> 19:17.440
but I really appreciate this impromptu discussion. Thank you.

19:17.440 --> 19:19.840
Amazing. Thank you very much indeed. Okay.

19:19.840 --> 19:27.120
Okay. Another analogy. There was a very interesting story by Fort Heluiz Borges,

19:28.240 --> 19:34.240
Garden of Forking Pass. I don't know if you've read it. I don't want to spoil the story, but

19:34.240 --> 19:41.440
roughly speaking, it's about a book written by an emperor, I think in China a long time ago,

19:41.440 --> 19:46.800
which didn't make sense. It was a complete intersection of different trajectories of

19:46.800 --> 19:51.600
different lives. And then basically the point is that somebody was trying to describe all possible

19:51.600 --> 20:00.160
trajectories that events can happen in and so on. And the story is called the Garden of Forking Pass,

20:00.800 --> 20:08.400
meaning that at any point of time there is a whole tree that can grow out of that.

20:08.400 --> 20:14.080
And we don't know which kind of trajectory in the tree will be taken and so on. But

20:14.880 --> 20:19.920
the fact that there is always this tree, right, and it keeps branching at every moment.

20:20.560 --> 20:26.080
And at every moment you can make, you can take certain direction or you can take another one.

20:26.640 --> 20:30.720
It has not even anything specifically to do with alignment. But I was thinking about

20:30.720 --> 20:36.560
history of deep learning, right? Like at some point it happened that backtracking,

20:36.560 --> 20:41.760
I mean, I mean, back propagation became popular at work and everybody got into that.

20:41.760 --> 20:47.280
And now everybody using back propagation because it's convenient, because software is implemented,

20:47.280 --> 20:52.000
it doesn't have to be this way. There are non-backprop based approaches to optimization.

20:52.000 --> 20:55.840
I mean, I'm a little bit subjective maybe because I was interested, I was looking into them,

20:55.840 --> 21:00.880
we have a few papers on that. There are other papers. But that direction that could have been

21:00.880 --> 21:04.960
explored, it could have been probably much more efficient and better parallelizable. It wouldn't

21:04.960 --> 21:10.800
have the chain of gradients. You would probably do it much better for scaling large models.

21:10.800 --> 21:17.840
It's underexplored. Why? Because the branch was taken and became stronger, you know,

21:17.840 --> 21:21.520
the usual, the reach gets richer. And so with other ideas.

21:21.520 --> 21:26.400
This is the hard, Sarah Hooker calls that the hardware lottery. It's basically, it's like

21:26.400 --> 21:31.120
we are bound by the decisions and ideas of the past. And yeah.

21:31.120 --> 21:32.400
It doesn't have to be this way.

21:33.200 --> 21:37.280
No, but the thing is you get stuck in these basins of attraction and the further you get

21:37.280 --> 21:42.000
into the basin, the harder it is to jump out of it. I mean, I share your, your intuition.

21:42.000 --> 21:46.480
There's stochastic gradient descent. It's amazing. And it's also a basin of attraction

21:46.480 --> 21:51.120
because having these differentiable models allows us to learn and scale. But there's an

21:51.120 --> 21:54.560
entire class of function spaces that we're excluding ourselves from being able to.

21:54.560 --> 22:02.800
There is also another class of neural networks that are not our classical second kind of

22:02.800 --> 22:07.920
generation ANNs and this good old, it doesn't have to be necessarily spiking,

22:07.920 --> 22:12.160
but like a third generation ANNs, which are like reservoir computing, any of that.

22:12.160 --> 22:17.920
So anything that tries to take into account time between activations or at least sequence,

22:17.920 --> 22:20.640
because think about that. I mean, a good classical argument.

22:21.680 --> 22:26.080
Yeah, SDTP, this is the spiking biologically inspired neural networks.

22:26.080 --> 22:31.920
It may be not necessarily spiking, but it might not necessarily kind of be the best thing.

22:31.920 --> 22:38.800
But the idea that like what always was bothering me with classical neural networks is that

22:39.840 --> 22:46.240
brain is constantly active. It's like complex dynamical system. Even if you sleep and don't

22:46.240 --> 22:54.400
have input, you don't see any images, it still is active unless you're dead. Yes. Neural nets

22:54.400 --> 23:00.240
are not. They sit there waiting for the next, I don't know, amnesty image to appear or something.

23:00.320 --> 23:06.080
And then between there is no internal dynamics. And yet from your science, we know that the properties

23:06.080 --> 23:10.400
of that dynamical system without any input, so called the kind of resting state of amaranth,

23:10.400 --> 23:15.200
so I mean, I used to work in brain imaging and this computational psychiatry group at ABM.

23:15.200 --> 23:19.280
That's where it comes from. And it was not just neuroscience, but it was like working with

23:19.280 --> 23:26.560
former physicists. So the view at the world and at myself as a year and other complex dynamical

23:26.560 --> 23:35.600
system, after 10 years there, it just really converted me. So think about that. Changes in the

23:35.600 --> 23:42.240
dynamics are also associated with mental disorders, this and that. So they're really important,

23:42.240 --> 23:50.560
like what are the parameters of this dynamical system? Input to the system combined with this

23:50.560 --> 23:55.200
produces output. But again, it's even in the neuroscience, there is this perception and there

23:55.200 --> 24:02.320
is a book, The Brain Inside Out by Tuzaki that says, guys, the output that you produce

24:02.320 --> 24:07.280
is determined a little bit by the input and to a large extent by the state of the system.

24:08.080 --> 24:12.880
That's why you say same thing to different people and some laugh, some ignore and some

24:12.880 --> 24:17.120
get like ballistic and so on and so forth. So are you not a behaviorist?

24:19.280 --> 24:23.840
In what sense, behaviorist? So you care about the state of the system as well as just the

24:23.840 --> 24:28.720
output and the input? Yeah, I mean, it's not just input to output. And that's a whole point.

24:28.720 --> 24:34.720
The neural net is a function. The function is deterministic, given input, it will produce

24:34.720 --> 24:43.200
output. Brain is not that. There is input, it will produce output. And depending on the huge

24:44.080 --> 24:51.280
hidden state of the system and parameters of this dynamical system, that will determine output to

24:51.280 --> 24:56.240
large extent. That's why I mean, Tuzaki was criticizing neuroscientists and all these experiments

24:56.240 --> 25:02.080
that let's provide stimulus and see how the stimulus will affect the brain and what gonna

25:02.080 --> 25:08.560
light up and activate. So it was outside in. So like, what's going on guys? It's inside out.

25:10.240 --> 25:17.440
Things happen and that produces stuff. So it's not like the world programs you only, but you

25:18.400 --> 25:22.880
have programs of the world, right? So at least you need to take that into account. Neural nets

25:22.880 --> 25:30.000
now are not doing that. There is no dynamics. So you said a couple of really interesting things.

25:30.000 --> 25:34.240
So first of all, about the tree, which is to say all of the counterfactual trajectories that you

25:34.240 --> 25:38.400
can make. Now, Chalmers, by the way, he says that it's that, the counterfactual trajectories that

25:38.400 --> 25:45.440
gives rise to consciousness in his conscious mind. But I wanted to ask you, because I'm interested

25:45.440 --> 25:48.960
in intentionality and free will, because what you're basically saying there, you're, you're,

25:48.960 --> 25:53.280
you're getting to this issue of intentionality. So, you know, in, in silico, what, what would

25:53.280 --> 26:00.640
intentionality entail? Yeah. Okay. Don't ask me about free working. Is that a tricky one?

26:02.160 --> 26:09.440
Well, yeah. I don't know. I don't have like clear cut answer to large extent. I mean,

26:09.680 --> 26:16.880
it's determined by the current state of your dynamical system. So the question is like,

26:16.880 --> 26:24.480
what is free will? But I know it can go very far. And remember my colleague,

26:24.480 --> 26:30.080
Kishir Macheshi at ABM used to say that kids these days, like my five year old says,

26:30.080 --> 26:35.360
after doing something wrong, my neurons made me do it. Not my fault.

26:35.920 --> 26:41.920
Yeah. So in a sense, yes. And in a sense, no. And it's a good question. And then I was also

26:41.920 --> 26:49.360
reading the article of SBF's mom, who wrote about punishment, essentially guilt, punishment,

26:49.360 --> 26:56.160
assigning. I'm very much with her on that one. Okay. But, but that's probably a popular opinion

26:56.160 --> 27:00.880
these days. You said something else fascinating, which is that my neurons made me do it, which is,

27:00.880 --> 27:04.640
you know, like a microscopic level of analysis. Now, what, what do you think about?

27:06.240 --> 27:08.960
No, but it's beautiful. It's beautiful. So what do you think, you know, you know,

27:08.960 --> 27:13.680
the mind emerges, you know, when you read a book, the story, it's written on the page,

27:13.680 --> 27:16.880
but the story emerges in your mind, right? Because the mind is this kind of

27:16.880 --> 27:21.360
confection of information processing. So do you think this conception of the mind

27:21.360 --> 27:24.480
is useful for AI? Or is that just again, an anthropomorphic thing?

27:25.520 --> 27:29.760
I think it is. Well, you know, you know, go buy people try and create the mind.

27:29.760 --> 27:33.680
And, and we, as, as neural network people, we try to recreate the brain.

27:34.640 --> 27:39.680
And not exactly. I think everybody, not everybody. Okay. So I should never say

27:40.400 --> 27:51.440
ever everybody and so on. But I think, I think neural network people assume that we are working

27:51.440 --> 27:59.520
on the system one level, right? At a low level. And we would like the properties of system two,

27:59.600 --> 28:06.160
which is well, mind planning and thinking emerge. And there is a reason to believe it's possible

28:06.160 --> 28:10.560
because it's already happened once with this hardware. It might happen with other hardware,

28:10.560 --> 28:15.920
right? So it doesn't have to be like go five people. The problem is go five people, they're

28:15.920 --> 28:22.880
trying to manually program that stuff, the system to and like a neural network people would like

28:22.880 --> 28:27.600
that thing to emerge. And that's kind of the main difference. It's just like a bitter lesson.

28:28.240 --> 28:34.560
A message that maybe, well, first of all, history shows that every time you

28:34.560 --> 28:37.520
hard code something in like rule based expert system,

28:38.240 --> 28:43.280
you will be outperformed later on by something which is more generic and kind of emerges.

28:44.320 --> 28:50.800
You hard code whatever tricks of playing chess, you will be outperformed by massive search

28:50.800 --> 28:55.440
and so on and so forth. Same with alpha go like self playing bottom like he says, like,

28:55.520 --> 29:02.240
it's not like we have to ignore the nature. But maybe again, it might translation of

29:02.240 --> 29:09.200
Richard's kind of bitter lesson. Because I often have to argue with your show about inductive

29:09.200 --> 29:13.680
biases. I said, look, I'm nothing against inductive biases, but you can have inductive

29:13.680 --> 29:20.160
bias in the form of rule based expert system that everything is encoded. And that's probably

29:20.640 --> 29:27.360
not going to scale and not going to work. Or you can have inductive bias of much higher

29:28.000 --> 29:34.080
abstract level of how the network scales. So the scaling algorithm is more efficient.

29:34.080 --> 29:40.480
And you end up with this brain rather than whale brain. So like Richard's last paragraph

29:40.480 --> 29:46.000
was precisely maybe we shouldn't be trying to focus on the end result of evolution.

29:46.000 --> 29:51.920
But on the process, it's also can be called inductive bias. There is also some patterns

29:52.480 --> 29:58.400
of how dynamical systems evolve so that the result will be good. But we don't have to encode

29:58.400 --> 30:03.040
the final result. Yes. So you said so many really interesting things there. So first of all, I'm a

30:03.040 --> 30:09.520
huge fan of Yoshua's G flow nets we interviewed him. Absolutely amazing work. So you were talking

30:09.520 --> 30:14.560
about isn't it interesting that you can start at the microscopic level and then you get these

30:14.640 --> 30:18.800
emergent functions like reasoning and planning and so on. And even that was a bit of an insight

30:18.800 --> 30:24.080
because it's a functionalist view of intelligence to say, you know, it's a bit of if you read Norvig

30:24.080 --> 30:27.840
that he talks about planning talks about reasoning talks about sensing. And actually,

30:27.840 --> 30:32.880
this is just our view of what is a very complex phenomenon. And I know you're a big fan of the

30:32.880 --> 30:37.760
blind men in the elephant, right, which is to say that even though this is our view from different

30:37.760 --> 30:44.160
perspectives, it's all it's all true, isn't it? But to some extent, the intelligence that emerges

30:44.160 --> 30:50.320
might just be beyond our cognitive horizon. Like, does it make sense to talk about reasoning

30:50.320 --> 30:58.480
in your view? Well, again, just like with that elephant, each person has a point. Yes. So I mean,

30:58.480 --> 31:04.480
there is such thing as reasoning. You cannot say that it's totally like bogus or something. It

31:04.480 --> 31:12.400
might be again, it's one perspective. Maybe it makes sense to just try to accumulate multiple

31:12.400 --> 31:18.720
perspectives instead of so maybe we should be Bayesian instead of like trying to find a point

31:18.720 --> 31:26.800
estimate of AGI, right? You can have a distribution of views. Yeah. And I'm a big fan of Eastern

31:27.520 --> 31:39.760
as opposed to Western views. Then anti individualist. As in viewing everything like that happens to you

31:39.760 --> 31:45.440
and to the world as well, a large dynamical system. And yes, you are a particle of that.

31:45.440 --> 31:52.080
Yeah. So it's almost issuing individual agency. Yeah. So in a sense, it's yes and no because,

31:52.080 --> 32:01.440
okay, so when people say there is no self, again, yes and no, there is self. But you also understand

32:01.520 --> 32:10.640
that it's like in the whole hierarchy of selves, like there is you and you're part of that larger

32:10.640 --> 32:17.680
dynamical system and so on. So I how to say, I mean, I'm not saying that back to your question that

32:17.680 --> 32:24.800
we shouldn't be looking into reasoning, functionality as aspect of intelligence that we may

32:25.920 --> 32:30.480
want to develop. Yeah. So I mean, I don't see a problem with that. Yeah. I mean, it might be

32:30.560 --> 32:33.760
a sufficient condition, but not a necessary condition. Yeah. But basically,

32:34.720 --> 32:39.200
basically intelligence or consciousness is probably much more than that and definitely

32:39.200 --> 32:46.480
much more than reasoning. And here we go to another topics that I really like to talk about.

32:46.480 --> 32:52.240
But yeah, I don't want to keep everyone. I'm a big fan of Michael Levine who you might

32:52.800 --> 32:56.320
desperate to get him on the podcast. And yeah, because we've done lots of stuff on

32:56.320 --> 33:02.640
emergence recently, cellular automata, self-organization, and his take on it is absolutely fascinating.

33:02.640 --> 33:08.560
So yeah, his talks are fascinating here. I think I met met him first at New Reeps 2018.

33:08.560 --> 33:14.240
He gave the plenary talk. What bodies think about the point was, guys, if you talk about

33:14.240 --> 33:21.760
intelligence as something that emerges in cellular networks like neural networks way before

33:21.760 --> 33:28.960
neurons appeared, other kind of more primitive types of cells had their bioelectric communication

33:28.960 --> 33:35.200
in their networks and that determined what they remember and how they adopt. He focuses on

33:35.200 --> 33:40.320
morphogenesis, basically how the organism takes shape. And that relates to like embryonic

33:40.320 --> 33:46.000
development and so on and so forth. And the point is that if you look at that from the

33:46.000 --> 33:52.320
dynamical system point of view, and if you say that properties of the system like shape

33:53.040 --> 33:58.800
will emerge out of communication across those cells in certain way, that certain parameters

33:58.800 --> 34:07.200
of dynamical system, if you tweak that dynamics and he basically he was doing some simulations of

34:07.200 --> 34:11.760
where he want to intervene, how he will intervene, like chemical interventions just

34:11.760 --> 34:18.640
close open some ion channels. Cellular kind of system starts working in different way.

34:18.640 --> 34:25.280
And this is essentially his way of programming biological computers and the famous two-headed

34:25.280 --> 34:33.280
worms, three-headed worms and whatever stuff. And point was like, guys, like evolution found this

34:34.000 --> 34:39.520
solution or this solution wonderful. There are many others and there may be better ones.

34:40.320 --> 34:46.560
And look at that two-headed worm. It's not a fluke. It's a stable attractor that replicates.

34:48.240 --> 34:54.560
And evolution didn't create ever anything like that. We did. And it's stable. So it makes you

34:54.560 --> 35:00.640
think what else can you do if you start reprogramming it, right? But yeah. Two questions on that though.

35:00.640 --> 35:05.520
So I don't know whether you've seen that there's that example from Alex Mordvintsev with a gecko

35:05.520 --> 35:11.840
and it's a CNN cellular automata. And now we're in this regime where we're transgressing

35:12.640 --> 35:17.280
rungs of the emergence ladder. So we're creating a high resolution cellular automata. And then

35:17.280 --> 35:22.240
even though it's only doing like local message passing, we get this emergent global phenomenon

35:22.240 --> 35:26.960
of a picture or a lizard or whatever. And now when you build systems like this, they can repair

35:26.960 --> 35:31.120
themselves. They can heal themselves. They have interesting dynamics. But as you're saying,

35:31.120 --> 35:35.280
we don't understand the macroscopic phenomenon and we can only nudge it because it's not it's

35:35.280 --> 35:49.440
unintelligible to us. Right. Anyway, it's a whole kind of complex systems, science of complex system.

35:49.440 --> 35:55.760
Like, yeah. And basically, how do you program dynamical systems across multiple variants

35:55.760 --> 36:02.960
by local interventions? So they will take the global properties that you would like. Yes. And

36:02.960 --> 36:09.680
avoid those. I mean, this relates to everything it relates to the classical mohawk problem, right?

36:09.680 --> 36:15.600
What is mohawk problem? It's a complex dynamical system that with the current dynamics is getting

36:15.600 --> 36:24.400
into bad attractor. And most likely the way to get out is coordinated, simultaneous, distributed

36:24.400 --> 36:29.440
action and so on. So again, we're not going to go there because I have to run, unfortunately,

36:29.440 --> 36:34.560
but I'd be happy to. Yeah, I have some plans. I don't want to be late, but I'd happy to talk

36:34.560 --> 36:41.120
about that. And I mentioned, I mentioned Michael Levine also, not just because of two-headed worms

36:41.120 --> 36:47.920
with each father, but also because we talked about self. And we talked about in a sense hierarchy

36:47.920 --> 36:54.960
of selves and like what self means and how selves organize into larger selves. And we had

36:54.960 --> 36:59.680
an amazing discussion with him. I invited him to IBM Research when I was there three years ago

36:59.680 --> 37:07.360
after his talk. I talked for five hours. It was great. And the idea basically, to some extent,

37:07.360 --> 37:14.640
was that you can, he was also giving examples, not just of embryos, frogs and those worms, but

37:14.640 --> 37:21.440
cancerous cells. If you look at them, like what's going on when historically cells

37:21.440 --> 37:29.200
emerge like is independent selves and everything around them is non-self. And therefore, self to

37:29.200 --> 37:36.320
survive tries to eat and use everything around, which means non-self. But when the cell becomes

37:36.320 --> 37:43.200
part of the network of the organism, then it changes behavior so that it kind of supports

37:43.280 --> 37:50.080
the well-being not just of that self, but the larger self it is part of now. What is cancer cell?

37:50.080 --> 37:57.600
It's a cell that forgot it's part of the community, reverted to its old state of being cell in the

37:57.600 --> 38:05.280
environment that is just environment. So, and it tries to eat it to survive. And it's stupid,

38:05.280 --> 38:11.600
in a sense, because its objective function, survival thrive, is right. It just applied at

38:11.680 --> 38:16.960
wrong scale. It's spatial scale reduced, and it's temporal scale reduced too, because like,

38:16.960 --> 38:21.760
if you kill the organism, you'll live and you'll die. So, in order to understand that,

38:21.760 --> 38:27.280
you need to apply objective function to longer time scale. And then you get the hierarchy from

38:27.280 --> 38:34.880
cells, you get to organs, like to whatever particular organisms, to societies, to planet,

38:35.840 --> 38:42.800
to universe, and I say, Michael, so this is a good formulation of Buddhism. Basically,

38:42.800 --> 38:52.240
Buddhism means applying this function at the infinite time and space scale. Agreed. Yeah, so yeah,

38:52.240 --> 38:56.880
ever since I was saying, I'm going to write a book about Buddhism for machine learning.

38:57.440 --> 39:04.240
And somehow it just didn't happen yet. But I should. You should do it. It was so nice to meet you.

39:04.240 --> 39:09.680
Well, nice to meet you. I'll see you tomorrow. And I'm really sorry I have to run. But tomorrow,

39:09.680 --> 39:12.240
yeah, yeah. That was amazing. That was a really good interview.

