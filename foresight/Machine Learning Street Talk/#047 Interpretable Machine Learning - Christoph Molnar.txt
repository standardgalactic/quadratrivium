Coming up later in today's presentation, I'm wondering at what point we're just developing
complex math models to explain complex math models and we really haven't, you know,
made much progress along the interpretability axis. So you have something you don't understand
and you explain it with something you don't understand? If I have if I have some general
formula, just some very general formula, and then I go in there and I go, you know what,
this formula has five parameters. And if I make this 1.75 and that one one-third and this one two
and that one zero, and I call this the Megatron activation potential, and I go and write a paper
about it, that's really just an arbitrary selection of a bunch of numbers. And then you gave it a
fancy mathematical passport and you got it published in some journal. And now everybody
has to memorize that as you know, the Megatron potential and kind of learn about it. And that's
a lot of what's going on right now is that it's really just a bunch of hacking.
Welcome back to Street Talk. Today, we're going to be talking about interpretable machine learning.
Enjoy. Interpretability has become one of the most important topics in machine learning.
And it's something that every data scientist needs to be familiar with. For hundreds of years,
we've had simple interpretable models like linear regression and rules-based systems.
But in recent years, there's obviously been a huge rise in more complex, bigger, nonlinear models.
And of course, predictions from these models are not always so easy to explain. So as we start to use
these more powerful, nonlinear models to actually make decisions on real world matters, then it's
inevitable that our attention must now turn to interpretability and explainability. When I
first started learning about machine learning algorithms, I was told they could be dangerous.
They were hard to understand. They were black boxes. But as Christoph lays out, it turns out
there is a whole plethora of techniques out there to explain why a model made a certain
prediction. Some models like low-dimensional linear regression are intrinsically interpretable.
You can just look at the model coefficients and that tells you exactly how the model is working
under the hood. Then there is a whole suite of methods that will actually work with any ML model.
Like training a local surrogate or a global surrogate. There's also Shapley values,
which is a really cool technique that allows you to distribute blame for the prediction amongst
the input features in a really theoretically sound, really principled way. And then there are domain
specific methods. For example, to explain image models, you can try to highlight the most relevant
parts of an input image by making saliency maps. And there's more. You can look at things like
example-based explanations where you try to find the smallest change in the input data that would
cause the output prediction to change. So maybe with this awesome new interpretability toolkit,
we can start to dispel that myth that machine learning models are all just black boxes that
can't be understood and can't be trusted. Christoph Molnar is one of the most important people
in the interpretable machine learning space. In 2018, he released his magnum opus,
interpretable machine learning, a guide for making black box models explainable.
Interpretability is often a deciding factor when a machine learning model is used in a product,
a decision process or research. Interpretability methods can be used to discover knowledge,
to debug or justify a model and its predictions, to control and improve the model, to reason about
potential biases in the model, as well as increase the societal acceptance of models.
But interpretability methods can be quite esoteric. They add an additional layer of complexity
and the potential pitfalls require expert understanding. Machine learning models are
inherently less interpretable than classical statistical models, but typically they have a
better predictive performance and that's because of their ability to handle nonlinear
relationships and also higher order feature interactions automatically. But do we have
to suffer this implicit trade-off between the complexity of a model and the lack of our ability
to understand it? Simplistic model approximations can often mask important information and be
misleading as a result. In classical statistics there's an entire field called model diagnostics
to do exactly this, to check that assumptions and simplifications have not been violated.
This is something that does not yet exist in interpretable machine learning.
Interpretability has exploded and matured in the last few years, in particular since the
deep learning revolution. We now have a better understanding of the weaknesses and strengths
of interpretability methods. A growing number of techniques are available at our fingertips
that can lead to the wrong conclusions if applied incorrectly. Is it even possible to
understand complex models or even humans for that matter in any meaningful way?
That is one of the questions that we're going to be discussing this evening.
Molnar also recently released a couple of papers where he discusses some of the important pitfalls
of interpretable machine learning methods. So some of the things that Christoph Molnar is
really concerned about is the lack of statistical rigor in IML methods. Molnar used to be a statistician.
Also he is exasperated with some of the misguided causal interpretations from some of these IML
methods. He also points out feature dependence or situations where you have shared information
between features. It completely breaks many of the IML methods and this is something that he
focuses on a lot. He also focuses philosophically on the broader impact of interpretability and
what interpretability even means frankly. It's a very nebulous term. So let's have a quick flick
through this paper, interpretable machine learning, a brief history, state of the art and challenges
and as well as pointing out some of the history of IML methods, we'll jump straight into one of
the challenges which is feature dependence. Molnar points out that feature dependence makes
attribution and extrapolation problematic. This is exactly what happens in partial dependency
plots for example. We are basically extrapolating and we are creating fictitious data points that
didn't really exist and these fictitious data points probably exist outside of the data distribution.
So Molnar thinks that the models that we build should reflect the causal structure in the world
but of course that is not really the case most of the time and he points out that statistical
learning is just reflecting surface feature correlations not the true causal structure beneath
the scenes. Causal structures would be more robust if we could actually capture them
and the predicted performance and learning causal factors is a conflicting goal which I think not
many people have thought about. So we need to think about when we can make causal interpretations
and a lot of work is underway in this field but being completely frank this is very nascent. There's
not really much out there at the moment. Molnar also points out this lack of statistical rigor
having been a statistician himself. He was exasperated when he came into the IML field just
to see that most IML methods do not even give you confidence estimates something which is
completely standard in the statistical world. Models and explanations are computed from data
which means they are subject to uncertainty but this is something which is just not captured
using current methods. He says that we need to be making distributional and structural assumptions.
He points out this risk of p-hacking something which is prevalence in the natural sciences.
This is something that could be coming to the world of IML very soon if we don't start thinking
about this more carefully. Molnar also points out that there is no accepted definition of
interpretable machine learning methods so it's not entirely clear how we can compare IML methods to
machine learning models. It's really easy to assess machine learning models because we have
benchmarks and we have ground truth labels. Those benchmarks are fraught with problems as well
but we can't really quantify how correct an explanation is and it doesn't really help that
there's a taxonomy of interpretability methods. There are objective methods like sparsity and
interaction strength and there are human-centered evaluations from domain experts or from lay people
and quite often you need to have quite a lot of technical knowledge to even understand
these assessments. He says that the setting of machine learning is too static. It doesn't reflect
how these models are actually used in practice and I really love this idea of thinking about a
process rather than thinking about just the model so he says we need to have a holistic view of the
entire process. He thinks that we need to think about how we explain predictions to folks from
diverse backgrounds, how we have interpretability at the societal level or at the institutional level
thinking much more broadly than we are at the moment. He also thinks that we need to reach out
to other disciplines for example psychologists and social scientists and he thinks that there's
lots of rich knowledge in computer science and statistics that we're just not using yet.
So in July of last year he also released this paper pitfalls to avoid when interpreting machine
learning models. In this paper he points out that there's a growing number of techniques
providing model interpretations but many will lead to the wrong conclusions if used incorrectly
and he goes on to point out many of those pitfalls. For example the first one is
assuming that the model generalizes well so assuming that the model has been fit correctly
if the model is underfit or overfit then the interpretation method will perform badly as well
and interpretation can only be as good as the model underlying it. So the next pitfall he points
out is the unnecessary use of complex models which is to say the use of opaque or complex
machine learning models when an interpretable model would have sufficed which is to say when
the performance of an interpretable model is only negligibly worse than one of these black box
models and to be honest this is something I see all the time I think the gratuitous use of complex
machine learning models is something which is really serious. One of the things I don't like
about machine learning is the laziness. I think we should always seek to understand and simplify
problems wherever we can it's the same thing in software engineering. We should always be trying
to create the most elegant and simple and maintainable solution. We shouldn't be trying to over
complicate things and I think that's a very you know the kiss principle is very generalizable here.
So he recommends to start with simple interpretable models like generalized linear models or lasso
models or additive models decision trees or decision rules and gradually ratcheting up the
complexity as required. So he also points out that ignoring feature dependence is super important
right and this is a problem that many of the IML methods have so he gives an example of
partial dependency plots where they extrapolate in areas where the model has little training data
and it can cause misleading interpretations. So these perturbations produce artificial data
points that are used for model predictions which in turn are aggregated to produce global
interpretation so that's a big problem. Another thing he points out is confusing correlation
with dependence so he gives an example here features with a Pearson correlation coefficient
close to zero can still be dependent and cause misleading model interpretations
while independence between two features implies that the Pearson correlation coefficient is zero
the converse is generally false. So there's a pretty cool example here this is a couple of features
that absolutely have a dependence on each other you can see it visualized here but you wouldn't
know that if you looked at the Pearson correlation it would have said that it wasn't significant.
Another one misleading effect due to interactions so there's a couple of things here there's the
partial dependency plots on a couple of dependent features and then he's used a simulation to kind
of trace all of these different features to see what the predicted label was and according to
these IML methods there is actually no clear dependency between these features and the predicted
outcome whereas you can see that that's just blatantly false. So something I've been meaning
to do for more than a year now is to go through Molnar's interpretability book and to make some
bite-sized videos on every single approach well Connor and I are actually going to do that over
on Machine Learning Dojo with the first one next week on Shapley Values so make sure you
subscribe to Dojo and check that out. Remember to like comment and subscribe we love reading your
comments and we'll see you back next week. Welcome back to the Machine Learning Street Talk YouTube
channel and podcast with my two compadres Connor Tan who runs the Thomas Bay's Appreciation Society
and MIT PhD Dr Keith Duggar. Now they say that Germans are known for beer, sausages, precision
and these days interpretable machine learning. We have an exemplar German on the show Christoph
Molnar. Now Christoph made waves in the community when he released his Magnum Opus interpretable
machine learning a guide for making black box models explainable. If a machine learning model
performs well why don't we just trust the model and ignore why it made a certain decision? Well
the problem is that a single metric such as classification accuracy is an incomplete description
of most real-world tasks now as according to Doshi Values and Kim in 2017. In Christoph's book he
introduces the importance of interpretability and reports an incredibly detailed taxonomy
of interpretability methods and his style of writing is at times entertaining and entirely
absent of hype and nonsense. He runs the gamut of interpretability models so for example model
agnostic methods like Lyme and Shapley values. Example-based methods such as counterfactual
examples and adversarial examples he motivates the importance of interpretability methods
but he's also extremely transparent about its current weaknesses and pitfalls. He's currently
finishing his PhD in interpretable machine learning at the Ludwig Maximilians University in Munich
after getting a stats master's from the same institution. He's recently written several
very interesting papers on interpretable machine learning for example pitfalls to avoid when
interpreting machine learning models in July of 2020 where Christoph detailed several problematic
model interpretations for example ignoring estimation uncertainty feature interactions
or confusing correlations with dependence. More recently he published a paper called
interpretable machine learning a brief history state of the art and challenges while he acknowledged
that the field is maturing nicely. He also spoke about some of the serious challenges in IML methods
such as the lack of statistical uncertainty, shared information between features, lack of a
clear definition of interpretability and the need for a more holistic view. Christoph Mulner
it's an absolute pleasure and welcome to the show. Thank you very much for the invitation glad to be here.
You know Christoph I have to say I really enjoyed your book. I read this actually
some months back in preparation for a completely different show. I loved how scientific it was
you know it was it was very much laying out essentially a survey of the facts a lay of the
land very objective evaluation. It had both the pros and cons you know of different approaches
examples to make them you know more understandable so kudos to you. I thought it was a great book
very enjoyable and very informative. I also loved how it lays out the beginning you know what the
goals that we're trying to achieve with with interpretability are especially kind of the
human goals right like what does it mean for an explanation to be good for people what kind of
explanations do people like and sometimes there can be conflicting conflicting goals there and
I think one thing that that I realized from reading your book is that that actually explanations can
be deceptively good yeah like I think I think the the the sort of cognitive bias maybe that we have
to look for contrastive explanations or counterfactual explanations like and principle it seems good
it's kind of like you know I'm sorry we can't give you this loan you know well why not like why
can't you give this loan well well we've detected really that you're a that you're a deadbeat what
do you mean I'm a deadbeat yeah you know you never pay your bills well let's see why okay let's look
through this and we find a decision tree here and and some big decision tree and we get to this one
little point what says here you know you didn't pay this furniture bill back in 2018 you know if
if only you'd have paid that furniture bill like we'd be able to give you the loan right but the
truth isn't that simple like it's actually buried all throughout yeah throughout the decision tree
right with so many contributing points yeah I think I like this chapter that you referenced
was about like kind of from the social view or the human view what what what people like or
prefer is explanations and the whole chapter is based on I forgot the title of the paper it's
like from Miller about like what we can learn from the social sciences about what a good
explanation is and was like a paper where I learned a lot and it was super interesting also to see
how like what people think are good explanation as you mentioned they should be contrastive
they should be short but they should also confirm to some prior knowledge that the people have
and I mean like objectively a lot of those things might not like you wouldn't say these are good
explanations in some sense like maybe maybe it's not good to give an explanation that fits with
a prior knowledge because it's not the correct one maybe so it was quite interesting to learn
and to think about like what's the human side of it that's a very cool part of your book I thought
the fact that it's actually quite interesting thinking about what we really want out of an
explanation I remember first of all looking at you know sharp values that are very fair and
will distribute the blame equally amongst all the different relevant features and then you turn
to something else like you know selective interpretations that in a way are way less
good because they're kind of arbitrary they'll just select a few a few a subset of the features
and give them all the blame but then it turns out that apparently that's what people actually
want as a useful intuptable explanation yeah so as I see there's like many many dimensions of
explainability or like what what can be a good explanation and one of these dimensions is maybe
sparsity that you have a short explanation with just a few facts that's something that people
prefer maybe but this might be in conflict with other dimensions of a good explanation
which should be maybe that all causes should be addressed by the explanation that plays some
role at least but this is of course in conflict with sparsity if you want this full attribution
like you get with shepley values for example so I that's why I also think there's not like just
one correct explanation but there's like many attributes or many dimensions on which you can
judge explanations yeah I think this is one of the problems because even machine learning is really
difficult right because we use benchmarks and benchmarks are just something that people have
come up with but you say in your you know you talk about one of the problems being that there's no
definition of IML methods to start with but at least in machine learning methods we have ground
truth which is which is significantly better in a way but if we if we can't quantify how good
an explanation is then where are we really because you talk about a kind of taxonomy of
interpretability methods right you say that there are objective evaluations like sparsity and
interaction strength and fidelity and humans human-centered evaluations you know which might
come from domain experts or lay people so I suppose you're just hitting straight on the
fact that this is actually quite nebulous isn't it yeah so yeah so in some sense like there's this
big criticisms okay this is not scientific or not well defined at least what interpretability is
how can we even do research in this area but I have a bit more relaxed view I mean otherwise I
should have stopped writing the book before I really started so I kind of see like this
endeavor of giving interpretability or bringing interpretability to machine learning it's more
like a first of all it's just a keyword so it's it kind of bundles all the methods together
that kind of aim to reduce this high-dimensional function to something well mostly it's something
in a lower dimension so we kind of just do this mapping something gets lost in a way this is fine
and it's I think part of science to find out like or to yeah some part of analysis to find out what
part gets lost so when you for example look at just some feature importance values for example
of course it's a summary of your model and a lot of information gets lost but I still think
it's useful to have obviously so many people use it but it's useful to have these
tools and we just have to understand what they do and how to interpret the results so how do
you interpret when like the feature importance is zero of a feature could that be quite dangerous
though because this you gave the example of random forests when you have a lot of shared
information between the features it would actually tell you that these correlated features have a
higher feature importance than you might otherwise expect so does this imply that we need to have
very detailed knowledge of how we should how we should use this information that we get from
IML methods yeah so it's also kind of the direction in which I write papers like this pitfalls
to avoid and stuff like this so I think so these are just tools so they do something with the
model a kind of distill some knowledge so for example for feature importance you kind of
measure how well does your model perform and then you measure again after you
shuffle one of the features and and then then you get something out of it so then we can ask
questions is this interpretable or not and it's kind of well not so relevant the question because
you just have to understand what what happens when you shuffle feature and one is for example
you kind of break the association between the feature and the prediction because now it doesn't
carry the information about the target anymore because you're shuffled in randomly in your model
so you kind of this this feature importance now measures how much performance you lose because
of this break of information but then you also when you think about this method and want to use
it you also have to understand that this shuffling for example breaks also association with your
other data feature like the features in your data so this is a limitation of the method and
what I think is needed is that we understand in which way these methods break or which
scenarios we're allowed to use them or how we are allowed to interpret them and I think the
situation is kind of similar to statistics where you have these models and and then you
interpret like the coefficients of your models you still like have to learn how you do the
interpretation what are the assumptions that have to be met that you're allowed to do this
interpretation and I think it's we're in a similar situation here with interpretability of machine
learning and I'm glad you mentioned sort of the old school linear models as well as dimensionality
in the thread because you make a very good point in the book which is look even these so-called
intrinsically interpretable models are only interpretable up to a certain dimensionality
and you know I have I have tons of experience with with multi-linear regression right and and I can
guarantee that beyond a very small number of dimensions those coefficients are not interpretable
because it starts to play a bunch of games where it's inflating one coefficient and another because
their difference is important and you know whatever else is happening a lot of correlated
structures are all essentially getting compressed into the small number of small number of weights
right and so as the dimensionality goes up I would say like no model is is intrinsically
interpretable same can be said of decision trees like anybody who's looked at a decision tree that's
come from real data you're going to find out it's not interpretable it's like oh look at this you
know market capitalization matters oh and it matters over here too and down here and and
actually I have to go through five checks on market capitalization before I get down to this
decision and maybe the features aren't that intuitive either and then you have to kind of
like mentally stack up like until you get to the decision like five decisions and then we have a
very complex rule that led you to the prediction yeah so I agree that there's it's not like I mean
I have I have this distinction the book like interpretable models and not so interpretable
models um but it's as you say it's like a gray like it's a scale really people could definitely
overhype how interpretable these white box models are right whether it's linear models
as I'm a I've worked with many physicists who uh have had guidelines that you should only ever use
models like a decision tree because it's possible in theory to write down on a piece of paper exactly
how the decision is made right yeah you can trace every decision but that's never actually useful
in practice is it since when have you ever looked at a decision tree fitted to data there's any
complexity and the fact that in theory it's possible to go and examine how it works yeah it's
completely irrelevant in practice isn't it yeah I mean it can be useful to have like a short decision
tree sometimes it but in practice it will not like give you probably the best predictions
um but it might be useful sometimes to shorten it artificially so even like
you're throwing away some some predictive accuracy um but you shorten it so you understand
that somehow it's manageable you can have a look at it and and see what's going on well there's also
you know the other issue is that as I was looking through a lot of the methods that you describe
and you survey in your book you know some of them um are not simple I mean if you start looking at
partial dependency plots and trying to explain what those are I mean you know you have to almost
have a deep mathematical knowledge really to appreciate them in the first place so I'm wondering
at what point we're just developing complex math models to explain complex math models and we really
haven't you know made much progress along the interpretability axis yeah yeah it's true it's
also like the criticism too like um so you have something you don't understand and you explain it
with something you don't understand um I think some methods are complex um but for some at least
there's some intuition how they work for partial dependence plot is kind of your this um intuition
that you do some intervention on your more or intervention on your data so you replace all your
like for one feature you replace all the values with one fixed value and kind of look at the
average prediction that you get afterwards and do this for a lot of points and then you connect
the points and you have this curve so kind of gives gives you the expected change over the feature
range maybe there was already a bit complex I don't know um maybe I'm too deep into the method
already um but yeah of course it's it's something additional people have to learn or if they agree
to use it of course could I get a quick take from you on saliency maps as an example because you
said in one of your youtube videos that saliency maps are glorified edge detectors they are not
good explanation at all and I've noticed now that many machine learning platform providers are building
these kind of um saliency maps into their models you know into their platforms and then it becomes a
kind of box ticking exercise where you can say okay well yeah we've done interoperability now
that's all you need to know and that really is quite a false sense of security isn't it
it's funny you mentioned the saliency maps because I'm writing a book chapter about it and
actually I'm I wanted to publish it today maybe I will or at least in the next few days um it has
been a long time in the making and it was very very frustrating like by far the most frustrating
chapter to write um number one reason is because there's so many methods out there uh reason number
two is I I can't judge really or if they work and it seems like they mostly don't or it's it's
still unclear like how you say you would judge that they work so they're like dozens of these
like integrated gradients gradients deconfinate deep tailor decomposition layer-wise relevance
propagation in 10 variants um so and then I mean you in the end you when you apply these methods on
they are also like for image classification and you get these nice-looking images and some areas
are highlighted some or not sometimes you can say okay this doesn't make sense at all um but if
if it kind of makes sense then you maybe would be inclined to trust the method um but then there's
this uh paper which is called um sanity checks for saliency maps and they kind of found out that
they the most of the methods are very similar to edge detectors meaning that they are kind of
insensitive to the model and the data which is very bad of course well if you change the model um
the explanation should obviously change um could could you expand on that a little bit so you said
it wasn't really reflection of the model or the data but what what would a perfect saliency map
look like well I don't know myself actually so I mean the the ideas that you saw the basic idea
of most of these methods is that you you have your class prediction or your class score and you
want to back propagate it not you want to back propagate it to the original image so you look
at the gradient um with respect to your input pixels and that's there's no not not one way to
do this but there are many different ways so that's also why we have so many different methods
and then they highlight which pixels were relevant for the classification um but yeah
they these these methods they have like a lot of like issues for example there's the issue of
saturation for example because of the real unit where you have flat parts of the gradient so if
you pass the gradient through that then um your method would say that some some neuron might not
be uh important at all and there's a lot of these little issues that these methods have um
yeah so but but back to your question like I think that's also the issue that I don't
wouldn't know how to answer I mean obviously it should be some area that should be highlighted on
this aliens method was important for the for the neural network um but then again I don't know how
the network decides so I couldn't like if I see an image I couldn't like highlight the part I mean
I could highlight the part where I think the network should look but then again I mean there are
lots of papers like the clever hands paper which saw like the reveal that there are some
sometimes it would look at watermarks on on the photo um so these are like these things that we
just don't know uh what the neural network basis this on have if I could take a stab at that answer
for one I think just the idea of quote a saliency map is a problem like there isn't one
map of of the importance of the pixels it's like they're they're operating on multiple
multiple dimensions or at least sort of multiple feature sets it's like if you ask me to tell you
know why is this image a dog you know well for one thing it's it's the overall shape you know it has
four legs and you know two ears sticking out over here that's one saliency another is that
it's it's got a certain color you know and it and it's coat and that's a that's a different
concept of what's salient and another is that there's a frisbee flying at it and its mouth is open
and it's about to catch it and I know dogs do that so they're kind of you know when your mind
analyzes an image it breaks it down into these many large scale kind of structural features
and I think that gets completely lost and most of the approach is the saliency maps this is
really important point actually because if you're just looking at the pixels on this kind of 2d
planar manifold that's only a very it is quite literally a surface view and I think Christoph you
said that there are all sorts of causal structures and even in the model itself right there are
these entangled neurons and surely that's giving me more insight into what's actually happening
just seeing a bunch of pixels and the other thing is that these models that they are completely
lacking in robustness so probably if you changed a few of the wrong pixels your saliency map has
just got completely broken right yeah so um but in in that vein some of these feature visualization
techniques you know like the deep dream type stuff maybe maybe that's a better way of of
interpreting these models yeah um so like for the one point you mentioned about the adversarial
examples so there's also a paper I forgot to title again um which that manipulated neural networks
so they would give the same prediction for all the images but different explanation like different
saliency maps so this is perfectly possible to create different explanations um for these saliency
maps um but but keeping the model like at least for the predictions the same there's another criticism
you can throw at saliency maps where they they can be quite deceiving you think they're useful and
they turn out not to be useful yeah there's a classic example of looking at you know comparing
a dog to a wolf and sometimes you see it's looking at the snow in the background and that's helpful
sometimes it highlights the animal and you think okay I understand it's looking at the face that's
why it thinks it's a dog because it's in the face and then you look at the predicted class for something
else like you know a cat or a frisbee or a house or a boat and it highlights the face as well yeah
so the saliency map for all these different classes looks the same and when you realize that
you realize this this saliency map hasn't actually told you anything about why it's gone for one class
versus the other all it said is that it's just highlighted the thing in the middle of the picture
yeah I think that's especially also when when you look at images you know like we're very good
with images yeah like we were very quick to see what's happening on a scene and such
so I think we're also very quick to make judgments oh yeah this makes sense this doesn't make sense
it's more difficult to interpret like if you have like a graph and there's like things going on inside
you have to like now understand what the method does and stuff like this but for an image like a
heatmap IR this area is highlighted makes sense case closed I like the method
yeah and that gets exactly back to the deceptively deceptively good explanations problem and explaining
complex things with complex things we don't understand you know so I think a lot of people
if they looked at it and again one of the points of this interpretability is really the social
aspects of it right like being able to convince people to be at ease with machine learning models
or to accept the results of of a machine learned you know decision process and I think if somebody
looks at an image of a dog you know they have no problem understanding that but if you showed them a
bunch of salience maps or or any of the other sort of you know feature projections if you will
like you said it takes a lot of deep understanding to understand those whereas the image is kind of
immediately obvious I think two of the main themes that you touch on is we'll get to the
to the probabilistic stuff the the Judeo pearls stuff in in a minute but I think the main issue
that you point out is feature dependence okay and and you say that when you have feature dependence
it makes attribution and extrapolation problematic so a dependence just means that you got correlated
or shared information between your features right so you say that in feature permutation methods
these things basically break everything when you have the shared information
and the extrapolated data points are no longer in the distribution and you say that there are
conditional permutation schemes you know that try and and maintain that joint distribution but
those things sometimes make it even worse right so do you think that's one of the most important
things that people should think about when using iml methods yeah at least so that's at least like a
very um deep issue I would say which is inherent in in most of the model agnostic methods where you
manipulate your data see what how the model prediction changes and then create your explanations
out of this sort of select the shadowy value line partial dependence plot feature importance
they all work with this mechanism of manipulation of the data prediction and then kind of aggregating
the results and most manipulations happen in isolation so that you for example when you
for feature importance you can meet one of the features as I like said before and then
well you break the association of target but also a few other features
but similar things happen if you use lines or you kind of replace parts of your image
but then again you also have to replace it with something like which is I think in line the
defaults with just a gray image and then of course it's not like it's outside of your data
distribution subtly because your network was not confronted with like these patchy images before
they had like just normal photographs usually and depends on your neural network but I mean you
certainly didn't train it on on images where parts were grayed out so it's pretty likely what the
model should predict and what will predict at this point but but you use these images to
create your data set like you send it through the neural network you get predictions and
you kind of aggregate from this your explanation but you left your data distribution and your model
can do anything then and the hope is that it doesn't do anything crazy but yeah you don't know
like it like a simple example from the medical field would be that you know height and weight
are highly correlated right and and on the other hand sort of the ratio or some relationship between
your your weight to your height that actually has very important medical consequences right that's
that's the measure of of health and so if I were to sit there and just permute say the height index
and create a whole bunch of people that had all these bizarre combinations of
of height and weight you know first of all those don't even probably exist in the data set and the
ones that do exist in the data set probably had some medical issues right yeah well you actually
gave a similar example I think you gave the example Christoph of a baby that earns a hundred
thousand dollars a year which is which is insane but when you talk about something like lime
maybe that's different because the cnn the what you know it shines a flashlight over the input
space in it and it's a kind of local method so in some sense you could argue that it doesn't
matter that you've grayed out all this other stuff because if the model was sufficiently well
trained in the first place it should hopefully learn to ignore the background or is that just
wishful thinking that's an interesting thought I haven't thought about it because like the property
of like that you have these filters that trust a wander over the image yeah maybe it would make
it more robust for these kind of interventions that we do when we create these images with lime
and shetley um yeah I haven't thought about it it could be uh well we've mentioned all of these
ways in which interpretability methods can go wrong right how the model might not be a realistic
the interpretability model might not be a good approximation to the actual ml model
so some people a bit controversially perhaps take the idea and remember that and say you're just
working up completely the wrong tree and you should give up using interpretability
interpretability methods to explain these black boxes this dish them instead just use an interpretable
model to begin with use a white box model I think there's an example um from compass in the US which
is that model to predict reoffending and I think quite famously there was a investigative journalists
that tried to interpret this model it was a black box model because it's proprietary right it's a
trade secret and they they fit it a proxy model a kind of a linear model and they made a report
saying okay we think your model is racist because it looks like it's it's taking race as a factor
and then some further work was done and they came back and said well actually you've just used a
uh interpretability model that doesn't really fit our model very well you've made some assumptions
that don't hold if you use a different interpretability model you get a completely different answer
that it doesn't use race at all as a factor and so you've got to kind of a you've got to a wrong
assumption by using a bad interpretability model and I think they were saying that this model
instead you could get just just as good a model of reoffending with like three FL statements you
know ditching this massive complex 100 and something features and just use three FL statements
based on I think age and reoffending so is that was that what we should do should we just drop
these methods and start using white boxes instead so I mean like one one thing to mention here is
that a white box is very soon also like a gray or black box if you add interactions if you have
many features and so on but putting that aside I would agree if you're in first place like that you
say you should start with like a white box so if you start modeling then then you then you should
consider these first like maybe they already solved your problem then it's perfect and you have a model
that is quite I mean stable it's interpretable I think that would be great but then I think the
next step would be to see like what like a black box or a machine learning model would give you in
terms of performance and then maybe if you see the gap is really big then maybe you can try
some feature engineering and close the gap maybe from the interpretative model to the machine learning
model but then you're probably already infusing some features that are not so interpretable
or maybe if you're using a linear regression model you're maybe using then splines and
interactions so you're already moving towards more complex models usually but then if you still have
a gap then I think you have to decide is the gap and predictive performance like worth changing to
a black box model so I think that's your decision will be different in many cases
as you relates back to the point you made on your paper about criticisms of using
interpretable machine learning models that some people leap straight away at using an overly
complex model and sometimes depending on the situation sometimes you know a linear model can
do just as well and have all these advantages it's so much easier to explain do you have a
philosophy from a high level here right because if it if it were a human if it were an airplane
pilot we don't really understand how the brain works right we would just test the pilot you've
got to fly the plane for 10 000 hours and if you don't crash then we'll let you fly so we don't
really seek to understand how his or her brain works but with machine learning models there's
this continuum right so if you use these complex black box models the predictive performance is
usually better but you're trading off understandability and assuming those things are completely
mutually exclusive what kind of decision process do you go through when you select these models
but by the way with machine learning right the reason why we use machine learning is because
we don't understand how to do something explicitly yeah is that a fair statement um yeah I would say
um when when it ate us like so high-dimensional so complex many interactions and so on
that your simple models don't cover the complex cannot cover the complexity I think then you
need machine learning would you rather understand exactly how the plane worked or would you
rather I mean if I was saying to you you can go and fly in a plane would you rather that you
understood how the plane worked or would you rather that the plane was tested why not both
so um I think we can do both it's to some degree so um of course with black box model we don't
exactly understand how they work um but in comparison to a pilot we can test them for
three more or less um so because I mean maybe it's not as good as an interpretable model
but we still can use a lot of methods to at least approximate and and try to understand a few
properties of this model so I think we are even in a situation where we don't have like these
complete like A or B decisions but we can have so if if the machine learning works much better
and it's like really robustly tested with lots of different data I would prefer machine learning
model I guess um but then I would also want to like people to to apply all these methods that
are available even if they are not perfect but still they give you something they give you some
insights so yeah and I think so Tim one answer to your question is that a lot of people's response
here and kind of demanding interpretability and having concerns about machine learning
it all comes down to generalizability and we've seen through using machine learning
that it breaks down in ways that that we don't like like for example sure maybe the soap dispenser
you know is really great at dispensing it dispensing soap you know 87 percent of time but
it just so happens to kind of be a race sensitive soap dispenser and just doesn't give any soap to
people with a certain skin color like we've kind of decided is a society that are that there are
certain generalizations or certain dimensions along which our models just have to perform
and also because a lot of these these things that break machine learning models are things
that happen quite quite regularly in the real world it's like a pilot you know a human being
pilot flying around if he looks down at the ground and sees a hot air balloon with a big
smiley face on it he's not going to crash the plane he's just going to be like oh yeah I forgot
about the uh the hot air balloon contest that's going on today where's a machine learning model
if it looks out a camera and sees something with a particular shape of lightning bolt you know it
might just decide it's time to like dive for the ground right and crash the plane like that's
sort of what these adversarial examples kind of show and I think that's why people are really
hungering for human understandable explanations because still to this day the human brain
is the only AGI really that that we have around yeah but deep learning models that they they
essentially memorize lots and lots of things and they have this sparse coding so in a way it's just
like the white box model even if we use interpretability methods we could enumerate all of the
things that they are learning and one of those things might be a sensitivity or lack of sensitivity
to hot air balloons or smiley faces on but even if we could enumerate all the things that they are
learning we wouldn't understand that either in the same way we don't understand how a real human's
brain works and I'm not sure whether we should view a human brain as a computer program and whether
that's a good thing or a bad thing but at some I guess what I'm saying is at some point we have
to accept that we're not going to understand a totally it's actually it's a good comparison
I think with humans you know if you're interviewing and you want to hire let's say a software developer
you tend to set them a coding interview you wouldn't think of taking them into surgery
opening up their brain and trying to find the neuron that predicts what the next you know
but if code is going to be and understanding how that works it's just why would you do it that way
instead you learn to trust humans by working with them giving them a test seeing how they
perform in the real world and I know maybe we're asking too much of a machine learning model if
you want to be able to understand these complex things in terms of like a bottom-up white box
set of rules. I think what a comparison falls a bit short is that that we have the luxury that we can
cut off the like cut open the brain of a machine learning model without breaking it
and without hurting it hopefully and we can do all these try out all these interpretation methods
see how it behaves under certain situations and I also would make a distinction between
we understand what's going on inside and doing like this kind of sensitivity analysis
where we just try out what happens in certain scenarios so it always do that that we can like
check like how it behaves so I mean feature importance is basically like a way to see
like how does it behave if we break some features and then we rank the features by this
as an importance we can do it and that's also the big difference between humans because we
can't test in the same way and yeah shouldn't probably. I think there's one other difference
though which is to do with the substrate of how neural networks work I think if I'm giving someone
a job interview or something I mean of course it's a very valuable process but I'm looking at
their values and I'm looking to try and understand how they would behave in different situations
and I'm coming up with lots of illustrative examples but the difference is with humans
we have that level of generalization we have a kind of guiding taxonomy of behaviors which means
if I know if I have guiding examples of what a human will do in certain situations I expect
that to generalize whereas my hypothesis is is that a deep neural network model is almost like
an infinite number of rules and there's absolutely no carryover between the rules
so knowing even some or even most of the rules doesn't really tell me about those edge cases.
Yeah I would agree that the edge cases are quite unforeseeable probably I mean at least we know
that they exist like with adversarial examples so even if we don't know like exactly what they
would look like or there's even an infinite amount of uh I mean there's an infinite amount of like
how you can change the image to make it like have a different class so we know so I think it's important
that we know that these exist at least. Yeah I love the point you made though that we have we
have the luxury to do analysis on these on these boxes because we can open them up and that that's
another point I'm pretty sure that you make this in your book as well which is that part of this is
just scientific inquiry it's it's like understanding better how to interpret and explain machine
learning models will probably actually contribute to us being able to construct even better machine
learning models isn't that true? So so you're basically saying that um also interpretability
might help to to be better at like create better machine learning models themselves?
Yeah like as we as we develop these interpretability methods because in a sense like you point out
earlier there are statistical projections of kind of the behavior of the model and so like a saliency
map you know if we can if we can kind of use that to learn the way in which the the neural networks
are behaving it may it can certainly give rise to intuitions on ways to alter the model. Yeah so I
also have seen approaches where they try like kind of fuse also these two worlds like interpretative
models or white box models and black box models so that you try to to generate features out of
the black box model which you then use in your more understandable white box model so um I think
this and and this also like by using similar techniques um to which you would use for interpretability
like detecting interactions for examples for example so um yeah these can be used also to
to build better models and also to build better interpretable models. The other and I'll make
one last point here which is another social good that can come out of interpretability is
imagine we've got you know an ML model that's not trying to make any decisions but it's just
trying to figure out what leads to happiness and success in life you know and so we analyze a whole
bunch of data and we find out well it's really important if you graduate from high school and
it's really important if you you know don't have children before you're married and you know all
these other factors if we can dive in and kind of isolate those factors it actually allows people
to have some guidance on oh look we've had this machine learning model that's analyzed a bunch of
data and it actually has some some understandable recommendations for how to lead a healthier
life or a better life or whatever. Yeah I think that this very good example were the fact that
you have a prediction model doesn't solve your problem so actually it's just a means to to some
other goal in this case understanding like what are the factors for happiness and one example I am
from a friend who worked at a telecom company and they built like a churn prediction model
to see like who will quit the telecom contract and then they started like the ones with the
highest likelihood they started sending out emails say maybe offering them a better deal
but actually the outcome was that well they they when they once they wrote to the customers they
well left and quit their contract so it's kind of had like so this is a case where the prediction
model actually works but then they people leave in this case probably because they
realized ah shit I have this contract still going on time to quit now so if you knew the
reasons why they are likely to churn then you could like better select like when you write some
email or maybe some other campaign or when maybe not to write anything at all yeah right now um
Christoph you have a background in stats which means you I mean you like Connor as well we take
an incredibly dim view of machine learning and you wonder how how is it possible for us to be
stabbing in the dark like this but you know you said that we need to be more rigorous and
there's no quantification of uncertainty with the current IML methods and I suspect you might
be working on some methods behind the the scenes on this but you know when you have models and
explanations which are computed from data they are subject to uncertainty and that's just not
modeled at all at the moment right so we need to be making some distributional and structural
assumptions that we're not making now and you point out that there's this phenomenon of p-hacking
which is a huge problem in the natural sciences which hasn't quite made its way to IML methods
yet but probably will do yeah so yeah the I think and in statistics we're really good at
quantifying uncertainty I mean this also has some darker sides with like the p-hacking and so on
but I still would say it's better to have um not only just one number or one explanation
but also have the distribution to do of this explanation or this number and to quantify what
uncertainties behind computing this number so when you have a linear model then you get
some coefficient which you interpret in the end but usually you don't just interpret the
coefficient but you look at the confidence intervals but we don't do it at the moment for
interpretability so you maybe get the saliency maps but how certain are you about maybe it's
a bad example because we don't so much on it but if you have like a feature importance value and
you get some result how like what's the range actually like how much variance is behind it
if I were to use slightly different data or refit my model again how similar would the number be
and I think that's something that will or should come to interpretability as well
it's funny how when we come to machine learning it's almost like open season and forgetting
everything you know about maths and stats you throw it all out the window okay so excited
about these algorithms right like one example is if you take a if you're fitting a model to predict
something that was unlikely I don't know maybe it was like a covid test for example and then
if you know the prevalence of covid you get it back you kind of know what the false positive
rate is going to be and so you notice that you think it's the multiple comparison issue right
you know that you're expecting a certain level of false positives when it comes to doing something
like feature importance or looking at interpretability from a thousand features and then five come
through is really really important you mentioned sometimes we just forget that multiple comparison
issue forget the fact that probably these five are going to be completely false positives and
probably completely meaningless yeah I agree especially if you have like these high-dimensionality
features and for the record I have to say I mean there are already approaches so especially for
feature importance because there's like a huge community in random forests for example and they
thought a lot about these issues and their tests for this and stuff like it but for the rest of
interpretability I think it could gain a lot thinking more about I mean this is very simple
stuff like multiple comparisons quantifying uncertainty does the stuff like statisticians
think like a long time already about it and I mean if you even if you leave the area of
interpretability and look at the benchmarks so even like if you have like accuracy like a table
and you see accuracies in it but there's no variance attached to it then it should be like
suspicious of it but because if you just retrain your neural network with a different seed you might
end up with a different accuracy in the end so and if you want to say a method is better than
another method you want to quantify how larger ranges of uncertainty do you I mean there are a
lot of things like the choice of data choice of splitting points and training and test data
weight initialization and so on so I think a lot of this rigor from statistics could help
the machine learning community and and machine learning science to become better
yeah but let's let's never forget this uh quite quite well known saying which is there are three
kinds of lies lies damned lies and statistics so you know that that's a lot of what's going on right
is is you know fundamentally whenever we go measure data and we have a model what we're
actually able to extract from that data and and the model is inherently probabilistic
it's a probability distribution right at the end of the day and we get into trouble anytime we try
to take that probability distribution and project it to numbers i.e. statistics like as soon as we
start trying to to generate and it doesn't matter whether it's it's a mean plus a confidence interval
or whatever the fact is we're throwing away information the totality of the information
sitting there in that weird multimodal you know spread out distribution right and then we we find
some way to simplify it and project it down to a set of numbers we've got a problem like that's
and if you forget that that's happening if you forget that you're throwing away all this information
I think that you know the tendency to do that isn't just simplification it's that oftentimes we
have to use these probabilistic things to reach a decision and as soon as we get to the point where
look i've got to choose either to go left or right give the loan or not give the loan as soon as we
get down to some point where we have to make a concrete decision we're forced you know we're
forced to project it right but along the way it's important not to lose sight of the the fact that
we're throwing away information fantastic well and by the way you also said something interesting
a minute ago Christoph which is about at least in most machine learning algorithms if you change
the random seed you know that there's enough stability there that it still gives you roughly
the the same model every time but in reinforcement learning if you change the random seed the entire
thing is completely broken but but yeah what Keith was saying about this this this information
and structure in models I think that's really interesting because people have said with reinforcement
learning you can actually learn causal factors right but that's not really true you're interacting
with the system but what you're learning is is a surface representation of causal factors so you
might learn that there's a causal factor between like a hose putting out fire but it wouldn't
actually learn that it was the water that put out you know that there was a causal relationship
between the water and the fire and this is the case with so many of our models as we were saying
earlier that there's there's just a a surface representation which doesn't actually represent
the reality of our world at all but this brings me on to the next point because you have a real
problem with causal interpretations of some of these iml metrics right and you you say that models
well the the goal of models is that they should reflect the causal structure right this is what
we want to do in science but most statistical learning just reflects these surface feature
correlations they they don't even scratch the surface of what we want so what are we going
to do right are you doing some work in this field to help us out here and and why are people making
these fallacious interpretations yeah so so i'm not not working on anything causality related at the
moment yeah but about about causality i mean kind of like um i i studied statistics bachelor and
master and zero i mean the only time we were talked about causality was when i heard the
sentence correlation does not imply causality and it was really about it so um i think it's
like really um yeah should be taught a lot more like how to think about causality like
just super simple things like you should include confounders or um like what types of features
if you include them in the model um like destroy your causal interpretation of another features
these these are not super difficult things so um then you don't have to like learn any like
difficult frameworks to work with or read like causality books on it that's like super simple
yeah rules of thumb for your features even um yeah and i think you also have to decide
or distinguish between like what's the goal of your model do you want a causal interpretation
or do you want to like because in a sense and you have to also distinguish between
two levels you have the real world level and the model level i mean once you use features for the
model they are causal for the model prediction of course because you designed it that way
and the question is when you are you allowed to go to the real world level where you say
okay this um the feature importance that i see here or also the feature dependence
plot that i see is also causal um or and may interpret it as a cause and or as a causal effect
also for for the real world and i think that also depends like if you need this interpretation
if you do scientific modeling for example then you probably want it um but that there can also
be good reasons to include non-causal features into your model if your goal is really just
prediction and and some feature might help you with the with a good prediction um but it might
not be causal at all yes but the problem is when we're using these deep learning models
they they will learn a structure which probably has no relationship to the real world whatsoever
but um i think causal um factors do generalize much better there's the example of um i don't know
with car crashes right male testosterone levels is a causal factor so that will probably generalize
to other locations where you didn't train your data on but unfortunately models don't really do that
well but so just real quickly on that tim like the only reason that we know testosterone
is a causal factor is is not from that data set it's from a bunch of mechanistic you know
scientific research and biology and and elsewhere um so you know i i'm kind of wondering how
it would be nice if at least machine learning methods could indicate that there may be the
possibility of a causal structure so just looking for underlying hidden structures um that that you
know they're more generalizable that could explain large pieces of the of the data and give kind of
a list of hey there might be a causal factor here like go investigate it but on on that there's a
difference between a causal factor and a causal structure i think that the challenge is that we
don't have enough fidelity in the structure that benjo by the way is doing some interesting work on
this using data driven approaches to um you know learn causal factors but it's the structure of the
factor graph which i think is the important thing i mean this is one of the most interesting parts
i think of machine learning actually trying to learn causation from a data set which you can do
right things like beta networks where you specify all your variables you connect these nodes with
edges and you can try to learn the optimal structure like the simplest structure and that
sometimes turns out to be the real life causal effect if you do it well but the difference is
you as a human you you know the causal structure and you've you've created that that graph so it's
not learned you've created it you can learn these things from data right you can actually you can
search over the set of all possible graphs all the possible edges and you have a bit of a loss
function you try to find a graph that fits the data well so it's got enough edges but it's not too
complex you're not relating everything to everything and so just from data without any human input
with structure learning you can sometimes get a model that kind of out of nothing will give you
the cause of relationships sometimes there is redundancy right because a graph that says a
implied near causes b equals a c that's identical to c causes b causes a right but even then with
structure learning you you've got this adjacency matrix and all of those nodes you've already come
up with a priori so what you want to learn is what the nodes are themselves right yeah i think
like what kona mentioned there's lots of moda setting where you have well defined features
and i think what tim referred to was more like the you don't even know what the features are like
if you have a convolutional neural network and and like what's an object um what is a feature that
like is disentangled also from other objects um so i think also there is the big issue that you
have this entanglement between concepts that i don't know that the frisbee is always with uh on
the same image as a as a dog so um maybe the the neural network can't even separate these two things
then because they are too entangled in the data set to even discover the structure that that is
really underlying the the real world in this case that's a fascinating point actually because one of
the reasons why there's no easy solution to adversarial examples is because you you learn these
these non robust features and you might just think to yourself well um fur is a low magnitude
feature it's really easy just to kind of create fur on anything and for the neural network into
thinking it's a cat and you just say well this is obvious right you just create some rules to say
well if it's if it's not an animal and fur then ignore the fur but actually the features are entangled
in this complex neural network so you can't do that but i wanted to move the discussion on a bit
so you said that there are some really interesting challenges ahead in in iml and what's fascinating
is you start talking about the process so you say that the setting of machine learning is too static
it doesn't reflect how these models are used in reality and models are embedded in a process or a
product or even complex people interactions and i love this right because i talk about ml devops
and machine learning models in isolation are irrelevant it's the people in the process that's
where the complexity is even with intelligence itself it's a process right you know intelligence
is the interaction between a brain a body and an environment and you know within the context of
this process you know we've got all of this rich information that we could be bringing in from other
disciplines and you're saying we should bring in compsci and stats folks and we should be bringing
in psychologists and social scientists and we need to also have interpretability at a higher level
at the institutional level right or at the society level so when you kind of broaden the discussion
out a little bit i think it adds a nice bit of flavor yeah so it's very especially as a scientist
it's so convenient to just have this fixed model a fixed data set and then you just geek out and
invent all these methods and so on but reality is that that you use the method some place and then
it interacts with the institution it's built with the developers it's built by with the people it
affects and my favorite example there is when you have this closed loop where your model makes
predictions and these predictions generate the next generation's data so for the next generation
of the model it produces the data so there's this example of the rent index where you have this model
that tells you how much rent you should like pay for a certain kind of apartment and so on
and this is actually like um legally binding so if you're a land lord you have to accept kind of
the range that is outputted by the model which also means that the data that is produced so the
new flats that are rented out in new apartments they all have to fit the model kind of and then
but then you use this data again to train your model so you have this very weird feedback loop
and I think it's also difficult to wrap your head around it and understand implications of it
that that same thing a very similar feedback loop was a fear in the you know in our algo
shambles video about the uk testing since they couldn't conduct the the uh what was it the
a-level test right Tim they they built some some models around that and so it would do things like
well you know if this school historically never had anyone in this grade bucket then we're not
going to assign anyone to that grade bucket in that school and so it's sort of this self-perpetuating
you know feedback loop we were reading through a lot of your work and you I mean I'm just going
to hit this point head on um you don't really talk that much about AI ethics and you know there's the
f word which is the fairness word and and I don't I don't recall you ever using that word
and is that something that you've deliberately shied away from um yeah I just like um
define it as outside of the scope like to talk to to I don't know talk about ethics so um fairness
metrics or so on because I think there's a really big field on its own and I just don't know as much
like about all these things so I know a little bit like about the fairness metrics that they're
out there um I also think I mean they're kind of like research wise a little bit overlapping but
more or less separate fields I think interpretability and fairness um but of course they have some
commonalities that I mean when you kind of to for fairness you have to look it's not necessarily
inside the model but you have to study how the model behaves um and that's kind of the connection
to interpretability I would say yeah well where yeah where I see the connection is work
like yours is helping to build the tool set that will allow people to apply human you know intuition
and and ethics and evaluations to machine learning because at the end of the day a lot of these are
human moral judgments or ethical judgments and it's important that people be happy with them
because you know we have to have the population as a whole understand and accept and be able to
move forward with the increasing role that machine learning is having in our lives and building that
tool set is necessary so it's like you said very early in this talk you know what do we do just
stick our heads in the sand and ignore it and just accept machine learning models are going to do
whatever they do as long as they fly the plane or you know don't kill too many people we're okay
like I don't think that's going to work like we have to build the tool set that you're talking
about and continue this process of exploring how to better explain and interpret ML models so that
human beings can have that oversight because it's the only thing that's going to give us
comfort really as a society I suppose the reason I segue to this is we were just talking about
the process and you you mentioned some of these feedback loops because we can have a very superficial
discussion and you could say well we need to be able to represent reality better than we do and
we have a whole tool set here to identify sources of bias or you know lack of robustness etc in
models but it's so much more complex than that because these models are used in a very complex
process and you get these very very complex dynamics emerging as a result of that and I think
we're only really just scratching the surface of understanding those dynamics yeah I think so too
as I said I think in science it's always very easy to to study things in isolation like study one
type of model study one type of adjustment for a deep neural network and hopefully we will see more
work emerge on this I think I've never read the paper like I mean of course discussed implications
but really like analyze like what happens in terms of the data and the model when we have like
multiple generations for example of a model and how it changes over time but this thing I mean to
study those things also means that you have to wait for a long time until you have these dynamics
and I think in many cases it's just starting that we use these models more extensively in our daily
life I have a question is is anyone because look interpretability metrics whatever they are say
saliency maps and you know could be partial dependency plots whatever you could actually
build in some requirements of those into the objective functions when you go to train models
so for example I'm just going to come up with a crazy idea I have no idea if this is relevant at
all but somebody could say look I want all my saliency maps to be you know sets of of a
bezier curves or something like that like they have to have a certain smoothness
property and you could actually put that as a constraint in the objective function has anybody
tried anything like that yeah there are approaches so I saw one paper they added some
some parts to their objective function so that when you create line explanations with line that
they were most more stable and there are a lot of things like for neural networks you have
disentanglement that you try that the feature maps or that the nodes learn disentangled concepts
there are ways to introduce like monotonicity so that a feature can always go into the effect of
a feature can always be in one direction not to like zigzag around so there are approaches to do
this to like have okay like interpretability constraints in your modeling here yeah because
I was just thinking this can go back to Connor you know Connor was saying earlier on why don't we
just create white box models maybe we can use if the definition to a human being of white box is
that it's interpretable and understandable if we can build into the objective functions when we're
actually training the network that it has these properties then we'll actually be helping to create
more white box you know models even if they are complex that's definitely an option but I think the
issue remains the same that you with it's similar to a white box model I mean you'll make some trade
offs in the end you have to make the judgment whether so when you put more constraints I mean
you can actually also help the model of course that if you I mean if you have some inductive biases
also which which you infuse into the model which help with predicting or be more more stable
but sometimes you might maybe also trade off with accuracy and you just have to like in the end you
have this yeah this set of models where some are more accurate some better than this one
interpretability dimension the other is cheaper to deploy and then you have this so this kind of
going into direction of like automatic machine learning and you don't get like just the best
performing one but you have this parater set like well so we have multiple objectives that you want
to hit and then there's not one model that works best but you have a set of models that
that have different trade-offs between these objectives and then you have to decide what
is the trade-off that you want to do that you want to have I guess there's no getting away from it is
the interpretability it is going to get more important and more important I think you mentioned
Christoph that you know we've had linear models for hundreds of years and then there's been this
big explosion and deep learning and then would you say about 2016 to 2018 that's when interpretability
is really kicked off I what do you think do you think where's it going are we just going to get
more and more attention paid to this area I don't know if you can get more than than this I don't
know yeah but I think it's at least here to stay and I think it's important I mean it has been
important before but of course with like the push from deep learning especially and that it just
became more clear to a lot of people that we need interpretability in some sense at least
yeah of course people have attempted it before and worked on it before it's just more urgent now
I really like the bit in your book talking about what's changed recently how interpretability is
coming together as a field you know with this a unification so you know in physics we love a
big unification when you take all these different things in the past and say oh they're all just
part of this one big framework and it was chap that chap paper was amazing wasn't it saying things
like lime deep lift layer-wise propagation shapely ah forget all them they're all special cases of
these additive feature attribution methods and we can prove that this is the only one that's
theoretically valid because it has these properties of symmetry it's got the stummy property so
you know everything that's been done before in interpretability well they're all in our
framework now and shapely values are the way forward yeah they're quite uh quite famous to
shapely values yeah would you believe them then I mean I guess in their paper they're kind of
they kind of disagree with lime a bit don't they they say well lime is a count of this but
they're going to be breaking our properties of efficiency and symmetry so lime is using the
wrong weights right they should be using this yeah kernel shape weights rather than the line weights
I think that's just a different approach also to think about it I mean you don't maybe you don't
I think I think the properties are quite attractive or meaningful at least um but also also the
line approach I'm very critical about lime because it um I think it's difficult to have the correct
to know like how to parameterize your your local models um so I think I'm a bit more of a fan
of shapely values because of the theoretical properties it comes with are you talking about
that distance measure in lime where you have to be able to quantify how far away is the permutation
yeah the like the kernel width yeah which is set to 0.75 I think so I just looked it up and
I mean it's it's a very difficult question it goes to the heart of like what's local um
because like I mean you have this this kernel that decides like how much you weight
all the data points around the point you want to explain and and like how how big is this area
I think this is very dependent on your model and your data and there's no answer no easy answer to
how to set it yeah or even more generally than that this this whole notion of what does it mean
to have a local interpretation method in you know in text or vision so in in vision there's this
super pixel concept which is something that seems to make intuitive sense but but does it you know
when you create all of these different uh maskings of different parts of the input space but um
with shapely values as well that they they are a beautiful a beautiful technique especially
because the the values are are quite meaningful but if you have shared information between the
features I mean Connor and I were talking about this for example you if you had the same model
where you were predicting someone's income and you put their I don't know let's say you had salary
in the model twice then the shapely value would be divided between the two duplicate fields right
so there just seems to be so much esoterica in these IML methods right are we expected to know
all of this stuff yeah I I think I mean that's why I wrote the book um to to capture these things
that you have to know all the these these uh disadvantages of the methods where I try to
be very honest I mean because I'm not too invested in them but yeah I think that's worth all tools
that we usually have um also with statistics and so on you you have to know like um these like as
you mentioned this if you have salary twice then it will just I mean depends also on what your model
does if it just picks one of the salary features or if it itself uses both so this also something
that will define how the shapely value will look like later on um but you have to know these things
if you want to use shapely values and interpret interpret them correctly yeah because I think
philosophically we've got we've got the real behavior and then we use these interpretability
methods and then we've got the kind of perceived behavior so we've got these these levels of
modeling or or do you know what I mean simplification and it's all well and true if you are dealing
with data scientists who understand how these you know methods work that's fine but invariably
data scientists need to present this information to lay people and they are not going to understand
all of the various different trade-offs and how information is being compressed and and lost and
so on so do you see that as a as a serious problem uh yes but it's not a new problem it's with any
number that you read in any newspaper uh I mean so in a sense I mean when when you look at uh
outcomes of statistical models that uh well everyone can understand well of course not because
you need training to understand how to interpret a linear model or any regression model yeah there's
difficulty but I don't think it's new in any sense um because there's always I mean any number that
you read anywhere has a very complex process um so I don't know if you have like COVID testing
numbers it's very complex like how the number was generated maybe like how it was aggregated over many
states and like what cases it includes and which it doesn't and so the number looks very innocent
and simple but there's a very long process behind it to produce it um maybe this process is a bit
uh a bit more black box or a bit more difficult if it comes out if there's some machine learning
in between machine learning model in between to generate a number um but yeah I think this
problem is well old well let me let me challenge a little bit here on something which is okay if
I have if I have some general formula just some very general formula and then I go in there and I go
you know what this formula has five parameters and if I make this one point seven five and that one
one third and this one two and that one zero and I call this the megatron you know uh activation
potential and I go and write a paper about it that's really just an arbitrary you know kind of
selection of a bunch of numbers and then you gave it a fancy mathematical passport and you got it
published in some journal and now everybody has to memorize that as you know the megatron potential
and kind of learn about it and that's a lot of what's going on right now is that it's really just
a bunch of hacking like it's people just they don't really know a general solution and they don't
know how to solve like in general the problem they're trying to solve and so they just hack around and
then the ones that are kind of famous or demonstrate some success in a particular combination you know
competition over in this corner or something it now becomes something that's part of the lexicon
that we all have to learn and I think like I look back on this like imagine what physics was like
before Leibniz and Newton you know invented calculus it's like everybody memorizing a whole
bunch of little purpose built kind of formulas and then along comes a general framework which
now we can just learn calculus and derive the special circumstances as needed. You're onto
something really interesting there which is that with IML methods we are we are kind of
compressing information down into a representation you know and then that that is a transport that
can be understood by different people but there's a trade-off right because as you said you can learn
calculus and that's a compact framework for doing lots of stuff but it's all about the amount of
common knowledge that is required so it's possible to compress something down just to one symbol
and that symbol could represent all of that knowledge but it doesn't help you because I still
need to learn all of that knowledge. Yeah but so to me calculus was a very beautiful and simple
framework that I could learn and then once I learned that simple thing I could go and solve all
kinds of problems with it that before I would have to memorize specific solutions or like the
quadratic formula for example is a student I didn't actually memorize the quadratic formula I just
learned how to complete the square and then I would just do complete the square and if somebody
asked me what the quadratic formula was I would just quickly derive it right because it was
easier to memorize the rule and then apply the rule to any situation rather than to memorize
all these little one-off you know kinds of hacks that we come up with. You're not normal Keith
right so most people won't be able to go and understand this because I think it's well no
these IML methods are brilliant for data scientists who can it's a framework right it's a
reference of understanding so assuming that people can understand how Shapley values work
then this is a beautiful representation to reason about the behavior of models. Sure but when I
first saw Shapley values I realized immediately there's a connection in you know Bayesian analysis
to marginalization you know all we're really doing here is computing the expected marginal you know
contribution to this value it's not a probability but it's still the same procedure being done right
and I think I'm going to throw myself in with the lay people to a degree because the reason I'm
always striving for simplifications is because I don't have the capacity to memorize all these little
arbitrary kinds of hacks and but I yet I could totally understand Bayesian analysis and like I
said you know previously in some other videos statistics made no sense to me until I learned
the Bayesian framework because that was based on very simple rules that I could then reapply as needed.
I think that what you refer to Keith maybe the worst situation is with the saliency maps because
you have so many methods and they all like back propagate the gradient and to do input pixels
and to um now do you have to like learn like how dozens of these framework works or like
how to interpret do interpretation with all of these and they're all kind of variants of each
other so mostly because they just there's some ambiguity how you how you back propagate the
gradient because because of the non-linear units and stuff and a little bit differences how you
can define this and so you have this huge like a sea of many different methods I think it would be
nice therefore as you said to have some like simplification where you say okay this is like
all these methods work under this one principle basically and we have these two parameters
and that's how they differ I think that's also that some I think I wrote something in a chapter
that the police stop inventing new methods for saliency maps so I think it's enough and we
should focus more on like doing this consolidation to like understand the limitations of the methods
and consolidate them to see like what's the commonalities in which ways do they differ and so on
that's probably actually my first impression actually when I first opened the interpretable ML
book I was amazed how many different things there are I've you know heard people say ah
you can't use machine learning it's just a black box so many times it'd almost been drilled into my
head then seeing all the things you know from white box models ways of training salient models
counterfactual explanations it's what a wonderful recipe right there are so many different things
that you can do I feel like now I trust ML models more than other kinds of things because I have
this amazing toolbox of ways to understand them the thing that strikes me though is
most of these methods as we were just saying they require interpretation by a human and a human who
understands how the method works I love this concept of turning machine learning into an
engineering discipline and being able to do a lot of these tests non-interactively and I think
Marco Rubirio has done a lot of work around the counterfactual examples and the data grouping
and what excites me about these methods is they seem like methods that we could actually run
as part of an automated process we still have to set thresholds maybe we could set a threshold
that said if this if this counterfactual example flips the switch on more than one percent of
examples then fail the build that seems reasonable but a saliency method I mean how the hell do you
say well if there's lots of red pixels over here then break the build I mean it's just ridiculous
yeah yeah so I've seen interesting approaches to like using interpretability also more
automatically like when you do model monitoring you can do things like create interpretations
and see if they significantly change over time for example so then have thresholds that warn you
that hey something's going on with your model so I think that's also interesting approaches there
yeah you know Tim to your point of making this an engineering field and even making interpretability
and and understandability an engineering field I mean I think that maybe that's why I like your
book so much Christoph as I think it's it's a step towards that direction it's like let's
survey everything and more importantly let's create a finite and hopefully smallish set of simple
concepts that we can all agree on and understand that we can use to catalog you know what's out there
so please keep up the good work you know I'm interested to see where this goes so final
question for you Christoph then I wonder what's what's next for interpretability like are we going
to the point where it's going to be almost a box ticking exercise where we can say yes our process
we've done the standard interpretability step I mean is it is computing power going to change it
I remember when Shaq the library came out it made that approach possible whereas previously you know
it was very hard very computation infusible my friend Angem who's a wonderful data scientist
sent me n-video rapids they've got that running on GPUs way faster than of course before is it
just going to become a standard step do you think or is it going to be something where you need
decent subject matter expertise and some real thought to do to really understand how a model
works so well predictions about the future are always hard so maybe more like what I wish or
what yeah maybe think we'll have code to happen um so I mean what we're seeing already is like a lot
of implementations of these methods so they're kind of getting a commonality a rook one can use it
very easily there's a lot of libraries out there in python r but also in like this machine learning
cloud tools they also have a lot of interpretation methods available now so in that sense I think
and it's maturing a lot I still believe that we need some expertise to understand them or at
least some good references and there will also be hopefully more than my book maybe have some
documentation when for these tools and people answering on stack overflow questions and whatnot
so I think um yeah it's it's getting we're getting that everyone can use it easily
I think it should never be a box ticking exercise it's a similar thing when if you have an AI ethics
you know governance process or something the last thing you want is for it just to be an
automatic response so I've just you know yeah I've thought about AI ethics um it needs to be
something that that we really engage with I think we need to abstract away a lot of the complexity
at the moment I think it's possible to come up with an interface to standardize the way that we
do interpretability and we can reduce down what we have now to certain primitives which means that
it can plug into an engineering process and it also means that we can abstract away some of the
complexity I think that's possible yeah I also would agree that it shouldn't be like just box
ticking but you can like for the initial um when you start interpreting a model that you just have
like with a click you have a report and then it shows you the most basic things but then you still
like should ask the like the question like does it really make sense that this feature is the most
important one or what's happening there with these weird interactions between the two features
let's dig a bit deeper here and see what's going on so I think um there's this one portion that is
just like this automated reporting thing um but this should then be like the starting point for
more critical uh questioning of the model and and and checking what's going on um for some specific
problems maybe so it's going to be you click on the molnar report and it gives you the the report
from the book right yeah there will be convenience well um christoph molnar thank you very much for
joining us today it's an absolute honor to have you on the show thanks for having me hey folks this
is tim in post script there's just a couple of thoughts that didn't come to my mind during the
interview that I think I'd like to quickly cover now the first thing is on the lack of fairness
the reason why I raised that is most folks who talk about AI ethics and fairness they use the
toolkit of interpretability methods quite often you know to apply their trade there are tools out
there to mitigate fairness and to detect fairness microsoft's fair learn is a great example of this
what we really need is an operating model or a set of guidelines on how to implement these tools
how do I identify sources of problematic correlations we need to have a database of problematic
correlations having a tool that allows me to identify and mitigate bias frankly is useless
what do I do with that as we mentioned on the show many of the machine learning cloud providers
whether it's data iq or azure ml and sage maker they all have these interpretability methods built
in now including saliency maps and it's just a box ticking exercise frankly it's completely useless
there is no accepted guidance on how these tools should be used right so if I'm a large company
and I'm building an operating model around how to implement fairness techniques just having the
technology is irrelevant it's about the people and the process and the the kind of operating
model of how we implement it and there is basically no useful information out there to help us do that
the other thing is we spoke about this becoming an engineering discipline which is to say what if we
could create an interface to abstract away some of the vagaries and esoteric of interpretability
methods we might come up with some primitives or some common language and then we can hide the
complexity behind the interface this is kind of what we do with mo dev ops already we automate
as much as we can then we templatize and remove friction out of the process we even create building
blocks using domain specific languages or yaml files and pipelines and and so on so what we do
is is we create a level of abstraction where people can compose together pipelines remember
when Conor made the comment that this might just become a box ticking exercise and this is something
we see in security and AI ethics already we can't really trust people to self report that the model
is behaving correctly or that the project has no concerns from an AI ethics point of view the whole
point here is process if we want to create an operating model and ensure best practices are
followed or any kind of standardization in a large organization we have to design a process
and many eyes make shallow holes so the process would mandate that a certain number of stakeholders
were involved in assessing the particular iml technique and validating it essentially and
then we would need to record that assessment so who said what when and then if the company ever
became audited or if god forbid there was some kind of a problem where the iml model did something
wrong and it caused the company lots of damage or it harmed the environment or society or something
like that we would then be able to rewind the clock and say okay well joe blogs said it was okay
because of xyz so that is an operating model it's a process and how to design such a process again
is completely absent speaking as a chief data scientist myself that's the kind of thing that
i'm interested in and it's very difficult for me to do that i really hope you've enjoyed the
episode today we've had so much fun making it remember to like comment and subscribe and we'll
see you back next week
