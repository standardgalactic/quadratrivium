start	end	text
0	1440	So, welcome back to MLST.
1440	7320	Today, we're going to be talking about this book by Dr. Thomas Parr, Giovanni Pazzullo,
7320	9320	and Professor Carl Friston.
9320	15280	Now the book is Active Inference, the free energy principle in mind, brain, and behavior.
15280	19680	So the book, from a pedagogical perspective, it's describing active inference from the
19680	22400	high road and the low road.
22400	26920	And the high road is a little bit kind of helicopter view, so it's saying, OK, we've
26920	33640	got these biological organisms or these living systems, and what do they do in order to be
33640	34640	living systems?
34640	40560	Well, they resist entropic forces acting on them by minimizing their free energy.
40560	44800	So it goes into the how question, but it also goes into the why question from a helicopter
44800	45800	view.
45800	51080	The low road of active inference is far more mechanistic, far more mathematical, and obviously
51080	54640	both all of the roads lead to Rome, if you like.
54640	58200	But the low road is talking about things like Bayesian mechanics, there's a primer
58200	62520	on probability theory, talking about things like variational inference, which is the way
62520	69320	that we solve these intractable optimization problems in active inference, and also talking
69320	74520	about framing active inference as a process theory, which is the latest incarnation of
74520	77440	the description of active inference.
77440	80840	So Professor Carl Friston wrote a preface for the book.
80840	85040	He said, active inference is a way of understanding sentient behavior.
85040	91760	The very fact that you are reading these lines means that you are engaging in active inference,
91760	98520	namely actively sampling the world in a particular way, because you believe you will learn something.
98520	100640	You are palpating.
100640	102040	This is beautiful, by the way.
102040	107280	Friston uses the most beautiful language, it's his signature, if you like, it's his
107280	108520	calling card.
108520	113040	He said, you are palpating this page with your eyes, simply because this is the kind
113040	118480	of action that will resolve uncertainty about what you're going to do next, indeed what
118480	120800	these words convey.
120800	127480	In short, he said, active inference puts action into perception, whereby perception is treated
127480	131120	as perceptual inference or hypothesis testing.
131120	136680	Active inference goes even further and considers planning as inference, that is inferring what
136680	141520	you're going to do next to resolve uncertainty about your lived world.
141520	146440	So I'm about to show you a sneaky clip of Professor Friston that we filmed in January.
146440	151120	I might publish the full show on MLSD in the future, but I just want to take this as an
151120	154080	opportunity to thank you so much for all of our Patreon supporters.
154080	159760	Honestly, it means so much to me because the last few months I've just been, you know,
159760	164880	trying to make this activity of mine, this passion of mine, a full-time job.
164880	167960	And it's not just because you love the show and you want to support me, you get early
167960	171680	access to content, you can join our private Patreon Discord.
171680	175920	We have bi-weekly calls where we, you know, talk about all sorts of random stuff and you
175920	178160	also get early access to lots of our content.
178160	179760	So, you know, please check that out.
179760	184440	But in the meantime, here's a little sneaky clip from Professor Friston.
184440	192960	The neural network is a generative model of the way in which its content work was generated.
192960	199080	And its only job is effectively to learn to be a good model of the content that it has
199080	202120	to assimilate.
202120	205280	If you put agency into the mix, you get to active inference.
205280	212200	And now that we've got a generative model that now has to decide which data to go and
212200	213200	solicit.
213200	218000	And that's actually quite a key move and also quite a thematic move.
218000	220120	So we're moving from perception machines.
220120	225800	We're moving from sort of neural networks in the service of, say, face recognition into
225800	235680	a much more natural science problem of how would you then choose which data in a smart
235680	243040	way you go and solicit in order to build the best models of the causes of the data that
243040	245240	you are in charge of gathering.
245560	250480	Dr. Thomas Parr is a postdoctoral scholar at the Wellcome Centre for Human Neuroimaging
250480	254960	at the Queen Square Institute of Neurology at University College London and a practice
254960	255960	in clinician.
255960	259240	Now, one of the reviews from the book was from Andy Clark.
259240	261080	He said, it should have been impossible.
261080	266200	A unified theory of life and mind laid out in 10 elegant chapters spanning the conceptual
266200	271280	landscape from the formal schemas and some of the neurobiology and then garnished with
271280	273920	practical recipes for active model design.
273920	278080	Philosophically astute and scientifically compelling, this book is essential reading
278080	281400	for anyone interested in minds, brains and action.
281400	283960	Well, I mean, thank you very much for having me on.
283960	285920	So I'm Thomas Parr.
285920	291760	I'm both a clinician and a theoretical neuroscientist.
291760	297160	So I've been working in active inference for a number of years now since I did my PhD
297160	303880	back in 2016 with Carl at the theoretical neurobiology group at Queen Square.
303880	310760	And I'm now based at Oxford where I split my time between research and clinical practice.
310760	313880	So tell me about the first time you met Carl.
313880	319720	The first time I met Carl, I was considering, so I was a medical student at the time at
319720	323600	UCL and I was considering doing a PhD.
323600	327320	And I remember arranging to meet with him and obviously being a relatively nerve-wracking
327320	333760	experience meeting one of the most famous neuroscientists in the world.
333760	337360	I remember discussing with him about it and saying, you know, this is what I'm interested
337360	338720	in.
338720	343760	Would you consider supervising my PhD if I were to get the funding for it?
343760	348840	And I remember he said, yes, all right, then, anything else.
348840	351400	And I asked, do you want to see my CV or anything like that?
351400	353880	And he said, no, I'll only forget it.
353880	354880	Yes.
355560	359000	That was my first encounter with Carl.
359000	365040	But since then, he's always been immensely supportive and has been, you know, exactly
365040	371360	the sort of mentor that I think anybody would want to be able to develop a skill set and
371360	372760	sort of proceed in science.
372760	375520	I come from a machine learning background.
375520	381440	And since discovering active inference and Carl's work, it's really broadened my horizons.
381480	386120	And at the moment, there's an obsession with things like chat GPT.
386120	392120	And I just wondered in your own articulation, how would you kind of pose the work that you
392120	394720	do in relation to that kind of technology?
394720	396120	It's a good question.
396120	402120	And I suppose there are many levels at which it could be answered, aren't there?
402120	406560	I guess thinking about something like chat GPT in that style of technology, it's clearly
406560	410440	been very, very effective at what it does.
410440	413560	But it's worth thinking about what is it that it does?
413560	417080	And I think chat GPT is an excellent example because so many people are familiar with it.
417080	423880	It has such impressive results in terms of being able to simulate very effectively what
423880	426960	it's like to have a conversation.
426960	431880	But ultimately, it is like most deep learning architectures.
431880	433960	It's a form of function approximation.
433960	442120	It's a form of being able to capture very well the output that would be expected under
442120	446000	some set of conditions given some input.
446000	452040	So you give it some text and it knows which text to predict.
452040	453040	And it's very good at that.
453040	455360	But in a sense, that's where it stops.
455360	458520	It doesn't necessarily do anything else.
458520	464120	That's very different to what you and I do when we engage with the world around us, when
464120	468040	we want to learn about the world around us, when we want to form our own beliefs about
468040	469120	what's going on.
469120	473040	And those are the things that I think it doesn't have in the same way.
473040	479920	It certainly can't act and go and seek out specific exchanges, specific conversations
479920	482360	that it might want to learn from.
482360	485440	Whereas you or I might do that if we wanted to know about something specifically, we'd
485440	488600	go and look for information about that thing.
488600	493560	And I think that's where active inference and the idea of having a generative world
493560	498560	model and understanding of what's there in your world that you can alter yourself, that
498560	504240	you can change is very different to a lot of more passive artificial intelligence.
504240	508640	Probably the point where things become closer is in fields like robotics, where you have
508640	510080	to account for both of those things.
510080	515840	You have to model a world that has yourself in it, where your actions affect the data
515840	516840	that you get in.
516840	520560	And I think that's probably where more of the convergence is likely to happen.
520560	521560	Yes.
521560	528640	So you're describing the difference, I guess, between an observational system and an interactive
528640	529640	system.
529640	536000	So in an interactive system, an agent can seek information and change or bend the environment
536000	537520	to suit its will.
537800	543040	Just to linger on this for a second, though, there are folks who do argue that neural networks
543040	547560	are more than hash tables, because I think of them the same way you do.
547560	550240	They essentially learn a function.
550240	555320	And if you densely sample it enough, just like a hash table, it can go and retrieve what
555320	557880	that function says given a certain input.
557880	563000	But there are folks who say, no, no, no, these models learn a world model.
563000	569120	So is given as an example, or with SORA, they say it's learned Navier stokes.
569120	570600	It's a really good question.
570600	574240	And I think there are some open questions here, and I wouldn't claim to have all the answers
574240	575960	to this one.
575960	582160	I think to be able, again, to take chat GPT, to be able to give the answer it does, clearly
582160	584880	it has captured something about the statistics of language.
584880	589720	It's uncovered something about the hidden causes.
589720	594240	So you could argue there is potentially an element of world modeling in there that is
594240	596240	left implicit.
596240	601280	I think it would be very difficult to pull that out or to sort of see that with any transparency
601280	603480	with something like chat GPT.
603480	608640	And so if it does have something of that sort, probably it's the methods that neuroscientists
608640	611960	have been using for years to understand the brain that might help to try and pull out
611960	616960	those same things in those sorts of architectures.
616960	621920	Maybe some sorts of deep learning and neural network models are very good at picking up
621920	627840	regularities in terms of dynamics as well and being able to predict trajectories.
627840	635840	And I think it's important to say that describing something as a function approximator is not
635840	637360	to criticize or belittle it.
637360	640480	It's a very important thing to be able to do.
640480	644200	And it may also be very important in certain types of inference.
644200	649280	So for instance, things like variational autoencoders are based upon often deep learning
649280	651600	neural network architectures.
651600	655800	But the function that is learned is the one that maps from the data I've got coming in
655800	659800	to the posterior beliefs or the parameters of the posterior beliefs that I would arrive
659800	665040	at were I to perform inference of the sort we might do in active inference.
665040	670280	So you've written an absolutely beautiful book on active inference.
670360	678800	And active inference, in my view, it's a theory of agency, which is to say it describes what
678800	679800	an agent does.
679800	681120	And I'm fascinated by agency.
681120	684760	But could you just start by, I mean, from your perspective, could you introduce the
684760	687080	book and tell us about your experience writing it?
687080	688080	Of course.
688080	693400	So the active inference book that we've written is a collaboration between myself, Giovanni
693400	698840	Pazzullo is based in Rome and Carl Friston, who has to take credit for development of
698840	702080	active inference in the first place.
702080	711520	And the book sort of rose out of our sense that there wasn't a unified book out there
711520	716200	or a resource out there to help people learn about what is ultimately a very interdisciplinary
716200	718120	field.
718120	722800	And so we've all had experience with students coming to us asking for resources, asking
722800	724200	what they need to read.
724200	728120	And it may be we refer them to a little bit of neuroscience work, a little bit of machine
728400	734440	learning, textbooks or specific pages on variational inference or whatever else, giving
734440	739840	people introductions to or places they can learn about the maths they need to be able
739840	740840	to do it.
740840	744840	But then also the biology, the underlying psychology, the long sort of tradition of
744840	749320	previous scientists who worked in related areas.
749320	752520	And so the book was an attempt to try and provide a place that people could find all
752520	756840	of that, or at least references to all the relevant things they needed for that, to stick
756920	762120	to the same sort of notation, which is one of the things that's often very difficult,
762120	767800	and the same formalisms and try and introduce everything in a very systematic way to people.
767800	774920	So I'm pleased here that you found it useful, and I hope other people will as well.
774920	781880	The experience of writing it, I mean, so that took place over several years, partly because
781880	785240	the pandemic got in the way in the middle.
785240	790600	So Giovanni and I were passing notes between one another over email and weren't able to
790600	793000	sort of meet in person to discuss it during that time period.
795160	801320	But I think we're all quite proud of the result that we've got out of that, and people seem
801320	802920	to have responded quite well to it.
802920	808200	You start off by talking about what you call a high road and a low road to active inference.
808200	809400	Can you sketch that out?
810280	816920	Yes, and I think this was one of Giovanni's very nice ideas about how to introduce it,
816920	819640	because as I say, it's very multidisciplinary.
819640	823640	There are lots of ways into active inference, and one of the things that's most difficult
823640	827320	for people who are getting into the field for the first time is knowing where to start.
827320	833080	Do they start dealing with the Bayesian brain, unconscious inference, and Helmholtzian ideas
833080	839080	like that, or do they start from a physics-based perspective and start working their way towards
839800	841560	something that looks like sentience?
842600	846120	And there are lots of different, lots of alternative ways people get into it.
846120	849800	The fact that you become interested via machine learning, the fact that other people
849800	854360	have become interested through biology, I developed an interest through neuroscience
855720	856920	while I was at medical school.
859240	861960	And the high road and the low road was a way of just trying to acknowledge
862760	868520	that difference or that difficulty of knowing where to begin, and saying that that's okay,
868600	872120	there are lots of different roads, but they ultimately end up in the same place.
873080	877640	The idea of the low road was to say, well, let's just take observations and psychology
877640	880760	sort of development of a number of ideas that are built up over time
882440	887320	that come to the idea that we're using internal models to explain our world,
887320	892360	that the brain is using something like Bayesian inference, or at least can be described as using
892360	899400	Bayesian inference, and go from there through the advances that lead you to active inference,
899400	904520	the idea that it's not just a passive process that you're also inferring what I'm going to do.
905640	910040	And furthermore, that when we're doing inference, we're changing our beliefs
910760	914840	to reflect what's in the world around us and to explain our sensory data.
915480	918840	But actually, when we're acting in the world, we can also change the world itself
918920	925400	to better comply with our beliefs. So it's that move from purely changing our beliefs to reflect the
925400	931080	world to also changing the world to reflect our beliefs. And that fascinating move that
931080	935720	actually both can be seen as optimization of exactly the same objective that they have the
935720	941800	same goal, that in both cases, it's really just improving the fit between us and our world.
943000	947720	So that's the sort of low road perspective. The high road perspective was the idea of saying,
948360	952920	well, let's start from the minimum number of assumptions we can, let's start from first
952920	960360	principles. And that takes a much more physics based approach. It says, if you have a creature
960360	966360	that is interacting with its world, then there are a number of things you've already committed to,
966360	971000	and that includes things like the persistence of that creature over a reasonable length of time,
971000	974920	the maintenance of a boundary between that creature and its world, and that sort of
974920	981880	self world distinction. And once you've committed to those things, you can then start to write down
981880	986520	the constraints that those imply in terms of the physical dynamics of that system.
986520	990760	And you can start to interpret those dynamics in terms of the functions they might be
990760	996920	optimizing, much like, much like if you were to write down the equations that underpin Newtonian
996920	1001880	dynamics, you can write them down in terms of their flows on Hamiltonian functions. And it's
1001880	1006600	following the same sort of logic to then get to flows on free energy functions, where free energy
1006600	1012120	is just a measure of that fit between us and our world. And so both roads ultimately end up leading
1012120	1020600	to this common endpoint, which is that to be an agent in our worlds, in the sort of worlds we
1020600	1027160	live in, we have to be able to change our beliefs, to reflect what's going on around us and change
1027240	1034840	the world through our dynamical flows on a free energy functional, to best fit with the sorts
1034840	1039400	of creatures we are. When we first started looking at the free energy principle, we were talking
1039400	1047400	about things. It was known as a theory of every thing, every space thing, which is to say, roughly
1047400	1055160	speaking, if a thing exists, what must the thing do to continue to exist? And just their continued
1055160	1061640	existence resisting entropic forces is what defines them, which gets us into the second law
1061640	1066280	of thermodynamics. Now, that sounds like quite a strange thing to say. Why do things need to
1066280	1071640	resist entropic forces? And I think there's a development in how a lot of these ideas are
1071640	1077880	presented over time, which you expect and hope for in science. And I think we've often taken
1077880	1083800	different perspectives at different points in time as to how we explain these ideas.
1084760	1091000	And resisting entropic forces is an idea that I think most people find relatively intuitive.
1091640	1099960	So the idea that the physical systems will tend to increase their entropy over time,
1101640	1108200	at least close systems, so that over time things will gradually dissipate, things that are highly
1108200	1112200	structured and highly ordered and can only exist in a very small number of configurations and
1112280	1117240	more likely to change into something that can exist in many different configurations than they
1117240	1123640	are to go in the opposite direction. But anything that persists over time and maintains its form
1123640	1129640	clearly resists that process of decay, at least to some extent, or at least for some period of time.
1131320	1136440	However, the opposite is also true. We're also not creatures that tend towards a zero entropy
1136440	1142840	state. We don't end up in a single configuration. We have to be flexible. We have to change in
1142840	1151320	various ways throughout our lifetime or even throughout our daily routine. So it's not quite
1151320	1157960	as simple as just saying you have to resist entropic change. It's more to say that entropic
1157960	1162760	change or the amount of entropy that you expect to develop has to be bounded both from above
1162760	1168600	and below, that there is a sort of optimum level to be at. And that optimum probably varies from
1168600	1173560	different, well, from person to person, from creature to creature, from thing to thing.
1174120	1179240	You could imagine a rock that doesn't need to do much. Its interface with the environment is
1179240	1187160	quite trivial versus us as agents. We are incredibly sophisticated. So for us to continue to exist,
1187160	1191080	we have many more ways of interfacing with the environment and we need to plan
1191080	1196520	many more steps ahead. So is that just a pure continuum between rocks and people?
1199160	1207880	I mean, in principle, yes. I mean, the notion of that persistence, of that resistance of entropy
1207880	1211720	will depend very much on what you are. And as you say, you could imagine a whole scale of
1211720	1217960	things in between. I mean, in a way that as you've highlighted with the rock,
1217960	1221560	some of the most boring things are the things with the, sorry, I shouldn't say that,
1221560	1228600	poor geologists who might find rocks very interesting. And I'm sure are very complex, but
1230760	1235720	from a sort of behavioral perspective, clearly things like us are much more interesting to study
1235720	1240600	than things like a rock. And part of that is that we actually have a higher degree of entropy in
1240600	1247640	how we live our daily lives compared to things like, I almost said organisms like rocks,
1247640	1253080	but things like rocks that are not behaving. The reason I was thinking about this is the
1253080	1258520	second law of thermodynamics was conceived, I don't know, 150 years ago or something like that.
1258520	1263720	And many people at the time thought that it was an affront on free will. I think the religious
1263720	1269560	people at the time were aghast at the idea that things were mapped out in this way.
1269560	1275560	It's always worth saying in this discussion that obviously the tendency for entropy to increase
1275560	1282520	from a physical perspective generally relates to closed systems of which we are not. And as soon
1282520	1287640	as you start talking about different compartments and interactions between them, you also introduce
1287640	1293560	the idea of several coupled systems. And so you can start to ask questions about the overall entropy
1293560	1301720	or the entropy of specific parts of that system. And agents and worlds are two compartments and
1301720	1307160	systems that exchange things with one another. And so are not closed systems almost by definition
1307160	1313000	that a closed system, again, from a sort of neuroscience standpoint is not necessarily
1313000	1320840	a very interesting system. So probably that deals with a large part of that. The question of free
1320840	1325720	will is always an interesting one and always a thorny one that I'm not going to claim to have any
1325800	1332920	expertise on or be able to answer. But I think it probably tackles a slightly
1332920	1339400	different thing from a cognitive science perspective, which is whether or not we believe
1339400	1344920	that the actions we're taking are actions that we've chosen. And that probably comes back into
1344920	1350440	another aspect of active inference, which is that idea that the way we're regulating our
1350440	1357160	worlds, the way we're perhaps changing the entropy of our environment depends upon our own
1358680	1365160	choices about it, our inferences about which one we're going to do next. And that feeds into things
1365160	1372600	like we've spoken about free energy, that that quantity that we use to both choose our actions,
1373480	1378040	an act in the world around us while also drawing inferences. But we can also talk about things
1378040	1383640	like expected free energy, which is a way of evaluating our future state and what would be a
1383640	1389400	good trajectory or a good way for the world to play out. And their entropy has a completely
1389400	1395560	different meaning and there are different sorts of entropy. So for instance, if I were choosing
1395560	1399240	between several different eye movements I could make while looking around this room,
1401320	1406840	the best eye movements I might choose are those for which I'm least certain about what I would see.
1406840	1412280	In other words, the highest entropy distribution. So once you start planning in the future and once
1412280	1416920	you start selecting things to resolve your uncertainty and to be more confident about the
1416920	1422680	world around you, you actually end up seeking out entropy, which it seems to then very much
1422680	1426200	contradict some of the other ideas that we were talking about, the idea that we're constantly
1426200	1432200	resisting it. But actually it's by seeking out the things that we're least certain about that we
1432200	1437800	can start to resolve that uncertainty and start to become more confident and more certain about the
1437800	1442920	world around us. Yes, resist entropy by seeking it out. That's a bit of a paradox. But even what
1442920	1449560	you were saying just a second ago about this description of how agents operate, it's very
1449560	1455320	principled. We were talking about this balancing epistemic foraging versus sticking with what you
1455320	1463080	know. And more broadly speaking, thinking of agency as this sophisticated cognition of
1463080	1467400	having preferences and bending the environment and so on. And I guess where I was going before
1467400	1475160	is it's tempting to think that this erodes free will. And I think of them quite adjacently in
1475160	1481480	my mind. If anything, I guess I would call myself a free will compatibilist, which means it doesn't
1481560	1489800	matter that it's predetermined. For me, free will, I'll try not to use the word free will, but
1489800	1494360	thinking of agency in this sophisticated way, whether it's predetermined or not is irrelevant.
1494360	1502360	It's the complex dynamics that distinguishes my agency from somebody else's. So I think agency
1502360	1507960	is better to think of than free will, if that makes sense. Yeah. And I think that's probably right.
1508200	1512600	And the experience of and the inference of agency as well, I think is part of that.
1513800	1517800	There's a potential link that you can draw here also to the idea of chaotic,
1517800	1527000	dynamical systems of which we essentially are examples. And the idea of chaos in that setting
1527000	1533960	is that if you start from two ever so slightly different initial conditions, your path and your
1533960	1539000	future may unfold in a completely different way. And I think that fits very nicely with what you're
1539000	1545000	saying about distinguishing my agency from somebody else's because you don't see it as if I were,
1545800	1551800	you know, the time going to behave in exactly the same way somebody else's. And part of the
1551800	1556680	reason for that is that you end up starting from a slightly different perspective to where they are,
1556680	1559880	and that might lead to wildly different futures for both of you.
1560440	1569480	So something I think about a lot is whether agents are ontologically real or whether they
1569480	1574760	are an instrumental fiction. And I think part of the complexity, especially with active inference
1574760	1580920	and the free energy principle is this hierarchical nesting. So we can think of agents inside agents
1580920	1587800	inside agents. And I guess the first question is, are they real and does it matter?
1588600	1590840	Define real for me.
1596520	1606600	Well, one argument would be that they are epiphenomenal, that they themselves don't affect
1606600	1610520	the system that they are. Is this a good way to think about it?
1613960	1617480	It is a very difficult question to try and contend with, isn't it? Because I think there
1617480	1622680	are so many words that come up here that are kind of laden with different semantics or different
1622680	1629960	meanings depending upon who you speak to and which camp they come from in the sort of philosophical
1629960	1638360	world. And that's why I sort of asked you to define real. And it's really difficult to define
1638360	1644840	what real means in that setting, isn't it? And I guess coming back to your original question there,
1644840	1649960	for me, does it matter if they're a sort of real thing or not? Probably not. It matters
1649960	1655960	whether it's useful. And I guess that sort of brings me to a point about one of the things I
1655960	1661400	find quite appealing about active inference as a way of doing science. And I think,
1663720	1667960	you know, having had an interest in things like neuroscience and psychology for some time,
1667960	1671960	I often found it quite frustrating to understand what people meant and the different language
1671960	1675800	they used in psychology to understand different aspects of cognitive function.
1677320	1681400	And I think, you know, it's worth acknowledging that actually lots of people mean completely
1681400	1688520	different things when they say attention. And some people say attention to mean the sort of
1688520	1693640	overt process of looking at something and paying attention to it. Other people use it to talk
1693640	1699720	about that the differences in gain in different sensory channels that they're trying to pay
1699720	1704120	attention to or not, you know, am I paying attention to colors versus something else?
1704120	1707160	And that's just turning up the volume of different pathways in your brain.
1708360	1712040	And I'm sure there are a world of other things that people mean by it as well.
1712840	1716040	But the idea of then trying to commit to a mathematical description of these things
1716600	1721320	means that a lot of that ambiguity just disappears, that if you put a word to a
1721320	1725320	particular mathematical quantity, as long as you define what that mathematical quantity is
1725320	1730600	and how it interacts with other things, then a lot of that ambiguity just isn't there.
1730600	1736200	And it forces you to commit to your assumptions in a much more specific way.
1737160	1741960	And so that's why I come back to say, does it necessarily matter if an agent is real or not?
1741960	1747640	I don't really know what that means. But if an agent is just a description of something that is
1747640	1752280	separated from its environment that persists for a certain length of time, that has a dynamical
1752360	1756600	structure that can be written down and a set of variables that can be partitioned off from another
1756600	1763240	bit of the world. For me, that's real enough to be useful. And so that's where I'd go with that one.
1763880	1771800	Yes, yes. This is fascinating. So it's a mathematical theory that carves the world up
1771800	1775720	in an intelligent way that explains what things do and what they don't do.
1776360	1782120	And I guess the ontological statement, maybe we can park that to one side,
1782760	1787320	because as you say, from a semantics point of view, people have very relativistic
1787320	1793080	understandings of things. And there's always the philosophical turtles all the way down. Well,
1793080	1797640	is it really real? Is it really real? But one thing that is interesting, though,
1797640	1803560	about active inference is that it's quite mathematically abstract. So when we were
1803560	1809560	saying, is it real? It doesn't even designate, is it physical? So for example, a boundary is just
1809560	1816520	talking about the statistical independence between states. And those don't necessarily
1816520	1821960	correspond to physical things. So I guess it could be applied to almost anything. It could be applied
1821960	1827640	to culture or memes or language or something like that. And it has been. Yes, indeed.
1828520	1833640	Yeah, it's a good point. And then you end up sort of dragged into the questions of what is
1833640	1839000	physical. What does that mean? Is physical just an expression of dynamics that evolve in time?
1839000	1842680	Because I mean, even committing to a temporal dimension tells you something about the world
1842680	1848760	you're living in. Are the boundaries that we're talking about, are the partitions,
1848760	1855960	are they spatial in nature or not? And, you know, I remember there was an article a little
1855960	1862040	while back that sort of made a lot of argument about this as to whether the partitions that
1862040	1867160	divide creatures from their environments are equivalent to statements of conditional independence
1867160	1872200	of the sort that are seen in machine learning or various other things. And arguing that there's
1872200	1879720	something inherently different about a physical boundary. For me, I was never completely convinced
1879720	1883800	by that, but partly because you have to then define what you mean by a physical boundary.
1883800	1887960	And I suspect it's the same sort of boundary, it's the same sort of conditional
1887960	1894680	dependencies and independences. But where those have specific semantics, whether those be temporal,
1894680	1900840	whether they be something where, you know, you can actually define a proper spatial metric
1900840	1906840	underneath the things that you're separating out. And clearly, that sort of boundary is very
1906840	1911640	important. But for me, that is just another form of the same sort of boundary. And as you say,
1911640	1917160	you can apply exactly the same sort of ideas to things that are not spatial, not sort of physical,
1917160	1927240	whatever that might mean. Yes. Yes. Because when I when I spoke with Carl last time, I was pressing
1927240	1934280	him on this idea of a non physical agent, and he was quite allergic to the idea. And I suppose,
1934280	1939160	even though mathematically, you could apply it first to other geometries, that would be
1939240	1943160	quite easy, because they have certain mathematical properties in terms of like, you know,
1943160	1947400	being locally connected and measure spaces and all that kind of stuff. But if you did say,
1947400	1955800	okay, I want to have an agent that represents a meme. How would that act? I don't know,
1955800	1966040	you get into modeling challenges, don't you? I suppose you do. I think the modeling challenge
1966040	1971080	is defining the boundary. I think the boundary is a very difficult thing to define sometimes
1971080	1975720	when you're dealing with something non spatial. That boundary, though, might be reflected in
1975720	1984440	the interactions between a meme and a community that that engage with it. It might be to do with
1984440	1989560	the expression of a meme in different parts of, I don't know, a network of some sort or social
1989560	1995480	network. I don't know how easy it would be. I've not tried to do it in that context. And I think
1995480	1999240	with many of these things, you never really know until you've had to go at doing it. But
2001400	2006280	I suppose the key things I would be thinking about are, is there a clean way of defining
2006280	2011240	a boundary for a meme? Is there something that the meme is doing to the outside world?
2011960	2014440	Is there something that the outside world is doing to the meme?
2016680	2021800	And I think if you're able to define those things convincingly, then perhaps there is a form of
2021880	2029960	agent that may be non physical, if that's how you choose to define it. But then I'm not sure what
2029960	2033560	physical means in this setting. Is there also an account of saying, well, actually,
2033560	2036600	if you can write down the dynamics of how a meme propagates through a network,
2037320	2040680	is that any different writing down the dynamics of another sort of physical system?
2041800	2049080	Yes, possibly not. But it is really interesting to me that something like language could be seen
2049080	2054440	as a life as a super organism, or even something like religion. And it seems to tick all of the
2054440	2061640	boxes that we talk about with a gentleness in physical agents, which is to say, let's say
2061640	2067560	a religion or even nationalism, you could say that the state of the Netherlands has certain
2067560	2075880	objectives. And clearly, there's a two way process here. So the state affects our behavior. And we,
2076360	2083000	our collective behavior influences the state. But this then, I think the reason why people don't
2083000	2089000	like to think in this way is we have psychological priors. So we are biased towards seeing a
2089000	2095960	gentleness in individual humans, but we tend not to think of non physical or diffuse things as being
2095960	2101480	agents. Yes, I think that's probably right. And again, it sort of brings us back to this whole
2101480	2107480	issue about the language that we use, that it comes laden with lots of prior beliefs about what it
2107480	2116360	means, which may vary from person to person. And there comes a point where you say, how important
2116360	2120920	is it that I commit to using this particular word to mean this particular thing in this setting?
2121720	2129880	But again, in your example of taking a nation or nation state as being a form of organism at a
2130120	2137400	higher level or form of agent, if you can show that there is a way of summarizing the dynamics of
2137400	2143960	that system, maybe some high order summary of the behavior of people in that system, voting
2143960	2150360	intentions, I don't know, you might then be able to show that it behaves in exactly the same way
2150360	2157480	mathematically as individuals within that system. Yeah. So this brings me on a little bit too. I've
2157480	2162040	been reading this book called The Mind is Flat by Nick Chaito and I'm speaking to him on Friday.
2162040	2167720	And his main take is that, I guess you could call him a connectionist, he's friends with Jeffrey
2167720	2175080	Hinton. And his main take is that there is no depth to the mind. So for years, psychologists have
2175080	2183240	built these abstract models to reason about how we think. So we do planning, and we do reasoning,
2183240	2188440	and we have perception, and we do this, and we do that. And also, we try and generate explanations
2188440	2194760	for our behavior. So we do this kind of post hoc confabulation. But when you study it, it's incredibly
2194760	2201960	incoherent and inconsistent. And he was talking all about how the brain is actually a kind of
2201960	2209640	predictive system, right? So we have these very sparse incoherent inputs, and we sometimes see
2209640	2214840	things that aren't there. And I think you speak about this in your book that there was a really
2214840	2220760	big shift. I think you referred to it as the Helms-Hotsian idea that the brain is a kind of
2220760	2226440	prediction machine, rather than our brain just kind of like building a simulacrum of the world
2226440	2233000	around us. I mean, how do you think about that as a neuroscientist? Yeah, I mean, I think prediction
2233000	2239480	has to be a key part of it. And the reason it's a key part of it is that it's a way of coupling
2239480	2245640	us to our world that without prediction, you know, if you're purely simulating what might be going on
2245640	2249960	without actually then correcting your simulation based upon what's actually going on or the input
2249960	2255240	you're getting from the world, then you're not going to get very far. So prediction is just an
2255240	2260520	efficient way of dealing with the issue of how do I update my beliefs? How do I update if you want
2260520	2264520	to call it a simulation? My simulation, my internal simulation of what's going on outside.
2265240	2271400	And once you cease to have that constraint, once the world ceases to constrain the simulation,
2271400	2275480	that's the point at which you start, as you say, hallucinating, seeing things that aren't there
2276360	2280920	and developing beliefs that just bear no relationship to or little relationship to reality.
2282200	2289160	Yeah, interesting. So I mean, one thing this Nick Chaitaguay was saying was that we see a
2289160	2295640	complex system and we adopt what Daniel Dennett calls the intentional stance. And that is I have
2296360	2303080	a self model, I have a model of your mind, and I observe behavior and I kind of impute
2303960	2310200	onto you a model and I can generate explanations. So as I say, Thomas did that because he must
2310200	2316280	have wanted to do this. And I guess you could argue that all of this is just a confabulation.
2316280	2324040	It's just an instrumental fiction. It's a way for us to explain behavior, but it doesn't really exist.
2324040	2328920	But then there's the question of, well, it's not that it doesn't exist. It's just that your mind
2329480	2335960	is incomprehensibly complex. So it's not that the mind is shallow. I prefer to think of it as
2335960	2341480	the mind has so much depth that it's beyond our cognitive horizon. And depth, I think, is an
2341480	2348120	interesting notion as well. I mean, it's the idea that comes under a lot of machine learning and
2348120	2354280	the idea of deep learning neural networks with multiple layers. And I think you're right that
2354280	2359880	depth is an important part of our generative models as well, of our brains models of the world.
2360840	2367720	And part of that comes from the fact that the world actually does separate out into a whole
2367720	2372760	different series of temporal scales of things that happen slowly, that contextualize things that
2372760	2377960	happen more quickly, that contextualize things that are even faster than that. And so one good
2377960	2382760	example of depth might be that if you're reading a book, then you have to bear in mind which page
2382760	2387160	you're on within that page, which sentence or which paragraph you want, within paragraph,
2387160	2392840	which sentence, within the sentence, which word, within the word, which letter. And by combining
2392840	2397720	your predictions sort of both down the system that way, but then updating your predictions
2397720	2402680	all the way back up again, you start to be able to make inferences about the overall narrative
2402680	2411400	that you're reading. The other thing you mentioned that I thought was interesting was the idea of
2411400	2417400	confabulation and of how we come to beliefs about other people's behavior. And I think the same
2417400	2421800	thing is also true about our own behavior and sort of making an inference about what we've done.
2422440	2427160	And this comes all the way back to the sense of agency again, doesn't it? It comes back to the
2427160	2431960	idea that I am inferring, I'm behaving in this way for this reason, because I've chosen to do this,
2431960	2439400	because I had this goal in mind. And to come back to the other question, is that real? Or is it
2439400	2445560	simply an inference about what I've done? I would suggest that it's certainly an inference about
2445560	2453080	what I've done, whether or not it's real. Giovanni and I put together some simulations
2453080	2458680	and some theoretical work a couple of years ago after a discussion at a conference about or a
2458680	2464200	workshop about machine understanding, suggesting that machine intelligence is one thing, but
2464200	2469640	actually understanding why you've come to a particular conclusion. ChatGPT being able to
2469640	2475800	explain to you why it came up with a specific sequence of words or why a convolutional neural
2475800	2482280	network classified an image in a particular way is one of the big issues really, and there are
2482280	2486840	solutions coming up, but it's one of the big issues in the deep learning community as to how
2486840	2490600	you have that transparency in terms of what the models are doing and why they're doing it.
2493320	2496840	Giovanni and I put together some work following that, looking at
2497560	2501560	understanding of our own actions from an active inference perspective, and there it was very much
2501560	2506760	framed as I have a series of hypotheses of things I might do, of reasons why I might do that.
2507560	2512200	And then after observing myself behaving in a particular way, I can then use my own behavior
2512200	2518200	as data that I then have to come up with an explanation for. And it's very interesting to
2518200	2523080	see what happens if you start depriving that of aspects of its behavior and to see the confabulations
2523080	2528600	that result from that. I can't remember where it came from originally, the idea of hallucinations
2528600	2537960	being a perception generally being effectively a constrained hallucination, where you take your
2537960	2541800	hallucination, your simulation of what's going on, and then you fix it to what's actually coming in.
2542920	2546440	But you could argue that actually a lot of our understanding about what we're doing is also
2546440	2549720	just a constrained confabulation in exactly the same way.
2550680	2556200	Yes, which is very ironic because people diminish GPT and because they say it's just
2556200	2563800	confabulating, whereas the preeminent neuroscientists of the day do basically make the same argument
2563800	2569480	about how the brain works, and even our communication now on conditioning your simulator.
2569480	2573480	So the semantics are drawn by your own model in simulation of the world,
2573480	2578040	rather than being the simulacrum of mine. You spoke about machine understanding,
2578040	2584040	I mean, there's this Chinese Rem argument. And we're in a really interesting time now because
2584680	2591880	we have artifacts that behave in a way which is isomorphic in many ways.
2592440	2600680	And it's so tempting to say, well, we're different. And you could make the ontological argument,
2600680	2606280	but this psychological argument is a big one as well, which is we're different because we have
2607080	2611400	beliefs, motives, volition, desires, we have all of these things.
2611400	2615160	But as we were just saying before, this is all post hoc confabulated.
2615160	2619560	We actually don't have consistent beliefs and desires. It's just a fiction.
2621240	2623960	Was it a fiction or is it a plausible explanation?
2625480	2630920	Well, I guess the thing that breaks it for me is the incoherence and inconsistency,
2631000	2638040	because you would think that we would be fully fledged human agents if we had consistent beliefs
2638040	2642920	and desires. And it's not to say that we don't because it feels like some of our goals are
2645000	2651560	they grounded in some way, like we need to eat food. But we think of ourselves as being
2651560	2657320	unique as humans, because we have higher level goals and beliefs that aren't necessarily instrumental
2657720	2662280	to eating food. And I guess those things in particular might be confabulatory.
2663160	2667720	Yes. So on the volition thing, that's something that really interests me.
2670440	2677080	An active inference agent is we draw a boundary around a thing and it can act in the environment
2677080	2682600	and it has preferences. And essentially, it has a generative model where it can produce these
2682600	2688360	plans, these policies, if you like, and at the end of every single plan is an end state.
2689480	2696360	So it's got all of these different goals in mind, if you like. And in the real world,
2697240	2704280	real in big air quotes, these things emerge. But when we design these agents, we need to
2704280	2711080	somehow impute the preferences onto them. So it feels like they have less agency if we
2711880	2717560	impute the preferences. Would you agree with that? Interesting question.
2719800	2729800	And a very relevant question in the current number of industry related applications of
2729800	2734360	active inference. I think we were speaking about earlier, there are a number of companies now
2734360	2739800	that have been set up looking at use of active inference based principles for various problems.
2740680	2745400	Companies like Versus that we spoke about before and Stan Hope AI that I do some work with as well.
2747160	2752920	And the issue there is very much, it's a different kind of issue to the biological
2752920	2758680	issue of describing how things work. And it's the issue of saying, if I now want to design an
2758680	2763160	agent to behave in a particular way, as you say, am I taking some agency away from that?
2763160	2769400	There are a couple of things to think about there. I suppose one is thinking about
2770840	2775240	do biological agents actually select their own preferences to begin with?
2777080	2781720	And I think most people would probably say they don't most of the time. There may be certain
2781720	2786360	circumstances where they do or where a particular preference might be conditionally dependent upon
2787240	2792280	the task I'm in, the scenario I'm in, whether I'm at work or at home or whatever else. But it's
2792280	2797160	not that I'm actually selecting this is what I want to want. There is a famous quote here,
2797160	2800280	but I can't remember what it is. I don't know whether you do. No.
2803960	2811000	No, it's escaped me about wanting what you want or wanting what you do or something along those
2811000	2817160	lines. Anyway, the point I'm making is that, to some extent, our preferences are given to us
2817160	2822360	effectively through a process of evolution, natural selection, previous experience that has
2822360	2829000	affected what is a good set of states to occupy. And those will often be a good set of states that
2829000	2838920	help my survival, that help the persistence of the species that I'm a part of. And arguably,
2838920	2845240	the same thing is true when you as a designer of a particular algorithm or an agent are giving it
2845320	2850120	a set of preferences. From its perspective, it's never selected them anyway. And that's the same
2850120	2856200	as you or I not necessarily having selected our preferences. There's one additional element that
2856200	2863960	I think is interesting to think about. And one of my colleagues and collaborators,
2863960	2868600	Nor Sajid, has done a lot of interesting work on this, which is the idea of learning your own
2868600	2874280	preferences, of actually saying, let's create an agent that isn't given preferences to begin with,
2874280	2880600	but is allowed to learn as it behaves what sort of goal states it ends up in.
2881880	2888280	And there you get some very interesting results. So she showed that these sorts of agents
2889640	2893720	may end up doing things that you just don't want them to do, that they end up forming a
2893720	2898040	particular pattern of being or a particular way of being that you as a designer might never have
2898040	2903160	envisaged. For example, in an environment with lots of potential holes that it can fall into,
2903160	2908280	some of these agents just become hole dwellers. They just decide, I found that the first few
2908280	2912280	times I did this task, I fell into the hole. So I've decided I'm probably the sort of creature
2912280	2918520	that likes living in a hole. So that's a situation where you can give it a certain agency. And maybe
2918520	2924520	that agency is the ability to sort of disagree with what you as a designer might expect or want
2924520	2930120	from it. Yes. This is so interesting. I mean, we're getting a little bit into, we'll have a
2930120	2939000	discussion about cybernetics and externalism. But so what you're describing there is the reason
2939000	2946680	why AI systems today are not sophisticated is because they are convergent. And that's usually
2946680	2953240	because they don't actually have any agency. So one of the hallmarks of the physical real
2953240	2959400	systems in the real world is that they have these divergent properties. And that's because you have
2959480	2964600	lots of independent agents following their own directiveness doing epistemic foraging. So
2964600	2968760	interesting stepping stones get discovered. And sometimes those stepping stones aren't what the
2968760	2974120	designer of the system would have liked, as you just said. So there's an interesting kind of paradox
2974120	2981160	there of how much agency do you want to imbue in the agents. But the other paradox is the physical
2981160	2987480	and social embeddedness. Because as you just said, cynically, we don't have as much agency as we
2987480	2992760	think we do, because we're embedded in the dynamics around us. And being part of this
2992760	2999960	overall system means that our agency is defined not just by our boundary, but it's by the history
2999960	3005160	of the system. It's the history of us sharing information of all of the things around us.
3005160	3010680	And all of these things inform what we do and what our preferences are. And then you say, well,
3010680	3015960	we can just drop a brand new agent in the system. And it doesn't quite work because it's a fish out
3016040	3021640	of water. It's not embedded in the ways that things that emerged in that system were in the
3021640	3029080	first place. But this does get us onto this discussion of externalism. So part of the fiction
3029080	3037960	of how we think about cognition is that we think of ourselves as islands that don't share information
3037960	3042760	dynamically with the outside world. And of course, active inference is a way of bridging
3042760	3045400	these two schools of thought. So can you kind of bring that in?
3047720	3052040	I mean, I think you've already done it in a sense. I'm not sure what else there is for me to say on
3052040	3061160	that. I'll try my best. So yes, I mean, active inference is about, well, it's about aboutness.
3061160	3068360	It's the idea that our brains and our internal state evolves in such a way that reflects beliefs
3068360	3073880	about what's outside. And I think that's one of the key things that you have to have for any sort
3073880	3079320	of intelligent system. And that doesn't necessarily exist with other approaches that exist in
3079320	3085960	neuroscience or artificial intelligence. It is that, and I'll just repeat that, it's very much
3085960	3093320	being, the aboutness is the key thing that what's happening in my head is a reflection or is a
3093320	3100280	description in some way is about what's happening outside my head. And maybe that's the link with
3100280	3105080	this sort of externalism. But it's not just unidirectional either. It's the fact that
3106520	3110120	I'm forming beliefs about what's happening in the outside world, but I'm also the one influencing
3110120	3115160	the outside world to change it to fit with the beliefs I have about how it should be.
3115160	3127640	Yes. Yes. So there's a kind of model. So we draw these boundaries. And we model the world around
3127640	3134520	us. And we influence the world around us. And that's essentially what active inference is.
3134520	3141480	I guess it might be useful just to sketch out the cognitive science idea of an activism or
3141480	3147640	cybernetic. So there were folks who really railed against this idea of representationalism,
3147640	3154520	which is this idea of model building in principle. And active inference is an integrated approach
3154520	3160120	where we allow some model building, but we also think of the world itself as being its own best
3160120	3167400	representations. How do we kind of bridge those two ideas? Yes. And I confess, I'm always lost in
3167400	3171800	the distinction between the sort of inactivists, radical inactivists, the sort of different levels
3171800	3181400	of stance you can take on this. And I think it comes down to that, that from an active inference
3181400	3187240	perspective, both your representations, if that's the right word, the beliefs you have about the world,
3187240	3191640	whether or not that meets the criteria for representation from an inactivist perspective
3192600	3199080	is very important. But it is only important in terms of how you act. If your beliefs did not
3199080	3203960	affect how you acted, clearly natural selection would not have selected you to form those beliefs.
3203960	3209480	I think it's the simple way of putting it. So let's talk about some of the kind of
3210280	3216680	the mathematical underpinnings here. So I think probably one of the main concepts we
3216680	3222040	should start on is this idea of surprise. And maybe we can talk about it in general terms,
3222040	3228680	and then we can move on to Bayesian surprise. So why is surprise so important in the free energy
3228680	3236760	principle? Well, it's central to it. It is the key thing that matters. And we talk about the free
3236760	3243880	energy principle. But in a sense, free energy is really there as a proxy for surprise. So yes,
3243880	3250040	what do we mean by surprise? And it's another one of those things like the high road and
3250040	3253560	the low road that you can approach from several different angles or several different lines of
3253560	3265000	attack. If you were modeling something, if you were a Bayesian, so if you took a particular
3265000	3269720	stance on probability theory and wanted to know, given my model, given my hypothesis,
3269800	3274360	what's the evidence for it? What you would normally do is calculate something known as
3274360	3281960	a marginal likelihood, which is just a measure of the fit between your model and the data that
3281960	3293480	you have that you're trying to explain. That fit trades off various different things. So it can
3293480	3298760	trade off how accurately your model is explaining the data against how far you've had to deviate
3298760	3303880	from your prior beliefs or from your initial assumptions in order to arrive at that explanation.
3306040	3312680	So that marginal likelihood, that evidence is effectively just the negative or the inverse
3312680	3319480	of surprise. So that that's one perspective on it, the better the fit, the simpler and most
3319480	3325400	accurate my explanation for something, the less surprised I will be by it. Another perspective
3325400	3331560	on surprise is just this more colloquial sense. It's the idea that, given what I would predict,
3331560	3338120	how far out of that prediction is it? One could take a more biological perspective on it and say,
3338120	3343560	imagine we are, well, we are homeostatic systems that have some set points. We want to keep our
3343560	3347160	temperature within a certain range, our blood pressure within a certain range, our heart rate
3347160	3352920	within a certain range. If we find ourselves deviating from that, that is effectively a surprise
3352920	3357640	because we're not where we expect to be. And so we enact various changes to bring
3358840	3364840	those parameters back in range. So we might, if our blood pressure is too low, we might increase
3364840	3368520	our heart rate to bring our blood pressure back up to the range we expect it to be in.
3369400	3376200	And that is, in a sense, what active inference is all about. It's just this idea of keeping things
3376200	3382040	within that minimally surprising range. But of course, once you put dynamics on it, once you
3382040	3387880	start unfolding that in time, you end up having to not just deal with how surprising things are now,
3387880	3393560	but you've got to try and anticipate surprise and behave in such a way that you allostatically
3393560	3399960	control your sensory inputs, both your intraceptive inputs like heart rate and blood pressure,
3399960	3409160	etc., but also your extraceptive sensations, your vision, your audition, and the like.
3412040	3416120	And there's almost no end to the perspective you could take on surprise. Another perspective
3416120	3424280	on it is that it's a reflective of, in a physical system, the improbability of being in a particular
3424280	3431000	state. From a lot of physics perspectives, improbability is also associated with energy.
3431000	3438520	It takes energy to bring things into less probable states. And without inputting energy into a system,
3438520	3443640	it will generally end up in its most probable state in the absence of that.
3445320	3449320	You think of things like Boltzmann's equation and the relationship there between energy and
3449320	3458360	probability. And that also has a link then to the idea of either a Hamiltonian or indeed a
3458360	3463400	steady state distribution, which is just what is the distribution things will end up in if left
3463400	3469880	to their own devices for a certain amount of time until things have probabilistically converged.
3469880	3475000	And that means that if I would construct a probability distribution over where things
3475000	3479160	will be at a long point of time in the future, there will come a point at which that probability
3479160	3485880	won't change any further. And the tendency of physical systems to go to those more probable
3485880	3494440	states is exactly the same as the tendency to avoid surprising states. And again, we could
3494440	3498920	sort of go on for a while, but I won't on sort of other ways of conceptualizing it. But hopefully
3498920	3504440	that sort of explains why it's such an important thing that underpins so much of what we do.
3505240	3509640	We're either trying to sort of evolve as a physical system towards more probable states.
3511000	3516600	Or we are homeostatic or allostatic organisms trying to maintain our internal parameters within
3516600	3523640	the right set points. Or we are more colloquially just trying to avoid things that are different to
3523640	3530520	what we predict. Or we are statisticians trying to fit our model to the world as best we can.
3530520	3533160	And all of those things come under the same umbrella of surprise.
3534920	3539160	Free energy comes in because surprise is not a trivial thing to compute.
3541640	3547080	Mathematically, it's often either intractable mathematically or computationally. And so it's
3547080	3552120	just not efficient to be able to calculate. But free energy is a way of then approximating
3552120	3557240	that surprise. It's a way of coming up with something that is close enough to it. Or
3558040	3562840	even more precisely as an upper bound on surprise. So if you're at the lowest point of your free energy,
3564040	3573160	then that limits how high your surprise can be. The key additional thing in free energy is that
3573160	3578760	the distance between that bound, your free energy and your surprise depends on how good your beliefs
3578760	3584120	about the world are. And that's where perception comes in. That by getting the best beliefs you
3584120	3590040	possibly can, you minimize the distance between your free energy and which is up a bounding of
3590040	3594920	surprise and the surprise itself. So then any further reduction in free energy, you would expect
3594920	3599720	to also result in a decrease. Sorry, any further decrease in free energy would also result in a
3599720	3604680	further decrease in surprise. I mean, there's a few things that struck me. I mean, first of all,
3604680	3611880	what struck me is that we're using the language of things like statistical mechanics and Bayesian
3612280	3619000	statistics and information theory, things like entropy and so on. And we're interchangeably
3620120	3624040	kind of speaking about the same thing from the perspective of different disciplines,
3624040	3631960	which I find very, very interesting. And on the surprise thing, even though in this formalism,
3631960	3638040	we are minimizing surprise, I think there's an interesting perspective that sometimes surprise
3638040	3644920	is what we want. So for example, the chess algorithm, the ELO algorithm, it's only when
3644920	3651000	something surprising happens that the weights get updated because it's information. Or people on
3651000	3657640	YouTube, my videos are that they get more views when they have a cash value, which means they
3657640	3661960	have information content, which means that, you know, they're actually surprising your predictive
3661960	3666360	model. Even Arnold Schwarzenegger used to joke about it, he said, you have to shock the muscles.
3666360	3670520	You know, you have to do what the muscles don't expect. Otherwise, there's not an adaptation. So
3670520	3675160	there's this interesting juxtaposition between actually seeking out surprise, even though you
3675160	3680200	can think of our brains overall as minimizing surprise. And what was the other thing I was
3680200	3684120	going to say? Yeah, you were just getting onto variational inference, which is really interesting.
3684120	3690040	So there's a couple of intractable statistical quantities in this mixture that we're talking
3690040	3696520	about. I think it's the log model evidence and the Bayesian posterior. And we can't represent
3696520	3701480	those things directly. So we have to put a proxy in there, which kind of captures most of the
3701480	3706360	information, but it's still possible to deal with it. So how does this variational inference work?
3707320	3712440	Yeah. So I suppose maybe the first thing to think about, though, is just to recap what Bayesian
3712440	3716920	inference is. I suppose we've been talking about it quite a lot without necessarily defining it.
3716920	3724280	And many of you listeners, I'm sure, will know already. But the idea is actually relatively
3724280	3728520	straightforward and well-established and quite widely used. And it's the idea that if I have
3728520	3735640	some beliefs about things that are in my world that I can't directly observe, I may have a sense
3735640	3741080	of what's plausible to begin with. And that's what we refer to as a prior probability. I then also
3741080	3748600	need to have a model that says, given the world is this way, what would I expect to actually observe?
3749160	3756920	So for instance, given where you are relative to me, I can predict a certain pattern on my retina.
3757560	3761560	And if you were somewhere else, I would expect a different pattern on my retina. So I might have
3761560	3765720	a prior range of plausibilities as to where you are relative to me. And then I have a model that
3765720	3770840	explains how I'm going to generate some data based upon that. And Bayesian inference basically
3770840	3777720	takes those two things and inverts them using Bayes' theorem and effectively just flips both of them
3777720	3783880	round. So you now say instead of a distribution of where you are relative to me, I'm now talking
3783880	3789640	about a distribution of all the possible things that I could see on my retina. And instead of
3789640	3797000	predicting the distribution on the retina given where you are, I now want to know the distribution
3797000	3803400	of where you are given what's on my retina. And Bayesian inference, much like active inference,
3803400	3807640	is full of all these interesting inversions where you sort of flip things round from how
3807640	3814280	they initially appeared. But the problem is calculating those two things, calculating the
3814280	3819640	flipped model. So the distribution of all the things on my retina here would now be my model
3819640	3828360	evidence, my inverse surprise. And the distribution of where you are relative to what's on my retina
3829240	3833800	is my posterior distribution. But those things are not always straightforward to calculate.
3834360	3840200	And so variational inference takes that problem and makes it into an optimization problem. It
3840200	3848120	writes down a function that quantifies how far am I away from my, or what would be the true posterior
3848120	3855480	if I'd used exact Bayes. And then it says, well, let's parameterize some approximate posterior
3855480	3860120	probability. So come up with a function that represents a probability distribution that's
3860120	3864520	easy to characterize, something like a Gaussian distribution where I know I just need my mean
3864520	3870920	and my variance. And then just changes that mean and variance until you minimize this function
3870920	3876360	that represents that discrepancy, minimize this free energy, also sometimes known as an evidence
3876360	3882120	lower bound, in which case you maximize it. And interestingly, once you've maximized your
3882120	3886440	evidence lower bound or minimized your free energy, you end up with a situation where
3887720	3894200	the free energy starts to approximate your log model evidence or your negative log surprise.
3895480	3901240	And your approximate posterior distribution, your variational distribution starts to look
3901240	3908760	much more like your exact posterior probability distribution. So it's another one of those
3908760	3913800	interesting scenarios where doing one thing optimizing one quantity ends up having a dual
3913800	3918040	purpose. And in active inference, the only additional thing you throw into that is that you
3918040	3925400	want to then also change your data itself. So you do the third thing you act on the world
3925400	3930360	to then optimize exactly the same objective. The interesting thing, I guess, is just contrasting
3930360	3935800	to machine learning again. So in machine learning, we also have these big parameterized models and we
3935800	3941160	do stochastic gradient descent. And some might think of deep learning, because obviously you
3941160	3945240	can think of everything as a Bayesian. So you can think of machine learning as being maximum
3945240	3952920	likelihood estimation. Why is it that we go full Bayesian when we do active inference? Why not
3952920	3958680	something like maximum likelihood estimation? It's an interesting question. And there are a couple
3958680	3964120	of answers you could give again, some of which are more technical, but some of which are
3967240	3972440	some of which are slightly more intuitive. And I think one of the more intuitive answers is that
3972440	3978360	by having an expression of plausibility of things in advance, you just maintain things
3978360	3984120	within a plausible region. So maximum likelihood for those who are unaware is where you essentially
3984200	3989560	throw away that prior probability, where you throw away any prior plausibility as to as to
3989560	3996440	what the state of the world might be. And you just try and find the value that would maximize
3996440	4001480	your likelihood, which is your prediction of how things would be under some hypothesis or under
4001480	4013080	some parameter setting. And I think the first thing to say is if you throw away that prior
4013080	4017960	information, then you end up potentially coming up with quite implausible solutions.
4019000	4023320	That's particularly relevant if you're dealing with what's known as an inverse problem. So where
4023320	4028840	there are multiple different things that could have caused the same outcome. An example that's
4028840	4033720	often given is that for any given shadow, there's almost an infinite number of things, configurations
4033720	4038040	of the sun and the shape of the thing that's casting the shadow that could lead to exactly the
4038040	4042520	same shadow. And so maximum likelihood approach just won't be able to tell the difference between
4042520	4047880	all of those things. However, if you have some prior on top of that, if you have some statement
4047880	4052760	of the plausible things that might cause it, you can come up with a much better estimate of those
4052760	4063800	sorts of things. Another way of looking at it is that when you're dealing with a maximum likelihood
4063800	4069160	estimate, you're throwing away all uncertainty about the solution. So you're coming up with a
4069160	4073320	point estimate and you're saying this is the most likely thing, but you're ignoring all of your
4073320	4079080	uncertainty about it. And I think that is in itself a relatively dangerous thing to do and can lead to
4079080	4083880	the problem of overfitting, where you start to become very confident about what you can see from
4083880	4094920	a relatively small sample of things and you can end up with all of these well-described in the media
4094920	4100520	scenarios of complete misclassifications based upon that sort of overconfidence just because
4100520	4107960	all the uncertainty is gone. A more technical way of looking at it, I think, is if you think about
4108680	4116120	what a free energy is. So free energy is our measure of our marginal likelihood that we're
4116120	4122760	using when we're doing Bayesian inference. And one way of separating out what a free energy
4122760	4129720	looks like is to have our complexity, which is effectively how far we needed to deviate from
4129720	4135000	our prior assumptions to come up with an explanation, and our accuracy, which is how well we can fit
4135000	4142520	our model. Accuracy is common to both maximum likelihood type approaches because we're trying
4142520	4150120	to find the value that most accurately predicts our data and also to Bayesian approaches.
4150120	4156200	Both want to do that. But what's thrown away in the maximum likelihood type approach is the
4156200	4163800	complexity bit, the how far do you deviate from your priors. So there's an inbuilt Occam's razor,
4163800	4170200	the idea that the simplest explanation is a priori more likely that you get from a Bayesian
4170200	4174200	approach that you throw away when you're dealing with maximum likelihood estimation.
4175160	4181480	I wondered to what extent does the active part play a role here. So even in machine learning,
4183000	4187800	there's something called active learning, where you dynamically retrain the model,
4187800	4192280	or there's something called machine teaching, where you dynamically select more salient data
4192280	4197880	to train the model, and the model gets much better. And in things like Bayesian optimization,
4197880	4203800	for example, by maintaining this distribution of all of your uncertainty in a principled way,
4203880	4209720	you can go and seek and find more information to kind of improve your knowledge on subsequent steps.
4209720	4215240	So I guess it's sort of bringing in this idea of it's not just what happens now,
4215240	4221000	it's about how can I improve my knowledge of the world over several steps.
4221720	4225640	Yes, and that reminds me about the point you were making earlier, that sometimes we actually do
4225640	4233640	things to surprise ourselves, which seems very counter-intuitive in the context of the idea that
4233640	4241480	we're trying to minimize surprises as our sole objective in life. And sometimes people talk about
4241480	4246040	this in terms of a dark room problem, the idea that actually if all you want to do is minimize
4246040	4250040	your surprise, you just go into a room, turn off the lights and stay there because you're not going
4250040	4260120	to experience anything that's going to surprise you. I mean, the answer to this problem is that
4260120	4268600	actually, as organisms, as creatures, we don't expect to be purely in a dark room. And the
4268600	4275320	sort of organism that would be is, again, probably not a very interesting one. And that what we predict,
4275320	4281080	what we'd be surprised by might be permanently staying in a dark room. But it goes even further
4281080	4287160	than that. And if you say, actually, I'm minimizing my surprise over time, I want to be in a predictable
4287160	4291800	world where I know what's going to happen next. The best way of doing that is to actually gather
4291800	4296040	as much information as you can about the world around you. So the first thing you do really is
4296040	4299640	you turn on the light and see what the room looks like, because that might then predict all the sorts
4299640	4303720	of things that could fall on you in that room and could potentially cause surprise. And by knowing
4303720	4309640	about it, you mitigate the surprise that you might get in the future. And as you say, you can only
4309640	4314120	really do that if you know what you're certain about. And so if you take a maximum likelihood
4314200	4318520	approach, if you work based on point estimates and you have no measure of your uncertainty,
4319320	4323560	then there's no way you can possibly know what you're uncertain about to be able to resolve
4323560	4330600	that uncertainty. So this brings me on to causality. We know that predictive systems,
4330600	4336120	which are aware of causal relationships, work better. But if we just bring it back to physics
4336120	4341080	first, I mean, to you, what do you think causality is?
4342200	4347160	It is a tricky issue as to what causality is. And I think whether it exists or not is really a
4347160	4354840	matter of how you define it, isn't it? And some would define that purely in terms of conditional
4354840	4361320	dependencies, that the behavior of one thing is conditionally dependent upon something else,
4361400	4366040	and therefore you could say that the one thing causes the other. But as we know from Bayes'
4366040	4370600	theorem, that's not quite good enough, because you can swap any conditional relationship around
4372440	4379000	through that process of inverting your model. Sometimes that causality is written into the
4379000	4385480	dynamics of a model. So this would be the approach used in things like dynamic causal
4385480	4390520	modeling of brain data, where you might say that the current neural activity in one area of the
4390520	4395160	brain affects maybe the rate of change of neural activity in another part of the brain.
4395800	4399480	And it's the way in which those dynamics are written in, the fact that it's one affects the
4399480	4407400	rate of change of the other, that gives it that causal flavor and a very directed perspective on
4407400	4415480	it. Probably the work that is most comprehensive on this is looking at people like Judea Pearl and
4415560	4421800	a lot of his work on causality. There's a lot of detail about the notion of an intervention.
4422360	4427720	And I suppose you can think of this in terms of how you might establish causation in a clinical
4427720	4433000	context. If you were to run a trial to try and establish whether one thing's caused another,
4433000	4437400	you need to make sure you're not inadvertently capturing a correlation or a conditional dependence
4437400	4443000	that could go either way, or a common cause of both things that depends upon something else.
4443880	4449800	And typically the way you do that is you intervene on the system. You randomize at the beginning
4449800	4454040	to make sure that people are assigned to different treatment groups at random,
4454040	4460200	so that you break that dependency upon something prior to it. And then anything that happens going
4460200	4467880	forward is going to depend on the intervention that you're doing. So I think that's probably the
4467960	4473720	key thing that gives you causality or perhaps defines causality. It's the idea that an intervention
4473720	4478040	is what will change it. If you intervene in one thing, that should then in a way that doesn't
4478040	4483240	necessarily match its natural distribution if you hadn't intervened at all, and then see what the
4483240	4490520	effect is. Yes, yes. I mean, and by the way, Judea Pearl is really interesting. I want to study
4490520	4496520	his book, The Book of Why. It's one thing that we've really dropped the ball on, actually.
4496520	4503160	But I suppose one way to think about it is if you go back to the core physical, in physics,
4503160	4507240	there's a whole bunch of equations to describe the world we live in. And those equations don't
4507240	4512760	have, they don't say anything about causality, and they're even reversible. And then you can think,
4512760	4517000	okay, well, maybe it's a little bit like the free energy principle. It's a lens,
4517000	4521560	like really, there's only dynamics. But when you look at these dynamical systems,
4521560	4528600	then behaviors emerge, and somewhere up that chain, you can say, okay, now we've got causality,
4528600	4534360	and it's something which is statistically efficacious to build it into our models. But
4536440	4542440	where does it come from? Well, it comes from us, doesn't it? It's a hypothesis to explain a particular
4542440	4551080	pattern of dynamic. Yes. And we might infer causation based upon, again, a particular pattern
4551080	4555880	of how one thing reacts to another. So if you imagine you've got the classic physics example,
4555880	4563160	billiard balls bouncing into one another, how do you know that the collision of one ball with
4563160	4570280	another is causative of the subsequent motion of the second ball? And you could argue that that's
4570280	4575160	due to a particular pattern of which variables affect which other variables and the particular
4575160	4580280	exchange between them. And this comes back quite nicely to things like the physics perspective
4580680	4586440	on the free energy principle, the idea that actually one could see the location of a particular
4586440	4594520	ball as being, you know, maybe it's internal state, and then the action that that then causes
4594520	4601000	is perhaps the, or in fact, you could say that the action is the position of the ball, the force
4601000	4606840	that results from that action is the sensory state of the next ball, which then changes its
4606840	4613960	velocity to then change its action relative to something else. You can sort of rearrange those
4613960	4619800	labels slightly, but there is a directional element to it. And in that sort of pattern of
4619800	4625240	causation, you really do expect the position of one ball to have an effect on the rate of change,
4625240	4629720	or in fact, even the rate of rate of change of the second ball, which again, I think brings us
4629720	4635480	back to those kinds of dynamical descriptions of causality where one thing might affect how
4635480	4641160	another thing changes. So you almost get it from the dynamics itself. But again, to some extent,
4641160	4645800	it comes back to semantics, doesn't it? It comes back to what do we mean by cause? Well, I suppose
4645800	4649960	cause is a hypothesis as to a particular configuration of things. But then you've got to
4649960	4654840	write down what does that hypothesis mean? What's my model of what a causation involves?
4655560	4661720	Yes, yes. I mean, we were just talking about, you know, build building these models. And one of
4661720	4668520	the bright differences from machine learning is that we need to build a generative model by hand.
4669560	4674520	So we have to define these these variables, and some of them are presumably observed, and some of
4674520	4683320	them are not observed. They're inferred. And that process seems like you would need to have a lot
4683320	4690440	of domain expertise. And it seems like something which is at least has a degree of subjectivity.
4690440	4694760	I mean, we were just talking about causality, for example, there are many ways you could model
4695320	4701160	the risk of cancer from smoking. It seems like there are many, many different ways of building
4701160	4707960	those models. So that subjectivity is interesting. I mean, are there principled ways of building
4707960	4713800	these models? Yes. And in a sense, it all comes back to the same thing again, it comes back to
4713800	4722280	which model minimizes the surprise the best. And but there are interesting questions amongst that.
4722280	4728840	So how do you actually choose the space of models that you want to compare? So you're right to say
4728840	4734520	that that that often there is some specific prior information that's put into models and active
4734520	4739000	inference. And very often we do end up sort of building models by hand to demonstrate a specific
4739720	4744760	outcome or a specific cognitive function. But there's no reason why it has to be that way.
4744760	4754040	You can build models through exposure to data, where where the models are selecting the data to
4754040	4759240	best build themselves. But the question is how you do that, how you start to add on additional
4759240	4764200	things, how you start to change the structure of your model. But there's a lot of ongoing research
4764200	4769000	into that. And I think there are now methods that are coming out that will allow you to allow an
4769000	4774520	active inference model to build itself. And the way it will do that will be sort of adding on
4774520	4781240	additional states and potential causes, adjusting beliefs about the mappings and the distributions
4781240	4789400	and the parameters of given this than that, adding an additional paths that different
4790120	4798040	or different transitions that systems will pursue. So it's a fascinating area. I think
4798040	4803880	it's one that's still a growing area. But it's this idea of structure learning of comparing
4803880	4808840	each alternative model based upon its free energy or model evidence or surprise as a way of
4810520	4813080	minimizing that by being able to better predict things.
4813960	4822120	Yeah, I mean, that's something that we humans, we seem to do really well. So we can, first of all,
4822120	4830760	via abduction, we can select relevant models to explain behavior, you know, what we observe.
4830760	4836120	But we also have the ability to create models. In fact, I think of intelligence as the ability
4836120	4842760	to create models. So we experience something. And I now construct a model to explain this
4842760	4851240	and similar experiences in experience space. But in a machine, it's really difficult. So in
4851240	4856280	machine learning, there's this bias variance trade off. So we deliberately reduce the size
4856280	4862200	of the approximation space to make it computationally tractable. And when we're talking about
4862200	4867720	building these models, just from observational data, it feels like there's an exponential
4867720	4872440	blow up of possible models. So I can imagine there might be a whole bunch of heuristics around
4872440	4877560	library learning or having modules. So these modules have worked well over there. So we'll
4877560	4882520	try composing together known modules rather than starting from scratch every single time. I mean,
4882520	4886680	what kind of work is being done there? I mean, I think I think you're right about, you know,
4886680	4893720	it's not going to be worth starting from scratch every time. You can sort of build models by saying,
4893720	4898840	okay, let's start with something very simple with a sort of known structure. And I think it's
4898840	4904280	sensible to use some priors in that rather than starting from complete, completely nothing,
4904280	4907720	because there are some things that we know about in the world. And there's no point hiding that
4907720	4913640	from the models we're trying to build. And that might be a simple structural thing like things
4913640	4917480	evolve in time. So one thing is conditioned upon the next is conditioned upon the next.
4917480	4922120	And things now will influence the data I observe things well in the past might not anymore.
4925720	4930520	But then then there's the question of, well, how can a model then grow? What are the things that
4930520	4935800	you can add to it or subtract from it? And subtraction is another key element. Because you
4935800	4939320	could take this whole problem from the other direction, and you could say, well, let's start
4939320	4943080	with a model that just has everything in it and take away bits until we've got the model that's
4943080	4946920	relevant to where we are at the moment. And we know that during development, there's a lot of
4946920	4952200	synaptic pruning that goes on and removal of synapses that we have when we're much younger
4953080	4959720	compared to compared to as you get older. So what can you add on? Well, it depends what your model
4959720	4963400	looks like. So if your model says there's a set of states that can evolve over time, there are
4963400	4966920	a set of outcomes that are generated, well, we know what the outcomes are, we know what the
4966920	4972680	data are, because we know what our sensory organs are. So it's the states that are going to change
4973160	4980280	so do we add in more states? Do we allow them to take more alternative values? Do we allow
4980280	4985960	their transitions to change in more than one different way? Which ones can I change? Which
4985960	4991800	ones can I not change? And it's really just asking these questions that helps you to grow your model.
4991800	4996440	So you say, well, let's try it. If I allow this state to take additional values, if it's not
4996440	5002520	providing a sufficiently good explanation for how things are at the moment. And if that improves
5002520	5008040	your prediction, that's good and you keep it and if it doesn't, then you get rid of it. Do I now need
5008040	5015800	to include additional state factors? So you could either say there is one sort of state of the world
5015800	5019960	that can take multiple different values, or you could actually this is contextualized by something
5019960	5024440	completely separate. So where am I along an x coordinate? You also need to know where you are
5024440	5031320	along y coordinate to be able to contextualize what you're predicting. So it's just asking what is in
5031320	5035560	a model? How do you build a model almost gives you the answers to the ways or the directions in
5035560	5041400	which you can grow it. The other thing you can then do when you're trying to work out how to grow it
5041400	5046120	is to say, well, let's treat this as the same sort of problem as exploring my world,
5046120	5050040	selecting actions that will then give me more information about the world. You could say,
5050040	5054360	well, actually, now let's treat my exploration of model space as being a similar process of
5054360	5063320	exploration. Which of these possible adjustments to my model might lead to a less ambiguous mapping
5063320	5068360	between what I'm predicting or what's in my world and what I'm currently predicting?
5068920	5075240	Yes, it rather brings me back to our comments about the space or the manifold that the models
5075240	5079960	sit on, whether they would have a kind of contiguity or whether they would have a gradient.
5080360	5085480	I guess I'm imagining a kind of topological space that the models would sit on. I don't know whether
5085480	5090600	it's worth bringing in. Obviously, you're a neuroscientist and the way brains work,
5091400	5098600	we must do this. Of course, there's this idea of nativism. Some psychologists think that we have
5098600	5103320	these models built in from birth and then the other school of thought is that we're just a
5103320	5109720	complete blank slate. If you read Jeff Hawkins, he talks about the neocortex as this magical
5109720	5116280	thing that just builds models on the fly. But perhaps one difference at least between brains
5116280	5124120	and machines is the multi-modality, which is to say we have so many different senses that
5125160	5135080	creates a gradient or that makes it tractable. Because when a model from a particular sensation
5135080	5139000	and starts predicting well, we can rapidly optimise and go in the right direction.
5139080	5143560	Because the problem seems to be that there are so many directions where we can go in,
5145160	5151000	doing some kind of monotonic gradient optimisation will often lead us into the wrong part of the
5151000	5158920	search space, so we've wasted our time. Yeah, I think that's a really good point,
5158920	5167160	absolutely. As soon as you know how one thing works or how vision works, I suppose vision
5167160	5173400	and proprioception is a good example, isn't it? If I recognise where my hand is and I can
5173400	5177880	make a good estimate of that visually, then that helps me tune my joint position sense as to where
5177880	5184840	my arm might be. And it's always fascinating to see situations where that breaks down, so there
5184840	5189880	are a number of conditions where if you lose your joint position sense, you're perfectly okay holding
5189880	5194040	your arm out like that until you close your eyes, at which point you start getting all these interesting
5194040	5199880	twitches and changes. So yes, the multimodality I think probably is a really key thing that really
5199880	5204840	does help constrain the other senses because you're just getting more information about each thing.
5205800	5210440	Maybe we should just talk about chapter 10 in general, because that was kind of like the
5210440	5216280	homecoming chapter, if you like sort of bringing together some of the ideas. So can you sketch
5216280	5223240	that out for me? Yeah, so I think towards the end of the book, the idea was to try and bring together
5223240	5227320	a lot of the themes that had been discussed earlier on, but to also make the point that,
5230600	5235880	well, I'll come back to one of the things you said earlier was about how it seems we're talking about
5235880	5241400	lots of different things from different perspectives, but actually they're really the same thing.
5241400	5249320	So we talked about how surprise is also a measure of steady state of energies of various sorts of
5250280	5256120	of statistics and model comparison of homeostatic set points, you know, that all of these things
5256120	5264600	can be seen through the same lens. But again, taking one of those inversions, you can invert
5264600	5267960	that lens and say, well, actually, you can start from the same thing and now project back into
5267960	5275480	all of these different fields. And I think that's a useful thing to do because I think it helps foster
5275560	5280600	multidisciplinary work, helps to engage people from different fields and areas,
5281560	5288280	and helps us know what's happening elsewhere so that you're not just duplicating everything that
5288280	5293640	people have already done. So I think it's really important to have those connections to different
5293640	5299800	areas. And the chapter 10 from the book was an aim to try and connect to those different areas,
5299800	5303240	whether it be to things you've spoken about, like cybernetics and inactivism,
5303320	5307800	and just to try and understand the relationship between each of them.
5307800	5312520	Well, I mean, quite a lot of people use this as a model of, you know, just things like
5312520	5318440	sentience and consciousness in general. And I often speak about the strange bedfellows of
5318440	5323480	the free energy principle. So, you know, there are, you know, autopoietic and activists and
5323480	5327640	phenomenologists and, you know, people talking about sentience and consciousness, you know,
5327640	5333480	obviously you're a clinician, you know, you're working in a hospital. So it's just this
5333480	5337960	incredible conflation of different people together, and they all bring their own lexicon with them.
5337960	5342360	But maybe we should just get on to this kind of sentience and consciousness thing, because that
5342360	5348840	seems quite mysterious. We almost come back to one of the themes we've spoken about a few times,
5348840	5356120	which is that the specific words we use for things in the effect that different people,
5356120	5359720	that has on different people. So some people, I think, would probably get very angry with the idea
5359720	5369640	of using sentience to describe some of the sort of simulations and models that we would develop.
5371400	5376120	But that comes down to what you mean by sentience. And I think one of the key things for sentience
5376120	5382360	is the aboutness we were talking about before. The idea that our brains or any sentient system
5382360	5388520	really is trying to try not to anthropomorphise too much, but it's almost impossible to do in
5388520	5397080	this setting, isn't it? Not trying to, but that the dynamics of some system internally to the system
5397080	5401800	are reflective of what's going on external to it, and that you can now start to see those dynamics
5401800	5407000	as being optimization of beliefs. And those beliefs are about what's happening in the outside world
5407080	5412680	and about how I'm affecting the outside world. And I think that probably gets to the root of
5412680	5418600	at least a definition of sentience and one that I'd be happy with, which is just the
5420040	5426120	dynamics of beliefs about what's external to us and how we want to change it.
5427800	5431880	And there are very few things other than that sort of inferential formalism that give you that.
5432840	5438200	Yes, I mean, in a way, one thing I like about it is, I mean, we are talking as physicists,
5438200	5447640	so we are materialists. It's very no-nonsense. It's quite reductive as well, because there are
5447640	5453560	those who believe that these kind of qualities that we're speaking about, certainly with
5453560	5459800	conscious experience, for example, that it's not reducible to these kind of simple explanations
5459880	5466120	that we're talking about, that it has a different character. David Chalmers talks about a philosophical
5466120	5474360	zombie. So for example, you might behave just like a real human being, but you could be divorced of
5474360	5482120	conscious experience. So he says that you can think of behavior, dynamics, and function,
5482120	5487320	and conscious experience as something entirely different. But as an observer, you would never
5487320	5494440	know. So yeah, it feels very no-nonsense, doesn't it? But that wouldn't be satisfying to a lot of
5494440	5503400	people. No, it probably wouldn't. You're right. Yeah, and particularly when you get onto questions
5503400	5510680	like consciousness as well, I mean, I think it does become very, very difficult, because once
5510680	5516120	you're putting forward or advocating a theoretical framework that seems like it's supposed to have
5516120	5522200	all the answers. I mean, in reality, it doesn't. I mean, I think it's a useful framework to be
5522200	5528680	able to ask the right questions or to be able to articulate your hypotheses. So if you think that
5528680	5535720	consciousness is based upon the idea of having some sense of trajectory of temporal extent and
5535720	5541640	different worlds I can choose between or different futures I can choose between, that might be a key
5541640	5544920	part of it. But for some people, that's not what they mean by consciousness.
5547560	5556280	I found in a particular reading books by people like Anil Seth on this sort of topic, I found one
5556280	5563480	of the interesting comparisons being the questions about consciousness versus questions about life.
5563480	5568120	And we almost don't ask what life is anymore. It doesn't necessarily seem that mysterious,
5568920	5574600	just because we've had so much of an understanding of the processes involved in life, the dynamics of
5574600	5581160	life and the way biology works, it's still much more to go. But the question of what life is just
5581160	5586200	doesn't seem as relevant today as I suspect it did many years ago with those sorts of questions
5586200	5590440	that were being posed. And perhaps we'll see the same thing with questions like consciousness.
5591800	5597800	Yeah, it's interesting though how vague many of these concepts are. And it's quite an interesting
5597800	5604760	thought experiment just to get someone to explain just an everyday thing, you know, like what happens
5604760	5612840	when you throw coffee on the floor. And just keep asking why. And just observing how incoherent and
5612840	5617240	incomplete the explanations are. And it's the same thing with life, it's the same thing of
5617240	5621960	consciousness, it's the same thing of causality, agency, intelligence, all of these different things.
5621960	5628200	And I guess most people don't spend time digging into their understandings of these things and
5628200	5634360	realizing how incoherent and incomplete they are. Life is quite an interesting one in particular,
5634360	5642200	because I think one of the achievements of active inference is blurring the definition of or the
5642200	5650120	demarcation between things which are and are not alive. For example, the orthopedic anactivists,
5650120	5656040	they think of biology as being instrumental. And what the, you know, free energy principle
5656040	5661320	does in my opinion, is it removes the need for this, it almost removes the need for biology
5661320	5667240	entirely. It just says it's just dynamics, it's just physics. But yeah, I mean, just on that
5667240	5672440	point, though, I think many of our ideas about the world are quite incoherent.
5673880	5678040	Yeah. And I think it's interesting that, you know, one of the things that you're saying,
5678040	5682280	and I would agree with you as one of the big advantages of active inference-based formalisms,
5682280	5687480	you'll probably find some people will say, that's a problem with it, that actually there is a clean
5687480	5693240	distinction in their mind between these different things. But then I think the challenge is to work
5693240	5699720	out what that distinction is, if it exists. And it may be a distinction in their mind that doesn't
5699720	5707480	exist in somebody else's mind. And so getting people to try and or trying to support people to
5707480	5714280	be able to express that in a very precise mathematical hypothesis, I think is quite a
5714280	5719000	useful way of trying to explore those problems. Because clearly, for some people, there is
5719000	5722680	something that's getting at it that is not quite explaining. And it's interesting to try
5722680	5728280	and explore that and to work out what that thing is. Indeed, indeed. And just final question,
5728280	5732520	what was your experience writing a book? And would you recommend it to other people?
5733000	5746120	I enjoyed writing it. I think it's time consuming and can feel like it's going on forever some of
5746120	5751320	the time compared to, you know, I think anyone who's had some experience of writing papers will
5751320	5755160	often find that at the point where you're ready to submit it, you're just sick of it and want to see
5755160	5761320	the back of it. And then it's rudely returned to you by the peer reviewers with lots of comments
5761640	5767880	writing a book, it obviously takes you much longer. So you end up being almost more sick of it at
5767880	5773080	various times. But it's quite fun as a collaborative project. It's quite interesting to get other
5773080	5777320	people's perspectives on it. And I was lucky to have great collaborators to write it with.
5778440	5783000	And I think it really is a good way of organizing your thoughts in a slightly more holistic way
5783800	5787640	than you would while focusing on a very specific topic in a research paper.
5788600	5793320	And I've also just enjoyed the response I've had from people who've read it,
5794040	5801880	some of whom have picked out a number of errors, not many. But generally,
5801880	5808680	everybody's been very supportive of that and people seem to have responded well to it,
5808680	5811480	which I think is always encouraging. And that's what we hope should happen.
5812040	5816360	Wonderful. Well, look, Thomas, it's been an absolute honor having you on the show. I really
5816360	5828920	appreciate you coming on. Thank you so much. Well, thank you. I've enjoyed it.
