{"text": " my pleasure. And I must say, I've been really impressed by all your questions. It showed that you did prepare and read papers and think about it. And that's very much appreciated. Thank you. Today is an incredibly special occasion. We have Professor Yoshua Benjiro on the show. Just honestly, I just can't get over it. But first of all, a little bit of housekeeping. So we've just launched a new Discord community. So please jump in there, say hello, introduce yourself. If you want to be, you know, part of the moderating community or just help us do stuff over there, we would love to talk with you. By popular demand, we've also added a couple of ways in which you can support us. So we now have a Patreon and a merch store. If you're interested in supporting some of the episodes of MLST then get in touch with us because we'd love to have a conversation with you. We're just doing so much cool stuff this year. We've already recorded about six episodes that we haven't released. And we've got some amazing people booked as well. So yeah, it's going to be incredible. As always, if you like the content here, please consider hitting the like and subscribe button and rating our podcast on iTunes because it really, really helps us out. I called it iTunes. Is it iTunes? Apple podcasts? I don't know, whatever it's called. Wait and Biases is the developer first MLS platform. And we're extremely proud today that they are sponsoring this episode. Now tracking machine learning experiments is difficult. Using the winging it methodology can only get you so far. What we need is a platform where we can compare models and visualize their performance characteristics against all of the previous runs and figure out the best hyper parameters to use. Now most importantly of all, this process needs to be reproducible. Sounds like a tool order, right? Well, this is exactly what the Wait and Biases platform does for you. Now you can even follow metrics from long running experiments in real time. I think it's really important to lean into the complex interaction between science and engineering in the ML DevOps lifecycle. Data scientists need valuable feedback and they need to communicate why they're running given experiments and they need to share their notes around the next steps. Reports keep this work well organized and connected to the Waits and Biases experiments which were run as opposed to just sharing random screenshots in Slack. It's so easy to create a report and share it with your team after you finished with your experimentation. You could just add notes for yourself as well to explore later on. You can keep a work log and you can even share your findings internally or externally. This is an absolute game changer. I'm a big believer in this kind of engineering rigor. I'm the CEO of a code review startup called Merge these days and I love how the pull request process and tooling immortalizes important collective decisions which were made during the software development lifecycle. Similarly, Waits and Biases immortalizes important decisions that were made during model development, experimentation and deployment. Remember, check out Waits and Biases today by going to 1db.com forward slash MLST and if you're interested in sponsoring future episodes, get in touch with us. Waits and Biases are currently sponsoring our premiere shows but we have lots of other content coming and opportunities for sponsorship so let us know. Cheers. Professor Yoshua Benjo has just released a bunch of papers around G-Flow Nets. Now G-Flow Nets exists squarely in the domain of active learning which is a model that can economically ask an oracle which is probably the real world for the most salient training examples to continue learning. The learner can choose or have an influence on the examples it gets and we want to learn a function which approximates the oracle efficiently. How should we pick the queries? How should we take into account not just the value of the predictor but also how certain we are about the predictors from the learning system? Areas of uncertainty or entropy are kind of like interesting candidates for us to explore further. We need to be able to imagine or invent queries to give to the oracle. Now one of the reasons that machine learning models are so sample and efficient is because of the combinatorial space of possible input examples. We can't train on everything because the space is just too large, it's vast. So you might have heard of a related concept of active learning called machine teaching which is an interactive version where the human interactively selects the most salient data to train a machine learning model maximizing the information gain in respect of the training samples. Now the reality is the function space that we're learning here is highly structured. We only really need to sample training data where most of the rich information exists in that function space. I mean, if you think about it, a machine learning model, it's just a joint probability distribution between signals and labels. And this distribution has modes or areas of density or information. And actually most of it is just areas of nothingness, which require fewer training examples to learn and to represent. Now, if you spoke to a to a Bayesian person like my friend Conor Tan at work, you know how to learn this distribution, they would bring up Markov chain Monte Carlo quicker than a whip it with a bumful of dynamite. Now Markov chain Monte Carlo is an increasingly popular sampling method for obtaining asymptotic information about unnormalised distributions or energy functions, especially for estimating the posterior distribution in Bayesian inference, which is where you've probably heard of it before. Now you can characterize a distribution without knowing all of the distributions mathematical properties. So if you don't have an analytical representation for it, just by randomly sampling values out of the distribution. Now a particular strength of Markov chain Monte Carlo is that it can be used to draw samples from distributions, even when all that is known about the distribution is how to calculate the density for different samples. Now the the Markov property of Markov chain Monte Carlo is this idea that random samples are generated by a special sequential process. And each random sample is used as a stepping stone to generate the next random sample. Now this might sound very complex, but the practical implementation is pretty simple. Markov chain Monte Carlo just starts with an initial guess, just one value that might plausibly be drawn from the distribution. And then we produce a chain of samples from this initial guess by adding random perturbations in the neighborhood of that example. And each new proposal drawn from that random perturbation distribution is either rejected or accepted. There are different flavors of this, of course, I mean, in particular, like tweaking, how the random proposals in the neighborhood are selected or whether the proposals are selected. The simplest heuristic being whether it's below the function or not. Now the idea is that Markov chain Monte Carlo methods, they capture a distribution with only a relatively small number of random samples. But the reality is anything but in high dimensions, and where the distribution has many modes spread far apart, it's actually exponentially expensive. There's a bunch of human orientated hacks to try and make this work well in specific cases. But we're missing a much more general machine learnable solution. This is the main reason why we haven't seen it used in many machine learning applications yet. Assuming that the function we want to learn has underlying structure, then we can escape the exponential time of Markov chain Monte Carlo with machine learning. And this is what Benzio calls systematic generalization, which is to say, how do we generalize far from the data in a way which is meaningful. Now G flow nets are an active learning framework, where the name of the game is to generate salient and diverse training data to augment our model in the most sample efficient way possible. For G flow nets to work, we need a reward function and a deterministic episodic environment. Does that sound familiar? Yes, just like reinforcement learning. Now a flow network is a directed graph with sources and sinks and edges carrying some amount of flow between them, you know, through intermediate nodes. So I think a good way to think about this is pipes of water. Now for our purposes, we define a flow network with a single source. So the root nodes, or you might say the sinks of the network correspond to the terminal states. Now it's designed to find the possible trajectories through our system. Okay, and just think of Alpha zero as being like a good analogy for these trajectories. Now the training objective is to make them approximately sample in proportion to the given reward function. This is in stark contrast to Alpha zero where we were sampling to maximize the expected reward. So Benzio's big idea is that we can have an interacting loop between a generative model and the real world. The real world is expensive. So why not train an imagination machine in our mind until we're ready and waiting to produce good questions to the real world, we could use imagined experiments to train our generator, then produce queries to the real world. We were thinking about a way to visualize how G flow nets work when the idea of a Galton board came to mind. A Galton board also known as a beam machine is a common prop in statistics courses, science museums, and fun gadget stores. The board has rows of interleaved pegs above a bottom row of buckets. Beads are filled into a funnel at the top of the board and then sprinkled on the top center peg. The beads bounce either to the left or to the right as they hit the pegs and eventually collect into buckets at the bottom. If the pegs are precisely and symmetrically arranged, the beads will aggregate at the bottom into a familiar binomial bell curve. Now imagine that the pegs were instead flow gates with adjustable valves that could direct the beads more to the left or more to the right to bias the flow paths. With such a machine, you could adjust the valves or flow rates to create any distribution. For example, to create a uniform distribution, we'd open up the gates flowing away from the center line of the board to drive more bead flow to the fewer number of paths leading to the edges and the corners. Or to create a multimodal distribution, we'd arrange the gates to split the flows into two or more streams that would then pile up in multiple humps or modes below. There's a lot of flexibility here. Indeed, given a distribution, there are generally multiple flow gate solutions to produce it. It'd be nice, wouldn't it? If we had an intelligent, principled way to train these gates. Enter G flow nets. G flow nets put a neural network, a brain behind the flow adjustments. A brain which can optimize the gates to match any distribution we desire. Here, we're interested in sampling a reward function in the context of reinforcement learning. In that context, this is a powerful simulation and sampling paradigm. You see, once the brain has tuned the flow weights, such a modified Galton board, or more generally, a flow network, can sample diverse paths quickly and efficiently, leading to the reward distribution. It's important to point out that the path sampling is more diverse doing it this way. Unlike classic reinforcement learning, a G flow net doesn't just fixate on a small number of high reward paths, it happens to find first. Instead, it stochastically samples a broad spectrum of paths in proportion to their reward. Sure, high reward paths will be sampled with higher weight. But the far larger population of low reward paths will get a share of the sampling as well. Why should we even bother with such paths? The answer is we need to balance exploitation or high reward with exploration or learning to better learn the reward function. This is especially important when dealing with complex real world scenarios of high uncertainty. For example, think of molecular drug discovery and design or navigating jungle terrain. In both those scenarios, we really know very little about how a particular path may play out. We might stumble into the next miracle cure or a pitfall of quicksand. To find the globally best paths, it's important to keep our options open. Beyond this sampling diversity, G flow nets also bring the full power of neural networks to discover latent structure and learn the reward function. This combined with their diverse sampling also makes G flow nets more robust when dealing with multimodal distributions, which are a common trap for greedy algorithms and Markov chain, Monte Carlo. If there is structure linking the multiple nodes, G flow nets can learn it and extrapolate to new modes and once discovered, they will by design drive the sampling to cover those modes and learn more structure. Overall, G flow nets seem to offer an intriguing new path pun intended for an intelligent sampling paradigm. So you might ask how are G flow nets different from Alpha zero? Well, the policy network and Alpha zero gives you a set of actions. Given a state, Alpha zero trains the policy network to maximize reward so that the trajectories all end up at the highest reward. Now what G flow nets do is they train so that the actions are distributed in proportion to the reward. So rather than pruning away all of the low reward trajectories, it will sample them just less often. Now there is a manifest difference between G flow nets in respect of exploration. I mean, you might argue that the Monte Carlo tree search is still doing wide exploration at the beginning. But in spite of its rapid convergence and pruning of load reward trajectories, it's still sampling from the underlying probability distribution, which has been scaled with a softmax. So that's actually not that much to explore in the first place. So in summary, G flow nets are better than Alpha zero Monte Carlo tree search in some sense, because they achieve the same goal by offloading the burning time and the stabilization time of Markov chain Monte Carlo. Remember this whole thing can be trained offline. And then when in inference mode, we can do it in a single shot. Whereas with Monte Carlo tree search, we actually had to do it in inference mode as well. The other thing is we're kind of offloading all of the human engineering required to sample efficiently from Markov chain Monte Carlo. And the other thing is diversity, baby. I mean, consider the difference between how G flow nets and Alpha zero sample the reward path distribution. If you looked at the distributions, you would see that Alpha zero has a little box around the mode. G flow nets is the whole distribution. We know very well that diversity preservation is critical in order to discover interesting stepping stones and search problems. Now finally, Benzio has published results showing the G flow nets converge exponentially faster than Markov chain Monte Carlo and PPL on some problems and finds more of the modes in the distribution function faster. Enjoy the show folks. Professor Yoshio Benzio is recognized worldwide as one of the leading experts in artificial intelligence. Indeed, a god father of deep learning. His pioneering work in deep learning earned him the Turing Award, which is the Nobel Prize of computing. He's a full professor at the University of Montreal, and the founder and scientific director of Miele, which is a prestigious community of more than 900 researchers specializing in machine learning and AI. He's one of the most cited computer scientists on the planet. And I can't even begin to articulate how honored we are today to have this conversation. Yoshio has done a lot of work recently on G flow nets, which are an active learning framework in a reinforcement learning configuration, where the name of the game is to request salient and diverse training data from the real world to augment our learned models in the most sample efficient way possible. Now we're trying to minimize the divergence between the path distribution and the reward distribution, and then sample paths according to the reward distribution. This is in stark contrast with traditional reinforcement learning, where we trying to maximize the expected reward. This approach is likely to find diverse strategies instead of being greedy and converging quickly after finding a single one. Anyway, Professor Benzio, this is amazing. Can you tell us about this exciting work in some of its applications? Yeah, I'm, I don't think I've been as excited about a new topic. At least in the last six or seven years as I'm now with G flow nets. And it's actually even much more than what you've been talking about. The way I think about G flow nets is a kind of framework for generic learnable inference for probabilistic machine learning. So one way to think about this is it's a learnable replacement for Monte Carlo Markov chain sampling. But actually, so there's that and I'll explain if you want why this is important and to use machine learning there. But but also, it can be used to estimate probabilities themselves, not just sampling, but also estimate intractable quantities like partition functions and a condition probabilities that would otherwise require summing over an intractable number of terms. So I think of this as the potentially, you know, there's still we're still at the beginnings of this has a Swiss army knife of probabilistic modeling that uses machine learning to be able to do things that look intractable, but do them efficiently thanks to generalization power of large neural nets. We've been trying to think of a way to help our listeners visualize what a what a G flow net does. And I wanted to run by a possibility to you. So I'm not sure if you've heard of Galton boards also called, you know, bean machines. And what they are is this prop that's often used by statistics professors at the start of say, an elementary introductory course to give a visual intuition. And it's a board that has these vertical buckets down at the bottom with interleaved rows of pegs above the buckets, and then beads are filled in into the top of the board, and they bounce either left or right as they hit the pegs. And they eventually collect down at the bottom. Yeah, yeah. Yeah, now now if the peg is a very good analogy, except that it's not a tree, I don't know how these things are, but you know, the ball can come to a place from two different paths or an potentially large number of paths. Right, right. And I think, given given there are some some differences, you know, the idea was that if the pegs or no, it's it's pretty close to exactly what it is. Yeah, and what we were thinking is that if the pegs on the Galton board are precisely and symmetrically arranged, you know, the beads will form a nice binomial curve at the bottom. And it seems like what G flow nets are capable of doing when they optimize the pathways. They're tweaking the pegs a little bit to the left, or a little to the right, to bias the flow of beads one way or the other. And in this way, a G flow net could arrange the pegs so that the beads could form any distribution at the bottom that we want. And for our purposes, that means the distribution that matches the reward function. So is this a good way to think about G flow nets? Yes, it is. Now, it's missing a really important aspect of it, which would be difficult to send visually, but that all of these peg weights, like the polytip that are boggles left or right, are not just like learned independently, like as a tabular machine earning, but that there's like one neural net that knows about the locations in this big board as input and tells, you know, how much relative weight should I, you know, go to go left or right at this position. So the reason this is important is because it allows for generalization. Because this board is huge, it's exponentially large. So there's no way you're going to learn, like a separate parameter for each of these choices. And so you have this neural net or potentially several neural nets, but that share allow you to share statistical strength, as we call it, share information across all the possible positions, so that you can generalize to places paths that it has never seen from a finite number of training trajectories that it sees while it's being trained. And that's crucial. Otherwise, you couldn't scale to large problems, which is really what we want to do. Professor Benjo, we spoke with Professor Carl Friston about his free energy principle, an active inference, which is pretty much a Bayesian flavored version of reinforcement learning. And he said that while we need to maintain entropy and stop models from increasing too much in complexity, we should balance entropy with accuracy in a principled way. And by the way, you can kind of think of them in just the audience think of entropy as keeping your options open. But Friston thinks that the Bellman-esque idea of reinforcement learning, which is to say maximizing expected reward is the objective is misguided. And we should instead perform inference over future paths, balancing expected reward of relative entropy. Is there a connection between these ideas? I mean, it seems like G flow nets are sampling paths proportional to the reward function, that will maintain as much entropy as the reward function itself. Yes, yes, exactly. It's a translation of the reward function into machinery that can sample, you know, the equivalent, the corresponding distribution. So yeah, I completely agree with what Carl was saying here. But as I said, what's interesting is, we can do things with G flow nets. In principle, we've done the math and some small scale experiments that we have now a number of papers, we can do things that go beyond sampling. But for example, estimate entropy itself. So entropy is notoriously difficult to estimate. And I mentioned in my talks on G flow nets that we can use the G flow net machinery to estimate entropy of say, an action distribution or a distribution over Bayesian parameters, for example, which is would be something you'd like to minimize if you're going to take an action in the world. And you have a model of the world that has uncertainty. And that connects with Carl, for instance, interest, you'd like to be able to choose an action that minimizes your uncertainty about how the world works, we know what are the latent things that may have happened. And good, you know, an important part of that is estimating the reward for the these exploratory actions, like, you know, children playing around is how much reduction in entropy of my knowledge of the world, I'm going to get through that action. So you need to be able to compute that reward. That reward word is basically an entropy over something you care about. And it turns out you can also do that with G flow nets. We're actually speaking with Friston again next week, do you have a question that you would like us to put to him? Well, he, you know, he's on the biology side of things much more than I am. And I believe there are amazing scientific opportunities to explore how the kind of machinery that G flow nets offer could be used by brains in order to do some of the things they do. Using your nets to model the probabilistic structure of the world, including uncertainty, which is something he cares about. But but also taking into consideration things like high level cognition, the global workspace theory, which is something I care a lot about, attention, they all kind of fit in the picture of G flow nets. So so I think there's a huge potential of research at the synergy of computational and theoretical neuroscience, and machine learning, probabilistic modeling of the kind that G flow nets propose to come up with a some proposals for explanatory theories about what the brain does, that's probabilistic. And, you know, I think he would be a great person to be part of that. Fascinating. Well, going a little bit further down that line, there are folks in the community who are huge advocates of biologically inspired approaches to machine intelligence. And, you know, one of the key ideas actually is diversity, discovery and preservation, both in how knowledge is acquired and represented. I mean, specifically evolutionary algorithm advocates, they differentiate themselves from gradient based single agent monolithic approaches like reinforcement learning. And they point out that their approaches overcome so called deception and search problems, you know, which is to say they don't get stuck in local minima, your approach seems to be achieving something very similar in the context of a gradient based reinforcement learning package. I mean, I don't see it as being mutually exclusive. But what's your take on this? Yeah, diversity is important when you're exploring and humans, especially young ones are exploration machines, they're trying to understand how the world works and they're acting in the world in order to get that information. Yeah, I agree that that search process needs to have a big bonus on on diversity, like on trying different ways of achieving something good, like better understanding how the world works. So it turns out that in the G flow net framework, you, you have a training objective that yields this kind of diversity and exploration, but is based on training large neural nets end to end. Now, it's a bit different from the usual end to end training, because we don't have an objective that objective we're trying to optimize is not tractable, actually. But we can sample these trajectories, which I think of like sampling thoughts, like our thought process is going through some chain of explanations, not a complete, and it doesn't represent all the explanations, but but what we found with our training objectives for G flow nets is that these sort of random randomized kind of views of the world are sufficient to give a training signal to the neural nets that do the real job. I'm curious. So this trade off between exploration versus exploitation. And this has come up in so many contexts, you know, throughout our show. And one in particular, as we talked to, you know, we've talked to multi arm banded folks, right? And G flow net seemed to capture this balance between exploration and exploitation. But the multi arm banded folks, you know, they dive deep in that research circle into this into this trade off. And I think they have some very principled ways and even very rigorous ways to analyze this fundamental trade off. To what extent do you think that that their research maybe could be applied to future G flow net variations? Like do you think maybe it might open up more options to fine tune the trade off between exploration and exploitation? Yeah, I mean, the banded research is very, very closely related to the G flow net thread. But G flow nets, as we have been using them, for example, for drug discovery, they are banded. It's just that the action space is not, you know, one out of n things. It's, it's combinatorial because you build these pieces. So the action space is not something you can enumerate. So you can't apply the typical banded algorithms, but a lot of the math is totally applicable. And in fact, what we use in the drug discovery setting is UCB upper confidence bound objective to learn a good exploration policy. So that comes out of the banded research. It what it does is it, you know, it combines the risk and reward expected reward, one of these together in a way that in theory guarantees that you will do an efficient exploration and find that where is the, you know, where's the money? Where's the reward, right? All of the possible places where you can get the reward. So in, in, in the G flow net papers, you often describe it as, you know, we want to sample not only the maximum reward path, in order to have more diversity in order to maybe figure out something that we didn't know if we were just to go to the maximum reward. And that speaks a little bit to the, like the things that we know that we don't know, right? We maybe know that, right, this seems like a lower reward trajectory might turn out to be a higher reward trajectory. However, exploration and reinforcement learning is also fundamentally addressing the things about the things that I don't know that I don't know, which is where stuff like random exploration and things like this comes in. Could you maybe comment a little bit on how you see sort of, because it seems to me that if I managed to sample according to what I think is the reward distribution, right, I still have this problem of maybe there is a deceptive rewards there are, you know, I need to take a step back, I may not know some sort of some, some area of the search space. And don't I just run into the same problems again? So, so the important trick here is you need your model of the reward distribution, or the reward function to be one that captures uncertainty, like, maybe in a Bayesian way, or, you know, whichever way, the Bayesian way, by the way, fits well with the G flow net framework, because we can consider the parameters of the reward function as latent variables, like you don't actually know the reward function, you're trying to figure it out from experiments. So the G flow net can sample, and not just like what you should be doing in order to acquire information, but also potential reward function. So, you know, we don't actually have a knowledge of how the, you know, what's going to be the rewards we're going to get in the world. Classical IRL is going, as you said, to the expected value and try to maximize that, whereas the G flow net approach is trying to acquire as much knowledge as possible about the underlying reward function. So you're trying to minimize the uncertainty. So your model with the G flow net is modeling the uncertainty, and then it can use it as a reward for the policy that is going to do action in the real world. So we're talking about different G flow nets. There's a G flow net that models the uncertainty in the reward that you're going to get from the real world. And that's like a Bayesian model. And then you have another G flow net that controls the policy that searches to and its reward is how much uncertainty reduction you're going to get by doing this or that. So, so yeah, you need to have a part of your model that is kind of aware of the fact that there are whole areas in the world that you don't know about or aspects of the world that you don't know about so that you can drive the exploration. I would love to know where some of the magic is coming from. The promise of G flow nets is that we can discover as many modes as possible in the path distribution. Traditionally in Markov chain Monte Carlo, we had to hack priors into the algorithm by hand, you know, to find new modes or areas of information efficiently, especially when they were very far apart or not very sharp. The hypothesis of G flow nets is that the structure of these modes is learnable on many problems, even in high dimensions. It's a little bit like saying we're getting a free lunch. I mean, actually, I think you used that exact phrase to describe what we're doing here. Many research avenues have tried to develop general methods to discover these structures and have failed. How do you think G flow nets will overcome this seemingly intractable curse? There is no guarantee that they will, because if there is no structure in the underlying function you're trying to discover. So let's say the reward function or the energy function that you care about, then having visited some finite number of modes like regions where your reward is high site is not going to tell you anything about what are the other good places, the other modes. So so there's no guarantee that it will work. But if if there is structure, then there is a free lunch. And we know machine learning is good at that. Like the last 10 years of deep learning and its success. What is it telling us? It's telling us that you can generalize right that these nets, I'm not saying they generalize perfectly, but they can generalize. So you can think of it like the machine learning problem is given some examples of good things, like, you know, places where you get reward, you can you generalize to other places. And the supervised learning way of thinking about it is, you know, given a candidate place, tell me how much reward I think I would get. The G for that sampler is learning the inverse function is like to sample, but it's kind of the same thing. It's just going in the other direction. Give me some, you know, sample some some good places that that you know, where the reward is high. So we now have a lot of experience in designing powerful your nets that can be leveraged to generalize in those spaces where we normally use MCMC. And if there is kind of regularities that allow to generalize, then all of that can be, you know, put to use. We mentioned earlier, reinforcement learning often being applied in a context where you have this kind of solid reward function. So let's say games, you know, playing chess. I'm really curious, what would happen hypothetically, if we applied G flow net, you know, to something like chess. So I mean, I think given the fact that reinforcement learning like say alpha zero is trained specifically to choose the best move rather than diverse moves, it seems obvious that maybe if given equal resources to both alpha zero and flow zero, alpha zero would probably beat flow zero. However, I think if flow zero were given more resources, say and trained to the same rating, say the same elo rating as alpha zero, it seems like flow zero, if you would, would play significantly more diverse and interesting games with a wider variety of styles. And I think you could even imagine also that it could be possible, even if given equal resources, but sufficiently high enough resources, that a hypothetical flow zero would consistently reach higher ratings, because it might find, you know, more interesting stepping stones that have the potential to avoid deception, because it can explore seemingly lower reward paths that ultimately develop into higher reward. More curious if you have any thoughts on that? Yeah. It's a good question. I would say where the kind of approach we've been pioneering with G flow nets might be really paying off is if you think about it from the perspective of the learner has a finite computational, you know, amount of resources, because in principle, right, if you had infinite compute, and you know the reward function, like the rules of chess or go, then you can just crank and find, you know, the policy that's best in every possible setting. Now, if you have finite resources, like, you know, you, you have a budget of compute, you'd like to use it efficiently. And so that's where the exploration exploitation trade off becomes important. And if you if you had a, say, a current policy that you're not completely sure is the right one. And, and then you're trying to say, Well, what, how should I play so that I'm going to improve my policy the most as in I'm going to reduce the uncertainty that, you know, it is the right policy, like that it picks the right things. So now we're getting closer to the kind of setting where it makes sense to use G flow nets. And then what I would expect, if we do the engineering work here, but based on the sort of much simpler problems we've looked at, is that it would converge faster. So given, if you look at on the x axis, the number of games you're playing. And on the y axis, how good is your policy measured like on other games. So that's where you would get. In other words, it's the learning curve that you might gain on asymptotically, everything is going to converge the optimal chess player, right? So the the place where it's interesting is to look at the learning curve how fast you learn. And here you want to sort of active learning thinking like, Well, I'm not just trying to win here. I'm trying to gather information so that I'll win more in the future. And it's a different objective. And that's where you need diversity and exploration and like a model of your own uncertainty and an active learning policy. How much do you think this could be part of not maybe only reward maximization things, but information collection, things like, I'm sure you're you're thinking about in, let's say the brain, there is there's sort of maybe a similar process going on and what do I still need to retrieve in order to give certain answers to questions, or maybe in our, let's say, big search engine, let's just name one for naming sake, let's Google, or so would would try to answer your query, not by just searching through their index, but by actively doing this multiple multiple things like, is this enough? Is this enough? Is this enough? Do you see connections to these types of things? Or are they inherently different? Because they might be not learning on the spot? What they're doing on the spot is acquiring information. And you want to do it in an efficient way. And that's where sort of the active learning thinking comes in. And I think it's actually a very big, practical problem in the deployment of like AI dialogue systems that are not chit chat, but they're trying to say help a user achieve, you know, get something get information or something like this. This is this is a huge need for this in, you know, the business world and search engines, and you know, it's much more than search engines. So I don't think we have the algorithms that do that right now. And it's kind of painful. The human has to know, you know, is driving. But if, if we had systems that could explicitly model their own, say, uncertainty about what the user needs or wants, or where to find information. And then, and you need like pretty powerful models of that, like it's not just galaxies, they're simple things. That's where G flow net strengths comes in, you can represent very, very complex distributions over compositional objects. It's not just a few numbers. And then I think you could get to much more efficient human machine interfaces. And the same, I believe the same methodology could be used more generally in scientific discovery. So what is scientific discovery? Like what is it that scientists do? They plan experiments that are going to allow them to reduce the uncertainty on their theories of, you know, some aspect of the world. It's the same problem. Yeah, you have a series of questions you're allowed to ask to nature. And you try to ask as few questions as possible to as quickly as possible, understand what's going on. Is there a connection fundamentally to I'm thinking of causality, which also I've seen a number of papers that you've collaborated on with people who are who are deep into causality research and so on. What do you think there is a a connection there where an agent could learn to uncover if you think about scientific discovery to uncover the fundamental causal structure of the world by asking such questions, like could there be a connection to that branch of research? And could this finally be like the unification of of something machine learning and the the world of causality? Yes, you guys are really asking all the right questions. Thank you so much. In fact, one of my main motivations for the pursuing the the G flow net research program is that I think it's the it's an ideal tool for implementing what I called in my talks, system to inductive biases. So what this means is there are lots of things we know from neuroscience and cognitive science about how we think. And we can bring that into the design of probabilistic machine learning, you know, based on deep learning is the building blocks. And one of the inductive biases, like one of the characteristics of how we think is we think causally, we're constantly asking the why questions we're trying to find explanations and so on. And, and and that connects with classical AI, like the way we think, to some extent, has also inspired classical AI, you know, rules and logic and and reasoning. And we haven't yet found the way to integrate these abilities in deep learning. And of course, lots of people are like, trying to and and that's important. But but I think the reason why G for nets give us an amazing handle on this is because they they're really good at representing distributions and sampling over graphs. And, and like a reasoning or a set of possible reasoning to explain something or to, you know, for planning. The these are graphs. And your thoughts can be seen as graphs, right? So think of like, maybe a simple version of this, think of a parse, like a semantic and syntactic parse of a sentence is a graph. But usually it's, you know, it's more than a tree, there are all sorts of semantic connections, including with knowledge graphs, right, which also graphs. So the ability to implicitly represent those distributions and sample pieces of them as thoughts is, I think, fundamental to how we think. And going back to causality, one of the hard questions that I think G for nets can help us with is causal discovery. So in other words, what is the underlying cause structure of the world, including the uncertainty about it? Given the things we observe, a lot of the research and causality has been okay, we observe these, these random variables, discover, you know, make inferences about, you know, whether what we can say about whether it goes to be and so on. But it's much harder to discover the causal graph that that, you know, in a large set of variables, and it's even harder. And really, nobody's done a real job there. To do this when what the learner sees is not the causal variables, but just like low level pixels. And you also have to figure out what are the causal variables and how they're related causal. And I think G planets can help us do that. This this opens up, this is so many avenues of questions, I think it'll probably almost be a future episode in itself. But let me just ask you about some of the basic ones, which is, as you mentioned, kind of learning the causality causality structure, much more difficult problem. And the first question is just how to represent the causality. And so you, you, you mentioned graphs, you know, graphs is one way. And of course, you can develop, you know, isomorphic ways of representing certain parts of logic as graphs, etc, depending on how, you know, how rich you make the graph structure. But there's also the other issue of, you know, when you're trying to build, and I think it's probably correct to call this a world model, right, like we're trying to build a causal that's the word I use. Okay, great. And I, and so I have one quick question about that, which is, you know, to me, to some people, world model is only the discriminative function. It's just that, you know, probability y given x, to me, it's more general. It's also the structure of x. Is that, is that also your, your view as well? Yes. Yes. Okay. And so in constructing those, those world models, some of the, let's say the pushback on on these type of generative techniques from, from folks that are more skew more towards the discriminative side is, hey, look, fine, you're going to go and try and build this generative model, it's going to be even more complicated than this discriminative model, because it also has to learn, you know, the structure on x. But I think the possible free lunch here, is that you can learn abstract structure on on x. And so if you learn these abstract world models, throwing away all the nitty gritty that doesn't really matter, you can potentially have very powerful, you know, predictive encoding, if you will, like, what's, what's your thoughts on that? Oh, that's what I've been thinking for almost 20 years. And one of the reasons why I've been interested in deep learning as a way to think of discovering abstract representations, you know, from the early days of deep learning, as in like mid like 2005 or something. And, and in the paper that Jan McCarr and I wrote about, and also other papers I wrote with some of my colleagues at the University of Montreal on, you know, deep learning around 2010, they are all about that notion that we would like these unsupervised learning procedures to discover these abstract factors, as we call them. But now I think it's not just the factors like the variables, but it's also more importantly, even how they're related to each other, which in the causal language is what we call causal mechanisms. And so here's a fundamental way of thinking about this. If you don't introduce the abstract kind of structure that exists in the world, then representing p of x, the input distribution is very difficult. It's, in other words, you'll need a lot of data to learn it. And it's not going to be generalizing very well. The whole point of abstraction is that it gives you very powerful abilities to generalize to new settings, including out of distribution, which is one of the hardest topics in machine learning right now. How do we extend what we do so that it generalizes well in new settings? And thinking causally about these abstract causal dependencies, as the things that are preserved across changes in distribution, like, if I go to the moon, it's the same laws of physics, but the distribution is very different. How do I generalize, you know, across such changes in distribution? It's because the learner is us, you know, if we, if we were, if we had the right education, has figured out the underlying, at least, you know, enough of the underlying causal mechanisms, that we can be transported in a different world, but where there's the same laws of physics, and we can predict what's going to happen, even though it looks completely different from, you know, our training environment. So the, the idea of extraction is really that if you introduce abstractions, the description length of the data becomes way smaller. And that's why you get generalization. Absolutely. I'm fascinated by these abstract categories. I think it's the most exciting thing in AI. I mean, Douglas Hofstadter spoke about cognitive categories, like the concept of sour grapes, for example, to represent the certain thing. And almost magically, our brain seems to arrange these cognitive categories. And it's not entirely clear to me whether they're an emergent phenomenon, or whether it's some other process. But the modes that you're discovering in G flow nets, they're a kind of category, these cognitive categories that I just spoke about our abstractions, also things like causality and geometric deep learning that they are kinds of categories. But I've always had this intuition that deep learning doesn't learn the categories on its own, it needs humans to kind of put priors into the model, as we do with geometric deep learning. Do you think that that will always be the case? Or can we have that meta level of learning? Yes. What I really want to do is build machines that can discover their own semantic categories, abstract ones that really help them understand the world. And of course, they're going to learn, you know, better and faster if we help them just like, you know, we teach kids, we don't let them discover the world by themselves. But we do have an ability to invent new categories. That's what scientists do all the time, right? Or artists and, you know, writers and philosophers and scholars, and ordinary people who find new solutions to problems, we do that all the time, our brain is a machine discovers new abstractions. Of course, that usually it's just like one little bit on top of all the things we got from our cultural input. But but that's the ability that we don't have right now in machine learning. And that is going to, I think, be a huge advantage. So now we're not in reinforcement learning, we're not in active learning, we're talking about unsupervised learning. So we're talking about how can a machine discover these often discrete concepts that somehow help it understand. So in other words, build a compact understanding of lots of things that generalize across many settings. And yeah, that that's that the path to build that is, is becoming more and more firm in my mind, as I move forward with G flow nets. So as a clue, there was a paper we had recently, I think in Europe's on that's connected to the global workspace theory that says that it's about discrete valued neural communication, I think is a title where the one interesting intuition here is connected to this is if you if you constrain the communication between different modules, say in the brain or in machine learning system, to use as few bits as possible and discrete is the way to get the very few bits. You can get better generalization. And there are good reasons for that that we try to explain in the paper. But but that's, that's it, you know, there's a clue here that discrete concepts emerge as a way to get better generalization. You you mentioned before, and in terms of discreteness, and what you mentioned before with graphs being very fundamental, it connects a little bit back to a paper that you, I think, provocatively titled the consciousness prior, where where you connect sort of the ideas of attention, sparse factor, graphs, language, things being discreet, things being describable by language, right? And, and I find that all to be very interesting. On the topic of consciousness, we would be, it would not be appropriate for us to not put this question to you. So you're not, you're not very active on Twitter, which is probably why you're so productive. But if currently, there is a bit of a of a thing happening on Twitter, namely, Ilya Satskever of Open AI has tweeted out a seemingly innocuous tweet saying, it may be that today's large neural networks are slightly conscious, which has resulted in quite a, let's say, a storm on of people agreeing, disagreeing. Obviously, he's he's talking about maybe, you know, the large language models we have today, which do incorporate a lot of the things you talk about, they do incorporate attention mechanisms, lots of them. Presumably, it's all one needs. They do incorporate language, they do incorporate discrete things with you know, discrete tokens and so on. What do you make of a statement like this? It may be that today's large neural networks are slightly conscious. Well, this one fundamental problem with such statements, which is we don't know what consciousness really is. So I think we have to have a bit of humility here. And I can't say what Ilya is saying is true or not. I think that this is more to consciousness than what we have in these large language models by a big gap. But that being said, and you know, we do need to work with our colleagues in your science and kind of science who are trying to figure out what consciousness is from a scientific perspective and philosophers who are helping also to make sense of that landscape. So we have to be careful with the use of those words. And you know, I was a bit liberal in the title of my paper. And I learned a lot about consciousness since then, learned that there's a lot that we don't understand that at the same time, there are enough bits that we know from from cognitive neuroscience that can serve as inspiration for how we could build machine learning systems that have similar, say conscious processing machinery. Okay, let's not say consciousness but just conscious processing machine because that's less controversial. And by the way, the word consciousness has been taboo with most of science for a long time. And it has become untapped, you know, the tabooed in neuroscience, because we're starting to be able to make measurements of what's going on inside your brain, while you're doing things consciously or not and so on and distinguish the parts that you're consciously aware of and the parts that are there in your brain, but you're not conscious. So we're trying to we're starting to make a lot of progress of what it means to be conscious of something or not. And I, you know, I think this is a very exciting and important scientific question. And I would rather like work on exploring hypotheses and theories to explain our conscious abilities, rather than make bold statements about whether current neural nets are conscious or not. Professor Benjo, we've got some David Chalmers on the show next month. Do you have any questions that you had put to him? I very much like a hypothesis about consciousness that Michael Graziano has put out to help explain the qualia, the subjective experience part that Chalmers wrote might be something science can't really, you know, touch. And so what's, you know, I'd like to hear what he has to say about these kinds of approaches. And one of the basic premise here is is very grounded in things we can do scientifically. It's to say, well, let's not try to figure out what is consciousness or subjective experience more specifically, you know, from a philosopher's chairs. But let's let's consider that as a phenomenon that is happening in the brain. I mean, unless you believe in sort of supernatural things, if it is happening, something is happening in the brain, and we can report about it. And we can, we can like, measure what's going on in various parts of your brain while this is happening. And then, you know, can we then come up with theories that explain why we feel that we have subjective experience? It's not saying whether consciousness exists or not or subjectivity. It's not whether it exists or not in some sort of logical sense. It's whether, you know, what is it that's going down in our brain that gives us that feeling and then make us say, Well, I am, you know, I'm conscious of x, y, or z. So so that's the that's the direction I find interesting because it opens the door for a scientific investigation. And Michael Grosjean has a specific theory about that which I find compelling that is really rooted in the idea that we have a world model. And then we we because we have an attention that focuses only parts of it at a time. And we need to have like a little mini world model that controls that attention. That creates a sort of separation between the the where the real knowledge is and sort of this more abstract control and machinery that could well, give us this illusion of Cartesian dualism, which I think is an illusion, but but you know, must be grounded in some you know, biological reality. And that's I think understanding that is is a very good question to ask. And I'd like to get to know what he thinks about such a research program. Thank you very cool. Yeah, thank you. I do have one kind of nitty gritty question because and partly partly based on some of your recent work on becoming more of a fan of semi supervised learning. And you know, you had a recent paper that was on interpolation consistency training. And what I found interesting about that is that if we consider one of the biggest challenges that we face in machine learning pretty much across the board is an overcoming the various, you know, curses, if you will, the various forms of intractability that we have an empirical learning methods. And in this context of semi supervised learning, that recent paper, it found significant improvements over state of the art by forcing linearity. So in this case, it was by this mix up between the unlabeled samples and their interpolated fake labels. And in the last decade, we've also seen values come to dominance in the field of neural networks, their piecewise linear recent work by Randall Belastriero, developed an interesting frame of reference which cast multi layer perceptrons as a decomposition method, which produces a honeycomb of linear cells in the ambient space and they're activated turned off or on by input examples. So my question is, why is linearity, whether it's piecewise or otherwise, dominating the state of the art in approximation methods, it almost seems to me like we've kind of gone back to the future, if you will, sort of leaving behind attempts at more smooth nonlinear methods and gone back to newer, albeit more complicated forms of linear approximation. Right. I would say something that's roughly linear is simpler. So having a regularizer that says, oh, you want to be roughly linear or locally linear, at least to as much extent as you can is a smoothness prior. So that's going to help generalization. But it could also hurt if that is too strong. And so having these piecewise linear kind of more type of solution is a good compromise. It says as few pieces as possible, and ideally organized in a compositional way. So that it's not just like a relu, it's more like the discrete abstract logic, you know, reasoning, things sitting on top, that's controlling the pieces. But but otherwise fairly simple in each how each of the pieces are, you know, like linear, for example. So one way to look at this is, if you look at classical, the kind of rules that classical AI researchers were using, each rule is fairly simple. It's, you know, like, it's almost linear, or it's very simple logic. But it's the composition of all those rules that gives the power of expression of these systems. Of course, the problem then is that they didn't know how to train them properly. But yeah, I think we, I think we learn to come up with these discrete ways of breaking up things into simpler pieces. And that in fact, I think if you're Bayesian about it, it just comes out naturally. And they're very, very weak assumptions. So in a way, it's it's almost, it is piecewise abstraction. So we're kind of back. Yes, that's what I would lean to, rather than piecewise linear. But linear, of course, is a broad part of, you know, it's an easy way to get simple. Amazing. Professor Benjo, I'm interested in your personal journey. So we've been talking about diverse trajectories. And I wanted to know about your own trajectory of research over the last 10 years. Now, one of my mates, a psychologist and symbolist, Professor Gary Marcus, presumably one of your best friends, by the way, he pointed out in his 2012 New Yorker article that MLPs lacked ways of representing causal relationships such as between diseases and their symptoms. And I think this has been a significant focus of yours in recent years as we've discussed. And he thought at the time that you were a bit too quote system one all the way. And he spoke then about the need for heterogeneous architectures and the acquisition of abstract concepts, compositionality and extrapolation, which I think has also been a huge focus of yours in the last decade or so. We really enjoyed watching your debate with Marcus. And by the way, we would love to host V2 of that debate. So if you're interested, you just let us know we'll do that. But he's often viewed as a heretic. And, you know, just forgetting about symbols versus neural networks for a minute. Am I right in thinking that you've converged in at least some ways in your thinking? And how would you characterize that from your perspective? So, yeah, I used to be in the 90s, a, you know, pure neural net subsymbolic connectionists researcher. And I did my grad studies at a time on neural nets at a time when the dominant way of thinking was these, you know, classical AI rule based system with no learning at all, and was dominant, meaning that the little group like, you know, Jan and Jeff and I and others who were thinking otherwise, had to, you know, defend our views. And and maybe that led to a kind of, you know, us versus them, I think, unhealthy way of thinking. And of course, I matured. And one of the big, so there, I think there are several turning points on that journey. Well, one of them in the in the 2000s was the realization of the importance of abstraction. So and the way to think about this maybe more concretely, because what does it mean to be abstract? Is that I was thinking, well, what would be the right kind of representation we want to have at the top level of our unsupervised deep nets, because we were doing mostly like unsupervised deep nest, like, you know, deep boz machines and stuff in that decade. And I was thinking, well, it would be things like words, right, things like the sort of concepts that we manipulate at the top level, well, it's words or, you know, the equivalent, maybe, with disambiguated. But yeah, we, it didn't seem that we have the right tools for that. And then it remained like an objective. And then in 2014, we discovered the power of attention. And that's closely connected to abstraction, because what it does is it focuses on a few things. And of course, that's our, you know, that's very much a characteristic of how we think a thought has very few elements in it. That means we have selected those elements. And that's where attention comes in. So it's getting closer to this ideal of building machines that think like humans. And then of course, in 2017, I wrote this consciousness prior paper where, you know, I discovered all the work on global workspace theory and, and it, you know, and the momentum is built up. And of course, now, you know, humans think and they use symbols, and they understand the very abstract relationships between them. And we need to build neural nets that can do that. So I guess where I've maybe departed from Gary, but maybe he's moved to is, it's going to be neural nets that do it, right? It's just that we're going to be training them in a special way. And that's what G flow nets really aiming at. So can I just say we, we asked many guests, these, these questions about their, their evolution. And sometimes they, they tend to be spicier than others. But I have to say, from my perspective, your answer was the most informative, the most gracious and the most noble of answers that we've heard so far to similar questions. So kudos to you. That was awesome. Thanks. I just cannot believe it. And we always do a hell of a lot of preparation. But it's gone to the point now where we know that we're not going to get more than about six questions in. So we, you know, we kind of like exponentially, you know, have an exponential prior on our questions. Well, he was awesome, though, with like, you know, we asked him to give relatively sort of three minute answers. And he stuck to that, which was really cool. I mean, that's, that's very helpful to have an interesting dialogue. And I, I can't believe how proud I am, you know, that he's, that he appreciates that we put the prep time into it. And, you know, had had decent questions that were hopefully interesting for him, as well as our, as well as our audience. So Dr. Kilcher, lightspeed Kilcher, what should I take? It's cool is, I mean, his, um, yeah, I think is the thing he mentioned at the end, like his humility, it kind of shines through everything he does. And he answers, he's like, you know, here's the best answer I can give. But, you know, he seems to be very, like, open and not, not, not very, yeah, one notices he's not on Twitter. It's like, it's noticed that was a brilliant question. I think we should post that question on our Twitter. Because, you know, that there's that a bit for people watching this in a year's time, it's probably forgotten about but yeah, that ilia guy from open AI said that the models might be slightly conscious. I was exasperated by that. Because I watched his interview on Lex. And I know by saying bad things about him, he will never come on our podcast, but I don't think he would have done anyway. So it doesn't matter. But yeah, I think that it's pretty bad. What? Why? Yeah, why? It's like, it's like, you don't think it's bad? No, he says, I think, because a lot of the folks at Open AI, they are, you know, like in the rationalist community, and they seriously believe that we're an imminent threat of the AI taking over the world and us being paper clips. And I think it's next, I listened to his interview on Lex, and he sounded like a salesman, talking about Codex and how it was going to revolutionize everything. And I honestly think that there's just such a divergence between what they're saying and reality right now. Well, not to drift too far away from from our guests today. But so I thought, I thought it was just kind of a shower thought, you know, like, you know, the the large neural networks of today might be a little bit conscious, right? And, and, and you just like, yeah, well, yeah, well, shower thought, and it is a shower thought like it needs on Twitter, it's just something you tweet out. And, and it brings up interesting questions, like it brings up interesting questions, like, you know, you're a you're a ball of neurons, like you're just a slap together piece of matter, right? You have consciousness. So clearly, like something about, you know, learning systems combined with data, or maybe not even combined with data gives rise to consciousness. So why can't why can't another, you know, in silico, slap together system of neurons ingested with data be slightly conscious or have like, some properties, like, and that's that's essentially, yeah, Benjo refused to give like a humble, the humble person he is, he refused to give like, you know, the the the strong take on that, but that would have because he might just this is my opinion, not his obviously, but reading the consciousness prior paper, it is not too far off. He formulates consciousness as having these elements of, you know, I have my internal state, which is sort of everything in my brain that I could bring bring up into my forefront, then I get some input from the outside world. And through the input, I then filter, like with an attention mechanism, I do I look what in my mind, could I now bring into focus, right? And that is by use of something like an attention mechanism. And then I take that thing. And I put it into these abstract concepts I use I represent. I represent the concepts in my head as a sparse factor graph. And by focusing on parts of that, I can then make inferences in this sparse factor graph and so on. Now, obviously, something like GPT three doesn't have all of that, at least not explicitly, but some of it is there, right? It's, you know, I have a piece of input, I have giant amount of weights, I use an attention mechanism to sort of see what I can focus on. Yeah, but yeah, but I think that I think that's a very declarative description of consciousness. And at its roots, it's about the phenomenological experience. Right. And I know we discussed computationalism and panpsychism. Let's not go down that rabbit hole. But surely, they don't think that this model can feel well, but so this is consciousness is not about feeling. It's about being being like aware of of like, I don't even know what it is. I'm just saying that it sounded not too far away from what the consciousness prior paper was about. And yes, I realize it's called the consciousness prior and not consciousness. But you know, yeah, I mean, I think he answered it the way a scientist should answer it. And I was really happy with his answer, which is, okay, a consciousness has to be some activity of neurons and firings or whatever in the brain or else we're talking about magic. And that's not in the field of science. And B, you know, whatever that thing is, it's obviously quite nuanced and complicated. And we don't have we don't know yet. So we need to have some humility here, which means we shouldn't be alarmist. So we don't need to be going in, you know, burning books tomorrow because because we created a, you know, GPT, whatever, that anytime its wheel is spinning, and it's actually suffering. You know, if you ask it a question that's too hard, and it's spinning, it's because you're hurting it and it's suffering. And so we need to turn it off like right away. But wait, we can't turn it off because then we'd be like, murdering, you know, a sentient being or something, like we're way, way too, in our infantile understanding of, you know, this type of complex, complex behavior, that's the human mind and consciousness to be at that point. So from my perspective, he answered it completely 100% scientifically. And there's a lot of folks out there who are supposed to be scientists that spend a lot of time with, you know, unscientific, you know, thinking about it. Cool. Let's talk a little bit. What one of the things that I really found interesting about Benjo's ideas, other than the causality stuff and the system to stuff is this notion of diversity. We've had conversations with Kenneth Stanley all about open endedness and diversity preservation. We've also had conversations with Friston about the importance of balancing relative entropy and so on. And we have all of these curses in empirical learning, right? The statistical curses, the approximation curses. Dimensionality, we have to mention dimensionality and even, I mean, you know, we're talking about curses in the Monty, you know, the Markov chain Monte Carlo in the sense of it being a high dimensional space. And we need to assume that there's some structure around where these modes are. So all of these approaches are ways of simultaneously, and, you know, being able to explore but not being cursed. So yeah, what was your take on that? Well, any one of my take was that I like that he's so interested in abstraction, because, you know, to me, that's been not only one, you know, it's not only one of the larger mysteries, at least for me, I mean, I don't know, of the kind of the universe is abstraction, idealism, you know, platonic thinking, whatever. I mean, the whole point is just that he views abstraction as a key to pragmatically useful, you know, pass forward. And it's a hard problem, a really hard problem. And, you know, his focus right now is on kind of graph based structures. And I have to admit, you know, for me to you, they're quite seductive and appealing. I don't know if they're the the right path forward, but it's definitely cool to see a lot of research, looking into graph based, you know, methods, or, you know, hyper graph based methods, whatever they are, they seem to definitely be a promising path forward. And I think we're in for, hopefully, if we can continue to progress at a reasonable rate, you know, some some interesting decades ahead. I mean, I would, I would also postulate that maybe our most of our, let's say benchmarks that we're thinking about today aren't necessarily suited to to because his argument was by creating abstractions, it might actually, you know, help your ability to learn something, right, which is a thing that we all intuitively understand in the world, if I have good abstractions, I can transfer my knowledge from here to here and from here to here. Yeah, in something like image net classification, or whatnot, or most of the benchmarks we have today, the necessity of abstractions is probably not like the data, the hardness of the problem probably doesn't require abstractions to be introduced. And therefore, the limiting factor here might not only be the models themselves, but also, let's say, our ability to even measure the progress one could make with abstractions. And I think that's gonna change maybe in the near future, because people are going into multimodality research and so on. And there, I think the concept of sort of concepts, maybe not abstractions, but at least something like concepts is way more, more important. Yeah, there's, let me just follow real quickly there, Tim, because there's something very interesting there to Yannick, which is the lack of good tools to deal with multimodal, you know, sets of data results. And a lot of times, we're just throwing out kind of valuable, valuable sources of data, just because, you know, we don't have a good tool sets to do with them, like think about the self driving car, like the whole, should it be vision versus LiDAR debate? Why? Why isn't it both? I mean, you know, if you can for $5, you can throw on some cheap, you know, LiDAR sensors or something, maybe not something fancy, but something cheap, why wouldn't we take advantage of that data? And it's, it's really because we don't have good tools to deal with, with multimodal data. We got to a good point in the discussion where we were talking about the nature of finding abstractions. And I wonder where the neural networks can find abstractions. Now, the, the cynical view is that humans kind of create these inductive priors, and they represent the abstraction. So certainly in the case of geometric deep learning, and that's kind of what's happening, we put the, the priors in there to reduce the size of the approximation space. And Keith and I had an interesting idea yesterday that there's a kind of analogy between geometric deep learning and causal representation learning. So I think Keith, you went online and you found a really interesting definition of a causal model, which is that it's kind of immune to, let's say, adversarial examples. So what a model does right now, is it learns a relationship essentially between let's say every single pixel and something happening, right, which is why that model is vulnerable. Yeah, so that was the, and yeah, and I would love to get your comment on that. But that was, you know, this paper, and I could go dig it up, and I can get the reference right now where it said, you know, hey, what is the difference between a causal or prediction from a causal model versus a prediction from a non causal model. And the point was that, well, almost by definition, really, if you have a causal model, then if you perturbed the inputs, the prediction that you get out of it remains a valid, a valid output, because after all, if it's a causal model, and it's reflective of a sort of the causal structure of the world or whatnot, then sure, that's a valid, valid output. Whereas, if it's non causal, it has the potential to learn all these kind of spurious, spurious structures, and therefore, that's why you get the capability of these adversarial examples where you just, you know, put a little rainbow pixel somewhere, and it messes up the class because it had this spurious connection. I mean, in the same vein, you could also, the adversarial examples are there because of inaccuracies, because we don't have the perfect discriminative function, right? I could also say, well, if I just had the correct discriminative functions, it doesn't need to be causal. If I just had like the right partitioning of my input space, then, you know, I'm super not vulnerable to adversarial attacks. I guess the real question would be, would that technically amount to a causal model if I had, you know, the perfect partitioning of the input space into my classes? I don't know, that's like, is there like a mathematical equivalent from that to a causal model? Who knows? Right. Yeah, I think there's probably, it's probably, certainly, if you have the perfect discriminative function, it's probably the discriminative function that you would derive from a causal model. I'm not 100% sure you can go go in the reverse, because I imagine there probably is some some information loss going from, you know, a causal model to, you know, like, for example, I'll give you an example. In the cases of, say, production systems, you know, so, so these little rewriting rules or whatever, the definition there of a causal system is one in which all the potential graphs, all the potential transition graphs that you can get to a particular output are isomorphic. So even though you have you can have like the perfect discriminative kind of function, there may be multiple possible graphs that you could have gotten there, but they're isomorphic. So I'm not quite sure, you know, how that would translate into this, this, this point. But I think you'd be just as good for the purpose of discriminating. I think it's related to the semantics discussion we're having in NLP. So people like Walid Saber say that neural networks don't have semantics. And in the same way, as I was just saying, blue pixels, I mean, in the real world, let's say male testosterone levels is causally linked to incidents of car crashes, which means you can now take the model in in Holland in a different country. And because it's a causal factor, it will extrapolate in the same way. But neural networks models, because what a human does is we would come up with the right representational abstraction, we would build a model, which is very reductionist, a neural network models everything to everything. And the semantics are all one thing. Well, okay, I don't, I'm not, I'm not too, too keen on discussing like semantics and whatnot with with NLP people. But I don't know, you know, like, like, I don't know, it often it often veers away and veers into semantics. It's like it's a bit too, you know, I like what what I think Conor Conor Lay, he said, like, when we talked him along. Oh, he wouldn't like that. I talked him a long time ago. And I happen to agree with him there is that you sort of have to see everything from the perspective of these models. Like if I'm a GPT three, my entire world is text input, right? And people can't somehow judge GPT three by, well, you don't even have whatever a connection to the real world, you don't even know that you don't go see a doctor if your plant is sick, right? Like, how can you not know that? Like, okay, they don't live in the real world, they live in the text world of the internet. And in that world, I'm not sure if there is not a level of abstraction happening in these models. Like, it's, it's, it's, um, yeah, I'm, I don't want to, I don't want to claim that these things do not form abstract things, it might not be the same abstract classes that we form, but they definitely form some level of abstraction. And of course, they can't transfer it because we only give them the one modality, right? But they may be able to transfer it between, you know, different areas of text, which they sometimes do, right? And yeah, so that's, I just wouldn't be so conclusive with respect to these things. That that's true. I think we've gone full circle now. So after speaking with Randall Ballastriro about the splines, that almost results in such a cynical reading of MLPs that they're just hash tables, but we're not using MLPs, we're using transformers and we're using CNNs. And actually, if you think of abstraction, just as being extrapolation, I think they are basically synonymous, it's about being able to extrapolate outside of your training set. Then those inductive priors are indeed producing abstractions. But the problem is humans design those inductive priors. What we want is to learn abstractions. And that's the thing that I don't think is happening. I'm kind of I'm kind of on the same page as Yannick and Connor on the one hand, which is, hey, if an abstraction is just a compression, you know, encoding of the input space, then of course, they're learning abstractions, right? I mean, they are throwing away, you know, information and retaining some, some abstract thing. I think, but I think that just kind of devolves into somewhat like, you know, bastardization, if you will, of what people mean when they say abstractions, because the types of abstractions that traditionally we think about as abstractions are simplifications. You know, they're, they're like, simplifications of more general longer range kind of structures. Whereas we know, and I think we all know this for sure, that a lot of the quote unquote abstractions that did a neural network learns are these kind of like shortcuts, right? They're like these low level borderline spurious kinds of abstractions. And that's why they break so easy. That's why they're so brittle. And I mean, there is this vagueness here, right? Like when is an abstraction, a good abstraction, I don't know. But I think it all kind of in a way misses the point. Like, what we're talking about here is that, and this is a lot of what Benjio said, right, which is that the goal here is to figure out how to get machine learning to learn structures that by virtue of their simplification, their simple abstractions are more generalizable out of distribution. Right, like that's, that's really kind of the goal here. And I mean, so the rest of it is just semantics, pun intended. I mean, the if you look across the world, a lot of, let's say, cultures and humans and so on must have the same abstractions, right? So it must mean a little bit that it's not just something you learn during your lifetime, right? So, right? Oh, absolutely. Not correct. It's it's learned by the it's learned by evolution, by the species, by, by life itself. Exactly, right. But but is like the, the analogy to us building in the correct ones as a shortcut for just evolution doing it using essentially random search, right? That is, it might, right, it's, it's a different, it's a different quality of we want machines to learn something. Because usually we think of when we say we want machines to learn something is we want him to ingest data akin to maybe what a human does during its lifetime. But the when you know, these sort of abstractions and the ability to form abstractions, they seem to be happening on a more fundamental shared level. Yes, you just put the pin in the center of the bullseye. I think that's exactly right. You know, there's a lot to be said for me. It's an epiphenomenon. And a lot of intelligence is embodied. And I agree that there's an awful lot of stuff going on and unbeknown to us with this clearly something that most people don't have a grasp on. Maybe this is why at the population level, maybe this is why I'm frequently miscommunicating with people because I never assumed that learning was about, you know, what a human being learns and a human being's lifetime. Like it's, to me, it's always been the evolution, you know, paradigm, it's like what's encoded in your neurons, what's encoded in your DNA, you know, what was learned by bacteria a long time ago, and how did that translate into what human beings are doing. So I don't know why, like, why so many people are focused on what a human being learns in their lifetime. I mean, it's more, you know, why is that the goal? I'm not sure. I know, but we run the risk of being very reductionist because Connolly, he said that it's an open question where the humans are even intelligent. And if you go down that line, very quickly, you start saying, oh, human beings are just hash tables like GBT three, clearly humans are intelligent in some way. Well, you can just take it as a, you know, matter of definition, but it's not a binary thing. Like again, why are we always into this black and white concept, something is or is not intelligent? Like that's not how I view things. I think there's a spectrum of intelligence from like zero to, I don't know, maybe infinity or something, some really large number beyond what what human beings are. And so it's this continuum. So that's why I like chelets kind of on the measure of intelligence, because even though it doesn't actually give us a, you know, quantitative way yet to measure intelligence, it at least is thinking along the right directions, which is how do you measure intelligence? And how do you define it as a category of activity? And then we can kind of get past this black and white, you know, thinking. Well, gentlemen, always a pleasure. Absolutely. Yeah, absolutely.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.56, "text": " my pleasure. And I must say, I've been really impressed by all your", "tokens": [50364, 452, 6834, 13, 400, 286, 1633, 584, 11, 286, 600, 668, 534, 11679, 538, 439, 428, 50542], "temperature": 0.0, "avg_logprob": -0.20586562738185976, "compression_ratio": 1.4470046082949308, "no_speech_prob": 0.14638617634773254}, {"id": 1, "seek": 0, "start": 3.56, "end": 8.36, "text": " questions. It showed that you did prepare and read papers and think", "tokens": [50542, 1651, 13, 467, 4712, 300, 291, 630, 5940, 293, 1401, 10577, 293, 519, 50782], "temperature": 0.0, "avg_logprob": -0.20586562738185976, "compression_ratio": 1.4470046082949308, "no_speech_prob": 0.14638617634773254}, {"id": 2, "seek": 0, "start": 8.36, "end": 12.76, "text": " about it. And that's very much appreciated. Thank you.", "tokens": [50782, 466, 309, 13, 400, 300, 311, 588, 709, 17169, 13, 1044, 291, 13, 51002], "temperature": 0.0, "avg_logprob": -0.20586562738185976, "compression_ratio": 1.4470046082949308, "no_speech_prob": 0.14638617634773254}, {"id": 3, "seek": 0, "start": 21.04, "end": 24.12, "text": " Today is an incredibly special occasion. We have Professor", "tokens": [51416, 2692, 307, 364, 6252, 2121, 9674, 13, 492, 362, 8419, 51570], "temperature": 0.0, "avg_logprob": -0.20586562738185976, "compression_ratio": 1.4470046082949308, "no_speech_prob": 0.14638617634773254}, {"id": 4, "seek": 0, "start": 24.12, "end": 27.96, "text": " Yoshua Benjiro on the show. Just honestly, I just can't get over", "tokens": [51570, 38949, 4398, 3964, 4013, 340, 322, 264, 855, 13, 1449, 6095, 11, 286, 445, 393, 380, 483, 670, 51762], "temperature": 0.0, "avg_logprob": -0.20586562738185976, "compression_ratio": 1.4470046082949308, "no_speech_prob": 0.14638617634773254}, {"id": 5, "seek": 2796, "start": 28.080000000000002, "end": 31.92, "text": " it. But first of all, a little bit of housekeeping. So we've", "tokens": [50370, 309, 13, 583, 700, 295, 439, 11, 257, 707, 857, 295, 48033, 13, 407, 321, 600, 50562], "temperature": 0.0, "avg_logprob": -0.12055851353539361, "compression_ratio": 1.7041800643086817, "no_speech_prob": 0.08439576625823975}, {"id": 6, "seek": 2796, "start": 31.92, "end": 35.32, "text": " just launched a new Discord community. So please jump in", "tokens": [50562, 445, 8730, 257, 777, 32623, 1768, 13, 407, 1767, 3012, 294, 50732], "temperature": 0.0, "avg_logprob": -0.12055851353539361, "compression_ratio": 1.7041800643086817, "no_speech_prob": 0.08439576625823975}, {"id": 7, "seek": 2796, "start": 35.32, "end": 39.160000000000004, "text": " there, say hello, introduce yourself. If you want to be, you", "tokens": [50732, 456, 11, 584, 7751, 11, 5366, 1803, 13, 759, 291, 528, 281, 312, 11, 291, 50924], "temperature": 0.0, "avg_logprob": -0.12055851353539361, "compression_ratio": 1.7041800643086817, "no_speech_prob": 0.08439576625823975}, {"id": 8, "seek": 2796, "start": 39.160000000000004, "end": 42.64, "text": " know, part of the moderating community or just help us do", "tokens": [50924, 458, 11, 644, 295, 264, 10494, 990, 1768, 420, 445, 854, 505, 360, 51098], "temperature": 0.0, "avg_logprob": -0.12055851353539361, "compression_ratio": 1.7041800643086817, "no_speech_prob": 0.08439576625823975}, {"id": 9, "seek": 2796, "start": 42.64, "end": 45.32, "text": " stuff over there, we would love to talk with you. By popular", "tokens": [51098, 1507, 670, 456, 11, 321, 576, 959, 281, 751, 365, 291, 13, 3146, 3743, 51232], "temperature": 0.0, "avg_logprob": -0.12055851353539361, "compression_ratio": 1.7041800643086817, "no_speech_prob": 0.08439576625823975}, {"id": 10, "seek": 2796, "start": 45.32, "end": 47.480000000000004, "text": " demand, we've also added a couple of ways in which you can", "tokens": [51232, 4733, 11, 321, 600, 611, 3869, 257, 1916, 295, 2098, 294, 597, 291, 393, 51340], "temperature": 0.0, "avg_logprob": -0.12055851353539361, "compression_ratio": 1.7041800643086817, "no_speech_prob": 0.08439576625823975}, {"id": 11, "seek": 2796, "start": 47.480000000000004, "end": 51.2, "text": " support us. So we now have a Patreon and a merch store. If", "tokens": [51340, 1406, 505, 13, 407, 321, 586, 362, 257, 15692, 293, 257, 12618, 3531, 13, 759, 51526], "temperature": 0.0, "avg_logprob": -0.12055851353539361, "compression_ratio": 1.7041800643086817, "no_speech_prob": 0.08439576625823975}, {"id": 12, "seek": 2796, "start": 51.2, "end": 53.84, "text": " you're interested in supporting some of the episodes of MLST", "tokens": [51526, 291, 434, 3102, 294, 7231, 512, 295, 264, 9313, 295, 376, 19198, 51, 51658], "temperature": 0.0, "avg_logprob": -0.12055851353539361, "compression_ratio": 1.7041800643086817, "no_speech_prob": 0.08439576625823975}, {"id": 13, "seek": 2796, "start": 53.84, "end": 55.44, "text": " then get in touch with us because we'd love to have a", "tokens": [51658, 550, 483, 294, 2557, 365, 505, 570, 321, 1116, 959, 281, 362, 257, 51738], "temperature": 0.0, "avg_logprob": -0.12055851353539361, "compression_ratio": 1.7041800643086817, "no_speech_prob": 0.08439576625823975}, {"id": 14, "seek": 5544, "start": 55.44, "end": 59.239999999999995, "text": " conversation with you. We're just doing so much cool stuff", "tokens": [50364, 3761, 365, 291, 13, 492, 434, 445, 884, 370, 709, 1627, 1507, 50554], "temperature": 0.0, "avg_logprob": -0.14484774457277172, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.2035139799118042}, {"id": 15, "seek": 5544, "start": 59.239999999999995, "end": 62.36, "text": " this year. We've already recorded about six episodes that we", "tokens": [50554, 341, 1064, 13, 492, 600, 1217, 8287, 466, 2309, 9313, 300, 321, 50710], "temperature": 0.0, "avg_logprob": -0.14484774457277172, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.2035139799118042}, {"id": 16, "seek": 5544, "start": 62.36, "end": 65.32, "text": " haven't released. And we've got some amazing people booked as", "tokens": [50710, 2378, 380, 4736, 13, 400, 321, 600, 658, 512, 2243, 561, 26735, 382, 50858], "temperature": 0.0, "avg_logprob": -0.14484774457277172, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.2035139799118042}, {"id": 17, "seek": 5544, "start": 65.32, "end": 68.8, "text": " well. So yeah, it's going to be incredible. As always, if you", "tokens": [50858, 731, 13, 407, 1338, 11, 309, 311, 516, 281, 312, 4651, 13, 1018, 1009, 11, 498, 291, 51032], "temperature": 0.0, "avg_logprob": -0.14484774457277172, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.2035139799118042}, {"id": 18, "seek": 5544, "start": 68.8, "end": 71.24, "text": " like the content here, please consider hitting the like and", "tokens": [51032, 411, 264, 2701, 510, 11, 1767, 1949, 8850, 264, 411, 293, 51154], "temperature": 0.0, "avg_logprob": -0.14484774457277172, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.2035139799118042}, {"id": 19, "seek": 5544, "start": 71.24, "end": 74.12, "text": " subscribe button and rating our podcast on iTunes because it", "tokens": [51154, 3022, 2960, 293, 10990, 527, 7367, 322, 33017, 570, 309, 51298], "temperature": 0.0, "avg_logprob": -0.14484774457277172, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.2035139799118042}, {"id": 20, "seek": 5544, "start": 74.12, "end": 76.64, "text": " really, really helps us out. I called it iTunes. Is it", "tokens": [51298, 534, 11, 534, 3665, 505, 484, 13, 286, 1219, 309, 33017, 13, 1119, 309, 51424], "temperature": 0.0, "avg_logprob": -0.14484774457277172, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.2035139799118042}, {"id": 21, "seek": 5544, "start": 76.64, "end": 79.75999999999999, "text": " iTunes? Apple podcasts? I don't know, whatever it's called.", "tokens": [51424, 33017, 30, 6373, 24045, 30, 286, 500, 380, 458, 11, 2035, 309, 311, 1219, 13, 51580], "temperature": 0.0, "avg_logprob": -0.14484774457277172, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.2035139799118042}, {"id": 22, "seek": 5544, "start": 81.2, "end": 84.52, "text": " Wait and Biases is the developer first MLS platform. And", "tokens": [51652, 3802, 293, 13007, 1957, 307, 264, 10754, 700, 376, 19198, 3663, 13, 400, 51818], "temperature": 0.0, "avg_logprob": -0.14484774457277172, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.2035139799118042}, {"id": 23, "seek": 8452, "start": 84.52, "end": 87.39999999999999, "text": " we're extremely proud today that they are sponsoring this", "tokens": [50364, 321, 434, 4664, 4570, 965, 300, 436, 366, 30311, 341, 50508], "temperature": 0.0, "avg_logprob": -0.10762964476139174, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.008058273233473301}, {"id": 24, "seek": 8452, "start": 87.39999999999999, "end": 90.84, "text": " episode. Now tracking machine learning experiments is", "tokens": [50508, 3500, 13, 823, 11603, 3479, 2539, 12050, 307, 50680], "temperature": 0.0, "avg_logprob": -0.10762964476139174, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.008058273233473301}, {"id": 25, "seek": 8452, "start": 90.84, "end": 94.84, "text": " difficult. Using the winging it methodology can only get you so", "tokens": [50680, 2252, 13, 11142, 264, 261, 8716, 309, 24850, 393, 787, 483, 291, 370, 50880], "temperature": 0.0, "avg_logprob": -0.10762964476139174, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.008058273233473301}, {"id": 26, "seek": 8452, "start": 94.84, "end": 98.96, "text": " far. What we need is a platform where we can compare models and", "tokens": [50880, 1400, 13, 708, 321, 643, 307, 257, 3663, 689, 321, 393, 6794, 5245, 293, 51086], "temperature": 0.0, "avg_logprob": -0.10762964476139174, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.008058273233473301}, {"id": 27, "seek": 8452, "start": 98.96, "end": 102.16, "text": " visualize their performance characteristics against all of", "tokens": [51086, 23273, 641, 3389, 10891, 1970, 439, 295, 51246], "temperature": 0.0, "avg_logprob": -0.10762964476139174, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.008058273233473301}, {"id": 28, "seek": 8452, "start": 102.16, "end": 105.39999999999999, "text": " the previous runs and figure out the best hyper parameters to", "tokens": [51246, 264, 3894, 6676, 293, 2573, 484, 264, 1151, 9848, 9834, 281, 51408], "temperature": 0.0, "avg_logprob": -0.10762964476139174, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.008058273233473301}, {"id": 29, "seek": 8452, "start": 105.39999999999999, "end": 109.24, "text": " use. Now most importantly of all, this process needs to be", "tokens": [51408, 764, 13, 823, 881, 8906, 295, 439, 11, 341, 1399, 2203, 281, 312, 51600], "temperature": 0.0, "avg_logprob": -0.10762964476139174, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.008058273233473301}, {"id": 30, "seek": 8452, "start": 109.32, "end": 112.92, "text": " reproducible. Sounds like a tool order, right? Well, this is", "tokens": [51604, 11408, 32128, 13, 14576, 411, 257, 2290, 1668, 11, 558, 30, 1042, 11, 341, 307, 51784], "temperature": 0.0, "avg_logprob": -0.10762964476139174, "compression_ratio": 1.610738255033557, "no_speech_prob": 0.008058273233473301}, {"id": 31, "seek": 11292, "start": 112.92, "end": 117.32000000000001, "text": " exactly what the Wait and Biases platform does for you. Now you", "tokens": [50364, 2293, 437, 264, 3802, 293, 13007, 1957, 3663, 775, 337, 291, 13, 823, 291, 50584], "temperature": 0.0, "avg_logprob": -0.11540237908224457, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0028888469096273184}, {"id": 32, "seek": 11292, "start": 117.32000000000001, "end": 120.68, "text": " can even follow metrics from long running experiments in real", "tokens": [50584, 393, 754, 1524, 16367, 490, 938, 2614, 12050, 294, 957, 50752], "temperature": 0.0, "avg_logprob": -0.11540237908224457, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0028888469096273184}, {"id": 33, "seek": 11292, "start": 120.68, "end": 124.24000000000001, "text": " time. I think it's really important to lean into the", "tokens": [50752, 565, 13, 286, 519, 309, 311, 534, 1021, 281, 11659, 666, 264, 50930], "temperature": 0.0, "avg_logprob": -0.11540237908224457, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0028888469096273184}, {"id": 34, "seek": 11292, "start": 124.24000000000001, "end": 128.6, "text": " complex interaction between science and engineering in the", "tokens": [50930, 3997, 9285, 1296, 3497, 293, 7043, 294, 264, 51148], "temperature": 0.0, "avg_logprob": -0.11540237908224457, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0028888469096273184}, {"id": 35, "seek": 11292, "start": 128.6, "end": 132.84, "text": " ML DevOps lifecycle. Data scientists need valuable feedback", "tokens": [51148, 21601, 43051, 45722, 13, 11888, 7708, 643, 8263, 5824, 51360], "temperature": 0.0, "avg_logprob": -0.11540237908224457, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0028888469096273184}, {"id": 36, "seek": 11292, "start": 132.92000000000002, "end": 135.32, "text": " and they need to communicate why they're running given", "tokens": [51364, 293, 436, 643, 281, 7890, 983, 436, 434, 2614, 2212, 51484], "temperature": 0.0, "avg_logprob": -0.11540237908224457, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0028888469096273184}, {"id": 37, "seek": 11292, "start": 135.32, "end": 138.68, "text": " experiments and they need to share their notes around the next", "tokens": [51484, 12050, 293, 436, 643, 281, 2073, 641, 5570, 926, 264, 958, 51652], "temperature": 0.0, "avg_logprob": -0.11540237908224457, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0028888469096273184}, {"id": 38, "seek": 11292, "start": 138.68, "end": 142.88, "text": " steps. Reports keep this work well organized and connected to", "tokens": [51652, 4439, 13, 45910, 1066, 341, 589, 731, 9983, 293, 4582, 281, 51862], "temperature": 0.0, "avg_logprob": -0.11540237908224457, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0028888469096273184}, {"id": 39, "seek": 14288, "start": 142.88, "end": 146.07999999999998, "text": " the Waits and Biases experiments which were run as opposed to", "tokens": [50364, 264, 3802, 82, 293, 13007, 1957, 12050, 597, 645, 1190, 382, 8851, 281, 50524], "temperature": 0.0, "avg_logprob": -0.08819065574838333, "compression_ratio": 1.6555183946488294, "no_speech_prob": 0.0003569336258806288}, {"id": 40, "seek": 14288, "start": 146.07999999999998, "end": 150.07999999999998, "text": " just sharing random screenshots in Slack. It's so easy to create", "tokens": [50524, 445, 5414, 4974, 40661, 294, 37211, 13, 467, 311, 370, 1858, 281, 1884, 50724], "temperature": 0.0, "avg_logprob": -0.08819065574838333, "compression_ratio": 1.6555183946488294, "no_speech_prob": 0.0003569336258806288}, {"id": 41, "seek": 14288, "start": 150.07999999999998, "end": 152.72, "text": " a report and share it with your team after you finished with", "tokens": [50724, 257, 2275, 293, 2073, 309, 365, 428, 1469, 934, 291, 4335, 365, 50856], "temperature": 0.0, "avg_logprob": -0.08819065574838333, "compression_ratio": 1.6555183946488294, "no_speech_prob": 0.0003569336258806288}, {"id": 42, "seek": 14288, "start": 152.72, "end": 155.56, "text": " your experimentation. You could just add notes for yourself as", "tokens": [50856, 428, 37142, 13, 509, 727, 445, 909, 5570, 337, 1803, 382, 50998], "temperature": 0.0, "avg_logprob": -0.08819065574838333, "compression_ratio": 1.6555183946488294, "no_speech_prob": 0.0003569336258806288}, {"id": 43, "seek": 14288, "start": 155.56, "end": 158.64, "text": " well to explore later on. You can keep a work log and you can", "tokens": [50998, 731, 281, 6839, 1780, 322, 13, 509, 393, 1066, 257, 589, 3565, 293, 291, 393, 51152], "temperature": 0.0, "avg_logprob": -0.08819065574838333, "compression_ratio": 1.6555183946488294, "no_speech_prob": 0.0003569336258806288}, {"id": 44, "seek": 14288, "start": 158.64, "end": 162.28, "text": " even share your findings internally or externally. This is", "tokens": [51152, 754, 2073, 428, 16483, 19501, 420, 40899, 13, 639, 307, 51334], "temperature": 0.0, "avg_logprob": -0.08819065574838333, "compression_ratio": 1.6555183946488294, "no_speech_prob": 0.0003569336258806288}, {"id": 45, "seek": 14288, "start": 162.28, "end": 165.96, "text": " an absolute game changer. I'm a big believer in this kind of", "tokens": [51334, 364, 8236, 1216, 22822, 13, 286, 478, 257, 955, 23892, 294, 341, 733, 295, 51518], "temperature": 0.0, "avg_logprob": -0.08819065574838333, "compression_ratio": 1.6555183946488294, "no_speech_prob": 0.0003569336258806288}, {"id": 46, "seek": 14288, "start": 165.96, "end": 169.51999999999998, "text": " engineering rigor. I'm the CEO of a code review startup called", "tokens": [51518, 7043, 42191, 13, 286, 478, 264, 9282, 295, 257, 3089, 3131, 18578, 1219, 51696], "temperature": 0.0, "avg_logprob": -0.08819065574838333, "compression_ratio": 1.6555183946488294, "no_speech_prob": 0.0003569336258806288}, {"id": 47, "seek": 16952, "start": 169.52, "end": 173.60000000000002, "text": " Merge these days and I love how the pull request process and", "tokens": [50364, 6124, 432, 613, 1708, 293, 286, 959, 577, 264, 2235, 5308, 1399, 293, 50568], "temperature": 0.0, "avg_logprob": -0.15334158755363303, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.3988456428050995}, {"id": 48, "seek": 16952, "start": 173.60000000000002, "end": 177.28, "text": " tooling immortalizes important collective decisions which were", "tokens": [50568, 46593, 31414, 5660, 1021, 12590, 5327, 597, 645, 50752], "temperature": 0.0, "avg_logprob": -0.15334158755363303, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.3988456428050995}, {"id": 49, "seek": 16952, "start": 177.28, "end": 180.52, "text": " made during the software development lifecycle. Similarly,", "tokens": [50752, 1027, 1830, 264, 4722, 3250, 45722, 13, 13157, 11, 50914], "temperature": 0.0, "avg_logprob": -0.15334158755363303, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.3988456428050995}, {"id": 50, "seek": 16952, "start": 180.72, "end": 184.0, "text": " Waits and Biases immortalizes important decisions that were", "tokens": [50924, 3802, 82, 293, 13007, 1957, 31414, 5660, 1021, 5327, 300, 645, 51088], "temperature": 0.0, "avg_logprob": -0.15334158755363303, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.3988456428050995}, {"id": 51, "seek": 16952, "start": 184.0, "end": 187.88, "text": " made during model development, experimentation and", "tokens": [51088, 1027, 1830, 2316, 3250, 11, 37142, 293, 51282], "temperature": 0.0, "avg_logprob": -0.15334158755363303, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.3988456428050995}, {"id": 52, "seek": 16952, "start": 187.88, "end": 192.32000000000002, "text": " deployment. Remember, check out Waits and Biases today by going", "tokens": [51282, 19317, 13, 5459, 11, 1520, 484, 3802, 82, 293, 13007, 1957, 965, 538, 516, 51504], "temperature": 0.0, "avg_logprob": -0.15334158755363303, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.3988456428050995}, {"id": 53, "seek": 16952, "start": 192.32000000000002, "end": 196.72, "text": " to 1db.com forward slash MLST and if you're interested in", "tokens": [51504, 281, 502, 67, 65, 13, 1112, 2128, 17330, 21601, 6840, 293, 498, 291, 434, 3102, 294, 51724], "temperature": 0.0, "avg_logprob": -0.15334158755363303, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.3988456428050995}, {"id": 54, "seek": 19672, "start": 196.72, "end": 199.96, "text": " sponsoring future episodes, get in touch with us. Waits and", "tokens": [50364, 30311, 2027, 9313, 11, 483, 294, 2557, 365, 505, 13, 3802, 82, 293, 50526], "temperature": 0.0, "avg_logprob": -0.16606284890856063, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.01185102853924036}, {"id": 55, "seek": 19672, "start": 199.96, "end": 203.24, "text": " Biases are currently sponsoring our premiere shows but we have", "tokens": [50526, 13007, 1957, 366, 4362, 30311, 527, 28372, 3110, 457, 321, 362, 50690], "temperature": 0.0, "avg_logprob": -0.16606284890856063, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.01185102853924036}, {"id": 56, "seek": 19672, "start": 203.24, "end": 205.52, "text": " lots of other content coming and opportunities for", "tokens": [50690, 3195, 295, 661, 2701, 1348, 293, 4786, 337, 50804], "temperature": 0.0, "avg_logprob": -0.16606284890856063, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.01185102853924036}, {"id": 57, "seek": 19672, "start": 205.52, "end": 207.72, "text": " sponsorship so let us know. Cheers.", "tokens": [50804, 42922, 370, 718, 505, 458, 13, 13006, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16606284890856063, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.01185102853924036}, {"id": 58, "seek": 19672, "start": 209.48, "end": 212.24, "text": " Professor Yoshua Benjo has just released a bunch of papers", "tokens": [51002, 8419, 38949, 4398, 3964, 5134, 575, 445, 4736, 257, 3840, 295, 10577, 51140], "temperature": 0.0, "avg_logprob": -0.16606284890856063, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.01185102853924036}, {"id": 59, "seek": 19672, "start": 212.28, "end": 216.16, "text": " around G-Flow Nets. Now G-Flow Nets exists squarely in the", "tokens": [51142, 926, 460, 12, 31091, 426, 1385, 13, 823, 460, 12, 31091, 426, 1385, 8198, 3732, 356, 294, 264, 51336], "temperature": 0.0, "avg_logprob": -0.16606284890856063, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.01185102853924036}, {"id": 60, "seek": 19672, "start": 216.16, "end": 220.8, "text": " domain of active learning which is a model that can economically", "tokens": [51336, 9274, 295, 4967, 2539, 597, 307, 257, 2316, 300, 393, 26811, 51568], "temperature": 0.0, "avg_logprob": -0.16606284890856063, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.01185102853924036}, {"id": 61, "seek": 19672, "start": 220.8, "end": 223.92, "text": " ask an oracle which is probably the real world for the most", "tokens": [51568, 1029, 364, 420, 7041, 597, 307, 1391, 264, 957, 1002, 337, 264, 881, 51724], "temperature": 0.0, "avg_logprob": -0.16606284890856063, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.01185102853924036}, {"id": 62, "seek": 22392, "start": 223.92, "end": 228.28, "text": " salient training examples to continue learning. The learner", "tokens": [50364, 1845, 1196, 3097, 5110, 281, 2354, 2539, 13, 440, 33347, 50582], "temperature": 0.0, "avg_logprob": -0.08911021120913394, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.015415736474096775}, {"id": 63, "seek": 22392, "start": 228.28, "end": 231.72, "text": " can choose or have an influence on the examples it gets and we", "tokens": [50582, 393, 2826, 420, 362, 364, 6503, 322, 264, 5110, 309, 2170, 293, 321, 50754], "temperature": 0.0, "avg_logprob": -0.08911021120913394, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.015415736474096775}, {"id": 64, "seek": 22392, "start": 231.72, "end": 234.6, "text": " want to learn a function which approximates the oracle", "tokens": [50754, 528, 281, 1466, 257, 2445, 597, 8542, 1024, 264, 420, 7041, 50898], "temperature": 0.0, "avg_logprob": -0.08911021120913394, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.015415736474096775}, {"id": 65, "seek": 22392, "start": 234.64, "end": 238.32, "text": " efficiently. How should we pick the queries? How should we take", "tokens": [50900, 19621, 13, 1012, 820, 321, 1888, 264, 24109, 30, 1012, 820, 321, 747, 51084], "temperature": 0.0, "avg_logprob": -0.08911021120913394, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.015415736474096775}, {"id": 66, "seek": 22392, "start": 238.32, "end": 242.39999999999998, "text": " into account not just the value of the predictor but also how", "tokens": [51084, 666, 2696, 406, 445, 264, 2158, 295, 264, 6069, 284, 457, 611, 577, 51288], "temperature": 0.0, "avg_logprob": -0.08911021120913394, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.015415736474096775}, {"id": 67, "seek": 22392, "start": 242.39999999999998, "end": 245.27999999999997, "text": " certain we are about the predictors from the learning", "tokens": [51288, 1629, 321, 366, 466, 264, 6069, 830, 490, 264, 2539, 51432], "temperature": 0.0, "avg_logprob": -0.08911021120913394, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.015415736474096775}, {"id": 68, "seek": 22392, "start": 245.27999999999997, "end": 250.0, "text": " system? Areas of uncertainty or entropy are kind of like", "tokens": [51432, 1185, 30, 2014, 296, 295, 15697, 420, 30867, 366, 733, 295, 411, 51668], "temperature": 0.0, "avg_logprob": -0.08911021120913394, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.015415736474096775}, {"id": 69, "seek": 22392, "start": 250.04, "end": 253.79999999999998, "text": " interesting candidates for us to explore further. We need to", "tokens": [51670, 1880, 11255, 337, 505, 281, 6839, 3052, 13, 492, 643, 281, 51858], "temperature": 0.0, "avg_logprob": -0.08911021120913394, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.015415736474096775}, {"id": 70, "seek": 25380, "start": 253.8, "end": 258.76, "text": " be able to imagine or invent queries to give to the oracle.", "tokens": [50364, 312, 1075, 281, 3811, 420, 7962, 24109, 281, 976, 281, 264, 420, 7041, 13, 50612], "temperature": 0.0, "avg_logprob": -0.10406634237913959, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008557820692658424}, {"id": 71, "seek": 25380, "start": 259.32, "end": 261.8, "text": " Now one of the reasons that machine learning models are so", "tokens": [50640, 823, 472, 295, 264, 4112, 300, 3479, 2539, 5245, 366, 370, 50764], "temperature": 0.0, "avg_logprob": -0.10406634237913959, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008557820692658424}, {"id": 72, "seek": 25380, "start": 261.8, "end": 265.12, "text": " sample and efficient is because of the combinatorial space of", "tokens": [50764, 6889, 293, 7148, 307, 570, 295, 264, 2512, 31927, 831, 1901, 295, 50930], "temperature": 0.0, "avg_logprob": -0.10406634237913959, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008557820692658424}, {"id": 73, "seek": 25380, "start": 265.12, "end": 268.92, "text": " possible input examples. We can't train on everything because", "tokens": [50930, 1944, 4846, 5110, 13, 492, 393, 380, 3847, 322, 1203, 570, 51120], "temperature": 0.0, "avg_logprob": -0.10406634237913959, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008557820692658424}, {"id": 74, "seek": 25380, "start": 268.92, "end": 272.88, "text": " the space is just too large, it's vast. So you might have heard", "tokens": [51120, 264, 1901, 307, 445, 886, 2416, 11, 309, 311, 8369, 13, 407, 291, 1062, 362, 2198, 51318], "temperature": 0.0, "avg_logprob": -0.10406634237913959, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008557820692658424}, {"id": 75, "seek": 25380, "start": 272.88, "end": 275.64, "text": " of a related concept of active learning called machine", "tokens": [51318, 295, 257, 4077, 3410, 295, 4967, 2539, 1219, 3479, 51456], "temperature": 0.0, "avg_logprob": -0.10406634237913959, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008557820692658424}, {"id": 76, "seek": 25380, "start": 275.64, "end": 278.88, "text": " teaching which is an interactive version where the human", "tokens": [51456, 4571, 597, 307, 364, 15141, 3037, 689, 264, 1952, 51618], "temperature": 0.0, "avg_logprob": -0.10406634237913959, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008557820692658424}, {"id": 77, "seek": 25380, "start": 279.16, "end": 283.40000000000003, "text": " interactively selects the most salient data to train a machine", "tokens": [51632, 4648, 3413, 3048, 82, 264, 881, 1845, 1196, 1412, 281, 3847, 257, 3479, 51844], "temperature": 0.0, "avg_logprob": -0.10406634237913959, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008557820692658424}, {"id": 78, "seek": 28340, "start": 283.4, "end": 287.47999999999996, "text": " learning model maximizing the information gain in respect of", "tokens": [50364, 2539, 2316, 5138, 3319, 264, 1589, 6052, 294, 3104, 295, 50568], "temperature": 0.0, "avg_logprob": -0.12577193848630216, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0013666606973856688}, {"id": 79, "seek": 28340, "start": 287.47999999999996, "end": 293.2, "text": " the training samples. Now the reality is the function space", "tokens": [50568, 264, 3097, 10938, 13, 823, 264, 4103, 307, 264, 2445, 1901, 50854], "temperature": 0.0, "avg_logprob": -0.12577193848630216, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0013666606973856688}, {"id": 80, "seek": 28340, "start": 293.28, "end": 296.76, "text": " that we're learning here is highly structured. We only really", "tokens": [50858, 300, 321, 434, 2539, 510, 307, 5405, 18519, 13, 492, 787, 534, 51032], "temperature": 0.0, "avg_logprob": -0.12577193848630216, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0013666606973856688}, {"id": 81, "seek": 28340, "start": 296.76, "end": 299.76, "text": " need to sample training data where most of the rich", "tokens": [51032, 643, 281, 6889, 3097, 1412, 689, 881, 295, 264, 4593, 51182], "temperature": 0.0, "avg_logprob": -0.12577193848630216, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0013666606973856688}, {"id": 82, "seek": 28340, "start": 299.76, "end": 302.91999999999996, "text": " information exists in that function space. I mean, if you", "tokens": [51182, 1589, 8198, 294, 300, 2445, 1901, 13, 286, 914, 11, 498, 291, 51340], "temperature": 0.0, "avg_logprob": -0.12577193848630216, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0013666606973856688}, {"id": 83, "seek": 28340, "start": 302.91999999999996, "end": 305.52, "text": " think about it, a machine learning model, it's just a joint", "tokens": [51340, 519, 466, 309, 11, 257, 3479, 2539, 2316, 11, 309, 311, 445, 257, 7225, 51470], "temperature": 0.0, "avg_logprob": -0.12577193848630216, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0013666606973856688}, {"id": 84, "seek": 28340, "start": 305.52, "end": 309.52, "text": " probability distribution between signals and labels. And this", "tokens": [51470, 8482, 7316, 1296, 12354, 293, 16949, 13, 400, 341, 51670], "temperature": 0.0, "avg_logprob": -0.12577193848630216, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0013666606973856688}, {"id": 85, "seek": 30952, "start": 309.52, "end": 316.28, "text": " distribution has modes or areas of density or information. And", "tokens": [50364, 7316, 575, 14068, 420, 3179, 295, 10305, 420, 1589, 13, 400, 50702], "temperature": 0.0, "avg_logprob": -0.14059826532999675, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.004006205126643181}, {"id": 86, "seek": 30952, "start": 316.4, "end": 320.52, "text": " actually most of it is just areas of nothingness, which", "tokens": [50708, 767, 881, 295, 309, 307, 445, 3179, 295, 1825, 1287, 11, 597, 50914], "temperature": 0.0, "avg_logprob": -0.14059826532999675, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.004006205126643181}, {"id": 87, "seek": 30952, "start": 320.52, "end": 325.59999999999997, "text": " require fewer training examples to learn and to represent. Now,", "tokens": [50914, 3651, 13366, 3097, 5110, 281, 1466, 293, 281, 2906, 13, 823, 11, 51168], "temperature": 0.0, "avg_logprob": -0.14059826532999675, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.004006205126643181}, {"id": 88, "seek": 30952, "start": 326.03999999999996, "end": 329.28, "text": " if you spoke to a to a Bayesian person like my friend Conor", "tokens": [51190, 498, 291, 7179, 281, 257, 281, 257, 7840, 42434, 954, 411, 452, 1277, 2656, 284, 51352], "temperature": 0.0, "avg_logprob": -0.14059826532999675, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.004006205126643181}, {"id": 89, "seek": 30952, "start": 329.28, "end": 333.0, "text": " Tan at work, you know how to learn this distribution, they", "tokens": [51352, 17046, 412, 589, 11, 291, 458, 577, 281, 1466, 341, 7316, 11, 436, 51538], "temperature": 0.0, "avg_logprob": -0.14059826532999675, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.004006205126643181}, {"id": 90, "seek": 30952, "start": 333.0, "end": 336.71999999999997, "text": " would bring up Markov chain Monte Carlo quicker than a whip it", "tokens": [51538, 576, 1565, 493, 3934, 5179, 5021, 38105, 45112, 16255, 813, 257, 22377, 309, 51724], "temperature": 0.0, "avg_logprob": -0.14059826532999675, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.004006205126643181}, {"id": 91, "seek": 33672, "start": 336.72, "end": 340.92, "text": " with a bumful of dynamite. Now Markov chain Monte Carlo is an", "tokens": [50364, 365, 257, 13309, 906, 295, 5999, 642, 13, 823, 3934, 5179, 5021, 38105, 45112, 307, 364, 50574], "temperature": 0.0, "avg_logprob": -0.10389389862885347, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0038226277101784945}, {"id": 92, "seek": 33672, "start": 340.92, "end": 344.20000000000005, "text": " increasingly popular sampling method for obtaining asymptotic", "tokens": [50574, 12980, 3743, 21179, 3170, 337, 36749, 35114, 9411, 50738], "temperature": 0.0, "avg_logprob": -0.10389389862885347, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0038226277101784945}, {"id": 93, "seek": 33672, "start": 344.20000000000005, "end": 347.16, "text": " information about unnormalised distributions or energy", "tokens": [50738, 1589, 466, 517, 23157, 2640, 37870, 420, 2281, 50886], "temperature": 0.0, "avg_logprob": -0.10389389862885347, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0038226277101784945}, {"id": 94, "seek": 33672, "start": 347.16, "end": 350.32000000000005, "text": " functions, especially for estimating the posterior", "tokens": [50886, 6828, 11, 2318, 337, 8017, 990, 264, 33529, 51044], "temperature": 0.0, "avg_logprob": -0.10389389862885347, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0038226277101784945}, {"id": 95, "seek": 33672, "start": 350.32000000000005, "end": 353.0, "text": " distribution in Bayesian inference, which is where you've", "tokens": [51044, 7316, 294, 7840, 42434, 38253, 11, 597, 307, 689, 291, 600, 51178], "temperature": 0.0, "avg_logprob": -0.10389389862885347, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0038226277101784945}, {"id": 96, "seek": 33672, "start": 353.0, "end": 355.88000000000005, "text": " probably heard of it before. Now you can characterize a", "tokens": [51178, 1391, 2198, 295, 309, 949, 13, 823, 291, 393, 38463, 257, 51322], "temperature": 0.0, "avg_logprob": -0.10389389862885347, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0038226277101784945}, {"id": 97, "seek": 33672, "start": 355.88000000000005, "end": 358.40000000000003, "text": " distribution without knowing all of the distributions", "tokens": [51322, 7316, 1553, 5276, 439, 295, 264, 37870, 51448], "temperature": 0.0, "avg_logprob": -0.10389389862885347, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0038226277101784945}, {"id": 98, "seek": 33672, "start": 358.40000000000003, "end": 360.6, "text": " mathematical properties. So if you don't have an analytical", "tokens": [51448, 18894, 7221, 13, 407, 498, 291, 500, 380, 362, 364, 29579, 51558], "temperature": 0.0, "avg_logprob": -0.10389389862885347, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0038226277101784945}, {"id": 99, "seek": 33672, "start": 360.6, "end": 364.08000000000004, "text": " representation for it, just by randomly sampling values out of", "tokens": [51558, 10290, 337, 309, 11, 445, 538, 16979, 21179, 4190, 484, 295, 51732], "temperature": 0.0, "avg_logprob": -0.10389389862885347, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0038226277101784945}, {"id": 100, "seek": 36408, "start": 364.08, "end": 367.88, "text": " the distribution. Now a particular strength of Markov chain", "tokens": [50364, 264, 7316, 13, 823, 257, 1729, 3800, 295, 3934, 5179, 5021, 50554], "temperature": 0.0, "avg_logprob": -0.0876952942381514, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0016740302089601755}, {"id": 101, "seek": 36408, "start": 367.88, "end": 370.64, "text": " Monte Carlo is that it can be used to draw samples from", "tokens": [50554, 38105, 45112, 307, 300, 309, 393, 312, 1143, 281, 2642, 10938, 490, 50692], "temperature": 0.0, "avg_logprob": -0.0876952942381514, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0016740302089601755}, {"id": 102, "seek": 36408, "start": 370.64, "end": 374.32, "text": " distributions, even when all that is known about the", "tokens": [50692, 37870, 11, 754, 562, 439, 300, 307, 2570, 466, 264, 50876], "temperature": 0.0, "avg_logprob": -0.0876952942381514, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0016740302089601755}, {"id": 103, "seek": 36408, "start": 374.32, "end": 377.68, "text": " distribution is how to calculate the density for different", "tokens": [50876, 7316, 307, 577, 281, 8873, 264, 10305, 337, 819, 51044], "temperature": 0.0, "avg_logprob": -0.0876952942381514, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0016740302089601755}, {"id": 104, "seek": 36408, "start": 377.68, "end": 382.28, "text": " samples. Now the the Markov property of Markov chain Monte", "tokens": [51044, 10938, 13, 823, 264, 264, 3934, 5179, 4707, 295, 3934, 5179, 5021, 38105, 51274], "temperature": 0.0, "avg_logprob": -0.0876952942381514, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0016740302089601755}, {"id": 105, "seek": 36408, "start": 382.28, "end": 386.4, "text": " Carlo is this idea that random samples are generated by a", "tokens": [51274, 45112, 307, 341, 1558, 300, 4974, 10938, 366, 10833, 538, 257, 51480], "temperature": 0.0, "avg_logprob": -0.0876952942381514, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0016740302089601755}, {"id": 106, "seek": 36408, "start": 386.4, "end": 390.52, "text": " special sequential process. And each random sample is used as a", "tokens": [51480, 2121, 42881, 1399, 13, 400, 1184, 4974, 6889, 307, 1143, 382, 257, 51686], "temperature": 0.0, "avg_logprob": -0.0876952942381514, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0016740302089601755}, {"id": 107, "seek": 39052, "start": 390.52, "end": 395.68, "text": " stepping stone to generate the next random sample. Now this", "tokens": [50364, 16821, 7581, 281, 8460, 264, 958, 4974, 6889, 13, 823, 341, 50622], "temperature": 0.0, "avg_logprob": -0.11228071513928865, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.009407016448676586}, {"id": 108, "seek": 39052, "start": 395.68, "end": 399.2, "text": " might sound very complex, but the practical implementation is", "tokens": [50622, 1062, 1626, 588, 3997, 11, 457, 264, 8496, 11420, 307, 50798], "temperature": 0.0, "avg_logprob": -0.11228071513928865, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.009407016448676586}, {"id": 109, "seek": 39052, "start": 399.2, "end": 402.32, "text": " pretty simple. Markov chain Monte Carlo just starts with an", "tokens": [50798, 1238, 2199, 13, 3934, 5179, 5021, 38105, 45112, 445, 3719, 365, 364, 50954], "temperature": 0.0, "avg_logprob": -0.11228071513928865, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.009407016448676586}, {"id": 110, "seek": 39052, "start": 402.32, "end": 406.52, "text": " initial guess, just one value that might plausibly be drawn", "tokens": [50954, 5883, 2041, 11, 445, 472, 2158, 300, 1062, 34946, 3545, 312, 10117, 51164], "temperature": 0.0, "avg_logprob": -0.11228071513928865, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.009407016448676586}, {"id": 111, "seek": 39052, "start": 406.52, "end": 410.03999999999996, "text": " from the distribution. And then we produce a chain of samples", "tokens": [51164, 490, 264, 7316, 13, 400, 550, 321, 5258, 257, 5021, 295, 10938, 51340], "temperature": 0.0, "avg_logprob": -0.11228071513928865, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.009407016448676586}, {"id": 112, "seek": 39052, "start": 410.03999999999996, "end": 414.0, "text": " from this initial guess by adding random perturbations in the", "tokens": [51340, 490, 341, 5883, 2041, 538, 5127, 4974, 40468, 763, 294, 264, 51538], "temperature": 0.0, "avg_logprob": -0.11228071513928865, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.009407016448676586}, {"id": 113, "seek": 39052, "start": 414.0, "end": 418.24, "text": " neighborhood of that example. And each new proposal drawn from", "tokens": [51538, 7630, 295, 300, 1365, 13, 400, 1184, 777, 11494, 10117, 490, 51750], "temperature": 0.0, "avg_logprob": -0.11228071513928865, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.009407016448676586}, {"id": 114, "seek": 41824, "start": 418.24, "end": 420.96000000000004, "text": " that random perturbation distribution is either rejected", "tokens": [50364, 300, 4974, 40468, 399, 7316, 307, 2139, 15749, 50500], "temperature": 0.0, "avg_logprob": -0.12569408946567112, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0013881282648071647}, {"id": 115, "seek": 41824, "start": 421.04, "end": 424.36, "text": " or accepted. There are different flavors of this, of course, I", "tokens": [50504, 420, 9035, 13, 821, 366, 819, 16303, 295, 341, 11, 295, 1164, 11, 286, 50670], "temperature": 0.0, "avg_logprob": -0.12569408946567112, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0013881282648071647}, {"id": 116, "seek": 41824, "start": 424.36, "end": 427.96000000000004, "text": " mean, in particular, like tweaking, how the random", "tokens": [50670, 914, 11, 294, 1729, 11, 411, 6986, 2456, 11, 577, 264, 4974, 50850], "temperature": 0.0, "avg_logprob": -0.12569408946567112, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0013881282648071647}, {"id": 117, "seek": 41824, "start": 427.96000000000004, "end": 430.92, "text": " proposals in the neighborhood are selected or whether the", "tokens": [50850, 20198, 294, 264, 7630, 366, 8209, 420, 1968, 264, 50998], "temperature": 0.0, "avg_logprob": -0.12569408946567112, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0013881282648071647}, {"id": 118, "seek": 41824, "start": 430.92, "end": 435.2, "text": " proposals are selected. The simplest heuristic being whether", "tokens": [50998, 20198, 366, 8209, 13, 440, 22811, 415, 374, 3142, 885, 1968, 51212], "temperature": 0.0, "avg_logprob": -0.12569408946567112, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0013881282648071647}, {"id": 119, "seek": 41824, "start": 435.2, "end": 439.08, "text": " it's below the function or not. Now the idea is that Markov", "tokens": [51212, 309, 311, 2507, 264, 2445, 420, 406, 13, 823, 264, 1558, 307, 300, 3934, 5179, 51406], "temperature": 0.0, "avg_logprob": -0.12569408946567112, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0013881282648071647}, {"id": 120, "seek": 41824, "start": 439.08, "end": 442.44, "text": " chain Monte Carlo methods, they capture a distribution with", "tokens": [51406, 5021, 38105, 45112, 7150, 11, 436, 7983, 257, 7316, 365, 51574], "temperature": 0.0, "avg_logprob": -0.12569408946567112, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0013881282648071647}, {"id": 121, "seek": 41824, "start": 442.44, "end": 445.96000000000004, "text": " only a relatively small number of random samples. But the", "tokens": [51574, 787, 257, 7226, 1359, 1230, 295, 4974, 10938, 13, 583, 264, 51750], "temperature": 0.0, "avg_logprob": -0.12569408946567112, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0013881282648071647}, {"id": 122, "seek": 44596, "start": 445.96, "end": 451.23999999999995, "text": " reality is anything but in high dimensions, and where the", "tokens": [50364, 4103, 307, 1340, 457, 294, 1090, 12819, 11, 293, 689, 264, 50628], "temperature": 0.0, "avg_logprob": -0.11516785410653173, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0030746173579245806}, {"id": 123, "seek": 44596, "start": 451.23999999999995, "end": 454.56, "text": " distribution has many modes spread far apart, it's actually", "tokens": [50628, 7316, 575, 867, 14068, 3974, 1400, 4936, 11, 309, 311, 767, 50794], "temperature": 0.0, "avg_logprob": -0.11516785410653173, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0030746173579245806}, {"id": 124, "seek": 44596, "start": 454.68, "end": 459.2, "text": " exponentially expensive. There's a bunch of human orientated", "tokens": [50800, 37330, 5124, 13, 821, 311, 257, 3840, 295, 1952, 8579, 770, 51026], "temperature": 0.0, "avg_logprob": -0.11516785410653173, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0030746173579245806}, {"id": 125, "seek": 44596, "start": 459.2, "end": 462.24, "text": " hacks to try and make this work well in specific cases. But we're", "tokens": [51026, 33617, 281, 853, 293, 652, 341, 589, 731, 294, 2685, 3331, 13, 583, 321, 434, 51178], "temperature": 0.0, "avg_logprob": -0.11516785410653173, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0030746173579245806}, {"id": 126, "seek": 44596, "start": 462.24, "end": 466.03999999999996, "text": " missing a much more general machine learnable solution. This is", "tokens": [51178, 5361, 257, 709, 544, 2674, 3479, 1466, 712, 3827, 13, 639, 307, 51368], "temperature": 0.0, "avg_logprob": -0.11516785410653173, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0030746173579245806}, {"id": 127, "seek": 44596, "start": 466.03999999999996, "end": 468.71999999999997, "text": " the main reason why we haven't seen it used in many machine", "tokens": [51368, 264, 2135, 1778, 983, 321, 2378, 380, 1612, 309, 1143, 294, 867, 3479, 51502], "temperature": 0.0, "avg_logprob": -0.11516785410653173, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0030746173579245806}, {"id": 128, "seek": 44596, "start": 468.71999999999997, "end": 471.47999999999996, "text": " learning applications yet. Assuming that the function we want to", "tokens": [51502, 2539, 5821, 1939, 13, 6281, 24919, 300, 264, 2445, 321, 528, 281, 51640], "temperature": 0.0, "avg_logprob": -0.11516785410653173, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0030746173579245806}, {"id": 129, "seek": 44596, "start": 471.47999999999996, "end": 475.47999999999996, "text": " learn has underlying structure, then we can escape the", "tokens": [51640, 1466, 575, 14217, 3877, 11, 550, 321, 393, 7615, 264, 51840], "temperature": 0.0, "avg_logprob": -0.11516785410653173, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.0030746173579245806}, {"id": 130, "seek": 47548, "start": 475.48, "end": 479.12, "text": " exponential time of Markov chain Monte Carlo with machine", "tokens": [50364, 21510, 565, 295, 3934, 5179, 5021, 38105, 45112, 365, 3479, 50546], "temperature": 0.0, "avg_logprob": -0.12004877781045847, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.0005441305693238974}, {"id": 131, "seek": 47548, "start": 479.12, "end": 482.68, "text": " learning. And this is what Benzio calls systematic", "tokens": [50546, 2539, 13, 400, 341, 307, 437, 3964, 89, 1004, 5498, 27249, 50724], "temperature": 0.0, "avg_logprob": -0.12004877781045847, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.0005441305693238974}, {"id": 132, "seek": 47548, "start": 482.72, "end": 487.28000000000003, "text": " generalization, which is to say, how do we generalize far from", "tokens": [50726, 2674, 2144, 11, 597, 307, 281, 584, 11, 577, 360, 321, 2674, 1125, 1400, 490, 50954], "temperature": 0.0, "avg_logprob": -0.12004877781045847, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.0005441305693238974}, {"id": 133, "seek": 47548, "start": 487.28000000000003, "end": 491.68, "text": " the data in a way which is meaningful. Now G flow nets are", "tokens": [50954, 264, 1412, 294, 257, 636, 597, 307, 10995, 13, 823, 460, 3095, 36170, 366, 51174], "temperature": 0.0, "avg_logprob": -0.12004877781045847, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.0005441305693238974}, {"id": 134, "seek": 47548, "start": 491.68, "end": 494.6, "text": " an active learning framework, where the name of the game is to", "tokens": [51174, 364, 4967, 2539, 8388, 11, 689, 264, 1315, 295, 264, 1216, 307, 281, 51320], "temperature": 0.0, "avg_logprob": -0.12004877781045847, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.0005441305693238974}, {"id": 135, "seek": 47548, "start": 494.6, "end": 498.08000000000004, "text": " generate salient and diverse training data to augment our", "tokens": [51320, 8460, 1845, 1196, 293, 9521, 3097, 1412, 281, 29919, 527, 51494], "temperature": 0.0, "avg_logprob": -0.12004877781045847, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.0005441305693238974}, {"id": 136, "seek": 47548, "start": 498.08000000000004, "end": 501.20000000000005, "text": " model in the most sample efficient way possible. For G flow", "tokens": [51494, 2316, 294, 264, 881, 6889, 7148, 636, 1944, 13, 1171, 460, 3095, 51650], "temperature": 0.0, "avg_logprob": -0.12004877781045847, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.0005441305693238974}, {"id": 137, "seek": 47548, "start": 501.20000000000005, "end": 504.96000000000004, "text": " nets to work, we need a reward function and a deterministic", "tokens": [51650, 36170, 281, 589, 11, 321, 643, 257, 7782, 2445, 293, 257, 15957, 3142, 51838], "temperature": 0.0, "avg_logprob": -0.12004877781045847, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.0005441305693238974}, {"id": 138, "seek": 50496, "start": 504.96, "end": 508.68, "text": " episodic environment. Does that sound familiar? Yes, just like", "tokens": [50364, 39200, 299, 2823, 13, 4402, 300, 1626, 4963, 30, 1079, 11, 445, 411, 50550], "temperature": 0.0, "avg_logprob": -0.1027649652390253, "compression_ratio": 1.6475095785440612, "no_speech_prob": 0.0011692282278090715}, {"id": 139, "seek": 50496, "start": 508.68, "end": 512.52, "text": " reinforcement learning. Now a flow network is a directed graph", "tokens": [50550, 29280, 2539, 13, 823, 257, 3095, 3209, 307, 257, 12898, 4295, 50742], "temperature": 0.0, "avg_logprob": -0.1027649652390253, "compression_ratio": 1.6475095785440612, "no_speech_prob": 0.0011692282278090715}, {"id": 140, "seek": 50496, "start": 512.72, "end": 518.0799999999999, "text": " with sources and sinks and edges carrying some amount of flow", "tokens": [50752, 365, 7139, 293, 43162, 293, 8819, 9792, 512, 2372, 295, 3095, 51020], "temperature": 0.0, "avg_logprob": -0.1027649652390253, "compression_ratio": 1.6475095785440612, "no_speech_prob": 0.0011692282278090715}, {"id": 141, "seek": 50496, "start": 518.0799999999999, "end": 520.88, "text": " between them, you know, through intermediate nodes. So I think a", "tokens": [51020, 1296, 552, 11, 291, 458, 11, 807, 19376, 13891, 13, 407, 286, 519, 257, 51160], "temperature": 0.0, "avg_logprob": -0.1027649652390253, "compression_ratio": 1.6475095785440612, "no_speech_prob": 0.0011692282278090715}, {"id": 142, "seek": 50496, "start": 520.88, "end": 524.0, "text": " good way to think about this is pipes of water. Now for our", "tokens": [51160, 665, 636, 281, 519, 466, 341, 307, 21882, 295, 1281, 13, 823, 337, 527, 51316], "temperature": 0.0, "avg_logprob": -0.1027649652390253, "compression_ratio": 1.6475095785440612, "no_speech_prob": 0.0011692282278090715}, {"id": 143, "seek": 50496, "start": 524.0, "end": 528.68, "text": " purposes, we define a flow network with a single source. So the", "tokens": [51316, 9932, 11, 321, 6964, 257, 3095, 3209, 365, 257, 2167, 4009, 13, 407, 264, 51550], "temperature": 0.0, "avg_logprob": -0.1027649652390253, "compression_ratio": 1.6475095785440612, "no_speech_prob": 0.0011692282278090715}, {"id": 144, "seek": 50496, "start": 528.68, "end": 531.28, "text": " root nodes, or you might say the sinks of the network", "tokens": [51550, 5593, 13891, 11, 420, 291, 1062, 584, 264, 43162, 295, 264, 3209, 51680], "temperature": 0.0, "avg_logprob": -0.1027649652390253, "compression_ratio": 1.6475095785440612, "no_speech_prob": 0.0011692282278090715}, {"id": 145, "seek": 53128, "start": 531.3199999999999, "end": 535.0, "text": " correspond to the terminal states. Now it's designed to find", "tokens": [50366, 6805, 281, 264, 14709, 4368, 13, 823, 309, 311, 4761, 281, 915, 50550], "temperature": 0.0, "avg_logprob": -0.10978657115589488, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0029342113994061947}, {"id": 146, "seek": 53128, "start": 535.0, "end": 539.36, "text": " the possible trajectories through our system. Okay, and just", "tokens": [50550, 264, 1944, 18257, 2083, 807, 527, 1185, 13, 1033, 11, 293, 445, 50768], "temperature": 0.0, "avg_logprob": -0.10978657115589488, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0029342113994061947}, {"id": 147, "seek": 53128, "start": 539.36, "end": 541.56, "text": " think of Alpha zero as being like a good analogy for these", "tokens": [50768, 519, 295, 20588, 4018, 382, 885, 411, 257, 665, 21663, 337, 613, 50878], "temperature": 0.0, "avg_logprob": -0.10978657115589488, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0029342113994061947}, {"id": 148, "seek": 53128, "start": 541.56, "end": 544.92, "text": " trajectories. Now the training objective is to make them", "tokens": [50878, 18257, 2083, 13, 823, 264, 3097, 10024, 307, 281, 652, 552, 51046], "temperature": 0.0, "avg_logprob": -0.10978657115589488, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0029342113994061947}, {"id": 149, "seek": 53128, "start": 544.9599999999999, "end": 549.12, "text": " approximately sample in proportion to the given reward", "tokens": [51048, 10447, 6889, 294, 16068, 281, 264, 2212, 7782, 51256], "temperature": 0.0, "avg_logprob": -0.10978657115589488, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0029342113994061947}, {"id": 150, "seek": 53128, "start": 549.12, "end": 552.68, "text": " function. This is in stark contrast to Alpha zero where we", "tokens": [51256, 2445, 13, 639, 307, 294, 17417, 8712, 281, 20588, 4018, 689, 321, 51434], "temperature": 0.0, "avg_logprob": -0.10978657115589488, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0029342113994061947}, {"id": 151, "seek": 53128, "start": 552.68, "end": 556.48, "text": " were sampling to maximize the expected reward. So Benzio's big", "tokens": [51434, 645, 21179, 281, 19874, 264, 5176, 7782, 13, 407, 3964, 89, 1004, 311, 955, 51624], "temperature": 0.0, "avg_logprob": -0.10978657115589488, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0029342113994061947}, {"id": 152, "seek": 53128, "start": 556.48, "end": 559.36, "text": " idea is that we can have an interacting loop between a", "tokens": [51624, 1558, 307, 300, 321, 393, 362, 364, 18017, 6367, 1296, 257, 51768], "temperature": 0.0, "avg_logprob": -0.10978657115589488, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0029342113994061947}, {"id": 153, "seek": 55936, "start": 559.36, "end": 562.4, "text": " generative model and the real world. The real world is", "tokens": [50364, 1337, 1166, 2316, 293, 264, 957, 1002, 13, 440, 957, 1002, 307, 50516], "temperature": 0.0, "avg_logprob": -0.12046073033259465, "compression_ratio": 1.7276422764227641, "no_speech_prob": 0.0015485535841435194}, {"id": 154, "seek": 55936, "start": 562.4, "end": 566.16, "text": " expensive. So why not train an imagination machine in our mind", "tokens": [50516, 5124, 13, 407, 983, 406, 3847, 364, 12938, 3479, 294, 527, 1575, 50704], "temperature": 0.0, "avg_logprob": -0.12046073033259465, "compression_ratio": 1.7276422764227641, "no_speech_prob": 0.0015485535841435194}, {"id": 155, "seek": 55936, "start": 566.28, "end": 569.5600000000001, "text": " until we're ready and waiting to produce good questions to the", "tokens": [50710, 1826, 321, 434, 1919, 293, 3806, 281, 5258, 665, 1651, 281, 264, 50874], "temperature": 0.0, "avg_logprob": -0.12046073033259465, "compression_ratio": 1.7276422764227641, "no_speech_prob": 0.0015485535841435194}, {"id": 156, "seek": 55936, "start": 569.5600000000001, "end": 572.92, "text": " real world, we could use imagined experiments to train our", "tokens": [50874, 957, 1002, 11, 321, 727, 764, 16590, 12050, 281, 3847, 527, 51042], "temperature": 0.0, "avg_logprob": -0.12046073033259465, "compression_ratio": 1.7276422764227641, "no_speech_prob": 0.0015485535841435194}, {"id": 157, "seek": 55936, "start": 572.92, "end": 578.6800000000001, "text": " generator, then produce queries to the real world. We were", "tokens": [51042, 19265, 11, 550, 5258, 24109, 281, 264, 957, 1002, 13, 492, 645, 51330], "temperature": 0.0, "avg_logprob": -0.12046073033259465, "compression_ratio": 1.7276422764227641, "no_speech_prob": 0.0015485535841435194}, {"id": 158, "seek": 55936, "start": 578.6800000000001, "end": 583.48, "text": " thinking about a way to visualize how G flow nets work when the", "tokens": [51330, 1953, 466, 257, 636, 281, 23273, 577, 460, 3095, 36170, 589, 562, 264, 51570], "temperature": 0.0, "avg_logprob": -0.12046073033259465, "compression_ratio": 1.7276422764227641, "no_speech_prob": 0.0015485535841435194}, {"id": 159, "seek": 55936, "start": 583.48, "end": 588.64, "text": " idea of a Galton board came to mind. A Galton board also known", "tokens": [51570, 1558, 295, 257, 7336, 1756, 3150, 1361, 281, 1575, 13, 316, 7336, 1756, 3150, 611, 2570, 51828], "temperature": 0.0, "avg_logprob": -0.12046073033259465, "compression_ratio": 1.7276422764227641, "no_speech_prob": 0.0015485535841435194}, {"id": 160, "seek": 58864, "start": 588.64, "end": 593.1999999999999, "text": " as a beam machine is a common prop in statistics courses,", "tokens": [50364, 382, 257, 14269, 3479, 307, 257, 2689, 2365, 294, 12523, 7712, 11, 50592], "temperature": 0.0, "avg_logprob": -0.11217241485913594, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0012065022019669414}, {"id": 161, "seek": 58864, "start": 593.56, "end": 599.52, "text": " science museums, and fun gadget stores. The board has rows of", "tokens": [50610, 3497, 23248, 11, 293, 1019, 38090, 9512, 13, 440, 3150, 575, 13241, 295, 50908], "temperature": 0.0, "avg_logprob": -0.11217241485913594, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0012065022019669414}, {"id": 162, "seek": 58864, "start": 599.52, "end": 605.24, "text": " interleaved pegs above a bottom row of buckets. Beads are filled", "tokens": [50908, 728, 306, 12865, 17199, 82, 3673, 257, 2767, 5386, 295, 32191, 13, 879, 5834, 366, 6412, 51194], "temperature": 0.0, "avg_logprob": -0.11217241485913594, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0012065022019669414}, {"id": 163, "seek": 58864, "start": 605.24, "end": 609.0, "text": " into a funnel at the top of the board and then sprinkled on the", "tokens": [51194, 666, 257, 24515, 412, 264, 1192, 295, 264, 3150, 293, 550, 30885, 1493, 322, 264, 51382], "temperature": 0.0, "avg_logprob": -0.11217241485913594, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0012065022019669414}, {"id": 164, "seek": 58864, "start": 609.0, "end": 613.36, "text": " top center peg. The beads bounce either to the left or to the", "tokens": [51382, 1192, 3056, 17199, 13, 440, 20369, 15894, 2139, 281, 264, 1411, 420, 281, 264, 51600], "temperature": 0.0, "avg_logprob": -0.11217241485913594, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0012065022019669414}, {"id": 165, "seek": 58864, "start": 613.36, "end": 617.52, "text": " right as they hit the pegs and eventually collect into buckets", "tokens": [51600, 558, 382, 436, 2045, 264, 17199, 82, 293, 4728, 2500, 666, 32191, 51808], "temperature": 0.0, "avg_logprob": -0.11217241485913594, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0012065022019669414}, {"id": 166, "seek": 61752, "start": 617.56, "end": 622.28, "text": " at the bottom. If the pegs are precisely and symmetrically", "tokens": [50366, 412, 264, 2767, 13, 759, 264, 17199, 82, 366, 13402, 293, 14232, 27965, 984, 50602], "temperature": 0.0, "avg_logprob": -0.10486344085342583, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0005527559551410377}, {"id": 167, "seek": 61752, "start": 622.28, "end": 625.64, "text": " arranged, the beads will aggregate at the bottom into a", "tokens": [50602, 18721, 11, 264, 20369, 486, 26118, 412, 264, 2767, 666, 257, 50770], "temperature": 0.0, "avg_logprob": -0.10486344085342583, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0005527559551410377}, {"id": 168, "seek": 61752, "start": 625.64, "end": 631.3199999999999, "text": " familiar binomial bell curve. Now imagine that the pegs were", "tokens": [50770, 4963, 5171, 47429, 4549, 7605, 13, 823, 3811, 300, 264, 17199, 82, 645, 51054], "temperature": 0.0, "avg_logprob": -0.10486344085342583, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0005527559551410377}, {"id": 169, "seek": 61752, "start": 631.3199999999999, "end": 635.76, "text": " instead flow gates with adjustable valves that could", "tokens": [51054, 2602, 3095, 19792, 365, 27804, 34950, 300, 727, 51276], "temperature": 0.0, "avg_logprob": -0.10486344085342583, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0005527559551410377}, {"id": 170, "seek": 61752, "start": 635.76, "end": 640.4, "text": " direct the beads more to the left or more to the right to", "tokens": [51276, 2047, 264, 20369, 544, 281, 264, 1411, 420, 544, 281, 264, 558, 281, 51508], "temperature": 0.0, "avg_logprob": -0.10486344085342583, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0005527559551410377}, {"id": 171, "seek": 61752, "start": 640.4, "end": 644.68, "text": " bias the flow paths. With such a machine, you could adjust the", "tokens": [51508, 12577, 264, 3095, 14518, 13, 2022, 1270, 257, 3479, 11, 291, 727, 4369, 264, 51722], "temperature": 0.0, "avg_logprob": -0.10486344085342583, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0005527559551410377}, {"id": 172, "seek": 64468, "start": 644.68, "end": 651.12, "text": " valves or flow rates to create any distribution. For example, to", "tokens": [50364, 34950, 420, 3095, 6846, 281, 1884, 604, 7316, 13, 1171, 1365, 11, 281, 50686], "temperature": 0.0, "avg_logprob": -0.10520230806790866, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0008040443644858897}, {"id": 173, "seek": 64468, "start": 651.12, "end": 654.68, "text": " create a uniform distribution, we'd open up the gates flowing", "tokens": [50686, 1884, 257, 9452, 7316, 11, 321, 1116, 1269, 493, 264, 19792, 13974, 50864], "temperature": 0.0, "avg_logprob": -0.10520230806790866, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0008040443644858897}, {"id": 174, "seek": 64468, "start": 654.7199999999999, "end": 659.9599999999999, "text": " away from the center line of the board to drive more bead flow", "tokens": [50866, 1314, 490, 264, 3056, 1622, 295, 264, 3150, 281, 3332, 544, 24117, 3095, 51128], "temperature": 0.0, "avg_logprob": -0.10520230806790866, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0008040443644858897}, {"id": 175, "seek": 64468, "start": 660.0799999999999, "end": 664.0799999999999, "text": " to the fewer number of paths leading to the edges and the", "tokens": [51134, 281, 264, 13366, 1230, 295, 14518, 5775, 281, 264, 8819, 293, 264, 51334], "temperature": 0.0, "avg_logprob": -0.10520230806790866, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0008040443644858897}, {"id": 176, "seek": 64468, "start": 664.0799999999999, "end": 670.0, "text": " corners. Or to create a multimodal distribution, we'd arrange", "tokens": [51334, 12413, 13, 1610, 281, 1884, 257, 32972, 378, 304, 7316, 11, 321, 1116, 9424, 51630], "temperature": 0.0, "avg_logprob": -0.10520230806790866, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0008040443644858897}, {"id": 177, "seek": 64468, "start": 670.0, "end": 673.92, "text": " the gates to split the flows into two or more streams that would", "tokens": [51630, 264, 19792, 281, 7472, 264, 12867, 666, 732, 420, 544, 15842, 300, 576, 51826], "temperature": 0.0, "avg_logprob": -0.10520230806790866, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0008040443644858897}, {"id": 178, "seek": 67392, "start": 673.92, "end": 679.92, "text": " then pile up in multiple humps or modes below. There's a lot of", "tokens": [50364, 550, 14375, 493, 294, 3866, 276, 16951, 420, 14068, 2507, 13, 821, 311, 257, 688, 295, 50664], "temperature": 0.0, "avg_logprob": -0.1293975889068289, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.00014425741392187774}, {"id": 179, "seek": 67392, "start": 679.92, "end": 683.8399999999999, "text": " flexibility here. Indeed, given a distribution, there are", "tokens": [50664, 12635, 510, 13, 15061, 11, 2212, 257, 7316, 11, 456, 366, 50860], "temperature": 0.0, "avg_logprob": -0.1293975889068289, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.00014425741392187774}, {"id": 180, "seek": 67392, "start": 683.8399999999999, "end": 688.7199999999999, "text": " generally multiple flow gate solutions to produce it. It'd be", "tokens": [50860, 5101, 3866, 3095, 8539, 6547, 281, 5258, 309, 13, 467, 1116, 312, 51104], "temperature": 0.0, "avg_logprob": -0.1293975889068289, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.00014425741392187774}, {"id": 181, "seek": 67392, "start": 688.7199999999999, "end": 692.92, "text": " nice, wouldn't it? If we had an intelligent, principled way to", "tokens": [51104, 1481, 11, 2759, 380, 309, 30, 759, 321, 632, 364, 13232, 11, 3681, 15551, 636, 281, 51314], "temperature": 0.0, "avg_logprob": -0.1293975889068289, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.00014425741392187774}, {"id": 182, "seek": 67392, "start": 692.92, "end": 698.3199999999999, "text": " train these gates. Enter G flow nets. G flow nets put a neural", "tokens": [51314, 3847, 613, 19792, 13, 10399, 460, 3095, 36170, 13, 460, 3095, 36170, 829, 257, 18161, 51584], "temperature": 0.0, "avg_logprob": -0.1293975889068289, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.00014425741392187774}, {"id": 183, "seek": 67392, "start": 698.3199999999999, "end": 702.92, "text": " network, a brain behind the flow adjustments. A brain which can", "tokens": [51584, 3209, 11, 257, 3567, 2261, 264, 3095, 18624, 13, 316, 3567, 597, 393, 51814], "temperature": 0.0, "avg_logprob": -0.1293975889068289, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.00014425741392187774}, {"id": 184, "seek": 70292, "start": 702.92, "end": 707.88, "text": " optimize the gates to match any distribution we desire. Here,", "tokens": [50364, 19719, 264, 19792, 281, 2995, 604, 7316, 321, 7516, 13, 1692, 11, 50612], "temperature": 0.0, "avg_logprob": -0.12323158840800441, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.000158429509610869}, {"id": 185, "seek": 70292, "start": 707.88, "end": 711.68, "text": " we're interested in sampling a reward function in the context", "tokens": [50612, 321, 434, 3102, 294, 21179, 257, 7782, 2445, 294, 264, 4319, 50802], "temperature": 0.0, "avg_logprob": -0.12323158840800441, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.000158429509610869}, {"id": 186, "seek": 70292, "start": 711.68, "end": 716.4799999999999, "text": " of reinforcement learning. In that context, this is a powerful", "tokens": [50802, 295, 29280, 2539, 13, 682, 300, 4319, 11, 341, 307, 257, 4005, 51042], "temperature": 0.0, "avg_logprob": -0.12323158840800441, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.000158429509610869}, {"id": 187, "seek": 70292, "start": 716.4799999999999, "end": 721.3199999999999, "text": " simulation and sampling paradigm. You see, once the brain has", "tokens": [51042, 16575, 293, 21179, 24709, 13, 509, 536, 11, 1564, 264, 3567, 575, 51284], "temperature": 0.0, "avg_logprob": -0.12323158840800441, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.000158429509610869}, {"id": 188, "seek": 70292, "start": 721.3199999999999, "end": 725.24, "text": " tuned the flow weights, such a modified Galton board, or more", "tokens": [51284, 10870, 264, 3095, 17443, 11, 1270, 257, 15873, 7336, 1756, 3150, 11, 420, 544, 51480], "temperature": 0.0, "avg_logprob": -0.12323158840800441, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.000158429509610869}, {"id": 189, "seek": 70292, "start": 725.24, "end": 730.4, "text": " generally, a flow network, can sample diverse paths quickly", "tokens": [51480, 5101, 11, 257, 3095, 3209, 11, 393, 6889, 9521, 14518, 2661, 51738], "temperature": 0.0, "avg_logprob": -0.12323158840800441, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.000158429509610869}, {"id": 190, "seek": 73040, "start": 730.52, "end": 735.1999999999999, "text": " and efficiently, leading to the reward distribution. It's", "tokens": [50370, 293, 19621, 11, 5775, 281, 264, 7782, 7316, 13, 467, 311, 50604], "temperature": 0.0, "avg_logprob": -0.10516550907721886, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.0008040031534619629}, {"id": 191, "seek": 73040, "start": 735.1999999999999, "end": 738.9599999999999, "text": " important to point out that the path sampling is more diverse", "tokens": [50604, 1021, 281, 935, 484, 300, 264, 3100, 21179, 307, 544, 9521, 50792], "temperature": 0.0, "avg_logprob": -0.10516550907721886, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.0008040031534619629}, {"id": 192, "seek": 73040, "start": 739.12, "end": 743.64, "text": " doing it this way. Unlike classic reinforcement learning, a G", "tokens": [50800, 884, 309, 341, 636, 13, 17657, 7230, 29280, 2539, 11, 257, 460, 51026], "temperature": 0.0, "avg_logprob": -0.10516550907721886, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.0008040031534619629}, {"id": 193, "seek": 73040, "start": 743.64, "end": 747.0799999999999, "text": " flow net doesn't just fixate on a small number of high reward", "tokens": [51026, 3095, 2533, 1177, 380, 445, 3191, 473, 322, 257, 1359, 1230, 295, 1090, 7782, 51198], "temperature": 0.0, "avg_logprob": -0.10516550907721886, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.0008040031534619629}, {"id": 194, "seek": 73040, "start": 747.0799999999999, "end": 751.6, "text": " paths, it happens to find first. Instead, it stochastically", "tokens": [51198, 14518, 11, 309, 2314, 281, 915, 700, 13, 7156, 11, 309, 342, 8997, 22808, 51424], "temperature": 0.0, "avg_logprob": -0.10516550907721886, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.0008040031534619629}, {"id": 195, "seek": 73040, "start": 751.6, "end": 755.4399999999999, "text": " samples a broad spectrum of paths in proportion to their reward.", "tokens": [51424, 10938, 257, 4152, 11143, 295, 14518, 294, 16068, 281, 641, 7782, 13, 51616], "temperature": 0.0, "avg_logprob": -0.10516550907721886, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.0008040031534619629}, {"id": 196, "seek": 73040, "start": 755.88, "end": 759.4399999999999, "text": " Sure, high reward paths will be sampled with higher weight. But", "tokens": [51638, 4894, 11, 1090, 7782, 14518, 486, 312, 3247, 15551, 365, 2946, 3364, 13, 583, 51816], "temperature": 0.0, "avg_logprob": -0.10516550907721886, "compression_ratio": 1.6425855513307985, "no_speech_prob": 0.0008040031534619629}, {"id": 197, "seek": 75944, "start": 759.44, "end": 763.6400000000001, "text": " the far larger population of low reward paths will get a share", "tokens": [50364, 264, 1400, 4833, 4415, 295, 2295, 7782, 14518, 486, 483, 257, 2073, 50574], "temperature": 0.0, "avg_logprob": -0.09164307912190756, "compression_ratio": 1.6338582677165354, "no_speech_prob": 2.4299735741806217e-05}, {"id": 198, "seek": 75944, "start": 763.6800000000001, "end": 767.24, "text": " of the sampling as well. Why should we even bother with such", "tokens": [50576, 295, 264, 21179, 382, 731, 13, 1545, 820, 321, 754, 8677, 365, 1270, 50754], "temperature": 0.0, "avg_logprob": -0.09164307912190756, "compression_ratio": 1.6338582677165354, "no_speech_prob": 2.4299735741806217e-05}, {"id": 199, "seek": 75944, "start": 767.24, "end": 772.1600000000001, "text": " paths? The answer is we need to balance exploitation or high", "tokens": [50754, 14518, 30, 440, 1867, 307, 321, 643, 281, 4772, 33122, 420, 1090, 51000], "temperature": 0.0, "avg_logprob": -0.09164307912190756, "compression_ratio": 1.6338582677165354, "no_speech_prob": 2.4299735741806217e-05}, {"id": 200, "seek": 75944, "start": 772.1600000000001, "end": 776.1600000000001, "text": " reward with exploration or learning to better learn the", "tokens": [51000, 7782, 365, 16197, 420, 2539, 281, 1101, 1466, 264, 51200], "temperature": 0.0, "avg_logprob": -0.09164307912190756, "compression_ratio": 1.6338582677165354, "no_speech_prob": 2.4299735741806217e-05}, {"id": 201, "seek": 75944, "start": 776.1600000000001, "end": 779.7600000000001, "text": " reward function. This is especially important when dealing", "tokens": [51200, 7782, 2445, 13, 639, 307, 2318, 1021, 562, 6260, 51380], "temperature": 0.0, "avg_logprob": -0.09164307912190756, "compression_ratio": 1.6338582677165354, "no_speech_prob": 2.4299735741806217e-05}, {"id": 202, "seek": 75944, "start": 779.7600000000001, "end": 784.96, "text": " with complex real world scenarios of high uncertainty. For", "tokens": [51380, 365, 3997, 957, 1002, 15077, 295, 1090, 15697, 13, 1171, 51640], "temperature": 0.0, "avg_logprob": -0.09164307912190756, "compression_ratio": 1.6338582677165354, "no_speech_prob": 2.4299735741806217e-05}, {"id": 203, "seek": 75944, "start": 784.96, "end": 789.2, "text": " example, think of molecular drug discovery and design or", "tokens": [51640, 1365, 11, 519, 295, 19046, 4110, 12114, 293, 1715, 420, 51852], "temperature": 0.0, "avg_logprob": -0.09164307912190756, "compression_ratio": 1.6338582677165354, "no_speech_prob": 2.4299735741806217e-05}, {"id": 204, "seek": 78944, "start": 789.44, "end": 794.12, "text": " navigating jungle terrain. In both those scenarios, we really", "tokens": [50364, 32054, 18228, 17674, 13, 682, 1293, 729, 15077, 11, 321, 534, 50598], "temperature": 0.0, "avg_logprob": -0.10901683500443382, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.00022339938732329756}, {"id": 205, "seek": 78944, "start": 794.12, "end": 799.08, "text": " know very little about how a particular path may play out. We", "tokens": [50598, 458, 588, 707, 466, 577, 257, 1729, 3100, 815, 862, 484, 13, 492, 50846], "temperature": 0.0, "avg_logprob": -0.10901683500443382, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.00022339938732329756}, {"id": 206, "seek": 78944, "start": 799.08, "end": 803.0, "text": " might stumble into the next miracle cure or a pitfall of", "tokens": [50846, 1062, 41302, 666, 264, 958, 14660, 13698, 420, 257, 10147, 6691, 295, 51042], "temperature": 0.0, "avg_logprob": -0.10901683500443382, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.00022339938732329756}, {"id": 207, "seek": 78944, "start": 803.0, "end": 807.5200000000001, "text": " quicksand. To find the globally best paths, it's important to", "tokens": [51042, 1702, 82, 474, 13, 1407, 915, 264, 18958, 1151, 14518, 11, 309, 311, 1021, 281, 51268], "temperature": 0.0, "avg_logprob": -0.10901683500443382, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.00022339938732329756}, {"id": 208, "seek": 78944, "start": 807.5200000000001, "end": 812.8000000000001, "text": " keep our options open. Beyond this sampling diversity, G", "tokens": [51268, 1066, 527, 3956, 1269, 13, 19707, 341, 21179, 8811, 11, 460, 51532], "temperature": 0.0, "avg_logprob": -0.10901683500443382, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.00022339938732329756}, {"id": 209, "seek": 78944, "start": 812.8000000000001, "end": 816.4000000000001, "text": " flow nets also bring the full power of neural networks to", "tokens": [51532, 3095, 36170, 611, 1565, 264, 1577, 1347, 295, 18161, 9590, 281, 51712], "temperature": 0.0, "avg_logprob": -0.10901683500443382, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.00022339938732329756}, {"id": 210, "seek": 81640, "start": 816.4399999999999, "end": 821.64, "text": " discover latent structure and learn the reward function. This", "tokens": [50366, 4411, 48994, 3877, 293, 1466, 264, 7782, 2445, 13, 639, 50626], "temperature": 0.0, "avg_logprob": -0.13852038206877532, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.0021155700087547302}, {"id": 211, "seek": 81640, "start": 821.68, "end": 826.16, "text": " combined with their diverse sampling also makes G flow nets", "tokens": [50628, 9354, 365, 641, 9521, 21179, 611, 1669, 460, 3095, 36170, 50852], "temperature": 0.0, "avg_logprob": -0.13852038206877532, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.0021155700087547302}, {"id": 212, "seek": 81640, "start": 826.16, "end": 830.16, "text": " more robust when dealing with multimodal distributions, which", "tokens": [50852, 544, 13956, 562, 6260, 365, 32972, 378, 304, 37870, 11, 597, 51052], "temperature": 0.0, "avg_logprob": -0.13852038206877532, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.0021155700087547302}, {"id": 213, "seek": 81640, "start": 830.16, "end": 834.36, "text": " are a common trap for greedy algorithms and Markov chain,", "tokens": [51052, 366, 257, 2689, 11487, 337, 28228, 14642, 293, 3934, 5179, 5021, 11, 51262], "temperature": 0.0, "avg_logprob": -0.13852038206877532, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.0021155700087547302}, {"id": 214, "seek": 81640, "start": 834.4, "end": 838.76, "text": " Monte Carlo. If there is structure linking the multiple", "tokens": [51264, 38105, 45112, 13, 759, 456, 307, 3877, 25775, 264, 3866, 51482], "temperature": 0.0, "avg_logprob": -0.13852038206877532, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.0021155700087547302}, {"id": 215, "seek": 81640, "start": 838.76, "end": 843.92, "text": " nodes, G flow nets can learn it and extrapolate to new modes", "tokens": [51482, 13891, 11, 460, 3095, 36170, 393, 1466, 309, 293, 48224, 473, 281, 777, 14068, 51740], "temperature": 0.0, "avg_logprob": -0.13852038206877532, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.0021155700087547302}, {"id": 216, "seek": 84392, "start": 844.0, "end": 848.8399999999999, "text": " and once discovered, they will by design drive the sampling to", "tokens": [50368, 293, 1564, 6941, 11, 436, 486, 538, 1715, 3332, 264, 21179, 281, 50610], "temperature": 0.0, "avg_logprob": -0.1598868261684071, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0008830009028315544}, {"id": 217, "seek": 84392, "start": 848.8399999999999, "end": 854.24, "text": " cover those modes and learn more structure. Overall, G flow", "tokens": [50610, 2060, 729, 14068, 293, 1466, 544, 3877, 13, 18420, 11, 460, 3095, 50880], "temperature": 0.0, "avg_logprob": -0.1598868261684071, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0008830009028315544}, {"id": 218, "seek": 84392, "start": 854.24, "end": 859.52, "text": " nets seem to offer an intriguing new path pun intended for an", "tokens": [50880, 36170, 1643, 281, 2626, 364, 32503, 777, 3100, 4468, 10226, 337, 364, 51144], "temperature": 0.0, "avg_logprob": -0.1598868261684071, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0008830009028315544}, {"id": 219, "seek": 84392, "start": 859.52, "end": 864.68, "text": " intelligent sampling paradigm. So you might ask how are G", "tokens": [51144, 13232, 21179, 24709, 13, 407, 291, 1062, 1029, 577, 366, 460, 51402], "temperature": 0.0, "avg_logprob": -0.1598868261684071, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0008830009028315544}, {"id": 220, "seek": 84392, "start": 864.68, "end": 867.92, "text": " flow nets different from Alpha zero? Well, the policy network", "tokens": [51402, 3095, 36170, 819, 490, 20588, 4018, 30, 1042, 11, 264, 3897, 3209, 51564], "temperature": 0.0, "avg_logprob": -0.1598868261684071, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0008830009028315544}, {"id": 221, "seek": 84392, "start": 867.92, "end": 871.4399999999999, "text": " and Alpha zero gives you a set of actions. Given a state, Alpha", "tokens": [51564, 293, 20588, 4018, 2709, 291, 257, 992, 295, 5909, 13, 18600, 257, 1785, 11, 20588, 51740], "temperature": 0.0, "avg_logprob": -0.1598868261684071, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.0008830009028315544}, {"id": 222, "seek": 87144, "start": 871.48, "end": 875.6, "text": " zero trains the policy network to maximize reward so that the", "tokens": [50366, 4018, 16329, 264, 3897, 3209, 281, 19874, 7782, 370, 300, 264, 50572], "temperature": 0.0, "avg_logprob": -0.08094082989738983, "compression_ratio": 1.728744939271255, "no_speech_prob": 0.0007915574824437499}, {"id": 223, "seek": 87144, "start": 875.6, "end": 879.6400000000001, "text": " trajectories all end up at the highest reward. Now what G", "tokens": [50572, 18257, 2083, 439, 917, 493, 412, 264, 6343, 7782, 13, 823, 437, 460, 50774], "temperature": 0.0, "avg_logprob": -0.08094082989738983, "compression_ratio": 1.728744939271255, "no_speech_prob": 0.0007915574824437499}, {"id": 224, "seek": 87144, "start": 879.6400000000001, "end": 883.5600000000001, "text": " flow nets do is they train so that the actions are distributed", "tokens": [50774, 3095, 36170, 360, 307, 436, 3847, 370, 300, 264, 5909, 366, 12631, 50970], "temperature": 0.0, "avg_logprob": -0.08094082989738983, "compression_ratio": 1.728744939271255, "no_speech_prob": 0.0007915574824437499}, {"id": 225, "seek": 87144, "start": 883.72, "end": 887.2800000000001, "text": " in proportion to the reward. So rather than pruning away all of", "tokens": [50978, 294, 16068, 281, 264, 7782, 13, 407, 2831, 813, 582, 37726, 1314, 439, 295, 51156], "temperature": 0.0, "avg_logprob": -0.08094082989738983, "compression_ratio": 1.728744939271255, "no_speech_prob": 0.0007915574824437499}, {"id": 226, "seek": 87144, "start": 887.2800000000001, "end": 890.6400000000001, "text": " the low reward trajectories, it will sample them just less", "tokens": [51156, 264, 2295, 7782, 18257, 2083, 11, 309, 486, 6889, 552, 445, 1570, 51324], "temperature": 0.0, "avg_logprob": -0.08094082989738983, "compression_ratio": 1.728744939271255, "no_speech_prob": 0.0007915574824437499}, {"id": 227, "seek": 87144, "start": 890.6400000000001, "end": 896.1600000000001, "text": " often. Now there is a manifest difference between G flow nets", "tokens": [51324, 2049, 13, 823, 456, 307, 257, 10067, 2649, 1296, 460, 3095, 36170, 51600], "temperature": 0.0, "avg_logprob": -0.08094082989738983, "compression_ratio": 1.728744939271255, "no_speech_prob": 0.0007915574824437499}, {"id": 228, "seek": 87144, "start": 896.1600000000001, "end": 898.8800000000001, "text": " in respect of exploration. I mean, you might argue that the", "tokens": [51600, 294, 3104, 295, 16197, 13, 286, 914, 11, 291, 1062, 9695, 300, 264, 51736], "temperature": 0.0, "avg_logprob": -0.08094082989738983, "compression_ratio": 1.728744939271255, "no_speech_prob": 0.0007915574824437499}, {"id": 229, "seek": 89888, "start": 898.88, "end": 902.4399999999999, "text": " Monte Carlo tree search is still doing wide exploration at the", "tokens": [50364, 38105, 45112, 4230, 3164, 307, 920, 884, 4874, 16197, 412, 264, 50542], "temperature": 0.0, "avg_logprob": -0.09929759918697297, "compression_ratio": 1.6876971608832807, "no_speech_prob": 0.0006262974929995835}, {"id": 230, "seek": 89888, "start": 902.4399999999999, "end": 905.92, "text": " beginning. But in spite of its rapid convergence and pruning of", "tokens": [50542, 2863, 13, 583, 294, 22794, 295, 1080, 7558, 32181, 293, 582, 37726, 295, 50716], "temperature": 0.0, "avg_logprob": -0.09929759918697297, "compression_ratio": 1.6876971608832807, "no_speech_prob": 0.0006262974929995835}, {"id": 231, "seek": 89888, "start": 905.92, "end": 909.28, "text": " load reward trajectories, it's still sampling from the", "tokens": [50716, 3677, 7782, 18257, 2083, 11, 309, 311, 920, 21179, 490, 264, 50884], "temperature": 0.0, "avg_logprob": -0.09929759918697297, "compression_ratio": 1.6876971608832807, "no_speech_prob": 0.0006262974929995835}, {"id": 232, "seek": 89888, "start": 909.28, "end": 911.4, "text": " underlying probability distribution, which has been", "tokens": [50884, 14217, 8482, 7316, 11, 597, 575, 668, 50990], "temperature": 0.0, "avg_logprob": -0.09929759918697297, "compression_ratio": 1.6876971608832807, "no_speech_prob": 0.0006262974929995835}, {"id": 233, "seek": 89888, "start": 911.4, "end": 914.72, "text": " scaled with a softmax. So that's actually not that much to", "tokens": [50990, 36039, 365, 257, 2787, 41167, 13, 407, 300, 311, 767, 406, 300, 709, 281, 51156], "temperature": 0.0, "avg_logprob": -0.09929759918697297, "compression_ratio": 1.6876971608832807, "no_speech_prob": 0.0006262974929995835}, {"id": 234, "seek": 89888, "start": 914.72, "end": 918.4, "text": " explore in the first place. So in summary, G flow nets are", "tokens": [51156, 6839, 294, 264, 700, 1081, 13, 407, 294, 12691, 11, 460, 3095, 36170, 366, 51340], "temperature": 0.0, "avg_logprob": -0.09929759918697297, "compression_ratio": 1.6876971608832807, "no_speech_prob": 0.0006262974929995835}, {"id": 235, "seek": 89888, "start": 918.4, "end": 921.6, "text": " better than Alpha zero Monte Carlo tree search in some sense,", "tokens": [51340, 1101, 813, 20588, 4018, 38105, 45112, 4230, 3164, 294, 512, 2020, 11, 51500], "temperature": 0.0, "avg_logprob": -0.09929759918697297, "compression_ratio": 1.6876971608832807, "no_speech_prob": 0.0006262974929995835}, {"id": 236, "seek": 89888, "start": 921.6, "end": 924.76, "text": " because they achieve the same goal by offloading the burning", "tokens": [51500, 570, 436, 4584, 264, 912, 3387, 538, 766, 2907, 278, 264, 9488, 51658], "temperature": 0.0, "avg_logprob": -0.09929759918697297, "compression_ratio": 1.6876971608832807, "no_speech_prob": 0.0006262974929995835}, {"id": 237, "seek": 89888, "start": 924.76, "end": 927.6, "text": " time and the stabilization time of Markov chain Monte Carlo.", "tokens": [51658, 565, 293, 264, 35476, 565, 295, 3934, 5179, 5021, 38105, 45112, 13, 51800], "temperature": 0.0, "avg_logprob": -0.09929759918697297, "compression_ratio": 1.6876971608832807, "no_speech_prob": 0.0006262974929995835}, {"id": 238, "seek": 92760, "start": 927.76, "end": 930.36, "text": " Remember this whole thing can be trained offline. And then", "tokens": [50372, 5459, 341, 1379, 551, 393, 312, 8895, 21857, 13, 400, 550, 50502], "temperature": 0.0, "avg_logprob": -0.08934104535006737, "compression_ratio": 1.7442622950819673, "no_speech_prob": 0.0007552629103884101}, {"id": 239, "seek": 92760, "start": 930.36, "end": 933.2, "text": " when in inference mode, we can do it in a single shot. Whereas", "tokens": [50502, 562, 294, 38253, 4391, 11, 321, 393, 360, 309, 294, 257, 2167, 3347, 13, 13813, 50644], "temperature": 0.0, "avg_logprob": -0.08934104535006737, "compression_ratio": 1.7442622950819673, "no_speech_prob": 0.0007552629103884101}, {"id": 240, "seek": 92760, "start": 933.2, "end": 936.08, "text": " with Monte Carlo tree search, we actually had to do it in", "tokens": [50644, 365, 38105, 45112, 4230, 3164, 11, 321, 767, 632, 281, 360, 309, 294, 50788], "temperature": 0.0, "avg_logprob": -0.08934104535006737, "compression_ratio": 1.7442622950819673, "no_speech_prob": 0.0007552629103884101}, {"id": 241, "seek": 92760, "start": 936.08, "end": 939.76, "text": " inference mode as well. The other thing is we're kind of", "tokens": [50788, 38253, 4391, 382, 731, 13, 440, 661, 551, 307, 321, 434, 733, 295, 50972], "temperature": 0.0, "avg_logprob": -0.08934104535006737, "compression_ratio": 1.7442622950819673, "no_speech_prob": 0.0007552629103884101}, {"id": 242, "seek": 92760, "start": 940.44, "end": 943.32, "text": " offloading all of the human engineering required to sample", "tokens": [51006, 766, 2907, 278, 439, 295, 264, 1952, 7043, 4739, 281, 6889, 51150], "temperature": 0.0, "avg_logprob": -0.08934104535006737, "compression_ratio": 1.7442622950819673, "no_speech_prob": 0.0007552629103884101}, {"id": 243, "seek": 92760, "start": 943.32, "end": 946.0400000000001, "text": " efficiently from Markov chain Monte Carlo. And the other thing", "tokens": [51150, 19621, 490, 3934, 5179, 5021, 38105, 45112, 13, 400, 264, 661, 551, 51286], "temperature": 0.0, "avg_logprob": -0.08934104535006737, "compression_ratio": 1.7442622950819673, "no_speech_prob": 0.0007552629103884101}, {"id": 244, "seek": 92760, "start": 946.0400000000001, "end": 949.48, "text": " is diversity, baby. I mean, consider the difference between", "tokens": [51286, 307, 8811, 11, 3186, 13, 286, 914, 11, 1949, 264, 2649, 1296, 51458], "temperature": 0.0, "avg_logprob": -0.08934104535006737, "compression_ratio": 1.7442622950819673, "no_speech_prob": 0.0007552629103884101}, {"id": 245, "seek": 92760, "start": 949.48, "end": 952.2, "text": " how G flow nets and Alpha zero sample the reward path", "tokens": [51458, 577, 460, 3095, 36170, 293, 20588, 4018, 6889, 264, 7782, 3100, 51594], "temperature": 0.0, "avg_logprob": -0.08934104535006737, "compression_ratio": 1.7442622950819673, "no_speech_prob": 0.0007552629103884101}, {"id": 246, "seek": 92760, "start": 952.2, "end": 954.88, "text": " distribution. If you looked at the distributions, you would", "tokens": [51594, 7316, 13, 759, 291, 2956, 412, 264, 37870, 11, 291, 576, 51728], "temperature": 0.0, "avg_logprob": -0.08934104535006737, "compression_ratio": 1.7442622950819673, "no_speech_prob": 0.0007552629103884101}, {"id": 247, "seek": 95488, "start": 954.88, "end": 959.08, "text": " see that Alpha zero has a little box around the mode. G flow", "tokens": [50364, 536, 300, 20588, 4018, 575, 257, 707, 2424, 926, 264, 4391, 13, 460, 3095, 50574], "temperature": 0.0, "avg_logprob": -0.14825950219081, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.005218257661908865}, {"id": 248, "seek": 95488, "start": 959.08, "end": 962.4, "text": " nets is the whole distribution. We know very well that", "tokens": [50574, 36170, 307, 264, 1379, 7316, 13, 492, 458, 588, 731, 300, 50740], "temperature": 0.0, "avg_logprob": -0.14825950219081, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.005218257661908865}, {"id": 249, "seek": 95488, "start": 962.4, "end": 965.84, "text": " diversity preservation is critical in order to discover", "tokens": [50740, 8811, 27257, 307, 4924, 294, 1668, 281, 4411, 50912], "temperature": 0.0, "avg_logprob": -0.14825950219081, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.005218257661908865}, {"id": 250, "seek": 95488, "start": 965.84, "end": 969.32, "text": " interesting stepping stones and search problems. Now finally,", "tokens": [50912, 1880, 16821, 14083, 293, 3164, 2740, 13, 823, 2721, 11, 51086], "temperature": 0.0, "avg_logprob": -0.14825950219081, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.005218257661908865}, {"id": 251, "seek": 95488, "start": 969.32, "end": 972.2, "text": " Benzio has published results showing the G flow nets converge", "tokens": [51086, 3964, 89, 1004, 575, 6572, 3542, 4099, 264, 460, 3095, 36170, 41881, 51230], "temperature": 0.0, "avg_logprob": -0.14825950219081, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.005218257661908865}, {"id": 252, "seek": 95488, "start": 972.2, "end": 975.64, "text": " exponentially faster than Markov chain Monte Carlo and PPL on", "tokens": [51230, 37330, 4663, 813, 3934, 5179, 5021, 38105, 45112, 293, 430, 21593, 322, 51402], "temperature": 0.0, "avg_logprob": -0.14825950219081, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.005218257661908865}, {"id": 253, "seek": 95488, "start": 975.64, "end": 979.48, "text": " some problems and finds more of the modes in the distribution", "tokens": [51402, 512, 2740, 293, 10704, 544, 295, 264, 14068, 294, 264, 7316, 51594], "temperature": 0.0, "avg_logprob": -0.14825950219081, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.005218257661908865}, {"id": 254, "seek": 95488, "start": 979.48, "end": 982.48, "text": " function faster. Enjoy the show folks.", "tokens": [51594, 2445, 4663, 13, 15411, 264, 855, 4024, 13, 51744], "temperature": 0.0, "avg_logprob": -0.14825950219081, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.005218257661908865}, {"id": 255, "seek": 98248, "start": 983.32, "end": 987.9200000000001, "text": " Professor Yoshio Benzio is recognized worldwide as one of", "tokens": [50406, 8419, 38949, 1004, 3964, 89, 1004, 307, 9823, 13485, 382, 472, 295, 50636], "temperature": 0.0, "avg_logprob": -0.14841997743857027, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.01761094480752945}, {"id": 256, "seek": 98248, "start": 987.9200000000001, "end": 992.48, "text": " the leading experts in artificial intelligence. Indeed, a god", "tokens": [50636, 264, 5775, 8572, 294, 11677, 7599, 13, 15061, 11, 257, 3044, 50864], "temperature": 0.0, "avg_logprob": -0.14841997743857027, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.01761094480752945}, {"id": 257, "seek": 98248, "start": 992.48, "end": 995.6, "text": " father of deep learning. His pioneering work in deep learning", "tokens": [50864, 3086, 295, 2452, 2539, 13, 2812, 19761, 1794, 589, 294, 2452, 2539, 51020], "temperature": 0.0, "avg_logprob": -0.14841997743857027, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.01761094480752945}, {"id": 258, "seek": 98248, "start": 995.6, "end": 998.88, "text": " earned him the Turing Award, which is the Nobel Prize of", "tokens": [51020, 12283, 796, 264, 314, 1345, 13894, 11, 597, 307, 264, 24611, 22604, 295, 51184], "temperature": 0.0, "avg_logprob": -0.14841997743857027, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.01761094480752945}, {"id": 259, "seek": 98248, "start": 998.88, "end": 1002.48, "text": " computing. He's a full professor at the University of Montreal,", "tokens": [51184, 15866, 13, 634, 311, 257, 1577, 8304, 412, 264, 3535, 295, 34180, 11, 51364], "temperature": 0.0, "avg_logprob": -0.14841997743857027, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.01761094480752945}, {"id": 260, "seek": 98248, "start": 1002.6800000000001, "end": 1006.6, "text": " and the founder and scientific director of Miele, which is a", "tokens": [51374, 293, 264, 14917, 293, 8134, 5391, 295, 376, 15949, 11, 597, 307, 257, 51570], "temperature": 0.0, "avg_logprob": -0.14841997743857027, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.01761094480752945}, {"id": 261, "seek": 98248, "start": 1006.6, "end": 1010.8000000000001, "text": " prestigious community of more than 900 researchers specializing", "tokens": [51570, 33510, 1768, 295, 544, 813, 22016, 10309, 2121, 3319, 51780], "temperature": 0.0, "avg_logprob": -0.14841997743857027, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.01761094480752945}, {"id": 262, "seek": 101080, "start": 1010.8, "end": 1014.56, "text": " in machine learning and AI. He's one of the most cited", "tokens": [50364, 294, 3479, 2539, 293, 7318, 13, 634, 311, 472, 295, 264, 881, 30134, 50552], "temperature": 0.0, "avg_logprob": -0.08068454474733587, "compression_ratio": 1.6564625850340136, "no_speech_prob": 0.005635744892060757}, {"id": 263, "seek": 101080, "start": 1014.56, "end": 1018.28, "text": " computer scientists on the planet. And I can't even begin to", "tokens": [50552, 3820, 7708, 322, 264, 5054, 13, 400, 286, 393, 380, 754, 1841, 281, 50738], "temperature": 0.0, "avg_logprob": -0.08068454474733587, "compression_ratio": 1.6564625850340136, "no_speech_prob": 0.005635744892060757}, {"id": 264, "seek": 101080, "start": 1018.28, "end": 1021.1999999999999, "text": " articulate how honored we are today to have this conversation.", "tokens": [50738, 30305, 577, 14556, 321, 366, 965, 281, 362, 341, 3761, 13, 50884], "temperature": 0.0, "avg_logprob": -0.08068454474733587, "compression_ratio": 1.6564625850340136, "no_speech_prob": 0.005635744892060757}, {"id": 265, "seek": 101080, "start": 1021.5999999999999, "end": 1025.12, "text": " Yoshio has done a lot of work recently on G flow nets, which", "tokens": [50904, 38949, 1004, 575, 1096, 257, 688, 295, 589, 3938, 322, 460, 3095, 36170, 11, 597, 51080], "temperature": 0.0, "avg_logprob": -0.08068454474733587, "compression_ratio": 1.6564625850340136, "no_speech_prob": 0.005635744892060757}, {"id": 266, "seek": 101080, "start": 1025.12, "end": 1028.04, "text": " are an active learning framework in a reinforcement learning", "tokens": [51080, 366, 364, 4967, 2539, 8388, 294, 257, 29280, 2539, 51226], "temperature": 0.0, "avg_logprob": -0.08068454474733587, "compression_ratio": 1.6564625850340136, "no_speech_prob": 0.005635744892060757}, {"id": 267, "seek": 101080, "start": 1028.04, "end": 1032.1599999999999, "text": " configuration, where the name of the game is to request salient", "tokens": [51226, 11694, 11, 689, 264, 1315, 295, 264, 1216, 307, 281, 5308, 1845, 1196, 51432], "temperature": 0.0, "avg_logprob": -0.08068454474733587, "compression_ratio": 1.6564625850340136, "no_speech_prob": 0.005635744892060757}, {"id": 268, "seek": 101080, "start": 1032.2, "end": 1036.0, "text": " and diverse training data from the real world to augment our", "tokens": [51434, 293, 9521, 3097, 1412, 490, 264, 957, 1002, 281, 29919, 527, 51624], "temperature": 0.0, "avg_logprob": -0.08068454474733587, "compression_ratio": 1.6564625850340136, "no_speech_prob": 0.005635744892060757}, {"id": 269, "seek": 101080, "start": 1036.0, "end": 1039.96, "text": " learned models in the most sample efficient way possible. Now", "tokens": [51624, 3264, 5245, 294, 264, 881, 6889, 7148, 636, 1944, 13, 823, 51822], "temperature": 0.0, "avg_logprob": -0.08068454474733587, "compression_ratio": 1.6564625850340136, "no_speech_prob": 0.005635744892060757}, {"id": 270, "seek": 103996, "start": 1039.96, "end": 1042.64, "text": " we're trying to minimize the divergence between the path", "tokens": [50364, 321, 434, 1382, 281, 17522, 264, 47387, 1296, 264, 3100, 50498], "temperature": 0.0, "avg_logprob": -0.11044000176822438, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.003163515357300639}, {"id": 271, "seek": 103996, "start": 1042.64, "end": 1046.1200000000001, "text": " distribution and the reward distribution, and then sample", "tokens": [50498, 7316, 293, 264, 7782, 7316, 11, 293, 550, 6889, 50672], "temperature": 0.0, "avg_logprob": -0.11044000176822438, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.003163515357300639}, {"id": 272, "seek": 103996, "start": 1046.1200000000001, "end": 1050.04, "text": " paths according to the reward distribution. This is in stark", "tokens": [50672, 14518, 4650, 281, 264, 7782, 7316, 13, 639, 307, 294, 17417, 50868], "temperature": 0.0, "avg_logprob": -0.11044000176822438, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.003163515357300639}, {"id": 273, "seek": 103996, "start": 1050.04, "end": 1052.8, "text": " contrast with traditional reinforcement learning, where we", "tokens": [50868, 8712, 365, 5164, 29280, 2539, 11, 689, 321, 51006], "temperature": 0.0, "avg_logprob": -0.11044000176822438, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.003163515357300639}, {"id": 274, "seek": 103996, "start": 1052.8, "end": 1056.76, "text": " trying to maximize the expected reward. This approach is likely", "tokens": [51006, 1382, 281, 19874, 264, 5176, 7782, 13, 639, 3109, 307, 3700, 51204], "temperature": 0.0, "avg_logprob": -0.11044000176822438, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.003163515357300639}, {"id": 275, "seek": 103996, "start": 1056.76, "end": 1060.28, "text": " to find diverse strategies instead of being greedy and", "tokens": [51204, 281, 915, 9521, 9029, 2602, 295, 885, 28228, 293, 51380], "temperature": 0.0, "avg_logprob": -0.11044000176822438, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.003163515357300639}, {"id": 276, "seek": 103996, "start": 1060.28, "end": 1063.72, "text": " converging quickly after finding a single one. Anyway,", "tokens": [51380, 9652, 3249, 2661, 934, 5006, 257, 2167, 472, 13, 5684, 11, 51552], "temperature": 0.0, "avg_logprob": -0.11044000176822438, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.003163515357300639}, {"id": 277, "seek": 103996, "start": 1063.8400000000001, "end": 1066.8400000000001, "text": " Professor Benzio, this is amazing. Can you tell us about this", "tokens": [51558, 8419, 3964, 89, 1004, 11, 341, 307, 2243, 13, 1664, 291, 980, 505, 466, 341, 51708], "temperature": 0.0, "avg_logprob": -0.11044000176822438, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.003163515357300639}, {"id": 278, "seek": 106684, "start": 1066.84, "end": 1068.84, "text": " exciting work in some of its applications?", "tokens": [50364, 4670, 589, 294, 512, 295, 1080, 5821, 30, 50464], "temperature": 0.0, "avg_logprob": -0.1766816775004069, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.0035318396985530853}, {"id": 279, "seek": 106684, "start": 1069.52, "end": 1074.52, "text": " Yeah, I'm, I don't think I've been as excited about a new topic.", "tokens": [50498, 865, 11, 286, 478, 11, 286, 500, 380, 519, 286, 600, 668, 382, 2919, 466, 257, 777, 4829, 13, 50748], "temperature": 0.0, "avg_logprob": -0.1766816775004069, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.0035318396985530853}, {"id": 280, "seek": 106684, "start": 1076.3999999999999, "end": 1080.8, "text": " At least in the last six or seven years as I'm now with G flow", "tokens": [50842, 1711, 1935, 294, 264, 1036, 2309, 420, 3407, 924, 382, 286, 478, 586, 365, 460, 3095, 51062], "temperature": 0.0, "avg_logprob": -0.1766816775004069, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.0035318396985530853}, {"id": 281, "seek": 106684, "start": 1080.8, "end": 1086.24, "text": " nets. And it's actually even much more than what you've been", "tokens": [51062, 36170, 13, 400, 309, 311, 767, 754, 709, 544, 813, 437, 291, 600, 668, 51334], "temperature": 0.0, "avg_logprob": -0.1766816775004069, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.0035318396985530853}, {"id": 282, "seek": 106684, "start": 1086.24, "end": 1091.1999999999998, "text": " talking about. The way I think about G flow nets is a kind of", "tokens": [51334, 1417, 466, 13, 440, 636, 286, 519, 466, 460, 3095, 36170, 307, 257, 733, 295, 51582], "temperature": 0.0, "avg_logprob": -0.1766816775004069, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.0035318396985530853}, {"id": 283, "seek": 109120, "start": 1091.72, "end": 1094.1200000000001, "text": " framework for generic", "tokens": [50390, 8388, 337, 19577, 50510], "temperature": 0.0, "avg_logprob": -0.2182749420849245, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.010164923965930939}, {"id": 284, "seek": 109120, "start": 1095.92, "end": 1102.8400000000001, "text": " learnable inference for probabilistic machine learning. So", "tokens": [50600, 1466, 712, 38253, 337, 31959, 3142, 3479, 2539, 13, 407, 50946], "temperature": 0.0, "avg_logprob": -0.2182749420849245, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.010164923965930939}, {"id": 285, "seek": 109120, "start": 1103.28, "end": 1108.16, "text": " one way to think about this is it's a learnable replacement for", "tokens": [50968, 472, 636, 281, 519, 466, 341, 307, 309, 311, 257, 1466, 712, 14419, 337, 51212], "temperature": 0.0, "avg_logprob": -0.2182749420849245, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.010164923965930939}, {"id": 286, "seek": 109120, "start": 1108.2, "end": 1112.56, "text": " Monte Carlo Markov chain sampling. But actually, so there's", "tokens": [51214, 38105, 45112, 3934, 5179, 5021, 21179, 13, 583, 767, 11, 370, 456, 311, 51432], "temperature": 0.0, "avg_logprob": -0.2182749420849245, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.010164923965930939}, {"id": 287, "seek": 109120, "start": 1112.56, "end": 1117.64, "text": " that and I'll explain if you want why this is important and to use", "tokens": [51432, 300, 293, 286, 603, 2903, 498, 291, 528, 983, 341, 307, 1021, 293, 281, 764, 51686], "temperature": 0.0, "avg_logprob": -0.2182749420849245, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.010164923965930939}, {"id": 288, "seek": 111764, "start": 1117.64, "end": 1121.16, "text": " machine learning there. But but also, it can be used to", "tokens": [50364, 3479, 2539, 456, 13, 583, 457, 611, 11, 309, 393, 312, 1143, 281, 50540], "temperature": 0.0, "avg_logprob": -0.19667758498080942, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0021814287174493074}, {"id": 289, "seek": 111764, "start": 1121.16, "end": 1124.96, "text": " estimate probabilities themselves, not just sampling, but", "tokens": [50540, 12539, 33783, 2969, 11, 406, 445, 21179, 11, 457, 50730], "temperature": 0.0, "avg_logprob": -0.19667758498080942, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0021814287174493074}, {"id": 290, "seek": 111764, "start": 1124.96, "end": 1125.92, "text": " also estimate", "tokens": [50730, 611, 12539, 50778], "temperature": 0.0, "avg_logprob": -0.19667758498080942, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0021814287174493074}, {"id": 291, "seek": 111764, "start": 1127.5600000000002, "end": 1131.6000000000001, "text": " intractable quantities like partition functions and a", "tokens": [50860, 560, 1897, 712, 22927, 411, 24808, 6828, 293, 257, 51062], "temperature": 0.0, "avg_logprob": -0.19667758498080942, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0021814287174493074}, {"id": 292, "seek": 111764, "start": 1131.6000000000001, "end": 1134.96, "text": " condition probabilities that would otherwise require summing", "tokens": [51062, 4188, 33783, 300, 576, 5911, 3651, 2408, 2810, 51230], "temperature": 0.0, "avg_logprob": -0.19667758498080942, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0021814287174493074}, {"id": 293, "seek": 111764, "start": 1134.96, "end": 1140.1200000000001, "text": " over an intractable number of terms. So I think of this as", "tokens": [51230, 670, 364, 560, 1897, 712, 1230, 295, 2115, 13, 407, 286, 519, 295, 341, 382, 51488], "temperature": 0.0, "avg_logprob": -0.19667758498080942, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0021814287174493074}, {"id": 294, "seek": 111764, "start": 1140.1200000000001, "end": 1143.64, "text": " the potentially, you know, there's still we're still at the", "tokens": [51488, 264, 7263, 11, 291, 458, 11, 456, 311, 920, 321, 434, 920, 412, 264, 51664], "temperature": 0.0, "avg_logprob": -0.19667758498080942, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0021814287174493074}, {"id": 295, "seek": 114364, "start": 1143.64, "end": 1148.2800000000002, "text": " beginnings of this has a Swiss army knife of probabilistic", "tokens": [50364, 37281, 295, 341, 575, 257, 21965, 7267, 7976, 295, 31959, 3142, 50596], "temperature": 0.0, "avg_logprob": -0.13097436755311256, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0005702739581465721}, {"id": 296, "seek": 114364, "start": 1148.2800000000002, "end": 1151.96, "text": " modeling that uses machine learning to be able to do things", "tokens": [50596, 15983, 300, 4960, 3479, 2539, 281, 312, 1075, 281, 360, 721, 50780], "temperature": 0.0, "avg_logprob": -0.13097436755311256, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0005702739581465721}, {"id": 297, "seek": 114364, "start": 1151.96, "end": 1156.1200000000001, "text": " that look intractable, but do them efficiently thanks to", "tokens": [50780, 300, 574, 560, 1897, 712, 11, 457, 360, 552, 19621, 3231, 281, 50988], "temperature": 0.0, "avg_logprob": -0.13097436755311256, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0005702739581465721}, {"id": 298, "seek": 114364, "start": 1156.1200000000001, "end": 1158.2800000000002, "text": " generalization power of large neural nets.", "tokens": [50988, 2674, 2144, 1347, 295, 2416, 18161, 36170, 13, 51096], "temperature": 0.0, "avg_logprob": -0.13097436755311256, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0005702739581465721}, {"id": 299, "seek": 114364, "start": 1159.5200000000002, "end": 1162.96, "text": " We've been trying to think of a way to help our listeners", "tokens": [51158, 492, 600, 668, 1382, 281, 519, 295, 257, 636, 281, 854, 527, 23274, 51330], "temperature": 0.0, "avg_logprob": -0.13097436755311256, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0005702739581465721}, {"id": 300, "seek": 114364, "start": 1163.1200000000001, "end": 1168.68, "text": " visualize what a what a G flow net does. And I wanted to run by", "tokens": [51338, 23273, 437, 257, 437, 257, 460, 3095, 2533, 775, 13, 400, 286, 1415, 281, 1190, 538, 51616], "temperature": 0.0, "avg_logprob": -0.13097436755311256, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0005702739581465721}, {"id": 301, "seek": 114364, "start": 1168.72, "end": 1171.5200000000002, "text": " a possibility to you. So I'm not sure if you've heard of", "tokens": [51618, 257, 7959, 281, 291, 13, 407, 286, 478, 406, 988, 498, 291, 600, 2198, 295, 51758], "temperature": 0.0, "avg_logprob": -0.13097436755311256, "compression_ratio": 1.6072874493927125, "no_speech_prob": 0.0005702739581465721}, {"id": 302, "seek": 117152, "start": 1172.0, "end": 1176.36, "text": " Galton boards also called, you know, bean machines. And what", "tokens": [50388, 7336, 1756, 13293, 611, 1219, 11, 291, 458, 11, 16230, 8379, 13, 400, 437, 50606], "temperature": 0.0, "avg_logprob": -0.12503357841855003, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0032724719494581223}, {"id": 303, "seek": 117152, "start": 1176.36, "end": 1179.68, "text": " they are is this prop that's often used by statistics", "tokens": [50606, 436, 366, 307, 341, 2365, 300, 311, 2049, 1143, 538, 12523, 50772], "temperature": 0.0, "avg_logprob": -0.12503357841855003, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0032724719494581223}, {"id": 304, "seek": 117152, "start": 1179.68, "end": 1183.08, "text": " professors at the start of say, an elementary introductory", "tokens": [50772, 15924, 412, 264, 722, 295, 584, 11, 364, 16429, 39048, 50942], "temperature": 0.0, "avg_logprob": -0.12503357841855003, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0032724719494581223}, {"id": 305, "seek": 117152, "start": 1183.08, "end": 1187.08, "text": " course to give a visual intuition. And it's a board that has", "tokens": [50942, 1164, 281, 976, 257, 5056, 24002, 13, 400, 309, 311, 257, 3150, 300, 575, 51142], "temperature": 0.0, "avg_logprob": -0.12503357841855003, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0032724719494581223}, {"id": 306, "seek": 117152, "start": 1187.08, "end": 1191.6399999999999, "text": " these vertical buckets down at the bottom with interleaved rows", "tokens": [51142, 613, 9429, 32191, 760, 412, 264, 2767, 365, 728, 306, 12865, 13241, 51370], "temperature": 0.0, "avg_logprob": -0.12503357841855003, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0032724719494581223}, {"id": 307, "seek": 117152, "start": 1191.6399999999999, "end": 1197.08, "text": " of pegs above the buckets, and then beads are filled in into the", "tokens": [51370, 295, 17199, 82, 3673, 264, 32191, 11, 293, 550, 20369, 366, 6412, 294, 666, 264, 51642], "temperature": 0.0, "avg_logprob": -0.12503357841855003, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0032724719494581223}, {"id": 308, "seek": 117152, "start": 1197.08, "end": 1199.8, "text": " top of the board, and they bounce either left or right as", "tokens": [51642, 1192, 295, 264, 3150, 11, 293, 436, 15894, 2139, 1411, 420, 558, 382, 51778], "temperature": 0.0, "avg_logprob": -0.12503357841855003, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0032724719494581223}, {"id": 309, "seek": 119980, "start": 1199.8, "end": 1203.2, "text": " they hit the pegs. And they eventually collect down at the", "tokens": [50364, 436, 2045, 264, 17199, 82, 13, 400, 436, 4728, 2500, 760, 412, 264, 50534], "temperature": 0.0, "avg_logprob": -0.18401547482139186, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00154858676251024}, {"id": 310, "seek": 119980, "start": 1203.2, "end": 1205.96, "text": " bottom. Yeah, yeah. Yeah, now now if the peg is a very good", "tokens": [50534, 2767, 13, 865, 11, 1338, 13, 865, 11, 586, 586, 498, 264, 17199, 307, 257, 588, 665, 50672], "temperature": 0.0, "avg_logprob": -0.18401547482139186, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00154858676251024}, {"id": 311, "seek": 119980, "start": 1205.96, "end": 1208.96, "text": " analogy, except that it's not a tree, I don't know how these", "tokens": [50672, 21663, 11, 3993, 300, 309, 311, 406, 257, 4230, 11, 286, 500, 380, 458, 577, 613, 50822], "temperature": 0.0, "avg_logprob": -0.18401547482139186, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00154858676251024}, {"id": 312, "seek": 119980, "start": 1208.96, "end": 1212.2, "text": " things are, but you know, the ball can come to a place from", "tokens": [50822, 721, 366, 11, 457, 291, 458, 11, 264, 2594, 393, 808, 281, 257, 1081, 490, 50984], "temperature": 0.0, "avg_logprob": -0.18401547482139186, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00154858676251024}, {"id": 313, "seek": 119980, "start": 1212.2, "end": 1214.8, "text": " two different paths or an potentially large number of", "tokens": [50984, 732, 819, 14518, 420, 364, 7263, 2416, 1230, 295, 51114], "temperature": 0.0, "avg_logprob": -0.18401547482139186, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00154858676251024}, {"id": 314, "seek": 119980, "start": 1214.8, "end": 1218.8, "text": " paths. Right, right. And I think, given given there are some", "tokens": [51114, 14518, 13, 1779, 11, 558, 13, 400, 286, 519, 11, 2212, 2212, 456, 366, 512, 51314], "temperature": 0.0, "avg_logprob": -0.18401547482139186, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00154858676251024}, {"id": 315, "seek": 119980, "start": 1218.84, "end": 1222.52, "text": " some differences, you know, the idea was that if the pegs or", "tokens": [51316, 512, 7300, 11, 291, 458, 11, 264, 1558, 390, 300, 498, 264, 17199, 82, 420, 51500], "temperature": 0.0, "avg_logprob": -0.18401547482139186, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00154858676251024}, {"id": 316, "seek": 119980, "start": 1222.52, "end": 1225.3999999999999, "text": " no, it's it's pretty close to exactly what it is. Yeah, and", "tokens": [51500, 572, 11, 309, 311, 309, 311, 1238, 1998, 281, 2293, 437, 309, 307, 13, 865, 11, 293, 51644], "temperature": 0.0, "avg_logprob": -0.18401547482139186, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00154858676251024}, {"id": 317, "seek": 119980, "start": 1225.3999999999999, "end": 1228.36, "text": " what we were thinking is that if the pegs on the Galton board", "tokens": [51644, 437, 321, 645, 1953, 307, 300, 498, 264, 17199, 82, 322, 264, 7336, 1756, 3150, 51792], "temperature": 0.0, "avg_logprob": -0.18401547482139186, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00154858676251024}, {"id": 318, "seek": 122836, "start": 1228.36, "end": 1231.7199999999998, "text": " are precisely and symmetrically arranged, you know, the beads", "tokens": [50364, 366, 13402, 293, 14232, 27965, 984, 18721, 11, 291, 458, 11, 264, 20369, 50532], "temperature": 0.0, "avg_logprob": -0.09241199493408203, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0017544847214594483}, {"id": 319, "seek": 122836, "start": 1231.76, "end": 1236.1999999999998, "text": " will form a nice binomial curve at the bottom. And it seems", "tokens": [50534, 486, 1254, 257, 1481, 5171, 47429, 7605, 412, 264, 2767, 13, 400, 309, 2544, 50756], "temperature": 0.0, "avg_logprob": -0.09241199493408203, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0017544847214594483}, {"id": 320, "seek": 122836, "start": 1236.1999999999998, "end": 1239.52, "text": " like what G flow nets are capable of doing when they", "tokens": [50756, 411, 437, 460, 3095, 36170, 366, 8189, 295, 884, 562, 436, 50922], "temperature": 0.0, "avg_logprob": -0.09241199493408203, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0017544847214594483}, {"id": 321, "seek": 122836, "start": 1239.52, "end": 1244.36, "text": " optimize the pathways. They're tweaking the pegs a little bit", "tokens": [50922, 19719, 264, 22988, 13, 814, 434, 6986, 2456, 264, 17199, 82, 257, 707, 857, 51164], "temperature": 0.0, "avg_logprob": -0.09241199493408203, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0017544847214594483}, {"id": 322, "seek": 122836, "start": 1244.36, "end": 1248.1599999999999, "text": " to the left, or a little to the right, to bias the flow of", "tokens": [51164, 281, 264, 1411, 11, 420, 257, 707, 281, 264, 558, 11, 281, 12577, 264, 3095, 295, 51354], "temperature": 0.0, "avg_logprob": -0.09241199493408203, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0017544847214594483}, {"id": 323, "seek": 122836, "start": 1248.1599999999999, "end": 1251.4399999999998, "text": " beads one way or the other. And in this way, a G flow net", "tokens": [51354, 20369, 472, 636, 420, 264, 661, 13, 400, 294, 341, 636, 11, 257, 460, 3095, 2533, 51518], "temperature": 0.0, "avg_logprob": -0.09241199493408203, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0017544847214594483}, {"id": 324, "seek": 122836, "start": 1251.4399999999998, "end": 1254.24, "text": " could arrange the pegs so that the beads could form any", "tokens": [51518, 727, 9424, 264, 17199, 82, 370, 300, 264, 20369, 727, 1254, 604, 51658], "temperature": 0.0, "avg_logprob": -0.09241199493408203, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0017544847214594483}, {"id": 325, "seek": 122836, "start": 1254.24, "end": 1257.6399999999999, "text": " distribution at the bottom that we want. And for our purposes,", "tokens": [51658, 7316, 412, 264, 2767, 300, 321, 528, 13, 400, 337, 527, 9932, 11, 51828], "temperature": 0.0, "avg_logprob": -0.09241199493408203, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0017544847214594483}, {"id": 326, "seek": 125764, "start": 1257.64, "end": 1261.8000000000002, "text": " that means the distribution that matches the reward function. So", "tokens": [50364, 300, 1355, 264, 7316, 300, 10676, 264, 7782, 2445, 13, 407, 50572], "temperature": 0.0, "avg_logprob": -0.2189607521922318, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0002531270729377866}, {"id": 327, "seek": 125764, "start": 1261.8000000000002, "end": 1265.5200000000002, "text": " is this a good way to think about G flow nets? Yes, it is. Now,", "tokens": [50572, 307, 341, 257, 665, 636, 281, 519, 466, 460, 3095, 36170, 30, 1079, 11, 309, 307, 13, 823, 11, 50758], "temperature": 0.0, "avg_logprob": -0.2189607521922318, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0002531270729377866}, {"id": 328, "seek": 125764, "start": 1265.5600000000002, "end": 1267.88, "text": " it's missing a really important aspect of it, which would be", "tokens": [50760, 309, 311, 5361, 257, 534, 1021, 4171, 295, 309, 11, 597, 576, 312, 50876], "temperature": 0.0, "avg_logprob": -0.2189607521922318, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0002531270729377866}, {"id": 329, "seek": 125764, "start": 1267.88, "end": 1274.8000000000002, "text": " difficult to send visually, but that all of these peg weights,", "tokens": [50876, 2252, 281, 2845, 19622, 11, 457, 300, 439, 295, 613, 17199, 17443, 11, 51222], "temperature": 0.0, "avg_logprob": -0.2189607521922318, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0002531270729377866}, {"id": 330, "seek": 125764, "start": 1275.4, "end": 1280.96, "text": " like the polytip that are boggles left or right, are not just", "tokens": [51252, 411, 264, 6754, 83, 647, 300, 366, 26132, 70, 904, 1411, 420, 558, 11, 366, 406, 445, 51530], "temperature": 0.0, "avg_logprob": -0.2189607521922318, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0002531270729377866}, {"id": 331, "seek": 125764, "start": 1280.96, "end": 1283.92, "text": " like learned independently, like as a tabular machine", "tokens": [51530, 411, 3264, 21761, 11, 411, 382, 257, 4421, 1040, 3479, 51678], "temperature": 0.0, "avg_logprob": -0.2189607521922318, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0002531270729377866}, {"id": 332, "seek": 128392, "start": 1283.92, "end": 1288.48, "text": " earning, but that there's like one neural net that knows about", "tokens": [50364, 12353, 11, 457, 300, 456, 311, 411, 472, 18161, 2533, 300, 3255, 466, 50592], "temperature": 0.0, "avg_logprob": -0.17462343099165936, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.007457866333425045}, {"id": 333, "seek": 128392, "start": 1288.52, "end": 1294.5600000000002, "text": " the locations in this big board as input and tells, you know, how", "tokens": [50594, 264, 9253, 294, 341, 955, 3150, 382, 4846, 293, 5112, 11, 291, 458, 11, 577, 50896], "temperature": 0.0, "avg_logprob": -0.17462343099165936, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.007457866333425045}, {"id": 334, "seek": 128392, "start": 1294.5600000000002, "end": 1298.0800000000002, "text": " much relative weight should I, you know, go to go left or right", "tokens": [50896, 709, 4972, 3364, 820, 286, 11, 291, 458, 11, 352, 281, 352, 1411, 420, 558, 51072], "temperature": 0.0, "avg_logprob": -0.17462343099165936, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.007457866333425045}, {"id": 335, "seek": 128392, "start": 1298.0800000000002, "end": 1302.64, "text": " at this position. So the reason this is important is because it", "tokens": [51072, 412, 341, 2535, 13, 407, 264, 1778, 341, 307, 1021, 307, 570, 309, 51300], "temperature": 0.0, "avg_logprob": -0.17462343099165936, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.007457866333425045}, {"id": 336, "seek": 128392, "start": 1302.64, "end": 1307.0, "text": " allows for generalization. Because this board is huge, it's", "tokens": [51300, 4045, 337, 2674, 2144, 13, 1436, 341, 3150, 307, 2603, 11, 309, 311, 51518], "temperature": 0.0, "avg_logprob": -0.17462343099165936, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.007457866333425045}, {"id": 337, "seek": 128392, "start": 1307.0, "end": 1310.1200000000001, "text": " exponentially large. So there's no way you're going to learn, like", "tokens": [51518, 37330, 2416, 13, 407, 456, 311, 572, 636, 291, 434, 516, 281, 1466, 11, 411, 51674], "temperature": 0.0, "avg_logprob": -0.17462343099165936, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.007457866333425045}, {"id": 338, "seek": 131012, "start": 1310.12, "end": 1314.28, "text": " a separate parameter for each of these choices. And so you have", "tokens": [50364, 257, 4994, 13075, 337, 1184, 295, 613, 7994, 13, 400, 370, 291, 362, 50572], "temperature": 0.0, "avg_logprob": -0.1417983913421631, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.005218172445893288}, {"id": 339, "seek": 131012, "start": 1314.28, "end": 1317.9599999999998, "text": " this neural net or potentially several neural nets, but that", "tokens": [50572, 341, 18161, 2533, 420, 7263, 2940, 18161, 36170, 11, 457, 300, 50756], "temperature": 0.0, "avg_logprob": -0.1417983913421631, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.005218172445893288}, {"id": 340, "seek": 131012, "start": 1317.9599999999998, "end": 1322.56, "text": " share allow you to share statistical strength, as we", "tokens": [50756, 2073, 2089, 291, 281, 2073, 22820, 3800, 11, 382, 321, 50986], "temperature": 0.0, "avg_logprob": -0.1417983913421631, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.005218172445893288}, {"id": 341, "seek": 131012, "start": 1322.56, "end": 1326.8, "text": " call it, share information across all the possible positions, so", "tokens": [50986, 818, 309, 11, 2073, 1589, 2108, 439, 264, 1944, 8432, 11, 370, 51198], "temperature": 0.0, "avg_logprob": -0.1417983913421631, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.005218172445893288}, {"id": 342, "seek": 131012, "start": 1326.8, "end": 1330.1999999999998, "text": " that you can generalize to places paths that it has never", "tokens": [51198, 300, 291, 393, 2674, 1125, 281, 3190, 14518, 300, 309, 575, 1128, 51368], "temperature": 0.0, "avg_logprob": -0.1417983913421631, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.005218172445893288}, {"id": 343, "seek": 131012, "start": 1330.1999999999998, "end": 1334.08, "text": " seen from a finite number of training trajectories that it", "tokens": [51368, 1612, 490, 257, 19362, 1230, 295, 3097, 18257, 2083, 300, 309, 51562], "temperature": 0.0, "avg_logprob": -0.1417983913421631, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.005218172445893288}, {"id": 344, "seek": 131012, "start": 1334.08, "end": 1336.9399999999998, "text": " sees while it's being trained. And that's crucial. Otherwise, you", "tokens": [51562, 8194, 1339, 309, 311, 885, 8895, 13, 400, 300, 311, 11462, 13, 10328, 11, 291, 51705], "temperature": 0.0, "avg_logprob": -0.1417983913421631, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.005218172445893288}, {"id": 345, "seek": 133694, "start": 1336.94, "end": 1339.98, "text": " couldn't scale to large problems, which is really what we", "tokens": [50364, 2809, 380, 4373, 281, 2416, 2740, 11, 597, 307, 534, 437, 321, 50516], "temperature": 0.0, "avg_logprob": -0.12824764536387884, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.002574192825704813}, {"id": 346, "seek": 133694, "start": 1339.98, "end": 1340.5, "text": " want to do.", "tokens": [50516, 528, 281, 360, 13, 50542], "temperature": 0.0, "avg_logprob": -0.12824764536387884, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.002574192825704813}, {"id": 347, "seek": 133694, "start": 1341.22, "end": 1344.26, "text": " Professor Benjo, we spoke with Professor Carl Friston about his", "tokens": [50578, 8419, 3964, 5134, 11, 321, 7179, 365, 8419, 14256, 1526, 47345, 466, 702, 50730], "temperature": 0.0, "avg_logprob": -0.12824764536387884, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.002574192825704813}, {"id": 348, "seek": 133694, "start": 1344.26, "end": 1347.78, "text": " free energy principle, an active inference, which is pretty much", "tokens": [50730, 1737, 2281, 8665, 11, 364, 4967, 38253, 11, 597, 307, 1238, 709, 50906], "temperature": 0.0, "avg_logprob": -0.12824764536387884, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.002574192825704813}, {"id": 349, "seek": 133694, "start": 1347.78, "end": 1350.6200000000001, "text": " a Bayesian flavored version of reinforcement learning. And he", "tokens": [50906, 257, 7840, 42434, 37261, 3037, 295, 29280, 2539, 13, 400, 415, 51048], "temperature": 0.0, "avg_logprob": -0.12824764536387884, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.002574192825704813}, {"id": 350, "seek": 133694, "start": 1350.6200000000001, "end": 1353.22, "text": " said that while we need to maintain entropy and stop", "tokens": [51048, 848, 300, 1339, 321, 643, 281, 6909, 30867, 293, 1590, 51178], "temperature": 0.0, "avg_logprob": -0.12824764536387884, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.002574192825704813}, {"id": 351, "seek": 133694, "start": 1353.22, "end": 1356.5, "text": " models from increasing too much in complexity, we should balance", "tokens": [51178, 5245, 490, 5662, 886, 709, 294, 14024, 11, 321, 820, 4772, 51342], "temperature": 0.0, "avg_logprob": -0.12824764536387884, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.002574192825704813}, {"id": 352, "seek": 133694, "start": 1356.5, "end": 1358.8600000000001, "text": " entropy with accuracy in a principled way. And by the way,", "tokens": [51342, 30867, 365, 14170, 294, 257, 3681, 15551, 636, 13, 400, 538, 264, 636, 11, 51460], "temperature": 0.0, "avg_logprob": -0.12824764536387884, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.002574192825704813}, {"id": 353, "seek": 133694, "start": 1358.8600000000001, "end": 1360.42, "text": " you can kind of think of them in just the audience think of", "tokens": [51460, 291, 393, 733, 295, 519, 295, 552, 294, 445, 264, 4034, 519, 295, 51538], "temperature": 0.0, "avg_logprob": -0.12824764536387884, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.002574192825704813}, {"id": 354, "seek": 133694, "start": 1360.42, "end": 1364.5, "text": " entropy as keeping your options open. But Friston thinks that", "tokens": [51538, 30867, 382, 5145, 428, 3956, 1269, 13, 583, 1526, 47345, 7309, 300, 51742], "temperature": 0.0, "avg_logprob": -0.12824764536387884, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.002574192825704813}, {"id": 355, "seek": 136450, "start": 1364.5, "end": 1366.86, "text": " the Bellman-esque idea of reinforcement learning, which is", "tokens": [50364, 264, 11485, 1601, 12, 47457, 1558, 295, 29280, 2539, 11, 597, 307, 50482], "temperature": 0.0, "avg_logprob": -0.14512622565553898, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.00022332555090542883}, {"id": 356, "seek": 136450, "start": 1366.86, "end": 1370.02, "text": " to say maximizing expected reward is the objective is", "tokens": [50482, 281, 584, 5138, 3319, 5176, 7782, 307, 264, 10024, 307, 50640], "temperature": 0.0, "avg_logprob": -0.14512622565553898, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.00022332555090542883}, {"id": 357, "seek": 136450, "start": 1370.02, "end": 1372.74, "text": " misguided. And we should instead perform inference over future", "tokens": [50640, 3346, 2794, 2112, 13, 400, 321, 820, 2602, 2042, 38253, 670, 2027, 50776], "temperature": 0.0, "avg_logprob": -0.14512622565553898, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.00022332555090542883}, {"id": 358, "seek": 136450, "start": 1372.74, "end": 1376.7, "text": " paths, balancing expected reward of relative entropy. Is there a", "tokens": [50776, 14518, 11, 22495, 5176, 7782, 295, 4972, 30867, 13, 1119, 456, 257, 50974], "temperature": 0.0, "avg_logprob": -0.14512622565553898, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.00022332555090542883}, {"id": 359, "seek": 136450, "start": 1376.7, "end": 1379.26, "text": " connection between these ideas? I mean, it seems like G flow", "tokens": [50974, 4984, 1296, 613, 3487, 30, 286, 914, 11, 309, 2544, 411, 460, 3095, 51102], "temperature": 0.0, "avg_logprob": -0.14512622565553898, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.00022332555090542883}, {"id": 360, "seek": 136450, "start": 1379.26, "end": 1381.42, "text": " nets are sampling paths proportional to the reward", "tokens": [51102, 36170, 366, 21179, 14518, 24969, 281, 264, 7782, 51210], "temperature": 0.0, "avg_logprob": -0.14512622565553898, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.00022332555090542883}, {"id": 361, "seek": 136450, "start": 1381.42, "end": 1384.82, "text": " function, that will maintain as much entropy as the reward", "tokens": [51210, 2445, 11, 300, 486, 6909, 382, 709, 30867, 382, 264, 7782, 51380], "temperature": 0.0, "avg_logprob": -0.14512622565553898, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.00022332555090542883}, {"id": 362, "seek": 136450, "start": 1384.82, "end": 1385.62, "text": " function itself.", "tokens": [51380, 2445, 2564, 13, 51420], "temperature": 0.0, "avg_logprob": -0.14512622565553898, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.00022332555090542883}, {"id": 363, "seek": 136450, "start": 1387.58, "end": 1391.62, "text": " Yes, yes, exactly. It's a translation of the reward", "tokens": [51518, 1079, 11, 2086, 11, 2293, 13, 467, 311, 257, 12853, 295, 264, 7782, 51720], "temperature": 0.0, "avg_logprob": -0.14512622565553898, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.00022332555090542883}, {"id": 364, "seek": 139162, "start": 1391.62, "end": 1397.1399999999999, "text": " function into machinery that can sample, you know, the equivalent,", "tokens": [50364, 2445, 666, 27302, 300, 393, 6889, 11, 291, 458, 11, 264, 10344, 11, 50640], "temperature": 0.0, "avg_logprob": -0.15800636045394406, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0051362402737140656}, {"id": 365, "seek": 139162, "start": 1397.1399999999999, "end": 1401.34, "text": " the corresponding distribution. So yeah, I completely agree with", "tokens": [50640, 264, 11760, 7316, 13, 407, 1338, 11, 286, 2584, 3986, 365, 50850], "temperature": 0.0, "avg_logprob": -0.15800636045394406, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0051362402737140656}, {"id": 366, "seek": 139162, "start": 1401.62, "end": 1407.6599999999999, "text": " what Carl was saying here. But as I said, what's interesting is,", "tokens": [50864, 437, 14256, 390, 1566, 510, 13, 583, 382, 286, 848, 11, 437, 311, 1880, 307, 11, 51166], "temperature": 0.0, "avg_logprob": -0.15800636045394406, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0051362402737140656}, {"id": 367, "seek": 139162, "start": 1409.1399999999999, "end": 1415.06, "text": " we can do things with G flow nets. In principle, we've done the", "tokens": [51240, 321, 393, 360, 721, 365, 460, 3095, 36170, 13, 682, 8665, 11, 321, 600, 1096, 264, 51536], "temperature": 0.0, "avg_logprob": -0.15800636045394406, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0051362402737140656}, {"id": 368, "seek": 139162, "start": 1415.06, "end": 1417.78, "text": " math and some small scale experiments that we have now a", "tokens": [51536, 5221, 293, 512, 1359, 4373, 12050, 300, 321, 362, 586, 257, 51672], "temperature": 0.0, "avg_logprob": -0.15800636045394406, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0051362402737140656}, {"id": 369, "seek": 139162, "start": 1417.78, "end": 1421.06, "text": " number of papers, we can do things that go beyond sampling.", "tokens": [51672, 1230, 295, 10577, 11, 321, 393, 360, 721, 300, 352, 4399, 21179, 13, 51836], "temperature": 0.0, "avg_logprob": -0.15800636045394406, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0051362402737140656}, {"id": 370, "seek": 142162, "start": 1422.1799999999998, "end": 1427.6599999999999, "text": " But for example, estimate entropy itself. So entropy is", "tokens": [50392, 583, 337, 1365, 11, 12539, 30867, 2564, 13, 407, 30867, 307, 50666], "temperature": 0.0, "avg_logprob": -0.1495143307579888, "compression_ratio": 1.6574585635359116, "no_speech_prob": 0.0006459029391407967}, {"id": 371, "seek": 142162, "start": 1428.06, "end": 1437.2199999999998, "text": " notoriously difficult to estimate. And I mentioned in my talks on", "tokens": [50686, 46772, 8994, 2252, 281, 12539, 13, 400, 286, 2835, 294, 452, 6686, 322, 51144], "temperature": 0.0, "avg_logprob": -0.1495143307579888, "compression_ratio": 1.6574585635359116, "no_speech_prob": 0.0006459029391407967}, {"id": 372, "seek": 142162, "start": 1437.2199999999998, "end": 1442.1399999999999, "text": " G flow nets that we can use the G flow net machinery to estimate", "tokens": [51144, 460, 3095, 36170, 300, 321, 393, 764, 264, 460, 3095, 2533, 27302, 281, 12539, 51390], "temperature": 0.0, "avg_logprob": -0.1495143307579888, "compression_ratio": 1.6574585635359116, "no_speech_prob": 0.0006459029391407967}, {"id": 373, "seek": 142162, "start": 1442.1799999999998, "end": 1446.62, "text": " entropy of say, an action distribution or a distribution", "tokens": [51392, 30867, 295, 584, 11, 364, 3069, 7316, 420, 257, 7316, 51614], "temperature": 0.0, "avg_logprob": -0.1495143307579888, "compression_ratio": 1.6574585635359116, "no_speech_prob": 0.0006459029391407967}, {"id": 374, "seek": 142162, "start": 1446.62, "end": 1449.9799999999998, "text": " over Bayesian parameters, for example, which is would be", "tokens": [51614, 670, 7840, 42434, 9834, 11, 337, 1365, 11, 597, 307, 576, 312, 51782], "temperature": 0.0, "avg_logprob": -0.1495143307579888, "compression_ratio": 1.6574585635359116, "no_speech_prob": 0.0006459029391407967}, {"id": 375, "seek": 144998, "start": 1449.98, "end": 1453.26, "text": " something you'd like to minimize if you're going to take an", "tokens": [50364, 746, 291, 1116, 411, 281, 17522, 498, 291, 434, 516, 281, 747, 364, 50528], "temperature": 0.0, "avg_logprob": -0.12832012354770553, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0013454012805595994}, {"id": 376, "seek": 144998, "start": 1453.26, "end": 1456.9, "text": " action in the world. And you have a model of the world that has", "tokens": [50528, 3069, 294, 264, 1002, 13, 400, 291, 362, 257, 2316, 295, 264, 1002, 300, 575, 50710], "temperature": 0.0, "avg_logprob": -0.12832012354770553, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0013454012805595994}, {"id": 377, "seek": 144998, "start": 1456.9, "end": 1460.78, "text": " uncertainty. And that connects with Carl, for instance,", "tokens": [50710, 15697, 13, 400, 300, 16967, 365, 14256, 11, 337, 5197, 11, 50904], "temperature": 0.0, "avg_logprob": -0.12832012354770553, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0013454012805595994}, {"id": 378, "seek": 144998, "start": 1460.78, "end": 1464.8600000000001, "text": " interest, you'd like to be able to choose an action that", "tokens": [50904, 1179, 11, 291, 1116, 411, 281, 312, 1075, 281, 2826, 364, 3069, 300, 51108], "temperature": 0.0, "avg_logprob": -0.12832012354770553, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0013454012805595994}, {"id": 379, "seek": 144998, "start": 1464.8600000000001, "end": 1468.46, "text": " minimizes your uncertainty about how the world works, we know", "tokens": [51108, 4464, 5660, 428, 15697, 466, 577, 264, 1002, 1985, 11, 321, 458, 51288], "temperature": 0.0, "avg_logprob": -0.12832012354770553, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0013454012805595994}, {"id": 380, "seek": 144998, "start": 1468.46, "end": 1472.9, "text": " what are the latent things that may have happened. And good, you", "tokens": [51288, 437, 366, 264, 48994, 721, 300, 815, 362, 2011, 13, 400, 665, 11, 291, 51510], "temperature": 0.0, "avg_logprob": -0.12832012354770553, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0013454012805595994}, {"id": 381, "seek": 144998, "start": 1472.9, "end": 1476.54, "text": " know, an important part of that is estimating the reward for", "tokens": [51510, 458, 11, 364, 1021, 644, 295, 300, 307, 8017, 990, 264, 7782, 337, 51692], "temperature": 0.0, "avg_logprob": -0.12832012354770553, "compression_ratio": 1.811965811965812, "no_speech_prob": 0.0013454012805595994}, {"id": 382, "seek": 147654, "start": 1476.58, "end": 1480.6599999999999, "text": " the these exploratory actions, like, you know, children playing", "tokens": [50366, 264, 613, 24765, 4745, 5909, 11, 411, 11, 291, 458, 11, 2227, 2433, 50570], "temperature": 0.0, "avg_logprob": -0.17001139107397048, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.004463000688701868}, {"id": 383, "seek": 147654, "start": 1480.6599999999999, "end": 1485.54, "text": " around is how much reduction in entropy of my knowledge of the", "tokens": [50570, 926, 307, 577, 709, 11004, 294, 30867, 295, 452, 3601, 295, 264, 50814], "temperature": 0.0, "avg_logprob": -0.17001139107397048, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.004463000688701868}, {"id": 384, "seek": 147654, "start": 1485.54, "end": 1488.06, "text": " world, I'm going to get through that action. So you need to be", "tokens": [50814, 1002, 11, 286, 478, 516, 281, 483, 807, 300, 3069, 13, 407, 291, 643, 281, 312, 50940], "temperature": 0.0, "avg_logprob": -0.17001139107397048, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.004463000688701868}, {"id": 385, "seek": 147654, "start": 1488.06, "end": 1491.3799999999999, "text": " able to compute that reward. That reward word is basically an", "tokens": [50940, 1075, 281, 14722, 300, 7782, 13, 663, 7782, 1349, 307, 1936, 364, 51106], "temperature": 0.0, "avg_logprob": -0.17001139107397048, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.004463000688701868}, {"id": 386, "seek": 147654, "start": 1491.3799999999999, "end": 1495.86, "text": " entropy over something you care about. And it turns out you can", "tokens": [51106, 30867, 670, 746, 291, 1127, 466, 13, 400, 309, 4523, 484, 291, 393, 51330], "temperature": 0.0, "avg_logprob": -0.17001139107397048, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.004463000688701868}, {"id": 387, "seek": 147654, "start": 1495.86, "end": 1497.54, "text": " also do that with G flow nets.", "tokens": [51330, 611, 360, 300, 365, 460, 3095, 36170, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17001139107397048, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.004463000688701868}, {"id": 388, "seek": 147654, "start": 1497.7, "end": 1499.98, "text": " We're actually speaking with Friston again next week, do you", "tokens": [51422, 492, 434, 767, 4124, 365, 1526, 47345, 797, 958, 1243, 11, 360, 291, 51536], "temperature": 0.0, "avg_logprob": -0.17001139107397048, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.004463000688701868}, {"id": 389, "seek": 147654, "start": 1499.98, "end": 1502.26, "text": " have a question that you would like us to put to him?", "tokens": [51536, 362, 257, 1168, 300, 291, 576, 411, 505, 281, 829, 281, 796, 30, 51650], "temperature": 0.0, "avg_logprob": -0.17001139107397048, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.004463000688701868}, {"id": 390, "seek": 150226, "start": 1503.14, "end": 1507.54, "text": " Well, he, you know, he's on the biology side of things much", "tokens": [50408, 1042, 11, 415, 11, 291, 458, 11, 415, 311, 322, 264, 14956, 1252, 295, 721, 709, 50628], "temperature": 0.0, "avg_logprob": -0.09536702292306083, "compression_ratio": 1.4803921568627452, "no_speech_prob": 0.0009396194946020842}, {"id": 391, "seek": 150226, "start": 1507.54, "end": 1514.14, "text": " more than I am. And I believe there are amazing scientific", "tokens": [50628, 544, 813, 286, 669, 13, 400, 286, 1697, 456, 366, 2243, 8134, 50958], "temperature": 0.0, "avg_logprob": -0.09536702292306083, "compression_ratio": 1.4803921568627452, "no_speech_prob": 0.0009396194946020842}, {"id": 392, "seek": 150226, "start": 1514.14, "end": 1522.3799999999999, "text": " opportunities to explore how the kind of machinery that G", "tokens": [50958, 4786, 281, 6839, 577, 264, 733, 295, 27302, 300, 460, 51370], "temperature": 0.0, "avg_logprob": -0.09536702292306083, "compression_ratio": 1.4803921568627452, "no_speech_prob": 0.0009396194946020842}, {"id": 393, "seek": 150226, "start": 1522.3799999999999, "end": 1527.26, "text": " flow nets offer could be used by brains in order to do some of", "tokens": [51370, 3095, 36170, 2626, 727, 312, 1143, 538, 15442, 294, 1668, 281, 360, 512, 295, 51614], "temperature": 0.0, "avg_logprob": -0.09536702292306083, "compression_ratio": 1.4803921568627452, "no_speech_prob": 0.0009396194946020842}, {"id": 394, "seek": 150226, "start": 1527.26, "end": 1531.58, "text": " the things they do. Using your nets to model the probabilistic", "tokens": [51614, 264, 721, 436, 360, 13, 11142, 428, 36170, 281, 2316, 264, 31959, 3142, 51830], "temperature": 0.0, "avg_logprob": -0.09536702292306083, "compression_ratio": 1.4803921568627452, "no_speech_prob": 0.0009396194946020842}, {"id": 395, "seek": 153158, "start": 1531.58, "end": 1533.86, "text": " structure of the world, including uncertainty, which is", "tokens": [50364, 3877, 295, 264, 1002, 11, 3009, 15697, 11, 597, 307, 50478], "temperature": 0.0, "avg_logprob": -0.1390371322631836, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0016219932585954666}, {"id": 396, "seek": 153158, "start": 1533.86, "end": 1538.4199999999998, "text": " something he cares about. But but also taking into consideration", "tokens": [50478, 746, 415, 12310, 466, 13, 583, 457, 611, 1940, 666, 12381, 50706], "temperature": 0.0, "avg_logprob": -0.1390371322631836, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0016219932585954666}, {"id": 397, "seek": 153158, "start": 1538.4199999999998, "end": 1541.78, "text": " things like high level cognition, the global workspace theory,", "tokens": [50706, 721, 411, 1090, 1496, 46905, 11, 264, 4338, 32706, 5261, 11, 50874], "temperature": 0.0, "avg_logprob": -0.1390371322631836, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0016219932585954666}, {"id": 398, "seek": 153158, "start": 1541.78, "end": 1545.62, "text": " which is something I care a lot about, attention, they all kind", "tokens": [50874, 597, 307, 746, 286, 1127, 257, 688, 466, 11, 3202, 11, 436, 439, 733, 51066], "temperature": 0.0, "avg_logprob": -0.1390371322631836, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0016219932585954666}, {"id": 399, "seek": 153158, "start": 1545.62, "end": 1553.78, "text": " of fit in the picture of G flow nets. So so I think there's a", "tokens": [51066, 295, 3318, 294, 264, 3036, 295, 460, 3095, 36170, 13, 407, 370, 286, 519, 456, 311, 257, 51474], "temperature": 0.0, "avg_logprob": -0.1390371322631836, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0016219932585954666}, {"id": 400, "seek": 153158, "start": 1553.82, "end": 1559.02, "text": " huge potential of research at the synergy of computational and", "tokens": [51476, 2603, 3995, 295, 2132, 412, 264, 50163, 295, 28270, 293, 51736], "temperature": 0.0, "avg_logprob": -0.1390371322631836, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.0016219932585954666}, {"id": 401, "seek": 155902, "start": 1559.02, "end": 1563.46, "text": " theoretical neuroscience, and machine learning, probabilistic", "tokens": [50364, 20864, 42762, 11, 293, 3479, 2539, 11, 31959, 3142, 50586], "temperature": 0.0, "avg_logprob": -0.14840412139892578, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.0019810639787465334}, {"id": 402, "seek": 155902, "start": 1563.46, "end": 1567.94, "text": " modeling of the kind that G flow nets propose to come up with a", "tokens": [50586, 15983, 295, 264, 733, 300, 460, 3095, 36170, 17421, 281, 808, 493, 365, 257, 50810], "temperature": 0.0, "avg_logprob": -0.14840412139892578, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.0019810639787465334}, {"id": 403, "seek": 155902, "start": 1568.98, "end": 1573.86, "text": " some proposals for explanatory theories about what the brain", "tokens": [50862, 512, 20198, 337, 9045, 4745, 13667, 466, 437, 264, 3567, 51106], "temperature": 0.0, "avg_logprob": -0.14840412139892578, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.0019810639787465334}, {"id": 404, "seek": 155902, "start": 1573.86, "end": 1578.3, "text": " does, that's probabilistic. And, you know, I think he would be a", "tokens": [51106, 775, 11, 300, 311, 31959, 3142, 13, 400, 11, 291, 458, 11, 286, 519, 415, 576, 312, 257, 51328], "temperature": 0.0, "avg_logprob": -0.14840412139892578, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.0019810639787465334}, {"id": 405, "seek": 155902, "start": 1578.3, "end": 1579.9, "text": " great person to be part of that.", "tokens": [51328, 869, 954, 281, 312, 644, 295, 300, 13, 51408], "temperature": 0.0, "avg_logprob": -0.14840412139892578, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.0019810639787465334}, {"id": 406, "seek": 155902, "start": 1580.22, "end": 1584.22, "text": " Fascinating. Well, going a little bit further down that line,", "tokens": [51424, 49098, 8205, 13, 1042, 11, 516, 257, 707, 857, 3052, 760, 300, 1622, 11, 51624], "temperature": 0.0, "avg_logprob": -0.14840412139892578, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.0019810639787465334}, {"id": 407, "seek": 155902, "start": 1584.42, "end": 1587.34, "text": " there are folks in the community who are huge advocates of", "tokens": [51634, 456, 366, 4024, 294, 264, 1768, 567, 366, 2603, 25160, 295, 51780], "temperature": 0.0, "avg_logprob": -0.14840412139892578, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.0019810639787465334}, {"id": 408, "seek": 158734, "start": 1587.34, "end": 1589.22, "text": " biologically inspired approaches to machine", "tokens": [50364, 3228, 17157, 7547, 11587, 281, 3479, 50458], "temperature": 0.0, "avg_logprob": -0.11705351670583089, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.025848858058452606}, {"id": 409, "seek": 158734, "start": 1589.22, "end": 1592.54, "text": " intelligence. And, you know, one of the key ideas actually is", "tokens": [50458, 7599, 13, 400, 11, 291, 458, 11, 472, 295, 264, 2141, 3487, 767, 307, 50624], "temperature": 0.0, "avg_logprob": -0.11705351670583089, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.025848858058452606}, {"id": 410, "seek": 158734, "start": 1592.54, "end": 1595.62, "text": " diversity, discovery and preservation, both in how", "tokens": [50624, 8811, 11, 12114, 293, 27257, 11, 1293, 294, 577, 50778], "temperature": 0.0, "avg_logprob": -0.11705351670583089, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.025848858058452606}, {"id": 411, "seek": 158734, "start": 1595.62, "end": 1598.26, "text": " knowledge is acquired and represented. I mean, specifically", "tokens": [50778, 3601, 307, 17554, 293, 10379, 13, 286, 914, 11, 4682, 50910], "temperature": 0.0, "avg_logprob": -0.11705351670583089, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.025848858058452606}, {"id": 412, "seek": 158734, "start": 1598.26, "end": 1601.3, "text": " evolutionary algorithm advocates, they differentiate", "tokens": [50910, 27567, 9284, 25160, 11, 436, 23203, 51062], "temperature": 0.0, "avg_logprob": -0.11705351670583089, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.025848858058452606}, {"id": 413, "seek": 158734, "start": 1601.3, "end": 1604.06, "text": " themselves from gradient based single agent monolithic", "tokens": [51062, 2969, 490, 16235, 2361, 2167, 9461, 1108, 42878, 51200], "temperature": 0.0, "avg_logprob": -0.11705351670583089, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.025848858058452606}, {"id": 414, "seek": 158734, "start": 1604.06, "end": 1607.1399999999999, "text": " approaches like reinforcement learning. And they point out that", "tokens": [51200, 11587, 411, 29280, 2539, 13, 400, 436, 935, 484, 300, 51354], "temperature": 0.0, "avg_logprob": -0.11705351670583089, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.025848858058452606}, {"id": 415, "seek": 158734, "start": 1607.22, "end": 1610.3799999999999, "text": " their approaches overcome so called deception and search", "tokens": [51358, 641, 11587, 10473, 370, 1219, 40451, 293, 3164, 51516], "temperature": 0.0, "avg_logprob": -0.11705351670583089, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.025848858058452606}, {"id": 416, "seek": 158734, "start": 1610.3799999999999, "end": 1612.54, "text": " problems, you know, which is to say they don't get stuck in", "tokens": [51516, 2740, 11, 291, 458, 11, 597, 307, 281, 584, 436, 500, 380, 483, 5541, 294, 51624], "temperature": 0.0, "avg_logprob": -0.11705351670583089, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.025848858058452606}, {"id": 417, "seek": 158734, "start": 1612.54, "end": 1615.6999999999998, "text": " local minima, your approach seems to be achieving something very", "tokens": [51624, 2654, 4464, 64, 11, 428, 3109, 2544, 281, 312, 19626, 746, 588, 51782], "temperature": 0.0, "avg_logprob": -0.11705351670583089, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.025848858058452606}, {"id": 418, "seek": 161570, "start": 1615.7, "end": 1618.46, "text": " similar in the context of a gradient based reinforcement", "tokens": [50364, 2531, 294, 264, 4319, 295, 257, 16235, 2361, 29280, 50502], "temperature": 0.0, "avg_logprob": -0.1413156234466278, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0006457187118940055}, {"id": 419, "seek": 161570, "start": 1618.46, "end": 1620.42, "text": " learning package. I mean, I don't see it as being mutually", "tokens": [50502, 2539, 7372, 13, 286, 914, 11, 286, 500, 380, 536, 309, 382, 885, 39144, 50600], "temperature": 0.0, "avg_logprob": -0.1413156234466278, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0006457187118940055}, {"id": 420, "seek": 161570, "start": 1620.42, "end": 1621.94, "text": " exclusive. But what's your take on this?", "tokens": [50600, 13005, 13, 583, 437, 311, 428, 747, 322, 341, 30, 50676], "temperature": 0.0, "avg_logprob": -0.1413156234466278, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0006457187118940055}, {"id": 421, "seek": 161570, "start": 1622.46, "end": 1627.26, "text": " Yeah, diversity is important when you're exploring and humans,", "tokens": [50702, 865, 11, 8811, 307, 1021, 562, 291, 434, 12736, 293, 6255, 11, 50942], "temperature": 0.0, "avg_logprob": -0.1413156234466278, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0006457187118940055}, {"id": 422, "seek": 161570, "start": 1627.26, "end": 1630.22, "text": " especially young ones are exploration machines, they're", "tokens": [50942, 2318, 2037, 2306, 366, 16197, 8379, 11, 436, 434, 51090], "temperature": 0.0, "avg_logprob": -0.1413156234466278, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0006457187118940055}, {"id": 423, "seek": 161570, "start": 1630.22, "end": 1632.46, "text": " trying to understand how the world works and they're acting in", "tokens": [51090, 1382, 281, 1223, 577, 264, 1002, 1985, 293, 436, 434, 6577, 294, 51202], "temperature": 0.0, "avg_logprob": -0.1413156234466278, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0006457187118940055}, {"id": 424, "seek": 161570, "start": 1632.46, "end": 1636.5, "text": " the world in order to get that information. Yeah, I agree that", "tokens": [51202, 264, 1002, 294, 1668, 281, 483, 300, 1589, 13, 865, 11, 286, 3986, 300, 51404], "temperature": 0.0, "avg_logprob": -0.1413156234466278, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0006457187118940055}, {"id": 425, "seek": 161570, "start": 1636.94, "end": 1643.74, "text": " that search process needs to have a big bonus on on diversity,", "tokens": [51426, 300, 3164, 1399, 2203, 281, 362, 257, 955, 10882, 322, 322, 8811, 11, 51766], "temperature": 0.0, "avg_logprob": -0.1413156234466278, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.0006457187118940055}, {"id": 426, "seek": 164374, "start": 1643.74, "end": 1648.34, "text": " like on trying different ways of achieving something good, like", "tokens": [50364, 411, 322, 1382, 819, 2098, 295, 19626, 746, 665, 11, 411, 50594], "temperature": 0.0, "avg_logprob": -0.12858898037082547, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0015007908223196864}, {"id": 427, "seek": 164374, "start": 1648.86, "end": 1653.9, "text": " better understanding how the world works. So it turns out that", "tokens": [50620, 1101, 3701, 577, 264, 1002, 1985, 13, 407, 309, 4523, 484, 300, 50872], "temperature": 0.0, "avg_logprob": -0.12858898037082547, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0015007908223196864}, {"id": 428, "seek": 164374, "start": 1653.9, "end": 1659.22, "text": " in the G flow net framework, you, you have a training objective", "tokens": [50872, 294, 264, 460, 3095, 2533, 8388, 11, 291, 11, 291, 362, 257, 3097, 10024, 51138], "temperature": 0.0, "avg_logprob": -0.12858898037082547, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0015007908223196864}, {"id": 429, "seek": 164374, "start": 1659.22, "end": 1663.42, "text": " that yields this kind of diversity and exploration, but is", "tokens": [51138, 300, 32168, 341, 733, 295, 8811, 293, 16197, 11, 457, 307, 51348], "temperature": 0.0, "avg_logprob": -0.12858898037082547, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0015007908223196864}, {"id": 430, "seek": 164374, "start": 1663.42, "end": 1667.22, "text": " based on training large neural nets end to end. Now, it's a bit", "tokens": [51348, 2361, 322, 3097, 2416, 18161, 36170, 917, 281, 917, 13, 823, 11, 309, 311, 257, 857, 51538], "temperature": 0.0, "avg_logprob": -0.12858898037082547, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0015007908223196864}, {"id": 431, "seek": 164374, "start": 1667.22, "end": 1672.26, "text": " different from the usual end to end training, because we don't", "tokens": [51538, 819, 490, 264, 7713, 917, 281, 917, 3097, 11, 570, 321, 500, 380, 51790], "temperature": 0.0, "avg_logprob": -0.12858898037082547, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0015007908223196864}, {"id": 432, "seek": 167226, "start": 1672.26, "end": 1675.34, "text": " have an objective that objective we're trying to optimize", "tokens": [50364, 362, 364, 10024, 300, 10024, 321, 434, 1382, 281, 19719, 50518], "temperature": 0.0, "avg_logprob": -0.16456205820299916, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0032701846212148666}, {"id": 433, "seek": 167226, "start": 1675.34, "end": 1680.66, "text": " is not tractable, actually. But we can sample these trajectories,", "tokens": [50518, 307, 406, 24207, 712, 11, 767, 13, 583, 321, 393, 6889, 613, 18257, 2083, 11, 50784], "temperature": 0.0, "avg_logprob": -0.16456205820299916, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0032701846212148666}, {"id": 434, "seek": 167226, "start": 1680.66, "end": 1684.22, "text": " which I think of like sampling thoughts, like our thought", "tokens": [50784, 597, 286, 519, 295, 411, 21179, 4598, 11, 411, 527, 1194, 50962], "temperature": 0.0, "avg_logprob": -0.16456205820299916, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0032701846212148666}, {"id": 435, "seek": 167226, "start": 1684.22, "end": 1687.82, "text": " process is going through some chain of explanations, not a", "tokens": [50962, 1399, 307, 516, 807, 512, 5021, 295, 28708, 11, 406, 257, 51142], "temperature": 0.0, "avg_logprob": -0.16456205820299916, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0032701846212148666}, {"id": 436, "seek": 167226, "start": 1687.82, "end": 1690.62, "text": " complete, and it doesn't represent all the explanations,", "tokens": [51142, 3566, 11, 293, 309, 1177, 380, 2906, 439, 264, 28708, 11, 51282], "temperature": 0.0, "avg_logprob": -0.16456205820299916, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0032701846212148666}, {"id": 437, "seek": 167226, "start": 1690.62, "end": 1694.3799999999999, "text": " but but what we found with our training objectives for G", "tokens": [51282, 457, 457, 437, 321, 1352, 365, 527, 3097, 15961, 337, 460, 51470], "temperature": 0.0, "avg_logprob": -0.16456205820299916, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0032701846212148666}, {"id": 438, "seek": 167226, "start": 1694.3799999999999, "end": 1699.26, "text": " flow nets is that these sort of random randomized kind of views", "tokens": [51470, 3095, 36170, 307, 300, 613, 1333, 295, 4974, 38513, 733, 295, 6809, 51714], "temperature": 0.0, "avg_logprob": -0.16456205820299916, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.0032701846212148666}, {"id": 439, "seek": 169926, "start": 1699.26, "end": 1703.1, "text": " of the world are sufficient to give a training signal to the", "tokens": [50364, 295, 264, 1002, 366, 11563, 281, 976, 257, 3097, 6358, 281, 264, 50556], "temperature": 0.0, "avg_logprob": -0.19112308352601295, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.0004728078201878816}, {"id": 440, "seek": 169926, "start": 1703.1, "end": 1705.3, "text": " neural nets that do the real job.", "tokens": [50556, 18161, 36170, 300, 360, 264, 957, 1691, 13, 50666], "temperature": 0.0, "avg_logprob": -0.19112308352601295, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.0004728078201878816}, {"id": 441, "seek": 169926, "start": 1706.3, "end": 1710.46, "text": " I'm curious. So this trade off between exploration versus", "tokens": [50716, 286, 478, 6369, 13, 407, 341, 4923, 766, 1296, 16197, 5717, 50924], "temperature": 0.0, "avg_logprob": -0.19112308352601295, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.0004728078201878816}, {"id": 442, "seek": 169926, "start": 1710.5, "end": 1714.82, "text": " exploitation. And this has come up in so many contexts, you know,", "tokens": [50926, 33122, 13, 400, 341, 575, 808, 493, 294, 370, 867, 30628, 11, 291, 458, 11, 51142], "temperature": 0.0, "avg_logprob": -0.19112308352601295, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.0004728078201878816}, {"id": 443, "seek": 169926, "start": 1714.82, "end": 1717.58, "text": " throughout our show. And one in particular, as we talked to, you", "tokens": [51142, 3710, 527, 855, 13, 400, 472, 294, 1729, 11, 382, 321, 2825, 281, 11, 291, 51280], "temperature": 0.0, "avg_logprob": -0.19112308352601295, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.0004728078201878816}, {"id": 444, "seek": 169926, "start": 1717.58, "end": 1722.1, "text": " know, we've talked to multi arm banded folks, right? And G flow", "tokens": [51280, 458, 11, 321, 600, 2825, 281, 4825, 3726, 4116, 292, 4024, 11, 558, 30, 400, 460, 3095, 51506], "temperature": 0.0, "avg_logprob": -0.19112308352601295, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.0004728078201878816}, {"id": 445, "seek": 169926, "start": 1722.1, "end": 1725.74, "text": " net seemed to capture this balance between exploration", "tokens": [51506, 2533, 6576, 281, 7983, 341, 4772, 1296, 16197, 51688], "temperature": 0.0, "avg_logprob": -0.19112308352601295, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.0004728078201878816}, {"id": 446, "seek": 172574, "start": 1725.74, "end": 1729.74, "text": " and exploitation. But the multi arm banded folks, you know, they", "tokens": [50364, 293, 33122, 13, 583, 264, 4825, 3726, 4116, 292, 4024, 11, 291, 458, 11, 436, 50564], "temperature": 0.0, "avg_logprob": -0.10555786245009478, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.0021826080046594143}, {"id": 447, "seek": 172574, "start": 1729.74, "end": 1733.7, "text": " dive deep in that research circle into this into this trade off.", "tokens": [50564, 9192, 2452, 294, 300, 2132, 6329, 666, 341, 666, 341, 4923, 766, 13, 50762], "temperature": 0.0, "avg_logprob": -0.10555786245009478, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.0021826080046594143}, {"id": 448, "seek": 172574, "start": 1733.7, "end": 1737.94, "text": " And I think they have some very principled ways and even very", "tokens": [50762, 400, 286, 519, 436, 362, 512, 588, 3681, 15551, 2098, 293, 754, 588, 50974], "temperature": 0.0, "avg_logprob": -0.10555786245009478, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.0021826080046594143}, {"id": 449, "seek": 172574, "start": 1737.94, "end": 1742.42, "text": " rigorous ways to analyze this fundamental trade off. To what", "tokens": [50974, 29882, 2098, 281, 12477, 341, 8088, 4923, 766, 13, 1407, 437, 51198], "temperature": 0.0, "avg_logprob": -0.10555786245009478, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.0021826080046594143}, {"id": 450, "seek": 172574, "start": 1742.42, "end": 1745.6200000000001, "text": " extent do you think that that their research maybe could be", "tokens": [51198, 8396, 360, 291, 519, 300, 300, 641, 2132, 1310, 727, 312, 51358], "temperature": 0.0, "avg_logprob": -0.10555786245009478, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.0021826080046594143}, {"id": 451, "seek": 172574, "start": 1745.6200000000001, "end": 1750.1, "text": " applied to future G flow net variations? Like do you think", "tokens": [51358, 6456, 281, 2027, 460, 3095, 2533, 17840, 30, 1743, 360, 291, 519, 51582], "temperature": 0.0, "avg_logprob": -0.10555786245009478, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.0021826080046594143}, {"id": 452, "seek": 172574, "start": 1750.1, "end": 1754.06, "text": " maybe it might open up more options to fine tune the", "tokens": [51582, 1310, 309, 1062, 1269, 493, 544, 3956, 281, 2489, 10864, 264, 51780], "temperature": 0.0, "avg_logprob": -0.10555786245009478, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.0021826080046594143}, {"id": 453, "seek": 175406, "start": 1754.06, "end": 1756.46, "text": " trade off between exploration and exploitation?", "tokens": [50364, 4923, 766, 1296, 16197, 293, 33122, 30, 50484], "temperature": 0.0, "avg_logprob": -0.1608122748297614, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0012252641608938575}, {"id": 454, "seek": 175406, "start": 1757.46, "end": 1762.3, "text": " Yeah, I mean, the banded research is very, very closely", "tokens": [50534, 865, 11, 286, 914, 11, 264, 4116, 292, 2132, 307, 588, 11, 588, 8185, 50776], "temperature": 0.0, "avg_logprob": -0.1608122748297614, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0012252641608938575}, {"id": 455, "seek": 175406, "start": 1762.3, "end": 1767.26, "text": " related to the G flow net thread. But G flow nets, as we have", "tokens": [50776, 4077, 281, 264, 460, 3095, 2533, 7207, 13, 583, 460, 3095, 36170, 11, 382, 321, 362, 51024], "temperature": 0.0, "avg_logprob": -0.1608122748297614, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0012252641608938575}, {"id": 456, "seek": 175406, "start": 1767.26, "end": 1770.86, "text": " been using them, for example, for drug discovery, they are", "tokens": [51024, 668, 1228, 552, 11, 337, 1365, 11, 337, 4110, 12114, 11, 436, 366, 51204], "temperature": 0.0, "avg_logprob": -0.1608122748297614, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0012252641608938575}, {"id": 457, "seek": 175406, "start": 1770.86, "end": 1775.3799999999999, "text": " banded. It's just that the action space is not, you know, one", "tokens": [51204, 4116, 292, 13, 467, 311, 445, 300, 264, 3069, 1901, 307, 406, 11, 291, 458, 11, 472, 51430], "temperature": 0.0, "avg_logprob": -0.1608122748297614, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0012252641608938575}, {"id": 458, "seek": 175406, "start": 1775.3799999999999, "end": 1780.54, "text": " out of n things. It's, it's combinatorial because you build", "tokens": [51430, 484, 295, 297, 721, 13, 467, 311, 11, 309, 311, 2512, 31927, 831, 570, 291, 1322, 51688], "temperature": 0.0, "avg_logprob": -0.1608122748297614, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0012252641608938575}, {"id": 459, "seek": 175406, "start": 1780.54, "end": 1783.86, "text": " these pieces. So the action space is not something you can", "tokens": [51688, 613, 3755, 13, 407, 264, 3069, 1901, 307, 406, 746, 291, 393, 51854], "temperature": 0.0, "avg_logprob": -0.1608122748297614, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0012252641608938575}, {"id": 460, "seek": 178386, "start": 1784.02, "end": 1788.06, "text": " enumerate. So you can't apply the typical banded algorithms, but", "tokens": [50372, 465, 15583, 473, 13, 407, 291, 393, 380, 3079, 264, 7476, 4116, 292, 14642, 11, 457, 50574], "temperature": 0.0, "avg_logprob": -0.1603059321641922, "compression_ratio": 1.3966480446927374, "no_speech_prob": 0.002358552999794483}, {"id": 461, "seek": 178386, "start": 1788.06, "end": 1792.3, "text": " a lot of the math is totally applicable. And in fact, what we", "tokens": [50574, 257, 688, 295, 264, 5221, 307, 3879, 21142, 13, 400, 294, 1186, 11, 437, 321, 50786], "temperature": 0.0, "avg_logprob": -0.1603059321641922, "compression_ratio": 1.3966480446927374, "no_speech_prob": 0.002358552999794483}, {"id": 462, "seek": 178386, "start": 1792.3, "end": 1799.26, "text": " use in the drug discovery setting is UCB upper confidence", "tokens": [50786, 764, 294, 264, 4110, 12114, 3287, 307, 14079, 33, 6597, 6687, 51134], "temperature": 0.0, "avg_logprob": -0.1603059321641922, "compression_ratio": 1.3966480446927374, "no_speech_prob": 0.002358552999794483}, {"id": 463, "seek": 178386, "start": 1799.26, "end": 1810.82, "text": " bound objective to learn a good exploration policy. So that comes", "tokens": [51134, 5472, 10024, 281, 1466, 257, 665, 16197, 3897, 13, 407, 300, 1487, 51712], "temperature": 0.0, "avg_logprob": -0.1603059321641922, "compression_ratio": 1.3966480446927374, "no_speech_prob": 0.002358552999794483}, {"id": 464, "seek": 181082, "start": 1810.86, "end": 1816.02, "text": " out of the banded research. It what it does is it, you know, it", "tokens": [50366, 484, 295, 264, 4116, 292, 2132, 13, 467, 437, 309, 775, 307, 309, 11, 291, 458, 11, 309, 50624], "temperature": 0.0, "avg_logprob": -0.2064725212428881, "compression_ratio": 1.6859903381642511, "no_speech_prob": 0.05250749737024307}, {"id": 465, "seek": 181082, "start": 1816.02, "end": 1821.98, "text": " combines the risk and reward expected reward, one of these", "tokens": [50624, 29520, 264, 3148, 293, 7782, 5176, 7782, 11, 472, 295, 613, 50922], "temperature": 0.0, "avg_logprob": -0.2064725212428881, "compression_ratio": 1.6859903381642511, "no_speech_prob": 0.05250749737024307}, {"id": 466, "seek": 181082, "start": 1821.98, "end": 1827.1, "text": " together in a way that in theory guarantees that you will do an", "tokens": [50922, 1214, 294, 257, 636, 300, 294, 5261, 32567, 300, 291, 486, 360, 364, 51178], "temperature": 0.0, "avg_logprob": -0.2064725212428881, "compression_ratio": 1.6859903381642511, "no_speech_prob": 0.05250749737024307}, {"id": 467, "seek": 181082, "start": 1827.1, "end": 1831.74, "text": " efficient exploration and find that where is the, you know,", "tokens": [51178, 7148, 16197, 293, 915, 300, 689, 307, 264, 11, 291, 458, 11, 51410], "temperature": 0.0, "avg_logprob": -0.2064725212428881, "compression_ratio": 1.6859903381642511, "no_speech_prob": 0.05250749737024307}, {"id": 468, "seek": 181082, "start": 1832.1399999999999, "end": 1835.62, "text": " where's the money? Where's the reward, right? All of the", "tokens": [51430, 689, 311, 264, 1460, 30, 2305, 311, 264, 7782, 11, 558, 30, 1057, 295, 264, 51604], "temperature": 0.0, "avg_logprob": -0.2064725212428881, "compression_ratio": 1.6859903381642511, "no_speech_prob": 0.05250749737024307}, {"id": 469, "seek": 181082, "start": 1835.62, "end": 1837.6599999999999, "text": " possible places where you can get the reward.", "tokens": [51604, 1944, 3190, 689, 291, 393, 483, 264, 7782, 13, 51706], "temperature": 0.0, "avg_logprob": -0.2064725212428881, "compression_ratio": 1.6859903381642511, "no_speech_prob": 0.05250749737024307}, {"id": 470, "seek": 183766, "start": 1838.14, "end": 1843.66, "text": " So in, in, in the G flow net papers, you often describe it as, you", "tokens": [50388, 407, 294, 11, 294, 11, 294, 264, 460, 3095, 2533, 10577, 11, 291, 2049, 6786, 309, 382, 11, 291, 50664], "temperature": 0.0, "avg_logprob": -0.12273829054124284, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.011321031488478184}, {"id": 471, "seek": 183766, "start": 1843.66, "end": 1848.18, "text": " know, we want to sample not only the maximum reward path, in", "tokens": [50664, 458, 11, 321, 528, 281, 6889, 406, 787, 264, 6674, 7782, 3100, 11, 294, 50890], "temperature": 0.0, "avg_logprob": -0.12273829054124284, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.011321031488478184}, {"id": 472, "seek": 183766, "start": 1848.18, "end": 1851.7, "text": " order to have more diversity in order to maybe figure out", "tokens": [50890, 1668, 281, 362, 544, 8811, 294, 1668, 281, 1310, 2573, 484, 51066], "temperature": 0.0, "avg_logprob": -0.12273829054124284, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.011321031488478184}, {"id": 473, "seek": 183766, "start": 1851.7, "end": 1855.26, "text": " something that we didn't know if we were just to go to the", "tokens": [51066, 746, 300, 321, 994, 380, 458, 498, 321, 645, 445, 281, 352, 281, 264, 51244], "temperature": 0.0, "avg_logprob": -0.12273829054124284, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.011321031488478184}, {"id": 474, "seek": 183766, "start": 1855.26, "end": 1858.74, "text": " maximum reward. And that speaks a little bit to the, like the", "tokens": [51244, 6674, 7782, 13, 400, 300, 10789, 257, 707, 857, 281, 264, 11, 411, 264, 51418], "temperature": 0.0, "avg_logprob": -0.12273829054124284, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.011321031488478184}, {"id": 475, "seek": 183766, "start": 1858.74, "end": 1863.8200000000002, "text": " things that we know that we don't know, right? We maybe know", "tokens": [51418, 721, 300, 321, 458, 300, 321, 500, 380, 458, 11, 558, 30, 492, 1310, 458, 51672], "temperature": 0.0, "avg_logprob": -0.12273829054124284, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.011321031488478184}, {"id": 476, "seek": 186382, "start": 1863.82, "end": 1868.1799999999998, "text": " that, right, this seems like a lower reward trajectory might", "tokens": [50364, 300, 11, 558, 11, 341, 2544, 411, 257, 3126, 7782, 21512, 1062, 50582], "temperature": 0.0, "avg_logprob": -0.13463801028681735, "compression_ratio": 1.7550200803212852, "no_speech_prob": 0.024037078022956848}, {"id": 477, "seek": 186382, "start": 1868.1799999999998, "end": 1872.22, "text": " turn out to be a higher reward trajectory. However, exploration", "tokens": [50582, 1261, 484, 281, 312, 257, 2946, 7782, 21512, 13, 2908, 11, 16197, 50784], "temperature": 0.0, "avg_logprob": -0.13463801028681735, "compression_ratio": 1.7550200803212852, "no_speech_prob": 0.024037078022956848}, {"id": 478, "seek": 186382, "start": 1872.22, "end": 1874.8999999999999, "text": " and reinforcement learning is also fundamentally addressing the", "tokens": [50784, 293, 29280, 2539, 307, 611, 17879, 14329, 264, 50918], "temperature": 0.0, "avg_logprob": -0.13463801028681735, "compression_ratio": 1.7550200803212852, "no_speech_prob": 0.024037078022956848}, {"id": 479, "seek": 186382, "start": 1874.8999999999999, "end": 1878.86, "text": " things about the things that I don't know that I don't know,", "tokens": [50918, 721, 466, 264, 721, 300, 286, 500, 380, 458, 300, 286, 500, 380, 458, 11, 51116], "temperature": 0.0, "avg_logprob": -0.13463801028681735, "compression_ratio": 1.7550200803212852, "no_speech_prob": 0.024037078022956848}, {"id": 480, "seek": 186382, "start": 1878.9399999999998, "end": 1883.1, "text": " which is where stuff like random exploration and things like", "tokens": [51120, 597, 307, 689, 1507, 411, 4974, 16197, 293, 721, 411, 51328], "temperature": 0.0, "avg_logprob": -0.13463801028681735, "compression_ratio": 1.7550200803212852, "no_speech_prob": 0.024037078022956848}, {"id": 481, "seek": 186382, "start": 1883.1, "end": 1886.98, "text": " this comes in. Could you maybe comment a little bit on how you", "tokens": [51328, 341, 1487, 294, 13, 7497, 291, 1310, 2871, 257, 707, 857, 322, 577, 291, 51522], "temperature": 0.0, "avg_logprob": -0.13463801028681735, "compression_ratio": 1.7550200803212852, "no_speech_prob": 0.024037078022956848}, {"id": 482, "seek": 186382, "start": 1886.98, "end": 1891.3, "text": " see sort of, because it seems to me that if I managed to sample", "tokens": [51522, 536, 1333, 295, 11, 570, 309, 2544, 281, 385, 300, 498, 286, 6453, 281, 6889, 51738], "temperature": 0.0, "avg_logprob": -0.13463801028681735, "compression_ratio": 1.7550200803212852, "no_speech_prob": 0.024037078022956848}, {"id": 483, "seek": 189130, "start": 1891.3, "end": 1895.7, "text": " according to what I think is the reward distribution, right, I", "tokens": [50364, 4650, 281, 437, 286, 519, 307, 264, 7782, 7316, 11, 558, 11, 286, 50584], "temperature": 0.0, "avg_logprob": -0.09688372413317363, "compression_ratio": 1.625, "no_speech_prob": 0.0012836508685722947}, {"id": 484, "seek": 189130, "start": 1895.7, "end": 1899.22, "text": " still have this problem of maybe there is a deceptive rewards", "tokens": [50584, 920, 362, 341, 1154, 295, 1310, 456, 307, 257, 368, 1336, 488, 17203, 50760], "temperature": 0.0, "avg_logprob": -0.09688372413317363, "compression_ratio": 1.625, "no_speech_prob": 0.0012836508685722947}, {"id": 485, "seek": 189130, "start": 1899.22, "end": 1902.78, "text": " there are, you know, I need to take a step back, I may not know", "tokens": [50760, 456, 366, 11, 291, 458, 11, 286, 643, 281, 747, 257, 1823, 646, 11, 286, 815, 406, 458, 50938], "temperature": 0.0, "avg_logprob": -0.09688372413317363, "compression_ratio": 1.625, "no_speech_prob": 0.0012836508685722947}, {"id": 486, "seek": 189130, "start": 1902.78, "end": 1906.82, "text": " some sort of some, some area of the search space. And don't I", "tokens": [50938, 512, 1333, 295, 512, 11, 512, 1859, 295, 264, 3164, 1901, 13, 400, 500, 380, 286, 51140], "temperature": 0.0, "avg_logprob": -0.09688372413317363, "compression_ratio": 1.625, "no_speech_prob": 0.0012836508685722947}, {"id": 487, "seek": 189130, "start": 1906.82, "end": 1908.98, "text": " just run into the same problems again?", "tokens": [51140, 445, 1190, 666, 264, 912, 2740, 797, 30, 51248], "temperature": 0.0, "avg_logprob": -0.09688372413317363, "compression_ratio": 1.625, "no_speech_prob": 0.0012836508685722947}, {"id": 488, "seek": 189130, "start": 1910.34, "end": 1916.7, "text": " So, so the important trick here is you need your model of the", "tokens": [51316, 407, 11, 370, 264, 1021, 4282, 510, 307, 291, 643, 428, 2316, 295, 264, 51634], "temperature": 0.0, "avg_logprob": -0.09688372413317363, "compression_ratio": 1.625, "no_speech_prob": 0.0012836508685722947}, {"id": 489, "seek": 191670, "start": 1916.7, "end": 1922.3, "text": " reward distribution, or the reward function to be one that", "tokens": [50364, 7782, 7316, 11, 420, 264, 7782, 2445, 281, 312, 472, 300, 50644], "temperature": 0.0, "avg_logprob": -0.15209547987262023, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0033234276343137026}, {"id": 490, "seek": 191670, "start": 1922.3, "end": 1925.7, "text": " captures uncertainty, like, maybe in a Bayesian way, or, you", "tokens": [50644, 27986, 15697, 11, 411, 11, 1310, 294, 257, 7840, 42434, 636, 11, 420, 11, 291, 50814], "temperature": 0.0, "avg_logprob": -0.15209547987262023, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0033234276343137026}, {"id": 491, "seek": 191670, "start": 1925.7, "end": 1929.54, "text": " know, whichever way, the Bayesian way, by the way, fits well", "tokens": [50814, 458, 11, 24123, 636, 11, 264, 7840, 42434, 636, 11, 538, 264, 636, 11, 9001, 731, 51006], "temperature": 0.0, "avg_logprob": -0.15209547987262023, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0033234276343137026}, {"id": 492, "seek": 191670, "start": 1929.54, "end": 1935.66, "text": " with the G flow net framework, because we can consider the", "tokens": [51006, 365, 264, 460, 3095, 2533, 8388, 11, 570, 321, 393, 1949, 264, 51312], "temperature": 0.0, "avg_logprob": -0.15209547987262023, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0033234276343137026}, {"id": 493, "seek": 191670, "start": 1935.94, "end": 1940.38, "text": " parameters of the reward function as latent variables, like", "tokens": [51326, 9834, 295, 264, 7782, 2445, 382, 48994, 9102, 11, 411, 51548], "temperature": 0.0, "avg_logprob": -0.15209547987262023, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0033234276343137026}, {"id": 494, "seek": 191670, "start": 1940.38, "end": 1942.5, "text": " you don't actually know the reward function, you're trying", "tokens": [51548, 291, 500, 380, 767, 458, 264, 7782, 2445, 11, 291, 434, 1382, 51654], "temperature": 0.0, "avg_logprob": -0.15209547987262023, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0033234276343137026}, {"id": 495, "seek": 191670, "start": 1942.5, "end": 1945.5, "text": " to figure it out from experiments. So the G flow", "tokens": [51654, 281, 2573, 309, 484, 490, 12050, 13, 407, 264, 460, 3095, 51804], "temperature": 0.0, "avg_logprob": -0.15209547987262023, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0033234276343137026}, {"id": 496, "seek": 194550, "start": 1945.5, "end": 1952.14, "text": " net can sample, and not just like what you should be doing in", "tokens": [50364, 2533, 393, 6889, 11, 293, 406, 445, 411, 437, 291, 820, 312, 884, 294, 50696], "temperature": 0.0, "avg_logprob": -0.1540670394897461, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.000362455437425524}, {"id": 497, "seek": 194550, "start": 1952.14, "end": 1957.54, "text": " order to acquire information, but also potential reward function.", "tokens": [50696, 1668, 281, 20001, 1589, 11, 457, 611, 3995, 7782, 2445, 13, 50966], "temperature": 0.0, "avg_logprob": -0.1540670394897461, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.000362455437425524}, {"id": 498, "seek": 194550, "start": 1957.54, "end": 1961.74, "text": " So, you know, we don't actually have a knowledge of how the, you", "tokens": [50966, 407, 11, 291, 458, 11, 321, 500, 380, 767, 362, 257, 3601, 295, 577, 264, 11, 291, 51176], "temperature": 0.0, "avg_logprob": -0.1540670394897461, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.000362455437425524}, {"id": 499, "seek": 194550, "start": 1961.74, "end": 1963.22, "text": " know, what's going to be the rewards we're going to get in", "tokens": [51176, 458, 11, 437, 311, 516, 281, 312, 264, 17203, 321, 434, 516, 281, 483, 294, 51250], "temperature": 0.0, "avg_logprob": -0.1540670394897461, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.000362455437425524}, {"id": 500, "seek": 194550, "start": 1963.22, "end": 1968.38, "text": " the world. Classical IRL is going, as you said, to the expected", "tokens": [51250, 264, 1002, 13, 9471, 804, 286, 10740, 307, 516, 11, 382, 291, 848, 11, 281, 264, 5176, 51508], "temperature": 0.0, "avg_logprob": -0.1540670394897461, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.000362455437425524}, {"id": 501, "seek": 194550, "start": 1968.38, "end": 1972.82, "text": " value and try to maximize that, whereas the G flow net approach", "tokens": [51508, 2158, 293, 853, 281, 19874, 300, 11, 9735, 264, 460, 3095, 2533, 3109, 51730], "temperature": 0.0, "avg_logprob": -0.1540670394897461, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.000362455437425524}, {"id": 502, "seek": 197282, "start": 1972.86, "end": 1976.4199999999998, "text": " is trying to acquire as much knowledge as possible about the", "tokens": [50366, 307, 1382, 281, 20001, 382, 709, 3601, 382, 1944, 466, 264, 50544], "temperature": 0.0, "avg_logprob": -0.1373782469847492, "compression_ratio": 1.9039301310043668, "no_speech_prob": 0.00279967300593853}, {"id": 503, "seek": 197282, "start": 1976.4199999999998, "end": 1979.3799999999999, "text": " underlying reward function. So you're trying to minimize the", "tokens": [50544, 14217, 7782, 2445, 13, 407, 291, 434, 1382, 281, 17522, 264, 50692], "temperature": 0.0, "avg_logprob": -0.1373782469847492, "compression_ratio": 1.9039301310043668, "no_speech_prob": 0.00279967300593853}, {"id": 504, "seek": 197282, "start": 1979.3799999999999, "end": 1984.86, "text": " uncertainty. So your model with the G flow net is modeling the", "tokens": [50692, 15697, 13, 407, 428, 2316, 365, 264, 460, 3095, 2533, 307, 15983, 264, 50966], "temperature": 0.0, "avg_logprob": -0.1373782469847492, "compression_ratio": 1.9039301310043668, "no_speech_prob": 0.00279967300593853}, {"id": 505, "seek": 197282, "start": 1984.86, "end": 1991.46, "text": " uncertainty, and then it can use it as a reward for the policy", "tokens": [50966, 15697, 11, 293, 550, 309, 393, 764, 309, 382, 257, 7782, 337, 264, 3897, 51296], "temperature": 0.0, "avg_logprob": -0.1373782469847492, "compression_ratio": 1.9039301310043668, "no_speech_prob": 0.00279967300593853}, {"id": 506, "seek": 197282, "start": 1991.46, "end": 1994.6599999999999, "text": " that is going to do action in the real world. So we're talking", "tokens": [51296, 300, 307, 516, 281, 360, 3069, 294, 264, 957, 1002, 13, 407, 321, 434, 1417, 51456], "temperature": 0.0, "avg_logprob": -0.1373782469847492, "compression_ratio": 1.9039301310043668, "no_speech_prob": 0.00279967300593853}, {"id": 507, "seek": 197282, "start": 1994.6599999999999, "end": 1997.46, "text": " about different G flow nets. There's a G flow net that models", "tokens": [51456, 466, 819, 460, 3095, 36170, 13, 821, 311, 257, 460, 3095, 2533, 300, 5245, 51596], "temperature": 0.0, "avg_logprob": -0.1373782469847492, "compression_ratio": 1.9039301310043668, "no_speech_prob": 0.00279967300593853}, {"id": 508, "seek": 197282, "start": 1997.46, "end": 2000.4199999999998, "text": " the uncertainty in the reward that you're going to get from the", "tokens": [51596, 264, 15697, 294, 264, 7782, 300, 291, 434, 516, 281, 483, 490, 264, 51744], "temperature": 0.0, "avg_logprob": -0.1373782469847492, "compression_ratio": 1.9039301310043668, "no_speech_prob": 0.00279967300593853}, {"id": 509, "seek": 200042, "start": 2000.42, "end": 2004.66, "text": " real world. And that's like a Bayesian model. And then you have", "tokens": [50364, 957, 1002, 13, 400, 300, 311, 411, 257, 7840, 42434, 2316, 13, 400, 550, 291, 362, 50576], "temperature": 0.0, "avg_logprob": -0.13430238021047491, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.0033749055583029985}, {"id": 510, "seek": 200042, "start": 2004.66, "end": 2009.3000000000002, "text": " another G flow net that controls the policy that searches to", "tokens": [50576, 1071, 460, 3095, 2533, 300, 9003, 264, 3897, 300, 26701, 281, 50808], "temperature": 0.0, "avg_logprob": -0.13430238021047491, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.0033749055583029985}, {"id": 511, "seek": 200042, "start": 2009.38, "end": 2013.14, "text": " and its reward is how much uncertainty reduction you're", "tokens": [50812, 293, 1080, 7782, 307, 577, 709, 15697, 11004, 291, 434, 51000], "temperature": 0.0, "avg_logprob": -0.13430238021047491, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.0033749055583029985}, {"id": 512, "seek": 200042, "start": 2013.14, "end": 2020.1000000000001, "text": " going to get by doing this or that. So, so yeah, you need to", "tokens": [51000, 516, 281, 483, 538, 884, 341, 420, 300, 13, 407, 11, 370, 1338, 11, 291, 643, 281, 51348], "temperature": 0.0, "avg_logprob": -0.13430238021047491, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.0033749055583029985}, {"id": 513, "seek": 200042, "start": 2020.1000000000001, "end": 2025.46, "text": " have a part of your model that is kind of aware of the fact", "tokens": [51348, 362, 257, 644, 295, 428, 2316, 300, 307, 733, 295, 3650, 295, 264, 1186, 51616], "temperature": 0.0, "avg_logprob": -0.13430238021047491, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.0033749055583029985}, {"id": 514, "seek": 200042, "start": 2025.46, "end": 2028.0600000000002, "text": " that there are whole areas in the world that you don't know", "tokens": [51616, 300, 456, 366, 1379, 3179, 294, 264, 1002, 300, 291, 500, 380, 458, 51746], "temperature": 0.0, "avg_logprob": -0.13430238021047491, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.0033749055583029985}, {"id": 515, "seek": 202806, "start": 2028.06, "end": 2030.58, "text": " about or aspects of the world that you don't know about so", "tokens": [50364, 466, 420, 7270, 295, 264, 1002, 300, 291, 500, 380, 458, 466, 370, 50490], "temperature": 0.0, "avg_logprob": -0.12507477402687073, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.002179117873311043}, {"id": 516, "seek": 202806, "start": 2030.58, "end": 2032.78, "text": " that you can drive the exploration.", "tokens": [50490, 300, 291, 393, 3332, 264, 16197, 13, 50600], "temperature": 0.0, "avg_logprob": -0.12507477402687073, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.002179117873311043}, {"id": 517, "seek": 202806, "start": 2033.58, "end": 2036.1399999999999, "text": " I would love to know where some of the magic is coming from.", "tokens": [50640, 286, 576, 959, 281, 458, 689, 512, 295, 264, 5585, 307, 1348, 490, 13, 50768], "temperature": 0.0, "avg_logprob": -0.12507477402687073, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.002179117873311043}, {"id": 518, "seek": 202806, "start": 2036.78, "end": 2039.74, "text": " The promise of G flow nets is that we can discover as many", "tokens": [50800, 440, 6228, 295, 460, 3095, 36170, 307, 300, 321, 393, 4411, 382, 867, 50948], "temperature": 0.0, "avg_logprob": -0.12507477402687073, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.002179117873311043}, {"id": 519, "seek": 202806, "start": 2039.74, "end": 2043.4199999999998, "text": " modes as possible in the path distribution. Traditionally in", "tokens": [50948, 14068, 382, 1944, 294, 264, 3100, 7316, 13, 22017, 15899, 294, 51132], "temperature": 0.0, "avg_logprob": -0.12507477402687073, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.002179117873311043}, {"id": 520, "seek": 202806, "start": 2043.54, "end": 2046.6599999999999, "text": " Markov chain Monte Carlo, we had to hack priors into the", "tokens": [51138, 3934, 5179, 5021, 38105, 45112, 11, 321, 632, 281, 10339, 1790, 830, 666, 264, 51294], "temperature": 0.0, "avg_logprob": -0.12507477402687073, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.002179117873311043}, {"id": 521, "seek": 202806, "start": 2046.6599999999999, "end": 2049.38, "text": " algorithm by hand, you know, to find new modes or areas of", "tokens": [51294, 9284, 538, 1011, 11, 291, 458, 11, 281, 915, 777, 14068, 420, 3179, 295, 51430], "temperature": 0.0, "avg_logprob": -0.12507477402687073, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.002179117873311043}, {"id": 522, "seek": 202806, "start": 2049.38, "end": 2051.58, "text": " information efficiently, especially when they were very", "tokens": [51430, 1589, 19621, 11, 2318, 562, 436, 645, 588, 51540], "temperature": 0.0, "avg_logprob": -0.12507477402687073, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.002179117873311043}, {"id": 523, "seek": 202806, "start": 2051.58, "end": 2055.1, "text": " far apart or not very sharp. The hypothesis of G flow nets is", "tokens": [51540, 1400, 4936, 420, 406, 588, 8199, 13, 440, 17291, 295, 460, 3095, 36170, 307, 51716], "temperature": 0.0, "avg_logprob": -0.12507477402687073, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.002179117873311043}, {"id": 524, "seek": 205510, "start": 2055.14, "end": 2058.38, "text": " that the structure of these modes is learnable on many", "tokens": [50366, 300, 264, 3877, 295, 613, 14068, 307, 1466, 712, 322, 867, 50528], "temperature": 0.0, "avg_logprob": -0.11992011483260027, "compression_ratio": 1.6813880126182965, "no_speech_prob": 0.0034270943142473698}, {"id": 525, "seek": 205510, "start": 2058.38, "end": 2061.06, "text": " problems, even in high dimensions. It's a little bit like", "tokens": [50528, 2740, 11, 754, 294, 1090, 12819, 13, 467, 311, 257, 707, 857, 411, 50662], "temperature": 0.0, "avg_logprob": -0.11992011483260027, "compression_ratio": 1.6813880126182965, "no_speech_prob": 0.0034270943142473698}, {"id": 526, "seek": 205510, "start": 2061.06, "end": 2063.1, "text": " saying we're getting a free lunch. I mean, actually, I think", "tokens": [50662, 1566, 321, 434, 1242, 257, 1737, 6349, 13, 286, 914, 11, 767, 11, 286, 519, 50764], "temperature": 0.0, "avg_logprob": -0.11992011483260027, "compression_ratio": 1.6813880126182965, "no_speech_prob": 0.0034270943142473698}, {"id": 527, "seek": 205510, "start": 2063.1, "end": 2065.98, "text": " you used that exact phrase to describe what we're doing here.", "tokens": [50764, 291, 1143, 300, 1900, 9535, 281, 6786, 437, 321, 434, 884, 510, 13, 50908], "temperature": 0.0, "avg_logprob": -0.11992011483260027, "compression_ratio": 1.6813880126182965, "no_speech_prob": 0.0034270943142473698}, {"id": 528, "seek": 205510, "start": 2066.22, "end": 2069.7, "text": " Many research avenues have tried to develop general methods to", "tokens": [50920, 5126, 2132, 43039, 362, 3031, 281, 1499, 2674, 7150, 281, 51094], "temperature": 0.0, "avg_logprob": -0.11992011483260027, "compression_ratio": 1.6813880126182965, "no_speech_prob": 0.0034270943142473698}, {"id": 529, "seek": 205510, "start": 2069.7, "end": 2072.7799999999997, "text": " discover these structures and have failed. How do you think", "tokens": [51094, 4411, 613, 9227, 293, 362, 7612, 13, 1012, 360, 291, 519, 51248], "temperature": 0.0, "avg_logprob": -0.11992011483260027, "compression_ratio": 1.6813880126182965, "no_speech_prob": 0.0034270943142473698}, {"id": 530, "seek": 205510, "start": 2072.7799999999997, "end": 2076.02, "text": " G flow nets will overcome this seemingly intractable curse?", "tokens": [51248, 460, 3095, 36170, 486, 10473, 341, 18709, 560, 1897, 712, 17139, 30, 51410], "temperature": 0.0, "avg_logprob": -0.11992011483260027, "compression_ratio": 1.6813880126182965, "no_speech_prob": 0.0034270943142473698}, {"id": 531, "seek": 205510, "start": 2077.02, "end": 2080.3399999999997, "text": " There is no guarantee that they will, because if there is no", "tokens": [51460, 821, 307, 572, 10815, 300, 436, 486, 11, 570, 498, 456, 307, 572, 51626], "temperature": 0.0, "avg_logprob": -0.11992011483260027, "compression_ratio": 1.6813880126182965, "no_speech_prob": 0.0034270943142473698}, {"id": 532, "seek": 205510, "start": 2080.3399999999997, "end": 2083.74, "text": " structure in the underlying function you're trying to", "tokens": [51626, 3877, 294, 264, 14217, 2445, 291, 434, 1382, 281, 51796], "temperature": 0.0, "avg_logprob": -0.11992011483260027, "compression_ratio": 1.6813880126182965, "no_speech_prob": 0.0034270943142473698}, {"id": 533, "seek": 208374, "start": 2083.74, "end": 2087.02, "text": " discover. So let's say the reward function or the energy", "tokens": [50364, 4411, 13, 407, 718, 311, 584, 264, 7782, 2445, 420, 264, 2281, 50528], "temperature": 0.0, "avg_logprob": -0.15104869660877046, "compression_ratio": 1.6812749003984064, "no_speech_prob": 0.0038227515760809183}, {"id": 534, "seek": 208374, "start": 2087.02, "end": 2092.1, "text": " function that you care about, then having visited some finite", "tokens": [50528, 2445, 300, 291, 1127, 466, 11, 550, 1419, 11220, 512, 19362, 50782], "temperature": 0.0, "avg_logprob": -0.15104869660877046, "compression_ratio": 1.6812749003984064, "no_speech_prob": 0.0038227515760809183}, {"id": 535, "seek": 208374, "start": 2092.1, "end": 2098.22, "text": " number of modes like regions where your reward is high site is", "tokens": [50782, 1230, 295, 14068, 411, 10682, 689, 428, 7782, 307, 1090, 3621, 307, 51088], "temperature": 0.0, "avg_logprob": -0.15104869660877046, "compression_ratio": 1.6812749003984064, "no_speech_prob": 0.0038227515760809183}, {"id": 536, "seek": 208374, "start": 2098.22, "end": 2100.3399999999997, "text": " not going to tell you anything about what are the other good", "tokens": [51088, 406, 516, 281, 980, 291, 1340, 466, 437, 366, 264, 661, 665, 51194], "temperature": 0.0, "avg_logprob": -0.15104869660877046, "compression_ratio": 1.6812749003984064, "no_speech_prob": 0.0038227515760809183}, {"id": 537, "seek": 208374, "start": 2100.3399999999997, "end": 2104.58, "text": " places, the other modes. So so there's no guarantee that it", "tokens": [51194, 3190, 11, 264, 661, 14068, 13, 407, 370, 456, 311, 572, 10815, 300, 309, 51406], "temperature": 0.0, "avg_logprob": -0.15104869660877046, "compression_ratio": 1.6812749003984064, "no_speech_prob": 0.0038227515760809183}, {"id": 538, "seek": 208374, "start": 2104.58, "end": 2109.12, "text": " will work. But if if there is structure, then there is a free", "tokens": [51406, 486, 589, 13, 583, 498, 498, 456, 307, 3877, 11, 550, 456, 307, 257, 1737, 51633], "temperature": 0.0, "avg_logprob": -0.15104869660877046, "compression_ratio": 1.6812749003984064, "no_speech_prob": 0.0038227515760809183}, {"id": 539, "seek": 208374, "start": 2109.12, "end": 2112.7799999999997, "text": " lunch. And we know machine learning is good at that. Like", "tokens": [51633, 6349, 13, 400, 321, 458, 3479, 2539, 307, 665, 412, 300, 13, 1743, 51816], "temperature": 0.0, "avg_logprob": -0.15104869660877046, "compression_ratio": 1.6812749003984064, "no_speech_prob": 0.0038227515760809183}, {"id": 540, "seek": 211278, "start": 2113.34, "end": 2117.5400000000004, "text": " the last 10 years of deep learning and its success. What is it", "tokens": [50392, 264, 1036, 1266, 924, 295, 2452, 2539, 293, 1080, 2245, 13, 708, 307, 309, 50602], "temperature": 0.0, "avg_logprob": -0.17742613320038697, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.002182130701839924}, {"id": 541, "seek": 211278, "start": 2117.5400000000004, "end": 2120.1400000000003, "text": " telling us? It's telling us that you can generalize right that", "tokens": [50602, 3585, 505, 30, 467, 311, 3585, 505, 300, 291, 393, 2674, 1125, 558, 300, 50732], "temperature": 0.0, "avg_logprob": -0.17742613320038697, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.002182130701839924}, {"id": 542, "seek": 211278, "start": 2120.1400000000003, "end": 2123.5400000000004, "text": " these nets, I'm not saying they generalize perfectly, but they", "tokens": [50732, 613, 36170, 11, 286, 478, 406, 1566, 436, 2674, 1125, 6239, 11, 457, 436, 50902], "temperature": 0.0, "avg_logprob": -0.17742613320038697, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.002182130701839924}, {"id": 543, "seek": 211278, "start": 2123.5400000000004, "end": 2128.9, "text": " can generalize. So you can think of it like the machine", "tokens": [50902, 393, 2674, 1125, 13, 407, 291, 393, 519, 295, 309, 411, 264, 3479, 51170], "temperature": 0.0, "avg_logprob": -0.17742613320038697, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.002182130701839924}, {"id": 544, "seek": 211278, "start": 2128.9, "end": 2134.26, "text": " learning problem is given some examples of good things, like,", "tokens": [51170, 2539, 1154, 307, 2212, 512, 5110, 295, 665, 721, 11, 411, 11, 51438], "temperature": 0.0, "avg_logprob": -0.17742613320038697, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.002182130701839924}, {"id": 545, "seek": 211278, "start": 2134.3, "end": 2137.26, "text": " you know, places where you get reward, you can you generalize", "tokens": [51440, 291, 458, 11, 3190, 689, 291, 483, 7782, 11, 291, 393, 291, 2674, 1125, 51588], "temperature": 0.0, "avg_logprob": -0.17742613320038697, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.002182130701839924}, {"id": 546, "seek": 211278, "start": 2137.26, "end": 2141.1800000000003, "text": " to other places. And the supervised learning way of", "tokens": [51588, 281, 661, 3190, 13, 400, 264, 46533, 2539, 636, 295, 51784], "temperature": 0.0, "avg_logprob": -0.17742613320038697, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.002182130701839924}, {"id": 547, "seek": 214118, "start": 2141.18, "end": 2144.06, "text": " thinking about it is, you know, given a candidate place, tell me", "tokens": [50364, 1953, 466, 309, 307, 11, 291, 458, 11, 2212, 257, 11532, 1081, 11, 980, 385, 50508], "temperature": 0.0, "avg_logprob": -0.20711566661966258, "compression_ratio": 1.7054263565891472, "no_speech_prob": 0.004607960116118193}, {"id": 548, "seek": 214118, "start": 2144.06, "end": 2147.74, "text": " how much reward I think I would get. The G for that sampler is", "tokens": [50508, 577, 709, 7782, 286, 519, 286, 576, 483, 13, 440, 460, 337, 300, 3247, 22732, 307, 50692], "temperature": 0.0, "avg_logprob": -0.20711566661966258, "compression_ratio": 1.7054263565891472, "no_speech_prob": 0.004607960116118193}, {"id": 549, "seek": 214118, "start": 2147.74, "end": 2150.8999999999996, "text": " learning the inverse function is like to sample, but it's kind of", "tokens": [50692, 2539, 264, 17340, 2445, 307, 411, 281, 6889, 11, 457, 309, 311, 733, 295, 50850], "temperature": 0.0, "avg_logprob": -0.20711566661966258, "compression_ratio": 1.7054263565891472, "no_speech_prob": 0.004607960116118193}, {"id": 550, "seek": 214118, "start": 2150.8999999999996, "end": 2154.02, "text": " the same thing. It's just going in the other direction. Give me", "tokens": [50850, 264, 912, 551, 13, 467, 311, 445, 516, 294, 264, 661, 3513, 13, 5303, 385, 51006], "temperature": 0.0, "avg_logprob": -0.20711566661966258, "compression_ratio": 1.7054263565891472, "no_speech_prob": 0.004607960116118193}, {"id": 551, "seek": 214118, "start": 2154.02, "end": 2157.7, "text": " some, you know, sample some some good places that that you know,", "tokens": [51006, 512, 11, 291, 458, 11, 6889, 512, 512, 665, 3190, 300, 300, 291, 458, 11, 51190], "temperature": 0.0, "avg_logprob": -0.20711566661966258, "compression_ratio": 1.7054263565891472, "no_speech_prob": 0.004607960116118193}, {"id": 552, "seek": 214118, "start": 2157.7, "end": 2162.52, "text": " where the reward is high. So we now have a lot of experience in", "tokens": [51190, 689, 264, 7782, 307, 1090, 13, 407, 321, 586, 362, 257, 688, 295, 1752, 294, 51431], "temperature": 0.0, "avg_logprob": -0.20711566661966258, "compression_ratio": 1.7054263565891472, "no_speech_prob": 0.004607960116118193}, {"id": 553, "seek": 214118, "start": 2162.52, "end": 2167.7799999999997, "text": " designing powerful your nets that can be leveraged to", "tokens": [51431, 14685, 4005, 428, 36170, 300, 393, 312, 12451, 2980, 281, 51694], "temperature": 0.0, "avg_logprob": -0.20711566661966258, "compression_ratio": 1.7054263565891472, "no_speech_prob": 0.004607960116118193}, {"id": 554, "seek": 216778, "start": 2167.78, "end": 2172.02, "text": " generalize in those spaces where we normally use MCMC. And if", "tokens": [50364, 2674, 1125, 294, 729, 7673, 689, 321, 5646, 764, 8797, 39261, 13, 400, 498, 50576], "temperature": 0.0, "avg_logprob": -0.14005835079452367, "compression_ratio": 1.592, "no_speech_prob": 0.0009696939378045499}, {"id": 555, "seek": 216778, "start": 2172.02, "end": 2177.5, "text": " there is kind of regularities that allow to generalize, then all", "tokens": [50576, 456, 307, 733, 295, 3890, 1088, 300, 2089, 281, 2674, 1125, 11, 550, 439, 50850], "temperature": 0.0, "avg_logprob": -0.14005835079452367, "compression_ratio": 1.592, "no_speech_prob": 0.0009696939378045499}, {"id": 556, "seek": 216778, "start": 2177.5, "end": 2180.3, "text": " of that can be, you know, put to use.", "tokens": [50850, 295, 300, 393, 312, 11, 291, 458, 11, 829, 281, 764, 13, 50990], "temperature": 0.0, "avg_logprob": -0.14005835079452367, "compression_ratio": 1.592, "no_speech_prob": 0.0009696939378045499}, {"id": 557, "seek": 216778, "start": 2181.1000000000004, "end": 2183.82, "text": " We mentioned earlier, reinforcement learning often", "tokens": [51030, 492, 2835, 3071, 11, 29280, 2539, 2049, 51166], "temperature": 0.0, "avg_logprob": -0.14005835079452367, "compression_ratio": 1.592, "no_speech_prob": 0.0009696939378045499}, {"id": 558, "seek": 216778, "start": 2183.82, "end": 2187.26, "text": " being applied in a context where you have this kind of solid", "tokens": [51166, 885, 6456, 294, 257, 4319, 689, 291, 362, 341, 733, 295, 5100, 51338], "temperature": 0.0, "avg_logprob": -0.14005835079452367, "compression_ratio": 1.592, "no_speech_prob": 0.0009696939378045499}, {"id": 559, "seek": 216778, "start": 2187.26, "end": 2190.1000000000004, "text": " reward function. So let's say games, you know, playing chess.", "tokens": [51338, 7782, 2445, 13, 407, 718, 311, 584, 2813, 11, 291, 458, 11, 2433, 24122, 13, 51480], "temperature": 0.0, "avg_logprob": -0.14005835079452367, "compression_ratio": 1.592, "no_speech_prob": 0.0009696939378045499}, {"id": 560, "seek": 216778, "start": 2190.5400000000004, "end": 2193.7000000000003, "text": " I'm really curious, what would happen hypothetically, if we", "tokens": [51502, 286, 478, 534, 6369, 11, 437, 576, 1051, 24371, 22652, 11, 498, 321, 51660], "temperature": 0.0, "avg_logprob": -0.14005835079452367, "compression_ratio": 1.592, "no_speech_prob": 0.0009696939378045499}, {"id": 561, "seek": 219370, "start": 2193.9399999999996, "end": 2199.54, "text": " applied G flow net, you know, to something like chess. So I mean,", "tokens": [50376, 6456, 460, 3095, 2533, 11, 291, 458, 11, 281, 746, 411, 24122, 13, 407, 286, 914, 11, 50656], "temperature": 0.0, "avg_logprob": -0.14006507986842995, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.020960010588169098}, {"id": 562, "seek": 219370, "start": 2199.54, "end": 2202.18, "text": " I think given the fact that reinforcement learning like say", "tokens": [50656, 286, 519, 2212, 264, 1186, 300, 29280, 2539, 411, 584, 50788], "temperature": 0.0, "avg_logprob": -0.14006507986842995, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.020960010588169098}, {"id": 563, "seek": 219370, "start": 2202.18, "end": 2205.2999999999997, "text": " alpha zero is trained specifically to choose the best", "tokens": [50788, 8961, 4018, 307, 8895, 4682, 281, 2826, 264, 1151, 50944], "temperature": 0.0, "avg_logprob": -0.14006507986842995, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.020960010588169098}, {"id": 564, "seek": 219370, "start": 2205.2999999999997, "end": 2209.58, "text": " move rather than diverse moves, it seems obvious that maybe if", "tokens": [50944, 1286, 2831, 813, 9521, 6067, 11, 309, 2544, 6322, 300, 1310, 498, 51158], "temperature": 0.0, "avg_logprob": -0.14006507986842995, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.020960010588169098}, {"id": 565, "seek": 219370, "start": 2209.58, "end": 2213.8599999999997, "text": " given equal resources to both alpha zero and flow zero, alpha", "tokens": [51158, 2212, 2681, 3593, 281, 1293, 8961, 4018, 293, 3095, 4018, 11, 8961, 51372], "temperature": 0.0, "avg_logprob": -0.14006507986842995, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.020960010588169098}, {"id": 566, "seek": 219370, "start": 2213.8599999999997, "end": 2218.06, "text": " zero would probably beat flow zero. However, I think if flow", "tokens": [51372, 4018, 576, 1391, 4224, 3095, 4018, 13, 2908, 11, 286, 519, 498, 3095, 51582], "temperature": 0.0, "avg_logprob": -0.14006507986842995, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.020960010588169098}, {"id": 567, "seek": 219370, "start": 2218.06, "end": 2221.62, "text": " zero were given more resources, say and trained to the same", "tokens": [51582, 4018, 645, 2212, 544, 3593, 11, 584, 293, 8895, 281, 264, 912, 51760], "temperature": 0.0, "avg_logprob": -0.14006507986842995, "compression_ratio": 1.7489711934156378, "no_speech_prob": 0.020960010588169098}, {"id": 568, "seek": 222162, "start": 2221.62, "end": 2225.94, "text": " rating, say the same elo rating as alpha zero, it seems like", "tokens": [50364, 10990, 11, 584, 264, 912, 38682, 10990, 382, 8961, 4018, 11, 309, 2544, 411, 50580], "temperature": 0.0, "avg_logprob": -0.11539872822008634, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.003706936724483967}, {"id": 569, "seek": 222162, "start": 2226.06, "end": 2229.74, "text": " flow zero, if you would, would play significantly more", "tokens": [50586, 3095, 4018, 11, 498, 291, 576, 11, 576, 862, 10591, 544, 50770], "temperature": 0.0, "avg_logprob": -0.11539872822008634, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.003706936724483967}, {"id": 570, "seek": 222162, "start": 2229.74, "end": 2234.46, "text": " diverse and interesting games with a wider variety of styles.", "tokens": [50770, 9521, 293, 1880, 2813, 365, 257, 11842, 5673, 295, 13273, 13, 51006], "temperature": 0.0, "avg_logprob": -0.11539872822008634, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.003706936724483967}, {"id": 571, "seek": 222162, "start": 2234.46, "end": 2237.06, "text": " And I think you could even imagine also that it could be", "tokens": [51006, 400, 286, 519, 291, 727, 754, 3811, 611, 300, 309, 727, 312, 51136], "temperature": 0.0, "avg_logprob": -0.11539872822008634, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.003706936724483967}, {"id": 572, "seek": 222162, "start": 2237.06, "end": 2241.5, "text": " possible, even if given equal resources, but sufficiently", "tokens": [51136, 1944, 11, 754, 498, 2212, 2681, 3593, 11, 457, 31868, 51358], "temperature": 0.0, "avg_logprob": -0.11539872822008634, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.003706936724483967}, {"id": 573, "seek": 222162, "start": 2241.5, "end": 2245.66, "text": " high enough resources, that a hypothetical flow zero would", "tokens": [51358, 1090, 1547, 3593, 11, 300, 257, 33053, 3095, 4018, 576, 51566], "temperature": 0.0, "avg_logprob": -0.11539872822008634, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.003706936724483967}, {"id": 574, "seek": 222162, "start": 2245.66, "end": 2249.2599999999998, "text": " consistently reach higher ratings, because it might find, you", "tokens": [51566, 14961, 2524, 2946, 24603, 11, 570, 309, 1062, 915, 11, 291, 51746], "temperature": 0.0, "avg_logprob": -0.11539872822008634, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.003706936724483967}, {"id": 575, "seek": 224926, "start": 2249.3, "end": 2252.34, "text": " know, more interesting stepping stones that have the", "tokens": [50366, 458, 11, 544, 1880, 16821, 14083, 300, 362, 264, 50518], "temperature": 0.0, "avg_logprob": -0.1705584113980517, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.0007320268778130412}, {"id": 576, "seek": 224926, "start": 2252.34, "end": 2256.34, "text": " potential to avoid deception, because it can explore seemingly", "tokens": [50518, 3995, 281, 5042, 40451, 11, 570, 309, 393, 6839, 18709, 50718], "temperature": 0.0, "avg_logprob": -0.1705584113980517, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.0007320268778130412}, {"id": 577, "seek": 224926, "start": 2256.6200000000003, "end": 2261.0600000000004, "text": " lower reward paths that ultimately develop into higher reward.", "tokens": [50732, 3126, 7782, 14518, 300, 6284, 1499, 666, 2946, 7782, 13, 50954], "temperature": 0.0, "avg_logprob": -0.1705584113980517, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.0007320268778130412}, {"id": 578, "seek": 224926, "start": 2261.1000000000004, "end": 2262.82, "text": " More curious if you have any thoughts on that?", "tokens": [50956, 5048, 6369, 498, 291, 362, 604, 4598, 322, 300, 30, 51042], "temperature": 0.0, "avg_logprob": -0.1705584113980517, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.0007320268778130412}, {"id": 579, "seek": 224926, "start": 2263.78, "end": 2271.1000000000004, "text": " Yeah. It's a good question. I would say where the kind of", "tokens": [51090, 865, 13, 467, 311, 257, 665, 1168, 13, 286, 576, 584, 689, 264, 733, 295, 51456], "temperature": 0.0, "avg_logprob": -0.1705584113980517, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.0007320268778130412}, {"id": 580, "seek": 224926, "start": 2271.1000000000004, "end": 2275.0200000000004, "text": " approach we've been pioneering with G flow nets might be really", "tokens": [51456, 3109, 321, 600, 668, 19761, 1794, 365, 460, 3095, 36170, 1062, 312, 534, 51652], "temperature": 0.0, "avg_logprob": -0.1705584113980517, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.0007320268778130412}, {"id": 581, "seek": 227502, "start": 2275.02, "end": 2279.86, "text": " paying off is if you think about it from the perspective of the", "tokens": [50364, 6229, 766, 307, 498, 291, 519, 466, 309, 490, 264, 4585, 295, 264, 50606], "temperature": 0.0, "avg_logprob": -0.13404417037963867, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.007932931184768677}, {"id": 582, "seek": 227502, "start": 2279.86, "end": 2283.34, "text": " learner has a finite computational, you know, amount of", "tokens": [50606, 33347, 575, 257, 19362, 28270, 11, 291, 458, 11, 2372, 295, 50780], "temperature": 0.0, "avg_logprob": -0.13404417037963867, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.007932931184768677}, {"id": 583, "seek": 227502, "start": 2283.34, "end": 2287.7, "text": " resources, because in principle, right, if you had infinite", "tokens": [50780, 3593, 11, 570, 294, 8665, 11, 558, 11, 498, 291, 632, 13785, 50998], "temperature": 0.0, "avg_logprob": -0.13404417037963867, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.007932931184768677}, {"id": 584, "seek": 227502, "start": 2287.7, "end": 2291.94, "text": " compute, and you know the reward function, like the rules of", "tokens": [50998, 14722, 11, 293, 291, 458, 264, 7782, 2445, 11, 411, 264, 4474, 295, 51210], "temperature": 0.0, "avg_logprob": -0.13404417037963867, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.007932931184768677}, {"id": 585, "seek": 227502, "start": 2291.94, "end": 2296.62, "text": " chess or go, then you can just crank and find, you know, the", "tokens": [51210, 24122, 420, 352, 11, 550, 291, 393, 445, 21263, 293, 915, 11, 291, 458, 11, 264, 51444], "temperature": 0.0, "avg_logprob": -0.13404417037963867, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.007932931184768677}, {"id": 586, "seek": 227502, "start": 2296.62, "end": 2301.78, "text": " policy that's best in every possible setting. Now, if you", "tokens": [51444, 3897, 300, 311, 1151, 294, 633, 1944, 3287, 13, 823, 11, 498, 291, 51702], "temperature": 0.0, "avg_logprob": -0.13404417037963867, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.007932931184768677}, {"id": 587, "seek": 230178, "start": 2301.78, "end": 2304.78, "text": " have finite resources, like, you know, you, you have a budget", "tokens": [50364, 362, 19362, 3593, 11, 411, 11, 291, 458, 11, 291, 11, 291, 362, 257, 4706, 50514], "temperature": 0.0, "avg_logprob": -0.14092962047721766, "compression_ratio": 1.6, "no_speech_prob": 0.0014547083992511034}, {"id": 588, "seek": 230178, "start": 2304.78, "end": 2310.78, "text": " of compute, you'd like to use it efficiently. And so that's", "tokens": [50514, 295, 14722, 11, 291, 1116, 411, 281, 764, 309, 19621, 13, 400, 370, 300, 311, 50814], "temperature": 0.0, "avg_logprob": -0.14092962047721766, "compression_ratio": 1.6, "no_speech_prob": 0.0014547083992511034}, {"id": 589, "seek": 230178, "start": 2310.78, "end": 2313.82, "text": " where the exploration exploitation trade off becomes", "tokens": [50814, 689, 264, 16197, 33122, 4923, 766, 3643, 50966], "temperature": 0.0, "avg_logprob": -0.14092962047721766, "compression_ratio": 1.6, "no_speech_prob": 0.0014547083992511034}, {"id": 590, "seek": 230178, "start": 2313.82, "end": 2324.34, "text": " important. And if you if you had a, say, a current policy that", "tokens": [50966, 1021, 13, 400, 498, 291, 498, 291, 632, 257, 11, 584, 11, 257, 2190, 3897, 300, 51492], "temperature": 0.0, "avg_logprob": -0.14092962047721766, "compression_ratio": 1.6, "no_speech_prob": 0.0014547083992511034}, {"id": 591, "seek": 230178, "start": 2324.34, "end": 2328.9, "text": " you're not completely sure is the right one. And, and then", "tokens": [51492, 291, 434, 406, 2584, 988, 307, 264, 558, 472, 13, 400, 11, 293, 550, 51720], "temperature": 0.0, "avg_logprob": -0.14092962047721766, "compression_ratio": 1.6, "no_speech_prob": 0.0014547083992511034}, {"id": 592, "seek": 232890, "start": 2328.9, "end": 2334.2200000000003, "text": " you're trying to say, Well, what, how should I play so that I'm", "tokens": [50364, 291, 434, 1382, 281, 584, 11, 1042, 11, 437, 11, 577, 820, 286, 862, 370, 300, 286, 478, 50630], "temperature": 0.0, "avg_logprob": -0.13526045865026012, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0018963624024763703}, {"id": 593, "seek": 232890, "start": 2334.2200000000003, "end": 2337.86, "text": " going to improve my policy the most as in I'm going to reduce", "tokens": [50630, 516, 281, 3470, 452, 3897, 264, 881, 382, 294, 286, 478, 516, 281, 5407, 50812], "temperature": 0.0, "avg_logprob": -0.13526045865026012, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0018963624024763703}, {"id": 594, "seek": 232890, "start": 2338.2200000000003, "end": 2341.2200000000003, "text": " the uncertainty that, you know, it is the right policy, like", "tokens": [50830, 264, 15697, 300, 11, 291, 458, 11, 309, 307, 264, 558, 3897, 11, 411, 50980], "temperature": 0.0, "avg_logprob": -0.13526045865026012, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0018963624024763703}, {"id": 595, "seek": 232890, "start": 2341.2200000000003, "end": 2345.62, "text": " that it picks the right things. So now we're getting closer to", "tokens": [50980, 300, 309, 16137, 264, 558, 721, 13, 407, 586, 321, 434, 1242, 4966, 281, 51200], "temperature": 0.0, "avg_logprob": -0.13526045865026012, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0018963624024763703}, {"id": 596, "seek": 232890, "start": 2345.62, "end": 2348.7400000000002, "text": " the kind of setting where it makes sense to use G flow nets.", "tokens": [51200, 264, 733, 295, 3287, 689, 309, 1669, 2020, 281, 764, 460, 3095, 36170, 13, 51356], "temperature": 0.0, "avg_logprob": -0.13526045865026012, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0018963624024763703}, {"id": 597, "seek": 232890, "start": 2349.42, "end": 2354.02, "text": " And then what I would expect, if we do the engineering work", "tokens": [51390, 400, 550, 437, 286, 576, 2066, 11, 498, 321, 360, 264, 7043, 589, 51620], "temperature": 0.0, "avg_logprob": -0.13526045865026012, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0018963624024763703}, {"id": 598, "seek": 232890, "start": 2354.02, "end": 2357.54, "text": " here, but based on the sort of much simpler problems we've", "tokens": [51620, 510, 11, 457, 2361, 322, 264, 1333, 295, 709, 18587, 2740, 321, 600, 51796], "temperature": 0.0, "avg_logprob": -0.13526045865026012, "compression_ratio": 1.6563706563706564, "no_speech_prob": 0.0018963624024763703}, {"id": 599, "seek": 235754, "start": 2357.54, "end": 2362.66, "text": " looked at, is that it would converge faster. So given, if you", "tokens": [50364, 2956, 412, 11, 307, 300, 309, 576, 41881, 4663, 13, 407, 2212, 11, 498, 291, 50620], "temperature": 0.0, "avg_logprob": -0.15671426524286686, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.006485786754637957}, {"id": 600, "seek": 235754, "start": 2362.66, "end": 2368.86, "text": " look at on the x axis, the number of games you're playing. And", "tokens": [50620, 574, 412, 322, 264, 2031, 10298, 11, 264, 1230, 295, 2813, 291, 434, 2433, 13, 400, 50930], "temperature": 0.0, "avg_logprob": -0.15671426524286686, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.006485786754637957}, {"id": 601, "seek": 235754, "start": 2368.86, "end": 2372.9, "text": " on the y axis, how good is your policy measured like on other", "tokens": [50930, 322, 264, 288, 10298, 11, 577, 665, 307, 428, 3897, 12690, 411, 322, 661, 51132], "temperature": 0.0, "avg_logprob": -0.15671426524286686, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.006485786754637957}, {"id": 602, "seek": 235754, "start": 2372.9, "end": 2375.98, "text": " games. So that's where you would get. In other words, it's the", "tokens": [51132, 2813, 13, 407, 300, 311, 689, 291, 576, 483, 13, 682, 661, 2283, 11, 309, 311, 264, 51286], "temperature": 0.0, "avg_logprob": -0.15671426524286686, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.006485786754637957}, {"id": 603, "seek": 235754, "start": 2375.98, "end": 2379.58, "text": " learning curve that you might gain on asymptotically, everything", "tokens": [51286, 2539, 7605, 300, 291, 1062, 6052, 322, 35114, 310, 984, 11, 1203, 51466], "temperature": 0.0, "avg_logprob": -0.15671426524286686, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.006485786754637957}, {"id": 604, "seek": 235754, "start": 2379.58, "end": 2383.7, "text": " is going to converge the optimal chess player, right? So the", "tokens": [51466, 307, 516, 281, 41881, 264, 16252, 24122, 4256, 11, 558, 30, 407, 264, 51672], "temperature": 0.0, "avg_logprob": -0.15671426524286686, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.006485786754637957}, {"id": 605, "seek": 235754, "start": 2383.7, "end": 2386.9, "text": " the place where it's interesting is to look at the learning", "tokens": [51672, 264, 1081, 689, 309, 311, 1880, 307, 281, 574, 412, 264, 2539, 51832], "temperature": 0.0, "avg_logprob": -0.15671426524286686, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.006485786754637957}, {"id": 606, "seek": 238690, "start": 2386.98, "end": 2390.34, "text": " curve how fast you learn. And here you want to sort of active", "tokens": [50368, 7605, 577, 2370, 291, 1466, 13, 400, 510, 291, 528, 281, 1333, 295, 4967, 50536], "temperature": 0.0, "avg_logprob": -0.1320519997523381, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.00497792661190033}, {"id": 607, "seek": 238690, "start": 2390.34, "end": 2393.58, "text": " learning thinking like, Well, I'm not just trying to win here.", "tokens": [50536, 2539, 1953, 411, 11, 1042, 11, 286, 478, 406, 445, 1382, 281, 1942, 510, 13, 50698], "temperature": 0.0, "avg_logprob": -0.1320519997523381, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.00497792661190033}, {"id": 608, "seek": 238690, "start": 2394.2200000000003, "end": 2398.98, "text": " I'm trying to gather information so that I'll win more in the", "tokens": [50730, 286, 478, 1382, 281, 5448, 1589, 370, 300, 286, 603, 1942, 544, 294, 264, 50968], "temperature": 0.0, "avg_logprob": -0.1320519997523381, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.00497792661190033}, {"id": 609, "seek": 238690, "start": 2398.98, "end": 2402.3, "text": " future. And it's a different objective. And that's where you", "tokens": [50968, 2027, 13, 400, 309, 311, 257, 819, 10024, 13, 400, 300, 311, 689, 291, 51134], "temperature": 0.0, "avg_logprob": -0.1320519997523381, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.00497792661190033}, {"id": 610, "seek": 238690, "start": 2402.3, "end": 2405.2200000000003, "text": " need diversity and exploration and like a model of your own", "tokens": [51134, 643, 8811, 293, 16197, 293, 411, 257, 2316, 295, 428, 1065, 51280], "temperature": 0.0, "avg_logprob": -0.1320519997523381, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.00497792661190033}, {"id": 611, "seek": 238690, "start": 2405.2200000000003, "end": 2408.1800000000003, "text": " uncertainty and an active learning policy.", "tokens": [51280, 15697, 293, 364, 4967, 2539, 3897, 13, 51428], "temperature": 0.0, "avg_logprob": -0.1320519997523381, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.00497792661190033}, {"id": 612, "seek": 238690, "start": 2409.38, "end": 2414.58, "text": " How much do you think this could be part of not maybe only", "tokens": [51488, 1012, 709, 360, 291, 519, 341, 727, 312, 644, 295, 406, 1310, 787, 51748], "temperature": 0.0, "avg_logprob": -0.1320519997523381, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.00497792661190033}, {"id": 613, "seek": 241458, "start": 2414.58, "end": 2420.54, "text": " reward maximization things, but information collection, things", "tokens": [50364, 7782, 5138, 2144, 721, 11, 457, 1589, 5765, 11, 721, 50662], "temperature": 0.0, "avg_logprob": -0.14597348492554943, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.006785809528082609}, {"id": 614, "seek": 241458, "start": 2420.54, "end": 2425.94, "text": " like, I'm sure you're you're thinking about in, let's say the", "tokens": [50662, 411, 11, 286, 478, 988, 291, 434, 291, 434, 1953, 466, 294, 11, 718, 311, 584, 264, 50932], "temperature": 0.0, "avg_logprob": -0.14597348492554943, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.006785809528082609}, {"id": 615, "seek": 241458, "start": 2425.94, "end": 2429.38, "text": " brain, there is there's sort of maybe a similar process going", "tokens": [50932, 3567, 11, 456, 307, 456, 311, 1333, 295, 1310, 257, 2531, 1399, 516, 51104], "temperature": 0.0, "avg_logprob": -0.14597348492554943, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.006785809528082609}, {"id": 616, "seek": 241458, "start": 2429.38, "end": 2432.86, "text": " on and what do I still need to retrieve in order to give certain", "tokens": [51104, 322, 293, 437, 360, 286, 920, 643, 281, 30254, 294, 1668, 281, 976, 1629, 51278], "temperature": 0.0, "avg_logprob": -0.14597348492554943, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.006785809528082609}, {"id": 617, "seek": 241458, "start": 2432.86, "end": 2437.7799999999997, "text": " answers to questions, or maybe in our, let's say, big search", "tokens": [51278, 6338, 281, 1651, 11, 420, 1310, 294, 527, 11, 718, 311, 584, 11, 955, 3164, 51524], "temperature": 0.0, "avg_logprob": -0.14597348492554943, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.006785809528082609}, {"id": 618, "seek": 241458, "start": 2437.7799999999997, "end": 2442.54, "text": " engine, let's just name one for naming sake, let's Google, or", "tokens": [51524, 2848, 11, 718, 311, 445, 1315, 472, 337, 25290, 9717, 11, 718, 311, 3329, 11, 420, 51762], "temperature": 0.0, "avg_logprob": -0.14597348492554943, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.006785809528082609}, {"id": 619, "seek": 244254, "start": 2442.58, "end": 2447.82, "text": " so would would try to answer your query, not by just searching", "tokens": [50366, 370, 576, 576, 853, 281, 1867, 428, 14581, 11, 406, 538, 445, 10808, 50628], "temperature": 0.0, "avg_logprob": -0.1408722821403952, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0008555203676223755}, {"id": 620, "seek": 244254, "start": 2447.82, "end": 2450.7, "text": " through their index, but by actively doing this multiple", "tokens": [50628, 807, 641, 8186, 11, 457, 538, 13022, 884, 341, 3866, 50772], "temperature": 0.0, "avg_logprob": -0.1408722821403952, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0008555203676223755}, {"id": 621, "seek": 244254, "start": 2450.82, "end": 2453.7799999999997, "text": " multiple things like, is this enough? Is this enough? Is this", "tokens": [50778, 3866, 721, 411, 11, 307, 341, 1547, 30, 1119, 341, 1547, 30, 1119, 341, 50926], "temperature": 0.0, "avg_logprob": -0.1408722821403952, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0008555203676223755}, {"id": 622, "seek": 244254, "start": 2453.7799999999997, "end": 2458.2599999999998, "text": " enough? Do you see connections to these types of things? Or are", "tokens": [50926, 1547, 30, 1144, 291, 536, 9271, 281, 613, 3467, 295, 721, 30, 1610, 366, 51150], "temperature": 0.0, "avg_logprob": -0.1408722821403952, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0008555203676223755}, {"id": 623, "seek": 244254, "start": 2458.2599999999998, "end": 2461.5, "text": " they inherently different? Because they might be not learning", "tokens": [51150, 436, 27993, 819, 30, 1436, 436, 1062, 312, 406, 2539, 51312], "temperature": 0.0, "avg_logprob": -0.1408722821403952, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0008555203676223755}, {"id": 624, "seek": 244254, "start": 2461.5, "end": 2462.2599999999998, "text": " on the spot?", "tokens": [51312, 322, 264, 4008, 30, 51350], "temperature": 0.0, "avg_logprob": -0.1408722821403952, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0008555203676223755}, {"id": 625, "seek": 244254, "start": 2462.82, "end": 2468.06, "text": " What they're doing on the spot is acquiring information. And you", "tokens": [51378, 708, 436, 434, 884, 322, 264, 4008, 307, 37374, 1589, 13, 400, 291, 51640], "temperature": 0.0, "avg_logprob": -0.1408722821403952, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0008555203676223755}, {"id": 626, "seek": 244254, "start": 2468.06, "end": 2470.14, "text": " want to do it in an efficient way. And that's where sort of the", "tokens": [51640, 528, 281, 360, 309, 294, 364, 7148, 636, 13, 400, 300, 311, 689, 1333, 295, 264, 51744], "temperature": 0.0, "avg_logprob": -0.1408722821403952, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0008555203676223755}, {"id": 627, "seek": 244254, "start": 2470.14, "end": 2471.74, "text": " active learning thinking comes in.", "tokens": [51744, 4967, 2539, 1953, 1487, 294, 13, 51824], "temperature": 0.0, "avg_logprob": -0.1408722821403952, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0008555203676223755}, {"id": 628, "seek": 247254, "start": 2472.7, "end": 2477.86, "text": " And I think it's actually a very big, practical problem in", "tokens": [50372, 400, 286, 519, 309, 311, 767, 257, 588, 955, 11, 8496, 1154, 294, 50630], "temperature": 0.0, "avg_logprob": -0.16777161571467034, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0008292208658531308}, {"id": 629, "seek": 247254, "start": 2477.86, "end": 2481.2599999999998, "text": " the deployment of like AI dialogue systems that are not", "tokens": [50630, 264, 19317, 295, 411, 7318, 10221, 3652, 300, 366, 406, 50800], "temperature": 0.0, "avg_logprob": -0.16777161571467034, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0008292208658531308}, {"id": 630, "seek": 247254, "start": 2481.2599999999998, "end": 2486.2599999999998, "text": " chit chat, but they're trying to say help a user achieve, you", "tokens": [50800, 417, 270, 5081, 11, 457, 436, 434, 1382, 281, 584, 854, 257, 4195, 4584, 11, 291, 51050], "temperature": 0.0, "avg_logprob": -0.16777161571467034, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0008292208658531308}, {"id": 631, "seek": 247254, "start": 2486.2599999999998, "end": 2488.9, "text": " know, get something get information or something like", "tokens": [51050, 458, 11, 483, 746, 483, 1589, 420, 746, 411, 51182], "temperature": 0.0, "avg_logprob": -0.16777161571467034, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0008292208658531308}, {"id": 632, "seek": 247254, "start": 2488.9, "end": 2492.46, "text": " this. This is this is a huge need for this in, you know, the", "tokens": [51182, 341, 13, 639, 307, 341, 307, 257, 2603, 643, 337, 341, 294, 11, 291, 458, 11, 264, 51360], "temperature": 0.0, "avg_logprob": -0.16777161571467034, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0008292208658531308}, {"id": 633, "seek": 247254, "start": 2492.46, "end": 2495.98, "text": " business world and search engines, and you know, it's much", "tokens": [51360, 1606, 1002, 293, 3164, 12982, 11, 293, 291, 458, 11, 309, 311, 709, 51536], "temperature": 0.0, "avg_logprob": -0.16777161571467034, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0008292208658531308}, {"id": 634, "seek": 247254, "start": 2495.98, "end": 2499.34, "text": " more than search engines. So I don't think we have the", "tokens": [51536, 544, 813, 3164, 12982, 13, 407, 286, 500, 380, 519, 321, 362, 264, 51704], "temperature": 0.0, "avg_logprob": -0.16777161571467034, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.0008292208658531308}, {"id": 635, "seek": 249934, "start": 2499.34, "end": 2503.98, "text": " algorithms that do that right now. And it's kind of painful. The", "tokens": [50364, 14642, 300, 360, 300, 558, 586, 13, 400, 309, 311, 733, 295, 11697, 13, 440, 50596], "temperature": 0.0, "avg_logprob": -0.18380980796002327, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0011333967559039593}, {"id": 636, "seek": 249934, "start": 2503.98, "end": 2509.42, "text": " human has to know, you know, is driving. But if, if we had", "tokens": [50596, 1952, 575, 281, 458, 11, 291, 458, 11, 307, 4840, 13, 583, 498, 11, 498, 321, 632, 50868], "temperature": 0.0, "avg_logprob": -0.18380980796002327, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0011333967559039593}, {"id": 637, "seek": 249934, "start": 2509.42, "end": 2513.98, "text": " systems that could explicitly model their own, say,", "tokens": [50868, 3652, 300, 727, 20803, 2316, 641, 1065, 11, 584, 11, 51096], "temperature": 0.0, "avg_logprob": -0.18380980796002327, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0011333967559039593}, {"id": 638, "seek": 249934, "start": 2513.98, "end": 2517.58, "text": " uncertainty about what the user needs or wants, or where to", "tokens": [51096, 15697, 466, 437, 264, 4195, 2203, 420, 2738, 11, 420, 689, 281, 51276], "temperature": 0.0, "avg_logprob": -0.18380980796002327, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0011333967559039593}, {"id": 639, "seek": 249934, "start": 2517.58, "end": 2523.58, "text": " find information. And then, and you need like pretty powerful", "tokens": [51276, 915, 1589, 13, 400, 550, 11, 293, 291, 643, 411, 1238, 4005, 51576], "temperature": 0.0, "avg_logprob": -0.18380980796002327, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0011333967559039593}, {"id": 640, "seek": 249934, "start": 2523.58, "end": 2526.3, "text": " models of that, like it's not just galaxies, they're simple", "tokens": [51576, 5245, 295, 300, 11, 411, 309, 311, 406, 445, 28755, 11, 436, 434, 2199, 51712], "temperature": 0.0, "avg_logprob": -0.18380980796002327, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.0011333967559039593}, {"id": 641, "seek": 252630, "start": 2526.3, "end": 2529.6200000000003, "text": " things. That's where G flow net strengths comes in, you can", "tokens": [50364, 721, 13, 663, 311, 689, 460, 3095, 2533, 16986, 1487, 294, 11, 291, 393, 50530], "temperature": 0.0, "avg_logprob": -0.1741888348649188, "compression_ratio": 1.5377777777777777, "no_speech_prob": 0.0021824887953698635}, {"id": 642, "seek": 252630, "start": 2529.6200000000003, "end": 2532.6600000000003, "text": " represent very, very complex distributions over", "tokens": [50530, 2906, 588, 11, 588, 3997, 37870, 670, 50682], "temperature": 0.0, "avg_logprob": -0.1741888348649188, "compression_ratio": 1.5377777777777777, "no_speech_prob": 0.0021824887953698635}, {"id": 643, "seek": 252630, "start": 2532.6600000000003, "end": 2539.1400000000003, "text": " compositional objects. It's not just a few numbers. And then I", "tokens": [50682, 10199, 2628, 6565, 13, 467, 311, 406, 445, 257, 1326, 3547, 13, 400, 550, 286, 51006], "temperature": 0.0, "avg_logprob": -0.1741888348649188, "compression_ratio": 1.5377777777777777, "no_speech_prob": 0.0021824887953698635}, {"id": 644, "seek": 252630, "start": 2539.1400000000003, "end": 2543.42, "text": " think you could get to much more efficient human machine", "tokens": [51006, 519, 291, 727, 483, 281, 709, 544, 7148, 1952, 3479, 51220], "temperature": 0.0, "avg_logprob": -0.1741888348649188, "compression_ratio": 1.5377777777777777, "no_speech_prob": 0.0021824887953698635}, {"id": 645, "seek": 252630, "start": 2543.42, "end": 2550.42, "text": " interfaces. And the same, I believe the same methodology", "tokens": [51220, 28416, 13, 400, 264, 912, 11, 286, 1697, 264, 912, 24850, 51570], "temperature": 0.0, "avg_logprob": -0.1741888348649188, "compression_ratio": 1.5377777777777777, "no_speech_prob": 0.0021824887953698635}, {"id": 646, "seek": 252630, "start": 2550.42, "end": 2553.42, "text": " could be used more generally in scientific discovery. So what", "tokens": [51570, 727, 312, 1143, 544, 5101, 294, 8134, 12114, 13, 407, 437, 51720], "temperature": 0.0, "avg_logprob": -0.1741888348649188, "compression_ratio": 1.5377777777777777, "no_speech_prob": 0.0021824887953698635}, {"id": 647, "seek": 255342, "start": 2553.46, "end": 2555.94, "text": " is scientific discovery? Like what is it that scientists do?", "tokens": [50366, 307, 8134, 12114, 30, 1743, 437, 307, 309, 300, 7708, 360, 30, 50490], "temperature": 0.0, "avg_logprob": -0.15485982596874237, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.005057847127318382}, {"id": 648, "seek": 255342, "start": 2556.58, "end": 2560.5, "text": " They plan experiments that are going to allow them to reduce", "tokens": [50522, 814, 1393, 12050, 300, 366, 516, 281, 2089, 552, 281, 5407, 50718], "temperature": 0.0, "avg_logprob": -0.15485982596874237, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.005057847127318382}, {"id": 649, "seek": 255342, "start": 2560.5, "end": 2563.98, "text": " the uncertainty on their theories of, you know, some", "tokens": [50718, 264, 15697, 322, 641, 13667, 295, 11, 291, 458, 11, 512, 50892], "temperature": 0.0, "avg_logprob": -0.15485982596874237, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.005057847127318382}, {"id": 650, "seek": 255342, "start": 2563.98, "end": 2566.42, "text": " aspect of the world. It's the same problem. Yeah, you have a", "tokens": [50892, 4171, 295, 264, 1002, 13, 467, 311, 264, 912, 1154, 13, 865, 11, 291, 362, 257, 51014], "temperature": 0.0, "avg_logprob": -0.15485982596874237, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.005057847127318382}, {"id": 651, "seek": 255342, "start": 2566.42, "end": 2570.34, "text": " series of questions you're allowed to ask to nature. And you", "tokens": [51014, 2638, 295, 1651, 291, 434, 4350, 281, 1029, 281, 3687, 13, 400, 291, 51210], "temperature": 0.0, "avg_logprob": -0.15485982596874237, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.005057847127318382}, {"id": 652, "seek": 255342, "start": 2570.34, "end": 2573.34, "text": " try to ask as few questions as possible to as quickly as", "tokens": [51210, 853, 281, 1029, 382, 1326, 1651, 382, 1944, 281, 382, 2661, 382, 51360], "temperature": 0.0, "avg_logprob": -0.15485982596874237, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.005057847127318382}, {"id": 653, "seek": 255342, "start": 2573.34, "end": 2575.06, "text": " possible, understand what's going on.", "tokens": [51360, 1944, 11, 1223, 437, 311, 516, 322, 13, 51446], "temperature": 0.0, "avg_logprob": -0.15485982596874237, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.005057847127318382}, {"id": 654, "seek": 255342, "start": 2576.02, "end": 2579.2200000000003, "text": " Is there a connection fundamentally to I'm thinking of", "tokens": [51494, 1119, 456, 257, 4984, 17879, 281, 286, 478, 1953, 295, 51654], "temperature": 0.0, "avg_logprob": -0.15485982596874237, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.005057847127318382}, {"id": 655, "seek": 255342, "start": 2579.2200000000003, "end": 2582.98, "text": " causality, which also I've seen a number of papers that you've", "tokens": [51654, 3302, 1860, 11, 597, 611, 286, 600, 1612, 257, 1230, 295, 10577, 300, 291, 600, 51842], "temperature": 0.0, "avg_logprob": -0.15485982596874237, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.005057847127318382}, {"id": 656, "seek": 258298, "start": 2583.02, "end": 2586.42, "text": " collaborated on with people who are who are deep into", "tokens": [50366, 42463, 322, 365, 561, 567, 366, 567, 366, 2452, 666, 50536], "temperature": 0.0, "avg_logprob": -0.18496218971584155, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0027901052962988615}, {"id": 657, "seek": 258298, "start": 2586.46, "end": 2590.9, "text": " causality research and so on. What do you think there is a", "tokens": [50538, 3302, 1860, 2132, 293, 370, 322, 13, 708, 360, 291, 519, 456, 307, 257, 50760], "temperature": 0.0, "avg_logprob": -0.18496218971584155, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0027901052962988615}, {"id": 658, "seek": 258298, "start": 2592.7400000000002, "end": 2598.18, "text": " a connection there where an agent could learn to uncover if", "tokens": [50852, 257, 4984, 456, 689, 364, 9461, 727, 1466, 281, 21694, 498, 51124], "temperature": 0.0, "avg_logprob": -0.18496218971584155, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0027901052962988615}, {"id": 659, "seek": 258298, "start": 2598.18, "end": 2600.7, "text": " you think about scientific discovery to uncover the", "tokens": [51124, 291, 519, 466, 8134, 12114, 281, 21694, 264, 51250], "temperature": 0.0, "avg_logprob": -0.18496218971584155, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0027901052962988615}, {"id": 660, "seek": 258298, "start": 2600.7, "end": 2604.78, "text": " fundamental causal structure of the world by asking such", "tokens": [51250, 8088, 38755, 3877, 295, 264, 1002, 538, 3365, 1270, 51454], "temperature": 0.0, "avg_logprob": -0.18496218971584155, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0027901052962988615}, {"id": 661, "seek": 258298, "start": 2604.78, "end": 2607.98, "text": " questions, like could there be a connection to that branch of", "tokens": [51454, 1651, 11, 411, 727, 456, 312, 257, 4984, 281, 300, 9819, 295, 51614], "temperature": 0.0, "avg_logprob": -0.18496218971584155, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0027901052962988615}, {"id": 662, "seek": 258298, "start": 2607.98, "end": 2612.34, "text": " research? And could this finally be like the unification of", "tokens": [51614, 2132, 30, 400, 727, 341, 2721, 312, 411, 264, 517, 3774, 295, 51832], "temperature": 0.0, "avg_logprob": -0.18496218971584155, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.0027901052962988615}, {"id": 663, "seek": 261234, "start": 2612.6200000000003, "end": 2617.5, "text": " of something machine learning and the the world of causality?", "tokens": [50378, 295, 746, 3479, 2539, 293, 264, 264, 1002, 295, 3302, 1860, 30, 50622], "temperature": 0.0, "avg_logprob": -0.16397507159740893, "compression_ratio": 1.52, "no_speech_prob": 0.0006659335922449827}, {"id": 664, "seek": 261234, "start": 2618.7400000000002, "end": 2621.2200000000003, "text": " Yes, you guys are really asking all the right questions. Thank", "tokens": [50684, 1079, 11, 291, 1074, 366, 534, 3365, 439, 264, 558, 1651, 13, 1044, 50808], "temperature": 0.0, "avg_logprob": -0.16397507159740893, "compression_ratio": 1.52, "no_speech_prob": 0.0006659335922449827}, {"id": 665, "seek": 261234, "start": 2621.2200000000003, "end": 2626.54, "text": " you so much. In fact, one of my main motivations for the", "tokens": [50808, 291, 370, 709, 13, 682, 1186, 11, 472, 295, 452, 2135, 39034, 337, 264, 51074], "temperature": 0.0, "avg_logprob": -0.16397507159740893, "compression_ratio": 1.52, "no_speech_prob": 0.0006659335922449827}, {"id": 666, "seek": 261234, "start": 2626.58, "end": 2631.26, "text": " pursuing the the G flow net research program is that I think", "tokens": [51076, 20222, 264, 264, 460, 3095, 2533, 2132, 1461, 307, 300, 286, 519, 51310], "temperature": 0.0, "avg_logprob": -0.16397507159740893, "compression_ratio": 1.52, "no_speech_prob": 0.0006659335922449827}, {"id": 667, "seek": 261234, "start": 2631.26, "end": 2638.1000000000004, "text": " it's the it's an ideal tool for implementing what I called in", "tokens": [51310, 309, 311, 264, 309, 311, 364, 7157, 2290, 337, 18114, 437, 286, 1219, 294, 51652], "temperature": 0.0, "avg_logprob": -0.16397507159740893, "compression_ratio": 1.52, "no_speech_prob": 0.0006659335922449827}, {"id": 668, "seek": 263810, "start": 2638.1, "end": 2643.46, "text": " my talks, system to inductive biases. So what this means is", "tokens": [50364, 452, 6686, 11, 1185, 281, 31612, 488, 32152, 13, 407, 437, 341, 1355, 307, 50632], "temperature": 0.0, "avg_logprob": -0.11847197872468795, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.003944031894207001}, {"id": 669, "seek": 263810, "start": 2643.94, "end": 2646.58, "text": " there are lots of things we know from neuroscience and", "tokens": [50656, 456, 366, 3195, 295, 721, 321, 458, 490, 42762, 293, 50788], "temperature": 0.0, "avg_logprob": -0.11847197872468795, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.003944031894207001}, {"id": 670, "seek": 263810, "start": 2646.58, "end": 2652.22, "text": " cognitive science about how we think. And we can bring that", "tokens": [50788, 15605, 3497, 466, 577, 321, 519, 13, 400, 321, 393, 1565, 300, 51070], "temperature": 0.0, "avg_logprob": -0.11847197872468795, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.003944031894207001}, {"id": 671, "seek": 263810, "start": 2652.2599999999998, "end": 2658.62, "text": " into the design of probabilistic machine learning, you know,", "tokens": [51072, 666, 264, 1715, 295, 31959, 3142, 3479, 2539, 11, 291, 458, 11, 51390], "temperature": 0.0, "avg_logprob": -0.11847197872468795, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.003944031894207001}, {"id": 672, "seek": 263810, "start": 2658.62, "end": 2662.94, "text": " based on deep learning is the building blocks. And one of the", "tokens": [51390, 2361, 322, 2452, 2539, 307, 264, 2390, 8474, 13, 400, 472, 295, 264, 51606], "temperature": 0.0, "avg_logprob": -0.11847197872468795, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.003944031894207001}, {"id": 673, "seek": 263810, "start": 2662.94, "end": 2666.22, "text": " inductive biases, like one of the characteristics of how we", "tokens": [51606, 31612, 488, 32152, 11, 411, 472, 295, 264, 10891, 295, 577, 321, 51770], "temperature": 0.0, "avg_logprob": -0.11847197872468795, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.003944031894207001}, {"id": 674, "seek": 266622, "start": 2666.22, "end": 2669.14, "text": " think is we think causally, we're constantly asking the why", "tokens": [50364, 519, 307, 321, 519, 3302, 379, 11, 321, 434, 6460, 3365, 264, 983, 50510], "temperature": 0.0, "avg_logprob": -0.13845572038130327, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0030259781051427126}, {"id": 675, "seek": 266622, "start": 2669.14, "end": 2674.02, "text": " questions we're trying to find explanations and so on. And, and", "tokens": [50510, 1651, 321, 434, 1382, 281, 915, 28708, 293, 370, 322, 13, 400, 11, 293, 50754], "temperature": 0.0, "avg_logprob": -0.13845572038130327, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0030259781051427126}, {"id": 676, "seek": 266622, "start": 2674.06, "end": 2677.9399999999996, "text": " and that connects with classical AI, like the way we think, to", "tokens": [50756, 293, 300, 16967, 365, 13735, 7318, 11, 411, 264, 636, 321, 519, 11, 281, 50950], "temperature": 0.0, "avg_logprob": -0.13845572038130327, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0030259781051427126}, {"id": 677, "seek": 266622, "start": 2677.9399999999996, "end": 2682.02, "text": " some extent, has also inspired classical AI, you know, rules", "tokens": [50950, 512, 8396, 11, 575, 611, 7547, 13735, 7318, 11, 291, 458, 11, 4474, 51154], "temperature": 0.0, "avg_logprob": -0.13845572038130327, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0030259781051427126}, {"id": 678, "seek": 266622, "start": 2682.02, "end": 2686.62, "text": " and logic and and reasoning. And we haven't yet found the way", "tokens": [51154, 293, 9952, 293, 293, 21577, 13, 400, 321, 2378, 380, 1939, 1352, 264, 636, 51384], "temperature": 0.0, "avg_logprob": -0.13845572038130327, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0030259781051427126}, {"id": 679, "seek": 266622, "start": 2686.62, "end": 2690.3799999999997, "text": " to integrate these abilities in deep learning. And of course,", "tokens": [51384, 281, 13365, 613, 11582, 294, 2452, 2539, 13, 400, 295, 1164, 11, 51572], "temperature": 0.0, "avg_logprob": -0.13845572038130327, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0030259781051427126}, {"id": 680, "seek": 266622, "start": 2690.3799999999997, "end": 2694.7, "text": " lots of people are like, trying to and and that's important.", "tokens": [51572, 3195, 295, 561, 366, 411, 11, 1382, 281, 293, 293, 300, 311, 1021, 13, 51788], "temperature": 0.0, "avg_logprob": -0.13845572038130327, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0030259781051427126}, {"id": 681, "seek": 269470, "start": 2695.7, "end": 2699.1, "text": " But but I think the reason why G for nets give us an amazing", "tokens": [50414, 583, 457, 286, 519, 264, 1778, 983, 460, 337, 36170, 976, 505, 364, 2243, 50584], "temperature": 0.0, "avg_logprob": -0.22389298742944067, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.0010813000844791532}, {"id": 682, "seek": 269470, "start": 2699.1, "end": 2703.9399999999996, "text": " handle on this is because they they're really good at", "tokens": [50584, 4813, 322, 341, 307, 570, 436, 436, 434, 534, 665, 412, 50826], "temperature": 0.0, "avg_logprob": -0.22389298742944067, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.0010813000844791532}, {"id": 683, "seek": 269470, "start": 2703.9399999999996, "end": 2708.2999999999997, "text": " representing distributions and sampling over graphs. And, and", "tokens": [50826, 13460, 37870, 293, 21179, 670, 24877, 13, 400, 11, 293, 51044], "temperature": 0.0, "avg_logprob": -0.22389298742944067, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.0010813000844791532}, {"id": 684, "seek": 269470, "start": 2708.2999999999997, "end": 2712.66, "text": " like a reasoning or a set of possible reasoning to explain", "tokens": [51044, 411, 257, 21577, 420, 257, 992, 295, 1944, 21577, 281, 2903, 51262], "temperature": 0.0, "avg_logprob": -0.22389298742944067, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.0010813000844791532}, {"id": 685, "seek": 269470, "start": 2712.66, "end": 2718.54, "text": " something or to, you know, for planning. The these are graphs.", "tokens": [51262, 746, 420, 281, 11, 291, 458, 11, 337, 5038, 13, 440, 613, 366, 24877, 13, 51556], "temperature": 0.0, "avg_logprob": -0.22389298742944067, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.0010813000844791532}, {"id": 686, "seek": 269470, "start": 2719.7799999999997, "end": 2724.2999999999997, "text": " And your thoughts can be seen as graphs, right? So think of like,", "tokens": [51618, 400, 428, 4598, 393, 312, 1612, 382, 24877, 11, 558, 30, 407, 519, 295, 411, 11, 51844], "temperature": 0.0, "avg_logprob": -0.22389298742944067, "compression_ratio": 1.60352422907489, "no_speech_prob": 0.0010813000844791532}, {"id": 687, "seek": 272430, "start": 2724.46, "end": 2727.1400000000003, "text": " maybe a simple version of this, think of a parse, like a", "tokens": [50372, 1310, 257, 2199, 3037, 295, 341, 11, 519, 295, 257, 48377, 11, 411, 257, 50506], "temperature": 0.0, "avg_logprob": -0.15837539610315543, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0008422731189057231}, {"id": 688, "seek": 272430, "start": 2727.1400000000003, "end": 2732.5, "text": " semantic and syntactic parse of a sentence is a graph. But", "tokens": [50506, 47982, 293, 23980, 19892, 48377, 295, 257, 8174, 307, 257, 4295, 13, 583, 50774], "temperature": 0.0, "avg_logprob": -0.15837539610315543, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0008422731189057231}, {"id": 689, "seek": 272430, "start": 2732.5, "end": 2734.26, "text": " usually it's, you know, it's more than a tree, there are all", "tokens": [50774, 2673, 309, 311, 11, 291, 458, 11, 309, 311, 544, 813, 257, 4230, 11, 456, 366, 439, 50862], "temperature": 0.0, "avg_logprob": -0.15837539610315543, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0008422731189057231}, {"id": 690, "seek": 272430, "start": 2734.26, "end": 2737.3, "text": " sorts of semantic connections, including with knowledge graphs,", "tokens": [50862, 7527, 295, 47982, 9271, 11, 3009, 365, 3601, 24877, 11, 51014], "temperature": 0.0, "avg_logprob": -0.15837539610315543, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0008422731189057231}, {"id": 691, "seek": 272430, "start": 2737.3, "end": 2742.9, "text": " right, which also graphs. So the ability to implicitly represent", "tokens": [51014, 558, 11, 597, 611, 24877, 13, 407, 264, 3485, 281, 26947, 356, 2906, 51294], "temperature": 0.0, "avg_logprob": -0.15837539610315543, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0008422731189057231}, {"id": 692, "seek": 272430, "start": 2742.94, "end": 2746.9, "text": " those distributions and sample pieces of them as thoughts is, I", "tokens": [51296, 729, 37870, 293, 6889, 3755, 295, 552, 382, 4598, 307, 11, 286, 51494], "temperature": 0.0, "avg_logprob": -0.15837539610315543, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0008422731189057231}, {"id": 693, "seek": 272430, "start": 2746.9, "end": 2750.7400000000002, "text": " think, fundamental to how we think. And going back to", "tokens": [51494, 519, 11, 8088, 281, 577, 321, 519, 13, 400, 516, 646, 281, 51686], "temperature": 0.0, "avg_logprob": -0.15837539610315543, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0008422731189057231}, {"id": 694, "seek": 272430, "start": 2750.7400000000002, "end": 2753.02, "text": " causality, one of the hard questions that I think G for", "tokens": [51686, 3302, 1860, 11, 472, 295, 264, 1152, 1651, 300, 286, 519, 460, 337, 51800], "temperature": 0.0, "avg_logprob": -0.15837539610315543, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.0008422731189057231}, {"id": 695, "seek": 275302, "start": 2753.02, "end": 2756.94, "text": " nets can help us with is causal discovery. So in other words,", "tokens": [50364, 36170, 393, 854, 505, 365, 307, 38755, 12114, 13, 407, 294, 661, 2283, 11, 50560], "temperature": 0.0, "avg_logprob": -0.17533452795186175, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.003482084721326828}, {"id": 696, "seek": 275302, "start": 2757.06, "end": 2759.42, "text": " what is the underlying cause structure of the world, including", "tokens": [50566, 437, 307, 264, 14217, 3082, 3877, 295, 264, 1002, 11, 3009, 50684], "temperature": 0.0, "avg_logprob": -0.17533452795186175, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.003482084721326828}, {"id": 697, "seek": 275302, "start": 2759.42, "end": 2763.98, "text": " the uncertainty about it? Given the things we observe, a lot of", "tokens": [50684, 264, 15697, 466, 309, 30, 18600, 264, 721, 321, 11441, 11, 257, 688, 295, 50912], "temperature": 0.0, "avg_logprob": -0.17533452795186175, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.003482084721326828}, {"id": 698, "seek": 275302, "start": 2763.98, "end": 2766.54, "text": " the research and causality has been okay, we observe these,", "tokens": [50912, 264, 2132, 293, 3302, 1860, 575, 668, 1392, 11, 321, 11441, 613, 11, 51040], "temperature": 0.0, "avg_logprob": -0.17533452795186175, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.003482084721326828}, {"id": 699, "seek": 275302, "start": 2766.58, "end": 2770.9, "text": " these random variables, discover, you know, make inferences", "tokens": [51042, 613, 4974, 9102, 11, 4411, 11, 291, 458, 11, 652, 13596, 2667, 51258], "temperature": 0.0, "avg_logprob": -0.17533452795186175, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.003482084721326828}, {"id": 700, "seek": 275302, "start": 2770.9, "end": 2774.14, "text": " about, you know, whether what we can say about whether it goes", "tokens": [51258, 466, 11, 291, 458, 11, 1968, 437, 321, 393, 584, 466, 1968, 309, 1709, 51420], "temperature": 0.0, "avg_logprob": -0.17533452795186175, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.003482084721326828}, {"id": 701, "seek": 275302, "start": 2774.14, "end": 2779.5, "text": " to be and so on. But it's much harder to discover the causal", "tokens": [51420, 281, 312, 293, 370, 322, 13, 583, 309, 311, 709, 6081, 281, 4411, 264, 38755, 51688], "temperature": 0.0, "avg_logprob": -0.17533452795186175, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.003482084721326828}, {"id": 702, "seek": 277950, "start": 2779.54, "end": 2784.02, "text": " graph that that, you know, in a large set of variables, and", "tokens": [50366, 4295, 300, 300, 11, 291, 458, 11, 294, 257, 2416, 992, 295, 9102, 11, 293, 50590], "temperature": 0.0, "avg_logprob": -0.15912140072800043, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002980090444907546}, {"id": 703, "seek": 277950, "start": 2784.02, "end": 2786.98, "text": " it's even harder. And really, nobody's done a real job there.", "tokens": [50590, 309, 311, 754, 6081, 13, 400, 534, 11, 5079, 311, 1096, 257, 957, 1691, 456, 13, 50738], "temperature": 0.0, "avg_logprob": -0.15912140072800043, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002980090444907546}, {"id": 704, "seek": 277950, "start": 2787.42, "end": 2791.06, "text": " To do this when what the learner sees is not the causal", "tokens": [50760, 1407, 360, 341, 562, 437, 264, 33347, 8194, 307, 406, 264, 38755, 50942], "temperature": 0.0, "avg_logprob": -0.15912140072800043, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002980090444907546}, {"id": 705, "seek": 277950, "start": 2791.06, "end": 2794.14, "text": " variables, but just like low level pixels. And you also have", "tokens": [50942, 9102, 11, 457, 445, 411, 2295, 1496, 18668, 13, 400, 291, 611, 362, 51096], "temperature": 0.0, "avg_logprob": -0.15912140072800043, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002980090444907546}, {"id": 706, "seek": 277950, "start": 2794.14, "end": 2796.26, "text": " to figure out what are the causal variables and how they're", "tokens": [51096, 281, 2573, 484, 437, 366, 264, 38755, 9102, 293, 577, 436, 434, 51202], "temperature": 0.0, "avg_logprob": -0.15912140072800043, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002980090444907546}, {"id": 707, "seek": 277950, "start": 2796.26, "end": 2799.22, "text": " related causal. And I think G planets can help us do that.", "tokens": [51202, 4077, 38755, 13, 400, 286, 519, 460, 15126, 393, 854, 505, 360, 300, 13, 51350], "temperature": 0.0, "avg_logprob": -0.15912140072800043, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002980090444907546}, {"id": 708, "seek": 277950, "start": 2799.94, "end": 2803.06, "text": " This this opens up, this is so many avenues of questions, I", "tokens": [51386, 639, 341, 9870, 493, 11, 341, 307, 370, 867, 43039, 295, 1651, 11, 286, 51542], "temperature": 0.0, "avg_logprob": -0.15912140072800043, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002980090444907546}, {"id": 709, "seek": 277950, "start": 2803.06, "end": 2806.78, "text": " think it'll probably almost be a future episode in itself. But", "tokens": [51542, 519, 309, 603, 1391, 1920, 312, 257, 2027, 3500, 294, 2564, 13, 583, 51728], "temperature": 0.0, "avg_logprob": -0.15912140072800043, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002980090444907546}, {"id": 710, "seek": 280678, "start": 2806.78, "end": 2812.1400000000003, "text": " let me just ask you about some of the basic ones, which is, as", "tokens": [50364, 718, 385, 445, 1029, 291, 466, 512, 295, 264, 3875, 2306, 11, 597, 307, 11, 382, 50632], "temperature": 0.0, "avg_logprob": -0.11971380579189991, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0010648652678355575}, {"id": 711, "seek": 280678, "start": 2812.1400000000003, "end": 2814.7400000000002, "text": " you mentioned, kind of learning the causality causality", "tokens": [50632, 291, 2835, 11, 733, 295, 2539, 264, 3302, 1860, 3302, 1860, 50762], "temperature": 0.0, "avg_logprob": -0.11971380579189991, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0010648652678355575}, {"id": 712, "seek": 280678, "start": 2814.7400000000002, "end": 2818.5800000000004, "text": " structure, much more difficult problem. And the first question", "tokens": [50762, 3877, 11, 709, 544, 2252, 1154, 13, 400, 264, 700, 1168, 50954], "temperature": 0.0, "avg_logprob": -0.11971380579189991, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0010648652678355575}, {"id": 713, "seek": 280678, "start": 2818.5800000000004, "end": 2821.9, "text": " is just how to represent the causality. And so you, you, you", "tokens": [50954, 307, 445, 577, 281, 2906, 264, 3302, 1860, 13, 400, 370, 291, 11, 291, 11, 291, 51120], "temperature": 0.0, "avg_logprob": -0.11971380579189991, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0010648652678355575}, {"id": 714, "seek": 280678, "start": 2821.9, "end": 2824.38, "text": " mentioned graphs, you know, graphs is one way. And of course,", "tokens": [51120, 2835, 24877, 11, 291, 458, 11, 24877, 307, 472, 636, 13, 400, 295, 1164, 11, 51244], "temperature": 0.0, "avg_logprob": -0.11971380579189991, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0010648652678355575}, {"id": 715, "seek": 280678, "start": 2824.7400000000002, "end": 2829.26, "text": " you can develop, you know, isomorphic ways of representing", "tokens": [51262, 291, 393, 1499, 11, 291, 458, 11, 307, 32702, 299, 2098, 295, 13460, 51488], "temperature": 0.0, "avg_logprob": -0.11971380579189991, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0010648652678355575}, {"id": 716, "seek": 280678, "start": 2829.6600000000003, "end": 2832.7400000000002, "text": " certain parts of logic as graphs, etc, depending on how, you", "tokens": [51508, 1629, 3166, 295, 9952, 382, 24877, 11, 5183, 11, 5413, 322, 577, 11, 291, 51662], "temperature": 0.0, "avg_logprob": -0.11971380579189991, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0010648652678355575}, {"id": 717, "seek": 280678, "start": 2832.7400000000002, "end": 2836.5, "text": " know, how rich you make the graph structure. But there's", "tokens": [51662, 458, 11, 577, 4593, 291, 652, 264, 4295, 3877, 13, 583, 456, 311, 51850], "temperature": 0.0, "avg_logprob": -0.11971380579189991, "compression_ratio": 1.8715953307392996, "no_speech_prob": 0.0010648652678355575}, {"id": 718, "seek": 283650, "start": 2836.5, "end": 2838.86, "text": " also the other issue of, you know, when you're trying to", "tokens": [50364, 611, 264, 661, 2734, 295, 11, 291, 458, 11, 562, 291, 434, 1382, 281, 50482], "temperature": 0.0, "avg_logprob": -0.15992991939471785, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.0006070084637030959}, {"id": 719, "seek": 283650, "start": 2838.86, "end": 2841.62, "text": " build, and I think it's probably correct to call this a world", "tokens": [50482, 1322, 11, 293, 286, 519, 309, 311, 1391, 3006, 281, 818, 341, 257, 1002, 50620], "temperature": 0.0, "avg_logprob": -0.15992991939471785, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.0006070084637030959}, {"id": 720, "seek": 283650, "start": 2841.62, "end": 2844.22, "text": " model, right, like we're trying to build a causal", "tokens": [50620, 2316, 11, 558, 11, 411, 321, 434, 1382, 281, 1322, 257, 38755, 50750], "temperature": 0.0, "avg_logprob": -0.15992991939471785, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.0006070084637030959}, {"id": 721, "seek": 283650, "start": 2844.42, "end": 2847.78, "text": " that's the word I use. Okay, great. And I, and so I have one", "tokens": [50760, 300, 311, 264, 1349, 286, 764, 13, 1033, 11, 869, 13, 400, 286, 11, 293, 370, 286, 362, 472, 50928], "temperature": 0.0, "avg_logprob": -0.15992991939471785, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.0006070084637030959}, {"id": 722, "seek": 283650, "start": 2847.78, "end": 2851.86, "text": " quick question about that, which is, you know, to me, to some", "tokens": [50928, 1702, 1168, 466, 300, 11, 597, 307, 11, 291, 458, 11, 281, 385, 11, 281, 512, 51132], "temperature": 0.0, "avg_logprob": -0.15992991939471785, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.0006070084637030959}, {"id": 723, "seek": 283650, "start": 2851.86, "end": 2854.86, "text": " people, world model is only the discriminative function. It's", "tokens": [51132, 561, 11, 1002, 2316, 307, 787, 264, 20828, 1166, 2445, 13, 467, 311, 51282], "temperature": 0.0, "avg_logprob": -0.15992991939471785, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.0006070084637030959}, {"id": 724, "seek": 283650, "start": 2854.86, "end": 2857.7, "text": " just that, you know, probability y given x, to me, it's more", "tokens": [51282, 445, 300, 11, 291, 458, 11, 8482, 288, 2212, 2031, 11, 281, 385, 11, 309, 311, 544, 51424], "temperature": 0.0, "avg_logprob": -0.15992991939471785, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.0006070084637030959}, {"id": 725, "seek": 283650, "start": 2857.7, "end": 2861.3, "text": " general. It's also the structure of x. Is that, is that also", "tokens": [51424, 2674, 13, 467, 311, 611, 264, 3877, 295, 2031, 13, 1119, 300, 11, 307, 300, 611, 51604], "temperature": 0.0, "avg_logprob": -0.15992991939471785, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.0006070084637030959}, {"id": 726, "seek": 283650, "start": 2861.3, "end": 2864.54, "text": " your, your view as well? Yes. Yes. Okay. And so in", "tokens": [51604, 428, 11, 428, 1910, 382, 731, 30, 1079, 13, 1079, 13, 1033, 13, 400, 370, 294, 51766], "temperature": 0.0, "avg_logprob": -0.15992991939471785, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.0006070084637030959}, {"id": 727, "seek": 286454, "start": 2864.58, "end": 2867.7, "text": " constructing those, those world models, some of the, let's say", "tokens": [50366, 39969, 729, 11, 729, 1002, 5245, 11, 512, 295, 264, 11, 718, 311, 584, 50522], "temperature": 0.0, "avg_logprob": -0.1179601816030649, "compression_ratio": 1.79136690647482, "no_speech_prob": 0.0023966017179191113}, {"id": 728, "seek": 286454, "start": 2867.7, "end": 2872.7, "text": " the pushback on on these type of generative techniques from, from", "tokens": [50522, 264, 2944, 3207, 322, 322, 613, 2010, 295, 1337, 1166, 7512, 490, 11, 490, 50772], "temperature": 0.0, "avg_logprob": -0.1179601816030649, "compression_ratio": 1.79136690647482, "no_speech_prob": 0.0023966017179191113}, {"id": 729, "seek": 286454, "start": 2872.7, "end": 2875.62, "text": " folks that are more skew more towards the discriminative side", "tokens": [50772, 4024, 300, 366, 544, 8756, 86, 544, 3030, 264, 20828, 1166, 1252, 50918], "temperature": 0.0, "avg_logprob": -0.1179601816030649, "compression_ratio": 1.79136690647482, "no_speech_prob": 0.0023966017179191113}, {"id": 730, "seek": 286454, "start": 2875.98, "end": 2878.86, "text": " is, hey, look, fine, you're going to go and try and build this", "tokens": [50936, 307, 11, 4177, 11, 574, 11, 2489, 11, 291, 434, 516, 281, 352, 293, 853, 293, 1322, 341, 51080], "temperature": 0.0, "avg_logprob": -0.1179601816030649, "compression_ratio": 1.79136690647482, "no_speech_prob": 0.0023966017179191113}, {"id": 731, "seek": 286454, "start": 2878.86, "end": 2881.58, "text": " generative model, it's going to be even more complicated than", "tokens": [51080, 1337, 1166, 2316, 11, 309, 311, 516, 281, 312, 754, 544, 6179, 813, 51216], "temperature": 0.0, "avg_logprob": -0.1179601816030649, "compression_ratio": 1.79136690647482, "no_speech_prob": 0.0023966017179191113}, {"id": 732, "seek": 286454, "start": 2881.58, "end": 2884.86, "text": " this discriminative model, because it also has to learn, you", "tokens": [51216, 341, 20828, 1166, 2316, 11, 570, 309, 611, 575, 281, 1466, 11, 291, 51380], "temperature": 0.0, "avg_logprob": -0.1179601816030649, "compression_ratio": 1.79136690647482, "no_speech_prob": 0.0023966017179191113}, {"id": 733, "seek": 286454, "start": 2884.86, "end": 2888.02, "text": " know, the structure on x. But I think the possible free lunch", "tokens": [51380, 458, 11, 264, 3877, 322, 2031, 13, 583, 286, 519, 264, 1944, 1737, 6349, 51538], "temperature": 0.0, "avg_logprob": -0.1179601816030649, "compression_ratio": 1.79136690647482, "no_speech_prob": 0.0023966017179191113}, {"id": 734, "seek": 286454, "start": 2888.02, "end": 2893.14, "text": " here, is that you can learn abstract structure on on x. And", "tokens": [51538, 510, 11, 307, 300, 291, 393, 1466, 12649, 3877, 322, 322, 2031, 13, 400, 51794], "temperature": 0.0, "avg_logprob": -0.1179601816030649, "compression_ratio": 1.79136690647482, "no_speech_prob": 0.0023966017179191113}, {"id": 735, "seek": 289314, "start": 2893.14, "end": 2897.1, "text": " so if you learn these abstract world models, throwing away all", "tokens": [50364, 370, 498, 291, 1466, 613, 12649, 1002, 5245, 11, 10238, 1314, 439, 50562], "temperature": 0.0, "avg_logprob": -0.13866843817368993, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0003799213736783713}, {"id": 736, "seek": 289314, "start": 2897.1, "end": 2899.7, "text": " the nitty gritty that doesn't really matter, you can potentially", "tokens": [50562, 264, 297, 10016, 677, 10016, 300, 1177, 380, 534, 1871, 11, 291, 393, 7263, 50692], "temperature": 0.0, "avg_logprob": -0.13866843817368993, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0003799213736783713}, {"id": 737, "seek": 289314, "start": 2899.7, "end": 2903.66, "text": " have very powerful, you know, predictive encoding, if you will,", "tokens": [50692, 362, 588, 4005, 11, 291, 458, 11, 35521, 43430, 11, 498, 291, 486, 11, 50890], "temperature": 0.0, "avg_logprob": -0.13866843817368993, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0003799213736783713}, {"id": 738, "seek": 289314, "start": 2903.7, "end": 2905.42, "text": " like, what's, what's your thoughts on that?", "tokens": [50892, 411, 11, 437, 311, 11, 437, 311, 428, 4598, 322, 300, 30, 50978], "temperature": 0.0, "avg_logprob": -0.13866843817368993, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0003799213736783713}, {"id": 739, "seek": 289314, "start": 2905.9, "end": 2910.74, "text": " Oh, that's what I've been thinking for almost 20 years. And", "tokens": [51002, 876, 11, 300, 311, 437, 286, 600, 668, 1953, 337, 1920, 945, 924, 13, 400, 51244], "temperature": 0.0, "avg_logprob": -0.13866843817368993, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0003799213736783713}, {"id": 740, "seek": 289314, "start": 2910.74, "end": 2914.14, "text": " one of the reasons why I've been interested in deep learning as", "tokens": [51244, 472, 295, 264, 4112, 983, 286, 600, 668, 3102, 294, 2452, 2539, 382, 51414], "temperature": 0.0, "avg_logprob": -0.13866843817368993, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0003799213736783713}, {"id": 741, "seek": 289314, "start": 2914.18, "end": 2919.14, "text": " a way to think of discovering abstract representations, you", "tokens": [51416, 257, 636, 281, 519, 295, 24773, 12649, 33358, 11, 291, 51664], "temperature": 0.0, "avg_logprob": -0.13866843817368993, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0003799213736783713}, {"id": 742, "seek": 291914, "start": 2919.14, "end": 2924.02, "text": " know, from the early days of deep learning, as in like mid like", "tokens": [50364, 458, 11, 490, 264, 2440, 1708, 295, 2452, 2539, 11, 382, 294, 411, 2062, 411, 50608], "temperature": 0.0, "avg_logprob": -0.2184250665747601, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.00678574712947011}, {"id": 743, "seek": 291914, "start": 2924.02, "end": 2929.18, "text": " 2005 or something. And, and in the paper that Jan McCarr and I", "tokens": [50608, 14394, 420, 746, 13, 400, 11, 293, 294, 264, 3035, 300, 4956, 12061, 2284, 293, 286, 50866], "temperature": 0.0, "avg_logprob": -0.2184250665747601, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.00678574712947011}, {"id": 744, "seek": 291914, "start": 2929.18, "end": 2933.02, "text": " wrote about, and also other papers I wrote with some of my", "tokens": [50866, 4114, 466, 11, 293, 611, 661, 10577, 286, 4114, 365, 512, 295, 452, 51058], "temperature": 0.0, "avg_logprob": -0.2184250665747601, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.00678574712947011}, {"id": 745, "seek": 291914, "start": 2933.02, "end": 2936.54, "text": " colleagues at the University of Montreal on, you know, deep", "tokens": [51058, 7734, 412, 264, 3535, 295, 34180, 322, 11, 291, 458, 11, 2452, 51234], "temperature": 0.0, "avg_logprob": -0.2184250665747601, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.00678574712947011}, {"id": 746, "seek": 291914, "start": 2936.54, "end": 2940.7799999999997, "text": " learning around 2010, they are all about that notion that we", "tokens": [51234, 2539, 926, 9657, 11, 436, 366, 439, 466, 300, 10710, 300, 321, 51446], "temperature": 0.0, "avg_logprob": -0.2184250665747601, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.00678574712947011}, {"id": 747, "seek": 291914, "start": 2940.7799999999997, "end": 2945.8599999999997, "text": " would like these unsupervised learning procedures to discover", "tokens": [51446, 576, 411, 613, 2693, 12879, 24420, 2539, 13846, 281, 4411, 51700], "temperature": 0.0, "avg_logprob": -0.2184250665747601, "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.00678574712947011}, {"id": 748, "seek": 294586, "start": 2945.86, "end": 2950.78, "text": " these abstract factors, as we call them. But now I think it's", "tokens": [50364, 613, 12649, 6771, 11, 382, 321, 818, 552, 13, 583, 586, 286, 519, 309, 311, 50610], "temperature": 0.0, "avg_logprob": -0.13292964299519858, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.0035920729860663414}, {"id": 749, "seek": 294586, "start": 2950.78, "end": 2954.46, "text": " not just the factors like the variables, but it's also more", "tokens": [50610, 406, 445, 264, 6771, 411, 264, 9102, 11, 457, 309, 311, 611, 544, 50794], "temperature": 0.0, "avg_logprob": -0.13292964299519858, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.0035920729860663414}, {"id": 750, "seek": 294586, "start": 2954.46, "end": 2958.1, "text": " importantly, even how they're related to each other, which in", "tokens": [50794, 8906, 11, 754, 577, 436, 434, 4077, 281, 1184, 661, 11, 597, 294, 50976], "temperature": 0.0, "avg_logprob": -0.13292964299519858, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.0035920729860663414}, {"id": 751, "seek": 294586, "start": 2958.1, "end": 2964.46, "text": " the causal language is what we call causal mechanisms. And so", "tokens": [50976, 264, 38755, 2856, 307, 437, 321, 818, 38755, 15902, 13, 400, 370, 51294], "temperature": 0.0, "avg_logprob": -0.13292964299519858, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.0035920729860663414}, {"id": 752, "seek": 294586, "start": 2964.5, "end": 2967.26, "text": " here's a fundamental way of thinking about this. If you", "tokens": [51296, 510, 311, 257, 8088, 636, 295, 1953, 466, 341, 13, 759, 291, 51434], "temperature": 0.0, "avg_logprob": -0.13292964299519858, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.0035920729860663414}, {"id": 753, "seek": 294586, "start": 2967.26, "end": 2971.02, "text": " don't introduce the abstract kind of structure that exists in", "tokens": [51434, 500, 380, 5366, 264, 12649, 733, 295, 3877, 300, 8198, 294, 51622], "temperature": 0.0, "avg_logprob": -0.13292964299519858, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.0035920729860663414}, {"id": 754, "seek": 297102, "start": 2971.02, "end": 2976.9, "text": " the world, then representing p of x, the input distribution is", "tokens": [50364, 264, 1002, 11, 550, 13460, 280, 295, 2031, 11, 264, 4846, 7316, 307, 50658], "temperature": 0.0, "avg_logprob": -0.1450474045493386, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.02884860709309578}, {"id": 755, "seek": 297102, "start": 2976.9, "end": 2980.74, "text": " very difficult. It's, in other words, you'll need a lot of data", "tokens": [50658, 588, 2252, 13, 467, 311, 11, 294, 661, 2283, 11, 291, 603, 643, 257, 688, 295, 1412, 50850], "temperature": 0.0, "avg_logprob": -0.1450474045493386, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.02884860709309578}, {"id": 756, "seek": 297102, "start": 2980.74, "end": 2984.54, "text": " to learn it. And it's not going to be generalizing very well. The", "tokens": [50850, 281, 1466, 309, 13, 400, 309, 311, 406, 516, 281, 312, 2674, 3319, 588, 731, 13, 440, 51040], "temperature": 0.0, "avg_logprob": -0.1450474045493386, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.02884860709309578}, {"id": 757, "seek": 297102, "start": 2984.54, "end": 2989.2599999999998, "text": " whole point of abstraction is that it gives you very powerful", "tokens": [51040, 1379, 935, 295, 37765, 307, 300, 309, 2709, 291, 588, 4005, 51276], "temperature": 0.0, "avg_logprob": -0.1450474045493386, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.02884860709309578}, {"id": 758, "seek": 297102, "start": 2989.2599999999998, "end": 2991.62, "text": " abilities to generalize to new settings, including out of", "tokens": [51276, 11582, 281, 2674, 1125, 281, 777, 6257, 11, 3009, 484, 295, 51394], "temperature": 0.0, "avg_logprob": -0.1450474045493386, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.02884860709309578}, {"id": 759, "seek": 297102, "start": 2991.62, "end": 2993.86, "text": " distribution, which is one of the hardest topics in machine", "tokens": [51394, 7316, 11, 597, 307, 472, 295, 264, 13158, 8378, 294, 3479, 51506], "temperature": 0.0, "avg_logprob": -0.1450474045493386, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.02884860709309578}, {"id": 760, "seek": 297102, "start": 2993.86, "end": 2997.1, "text": " learning right now. How do we extend what we do so that it", "tokens": [51506, 2539, 558, 586, 13, 1012, 360, 321, 10101, 437, 321, 360, 370, 300, 309, 51668], "temperature": 0.0, "avg_logprob": -0.1450474045493386, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.02884860709309578}, {"id": 761, "seek": 299710, "start": 2997.1, "end": 3001.2999999999997, "text": " generalizes well in new settings? And thinking causally", "tokens": [50364, 2674, 5660, 731, 294, 777, 6257, 30, 400, 1953, 3302, 379, 50574], "temperature": 0.0, "avg_logprob": -0.14147782855563693, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.005137091968208551}, {"id": 762, "seek": 299710, "start": 3001.2999999999997, "end": 3005.9, "text": " about these abstract causal dependencies, as the things that", "tokens": [50574, 466, 613, 12649, 38755, 36606, 11, 382, 264, 721, 300, 50804], "temperature": 0.0, "avg_logprob": -0.14147782855563693, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.005137091968208551}, {"id": 763, "seek": 299710, "start": 3005.9, "end": 3010.58, "text": " are preserved across changes in distribution, like, if I go to", "tokens": [50804, 366, 22242, 2108, 2962, 294, 7316, 11, 411, 11, 498, 286, 352, 281, 51038], "temperature": 0.0, "avg_logprob": -0.14147782855563693, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.005137091968208551}, {"id": 764, "seek": 299710, "start": 3010.58, "end": 3014.8199999999997, "text": " the moon, it's the same laws of physics, but the distribution is", "tokens": [51038, 264, 7135, 11, 309, 311, 264, 912, 6064, 295, 10649, 11, 457, 264, 7316, 307, 51250], "temperature": 0.0, "avg_logprob": -0.14147782855563693, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.005137091968208551}, {"id": 765, "seek": 299710, "start": 3014.8199999999997, "end": 3018.46, "text": " very different. How do I generalize, you know, across such", "tokens": [51250, 588, 819, 13, 1012, 360, 286, 2674, 1125, 11, 291, 458, 11, 2108, 1270, 51432], "temperature": 0.0, "avg_logprob": -0.14147782855563693, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.005137091968208551}, {"id": 766, "seek": 299710, "start": 3018.46, "end": 3023.98, "text": " changes in distribution? It's because the learner is us, you", "tokens": [51432, 2962, 294, 7316, 30, 467, 311, 570, 264, 33347, 307, 505, 11, 291, 51708], "temperature": 0.0, "avg_logprob": -0.14147782855563693, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.005137091968208551}, {"id": 767, "seek": 302398, "start": 3023.98, "end": 3028.06, "text": " know, if we, if we were, if we had the right education, has", "tokens": [50364, 458, 11, 498, 321, 11, 498, 321, 645, 11, 498, 321, 632, 264, 558, 3309, 11, 575, 50568], "temperature": 0.0, "avg_logprob": -0.15006914544612804, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.001986938528716564}, {"id": 768, "seek": 302398, "start": 3028.06, "end": 3031.98, "text": " figured out the underlying, at least, you know, enough of the", "tokens": [50568, 8932, 484, 264, 14217, 11, 412, 1935, 11, 291, 458, 11, 1547, 295, 264, 50764], "temperature": 0.0, "avg_logprob": -0.15006914544612804, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.001986938528716564}, {"id": 769, "seek": 302398, "start": 3031.98, "end": 3035.7400000000002, "text": " underlying causal mechanisms, that we can be transported in a", "tokens": [50764, 14217, 38755, 15902, 11, 300, 321, 393, 312, 29373, 294, 257, 50952], "temperature": 0.0, "avg_logprob": -0.15006914544612804, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.001986938528716564}, {"id": 770, "seek": 302398, "start": 3035.7400000000002, "end": 3039.94, "text": " different world, but where there's the same laws of physics,", "tokens": [50952, 819, 1002, 11, 457, 689, 456, 311, 264, 912, 6064, 295, 10649, 11, 51162], "temperature": 0.0, "avg_logprob": -0.15006914544612804, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.001986938528716564}, {"id": 771, "seek": 302398, "start": 3040.34, "end": 3044.3, "text": " and we can predict what's going to happen, even though it looks", "tokens": [51182, 293, 321, 393, 6069, 437, 311, 516, 281, 1051, 11, 754, 1673, 309, 1542, 51380], "temperature": 0.0, "avg_logprob": -0.15006914544612804, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.001986938528716564}, {"id": 772, "seek": 302398, "start": 3044.3, "end": 3047.86, "text": " completely different from, you know, our training environment.", "tokens": [51380, 2584, 819, 490, 11, 291, 458, 11, 527, 3097, 2823, 13, 51558], "temperature": 0.0, "avg_logprob": -0.15006914544612804, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.001986938528716564}, {"id": 773, "seek": 304786, "start": 3048.38, "end": 3055.82, "text": " So the, the idea of extraction is really that if you introduce", "tokens": [50390, 407, 264, 11, 264, 1558, 295, 30197, 307, 534, 300, 498, 291, 5366, 50762], "temperature": 0.0, "avg_logprob": -0.1665504162128155, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.0057268342934548855}, {"id": 774, "seek": 304786, "start": 3055.82, "end": 3061.5, "text": " abstractions, the description length of the data becomes way", "tokens": [50762, 12649, 626, 11, 264, 3855, 4641, 295, 264, 1412, 3643, 636, 51046], "temperature": 0.0, "avg_logprob": -0.1665504162128155, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.0057268342934548855}, {"id": 775, "seek": 304786, "start": 3061.5, "end": 3063.78, "text": " smaller. And that's why you get generalization.", "tokens": [51046, 4356, 13, 400, 300, 311, 983, 291, 483, 2674, 2144, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1665504162128155, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.0057268342934548855}, {"id": 776, "seek": 304786, "start": 3065.38, "end": 3066.06, "text": " Absolutely.", "tokens": [51240, 7021, 13, 51274], "temperature": 0.0, "avg_logprob": -0.1665504162128155, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.0057268342934548855}, {"id": 777, "seek": 304786, "start": 3066.2200000000003, "end": 3070.2200000000003, "text": " I'm fascinated by these abstract categories. I think it's the", "tokens": [51282, 286, 478, 24597, 538, 613, 12649, 10479, 13, 286, 519, 309, 311, 264, 51482], "temperature": 0.0, "avg_logprob": -0.1665504162128155, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.0057268342934548855}, {"id": 778, "seek": 304786, "start": 3070.2200000000003, "end": 3073.06, "text": " most exciting thing in AI. I mean, Douglas Hofstadter spoke", "tokens": [51482, 881, 4670, 551, 294, 7318, 13, 286, 914, 11, 23010, 37379, 48299, 391, 7179, 51624], "temperature": 0.0, "avg_logprob": -0.1665504162128155, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.0057268342934548855}, {"id": 779, "seek": 304786, "start": 3073.06, "end": 3076.86, "text": " about cognitive categories, like the concept of sour grapes,", "tokens": [51624, 466, 15605, 10479, 11, 411, 264, 3410, 295, 11006, 28032, 11, 51814], "temperature": 0.0, "avg_logprob": -0.1665504162128155, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.0057268342934548855}, {"id": 780, "seek": 307686, "start": 3076.86, "end": 3079.9, "text": " for example, to represent the certain thing. And almost", "tokens": [50364, 337, 1365, 11, 281, 2906, 264, 1629, 551, 13, 400, 1920, 50516], "temperature": 0.0, "avg_logprob": -0.1216493043743196, "compression_ratio": 1.7705479452054795, "no_speech_prob": 0.0014270299579948187}, {"id": 781, "seek": 307686, "start": 3079.9, "end": 3083.7400000000002, "text": " magically, our brain seems to arrange these cognitive", "tokens": [50516, 39763, 11, 527, 3567, 2544, 281, 9424, 613, 15605, 50708], "temperature": 0.0, "avg_logprob": -0.1216493043743196, "compression_ratio": 1.7705479452054795, "no_speech_prob": 0.0014270299579948187}, {"id": 782, "seek": 307686, "start": 3083.7400000000002, "end": 3085.6600000000003, "text": " categories. And it's not entirely clear to me whether they're", "tokens": [50708, 10479, 13, 400, 309, 311, 406, 7696, 1850, 281, 385, 1968, 436, 434, 50804], "temperature": 0.0, "avg_logprob": -0.1216493043743196, "compression_ratio": 1.7705479452054795, "no_speech_prob": 0.0014270299579948187}, {"id": 783, "seek": 307686, "start": 3085.6600000000003, "end": 3089.26, "text": " an emergent phenomenon, or whether it's some other process.", "tokens": [50804, 364, 4345, 6930, 14029, 11, 420, 1968, 309, 311, 512, 661, 1399, 13, 50984], "temperature": 0.0, "avg_logprob": -0.1216493043743196, "compression_ratio": 1.7705479452054795, "no_speech_prob": 0.0014270299579948187}, {"id": 784, "seek": 307686, "start": 3089.54, "end": 3091.98, "text": " But the modes that you're discovering in G flow nets,", "tokens": [50998, 583, 264, 14068, 300, 291, 434, 24773, 294, 460, 3095, 36170, 11, 51120], "temperature": 0.0, "avg_logprob": -0.1216493043743196, "compression_ratio": 1.7705479452054795, "no_speech_prob": 0.0014270299579948187}, {"id": 785, "seek": 307686, "start": 3091.98, "end": 3095.54, "text": " they're a kind of category, these cognitive categories that I", "tokens": [51120, 436, 434, 257, 733, 295, 7719, 11, 613, 15605, 10479, 300, 286, 51298], "temperature": 0.0, "avg_logprob": -0.1216493043743196, "compression_ratio": 1.7705479452054795, "no_speech_prob": 0.0014270299579948187}, {"id": 786, "seek": 307686, "start": 3095.54, "end": 3098.1800000000003, "text": " just spoke about our abstractions, also things like", "tokens": [51298, 445, 7179, 466, 527, 12649, 626, 11, 611, 721, 411, 51430], "temperature": 0.0, "avg_logprob": -0.1216493043743196, "compression_ratio": 1.7705479452054795, "no_speech_prob": 0.0014270299579948187}, {"id": 787, "seek": 307686, "start": 3098.1800000000003, "end": 3101.06, "text": " causality and geometric deep learning that they are kinds of", "tokens": [51430, 3302, 1860, 293, 33246, 2452, 2539, 300, 436, 366, 3685, 295, 51574], "temperature": 0.0, "avg_logprob": -0.1216493043743196, "compression_ratio": 1.7705479452054795, "no_speech_prob": 0.0014270299579948187}, {"id": 788, "seek": 307686, "start": 3101.06, "end": 3103.42, "text": " categories. But I've always had this intuition that deep", "tokens": [51574, 10479, 13, 583, 286, 600, 1009, 632, 341, 24002, 300, 2452, 51692], "temperature": 0.0, "avg_logprob": -0.1216493043743196, "compression_ratio": 1.7705479452054795, "no_speech_prob": 0.0014270299579948187}, {"id": 789, "seek": 310342, "start": 3103.42, "end": 3107.7000000000003, "text": " learning doesn't learn the categories on its own, it needs", "tokens": [50364, 2539, 1177, 380, 1466, 264, 10479, 322, 1080, 1065, 11, 309, 2203, 50578], "temperature": 0.0, "avg_logprob": -0.1004603385925293, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0027978764846920967}, {"id": 790, "seek": 310342, "start": 3107.7000000000003, "end": 3112.2200000000003, "text": " humans to kind of put priors into the model, as we do with", "tokens": [50578, 6255, 281, 733, 295, 829, 1790, 830, 666, 264, 2316, 11, 382, 321, 360, 365, 50804], "temperature": 0.0, "avg_logprob": -0.1004603385925293, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0027978764846920967}, {"id": 791, "seek": 310342, "start": 3112.26, "end": 3115.1, "text": " geometric deep learning. Do you think that that will always be", "tokens": [50806, 33246, 2452, 2539, 13, 1144, 291, 519, 300, 300, 486, 1009, 312, 50948], "temperature": 0.0, "avg_logprob": -0.1004603385925293, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0027978764846920967}, {"id": 792, "seek": 310342, "start": 3115.1, "end": 3117.38, "text": " the case? Or can we have that meta level of learning?", "tokens": [50948, 264, 1389, 30, 1610, 393, 321, 362, 300, 19616, 1496, 295, 2539, 30, 51062], "temperature": 0.0, "avg_logprob": -0.1004603385925293, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0027978764846920967}, {"id": 793, "seek": 310342, "start": 3118.1800000000003, "end": 3122.82, "text": " Yes. What I really want to do is build machines that can", "tokens": [51102, 1079, 13, 708, 286, 534, 528, 281, 360, 307, 1322, 8379, 300, 393, 51334], "temperature": 0.0, "avg_logprob": -0.1004603385925293, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0027978764846920967}, {"id": 794, "seek": 310342, "start": 3122.82, "end": 3126.14, "text": " discover their own semantic categories, abstract ones that", "tokens": [51334, 4411, 641, 1065, 47982, 10479, 11, 12649, 2306, 300, 51500], "temperature": 0.0, "avg_logprob": -0.1004603385925293, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0027978764846920967}, {"id": 795, "seek": 310342, "start": 3126.14, "end": 3129.7400000000002, "text": " really help them understand the world. And of course, they're", "tokens": [51500, 534, 854, 552, 1223, 264, 1002, 13, 400, 295, 1164, 11, 436, 434, 51680], "temperature": 0.0, "avg_logprob": -0.1004603385925293, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0027978764846920967}, {"id": 796, "seek": 310342, "start": 3129.7400000000002, "end": 3133.2200000000003, "text": " going to learn, you know, better and faster if we help them just", "tokens": [51680, 516, 281, 1466, 11, 291, 458, 11, 1101, 293, 4663, 498, 321, 854, 552, 445, 51854], "temperature": 0.0, "avg_logprob": -0.1004603385925293, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0027978764846920967}, {"id": 797, "seek": 313322, "start": 3133.22, "end": 3135.7, "text": " like, you know, we teach kids, we don't let them discover the", "tokens": [50364, 411, 11, 291, 458, 11, 321, 2924, 2301, 11, 321, 500, 380, 718, 552, 4411, 264, 50488], "temperature": 0.0, "avg_logprob": -0.11217702021364306, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0005611951928585768}, {"id": 798, "seek": 313322, "start": 3135.7, "end": 3141.7799999999997, "text": " world by themselves. But we do have an ability to invent new", "tokens": [50488, 1002, 538, 2969, 13, 583, 321, 360, 362, 364, 3485, 281, 7962, 777, 50792], "temperature": 0.0, "avg_logprob": -0.11217702021364306, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0005611951928585768}, {"id": 799, "seek": 313322, "start": 3141.7799999999997, "end": 3144.02, "text": " categories. That's what scientists do all the time,", "tokens": [50792, 10479, 13, 663, 311, 437, 7708, 360, 439, 264, 565, 11, 50904], "temperature": 0.0, "avg_logprob": -0.11217702021364306, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0005611951928585768}, {"id": 800, "seek": 313322, "start": 3144.02, "end": 3149.3799999999997, "text": " right? Or artists and, you know, writers and philosophers and", "tokens": [50904, 558, 30, 1610, 6910, 293, 11, 291, 458, 11, 13491, 293, 36839, 293, 51172], "temperature": 0.0, "avg_logprob": -0.11217702021364306, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0005611951928585768}, {"id": 801, "seek": 313322, "start": 3149.3799999999997, "end": 3152.7, "text": " scholars, and ordinary people who find new solutions to", "tokens": [51172, 8553, 11, 293, 10547, 561, 567, 915, 777, 6547, 281, 51338], "temperature": 0.0, "avg_logprob": -0.11217702021364306, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0005611951928585768}, {"id": 802, "seek": 313322, "start": 3152.7, "end": 3156.06, "text": " problems, we do that all the time, our brain is a machine", "tokens": [51338, 2740, 11, 321, 360, 300, 439, 264, 565, 11, 527, 3567, 307, 257, 3479, 51506], "temperature": 0.0, "avg_logprob": -0.11217702021364306, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0005611951928585768}, {"id": 803, "seek": 313322, "start": 3156.1, "end": 3159.62, "text": " discovers new abstractions. Of course, that usually it's just", "tokens": [51508, 44522, 777, 12649, 626, 13, 2720, 1164, 11, 300, 2673, 309, 311, 445, 51684], "temperature": 0.0, "avg_logprob": -0.11217702021364306, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0005611951928585768}, {"id": 804, "seek": 313322, "start": 3159.62, "end": 3162.58, "text": " like one little bit on top of all the things we got from our", "tokens": [51684, 411, 472, 707, 857, 322, 1192, 295, 439, 264, 721, 321, 658, 490, 527, 51832], "temperature": 0.0, "avg_logprob": -0.11217702021364306, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0005611951928585768}, {"id": 805, "seek": 316258, "start": 3162.62, "end": 3168.18, "text": " cultural input. But but that's the ability that we don't have", "tokens": [50366, 6988, 4846, 13, 583, 457, 300, 311, 264, 3485, 300, 321, 500, 380, 362, 50644], "temperature": 0.0, "avg_logprob": -0.11926804059817467, "compression_ratio": 1.6720430107526882, "no_speech_prob": 0.0009249678114429116}, {"id": 806, "seek": 316258, "start": 3168.18, "end": 3173.02, "text": " right now in machine learning. And that is going to, I think, be", "tokens": [50644, 558, 586, 294, 3479, 2539, 13, 400, 300, 307, 516, 281, 11, 286, 519, 11, 312, 50886], "temperature": 0.0, "avg_logprob": -0.11926804059817467, "compression_ratio": 1.6720430107526882, "no_speech_prob": 0.0009249678114429116}, {"id": 807, "seek": 316258, "start": 3173.02, "end": 3176.5, "text": " a huge advantage. So now we're not in reinforcement learning,", "tokens": [50886, 257, 2603, 5002, 13, 407, 586, 321, 434, 406, 294, 29280, 2539, 11, 51060], "temperature": 0.0, "avg_logprob": -0.11926804059817467, "compression_ratio": 1.6720430107526882, "no_speech_prob": 0.0009249678114429116}, {"id": 808, "seek": 316258, "start": 3176.5, "end": 3179.62, "text": " we're not in active learning, we're talking about unsupervised", "tokens": [51060, 321, 434, 406, 294, 4967, 2539, 11, 321, 434, 1417, 466, 2693, 12879, 24420, 51216], "temperature": 0.0, "avg_logprob": -0.11926804059817467, "compression_ratio": 1.6720430107526882, "no_speech_prob": 0.0009249678114429116}, {"id": 809, "seek": 316258, "start": 3179.62, "end": 3184.58, "text": " learning. So we're talking about how can a machine discover", "tokens": [51216, 2539, 13, 407, 321, 434, 1417, 466, 577, 393, 257, 3479, 4411, 51464], "temperature": 0.0, "avg_logprob": -0.11926804059817467, "compression_ratio": 1.6720430107526882, "no_speech_prob": 0.0009249678114429116}, {"id": 810, "seek": 318458, "start": 3184.58, "end": 3194.2599999999998, "text": " these often discrete concepts that somehow help it understand.", "tokens": [50364, 613, 2049, 27706, 10392, 300, 6063, 854, 309, 1223, 13, 50848], "temperature": 0.0, "avg_logprob": -0.1917802034798315, "compression_ratio": 1.5246913580246915, "no_speech_prob": 0.1141323372721672}, {"id": 811, "seek": 318458, "start": 3194.2599999999998, "end": 3197.02, "text": " So in other words, build a compact understanding of lots of", "tokens": [50848, 407, 294, 661, 2283, 11, 1322, 257, 14679, 3701, 295, 3195, 295, 50986], "temperature": 0.0, "avg_logprob": -0.1917802034798315, "compression_ratio": 1.5246913580246915, "no_speech_prob": 0.1141323372721672}, {"id": 812, "seek": 318458, "start": 3197.02, "end": 3202.38, "text": " things that generalize across many settings. And yeah, that", "tokens": [50986, 721, 300, 2674, 1125, 2108, 867, 6257, 13, 400, 1338, 11, 300, 51254], "temperature": 0.0, "avg_logprob": -0.1917802034798315, "compression_ratio": 1.5246913580246915, "no_speech_prob": 0.1141323372721672}, {"id": 813, "seek": 318458, "start": 3202.42, "end": 3208.9, "text": " that's that the path to build that is, is becoming more and more", "tokens": [51256, 300, 311, 300, 264, 3100, 281, 1322, 300, 307, 11, 307, 5617, 544, 293, 544, 51580], "temperature": 0.0, "avg_logprob": -0.1917802034798315, "compression_ratio": 1.5246913580246915, "no_speech_prob": 0.1141323372721672}, {"id": 814, "seek": 320890, "start": 3209.9, "end": 3215.86, "text": " firm in my mind, as I move forward with G flow nets. So as a", "tokens": [50414, 6174, 294, 452, 1575, 11, 382, 286, 1286, 2128, 365, 460, 3095, 36170, 13, 407, 382, 257, 50712], "temperature": 0.0, "avg_logprob": -0.20978397793240017, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.004679563455283642}, {"id": 815, "seek": 320890, "start": 3215.86, "end": 3219.42, "text": " clue, there was a paper we had recently, I think in Europe's", "tokens": [50712, 13602, 11, 456, 390, 257, 3035, 321, 632, 3938, 11, 286, 519, 294, 3315, 311, 50890], "temperature": 0.0, "avg_logprob": -0.20978397793240017, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.004679563455283642}, {"id": 816, "seek": 320890, "start": 3219.82, "end": 3223.82, "text": " on that's connected to the global workspace theory that says", "tokens": [50910, 322, 300, 311, 4582, 281, 264, 4338, 32706, 5261, 300, 1619, 51110], "temperature": 0.0, "avg_logprob": -0.20978397793240017, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.004679563455283642}, {"id": 817, "seek": 320890, "start": 3223.82, "end": 3227.5, "text": " that it's about discrete valued neural communication, I think", "tokens": [51110, 300, 309, 311, 466, 27706, 22608, 18161, 6101, 11, 286, 519, 51294], "temperature": 0.0, "avg_logprob": -0.20978397793240017, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.004679563455283642}, {"id": 818, "seek": 320890, "start": 3227.5, "end": 3231.86, "text": " is a title where the one interesting intuition here is", "tokens": [51294, 307, 257, 4876, 689, 264, 472, 1880, 24002, 510, 307, 51512], "temperature": 0.0, "avg_logprob": -0.20978397793240017, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.004679563455283642}, {"id": 819, "seek": 320890, "start": 3231.86, "end": 3238.6600000000003, "text": " connected to this is if you if you constrain the communication", "tokens": [51512, 4582, 281, 341, 307, 498, 291, 498, 291, 1817, 7146, 264, 6101, 51852], "temperature": 0.0, "avg_logprob": -0.20978397793240017, "compression_ratio": 1.6682027649769586, "no_speech_prob": 0.004679563455283642}, {"id": 820, "seek": 323866, "start": 3238.66, "end": 3241.06, "text": " between different modules, say in the brain or in machine", "tokens": [50364, 1296, 819, 16679, 11, 584, 294, 264, 3567, 420, 294, 3479, 50484], "temperature": 0.0, "avg_logprob": -0.17701040688207595, "compression_ratio": 1.796, "no_speech_prob": 0.0036423318088054657}, {"id": 821, "seek": 323866, "start": 3241.06, "end": 3244.54, "text": " learning system, to use as few bits as possible and discrete is", "tokens": [50484, 2539, 1185, 11, 281, 764, 382, 1326, 9239, 382, 1944, 293, 27706, 307, 50658], "temperature": 0.0, "avg_logprob": -0.17701040688207595, "compression_ratio": 1.796, "no_speech_prob": 0.0036423318088054657}, {"id": 822, "seek": 323866, "start": 3244.54, "end": 3248.22, "text": " the way to get the very few bits. You can get better", "tokens": [50658, 264, 636, 281, 483, 264, 588, 1326, 9239, 13, 509, 393, 483, 1101, 50842], "temperature": 0.0, "avg_logprob": -0.17701040688207595, "compression_ratio": 1.796, "no_speech_prob": 0.0036423318088054657}, {"id": 823, "seek": 323866, "start": 3248.22, "end": 3252.18, "text": " generalization. And there are good reasons for that that we", "tokens": [50842, 2674, 2144, 13, 400, 456, 366, 665, 4112, 337, 300, 300, 321, 51040], "temperature": 0.0, "avg_logprob": -0.17701040688207595, "compression_ratio": 1.796, "no_speech_prob": 0.0036423318088054657}, {"id": 824, "seek": 323866, "start": 3252.18, "end": 3254.98, "text": " try to explain in the paper. But but that's, that's it, you", "tokens": [51040, 853, 281, 2903, 294, 264, 3035, 13, 583, 457, 300, 311, 11, 300, 311, 309, 11, 291, 51180], "temperature": 0.0, "avg_logprob": -0.17701040688207595, "compression_ratio": 1.796, "no_speech_prob": 0.0036423318088054657}, {"id": 825, "seek": 323866, "start": 3254.98, "end": 3259.98, "text": " know, there's a clue here that discrete concepts emerge as a", "tokens": [51180, 458, 11, 456, 311, 257, 13602, 510, 300, 27706, 10392, 21511, 382, 257, 51430], "temperature": 0.0, "avg_logprob": -0.17701040688207595, "compression_ratio": 1.796, "no_speech_prob": 0.0036423318088054657}, {"id": 826, "seek": 323866, "start": 3259.98, "end": 3261.46, "text": " way to get better generalization.", "tokens": [51430, 636, 281, 483, 1101, 2674, 2144, 13, 51504], "temperature": 0.0, "avg_logprob": -0.17701040688207595, "compression_ratio": 1.796, "no_speech_prob": 0.0036423318088054657}, {"id": 827, "seek": 323866, "start": 3262.8599999999997, "end": 3267.8199999999997, "text": " You you mentioned before, and in terms of discreteness, and", "tokens": [51574, 509, 291, 2835, 949, 11, 293, 294, 2115, 295, 2983, 35383, 442, 11, 293, 51822], "temperature": 0.0, "avg_logprob": -0.17701040688207595, "compression_ratio": 1.796, "no_speech_prob": 0.0036423318088054657}, {"id": 828, "seek": 326782, "start": 3267.86, "end": 3272.1400000000003, "text": " what you mentioned before with graphs being very fundamental, it", "tokens": [50366, 437, 291, 2835, 949, 365, 24877, 885, 588, 8088, 11, 309, 50580], "temperature": 0.0, "avg_logprob": -0.15395671330141217, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0038763582706451416}, {"id": 829, "seek": 326782, "start": 3272.1400000000003, "end": 3275.6200000000003, "text": " connects a little bit back to a paper that you, I think,", "tokens": [50580, 16967, 257, 707, 857, 646, 281, 257, 3035, 300, 291, 11, 286, 519, 11, 50754], "temperature": 0.0, "avg_logprob": -0.15395671330141217, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0038763582706451416}, {"id": 830, "seek": 326782, "start": 3275.6200000000003, "end": 3280.86, "text": " provocatively titled the consciousness prior, where where", "tokens": [50754, 24568, 19020, 19841, 264, 10081, 4059, 11, 689, 689, 51016], "temperature": 0.0, "avg_logprob": -0.15395671330141217, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0038763582706451416}, {"id": 831, "seek": 326782, "start": 3280.86, "end": 3284.98, "text": " you connect sort of the ideas of attention, sparse factor,", "tokens": [51016, 291, 1745, 1333, 295, 264, 3487, 295, 3202, 11, 637, 11668, 5952, 11, 51222], "temperature": 0.0, "avg_logprob": -0.15395671330141217, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0038763582706451416}, {"id": 832, "seek": 326782, "start": 3284.98, "end": 3288.6600000000003, "text": " graphs, language, things being discreet, things being", "tokens": [51222, 24877, 11, 2856, 11, 721, 885, 2983, 4751, 11, 721, 885, 51406], "temperature": 0.0, "avg_logprob": -0.15395671330141217, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0038763582706451416}, {"id": 833, "seek": 326782, "start": 3288.6600000000003, "end": 3294.5, "text": " describable by language, right? And, and I find that all to be", "tokens": [51406, 2189, 65, 712, 538, 2856, 11, 558, 30, 400, 11, 293, 286, 915, 300, 439, 281, 312, 51698], "temperature": 0.0, "avg_logprob": -0.15395671330141217, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0038763582706451416}, {"id": 834, "seek": 329450, "start": 3294.54, "end": 3300.1, "text": " very interesting. On the topic of consciousness, we would be, it", "tokens": [50366, 588, 1880, 13, 1282, 264, 4829, 295, 10081, 11, 321, 576, 312, 11, 309, 50644], "temperature": 0.0, "avg_logprob": -0.17946279891813643, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.04866667464375496}, {"id": 835, "seek": 329450, "start": 3300.1, "end": 3303.1, "text": " would not be appropriate for us to not put this question to you.", "tokens": [50644, 576, 406, 312, 6854, 337, 505, 281, 406, 829, 341, 1168, 281, 291, 13, 50794], "temperature": 0.0, "avg_logprob": -0.17946279891813643, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.04866667464375496}, {"id": 836, "seek": 329450, "start": 3303.1, "end": 3307.06, "text": " So you're not, you're not very active on Twitter, which is", "tokens": [50794, 407, 291, 434, 406, 11, 291, 434, 406, 588, 4967, 322, 5794, 11, 597, 307, 50992], "temperature": 0.0, "avg_logprob": -0.17946279891813643, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.04866667464375496}, {"id": 837, "seek": 329450, "start": 3307.06, "end": 3310.94, "text": " probably why you're so productive. But if currently,", "tokens": [50992, 1391, 983, 291, 434, 370, 13304, 13, 583, 498, 4362, 11, 51186], "temperature": 0.0, "avg_logprob": -0.17946279891813643, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.04866667464375496}, {"id": 838, "seek": 329450, "start": 3311.06, "end": 3314.58, "text": " there is a bit of a of a thing happening on Twitter, namely,", "tokens": [51192, 456, 307, 257, 857, 295, 257, 295, 257, 551, 2737, 322, 5794, 11, 20926, 11, 51368], "temperature": 0.0, "avg_logprob": -0.17946279891813643, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.04866667464375496}, {"id": 839, "seek": 329450, "start": 3315.02, "end": 3321.9, "text": " Ilya Satskever of Open AI has tweeted out a seemingly innocuous", "tokens": [51390, 286, 45106, 318, 1720, 330, 331, 295, 7238, 7318, 575, 25646, 484, 257, 18709, 10843, 12549, 51734], "temperature": 0.0, "avg_logprob": -0.17946279891813643, "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.04866667464375496}, {"id": 840, "seek": 332190, "start": 3322.1800000000003, "end": 3327.6600000000003, "text": " tweet saying, it may be that today's large neural networks are", "tokens": [50378, 15258, 1566, 11, 309, 815, 312, 300, 965, 311, 2416, 18161, 9590, 366, 50652], "temperature": 0.0, "avg_logprob": -0.12440476947360568, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.041802797466516495}, {"id": 841, "seek": 332190, "start": 3327.6600000000003, "end": 3333.58, "text": " slightly conscious, which has resulted in quite a, let's say,", "tokens": [50652, 4748, 6648, 11, 597, 575, 18753, 294, 1596, 257, 11, 718, 311, 584, 11, 50948], "temperature": 0.0, "avg_logprob": -0.12440476947360568, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.041802797466516495}, {"id": 842, "seek": 332190, "start": 3333.58, "end": 3338.3, "text": " a storm on of people agreeing, disagreeing. Obviously, he's", "tokens": [50948, 257, 7679, 322, 295, 561, 36900, 11, 14091, 278, 13, 7580, 11, 415, 311, 51184], "temperature": 0.0, "avg_logprob": -0.12440476947360568, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.041802797466516495}, {"id": 843, "seek": 332190, "start": 3338.3, "end": 3341.3, "text": " he's talking about maybe, you know, the large language models", "tokens": [51184, 415, 311, 1417, 466, 1310, 11, 291, 458, 11, 264, 2416, 2856, 5245, 51334], "temperature": 0.0, "avg_logprob": -0.12440476947360568, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.041802797466516495}, {"id": 844, "seek": 332190, "start": 3341.3, "end": 3344.54, "text": " we have today, which do incorporate a lot of the things", "tokens": [51334, 321, 362, 965, 11, 597, 360, 16091, 257, 688, 295, 264, 721, 51496], "temperature": 0.0, "avg_logprob": -0.12440476947360568, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.041802797466516495}, {"id": 845, "seek": 332190, "start": 3344.54, "end": 3347.86, "text": " you talk about, they do incorporate attention mechanisms,", "tokens": [51496, 291, 751, 466, 11, 436, 360, 16091, 3202, 15902, 11, 51662], "temperature": 0.0, "avg_logprob": -0.12440476947360568, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.041802797466516495}, {"id": 846, "seek": 334786, "start": 3347.94, "end": 3352.1400000000003, "text": " lots of them. Presumably, it's all one needs. They do", "tokens": [50368, 3195, 295, 552, 13, 2718, 449, 1188, 11, 309, 311, 439, 472, 2203, 13, 814, 360, 50578], "temperature": 0.0, "avg_logprob": -0.12732900182406107, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00243060989305377}, {"id": 847, "seek": 334786, "start": 3352.1400000000003, "end": 3355.26, "text": " incorporate language, they do incorporate discrete things with", "tokens": [50578, 16091, 2856, 11, 436, 360, 16091, 27706, 721, 365, 50734], "temperature": 0.0, "avg_logprob": -0.12732900182406107, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00243060989305377}, {"id": 848, "seek": 334786, "start": 3355.26, "end": 3359.1400000000003, "text": " you know, discrete tokens and so on. What do you make of a", "tokens": [50734, 291, 458, 11, 27706, 22667, 293, 370, 322, 13, 708, 360, 291, 652, 295, 257, 50928], "temperature": 0.0, "avg_logprob": -0.12732900182406107, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00243060989305377}, {"id": 849, "seek": 334786, "start": 3359.1400000000003, "end": 3362.3, "text": " statement like this? It may be that today's large neural", "tokens": [50928, 5629, 411, 341, 30, 467, 815, 312, 300, 965, 311, 2416, 18161, 51086], "temperature": 0.0, "avg_logprob": -0.12732900182406107, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00243060989305377}, {"id": 850, "seek": 334786, "start": 3362.3, "end": 3364.3, "text": " networks are slightly conscious.", "tokens": [51086, 9590, 366, 4748, 6648, 13, 51186], "temperature": 0.0, "avg_logprob": -0.12732900182406107, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00243060989305377}, {"id": 851, "seek": 334786, "start": 3365.38, "end": 3370.06, "text": " Well, this one fundamental problem with such statements,", "tokens": [51240, 1042, 11, 341, 472, 8088, 1154, 365, 1270, 12363, 11, 51474], "temperature": 0.0, "avg_logprob": -0.12732900182406107, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00243060989305377}, {"id": 852, "seek": 334786, "start": 3371.5, "end": 3375.94, "text": " which is we don't know what consciousness really is. So I", "tokens": [51546, 597, 307, 321, 500, 380, 458, 437, 10081, 534, 307, 13, 407, 286, 51768], "temperature": 0.0, "avg_logprob": -0.12732900182406107, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00243060989305377}, {"id": 853, "seek": 337594, "start": 3375.98, "end": 3381.42, "text": " think we have to have a bit of humility here. And I can't say", "tokens": [50366, 519, 321, 362, 281, 362, 257, 857, 295, 27106, 510, 13, 400, 286, 393, 380, 584, 50638], "temperature": 0.0, "avg_logprob": -0.1536362901025889, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.00034586444962769747}, {"id": 854, "seek": 337594, "start": 3381.46, "end": 3384.7400000000002, "text": " what Ilya is saying is true or not. I think that this is more to", "tokens": [50640, 437, 286, 45106, 307, 1566, 307, 2074, 420, 406, 13, 286, 519, 300, 341, 307, 544, 281, 50804], "temperature": 0.0, "avg_logprob": -0.1536362901025889, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.00034586444962769747}, {"id": 855, "seek": 337594, "start": 3384.7400000000002, "end": 3389.06, "text": " consciousness than what we have in these large language models by", "tokens": [50804, 10081, 813, 437, 321, 362, 294, 613, 2416, 2856, 5245, 538, 51020], "temperature": 0.0, "avg_logprob": -0.1536362901025889, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.00034586444962769747}, {"id": 856, "seek": 337594, "start": 3389.06, "end": 3395.54, "text": " a big gap. But that being said, and you know, we do need to work", "tokens": [51020, 257, 955, 7417, 13, 583, 300, 885, 848, 11, 293, 291, 458, 11, 321, 360, 643, 281, 589, 51344], "temperature": 0.0, "avg_logprob": -0.1536362901025889, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.00034586444962769747}, {"id": 857, "seek": 337594, "start": 3395.54, "end": 3399.58, "text": " with our colleagues in your science and kind of science who", "tokens": [51344, 365, 527, 7734, 294, 428, 3497, 293, 733, 295, 3497, 567, 51546], "temperature": 0.0, "avg_logprob": -0.1536362901025889, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.00034586444962769747}, {"id": 858, "seek": 337594, "start": 3399.58, "end": 3402.86, "text": " are trying to figure out what consciousness is from a scientific", "tokens": [51546, 366, 1382, 281, 2573, 484, 437, 10081, 307, 490, 257, 8134, 51710], "temperature": 0.0, "avg_logprob": -0.1536362901025889, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.00034586444962769747}, {"id": 859, "seek": 340286, "start": 3403.3, "end": 3407.2200000000003, "text": " perspective and philosophers who are helping also to make sense of", "tokens": [50386, 4585, 293, 36839, 567, 366, 4315, 611, 281, 652, 2020, 295, 50582], "temperature": 0.0, "avg_logprob": -0.13566602686400053, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0021798484958708286}, {"id": 860, "seek": 340286, "start": 3407.2200000000003, "end": 3414.06, "text": " that landscape. So we have to be careful with the use of those", "tokens": [50582, 300, 9661, 13, 407, 321, 362, 281, 312, 5026, 365, 264, 764, 295, 729, 50924], "temperature": 0.0, "avg_logprob": -0.13566602686400053, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0021798484958708286}, {"id": 861, "seek": 340286, "start": 3414.06, "end": 3416.6600000000003, "text": " words. And you know, I was a bit liberal in the title of my", "tokens": [50924, 2283, 13, 400, 291, 458, 11, 286, 390, 257, 857, 13767, 294, 264, 4876, 295, 452, 51054], "temperature": 0.0, "avg_logprob": -0.13566602686400053, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0021798484958708286}, {"id": 862, "seek": 340286, "start": 3416.78, "end": 3421.42, "text": " paper. And I learned a lot about consciousness since then,", "tokens": [51060, 3035, 13, 400, 286, 3264, 257, 688, 466, 10081, 1670, 550, 11, 51292], "temperature": 0.0, "avg_logprob": -0.13566602686400053, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0021798484958708286}, {"id": 863, "seek": 340286, "start": 3422.26, "end": 3425.42, "text": " learned that there's a lot that we don't understand that at the", "tokens": [51334, 3264, 300, 456, 311, 257, 688, 300, 321, 500, 380, 1223, 300, 412, 264, 51492], "temperature": 0.0, "avg_logprob": -0.13566602686400053, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0021798484958708286}, {"id": 864, "seek": 340286, "start": 3425.42, "end": 3428.5, "text": " same time, there are enough bits that we know from from", "tokens": [51492, 912, 565, 11, 456, 366, 1547, 9239, 300, 321, 458, 490, 490, 51646], "temperature": 0.0, "avg_logprob": -0.13566602686400053, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0021798484958708286}, {"id": 865, "seek": 342850, "start": 3428.54, "end": 3434.86, "text": " cognitive neuroscience that can serve as inspiration for how we", "tokens": [50366, 15605, 42762, 300, 393, 4596, 382, 10249, 337, 577, 321, 50682], "temperature": 0.0, "avg_logprob": -0.15535396575927735, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.010479239746928215}, {"id": 866, "seek": 342850, "start": 3434.86, "end": 3438.54, "text": " could build machine learning systems that have similar, say", "tokens": [50682, 727, 1322, 3479, 2539, 3652, 300, 362, 2531, 11, 584, 50866], "temperature": 0.0, "avg_logprob": -0.15535396575927735, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.010479239746928215}, {"id": 867, "seek": 342850, "start": 3438.54, "end": 3441.58, "text": " conscious processing machinery. Okay, let's not say consciousness", "tokens": [50866, 6648, 9007, 27302, 13, 1033, 11, 718, 311, 406, 584, 10081, 51018], "temperature": 0.0, "avg_logprob": -0.15535396575927735, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.010479239746928215}, {"id": 868, "seek": 342850, "start": 3441.58, "end": 3443.5, "text": " but just conscious processing machine because that's less", "tokens": [51018, 457, 445, 6648, 9007, 3479, 570, 300, 311, 1570, 51114], "temperature": 0.0, "avg_logprob": -0.15535396575927735, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.010479239746928215}, {"id": 869, "seek": 342850, "start": 3443.5, "end": 3447.3, "text": " controversial. And by the way, the word consciousness has been", "tokens": [51114, 17323, 13, 400, 538, 264, 636, 11, 264, 1349, 10081, 575, 668, 51304], "temperature": 0.0, "avg_logprob": -0.15535396575927735, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.010479239746928215}, {"id": 870, "seek": 342850, "start": 3447.3, "end": 3452.26, "text": " taboo with most of science for a long time. And it has become", "tokens": [51304, 4421, 1986, 365, 881, 295, 3497, 337, 257, 938, 565, 13, 400, 309, 575, 1813, 51552], "temperature": 0.0, "avg_logprob": -0.15535396575927735, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.010479239746928215}, {"id": 871, "seek": 342850, "start": 3452.3, "end": 3456.54, "text": " untapped, you know, the tabooed in neuroscience, because we're", "tokens": [51554, 517, 1328, 3320, 11, 291, 458, 11, 264, 4421, 1986, 292, 294, 42762, 11, 570, 321, 434, 51766], "temperature": 0.0, "avg_logprob": -0.15535396575927735, "compression_ratio": 1.8049792531120332, "no_speech_prob": 0.010479239746928215}, {"id": 872, "seek": 345654, "start": 3456.54, "end": 3459.06, "text": " starting to be able to make measurements of what's going on", "tokens": [50364, 2891, 281, 312, 1075, 281, 652, 15383, 295, 437, 311, 516, 322, 50490], "temperature": 0.0, "avg_logprob": -0.10670421773737127, "compression_ratio": 1.8680851063829786, "no_speech_prob": 0.003480849089100957}, {"id": 873, "seek": 345654, "start": 3459.1, "end": 3462.82, "text": " inside your brain, while you're doing things consciously or not", "tokens": [50492, 1854, 428, 3567, 11, 1339, 291, 434, 884, 721, 32538, 420, 406, 50678], "temperature": 0.0, "avg_logprob": -0.10670421773737127, "compression_ratio": 1.8680851063829786, "no_speech_prob": 0.003480849089100957}, {"id": 874, "seek": 345654, "start": 3462.82, "end": 3466.94, "text": " and so on and distinguish the parts that you're consciously", "tokens": [50678, 293, 370, 322, 293, 20206, 264, 3166, 300, 291, 434, 32538, 50884], "temperature": 0.0, "avg_logprob": -0.10670421773737127, "compression_ratio": 1.8680851063829786, "no_speech_prob": 0.003480849089100957}, {"id": 875, "seek": 345654, "start": 3466.94, "end": 3469.62, "text": " aware of and the parts that are there in your brain, but you're", "tokens": [50884, 3650, 295, 293, 264, 3166, 300, 366, 456, 294, 428, 3567, 11, 457, 291, 434, 51018], "temperature": 0.0, "avg_logprob": -0.10670421773737127, "compression_ratio": 1.8680851063829786, "no_speech_prob": 0.003480849089100957}, {"id": 876, "seek": 345654, "start": 3469.62, "end": 3472.14, "text": " not conscious. So we're trying to we're starting to make a lot of", "tokens": [51018, 406, 6648, 13, 407, 321, 434, 1382, 281, 321, 434, 2891, 281, 652, 257, 688, 295, 51144], "temperature": 0.0, "avg_logprob": -0.10670421773737127, "compression_ratio": 1.8680851063829786, "no_speech_prob": 0.003480849089100957}, {"id": 877, "seek": 345654, "start": 3472.14, "end": 3475.42, "text": " progress of what it means to be conscious of something or not.", "tokens": [51144, 4205, 295, 437, 309, 1355, 281, 312, 6648, 295, 746, 420, 406, 13, 51308], "temperature": 0.0, "avg_logprob": -0.10670421773737127, "compression_ratio": 1.8680851063829786, "no_speech_prob": 0.003480849089100957}, {"id": 878, "seek": 345654, "start": 3476.94, "end": 3482.18, "text": " And I, you know, I think this is a very exciting and important", "tokens": [51384, 400, 286, 11, 291, 458, 11, 286, 519, 341, 307, 257, 588, 4670, 293, 1021, 51646], "temperature": 0.0, "avg_logprob": -0.10670421773737127, "compression_ratio": 1.8680851063829786, "no_speech_prob": 0.003480849089100957}, {"id": 879, "seek": 348218, "start": 3482.22, "end": 3488.54, "text": " scientific question. And I would rather like work on exploring", "tokens": [50366, 8134, 1168, 13, 400, 286, 576, 2831, 411, 589, 322, 12736, 50682], "temperature": 0.0, "avg_logprob": -0.17366144928751112, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.0040648337453603745}, {"id": 880, "seek": 348218, "start": 3488.54, "end": 3492.46, "text": " hypotheses and theories to explain our conscious abilities,", "tokens": [50682, 49969, 293, 13667, 281, 2903, 527, 6648, 11582, 11, 50878], "temperature": 0.0, "avg_logprob": -0.17366144928751112, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.0040648337453603745}, {"id": 881, "seek": 348218, "start": 3493.4199999999996, "end": 3496.58, "text": " rather than make bold statements about whether current neural", "tokens": [50926, 2831, 813, 652, 11928, 12363, 466, 1968, 2190, 18161, 51084], "temperature": 0.0, "avg_logprob": -0.17366144928751112, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.0040648337453603745}, {"id": 882, "seek": 348218, "start": 3496.58, "end": 3497.8999999999996, "text": " nets are conscious or not.", "tokens": [51084, 36170, 366, 6648, 420, 406, 13, 51150], "temperature": 0.0, "avg_logprob": -0.17366144928751112, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.0040648337453603745}, {"id": 883, "seek": 348218, "start": 3498.62, "end": 3502.58, "text": " Professor Benjo, we've got some David Chalmers on the show next", "tokens": [51186, 8419, 3964, 5134, 11, 321, 600, 658, 512, 4389, 761, 304, 18552, 322, 264, 855, 958, 51384], "temperature": 0.0, "avg_logprob": -0.17366144928751112, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.0040648337453603745}, {"id": 884, "seek": 348218, "start": 3502.58, "end": 3505.58, "text": " month. Do you have any questions that you had put to him?", "tokens": [51384, 1618, 13, 1144, 291, 362, 604, 1651, 300, 291, 632, 829, 281, 796, 30, 51534], "temperature": 0.0, "avg_logprob": -0.17366144928751112, "compression_ratio": 1.5205479452054795, "no_speech_prob": 0.0040648337453603745}, {"id": 885, "seek": 350558, "start": 3506.42, "end": 3516.06, "text": " I very much like a hypothesis about consciousness that Michael", "tokens": [50406, 286, 588, 709, 411, 257, 17291, 466, 10081, 300, 5116, 50888], "temperature": 0.0, "avg_logprob": -0.19322822310707785, "compression_ratio": 1.3404255319148937, "no_speech_prob": 0.0011320651974529028}, {"id": 886, "seek": 350558, "start": 3516.06, "end": 3525.38, "text": " Graziano has put out to help explain the qualia, the subjective", "tokens": [50888, 8985, 89, 6254, 575, 829, 484, 281, 854, 2903, 264, 4101, 654, 11, 264, 25972, 51354], "temperature": 0.0, "avg_logprob": -0.19322822310707785, "compression_ratio": 1.3404255319148937, "no_speech_prob": 0.0011320651974529028}, {"id": 887, "seek": 350558, "start": 3525.38, "end": 3531.34, "text": " experience part that Chalmers wrote might be something science", "tokens": [51354, 1752, 644, 300, 761, 304, 18552, 4114, 1062, 312, 746, 3497, 51652], "temperature": 0.0, "avg_logprob": -0.19322822310707785, "compression_ratio": 1.3404255319148937, "no_speech_prob": 0.0011320651974529028}, {"id": 888, "seek": 353134, "start": 3531.34, "end": 3538.7400000000002, "text": " can't really, you know, touch. And so what's, you know, I'd like", "tokens": [50364, 393, 380, 534, 11, 291, 458, 11, 2557, 13, 400, 370, 437, 311, 11, 291, 458, 11, 286, 1116, 411, 50734], "temperature": 0.0, "avg_logprob": -0.16281389608615782, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.0027129275258630514}, {"id": 889, "seek": 353134, "start": 3538.7400000000002, "end": 3543.06, "text": " to hear what he has to say about these kinds of approaches. And", "tokens": [50734, 281, 1568, 437, 415, 575, 281, 584, 466, 613, 3685, 295, 11587, 13, 400, 50950], "temperature": 0.0, "avg_logprob": -0.16281389608615782, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.0027129275258630514}, {"id": 890, "seek": 353134, "start": 3543.2200000000003, "end": 3550.7000000000003, "text": " one of the basic premise here is is very grounded in things we", "tokens": [50958, 472, 295, 264, 3875, 22045, 510, 307, 307, 588, 23535, 294, 721, 321, 51332], "temperature": 0.0, "avg_logprob": -0.16281389608615782, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.0027129275258630514}, {"id": 891, "seek": 353134, "start": 3550.7000000000003, "end": 3556.46, "text": " can do scientifically. It's to say, well, let's not try to", "tokens": [51332, 393, 360, 39719, 13, 467, 311, 281, 584, 11, 731, 11, 718, 311, 406, 853, 281, 51620], "temperature": 0.0, "avg_logprob": -0.16281389608615782, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.0027129275258630514}, {"id": 892, "seek": 353134, "start": 3556.46, "end": 3559.78, "text": " figure out what is consciousness or subjective", "tokens": [51620, 2573, 484, 437, 307, 10081, 420, 25972, 51786], "temperature": 0.0, "avg_logprob": -0.16281389608615782, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.0027129275258630514}, {"id": 893, "seek": 355978, "start": 3559.78, "end": 3564.94, "text": " experience more specifically, you know, from a philosopher's", "tokens": [50364, 1752, 544, 4682, 11, 291, 458, 11, 490, 257, 29805, 311, 50622], "temperature": 0.0, "avg_logprob": -0.12763955745291203, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00689955847337842}, {"id": 894, "seek": 355978, "start": 3564.94, "end": 3570.1000000000004, "text": " chairs. But let's let's consider that as a phenomenon that is", "tokens": [50622, 18299, 13, 583, 718, 311, 718, 311, 1949, 300, 382, 257, 14029, 300, 307, 50880], "temperature": 0.0, "avg_logprob": -0.12763955745291203, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00689955847337842}, {"id": 895, "seek": 355978, "start": 3570.1000000000004, "end": 3572.5800000000004, "text": " happening in the brain. I mean, unless you believe in sort of", "tokens": [50880, 2737, 294, 264, 3567, 13, 286, 914, 11, 5969, 291, 1697, 294, 1333, 295, 51004], "temperature": 0.0, "avg_logprob": -0.12763955745291203, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00689955847337842}, {"id": 896, "seek": 355978, "start": 3572.5800000000004, "end": 3575.3, "text": " supernatural things, if it is happening, something is happening", "tokens": [51004, 25678, 721, 11, 498, 309, 307, 2737, 11, 746, 307, 2737, 51140], "temperature": 0.0, "avg_logprob": -0.12763955745291203, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00689955847337842}, {"id": 897, "seek": 355978, "start": 3575.3, "end": 3578.36, "text": " in the brain, and we can report about it. And we can, we can", "tokens": [51140, 294, 264, 3567, 11, 293, 321, 393, 2275, 466, 309, 13, 400, 321, 393, 11, 321, 393, 51293], "temperature": 0.0, "avg_logprob": -0.12763955745291203, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00689955847337842}, {"id": 898, "seek": 355978, "start": 3578.36, "end": 3581.78, "text": " like, measure what's going on in various parts of your brain", "tokens": [51293, 411, 11, 3481, 437, 311, 516, 322, 294, 3683, 3166, 295, 428, 3567, 51464], "temperature": 0.0, "avg_logprob": -0.12763955745291203, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.00689955847337842}, {"id": 899, "seek": 358178, "start": 3581.78, "end": 3590.5400000000004, "text": " while this is happening. And then, you know, can we then come up", "tokens": [50364, 1339, 341, 307, 2737, 13, 400, 550, 11, 291, 458, 11, 393, 321, 550, 808, 493, 50802], "temperature": 0.0, "avg_logprob": -0.17964748626059673, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.055771976709365845}, {"id": 900, "seek": 358178, "start": 3590.5400000000004, "end": 3595.5800000000004, "text": " with theories that explain why we feel that we have subjective", "tokens": [50802, 365, 13667, 300, 2903, 983, 321, 841, 300, 321, 362, 25972, 51054], "temperature": 0.0, "avg_logprob": -0.17964748626059673, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.055771976709365845}, {"id": 901, "seek": 358178, "start": 3595.5800000000004, "end": 3597.96, "text": " experience? It's not saying whether consciousness exists or", "tokens": [51054, 1752, 30, 467, 311, 406, 1566, 1968, 10081, 8198, 420, 51173], "temperature": 0.0, "avg_logprob": -0.17964748626059673, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.055771976709365845}, {"id": 902, "seek": 358178, "start": 3597.96, "end": 3600.94, "text": " not or subjectivity. It's not whether it exists or not in some", "tokens": [51173, 406, 420, 3983, 4253, 13, 467, 311, 406, 1968, 309, 8198, 420, 406, 294, 512, 51322], "temperature": 0.0, "avg_logprob": -0.17964748626059673, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.055771976709365845}, {"id": 903, "seek": 358178, "start": 3600.94, "end": 3603.5400000000004, "text": " sort of logical sense. It's whether, you know, what is it", "tokens": [51322, 1333, 295, 14978, 2020, 13, 467, 311, 1968, 11, 291, 458, 11, 437, 307, 309, 51452], "temperature": 0.0, "avg_logprob": -0.17964748626059673, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.055771976709365845}, {"id": 904, "seek": 358178, "start": 3603.5400000000004, "end": 3606.34, "text": " that's going down in our brain that gives us that feeling and", "tokens": [51452, 300, 311, 516, 760, 294, 527, 3567, 300, 2709, 505, 300, 2633, 293, 51592], "temperature": 0.0, "avg_logprob": -0.17964748626059673, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.055771976709365845}, {"id": 905, "seek": 360634, "start": 3606.34, "end": 3612.54, "text": " then make us say, Well, I am, you know, I'm conscious of x, y,", "tokens": [50364, 550, 652, 505, 584, 11, 1042, 11, 286, 669, 11, 291, 458, 11, 286, 478, 6648, 295, 2031, 11, 288, 11, 50674], "temperature": 0.0, "avg_logprob": -0.20385249825411064, "compression_ratio": 1.5119617224880382, "no_speech_prob": 0.003536998527124524}, {"id": 906, "seek": 360634, "start": 3612.54, "end": 3617.86, "text": " or z. So so that's the that's the direction I find interesting", "tokens": [50674, 420, 710, 13, 407, 370, 300, 311, 264, 300, 311, 264, 3513, 286, 915, 1880, 50940], "temperature": 0.0, "avg_logprob": -0.20385249825411064, "compression_ratio": 1.5119617224880382, "no_speech_prob": 0.003536998527124524}, {"id": 907, "seek": 360634, "start": 3617.86, "end": 3621.42, "text": " because it opens the door for a scientific investigation. And", "tokens": [50940, 570, 309, 9870, 264, 2853, 337, 257, 8134, 9627, 13, 400, 51118], "temperature": 0.0, "avg_logprob": -0.20385249825411064, "compression_ratio": 1.5119617224880382, "no_speech_prob": 0.003536998527124524}, {"id": 908, "seek": 360634, "start": 3621.42, "end": 3626.34, "text": " Michael Grosjean has a specific theory about that which I find", "tokens": [51118, 5116, 2606, 329, 2884, 282, 575, 257, 2685, 5261, 466, 300, 597, 286, 915, 51364], "temperature": 0.0, "avg_logprob": -0.20385249825411064, "compression_ratio": 1.5119617224880382, "no_speech_prob": 0.003536998527124524}, {"id": 909, "seek": 360634, "start": 3626.34, "end": 3632.38, "text": " compelling that is really rooted in the idea that we have a world", "tokens": [51364, 20050, 300, 307, 534, 25277, 294, 264, 1558, 300, 321, 362, 257, 1002, 51666], "temperature": 0.0, "avg_logprob": -0.20385249825411064, "compression_ratio": 1.5119617224880382, "no_speech_prob": 0.003536998527124524}, {"id": 910, "seek": 363238, "start": 3632.38, "end": 3638.1800000000003, "text": " model. And then we we because we have an attention that focuses", "tokens": [50364, 2316, 13, 400, 550, 321, 321, 570, 321, 362, 364, 3202, 300, 16109, 50654], "temperature": 0.0, "avg_logprob": -0.17936072478423248, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.011151096783578396}, {"id": 911, "seek": 363238, "start": 3638.1800000000003, "end": 3643.98, "text": " only parts of it at a time. And we need to have like a little", "tokens": [50654, 787, 3166, 295, 309, 412, 257, 565, 13, 400, 321, 643, 281, 362, 411, 257, 707, 50944], "temperature": 0.0, "avg_logprob": -0.17936072478423248, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.011151096783578396}, {"id": 912, "seek": 363238, "start": 3644.1, "end": 3648.82, "text": " mini world model that controls that attention. That creates a", "tokens": [50950, 8382, 1002, 2316, 300, 9003, 300, 3202, 13, 663, 7829, 257, 51186], "temperature": 0.0, "avg_logprob": -0.17936072478423248, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.011151096783578396}, {"id": 913, "seek": 363238, "start": 3648.82, "end": 3655.86, "text": " sort of separation between the the where the real knowledge is", "tokens": [51186, 1333, 295, 14634, 1296, 264, 264, 689, 264, 957, 3601, 307, 51538], "temperature": 0.0, "avg_logprob": -0.17936072478423248, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.011151096783578396}, {"id": 914, "seek": 363238, "start": 3655.86, "end": 3660.58, "text": " and sort of this more abstract control and machinery that could", "tokens": [51538, 293, 1333, 295, 341, 544, 12649, 1969, 293, 27302, 300, 727, 51774], "temperature": 0.0, "avg_logprob": -0.17936072478423248, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.011151096783578396}, {"id": 915, "seek": 366058, "start": 3660.62, "end": 3666.02, "text": " well, give us this illusion of Cartesian dualism, which I think", "tokens": [50366, 731, 11, 976, 505, 341, 18854, 295, 22478, 42434, 11848, 1434, 11, 597, 286, 519, 50636], "temperature": 0.0, "avg_logprob": -0.20107093553864555, "compression_ratio": 1.5458937198067633, "no_speech_prob": 0.001080688671208918}, {"id": 916, "seek": 366058, "start": 3666.06, "end": 3669.98, "text": " is an illusion, but but you know, must be grounded in some", "tokens": [50638, 307, 364, 18854, 11, 457, 457, 291, 458, 11, 1633, 312, 23535, 294, 512, 50834], "temperature": 0.0, "avg_logprob": -0.20107093553864555, "compression_ratio": 1.5458937198067633, "no_speech_prob": 0.001080688671208918}, {"id": 917, "seek": 366058, "start": 3671.86, "end": 3672.98, "text": " you know, biological", "tokens": [50928, 291, 458, 11, 13910, 50984], "temperature": 0.0, "avg_logprob": -0.20107093553864555, "compression_ratio": 1.5458937198067633, "no_speech_prob": 0.001080688671208918}, {"id": 918, "seek": 366058, "start": 3675.74, "end": 3679.58, "text": " reality. And that's I think understanding that is is a very", "tokens": [51122, 4103, 13, 400, 300, 311, 286, 519, 3701, 300, 307, 307, 257, 588, 51314], "temperature": 0.0, "avg_logprob": -0.20107093553864555, "compression_ratio": 1.5458937198067633, "no_speech_prob": 0.001080688671208918}, {"id": 919, "seek": 366058, "start": 3679.58, "end": 3682.58, "text": " good question to ask. And I'd like to get to know what he", "tokens": [51314, 665, 1168, 281, 1029, 13, 400, 286, 1116, 411, 281, 483, 281, 458, 437, 415, 51464], "temperature": 0.0, "avg_logprob": -0.20107093553864555, "compression_ratio": 1.5458937198067633, "no_speech_prob": 0.001080688671208918}, {"id": 920, "seek": 366058, "start": 3682.58, "end": 3684.18, "text": " thinks about such a research program.", "tokens": [51464, 7309, 466, 1270, 257, 2132, 1461, 13, 51544], "temperature": 0.0, "avg_logprob": -0.20107093553864555, "compression_ratio": 1.5458937198067633, "no_speech_prob": 0.001080688671208918}, {"id": 921, "seek": 366058, "start": 3685.2999999999997, "end": 3686.22, "text": " Thank you very cool.", "tokens": [51600, 1044, 291, 588, 1627, 13, 51646], "temperature": 0.0, "avg_logprob": -0.20107093553864555, "compression_ratio": 1.5458937198067633, "no_speech_prob": 0.001080688671208918}, {"id": 922, "seek": 368622, "start": 3686.7799999999997, "end": 3690.7799999999997, "text": " Yeah, thank you. I do have one kind of nitty gritty question", "tokens": [50392, 865, 11, 1309, 291, 13, 286, 360, 362, 472, 733, 295, 297, 10016, 677, 10016, 1168, 50592], "temperature": 0.0, "avg_logprob": -0.10716265499001683, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.0017271597171202302}, {"id": 923, "seek": 368622, "start": 3690.7799999999997, "end": 3694.4599999999996, "text": " because and partly partly based on some of your recent work on", "tokens": [50592, 570, 293, 17031, 17031, 2361, 322, 512, 295, 428, 5162, 589, 322, 50776], "temperature": 0.0, "avg_logprob": -0.10716265499001683, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.0017271597171202302}, {"id": 924, "seek": 368622, "start": 3694.4599999999996, "end": 3700.58, "text": " becoming more of a fan of semi supervised learning. And you", "tokens": [50776, 5617, 544, 295, 257, 3429, 295, 12909, 46533, 2539, 13, 400, 291, 51082], "temperature": 0.0, "avg_logprob": -0.10716265499001683, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.0017271597171202302}, {"id": 925, "seek": 368622, "start": 3700.58, "end": 3704.18, "text": " know, you had a recent paper that was on interpolation", "tokens": [51082, 458, 11, 291, 632, 257, 5162, 3035, 300, 390, 322, 44902, 399, 51262], "temperature": 0.0, "avg_logprob": -0.10716265499001683, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.0017271597171202302}, {"id": 926, "seek": 368622, "start": 3704.18, "end": 3708.8999999999996, "text": " consistency training. And what I found interesting about that is", "tokens": [51262, 14416, 3097, 13, 400, 437, 286, 1352, 1880, 466, 300, 307, 51498], "temperature": 0.0, "avg_logprob": -0.10716265499001683, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.0017271597171202302}, {"id": 927, "seek": 368622, "start": 3708.8999999999996, "end": 3712.58, "text": " that if we consider one of the biggest challenges that we face", "tokens": [51498, 300, 498, 321, 1949, 472, 295, 264, 3880, 4759, 300, 321, 1851, 51682], "temperature": 0.0, "avg_logprob": -0.10716265499001683, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.0017271597171202302}, {"id": 928, "seek": 368622, "start": 3712.58, "end": 3715.58, "text": " in machine learning pretty much across the board is an", "tokens": [51682, 294, 3479, 2539, 1238, 709, 2108, 264, 3150, 307, 364, 51832], "temperature": 0.0, "avg_logprob": -0.10716265499001683, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.0017271597171202302}, {"id": 929, "seek": 371558, "start": 3715.62, "end": 3718.7, "text": " overcoming the various, you know, curses, if you will, the", "tokens": [50366, 38047, 264, 3683, 11, 291, 458, 11, 1262, 6196, 11, 498, 291, 486, 11, 264, 50520], "temperature": 0.0, "avg_logprob": -0.12314716081940726, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.0010321049485355616}, {"id": 930, "seek": 371558, "start": 3718.7, "end": 3722.66, "text": " various forms of intractability that we have an empirical", "tokens": [50520, 3683, 6422, 295, 560, 1897, 2310, 300, 321, 362, 364, 31886, 50718], "temperature": 0.0, "avg_logprob": -0.12314716081940726, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.0010321049485355616}, {"id": 931, "seek": 371558, "start": 3722.66, "end": 3726.5, "text": " learning methods. And in this context of semi supervised", "tokens": [50718, 2539, 7150, 13, 400, 294, 341, 4319, 295, 12909, 46533, 50910], "temperature": 0.0, "avg_logprob": -0.12314716081940726, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.0010321049485355616}, {"id": 932, "seek": 371558, "start": 3726.5, "end": 3731.2599999999998, "text": " learning, that recent paper, it found significant improvements", "tokens": [50910, 2539, 11, 300, 5162, 3035, 11, 309, 1352, 4776, 13797, 51148], "temperature": 0.0, "avg_logprob": -0.12314716081940726, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.0010321049485355616}, {"id": 933, "seek": 371558, "start": 3731.2999999999997, "end": 3735.9, "text": " over state of the art by forcing linearity. So in this case, it", "tokens": [51150, 670, 1785, 295, 264, 1523, 538, 19030, 8213, 507, 13, 407, 294, 341, 1389, 11, 309, 51380], "temperature": 0.0, "avg_logprob": -0.12314716081940726, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.0010321049485355616}, {"id": 934, "seek": 371558, "start": 3735.9, "end": 3740.66, "text": " was by this mix up between the unlabeled samples and their", "tokens": [51380, 390, 538, 341, 2890, 493, 1296, 264, 32118, 18657, 292, 10938, 293, 641, 51618], "temperature": 0.0, "avg_logprob": -0.12314716081940726, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.0010321049485355616}, {"id": 935, "seek": 374066, "start": 3740.94, "end": 3745.74, "text": " interpolated fake labels. And in the last decade, we've also", "tokens": [50378, 44902, 770, 7592, 16949, 13, 400, 294, 264, 1036, 10378, 11, 321, 600, 611, 50618], "temperature": 0.0, "avg_logprob": -0.15507281394231887, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.010487297549843788}, {"id": 936, "seek": 374066, "start": 3745.74, "end": 3749.74, "text": " seen values come to dominance in the field of neural networks,", "tokens": [50618, 1612, 4190, 808, 281, 34987, 294, 264, 2519, 295, 18161, 9590, 11, 50818], "temperature": 0.0, "avg_logprob": -0.15507281394231887, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.010487297549843788}, {"id": 937, "seek": 374066, "start": 3749.74, "end": 3754.8999999999996, "text": " their piecewise linear recent work by Randall Belastriero,", "tokens": [50818, 641, 2522, 3711, 8213, 5162, 589, 538, 23614, 336, 6248, 525, 470, 2032, 11, 51076], "temperature": 0.0, "avg_logprob": -0.15507281394231887, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.010487297549843788}, {"id": 938, "seek": 374066, "start": 3755.22, "end": 3757.98, "text": " developed an interesting frame of reference which cast", "tokens": [51092, 4743, 364, 1880, 3920, 295, 6408, 597, 4193, 51230], "temperature": 0.0, "avg_logprob": -0.15507281394231887, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.010487297549843788}, {"id": 939, "seek": 374066, "start": 3757.98, "end": 3761.8599999999997, "text": " multi layer perceptrons as a decomposition method, which", "tokens": [51230, 4825, 4583, 43276, 13270, 382, 257, 48356, 3170, 11, 597, 51424], "temperature": 0.0, "avg_logprob": -0.15507281394231887, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.010487297549843788}, {"id": 940, "seek": 374066, "start": 3761.8599999999997, "end": 3766.7, "text": " produces a honeycomb of linear cells in the ambient space and", "tokens": [51424, 14725, 257, 8330, 38763, 295, 8213, 5438, 294, 264, 22997, 1901, 293, 51666], "temperature": 0.0, "avg_logprob": -0.15507281394231887, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.010487297549843788}, {"id": 941, "seek": 376670, "start": 3766.7799999999997, "end": 3770.66, "text": " they're activated turned off or on by input examples. So my", "tokens": [50368, 436, 434, 18157, 3574, 766, 420, 322, 538, 4846, 5110, 13, 407, 452, 50562], "temperature": 0.0, "avg_logprob": -0.12354673385620117, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.003172250697389245}, {"id": 942, "seek": 376670, "start": 3770.66, "end": 3774.2599999999998, "text": " question is, why is linearity, whether it's piecewise or", "tokens": [50562, 1168, 307, 11, 983, 307, 8213, 507, 11, 1968, 309, 311, 2522, 3711, 420, 50742], "temperature": 0.0, "avg_logprob": -0.12354673385620117, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.003172250697389245}, {"id": 943, "seek": 376670, "start": 3774.2599999999998, "end": 3778.46, "text": " otherwise, dominating the state of the art in approximation", "tokens": [50742, 5911, 11, 43306, 264, 1785, 295, 264, 1523, 294, 28023, 50952], "temperature": 0.0, "avg_logprob": -0.12354673385620117, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.003172250697389245}, {"id": 944, "seek": 376670, "start": 3778.46, "end": 3781.58, "text": " methods, it almost seems to me like we've kind of gone back to", "tokens": [50952, 7150, 11, 309, 1920, 2544, 281, 385, 411, 321, 600, 733, 295, 2780, 646, 281, 51108], "temperature": 0.0, "avg_logprob": -0.12354673385620117, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.003172250697389245}, {"id": 945, "seek": 376670, "start": 3781.58, "end": 3784.66, "text": " the future, if you will, sort of leaving behind attempts at more", "tokens": [51108, 264, 2027, 11, 498, 291, 486, 11, 1333, 295, 5012, 2261, 15257, 412, 544, 51262], "temperature": 0.0, "avg_logprob": -0.12354673385620117, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.003172250697389245}, {"id": 946, "seek": 376670, "start": 3784.9399999999996, "end": 3788.74, "text": " smooth nonlinear methods and gone back to newer, albeit more", "tokens": [51276, 5508, 2107, 28263, 7150, 293, 2780, 646, 281, 17628, 11, 43654, 544, 51466], "temperature": 0.0, "avg_logprob": -0.12354673385620117, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.003172250697389245}, {"id": 947, "seek": 376670, "start": 3788.74, "end": 3792.98, "text": " complicated forms of linear approximation.", "tokens": [51466, 6179, 6422, 295, 8213, 28023, 13, 51678], "temperature": 0.0, "avg_logprob": -0.12354673385620117, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.003172250697389245}, {"id": 948, "seek": 379298, "start": 3793.86, "end": 3798.94, "text": " Right. I would say something that's roughly linear is", "tokens": [50408, 1779, 13, 286, 576, 584, 746, 300, 311, 9810, 8213, 307, 50662], "temperature": 0.0, "avg_logprob": -0.14397070718848187, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.00235898164100945}, {"id": 949, "seek": 379298, "start": 3798.94, "end": 3802.82, "text": " simpler. So having a regularizer that says, oh, you want to be", "tokens": [50662, 18587, 13, 407, 1419, 257, 3890, 6545, 300, 1619, 11, 1954, 11, 291, 528, 281, 312, 50856], "temperature": 0.0, "avg_logprob": -0.14397070718848187, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.00235898164100945}, {"id": 950, "seek": 379298, "start": 3802.82, "end": 3806.26, "text": " roughly linear or locally linear, at least to as much", "tokens": [50856, 9810, 8213, 420, 16143, 8213, 11, 412, 1935, 281, 382, 709, 51028], "temperature": 0.0, "avg_logprob": -0.14397070718848187, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.00235898164100945}, {"id": 951, "seek": 379298, "start": 3806.26, "end": 3810.9, "text": " extent as you can is a smoothness prior. So that's going to", "tokens": [51028, 8396, 382, 291, 393, 307, 257, 5508, 1287, 4059, 13, 407, 300, 311, 516, 281, 51260], "temperature": 0.0, "avg_logprob": -0.14397070718848187, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.00235898164100945}, {"id": 952, "seek": 379298, "start": 3810.9, "end": 3816.3, "text": " help generalization. But it could also hurt if that is too", "tokens": [51260, 854, 2674, 2144, 13, 583, 309, 727, 611, 4607, 498, 300, 307, 886, 51530], "temperature": 0.0, "avg_logprob": -0.14397070718848187, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.00235898164100945}, {"id": 953, "seek": 379298, "start": 3816.3, "end": 3820.18, "text": " strong. And so having these piecewise linear kind of more", "tokens": [51530, 2068, 13, 400, 370, 1419, 613, 2522, 3711, 8213, 733, 295, 544, 51724], "temperature": 0.0, "avg_logprob": -0.14397070718848187, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.00235898164100945}, {"id": 954, "seek": 382018, "start": 3820.7799999999997, "end": 3825.98, "text": " type of solution is a good compromise. It says as few pieces", "tokens": [50394, 2010, 295, 3827, 307, 257, 665, 18577, 13, 467, 1619, 382, 1326, 3755, 50654], "temperature": 0.0, "avg_logprob": -0.1842507803311912, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.002671813126653433}, {"id": 955, "seek": 382018, "start": 3825.98, "end": 3830.2599999999998, "text": " as possible, and ideally organized in a compositional way. So", "tokens": [50654, 382, 1944, 11, 293, 22915, 9983, 294, 257, 10199, 2628, 636, 13, 407, 50868], "temperature": 0.0, "avg_logprob": -0.1842507803311912, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.002671813126653433}, {"id": 956, "seek": 382018, "start": 3830.2599999999998, "end": 3835.66, "text": " that it's not just like a relu, it's more like the discrete", "tokens": [50868, 300, 309, 311, 406, 445, 411, 257, 1039, 84, 11, 309, 311, 544, 411, 264, 27706, 51138], "temperature": 0.0, "avg_logprob": -0.1842507803311912, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.002671813126653433}, {"id": 957, "seek": 382018, "start": 3835.7, "end": 3839.3399999999997, "text": " abstract logic, you know, reasoning, things sitting on", "tokens": [51140, 12649, 9952, 11, 291, 458, 11, 21577, 11, 721, 3798, 322, 51322], "temperature": 0.0, "avg_logprob": -0.1842507803311912, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.002671813126653433}, {"id": 958, "seek": 382018, "start": 3839.3399999999997, "end": 3844.06, "text": " top, that's controlling the pieces. But but otherwise fairly", "tokens": [51322, 1192, 11, 300, 311, 14905, 264, 3755, 13, 583, 457, 5911, 6457, 51558], "temperature": 0.0, "avg_logprob": -0.1842507803311912, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.002671813126653433}, {"id": 959, "seek": 382018, "start": 3844.06, "end": 3848.46, "text": " simple in each how each of the pieces are, you know, like", "tokens": [51558, 2199, 294, 1184, 577, 1184, 295, 264, 3755, 366, 11, 291, 458, 11, 411, 51778], "temperature": 0.0, "avg_logprob": -0.1842507803311912, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.002671813126653433}, {"id": 960, "seek": 384846, "start": 3848.46, "end": 3852.42, "text": " linear, for example. So one way to look at this is, if you", "tokens": [50364, 8213, 11, 337, 1365, 13, 407, 472, 636, 281, 574, 412, 341, 307, 11, 498, 291, 50562], "temperature": 0.0, "avg_logprob": -0.12947849351532606, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0019870123360306025}, {"id": 961, "seek": 384846, "start": 3852.42, "end": 3855.66, "text": " look at classical, the kind of rules that classical AI", "tokens": [50562, 574, 412, 13735, 11, 264, 733, 295, 4474, 300, 13735, 7318, 50724], "temperature": 0.0, "avg_logprob": -0.12947849351532606, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0019870123360306025}, {"id": 962, "seek": 384846, "start": 3855.66, "end": 3859.66, "text": " researchers were using, each rule is fairly simple. It's, you", "tokens": [50724, 10309, 645, 1228, 11, 1184, 4978, 307, 6457, 2199, 13, 467, 311, 11, 291, 50924], "temperature": 0.0, "avg_logprob": -0.12947849351532606, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0019870123360306025}, {"id": 963, "seek": 384846, "start": 3859.66, "end": 3865.98, "text": " know, like, it's almost linear, or it's very simple logic. But", "tokens": [50924, 458, 11, 411, 11, 309, 311, 1920, 8213, 11, 420, 309, 311, 588, 2199, 9952, 13, 583, 51240], "temperature": 0.0, "avg_logprob": -0.12947849351532606, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0019870123360306025}, {"id": 964, "seek": 384846, "start": 3865.98, "end": 3869.9, "text": " it's the composition of all those rules that gives the power of", "tokens": [51240, 309, 311, 264, 12686, 295, 439, 729, 4474, 300, 2709, 264, 1347, 295, 51436], "temperature": 0.0, "avg_logprob": -0.12947849351532606, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0019870123360306025}, {"id": 965, "seek": 384846, "start": 3869.9, "end": 3872.62, "text": " expression of these systems. Of course, the problem then is that", "tokens": [51436, 6114, 295, 613, 3652, 13, 2720, 1164, 11, 264, 1154, 550, 307, 300, 51572], "temperature": 0.0, "avg_logprob": -0.12947849351532606, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0019870123360306025}, {"id": 966, "seek": 387262, "start": 3872.62, "end": 3880.22, "text": " they didn't know how to train them properly. But yeah, I think", "tokens": [50364, 436, 994, 380, 458, 577, 281, 3847, 552, 6108, 13, 583, 1338, 11, 286, 519, 50744], "temperature": 0.0, "avg_logprob": -0.1707374382019043, "compression_ratio": 1.348148148148148, "no_speech_prob": 0.005057457834482193}, {"id": 967, "seek": 387262, "start": 3881.38, "end": 3894.3399999999997, "text": " we, I think we learn to come up with these discrete ways of", "tokens": [50802, 321, 11, 286, 519, 321, 1466, 281, 808, 493, 365, 613, 27706, 2098, 295, 51450], "temperature": 0.0, "avg_logprob": -0.1707374382019043, "compression_ratio": 1.348148148148148, "no_speech_prob": 0.005057457834482193}, {"id": 968, "seek": 387262, "start": 3894.38, "end": 3901.02, "text": " breaking up things into simpler pieces. And that in fact, I", "tokens": [51452, 7697, 493, 721, 666, 18587, 3755, 13, 400, 300, 294, 1186, 11, 286, 51784], "temperature": 0.0, "avg_logprob": -0.1707374382019043, "compression_ratio": 1.348148148148148, "no_speech_prob": 0.005057457834482193}, {"id": 969, "seek": 390102, "start": 3901.02, "end": 3903.9, "text": " think if you're Bayesian about it, it just comes out naturally.", "tokens": [50364, 519, 498, 291, 434, 7840, 42434, 466, 309, 11, 309, 445, 1487, 484, 8195, 13, 50508], "temperature": 0.0, "avg_logprob": -0.19676782156674916, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.006687632296234369}, {"id": 970, "seek": 390102, "start": 3904.02, "end": 3906.14, "text": " And they're very, very weak assumptions.", "tokens": [50514, 400, 436, 434, 588, 11, 588, 5336, 17695, 13, 50620], "temperature": 0.0, "avg_logprob": -0.19676782156674916, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.006687632296234369}, {"id": 971, "seek": 390102, "start": 3907.5, "end": 3911.94, "text": " So in a way, it's it's almost, it is piecewise abstraction. So", "tokens": [50688, 407, 294, 257, 636, 11, 309, 311, 309, 311, 1920, 11, 309, 307, 2522, 3711, 37765, 13, 407, 50910], "temperature": 0.0, "avg_logprob": -0.19676782156674916, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.006687632296234369}, {"id": 972, "seek": 390102, "start": 3911.94, "end": 3915.74, "text": " we're kind of back. Yes, that's what I would lean to, rather", "tokens": [50910, 321, 434, 733, 295, 646, 13, 1079, 11, 300, 311, 437, 286, 576, 11659, 281, 11, 2831, 51100], "temperature": 0.0, "avg_logprob": -0.19676782156674916, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.006687632296234369}, {"id": 973, "seek": 390102, "start": 3915.74, "end": 3920.02, "text": " than piecewise linear. But linear, of course, is a broad part", "tokens": [51100, 813, 2522, 3711, 8213, 13, 583, 8213, 11, 295, 1164, 11, 307, 257, 4152, 644, 51314], "temperature": 0.0, "avg_logprob": -0.19676782156674916, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.006687632296234369}, {"id": 974, "seek": 390102, "start": 3920.02, "end": 3922.58, "text": " of, you know, it's an easy way to get simple.", "tokens": [51314, 295, 11, 291, 458, 11, 309, 311, 364, 1858, 636, 281, 483, 2199, 13, 51442], "temperature": 0.0, "avg_logprob": -0.19676782156674916, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.006687632296234369}, {"id": 975, "seek": 390102, "start": 3923.7, "end": 3926.74, "text": " Amazing. Professor Benjo, I'm interested in your personal", "tokens": [51498, 14165, 13, 8419, 3964, 5134, 11, 286, 478, 3102, 294, 428, 2973, 51650], "temperature": 0.0, "avg_logprob": -0.19676782156674916, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.006687632296234369}, {"id": 976, "seek": 390102, "start": 3926.74, "end": 3929.54, "text": " journey. So we've been talking about diverse trajectories. And", "tokens": [51650, 4671, 13, 407, 321, 600, 668, 1417, 466, 9521, 18257, 2083, 13, 400, 51790], "temperature": 0.0, "avg_logprob": -0.19676782156674916, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.006687632296234369}, {"id": 977, "seek": 392954, "start": 3929.62, "end": 3932.18, "text": " I wanted to know about your own trajectory of research over the", "tokens": [50368, 286, 1415, 281, 458, 466, 428, 1065, 21512, 295, 2132, 670, 264, 50496], "temperature": 0.0, "avg_logprob": -0.11383659189397638, "compression_ratio": 1.6322188449848025, "no_speech_prob": 0.017113395035266876}, {"id": 978, "seek": 392954, "start": 3932.18, "end": 3935.54, "text": " last 10 years. Now, one of my mates, a psychologist and", "tokens": [50496, 1036, 1266, 924, 13, 823, 11, 472, 295, 452, 31488, 11, 257, 29514, 293, 50664], "temperature": 0.0, "avg_logprob": -0.11383659189397638, "compression_ratio": 1.6322188449848025, "no_speech_prob": 0.017113395035266876}, {"id": 979, "seek": 392954, "start": 3935.54, "end": 3939.02, "text": " symbolist, Professor Gary Marcus, presumably one of your best", "tokens": [50664, 5986, 468, 11, 8419, 13788, 26574, 11, 26742, 472, 295, 428, 1151, 50838], "temperature": 0.0, "avg_logprob": -0.11383659189397638, "compression_ratio": 1.6322188449848025, "no_speech_prob": 0.017113395035266876}, {"id": 980, "seek": 392954, "start": 3939.02, "end": 3942.34, "text": " friends, by the way, he pointed out in his 2012 New Yorker", "tokens": [50838, 1855, 11, 538, 264, 636, 11, 415, 10932, 484, 294, 702, 9125, 1873, 3609, 260, 51004], "temperature": 0.0, "avg_logprob": -0.11383659189397638, "compression_ratio": 1.6322188449848025, "no_speech_prob": 0.017113395035266876}, {"id": 981, "seek": 392954, "start": 3942.34, "end": 3945.66, "text": " article that MLPs lacked ways of representing causal", "tokens": [51004, 7222, 300, 21601, 23043, 41481, 2098, 295, 13460, 38755, 51170], "temperature": 0.0, "avg_logprob": -0.11383659189397638, "compression_ratio": 1.6322188449848025, "no_speech_prob": 0.017113395035266876}, {"id": 982, "seek": 392954, "start": 3945.66, "end": 3948.94, "text": " relationships such as between diseases and their symptoms. And", "tokens": [51170, 6159, 1270, 382, 1296, 11044, 293, 641, 8332, 13, 400, 51334], "temperature": 0.0, "avg_logprob": -0.11383659189397638, "compression_ratio": 1.6322188449848025, "no_speech_prob": 0.017113395035266876}, {"id": 983, "seek": 392954, "start": 3949.06, "end": 3950.9, "text": " I think this has been a significant focus of yours in", "tokens": [51340, 286, 519, 341, 575, 668, 257, 4776, 1879, 295, 6342, 294, 51432], "temperature": 0.0, "avg_logprob": -0.11383659189397638, "compression_ratio": 1.6322188449848025, "no_speech_prob": 0.017113395035266876}, {"id": 984, "seek": 392954, "start": 3950.9, "end": 3954.54, "text": " recent years as we've discussed. And he thought at the time that", "tokens": [51432, 5162, 924, 382, 321, 600, 7152, 13, 400, 415, 1194, 412, 264, 565, 300, 51614], "temperature": 0.0, "avg_logprob": -0.11383659189397638, "compression_ratio": 1.6322188449848025, "no_speech_prob": 0.017113395035266876}, {"id": 985, "seek": 392954, "start": 3954.54, "end": 3958.42, "text": " you were a bit too quote system one all the way. And he spoke", "tokens": [51614, 291, 645, 257, 857, 886, 6513, 1185, 472, 439, 264, 636, 13, 400, 415, 7179, 51808], "temperature": 0.0, "avg_logprob": -0.11383659189397638, "compression_ratio": 1.6322188449848025, "no_speech_prob": 0.017113395035266876}, {"id": 986, "seek": 395842, "start": 3958.42, "end": 3960.78, "text": " then about the need for heterogeneous architectures and", "tokens": [50364, 550, 466, 264, 643, 337, 20789, 31112, 6331, 1303, 293, 50482], "temperature": 0.0, "avg_logprob": -0.10237846374511719, "compression_ratio": 1.6807387862796834, "no_speech_prob": 0.001509263995103538}, {"id": 987, "seek": 395842, "start": 3960.78, "end": 3963.7000000000003, "text": " the acquisition of abstract concepts, compositionality and", "tokens": [50482, 264, 21668, 295, 12649, 10392, 11, 12686, 1860, 293, 50628], "temperature": 0.0, "avg_logprob": -0.10237846374511719, "compression_ratio": 1.6807387862796834, "no_speech_prob": 0.001509263995103538}, {"id": 988, "seek": 395842, "start": 3963.7000000000003, "end": 3967.06, "text": " extrapolation, which I think has also been a huge focus of yours", "tokens": [50628, 48224, 399, 11, 597, 286, 519, 575, 611, 668, 257, 2603, 1879, 295, 6342, 50796], "temperature": 0.0, "avg_logprob": -0.10237846374511719, "compression_ratio": 1.6807387862796834, "no_speech_prob": 0.001509263995103538}, {"id": 989, "seek": 395842, "start": 3967.06, "end": 3969.78, "text": " in the last decade or so. We really enjoyed watching your", "tokens": [50796, 294, 264, 1036, 10378, 420, 370, 13, 492, 534, 4626, 1976, 428, 50932], "temperature": 0.0, "avg_logprob": -0.10237846374511719, "compression_ratio": 1.6807387862796834, "no_speech_prob": 0.001509263995103538}, {"id": 990, "seek": 395842, "start": 3969.78, "end": 3973.34, "text": " debate with Marcus. And by the way, we would love to host V2 of", "tokens": [50932, 7958, 365, 26574, 13, 400, 538, 264, 636, 11, 321, 576, 959, 281, 3975, 691, 17, 295, 51110], "temperature": 0.0, "avg_logprob": -0.10237846374511719, "compression_ratio": 1.6807387862796834, "no_speech_prob": 0.001509263995103538}, {"id": 991, "seek": 395842, "start": 3973.34, "end": 3975.1, "text": " that debate. So if you're interested, you just let us", "tokens": [51110, 300, 7958, 13, 407, 498, 291, 434, 3102, 11, 291, 445, 718, 505, 51198], "temperature": 0.0, "avg_logprob": -0.10237846374511719, "compression_ratio": 1.6807387862796834, "no_speech_prob": 0.001509263995103538}, {"id": 992, "seek": 395842, "start": 3975.1, "end": 3978.02, "text": " know we'll do that. But he's often viewed as a heretic. And,", "tokens": [51198, 458, 321, 603, 360, 300, 13, 583, 415, 311, 2049, 19174, 382, 257, 720, 3532, 13, 400, 11, 51344], "temperature": 0.0, "avg_logprob": -0.10237846374511719, "compression_ratio": 1.6807387862796834, "no_speech_prob": 0.001509263995103538}, {"id": 993, "seek": 395842, "start": 3978.14, "end": 3980.58, "text": " you know, just forgetting about symbols versus neural networks", "tokens": [51350, 291, 458, 11, 445, 25428, 466, 16944, 5717, 18161, 9590, 51472], "temperature": 0.0, "avg_logprob": -0.10237846374511719, "compression_ratio": 1.6807387862796834, "no_speech_prob": 0.001509263995103538}, {"id": 994, "seek": 395842, "start": 3980.58, "end": 3983.26, "text": " for a minute. Am I right in thinking that you've converged in", "tokens": [51472, 337, 257, 3456, 13, 2012, 286, 558, 294, 1953, 300, 291, 600, 9652, 3004, 294, 51606], "temperature": 0.0, "avg_logprob": -0.10237846374511719, "compression_ratio": 1.6807387862796834, "no_speech_prob": 0.001509263995103538}, {"id": 995, "seek": 395842, "start": 3983.26, "end": 3985.5, "text": " at least some ways in your thinking? And how would you", "tokens": [51606, 412, 1935, 512, 2098, 294, 428, 1953, 30, 400, 577, 576, 291, 51718], "temperature": 0.0, "avg_logprob": -0.10237846374511719, "compression_ratio": 1.6807387862796834, "no_speech_prob": 0.001509263995103538}, {"id": 996, "seek": 395842, "start": 3985.5, "end": 3986.94, "text": " characterize that from your perspective?", "tokens": [51718, 38463, 300, 490, 428, 4585, 30, 51790], "temperature": 0.0, "avg_logprob": -0.10237846374511719, "compression_ratio": 1.6807387862796834, "no_speech_prob": 0.001509263995103538}, {"id": 997, "seek": 398694, "start": 3987.82, "end": 3998.9, "text": " So, yeah, I used to be in the 90s, a, you know, pure neural net", "tokens": [50408, 407, 11, 1338, 11, 286, 1143, 281, 312, 294, 264, 4289, 82, 11, 257, 11, 291, 458, 11, 6075, 18161, 2533, 50962], "temperature": 0.0, "avg_logprob": -0.18940470899854386, "compression_ratio": 1.3805970149253732, "no_speech_prob": 0.0006068581133149564}, {"id": 998, "seek": 398694, "start": 4001.38, "end": 4010.48, "text": " subsymbolic connectionists researcher. And I did my grad", "tokens": [51086, 2090, 88, 5612, 299, 4984, 1751, 21751, 13, 400, 286, 630, 452, 2771, 51541], "temperature": 0.0, "avg_logprob": -0.18940470899854386, "compression_ratio": 1.3805970149253732, "no_speech_prob": 0.0006068581133149564}, {"id": 999, "seek": 398694, "start": 4010.48, "end": 4015.7400000000002, "text": " studies at a time on neural nets at a time when the dominant way", "tokens": [51541, 5313, 412, 257, 565, 322, 18161, 36170, 412, 257, 565, 562, 264, 15657, 636, 51804], "temperature": 0.0, "avg_logprob": -0.18940470899854386, "compression_ratio": 1.3805970149253732, "no_speech_prob": 0.0006068581133149564}, {"id": 1000, "seek": 401574, "start": 4015.74, "end": 4019.58, "text": " of thinking was these, you know, classical AI rule based system", "tokens": [50364, 295, 1953, 390, 613, 11, 291, 458, 11, 13735, 7318, 4978, 2361, 1185, 50556], "temperature": 0.0, "avg_logprob": -0.14448537265553194, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0037051369436085224}, {"id": 1001, "seek": 401574, "start": 4019.58, "end": 4023.22, "text": " with no learning at all, and was dominant, meaning that the", "tokens": [50556, 365, 572, 2539, 412, 439, 11, 293, 390, 15657, 11, 3620, 300, 264, 50738], "temperature": 0.0, "avg_logprob": -0.14448537265553194, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0037051369436085224}, {"id": 1002, "seek": 401574, "start": 4023.22, "end": 4026.58, "text": " little group like, you know, Jan and Jeff and I and others who", "tokens": [50738, 707, 1594, 411, 11, 291, 458, 11, 4956, 293, 7506, 293, 286, 293, 2357, 567, 50906], "temperature": 0.0, "avg_logprob": -0.14448537265553194, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0037051369436085224}, {"id": 1003, "seek": 401574, "start": 4026.58, "end": 4033.7, "text": " were thinking otherwise, had to, you know, defend our views. And", "tokens": [50906, 645, 1953, 5911, 11, 632, 281, 11, 291, 458, 11, 8602, 527, 6809, 13, 400, 51262], "temperature": 0.0, "avg_logprob": -0.14448537265553194, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0037051369436085224}, {"id": 1004, "seek": 401574, "start": 4034.74, "end": 4040.7, "text": " and maybe that led to a kind of, you know, us versus them, I", "tokens": [51314, 293, 1310, 300, 4684, 281, 257, 733, 295, 11, 291, 458, 11, 505, 5717, 552, 11, 286, 51612], "temperature": 0.0, "avg_logprob": -0.14448537265553194, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0037051369436085224}, {"id": 1005, "seek": 404070, "start": 4040.7, "end": 4048.46, "text": " think, unhealthy way of thinking. And of course, I matured. And", "tokens": [50364, 519, 11, 29147, 636, 295, 1953, 13, 400, 295, 1164, 11, 286, 14442, 67, 13, 400, 50752], "temperature": 0.0, "avg_logprob": -0.18642376378639458, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.003221494145691395}, {"id": 1006, "seek": 404070, "start": 4050.06, "end": 4053.2599999999998, "text": " one of the big, so there, I think there are several turning", "tokens": [50832, 472, 295, 264, 955, 11, 370, 456, 11, 286, 519, 456, 366, 2940, 6246, 50992], "temperature": 0.0, "avg_logprob": -0.18642376378639458, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.003221494145691395}, {"id": 1007, "seek": 404070, "start": 4053.2599999999998, "end": 4058.2999999999997, "text": " points on that journey. Well, one of them in the in the 2000s", "tokens": [50992, 2793, 322, 300, 4671, 13, 1042, 11, 472, 295, 552, 294, 264, 294, 264, 8132, 82, 51244], "temperature": 0.0, "avg_logprob": -0.18642376378639458, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.003221494145691395}, {"id": 1008, "seek": 404070, "start": 4058.2999999999997, "end": 4061.4199999999996, "text": " was the realization of the importance of abstraction. So", "tokens": [51244, 390, 264, 25138, 295, 264, 7379, 295, 37765, 13, 407, 51400], "temperature": 0.0, "avg_logprob": -0.18642376378639458, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.003221494145691395}, {"id": 1009, "seek": 404070, "start": 4062.5, "end": 4064.7999999999997, "text": " and the way to think about this maybe more concretely, because", "tokens": [51454, 293, 264, 636, 281, 519, 466, 341, 1310, 544, 39481, 736, 11, 570, 51569], "temperature": 0.0, "avg_logprob": -0.18642376378639458, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.003221494145691395}, {"id": 1010, "seek": 404070, "start": 4064.7999999999997, "end": 4066.8999999999996, "text": " what does it mean to be abstract? Is that I was thinking,", "tokens": [51569, 437, 775, 309, 914, 281, 312, 12649, 30, 1119, 300, 286, 390, 1953, 11, 51674], "temperature": 0.0, "avg_logprob": -0.18642376378639458, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.003221494145691395}, {"id": 1011, "seek": 406690, "start": 4067.2200000000003, "end": 4071.82, "text": " well, what would be the right kind of representation we want to", "tokens": [50380, 731, 11, 437, 576, 312, 264, 558, 733, 295, 10290, 321, 528, 281, 50610], "temperature": 0.0, "avg_logprob": -0.22950029787809953, "compression_ratio": 1.8016877637130801, "no_speech_prob": 0.0008688479429110885}, {"id": 1012, "seek": 406690, "start": 4071.82, "end": 4074.82, "text": " have at the top level of our unsupervised deep nets, because", "tokens": [50610, 362, 412, 264, 1192, 1496, 295, 527, 2693, 12879, 24420, 2452, 36170, 11, 570, 50760], "temperature": 0.0, "avg_logprob": -0.22950029787809953, "compression_ratio": 1.8016877637130801, "no_speech_prob": 0.0008688479429110885}, {"id": 1013, "seek": 406690, "start": 4074.82, "end": 4076.98, "text": " we were doing mostly like unsupervised deep nest, like,", "tokens": [50760, 321, 645, 884, 5240, 411, 2693, 12879, 24420, 2452, 15646, 11, 411, 11, 50868], "temperature": 0.0, "avg_logprob": -0.22950029787809953, "compression_ratio": 1.8016877637130801, "no_speech_prob": 0.0008688479429110885}, {"id": 1014, "seek": 406690, "start": 4076.98, "end": 4081.48, "text": " you know, deep boz machines and stuff in that decade. And I was", "tokens": [50868, 291, 458, 11, 2452, 748, 89, 8379, 293, 1507, 294, 300, 10378, 13, 400, 286, 390, 51093], "temperature": 0.0, "avg_logprob": -0.22950029787809953, "compression_ratio": 1.8016877637130801, "no_speech_prob": 0.0008688479429110885}, {"id": 1015, "seek": 406690, "start": 4081.48, "end": 4084.94, "text": " thinking, well, it would be things like words, right, things", "tokens": [51093, 1953, 11, 731, 11, 309, 576, 312, 721, 411, 2283, 11, 558, 11, 721, 51266], "temperature": 0.0, "avg_logprob": -0.22950029787809953, "compression_ratio": 1.8016877637130801, "no_speech_prob": 0.0008688479429110885}, {"id": 1016, "seek": 406690, "start": 4084.94, "end": 4088.38, "text": " like the sort of concepts that we manipulate at the top level,", "tokens": [51266, 411, 264, 1333, 295, 10392, 300, 321, 20459, 412, 264, 1192, 1496, 11, 51438], "temperature": 0.0, "avg_logprob": -0.22950029787809953, "compression_ratio": 1.8016877637130801, "no_speech_prob": 0.0008688479429110885}, {"id": 1017, "seek": 406690, "start": 4088.38, "end": 4092.38, "text": " well, it's words or, you know, the equivalent, maybe, with", "tokens": [51438, 731, 11, 309, 311, 2283, 420, 11, 291, 458, 11, 264, 10344, 11, 1310, 11, 365, 51638], "temperature": 0.0, "avg_logprob": -0.22950029787809953, "compression_ratio": 1.8016877637130801, "no_speech_prob": 0.0008688479429110885}, {"id": 1018, "seek": 409238, "start": 4092.98, "end": 4099.26, "text": " disambiguated. But yeah, we, it didn't seem that we have the", "tokens": [50394, 717, 2173, 16397, 770, 13, 583, 1338, 11, 321, 11, 309, 994, 380, 1643, 300, 321, 362, 264, 50708], "temperature": 0.0, "avg_logprob": -0.15840398697626024, "compression_ratio": 1.53, "no_speech_prob": 0.001647841650992632}, {"id": 1019, "seek": 409238, "start": 4099.26, "end": 4103.02, "text": " right tools for that. And then it remained like an objective. And", "tokens": [50708, 558, 3873, 337, 300, 13, 400, 550, 309, 12780, 411, 364, 10024, 13, 400, 50896], "temperature": 0.0, "avg_logprob": -0.15840398697626024, "compression_ratio": 1.53, "no_speech_prob": 0.001647841650992632}, {"id": 1020, "seek": 409238, "start": 4103.02, "end": 4112.16, "text": " then in 2014, we discovered the power of attention. And that's", "tokens": [50896, 550, 294, 8227, 11, 321, 6941, 264, 1347, 295, 3202, 13, 400, 300, 311, 51353], "temperature": 0.0, "avg_logprob": -0.15840398697626024, "compression_ratio": 1.53, "no_speech_prob": 0.001647841650992632}, {"id": 1021, "seek": 409238, "start": 4112.18, "end": 4115.78, "text": " closely connected to abstraction, because what it does is", "tokens": [51354, 8185, 4582, 281, 37765, 11, 570, 437, 309, 775, 307, 51534], "temperature": 0.0, "avg_logprob": -0.15840398697626024, "compression_ratio": 1.53, "no_speech_prob": 0.001647841650992632}, {"id": 1022, "seek": 409238, "start": 4115.78, "end": 4119.14, "text": " it focuses on a few things. And of course, that's our, you", "tokens": [51534, 309, 16109, 322, 257, 1326, 721, 13, 400, 295, 1164, 11, 300, 311, 527, 11, 291, 51702], "temperature": 0.0, "avg_logprob": -0.15840398697626024, "compression_ratio": 1.53, "no_speech_prob": 0.001647841650992632}, {"id": 1023, "seek": 411914, "start": 4119.14, "end": 4123.06, "text": " know, that's very much a characteristic of how we think a", "tokens": [50364, 458, 11, 300, 311, 588, 709, 257, 16282, 295, 577, 321, 519, 257, 50560], "temperature": 0.0, "avg_logprob": -0.13736732727890716, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.002114534378051758}, {"id": 1024, "seek": 411914, "start": 4123.06, "end": 4126.62, "text": " thought has very few elements in it. That means we have selected", "tokens": [50560, 1194, 575, 588, 1326, 4959, 294, 309, 13, 663, 1355, 321, 362, 8209, 50738], "temperature": 0.0, "avg_logprob": -0.13736732727890716, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.002114534378051758}, {"id": 1025, "seek": 411914, "start": 4126.62, "end": 4129.3, "text": " those elements. And that's where attention comes in. So it's", "tokens": [50738, 729, 4959, 13, 400, 300, 311, 689, 3202, 1487, 294, 13, 407, 309, 311, 50872], "temperature": 0.0, "avg_logprob": -0.13736732727890716, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.002114534378051758}, {"id": 1026, "seek": 411914, "start": 4129.3, "end": 4135.58, "text": " getting closer to this ideal of building machines that think like", "tokens": [50872, 1242, 4966, 281, 341, 7157, 295, 2390, 8379, 300, 519, 411, 51186], "temperature": 0.0, "avg_logprob": -0.13736732727890716, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.002114534378051758}, {"id": 1027, "seek": 411914, "start": 4135.58, "end": 4140.06, "text": " humans. And then of course, in 2017, I wrote this consciousness", "tokens": [51186, 6255, 13, 400, 550, 295, 1164, 11, 294, 6591, 11, 286, 4114, 341, 10081, 51410], "temperature": 0.0, "avg_logprob": -0.13736732727890716, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.002114534378051758}, {"id": 1028, "seek": 411914, "start": 4140.06, "end": 4142.740000000001, "text": " prior paper where, you know, I discovered all the work on global", "tokens": [51410, 4059, 3035, 689, 11, 291, 458, 11, 286, 6941, 439, 264, 589, 322, 4338, 51544], "temperature": 0.0, "avg_logprob": -0.13736732727890716, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.002114534378051758}, {"id": 1029, "seek": 411914, "start": 4142.740000000001, "end": 4146.1, "text": " workspace theory and, and it, you know, and the momentum is", "tokens": [51544, 32706, 5261, 293, 11, 293, 309, 11, 291, 458, 11, 293, 264, 11244, 307, 51712], "temperature": 0.0, "avg_logprob": -0.13736732727890716, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.002114534378051758}, {"id": 1030, "seek": 414610, "start": 4146.1, "end": 4152.620000000001, "text": " built up. And of course, now, you know, humans think and they", "tokens": [50364, 3094, 493, 13, 400, 295, 1164, 11, 586, 11, 291, 458, 11, 6255, 519, 293, 436, 50690], "temperature": 0.0, "avg_logprob": -0.13340530010184856, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.005218069069087505}, {"id": 1031, "seek": 414610, "start": 4152.620000000001, "end": 4157.46, "text": " use symbols, and they understand the very abstract", "tokens": [50690, 764, 16944, 11, 293, 436, 1223, 264, 588, 12649, 50932], "temperature": 0.0, "avg_logprob": -0.13340530010184856, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.005218069069087505}, {"id": 1032, "seek": 414610, "start": 4157.46, "end": 4160.42, "text": " relationships between them. And we need to build neural nets that", "tokens": [50932, 6159, 1296, 552, 13, 400, 321, 643, 281, 1322, 18161, 36170, 300, 51080], "temperature": 0.0, "avg_logprob": -0.13340530010184856, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.005218069069087505}, {"id": 1033, "seek": 414610, "start": 4160.42, "end": 4166.700000000001, "text": " can do that. So I guess where I've maybe departed from Gary,", "tokens": [51080, 393, 360, 300, 13, 407, 286, 2041, 689, 286, 600, 1310, 47018, 490, 13788, 11, 51394], "temperature": 0.0, "avg_logprob": -0.13340530010184856, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.005218069069087505}, {"id": 1034, "seek": 414610, "start": 4166.700000000001, "end": 4171.18, "text": " but maybe he's moved to is, it's going to be neural nets that", "tokens": [51394, 457, 1310, 415, 311, 4259, 281, 307, 11, 309, 311, 516, 281, 312, 18161, 36170, 300, 51618], "temperature": 0.0, "avg_logprob": -0.13340530010184856, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.005218069069087505}, {"id": 1035, "seek": 414610, "start": 4171.18, "end": 4173.620000000001, "text": " do it, right? It's just that we're going to be training them in", "tokens": [51618, 360, 309, 11, 558, 30, 467, 311, 445, 300, 321, 434, 516, 281, 312, 3097, 552, 294, 51740], "temperature": 0.0, "avg_logprob": -0.13340530010184856, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.005218069069087505}, {"id": 1036, "seek": 417362, "start": 4173.66, "end": 4176.42, "text": " a special way. And that's what G flow nets really aiming at.", "tokens": [50366, 257, 2121, 636, 13, 400, 300, 311, 437, 460, 3095, 36170, 534, 20253, 412, 13, 50504], "temperature": 0.0, "avg_logprob": -0.16848085607801164, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.006095754448324442}, {"id": 1037, "seek": 417362, "start": 4177.66, "end": 4180.82, "text": " So can I just say we, we asked many guests, these, these", "tokens": [50566, 407, 393, 286, 445, 584, 321, 11, 321, 2351, 867, 9804, 11, 613, 11, 613, 50724], "temperature": 0.0, "avg_logprob": -0.16848085607801164, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.006095754448324442}, {"id": 1038, "seek": 417362, "start": 4180.82, "end": 4183.86, "text": " questions about their, their evolution. And sometimes they,", "tokens": [50724, 1651, 466, 641, 11, 641, 9303, 13, 400, 2171, 436, 11, 50876], "temperature": 0.0, "avg_logprob": -0.16848085607801164, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.006095754448324442}, {"id": 1039, "seek": 417362, "start": 4184.22, "end": 4187.62, "text": " they tend to be spicier than others. But I have to say, from my", "tokens": [50894, 436, 3928, 281, 312, 41418, 811, 813, 2357, 13, 583, 286, 362, 281, 584, 11, 490, 452, 51064], "temperature": 0.0, "avg_logprob": -0.16848085607801164, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.006095754448324442}, {"id": 1040, "seek": 417362, "start": 4187.62, "end": 4192.0599999999995, "text": " perspective, your answer was the most informative, the most", "tokens": [51064, 4585, 11, 428, 1867, 390, 264, 881, 27759, 11, 264, 881, 51286], "temperature": 0.0, "avg_logprob": -0.16848085607801164, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.006095754448324442}, {"id": 1041, "seek": 417362, "start": 4192.0599999999995, "end": 4196.86, "text": " gracious and the most noble of answers that we've heard so far", "tokens": [51286, 36113, 293, 264, 881, 20171, 295, 6338, 300, 321, 600, 2198, 370, 1400, 51526], "temperature": 0.0, "avg_logprob": -0.16848085607801164, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.006095754448324442}, {"id": 1042, "seek": 417362, "start": 4196.86, "end": 4200.78, "text": " to similar questions. So kudos to you. That was awesome.", "tokens": [51526, 281, 2531, 1651, 13, 407, 350, 35063, 281, 291, 13, 663, 390, 3476, 13, 51722], "temperature": 0.0, "avg_logprob": -0.16848085607801164, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.006095754448324442}, {"id": 1043, "seek": 420078, "start": 4200.86, "end": 4201.3, "text": " Thanks.", "tokens": [50368, 2561, 13, 50390], "temperature": 0.0, "avg_logprob": -0.14453145345052085, "compression_ratio": 1.7284345047923322, "no_speech_prob": 0.001500698970630765}, {"id": 1044, "seek": 420078, "start": 4202.5, "end": 4205.7, "text": " I just cannot believe it. And we always do a hell of a lot of", "tokens": [50450, 286, 445, 2644, 1697, 309, 13, 400, 321, 1009, 360, 257, 4921, 295, 257, 688, 295, 50610], "temperature": 0.0, "avg_logprob": -0.14453145345052085, "compression_ratio": 1.7284345047923322, "no_speech_prob": 0.001500698970630765}, {"id": 1045, "seek": 420078, "start": 4205.7, "end": 4208.54, "text": " preparation. But it's gone to the point now where we know that", "tokens": [50610, 13081, 13, 583, 309, 311, 2780, 281, 264, 935, 586, 689, 321, 458, 300, 50752], "temperature": 0.0, "avg_logprob": -0.14453145345052085, "compression_ratio": 1.7284345047923322, "no_speech_prob": 0.001500698970630765}, {"id": 1046, "seek": 420078, "start": 4208.54, "end": 4211.54, "text": " we're not going to get more than about six questions in. So we,", "tokens": [50752, 321, 434, 406, 516, 281, 483, 544, 813, 466, 2309, 1651, 294, 13, 407, 321, 11, 50902], "temperature": 0.0, "avg_logprob": -0.14453145345052085, "compression_ratio": 1.7284345047923322, "no_speech_prob": 0.001500698970630765}, {"id": 1047, "seek": 420078, "start": 4211.78, "end": 4214.38, "text": " you know, we kind of like exponentially, you know, have an", "tokens": [50914, 291, 458, 11, 321, 733, 295, 411, 37330, 11, 291, 458, 11, 362, 364, 51044], "temperature": 0.0, "avg_logprob": -0.14453145345052085, "compression_ratio": 1.7284345047923322, "no_speech_prob": 0.001500698970630765}, {"id": 1048, "seek": 420078, "start": 4214.38, "end": 4216.0599999999995, "text": " exponential prior on our questions.", "tokens": [51044, 21510, 4059, 322, 527, 1651, 13, 51128], "temperature": 0.0, "avg_logprob": -0.14453145345052085, "compression_ratio": 1.7284345047923322, "no_speech_prob": 0.001500698970630765}, {"id": 1049, "seek": 420078, "start": 4216.0599999999995, "end": 4219.0199999999995, "text": " Well, he was awesome, though, with like, you know, we asked him", "tokens": [51128, 1042, 11, 415, 390, 3476, 11, 1673, 11, 365, 411, 11, 291, 458, 11, 321, 2351, 796, 51276], "temperature": 0.0, "avg_logprob": -0.14453145345052085, "compression_ratio": 1.7284345047923322, "no_speech_prob": 0.001500698970630765}, {"id": 1050, "seek": 420078, "start": 4219.139999999999, "end": 4223.099999999999, "text": " to give relatively sort of three minute answers. And he stuck to", "tokens": [51282, 281, 976, 7226, 1333, 295, 1045, 3456, 6338, 13, 400, 415, 5541, 281, 51480], "temperature": 0.0, "avg_logprob": -0.14453145345052085, "compression_ratio": 1.7284345047923322, "no_speech_prob": 0.001500698970630765}, {"id": 1051, "seek": 420078, "start": 4223.099999999999, "end": 4226.0199999999995, "text": " that, which was really cool. I mean, that's, that's very", "tokens": [51480, 300, 11, 597, 390, 534, 1627, 13, 286, 914, 11, 300, 311, 11, 300, 311, 588, 51626], "temperature": 0.0, "avg_logprob": -0.14453145345052085, "compression_ratio": 1.7284345047923322, "no_speech_prob": 0.001500698970630765}, {"id": 1052, "seek": 420078, "start": 4226.0199999999995, "end": 4229.46, "text": " helpful to have an interesting dialogue. And I, I can't believe", "tokens": [51626, 4961, 281, 362, 364, 1880, 10221, 13, 400, 286, 11, 286, 393, 380, 1697, 51798], "temperature": 0.0, "avg_logprob": -0.14453145345052085, "compression_ratio": 1.7284345047923322, "no_speech_prob": 0.001500698970630765}, {"id": 1053, "seek": 422946, "start": 4229.5, "end": 4232.82, "text": " how proud I am, you know, that he's, that he appreciates that we", "tokens": [50366, 577, 4570, 286, 669, 11, 291, 458, 11, 300, 415, 311, 11, 300, 415, 3616, 1024, 300, 321, 50532], "temperature": 0.0, "avg_logprob": -0.19730504821328557, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.002081947633996606}, {"id": 1054, "seek": 422946, "start": 4232.82, "end": 4236.34, "text": " put the prep time into it. And, you know, had had decent", "tokens": [50532, 829, 264, 2666, 565, 666, 309, 13, 400, 11, 291, 458, 11, 632, 632, 8681, 50708], "temperature": 0.0, "avg_logprob": -0.19730504821328557, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.002081947633996606}, {"id": 1055, "seek": 422946, "start": 4236.34, "end": 4239.7, "text": " questions that were hopefully interesting for him, as well as", "tokens": [50708, 1651, 300, 645, 4696, 1880, 337, 796, 11, 382, 731, 382, 50876], "temperature": 0.0, "avg_logprob": -0.19730504821328557, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.002081947633996606}, {"id": 1056, "seek": 422946, "start": 4239.7, "end": 4244.46, "text": " our, as well as our audience. So Dr. Kilcher, lightspeed", "tokens": [50876, 527, 11, 382, 731, 382, 527, 4034, 13, 407, 2491, 13, 23912, 6759, 11, 5811, 494, 292, 51114], "temperature": 0.0, "avg_logprob": -0.19730504821328557, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.002081947633996606}, {"id": 1057, "seek": 422946, "start": 4244.46, "end": 4246.1, "text": " Kilcher, what should I take?", "tokens": [51114, 23912, 6759, 11, 437, 820, 286, 747, 30, 51196], "temperature": 0.0, "avg_logprob": -0.19730504821328557, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.002081947633996606}, {"id": 1058, "seek": 422946, "start": 4246.82, "end": 4251.7, "text": " It's cool is, I mean, his, um, yeah, I think is the thing he", "tokens": [51232, 467, 311, 1627, 307, 11, 286, 914, 11, 702, 11, 1105, 11, 1338, 11, 286, 519, 307, 264, 551, 415, 51476], "temperature": 0.0, "avg_logprob": -0.19730504821328557, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.002081947633996606}, {"id": 1059, "seek": 422946, "start": 4251.7, "end": 4254.7, "text": " mentioned at the end, like his humility, it kind of shines", "tokens": [51476, 2835, 412, 264, 917, 11, 411, 702, 27106, 11, 309, 733, 295, 28056, 51626], "temperature": 0.0, "avg_logprob": -0.19730504821328557, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.002081947633996606}, {"id": 1060, "seek": 422946, "start": 4254.7, "end": 4258.74, "text": " through everything he does. And he answers, he's like, you know,", "tokens": [51626, 807, 1203, 415, 775, 13, 400, 415, 6338, 11, 415, 311, 411, 11, 291, 458, 11, 51828], "temperature": 0.0, "avg_logprob": -0.19730504821328557, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.002081947633996606}, {"id": 1061, "seek": 425874, "start": 4258.78, "end": 4263.38, "text": " here's the best answer I can give. But, you know, he seems to", "tokens": [50366, 510, 311, 264, 1151, 1867, 286, 393, 976, 13, 583, 11, 291, 458, 11, 415, 2544, 281, 50596], "temperature": 0.0, "avg_logprob": -0.1930927526755411, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.001132858800701797}, {"id": 1062, "seek": 425874, "start": 4263.38, "end": 4270.74, "text": " be very, like, open and not, not, not very, yeah, one notices", "tokens": [50596, 312, 588, 11, 411, 11, 1269, 293, 406, 11, 406, 11, 406, 588, 11, 1338, 11, 472, 32978, 50964], "temperature": 0.0, "avg_logprob": -0.1930927526755411, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.001132858800701797}, {"id": 1063, "seek": 425874, "start": 4270.74, "end": 4274.66, "text": " he's not on Twitter. It's like, it's noticed that was a", "tokens": [50964, 415, 311, 406, 322, 5794, 13, 467, 311, 411, 11, 309, 311, 5694, 300, 390, 257, 51160], "temperature": 0.0, "avg_logprob": -0.1930927526755411, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.001132858800701797}, {"id": 1064, "seek": 425874, "start": 4274.66, "end": 4278.46, "text": " brilliant question. I think we should post that question on our", "tokens": [51160, 10248, 1168, 13, 286, 519, 321, 820, 2183, 300, 1168, 322, 527, 51350], "temperature": 0.0, "avg_logprob": -0.1930927526755411, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.001132858800701797}, {"id": 1065, "seek": 425874, "start": 4278.46, "end": 4281.86, "text": " Twitter. Because, you know, that there's that a bit for people", "tokens": [51350, 5794, 13, 1436, 11, 291, 458, 11, 300, 456, 311, 300, 257, 857, 337, 561, 51520], "temperature": 0.0, "avg_logprob": -0.1930927526755411, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.001132858800701797}, {"id": 1066, "seek": 425874, "start": 4281.86, "end": 4283.7, "text": " watching this in a year's time, it's probably forgotten about", "tokens": [51520, 1976, 341, 294, 257, 1064, 311, 565, 11, 309, 311, 1391, 11832, 466, 51612], "temperature": 0.0, "avg_logprob": -0.1930927526755411, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.001132858800701797}, {"id": 1067, "seek": 425874, "start": 4283.7, "end": 4287.26, "text": " but yeah, that ilia guy from open AI said that the models might", "tokens": [51612, 457, 1338, 11, 300, 1930, 654, 2146, 490, 1269, 7318, 848, 300, 264, 5245, 1062, 51790], "temperature": 0.0, "avg_logprob": -0.1930927526755411, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.001132858800701797}, {"id": 1068, "seek": 428726, "start": 4287.26, "end": 4291.18, "text": " be slightly conscious. I was exasperated by that. Because I", "tokens": [50364, 312, 4748, 6648, 13, 286, 390, 454, 296, 610, 770, 538, 300, 13, 1436, 286, 50560], "temperature": 0.0, "avg_logprob": -0.22556304931640625, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.06547806411981583}, {"id": 1069, "seek": 428726, "start": 4291.18, "end": 4294.5, "text": " watched his interview on Lex. And I know by saying bad things", "tokens": [50560, 6337, 702, 4049, 322, 24086, 13, 400, 286, 458, 538, 1566, 1578, 721, 50726], "temperature": 0.0, "avg_logprob": -0.22556304931640625, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.06547806411981583}, {"id": 1070, "seek": 428726, "start": 4294.5, "end": 4296.34, "text": " about him, he will never come on our podcast, but I don't think", "tokens": [50726, 466, 796, 11, 415, 486, 1128, 808, 322, 527, 7367, 11, 457, 286, 500, 380, 519, 50818], "temperature": 0.0, "avg_logprob": -0.22556304931640625, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.06547806411981583}, {"id": 1071, "seek": 428726, "start": 4296.34, "end": 4299.02, "text": " he would have done anyway. So it doesn't matter. But yeah, I", "tokens": [50818, 415, 576, 362, 1096, 4033, 13, 407, 309, 1177, 380, 1871, 13, 583, 1338, 11, 286, 50952], "temperature": 0.0, "avg_logprob": -0.22556304931640625, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.06547806411981583}, {"id": 1072, "seek": 428726, "start": 4299.02, "end": 4301.7, "text": " think that it's pretty bad.", "tokens": [50952, 519, 300, 309, 311, 1238, 1578, 13, 51086], "temperature": 0.0, "avg_logprob": -0.22556304931640625, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.06547806411981583}, {"id": 1073, "seek": 428726, "start": 4302.38, "end": 4307.38, "text": " What? Why? Yeah, why? It's like, it's like, you don't think", "tokens": [51120, 708, 30, 1545, 30, 865, 11, 983, 30, 467, 311, 411, 11, 309, 311, 411, 11, 291, 500, 380, 519, 51370], "temperature": 0.0, "avg_logprob": -0.22556304931640625, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.06547806411981583}, {"id": 1074, "seek": 428726, "start": 4307.38, "end": 4312.06, "text": " it's bad? No, he says, I think, because a lot of the folks at", "tokens": [51370, 309, 311, 1578, 30, 883, 11, 415, 1619, 11, 286, 519, 11, 570, 257, 688, 295, 264, 4024, 412, 51604], "temperature": 0.0, "avg_logprob": -0.22556304931640625, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.06547806411981583}, {"id": 1075, "seek": 428726, "start": 4312.06, "end": 4315.62, "text": " Open AI, they are, you know, like in the rationalist community,", "tokens": [51604, 7238, 7318, 11, 436, 366, 11, 291, 458, 11, 411, 294, 264, 15090, 468, 1768, 11, 51782], "temperature": 0.0, "avg_logprob": -0.22556304931640625, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.06547806411981583}, {"id": 1076, "seek": 431562, "start": 4315.82, "end": 4320.42, "text": " and they seriously believe that we're an imminent threat of the AI", "tokens": [50374, 293, 436, 6638, 1697, 300, 321, 434, 364, 44339, 4734, 295, 264, 7318, 50604], "temperature": 0.0, "avg_logprob": -0.18321454208509058, "compression_ratio": 1.5765124555160142, "no_speech_prob": 0.007207738701254129}, {"id": 1077, "seek": 431562, "start": 4320.66, "end": 4323.94, "text": " taking over the world and us being paper clips. And I think", "tokens": [50616, 1940, 670, 264, 1002, 293, 505, 885, 3035, 13117, 13, 400, 286, 519, 50780], "temperature": 0.0, "avg_logprob": -0.18321454208509058, "compression_ratio": 1.5765124555160142, "no_speech_prob": 0.007207738701254129}, {"id": 1078, "seek": 431562, "start": 4323.94, "end": 4327.42, "text": " it's next, I listened to his interview on Lex, and he sounded", "tokens": [50780, 309, 311, 958, 11, 286, 13207, 281, 702, 4049, 322, 24086, 11, 293, 415, 17714, 50954], "temperature": 0.0, "avg_logprob": -0.18321454208509058, "compression_ratio": 1.5765124555160142, "no_speech_prob": 0.007207738701254129}, {"id": 1079, "seek": 431562, "start": 4327.42, "end": 4330.42, "text": " like a salesman, talking about Codex and how it was going to", "tokens": [50954, 411, 257, 5763, 1601, 11, 1417, 466, 15549, 87, 293, 577, 309, 390, 516, 281, 51104], "temperature": 0.0, "avg_logprob": -0.18321454208509058, "compression_ratio": 1.5765124555160142, "no_speech_prob": 0.007207738701254129}, {"id": 1080, "seek": 431562, "start": 4330.42, "end": 4333.42, "text": " revolutionize everything. And I honestly think that there's just", "tokens": [51104, 8894, 1125, 1203, 13, 400, 286, 6095, 519, 300, 456, 311, 445, 51254], "temperature": 0.0, "avg_logprob": -0.18321454208509058, "compression_ratio": 1.5765124555160142, "no_speech_prob": 0.007207738701254129}, {"id": 1081, "seek": 431562, "start": 4333.42, "end": 4337.9, "text": " such a divergence between what they're saying and reality right", "tokens": [51254, 1270, 257, 47387, 1296, 437, 436, 434, 1566, 293, 4103, 558, 51478], "temperature": 0.0, "avg_logprob": -0.18321454208509058, "compression_ratio": 1.5765124555160142, "no_speech_prob": 0.007207738701254129}, {"id": 1082, "seek": 431562, "start": 4337.9, "end": 4338.14, "text": " now.", "tokens": [51478, 586, 13, 51490], "temperature": 0.0, "avg_logprob": -0.18321454208509058, "compression_ratio": 1.5765124555160142, "no_speech_prob": 0.007207738701254129}, {"id": 1083, "seek": 431562, "start": 4339.3, "end": 4343.74, "text": " Well, not to drift too far away from from our guests today.", "tokens": [51548, 1042, 11, 406, 281, 19699, 886, 1400, 1314, 490, 490, 527, 9804, 965, 13, 51770], "temperature": 0.0, "avg_logprob": -0.18321454208509058, "compression_ratio": 1.5765124555160142, "no_speech_prob": 0.007207738701254129}, {"id": 1084, "seek": 434374, "start": 4344.099999999999, "end": 4348.26, "text": " But so I thought, I thought it was just kind of a shower", "tokens": [50382, 583, 370, 286, 1194, 11, 286, 1194, 309, 390, 445, 733, 295, 257, 10128, 50590], "temperature": 0.0, "avg_logprob": -0.2106358932726311, "compression_ratio": 2.0695652173913044, "no_speech_prob": 0.010000239126384258}, {"id": 1085, "seek": 434374, "start": 4348.26, "end": 4352.38, "text": " thought, you know, like, you know, the the large neural", "tokens": [50590, 1194, 11, 291, 458, 11, 411, 11, 291, 458, 11, 264, 264, 2416, 18161, 50796], "temperature": 0.0, "avg_logprob": -0.2106358932726311, "compression_ratio": 2.0695652173913044, "no_speech_prob": 0.010000239126384258}, {"id": 1086, "seek": 434374, "start": 4352.38, "end": 4355.5, "text": " networks of today might be a little bit conscious, right?", "tokens": [50796, 9590, 295, 965, 1062, 312, 257, 707, 857, 6648, 11, 558, 30, 50952], "temperature": 0.0, "avg_logprob": -0.2106358932726311, "compression_ratio": 2.0695652173913044, "no_speech_prob": 0.010000239126384258}, {"id": 1087, "seek": 434374, "start": 4355.5, "end": 4359.0599999999995, "text": " And, and, and you just like, yeah, well, yeah, well, shower", "tokens": [50952, 400, 11, 293, 11, 293, 291, 445, 411, 11, 1338, 11, 731, 11, 1338, 11, 731, 11, 10128, 51130], "temperature": 0.0, "avg_logprob": -0.2106358932726311, "compression_ratio": 2.0695652173913044, "no_speech_prob": 0.010000239126384258}, {"id": 1088, "seek": 434374, "start": 4359.0599999999995, "end": 4361.46, "text": " thought, and it is a shower thought like it needs on", "tokens": [51130, 1194, 11, 293, 309, 307, 257, 10128, 1194, 411, 309, 2203, 322, 51250], "temperature": 0.0, "avg_logprob": -0.2106358932726311, "compression_ratio": 2.0695652173913044, "no_speech_prob": 0.010000239126384258}, {"id": 1089, "seek": 434374, "start": 4361.46, "end": 4365.5, "text": " Twitter, it's just something you tweet out. And, and it brings up", "tokens": [51250, 5794, 11, 309, 311, 445, 746, 291, 15258, 484, 13, 400, 11, 293, 309, 5607, 493, 51452], "temperature": 0.0, "avg_logprob": -0.2106358932726311, "compression_ratio": 2.0695652173913044, "no_speech_prob": 0.010000239126384258}, {"id": 1090, "seek": 434374, "start": 4365.5, "end": 4368.099999999999, "text": " interesting questions, like it brings up interesting questions,", "tokens": [51452, 1880, 1651, 11, 411, 309, 5607, 493, 1880, 1651, 11, 51582], "temperature": 0.0, "avg_logprob": -0.2106358932726311, "compression_ratio": 2.0695652173913044, "no_speech_prob": 0.010000239126384258}, {"id": 1091, "seek": 434374, "start": 4368.099999999999, "end": 4371.099999999999, "text": " like, you know, you're a you're a ball of neurons, like you're", "tokens": [51582, 411, 11, 291, 458, 11, 291, 434, 257, 291, 434, 257, 2594, 295, 22027, 11, 411, 291, 434, 51732], "temperature": 0.0, "avg_logprob": -0.2106358932726311, "compression_ratio": 2.0695652173913044, "no_speech_prob": 0.010000239126384258}, {"id": 1092, "seek": 437110, "start": 4371.14, "end": 4373.740000000001, "text": " just a slap together piece of matter, right? You have", "tokens": [50366, 445, 257, 21075, 1214, 2522, 295, 1871, 11, 558, 30, 509, 362, 50496], "temperature": 0.0, "avg_logprob": -0.15788216923558435, "compression_ratio": 1.7537688442211055, "no_speech_prob": 0.006485930178314447}, {"id": 1093, "seek": 437110, "start": 4373.740000000001, "end": 4378.34, "text": " consciousness. So clearly, like something about, you know,", "tokens": [50496, 10081, 13, 407, 4448, 11, 411, 746, 466, 11, 291, 458, 11, 50726], "temperature": 0.0, "avg_logprob": -0.15788216923558435, "compression_ratio": 1.7537688442211055, "no_speech_prob": 0.006485930178314447}, {"id": 1094, "seek": 437110, "start": 4378.54, "end": 4381.900000000001, "text": " learning systems combined with data, or maybe not even", "tokens": [50736, 2539, 3652, 9354, 365, 1412, 11, 420, 1310, 406, 754, 50904], "temperature": 0.0, "avg_logprob": -0.15788216923558435, "compression_ratio": 1.7537688442211055, "no_speech_prob": 0.006485930178314447}, {"id": 1095, "seek": 437110, "start": 4381.900000000001, "end": 4386.1, "text": " combined with data gives rise to consciousness. So why can't", "tokens": [50904, 9354, 365, 1412, 2709, 6272, 281, 10081, 13, 407, 983, 393, 380, 51114], "temperature": 0.0, "avg_logprob": -0.15788216923558435, "compression_ratio": 1.7537688442211055, "no_speech_prob": 0.006485930178314447}, {"id": 1096, "seek": 437110, "start": 4386.5, "end": 4392.54, "text": " why can't another, you know, in silico, slap together system", "tokens": [51134, 983, 393, 380, 1071, 11, 291, 458, 11, 294, 3425, 2789, 11, 21075, 1214, 1185, 51436], "temperature": 0.0, "avg_logprob": -0.15788216923558435, "compression_ratio": 1.7537688442211055, "no_speech_prob": 0.006485930178314447}, {"id": 1097, "seek": 437110, "start": 4392.54, "end": 4397.660000000001, "text": " of neurons ingested with data be slightly conscious or have", "tokens": [51436, 295, 22027, 3957, 21885, 365, 1412, 312, 4748, 6648, 420, 362, 51692], "temperature": 0.0, "avg_logprob": -0.15788216923558435, "compression_ratio": 1.7537688442211055, "no_speech_prob": 0.006485930178314447}, {"id": 1098, "seek": 439766, "start": 4397.7, "end": 4401.82, "text": " like, some properties, like, and that's that's essentially, yeah,", "tokens": [50366, 411, 11, 512, 7221, 11, 411, 11, 293, 300, 311, 300, 311, 4476, 11, 1338, 11, 50572], "temperature": 0.0, "avg_logprob": -0.23224042386424784, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.029733173549175262}, {"id": 1099, "seek": 439766, "start": 4402.62, "end": 4407.78, "text": " Benjo refused to give like a humble, the humble person he is,", "tokens": [50612, 3964, 5134, 14654, 281, 976, 411, 257, 16735, 11, 264, 16735, 954, 415, 307, 11, 50870], "temperature": 0.0, "avg_logprob": -0.23224042386424784, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.029733173549175262}, {"id": 1100, "seek": 439766, "start": 4407.78, "end": 4411.82, "text": " he refused to give like, you know, the the the strong take on", "tokens": [50870, 415, 14654, 281, 976, 411, 11, 291, 458, 11, 264, 264, 264, 2068, 747, 322, 51072], "temperature": 0.0, "avg_logprob": -0.23224042386424784, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.029733173549175262}, {"id": 1101, "seek": 439766, "start": 4411.82, "end": 4416.62, "text": " that, but that would have because he might just this is my", "tokens": [51072, 300, 11, 457, 300, 576, 362, 570, 415, 1062, 445, 341, 307, 452, 51312], "temperature": 0.0, "avg_logprob": -0.23224042386424784, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.029733173549175262}, {"id": 1102, "seek": 439766, "start": 4416.62, "end": 4420.7, "text": " opinion, not his obviously, but reading the consciousness prior", "tokens": [51312, 4800, 11, 406, 702, 2745, 11, 457, 3760, 264, 10081, 4059, 51516], "temperature": 0.0, "avg_logprob": -0.23224042386424784, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.029733173549175262}, {"id": 1103, "seek": 439766, "start": 4420.7, "end": 4425.98, "text": " paper, it is not too far off. He formulates consciousness as", "tokens": [51516, 3035, 11, 309, 307, 406, 886, 1400, 766, 13, 634, 1254, 26192, 10081, 382, 51780], "temperature": 0.0, "avg_logprob": -0.23224042386424784, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.029733173549175262}, {"id": 1104, "seek": 442598, "start": 4425.98, "end": 4430.139999999999, "text": " having these elements of, you know, I have my internal state,", "tokens": [50364, 1419, 613, 4959, 295, 11, 291, 458, 11, 286, 362, 452, 6920, 1785, 11, 50572], "temperature": 0.0, "avg_logprob": -0.11391511897450869, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.042040228843688965}, {"id": 1105, "seek": 442598, "start": 4430.139999999999, "end": 4434.86, "text": " which is sort of everything in my brain that I could bring bring", "tokens": [50572, 597, 307, 1333, 295, 1203, 294, 452, 3567, 300, 286, 727, 1565, 1565, 50808], "temperature": 0.0, "avg_logprob": -0.11391511897450869, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.042040228843688965}, {"id": 1106, "seek": 442598, "start": 4434.86, "end": 4438.86, "text": " up into my forefront, then I get some input from the outside", "tokens": [50808, 493, 666, 452, 27287, 11, 550, 286, 483, 512, 4846, 490, 264, 2380, 51008], "temperature": 0.0, "avg_logprob": -0.11391511897450869, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.042040228843688965}, {"id": 1107, "seek": 442598, "start": 4438.86, "end": 4443.66, "text": " world. And through the input, I then filter, like with an", "tokens": [51008, 1002, 13, 400, 807, 264, 4846, 11, 286, 550, 6608, 11, 411, 365, 364, 51248], "temperature": 0.0, "avg_logprob": -0.11391511897450869, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.042040228843688965}, {"id": 1108, "seek": 442598, "start": 4443.66, "end": 4449.419999999999, "text": " attention mechanism, I do I look what in my mind, could I now", "tokens": [51248, 3202, 7513, 11, 286, 360, 286, 574, 437, 294, 452, 1575, 11, 727, 286, 586, 51536], "temperature": 0.0, "avg_logprob": -0.11391511897450869, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.042040228843688965}, {"id": 1109, "seek": 442598, "start": 4449.419999999999, "end": 4454.0599999999995, "text": " bring into focus, right? And that is by use of something like", "tokens": [51536, 1565, 666, 1879, 11, 558, 30, 400, 300, 307, 538, 764, 295, 746, 411, 51768], "temperature": 0.0, "avg_logprob": -0.11391511897450869, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.042040228843688965}, {"id": 1110, "seek": 445406, "start": 4454.06, "end": 4459.620000000001, "text": " an attention mechanism. And then I take that thing. And I put it", "tokens": [50364, 364, 3202, 7513, 13, 400, 550, 286, 747, 300, 551, 13, 400, 286, 829, 309, 50642], "temperature": 0.0, "avg_logprob": -0.12230259895324708, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.012816673144698143}, {"id": 1111, "seek": 445406, "start": 4459.620000000001, "end": 4466.54, "text": " into these abstract concepts I use I represent. I represent the", "tokens": [50642, 666, 613, 12649, 10392, 286, 764, 286, 2906, 13, 286, 2906, 264, 50988], "temperature": 0.0, "avg_logprob": -0.12230259895324708, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.012816673144698143}, {"id": 1112, "seek": 445406, "start": 4466.54, "end": 4470.9800000000005, "text": " concepts in my head as a sparse factor graph. And by focusing on", "tokens": [50988, 10392, 294, 452, 1378, 382, 257, 637, 11668, 5952, 4295, 13, 400, 538, 8416, 322, 51210], "temperature": 0.0, "avg_logprob": -0.12230259895324708, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.012816673144698143}, {"id": 1113, "seek": 445406, "start": 4470.9800000000005, "end": 4474.38, "text": " parts of that, I can then make inferences in this sparse", "tokens": [51210, 3166, 295, 300, 11, 286, 393, 550, 652, 13596, 2667, 294, 341, 637, 11668, 51380], "temperature": 0.0, "avg_logprob": -0.12230259895324708, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.012816673144698143}, {"id": 1114, "seek": 445406, "start": 4474.38, "end": 4478.26, "text": " factor graph and so on. Now, obviously, something like GPT three", "tokens": [51380, 5952, 4295, 293, 370, 322, 13, 823, 11, 2745, 11, 746, 411, 26039, 51, 1045, 51574], "temperature": 0.0, "avg_logprob": -0.12230259895324708, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.012816673144698143}, {"id": 1115, "seek": 445406, "start": 4478.26, "end": 4482.820000000001, "text": " doesn't have all of that, at least not explicitly, but some of", "tokens": [51574, 1177, 380, 362, 439, 295, 300, 11, 412, 1935, 406, 20803, 11, 457, 512, 295, 51802], "temperature": 0.0, "avg_logprob": -0.12230259895324708, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.012816673144698143}, {"id": 1116, "seek": 448282, "start": 4482.82, "end": 4486.78, "text": " it is there, right? It's, you know, I have a piece of input, I", "tokens": [50364, 309, 307, 456, 11, 558, 30, 467, 311, 11, 291, 458, 11, 286, 362, 257, 2522, 295, 4846, 11, 286, 50562], "temperature": 0.0, "avg_logprob": -0.13650301098823547, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.006094135344028473}, {"id": 1117, "seek": 448282, "start": 4486.78, "end": 4490.66, "text": " have giant amount of weights, I use an attention mechanism to", "tokens": [50562, 362, 7410, 2372, 295, 17443, 11, 286, 764, 364, 3202, 7513, 281, 50756], "temperature": 0.0, "avg_logprob": -0.13650301098823547, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.006094135344028473}, {"id": 1118, "seek": 448282, "start": 4490.66, "end": 4492.7, "text": " sort of see what I can focus on.", "tokens": [50756, 1333, 295, 536, 437, 286, 393, 1879, 322, 13, 50858], "temperature": 0.0, "avg_logprob": -0.13650301098823547, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.006094135344028473}, {"id": 1119, "seek": 448282, "start": 4493.66, "end": 4496.139999999999, "text": " Yeah, but yeah, but I think that I think that's a very", "tokens": [50906, 865, 11, 457, 1338, 11, 457, 286, 519, 300, 286, 519, 300, 311, 257, 588, 51030], "temperature": 0.0, "avg_logprob": -0.13650301098823547, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.006094135344028473}, {"id": 1120, "seek": 448282, "start": 4496.139999999999, "end": 4500.179999999999, "text": " declarative description of consciousness. And at its roots,", "tokens": [51030, 16694, 1166, 3855, 295, 10081, 13, 400, 412, 1080, 10669, 11, 51232], "temperature": 0.0, "avg_logprob": -0.13650301098823547, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.006094135344028473}, {"id": 1121, "seek": 448282, "start": 4500.179999999999, "end": 4503.66, "text": " it's about the phenomenological experience. Right. And I know", "tokens": [51232, 309, 311, 466, 264, 9388, 4383, 1752, 13, 1779, 13, 400, 286, 458, 51406], "temperature": 0.0, "avg_logprob": -0.13650301098823547, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.006094135344028473}, {"id": 1122, "seek": 448282, "start": 4503.66, "end": 4507.42, "text": " we discussed computationalism and panpsychism. Let's not go down", "tokens": [51406, 321, 7152, 28270, 1434, 293, 2462, 1878, 16384, 1434, 13, 961, 311, 406, 352, 760, 51594], "temperature": 0.0, "avg_logprob": -0.13650301098823547, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.006094135344028473}, {"id": 1123, "seek": 448282, "start": 4507.42, "end": 4511.66, "text": " that rabbit hole. But surely, they don't think that this model can", "tokens": [51594, 300, 19509, 5458, 13, 583, 11468, 11, 436, 500, 380, 519, 300, 341, 2316, 393, 51806], "temperature": 0.0, "avg_logprob": -0.13650301098823547, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.006094135344028473}, {"id": 1124, "seek": 451166, "start": 4511.7, "end": 4513.22, "text": " feel well, but so this is", "tokens": [50366, 841, 731, 11, 457, 370, 341, 307, 50442], "temperature": 0.0, "avg_logprob": -0.20484040333674505, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.002322804881259799}, {"id": 1125, "seek": 451166, "start": 4513.22, "end": 4517.0599999999995, "text": " consciousness is not about feeling. It's about being being", "tokens": [50442, 10081, 307, 406, 466, 2633, 13, 467, 311, 466, 885, 885, 50634], "temperature": 0.0, "avg_logprob": -0.20484040333674505, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.002322804881259799}, {"id": 1126, "seek": 451166, "start": 4517.0599999999995, "end": 4522.82, "text": " like aware of of like, I don't even know what it is. I'm just", "tokens": [50634, 411, 3650, 295, 295, 411, 11, 286, 500, 380, 754, 458, 437, 309, 307, 13, 286, 478, 445, 50922], "temperature": 0.0, "avg_logprob": -0.20484040333674505, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.002322804881259799}, {"id": 1127, "seek": 451166, "start": 4522.82, "end": 4527.78, "text": " saying that it sounded not too far away from what the", "tokens": [50922, 1566, 300, 309, 17714, 406, 886, 1400, 1314, 490, 437, 264, 51170], "temperature": 0.0, "avg_logprob": -0.20484040333674505, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.002322804881259799}, {"id": 1128, "seek": 451166, "start": 4527.78, "end": 4531.7, "text": " consciousness prior paper was about. And yes, I realize it's", "tokens": [51170, 10081, 4059, 3035, 390, 466, 13, 400, 2086, 11, 286, 4325, 309, 311, 51366], "temperature": 0.0, "avg_logprob": -0.20484040333674505, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.002322804881259799}, {"id": 1129, "seek": 451166, "start": 4531.7, "end": 4534.94, "text": " called the consciousness prior and not consciousness. But you", "tokens": [51366, 1219, 264, 10081, 4059, 293, 406, 10081, 13, 583, 291, 51528], "temperature": 0.0, "avg_logprob": -0.20484040333674505, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.002322804881259799}, {"id": 1130, "seek": 451166, "start": 4534.94, "end": 4535.18, "text": " know,", "tokens": [51528, 458, 11, 51540], "temperature": 0.0, "avg_logprob": -0.20484040333674505, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.002322804881259799}, {"id": 1131, "seek": 451166, "start": 4535.98, "end": 4538.3, "text": " yeah, I mean, I think he answered it the way a scientist", "tokens": [51580, 1338, 11, 286, 914, 11, 286, 519, 415, 10103, 309, 264, 636, 257, 12662, 51696], "temperature": 0.0, "avg_logprob": -0.20484040333674505, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.002322804881259799}, {"id": 1132, "seek": 453830, "start": 4538.38, "end": 4541.18, "text": " should answer it. And I was really happy with his answer,", "tokens": [50368, 820, 1867, 309, 13, 400, 286, 390, 534, 2055, 365, 702, 1867, 11, 50508], "temperature": 0.0, "avg_logprob": -0.13207451747013974, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0049048420041799545}, {"id": 1133, "seek": 453830, "start": 4541.18, "end": 4547.5, "text": " which is, okay, a consciousness has to be some activity of", "tokens": [50508, 597, 307, 11, 1392, 11, 257, 10081, 575, 281, 312, 512, 5191, 295, 50824], "temperature": 0.0, "avg_logprob": -0.13207451747013974, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0049048420041799545}, {"id": 1134, "seek": 453830, "start": 4547.5, "end": 4550.3, "text": " neurons and firings or whatever in the brain or else we're", "tokens": [50824, 22027, 293, 12159, 1109, 420, 2035, 294, 264, 3567, 420, 1646, 321, 434, 50964], "temperature": 0.0, "avg_logprob": -0.13207451747013974, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0049048420041799545}, {"id": 1135, "seek": 453830, "start": 4550.3, "end": 4554.3, "text": " talking about magic. And that's not in the field of science. And", "tokens": [50964, 1417, 466, 5585, 13, 400, 300, 311, 406, 294, 264, 2519, 295, 3497, 13, 400, 51164], "temperature": 0.0, "avg_logprob": -0.13207451747013974, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0049048420041799545}, {"id": 1136, "seek": 453830, "start": 4554.3, "end": 4557.2, "text": " B, you know, whatever that thing is, it's obviously quite", "tokens": [51164, 363, 11, 291, 458, 11, 2035, 300, 551, 307, 11, 309, 311, 2745, 1596, 51309], "temperature": 0.0, "avg_logprob": -0.13207451747013974, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0049048420041799545}, {"id": 1137, "seek": 453830, "start": 4557.2, "end": 4560.9400000000005, "text": " nuanced and complicated. And we don't have we don't know yet.", "tokens": [51309, 45115, 293, 6179, 13, 400, 321, 500, 380, 362, 321, 500, 380, 458, 1939, 13, 51496], "temperature": 0.0, "avg_logprob": -0.13207451747013974, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0049048420041799545}, {"id": 1138, "seek": 453830, "start": 4560.9800000000005, "end": 4565.1, "text": " So we need to have some humility here, which means we", "tokens": [51498, 407, 321, 643, 281, 362, 512, 27106, 510, 11, 597, 1355, 321, 51704], "temperature": 0.0, "avg_logprob": -0.13207451747013974, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0049048420041799545}, {"id": 1139, "seek": 453830, "start": 4565.1, "end": 4568.14, "text": " shouldn't be alarmist. So we don't need to be going in, you", "tokens": [51704, 4659, 380, 312, 14183, 468, 13, 407, 321, 500, 380, 643, 281, 312, 516, 294, 11, 291, 51856], "temperature": 0.0, "avg_logprob": -0.13207451747013974, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0049048420041799545}, {"id": 1140, "seek": 456814, "start": 4568.9800000000005, "end": 4573.02, "text": " know, burning books tomorrow because because we created a, you", "tokens": [50406, 458, 11, 9488, 3642, 4153, 570, 570, 321, 2942, 257, 11, 291, 50608], "temperature": 0.0, "avg_logprob": -0.14427704597587016, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0006070370436646044}, {"id": 1141, "seek": 456814, "start": 4573.02, "end": 4576.5, "text": " know, GPT, whatever, that anytime its wheel is spinning,", "tokens": [50608, 458, 11, 26039, 51, 11, 2035, 11, 300, 13038, 1080, 5589, 307, 15640, 11, 50782], "temperature": 0.0, "avg_logprob": -0.14427704597587016, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0006070370436646044}, {"id": 1142, "seek": 456814, "start": 4576.5, "end": 4578.860000000001, "text": " and it's actually suffering. You know, if you ask it a", "tokens": [50782, 293, 309, 311, 767, 7755, 13, 509, 458, 11, 498, 291, 1029, 309, 257, 50900], "temperature": 0.0, "avg_logprob": -0.14427704597587016, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0006070370436646044}, {"id": 1143, "seek": 456814, "start": 4578.860000000001, "end": 4581.06, "text": " question that's too hard, and it's spinning, it's because", "tokens": [50900, 1168, 300, 311, 886, 1152, 11, 293, 309, 311, 15640, 11, 309, 311, 570, 51010], "temperature": 0.0, "avg_logprob": -0.14427704597587016, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0006070370436646044}, {"id": 1144, "seek": 456814, "start": 4581.06, "end": 4583.660000000001, "text": " you're hurting it and it's suffering. And so we need to", "tokens": [51010, 291, 434, 17744, 309, 293, 309, 311, 7755, 13, 400, 370, 321, 643, 281, 51140], "temperature": 0.0, "avg_logprob": -0.14427704597587016, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0006070370436646044}, {"id": 1145, "seek": 456814, "start": 4583.660000000001, "end": 4586.34, "text": " turn it off like right away. But wait, we can't turn it off", "tokens": [51140, 1261, 309, 766, 411, 558, 1314, 13, 583, 1699, 11, 321, 393, 380, 1261, 309, 766, 51274], "temperature": 0.0, "avg_logprob": -0.14427704597587016, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0006070370436646044}, {"id": 1146, "seek": 456814, "start": 4586.34, "end": 4589.3, "text": " because then we'd be like, murdering, you know, a sentient", "tokens": [51274, 570, 550, 321, 1116, 312, 411, 11, 6568, 278, 11, 291, 458, 11, 257, 2279, 1196, 51422], "temperature": 0.0, "avg_logprob": -0.14427704597587016, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0006070370436646044}, {"id": 1147, "seek": 456814, "start": 4589.3, "end": 4593.46, "text": " being or something, like we're way, way too, in our infantile", "tokens": [51422, 885, 420, 746, 11, 411, 321, 434, 636, 11, 636, 886, 11, 294, 527, 16757, 794, 51630], "temperature": 0.0, "avg_logprob": -0.14427704597587016, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0006070370436646044}, {"id": 1148, "seek": 459346, "start": 4593.5, "end": 4597.96, "text": " understanding of, you know, this type of complex, complex", "tokens": [50366, 3701, 295, 11, 291, 458, 11, 341, 2010, 295, 3997, 11, 3997, 50589], "temperature": 0.0, "avg_logprob": -0.13811830572179845, "compression_ratio": 1.6589147286821706, "no_speech_prob": 0.003593088360503316}, {"id": 1149, "seek": 459346, "start": 4597.96, "end": 4601.18, "text": " behavior, that's the human mind and consciousness to be at that", "tokens": [50589, 5223, 11, 300, 311, 264, 1952, 1575, 293, 10081, 281, 312, 412, 300, 50750], "temperature": 0.0, "avg_logprob": -0.13811830572179845, "compression_ratio": 1.6589147286821706, "no_speech_prob": 0.003593088360503316}, {"id": 1150, "seek": 459346, "start": 4601.18, "end": 4605.66, "text": " point. So from my perspective, he answered it completely 100%", "tokens": [50750, 935, 13, 407, 490, 452, 4585, 11, 415, 10103, 309, 2584, 2319, 4, 50974], "temperature": 0.0, "avg_logprob": -0.13811830572179845, "compression_ratio": 1.6589147286821706, "no_speech_prob": 0.003593088360503316}, {"id": 1151, "seek": 459346, "start": 4605.66, "end": 4608.86, "text": " scientifically. And there's a lot of folks out there who are", "tokens": [50974, 39719, 13, 400, 456, 311, 257, 688, 295, 4024, 484, 456, 567, 366, 51134], "temperature": 0.0, "avg_logprob": -0.13811830572179845, "compression_ratio": 1.6589147286821706, "no_speech_prob": 0.003593088360503316}, {"id": 1152, "seek": 459346, "start": 4608.86, "end": 4612.14, "text": " supposed to be scientists that spend a lot of time with, you", "tokens": [51134, 3442, 281, 312, 7708, 300, 3496, 257, 688, 295, 565, 365, 11, 291, 51298], "temperature": 0.0, "avg_logprob": -0.13811830572179845, "compression_ratio": 1.6589147286821706, "no_speech_prob": 0.003593088360503316}, {"id": 1153, "seek": 459346, "start": 4612.14, "end": 4617.22, "text": " know, unscientific, you know, thinking about it. Cool. Let's", "tokens": [51298, 458, 11, 2693, 5412, 1089, 11, 291, 458, 11, 1953, 466, 309, 13, 8561, 13, 961, 311, 51552], "temperature": 0.0, "avg_logprob": -0.13811830572179845, "compression_ratio": 1.6589147286821706, "no_speech_prob": 0.003593088360503316}, {"id": 1154, "seek": 459346, "start": 4617.22, "end": 4620.42, "text": " talk a little bit. What one of the things that I really found", "tokens": [51552, 751, 257, 707, 857, 13, 708, 472, 295, 264, 721, 300, 286, 534, 1352, 51712], "temperature": 0.0, "avg_logprob": -0.13811830572179845, "compression_ratio": 1.6589147286821706, "no_speech_prob": 0.003593088360503316}, {"id": 1155, "seek": 462042, "start": 4620.42, "end": 4623.9400000000005, "text": " interesting about Benjo's ideas, other than the causality", "tokens": [50364, 1880, 466, 3964, 5134, 311, 3487, 11, 661, 813, 264, 3302, 1860, 50540], "temperature": 0.0, "avg_logprob": -0.21281493337530838, "compression_ratio": 1.76, "no_speech_prob": 0.0845167487859726}, {"id": 1156, "seek": 462042, "start": 4623.9400000000005, "end": 4627.78, "text": " stuff and the system to stuff is this notion of diversity.", "tokens": [50540, 1507, 293, 264, 1185, 281, 1507, 307, 341, 10710, 295, 8811, 13, 50732], "temperature": 0.0, "avg_logprob": -0.21281493337530838, "compression_ratio": 1.76, "no_speech_prob": 0.0845167487859726}, {"id": 1157, "seek": 462042, "start": 4628.38, "end": 4630.9, "text": " We've had conversations with Kenneth Stanley all about open", "tokens": [50762, 492, 600, 632, 7315, 365, 33735, 28329, 439, 466, 1269, 50888], "temperature": 0.0, "avg_logprob": -0.21281493337530838, "compression_ratio": 1.76, "no_speech_prob": 0.0845167487859726}, {"id": 1158, "seek": 462042, "start": 4630.9, "end": 4633.58, "text": " endedness and diversity preservation. We've also had", "tokens": [50888, 4590, 1287, 293, 8811, 27257, 13, 492, 600, 611, 632, 51022], "temperature": 0.0, "avg_logprob": -0.21281493337530838, "compression_ratio": 1.76, "no_speech_prob": 0.0845167487859726}, {"id": 1159, "seek": 462042, "start": 4633.58, "end": 4638.34, "text": " conversations with Friston about the importance of balancing", "tokens": [51022, 7315, 365, 1526, 47345, 466, 264, 7379, 295, 22495, 51260], "temperature": 0.0, "avg_logprob": -0.21281493337530838, "compression_ratio": 1.76, "no_speech_prob": 0.0845167487859726}, {"id": 1160, "seek": 462042, "start": 4638.34, "end": 4642.06, "text": " relative entropy and so on. And we have all of these curses in", "tokens": [51260, 4972, 30867, 293, 370, 322, 13, 400, 321, 362, 439, 295, 613, 1262, 6196, 294, 51446], "temperature": 0.0, "avg_logprob": -0.21281493337530838, "compression_ratio": 1.76, "no_speech_prob": 0.0845167487859726}, {"id": 1161, "seek": 462042, "start": 4642.06, "end": 4644.7, "text": " empirical learning, right? The statistical curses, the", "tokens": [51446, 31886, 2539, 11, 558, 30, 440, 22820, 1262, 6196, 11, 264, 51578], "temperature": 0.0, "avg_logprob": -0.21281493337530838, "compression_ratio": 1.76, "no_speech_prob": 0.0845167487859726}, {"id": 1162, "seek": 462042, "start": 4644.7, "end": 4645.9, "text": " approximation curses.", "tokens": [51578, 28023, 1262, 6196, 13, 51638], "temperature": 0.0, "avg_logprob": -0.21281493337530838, "compression_ratio": 1.76, "no_speech_prob": 0.0845167487859726}, {"id": 1163, "seek": 462042, "start": 4646.74, "end": 4650.34, "text": " Dimensionality, we have to mention dimensionality and", "tokens": [51680, 20975, 3378, 1860, 11, 321, 362, 281, 2152, 10139, 1860, 293, 51860], "temperature": 0.0, "avg_logprob": -0.21281493337530838, "compression_ratio": 1.76, "no_speech_prob": 0.0845167487859726}, {"id": 1164, "seek": 465034, "start": 4650.34, "end": 4652.9800000000005, "text": " even, I mean, you know, we're talking about curses in the", "tokens": [50364, 754, 11, 286, 914, 11, 291, 458, 11, 321, 434, 1417, 466, 1262, 6196, 294, 264, 50496], "temperature": 0.0, "avg_logprob": -0.17708686140717053, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0002650473325047642}, {"id": 1165, "seek": 465034, "start": 4652.9800000000005, "end": 4655.74, "text": " Monty, you know, the Markov chain Monte Carlo in the sense of", "tokens": [50496, 4713, 874, 11, 291, 458, 11, 264, 3934, 5179, 5021, 38105, 45112, 294, 264, 2020, 295, 50634], "temperature": 0.0, "avg_logprob": -0.17708686140717053, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0002650473325047642}, {"id": 1166, "seek": 465034, "start": 4655.74, "end": 4658.66, "text": " it being a high dimensional space. And we need to assume that", "tokens": [50634, 309, 885, 257, 1090, 18795, 1901, 13, 400, 321, 643, 281, 6552, 300, 50780], "temperature": 0.0, "avg_logprob": -0.17708686140717053, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0002650473325047642}, {"id": 1167, "seek": 465034, "start": 4658.66, "end": 4662.7, "text": " there's some structure around where these modes are. So all of", "tokens": [50780, 456, 311, 512, 3877, 926, 689, 613, 14068, 366, 13, 407, 439, 295, 50982], "temperature": 0.0, "avg_logprob": -0.17708686140717053, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0002650473325047642}, {"id": 1168, "seek": 465034, "start": 4662.7, "end": 4665.900000000001, "text": " these approaches are ways of simultaneously, and, you know,", "tokens": [50982, 613, 11587, 366, 2098, 295, 16561, 11, 293, 11, 291, 458, 11, 51142], "temperature": 0.0, "avg_logprob": -0.17708686140717053, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0002650473325047642}, {"id": 1169, "seek": 465034, "start": 4665.900000000001, "end": 4668.78, "text": " being able to explore but not being cursed. So yeah, what was", "tokens": [51142, 885, 1075, 281, 6839, 457, 406, 885, 29498, 13, 407, 1338, 11, 437, 390, 51286], "temperature": 0.0, "avg_logprob": -0.17708686140717053, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0002650473325047642}, {"id": 1170, "seek": 465034, "start": 4668.78, "end": 4669.3, "text": " your take on that?", "tokens": [51286, 428, 747, 322, 300, 30, 51312], "temperature": 0.0, "avg_logprob": -0.17708686140717053, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0002650473325047642}, {"id": 1171, "seek": 465034, "start": 4670.26, "end": 4677.02, "text": " Well, any one of my take was that I like that he's so", "tokens": [51360, 1042, 11, 604, 472, 295, 452, 747, 390, 300, 286, 411, 300, 415, 311, 370, 51698], "temperature": 0.0, "avg_logprob": -0.17708686140717053, "compression_ratio": 1.6884615384615385, "no_speech_prob": 0.0002650473325047642}, {"id": 1172, "seek": 467702, "start": 4677.06, "end": 4681.18, "text": " interested in abstraction, because, you know, to me, that's", "tokens": [50366, 3102, 294, 37765, 11, 570, 11, 291, 458, 11, 281, 385, 11, 300, 311, 50572], "temperature": 0.0, "avg_logprob": -0.11883983927324784, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.04600686579942703}, {"id": 1173, "seek": 467702, "start": 4681.18, "end": 4684.14, "text": " been not only one, you know, it's not only one of the larger", "tokens": [50572, 668, 406, 787, 472, 11, 291, 458, 11, 309, 311, 406, 787, 472, 295, 264, 4833, 50720], "temperature": 0.0, "avg_logprob": -0.11883983927324784, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.04600686579942703}, {"id": 1174, "seek": 467702, "start": 4684.18, "end": 4687.580000000001, "text": " mysteries, at least for me, I mean, I don't know, of the kind of", "tokens": [50722, 30785, 11, 412, 1935, 337, 385, 11, 286, 914, 11, 286, 500, 380, 458, 11, 295, 264, 733, 295, 50892], "temperature": 0.0, "avg_logprob": -0.11883983927324784, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.04600686579942703}, {"id": 1175, "seek": 467702, "start": 4687.580000000001, "end": 4692.5, "text": " the universe is abstraction, idealism, you know, platonic", "tokens": [50892, 264, 6445, 307, 37765, 11, 7157, 1434, 11, 291, 458, 11, 3403, 11630, 51138], "temperature": 0.0, "avg_logprob": -0.11883983927324784, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.04600686579942703}, {"id": 1176, "seek": 467702, "start": 4692.5, "end": 4695.1, "text": " thinking, whatever. I mean, the whole point is just that he", "tokens": [51138, 1953, 11, 2035, 13, 286, 914, 11, 264, 1379, 935, 307, 445, 300, 415, 51268], "temperature": 0.0, "avg_logprob": -0.11883983927324784, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.04600686579942703}, {"id": 1177, "seek": 467702, "start": 4695.1, "end": 4700.06, "text": " views abstraction as a key to pragmatically useful, you know,", "tokens": [51268, 6809, 37765, 382, 257, 2141, 281, 33394, 76, 5030, 4420, 11, 291, 458, 11, 51516], "temperature": 0.0, "avg_logprob": -0.11883983927324784, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.04600686579942703}, {"id": 1178, "seek": 467702, "start": 4700.06, "end": 4704.860000000001, "text": " pass forward. And it's a hard problem, a really hard problem.", "tokens": [51516, 1320, 2128, 13, 400, 309, 311, 257, 1152, 1154, 11, 257, 534, 1152, 1154, 13, 51756], "temperature": 0.0, "avg_logprob": -0.11883983927324784, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.04600686579942703}, {"id": 1179, "seek": 470486, "start": 4704.94, "end": 4709.179999999999, "text": " And, you know, his focus right now is on kind of graph based", "tokens": [50368, 400, 11, 291, 458, 11, 702, 1879, 558, 586, 307, 322, 733, 295, 4295, 2361, 50580], "temperature": 0.0, "avg_logprob": -0.10906124114990234, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.001324907992966473}, {"id": 1180, "seek": 470486, "start": 4709.62, "end": 4712.54, "text": " structures. And I have to admit, you know, for me to you,", "tokens": [50602, 9227, 13, 400, 286, 362, 281, 9796, 11, 291, 458, 11, 337, 385, 281, 291, 11, 50748], "temperature": 0.0, "avg_logprob": -0.10906124114990234, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.001324907992966473}, {"id": 1181, "seek": 470486, "start": 4712.54, "end": 4715.0599999999995, "text": " they're quite seductive and appealing. I don't know if", "tokens": [50748, 436, 434, 1596, 9643, 11130, 488, 293, 23842, 13, 286, 500, 380, 458, 498, 50874], "temperature": 0.0, "avg_logprob": -0.10906124114990234, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.001324907992966473}, {"id": 1182, "seek": 470486, "start": 4715.0599999999995, "end": 4717.98, "text": " they're the the right path forward, but it's definitely", "tokens": [50874, 436, 434, 264, 264, 558, 3100, 2128, 11, 457, 309, 311, 2138, 51020], "temperature": 0.0, "avg_logprob": -0.10906124114990234, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.001324907992966473}, {"id": 1183, "seek": 470486, "start": 4717.98, "end": 4721.74, "text": " cool to see a lot of research, looking into graph based, you", "tokens": [51020, 1627, 281, 536, 257, 688, 295, 2132, 11, 1237, 666, 4295, 2361, 11, 291, 51208], "temperature": 0.0, "avg_logprob": -0.10906124114990234, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.001324907992966473}, {"id": 1184, "seek": 470486, "start": 4721.74, "end": 4724.94, "text": " know, methods, or, you know, hyper graph based methods,", "tokens": [51208, 458, 11, 7150, 11, 420, 11, 291, 458, 11, 9848, 4295, 2361, 7150, 11, 51368], "temperature": 0.0, "avg_logprob": -0.10906124114990234, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.001324907992966473}, {"id": 1185, "seek": 470486, "start": 4724.94, "end": 4728.62, "text": " whatever they are, they seem to definitely be a promising", "tokens": [51368, 2035, 436, 366, 11, 436, 1643, 281, 2138, 312, 257, 20257, 51552], "temperature": 0.0, "avg_logprob": -0.10906124114990234, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.001324907992966473}, {"id": 1186, "seek": 470486, "start": 4728.62, "end": 4732.179999999999, "text": " path forward. And I think we're in for, hopefully, if we can", "tokens": [51552, 3100, 2128, 13, 400, 286, 519, 321, 434, 294, 337, 11, 4696, 11, 498, 321, 393, 51730], "temperature": 0.0, "avg_logprob": -0.10906124114990234, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.001324907992966473}, {"id": 1187, "seek": 473218, "start": 4732.18, "end": 4735.38, "text": " continue to progress at a reasonable rate, you know, some", "tokens": [50364, 2354, 281, 4205, 412, 257, 10585, 3314, 11, 291, 458, 11, 512, 50524], "temperature": 0.0, "avg_logprob": -0.16128304425407858, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0018096109852194786}, {"id": 1188, "seek": 473218, "start": 4735.38, "end": 4737.26, "text": " some interesting decades ahead.", "tokens": [50524, 512, 1880, 7878, 2286, 13, 50618], "temperature": 0.0, "avg_logprob": -0.16128304425407858, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0018096109852194786}, {"id": 1189, "seek": 473218, "start": 4738.740000000001, "end": 4745.34, "text": " I mean, I would, I would also postulate that maybe our most of", "tokens": [50692, 286, 914, 11, 286, 576, 11, 286, 576, 611, 2183, 5256, 300, 1310, 527, 881, 295, 51022], "temperature": 0.0, "avg_logprob": -0.16128304425407858, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0018096109852194786}, {"id": 1190, "seek": 473218, "start": 4745.34, "end": 4748.820000000001, "text": " our, let's say benchmarks that we're thinking about today aren't", "tokens": [51022, 527, 11, 718, 311, 584, 43751, 300, 321, 434, 1953, 466, 965, 3212, 380, 51196], "temperature": 0.0, "avg_logprob": -0.16128304425407858, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0018096109852194786}, {"id": 1191, "seek": 473218, "start": 4748.860000000001, "end": 4754.740000000001, "text": " necessarily suited to to because his argument was by creating", "tokens": [51198, 4725, 24736, 281, 281, 570, 702, 6770, 390, 538, 4084, 51492], "temperature": 0.0, "avg_logprob": -0.16128304425407858, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0018096109852194786}, {"id": 1192, "seek": 473218, "start": 4754.740000000001, "end": 4759.700000000001, "text": " abstractions, it might actually, you know, help your ability to", "tokens": [51492, 12649, 626, 11, 309, 1062, 767, 11, 291, 458, 11, 854, 428, 3485, 281, 51740], "temperature": 0.0, "avg_logprob": -0.16128304425407858, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0018096109852194786}, {"id": 1193, "seek": 475970, "start": 4759.74, "end": 4762.74, "text": " learn something, right, which is a thing that we all", "tokens": [50366, 1466, 746, 11, 558, 11, 597, 307, 257, 551, 300, 321, 439, 50516], "temperature": 0.0, "avg_logprob": -0.11782759753140536, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.06844938546419144}, {"id": 1194, "seek": 475970, "start": 4762.74, "end": 4765.34, "text": " intuitively understand in the world, if I have good", "tokens": [50516, 46506, 1223, 294, 264, 1002, 11, 498, 286, 362, 665, 50646], "temperature": 0.0, "avg_logprob": -0.11782759753140536, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.06844938546419144}, {"id": 1195, "seek": 475970, "start": 4765.34, "end": 4768.099999999999, "text": " abstractions, I can transfer my knowledge from here to here and", "tokens": [50646, 12649, 626, 11, 286, 393, 5003, 452, 3601, 490, 510, 281, 510, 293, 50784], "temperature": 0.0, "avg_logprob": -0.11782759753140536, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.06844938546419144}, {"id": 1196, "seek": 475970, "start": 4768.099999999999, "end": 4771.0199999999995, "text": " from here to here. Yeah, in something like image net", "tokens": [50784, 490, 510, 281, 510, 13, 865, 11, 294, 746, 411, 3256, 2533, 50930], "temperature": 0.0, "avg_logprob": -0.11782759753140536, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.06844938546419144}, {"id": 1197, "seek": 475970, "start": 4771.0199999999995, "end": 4774.3, "text": " classification, or whatnot, or most of the benchmarks we have", "tokens": [50930, 21538, 11, 420, 25882, 11, 420, 881, 295, 264, 43751, 321, 362, 51094], "temperature": 0.0, "avg_logprob": -0.11782759753140536, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.06844938546419144}, {"id": 1198, "seek": 475970, "start": 4774.3, "end": 4778.98, "text": " today, the necessity of abstractions is probably not like", "tokens": [51094, 965, 11, 264, 24217, 295, 12649, 626, 307, 1391, 406, 411, 51328], "temperature": 0.0, "avg_logprob": -0.11782759753140536, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.06844938546419144}, {"id": 1199, "seek": 475970, "start": 4778.98, "end": 4782.42, "text": " the data, the hardness of the problem probably doesn't", "tokens": [51328, 264, 1412, 11, 264, 44019, 295, 264, 1154, 1391, 1177, 380, 51500], "temperature": 0.0, "avg_logprob": -0.11782759753140536, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.06844938546419144}, {"id": 1200, "seek": 475970, "start": 4782.42, "end": 4786.98, "text": " require abstractions to be introduced. And therefore, the", "tokens": [51500, 3651, 12649, 626, 281, 312, 7268, 13, 400, 4412, 11, 264, 51728], "temperature": 0.0, "avg_logprob": -0.11782759753140536, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.06844938546419144}, {"id": 1201, "seek": 478698, "start": 4786.98, "end": 4790.459999999999, "text": " limiting factor here might not only be the models themselves,", "tokens": [50364, 22083, 5952, 510, 1062, 406, 787, 312, 264, 5245, 2969, 11, 50538], "temperature": 0.0, "avg_logprob": -0.12405200143462246, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.012035644613206387}, {"id": 1202, "seek": 478698, "start": 4790.459999999999, "end": 4797.139999999999, "text": " but also, let's say, our ability to even measure the progress", "tokens": [50538, 457, 611, 11, 718, 311, 584, 11, 527, 3485, 281, 754, 3481, 264, 4205, 50872], "temperature": 0.0, "avg_logprob": -0.12405200143462246, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.012035644613206387}, {"id": 1203, "seek": 478698, "start": 4797.139999999999, "end": 4800.459999999999, "text": " one could make with abstractions. And I think that's gonna change", "tokens": [50872, 472, 727, 652, 365, 12649, 626, 13, 400, 286, 519, 300, 311, 799, 1319, 51038], "temperature": 0.0, "avg_logprob": -0.12405200143462246, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.012035644613206387}, {"id": 1204, "seek": 478698, "start": 4800.459999999999, "end": 4803.62, "text": " maybe in the near future, because people are going into", "tokens": [51038, 1310, 294, 264, 2651, 2027, 11, 570, 561, 366, 516, 666, 51196], "temperature": 0.0, "avg_logprob": -0.12405200143462246, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.012035644613206387}, {"id": 1205, "seek": 478698, "start": 4803.62, "end": 4808.299999999999, "text": " multimodality research and so on. And there, I think the concept", "tokens": [51196, 32972, 378, 1860, 2132, 293, 370, 322, 13, 400, 456, 11, 286, 519, 264, 3410, 51430], "temperature": 0.0, "avg_logprob": -0.12405200143462246, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.012035644613206387}, {"id": 1206, "seek": 478698, "start": 4808.299999999999, "end": 4812.86, "text": " of sort of concepts, maybe not abstractions, but at least", "tokens": [51430, 295, 1333, 295, 10392, 11, 1310, 406, 12649, 626, 11, 457, 412, 1935, 51658], "temperature": 0.0, "avg_logprob": -0.12405200143462246, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.012035644613206387}, {"id": 1207, "seek": 478698, "start": 4812.86, "end": 4815.98, "text": " something like concepts is way more, more important.", "tokens": [51658, 746, 411, 10392, 307, 636, 544, 11, 544, 1021, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12405200143462246, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.012035644613206387}, {"id": 1208, "seek": 481698, "start": 4817.0199999999995, "end": 4819.54, "text": " Yeah, there's, let me just follow real quickly there, Tim,", "tokens": [50366, 865, 11, 456, 311, 11, 718, 385, 445, 1524, 957, 2661, 456, 11, 7172, 11, 50492], "temperature": 0.0, "avg_logprob": -0.16080299775991866, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001324862940236926}, {"id": 1209, "seek": 481698, "start": 4819.54, "end": 4822.379999999999, "text": " because there's something very interesting there to Yannick,", "tokens": [50492, 570, 456, 311, 746, 588, 1880, 456, 281, 398, 969, 618, 11, 50634], "temperature": 0.0, "avg_logprob": -0.16080299775991866, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001324862940236926}, {"id": 1210, "seek": 481698, "start": 4822.379999999999, "end": 4827.099999999999, "text": " which is the lack of good tools to deal with multimodal, you", "tokens": [50634, 597, 307, 264, 5011, 295, 665, 3873, 281, 2028, 365, 32972, 378, 304, 11, 291, 50870], "temperature": 0.0, "avg_logprob": -0.16080299775991866, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001324862940236926}, {"id": 1211, "seek": 481698, "start": 4827.099999999999, "end": 4830.259999999999, "text": " know, sets of data results. And a lot of times, we're just", "tokens": [50870, 458, 11, 6352, 295, 1412, 3542, 13, 400, 257, 688, 295, 1413, 11, 321, 434, 445, 51028], "temperature": 0.0, "avg_logprob": -0.16080299775991866, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001324862940236926}, {"id": 1212, "seek": 481698, "start": 4830.259999999999, "end": 4833.82, "text": " throwing out kind of valuable, valuable sources of data, just", "tokens": [51028, 10238, 484, 733, 295, 8263, 11, 8263, 7139, 295, 1412, 11, 445, 51206], "temperature": 0.0, "avg_logprob": -0.16080299775991866, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001324862940236926}, {"id": 1213, "seek": 481698, "start": 4833.82, "end": 4836.82, "text": " because, you know, we don't have a good tool sets to do with", "tokens": [51206, 570, 11, 291, 458, 11, 321, 500, 380, 362, 257, 665, 2290, 6352, 281, 360, 365, 51356], "temperature": 0.0, "avg_logprob": -0.16080299775991866, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001324862940236926}, {"id": 1214, "seek": 481698, "start": 4836.82, "end": 4839.219999999999, "text": " them, like think about the self driving car, like the whole,", "tokens": [51356, 552, 11, 411, 519, 466, 264, 2698, 4840, 1032, 11, 411, 264, 1379, 11, 51476], "temperature": 0.0, "avg_logprob": -0.16080299775991866, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001324862940236926}, {"id": 1215, "seek": 481698, "start": 4839.62, "end": 4843.139999999999, "text": " should it be vision versus LiDAR debate? Why? Why isn't it", "tokens": [51496, 820, 309, 312, 5201, 5717, 8349, 35, 1899, 7958, 30, 1545, 30, 1545, 1943, 380, 309, 51672], "temperature": 0.0, "avg_logprob": -0.16080299775991866, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001324862940236926}, {"id": 1216, "seek": 484314, "start": 4843.14, "end": 4846.820000000001, "text": " both? I mean, you know, if you can for $5, you can throw on", "tokens": [50364, 1293, 30, 286, 914, 11, 291, 458, 11, 498, 291, 393, 337, 1848, 20, 11, 291, 393, 3507, 322, 50548], "temperature": 0.0, "avg_logprob": -0.10132001920510794, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0010321203153580427}, {"id": 1217, "seek": 484314, "start": 4846.820000000001, "end": 4850.62, "text": " some cheap, you know, LiDAR sensors or something, maybe not", "tokens": [50548, 512, 7084, 11, 291, 458, 11, 8349, 35, 1899, 14840, 420, 746, 11, 1310, 406, 50738], "temperature": 0.0, "avg_logprob": -0.10132001920510794, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0010321203153580427}, {"id": 1218, "seek": 484314, "start": 4850.62, "end": 4852.9400000000005, "text": " something fancy, but something cheap, why wouldn't we take", "tokens": [50738, 746, 10247, 11, 457, 746, 7084, 11, 983, 2759, 380, 321, 747, 50854], "temperature": 0.0, "avg_logprob": -0.10132001920510794, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0010321203153580427}, {"id": 1219, "seek": 484314, "start": 4852.9400000000005, "end": 4856.860000000001, "text": " advantage of that data? And it's, it's really because we don't", "tokens": [50854, 5002, 295, 300, 1412, 30, 400, 309, 311, 11, 309, 311, 534, 570, 321, 500, 380, 51050], "temperature": 0.0, "avg_logprob": -0.10132001920510794, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0010321203153580427}, {"id": 1220, "seek": 484314, "start": 4856.860000000001, "end": 4860.700000000001, "text": " have good tools to deal with, with multimodal data.", "tokens": [51050, 362, 665, 3873, 281, 2028, 365, 11, 365, 32972, 378, 304, 1412, 13, 51242], "temperature": 0.0, "avg_logprob": -0.10132001920510794, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0010321203153580427}, {"id": 1221, "seek": 484314, "start": 4861.34, "end": 4864.62, "text": " We got to a good point in the discussion where we were talking", "tokens": [51274, 492, 658, 281, 257, 665, 935, 294, 264, 5017, 689, 321, 645, 1417, 51438], "temperature": 0.0, "avg_logprob": -0.10132001920510794, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0010321203153580427}, {"id": 1222, "seek": 484314, "start": 4864.62, "end": 4868.5, "text": " about the nature of finding abstractions. And I wonder where", "tokens": [51438, 466, 264, 3687, 295, 5006, 12649, 626, 13, 400, 286, 2441, 689, 51632], "temperature": 0.0, "avg_logprob": -0.10132001920510794, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0010321203153580427}, {"id": 1223, "seek": 484314, "start": 4868.5, "end": 4870.9800000000005, "text": " the neural networks can find abstractions. Now, the, the", "tokens": [51632, 264, 18161, 9590, 393, 915, 12649, 626, 13, 823, 11, 264, 11, 264, 51756], "temperature": 0.0, "avg_logprob": -0.10132001920510794, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0010321203153580427}, {"id": 1224, "seek": 487098, "start": 4871.0199999999995, "end": 4874.94, "text": " cynical view is that humans kind of create these inductive", "tokens": [50366, 46345, 1910, 307, 300, 6255, 733, 295, 1884, 613, 31612, 488, 50562], "temperature": 0.0, "avg_logprob": -0.10209558434682349, "compression_ratio": 1.89375, "no_speech_prob": 0.0016679445980116725}, {"id": 1225, "seek": 487098, "start": 4874.94, "end": 4877.299999999999, "text": " priors, and they represent the abstraction. So certainly in the", "tokens": [50562, 1790, 830, 11, 293, 436, 2906, 264, 37765, 13, 407, 3297, 294, 264, 50680], "temperature": 0.0, "avg_logprob": -0.10209558434682349, "compression_ratio": 1.89375, "no_speech_prob": 0.0016679445980116725}, {"id": 1226, "seek": 487098, "start": 4877.299999999999, "end": 4879.9, "text": " case of geometric deep learning, and that's kind of what's", "tokens": [50680, 1389, 295, 33246, 2452, 2539, 11, 293, 300, 311, 733, 295, 437, 311, 50810], "temperature": 0.0, "avg_logprob": -0.10209558434682349, "compression_ratio": 1.89375, "no_speech_prob": 0.0016679445980116725}, {"id": 1227, "seek": 487098, "start": 4879.9, "end": 4883.219999999999, "text": " happening, we put the, the priors in there to reduce the size of", "tokens": [50810, 2737, 11, 321, 829, 264, 11, 264, 1790, 830, 294, 456, 281, 5407, 264, 2744, 295, 50976], "temperature": 0.0, "avg_logprob": -0.10209558434682349, "compression_ratio": 1.89375, "no_speech_prob": 0.0016679445980116725}, {"id": 1228, "seek": 487098, "start": 4883.219999999999, "end": 4885.78, "text": " the approximation space. And Keith and I had an interesting", "tokens": [50976, 264, 28023, 1901, 13, 400, 20613, 293, 286, 632, 364, 1880, 51104], "temperature": 0.0, "avg_logprob": -0.10209558434682349, "compression_ratio": 1.89375, "no_speech_prob": 0.0016679445980116725}, {"id": 1229, "seek": 487098, "start": 4885.78, "end": 4888.139999999999, "text": " idea yesterday that there's a kind of analogy between geometric", "tokens": [51104, 1558, 5186, 300, 456, 311, 257, 733, 295, 21663, 1296, 33246, 51222], "temperature": 0.0, "avg_logprob": -0.10209558434682349, "compression_ratio": 1.89375, "no_speech_prob": 0.0016679445980116725}, {"id": 1230, "seek": 487098, "start": 4888.139999999999, "end": 4890.62, "text": " deep learning and causal representation learning. So I", "tokens": [51222, 2452, 2539, 293, 38755, 10290, 2539, 13, 407, 286, 51346], "temperature": 0.0, "avg_logprob": -0.10209558434682349, "compression_ratio": 1.89375, "no_speech_prob": 0.0016679445980116725}, {"id": 1231, "seek": 487098, "start": 4890.62, "end": 4892.9, "text": " think Keith, you went online and you found a really interesting", "tokens": [51346, 519, 20613, 11, 291, 1437, 2950, 293, 291, 1352, 257, 534, 1880, 51460], "temperature": 0.0, "avg_logprob": -0.10209558434682349, "compression_ratio": 1.89375, "no_speech_prob": 0.0016679445980116725}, {"id": 1232, "seek": 487098, "start": 4892.9, "end": 4895.82, "text": " definition of a causal model, which is that it's kind of", "tokens": [51460, 7123, 295, 257, 38755, 2316, 11, 597, 307, 300, 309, 311, 733, 295, 51606], "temperature": 0.0, "avg_logprob": -0.10209558434682349, "compression_ratio": 1.89375, "no_speech_prob": 0.0016679445980116725}, {"id": 1233, "seek": 487098, "start": 4895.82, "end": 4900.099999999999, "text": " immune to, let's say, adversarial examples. So what a model", "tokens": [51606, 11992, 281, 11, 718, 311, 584, 11, 17641, 44745, 5110, 13, 407, 437, 257, 2316, 51820], "temperature": 0.0, "avg_logprob": -0.10209558434682349, "compression_ratio": 1.89375, "no_speech_prob": 0.0016679445980116725}, {"id": 1234, "seek": 490010, "start": 4900.14, "end": 4903.1, "text": " does right now, is it learns a relationship essentially between", "tokens": [50366, 775, 558, 586, 11, 307, 309, 27152, 257, 2480, 4476, 1296, 50514], "temperature": 0.0, "avg_logprob": -0.2046974306184102, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0018098994623869658}, {"id": 1235, "seek": 490010, "start": 4903.1, "end": 4906.22, "text": " let's say every single pixel and something happening, right,", "tokens": [50514, 718, 311, 584, 633, 2167, 19261, 293, 746, 2737, 11, 558, 11, 50670], "temperature": 0.0, "avg_logprob": -0.2046974306184102, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0018098994623869658}, {"id": 1236, "seek": 490010, "start": 4906.26, "end": 4909.06, "text": " which is why that model is vulnerable.", "tokens": [50672, 597, 307, 983, 300, 2316, 307, 10955, 13, 50812], "temperature": 0.0, "avg_logprob": -0.2046974306184102, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0018098994623869658}, {"id": 1237, "seek": 490010, "start": 4909.58, "end": 4912.620000000001, "text": " Yeah, so that was the, and yeah, and I would love to get your", "tokens": [50838, 865, 11, 370, 300, 390, 264, 11, 293, 1338, 11, 293, 286, 576, 959, 281, 483, 428, 50990], "temperature": 0.0, "avg_logprob": -0.2046974306184102, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0018098994623869658}, {"id": 1238, "seek": 490010, "start": 4912.620000000001, "end": 4916.02, "text": " comment on that. But that was, you know, this paper, and I", "tokens": [50990, 2871, 322, 300, 13, 583, 300, 390, 11, 291, 458, 11, 341, 3035, 11, 293, 286, 51160], "temperature": 0.0, "avg_logprob": -0.2046974306184102, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0018098994623869658}, {"id": 1239, "seek": 490010, "start": 4916.02, "end": 4918.42, "text": " could go dig it up, and I can get the reference right now where", "tokens": [51160, 727, 352, 2528, 309, 493, 11, 293, 286, 393, 483, 264, 6408, 558, 586, 689, 51280], "temperature": 0.0, "avg_logprob": -0.2046974306184102, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0018098994623869658}, {"id": 1240, "seek": 490010, "start": 4918.42, "end": 4922.26, "text": " it said, you know, hey, what is the difference between a causal", "tokens": [51280, 309, 848, 11, 291, 458, 11, 4177, 11, 437, 307, 264, 2649, 1296, 257, 38755, 51472], "temperature": 0.0, "avg_logprob": -0.2046974306184102, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0018098994623869658}, {"id": 1241, "seek": 490010, "start": 4922.58, "end": 4926.46, "text": " or prediction from a causal model versus a prediction from a", "tokens": [51488, 420, 17630, 490, 257, 38755, 2316, 5717, 257, 17630, 490, 257, 51682], "temperature": 0.0, "avg_logprob": -0.2046974306184102, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0018098994623869658}, {"id": 1242, "seek": 492646, "start": 4926.46, "end": 4930.94, "text": " non causal model. And the point was that, well, almost by", "tokens": [50364, 2107, 38755, 2316, 13, 400, 264, 935, 390, 300, 11, 731, 11, 1920, 538, 50588], "temperature": 0.0, "avg_logprob": -0.11093534322885366, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.003593245055526495}, {"id": 1243, "seek": 492646, "start": 4930.94, "end": 4935.74, "text": " definition, really, if you have a causal model, then if you", "tokens": [50588, 7123, 11, 534, 11, 498, 291, 362, 257, 38755, 2316, 11, 550, 498, 291, 50828], "temperature": 0.0, "avg_logprob": -0.11093534322885366, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.003593245055526495}, {"id": 1244, "seek": 492646, "start": 4935.74, "end": 4938.46, "text": " perturbed the inputs, the prediction that you get out of", "tokens": [50828, 13269, 374, 2883, 264, 15743, 11, 264, 17630, 300, 291, 483, 484, 295, 50964], "temperature": 0.0, "avg_logprob": -0.11093534322885366, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.003593245055526495}, {"id": 1245, "seek": 492646, "start": 4938.46, "end": 4942.34, "text": " it remains a valid, a valid output, because after all, if", "tokens": [50964, 309, 7023, 257, 7363, 11, 257, 7363, 5598, 11, 570, 934, 439, 11, 498, 51158], "temperature": 0.0, "avg_logprob": -0.11093534322885366, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.003593245055526495}, {"id": 1246, "seek": 492646, "start": 4942.34, "end": 4945.06, "text": " it's a causal model, and it's reflective of a sort of the", "tokens": [51158, 309, 311, 257, 38755, 2316, 11, 293, 309, 311, 28931, 295, 257, 1333, 295, 264, 51294], "temperature": 0.0, "avg_logprob": -0.11093534322885366, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.003593245055526495}, {"id": 1247, "seek": 492646, "start": 4945.06, "end": 4948.14, "text": " causal structure of the world or whatnot, then sure, that's a", "tokens": [51294, 38755, 3877, 295, 264, 1002, 420, 25882, 11, 550, 988, 11, 300, 311, 257, 51448], "temperature": 0.0, "avg_logprob": -0.11093534322885366, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.003593245055526495}, {"id": 1248, "seek": 492646, "start": 4948.14, "end": 4952.06, "text": " valid, valid output. Whereas, if it's non causal, it has the", "tokens": [51448, 7363, 11, 7363, 5598, 13, 13813, 11, 498, 309, 311, 2107, 38755, 11, 309, 575, 264, 51644], "temperature": 0.0, "avg_logprob": -0.11093534322885366, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.003593245055526495}, {"id": 1249, "seek": 492646, "start": 4952.06, "end": 4955.58, "text": " potential to learn all these kind of spurious, spurious", "tokens": [51644, 3995, 281, 1466, 439, 613, 733, 295, 637, 24274, 11, 637, 24274, 51820], "temperature": 0.0, "avg_logprob": -0.11093534322885366, "compression_ratio": 1.8537549407114624, "no_speech_prob": 0.003593245055526495}, {"id": 1250, "seek": 495558, "start": 4955.58, "end": 4957.58, "text": " structures, and therefore, that's why you get the", "tokens": [50364, 9227, 11, 293, 4412, 11, 300, 311, 983, 291, 483, 264, 50464], "temperature": 0.0, "avg_logprob": -0.13442950778537327, "compression_ratio": 1.8551236749116609, "no_speech_prob": 0.0019876244477927685}, {"id": 1251, "seek": 495558, "start": 4957.58, "end": 4960.94, "text": " capability of these adversarial examples where you just, you", "tokens": [50464, 13759, 295, 613, 17641, 44745, 5110, 689, 291, 445, 11, 291, 50632], "temperature": 0.0, "avg_logprob": -0.13442950778537327, "compression_ratio": 1.8551236749116609, "no_speech_prob": 0.0019876244477927685}, {"id": 1252, "seek": 495558, "start": 4960.94, "end": 4964.54, "text": " know, put a little rainbow pixel somewhere, and it messes up the", "tokens": [50632, 458, 11, 829, 257, 707, 18526, 19261, 4079, 11, 293, 309, 2082, 279, 493, 264, 50812], "temperature": 0.0, "avg_logprob": -0.13442950778537327, "compression_ratio": 1.8551236749116609, "no_speech_prob": 0.0019876244477927685}, {"id": 1253, "seek": 495558, "start": 4964.54, "end": 4967.14, "text": " class because it had this spurious connection.", "tokens": [50812, 1508, 570, 309, 632, 341, 637, 24274, 4984, 13, 50942], "temperature": 0.0, "avg_logprob": -0.13442950778537327, "compression_ratio": 1.8551236749116609, "no_speech_prob": 0.0019876244477927685}, {"id": 1254, "seek": 495558, "start": 4967.58, "end": 4971.0199999999995, "text": " I mean, in the same vein, you could also, the adversarial", "tokens": [50964, 286, 914, 11, 294, 264, 912, 30669, 11, 291, 727, 611, 11, 264, 17641, 44745, 51136], "temperature": 0.0, "avg_logprob": -0.13442950778537327, "compression_ratio": 1.8551236749116609, "no_speech_prob": 0.0019876244477927685}, {"id": 1255, "seek": 495558, "start": 4971.0199999999995, "end": 4974.38, "text": " examples are there because of inaccuracies, because we don't", "tokens": [51136, 5110, 366, 456, 570, 295, 37957, 374, 20330, 11, 570, 321, 500, 380, 51304], "temperature": 0.0, "avg_logprob": -0.13442950778537327, "compression_ratio": 1.8551236749116609, "no_speech_prob": 0.0019876244477927685}, {"id": 1256, "seek": 495558, "start": 4974.38, "end": 4977.74, "text": " have the perfect discriminative function, right? I could also", "tokens": [51304, 362, 264, 2176, 20828, 1166, 2445, 11, 558, 30, 286, 727, 611, 51472], "temperature": 0.0, "avg_logprob": -0.13442950778537327, "compression_ratio": 1.8551236749116609, "no_speech_prob": 0.0019876244477927685}, {"id": 1257, "seek": 495558, "start": 4977.74, "end": 4981.5, "text": " say, well, if I just had the correct discriminative functions,", "tokens": [51472, 584, 11, 731, 11, 498, 286, 445, 632, 264, 3006, 20828, 1166, 6828, 11, 51660], "temperature": 0.0, "avg_logprob": -0.13442950778537327, "compression_ratio": 1.8551236749116609, "no_speech_prob": 0.0019876244477927685}, {"id": 1258, "seek": 495558, "start": 4981.5, "end": 4984.86, "text": " it doesn't need to be causal. If I just had like the right", "tokens": [51660, 309, 1177, 380, 643, 281, 312, 38755, 13, 759, 286, 445, 632, 411, 264, 558, 51828], "temperature": 0.0, "avg_logprob": -0.13442950778537327, "compression_ratio": 1.8551236749116609, "no_speech_prob": 0.0019876244477927685}, {"id": 1259, "seek": 498486, "start": 4984.86, "end": 4989.78, "text": " partitioning of my input space, then, you know, I'm super not", "tokens": [50364, 24808, 278, 295, 452, 4846, 1901, 11, 550, 11, 291, 458, 11, 286, 478, 1687, 406, 50610], "temperature": 0.0, "avg_logprob": -0.16280264757117446, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.013423070311546326}, {"id": 1260, "seek": 498486, "start": 4989.78, "end": 4993.78, "text": " vulnerable to adversarial attacks. I guess the real question", "tokens": [50610, 10955, 281, 17641, 44745, 8122, 13, 286, 2041, 264, 957, 1168, 50810], "temperature": 0.0, "avg_logprob": -0.16280264757117446, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.013423070311546326}, {"id": 1261, "seek": 498486, "start": 4993.78, "end": 4999.259999999999, "text": " would be, would that technically amount to a causal model if I", "tokens": [50810, 576, 312, 11, 576, 300, 12120, 2372, 281, 257, 38755, 2316, 498, 286, 51084], "temperature": 0.0, "avg_logprob": -0.16280264757117446, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.013423070311546326}, {"id": 1262, "seek": 498486, "start": 4999.259999999999, "end": 5002.94, "text": " had, you know, the perfect partitioning of the input", "tokens": [51084, 632, 11, 291, 458, 11, 264, 2176, 24808, 278, 295, 264, 4846, 51268], "temperature": 0.0, "avg_logprob": -0.16280264757117446, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.013423070311546326}, {"id": 1263, "seek": 498486, "start": 5002.94, "end": 5007.139999999999, "text": " space into my classes? I don't know, that's like, is there like", "tokens": [51268, 1901, 666, 452, 5359, 30, 286, 500, 380, 458, 11, 300, 311, 411, 11, 307, 456, 411, 51478], "temperature": 0.0, "avg_logprob": -0.16280264757117446, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.013423070311546326}, {"id": 1264, "seek": 498486, "start": 5007.139999999999, "end": 5010.339999999999, "text": " a mathematical equivalent from that to a causal model? Who", "tokens": [51478, 257, 18894, 10344, 490, 300, 281, 257, 38755, 2316, 30, 2102, 51638], "temperature": 0.0, "avg_logprob": -0.16280264757117446, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.013423070311546326}, {"id": 1265, "seek": 498486, "start": 5010.339999999999, "end": 5010.82, "text": " knows?", "tokens": [51638, 3255, 30, 51662], "temperature": 0.0, "avg_logprob": -0.16280264757117446, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.013423070311546326}, {"id": 1266, "seek": 501082, "start": 5011.179999999999, "end": 5015.299999999999, "text": " Right. Yeah, I think there's probably, it's probably, certainly,", "tokens": [50382, 1779, 13, 865, 11, 286, 519, 456, 311, 1391, 11, 309, 311, 1391, 11, 3297, 11, 50588], "temperature": 0.0, "avg_logprob": -0.14463859904896129, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.0015010740607976913}, {"id": 1267, "seek": 501082, "start": 5015.74, "end": 5018.74, "text": " if you have the perfect discriminative function, it's", "tokens": [50610, 498, 291, 362, 264, 2176, 20828, 1166, 2445, 11, 309, 311, 50760], "temperature": 0.0, "avg_logprob": -0.14463859904896129, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.0015010740607976913}, {"id": 1268, "seek": 501082, "start": 5018.74, "end": 5021.94, "text": " probably the discriminative function that you would derive", "tokens": [50760, 1391, 264, 20828, 1166, 2445, 300, 291, 576, 28446, 50920], "temperature": 0.0, "avg_logprob": -0.14463859904896129, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.0015010740607976913}, {"id": 1269, "seek": 501082, "start": 5021.94, "end": 5026.139999999999, "text": " from a causal model. I'm not 100% sure you can go go in the", "tokens": [50920, 490, 257, 38755, 2316, 13, 286, 478, 406, 2319, 4, 988, 291, 393, 352, 352, 294, 264, 51130], "temperature": 0.0, "avg_logprob": -0.14463859904896129, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.0015010740607976913}, {"id": 1270, "seek": 501082, "start": 5026.139999999999, "end": 5028.78, "text": " reverse, because I imagine there probably is some some", "tokens": [51130, 9943, 11, 570, 286, 3811, 456, 1391, 307, 512, 512, 51262], "temperature": 0.0, "avg_logprob": -0.14463859904896129, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.0015010740607976913}, {"id": 1271, "seek": 501082, "start": 5028.78, "end": 5033.62, "text": " information loss going from, you know, a causal model to, you", "tokens": [51262, 1589, 4470, 516, 490, 11, 291, 458, 11, 257, 38755, 2316, 281, 11, 291, 51504], "temperature": 0.0, "avg_logprob": -0.14463859904896129, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.0015010740607976913}, {"id": 1272, "seek": 501082, "start": 5033.62, "end": 5037.78, "text": " know, like, for example, I'll give you an example. In the cases", "tokens": [51504, 458, 11, 411, 11, 337, 1365, 11, 286, 603, 976, 291, 364, 1365, 13, 682, 264, 3331, 51712], "temperature": 0.0, "avg_logprob": -0.14463859904896129, "compression_ratio": 1.817391304347826, "no_speech_prob": 0.0015010740607976913}, {"id": 1273, "seek": 503778, "start": 5037.82, "end": 5040.66, "text": " of, say, production systems, you know, so, so these little", "tokens": [50366, 295, 11, 584, 11, 4265, 3652, 11, 291, 458, 11, 370, 11, 370, 613, 707, 50508], "temperature": 0.0, "avg_logprob": -0.13300334706025965, "compression_ratio": 1.781512605042017, "no_speech_prob": 0.0027148565277457237}, {"id": 1274, "seek": 503778, "start": 5041.139999999999, "end": 5045.74, "text": " rewriting rules or whatever, the definition there of a causal", "tokens": [50532, 319, 19868, 4474, 420, 2035, 11, 264, 7123, 456, 295, 257, 38755, 50762], "temperature": 0.0, "avg_logprob": -0.13300334706025965, "compression_ratio": 1.781512605042017, "no_speech_prob": 0.0027148565277457237}, {"id": 1275, "seek": 503778, "start": 5046.099999999999, "end": 5051.82, "text": " system is one in which all the potential graphs, all the", "tokens": [50780, 1185, 307, 472, 294, 597, 439, 264, 3995, 24877, 11, 439, 264, 51066], "temperature": 0.0, "avg_logprob": -0.13300334706025965, "compression_ratio": 1.781512605042017, "no_speech_prob": 0.0027148565277457237}, {"id": 1276, "seek": 503778, "start": 5051.82, "end": 5054.78, "text": " potential transition graphs that you can get to a particular", "tokens": [51066, 3995, 6034, 24877, 300, 291, 393, 483, 281, 257, 1729, 51214], "temperature": 0.0, "avg_logprob": -0.13300334706025965, "compression_ratio": 1.781512605042017, "no_speech_prob": 0.0027148565277457237}, {"id": 1277, "seek": 503778, "start": 5054.78, "end": 5058.98, "text": " output are isomorphic. So even though you have you can have", "tokens": [51214, 5598, 366, 307, 32702, 299, 13, 407, 754, 1673, 291, 362, 291, 393, 362, 51424], "temperature": 0.0, "avg_logprob": -0.13300334706025965, "compression_ratio": 1.781512605042017, "no_speech_prob": 0.0027148565277457237}, {"id": 1278, "seek": 503778, "start": 5059.34, "end": 5062.98, "text": " like the perfect discriminative kind of function, there may be", "tokens": [51442, 411, 264, 2176, 20828, 1166, 733, 295, 2445, 11, 456, 815, 312, 51624], "temperature": 0.0, "avg_logprob": -0.13300334706025965, "compression_ratio": 1.781512605042017, "no_speech_prob": 0.0027148565277457237}, {"id": 1279, "seek": 503778, "start": 5062.98, "end": 5066.139999999999, "text": " multiple possible graphs that you could have gotten there, but", "tokens": [51624, 3866, 1944, 24877, 300, 291, 727, 362, 5768, 456, 11, 457, 51782], "temperature": 0.0, "avg_logprob": -0.13300334706025965, "compression_ratio": 1.781512605042017, "no_speech_prob": 0.0027148565277457237}, {"id": 1280, "seek": 506614, "start": 5066.14, "end": 5069.46, "text": " they're isomorphic. So I'm not quite sure, you know, how that", "tokens": [50364, 436, 434, 307, 32702, 299, 13, 407, 286, 478, 406, 1596, 988, 11, 291, 458, 11, 577, 300, 50530], "temperature": 0.0, "avg_logprob": -0.12338259725859671, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.00029593706130981445}, {"id": 1281, "seek": 506614, "start": 5069.46, "end": 5073.740000000001, "text": " would translate into this, this, this point. But I think you'd", "tokens": [50530, 576, 13799, 666, 341, 11, 341, 11, 341, 935, 13, 583, 286, 519, 291, 1116, 50744], "temperature": 0.0, "avg_logprob": -0.12338259725859671, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.00029593706130981445}, {"id": 1282, "seek": 506614, "start": 5073.740000000001, "end": 5077.1, "text": " be just as good for the purpose of discriminating.", "tokens": [50744, 312, 445, 382, 665, 337, 264, 4334, 295, 20828, 990, 13, 50912], "temperature": 0.0, "avg_logprob": -0.12338259725859671, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.00029593706130981445}, {"id": 1283, "seek": 506614, "start": 5078.740000000001, "end": 5081.18, "text": " I think it's related to the semantics discussion we're", "tokens": [50994, 286, 519, 309, 311, 4077, 281, 264, 4361, 45298, 5017, 321, 434, 51116], "temperature": 0.0, "avg_logprob": -0.12338259725859671, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.00029593706130981445}, {"id": 1284, "seek": 506614, "start": 5081.18, "end": 5085.14, "text": " having in NLP. So people like Walid Saber say that neural", "tokens": [51116, 1419, 294, 426, 45196, 13, 407, 561, 411, 9707, 327, 13915, 260, 584, 300, 18161, 51314], "temperature": 0.0, "avg_logprob": -0.12338259725859671, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.00029593706130981445}, {"id": 1285, "seek": 506614, "start": 5085.14, "end": 5088.18, "text": " networks don't have semantics. And in the same way, as I was", "tokens": [51314, 9590, 500, 380, 362, 4361, 45298, 13, 400, 294, 264, 912, 636, 11, 382, 286, 390, 51466], "temperature": 0.0, "avg_logprob": -0.12338259725859671, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.00029593706130981445}, {"id": 1286, "seek": 506614, "start": 5088.18, "end": 5091.34, "text": " just saying, blue pixels, I mean, in the real world, let's say", "tokens": [51466, 445, 1566, 11, 3344, 18668, 11, 286, 914, 11, 294, 264, 957, 1002, 11, 718, 311, 584, 51624], "temperature": 0.0, "avg_logprob": -0.12338259725859671, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.00029593706130981445}, {"id": 1287, "seek": 506614, "start": 5091.34, "end": 5095.820000000001, "text": " male testosterone levels is causally linked to incidents of", "tokens": [51624, 7133, 33417, 4358, 307, 3302, 379, 9408, 281, 21139, 295, 51848], "temperature": 0.0, "avg_logprob": -0.12338259725859671, "compression_ratio": 1.5892255892255893, "no_speech_prob": 0.00029593706130981445}, {"id": 1288, "seek": 509582, "start": 5095.86, "end": 5099.5, "text": " car crashes, which means you can now take the model in in", "tokens": [50366, 1032, 28642, 11, 597, 1355, 291, 393, 586, 747, 264, 2316, 294, 294, 50548], "temperature": 0.0, "avg_logprob": -0.14172898864746095, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.002976601244881749}, {"id": 1289, "seek": 509582, "start": 5099.5, "end": 5102.099999999999, "text": " Holland in a different country. And because it's a causal", "tokens": [50548, 27201, 294, 257, 819, 1941, 13, 400, 570, 309, 311, 257, 38755, 50678], "temperature": 0.0, "avg_logprob": -0.14172898864746095, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.002976601244881749}, {"id": 1290, "seek": 509582, "start": 5102.099999999999, "end": 5105.38, "text": " factor, it will extrapolate in the same way. But neural", "tokens": [50678, 5952, 11, 309, 486, 48224, 473, 294, 264, 912, 636, 13, 583, 18161, 50842], "temperature": 0.0, "avg_logprob": -0.14172898864746095, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.002976601244881749}, {"id": 1291, "seek": 509582, "start": 5105.38, "end": 5109.139999999999, "text": " networks models, because what a human does is we would come up", "tokens": [50842, 9590, 5245, 11, 570, 437, 257, 1952, 775, 307, 321, 576, 808, 493, 51030], "temperature": 0.0, "avg_logprob": -0.14172898864746095, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.002976601244881749}, {"id": 1292, "seek": 509582, "start": 5109.139999999999, "end": 5112.42, "text": " with the right representational abstraction, we would build a", "tokens": [51030, 365, 264, 558, 2906, 1478, 37765, 11, 321, 576, 1322, 257, 51194], "temperature": 0.0, "avg_logprob": -0.14172898864746095, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.002976601244881749}, {"id": 1293, "seek": 509582, "start": 5112.42, "end": 5115.66, "text": " model, which is very reductionist, a neural network models", "tokens": [51194, 2316, 11, 597, 307, 588, 11004, 468, 11, 257, 18161, 3209, 5245, 51356], "temperature": 0.0, "avg_logprob": -0.14172898864746095, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.002976601244881749}, {"id": 1294, "seek": 509582, "start": 5115.66, "end": 5118.98, "text": " everything to everything. And the semantics are all one thing.", "tokens": [51356, 1203, 281, 1203, 13, 400, 264, 4361, 45298, 366, 439, 472, 551, 13, 51522], "temperature": 0.0, "avg_logprob": -0.14172898864746095, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.002976601244881749}, {"id": 1295, "seek": 509582, "start": 5119.38, "end": 5124.219999999999, "text": " Well, okay, I don't, I'm not, I'm not too, too keen on", "tokens": [51542, 1042, 11, 1392, 11, 286, 500, 380, 11, 286, 478, 406, 11, 286, 478, 406, 886, 11, 886, 20297, 322, 51784], "temperature": 0.0, "avg_logprob": -0.14172898864746095, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.002976601244881749}, {"id": 1296, "seek": 512422, "start": 5124.58, "end": 5128.740000000001, "text": " discussing like semantics and whatnot with with NLP people.", "tokens": [50382, 10850, 411, 4361, 45298, 293, 25882, 365, 365, 426, 45196, 561, 13, 50590], "temperature": 0.0, "avg_logprob": -0.30629732211430866, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.026717467233538628}, {"id": 1297, "seek": 512422, "start": 5128.900000000001, "end": 5138.860000000001, "text": " But I don't know, you know, like, like, I don't know, it", "tokens": [50598, 583, 286, 500, 380, 458, 11, 291, 458, 11, 411, 11, 411, 11, 286, 500, 380, 458, 11, 309, 51096], "temperature": 0.0, "avg_logprob": -0.30629732211430866, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.026717467233538628}, {"id": 1298, "seek": 512422, "start": 5138.860000000001, "end": 5144.06, "text": " often it often veers away and veers into semantics. It's like", "tokens": [51096, 2049, 309, 2049, 1241, 433, 1314, 293, 1241, 433, 666, 4361, 45298, 13, 467, 311, 411, 51356], "temperature": 0.0, "avg_logprob": -0.30629732211430866, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.026717467233538628}, {"id": 1299, "seek": 512422, "start": 5144.06, "end": 5148.860000000001, "text": " it's a bit too, you know, I like what what I think Conor Conor", "tokens": [51356, 309, 311, 257, 857, 886, 11, 291, 458, 11, 286, 411, 437, 437, 286, 519, 2656, 284, 2656, 284, 51596], "temperature": 0.0, "avg_logprob": -0.30629732211430866, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.026717467233538628}, {"id": 1300, "seek": 512422, "start": 5148.860000000001, "end": 5153.06, "text": " Lay, he said, like, when we talked him along. Oh, he wouldn't", "tokens": [51596, 20084, 11, 415, 848, 11, 411, 11, 562, 321, 2825, 796, 2051, 13, 876, 11, 415, 2759, 380, 51806], "temperature": 0.0, "avg_logprob": -0.30629732211430866, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.026717467233538628}, {"id": 1301, "seek": 515306, "start": 5153.3, "end": 5156.26, "text": " like that. I talked him a long time ago. And I happen to agree", "tokens": [50376, 411, 300, 13, 286, 2825, 796, 257, 938, 565, 2057, 13, 400, 286, 1051, 281, 3986, 50524], "temperature": 0.0, "avg_logprob": -0.15439496721540177, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.008569919504225254}, {"id": 1302, "seek": 515306, "start": 5156.26, "end": 5159.22, "text": " with him there is that you sort of have to see everything from", "tokens": [50524, 365, 796, 456, 307, 300, 291, 1333, 295, 362, 281, 536, 1203, 490, 50672], "temperature": 0.0, "avg_logprob": -0.15439496721540177, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.008569919504225254}, {"id": 1303, "seek": 515306, "start": 5159.22, "end": 5163.700000000001, "text": " the perspective of these models. Like if I'm a GPT three, my", "tokens": [50672, 264, 4585, 295, 613, 5245, 13, 1743, 498, 286, 478, 257, 26039, 51, 1045, 11, 452, 50896], "temperature": 0.0, "avg_logprob": -0.15439496721540177, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.008569919504225254}, {"id": 1304, "seek": 515306, "start": 5163.700000000001, "end": 5167.580000000001, "text": " entire world is text input, right? And people can't somehow", "tokens": [50896, 2302, 1002, 307, 2487, 4846, 11, 558, 30, 400, 561, 393, 380, 6063, 51090], "temperature": 0.0, "avg_logprob": -0.15439496721540177, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.008569919504225254}, {"id": 1305, "seek": 515306, "start": 5167.580000000001, "end": 5172.26, "text": " judge GPT three by, well, you don't even have whatever a", "tokens": [51090, 6995, 26039, 51, 1045, 538, 11, 731, 11, 291, 500, 380, 754, 362, 2035, 257, 51324], "temperature": 0.0, "avg_logprob": -0.15439496721540177, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.008569919504225254}, {"id": 1306, "seek": 515306, "start": 5172.26, "end": 5174.860000000001, "text": " connection to the real world, you don't even know that you don't", "tokens": [51324, 4984, 281, 264, 957, 1002, 11, 291, 500, 380, 754, 458, 300, 291, 500, 380, 51454], "temperature": 0.0, "avg_logprob": -0.15439496721540177, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.008569919504225254}, {"id": 1307, "seek": 515306, "start": 5174.860000000001, "end": 5179.34, "text": " go see a doctor if your plant is sick, right? Like, how can you", "tokens": [51454, 352, 536, 257, 4631, 498, 428, 3709, 307, 4998, 11, 558, 30, 1743, 11, 577, 393, 291, 51678], "temperature": 0.0, "avg_logprob": -0.15439496721540177, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.008569919504225254}, {"id": 1308, "seek": 515306, "start": 5179.34, "end": 5181.9400000000005, "text": " not know that? Like, okay, they don't live in the real world,", "tokens": [51678, 406, 458, 300, 30, 1743, 11, 1392, 11, 436, 500, 380, 1621, 294, 264, 957, 1002, 11, 51808], "temperature": 0.0, "avg_logprob": -0.15439496721540177, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.008569919504225254}, {"id": 1309, "seek": 518194, "start": 5181.94, "end": 5185.82, "text": " they live in the text world of the internet. And in that world,", "tokens": [50364, 436, 1621, 294, 264, 2487, 1002, 295, 264, 4705, 13, 400, 294, 300, 1002, 11, 50558], "temperature": 0.0, "avg_logprob": -0.1326852155768353, "compression_ratio": 1.7541899441340782, "no_speech_prob": 0.0009391683270223439}, {"id": 1310, "seek": 518194, "start": 5186.0599999999995, "end": 5191.139999999999, "text": " I'm not sure if there is not a level of abstraction happening", "tokens": [50570, 286, 478, 406, 988, 498, 456, 307, 406, 257, 1496, 295, 37765, 2737, 50824], "temperature": 0.0, "avg_logprob": -0.1326852155768353, "compression_ratio": 1.7541899441340782, "no_speech_prob": 0.0009391683270223439}, {"id": 1311, "seek": 518194, "start": 5191.259999999999, "end": 5199.299999999999, "text": " in these models. Like, it's, it's, it's, um, yeah, I'm, I don't", "tokens": [50830, 294, 613, 5245, 13, 1743, 11, 309, 311, 11, 309, 311, 11, 309, 311, 11, 1105, 11, 1338, 11, 286, 478, 11, 286, 500, 380, 51232], "temperature": 0.0, "avg_logprob": -0.1326852155768353, "compression_ratio": 1.7541899441340782, "no_speech_prob": 0.0009391683270223439}, {"id": 1312, "seek": 518194, "start": 5199.299999999999, "end": 5203.86, "text": " want to, I don't want to claim that these things do not form", "tokens": [51232, 528, 281, 11, 286, 500, 380, 528, 281, 3932, 300, 613, 721, 360, 406, 1254, 51460], "temperature": 0.0, "avg_logprob": -0.1326852155768353, "compression_ratio": 1.7541899441340782, "no_speech_prob": 0.0009391683270223439}, {"id": 1313, "seek": 518194, "start": 5204.139999999999, "end": 5208.179999999999, "text": " abstract things, it might not be the same abstract classes that", "tokens": [51474, 12649, 721, 11, 309, 1062, 406, 312, 264, 912, 12649, 5359, 300, 51676], "temperature": 0.0, "avg_logprob": -0.1326852155768353, "compression_ratio": 1.7541899441340782, "no_speech_prob": 0.0009391683270223439}, {"id": 1314, "seek": 520818, "start": 5208.18, "end": 5212.780000000001, "text": " we form, but they definitely form some level of abstraction.", "tokens": [50364, 321, 1254, 11, 457, 436, 2138, 1254, 512, 1496, 295, 37765, 13, 50594], "temperature": 0.0, "avg_logprob": -0.12251319541587485, "compression_ratio": 1.6, "no_speech_prob": 0.0046791089698672295}, {"id": 1315, "seek": 520818, "start": 5213.1, "end": 5216.1, "text": " And of course, they can't transfer it because we only give", "tokens": [50610, 400, 295, 1164, 11, 436, 393, 380, 5003, 309, 570, 321, 787, 976, 50760], "temperature": 0.0, "avg_logprob": -0.12251319541587485, "compression_ratio": 1.6, "no_speech_prob": 0.0046791089698672295}, {"id": 1316, "seek": 520818, "start": 5216.1, "end": 5219.66, "text": " them the one modality, right? But they may be able to transfer", "tokens": [50760, 552, 264, 472, 1072, 1860, 11, 558, 30, 583, 436, 815, 312, 1075, 281, 5003, 50938], "temperature": 0.0, "avg_logprob": -0.12251319541587485, "compression_ratio": 1.6, "no_speech_prob": 0.0046791089698672295}, {"id": 1317, "seek": 520818, "start": 5219.66, "end": 5225.02, "text": " it between, you know, different areas of text, which they", "tokens": [50938, 309, 1296, 11, 291, 458, 11, 819, 3179, 295, 2487, 11, 597, 436, 51206], "temperature": 0.0, "avg_logprob": -0.12251319541587485, "compression_ratio": 1.6, "no_speech_prob": 0.0046791089698672295}, {"id": 1318, "seek": 520818, "start": 5225.14, "end": 5230.5, "text": " sometimes do, right? And yeah, so that's, I just wouldn't be so", "tokens": [51212, 2171, 360, 11, 558, 30, 400, 1338, 11, 370, 300, 311, 11, 286, 445, 2759, 380, 312, 370, 51480], "temperature": 0.0, "avg_logprob": -0.12251319541587485, "compression_ratio": 1.6, "no_speech_prob": 0.0046791089698672295}, {"id": 1319, "seek": 520818, "start": 5230.54, "end": 5233.26, "text": " conclusive with respect to these things.", "tokens": [51482, 1588, 7233, 365, 3104, 281, 613, 721, 13, 51618], "temperature": 0.0, "avg_logprob": -0.12251319541587485, "compression_ratio": 1.6, "no_speech_prob": 0.0046791089698672295}, {"id": 1320, "seek": 520818, "start": 5233.3, "end": 5237.02, "text": " That that's true. I think we've gone full circle now. So after", "tokens": [51620, 663, 300, 311, 2074, 13, 286, 519, 321, 600, 2780, 1577, 6329, 586, 13, 407, 934, 51806], "temperature": 0.0, "avg_logprob": -0.12251319541587485, "compression_ratio": 1.6, "no_speech_prob": 0.0046791089698672295}, {"id": 1321, "seek": 523702, "start": 5237.06, "end": 5241.740000000001, "text": " speaking with Randall Ballastriro about the splines, that almost", "tokens": [50366, 4124, 365, 23614, 336, 10744, 525, 470, 340, 466, 264, 4732, 1652, 11, 300, 1920, 50600], "temperature": 0.0, "avg_logprob": -0.1360201964507232, "compression_ratio": 1.6342412451361867, "no_speech_prob": 0.013205348514020443}, {"id": 1322, "seek": 523702, "start": 5242.18, "end": 5245.1, "text": " results in such a cynical reading of MLPs that they're just", "tokens": [50622, 3542, 294, 1270, 257, 46345, 3760, 295, 21601, 23043, 300, 436, 434, 445, 50768], "temperature": 0.0, "avg_logprob": -0.1360201964507232, "compression_ratio": 1.6342412451361867, "no_speech_prob": 0.013205348514020443}, {"id": 1323, "seek": 523702, "start": 5245.1, "end": 5247.9800000000005, "text": " hash tables, but we're not using MLPs, we're using", "tokens": [50768, 22019, 8020, 11, 457, 321, 434, 406, 1228, 21601, 23043, 11, 321, 434, 1228, 50912], "temperature": 0.0, "avg_logprob": -0.1360201964507232, "compression_ratio": 1.6342412451361867, "no_speech_prob": 0.013205348514020443}, {"id": 1324, "seek": 523702, "start": 5247.9800000000005, "end": 5251.14, "text": " transformers and we're using CNNs. And actually, if you think", "tokens": [50912, 4088, 433, 293, 321, 434, 1228, 24859, 82, 13, 400, 767, 11, 498, 291, 519, 51070], "temperature": 0.0, "avg_logprob": -0.1360201964507232, "compression_ratio": 1.6342412451361867, "no_speech_prob": 0.013205348514020443}, {"id": 1325, "seek": 523702, "start": 5251.14, "end": 5254.740000000001, "text": " of abstraction, just as being extrapolation, I think they are", "tokens": [51070, 295, 37765, 11, 445, 382, 885, 48224, 399, 11, 286, 519, 436, 366, 51250], "temperature": 0.0, "avg_logprob": -0.1360201964507232, "compression_ratio": 1.6342412451361867, "no_speech_prob": 0.013205348514020443}, {"id": 1326, "seek": 523702, "start": 5254.740000000001, "end": 5257.9800000000005, "text": " basically synonymous, it's about being able to extrapolate", "tokens": [51250, 1936, 5451, 18092, 11, 309, 311, 466, 885, 1075, 281, 48224, 473, 51412], "temperature": 0.0, "avg_logprob": -0.1360201964507232, "compression_ratio": 1.6342412451361867, "no_speech_prob": 0.013205348514020443}, {"id": 1327, "seek": 523702, "start": 5257.9800000000005, "end": 5261.820000000001, "text": " outside of your training set. Then those inductive priors are", "tokens": [51412, 2380, 295, 428, 3097, 992, 13, 1396, 729, 31612, 488, 1790, 830, 366, 51604], "temperature": 0.0, "avg_logprob": -0.1360201964507232, "compression_ratio": 1.6342412451361867, "no_speech_prob": 0.013205348514020443}, {"id": 1328, "seek": 526182, "start": 5261.82, "end": 5266.46, "text": " indeed producing abstractions. But the problem is humans", "tokens": [50364, 6451, 10501, 12649, 626, 13, 583, 264, 1154, 307, 6255, 50596], "temperature": 0.0, "avg_logprob": -0.11246898485266645, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.00648684985935688}, {"id": 1329, "seek": 526182, "start": 5266.46, "end": 5269.78, "text": " design those inductive priors. What we want is to learn", "tokens": [50596, 1715, 729, 31612, 488, 1790, 830, 13, 708, 321, 528, 307, 281, 1466, 50762], "temperature": 0.0, "avg_logprob": -0.11246898485266645, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.00648684985935688}, {"id": 1330, "seek": 526182, "start": 5269.78, "end": 5272.0199999999995, "text": " abstractions. And that's the thing that I don't think is", "tokens": [50762, 12649, 626, 13, 400, 300, 311, 264, 551, 300, 286, 500, 380, 519, 307, 50874], "temperature": 0.0, "avg_logprob": -0.11246898485266645, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.00648684985935688}, {"id": 1331, "seek": 526182, "start": 5272.0199999999995, "end": 5277.82, "text": " happening. I'm kind of I'm kind of on the same page as Yannick", "tokens": [50874, 2737, 13, 286, 478, 733, 295, 286, 478, 733, 295, 322, 264, 912, 3028, 382, 398, 969, 618, 51164], "temperature": 0.0, "avg_logprob": -0.11246898485266645, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.00648684985935688}, {"id": 1332, "seek": 526182, "start": 5277.82, "end": 5282.7, "text": " and Connor on the one hand, which is, hey, if an abstraction is", "tokens": [51164, 293, 33133, 322, 264, 472, 1011, 11, 597, 307, 11, 4177, 11, 498, 364, 37765, 307, 51408], "temperature": 0.0, "avg_logprob": -0.11246898485266645, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.00648684985935688}, {"id": 1333, "seek": 526182, "start": 5282.7, "end": 5288.219999999999, "text": " just a compression, you know, encoding of the input space, then", "tokens": [51408, 445, 257, 19355, 11, 291, 458, 11, 43430, 295, 264, 4846, 1901, 11, 550, 51684], "temperature": 0.0, "avg_logprob": -0.11246898485266645, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.00648684985935688}, {"id": 1334, "seek": 526182, "start": 5288.219999999999, "end": 5290.5, "text": " of course, they're learning abstractions, right? I mean,", "tokens": [51684, 295, 1164, 11, 436, 434, 2539, 12649, 626, 11, 558, 30, 286, 914, 11, 51798], "temperature": 0.0, "avg_logprob": -0.11246898485266645, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.00648684985935688}, {"id": 1335, "seek": 529050, "start": 5290.5, "end": 5293.74, "text": " they are throwing away, you know, information and retaining", "tokens": [50364, 436, 366, 10238, 1314, 11, 291, 458, 11, 1589, 293, 34936, 50526], "temperature": 0.0, "avg_logprob": -0.1476978052442319, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0002694729482755065}, {"id": 1336, "seek": 529050, "start": 5293.74, "end": 5297.22, "text": " some, some abstract thing. I think, but I think that just kind", "tokens": [50526, 512, 11, 512, 12649, 551, 13, 286, 519, 11, 457, 286, 519, 300, 445, 733, 50700], "temperature": 0.0, "avg_logprob": -0.1476978052442319, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0002694729482755065}, {"id": 1337, "seek": 529050, "start": 5297.22, "end": 5303.58, "text": " of devolves into somewhat like, you know, bastardization, if you", "tokens": [50700, 295, 1905, 401, 977, 666, 8344, 411, 11, 291, 458, 11, 23569, 2144, 11, 498, 291, 51018], "temperature": 0.0, "avg_logprob": -0.1476978052442319, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0002694729482755065}, {"id": 1338, "seek": 529050, "start": 5303.58, "end": 5307.14, "text": " will, of what people mean when they say abstractions, because", "tokens": [51018, 486, 11, 295, 437, 561, 914, 562, 436, 584, 12649, 626, 11, 570, 51196], "temperature": 0.0, "avg_logprob": -0.1476978052442319, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0002694729482755065}, {"id": 1339, "seek": 529050, "start": 5307.5, "end": 5311.14, "text": " the types of abstractions that traditionally we think about as", "tokens": [51214, 264, 3467, 295, 12649, 626, 300, 19067, 321, 519, 466, 382, 51396], "temperature": 0.0, "avg_logprob": -0.1476978052442319, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0002694729482755065}, {"id": 1340, "seek": 529050, "start": 5311.14, "end": 5314.74, "text": " abstractions are simplifications. You know, they're, they're", "tokens": [51396, 12649, 626, 366, 6883, 7833, 13, 509, 458, 11, 436, 434, 11, 436, 434, 51576], "temperature": 0.0, "avg_logprob": -0.1476978052442319, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0002694729482755065}, {"id": 1341, "seek": 529050, "start": 5314.74, "end": 5319.78, "text": " like, simplifications of more general longer range kind of", "tokens": [51576, 411, 11, 6883, 7833, 295, 544, 2674, 2854, 3613, 733, 295, 51828], "temperature": 0.0, "avg_logprob": -0.1476978052442319, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0002694729482755065}, {"id": 1342, "seek": 531978, "start": 5319.78, "end": 5322.98, "text": " structures. Whereas we know, and I think we all know this for", "tokens": [50364, 9227, 13, 13813, 321, 458, 11, 293, 286, 519, 321, 439, 458, 341, 337, 50524], "temperature": 0.0, "avg_logprob": -0.11773795207948175, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.002889313967898488}, {"id": 1343, "seek": 531978, "start": 5322.98, "end": 5327.74, "text": " sure, that a lot of the quote unquote abstractions that did a", "tokens": [50524, 988, 11, 300, 257, 688, 295, 264, 6513, 37557, 12649, 626, 300, 630, 257, 50762], "temperature": 0.0, "avg_logprob": -0.11773795207948175, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.002889313967898488}, {"id": 1344, "seek": 531978, "start": 5327.78, "end": 5332.34, "text": " neural network learns are these kind of like shortcuts, right?", "tokens": [50764, 18161, 3209, 27152, 366, 613, 733, 295, 411, 34620, 11, 558, 30, 50992], "temperature": 0.0, "avg_logprob": -0.11773795207948175, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.002889313967898488}, {"id": 1345, "seek": 531978, "start": 5332.34, "end": 5336.54, "text": " They're like these low level borderline spurious kinds of", "tokens": [50992, 814, 434, 411, 613, 2295, 1496, 7838, 1889, 637, 24274, 3685, 295, 51202], "temperature": 0.0, "avg_logprob": -0.11773795207948175, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.002889313967898488}, {"id": 1346, "seek": 531978, "start": 5336.58, "end": 5339.42, "text": " abstractions. And that's why they break so easy. That's why", "tokens": [51204, 12649, 626, 13, 400, 300, 311, 983, 436, 1821, 370, 1858, 13, 663, 311, 983, 51346], "temperature": 0.0, "avg_logprob": -0.11773795207948175, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.002889313967898488}, {"id": 1347, "seek": 531978, "start": 5339.42, "end": 5342.34, "text": " they're so brittle. And I mean, there is this vagueness here,", "tokens": [51346, 436, 434, 370, 49325, 13, 400, 286, 914, 11, 456, 307, 341, 13501, 7801, 442, 510, 11, 51492], "temperature": 0.0, "avg_logprob": -0.11773795207948175, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.002889313967898488}, {"id": 1348, "seek": 531978, "start": 5342.34, "end": 5345.0199999999995, "text": " right? Like when is an abstraction, a good abstraction,", "tokens": [51492, 558, 30, 1743, 562, 307, 364, 37765, 11, 257, 665, 37765, 11, 51626], "temperature": 0.0, "avg_logprob": -0.11773795207948175, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.002889313967898488}, {"id": 1349, "seek": 531978, "start": 5345.0199999999995, "end": 5347.66, "text": " I don't know. But I think it all kind of in a way misses the", "tokens": [51626, 286, 500, 380, 458, 13, 583, 286, 519, 309, 439, 733, 295, 294, 257, 636, 29394, 264, 51758], "temperature": 0.0, "avg_logprob": -0.11773795207948175, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.002889313967898488}, {"id": 1350, "seek": 534766, "start": 5347.66, "end": 5352.66, "text": " point. Like, what we're talking about here is that, and this is a", "tokens": [50364, 935, 13, 1743, 11, 437, 321, 434, 1417, 466, 510, 307, 300, 11, 293, 341, 307, 257, 50614], "temperature": 0.0, "avg_logprob": -0.164968626839774, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.0026314256247133017}, {"id": 1351, "seek": 534766, "start": 5352.66, "end": 5356.58, "text": " lot of what Benjio said, right, which is that the goal here is", "tokens": [50614, 688, 295, 437, 3964, 73, 1004, 848, 11, 558, 11, 597, 307, 300, 264, 3387, 510, 307, 50810], "temperature": 0.0, "avg_logprob": -0.164968626839774, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.0026314256247133017}, {"id": 1352, "seek": 534766, "start": 5356.58, "end": 5361.82, "text": " to figure out how to get machine learning to learn", "tokens": [50810, 281, 2573, 484, 577, 281, 483, 3479, 2539, 281, 1466, 51072], "temperature": 0.0, "avg_logprob": -0.164968626839774, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.0026314256247133017}, {"id": 1353, "seek": 534766, "start": 5362.38, "end": 5368.46, "text": " structures that by virtue of their simplification, their simple", "tokens": [51100, 9227, 300, 538, 20816, 295, 641, 6883, 3774, 11, 641, 2199, 51404], "temperature": 0.0, "avg_logprob": -0.164968626839774, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.0026314256247133017}, {"id": 1354, "seek": 534766, "start": 5368.46, "end": 5373.22, "text": " abstractions are more generalizable out of distribution.", "tokens": [51404, 12649, 626, 366, 544, 2674, 22395, 484, 295, 7316, 13, 51642], "temperature": 0.0, "avg_logprob": -0.164968626839774, "compression_ratio": 1.5873015873015872, "no_speech_prob": 0.0026314256247133017}, {"id": 1355, "seek": 537322, "start": 5373.9800000000005, "end": 5377.54, "text": " Right, like that's, that's really kind of the goal here. And I", "tokens": [50402, 1779, 11, 411, 300, 311, 11, 300, 311, 534, 733, 295, 264, 3387, 510, 13, 400, 286, 50580], "temperature": 0.0, "avg_logprob": -0.1772653479325144, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.0031720995903015137}, {"id": 1356, "seek": 537322, "start": 5377.54, "end": 5381.42, "text": " mean, so the rest of it is just semantics, pun intended. I mean,", "tokens": [50580, 914, 11, 370, 264, 1472, 295, 309, 307, 445, 4361, 45298, 11, 4468, 10226, 13, 286, 914, 11, 50774], "temperature": 0.0, "avg_logprob": -0.1772653479325144, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.0031720995903015137}, {"id": 1357, "seek": 537322, "start": 5381.42, "end": 5381.740000000001, "text": " the", "tokens": [50774, 264, 50790], "temperature": 0.0, "avg_logprob": -0.1772653479325144, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.0031720995903015137}, {"id": 1358, "seek": 537322, "start": 5384.02, "end": 5389.900000000001, "text": " if you look across the world, a lot of, let's say, cultures and", "tokens": [50904, 498, 291, 574, 2108, 264, 1002, 11, 257, 688, 295, 11, 718, 311, 584, 11, 12951, 293, 51198], "temperature": 0.0, "avg_logprob": -0.1772653479325144, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.0031720995903015137}, {"id": 1359, "seek": 537322, "start": 5389.900000000001, "end": 5395.9800000000005, "text": " humans and so on must have the same abstractions, right? So it", "tokens": [51198, 6255, 293, 370, 322, 1633, 362, 264, 912, 12649, 626, 11, 558, 30, 407, 309, 51502], "temperature": 0.0, "avg_logprob": -0.1772653479325144, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.0031720995903015137}, {"id": 1360, "seek": 537322, "start": 5396.1, "end": 5400.06, "text": " must mean a little bit that it's not just something you learn", "tokens": [51508, 1633, 914, 257, 707, 857, 300, 309, 311, 406, 445, 746, 291, 1466, 51706], "temperature": 0.0, "avg_logprob": -0.1772653479325144, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.0031720995903015137}, {"id": 1361, "seek": 540006, "start": 5400.06, "end": 5404.700000000001, "text": " during your lifetime, right? So, right? Oh, absolutely.", "tokens": [50364, 1830, 428, 11364, 11, 558, 30, 407, 11, 558, 30, 876, 11, 3122, 13, 50596], "temperature": 0.0, "avg_logprob": -0.26626121788694146, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.027132540941238403}, {"id": 1362, "seek": 540006, "start": 5404.700000000001, "end": 5408.18, "text": " Not correct. It's it's learned by the it's learned by evolution,", "tokens": [50596, 1726, 3006, 13, 467, 311, 309, 311, 3264, 538, 264, 309, 311, 3264, 538, 9303, 11, 50770], "temperature": 0.0, "avg_logprob": -0.26626121788694146, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.027132540941238403}, {"id": 1363, "seek": 540006, "start": 5408.22, "end": 5410.54, "text": " by the species, by, by life itself.", "tokens": [50772, 538, 264, 6172, 11, 538, 11, 538, 993, 2564, 13, 50888], "temperature": 0.0, "avg_logprob": -0.26626121788694146, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.027132540941238403}, {"id": 1364, "seek": 540006, "start": 5410.580000000001, "end": 5416.14, "text": " Exactly, right. But but is like the, the analogy to us building", "tokens": [50890, 7587, 11, 558, 13, 583, 457, 307, 411, 264, 11, 264, 21663, 281, 505, 2390, 51168], "temperature": 0.0, "avg_logprob": -0.26626121788694146, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.027132540941238403}, {"id": 1365, "seek": 540006, "start": 5416.14, "end": 5420.38, "text": " in the correct ones as a shortcut for just evolution doing it", "tokens": [51168, 294, 264, 3006, 2306, 382, 257, 24822, 337, 445, 9303, 884, 309, 51380], "temperature": 0.0, "avg_logprob": -0.26626121788694146, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.027132540941238403}, {"id": 1366, "seek": 540006, "start": 5420.38, "end": 5425.46, "text": " using essentially random search, right? That is, it might, right,", "tokens": [51380, 1228, 4476, 4974, 3164, 11, 558, 30, 663, 307, 11, 309, 1062, 11, 558, 11, 51634], "temperature": 0.0, "avg_logprob": -0.26626121788694146, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.027132540941238403}, {"id": 1367, "seek": 540006, "start": 5425.46, "end": 5428.9800000000005, "text": " it's, it's a different, it's a different quality of we want", "tokens": [51634, 309, 311, 11, 309, 311, 257, 819, 11, 309, 311, 257, 819, 3125, 295, 321, 528, 51810], "temperature": 0.0, "avg_logprob": -0.26626121788694146, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.027132540941238403}, {"id": 1368, "seek": 542898, "start": 5429.0199999999995, "end": 5432.94, "text": " machines to learn something. Because usually we think of when", "tokens": [50366, 8379, 281, 1466, 746, 13, 1436, 2673, 321, 519, 295, 562, 50562], "temperature": 0.0, "avg_logprob": -0.12182622704624145, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.004674626048654318}, {"id": 1369, "seek": 542898, "start": 5432.94, "end": 5435.5, "text": " we say we want machines to learn something is we want him to", "tokens": [50562, 321, 584, 321, 528, 8379, 281, 1466, 746, 307, 321, 528, 796, 281, 50690], "temperature": 0.0, "avg_logprob": -0.12182622704624145, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.004674626048654318}, {"id": 1370, "seek": 542898, "start": 5435.5, "end": 5441.0599999999995, "text": " ingest data akin to maybe what a human does during its lifetime.", "tokens": [50690, 3957, 377, 1412, 47540, 281, 1310, 437, 257, 1952, 775, 1830, 1080, 11364, 13, 50968], "temperature": 0.0, "avg_logprob": -0.12182622704624145, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.004674626048654318}, {"id": 1371, "seek": 542898, "start": 5441.54, "end": 5445.54, "text": " But the when you know, these sort of abstractions and the", "tokens": [50992, 583, 264, 562, 291, 458, 11, 613, 1333, 295, 12649, 626, 293, 264, 51192], "temperature": 0.0, "avg_logprob": -0.12182622704624145, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.004674626048654318}, {"id": 1372, "seek": 542898, "start": 5445.54, "end": 5448.78, "text": " ability to form abstractions, they seem to be happening on a", "tokens": [51192, 3485, 281, 1254, 12649, 626, 11, 436, 1643, 281, 312, 2737, 322, 257, 51354], "temperature": 0.0, "avg_logprob": -0.12182622704624145, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.004674626048654318}, {"id": 1373, "seek": 542898, "start": 5448.78, "end": 5451.0199999999995, "text": " more fundamental shared level.", "tokens": [51354, 544, 8088, 5507, 1496, 13, 51466], "temperature": 0.0, "avg_logprob": -0.12182622704624145, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.004674626048654318}, {"id": 1374, "seek": 542898, "start": 5451.299999999999, "end": 5455.139999999999, "text": " Yes, you just put the pin in the center of the bullseye. I think", "tokens": [51480, 1079, 11, 291, 445, 829, 264, 5447, 294, 264, 3056, 295, 264, 4693, 405, 1200, 13, 286, 519, 51672], "temperature": 0.0, "avg_logprob": -0.12182622704624145, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.004674626048654318}, {"id": 1375, "seek": 542898, "start": 5455.139999999999, "end": 5458.0599999999995, "text": " that's exactly right. You know, there's a lot to be said for", "tokens": [51672, 300, 311, 2293, 558, 13, 509, 458, 11, 456, 311, 257, 688, 281, 312, 848, 337, 51818], "temperature": 0.0, "avg_logprob": -0.12182622704624145, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.004674626048654318}, {"id": 1376, "seek": 545806, "start": 5458.1, "end": 5461.900000000001, "text": " me. It's an epiphenomenon. And a lot of intelligence is", "tokens": [50366, 385, 13, 467, 311, 364, 2388, 647, 2932, 4726, 266, 13, 400, 257, 688, 295, 7599, 307, 50556], "temperature": 0.0, "avg_logprob": -0.17749568148776218, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.0010982737876474857}, {"id": 1377, "seek": 545806, "start": 5461.900000000001, "end": 5466.06, "text": " embodied. And I agree that there's an awful lot of stuff going", "tokens": [50556, 42046, 13, 400, 286, 3986, 300, 456, 311, 364, 11232, 688, 295, 1507, 516, 50764], "temperature": 0.0, "avg_logprob": -0.17749568148776218, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.0010982737876474857}, {"id": 1378, "seek": 545806, "start": 5466.06, "end": 5470.54, "text": " on and unbeknown to us with this clearly something that most", "tokens": [50764, 322, 293, 517, 650, 6861, 281, 505, 365, 341, 4448, 746, 300, 881, 50988], "temperature": 0.0, "avg_logprob": -0.17749568148776218, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.0010982737876474857}, {"id": 1379, "seek": 545806, "start": 5470.54, "end": 5474.42, "text": " people don't have a grasp on. Maybe this is why at the", "tokens": [50988, 561, 500, 380, 362, 257, 21743, 322, 13, 2704, 341, 307, 983, 412, 264, 51182], "temperature": 0.0, "avg_logprob": -0.17749568148776218, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.0010982737876474857}, {"id": 1380, "seek": 545806, "start": 5474.42, "end": 5477.820000000001, "text": " population level, maybe this is why I'm frequently miscommunicating", "tokens": [51182, 4415, 1496, 11, 1310, 341, 307, 983, 286, 478, 10374, 3346, 25451, 30541, 51352], "temperature": 0.0, "avg_logprob": -0.17749568148776218, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.0010982737876474857}, {"id": 1381, "seek": 545806, "start": 5477.820000000001, "end": 5481.06, "text": " with people because I never assumed that learning was about,", "tokens": [51352, 365, 561, 570, 286, 1128, 15895, 300, 2539, 390, 466, 11, 51514], "temperature": 0.0, "avg_logprob": -0.17749568148776218, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.0010982737876474857}, {"id": 1382, "seek": 545806, "start": 5481.54, "end": 5484.3, "text": " you know, what a human being learns and a human being's", "tokens": [51538, 291, 458, 11, 437, 257, 1952, 885, 27152, 293, 257, 1952, 885, 311, 51676], "temperature": 0.0, "avg_logprob": -0.17749568148776218, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.0010982737876474857}, {"id": 1383, "seek": 548430, "start": 5484.34, "end": 5488.14, "text": " lifetime. Like it's, to me, it's always been the evolution,", "tokens": [50366, 11364, 13, 1743, 309, 311, 11, 281, 385, 11, 309, 311, 1009, 668, 264, 9303, 11, 50556], "temperature": 0.0, "avg_logprob": -0.14021520943477236, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.017979223281145096}, {"id": 1384, "seek": 548430, "start": 5488.74, "end": 5491.74, "text": " you know, paradigm, it's like what's encoded in your neurons,", "tokens": [50586, 291, 458, 11, 24709, 11, 309, 311, 411, 437, 311, 2058, 12340, 294, 428, 22027, 11, 50736], "temperature": 0.0, "avg_logprob": -0.14021520943477236, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.017979223281145096}, {"id": 1385, "seek": 548430, "start": 5491.74, "end": 5495.5, "text": " what's encoded in your DNA, you know, what was learned by", "tokens": [50736, 437, 311, 2058, 12340, 294, 428, 8272, 11, 291, 458, 11, 437, 390, 3264, 538, 50924], "temperature": 0.0, "avg_logprob": -0.14021520943477236, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.017979223281145096}, {"id": 1386, "seek": 548430, "start": 5495.5, "end": 5498.9400000000005, "text": " bacteria a long time ago, and how did that translate into what", "tokens": [50924, 11763, 257, 938, 565, 2057, 11, 293, 577, 630, 300, 13799, 666, 437, 51096], "temperature": 0.0, "avg_logprob": -0.14021520943477236, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.017979223281145096}, {"id": 1387, "seek": 548430, "start": 5498.9400000000005, "end": 5502.46, "text": " human beings are doing. So I don't know why, like, why so", "tokens": [51096, 1952, 8958, 366, 884, 13, 407, 286, 500, 380, 458, 983, 11, 411, 11, 983, 370, 51272], "temperature": 0.0, "avg_logprob": -0.14021520943477236, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.017979223281145096}, {"id": 1388, "seek": 548430, "start": 5502.46, "end": 5506.14, "text": " many people are focused on what a human being learns in their", "tokens": [51272, 867, 561, 366, 5178, 322, 437, 257, 1952, 885, 27152, 294, 641, 51456], "temperature": 0.0, "avg_logprob": -0.14021520943477236, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.017979223281145096}, {"id": 1389, "seek": 548430, "start": 5506.14, "end": 5509.38, "text": " lifetime. I mean, it's more, you know, why is that the goal?", "tokens": [51456, 11364, 13, 286, 914, 11, 309, 311, 544, 11, 291, 458, 11, 983, 307, 300, 264, 3387, 30, 51618], "temperature": 0.0, "avg_logprob": -0.14021520943477236, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.017979223281145096}, {"id": 1390, "seek": 548430, "start": 5509.42, "end": 5510.1, "text": " I'm not sure.", "tokens": [51620, 286, 478, 406, 988, 13, 51654], "temperature": 0.0, "avg_logprob": -0.14021520943477236, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.017979223281145096}, {"id": 1391, "seek": 548430, "start": 5510.3, "end": 5513.34, "text": " I know, but we run the risk of being very reductionist because", "tokens": [51664, 286, 458, 11, 457, 321, 1190, 264, 3148, 295, 885, 588, 11004, 468, 570, 51816], "temperature": 0.0, "avg_logprob": -0.14021520943477236, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.017979223281145096}, {"id": 1392, "seek": 551334, "start": 5513.38, "end": 5517.06, "text": " Connolly, he said that it's an open question where the humans", "tokens": [50366, 2656, 1771, 13020, 11, 415, 848, 300, 309, 311, 364, 1269, 1168, 689, 264, 6255, 50550], "temperature": 0.0, "avg_logprob": -0.16293447667902167, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0003053148393519223}, {"id": 1393, "seek": 551334, "start": 5517.06, "end": 5521.1, "text": " are even intelligent. And if you go down that line, very", "tokens": [50550, 366, 754, 13232, 13, 400, 498, 291, 352, 760, 300, 1622, 11, 588, 50752], "temperature": 0.0, "avg_logprob": -0.16293447667902167, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0003053148393519223}, {"id": 1394, "seek": 551334, "start": 5521.1, "end": 5524.06, "text": " quickly, you start saying, oh, human beings are just hash", "tokens": [50752, 2661, 11, 291, 722, 1566, 11, 1954, 11, 1952, 8958, 366, 445, 22019, 50900], "temperature": 0.0, "avg_logprob": -0.16293447667902167, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0003053148393519223}, {"id": 1395, "seek": 551334, "start": 5524.06, "end": 5527.66, "text": " tables like GBT three, clearly humans are intelligent in some", "tokens": [50900, 8020, 411, 26809, 51, 1045, 11, 4448, 6255, 366, 13232, 294, 512, 51080], "temperature": 0.0, "avg_logprob": -0.16293447667902167, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0003053148393519223}, {"id": 1396, "seek": 551334, "start": 5527.66, "end": 5527.9400000000005, "text": " way.", "tokens": [51080, 636, 13, 51094], "temperature": 0.0, "avg_logprob": -0.16293447667902167, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0003053148393519223}, {"id": 1397, "seek": 551334, "start": 5528.02, "end": 5530.42, "text": " Well, you can just take it as a, you know, matter of", "tokens": [51098, 1042, 11, 291, 393, 445, 747, 309, 382, 257, 11, 291, 458, 11, 1871, 295, 51218], "temperature": 0.0, "avg_logprob": -0.16293447667902167, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0003053148393519223}, {"id": 1398, "seek": 551334, "start": 5530.42, "end": 5533.78, "text": " definition, but it's not a binary thing. Like again, why are", "tokens": [51218, 7123, 11, 457, 309, 311, 406, 257, 17434, 551, 13, 1743, 797, 11, 983, 366, 51386], "temperature": 0.0, "avg_logprob": -0.16293447667902167, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0003053148393519223}, {"id": 1399, "seek": 551334, "start": 5533.78, "end": 5536.62, "text": " we always into this black and white concept, something is or", "tokens": [51386, 321, 1009, 666, 341, 2211, 293, 2418, 3410, 11, 746, 307, 420, 51528], "temperature": 0.0, "avg_logprob": -0.16293447667902167, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0003053148393519223}, {"id": 1400, "seek": 551334, "start": 5536.62, "end": 5539.9800000000005, "text": " is not intelligent? Like that's not how I view things. I think", "tokens": [51528, 307, 406, 13232, 30, 1743, 300, 311, 406, 577, 286, 1910, 721, 13, 286, 519, 51696], "temperature": 0.0, "avg_logprob": -0.16293447667902167, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0003053148393519223}, {"id": 1401, "seek": 553998, "start": 5539.98, "end": 5544.78, "text": " there's a spectrum of intelligence from like zero to, I", "tokens": [50364, 456, 311, 257, 11143, 295, 7599, 490, 411, 4018, 281, 11, 286, 50604], "temperature": 0.0, "avg_logprob": -0.1383222184091244, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.0027147436048835516}, {"id": 1402, "seek": 553998, "start": 5544.78, "end": 5547.74, "text": " don't know, maybe infinity or something, some really large", "tokens": [50604, 500, 380, 458, 11, 1310, 13202, 420, 746, 11, 512, 534, 2416, 50752], "temperature": 0.0, "avg_logprob": -0.1383222184091244, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.0027147436048835516}, {"id": 1403, "seek": 553998, "start": 5547.74, "end": 5551.54, "text": " number beyond what what human beings are. And so it's this", "tokens": [50752, 1230, 4399, 437, 437, 1952, 8958, 366, 13, 400, 370, 309, 311, 341, 50942], "temperature": 0.0, "avg_logprob": -0.1383222184091244, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.0027147436048835516}, {"id": 1404, "seek": 553998, "start": 5551.54, "end": 5555.299999999999, "text": " continuum. So that's why I like chelets kind of on the measure", "tokens": [50942, 36120, 13, 407, 300, 311, 983, 286, 411, 417, 338, 1385, 733, 295, 322, 264, 3481, 51130], "temperature": 0.0, "avg_logprob": -0.1383222184091244, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.0027147436048835516}, {"id": 1405, "seek": 553998, "start": 5555.299999999999, "end": 5559.419999999999, "text": " of intelligence, because even though it doesn't actually give", "tokens": [51130, 295, 7599, 11, 570, 754, 1673, 309, 1177, 380, 767, 976, 51336], "temperature": 0.0, "avg_logprob": -0.1383222184091244, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.0027147436048835516}, {"id": 1406, "seek": 553998, "start": 5559.419999999999, "end": 5563.86, "text": " us a, you know, quantitative way yet to measure intelligence, it", "tokens": [51336, 505, 257, 11, 291, 458, 11, 27778, 636, 1939, 281, 3481, 7599, 11, 309, 51558], "temperature": 0.0, "avg_logprob": -0.1383222184091244, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.0027147436048835516}, {"id": 1407, "seek": 553998, "start": 5563.86, "end": 5566.58, "text": " at least is thinking along the right directions, which is how", "tokens": [51558, 412, 1935, 307, 1953, 2051, 264, 558, 11095, 11, 597, 307, 577, 51694], "temperature": 0.0, "avg_logprob": -0.1383222184091244, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.0027147436048835516}, {"id": 1408, "seek": 556658, "start": 5566.62, "end": 5571.18, "text": " do you measure intelligence? And how do you define it as a", "tokens": [50366, 360, 291, 3481, 7599, 30, 400, 577, 360, 291, 6964, 309, 382, 257, 50594], "temperature": 0.0, "avg_logprob": -0.20789697964986165, "compression_ratio": 1.4350649350649352, "no_speech_prob": 0.0014101890847086906}, {"id": 1409, "seek": 556658, "start": 5571.18, "end": 5576.46, "text": " category of activity? And then we can kind of get past this", "tokens": [50594, 7719, 295, 5191, 30, 400, 550, 321, 393, 733, 295, 483, 1791, 341, 50858], "temperature": 0.0, "avg_logprob": -0.20789697964986165, "compression_ratio": 1.4350649350649352, "no_speech_prob": 0.0014101890847086906}, {"id": 1410, "seek": 556658, "start": 5576.46, "end": 5578.22, "text": " black and white, you know, thinking.", "tokens": [50858, 2211, 293, 2418, 11, 291, 458, 11, 1953, 13, 50946], "temperature": 0.0, "avg_logprob": -0.20789697964986165, "compression_ratio": 1.4350649350649352, "no_speech_prob": 0.0014101890847086906}, {"id": 1411, "seek": 556658, "start": 5580.54, "end": 5581.5, "text": " Well, gentlemen,", "tokens": [51062, 1042, 11, 11669, 11, 51110], "temperature": 0.0, "avg_logprob": -0.20789697964986165, "compression_ratio": 1.4350649350649352, "no_speech_prob": 0.0014101890847086906}, {"id": 1412, "seek": 556658, "start": 5582.0599999999995, "end": 5585.26, "text": " always a pleasure. Absolutely. Yeah, absolutely.", "tokens": [51138, 1009, 257, 6834, 13, 7021, 13, 865, 11, 3122, 13, 51298], "temperature": 0.0, "avg_logprob": -0.20789697964986165, "compression_ratio": 1.4350649350649352, "no_speech_prob": 0.0014101890847086906}], "language": "en"}