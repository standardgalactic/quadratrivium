WEBVTT

00:00.000 --> 00:06.000
I had the great pleasure of catching up with Professor Larissa Soldatova from Goldsmiths University.

00:06.000 --> 00:10.000
She's really interested in automating the process of science.

00:10.000 --> 00:17.000
And in order to do that, you need to know about logic, abduction, knowledge and semantics.

00:17.000 --> 00:24.000
Biology is so complex, we still scratch the surface trying to understand.

00:24.000 --> 00:30.000
And there are not enough scientists in the world to do it.

00:30.000 --> 00:36.000
And if we can have some AI help, that would be probably a good thing.

00:36.000 --> 00:45.000
So my presentation about reflecting on automation of science, history of it, where we are with it now,

00:45.000 --> 00:52.000
how much we can automate now and where we're probably heading towards.

00:52.000 --> 00:56.000
So I'm really interested in the philosophy of science, which is to say,

00:56.000 --> 01:00.000
what is the meta process that scientists go through when they do science?

01:00.000 --> 01:09.000
So science is about generating theories, explanations about how the world is this way and also how it's not that way.

01:09.000 --> 01:19.000
But science is based on empiricism, which is basically following the data, doing experiments and trying to discount hypotheses.

01:19.000 --> 01:24.000
But we still have this chicken and egg problem, which is where do the hypotheses come from?

01:24.000 --> 01:28.000
And this is very related to the bias variance trade-off in machine learning.

01:28.000 --> 01:33.000
Generally, we can't start from a blank slate. We can't start with nothing.

01:33.000 --> 01:38.000
We need to have some world knowledge to structure our search process.

01:38.000 --> 01:45.000
And then we run into trouble. How much should we seed the system with priors and world knowledge?

01:45.000 --> 01:50.000
There are so many things in our physical world that it just feels like we should encode it into our systems.

01:50.000 --> 01:56.000
It'd be a waste of time not to. But then there are some things that we feel that the system should learn.

01:56.000 --> 02:02.000
But if we bias the system too much, then we might accrue a kind of approximation error

02:02.000 --> 02:07.000
because there are things that our system just won't be able to search because we've biased it too much.

02:07.000 --> 02:14.000
So there's always this trade-off between having the flexibility to find new things versus cutting down the search base.

02:14.000 --> 02:22.000
The next big step will be when we will be able to consume more knowledge.

02:22.000 --> 02:30.000
So not just data, but if we start using what we know.

02:30.000 --> 02:40.000
Specific knowledge is a major source of machine and human intelligence and will suffice even with simple inference.

02:40.000 --> 02:46.000
So if you take an AI system, it's mostly taking lots of data.

02:46.000 --> 02:57.000
In the best case, it will take lots of different data, put it together, and then it outputs some predictive model or something else or piece of art.

02:57.000 --> 03:00.000
But knowledge is still hugely underused.

03:00.000 --> 03:05.000
No, one reason there is not much available in forms that computers can consume.

03:05.000 --> 03:14.000
But just imagine if even these new models were immediately fed back and could be an input, so it's called closed loop systems,

03:14.000 --> 03:17.000
then probably it would be more intelligent.

03:17.000 --> 03:18.000
So this is quite interesting.

03:18.000 --> 03:25.000
So going back to the 80s, the modus operandi in artificial intelligence was encoding knowledge into expert systems

03:25.000 --> 03:32.000
because we knew that if you could encode knowledge, you can deduce new knowledge and new facts from existing knowledge.

03:32.000 --> 03:38.000
So knowledge isn't just data, knowledge is one level of abstraction higher where you actually know the relations

03:38.000 --> 03:42.000
and you can apply some kind of functions between data.

03:42.000 --> 03:44.000
The problem is it's very brittle.

03:44.000 --> 03:49.000
Famously, Doug Lenitz created this project called The Psych Project,

03:49.000 --> 03:53.000
which had millions and millions of facts and relations and things in it.

03:53.000 --> 03:56.000
And it was almost a complete waste of time.

03:56.000 --> 03:59.000
It was too brittle, it wasn't particularly useful.

03:59.000 --> 04:02.000
But empirically, though, we seem to be wasting our time with this blank slate approach

04:02.000 --> 04:06.000
because machine learning moved away from really any explicit domain knowledge

04:06.000 --> 04:09.000
and it just became these meta priors.

04:09.000 --> 04:17.000
So things like symmetries and, I don't know, learning schedules and weight decay and stuff like that.

04:17.000 --> 04:23.000
And the models only had very abstract priors in the sense of how to learn things statistically.

04:23.000 --> 04:25.000
They didn't have any domain knowledge at all.

04:25.000 --> 04:29.000
But there are so many things in our physical world that we just understand

04:29.000 --> 04:32.000
and we should be able to explicitly code into our system.

04:32.000 --> 04:38.000
But we have this paradox which is that every time we try and encode high level knowledge into systems,

04:38.000 --> 04:43.000
it just introduces brittleness and this is what Rich Sutton warned against in his bitter lesson essay.

04:43.000 --> 04:47.000
What's the difference for you between data and knowledge?

04:47.000 --> 04:55.000
Yes, it's a good question and in different textbooks you can actually find different definitions.

04:55.000 --> 05:00.000
But people who are working in knowledge representations, they have very clear ideas.

05:00.000 --> 05:02.000
So data is facts.

05:02.000 --> 05:05.000
Knowledge is when you go level up.

05:05.000 --> 05:11.000
It's like rules, relations, ultimately better if you have models.

05:11.000 --> 05:14.000
Better if you have executable models.

05:14.000 --> 05:19.000
So you can put something in, get something out.

05:19.000 --> 05:21.000
So this is knowledge.

05:21.000 --> 05:24.000
A lot of knowledge you can discover directly from data.

05:24.000 --> 05:29.000
If you have a lot of data, you can discover these rules, you can construct models.

05:29.000 --> 05:35.000
But why don't you represent it immediately as knowledge if you already know it?

05:35.000 --> 05:38.000
Why? It's very inefficient way.

05:38.000 --> 05:43.000
It takes a lot of energy, these neural networks, they are destroying the planet.

05:43.000 --> 05:51.000
You actually can encode a lot what we know in quite compact way and you can reuse it.

05:51.000 --> 05:54.000
A lot what we know actually is not in the data.

05:54.000 --> 06:00.000
Just an example, my collaborator gave this example.

06:00.000 --> 06:04.000
They were modeling ecological systems and you can have a lot of data

06:04.000 --> 06:09.000
and you can discover from data that bear has between two and five cups

06:09.000 --> 06:11.000
and if it's more north it's less cup.

06:11.000 --> 06:17.000
But why bother if we know you just encode it as a knowledge so you can extract it

06:17.000 --> 06:22.000
but you also can just encode immediately and you can encode a lot additionally.

06:22.000 --> 06:24.000
What is not in the data?

06:24.000 --> 06:27.000
That is why it's more powerful and promising.

06:27.000 --> 06:35.000
Larissa spoke about a project about 60 years ago to automate the scientific discovery of chemical compounds.

06:35.000 --> 06:40.000
Not too dissimilar actually to Alpha Fold which came out recently from DeepMind

06:40.000 --> 06:45.000
but anyway she's kind of saying that if you look at how these systems work today

06:45.000 --> 06:48.000
it's not dramatically different than it was 60 years ago.

06:48.000 --> 06:50.000
What chemists would do?

06:50.000 --> 06:55.000
And they put in place a whole pipeline, knowledge acquisition

06:55.000 --> 07:04.000
also giving output in forms that is good for users in this case chemists.

07:04.000 --> 07:09.000
It really produced very interesting promising chemical structure.

07:09.000 --> 07:12.000
Moreover this system actually was multi-agent.

07:12.000 --> 07:16.000
It was heuristic dendral, metadendral and it was closed loop.

07:16.000 --> 07:23.000
So whatever it managed to discover it immediately was going back and improved over time.

07:23.000 --> 07:30.000
So it was 60 years ago and I would claim we haven't progressed much further.

07:30.000 --> 07:34.000
Of course there were other fantastic systems

07:34.000 --> 07:41.000
but the underlying architecture and principle how it operates is still the same.

07:42.000 --> 07:51.000
So it's called the robot scientist and this was introduced again 15-20 years ago now.

07:51.000 --> 08:01.000
So the idea is to develop a system, AI system that is capable of generating hypothesis, designing experiments

08:01.000 --> 08:09.000
having real robotic labs to carry out these experiments, analyze results, do it in cycle and hopefully discover something new.

08:09.000 --> 08:15.000
So it's a lot of data, external data, whatever data sets are relevant,

08:15.000 --> 08:20.000
plus internally produced because it's a robotic lab, very powerful,

08:20.000 --> 08:26.000
it can run in parallel, like 1000 biologists working in parallel in lab.

08:26.000 --> 08:30.000
But it also has this knowledge, formalized knowledge.

08:30.000 --> 08:36.000
It has domain knowledge, the first system worked with yeast biology

08:36.000 --> 08:44.000
and it's also had this knowledge about what scientific discovery is, what key elements, how they're connected,

08:44.000 --> 08:49.000
what you need to do, just like our experimental practices, how to design experiments.

08:49.000 --> 08:52.000
You need to explain to machine at all.

08:52.000 --> 08:55.000
So in science we need to do abduction.

08:55.000 --> 09:00.000
This is absolutely delicious because you've heard us speak about abduction ad nauseam on MLST

09:00.000 --> 09:04.000
and Larissa is out there talking about it, which is wonderful.

09:04.000 --> 09:10.000
But yeah, you have to start with a hypothesis or a set of hypotheses in science before you do inference.

09:10.000 --> 09:15.000
So inference is when you kind of learn mappings from hypotheses,

09:15.000 --> 09:20.000
but abduction is how you first select the relevant hypotheses,

09:20.000 --> 09:26.000
hypotheses that could generate powerful explanations for some phenomena.

09:26.000 --> 09:29.000
And it's very creative, it's very mysterious.

09:29.000 --> 09:32.000
So what is the logician 101 definition of abduction?

09:32.000 --> 09:36.000
Well, it's reasoning to the best explanation.

09:36.000 --> 09:38.000
Is that helpful?

09:38.000 --> 09:40.000
Maybe, maybe.

09:40.000 --> 09:45.000
I love the kind of cognitive science view on things, which is, you know, an explanation.

09:45.000 --> 09:50.000
It's a causal model, which helps us understand the world because it gives explanatory power.

09:50.000 --> 09:53.000
It carves the world up by the joints.

09:53.000 --> 09:54.000
Why is the world this way?

09:54.000 --> 09:56.000
Why is the world not that way?

09:56.000 --> 10:02.000
And the raw building materials of these theories are cognitive priors,

10:02.000 --> 10:06.000
and they're the same kind of priors that we imbue into machine learning models.

10:06.000 --> 10:10.000
Noam Chomsky says that some of these priors are built into our brains.

10:10.000 --> 10:14.000
I'm a big fan that many of the priors are socially embedded.

10:14.000 --> 10:17.000
They're like software that float around mimetically.

10:17.000 --> 10:22.000
But anyway, abduction is about you have a bunch of priors in the system,

10:22.000 --> 10:27.000
and we as humans have this magical ability to grab the relevant priors,

10:27.000 --> 10:32.000
stick them together into small models that give explanatory power,

10:32.000 --> 10:37.000
and selecting a few of those models, and that's what we do when we do abduction.

10:37.000 --> 10:42.000
So the key question is basically, can we automate that in a machine?

10:42.000 --> 10:47.000
I will focus on hypothesis because in philosophy of science for a long time,

10:47.000 --> 10:53.000
we considered that it's something that you cannot automate, that only humans can do it.

10:53.000 --> 10:58.000
And to enable these hypotheses, this generation,

10:58.000 --> 11:02.000
so you need to go out of the system, out of what you already know.

11:02.000 --> 11:06.000
So you cannot use conventional logic like abduction.

11:06.000 --> 11:08.000
You need to use something else.

11:08.000 --> 11:11.000
And I'm used abduction, so there are other logic that you can use,

11:11.000 --> 11:16.000
but you need to go outside of the system to have these guesses,

11:16.000 --> 11:21.000
and then try to see if some of it makes sense or not.

11:21.000 --> 11:22.000
It's not trivial.

11:22.000 --> 11:31.000
Focusing on hypothesis, like you want to test hypothesis that this particular gene has that function,

11:31.000 --> 11:36.000
but you actually cannot put gene into well, or you cannot measure function.

11:36.000 --> 11:39.000
So you start doing some inferences.

11:39.000 --> 11:42.000
What if we replace it to use some proxy?

11:42.000 --> 11:48.000
If we use something with this gene knocked out, and if we put metabolites,

11:48.000 --> 11:51.000
and so you go deeper, deeper, deeper.

11:51.000 --> 11:55.000
So biologists do it just normal practices,

11:55.000 --> 12:00.000
but when you try to explain it to computer how to do it, you really need to think.

12:00.000 --> 12:03.000
So what steps we're taking,

12:03.000 --> 12:08.000
we actually very rarely experiment with something that we make conclusions about.

12:08.000 --> 12:12.000
So our conclusions are not always accurate, so then you have to go back.

12:12.000 --> 12:16.000
So it was very interesting for us to try to automate it,

12:16.000 --> 12:21.000
and then also reflect on how humans are doing science, especially experimental science.

12:21.000 --> 12:24.000
There are many levels to this as well.

12:24.000 --> 12:26.000
So if you want to go and get a job at Google now,

12:26.000 --> 12:32.000
they already have a bit of a meritocracy which is based on kind of what we're talking about here.

12:32.000 --> 12:36.000
To be very senior at Google, you need to find people,

12:36.000 --> 12:40.000
and to be one step below that, you need to find the areas,

12:40.000 --> 12:42.000
and below that you need to solve abstract problems,

12:42.000 --> 12:45.000
and then below that you need to solve specific problems.

12:45.000 --> 12:51.000
So there's a kind of hierarchy in the amount of creativity and ambiguity that you can tolerate,

12:51.000 --> 12:55.000
and that's what scientists do, so they're always traversing these levels.

12:55.000 --> 12:58.000
Abduction is about finding the areas,

12:58.000 --> 13:02.000
and then once you perform inference on these hypotheses that you have abducted,

13:02.000 --> 13:05.000
then that's solving a much more specific problem,

13:05.000 --> 13:07.000
and it gets more and more specific.

13:07.000 --> 13:13.000
The key point is that science is combinatorially, exponentially expensive,

13:13.000 --> 13:15.000
just to run experiments.

13:15.000 --> 13:19.000
And there isn't enough time in the world, there aren't enough scientists in the world.

13:19.000 --> 13:22.000
Yes, having AI scientists can help us,

13:22.000 --> 13:27.000
but we still need to always reflexively and abstractly go one level up.

13:27.000 --> 13:30.000
We always need to be asking ourselves the question, can we do this better?

13:30.000 --> 13:34.000
If we had a different perspective or a different view, could we make this more efficient?

13:34.000 --> 13:37.000
And then could we make this more efficient?

13:37.000 --> 13:39.000
And that's what we humans can do,

13:39.000 --> 13:42.000
and that's what we need to imbue into AI robot scientists.

13:42.000 --> 13:48.000
Well, you need to test like five, six drugs, work on these combinations.

13:48.000 --> 13:51.000
So yes, it's combinatorical problem.

13:51.000 --> 13:58.000
If you do it in a lab, we just don't have enough cells in our blood to run all these experiments.

13:58.000 --> 14:02.000
So this is a justification when you need such system.

14:02.000 --> 14:06.000
You first do reasoning based on all what we know,

14:06.000 --> 14:11.000
with all these gaps, contradictions, but the best what you can do.

14:11.000 --> 14:16.000
Output some plausible hypothesis is what should work in this situation.

14:16.000 --> 14:21.000
Then you can do some simulation, again, the computational,

14:21.000 --> 14:26.000
come up with some hundreds, several hundred hypothesis,

14:26.000 --> 14:33.000
that then you can automatically test so that there's a cheaper way of doing it using robotics.

14:33.000 --> 14:37.000
Yes, so I'm really interested in this idea of human cyborgs,

14:37.000 --> 14:40.000
basically where humans and AIs work together.

14:40.000 --> 14:48.000
The problem is it's not easy to say that combining humans and AIs together produces more productivity.

14:48.000 --> 14:51.000
It probably does, but we don't know for sure.

14:51.000 --> 14:56.000
There was a piece out by ThoughtWorks recently where they were talking about generative AI or co-pilot,

14:56.000 --> 14:59.000
making developers two times more productive.

14:59.000 --> 15:01.000
How do they measure that?

15:01.000 --> 15:03.000
Software engineering is not a reducible activity.

15:03.000 --> 15:05.000
It's a very complex phenomenon.

15:05.000 --> 15:09.000
People have tried to measure the productivity of software engineers for many years.

15:09.000 --> 15:15.000
How many lines of code do they write and the storyboard and issue tracking and stuff like that.

15:15.000 --> 15:20.000
It's all rubbish, and the reason for that is it's a very complex, irreducible phenomenon.

15:20.000 --> 15:22.000
Therein lies the problem.

15:22.000 --> 15:27.000
There's a meme going around on LinkedIn at the moment talking about how with co-pilot,

15:27.000 --> 15:33.000
you can generate the code ten times faster, but you then spend ten times more time debugging the code,

15:33.000 --> 15:38.000
and that's because there's a technical debt, a new type of technical debt, understanding debt.

15:38.000 --> 15:44.000
You generated all of this code and it works, and then in order to fix a problem, you need to actually understand it.

15:44.000 --> 15:47.000
The mental model is not in the code.

15:47.000 --> 15:51.000
Actually, in any software engineering project, the mental model is a memetic social thing.

15:51.000 --> 15:56.000
It's in the brains of the developers, and it kind of floats around in the ether.

15:56.000 --> 15:58.000
It's not in the code.

15:58.000 --> 16:00.000
So, yeah, it's a similar thing here.

16:00.000 --> 16:05.000
If we had an AI robotic scientist, how would we know that it's making us go faster?

16:05.000 --> 16:09.000
I don't think such systems will replace humans.

16:09.000 --> 16:20.000
As you see, at the best, they can automate some science, some experimental-driven science,

16:20.000 --> 16:28.000
but it's an important part of science, and if it can be automated, it will be of great help.

16:28.000 --> 16:34.000
Humans still have distinct advantages in many areas.

16:34.000 --> 16:36.000
Hopefully, we will retain it.

16:36.000 --> 16:44.000
Ultimately, the best way is to work together, take advantage, and really, because it's him.

16:44.000 --> 16:49.000
Then maybe we can make a faster progress.

16:49.000 --> 16:50.000
Welcome to MLST.

16:50.000 --> 16:53.000
We are here with Professor Larissa Soldatova.

16:53.000 --> 17:01.000
Larissa joined Goldsmiths in November 2017 as director of the Online Masters in Data Science program.

17:01.000 --> 17:05.000
She's an internationally recognized expert in artificial intelligence,

17:05.000 --> 17:12.000
particularly in discovery science, reasoning, knowledge representation, and semantic technologies.

17:12.000 --> 17:16.000
Larissa has also been working on the Robot Scientist project,

17:16.000 --> 17:20.000
which investigates which processes of scientific discovery can be automated

17:20.000 --> 17:24.000
and how robotic and human scientists can work together.

17:24.000 --> 17:31.000
The Robot Scientist, Adam, was the first system which made an autonomous scientific discovery.

17:31.000 --> 17:36.000
She's also involved in a number of international projects in the development of semantic standards,

17:36.000 --> 17:41.000
for example, the machine learning schema, the robotics task ontology standard,

17:41.000 --> 17:45.000
and also the laboratory protocols exact.

17:45.000 --> 17:48.000
It's a pleasure to have you here, Larissa.

17:48.000 --> 17:52.000
Maybe you could just start off by telling us about the events and your talk today.

17:52.000 --> 17:58.000
My talk was about where we are with automating science.

17:58.000 --> 18:04.000
Yes, it's very creative endeavor to do science,

18:04.000 --> 18:11.000
but there are parts of it that scientists try to automate for a long time.

18:11.000 --> 18:19.000
So the history of scientific discovery as a subject of computer science

18:19.000 --> 18:23.000
is actually more than half a century old.

18:23.000 --> 18:31.000
The first systems were developed in the 1960s in Stanford.

18:31.000 --> 18:38.000
The first system was inspired by the need to go to other planets,

18:38.000 --> 18:43.000
collect samples, like if you go to Mars.

18:43.000 --> 18:48.000
You cannot send many samples back.

18:48.000 --> 18:53.000
If you send data through what you managed to analyze,

18:53.000 --> 19:00.000
it will take so long to send it to human scientists to get some instructions what to do next.

19:00.000 --> 19:06.000
So that would stimulate the development of autonomous intelligence systems

19:06.000 --> 19:14.000
that need to collect something, do something, decide what to do next, what it means.

19:15.000 --> 19:21.000
Since then, it was a slow but steady progress.

19:21.000 --> 19:26.000
So the systems that we are working on, it's already over 20 years.

19:26.000 --> 19:28.000
Yes, it's called Robot Scientist.

19:28.000 --> 19:30.000
And yes, the first was Adam.

19:30.000 --> 19:38.000
It's actually stand for adaptive machines, but yeah, Adam, because it was the first.

19:38.000 --> 19:43.000
Not to be confused with the other Adam for training neural networks?

19:43.000 --> 19:45.000
Yes, it's a good point.

19:45.000 --> 19:51.000
So the principle of such system is that it's very knowledge-intensive.

19:51.000 --> 19:59.000
So we hear a lot about progress in AI, but it's mostly data-driven.

19:59.000 --> 20:03.000
So now we have so much data and also computational power,

20:03.000 --> 20:07.000
so it can be processed and something useful output.

20:07.000 --> 20:14.000
And I believe that the next step will be when a lot of knowledge will be available

20:14.000 --> 20:16.000
in the form that computers can take.

20:16.000 --> 20:18.000
So right now they are taking data.

20:18.000 --> 20:27.000
If we can give our knowledge and let them to reason with it, so that will be just...

20:27.000 --> 20:31.000
And to do scientific discovery, of course, you need to have knowledge.

20:31.000 --> 20:41.000
Scientists still insure supply because it takes like 25 years to produce.

20:41.000 --> 20:43.000
Wow, you need to educate.

20:43.000 --> 20:46.000
It's many, many years of studying.

20:46.000 --> 20:56.000
And if we can create systems and also knowledge models starting from some domain knowledge

20:56.000 --> 21:00.000
in some particular areas, so we are working with biological systems.

21:00.000 --> 21:07.000
So if you can encode in machine-processable form, what we know,

21:07.000 --> 21:11.000
and also encode how we do science.

21:11.000 --> 21:14.000
So how we formulate hypothesis.

21:14.000 --> 21:17.000
Of course, not any, but at least some.

21:17.000 --> 21:20.000
It's actually quite algorithmic.

21:20.000 --> 21:22.000
Yes, I'd love to know more about that.

21:22.000 --> 21:24.000
So you said some really interesting things.

21:24.000 --> 21:27.000
I mean, first of all, as you say, if we want to learn about Mars,

21:27.000 --> 21:29.000
it's very sample inefficient because it's very expensive.

21:29.000 --> 21:32.000
And then we get to the purpose of science.

21:32.000 --> 21:38.000
And in my opinion, the purpose of science is about generating intelligible explanations.

21:38.000 --> 21:40.000
And then we get to knowledge.

21:40.000 --> 21:45.000
And I think you were just alluding to data and information is not knowledge.

21:45.000 --> 21:48.000
And I know you're an expert in knowledge representation.

21:48.000 --> 21:56.000
So perhaps we could just touch on, does knowledge have some primacy over just normal information?

21:57.000 --> 22:03.000
Okay, so it's very true that different textbooks will give different definition.

22:03.000 --> 22:06.000
And sometimes there is no distinction between data and information.

22:06.000 --> 22:08.000
It's like synonym.

22:08.000 --> 22:13.000
But yes, in my area of research, we are very clear about that.

22:13.000 --> 22:14.000
Naturally.

22:14.000 --> 22:17.000
So what we can see the data is facts.

22:17.000 --> 22:18.000
Yes.

22:18.000 --> 22:21.000
So this gene has this function.

22:21.000 --> 22:25.000
And duck is birds.

22:25.000 --> 22:29.000
Knowledge is when you go further.

22:29.000 --> 22:38.000
So when you have rules, when you have, like, even connections or links between facts,

22:38.000 --> 22:40.000
you can go further.

22:40.000 --> 22:43.000
Yes, more knowledge model.

22:44.000 --> 22:55.000
If you have a lot of data, how's prices in this area, how it was affected by flooding,

22:55.000 --> 23:00.000
or so they dropped how much, then you can detect this patterns.

23:00.000 --> 23:02.000
So this is knowledge.

23:02.000 --> 23:07.000
So if there is flooding in the area, prices will drop.

23:07.000 --> 23:09.000
So this is the rule.

23:09.000 --> 23:14.000
And you can represent it in various forms.

23:14.000 --> 23:22.000
So if we can represent it in forms that you can put it into memory of computers,

23:22.000 --> 23:24.000
then it can be used.

23:24.000 --> 23:29.000
And if you can be used together with data, if you have all these components,

23:29.000 --> 23:31.000
it will be even more intelligent.

23:31.000 --> 23:32.000
Interesting.

23:32.000 --> 23:38.000
I guess where I was going with the question a little bit was also, can knowledge be probabilistic?

23:38.000 --> 23:39.000
Of course.

23:39.000 --> 23:41.000
All our knowledge is probabilistic.

23:41.000 --> 23:44.000
We are not certain about anything.

23:44.000 --> 23:48.000
Even if we think, yes, this is the state of the art.

23:48.000 --> 23:53.000
So much knowledge was changed, was refuted.

23:53.000 --> 23:57.000
So we always walk in this probabilistic framework.

23:57.000 --> 24:00.000
We not always reflect on it.

24:00.000 --> 24:06.000
But if you want to build a system that really outputs something,

24:06.000 --> 24:10.000
it is underlying.

24:10.000 --> 24:14.000
So the other side of working on such a system,

24:14.000 --> 24:20.000
it gives you a chance to understand better about what science is.

24:20.000 --> 24:24.000
There are no guaranteed truths.

24:24.000 --> 24:27.000
It's just what we believe in at this point,

24:27.000 --> 24:31.000
given all data we have and the other series,

24:31.000 --> 24:33.000
we just walk together.

24:33.000 --> 24:35.000
But it can change in the future.

24:35.000 --> 24:37.000
There are a couple of things.

24:37.000 --> 24:41.000
Rationalists think that there is only certainty.

24:41.000 --> 24:43.000
You either know it or you don't.

24:43.000 --> 24:51.000
I believe that Bayesian reasoning is an extension of logical reasoning in the domain of uncertainty.

24:51.000 --> 24:53.000
Absolutely correct.

24:53.000 --> 24:55.000
And it's important to remember about it.

24:55.000 --> 25:02.000
It's important to remember even when we read newspapers.

25:02.000 --> 25:06.000
But even scientific articles,

25:06.000 --> 25:11.000
scientists actually tend to overgeneralize.

25:11.000 --> 25:15.000
They are very excited about what they discovered.

25:15.000 --> 25:19.000
It's important to step back and think that,

25:19.000 --> 25:21.000
yes, it actually can be refuted.

25:21.000 --> 25:24.000
It can be only one explanation.

25:24.000 --> 25:29.000
And if you look at history of science,

25:29.000 --> 25:33.000
in Newtonian physics,

25:33.000 --> 25:38.000
it looked like the ultimate truth and said no.

25:38.000 --> 25:42.000
The theory of relativity turned it all upside down.

25:42.000 --> 25:45.000
It's only some fraction of it.

25:45.000 --> 25:46.000
Yes.

25:46.000 --> 25:51.000
To what extent should knowledge be grounded in the physical world?

25:51.000 --> 25:57.000
So, the whole science, it has to be verifiable.

25:57.000 --> 26:04.000
You can imagine many things plausible,

26:04.000 --> 26:11.000
but unless you really show that this corresponds to reality,

26:11.000 --> 26:13.000
we cannot trust it.

26:13.000 --> 26:17.000
Of course, there can be maybe another explanation,

26:17.000 --> 26:21.000
but at least this is connected to what we observe or even better

26:21.000 --> 26:26.000
if we can design an experiment to show there is something behind it.

26:26.000 --> 26:30.000
So, this is how science always worked.

26:30.000 --> 26:34.000
Of course, I referred to experimental science

26:34.000 --> 26:37.000
like biology, physics, where you actually can choose.

26:37.000 --> 26:41.000
There are sort of experiments where we will not go astray

26:41.000 --> 26:47.000
as we cannot build an automated system to do that.

26:47.000 --> 26:48.000
Interesting.

26:48.000 --> 26:51.000
I wondered whether you would define yourself as a neat or a scruffy

26:51.000 --> 26:53.000
when it comes to knowledge representation.

26:53.000 --> 26:55.000
I mean, for example, if I define...

26:55.000 --> 26:58.000
And by the way, a neat is like a puritanical,

26:58.000 --> 27:01.000
simple underlying principle, parsimony.

27:01.000 --> 27:03.000
So, if I wanted to define a chair,

27:03.000 --> 27:08.000
I could think of thousands of different analogies to describe a chair.

27:08.000 --> 27:12.000
Is it collapsible or is it just very complicated?

27:12.000 --> 27:17.000
I am a very neat and I can give you a perfect example

27:17.000 --> 27:19.000
how to model a chair.

27:19.000 --> 27:20.000
Go on.

27:20.000 --> 27:24.000
We are very neat because we work with these reasoning systems

27:24.000 --> 27:27.000
and they need sharp, clean logic.

27:27.000 --> 27:30.000
And if you complicate matters, it complicates reasoning.

27:30.000 --> 27:33.000
A chance at something will be really discovered

27:33.000 --> 27:36.000
and we want to connect different bits of knowledge, different data.

27:36.000 --> 27:39.000
So, the more streamlined it is, the better.

27:39.000 --> 27:44.000
And if you use like minimum of relations

27:44.000 --> 27:49.000
and so over years, I just learn to think that way

27:49.000 --> 27:51.000
and about chair.

27:51.000 --> 27:54.000
It's actually examples that I used in my lectures

27:54.000 --> 27:56.000
when I was explaining to students.

27:56.000 --> 27:57.000
Wonderful.

27:57.000 --> 27:59.000
So, I was showing them just examples.

27:59.000 --> 28:01.000
So, these are chairs.

28:01.000 --> 28:03.000
They can look very differently.

28:03.000 --> 28:05.000
Some don't have any legs.

28:05.000 --> 28:07.000
Some have three legs.

28:07.000 --> 28:09.000
So, how to explain computers?

28:09.000 --> 28:10.000
This is a chair.

28:10.000 --> 28:11.000
Yes.

28:11.000 --> 28:12.000
So, you can...

28:12.000 --> 28:15.000
Yes, you can represent many, many, many ways.

28:15.000 --> 28:19.000
But a good knowledge model

28:19.000 --> 28:23.000
will focus what we call intrinsic property.

28:23.000 --> 28:25.000
What makes it a chair?

28:28.000 --> 28:29.000
Give me an example.

28:29.000 --> 28:31.000
What makes it a chair?

28:31.000 --> 28:34.000
It's function to be seated on.

28:34.000 --> 28:37.000
So, it's a relational model.

28:37.000 --> 28:40.000
So, it was designed.

28:40.000 --> 28:43.000
It was produced for it.

28:43.000 --> 28:44.000
Yeah.

28:44.000 --> 28:45.000
This is so interesting.

28:45.000 --> 28:47.000
It can take many shapes, many colors,

28:47.000 --> 28:50.000
but this is what defines a chair.

28:50.000 --> 28:52.000
And because we produced it human,

28:52.000 --> 28:57.000
so we can say this is function.

28:57.000 --> 28:59.000
And all other...

28:59.000 --> 29:01.000
Yes, it can have other properties

29:01.000 --> 29:03.000
like what color it is, what shape.

29:03.000 --> 29:06.000
But ultimately, this would make it a chair.

29:06.000 --> 29:07.000
Yes.

29:07.000 --> 29:10.000
Because when I hear philosophers talking about semantics,

29:10.000 --> 29:12.000
they have to come up with some kind of a relational model.

29:12.000 --> 29:15.000
And they will choose the view of function

29:15.000 --> 29:18.000
or purpose is another good one.

29:18.000 --> 29:22.000
And I guess what I'm saying is that it's a little bit anthropocentric

29:22.000 --> 29:26.000
and like function.

29:26.000 --> 29:28.000
For example, a hospital.

29:28.000 --> 29:30.000
That building used to be a hospital,

29:30.000 --> 29:32.000
but it's not a hospital anymore.

29:32.000 --> 29:38.000
But again, why it is so centric?

29:38.000 --> 29:41.000
Because it's us who made it.

29:41.000 --> 29:43.000
So we have control over it.

29:43.000 --> 29:46.000
Yes, it was hospital, now it's no more hospital.

29:46.000 --> 29:49.000
Some social convention, some decision was made,

29:49.000 --> 29:51.000
it was repurposed.

29:51.000 --> 29:53.000
Now it is not hospital, it's something else.

29:53.000 --> 29:57.000
But if you take biological systems like gene function,

29:57.000 --> 30:00.000
we have nothing to do with that.

30:00.000 --> 30:02.000
It evolved to be there.

30:02.000 --> 30:05.000
It's still intrinsic property of it,

30:05.000 --> 30:08.000
what function it has, what it's for.

30:08.000 --> 30:11.000
Otherwise you can be very far.

30:11.000 --> 30:14.000
Again, going back to a chair,

30:14.000 --> 30:16.000
I can sit on a table.

30:16.000 --> 30:18.000
It doesn't make it a chair

30:18.000 --> 30:20.000
because it's not primarily function.

30:20.000 --> 30:23.000
This is not what this was made for.

30:23.000 --> 30:24.000
Yes, yes.

30:24.000 --> 30:26.000
So if you agreed on it,

30:26.000 --> 30:29.000
you need to have this clarity,

30:29.000 --> 30:32.000
also agreement that this is how you model things.

30:32.000 --> 30:34.000
Then you start to have consistency.

30:34.000 --> 30:36.000
What is a chair? What is a table?

30:36.000 --> 30:39.000
Because if you want to have intelligences,

30:39.000 --> 30:42.000
then to design your office, for example.

30:42.000 --> 30:44.000
So how will it do?

30:44.000 --> 30:47.000
How it will distinguish what is what?

30:47.000 --> 30:51.000
So this is quite similar in principle to Wittgenstein

30:51.000 --> 30:54.000
said that the meaning of a word is in its use.

30:54.000 --> 30:57.000
And I'm friends with a colleague of yours, Mark Bishop,

30:57.000 --> 31:00.000
who says the meaning of computation is in its use.

31:00.000 --> 31:03.000
But that's a very kind of relativistic,

31:03.000 --> 31:06.000
social emergentist kind of view.

31:06.000 --> 31:10.000
I guess what I'm saying is do you think there's a platonic meaning?

31:10.000 --> 31:14.000
So for me, it's very easy to answer on these questions

31:14.000 --> 31:18.000
because I actually work in the development of real systems

31:18.000 --> 31:21.000
that need to take it, reason with it and produce something.

31:21.000 --> 31:25.000
Justification of what we are doing and how we are doing it works.

31:25.000 --> 31:28.000
You can do it differently.

31:28.000 --> 31:30.000
You can take different points of view.

31:30.000 --> 31:32.000
Let's design a different system based on it

31:32.000 --> 31:34.000
and then compare what is better.

31:34.000 --> 31:36.000
No one has done it.

31:36.000 --> 31:40.000
So this is what is done, how it's done, it's working.

31:40.000 --> 31:42.000
Okay, so that's very pragmatic.

31:42.000 --> 31:47.000
Yes, I would say it's not relativistic.

31:47.000 --> 31:53.000
It's a pragmatic approach to producing knowledge

31:53.000 --> 31:55.000
in machine-consumable form.

31:55.000 --> 31:57.000
It's not optimal, it's not the best,

31:57.000 --> 32:02.000
but it's something good that computers can work with

32:02.000 --> 32:04.000
and produce something.

32:04.000 --> 32:06.000
I think you might be a closet scruffy.

32:06.000 --> 32:14.000
Okay, are you familiar with the Psyche project from Doug Lennet?

32:14.000 --> 32:16.000
I think so.

32:16.000 --> 32:20.000
It's almost like preaching to the choir.

32:20.000 --> 32:25.000
What kind of retrospective comments do you have about that project?

32:25.000 --> 32:30.000
I think they just attempted to do too much.

32:30.000 --> 32:33.000
So it's like in AI.

32:33.000 --> 32:44.000
So this was a constant genetic AI versus something practical, fragmented.

32:44.000 --> 32:48.000
So of course it's wonderful to have a system that can solve any task

32:48.000 --> 32:51.000
for that you need such knowledge models.

32:51.000 --> 32:53.000
But just not say yet.

32:54.000 --> 32:56.000
Maybe in the future.

32:56.000 --> 33:02.000
Right now we're trying to model some fragments, some domains of knowledge.

33:02.000 --> 33:07.000
And if we have enough such fragments, hopefully we can combine.

33:07.000 --> 33:14.000
But ultimately, if you want to have general AI, you will need such models.

33:14.000 --> 33:16.000
Interesting.

33:16.000 --> 33:21.000
One contrast people make is, I have friends who are in the domain of certainty.

33:21.000 --> 33:24.000
So I think knowledge is either you know it or you didn't.

33:24.000 --> 33:27.000
And the reason that they give and the reason why they don't like language models

33:27.000 --> 33:31.000
is because they say, well, when you have actual knowledge, so you actually know,

33:31.000 --> 33:34.000
you can deduce new facts and new knowledge.

33:34.000 --> 33:37.000
So you can create more knowledge.

33:37.000 --> 33:41.000
And they would say with a language model, because it's only pretending to reason,

33:41.000 --> 33:44.000
you can't create any new knowledge with it.

33:44.000 --> 33:50.000
It's actually an advantage we already started using it to generate hypothesis.

33:50.000 --> 33:58.000
Because it's not rigid deduction, it is how you can actually make new guesses.

33:58.000 --> 34:00.000
And some of them might be true.

34:00.000 --> 34:03.000
You just need to experiment to show it.

34:03.000 --> 34:06.000
So we already started.

34:06.000 --> 34:11.000
We generated some very interesting hypothesis.

34:11.000 --> 34:14.000
We're already organizing experiments to test them.

34:14.000 --> 34:16.000
Yes.

34:16.000 --> 34:20.000
I believe that science is about abduction.

34:20.000 --> 34:23.000
So like creating this plausible set of hypotheses.

34:23.000 --> 34:26.000
And we're at an event all about creativity.

34:26.000 --> 34:29.000
And humans have that spark of inspiration, don't they?

34:29.000 --> 34:33.000
We just have an idea about plausible hypotheses.

34:33.000 --> 34:39.000
And maybe the world is quite sclerotic and predictable and boring.

34:39.000 --> 34:44.000
But do you think that there's some spark that we won't get from Ayo?

34:44.000 --> 34:46.000
I completely agree with you.

34:46.000 --> 34:48.000
And it's exactly what we use.

34:48.000 --> 34:53.000
We use abduction in our reasoning system to generate hypothesis.

34:53.000 --> 34:54.000
Yes.

34:54.000 --> 35:00.000
So in some simple application areas like in drug design, you use abduction.

35:00.000 --> 35:06.000
You just have many, many, many examples of chemical structures, short activities.

35:06.000 --> 35:10.000
So you can, if you see similar structure, you induce it.

35:10.000 --> 35:13.000
It might show similar activity again.

35:13.000 --> 35:14.000
It's not always.

35:14.000 --> 35:18.000
So still, you cannot infer ultimate truth.

35:18.000 --> 35:20.000
That is why you need experiments.

35:20.000 --> 35:26.000
But abduction, yes, this is something that takes you outside of what you already know.

35:26.000 --> 35:27.000
Yes.

35:27.000 --> 35:28.000
I completely agree.

35:28.000 --> 35:33.000
I think it's the most exciting part.

35:33.000 --> 35:40.000
But again, this is what is technological.

35:40.000 --> 35:44.000
So there are reasoners that can do abduction.

35:44.000 --> 35:49.000
There are languages that can handle it.

35:49.000 --> 35:53.000
So we have many building blocks to build such system.

35:53.000 --> 35:54.000
Yes.

35:54.000 --> 35:55.000
Yes.

35:55.000 --> 35:59.000
And are you excited about the prospect of neurosymbolic architectures?

35:59.000 --> 36:06.000
Or are you still very old school and you like these explicit knowledge representation?

36:06.000 --> 36:10.000
That is a difficult question.

36:10.000 --> 36:14.000
Yes, I'm very excited about what is happening.

36:14.000 --> 36:21.000
I was skeptical about all these natural language processing.

36:21.000 --> 36:24.000
We tried to work with NLP people.

36:24.000 --> 36:30.000
And the results were quite disappointing how much useful information they actually could extract.

36:30.000 --> 36:34.000
And then such progress.

36:34.000 --> 36:36.000
So of course, it is exciting.

36:36.000 --> 36:42.000
And we already started using it in our scientific work.

36:42.000 --> 36:59.000
So at the same time, I believe there is a place for this very clean, crisp model just because you need something as simple as possible just to enable reasoning.

36:59.000 --> 37:07.000
Just because all these reasoning engines are powerful.

37:07.000 --> 37:15.000
But if they can reason over simpler structures, there is more chances they will do it better.

37:15.000 --> 37:25.000
So for practical purposes, from all my experiences, the cleaner you can get, the better it works.

37:25.000 --> 37:36.000
And ultimately, when we have more technological progress, we probably will be able to more reflect how it is in our brain.

37:36.000 --> 37:37.000
Yes.

37:37.000 --> 37:40.000
But there is also robustness and reliability and trustworthiness.

37:40.000 --> 37:41.000
Precisely.

37:41.000 --> 37:47.000
So you actually then can explain how it was done through this explainability.

37:47.000 --> 37:49.000
It's all trustworthiness.

37:49.000 --> 37:53.000
So if people understand how it works, if we can explain, this is how it happened.

37:53.000 --> 37:56.000
This is what was taken as input.

37:56.000 --> 37:58.000
This is how it was done.

37:58.000 --> 38:00.000
And people will trust it more.

38:00.000 --> 38:06.000
And especially like systems that we are working on, applications for cancer research.

38:06.000 --> 38:16.000
What doctor would recommend these drugs if they don't understand how this system came up with the suggestions?

38:16.000 --> 38:17.000
Yes.

38:17.000 --> 38:23.000
And again, the cleaner your representation, the easier to explain it.

38:23.000 --> 38:24.000
I know.

38:24.000 --> 38:25.000
I really agree with you.

38:25.000 --> 38:39.000
The only intuition I have is that if we're designing a novel molecule, for example, even if we knew or could understand how it was created, would we really understand it?

38:39.000 --> 38:41.000
It seems so complicated, doesn't it?

38:41.000 --> 38:43.000
It is.

38:43.000 --> 38:45.000
But again, there are different logics.

38:45.000 --> 38:50.000
So for example, physiologic, you can build system around that.

38:50.000 --> 39:01.000
So ultimately, let's try to build more such systems and then you just force to understand.

39:01.000 --> 39:04.000
So if you can build it, then you understand how it's working.

39:04.000 --> 39:05.000
Yes.

39:05.000 --> 39:09.000
Inevitably.

39:09.000 --> 39:16.000
So I still believe that some portion and put in portion of this can be modeled.

39:16.000 --> 39:20.000
Something probably not in any near future.

39:20.000 --> 39:24.000
It's just too complex, too fast.

39:24.000 --> 39:26.000
Louisa has been an absolute honor.

39:26.000 --> 39:30.000
How can people find out more about you and what you're up to?

39:30.000 --> 39:36.000
We're actually very bad with telling about what we are doing.

39:36.000 --> 39:40.000
That is why I'm so glad that you invited me to talk about it.

39:40.000 --> 39:47.000
We just buried in our work and the best we will publish our scientific papers and it is there with top.

39:47.000 --> 39:52.000
Yes, you teach students, you tell students, it's based on your research, you use examples.

39:52.000 --> 40:00.000
But apart from that, we don't do much and we are really quite keen to change this.

40:00.000 --> 40:09.000
I think a lot of people are intimidated by papers and science and if only people knew how many fascinating ideas there are in there,

40:09.000 --> 40:13.000
I think they just need a hook and it's so interesting.

40:13.000 --> 40:15.000
No, I blame scientists.

40:15.000 --> 40:20.000
They keep this complicated language even if sometimes they don't need it.

40:20.000 --> 40:25.000
So they just keep people away from it.

40:25.000 --> 40:26.000
Yes, they do.

40:26.000 --> 40:32.000
No, I think scientists should do more about explaining their work, how they are doing it.

40:32.000 --> 40:38.000
I always thought that we are doing our work, someone else will.

40:38.000 --> 40:51.000
It's so full that AI has created problems, but no, it's actually us who have to do it and explain and tell and also warn.

40:51.000 --> 40:54.000
Yeah, well now people can ask GPT for.

40:54.000 --> 40:59.000
But what it generates, it's not all of us correct.

40:59.000 --> 41:02.000
That's true, it's not, but it's quite good.

41:02.000 --> 41:04.000
It's good, it's very impressive.

41:04.000 --> 41:06.000
Yeah, it's quite good.

41:06.000 --> 41:07.000
Larissa has been an honor.

41:07.000 --> 41:08.000
Thank you so much.

41:08.000 --> 41:09.000
Thank you.

41:09.000 --> 41:10.000
Thank you very much.

41:10.000 --> 41:11.000
My pleasure.

41:11.000 --> 41:15.000
I'm so impressed by how much you know and understand.

41:15.000 --> 41:16.000
Oh really?

41:16.000 --> 41:17.000
Wow.

41:17.000 --> 41:20.000
So many people around know abduction.

41:20.000 --> 41:21.000
Come on.

41:21.000 --> 41:23.000
We are a computer science podcast.

41:23.000 --> 41:26.000
I think it's one of the most fundamental things.

41:26.000 --> 41:32.000
Because you know, going back to Aristotle, you know, with the weak syllogisms and the strong syllogisms.

41:32.000 --> 41:39.000
And, you know, I think of induction is actually quite interesting because most people use it to include abduction.

41:39.000 --> 41:46.000
So not only creating the map to the hypothesis space, but actually, you know, abduction is just creating the plausible.

41:46.000 --> 41:53.000
I completely agree because if you clear about it, so you clear about logic, so you can have fuzzy concept.

41:53.000 --> 41:57.000
All this probabilistic, but underlying logic must be crisp.

41:57.000 --> 41:58.000
You need to understand.

41:58.000 --> 42:04.000
And if it's abduction, precisely, it's only with some probabilities that it's correct.

