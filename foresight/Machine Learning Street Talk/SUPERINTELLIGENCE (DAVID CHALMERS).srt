1
00:00:00,000 --> 00:00:05,680
He moved to Indiana University in 1989, where he obtained his PhD in philosophy and cognitive

2
00:00:05,680 --> 00:00:11,360
science, working for the legendary Douglas Hofstadter, by the way. I've got his book here

3
00:00:11,360 --> 00:00:16,880
in the Center for Research on Concepts and Cognition. Douglas, by the way, is one of the most

4
00:00:16,880 --> 00:00:21,360
legendary figures in the AI space. We also had the pleasure of interviewing one of his other

5
00:00:21,360 --> 00:00:27,120
PhD students, Professor Melanie Mitchell. Now, David recently wrote this fascinating book called

6
00:00:27,120 --> 00:00:34,960
Reality Plus. In that book, he discussed the three central philosophical questions, actually.

7
00:00:34,960 --> 00:00:40,320
The reality question, which is to say, are virtual worlds real? His answer to that is yes.

8
00:00:40,960 --> 00:00:45,600
The knowledge question, which is to say, can we know whether or not we're in a virtual world?

9
00:00:45,600 --> 00:00:53,440
His answer to that is no. Also, the value question, which is to say, can you lead a good life

10
00:00:53,440 --> 00:00:59,280
in a virtual world? And his answer to that is a resounding yes. Now, you probably also heard

11
00:00:59,280 --> 00:01:04,800
of this notion of an extended mind, something which David formulated with Professor Andy Clark,

12
00:01:04,800 --> 00:01:11,360
and they described the idea as active externalism, based on the active role of the environment in

13
00:01:11,360 --> 00:01:16,400
driving cognitive processes, or put simply, you might think of your phone as being an extension

14
00:01:16,400 --> 00:01:21,680
of your mind, for example. What was it like to work with Douglas during your PhD? Oh, he was great.

15
00:01:21,680 --> 00:01:31,200
He was so interested in so many things. It was officially, it was an AI lab, and most of the

16
00:01:31,200 --> 00:01:36,640
people there were AI researchers. You mentioned Melanie Mitchell. She was my colleague there.

17
00:01:36,640 --> 00:01:44,800
Bob French, who's done important work on AI, Gary McGraw, Jim Marshall, Wong-Pay, and others.

18
00:01:45,760 --> 00:01:49,920
I was the only philosopher. People were interested in so many things, whether it was,

19
00:01:50,640 --> 00:01:59,920
we had workshops on humor, or creativity, on mathematics, on politics, on everything,

20
00:01:59,920 --> 00:02:06,800
as well as all the stuff on AI, analogy, concepts. It was also a very exciting time to be there,

21
00:02:06,800 --> 00:02:17,440
because this was one of the, in the boom and bust cycle of machine learning and of neural

22
00:02:17,440 --> 00:02:24,240
networks. This was one of the boom periods, the early 1990s. The PDP books had just come out,

23
00:02:24,240 --> 00:02:31,360
parallel distributed processing by Rubble Hart, McClelland, and Hinton. There was so much excitement

24
00:02:31,360 --> 00:02:39,120
about the capacities of neural networks. I ended up writing a few papers on machine learning

25
00:02:39,120 --> 00:02:44,720
back then. One was on the evolution of learning systems. One was on getting machines to learn

26
00:02:45,360 --> 00:02:54,080
structural generalizations. All with fairly basic neural networks. A few years after that,

27
00:02:54,080 --> 00:02:59,760
the bottom fell out of neural networks for another 15 years or so, but this was a very

28
00:02:59,760 --> 00:03:04,480
exciting period to be there. One thing about Doug that you don't quite get from his books,

29
00:03:06,720 --> 00:03:11,280
his books, he's so enthusiastic about everything. He's not into all these ideas and so on, but it

30
00:03:11,280 --> 00:03:18,000
turns out there's 5% of things in the world or 1% that he's enthusiastic about. The other 99% he hates.

31
00:03:20,400 --> 00:03:27,040
He's like, he likes certain approaches to AI he loved, but even neural networks, he was like,

32
00:03:28,160 --> 00:03:35,520
he was not a big fan of most research in neural networks back then. He said,

33
00:03:35,520 --> 00:03:40,560
I think it was a, this is a bandwagon or even he said a bad wagon. Likewise for philosophy,

34
00:03:40,560 --> 00:03:44,320
there's bits of philosophy he loves. There's a lot of it. There's a lot of it he doesn't like.

35
00:03:44,320 --> 00:03:50,160
So maybe in person, you get more of a, more of that very opinionated side, but really he's a,

36
00:03:51,520 --> 00:03:56,080
he's just, you know, that book Go to Lesha Bark was just, I mean, it was what drew me into philosophy

37
00:03:56,080 --> 00:04:01,680
and AI as a teenager and it's so rich. And you go back to that book, there's still so many ideas

38
00:04:01,680 --> 00:04:05,920
there. The Mind's Eye is another book he edited with Dan Dennett. It was reading the Mind's Eye

39
00:04:05,920 --> 00:04:11,280
that really first got me thinking about issues about consciousness and the mind-body problem.

40
00:04:11,280 --> 00:04:15,840
So yeah, he's a very rich thinker. Yeah. Well, thank God for Douglas Tostata.

41
00:04:15,840 --> 00:04:21,040
Yeah, I love the Mind's Eye. It has one of the, one of the eeriest stories of all. What was it?

42
00:04:21,040 --> 00:04:26,080
The riddle of the universe and its solution where there's, there's some image or some text that if

43
00:04:26,080 --> 00:04:31,600
you read it, it causes your brain basically to core dump. And so it becomes this infectious thing

44
00:04:31,600 --> 00:04:35,920
where anybody who goes into that office and reads that thing, they just crash and they never,

45
00:04:35,920 --> 00:04:40,320
they go into a coma and they never wake up again. And so, you know, there's a huge investigation

46
00:04:40,320 --> 00:04:44,560
to figure out what's going on and more and more people keep going into comas, you know, because

47
00:04:44,560 --> 00:04:50,400
of this crazy ideas in that book. Could I just pick up on this point because you were saying that

48
00:04:50,400 --> 00:04:54,640
Douglas Tostata turned his nose up at Neural Networks. And I hope he still would actually,

49
00:04:54,640 --> 00:04:58,480
and Melanie Mitchell certainly does. And I mean, Douglas had that paper out called

50
00:04:58,480 --> 00:05:03,520
The Core of Cognition. He was always huge about this idea of the primacy of analogy making.

51
00:05:03,520 --> 00:05:08,000
And actually, there's a few researchers today that are mirroring that, that idea that Francois

52
00:05:08,000 --> 00:05:12,880
Chollet, for example, he says intelligence is literally sensitivity to abstract analogies.

53
00:05:12,880 --> 00:05:17,040
It's not memorizing the internet. So I think, thank God that we do have people out there

54
00:05:17,040 --> 00:05:21,680
thinking slightly differently because the modus operandi today is that we need to build a big

55
00:05:21,680 --> 00:05:26,240
hash table of everything. And even the way we formalize intelligence, as we were just saying

56
00:05:26,240 --> 00:05:31,840
previously, we're not really paying attention to why or how the things do what they do. We're

57
00:05:31,840 --> 00:05:35,760
just looking at the behavioral output bit like the Turing test, as long as it looks and smells

58
00:05:35,760 --> 00:05:42,720
like a duck, then it must be a duck. So thank God for Douglas Tostata. Yeah, I think he's always been

59
00:05:43,360 --> 00:05:48,880
at the same time an enthusiast about the possibility of AI while being somewhat skeptical

60
00:05:48,880 --> 00:05:56,000
about the capacities of existing AI and about the kind of hype that suggests that AI might just be

61
00:05:56,480 --> 00:06:01,680
10 or 20 years around the corner. So in the early 90s, I think, yeah, that was,

62
00:06:03,040 --> 00:06:08,080
that was especially, especially natural. Back around then, people would say a year spent working

63
00:06:08,080 --> 00:06:13,360
in AI is enough to make you believe in God. It was so hard to get anything even like

64
00:06:15,680 --> 00:06:20,560
any kind of intelligence out of an AI system. And I think, you know, Doug was equally skeptical

65
00:06:20,560 --> 00:06:26,720
of both the symbolic and the connectionist on neural network approaches back then.

66
00:06:27,840 --> 00:06:32,880
I think ultimately his sympathies lay with it on the neural network side of things,

67
00:06:32,880 --> 00:06:37,200
with the idea that intelligence could in principle bubble up from a million, you know,

68
00:06:37,200 --> 00:06:42,000
from 100 billion separate little interactions, intelligence could bubble up from there. But

69
00:06:42,000 --> 00:06:47,520
I still think he'd be inclined to think that current approaches are too statistical, too simple,

70
00:06:47,520 --> 00:06:53,760
and so on. That said, you have to look at the, at the progress in machine learning over the last

71
00:06:54,800 --> 00:06:58,400
10 years. And it's been amazing and surprising. And I think even somebody like,

72
00:06:59,280 --> 00:07:02,800
even people like Melanie or like Doug are going to have to say this has been

73
00:07:02,800 --> 00:07:08,400
something they did not expect. And that they did not predict. So I actually, I was back in

74
00:07:08,400 --> 00:07:14,480
Indiana just over two years ago, just before the pandemic got going February 2020. And

75
00:07:15,440 --> 00:07:18,720
I don't know, maybe that was before GBT three, but still there've been all these amazing

76
00:07:19,840 --> 00:07:24,880
developments in machine learning over the last few years. I asked Doug about this,

77
00:07:24,880 --> 00:07:28,720
and what do you make of this? And because he's on the record of saying, you know,

78
00:07:28,720 --> 00:07:33,360
there will be AI eventually, but it's going to have to be involve all these new kinds of complexity,

79
00:07:33,360 --> 00:07:37,680
not not something simple like this. And he says, Yeah, well, this is, this is troubling.

80
00:07:37,680 --> 00:07:41,760
This is concerning, you know, it could be, I don't know yet, but it could be that I was wrong.

81
00:07:42,320 --> 00:07:47,440
It could be there are simpler passes, paths to AI. And his attitude was that would be very

82
00:07:47,440 --> 00:07:52,400
disappointing. It turned out that you could actually train up an AGI, just using those

83
00:07:53,120 --> 00:07:57,440
simple methods to human levels. That would make, I think Doug's view was that would make kind of

84
00:07:57,440 --> 00:08:04,560
human level intelligence less, less grand and remarkable than, than he had thought. Well,

85
00:08:04,560 --> 00:08:09,360
actually, back in, back in Goethe-les-Sherbach, I think he said that even to have a machine that

86
00:08:09,360 --> 00:08:14,000
could beat a human at chess, it would have to be good at everything would be a good composer,

87
00:08:14,000 --> 00:08:19,040
it could tell jokes and so on. Okay, that one, that view got rolled out back in the,

88
00:08:19,040 --> 00:08:27,920
back in the 90s. So the question is, you know, is this, is this ever growing progress of just,

89
00:08:27,920 --> 00:08:31,200
of the kind of machine learning that says just throw a whole lot of compute,

90
00:08:31,200 --> 00:08:36,240
and a whole lot of data at it, and see what happens. Is that eventually going to get us to

91
00:08:36,320 --> 00:08:39,840
human level intelligence? Or is it, is it just going to get us so far

92
00:08:40,800 --> 00:08:44,560
with, and there's going to be principled limitations? I've always been on the side of,

93
00:08:45,360 --> 00:08:50,560
they'll probably only get us so far. But I have to say those principled limitations,

94
00:08:50,560 --> 00:08:57,840
those obstacles that have not yet been conquered are getting smaller and smaller. And the progress,

95
00:08:57,840 --> 00:09:01,120
if the progress of the last five or 10 years continues for another five or 10 years,

96
00:09:01,680 --> 00:09:06,960
then who's to say what's going to be left? Yeah, there was a fascinating anecdote in,

97
00:09:06,960 --> 00:09:12,400
in Melanie's book about how she and Hofstadter went to the Googleplex one time. And basically,

98
00:09:12,400 --> 00:09:17,360
as you said, Douglas was terrified that intelligence might be disappointingly simple to

99
00:09:17,360 --> 00:09:22,800
mechanize because he felt of the mind of Chopin as being infinitely nuanced. And just,

100
00:09:22,800 --> 00:09:26,960
just the incredible process that must have gone through his mind when he, when he produced his

101
00:09:26,960 --> 00:09:32,640
music. But I wanted to, and just quickly, by the way, you said that there was a conception in,

102
00:09:32,640 --> 00:09:38,080
in the 70s that task specific skill was what was required for intelligence or a collection of,

103
00:09:38,080 --> 00:09:44,160
of specific skills. And, and now the mindset is much more towards task acquisition efficiency

104
00:09:44,160 --> 00:09:49,440
and generalization. But I wanted to just quickly pick you up on the so-called intelligence

105
00:09:49,440 --> 00:09:54,400
explosion question. So this is a subject which Nick Bostrom has popularized after his book,

106
00:09:54,480 --> 00:09:59,680
Superintelligence. Personally speaking, we're not particularly sympathetic to this view. And

107
00:09:59,680 --> 00:10:04,560
Saigre Francois-Labe, he said in a blog post recently that this line of reasoning represents

108
00:10:04,560 --> 00:10:09,040
a misunderstanding of intelligence. He said that in his opinion, intelligence is situational.

109
00:10:09,680 --> 00:10:13,840
He said that our environment puts a hard limit on our individual intelligences. He said that

110
00:10:13,840 --> 00:10:18,720
most of our intelligence is not in the brain, it's externalized as civilization. And that an

111
00:10:18,720 --> 00:10:23,840
individual brain cannot implement recursive intelligence augmentation like a Godel machine.

112
00:10:23,840 --> 00:10:28,560
He also said that there are already many examples of recursively self-improving systems.

113
00:10:28,560 --> 00:10:32,880
Even personal investing, for example, is a recursively self-improving system. The more

114
00:10:32,880 --> 00:10:38,640
money you have, the more money you make. Anyway, so Bostrom described a thought experiment in 2003.

115
00:10:39,280 --> 00:10:43,280
I'm sure you've heard of this. The scenario describes an artificial, you know, like a very

116
00:10:43,280 --> 00:10:47,840
advanced artificial intelligence task with manufacturing paper clips. If such a machine

117
00:10:47,840 --> 00:10:52,000
were not programmed to value human life, then given enough power over its environment,

118
00:10:52,000 --> 00:10:56,240
it would try to turn all the matter in the universe, including human beings,

119
00:10:56,240 --> 00:11:01,040
into paper clips or machines which could manufacture paper clips. Do you think we might

120
00:11:01,040 --> 00:11:05,440
be on the precipice of being turned into paper clips, as Bostrom famously described in his

121
00:11:05,440 --> 00:11:10,800
thought experiment? Yeah, look, it's there's two different issues here. One is, will we get to

122
00:11:10,800 --> 00:11:17,520
some kind of much greater than human superintelligence relatively soon by some kind of intelligence

123
00:11:17,520 --> 00:11:23,680
explosion process? And second, if that happens, are there major dangers around? Yeah, I wrote

124
00:11:23,680 --> 00:11:29,360
about both of these things back in 2009. I had an article called, yeah, the Singularity

125
00:11:29,360 --> 00:11:35,360
of Philosophical Analysis, where I tried to take this line of reasoning for an intelligence

126
00:11:35,360 --> 00:11:42,240
explosion through recursive, through basically through recursive design of ever more sophisticated

127
00:11:42,240 --> 00:11:46,480
AIs. I tried to take that and turn it into an argument. I mean, the classic statement of this

128
00:11:46,480 --> 00:11:53,520
comes from I.J. Goode, the statistician and philosopher back in 1965 on the design of an

129
00:11:53,520 --> 00:11:58,800
ultra intelligent machine where he puts the basic idea right there that once you've got a machine

130
00:11:58,800 --> 00:12:06,560
which is smarter than a human, it will be able to design a machine which is smarter still,

131
00:12:06,560 --> 00:12:12,400
and then you're going to get recursive, runaway explosion of intelligence. I tried to analyze

132
00:12:12,400 --> 00:12:18,480
that to set out that article, that argument in as much detail as I could, analyze where it could

133
00:12:18,480 --> 00:12:22,880
go right, where it could go wrong, what the possible obstacles would be, and it's a long story.

134
00:12:23,840 --> 00:12:30,160
If anyone wants to look it up, it's out there on my website. But I in the end became convinced

135
00:12:30,160 --> 00:12:36,640
this is a pretty powerful argument. There's only so many ways it could go wrong. I think it's

136
00:12:36,640 --> 00:12:41,600
important that not every recursive augmentation process is going to lead to an intelligence

137
00:12:41,680 --> 00:12:46,880
explosion. It could easily bottom out, could asymptote before human intelligence. But I do

138
00:12:46,880 --> 00:12:51,200
think that once we start from greater than human intelligence, we have to find some way to get

139
00:12:51,200 --> 00:12:56,080
to greater than human intelligence first. This explosion won't get you that. But once you get

140
00:12:56,080 --> 00:13:02,240
there, then there's pretty good reason to think things in principle can take off from there.

141
00:13:02,240 --> 00:13:08,880
If intelligence is extended, I'm a big fan of the idea that intelligence is extended into the

142
00:13:08,880 --> 00:13:15,280
environment. But as far as I can tell, all that can in principle be augmented too. We develop

143
00:13:15,280 --> 00:13:21,760
extended systems, which are smarter than humans, and then they'll be able to design even better

144
00:13:21,760 --> 00:13:29,200
extended systems. And we could then have an intelligence explosion of extended intelligences.

145
00:13:29,200 --> 00:13:34,720
So I'm actually, nothing about this gets you to human level intelligence. But once we get to human

146
00:13:34,720 --> 00:13:40,560
level intelligence and a little bit beyond, then I think there's a pretty good case that there's some

147
00:13:40,560 --> 00:13:46,160
kind of potential explosion in the offing. Then the other issue you mentioned Bostrom and the paper

148
00:13:46,160 --> 00:13:52,800
clips is, yeah, what does this mean for the future of humanity? I guess I don't know what I'd say

149
00:13:52,800 --> 00:13:58,480
about the probabilities, but I'd say, yeah, once you have greater than human artificial general

150
00:13:58,480 --> 00:14:04,320
intelligence, then there's many ways that can go wrong for the obvious reasons that such a being

151
00:14:04,320 --> 00:14:10,000
is going to be extremely powerful. The most intelligent beings in the universe tend to be

152
00:14:10,000 --> 00:14:16,400
the most powerful for obvious reasons. Whatever they want, they have the capacity to get. So it's

153
00:14:16,400 --> 00:14:23,600
going to be extremely important that our AGI systems want the right things. That is, they have

154
00:14:23,600 --> 00:14:29,120
the right kind of goals. Or as people put it, these days that they are aligned with human goals.

155
00:14:30,000 --> 00:14:35,760
Because if they're even a little bit misaligned, then there's going to be the capacity for things

156
00:14:35,760 --> 00:14:39,840
to go very badly wrong. I know there are some people who think that the alignment is going to

157
00:14:39,840 --> 00:14:45,680
have to be so precise that, you know, missed by just the tiniest bit and will destroy the universe,

158
00:14:45,680 --> 00:14:51,600
whereas others think it's extremely robust. It may be more robust than that. I'm not

159
00:14:51,600 --> 00:14:55,200
totally sure about that. But I'm certainly on the side of people who think we have to take this

160
00:14:55,200 --> 00:15:01,200
issue extremely seriously. And there is at least potential existential risks here that if AGI is

161
00:15:01,200 --> 00:15:07,200
produced in an unthinking way, perhaps say in a military or a financial context where there's

162
00:15:07,200 --> 00:15:17,920
an AI arms race, and we suddenly have greater than human AIs that can achieve arbitrary goals,

163
00:15:17,920 --> 00:15:24,960
then suddenly it becomes an extremely sensitive matter what their goals are. So I'm certainly on

164
00:15:24,960 --> 00:15:28,160
Bostrom's side when it comes to, yeah, this is something we should take seriously.

165
00:15:28,880 --> 00:15:32,560
But isn't there a bit of, you know, it's a big distance to go from

166
00:15:33,760 --> 00:15:39,280
superior to human intelligence and achieve anything you want. I mean, I'm relatively

167
00:15:39,280 --> 00:15:46,320
intelligent, but I can't achieve flight, you know, by myself without, you know, apparatus to do that

168
00:15:46,320 --> 00:15:51,040
and airplane wings, whatever. I mean, there are physical limitations in the world. And I think

169
00:15:51,040 --> 00:15:55,200
sometimes there's this assumption that intelligence can kind of go to infinity,

170
00:15:55,200 --> 00:16:00,800
where in fact, maybe intelligence itself kind of bottoms out at IQ 1000 or something, there's

171
00:16:00,800 --> 00:16:05,360
just not much, you know, you can do beyond that certain IQ. I mean, isn't there a degree of

172
00:16:06,160 --> 00:16:12,400
kind of speculative, you know, extrapolation that we need to account for there?

173
00:16:13,840 --> 00:16:17,920
I would say this is certainly one way that things could, that the argument could fail,

174
00:16:17,920 --> 00:16:21,440
is if it turns out that basically there are diminishing, there's some kind of intelligence

175
00:16:21,440 --> 00:16:27,680
ceiling, and there's some kind of diminishing returns towards this. Just there is such a

176
00:16:27,680 --> 00:16:33,120
ceiling that we might find that when we make a being which is 10% smarter than us on some scale,

177
00:16:33,680 --> 00:16:40,240
it could only make a being which is 5% smarter than it. And that being will make a machine,

178
00:16:40,240 --> 00:16:45,920
make a being which is only 2.5% smarter than it. And all this will kind of asymptote to some

179
00:16:46,880 --> 00:16:52,000
intelligence ceiling. And I don't know, this turns on very subtle issues about the structure

180
00:16:52,000 --> 00:16:57,680
of intelligence space. I'm rather doubtful there is such an intelligence ceiling, or if there is

181
00:16:57,680 --> 00:17:02,400
one, maybe it's something like, you know, the limits of computability compared to, you know,

182
00:17:02,400 --> 00:17:07,840
hypercomputation that an infinite system could do. But I think that ceiling is so high that there's

183
00:17:07,840 --> 00:17:14,080
room for an awful lot of super intelligence before we get there. But in any case, I would say that,

184
00:17:14,080 --> 00:17:19,040
you know, for the purposes of, say, caution and thinking about the future, I would just turn the

185
00:17:19,040 --> 00:17:23,280
point back on you and say that the thought that there is such an intelligence ceiling is itself

186
00:17:23,280 --> 00:17:28,400
an extremely speculative one. I wouldn't want to rely on this, on this extremely speculative thought

187
00:17:28,400 --> 00:17:33,840
to kind of protect us from, from the, you know, potential risks of AGI in the future. If there's

188
00:17:33,840 --> 00:17:39,840
only a 20% chance there's not such an intelligence ceiling, then this is something that we very

189
00:17:39,840 --> 00:17:44,720
much need to be, to be worrying about. Yeah, I mean, fair enough, it's certainly a risk factor.

190
00:17:44,720 --> 00:17:50,400
It's certainly something that we need to need to keep a handle on. Well, let me ask you about one

191
00:17:50,400 --> 00:17:56,320
specific time there. So I'm thinking you're probably familiar with Carl Friston and, you know,

192
00:17:56,320 --> 00:18:00,960
his free energy principle. And he sends his regards, by the way, we talked to him a couple

193
00:18:00,960 --> 00:18:06,160
weeks back. And, and he wanted to ask you about kind of one line of thinking that he's been exploring

194
00:18:06,800 --> 00:18:12,720
lately. And I want to give you a quote from his 2018 article and my self conscious,

195
00:18:12,720 --> 00:18:18,800
or does self organization entail self consciousness. And he said, the proposal on offer here

196
00:18:19,520 --> 00:18:24,960
is that the mind comes into being when self evidencing has a temporal thickness,

197
00:18:25,680 --> 00:18:31,760
or counterfactual depth, which grounds inferences about the consequences of my action.

198
00:18:32,480 --> 00:18:38,720
On this view, consciousness is nothing more than the inference about my future, namely,

199
00:18:38,720 --> 00:18:45,120
self evidencing consequences of what I could do. What do you think about that, that perspective?

200
00:18:46,080 --> 00:18:50,560
Yeah, I'd have to know more about the connection to consciousness. I know that yeah, Friston is

201
00:18:51,280 --> 00:18:57,040
very has developed very deeply the idea of the mind as a prediction machine, a mind which is

202
00:18:57,040 --> 00:19:05,200
basically set up to, you know, predict whatever signal is coming next. And that's with that one

203
00:19:05,200 --> 00:19:10,640
basic key loss, you know, predict what's next, what's next, what's next, then you get to build these

204
00:19:10,640 --> 00:19:17,760
amazing models of the world with all of these, all of these, these capacities. And that's a

205
00:19:17,760 --> 00:19:23,440
really interesting perspective thinking about the mind and intelligence in general. And it's

206
00:19:23,440 --> 00:19:29,760
got to be at least one huge part of the story, even if it's not the whole story as Friston thinks

207
00:19:29,760 --> 00:19:36,080
it is, but I've never really understood the distinct what this kind of predictive approach has to say

208
00:19:36,080 --> 00:19:42,240
distinctively about consciousness. Because presumably there's a whole lot of different

209
00:19:42,240 --> 00:19:48,640
predictive processes at all kinds of levels of the hierarchy, including at the very early vision

210
00:19:48,640 --> 00:19:54,400
and very late cognition, and the whole mind is engaged in coming up with these predictions,

211
00:19:54,400 --> 00:20:01,600
but only some limited part of it is conscious. What you just said about, yeah, trying to figure out

212
00:20:01,600 --> 00:20:05,920
the predictions consequent on our actions, sounds to me like a very general statement of

213
00:20:07,040 --> 00:20:10,880
what the predictive approach says about the mind in general. And I haven't yet heard what is the

214
00:20:10,880 --> 00:20:16,160
part that corresponds to consciousness. Why, for example, or some representations get to be

215
00:20:16,160 --> 00:20:21,040
conscious where so much of it in the brain is not, I can give you I can give you a bit more

216
00:20:21,040 --> 00:20:27,040
detail, which may be helpful, because we did dig into him with a on a bit. And he said, for one thing,

217
00:20:27,040 --> 00:20:32,800
he expected that perhaps part of your response might might entail or talk about the meta hard

218
00:20:32,800 --> 00:20:38,320
problem. You know, why is it that certain beings, i.e. things like philosophers, and people like you

219
00:20:38,320 --> 00:20:42,960
and me puzzle so much about our qualitative experience. And the argument he makes there,

220
00:20:42,960 --> 00:20:48,320
he says that if we are inference machines that are built to actively self evidence,

221
00:20:48,320 --> 00:20:53,840
then that necessarily entails we need to have a generative model about our experienced world.

222
00:20:54,480 --> 00:20:59,760
And if we have that that generative model about our experience world, our experience world,

223
00:20:59,760 --> 00:21:06,640
then we have to entertain the hypothesis that we are things having a qualitative experience,

224
00:21:06,640 --> 00:21:11,760
along with the alternate to that hypothesis, which is that we're not having qualitative

225
00:21:11,760 --> 00:21:17,040
experiences. And so essentially that the capability to model the world generatively

226
00:21:17,600 --> 00:21:22,960
really requires that we entertain this hypothesis that we're actually having qualitative experiences

227
00:21:22,960 --> 00:21:28,640
or maybe not. And that's why we pontificate about it. Yeah, it's interesting. And I think the meta

228
00:21:28,640 --> 00:21:34,480
problem is a, yeah, as a promising approach is the meta problem is your why do we say and think

229
00:21:34,480 --> 00:21:39,040
the things we do about consciousness, instead of explaining consciousness directly,

230
00:21:39,040 --> 00:21:46,320
let's explain, you know, our internal model of consciousness. And yeah, there's got to be

231
00:21:46,320 --> 00:21:52,000
such a model. So I think this is a promising approach to take. I still don't fully, I mean,

232
00:21:52,000 --> 00:21:56,480
I think if you take the predictive approach, so what you would expect is, is the system would have

233
00:21:56,480 --> 00:22:02,240
many different models, you know, a big complex model of the world at all levels, it doesn't

234
00:22:02,240 --> 00:22:09,360
just correspond to experienced reality, but the models the world way beyond what's experienced,

235
00:22:09,360 --> 00:22:16,640
it would also you'd also expect the model to have a model of the mind to have a model of ourselves

236
00:22:16,640 --> 00:22:22,000
and relation to the world. But what actually happens in the in the human mind is we have,

237
00:22:23,120 --> 00:22:27,280
we have models at all levels, you know, there's like so many different levels of say of

238
00:22:27,280 --> 00:22:34,960
representation, even in the visual hierarchy. And somehow, though, only one of those

239
00:22:34,960 --> 00:22:38,720
levels seems to correspond to consciousness. The question is, why now do we need a distinctive

240
00:22:38,720 --> 00:22:46,560
model of those representations in us, which correspond to conscious experience? One idea,

241
00:22:46,560 --> 00:22:51,040
I think, one idea I quite like is that this could be like a simplification. In fact,

242
00:22:51,040 --> 00:22:56,160
we have millions of layers of representation of the world. But to build all that into our model

243
00:22:56,160 --> 00:23:01,360
of ourselves, and our relation of the world is going to be too complex. So we basically,

244
00:23:02,000 --> 00:23:07,280
we oversimplify by saying, ah, there's this one special relationship we have to the world,

245
00:23:07,280 --> 00:23:13,120
we call it consciousness or experience. And yet we experience certain things and then we use them

246
00:23:13,120 --> 00:23:18,880
to reason about them. And this is massively oversimplified as a model of the mind. But it could

247
00:23:18,880 --> 00:23:24,240
be that that simplification is then what actually gives us the sense that we have this special

248
00:23:24,240 --> 00:23:28,560
thing called consciousness. At least maybe that could explain why it seems to us that we have

249
00:23:28,560 --> 00:23:35,280
some special representations of the world. It's a further question why those conscious

250
00:23:35,280 --> 00:23:41,360
representations should seem to be so ineffable and subjective and hard to explain. And what

251
00:23:41,360 --> 00:23:46,160
in what Carl has written about this, I think he and Andy Clark had some ideas about the meta

252
00:23:46,160 --> 00:23:51,120
problem to try and push on this. Maybe that maybe there'd be certain representations that

253
00:23:51,200 --> 00:23:55,520
we'd have to be especially certain that we have them. Maybe that would give rise to

254
00:23:56,320 --> 00:24:01,680
the Descartes idea that, well, I'm not sure about the world, but I know that I'm thinking.

255
00:24:02,640 --> 00:24:07,840
I think, therefore, I am. And they had some kind of story about how this could get the whole,

256
00:24:07,840 --> 00:24:13,600
I think, therefore, I am certainty in one's own mind going. Anyway, I think it's an interesting

257
00:24:13,600 --> 00:24:16,720
approach and I'll be very cool to see if they can develop it further.

258
00:24:16,960 --> 00:24:21,600
Fascinating. I wanted to dig into this modeling thing. I was even thinking a second ago when

259
00:24:21,600 --> 00:24:26,080
you were talking about intelligence, that straight away you did the Hutter thing and

260
00:24:26,080 --> 00:24:31,040
we're talking about agents performing in environments and so on. And even that is a model.

261
00:24:31,040 --> 00:24:35,280
And of course, we're talking about complex phenomena and the way we model things depends

262
00:24:35,280 --> 00:24:40,640
on the level of analysis. But I'm really fascinated by this idea that some phenomena is so complex

263
00:24:40,640 --> 00:24:45,520
that it cannot be formalized or communicated, almost as if there's a representation problem.

264
00:24:45,520 --> 00:24:49,200
Now, you discussed in your consciousness book whether consciousness itself could be

265
00:24:49,200 --> 00:24:54,000
reductively explained and your knowledge argument, you spoke of this neuroscientist Mary

266
00:24:54,000 --> 00:24:57,600
that had been brought up in a black and white room. She's never seen any colors except for

267
00:24:57,600 --> 00:25:01,440
black and white and shades of gray. She's nevertheless one of the world's leading

268
00:25:01,440 --> 00:25:05,600
neuroscientists specializing in neurophysiology of color vision. She knows everything there is

269
00:25:05,600 --> 00:25:10,240
to know about neural processes involved in visual information processing, about the physics of

270
00:25:10,240 --> 00:25:14,960
optical processes, about the physical makeup of objects in the environment. But she doesn't know

271
00:25:15,040 --> 00:25:19,600
what it's like to see red. No amount of reasoning from physical facts alone will give her this

272
00:25:19,600 --> 00:25:25,520
knowledge. Physical facts about systems do not tell us what their conscious experiences are like.

273
00:25:25,520 --> 00:25:30,800
Now, you're speaking about this phenomenon in respect of the conscious or the phenomenological

274
00:25:30,800 --> 00:25:35,280
experience. But I think it's a much bigger problem of representation with any complex system, right?

275
00:25:35,280 --> 00:25:39,360
So what I find fascinating is that all of us have a conscious experience, but it's completely

276
00:25:39,360 --> 00:25:43,680
ineffable, as you just said, it's impossible for us to communicate it to others. And whenever

277
00:25:43,680 --> 00:25:48,080
we try to do so, we're reaching, right? Just like the blind men in the elephant, we end up defining

278
00:25:48,080 --> 00:25:53,440
some weird abstract motif, right? Chopping off 90% of the truth. The thing that fascinates me is

279
00:25:53,440 --> 00:25:58,080
that we need to have some kind of formalism or reduction in order to communicate, you know,

280
00:25:58,080 --> 00:26:02,560
in order to know or even understand anything. But so often is the case that all of the nuance

281
00:26:02,560 --> 00:26:08,080
and richness of the phenomena is lost in doing so. So I suspect that any formalism of a complex

282
00:26:08,080 --> 00:26:12,160
system might blind us from discovering a much better and richer formalism later because it

283
00:26:12,160 --> 00:26:17,440
kind of frames our thinking in quite a pernicious way in your book. So as I said, you were trying

284
00:26:17,440 --> 00:26:22,480
to separate the phenomenological experience as something that couldn't be described. But do

285
00:26:22,480 --> 00:26:30,240
you think it could be extended to any complex system? Well, we don't. As far as we know,

286
00:26:30,240 --> 00:26:35,920
you know, some complex systems actually have conscious subjective experience. But, you know,

287
00:26:36,000 --> 00:26:42,880
most of them don't. You know, this Mac that I'm using right now is a very complex system,

288
00:26:42,880 --> 00:26:47,840
but not much reason to think that it's conscious despite the complexity of what's going on

289
00:26:47,840 --> 00:26:53,360
within it. So certain kinds of complexity go along with consciousness. But if we were to kind of

290
00:26:53,360 --> 00:26:58,640
return to that meta problem approach for a moment, maybe there are certain kinds of properties

291
00:26:59,520 --> 00:27:04,960
of a complex system that tend to produce reports, for example, that the system

292
00:27:04,960 --> 00:27:12,560
is conscious. So maybe some complex systems have the capacity for a certain kind of direct

293
00:27:12,560 --> 00:27:17,520
self-modeling that corresponds to what we think of as introspection. We have introspection,

294
00:27:17,520 --> 00:27:22,400
which is a way of saying, this is what I'm perceiving right now. This is what I'm thinking

295
00:27:22,480 --> 00:27:28,320
right now. This is what I'm feeling right now. And we build a model of ourselves,

296
00:27:28,320 --> 00:27:32,960
and it may well be that that model is highly oversimplified. You don't have access

297
00:27:32,960 --> 00:27:39,600
to all these facts about ourselves. So perhaps you could tell a story where the kinds of complex

298
00:27:39,600 --> 00:27:44,800
systems that give rise at least to this capacity for introspection are then at least going to report

299
00:27:44,800 --> 00:27:53,200
themselves as being conscious. And maybe that could get at some element, maybe sort of the

300
00:27:53,200 --> 00:28:01,840
ineffability of consciousness. You'd expect to build these very simplified self-models. We wouldn't

301
00:28:01,840 --> 00:28:10,640
know immediately how to extend to other people. I mean, I still think, in principle, you could

302
00:28:10,640 --> 00:28:15,520
take Mary, who knows all about the human brain, and she could come to know all about those models

303
00:28:16,080 --> 00:28:21,680
in other people. But it still seems that she's never actually experienced red for herself.

304
00:28:21,680 --> 00:28:26,080
There's still something really crucial about this objective experience that she doesn't know.

305
00:28:26,080 --> 00:28:30,800
She doesn't know what it's like to experience red, and knowing all about the details of the model

306
00:28:30,800 --> 00:28:36,400
still hasn't told her that. So I think that's still something that needs explaining. Some people at

307
00:28:36,400 --> 00:28:43,120
this point just say that sense of something extra is an illusion. Something extra that the model

308
00:28:43,120 --> 00:28:47,440
hasn't explained is an illusion. But that's really where a lot of the action is at then.

309
00:28:50,080 --> 00:28:53,920
Just quickly, do you think there could be a sense of something extra to intelligence,

310
00:28:53,920 --> 00:29:01,600
as well as consciousness? Probably, yeah, we model our own intelligence with massively

311
00:29:01,600 --> 00:29:10,400
oversimplified self-models that were programmed into us by nature that model us as these agents

312
00:29:10,400 --> 00:29:20,320
with incredible capacities, free will, rationality, reason. It probably, again, it will, yeah, maybe

313
00:29:20,960 --> 00:29:24,800
I talked about consciousness is involving the subjective elements, intelligence is involving

314
00:29:24,800 --> 00:29:30,400
the objective elements. But yeah, we probably have oversimplified models of those conscious

315
00:29:32,400 --> 00:29:38,800
of those behavioral elements as well, perhaps that make us out to be more rational, or more free,

316
00:29:39,760 --> 00:29:47,920
or more capable than we actually are. I wanted to ask, what is an interesting simulation? Is

317
00:29:47,920 --> 00:29:53,280
our universe interesting or not? Because we represent just a pinprick of intelligence. So

318
00:29:53,280 --> 00:29:58,800
should intelligence be more spread out in the eyes of the simulator? Or in the vast majority of

319
00:29:58,800 --> 00:30:03,520
instances, would there just be gas everywhere or a singularity? Maybe stars can't form.

320
00:30:04,400 --> 00:30:10,320
Maybe the interesting phenomena itself is on the boundary between chaos and order,

321
00:30:11,040 --> 00:30:15,440
or between order and disorder, I should say, which is just a tiny sliver. So what do you think

322
00:30:15,440 --> 00:30:20,720
makes an interesting simulation? I don't know. I think it probably depends on your perspective,

323
00:30:20,720 --> 00:30:26,960
and it might, for example, depend on the perspective of the simulators, what they're after. One thing

324
00:30:26,960 --> 00:30:32,400
that a simulator might be doing is just create a whole lot of different universes with different

325
00:30:32,400 --> 00:30:39,600
potential laws of physics that they're simulating just to see what happens. And maybe if they're

326
00:30:39,600 --> 00:30:43,840
interested in, say, life or intelligence, then it could be that they're going to find that, okay,

327
00:30:43,840 --> 00:30:49,360
well, 99% of these simulations don't produce anything like life or intelligence. And yeah,

328
00:30:49,360 --> 00:30:56,080
1% of them produce life, and 0.01% lead to intelligence. So if that's what they're interested in,

329
00:30:57,040 --> 00:31:02,720
in studying, fantastic. But they might be interested in who's to say the laws of physics or

330
00:31:02,720 --> 00:31:08,560
galaxy formation, more generally, totally independent of life and intelligence. So I don't

331
00:31:08,560 --> 00:31:14,640
think there's any single standard of what's interesting. I mean, to me, as a philosopher

332
00:31:14,640 --> 00:31:19,440
interested in consciousness, I'm especially interested in this question of what kinds of

333
00:31:19,440 --> 00:31:26,240
simulations might actually develop conscious beings within them, not least because that's

334
00:31:26,240 --> 00:31:30,880
going to be especially relevant to our situation. If we're in a simulation, it seems we're conscious.

335
00:31:31,520 --> 00:31:37,840
So there's a question about just how this kind of simulation might get set up. But I think this

336
00:31:37,840 --> 00:31:43,440
whole, I mean, already simulations are used in actual practice for a million different purposes

337
00:31:43,440 --> 00:31:48,160
by scientists studying this phenomenon or that phenomenon, by people doing entertainment, by

338
00:31:48,160 --> 00:31:53,040
people doing prediction of the future, by people doing simulation of the past. And I guess when

339
00:31:53,040 --> 00:31:58,640
it comes to simulated universes, all of those sources of interest may themselves be present.

