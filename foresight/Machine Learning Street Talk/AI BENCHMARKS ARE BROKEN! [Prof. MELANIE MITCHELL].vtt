WEBVTT

00:00.000 --> 00:09.720
Let's have Melanie Mitchell to give our final opening statement, six minutes on the clock,

00:09.720 --> 00:10.720
Melanie.

00:10.720 --> 00:13.860
Yeah, so this is my opportunity to say I love Melanie.

00:13.860 --> 00:19.400
She is amazing, and she's coming back on MLST in about two weeks.

00:19.400 --> 00:20.400
Oh, amazing.

00:20.400 --> 00:21.400
Yeah, that's good.

00:21.400 --> 00:23.840
Yeah, she gives a good representation of herself in here, I think.

00:23.840 --> 00:24.840
Melanie is amazing.

00:25.520 --> 00:30.720
Fears about machines unleashing human extinction have deep roots in our collective psyche.

00:30.720 --> 00:34.480
These fears are as old as the invention of machines themselves.

00:34.480 --> 00:40.360
But tonight, we're debating whether these fears belong in the realm of science fiction

00:40.360 --> 00:48.680
and philosophical speculation, or whether AI is an actual real life existential threat.

00:48.680 --> 00:55.480
I'm going to argue that AI does not pose such a threat in any reasonably near future.

00:55.480 --> 01:01.320
Large language models have sparked heated debate on whether AI's exhibit genuine understanding

01:01.320 --> 01:03.960
of language and the world.

01:03.960 --> 01:09.420
With capabilities rivaling humans across diverse benchmarks, some hail language models as

01:09.420 --> 01:12.320
harbingers of real intelligence.

01:12.320 --> 01:18.400
But skeptics argue that their mastery is skin deep, lacking true comprehension.

01:18.400 --> 01:23.560
So how can we assess these claims and gain insight into the nature of what it means to

01:23.560 --> 01:24.560
understand?

01:24.560 --> 01:31.000
Now, on the show today, we have Professor Melanie Mitchell, a leading thinker on AI and intelligence,

01:31.000 --> 01:35.440
and one of the researchers in the community I personally most align with and look up to

01:35.440 --> 01:37.240
the most.

01:37.240 --> 01:43.520
Melanie's distinguished career crosses computer science, complex systems, and cognitive science,

01:43.520 --> 01:48.620
and she wrote the influential books Artificial Intelligence, A Guide for Thinking Humans,

01:48.620 --> 01:51.680
and also Complexity, A Guided Tour.

01:51.680 --> 01:57.320
Now central to Melanie's perspective is the idea that human understanding relies on flexible

01:57.320 --> 02:01.560
mental models grounded in sensory experience.

02:01.560 --> 02:08.640
Now she wrote that understanding language requires having the concepts that language describes.

02:08.640 --> 02:14.040
Large language models are trained purely on statistical relationships between words.

02:14.040 --> 02:18.600
Their knowledge is not grounded in a causal model of reality.

02:18.600 --> 02:24.320
Now Melanie is the Davis Professor of Complexity at the Santa Fe Institute, and her major work

02:24.320 --> 02:30.600
has been in the areas of analogical reasoning, complex systems, genetic algorithms, and cellular

02:30.600 --> 02:32.100
automata.

02:32.100 --> 02:35.480
She's achieved legendary status in the field of AI.

02:35.480 --> 02:41.320
She received her PhD in 1990 from the University of Michigan under Douglas Hofstadter, the

02:41.320 --> 02:44.240
famous author of Godel Escherbach.

02:44.240 --> 02:49.600
Melanie argues that we must rethink how AI systems are evaluated.

02:49.600 --> 02:54.480
Typical benchmarks summarize aggregate performance and, you know, these obscure failure modes

02:54.480 --> 02:57.680
and mask the underlying mechanisms.

02:57.680 --> 03:03.600
We need rigorous granular testing focused keenly on abstract generalization.

03:03.600 --> 03:07.760
Sort of like a sorcerer's apprentice gone nuclear.

03:07.760 --> 03:12.120
For example, Yoshua Benjiro wrote about this thought experiment.

03:12.120 --> 03:17.360
We might ask an AI to fix climate change and to solve the problem it could design a virus

03:17.360 --> 03:26.360
that decimates the human population, presto, humans dead, no more carbon emissions.

03:26.360 --> 03:34.280
This is an example of what's called the fallacy of dumb superintelligence.

03:34.280 --> 03:40.400
That is, it's a fallacy to think that a machine could be, quote, smarter than humans in all

03:40.400 --> 03:47.320
respects, unquote, and still lack any common sense understanding of humans, such as understanding

03:47.320 --> 03:52.840
why we made the request to fix climate change and the fact that we prefer not to be wiped

03:52.840 --> 03:55.040
out.

03:55.040 --> 04:02.200
This is all about having insight into one's goals and the likely effect of one's actions.

04:02.200 --> 04:07.680
We would never give unchecked autonomy and resources to an AI that lacked these basic

04:07.680 --> 04:09.480
aspects of intelligence.

04:09.480 --> 04:12.320
It just does not make sense.

04:12.320 --> 04:13.320
The third scenario.

04:13.320 --> 04:14.760
Yeah, that is absolutely.

04:14.760 --> 04:18.640
She made that point so much more eloquently than I've tried to make it in the past.

04:18.640 --> 04:22.160
Yeah, even earlier in this conversation, I was trying to get that across, but that's

04:22.160 --> 04:23.160
exactly it.

04:23.160 --> 04:24.160
It's this dumb superintelligence.

04:25.160 --> 04:26.160
Yeah, exactly.

04:26.160 --> 04:31.880
Anyway, folks, I hope you enjoy the show, and now I bring you Professor Melanie Mitchell.

04:31.880 --> 04:37.920
Sounds like almost like there's a very quiet supercomputer running behind the screen.

04:37.920 --> 04:38.920
It's my brain.

04:38.920 --> 04:39.920
Yeah.

04:39.920 --> 04:41.920
I think that's what this is.

04:41.920 --> 04:46.320
You know, we can robustly adapt much more so than GPT-4.

04:46.320 --> 04:48.320
You and I have the same chair.

04:48.320 --> 04:50.320
We have the same chair, I think.

04:50.320 --> 04:51.320
Oh, yeah.

04:51.320 --> 04:52.320
I can't see your chair.

04:52.320 --> 04:53.320
Yeah.

04:53.960 --> 04:54.960
Me too.

04:54.960 --> 04:55.960
They're home in Miller.

04:55.960 --> 04:58.600
Yeah, I think they're all the same chair.

04:58.600 --> 04:59.600
Yeah.

04:59.600 --> 05:00.600
Yeah.

05:00.600 --> 05:01.600
Excellent chair.

05:01.600 --> 05:02.600
Yep, chair buddies.

05:02.600 --> 05:03.600
Yeah.

05:03.600 --> 05:10.840
I felt it would be hundreds of years before anything even remotely like a human mind would

05:10.840 --> 05:17.000
be asymptotically approaching the level of the human mind, but from beneath.

05:17.000 --> 05:25.680
I never imagined that computers would rival or let alone surpass human intelligence, but

05:25.680 --> 05:29.320
it seemed to me like it was a goal that was so far away.

05:29.320 --> 05:35.040
I wasn't worried about it, but when certain systems started appearing and then this started

05:35.040 --> 05:41.400
happening at an accelerating pace, it felt as if not only are my belief systems collapsing,

05:41.400 --> 05:50.560
but it feels as if the entire human race is going to be eclipsed and left in the dust.

05:50.560 --> 05:55.160
Douglas Hofstadter, he came out as a doomer.

05:55.160 --> 05:59.320
Well, I don't know if he came out exactly.

05:59.320 --> 06:02.040
He's been a doomer for quite a while.

06:02.040 --> 06:03.520
Oh, go on.

06:03.520 --> 06:05.320
I wasn't aware of that.

06:05.320 --> 06:10.000
Well, I don't, you know, doomer is, you know, there's different kinds of doomers.

06:10.760 --> 06:16.500
In my AI book, the first chapter, the prologue is called Terrified, and it's all about how

06:16.500 --> 06:24.720
Doug is very terrified about AI and the possible things that are going to come.

06:24.720 --> 06:34.600
That was based on a talk he gave in 2013, and earlier than that, he was extremely worried

06:34.600 --> 06:41.800
about the singularity, the idea of the singularity from Kurzweil, and wrote quite a bit about

06:41.800 --> 06:42.800
that.

06:42.800 --> 06:49.040
So, I feel like that it's not that new, but maybe this is sort of because there's so much

06:49.040 --> 06:54.960
talk about AI, doom, and so on, that this is kind of, people are kind of paying attention

06:54.960 --> 06:55.960
now.

06:55.960 --> 06:59.480
Yeah, I don't know whether I misunderstood something because I read out, you had this

06:59.480 --> 07:05.040
beautiful piece about the Googleplex in Chopin, and he was terrified that cognition might

07:05.040 --> 07:10.560
be disappointingly simple to mechanize, and, you know, surely we couldn't replicate the

07:10.560 --> 07:16.120
infinite nuance of the mental state that went into writing that beautiful music.

07:16.120 --> 07:20.040
But so maybe he was worried about it, but he didn't think it was possible in principle

07:20.040 --> 07:21.040
or something.

07:21.040 --> 07:26.960
Well, no, he was quite worried about that it was going to happen sooner than he thought,

07:26.960 --> 07:32.400
and that, you know, his quote that it's AI is going to leave us in the dust.

07:32.400 --> 07:34.640
So that's kind of his flavor of doomer.

07:34.640 --> 07:43.720
I'm not sure he has the same, like, existential worry about things as, like, Stuart Russell

07:43.720 --> 07:44.720
or somebody.

07:44.720 --> 07:45.720
Okay.

07:45.720 --> 07:52.760
So he's not so worried about them necessarily churning us into, you know, fertilizer or

07:52.760 --> 07:56.920
raw materials or something, but just that it's not so specific, I think.

07:57.880 --> 08:02.680
But yeah, yeah, I talk to him about it all the time, and he wavers.

08:04.000 --> 08:08.480
Oh, interesting, because I've heard you define yourself as a centrist on other podcasts,

08:08.480 --> 08:15.320
because I'm sure the doomers would lump you in with Cholet and Lacune, maybe, and some

08:15.320 --> 08:21.000
of the critics, but you do think that these models are intelligent, right?

08:21.960 --> 08:23.560
I do think that they're intelligent.

08:23.640 --> 08:29.400
Well, you know, intelligence is an ill-defined notion.

08:30.040 --> 08:30.520
Oh, yeah.

08:30.520 --> 08:36.040
It's multidimensional, and, you know, I don't know if we can say yes or no about something

08:36.040 --> 08:41.680
being intelligent rather than, you know, intelligent in certain ways or to certain degrees.

08:42.640 --> 08:43.000
Yeah.

08:43.000 --> 08:45.280
Well, we've got so much to get into.

08:45.280 --> 08:52.440
I mean, I think slowly we'll talk about arc and your concept arc work, but I kind of agree

08:52.440 --> 08:56.600
with you that, and actually you had that paper out about the four fallacies, and you spoke

08:56.600 --> 09:03.400
about this fallacy of pure intelligence, and I kind of agree that the gnarly reality is far

09:03.400 --> 09:04.360
more complex than that.

09:04.360 --> 09:09.800
There was a really interesting paper that you linked on, no, it was an article by Dileep George,

09:09.800 --> 09:13.960
and he said that a university professor has a much better understanding of a vector,

09:14.520 --> 09:20.280
because it's just grounded in so many real-world situations and contexts and so on,

09:20.280 --> 09:24.680
and an undergraduate or, indeed, a language model would have a very ungrounded, very kind

09:24.680 --> 09:31.800
of low-resolution idea of what this concept is, and it kind of leans away from this puritanical,

09:31.800 --> 09:37.240
ungrounded, abstract form of intelligence to something which is really very complex and intermingled.

09:37.960 --> 09:38.360
Yeah.

09:38.360 --> 09:39.560
I mean, I agree with that.

09:41.080 --> 09:45.160
Well, except that there's another aspect to that, too, which you write about, which is,

09:45.960 --> 09:49.720
I agree that that happens, but what the human mind also seems to do is,

09:50.360 --> 09:56.920
as the thing becomes more grounded in more cases, then we develop yet another concept

09:56.920 --> 10:03.480
that kind of describes the similar aspects that we see throughout all those different

10:03.480 --> 10:04.280
concepts, right?

10:04.280 --> 10:08.920
So we're kind of this iterative loop where we're always finding more and more context,

10:08.920 --> 10:14.520
and then we're also finding newer and newer concepts that span those increasing contexts.

10:14.520 --> 10:15.320
Is that fair?

10:15.320 --> 10:16.280
Yeah, sure, yeah.

10:17.000 --> 10:21.080
I mean, that kind of goes along with the whole sort of

10:23.160 --> 10:31.000
metaphor theory of cognition, of Lake Offit at all, and that we're sort of building on these

10:31.000 --> 10:34.440
physical metaphors, and we can build up many, many layers of abstraction.

10:36.760 --> 10:38.280
So, yeah, we can talk about that.

10:39.800 --> 10:40.920
We're not recording yet, right?

10:41.560 --> 10:42.280
Oh, no, we are.

10:42.280 --> 10:42.520
We are.

10:42.520 --> 10:43.240
This is all recording.

10:43.240 --> 10:43.800
Oh, we are?

10:44.040 --> 10:45.800
I hope you're okay with what you said so far.

10:45.800 --> 10:51.960
But yeah, so there's the Lake Off building on the body of symbols as pointers.

10:51.960 --> 10:55.400
And by the way, that Dileep George article was really fascinating because it was saying

10:55.400 --> 10:57.560
that language is a conditioning force.

10:57.560 --> 11:02.360
So actually, we all have these high-resolution world simulators built into us, and we kind of

11:03.000 --> 11:07.960
condition how that operates and generate counterfactuals through language,

11:07.960 --> 11:09.240
which I thought was quite interesting.

11:10.120 --> 11:10.440
Yeah.

11:11.400 --> 11:11.880
Yeah.

11:11.880 --> 11:15.000
But, Tim, why don't you frame up the debate?

11:15.000 --> 11:16.440
Because we found a beautiful paragraph.

11:17.080 --> 11:17.480
We did.

11:17.480 --> 11:17.720
We did.

11:17.720 --> 11:19.640
We found an amazing bit.

11:19.640 --> 11:20.440
Yeah.

11:20.440 --> 11:24.200
But just to close the loop on what I was saying, we were discussing an activism last night.

11:24.200 --> 11:27.560
I'm not sure if you're familiar with some of these externalized forms of cognition,

11:27.560 --> 11:30.200
and we were talking about the concept of a goal.

11:30.760 --> 11:35.480
And agents, of course, they just have these high-resolution belief trajectories of,

11:36.040 --> 11:37.880
you know, I can do all of these different actions.

11:37.880 --> 11:39.320
And that's not really a goal.

11:39.320 --> 11:44.520
You know, a goal is this very abstract thing which emerges at the system level,

11:44.520 --> 11:47.880
and no individual agents in the system have a concept of a goal.

11:47.880 --> 11:51.000
And it might be similar with some of these concepts that we're talking about,

11:51.000 --> 11:54.600
which is, to what extent do they exist, and to what extent are they just

11:54.600 --> 11:58.680
something intelligible that we can point to, but they don't really meaningfully exist

11:58.680 --> 12:00.200
in the system at a high-resolution.

12:01.480 --> 12:06.200
Are you talking about an AI or in people, a little piece here?

12:06.200 --> 12:06.760
All of the above.

12:07.720 --> 12:10.840
I mean, I think goal is a wonderful example, because we think of it.

12:10.840 --> 12:13.560
I mean, it's even one of Spelke's core priors.

12:13.560 --> 12:15.400
It seems like something so primitive.

12:16.520 --> 12:20.440
But I don't think they really do exist in us.

12:20.440 --> 12:25.560
I mean, we're interesting because we have this reflexive conception of a goal,

12:25.560 --> 12:28.120
but does a mouse have a goal?

12:30.520 --> 12:36.680
Right. I mean, goal is another one of those words that, you know, we use in a very fluid

12:36.680 --> 12:41.160
way. So we talk about, for instance, a reinforcement learning agent having a goal

12:41.880 --> 12:48.120
that we've given to it, right? Or it might have a goal to kind of maximize its information gain

12:48.120 --> 12:56.440
or something. But is that the same thing as a human having a goal that it's like,

12:56.440 --> 13:04.760
you know, to graduate from college or to, you know, make something of your life for

13:04.760 --> 13:08.840
all of these things? It's a very different sense of goal.

13:08.840 --> 13:17.960
And so I would say, yes, a mouse has goals, but those goals are different in degree and in kind

13:17.960 --> 13:23.240
of qualitatively than many of the things we call goals in humans and in machines.

13:23.240 --> 13:29.720
So I think goal is one of those sort of anthropomorphizing words that we need to

13:29.800 --> 13:36.600
be careful about when we equate goals in these different systems as being the same thing.

13:37.160 --> 13:41.480
And I actually, you know, had a discussion with, I think it was with Stuart Russell

13:42.760 --> 13:49.960
about the notion of goal. And his view, and I think this is a view of many other people

13:50.840 --> 13:57.880
in AI, is that large language models actually have goals, complex goals,

13:58.440 --> 14:09.000
that they, that emerge out of this, you know, their loss function of predicting the next token,

14:10.120 --> 14:16.440
because the only way to successfully predict the next token in human language is to develop

14:16.440 --> 14:22.120
human-like goals. I find that dubious, but it's an interesting perspective.

14:22.360 --> 14:28.760
Yeah, I'm amenable to it, because there's always this dichotomy, as you say, of there's the objective,

14:28.760 --> 14:32.840
there's perplexity, and there's these emergent goals, and there's even this simulator's theory

14:32.840 --> 14:38.040
of large language models, which is that they're a superposition of agents. And it's quite situated

14:38.040 --> 14:43.240
as well, because goals kind of materialize depending on your perspective. So if you use a

14:43.240 --> 14:47.320
language model in a certain way from a certain perspective, it might appear that there is some

14:47.320 --> 14:52.200
kind of goal there, but of course, it's just an aspect onto something which is very complex.

14:52.760 --> 14:56.760
But I think we should frame up this beautiful piece, actually, from your

14:57.400 --> 15:01.560
Modes of Understanding paper from much this year. I always call it the Modes of Understanding paper.

15:01.560 --> 15:06.040
It was actually titled The Debate over Understanding in AI's Large Language Models.

15:06.040 --> 15:11.480
And you said, towards the end, that the key question of the debate about understanding in

15:11.560 --> 15:18.040
large language models is, one, is talking of understanding in such systems simply a category

15:18.040 --> 15:22.920
error, which is mistaking associations between language tokens for associations between

15:23.560 --> 15:30.200
tokens and physical, social, and mental experience? In short, is it the case that these models are

15:30.200 --> 15:36.760
not and will never be the kind of things that can understand, or conversely, to do these systems or

15:36.760 --> 15:41.720
their near term successes? Actually, even in the absence of physical experience, create something

15:41.720 --> 15:46.520
like the rich concept based mental models that are central to human understanding. And if so,

15:46.520 --> 15:51.800
does scaling these models create even better concepts? Or three, if these systems do not

15:51.800 --> 15:57.320
create such concepts, can their unimaginably large systems of statistical correlations,

15:57.320 --> 16:02.760
produce abilities that are fundamentally equivalent to human understanding, or indeed that enable

16:02.760 --> 16:08.120
new forms of higher order logic that humans are incapable of accessing? And at this point,

16:08.120 --> 16:15.560
will it still make sense to call such correlations spurious and the resulting solutions shortcuts?

16:15.560 --> 16:21.400
And would it make sense to see these systems' behaviors not as competence without comprehension,

16:21.400 --> 16:26.280
but as a new, non-human form of understanding? And you said that these questions are no longer

16:26.280 --> 16:31.720
in the realm of abstract philosophical discussions, but they touch on very real concerns about the

16:31.720 --> 16:37.960
capabilities and robustness and safety and ethics of AI systems. So let's use that as a leader.

16:37.960 --> 16:41.480
What do you think, Melanie? It's beautiful. That was a beautiful paragraph, by the way.

16:41.480 --> 16:45.720
Yeah, it's so good. Wow. This exactly crystallizes the discussion.

16:48.280 --> 16:55.400
Yeah, I think that it's something that we in AI are all grappling with now. And I think it's

16:55.400 --> 17:02.920
something that the history of AI has forced us to grapple with mental terms like understand,

17:03.720 --> 17:13.080
or consciousness, and even intelligence. Because we keep saying, oh, well, understanding, if you

17:13.080 --> 17:23.080
can do X, then that means that you're actually understanding. You can't do language translation

17:23.080 --> 17:30.280
without understanding. You can't do speech to text without understanding. You can't generate

17:30.280 --> 17:39.720
articulate natural language without understanding. And I think this is, in many cases, we then step

17:39.720 --> 17:44.360
back and say, wait, that's not what we meant by understanding. It turns out you can do all these

17:44.360 --> 17:49.080
things without understanding. So we're sort of saying, well, we didn't really know what we meant

17:49.080 --> 17:56.600
by the term understanding, I think. And often, some people criticize that as moving the goalposts.

17:58.200 --> 18:00.520
You're moving the goalposts. The so-called AI effect, right?

18:00.520 --> 18:02.200
Right. It's the AI effect.

18:02.200 --> 18:11.800
But I think of it more as AI is forcing people to really refine their notions

18:12.680 --> 18:17.160
of that that have been quite fuzzy about what these terms actually mean.

18:18.200 --> 18:25.320
And there was a fantastic talk by Dave Chalmers, the philosopher, who I think you've probably had

18:25.320 --> 18:33.640
on this show, where he talks about conceptual engineering, which is something that philosophers

18:33.640 --> 18:39.240
do where they take a term like understanding and they refine it. And he said, okay, well, we have

18:40.200 --> 18:48.760
p-understanding, which is like personal, phenomenological. And then we have c-understanding

18:48.760 --> 18:54.520
and e-understanding and x-understanding and all these different letters that meant to say that

18:54.520 --> 19:01.480
this term is not a unified thing that we can apply to a system. We have to really specify what we

19:01.480 --> 19:08.040
mean exactly. Well, one way I've come to think about it, and it's largely from reading your work

19:08.040 --> 19:12.760
and your assessments about it, is that for the first time, we're actually being forced to do

19:13.720 --> 19:20.920
the science of machine cognition, right? Because for too long, it's either just not been sophisticated

19:20.920 --> 19:26.840
enough. Why bother? Like it's obviously not doing any cognition. And as you point out, it's now

19:26.840 --> 19:32.520
actually having real world impacts. And so we actually have to start doing the science, right?

19:32.520 --> 19:37.640
We have to say, okay, does this thing have cognition? Here's a hypothesis. Let's do some

19:37.640 --> 19:42.360
test. Okay, it failed. What was the failure mode? Why did it fail? Let's understand that more. How

19:42.360 --> 19:47.880
can we engineer it not to fail? It's like we can no longer ignore adversarial examples,

19:47.880 --> 19:51.800
shortcut learning, et cetera. We have to finally grapple with it, it seems to me.

19:52.680 --> 19:57.080
Yeah, I think that's exactly right. And what's interesting is we, computer scientists, were

19:57.080 --> 20:05.480
never trained in experimental methods. We never learned about like controls and confounding things.

20:05.480 --> 20:18.840
It's a great point. And so now people are doing, applying human tests of understanding or intelligence

20:18.840 --> 20:26.520
or reasoning, what have you, to machines without having the right experimental methods to say whether

20:26.520 --> 20:33.480
or not what they're testing is actually valid. So there's a cognitive scientist named Michael

20:33.480 --> 20:38.760
Frank at Stanford, who's been writing a lot of stuff about experimental method and how do you

20:38.760 --> 20:48.120
apply it to AI and why you need sort of expertise in this area to really make sense of these systems.

20:48.120 --> 20:54.120
And I'm totally on board with that. Yeah, we'll talk about your piece with Tannenbaum later,

20:54.120 --> 20:59.240
but as you say, a lot of AI folks don't really think about experiment design. But actually,

20:59.240 --> 21:03.480
even with Chalet's ARC challenge, maybe we should talk about that. So he invented this

21:03.480 --> 21:08.920
measure of intelligence, which unfortunately was not computable, but it was mathematically

21:08.920 --> 21:14.760
beautiful. Basically saying that, and he's a huge Spelke fan, I kind of put Chalet very,

21:14.760 --> 21:20.680
very close to you actually in AI researcher space. And his measure of intelligence basically says,

21:21.320 --> 21:26.360
I give you priors, I give you experience, you give me a skill program, it extrapolates into these

21:26.360 --> 21:31.160
different spaces and experience space. And the kind of the conversion ratio between those

21:31.160 --> 21:37.000
priors and experience and the space that I get is intelligence. And that's very interesting.

21:37.000 --> 21:41.880
And then he produced this corpus, this ARC challenge. And it's a bit like an IQ test. It's

21:41.880 --> 21:49.000
this kind of 2D gridded colored cells. And you have a couple of examples, and you have to do

21:49.000 --> 21:53.880
another one or two examples. And it was very diverse because it was testing what he called

21:53.880 --> 21:59.400
developer aware generalization. And there were a couple of issues with that. So you wrote this

21:59.400 --> 22:05.320
beautiful concept ARC paper, and maybe you can introduce that. But one of the things you pointed

22:05.320 --> 22:11.880
out, which I felt was quite interesting is that even if people succeeded on Francois's challenge,

22:11.880 --> 22:16.760
it wouldn't necessarily be what we would call intelligence, because it's not necessarily

22:16.760 --> 22:22.120
demonstrating systematic generalization beyond those one or two examples in his test set.

22:22.600 --> 22:30.920
So our motivation was twofold. So first of all, I love the ARC challenge. It's beautiful.

22:31.880 --> 22:43.320
It's super elegant. And I'm very sympathetic with Francois' definition of intelligence,

22:43.320 --> 22:49.080
although I think there's probably, again, intelligence is very multi-dimensional. But

22:49.160 --> 22:55.400
this is one aspect of it for sure. And his problems are great because they

22:56.200 --> 23:02.600
just give a few examples, and people are pretty good at abstracting a rule or a concept from

23:02.600 --> 23:11.000
just a few examples. And they don't use language, so they don't get into the whole prior knowledge

23:11.000 --> 23:21.720
of language and a lot of things that you don't want to confound these tests. But one of the

23:21.720 --> 23:26.600
problems with ARC is that many of the problems are quite hard. They're quite hard for people.

23:27.320 --> 23:35.720
And they're so hard that they don't really differentiate between different programs that

23:35.720 --> 23:42.120
are attempting to solve this challenge. So there was a Kaggle competition with the ARC challenge,

23:42.120 --> 23:50.360
and there were two, the two best programs got about, they each got about 20% accuracy on the

23:50.360 --> 24:00.360
hidden test set. So it didn't really distinguish them at all. And the other problem was that,

24:01.320 --> 24:06.840
as you mentioned, the test wasn't very systematic, meaning that let's say there's a

24:07.640 --> 24:13.880
problem in ARC that deals with the concept of inside, something being inside something else.

24:13.880 --> 24:19.800
And let's say that something, a program gets that one right. Does that mean that it understands

24:19.800 --> 24:26.520
the concept of inside in a general way? Well, we don't know because the test doesn't test that

24:26.520 --> 24:34.680
systematically. And that was actually intentional from Sholey, because he didn't want any way to,

24:35.400 --> 24:42.920
for programs to be able to reverse engineer the generation process of these problems.

24:42.920 --> 24:48.600
So if you say, oh, well, I'm going to deal with these 10 concepts, then somebody presumably

24:48.600 --> 24:56.440
could reverse engineer those, the problems and not be general. But for us, we wanted to say, well,

24:56.440 --> 25:02.200
how would you just systematically test a program for understanding of a concept of a very basic,

25:02.200 --> 25:08.440
spatial or semantic concept? And so what we did was we took the ARC domain and we created

25:09.560 --> 25:17.320
about almost 500 new problems that were systematically grouped into concept groups.

25:17.320 --> 25:24.440
So like inside of, that was one of the groups. And so we looked at, we created several problems

25:24.440 --> 25:31.560
that were variations on that concept. And there were variations that ranged in like abstraction,

25:31.560 --> 25:41.080
degree of abstraction, and sort of complexity of the problem. And the hypothesis was that if a

25:41.720 --> 25:49.880
human or a program could successfully solve the problems in a given concept group, they really

25:49.880 --> 25:57.080
do have a good sort of grasp of that concept. So this was the genesis of concept ARC.

25:58.200 --> 26:03.160
You know, it's fascinating because so you're, you're attempting again to build the science of

26:03.960 --> 26:08.600
machine cognitive science, essentially. And hey, it has to be systematized, we need to have these

26:08.600 --> 26:14.280
concept categories, we need to be able to generate examples of progressive complexity and, you know,

26:14.280 --> 26:19.080
layer of abstraction, everything. And then yet you mentioned Chalet intentionally didn't

26:19.080 --> 26:22.840
systematize it to avoid reverse engineering. And that's kind of a fascinating

26:23.480 --> 26:29.480
point because reverse engineering can even happen, you know, just by way of selection bias. So I mean,

26:29.480 --> 26:33.960
researchers are out there, they're fooling around with different neural network structures, maybe

26:33.960 --> 26:39.160
I'll add like a you here or some horseshoe over there. And lo and behold, suddenly, it works

26:39.160 --> 26:44.440
really well on the concept of inside out. And I'm going to claim this is machine learning,

26:44.440 --> 26:49.320
even though it was actually human engineering that sort of put that structure into the network.

26:49.320 --> 26:54.440
So in the long term, you know, how do we, how do we balance that? Or how do we avoid it? Or how do

26:54.440 --> 27:00.520
we test for machine induced, you know, prior knowledge versus actual machine learning?

27:01.480 --> 27:10.440
Yeah, no, I understand it's a hard problem. And I think, you know, the goal with this concept arc

27:11.720 --> 27:17.560
benchmark wasn't to sort of supplant arc in any way, it was really meant to be complementary.

27:18.200 --> 27:27.480
And it was meant to be kind of a stepping stone to the much larger and more difficult arc set.

27:28.040 --> 27:32.760
Because I think, you know, even if I tell you all of these problems have to do with

27:32.760 --> 27:41.240
the concept of inside versus outside, you would still have to have a good grasp of those concepts

27:41.240 --> 27:48.600
in order to solve these problems. And I'm not sure that you could sort of engineer something

27:49.160 --> 27:57.320
that would solve those cons problems of that concept in general, without having a more,

27:57.320 --> 28:01.480
you know, really a general understanding in some sense of those that concept.

28:02.120 --> 28:10.520
But to Keith's your point, I think having a static benchmark is a problem, sort of putting out a

28:10.520 --> 28:17.640
benchmark that everybody can kind of try and optimize their program to solve. We've seen

28:17.640 --> 28:27.800
that over and over again. That ends up being sort of a way that people end up reverse engineering

28:28.680 --> 28:36.120
to a particular task rather than to a more general set of conceptual understanding. So

28:36.120 --> 28:42.360
I do think that we have to keep changing our benchmarks. We can't just say, okay, here's image

28:42.360 --> 28:49.160
net, go, you know, beat on that for the next 20 years until you've solved it.

28:49.720 --> 28:52.760
That's not going to yield general intelligence.

28:53.880 --> 28:58.520
Yeah, I think one of the issues we're talking about in general is extrapolation. So, you know,

28:58.520 --> 29:04.920
Sholey used extrapolation to talk about skill programs and being able to do things beyond

29:04.920 --> 29:11.000
your priors and experience. But with benchmarks, it's about human extrapolation. So I think part

29:11.080 --> 29:15.560
of the problem with the risk debate, by the way, why everyone's so suddenly worried about risk is

29:15.560 --> 29:21.560
because of this benchmark problem. And that's because we see that humans who can do A can do

29:21.560 --> 29:27.800
B. And now we see machines that can do A. And we have all of these built-in assumptions in

29:27.800 --> 29:32.360
benchmarks. And we don't really realize that we're talking about machines now. We're not talking

29:32.360 --> 29:38.440
about computers anymore. And I think it's causing a real problem. I don't want to be hyperbolic here,

29:38.440 --> 29:44.280
but it feels like there's this massive delusion taking over the entire machine learning community.

29:44.280 --> 29:49.560
And we're seriously talking about AI risk. And I think it all comes down to these

29:49.560 --> 29:57.160
benchmarks fundamentally. Yeah, I do think all of our benchmarks have, as you say,

29:57.160 --> 30:05.320
have this problem of that they have assumptions built in that if a human could do this, that

30:05.320 --> 30:09.720
then the machine must, if the machine does it, it has the same kind of

30:12.040 --> 30:17.880
generalization capacity as a human who could solve that problem. This goes back all the way to say

30:19.640 --> 30:27.480
chess as a benchmark. So people used to think that if, because if a human can play chess at a

30:27.480 --> 30:34.280
grandmaster level, that means they must be super intelligent in other ways, that if a machine

30:34.280 --> 30:40.920
could play chess at that level, it would also be super intelligent like a human. Herbert Simon

30:40.920 --> 30:48.120
even said that explicitly. But then we saw that chess actually could be conquered by very

30:48.120 --> 30:57.800
unintelligent brute force search that didn't generalize in any way. So I think this is an issue

30:57.800 --> 31:05.480
today with large language models. They can do things like pass the bar exam and pass other

31:06.120 --> 31:14.360
standardized human tests of skill or intelligence. But what does that mean? It doesn't necessarily

31:14.360 --> 31:18.120
mean the same thing for a machine as it does for a human for many different reasons.

31:19.320 --> 31:23.800
Yeah, I guess it's a similar thing with the McCorduck effect that we have relative pointers

31:23.800 --> 31:29.240
to what we think of as being intelligence. We just point to something and then when that thing

31:30.120 --> 31:37.480
becomes easy, then we need to kind of move the pointer. Yeah, I think it also feeds into, as

31:37.480 --> 31:43.560
Tim was saying, I think it heightens the fear of existential risk because of this, this concept

31:43.560 --> 31:50.120
that we have of intelligence always wins, which even among humans is, is a flawed concept, right?

31:50.120 --> 31:54.360
I mean, you know, many nerds who grew up through elementary school can tell you like

31:54.360 --> 32:00.280
intelligence doesn't always win, right? Like sometimes it's numbers or brute force or

32:00.280 --> 32:04.520
whatever else kind of kind of wins. And they assume like, well, if we were to have this

32:04.520 --> 32:10.600
purified intelligence that was super intelligence, it would be as if a human brain were super

32:10.600 --> 32:15.880
intelligent and they'd be able to do everything a human being could do and hurt other people and

32:15.880 --> 32:21.640
conquer the world and fight wars. And that again, is this anthropomorphic projection, right?

32:22.440 --> 32:30.280
Yeah, I mean, right. So, and it's this notion that intelligence is this thing that you can just

32:30.280 --> 32:37.000
have more and more of. Forever. Forever. Or so far that it's just beyond any, you know,

32:37.000 --> 32:42.040
it's almost magical, right? And it's capable. Right. And it's not, you know, a different

32:42.040 --> 32:49.240
view of intelligence is that it's a collection of adaptations to specific problems for a particular

32:49.240 --> 33:00.200
kind of organism in an environment. And it's not the sort of an open-ended, pure domain

33:00.200 --> 33:06.920
independent thing. So, I think this is why, you know, you see a lot of discussion of super

33:07.000 --> 33:16.680
intelligence, AGI, you know, AI risk in among computer scientists, but you don't see a lot of it

33:17.240 --> 33:24.440
discussed among like psychologists or animal intelligence people or other cognitive scientists.

33:25.400 --> 33:28.120
Because that's not the way that they understand intelligence.

33:29.240 --> 33:33.160
I would love to explore more about that because, I mean, only yesterday when we were talking about

33:33.240 --> 33:39.560
an activism, we're also talking about Gibson's ecological psychology. And even Elizabeth Spelke,

33:39.560 --> 33:44.520
I mean, this kind of cognitive psychology view is very related to nativism. It's this idea that we

33:44.520 --> 33:50.200
have these fundamental cognitive primitives and intelligence in some sense is just traversing

33:50.200 --> 33:56.120
or recomposing this library of cognitive modules that we have. And those modules are very physically

33:56.120 --> 34:01.560
situated, you know, they tell you something about the environment that you're in. Which means that

34:01.640 --> 34:07.000
intelligence is just very gnarly and it's very kind of coupled to the environment we're in. It

34:07.000 --> 34:11.560
can't really be magically abstracted in a computer with infinite scale.

34:12.760 --> 34:18.360
Yeah, I think that's right. That's, you know, people have different views about the nativism,

34:19.880 --> 34:26.040
empiricism, debate. And there's whole different schools and cognitive science about like how

34:26.760 --> 34:32.840
how much is learned, how much is evolutionarily built in and all of that. But I think most people

34:33.400 --> 34:43.800
in the field would agree with what you said that intelligence is very gnarly. It is situated, it

34:43.800 --> 34:54.280
is specific to particular domains of concern to a particular organism, and that it's not easily

34:54.280 --> 34:57.960
abstractable. You know, that back in the early days of AI we had

35:00.520 --> 35:06.360
Newell and Simon, two of the pioneers of AI who had this thing called the physical symbol system

35:06.360 --> 35:13.800
hypothesis, which was that basically you could sift off intelligence from any material substrate

35:13.800 --> 35:18.840
like the brain and put it in some other material substrate like a computer. They were thinking

35:18.840 --> 35:27.160
about symbols, but nowadays people have the same kind of view with neural nets or

35:28.600 --> 35:34.920
transformers or whatever, that you can take human intelligence that's very situated and

35:35.800 --> 35:41.720
tied to the environment and sort of sift off the pure part and leave all of that bodily stuff

35:42.360 --> 35:46.520
and you can get something like superintelligence. And I don't think most people in cognitive

35:46.520 --> 35:52.040
science would agree with that. Well, on the other hand though, I think, and I'd be curious to get

35:52.040 --> 35:57.480
your take on this, is one direction that that comes from is for those of us, and I include

35:57.480 --> 36:03.480
myself in this camp tentatively, that at the end of the day what the brain does is some form of

36:03.480 --> 36:08.120
computation. You know, like absence, the proof that there's such a thing as hypercomputation,

36:08.120 --> 36:14.040
like our brain, all of its calculations could be embodied in a large enough

36:14.600 --> 36:19.000
you know, Turing machine and a large enough computer of some kind. And therefore, everything

36:19.000 --> 36:27.160
that we do, including our intelligent activities, could be coded somehow or another into a Turing

36:27.160 --> 36:31.800
equivalent system. And for the record, I don't believe neural networks are. I've said this like

36:31.800 --> 36:35.960
multiple times, at least in their current manifestations, they're not, they're just a

36:35.960 --> 36:40.120
feed forward, you know, thing at the end of the day. But if you actually had a computer, you could

36:40.120 --> 36:47.160
have human symbolic intelligence encoded. Like, where do you stand on that, on that debate, if you

36:47.160 --> 36:56.200
will? Yeah, I have nothing against the idea that the brain does computations. I think that's,

36:57.080 --> 37:05.480
that's, you know, one possible way to look at it. And that those kinds of computations could be

37:05.480 --> 37:10.600
implemented in another kind of computer. But the brain is a very special kind of

37:10.600 --> 37:15.480
sort of biological computer that's been evolved to do specific things. And one of the main things

37:15.480 --> 37:21.160
the brain has been evolved to do is control the body, and in particular kinds of environments.

37:21.800 --> 37:28.680
And so I think the brain is doing computations, but it's doing very, very highly evolved, very

37:28.680 --> 37:39.480
domain specific computations that perhaps don't necessarily make sense without having a body.

37:41.640 --> 37:52.600
Now, that's debatable. But it does seem like a lot of the way that we reason is by reference to our own

37:52.760 --> 37:57.400
sort of episodic experience in the world.

37:59.160 --> 38:04.760
Or at least to the capabilities that have been built into us, you know, like visual, using our

38:04.760 --> 38:11.240
visual cortex to imagine cubes and steers and whatever else we need to solve a physics problem

38:11.240 --> 38:17.480
or a geometric problem. Sure, sure. Yeah, so I'm fine with saying the brain is a computer of a certain

38:17.480 --> 38:28.840
kind, but that's not to say that it's going to be, you can just kind of lift off the computations

38:31.080 --> 38:37.640
and then put them in a different substrate and kind of get everything that's human like,

38:37.640 --> 38:42.520
because I'm not sure that those computations are going to make sense in the absence of the rest

38:43.080 --> 38:49.720
of the organism. Yeah, there was something that always confused me about the autopoietic

38:49.720 --> 38:56.280
inactivists, because of course they as they issue representationalism and information

38:56.280 --> 39:01.640
processing, but they also issue computationalism in general. And as Keith was just saying, I don't

39:01.640 --> 39:06.280
even if cognition is externalized, I don't see any reason why in principle, you couldn't just

39:06.280 --> 39:11.720
compute the entire system and and recreate the computation. I just wanted to close the loop on

39:11.720 --> 39:18.600
the ARC challenge stuff though. So you said that the winning solutions to Francois' challenge on

39:18.600 --> 39:23.320
Kaggle, they were quite simplistic in a way. They were like a genetic search over lots of

39:23.320 --> 39:28.600
primitive kind of functions. And even the winner said that they didn't feel it was a satisfying

39:28.600 --> 39:33.480
solution, which was interesting. And then you tried it on GPT4. And I think you said you got

39:33.480 --> 39:39.960
around 30%. There's now a deep mind paper out very recently, which just basically turned it all into

39:40.040 --> 39:45.720
a character set with a random mapping, put it into GPT4, I think got nearly 60%. Even

39:45.720 --> 39:51.560
even somewhat invariant to the translation between the character set mapping. Some folks on our

39:51.560 --> 39:55.800
Discord forum tried to reproduce it and couldn't. That's the problem with GPT4. You can never

39:55.800 --> 40:02.600
reproduce anything. But I was just wondering, would you consider that to be an elegant solution?

40:02.600 --> 40:06.120
It's not really much better than searching over a DSL, is it?

40:06.840 --> 40:09.240
By that, you mean giving it to GPT4?

40:10.280 --> 40:15.400
Well, I mean, it's quite an interesting thing, isn't it? If there's the McCorduck effect,

40:15.400 --> 40:21.320
and even before you get to a solution, what would a good AI solution look like? I mean,

40:21.320 --> 40:25.800
what would someone have to create for you to say, oh, that's a really cool AI solution?

40:25.880 --> 40:36.360
Well, if you had a program that really could solve these tasks in a general way,

40:36.360 --> 40:43.000
that would, however it worked, it would be a good AI solution. I don't necessarily think we have to

40:43.000 --> 40:49.880
have something like the way people do it. Well, let me see if I can guess, though,

40:49.880 --> 40:55.480
maybe an extension to what you said. It's in line with your argument that the benchmarks

40:55.480 --> 41:00.520
have to evolve. Because I think that these benchmarks really is just first pass or low

41:00.520 --> 41:05.320
pass filters. It's like they weed out the junk. It's like, well, if you can't pass the art challenge,

41:05.320 --> 41:09.240
I'm not going to bother with you. If you pass the art challenge, now we have to look further,

41:09.240 --> 41:15.000
right? Which is like, okay, so it's been able to generalize along these 19 concepts that we've

41:15.000 --> 41:22.520
defined in concept art with little pixel grids. What about if we give it full frame pictures

41:22.520 --> 41:28.280
or video or something? Is it able to generalize there? No, okay, it failed. Why did it fail?

41:28.280 --> 41:32.360
Well, now we need to do some more engineering. It's going to be this kind of never ending sort

41:32.360 --> 41:38.280
of iterative process. So I would say if something passes arc or concept arc, then it's worthy of

41:38.280 --> 41:46.280
further study. Sure. Yeah, I agree. I mean, one question is that arcs are very idealized kind

41:46.360 --> 41:53.880
of micro world type domain. So does it capture what's interesting about the real world

41:54.600 --> 42:01.000
in terms of abstraction? To some extent, yes, probably, and to some extent, probably no.

42:01.000 --> 42:08.680
So you're right. Solving arc doesn't mean we're at AGI, if you want to talk about that.

42:08.680 --> 42:15.240
It's like in chess, what you brought up earlier. If you took whatever the current best,

42:15.240 --> 42:19.960
let's say LC zero or something like that, and it's been trained on standard chess,

42:19.960 --> 42:25.400
and then you have a go play chess 960, formerly called Fisher random, where you just random,

42:25.400 --> 42:30.440
it's going to suck like humans are going to destroy it, right? Because humans have learned

42:30.440 --> 42:37.160
a more generalized and by the way, that also destroys human beings who rely on memory and

42:37.160 --> 42:42.120
just sort of like the memorized positions that haven't learned, let's say the skill

42:42.120 --> 42:46.440
of playing chess, right? And so this is the type of thing that's going to happen, right?

42:46.440 --> 42:50.920
It's like you say, when you take this intelligence and try to apply it to a different context,

42:51.560 --> 42:55.320
that's when the rubber meets the road as to whether or not you really learned

42:55.320 --> 43:00.520
the concepts, right? Yeah, no, definitely. I agree. And I don't think like our concept arc

43:00.520 --> 43:07.080
wasn't meant to be like a test of AGI in any sense. It was meant to be kind of a stepping stone to

43:07.080 --> 43:14.600
getting to abilities for abstraction. And clearly, if some program was able to solve all of the

43:14.600 --> 43:21.080
problems in that domain, and we'd have to then test further, we'd have to have it be able to

43:21.080 --> 43:26.760
extrapolate to a new kind of domain that tested the same kinds of concepts. So you're right,

43:26.760 --> 43:32.600
there's no end in some sense. But at some point, I guess, and I don't know when that point is,

43:32.600 --> 43:37.320
we have to say, well, this thing seems to be understanding this concept.

43:39.960 --> 43:43.720
That's the wonderful continuum, though, because you said earlier, there's something deeply

43:43.720 --> 43:50.040
unsatisfying about chess brute forcing everything. And when we apply Francois' measure of intelligence,

43:50.040 --> 43:56.040
we don't think of that as intelligent because it's just brute force experience. And then we

43:56.040 --> 44:00.040
find something which is a little bit more efficient. So it's something which appears to work. But

44:00.760 --> 44:05.400
now, another interesting thing is when you talk about concepts, you had this beautiful article

44:05.400 --> 44:10.920
out earlier that she had talking about, on top of, she's on top of the world. And what would

44:10.920 --> 44:18.200
Dali draw? It would draw a globe with someone dancing on top of it, or I'm on the TV. What

44:18.200 --> 44:23.560
does that mean? It should mean that I'm actually being rendered on the TV. Now, it's kind of like

44:23.560 --> 44:28.840
what we were saying with goals, isn't it? Because this skill program, someone just goes on Kaggle

44:28.840 --> 44:34.200
and they gives you this program and it seems to work. But it's horribly complicated. And how do

44:34.200 --> 44:39.880
you know that the internal representations are in any way related to these abstractions? And do

44:39.880 --> 44:45.400
you think that the abstractions as well are somehow universal in the same way Spelki would say that

44:45.400 --> 44:52.920
the cognitive priors are? Yeah, I think it's something we can't say. And we don't know with

44:53.000 --> 44:59.720
humans. And we don't know with machines, because both of these are very complex systems that are

44:59.720 --> 45:08.120
hard to kind of pull apart. What are the internal representations? So in most cases, we have to

45:08.120 --> 45:17.880
rely on behavior, which is very noisy. It can be misleading. And it turns out that humans

45:18.680 --> 45:28.120
often are not, if you give them a problem, like a reasoning problem, in a familiar domain,

45:28.840 --> 45:34.040
they're much better at doing that problem as doing the exact same reasoning kind of abstract

45:34.040 --> 45:41.720
reasoning task in an unfamiliar domain. And I think that's something that people have shown

45:41.720 --> 45:50.600
is also true of large language models, because they've learned from human language and have

45:50.600 --> 45:55.960
incorporated sort of the statistics of some of the statistics of human experience that they're

45:55.960 --> 46:01.400
much better on familiar domains than on non-familiar domains. But the one thing that humans can do

46:01.400 --> 46:07.480
is often they can kind of transcend that and learn how to reason much more abstractly,

46:07.800 --> 46:14.760
which I don't know if we will get to that point with language models yet. So there's a wonderful

46:14.760 --> 46:24.760
paper that just came out from a group at MIT and some other places called, I can't remember

46:24.760 --> 46:31.720
what it was called, it was something like reasoning versus reciting. And what they do is

46:32.280 --> 46:39.480
they talk about this notion of a counterfactual task, which is if you can do one task, like

46:39.480 --> 46:44.840
addition and base 10, and you really understand that notion of addition, you should be able to do

46:44.840 --> 46:51.160
addition and base eight. And so, but you haven't had as much experience as like for a language model,

46:51.160 --> 46:59.000
it's not almost all of the training data has to do with base 10. So, but can, so they tested,

46:59.000 --> 47:01.960
they did a whole bunch of these so-called counterfactual tasks

47:03.240 --> 47:09.080
and showed that GPT-4 is really good at the original task, but not so good at the counterfactual task.

47:10.280 --> 47:14.920
So it's not, in some sense, it is relying on sort of patterns in its training data rather than

47:16.200 --> 47:18.360
genuine abstraction.

47:19.880 --> 47:21.560
It's a stochastic parrot, right?

47:22.120 --> 47:25.640
Well, you know, it could be argued that humans do that a lot too.

47:26.600 --> 47:31.320
I don't know if you called a stochastic parrot, but it's more like a pattern matcher.

47:32.520 --> 47:39.560
And it's not, it's not reasoning about the things in the sense that we think of reasoning,

47:39.560 --> 47:46.200
you know, as sort of domain independent ability. It's very domain dependent.

47:49.160 --> 47:52.280
Yeah, so the difference is that I guess the difference I would say is that humans,

47:52.520 --> 48:03.080
it can kind of overcome that domain dependency in some cases and actually get to the true

48:03.080 --> 48:06.680
abstraction, but I don't know that language models can.

48:08.360 --> 48:13.880
Yeah, I mean, there's a couple of things here. So first of all, these language models fail at

48:13.880 --> 48:21.080
things which four-year-old children can do. And they can pass the bar exam, but as you've said

48:21.080 --> 48:24.600
previously, you wouldn't want one of these things to actually go and practice more.

48:24.600 --> 48:31.560
My God, could you imagine the thought? And there was this Sparks of AGI paper where they gave this,

48:31.560 --> 48:35.160
I mean, maybe you could recite this better than me, but there was the thing about the

48:35.160 --> 48:40.440
book Nine Eggs, a laptop, a bottle, and a nail. Can you balance it in a stable manner?

48:40.440 --> 48:45.960
And this comes back to the experiment design because, my God, in any other discipline of science,

48:45.960 --> 48:50.760
they would just tear this apart. They would say, well, that's not very robust. I mean,

48:50.760 --> 48:58.040
you came up with an example with a pudding, a marshmallow, a toothpick. How would it balance it?

49:00.040 --> 49:03.800
Yeah, did it not balance the full glass of water on top of the marshmallow?

49:04.840 --> 49:09.960
Well, it stuck the toothpick into the marshmallow and then that's not exactly what we had in mind.

49:10.520 --> 49:14.440
No, and in fact, the Sparks of AGI paper, they explicitly said,

49:15.400 --> 49:19.320
we're doing anthropology, not cognitive science.

49:20.600 --> 49:24.520
Well, that's not the way it was interpreted. Unfortunately, there are YouTube channels

49:24.520 --> 49:31.160
now dedicated to educating people on AI and they're taking this as gospel. I mean, what's going on?

49:31.880 --> 49:40.120
I think there's just not as much of a focus on sort of scientific method in this field as there

49:40.120 --> 49:49.960
should be. And I think in science, if you're looking at a phenomenon and you're trying to

49:49.960 --> 49:57.240
replicate it, if it only replicates half the time, that's not a replication. That's not a

49:57.240 --> 50:03.720
robust replication. Whereas for language models, people are saying, well, if it can do this task

50:03.720 --> 50:11.720
once in one particular circumstances, then it probably has this more general capability.

50:12.280 --> 50:18.440
So if it can do this stacking problem once, then wow, it has physical common sense.

50:20.760 --> 50:28.520
And people with my marshmallow example, people, of course, jumped on it and said, wait,

50:28.520 --> 50:33.400
if you prompt it in a certain way and you do all this prompt engineering,

50:33.400 --> 50:39.000
human engineering, it does it right. And then like, well, that's not the point.

50:39.720 --> 50:44.760
The point is not any particular example. The point is figuring out how to test things

50:44.760 --> 50:51.400
so that you actually have some kind of robust ability for replicating a capability,

50:52.680 --> 50:57.080
which we haven't seen with experiments on language models very much. I mean, people

50:57.080 --> 51:02.840
are starting to do this. People are starting to do this kind of more scientifically grounded,

51:02.840 --> 51:10.360
experimental method on language models, but it's still not very, there's not very much of it.

51:11.240 --> 51:16.200
So you might appreciate a phrase I recently coined because it covers this leakage too,

51:16.200 --> 51:22.120
of like sort of leakage of human knowledge, which is if you can't find the priors, look in the mirror.

51:23.080 --> 51:27.320
It's like, we have to learn how to do experimental science and computer science,

51:27.320 --> 51:32.280
and you've got to guard against this type of leak at Drillian, human engineering,

51:32.280 --> 51:37.960
and over-involved and whatever. And this is why I really want to collaborate with people in

51:37.960 --> 51:44.680
developmental psychology, with people in animal cognition who face this kind of issue all the

51:44.680 --> 51:55.800
time. And one example was, I got from a developmental psychologist was that sometimes

51:55.800 --> 52:05.480
like a three-year-old can tell you something like four plus three is seven, but if you say,

52:05.480 --> 52:10.040
if you give them a bunch of marbles and say, pick out four of them, they can't do it.

52:10.360 --> 52:17.480
So there you say, okay, that this kid doesn't understand the concept of four,

52:17.480 --> 52:23.640
they're kind of just reciting something that they've heard. And this is the kind of experiments

52:23.640 --> 52:30.040
that people in developmental psychology do all the time to really tease out what the system,

52:30.040 --> 52:36.680
what babies and children know and what they can do. And it's not an easy thing to do in

52:37.400 --> 52:43.800
this kind of experiment. The problem with that is it's extremely complex and requires so much

52:43.800 --> 52:48.600
domain knowledge. So it takes a very long time, because I think there was another article that

52:48.600 --> 52:54.440
spoke about how we study rats. And those folks in different disciplines, they're really,

52:54.440 --> 53:00.120
really good experimental design, and they have experts who kind of create very, very clear

53:00.120 --> 53:06.040
criteria for measuring this behavior. And with AI, everything's going up on archive,

53:06.040 --> 53:09.960
and everything's going a million miles an hour. And by the time you actually design

53:09.960 --> 53:13.720
a systematic rigorous study for the first thing, there's already another paper coming out,

53:13.720 --> 53:17.160
which is claiming to do it differently. So we just can't keep up. It's just,

53:17.160 --> 53:22.440
it's an absolute nightmare. Absolutely. Yeah. Agreed.

53:24.280 --> 53:27.880
I want to just, so I'll quickly touch on one more thing. And I know Keith wants to go into

53:27.880 --> 53:32.520
complexity. But yeah, so the information leakage is a problem. The brittleness is a problem. I do

53:32.520 --> 53:39.080
think of these GPT models a bit like a database. And so anything that requires physical grounding,

53:39.080 --> 53:42.680
of course, doesn't work very well. Some things work surprisingly well, like, you know,

53:42.680 --> 53:46.760
programming, because programming is mostly in the internet, it still has all sorts of

53:46.760 --> 53:53.320
failure modes, and it's not very reliable, but it's surprisingly reliable. But you put a paper

53:53.320 --> 53:57.160
out with Tanenbaum and a whole bunch of other people. And you actually said, well, if you want

53:57.160 --> 54:02.280
some policy advice, if you really want to think about how we can improve the situation, you said,

54:02.360 --> 54:08.840
aggregating benchmarks and also giving instance level failure modes can actually help us understand

54:08.840 --> 54:15.080
why things went wrong or, you know, why things gave us the right answer for the wrong reasons.

54:15.720 --> 54:20.440
And there were all sorts of limiting factors, you said. You know, we have this kind of

54:20.440 --> 54:25.880
censorship by concision. You're only allowed to have seven pages in your conference workshop paper,

54:25.880 --> 54:29.640
and there's no policy about this. So can you give us a heads up on that?

54:30.280 --> 54:36.600
Yeah, I mean, you know, traditionally in machine learning, people use accuracy and similar kinds

54:36.600 --> 54:44.520
of aggregate measures to report their results. And, you know, if someone tells you that the accuracy

54:44.520 --> 54:53.000
was, you know, 78%, what does that tell you exactly? I think, you know, this gets back to the idea of

54:53.000 --> 54:57.640
scientific method. You know, in science, the most interesting things are the failures.

54:58.600 --> 55:01.800
And those are the things you really have to focus on. It's like, why did it fail?

55:02.440 --> 55:08.680
And that's what we need to know to understand machine learning systems. So the most simple

55:09.640 --> 55:12.840
kind of reporting would be just to report for every instance in your

55:13.720 --> 55:20.520
benchmark, your data set. How did the system do? What was its answer? And that's not, you know,

55:21.320 --> 55:30.360
it doesn't seem like a very big ask, but it would be very useful. And we now have in conferences,

55:31.080 --> 55:36.440
you're allowed to have some kind of supplementary material online. So you could have this available.

55:37.000 --> 55:45.320
And we did this for our concept arc paper. We showed for every instance, like what humans did,

55:45.320 --> 55:52.120
what machines did, we tried to analyze the errors of the system. And I think this these kinds of

55:52.120 --> 55:57.320
reporting will be will give us a lot more insight into what these systems are doing and what their

55:57.320 --> 56:05.480
like real capabilities are. Yeah. And it's, and back to the difficulty that Tim mentioned earlier,

56:06.120 --> 56:10.840
totally agree. And this is work that has to be done. Like if, if we are going to build a science

56:10.840 --> 56:17.000
of machine cognition, you know, this work has to be done. Yeah, I think. And I just want to shout

56:17.000 --> 56:22.920
out to Ryan Bernal, who spearheaded that paper, because he really is the one pushing for all

56:22.920 --> 56:29.880
this. And I think it's fantastic. So just in the last few minutes, you know, since we have you,

56:30.680 --> 56:36.840
complexity and complexity theory is a topic I really love. I'm not an expert in it at all,

56:36.840 --> 56:41.480
but I like to think about I like to explore it. I'm just curious, you know, from your perspective,

56:41.480 --> 56:46.520
um, what are some of the most interesting things happening right now in complexity theory? And

56:46.520 --> 56:51.560
if I wanted to go learn a bit more and check out just some cool, you know, latest stuff, what should

56:51.560 --> 56:58.040
what should we go look at? So I think there's, you know, there's a lot of interesting stuff going on,

56:58.040 --> 57:03.800
obviously, and complex systems is a huge umbrella for a lot of research. But

57:04.760 --> 57:11.480
if you're interested in the one big topic that people look at is called scaling. And it's the

57:11.480 --> 57:18.120
question of like, what happens to a system as it gets bigger in some sense? So this started out

57:18.120 --> 57:27.880
with some work on the sort of energy use of systems like animals as they as their maths increases.

57:28.680 --> 57:35.320
And people discovered some really interesting scaling laws that were very non-intuitive and

57:35.320 --> 57:42.840
they were able to explain these laws using ideas like fractal fractals and the fractal structure

57:42.840 --> 57:49.640
of complex systems. But now, so this is all on like biological metabolism and things like that.

57:50.360 --> 57:57.240
But now a lot of people are extending that scaling work to cities. So asking what happens

57:57.880 --> 58:04.360
to cities when they increase in size, either in area or in population size. And

58:05.400 --> 58:09.640
there's all kinds of phenomena that you can see, like what's the rate of innovation

58:10.440 --> 58:17.880
measured by something like patents? And what's the rate of sort of energy usage by a city?

58:19.880 --> 58:26.280
And what's how do these things change? Even like the happiness of the people,

58:27.240 --> 58:31.880
you know, are people in New York happier than people in Santa Fe, which is a much smaller city?

58:34.520 --> 58:42.040
These things scale in really interesting ways. And it's opening up a lot of new ideas about how

58:42.760 --> 58:50.440
social systems work. And how... Is it a similar thing that you can't trust the benchmarks? Because

58:50.440 --> 58:55.400
how happy people are, might you look at the rate of antidepressant usage or something?

58:55.400 --> 58:59.880
Yeah. So you do have all these... Right. I don't know if that's exactly what they use, but

59:01.160 --> 59:05.160
you do have to look at ways to measure these things, which can be questioned.

59:06.840 --> 59:12.520
But there are a lot of really... And I think this whole science, the science of cities, is

59:13.400 --> 59:19.240
it's very preliminary. And there's a lot of ideas about how to measure these things, how to

59:19.480 --> 59:26.360
develop sort of analytical descriptions or laws that govern certain properties and how to

59:26.360 --> 59:32.760
interpret them. But there's just a lot of really interesting work in this. And it turns out that

59:32.760 --> 59:40.520
now that everybody has a cell phone, you can really do a lot of tracking. A lot of these quantities

59:40.520 --> 59:48.280
can be tracked by people's sort of their movement, their interaction with other people, and all these

59:48.840 --> 59:55.880
things that you can measure using cell phones. So that's very cool.

59:57.160 --> 01:00:03.000
That is... Yeah. Thank you. That sounds actually fascinating. And one reason why for me particularly

01:00:03.000 --> 01:00:10.120
is... Are you familiar with Asimov's Foundation series? Yeah. So you know, psycho history in

01:00:10.120 --> 01:00:16.760
there was the science... And it was almost like a thermodynamics of human behavior that was only

01:00:16.840 --> 01:00:21.560
applicable at kind of planet scale and beyond. So it's like these scaling laws. So this is maybe

01:00:21.560 --> 01:00:26.200
one step towards... Very similar, psycho history. Yeah. One step towards psycho history of Asimov's

01:00:26.200 --> 01:00:33.000
kind. Exactly. Yeah. Cool. And in closing, does that give you intuition on the scaling of intelligence?

01:00:34.760 --> 01:00:45.160
Well... That's a great question. And I think, you know, one question you can ask is like,

01:00:45.160 --> 01:00:49.480
there's individual intelligence and then there's collective intelligence.

01:00:50.680 --> 01:00:56.120
And how much of the intelligence that we have individually is actually grounded in a more

01:00:56.120 --> 01:01:03.080
collective intelligence? You know, there's many things that I don't know, like I don't understand

01:01:03.080 --> 01:01:09.960
quantum mechanics or something, but I know somebody who does. And therefore, I feel like it's understood.

01:01:10.360 --> 01:01:17.480
And a lot of our intelligence, I think, is sort of more social than we think.

01:01:19.240 --> 01:01:25.880
Oh, absolutely. And folks should definitely read Melanie's book. So your complexity book,

01:01:25.880 --> 01:01:30.040
we actually covered that quite a lot on our show on Emergence. It's absolutely wonderful. And of

01:01:30.040 --> 01:01:36.680
course, your book on AI is probably the best book on AI I've ever read. It's up there with

01:01:36.680 --> 01:01:41.560
Christopher Sommerfield's book. But anyway, Melanie, honestly, you are my hero. Thank you so

01:01:41.560 --> 01:01:45.960
much for coming on MLS2. I really appreciate it. Thanks so much for having me. I really enjoyed

01:01:45.960 --> 01:01:47.640
it. It's great talking to you.

