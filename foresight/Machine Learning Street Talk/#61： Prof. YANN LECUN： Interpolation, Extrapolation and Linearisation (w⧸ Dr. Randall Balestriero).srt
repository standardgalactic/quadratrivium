1
00:00:00,000 --> 00:00:02,480
Coming up later in Machine Learning Street Talk,

2
00:00:02,480 --> 00:00:07,560
Professor Jan Le Koon, the godfather of deep learning.

3
00:00:07,560 --> 00:00:12,840
There's been a lot of people who have been sort of saying

4
00:00:12,840 --> 00:00:15,480
there's a limitation to deep learning, let's say,

5
00:00:15,480 --> 00:00:17,880
or machine learning more generally.

6
00:00:17,880 --> 00:00:20,240
Because it's obvious that those things basically

7
00:00:20,240 --> 00:00:22,640
do curve fitting, and that only works for interpolation

8
00:00:22,640 --> 00:00:24,400
and not for extrapolation.

9
00:00:24,480 --> 00:00:30,680
And that kind of dismissal always sounded wrong to me.

10
00:00:30,680 --> 00:00:35,120
Would this qualitative difference be in the form of fundamentally

11
00:00:35,120 --> 00:00:37,960
different things from deep learning, things that

12
00:00:37,960 --> 00:00:40,280
are like discrete symbolic reasoning

13
00:00:40,280 --> 00:00:42,080
or things of that type?

14
00:00:42,080 --> 00:00:44,280
And to that, my answer is clearly no.

15
00:00:44,280 --> 00:00:46,360
I do not believe that's the case.

16
00:00:46,360 --> 00:00:48,280
That's a limitation of supervised learning.

17
00:00:48,280 --> 00:00:52,000
It has absolutely nothing to do with deep learning.

18
00:00:52,000 --> 00:00:53,160
Computing on things.

19
00:00:53,160 --> 00:00:55,480
OK, and I put a stop right there.

20
00:00:55,480 --> 00:00:57,760
We also had a fascinating conversation

21
00:00:57,760 --> 00:01:02,280
with Randall Bellisterio from Meta AI Research.

22
00:01:02,280 --> 00:01:04,160
So I think it's two different things

23
00:01:04,160 --> 00:01:07,600
to be able to interpolate or move on your manifold

24
00:01:07,600 --> 00:01:11,240
and being in the interpolation regime from your training set.

25
00:01:11,240 --> 00:01:14,040
So is this similar to interpolation?

26
00:01:14,040 --> 00:01:15,440
Well, I mean, all of machine learning

27
00:01:15,440 --> 00:01:18,240
is similar to interpolation if you want, right?

28
00:01:18,240 --> 00:01:21,560
When you train a linear regression on scalar values,

29
00:01:21,560 --> 00:01:22,960
you're training a model, right?

30
00:01:22,960 --> 00:01:25,600
You're giving a bunch of pairs X and Y.

31
00:01:25,600 --> 00:01:29,560
You're asking what are the best values of A and B for Y equals

32
00:01:29,560 --> 00:01:33,640
AX plus B that minimizes the squared error of the prediction

33
00:01:33,640 --> 00:01:35,520
of a line to all of the points, right?

34
00:01:35,520 --> 00:01:36,880
That's linear regression.

35
00:01:36,880 --> 00:01:38,320
That's interpolation.

36
00:01:38,320 --> 00:01:39,960
All of machine learning is interpolation.

37
00:01:39,960 --> 00:01:41,720
In a high-dimensional space, there

38
00:01:41,720 --> 00:01:44,040
is essentially no such thing as interpolation.

39
00:01:44,040 --> 00:01:45,840
Everything is extrapolation.

40
00:01:45,840 --> 00:01:48,960
So imagine you are in a space of images, right?

41
00:01:48,960 --> 00:01:51,680
So you have a car images 256 by 256.

42
00:01:51,720 --> 00:01:54,440
So it's 200,000 dimensional input space.

43
00:01:54,440 --> 00:01:56,000
Even if you have a million samples,

44
00:01:56,000 --> 00:02:00,240
you're only covering a tiny portion of the dimensions

45
00:02:00,240 --> 00:02:01,120
of that space, right?

46
00:02:01,120 --> 00:02:05,960
Those images are in a tiny sliver of surface

47
00:02:05,960 --> 00:02:09,200
among the space of all possible combinations

48
00:02:09,200 --> 00:02:10,680
of values of pixels.

49
00:02:10,680 --> 00:02:13,560
So when you show the system a new image,

50
00:02:13,560 --> 00:02:16,240
it's very unlikely that this image is a linear combination

51
00:02:16,240 --> 00:02:17,200
of previous images.

52
00:02:17,200 --> 00:02:20,440
What you're doing is extra-appalation, not interpolation.

53
00:02:20,480 --> 00:02:22,480
And in high-dimension, all of machine learning

54
00:02:22,480 --> 00:02:24,640
is extrapolation, which is why it's high.

55
00:02:24,640 --> 00:02:26,800
Now, that was a clip that went out on the Chalet show.

56
00:02:26,800 --> 00:02:29,480
And I think I understand now what Lacune meant.

57
00:02:29,480 --> 00:02:31,640
He's saying that our intuition of interpolation

58
00:02:31,640 --> 00:02:35,200
is only correct in a very low number of dimensions.

59
00:02:35,200 --> 00:02:36,880
It's a mathematical impossibility

60
00:02:36,880 --> 00:02:39,640
to have statistical generalization in high dimensions.

61
00:02:39,640 --> 00:02:42,080
So he's not saying that neural networks are really

62
00:02:42,080 --> 00:02:44,880
extrapolating in the sense of continuing the pattern.

63
00:02:44,880 --> 00:02:47,240
He's saying our definition of extrapolation,

64
00:02:47,240 --> 00:02:49,760
the convex whole membership, is broken.

65
00:02:49,800 --> 00:02:52,080
Machine learning does not and will never

66
00:02:52,080 --> 00:02:53,440
work in high dimensions.

67
00:02:53,440 --> 00:02:55,520
That's why we've invented so many tricks

68
00:02:55,520 --> 00:02:58,880
to reduce the statistical and approximation complexity

69
00:02:58,880 --> 00:03:01,120
of problems, just like we do in computer science

70
00:03:01,120 --> 00:03:03,560
of the computational complexity of algorithms.

71
00:03:03,560 --> 00:03:08,080
Lacune is not saying that deep learning models are clairvoyant.

72
00:03:08,080 --> 00:03:11,240
Jan Lacune thinks that it's specious to say

73
00:03:11,240 --> 00:03:13,840
that neural network models are interpolating

74
00:03:13,840 --> 00:03:18,840
because in high dimensions, everything is extrapolation.

75
00:03:20,080 --> 00:03:21,600
Oh boy.

76
00:03:21,600 --> 00:03:24,680
We started thinking about making this show many months ago

77
00:03:24,680 --> 00:03:27,200
when Randall Belastriro and Jerome Pesente

78
00:03:27,200 --> 00:03:30,120
and Jan Lacune first released their paper,

79
00:03:30,120 --> 00:03:34,280
Learning in High Dimensions Always Amounts to Extrapolation.

80
00:03:34,280 --> 00:03:36,960
And it feels like a lot has happened since then.

81
00:03:36,960 --> 00:03:39,200
I mean, they do say the mark of an educated mind

82
00:03:39,200 --> 00:03:41,960
is being able to change your opinion

83
00:03:41,960 --> 00:03:44,120
in light of new evidence.

84
00:03:44,120 --> 00:03:46,560
Where we are now compared to where we were then,

85
00:03:46,560 --> 00:03:48,400
I mean, let's just say, I feel like I'm standing

86
00:03:48,400 --> 00:03:50,360
on the surface of Pluto.

87
00:03:51,800 --> 00:03:54,960
I was initially quite skeptical of this paper

88
00:03:54,960 --> 00:03:56,640
and I was trying to pick it apart.

89
00:03:56,640 --> 00:04:00,000
And now it feels like we've done 180 degrees,

90
00:04:00,000 --> 00:04:02,760
not only on the way we think about the paper,

91
00:04:02,760 --> 00:04:05,200
but how we think about neural networks in general.

92
00:04:05,200 --> 00:04:07,440
And we really wanna try and impart some of that knowledge

93
00:04:07,440 --> 00:04:08,960
with you today in the introduction,

94
00:04:08,960 --> 00:04:10,680
which might be quite ambitious.

95
00:04:10,680 --> 00:04:11,680
So please bear with us.

96
00:04:11,680 --> 00:04:14,320
But generally speaking, the structure of the show today,

97
00:04:14,320 --> 00:04:15,520
it's gonna be a big show.

98
00:04:15,520 --> 00:04:16,720
It's gonna be a big show.

99
00:04:16,720 --> 00:04:20,440
Use the table of contents, skip around a bit.

100
00:04:20,440 --> 00:04:22,400
But there's an intro, there's a, you know,

101
00:04:22,400 --> 00:04:24,280
where we talk about this blind view

102
00:04:24,280 --> 00:04:26,640
or this boundary view of neural networks,

103
00:04:26,640 --> 00:04:28,120
which I think is very interesting.

104
00:04:28,120 --> 00:04:30,160
There's a conversation with Yann LeCun,

105
00:04:30,160 --> 00:04:31,720
the godfather of deep learning.

106
00:04:31,720 --> 00:04:35,120
And we also speak with Randall Belastriro,

107
00:04:35,120 --> 00:04:38,000
who released this beautiful paper

108
00:04:38,000 --> 00:04:40,760
about thinking about neural networks in this new way.

109
00:04:40,760 --> 00:04:42,000
And we have a debrief as well.

110
00:04:42,000 --> 00:04:43,520
So I've not edited the show yet,

111
00:04:43,520 --> 00:04:45,640
but it's gonna be about three hours.

112
00:04:45,640 --> 00:04:47,520
Anyway, the show must go on.

113
00:04:47,520 --> 00:04:51,880
So as I was saying, particularly speaking with Randall,

114
00:04:51,880 --> 00:04:53,160
was a revelation.

115
00:04:53,160 --> 00:04:56,040
I mean, I originally thought that the authors were saying

116
00:04:56,040 --> 00:04:58,040
that people shouldn't think of neural networks

117
00:04:58,040 --> 00:05:01,600
as interpolators because they thought neural networks

118
00:05:01,600 --> 00:05:03,520
were doing something even more sophisticated.

119
00:05:03,520 --> 00:05:06,480
But what I came to realize, at least in Randall's case,

120
00:05:06,480 --> 00:05:08,360
is he's even more cynical about the behavior

121
00:05:08,360 --> 00:05:10,440
of neural networks than Gary Marcus.

122
00:05:10,440 --> 00:05:13,360
He doesn't think that they're fitting curves at all.

123
00:05:13,400 --> 00:05:15,200
Reading Randall's recent spline theory

124
00:05:15,200 --> 00:05:18,080
of deep learning paper has completely changed the way

125
00:05:18,080 --> 00:05:20,760
that I think about their behavior.

126
00:05:20,760 --> 00:05:24,920
So his view, basically, is that neural networks

127
00:05:24,920 --> 00:05:27,160
recursively chop up the input space

128
00:05:28,040 --> 00:05:32,040
into these little convex cells, or polyhedra,

129
00:05:32,040 --> 00:05:34,600
conceptually similar to decision trees,

130
00:05:34,600 --> 00:05:37,880
but one where the regions in the layer can share information,

131
00:05:37,880 --> 00:05:40,880
which means that different faces of these polyhedra

132
00:05:40,880 --> 00:05:43,480
will correspond to different hyperplanes

133
00:05:43,480 --> 00:05:46,640
set down by different neurons in the neural network,

134
00:05:46,640 --> 00:05:48,800
of course, ones which are topologically addressable

135
00:05:48,800 --> 00:05:50,080
from that location.

136
00:05:50,080 --> 00:05:54,920
So to me, this ends the notion that these neural networks

137
00:05:54,920 --> 00:05:56,640
are learning smooth data manifolds

138
00:05:56,640 --> 00:05:58,920
or performing smooth geometric transformations

139
00:05:58,920 --> 00:06:01,080
in the generative setting, which I used to think of

140
00:06:01,080 --> 00:06:03,400
as being a kind of diffeomorphism.

141
00:06:03,400 --> 00:06:04,960
It also gave me the realization that,

142
00:06:04,960 --> 00:06:07,720
at least in inference, each input prediction

143
00:06:07,720 --> 00:06:12,080
is representable with a single linear affine transformation,

144
00:06:12,080 --> 00:06:14,960
which leads directly to the next realization,

145
00:06:14,960 --> 00:06:17,520
which is that the latent space is not homogenous.

146
00:06:17,520 --> 00:06:19,160
It's potentially a different space

147
00:06:19,160 --> 00:06:21,200
for every single input example.

148
00:06:21,200 --> 00:06:22,960
I just didn't realize that before.

149
00:06:22,960 --> 00:06:24,400
You know, suddenly a lot of the magic

150
00:06:24,400 --> 00:06:25,600
of deep learning has vanished,

151
00:06:25,600 --> 00:06:28,320
and I see them in the same light as things like decision trees

152
00:06:28,320 --> 00:06:30,200
and SVMs in classical machine learning.

153
00:06:30,200 --> 00:06:32,320
I mean, probably because I understood

154
00:06:32,320 --> 00:06:33,720
those things very deeply.

155
00:06:34,640 --> 00:06:36,200
So yeah, neural networks have lost

156
00:06:36,200 --> 00:06:37,360
a little bit of their mystery,

157
00:06:37,360 --> 00:06:39,640
but I think it's a good thing.

158
00:06:39,640 --> 00:06:41,680
Reminds me a little bit of this parable

159
00:06:41,680 --> 00:06:44,240
of the blind men and the elephant,

160
00:06:44,240 --> 00:06:46,400
where, let's say you've got four blind men

161
00:06:46,400 --> 00:06:47,680
around an elephant.

162
00:06:47,680 --> 00:06:49,720
One's got his arm around the trunk.

163
00:06:49,720 --> 00:06:51,520
Oh, it feels like a snake.

164
00:06:51,520 --> 00:06:54,560
One's feeling the tail, or it looks like a rope,

165
00:06:54,560 --> 00:06:56,080
or it feels like a rope.

166
00:06:56,080 --> 00:06:58,720
One's on the side, or it feels like a wall.

167
00:06:58,720 --> 00:07:01,400
And it's very similar situation with neural networks

168
00:07:01,400 --> 00:07:04,240
that, you know, the experience of these blind men,

169
00:07:04,240 --> 00:07:07,320
it's all the truth, but it's not the complete truth.

170
00:07:07,320 --> 00:07:09,560
We're all just trying to understand this thing

171
00:07:09,560 --> 00:07:11,120
from different angles.

172
00:07:11,120 --> 00:07:14,960
Now anyway, the two key challenges in machine learning

173
00:07:14,960 --> 00:07:17,760
are one, the curse of dimensionality.

174
00:07:17,760 --> 00:07:19,200
And there's a corollary of that,

175
00:07:19,200 --> 00:07:21,080
knowing what to ignore in the input space,

176
00:07:21,080 --> 00:07:24,040
because it blows up exponentially with the dimensions.

177
00:07:24,040 --> 00:07:27,240
And two, the extrapolation problem.

178
00:07:27,240 --> 00:07:29,280
What happens when you extrapolate

179
00:07:29,280 --> 00:07:31,000
outside of the training data?

180
00:07:31,000 --> 00:07:35,400
And why does this notion of extrapolation even matter?

181
00:07:35,400 --> 00:07:38,640
Well put simply, it matters because of the implications

182
00:07:38,640 --> 00:07:42,240
it has for generalization in deep learning.

183
00:07:42,240 --> 00:07:45,240
Now, why do deep learning models work at all?

184
00:07:45,240 --> 00:07:47,600
Well, I think there's an incredible amount of engineering

185
00:07:47,600 --> 00:07:51,220
which goes into the state of the art machine learning models,

186
00:07:51,220 --> 00:07:54,880
which makes them work very well on specific tasks.

187
00:07:54,880 --> 00:07:58,680
But it's easy to deceive yourself with neural networks.

188
00:07:58,680 --> 00:08:01,320
Literally, everything about your training process

189
00:08:01,320 --> 00:08:03,760
and predictive architecture could be leaking

190
00:08:03,760 --> 00:08:06,360
the main specific information into your model.

191
00:08:06,360 --> 00:08:09,000
Neural networks are not really blank slate models

192
00:08:09,000 --> 00:08:10,480
like we've been led to believe.

193
00:08:10,480 --> 00:08:12,880
A lot of human crafted domain knowledge

194
00:08:12,880 --> 00:08:15,840
goes into these models.

195
00:08:15,840 --> 00:08:17,560
By the way, this is why Francois Schollet,

196
00:08:17,560 --> 00:08:19,480
I mean, he talks about this notion

197
00:08:19,480 --> 00:08:21,240
of developer aware generalization,

198
00:08:21,240 --> 00:08:24,320
which is being able to generalize to tasks

199
00:08:24,320 --> 00:08:26,280
which the developer of the system

200
00:08:26,280 --> 00:08:28,080
didn't know about at the time.

201
00:08:29,060 --> 00:08:32,200
Now, weights and biases is the developer first

202
00:08:32,240 --> 00:08:35,640
of all, MLops platform build better models faster

203
00:08:35,640 --> 00:08:38,560
with experiment tracking, dataset versioning

204
00:08:38,560 --> 00:08:40,160
and model management.

205
00:08:40,160 --> 00:08:42,480
It provides a platform and a set of tools

206
00:08:42,480 --> 00:08:44,960
which you can use for the entire life cycle

207
00:08:44,960 --> 00:08:46,840
of your machine learning process.

208
00:08:46,840 --> 00:08:49,600
Every team has a reliable system of record

209
00:08:49,600 --> 00:08:50,880
for their engineering system, right?

210
00:08:50,880 --> 00:08:53,360
Whether it's Azure DevOps for software engineering

211
00:08:53,360 --> 00:08:55,680
or Confluence for team wikis.

212
00:08:55,680 --> 00:08:58,560
Weights and biases is your system of record

213
00:08:58,560 --> 00:08:59,880
for machine learning.

214
00:08:59,880 --> 00:09:02,600
Now, for your quick link and to help out the podcast,

215
00:09:02,600 --> 00:09:06,280
head over to 1db.me forward slash MLST.

216
00:09:06,280 --> 00:09:08,880
I'm extremely proud actually that we've been sponsored

217
00:09:08,880 --> 00:09:09,800
by weights and biases.

218
00:09:09,800 --> 00:09:12,080
I've been a huge fan of what they've been doing

219
00:09:12,080 --> 00:09:13,240
since the very beginning.

220
00:09:13,240 --> 00:09:15,800
And also I've been a part of their community forum.

221
00:09:15,800 --> 00:09:17,800
They are the one company who in my opinion

222
00:09:17,800 --> 00:09:19,880
has totally changed the game

223
00:09:19,880 --> 00:09:22,640
around what is the highly nuanced process

224
00:09:22,640 --> 00:09:25,240
and the kind of multidisciplinary team actions

225
00:09:25,240 --> 00:09:28,040
that are required to get machine learning models

226
00:09:28,040 --> 00:09:29,600
safely into production.

227
00:09:29,600 --> 00:09:32,280
I've been a chief data scientist for a major corporation

228
00:09:32,280 --> 00:09:33,440
for the last 18 months or so.

229
00:09:33,440 --> 00:09:35,720
And I've always been a big believer

230
00:09:35,720 --> 00:09:37,560
in introducing engineering rigor

231
00:09:37,560 --> 00:09:39,400
to machine learning and data science.

232
00:09:39,400 --> 00:09:41,560
Engineering fundamentals are so important

233
00:09:41,560 --> 00:09:44,080
when you bring machine learning systems to production

234
00:09:44,080 --> 00:09:47,800
as is helping data scientists to become first class citizens

235
00:09:47,800 --> 00:09:51,660
in the entire life cycle of model development and deployment.

236
00:09:51,660 --> 00:09:53,720
It's almost a shame that MLST has become

237
00:09:53,720 --> 00:09:55,200
quite science orientated

238
00:09:55,200 --> 00:09:58,000
because in my day job, I'm an engineer first.

239
00:09:58,000 --> 00:10:00,640
I would love to make more content about this stuff.

240
00:10:00,640 --> 00:10:02,880
Anyway, weights and biases helps you go faster.

241
00:10:02,880 --> 00:10:04,400
It makes your team more resilient

242
00:10:04,400 --> 00:10:06,100
by increasing your knowledge sharing.

243
00:10:06,100 --> 00:10:07,120
It helps you build models

244
00:10:07,120 --> 00:10:09,640
which not only have better predictive performance

245
00:10:09,640 --> 00:10:12,000
but are more safe and secure.

246
00:10:12,000 --> 00:10:14,720
Weights and biases gives you better management information

247
00:10:14,720 --> 00:10:16,980
allowing your company to make better decisions

248
00:10:16,980 --> 00:10:19,840
and build data products with greater transparency

249
00:10:19,840 --> 00:10:21,160
than ever before.

250
00:10:21,160 --> 00:10:23,440
And critically, because weights and biases

251
00:10:23,440 --> 00:10:26,080
orchestrates the entire machine learning process,

252
00:10:26,080 --> 00:10:27,680
your models are reproducible

253
00:10:27,680 --> 00:10:30,240
and important decisions are immortalized.

254
00:10:30,240 --> 00:10:32,680
This fine level of control lets you set guardrails

255
00:10:32,680 --> 00:10:36,480
where you need to and reduce friction wherever possible.

256
00:10:36,480 --> 00:10:39,400
I've designed many ML DevOps systems over the years,

257
00:10:39,400 --> 00:10:41,360
including when I worked at Microsoft.

258
00:10:41,360 --> 00:10:43,360
The technology has really come of age now

259
00:10:43,360 --> 00:10:45,200
and I think you should be using a platform

260
00:10:45,200 --> 00:10:46,520
like weights and biases

261
00:10:46,520 --> 00:10:49,120
to help you train your machine learning models

262
00:10:49,120 --> 00:10:51,080
and to get them into production.

263
00:10:51,960 --> 00:10:54,840
Reproducibility has always been one of the biggest problems

264
00:10:54,840 --> 00:10:55,880
in machine learning.

265
00:10:55,880 --> 00:10:57,840
And the reason for that is that these systems

266
00:10:57,840 --> 00:10:59,600
are quite brittle, frankly.

267
00:10:59,600 --> 00:11:01,920
Their runtime characteristics depend strongly

268
00:11:01,920 --> 00:11:04,000
on the vagaries of the training regime,

269
00:11:04,000 --> 00:11:05,720
the choice of hyperparameters,

270
00:11:05,720 --> 00:11:08,280
even the hardware that they were trained on.

271
00:11:08,280 --> 00:11:09,800
You know, you always get into this syndrome

272
00:11:09,800 --> 00:11:11,800
where it works on my machine

273
00:11:11,800 --> 00:11:13,200
but I can't put it into production

274
00:11:13,200 --> 00:11:15,040
because I've got no idea

275
00:11:15,040 --> 00:11:17,560
how to recreate this thing somewhere else.

276
00:11:17,560 --> 00:11:19,680
I work in a highly regulated industry

277
00:11:19,680 --> 00:11:21,800
and we often need to wind the clock back

278
00:11:21,800 --> 00:11:24,560
to try and understand how a model was created

279
00:11:24,600 --> 00:11:26,200
which decisions were made

280
00:11:26,200 --> 00:11:27,480
and possibly go back later

281
00:11:27,480 --> 00:11:29,440
to reason about a model's behavior.

282
00:11:29,440 --> 00:11:32,600
This is precisely what weights and biases does.

283
00:11:32,600 --> 00:11:34,560
Now, it's completely free for academics

284
00:11:34,560 --> 00:11:35,880
and it's simple to get started.

285
00:11:35,880 --> 00:11:37,880
You just have to add a few lines of code.

286
00:11:37,880 --> 00:11:39,400
So what are you waiting for?

287
00:11:39,400 --> 00:11:40,960
Remember to click on our special link

288
00:11:40,960 --> 00:11:42,240
because it helps us out.

289
00:11:42,240 --> 00:11:44,920
That's 1db.me forward slash MLST.

290
00:11:44,920 --> 00:11:46,720
We are so grateful for the sponsorship

291
00:11:46,720 --> 00:11:47,880
from weights and biases.

292
00:11:47,880 --> 00:11:49,640
And also feel free to get in touch with us

293
00:11:49,640 --> 00:11:51,600
if you're interested in sponsoring the show.

294
00:11:51,600 --> 00:11:53,720
It'll help us scale up the operation a little bit

295
00:11:53,720 --> 00:11:55,640
because at the moment I'm doing all the editing

296
00:11:55,640 --> 00:11:56,680
and it takes a lot of time.

297
00:11:56,680 --> 00:11:58,480
Anyway, thank you very much.

298
00:11:58,480 --> 00:12:01,160
Randall's other recent work demonstrates

299
00:12:01,160 --> 00:12:03,720
that a large class of neural networks

300
00:12:03,720 --> 00:12:08,040
including CNNs, ResNets, RNNs and beyond,

301
00:12:08,040 --> 00:12:11,480
those that use piecewise linear activation functions

302
00:12:11,480 --> 00:12:15,160
like RELU can be entirely rewritten

303
00:12:15,160 --> 00:12:17,760
as compositions of linear functions

304
00:12:17,760 --> 00:12:22,040
arranged in polyhedra or cells in the input space.

305
00:12:22,960 --> 00:12:27,880
It's a high resolution slice and dice into linear forms

306
00:12:27,880 --> 00:12:31,800
like a fruit ninja set loose on a marching cubes algorithm

307
00:12:31,800 --> 00:12:35,160
such as K-means clustering, matched filter banks

308
00:12:35,160 --> 00:12:37,240
and vector quantitization,

309
00:12:37,240 --> 00:12:40,640
which in plain English means locality sensitive clustering.

310
00:12:41,520 --> 00:12:43,320
It also provides new insight

311
00:12:43,320 --> 00:12:47,000
into the functioning of neural networks themselves.

312
00:12:47,000 --> 00:12:49,120
For example, with this view,

313
00:12:49,120 --> 00:12:50,800
what a neural network is doing

314
00:12:50,800 --> 00:12:53,920
is performing an input sensitive lookup

315
00:12:53,920 --> 00:12:55,720
for a matching filter

316
00:12:55,720 --> 00:12:58,240
and then computing a simple inner product

317
00:12:58,240 --> 00:13:00,760
between that filter and the input signal.

318
00:13:01,800 --> 00:13:03,120
If that doesn't shed light

319
00:13:03,120 --> 00:13:06,240
on Francois Chalet's characterization of neural networks

320
00:13:06,240 --> 00:13:08,880
as locally sensitive hash tables,

321
00:13:08,880 --> 00:13:11,120
then I don't know what will.

322
00:13:11,120 --> 00:13:13,400
Another cool thing to come out of Randall's work

323
00:13:13,400 --> 00:13:15,520
was a geometrically principled way

324
00:13:15,520 --> 00:13:18,920
of devising regularization penalty terms

325
00:13:19,000 --> 00:13:21,160
which can improve neural network performance

326
00:13:21,160 --> 00:13:24,200
by orthogonalizing the placement

327
00:13:24,200 --> 00:13:26,720
of those latent hyperplane boundaries

328
00:13:26,720 --> 00:13:30,200
to increase their representational power.

329
00:13:30,200 --> 00:13:32,520
In short, looking at neural networks

330
00:13:32,520 --> 00:13:36,520
through this piecewise linear kaleidoscope, if you will,

331
00:13:36,520 --> 00:13:39,480
is opening new avenues of technical understanding

332
00:13:39,480 --> 00:13:40,680
and exploration.

333
00:13:40,760 --> 00:13:43,760
Now, we spoke with Yannick about this as well,

334
00:13:43,760 --> 00:13:46,600
and he commented that he had looked into these polyhedra,

335
00:13:46,600 --> 00:13:50,400
which he called relu cells in his own research.

336
00:13:50,400 --> 00:13:52,600
While he agreed that the boundaries between them

337
00:13:52,600 --> 00:13:54,960
are not technically smooth,

338
00:13:54,960 --> 00:13:56,800
there are so many of them,

339
00:13:56,800 --> 00:13:59,920
combinatorially many possible polyhedra, in fact,

340
00:13:59,920 --> 00:14:03,280
since each can be defined by any combination

341
00:14:03,280 --> 00:14:05,800
of topologically addressable hyperblogics

342
00:14:05,800 --> 00:14:08,640
that can be found in a neural network.

343
00:14:08,760 --> 00:14:11,880
So, if you look at some of the topologically addressable

344
00:14:11,880 --> 00:14:14,920
hyperplane boundaries, there are so many,

345
00:14:14,920 --> 00:14:16,920
Yannick thinks that neural networks

346
00:14:16,920 --> 00:14:20,320
can make them effectively smooth by arranging them

347
00:14:20,320 --> 00:14:24,400
so that they change very little from neighbor to neighbor.

348
00:14:24,400 --> 00:14:28,200
This insight came from his work in adversarial examples,

349
00:14:28,200 --> 00:14:31,200
where the name of the game is to perturbed the input

350
00:14:31,200 --> 00:14:34,440
as little as possible to the nearest polyhedron

351
00:14:34,440 --> 00:14:36,240
with a different class.

352
00:14:36,240 --> 00:14:39,720
He, Randall's work has transformed the way I think

353
00:14:39,720 --> 00:14:41,720
about neural networks.

354
00:14:41,720 --> 00:14:45,760
I know what they're doing at a much deeper level now.

355
00:14:45,760 --> 00:14:47,760
Each layer of a neural network

356
00:14:47,760 --> 00:14:50,680
contributes a new set of hyperplanes,

357
00:14:50,680 --> 00:14:53,720
and the relu's act to toggle the hyperplanes

358
00:14:53,720 --> 00:14:56,160
in an input sensitive way.

359
00:14:56,160 --> 00:15:00,040
Every layer, indeed every neuron in the network,

360
00:15:00,040 --> 00:15:02,080
not just the final layer,

361
00:15:02,080 --> 00:15:05,640
participates in placing flat decision boundaries,

362
00:15:05,640 --> 00:15:09,400
defining a honeycomb of affine cells.

363
00:15:10,560 --> 00:15:13,880
Each input signal will fall into one of these cells

364
00:15:13,880 --> 00:15:18,280
and be transformed by the combined affine transformation

365
00:15:18,280 --> 00:15:20,400
of every neuron it activated.

366
00:15:22,040 --> 00:15:26,320
I also used to think of a single unified latent space

367
00:15:26,320 --> 00:15:27,840
at each layer.

368
00:15:27,840 --> 00:15:31,360
However, it seems obvious that any particular input

369
00:15:31,360 --> 00:15:33,960
will toggle in an input specific way

370
00:15:33,960 --> 00:15:35,440
a set of hyperplanes.

371
00:15:35,440 --> 00:15:39,560
By virtue of the relu's, it does or does not activate.

372
00:15:39,560 --> 00:15:42,880
Therefore, different populations of input samples

373
00:15:42,880 --> 00:15:47,640
will reside in different latent spaces at any one layer

374
00:15:47,640 --> 00:15:51,960
defined by the activated set of hyperplanes.

375
00:15:51,960 --> 00:15:53,840
So it seems to me that instead of having

376
00:15:53,840 --> 00:15:56,080
a unified latent space,

377
00:15:56,080 --> 00:16:01,080
we have input specific latent spaces plural at each layer.

378
00:16:02,080 --> 00:16:04,440
There's also the matter of whether classifiers

379
00:16:04,440 --> 00:16:07,600
are even learning the data manifold at all.

380
00:16:07,600 --> 00:16:09,360
I have my doubts.

381
00:16:09,360 --> 00:16:12,440
They may be learning predictably useful aspects

382
00:16:12,440 --> 00:16:14,080
of the manifold.

383
00:16:14,080 --> 00:16:17,160
However, neural network classifiers are optimized

384
00:16:17,160 --> 00:16:19,480
to find class boundaries,

385
00:16:19,480 --> 00:16:23,120
and any structure between boundaries can be ignored.

386
00:16:24,240 --> 00:16:26,800
The idea that learning boundary manifolds

387
00:16:26,800 --> 00:16:31,040
necessarily means learning intrinsic connection manifolds

388
00:16:31,360 --> 00:16:33,560
is where I'm really struggling now.

389
00:16:33,560 --> 00:16:38,000
I can't convince myself that optimizing for separation

390
00:16:38,000 --> 00:16:41,800
will give the same outcome as optimizing for connection.

391
00:16:42,720 --> 00:16:45,360
For example, training the same neural network

392
00:16:45,360 --> 00:16:48,880
in two different ways, one for classification

393
00:16:48,880 --> 00:16:52,000
and once for decoding or generation,

394
00:16:52,000 --> 00:16:54,880
will result in very different network weights

395
00:16:54,880 --> 00:16:56,800
and latent structure.

396
00:16:56,800 --> 00:16:58,880
It's even more difficult to accept now

397
00:16:58,880 --> 00:17:02,920
that I'm thinking of these manifolds as piecewise linear,

398
00:17:02,920 --> 00:17:04,720
where what the neural network has learned

399
00:17:04,720 --> 00:17:07,800
is combinations of separating hyperplanes

400
00:17:07,800 --> 00:17:12,400
that chop the space into single class polyhedra.

401
00:17:12,400 --> 00:17:16,080
After all, there is nothing in the objective function

402
00:17:16,080 --> 00:17:19,480
which cares about the structure within a polyhedra.

403
00:17:19,480 --> 00:17:22,920
As long as the class is correct, we're good.

404
00:17:22,920 --> 00:17:25,760
Nor is there anything necessarily optimizing

405
00:17:25,760 --> 00:17:28,560
for global manifold structure

406
00:17:28,560 --> 00:17:31,880
beyond just efficient and parsimonious use

407
00:17:31,880 --> 00:17:34,080
of the available parameter space

408
00:17:34,080 --> 00:17:37,480
and whatever prior structure was hard-coded

409
00:17:37,480 --> 00:17:41,120
by humans and the network constraints and topology itself.

410
00:17:42,720 --> 00:17:44,520
Randall commented in the interview

411
00:17:44,520 --> 00:17:46,560
that in high-dimensional spaces,

412
00:17:46,560 --> 00:17:49,240
you can easily separate everything

413
00:17:49,240 --> 00:17:52,320
but your generalization performance might be bad.

414
00:17:52,320 --> 00:17:56,520
So you want to trade off separability with dimensionality.

415
00:17:56,560 --> 00:17:59,880
He thinks the merit of deep neural networks

416
00:17:59,880 --> 00:18:03,320
is being able to find a nonlinear transformation

417
00:18:03,320 --> 00:18:06,520
which retains separating hyperplanes

418
00:18:06,520 --> 00:18:09,240
while reducing dimensionality enough

419
00:18:09,240 --> 00:18:12,200
to confer generalization power.

420
00:18:12,200 --> 00:18:13,760
I mean, I couldn't help but notice

421
00:18:13,760 --> 00:18:17,120
that everything in the machine learning world is linear.

422
00:18:17,120 --> 00:18:19,720
All the popular algorithms are linear

423
00:18:19,720 --> 00:18:22,000
and any nonlinearity is a trick.

424
00:18:22,000 --> 00:18:24,400
I mean, we just apply some nonlinear transformation

425
00:18:24,400 --> 00:18:27,080
to the data before running it through our algorithms

426
00:18:27,080 --> 00:18:29,600
just like how it is in support vector machines

427
00:18:29,600 --> 00:18:32,760
or kernel ridge regression in Gaussian processors.

428
00:18:32,760 --> 00:18:34,480
Deep learning models are no different.

429
00:18:34,480 --> 00:18:37,520
We're just placing these relus all over the input space

430
00:18:37,520 --> 00:18:39,120
to slice it and dice it.

431
00:18:41,040 --> 00:18:41,880
Let's think about this.

432
00:18:41,880 --> 00:18:44,560
What is the simplest mathematical model?

433
00:18:44,560 --> 00:18:46,160
Linear, right?

434
00:18:46,160 --> 00:18:49,120
What's the next most simple one?

435
00:18:49,120 --> 00:18:50,880
Piecewise linear.

436
00:18:50,880 --> 00:18:53,320
So machine learning hasn't even evolved yet

437
00:18:53,320 --> 00:18:54,240
to the second order.

438
00:18:54,280 --> 00:18:56,840
Neural networks in their contemporary usage

439
00:18:56,840 --> 00:18:59,560
only include the minimum possible nonlinearity

440
00:18:59,560 --> 00:19:01,720
of piecewise linear.

441
00:19:01,720 --> 00:19:03,800
If they didn't, an entire neural network model

442
00:19:03,800 --> 00:19:07,280
could be described as one monolithic linear function

443
00:19:07,280 --> 00:19:09,680
with a single transformation matrix.

444
00:19:09,680 --> 00:19:12,200
This view, I think, should give you a cleaner

445
00:19:12,200 --> 00:19:15,240
and analytical view of neural networks.

446
00:19:15,240 --> 00:19:17,480
Now, there are some reasons, in my opinion,

447
00:19:17,480 --> 00:19:19,000
for the tendency towards linear.

448
00:19:19,000 --> 00:19:21,200
I mean, computability is one of the reasons,

449
00:19:21,200 --> 00:19:25,720
but almost every part of mathematics favours linearization,

450
00:19:25,720 --> 00:19:27,920
whether it's Newton's method or calculus

451
00:19:27,920 --> 00:19:31,440
or linear algebra solving PDEs,

452
00:19:31,440 --> 00:19:32,520
plenty of other examples.

453
00:19:32,520 --> 00:19:35,720
So it shouldn't come as a huge surprise.

454
00:19:35,720 --> 00:19:39,200
Not only that, the function space of nonlinear functions

455
00:19:39,200 --> 00:19:40,880
is exponentially larger.

456
00:19:40,880 --> 00:19:42,360
And remember, in machine learning,

457
00:19:42,360 --> 00:19:44,440
the big challenge is to reduce the size

458
00:19:44,440 --> 00:19:47,000
of the approximation class.

459
00:19:47,000 --> 00:19:48,680
So what about boundaries?

460
00:19:48,680 --> 00:19:50,400
Now, the purpose of every value cell

461
00:19:50,400 --> 00:19:53,480
is to define a boundary in the ambient space

462
00:19:53,480 --> 00:19:56,360
to chop off what is no longer required.

463
00:19:56,360 --> 00:19:58,920
All you have is a linear separating hyperplane.

464
00:19:58,920 --> 00:20:01,840
And downstream, you work out the distance

465
00:20:01,840 --> 00:20:05,040
from the hyperplanes that you're on the right side of,

466
00:20:05,040 --> 00:20:06,640
you know, the ones that didn't get chopped off.

467
00:20:06,640 --> 00:20:08,600
So the magic of neural networks

468
00:20:08,600 --> 00:20:12,920
is actually learning what to ignore in the ambient space.

469
00:20:12,920 --> 00:20:15,960
Now, I think a problem with my previous intuition

470
00:20:15,960 --> 00:20:19,280
is that, like most people, I imagine to neural network,

471
00:20:19,280 --> 00:20:22,080
latent space is a bit like a UMAP or a Disney projection

472
00:20:22,080 --> 00:20:25,360
plot, and this leads us to misunderstand their behavior.

473
00:20:25,360 --> 00:20:28,400
The latent space that these examples get projected into

474
00:20:28,400 --> 00:20:29,680
is not homogeneous.

475
00:20:29,680 --> 00:20:31,480
Depending on which cell you fell into

476
00:20:31,480 --> 00:20:33,920
in the input space or the ambient space,

477
00:20:33,920 --> 00:20:36,480
a different affine transformation will be applied,

478
00:20:36,480 --> 00:20:39,160
sending you to a different region of the latent space.

479
00:20:39,160 --> 00:20:42,520
So the latent space is kind of stitched together

480
00:20:42,520 --> 00:20:46,440
like bits of a cosmic jigsaw puzzle in the ambient space.

481
00:20:46,440 --> 00:20:48,760
And then when you run UMAP on the latent,

482
00:20:48,760 --> 00:20:50,480
you see all of the clusters, but you'd

483
00:20:50,480 --> 00:20:52,960
be forgiven for thinking that it was performing

484
00:20:52,960 --> 00:20:56,360
some kind of smooth, diffeomorphic transformation

485
00:20:56,360 --> 00:20:59,000
of the entire input space and successive layers

486
00:20:59,000 --> 00:21:01,520
in the neural network, or even learning the topology

487
00:21:01,520 --> 00:21:02,800
of the data manifold.

488
00:21:02,800 --> 00:21:04,880
I think it's much better to think of neural networks

489
00:21:04,880 --> 00:21:08,080
as quantizing the input space, much like a vector search

490
00:21:08,080 --> 00:21:11,280
engine does using locality-sensitive hashing.

491
00:21:11,280 --> 00:21:14,520
Now, imagine a classification problem on the Cartesian plane

492
00:21:14,520 --> 00:21:17,800
where the upper right and lower left quadrants are blue,

493
00:21:17,800 --> 00:21:21,400
and the upper left and lower right quadrants are orange.

494
00:21:21,400 --> 00:21:24,480
Now, as a minimal example, if we trained a single layer

495
00:21:24,480 --> 00:21:27,840
for neuron-relu neural network to fit this,

496
00:21:27,840 --> 00:21:30,640
it will fit four diagonal hyperplanes,

497
00:21:30,640 --> 00:21:34,400
two of which are reflected versions of the same hyperplane.

498
00:21:34,400 --> 00:21:36,120
Now, by the way, you can play with this

499
00:21:36,120 --> 00:21:38,560
on the TensorFlow playground, which we think is probably

500
00:21:38,560 --> 00:21:41,520
one of the best tools for building strong intuition

501
00:21:41,520 --> 00:21:43,680
on how neural networks work.

502
00:21:43,680 --> 00:21:46,720
Now, what it does is it shows you how the ambient space

503
00:21:46,720 --> 00:21:49,000
is being effectively subdivided by all of these

504
00:21:49,000 --> 00:21:50,760
addressable hyperplanes.

505
00:21:50,760 --> 00:21:53,400
But this quadrant example is unintuitive in the tool

506
00:21:53,400 --> 00:21:56,800
because getting your head around how the relus act together

507
00:21:56,800 --> 00:22:00,360
to combine these hyperplanes to carve up the ambient space

508
00:22:00,360 --> 00:22:02,200
isn't immediately obvious.

509
00:22:02,200 --> 00:22:05,040
Now, when you run the tool on a more complex example,

510
00:22:05,040 --> 00:22:07,200
so for example, a spiral manifold,

511
00:22:07,200 --> 00:22:11,080
you'll now see the artifacts of these hyperplanes everywhere.

512
00:22:11,080 --> 00:22:13,840
When these piecewise linear chops are composed together

513
00:22:13,840 --> 00:22:16,320
in the second layer, we get a decision surface

514
00:22:16,320 --> 00:22:19,080
in the ambient space, which can appear smooth given

515
00:22:19,080 --> 00:22:21,600
enough pieces, but it's actually a composition

516
00:22:21,600 --> 00:22:23,720
of piecewise linear functions.

517
00:22:23,720 --> 00:22:26,040
Now, as you can see, it's just chopping up the input space

518
00:22:26,040 --> 00:22:28,280
in every neuron in the first layer,

519
00:22:28,280 --> 00:22:31,080
each one with a different angle and translation.

520
00:22:31,080 --> 00:22:32,440
And when you get to the second layer,

521
00:22:32,440 --> 00:22:35,280
it's chopping up the space in a more sophisticated way

522
00:22:35,280 --> 00:22:38,400
by composing together the chops from the previous layer.

523
00:22:38,400 --> 00:22:40,160
Now, the first thing to realize here

524
00:22:40,160 --> 00:22:42,520
is that for each test input example,

525
00:22:42,520 --> 00:22:44,440
and every example in its vicinity,

526
00:22:44,440 --> 00:22:46,600
its class projection is definable

527
00:22:46,600 --> 00:22:48,560
with a single affine projection.

528
00:22:48,560 --> 00:22:50,400
There's no continuous morphing going on here.

529
00:22:50,400 --> 00:22:53,800
It's more like a jigsaw puzzle being pieced together

530
00:22:53,800 --> 00:22:57,520
to form many parts of a bigger ambient space.

531
00:22:57,520 --> 00:23:00,240
Now, oftentimes we see negative weights,

532
00:23:00,240 --> 00:23:03,200
which actually allows the hyperplanes to combine

533
00:23:03,200 --> 00:23:05,440
in interesting ways to kind of partially cancel

534
00:23:05,440 --> 00:23:09,200
each other out to allow more complex decision boundaries.

535
00:23:09,200 --> 00:23:12,920
Any hyperplane can be reused by multiple other neurons,

536
00:23:12,920 --> 00:23:14,680
whichever neurons it can reach

537
00:23:14,680 --> 00:23:17,480
via some topologically addressable pathway.

538
00:23:17,480 --> 00:23:19,120
Every hyperplane can be the face

539
00:23:19,120 --> 00:23:21,520
of multiple polyhedra in the ambient space.

540
00:23:21,520 --> 00:23:23,080
This is how they share information.

541
00:23:23,080 --> 00:23:24,400
And it should be obvious by now

542
00:23:24,400 --> 00:23:27,080
that there are two to the N addressable polyhedra

543
00:23:27,080 --> 00:23:30,040
in the ambient space where N is the number of neurons.

544
00:23:30,040 --> 00:23:31,800
Now, we also observed that the neural network

545
00:23:31,800 --> 00:23:34,360
is only capable of slicing and dicing the input space

546
00:23:34,360 --> 00:23:35,720
with flat planes.

547
00:23:35,720 --> 00:23:38,280
The only smooth nonlinearities enter the picture here

548
00:23:38,280 --> 00:23:41,440
where we as data scientists linearize the data

549
00:23:41,440 --> 00:23:44,200
by performing some smooth nonlinear transformation

550
00:23:44,200 --> 00:23:46,560
before it even enters the neural network,

551
00:23:46,560 --> 00:23:47,880
much like is the case

552
00:23:47,880 --> 00:23:49,960
with other machine learning algorithms.

553
00:23:49,960 --> 00:23:52,640
Now, even on the spiral data set example,

554
00:23:52,640 --> 00:23:55,600
the quickest way to make the neural network fit the data

555
00:23:55,600 --> 00:23:57,640
is to effectively linearize the data

556
00:23:57,640 --> 00:24:00,000
by applying a nonlinear transformation

557
00:24:00,000 --> 00:24:01,800
before it even gets in.

558
00:24:01,800 --> 00:24:02,880
Now, we also played around

559
00:24:02,880 --> 00:24:05,600
with this kind of contrived circular manifold data set

560
00:24:05,600 --> 00:24:08,240
where there's a ball of data encompassed

561
00:24:08,240 --> 00:24:09,320
by a circle around it.

562
00:24:09,320 --> 00:24:11,400
And it's possible to do a better job on that

563
00:24:11,400 --> 00:24:13,400
with one relu neuron, right?

564
00:24:13,400 --> 00:24:15,160
But with two nonlinear transformations

565
00:24:15,160 --> 00:24:16,920
of the input data set,

566
00:24:16,920 --> 00:24:19,040
rather than fit this huge bunch

567
00:24:19,040 --> 00:24:21,280
of piecewise linear relus on the ambient data.

568
00:24:21,280 --> 00:24:22,560
I mean, it should be obvious, right?

569
00:24:22,560 --> 00:24:25,240
But this idea that you don't need feature engineering

570
00:24:25,240 --> 00:24:27,920
in neural networks, I think is nonsense.

571
00:24:27,920 --> 00:24:30,280
Now, given how obvious it is that neural networks

572
00:24:30,280 --> 00:24:31,840
are just chopping up the input space

573
00:24:31,840 --> 00:24:33,560
with these piecewise linear functions,

574
00:24:33,560 --> 00:24:34,680
it begs the question,

575
00:24:34,680 --> 00:24:37,560
how could they possibly extrapolate in any meaningful way,

576
00:24:37,560 --> 00:24:39,520
right, especially in the general setting?

577
00:24:39,520 --> 00:24:42,840
And by the way, when we say extrapolation,

578
00:24:42,840 --> 00:24:45,320
we're talking about this more general sense

579
00:24:45,320 --> 00:24:46,680
of continuing the pattern,

580
00:24:46,680 --> 00:24:51,240
not the convex whole notion in the given in the paper.

581
00:24:51,240 --> 00:24:53,920
Regarding these piecewise linear functions,

582
00:24:53,920 --> 00:24:56,320
I remember an impactful moment

583
00:24:56,320 --> 00:24:58,680
for more than 20 years ago in grad school,

584
00:24:59,520 --> 00:25:02,400
a visiting professor who had multiple science

585
00:25:02,400 --> 00:25:03,920
and nature publications,

586
00:25:03,920 --> 00:25:06,960
you know, a pinnacle of academic achievement,

587
00:25:06,960 --> 00:25:09,760
was sharing his insights on how we students

588
00:25:09,760 --> 00:25:13,760
might also extract such papers ourselves from nature.

589
00:25:13,760 --> 00:25:15,720
And he was showing figures from his paper

590
00:25:15,720 --> 00:25:18,520
and they all had two things in common.

591
00:25:18,520 --> 00:25:22,320
First, they depicted very simple relationships

592
00:25:22,320 --> 00:25:25,120
that were previously undiscovered, okay?

593
00:25:25,120 --> 00:25:29,680
And second, all the fitted models were piecewise linear.

594
00:25:30,680 --> 00:25:33,120
He even explicitly commented along the lines

595
00:25:33,120 --> 00:25:36,040
that of course there's some underlying

596
00:25:36,040 --> 00:25:39,040
smooth nonlinear relationship,

597
00:25:39,040 --> 00:25:42,480
but absent a solid theoretical model of that,

598
00:25:42,480 --> 00:25:45,920
which is often going to be the case for new discoveries,

599
00:25:45,920 --> 00:25:50,840
you're better off with simple piecewise linear models.

600
00:25:50,840 --> 00:25:52,200
I guess when in doubt,

601
00:25:52,200 --> 00:25:55,480
Occam's razor always makes straight cuts.

602
00:25:56,600 --> 00:25:58,280
And now it's clear to me

603
00:25:58,280 --> 00:26:01,240
that the successful deep networks of today

604
00:26:01,240 --> 00:26:03,720
are following that advice.

605
00:26:03,720 --> 00:26:07,280
If you've ever wondered why piecewise linear activation

606
00:26:07,280 --> 00:26:10,160
functions are dominating the field,

607
00:26:10,160 --> 00:26:13,840
maybe it's because they've abandoned all pretense

608
00:26:13,840 --> 00:26:16,920
at finding smooth nonlinear models

609
00:26:16,920 --> 00:26:21,840
and are keeping it simple by fitting piecewise linear models,

610
00:26:21,840 --> 00:26:25,800
albeit at machine scale with billions of parameters.

611
00:26:26,800 --> 00:26:31,160
Randall's spline work makes that bit of philosophical insight

612
00:26:31,240 --> 00:26:34,080
brutally clear in my opinion.

613
00:26:34,080 --> 00:26:36,280
It conjures a scene of Darth Vader,

614
00:26:36,280 --> 00:26:39,640
that cybernetic warlord towering over

615
00:26:39,640 --> 00:26:41,880
a growing neural network saying,

616
00:26:41,880 --> 00:26:44,780
embrace your hyperplanes.

617
00:26:45,920 --> 00:26:48,040
Now, you know, I've always been skeptical

618
00:26:48,040 --> 00:26:50,240
and often remind us of the limitations

619
00:26:50,240 --> 00:26:52,320
of today's machine learning.

620
00:26:52,320 --> 00:26:55,720
I'd say things like ML isn't magic learning,

621
00:26:56,600 --> 00:26:57,680
but for a moment,

622
00:26:57,680 --> 00:27:01,560
I too allowed myself to become deluded into thinking

623
00:27:01,560 --> 00:27:04,760
that they are creating some kind of nonlinearity

624
00:27:04,760 --> 00:27:07,880
beyond piecewise linearity.

625
00:27:07,880 --> 00:27:12,840
But relu is by far the dominant activation function

626
00:27:12,840 --> 00:27:16,440
because it stops pretending at anything other

627
00:27:16,440 --> 00:27:18,840
than piecewise linear.

628
00:27:18,840 --> 00:27:22,880
Just stick in a flat boundary threshold and the line.

629
00:27:22,880 --> 00:27:24,880
A neuron puts in a hyperplane

630
00:27:24,880 --> 00:27:27,120
and then lets the rest of the network

631
00:27:27,120 --> 00:27:29,300
chop more as needed.

632
00:27:29,300 --> 00:27:31,800
All the success of neural networks

633
00:27:31,800 --> 00:27:36,380
seems explained by piecewise linear functions.

634
00:27:37,600 --> 00:27:40,760
I also find it intriguing that our own brains,

635
00:27:40,760 --> 00:27:43,440
our own wet neural networks,

636
00:27:43,440 --> 00:27:45,520
have somehow gained access

637
00:27:45,520 --> 00:27:48,940
to smooth nonlinear imagination.

638
00:27:49,920 --> 00:27:53,220
Just look at the laws we've defined in physics.

639
00:27:54,180 --> 00:27:59,060
Many exhibit nonlinear but smooth structure.

640
00:27:59,060 --> 00:28:00,420
On the other hand,

641
00:28:00,420 --> 00:28:02,820
neural networks chop up the input space

642
00:28:02,820 --> 00:28:07,240
with flat boundaries and sharp edges.

643
00:28:07,240 --> 00:28:11,700
How can we possibly expect them to learn or discover

644
00:28:11,700 --> 00:28:14,200
the kinds of smooth relationships

645
00:28:14,200 --> 00:28:18,140
that seem fundamental to science and reality?

646
00:28:18,140 --> 00:28:20,200
All feature engineering and representation learning

647
00:28:20,200 --> 00:28:22,380
and machine learning is about finding these

648
00:28:22,420 --> 00:28:24,620
interpolative representations.

649
00:28:24,620 --> 00:28:26,300
Now, I saw a really interesting example of this

650
00:28:26,300 --> 00:28:28,620
when I was reading Francois Chouelet's book recently

651
00:28:28,620 --> 00:28:32,920
and he spoke of pixel grids of watch faces, right?

652
00:28:32,920 --> 00:28:36,780
Now, they're not interpolative in the ambient space.

653
00:28:36,780 --> 00:28:38,780
If you take the average of two of these images,

654
00:28:38,780 --> 00:28:42,860
you'll just get four faded clock hands

655
00:28:42,860 --> 00:28:44,180
on top of each other, right?

656
00:28:44,180 --> 00:28:46,020
Superposed, not very helpful.

657
00:28:46,020 --> 00:28:48,340
But however, if you represent the hands

658
00:28:48,340 --> 00:28:49,980
with Euclidean coordinates,

659
00:28:49,980 --> 00:28:52,540
then the problem becomes more interpolated

660
00:28:52,540 --> 00:28:55,300
but you're still not all the way there.

661
00:28:55,300 --> 00:28:57,100
The average of two points of interest

662
00:28:57,100 --> 00:28:59,540
doesn't fall on this intended circle manifold

663
00:28:59,540 --> 00:29:01,580
that we're interested in.

664
00:29:01,580 --> 00:29:02,900
You know, it would be interpolated

665
00:29:02,900 --> 00:29:05,700
if we use some relevant distance function

666
00:29:05,700 --> 00:29:08,400
which was some geodesic interpolation.

667
00:29:09,460 --> 00:29:10,580
But you know, anyway,

668
00:29:10,580 --> 00:29:13,180
if we encoded the problem using polar coordinates,

669
00:29:13,180 --> 00:29:15,420
then it becomes linearly interpolated, right?

670
00:29:15,420 --> 00:29:16,580
The problem is solved

671
00:29:16,580 --> 00:29:19,400
and you can generalize perfectly to any new clock face.

672
00:29:19,400 --> 00:29:21,000
But I mean, in that case,

673
00:29:21,000 --> 00:29:22,480
you wouldn't even need a neural network

674
00:29:22,480 --> 00:29:25,040
and the kicker is that we as data scientists

675
00:29:25,040 --> 00:29:27,880
would have to figure this out ourselves, right?

676
00:29:27,880 --> 00:29:29,800
The neural network wouldn't be particularly efficient

677
00:29:29,800 --> 00:29:33,200
at doing so, especially if we, the data scientists,

678
00:29:33,200 --> 00:29:35,900
didn't feed in the relevant nonlinear transformation

679
00:29:35,900 --> 00:29:39,000
before it went into the model.

680
00:29:39,000 --> 00:29:40,720
So anyway, if there exists

681
00:29:40,720 --> 00:29:42,640
a nonlinear transformation of this problem,

682
00:29:42,640 --> 00:29:45,080
then, you know, which makes it interpolated,

683
00:29:45,080 --> 00:29:46,420
then we can say in some sense

684
00:29:46,420 --> 00:29:48,840
that it's got a lower intrinsic dimension.

685
00:29:48,840 --> 00:29:50,280
So crucially, deep learning models,

686
00:29:50,280 --> 00:29:53,720
they can be understood as unconstrained surfaces.

687
00:29:53,720 --> 00:29:55,180
They do have some structure

688
00:29:55,180 --> 00:29:56,860
coming from their architectural priors,

689
00:29:56,860 --> 00:30:00,520
but that structure only serves to restrict the search space

690
00:30:00,520 --> 00:30:03,200
to a smaller space of unconstrained surfaces.

691
00:30:03,200 --> 00:30:06,060
It doesn't provide the kind of inductive prior

692
00:30:06,060 --> 00:30:09,080
that would enable stronger generalization.

693
00:30:09,080 --> 00:30:12,160
A model architecture does not contain a model of the domain

694
00:30:12,160 --> 00:30:14,780
except in the extremely restricted sense

695
00:30:14,780 --> 00:30:17,360
of injecting priors such as, I don't know,

696
00:30:17,360 --> 00:30:20,520
translational equivalents in convolutional neural networks.

697
00:30:20,520 --> 00:30:23,960
If you want to generalize in a more systematic fashion,

698
00:30:23,960 --> 00:30:26,600
you either need a model with a strong inductive prior

699
00:30:26,600 --> 00:30:28,160
or a model which doesn't operate

700
00:30:28,160 --> 00:30:31,800
by empirically slicing up the ambient space,

701
00:30:31,800 --> 00:30:34,280
yeah, like a deep learning model does.

702
00:30:34,280 --> 00:30:36,240
A discrete symbolic program

703
00:30:36,240 --> 00:30:37,600
would be an interesting alternative.

704
00:30:37,600 --> 00:30:41,480
I mean, just imagine the program Y equals X squared.

705
00:30:41,480 --> 00:30:44,000
It generalizes to any arbitrary number, right?

706
00:30:44,000 --> 00:30:45,320
Because it's highly structured.

707
00:30:45,320 --> 00:30:47,560
You can fit it with three training points.

708
00:30:47,560 --> 00:30:49,800
And once fit, it'll generalize to anything,

709
00:30:49,800 --> 00:30:52,680
even things that are outside of the training range.

710
00:30:52,680 --> 00:30:54,000
But the kicker, of course,

711
00:30:54,000 --> 00:30:55,880
is that you wouldn't know which curve to use

712
00:30:55,880 --> 00:30:58,800
if you only saw three points in the first place.

713
00:30:58,800 --> 00:31:00,680
Now, let's think of another example.

714
00:31:00,680 --> 00:31:04,340
And imagine a discrete staircase space.

715
00:31:04,340 --> 00:31:06,000
So in this particular example,

716
00:31:06,000 --> 00:31:09,680
an unconstrained curve gives you complete garbage,

717
00:31:09,680 --> 00:31:12,920
but a structured model with the correct priors,

718
00:31:12,920 --> 00:31:14,560
you know, it can still be a curve,

719
00:31:14,560 --> 00:31:17,320
will generalize in the extrapolation regime.

720
00:31:17,320 --> 00:31:19,800
And this prior basically means that you need to encode it

721
00:31:19,800 --> 00:31:20,640
or linearize it

722
00:31:20,640 --> 00:31:23,160
before it even gets into the neural network.

723
00:31:23,160 --> 00:31:25,520
In both cases, you're making the neural network

724
00:31:25,520 --> 00:31:27,800
work extremely well for a specific thing,

725
00:31:27,800 --> 00:31:31,440
but not very well for generalizing between new tasks.

726
00:31:31,440 --> 00:31:32,760
So anyway, when we say

727
00:31:32,760 --> 00:31:35,480
that deep learning models generalize via interpolation,

728
00:31:35,480 --> 00:31:37,560
what we're really saying is that these models

729
00:31:37,560 --> 00:31:38,400
that we're using today,

730
00:31:38,400 --> 00:31:39,640
that they work well with data

731
00:31:39,640 --> 00:31:42,440
that's within the training distribution of the problem,

732
00:31:42,440 --> 00:31:46,840
right, well, a problem that's intrinsically interpolative,

733
00:31:46,840 --> 00:31:50,160
but they won't generalize systematically to anything else.

734
00:31:50,160 --> 00:31:52,360
You won't generalize when looking at problems

735
00:31:52,360 --> 00:31:54,440
that are not interpolative in nature

736
00:31:54,440 --> 00:31:58,400
and problems that are outside the training distribution.

737
00:31:58,400 --> 00:31:59,280
So what do we talk about

738
00:31:59,280 --> 00:32:01,520
this binary notion of extrapolation?

739
00:32:01,520 --> 00:32:03,760
Well, the problem with the binary convex

740
00:32:03,760 --> 00:32:05,800
whole notion of extrapolation is that

741
00:32:05,800 --> 00:32:07,480
we're promoting this idea

742
00:32:07,480 --> 00:32:11,240
that the moment an example falls epsilon outside the hole,

743
00:32:11,240 --> 00:32:13,640
that the prediction qualitatively changes.

744
00:32:13,640 --> 00:32:17,040
Instead, I think it would be more of a distance question,

745
00:32:17,040 --> 00:32:19,560
which is to say, the further away you get

746
00:32:19,560 --> 00:32:22,760
from this convex hole, the greater the uncertainty.

747
00:32:22,760 --> 00:32:24,320
I mean, it's not like you fall off a cliff

748
00:32:24,320 --> 00:32:26,400
when you step outside the convex hole.

749
00:32:26,400 --> 00:32:28,360
Your approximation of the latent manifold

750
00:32:28,360 --> 00:32:30,080
remains valid for a little while,

751
00:32:30,080 --> 00:32:31,880
although it will quickly degrade

752
00:32:31,880 --> 00:32:33,480
as you move further outside,

753
00:32:33,480 --> 00:32:36,040
unless your model has some strong structural prior

754
00:32:36,040 --> 00:32:38,760
that matches the regularity of that latent manifold.

755
00:32:38,760 --> 00:32:40,720
So this brings up an important point, right?

756
00:32:40,800 --> 00:32:43,120
The relevant question is not whether you're inside

757
00:32:43,120 --> 00:32:44,640
or outside the convex hole.

758
00:32:44,640 --> 00:32:48,080
It's how far you are from the nearest training data points.

759
00:32:48,080 --> 00:32:50,160
If you're outside the convex hole,

760
00:32:50,160 --> 00:32:52,440
but you remain close to the training data,

761
00:32:52,440 --> 00:32:55,160
then your model remains an accurate approximation

762
00:32:55,160 --> 00:32:56,520
of the latent manifold.

763
00:32:56,520 --> 00:32:58,920
And even if you're inside the convex hole,

764
00:32:58,920 --> 00:33:01,200
you're in a region that wasn't densely sampled

765
00:33:01,200 --> 00:33:02,720
at training time, right?

766
00:33:02,720 --> 00:33:05,200
Where there's no nearby training points.

767
00:33:05,200 --> 00:33:06,560
Your unconstrained model

768
00:33:06,560 --> 00:33:08,960
may not accurately approximate the latent manifold.

769
00:33:08,960 --> 00:33:12,960
So inside or outside doesn't really tell you very much.

770
00:33:12,960 --> 00:33:14,800
It's all about the proximity.

771
00:33:14,800 --> 00:33:17,520
You're performing local generalization

772
00:33:17,520 --> 00:33:19,880
and the quality of your ability to generalize

773
00:33:19,880 --> 00:33:23,040
to new situations depends entirely

774
00:33:23,040 --> 00:33:25,960
on their proximity to known situations.

775
00:33:25,960 --> 00:33:27,920
However, if you're dealing with training data

776
00:33:27,920 --> 00:33:29,200
that is densely sampled

777
00:33:29,200 --> 00:33:31,360
within a certain region of the problem space,

778
00:33:31,360 --> 00:33:34,480
then the question, can I generalize here

779
00:33:34,480 --> 00:33:37,200
becomes approximately equivalent to,

780
00:33:37,200 --> 00:33:39,920
is this inside the convex hole or outside?

781
00:33:39,920 --> 00:33:41,520
Now, the whole point of feature engineering

782
00:33:41,520 --> 00:33:44,480
is to make data sets interpolative.

783
00:33:44,480 --> 00:33:47,400
Either us data scientists design the features by hand

784
00:33:47,400 --> 00:33:51,000
or neural networks learn them as part of the training process

785
00:33:51,000 --> 00:33:53,800
in big air quotes for the podcast listeners.

786
00:33:53,800 --> 00:33:56,280
So, I think when folks make the argument

787
00:33:56,280 --> 00:33:57,920
that machine learning models generalize

788
00:33:57,920 --> 00:34:01,360
via interpolation or that computer vision data sets,

789
00:34:01,360 --> 00:34:02,840
that they're interpolative,

790
00:34:02,840 --> 00:34:05,200
what they mean is that there's an encoding space

791
00:34:05,200 --> 00:34:08,000
for which the problem becomes interpolative

792
00:34:08,000 --> 00:34:11,600
or there exists a non-Euclidean pairwise distance function

793
00:34:11,600 --> 00:34:15,080
between the instances that makes the problem interpolative,

794
00:34:15,080 --> 00:34:16,920
which is basically the same thing.

795
00:34:16,920 --> 00:34:19,120
So, if it's possible to do this,

796
00:34:19,120 --> 00:34:22,320
then you can say that the problem is intrinsically interpolative

797
00:34:22,320 --> 00:34:25,560
but your current representation of the problem is not.

798
00:34:25,560 --> 00:34:27,480
So the key endeavor in machine learning

799
00:34:27,480 --> 00:34:29,720
is to find better representations,

800
00:34:29,720 --> 00:34:33,120
representations which reveal the interpolative nature

801
00:34:33,120 --> 00:34:33,960
of the problem.

802
00:34:34,880 --> 00:34:37,880
Now, you might just cynically write off this paper

803
00:34:37,880 --> 00:34:39,360
as being a trivial finding, right?

804
00:34:39,360 --> 00:34:42,480
You know, it seems trivial that the pixel space

805
00:34:42,480 --> 00:34:44,720
is not linearly interpolatable.

806
00:34:44,720 --> 00:34:46,320
Something that we've known about for decades.

807
00:34:46,320 --> 00:34:49,560
I mean, that's why computer vision engineers in the 1980s

808
00:34:49,560 --> 00:34:51,440
would write feature detectors

809
00:34:51,440 --> 00:34:53,420
to create an interpolative space

810
00:34:53,420 --> 00:34:56,160
for machine learning algorithms to work on, right?

811
00:34:56,160 --> 00:34:57,560
If you take any two images

812
00:34:57,560 --> 00:35:00,120
and you interpolate between them, what do you get?

813
00:35:00,120 --> 00:35:02,240
You just get a faded copy of both.

814
00:35:02,280 --> 00:35:05,720
Everyone in the field has known about this for decades, right?

815
00:35:05,720 --> 00:35:09,280
That examples are not interpolative

816
00:35:09,280 --> 00:35:12,160
in their original encoding space or their pixel space.

817
00:35:12,160 --> 00:35:13,440
I think the most important thing though

818
00:35:13,440 --> 00:35:15,400
is that this paper also shows

819
00:35:15,400 --> 00:35:17,760
that the models are not interpolative

820
00:35:17,760 --> 00:35:20,220
in their latent space either.

821
00:35:20,220 --> 00:35:21,140
That's a shocker.

822
00:35:22,040 --> 00:35:23,920
Now, what does Randall say about all of this?

823
00:35:23,920 --> 00:35:27,440
Well, Randall Bellisterio thinks that the steelman

824
00:35:27,440 --> 00:35:29,760
for this interpolation argument would be,

825
00:35:29,760 --> 00:35:32,620
well, what if you had very low dimensional,

826
00:35:32,620 --> 00:35:34,880
kind of like approximation of your data

827
00:35:34,880 --> 00:35:36,520
with very few factors of variation,

828
00:35:36,520 --> 00:35:39,600
which could be easily linearized in the latent space,

829
00:35:39,600 --> 00:35:42,440
then it might be interpolative.

830
00:35:42,440 --> 00:35:45,960
Randall thinks that the very concept of interpolation,

831
00:35:45,960 --> 00:35:48,080
I mean, it was defined about 50 years ago

832
00:35:48,080 --> 00:35:50,680
to describe these very small models

833
00:35:50,680 --> 00:35:53,640
with very few factors of variation.

834
00:35:53,640 --> 00:35:56,800
And it's just not relevant today, right?

835
00:35:56,800 --> 00:35:57,800
They showed in their paper

836
00:35:57,920 --> 00:36:00,140
that even in the relatively small latent space

837
00:36:00,140 --> 00:36:03,280
on a popular neural network classifier,

838
00:36:03,280 --> 00:36:05,320
interpolation does not occur.

839
00:36:05,320 --> 00:36:07,960
He thinks that most people think of interpolation

840
00:36:07,960 --> 00:36:10,160
by kind of conceptualizing their data

841
00:36:10,160 --> 00:36:12,560
into a few important latent factors.

842
00:36:12,560 --> 00:36:14,160
You might have dogs, for example,

843
00:36:14,160 --> 00:36:16,200
and they might exhibit a latent color,

844
00:36:16,200 --> 00:36:18,800
and a new color might be an interpolation

845
00:36:18,800 --> 00:36:20,440
between observed colors.

846
00:36:20,440 --> 00:36:22,640
But these guys think that we need to have

847
00:36:22,640 --> 00:36:25,640
an entirely new definition of interpolation.

848
00:36:25,680 --> 00:36:27,920
I think the main purpose behind their paper,

849
00:36:27,920 --> 00:36:30,480
according to them, is to show that even though

850
00:36:30,480 --> 00:36:32,800
the intuition that people have of interpolation

851
00:36:32,800 --> 00:36:34,600
works well in low dimensions,

852
00:36:34,600 --> 00:36:36,880
it falls down completely flat on its face

853
00:36:36,880 --> 00:36:38,120
in higher dimensions.

854
00:36:38,120 --> 00:36:40,720
Actually, the probability of your test data

855
00:36:40,720 --> 00:36:43,520
being in the convex hull of your training data

856
00:36:43,520 --> 00:36:46,720
is near zero past a certain number of dimensions.

857
00:36:46,720 --> 00:36:48,680
So the current definition of interpolation,

858
00:36:48,680 --> 00:36:50,400
which is to say like this notion

859
00:36:50,400 --> 00:36:51,880
of convex hull membership,

860
00:36:51,880 --> 00:36:55,120
it's too rigid to think about how interpolation works

861
00:36:55,120 --> 00:36:56,600
in neural networks.

862
00:36:56,600 --> 00:37:00,360
So Randall thinks that any new definition of extrapolation

863
00:37:00,360 --> 00:37:04,040
should be directly linked to generalization itself.

864
00:37:04,040 --> 00:37:05,760
This is what they're trying to get at.

865
00:37:05,760 --> 00:37:08,960
This is a clip from our show with Professor Max Welling.

866
00:37:08,960 --> 00:37:09,920
The first thing I wanna say,

867
00:37:09,920 --> 00:37:12,240
there is no machine learning without assumptions.

868
00:37:12,240 --> 00:37:16,040
It just basically, you have to interpolate between the dots,

869
00:37:16,040 --> 00:37:18,520
and to interpolate means that you have to make assumptions

870
00:37:18,520 --> 00:37:20,080
on smoothness or something like that.

871
00:37:20,080 --> 00:37:23,000
So the machine learning doesn't exist without assumptions.

872
00:37:23,000 --> 00:37:24,080
I think that's very clear.

873
00:37:24,080 --> 00:37:26,960
But clearly it's a dial, right?

874
00:37:26,960 --> 00:37:29,160
So you can have, on the one end,

875
00:37:29,160 --> 00:37:31,480
you can have problems with a huge amount of data.

876
00:37:31,480 --> 00:37:33,720
It has to be available clearly.

877
00:37:33,720 --> 00:37:37,000
And there you can dial down your inductive biases.

878
00:37:37,000 --> 00:37:40,600
You can basically say that the data do most of the work

879
00:37:40,600 --> 00:37:41,920
in some sense.

880
00:37:41,920 --> 00:37:44,960
Now, I think it's a good thing to build intuition

881
00:37:44,960 --> 00:37:46,960
about machine learning principally

882
00:37:46,960 --> 00:37:49,120
as an interpolation problem, right?

883
00:37:49,120 --> 00:37:50,840
So we're given the training data,

884
00:37:50,840 --> 00:37:53,080
and we need to cleverly interpolate

885
00:37:53,080 --> 00:37:55,400
between the training examples to reason

886
00:37:55,400 --> 00:37:58,600
about the statistical context of the test examples.

887
00:37:58,600 --> 00:38:00,840
This is what machine learning is in a nutshell.

888
00:38:00,840 --> 00:38:04,200
And actually, a researcher friend of mine

889
00:38:04,200 --> 00:38:06,120
from MetaAI Research in Silicon Valley,

890
00:38:06,120 --> 00:38:08,160
a guy called Dr. Thomas Lukes,

891
00:38:08,160 --> 00:38:09,840
he published a paper a couple of years ago

892
00:38:09,840 --> 00:38:13,000
called Interpolation of Sparse High-Dimensional Data.

893
00:38:13,000 --> 00:38:15,320
And he showed that it was indeed possible

894
00:38:15,320 --> 00:38:18,440
to perform competitively with multi-layer perceptrons

895
00:38:18,440 --> 00:38:22,040
for a regression problem using pure play interpolation methods

896
00:38:22,080 --> 00:38:26,080
like Voronoi and Dolornoi triangulation and spline methods.

897
00:38:26,080 --> 00:38:28,920
I mean, think of creating simplexes

898
00:38:28,920 --> 00:38:31,320
of the nearest neighbors around a training data

899
00:38:31,320 --> 00:38:33,480
and then averaging the results together.

900
00:38:33,480 --> 00:38:36,000
This simple method works remarkably well

901
00:38:36,000 --> 00:38:37,880
up to about 30 dimensions.

902
00:38:37,880 --> 00:38:39,880
I mean, obviously eventually it gets deranged

903
00:38:39,880 --> 00:38:41,360
by the curse of dimensionality,

904
00:38:41,360 --> 00:38:44,120
but as we'll discover slightly later,

905
00:38:44,120 --> 00:38:46,800
neural networks might have a slight advantage

906
00:38:46,800 --> 00:38:49,280
over pure play simplex interpolation

907
00:38:49,280 --> 00:38:50,520
just because they work principally

908
00:38:50,520 --> 00:38:54,120
by figuring out boundaries of the input space to exclude.

909
00:38:54,120 --> 00:38:57,160
So, you know, if you're on the zero side of the ReLU, that is.

910
00:38:57,160 --> 00:38:59,240
So that helps a lot in high dimensions

911
00:38:59,240 --> 00:39:00,960
when you have a poor sampling density

912
00:39:00,960 --> 00:39:03,040
in a particular region of the input space.

913
00:39:03,040 --> 00:39:07,120
But anyway, if you do principally think of machine learning

914
00:39:07,120 --> 00:39:08,960
as being an interpolation method,

915
00:39:08,960 --> 00:39:11,080
it raises some interesting ideas.

916
00:39:11,080 --> 00:39:13,760
You know, Thomas said to me that the crux of the problem

917
00:39:13,760 --> 00:39:15,840
is that we're trying to approximate a function

918
00:39:15,840 --> 00:39:17,760
that has spatially unique behavior

919
00:39:17,760 --> 00:39:19,720
in more than about 20 or so dimensions.

920
00:39:19,720 --> 00:39:22,240
Like in that case, it's hopeless.

921
00:39:22,240 --> 00:39:23,720
There's no question about it.

922
00:39:23,720 --> 00:39:26,680
He says that you can increase the data density,

923
00:39:26,680 --> 00:39:28,720
you know, like in 100 dimensions,

924
00:39:28,720 --> 00:39:32,120
but to get a grid with 10 points on a side, you know,

925
00:39:32,120 --> 00:39:34,200
which is to say 10 to the power of 100,

926
00:39:34,200 --> 00:39:36,240
there aren't enough protons in the universe

927
00:39:36,240 --> 00:39:37,880
that could convexly cover it.

928
00:39:37,880 --> 00:39:39,760
So, and Thomas concluded by saying,

929
00:39:39,760 --> 00:39:42,600
what this means is that everything that we do successfully

930
00:39:42,600 --> 00:39:45,280
approximate now with millions or billions

931
00:39:45,280 --> 00:39:47,120
or even trillions of data points,

932
00:39:47,160 --> 00:39:49,160
which accounts for a lot of the successes

933
00:39:49,160 --> 00:39:52,080
in machine learning, these data sets

934
00:39:52,080 --> 00:39:56,760
only have spatially novel behavior in very few dimensions,

935
00:39:56,760 --> 00:39:59,040
right, or a varying gradient in very few dimensions,

936
00:39:59,040 --> 00:40:00,240
16 or fewer.

937
00:40:00,240 --> 00:40:03,120
If we suppose that the true function has a varying gradient

938
00:40:03,120 --> 00:40:05,360
in more than that many dimensions,

939
00:40:05,360 --> 00:40:08,800
there's simply not enough data in the world to approximate it.

940
00:40:08,800 --> 00:40:11,880
The mathematics here is unequivocal.

941
00:40:13,120 --> 00:40:16,520
So, what actually happens when the test examples

942
00:40:16,560 --> 00:40:19,040
outside the convex hull of the training data?

943
00:40:19,040 --> 00:40:22,520
What happens when we're in an extrapolative regime?

944
00:40:22,520 --> 00:40:24,920
Well, it's increasingly impossible in high dimensions

945
00:40:24,920 --> 00:40:28,000
for the training set to be statistically representative

946
00:40:28,000 --> 00:40:29,120
of the test set.

947
00:40:29,120 --> 00:40:32,000
Test set instances will often distinctly differ

948
00:40:32,000 --> 00:40:34,120
from anything that we've seen during training.

949
00:40:34,120 --> 00:40:34,960
In machine learning,

950
00:40:34,960 --> 00:40:37,760
the test set will never fully characterize

951
00:40:37,760 --> 00:40:39,560
the problem that we're interested in.

952
00:40:39,560 --> 00:40:41,840
So, in high dimensional settings,

953
00:40:41,840 --> 00:40:44,200
features are often linearly correlated as well,

954
00:40:44,200 --> 00:40:47,160
which makes it challenging to know what information to use

955
00:40:47,160 --> 00:40:48,320
and what to discard.

956
00:40:48,320 --> 00:40:51,040
This is another fundamental problem of machine learning.

957
00:40:51,040 --> 00:40:54,440
Many machine learning approaches depend on modeling

958
00:40:54,440 --> 00:40:56,280
these local statistical relationships

959
00:40:56,280 --> 00:40:57,720
between the training samples,

960
00:40:57,720 --> 00:40:58,800
but in high dimensions,

961
00:40:58,800 --> 00:41:01,200
the probability of a new test example

962
00:41:01,200 --> 00:41:03,320
being in the convex hull of the training data

963
00:41:03,320 --> 00:41:06,000
goes to zero extremely quickly.

964
00:41:06,000 --> 00:41:08,360
So, making machine learning models work

965
00:41:08,360 --> 00:41:11,760
in an extrapolative regime necessitates the introduction

966
00:41:11,840 --> 00:41:13,480
of inductive biases,

967
00:41:13,480 --> 00:41:16,880
which are a way of adding a kind of enforced smoothness

968
00:41:16,880 --> 00:41:18,320
to the model predictions.

969
00:41:18,320 --> 00:41:21,520
So, if you hit the right bias,

970
00:41:21,520 --> 00:41:23,400
then it can be beneficial.

971
00:41:23,400 --> 00:41:26,440
If you impose the wrong bias,

972
00:41:26,440 --> 00:41:27,840
then it's gonna hurt you.

973
00:41:27,840 --> 00:41:29,280
And this is a well-known trade-off.

974
00:41:29,280 --> 00:41:32,840
So, of course, the whole endeavor of machine learning

975
00:41:32,840 --> 00:41:36,240
is defining the right inductive biases

976
00:41:36,240 --> 00:41:39,720
and leaving whatever you don't know to the data,

977
00:41:39,720 --> 00:41:42,680
and then basically learning to focus your models

978
00:41:42,680 --> 00:41:45,000
on the data that you're actually seeing.

979
00:41:45,000 --> 00:41:46,760
This is how we stop models

980
00:41:46,760 --> 00:41:48,600
from fitting the noise in the data

981
00:41:48,600 --> 00:41:50,440
or going completely haywire

982
00:41:50,440 --> 00:41:53,000
outside the convex hull of the training data.

983
00:41:53,000 --> 00:41:56,480
Extrapolation requires that sensible answers

984
00:41:56,480 --> 00:41:59,240
are given for all of the elements of the input space,

985
00:41:59,240 --> 00:42:02,360
even in regions not seen during training.

986
00:42:02,360 --> 00:42:05,040
And in particular, those within the convex hull

987
00:42:05,040 --> 00:42:07,160
of, let's say, some hypothetical,

988
00:42:07,200 --> 00:42:10,080
infinitely-sampled training data set.

989
00:42:10,080 --> 00:42:12,240
Now, what we do in neural networks

990
00:42:12,240 --> 00:42:14,200
and in many other machine learning algorithms,

991
00:42:14,200 --> 00:42:16,200
for that matter, is we just take a bunch

992
00:42:16,200 --> 00:42:19,720
of parameterized basis functions and we stack them

993
00:42:19,720 --> 00:42:21,880
and we fit them to our training data

994
00:42:21,880 --> 00:42:23,560
until it's well-described.

995
00:42:23,560 --> 00:42:26,400
The most important considerations for interpolation

996
00:42:26,400 --> 00:42:29,800
are the smoothness and robustness of these models.

997
00:42:29,800 --> 00:42:32,520
We want our models to produce gradual changes

998
00:42:32,520 --> 00:42:34,080
between the training points.

999
00:42:34,080 --> 00:42:35,600
Points outside the training set

1000
00:42:35,600 --> 00:42:37,600
should be handled with great care,

1001
00:42:37,600 --> 00:42:41,400
but what exactly that means depends greatly on the problem.

1002
00:42:41,400 --> 00:42:44,200
In order to even know that we're in an extrapolative regime,

1003
00:42:44,200 --> 00:42:45,800
the basis functions must be based

1004
00:42:45,800 --> 00:42:48,120
near to the training examples.

1005
00:42:48,120 --> 00:42:50,600
Typically, neural networks do not behave sensibly

1006
00:42:50,600 --> 00:42:53,200
in regions where there's no training examples, right,

1007
00:42:53,200 --> 00:42:54,960
because their learned basis functions

1008
00:42:54,960 --> 00:42:57,520
have been localized around the training data

1009
00:42:57,520 --> 00:42:59,440
in the ambient space.

1010
00:42:59,440 --> 00:43:01,200
It's also worth noting that extrapolation

1011
00:43:01,200 --> 00:43:03,920
and interpolation are two completely different regimes.

1012
00:43:03,920 --> 00:43:06,240
Optimizing for one, typically,

1013
00:43:06,240 --> 00:43:08,800
means being worse at the other.

1014
00:43:08,800 --> 00:43:10,560
Now Randall said that the boundary view

1015
00:43:10,560 --> 00:43:12,240
of neural networks is very clear

1016
00:43:12,240 --> 00:43:13,840
in the discriminative setting.

1017
00:43:13,840 --> 00:43:16,200
You only need interpolation to try

1018
00:43:16,200 --> 00:43:18,160
and understand how neural networks work

1019
00:43:18,160 --> 00:43:19,840
in the generative setting.

1020
00:43:19,840 --> 00:43:21,120
This is a key point.

1021
00:43:21,120 --> 00:43:24,440
In the generative setting, when you interpolate the latent,

1022
00:43:24,440 --> 00:43:26,400
we see at first glance what looks

1023
00:43:26,400 --> 00:43:28,240
like geometric morphing, right?

1024
00:43:28,240 --> 00:43:30,480
Looking at this, you'd be forgiven for thinking

1025
00:43:30,480 --> 00:43:33,120
that we're traversing some smooth latent manifold,

1026
00:43:33,120 --> 00:43:36,280
but on closer inspection, it's not geometric

1027
00:43:36,280 --> 00:43:37,560
or diffeomorphic at all.

1028
00:43:37,560 --> 00:43:40,720
It's more of a fuzzy, high-resolution fade-in.

1029
00:43:40,720 --> 00:43:43,280
Now, one commonly used visual argument

1030
00:43:43,280 --> 00:43:44,680
for the manifold hypothesis

1031
00:43:44,680 --> 00:43:48,560
is the MNIST digit interpolation in a generative model.

1032
00:43:48,560 --> 00:43:50,800
Our own investigation shows that there's a degree

1033
00:43:50,800 --> 00:43:53,080
of cherry picking in some of the visual examples

1034
00:43:53,080 --> 00:43:54,480
used to demonstrate this.

1035
00:43:54,480 --> 00:43:57,240
It's not actually hard to stumble across cases

1036
00:43:57,240 --> 00:44:00,840
where purported manifold interpolation is no better

1037
00:44:00,880 --> 00:44:03,280
than ambient-space linear interpolation.

1038
00:44:03,280 --> 00:44:05,720
There are many examples where latent-space interpolation

1039
00:44:05,720 --> 00:44:08,640
gives superposed and fuzzy intermediate representations

1040
00:44:08,640 --> 00:44:12,200
and even crisp images, which are unrecognizable.

1041
00:44:12,200 --> 00:44:14,440
The blur and cutting and gluing,

1042
00:44:14,440 --> 00:44:16,360
which is apparent on these images,

1043
00:44:16,360 --> 00:44:19,120
show that it's definitely not a diffeomorphism

1044
00:44:19,120 --> 00:44:20,440
in the ambient space.

1045
00:44:20,440 --> 00:44:22,120
And even though the interpolation path

1046
00:44:22,120 --> 00:44:24,120
is continuous in the latent space,

1047
00:44:24,120 --> 00:44:26,840
it's questionable whether that path is semantically relevant

1048
00:44:26,840 --> 00:44:29,000
to the classification task at hand.

1049
00:44:29,000 --> 00:44:31,120
I mean, just think of the mean value theorem.

1050
00:44:31,120 --> 00:44:32,840
It tells you that if you take the value

1051
00:44:32,840 --> 00:44:36,240
of any continuous function at two points, right,

1052
00:44:36,240 --> 00:44:38,360
you can also find a point between them

1053
00:44:38,360 --> 00:44:40,560
where the function hits the average of those values.

1054
00:44:40,560 --> 00:44:42,000
So in other words,

1055
00:44:42,000 --> 00:44:44,720
any continuous function produces a manifold,

1056
00:44:44,720 --> 00:44:46,720
but that doesn't tell you anything interesting

1057
00:44:46,720 --> 00:44:49,640
about the function beyond what we already know, right,

1058
00:44:49,640 --> 00:44:50,840
that it's continuous.

1059
00:44:51,840 --> 00:44:54,800
Now, I'm not so sure that these manifolds are even smooth.

1060
00:44:54,800 --> 00:44:56,320
When we're talking about smoothness,

1061
00:44:56,320 --> 00:44:58,400
we're only talking about local smoothness

1062
00:44:58,440 --> 00:45:00,840
and inside the polyhedrus convex hole.

1063
00:45:00,840 --> 00:45:02,320
It's linear, right?

1064
00:45:02,320 --> 00:45:04,360
But there's no smooth surfaces or manifolds

1065
00:45:04,360 --> 00:45:05,880
anywhere to be seen.

1066
00:45:05,880 --> 00:45:07,840
For any particular node in a neural network,

1067
00:45:07,840 --> 00:45:10,640
it's actually a linear function of a subset

1068
00:45:10,640 --> 00:45:13,080
of the upstream linear functions, right?

1069
00:45:13,080 --> 00:45:14,560
Very much like a decision tree,

1070
00:45:14,560 --> 00:45:16,520
but with information sharing.

1071
00:45:16,520 --> 00:45:18,640
Now, another often used visual aid

1072
00:45:18,640 --> 00:45:20,280
is this idea that a neural network

1073
00:45:20,280 --> 00:45:24,240
is effectively uncrumpling and smoothing the ambient space,

1074
00:45:24,240 --> 00:45:26,600
much like you would do with a sheet of paper

1075
00:45:26,600 --> 00:45:28,280
with successive transformations

1076
00:45:28,280 --> 00:45:30,120
in the layers of the neural network.

1077
00:45:30,120 --> 00:45:32,640
Keith and I previously agreed with this view of neural networks

1078
00:45:32,640 --> 00:45:35,320
as progressively kind of flattening out the paper,

1079
00:45:35,320 --> 00:45:37,520
but it now seems to us like they may instead

1080
00:45:37,520 --> 00:45:40,480
be progressively inserting planes

1081
00:45:40,480 --> 00:45:43,000
aligned to the facets of the paper ball

1082
00:45:43,000 --> 00:45:46,680
to chop out locally affine polyhedra or cells

1083
00:45:46,680 --> 00:45:49,320
to cover the paper's polyhedra.

1084
00:45:49,320 --> 00:45:52,000
So for us, this is an entirely new way

1085
00:45:52,000 --> 00:45:54,280
to think about neural networks, okay?

1086
00:45:54,280 --> 00:45:57,320
So in a way, this is a new parlor trick, right?

1087
00:45:57,320 --> 00:45:59,200
Do you remember that thing, the game of life,

1088
00:45:59,200 --> 00:46:00,640
Conway's game of life?

1089
00:46:00,640 --> 00:46:03,240
It looks like the shapes are smoothly morphing,

1090
00:46:03,240 --> 00:46:06,480
but they're actually toggling pixels with discrete rules.

1091
00:46:06,480 --> 00:46:09,240
Gary Marcus talks about the parlor trick of intelligence,

1092
00:46:09,240 --> 00:46:11,680
but isn't it ironic that there are more parlor tricks

1093
00:46:11,680 --> 00:46:13,520
going on than most people realize?

1094
00:46:13,520 --> 00:46:16,520
Namely, that rather than doing smooth geometric morphing

1095
00:46:16,520 --> 00:46:19,400
via interpolation, these networks are actually chopping up

1096
00:46:19,400 --> 00:46:22,280
and composing linear polyhedra.

1097
00:46:22,280 --> 00:46:25,120
Now, if you interpolate between two latent classes,

1098
00:46:25,120 --> 00:46:28,760
it might traverse several polyhedra in the intermediate space.

1099
00:46:28,760 --> 00:46:31,320
Along the way, it would pick up characteristics

1100
00:46:31,320 --> 00:46:33,040
from all of those polyhedra.

1101
00:46:33,040 --> 00:46:36,400
Some of the black regions are impossible regions,

1102
00:46:36,400 --> 00:46:39,840
so it's not possible to get there from the latent space,

1103
00:46:39,840 --> 00:46:41,520
which is very, very interesting.

1104
00:46:41,520 --> 00:46:44,120
You only see the illusion of continuous morphing,

1105
00:46:44,120 --> 00:46:47,920
where the neighboring cells are very small and very similar.

1106
00:46:47,920 --> 00:46:50,560
This is Professor Michael Bronstein.

1107
00:46:50,560 --> 00:46:52,720
I like to think of geometric deep learning

1108
00:46:52,720 --> 00:46:56,560
as not a single method or architecture, but as a mindset.

1109
00:46:56,560 --> 00:46:59,440
It's a way of looking at machine learning problems

1110
00:46:59,440 --> 00:47:03,480
from the first principles of symmetry and invariance.

1111
00:47:03,480 --> 00:47:07,600
And symmetry is a key idea that underpins our physical world

1112
00:47:07,600 --> 00:47:10,920
and the data that is created by physical processes.

1113
00:47:10,920 --> 00:47:13,560
And accounting for this structure

1114
00:47:13,560 --> 00:47:15,800
allows us to beat the course of dimensionality

1115
00:47:15,800 --> 00:47:17,840
in machine learning problems.

1116
00:47:17,840 --> 00:47:19,440
So what is the manifold hypothesis?

1117
00:47:19,440 --> 00:47:22,760
Well, natural data falls on smooth manifolds.

1118
00:47:22,760 --> 00:47:25,160
The manifold hypothesis states that real-world

1119
00:47:25,160 --> 00:47:29,000
high-dimensional data lie on low-dimensional manifolds,

1120
00:47:29,000 --> 00:47:31,600
embedded in the high-dimensional space.

1121
00:47:31,600 --> 00:47:33,360
When people invoke this hypothesis

1122
00:47:33,360 --> 00:47:34,840
in a machine learning context,

1123
00:47:34,840 --> 00:47:37,200
they're generally suggesting that neural networks

1124
00:47:37,200 --> 00:47:39,560
are actually learning this data manifold,

1125
00:47:39,560 --> 00:47:42,080
which we now find quite hard to believe, frankly.

1126
00:47:42,080 --> 00:47:43,520
At best, we think they're learning

1127
00:47:43,520 --> 00:47:45,400
some approximate aspects of it.

1128
00:47:50,400 --> 00:47:52,320
So essentially, all machine learning problems

1129
00:47:52,320 --> 00:47:53,840
that we need to deal with nowadays

1130
00:47:53,840 --> 00:47:56,120
are extremely highly dimensional.

1131
00:47:56,120 --> 00:47:58,800
Even basic image problems live in thousands

1132
00:47:58,800 --> 00:48:01,040
or even millions of dimensions.

1133
00:48:01,040 --> 00:48:03,080
Now, I think most people have this intuition

1134
00:48:03,080 --> 00:48:06,080
of convex-hole membership, which is to say, in two dimensions.

1135
00:48:06,080 --> 00:48:08,240
As you sample more and more training data,

1136
00:48:08,240 --> 00:48:11,640
the convex-hole eventually fills the entire space.

1137
00:48:11,640 --> 00:48:13,920
But the kicker is that in higher dimensions,

1138
00:48:13,920 --> 00:48:17,240
the space is so vast, this will never happen.

1139
00:48:17,760 --> 00:48:19,840
High-dimensional learning is impossible

1140
00:48:19,840 --> 00:48:22,000
due to the curse of dimensionality.

1141
00:48:22,000 --> 00:48:25,280
It only works if we make some very strong assumptions

1142
00:48:25,280 --> 00:48:27,560
about the regularities in the space of functions

1143
00:48:27,560 --> 00:48:29,120
that we need to search through.

1144
00:48:29,120 --> 00:48:31,080
The classical assumptions that we make

1145
00:48:31,080 --> 00:48:33,720
in machine learning are no longer appropriate.

1146
00:48:33,720 --> 00:48:36,560
So in general, learning in high dimensions is intractable.

1147
00:48:36,560 --> 00:48:38,600
The number of samples grows exponentially

1148
00:48:38,600 --> 00:48:39,920
with the number of dimensions.

1149
00:48:39,920 --> 00:48:41,640
And the curse of dimensionality refers

1150
00:48:41,640 --> 00:48:43,680
to the various phenomena that arise

1151
00:48:43,680 --> 00:48:46,040
when analyzing and organizing data

1152
00:48:46,080 --> 00:48:49,080
in high-dimensional spaces that do not occur

1153
00:48:49,080 --> 00:48:50,480
in low-dimensional settings,

1154
00:48:50,480 --> 00:48:54,080
such as the three-dimensional physical reality

1155
00:48:54,080 --> 00:48:56,240
of everyday experience.

1156
00:48:56,240 --> 00:48:57,920
Now, the common theme of these problems

1157
00:48:57,920 --> 00:49:00,800
is that when the dimensionality increases,

1158
00:49:00,800 --> 00:49:03,720
the volume of the space increases so fast

1159
00:49:03,720 --> 00:49:06,280
that the available data becomes sparse.

1160
00:49:06,280 --> 00:49:09,200
And this sparsity is problematic for any method

1161
00:49:09,200 --> 00:49:11,640
that requires statistical significance.

1162
00:49:11,640 --> 00:49:14,440
In order to obtain a statistically sound

1163
00:49:14,440 --> 00:49:16,840
and reliable result, the amount of data needed

1164
00:49:16,840 --> 00:49:19,520
to support the result grows exponentially

1165
00:49:19,520 --> 00:49:20,960
with the dimensionality.

1166
00:49:20,960 --> 00:49:23,800
So the curse of dimensionality in a nutshell

1167
00:49:23,800 --> 00:49:25,960
is that the probability of a new data point

1168
00:49:25,960 --> 00:49:28,640
being inside the convex hull of your training data

1169
00:49:28,640 --> 00:49:31,680
decreases exponentially with the number of dimensions.

1170
00:49:31,680 --> 00:49:34,960
As an example, to estimate a standard normal density

1171
00:49:34,960 --> 00:49:37,920
in 10 dimensions with a relative mean square error

1172
00:49:37,920 --> 00:49:41,120
of 0.1 using an efficient non-parametric technique

1173
00:49:41,120 --> 00:49:45,440
would require more than 800,000 samples.

1174
00:49:45,440 --> 00:49:47,160
Now, in the last show,

1175
00:49:47,160 --> 00:49:49,760
we discussed geometric deep learning in great detail

1176
00:49:49,760 --> 00:49:52,360
and in some sense extrapolation

1177
00:49:52,360 --> 00:49:54,600
in the curse of dimensionality are analogous.

1178
00:49:54,600 --> 00:49:55,960
The reason that these folks wanted

1179
00:49:55,960 --> 00:49:58,120
to combat the curse of dimensionality

1180
00:49:58,120 --> 00:49:59,720
was they wanted to build models

1181
00:49:59,720 --> 00:50:01,960
which could extrapolate outside the training range

1182
00:50:01,960 --> 00:50:04,000
using geometrical priors.

1183
00:50:04,000 --> 00:50:06,840
I think it's possible to build understanding

1184
00:50:06,840 --> 00:50:08,640
of the curse of dimensionality

1185
00:50:08,640 --> 00:50:12,880
through visual analogy to familiar lower dimensional shapes

1186
00:50:12,880 --> 00:50:16,200
such as circles, squares, balls and cubes.

1187
00:50:17,200 --> 00:50:20,560
Imagine perfectly sampling an entire space

1188
00:50:20,560 --> 00:50:22,440
with a regular grid.

1189
00:50:22,440 --> 00:50:26,040
This would partition space into squares or cubes

1190
00:50:26,040 --> 00:50:30,440
or hyper cubes with a sample in the center of each.

1191
00:50:31,480 --> 00:50:34,520
Now imagine around each sample,

1192
00:50:34,520 --> 00:50:37,760
a disc or ball or hyper ball

1193
00:50:37,760 --> 00:50:42,320
representing that point's region of nearness or influence.

1194
00:50:42,320 --> 00:50:45,520
And let's ask how much of the total volume

1195
00:50:45,520 --> 00:50:49,180
of that point's grid cell is actually near the sample?

1196
00:50:50,240 --> 00:50:51,960
The answer is a fraction

1197
00:50:51,960 --> 00:50:55,200
which diminishes faster than exponentially

1198
00:50:55,200 --> 00:50:56,640
with increasing dimension.

1199
00:50:57,640 --> 00:50:59,400
First think of the 2D case,

1200
00:50:59,400 --> 00:51:02,520
a disc or two ball with diameter one

1201
00:51:02,520 --> 00:51:06,560
inscribed in a square or two cube with sides of length one.

1202
00:51:07,480 --> 00:51:11,040
In each corner we of course have a dart shaped chunk

1203
00:51:11,040 --> 00:51:13,640
that isn't covered by the disc.

1204
00:51:13,640 --> 00:51:15,040
A trick to think about this

1205
00:51:15,040 --> 00:51:17,800
which I think will help in higher dimensions

1206
00:51:17,800 --> 00:51:20,720
is to imagine scanning a line segment

1207
00:51:20,720 --> 00:51:23,680
along one dimension from side to side.

1208
00:51:24,520 --> 00:51:26,080
At the edge of the square,

1209
00:51:26,080 --> 00:51:29,200
none of the segment is covered by the disc.

1210
00:51:29,200 --> 00:51:30,440
At the very center,

1211
00:51:30,440 --> 00:51:33,640
the entire segment is covered by the disc.

1212
00:51:33,640 --> 00:51:37,080
And then as it scans towards the other edge of the square,

1213
00:51:37,080 --> 00:51:39,240
the covered section begins shrinking

1214
00:51:39,240 --> 00:51:41,520
and then rapidly falls to zero.

1215
00:51:42,440 --> 00:51:47,400
The total coverage then is just the sum over that scan.

1216
00:51:47,400 --> 00:51:48,480
In two dimensions,

1217
00:51:48,480 --> 00:51:52,280
that is just the area of a disc with diameter one

1218
00:51:52,280 --> 00:51:54,400
and is about 79%.

1219
00:51:55,520 --> 00:51:57,400
Extending to three dimensions,

1220
00:51:57,400 --> 00:51:59,960
imagine a ball in a cube

1221
00:51:59,960 --> 00:52:03,760
and we scan a square from one face through the ball

1222
00:52:03,760 --> 00:52:05,960
to the opposite face.

1223
00:52:05,960 --> 00:52:07,920
As it passes the center,

1224
00:52:07,920 --> 00:52:12,160
we have our familiar inscribed disc in a square.

1225
00:52:12,160 --> 00:52:14,880
This is already missing the corner darts

1226
00:52:14,880 --> 00:52:17,320
and as we scan towards the edge of the cube,

1227
00:52:17,320 --> 00:52:20,320
the darts expand and surround the disc

1228
00:52:20,320 --> 00:52:22,760
as it shrinks to zero.

1229
00:52:22,760 --> 00:52:24,720
So by adding a third dimension,

1230
00:52:24,720 --> 00:52:27,280
we've lost even more coverage

1231
00:52:27,320 --> 00:52:32,320
and the volume of a diameter one ball is only 52%.

1232
00:52:33,800 --> 00:52:36,880
In fact, if we continue on to higher dimension,

1233
00:52:36,880 --> 00:52:39,680
the volume of a diameter one hyperball

1234
00:52:39,680 --> 00:52:42,720
decays faster than exponential,

1235
00:52:42,720 --> 00:52:45,880
factorially fast in fact.

1236
00:52:45,880 --> 00:52:48,760
The volume decays so fast

1237
00:52:48,760 --> 00:52:51,040
that even if we allowed for sampling

1238
00:52:51,040 --> 00:52:55,560
with the best possible densely packed balls,

1239
00:52:55,600 --> 00:52:58,800
theoretical work on hypersphere packing

1240
00:52:58,800 --> 00:53:01,240
tells us that the volume occupied

1241
00:53:01,240 --> 00:53:04,080
by optimally packed hyperballs

1242
00:53:04,080 --> 00:53:07,760
would still decay at least exponentially

1243
00:53:07,760 --> 00:53:10,160
with increasing dimensions.

1244
00:53:10,160 --> 00:53:12,360
This is the curse.

1245
00:53:13,280 --> 00:53:15,400
As dimensionality grows,

1246
00:53:15,400 --> 00:53:18,680
space expands exponentially,

1247
00:53:18,680 --> 00:53:21,240
points grow further apart

1248
00:53:21,240 --> 00:53:24,820
and the volume near each point vanishes.

1249
00:53:26,520 --> 00:53:29,040
In this episode, our guests argue this curse

1250
00:53:29,040 --> 00:53:32,680
dunes traditional concepts of interpolation,

1251
00:53:32,680 --> 00:53:35,560
even if we allow for the high dimensional

1252
00:53:35,560 --> 00:53:39,360
transformative power of deep neural networks.

1253
00:53:39,360 --> 00:53:41,680
Yeah, so the course of dimensionality,

1254
00:53:41,680 --> 00:53:46,680
it refers generally to the inability of algorithms

1255
00:53:50,160 --> 00:53:53,080
to keep certifying certain performance

1256
00:53:53,080 --> 00:53:54,840
as the data becomes more complex

1257
00:53:54,840 --> 00:53:56,360
and data becoming more complex here

1258
00:53:56,360 --> 00:53:58,960
means that you have more and more dimensions,

1259
00:53:58,960 --> 00:54:00,440
more and more pixels.

1260
00:54:00,440 --> 00:54:05,440
And so this inability of like scaling,

1261
00:54:05,720 --> 00:54:07,240
basically it's like it really says

1262
00:54:07,240 --> 00:54:09,880
that if I scale up the input,

1263
00:54:09,880 --> 00:54:11,560
my algorithm is gonna have more and more trouble

1264
00:54:11,560 --> 00:54:12,760
to keep the pace.

1265
00:54:12,760 --> 00:54:16,960
And so this curse can take different flavors, right?

1266
00:54:16,960 --> 00:54:20,640
So this curse might have like a statistical reason

1267
00:54:20,680 --> 00:54:25,480
in the sense that as I make my input space bigger,

1268
00:54:26,640 --> 00:54:29,720
there would be many, many, many much exponentially more

1269
00:54:30,800 --> 00:54:33,040
functions, real functions out there

1270
00:54:33,040 --> 00:54:34,520
that would explain the training set

1271
00:54:34,520 --> 00:54:37,520
that would basically pass through the training points.

1272
00:54:37,520 --> 00:54:40,120
And so the more dimensions I add,

1273
00:54:40,120 --> 00:54:43,480
the more uncertainty I have about the true function, right?

1274
00:54:43,480 --> 00:54:45,680
So I would need more and more training samples

1275
00:54:45,680 --> 00:54:47,040
to keep the pace.

1276
00:54:47,040 --> 00:54:50,240
This curse can also be from the approximation side, right?

1277
00:54:50,240 --> 00:54:52,280
So in the sense that the number of neurons

1278
00:54:52,280 --> 00:54:55,440
that I'm considering to approximate my target function,

1279
00:54:55,440 --> 00:54:57,840
I might need to keep adding more and more neurons

1280
00:54:57,840 --> 00:55:00,840
at the rate that is exponentially in dimension.

1281
00:55:00,840 --> 00:55:04,160
And the curse can also be from the computational side, right?

1282
00:55:04,160 --> 00:55:07,040
The sense that if I keep adding parameters

1283
00:55:07,040 --> 00:55:09,520
and parameters to my training model,

1284
00:55:09,520 --> 00:55:13,880
I might have to optimize to solve an optimization problem

1285
00:55:13,880 --> 00:55:16,280
that becomes exponentially harder.

1286
00:55:16,280 --> 00:55:19,200
And so you can see that you are basically bombarded

1287
00:55:19,200 --> 00:55:21,480
by three different, by all angles.

1288
00:55:21,480 --> 00:55:25,160
And so an algorithm like here in the context

1289
00:55:25,160 --> 00:55:28,280
of statistical learning or learning theory, if you want,

1290
00:55:28,280 --> 00:55:30,960
having a kind of a theorem that would say,

1291
00:55:30,960 --> 00:55:33,760
yes, I can promise you that you can learn,

1292
00:55:33,760 --> 00:55:36,320
you need to actually solve these three problems at once, right?

1293
00:55:36,320 --> 00:55:38,680
You need to be able to say that in their condition

1294
00:55:38,680 --> 00:55:40,760
that you're studying, you have an algorithm

1295
00:55:40,760 --> 00:55:42,760
that it does not suffer from approximation

1296
00:55:42,760 --> 00:55:45,000
nor statistical nor computational crisis.

1297
00:55:46,000 --> 00:55:48,160
So as you can imagine, it's very hard, right?

1298
00:55:50,120 --> 00:55:53,200
Today is a big day here at Machine Learning Street Talk.

1299
00:55:53,200 --> 00:55:55,200
We have invited one of the Godfathers

1300
00:55:55,200 --> 00:55:56,920
of deep learning on the show,

1301
00:55:56,920 --> 00:55:59,920
none other than Professor Yan Le-Kun.

1302
00:55:59,920 --> 00:56:03,720
Machine learning royalty, he has met the chiefs AI scientists

1303
00:56:03,720 --> 00:56:05,200
and a Turing Award winner,

1304
00:56:05,200 --> 00:56:08,120
which I don't have to tell you is a big deal.

1305
00:56:08,120 --> 00:56:11,720
Wiredly recognized as the Nobel Prize of Computing.

1306
00:56:11,720 --> 00:56:16,720
Yan Le-Kun was born in the suburbs of Gay Pari in the 1960s.

1307
00:56:17,640 --> 00:56:19,920
He received his PhD in computer science

1308
00:56:19,920 --> 00:56:24,000
from the modern day Sir Vaughan University in 1987,

1309
00:56:24,000 --> 00:56:27,760
during which he proposed an early form of back propagation,

1310
00:56:27,760 --> 00:56:29,280
which is of course the backbone

1311
00:56:29,280 --> 00:56:32,000
for training all neural networks.

1312
00:56:32,000 --> 00:56:33,440
Whilst he was at Bell Labs,

1313
00:56:33,440 --> 00:56:36,080
he invented convolutional neural networks,

1314
00:56:36,080 --> 00:56:37,400
which again are the backbone

1315
00:56:37,400 --> 00:56:41,080
of most major deep learning architectures in production today.

1316
00:56:42,440 --> 00:56:45,520
He has been at New York University since 2003,

1317
00:56:45,520 --> 00:56:47,920
where he is still the professor of computer science

1318
00:56:47,920 --> 00:56:49,360
and neural science.

1319
00:56:49,360 --> 00:56:52,160
Apart from being perhaps the most known researcher

1320
00:56:52,160 --> 00:56:54,000
and main ambassador for deep learning,

1321
00:56:54,000 --> 00:56:56,280
he has championed self-supervised learning

1322
00:56:56,280 --> 00:56:58,200
and energy-based models.

1323
00:56:58,200 --> 00:57:03,040
In 2013, he created the hugely prestigious ICLR conference

1324
00:57:03,040 --> 00:57:04,720
with Yoshua Ben-Gio.

1325
00:57:07,240 --> 00:57:09,920
A lover of the green screen IC.

1326
00:57:09,920 --> 00:57:10,920
Well, normally,

1327
00:57:11,680 --> 00:57:17,200
when I use Zoom, I put a substituted background.

1328
00:57:17,200 --> 00:57:20,960
Well, Tim could do it for you and post, if you want.

1329
00:57:20,960 --> 00:57:21,920
If you want.

1330
00:57:21,920 --> 00:57:23,480
Well, it's never a very good cut-out.

1331
00:57:23,480 --> 00:57:25,960
I did hack together some Python code to do it,

1332
00:57:25,960 --> 00:57:28,480
but there's no substitute for the real thing.

1333
00:57:29,360 --> 00:57:31,240
Yeah, I attempted to do this.

1334
00:57:31,240 --> 00:57:33,760
I'm running Linux, so I attempted to do this

1335
00:57:33,760 --> 00:57:37,240
by making a fake video driver,

1336
00:57:37,240 --> 00:57:39,400
but introduces a little bit of delay, so.

1337
00:57:41,920 --> 00:57:44,360
Professor Lacune, it's an absolute honour

1338
00:57:44,360 --> 00:57:45,520
to have you on the show.

1339
00:57:45,520 --> 00:57:49,920
When I first started MLST with Yanakin and Keith and Connor,

1340
00:57:49,920 --> 00:57:51,920
we were discussing how long it might take

1341
00:57:51,920 --> 00:57:55,280
to finally get the main man himself on the show.

1342
00:57:55,280 --> 00:57:58,480
You've been a huge inspiration to all of us here on MLST

1343
00:57:58,480 --> 00:58:01,120
and also you've inspired millions around the world

1344
00:58:01,120 --> 00:58:04,760
to embark on successful careers in data science

1345
00:58:04,760 --> 00:58:06,920
and to dream about what might be possible

1346
00:58:06,920 --> 00:58:08,840
with artificial intelligence.

1347
00:58:09,000 --> 00:58:11,080
Now, we've read your recent paper,

1348
00:58:11,080 --> 00:58:14,480
Learning in High Dimensions Always Amounts to Extrapolation,

1349
00:58:14,480 --> 00:58:16,440
which you co-authored with Randall,

1350
00:58:16,440 --> 00:58:18,960
Belastriro and Jerome Pesente.

1351
00:58:18,960 --> 00:58:20,760
Let's get straight into it.

1352
00:58:20,760 --> 00:58:23,480
So, we were wondering,

1353
00:58:23,480 --> 00:58:25,440
why did you write this paper, basically?

1354
00:58:27,640 --> 00:58:31,680
We were thinking whether interpolation and extrapolation

1355
00:58:31,680 --> 00:58:32,880
is a useful dichotomy.

1356
00:58:32,880 --> 00:58:34,320
I mean, at the end of the day,

1357
00:58:34,320 --> 00:58:36,600
we measure performance of learning methods

1358
00:58:36,600 --> 00:58:39,000
with accepted metrics of predicted performance,

1359
00:58:39,000 --> 00:58:40,160
such as accuracy.

1360
00:58:40,160 --> 00:58:42,880
So, suppose everyone adopts your linear convention,

1361
00:58:42,880 --> 00:58:45,280
which is a convex whole membership,

1362
00:58:45,280 --> 00:58:47,400
and concludes that high-dimensional learning

1363
00:58:47,400 --> 00:58:49,360
is always extrapolation.

1364
00:58:49,360 --> 00:58:50,200
How is that useful?

1365
00:58:50,200 --> 00:58:51,520
What do we do with this knowledge?

1366
00:58:51,520 --> 00:58:54,200
And vice versa, suppose that everyone adopts

1367
00:58:54,200 --> 00:58:55,720
a different definition

1368
00:58:55,720 --> 00:58:58,840
and concludes that learning is always interpolation.

1369
00:58:58,840 --> 00:59:00,640
What difference would that make

1370
00:59:00,640 --> 00:59:02,760
in machine learning research and practice?

1371
00:59:02,760 --> 00:59:05,880
So, in sum, why is it important to distinguish

1372
00:59:05,920 --> 00:59:07,840
whether we're predicting by interpolation or extrapolation?

1373
00:59:07,840 --> 00:59:10,960
Okay, very interesting question.

1374
00:59:10,960 --> 00:59:12,320
So, first of all,

1375
00:59:12,320 --> 00:59:16,360
this may be the first time you have me as an interviewee,

1376
00:59:16,360 --> 00:59:17,920
but I've watched a bunch of your videos.

1377
00:59:17,920 --> 00:59:20,000
So, you've had me as an audience, at least.

1378
00:59:21,480 --> 00:59:23,520
Which I find really interesting, actually.

1379
00:59:23,520 --> 00:59:26,760
So, the answer to your question is,

1380
00:59:26,760 --> 00:59:28,240
the whole point of the paper is to show

1381
00:59:28,240 --> 00:59:31,560
that this notion of interpolation versus extrapolation

1382
00:59:31,560 --> 00:59:34,360
is not useful, essentially.

1383
00:59:35,240 --> 00:59:37,960
That, you know, there's been a lot of people

1384
00:59:39,000 --> 00:59:42,440
who've been sort of saying there's a limitation

1385
00:59:42,440 --> 00:59:44,200
to deep learning, let's say,

1386
00:59:44,200 --> 00:59:46,600
or machine learning more generally,

1387
00:59:46,600 --> 00:59:48,520
because it's obvious that those things

1388
00:59:48,520 --> 00:59:49,720
basically do curve fitting,

1389
00:59:49,720 --> 00:59:51,320
and that only works for interpolation

1390
00:59:51,320 --> 00:59:53,160
and not for extrapolation.

1391
00:59:53,160 --> 00:59:58,160
And that kind of dismissal always sounded wrong to me

1392
00:59:59,280 --> 01:00:01,200
because in high-dimension,

1393
01:00:01,200 --> 01:00:02,920
your geometry in high-dimension is very different

1394
01:00:02,920 --> 01:00:06,480
from the intuition that we form

1395
01:00:06,480 --> 01:00:09,520
with curve fitting and low-dimension, right?

1396
01:00:09,520 --> 01:00:12,040
So, part of the motivation for this paper

1397
01:00:12,040 --> 01:00:14,360
is it was to kind of, you know,

1398
01:00:14,360 --> 01:00:18,640
perhaps help some people gain some intuition

1399
01:00:18,640 --> 01:00:21,520
about what really is taking place

1400
01:00:21,520 --> 01:00:24,040
in machine learning and high-dimension.

1401
01:00:24,040 --> 01:00:26,400
And also kind of, you know,

1402
01:00:26,400 --> 01:00:28,280
dispel the myth, essentially,

1403
01:00:28,280 --> 01:00:31,520
that machine learning and deep learning, in particular,

1404
01:00:31,520 --> 01:00:33,120
only does interpolation.

1405
01:00:33,120 --> 01:00:34,120
Of course, it depends a lot

1406
01:00:34,120 --> 01:00:36,160
on your definition of interpolation.

1407
01:00:36,160 --> 01:00:37,800
So here we adopted definition,

1408
01:00:37,800 --> 01:00:41,920
which is, you know, an obvious and simple generalization

1409
01:00:41,920 --> 01:00:45,640
of interpolation in low-dimension,

1410
01:00:45,640 --> 01:00:46,760
which is that, you know,

1411
01:00:46,760 --> 01:00:49,800
you interpolate when a point is in between the points

1412
01:00:49,800 --> 01:00:50,640
you already know.

1413
01:00:52,120 --> 01:00:55,320
And the generalization of this in high-dimension

1414
01:00:55,320 --> 01:00:58,400
is, you know, you interpolate when a new point

1415
01:00:58,400 --> 01:01:00,120
is inside the convex hull of the points

1416
01:01:00,120 --> 01:01:03,440
that you already know.

1417
01:01:03,440 --> 01:01:07,040
Now, what you have to realize, of course,

1418
01:01:07,040 --> 01:01:08,000
is that in high-dimension,

1419
01:01:08,000 --> 01:01:10,520
the volume of that space is actually tiny

1420
01:01:10,520 --> 01:01:13,520
compared to the overall volume of space

1421
01:01:13,520 --> 01:01:15,960
that would be filled, you know, in high-dimension.

1422
01:01:15,960 --> 01:01:19,720
And so that's kind of the intuition behind this paper

1423
01:01:19,720 --> 01:01:23,200
that, you know, any new point,

1424
01:01:23,200 --> 01:01:27,160
regardless of how you sample it to some extent,

1425
01:01:27,240 --> 01:01:29,760
any kind of reasonable ways to sample points,

1426
01:01:31,560 --> 01:01:32,520
you know, in high-dimension,

1427
01:01:32,520 --> 01:01:35,080
new points are always going to be outside the convex hull,

1428
01:01:35,080 --> 01:01:36,920
almost always going to be outside the convex hull

1429
01:01:36,920 --> 01:01:40,480
of existing points with that definition.

1430
01:01:40,480 --> 01:01:41,320
Interesting.

1431
01:01:41,320 --> 01:01:42,480
Now, there's a second part to your question.

1432
01:01:42,480 --> 01:01:43,320
It's that, you know,

1433
01:01:43,320 --> 01:01:45,280
is there a more sensible definition

1434
01:01:45,280 --> 01:01:48,200
of interpolation and extrapolation?

1435
01:01:48,200 --> 01:01:49,880
And the answer is probably yes, you know,

1436
01:01:49,880 --> 01:01:51,360
the paper doesn't address it,

1437
01:01:51,360 --> 01:01:56,360
but there are other definitions of hulls,

1438
01:01:56,720 --> 01:01:58,600
if you want, they're not necessarily convex hulls,

1439
01:01:58,600 --> 01:02:01,600
or they're not necessarily the usual type of convex hull.

1440
01:02:01,600 --> 01:02:05,040
So for example, there is a definition of a hull

1441
01:02:05,040 --> 01:02:06,280
for a cloud of points

1442
01:02:06,280 --> 01:02:09,880
that would be the smallest hyperboloid,

1443
01:02:10,640 --> 01:02:14,040
paraboloid, or ellipsoid, I should say, actually,

1444
01:02:15,320 --> 01:02:17,880
the smallest ellipsoid that contains all the points, right?

1445
01:02:17,880 --> 01:02:21,000
So some points are going to be on the surface

1446
01:02:21,000 --> 01:02:23,880
of the ellipsoid, but most of them are going to be inside.

1447
01:02:23,880 --> 01:02:26,480
And for this type of,

1448
01:02:26,480 --> 01:02:29,600
and now your definition of interpolation is that,

1449
01:02:29,600 --> 01:02:31,880
is a new point likely to be inside the ellipsoid

1450
01:02:31,880 --> 01:02:33,520
of a previous point or outside?

1451
01:02:34,720 --> 01:02:38,000
And the answer to that is probably very different

1452
01:02:38,000 --> 01:02:39,880
from the one in the paper,

1453
01:02:39,880 --> 01:02:41,880
in the sense that it's very likely

1454
01:02:41,880 --> 01:02:43,600
for a lot of natural data,

1455
01:02:43,600 --> 01:02:46,320
new points are likely to be inside

1456
01:02:46,320 --> 01:02:49,000
the containing ellipsoid.

1457
01:02:49,000 --> 01:02:51,920
So it very much depends on what you mean,

1458
01:02:51,920 --> 01:02:53,920
but it's just that the notion of interpolation

1459
01:02:53,920 --> 01:02:56,920
in high dimensional space or intuition

1460
01:02:56,920 --> 01:02:59,680
are kind of biased toward low dimension

1461
01:02:59,680 --> 01:03:02,280
and we have to be very careful what we say.

1462
01:03:03,240 --> 01:03:05,440
So that was the main thing.

1463
01:03:05,440 --> 01:03:10,440
And we have a bunch of tons of mathematicians

1464
01:03:10,880 --> 01:03:12,960
that worked on these questions for many years

1465
01:03:12,960 --> 01:03:15,520
and there's a whole bunch of theorems about this

1466
01:03:15,520 --> 01:03:17,800
that we survey in the paper.

1467
01:03:18,800 --> 01:03:20,160
Very interesting.

1468
01:03:20,160 --> 01:03:22,640
Well, we'll dig more into that in a second,

1469
01:03:22,640 --> 01:03:25,560
but folks like Gary Marcus make the case

1470
01:03:25,560 --> 01:03:29,520
that deep learning models don't reason and only interpolate.

1471
01:03:29,520 --> 01:03:31,800
Now, I know you disagree vehemently,

1472
01:03:31,800 --> 01:03:35,000
but my intuition is that reasoning and extrapolation

1473
01:03:35,000 --> 01:03:37,240
are somewhat synonymous.

1474
01:03:37,240 --> 01:03:40,800
Is it, by arguing that deep learning models extrapolate,

1475
01:03:40,800 --> 01:03:44,160
are you kind of making the argument that they reason as well?

1476
01:03:44,160 --> 01:03:45,720
Because that would make a strong case

1477
01:03:45,720 --> 01:03:47,280
that deep learning models could scout

1478
01:03:47,280 --> 01:03:48,400
artificial general intelligence.

1479
01:03:48,400 --> 01:03:50,880
Okay, so no, the short answer is no.

1480
01:03:51,960 --> 01:03:56,960
But there are important questions in there.

1481
01:03:57,960 --> 01:04:00,560
First of all, what's our definition of reasoning?

1482
01:04:01,960 --> 01:04:05,280
What is the process by which we elaborate models

1483
01:04:05,280 --> 01:04:07,600
and is there a qualitative difference

1484
01:04:07,600 --> 01:04:12,520
between a models that merely performs curve feeding

1485
01:04:12,520 --> 01:04:14,920
as we normally know it

1486
01:04:14,920 --> 01:04:17,720
and a model that has a,

1487
01:04:17,720 --> 01:04:21,680
let's say to adopt a terminology that others have proposed

1488
01:04:22,840 --> 01:04:25,840
that models that establish sort of a causal model

1489
01:04:25,840 --> 01:04:27,520
of the data you're observing,

1490
01:04:28,960 --> 01:04:31,000
which can be the basis for reasoning

1491
01:04:31,000 --> 01:04:32,840
and things like that, right?

1492
01:04:34,480 --> 01:04:36,840
And the answer to this is probably no.

1493
01:04:36,840 --> 01:04:38,560
There is a difference, of course,

1494
01:04:38,560 --> 01:04:41,080
but is it an essential qualitative difference?

1495
01:04:41,080 --> 01:04:42,840
I'm not entirely sure.

1496
01:04:42,880 --> 01:04:45,440
And then there is the argument,

1497
01:04:45,440 --> 01:04:46,920
if there is a qualitative difference,

1498
01:04:46,920 --> 01:04:48,120
which I'm not sure about,

1499
01:04:49,320 --> 01:04:53,200
would this qualitative difference be in the form of

1500
01:04:53,200 --> 01:04:55,640
fundamentally different things from deep learning,

1501
01:04:55,640 --> 01:04:58,920
things that are like discrete symbolic reasoning

1502
01:04:58,920 --> 01:05:00,720
or things of that type?

1503
01:05:00,720 --> 01:05:02,920
And to that, my answer is clearly no.

1504
01:05:02,920 --> 01:05:04,680
I do not believe that's the case.

1505
01:05:05,760 --> 01:05:08,320
So I think reasoning is certainly,

1506
01:05:08,320 --> 01:05:09,800
I always list in my talks,

1507
01:05:10,800 --> 01:05:13,800
the ability, giving the ability to learning machines

1508
01:05:13,800 --> 01:05:16,600
to reason is one of the main challenges

1509
01:05:16,600 --> 01:05:21,600
of the next decade or perhaps couple of decades in AI.

1510
01:05:21,880 --> 01:05:24,920
So I'm clearly aware of the fact

1511
01:05:24,920 --> 01:05:26,840
that they don't do this very well at the moment.

1512
01:05:26,840 --> 01:05:28,360
The big question I think is,

1513
01:05:28,360 --> 01:05:31,920
how do we get machines to reason

1514
01:05:31,920 --> 01:05:34,240
in ways that are compatible with deep learning?

1515
01:05:34,240 --> 01:05:36,320
Because most of the criticism

1516
01:05:36,320 --> 01:05:39,480
that I've heard from Gary Marcus and several others

1517
01:05:39,520 --> 01:05:41,400
towards deep learning is not a criticism

1518
01:05:41,400 --> 01:05:42,440
towards deep learning.

1519
01:05:42,440 --> 01:05:44,840
It's a criticism towards supervised learning.

1520
01:05:44,840 --> 01:05:46,680
And I agree with them.

1521
01:05:46,680 --> 01:05:48,880
Supervised learning sucks.

1522
01:05:48,880 --> 01:05:51,440
I mean, it's very limited in the sense that

1523
01:05:51,440 --> 01:05:54,720
you can train machines to do very specific tasks.

1524
01:05:54,720 --> 01:05:57,200
And because they're trying to do very specific tasks,

1525
01:05:57,200 --> 01:05:59,160
they're going to use all the biases that are in the data

1526
01:05:59,160 --> 01:06:01,280
to do that task.

1527
01:06:01,280 --> 01:06:04,080
And if you try to get outside of that task,

1528
01:06:04,080 --> 01:06:05,520
they're not gonna perform very well.

1529
01:06:05,520 --> 01:06:07,480
That's the limitation of supervised learning.

1530
01:06:07,480 --> 01:06:10,640
It has absolutely nothing to do with deep learning.

1531
01:06:11,560 --> 01:06:15,200
So regardless of which learning techniques you're gonna use,

1532
01:06:15,200 --> 01:06:16,640
you're gonna have that problem.

1533
01:06:16,640 --> 01:06:18,600
It's a problem with supervised learning.

1534
01:06:18,600 --> 01:06:21,920
So I take exception with the confusion

1535
01:06:21,920 --> 01:06:23,400
between deep learning and supervised learning.

1536
01:06:23,400 --> 01:06:25,040
Now, of course today, most of supervised learning

1537
01:06:25,040 --> 01:06:26,200
is deep learning,

1538
01:06:26,200 --> 01:06:29,000
but it's the limitation of supervised learning.

1539
01:06:29,000 --> 01:06:32,040
And as you probably know,

1540
01:06:32,040 --> 01:06:36,920
I've been a very strong advocate of self-supervised learning,

1541
01:06:36,920 --> 01:06:41,160
sort of moving away from task-specific supervised learning

1542
01:06:41,160 --> 01:06:43,080
towards more kind of generic learning

1543
01:06:43,080 --> 01:06:45,920
followed by specialization,

1544
01:06:45,920 --> 01:06:48,760
using supervised or reinforcement learning.

1545
01:06:48,760 --> 01:06:53,680
And in that, I've kind of followed the path of Jeff Hinton,

1546
01:06:53,680 --> 01:06:58,680
who's been basically advocating for this for 40 years now.

1547
01:07:00,520 --> 01:07:03,120
And for me, it's less time.

1548
01:07:03,120 --> 01:07:06,040
I disagreed with him originally and changed my mind

1549
01:07:06,720 --> 01:07:08,120
about 20 years ago.

1550
01:07:08,120 --> 01:07:13,120
So that's the story, really.

1551
01:07:13,480 --> 01:07:16,400
It's awesome that you have the ability to change your mind

1552
01:07:16,400 --> 01:07:18,720
and admit that this is a good thing.

1553
01:07:18,720 --> 01:07:20,840
We run into so many people that are just,

1554
01:07:20,840 --> 01:07:23,040
they don't wanna change their mind no matter what.

1555
01:07:23,040 --> 01:07:26,440
It's seen as a bad thing, which it isn't at all.

1556
01:07:26,440 --> 01:07:30,040
No, I mean, I think the essence of being a scientist

1557
01:07:30,040 --> 01:07:35,040
is to be able to change your mind in the face of evidence.

1558
01:07:35,680 --> 01:07:39,760
You cannot be a scientist if you have preconceived ideas.

1559
01:07:39,760 --> 01:07:42,600
On the other hand, I've also been known

1560
01:07:43,640 --> 01:07:47,120
to hold very tight to ideas that I thought were true

1561
01:07:47,120 --> 01:07:51,600
in the face of considerable differing opinion

1562
01:07:51,600 --> 01:07:52,840
from my dear colleague.

1563
01:07:52,840 --> 01:07:57,840
So it also helps to have deeply held convictions sometimes.

1564
01:08:00,040 --> 01:08:04,280
So to this notion of interpolation,

1565
01:08:04,320 --> 01:08:07,440
I think we've also, and you mentioned this, right?

1566
01:08:07,440 --> 01:08:08,920
It depends on your definition.

1567
01:08:08,920 --> 01:08:12,440
And we've also had a little bit of the feeling

1568
01:08:12,440 --> 01:08:15,520
that people might be talking past one another

1569
01:08:15,520 --> 01:08:17,920
when they criticize, can it interpolate?

1570
01:08:17,920 --> 01:08:19,680
Can't it interpolate?

1571
01:08:19,680 --> 01:08:21,240
And we've come up with this example

1572
01:08:21,240 --> 01:08:24,440
of I give a classifier a dog.

1573
01:08:24,440 --> 01:08:27,520
And that dog is like the most doggy dog I've ever seen.

1574
01:08:27,520 --> 01:08:29,440
Like it's like such a dog.

1575
01:08:29,440 --> 01:08:32,880
It's more dog than any dog I had in the training dataset.

1576
01:08:32,920 --> 01:08:37,480
So clearly that dog is like outside of the training distribution,

1577
01:08:37,480 --> 01:08:40,200
outside of the convex hull in any space,

1578
01:08:40,200 --> 01:08:44,840
like be that the original space, be that the latent space.

1579
01:08:44,840 --> 01:08:48,760
It's like, all that matters is that it's at the correct side

1580
01:08:48,760 --> 01:08:51,360
of the classifying hyperplane.

1581
01:08:51,360 --> 01:08:55,440
So that would not be contained on sort of in the convex hull.

1582
01:08:55,440 --> 01:08:58,760
It would not be contained in the smallest ellipsoid

1583
01:08:58,760 --> 01:08:59,760
and so on.

1584
01:08:59,760 --> 01:09:04,040
So do you think when people talk about interpolation,

1585
01:09:04,040 --> 01:09:09,160
they might be talking about something maybe different?

1586
01:09:09,160 --> 01:09:10,720
Because you can also make the example

1587
01:09:10,720 --> 01:09:13,600
if I take the convex hull of all the training data points

1588
01:09:13,600 --> 01:09:15,800
and I take a point in the middle of it,

1589
01:09:15,800 --> 01:09:18,560
it's the neural network going to be pretty bad at it, right?

1590
01:09:18,560 --> 01:09:22,120
That's going to be like a messy blur of pixels

1591
01:09:22,120 --> 01:09:25,280
and it's not going to be very, very good at it.

1592
01:09:25,280 --> 01:09:32,240
So could you maybe make the strongest argument you could

1593
01:09:32,240 --> 01:09:37,240
for people saying neural networks just interpolate,

1594
01:09:37,240 --> 01:09:41,400
but what notion of interpolation would you substitute?

1595
01:09:41,400 --> 01:09:45,080
Would it be like, oh, they just nearest neighbor classify

1596
01:09:45,080 --> 01:09:51,360
or if you had to give the best shot at sort of doing the,

1597
01:09:51,360 --> 01:09:54,640
oh, neural networks just do something, what would it be?

1598
01:09:55,520 --> 01:09:57,160
I wouldn't give any answer of this type

1599
01:09:57,160 --> 01:10:02,640
because the answer would very heavily depend on the architecture.

1600
01:10:02,640 --> 01:10:09,800
So for example, if most of the layers in a neural net,

1601
01:10:09,800 --> 01:10:12,040
I mean, you can use weighted sums and sigmoids

1602
01:10:12,040 --> 01:10:13,480
or weighted sums and values, right?

1603
01:10:13,480 --> 01:10:19,040
And that effectively performs a classification separating

1604
01:10:19,040 --> 01:10:24,680
the input into kind of half, two halves with the hyperplanes.

1605
01:10:24,680 --> 01:10:26,680
But you could also use a Euclidean distance unit.

1606
01:10:26,680 --> 01:10:28,600
So Euclidean distance unit computes the distance

1607
01:10:28,600 --> 01:10:31,640
of the input vector to a weight vector,

1608
01:10:31,640 --> 01:10:33,680
which is not a weight vector anymore,

1609
01:10:33,680 --> 01:10:35,840
and then passes it through some sort of decreasing function

1610
01:10:35,840 --> 01:10:37,640
like an exponential.

1611
01:10:37,640 --> 01:10:41,040
And what that gives you is a Gaussian bump in input space

1612
01:10:41,040 --> 01:10:43,840
where the activity will be high

1613
01:10:43,840 --> 01:10:47,640
if the input is close to the vector and low if it's not.

1614
01:10:47,640 --> 01:10:50,520
When you have an attention layer,

1615
01:10:50,520 --> 01:10:54,640
where you have a whole bunch of those kind of vector comparison

1616
01:10:54,640 --> 01:10:56,960
where the vectors are normalized and then you do a softmax,

1617
01:10:56,960 --> 01:11:00,640
you're basically doing sort of a multinomial version of this.

1618
01:11:00,640 --> 01:11:03,640
And this is using transformers, right?

1619
01:11:03,640 --> 01:11:05,880
And in all kinds of architectures these days.

1620
01:11:05,880 --> 01:11:09,760
So those things that compare vectors with each other

1621
01:11:09,760 --> 01:11:13,200
and only react to the two vectors nearby

1622
01:11:13,200 --> 01:11:16,600
do a sort of glorified nearest neighbor

1623
01:11:16,600 --> 01:11:17,960
with some interpolation.

1624
01:11:17,960 --> 01:11:25,520
By the way, most kernel-based methods do this, right?

1625
01:11:25,520 --> 01:11:30,120
Kernel methods basically are one layer of such PRS comparisons.

1626
01:11:30,120 --> 01:11:33,240
I mean comparisons of the input with the training samples.

1627
01:11:33,240 --> 01:11:35,120
And then you pass that through some function,

1628
01:11:35,120 --> 01:11:36,960
some response function.

1629
01:11:36,960 --> 01:11:40,480
So Gaussian SVMs, for example, are the perfect example of this.

1630
01:11:40,480 --> 01:11:42,920
And then you take those cores and you compute a linear combination.

1631
01:11:42,920 --> 01:11:46,800
That's glorified interpolating nearest neighbor.

1632
01:11:46,800 --> 01:11:48,840
To some extent, transformers do this as well.

1633
01:11:48,840 --> 01:11:52,880
Transformers are the basic module of a transformer.

1634
01:11:52,880 --> 01:11:57,800
It's basically an associative memory that compares the incoming vector

1635
01:11:57,800 --> 01:12:00,880
to a bunch of keys and then gives you an answer

1636
01:12:00,880 --> 01:12:03,360
that is some linear combination of values, right?

1637
01:12:03,360 --> 01:12:07,640
So that is a good idea.

1638
01:12:07,640 --> 01:12:09,480
It learns fast.

1639
01:12:09,480 --> 01:12:12,640
There used to be a whole series of models that were now forgotten,

1640
01:12:12,640 --> 01:12:15,960
that are now forgotten in the 90s called RBF networks.

1641
01:12:15,960 --> 01:12:18,480
So an RBF network was basically a two-layer neural net.

1642
01:12:18,480 --> 01:12:20,480
Well, the first layer was very much like an SVM.

1643
01:12:20,480 --> 01:12:23,520
This was before SVMs that, again,

1644
01:12:23,520 --> 01:12:26,320
had riddle-based functions, responses, right?

1645
01:12:26,320 --> 01:12:30,000
Comparing an input to vectors and passing it to an exponential

1646
01:12:30,000 --> 01:12:31,960
or something like this.

1647
01:12:31,960 --> 01:12:33,840
And then you would initialize the first layer.

1648
01:12:33,840 --> 01:12:34,960
You could train it with backprop,

1649
01:12:34,960 --> 01:12:36,360
but it would get stuck in local minima.

1650
01:12:36,360 --> 01:12:38,160
So that wasn't a good idea.

1651
01:12:38,160 --> 01:12:42,600
You had to initialize the first layer with something like k-means

1652
01:12:42,600 --> 01:12:45,720
or mixer or Gaussian or something like that.

1653
01:12:45,720 --> 01:12:48,680
And then you could either just train the second layer

1654
01:12:48,680 --> 01:12:52,040
or fine-tune the entire thing with backprop.

1655
01:12:52,040 --> 01:12:53,040
And that worked pretty well.

1656
01:12:53,040 --> 01:12:57,280
It was actually a fairly fast learner.

1657
01:12:57,280 --> 01:12:59,840
They were faster than the neural nets.

1658
01:12:59,840 --> 01:13:01,880
So those things, for those things,

1659
01:13:01,880 --> 01:13:04,280
the answer to your question is of one type.

1660
01:13:04,280 --> 01:13:08,200
They're basically doing interpolation with kernels.

1661
01:13:08,200 --> 01:13:13,280
And it's very much like a smooth version of nearest neighbors.

1662
01:13:13,280 --> 01:13:15,240
But then for classical neural nets,

1663
01:13:15,240 --> 01:13:17,240
where you have either a hyperboleic tantrum,

1664
01:13:17,240 --> 01:13:19,800
nonlinearity, or a value, or something of that type,

1665
01:13:19,800 --> 01:13:23,080
something with a kink in it, or two kinks,

1666
01:13:23,080 --> 01:13:24,480
the answer is different.

1667
01:13:24,480 --> 01:13:27,800
There it's a whole cone of response

1668
01:13:27,800 --> 01:13:32,600
that will produce a positive response versus not

1669
01:13:32,600 --> 01:13:34,720
if you take a combination of units, right?

1670
01:13:34,720 --> 01:13:39,520
So does it make sense to talk about interpolation

1671
01:13:39,520 --> 01:13:40,520
in that kind of geometry?

1672
01:13:40,520 --> 01:13:42,000
I'm not sure.

1673
01:13:42,000 --> 01:13:44,640
I think people should just not use

1674
01:13:44,640 --> 01:13:48,680
the word interpolation for that situation.

1675
01:13:48,680 --> 01:13:54,720
But the fact that kind of things in a neural net,

1676
01:13:54,720 --> 01:13:56,120
the response of a neural net actually

1677
01:13:56,120 --> 01:14:01,320
are kind of half-spaced, extend beyond the training points,

1678
01:14:01,320 --> 01:14:03,680
perhaps has something to do with the fact

1679
01:14:03,680 --> 01:14:06,120
that they do extrapolate in certain ways

1680
01:14:06,120 --> 01:14:08,400
that may or may not be relevant for the problem,

1681
01:14:08,400 --> 01:14:11,640
but they do extrapolate.

1682
01:14:11,640 --> 01:14:14,520
Well, so we've taught some about the interpolation

1683
01:14:14,520 --> 01:14:16,760
versus extrapolation versus what was in the paper,

1684
01:14:16,760 --> 01:14:19,680
because the paper is somewhat a rigid definition.

1685
01:14:19,680 --> 01:14:21,720
It's like there's this convex hull,

1686
01:14:21,720 --> 01:14:24,040
and if you're inside, it's interpolation.

1687
01:14:24,040 --> 01:14:26,840
If you're outside, it's extrapolation.

1688
01:14:26,840 --> 01:14:29,920
And you talked about maybe we could use a ball and ellipsoid

1689
01:14:29,920 --> 01:14:32,000
instead, but there's kind of a key thing there,

1690
01:14:32,000 --> 01:14:35,200
which is that it's going across all dimensions.

1691
01:14:35,200 --> 01:14:39,240
So you're inside the convex hull.

1692
01:14:39,240 --> 01:14:43,120
It's a necessary condition that on every single dimension,

1693
01:14:43,120 --> 01:14:46,200
your sample data point falls within the range

1694
01:14:46,200 --> 01:14:47,560
of the training data.

1695
01:14:47,560 --> 01:14:49,680
We could kind of go the opposite extreme

1696
01:14:49,680 --> 01:14:53,800
and say that you're interpolating if any dimension falls

1697
01:14:53,800 --> 01:14:55,840
within the training domain, or rather,

1698
01:14:55,840 --> 01:15:00,120
you're extrapolating only if every single dimension falls

1699
01:15:00,120 --> 01:15:01,360
outside the training range.

1700
01:15:01,360 --> 01:15:04,600
And both of those are kind of these exponential extremes.

1701
01:15:04,600 --> 01:15:07,120
And it would seem like the truth is maybe somewhere

1702
01:15:07,120 --> 01:15:09,520
in between, like there's a subset of dimensions

1703
01:15:09,520 --> 01:15:13,400
that might be salient for any particular data point.

1704
01:15:13,400 --> 01:15:16,120
So why are we kind of using this very exponential extreme

1705
01:15:16,120 --> 01:15:17,560
definition?

1706
01:15:17,560 --> 01:15:18,880
It was still exponential.

1707
01:15:18,880 --> 01:15:21,080
Even if you, you know, you can try

1708
01:15:21,080 --> 01:15:23,720
to divide the set of dimensions into the ones that are useful

1709
01:15:23,720 --> 01:15:26,560
and the ones that are just going to use parameters that

1710
01:15:26,560 --> 01:15:31,480
are useless, but that are not relevant for the task at hand.

1711
01:15:31,480 --> 01:15:33,400
But first of all, that task is very difficult.

1712
01:15:33,400 --> 01:15:35,880
I mean, basically, the entire machine learning problem

1713
01:15:35,920 --> 01:15:38,320
is exactly that, trying to figure out

1714
01:15:38,320 --> 01:15:41,040
what information in the input is relevant for the task

1715
01:15:41,040 --> 01:15:44,000
and what part should basically be considered

1716
01:15:44,000 --> 01:15:46,920
as noise or useless parameters.

1717
01:15:46,920 --> 01:15:49,160
So solving that problem is solving the machine learning

1718
01:15:49,160 --> 01:15:50,360
problem, first of all.

1719
01:15:50,360 --> 01:15:54,000
Second of all, what we show in the paper is that regardless,

1720
01:15:54,000 --> 01:15:58,120
so the experiment that Randall did, as a matter of fact,

1721
01:15:58,120 --> 01:16:04,200
is train a ResNet 18 or 50 or whatever on a ResNet

1722
01:16:04,200 --> 01:16:05,680
or CIFAR or MNIST.

1723
01:16:05,680 --> 01:16:08,240
And then take the output representation

1724
01:16:08,240 --> 01:16:16,120
and see whether this sort of exponential growth of number

1725
01:16:16,120 --> 01:16:19,440
of data points to stay within the interpolation regime

1726
01:16:19,440 --> 01:16:20,040
still exists.

1727
01:16:20,040 --> 01:16:21,800
And it still exists.

1728
01:16:21,800 --> 01:16:24,400
It's just that the dimension now, instead of being

1729
01:16:24,400 --> 01:16:28,120
the input dimension of the data, which may be very large,

1730
01:16:28,120 --> 01:16:30,240
is the dimension of the embedding.

1731
01:16:30,240 --> 01:16:33,320
But as soon as the dimension of the embedding

1732
01:16:33,320 --> 01:16:37,400
is larger than 20 or so, the number of training samples

1733
01:16:37,400 --> 01:16:41,680
you would need to stay within the interpolation regime

1734
01:16:41,680 --> 01:16:44,520
is already going to be very large.

1735
01:16:44,520 --> 01:16:47,400
2 to the 20 is a large number.

1736
01:16:47,400 --> 01:16:50,760
So there is another experiment that

1737
01:16:50,760 --> 01:16:52,720
shows that if your entire data set is

1738
01:16:52,720 --> 01:16:55,440
contained within a linear subspace,

1739
01:16:55,440 --> 01:16:59,520
so the ambient space may be of dimension 100,

1740
01:16:59,520 --> 01:17:03,040
but the entire data set is within a linear subspace

1741
01:17:03,040 --> 01:17:05,360
of dimension 4, then what matters

1742
01:17:05,360 --> 01:17:09,120
is the dimension 4, not the dimension 100.

1743
01:17:09,120 --> 01:17:16,920
So automatically, that convex hull process, what

1744
01:17:16,920 --> 01:17:19,000
matters to it is not the dimension of the input space,

1745
01:17:19,000 --> 01:17:22,080
it's the dimension of the linear subspace that

1746
01:17:22,080 --> 01:17:24,760
contains all the points.

1747
01:17:24,760 --> 01:17:26,800
Right, but I think what I'm saying though

1748
01:17:26,800 --> 01:17:31,000
is that you can invert the definition so that whether or not

1749
01:17:31,040 --> 01:17:33,840
you're extrapolating just becomes 1 minus,

1750
01:17:33,840 --> 01:17:36,160
whether or not you're interpolating, or vice versa.

1751
01:17:36,160 --> 01:17:38,920
And so you can wind up with an equally extreme definition

1752
01:17:38,920 --> 01:17:42,760
that concludes everything is interpolation.

1753
01:17:42,760 --> 01:17:46,160
So I'm just wondering, it seems like there should be a balance.

1754
01:17:46,160 --> 01:17:48,000
And this kind of gets to what you said earlier.

1755
01:17:48,000 --> 01:17:53,120
Is there a more useful definition of interpolation

1756
01:17:53,120 --> 01:17:56,800
versus extrapolation that could have some utility

1757
01:17:56,800 --> 01:17:57,840
for machine learning?

1758
01:17:57,840 --> 01:18:00,880
I mean, it's not clear you would get much out of it.

1759
01:18:01,320 --> 01:18:03,240
I give you a potential candidate,

1760
01:18:03,240 --> 01:18:07,760
which is whether the points are contained in a ellipsoid that

1761
01:18:07,760 --> 01:18:10,080
contains all the points, you could make it a sphere.

1762
01:18:10,080 --> 01:18:13,360
A sphere would be, of course, bigger volume

1763
01:18:13,360 --> 01:18:15,360
because the data doesn't necessarily

1764
01:18:15,360 --> 01:18:18,280
have the same radius in all dimensions.

1765
01:18:18,280 --> 01:18:22,920
But I think the result would be fairly similar for both,

1766
01:18:22,920 --> 01:18:28,560
for both definitions that in that case,

1767
01:18:28,560 --> 01:18:30,120
things would be mostly interpolation.

1768
01:18:30,120 --> 01:18:32,800
But that would be kind of a weird definition

1769
01:18:32,800 --> 01:18:37,280
of interpolation in the sense that it

1770
01:18:37,280 --> 01:18:40,480
would rely on a false intuition about high dimensional

1771
01:18:40,480 --> 01:18:42,200
geometry.

1772
01:18:42,200 --> 01:18:46,640
OK, calling this interpolation basically

1773
01:18:46,640 --> 01:18:49,200
would mean that you have a completely wrong idea about how

1774
01:18:49,200 --> 01:18:50,840
things behave in high dimension, right,

1775
01:18:50,840 --> 01:18:54,200
about geometry in high dimension.

1776
01:18:54,200 --> 01:18:55,760
I'm trying to get some intuition about this

1777
01:18:55,760 --> 01:18:59,360
because we spoke to Professor Bronstein and his friends,

1778
01:18:59,360 --> 01:19:01,920
including Joanne Bruner, and they're

1779
01:19:01,920 --> 01:19:06,120
talking about geometric learning and their approach

1780
01:19:06,120 --> 01:19:07,920
to defeating the curse of dimensionality

1781
01:19:07,920 --> 01:19:11,320
is finding geometric priors to reduce the hypothesis space,

1782
01:19:11,320 --> 01:19:12,400
which is quite interesting.

1783
01:19:12,400 --> 01:19:16,000
A lot of this is about our intuitions of interpolation.

1784
01:19:16,000 --> 01:19:18,640
Because if I take an autoencoder and I train it on MNIST

1785
01:19:18,640 --> 01:19:21,360
and I start interpolating between the train

1786
01:19:21,360 --> 01:19:24,360
or the test examples, it is learning this continuous

1787
01:19:24,360 --> 01:19:25,560
geometric morphing.

1788
01:19:25,560 --> 01:19:28,840
And that is what people's intuition is about interpolation.

1789
01:19:28,840 --> 01:19:32,280
I know you would say that's extrapolation, right?

1790
01:19:32,280 --> 01:19:35,080
Yeah, no, I think it would be a type of interpolation,

1791
01:19:35,080 --> 01:19:37,240
but it would be some sort of geodesic interpolation,

1792
01:19:37,240 --> 01:19:40,120
interpolation on a manifold, right?

1793
01:19:40,120 --> 01:19:42,000
I mean, so certainly if you have some idea

1794
01:19:42,000 --> 01:19:44,720
about the structure of the data manifold,

1795
01:19:44,720 --> 01:19:47,080
you can interpolate within that manifold

1796
01:19:47,080 --> 01:19:50,480
without making big mistakes.

1797
01:19:50,480 --> 01:19:53,280
But then you're back to the original problem

1798
01:19:53,280 --> 01:19:53,800
of machine learning.

1799
01:19:53,800 --> 01:19:54,720
What is that manifold?

1800
01:19:54,720 --> 01:19:55,880
How do you learn that manifold?

1801
01:19:55,880 --> 01:19:59,320
That's one of the essential problems of machine learning.

1802
01:19:59,320 --> 01:20:01,600
And learning the structure of a data manifold

1803
01:20:01,600 --> 01:20:06,240
is a much more complex problem than learning a task

1804
01:20:06,240 --> 01:20:08,920
of classifying objects on that manifold.

1805
01:20:08,920 --> 01:20:11,800
For example, classifying points on that manifold.

1806
01:20:11,800 --> 01:20:17,200
So there is this old adage by which

1807
01:20:17,200 --> 01:20:22,320
I used to be a big fan of, which is why I was

1808
01:20:22,320 --> 01:20:24,200
disagreeing with Jeff Hinton about the usefulness

1809
01:20:24,200 --> 01:20:26,480
of unsupervised learning.

1810
01:20:26,480 --> 01:20:32,200
Because if you have a task at hand for which you have data

1811
01:20:32,200 --> 01:20:35,200
that you can use to train a supervised system,

1812
01:20:35,200 --> 01:20:41,920
why would you go to the trouble of pre-training a system

1813
01:20:41,920 --> 01:20:46,200
in unsupervised mode knowing that the unsupervised learning

1814
01:20:46,200 --> 01:20:49,840
problem is considerably more complicated both from every

1815
01:20:49,840 --> 01:20:52,920
aspect you can think of, certainly

1816
01:20:52,920 --> 01:20:55,320
from the theoretical point of view.

1817
01:20:55,320 --> 01:21:00,040
Vladimir Vapnik actually has kind of a similar opinion.

1818
01:21:00,040 --> 01:21:03,080
One of the few things that he and I agree on,

1819
01:21:03,080 --> 01:21:05,840
or agreed on, at least, which is why would you

1820
01:21:05,840 --> 01:21:08,280
want to solve a more complex problem than you have to?

1821
01:21:08,280 --> 01:21:10,520
But of course, that forgets the fact

1822
01:21:10,520 --> 01:21:12,680
that you don't want to solve a single problem.

1823
01:21:12,680 --> 01:21:16,880
You want to take advantage of multiple problems

1824
01:21:16,880 --> 01:21:23,920
and whatever data you have available at your disposal

1825
01:21:23,920 --> 01:21:26,800
to prepare for learning a task.

1826
01:21:26,800 --> 01:21:34,520
And we have access to considerably more unlabeled data

1827
01:21:34,520 --> 01:21:37,440
than we have access to labeled data.

1828
01:21:37,440 --> 01:21:42,320
And therefore, why not use unlabeled data in large quantity

1829
01:21:42,320 --> 01:21:44,240
to pre-train very large neural nets

1830
01:21:44,240 --> 01:21:46,120
so that we can function them for the tasks

1831
01:21:46,520 --> 01:21:47,720
that we are interested in?

1832
01:21:47,720 --> 01:21:49,480
So that's the whole idea of self-supervised learning.

1833
01:21:49,480 --> 01:21:52,720
And of course, we all know that that kind of strategy has

1834
01:21:52,720 --> 01:21:56,160
been unbelievably successful in natural language

1835
01:21:56,160 --> 01:22:00,760
processing with denoising autoencoder or master

1836
01:22:00,760 --> 01:22:04,960
autoencoder or bird-style training of transformers

1837
01:22:04,960 --> 01:22:08,200
followed by a supervised phase.

1838
01:22:08,200 --> 01:22:10,440
That success has not yet translated

1839
01:22:10,440 --> 01:22:14,880
in the domain of vision, although I'm sort of predicting

1840
01:22:14,880 --> 01:22:17,280
that it will happen very soon.

1841
01:22:17,280 --> 01:22:22,200
But I mean, there's a lot of interesting avenues there

1842
01:22:22,200 --> 01:22:25,120
and recent progress.

1843
01:22:25,120 --> 01:22:27,680
And then there is the cake analogy, right?

1844
01:22:27,680 --> 01:22:32,720
The fact that any sample in sort of a self-supervised context

1845
01:22:32,720 --> 01:22:36,400
give you way more information than a supervised sample,

1846
01:22:36,400 --> 01:22:38,400
the label from supervised learning,

1847
01:22:38,400 --> 01:22:42,600
a fortiori reinforcement for the context of reinforcement.

1848
01:22:42,600 --> 01:22:44,760
Absolutely.

1849
01:22:45,080 --> 01:22:46,440
We interviewed Ishan.

1850
01:22:46,440 --> 01:22:48,640
So we've done a show on self-supervised learning.

1851
01:22:48,640 --> 01:22:50,840
We're huge fans of self-supervised learning.

1852
01:22:50,840 --> 01:22:54,360
And I know your vision is to get to these latent predictive

1853
01:22:54,360 --> 01:22:56,200
models to solve that problem.

1854
01:22:56,200 --> 01:22:59,800
That's something else I changed my mind on the last two years.

1855
01:22:59,800 --> 01:23:02,000
This may be outside the scope of this particular interview.

1856
01:23:02,000 --> 01:23:05,640
But yeah, there's basically two major architectures, right,

1857
01:23:05,640 --> 01:23:07,200
for self-supervised learning, particularly

1858
01:23:07,200 --> 01:23:09,040
in the context of vision.

1859
01:23:09,040 --> 01:23:12,840
And the main characteristic of both of them

1860
01:23:12,840 --> 01:23:16,920
is the fact that it can handle multimodal prediction.

1861
01:23:16,920 --> 01:23:18,600
So if you have a system, let's say you

1862
01:23:18,600 --> 01:23:20,720
want to do video prediction or something like that.

1863
01:23:20,720 --> 01:23:22,600
So you have a piece of a video clip.

1864
01:23:22,600 --> 01:23:25,640
You want to predict the next video clip.

1865
01:23:25,640 --> 01:23:27,080
Or you just want a machine that tells you

1866
01:23:27,080 --> 01:23:30,240
whether a proposed video clip continuation clip is

1867
01:23:30,240 --> 01:23:31,920
a good continuation of the previous one.

1868
01:23:31,920 --> 01:23:33,240
You don't want it to predict.

1869
01:23:33,240 --> 01:23:36,080
You just want it to tell you whether it's a good one.

1870
01:23:36,080 --> 01:23:37,120
So you have two architectures.

1871
01:23:37,120 --> 01:23:40,000
The first one is a latent variable predictive architecture

1872
01:23:40,000 --> 01:23:42,120
that predicts the next video clip.

1873
01:23:42,120 --> 01:23:44,080
And of course, you have to parameterize the prediction

1874
01:23:44,080 --> 01:23:46,560
with a latent variable, because there are multiple predictions

1875
01:23:46,560 --> 01:23:48,040
that are plausible.

1876
01:23:48,040 --> 01:23:49,840
So I used to be a big fan of that.

1877
01:23:49,840 --> 01:23:51,640
And about two years ago, I changed my mind.

1878
01:23:51,640 --> 01:23:54,680
Maybe a year and a half.

1879
01:23:54,680 --> 01:23:57,880
The other approach is something I played with in the early 90s,

1880
01:23:57,880 --> 01:23:59,800
came up with some of the early models for this.

1881
01:23:59,800 --> 01:24:01,880
And it's called a joint embedding architecture,

1882
01:24:01,880 --> 01:24:05,440
where you have the first and the second video clip both going

1883
01:24:05,440 --> 01:24:07,600
through an neural net.

1884
01:24:07,600 --> 01:24:09,200
And then what you're doing is you're

1885
01:24:09,200 --> 01:24:11,640
training the system so that the representation of the second

1886
01:24:11,640 --> 01:24:14,720
video clip is easily predictable from the representation

1887
01:24:14,720 --> 01:24:16,760
of the first one.

1888
01:24:16,760 --> 01:24:18,240
So there you're not predicting pixels.

1889
01:24:18,240 --> 01:24:20,680
You're predicting representations.

1890
01:24:20,680 --> 01:24:22,480
But you still have a system that can tell you

1891
01:24:22,480 --> 01:24:23,840
here is a video clip and the second one

1892
01:24:23,840 --> 01:24:25,520
tell me if they are compatible, if one

1893
01:24:25,520 --> 01:24:27,960
is a good continuation of the other.

1894
01:24:27,960 --> 01:24:30,720
The main reason why I stayed away from those architectures,

1895
01:24:30,720 --> 01:24:34,120
because I knew that you had to use, in the past,

1896
01:24:34,120 --> 01:24:35,680
you had to use contrastive learning.

1897
01:24:35,680 --> 01:24:38,400
You had to have pairs of things that are compatible,

1898
01:24:38,400 --> 01:24:40,480
as well as pairs of things that are incompatible.

1899
01:24:40,480 --> 01:24:42,280
And in high dimension, there's just too many ways

1900
01:24:42,280 --> 01:24:44,120
two things can be incompatible.

1901
01:24:44,120 --> 01:24:46,000
And so that was going to do to failure.

1902
01:24:46,000 --> 01:24:49,040
And I played with this back in the suit

1903
01:24:49,040 --> 01:24:53,520
to be called Siamese networks at a paper in 1992, 1993,

1904
01:24:53,520 --> 01:24:56,360
on doing signature verification using those techniques.

1905
01:24:56,360 --> 01:24:59,560
Jeff Hinton had a slightly different method

1906
01:24:59,560 --> 01:25:02,680
based on maximizing mutual information with his former

1907
01:25:02,680 --> 01:25:04,720
student, Sue Becker.

1908
01:25:04,720 --> 01:25:06,240
But then in the last two years, we've

1909
01:25:06,240 --> 01:25:09,400
had methods that are non-contrastive

1910
01:25:09,800 --> 01:25:13,560
that allows us to train those joint embedding

1911
01:25:13,560 --> 01:25:14,120
architectures.

1912
01:25:14,120 --> 01:25:16,440
So I've become a big fan of them now.

1913
01:25:16,440 --> 01:25:20,240
And it's because of algorithms like BYUL,

1914
01:25:20,240 --> 01:25:26,040
from our friends at DeepBind, like Barlow Twins, whose idea came

1915
01:25:26,040 --> 01:25:30,080
from Stephane Denis, who's doing a postdoc with me at FAIR.

1916
01:25:30,080 --> 01:25:36,360
He's now a professor at the University of Alto in Finland.

1917
01:25:37,320 --> 01:25:38,640
And then more recently, Vic Craig,

1918
01:25:38,640 --> 01:25:43,920
which is a kind of improvement, if you want,

1919
01:25:43,920 --> 01:25:45,880
on Barlow Twins, that is also based

1920
01:25:45,880 --> 01:25:47,800
on the same idea that Jeff Hinton and Sue Becker

1921
01:25:47,800 --> 01:25:51,040
had of maximizing a measure of mutual information between the

1922
01:25:51,040 --> 01:25:54,600
outputs to networks, but in a slightly different way.

1923
01:25:54,600 --> 01:25:56,600
So I'm really excited about those things.

1924
01:25:56,600 --> 01:25:59,240
And again, I change my mind all the time,

1925
01:25:59,240 --> 01:26:06,120
whenever a good idea seems to be overcome by a better one.

1926
01:26:06,160 --> 01:26:11,320
So in this whole space of maybe interpolation, extrapolation,

1927
01:26:11,320 --> 01:26:13,480
but also data manifolds, and so on,

1928
01:26:13,480 --> 01:26:16,840
what's your view on things like data augmentation,

1929
01:26:16,840 --> 01:26:21,360
training and simulation, and domain adaptation, and so on?

1930
01:26:21,360 --> 01:26:25,000
Because it could be argued that these things increase

1931
01:26:25,000 --> 01:26:28,480
the convex hull of the training data.

1932
01:26:28,480 --> 01:26:31,800
They sort of make the distribution broader.

1933
01:26:31,800 --> 01:26:35,040
Or is that just also out of question?

1934
01:26:35,040 --> 01:26:35,560
Not much.

1935
01:26:35,560 --> 01:26:37,040
I think data augmentation does not

1936
01:26:37,040 --> 01:26:42,720
increase the volume of the data point cloud,

1937
01:26:42,720 --> 01:26:44,520
if you want, very much.

1938
01:26:44,520 --> 01:26:48,280
Because those augmentations are generally fairly local,

1939
01:26:48,280 --> 01:26:50,480
in dimensions that are already explored.

1940
01:26:50,480 --> 01:26:53,480
So it may increase it a little bit,

1941
01:26:53,480 --> 01:26:56,960
but not significantly, I think.

1942
01:26:56,960 --> 01:26:58,800
Obviously, data augmentation is very useful.

1943
01:26:58,800 --> 01:27:01,880
I mean, we've used this for decades,

1944
01:27:01,880 --> 01:27:04,560
so it's not a new phenomenon either.

1945
01:27:04,560 --> 01:27:07,120
There's a lot of ideas along those lines, right?

1946
01:27:07,120 --> 01:27:12,200
So it's the idea where you give two examples,

1947
01:27:12,200 --> 01:27:15,080
and you have two examples in your training set,

1948
01:27:15,080 --> 01:27:18,200
and you actually do an interpolation in input space

1949
01:27:18,200 --> 01:27:20,480
to generate a fake example that's in between the two,

1950
01:27:20,480 --> 01:27:22,600
and you try to produce the intermediate target

1951
01:27:22,600 --> 01:27:24,840
between the two original examples.

1952
01:27:24,840 --> 01:27:25,560
Mix up.

1953
01:27:25,560 --> 01:27:26,480
Mix up.

1954
01:27:26,480 --> 01:27:27,000
Mix up.

1955
01:27:27,000 --> 01:27:28,080
Yeah.

1956
01:27:28,080 --> 01:27:28,600
There is that.

1957
01:27:28,600 --> 01:27:29,720
There is distillation.

1958
01:27:29,720 --> 01:27:32,040
There is various techniques like this

1959
01:27:32,040 --> 01:27:35,200
are basically implicitly kind of ways

1960
01:27:35,200 --> 01:27:37,400
to fill in the space between samples, right,

1961
01:27:37,400 --> 01:27:41,160
with other kind of fake samples, or virtual samples,

1962
01:27:41,160 --> 01:27:42,360
if you want.

1963
01:27:42,360 --> 01:27:46,600
There was a paper by Patrick Simard and me,

1964
01:27:46,600 --> 01:27:50,400
and John Denker many years ago, when we were all at Bell Labs,

1965
01:27:50,400 --> 01:27:51,760
on something called tangent prop.

1966
01:27:51,760 --> 01:27:55,720
So the idea of tangent prop was kind of somewhat similar.

1967
01:27:55,720 --> 01:27:58,640
The idea was you take a training sample,

1968
01:27:58,640 --> 01:28:01,080
and you're going to be able to distort that training

1969
01:28:01,080 --> 01:28:02,760
sample in several ways.

1970
01:28:02,760 --> 01:28:05,800
You could generate points by data augmentation.

1971
01:28:05,800 --> 01:28:08,360
But the other thing you can do is just figure out

1972
01:28:08,360 --> 01:28:15,840
the plane in which those augmentations live, OK?

1973
01:28:15,840 --> 01:28:18,560
And that plane is going to be a tangent plane

1974
01:28:18,560 --> 01:28:21,280
to the data manifold.

1975
01:28:21,280 --> 01:28:25,800
What you want is your, for a given class, for example, right?

1976
01:28:25,800 --> 01:28:28,520
So what you want is your input output function

1977
01:28:28,520 --> 01:28:30,280
that the neural net learns to be invariant

1978
01:28:30,280 --> 01:28:33,040
to those little distortions.

1979
01:28:33,040 --> 01:28:35,280
And you can do this by just augmenting the data,

1980
01:28:35,280 --> 01:28:37,640
or you can do this by explicitly having a regularization

1981
01:28:37,640 --> 01:28:40,920
term that says the overall derivative of the function

1982
01:28:40,920 --> 01:28:45,560
in the direction of the spending vectors of that plane

1983
01:28:45,560 --> 01:28:48,440
should be 0, or should be whatever it is that you want it

1984
01:28:48,440 --> 01:28:51,280
to be, but 0 is a good target for this.

1985
01:28:51,280 --> 01:28:53,240
So that's called tangent prop.

1986
01:28:53,240 --> 01:28:57,560
And you can think of it as a regularizer that says,

1987
01:28:57,560 --> 01:28:59,920
I don't just want my input output function

1988
01:28:59,920 --> 01:29:01,200
to have this value at this point.

1989
01:29:01,200 --> 01:29:03,320
I also want its derivative to be 0

1990
01:29:03,320 --> 01:29:06,400
when I change the input in those directions.

1991
01:29:06,400 --> 01:29:10,400
That's another indirect way of doing data augmentation

1992
01:29:10,400 --> 01:29:13,800
without doing data augmentation, essentially.

1993
01:29:13,800 --> 01:29:16,040
And I think there's a lot of those ideas

1994
01:29:16,040 --> 01:29:19,960
that are very useful, certainly.

1995
01:29:19,960 --> 01:29:25,680
So I think there seems to be a spectrum coming back

1996
01:29:25,680 --> 01:29:27,320
to this interpolation, extrapolation.

1997
01:29:27,320 --> 01:29:29,520
You said yourself you don't believe neural networks

1998
01:29:29,520 --> 01:29:32,440
can do something like discrete, abstractive reasoning,

1999
01:29:32,440 --> 01:29:33,120
and so on.

2000
01:29:33,120 --> 01:29:34,720
No, no, no, I didn't say that.

2001
01:29:34,720 --> 01:29:41,080
Or I said, we need to do work for them to be able to do that.

2002
01:29:41,080 --> 01:29:44,560
But I have no doubt that eventually they will.

2003
01:29:44,560 --> 01:29:46,480
I mean, there's a lot of work in this area already.

2004
01:29:46,480 --> 01:29:49,680
And I'll give you some more specific examples if you want.

2005
01:29:49,680 --> 01:29:53,120
I mean, that was actually kind of the nature of my question.

2006
01:29:53,120 --> 01:29:56,400
Where do you think there is this spectrum of what

2007
01:29:56,400 --> 01:29:59,560
people think neural networks are doing now

2008
01:29:59,560 --> 01:30:01,480
or will be able to do later?

2009
01:30:01,480 --> 01:30:04,880
Where do you think the actual biggest disagreement

2010
01:30:04,880 --> 01:30:08,320
between the community right now lies?

2011
01:30:08,320 --> 01:30:11,920
And what can we do to, what experiments, what evidence

2012
01:30:11,920 --> 01:30:15,000
can we gather to solve these disagreements?

2013
01:30:15,000 --> 01:30:15,320
Right.

2014
01:30:15,320 --> 01:30:20,600
So I think we shouldn't talk too much about neural networks

2015
01:30:20,600 --> 01:30:25,200
because people have a relatively narrow picture of what

2016
01:30:25,200 --> 01:30:26,640
a neural network is, right?

2017
01:30:26,640 --> 01:30:28,600
It's a bunch of layers of neurons

2018
01:30:28,600 --> 01:30:30,960
that perform wetted sums and pass a result

2019
01:30:30,960 --> 01:30:32,400
through a nonlinear function.

2020
01:30:32,400 --> 01:30:33,920
And there's a fixed number of layers.

2021
01:30:33,920 --> 01:30:38,640
Maybe they can be recurrent, and you produce an output.

2022
01:30:38,640 --> 01:30:41,760
That's a very restricted view of what deep learning systems

2023
01:30:41,760 --> 01:30:42,920
can do.

2024
01:30:42,920 --> 01:30:49,720
So let me take an example of what reasoning might mean.

2025
01:30:49,720 --> 01:30:52,840
Reasoning might be seen at least one particular type of reasoning

2026
01:30:52,840 --> 01:30:58,000
might be seen as a minimization of an energy function,

2027
01:30:58,000 --> 01:31:00,240
not with respect to parameters in a neural net,

2028
01:31:00,240 --> 01:31:02,680
but with respect to latent variables.

2029
01:31:02,680 --> 01:31:05,520
And a lot of systems that are in use today do that.

2030
01:31:05,520 --> 01:31:08,240
So a lot of speech recognition systems, for example,

2031
01:31:08,240 --> 01:31:12,720
have a decoder on the output, which essentially given

2032
01:31:12,720 --> 01:31:19,920
a list of scores for what a particular segment of speech

2033
01:31:19,920 --> 01:31:21,800
could be in terms of what sound it could be,

2034
01:31:21,920 --> 01:31:25,280
or what syllable, or whatever, the decoder basically

2035
01:31:25,280 --> 01:31:29,080
figures out what is the best interpretation of that sequence

2036
01:31:29,080 --> 01:31:35,000
of sound that makes it a legal sentence in the dictionary.

2037
01:31:35,000 --> 01:31:39,320
Techniques like this have been used for 25 years now

2038
01:31:39,320 --> 01:31:42,160
in the context of speech recognition and handwriting

2039
01:31:42,160 --> 01:31:45,040
recognition, even before neural nets were used for those things.

2040
01:31:45,040 --> 01:31:51,360
So back in the old days of hidden Markov models.

2041
01:31:51,400 --> 01:31:55,640
And those techniques have been really developed.

2042
01:31:55,640 --> 01:31:58,600
Now, if you think about what those techniques do,

2043
01:31:58,600 --> 01:32:01,000
first of all, all the operations that are done in those systems

2044
01:32:01,000 --> 01:32:01,640
are differentiable.

2045
01:32:01,640 --> 01:32:04,000
You can backpropagate gradient through an operation

2046
01:32:04,000 --> 01:32:07,520
that will figure out the shortest path in a graph

2047
01:32:07,520 --> 01:32:09,360
using dynamic programming.

2048
01:32:09,360 --> 01:32:11,040
The first paper on this was in 1991

2049
01:32:11,040 --> 01:32:13,040
by my friend Leon Boutou and Xavier de Leoncourt.

2050
01:32:13,040 --> 01:32:17,360
This is not recent stuff that we're trying to do speech recognition.

2051
01:32:17,360 --> 01:32:20,280
You can backpropagate gradient through a lot of different modules.

2052
01:32:20,320 --> 01:32:25,040
What those things do is that they infer a latent variable

2053
01:32:25,040 --> 01:32:27,120
by basically doing energy minimization.

2054
01:32:27,120 --> 01:32:28,520
Finding the shortest path in a graph

2055
01:32:28,520 --> 01:32:32,440
is a form of energy minimization.

2056
01:32:32,440 --> 01:32:34,800
And so you can have, in a deep learning system,

2057
01:32:34,800 --> 01:32:36,920
you can have a module that has a latent variable.

2058
01:32:36,920 --> 01:32:39,160
And what this module will do is figure out

2059
01:32:39,160 --> 01:32:41,840
a value of the latent variable that minimizes some energy

2060
01:32:41,840 --> 01:32:42,480
function.

2061
01:32:42,480 --> 01:32:43,720
It could be a prediction error.

2062
01:32:43,720 --> 01:32:45,160
It could be something else.

2063
01:32:45,160 --> 01:32:48,520
It could be regularizers in it, et cetera.

2064
01:32:48,520 --> 01:32:51,080
But it basically performs a minimization.

2065
01:32:51,080 --> 01:32:57,240
And that type of operation basically

2066
01:32:57,240 --> 01:33:02,400
can be used to, like, you can formulate

2067
01:33:02,400 --> 01:33:06,720
most types of reasoning in that form.

2068
01:33:06,720 --> 01:33:10,080
Now, it's not necessarily in a continuous differentiable space,

2069
01:33:10,080 --> 01:33:12,640
but almost all reasoning, even in classical AI,

2070
01:33:12,640 --> 01:33:16,200
can be formulated in terms of optimizing some sort of function.

2071
01:33:16,200 --> 01:33:17,640
Like, you won't do a SAT problem, right?

2072
01:33:17,640 --> 01:33:20,360
If you have a collection of Boolean formula,

2073
01:33:20,360 --> 01:33:22,680
you want to find a combination of variables

2074
01:33:22,680 --> 01:33:23,920
that satisfy this formula.

2075
01:33:23,920 --> 01:33:25,040
It's an optimization problem.

2076
01:33:25,040 --> 01:33:26,280
It's combinatorial optimization,

2077
01:33:26,280 --> 01:33:28,040
but it's an optimization problem.

2078
01:33:28,040 --> 01:33:28,920
You want to do planning.

2079
01:33:28,920 --> 01:33:31,560
So planning is the best example.

2080
01:33:31,560 --> 01:33:36,760
So you can do a planning where the thing you're controlling

2081
01:33:36,760 --> 01:33:39,440
as discrete actions and discrete states,

2082
01:33:39,440 --> 01:33:41,240
and you have to use dynamic programming.

2083
01:33:41,240 --> 01:33:44,560
You can back propagate gradient through a system that

2084
01:33:44,560 --> 01:33:45,680
does that.

2085
01:33:45,680 --> 01:33:49,040
But more classical planning is in continuous space.

2086
01:33:49,040 --> 01:33:52,080
So planning for, like, planning the trajectory of an arm

2087
01:33:52,080 --> 01:33:55,600
for a robot, planning the trajectory of a rocket,

2088
01:33:55,600 --> 01:33:58,760
what you have is a differentiable model,

2089
01:33:58,760 --> 01:34:01,800
dynamical model of what is the state of the system

2090
01:34:01,800 --> 01:34:04,000
you're trying to control at time t plus 1

2091
01:34:04,000 --> 01:34:06,280
as a function of the state at time t

2092
01:34:06,280 --> 01:34:07,720
and as a function of the action you take.

2093
01:34:07,720 --> 01:34:09,920
And perhaps as a function of some random variable,

2094
01:34:09,920 --> 01:34:13,040
you can't measure from the environment

2095
01:34:13,040 --> 01:34:16,040
to something like that to make it non-deterministic.

2096
01:34:16,040 --> 01:34:18,880
Now, you can enroll that model.

2097
01:34:18,880 --> 01:34:23,360
So start with time equals 0, and then make a hypothesis

2098
01:34:23,360 --> 01:34:25,560
about a sequence of action, and then

2099
01:34:25,560 --> 01:34:29,760
apply your predictive model of the next state of the system.

2100
01:34:29,760 --> 01:34:31,920
And then at the end, you can measure some cost function.

2101
01:34:31,920 --> 01:34:36,520
Is my rocket docking with a space station or it's far away?

2102
01:34:36,520 --> 01:34:38,360
And how much fuel have I consumed?

2103
01:34:38,360 --> 01:34:40,280
Or whatever, right?

2104
01:34:40,280 --> 01:34:44,400
Or have I reached the goal of the arm,

2105
01:34:44,400 --> 01:34:46,920
avoiding all the obstacles, right?

2106
01:34:46,920 --> 01:34:50,000
So what you can do now is an inference process

2107
01:34:50,000 --> 01:34:52,200
which consists in figuring out what is the sequence of action

2108
01:34:52,200 --> 01:34:55,080
I should take to minimize this cost function according

2109
01:34:55,080 --> 01:34:56,960
to my dynamical model.

2110
01:34:56,960 --> 01:34:58,640
You can do this by gradient descent.

2111
01:34:58,640 --> 01:35:02,320
And basically, that's what the Kitty-Bison algorithm,

2112
01:35:02,320 --> 01:35:05,400
in optimal control, that goes back to 1962.

2113
01:35:05,400 --> 01:35:07,480
And it consists in basically doing backprop through time.

2114
01:35:07,480 --> 01:35:08,680
It's as simple as that, right?

2115
01:35:08,720 --> 01:35:10,240
So you do backprop through time.

2116
01:35:10,240 --> 01:35:14,120
So in effect, optimal control theorist invented backprop

2117
01:35:14,120 --> 01:35:15,280
back in the early 60s.

2118
01:35:15,280 --> 01:35:17,560
But nobody realized you could use this for machine

2119
01:35:17,560 --> 01:35:20,040
learning until the V-80s, essentially.

2120
01:35:20,040 --> 01:35:22,240
Or the 70s.

2121
01:35:22,240 --> 01:35:24,600
Just to jump in there, because there's a little caveat there,

2122
01:35:24,600 --> 01:35:26,840
which is that you can do that at backprop

2123
01:35:26,840 --> 01:35:29,560
if you have a fixed number of steps.

2124
01:35:29,560 --> 01:35:31,960
Like what backprop can't handle is the case

2125
01:35:31,960 --> 01:35:34,120
where I want some expandable number of steps.

2126
01:35:34,120 --> 01:35:37,520
We have no algorithms that can currently optimize.

2127
01:35:37,520 --> 01:35:43,120
For example, neural networks with a variable number of layers.

2128
01:35:43,120 --> 01:35:44,720
Like we just can't train those, right?

2129
01:35:44,720 --> 01:35:45,560
No, it's not true.

2130
01:35:45,560 --> 01:35:46,080
It's not true.

2131
01:35:46,080 --> 01:35:47,680
That's what recurrent nets are.

2132
01:35:47,680 --> 01:35:50,640
You can have a varying number of iterations in your recurrent

2133
01:35:50,640 --> 01:35:50,960
net.

2134
01:35:50,960 --> 01:35:53,560
And what I'm describing here is an unfolded recurrent net.

2135
01:35:53,560 --> 01:35:55,960
It's the same model that you apply every time step, right?

2136
01:35:55,960 --> 01:35:57,920
So it's very much like a recurrent net.

2137
01:35:57,920 --> 01:36:01,960
And you can very well have an unknown number of steps.

2138
01:36:01,960 --> 01:36:03,560
You don't know a priori how long it's

2139
01:36:03,560 --> 01:36:06,400
going to take for your rocket to get to the space station, right?

2140
01:36:06,600 --> 01:36:10,160
So you may have kind of a large potential number of steps.

2141
01:36:10,160 --> 01:36:13,280
And you can have a cost function which will count,

2142
01:36:13,280 --> 01:36:15,560
like how much time it's going to take to get there.

2143
01:36:15,560 --> 01:36:17,920
And this will be part of the optimization, for example.

2144
01:36:17,920 --> 01:36:19,480
And this is classic optimal control.

2145
01:36:19,480 --> 01:36:22,440
I'm not telling you anything that I came up with.

2146
01:36:22,440 --> 01:36:26,240
This is from at least the 60s and 70s.

2147
01:36:26,240 --> 01:36:29,920
No, but the backprop, that variable number of layers

2148
01:36:29,920 --> 01:36:33,080
has to be done in kind of a classic iterative,

2149
01:36:33,080 --> 01:36:35,440
try out the variable number of layers

2150
01:36:35,480 --> 01:36:37,360
and see what the backprop comes up with.

2151
01:36:37,360 --> 01:36:40,320
Or you start with a very large number of layers

2152
01:36:40,320 --> 01:36:42,200
and then let backprop try and find.

2153
01:36:42,200 --> 01:36:45,520
But still, what I'm saying is there is fundamentally

2154
01:36:45,520 --> 01:36:48,560
two different kinds of computation that are at play here.

2155
01:36:48,560 --> 01:36:51,600
One is like the finite fixed thing,

2156
01:36:51,600 --> 01:36:54,640
and then you do some differentiable optimization on it.

2157
01:36:54,640 --> 01:36:57,600
Another kind of computation is this discrete symbolic

2158
01:36:57,600 --> 01:36:59,280
that has like an expandable memory

2159
01:36:59,280 --> 01:37:01,320
and an unbounded amount of time

2160
01:37:01,320 --> 01:37:03,120
to sit there kind of computing on things.

2161
01:37:03,160 --> 01:37:05,080
Okay, I put a stop right there.

2162
01:37:06,080 --> 01:37:07,640
What I'm describing has nothing to do

2163
01:37:07,640 --> 01:37:09,560
with symbolic, discrete, or anything.

2164
01:37:09,560 --> 01:37:11,640
I understand that, I understand that.

2165
01:37:11,640 --> 01:37:14,520
But you seem to be equating a variable number

2166
01:37:14,520 --> 01:37:17,800
with discrete symbolic, this is two different things.

2167
01:37:17,800 --> 01:37:19,960
Okay, let me pose it in this form,

2168
01:37:19,960 --> 01:37:22,080
which is that I can very easily write down

2169
01:37:22,080 --> 01:37:26,760
a symbolic program that can output the arbitrary digit of pi,

2170
01:37:26,760 --> 01:37:28,760
like the nth digit of pi.

2171
01:37:28,760 --> 01:37:31,400
Nobody can train a neural network that can do that.

2172
01:37:32,360 --> 01:37:35,080
So what's the difference between these two types

2173
01:37:35,080 --> 01:37:38,360
of computation and where in the future might we go

2174
01:37:38,360 --> 01:37:41,480
with neural networks or by augmenting neural networks,

2175
01:37:41,480 --> 01:37:44,840
whether it's differentiable Turing machines or whatever,

2176
01:37:44,840 --> 01:37:46,840
or neural Turing machines,

2177
01:37:46,840 --> 01:37:49,240
to try and bridge that gap and capability?

2178
01:37:49,240 --> 01:37:51,040
Okay, before we bridge that gap,

2179
01:37:51,040 --> 01:37:54,800
the algorithm to compute the digit of pi,

2180
01:37:54,800 --> 01:37:57,120
there's only a tiny number of humans,

2181
01:37:57,120 --> 01:38:01,080
only in the last few centuries that I figured this one out.

2182
01:38:01,560 --> 01:38:04,960
I'm interested in how is it that a cat can plan

2183
01:38:04,960 --> 01:38:08,800
to jump on the table and not fall or even open a door

2184
01:38:08,800 --> 01:38:10,520
or do things like that, right?

2185
01:38:10,520 --> 01:38:11,880
Once we figure this one out,

2186
01:38:11,880 --> 01:38:13,920
maybe we can think about kind of more complex stuff,

2187
01:38:13,920 --> 01:38:16,840
like designing algorithms that involve

2188
01:38:16,840 --> 01:38:18,480
complex mathematical concepts.

2189
01:38:18,480 --> 01:38:22,920
But I think we're way, we're not there, right?

2190
01:38:22,920 --> 01:38:26,880
We're faced with much more fundamental issues

2191
01:38:26,880 --> 01:38:30,560
of how do we learn predictive models to the world

2192
01:38:30,680 --> 01:38:33,000
by observing it and things of that type,

2193
01:38:33,000 --> 01:38:36,440
which are much more basic that most animals can do

2194
01:38:36,440 --> 01:38:41,440
and digit of pi, it's some kind of a line.

2195
01:38:43,560 --> 01:38:45,640
So, because you're talking about model predictive control,

2196
01:38:45,640 --> 01:38:49,640
is it possible that there are different shapes of problem?

2197
01:38:49,640 --> 01:38:52,600
So, for example, some problems are interpolative

2198
01:38:52,600 --> 01:38:56,720
and are solvable using differentiable models.

2199
01:38:56,720 --> 01:38:58,840
Do you think there exists problems

2200
01:38:59,560 --> 01:39:01,400
that are quite discreet in nature

2201
01:39:01,400 --> 01:39:04,000
and a different type of approach would be required?

2202
01:39:04,000 --> 01:39:04,840
Of course, yeah.

2203
01:39:04,840 --> 01:39:07,320
I mean, there is certainly a lot of situations

2204
01:39:07,320 --> 01:39:11,920
where the mapping from action to result

2205
01:39:11,920 --> 01:39:15,440
is very kind of discontinuous, if you want, right?

2206
01:39:15,440 --> 01:39:16,920
This is qualitatively different.

2207
01:39:17,800 --> 01:39:22,800
And so there are many situations where

2208
01:39:23,000 --> 01:39:25,240
or it's somewhat continuous

2209
01:39:25,240 --> 01:39:30,240
and situations where you change your action a little bit

2210
01:39:30,280 --> 01:39:33,520
and it results in a completely different outcome.

2211
01:39:33,520 --> 01:39:35,240
And so the big question, I think,

2212
01:39:35,240 --> 01:39:40,240
is how you handle that kind of uncertainty in the search.

2213
01:39:40,360 --> 01:39:42,680
And you can think of sort of two extremes.

2214
01:39:42,680 --> 01:39:45,000
So at one extreme in the continuous case

2215
01:39:45,000 --> 01:39:46,680
is the case where you're planning the trajectory

2216
01:39:46,680 --> 01:39:51,680
of a rocket or that's pretty continuous

2217
01:39:52,280 --> 01:39:54,080
and differentiable and everything you want.

2218
01:39:54,080 --> 01:39:55,320
And you don't even need to learn the model.

2219
01:39:55,320 --> 01:39:57,200
You basically write it down, right?

2220
01:39:57,200 --> 01:39:59,480
But there are situations there where you're flying a drone

2221
01:39:59,480 --> 01:40:01,520
or something where you might need to learn the model

2222
01:40:01,520 --> 01:40:04,080
because there's so many kind of nonlinear effects

2223
01:40:04,080 --> 01:40:05,720
that it's probably better to learn it

2224
01:40:05,720 --> 01:40:06,760
and people are working on this.

2225
01:40:06,760 --> 01:40:09,480
Same for like working robots and stuff like that.

2226
01:40:09,480 --> 01:40:11,000
Then there are things that are a little intermediate

2227
01:40:11,000 --> 01:40:14,640
where there are sort of hard constraints on what you can do.

2228
01:40:14,640 --> 01:40:17,000
So you want to grab an object with a robot arm,

2229
01:40:17,000 --> 01:40:18,400
but there is obstacles in between

2230
01:40:18,400 --> 01:40:21,040
and you don't want to bump into them and things like this.

2231
01:40:21,040 --> 01:40:23,120
So people tend to put like penalty functions

2232
01:40:23,120 --> 01:40:25,120
to make this sort of more continuous,

2233
01:40:25,120 --> 01:40:27,360
but there's sort of qualitative difference

2234
01:40:27,360 --> 01:40:30,160
between using your left arm or your right arm, for example,

2235
01:40:30,160 --> 01:40:35,000
or going, scratching your left ear with your left hand

2236
01:40:35,000 --> 01:40:37,560
or with your right ear going to the back of your head, right?

2237
01:40:37,560 --> 01:40:40,440
Those are qualitatively different solutions.

2238
01:40:40,440 --> 01:40:42,840
And then all the way to the other side,

2239
01:40:42,840 --> 01:40:46,000
there is intrinsically discrete problems

2240
01:40:46,000 --> 01:40:49,400
with that may be fully observable

2241
01:40:49,400 --> 01:40:53,040
with some uncertainty like chess and go-playing, okay?

2242
01:40:53,960 --> 01:40:57,680
And those we can handle to some extent

2243
01:40:57,680 --> 01:41:00,680
because the number of actions is finite.

2244
01:41:00,680 --> 01:41:02,200
It goes exponentially, but it's finite.

2245
01:41:02,200 --> 01:41:07,200
And so using kind of ways to direct the search,

2246
01:41:07,440 --> 01:41:10,600
despite the fact that the search space is exponential,

2247
01:41:10,600 --> 01:41:13,080
using neural nets as basically evaluation functions

2248
01:41:13,080 --> 01:41:14,640
to direct the search in the right way

2249
01:41:14,640 --> 01:41:18,800
and doing multi-color tree search and blah, blah, blah,

2250
01:41:18,800 --> 01:41:20,720
will work, okay?

2251
01:41:20,720 --> 01:41:22,280
With sufficiently many trials,

2252
01:41:23,760 --> 01:41:25,320
in the completely deterministic,

2253
01:41:25,320 --> 01:41:28,280
fully observable, differentiable case,

2254
01:41:28,280 --> 01:41:31,520
that's classical model predictive control, that's fine.

2255
01:41:31,520 --> 01:41:35,120
Then there is some stuff that we really don't know how to do

2256
01:41:35,120 --> 01:41:36,160
and it's twofold.

2257
01:41:36,160 --> 01:41:39,520
One is the model is not given to us

2258
01:41:39,520 --> 01:41:43,560
by equations derived from first principles, right?

2259
01:41:43,560 --> 01:41:46,840
So the stuff we're trying to do is in the real world

2260
01:41:46,840 --> 01:41:48,160
and it's got complicated dynamics

2261
01:41:48,160 --> 01:41:49,960
that we can't just model from first principle.

2262
01:41:49,960 --> 01:41:51,360
So we have to learn the model.

2263
01:41:52,520 --> 01:41:54,600
That's the first issue.

2264
01:41:54,600 --> 01:41:57,160
Second issue, the model lives in the world

2265
01:41:57,160 --> 01:41:59,560
and the world is not completely predictable.

2266
01:41:59,560 --> 01:42:01,320
It may be deterministic,

2267
01:42:01,320 --> 01:42:04,640
but you don't have full observation.

2268
01:42:04,640 --> 01:42:07,440
So you cannot predict exactly what's gonna happen

2269
01:42:07,440 --> 01:42:08,960
in the world because the world is being the world

2270
01:42:08,960 --> 01:42:10,800
or as a consequence of your actions.

2271
01:42:12,240 --> 01:42:13,680
So how do you deal with the certainty?

2272
01:42:13,680 --> 01:42:14,920
And for that, you need predictive models

2273
01:42:14,920 --> 01:42:16,120
that can represent uncertainty.

2274
01:42:16,120 --> 01:42:19,800
And we are back to the issue I was telling you about earlier.

2275
01:42:19,800 --> 01:42:21,080
Do you need latent variable models

2276
01:42:21,080 --> 01:42:22,680
or joint embedding architectures, right?

2277
01:42:22,680 --> 01:42:26,240
That's, and I've changed my mind about this as I told you.

2278
01:42:26,240 --> 01:42:31,240
So then there is the third obstacle,

2279
01:42:31,920 --> 01:42:35,480
which is, is the problem we're trying to solve

2280
01:42:35,480 --> 01:42:38,640
of the so continuous differentiable nature

2281
01:42:38,640 --> 01:42:41,680
or of the kind of completely discreet,

2282
01:42:41,680 --> 01:42:42,800
you know, qualitatively discreet

2283
01:42:42,800 --> 01:42:45,160
depending on what action you take nature.

2284
01:42:45,160 --> 01:42:48,640
And I think most problems are some combination of the two.

2285
01:42:48,680 --> 01:42:52,600
So, you know, you're trying to build a box

2286
01:42:52,600 --> 01:42:54,120
at a wood or something like that, right?

2287
01:42:54,120 --> 01:42:56,640
You can make the box bigger or smaller.

2288
01:42:56,640 --> 01:42:58,760
You know, you can hit the nail in this way or that way.

2289
01:42:58,760 --> 01:43:02,040
And that may be sort of continuous and differentiable.

2290
01:43:02,040 --> 01:43:03,040
But then there is, you know,

2291
01:43:03,040 --> 01:43:05,120
you put glue or screws or nails,

2292
01:43:05,120 --> 01:43:09,360
do you, or use kind of more classical carpentry or whatever.

2293
01:43:09,360 --> 01:43:11,960
And those are kind of discreet choices.

2294
01:43:11,960 --> 01:43:13,960
What type of wood are you using, you know, things like that.

2295
01:43:13,960 --> 01:43:16,360
So I think the, you know,

2296
01:43:16,360 --> 01:43:18,760
human mind is able to deal with all of those situations,

2297
01:43:18,760 --> 01:43:22,200
have, you know, know to use differentiable continuous stuff

2298
01:43:22,200 --> 01:43:26,280
when they have to and use the sort of discreet exploration

2299
01:43:26,280 --> 01:43:28,080
when we have to as well.

2300
01:43:28,080 --> 01:43:31,360
But we have to realize that humans are really, really bad

2301
01:43:31,360 --> 01:43:33,680
at the discreet exploration stuff.

2302
01:43:33,680 --> 01:43:34,760
We totally suck at it.

2303
01:43:36,360 --> 01:43:38,560
If we didn't, then we would be better than computers

2304
01:43:38,560 --> 01:43:40,920
at playing chess and go, but we're not.

2305
01:43:40,920 --> 01:43:42,560
We're actually really, really bad.

2306
01:43:43,560 --> 01:43:45,560
So...

2307
01:43:45,560 --> 01:43:47,560
That's fascinating.

2308
01:43:47,560 --> 01:43:51,560
So you seem to be saying in a way that you are a fan of hybrid models.

2309
01:43:51,560 --> 01:43:52,560
No.

2310
01:43:52,560 --> 01:43:55,560
And something, well, something like AlphaGo, for example.

2311
01:43:55,560 --> 01:43:58,560
I mean, that's basically, there's an interpolative space

2312
01:43:58,560 --> 01:44:00,560
which, you know, guides a discreet search.

2313
01:44:00,560 --> 01:44:02,560
Let's say it like that.

2314
01:44:02,560 --> 01:44:04,560
Do you think that's, is that a good thing?

2315
01:44:04,560 --> 01:44:06,560
Or do you think that's...

2316
01:44:06,560 --> 01:44:08,560
So you like that kind of model?

2317
01:44:08,560 --> 01:44:12,560
Okay, there is something very interesting about, you know,

2318
01:44:12,560 --> 01:44:14,560
in the context of reinforcement running about this,

2319
01:44:14,560 --> 01:44:16,560
which is actual critic models, right?

2320
01:44:16,560 --> 01:44:20,560
And you could think of all of the stuff that actually works

2321
01:44:20,560 --> 01:44:22,560
in reinforcement running.

2322
01:44:22,560 --> 01:44:24,560
I mean, they don't work in the real world, right?

2323
01:44:24,560 --> 01:44:27,560
But they work in games and stuff and simulated environment.

2324
01:44:27,560 --> 01:44:32,560
They very often use actual critic type architectures.

2325
01:44:32,560 --> 01:44:34,560
And the idea of a critic, you know,

2326
01:44:34,560 --> 01:44:37,560
goes back to early papers by Saturn and Bartow.

2327
01:44:37,560 --> 01:44:42,560
And the basic idea of a critic is to have a differentiable

2328
01:44:42,560 --> 01:44:45,560
approximation of your value function, right?

2329
01:44:45,560 --> 01:44:50,560
So you train a small neural net essentially to compute,

2330
01:44:50,560 --> 01:44:55,560
to estimate, to predict the value function from your state.

2331
01:44:55,560 --> 01:44:57,560
And the reason you need this is now you can propagate

2332
01:44:57,560 --> 01:44:59,560
gradient through it, right?

2333
01:44:59,560 --> 01:45:03,560
So that's kind of the first step into sort of making

2334
01:45:03,560 --> 01:45:04,560
the world differentiable.

2335
01:45:04,560 --> 01:45:06,560
You're just making the value function differentiable.

2336
01:45:06,560 --> 01:45:10,560
Now, inside of the world, there's two parts in my opinion.

2337
01:45:10,560 --> 01:45:13,560
And this is the source of the idea behind model-based reinforcement

2338
01:45:13,560 --> 01:45:19,560
running is that you have the world, right?

2339
01:45:19,560 --> 01:45:23,560
The world is going from state to state because it wants to

2340
01:45:23,560 --> 01:45:25,560
or because you're taking an action.

2341
01:45:25,560 --> 01:45:29,560
And then there is a value function or, you know, a cost function.

2342
01:45:29,560 --> 01:45:32,560
I prefer to talk in terms of cost function that takes the state

2343
01:45:32,560 --> 01:45:38,560
of the world and gives you kind of estimate of that cost, right?

2344
01:45:38,560 --> 01:45:40,560
Or gives you the cost.

2345
01:45:40,560 --> 01:45:46,560
Now, the world itself and the value function that takes the state

2346
01:45:46,560 --> 01:45:51,560
of the world and gives you pain or pleasure, right, is unknown.

2347
01:45:51,560 --> 01:45:55,560
But what you can do is build a differentiable model of that, right?

2348
01:45:55,560 --> 01:45:58,560
So a differentiable model of that is what NPC is all about.

2349
01:45:58,560 --> 01:46:01,560
You build a model of the world that predicts the new state

2350
01:46:01,560 --> 01:46:03,560
of the world as a function of the previous state.

2351
01:46:03,560 --> 01:46:05,560
It doesn't have to be a complete state.

2352
01:46:05,560 --> 01:46:09,560
It has to be a state that's complete enough to contain the relevant information

2353
01:46:09,560 --> 01:46:12,560
about the world, you know, relative to your task, right?

2354
01:46:12,560 --> 01:46:16,560
And then you have a differentiable function that you learn

2355
01:46:16,560 --> 01:46:21,560
that learns to predict the reward or the cost from your estimate

2356
01:46:21,560 --> 01:46:23,560
of the state of the world.

2357
01:46:23,560 --> 01:46:27,560
So what you have now is, you know, a neural net inside your agent

2358
01:46:27,560 --> 01:46:32,560
basically can simulate the world and simulate the cost

2359
01:46:32,560 --> 01:46:36,560
that is going to result from the state of the world in a differentiable way.

2360
01:46:36,560 --> 01:46:41,560
So now you can use gradient descent or gradient based methods for two things.

2361
01:46:41,560 --> 01:46:45,560
One for inferring a sequence of action that will minimize a particular cost.

2362
01:46:45,560 --> 01:46:48,560
Okay, there's no learning there.

2363
01:46:48,560 --> 01:46:54,560
Two, to learn a policy that will learn to produce the right action

2364
01:46:54,560 --> 01:46:58,560
even the state without having to do model predictive control,

2365
01:46:58,560 --> 01:47:04,560
without having to do this inference by energy minimization, if you want.

2366
01:47:04,560 --> 01:47:09,560
And that, in my opinion, explains the process that we observe in humans

2367
01:47:09,560 --> 01:47:13,560
by which when you learn a new task, you go from the, you know,

2368
01:47:13,560 --> 01:47:17,560
Daniel Kahneman system two to Daniel Kahneman system one, right?

2369
01:47:17,560 --> 01:47:19,560
So, you know, you learn to drive, you're learning to drive,

2370
01:47:19,560 --> 01:47:22,560
you're using, of course, your entire model of the world that you've learned

2371
01:47:22,560 --> 01:47:27,560
in the last 18 years, if you are 18, to predict that, you know,

2372
01:47:27,560 --> 01:47:30,560
when you turn the wheel of the car to the right, the car will go to the right

2373
01:47:30,560 --> 01:47:33,560
and if there's a cliff next to you, the car is going to fall off the cliff

2374
01:47:33,560 --> 01:47:35,560
and, you know, you're going to die, right?

2375
01:47:35,560 --> 01:47:38,560
You don't have to try this to know that this is going to happen.

2376
01:47:38,560 --> 01:47:44,560
You can rely on your internal model to, you know, avoid yourself a lot of pain, right?

2377
01:47:44,560 --> 01:47:51,560
And so, but you pay attention to the situation.

2378
01:47:51,560 --> 01:47:53,560
You pay a lot of attention to the situation.

2379
01:47:53,560 --> 01:47:56,560
You're completely deliberate about it.

2380
01:47:56,560 --> 01:48:00,560
You imagine all kinds of scenarios and you drive slowly

2381
01:48:00,560 --> 01:48:04,560
so you leave yourself enough time to actually do this kind of reasoning.

2382
01:48:04,560 --> 01:48:09,560
And then after maybe 20, 50 hours of practice,

2383
01:48:09,560 --> 01:48:13,560
it becomes subconscious and automatic, you know?

2384
01:48:13,560 --> 01:48:15,560
That's even true for chess players.

2385
01:48:15,560 --> 01:48:18,560
So, that's an interesting thing about chess, right?

2386
01:48:18,560 --> 01:48:21,560
I played once. I'm a terrible chess player, by the way.

2387
01:48:21,560 --> 01:48:25,560
And I played once a simultaneous game against a grandmaster.

2388
01:48:25,560 --> 01:48:30,560
So, he was, you know, he was playing against like 50 other people.

2389
01:48:30,560 --> 01:48:33,560
And so, I had plenty of time to think about my move, right?

2390
01:48:33,560 --> 01:48:38,560
Because he had to kind of play with the 49 other players before getting to me.

2391
01:48:38,560 --> 01:48:41,560
And so, I wait for him to come and make one move.

2392
01:48:41,560 --> 01:48:45,560
And then, you know, in one second, or first of all, he does, like, you know,

2393
01:48:45,560 --> 01:48:47,560
play something stupid, which I did.

2394
01:48:47,560 --> 01:48:49,560
And he moves within one second.

2395
01:48:49,560 --> 01:48:50,560
He doesn't have to think about it, right?

2396
01:48:50,560 --> 01:48:52,560
It's completely subconscious to him.

2397
01:48:52,560 --> 01:48:54,560
You know, it's just pattern recognition, right?

2398
01:48:54,560 --> 01:48:58,560
He's got this, you know, covenants predicting the next move,

2399
01:48:58,560 --> 01:49:00,560
you know, completely instinctively.

2400
01:49:00,560 --> 01:49:03,560
He doesn't have to think because I'm not, you know,

2401
01:49:03,560 --> 01:49:08,560
I'm not good enough for him to really kind of cause his system to kick in.

2402
01:49:08,560 --> 01:49:13,560
And of course, you know, he beat me in 10, you know, in 10 plays, right?

2403
01:49:13,560 --> 01:49:16,560
I mean, as I told you, I'm terrible.

2404
01:49:16,560 --> 01:49:19,560
You know, I learned to drive a long time ago.

2405
01:49:19,560 --> 01:49:28,560
But as it turned out, very recently, I went to drive a sports car on a raceway.

2406
01:49:28,560 --> 01:49:33,560
And, you know, again, the first few times, you were very deliberate about it, you know?

2407
01:49:33,560 --> 01:49:38,560
You explain a few things, and then you basically have to integrate all of that by yourself.

2408
01:49:38,560 --> 01:49:41,560
Over the course of a day, you get better and better, like much better,

2409
01:49:41,560 --> 01:49:47,560
just by, you know, basically compiling what at first is deliberate

2410
01:49:47,560 --> 01:49:51,560
into something that becomes automatic and subconscious.

2411
01:49:51,560 --> 01:49:55,560
Indeed. And also, abstractly reusing knowledge that you've gleaned elsewhere

2412
01:49:55,560 --> 01:49:56,560
and applying it in a new situation.

2413
01:49:56,560 --> 01:50:00,560
But anyway, Professor Yanlacun, thank you so much for joining us today.

2414
01:50:00,560 --> 01:50:01,560
Thank you. It's been an absolute honor.

2415
01:50:01,560 --> 01:50:02,560
Well, it's been a pleasure.

2416
01:50:02,560 --> 01:50:04,560
You guys are doing great work.

2417
01:50:04,560 --> 01:50:06,560
So, you know, keep going.

2418
01:50:06,560 --> 01:50:08,560
Thank you for being a fan, too.

2419
01:50:08,560 --> 01:50:10,560
Thank you so much for watching some episodes.

2420
01:50:10,560 --> 01:50:11,560
Thank you so much.

2421
01:50:11,560 --> 01:50:12,560
Wonderful.

2422
01:50:12,560 --> 01:50:14,560
Rando, introduce yourself.

2423
01:50:14,560 --> 01:50:21,560
Okay, so I joined FAIR, or should I say Meta AI Research, I guess.

2424
01:50:21,560 --> 01:50:28,560
Last June, for a postdoc position, I was doing my PhD at Rice University

2425
01:50:28,560 --> 01:50:31,560
under the supervision of Professor Richard Baronyuk.

2426
01:50:31,560 --> 01:50:36,560
And during my PhD, I focused on basically trying to understand deep networks

2427
01:50:36,560 --> 01:50:39,560
from a geometrical point of view through spline theory.

2428
01:50:39,560 --> 01:50:43,560
And now I'm trying to, let's say, expand my horizons

2429
01:50:43,560 --> 01:50:49,560
and do more diversified research topics with Yanlacun.

2430
01:50:49,560 --> 01:50:56,560
And for this paper, basically, the main goal was to try to understand

2431
01:50:56,560 --> 01:51:03,560
through a lot of empirical experiments, what do we understand by interpolation?

2432
01:51:03,560 --> 01:51:05,560
Does it occur in practice?

2433
01:51:05,560 --> 01:51:10,560
And does it make sense to use interpolation as we know it,

2434
01:51:10,560 --> 01:51:15,560
as, let's say, a measure of generalization performance for current networks?

2435
01:51:15,560 --> 01:51:20,560
And the main point is really to say that the current definition of interpolation,

2436
01:51:20,560 --> 01:51:28,560
which uses this convex hull, might be too rigid to really provide any meaningful intuition.

2437
01:51:28,560 --> 01:51:36,560
And so we either need to adapt this definition or just entirely think about this in a different angle.

2438
01:51:36,560 --> 01:51:43,560
But yeah, the current definition is not good enough for the current data regimes that we are in right now.

2439
01:51:43,560 --> 01:51:46,560
I don't know if it's precise enough.

2440
01:51:46,560 --> 01:51:47,560
Perfect.

2441
01:51:47,560 --> 01:51:48,560
No, it's wonderful.

2442
01:51:48,560 --> 01:51:49,560
Cool.

2443
01:51:49,560 --> 01:51:52,560
Sorry, I need to get into the mood again.

2444
01:51:52,560 --> 01:51:53,560
Yeah.

2445
01:51:53,560 --> 01:51:55,560
Hi, Randall.

2446
01:51:55,560 --> 01:51:58,560
It's really cool to have you here.

2447
01:51:58,560 --> 01:52:00,560
We've enjoyed reading the paper.

2448
01:52:00,560 --> 01:52:04,560
It's quite a short and concise paper, I have to say.

2449
01:52:04,560 --> 01:52:08,560
And the experiments are quite, I find them to be really on point,

2450
01:52:08,560 --> 01:52:14,560
especially where you look at the latent space experiments.

2451
01:52:14,560 --> 01:52:19,560
Because a lot of people would say, of course we're not interpolating in data space.

2452
01:52:19,560 --> 01:52:22,560
We're interpolating in the latent space.

2453
01:52:22,560 --> 01:52:29,560
Yet, even in the latent space, you know, and we've talked a little bit about the notion of interpolation and extrapolation.

2454
01:52:29,560 --> 01:52:35,560
Is it fair to say that the paper is just sort of a negative argument?

2455
01:52:35,560 --> 01:52:40,560
Is it fair to say that the main point of the paper is arguing,

2456
01:52:40,560 --> 01:52:48,560
look, interpolation is the wrong concept you're looking for when you criticize these models?

2457
01:52:48,560 --> 01:52:49,560
Yes.

2458
01:52:49,560 --> 01:52:51,560
I don't think it's negative per se.

2459
01:52:51,560 --> 01:52:59,560
It's more, let's say, a call to change the definition for the current usage of machine learning models.

2460
01:52:59,560 --> 01:53:03,560
Because now we're not in low dimension regimes anymore.

2461
01:53:03,560 --> 01:53:10,560
And so using those concepts that have been defined 50 years ago when we were looking at univariate models,

2462
01:53:10,560 --> 01:53:12,560
does not really make sense.

2463
01:53:12,560 --> 01:53:16,560
So I think the intuition behind what we try to mean by interpolation is right.

2464
01:53:16,560 --> 01:53:17,560
We should not change that.

2465
01:53:17,560 --> 01:53:22,560
We should change the mathematical definition of it.

2466
01:53:22,560 --> 01:53:28,560
And yeah, like you say, people could argue, yes, but you have interpolation in the latent space and so on.

2467
01:53:28,560 --> 01:53:32,560
But we showed that even in a classifier setting, it does not happen.

2468
01:53:32,560 --> 01:53:39,560
And you can also show that in a generative network setting, it will not happen again or because of the dimensionality.

2469
01:53:39,560 --> 01:53:43,560
So it's really about going to the high dimensional setting.

2470
01:53:43,560 --> 01:53:48,560
Then things start to break and we have to adapt to that basically.

2471
01:53:48,560 --> 01:53:51,560
And we've already asked Jan this question.

2472
01:53:51,560 --> 01:54:00,560
But if you had to give your best shot at making the argument that these people want to make when they say,

2473
01:54:00,560 --> 01:54:02,560
oh, it's just interpolating.

2474
01:54:02,560 --> 01:54:09,560
If you had to give your best shot at making that argument successfully, you know, what would you change?

2475
01:54:09,560 --> 01:54:16,560
How would you change the notion of interpolation or what argument would you make for those people?

2476
01:54:16,560 --> 01:54:22,560
So I think what most people try to say is they try first to conceptualize the data that they have.

2477
01:54:22,560 --> 01:54:24,560
So for example, you have an apple, right?

2478
01:54:24,560 --> 01:54:25,560
You have different colors.

2479
01:54:25,560 --> 01:54:33,560
And if you think of this color as being your latent space, then you can say, okay, between green and red, you have a new color.

2480
01:54:33,560 --> 01:54:34,560
But it's in between the two.

2481
01:54:34,560 --> 01:54:36,560
You are in an interpolation regime, right?

2482
01:54:36,560 --> 01:54:39,560
So all of this point of view is from a generative perspective.

2483
01:54:39,560 --> 01:54:43,560
And this is because you only think of a few factors of variations like this.

2484
01:54:43,560 --> 01:54:47,560
And then you think, okay, everything should be in interpolation regime.

2485
01:54:47,560 --> 01:54:54,560
But even with that definition, if you start having a lot of factors of variation, then the course of dimensionality will kick again.

2486
01:54:54,560 --> 01:54:58,560
And you will never be in interpolation regime, even in a generative setting.

2487
01:54:58,560 --> 01:55:11,560
So for them, the best argument would be if I don't consider the real data set, but very low dimensional approximation, very rough, which can be explained only with a very few factors of variation.

2488
01:55:11,560 --> 01:55:15,560
And I can somehow linearize those factors in my latent space.

2489
01:55:15,560 --> 01:55:19,560
Then I will have more chance at being in an interpolation regime.

2490
01:55:19,560 --> 01:55:24,560
So we'll have to have a sort of lossy compression of your data, if you will.

2491
01:55:24,560 --> 01:55:27,560
And then you can try to reach there with more chances.

2492
01:55:27,560 --> 01:55:31,560
So I think you just still manned the argument for interpolation.

2493
01:55:31,560 --> 01:55:37,560
So I think that's precisely what folks do argue is happening in a deep neural network.

2494
01:55:37,560 --> 01:55:39,560
Do you not believe that?

2495
01:55:39,560 --> 01:55:46,560
Well, as soon as you have high dimensionality settings, then it does not happen almost surely.

2496
01:55:46,560 --> 01:55:52,560
And I mean, you could argue, so for example, let's say you take a gun or any big generative network, right?

2497
01:55:52,560 --> 01:55:59,560
In the latent space, you have hundreds of dimensions and you sample those guys from a Gaussian distribution.

2498
01:55:59,560 --> 01:56:07,560
So even if you were saying, OK, your training set is sample from a generative model and your test set is sample from the same generative model.

2499
01:56:07,560 --> 01:56:10,560
And in that latent space, you have interpolation.

2500
01:56:10,560 --> 01:56:23,560
Well, it's wrong from the beginning because in that latent space alone, you will never have a Gaussian sample that lies in an interpolation regime as soon as the dimensionality is greater than, let's say, 15 or 20.

2501
01:56:23,560 --> 01:56:29,560
So it does not happen because of the dimensionality, unless you have very degenerate things.

2502
01:56:29,560 --> 01:56:36,560
Of course, if your generative network just speeds out a constant, then in pixel space, you will have interpolation.

2503
01:56:36,560 --> 01:56:39,560
But this is degenerate by definition.

2504
01:56:39,560 --> 01:56:44,560
I'd like to pick up on that, if you don't mind.

2505
01:56:44,560 --> 01:56:55,560
Yeah, so what I want to pick up on is, again, all of this hinges on definition one, which is in the paper, which is membership within this convex hull.

2506
01:56:55,560 --> 01:57:02,560
And there's a sense in which that's an extremely rigid definition of interpolation, right?

2507
01:57:03,560 --> 01:57:18,560
And I think I heard you earlier say that what we need to do is redefine what we mean by interpolation in higher dimension because the intuition of interpolation is still correct, but we need to redefine what we mean by interpolation.

2508
01:57:18,560 --> 01:57:28,560
So if you took a shot at redefining interpolation, how would you define it? What do you think is a better definition of interpolation?

2509
01:57:28,560 --> 01:57:36,560
So I think it's very task dependent. So let's say you want to look at a task which is pure object classification.

2510
01:57:36,560 --> 01:57:48,560
In that case, I think going back to what we were saying earlier on first trying to compress some of your data so that you can explain it with as little factors of variation as possible.

2511
01:57:48,560 --> 01:57:53,560
And then you can use the current definition just on a compression of your data.

2512
01:57:53,560 --> 01:58:04,560
Then it could make sense because you don't really mind about the finicky details of your objects if you just want to differentiate between different classes of objects.

2513
01:58:04,560 --> 01:58:18,560
But in another setting where you might be trying, I don't know, to denoise samples or things like that, then you might want to have a very more specific definition based on what you try to denoise in your data and so on.

2514
01:58:18,560 --> 01:58:33,560
So I don't think there will be a general definition that works across the board. It will be really dependent on the type of task you are looking down downstream. It's really the key.

2515
01:58:33,560 --> 01:58:49,560
Let me throw something out there because I've been thinking about this and I tried to ask Professor Lacune about this, but I didn't communicate it clearly, which is that for something to be in the convex hull, so in dimensions,

2516
01:58:49,560 --> 01:59:02,560
for something to be within a convex hull, it's a necessary condition that on every single dimension, the sample data point lies within the range that was sampled.

2517
01:59:02,560 --> 01:59:10,560
So that's a necessary but not sufficient condition because the convex hull is actually even smaller than that space.

2518
01:59:10,560 --> 01:59:19,560
But it's necessary that it be within that axis-aligned bounding box. It has to be inside there on every dimension.

2519
01:59:19,560 --> 01:59:33,560
And so what if I were to invert this and say that, well, instead I'm going to define extrapolation as on every single dimension, it has to be outside the sample range.

2520
01:59:33,560 --> 01:59:46,560
So in other words, the point has to be outside the axis-aligned bounding box. That would actually come to the complete inverse conclusion because then exponentially so all machine learning would be interpolation

2521
01:59:46,560 --> 01:59:51,560
because it's very, very unlikely that you're outside every single dimension.

2522
01:59:51,560 --> 02:00:08,560
So I'm thinking somewhere in between there, there's almost like a fractional concept of interpolation versus extrapolation where we can say it's almost like on average, how many of the dimensions do you hit inside the sample point?

2523
02:00:08,560 --> 02:00:16,560
So maybe I'm inside 20% of the dimensions on average. Could that be like a route to any type of improved definition?

2524
02:00:16,560 --> 02:00:23,560
So that's actually a very good point. So basically what you are saying is instead looking at, let's say, the closest, enclosing hypercube.

2525
02:00:23,560 --> 02:00:29,560
And then you say, okay, if you're in that hypercube or not, you're in interpolation or extrapolation regime.

2526
02:00:29,560 --> 02:00:38,560
And that's actually something we're looking at right now, which is not the smallest hypercube but the smallest ellipsoid that encloses your data.

2527
02:00:38,560 --> 02:00:43,560
And it's somewhere in between the convex hull and the hypercube that you mentioned.

2528
02:00:43,560 --> 02:00:57,560
So for sure, there is some ways in between those two extremes where you will have a meaningful interpolation and extrapolation regime that does not collapse all one way or the other way.

2529
02:00:57,560 --> 02:01:05,560
So this is for sure one interesting route. And this could potentially be, let's say, task agnostic.

2530
02:01:05,560 --> 02:01:14,560
But then again, would it be precise enough to re-quantify generation performances per se? I don't know yet.

2531
02:01:14,560 --> 02:01:17,560
But yeah, that's something we are looking into right now.

2532
02:01:18,560 --> 02:01:38,560
And the reason that we kind of came to that thought process is because there is an intuition that machine learning is kind of very good at taking all these dimensions that we sample and saying, you know, it's really only a subset of those dimensions that matter, at least in some transform way.

2533
02:01:38,560 --> 02:01:54,560
Like for example, suppose I was doing, suppose I only had 16 dimensions of ambient space and everybody agreed, yeah, given the amount of data we have, we are interpolating, like almost all the time we're in this rigid definition of a convex hull.

2534
02:01:54,560 --> 02:02:02,560
And then somebody comes along and says, oh, by the way, we upgraded all our sensors now and we added 240 dimensions.

2535
02:02:02,560 --> 02:02:12,560
And if all those dimensions happen to not be useful for the problem that we're trying to solve or classification problem or whatever, it would be weird to say now we're all of a sudden extrapolating.

2536
02:02:12,560 --> 02:02:24,560
Because if the neural network is doing what it should be doing, it will ignore all those irrelevant definitions and continue just calculating exactly what it was calculating before.

2537
02:02:25,560 --> 02:02:28,560
Yes, that's a very good point.

2538
02:02:28,560 --> 02:02:39,560
But I think for this to actually work, you will need to assume that you have enough actual samples so that when you train your network understands that basically those dimensions are pure noise.

2539
02:02:39,560 --> 02:02:43,560
It does not need it to solve the task at hand and it disregards it.

2540
02:02:43,560 --> 02:02:50,560
So you have some all regularization terms that kicks in and it does not try to use them to minimize the training loss.

2541
02:02:50,560 --> 02:02:53,560
But I think in practice, that's not really what we see.

2542
02:02:53,560 --> 02:03:11,560
If you take like image task classifications, even especially now with self-supervised learning methods, we see that actually most of the information that we could think is irrelevant for the task is actually kept because it can of course help in reducing the training loss.

2543
02:03:11,560 --> 02:03:18,560
So I think for what you said to work, you really need to be sure that either you have the perfect regularizer,

2544
02:03:18,560 --> 02:03:23,560
or you have the perfect model and regime of training samples and so on.

2545
02:03:23,560 --> 02:03:29,560
But in a general setting, I think it will be tricky to claim it like this.

2546
02:03:29,560 --> 02:03:31,560
That's a very interesting point.

2547
02:03:31,560 --> 02:03:38,560
So you're saying when you add these extra dimensions, even to learn to ignore them, you have to have lots of data.

2548
02:03:38,560 --> 02:03:44,560
Yes, yes, or regularization or some mechanisms that exactly.

2549
02:03:44,560 --> 02:03:46,560
Yeah, very interesting.

2550
02:03:46,560 --> 02:03:48,560
That's one thing.

2551
02:03:48,560 --> 02:03:54,560
That's one reason why, let's say, add up parameterization by end is sometimes useful.

2552
02:03:54,560 --> 02:04:03,560
Because if you know which things are useless for your task, you can by end remove them from the data that you feed to your deep net or whatever model.

2553
02:04:03,560 --> 02:04:06,560
And then doing this will improve generalization.

2554
02:04:06,560 --> 02:04:13,560
So that's why in some cases it's still useful to do those preprocessing steps.

2555
02:04:13,560 --> 02:04:14,560
Right.

2556
02:04:14,560 --> 02:04:16,560
Could you give me a bit of intuition here?

2557
02:04:16,560 --> 02:04:22,560
So we've been listening to quite a few folks that make the argument about the interpolative nature of deep learning.

2558
02:04:22,560 --> 02:04:28,560
And you could argue that any kind of processing and machine learning, not just feature transformations,

2559
02:04:28,560 --> 02:04:31,560
but even things like regularization and domain randomization,

2560
02:04:31,560 --> 02:04:37,560
that there are all ways of making the problem more interpolative and it would be appropriate for an interpolated problem.

2561
02:04:37,560 --> 02:04:42,560
So I'm thinking to myself, I mean, in this example, you take pictures of clock faces,

2562
02:04:42,560 --> 02:04:47,560
and it's not interpolative in the ambient space, you perform some kind of feature transformation.

2563
02:04:47,560 --> 02:04:51,560
There exists some nonlinear transformation that transforms it into an interpolative space.

2564
02:04:51,560 --> 02:04:52,560
That's what these people say.

2565
02:04:52,560 --> 02:04:57,560
But I'm wondering, you know, I'm interested in the curse of dimensionality.

2566
02:04:57,560 --> 02:05:00,560
You know, why does deep learning work at all?

2567
02:05:00,560 --> 02:05:07,560
And I've spoken to folks that talk about creating various priors to combat the curse of dimensionality.

2568
02:05:07,560 --> 02:05:11,560
But why do you think that deep learning works at all in these high dimensional spaces?

2569
02:05:12,560 --> 02:05:22,560
Well, so first, I might say something a bit speculative or not agreed upon by everyone,

2570
02:05:22,560 --> 02:05:23,560
but I don't think you can...

2571
02:05:23,560 --> 02:05:24,560
We love it. We love it. Please do.

2572
02:05:24,560 --> 02:05:25,560
Exactly.

2573
02:05:25,560 --> 02:05:29,560
I don't think you can state so generally that deep learning works right.

2574
02:05:29,560 --> 02:05:31,560
You need the right architecture.

2575
02:05:31,560 --> 02:05:33,560
You need the right loss function.

2576
02:05:33,560 --> 02:05:36,560
You need everything right for it to work.

2577
02:05:36,560 --> 02:05:39,560
That's why it works now because we spend, I don't know,

2578
02:05:39,560 --> 02:05:43,560
much amount of money and manpower to get to where we are now.

2579
02:05:43,560 --> 02:05:46,560
But if you just take a plain MLP, you apply it on ImageNet,

2580
02:05:46,560 --> 02:05:49,560
I don't think you will say it's working right.

2581
02:05:49,560 --> 02:05:51,560
So I think right now it's working.

2582
02:05:51,560 --> 02:05:55,560
I mean, what we have is working because we basically, by end,

2583
02:05:55,560 --> 02:06:01,560
did all the cross-validation such that the network that we are using is regularized enough

2584
02:06:02,560 --> 02:06:06,560
to only learn the necessary or meaningful information,

2585
02:06:06,560 --> 02:06:11,560
or at least as best as we can to provide good generalization performances.

2586
02:06:11,560 --> 02:06:19,560
So, yeah, it's a bit too general of a statement to say that deep learning works out of the box for everything.

2587
02:06:19,560 --> 02:06:20,560
In fact, many cases...

2588
02:06:20,560 --> 02:06:21,560
I love that point.

2589
02:06:21,560 --> 02:06:22,560
Yes, yes.

2590
02:06:22,560 --> 02:06:28,560
Many, if you go into not just image classification, but let's say audio classification

2591
02:06:28,560 --> 02:06:32,560
and some maybe trickier data set where you don't have a lot of samples,

2592
02:06:32,560 --> 02:06:36,560
then everything starts to break quite rapidly, right?

2593
02:06:36,560 --> 02:06:40,560
You have always people trying to go into the medical field as well,

2594
02:06:40,560 --> 02:06:44,560
but since it's very hard to generalize between patients,

2595
02:06:44,560 --> 02:06:48,560
between recording devices and so on, things get very, very messy

2596
02:06:48,560 --> 02:06:53,560
and everything is ad hoc and optimization, basically.

2597
02:06:54,560 --> 02:06:57,560
I don't think we're at this point yet where you can say,

2598
02:06:57,560 --> 02:06:59,560
that's it, deep learning works out.

2599
02:06:59,560 --> 02:07:01,560
Yeah, can I jump in real quickly here, Tim?

2600
02:07:01,560 --> 02:07:03,560
Because what I want to say is, brilliant point.

2601
02:07:03,560 --> 02:07:07,560
I mean, and when you think about it, let's take Professor LeCun,

2602
02:07:07,560 --> 02:07:12,560
you know, one of his great invention, right, the CNN, Convolutional Neural Network.

2603
02:07:12,560 --> 02:07:16,560
That was discovered by a human being, like a machine,

2604
02:07:16,560 --> 02:07:19,560
and no neural network learned to do convolution.

2605
02:07:19,560 --> 02:07:24,560
LeCun taught machines how to do convolution.

2606
02:07:24,560 --> 02:07:27,560
And so we oftentimes discount this and say,

2607
02:07:27,560 --> 02:07:31,560
look how great machine learning works, we discount all the human engineering

2608
02:07:31,560 --> 02:07:34,560
that has gone into actually making machine learning work

2609
02:07:34,560 --> 02:07:36,560
through specific architectures.

2610
02:07:36,560 --> 02:07:38,560
Yes, exactly, exactly.

2611
02:07:38,560 --> 02:07:42,560
Basically, we are guiding where to go,

2612
02:07:42,560 --> 02:07:46,560
and then of course, once we guide enough, the machine knows what to do.

2613
02:07:46,560 --> 02:07:51,560
I mean, this is not a reduction of what we can do with deep learning, right?

2614
02:07:51,560 --> 02:07:54,560
But he's just saying that a generalized statement

2615
02:07:54,560 --> 02:07:59,560
out of the box, deep learning works even in a new task that we never tried before

2616
02:07:59,560 --> 02:08:01,560
is an overstatement.

2617
02:08:01,560 --> 02:08:04,560
But I still want to linger on this point of why you said,

2618
02:08:04,560 --> 02:08:06,560
or here's just a random projection of the data,

2619
02:08:06,560 --> 02:08:09,560
and then here's a ResNet projection, here's a trained ResNet projection.

2620
02:08:09,560 --> 02:08:14,560
What I took from that is irrespective, it's all in an extrapolative regime.

2621
02:08:14,560 --> 02:08:17,560
And there was a guy who posted an article

2622
02:08:17,560 --> 02:08:20,560
who is disparagingly saying that neural networks are just interpolators,

2623
02:08:20,560 --> 02:08:23,560
analogous to a Taylor series approximation

2624
02:08:23,560 --> 02:08:26,560
within a small part of the domain of a function,

2625
02:08:26,560 --> 02:08:28,560
and it goes haywire outside of the domain.

2626
02:08:28,560 --> 02:08:31,560
And there are a lot of people that would make that case,

2627
02:08:31,560 --> 02:08:34,560
that neural networks cannot extrapolate outside of the training range,

2628
02:08:34,560 --> 02:08:37,560
but they do seem to work remarkably well,

2629
02:08:37,560 --> 02:08:38,560
given the curse of dimensionality.

2630
02:08:38,560 --> 02:08:40,560
So why is that exactly?

2631
02:08:40,560 --> 02:08:45,560
So let's say for simplification that you have a binary classification problem.

2632
02:08:45,560 --> 02:08:48,560
So once you go into your Latin space,

2633
02:08:48,560 --> 02:08:50,560
or if you just do linear classification,

2634
02:08:50,560 --> 02:08:52,560
if you stay in your ambient space,

2635
02:08:52,560 --> 02:08:55,560
basically you say you have class 0 or 1

2636
02:08:55,560 --> 02:08:58,560
based on which side of an hyperplane you lie in.

2637
02:08:58,560 --> 02:09:01,560
Now, if you are on the good side,

2638
02:09:01,560 --> 02:09:03,560
you can go to the infinity, right?

2639
02:09:03,560 --> 02:09:06,560
You can go to extrapolation regime,

2640
02:09:06,560 --> 02:09:08,560
as far as you want from the training set,

2641
02:09:08,560 --> 02:09:11,560
as long as you are in the correct side of that hyperplane,

2642
02:09:11,560 --> 02:09:13,560
you will have good generalization performance.

2643
02:09:13,560 --> 02:09:16,560
So it's not exactly correlated.

2644
02:09:16,560 --> 02:09:18,560
The only thing you need,

2645
02:09:18,560 --> 02:09:21,560
some O is to have the orientation of the axis

2646
02:09:21,560 --> 02:09:23,560
where you will extrapolate

2647
02:09:23,560 --> 02:09:27,560
to be somewhat aligned in the orthogonal space of that hyperplane.

2648
02:09:27,560 --> 02:09:29,560
Once you have that, then you will generalize,

2649
02:09:29,560 --> 02:09:31,560
even though you can be in extrapolation regime

2650
02:09:31,560 --> 02:09:34,560
in ambient or feature space.

2651
02:09:34,560 --> 02:09:39,560
So that's one thing that is maybe proper to linear classification.

2652
02:09:39,560 --> 02:09:43,560
If you were doing maybe some other type of classifier on top of it,

2653
02:09:43,560 --> 02:09:45,560
it might change.

2654
02:09:45,560 --> 02:09:47,560
But given the current settings,

2655
02:09:47,560 --> 02:09:50,560
I don't think people should expect bad performance

2656
02:09:50,560 --> 02:09:53,560
because you are not in an interpolation regime.

2657
02:09:53,560 --> 02:09:55,560
This is quite of a shortcut,

2658
02:09:55,560 --> 02:09:58,560
maybe that it guided by our intuition, right?

2659
02:09:58,560 --> 02:10:01,560
We think it's much easier to classify something

2660
02:10:01,560 --> 02:10:03,560
if it's in an interpolation regime.

2661
02:10:03,560 --> 02:10:06,560
But if you just look at the plane in our classifier,

2662
02:10:06,560 --> 02:10:09,560
that's not the case.

2663
02:10:09,560 --> 02:10:11,560
Well, so just to steal man a bit

2664
02:10:11,560 --> 02:10:15,560
what say others on the interpolation side,

2665
02:10:15,560 --> 02:10:18,560
again, that's only if you're buying into a very rigid definition

2666
02:10:18,560 --> 02:10:21,560
of interpolation, which the way the paper defines it

2667
02:10:21,560 --> 02:10:23,560
is in a linear way.

2668
02:10:23,560 --> 02:10:27,560
I mean, it's defining interpolation by this linear convex hull.

2669
02:10:27,560 --> 02:10:30,560
And of course, neural networks, for example,

2670
02:10:30,560 --> 02:10:32,560
are highly nonlinear systems.

2671
02:10:32,560 --> 02:10:36,560
And what you just said, sort of which side of a separating hyperplane you're on,

2672
02:10:36,560 --> 02:10:38,560
that itself is a nonlinear function.

2673
02:10:38,560 --> 02:10:42,560
And so I think the point you made early on was very good,

2674
02:10:42,560 --> 02:10:45,560
which is we have this intuition of what interpolation is.

2675
02:10:45,560 --> 02:10:49,560
We haven't yet got a good definition for interpolation

2676
02:10:49,560 --> 02:10:54,560
in multi or high-dimensionality nonlinear cases.

2677
02:10:54,560 --> 02:10:59,560
But obviously the linear definition doesn't really apply.

2678
02:10:59,560 --> 02:11:01,560
Yeah, exactly.

2679
02:11:01,560 --> 02:11:06,560
It's really because, I mean, it's linear,

2680
02:11:06,560 --> 02:11:10,560
but again, you can consider your data as being nonlinearly transformed.

2681
02:11:10,560 --> 02:11:14,560
The problem is still that if you have high-dimensionality,

2682
02:11:14,560 --> 02:11:18,560
then to be within a convex hull becomes exponentially hard

2683
02:11:18,560 --> 02:11:21,560
using the convex hull of the training side.

2684
02:11:21,560 --> 02:11:24,560
If you define something else, like you said before the hypercube,

2685
02:11:24,560 --> 02:11:26,560
it could be the opposite.

2686
02:11:26,560 --> 02:11:29,560
It's always in interpolation regime.

2687
02:11:29,560 --> 02:11:34,560
And the key is basically to find a meaningful set

2688
02:11:34,560 --> 02:11:38,560
such that you don't go all the way in one direction or the other.

2689
02:11:38,560 --> 02:11:42,560
And because I think the main point also is that this should give intuition

2690
02:11:42,560 --> 02:11:45,560
into the generalization performance of a model, right?

2691
02:11:45,560 --> 02:11:50,560
Because if you can detect a sample is in an extrapolation regime,

2692
02:11:50,560 --> 02:11:53,560
but your model still performs very good on it.

2693
02:11:53,560 --> 02:11:56,560
I don't know if it has really a lot of practical use.

2694
02:11:56,560 --> 02:12:00,560
So what will be good as well is to have the basically

2695
02:12:00,560 --> 02:12:06,560
generalization performance of a model correlate with your definition of extrapolation.

2696
02:12:06,560 --> 02:12:09,560
And that's really what we are trying to get.

2697
02:12:09,560 --> 02:12:14,560
And that's why I was saying that probably you might have a task-dependent definition,

2698
02:12:14,560 --> 02:12:20,560
because that might be where you get the best precise definition to reach that.

2699
02:12:21,560 --> 02:12:26,560
I was wondering to what extent, if any of this invalidates the so-called manifold hypothesis.

2700
02:12:26,560 --> 02:12:30,560
So I think when most people speak of interpolation and deep learning,

2701
02:12:30,560 --> 02:12:32,560
their intuition is something along the lines of,

2702
02:12:32,560 --> 02:12:36,560
well, the model learns some manifold in the latent space,

2703
02:12:36,560 --> 02:12:42,560
and interpolation means I'm just kind of like traversing the geodesic on that manifold.

2704
02:12:42,560 --> 02:12:45,560
And when you visualize the results on an autoencoder or something,

2705
02:12:45,560 --> 02:12:48,560
you can see this kind of continuous geometric morphing

2706
02:12:48,560 --> 02:12:51,560
of, let's say, one image into another image.

2707
02:12:51,560 --> 02:12:55,560
And that manifold, I think you said in your paper that the actual data manifold,

2708
02:12:55,560 --> 02:12:59,560
it's not possible to approximate that well, but it's doing something interesting.

2709
02:12:59,560 --> 02:13:03,560
And a prediction on that manifold in the intermediate space,

2710
02:13:03,560 --> 02:13:05,560
it's not massively deranged, is it?

2711
02:13:05,560 --> 02:13:07,560
It's still doing something very useful, statistically.

2712
02:13:07,560 --> 02:13:08,560
Yes, yes, yes, totally.

2713
02:13:08,560 --> 02:13:11,560
So I think it does not at all contradict it.

2714
02:13:11,560 --> 02:13:16,560
And in fact, think of a very, very simple example where you say your data

2715
02:13:16,560 --> 02:13:20,560
is a linear manifold of dimension 100, let's say,

2716
02:13:20,560 --> 02:13:25,560
which is a relatively meaningful number of dimensions even for images.

2717
02:13:25,560 --> 02:13:28,560
But now suppose it's completely linear.

2718
02:13:28,560 --> 02:13:32,560
Well, even in that case, it will be hard to be in interpolation regime

2719
02:13:32,560 --> 02:13:34,560
just because you have 100 dimensions

2720
02:13:34,560 --> 02:13:40,560
and picking a new sample that lies in the convex of your training set is exponentially higher.

2721
02:13:40,560 --> 02:13:44,560
So you can have a manifold for your data.

2722
02:13:44,560 --> 02:13:46,560
It can even be linear.

2723
02:13:46,560 --> 02:13:48,560
So the simplest manifold you wish for,

2724
02:13:48,560 --> 02:13:51,560
the only problem is the dimensionality of that manifold.

2725
02:13:51,560 --> 02:13:55,560
Now that being said, again, it does not mean that you cannot do

2726
02:13:55,560 --> 02:13:59,560
basically moving on the geodesic of your manifold

2727
02:13:59,560 --> 02:14:01,560
or you cannot do some sort of interpolation

2728
02:14:01,560 --> 02:14:06,560
because that's what happens in most of the current state-of-the-art generative networks, right?

2729
02:14:06,560 --> 02:14:10,560
It's just that you don't need to be in interpolation regime

2730
02:14:10,560 --> 02:14:14,560
to have a correct generation process, basically.

2731
02:14:14,560 --> 02:14:18,560
And that's, again, one thing that is very important.

2732
02:14:18,560 --> 02:14:21,560
And you mentioned, you have those examples.

2733
02:14:21,560 --> 02:14:24,560
That's true that the interpolation is extremely nice,

2734
02:14:24,560 --> 02:14:28,560
but again, it does not say that the points you start with

2735
02:14:28,560 --> 02:14:32,560
and the point you end up with are in interpolation regime of your training set.

2736
02:14:32,560 --> 02:14:37,560
So I think it's two different things to be able to interpolate or move on your manifold

2737
02:14:37,560 --> 02:14:41,560
and being in the interpolation regime from your training set.

2738
02:14:41,560 --> 02:14:46,560
So what's intriguing me about, and I want to clarify something earlier,

2739
02:14:46,560 --> 02:14:52,560
which is that though a hyperplane is a linear construct,

2740
02:14:52,560 --> 02:14:58,560
the function that says you're on one side or the other of a hyperplane is a nonlinear function.

2741
02:14:58,560 --> 02:15:03,560
And in that case, it's a digital function.

2742
02:15:03,560 --> 02:15:06,560
You're either on this side or that side, it's zero or one.

2743
02:15:06,560 --> 02:15:12,560
What's kind of interesting is you gave that as an explanation for why machine learning works at all,

2744
02:15:12,560 --> 02:15:18,560
which is that for data sets that we care about, or problem spaces that we care about,

2745
02:15:18,560 --> 02:15:25,560
it seems to often be the case that we can do enough sequences of linear and nonlinear transformations

2746
02:15:25,560 --> 02:15:32,560
to arrange the data such that it falls on one side or another of a separating plane.

2747
02:15:32,560 --> 02:15:39,560
In a sense, it seems like real-world data is often separable if you do enough transformations on it.

2748
02:15:39,560 --> 02:15:47,560
And since that separability is a very digital concept, is there anything interesting in that?

2749
02:15:47,560 --> 02:15:49,560
Any intuition?

2750
02:15:49,560 --> 02:15:52,560
Yes, yes. So for this, you still have to be careful, right?

2751
02:15:52,560 --> 02:15:59,560
Because if you go in high-dimensional spaces, you can basically separate everything very easily.

2752
02:15:59,560 --> 02:16:02,560
But your generalization performance might be bad.

2753
02:16:02,560 --> 02:16:09,560
So basically, what you want is really to have, let's say, a good ratio of how much you expand dimension,

2754
02:16:09,560 --> 02:16:14,560
how much you nonlinearly transform your data, to then reach a good generalization performance.

2755
02:16:14,560 --> 02:16:18,560
Because otherwise, you can just fall back into a kernel regime, let's say,

2756
02:16:18,560 --> 02:16:23,560
and you expand so much your space that sure you can separate everything on your training set,

2757
02:16:23,560 --> 02:16:26,560
but then the generalization is going to be very poor.

2758
02:16:26,560 --> 02:16:32,560
And that's one strength of deep networks, right, is that you have so much of those nonlinear transformations

2759
02:16:32,560 --> 02:16:38,560
that you can somehow not expand the dimension of your space too much or even contract it,

2760
02:16:38,560 --> 02:16:47,560
and still have a separating hyperplane in the end where generalization is much higher than in other class of models.

2761
02:16:47,560 --> 02:16:53,560
So I think the key is really to have those meaningful nonlinear transformations

2762
02:16:53,560 --> 02:16:56,560
such that you don't have to increase your space too much.

2763
02:16:56,560 --> 02:16:59,560
You just shape it around, if you will.

2764
02:16:59,560 --> 02:17:02,560
And what you said earlier is exactly true.

2765
02:17:02,560 --> 02:17:06,560
If you think of which side you are in, basically, you are binarizing your data, right?

2766
02:17:06,560 --> 02:17:15,560
And if you have good classification, it means all the classes are assigned to the same labels, which is 1 or 0.

2767
02:17:15,560 --> 02:17:22,560
And if you think of it still in interpolation regime, then suddenly you are in interpolation regime, right?

2768
02:17:22,560 --> 02:17:30,560
You are a 1. The new sample is a 1 after binarization, and you become interpolation regime, and you have a good performance.

2769
02:17:30,560 --> 02:17:36,560
But this comes after this compression step, if you will, or discretization step.

2770
02:17:36,560 --> 02:17:41,560
And that might be another direction to explore as well.

2771
02:17:41,560 --> 02:17:47,560
If you start to quantize things or to compress things, as we were saying a bit at the beginning,

2772
02:17:47,560 --> 02:17:53,560
then you can reach the interpolation much more easily as well.

2773
02:17:53,560 --> 02:17:55,560
I have a couple of questions.

2774
02:17:55,560 --> 02:18:01,560
So again, in Table 1, I mean, your paper is making the argument that everything is extrapolation,

2775
02:18:01,560 --> 02:18:04,560
given this convex notion in high-dimensional space.

2776
02:18:04,560 --> 02:18:12,560
But if we zoom in a little bit, though, so you are using a pre-trained ResNet classifier, pre-trained on ImageNet,

2777
02:18:12,560 --> 02:18:18,560
and how do all of these things change the structure of the latent space in a meaningful way?

2778
02:18:18,560 --> 02:18:21,560
And the embedding space is also highly distributed.

2779
02:18:21,560 --> 02:18:25,560
And we were wondering, can you give us some intuition here?

2780
02:18:25,560 --> 02:18:30,560
So is the information likely to be quite evenly distributed over the latent,

2781
02:18:30,560 --> 02:18:36,560
or do you think it's actually quite bunched up and sparsely encoded in few of the features?

2782
02:18:37,560 --> 02:18:41,560
So I think this will depend on which training setting you use, right?

2783
02:18:41,560 --> 02:18:44,560
So for example, if you start using dropout and things like that,

2784
02:18:44,560 --> 02:18:52,560
you will try to have a more evenly distribution of your information to have a more stable loss when you drop those units.

2785
02:18:52,560 --> 02:18:54,560
So I don't think you have a general answer for that.

2786
02:18:54,560 --> 02:19:00,560
It will depend on the type of regularization you have or training is done, etc.

2787
02:19:00,560 --> 02:19:06,560
But you have to keep in mind that what you try to do with gradient descent is just minimize your loss, right?

2788
02:19:06,560 --> 02:19:11,560
But then with cross-ontropolis, let's say, your gradient starts to vanish as you become really good

2789
02:19:11,560 --> 02:19:14,560
and you stop learning where you are, basically.

2790
02:19:14,560 --> 02:19:21,560
So given that, depending on your initialization, you will still try to make the best of what you get.

2791
02:19:21,560 --> 02:19:29,560
And even if it means learning redundant dimensions, if this can reduce your loss further at a more rapid rate,

2792
02:19:29,560 --> 02:19:30,560
that's what you will do.

2793
02:19:30,560 --> 02:19:39,560
So if you don't impose any regularization or anything, there is no clear reason to assume that everything is well organized and so on.

2794
02:19:39,560 --> 02:19:41,560
And that's what we see even in generative networks.

2795
02:19:41,560 --> 02:19:48,560
You have to start putting so much regularization to try to have disentanglement and to try to make sense of those latent spaces.

2796
02:19:48,560 --> 02:19:57,560
Because otherwise, you just try to learn what minimizes your loss with the most short-term view of your loss landscape.

2797
02:19:57,560 --> 02:20:03,560
So basically, that could be built if you have some specific regularizer.

2798
02:20:03,560 --> 02:20:07,560
But otherwise, it will not occur naturally.

2799
02:20:07,560 --> 02:20:14,560
Of course, and again, if you wanted to only retain the minimal information to solve the task at hand,

2800
02:20:14,560 --> 02:20:19,560
then you will see much more interpolation regime in that latent space.

2801
02:20:19,560 --> 02:20:28,560
If you think of MNIST, for example, you will disregard the translation of your digits, the rotation of the digits, all those things.

2802
02:20:28,560 --> 02:20:31,560
All those things will be disregarded when you reach the latent space.

2803
02:20:31,560 --> 02:20:36,560
And then you will basically be in interpolation regime most of the time.

2804
02:20:36,560 --> 02:20:45,560
But since you keep as much information as possible to try to minimize your loss as best as you can, then you basically occupy as much as you can.

2805
02:20:45,560 --> 02:20:50,560
Unless, again, you have some degeneracies because of the whole architecture tricks.

2806
02:20:50,560 --> 02:20:57,560
For example, if you have a bottleneck layer, you will limit the dimensionality of the manifold you span in the latent space.

2807
02:20:57,560 --> 02:21:01,560
So you can have all those parameters that can play a big role.

2808
02:21:01,560 --> 02:21:06,560
So it will be in the general setting. I don't think you could assume anything.

2809
02:21:07,560 --> 02:21:17,560
And I think there's also a relationship, too, between the dimensionality of the latent space and, let's say, some intrinsic dimensionality of the problem.

2810
02:21:17,560 --> 02:21:27,560
So if the intrinsic dimensionality of the problem only takes five dimensions to solve, and yet I give a latent space of 256 dimensions,

2811
02:21:28,560 --> 02:21:36,560
I think what I hear you saying is that, of course, gradient descent is going to make some use of those other 251 dimensions,

2812
02:21:36,560 --> 02:21:43,560
but they're going to have maybe a very minuscule or diminishing effect on the latent space.

2813
02:21:43,560 --> 02:21:50,560
Whereas on the other hand, if I then took that same network and increased the complexity of the problem,

2814
02:21:50,560 --> 02:21:55,560
we could end up with, for example, it's sparsifying for any particular class.

2815
02:21:55,560 --> 02:22:04,560
So if we're doing some multi-class problem, we may find that it sort of arranges these seven dimensions to solve the dog versus hot dog problem,

2816
02:22:04,560 --> 02:22:09,560
and these 12 dimensions to dissolve the car versus motorcycle problem.

2817
02:22:09,560 --> 02:22:16,560
It might be forced to make more compact use of that latent information space per class. Is that fair?

2818
02:22:16,560 --> 02:22:18,560
Yes, so that's a very good point.

2819
02:22:18,560 --> 02:22:27,560
So first of the first things that you said, one thing to keep in mind, so let's say your data is even linear manifold of dimension one,

2820
02:22:27,560 --> 02:22:33,560
and then you go through a deep net, and then somehow it's already linearly separable.

2821
02:22:33,560 --> 02:22:37,560
Then you only need to learn the identity mapping with your deep net to solve the task.

2822
02:22:37,560 --> 02:22:41,560
But if you start from a random initialization, it's extremely hard to do that.

2823
02:22:41,560 --> 02:22:44,560
That's why people came up with ResNet and all those things.

2824
02:22:44,560 --> 02:22:54,560
So already from this point of view, it means that almost surely your linear one-dimensional manifold will become highly non-linear,

2825
02:22:54,560 --> 02:23:00,560
still one-dimensional, but very highly non-linear in the latent space of your classifier.

2826
02:23:00,560 --> 02:23:06,560
And as we showed in the figure one of the paper, even if the intrinsic dimension is one,

2827
02:23:06,560 --> 02:23:11,560
if you are highly non-linear in your space, then you will basically never be in interpolation regime.

2828
02:23:12,560 --> 02:23:17,560
So you have those sort of artifacts that come just from the fact that learning simple mapping,

2829
02:23:17,560 --> 02:23:22,560
let's say, with a non-linear deep network is not always simple.

2830
02:23:22,560 --> 02:23:32,560
And those artifacts will be introduced right away because it's so hard in your parameter space to reach that point where you have identity mapping.

2831
02:23:32,560 --> 02:23:42,560
So this is another effect that kicks in and that can somehow remove those assumptions that even if your data is in a low-dimensional regime

2832
02:23:42,560 --> 02:23:49,560
and almost linear, it will be preserved in the output space of your network.

2833
02:23:49,560 --> 02:23:54,560
So this is something really important to keep in mind as well.

2834
02:23:54,560 --> 02:24:03,560
So, Randall, I also noticed in about 2018, you were the first author on some really interesting work and it was called a spline theory of deep learning.

2835
02:24:03,560 --> 02:24:06,560
And then I think the next year it got into Neuribs.

2836
02:24:06,560 --> 02:24:09,560
So I'm just reading a bit from the abstract.

2837
02:24:09,560 --> 02:24:16,560
So you said you built a rigorous bridge between deep networks and approximation theory via spline functions and operators.

2838
02:24:16,560 --> 02:24:25,560
And you actually think that this can explain a lot about deep learning in the sense of then being a composition of these max-safine spline operators.

2839
02:24:25,560 --> 02:24:28,560
Has any of this work informed your view of deep learning now?

2840
02:24:28,560 --> 02:24:30,560
Yes, a lot actually.

2841
02:24:30,560 --> 02:24:38,560
So first of all, I want to be precise that of course a lot of people knew about the fact that if you use, for example,

2842
02:24:38,560 --> 02:24:46,560
volume activations, absolute value and so on, or max-pulling, then the whole network is continuous piecewise linear mapping.

2843
02:24:46,560 --> 02:24:55,560
So what we did mostly is to make this a bit more rigorous and try to understand what the partition of the input space looks like,

2844
02:24:55,560 --> 02:25:02,560
what the perigen mapping looks like, and how can we use that to gain some more intuitions into what's happening.

2845
02:25:02,560 --> 02:25:08,560
And the nice thing with this is that if you think about it, there is nothing simpler than piecewise linear mappings, right?

2846
02:25:08,560 --> 02:25:18,560
Basically, it says that if you are in a region of your space, a region from the partition of your mapping, then the input-output mapping will stay linear.

2847
02:25:18,560 --> 02:25:24,560
And this allows to do a lot of analysis, for example, to adversarial perturbation or all these type of things.

2848
02:25:24,560 --> 02:25:28,560
And it can also open the door to other lines of research.

2849
02:25:28,560 --> 02:25:38,560
For example, one thing we did, I think it was NeurIPS last year or two years ago, was to use that to derive the exact expectation maximization algorithm for the generative networks,

2850
02:25:38,560 --> 02:25:44,560
because now you have a cleaner, let's say, or simpler analytical form of your network.

2851
02:25:44,560 --> 02:25:52,560
And this really opens to me the door to derive some more interesting theoretical results.

2852
02:25:52,560 --> 02:26:08,560
It does not really help intuitively, because I think everyone had this intuition from before already, but it's mostly a mathematical tool that allows to derive with small is some interesting results.

2853
02:26:08,560 --> 02:26:24,560
And I think one important thing as well is that you have really this dichotomy right between the, let's say, old school signal processing, template matching type of academia researchers,

2854
02:26:24,560 --> 02:26:28,560
and the new school with deep learning, everything has to be trained and so on.

2855
02:26:28,560 --> 02:26:32,560
And what is interesting in this paper is that we somehow bridge the two.

2856
02:26:32,560 --> 02:26:46,560
We say that basically a deep net is a very smart way to build an adaptive spline, which will learn automatically its partition of the input space and the perigen affine mapping, such that it works in high dimension.

2857
02:26:46,560 --> 02:26:49,560
And this was not known before by anyone in the spline theory.

2858
02:26:49,560 --> 02:27:05,560
So I think to speak about adaptive spline in high dimension, no one has no idea what to do except dimension two or three, maybe, because a lot of PDE work uses, but outside of dimension three, no one thinks about splines.

2859
02:27:05,560 --> 02:27:11,560
So I think there's a very strong result is to bridge those two different fields.

2860
02:27:11,560 --> 02:27:21,560
That's fascinating. I think one of the issues, because I'm speaking to Juan Bruno about this, and he was saying there was a big tradition in harmonic analysis of trying to reason about the behavior of these models.

2861
02:27:21,560 --> 02:27:31,560
And we did have the universal function approximation theorem, which in a sense is talking about stacking basis functions, you know, to approximate an arbitrary function.

2862
02:27:31,560 --> 02:27:42,560
But you say in your abstract here that the spline partition of the input signal opens up in a new, you know, opens up a new geometric avenue to study how deep neural networks organize signals in a hierarchical fashion.

2863
02:27:42,560 --> 02:27:49,560
I don't think people really have much of an intuition on how these models behave and how to reason about them.

2864
02:27:49,560 --> 02:27:51,560
Yes. Yeah, that's a very good point.

2865
02:27:51,560 --> 02:27:59,560
So one thing that we did, for example, is to study all the partition of the mapping evolves as you go through the layers.

2866
02:27:59,560 --> 02:28:05,560
And what we show is that at each layer, you keep subdividing your current partition.

2867
02:28:05,560 --> 02:28:18,560
And this is very interesting because once you know that if you look at a binary classification, for example, you get that the decision boundary is basically linear in each region of the partition.

2868
02:28:18,560 --> 02:28:28,560
And so what this tells you is that as you add layer, what you have to do is to refine the regions that still contain more than one class within them.

2869
02:28:28,560 --> 02:28:39,560
So this kind really brings insights and maybe opens the door to building new learning techniques to how many layers you should stack, which regions should they subdivide and so on.

2870
02:28:39,560 --> 02:28:44,560
And this is really akin to decision trees and how they build their partition as well.

2871
02:28:44,560 --> 02:28:51,560
So there is a lot of work that could be done also to maybe bridge the two or use one to understand the other.

2872
02:28:51,560 --> 02:29:04,560
And it's really geometrical because it's in the whole field of computational geometry and so on because we have those hyperplane arrangements, those half-spaces, intersection of them, hyperplane desalation.

2873
02:29:04,560 --> 02:29:07,560
It's also a known system.

2874
02:29:07,560 --> 02:29:14,560
So it's really geometrical and it can have a lot of interesting insights to understand what's happening.

2875
02:29:14,560 --> 02:29:18,560
It also provides nice visualization tools.

2876
02:29:18,560 --> 02:29:21,560
It's really fascinating.

2877
02:29:21,560 --> 02:29:27,560
Would you be interested in coming back on the show for a future episode just dedicated to this topic?

2878
02:29:27,560 --> 02:29:30,560
That'd be amazing.

2879
02:29:30,560 --> 02:29:35,560
Randall also actually released a paper called Neural Decision Trees.

2880
02:29:35,560 --> 02:29:42,560
And in the abstract he said they propose a synergistic melting of neural networks and decision trees.

2881
02:29:42,560 --> 02:29:47,560
So this is something that you've been thinking about actually from many angles.

2882
02:29:47,560 --> 02:29:49,560
Yes, yes, yes.

2883
02:29:49,560 --> 02:29:55,560
And that's very, very synergic to think about one from the point of view of the other as well, right?

2884
02:29:55,560 --> 02:30:08,560
Because if you think of a decision tree, the real limitation of it is that all you subdivide one region by adding a node does not really tell you how to subdivide another region in another part of the space.

2885
02:30:08,560 --> 02:30:14,560
You don't have this, let's say, communication or friendly help between the region subdivision.

2886
02:30:14,560 --> 02:30:23,560
But in a deep net, what you do actually is that if you know how to subdivide one region, then it will automatically enforce or you subdivide nearby regions.

2887
02:30:23,560 --> 02:30:31,560
And through this, suddenly you have a mechanism that appears which is that you don't need samples in each region to know to subdivide them.

2888
02:30:31,560 --> 02:30:38,560
You just need samples in some of the regions and then it will guide you on how to subdivide regions of the space without samples.

2889
02:30:38,560 --> 02:30:47,560
And that's something that is extremely strong when you go to high dimensional settings because you cannot have samples in all parts of the space by definition.

2890
02:30:47,560 --> 02:30:56,560
So I think this is extremely nice to have both point of view because then you can try to use the strengths of one to maybe improve the other.

2891
02:30:56,560 --> 02:30:59,560
So that's really interesting to me.

2892
02:30:59,560 --> 02:31:14,560
I want to ask you this question that we ask a lot of our guests because it's just at least something that's kind of profound interest to me is that there is this apparent dichotomy between continuous and discreet.

2893
02:31:14,560 --> 02:31:31,560
It's like the human brain is at the most lowest possible level and analog kind of continuous system and yet it evolved all these sort of discreet almost computations on top of it like pulses that either fire or don't fire like that type of thing.

2894
02:31:31,560 --> 02:31:47,560
And in the in the learning world or let's say really in the computation world we have the fact that you know I can write a very short piece of symbolic code discreet code that that can go and calculate the nth digit of Pi or something like that.

2895
02:31:47,560 --> 02:31:55,560
But it's almost impossible to train or it is currently impossible to train any neural network of fixed depth to do the same thing.

2896
02:31:55,560 --> 02:32:06,560
And so we have this this weird you know different regimes we have the discreet kind of logic reasoning type world and then we have the continuous differentiable type of world.

2897
02:32:06,560 --> 02:32:28,560
Do you view those as I mean are they fundamentally different regimes and we're always going to have hybrid systems that kind of combine both types of reasoning or is it possible to just say project that discreet computation fully into an analog type space if you just have enough you know parameters or something.

2898
02:32:28,560 --> 02:32:50,560
Yeah I think so I think it really depends on the resources that you have right because to me at least it seems that the hybrid system might be the most efficient where you can easily let's say cluster different settings into groups and then and for this you can just have a discreet settings and then within each group.

2899
02:32:51,560 --> 02:32:56,560
Discreetization is not good enough anymore and you need to go into the continuous regime.

2900
02:32:56,560 --> 02:33:07,560
So I think it will depend on the application you have at hand or or efficient you want to be doing it either energy wise or anything else.

2901
02:33:07,560 --> 02:33:28,560
So I think it's not not clear if one should dominate those are necessarily like for example in the if you go back to the spline setting of a deep net you have discreet setting which is basically your partition which which which region you are in and then within that region.

2902
02:33:28,560 --> 02:33:37,560
You have a linear transformation of the mapping which is basically continuous and both interact if you adapt one the other one change and so on.

2903
02:33:37,560 --> 02:33:51,560
And I think having this type of hybrid systems and where somehow learning through the continuous part adapts the discreet part is what is extremely powerful.

2904
02:33:51,560 --> 02:34:06,560
And that's I think one extremely beautiful property of current networks is that they do automatically this adaptive training of their discreet path through training of their continuous parameters.

2905
02:34:06,560 --> 02:34:09,560
And that's why they are so efficient.

2906
02:34:09,560 --> 02:34:18,560
If you think of pure approximation theory and you have an adaptive spline in one day that's why you have the best convergence rate basically.

2907
02:34:18,560 --> 02:34:22,560
So I think you really need both systems to interact.

2908
02:34:22,560 --> 02:34:29,560
If they don't interact then I think it's really easy to become suboptimal and interesting.

2909
02:34:29,560 --> 02:34:37,560
Because we put a similar question to Lacune and he was kind of saying that in an ideal world we would have a discreet system as well.

2910
02:34:37,560 --> 02:34:42,560
Humans are really bad at playing chess because we don't have that discreet system built in.

2911
02:34:42,560 --> 02:34:51,560
But the problem I think people like Lacune have with these discrete systems is typically they're symbolic and they're statically coded.

2912
02:34:51,560 --> 02:34:59,560
You could start talking about getting into a discreet program search and you could even guide that program search based on some deep learning model.

2913
02:34:59,560 --> 02:35:10,560
But I don't think to Keith's point I don't think it's really possible to do that well inside the continuous domain because if the problem even was learnable with stochastic gradient descent.

2914
02:35:10,560 --> 02:35:15,560
The representation would be glitchy. It just wouldn't work.

2915
02:35:15,560 --> 02:35:22,560
I think it depends a lot too on what are you trying to achieve with a model you build.

2916
02:35:22,560 --> 02:35:33,560
If you just try to be as close as possible to let's say what the human brain is doing then you might impose yourself to have some restriction on do you want it to be discreet or not.

2917
02:35:33,560 --> 02:35:39,560
Or if you just want to have a model that you can deploy on a task and it can solve the task as best as you want.

2918
02:35:39,560 --> 02:35:50,560
So I think depending on what is your goal and what are you trying to imitate with the model would change or you answer that as well.

2919
02:35:50,560 --> 02:35:54,560
But what if the goal was task acquisition efficiency?

2920
02:35:54,560 --> 02:35:59,560
So it's like I don't know what the task is yet.

2921
02:35:59,560 --> 02:36:12,560
Yeah I think that's like again to me a hybrid system where you have interaction between both parts intuitively would be the most efficient.

2922
02:36:12,560 --> 02:36:18,560
But yeah it might not be true for all settings.

2923
02:36:19,560 --> 02:36:30,560
When I was looking at figure three both Tim and I were interested in the fact that if you look at the MNIST data which to a human being is kind of a simpler data set.

2924
02:36:30,560 --> 02:36:34,560
One, two, three we know how to do that type of thing.

2925
02:36:35,560 --> 02:36:49,560
That as you increase dimensionality it much more rapidly becomes extrapolation versus image net which seems to kind of more slowly transition from interpolation to image extrapolation.

2926
02:36:49,560 --> 02:37:01,560
And what I'm wondering is the intuition I got from that and I wonder if this is completely wrong or it's correct is that for machine learning there's a sense in which MNIST is actually a harder problem.

2927
02:37:01,560 --> 02:37:12,560
Because it has to look at kind of global relationships like it has to try and say well there's a circle over here that's kind of oriented with respect to a line that's kind of further away.

2928
02:37:12,560 --> 02:37:26,560
And so it's harder for it to do that whereas we know that with like image net very frequently ML sort of devolves to looking at these micro texture like you know well everything that has this shade of yellow is a school bus you know type of thing.

2929
02:37:26,560 --> 02:37:33,560
Is that an intuition one can take from that plot or what does it mean that MNIST decreases so much more quickly.

2930
02:37:33,560 --> 02:37:39,560
Yes, so that's actually some things that we tried to clarify with a figure that comes after this one.

2931
02:37:39,560 --> 02:37:48,560
Basically the thing that you really need to be careful about and keep in mind is that if you look at for example a 16 by 16 patch for MNIST.

2932
02:37:48,560 --> 02:37:56,560
You have basically maybe most of the information you need to solve the task and you have a lot of texture about your DG etc.

2933
02:37:56,560 --> 02:38:08,560
If you look at image net and you take a 16 by 16 patch you have basically no information about the class it's extremely small patch it's almost constant across the spatial dimensions right.

2934
02:38:08,560 --> 02:38:13,560
It's basically a very small percentage of your full image.

2935
02:38:13,560 --> 02:38:22,560
So that's why MNIST goes much more quickly to extrapolation regime for a fixed dimensionality in pixel space.

2936
02:38:22,560 --> 02:38:28,560
Because the information you have in that amount of dimension is greater than for the image net case.

2937
02:38:28,560 --> 02:38:33,560
And this is only because it's already much more done sample right.

2938
02:38:33,560 --> 02:38:46,560
If we were looking at MNIST but with a 224 by 224 spatial dimension and we look at a fixed dimensionality then you will not have this difference anymore and it might even reverse.

2939
02:38:46,560 --> 02:38:58,560
So the really important thing to keep in mind is that even though it's the same dimension for both it does not represent the same proportion of image that is present within that patch.

2940
02:38:58,560 --> 02:39:05,560
And that's why you have those differences in that's mostly why you have those differences in those curves.

2941
02:39:05,560 --> 02:39:20,560
It's funny because we had the opposite intuition and so in the following figure 4 you're showing then on MNIST that more of the variance is explained with fewer of the principal components on MNIST.

2942
02:39:20,560 --> 02:39:26,560
And as you say that that does just because it's those pixels on MNIST are more salient for the problem.

2943
02:39:26,560 --> 02:39:29,560
I mean just so I understand because we were debating this a little bit.

2944
02:39:29,560 --> 02:39:32,560
Could you just give us your articulation of figure 4?

2945
02:39:32,560 --> 02:39:47,560
Yes sure. So basically what we are looking for the increasing dimensionality for the three data set is we pick a number of dimension in the spatial space.

2946
02:39:47,560 --> 02:39:50,560
So we do this by extracting a patch.

2947
02:39:50,560 --> 02:39:56,560
And then what we do is that we extract of course the same patch for all the samples.

2948
02:39:56,560 --> 02:40:04,560
And then we are looking at the proportion of the test set patches that are in interpolation regime and we report this.

2949
02:40:04,560 --> 02:40:16,560
Now for the PCA plot what we do is basically we look at once you extract this patch how many principal components you need to explain to perfectly reconstruct those patches.

2950
02:40:16,560 --> 02:40:20,560
Or you could say to explain the variance in those patches.

2951
02:40:20,560 --> 02:40:27,560
And this gives direct relationship because it shows how concentrated you are on lower dimensional manifolds.

2952
02:40:27,560 --> 02:40:31,560
A fine one of course that learns through the principal components.

2953
02:40:31,560 --> 02:40:41,560
And it means that if you can encode much more information with much less principal components then you lie on a lower dimensional affine manifold.

2954
02:40:41,560 --> 02:40:48,560
And this coupled with figure 1 shows that basically it's much easier to be in interpolation regime.

2955
02:40:48,560 --> 02:41:01,560
So the whole point of using this PCA plot was to show how good low dimensional affine manifold represents the current extracted patches.

2956
02:41:01,560 --> 02:41:07,560
And then to use this as a way to justify the extrapolation regime curve that we see.

2957
02:41:07,560 --> 02:41:11,560
So because again in the PCA regime it's linear manifold.

2958
02:41:11,560 --> 02:41:21,560
So if only two components for example perfectly describe all your patches then you will need very few training samples to be in interpolation regime.

2959
02:41:21,560 --> 02:41:23,560
Okay that makes sense.

2960
02:41:23,560 --> 02:41:29,560
And then the staircase effect on the smooth subsample row is a function of the size of the Gaussian filter used to smooth it?

2961
02:41:29,560 --> 02:41:38,560
Yeah so the staircase occurs basically whenever the number of dimensions increase and we get a new bigger patch to get it.

2962
02:41:38,560 --> 02:41:42,560
Because there is different ways to get it right.

2963
02:41:42,560 --> 02:41:52,560
One would be to always extract the center patch and remove some dimensions if you don't have exact number of dimensions that you can represent with a square patch.

2964
02:41:52,560 --> 02:41:58,560
Another thing you could do is to first smooth subsample and then remove the dimensions.

2965
02:41:58,560 --> 02:42:02,560
So there is different variants on how to extract those patches.

2966
02:42:02,560 --> 02:42:09,560
We try to show two different ones to show that the results do not really depend on how you do this process.

2967
02:42:09,560 --> 02:42:14,560
But yeah you will have some little different artifacts like this.

2968
02:42:14,560 --> 02:42:21,560
It does not change the overall trend but yeah it can change the small trends when you change from one dimension to another.

2969
02:42:21,560 --> 02:42:25,560
Well let me ask a question here about figure four.

2970
02:42:25,560 --> 02:42:31,560
Again about the intuition that we had on leaving aside interpolation and extrapolation for the moment.

2971
02:42:31,560 --> 02:42:43,560
It seems that MNIST for a given amount of variance explained and for a given dimensionality MNIST requires more principal components.

2972
02:42:43,560 --> 02:42:46,560
For a given amount of dimension yeah.

2973
02:42:46,560 --> 02:42:54,560
If we fix the dimension and we fix the percent variance explained MNIST requires more principal components than ImageNet.

2974
02:42:54,560 --> 02:42:58,560
That seems to me to tell me that MNIST is a more difficult problem.

2975
02:42:58,560 --> 02:43:00,560
Is that not true?

2976
02:43:00,560 --> 02:43:05,560
I think it's a bit so for a specific number of dimensions you could argue that.

2977
02:43:05,560 --> 02:43:12,560
But the thing you have to recall is that because MNIST images are much smaller in spatial dimensions.

2978
02:43:12,560 --> 02:43:18,560
If you have a 16 by 16 patch you have basically the whole MNIST dataset let's say.

2979
02:43:18,560 --> 02:43:24,560
And so just having a few principal components is not enough to really reconstruct the whole MNIST dataset.

2980
02:43:24,560 --> 02:43:30,560
Now on ImageNet a 16 by 16 patch is almost a constant texture right.

2981
02:43:30,560 --> 02:43:33,560
You have a few different colors but you don't have a lot of variation.

2982
02:43:33,560 --> 02:43:38,560
It's basically through the spatial dimensions it's basically constant.

2983
02:43:38,560 --> 02:43:45,560
And what this means is that with only three or four principal components basically one for each color channel.

2984
02:43:45,560 --> 02:43:49,560
You can perfectly reconstruct all those 16 by 16 patches.

2985
02:43:49,560 --> 02:43:58,560
So to really get to the conclusions you are saying what you will have to do first is to either done sample ImageNet to be 28 by 28

2986
02:43:58,560 --> 02:44:02,560
or up sample MNIST to be 224 by 224.

2987
02:44:02,560 --> 02:44:10,560
And if you do that then basically I think you will have the roughly same evolution of the interpolation regime.

2988
02:44:10,560 --> 02:44:20,560
Because the 16 by 16 patch on this extra up sample MNIST image will be either completely black or completely white.

2989
02:44:20,560 --> 02:44:24,560
And in this case you will still need a few principal components.

2990
02:44:24,560 --> 02:44:29,560
So this is also something very interesting right because it shows that maybe for the task at hand

2991
02:44:29,560 --> 02:44:34,560
you might not need to have such a high resolution image you might done sample.

2992
02:44:34,560 --> 02:44:41,560
But because when you done sample you basically keep those in crucial information

2993
02:44:41,560 --> 02:44:44,560
you don't necessarily go faster to interpolation regime.

2994
02:44:44,560 --> 02:44:47,560
So this is another point.

2995
02:44:47,560 --> 02:44:55,560
It's really tricky you should think of it as how much of the image do I uncut given that number of dimensions.

2996
02:44:55,560 --> 02:45:01,560
And then given that this plot might be easier to understand.

2997
02:45:01,560 --> 02:45:04,560
16 by 16 by each.

2998
02:45:04,560 --> 02:45:09,560
Yeah exactly so 16 by 16 on MNIST is maybe around 60-70% of the image.

2999
02:45:09,560 --> 02:45:14,560
While 16 by 16 on ImageNet is maybe around maybe 5% of the image.

3000
02:45:14,560 --> 02:45:18,560
And that's why you have those different regimes that appear.

3001
02:45:18,560 --> 02:45:20,560
Incredible.

3002
02:45:20,560 --> 02:45:23,560
Randall thank you so much for joining us this evening.

3003
02:45:23,560 --> 02:45:25,560
Sure sure thanks.

3004
02:45:25,560 --> 02:45:28,560
So we just spoke with Randall what was your take on that Keith?

3005
02:45:28,560 --> 02:45:32,560
Absolutely brilliant it was a true pleasure to speak with him.

3006
02:45:32,560 --> 02:45:38,560
For me it cleared up a lot of thoughts and issues I was having with this paper.

3007
02:45:38,560 --> 02:45:51,560
So for example right up front what he says is my purpose behind this paper is to show that even though the intuition that people have of interpolation

3008
02:45:51,560 --> 02:45:56,560
like the intuition that we have of interpolation is good.

3009
02:45:56,560 --> 02:46:02,560
The mathematical definition that we have of interpolation is not useful in high dimension.

3010
02:46:02,560 --> 02:46:08,560
What I thought was interesting too is when we asked about well what about the manifold concept.

3011
02:46:08,560 --> 02:46:12,560
You know why isn't that sort of the definition of interpolation.

3012
02:46:12,560 --> 02:46:20,560
And he brought up a really strong argument there as he said well let's just take the simple case of suppose the problem you're trying to solve

3013
02:46:20,560 --> 02:46:28,560
just is linear you know like it just everything is a linear data set linear problem we're trying to solve like even in that case

3014
02:46:28,560 --> 02:46:34,560
this linear case in high enough dimension interpolation doesn't work.

3015
02:46:34,560 --> 02:46:42,560
And there your manifold is just literally a convex hull you know and so sure you can have kind of a nonlinear transformation

3016
02:46:42,560 --> 02:46:47,560
and a nonlinear shape and whatever you're still hit by this curse of dimensionality.

3017
02:46:47,560 --> 02:46:58,560
And he you know he brought up the point that like you know of course if your problem compresses down enough to where only a small number of

3018
02:46:58,560 --> 02:47:06,560
of transformed dimensions right latent space dimensions matter and everything then you can be said that you're you're interpolating

3019
02:47:06,560 --> 02:47:12,560
you know because we're not really hit by that curse of dimensionality because we stripped away all the dimensionality down to these dimensions

3020
02:47:12,560 --> 02:47:19,560
we've gotten lucky our problem space or data samples etc allowed us to do that okay but that's not going to be all problems

3021
02:47:19,560 --> 02:47:27,560
like some problems may just intrinsically have high dimensionality of interpolation that's not useful.

3022
02:47:27,560 --> 02:47:37,560
So we need to do something better we need to we need to come up with a definition of interpolation that maintains the intuitive notion

3023
02:47:37,560 --> 02:47:42,560
that we have of interpolation but that continues to work in high dimension.

3024
02:47:42,560 --> 02:47:49,560
And you know and he made a very you know very interesting kind of end goal here which is like if we can get a definition of interpolation

3025
02:47:49,560 --> 02:47:57,560
that ends the approaching this concept of generalization you know that's that's what we're trying to achieve really.

3026
02:47:57,560 --> 02:48:01,560
And what do you think his take on generalization is then.

3027
02:48:01,560 --> 02:48:07,560
Well as he made it you made a comment that you know on the one hand you could just ridiculously specify the space and make it

3028
02:48:07,560 --> 02:48:10,560
trivially separable but then you lose generalization.

3029
02:48:10,560 --> 02:48:18,560
Well yeah and I think I mean because because we did ask or you asked you know put the question what if what you're what if the problem

3030
02:48:18,560 --> 02:48:25,560
you're trying to solve is the ability to solve novel problems like you know what happens in of course that was in the context of our

3031
02:48:25,560 --> 02:48:34,560
discussion about hybrid systems where you're combining the continuous with with discreet and they're able to ping pong and kind of modify each other.

3032
02:48:34,560 --> 02:48:42,560
And I think what we got from him there and consistently really throughout throughout our discussion.

3033
02:48:42,560 --> 02:48:50,560
And what's interesting is this this totally aligns actually with Francois Chalet as well which is that this all these questions like a lot of these

3034
02:48:50,560 --> 02:48:56,560
very difficult questions that we're asking are problem specific and there is no.

3035
02:48:56,560 --> 02:49:06,560
We don't currently have some one size fits all set of concepts that fits well for it for every every problem space you know it's very task specific

3036
02:49:06,560 --> 02:49:10,560
very problem specific you know data specific.

3037
02:49:10,560 --> 02:49:18,560
So I think that's an area where we got dive deeper with him but I didn't hear anything definitive you know today.

3038
02:49:18,560 --> 02:49:22,560
It's a wrap we just interviewed the Godfather of deep learning.

3039
02:49:22,560 --> 02:49:23,560
How's that possible.

3040
02:49:23,560 --> 02:49:27,560
I think we can just quit now we might as well just shut the channel down.

3041
02:49:27,560 --> 02:49:29,560
Yeah.

3042
02:49:29,560 --> 02:49:33,560
I mean obviously after we've published it.

3043
02:49:33,560 --> 02:49:35,560
That's the singularity.

3044
02:49:35,560 --> 02:49:37,560
So what's your take.

3045
02:49:37,560 --> 02:49:43,560
I would have I would I think I think this but we also interrupted him a little bit.

3046
02:49:43,560 --> 02:49:49,560
Maybe the question was I would have loved for him to be more a bit unlike you know where's the actual disagreement.

3047
02:49:49,560 --> 02:49:50,560
Right.

3048
02:49:50,560 --> 02:49:59,560
Because a lot of times you know when we when we put put you know questions to him like OK other people say this he was like oh yeah I agree right.

3049
02:49:59,560 --> 02:50:08,560
And this seems it seems to be a general sentiment that's also when we talk to other people about you know perceived disagreements they're always very

3050
02:50:08,560 --> 02:50:20,560
you know being being being good being academics being also friends probably with a lot of people you always like yeah you know they have they have good point you know we essentially agree on all the

3051
02:50:20,560 --> 02:50:40,560
on all the things right but then I sort of want to know where actually do people disagree right if it's if it's you know in and that that that is a little bit I mean I obviously have a feeling but it's still a little bit elusive and

3052
02:50:40,560 --> 02:50:50,560
and if people disagree on the actual technical nature or more on the philosophical end of what do what does something mean.

3053
02:50:50,560 --> 02:50:58,560
I definitely think there is from hearing now and I think from his answer to that question.

3054
02:50:59,560 --> 02:51:07,560
I could hear a little bit in that he seems to be more optimistic on what these learning systems can do.

3055
02:51:07,560 --> 02:51:20,560
Then maybe other people are right because some people seem to have really kind of a hard stop on like this will never be possible with like a learning system.

3056
02:51:20,560 --> 02:51:30,560
Whereas it seemed it seemed it seemed that he had sort of a more optimistic outlook and said of course they can't do it now right.

3057
02:51:30,560 --> 02:51:43,560
However you know we work on it we modify them you know we're we're figuring out how do we need to build these systems such that you know a learning system can conceivably.

3058
02:51:43,560 --> 02:51:56,560
You know do many of the things that people would call a kind of reasoning and I think that's why he went into you know let me give you an example of reasoning to sort of show look here you know.

3059
02:51:56,560 --> 02:52:09,560
Here is here is an example of a reasoning that neural networks already do and that means that something like reasoning in general isn't too far away.

3060
02:52:09,560 --> 02:52:14,560
And that's ultimately where the disagreement might be with other people.

3061
02:52:14,560 --> 02:52:20,560
I mean I think that's that was my take on his answer to our first question like the you know why'd you write this paper.

3062
02:52:20,560 --> 02:52:28,560
Is I think and I may go back and watch this but I think essentially what he was saying is you know sometimes people.

3063
02:52:28,560 --> 02:52:34,560
They have some definition in their mind whatever it is of interpolation and extrapolation.

3064
02:52:34,560 --> 02:52:46,560
And they come up with kind of an argument to say see the this this machine learning is doing interpolation and in a binary sense it can't do extrapolation.

3065
02:52:46,560 --> 02:52:58,560
And so therefore there's this entire class of things you know that it's not capable of doing and I think that's what his objection was which is look like this is not a useful distinction to say between.

3066
02:52:59,560 --> 02:53:14,560
You know it's not a useful distinction to draw between quote unquote interpolation and extrapolation like here's an example of a definition right that's a relatively standard definition of interpolation and if we apply this interpolation to.

3067
02:53:14,560 --> 02:53:20,560
Theoretically what machine learning is doing and empirically to what machine learning is doing it's always extrapolating.

3068
02:53:20,560 --> 02:53:27,560
So I think that's what he's objecting to is just like look don't come to some strong conclusion about what.

3069
02:53:27,560 --> 02:53:38,560
Machine learning neural networks I'm not sure what the right way to phrase things is now what it's capable of doing like we just need to do more work to expand its capabilities.

3070
02:53:38,560 --> 02:53:48,560
Folks like Gary Marcus making this case of we need discrete models because they can abstract broadly they give a couple of examples you know binary encoding for numbers.

3071
02:53:48,560 --> 02:53:56,560
And there's one example with can you have a model that reverses the bits or I think there was another example in his algebraic mind which was about can you.

3072
02:53:56,560 --> 02:54:06,560
Generalize from the even numbers to the odd numbers and you can't do that and the reason you can't do it is the new examples are completely outside of the training space of the input data.

3073
02:54:06,560 --> 02:54:11,560
And probably that problem is not interpolative or it's not differentiable.

3074
02:54:11,560 --> 02:54:21,560
I mean what lead sub is the same thing he says in language processing in particular can like language understanding in order to have broad generalization you need to have abstract rules.

3075
02:54:21,560 --> 02:54:30,560
So for example being able to generalize from Mary loves John to Mary loves Jane they make the argument that with a statistical approach that just wouldn't be possible.

3076
02:54:30,560 --> 02:54:49,560
Well I think so I think and this is just my opinion but I think his answer to my digits a pie question kind of shed some light on what his position is on this which is that it's too early to theoretically and scientifically for us to make that that determination

3077
02:54:49,560 --> 02:55:00,560
because he said look okay people only been able to calculate digits of pie you know like within the last hundred years or whatever I'm not talking like we're not there yet I'm not talking about that class of problems I'm talking about these.

3078
02:55:00,560 --> 02:55:01,560
Try numbers as well.

3079
02:55:01,560 --> 02:55:11,560
Right so I'm talking about these things like a cat jumping up here and so some of these examples that Gary Marcus may bring up may fall into that even though they seem simple okay they seem deceptively simple.

3080
02:55:11,560 --> 02:55:28,560
They may still fall in the bucket of these kind of much more towards the discrete spectrum of capabilities which we can't currently do with with our methods and machine learning but we're going to continue and continue to get closer to that that into the spectrum.

3081
02:55:29,560 --> 02:55:39,560
And so it's and so it's premature to say that like machine learning and again whatever you can see that to be neural networks whatever will never be able to get there.

3082
02:55:39,560 --> 02:56:03,560
Like he and he and Jan is optimistic that we'll be able to get there I'm not as optimistic I think there is a very there's just a qualitative difference between you know structural topological discrete reasoning and continuous differentiable reasoning and I don't know how we're ever going to get that gap bridge to brought up some some methods to think about but.

3083
02:56:04,560 --> 02:56:08,560
Right but the key question is whether it's possible at all.

3084
02:56:08,560 --> 02:56:18,560
And there was an article by a guy called Andrew Yee I think and it really annoyed you and the Coon and he said look people are still making the argument that neural networks interpolate and in that article.

3085
02:56:18,560 --> 02:56:29,560
He was basically saying well neural networks are a little bit like a Taylor series approximation you take a function and you just kind of approximate it you know inside a certain range and then it goes haywire outside of the range.

3086
02:56:29,560 --> 02:56:35,560
And Francoise Relay came on the podcast and he said look you know it's a little bit like in Fourier analysis when you try and you kind of like.

3087
02:56:35,560 --> 02:56:44,560
You take these little signs and cosines and you fit it to a discrete function and it's glitchy right and actually if you look at harmonic analysis this is what.

3088
02:56:45,560 --> 02:57:00,560
As you and Bruno said on the show on the on the geometric show he said that what a neural network even the universal function approximation theorem it's all about stacking these basis functions together right to kind of approximate some target function well if that's all neural

3089
02:57:00,560 --> 02:57:02,560
networks do how could they possibly generalize.

3090
02:57:03,560 --> 02:57:10,560
Look I don't want to use the word like generalized I don't know that what I would say is this which is that.

3091
02:57:10,560 --> 02:57:22,560
No matter how discreet in appearance the human brain is you know all the signals that the neurons generate receive whatever you know at the end of the day.

3092
02:57:22,560 --> 02:57:28,560
Are continuous functions I mean they're you know a charge that has a continuous function.

3093
02:57:28,560 --> 02:57:36,560
Now somehow or another like the brain in its structure does take that continuous analog.

3094
02:57:36,560 --> 02:57:52,560
I should have said analog you know probably there were but it takes that analog computational environment and produces digital reasoning so I don't think it's beyond that like I just don't see how anyone could reach the conclusion that it will be impossible like to do.

3095
02:57:53,560 --> 02:58:07,560
Discreet reasoning with with neural networks I think it's just a question of like for me I think is more of a question is that the efficient way to do it like you know if you fast forward 2000 years from now when people have figured out all these problems and we have like you know

3096
02:58:08,560 --> 02:58:27,560
just walking around killing us or working for us whatever the thing may be you know are they going to be using a neural network for everything or is it going to be a neural network with you know some classic digital compute components with some other stuff like a hybrid kind of structural system that does things I think is more what I'm asking.

3097
02:58:28,560 --> 02:58:45,560
Like Lacan hint and they of course that that's what I was going to say they have like nature on their side in that in no matter how much we cut open the human brain we don't find like a discrete computer in there like of course the individual neural spikes are discrete like there's

3098
02:58:45,560 --> 02:59:06,560
sort of charge or no charge but then there is like continuous release of neurotransmitter so I agree like the brain is like a very continuous distributed machine and there is no no there's no discrete thing in there there's no part of the brain where it's like look one one brain brain

3099
02:59:07,560 --> 02:59:32,560
he said that and there is a level at which that's true which is there's a spike train and you know we can kind of recognize a spike because it's a maximum in this analog dimension but my point is that still an underlying analog dimension and so I don't see why in principle you couldn't build a neural network that you know has like these kind of continuous

3100
02:59:32,560 --> 02:59:44,560
values but still ends up with something that that's that synthesizes a a discrete you know decision sure but wouldn't it be glitchy and it still wouldn't extrapolate.

3101
02:59:45,560 --> 02:59:54,560
I have no idea like what you know I just don't know I think I think this kind of point is we're too early on to reach these conclusions that it cannot be done.

3102
02:59:54,560 --> 03:00:09,560
I do sort of feel that it's just not the efficient way to do it that you know we're going to end up with a case where we're going to have systems that have analog continuous chunks that are neural networks or whatever and we're also going to overlay that with

3103
03:00:09,560 --> 03:00:26,560
with digital computation that's implemented by the typical kind of digital computer that's going to be these hybrid systems working together just like you brought up with hey look you know alpha alpha zero alpha go all those things or it can be viewed in the same kind of hybrid way right.

3104
03:00:26,560 --> 03:00:36,560
But that's exactly I mean you said this is what you said and also what when he when he said like so first when he said you know we're far away from that.

3105
03:00:37,560 --> 03:00:45,560
With the digits of pie and so on but also when he said you know humans are actually pretty bad at chess or at discreet exploration in general and that is.

3106
03:00:46,560 --> 03:00:51,560
That's how humans do it right humans build discreet reasoning.

3107
03:00:52,560 --> 03:01:05,560
On top of this sort of neural continuous function and it's actually really hard like to do discreet reasoning in your head is you can do it slowly and you have to do it deliberately with attention.

3108
03:01:05,560 --> 03:01:12,560
If you do like multiplication of five digit number in your head right this is not a this is not a oh my god feeling.

3109
03:01:13,560 --> 03:01:16,560
This is like you like you sit there and you have to like keep all the stuff.

3110
03:01:17,560 --> 03:01:31,560
It's not a hash table look up like the multiplying numbers of the thing and we can like we can build children's toys like that for like 20 cents that multiply 10 digit numbers easily right so I like yeah I think this is.

3111
03:01:31,560 --> 03:01:41,560
Yeah I think this is we teach children point that way is we teach them hash table look up like memorize you know yeah all the numbers from one to a hundred multiply or something and then we teach them an algorithm.

3112
03:01:42,560 --> 03:02:00,560
You know we teach them this exact thing and we teach them hash table look up and and this is built on so if you and I think that if you build if you build this discreet reasoning you can probably build it on top of these models right but yes is it the most efficient way to do it.

3113
03:02:01,560 --> 03:02:05,560
Maybe maybe not right but then again if you do it.

3114
03:02:06,560 --> 03:02:21,560
If you manage to do it train it you seem at least from at least from from the perspective of humans you seem to have something insanely powerful right because you know if you just have a discrete algorithm you have the algorithm but if you have a.

3115
03:02:21,560 --> 03:02:39,560
Discrete if you've actually managed to train a discrete algorithm on top of this continuous function it will be able to sort of learn in the same continuous way that you know a neural network learns but you know it will be sort of be able to self modify its discreet algorithm and that.

3116
03:02:40,560 --> 03:02:49,560
Yeah I think you know I agree I'd also be in the maybe longer term future positive that might be possible.

3117
03:02:49,560 --> 03:02:57,560
I think that's because I said to him and do you think alpha goes a good thing do you think is a good thing to have a kind of.

3118
03:02:58,560 --> 03:03:11,560
Discreet search which is guided by a neural network and he didn't like it and I assume he didn't like it for the reason you just said you and it which is that the discrete part of it is hard coded and it's been told to do a specific task it's it's not learning.

3119
03:03:11,560 --> 03:03:17,560
I think look who would rather have something which is entirely differentiable right so because.

3120
03:03:18,560 --> 03:03:26,560
Look who wants learning end to end in a in a in a neural network right yeah and and also it's almost like he was saying before well.

3121
03:03:27,560 --> 03:03:41,560
Humans can't do discrete reasoning so it would actually be a more sophisticated form of intelligence if we had discrete reasoning but do we necessarily even want that in our models or is it just the fact that he wants it in the models but he wants it to be differentiable and learnable as part of the main model.

3122
03:03:42,560 --> 03:03:50,560
I think so because that's where he got with the you know the critic actor critic type models I think if people nowadays talk about differentiable they always.

3123
03:03:51,560 --> 03:04:04,560
Talk about sort of they always talk about like single shot feet forward like you know I input something into the machine and outcomes of prediction whereas he also makes a lot of arguments for you know this.

3124
03:04:05,560 --> 03:04:17,560
Energy minimization which which means that essentially at test time at inference time I do a minimization algorithm and you know when he when he talked about reasoning he was mentioning that.

3125
03:04:18,560 --> 03:04:31,560
As an example as essentially saying look I have my trained neural network and now at inference time I actually still perform an algorithm on top of that right to minimize some energy function given my trained model of the world let's say so.

3126
03:04:31,560 --> 03:04:47,560
I think you know we might be able to make that more learned by also learning the algorithm that we do at inference time but I don't necessarily I don't necessarily think that he's talking about you know we need.

3127
03:04:48,560 --> 03:04:58,560
Like we need to end to end learn like a machine where you simply input something and then out pops through forward prop out pops and all of these things are just reasoning by the back door.

3128
03:04:59,560 --> 03:05:05,560
It's even with a critic as I understand that that's just the way of hacking that the value the advantage function right you know with some.

3129
03:05:06,560 --> 03:05:10,560
I'm not an expert in reinforcement learning but I think that's what it is the same thing with the energy minimization.

3130
03:05:11,560 --> 03:05:12,560
Yeah I kind of agree with you there.

3131
03:05:13,560 --> 03:05:21,560
Yeah but the energy minimization has this particular thing of this idea which I think today is underexplored of to actually do something at inference time.

3132
03:05:22,560 --> 03:05:29,560
Like to not just forward prop and that I feel it's I feel it's under underexplored nowadays.

3133
03:05:30,560 --> 03:05:48,560
I agree with you there too but I think what kind of sometimes I feel like this is feature creep in a sense it's like you know we have neural networks and we know what deep learning is and some people now want that to be redefined it's just a general purpose research paradigm that includes all possible things that we can do with.

3134
03:05:48,560 --> 03:06:06,560
You know machines or chemicals it's like what use is that like what use is it defining you know all types of computation is as differentiable computation like we lose some ability to talk about these and sort of the same way that was kind of like what I was saying you know nobody I mean look.

3135
03:06:07,560 --> 03:06:23,560
Allowing for a variable number of layers in a neural network that's discrete computation right that's not differentiable training nobody knows how to train in like an arbitrarily unbounded number of layers in some differentiable way right.

3136
03:06:24,560 --> 03:06:26,560
But there's the neural ODE stuff.

3137
03:06:27,560 --> 03:06:43,560
Well all those types of things run into big training problems like things become really hard to train when you have these types of yeah of essentially discrete you know transitions right or combinatorial kinds of kinds of transitions.

3138
03:06:44,560 --> 03:06:59,560
And this is classic stuff I mean this is really classic stuff it's like even if you try to do mixed integer optimization right so you have a problem that has some combination of integer variables discrete variables and some combination of continuous variables.

3139
03:06:59,560 --> 03:07:03,560
There's all kinds of hacks to try to do that in differentiable ways.

3140
03:07:04,560 --> 03:07:18,560
They don't in general arrive at the optimal discrete solution like you do some continuous stuff and then at the end you kind of discretize it in one way or another and you wind up with a solution that's kind of approximately correct.

3141
03:07:18,560 --> 03:07:27,560
But you're not guaranteed that you're going to find the one that's actually correct if you discreetly combinatorially examine that that space right.

3142
03:07:28,560 --> 03:07:36,560
Can we finish by talking about a couple of things so first of all there's table one and the structure of the latent table one is the biggest.

3143
03:07:37,560 --> 03:07:51,560
The biggest argument against the most prevalent argument against this paper sorry for the triple negative but you know when when you know when the paper starts out by saying you know we build a convex hull of the training data.

3144
03:07:51,560 --> 03:08:04,560
If your point is not in the convex hull you know not interpolating and then people go wait wait wait wait wait but you're talking about the input space of data you're talking about the pixel space and you know for sure we're not talking about the pixel space.

3145
03:08:04,560 --> 03:08:17,560
Because you know the neural network is you know the data manifold and who but we're talking about you know if you go to the last layer to the latent space before the classification it's just a linear class.

3146
03:08:17,560 --> 03:08:26,560
You know there in that space we're talking about like interpolation right there is where the neural networks interpolate in this experiment clearly shows like.

3147
03:08:26,560 --> 03:08:41,560
Okay might be a rigid definition of interpolation but it clearly shows like no even in that space on a trained Resnet there is no like no interpolation going on as soon as you pick like 30 dimensions or more.

3148
03:08:41,560 --> 03:08:54,560
It's all outside of the convex hull of training right but but this this this gets to the punchline right because Keith was going to ask a really cool question which is like well imagine it was a very small latent and it was interpolating.

3149
03:08:54,560 --> 03:09:03,560
And then what if you up projected it like to one or two four dimensions and that you know with with that suddenly now be extrapolating right so that we almost need to have an information theoretical.

3150
03:09:03,560 --> 03:09:16,560
Theoretic way of looking at this but anyway but my intuition is that deep learning models encode the most high frequency information into the latent space right and you know this information would be encoded in a minimally distributed way.

3151
03:09:16,560 --> 03:09:21,560
To denoise the predictive task which is to say right you know there's a few dimensions of the latent.

3152
03:09:22,560 --> 03:09:37,560
You know which should be encoding the actual things that that you trained it on so my intuition is actually most of the dimensions of the latent are kind of just encoding low frequency information so you can discard them right and I know you said the other day well

3153
03:09:37,560 --> 03:09:45,560
Well maybe the whole point of neural networks is there a distributed representation so maybe they are distributed overall of the layers overall of the dimensions in the latent.

3154
03:09:45,560 --> 03:10:03,560
I mean I would I would actually argue not that neural networks are putting the data in a minimally distributed way but in like a maximally like why I think just just my intuition is that a neural if if I were a neural network and I had all of these dimensions at my disposal.

3155
03:10:03,560 --> 03:10:09,560
I would encode lots of information redundantly if it like if it were too much space.

3156
03:10:09,560 --> 03:10:24,560
I would like encode the same information pretty much redundantly in many of the dimensions that I could noisy right so you're taking a soft max and if you're noisily aggregating overall of those features that you don't want to do that.

3157
03:10:25,560 --> 03:10:36,560
Yeah but still if I have backprop right and the backprop path goes through each of the dimensions so for each of the dimensions I'm asking myself how can I change this one to make my prediction even better right.

3158
03:10:36,560 --> 03:10:51,560
It doesn't matter if over there one of the dimensions is already doing the classification for me right the backprop the nature of backprop means that you know I'm going to every single dimension independently and deciding how can I change this one.

3159
03:10:52,560 --> 03:11:07,560
To make it even better so that's why I think lots of information if the latent is too big lots of information will be encoded right there is if the latent is too big and so this is where all the pruning literature comes into which is that.

3160
03:11:07,560 --> 03:11:13,560
The majority of the time people are running neural networks and situations where the latent space is too big.

3161
03:11:13,560 --> 03:11:33,560
Like they're just you know we just flat out have far too many you know far too large of a latent space and so even though it may be encoded you know densely there like it may be putting all kind of little bits of extra information it's probably only adding little plus or minus you know 0.1% type things to the accuracy.

3162
03:11:33,560 --> 03:11:39,560
Whereas and so since it is only adding these little kind of very tiny values there right.

3163
03:11:39,560 --> 03:11:53,560
The only meaningful way to talk about interpolation or extrapolation because you've only got little bits of stuff you're using on that entire dimension that you've added there is really more just are you interpolating on the most salient dimensions.

3164
03:11:53,560 --> 03:12:05,560
You know which is again back to my question about why are we you know why are we concerned with whether or not every single dimension falls within within the sample range.

3165
03:12:05,560 --> 03:12:15,560
Yeah exactly but there's a few things that I mean it's really good that you bring up sparsity I think that's fantastic that you brought that in because as you say most of that information is redundant.

3166
03:12:15,560 --> 03:12:26,560
But then what Yannick was saying was interesting does distributing all that information rather than my intuition is it increases noise but actually I think Yannick saying it increases precision.

3167
03:12:26,560 --> 03:12:36,560
But in a very tiny tiny like it's a tiny tiny contribution so let's suppose the latent space is not too large right like if the latent space is not too large.

3168
03:12:36,560 --> 03:12:51,560
In other words it's let's say it's just barely big enough to classify your images and let's suppose you're doing multi class so we've got 10 10 10 classes you know and we just barely got enough latent space for it.

3169
03:12:51,560 --> 03:13:06,560
My intuition would tell me that if those classes are somewhat different from one another like it's not we're classifying brown dogs from white dogs from like every other you know simple kind of dog but they're different from each other and it's not easy to determine.

3170
03:13:06,560 --> 03:13:24,560
You know which is which that what would and just just to guess is it probably what would happen is you would find that you know these five bits are kind of the ones that tell you whether something is a dog and these seven bits tell you if it's a duck and these four bits tell you if it's a gun and these five

3171
03:13:24,560 --> 03:13:39,560
bits tell you if it's an image of the sky like I would guess that it would end up encoding it that way since they don't have a lot in common you can't really you can't really use entanglement like too much except for a few dimensions there might be some overlaps but.

3172
03:13:39,560 --> 03:13:57,560
Yeah I think I think what what people what people mean a little bit is when they when they start going down the interpolation road is that you know we've played with GPT three as well right and then you you do something with it you enter something and in some places you're like.

3173
03:13:58,560 --> 03:14:13,560
I see how you did that right you like you just you just took that that newspaper article and you just kind of replace some stuff in it right like you you sort of where whereas if you were to to talk to like a human you you sort of.

3174
03:14:14,560 --> 03:14:27,560
Like that stuff wouldn't happen as much even and then of course there's there's the argument I think people that say well it's just interpolating or it might also be you know it's just sort of repeating the stuff in the training data.

3175
03:14:28,560 --> 03:14:44,560
I think what they what I'd like to see is more like the pattern that these models extract aren't sufficiently high level for for them right and then I think the entire discussion is can we get to.

3176
03:14:45,560 --> 03:15:09,560
Arbitrary high abstract levels of pattern recognition with such models if we engineer then train them in the correct way or is there is there I guess some fundamental limitation to that and yes as we said that the answer the answer might be might be quite far off as as for the.

3177
03:15:10,560 --> 03:15:16,560
Number of you know latent dimensions and so on I I mean I agree with with Keith I think.

3178
03:15:17,560 --> 03:15:27,560
Having a big latent space and also having big weights and so on is might be more of a of a necessity for training.

3179
03:15:28,560 --> 03:15:45,560
Then it really is for for encoding like it appears to help if we have lots of weights to train such that sort of we get to combinatorically try out combinations of weights only few of which might ultimately end up being important right.

3180
03:15:45,560 --> 03:15:54,560
That's kind of a lottery hypothesis sort of way of going down so yeah I agree you know most of most of the information ultimately might be.

3181
03:15:55,560 --> 03:16:13,560
Only contributing a little bit right but my my my intuition would meet be that you know this is kind of because it's kind of redundant information right because it's like you know I'm encoding this over here I'm also encoding it in like a tiny little different way over here.

3182
03:16:14,560 --> 03:16:21,560
Or or some add on or some uncertainty or some one training samples a bit different so I'm going to put that right here.

3183
03:16:22,560 --> 03:16:41,560
Yeah that's fascinating what you said about Frank whose work that I never really heard you articulate it like that but it is actually kind of like a search problem rather than a learning problem what you're doing is you're giving it all of the possible you know you give a random initialization on a densely connected network and you're saying you know just go and find the ones that work rather than create it from first principle.

3184
03:16:42,560 --> 03:16:59,560
Yeah that's why initializations are so important right is is because you sort of you sort of try like your initialization essentially is is your your your buffet for SGD to like choose a good one from to then go ahead and refine.

3185
03:17:00,560 --> 03:17:04,560
But how much refining is it is it more is it more finding things that already work versus refining.

3186
03:17:05,560 --> 03:17:26,560
Yeah it's it's it's well the same but what it is not is sort of learning from scratch that that's what like people like we cannot you cannot initialize neural network from zeros and then have it have it learn well at least not today maybe that's going to come in in some form but initialization is actually pretty

3187
03:17:26,560 --> 03:17:48,560
important pretty crazy and that's crazy and that's yeah that's like one hint that you know we're we're not essentially we're never essentially training from scratch where we're we're sort of training we're sort of giving the choice of many combinations of good weights or of semi good weights and it has to pick sort of the good ones to continue exactly it's already pre trained you just don't know which.

3188
03:17:49,560 --> 03:18:09,560
Yeah I mean that the argument that a little bit of the argument against that is in in sort of the evolutionary approach where you say you know you can make the argument you know humans have sort of develop these abilities to reason to recognize super high level patterns while only having a continuous brain.

3189
03:18:10,560 --> 03:18:28,560
But then the other side of this is yeah but it's not like a single human that has achieved that right it's not like one single learning system that has achieved that but it's actually like this evolutionary system which is in essence a massively distributed combinatorical.

3190
03:18:29,560 --> 03:18:40,560
Trial and error search right and that is that is not a learning system so to say as we imagine it today it at the end you end up with a learning result but.

3191
03:18:41,560 --> 03:18:57,560
The evolutionary algorithm is way different than we imagine learning it's it's not even all it's not even all humans that have learned it it's it's all you know individuals of all tens of millions of species that have ever lived on earth that have that have learned.

3192
03:18:58,560 --> 03:19:11,560
Which is pretty much like like the lottery ticket hypothesis let's just let's just randomly train crap tons of you know weights and then slowly prune them and see what happens.

3193
03:19:12,560 --> 03:19:20,560
Right brilliant well gentlemen it's been an absolute pleasure I guess I guess we'll make this a Christmas special I mean it is pretty special.

3194
03:19:21,560 --> 03:19:26,560
Oh yeah let's be honest so yeah this is gonna warrant a hat we're gonna sign off.

3195
03:19:28,560 --> 03:19:29,560
Nice meeting you.

3196
03:19:30,560 --> 03:19:38,560
Indeed but thanks for bearing with us folks we have had a couple of months off I've had a bit of a break and you know the guys have had a bit of a break so yeah we're back.

3197
03:19:39,560 --> 03:19:40,560
Good to see you all again.

3198
03:19:41,560 --> 03:19:42,560
Peace out.

