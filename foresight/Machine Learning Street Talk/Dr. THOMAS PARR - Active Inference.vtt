WEBVTT

00:00.000 --> 00:01.440
So, welcome back to MLST.

00:01.440 --> 00:07.320
Today, we're going to be talking about this book by Dr. Thomas Parr, Giovanni Pazzullo,

00:07.320 --> 00:09.320
and Professor Carl Friston.

00:09.320 --> 00:15.280
Now the book is Active Inference, the free energy principle in mind, brain, and behavior.

00:15.280 --> 00:19.680
So the book, from a pedagogical perspective, it's describing active inference from the

00:19.680 --> 00:22.400
high road and the low road.

00:22.400 --> 00:26.920
And the high road is a little bit kind of helicopter view, so it's saying, OK, we've

00:26.920 --> 00:33.640
got these biological organisms or these living systems, and what do they do in order to be

00:33.640 --> 00:34.640
living systems?

00:34.640 --> 00:40.560
Well, they resist entropic forces acting on them by minimizing their free energy.

00:40.560 --> 00:44.800
So it goes into the how question, but it also goes into the why question from a helicopter

00:44.800 --> 00:45.800
view.

00:45.800 --> 00:51.080
The low road of active inference is far more mechanistic, far more mathematical, and obviously

00:51.080 --> 00:54.640
both all of the roads lead to Rome, if you like.

00:54.640 --> 00:58.200
But the low road is talking about things like Bayesian mechanics, there's a primer

00:58.200 --> 01:02.520
on probability theory, talking about things like variational inference, which is the way

01:02.520 --> 01:09.320
that we solve these intractable optimization problems in active inference, and also talking

01:09.320 --> 01:14.520
about framing active inference as a process theory, which is the latest incarnation of

01:14.520 --> 01:17.440
the description of active inference.

01:17.440 --> 01:20.840
So Professor Carl Friston wrote a preface for the book.

01:20.840 --> 01:25.040
He said, active inference is a way of understanding sentient behavior.

01:25.040 --> 01:31.760
The very fact that you are reading these lines means that you are engaging in active inference,

01:31.760 --> 01:38.520
namely actively sampling the world in a particular way, because you believe you will learn something.

01:38.520 --> 01:40.640
You are palpating.

01:40.640 --> 01:42.040
This is beautiful, by the way.

01:42.040 --> 01:47.280
Friston uses the most beautiful language, it's his signature, if you like, it's his

01:47.280 --> 01:48.520
calling card.

01:48.520 --> 01:53.040
He said, you are palpating this page with your eyes, simply because this is the kind

01:53.040 --> 01:58.480
of action that will resolve uncertainty about what you're going to do next, indeed what

01:58.480 --> 02:00.800
these words convey.

02:00.800 --> 02:07.480
In short, he said, active inference puts action into perception, whereby perception is treated

02:07.480 --> 02:11.120
as perceptual inference or hypothesis testing.

02:11.120 --> 02:16.680
Active inference goes even further and considers planning as inference, that is inferring what

02:16.680 --> 02:21.520
you're going to do next to resolve uncertainty about your lived world.

02:21.520 --> 02:26.440
So I'm about to show you a sneaky clip of Professor Friston that we filmed in January.

02:26.440 --> 02:31.120
I might publish the full show on MLSD in the future, but I just want to take this as an

02:31.120 --> 02:34.080
opportunity to thank you so much for all of our Patreon supporters.

02:34.080 --> 02:39.760
Honestly, it means so much to me because the last few months I've just been, you know,

02:39.760 --> 02:44.880
trying to make this activity of mine, this passion of mine, a full-time job.

02:44.880 --> 02:47.960
And it's not just because you love the show and you want to support me, you get early

02:47.960 --> 02:51.680
access to content, you can join our private Patreon Discord.

02:51.680 --> 02:55.920
We have bi-weekly calls where we, you know, talk about all sorts of random stuff and you

02:55.920 --> 02:58.160
also get early access to lots of our content.

02:58.160 --> 02:59.760
So, you know, please check that out.

02:59.760 --> 03:04.440
But in the meantime, here's a little sneaky clip from Professor Friston.

03:04.440 --> 03:12.960
The neural network is a generative model of the way in which its content work was generated.

03:12.960 --> 03:19.080
And its only job is effectively to learn to be a good model of the content that it has

03:19.080 --> 03:22.120
to assimilate.

03:22.120 --> 03:25.280
If you put agency into the mix, you get to active inference.

03:25.280 --> 03:32.200
And now that we've got a generative model that now has to decide which data to go and

03:32.200 --> 03:33.200
solicit.

03:33.200 --> 03:38.000
And that's actually quite a key move and also quite a thematic move.

03:38.000 --> 03:40.120
So we're moving from perception machines.

03:40.120 --> 03:45.800
We're moving from sort of neural networks in the service of, say, face recognition into

03:45.800 --> 03:55.680
a much more natural science problem of how would you then choose which data in a smart

03:55.680 --> 04:03.040
way you go and solicit in order to build the best models of the causes of the data that

04:03.040 --> 04:05.240
you are in charge of gathering.

04:05.560 --> 04:10.480
Dr. Thomas Parr is a postdoctoral scholar at the Wellcome Centre for Human Neuroimaging

04:10.480 --> 04:14.960
at the Queen Square Institute of Neurology at University College London and a practice

04:14.960 --> 04:15.960
in clinician.

04:15.960 --> 04:19.240
Now, one of the reviews from the book was from Andy Clark.

04:19.240 --> 04:21.080
He said, it should have been impossible.

04:21.080 --> 04:26.200
A unified theory of life and mind laid out in 10 elegant chapters spanning the conceptual

04:26.200 --> 04:31.280
landscape from the formal schemas and some of the neurobiology and then garnished with

04:31.280 --> 04:33.920
practical recipes for active model design.

04:33.920 --> 04:38.080
Philosophically astute and scientifically compelling, this book is essential reading

04:38.080 --> 04:41.400
for anyone interested in minds, brains and action.

04:41.400 --> 04:43.960
Well, I mean, thank you very much for having me on.

04:43.960 --> 04:45.920
So I'm Thomas Parr.

04:45.920 --> 04:51.760
I'm both a clinician and a theoretical neuroscientist.

04:51.760 --> 04:57.160
So I've been working in active inference for a number of years now since I did my PhD

04:57.160 --> 05:03.880
back in 2016 with Carl at the theoretical neurobiology group at Queen Square.

05:03.880 --> 05:10.760
And I'm now based at Oxford where I split my time between research and clinical practice.

05:10.760 --> 05:13.880
So tell me about the first time you met Carl.

05:13.880 --> 05:19.720
The first time I met Carl, I was considering, so I was a medical student at the time at

05:19.720 --> 05:23.600
UCL and I was considering doing a PhD.

05:23.600 --> 05:27.320
And I remember arranging to meet with him and obviously being a relatively nerve-wracking

05:27.320 --> 05:33.760
experience meeting one of the most famous neuroscientists in the world.

05:33.760 --> 05:37.360
I remember discussing with him about it and saying, you know, this is what I'm interested

05:37.360 --> 05:38.720
in.

05:38.720 --> 05:43.760
Would you consider supervising my PhD if I were to get the funding for it?

05:43.760 --> 05:48.840
And I remember he said, yes, all right, then, anything else.

05:48.840 --> 05:51.400
And I asked, do you want to see my CV or anything like that?

05:51.400 --> 05:53.880
And he said, no, I'll only forget it.

05:53.880 --> 05:54.880
Yes.

05:55.560 --> 05:59.000
That was my first encounter with Carl.

05:59.000 --> 06:05.040
But since then, he's always been immensely supportive and has been, you know, exactly

06:05.040 --> 06:11.360
the sort of mentor that I think anybody would want to be able to develop a skill set and

06:11.360 --> 06:12.760
sort of proceed in science.

06:12.760 --> 06:15.520
I come from a machine learning background.

06:15.520 --> 06:21.440
And since discovering active inference and Carl's work, it's really broadened my horizons.

06:21.480 --> 06:26.120
And at the moment, there's an obsession with things like chat GPT.

06:26.120 --> 06:32.120
And I just wondered in your own articulation, how would you kind of pose the work that you

06:32.120 --> 06:34.720
do in relation to that kind of technology?

06:34.720 --> 06:36.120
It's a good question.

06:36.120 --> 06:42.120
And I suppose there are many levels at which it could be answered, aren't there?

06:42.120 --> 06:46.560
I guess thinking about something like chat GPT in that style of technology, it's clearly

06:46.560 --> 06:50.440
been very, very effective at what it does.

06:50.440 --> 06:53.560
But it's worth thinking about what is it that it does?

06:53.560 --> 06:57.080
And I think chat GPT is an excellent example because so many people are familiar with it.

06:57.080 --> 07:03.880
It has such impressive results in terms of being able to simulate very effectively what

07:03.880 --> 07:06.960
it's like to have a conversation.

07:06.960 --> 07:11.880
But ultimately, it is like most deep learning architectures.

07:11.880 --> 07:13.960
It's a form of function approximation.

07:13.960 --> 07:22.120
It's a form of being able to capture very well the output that would be expected under

07:22.120 --> 07:26.000
some set of conditions given some input.

07:26.000 --> 07:32.040
So you give it some text and it knows which text to predict.

07:32.040 --> 07:33.040
And it's very good at that.

07:33.040 --> 07:35.360
But in a sense, that's where it stops.

07:35.360 --> 07:38.520
It doesn't necessarily do anything else.

07:38.520 --> 07:44.120
That's very different to what you and I do when we engage with the world around us, when

07:44.120 --> 07:48.040
we want to learn about the world around us, when we want to form our own beliefs about

07:48.040 --> 07:49.120
what's going on.

07:49.120 --> 07:53.040
And those are the things that I think it doesn't have in the same way.

07:53.040 --> 07:59.920
It certainly can't act and go and seek out specific exchanges, specific conversations

07:59.920 --> 08:02.360
that it might want to learn from.

08:02.360 --> 08:05.440
Whereas you or I might do that if we wanted to know about something specifically, we'd

08:05.440 --> 08:08.600
go and look for information about that thing.

08:08.600 --> 08:13.560
And I think that's where active inference and the idea of having a generative world

08:13.560 --> 08:18.560
model and understanding of what's there in your world that you can alter yourself, that

08:18.560 --> 08:24.240
you can change is very different to a lot of more passive artificial intelligence.

08:24.240 --> 08:28.640
Probably the point where things become closer is in fields like robotics, where you have

08:28.640 --> 08:30.080
to account for both of those things.

08:30.080 --> 08:35.840
You have to model a world that has yourself in it, where your actions affect the data

08:35.840 --> 08:36.840
that you get in.

08:36.840 --> 08:40.560
And I think that's probably where more of the convergence is likely to happen.

08:40.560 --> 08:41.560
Yes.

08:41.560 --> 08:48.640
So you're describing the difference, I guess, between an observational system and an interactive

08:48.640 --> 08:49.640
system.

08:49.640 --> 08:56.000
So in an interactive system, an agent can seek information and change or bend the environment

08:56.000 --> 08:57.520
to suit its will.

08:57.800 --> 09:03.040
Just to linger on this for a second, though, there are folks who do argue that neural networks

09:03.040 --> 09:07.560
are more than hash tables, because I think of them the same way you do.

09:07.560 --> 09:10.240
They essentially learn a function.

09:10.240 --> 09:15.320
And if you densely sample it enough, just like a hash table, it can go and retrieve what

09:15.320 --> 09:17.880
that function says given a certain input.

09:17.880 --> 09:23.000
But there are folks who say, no, no, no, these models learn a world model.

09:23.000 --> 09:29.120
So is given as an example, or with SORA, they say it's learned Navier stokes.

09:29.120 --> 09:30.600
It's a really good question.

09:30.600 --> 09:34.240
And I think there are some open questions here, and I wouldn't claim to have all the answers

09:34.240 --> 09:35.960
to this one.

09:35.960 --> 09:42.160
I think to be able, again, to take chat GPT, to be able to give the answer it does, clearly

09:42.160 --> 09:44.880
it has captured something about the statistics of language.

09:44.880 --> 09:49.720
It's uncovered something about the hidden causes.

09:49.720 --> 09:54.240
So you could argue there is potentially an element of world modeling in there that is

09:54.240 --> 09:56.240
left implicit.

09:56.240 --> 10:01.280
I think it would be very difficult to pull that out or to sort of see that with any transparency

10:01.280 --> 10:03.480
with something like chat GPT.

10:03.480 --> 10:08.640
And so if it does have something of that sort, probably it's the methods that neuroscientists

10:08.640 --> 10:11.960
have been using for years to understand the brain that might help to try and pull out

10:11.960 --> 10:16.960
those same things in those sorts of architectures.

10:16.960 --> 10:21.920
Maybe some sorts of deep learning and neural network models are very good at picking up

10:21.920 --> 10:27.840
regularities in terms of dynamics as well and being able to predict trajectories.

10:27.840 --> 10:35.840
And I think it's important to say that describing something as a function approximator is not

10:35.840 --> 10:37.360
to criticize or belittle it.

10:37.360 --> 10:40.480
It's a very important thing to be able to do.

10:40.480 --> 10:44.200
And it may also be very important in certain types of inference.

10:44.200 --> 10:49.280
So for instance, things like variational autoencoders are based upon often deep learning

10:49.280 --> 10:51.600
neural network architectures.

10:51.600 --> 10:55.800
But the function that is learned is the one that maps from the data I've got coming in

10:55.800 --> 10:59.800
to the posterior beliefs or the parameters of the posterior beliefs that I would arrive

10:59.800 --> 11:05.040
at were I to perform inference of the sort we might do in active inference.

11:05.040 --> 11:10.280
So you've written an absolutely beautiful book on active inference.

11:10.360 --> 11:18.800
And active inference, in my view, it's a theory of agency, which is to say it describes what

11:18.800 --> 11:19.800
an agent does.

11:19.800 --> 11:21.120
And I'm fascinated by agency.

11:21.120 --> 11:24.760
But could you just start by, I mean, from your perspective, could you introduce the

11:24.760 --> 11:27.080
book and tell us about your experience writing it?

11:27.080 --> 11:28.080
Of course.

11:28.080 --> 11:33.400
So the active inference book that we've written is a collaboration between myself, Giovanni

11:33.400 --> 11:38.840
Pazzullo is based in Rome and Carl Friston, who has to take credit for development of

11:38.840 --> 11:42.080
active inference in the first place.

11:42.080 --> 11:51.520
And the book sort of rose out of our sense that there wasn't a unified book out there

11:51.520 --> 11:56.200
or a resource out there to help people learn about what is ultimately a very interdisciplinary

11:56.200 --> 11:58.120
field.

11:58.120 --> 12:02.800
And so we've all had experience with students coming to us asking for resources, asking

12:02.800 --> 12:04.200
what they need to read.

12:04.200 --> 12:08.120
And it may be we refer them to a little bit of neuroscience work, a little bit of machine

12:08.400 --> 12:14.440
learning, textbooks or specific pages on variational inference or whatever else, giving

12:14.440 --> 12:19.840
people introductions to or places they can learn about the maths they need to be able

12:19.840 --> 12:20.840
to do it.

12:20.840 --> 12:24.840
But then also the biology, the underlying psychology, the long sort of tradition of

12:24.840 --> 12:29.320
previous scientists who worked in related areas.

12:29.320 --> 12:32.520
And so the book was an attempt to try and provide a place that people could find all

12:32.520 --> 12:36.840
of that, or at least references to all the relevant things they needed for that, to stick

12:36.920 --> 12:42.120
to the same sort of notation, which is one of the things that's often very difficult,

12:42.120 --> 12:47.800
and the same formalisms and try and introduce everything in a very systematic way to people.

12:47.800 --> 12:54.920
So I'm pleased here that you found it useful, and I hope other people will as well.

12:54.920 --> 13:01.880
The experience of writing it, I mean, so that took place over several years, partly because

13:01.880 --> 13:05.240
the pandemic got in the way in the middle.

13:05.240 --> 13:10.600
So Giovanni and I were passing notes between one another over email and weren't able to

13:10.600 --> 13:13.000
sort of meet in person to discuss it during that time period.

13:15.160 --> 13:21.320
But I think we're all quite proud of the result that we've got out of that, and people seem

13:21.320 --> 13:22.920
to have responded quite well to it.

13:22.920 --> 13:28.200
You start off by talking about what you call a high road and a low road to active inference.

13:28.200 --> 13:29.400
Can you sketch that out?

13:30.280 --> 13:36.920
Yes, and I think this was one of Giovanni's very nice ideas about how to introduce it,

13:36.920 --> 13:39.640
because as I say, it's very multidisciplinary.

13:39.640 --> 13:43.640
There are lots of ways into active inference, and one of the things that's most difficult

13:43.640 --> 13:47.320
for people who are getting into the field for the first time is knowing where to start.

13:47.320 --> 13:53.080
Do they start dealing with the Bayesian brain, unconscious inference, and Helmholtzian ideas

13:53.080 --> 13:59.080
like that, or do they start from a physics-based perspective and start working their way towards

13:59.800 --> 14:01.560
something that looks like sentience?

14:02.600 --> 14:06.120
And there are lots of different, lots of alternative ways people get into it.

14:06.120 --> 14:09.800
The fact that you become interested via machine learning, the fact that other people

14:09.800 --> 14:14.360
have become interested through biology, I developed an interest through neuroscience

14:15.720 --> 14:16.920
while I was at medical school.

14:19.240 --> 14:21.960
And the high road and the low road was a way of just trying to acknowledge

14:22.760 --> 14:28.520
that difference or that difficulty of knowing where to begin, and saying that that's okay,

14:28.600 --> 14:32.120
there are lots of different roads, but they ultimately end up in the same place.

14:33.080 --> 14:37.640
The idea of the low road was to say, well, let's just take observations and psychology

14:37.640 --> 14:40.760
sort of development of a number of ideas that are built up over time

14:42.440 --> 14:47.320
that come to the idea that we're using internal models to explain our world,

14:47.320 --> 14:52.360
that the brain is using something like Bayesian inference, or at least can be described as using

14:52.360 --> 14:59.400
Bayesian inference, and go from there through the advances that lead you to active inference,

14:59.400 --> 15:04.520
the idea that it's not just a passive process that you're also inferring what I'm going to do.

15:05.640 --> 15:10.040
And furthermore, that when we're doing inference, we're changing our beliefs

15:10.760 --> 15:14.840
to reflect what's in the world around us and to explain our sensory data.

15:15.480 --> 15:18.840
But actually, when we're acting in the world, we can also change the world itself

15:18.920 --> 15:25.400
to better comply with our beliefs. So it's that move from purely changing our beliefs to reflect the

15:25.400 --> 15:31.080
world to also changing the world to reflect our beliefs. And that fascinating move that

15:31.080 --> 15:35.720
actually both can be seen as optimization of exactly the same objective that they have the

15:35.720 --> 15:41.800
same goal, that in both cases, it's really just improving the fit between us and our world.

15:43.000 --> 15:47.720
So that's the sort of low road perspective. The high road perspective was the idea of saying,

15:48.360 --> 15:52.920
well, let's start from the minimum number of assumptions we can, let's start from first

15:52.920 --> 16:00.360
principles. And that takes a much more physics based approach. It says, if you have a creature

16:00.360 --> 16:06.360
that is interacting with its world, then there are a number of things you've already committed to,

16:06.360 --> 16:11.000
and that includes things like the persistence of that creature over a reasonable length of time,

16:11.000 --> 16:14.920
the maintenance of a boundary between that creature and its world, and that sort of

16:14.920 --> 16:21.880
self world distinction. And once you've committed to those things, you can then start to write down

16:21.880 --> 16:26.520
the constraints that those imply in terms of the physical dynamics of that system.

16:26.520 --> 16:30.760
And you can start to interpret those dynamics in terms of the functions they might be

16:30.760 --> 16:36.920
optimizing, much like, much like if you were to write down the equations that underpin Newtonian

16:36.920 --> 16:41.880
dynamics, you can write them down in terms of their flows on Hamiltonian functions. And it's

16:41.880 --> 16:46.600
following the same sort of logic to then get to flows on free energy functions, where free energy

16:46.600 --> 16:52.120
is just a measure of that fit between us and our world. And so both roads ultimately end up leading

16:52.120 --> 17:00.600
to this common endpoint, which is that to be an agent in our worlds, in the sort of worlds we

17:00.600 --> 17:07.160
live in, we have to be able to change our beliefs, to reflect what's going on around us and change

17:07.240 --> 17:14.840
the world through our dynamical flows on a free energy functional, to best fit with the sorts

17:14.840 --> 17:19.400
of creatures we are. When we first started looking at the free energy principle, we were talking

17:19.400 --> 17:27.400
about things. It was known as a theory of every thing, every space thing, which is to say, roughly

17:27.400 --> 17:35.160
speaking, if a thing exists, what must the thing do to continue to exist? And just their continued

17:35.160 --> 17:41.640
existence resisting entropic forces is what defines them, which gets us into the second law

17:41.640 --> 17:46.280
of thermodynamics. Now, that sounds like quite a strange thing to say. Why do things need to

17:46.280 --> 17:51.640
resist entropic forces? And I think there's a development in how a lot of these ideas are

17:51.640 --> 17:57.880
presented over time, which you expect and hope for in science. And I think we've often taken

17:57.880 --> 18:03.800
different perspectives at different points in time as to how we explain these ideas.

18:04.760 --> 18:11.000
And resisting entropic forces is an idea that I think most people find relatively intuitive.

18:11.640 --> 18:19.960
So the idea that the physical systems will tend to increase their entropy over time,

18:21.640 --> 18:28.200
at least close systems, so that over time things will gradually dissipate, things that are highly

18:28.200 --> 18:32.200
structured and highly ordered and can only exist in a very small number of configurations and

18:32.280 --> 18:37.240
more likely to change into something that can exist in many different configurations than they

18:37.240 --> 18:43.640
are to go in the opposite direction. But anything that persists over time and maintains its form

18:43.640 --> 18:49.640
clearly resists that process of decay, at least to some extent, or at least for some period of time.

18:51.320 --> 18:56.440
However, the opposite is also true. We're also not creatures that tend towards a zero entropy

18:56.440 --> 19:02.840
state. We don't end up in a single configuration. We have to be flexible. We have to change in

19:02.840 --> 19:11.320
various ways throughout our lifetime or even throughout our daily routine. So it's not quite

19:11.320 --> 19:17.960
as simple as just saying you have to resist entropic change. It's more to say that entropic

19:17.960 --> 19:22.760
change or the amount of entropy that you expect to develop has to be bounded both from above

19:22.760 --> 19:28.600
and below, that there is a sort of optimum level to be at. And that optimum probably varies from

19:28.600 --> 19:33.560
different, well, from person to person, from creature to creature, from thing to thing.

19:34.120 --> 19:39.240
You could imagine a rock that doesn't need to do much. Its interface with the environment is

19:39.240 --> 19:47.160
quite trivial versus us as agents. We are incredibly sophisticated. So for us to continue to exist,

19:47.160 --> 19:51.080
we have many more ways of interfacing with the environment and we need to plan

19:51.080 --> 19:56.520
many more steps ahead. So is that just a pure continuum between rocks and people?

19:59.160 --> 20:07.880
I mean, in principle, yes. I mean, the notion of that persistence, of that resistance of entropy

20:07.880 --> 20:11.720
will depend very much on what you are. And as you say, you could imagine a whole scale of

20:11.720 --> 20:17.960
things in between. I mean, in a way that as you've highlighted with the rock,

20:17.960 --> 20:21.560
some of the most boring things are the things with the, sorry, I shouldn't say that,

20:21.560 --> 20:28.600
poor geologists who might find rocks very interesting. And I'm sure are very complex, but

20:30.760 --> 20:35.720
from a sort of behavioral perspective, clearly things like us are much more interesting to study

20:35.720 --> 20:40.600
than things like a rock. And part of that is that we actually have a higher degree of entropy in

20:40.600 --> 20:47.640
how we live our daily lives compared to things like, I almost said organisms like rocks,

20:47.640 --> 20:53.080
but things like rocks that are not behaving. The reason I was thinking about this is the

20:53.080 --> 20:58.520
second law of thermodynamics was conceived, I don't know, 150 years ago or something like that.

20:58.520 --> 21:03.720
And many people at the time thought that it was an affront on free will. I think the religious

21:03.720 --> 21:09.560
people at the time were aghast at the idea that things were mapped out in this way.

21:09.560 --> 21:15.560
It's always worth saying in this discussion that obviously the tendency for entropy to increase

21:15.560 --> 21:22.520
from a physical perspective generally relates to closed systems of which we are not. And as soon

21:22.520 --> 21:27.640
as you start talking about different compartments and interactions between them, you also introduce

21:27.640 --> 21:33.560
the idea of several coupled systems. And so you can start to ask questions about the overall entropy

21:33.560 --> 21:41.720
or the entropy of specific parts of that system. And agents and worlds are two compartments and

21:41.720 --> 21:47.160
systems that exchange things with one another. And so are not closed systems almost by definition

21:47.160 --> 21:53.000
that a closed system, again, from a sort of neuroscience standpoint is not necessarily

21:53.000 --> 22:00.840
a very interesting system. So probably that deals with a large part of that. The question of free

22:00.840 --> 22:05.720
will is always an interesting one and always a thorny one that I'm not going to claim to have any

22:05.800 --> 22:12.920
expertise on or be able to answer. But I think it probably tackles a slightly

22:12.920 --> 22:19.400
different thing from a cognitive science perspective, which is whether or not we believe

22:19.400 --> 22:24.920
that the actions we're taking are actions that we've chosen. And that probably comes back into

22:24.920 --> 22:30.440
another aspect of active inference, which is that idea that the way we're regulating our

22:30.440 --> 22:37.160
worlds, the way we're perhaps changing the entropy of our environment depends upon our own

22:38.680 --> 22:45.160
choices about it, our inferences about which one we're going to do next. And that feeds into things

22:45.160 --> 22:52.600
like we've spoken about free energy, that that quantity that we use to both choose our actions,

22:53.480 --> 22:58.040
an act in the world around us while also drawing inferences. But we can also talk about things

22:58.040 --> 23:03.640
like expected free energy, which is a way of evaluating our future state and what would be a

23:03.640 --> 23:09.400
good trajectory or a good way for the world to play out. And their entropy has a completely

23:09.400 --> 23:15.560
different meaning and there are different sorts of entropy. So for instance, if I were choosing

23:15.560 --> 23:19.240
between several different eye movements I could make while looking around this room,

23:21.320 --> 23:26.840
the best eye movements I might choose are those for which I'm least certain about what I would see.

23:26.840 --> 23:32.280
In other words, the highest entropy distribution. So once you start planning in the future and once

23:32.280 --> 23:36.920
you start selecting things to resolve your uncertainty and to be more confident about the

23:36.920 --> 23:42.680
world around you, you actually end up seeking out entropy, which it seems to then very much

23:42.680 --> 23:46.200
contradict some of the other ideas that we were talking about, the idea that we're constantly

23:46.200 --> 23:52.200
resisting it. But actually it's by seeking out the things that we're least certain about that we

23:52.200 --> 23:57.800
can start to resolve that uncertainty and start to become more confident and more certain about the

23:57.800 --> 24:02.920
world around us. Yes, resist entropy by seeking it out. That's a bit of a paradox. But even what

24:02.920 --> 24:09.560
you were saying just a second ago about this description of how agents operate, it's very

24:09.560 --> 24:15.320
principled. We were talking about this balancing epistemic foraging versus sticking with what you

24:15.320 --> 24:23.080
know. And more broadly speaking, thinking of agency as this sophisticated cognition of

24:23.080 --> 24:27.400
having preferences and bending the environment and so on. And I guess where I was going before

24:27.400 --> 24:35.160
is it's tempting to think that this erodes free will. And I think of them quite adjacently in

24:35.160 --> 24:41.480
my mind. If anything, I guess I would call myself a free will compatibilist, which means it doesn't

24:41.560 --> 24:49.800
matter that it's predetermined. For me, free will, I'll try not to use the word free will, but

24:49.800 --> 24:54.360
thinking of agency in this sophisticated way, whether it's predetermined or not is irrelevant.

24:54.360 --> 25:02.360
It's the complex dynamics that distinguishes my agency from somebody else's. So I think agency

25:02.360 --> 25:07.960
is better to think of than free will, if that makes sense. Yeah. And I think that's probably right.

25:08.200 --> 25:12.600
And the experience of and the inference of agency as well, I think is part of that.

25:13.800 --> 25:17.800
There's a potential link that you can draw here also to the idea of chaotic,

25:17.800 --> 25:27.000
dynamical systems of which we essentially are examples. And the idea of chaos in that setting

25:27.000 --> 25:33.960
is that if you start from two ever so slightly different initial conditions, your path and your

25:33.960 --> 25:39.000
future may unfold in a completely different way. And I think that fits very nicely with what you're

25:39.000 --> 25:45.000
saying about distinguishing my agency from somebody else's because you don't see it as if I were,

25:45.800 --> 25:51.800
you know, the time going to behave in exactly the same way somebody else's. And part of the

25:51.800 --> 25:56.680
reason for that is that you end up starting from a slightly different perspective to where they are,

25:56.680 --> 25:59.880
and that might lead to wildly different futures for both of you.

26:00.440 --> 26:09.480
So something I think about a lot is whether agents are ontologically real or whether they

26:09.480 --> 26:14.760
are an instrumental fiction. And I think part of the complexity, especially with active inference

26:14.760 --> 26:20.920
and the free energy principle is this hierarchical nesting. So we can think of agents inside agents

26:20.920 --> 26:27.800
inside agents. And I guess the first question is, are they real and does it matter?

26:28.600 --> 26:30.840
Define real for me.

26:36.520 --> 26:46.600
Well, one argument would be that they are epiphenomenal, that they themselves don't affect

26:46.600 --> 26:50.520
the system that they are. Is this a good way to think about it?

26:53.960 --> 26:57.480
It is a very difficult question to try and contend with, isn't it? Because I think there

26:57.480 --> 27:02.680
are so many words that come up here that are kind of laden with different semantics or different

27:02.680 --> 27:09.960
meanings depending upon who you speak to and which camp they come from in the sort of philosophical

27:09.960 --> 27:18.360
world. And that's why I sort of asked you to define real. And it's really difficult to define

27:18.360 --> 27:24.840
what real means in that setting, isn't it? And I guess coming back to your original question there,

27:24.840 --> 27:29.960
for me, does it matter if they're a sort of real thing or not? Probably not. It matters

27:29.960 --> 27:35.960
whether it's useful. And I guess that sort of brings me to a point about one of the things I

27:35.960 --> 27:41.400
find quite appealing about active inference as a way of doing science. And I think,

27:43.720 --> 27:47.960
you know, having had an interest in things like neuroscience and psychology for some time,

27:47.960 --> 27:51.960
I often found it quite frustrating to understand what people meant and the different language

27:51.960 --> 27:55.800
they used in psychology to understand different aspects of cognitive function.

27:57.320 --> 28:01.400
And I think, you know, it's worth acknowledging that actually lots of people mean completely

28:01.400 --> 28:08.520
different things when they say attention. And some people say attention to mean the sort of

28:08.520 --> 28:13.640
overt process of looking at something and paying attention to it. Other people use it to talk

28:13.640 --> 28:19.720
about that the differences in gain in different sensory channels that they're trying to pay

28:19.720 --> 28:24.120
attention to or not, you know, am I paying attention to colors versus something else?

28:24.120 --> 28:27.160
And that's just turning up the volume of different pathways in your brain.

28:28.360 --> 28:32.040
And I'm sure there are a world of other things that people mean by it as well.

28:32.840 --> 28:36.040
But the idea of then trying to commit to a mathematical description of these things

28:36.600 --> 28:41.320
means that a lot of that ambiguity just disappears, that if you put a word to a

28:41.320 --> 28:45.320
particular mathematical quantity, as long as you define what that mathematical quantity is

28:45.320 --> 28:50.600
and how it interacts with other things, then a lot of that ambiguity just isn't there.

28:50.600 --> 28:56.200
And it forces you to commit to your assumptions in a much more specific way.

28:57.160 --> 29:01.960
And so that's why I come back to say, does it necessarily matter if an agent is real or not?

29:01.960 --> 29:07.640
I don't really know what that means. But if an agent is just a description of something that is

29:07.640 --> 29:12.280
separated from its environment that persists for a certain length of time, that has a dynamical

29:12.360 --> 29:16.600
structure that can be written down and a set of variables that can be partitioned off from another

29:16.600 --> 29:23.240
bit of the world. For me, that's real enough to be useful. And so that's where I'd go with that one.

29:23.880 --> 29:31.800
Yes, yes. This is fascinating. So it's a mathematical theory that carves the world up

29:31.800 --> 29:35.720
in an intelligent way that explains what things do and what they don't do.

29:36.360 --> 29:42.120
And I guess the ontological statement, maybe we can park that to one side,

29:42.760 --> 29:47.320
because as you say, from a semantics point of view, people have very relativistic

29:47.320 --> 29:53.080
understandings of things. And there's always the philosophical turtles all the way down. Well,

29:53.080 --> 29:57.640
is it really real? Is it really real? But one thing that is interesting, though,

29:57.640 --> 30:03.560
about active inference is that it's quite mathematically abstract. So when we were

30:03.560 --> 30:09.560
saying, is it real? It doesn't even designate, is it physical? So for example, a boundary is just

30:09.560 --> 30:16.520
talking about the statistical independence between states. And those don't necessarily

30:16.520 --> 30:21.960
correspond to physical things. So I guess it could be applied to almost anything. It could be applied

30:21.960 --> 30:27.640
to culture or memes or language or something like that. And it has been. Yes, indeed.

30:28.520 --> 30:33.640
Yeah, it's a good point. And then you end up sort of dragged into the questions of what is

30:33.640 --> 30:39.000
physical. What does that mean? Is physical just an expression of dynamics that evolve in time?

30:39.000 --> 30:42.680
Because I mean, even committing to a temporal dimension tells you something about the world

30:42.680 --> 30:48.760
you're living in. Are the boundaries that we're talking about, are the partitions,

30:48.760 --> 30:55.960
are they spatial in nature or not? And, you know, I remember there was an article a little

30:55.960 --> 31:02.040
while back that sort of made a lot of argument about this as to whether the partitions that

31:02.040 --> 31:07.160
divide creatures from their environments are equivalent to statements of conditional independence

31:07.160 --> 31:12.200
of the sort that are seen in machine learning or various other things. And arguing that there's

31:12.200 --> 31:19.720
something inherently different about a physical boundary. For me, I was never completely convinced

31:19.720 --> 31:23.800
by that, but partly because you have to then define what you mean by a physical boundary.

31:23.800 --> 31:27.960
And I suspect it's the same sort of boundary, it's the same sort of conditional

31:27.960 --> 31:34.680
dependencies and independences. But where those have specific semantics, whether those be temporal,

31:34.680 --> 31:40.840
whether they be something where, you know, you can actually define a proper spatial metric

31:40.840 --> 31:46.840
underneath the things that you're separating out. And clearly, that sort of boundary is very

31:46.840 --> 31:51.640
important. But for me, that is just another form of the same sort of boundary. And as you say,

31:51.640 --> 31:57.160
you can apply exactly the same sort of ideas to things that are not spatial, not sort of physical,

31:57.160 --> 32:07.240
whatever that might mean. Yes. Yes. Because when I when I spoke with Carl last time, I was pressing

32:07.240 --> 32:14.280
him on this idea of a non physical agent, and he was quite allergic to the idea. And I suppose,

32:14.280 --> 32:19.160
even though mathematically, you could apply it first to other geometries, that would be

32:19.240 --> 32:23.160
quite easy, because they have certain mathematical properties in terms of like, you know,

32:23.160 --> 32:27.400
being locally connected and measure spaces and all that kind of stuff. But if you did say,

32:27.400 --> 32:35.800
okay, I want to have an agent that represents a meme. How would that act? I don't know,

32:35.800 --> 32:46.040
you get into modeling challenges, don't you? I suppose you do. I think the modeling challenge

32:46.040 --> 32:51.080
is defining the boundary. I think the boundary is a very difficult thing to define sometimes

32:51.080 --> 32:55.720
when you're dealing with something non spatial. That boundary, though, might be reflected in

32:55.720 --> 33:04.440
the interactions between a meme and a community that that engage with it. It might be to do with

33:04.440 --> 33:09.560
the expression of a meme in different parts of, I don't know, a network of some sort or social

33:09.560 --> 33:15.480
network. I don't know how easy it would be. I've not tried to do it in that context. And I think

33:15.480 --> 33:19.240
with many of these things, you never really know until you've had to go at doing it. But

33:21.400 --> 33:26.280
I suppose the key things I would be thinking about are, is there a clean way of defining

33:26.280 --> 33:31.240
a boundary for a meme? Is there something that the meme is doing to the outside world?

33:31.960 --> 33:34.440
Is there something that the outside world is doing to the meme?

33:36.680 --> 33:41.800
And I think if you're able to define those things convincingly, then perhaps there is a form of

33:41.880 --> 33:49.960
agent that may be non physical, if that's how you choose to define it. But then I'm not sure what

33:49.960 --> 33:53.560
physical means in this setting. Is there also an account of saying, well, actually,

33:53.560 --> 33:56.600
if you can write down the dynamics of how a meme propagates through a network,

33:57.320 --> 34:00.680
is that any different writing down the dynamics of another sort of physical system?

34:01.800 --> 34:09.080
Yes, possibly not. But it is really interesting to me that something like language could be seen

34:09.080 --> 34:14.440
as a life as a super organism, or even something like religion. And it seems to tick all of the

34:14.440 --> 34:21.640
boxes that we talk about with a gentleness in physical agents, which is to say, let's say

34:21.640 --> 34:27.560
a religion or even nationalism, you could say that the state of the Netherlands has certain

34:27.560 --> 34:35.880
objectives. And clearly, there's a two way process here. So the state affects our behavior. And we,

34:36.360 --> 34:43.000
our collective behavior influences the state. But this then, I think the reason why people don't

34:43.000 --> 34:49.000
like to think in this way is we have psychological priors. So we are biased towards seeing a

34:49.000 --> 34:55.960
gentleness in individual humans, but we tend not to think of non physical or diffuse things as being

34:55.960 --> 35:01.480
agents. Yes, I think that's probably right. And again, it sort of brings us back to this whole

35:01.480 --> 35:07.480
issue about the language that we use, that it comes laden with lots of prior beliefs about what it

35:07.480 --> 35:16.360
means, which may vary from person to person. And there comes a point where you say, how important

35:16.360 --> 35:20.920
is it that I commit to using this particular word to mean this particular thing in this setting?

35:21.720 --> 35:29.880
But again, in your example of taking a nation or nation state as being a form of organism at a

35:30.120 --> 35:37.400
higher level or form of agent, if you can show that there is a way of summarizing the dynamics of

35:37.400 --> 35:43.960
that system, maybe some high order summary of the behavior of people in that system, voting

35:43.960 --> 35:50.360
intentions, I don't know, you might then be able to show that it behaves in exactly the same way

35:50.360 --> 35:57.480
mathematically as individuals within that system. Yeah. So this brings me on a little bit too. I've

35:57.480 --> 36:02.040
been reading this book called The Mind is Flat by Nick Chaito and I'm speaking to him on Friday.

36:02.040 --> 36:07.720
And his main take is that, I guess you could call him a connectionist, he's friends with Jeffrey

36:07.720 --> 36:15.080
Hinton. And his main take is that there is no depth to the mind. So for years, psychologists have

36:15.080 --> 36:23.240
built these abstract models to reason about how we think. So we do planning, and we do reasoning,

36:23.240 --> 36:28.440
and we have perception, and we do this, and we do that. And also, we try and generate explanations

36:28.440 --> 36:34.760
for our behavior. So we do this kind of post hoc confabulation. But when you study it, it's incredibly

36:34.760 --> 36:41.960
incoherent and inconsistent. And he was talking all about how the brain is actually a kind of

36:41.960 --> 36:49.640
predictive system, right? So we have these very sparse incoherent inputs, and we sometimes see

36:49.640 --> 36:54.840
things that aren't there. And I think you speak about this in your book that there was a really

36:54.840 --> 37:00.760
big shift. I think you referred to it as the Helms-Hotsian idea that the brain is a kind of

37:00.760 --> 37:06.440
prediction machine, rather than our brain just kind of like building a simulacrum of the world

37:06.440 --> 37:13.000
around us. I mean, how do you think about that as a neuroscientist? Yeah, I mean, I think prediction

37:13.000 --> 37:19.480
has to be a key part of it. And the reason it's a key part of it is that it's a way of coupling

37:19.480 --> 37:25.640
us to our world that without prediction, you know, if you're purely simulating what might be going on

37:25.640 --> 37:29.960
without actually then correcting your simulation based upon what's actually going on or the input

37:29.960 --> 37:35.240
you're getting from the world, then you're not going to get very far. So prediction is just an

37:35.240 --> 37:40.520
efficient way of dealing with the issue of how do I update my beliefs? How do I update if you want

37:40.520 --> 37:44.520
to call it a simulation? My simulation, my internal simulation of what's going on outside.

37:45.240 --> 37:51.400
And once you cease to have that constraint, once the world ceases to constrain the simulation,

37:51.400 --> 37:55.480
that's the point at which you start, as you say, hallucinating, seeing things that aren't there

37:56.360 --> 38:00.920
and developing beliefs that just bear no relationship to or little relationship to reality.

38:02.200 --> 38:09.160
Yeah, interesting. So I mean, one thing this Nick Chaitaguay was saying was that we see a

38:09.160 --> 38:15.640
complex system and we adopt what Daniel Dennett calls the intentional stance. And that is I have

38:16.360 --> 38:23.080
a self model, I have a model of your mind, and I observe behavior and I kind of impute

38:23.960 --> 38:30.200
onto you a model and I can generate explanations. So as I say, Thomas did that because he must

38:30.200 --> 38:36.280
have wanted to do this. And I guess you could argue that all of this is just a confabulation.

38:36.280 --> 38:44.040
It's just an instrumental fiction. It's a way for us to explain behavior, but it doesn't really exist.

38:44.040 --> 38:48.920
But then there's the question of, well, it's not that it doesn't exist. It's just that your mind

38:49.480 --> 38:55.960
is incomprehensibly complex. So it's not that the mind is shallow. I prefer to think of it as

38:55.960 --> 39:01.480
the mind has so much depth that it's beyond our cognitive horizon. And depth, I think, is an

39:01.480 --> 39:08.120
interesting notion as well. I mean, it's the idea that comes under a lot of machine learning and

39:08.120 --> 39:14.280
the idea of deep learning neural networks with multiple layers. And I think you're right that

39:14.280 --> 39:19.880
depth is an important part of our generative models as well, of our brains models of the world.

39:20.840 --> 39:27.720
And part of that comes from the fact that the world actually does separate out into a whole

39:27.720 --> 39:32.760
different series of temporal scales of things that happen slowly, that contextualize things that

39:32.760 --> 39:37.960
happen more quickly, that contextualize things that are even faster than that. And so one good

39:37.960 --> 39:42.760
example of depth might be that if you're reading a book, then you have to bear in mind which page

39:42.760 --> 39:47.160
you're on within that page, which sentence or which paragraph you want, within paragraph,

39:47.160 --> 39:52.840
which sentence, within the sentence, which word, within the word, which letter. And by combining

39:52.840 --> 39:57.720
your predictions sort of both down the system that way, but then updating your predictions

39:57.720 --> 40:02.680
all the way back up again, you start to be able to make inferences about the overall narrative

40:02.680 --> 40:11.400
that you're reading. The other thing you mentioned that I thought was interesting was the idea of

40:11.400 --> 40:17.400
confabulation and of how we come to beliefs about other people's behavior. And I think the same

40:17.400 --> 40:21.800
thing is also true about our own behavior and sort of making an inference about what we've done.

40:22.440 --> 40:27.160
And this comes all the way back to the sense of agency again, doesn't it? It comes back to the

40:27.160 --> 40:31.960
idea that I am inferring, I'm behaving in this way for this reason, because I've chosen to do this,

40:31.960 --> 40:39.400
because I had this goal in mind. And to come back to the other question, is that real? Or is it

40:39.400 --> 40:45.560
simply an inference about what I've done? I would suggest that it's certainly an inference about

40:45.560 --> 40:53.080
what I've done, whether or not it's real. Giovanni and I put together some simulations

40:53.080 --> 40:58.680
and some theoretical work a couple of years ago after a discussion at a conference about or a

40:58.680 --> 41:04.200
workshop about machine understanding, suggesting that machine intelligence is one thing, but

41:04.200 --> 41:09.640
actually understanding why you've come to a particular conclusion. ChatGPT being able to

41:09.640 --> 41:15.800
explain to you why it came up with a specific sequence of words or why a convolutional neural

41:15.800 --> 41:22.280
network classified an image in a particular way is one of the big issues really, and there are

41:22.280 --> 41:26.840
solutions coming up, but it's one of the big issues in the deep learning community as to how

41:26.840 --> 41:30.600
you have that transparency in terms of what the models are doing and why they're doing it.

41:33.320 --> 41:36.840
Giovanni and I put together some work following that, looking at

41:37.560 --> 41:41.560
understanding of our own actions from an active inference perspective, and there it was very much

41:41.560 --> 41:46.760
framed as I have a series of hypotheses of things I might do, of reasons why I might do that.

41:47.560 --> 41:52.200
And then after observing myself behaving in a particular way, I can then use my own behavior

41:52.200 --> 41:58.200
as data that I then have to come up with an explanation for. And it's very interesting to

41:58.200 --> 42:03.080
see what happens if you start depriving that of aspects of its behavior and to see the confabulations

42:03.080 --> 42:08.600
that result from that. I can't remember where it came from originally, the idea of hallucinations

42:08.600 --> 42:17.960
being a perception generally being effectively a constrained hallucination, where you take your

42:17.960 --> 42:21.800
hallucination, your simulation of what's going on, and then you fix it to what's actually coming in.

42:22.920 --> 42:26.440
But you could argue that actually a lot of our understanding about what we're doing is also

42:26.440 --> 42:29.720
just a constrained confabulation in exactly the same way.

42:30.680 --> 42:36.200
Yes, which is very ironic because people diminish GPT and because they say it's just

42:36.200 --> 42:43.800
confabulating, whereas the preeminent neuroscientists of the day do basically make the same argument

42:43.800 --> 42:49.480
about how the brain works, and even our communication now on conditioning your simulator.

42:49.480 --> 42:53.480
So the semantics are drawn by your own model in simulation of the world,

42:53.480 --> 42:58.040
rather than being the simulacrum of mine. You spoke about machine understanding,

42:58.040 --> 43:04.040
I mean, there's this Chinese Rem argument. And we're in a really interesting time now because

43:04.680 --> 43:11.880
we have artifacts that behave in a way which is isomorphic in many ways.

43:12.440 --> 43:20.680
And it's so tempting to say, well, we're different. And you could make the ontological argument,

43:20.680 --> 43:26.280
but this psychological argument is a big one as well, which is we're different because we have

43:27.080 --> 43:31.400
beliefs, motives, volition, desires, we have all of these things.

43:31.400 --> 43:35.160
But as we were just saying before, this is all post hoc confabulated.

43:35.160 --> 43:39.560
We actually don't have consistent beliefs and desires. It's just a fiction.

43:41.240 --> 43:43.960
Was it a fiction or is it a plausible explanation?

43:45.480 --> 43:50.920
Well, I guess the thing that breaks it for me is the incoherence and inconsistency,

43:51.000 --> 43:58.040
because you would think that we would be fully fledged human agents if we had consistent beliefs

43:58.040 --> 44:02.920
and desires. And it's not to say that we don't because it feels like some of our goals are

44:05.000 --> 44:11.560
they grounded in some way, like we need to eat food. But we think of ourselves as being

44:11.560 --> 44:17.320
unique as humans, because we have higher level goals and beliefs that aren't necessarily instrumental

44:17.720 --> 44:22.280
to eating food. And I guess those things in particular might be confabulatory.

44:23.160 --> 44:27.720
Yes. So on the volition thing, that's something that really interests me.

44:30.440 --> 44:37.080
An active inference agent is we draw a boundary around a thing and it can act in the environment

44:37.080 --> 44:42.600
and it has preferences. And essentially, it has a generative model where it can produce these

44:42.600 --> 44:48.360
plans, these policies, if you like, and at the end of every single plan is an end state.

44:49.480 --> 44:56.360
So it's got all of these different goals in mind, if you like. And in the real world,

44:57.240 --> 45:04.280
real in big air quotes, these things emerge. But when we design these agents, we need to

45:04.280 --> 45:11.080
somehow impute the preferences onto them. So it feels like they have less agency if we

45:11.880 --> 45:17.560
impute the preferences. Would you agree with that? Interesting question.

45:19.800 --> 45:29.800
And a very relevant question in the current number of industry related applications of

45:29.800 --> 45:34.360
active inference. I think we were speaking about earlier, there are a number of companies now

45:34.360 --> 45:39.800
that have been set up looking at use of active inference based principles for various problems.

45:40.680 --> 45:45.400
Companies like Versus that we spoke about before and Stan Hope AI that I do some work with as well.

45:47.160 --> 45:52.920
And the issue there is very much, it's a different kind of issue to the biological

45:52.920 --> 45:58.680
issue of describing how things work. And it's the issue of saying, if I now want to design an

45:58.680 --> 46:03.160
agent to behave in a particular way, as you say, am I taking some agency away from that?

46:03.160 --> 46:09.400
There are a couple of things to think about there. I suppose one is thinking about

46:10.840 --> 46:15.240
do biological agents actually select their own preferences to begin with?

46:17.080 --> 46:21.720
And I think most people would probably say they don't most of the time. There may be certain

46:21.720 --> 46:26.360
circumstances where they do or where a particular preference might be conditionally dependent upon

46:27.240 --> 46:32.280
the task I'm in, the scenario I'm in, whether I'm at work or at home or whatever else. But it's

46:32.280 --> 46:37.160
not that I'm actually selecting this is what I want to want. There is a famous quote here,

46:37.160 --> 46:40.280
but I can't remember what it is. I don't know whether you do. No.

46:43.960 --> 46:51.000
No, it's escaped me about wanting what you want or wanting what you do or something along those

46:51.000 --> 46:57.160
lines. Anyway, the point I'm making is that, to some extent, our preferences are given to us

46:57.160 --> 47:02.360
effectively through a process of evolution, natural selection, previous experience that has

47:02.360 --> 47:09.000
affected what is a good set of states to occupy. And those will often be a good set of states that

47:09.000 --> 47:18.920
help my survival, that help the persistence of the species that I'm a part of. And arguably,

47:18.920 --> 47:25.240
the same thing is true when you as a designer of a particular algorithm or an agent are giving it

47:25.320 --> 47:30.120
a set of preferences. From its perspective, it's never selected them anyway. And that's the same

47:30.120 --> 47:36.200
as you or I not necessarily having selected our preferences. There's one additional element that

47:36.200 --> 47:43.960
I think is interesting to think about. And one of my colleagues and collaborators,

47:43.960 --> 47:48.600
Nor Sajid, has done a lot of interesting work on this, which is the idea of learning your own

47:48.600 --> 47:54.280
preferences, of actually saying, let's create an agent that isn't given preferences to begin with,

47:54.280 --> 48:00.600
but is allowed to learn as it behaves what sort of goal states it ends up in.

48:01.880 --> 48:08.280
And there you get some very interesting results. So she showed that these sorts of agents

48:09.640 --> 48:13.720
may end up doing things that you just don't want them to do, that they end up forming a

48:13.720 --> 48:18.040
particular pattern of being or a particular way of being that you as a designer might never have

48:18.040 --> 48:23.160
envisaged. For example, in an environment with lots of potential holes that it can fall into,

48:23.160 --> 48:28.280
some of these agents just become hole dwellers. They just decide, I found that the first few

48:28.280 --> 48:32.280
times I did this task, I fell into the hole. So I've decided I'm probably the sort of creature

48:32.280 --> 48:38.520
that likes living in a hole. So that's a situation where you can give it a certain agency. And maybe

48:38.520 --> 48:44.520
that agency is the ability to sort of disagree with what you as a designer might expect or want

48:44.520 --> 48:50.120
from it. Yes. This is so interesting. I mean, we're getting a little bit into, we'll have a

48:50.120 --> 48:59.000
discussion about cybernetics and externalism. But so what you're describing there is the reason

48:59.000 --> 49:06.680
why AI systems today are not sophisticated is because they are convergent. And that's usually

49:06.680 --> 49:13.240
because they don't actually have any agency. So one of the hallmarks of the physical real

49:13.240 --> 49:19.400
systems in the real world is that they have these divergent properties. And that's because you have

49:19.480 --> 49:24.600
lots of independent agents following their own directiveness doing epistemic foraging. So

49:24.600 --> 49:28.760
interesting stepping stones get discovered. And sometimes those stepping stones aren't what the

49:28.760 --> 49:34.120
designer of the system would have liked, as you just said. So there's an interesting kind of paradox

49:34.120 --> 49:41.160
there of how much agency do you want to imbue in the agents. But the other paradox is the physical

49:41.160 --> 49:47.480
and social embeddedness. Because as you just said, cynically, we don't have as much agency as we

49:47.480 --> 49:52.760
think we do, because we're embedded in the dynamics around us. And being part of this

49:52.760 --> 49:59.960
overall system means that our agency is defined not just by our boundary, but it's by the history

49:59.960 --> 50:05.160
of the system. It's the history of us sharing information of all of the things around us.

50:05.160 --> 50:10.680
And all of these things inform what we do and what our preferences are. And then you say, well,

50:10.680 --> 50:15.960
we can just drop a brand new agent in the system. And it doesn't quite work because it's a fish out

50:16.040 --> 50:21.640
of water. It's not embedded in the ways that things that emerged in that system were in the

50:21.640 --> 50:29.080
first place. But this does get us onto this discussion of externalism. So part of the fiction

50:29.080 --> 50:37.960
of how we think about cognition is that we think of ourselves as islands that don't share information

50:37.960 --> 50:42.760
dynamically with the outside world. And of course, active inference is a way of bridging

50:42.760 --> 50:45.400
these two schools of thought. So can you kind of bring that in?

50:47.720 --> 50:52.040
I mean, I think you've already done it in a sense. I'm not sure what else there is for me to say on

50:52.040 --> 51:01.160
that. I'll try my best. So yes, I mean, active inference is about, well, it's about aboutness.

51:01.160 --> 51:08.360
It's the idea that our brains and our internal state evolves in such a way that reflects beliefs

51:08.360 --> 51:13.880
about what's outside. And I think that's one of the key things that you have to have for any sort

51:13.880 --> 51:19.320
of intelligent system. And that doesn't necessarily exist with other approaches that exist in

51:19.320 --> 51:25.960
neuroscience or artificial intelligence. It is that, and I'll just repeat that, it's very much

51:25.960 --> 51:33.320
being, the aboutness is the key thing that what's happening in my head is a reflection or is a

51:33.320 --> 51:40.280
description in some way is about what's happening outside my head. And maybe that's the link with

51:40.280 --> 51:45.080
this sort of externalism. But it's not just unidirectional either. It's the fact that

51:46.520 --> 51:50.120
I'm forming beliefs about what's happening in the outside world, but I'm also the one influencing

51:50.120 --> 51:55.160
the outside world to change it to fit with the beliefs I have about how it should be.

51:55.160 --> 52:07.640
Yes. Yes. So there's a kind of model. So we draw these boundaries. And we model the world around

52:07.640 --> 52:14.520
us. And we influence the world around us. And that's essentially what active inference is.

52:14.520 --> 52:21.480
I guess it might be useful just to sketch out the cognitive science idea of an activism or

52:21.480 --> 52:27.640
cybernetic. So there were folks who really railed against this idea of representationalism,

52:27.640 --> 52:34.520
which is this idea of model building in principle. And active inference is an integrated approach

52:34.520 --> 52:40.120
where we allow some model building, but we also think of the world itself as being its own best

52:40.120 --> 52:47.400
representations. How do we kind of bridge those two ideas? Yes. And I confess, I'm always lost in

52:47.400 --> 52:51.800
the distinction between the sort of inactivists, radical inactivists, the sort of different levels

52:51.800 --> 53:01.400
of stance you can take on this. And I think it comes down to that, that from an active inference

53:01.400 --> 53:07.240
perspective, both your representations, if that's the right word, the beliefs you have about the world,

53:07.240 --> 53:11.640
whether or not that meets the criteria for representation from an inactivist perspective

53:12.600 --> 53:19.080
is very important. But it is only important in terms of how you act. If your beliefs did not

53:19.080 --> 53:23.960
affect how you acted, clearly natural selection would not have selected you to form those beliefs.

53:23.960 --> 53:29.480
I think it's the simple way of putting it. So let's talk about some of the kind of

53:30.280 --> 53:36.680
the mathematical underpinnings here. So I think probably one of the main concepts we

53:36.680 --> 53:42.040
should start on is this idea of surprise. And maybe we can talk about it in general terms,

53:42.040 --> 53:48.680
and then we can move on to Bayesian surprise. So why is surprise so important in the free energy

53:48.680 --> 53:56.760
principle? Well, it's central to it. It is the key thing that matters. And we talk about the free

53:56.760 --> 54:03.880
energy principle. But in a sense, free energy is really there as a proxy for surprise. So yes,

54:03.880 --> 54:10.040
what do we mean by surprise? And it's another one of those things like the high road and

54:10.040 --> 54:13.560
the low road that you can approach from several different angles or several different lines of

54:13.560 --> 54:25.000
attack. If you were modeling something, if you were a Bayesian, so if you took a particular

54:25.000 --> 54:29.720
stance on probability theory and wanted to know, given my model, given my hypothesis,

54:29.800 --> 54:34.360
what's the evidence for it? What you would normally do is calculate something known as

54:34.360 --> 54:41.960
a marginal likelihood, which is just a measure of the fit between your model and the data that

54:41.960 --> 54:53.480
you have that you're trying to explain. That fit trades off various different things. So it can

54:53.480 --> 54:58.760
trade off how accurately your model is explaining the data against how far you've had to deviate

54:58.760 --> 55:03.880
from your prior beliefs or from your initial assumptions in order to arrive at that explanation.

55:06.040 --> 55:12.680
So that marginal likelihood, that evidence is effectively just the negative or the inverse

55:12.680 --> 55:19.480
of surprise. So that that's one perspective on it, the better the fit, the simpler and most

55:19.480 --> 55:25.400
accurate my explanation for something, the less surprised I will be by it. Another perspective

55:25.400 --> 55:31.560
on surprise is just this more colloquial sense. It's the idea that, given what I would predict,

55:31.560 --> 55:38.120
how far out of that prediction is it? One could take a more biological perspective on it and say,

55:38.120 --> 55:43.560
imagine we are, well, we are homeostatic systems that have some set points. We want to keep our

55:43.560 --> 55:47.160
temperature within a certain range, our blood pressure within a certain range, our heart rate

55:47.160 --> 55:52.920
within a certain range. If we find ourselves deviating from that, that is effectively a surprise

55:52.920 --> 55:57.640
because we're not where we expect to be. And so we enact various changes to bring

55:58.840 --> 56:04.840
those parameters back in range. So we might, if our blood pressure is too low, we might increase

56:04.840 --> 56:08.520
our heart rate to bring our blood pressure back up to the range we expect it to be in.

56:09.400 --> 56:16.200
And that is, in a sense, what active inference is all about. It's just this idea of keeping things

56:16.200 --> 56:22.040
within that minimally surprising range. But of course, once you put dynamics on it, once you

56:22.040 --> 56:27.880
start unfolding that in time, you end up having to not just deal with how surprising things are now,

56:27.880 --> 56:33.560
but you've got to try and anticipate surprise and behave in such a way that you allostatically

56:33.560 --> 56:39.960
control your sensory inputs, both your intraceptive inputs like heart rate and blood pressure,

56:39.960 --> 56:49.160
etc., but also your extraceptive sensations, your vision, your audition, and the like.

56:52.040 --> 56:56.120
And there's almost no end to the perspective you could take on surprise. Another perspective

56:56.120 --> 57:04.280
on it is that it's a reflective of, in a physical system, the improbability of being in a particular

57:04.280 --> 57:11.000
state. From a lot of physics perspectives, improbability is also associated with energy.

57:11.000 --> 57:18.520
It takes energy to bring things into less probable states. And without inputting energy into a system,

57:18.520 --> 57:23.640
it will generally end up in its most probable state in the absence of that.

57:25.320 --> 57:29.320
You think of things like Boltzmann's equation and the relationship there between energy and

57:29.320 --> 57:38.360
probability. And that also has a link then to the idea of either a Hamiltonian or indeed a

57:38.360 --> 57:43.400
steady state distribution, which is just what is the distribution things will end up in if left

57:43.400 --> 57:49.880
to their own devices for a certain amount of time until things have probabilistically converged.

57:49.880 --> 57:55.000
And that means that if I would construct a probability distribution over where things

57:55.000 --> 57:59.160
will be at a long point of time in the future, there will come a point at which that probability

57:59.160 --> 58:05.880
won't change any further. And the tendency of physical systems to go to those more probable

58:05.880 --> 58:14.440
states is exactly the same as the tendency to avoid surprising states. And again, we could

58:14.440 --> 58:18.920
sort of go on for a while, but I won't on sort of other ways of conceptualizing it. But hopefully

58:18.920 --> 58:24.440
that sort of explains why it's such an important thing that underpins so much of what we do.

58:25.240 --> 58:29.640
We're either trying to sort of evolve as a physical system towards more probable states.

58:31.000 --> 58:36.600
Or we are homeostatic or allostatic organisms trying to maintain our internal parameters within

58:36.600 --> 58:43.640
the right set points. Or we are more colloquially just trying to avoid things that are different to

58:43.640 --> 58:50.520
what we predict. Or we are statisticians trying to fit our model to the world as best we can.

58:50.520 --> 58:53.160
And all of those things come under the same umbrella of surprise.

58:54.920 --> 58:59.160
Free energy comes in because surprise is not a trivial thing to compute.

59:01.640 --> 59:07.080
Mathematically, it's often either intractable mathematically or computationally. And so it's

59:07.080 --> 59:12.120
just not efficient to be able to calculate. But free energy is a way of then approximating

59:12.120 --> 59:17.240
that surprise. It's a way of coming up with something that is close enough to it. Or

59:18.040 --> 59:22.840
even more precisely as an upper bound on surprise. So if you're at the lowest point of your free energy,

59:24.040 --> 59:33.160
then that limits how high your surprise can be. The key additional thing in free energy is that

59:33.160 --> 59:38.760
the distance between that bound, your free energy and your surprise depends on how good your beliefs

59:38.760 --> 59:44.120
about the world are. And that's where perception comes in. That by getting the best beliefs you

59:44.120 --> 59:50.040
possibly can, you minimize the distance between your free energy and which is up a bounding of

59:50.040 --> 59:54.920
surprise and the surprise itself. So then any further reduction in free energy, you would expect

59:54.920 --> 59:59.720
to also result in a decrease. Sorry, any further decrease in free energy would also result in a

59:59.720 --> 01:00:04.680
further decrease in surprise. I mean, there's a few things that struck me. I mean, first of all,

01:00:04.680 --> 01:00:11.880
what struck me is that we're using the language of things like statistical mechanics and Bayesian

01:00:12.280 --> 01:00:19.000
statistics and information theory, things like entropy and so on. And we're interchangeably

01:00:20.120 --> 01:00:24.040
kind of speaking about the same thing from the perspective of different disciplines,

01:00:24.040 --> 01:00:31.960
which I find very, very interesting. And on the surprise thing, even though in this formalism,

01:00:31.960 --> 01:00:38.040
we are minimizing surprise, I think there's an interesting perspective that sometimes surprise

01:00:38.040 --> 01:00:44.920
is what we want. So for example, the chess algorithm, the ELO algorithm, it's only when

01:00:44.920 --> 01:00:51.000
something surprising happens that the weights get updated because it's information. Or people on

01:00:51.000 --> 01:00:57.640
YouTube, my videos are that they get more views when they have a cash value, which means they

01:00:57.640 --> 01:01:01.960
have information content, which means that, you know, they're actually surprising your predictive

01:01:01.960 --> 01:01:06.360
model. Even Arnold Schwarzenegger used to joke about it, he said, you have to shock the muscles.

01:01:06.360 --> 01:01:10.520
You know, you have to do what the muscles don't expect. Otherwise, there's not an adaptation. So

01:01:10.520 --> 01:01:15.160
there's this interesting juxtaposition between actually seeking out surprise, even though you

01:01:15.160 --> 01:01:20.200
can think of our brains overall as minimizing surprise. And what was the other thing I was

01:01:20.200 --> 01:01:24.120
going to say? Yeah, you were just getting onto variational inference, which is really interesting.

01:01:24.120 --> 01:01:30.040
So there's a couple of intractable statistical quantities in this mixture that we're talking

01:01:30.040 --> 01:01:36.520
about. I think it's the log model evidence and the Bayesian posterior. And we can't represent

01:01:36.520 --> 01:01:41.480
those things directly. So we have to put a proxy in there, which kind of captures most of the

01:01:41.480 --> 01:01:46.360
information, but it's still possible to deal with it. So how does this variational inference work?

01:01:47.320 --> 01:01:52.440
Yeah. So I suppose maybe the first thing to think about, though, is just to recap what Bayesian

01:01:52.440 --> 01:01:56.920
inference is. I suppose we've been talking about it quite a lot without necessarily defining it.

01:01:56.920 --> 01:02:04.280
And many of you listeners, I'm sure, will know already. But the idea is actually relatively

01:02:04.280 --> 01:02:08.520
straightforward and well-established and quite widely used. And it's the idea that if I have

01:02:08.520 --> 01:02:15.640
some beliefs about things that are in my world that I can't directly observe, I may have a sense

01:02:15.640 --> 01:02:21.080
of what's plausible to begin with. And that's what we refer to as a prior probability. I then also

01:02:21.080 --> 01:02:28.600
need to have a model that says, given the world is this way, what would I expect to actually observe?

01:02:29.160 --> 01:02:36.920
So for instance, given where you are relative to me, I can predict a certain pattern on my retina.

01:02:37.560 --> 01:02:41.560
And if you were somewhere else, I would expect a different pattern on my retina. So I might have

01:02:41.560 --> 01:02:45.720
a prior range of plausibilities as to where you are relative to me. And then I have a model that

01:02:45.720 --> 01:02:50.840
explains how I'm going to generate some data based upon that. And Bayesian inference basically

01:02:50.840 --> 01:02:57.720
takes those two things and inverts them using Bayes' theorem and effectively just flips both of them

01:02:57.720 --> 01:03:03.880
round. So you now say instead of a distribution of where you are relative to me, I'm now talking

01:03:03.880 --> 01:03:09.640
about a distribution of all the possible things that I could see on my retina. And instead of

01:03:09.640 --> 01:03:17.000
predicting the distribution on the retina given where you are, I now want to know the distribution

01:03:17.000 --> 01:03:23.400
of where you are given what's on my retina. And Bayesian inference, much like active inference,

01:03:23.400 --> 01:03:27.640
is full of all these interesting inversions where you sort of flip things round from how

01:03:27.640 --> 01:03:34.280
they initially appeared. But the problem is calculating those two things, calculating the

01:03:34.280 --> 01:03:39.640
flipped model. So the distribution of all the things on my retina here would now be my model

01:03:39.640 --> 01:03:48.360
evidence, my inverse surprise. And the distribution of where you are relative to what's on my retina

01:03:49.240 --> 01:03:53.800
is my posterior distribution. But those things are not always straightforward to calculate.

01:03:54.360 --> 01:04:00.200
And so variational inference takes that problem and makes it into an optimization problem. It

01:04:00.200 --> 01:04:08.120
writes down a function that quantifies how far am I away from my, or what would be the true posterior

01:04:08.120 --> 01:04:15.480
if I'd used exact Bayes. And then it says, well, let's parameterize some approximate posterior

01:04:15.480 --> 01:04:20.120
probability. So come up with a function that represents a probability distribution that's

01:04:20.120 --> 01:04:24.520
easy to characterize, something like a Gaussian distribution where I know I just need my mean

01:04:24.520 --> 01:04:30.920
and my variance. And then just changes that mean and variance until you minimize this function

01:04:30.920 --> 01:04:36.360
that represents that discrepancy, minimize this free energy, also sometimes known as an evidence

01:04:36.360 --> 01:04:42.120
lower bound, in which case you maximize it. And interestingly, once you've maximized your

01:04:42.120 --> 01:04:46.440
evidence lower bound or minimized your free energy, you end up with a situation where

01:04:47.720 --> 01:04:54.200
the free energy starts to approximate your log model evidence or your negative log surprise.

01:04:55.480 --> 01:05:01.240
And your approximate posterior distribution, your variational distribution starts to look

01:05:01.240 --> 01:05:08.760
much more like your exact posterior probability distribution. So it's another one of those

01:05:08.760 --> 01:05:13.800
interesting scenarios where doing one thing optimizing one quantity ends up having a dual

01:05:13.800 --> 01:05:18.040
purpose. And in active inference, the only additional thing you throw into that is that you

01:05:18.040 --> 01:05:25.400
want to then also change your data itself. So you do the third thing you act on the world

01:05:25.400 --> 01:05:30.360
to then optimize exactly the same objective. The interesting thing, I guess, is just contrasting

01:05:30.360 --> 01:05:35.800
to machine learning again. So in machine learning, we also have these big parameterized models and we

01:05:35.800 --> 01:05:41.160
do stochastic gradient descent. And some might think of deep learning, because obviously you

01:05:41.160 --> 01:05:45.240
can think of everything as a Bayesian. So you can think of machine learning as being maximum

01:05:45.240 --> 01:05:52.920
likelihood estimation. Why is it that we go full Bayesian when we do active inference? Why not

01:05:52.920 --> 01:05:58.680
something like maximum likelihood estimation? It's an interesting question. And there are a couple

01:05:58.680 --> 01:06:04.120
of answers you could give again, some of which are more technical, but some of which are

01:06:07.240 --> 01:06:12.440
some of which are slightly more intuitive. And I think one of the more intuitive answers is that

01:06:12.440 --> 01:06:18.360
by having an expression of plausibility of things in advance, you just maintain things

01:06:18.360 --> 01:06:24.120
within a plausible region. So maximum likelihood for those who are unaware is where you essentially

01:06:24.200 --> 01:06:29.560
throw away that prior probability, where you throw away any prior plausibility as to as to

01:06:29.560 --> 01:06:36.440
what the state of the world might be. And you just try and find the value that would maximize

01:06:36.440 --> 01:06:41.480
your likelihood, which is your prediction of how things would be under some hypothesis or under

01:06:41.480 --> 01:06:53.080
some parameter setting. And I think the first thing to say is if you throw away that prior

01:06:53.080 --> 01:06:57.960
information, then you end up potentially coming up with quite implausible solutions.

01:06:59.000 --> 01:07:03.320
That's particularly relevant if you're dealing with what's known as an inverse problem. So where

01:07:03.320 --> 01:07:08.840
there are multiple different things that could have caused the same outcome. An example that's

01:07:08.840 --> 01:07:13.720
often given is that for any given shadow, there's almost an infinite number of things, configurations

01:07:13.720 --> 01:07:18.040
of the sun and the shape of the thing that's casting the shadow that could lead to exactly the

01:07:18.040 --> 01:07:22.520
same shadow. And so maximum likelihood approach just won't be able to tell the difference between

01:07:22.520 --> 01:07:27.880
all of those things. However, if you have some prior on top of that, if you have some statement

01:07:27.880 --> 01:07:32.760
of the plausible things that might cause it, you can come up with a much better estimate of those

01:07:32.760 --> 01:07:43.800
sorts of things. Another way of looking at it is that when you're dealing with a maximum likelihood

01:07:43.800 --> 01:07:49.160
estimate, you're throwing away all uncertainty about the solution. So you're coming up with a

01:07:49.160 --> 01:07:53.320
point estimate and you're saying this is the most likely thing, but you're ignoring all of your

01:07:53.320 --> 01:07:59.080
uncertainty about it. And I think that is in itself a relatively dangerous thing to do and can lead to

01:07:59.080 --> 01:08:03.880
the problem of overfitting, where you start to become very confident about what you can see from

01:08:03.880 --> 01:08:14.920
a relatively small sample of things and you can end up with all of these well-described in the media

01:08:14.920 --> 01:08:20.520
scenarios of complete misclassifications based upon that sort of overconfidence just because

01:08:20.520 --> 01:08:27.960
all the uncertainty is gone. A more technical way of looking at it, I think, is if you think about

01:08:28.680 --> 01:08:36.120
what a free energy is. So free energy is our measure of our marginal likelihood that we're

01:08:36.120 --> 01:08:42.760
using when we're doing Bayesian inference. And one way of separating out what a free energy

01:08:42.760 --> 01:08:49.720
looks like is to have our complexity, which is effectively how far we needed to deviate from

01:08:49.720 --> 01:08:55.000
our prior assumptions to come up with an explanation, and our accuracy, which is how well we can fit

01:08:55.000 --> 01:09:02.520
our model. Accuracy is common to both maximum likelihood type approaches because we're trying

01:09:02.520 --> 01:09:10.120
to find the value that most accurately predicts our data and also to Bayesian approaches.

01:09:10.120 --> 01:09:16.200
Both want to do that. But what's thrown away in the maximum likelihood type approach is the

01:09:16.200 --> 01:09:23.800
complexity bit, the how far do you deviate from your priors. So there's an inbuilt Occam's razor,

01:09:23.800 --> 01:09:30.200
the idea that the simplest explanation is a priori more likely that you get from a Bayesian

01:09:30.200 --> 01:09:34.200
approach that you throw away when you're dealing with maximum likelihood estimation.

01:09:35.160 --> 01:09:41.480
I wondered to what extent does the active part play a role here. So even in machine learning,

01:09:43.000 --> 01:09:47.800
there's something called active learning, where you dynamically retrain the model,

01:09:47.800 --> 01:09:52.280
or there's something called machine teaching, where you dynamically select more salient data

01:09:52.280 --> 01:09:57.880
to train the model, and the model gets much better. And in things like Bayesian optimization,

01:09:57.880 --> 01:10:03.800
for example, by maintaining this distribution of all of your uncertainty in a principled way,

01:10:03.880 --> 01:10:09.720
you can go and seek and find more information to kind of improve your knowledge on subsequent steps.

01:10:09.720 --> 01:10:15.240
So I guess it's sort of bringing in this idea of it's not just what happens now,

01:10:15.240 --> 01:10:21.000
it's about how can I improve my knowledge of the world over several steps.

01:10:21.720 --> 01:10:25.640
Yes, and that reminds me about the point you were making earlier, that sometimes we actually do

01:10:25.640 --> 01:10:33.640
things to surprise ourselves, which seems very counter-intuitive in the context of the idea that

01:10:33.640 --> 01:10:41.480
we're trying to minimize surprises as our sole objective in life. And sometimes people talk about

01:10:41.480 --> 01:10:46.040
this in terms of a dark room problem, the idea that actually if all you want to do is minimize

01:10:46.040 --> 01:10:50.040
your surprise, you just go into a room, turn off the lights and stay there because you're not going

01:10:50.040 --> 01:11:00.120
to experience anything that's going to surprise you. I mean, the answer to this problem is that

01:11:00.120 --> 01:11:08.600
actually, as organisms, as creatures, we don't expect to be purely in a dark room. And the

01:11:08.600 --> 01:11:15.320
sort of organism that would be is, again, probably not a very interesting one. And that what we predict,

01:11:15.320 --> 01:11:21.080
what we'd be surprised by might be permanently staying in a dark room. But it goes even further

01:11:21.080 --> 01:11:27.160
than that. And if you say, actually, I'm minimizing my surprise over time, I want to be in a predictable

01:11:27.160 --> 01:11:31.800
world where I know what's going to happen next. The best way of doing that is to actually gather

01:11:31.800 --> 01:11:36.040
as much information as you can about the world around you. So the first thing you do really is

01:11:36.040 --> 01:11:39.640
you turn on the light and see what the room looks like, because that might then predict all the sorts

01:11:39.640 --> 01:11:43.720
of things that could fall on you in that room and could potentially cause surprise. And by knowing

01:11:43.720 --> 01:11:49.640
about it, you mitigate the surprise that you might get in the future. And as you say, you can only

01:11:49.640 --> 01:11:54.120
really do that if you know what you're certain about. And so if you take a maximum likelihood

01:11:54.200 --> 01:11:58.520
approach, if you work based on point estimates and you have no measure of your uncertainty,

01:11:59.320 --> 01:12:03.560
then there's no way you can possibly know what you're uncertain about to be able to resolve

01:12:03.560 --> 01:12:10.600
that uncertainty. So this brings me on to causality. We know that predictive systems,

01:12:10.600 --> 01:12:16.120
which are aware of causal relationships, work better. But if we just bring it back to physics

01:12:16.120 --> 01:12:21.080
first, I mean, to you, what do you think causality is?

01:12:22.200 --> 01:12:27.160
It is a tricky issue as to what causality is. And I think whether it exists or not is really a

01:12:27.160 --> 01:12:34.840
matter of how you define it, isn't it? And some would define that purely in terms of conditional

01:12:34.840 --> 01:12:41.320
dependencies, that the behavior of one thing is conditionally dependent upon something else,

01:12:41.400 --> 01:12:46.040
and therefore you could say that the one thing causes the other. But as we know from Bayes'

01:12:46.040 --> 01:12:50.600
theorem, that's not quite good enough, because you can swap any conditional relationship around

01:12:52.440 --> 01:12:59.000
through that process of inverting your model. Sometimes that causality is written into the

01:12:59.000 --> 01:13:05.480
dynamics of a model. So this would be the approach used in things like dynamic causal

01:13:05.480 --> 01:13:10.520
modeling of brain data, where you might say that the current neural activity in one area of the

01:13:10.520 --> 01:13:15.160
brain affects maybe the rate of change of neural activity in another part of the brain.

01:13:15.800 --> 01:13:19.480
And it's the way in which those dynamics are written in, the fact that it's one affects the

01:13:19.480 --> 01:13:27.400
rate of change of the other, that gives it that causal flavor and a very directed perspective on

01:13:27.400 --> 01:13:35.480
it. Probably the work that is most comprehensive on this is looking at people like Judea Pearl and

01:13:35.560 --> 01:13:41.800
a lot of his work on causality. There's a lot of detail about the notion of an intervention.

01:13:42.360 --> 01:13:47.720
And I suppose you can think of this in terms of how you might establish causation in a clinical

01:13:47.720 --> 01:13:53.000
context. If you were to run a trial to try and establish whether one thing's caused another,

01:13:53.000 --> 01:13:57.400
you need to make sure you're not inadvertently capturing a correlation or a conditional dependence

01:13:57.400 --> 01:14:03.000
that could go either way, or a common cause of both things that depends upon something else.

01:14:03.880 --> 01:14:09.800
And typically the way you do that is you intervene on the system. You randomize at the beginning

01:14:09.800 --> 01:14:14.040
to make sure that people are assigned to different treatment groups at random,

01:14:14.040 --> 01:14:20.200
so that you break that dependency upon something prior to it. And then anything that happens going

01:14:20.200 --> 01:14:27.880
forward is going to depend on the intervention that you're doing. So I think that's probably the

01:14:27.960 --> 01:14:33.720
key thing that gives you causality or perhaps defines causality. It's the idea that an intervention

01:14:33.720 --> 01:14:38.040
is what will change it. If you intervene in one thing, that should then in a way that doesn't

01:14:38.040 --> 01:14:43.240
necessarily match its natural distribution if you hadn't intervened at all, and then see what the

01:14:43.240 --> 01:14:50.520
effect is. Yes, yes. I mean, and by the way, Judea Pearl is really interesting. I want to study

01:14:50.520 --> 01:14:56.520
his book, The Book of Why. It's one thing that we've really dropped the ball on, actually.

01:14:56.520 --> 01:15:03.160
But I suppose one way to think about it is if you go back to the core physical, in physics,

01:15:03.160 --> 01:15:07.240
there's a whole bunch of equations to describe the world we live in. And those equations don't

01:15:07.240 --> 01:15:12.760
have, they don't say anything about causality, and they're even reversible. And then you can think,

01:15:12.760 --> 01:15:17.000
okay, well, maybe it's a little bit like the free energy principle. It's a lens,

01:15:17.000 --> 01:15:21.560
like really, there's only dynamics. But when you look at these dynamical systems,

01:15:21.560 --> 01:15:28.600
then behaviors emerge, and somewhere up that chain, you can say, okay, now we've got causality,

01:15:28.600 --> 01:15:34.360
and it's something which is statistically efficacious to build it into our models. But

01:15:36.440 --> 01:15:42.440
where does it come from? Well, it comes from us, doesn't it? It's a hypothesis to explain a particular

01:15:42.440 --> 01:15:51.080
pattern of dynamic. Yes. And we might infer causation based upon, again, a particular pattern

01:15:51.080 --> 01:15:55.880
of how one thing reacts to another. So if you imagine you've got the classic physics example,

01:15:55.880 --> 01:16:03.160
billiard balls bouncing into one another, how do you know that the collision of one ball with

01:16:03.160 --> 01:16:10.280
another is causative of the subsequent motion of the second ball? And you could argue that that's

01:16:10.280 --> 01:16:15.160
due to a particular pattern of which variables affect which other variables and the particular

01:16:15.160 --> 01:16:20.280
exchange between them. And this comes back quite nicely to things like the physics perspective

01:16:20.680 --> 01:16:26.440
on the free energy principle, the idea that actually one could see the location of a particular

01:16:26.440 --> 01:16:34.520
ball as being, you know, maybe it's internal state, and then the action that that then causes

01:16:34.520 --> 01:16:41.000
is perhaps the, or in fact, you could say that the action is the position of the ball, the force

01:16:41.000 --> 01:16:46.840
that results from that action is the sensory state of the next ball, which then changes its

01:16:46.840 --> 01:16:53.960
velocity to then change its action relative to something else. You can sort of rearrange those

01:16:53.960 --> 01:16:59.800
labels slightly, but there is a directional element to it. And in that sort of pattern of

01:16:59.800 --> 01:17:05.240
causation, you really do expect the position of one ball to have an effect on the rate of change,

01:17:05.240 --> 01:17:09.720
or in fact, even the rate of rate of change of the second ball, which again, I think brings us

01:17:09.720 --> 01:17:15.480
back to those kinds of dynamical descriptions of causality where one thing might affect how

01:17:15.480 --> 01:17:21.160
another thing changes. So you almost get it from the dynamics itself. But again, to some extent,

01:17:21.160 --> 01:17:25.800
it comes back to semantics, doesn't it? It comes back to what do we mean by cause? Well, I suppose

01:17:25.800 --> 01:17:29.960
cause is a hypothesis as to a particular configuration of things. But then you've got to

01:17:29.960 --> 01:17:34.840
write down what does that hypothesis mean? What's my model of what a causation involves?

01:17:35.560 --> 01:17:41.720
Yes, yes. I mean, we were just talking about, you know, build building these models. And one of

01:17:41.720 --> 01:17:48.520
the bright differences from machine learning is that we need to build a generative model by hand.

01:17:49.560 --> 01:17:54.520
So we have to define these these variables, and some of them are presumably observed, and some of

01:17:54.520 --> 01:18:03.320
them are not observed. They're inferred. And that process seems like you would need to have a lot

01:18:03.320 --> 01:18:10.440
of domain expertise. And it seems like something which is at least has a degree of subjectivity.

01:18:10.440 --> 01:18:14.760
I mean, we were just talking about causality, for example, there are many ways you could model

01:18:15.320 --> 01:18:21.160
the risk of cancer from smoking. It seems like there are many, many different ways of building

01:18:21.160 --> 01:18:27.960
those models. So that subjectivity is interesting. I mean, are there principled ways of building

01:18:27.960 --> 01:18:33.800
these models? Yes. And in a sense, it all comes back to the same thing again, it comes back to

01:18:33.800 --> 01:18:42.280
which model minimizes the surprise the best. And but there are interesting questions amongst that.

01:18:42.280 --> 01:18:48.840
So how do you actually choose the space of models that you want to compare? So you're right to say

01:18:48.840 --> 01:18:54.520
that that that often there is some specific prior information that's put into models and active

01:18:54.520 --> 01:18:59.000
inference. And very often we do end up sort of building models by hand to demonstrate a specific

01:18:59.720 --> 01:19:04.760
outcome or a specific cognitive function. But there's no reason why it has to be that way.

01:19:04.760 --> 01:19:14.040
You can build models through exposure to data, where where the models are selecting the data to

01:19:14.040 --> 01:19:19.240
best build themselves. But the question is how you do that, how you start to add on additional

01:19:19.240 --> 01:19:24.200
things, how you start to change the structure of your model. But there's a lot of ongoing research

01:19:24.200 --> 01:19:29.000
into that. And I think there are now methods that are coming out that will allow you to allow an

01:19:29.000 --> 01:19:34.520
active inference model to build itself. And the way it will do that will be sort of adding on

01:19:34.520 --> 01:19:41.240
additional states and potential causes, adjusting beliefs about the mappings and the distributions

01:19:41.240 --> 01:19:49.400
and the parameters of given this than that, adding an additional paths that different

01:19:50.120 --> 01:19:58.040
or different transitions that systems will pursue. So it's a fascinating area. I think

01:19:58.040 --> 01:20:03.880
it's one that's still a growing area. But it's this idea of structure learning of comparing

01:20:03.880 --> 01:20:08.840
each alternative model based upon its free energy or model evidence or surprise as a way of

01:20:10.520 --> 01:20:13.080
minimizing that by being able to better predict things.

01:20:13.960 --> 01:20:22.120
Yeah, I mean, that's something that we humans, we seem to do really well. So we can, first of all,

01:20:22.120 --> 01:20:30.760
via abduction, we can select relevant models to explain behavior, you know, what we observe.

01:20:30.760 --> 01:20:36.120
But we also have the ability to create models. In fact, I think of intelligence as the ability

01:20:36.120 --> 01:20:42.760
to create models. So we experience something. And I now construct a model to explain this

01:20:42.760 --> 01:20:51.240
and similar experiences in experience space. But in a machine, it's really difficult. So in

01:20:51.240 --> 01:20:56.280
machine learning, there's this bias variance trade off. So we deliberately reduce the size

01:20:56.280 --> 01:21:02.200
of the approximation space to make it computationally tractable. And when we're talking about

01:21:02.200 --> 01:21:07.720
building these models, just from observational data, it feels like there's an exponential

01:21:07.720 --> 01:21:12.440
blow up of possible models. So I can imagine there might be a whole bunch of heuristics around

01:21:12.440 --> 01:21:17.560
library learning or having modules. So these modules have worked well over there. So we'll

01:21:17.560 --> 01:21:22.520
try composing together known modules rather than starting from scratch every single time. I mean,

01:21:22.520 --> 01:21:26.680
what kind of work is being done there? I mean, I think I think you're right about, you know,

01:21:26.680 --> 01:21:33.720
it's not going to be worth starting from scratch every time. You can sort of build models by saying,

01:21:33.720 --> 01:21:38.840
okay, let's start with something very simple with a sort of known structure. And I think it's

01:21:38.840 --> 01:21:44.280
sensible to use some priors in that rather than starting from complete, completely nothing,

01:21:44.280 --> 01:21:47.720
because there are some things that we know about in the world. And there's no point hiding that

01:21:47.720 --> 01:21:53.640
from the models we're trying to build. And that might be a simple structural thing like things

01:21:53.640 --> 01:21:57.480
evolve in time. So one thing is conditioned upon the next is conditioned upon the next.

01:21:57.480 --> 01:22:02.120
And things now will influence the data I observe things well in the past might not anymore.

01:22:05.720 --> 01:22:10.520
But then then there's the question of, well, how can a model then grow? What are the things that

01:22:10.520 --> 01:22:15.800
you can add to it or subtract from it? And subtraction is another key element. Because you

01:22:15.800 --> 01:22:19.320
could take this whole problem from the other direction, and you could say, well, let's start

01:22:19.320 --> 01:22:23.080
with a model that just has everything in it and take away bits until we've got the model that's

01:22:23.080 --> 01:22:26.920
relevant to where we are at the moment. And we know that during development, there's a lot of

01:22:26.920 --> 01:22:32.200
synaptic pruning that goes on and removal of synapses that we have when we're much younger

01:22:33.080 --> 01:22:39.720
compared to compared to as you get older. So what can you add on? Well, it depends what your model

01:22:39.720 --> 01:22:43.400
looks like. So if your model says there's a set of states that can evolve over time, there are

01:22:43.400 --> 01:22:46.920
a set of outcomes that are generated, well, we know what the outcomes are, we know what the

01:22:46.920 --> 01:22:52.680
data are, because we know what our sensory organs are. So it's the states that are going to change

01:22:53.160 --> 01:23:00.280
so do we add in more states? Do we allow them to take more alternative values? Do we allow

01:23:00.280 --> 01:23:05.960
their transitions to change in more than one different way? Which ones can I change? Which

01:23:05.960 --> 01:23:11.800
ones can I not change? And it's really just asking these questions that helps you to grow your model.

01:23:11.800 --> 01:23:16.440
So you say, well, let's try it. If I allow this state to take additional values, if it's not

01:23:16.440 --> 01:23:22.520
providing a sufficiently good explanation for how things are at the moment. And if that improves

01:23:22.520 --> 01:23:28.040
your prediction, that's good and you keep it and if it doesn't, then you get rid of it. Do I now need

01:23:28.040 --> 01:23:35.800
to include additional state factors? So you could either say there is one sort of state of the world

01:23:35.800 --> 01:23:39.960
that can take multiple different values, or you could actually this is contextualized by something

01:23:39.960 --> 01:23:44.440
completely separate. So where am I along an x coordinate? You also need to know where you are

01:23:44.440 --> 01:23:51.320
along y coordinate to be able to contextualize what you're predicting. So it's just asking what is in

01:23:51.320 --> 01:23:55.560
a model? How do you build a model almost gives you the answers to the ways or the directions in

01:23:55.560 --> 01:24:01.400
which you can grow it. The other thing you can then do when you're trying to work out how to grow it

01:24:01.400 --> 01:24:06.120
is to say, well, let's treat this as the same sort of problem as exploring my world,

01:24:06.120 --> 01:24:10.040
selecting actions that will then give me more information about the world. You could say,

01:24:10.040 --> 01:24:14.360
well, actually, now let's treat my exploration of model space as being a similar process of

01:24:14.360 --> 01:24:23.320
exploration. Which of these possible adjustments to my model might lead to a less ambiguous mapping

01:24:23.320 --> 01:24:28.360
between what I'm predicting or what's in my world and what I'm currently predicting?

01:24:28.920 --> 01:24:35.240
Yes, it rather brings me back to our comments about the space or the manifold that the models

01:24:35.240 --> 01:24:39.960
sit on, whether they would have a kind of contiguity or whether they would have a gradient.

01:24:40.360 --> 01:24:45.480
I guess I'm imagining a kind of topological space that the models would sit on. I don't know whether

01:24:45.480 --> 01:24:50.600
it's worth bringing in. Obviously, you're a neuroscientist and the way brains work,

01:24:51.400 --> 01:24:58.600
we must do this. Of course, there's this idea of nativism. Some psychologists think that we have

01:24:58.600 --> 01:25:03.320
these models built in from birth and then the other school of thought is that we're just a

01:25:03.320 --> 01:25:09.720
complete blank slate. If you read Jeff Hawkins, he talks about the neocortex as this magical

01:25:09.720 --> 01:25:16.280
thing that just builds models on the fly. But perhaps one difference at least between brains

01:25:16.280 --> 01:25:24.120
and machines is the multi-modality, which is to say we have so many different senses that

01:25:25.160 --> 01:25:35.080
creates a gradient or that makes it tractable. Because when a model from a particular sensation

01:25:35.080 --> 01:25:39.000
and starts predicting well, we can rapidly optimise and go in the right direction.

01:25:39.080 --> 01:25:43.560
Because the problem seems to be that there are so many directions where we can go in,

01:25:45.160 --> 01:25:51.000
doing some kind of monotonic gradient optimisation will often lead us into the wrong part of the

01:25:51.000 --> 01:25:58.920
search space, so we've wasted our time. Yeah, I think that's a really good point,

01:25:58.920 --> 01:26:07.160
absolutely. As soon as you know how one thing works or how vision works, I suppose vision

01:26:07.160 --> 01:26:13.400
and proprioception is a good example, isn't it? If I recognise where my hand is and I can

01:26:13.400 --> 01:26:17.880
make a good estimate of that visually, then that helps me tune my joint position sense as to where

01:26:17.880 --> 01:26:24.840
my arm might be. And it's always fascinating to see situations where that breaks down, so there

01:26:24.840 --> 01:26:29.880
are a number of conditions where if you lose your joint position sense, you're perfectly okay holding

01:26:29.880 --> 01:26:34.040
your arm out like that until you close your eyes, at which point you start getting all these interesting

01:26:34.040 --> 01:26:39.880
twitches and changes. So yes, the multimodality I think probably is a really key thing that really

01:26:39.880 --> 01:26:44.840
does help constrain the other senses because you're just getting more information about each thing.

01:26:45.800 --> 01:26:50.440
Maybe we should just talk about chapter 10 in general, because that was kind of like the

01:26:50.440 --> 01:26:56.280
homecoming chapter, if you like sort of bringing together some of the ideas. So can you sketch

01:26:56.280 --> 01:27:03.240
that out for me? Yeah, so I think towards the end of the book, the idea was to try and bring together

01:27:03.240 --> 01:27:07.320
a lot of the themes that had been discussed earlier on, but to also make the point that,

01:27:10.600 --> 01:27:15.880
well, I'll come back to one of the things you said earlier was about how it seems we're talking about

01:27:15.880 --> 01:27:21.400
lots of different things from different perspectives, but actually they're really the same thing.

01:27:21.400 --> 01:27:29.320
So we talked about how surprise is also a measure of steady state of energies of various sorts of

01:27:30.280 --> 01:27:36.120
of statistics and model comparison of homeostatic set points, you know, that all of these things

01:27:36.120 --> 01:27:44.600
can be seen through the same lens. But again, taking one of those inversions, you can invert

01:27:44.600 --> 01:27:47.960
that lens and say, well, actually, you can start from the same thing and now project back into

01:27:47.960 --> 01:27:55.480
all of these different fields. And I think that's a useful thing to do because I think it helps foster

01:27:55.560 --> 01:28:00.600
multidisciplinary work, helps to engage people from different fields and areas,

01:28:01.560 --> 01:28:08.280
and helps us know what's happening elsewhere so that you're not just duplicating everything that

01:28:08.280 --> 01:28:13.640
people have already done. So I think it's really important to have those connections to different

01:28:13.640 --> 01:28:19.800
areas. And the chapter 10 from the book was an aim to try and connect to those different areas,

01:28:19.800 --> 01:28:23.240
whether it be to things you've spoken about, like cybernetics and inactivism,

01:28:23.320 --> 01:28:27.800
and just to try and understand the relationship between each of them.

01:28:27.800 --> 01:28:32.520
Well, I mean, quite a lot of people use this as a model of, you know, just things like

01:28:32.520 --> 01:28:38.440
sentience and consciousness in general. And I often speak about the strange bedfellows of

01:28:38.440 --> 01:28:43.480
the free energy principle. So, you know, there are, you know, autopoietic and activists and

01:28:43.480 --> 01:28:47.640
phenomenologists and, you know, people talking about sentience and consciousness, you know,

01:28:47.640 --> 01:28:53.480
obviously you're a clinician, you know, you're working in a hospital. So it's just this

01:28:53.480 --> 01:28:57.960
incredible conflation of different people together, and they all bring their own lexicon with them.

01:28:57.960 --> 01:29:02.360
But maybe we should just get on to this kind of sentience and consciousness thing, because that

01:29:02.360 --> 01:29:08.840
seems quite mysterious. We almost come back to one of the themes we've spoken about a few times,

01:29:08.840 --> 01:29:16.120
which is that the specific words we use for things in the effect that different people,

01:29:16.120 --> 01:29:19.720
that has on different people. So some people, I think, would probably get very angry with the idea

01:29:19.720 --> 01:29:29.640
of using sentience to describe some of the sort of simulations and models that we would develop.

01:29:31.400 --> 01:29:36.120
But that comes down to what you mean by sentience. And I think one of the key things for sentience

01:29:36.120 --> 01:29:42.360
is the aboutness we were talking about before. The idea that our brains or any sentient system

01:29:42.360 --> 01:29:48.520
really is trying to try not to anthropomorphise too much, but it's almost impossible to do in

01:29:48.520 --> 01:29:57.080
this setting, isn't it? Not trying to, but that the dynamics of some system internally to the system

01:29:57.080 --> 01:30:01.800
are reflective of what's going on external to it, and that you can now start to see those dynamics

01:30:01.800 --> 01:30:07.000
as being optimization of beliefs. And those beliefs are about what's happening in the outside world

01:30:07.080 --> 01:30:12.680
and about how I'm affecting the outside world. And I think that probably gets to the root of

01:30:12.680 --> 01:30:18.600
at least a definition of sentience and one that I'd be happy with, which is just the

01:30:20.040 --> 01:30:26.120
dynamics of beliefs about what's external to us and how we want to change it.

01:30:27.800 --> 01:30:31.880
And there are very few things other than that sort of inferential formalism that give you that.

01:30:32.840 --> 01:30:38.200
Yes, I mean, in a way, one thing I like about it is, I mean, we are talking as physicists,

01:30:38.200 --> 01:30:47.640
so we are materialists. It's very no-nonsense. It's quite reductive as well, because there are

01:30:47.640 --> 01:30:53.560
those who believe that these kind of qualities that we're speaking about, certainly with

01:30:53.560 --> 01:30:59.800
conscious experience, for example, that it's not reducible to these kind of simple explanations

01:30:59.880 --> 01:31:06.120
that we're talking about, that it has a different character. David Chalmers talks about a philosophical

01:31:06.120 --> 01:31:14.360
zombie. So for example, you might behave just like a real human being, but you could be divorced of

01:31:14.360 --> 01:31:22.120
conscious experience. So he says that you can think of behavior, dynamics, and function,

01:31:22.120 --> 01:31:27.320
and conscious experience as something entirely different. But as an observer, you would never

01:31:27.320 --> 01:31:34.440
know. So yeah, it feels very no-nonsense, doesn't it? But that wouldn't be satisfying to a lot of

01:31:34.440 --> 01:31:43.400
people. No, it probably wouldn't. You're right. Yeah, and particularly when you get onto questions

01:31:43.400 --> 01:31:50.680
like consciousness as well, I mean, I think it does become very, very difficult, because once

01:31:50.680 --> 01:31:56.120
you're putting forward or advocating a theoretical framework that seems like it's supposed to have

01:31:56.120 --> 01:32:02.200
all the answers. I mean, in reality, it doesn't. I mean, I think it's a useful framework to be

01:32:02.200 --> 01:32:08.680
able to ask the right questions or to be able to articulate your hypotheses. So if you think that

01:32:08.680 --> 01:32:15.720
consciousness is based upon the idea of having some sense of trajectory of temporal extent and

01:32:15.720 --> 01:32:21.640
different worlds I can choose between or different futures I can choose between, that might be a key

01:32:21.640 --> 01:32:24.920
part of it. But for some people, that's not what they mean by consciousness.

01:32:27.560 --> 01:32:36.280
I found in a particular reading books by people like Anil Seth on this sort of topic, I found one

01:32:36.280 --> 01:32:43.480
of the interesting comparisons being the questions about consciousness versus questions about life.

01:32:43.480 --> 01:32:48.120
And we almost don't ask what life is anymore. It doesn't necessarily seem that mysterious,

01:32:48.920 --> 01:32:54.600
just because we've had so much of an understanding of the processes involved in life, the dynamics of

01:32:54.600 --> 01:33:01.160
life and the way biology works, it's still much more to go. But the question of what life is just

01:33:01.160 --> 01:33:06.200
doesn't seem as relevant today as I suspect it did many years ago with those sorts of questions

01:33:06.200 --> 01:33:10.440
that were being posed. And perhaps we'll see the same thing with questions like consciousness.

01:33:11.800 --> 01:33:17.800
Yeah, it's interesting though how vague many of these concepts are. And it's quite an interesting

01:33:17.800 --> 01:33:24.760
thought experiment just to get someone to explain just an everyday thing, you know, like what happens

01:33:24.760 --> 01:33:32.840
when you throw coffee on the floor. And just keep asking why. And just observing how incoherent and

01:33:32.840 --> 01:33:37.240
incomplete the explanations are. And it's the same thing with life, it's the same thing of

01:33:37.240 --> 01:33:41.960
consciousness, it's the same thing of causality, agency, intelligence, all of these different things.

01:33:41.960 --> 01:33:48.200
And I guess most people don't spend time digging into their understandings of these things and

01:33:48.200 --> 01:33:54.360
realizing how incoherent and incomplete they are. Life is quite an interesting one in particular,

01:33:54.360 --> 01:34:02.200
because I think one of the achievements of active inference is blurring the definition of or the

01:34:02.200 --> 01:34:10.120
demarcation between things which are and are not alive. For example, the orthopedic anactivists,

01:34:10.120 --> 01:34:16.040
they think of biology as being instrumental. And what the, you know, free energy principle

01:34:16.040 --> 01:34:21.320
does in my opinion, is it removes the need for this, it almost removes the need for biology

01:34:21.320 --> 01:34:27.240
entirely. It just says it's just dynamics, it's just physics. But yeah, I mean, just on that

01:34:27.240 --> 01:34:32.440
point, though, I think many of our ideas about the world are quite incoherent.

01:34:33.880 --> 01:34:38.040
Yeah. And I think it's interesting that, you know, one of the things that you're saying,

01:34:38.040 --> 01:34:42.280
and I would agree with you as one of the big advantages of active inference-based formalisms,

01:34:42.280 --> 01:34:47.480
you'll probably find some people will say, that's a problem with it, that actually there is a clean

01:34:47.480 --> 01:34:53.240
distinction in their mind between these different things. But then I think the challenge is to work

01:34:53.240 --> 01:34:59.720
out what that distinction is, if it exists. And it may be a distinction in their mind that doesn't

01:34:59.720 --> 01:35:07.480
exist in somebody else's mind. And so getting people to try and or trying to support people to

01:35:07.480 --> 01:35:14.280
be able to express that in a very precise mathematical hypothesis, I think is quite a

01:35:14.280 --> 01:35:19.000
useful way of trying to explore those problems. Because clearly, for some people, there is

01:35:19.000 --> 01:35:22.680
something that's getting at it that is not quite explaining. And it's interesting to try

01:35:22.680 --> 01:35:28.280
and explore that and to work out what that thing is. Indeed, indeed. And just final question,

01:35:28.280 --> 01:35:32.520
what was your experience writing a book? And would you recommend it to other people?

01:35:33.000 --> 01:35:46.120
I enjoyed writing it. I think it's time consuming and can feel like it's going on forever some of

01:35:46.120 --> 01:35:51.320
the time compared to, you know, I think anyone who's had some experience of writing papers will

01:35:51.320 --> 01:35:55.160
often find that at the point where you're ready to submit it, you're just sick of it and want to see

01:35:55.160 --> 01:36:01.320
the back of it. And then it's rudely returned to you by the peer reviewers with lots of comments

01:36:01.640 --> 01:36:07.880
writing a book, it obviously takes you much longer. So you end up being almost more sick of it at

01:36:07.880 --> 01:36:13.080
various times. But it's quite fun as a collaborative project. It's quite interesting to get other

01:36:13.080 --> 01:36:17.320
people's perspectives on it. And I was lucky to have great collaborators to write it with.

01:36:18.440 --> 01:36:23.000
And I think it really is a good way of organizing your thoughts in a slightly more holistic way

01:36:23.800 --> 01:36:27.640
than you would while focusing on a very specific topic in a research paper.

01:36:28.600 --> 01:36:33.320
And I've also just enjoyed the response I've had from people who've read it,

01:36:34.040 --> 01:36:41.880
some of whom have picked out a number of errors, not many. But generally,

01:36:41.880 --> 01:36:48.680
everybody's been very supportive of that and people seem to have responded well to it,

01:36:48.680 --> 01:36:51.480
which I think is always encouraging. And that's what we hope should happen.

01:36:52.040 --> 01:36:56.360
Wonderful. Well, look, Thomas, it's been an absolute honor having you on the show. I really

01:36:56.360 --> 01:37:08.920
appreciate you coming on. Thank you so much. Well, thank you. I've enjoyed it.

