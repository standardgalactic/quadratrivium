1
00:00:00,000 --> 00:00:09,720
Let's have Melanie Mitchell to give our final opening statement, six minutes on the clock,

2
00:00:09,720 --> 00:00:10,720
Melanie.

3
00:00:10,720 --> 00:00:13,860
Yeah, so this is my opportunity to say I love Melanie.

4
00:00:13,860 --> 00:00:19,400
She is amazing, and she's coming back on MLST in about two weeks.

5
00:00:19,400 --> 00:00:20,400
Oh, amazing.

6
00:00:20,400 --> 00:00:21,400
Yeah, that's good.

7
00:00:21,400 --> 00:00:23,840
Yeah, she gives a good representation of herself in here, I think.

8
00:00:23,840 --> 00:00:24,840
Melanie is amazing.

9
00:00:25,520 --> 00:00:30,720
Fears about machines unleashing human extinction have deep roots in our collective psyche.

10
00:00:30,720 --> 00:00:34,480
These fears are as old as the invention of machines themselves.

11
00:00:34,480 --> 00:00:40,360
But tonight, we're debating whether these fears belong in the realm of science fiction

12
00:00:40,360 --> 00:00:48,680
and philosophical speculation, or whether AI is an actual real life existential threat.

13
00:00:48,680 --> 00:00:55,480
I'm going to argue that AI does not pose such a threat in any reasonably near future.

14
00:00:55,480 --> 00:01:01,320
Large language models have sparked heated debate on whether AI's exhibit genuine understanding

15
00:01:01,320 --> 00:01:03,960
of language and the world.

16
00:01:03,960 --> 00:01:09,420
With capabilities rivaling humans across diverse benchmarks, some hail language models as

17
00:01:09,420 --> 00:01:12,320
harbingers of real intelligence.

18
00:01:12,320 --> 00:01:18,400
But skeptics argue that their mastery is skin deep, lacking true comprehension.

19
00:01:18,400 --> 00:01:23,560
So how can we assess these claims and gain insight into the nature of what it means to

20
00:01:23,560 --> 00:01:24,560
understand?

21
00:01:24,560 --> 00:01:31,000
Now, on the show today, we have Professor Melanie Mitchell, a leading thinker on AI and intelligence,

22
00:01:31,000 --> 00:01:35,440
and one of the researchers in the community I personally most align with and look up to

23
00:01:35,440 --> 00:01:37,240
the most.

24
00:01:37,240 --> 00:01:43,520
Melanie's distinguished career crosses computer science, complex systems, and cognitive science,

25
00:01:43,520 --> 00:01:48,620
and she wrote the influential books Artificial Intelligence, A Guide for Thinking Humans,

26
00:01:48,620 --> 00:01:51,680
and also Complexity, A Guided Tour.

27
00:01:51,680 --> 00:01:57,320
Now central to Melanie's perspective is the idea that human understanding relies on flexible

28
00:01:57,320 --> 00:02:01,560
mental models grounded in sensory experience.

29
00:02:01,560 --> 00:02:08,640
Now she wrote that understanding language requires having the concepts that language describes.

30
00:02:08,640 --> 00:02:14,040
Large language models are trained purely on statistical relationships between words.

31
00:02:14,040 --> 00:02:18,600
Their knowledge is not grounded in a causal model of reality.

32
00:02:18,600 --> 00:02:24,320
Now Melanie is the Davis Professor of Complexity at the Santa Fe Institute, and her major work

33
00:02:24,320 --> 00:02:30,600
has been in the areas of analogical reasoning, complex systems, genetic algorithms, and cellular

34
00:02:30,600 --> 00:02:32,100
automata.

35
00:02:32,100 --> 00:02:35,480
She's achieved legendary status in the field of AI.

36
00:02:35,480 --> 00:02:41,320
She received her PhD in 1990 from the University of Michigan under Douglas Hofstadter, the

37
00:02:41,320 --> 00:02:44,240
famous author of Godel Escherbach.

38
00:02:44,240 --> 00:02:49,600
Melanie argues that we must rethink how AI systems are evaluated.

39
00:02:49,600 --> 00:02:54,480
Typical benchmarks summarize aggregate performance and, you know, these obscure failure modes

40
00:02:54,480 --> 00:02:57,680
and mask the underlying mechanisms.

41
00:02:57,680 --> 00:03:03,600
We need rigorous granular testing focused keenly on abstract generalization.

42
00:03:03,600 --> 00:03:07,760
Sort of like a sorcerer's apprentice gone nuclear.

43
00:03:07,760 --> 00:03:12,120
For example, Yoshua Benjiro wrote about this thought experiment.

44
00:03:12,120 --> 00:03:17,360
We might ask an AI to fix climate change and to solve the problem it could design a virus

45
00:03:17,360 --> 00:03:26,360
that decimates the human population, presto, humans dead, no more carbon emissions.

46
00:03:26,360 --> 00:03:34,280
This is an example of what's called the fallacy of dumb superintelligence.

47
00:03:34,280 --> 00:03:40,400
That is, it's a fallacy to think that a machine could be, quote, smarter than humans in all

48
00:03:40,400 --> 00:03:47,320
respects, unquote, and still lack any common sense understanding of humans, such as understanding

49
00:03:47,320 --> 00:03:52,840
why we made the request to fix climate change and the fact that we prefer not to be wiped

50
00:03:52,840 --> 00:03:55,040
out.

51
00:03:55,040 --> 00:04:02,200
This is all about having insight into one's goals and the likely effect of one's actions.

52
00:04:02,200 --> 00:04:07,680
We would never give unchecked autonomy and resources to an AI that lacked these basic

53
00:04:07,680 --> 00:04:09,480
aspects of intelligence.

54
00:04:09,480 --> 00:04:12,320
It just does not make sense.

55
00:04:12,320 --> 00:04:13,320
The third scenario.

56
00:04:13,320 --> 00:04:14,760
Yeah, that is absolutely.

57
00:04:14,760 --> 00:04:18,640
She made that point so much more eloquently than I've tried to make it in the past.

58
00:04:18,640 --> 00:04:22,160
Yeah, even earlier in this conversation, I was trying to get that across, but that's

59
00:04:22,160 --> 00:04:23,160
exactly it.

60
00:04:23,160 --> 00:04:24,160
It's this dumb superintelligence.

61
00:04:25,160 --> 00:04:26,160
Yeah, exactly.

62
00:04:26,160 --> 00:04:31,880
Anyway, folks, I hope you enjoy the show, and now I bring you Professor Melanie Mitchell.

63
00:04:31,880 --> 00:04:37,920
Sounds like almost like there's a very quiet supercomputer running behind the screen.

64
00:04:37,920 --> 00:04:38,920
It's my brain.

65
00:04:38,920 --> 00:04:39,920
Yeah.

66
00:04:39,920 --> 00:04:41,920
I think that's what this is.

67
00:04:41,920 --> 00:04:46,320
You know, we can robustly adapt much more so than GPT-4.

68
00:04:46,320 --> 00:04:48,320
You and I have the same chair.

69
00:04:48,320 --> 00:04:50,320
We have the same chair, I think.

70
00:04:50,320 --> 00:04:51,320
Oh, yeah.

71
00:04:51,320 --> 00:04:52,320
I can't see your chair.

72
00:04:52,320 --> 00:04:53,320
Yeah.

73
00:04:53,960 --> 00:04:54,960
Me too.

74
00:04:54,960 --> 00:04:55,960
They're home in Miller.

75
00:04:55,960 --> 00:04:58,600
Yeah, I think they're all the same chair.

76
00:04:58,600 --> 00:04:59,600
Yeah.

77
00:04:59,600 --> 00:05:00,600
Yeah.

78
00:05:00,600 --> 00:05:01,600
Excellent chair.

79
00:05:01,600 --> 00:05:02,600
Yep, chair buddies.

80
00:05:02,600 --> 00:05:03,600
Yeah.

81
00:05:03,600 --> 00:05:10,840
I felt it would be hundreds of years before anything even remotely like a human mind would

82
00:05:10,840 --> 00:05:17,000
be asymptotically approaching the level of the human mind, but from beneath.

83
00:05:17,000 --> 00:05:25,680
I never imagined that computers would rival or let alone surpass human intelligence, but

84
00:05:25,680 --> 00:05:29,320
it seemed to me like it was a goal that was so far away.

85
00:05:29,320 --> 00:05:35,040
I wasn't worried about it, but when certain systems started appearing and then this started

86
00:05:35,040 --> 00:05:41,400
happening at an accelerating pace, it felt as if not only are my belief systems collapsing,

87
00:05:41,400 --> 00:05:50,560
but it feels as if the entire human race is going to be eclipsed and left in the dust.

88
00:05:50,560 --> 00:05:55,160
Douglas Hofstadter, he came out as a doomer.

89
00:05:55,160 --> 00:05:59,320
Well, I don't know if he came out exactly.

90
00:05:59,320 --> 00:06:02,040
He's been a doomer for quite a while.

91
00:06:02,040 --> 00:06:03,520
Oh, go on.

92
00:06:03,520 --> 00:06:05,320
I wasn't aware of that.

93
00:06:05,320 --> 00:06:10,000
Well, I don't, you know, doomer is, you know, there's different kinds of doomers.

94
00:06:10,760 --> 00:06:16,500
In my AI book, the first chapter, the prologue is called Terrified, and it's all about how

95
00:06:16,500 --> 00:06:24,720
Doug is very terrified about AI and the possible things that are going to come.

96
00:06:24,720 --> 00:06:34,600
That was based on a talk he gave in 2013, and earlier than that, he was extremely worried

97
00:06:34,600 --> 00:06:41,800
about the singularity, the idea of the singularity from Kurzweil, and wrote quite a bit about

98
00:06:41,800 --> 00:06:42,800
that.

99
00:06:42,800 --> 00:06:49,040
So, I feel like that it's not that new, but maybe this is sort of because there's so much

100
00:06:49,040 --> 00:06:54,960
talk about AI, doom, and so on, that this is kind of, people are kind of paying attention

101
00:06:54,960 --> 00:06:55,960
now.

102
00:06:55,960 --> 00:06:59,480
Yeah, I don't know whether I misunderstood something because I read out, you had this

103
00:06:59,480 --> 00:07:05,040
beautiful piece about the Googleplex in Chopin, and he was terrified that cognition might

104
00:07:05,040 --> 00:07:10,560
be disappointingly simple to mechanize, and, you know, surely we couldn't replicate the

105
00:07:10,560 --> 00:07:16,120
infinite nuance of the mental state that went into writing that beautiful music.

106
00:07:16,120 --> 00:07:20,040
But so maybe he was worried about it, but he didn't think it was possible in principle

107
00:07:20,040 --> 00:07:21,040
or something.

108
00:07:21,040 --> 00:07:26,960
Well, no, he was quite worried about that it was going to happen sooner than he thought,

109
00:07:26,960 --> 00:07:32,400
and that, you know, his quote that it's AI is going to leave us in the dust.

110
00:07:32,400 --> 00:07:34,640
So that's kind of his flavor of doomer.

111
00:07:34,640 --> 00:07:43,720
I'm not sure he has the same, like, existential worry about things as, like, Stuart Russell

112
00:07:43,720 --> 00:07:44,720
or somebody.

113
00:07:44,720 --> 00:07:45,720
Okay.

114
00:07:45,720 --> 00:07:52,760
So he's not so worried about them necessarily churning us into, you know, fertilizer or

115
00:07:52,760 --> 00:07:56,920
raw materials or something, but just that it's not so specific, I think.

116
00:07:57,880 --> 00:08:02,680
But yeah, yeah, I talk to him about it all the time, and he wavers.

117
00:08:04,000 --> 00:08:08,480
Oh, interesting, because I've heard you define yourself as a centrist on other podcasts,

118
00:08:08,480 --> 00:08:15,320
because I'm sure the doomers would lump you in with Cholet and Lacune, maybe, and some

119
00:08:15,320 --> 00:08:21,000
of the critics, but you do think that these models are intelligent, right?

120
00:08:21,960 --> 00:08:23,560
I do think that they're intelligent.

121
00:08:23,640 --> 00:08:29,400
Well, you know, intelligence is an ill-defined notion.

122
00:08:30,040 --> 00:08:30,520
Oh, yeah.

123
00:08:30,520 --> 00:08:36,040
It's multidimensional, and, you know, I don't know if we can say yes or no about something

124
00:08:36,040 --> 00:08:41,680
being intelligent rather than, you know, intelligent in certain ways or to certain degrees.

125
00:08:42,640 --> 00:08:43,000
Yeah.

126
00:08:43,000 --> 00:08:45,280
Well, we've got so much to get into.

127
00:08:45,280 --> 00:08:52,440
I mean, I think slowly we'll talk about arc and your concept arc work, but I kind of agree

128
00:08:52,440 --> 00:08:56,600
with you that, and actually you had that paper out about the four fallacies, and you spoke

129
00:08:56,600 --> 00:09:03,400
about this fallacy of pure intelligence, and I kind of agree that the gnarly reality is far

130
00:09:03,400 --> 00:09:04,360
more complex than that.

131
00:09:04,360 --> 00:09:09,800
There was a really interesting paper that you linked on, no, it was an article by Dileep George,

132
00:09:09,800 --> 00:09:13,960
and he said that a university professor has a much better understanding of a vector,

133
00:09:14,520 --> 00:09:20,280
because it's just grounded in so many real-world situations and contexts and so on,

134
00:09:20,280 --> 00:09:24,680
and an undergraduate or, indeed, a language model would have a very ungrounded, very kind

135
00:09:24,680 --> 00:09:31,800
of low-resolution idea of what this concept is, and it kind of leans away from this puritanical,

136
00:09:31,800 --> 00:09:37,240
ungrounded, abstract form of intelligence to something which is really very complex and intermingled.

137
00:09:37,960 --> 00:09:38,360
Yeah.

138
00:09:38,360 --> 00:09:39,560
I mean, I agree with that.

139
00:09:41,080 --> 00:09:45,160
Well, except that there's another aspect to that, too, which you write about, which is,

140
00:09:45,960 --> 00:09:49,720
I agree that that happens, but what the human mind also seems to do is,

141
00:09:50,360 --> 00:09:56,920
as the thing becomes more grounded in more cases, then we develop yet another concept

142
00:09:56,920 --> 00:10:03,480
that kind of describes the similar aspects that we see throughout all those different

143
00:10:03,480 --> 00:10:04,280
concepts, right?

144
00:10:04,280 --> 00:10:08,920
So we're kind of this iterative loop where we're always finding more and more context,

145
00:10:08,920 --> 00:10:14,520
and then we're also finding newer and newer concepts that span those increasing contexts.

146
00:10:14,520 --> 00:10:15,320
Is that fair?

147
00:10:15,320 --> 00:10:16,280
Yeah, sure, yeah.

148
00:10:17,000 --> 00:10:21,080
I mean, that kind of goes along with the whole sort of

149
00:10:23,160 --> 00:10:31,000
metaphor theory of cognition, of Lake Offit at all, and that we're sort of building on these

150
00:10:31,000 --> 00:10:34,440
physical metaphors, and we can build up many, many layers of abstraction.

151
00:10:36,760 --> 00:10:38,280
So, yeah, we can talk about that.

152
00:10:39,800 --> 00:10:40,920
We're not recording yet, right?

153
00:10:41,560 --> 00:10:42,280
Oh, no, we are.

154
00:10:42,280 --> 00:10:42,520
We are.

155
00:10:42,520 --> 00:10:43,240
This is all recording.

156
00:10:43,240 --> 00:10:43,800
Oh, we are?

157
00:10:44,040 --> 00:10:45,800
I hope you're okay with what you said so far.

158
00:10:45,800 --> 00:10:51,960
But yeah, so there's the Lake Off building on the body of symbols as pointers.

159
00:10:51,960 --> 00:10:55,400
And by the way, that Dileep George article was really fascinating because it was saying

160
00:10:55,400 --> 00:10:57,560
that language is a conditioning force.

161
00:10:57,560 --> 00:11:02,360
So actually, we all have these high-resolution world simulators built into us, and we kind of

162
00:11:03,000 --> 00:11:07,960
condition how that operates and generate counterfactuals through language,

163
00:11:07,960 --> 00:11:09,240
which I thought was quite interesting.

164
00:11:10,120 --> 00:11:10,440
Yeah.

165
00:11:11,400 --> 00:11:11,880
Yeah.

166
00:11:11,880 --> 00:11:15,000
But, Tim, why don't you frame up the debate?

167
00:11:15,000 --> 00:11:16,440
Because we found a beautiful paragraph.

168
00:11:17,080 --> 00:11:17,480
We did.

169
00:11:17,480 --> 00:11:17,720
We did.

170
00:11:17,720 --> 00:11:19,640
We found an amazing bit.

171
00:11:19,640 --> 00:11:20,440
Yeah.

172
00:11:20,440 --> 00:11:24,200
But just to close the loop on what I was saying, we were discussing an activism last night.

173
00:11:24,200 --> 00:11:27,560
I'm not sure if you're familiar with some of these externalized forms of cognition,

174
00:11:27,560 --> 00:11:30,200
and we were talking about the concept of a goal.

175
00:11:30,760 --> 00:11:35,480
And agents, of course, they just have these high-resolution belief trajectories of,

176
00:11:36,040 --> 00:11:37,880
you know, I can do all of these different actions.

177
00:11:37,880 --> 00:11:39,320
And that's not really a goal.

178
00:11:39,320 --> 00:11:44,520
You know, a goal is this very abstract thing which emerges at the system level,

179
00:11:44,520 --> 00:11:47,880
and no individual agents in the system have a concept of a goal.

180
00:11:47,880 --> 00:11:51,000
And it might be similar with some of these concepts that we're talking about,

181
00:11:51,000 --> 00:11:54,600
which is, to what extent do they exist, and to what extent are they just

182
00:11:54,600 --> 00:11:58,680
something intelligible that we can point to, but they don't really meaningfully exist

183
00:11:58,680 --> 00:12:00,200
in the system at a high-resolution.

184
00:12:01,480 --> 00:12:06,200
Are you talking about an AI or in people, a little piece here?

185
00:12:06,200 --> 00:12:06,760
All of the above.

186
00:12:07,720 --> 00:12:10,840
I mean, I think goal is a wonderful example, because we think of it.

187
00:12:10,840 --> 00:12:13,560
I mean, it's even one of Spelke's core priors.

188
00:12:13,560 --> 00:12:15,400
It seems like something so primitive.

189
00:12:16,520 --> 00:12:20,440
But I don't think they really do exist in us.

190
00:12:20,440 --> 00:12:25,560
I mean, we're interesting because we have this reflexive conception of a goal,

191
00:12:25,560 --> 00:12:28,120
but does a mouse have a goal?

192
00:12:30,520 --> 00:12:36,680
Right. I mean, goal is another one of those words that, you know, we use in a very fluid

193
00:12:36,680 --> 00:12:41,160
way. So we talk about, for instance, a reinforcement learning agent having a goal

194
00:12:41,880 --> 00:12:48,120
that we've given to it, right? Or it might have a goal to kind of maximize its information gain

195
00:12:48,120 --> 00:12:56,440
or something. But is that the same thing as a human having a goal that it's like,

196
00:12:56,440 --> 00:13:04,760
you know, to graduate from college or to, you know, make something of your life for

197
00:13:04,760 --> 00:13:08,840
all of these things? It's a very different sense of goal.

198
00:13:08,840 --> 00:13:17,960
And so I would say, yes, a mouse has goals, but those goals are different in degree and in kind

199
00:13:17,960 --> 00:13:23,240
of qualitatively than many of the things we call goals in humans and in machines.

200
00:13:23,240 --> 00:13:29,720
So I think goal is one of those sort of anthropomorphizing words that we need to

201
00:13:29,800 --> 00:13:36,600
be careful about when we equate goals in these different systems as being the same thing.

202
00:13:37,160 --> 00:13:41,480
And I actually, you know, had a discussion with, I think it was with Stuart Russell

203
00:13:42,760 --> 00:13:49,960
about the notion of goal. And his view, and I think this is a view of many other people

204
00:13:50,840 --> 00:13:57,880
in AI, is that large language models actually have goals, complex goals,

205
00:13:58,440 --> 00:14:09,000
that they, that emerge out of this, you know, their loss function of predicting the next token,

206
00:14:10,120 --> 00:14:16,440
because the only way to successfully predict the next token in human language is to develop

207
00:14:16,440 --> 00:14:22,120
human-like goals. I find that dubious, but it's an interesting perspective.

208
00:14:22,360 --> 00:14:28,760
Yeah, I'm amenable to it, because there's always this dichotomy, as you say, of there's the objective,

209
00:14:28,760 --> 00:14:32,840
there's perplexity, and there's these emergent goals, and there's even this simulator's theory

210
00:14:32,840 --> 00:14:38,040
of large language models, which is that they're a superposition of agents. And it's quite situated

211
00:14:38,040 --> 00:14:43,240
as well, because goals kind of materialize depending on your perspective. So if you use a

212
00:14:43,240 --> 00:14:47,320
language model in a certain way from a certain perspective, it might appear that there is some

213
00:14:47,320 --> 00:14:52,200
kind of goal there, but of course, it's just an aspect onto something which is very complex.

214
00:14:52,760 --> 00:14:56,760
But I think we should frame up this beautiful piece, actually, from your

215
00:14:57,400 --> 00:15:01,560
Modes of Understanding paper from much this year. I always call it the Modes of Understanding paper.

216
00:15:01,560 --> 00:15:06,040
It was actually titled The Debate over Understanding in AI's Large Language Models.

217
00:15:06,040 --> 00:15:11,480
And you said, towards the end, that the key question of the debate about understanding in

218
00:15:11,560 --> 00:15:18,040
large language models is, one, is talking of understanding in such systems simply a category

219
00:15:18,040 --> 00:15:22,920
error, which is mistaking associations between language tokens for associations between

220
00:15:23,560 --> 00:15:30,200
tokens and physical, social, and mental experience? In short, is it the case that these models are

221
00:15:30,200 --> 00:15:36,760
not and will never be the kind of things that can understand, or conversely, to do these systems or

222
00:15:36,760 --> 00:15:41,720
their near term successes? Actually, even in the absence of physical experience, create something

223
00:15:41,720 --> 00:15:46,520
like the rich concept based mental models that are central to human understanding. And if so,

224
00:15:46,520 --> 00:15:51,800
does scaling these models create even better concepts? Or three, if these systems do not

225
00:15:51,800 --> 00:15:57,320
create such concepts, can their unimaginably large systems of statistical correlations,

226
00:15:57,320 --> 00:16:02,760
produce abilities that are fundamentally equivalent to human understanding, or indeed that enable

227
00:16:02,760 --> 00:16:08,120
new forms of higher order logic that humans are incapable of accessing? And at this point,

228
00:16:08,120 --> 00:16:15,560
will it still make sense to call such correlations spurious and the resulting solutions shortcuts?

229
00:16:15,560 --> 00:16:21,400
And would it make sense to see these systems' behaviors not as competence without comprehension,

230
00:16:21,400 --> 00:16:26,280
but as a new, non-human form of understanding? And you said that these questions are no longer

231
00:16:26,280 --> 00:16:31,720
in the realm of abstract philosophical discussions, but they touch on very real concerns about the

232
00:16:31,720 --> 00:16:37,960
capabilities and robustness and safety and ethics of AI systems. So let's use that as a leader.

233
00:16:37,960 --> 00:16:41,480
What do you think, Melanie? It's beautiful. That was a beautiful paragraph, by the way.

234
00:16:41,480 --> 00:16:45,720
Yeah, it's so good. Wow. This exactly crystallizes the discussion.

235
00:16:48,280 --> 00:16:55,400
Yeah, I think that it's something that we in AI are all grappling with now. And I think it's

236
00:16:55,400 --> 00:17:02,920
something that the history of AI has forced us to grapple with mental terms like understand,

237
00:17:03,720 --> 00:17:13,080
or consciousness, and even intelligence. Because we keep saying, oh, well, understanding, if you

238
00:17:13,080 --> 00:17:23,080
can do X, then that means that you're actually understanding. You can't do language translation

239
00:17:23,080 --> 00:17:30,280
without understanding. You can't do speech to text without understanding. You can't generate

240
00:17:30,280 --> 00:17:39,720
articulate natural language without understanding. And I think this is, in many cases, we then step

241
00:17:39,720 --> 00:17:44,360
back and say, wait, that's not what we meant by understanding. It turns out you can do all these

242
00:17:44,360 --> 00:17:49,080
things without understanding. So we're sort of saying, well, we didn't really know what we meant

243
00:17:49,080 --> 00:17:56,600
by the term understanding, I think. And often, some people criticize that as moving the goalposts.

244
00:17:58,200 --> 00:18:00,520
You're moving the goalposts. The so-called AI effect, right?

245
00:18:00,520 --> 00:18:02,200
Right. It's the AI effect.

246
00:18:02,200 --> 00:18:11,800
But I think of it more as AI is forcing people to really refine their notions

247
00:18:12,680 --> 00:18:17,160
of that that have been quite fuzzy about what these terms actually mean.

248
00:18:18,200 --> 00:18:25,320
And there was a fantastic talk by Dave Chalmers, the philosopher, who I think you've probably had

249
00:18:25,320 --> 00:18:33,640
on this show, where he talks about conceptual engineering, which is something that philosophers

250
00:18:33,640 --> 00:18:39,240
do where they take a term like understanding and they refine it. And he said, okay, well, we have

251
00:18:40,200 --> 00:18:48,760
p-understanding, which is like personal, phenomenological. And then we have c-understanding

252
00:18:48,760 --> 00:18:54,520
and e-understanding and x-understanding and all these different letters that meant to say that

253
00:18:54,520 --> 00:19:01,480
this term is not a unified thing that we can apply to a system. We have to really specify what we

254
00:19:01,480 --> 00:19:08,040
mean exactly. Well, one way I've come to think about it, and it's largely from reading your work

255
00:19:08,040 --> 00:19:12,760
and your assessments about it, is that for the first time, we're actually being forced to do

256
00:19:13,720 --> 00:19:20,920
the science of machine cognition, right? Because for too long, it's either just not been sophisticated

257
00:19:20,920 --> 00:19:26,840
enough. Why bother? Like it's obviously not doing any cognition. And as you point out, it's now

258
00:19:26,840 --> 00:19:32,520
actually having real world impacts. And so we actually have to start doing the science, right?

259
00:19:32,520 --> 00:19:37,640
We have to say, okay, does this thing have cognition? Here's a hypothesis. Let's do some

260
00:19:37,640 --> 00:19:42,360
test. Okay, it failed. What was the failure mode? Why did it fail? Let's understand that more. How

261
00:19:42,360 --> 00:19:47,880
can we engineer it not to fail? It's like we can no longer ignore adversarial examples,

262
00:19:47,880 --> 00:19:51,800
shortcut learning, et cetera. We have to finally grapple with it, it seems to me.

263
00:19:52,680 --> 00:19:57,080
Yeah, I think that's exactly right. And what's interesting is we, computer scientists, were

264
00:19:57,080 --> 00:20:05,480
never trained in experimental methods. We never learned about like controls and confounding things.

265
00:20:05,480 --> 00:20:18,840
It's a great point. And so now people are doing, applying human tests of understanding or intelligence

266
00:20:18,840 --> 00:20:26,520
or reasoning, what have you, to machines without having the right experimental methods to say whether

267
00:20:26,520 --> 00:20:33,480
or not what they're testing is actually valid. So there's a cognitive scientist named Michael

268
00:20:33,480 --> 00:20:38,760
Frank at Stanford, who's been writing a lot of stuff about experimental method and how do you

269
00:20:38,760 --> 00:20:48,120
apply it to AI and why you need sort of expertise in this area to really make sense of these systems.

270
00:20:48,120 --> 00:20:54,120
And I'm totally on board with that. Yeah, we'll talk about your piece with Tannenbaum later,

271
00:20:54,120 --> 00:20:59,240
but as you say, a lot of AI folks don't really think about experiment design. But actually,

272
00:20:59,240 --> 00:21:03,480
even with Chalet's ARC challenge, maybe we should talk about that. So he invented this

273
00:21:03,480 --> 00:21:08,920
measure of intelligence, which unfortunately was not computable, but it was mathematically

274
00:21:08,920 --> 00:21:14,760
beautiful. Basically saying that, and he's a huge Spelke fan, I kind of put Chalet very,

275
00:21:14,760 --> 00:21:20,680
very close to you actually in AI researcher space. And his measure of intelligence basically says,

276
00:21:21,320 --> 00:21:26,360
I give you priors, I give you experience, you give me a skill program, it extrapolates into these

277
00:21:26,360 --> 00:21:31,160
different spaces and experience space. And the kind of the conversion ratio between those

278
00:21:31,160 --> 00:21:37,000
priors and experience and the space that I get is intelligence. And that's very interesting.

279
00:21:37,000 --> 00:21:41,880
And then he produced this corpus, this ARC challenge. And it's a bit like an IQ test. It's

280
00:21:41,880 --> 00:21:49,000
this kind of 2D gridded colored cells. And you have a couple of examples, and you have to do

281
00:21:49,000 --> 00:21:53,880
another one or two examples. And it was very diverse because it was testing what he called

282
00:21:53,880 --> 00:21:59,400
developer aware generalization. And there were a couple of issues with that. So you wrote this

283
00:21:59,400 --> 00:22:05,320
beautiful concept ARC paper, and maybe you can introduce that. But one of the things you pointed

284
00:22:05,320 --> 00:22:11,880
out, which I felt was quite interesting is that even if people succeeded on Francois's challenge,

285
00:22:11,880 --> 00:22:16,760
it wouldn't necessarily be what we would call intelligence, because it's not necessarily

286
00:22:16,760 --> 00:22:22,120
demonstrating systematic generalization beyond those one or two examples in his test set.

287
00:22:22,600 --> 00:22:30,920
So our motivation was twofold. So first of all, I love the ARC challenge. It's beautiful.

288
00:22:31,880 --> 00:22:43,320
It's super elegant. And I'm very sympathetic with Francois' definition of intelligence,

289
00:22:43,320 --> 00:22:49,080
although I think there's probably, again, intelligence is very multi-dimensional. But

290
00:22:49,160 --> 00:22:55,400
this is one aspect of it for sure. And his problems are great because they

291
00:22:56,200 --> 00:23:02,600
just give a few examples, and people are pretty good at abstracting a rule or a concept from

292
00:23:02,600 --> 00:23:11,000
just a few examples. And they don't use language, so they don't get into the whole prior knowledge

293
00:23:11,000 --> 00:23:21,720
of language and a lot of things that you don't want to confound these tests. But one of the

294
00:23:21,720 --> 00:23:26,600
problems with ARC is that many of the problems are quite hard. They're quite hard for people.

295
00:23:27,320 --> 00:23:35,720
And they're so hard that they don't really differentiate between different programs that

296
00:23:35,720 --> 00:23:42,120
are attempting to solve this challenge. So there was a Kaggle competition with the ARC challenge,

297
00:23:42,120 --> 00:23:50,360
and there were two, the two best programs got about, they each got about 20% accuracy on the

298
00:23:50,360 --> 00:24:00,360
hidden test set. So it didn't really distinguish them at all. And the other problem was that,

299
00:24:01,320 --> 00:24:06,840
as you mentioned, the test wasn't very systematic, meaning that let's say there's a

300
00:24:07,640 --> 00:24:13,880
problem in ARC that deals with the concept of inside, something being inside something else.

301
00:24:13,880 --> 00:24:19,800
And let's say that something, a program gets that one right. Does that mean that it understands

302
00:24:19,800 --> 00:24:26,520
the concept of inside in a general way? Well, we don't know because the test doesn't test that

303
00:24:26,520 --> 00:24:34,680
systematically. And that was actually intentional from Sholey, because he didn't want any way to,

304
00:24:35,400 --> 00:24:42,920
for programs to be able to reverse engineer the generation process of these problems.

305
00:24:42,920 --> 00:24:48,600
So if you say, oh, well, I'm going to deal with these 10 concepts, then somebody presumably

306
00:24:48,600 --> 00:24:56,440
could reverse engineer those, the problems and not be general. But for us, we wanted to say, well,

307
00:24:56,440 --> 00:25:02,200
how would you just systematically test a program for understanding of a concept of a very basic,

308
00:25:02,200 --> 00:25:08,440
spatial or semantic concept? And so what we did was we took the ARC domain and we created

309
00:25:09,560 --> 00:25:17,320
about almost 500 new problems that were systematically grouped into concept groups.

310
00:25:17,320 --> 00:25:24,440
So like inside of, that was one of the groups. And so we looked at, we created several problems

311
00:25:24,440 --> 00:25:31,560
that were variations on that concept. And there were variations that ranged in like abstraction,

312
00:25:31,560 --> 00:25:41,080
degree of abstraction, and sort of complexity of the problem. And the hypothesis was that if a

313
00:25:41,720 --> 00:25:49,880
human or a program could successfully solve the problems in a given concept group, they really

314
00:25:49,880 --> 00:25:57,080
do have a good sort of grasp of that concept. So this was the genesis of concept ARC.

315
00:25:58,200 --> 00:26:03,160
You know, it's fascinating because so you're, you're attempting again to build the science of

316
00:26:03,960 --> 00:26:08,600
machine cognitive science, essentially. And hey, it has to be systematized, we need to have these

317
00:26:08,600 --> 00:26:14,280
concept categories, we need to be able to generate examples of progressive complexity and, you know,

318
00:26:14,280 --> 00:26:19,080
layer of abstraction, everything. And then yet you mentioned Chalet intentionally didn't

319
00:26:19,080 --> 00:26:22,840
systematize it to avoid reverse engineering. And that's kind of a fascinating

320
00:26:23,480 --> 00:26:29,480
point because reverse engineering can even happen, you know, just by way of selection bias. So I mean,

321
00:26:29,480 --> 00:26:33,960
researchers are out there, they're fooling around with different neural network structures, maybe

322
00:26:33,960 --> 00:26:39,160
I'll add like a you here or some horseshoe over there. And lo and behold, suddenly, it works

323
00:26:39,160 --> 00:26:44,440
really well on the concept of inside out. And I'm going to claim this is machine learning,

324
00:26:44,440 --> 00:26:49,320
even though it was actually human engineering that sort of put that structure into the network.

325
00:26:49,320 --> 00:26:54,440
So in the long term, you know, how do we, how do we balance that? Or how do we avoid it? Or how do

326
00:26:54,440 --> 00:27:00,520
we test for machine induced, you know, prior knowledge versus actual machine learning?

327
00:27:01,480 --> 00:27:10,440
Yeah, no, I understand it's a hard problem. And I think, you know, the goal with this concept arc

328
00:27:11,720 --> 00:27:17,560
benchmark wasn't to sort of supplant arc in any way, it was really meant to be complementary.

329
00:27:18,200 --> 00:27:27,480
And it was meant to be kind of a stepping stone to the much larger and more difficult arc set.

330
00:27:28,040 --> 00:27:32,760
Because I think, you know, even if I tell you all of these problems have to do with

331
00:27:32,760 --> 00:27:41,240
the concept of inside versus outside, you would still have to have a good grasp of those concepts

332
00:27:41,240 --> 00:27:48,600
in order to solve these problems. And I'm not sure that you could sort of engineer something

333
00:27:49,160 --> 00:27:57,320
that would solve those cons problems of that concept in general, without having a more,

334
00:27:57,320 --> 00:28:01,480
you know, really a general understanding in some sense of those that concept.

335
00:28:02,120 --> 00:28:10,520
But to Keith's your point, I think having a static benchmark is a problem, sort of putting out a

336
00:28:10,520 --> 00:28:17,640
benchmark that everybody can kind of try and optimize their program to solve. We've seen

337
00:28:17,640 --> 00:28:27,800
that over and over again. That ends up being sort of a way that people end up reverse engineering

338
00:28:28,680 --> 00:28:36,120
to a particular task rather than to a more general set of conceptual understanding. So

339
00:28:36,120 --> 00:28:42,360
I do think that we have to keep changing our benchmarks. We can't just say, okay, here's image

340
00:28:42,360 --> 00:28:49,160
net, go, you know, beat on that for the next 20 years until you've solved it.

341
00:28:49,720 --> 00:28:52,760
That's not going to yield general intelligence.

342
00:28:53,880 --> 00:28:58,520
Yeah, I think one of the issues we're talking about in general is extrapolation. So, you know,

343
00:28:58,520 --> 00:29:04,920
Sholey used extrapolation to talk about skill programs and being able to do things beyond

344
00:29:04,920 --> 00:29:11,000
your priors and experience. But with benchmarks, it's about human extrapolation. So I think part

345
00:29:11,080 --> 00:29:15,560
of the problem with the risk debate, by the way, why everyone's so suddenly worried about risk is

346
00:29:15,560 --> 00:29:21,560
because of this benchmark problem. And that's because we see that humans who can do A can do

347
00:29:21,560 --> 00:29:27,800
B. And now we see machines that can do A. And we have all of these built-in assumptions in

348
00:29:27,800 --> 00:29:32,360
benchmarks. And we don't really realize that we're talking about machines now. We're not talking

349
00:29:32,360 --> 00:29:38,440
about computers anymore. And I think it's causing a real problem. I don't want to be hyperbolic here,

350
00:29:38,440 --> 00:29:44,280
but it feels like there's this massive delusion taking over the entire machine learning community.

351
00:29:44,280 --> 00:29:49,560
And we're seriously talking about AI risk. And I think it all comes down to these

352
00:29:49,560 --> 00:29:57,160
benchmarks fundamentally. Yeah, I do think all of our benchmarks have, as you say,

353
00:29:57,160 --> 00:30:05,320
have this problem of that they have assumptions built in that if a human could do this, that

354
00:30:05,320 --> 00:30:09,720
then the machine must, if the machine does it, it has the same kind of

355
00:30:12,040 --> 00:30:17,880
generalization capacity as a human who could solve that problem. This goes back all the way to say

356
00:30:19,640 --> 00:30:27,480
chess as a benchmark. So people used to think that if, because if a human can play chess at a

357
00:30:27,480 --> 00:30:34,280
grandmaster level, that means they must be super intelligent in other ways, that if a machine

358
00:30:34,280 --> 00:30:40,920
could play chess at that level, it would also be super intelligent like a human. Herbert Simon

359
00:30:40,920 --> 00:30:48,120
even said that explicitly. But then we saw that chess actually could be conquered by very

360
00:30:48,120 --> 00:30:57,800
unintelligent brute force search that didn't generalize in any way. So I think this is an issue

361
00:30:57,800 --> 00:31:05,480
today with large language models. They can do things like pass the bar exam and pass other

362
00:31:06,120 --> 00:31:14,360
standardized human tests of skill or intelligence. But what does that mean? It doesn't necessarily

363
00:31:14,360 --> 00:31:18,120
mean the same thing for a machine as it does for a human for many different reasons.

364
00:31:19,320 --> 00:31:23,800
Yeah, I guess it's a similar thing with the McCorduck effect that we have relative pointers

365
00:31:23,800 --> 00:31:29,240
to what we think of as being intelligence. We just point to something and then when that thing

366
00:31:30,120 --> 00:31:37,480
becomes easy, then we need to kind of move the pointer. Yeah, I think it also feeds into, as

367
00:31:37,480 --> 00:31:43,560
Tim was saying, I think it heightens the fear of existential risk because of this, this concept

368
00:31:43,560 --> 00:31:50,120
that we have of intelligence always wins, which even among humans is, is a flawed concept, right?

369
00:31:50,120 --> 00:31:54,360
I mean, you know, many nerds who grew up through elementary school can tell you like

370
00:31:54,360 --> 00:32:00,280
intelligence doesn't always win, right? Like sometimes it's numbers or brute force or

371
00:32:00,280 --> 00:32:04,520
whatever else kind of kind of wins. And they assume like, well, if we were to have this

372
00:32:04,520 --> 00:32:10,600
purified intelligence that was super intelligence, it would be as if a human brain were super

373
00:32:10,600 --> 00:32:15,880
intelligent and they'd be able to do everything a human being could do and hurt other people and

374
00:32:15,880 --> 00:32:21,640
conquer the world and fight wars. And that again, is this anthropomorphic projection, right?

375
00:32:22,440 --> 00:32:30,280
Yeah, I mean, right. So, and it's this notion that intelligence is this thing that you can just

376
00:32:30,280 --> 00:32:37,000
have more and more of. Forever. Forever. Or so far that it's just beyond any, you know,

377
00:32:37,000 --> 00:32:42,040
it's almost magical, right? And it's capable. Right. And it's not, you know, a different

378
00:32:42,040 --> 00:32:49,240
view of intelligence is that it's a collection of adaptations to specific problems for a particular

379
00:32:49,240 --> 00:33:00,200
kind of organism in an environment. And it's not the sort of an open-ended, pure domain

380
00:33:00,200 --> 00:33:06,920
independent thing. So, I think this is why, you know, you see a lot of discussion of super

381
00:33:07,000 --> 00:33:16,680
intelligence, AGI, you know, AI risk in among computer scientists, but you don't see a lot of it

382
00:33:17,240 --> 00:33:24,440
discussed among like psychologists or animal intelligence people or other cognitive scientists.

383
00:33:25,400 --> 00:33:28,120
Because that's not the way that they understand intelligence.

384
00:33:29,240 --> 00:33:33,160
I would love to explore more about that because, I mean, only yesterday when we were talking about

385
00:33:33,240 --> 00:33:39,560
an activism, we're also talking about Gibson's ecological psychology. And even Elizabeth Spelke,

386
00:33:39,560 --> 00:33:44,520
I mean, this kind of cognitive psychology view is very related to nativism. It's this idea that we

387
00:33:44,520 --> 00:33:50,200
have these fundamental cognitive primitives and intelligence in some sense is just traversing

388
00:33:50,200 --> 00:33:56,120
or recomposing this library of cognitive modules that we have. And those modules are very physically

389
00:33:56,120 --> 00:34:01,560
situated, you know, they tell you something about the environment that you're in. Which means that

390
00:34:01,640 --> 00:34:07,000
intelligence is just very gnarly and it's very kind of coupled to the environment we're in. It

391
00:34:07,000 --> 00:34:11,560
can't really be magically abstracted in a computer with infinite scale.

392
00:34:12,760 --> 00:34:18,360
Yeah, I think that's right. That's, you know, people have different views about the nativism,

393
00:34:19,880 --> 00:34:26,040
empiricism, debate. And there's whole different schools and cognitive science about like how

394
00:34:26,760 --> 00:34:32,840
how much is learned, how much is evolutionarily built in and all of that. But I think most people

395
00:34:33,400 --> 00:34:43,800
in the field would agree with what you said that intelligence is very gnarly. It is situated, it

396
00:34:43,800 --> 00:34:54,280
is specific to particular domains of concern to a particular organism, and that it's not easily

397
00:34:54,280 --> 00:34:57,960
abstractable. You know, that back in the early days of AI we had

398
00:35:00,520 --> 00:35:06,360
Newell and Simon, two of the pioneers of AI who had this thing called the physical symbol system

399
00:35:06,360 --> 00:35:13,800
hypothesis, which was that basically you could sift off intelligence from any material substrate

400
00:35:13,800 --> 00:35:18,840
like the brain and put it in some other material substrate like a computer. They were thinking

401
00:35:18,840 --> 00:35:27,160
about symbols, but nowadays people have the same kind of view with neural nets or

402
00:35:28,600 --> 00:35:34,920
transformers or whatever, that you can take human intelligence that's very situated and

403
00:35:35,800 --> 00:35:41,720
tied to the environment and sort of sift off the pure part and leave all of that bodily stuff

404
00:35:42,360 --> 00:35:46,520
and you can get something like superintelligence. And I don't think most people in cognitive

405
00:35:46,520 --> 00:35:52,040
science would agree with that. Well, on the other hand though, I think, and I'd be curious to get

406
00:35:52,040 --> 00:35:57,480
your take on this, is one direction that that comes from is for those of us, and I include

407
00:35:57,480 --> 00:36:03,480
myself in this camp tentatively, that at the end of the day what the brain does is some form of

408
00:36:03,480 --> 00:36:08,120
computation. You know, like absence, the proof that there's such a thing as hypercomputation,

409
00:36:08,120 --> 00:36:14,040
like our brain, all of its calculations could be embodied in a large enough

410
00:36:14,600 --> 00:36:19,000
you know, Turing machine and a large enough computer of some kind. And therefore, everything

411
00:36:19,000 --> 00:36:27,160
that we do, including our intelligent activities, could be coded somehow or another into a Turing

412
00:36:27,160 --> 00:36:31,800
equivalent system. And for the record, I don't believe neural networks are. I've said this like

413
00:36:31,800 --> 00:36:35,960
multiple times, at least in their current manifestations, they're not, they're just a

414
00:36:35,960 --> 00:36:40,120
feed forward, you know, thing at the end of the day. But if you actually had a computer, you could

415
00:36:40,120 --> 00:36:47,160
have human symbolic intelligence encoded. Like, where do you stand on that, on that debate, if you

416
00:36:47,160 --> 00:36:56,200
will? Yeah, I have nothing against the idea that the brain does computations. I think that's,

417
00:36:57,080 --> 00:37:05,480
that's, you know, one possible way to look at it. And that those kinds of computations could be

418
00:37:05,480 --> 00:37:10,600
implemented in another kind of computer. But the brain is a very special kind of

419
00:37:10,600 --> 00:37:15,480
sort of biological computer that's been evolved to do specific things. And one of the main things

420
00:37:15,480 --> 00:37:21,160
the brain has been evolved to do is control the body, and in particular kinds of environments.

421
00:37:21,800 --> 00:37:28,680
And so I think the brain is doing computations, but it's doing very, very highly evolved, very

422
00:37:28,680 --> 00:37:39,480
domain specific computations that perhaps don't necessarily make sense without having a body.

423
00:37:41,640 --> 00:37:52,600
Now, that's debatable. But it does seem like a lot of the way that we reason is by reference to our own

424
00:37:52,760 --> 00:37:57,400
sort of episodic experience in the world.

425
00:37:59,160 --> 00:38:04,760
Or at least to the capabilities that have been built into us, you know, like visual, using our

426
00:38:04,760 --> 00:38:11,240
visual cortex to imagine cubes and steers and whatever else we need to solve a physics problem

427
00:38:11,240 --> 00:38:17,480
or a geometric problem. Sure, sure. Yeah, so I'm fine with saying the brain is a computer of a certain

428
00:38:17,480 --> 00:38:28,840
kind, but that's not to say that it's going to be, you can just kind of lift off the computations

429
00:38:31,080 --> 00:38:37,640
and then put them in a different substrate and kind of get everything that's human like,

430
00:38:37,640 --> 00:38:42,520
because I'm not sure that those computations are going to make sense in the absence of the rest

431
00:38:43,080 --> 00:38:49,720
of the organism. Yeah, there was something that always confused me about the autopoietic

432
00:38:49,720 --> 00:38:56,280
inactivists, because of course they as they issue representationalism and information

433
00:38:56,280 --> 00:39:01,640
processing, but they also issue computationalism in general. And as Keith was just saying, I don't

434
00:39:01,640 --> 00:39:06,280
even if cognition is externalized, I don't see any reason why in principle, you couldn't just

435
00:39:06,280 --> 00:39:11,720
compute the entire system and and recreate the computation. I just wanted to close the loop on

436
00:39:11,720 --> 00:39:18,600
the ARC challenge stuff though. So you said that the winning solutions to Francois' challenge on

437
00:39:18,600 --> 00:39:23,320
Kaggle, they were quite simplistic in a way. They were like a genetic search over lots of

438
00:39:23,320 --> 00:39:28,600
primitive kind of functions. And even the winner said that they didn't feel it was a satisfying

439
00:39:28,600 --> 00:39:33,480
solution, which was interesting. And then you tried it on GPT4. And I think you said you got

440
00:39:33,480 --> 00:39:39,960
around 30%. There's now a deep mind paper out very recently, which just basically turned it all into

441
00:39:40,040 --> 00:39:45,720
a character set with a random mapping, put it into GPT4, I think got nearly 60%. Even

442
00:39:45,720 --> 00:39:51,560
even somewhat invariant to the translation between the character set mapping. Some folks on our

443
00:39:51,560 --> 00:39:55,800
Discord forum tried to reproduce it and couldn't. That's the problem with GPT4. You can never

444
00:39:55,800 --> 00:40:02,600
reproduce anything. But I was just wondering, would you consider that to be an elegant solution?

445
00:40:02,600 --> 00:40:06,120
It's not really much better than searching over a DSL, is it?

446
00:40:06,840 --> 00:40:09,240
By that, you mean giving it to GPT4?

447
00:40:10,280 --> 00:40:15,400
Well, I mean, it's quite an interesting thing, isn't it? If there's the McCorduck effect,

448
00:40:15,400 --> 00:40:21,320
and even before you get to a solution, what would a good AI solution look like? I mean,

449
00:40:21,320 --> 00:40:25,800
what would someone have to create for you to say, oh, that's a really cool AI solution?

450
00:40:25,880 --> 00:40:36,360
Well, if you had a program that really could solve these tasks in a general way,

451
00:40:36,360 --> 00:40:43,000
that would, however it worked, it would be a good AI solution. I don't necessarily think we have to

452
00:40:43,000 --> 00:40:49,880
have something like the way people do it. Well, let me see if I can guess, though,

453
00:40:49,880 --> 00:40:55,480
maybe an extension to what you said. It's in line with your argument that the benchmarks

454
00:40:55,480 --> 00:41:00,520
have to evolve. Because I think that these benchmarks really is just first pass or low

455
00:41:00,520 --> 00:41:05,320
pass filters. It's like they weed out the junk. It's like, well, if you can't pass the art challenge,

456
00:41:05,320 --> 00:41:09,240
I'm not going to bother with you. If you pass the art challenge, now we have to look further,

457
00:41:09,240 --> 00:41:15,000
right? Which is like, okay, so it's been able to generalize along these 19 concepts that we've

458
00:41:15,000 --> 00:41:22,520
defined in concept art with little pixel grids. What about if we give it full frame pictures

459
00:41:22,520 --> 00:41:28,280
or video or something? Is it able to generalize there? No, okay, it failed. Why did it fail?

460
00:41:28,280 --> 00:41:32,360
Well, now we need to do some more engineering. It's going to be this kind of never ending sort

461
00:41:32,360 --> 00:41:38,280
of iterative process. So I would say if something passes arc or concept arc, then it's worthy of

462
00:41:38,280 --> 00:41:46,280
further study. Sure. Yeah, I agree. I mean, one question is that arcs are very idealized kind

463
00:41:46,360 --> 00:41:53,880
of micro world type domain. So does it capture what's interesting about the real world

464
00:41:54,600 --> 00:42:01,000
in terms of abstraction? To some extent, yes, probably, and to some extent, probably no.

465
00:42:01,000 --> 00:42:08,680
So you're right. Solving arc doesn't mean we're at AGI, if you want to talk about that.

466
00:42:08,680 --> 00:42:15,240
It's like in chess, what you brought up earlier. If you took whatever the current best,

467
00:42:15,240 --> 00:42:19,960
let's say LC zero or something like that, and it's been trained on standard chess,

468
00:42:19,960 --> 00:42:25,400
and then you have a go play chess 960, formerly called Fisher random, where you just random,

469
00:42:25,400 --> 00:42:30,440
it's going to suck like humans are going to destroy it, right? Because humans have learned

470
00:42:30,440 --> 00:42:37,160
a more generalized and by the way, that also destroys human beings who rely on memory and

471
00:42:37,160 --> 00:42:42,120
just sort of like the memorized positions that haven't learned, let's say the skill

472
00:42:42,120 --> 00:42:46,440
of playing chess, right? And so this is the type of thing that's going to happen, right?

473
00:42:46,440 --> 00:42:50,920
It's like you say, when you take this intelligence and try to apply it to a different context,

474
00:42:51,560 --> 00:42:55,320
that's when the rubber meets the road as to whether or not you really learned

475
00:42:55,320 --> 00:43:00,520
the concepts, right? Yeah, no, definitely. I agree. And I don't think like our concept arc

476
00:43:00,520 --> 00:43:07,080
wasn't meant to be like a test of AGI in any sense. It was meant to be kind of a stepping stone to

477
00:43:07,080 --> 00:43:14,600
getting to abilities for abstraction. And clearly, if some program was able to solve all of the

478
00:43:14,600 --> 00:43:21,080
problems in that domain, and we'd have to then test further, we'd have to have it be able to

479
00:43:21,080 --> 00:43:26,760
extrapolate to a new kind of domain that tested the same kinds of concepts. So you're right,

480
00:43:26,760 --> 00:43:32,600
there's no end in some sense. But at some point, I guess, and I don't know when that point is,

481
00:43:32,600 --> 00:43:37,320
we have to say, well, this thing seems to be understanding this concept.

482
00:43:39,960 --> 00:43:43,720
That's the wonderful continuum, though, because you said earlier, there's something deeply

483
00:43:43,720 --> 00:43:50,040
unsatisfying about chess brute forcing everything. And when we apply Francois' measure of intelligence,

484
00:43:50,040 --> 00:43:56,040
we don't think of that as intelligent because it's just brute force experience. And then we

485
00:43:56,040 --> 00:44:00,040
find something which is a little bit more efficient. So it's something which appears to work. But

486
00:44:00,760 --> 00:44:05,400
now, another interesting thing is when you talk about concepts, you had this beautiful article

487
00:44:05,400 --> 00:44:10,920
out earlier that she had talking about, on top of, she's on top of the world. And what would

488
00:44:10,920 --> 00:44:18,200
Dali draw? It would draw a globe with someone dancing on top of it, or I'm on the TV. What

489
00:44:18,200 --> 00:44:23,560
does that mean? It should mean that I'm actually being rendered on the TV. Now, it's kind of like

490
00:44:23,560 --> 00:44:28,840
what we were saying with goals, isn't it? Because this skill program, someone just goes on Kaggle

491
00:44:28,840 --> 00:44:34,200
and they gives you this program and it seems to work. But it's horribly complicated. And how do

492
00:44:34,200 --> 00:44:39,880
you know that the internal representations are in any way related to these abstractions? And do

493
00:44:39,880 --> 00:44:45,400
you think that the abstractions as well are somehow universal in the same way Spelki would say that

494
00:44:45,400 --> 00:44:52,920
the cognitive priors are? Yeah, I think it's something we can't say. And we don't know with

495
00:44:53,000 --> 00:44:59,720
humans. And we don't know with machines, because both of these are very complex systems that are

496
00:44:59,720 --> 00:45:08,120
hard to kind of pull apart. What are the internal representations? So in most cases, we have to

497
00:45:08,120 --> 00:45:17,880
rely on behavior, which is very noisy. It can be misleading. And it turns out that humans

498
00:45:18,680 --> 00:45:28,120
often are not, if you give them a problem, like a reasoning problem, in a familiar domain,

499
00:45:28,840 --> 00:45:34,040
they're much better at doing that problem as doing the exact same reasoning kind of abstract

500
00:45:34,040 --> 00:45:41,720
reasoning task in an unfamiliar domain. And I think that's something that people have shown

501
00:45:41,720 --> 00:45:50,600
is also true of large language models, because they've learned from human language and have

502
00:45:50,600 --> 00:45:55,960
incorporated sort of the statistics of some of the statistics of human experience that they're

503
00:45:55,960 --> 00:46:01,400
much better on familiar domains than on non-familiar domains. But the one thing that humans can do

504
00:46:01,400 --> 00:46:07,480
is often they can kind of transcend that and learn how to reason much more abstractly,

505
00:46:07,800 --> 00:46:14,760
which I don't know if we will get to that point with language models yet. So there's a wonderful

506
00:46:14,760 --> 00:46:24,760
paper that just came out from a group at MIT and some other places called, I can't remember

507
00:46:24,760 --> 00:46:31,720
what it was called, it was something like reasoning versus reciting. And what they do is

508
00:46:32,280 --> 00:46:39,480
they talk about this notion of a counterfactual task, which is if you can do one task, like

509
00:46:39,480 --> 00:46:44,840
addition and base 10, and you really understand that notion of addition, you should be able to do

510
00:46:44,840 --> 00:46:51,160
addition and base eight. And so, but you haven't had as much experience as like for a language model,

511
00:46:51,160 --> 00:46:59,000
it's not almost all of the training data has to do with base 10. So, but can, so they tested,

512
00:46:59,000 --> 00:47:01,960
they did a whole bunch of these so-called counterfactual tasks

513
00:47:03,240 --> 00:47:09,080
and showed that GPT-4 is really good at the original task, but not so good at the counterfactual task.

514
00:47:10,280 --> 00:47:14,920
So it's not, in some sense, it is relying on sort of patterns in its training data rather than

515
00:47:16,200 --> 00:47:18,360
genuine abstraction.

516
00:47:19,880 --> 00:47:21,560
It's a stochastic parrot, right?

517
00:47:22,120 --> 00:47:25,640
Well, you know, it could be argued that humans do that a lot too.

518
00:47:26,600 --> 00:47:31,320
I don't know if you called a stochastic parrot, but it's more like a pattern matcher.

519
00:47:32,520 --> 00:47:39,560
And it's not, it's not reasoning about the things in the sense that we think of reasoning,

520
00:47:39,560 --> 00:47:46,200
you know, as sort of domain independent ability. It's very domain dependent.

521
00:47:49,160 --> 00:47:52,280
Yeah, so the difference is that I guess the difference I would say is that humans,

522
00:47:52,520 --> 00:48:03,080
it can kind of overcome that domain dependency in some cases and actually get to the true

523
00:48:03,080 --> 00:48:06,680
abstraction, but I don't know that language models can.

524
00:48:08,360 --> 00:48:13,880
Yeah, I mean, there's a couple of things here. So first of all, these language models fail at

525
00:48:13,880 --> 00:48:21,080
things which four-year-old children can do. And they can pass the bar exam, but as you've said

526
00:48:21,080 --> 00:48:24,600
previously, you wouldn't want one of these things to actually go and practice more.

527
00:48:24,600 --> 00:48:31,560
My God, could you imagine the thought? And there was this Sparks of AGI paper where they gave this,

528
00:48:31,560 --> 00:48:35,160
I mean, maybe you could recite this better than me, but there was the thing about the

529
00:48:35,160 --> 00:48:40,440
book Nine Eggs, a laptop, a bottle, and a nail. Can you balance it in a stable manner?

530
00:48:40,440 --> 00:48:45,960
And this comes back to the experiment design because, my God, in any other discipline of science,

531
00:48:45,960 --> 00:48:50,760
they would just tear this apart. They would say, well, that's not very robust. I mean,

532
00:48:50,760 --> 00:48:58,040
you came up with an example with a pudding, a marshmallow, a toothpick. How would it balance it?

533
00:49:00,040 --> 00:49:03,800
Yeah, did it not balance the full glass of water on top of the marshmallow?

534
00:49:04,840 --> 00:49:09,960
Well, it stuck the toothpick into the marshmallow and then that's not exactly what we had in mind.

535
00:49:10,520 --> 00:49:14,440
No, and in fact, the Sparks of AGI paper, they explicitly said,

536
00:49:15,400 --> 00:49:19,320
we're doing anthropology, not cognitive science.

537
00:49:20,600 --> 00:49:24,520
Well, that's not the way it was interpreted. Unfortunately, there are YouTube channels

538
00:49:24,520 --> 00:49:31,160
now dedicated to educating people on AI and they're taking this as gospel. I mean, what's going on?

539
00:49:31,880 --> 00:49:40,120
I think there's just not as much of a focus on sort of scientific method in this field as there

540
00:49:40,120 --> 00:49:49,960
should be. And I think in science, if you're looking at a phenomenon and you're trying to

541
00:49:49,960 --> 00:49:57,240
replicate it, if it only replicates half the time, that's not a replication. That's not a

542
00:49:57,240 --> 00:50:03,720
robust replication. Whereas for language models, people are saying, well, if it can do this task

543
00:50:03,720 --> 00:50:11,720
once in one particular circumstances, then it probably has this more general capability.

544
00:50:12,280 --> 00:50:18,440
So if it can do this stacking problem once, then wow, it has physical common sense.

545
00:50:20,760 --> 00:50:28,520
And people with my marshmallow example, people, of course, jumped on it and said, wait,

546
00:50:28,520 --> 00:50:33,400
if you prompt it in a certain way and you do all this prompt engineering,

547
00:50:33,400 --> 00:50:39,000
human engineering, it does it right. And then like, well, that's not the point.

548
00:50:39,720 --> 00:50:44,760
The point is not any particular example. The point is figuring out how to test things

549
00:50:44,760 --> 00:50:51,400
so that you actually have some kind of robust ability for replicating a capability,

550
00:50:52,680 --> 00:50:57,080
which we haven't seen with experiments on language models very much. I mean, people

551
00:50:57,080 --> 00:51:02,840
are starting to do this. People are starting to do this kind of more scientifically grounded,

552
00:51:02,840 --> 00:51:10,360
experimental method on language models, but it's still not very, there's not very much of it.

553
00:51:11,240 --> 00:51:16,200
So you might appreciate a phrase I recently coined because it covers this leakage too,

554
00:51:16,200 --> 00:51:22,120
of like sort of leakage of human knowledge, which is if you can't find the priors, look in the mirror.

555
00:51:23,080 --> 00:51:27,320
It's like, we have to learn how to do experimental science and computer science,

556
00:51:27,320 --> 00:51:32,280
and you've got to guard against this type of leak at Drillian, human engineering,

557
00:51:32,280 --> 00:51:37,960
and over-involved and whatever. And this is why I really want to collaborate with people in

558
00:51:37,960 --> 00:51:44,680
developmental psychology, with people in animal cognition who face this kind of issue all the

559
00:51:44,680 --> 00:51:55,800
time. And one example was, I got from a developmental psychologist was that sometimes

560
00:51:55,800 --> 00:52:05,480
like a three-year-old can tell you something like four plus three is seven, but if you say,

561
00:52:05,480 --> 00:52:10,040
if you give them a bunch of marbles and say, pick out four of them, they can't do it.

562
00:52:10,360 --> 00:52:17,480
So there you say, okay, that this kid doesn't understand the concept of four,

563
00:52:17,480 --> 00:52:23,640
they're kind of just reciting something that they've heard. And this is the kind of experiments

564
00:52:23,640 --> 00:52:30,040
that people in developmental psychology do all the time to really tease out what the system,

565
00:52:30,040 --> 00:52:36,680
what babies and children know and what they can do. And it's not an easy thing to do in

566
00:52:37,400 --> 00:52:43,800
this kind of experiment. The problem with that is it's extremely complex and requires so much

567
00:52:43,800 --> 00:52:48,600
domain knowledge. So it takes a very long time, because I think there was another article that

568
00:52:48,600 --> 00:52:54,440
spoke about how we study rats. And those folks in different disciplines, they're really,

569
00:52:54,440 --> 00:53:00,120
really good experimental design, and they have experts who kind of create very, very clear

570
00:53:00,120 --> 00:53:06,040
criteria for measuring this behavior. And with AI, everything's going up on archive,

571
00:53:06,040 --> 00:53:09,960
and everything's going a million miles an hour. And by the time you actually design

572
00:53:09,960 --> 00:53:13,720
a systematic rigorous study for the first thing, there's already another paper coming out,

573
00:53:13,720 --> 00:53:17,160
which is claiming to do it differently. So we just can't keep up. It's just,

574
00:53:17,160 --> 00:53:22,440
it's an absolute nightmare. Absolutely. Yeah. Agreed.

575
00:53:24,280 --> 00:53:27,880
I want to just, so I'll quickly touch on one more thing. And I know Keith wants to go into

576
00:53:27,880 --> 00:53:32,520
complexity. But yeah, so the information leakage is a problem. The brittleness is a problem. I do

577
00:53:32,520 --> 00:53:39,080
think of these GPT models a bit like a database. And so anything that requires physical grounding,

578
00:53:39,080 --> 00:53:42,680
of course, doesn't work very well. Some things work surprisingly well, like, you know,

579
00:53:42,680 --> 00:53:46,760
programming, because programming is mostly in the internet, it still has all sorts of

580
00:53:46,760 --> 00:53:53,320
failure modes, and it's not very reliable, but it's surprisingly reliable. But you put a paper

581
00:53:53,320 --> 00:53:57,160
out with Tanenbaum and a whole bunch of other people. And you actually said, well, if you want

582
00:53:57,160 --> 00:54:02,280
some policy advice, if you really want to think about how we can improve the situation, you said,

583
00:54:02,360 --> 00:54:08,840
aggregating benchmarks and also giving instance level failure modes can actually help us understand

584
00:54:08,840 --> 00:54:15,080
why things went wrong or, you know, why things gave us the right answer for the wrong reasons.

585
00:54:15,720 --> 00:54:20,440
And there were all sorts of limiting factors, you said. You know, we have this kind of

586
00:54:20,440 --> 00:54:25,880
censorship by concision. You're only allowed to have seven pages in your conference workshop paper,

587
00:54:25,880 --> 00:54:29,640
and there's no policy about this. So can you give us a heads up on that?

588
00:54:30,280 --> 00:54:36,600
Yeah, I mean, you know, traditionally in machine learning, people use accuracy and similar kinds

589
00:54:36,600 --> 00:54:44,520
of aggregate measures to report their results. And, you know, if someone tells you that the accuracy

590
00:54:44,520 --> 00:54:53,000
was, you know, 78%, what does that tell you exactly? I think, you know, this gets back to the idea of

591
00:54:53,000 --> 00:54:57,640
scientific method. You know, in science, the most interesting things are the failures.

592
00:54:58,600 --> 00:55:01,800
And those are the things you really have to focus on. It's like, why did it fail?

593
00:55:02,440 --> 00:55:08,680
And that's what we need to know to understand machine learning systems. So the most simple

594
00:55:09,640 --> 00:55:12,840
kind of reporting would be just to report for every instance in your

595
00:55:13,720 --> 00:55:20,520
benchmark, your data set. How did the system do? What was its answer? And that's not, you know,

596
00:55:21,320 --> 00:55:30,360
it doesn't seem like a very big ask, but it would be very useful. And we now have in conferences,

597
00:55:31,080 --> 00:55:36,440
you're allowed to have some kind of supplementary material online. So you could have this available.

598
00:55:37,000 --> 00:55:45,320
And we did this for our concept arc paper. We showed for every instance, like what humans did,

599
00:55:45,320 --> 00:55:52,120
what machines did, we tried to analyze the errors of the system. And I think this these kinds of

600
00:55:52,120 --> 00:55:57,320
reporting will be will give us a lot more insight into what these systems are doing and what their

601
00:55:57,320 --> 00:56:05,480
like real capabilities are. Yeah. And it's, and back to the difficulty that Tim mentioned earlier,

602
00:56:06,120 --> 00:56:10,840
totally agree. And this is work that has to be done. Like if, if we are going to build a science

603
00:56:10,840 --> 00:56:17,000
of machine cognition, you know, this work has to be done. Yeah, I think. And I just want to shout

604
00:56:17,000 --> 00:56:22,920
out to Ryan Bernal, who spearheaded that paper, because he really is the one pushing for all

605
00:56:22,920 --> 00:56:29,880
this. And I think it's fantastic. So just in the last few minutes, you know, since we have you,

606
00:56:30,680 --> 00:56:36,840
complexity and complexity theory is a topic I really love. I'm not an expert in it at all,

607
00:56:36,840 --> 00:56:41,480
but I like to think about I like to explore it. I'm just curious, you know, from your perspective,

608
00:56:41,480 --> 00:56:46,520
um, what are some of the most interesting things happening right now in complexity theory? And

609
00:56:46,520 --> 00:56:51,560
if I wanted to go learn a bit more and check out just some cool, you know, latest stuff, what should

610
00:56:51,560 --> 00:56:58,040
what should we go look at? So I think there's, you know, there's a lot of interesting stuff going on,

611
00:56:58,040 --> 00:57:03,800
obviously, and complex systems is a huge umbrella for a lot of research. But

612
00:57:04,760 --> 00:57:11,480
if you're interested in the one big topic that people look at is called scaling. And it's the

613
00:57:11,480 --> 00:57:18,120
question of like, what happens to a system as it gets bigger in some sense? So this started out

614
00:57:18,120 --> 00:57:27,880
with some work on the sort of energy use of systems like animals as they as their maths increases.

615
00:57:28,680 --> 00:57:35,320
And people discovered some really interesting scaling laws that were very non-intuitive and

616
00:57:35,320 --> 00:57:42,840
they were able to explain these laws using ideas like fractal fractals and the fractal structure

617
00:57:42,840 --> 00:57:49,640
of complex systems. But now, so this is all on like biological metabolism and things like that.

618
00:57:50,360 --> 00:57:57,240
But now a lot of people are extending that scaling work to cities. So asking what happens

619
00:57:57,880 --> 00:58:04,360
to cities when they increase in size, either in area or in population size. And

620
00:58:05,400 --> 00:58:09,640
there's all kinds of phenomena that you can see, like what's the rate of innovation

621
00:58:10,440 --> 00:58:17,880
measured by something like patents? And what's the rate of sort of energy usage by a city?

622
00:58:19,880 --> 00:58:26,280
And what's how do these things change? Even like the happiness of the people,

623
00:58:27,240 --> 00:58:31,880
you know, are people in New York happier than people in Santa Fe, which is a much smaller city?

624
00:58:34,520 --> 00:58:42,040
These things scale in really interesting ways. And it's opening up a lot of new ideas about how

625
00:58:42,760 --> 00:58:50,440
social systems work. And how... Is it a similar thing that you can't trust the benchmarks? Because

626
00:58:50,440 --> 00:58:55,400
how happy people are, might you look at the rate of antidepressant usage or something?

627
00:58:55,400 --> 00:58:59,880
Yeah. So you do have all these... Right. I don't know if that's exactly what they use, but

628
00:59:01,160 --> 00:59:05,160
you do have to look at ways to measure these things, which can be questioned.

629
00:59:06,840 --> 00:59:12,520
But there are a lot of really... And I think this whole science, the science of cities, is

630
00:59:13,400 --> 00:59:19,240
it's very preliminary. And there's a lot of ideas about how to measure these things, how to

631
00:59:19,480 --> 00:59:26,360
develop sort of analytical descriptions or laws that govern certain properties and how to

632
00:59:26,360 --> 00:59:32,760
interpret them. But there's just a lot of really interesting work in this. And it turns out that

633
00:59:32,760 --> 00:59:40,520
now that everybody has a cell phone, you can really do a lot of tracking. A lot of these quantities

634
00:59:40,520 --> 00:59:48,280
can be tracked by people's sort of their movement, their interaction with other people, and all these

635
00:59:48,840 --> 00:59:55,880
things that you can measure using cell phones. So that's very cool.

636
00:59:57,160 --> 01:00:03,000
That is... Yeah. Thank you. That sounds actually fascinating. And one reason why for me particularly

637
01:00:03,000 --> 01:00:10,120
is... Are you familiar with Asimov's Foundation series? Yeah. So you know, psycho history in

638
01:00:10,120 --> 01:00:16,760
there was the science... And it was almost like a thermodynamics of human behavior that was only

639
01:00:16,840 --> 01:00:21,560
applicable at kind of planet scale and beyond. So it's like these scaling laws. So this is maybe

640
01:00:21,560 --> 01:00:26,200
one step towards... Very similar, psycho history. Yeah. One step towards psycho history of Asimov's

641
01:00:26,200 --> 01:00:33,000
kind. Exactly. Yeah. Cool. And in closing, does that give you intuition on the scaling of intelligence?

642
01:00:34,760 --> 01:00:45,160
Well... That's a great question. And I think, you know, one question you can ask is like,

643
01:00:45,160 --> 01:00:49,480
there's individual intelligence and then there's collective intelligence.

644
01:00:50,680 --> 01:00:56,120
And how much of the intelligence that we have individually is actually grounded in a more

645
01:00:56,120 --> 01:01:03,080
collective intelligence? You know, there's many things that I don't know, like I don't understand

646
01:01:03,080 --> 01:01:09,960
quantum mechanics or something, but I know somebody who does. And therefore, I feel like it's understood.

647
01:01:10,360 --> 01:01:17,480
And a lot of our intelligence, I think, is sort of more social than we think.

648
01:01:19,240 --> 01:01:25,880
Oh, absolutely. And folks should definitely read Melanie's book. So your complexity book,

649
01:01:25,880 --> 01:01:30,040
we actually covered that quite a lot on our show on Emergence. It's absolutely wonderful. And of

650
01:01:30,040 --> 01:01:36,680
course, your book on AI is probably the best book on AI I've ever read. It's up there with

651
01:01:36,680 --> 01:01:41,560
Christopher Sommerfield's book. But anyway, Melanie, honestly, you are my hero. Thank you so

652
01:01:41,560 --> 01:01:45,960
much for coming on MLS2. I really appreciate it. Thanks so much for having me. I really enjoyed

653
01:01:45,960 --> 01:01:47,640
it. It's great talking to you.

