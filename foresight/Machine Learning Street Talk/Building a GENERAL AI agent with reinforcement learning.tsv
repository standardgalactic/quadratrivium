start	end	text
0	5880	Open-endedness is essentially, you know, we're studying systems that can generate their own data in an infinite capacity
5880	10520	And so it's systems that essentially if you run it for longer and longer they get more and more complex
10520	13760	They generate more and more quote-unquote interestingness or interesting data
14240	19040	And so if we can actually, you know, crack this nut of how do we actually come up with a
19560	22520	Self-improving system in the sense that it keeps generating interesting data
23040	28100	We can then use that data to train further train our models
28140	34220	But of course you get into this perpetual data machine type of idea where obviously, you know
34220	36220	There's how do you generate more data?
36380	40540	If you know the data is ultimately coming from a model that you probably trained on previous data
40540	42180	How do you get net new information from that?
42180	47420	Well, I think a lot of this is actually just resolved purely again going back to this idea of the reward function
47420	53580	Right or a preference function where there is outside information coming in through some sort of filtering criteria
53580	61820	For example human designers in the loop or designers designing some sort of preference model that could essentially automatically rate the kinds of automatic
62460	66980	Data that's being generated by these open-ended systems. What does waker stand for?
67060	72460	Right, so waker stands for a weighted acquisition of knowledge across environments for robustness
73420	75420	Fantastic, and what was the title of the paper?
76420	79420	Oh, right. Yeah reward free curricula. Oh girl. What was the title?
80420	85140	reward free curricula for training robust world models, that was it. Okay, so
86860	91460	Give us the elevator pitch. Yeah, totally. So basically like the overarching
92620	97620	Question that we're trying to answer with this paper is like how should we go about training like very general agents?
97820	103300	So in the context of the paper, we think of a general agent as being one that's able to perform a lot of different tasks
103380	107860	So we might think of these as different reward functions or for thinking of it from a reinforcement learning perspective
108100	110940	But also be able to perform those tasks in lots of different environments
110940	116860	So, you know, we don't want a robot to just be able to do you know pick up tasks do tasks in my like my kitchen
116860	118860	specifically we want the robot to be able to go into like arbitrary
119420	124320	Apartments and also be able to do those tasks in like arbitrary environments. And so we kind of thought about like, yeah
124340	127100	How do we want to create an agent that can do such a thing?
127100	132540	And we argue in the paper that a good way of doing it would be to have an agent that has a very general world model
132900	138740	So a world model meaning that it can predict the outcome of sequences of actions and predict what will happen if it does certain actions
138740	142740	And so we argue if we have a very general world model that can lead to a very general agent
142740	146460	That's able to perform, you know, a variety of tasks in different environments
146460	151620	And so then, you know, once we've established that we kind of ask the question of how do we get a very general world model?
151620	153860	And what does it mean to have a good world model that works?
154260	160140	While in a very general setting across different environments and different tasks like how do we define that and how should we gather data to do that?
161100	166700	Beautiful. So I really enjoyed reading the paper and it reminded me a lot of Kenneth Stanley's poet paper
166860	173220	So he was doing this thing called curriculum learning and it's really related to machine teaching as well
173220	178300	There's quite a few things in machine learning where you say well if we had a really principled way of
178700	183300	Selecting the best training data and presenting it to the learner in the best possible order
183340	190060	Could the learner be better and in that poet paper Stanley was kind of generating a diverse set of
190300	197420	Environments and like training a learner on those things and you're doing something very similar and you're using this mini max regret
197420	201620	Which is a concept from decision theory. Can you bring that in? Yeah, absolutely. So
202620	205260	So I guess we have this notion of like wanting to be
205740	208460	To perform well across a wide range of scenarios, right?
208500	213180	So scenarios in our context mean like different environments and different tasks and kind of like the most
213780	218820	Standard way of thinking about that, especially in reinforcement learning or a machine learning in general as you think about like the average performance
218820	223220	So so how do I optimize like the expected reward across all of these different scenarios?
223780	227100	and a lot of the work that that munchies done as well kind of argues that
227660	230900	Just just optimizing for expectation isn't necessarily the best
231700	235180	The best objective so, you know, we can imagine in the real world
235220	238620	We don't really know like the distribution over possible tasks or or anything
238620	242860	Well, you know in most situations, we don't know things like that and so maybe a better objective is to try and be
243220	247220	robust instead and robust basically we can think of that as meaning like we should do
247700	252220	Reasonably well in every situation we could be in and that that's kind of what a robust objective is
252940	256660	And one of the ways that you can define a robust objective is via mini max regret
256660	262300	And so regret means like suboptimality like how well that I do relative to the best I could have possibly done
262340	265220	So that means basically the same thing as it does in normal English
265620	270660	And so the mini max regret objective basically says across all possible situations. I want to try and do
271700	277020	Minimize the regret across all possible situations minimize the maximum regret I should say so that means in all possible situations
277020	280380	We should do almost as well as the best we could have possibly done
280900	284300	And I guess just to contrast this against the standard objective for robustness
284300	291300	So the more common objective for robustness at least traditionally is like a maximum performance. That means maximize the performance
291820	296900	While the environments like minimizing and choosing the most adversarial environment or the most adversarial scenario
297820	302860	But but the problem with kind of the maximum objective is that in some environments you just can't do anything
302860	305060	Let's say it's like some such situations is too hard
305060	310620	You're doomed and so if in some situations you're doomed and you always get like zero reward or negative infinity reward
310700	316420	That means there's no incentive to try and do better in any other environment because your maximum reward is always going to be zero
316420	322140	And so therefore I think like minty argues as well as Michael Dennis and a lot of these recent papers argue that mini max regret
322140	328100	So minimizing the maximum self-optimality is actually like a better objective for a general agent. That's robust fascinating. So
328900	332980	If I understand correctly is it a way of saying I want to have the best case
333540	335260	worst
335260	341140	Expected regret. Yes. So basically mini max regret is saying that if you assume that you know
341140	346420	The environment is adversarial to you in some way like when you're training or at inference time when you're actually
346980	348980	Testing your policy out in the real world
349460	356380	Mini max regret is saying the agent should behave the model should behave in a way that minimizes its worst case possible regret
356660	360700	Over all the possible conditions of the world that this adversary could choose
360900	364740	What's really interesting about this paper is we are talking about the reward free
365300	371420	Exploration phase and we're also talking about the domain of model based reinforcement learning as opposed to
371980	374660	You know, let's say value based reinforcement learning where
376220	379860	You get this entanglement, right? So the dynamics the model of the world
379860	383380	It's still in there, but it's kind of in meshed with this with this value model
383420	385900	Whereas in model based reinforcement learning in a principal way
385900	391220	We kind of separate out the parts so that we can do explicit planning and imagination and simulations and stuff like that
391300	394340	So we're very much in this model based domain, right? Yeah, absolutely
394340	400100	Yes, we focused on yeah model based reinforcement learning or some people like to call this like the world model setting more recently
400340	403700	But yeah, like you said we you know in typical like model free reinforcement learning
403780	406740	We we typically aim to learn a policy and a value function and yeah
406740	412900	As you said like that value function is kind of implicitly encoding the dynamics through the fact that we learn the value function using the bellman equation
412980	418900	So so the bellman equation kind of propagates the information between like transition and the environments through the value function
418900	421780	So so the value function will like implicitly have the dynamics in it
422580	426420	But in model based reinforcement learning we want to very explicitly model the dynamics of the environment
426820	430580	And so what I mean by that is we want to be able to take some previous sequence of observations
430820	433940	Perhaps those are images and then also condition on the next action
433940	438420	We want to take in the environment and then be able to predict the distribution over the next observation or states
438420	440740	We're very explicitly modeling the dynamics of the environment
441060	445380	Okay, now this is really interesting because you know people think about reinforcement learning and in reinforcement learning
445380	448740	You don't so much care about having a model of the world
449060	455220	You care about building trajectories that lead to some you know task or goal or whatever that you're interested in so like
455940	459860	I mean just just in broader terms. What what what do we get from explicitly modeling the world?
460260	463380	So there are there are a few arguments for why we would want to explicitly model the environment
463380	468740	So so one of which is um a lot of people would argue that you get better sample efficiency by modeling the environment
468980	472180	And the argument for this is you know the reward function might be quite sparse
472500	476900	And so if you're just relying on like the propagation of rewards backwards to try and learn the optimal behavior
477140	482820	That might not be as efficient as actually learning the dynamics because the dynamics can be learned from every single transition that you have
482900	485460	It's kind of like a standard supervised or unsupervised learning problem
485460	490900	So so you kind of have like a richer signal to learn from which might arguably lead to better sample efficiency
491460	492900	um, but I think like
492900	497060	More concrete arguments that I would argue for or that if you have a model of the environment
497140	502260	It's it's some kind of more general thing that you can then use to develop better decision making later on
502260	504260	So so if you just learn a value function
504900	510180	You're kind of only learning how to optimally do that specific reward function or optimize that specific reward function
510660	514820	Um, but if we have a model of the environment, we can kind of arbitrarily be given some task later down
514820	520340	Whether it be a reward function or a goal state or something like that and we can then plan to optimize that task later down the road
520660	522500	so I would think that um
522500	525940	You know, it's kind of a much more general way of having a powerful decision making agent
526260	530340	Rather than just specifying like one task and learning the optimal kind of policy for one task
530660	532660	and I guess another thing that I'll add to that is um
533460	537460	Rather than only learning like a feedforward policy like you wouldn't reinforcement learning
537540	539540	So something that maps directly to actions
539620	542580	The other thing that a world model allows you to do is also to do online planning
542660	546100	So you can imagine at test time we're trying to deploy it in the environment
546260	550340	But we can actually do a bit more further planning through the world model to then work out what the best action is
550740	553780	Rather than relying on just a neural network to immediately output an action
554180	557380	And there's kind of a lot of work showing that if you can do this like planning at test time
557700	561300	You can kind of get a lot of a better performance on a lot of environments, especially things that
561700	565460	That really rely on um search to do well things like go and like these kind of games
565460	568100	We do have to think explicitly ahead in the environment
568500	572580	And so I would think those are the main reasons you would want to consider um a learning a world model
572820	575700	And maybe a last point I'll just add is that I think this is kind of a um
576340	578420	Again, like unclear whether this is true necessarily
578420	583220	But but I think some people would argue that a world model will generalize better than learning a value function
583540	587380	So you can imagine like a world model is learning things like you know state transitions
587540	589940	So you can imagine if you if you're training on straight transitions
590020	593380	The model is kind of implicitly being forced to learn something like physics or something like that
593620	596500	And so if you're like very explicitly forcing the model to learn something like physics
596820	597620	You could argue, you know
597620	601300	We'll go to some new state and the rules of physics will still hold and therefore the world model will still be
601620	604820	Quite good at the new state potentially whereas if you learn a value function
605220	608580	I guess it's a little bit less clear as to whether you're putting a new situation
608740	612100	Will the same kind of structure of that value hold as it would a model
612340	615060	Anyway, so that was a bit of a long answer, but no, no, it's fascinating
615060	620020	I mean when I was reading the paper that one of the reads I got is um in machine learning
620020	625460	We are often overcoming the curse of sparsity. So of course like in trajectories and reinforcement learning that that's quite intuitive
625700	628740	But even in learning the world model itself the model
629300	634260	Just because of the way they're trained it tends to compress the world into small little motifs and
634900	640100	Actually, the world is quite complicated and we need to combine the motifs together in lots of interesting and rich ways
640500	645860	And by exploring through the world model, we're almost kind of like make it we're forcing it to make those connections
646020	648740	Yeah, and I think um, you know to follow up on mark's um
649540	650260	Mark's point
650260	654580	I think it's also interesting because especially in the waker paper, uh, the world model setting
654580	657060	We're looking at specifically reward free world models
657300	662660	And so essentially there's this explicit decision to separate separate out the two components of world model
662900	667380	Which is essentially the dynamics function, which tells you how things transition from state to state
667380	671460	How does a state transition state of the world transition to the next state of the world?
671700	676500	Given an action that the model or the agent is taking in that world and the reward that it receives
676580	679460	So the slider part the reward is defined by the reward function
679780	685940	And so, uh, you know, I think mark was uh to follow up on his point a lot of the benefits of the world model is
686260	688660	In this design arrangement is that you can
689300	693220	compositionally separate out this dynamics aspect from the reward aspect
693460	698900	So the general idea would be why shouldn't agent train in such a world model be able to generalize to a new setting?
699060	703860	Well, maybe if that setting shares a lot of the underlying dynamics in that version of the world
703940	708820	for example rules of physics and the agent has learned how to exploit those to accomplish, um
709460	712500	Navigation around that environment or reach different types of tasks
712900	714900	Achieve different kinds of tasks in that environment
715060	718260	Then you can um sort of superimpose a different reward function
718580	723700	That essentially defines a different task because the reward function defines what task success is
723940	729220	so you can essentially superimpose different tasks on top of that dynamics model and you would
729540	734980	You know, you could expect that the agent could learn more quickly because it's already mastered sort of the foundational skills of
735220	739460	navigating or manipulating different aspects of the dynamics of that world
739780	744020	We've been on a bit of a journey here. I think over the last few years in the literature of
744100	745220	um
745220	752820	We we want to have robust models and we're doing that by kind of perturbing and you know making a bunch of manipulations to the environment
753060	757300	And there there was this domain randomization and there's like unsupervised environment design
757300	760660	And of course your your iteration now is doing this in in the domain of
761380	766900	Reward-free exploration, but can you take us on on that journey sort of maybe starting with um domain randomization?
767140	772340	kind of just to uh elaborate on something that mark was previously talking about which is that the typical
772660	775620	you know standard setup in machine learning is to
776180	780500	Uh, essentially optimize a model's performance uh over a uniform distribution
780980	784340	Over the data points and so this is really just randomly sampling data points
784340	788340	And we try to minimize the loss over those data points for whatever objective
788420	792180	We're trying to minimize or maximize in reinforcement learning. Um
792740	795620	We want to train agents that can perform well in lots of different
796180	798180	versions of the environment and so
798580	800580	You can think of each environment
800580	804580	Almost as a bundle of data points, right? It's kind of the set of trajectories that the agent can
805860	811460	Can encounter within that version of the world and we essentially in reinforcement learning we want to learn to maximize
812100	816260	The reward of the agent uh in that set of trajectories
816260	818260	So we want to specifically start to
819060	825060	actively pursue those trajectories that give us the highest reward and we learn from the reward signal as the feedback signal for
825380	829940	Figuring out, you know, which actions and therefore which trajectories will lead to maximizing that reward
830340	837620	and so typically um when we operate in the multitask setting, uh, we essentially randomly sample different versions of the environment
837860	844100	And essentially have the agent try to maximize its performance its reward on that random sample of environments
844740	845780	uniformly
845780	848180	Sampled from, you know, the set of possible environments
848980	850980	And this is essentially
850980	858500	Causing the agent it'll cause the agent to learn a policy that's optimal for essentially uniform distribution over those environments
858900	865780	Um, but of course this is kind of a naive assumption because we essentially are assuming that every possible version of the environment is equally likely
866020	870900	Which is obviously not true because some versions of the world will not be as likely as other as others
871140	874740	Uh, for example, like if you walk outside the sky is usually blue and not green
874980	879220	And so, you know, when the sky is orange, maybe that happens if you're in california
879300	881460	There's a wildfire, but that's not usually the case
881780	886580	And so instead what we can do is we can turn to decision theory and think of
887220	890820	Sort of more sensible approaches to what it means to act optimally
891300	898420	When you're uncertain about uh, what state of the world the world will be in and so the thing that we focus on in this paper
899380	903140	Is this idea of minimax regret where it is this idea again of
903780	908340	Having the agent act in a way that essentially minimizes its worst case regret
909860	911860	In any possible, uh, state of the world
912100	918020	So largely, you know, this is a shift from randomly sample what it means in practice is you want to shift from randomly sampling
918420	924660	environments during training to essentially, uh, sampling environments that maximize the agent's regret
925220	931060	And what this means is you're now actively sampling for those environment settings where the agents, um,
932260	938980	Experiencing the most regret and here regret is defined just simply as what does the optimal agent do in that version of the environment?
939300	942260	And what did this current agent that's learning do in that environment?
942260	947380	And so there's this gap in performance and you want to actively find those environments where that gap is maximal
947780	953940	And if you view this as this adversarial game now between, you know, uh, an adversary like nature
953940	960980	That's choosing the environment and the agent that's learning to solve the environment. Um, you can think of the adversary as, you know, having a
961780	965220	Payoff function in that game or it's rewarded for the
965700	970260	Based on the regret that the agent experiences and the agent is trying to shrink that regret
970260	976100	So the agent you can think of as being rewarded for, you know, um, the the negative of that reward
976100	979780	So the agent's reward signal is you can think of as the negative of the regret
980180	984100	And so now you have the setting where you can essentially view this training process
984500	989060	this active sampling process as a two player zero sum game where the
989460	996340	Adversary is, you know, rewarded for the regret of the agent in each environment it chooses and the agent is rewarded based on the
997060	1003380	The agent receives the negative regret as its payoff. And so, um, we know that into player zero sum games
1003380	1007460	There's always a this there's always a solution called a Nash equilibrium
1007540	1010740	and so this is an idea in game theory where basically this is
1011140	1017620	um, a choice of behaviors on both parties or a choice of strategies on both parties in the game such that, um
1018500	1021620	No player can do better unless the other player changes their strategy
1021780	1025060	And so you can think of this as a situation where, you know, I'm not
1025620	1031460	Neither player is incentivized to deviate from their behavior. Uh, once they reach this choice of mutual strategies
1031940	1036980	And so we know that all two player zero sum games have a Nash equilibrium
1037380	1040820	A set of strategies between the two players and in this case
1041300	1043860	We know there's additional theorem called the mini max theorem
1044100	1049620	Which says that when in a two player zero sum game specifically two players and zero sum when, um,
1050020	1055860	You are at the Nash equilibrium setting then each player must be playing what's called the mini max
1056740	1061060	The mini max strategy, which means that each player is minimizing the maximum
1062420	1065540	Minimizing the maximum reward for the other player
1065860	1072420	And so here the reward again is the regret and therefore just based on this known, you know, theorem about two player zero sum games
1072660	1077140	We know that, um, the agent which is, you know, receiving the payoff of negative regret
1077220	1081700	It's the min player. It must be implementing the min and max regret strategy
1082020	1091540	And so this is how we essentially can shape the training process to essentially, um, arrive at an agent that performs mini max regret decision making
1091860	1096500	Rather than decision making that optimizes, um, just a uniform sample of environments
1096820	1100420	Okay, so kind of play back, um, some of those things as I understand it
1100740	1107460	So, um, essentially we we are we're building a model which will learn to select the environments where we perform badly on
1107780	1112980	And then we fine-tune on those environments because we're leaning into the gaps. We're saying where where do I perform badly?
1113140	1119380	Let's fine-tune on that and then you're saying that if we continue to do this as a kind of adversarial sampling game
1119620	1123220	That we will reach a Nash equilibrium. So it will converge in a good place
1123540	1126020	But help me understand that why would it
1126740	1132020	You know, it seems to me intuitively that it might be unstable or it might not quite why does it converge?
1132500	1134500	So there's no guarantees around convergence
1135140	1138820	And so I think this is an area where there's a lot of room for innovation
1139220	1145060	Uh around these methods a lot of this is um, this is more I would say like theoretical motivation around why we think
1145460	1152260	actively sampling environment settings based on, um, estimates of regret is a good idea and another point related to that
1152500	1158740	Around sort of this gap between the theory. I I just um explained and in practice is that
1159460	1161940	Regret itself is a pretty hard quantity to actually
1162500	1166900	Measure in practice because you know knowing regrets defined as what's optimal performance
1167780	1169700	minus my agents performance
1169700	1174500	So you kind of have to know what optimal performance is and in general you don't know the optimal behavior
1174500	1179940	Therefore you don't really know the optimal performance on any environment unless it's like a very toy setting and so
1180820	1183300	In practice, we also use approximations for the regret
1183940	1186900	in order to do this kind of active sampling and so
1187940	1190340	There's a lot of deviations between theory and practice
1191380	1192340	So
1192340	1196100	There's no guarantees, you know that different forms of gradient based optimization
1196660	1200180	For rl training would actually lead to converging to Nash equilibria
1200580	1206180	A lot of the theory is just stating that if you were to run the system the learning system for a long time if we make the assumption that
1207380	1209380	the optimization algorithm is
1210020	1213780	fairly good at producing, you know an improved response to the
1214500	1220980	Other player in this type of zero sum game you if you're assuming that if the successive sort of series of best responses
1221620	1224020	That the optimization algorithm is generating
1224900	1231300	Continues to improve over the previous ones you could make the assumption that maybe eventually it does get to that equilibrium
1231380	1234580	But there is no mathematical guarantee that this actually happens
1235380	1237380	what we want to do is
1237780	1240180	You know build this latent dynamics
1241140	1245620	You know a predictive model which is a simulacrum of what the idealized version is
1245940	1249620	But we don't have a way of directly computing the regrets. So we kind of perform
1250420	1253460	You know, we learn a proxy for that regret. How does that work?
1253700	1257140	So we think of regret in the following way. So so there's kind of this um
1257780	1264180	Old school result from like um mdp theory or maybe it's not that old but like 20 years ago or something like that called the simulation lemma
1264500	1266500	and that basically says that you know
1266740	1269700	If we let's assume for now that we we have like an optimal planner
1269780	1273860	So we can give our like model of the world to this optimal planner and end some reward function
1273940	1276260	Let's say later down the road we get given some reward function
1276740	1281300	And so we give the model and the reward function to our optimal planner and we assume that this planner can return
1281700	1283700	The optimal policy in our model
1284260	1286260	So we kind of have this, you know planning oracle
1286980	1288820	And if we assume that we can do that
1288820	1292180	Then we can think about the difference between like how good the policy would be from
1292740	1296660	Our planning oracle in the model versus the truly optimal policy in the real world
1297460	1301940	And so what the simulation lemma tells us is that you know the difference between these two policies
1301940	1304980	So the one found by acting optimally in the model versus the truly optimal one
1305460	1309060	Is bounded essentially by the error between the model and the real world
1309780	1313220	Under the distribution of states that the policy would generate
1313460	1318340	So so, you know, it only it only matters that we have low error where the policy would go essentially because you know
1318580	1321060	If there are some states that are just completely irrelevant what the policy is going to do
1321060	1323060	It's not really going to matter if the if the model is not
1323540	1324820	Accurate and there
1324820	1326820	So we kind of use this result to think about the regret
1326900	1329540	So that that gives us like, you know, if we have like one
1330340	1333700	One true mdp and one model of an mdp and one reward function
1334340	1336180	The simulation lemma can tell us, you know
1336180	1340100	What would kind of be the regret if we did this optimal planning within this one model of the
1340980	1342260	Of the mdp
1342260	1346180	But then in our work, we're not really interested in the setting of like one mdp one reward function
1346900	1352820	Um, so we start to think about, you know, what happens if we have arbitrarily many environments as well as arbitrarily many reward functions
1352980	1354500	Which we don't know in advance
1354500	1358420	And then I guess the other thing that I should say like you you alluded to like latent dynamics is
1358660	1364100	You know, these existing results are assuming that we have an mdp. That's fully observable meaning, you know exactly what the state of the environment is
1364660	1368980	Um, but usually when we think about like world models or even or just maybe more modern reinforcement learning
1369620	1372900	We're really interested in learning from like quite high dimensional signals. So
1373460	1375220	images or maybe
1375220	1378500	Probably images, but maybe there are the high high dimensional signals we want to reason about
1379300	1383780	And because we're just using image observations, this means that the world is like partially observable
1383780	1387300	Like we can't infer everything we need to know about the world just from one image, you know
1388420	1393220	For basically any physical task like the velocity of objects is important, but you can't infer that just from one image
1393940	1397140	Um, so in this partially observable environments, we really want to take
1397700	1402660	A sequence of observations because we need to to use those sequence of observations to infer what the state is
1402980	1406100	So, you know viewing a sequence of images will help me to infer what the um
1406820	1410340	The velocities are for example, and so we can think of this as inferring like a belief
1411060	1413940	A belief over what the state is and a partially observable mdp
1414660	1419300	Um, so we need this full sequence of images and we need to use the full sequence of images to then to be able to predict ahead
1419300	1423700	What the next observation will be and that's kind of what you know, most world models are attempting to do
1424260	1429540	Um, but if we just like taken a bunch of images and then try and directly predict images again, that's like quite a hard problem
1430260	1432980	Um to just like just predict straight an image space
1433460	1436900	And so the most common thing to do is kind of to take your previous sequence of images
1437300	1441940	And then try and get like some compressed representation of the history of images into like the latent state
1442660	1444660	And then predict the dynamics in the latent state
1445380	1447300	So yeah, so I have my sequence of images
1447380	1449940	I kind of compress these somehow into some vector
1450420	1455540	And then I give it a new new action and I try and predict what the next kind of latent vector will be given this new action
1455860	1458500	And this now represents my prediction of the dynamics in the world
1458900	1462420	And then if I want to um, you know predict what the next observation would be an image space
1462420	1464100	Then I can also decode that back to an image
1464660	1469060	Um, but then a lot of works also argue that maybe we don't want to actually learn to predict the entire image
1469060	1471300	So maybe you don't want to actually decode the entire image
1471300	1477140	But that's that's another aspect that we might want to get into but there's this whole broad story of of um working in the latent space
1477540	1481860	And um in reinforcement learning there was that paper called world models by you know, david haran and schmidhuber
1482340	1488500	And it also I think has a relationship with you know, what lakoon's doing with jepper and these like you know joint embedding prediction architecture
1488500	1491060	So there seems to be something magical about working in in the latent space
1491380	1495460	And also you were talking about um, you know partially observable markoff decision processors
1495780	1499140	And you know, that seems to be this idea that we need to have a modeling framework for the world
1499540	1504260	And I guess like the ideal situation would be is that like we just we we knew exactly what would happen
1504660	1507140	You know every single time step in every single state
1507700	1510820	Um, but we don't you know, so so we model it as a partially observable
1511060	1513940	Markov decision process and the markov bit is quite interesting as well
1513940	1517860	I mean maybe and you guys can just sort of introduce what why do we use that as a model?
1518260	1523620	So markovian basically just means you only need to look at like the current state to be able to infer all the information about the system
1524340	1526420	um, so so in a markov decision process
1526580	1531620	We have some state and then we assume that we're able to take some actions and given some state in some action
1531620	1533780	We get some distribution over next states of the system
1534100	1537380	And then the the system will transition according to that distribution to the next state
1537620	1541780	And this is just like kind of a general framework for modeling like systems that we might want to control
1541940	1545060	So, you know, it kind of dates back to like early work and control theory
1545060	1547380	But then it's also the main framework used in reinforcement learning
1547860	1548020	um
1548020	1550740	Yeah, and the reinforcement learning setting because it's the decision process
1550820	1556660	We we also add an reward function which tells us how good it is to be in a certain state or to execute a certain state action pair
1557300	1560900	Um, but yeah, as you said with relating to like partial observability and a lot of like systems
1560980	1563460	We we don't actually know what the true like state of the world is
1563460	1568420	So so you can imagine, you know, if we want to think of the entire world as a partially observable mdp
1569220	1574900	We can't just have some vector telling us exactly what the true configuration of the world is or maybe that exists
1574900	1579220	But we can't we definitely can't just know that and so we usually think of it as being a partially observable system
1579940	1584660	Um, so this means that like given given the state, um, you know at each step
1584660	1588580	We'll basically get some distribution over observations and we just get to observe that observation
1588980	1594740	So, you know, the state of the world could be what it currently is in here and maybe my um observation is like a camera image
1594820	1596580	so I only get some
1596580	1599700	Camera image of the world that allows me to infer a bit of information about the state
1600340	1602820	Um, and because it only allows me to infer a bit of information about the state
1602820	1604260	It doesn't tell me the whole state
1604260	1608740	It really you need to keep track of all of the observations you have to be able to keep track of all the information
1608740	1609700	You have about the world
1609700	1615700	So, you know, you can imagine um, if the task is for me to remember how to get out the door a while ago
1615940	1621460	Um, you know, I don't just need to be able to like look at my current image of the world to be able to infer that information
1621460	1624020	I need to have kept track of like all my previous information as well
1624580	1630100	Um, so that's kind of why we think about often want to think about like partially observable environments as opposed to fully observable ones
1630340	1635140	Amazing amazing. So so minci, maybe you can um bring in this this latent idea
1635860	1638660	And and sort of contrast that to what lacuna is doing as well
1639060	1644740	Sure, I mean, so I think in machine learning and deep learning, uh, there's this general paradigm that's been around
1644980	1646980	You know since the inception which is learning
1647940	1653540	latent latent representations of data and one of the benefits of learning latent representation is that
1654340	1658660	You know, ideally your objective, uh, that leads to learning these latent representations
1658820	1665540	Is that you are ultimately learning a lower dimensional representation of the data or dynamics that you're modeling like in our case with the world model
1665860	1672900	Um, that captures just what is necessary. It's a more compact representation of just the information that's necessary to predict
1673300	1677860	The task you're trying to predict and so um with uh with our case
1678340	1681940	Or latent space world models a lot of the benefit of working in the latent space
1682020	1684980	Is that if as opposed to working in the full image space?
1685060	1685540	for example
1685540	1690580	If your observations are images like in a video game is that there could be a lot of spurious features
1690820	1697220	Or you know a lot of additional information that you could be expending lots of compute and um, you know gradient updates
1697460	1702820	Just to learn those patterns when they don't actually impact the ultimate um transition dynamics
1702900	1706740	Or reward dynamics that you need to learn in order to do well in that environment
1706900	1710820	So one example is if you have a game where, you know, maybe the background is different
1711140	1716820	Uh, because it's daytime or nighttime or it's close to sunset. Um, but ultimately, you know, the background
1717380	1719380	doesn't really impact
1719460	1725140	How the player moves around in the environment or whether they've reached the end goal of the task and so
1725780	1729780	If you're training a model where it needs to compress a lot of this information
1729860	1733540	First into a smaller dimensional latent vector or latent representation
1733940	1738260	Um, you don't really need you would expect that latent representation not to actually capture
1738500	1743300	It would start to ignore the background color and it might only capture certain features of the environment that can
1743860	1748260	Essentially if you were to decode it back out it might only capture certain information about the environment
1748340	1750900	That's predictive of the actual task that you want to solve
1751300	1754500	Um, so maybe if the task is to say reach a coin at the end of a level
1754740	1759620	Then maybe the latent representation would capture the presence of the coin or whether the the proximity of the character
1759780	1761780	You're controlling to the coin
1762180	1763540	and so
1763540	1764980	With the jeppa related work
1764980	1770660	I think a lot of this is also, you know, motivated with this idea where if we can learn a better latent space representation
1770980	1774260	Um of images or videos or whatever modality we're trying to model
1774500	1779380	Um, it's a much lower dimensional computationally efficient representation. Uh that you can
1780260	1783220	You can effectively use for downstream tasks. Um
1784180	1788740	I'm not I'm actually not super familiar with exactly, you know, the the visual jeppa
1790740	1796740	Objective so you don't think I can say too much about that. Oh, that's okay. Yeah. I mean, but yeah, I mean you pretty much nailed it
1796980	1798340	so, um, I mean
1798340	1801460	Lacune even gives the example of like, um, you know in self-driving cars
1801780	1804580	You might not be interested in the leaves on on on the road, you know
1804580	1807700	So like with increasing levels of of nesting you kind of like learn to
1808100	1811620	Ignore the things that are not relevant and focus on the things that that are relevant
1812020	1817220	But we're almost getting to the center of the bulls I hear so intelligence to me is all about model building
1817380	1822180	And and that's what these abstractions are. They're they're models that kind of are predictive about the thing that that that's relevant
1822260	1824900	and kind of like ignoring what is not relevant and
1825380	1830660	We build better models when we have a curriculum. I mean apparently this happens in nature as well. Max Bennett
1830660	1834340	I was talking to him the other day and he said, you know, our genome doesn't encode all of our skills
1834580	1839060	Um explicitly because it would be too inefficient to do so, but they do encode a kind of curriculum
1839300	1843700	So we teach babies. Yeah, we babble with babies and we teach babies how to talk and stuff like that
1843940	1849380	So so the curricula is is really important and then we're getting to the center of the bull's eye
1849460	1851700	Which is intelligence in in general now
1852260	1858260	I think Lacune thinks that it's specialized and and what that means is that there are there are motifs
1858740	1861700	That's statistically generalized and what that means is that
1862580	1863780	You do need
1863780	1867620	environments you need to find motifs that are present in
1868260	1872900	In as many environments as possible and those are the generalizing features. Do would you agree with that?
1873380	1875380	Yeah, definitely. I think that a lot of um
1876020	1880260	So a lot of really powerful machine learning methods, for example, uh are trained in simulation
1880660	1884500	And when you're training in simulation, there's a concept in control from control literature
1884900	1889460	Called the sim 2 real gap and essentially this is essentially quantifying a performance difference between
1890020	1897460	Well, it's quantifying a few things one is just how different is the are the actual physical or other other kinds of dynamics captured by your simulator
1897540	1904180	Compared to reality. So if you have a physics simulator, how accurate are for example the friction dynamics or different kinds of contact dynamics?
1904660	1909380	In your robotic simulator compared to those actual dynamics in the real world with a real robot
1909700	1912900	Um, and this also leads to a sim 2 real gap in terms of performance
1913060	1919460	So if you train in the simulator, you know, a lot of times what machine learning is really good at is it's really good at learning to exploit
1919620	1925220	Whatever system you're training this the model in and so it's fairly um common for
1925620	1930900	You know systems that are models that are trained within a simulator to learn to eventually exploit the simulator
1931140	1933140	and so actually like one big area of um
1933300	1938420	Games ai is using is actually leveraging this idea where they essentially use ml models
1938580	1945060	They optimize ml models to within a certain game environment to try to find bugs within that environment to look for exploits automatically
1945300	1948500	Um, so ml systems are very good at finding exploits in whatever system you have
1948740	1954580	But then the issue is those exploits are usually where exactly where the gap between your simulator and reality resides
1954820	1960980	And so you actually don't want your model to learn to exploit these differences between the simulator and reality to get a high performance
1961300	1965220	Uh, because that kind of defeats the purpose of then later transferring your model
1965540	1972500	That's trained in simulation to reality because now in reality, obviously the model can't exploit those same those same glitches within the simulator
1972900	1974900	Um, yeah, so yeah
1974900	1980180	Yeah, I mean because the reason this is really interesting is is that the the premise of your paper is that
1980900	1984020	It is possible to build a generalist agent
1984420	1989940	Which means it's an agent that can be fine tuned and work really well on a whole bunch of downstream tasks
1990260	1995300	And to me that implies that at least in our physical world in any situation
1995380	2001780	You might use this agent that there are general motifs that it could have learned during free training that it could like, you know
2002020	2008820	Become activated in any situation. Um, does that is that fair? Yeah, maybe I can say something about um
2009140	2012900	Just the way that we should could think about like the different like latent dynamic subjective
2012980	2017540	So so I think I agree that like at least when I try and think about how I think or how people think
2017620	2018820	I think I agree that like
2018820	2023380	You know a truly intelligent system should kind of think through the world and like a very compressed representation of the world
2023380	2025940	Like if I'm trying to like think through how to go to the airport
2026020	2031140	Like I'm definitely not like predicting ahead in terms of like the raw image space of trying to predict every image
2031140	2035700	I might observe on the way the airport and things like this. And so I think we have this kind of like trade-off between, you know
2036500	2040020	Um, like we said with the bgf of paper like should should be just try and like
2040500	2043940	Kind of basically model like the minimum information we need about the world to try and you know
2044020	2048340	Do the do the relevant task in the world? I think what you're saying. I think that probably is
2049140	2052500	Maybe more what we think about when we think about like human intelligence or something like that
2053060	2055780	Um, but then there's also this other way where we just say we're going to just like
2056500	2058820	Enforce the model to be able to predict ahead every single image
2059220	2063140	And so in our paper, we do actually enforce that the model has to predict the next image
2064020	2066020	um, and so um
2066340	2068740	Basically what this might mean is yeah, like maybe the model does
2069300	2074580	You know, hopefully it does like like you said like kind of capture the underlying like true things that matter in the environment
2074820	2077380	But it might also mean like what we're saying with like the leaves example
2077380	2080580	Like this might force the model to kind of capture a lot of irrelevant details
2080580	2082820	That don't really matter like the leaves on the ground and things like this
2083220	2086100	And so, you know, maybe that means it isn't actually capturing the underlying motifs
2086100	2090180	It's actually just getting good at image generation. Um, but then I've or image prediction I should say
2090980	2095940	Um, but then I've also heard arguments kind of saying, you know, so what if people don't really think in terms of like image prediction
2096100	2098980	You know, I you know, we think in terms of like more like these high level motifs
2099300	2101380	But people have other people would argue that you know
2101620	2104660	Kind of the machine learning machinery is there to do really good image prediction
2105060	2109060	So so if if we if we can get a model that can actually just like predict images ahead really well
2109540	2112900	Um, and not really worry so much about whether it's reasoning about these like high level features
2113220	2117940	You know, if you can predict images ahead really well, you know, that's enough to make to do good decision making a lot of context
2118180	2119940	So I think there's this kind of like
2119940	2122740	Contrasting ways of thinking about, you know, image prediction is good enough
2122740	2126580	We'll just predict like really visually good scenes and that will be good enough for decision making
2126900	2130340	Or do we want to force the model to try and reason about like more abstract features of the environment?
2130340	2135380	And that's kind of a more intelligent way of reasoning about the world. Um, and yeah, I think that's a very interesting trade-off
2136420	2138420	Yeah, yeah, I mean like it's um
2139140	2141220	Like the biggest problem in machine learning is overfitting, you know
2141220	2143940	So as you say like that, there are all of these statistically generalizing features
2143940	2148100	But they generalize within the domain and the domain might be like your your simulator or like, you know
2148100	2150820	How you're training it rather than how it's being used in in production
2151060	2155780	And then as you say that there's also this um almost human chauvinistic or puritanical view on this which is that well
2156340	2160420	You know, it does the right thing for the wrong reasons or I use different motifs to do the reasoning
2160420	2162900	So that thing must be doing it wrong. Do you know what I mean?
2163220	2167860	And um, I was talking with chris bishop at msr the other day and and you know, he's um big on
2168100	2173460	Symmetries and yeah, you know, the kind of stuff that like max welling and takako hen and bronstein and um
2173460	2175380	The deep mind has done loads of cool stuff on on this
2175620	2180900	But it's this idea that like we know the world um has a certain geometry. It has certain physical priors
2180900	2185780	So like we can deliberately um, you know, kind of construct the approximation class in machine learning
2186260	2190580	Methods so so that like we make it an easier problem, right? Because we because we know we know the thing is in there
2191700	2193700	Yeah, so I mean, I guess sort of the uh
2194260	2196260	Slight tangent I went into around the sim to real gap
2196420	2199540	I guess part of the point I wanted to make there is that um, you know
2199540	2202420	One way around the sim to real gap is you could try to train
2203060	2207060	You could try to parametrize a very large space of possible versions of reality
2207300	2213380	And this is kind of the motivation behind this method of domain randomization where you sort of say this is the you know
2213380	2216660	This is the specific task domain I care about I can parametrize the different
2216980	2219380	Uh versions of the task with a few parameters
2219620	2226020	And I basically want to search over the space of parameters and train my model or my agent on all possible variations of this world
2226180	2230820	But obviously that's not very sample efficient because that design space could be huge could be massive
2231060	2234820	And so instead we like these active sampling strategies like we were talking about earlier
2235380	2237380	around mini max regret style
2237540	2242340	Active sampling where you sample those environments that maximize your regret or some other type of objective
2242420	2245940	Maybe like uncertainty uh similar to what we do in the waker paper
2246500	2249860	But ultimately these things these active sampling process it leads to
2250660	2253300	What we like to call an auto curriculum automatic curriculum
2253940	2257060	And this is in contrast to prior curriculum learning works because here
2257380	2262500	This is an automatically generated curriculum. So you you can kind of not have any pre-defined notion of what is
2262900	2268900	Easy or hard it's purely fixed to what is easier or hard for the model in terms of how good the model is at
2269060	2271780	Performing at those tasks. And so it's nice. It's an automatic curriculum
2271940	2276660	So you can think of it as almost like weaving a path through this high-dimensional design space
2277380	2279380	automatically such that if the
2279380	2285620	Agent or model were to train on data along this path of environments through its experiences in this path of environments during the training curriculum
2285860	2289060	It'll basically be maximizing some sort of information gain objective
2289380	2290100	um
2290100	2294500	Because you know, for example regret if there's a high regret that's that means there's a high
2295300	2298420	Ceiling there's a high gap in terms of how much the agent can improve
2298660	2301140	Which implies that there's a lot more for the agent to learn in those environments
2301300	2306740	So it's sort of this like optimal you want to find this optimal path weaving through the high-dimensional design space of environments
2307060	2314820	Now the danger here is that as you do this, uh, auto curriculum the auto curriculum, uh, could also go haywire very easily because
2315060	2320580	The design space is so big if you're training in simulation, which we have to do because these methods are so sample inefficient
2320580	2323780	We need so much data to train them. Um, you want to train in simulation
2323780	2326820	But if you're doing the auto curriculum in the simulation design space
2327220	2333380	It could start to veer very easily and quickly into different corners or niches of the design space where
2333940	2340260	You know the parameters no longer really make sense in terms of mapping to a physical reality or a real world scenario
2340580	2342580	That we as human users
2342660	2345620	Uh actually care about and so kind of it would be you know
2345780	2349860	It would defeat the purpose of spending all this compute to train this model that could then help us in the real world
2349940	2354420	Because now it's veering off into parts of the design space that don't really matter for humans
2354420	2360020	It's kind of noisy parts of the design space. And so this kind of leads us to this question of grounding
2360020	2363620	How do we ground curricula? How do we align the curricula such that you know?
2363620	2368420	They can still do their exploration through this active sampling type of procedure over the environment design space
2368580	2375220	But at the same still at the same time maintain at least some proximity to the parts of that design space that are relevant to
2375700	2378340	What humans care about in terms of the actual tasks they represent
2378500	2381940	I mean i've been speaking with kenneth stanley a lot recently and we're talking about open-endedness
2382340	2385860	And in general i've been trying to come at this problem from multiple angles
2385940	2391060	And i've been using the lens of agency because i think agency is something that happens in the real world
2391060	2395140	And that's why we have this divergent process because we have multiple agents, you know, kind of like
2395540	2399540	You know undirected following their own gradient of interestingness. So in in evolution
2399540	2402980	That's a great example that it is this divergent process, but it's also grounded
2402980	2408340	It's physically grounded, you know, so it's like the physical world creates some kind of constraints on on the things that are found
2408660	2412900	And i mean, you know clune called this ai generating algorithms. There's quite a few different takes on this
2412900	2416020	But the idea is that um to search this complex search space
2416100	2421940	We we need to have a divergent search and that's like we actually need to create the problems and the solutions
2421940	2423220	So like in the real world
2423220	2427620	The the you know the giraffes had the problem of like eating the leaves from from from the trees
2427780	2430260	And the problems and the solutions get generated in tandem
2430420	2432500	And this whole thing just kind of grows and grows and grows
2432660	2439460	And that seems to be the most important feature that is missing in current ai systems and the grounding or the
2440660	2443940	Stanley calls it the gradient of interestingness. I'm not sure whether you'd agree with that
2444020	2449060	But um, i mean what mark what what what do you think about the importance of like this divergence in ai?
2449700	2452020	kind of the current paradigm of machine learning
2452660	2458100	Of kind of like, you know gathering some data set beforehand or specifying some simulated beforehand if it's reinforcement learning
2458580	2462580	Is kind of good enough to do like a lot of reasonable tasks that we might care about
2463060	2463860	um, you know
2463860	2468900	Like obviously like predicting language or generating simulated language or performing very well at some simulated task in rl
2469140	2472660	But it definitely seems like the next step towards like very general agents that are kind of
2473460	2476100	You know, I guess maybe I don't know if we want to use the term agi
2476100	2479540	But there's something something more along the lines of a general agent that's kind of you know
2479860	2483140	able to kind of self improve and learn in more diverse environments
2483620	2487620	Um, it definitely seems like that's kind of the next step of where machine learning will go
2488180	2490820	And if we're going to get to that point, I kind of agree with the idea that
2491540	2496900	You know, it certainly doesn't make sense to have some agent that just randomly trying to gather completely random new knowledge
2497220	2500580	Like it certainly seems to make sense that you know, you know, even as a human
2501220	2506100	To improve your intelligence you kind of selectively try and find out the areas in which like you can gather more
2506500	2509540	More information or more knowledge and things like this and this is kind of what you know
2509700	2514340	Leads to this kind of I guess branching or you know, like you said like the diverse set of things um
2515140	2517540	That you might want to learn more about and so yeah
2517540	2521620	I think like it clearly seems to make sense that like this kind of more open-ended this thinking is probably going to be like
2522020	2524340	The next paradigm of how we think about these kinds of systems
2524580	2526580	But I'll I think mentally we'll have more to say about this
2526980	2532820	I think the reason open-endedness is so interesting now is I think we're uh, there's there's a few reasons why I think it's like
2532900	2539700	newly relevant to this current era of machine learning because these ideas have been around for quite a while like, um, Ken Stanley, Joe Lehmann
2540020	2542020	um, Jeff Klune, uh
2542020	2544180	Lisa Soros these a lot of these researchers, they've
2544900	2550180	They've been thinking about open-endedness and novelty based search divergent search for decades. Um
2550740	2555060	I think it's really interesting to think about why there's sort of this resurgence of these ideas now
2555300	2557300	and I think a lot of it is because
2557380	2559700	It is again, you know, it's it's sort of following the same
2560100	2560820	um
2560820	2563700	Sort of uh tailwinds that have been driving a lot of the ml industry
2563700	2566980	Which is just like much better compute much larger datasets
2567300	2569780	And I think what we're seeing now is that we know that
2570260	2575140	Modern deep learning methods work best when we can scale up the compute and the data. That's how you get them to work
2575540	2579380	Um to the to their maximal capabilities. Um at some point
2579380	2582420	We're going to run out of data and a lot of people are now starting to talk about
2582660	2584740	You know this as sort of a pending issue on the horizon
2584740	2589140	Which is you know at the current rate of consuming data for training our foundation models at some point
2589140	2592500	We're going to run out of data. We're going to where are we going to get the next trillion tokens from?
2592900	2595620	Um, and so I think a lot of this uh now
2596020	2600340	points a lot of the interest to open-endedness because open-endedness is essentially, you know
2600580	2603460	We're studying systems that can generate their own data in an infinite
2604020	2607700	Capacity and so it's systems that essentially if you run it for longer and longer
2608020	2612420	They get more and more complex. They generate more and more quote-unquote interestingness or interesting data
2612820	2613220	um
2613220	2616580	And so if we can actually, you know crack this nut of how do we actually
2616980	2621220	Come up with a self-improving system in the sense that it keeps generating interesting data
2621940	2626740	We can then use that data to train further train our models
2626980	2630900	But of course you get into this perpetual data machine type of
2631460	2634500	Idea where obviously, you know, there's how do you generate more data?
2635380	2639140	If you know the data is ultimately coming from a model that you probably trained on previous data
2639220	2640980	How do you get net new information from that?
2640980	2645940	Well, I think a lot of this is actually just resolved purely again going back to this idea of the reward function
2646100	2652180	Right or a preference function where there is outside information coming in through some sort of filtering criteria
2652420	2654420	For example human designers in the loop
2654820	2660420	Or designers designing some sort of preference model that could essentially automatically rate the kinds of automatic
2661300	2663540	Data that's being generated by these open-ended systems
2663700	2668580	And if we can do this kind of filtering we can essentially automatically find start to automatically find
2669140	2673060	Useful net new data net new trajectories net new even, you know, maybe
2673940	2678020	Sentences like tokens or net new content to train our models on
2678340	2684820	I've been thinking a lot about creativity recently and I think creativity is is is the other half of the coin of intelligence
2685220	2689700	So in the world we live in I think that the intelligent process is is us
2690100	2692100	We are a divergent search and we are
2692580	2696420	And basically tackling a complex search space and we are building knowledge
2696740	2698820	And we are memetically sharing them in our society
2698820	2702660	We're embedding them in our language and then language models come and like acquire all of that knowledge
2702900	2706900	So the cynical take is that ai today doesn't you know generalize and
2707700	2710260	It doesn't it doesn't creatively find new knowledge
2710500	2713300	It just is a representation of the knowledge that we have found
2713620	2718260	But it's not black and white is it so the the work that you're doing is a great example of no no no
2718660	2726660	You can generate new knowledge by exploring these complex search spaces and even though you're exploring existing models
2727060	2731860	You're discovering interesting and novel combinations of those models that have not been found before
2731860	2736100	So it's creating a novel margin on something that was not there before
2736420	2740660	But I suppose the ideal future we want to get into is that we really can just
2741460	2744340	From a far deeper level generate new knowledge
2745140	2750580	Yeah, I think one interesting thing that I've been thinking about more recently, you know is that um sort of the you know
2750580	2752340	The high level question is just
2752340	2759220	Right now all of the state of the rai systems from chat gbt to stable diffusion style models for text image generation
2759380	2763700	All of these systems they're they're amazing very impressive, you know
2763780	2768260	Like five years ago. I would not have believed that these systems could exist at this level of performance today
2768660	2773460	But uh, ultimately, uh, what they do is they're in the they're they're in the q&a business
2773620	2778100	So I basically ask these systems a question or I give them a command and they give me an answer
2778740	2783300	Um, and so I think the next frontier of ai is really how do we design systems that don't just
2783940	2787460	Answer questions, but they actually are the ones that start to ask the questions
2788020	2792100	And I think once we can have ai systems that start to ask interesting questions
2792740	2798820	Um, that's when we start to get closer to I think traditional notions of what uh strong agi might be
2799140	2801140	Okay, so so again really really interesting now
2801460	2805460	So we're getting into agency and and people think that oh you could give a language model agency
2805460	2808020	You just like you know run it in a loop and interesting things will happen
2808180	2811540	Well, well, that's not true because the whole point of open-endedness is to prove that
2811860	2815460	Existing systems converge so they don't diverge so they don't accumulate information
2815700	2819060	So we would need to create a kind of agent that like, you know, it would just keep running
2819060	2823060	And it would just keep doing interesting and novel things that would keep accumulating information
2823460	2828180	And I think that the reason why language models don't have agency is because they are essentially
2828740	2834420	A low entropy model and what that means is during training a lot of the the sort of like the unnecessary
2834980	2840100	Um, you know complexity was snipped off. So the models only know about relevant things in the next step
2840100	2846180	What's the next best token and it feels like we would need to have not only a higher entropy search
2846420	2850900	But we would also need to have um a diverse set of models that are actively
2851620	2856740	Continually learning and and diverging from from each other, but that's just my take. I mean, what do you guys think about that?
2857060	2858740	Yeah, I think that
2858740	2863540	So I guess this relates quite a lot to this idea of like intrinsic motivation, which is something that we utilize in our paper and I guess
2864340	2866340	I guess the idea with that is like
2866820	2871700	You know, if we're trying to like gather new data in the environment, like we shouldn't necessarily be constrained to just try and
2871940	2874580	Gather new data that's like good for a specific task
2876180	2882020	And so I guess this kind of you know, so intrinsic motivation basically says I should just gather new information because it's novel
2883460	2887540	And things like this and so we can basically like specifically try and gather information that you know
2888020	2894340	Reduces our uncertainty about the environment and and or similar objectives that that don't rely on some external reward signal
2894660	2899940	And I think we when you get to the situation where the model is able to like self-improve in the absence of an external reward signal
2900020	2904420	So intrinsic meaning that the the signal for what you should get is just purely generated by the model
2904420	2906420	So it's purely intrinsic to the model
2906660	2912660	Um, so I think the situation where you know, you have the model that's able to self-improve without any external signal without a human
2912660	2916580	Having to define what the reward is or what the objective is or this was good data. This was bad data
2917140	2922020	Um, I feel like that does feel like a lot closer to the notion of agency because of the fact you don't have kind of some
2922340	2924580	External person defining what's good and what's bad?
2925140	2929780	And so yeah, I think like this the like and you also mentioned the word like creativity because I think
2930180	2932180	At least in the context of things that
2932260	2938260	I've done in terms of machine learning or reinforcement learning. I think like intrinsic motivation feels like the closest thing related to creativity
2938580	2943940	So you're basically like trying to gather information because it's novel or because you think it's or the model thinks it's interesting
2944340	2945780	rather than because
2945780	2947780	You know, it satisfies some objective
2948020	2953540	And so I think we could maybe say like intrinsic motivation is in some sense like an objective for being creative as well
2954180	2957540	Um, I don't know if you have any thoughts about this. Yeah, I think I think that uh
2958260	2963940	It's I think there's definitely a hugely deep connection between intrinsic motivation and creativity. Um
2964500	2967860	In the literature intrinsic motivations also sometimes called artificial curiosity
2967940	2970420	So this is a term that was coined by Juergen Schmidt-Huber
2970900	2973540	Could you could you explain it just what it is? Yeah, so oh, yeah
2973540	2977940	So taking a step back intrinsic motivation is essentially um in reinforcement learning
2977940	2983300	We train on reward signals and as mark was saying, um, we typically train on external reward signal by external
2983300	2989140	We mean that this is a task based reward. So this is um external in the sense that something outside of the agent
2989140	2993780	That's learning like the human system designer decided that this is what the reward signal is for the task
2994100	2998740	Uh intrinsic means that we want to we don't design directly the reward signal
2998820	3001780	But we're actually using some aspect of the model itself
3002180	3006900	In order to drive the models learning forward. And so one example of this could be prediction error
3007060	3012420	So if the model, uh has a large prediction error on a certain task like averaged over each time step
3012660	3019300	We can use that as a reward signal and say, hey, you want to visit more parts of the environment where you're bad at predicting
3019620	3023700	Um, how the state will transition when you act in that part of the environment. And so
3024260	3028260	Uh, as you can see, this is very similar to maybe like intuitive notions of what curiosity is
3028900	3030900	Curiosity and different forms of play
3030980	3035460	Um in the psychology literature, a lot of people actually argue that, you know, different forms of play
3035940	3041860	In curiosity really they they amount to you can model these behaviors as essentially a person trying to
3042500	3045780	Engage in activities where, you know, they're not very good at predicting the outcome
3045940	3047540	And that's kind of what makes you could argue
3047540	3053860	That's kind of what makes certain kinds of entertainment fun because or entertaining because you can't actually predict what will happen
3054180	3055140	um
3055140	3059300	You know in a few frames of the movie like a movie wouldn't be very interesting or a book would not be very interesting
3059540	3063460	If you can predict what will happen in the rest of the book just by reading the first few pages
3063860	3068340	Uh, and so intrinsic motivation is really saying let's guide the model towards parts of the environment or the world
3068580	3071300	Or experiences where it's similarly unpredictable
3071780	3076180	Stanley speaks about this this concept of deception or we call it the false compass
3076500	3082820	Which is this idea that any objective and even you you could say exploring all of the search space is an objective
3082820	3086260	So he said every every objective has deception and if you monotonically
3086660	3089700	Optimize any objective you will always lead into you know, like a
3090340	3097300	Deceptive part of the search space, but then like the counter argument is okay. Well, let's let's not um, let's not have any principles for doing the
3098020	3103220	You know the the exploration. Let's just do something completely random and that doesn't seem very good
3103380	3111220	So so then, you know, there's this concept of well, how do I how do I imbue some concept of what's interesting without falling victim to deception?
3111300	3117220	Yes, so ken stanley, uh has a famous essay in the realm of open-endedness where he points out
3118580	3120580	That this notion of interestingness
3121220	3126100	Is uh, ultimately a subjective concept and so even in the case of intrinsic motivation
3126100	3129300	Which I think is you know in practice we can get a lot of mileage out of this
3129940	3134500	And we've seen this in a lot of domains where exploration helps a lot like even in the wakeer paper
3134500	3139620	it's largely founded on this idea on how we exploit intrinsic motivation for learning world models, but
3140420	3143140	Ultimately, you know, these these model based
3143780	3150340	Measures of intrinsic motivation. They are by definition based on the particular model at play and so
3151220	3156740	At some point, you know, you're you're starting to over fit to what that specific model finds interesting
3157220	3162020	And of course what that model finds interesting if your measure of interestingness is something like a prediction error
3163140	3169460	Is going to be a function of you know, the specific architecture of the model the actual inductive biases of that model
3170100	3173860	The capacity of that model to learn and so you could imagine a model where you know
3174260	3177780	At the beginning it's looking for lots of interesting parts of a particular video game environment
3177860	3180820	But at some point, you know, it might saturate what it can represent
3181060	3184020	And what it can learn and at some point it might start to find things
3184100	3187940	It's explored before interesting just because it's starting to forget those parts of the environment
3188100	3192020	You know if you have like a very rich stream of different kinds of environments that it's exploring
3192180	3198180	So ultimately this is like an example of deception because now it's like I I think that my model is the model thinks it's exploring
3198260	3201300	Parts of the environment that it finds interesting based on this prediction error
3201460	3206740	But ultimately it might actually start to go back to other parts of the environment because of issues of model capacity
3206820	3209860	And another really famous example of this issue would be like the noisy tv
3210020	3213220	So like if your environment has you know, this this uh
3213860	3218820	Noisy tv where it's just showing random noise random rgb pixels. Um, you know, that's
3219460	3222260	You know, that's not something you can actually predict because it's just noise
3222500	3228100	And so the model if your intrinsic motivation is really just to search for novelty in the form of prediction error
3228260	3233860	It might just start staring at this tv forever because it's something that it just can't predict and I know just by looking at that tv
3233940	3240500	It'll be maximizing its prediction error. Yeah. Yeah, it's so interesting. Um, so so just coming into rich stuff in a little bit
3240500	3246420	So he had this idea called um reward is enough and and essentially that doesn't make in the case that you know
3246500	3251700	Just using um implicit uh motivation all the stuff that that you've just been speaking about using this trajectory
3252180	3255940	You know optimization process that we can do everything we need to do
3256260	3262020	And in in your paper, you're kind of making an argument similar to what lakuna has been making for years about self supervised image learning
3262020	3265700	That what we should do guys is let's let's kind of pre-train a base model
3266020	3269060	So this model um understands environmental dynamics really well
3269380	3272820	And then we stick a reward in there and and we build um agents after that
3272900	3278500	So does it in any way reinforce or pun intended uh satan or or do you think it's still complimentary?
3279140	3284500	I think it's still complimentary at least if I understand the the meaning of the reward is enough paper because my understanding of that
3285060	3288580	Um line of thought is basically saying that you know, we can kind of specify
3289140	3294500	You know any tasks that we might want an intelligent agent to do as optimizing a reward in some like mdp or promdp
3294580	3298740	So market decision process or something like that and I think our work isn't contrary to that in the sense of like
3299540	3305620	You know, I do think that that probably is a sufficient framework to be able to model any any kind of behavior that we might want an agent to
3305620	3310100	Do but I think when it comes to actually like practically implementing that idea. There's a lot of difficulties
3310500	3315540	So the first one might be um, you know, how do we even specify that reward function?
3315860	3318180	so, you know, if the reward function is to um
3318820	3321380	Have a good life or something like this like there's obviously like
3321940	3325780	You know, maybe there is some like numerical way of defining that in terms of an mdp
3326580	3332100	But there's like not actually a good way of writing down that function that maps what I do to whether I'm getting good rewards
3332420	3336500	And so I think there's this kind of like, you know, I think that's a good framework for like thinking about any problem
3336740	3340340	But then you have these kind of like practical issues of how do you actually define rewards?
3340340	3342340	And how do you how do you say like?
3342580	3347460	Were there an agents doing well and not doing well and things like this? Um, and so I think that's still um
3348020	3351540	Even with the world models lines of work. I think that's still like kind of quite a difficult issue
3351940	3356900	So so so the world models lines of work kind of, you know, allow you to model, you know, predicting ahead in the environment
3357460	3359700	Which is a very useful thing for doing a lot of tasks
3360340	3362980	Um, but then if you actually want to optimize some specific task
3363380	3365780	You still have this problem of like, how do you define the reward?
3366020	3369940	And so we eventually want to get to this point of being able to like inject a reward into the world model
3370020	3372980	So we're kind of in agreement with that kind of line of thinking in a sense
3373060	3376980	We're eventually going to use a reward to derive the the the desired intelligent behavior
3376980	3378660	So I don't think there's any conflict in that sense
3378900	3382740	But we still have this kind of problem of how do we inject that reward into the the world model?
3382740	3384740	How do we define what that reward should be?
3384980	3386980	um and the case of um
3387380	3389380	You know one of the easiest things to do for example
3389460	3392340	Would just be to label each image with reward and then you can kind of
3392660	3397300	Encode that image into the latent space of the world model and then use that to define how good a certain thing is
3397620	3400020	And that's kind of the style of thinking what we think of in our work
3400500	3404180	Um, but I don't think that overcomes this like overarching issue of in general
3404260	3409060	It's you know rewards can define everything, but how do you in practice like get that function is pretty hard
3410180	3414500	Yeah, I mean in a sense reward is enough is sort of a tontology because once you know the reward
3414580	3415700	um
3415700	3417700	If you know the reward function for your environment
3417860	3420980	You can essentially compute the value function, which gives you the optimal policy
3421220	3421540	um
3421540	3424820	And so reward has to be enough if you know the reward function and so
3425460	3428900	Uh, I think the more interesting question is definitely like what is enough for the reward?
3429140	3436340	What is enough to actually have a system automatically figure out what are interesting new rewards for us to train new agents?
3436340	3442020	We're new models on or continue training existing models on and I think this goes back to the question of environment design
3442020	3447700	This is largely the motivation of that line of work this auto curricula environment design where essentially if we can automatically
3447780	3451780	Weave through this path of possible environments of the design space of the environments
3452180	3458660	The design space clearly will encompass like a big part of the design space is also encompassing the reward for those tasks
3458980	3464500	And so essentially we want to find a curriculum automatic curriculum or path through the possible reward functions
3464820	3467460	In which we can start to train a more and more general agent
3467700	3469700	But then the interesting question is again
3469780	3475940	Like what exactly is the right notion of interestingness in order to drive that curriculum that path through the design space
3476100	3481300	of possible things we could be training our model or agent on and um, and that's I think
3481700	3487060	One of the most interesting open questions and it relates to the question as well of how do we get the model to ask the questions?
3487700	3491220	Because really what drives humans in terms of asking further questions
3492020	3497220	Is our own implicit notion of interestingness which is informed by things like the scientific method and you know
3497300	3500340	being able to create explanations about the world
3500740	3505460	And we find things interesting when we can't actually explain some phenomenon about the world
3505940	3507940	Based on existing theories or explanations
3508260	3513700	And so I think what's really missing for a well-grounded, you know, human interpretable version of
3514180	3520100	Interestingness is having models that can essentially come up with their own theories about the world and start to probe those theories
3520260	3523620	For where there's mismatch between, you know, the their learned theory of the world
3524020	3527700	And evidence that new evidence that they find from experiences in the world
3528020	3532900	Yeah, it's so interesting and and um, I mean when I make the argument that agent should be physically and socially embedded
3532980	3538100	It's it's actually quite a simple argument, which is just the guardrails. It's that interesting this thing
3538180	3545060	I think that that is how, you know, having um, uh agency but with the guardrails of our physical and social embedding
3545140	3548180	So, you know, we're we're sampling things that make sense because they're already there
3548420	3552100	That, you know, but but but obviously we can go off piece to little bit as individual agents
3552340	3555860	I I feel that that's what helps that process just coming back to Sutton
3555940	3558100	It's entirely possible that I've misunderstood Sutton, by the way
3558100	3564420	So my my interpretation of reward is enough and it might be true as you say that it's tautological given that if you already knew
3564740	3568740	The reward function for a particular environment then it could do everything that it needed to do
3568820	3574900	But my interpretation of reward is enough is that it would lead to um, a general intelligence and you know
3574980	3579380	General in in the kind of magical sense that it would work in in any possible situation
3579700	3584900	But if it is specialized in the way that we agreed earlier that there exists a reward function
3584980	3586980	Which would inco you know codify
3587140	3592740	Motifs and things that you know, you need to know or optimize in a particular environment or set of environments
3593140	3597220	Then to me that's still specialized intelligence and I would agree. Yeah. Yeah, yeah
3597300	3602260	That's that I think that aligns with my take as well where I think if you have a reward function
3603140	3605140	It's already sort of applying
3605620	3609220	Largely applies to at least the examples in that position paper about reward is enough
3609220	3612100	It seems like most of the reward functions they discuss are largely
3612980	3614980	Grounded in a specific task
3615060	3617620	And I think that if you have the reward function for a specific task
3617940	3622020	Then it definitely seems that you can have some optimization or learning algorithm
3622260	3625540	That essentially learns to optimize that reward and therefore achieve that task
3626420	3628500	So I do think sort of the open question that
3628980	3631700	Uh, I think same reward is enough
3631780	3636500	I think it kind of passes the buck up further one level to the question of where that reward comes from
3636660	3641940	And I do think that having systems that can automatically design interesting new rewards. That seems like the frontier
3642180	3647220	Yeah, I agree and and you know because to me intelligence is about discovering the knowledge and the knowledge is the reward function
3647220	3651060	So if it was like kind of baking the knowledge in into the system, um, okay
3651060	3656900	so another sort of galaxy brain take is um, I was talking to bishop about this the other day and um
3657620	3660180	Do you think of like deep learning models as one model?
3660180	3666420	Or do you think of them as a sort of like intrinsic ensemble of models because they they get they behave differently in an input sensitive way
3666740	3670340	So, you know, like depending on the prompts you put into language into a language model
3670580	3675620	You might find that like a different part of the weight space gets activated and and essentially it's like retrieving a mini program
3675860	3680180	And that program is being run, but it's not it's not model building. It's like model
3680900	3682900	Retrieving, but we would would you agree of that?
3683140	3684180	Hmm
3684180	3686180	I guess i'm not sure about that like
3686180	3688500	Like within like subsets of a single homogeneous model
3688500	3692500	But I guess the thing that I like to think about that's I think quite related to this is this idea of like
3693140	3696740	And I think yamakun also kind of well a lot of people have laid out like a similar architecture
3696740	3701300	It's like, you know, should we think of intelligent agents as having kind of like separate subsystems that can maybe
3701860	3705060	Like be thought of as different neural networks. And so, you know, we could have like, you know
3705860	3709060	Um, the standard notion of a policy which is like outputting actions
3709380	3712820	And maybe we also want to have the notion of like a prediction model more like a world model that predicts
3712820	3717540	What might go ahead in the world as well as maybe like a planner that is somehow good at like optimizing in that model
3717540	3721860	And so we could kind of think of all these things as like separate subcomponents that we assume an intelligent
3722100	3723700	You know an intelligent
3723700	3726660	Thing would have like an intelligent thing should be able to predict ahead in the world
3726660	3728660	It should also be able to output actions
3728660	3732260	It should hopefully maybe be able to infer like why other things happened and things like this
3732740	3736980	And so I guess as to whether we think that should you know be just like one homogeneous model
3737780	3741380	For which maybe you query it and maybe you know different aspects of that model are kind of um
3742020	3745700	You know handle different aspects of the query or whether we should think of those as separate components
3745700	3749620	I'm not I'm not really sure as to whether it matters whether they're separate components or not because yeah
3749620	3752580	I agree that you probably could just have like one massive model that does all these things
3752980	3755140	And I think at least from the the trend that I've been seeing
3756340	3760580	In kind of the world models literature and and also just like I guess the rl literature
3760580	3762580	Or maybe just we should call it the foundation model literature
3762740	3766820	Is you kind of don't want to have like a separate model that does the prediction for actions and a separate model
3766820	3770020	That does the prediction observations like why not just have one massive model
3770260	3774020	That's jointly trained to predict everything you might want to query and then depending on the different query
3774100	3779780	You know it will just either predict an action or a predictive video sequence or it can be conditioned on actions or conditioned on language
3780020	3784340	So I think in this sense like this kind of model like you said is more like just one massive model
3784340	3787220	But it kind of has like a lots of different sub tasks that it's able to do
3787780	3788340	um
3788340	3792340	And so maybe this is actually like the more effective way of training a model because then you kind of get generalization
3792340	3794340	Across these different sub tasks as well
3794500	3796500	Well, yeah, and the reason I'm asking the question is um
3797140	3801860	It seemed I mean like you know for for an outsider coming in it looks like statistics has broken
3802100	3805140	You know in the olden days we used to talk about the no free lunch theorem used to say like you know
3805140	3807620	You need to have specialized models for different situations
3807860	3810100	And now the narrative is that we have generalist models
3810100	3814260	We have foundation models and and they are better than the specialized models in a strong sense
3814660	3817780	And you know and I like to sort of push on this a little bit and see well
3817860	3823700	When when does it break because we know that there are like these physics inspired models with inductive priors that you know
3823780	3827940	Know about invariance of you know like molecules and drug discovery and stuff like that
3827940	3830260	And surely they would be better than a language model
3830260	3834420	But no no no now they're training language models on mathematical conjecturing and like you know like
3835060	3839700	Drug formulation using tokens and and so on so you know as an outsider you might just think well
3839700	3841860	We can just use a big transformers model for everything
3842820	3846260	I I think a lot of this does come from um well, so
3847220	3849540	I think the attention-based transformer architecture is
3850100	3855140	Proven empirically to just be highly scalable highly effective at learning lots of different kinds of data distributions
3855780	3859300	But I think also part of it is just that we're just starting to enter this regime
3859300	3864500	When we're just training these models on an insanely large amount of data, and I think that a lot of times
3865380	3869780	We need to sort of take a step back and really consider the amazing performances on different tasks
3870180	3873620	And really think about you know how much information was actually leaked into
3874340	3876340	This task in the training data because
3877300	3879300	Right now. We're really just training
3880020	3882020	these huge models on
3882020	3885700	I think I would say that we're largely training them on the test distribution in many cases
3886420	3889700	I do there I have seen like lots of examples of
3890180	3893540	Truly impressive behaviors from these models that that do seem like
3894340	3897220	Truly novel like zero shot generalization to unseen tasks
3897700	3900820	Like there was a recent example. I saw on twitter or someone
3901380	3904580	Had like a very low resource like rare language and they gave it a few
3904980	3910980	They gave I think the cloud 3 model a few examples and it was able to essentially perfectly reproduce new utterances in that language
3911540	3913540	So that does seem very impressive
3913780	3919140	But it does seem at the same time, you know a lot of the performances for example on elsat or like ap biology exams
3919300	3921380	I imagine a lot of that is really a function of just
3922340	3927780	Literally giving the model the test domain in terms of information during the training step
3928180	3929220	Okay, okay
3929220	3932260	So there are like two schools of thought on this when we talk about world models
3932260	3936980	You know people are talking about sorrow and is it building a world model and and it certainly seems to be it seems to be doing
3937060	3938420	I mean, obviously it's not doing navier stokes
3938420	3941220	It's not doing like fluid dynamics, but it seems to be doing something like that
3941460	3945140	So like one extreme view is that it is just a hash table
3945540	3952180	And you know, it's it's kind of doing some diffused approximate retrieval or whatever another school of thought is that it's like a simulator
3952420	3957220	And you know people talk about the simulator's view of large language models and you know, like it's like it's modeling
3957540	3960660	Not only, you know, just to just just the words and the language
3960740	3964500	But it's also implicitly learned to model the world and the people and all of us
3964980	3969700	So that's the spectrum. I mean like uh mark, where do you think these things are on that spectrum?
3969940	3974340	Yeah, I think we have like it would be great to be able to play around with it and kind of see what we can get out of it
3974420	3976900	But I think I think if you can for example
3977860	3980900	You know after each kind of you know, so it's a language condition model
3980980	3986740	So if after each kind of frame you could for example put in a different language language kind of conditioning and say like
3987220	3992820	You know, what happens here if you know, the mug was pushed off the table instead of whatever else was originally happening in the video
3993060	3997380	And so if you can basically do this kind of like counterfactual like interventional predictions where you kind of
3997860	4001940	Give some new action and then you're able to see like the alternative outcome of that new action
4002500	4008340	I think if the model is able to do that then I would think that it does have a pretty good understanding of how the world works in the sense of
4008740	4014100	You know, I really think like if you can predict the outcome of any action given some sequence of observations
4014100	4018580	I do think that's a pretty good proxy for being able to say if you can do that you really do understand how the world works
4020980	4022660	And so I think if the model can do that
4022660	4026820	I would be kind of inclined to say that it does have a kind of world model in the sense of understanding
4027220	4029540	The underlying world but then there might also be a chance that you know
4030340	4035700	You know these models aren't like you said it's more just like a diffuse retrieval and perhaps if you try and do like a very
4036100	4039540	Fine grain conditioning on a slightly different outcome different like conditioning
4039540	4042260	Maybe it won't actually give you the correct kind of counterfactual prediction
4042660	4047460	And so I think maybe we'd have to see how good these models are at generalizing to slightly different inputs and things like that
4047700	4052500	To really see if it understands things well, or it is just like kind of generating some arbitrary video
4052740	4057700	Yeah, I think it's a double whammy because our colloquial use of language and like you know use of models and intelligence
4057700	4062980	It's so static that like, you know, we we um, we think of that as being intelligence
4063380	4066980	But but we're still going like we're now create we're creating knowledge right now
4066980	4069940	We're creating models because we're exploring we're doing exactly what you said Minshew
4069940	4073780	We're like we're exploring the search space and we're building models and we're combining them together
4074100	4077780	And you know, presumably we would diverge quite quickly from from from the language models
4077780	4082900	But I mean what what's your take on on this idea that they are, you know, potentially world simulators? Yeah
4083300	4088740	um, so just regarding the the sort of lookup analogy for these large models, I think it's
4089460	4095060	So my mental model is similar to that. Um, although I think it's it's very close to um, I think a really good write-up of
4095620	4097620	of the of this alternative take
4098100	4099380	Which is more like
4099380	4101940	There's an alternative take which is that it is kind of like a lookup table
4102180	4108420	But the prompt itself is a key that maps not to a specific sort of response, but to potentially like a function
4108740	4112660	Yeah, and a vast space of functions and france wash relay had a really good
4113220	4116180	Sort of blog post where he kind of goes more into the details of this viewpoint
4116340	4122420	But I think that that really, you know resonates with my intuition of how these things behave where it's not literally looking up like
4122660	4124660	A key value in a hash table
4124740	4129140	It seems more like it's these models have learned over tremendous amounts of data to compress that data
4129140	4134260	They have to learn, I think more abstract functions that help to explain that data and therefore they're learning functions
4134420	4136420	So they're approximating some kind of function
4136660	4138180	Or a vast family of functions
4138500	4142260	And I think the prompt really acts like as a key that essentially activates a particular function
4142500	4148100	And so you can kind of think of you know in the classical world where one neural network equals one function like basically it's mapping from
4148500	4153060	Images to image net labels now like foundation model in the foundation model regime
4153140	4158260	It's like one foundation model is essentially kind of like a giant database of lots and lots of different functions
4159300	4162340	That's basically activated selectively based on the input with prompt
4162820	4165380	Um, and I do think that you know based on this
4165460	4169780	I think it's definitely possible that with enough data from the world enough experiential data
4170020	4176420	That these foundation models can learn sort of a basis set of dynamics and transitions that explain how the world works
4177060	4182980	And essentially if it does learn these transitions, um, for example in like the massive amount of video data that swore is trained on
4183220	4188660	Um, I would say that yeah, I would agree that they are essentially starting to approximate, uh world models
4189060	4191860	Sure. Yeah, so yeah, these are two um separate papers. So
4192420	4199620	So the first one being dreamer led by like Dan and jar Haffner. So this is um, you know example of work in the space of world models and so
4200580	4203620	Basically what dreamer involves doing is like a way of training a world model
4203860	4210820	And then also showing that you can just generate synthetic data in the small model and then optimize decision making like purely using the synthetic data
4211540	4212740	um
4212740	4218580	So we talked a little bit earlier about like partially observable mdps. So we want to like take kind of the sequence of observations
4219380	4224100	Um, and then be able to predict like the next a distribution of the next observation given some action
4224900	4228500	and so we also talked about how you might want to like compress this into like a um
4229300	4231860	More compressed representation of your of the previous observation
4231940	4237460	So basically what dreamer proposes to do and a lot of works on world modeling is to take your previous sequence of observations
4237860	4240100	And then you map them to some compressed representation
4240900	4244740	And then could predict ahead in this latent space. Um, the next uh, latent
4245540	4250740	Latent state condition on the action and then yeah, the really interesting thing about this is that now, um, you know
4251220	4254420	We can in general predict what's going to happen to condition on different actions
4254660	4257700	So now if you want to get like interesting behavior out of something like dreamer
4257780	4261540	You can then go ahead and generate a lot of synthetic data using dreamer
4262260	4265380	Or the dreamer world model and then use that to optimize behavior
4265780	4270420	And so in dreamer basically the way it's done is by doing like on policy reinforcement learning in the world model
4270500	4273380	So a lot of people call this like reinforcement learning and imagination
4273700	4279940	So it's basically, you know, you're imagining a bunch of synthetic data then using that to like use some standard reinforcement learning algorithm and then optimize
4281060	4282580	behavior in some sense
4282980	4285860	And then you could also do other things like Monte Carlo tree search
4285940	4288980	Which is like closer to like the works on on mu zero and things like this
4289540	4295300	Creativity is a little bit like a cloud and all the creativity only happens on the surface of the cloud
4295540	4299940	So there's this interesting thing that like creative discovery depends on the history of all the things that I discovered before
4300180	4304020	And typically like new discovery only happens at the end of the chain not back in in the middle
4305060	4308980	Exactly and and there's also this notion that creativity happens through knowledge
4309060	4312420	So like knowledge new knowledge doesn't come from the ether. It's kind of
4313380	4316020	There's some creative component to it, but it's it's on the
4316900	4322180	The the trodden path of existing knowledge that we already have. Yeah, that wasn't a very good question
4322500	4326020	But you see I mean so so when we talk about imagination through like, you know
4326100	4329140	Like reinforcement learning policies and and so on what we're saying is like, you know
4329220	4332820	I'm I'm imagining all of these like possible, you know worlds and so on
4332980	4337140	But I'm using the cognitive primitives of all of the stuff that I already know
4338020	4340820	Yeah, I think knowledge is definitely a compounding
4341780	4343780	compounding
4343780	4345620	artifact
4345620	4349700	That's basically like the culmination of everything all the experiences that we
4350660	4354420	That we encounter like throughout our whole life and through also like beyond, you know
4354500	4362180	Going backwards beyond like even our individual lives into like the cultural knowledge that's shared and what's really cool about language models
4362900	4364900	is that they are essentially a
4364900	4367220	codification of cultural knowledge and so
4367620	4373300	Jeff Klune has this concept of AI generating AI and so he's got multiple pillars of essentially what it takes for
4373940	4375940	You to have AI systems that generate
4376020	4381060	General AI systems and he recently added actually like as a fundamental piece of this in in his framework
4381380	4383540	This idea of building on top of foundation models
4383940	4387460	And so he says he calls it like standing on the shoulders of giant foundation models
4388100	4392980	Which is I think really just sort of the ml equivalent of building on top of cultural knowledge
4393460	4395860	There's there's a real shift recently towards talking about
4396340	4398020	um synthetic data
4398020	4401380	And as we were just saying like, you know synthetic data, it doesn't come from the ether
4401700	4408740	So we already know stuff about the world. We we build simulators and we kind of generate new
4409300	4415860	Information but in the neighborhood of things that we already know and then we kind of like iterate and fine-tune on the generated data
4416340	4417700	um
4417700	4419540	What what do you think about that process?
4419540	4423940	Yeah, no, I think yeah, maybe I'll bring it back to this like the plan to explore line of work. So, yeah
4424900	4428180	um, so so basically like the motivation of that kind of work is like
4428740	4431700	Kind of saying, you know, we might have some like previous data set or something
4431700	4433620	And we've trained our world model on that data set
4433780	4437700	But we really want to go out and like gather more data and then like improve the world model
4438580	4439540	um
4439540	4441540	By gathering more data
4441540	4446020	And so we can use things like intrinsic motivation to then give us like a reward signal within the world model
4446340	4449620	So in the sense of something like prediction error, which mentioned earlier
4449620	4454100	So now we can basically like train a policy in the world model that's now not trained for a specific task
4454420	4457380	But it's trained to go out and gather information in the world
4458100	4461700	So basically now, you know, you do this imagining in the world model to imagine ahead
4461860	4464900	But instead of imagining ahead, how do I do a task? Well, you're imagining ahead
4465140	4471540	How do I get to states that I don't know what happens and therefore we'll learn more and that's basically like the motivation behind plan to explore
4472580	4474580	um, and then and our um
4474740	4479780	Paper waker it's it's kind of like inspired by plan to explore as well as works on like auto-curricular
4480580	4482580	and so basically what we're trying to say is
4483220	4487060	You know plan to explore is good for for getting an agent to go out and gather data
4487300	4492500	Um within a single environment and you know and presumably once you've gathered enough data within a single environment
4492500	4495620	Then you can generate a bunch of synthetic data in that single environment
4496020	4500900	And then do what we discuss with dreamer in terms of like optimizing a policy for that very specific environment
4501780	4504020	um, but what we're really interested in is saying, you know
4504660	4507620	Let's not assume that we have like one specific environment beforehand
4507860	4511700	Let's assume that you know, there's some space of you know broad range of scenarios
4511780	4513380	Like we want a very like general agents
4513380	4517300	There might be a bunch of different environments and then within that what those different environments
4517300	4520260	We kind of want to be able to to handle absolutely any task
4521140	4526580	And so in the waker paper, we're basically saying like, you know, how should we gather the data within um
4527300	4532420	Within this like broad space of possible environments and tasks such that we can train a very good world model
4532820	4536900	And then once we have that world model, that's kind of like capable across environments and tasks
4537220	4541300	You know the assumption is that we can then use that to generate good synthetic data, which we can then
4541700	4543700	Um use to optimize behavior
4544580	4547620	And so maybe to talk a little bit about like how we formalize this problem
4548020	4551780	Um, so, you know, we mentioned earlier this idea of like the simulation lemma
4551860	4556260	So we basically say that or an existing work that says like in a single environment
4556580	4559380	We can bound the gap between the optimal policy
4559860	4564580	That's trained in the world model so trained in the synthetic data to the to the truly optimal policy
4564980	4568820	By the error in the world model and the distribution of states generated by that policy
4569460	4574500	So it's kind of intuitive like the world model should have, you know, low error and then we will get a good policy out of it
4575220	4579700	But then what we're trying to say is like now, let's assume we don't know what the environment is beforehand
4579700	4582100	And we also don't know what the task is beforehand
4582340	4585940	So how do we get like a good world model that can handle like all of those situations?
4586660	4588900	When we later want to go ahead and optimize some task
4589620	4590740	um
4590740	4592740	And so the way that we do this is we basically yeah
4593220	4599540	We then use this notion of mini max regret to say that the policy should have like low maximum regret across this hot entire space of environments
4600100	4602580	And then using the simulation lemma we can basically say now
4603220	4607300	Now the um the world model has to have low error across all environments
4607940	4613220	Under the distribution of states generated by the optimal policy for any future task
4613860	4617700	Um, so we're going to say like yeah, the world model has to be good for any environment and
4618260	4622100	Under, you know in any area that the policy might go to that's relevant to the future tasks
4623140	4626740	And then what we kind of say in the paper is, you know, if we want a truly general agent
4626980	4629300	We're not going to know what the distribution of tasks is beforehand
4629300	4632900	So we don't know we don't know what the reward function is. We don't have a set of reward functions
4633620	4637380	Um, you know, we're just going to kind of assume the agent has to do anything later down the line
4637380	4640900	And this is kind of like related to this idea of like open-endedness that we've talked a lot about
4641780	4645060	And so if we don't know what the task is going to be like later down the line
4646020	4650740	Then the best assumption we can do is say that, you know, it could be any reward function later down the line
4651380	4655540	Which is maybe not the best assumption because as we talked a bit earlier if you're just kind of
4656180	4659300	You know, we talked about a bit about intrinsic motivation and interestingness
4659620	4662660	And if you kind of assume the task can be absolutely anything later down the line
4663060	4667300	You're kind of assuming that, you know, the agent might want to do something completely ridiculous later like it
4668100	4672340	If you do this in robotics, that might mean the task is just to do like backflips later or something like that
4672340	4676340	But you have no interest in doing that. So it's it's not clear if that's really a good assumption about
4676740	4680180	How we should think about what tasks might be interesting later, but that's the assumption we make
4680180	4682580	So we assume the task can be absolutely anything later down the line
4684340	4687220	So so now we have to get a to the point where we have the world model
4687700	4693140	Which is good for any environment and under the distribution of states generated for any task or any optimal reward function
4695300	4698340	And to do this we basically like leverage two different techniques
4699300	4701300	So to generate this state um
4701620	4705940	So to handle the aspect that we don't know what the task is later down the line. We assumed that um
4706820	4711940	We have an intrinsically motivated policy that's basically seeking out the maximum uncertainty in any single environment
4712500	4714500	And so basically if if this um
4714980	4718900	If this intrinsically motivated policy is seeking out the maximum uncertainty in every environment
4719380	4726020	Um, it's kind of like estimating for us what the maximum uncertainty is in every environment because it's like actively finding uncertainty in every environment
4726820	4730180	So now we have a policy that's finding like the maximum uncertainty in every environment
4730820	4734420	And then if we want to optimize this like mini max criterion across environments
4734820	4738260	We kind of need the maximum uncertainty to be low across all environments. So
4738980	4744340	So we kind of have to have like um, you know, this policy isn't able to find like lots of big errors across all different environments
4745300	4746260	um
4746260	4747220	And so
4747220	4751540	Basically, you know, what we could think like what what happened in practice is, you know
4751540	4753140	You can imagine there are a bunch of different environments
4753140	4756020	Some which are like a low complexity and some of which are high complexity
4756900	4760420	And if we just kind of naively sample from those two different environments data, you know
4760500	4763300	Our world model is going to very quickly get good at the low complexity environment
4763940	4769060	And then it's going to leave a lot more data from that high complexity environment to eventually get the errors low in the high complexity environment
4769940	4774660	So to bring it back to the title of the paper, which is weighted acquisition of knowledge across environments for a bussiness
4774980	4777140	So the idea here is that we're basically going to
4778100	4784180	Change how we sample that distribution of data across environments to make sure that maximum uncertainty stays low across environments
4784580	4789860	So what this ends up looking like is, you know, we're going to sample less data from the environment that has lower complexity
4790500	4794340	And then we're going to actively sample more data from the environment that has higher complexity
4794660	4797620	Such that we we bring those errors down on the higher complexity environments
4798180	4803540	And I guess it's a little bit different to existing works on curricula because normally in curricula like automatic curriculum learning
4804180	4806100	You kind of assume that you have some reward function
4806100	4812740	Which is telling you how well the policy is doing in each environment and use use that specific like metric of how well the policy is doing
4813540	4817060	To determine, um, you know, where the policy has more potential to learn
4817380	4820580	But because we're making this assumption that, you know, we don't know what the reward function is
4820740	4823860	We're trying to get a general agent that can kind of do any task any reward function
4824740	4827700	Um, we don't assume that we know that reward function beforehand
4827700	4831620	So we can't use reward as a metric of saying like I need more data from here or I need more data from here
4832420	4835540	But then kind of the main argument of the paper is showing that, you know
4835620	4838820	If we just think about this in terms of prediction error in the world model
4839540	4843140	Like we can actually use that as like an intrinsic motivation signal to say, you know
4843220	4848180	Does the agent need to gather more data from this environment or from this environment without access to reward function
4848740	4850740	and so we could kind of think of um
4851060	4857300	This work as kind of a more general approach to automatic curriculum learning in the sense of like we're not assuming that you have a reward function beforehand
4857300	4859300	We're kind of agnostic to what the task is
4860100	4864100	And because and to kind of distill that knowledge that's that's gathered without the reward function
4864180	4866980	We use the world model as a mechanism to like distill that knowledge
4867300	4871300	Because if you just like naively have an agent gathering information with no reward function
4872340	4874820	You know, how do you how do you kind of put that knowledge into the agent?
4874820	4877140	And we kind of argue the best way of doing that is the world model
4878260	4882980	So that's kind of a summary of like the waker paper and what like what the ultimate algorithm ends up doing
4883540	4888340	So I mean essentially you're doing a high entropy search. So you're you're leaning into
4889140	4892020	Areas of complexity and you're building a higher complexity model
4892020	4896500	Which goes against the grain of the intuition of like Occam's razor that should have simple models
4896500	4902020	So you're you're almost deliberately saying no, I want I want to model the the complexity and have more of that
4902340	4905620	And then the other interesting thing is like from from a curriculum learning point of view
4905620	4910420	I think traditionally we did explicit curriculum learning and you know, we might have some
4910980	4914580	Principles around having a monotonically increasing curriculum of complexity
4914980	4916980	Whereas here by leaning into
4917940	4921300	Environments where we do worse on so we're selecting them based on prediction error
4921460	4926740	We're actually implicitly getting a kind of monotonically increasing complexity, which just happens to work really well
4927540	4931460	Yeah, I guess actually it actually almost ends up being in the opposite direction
4931460	4935140	So so by leaning into the the the higher complexity environments more
4935140	4938420	We're kind of saying let's prioritize the harder environments more to begin with
4938420	4941700	So let's like gather more data in the higher complexity environments
4942660	4947140	Um, you know, because I guess intuitively if you kind of want to be good across all environments
4947620	4950180	You kind of need more data from the higher complexity environments
4950500	4954260	And we don't really explicitly think about an ordering of going first from easy to hard
4954740	4957540	Um, I guess that maybe there is a something to look into there because
4958180	4959060	You know
4959060	4962660	Like a lot of these works go from low complexity to high complexity because it's kind of easier to learn
4962660	4967540	An initial policy that can kind of do something in the low complexity environment and then you build up the complexity
4967940	4972100	Gradually, um, but I think that that idea is most useful when you know what the task is
4972100	4974980	So you could imagine if the task is like low commotion if it's walking
4975380	4978180	You kind of want to first learn a policy that's able to walk on flat ground
4978260	4982420	And then maybe gradually build up the complexity like add and bumps and then eventually it can walk on like a very
4983060	4985940	Complicated terrain so it kind of makes sense to go from low to high complexity
4986500	4988500	um, but in this work we're focusing on
4989300	4993540	purely intrinsic motivation meaning that the policy is not trying to learn a specific task
4993860	4997860	It's trying to just seek out um uncertainty and like reduce uncertainty
4998260	5000180	And so we don't really have the notion of you know
5000180	5002820	You first need to be able to learn how to do something on an easy
5003380	5008340	An easy environment and then move towards harder environments because there is no specific task that we're trying to learn
5008660	5013860	And so I think for this reason, you know, we wouldn't didn't really focus on this notion of moving from easier to harder environments
5013860	5017140	So that actually, you know, we're consistently something more data from the hard environments
5017780	5021700	And I guess I think this relates or I think this is something that you brought up when we when we worked on this is like
5022420	5028260	You know, I think we can really relate this idea to like a lot of different contexts including things like like language models, for example
5029140	5030020	um
5030020	5034740	So, you know, you can imagine if I'm training an llm. I don't really necessarily have this, you know
5034820	5037300	Not really a reward function in some sense. You're just trying to
5038100	5040100	Do like unsupervised prediction
5040340	5047060	And so, you know, we could for example take the prediction error of like a language model and a bunch of different domains and say, you know, the language model is
5048020	5051460	Not very good at predicting a language about some certain task or something like that
5051940	5056340	And you know, we could say, you know, and intuitively the same thing kinds of holds if it's not very good at predicting, you know
5057380	5060900	What the next token is in french like we should presumably gather more data in french
5061460	5065140	And that so that kind of gives us a way of like actively gathering the appropriate data
5065620	5069620	Um, and so yeah, I think this idea of like gathering more data based on certainty
5069620	5072340	Obviously is a very general idea like the idea of like active learning
5072900	5074500	Um, but we kind of like
5074500	5078260	Specialized that into thinking about how do we think about this in terms of the reinforcement learning setting?
5078740	5082900	It might be interesting to talk about as well like sort of because we looked at some of the metrics as well, right?
5082900	5086580	The environment complexity metrics. Yeah, we don't have the external notion of difficulty
5086660	5093140	But we we also did look at sort of the emergent, uh, curriculum. Yeah. Yeah. Yeah. Gotcha. Yeah, so I guess um
5093780	5095220	So it kind of depended on the environment
5095220	5099220	So in some environments, you just kind of got this like very straightforward behavior of like, you know
5099300	5101860	Consistently gather more data in the more complex environment
5102660	5106180	um, but because we're we're actively trying to gather data, um
5106900	5112900	Of the the environments for which the uncertainty is the highest kind of this curriculum could change over over the course of training
5112980	5115220	So so what happened in some of the other environments?
5115220	5118740	For example, is that initially all the environments are just like high uncertainty
5119060	5124420	Like there's like all environments are kind of misunderstood therefore like sample all environments like equally more or less
5124580	5126580	To just get a rough understanding
5126740	5129540	And then you know as as the model would improve on the simplest environments
5129540	5133380	Then we would see like more and more emphasis towards sampling the highest complexity environments
5133700	5137780	So I guess in that sense we would get something to more like kind of what you said in terms of like a standard curriculum
5138020	5141140	But a bit different in the sense of like initially everything is uncertain
5141300	5143300	So we're just going to sample everything uniformly
5143780	5146340	Um, but then we kind of get a better understanding of which of the environments
5146420	5151780	You know the uncertainty remains high on these higher complexity ones and those are the ones we need to like go out and gather more data
5152100	5152820	Yeah
5152820	5154100	I mean I can see this both ways
5154100	5157380	I mean certainly from like a Bayesian optimization point of view that there's something to be said for
5157540	5162340	Um, you know, this is where I'm uncertain going gather more data where where I have highest uncertainty
5162820	5165300	And uh, as you say like traditionally in curriculum learning
5165540	5168500	We are told that we need to have monotonic increasing complexity
5168500	5172500	But as you just said that's when we have a particular task in mind now neural networks
5172500	5175780	They're a little bit like a block of clay aren't they so you know, it starts off with
5176260	5181460	Abject complexity and then we do stand, you know, we do um stochastic gradient descent and we chip away at the clay
5181540	5187300	And we kind of build we sculpt a statue that that that we want to build and I'm just trying to get an intuition here
5187300	5189300	So like with this maximum entropy
5189540	5191540	Search, you know like high entropy search
5191620	5193060	What we're doing is is we're saying okay
5193060	5198660	Well, here are some complex models and these models must contain motifs that tell us a lot of information
5199060	5200900	It's a little bit like the elo algorithm in chess
5200980	5204740	You know, you actually get information gain when something surprising happened
5205060	5209700	So here's a big block of complexity and I'm going to try and infer
5210260	5213940	What the motifs are in that complexity that that explain the information that I'm missing
5214180	5219220	I think that a lot of this ultimately traces back to sort of there's like this like fundamental pattern
5219620	5223140	towards uh, I think that like ties a lot of these ideas around active
5223620	5230340	Um active experiment design or like active sampling, which is and all these autocurricular methods, which is you essentially want to devise
5230900	5236340	Uh, what you know nowadays we call a self supervised objective or self self supervised training algorithm
5236740	5241860	Um, where essentially you have the system essentially use signals. It produces itself
5242340	5248420	Um during the training or evaluation process in order to drive itself forward in terms of deciding what future data to train on
5248740	5255060	And so, you know, we sometimes call these kinds of systems autocurricular as well because it's automatically generating this curriculum of
5255380	5259220	Tasks to train on and I think the sort of like the fundamental connecting
5260260	5264020	Uh pattern here is just that this the signal that we use to drive the training
5264180	5268660	It's always going to be based on something like, uh, an uncertainty signal or, um
5269060	5273300	Going back to the open-endedness literature something like a classic notion of interestingness
5273780	5278580	And I think there's just a lot of different possible choices for this metric and so
5279380	5281380	One for example, we talked a lot about
5281380	5282260	Minimax regret
5282260	5287620	So regret could be one of these driving signals because it measures the existence of a performance gap and therefore
5287700	5291780	Probably an information gap as well in terms of learning to master those tasks with high regret
5292260	5298340	But also uncertainty is also another one it ties back to novelty because novel environments you will be more uncertain within
5298740	5303540	And so there's fundamentally lots of different sort of branches of these autocurricular that you could use
5303540	5307780	Depending on this search objective that you use to drive this exploration process
5308900	5312820	Can we contrast this to you know, like, um, large language models that they are self-supervised learning
5312980	5318340	So, you know, we do this self-supervised objective, you know, which is like, you know, typically predict in the next word
5318580	5321460	And it's a similar thing with, um, self-supervised, um image
5322260	5329300	Learning now the difference is with that is you're talking about a principled way of, you know, seeking specific information
5329620	5334900	You know with, um, let's say high entropy and that would lead to an implicit curricula
5335220	5338980	Whereas with language modeling language modeling, there is no implicit curricula
5339220	5345780	But I might argue that there kind of is because the way the model does this continual learning, um, it might regularize itself
5345780	5349860	So if you give it sort of surprising and weird information, the language model might just kind of brush it off
5350020	5354500	And if you reinforce things that it already knows then it's almost like a stream of channels, you know
5354500	5358420	It'll say, okay, you know go and go and pay attention to that. So it's almost like it's implicit
5358660	5362660	Yeah, and I would say that in some ways it's almost explicit in terms of how we design these systems
5363220	5366580	A lot of times like if you look at, for example, open ai's job listings
5366740	5371860	They're actually hiring specifically for experts in different domains to essentially create the next
5372500	5376500	Batch of supervised data to train or instruction tune their models on
5376900	5381060	For example, they hire biologists or they hire people with legal expertise to generate this data
5381380	5387220	And you can think of this essentially as a human steered or human driven version of this active sampling process, right?
5387460	5394340	Because essentially they know that the model tends to get high perplexity or they don't it doesn't perform as well on this domain of tasks
5394500	5400740	It doesn't get as high of an LSAT score as it could and so you can essentially, you know, it's it's beyond an algorithm at this point
5400820	5406100	Right, it's kind of the super algorithm where you have the system designers now also being part of the data collection process
5406580	5408580	and in a way
5408580	5413540	supervised learning is really just sort of one point in a continual learning process where, you know
5414100	5418500	Classically, we just looked at one step of this which is here's a batch of data train on that but really
5419380	5423460	Building machine learning systems, especially nowadays. Everything's in production. These are all live systems
5423700	5427540	You have to keep it up to date. You have to keep it continually generalizing to new knowledge
5428820	5436100	Like chat gpt or clod or gemini and so really it's sort of this pattern over and over again in sequence where you collect a batch of data
5436500	5438820	Train your model on that collect the next batch of data
5439620	5441620	You know continue training your model on that
5442180	5448660	And really you want to be selective about what the next batch of data is because obviously if you just retrain it on the previous batch of data
5449220	5451060	It's going to overfit to that data
5451060	5456980	Beyond a few epochs or it's not going to you know get as much novel information from it just because it's already trained on it
5457220	5461140	So you do want to selectively actively collect the data
5461540	5466020	And so I think we kind of almost explicitly already do this at a systems level
5466820	5472340	And I think the next frontier is really just having systems that self-improve in this way where they can start to guide
5472660	5478660	More of their own active data collection. I love this way of thinking about it. You know like gbt4 is a memetic intelligence
5478660	5482020	It's not just like you know a bunch of weights on on a on a server somewhere
5482420	5486020	And so you could argue, you know, there's this concept called graduate student descent
5486020	5489940	Which is what happens in academia or even as you just articulated with open ai
5489940	5493060	It's a little bit like an epic mechanical turk right where you know
5493780	5494900	They are monitoring the logs
5494900	5500100	They know when things go go badly and then they lean into it in the same way you are they go in higher experts
5500100	5502980	And they kind of like add more and more data in all of the holes
5503140	5506180	And eventually there are no more pockets of like abject failure
5506180	5511460	It just it just appears to work really well for everyone and people start to say that it's you know, generally intelligent
5511700	5514980	So yeah, so there's this interesting systems view of of intelligence
5515220	5517860	Yeah, it kind of starts to mimic just the scientific process in a way
5518420	5522260	Where we're sort of we were putting a lot of hope in the model to basically be able to distill
5522820	5525700	information from sort of the net news batch of data that we collect
5526580	5528820	You know that we know the model currently doesn't explain well
5529060	5535140	And we we we put a lot of faith and gradient descent in order to basically be able to come up with updates to the weights
5535300	5541140	That better explain that data. So we're kind of we're kind of already treating the system as almost like an automated
5541860	5545140	Scientist or an automated version of this like continual
5545860	5548420	process of creating theories and explanations about the world
5549300	5551220	But of course, you know
5551220	5555860	Humans are still much better at language models at doing this or large models at doing this
5555860	5559220	So I do think there clearly seems like a huge gap in terms of
5559540	5564260	Well, we still have work that needs to be done in order to build systems that can actually build much more robust theories
5565140	5568180	Based on like net do new data and even seeking that out as humans do
5568580	5574100	Interesting and certainly, you know in in this broader memetic intelligence. We are still the sources of agency
5574820	5579300	But um, we were just sort of talking a minute ago about there being two types of ai
5579380	5582980	You know, there's there's an ai where we are the generating sources of agency
5582980	5587620	But there might potentially be another ai in the future where that that is the generating source of agency
5589220	5591700	Yeah, I so I think that um
5592340	5594820	This kind of ties into my my the framework
5594820	5597380	I personally used to think about open-ended systems as well
5598020	5603780	Where I think that you know at a high level you can you can study ai sort of in silico
5603860	5610420	You can study it in systems that you control that you design and that you try to like have the ai model self-improve within
5610740	5612740	And so you can try to build
5612980	5618100	Systems that self-improve within silico and that's going to lead to potentially some issues around like the grounding problem
5618260	5624900	Where essentially it starts to the auto the auto curricular exploratory process starts to veer into parts pockets of the design space
5624900	5627060	That are not relevant to tasks you care about
5627460	5632020	Um, and so this is kind of the danger of like generating open-ended systems in silico
5632100	5635860	And I think it's very similar to potential dangers of generating agi in silico
5636260	5637300	um
5637300	5641700	And I think the alternative is really just what are existing intelligent systems
5642100	5647860	And how do we actually amplify the efficiency the efficacy of those systems the intelligence within those systems?
5648100	5654180	And so you can kind of think of like sort of the entire enterprise of ai research as do we want to generate like ai or intelligence from scratch
5654500	5656500	Or do we want to build tools?
5656500	5661300	You know motivated or inspired by human intelligence and other intelligent systems and use that to further amplify
5661620	5664260	existing intelligence like human creativity human intelligence
5664740	5668180	Could you argue because if intelligence is a divergent search process?
5668900	5672900	You might be tempted to think that well if we had loads of tools to help us share
5673220	5678500	The models and help other people discover the models that i've created that that will help us generally be more intelligent
5678660	5685380	But could you make the counter argument that i'm actually sequestering agency or stealing agency from other people because rather than thinking for themselves
5685620	5688020	And discovering novel models. They're just going to use my model
5688420	5693300	Yeah, I mean I think that in the best case scenario you're building systems that essentially, you know
5694100	5700340	Not you know to to think about how you know as existing systems nowadays can build on the shoulders of foundation models
5700580	5706740	You really want the to build models where even humans can stand on their shoulders where the humans can basically leverage the
5707060	5708420	existing expertise or
5708420	5714180	Automative capabilities of those models to then like move further beyond what they're naturally capable of doing
5714500	5718740	And really that pushes the frontier of the knowledge that we can create as a civilization
5718980	5726020	And so you're already starting to see this where there's some recent studies that show for example like junior software engineers that use systems like
5726580	5728900	Chat gpt to help them with coding at work
5729140	5732740	They actually now are starting to match the performance of more senior engineers
5733300	5734900	Because it sort of levels the playing field
5735060	5740980	But that also translates into just like net more productivity per software engineer. And so
5741940	5744100	I think that it's more just unlocking sort of
5744740	5749940	Existing bottleneck and how productive each individual can be and really just means that each individual can create a lot more value
5750020	5752020	Can discover a lot more knowledge
5752340	5753540	Than before
5753540	5757780	Okay, but I mean do you think that it creates a tendency towards boilerplate though
5757780	5760660	So we're more we're more efficient at doing things that exist
5761060	5764420	But you know like on on the frontier we might have a slowdown
5764580	5768020	There's definitely the danger that it can lock you in to certain patterns
5768180	5772500	Right. So basically if chat gpt always returns a certain boilerplate that might have an anti pattern in it
5772820	5779700	Um, if that stays around it could self-amplify and then future generations of programmers might just adopt that by default because it's what's already
5780100	5784420	Generated by autocomplete. So I think that that's also another really interesting realm of questions
5784500	5789940	Which is basically how do you um, how do you avoid these kinds of uh, these local optima?
5790340	5793300	When you start to train a model on its own outputs
5793620	5798420	And I think again like sort of the solution will start to look like some form of novelty search or exploration
5798980	5803860	Makes sense. Okay. Um, what do you guys think about like, um, you know academic academia versus industry and
5804580	5808980	Some say there's a bit of a brain drain from academia. Totally. Yeah, I think there's like a very
5809700	5813780	Very clear trade-off between the two and they said they both have like fantastic things going for them
5814260	5817060	And I guess the trade-off being you know academic freedom
5817780	5822260	An academia and be able to like individually pursue ideas like purely for curiosity's sake
5823140	5826020	And um, you know, that's something I've really loved about academia
5826020	5831620	But I guess you know, I guess the general trend and machine learning research at the moment is kind of towards like larger scale projects
5831700	5832660	especially
5832660	5838020	You know a lot of the properties that we might want to see kind of only emerge when you expend a lot of compute and therefore
5838900	5840820	You know a lot of interesting research can kind of
5842180	5846180	Maybe not only be done in an industry, but it's a lot easier to do some kinds of research in industry
5846660	5849540	And so I think this kind of leads this trade-off of do you want freedom?
5849540	5852580	Or do you want to be on these like larger projects that are potentially more impactful?
5852980	5855700	And so yeah, I've really struggled with that trade-off. I think they they both
5856180	5861060	Have big pros and cons. I don't know what you think minty. Yeah, I I think that um
5862180	5869700	Industry is I I think I like at a very like first word rough approximation would be to say that industry focuses much more on
5870180	5872180	um exploitation and
5872660	5876500	academia is where you know in principle you should get a lot more exploration
5877460	5879460	But I do think that currently
5879780	5880660	both
5880660	5885460	Systems are kind of like entwined in the same sort of reward function at a high level where essentially
5886900	5889700	You know if if if you're if you care a lot about
5890340	5896820	Citations and a short-term greedy algorithm for maximizing citations would be to focus your research efforts on
5897540	5899540	sort of whatever topic is
5899860	5902180	Trendy or hyped at the current time
5902260	5904900	And so like I think you see tons of people obviously
5905380	5908660	Working on language models partly because it really is a fascinating subject
5908740	5914260	And it really is like the most powerful form of deep learning we have so I understand why everyone's working on it
5914420	5916260	but I also think that um
5916260	5920180	A lot of it is kind of you do get this sort of rich gets richer effect around
5920580	5927140	Different topics that people tend to gravitate towards and you lose a lot of the exploration that you should otherwise have
5927620	5928340	um
5928340	5933860	And that's partly because you know like both industry and academia are at some level optimizing for a similar
5935460	5938020	Sort of reputational status or citation count sort of metric
5939060	5942740	And so I think that's an issue, but I also think that in some ways
5943140	5945620	Uh industry you could say has
5946340	5951300	Additional benefit where I do think that from like a short-term point of view industry is better poised to
5952020	5953700	make certain
5953700	5959220	Higher impact research not just because of the resources available to industry, but also partly because
5959780	5961140	um
5961140	5963460	Sort of industry, uh, you know
5963460	5966660	Rides or dies based on whether the actual research artifact you produce
5967380	5973300	Is useful and so I think that's like a very powerful reward function that is not necessarily true for academia
5973540	5976340	Um, and then sort of on the to take the counter position
5976340	5982020	I think academia obviously, you know, you have a lot more freedom to just explore ideas that don't need to be on that critical path
5982020	5986660	For value creation immediately and so it gives you a lot more scope to potentially find like the next big thing
5987060	5991540	And so I think really it's about like if you want to if you want to take the bet that you can
5992180	5994260	You know play a part in discovering the next big thing
5994740	5998660	Then and that's that's suited to your taste for research then academia makes more sense
5999060	6001060	but if you know, um, you want to
6001620	6006580	You want to maximize the probability you'll have a higher impact in sort of like a near horizon line of work
6006820	6011780	Then industry is definitely I think a better bet rich Sutton, you know, he had this bitter lesson essay
6012100	6017220	And he made the argument that it's just all computation and there are no shortcuts and you can even think of you know
6017780	6019780	Maybe we're not very intelligent
6019860	6024820	Evolution has just been running for a very very long time and we are the result of that
6025060	6030180	So in in a sense, do you think that we could make strides in intelligence?
6030580	6034900	You know just through ingenuity or are we always going to need loads of computer power?
6036260	6041220	This definitely like makes me think of like the recent trend that we've been seeing even in like kind of the reinforcement learning
6041220	6043780	Literature lately, which is like these kind of large scale
6044900	6050740	Like mostly industry projects that are kind of they're even ditching the idea of doing like sequential decision making so
6051860	6055700	You know you have all these algorithms that are like, you know optimal planning and so forth
6056020	6058020	But we're kind of seeing
6058020	6063940	A trend towards you know, even ditching that complexity of algorithm and just going straight to just copy what the human did
6064260	6066740	and so kind of reducing the problem to you know
6067620	6069620	essentially no real algorithmic
6070340	6073220	Innovation and more just like can you gather enough expert data?
6073700	6076580	And I think yeah, I guess the reason why that trend is occurring is
6077140	6079140	Is I guess like you said there's kind of been
6079700	6082020	You know the bit lesson kind of said that you know
6082420	6086180	Just being able to scale with more data and more compute is kind of the most important thing
6086660	6091540	And a lot of the more complex algorithms, especially around like reinforcement learning are actually like quite challenging to scale up
6092020	6094980	especially like online reinforcement learning if you want to go out and like
6095780	6099220	Actually have an agent like actively collecting data in a bunch of different environments
6099620	6103140	And updating itself online like that's so much like engineering infrastructure to set up
6103540	6106900	And so I think there's this this trend towards just like the simplest algorithm possible
6106900	6111220	Which is like not even reinforcement learning not even planning just copy an expert
6111700	6113460	but I think that
6113460	6117460	That's like you kind of said earlier with like this kind of like short-term exploitation
6117940	6122340	I think this is you know, it it kind of makes sense to exploit this now and push it as far as possible because
6122660	6126900	You know, it's very easy to just train a large transformer and then gather as much data as possible
6127220	6130980	And I think in areas like robotics, we haven't really seen like how far can that go like
6131300	6137780	Can you actually get a generally useful robotics platform just by gathering more expert demonstrations and training a larger and larger transformer?
6138100	6142980	And so I think it does kind of make sense that why like a lot of industry projects are pursuing that because we don't really know
6143380	6147380	You know, will will that actually hit a bottleneck or or if you just gather enough data
6147780	6149540	Will that will that kind of be sufficient?
6150100	6151300	And I guess like
6151300	6155460	You know, you could argue that I think it's probably true that there must be a better algorithm out there
6155700	6157860	That can and principle do this in a more efficient way
6158340	6162100	But I guess if it's just easier to just gather more data and just do imitation learning
6162180	6164660	I can see that there's at least a business case for trying that
6165540	6171460	So I guess I'm on the the opinion of like, you know, there must be a more efficient way of getting to like a more intelligent system
6171780	6176660	But it's not necessarily clear that just scaling like raw supervised learning or unsupervised learning
6176980	6180180	Like won't get you there and so it does make sense to pursue that first
6180420	6185780	But kind of what I hope and expect to see is that eventually pure imitation learning or pure unsupervised learning will kind of
6186180	6188980	Run out of steam and everything will plateau and I think at that point
6189460	6194020	You know, then these like more complicated algorithms about gathering more data reinforcement learning planning, etc
6194020	6195620	Will really come into their own
6195620	6198820	And so I guess this again relates back to like the academia industry trade-off like, you know
6198820	6202260	A lot of the projects in the industry are just going to kind of be exploiting gathering data right now
6202740	6205140	Whereas maybe there's a lot of scope to do these kind of more
6205620	6210660	Exploratory exploratory projects where maybe that will get you to like the next frontier a few years down the line
6211140	6212740	I don't know what you think about this
6212740	6214820	Yeah, I definitely think that um
6214820	6219460	Yeah, just like treating everything as just supervised learning it does tend to work because we have large data sets
6219540	6223940	But um, I think again like the challenge is just at some point we will run out of tokens
6223940	6230420	We'll run out of data to train on and so that's why the self-improving more self exploratory systems will be more and more
6230740	6233460	I think paramount to like driving performance even further
6233620	6237780	So if we want to sort of break beyond sort of the token limit of like the data that's available now
6238020	6241060	We actually need these systems to generate their own tokens their own synthetic data
6241700	6246420	And that's that's where like the self play auto curricula exploration types of algorithms will start to
6247620	6251860	Become more and more prominent and obviously you need an environment in which to do that exploration
6252180	6254180	And that's where the world model
6254580	6260020	Line of research is going to be very powerful just because that allows you to really sort of milk all of the value within
6260580	6267060	The existing previous data you have seen by creating these role models where you might be able to do like counterfactual trajectories and really learn much more
6267460	6269460	Um, amplify the existing data you had
6270020	6275460	Yeah, I mean, I think one of one of the key things for me, um is modeling dynamics. So, um
6276340	6280420	It's quite interesting actually with the human knowledge things or even looking at the innovations from from deep mind
6280420	6283540	You know early versions of alpha go were bootstrapped with human knowledge
6283780	6286260	And then there was the alpha zero. So it was actually doing what we're talking about
6286260	6290340	It was actually discovering knowledge on its own and um in principle. That's a great idea
6290420	6296420	But of course like an irrestricted domain, it's tractable but in the real world it isn't and I'm not sure whether it makes sense to use
6296500	6302260	The the computation and you know information metaphor for the real world and humans and so on but but the basic idea is that
6302420	6305220	We are all real agents the universe is a massive computer
6305540	6310340	We're discovering all of this knowledge and then we're bootstrapping that into a machine learning algorithm
6310660	6316180	And then the question is well, if you kind of just capture the thing now without the dynamics that produced it
6316420	6320180	Um, will the system be robust and could you still um, you know, kind of
6320260	6324660	Carry on as we were in the real world if that makes sense. So um, but yeah
6324660	6327860	The interesting thing with the work you've done is is that you are modeling agential systems
6327860	6334500	You are modeling dynamics, but could that be used for you know, much more complex tasks like the real world
6335140	6340260	Like simulating much more complex systems in the real world. Yeah, I think that if you if you
6341380	6346020	So I think that just purely imitation learning alone is not really going to get you there
6346900	6350260	But I think that if you can if you can imitate
6351380	6354020	So one is sort of finding the set of tasks
6354100	6358660	I think that if you find the set of tasks or reward functions that could be relevant
6358820	6364900	Then you can start to simulate things that are otherwise really hard to capture by just purely imitating historical trajectories
6365380	6372740	So for example strategic adaptation type of behaviors are really hard because those are sort of an open-ended space of behaviors where
6373460	6376740	If you basically have like a stock market, for example, that's a really good example
6377060	6379620	Where if you have a stock market, that's a very open-ended system
6379780	6383380	And like different traders will have different strategies that are best responses to each other
6383620	6387300	And then over time the set of strategies evolves over time in an open-ended way
6387620	6393540	Um, you know trading strategies that worked 10 years ago probably won't work very well today because people have sort of um
6394340	6399700	They've sort of figured out those strategies. And so they won't be very competitive. And so um
6400340	6403460	I don't see an an imitation learning system being able to sort of
6404100	6408740	Um, generalize to that level of complexity just because by definition it's imitating previous
6409220	6415060	Uh trajectories and therefore strategies. So I think you need some notion of like a um a more uh
6415540	6419300	More interactive trial and error learning that allows for strategic adaptation
6419380	6425460	And that requires some notion of a payoff or a reward. And so you kind of need to have this this idea of um
6426900	6428900	You you can't just purely I think learn
6428980	6430180	Uh
6430180	6433060	A model of something like the stock market just based on previous data
6433060	6437060	You really need to have more inductive biases around uh, sort of you know
6437380	6443620	What creates a payoff or what the actual reward function is for each of the traders? Uh, but that might be something that you could
6444100	6445300	um
6445300	6447300	You could learn over time, but I
6447700	6451940	But maybe not in the yeah, so this is kind of like this is not very coherent
6451940	6456260	But I feel like uh, you might need something that looks more like learning over space of programs
6456500	6459860	That starts to encompass different kinds of uh tasks
6460020	6464660	And then you can basically simulate those tasks to completion with agents that can essentially
6465060	6467140	Uh try to self-improve against other agents
6467860	6473700	The stock market I think is a wonderful metaphor for we're talking about and for for two reasons first of all from the grounding reason
6473940	6476980	Because you know like the the the the the memetic world is very ungrounded
6477060	6481460	And that's why we develop as humans lots of weird shared delusions about things because it's actually like you know
6481700	6483780	It can go in it can go in almost any direction
6484020	6489300	And also the concept of alpha I think is really important because a trading strategy works really well today
6489620	6494740	And then when other people learn about it it no longer provides an advantage because everyone else knows about it
6494900	6499540	And I feel it's the same with language models. So, you know, like gbt4 pros was really novel and cool
6499860	6503300	It was great to you know, have like a ted talk speech when it came out
6503620	6506500	And now it doesn't seem cool anymore because everyone's using it on linkedin
6506740	6510340	So it's almost like that we need to have this in like continuous
6510900	6514420	creative evolving process producing new sources of alpha
6514820	6520180	And the paradox is that if everyone has access to the same model it can't be a source of alpha by definition
6520340	6524500	Yeah, I guess on that like topic because we kind of talked about like synthetic data earlier
6524580	6530100	And you kind of said like you know one one mechanism towards getting like a kind of self-improving system that is able to kind of
6530260	6531060	You know
6531060	6534260	Continue to improve is to kind of like filter the synthetic data for example
6534340	6538020	So you might kind of you know have the the new system and then we generate some more data
6538020	6541940	And then we kind of have some like filtering mechanism to say that you know in the current stock market
6541940	6545540	This is this is good data or what you know, whatever system we're thinking about and then we can kind of like
6546340	6548340	Use that to enable the model to improve
6548980	6550980	You know and adapt to the new system
6551060	6554020	But something I've always like why like thought about is like well
6554020	6558660	I guess one is is it really trivial to be able to like filter that you know new synthetic data
6559140	6564340	And then two it feels like if you're just relying on like filtering existing synthetic data
6564900	6569220	Like isn't that a never to go into kind of plateau and so I guess eventually
6569220	6574260	You know we talked about how you kind of said that you do actually actively not need to go out and get real more real data
6574580	6579460	But I guess I'm kind of asking you do you think this idea of just like filtering synthetic data from a model is kind of
6580100	6584900	Sufficient to always be able to adapt and improve or is it always going to be a mixture of like more real data
6585300	6590020	Plus synthetic data filtering. I think it's the latter just because um at some point you would expect that
6590820	6595620	The synthetic data you do generate it'll start to sort of saturate like what's already in the model
6596180	6601220	Just because the model is trained on a finite amount of information. So at some point you're just going to start to see more and more
6602340	6606900	Especially like the more likely trajectories or sequences of samples. You'll start to see that
6607620	6612820	More and more and so you're not really going to be very sample efficient in terms of searching for the synthetic data
6613540	6618900	So can you can you tell us about the results of the paper? Totally. Yeah, so basically we evaluate this um
6619620	6624660	This algorithm on a bunch of like synthetic simulated domains kind of like robotics related tasks
6625140	6628340	Um and kind of yet environments where there's like varying levels of complexity
6628420	6632260	So, you know, you might have a robot pushing around a variable number of like objects
6632340	6635460	Or maybe you have different terrain that the robot might want to um
6636580	6639540	Learn to kind of you know do locomotion over and things like this
6640180	6645860	Um and so kind of you know, the main comparison we make is like how well does waker work relative to like naive domain randomization
6645860	6651620	So how well does it work if you just like uniformly sample the space of environments versus if you do actively seek out
6651940	6654260	The environments that have this like higher uncertainty
6654980	6658740	Um and so basically what we show is that you know, if we do the waker approach
6659060	6663540	We still do like very well on average, but we consistently do better in terms of robustness
6663940	6668420	And so robustness by robustness. I mean here that that we do better in terms of the worst environment
6668980	6671780	That the agent is evaluated under and so this kind of means, you know
6671780	6676260	If the agent is able to do well in the worst environments that it that it is evaluated under
6676420	6679300	That kind of shows that it's able to do well across all environments because
6679700	6684900	Its worst performance is still good. Um, so we kind of this this shows that we we achieve this like robustness property
6685460	6687460	Which we talked about in terms of like mini max regret
6688020	6693380	But we evaluate we don't evaluate it in terms of like the true notion of mini max regret because as we talked about earlier
6693700	6699380	Actually evaluating regret exactly as difficult because that that requires knowing the exact true optimal performance
6699940	6702980	Which isn't something we can really know. So instead we we just show that you know
6703060	6708980	The agent performs well across all environments more so than if you just like naively sampled the environments uniformly
6709780	6714580	And in terms of decomposing the performance across the spectrum of possible environments
6714580	6719540	So like, you know the ideal situation is that we have a very simple model which just generalizes
6719540	6724980	So we happen to have found the golden motif, you know, there's a spectrum of correlations almost all of them are spurious
6725060	6728420	But we've just you know, just by through some sheer magic
6728660	6732340	We found the best motif to work in all situations. Probably that's not quite true
6732420	6737460	Probably there are some good generalizing motifs and the model has also kind of like memorized the long tail
6737940	6742980	And that there's some degree of like, you know, it works really well on on the test set that might not out of domain distribution
6743140	6746100	Do you have any like way of reasoning about what that is?
6746980	6748260	um
6748260	6749540	so
6749540	6753620	Yeah, I agree. I guess there's like not necessarily it's not necessarily the case that by like
6754100	6756420	Focusing more on these like long tail examples
6756500	6761620	That's necessarily the best way of training the best model because like you said like maybe it happens to be the case that
6762020	6766020	If the model is trained on some certain subset of the tasks like that will actually generalize better
6766100	6768820	but but I think in practice, that's not something we can really um
6769780	6775220	Really know how to you know, like optimally select the best kind of set of tasks that will generalize well
6775540	6779060	And so we do focus more on on like, you know, these these kind of long tail tasks
6779060	6782260	Or like the ones that we might see rarely and therefore have high uncertainty about
6782980	6783780	um
6783780	6786500	In terms of like the the out of distribution generalization
6786580	6790980	So so we do also do some experiments like looking at how well does the model generalize out of distribution?
6791860	6794900	And basically what we show is that if we train the model in this way
6795220	6798420	And then we give it some more environments that hasn't seen at test time
6798900	6803300	Um, if the environments are more complex then then we've seen sorry hasn't seen at training time
6804180	6808500	Basically like this model then generalizes better to out of distribution environments that are like more complex
6808500	6812260	Which is kind of what you'd expect because we've kind of biased something towards more complexity
6812420	6815540	We're able to generalize better to out of distribution environments that have higher complexity
6815940	6820420	And then I guess the question is like do we care about out of distribution environments that have higher complexity?
6820420	6822900	Like what about the out of distribution environments that have lower complexity?
6823540	6825540	and I would argue that you know
6825940	6830180	Basically the lower environment out of distribution distribution environments that have lower complexity
6830180	6835540	Like we would already expect that the model is able to do very well at so so there's not really much of a difference there because you know
6836020	6839300	Almost any reasonably trained model can handle the very simplest environment
6839460	6843780	So what we really care about is can we generalize out of distribution to like higher complexity environments?
6844180	6847140	And so by biasing the something towards the higher complexity environments
6847140	6851060	We do show that we're able to generalize further out of distribution to even higher complexity environments
6851300	6854420	Okay, but is there any way of knowing whether it's kind of like
6854820	6859860	Memorizing the high complexity instances or whether it's still learning abstract motifs and generalizing between them
6860180	6866260	Yeah, that's a great question. I think that's a really interesting question generally for ml as a field right now
6866260	6868260	Which is better evaluation benchmarks for
6868900	6871460	Generalization within different kinds of models
6871780	6877940	Um, and like we we alluded to earlier. There's kind of this issue of data leakage between training and test set
6878100	6883540	Which is um, which is definitely an issue that is currently happening with large language models
6883940	6888820	Um, it doesn't take away from the impressiveness of these models because clearly there is a strong generalization
6889140	6894100	aspect to their behavior, but I do think that in terms of measuring performance on specific benchmarks
6894420	6897940	Um, we really need to solve this problem. How do we have these clean data sets?
6898500	6900260	That allow us to
6900260	6905620	To truly test on inputs that the model hasn't seen at training. Um, I think in the case of
6906420	6908420	reinforcement learning
6908420	6912500	That's a bit more difficult just because usually we focus on a particular task domain
6912660	6915060	And so there's always going to be some shared similarities within the task
6915060	6920980	But obviously, uh, we didn't do this in this paper, but we could try things where we have more um more controlled
6921620	6927940	Settings where we you know change one aspect of the environment and really see if it's learning specific causal relationships between
6928580	6930580	Things that have to be accomplished in that task
6930900	6934820	But we didn't do that. Um, that I actually think would be a really interesting idea for
6935460	6937460	A new evaluation environment for rl
6937940	6941460	Yeah, I mean the benchmarks thing is just a huge challenge in in machine learning
6941940	6945220	In general, but just just to kind of round off off the interview
6945220	6949140	I mean minchie you you were talking about you're doing some work with um edgreff instead and it is amazing
6949140	6954260	I'm getting edg back on and um, you said that um, you've been looking into this kind of the interface
6954580	6956740	Between humans and machine learning. Can you tell me about that?
6956980	6962100	Yeah, so just to not say too much about it because um, it's related to current work that's happening at DeepMind
6962500	6966340	Um is just that you know, I think from personally from a high level point of view
6966660	6970660	I'm very interested, you know talking about this divide sort of this fork in the road in terms of
6970980	6976100	What's the path to open studying open-endedness studying it in silico or studying it in
6976660	6980580	situ in the setting of an actual open-ended system like a user
6981460	6986500	App interaction or you know the interaction between a user and a piece of software on the web
6987380	6990100	Or potentially with many other users. There are such rich
6990900	6997540	Existing systems online that are already open-ended because they amplify or connect the creativity and knowledge of humans
6998180	7004180	To create more knowledge and more creative artifacts. And so I think what's really uh, interesting in my mind now is sort of studying
7004900	7009060	Systems or algorithms that allow us to better steer the creativity of humans
7009700	7011700	As they are mediated by software
7012420	7015380	And basically allow us to essentially amplify
7015940	7023060	Existing intelligent or creative systems that are open-ended so amplify existing open open-endedness rather than try to build it from scratch
7024020	7028580	Amazing guys. It's been an honor to have you on MLS T. Thank you so much. Thanks. Thank you. Yeah
7029380	7031380	Great cool. Yeah, we're done
