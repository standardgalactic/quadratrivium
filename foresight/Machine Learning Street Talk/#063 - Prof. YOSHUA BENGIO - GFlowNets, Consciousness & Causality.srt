1
00:00:00,000 --> 00:00:03,560
my pleasure. And I must say, I've been really impressed by all your

2
00:00:03,560 --> 00:00:08,360
questions. It showed that you did prepare and read papers and think

3
00:00:08,360 --> 00:00:12,760
about it. And that's very much appreciated. Thank you.

4
00:00:21,040 --> 00:00:24,120
Today is an incredibly special occasion. We have Professor

5
00:00:24,120 --> 00:00:27,960
Yoshua Benjiro on the show. Just honestly, I just can't get over

6
00:00:28,080 --> 00:00:31,920
it. But first of all, a little bit of housekeeping. So we've

7
00:00:31,920 --> 00:00:35,320
just launched a new Discord community. So please jump in

8
00:00:35,320 --> 00:00:39,160
there, say hello, introduce yourself. If you want to be, you

9
00:00:39,160 --> 00:00:42,640
know, part of the moderating community or just help us do

10
00:00:42,640 --> 00:00:45,320
stuff over there, we would love to talk with you. By popular

11
00:00:45,320 --> 00:00:47,480
demand, we've also added a couple of ways in which you can

12
00:00:47,480 --> 00:00:51,200
support us. So we now have a Patreon and a merch store. If

13
00:00:51,200 --> 00:00:53,840
you're interested in supporting some of the episodes of MLST

14
00:00:53,840 --> 00:00:55,440
then get in touch with us because we'd love to have a

15
00:00:55,440 --> 00:00:59,240
conversation with you. We're just doing so much cool stuff

16
00:00:59,240 --> 00:01:02,360
this year. We've already recorded about six episodes that we

17
00:01:02,360 --> 00:01:05,320
haven't released. And we've got some amazing people booked as

18
00:01:05,320 --> 00:01:08,800
well. So yeah, it's going to be incredible. As always, if you

19
00:01:08,800 --> 00:01:11,240
like the content here, please consider hitting the like and

20
00:01:11,240 --> 00:01:14,120
subscribe button and rating our podcast on iTunes because it

21
00:01:14,120 --> 00:01:16,640
really, really helps us out. I called it iTunes. Is it

22
00:01:16,640 --> 00:01:19,760
iTunes? Apple podcasts? I don't know, whatever it's called.

23
00:01:21,200 --> 00:01:24,520
Wait and Biases is the developer first MLS platform. And

24
00:01:24,520 --> 00:01:27,400
we're extremely proud today that they are sponsoring this

25
00:01:27,400 --> 00:01:30,840
episode. Now tracking machine learning experiments is

26
00:01:30,840 --> 00:01:34,840
difficult. Using the winging it methodology can only get you so

27
00:01:34,840 --> 00:01:38,960
far. What we need is a platform where we can compare models and

28
00:01:38,960 --> 00:01:42,160
visualize their performance characteristics against all of

29
00:01:42,160 --> 00:01:45,400
the previous runs and figure out the best hyper parameters to

30
00:01:45,400 --> 00:01:49,240
use. Now most importantly of all, this process needs to be

31
00:01:49,320 --> 00:01:52,920
reproducible. Sounds like a tool order, right? Well, this is

32
00:01:52,920 --> 00:01:57,320
exactly what the Wait and Biases platform does for you. Now you

33
00:01:57,320 --> 00:02:00,680
can even follow metrics from long running experiments in real

34
00:02:00,680 --> 00:02:04,240
time. I think it's really important to lean into the

35
00:02:04,240 --> 00:02:08,600
complex interaction between science and engineering in the

36
00:02:08,600 --> 00:02:12,840
ML DevOps lifecycle. Data scientists need valuable feedback

37
00:02:12,920 --> 00:02:15,320
and they need to communicate why they're running given

38
00:02:15,320 --> 00:02:18,680
experiments and they need to share their notes around the next

39
00:02:18,680 --> 00:02:22,880
steps. Reports keep this work well organized and connected to

40
00:02:22,880 --> 00:02:26,080
the Waits and Biases experiments which were run as opposed to

41
00:02:26,080 --> 00:02:30,080
just sharing random screenshots in Slack. It's so easy to create

42
00:02:30,080 --> 00:02:32,720
a report and share it with your team after you finished with

43
00:02:32,720 --> 00:02:35,560
your experimentation. You could just add notes for yourself as

44
00:02:35,560 --> 00:02:38,640
well to explore later on. You can keep a work log and you can

45
00:02:38,640 --> 00:02:42,280
even share your findings internally or externally. This is

46
00:02:42,280 --> 00:02:45,960
an absolute game changer. I'm a big believer in this kind of

47
00:02:45,960 --> 00:02:49,520
engineering rigor. I'm the CEO of a code review startup called

48
00:02:49,520 --> 00:02:53,600
Merge these days and I love how the pull request process and

49
00:02:53,600 --> 00:02:57,280
tooling immortalizes important collective decisions which were

50
00:02:57,280 --> 00:03:00,520
made during the software development lifecycle. Similarly,

51
00:03:00,720 --> 00:03:04,000
Waits and Biases immortalizes important decisions that were

52
00:03:04,000 --> 00:03:07,880
made during model development, experimentation and

53
00:03:07,880 --> 00:03:12,320
deployment. Remember, check out Waits and Biases today by going

54
00:03:12,320 --> 00:03:16,720
to 1db.com forward slash MLST and if you're interested in

55
00:03:16,720 --> 00:03:19,960
sponsoring future episodes, get in touch with us. Waits and

56
00:03:19,960 --> 00:03:23,240
Biases are currently sponsoring our premiere shows but we have

57
00:03:23,240 --> 00:03:25,520
lots of other content coming and opportunities for

58
00:03:25,520 --> 00:03:27,720
sponsorship so let us know. Cheers.

59
00:03:29,480 --> 00:03:32,240
Professor Yoshua Benjo has just released a bunch of papers

60
00:03:32,280 --> 00:03:36,160
around G-Flow Nets. Now G-Flow Nets exists squarely in the

61
00:03:36,160 --> 00:03:40,800
domain of active learning which is a model that can economically

62
00:03:40,800 --> 00:03:43,920
ask an oracle which is probably the real world for the most

63
00:03:43,920 --> 00:03:48,280
salient training examples to continue learning. The learner

64
00:03:48,280 --> 00:03:51,720
can choose or have an influence on the examples it gets and we

65
00:03:51,720 --> 00:03:54,600
want to learn a function which approximates the oracle

66
00:03:54,640 --> 00:03:58,320
efficiently. How should we pick the queries? How should we take

67
00:03:58,320 --> 00:04:02,400
into account not just the value of the predictor but also how

68
00:04:02,400 --> 00:04:05,280
certain we are about the predictors from the learning

69
00:04:05,280 --> 00:04:10,000
system? Areas of uncertainty or entropy are kind of like

70
00:04:10,040 --> 00:04:13,800
interesting candidates for us to explore further. We need to

71
00:04:13,800 --> 00:04:18,760
be able to imagine or invent queries to give to the oracle.

72
00:04:19,320 --> 00:04:21,800
Now one of the reasons that machine learning models are so

73
00:04:21,800 --> 00:04:25,120
sample and efficient is because of the combinatorial space of

74
00:04:25,120 --> 00:04:28,920
possible input examples. We can't train on everything because

75
00:04:28,920 --> 00:04:32,880
the space is just too large, it's vast. So you might have heard

76
00:04:32,880 --> 00:04:35,640
of a related concept of active learning called machine

77
00:04:35,640 --> 00:04:38,880
teaching which is an interactive version where the human

78
00:04:39,160 --> 00:04:43,400
interactively selects the most salient data to train a machine

79
00:04:43,400 --> 00:04:47,480
learning model maximizing the information gain in respect of

80
00:04:47,480 --> 00:04:53,200
the training samples. Now the reality is the function space

81
00:04:53,280 --> 00:04:56,760
that we're learning here is highly structured. We only really

82
00:04:56,760 --> 00:04:59,760
need to sample training data where most of the rich

83
00:04:59,760 --> 00:05:02,920
information exists in that function space. I mean, if you

84
00:05:02,920 --> 00:05:05,520
think about it, a machine learning model, it's just a joint

85
00:05:05,520 --> 00:05:09,520
probability distribution between signals and labels. And this

86
00:05:09,520 --> 00:05:16,280
distribution has modes or areas of density or information. And

87
00:05:16,400 --> 00:05:20,520
actually most of it is just areas of nothingness, which

88
00:05:20,520 --> 00:05:25,600
require fewer training examples to learn and to represent. Now,

89
00:05:26,040 --> 00:05:29,280
if you spoke to a to a Bayesian person like my friend Conor

90
00:05:29,280 --> 00:05:33,000
Tan at work, you know how to learn this distribution, they

91
00:05:33,000 --> 00:05:36,720
would bring up Markov chain Monte Carlo quicker than a whip it

92
00:05:36,720 --> 00:05:40,920
with a bumful of dynamite. Now Markov chain Monte Carlo is an

93
00:05:40,920 --> 00:05:44,200
increasingly popular sampling method for obtaining asymptotic

94
00:05:44,200 --> 00:05:47,160
information about unnormalised distributions or energy

95
00:05:47,160 --> 00:05:50,320
functions, especially for estimating the posterior

96
00:05:50,320 --> 00:05:53,000
distribution in Bayesian inference, which is where you've

97
00:05:53,000 --> 00:05:55,880
probably heard of it before. Now you can characterize a

98
00:05:55,880 --> 00:05:58,400
distribution without knowing all of the distributions

99
00:05:58,400 --> 00:06:00,600
mathematical properties. So if you don't have an analytical

100
00:06:00,600 --> 00:06:04,080
representation for it, just by randomly sampling values out of

101
00:06:04,080 --> 00:06:07,880
the distribution. Now a particular strength of Markov chain

102
00:06:07,880 --> 00:06:10,640
Monte Carlo is that it can be used to draw samples from

103
00:06:10,640 --> 00:06:14,320
distributions, even when all that is known about the

104
00:06:14,320 --> 00:06:17,680
distribution is how to calculate the density for different

105
00:06:17,680 --> 00:06:22,280
samples. Now the the Markov property of Markov chain Monte

106
00:06:22,280 --> 00:06:26,400
Carlo is this idea that random samples are generated by a

107
00:06:26,400 --> 00:06:30,520
special sequential process. And each random sample is used as a

108
00:06:30,520 --> 00:06:35,680
stepping stone to generate the next random sample. Now this

109
00:06:35,680 --> 00:06:39,200
might sound very complex, but the practical implementation is

110
00:06:39,200 --> 00:06:42,320
pretty simple. Markov chain Monte Carlo just starts with an

111
00:06:42,320 --> 00:06:46,520
initial guess, just one value that might plausibly be drawn

112
00:06:46,520 --> 00:06:50,040
from the distribution. And then we produce a chain of samples

113
00:06:50,040 --> 00:06:54,000
from this initial guess by adding random perturbations in the

114
00:06:54,000 --> 00:06:58,240
neighborhood of that example. And each new proposal drawn from

115
00:06:58,240 --> 00:07:00,960
that random perturbation distribution is either rejected

116
00:07:01,040 --> 00:07:04,360
or accepted. There are different flavors of this, of course, I

117
00:07:04,360 --> 00:07:07,960
mean, in particular, like tweaking, how the random

118
00:07:07,960 --> 00:07:10,920
proposals in the neighborhood are selected or whether the

119
00:07:10,920 --> 00:07:15,200
proposals are selected. The simplest heuristic being whether

120
00:07:15,200 --> 00:07:19,080
it's below the function or not. Now the idea is that Markov

121
00:07:19,080 --> 00:07:22,440
chain Monte Carlo methods, they capture a distribution with

122
00:07:22,440 --> 00:07:25,960
only a relatively small number of random samples. But the

123
00:07:25,960 --> 00:07:31,240
reality is anything but in high dimensions, and where the

124
00:07:31,240 --> 00:07:34,560
distribution has many modes spread far apart, it's actually

125
00:07:34,680 --> 00:07:39,200
exponentially expensive. There's a bunch of human orientated

126
00:07:39,200 --> 00:07:42,240
hacks to try and make this work well in specific cases. But we're

127
00:07:42,240 --> 00:07:46,040
missing a much more general machine learnable solution. This is

128
00:07:46,040 --> 00:07:48,720
the main reason why we haven't seen it used in many machine

129
00:07:48,720 --> 00:07:51,480
learning applications yet. Assuming that the function we want to

130
00:07:51,480 --> 00:07:55,480
learn has underlying structure, then we can escape the

131
00:07:55,480 --> 00:07:59,120
exponential time of Markov chain Monte Carlo with machine

132
00:07:59,120 --> 00:08:02,680
learning. And this is what Benzio calls systematic

133
00:08:02,720 --> 00:08:07,280
generalization, which is to say, how do we generalize far from

134
00:08:07,280 --> 00:08:11,680
the data in a way which is meaningful. Now G flow nets are

135
00:08:11,680 --> 00:08:14,600
an active learning framework, where the name of the game is to

136
00:08:14,600 --> 00:08:18,080
generate salient and diverse training data to augment our

137
00:08:18,080 --> 00:08:21,200
model in the most sample efficient way possible. For G flow

138
00:08:21,200 --> 00:08:24,960
nets to work, we need a reward function and a deterministic

139
00:08:24,960 --> 00:08:28,680
episodic environment. Does that sound familiar? Yes, just like

140
00:08:28,680 --> 00:08:32,520
reinforcement learning. Now a flow network is a directed graph

141
00:08:32,720 --> 00:08:38,080
with sources and sinks and edges carrying some amount of flow

142
00:08:38,080 --> 00:08:40,880
between them, you know, through intermediate nodes. So I think a

143
00:08:40,880 --> 00:08:44,000
good way to think about this is pipes of water. Now for our

144
00:08:44,000 --> 00:08:48,680
purposes, we define a flow network with a single source. So the

145
00:08:48,680 --> 00:08:51,280
root nodes, or you might say the sinks of the network

146
00:08:51,320 --> 00:08:55,000
correspond to the terminal states. Now it's designed to find

147
00:08:55,000 --> 00:08:59,360
the possible trajectories through our system. Okay, and just

148
00:08:59,360 --> 00:09:01,560
think of Alpha zero as being like a good analogy for these

149
00:09:01,560 --> 00:09:04,920
trajectories. Now the training objective is to make them

150
00:09:04,960 --> 00:09:09,120
approximately sample in proportion to the given reward

151
00:09:09,120 --> 00:09:12,680
function. This is in stark contrast to Alpha zero where we

152
00:09:12,680 --> 00:09:16,480
were sampling to maximize the expected reward. So Benzio's big

153
00:09:16,480 --> 00:09:19,360
idea is that we can have an interacting loop between a

154
00:09:19,360 --> 00:09:22,400
generative model and the real world. The real world is

155
00:09:22,400 --> 00:09:26,160
expensive. So why not train an imagination machine in our mind

156
00:09:26,280 --> 00:09:29,560
until we're ready and waiting to produce good questions to the

157
00:09:29,560 --> 00:09:32,920
real world, we could use imagined experiments to train our

158
00:09:32,920 --> 00:09:38,680
generator, then produce queries to the real world. We were

159
00:09:38,680 --> 00:09:43,480
thinking about a way to visualize how G flow nets work when the

160
00:09:43,480 --> 00:09:48,640
idea of a Galton board came to mind. A Galton board also known

161
00:09:48,640 --> 00:09:53,200
as a beam machine is a common prop in statistics courses,

162
00:09:53,560 --> 00:09:59,520
science museums, and fun gadget stores. The board has rows of

163
00:09:59,520 --> 00:10:05,240
interleaved pegs above a bottom row of buckets. Beads are filled

164
00:10:05,240 --> 00:10:09,000
into a funnel at the top of the board and then sprinkled on the

165
00:10:09,000 --> 00:10:13,360
top center peg. The beads bounce either to the left or to the

166
00:10:13,360 --> 00:10:17,520
right as they hit the pegs and eventually collect into buckets

167
00:10:17,560 --> 00:10:22,280
at the bottom. If the pegs are precisely and symmetrically

168
00:10:22,280 --> 00:10:25,640
arranged, the beads will aggregate at the bottom into a

169
00:10:25,640 --> 00:10:31,320
familiar binomial bell curve. Now imagine that the pegs were

170
00:10:31,320 --> 00:10:35,760
instead flow gates with adjustable valves that could

171
00:10:35,760 --> 00:10:40,400
direct the beads more to the left or more to the right to

172
00:10:40,400 --> 00:10:44,680
bias the flow paths. With such a machine, you could adjust the

173
00:10:44,680 --> 00:10:51,120
valves or flow rates to create any distribution. For example, to

174
00:10:51,120 --> 00:10:54,680
create a uniform distribution, we'd open up the gates flowing

175
00:10:54,720 --> 00:10:59,960
away from the center line of the board to drive more bead flow

176
00:11:00,080 --> 00:11:04,080
to the fewer number of paths leading to the edges and the

177
00:11:04,080 --> 00:11:10,000
corners. Or to create a multimodal distribution, we'd arrange

178
00:11:10,000 --> 00:11:13,920
the gates to split the flows into two or more streams that would

179
00:11:13,920 --> 00:11:19,920
then pile up in multiple humps or modes below. There's a lot of

180
00:11:19,920 --> 00:11:23,840
flexibility here. Indeed, given a distribution, there are

181
00:11:23,840 --> 00:11:28,720
generally multiple flow gate solutions to produce it. It'd be

182
00:11:28,720 --> 00:11:32,920
nice, wouldn't it? If we had an intelligent, principled way to

183
00:11:32,920 --> 00:11:38,320
train these gates. Enter G flow nets. G flow nets put a neural

184
00:11:38,320 --> 00:11:42,920
network, a brain behind the flow adjustments. A brain which can

185
00:11:42,920 --> 00:11:47,880
optimize the gates to match any distribution we desire. Here,

186
00:11:47,880 --> 00:11:51,680
we're interested in sampling a reward function in the context

187
00:11:51,680 --> 00:11:56,480
of reinforcement learning. In that context, this is a powerful

188
00:11:56,480 --> 00:12:01,320
simulation and sampling paradigm. You see, once the brain has

189
00:12:01,320 --> 00:12:05,240
tuned the flow weights, such a modified Galton board, or more

190
00:12:05,240 --> 00:12:10,400
generally, a flow network, can sample diverse paths quickly

191
00:12:10,520 --> 00:12:15,200
and efficiently, leading to the reward distribution. It's

192
00:12:15,200 --> 00:12:18,960
important to point out that the path sampling is more diverse

193
00:12:19,120 --> 00:12:23,640
doing it this way. Unlike classic reinforcement learning, a G

194
00:12:23,640 --> 00:12:27,080
flow net doesn't just fixate on a small number of high reward

195
00:12:27,080 --> 00:12:31,600
paths, it happens to find first. Instead, it stochastically

196
00:12:31,600 --> 00:12:35,440
samples a broad spectrum of paths in proportion to their reward.

197
00:12:35,880 --> 00:12:39,440
Sure, high reward paths will be sampled with higher weight. But

198
00:12:39,440 --> 00:12:43,640
the far larger population of low reward paths will get a share

199
00:12:43,680 --> 00:12:47,240
of the sampling as well. Why should we even bother with such

200
00:12:47,240 --> 00:12:52,160
paths? The answer is we need to balance exploitation or high

201
00:12:52,160 --> 00:12:56,160
reward with exploration or learning to better learn the

202
00:12:56,160 --> 00:12:59,760
reward function. This is especially important when dealing

203
00:12:59,760 --> 00:13:04,960
with complex real world scenarios of high uncertainty. For

204
00:13:04,960 --> 00:13:09,200
example, think of molecular drug discovery and design or

205
00:13:09,440 --> 00:13:14,120
navigating jungle terrain. In both those scenarios, we really

206
00:13:14,120 --> 00:13:19,080
know very little about how a particular path may play out. We

207
00:13:19,080 --> 00:13:23,000
might stumble into the next miracle cure or a pitfall of

208
00:13:23,000 --> 00:13:27,520
quicksand. To find the globally best paths, it's important to

209
00:13:27,520 --> 00:13:32,800
keep our options open. Beyond this sampling diversity, G

210
00:13:32,800 --> 00:13:36,400
flow nets also bring the full power of neural networks to

211
00:13:36,440 --> 00:13:41,640
discover latent structure and learn the reward function. This

212
00:13:41,680 --> 00:13:46,160
combined with their diverse sampling also makes G flow nets

213
00:13:46,160 --> 00:13:50,160
more robust when dealing with multimodal distributions, which

214
00:13:50,160 --> 00:13:54,360
are a common trap for greedy algorithms and Markov chain,

215
00:13:54,400 --> 00:13:58,760
Monte Carlo. If there is structure linking the multiple

216
00:13:58,760 --> 00:14:03,920
nodes, G flow nets can learn it and extrapolate to new modes

217
00:14:04,000 --> 00:14:08,840
and once discovered, they will by design drive the sampling to

218
00:14:08,840 --> 00:14:14,240
cover those modes and learn more structure. Overall, G flow

219
00:14:14,240 --> 00:14:19,520
nets seem to offer an intriguing new path pun intended for an

220
00:14:19,520 --> 00:14:24,680
intelligent sampling paradigm. So you might ask how are G

221
00:14:24,680 --> 00:14:27,920
flow nets different from Alpha zero? Well, the policy network

222
00:14:27,920 --> 00:14:31,440
and Alpha zero gives you a set of actions. Given a state, Alpha

223
00:14:31,480 --> 00:14:35,600
zero trains the policy network to maximize reward so that the

224
00:14:35,600 --> 00:14:39,640
trajectories all end up at the highest reward. Now what G

225
00:14:39,640 --> 00:14:43,560
flow nets do is they train so that the actions are distributed

226
00:14:43,720 --> 00:14:47,280
in proportion to the reward. So rather than pruning away all of

227
00:14:47,280 --> 00:14:50,640
the low reward trajectories, it will sample them just less

228
00:14:50,640 --> 00:14:56,160
often. Now there is a manifest difference between G flow nets

229
00:14:56,160 --> 00:14:58,880
in respect of exploration. I mean, you might argue that the

230
00:14:58,880 --> 00:15:02,440
Monte Carlo tree search is still doing wide exploration at the

231
00:15:02,440 --> 00:15:05,920
beginning. But in spite of its rapid convergence and pruning of

232
00:15:05,920 --> 00:15:09,280
load reward trajectories, it's still sampling from the

233
00:15:09,280 --> 00:15:11,400
underlying probability distribution, which has been

234
00:15:11,400 --> 00:15:14,720
scaled with a softmax. So that's actually not that much to

235
00:15:14,720 --> 00:15:18,400
explore in the first place. So in summary, G flow nets are

236
00:15:18,400 --> 00:15:21,600
better than Alpha zero Monte Carlo tree search in some sense,

237
00:15:21,600 --> 00:15:24,760
because they achieve the same goal by offloading the burning

238
00:15:24,760 --> 00:15:27,600
time and the stabilization time of Markov chain Monte Carlo.

239
00:15:27,760 --> 00:15:30,360
Remember this whole thing can be trained offline. And then

240
00:15:30,360 --> 00:15:33,200
when in inference mode, we can do it in a single shot. Whereas

241
00:15:33,200 --> 00:15:36,080
with Monte Carlo tree search, we actually had to do it in

242
00:15:36,080 --> 00:15:39,760
inference mode as well. The other thing is we're kind of

243
00:15:40,440 --> 00:15:43,320
offloading all of the human engineering required to sample

244
00:15:43,320 --> 00:15:46,040
efficiently from Markov chain Monte Carlo. And the other thing

245
00:15:46,040 --> 00:15:49,480
is diversity, baby. I mean, consider the difference between

246
00:15:49,480 --> 00:15:52,200
how G flow nets and Alpha zero sample the reward path

247
00:15:52,200 --> 00:15:54,880
distribution. If you looked at the distributions, you would

248
00:15:54,880 --> 00:15:59,080
see that Alpha zero has a little box around the mode. G flow

249
00:15:59,080 --> 00:16:02,400
nets is the whole distribution. We know very well that

250
00:16:02,400 --> 00:16:05,840
diversity preservation is critical in order to discover

251
00:16:05,840 --> 00:16:09,320
interesting stepping stones and search problems. Now finally,

252
00:16:09,320 --> 00:16:12,200
Benzio has published results showing the G flow nets converge

253
00:16:12,200 --> 00:16:15,640
exponentially faster than Markov chain Monte Carlo and PPL on

254
00:16:15,640 --> 00:16:19,480
some problems and finds more of the modes in the distribution

255
00:16:19,480 --> 00:16:22,480
function faster. Enjoy the show folks.

256
00:16:23,320 --> 00:16:27,920
Professor Yoshio Benzio is recognized worldwide as one of

257
00:16:27,920 --> 00:16:32,480
the leading experts in artificial intelligence. Indeed, a god

258
00:16:32,480 --> 00:16:35,600
father of deep learning. His pioneering work in deep learning

259
00:16:35,600 --> 00:16:38,880
earned him the Turing Award, which is the Nobel Prize of

260
00:16:38,880 --> 00:16:42,480
computing. He's a full professor at the University of Montreal,

261
00:16:42,680 --> 00:16:46,600
and the founder and scientific director of Miele, which is a

262
00:16:46,600 --> 00:16:50,800
prestigious community of more than 900 researchers specializing

263
00:16:50,800 --> 00:16:54,560
in machine learning and AI. He's one of the most cited

264
00:16:54,560 --> 00:16:58,280
computer scientists on the planet. And I can't even begin to

265
00:16:58,280 --> 00:17:01,200
articulate how honored we are today to have this conversation.

266
00:17:01,600 --> 00:17:05,120
Yoshio has done a lot of work recently on G flow nets, which

267
00:17:05,120 --> 00:17:08,040
are an active learning framework in a reinforcement learning

268
00:17:08,040 --> 00:17:12,160
configuration, where the name of the game is to request salient

269
00:17:12,200 --> 00:17:16,000
and diverse training data from the real world to augment our

270
00:17:16,000 --> 00:17:19,960
learned models in the most sample efficient way possible. Now

271
00:17:19,960 --> 00:17:22,640
we're trying to minimize the divergence between the path

272
00:17:22,640 --> 00:17:26,120
distribution and the reward distribution, and then sample

273
00:17:26,120 --> 00:17:30,040
paths according to the reward distribution. This is in stark

274
00:17:30,040 --> 00:17:32,800
contrast with traditional reinforcement learning, where we

275
00:17:32,800 --> 00:17:36,760
trying to maximize the expected reward. This approach is likely

276
00:17:36,760 --> 00:17:40,280
to find diverse strategies instead of being greedy and

277
00:17:40,280 --> 00:17:43,720
converging quickly after finding a single one. Anyway,

278
00:17:43,840 --> 00:17:46,840
Professor Benzio, this is amazing. Can you tell us about this

279
00:17:46,840 --> 00:17:48,840
exciting work in some of its applications?

280
00:17:49,520 --> 00:17:54,520
Yeah, I'm, I don't think I've been as excited about a new topic.

281
00:17:56,400 --> 00:18:00,800
At least in the last six or seven years as I'm now with G flow

282
00:18:00,800 --> 00:18:06,240
nets. And it's actually even much more than what you've been

283
00:18:06,240 --> 00:18:11,200
talking about. The way I think about G flow nets is a kind of

284
00:18:11,720 --> 00:18:14,120
framework for generic

285
00:18:15,920 --> 00:18:22,840
learnable inference for probabilistic machine learning. So

286
00:18:23,280 --> 00:18:28,160
one way to think about this is it's a learnable replacement for

287
00:18:28,200 --> 00:18:32,560
Monte Carlo Markov chain sampling. But actually, so there's

288
00:18:32,560 --> 00:18:37,640
that and I'll explain if you want why this is important and to use

289
00:18:37,640 --> 00:18:41,160
machine learning there. But but also, it can be used to

290
00:18:41,160 --> 00:18:44,960
estimate probabilities themselves, not just sampling, but

291
00:18:44,960 --> 00:18:45,920
also estimate

292
00:18:47,560 --> 00:18:51,600
intractable quantities like partition functions and a

293
00:18:51,600 --> 00:18:54,960
condition probabilities that would otherwise require summing

294
00:18:54,960 --> 00:19:00,120
over an intractable number of terms. So I think of this as

295
00:19:00,120 --> 00:19:03,640
the potentially, you know, there's still we're still at the

296
00:19:03,640 --> 00:19:08,280
beginnings of this has a Swiss army knife of probabilistic

297
00:19:08,280 --> 00:19:11,960
modeling that uses machine learning to be able to do things

298
00:19:11,960 --> 00:19:16,120
that look intractable, but do them efficiently thanks to

299
00:19:16,120 --> 00:19:18,280
generalization power of large neural nets.

300
00:19:19,520 --> 00:19:22,960
We've been trying to think of a way to help our listeners

301
00:19:23,120 --> 00:19:28,680
visualize what a what a G flow net does. And I wanted to run by

302
00:19:28,720 --> 00:19:31,520
a possibility to you. So I'm not sure if you've heard of

303
00:19:32,000 --> 00:19:36,360
Galton boards also called, you know, bean machines. And what

304
00:19:36,360 --> 00:19:39,680
they are is this prop that's often used by statistics

305
00:19:39,680 --> 00:19:43,080
professors at the start of say, an elementary introductory

306
00:19:43,080 --> 00:19:47,080
course to give a visual intuition. And it's a board that has

307
00:19:47,080 --> 00:19:51,640
these vertical buckets down at the bottom with interleaved rows

308
00:19:51,640 --> 00:19:57,080
of pegs above the buckets, and then beads are filled in into the

309
00:19:57,080 --> 00:19:59,800
top of the board, and they bounce either left or right as

310
00:19:59,800 --> 00:20:03,200
they hit the pegs. And they eventually collect down at the

311
00:20:03,200 --> 00:20:05,960
bottom. Yeah, yeah. Yeah, now now if the peg is a very good

312
00:20:05,960 --> 00:20:08,960
analogy, except that it's not a tree, I don't know how these

313
00:20:08,960 --> 00:20:12,200
things are, but you know, the ball can come to a place from

314
00:20:12,200 --> 00:20:14,800
two different paths or an potentially large number of

315
00:20:14,800 --> 00:20:18,800
paths. Right, right. And I think, given given there are some

316
00:20:18,840 --> 00:20:22,520
some differences, you know, the idea was that if the pegs or

317
00:20:22,520 --> 00:20:25,400
no, it's it's pretty close to exactly what it is. Yeah, and

318
00:20:25,400 --> 00:20:28,360
what we were thinking is that if the pegs on the Galton board

319
00:20:28,360 --> 00:20:31,720
are precisely and symmetrically arranged, you know, the beads

320
00:20:31,760 --> 00:20:36,200
will form a nice binomial curve at the bottom. And it seems

321
00:20:36,200 --> 00:20:39,520
like what G flow nets are capable of doing when they

322
00:20:39,520 --> 00:20:44,360
optimize the pathways. They're tweaking the pegs a little bit

323
00:20:44,360 --> 00:20:48,160
to the left, or a little to the right, to bias the flow of

324
00:20:48,160 --> 00:20:51,440
beads one way or the other. And in this way, a G flow net

325
00:20:51,440 --> 00:20:54,240
could arrange the pegs so that the beads could form any

326
00:20:54,240 --> 00:20:57,640
distribution at the bottom that we want. And for our purposes,

327
00:20:57,640 --> 00:21:01,800
that means the distribution that matches the reward function. So

328
00:21:01,800 --> 00:21:05,520
is this a good way to think about G flow nets? Yes, it is. Now,

329
00:21:05,560 --> 00:21:07,880
it's missing a really important aspect of it, which would be

330
00:21:07,880 --> 00:21:14,800
difficult to send visually, but that all of these peg weights,

331
00:21:15,400 --> 00:21:20,960
like the polytip that are boggles left or right, are not just

332
00:21:20,960 --> 00:21:23,920
like learned independently, like as a tabular machine

333
00:21:23,920 --> 00:21:28,480
earning, but that there's like one neural net that knows about

334
00:21:28,520 --> 00:21:34,560
the locations in this big board as input and tells, you know, how

335
00:21:34,560 --> 00:21:38,080
much relative weight should I, you know, go to go left or right

336
00:21:38,080 --> 00:21:42,640
at this position. So the reason this is important is because it

337
00:21:42,640 --> 00:21:47,000
allows for generalization. Because this board is huge, it's

338
00:21:47,000 --> 00:21:50,120
exponentially large. So there's no way you're going to learn, like

339
00:21:50,120 --> 00:21:54,280
a separate parameter for each of these choices. And so you have

340
00:21:54,280 --> 00:21:57,960
this neural net or potentially several neural nets, but that

341
00:21:57,960 --> 00:22:02,560
share allow you to share statistical strength, as we

342
00:22:02,560 --> 00:22:06,800
call it, share information across all the possible positions, so

343
00:22:06,800 --> 00:22:10,200
that you can generalize to places paths that it has never

344
00:22:10,200 --> 00:22:14,080
seen from a finite number of training trajectories that it

345
00:22:14,080 --> 00:22:16,940
sees while it's being trained. And that's crucial. Otherwise, you

346
00:22:16,940 --> 00:22:19,980
couldn't scale to large problems, which is really what we

347
00:22:19,980 --> 00:22:20,500
want to do.

348
00:22:21,220 --> 00:22:24,260
Professor Benjo, we spoke with Professor Carl Friston about his

349
00:22:24,260 --> 00:22:27,780
free energy principle, an active inference, which is pretty much

350
00:22:27,780 --> 00:22:30,620
a Bayesian flavored version of reinforcement learning. And he

351
00:22:30,620 --> 00:22:33,220
said that while we need to maintain entropy and stop

352
00:22:33,220 --> 00:22:36,500
models from increasing too much in complexity, we should balance

353
00:22:36,500 --> 00:22:38,860
entropy with accuracy in a principled way. And by the way,

354
00:22:38,860 --> 00:22:40,420
you can kind of think of them in just the audience think of

355
00:22:40,420 --> 00:22:44,500
entropy as keeping your options open. But Friston thinks that

356
00:22:44,500 --> 00:22:46,860
the Bellman-esque idea of reinforcement learning, which is

357
00:22:46,860 --> 00:22:50,020
to say maximizing expected reward is the objective is

358
00:22:50,020 --> 00:22:52,740
misguided. And we should instead perform inference over future

359
00:22:52,740 --> 00:22:56,700
paths, balancing expected reward of relative entropy. Is there a

360
00:22:56,700 --> 00:22:59,260
connection between these ideas? I mean, it seems like G flow

361
00:22:59,260 --> 00:23:01,420
nets are sampling paths proportional to the reward

362
00:23:01,420 --> 00:23:04,820
function, that will maintain as much entropy as the reward

363
00:23:04,820 --> 00:23:05,620
function itself.

364
00:23:07,580 --> 00:23:11,620
Yes, yes, exactly. It's a translation of the reward

365
00:23:11,620 --> 00:23:17,140
function into machinery that can sample, you know, the equivalent,

366
00:23:17,140 --> 00:23:21,340
the corresponding distribution. So yeah, I completely agree with

367
00:23:21,620 --> 00:23:27,660
what Carl was saying here. But as I said, what's interesting is,

368
00:23:29,140 --> 00:23:35,060
we can do things with G flow nets. In principle, we've done the

369
00:23:35,060 --> 00:23:37,780
math and some small scale experiments that we have now a

370
00:23:37,780 --> 00:23:41,060
number of papers, we can do things that go beyond sampling.

371
00:23:42,180 --> 00:23:47,660
But for example, estimate entropy itself. So entropy is

372
00:23:48,060 --> 00:23:57,220
notoriously difficult to estimate. And I mentioned in my talks on

373
00:23:57,220 --> 00:24:02,140
G flow nets that we can use the G flow net machinery to estimate

374
00:24:02,180 --> 00:24:06,620
entropy of say, an action distribution or a distribution

375
00:24:06,620 --> 00:24:09,980
over Bayesian parameters, for example, which is would be

376
00:24:09,980 --> 00:24:13,260
something you'd like to minimize if you're going to take an

377
00:24:13,260 --> 00:24:16,900
action in the world. And you have a model of the world that has

378
00:24:16,900 --> 00:24:20,780
uncertainty. And that connects with Carl, for instance,

379
00:24:20,780 --> 00:24:24,860
interest, you'd like to be able to choose an action that

380
00:24:24,860 --> 00:24:28,460
minimizes your uncertainty about how the world works, we know

381
00:24:28,460 --> 00:24:32,900
what are the latent things that may have happened. And good, you

382
00:24:32,900 --> 00:24:36,540
know, an important part of that is estimating the reward for

383
00:24:36,580 --> 00:24:40,660
the these exploratory actions, like, you know, children playing

384
00:24:40,660 --> 00:24:45,540
around is how much reduction in entropy of my knowledge of the

385
00:24:45,540 --> 00:24:48,060
world, I'm going to get through that action. So you need to be

386
00:24:48,060 --> 00:24:51,380
able to compute that reward. That reward word is basically an

387
00:24:51,380 --> 00:24:55,860
entropy over something you care about. And it turns out you can

388
00:24:55,860 --> 00:24:57,540
also do that with G flow nets.

389
00:24:57,700 --> 00:24:59,980
We're actually speaking with Friston again next week, do you

390
00:24:59,980 --> 00:25:02,260
have a question that you would like us to put to him?

391
00:25:03,140 --> 00:25:07,540
Well, he, you know, he's on the biology side of things much

392
00:25:07,540 --> 00:25:14,140
more than I am. And I believe there are amazing scientific

393
00:25:14,140 --> 00:25:22,380
opportunities to explore how the kind of machinery that G

394
00:25:22,380 --> 00:25:27,260
flow nets offer could be used by brains in order to do some of

395
00:25:27,260 --> 00:25:31,580
the things they do. Using your nets to model the probabilistic

396
00:25:31,580 --> 00:25:33,860
structure of the world, including uncertainty, which is

397
00:25:33,860 --> 00:25:38,420
something he cares about. But but also taking into consideration

398
00:25:38,420 --> 00:25:41,780
things like high level cognition, the global workspace theory,

399
00:25:41,780 --> 00:25:45,620
which is something I care a lot about, attention, they all kind

400
00:25:45,620 --> 00:25:53,780
of fit in the picture of G flow nets. So so I think there's a

401
00:25:53,820 --> 00:25:59,020
huge potential of research at the synergy of computational and

402
00:25:59,020 --> 00:26:03,460
theoretical neuroscience, and machine learning, probabilistic

403
00:26:03,460 --> 00:26:07,940
modeling of the kind that G flow nets propose to come up with a

404
00:26:08,980 --> 00:26:13,860
some proposals for explanatory theories about what the brain

405
00:26:13,860 --> 00:26:18,300
does, that's probabilistic. And, you know, I think he would be a

406
00:26:18,300 --> 00:26:19,900
great person to be part of that.

407
00:26:20,220 --> 00:26:24,220
Fascinating. Well, going a little bit further down that line,

408
00:26:24,420 --> 00:26:27,340
there are folks in the community who are huge advocates of

409
00:26:27,340 --> 00:26:29,220
biologically inspired approaches to machine

410
00:26:29,220 --> 00:26:32,540
intelligence. And, you know, one of the key ideas actually is

411
00:26:32,540 --> 00:26:35,620
diversity, discovery and preservation, both in how

412
00:26:35,620 --> 00:26:38,260
knowledge is acquired and represented. I mean, specifically

413
00:26:38,260 --> 00:26:41,300
evolutionary algorithm advocates, they differentiate

414
00:26:41,300 --> 00:26:44,060
themselves from gradient based single agent monolithic

415
00:26:44,060 --> 00:26:47,140
approaches like reinforcement learning. And they point out that

416
00:26:47,220 --> 00:26:50,380
their approaches overcome so called deception and search

417
00:26:50,380 --> 00:26:52,540
problems, you know, which is to say they don't get stuck in

418
00:26:52,540 --> 00:26:55,700
local minima, your approach seems to be achieving something very

419
00:26:55,700 --> 00:26:58,460
similar in the context of a gradient based reinforcement

420
00:26:58,460 --> 00:27:00,420
learning package. I mean, I don't see it as being mutually

421
00:27:00,420 --> 00:27:01,940
exclusive. But what's your take on this?

422
00:27:02,460 --> 00:27:07,260
Yeah, diversity is important when you're exploring and humans,

423
00:27:07,260 --> 00:27:10,220
especially young ones are exploration machines, they're

424
00:27:10,220 --> 00:27:12,460
trying to understand how the world works and they're acting in

425
00:27:12,460 --> 00:27:16,500
the world in order to get that information. Yeah, I agree that

426
00:27:16,940 --> 00:27:23,740
that search process needs to have a big bonus on on diversity,

427
00:27:23,740 --> 00:27:28,340
like on trying different ways of achieving something good, like

428
00:27:28,860 --> 00:27:33,900
better understanding how the world works. So it turns out that

429
00:27:33,900 --> 00:27:39,220
in the G flow net framework, you, you have a training objective

430
00:27:39,220 --> 00:27:43,420
that yields this kind of diversity and exploration, but is

431
00:27:43,420 --> 00:27:47,220
based on training large neural nets end to end. Now, it's a bit

432
00:27:47,220 --> 00:27:52,260
different from the usual end to end training, because we don't

433
00:27:52,260 --> 00:27:55,340
have an objective that objective we're trying to optimize

434
00:27:55,340 --> 00:28:00,660
is not tractable, actually. But we can sample these trajectories,

435
00:28:00,660 --> 00:28:04,220
which I think of like sampling thoughts, like our thought

436
00:28:04,220 --> 00:28:07,820
process is going through some chain of explanations, not a

437
00:28:07,820 --> 00:28:10,620
complete, and it doesn't represent all the explanations,

438
00:28:10,620 --> 00:28:14,380
but but what we found with our training objectives for G

439
00:28:14,380 --> 00:28:19,260
flow nets is that these sort of random randomized kind of views

440
00:28:19,260 --> 00:28:23,100
of the world are sufficient to give a training signal to the

441
00:28:23,100 --> 00:28:25,300
neural nets that do the real job.

442
00:28:26,300 --> 00:28:30,460
I'm curious. So this trade off between exploration versus

443
00:28:30,500 --> 00:28:34,820
exploitation. And this has come up in so many contexts, you know,

444
00:28:34,820 --> 00:28:37,580
throughout our show. And one in particular, as we talked to, you

445
00:28:37,580 --> 00:28:42,100
know, we've talked to multi arm banded folks, right? And G flow

446
00:28:42,100 --> 00:28:45,740
net seemed to capture this balance between exploration

447
00:28:45,740 --> 00:28:49,740
and exploitation. But the multi arm banded folks, you know, they

448
00:28:49,740 --> 00:28:53,700
dive deep in that research circle into this into this trade off.

449
00:28:53,700 --> 00:28:57,940
And I think they have some very principled ways and even very

450
00:28:57,940 --> 00:29:02,420
rigorous ways to analyze this fundamental trade off. To what

451
00:29:02,420 --> 00:29:05,620
extent do you think that that their research maybe could be

452
00:29:05,620 --> 00:29:10,100
applied to future G flow net variations? Like do you think

453
00:29:10,100 --> 00:29:14,060
maybe it might open up more options to fine tune the

454
00:29:14,060 --> 00:29:16,460
trade off between exploration and exploitation?

455
00:29:17,460 --> 00:29:22,300
Yeah, I mean, the banded research is very, very closely

456
00:29:22,300 --> 00:29:27,260
related to the G flow net thread. But G flow nets, as we have

457
00:29:27,260 --> 00:29:30,860
been using them, for example, for drug discovery, they are

458
00:29:30,860 --> 00:29:35,380
banded. It's just that the action space is not, you know, one

459
00:29:35,380 --> 00:29:40,540
out of n things. It's, it's combinatorial because you build

460
00:29:40,540 --> 00:29:43,860
these pieces. So the action space is not something you can

461
00:29:44,020 --> 00:29:48,060
enumerate. So you can't apply the typical banded algorithms, but

462
00:29:48,060 --> 00:29:52,300
a lot of the math is totally applicable. And in fact, what we

463
00:29:52,300 --> 00:29:59,260
use in the drug discovery setting is UCB upper confidence

464
00:29:59,260 --> 00:30:10,820
bound objective to learn a good exploration policy. So that comes

465
00:30:10,860 --> 00:30:16,020
out of the banded research. It what it does is it, you know, it

466
00:30:16,020 --> 00:30:21,980
combines the risk and reward expected reward, one of these

467
00:30:21,980 --> 00:30:27,100
together in a way that in theory guarantees that you will do an

468
00:30:27,100 --> 00:30:31,740
efficient exploration and find that where is the, you know,

469
00:30:32,140 --> 00:30:35,620
where's the money? Where's the reward, right? All of the

470
00:30:35,620 --> 00:30:37,660
possible places where you can get the reward.

471
00:30:38,140 --> 00:30:43,660
So in, in, in the G flow net papers, you often describe it as, you

472
00:30:43,660 --> 00:30:48,180
know, we want to sample not only the maximum reward path, in

473
00:30:48,180 --> 00:30:51,700
order to have more diversity in order to maybe figure out

474
00:30:51,700 --> 00:30:55,260
something that we didn't know if we were just to go to the

475
00:30:55,260 --> 00:30:58,740
maximum reward. And that speaks a little bit to the, like the

476
00:30:58,740 --> 00:31:03,820
things that we know that we don't know, right? We maybe know

477
00:31:03,820 --> 00:31:08,180
that, right, this seems like a lower reward trajectory might

478
00:31:08,180 --> 00:31:12,220
turn out to be a higher reward trajectory. However, exploration

479
00:31:12,220 --> 00:31:14,900
and reinforcement learning is also fundamentally addressing the

480
00:31:14,900 --> 00:31:18,860
things about the things that I don't know that I don't know,

481
00:31:18,940 --> 00:31:23,100
which is where stuff like random exploration and things like

482
00:31:23,100 --> 00:31:26,980
this comes in. Could you maybe comment a little bit on how you

483
00:31:26,980 --> 00:31:31,300
see sort of, because it seems to me that if I managed to sample

484
00:31:31,300 --> 00:31:35,700
according to what I think is the reward distribution, right, I

485
00:31:35,700 --> 00:31:39,220
still have this problem of maybe there is a deceptive rewards

486
00:31:39,220 --> 00:31:42,780
there are, you know, I need to take a step back, I may not know

487
00:31:42,780 --> 00:31:46,820
some sort of some, some area of the search space. And don't I

488
00:31:46,820 --> 00:31:48,980
just run into the same problems again?

489
00:31:50,340 --> 00:31:56,700
So, so the important trick here is you need your model of the

490
00:31:56,700 --> 00:32:02,300
reward distribution, or the reward function to be one that

491
00:32:02,300 --> 00:32:05,700
captures uncertainty, like, maybe in a Bayesian way, or, you

492
00:32:05,700 --> 00:32:09,540
know, whichever way, the Bayesian way, by the way, fits well

493
00:32:09,540 --> 00:32:15,660
with the G flow net framework, because we can consider the

494
00:32:15,940 --> 00:32:20,380
parameters of the reward function as latent variables, like

495
00:32:20,380 --> 00:32:22,500
you don't actually know the reward function, you're trying

496
00:32:22,500 --> 00:32:25,500
to figure it out from experiments. So the G flow

497
00:32:25,500 --> 00:32:32,140
net can sample, and not just like what you should be doing in

498
00:32:32,140 --> 00:32:37,540
order to acquire information, but also potential reward function.

499
00:32:37,540 --> 00:32:41,740
So, you know, we don't actually have a knowledge of how the, you

500
00:32:41,740 --> 00:32:43,220
know, what's going to be the rewards we're going to get in

501
00:32:43,220 --> 00:32:48,380
the world. Classical IRL is going, as you said, to the expected

502
00:32:48,380 --> 00:32:52,820
value and try to maximize that, whereas the G flow net approach

503
00:32:52,860 --> 00:32:56,420
is trying to acquire as much knowledge as possible about the

504
00:32:56,420 --> 00:32:59,380
underlying reward function. So you're trying to minimize the

505
00:32:59,380 --> 00:33:04,860
uncertainty. So your model with the G flow net is modeling the

506
00:33:04,860 --> 00:33:11,460
uncertainty, and then it can use it as a reward for the policy

507
00:33:11,460 --> 00:33:14,660
that is going to do action in the real world. So we're talking

508
00:33:14,660 --> 00:33:17,460
about different G flow nets. There's a G flow net that models

509
00:33:17,460 --> 00:33:20,420
the uncertainty in the reward that you're going to get from the

510
00:33:20,420 --> 00:33:24,660
real world. And that's like a Bayesian model. And then you have

511
00:33:24,660 --> 00:33:29,300
another G flow net that controls the policy that searches to

512
00:33:29,380 --> 00:33:33,140
and its reward is how much uncertainty reduction you're

513
00:33:33,140 --> 00:33:40,100
going to get by doing this or that. So, so yeah, you need to

514
00:33:40,100 --> 00:33:45,460
have a part of your model that is kind of aware of the fact

515
00:33:45,460 --> 00:33:48,060
that there are whole areas in the world that you don't know

516
00:33:48,060 --> 00:33:50,580
about or aspects of the world that you don't know about so

517
00:33:50,580 --> 00:33:52,780
that you can drive the exploration.

518
00:33:53,580 --> 00:33:56,140
I would love to know where some of the magic is coming from.

519
00:33:56,780 --> 00:33:59,740
The promise of G flow nets is that we can discover as many

520
00:33:59,740 --> 00:34:03,420
modes as possible in the path distribution. Traditionally in

521
00:34:03,540 --> 00:34:06,660
Markov chain Monte Carlo, we had to hack priors into the

522
00:34:06,660 --> 00:34:09,380
algorithm by hand, you know, to find new modes or areas of

523
00:34:09,380 --> 00:34:11,580
information efficiently, especially when they were very

524
00:34:11,580 --> 00:34:15,100
far apart or not very sharp. The hypothesis of G flow nets is

525
00:34:15,140 --> 00:34:18,380
that the structure of these modes is learnable on many

526
00:34:18,380 --> 00:34:21,060
problems, even in high dimensions. It's a little bit like

527
00:34:21,060 --> 00:34:23,100
saying we're getting a free lunch. I mean, actually, I think

528
00:34:23,100 --> 00:34:25,980
you used that exact phrase to describe what we're doing here.

529
00:34:26,220 --> 00:34:29,700
Many research avenues have tried to develop general methods to

530
00:34:29,700 --> 00:34:32,780
discover these structures and have failed. How do you think

531
00:34:32,780 --> 00:34:36,020
G flow nets will overcome this seemingly intractable curse?

532
00:34:37,020 --> 00:34:40,340
There is no guarantee that they will, because if there is no

533
00:34:40,340 --> 00:34:43,740
structure in the underlying function you're trying to

534
00:34:43,740 --> 00:34:47,020
discover. So let's say the reward function or the energy

535
00:34:47,020 --> 00:34:52,100
function that you care about, then having visited some finite

536
00:34:52,100 --> 00:34:58,220
number of modes like regions where your reward is high site is

537
00:34:58,220 --> 00:35:00,340
not going to tell you anything about what are the other good

538
00:35:00,340 --> 00:35:04,580
places, the other modes. So so there's no guarantee that it

539
00:35:04,580 --> 00:35:09,120
will work. But if if there is structure, then there is a free

540
00:35:09,120 --> 00:35:12,780
lunch. And we know machine learning is good at that. Like

541
00:35:13,340 --> 00:35:17,540
the last 10 years of deep learning and its success. What is it

542
00:35:17,540 --> 00:35:20,140
telling us? It's telling us that you can generalize right that

543
00:35:20,140 --> 00:35:23,540
these nets, I'm not saying they generalize perfectly, but they

544
00:35:23,540 --> 00:35:28,900
can generalize. So you can think of it like the machine

545
00:35:28,900 --> 00:35:34,260
learning problem is given some examples of good things, like,

546
00:35:34,300 --> 00:35:37,260
you know, places where you get reward, you can you generalize

547
00:35:37,260 --> 00:35:41,180
to other places. And the supervised learning way of

548
00:35:41,180 --> 00:35:44,060
thinking about it is, you know, given a candidate place, tell me

549
00:35:44,060 --> 00:35:47,740
how much reward I think I would get. The G for that sampler is

550
00:35:47,740 --> 00:35:50,900
learning the inverse function is like to sample, but it's kind of

551
00:35:50,900 --> 00:35:54,020
the same thing. It's just going in the other direction. Give me

552
00:35:54,020 --> 00:35:57,700
some, you know, sample some some good places that that you know,

553
00:35:57,700 --> 00:36:02,520
where the reward is high. So we now have a lot of experience in

554
00:36:02,520 --> 00:36:07,780
designing powerful your nets that can be leveraged to

555
00:36:07,780 --> 00:36:12,020
generalize in those spaces where we normally use MCMC. And if

556
00:36:12,020 --> 00:36:17,500
there is kind of regularities that allow to generalize, then all

557
00:36:17,500 --> 00:36:20,300
of that can be, you know, put to use.

558
00:36:21,100 --> 00:36:23,820
We mentioned earlier, reinforcement learning often

559
00:36:23,820 --> 00:36:27,260
being applied in a context where you have this kind of solid

560
00:36:27,260 --> 00:36:30,100
reward function. So let's say games, you know, playing chess.

561
00:36:30,540 --> 00:36:33,700
I'm really curious, what would happen hypothetically, if we

562
00:36:33,940 --> 00:36:39,540
applied G flow net, you know, to something like chess. So I mean,

563
00:36:39,540 --> 00:36:42,180
I think given the fact that reinforcement learning like say

564
00:36:42,180 --> 00:36:45,300
alpha zero is trained specifically to choose the best

565
00:36:45,300 --> 00:36:49,580
move rather than diverse moves, it seems obvious that maybe if

566
00:36:49,580 --> 00:36:53,860
given equal resources to both alpha zero and flow zero, alpha

567
00:36:53,860 --> 00:36:58,060
zero would probably beat flow zero. However, I think if flow

568
00:36:58,060 --> 00:37:01,620
zero were given more resources, say and trained to the same

569
00:37:01,620 --> 00:37:05,940
rating, say the same elo rating as alpha zero, it seems like

570
00:37:06,060 --> 00:37:09,740
flow zero, if you would, would play significantly more

571
00:37:09,740 --> 00:37:14,460
diverse and interesting games with a wider variety of styles.

572
00:37:14,460 --> 00:37:17,060
And I think you could even imagine also that it could be

573
00:37:17,060 --> 00:37:21,500
possible, even if given equal resources, but sufficiently

574
00:37:21,500 --> 00:37:25,660
high enough resources, that a hypothetical flow zero would

575
00:37:25,660 --> 00:37:29,260
consistently reach higher ratings, because it might find, you

576
00:37:29,300 --> 00:37:32,340
know, more interesting stepping stones that have the

577
00:37:32,340 --> 00:37:36,340
potential to avoid deception, because it can explore seemingly

578
00:37:36,620 --> 00:37:41,060
lower reward paths that ultimately develop into higher reward.

579
00:37:41,100 --> 00:37:42,820
More curious if you have any thoughts on that?

580
00:37:43,780 --> 00:37:51,100
Yeah. It's a good question. I would say where the kind of

581
00:37:51,100 --> 00:37:55,020
approach we've been pioneering with G flow nets might be really

582
00:37:55,020 --> 00:37:59,860
paying off is if you think about it from the perspective of the

583
00:37:59,860 --> 00:38:03,340
learner has a finite computational, you know, amount of

584
00:38:03,340 --> 00:38:07,700
resources, because in principle, right, if you had infinite

585
00:38:07,700 --> 00:38:11,940
compute, and you know the reward function, like the rules of

586
00:38:11,940 --> 00:38:16,620
chess or go, then you can just crank and find, you know, the

587
00:38:16,620 --> 00:38:21,780
policy that's best in every possible setting. Now, if you

588
00:38:21,780 --> 00:38:24,780
have finite resources, like, you know, you, you have a budget

589
00:38:24,780 --> 00:38:30,780
of compute, you'd like to use it efficiently. And so that's

590
00:38:30,780 --> 00:38:33,820
where the exploration exploitation trade off becomes

591
00:38:33,820 --> 00:38:44,340
important. And if you if you had a, say, a current policy that

592
00:38:44,340 --> 00:38:48,900
you're not completely sure is the right one. And, and then

593
00:38:48,900 --> 00:38:54,220
you're trying to say, Well, what, how should I play so that I'm

594
00:38:54,220 --> 00:38:57,860
going to improve my policy the most as in I'm going to reduce

595
00:38:58,220 --> 00:39:01,220
the uncertainty that, you know, it is the right policy, like

596
00:39:01,220 --> 00:39:05,620
that it picks the right things. So now we're getting closer to

597
00:39:05,620 --> 00:39:08,740
the kind of setting where it makes sense to use G flow nets.

598
00:39:09,420 --> 00:39:14,020
And then what I would expect, if we do the engineering work

599
00:39:14,020 --> 00:39:17,540
here, but based on the sort of much simpler problems we've

600
00:39:17,540 --> 00:39:22,660
looked at, is that it would converge faster. So given, if you

601
00:39:22,660 --> 00:39:28,860
look at on the x axis, the number of games you're playing. And

602
00:39:28,860 --> 00:39:32,900
on the y axis, how good is your policy measured like on other

603
00:39:32,900 --> 00:39:35,980
games. So that's where you would get. In other words, it's the

604
00:39:35,980 --> 00:39:39,580
learning curve that you might gain on asymptotically, everything

605
00:39:39,580 --> 00:39:43,700
is going to converge the optimal chess player, right? So the

606
00:39:43,700 --> 00:39:46,900
the place where it's interesting is to look at the learning

607
00:39:46,980 --> 00:39:50,340
curve how fast you learn. And here you want to sort of active

608
00:39:50,340 --> 00:39:53,580
learning thinking like, Well, I'm not just trying to win here.

609
00:39:54,220 --> 00:39:58,980
I'm trying to gather information so that I'll win more in the

610
00:39:58,980 --> 00:40:02,300
future. And it's a different objective. And that's where you

611
00:40:02,300 --> 00:40:05,220
need diversity and exploration and like a model of your own

612
00:40:05,220 --> 00:40:08,180
uncertainty and an active learning policy.

613
00:40:09,380 --> 00:40:14,580
How much do you think this could be part of not maybe only

614
00:40:14,580 --> 00:40:20,540
reward maximization things, but information collection, things

615
00:40:20,540 --> 00:40:25,940
like, I'm sure you're you're thinking about in, let's say the

616
00:40:25,940 --> 00:40:29,380
brain, there is there's sort of maybe a similar process going

617
00:40:29,380 --> 00:40:32,860
on and what do I still need to retrieve in order to give certain

618
00:40:32,860 --> 00:40:37,780
answers to questions, or maybe in our, let's say, big search

619
00:40:37,780 --> 00:40:42,540
engine, let's just name one for naming sake, let's Google, or

620
00:40:42,580 --> 00:40:47,820
so would would try to answer your query, not by just searching

621
00:40:47,820 --> 00:40:50,700
through their index, but by actively doing this multiple

622
00:40:50,820 --> 00:40:53,780
multiple things like, is this enough? Is this enough? Is this

623
00:40:53,780 --> 00:40:58,260
enough? Do you see connections to these types of things? Or are

624
00:40:58,260 --> 00:41:01,500
they inherently different? Because they might be not learning

625
00:41:01,500 --> 00:41:02,260
on the spot?

626
00:41:02,820 --> 00:41:08,060
What they're doing on the spot is acquiring information. And you

627
00:41:08,060 --> 00:41:10,140
want to do it in an efficient way. And that's where sort of the

628
00:41:10,140 --> 00:41:11,740
active learning thinking comes in.

629
00:41:12,700 --> 00:41:17,860
And I think it's actually a very big, practical problem in

630
00:41:17,860 --> 00:41:21,260
the deployment of like AI dialogue systems that are not

631
00:41:21,260 --> 00:41:26,260
chit chat, but they're trying to say help a user achieve, you

632
00:41:26,260 --> 00:41:28,900
know, get something get information or something like

633
00:41:28,900 --> 00:41:32,460
this. This is this is a huge need for this in, you know, the

634
00:41:32,460 --> 00:41:35,980
business world and search engines, and you know, it's much

635
00:41:35,980 --> 00:41:39,340
more than search engines. So I don't think we have the

636
00:41:39,340 --> 00:41:43,980
algorithms that do that right now. And it's kind of painful. The

637
00:41:43,980 --> 00:41:49,420
human has to know, you know, is driving. But if, if we had

638
00:41:49,420 --> 00:41:53,980
systems that could explicitly model their own, say,

639
00:41:53,980 --> 00:41:57,580
uncertainty about what the user needs or wants, or where to

640
00:41:57,580 --> 00:42:03,580
find information. And then, and you need like pretty powerful

641
00:42:03,580 --> 00:42:06,300
models of that, like it's not just galaxies, they're simple

642
00:42:06,300 --> 00:42:09,620
things. That's where G flow net strengths comes in, you can

643
00:42:09,620 --> 00:42:12,660
represent very, very complex distributions over

644
00:42:12,660 --> 00:42:19,140
compositional objects. It's not just a few numbers. And then I

645
00:42:19,140 --> 00:42:23,420
think you could get to much more efficient human machine

646
00:42:23,420 --> 00:42:30,420
interfaces. And the same, I believe the same methodology

647
00:42:30,420 --> 00:42:33,420
could be used more generally in scientific discovery. So what

648
00:42:33,460 --> 00:42:35,940
is scientific discovery? Like what is it that scientists do?

649
00:42:36,580 --> 00:42:40,500
They plan experiments that are going to allow them to reduce

650
00:42:40,500 --> 00:42:43,980
the uncertainty on their theories of, you know, some

651
00:42:43,980 --> 00:42:46,420
aspect of the world. It's the same problem. Yeah, you have a

652
00:42:46,420 --> 00:42:50,340
series of questions you're allowed to ask to nature. And you

653
00:42:50,340 --> 00:42:53,340
try to ask as few questions as possible to as quickly as

654
00:42:53,340 --> 00:42:55,060
possible, understand what's going on.

655
00:42:56,020 --> 00:42:59,220
Is there a connection fundamentally to I'm thinking of

656
00:42:59,220 --> 00:43:02,980
causality, which also I've seen a number of papers that you've

657
00:43:03,020 --> 00:43:06,420
collaborated on with people who are who are deep into

658
00:43:06,460 --> 00:43:10,900
causality research and so on. What do you think there is a

659
00:43:12,740 --> 00:43:18,180
a connection there where an agent could learn to uncover if

660
00:43:18,180 --> 00:43:20,700
you think about scientific discovery to uncover the

661
00:43:20,700 --> 00:43:24,780
fundamental causal structure of the world by asking such

662
00:43:24,780 --> 00:43:27,980
questions, like could there be a connection to that branch of

663
00:43:27,980 --> 00:43:32,340
research? And could this finally be like the unification of

664
00:43:32,620 --> 00:43:37,500
of something machine learning and the the world of causality?

665
00:43:38,740 --> 00:43:41,220
Yes, you guys are really asking all the right questions. Thank

666
00:43:41,220 --> 00:43:46,540
you so much. In fact, one of my main motivations for the

667
00:43:46,580 --> 00:43:51,260
pursuing the the G flow net research program is that I think

668
00:43:51,260 --> 00:43:58,100
it's the it's an ideal tool for implementing what I called in

669
00:43:58,100 --> 00:44:03,460
my talks, system to inductive biases. So what this means is

670
00:44:03,940 --> 00:44:06,580
there are lots of things we know from neuroscience and

671
00:44:06,580 --> 00:44:12,220
cognitive science about how we think. And we can bring that

672
00:44:12,260 --> 00:44:18,620
into the design of probabilistic machine learning, you know,

673
00:44:18,620 --> 00:44:22,940
based on deep learning is the building blocks. And one of the

674
00:44:22,940 --> 00:44:26,220
inductive biases, like one of the characteristics of how we

675
00:44:26,220 --> 00:44:29,140
think is we think causally, we're constantly asking the why

676
00:44:29,140 --> 00:44:34,020
questions we're trying to find explanations and so on. And, and

677
00:44:34,060 --> 00:44:37,940
and that connects with classical AI, like the way we think, to

678
00:44:37,940 --> 00:44:42,020
some extent, has also inspired classical AI, you know, rules

679
00:44:42,020 --> 00:44:46,620
and logic and and reasoning. And we haven't yet found the way

680
00:44:46,620 --> 00:44:50,380
to integrate these abilities in deep learning. And of course,

681
00:44:50,380 --> 00:44:54,700
lots of people are like, trying to and and that's important.

682
00:44:55,700 --> 00:44:59,100
But but I think the reason why G for nets give us an amazing

683
00:44:59,100 --> 00:45:03,940
handle on this is because they they're really good at

684
00:45:03,940 --> 00:45:08,300
representing distributions and sampling over graphs. And, and

685
00:45:08,300 --> 00:45:12,660
like a reasoning or a set of possible reasoning to explain

686
00:45:12,660 --> 00:45:18,540
something or to, you know, for planning. The these are graphs.

687
00:45:19,780 --> 00:45:24,300
And your thoughts can be seen as graphs, right? So think of like,

688
00:45:24,460 --> 00:45:27,140
maybe a simple version of this, think of a parse, like a

689
00:45:27,140 --> 00:45:32,500
semantic and syntactic parse of a sentence is a graph. But

690
00:45:32,500 --> 00:45:34,260
usually it's, you know, it's more than a tree, there are all

691
00:45:34,260 --> 00:45:37,300
sorts of semantic connections, including with knowledge graphs,

692
00:45:37,300 --> 00:45:42,900
right, which also graphs. So the ability to implicitly represent

693
00:45:42,940 --> 00:45:46,900
those distributions and sample pieces of them as thoughts is, I

694
00:45:46,900 --> 00:45:50,740
think, fundamental to how we think. And going back to

695
00:45:50,740 --> 00:45:53,020
causality, one of the hard questions that I think G for

696
00:45:53,020 --> 00:45:56,940
nets can help us with is causal discovery. So in other words,

697
00:45:57,060 --> 00:45:59,420
what is the underlying cause structure of the world, including

698
00:45:59,420 --> 00:46:03,980
the uncertainty about it? Given the things we observe, a lot of

699
00:46:03,980 --> 00:46:06,540
the research and causality has been okay, we observe these,

700
00:46:06,580 --> 00:46:10,900
these random variables, discover, you know, make inferences

701
00:46:10,900 --> 00:46:14,140
about, you know, whether what we can say about whether it goes

702
00:46:14,140 --> 00:46:19,500
to be and so on. But it's much harder to discover the causal

703
00:46:19,540 --> 00:46:24,020
graph that that, you know, in a large set of variables, and

704
00:46:24,020 --> 00:46:26,980
it's even harder. And really, nobody's done a real job there.

705
00:46:27,420 --> 00:46:31,060
To do this when what the learner sees is not the causal

706
00:46:31,060 --> 00:46:34,140
variables, but just like low level pixels. And you also have

707
00:46:34,140 --> 00:46:36,260
to figure out what are the causal variables and how they're

708
00:46:36,260 --> 00:46:39,220
related causal. And I think G planets can help us do that.

709
00:46:39,940 --> 00:46:43,060
This this opens up, this is so many avenues of questions, I

710
00:46:43,060 --> 00:46:46,780
think it'll probably almost be a future episode in itself. But

711
00:46:46,780 --> 00:46:52,140
let me just ask you about some of the basic ones, which is, as

712
00:46:52,140 --> 00:46:54,740
you mentioned, kind of learning the causality causality

713
00:46:54,740 --> 00:46:58,580
structure, much more difficult problem. And the first question

714
00:46:58,580 --> 00:47:01,900
is just how to represent the causality. And so you, you, you

715
00:47:01,900 --> 00:47:04,380
mentioned graphs, you know, graphs is one way. And of course,

716
00:47:04,740 --> 00:47:09,260
you can develop, you know, isomorphic ways of representing

717
00:47:09,660 --> 00:47:12,740
certain parts of logic as graphs, etc, depending on how, you

718
00:47:12,740 --> 00:47:16,500
know, how rich you make the graph structure. But there's

719
00:47:16,500 --> 00:47:18,860
also the other issue of, you know, when you're trying to

720
00:47:18,860 --> 00:47:21,620
build, and I think it's probably correct to call this a world

721
00:47:21,620 --> 00:47:24,220
model, right, like we're trying to build a causal

722
00:47:24,420 --> 00:47:27,780
that's the word I use. Okay, great. And I, and so I have one

723
00:47:27,780 --> 00:47:31,860
quick question about that, which is, you know, to me, to some

724
00:47:31,860 --> 00:47:34,860
people, world model is only the discriminative function. It's

725
00:47:34,860 --> 00:47:37,700
just that, you know, probability y given x, to me, it's more

726
00:47:37,700 --> 00:47:41,300
general. It's also the structure of x. Is that, is that also

727
00:47:41,300 --> 00:47:44,540
your, your view as well? Yes. Yes. Okay. And so in

728
00:47:44,580 --> 00:47:47,700
constructing those, those world models, some of the, let's say

729
00:47:47,700 --> 00:47:52,700
the pushback on on these type of generative techniques from, from

730
00:47:52,700 --> 00:47:55,620
folks that are more skew more towards the discriminative side

731
00:47:55,980 --> 00:47:58,860
is, hey, look, fine, you're going to go and try and build this

732
00:47:58,860 --> 00:48:01,580
generative model, it's going to be even more complicated than

733
00:48:01,580 --> 00:48:04,860
this discriminative model, because it also has to learn, you

734
00:48:04,860 --> 00:48:08,020
know, the structure on x. But I think the possible free lunch

735
00:48:08,020 --> 00:48:13,140
here, is that you can learn abstract structure on on x. And

736
00:48:13,140 --> 00:48:17,100
so if you learn these abstract world models, throwing away all

737
00:48:17,100 --> 00:48:19,700
the nitty gritty that doesn't really matter, you can potentially

738
00:48:19,700 --> 00:48:23,660
have very powerful, you know, predictive encoding, if you will,

739
00:48:23,700 --> 00:48:25,420
like, what's, what's your thoughts on that?

740
00:48:25,900 --> 00:48:30,740
Oh, that's what I've been thinking for almost 20 years. And

741
00:48:30,740 --> 00:48:34,140
one of the reasons why I've been interested in deep learning as

742
00:48:34,180 --> 00:48:39,140
a way to think of discovering abstract representations, you

743
00:48:39,140 --> 00:48:44,020
know, from the early days of deep learning, as in like mid like

744
00:48:44,020 --> 00:48:49,180
2005 or something. And, and in the paper that Jan McCarr and I

745
00:48:49,180 --> 00:48:53,020
wrote about, and also other papers I wrote with some of my

746
00:48:53,020 --> 00:48:56,540
colleagues at the University of Montreal on, you know, deep

747
00:48:56,540 --> 00:49:00,780
learning around 2010, they are all about that notion that we

748
00:49:00,780 --> 00:49:05,860
would like these unsupervised learning procedures to discover

749
00:49:05,860 --> 00:49:10,780
these abstract factors, as we call them. But now I think it's

750
00:49:10,780 --> 00:49:14,460
not just the factors like the variables, but it's also more

751
00:49:14,460 --> 00:49:18,100
importantly, even how they're related to each other, which in

752
00:49:18,100 --> 00:49:24,460
the causal language is what we call causal mechanisms. And so

753
00:49:24,500 --> 00:49:27,260
here's a fundamental way of thinking about this. If you

754
00:49:27,260 --> 00:49:31,020
don't introduce the abstract kind of structure that exists in

755
00:49:31,020 --> 00:49:36,900
the world, then representing p of x, the input distribution is

756
00:49:36,900 --> 00:49:40,740
very difficult. It's, in other words, you'll need a lot of data

757
00:49:40,740 --> 00:49:44,540
to learn it. And it's not going to be generalizing very well. The

758
00:49:44,540 --> 00:49:49,260
whole point of abstraction is that it gives you very powerful

759
00:49:49,260 --> 00:49:51,620
abilities to generalize to new settings, including out of

760
00:49:51,620 --> 00:49:53,860
distribution, which is one of the hardest topics in machine

761
00:49:53,860 --> 00:49:57,100
learning right now. How do we extend what we do so that it

762
00:49:57,100 --> 00:50:01,300
generalizes well in new settings? And thinking causally

763
00:50:01,300 --> 00:50:05,900
about these abstract causal dependencies, as the things that

764
00:50:05,900 --> 00:50:10,580
are preserved across changes in distribution, like, if I go to

765
00:50:10,580 --> 00:50:14,820
the moon, it's the same laws of physics, but the distribution is

766
00:50:14,820 --> 00:50:18,460
very different. How do I generalize, you know, across such

767
00:50:18,460 --> 00:50:23,980
changes in distribution? It's because the learner is us, you

768
00:50:23,980 --> 00:50:28,060
know, if we, if we were, if we had the right education, has

769
00:50:28,060 --> 00:50:31,980
figured out the underlying, at least, you know, enough of the

770
00:50:31,980 --> 00:50:35,740
underlying causal mechanisms, that we can be transported in a

771
00:50:35,740 --> 00:50:39,940
different world, but where there's the same laws of physics,

772
00:50:40,340 --> 00:50:44,300
and we can predict what's going to happen, even though it looks

773
00:50:44,300 --> 00:50:47,860
completely different from, you know, our training environment.

774
00:50:48,380 --> 00:50:55,820
So the, the idea of extraction is really that if you introduce

775
00:50:55,820 --> 00:51:01,500
abstractions, the description length of the data becomes way

776
00:51:01,500 --> 00:51:03,780
smaller. And that's why you get generalization.

777
00:51:05,380 --> 00:51:06,060
Absolutely.

778
00:51:06,220 --> 00:51:10,220
I'm fascinated by these abstract categories. I think it's the

779
00:51:10,220 --> 00:51:13,060
most exciting thing in AI. I mean, Douglas Hofstadter spoke

780
00:51:13,060 --> 00:51:16,860
about cognitive categories, like the concept of sour grapes,

781
00:51:16,860 --> 00:51:19,900
for example, to represent the certain thing. And almost

782
00:51:19,900 --> 00:51:23,740
magically, our brain seems to arrange these cognitive

783
00:51:23,740 --> 00:51:25,660
categories. And it's not entirely clear to me whether they're

784
00:51:25,660 --> 00:51:29,260
an emergent phenomenon, or whether it's some other process.

785
00:51:29,540 --> 00:51:31,980
But the modes that you're discovering in G flow nets,

786
00:51:31,980 --> 00:51:35,540
they're a kind of category, these cognitive categories that I

787
00:51:35,540 --> 00:51:38,180
just spoke about our abstractions, also things like

788
00:51:38,180 --> 00:51:41,060
causality and geometric deep learning that they are kinds of

789
00:51:41,060 --> 00:51:43,420
categories. But I've always had this intuition that deep

790
00:51:43,420 --> 00:51:47,700
learning doesn't learn the categories on its own, it needs

791
00:51:47,700 --> 00:51:52,220
humans to kind of put priors into the model, as we do with

792
00:51:52,260 --> 00:51:55,100
geometric deep learning. Do you think that that will always be

793
00:51:55,100 --> 00:51:57,380
the case? Or can we have that meta level of learning?

794
00:51:58,180 --> 00:52:02,820
Yes. What I really want to do is build machines that can

795
00:52:02,820 --> 00:52:06,140
discover their own semantic categories, abstract ones that

796
00:52:06,140 --> 00:52:09,740
really help them understand the world. And of course, they're

797
00:52:09,740 --> 00:52:13,220
going to learn, you know, better and faster if we help them just

798
00:52:13,220 --> 00:52:15,700
like, you know, we teach kids, we don't let them discover the

799
00:52:15,700 --> 00:52:21,780
world by themselves. But we do have an ability to invent new

800
00:52:21,780 --> 00:52:24,020
categories. That's what scientists do all the time,

801
00:52:24,020 --> 00:52:29,380
right? Or artists and, you know, writers and philosophers and

802
00:52:29,380 --> 00:52:32,700
scholars, and ordinary people who find new solutions to

803
00:52:32,700 --> 00:52:36,060
problems, we do that all the time, our brain is a machine

804
00:52:36,100 --> 00:52:39,620
discovers new abstractions. Of course, that usually it's just

805
00:52:39,620 --> 00:52:42,580
like one little bit on top of all the things we got from our

806
00:52:42,620 --> 00:52:48,180
cultural input. But but that's the ability that we don't have

807
00:52:48,180 --> 00:52:53,020
right now in machine learning. And that is going to, I think, be

808
00:52:53,020 --> 00:52:56,500
a huge advantage. So now we're not in reinforcement learning,

809
00:52:56,500 --> 00:52:59,620
we're not in active learning, we're talking about unsupervised

810
00:52:59,620 --> 00:53:04,580
learning. So we're talking about how can a machine discover

811
00:53:04,580 --> 00:53:14,260
these often discrete concepts that somehow help it understand.

812
00:53:14,260 --> 00:53:17,020
So in other words, build a compact understanding of lots of

813
00:53:17,020 --> 00:53:22,380
things that generalize across many settings. And yeah, that

814
00:53:22,420 --> 00:53:28,900
that's that the path to build that is, is becoming more and more

815
00:53:29,900 --> 00:53:35,860
firm in my mind, as I move forward with G flow nets. So as a

816
00:53:35,860 --> 00:53:39,420
clue, there was a paper we had recently, I think in Europe's

817
00:53:39,820 --> 00:53:43,820
on that's connected to the global workspace theory that says

818
00:53:43,820 --> 00:53:47,500
that it's about discrete valued neural communication, I think

819
00:53:47,500 --> 00:53:51,860
is a title where the one interesting intuition here is

820
00:53:51,860 --> 00:53:58,660
connected to this is if you if you constrain the communication

821
00:53:58,660 --> 00:54:01,060
between different modules, say in the brain or in machine

822
00:54:01,060 --> 00:54:04,540
learning system, to use as few bits as possible and discrete is

823
00:54:04,540 --> 00:54:08,220
the way to get the very few bits. You can get better

824
00:54:08,220 --> 00:54:12,180
generalization. And there are good reasons for that that we

825
00:54:12,180 --> 00:54:14,980
try to explain in the paper. But but that's, that's it, you

826
00:54:14,980 --> 00:54:19,980
know, there's a clue here that discrete concepts emerge as a

827
00:54:19,980 --> 00:54:21,460
way to get better generalization.

828
00:54:22,860 --> 00:54:27,820
You you mentioned before, and in terms of discreteness, and

829
00:54:27,860 --> 00:54:32,140
what you mentioned before with graphs being very fundamental, it

830
00:54:32,140 --> 00:54:35,620
connects a little bit back to a paper that you, I think,

831
00:54:35,620 --> 00:54:40,860
provocatively titled the consciousness prior, where where

832
00:54:40,860 --> 00:54:44,980
you connect sort of the ideas of attention, sparse factor,

833
00:54:44,980 --> 00:54:48,660
graphs, language, things being discreet, things being

834
00:54:48,660 --> 00:54:54,500
describable by language, right? And, and I find that all to be

835
00:54:54,540 --> 00:55:00,100
very interesting. On the topic of consciousness, we would be, it

836
00:55:00,100 --> 00:55:03,100
would not be appropriate for us to not put this question to you.

837
00:55:03,100 --> 00:55:07,060
So you're not, you're not very active on Twitter, which is

838
00:55:07,060 --> 00:55:10,940
probably why you're so productive. But if currently,

839
00:55:11,060 --> 00:55:14,580
there is a bit of a of a thing happening on Twitter, namely,

840
00:55:15,020 --> 00:55:21,900
Ilya Satskever of Open AI has tweeted out a seemingly innocuous

841
00:55:22,180 --> 00:55:27,660
tweet saying, it may be that today's large neural networks are

842
00:55:27,660 --> 00:55:33,580
slightly conscious, which has resulted in quite a, let's say,

843
00:55:33,580 --> 00:55:38,300
a storm on of people agreeing, disagreeing. Obviously, he's

844
00:55:38,300 --> 00:55:41,300
he's talking about maybe, you know, the large language models

845
00:55:41,300 --> 00:55:44,540
we have today, which do incorporate a lot of the things

846
00:55:44,540 --> 00:55:47,860
you talk about, they do incorporate attention mechanisms,

847
00:55:47,940 --> 00:55:52,140
lots of them. Presumably, it's all one needs. They do

848
00:55:52,140 --> 00:55:55,260
incorporate language, they do incorporate discrete things with

849
00:55:55,260 --> 00:55:59,140
you know, discrete tokens and so on. What do you make of a

850
00:55:59,140 --> 00:56:02,300
statement like this? It may be that today's large neural

851
00:56:02,300 --> 00:56:04,300
networks are slightly conscious.

852
00:56:05,380 --> 00:56:10,060
Well, this one fundamental problem with such statements,

853
00:56:11,500 --> 00:56:15,940
which is we don't know what consciousness really is. So I

854
00:56:15,980 --> 00:56:21,420
think we have to have a bit of humility here. And I can't say

855
00:56:21,460 --> 00:56:24,740
what Ilya is saying is true or not. I think that this is more to

856
00:56:24,740 --> 00:56:29,060
consciousness than what we have in these large language models by

857
00:56:29,060 --> 00:56:35,540
a big gap. But that being said, and you know, we do need to work

858
00:56:35,540 --> 00:56:39,580
with our colleagues in your science and kind of science who

859
00:56:39,580 --> 00:56:42,860
are trying to figure out what consciousness is from a scientific

860
00:56:43,300 --> 00:56:47,220
perspective and philosophers who are helping also to make sense of

861
00:56:47,220 --> 00:56:54,060
that landscape. So we have to be careful with the use of those

862
00:56:54,060 --> 00:56:56,660
words. And you know, I was a bit liberal in the title of my

863
00:56:56,780 --> 00:57:01,420
paper. And I learned a lot about consciousness since then,

864
00:57:02,260 --> 00:57:05,420
learned that there's a lot that we don't understand that at the

865
00:57:05,420 --> 00:57:08,500
same time, there are enough bits that we know from from

866
00:57:08,540 --> 00:57:14,860
cognitive neuroscience that can serve as inspiration for how we

867
00:57:14,860 --> 00:57:18,540
could build machine learning systems that have similar, say

868
00:57:18,540 --> 00:57:21,580
conscious processing machinery. Okay, let's not say consciousness

869
00:57:21,580 --> 00:57:23,500
but just conscious processing machine because that's less

870
00:57:23,500 --> 00:57:27,300
controversial. And by the way, the word consciousness has been

871
00:57:27,300 --> 00:57:32,260
taboo with most of science for a long time. And it has become

872
00:57:32,300 --> 00:57:36,540
untapped, you know, the tabooed in neuroscience, because we're

873
00:57:36,540 --> 00:57:39,060
starting to be able to make measurements of what's going on

874
00:57:39,100 --> 00:57:42,820
inside your brain, while you're doing things consciously or not

875
00:57:42,820 --> 00:57:46,940
and so on and distinguish the parts that you're consciously

876
00:57:46,940 --> 00:57:49,620
aware of and the parts that are there in your brain, but you're

877
00:57:49,620 --> 00:57:52,140
not conscious. So we're trying to we're starting to make a lot of

878
00:57:52,140 --> 00:57:55,420
progress of what it means to be conscious of something or not.

879
00:57:56,940 --> 00:58:02,180
And I, you know, I think this is a very exciting and important

880
00:58:02,220 --> 00:58:08,540
scientific question. And I would rather like work on exploring

881
00:58:08,540 --> 00:58:12,460
hypotheses and theories to explain our conscious abilities,

882
00:58:13,420 --> 00:58:16,580
rather than make bold statements about whether current neural

883
00:58:16,580 --> 00:58:17,900
nets are conscious or not.

884
00:58:18,620 --> 00:58:22,580
Professor Benjo, we've got some David Chalmers on the show next

885
00:58:22,580 --> 00:58:25,580
month. Do you have any questions that you had put to him?

886
00:58:26,420 --> 00:58:36,060
I very much like a hypothesis about consciousness that Michael

887
00:58:36,060 --> 00:58:45,380
Graziano has put out to help explain the qualia, the subjective

888
00:58:45,380 --> 00:58:51,340
experience part that Chalmers wrote might be something science

889
00:58:51,340 --> 00:58:58,740
can't really, you know, touch. And so what's, you know, I'd like

890
00:58:58,740 --> 00:59:03,060
to hear what he has to say about these kinds of approaches. And

891
00:59:03,220 --> 00:59:10,700
one of the basic premise here is is very grounded in things we

892
00:59:10,700 --> 00:59:16,460
can do scientifically. It's to say, well, let's not try to

893
00:59:16,460 --> 00:59:19,780
figure out what is consciousness or subjective

894
00:59:19,780 --> 00:59:24,940
experience more specifically, you know, from a philosopher's

895
00:59:24,940 --> 00:59:30,100
chairs. But let's let's consider that as a phenomenon that is

896
00:59:30,100 --> 00:59:32,580
happening in the brain. I mean, unless you believe in sort of

897
00:59:32,580 --> 00:59:35,300
supernatural things, if it is happening, something is happening

898
00:59:35,300 --> 00:59:38,360
in the brain, and we can report about it. And we can, we can

899
00:59:38,360 --> 00:59:41,780
like, measure what's going on in various parts of your brain

900
00:59:41,780 --> 00:59:50,540
while this is happening. And then, you know, can we then come up

901
00:59:50,540 --> 00:59:55,580
with theories that explain why we feel that we have subjective

902
00:59:55,580 --> 00:59:57,960
experience? It's not saying whether consciousness exists or

903
00:59:57,960 --> 01:00:00,940
not or subjectivity. It's not whether it exists or not in some

904
01:00:00,940 --> 01:00:03,540
sort of logical sense. It's whether, you know, what is it

905
01:00:03,540 --> 01:00:06,340
that's going down in our brain that gives us that feeling and

906
01:00:06,340 --> 01:00:12,540
then make us say, Well, I am, you know, I'm conscious of x, y,

907
01:00:12,540 --> 01:00:17,860
or z. So so that's the that's the direction I find interesting

908
01:00:17,860 --> 01:00:21,420
because it opens the door for a scientific investigation. And

909
01:00:21,420 --> 01:00:26,340
Michael Grosjean has a specific theory about that which I find

910
01:00:26,340 --> 01:00:32,380
compelling that is really rooted in the idea that we have a world

911
01:00:32,380 --> 01:00:38,180
model. And then we we because we have an attention that focuses

912
01:00:38,180 --> 01:00:43,980
only parts of it at a time. And we need to have like a little

913
01:00:44,100 --> 01:00:48,820
mini world model that controls that attention. That creates a

914
01:00:48,820 --> 01:00:55,860
sort of separation between the the where the real knowledge is

915
01:00:55,860 --> 01:01:00,580
and sort of this more abstract control and machinery that could

916
01:01:00,620 --> 01:01:06,020
well, give us this illusion of Cartesian dualism, which I think

917
01:01:06,060 --> 01:01:09,980
is an illusion, but but you know, must be grounded in some

918
01:01:11,860 --> 01:01:12,980
you know, biological

919
01:01:15,740 --> 01:01:19,580
reality. And that's I think understanding that is is a very

920
01:01:19,580 --> 01:01:22,580
good question to ask. And I'd like to get to know what he

921
01:01:22,580 --> 01:01:24,180
thinks about such a research program.

922
01:01:25,300 --> 01:01:26,220
Thank you very cool.

923
01:01:26,780 --> 01:01:30,780
Yeah, thank you. I do have one kind of nitty gritty question

924
01:01:30,780 --> 01:01:34,460
because and partly partly based on some of your recent work on

925
01:01:34,460 --> 01:01:40,580
becoming more of a fan of semi supervised learning. And you

926
01:01:40,580 --> 01:01:44,180
know, you had a recent paper that was on interpolation

927
01:01:44,180 --> 01:01:48,900
consistency training. And what I found interesting about that is

928
01:01:48,900 --> 01:01:52,580
that if we consider one of the biggest challenges that we face

929
01:01:52,580 --> 01:01:55,580
in machine learning pretty much across the board is an

930
01:01:55,620 --> 01:01:58,700
overcoming the various, you know, curses, if you will, the

931
01:01:58,700 --> 01:02:02,660
various forms of intractability that we have an empirical

932
01:02:02,660 --> 01:02:06,500
learning methods. And in this context of semi supervised

933
01:02:06,500 --> 01:02:11,260
learning, that recent paper, it found significant improvements

934
01:02:11,300 --> 01:02:15,900
over state of the art by forcing linearity. So in this case, it

935
01:02:15,900 --> 01:02:20,660
was by this mix up between the unlabeled samples and their

936
01:02:20,940 --> 01:02:25,740
interpolated fake labels. And in the last decade, we've also

937
01:02:25,740 --> 01:02:29,740
seen values come to dominance in the field of neural networks,

938
01:02:29,740 --> 01:02:34,900
their piecewise linear recent work by Randall Belastriero,

939
01:02:35,220 --> 01:02:37,980
developed an interesting frame of reference which cast

940
01:02:37,980 --> 01:02:41,860
multi layer perceptrons as a decomposition method, which

941
01:02:41,860 --> 01:02:46,700
produces a honeycomb of linear cells in the ambient space and

942
01:02:46,780 --> 01:02:50,660
they're activated turned off or on by input examples. So my

943
01:02:50,660 --> 01:02:54,260
question is, why is linearity, whether it's piecewise or

944
01:02:54,260 --> 01:02:58,460
otherwise, dominating the state of the art in approximation

945
01:02:58,460 --> 01:03:01,580
methods, it almost seems to me like we've kind of gone back to

946
01:03:01,580 --> 01:03:04,660
the future, if you will, sort of leaving behind attempts at more

947
01:03:04,940 --> 01:03:08,740
smooth nonlinear methods and gone back to newer, albeit more

948
01:03:08,740 --> 01:03:12,980
complicated forms of linear approximation.

949
01:03:13,860 --> 01:03:18,940
Right. I would say something that's roughly linear is

950
01:03:18,940 --> 01:03:22,820
simpler. So having a regularizer that says, oh, you want to be

951
01:03:22,820 --> 01:03:26,260
roughly linear or locally linear, at least to as much

952
01:03:26,260 --> 01:03:30,900
extent as you can is a smoothness prior. So that's going to

953
01:03:30,900 --> 01:03:36,300
help generalization. But it could also hurt if that is too

954
01:03:36,300 --> 01:03:40,180
strong. And so having these piecewise linear kind of more

955
01:03:40,780 --> 01:03:45,980
type of solution is a good compromise. It says as few pieces

956
01:03:45,980 --> 01:03:50,260
as possible, and ideally organized in a compositional way. So

957
01:03:50,260 --> 01:03:55,660
that it's not just like a relu, it's more like the discrete

958
01:03:55,700 --> 01:03:59,340
abstract logic, you know, reasoning, things sitting on

959
01:03:59,340 --> 01:04:04,060
top, that's controlling the pieces. But but otherwise fairly

960
01:04:04,060 --> 01:04:08,460
simple in each how each of the pieces are, you know, like

961
01:04:08,460 --> 01:04:12,420
linear, for example. So one way to look at this is, if you

962
01:04:12,420 --> 01:04:15,660
look at classical, the kind of rules that classical AI

963
01:04:15,660 --> 01:04:19,660
researchers were using, each rule is fairly simple. It's, you

964
01:04:19,660 --> 01:04:25,980
know, like, it's almost linear, or it's very simple logic. But

965
01:04:25,980 --> 01:04:29,900
it's the composition of all those rules that gives the power of

966
01:04:29,900 --> 01:04:32,620
expression of these systems. Of course, the problem then is that

967
01:04:32,620 --> 01:04:40,220
they didn't know how to train them properly. But yeah, I think

968
01:04:41,380 --> 01:04:54,340
we, I think we learn to come up with these discrete ways of

969
01:04:54,380 --> 01:05:01,020
breaking up things into simpler pieces. And that in fact, I

970
01:05:01,020 --> 01:05:03,900
think if you're Bayesian about it, it just comes out naturally.

971
01:05:04,020 --> 01:05:06,140
And they're very, very weak assumptions.

972
01:05:07,500 --> 01:05:11,940
So in a way, it's it's almost, it is piecewise abstraction. So

973
01:05:11,940 --> 01:05:15,740
we're kind of back. Yes, that's what I would lean to, rather

974
01:05:15,740 --> 01:05:20,020
than piecewise linear. But linear, of course, is a broad part

975
01:05:20,020 --> 01:05:22,580
of, you know, it's an easy way to get simple.

976
01:05:23,700 --> 01:05:26,740
Amazing. Professor Benjo, I'm interested in your personal

977
01:05:26,740 --> 01:05:29,540
journey. So we've been talking about diverse trajectories. And

978
01:05:29,620 --> 01:05:32,180
I wanted to know about your own trajectory of research over the

979
01:05:32,180 --> 01:05:35,540
last 10 years. Now, one of my mates, a psychologist and

980
01:05:35,540 --> 01:05:39,020
symbolist, Professor Gary Marcus, presumably one of your best

981
01:05:39,020 --> 01:05:42,340
friends, by the way, he pointed out in his 2012 New Yorker

982
01:05:42,340 --> 01:05:45,660
article that MLPs lacked ways of representing causal

983
01:05:45,660 --> 01:05:48,940
relationships such as between diseases and their symptoms. And

984
01:05:49,060 --> 01:05:50,900
I think this has been a significant focus of yours in

985
01:05:50,900 --> 01:05:54,540
recent years as we've discussed. And he thought at the time that

986
01:05:54,540 --> 01:05:58,420
you were a bit too quote system one all the way. And he spoke

987
01:05:58,420 --> 01:06:00,780
then about the need for heterogeneous architectures and

988
01:06:00,780 --> 01:06:03,700
the acquisition of abstract concepts, compositionality and

989
01:06:03,700 --> 01:06:07,060
extrapolation, which I think has also been a huge focus of yours

990
01:06:07,060 --> 01:06:09,780
in the last decade or so. We really enjoyed watching your

991
01:06:09,780 --> 01:06:13,340
debate with Marcus. And by the way, we would love to host V2 of

992
01:06:13,340 --> 01:06:15,100
that debate. So if you're interested, you just let us

993
01:06:15,100 --> 01:06:18,020
know we'll do that. But he's often viewed as a heretic. And,

994
01:06:18,140 --> 01:06:20,580
you know, just forgetting about symbols versus neural networks

995
01:06:20,580 --> 01:06:23,260
for a minute. Am I right in thinking that you've converged in

996
01:06:23,260 --> 01:06:25,500
at least some ways in your thinking? And how would you

997
01:06:25,500 --> 01:06:26,940
characterize that from your perspective?

998
01:06:27,820 --> 01:06:38,900
So, yeah, I used to be in the 90s, a, you know, pure neural net

999
01:06:41,380 --> 01:06:50,480
subsymbolic connectionists researcher. And I did my grad

1000
01:06:50,480 --> 01:06:55,740
studies at a time on neural nets at a time when the dominant way

1001
01:06:55,740 --> 01:06:59,580
of thinking was these, you know, classical AI rule based system

1002
01:06:59,580 --> 01:07:03,220
with no learning at all, and was dominant, meaning that the

1003
01:07:03,220 --> 01:07:06,580
little group like, you know, Jan and Jeff and I and others who

1004
01:07:06,580 --> 01:07:13,700
were thinking otherwise, had to, you know, defend our views. And

1005
01:07:14,740 --> 01:07:20,700
and maybe that led to a kind of, you know, us versus them, I

1006
01:07:20,700 --> 01:07:28,460
think, unhealthy way of thinking. And of course, I matured. And

1007
01:07:30,060 --> 01:07:33,260
one of the big, so there, I think there are several turning

1008
01:07:33,260 --> 01:07:38,300
points on that journey. Well, one of them in the in the 2000s

1009
01:07:38,300 --> 01:07:41,420
was the realization of the importance of abstraction. So

1010
01:07:42,500 --> 01:07:44,800
and the way to think about this maybe more concretely, because

1011
01:07:44,800 --> 01:07:46,900
what does it mean to be abstract? Is that I was thinking,

1012
01:07:47,220 --> 01:07:51,820
well, what would be the right kind of representation we want to

1013
01:07:51,820 --> 01:07:54,820
have at the top level of our unsupervised deep nets, because

1014
01:07:54,820 --> 01:07:56,980
we were doing mostly like unsupervised deep nest, like,

1015
01:07:56,980 --> 01:08:01,480
you know, deep boz machines and stuff in that decade. And I was

1016
01:08:01,480 --> 01:08:04,940
thinking, well, it would be things like words, right, things

1017
01:08:04,940 --> 01:08:08,380
like the sort of concepts that we manipulate at the top level,

1018
01:08:08,380 --> 01:08:12,380
well, it's words or, you know, the equivalent, maybe, with

1019
01:08:12,980 --> 01:08:19,260
disambiguated. But yeah, we, it didn't seem that we have the

1020
01:08:19,260 --> 01:08:23,020
right tools for that. And then it remained like an objective. And

1021
01:08:23,020 --> 01:08:32,160
then in 2014, we discovered the power of attention. And that's

1022
01:08:32,180 --> 01:08:35,780
closely connected to abstraction, because what it does is

1023
01:08:35,780 --> 01:08:39,140
it focuses on a few things. And of course, that's our, you

1024
01:08:39,140 --> 01:08:43,060
know, that's very much a characteristic of how we think a

1025
01:08:43,060 --> 01:08:46,620
thought has very few elements in it. That means we have selected

1026
01:08:46,620 --> 01:08:49,300
those elements. And that's where attention comes in. So it's

1027
01:08:49,300 --> 01:08:55,580
getting closer to this ideal of building machines that think like

1028
01:08:55,580 --> 01:09:00,060
humans. And then of course, in 2017, I wrote this consciousness

1029
01:09:00,060 --> 01:09:02,740
prior paper where, you know, I discovered all the work on global

1030
01:09:02,740 --> 01:09:06,100
workspace theory and, and it, you know, and the momentum is

1031
01:09:06,100 --> 01:09:12,620
built up. And of course, now, you know, humans think and they

1032
01:09:12,620 --> 01:09:17,460
use symbols, and they understand the very abstract

1033
01:09:17,460 --> 01:09:20,420
relationships between them. And we need to build neural nets that

1034
01:09:20,420 --> 01:09:26,700
can do that. So I guess where I've maybe departed from Gary,

1035
01:09:26,700 --> 01:09:31,180
but maybe he's moved to is, it's going to be neural nets that

1036
01:09:31,180 --> 01:09:33,620
do it, right? It's just that we're going to be training them in

1037
01:09:33,660 --> 01:09:36,420
a special way. And that's what G flow nets really aiming at.

1038
01:09:37,660 --> 01:09:40,820
So can I just say we, we asked many guests, these, these

1039
01:09:40,820 --> 01:09:43,860
questions about their, their evolution. And sometimes they,

1040
01:09:44,220 --> 01:09:47,620
they tend to be spicier than others. But I have to say, from my

1041
01:09:47,620 --> 01:09:52,060
perspective, your answer was the most informative, the most

1042
01:09:52,060 --> 01:09:56,860
gracious and the most noble of answers that we've heard so far

1043
01:09:56,860 --> 01:10:00,780
to similar questions. So kudos to you. That was awesome.

1044
01:10:00,860 --> 01:10:01,300
Thanks.

1045
01:10:02,500 --> 01:10:05,700
I just cannot believe it. And we always do a hell of a lot of

1046
01:10:05,700 --> 01:10:08,540
preparation. But it's gone to the point now where we know that

1047
01:10:08,540 --> 01:10:11,540
we're not going to get more than about six questions in. So we,

1048
01:10:11,780 --> 01:10:14,380
you know, we kind of like exponentially, you know, have an

1049
01:10:14,380 --> 01:10:16,060
exponential prior on our questions.

1050
01:10:16,060 --> 01:10:19,020
Well, he was awesome, though, with like, you know, we asked him

1051
01:10:19,140 --> 01:10:23,100
to give relatively sort of three minute answers. And he stuck to

1052
01:10:23,100 --> 01:10:26,020
that, which was really cool. I mean, that's, that's very

1053
01:10:26,020 --> 01:10:29,460
helpful to have an interesting dialogue. And I, I can't believe

1054
01:10:29,500 --> 01:10:32,820
how proud I am, you know, that he's, that he appreciates that we

1055
01:10:32,820 --> 01:10:36,340
put the prep time into it. And, you know, had had decent

1056
01:10:36,340 --> 01:10:39,700
questions that were hopefully interesting for him, as well as

1057
01:10:39,700 --> 01:10:44,460
our, as well as our audience. So Dr. Kilcher, lightspeed

1058
01:10:44,460 --> 01:10:46,100
Kilcher, what should I take?

1059
01:10:46,820 --> 01:10:51,700
It's cool is, I mean, his, um, yeah, I think is the thing he

1060
01:10:51,700 --> 01:10:54,700
mentioned at the end, like his humility, it kind of shines

1061
01:10:54,700 --> 01:10:58,740
through everything he does. And he answers, he's like, you know,

1062
01:10:58,780 --> 01:11:03,380
here's the best answer I can give. But, you know, he seems to

1063
01:11:03,380 --> 01:11:10,740
be very, like, open and not, not, not very, yeah, one notices

1064
01:11:10,740 --> 01:11:14,660
he's not on Twitter. It's like, it's noticed that was a

1065
01:11:14,660 --> 01:11:18,460
brilliant question. I think we should post that question on our

1066
01:11:18,460 --> 01:11:21,860
Twitter. Because, you know, that there's that a bit for people

1067
01:11:21,860 --> 01:11:23,700
watching this in a year's time, it's probably forgotten about

1068
01:11:23,700 --> 01:11:27,260
but yeah, that ilia guy from open AI said that the models might

1069
01:11:27,260 --> 01:11:31,180
be slightly conscious. I was exasperated by that. Because I

1070
01:11:31,180 --> 01:11:34,500
watched his interview on Lex. And I know by saying bad things

1071
01:11:34,500 --> 01:11:36,340
about him, he will never come on our podcast, but I don't think

1072
01:11:36,340 --> 01:11:39,020
he would have done anyway. So it doesn't matter. But yeah, I

1073
01:11:39,020 --> 01:11:41,700
think that it's pretty bad.

1074
01:11:42,380 --> 01:11:47,380
What? Why? Yeah, why? It's like, it's like, you don't think

1075
01:11:47,380 --> 01:11:52,060
it's bad? No, he says, I think, because a lot of the folks at

1076
01:11:52,060 --> 01:11:55,620
Open AI, they are, you know, like in the rationalist community,

1077
01:11:55,820 --> 01:12:00,420
and they seriously believe that we're an imminent threat of the AI

1078
01:12:00,660 --> 01:12:03,940
taking over the world and us being paper clips. And I think

1079
01:12:03,940 --> 01:12:07,420
it's next, I listened to his interview on Lex, and he sounded

1080
01:12:07,420 --> 01:12:10,420
like a salesman, talking about Codex and how it was going to

1081
01:12:10,420 --> 01:12:13,420
revolutionize everything. And I honestly think that there's just

1082
01:12:13,420 --> 01:12:17,900
such a divergence between what they're saying and reality right

1083
01:12:17,900 --> 01:12:18,140
now.

1084
01:12:19,300 --> 01:12:23,740
Well, not to drift too far away from from our guests today.

1085
01:12:24,100 --> 01:12:28,260
But so I thought, I thought it was just kind of a shower

1086
01:12:28,260 --> 01:12:32,380
thought, you know, like, you know, the the large neural

1087
01:12:32,380 --> 01:12:35,500
networks of today might be a little bit conscious, right?

1088
01:12:35,500 --> 01:12:39,060
And, and, and you just like, yeah, well, yeah, well, shower

1089
01:12:39,060 --> 01:12:41,460
thought, and it is a shower thought like it needs on

1090
01:12:41,460 --> 01:12:45,500
Twitter, it's just something you tweet out. And, and it brings up

1091
01:12:45,500 --> 01:12:48,100
interesting questions, like it brings up interesting questions,

1092
01:12:48,100 --> 01:12:51,100
like, you know, you're a you're a ball of neurons, like you're

1093
01:12:51,140 --> 01:12:53,740
just a slap together piece of matter, right? You have

1094
01:12:53,740 --> 01:12:58,340
consciousness. So clearly, like something about, you know,

1095
01:12:58,540 --> 01:13:01,900
learning systems combined with data, or maybe not even

1096
01:13:01,900 --> 01:13:06,100
combined with data gives rise to consciousness. So why can't

1097
01:13:06,500 --> 01:13:12,540
why can't another, you know, in silico, slap together system

1098
01:13:12,540 --> 01:13:17,660
of neurons ingested with data be slightly conscious or have

1099
01:13:17,700 --> 01:13:21,820
like, some properties, like, and that's that's essentially, yeah,

1100
01:13:22,620 --> 01:13:27,780
Benjo refused to give like a humble, the humble person he is,

1101
01:13:27,780 --> 01:13:31,820
he refused to give like, you know, the the the strong take on

1102
01:13:31,820 --> 01:13:36,620
that, but that would have because he might just this is my

1103
01:13:36,620 --> 01:13:40,700
opinion, not his obviously, but reading the consciousness prior

1104
01:13:40,700 --> 01:13:45,980
paper, it is not too far off. He formulates consciousness as

1105
01:13:45,980 --> 01:13:50,140
having these elements of, you know, I have my internal state,

1106
01:13:50,140 --> 01:13:54,860
which is sort of everything in my brain that I could bring bring

1107
01:13:54,860 --> 01:13:58,860
up into my forefront, then I get some input from the outside

1108
01:13:58,860 --> 01:14:03,660
world. And through the input, I then filter, like with an

1109
01:14:03,660 --> 01:14:09,420
attention mechanism, I do I look what in my mind, could I now

1110
01:14:09,420 --> 01:14:14,060
bring into focus, right? And that is by use of something like

1111
01:14:14,060 --> 01:14:19,620
an attention mechanism. And then I take that thing. And I put it

1112
01:14:19,620 --> 01:14:26,540
into these abstract concepts I use I represent. I represent the

1113
01:14:26,540 --> 01:14:30,980
concepts in my head as a sparse factor graph. And by focusing on

1114
01:14:30,980 --> 01:14:34,380
parts of that, I can then make inferences in this sparse

1115
01:14:34,380 --> 01:14:38,260
factor graph and so on. Now, obviously, something like GPT three

1116
01:14:38,260 --> 01:14:42,820
doesn't have all of that, at least not explicitly, but some of

1117
01:14:42,820 --> 01:14:46,780
it is there, right? It's, you know, I have a piece of input, I

1118
01:14:46,780 --> 01:14:50,660
have giant amount of weights, I use an attention mechanism to

1119
01:14:50,660 --> 01:14:52,700
sort of see what I can focus on.

1120
01:14:53,660 --> 01:14:56,140
Yeah, but yeah, but I think that I think that's a very

1121
01:14:56,140 --> 01:15:00,180
declarative description of consciousness. And at its roots,

1122
01:15:00,180 --> 01:15:03,660
it's about the phenomenological experience. Right. And I know

1123
01:15:03,660 --> 01:15:07,420
we discussed computationalism and panpsychism. Let's not go down

1124
01:15:07,420 --> 01:15:11,660
that rabbit hole. But surely, they don't think that this model can

1125
01:15:11,700 --> 01:15:13,220
feel well, but so this is

1126
01:15:13,220 --> 01:15:17,060
consciousness is not about feeling. It's about being being

1127
01:15:17,060 --> 01:15:22,820
like aware of of like, I don't even know what it is. I'm just

1128
01:15:22,820 --> 01:15:27,780
saying that it sounded not too far away from what the

1129
01:15:27,780 --> 01:15:31,700
consciousness prior paper was about. And yes, I realize it's

1130
01:15:31,700 --> 01:15:34,940
called the consciousness prior and not consciousness. But you

1131
01:15:34,940 --> 01:15:35,180
know,

1132
01:15:35,980 --> 01:15:38,300
yeah, I mean, I think he answered it the way a scientist

1133
01:15:38,380 --> 01:15:41,180
should answer it. And I was really happy with his answer,

1134
01:15:41,180 --> 01:15:47,500
which is, okay, a consciousness has to be some activity of

1135
01:15:47,500 --> 01:15:50,300
neurons and firings or whatever in the brain or else we're

1136
01:15:50,300 --> 01:15:54,300
talking about magic. And that's not in the field of science. And

1137
01:15:54,300 --> 01:15:57,200
B, you know, whatever that thing is, it's obviously quite

1138
01:15:57,200 --> 01:16:00,940
nuanced and complicated. And we don't have we don't know yet.

1139
01:16:00,980 --> 01:16:05,100
So we need to have some humility here, which means we

1140
01:16:05,100 --> 01:16:08,140
shouldn't be alarmist. So we don't need to be going in, you

1141
01:16:08,980 --> 01:16:13,020
know, burning books tomorrow because because we created a, you

1142
01:16:13,020 --> 01:16:16,500
know, GPT, whatever, that anytime its wheel is spinning,

1143
01:16:16,500 --> 01:16:18,860
and it's actually suffering. You know, if you ask it a

1144
01:16:18,860 --> 01:16:21,060
question that's too hard, and it's spinning, it's because

1145
01:16:21,060 --> 01:16:23,660
you're hurting it and it's suffering. And so we need to

1146
01:16:23,660 --> 01:16:26,340
turn it off like right away. But wait, we can't turn it off

1147
01:16:26,340 --> 01:16:29,300
because then we'd be like, murdering, you know, a sentient

1148
01:16:29,300 --> 01:16:33,460
being or something, like we're way, way too, in our infantile

1149
01:16:33,500 --> 01:16:37,960
understanding of, you know, this type of complex, complex

1150
01:16:37,960 --> 01:16:41,180
behavior, that's the human mind and consciousness to be at that

1151
01:16:41,180 --> 01:16:45,660
point. So from my perspective, he answered it completely 100%

1152
01:16:45,660 --> 01:16:48,860
scientifically. And there's a lot of folks out there who are

1153
01:16:48,860 --> 01:16:52,140
supposed to be scientists that spend a lot of time with, you

1154
01:16:52,140 --> 01:16:57,220
know, unscientific, you know, thinking about it. Cool. Let's

1155
01:16:57,220 --> 01:17:00,420
talk a little bit. What one of the things that I really found

1156
01:17:00,420 --> 01:17:03,940
interesting about Benjo's ideas, other than the causality

1157
01:17:03,940 --> 01:17:07,780
stuff and the system to stuff is this notion of diversity.

1158
01:17:08,380 --> 01:17:10,900
We've had conversations with Kenneth Stanley all about open

1159
01:17:10,900 --> 01:17:13,580
endedness and diversity preservation. We've also had

1160
01:17:13,580 --> 01:17:18,340
conversations with Friston about the importance of balancing

1161
01:17:18,340 --> 01:17:22,060
relative entropy and so on. And we have all of these curses in

1162
01:17:22,060 --> 01:17:24,700
empirical learning, right? The statistical curses, the

1163
01:17:24,700 --> 01:17:25,900
approximation curses.

1164
01:17:26,740 --> 01:17:30,340
Dimensionality, we have to mention dimensionality and

1165
01:17:30,340 --> 01:17:32,980
even, I mean, you know, we're talking about curses in the

1166
01:17:32,980 --> 01:17:35,740
Monty, you know, the Markov chain Monte Carlo in the sense of

1167
01:17:35,740 --> 01:17:38,660
it being a high dimensional space. And we need to assume that

1168
01:17:38,660 --> 01:17:42,700
there's some structure around where these modes are. So all of

1169
01:17:42,700 --> 01:17:45,900
these approaches are ways of simultaneously, and, you know,

1170
01:17:45,900 --> 01:17:48,780
being able to explore but not being cursed. So yeah, what was

1171
01:17:48,780 --> 01:17:49,300
your take on that?

1172
01:17:50,260 --> 01:17:57,020
Well, any one of my take was that I like that he's so

1173
01:17:57,060 --> 01:18:01,180
interested in abstraction, because, you know, to me, that's

1174
01:18:01,180 --> 01:18:04,140
been not only one, you know, it's not only one of the larger

1175
01:18:04,180 --> 01:18:07,580
mysteries, at least for me, I mean, I don't know, of the kind of

1176
01:18:07,580 --> 01:18:12,500
the universe is abstraction, idealism, you know, platonic

1177
01:18:12,500 --> 01:18:15,100
thinking, whatever. I mean, the whole point is just that he

1178
01:18:15,100 --> 01:18:20,060
views abstraction as a key to pragmatically useful, you know,

1179
01:18:20,060 --> 01:18:24,860
pass forward. And it's a hard problem, a really hard problem.

1180
01:18:24,940 --> 01:18:29,180
And, you know, his focus right now is on kind of graph based

1181
01:18:29,620 --> 01:18:32,540
structures. And I have to admit, you know, for me to you,

1182
01:18:32,540 --> 01:18:35,060
they're quite seductive and appealing. I don't know if

1183
01:18:35,060 --> 01:18:37,980
they're the the right path forward, but it's definitely

1184
01:18:37,980 --> 01:18:41,740
cool to see a lot of research, looking into graph based, you

1185
01:18:41,740 --> 01:18:44,940
know, methods, or, you know, hyper graph based methods,

1186
01:18:44,940 --> 01:18:48,620
whatever they are, they seem to definitely be a promising

1187
01:18:48,620 --> 01:18:52,180
path forward. And I think we're in for, hopefully, if we can

1188
01:18:52,180 --> 01:18:55,380
continue to progress at a reasonable rate, you know, some

1189
01:18:55,380 --> 01:18:57,260
some interesting decades ahead.

1190
01:18:58,740 --> 01:19:05,340
I mean, I would, I would also postulate that maybe our most of

1191
01:19:05,340 --> 01:19:08,820
our, let's say benchmarks that we're thinking about today aren't

1192
01:19:08,860 --> 01:19:14,740
necessarily suited to to because his argument was by creating

1193
01:19:14,740 --> 01:19:19,700
abstractions, it might actually, you know, help your ability to

1194
01:19:19,740 --> 01:19:22,740
learn something, right, which is a thing that we all

1195
01:19:22,740 --> 01:19:25,340
intuitively understand in the world, if I have good

1196
01:19:25,340 --> 01:19:28,100
abstractions, I can transfer my knowledge from here to here and

1197
01:19:28,100 --> 01:19:31,020
from here to here. Yeah, in something like image net

1198
01:19:31,020 --> 01:19:34,300
classification, or whatnot, or most of the benchmarks we have

1199
01:19:34,300 --> 01:19:38,980
today, the necessity of abstractions is probably not like

1200
01:19:38,980 --> 01:19:42,420
the data, the hardness of the problem probably doesn't

1201
01:19:42,420 --> 01:19:46,980
require abstractions to be introduced. And therefore, the

1202
01:19:46,980 --> 01:19:50,460
limiting factor here might not only be the models themselves,

1203
01:19:50,460 --> 01:19:57,140
but also, let's say, our ability to even measure the progress

1204
01:19:57,140 --> 01:20:00,460
one could make with abstractions. And I think that's gonna change

1205
01:20:00,460 --> 01:20:03,620
maybe in the near future, because people are going into

1206
01:20:03,620 --> 01:20:08,300
multimodality research and so on. And there, I think the concept

1207
01:20:08,300 --> 01:20:12,860
of sort of concepts, maybe not abstractions, but at least

1208
01:20:12,860 --> 01:20:15,980
something like concepts is way more, more important.

1209
01:20:17,020 --> 01:20:19,540
Yeah, there's, let me just follow real quickly there, Tim,

1210
01:20:19,540 --> 01:20:22,380
because there's something very interesting there to Yannick,

1211
01:20:22,380 --> 01:20:27,100
which is the lack of good tools to deal with multimodal, you

1212
01:20:27,100 --> 01:20:30,260
know, sets of data results. And a lot of times, we're just

1213
01:20:30,260 --> 01:20:33,820
throwing out kind of valuable, valuable sources of data, just

1214
01:20:33,820 --> 01:20:36,820
because, you know, we don't have a good tool sets to do with

1215
01:20:36,820 --> 01:20:39,220
them, like think about the self driving car, like the whole,

1216
01:20:39,620 --> 01:20:43,140
should it be vision versus LiDAR debate? Why? Why isn't it

1217
01:20:43,140 --> 01:20:46,820
both? I mean, you know, if you can for $5, you can throw on

1218
01:20:46,820 --> 01:20:50,620
some cheap, you know, LiDAR sensors or something, maybe not

1219
01:20:50,620 --> 01:20:52,940
something fancy, but something cheap, why wouldn't we take

1220
01:20:52,940 --> 01:20:56,860
advantage of that data? And it's, it's really because we don't

1221
01:20:56,860 --> 01:21:00,700
have good tools to deal with, with multimodal data.

1222
01:21:01,340 --> 01:21:04,620
We got to a good point in the discussion where we were talking

1223
01:21:04,620 --> 01:21:08,500
about the nature of finding abstractions. And I wonder where

1224
01:21:08,500 --> 01:21:10,980
the neural networks can find abstractions. Now, the, the

1225
01:21:11,020 --> 01:21:14,940
cynical view is that humans kind of create these inductive

1226
01:21:14,940 --> 01:21:17,300
priors, and they represent the abstraction. So certainly in the

1227
01:21:17,300 --> 01:21:19,900
case of geometric deep learning, and that's kind of what's

1228
01:21:19,900 --> 01:21:23,220
happening, we put the, the priors in there to reduce the size of

1229
01:21:23,220 --> 01:21:25,780
the approximation space. And Keith and I had an interesting

1230
01:21:25,780 --> 01:21:28,140
idea yesterday that there's a kind of analogy between geometric

1231
01:21:28,140 --> 01:21:30,620
deep learning and causal representation learning. So I

1232
01:21:30,620 --> 01:21:32,900
think Keith, you went online and you found a really interesting

1233
01:21:32,900 --> 01:21:35,820
definition of a causal model, which is that it's kind of

1234
01:21:35,820 --> 01:21:40,100
immune to, let's say, adversarial examples. So what a model

1235
01:21:40,140 --> 01:21:43,100
does right now, is it learns a relationship essentially between

1236
01:21:43,100 --> 01:21:46,220
let's say every single pixel and something happening, right,

1237
01:21:46,260 --> 01:21:49,060
which is why that model is vulnerable.

1238
01:21:49,580 --> 01:21:52,620
Yeah, so that was the, and yeah, and I would love to get your

1239
01:21:52,620 --> 01:21:56,020
comment on that. But that was, you know, this paper, and I

1240
01:21:56,020 --> 01:21:58,420
could go dig it up, and I can get the reference right now where

1241
01:21:58,420 --> 01:22:02,260
it said, you know, hey, what is the difference between a causal

1242
01:22:02,580 --> 01:22:06,460
or prediction from a causal model versus a prediction from a

1243
01:22:06,460 --> 01:22:10,940
non causal model. And the point was that, well, almost by

1244
01:22:10,940 --> 01:22:15,740
definition, really, if you have a causal model, then if you

1245
01:22:15,740 --> 01:22:18,460
perturbed the inputs, the prediction that you get out of

1246
01:22:18,460 --> 01:22:22,340
it remains a valid, a valid output, because after all, if

1247
01:22:22,340 --> 01:22:25,060
it's a causal model, and it's reflective of a sort of the

1248
01:22:25,060 --> 01:22:28,140
causal structure of the world or whatnot, then sure, that's a

1249
01:22:28,140 --> 01:22:32,060
valid, valid output. Whereas, if it's non causal, it has the

1250
01:22:32,060 --> 01:22:35,580
potential to learn all these kind of spurious, spurious

1251
01:22:35,580 --> 01:22:37,580
structures, and therefore, that's why you get the

1252
01:22:37,580 --> 01:22:40,940
capability of these adversarial examples where you just, you

1253
01:22:40,940 --> 01:22:44,540
know, put a little rainbow pixel somewhere, and it messes up the

1254
01:22:44,540 --> 01:22:47,140
class because it had this spurious connection.

1255
01:22:47,580 --> 01:22:51,020
I mean, in the same vein, you could also, the adversarial

1256
01:22:51,020 --> 01:22:54,380
examples are there because of inaccuracies, because we don't

1257
01:22:54,380 --> 01:22:57,740
have the perfect discriminative function, right? I could also

1258
01:22:57,740 --> 01:23:01,500
say, well, if I just had the correct discriminative functions,

1259
01:23:01,500 --> 01:23:04,860
it doesn't need to be causal. If I just had like the right

1260
01:23:04,860 --> 01:23:09,780
partitioning of my input space, then, you know, I'm super not

1261
01:23:09,780 --> 01:23:13,780
vulnerable to adversarial attacks. I guess the real question

1262
01:23:13,780 --> 01:23:19,260
would be, would that technically amount to a causal model if I

1263
01:23:19,260 --> 01:23:22,940
had, you know, the perfect partitioning of the input

1264
01:23:22,940 --> 01:23:27,140
space into my classes? I don't know, that's like, is there like

1265
01:23:27,140 --> 01:23:30,340
a mathematical equivalent from that to a causal model? Who

1266
01:23:30,340 --> 01:23:30,820
knows?

1267
01:23:31,180 --> 01:23:35,300
Right. Yeah, I think there's probably, it's probably, certainly,

1268
01:23:35,740 --> 01:23:38,740
if you have the perfect discriminative function, it's

1269
01:23:38,740 --> 01:23:41,940
probably the discriminative function that you would derive

1270
01:23:41,940 --> 01:23:46,140
from a causal model. I'm not 100% sure you can go go in the

1271
01:23:46,140 --> 01:23:48,780
reverse, because I imagine there probably is some some

1272
01:23:48,780 --> 01:23:53,620
information loss going from, you know, a causal model to, you

1273
01:23:53,620 --> 01:23:57,780
know, like, for example, I'll give you an example. In the cases

1274
01:23:57,820 --> 01:24:00,660
of, say, production systems, you know, so, so these little

1275
01:24:01,140 --> 01:24:05,740
rewriting rules or whatever, the definition there of a causal

1276
01:24:06,100 --> 01:24:11,820
system is one in which all the potential graphs, all the

1277
01:24:11,820 --> 01:24:14,780
potential transition graphs that you can get to a particular

1278
01:24:14,780 --> 01:24:18,980
output are isomorphic. So even though you have you can have

1279
01:24:19,340 --> 01:24:22,980
like the perfect discriminative kind of function, there may be

1280
01:24:22,980 --> 01:24:26,140
multiple possible graphs that you could have gotten there, but

1281
01:24:26,140 --> 01:24:29,460
they're isomorphic. So I'm not quite sure, you know, how that

1282
01:24:29,460 --> 01:24:33,740
would translate into this, this, this point. But I think you'd

1283
01:24:33,740 --> 01:24:37,100
be just as good for the purpose of discriminating.

1284
01:24:38,740 --> 01:24:41,180
I think it's related to the semantics discussion we're

1285
01:24:41,180 --> 01:24:45,140
having in NLP. So people like Walid Saber say that neural

1286
01:24:45,140 --> 01:24:48,180
networks don't have semantics. And in the same way, as I was

1287
01:24:48,180 --> 01:24:51,340
just saying, blue pixels, I mean, in the real world, let's say

1288
01:24:51,340 --> 01:24:55,820
male testosterone levels is causally linked to incidents of

1289
01:24:55,860 --> 01:24:59,500
car crashes, which means you can now take the model in in

1290
01:24:59,500 --> 01:25:02,100
Holland in a different country. And because it's a causal

1291
01:25:02,100 --> 01:25:05,380
factor, it will extrapolate in the same way. But neural

1292
01:25:05,380 --> 01:25:09,140
networks models, because what a human does is we would come up

1293
01:25:09,140 --> 01:25:12,420
with the right representational abstraction, we would build a

1294
01:25:12,420 --> 01:25:15,660
model, which is very reductionist, a neural network models

1295
01:25:15,660 --> 01:25:18,980
everything to everything. And the semantics are all one thing.

1296
01:25:19,380 --> 01:25:24,220
Well, okay, I don't, I'm not, I'm not too, too keen on

1297
01:25:24,580 --> 01:25:28,740
discussing like semantics and whatnot with with NLP people.

1298
01:25:28,900 --> 01:25:38,860
But I don't know, you know, like, like, I don't know, it

1299
01:25:38,860 --> 01:25:44,060
often it often veers away and veers into semantics. It's like

1300
01:25:44,060 --> 01:25:48,860
it's a bit too, you know, I like what what I think Conor Conor

1301
01:25:48,860 --> 01:25:53,060
Lay, he said, like, when we talked him along. Oh, he wouldn't

1302
01:25:53,300 --> 01:25:56,260
like that. I talked him a long time ago. And I happen to agree

1303
01:25:56,260 --> 01:25:59,220
with him there is that you sort of have to see everything from

1304
01:25:59,220 --> 01:26:03,700
the perspective of these models. Like if I'm a GPT three, my

1305
01:26:03,700 --> 01:26:07,580
entire world is text input, right? And people can't somehow

1306
01:26:07,580 --> 01:26:12,260
judge GPT three by, well, you don't even have whatever a

1307
01:26:12,260 --> 01:26:14,860
connection to the real world, you don't even know that you don't

1308
01:26:14,860 --> 01:26:19,340
go see a doctor if your plant is sick, right? Like, how can you

1309
01:26:19,340 --> 01:26:21,940
not know that? Like, okay, they don't live in the real world,

1310
01:26:21,940 --> 01:26:25,820
they live in the text world of the internet. And in that world,

1311
01:26:26,060 --> 01:26:31,140
I'm not sure if there is not a level of abstraction happening

1312
01:26:31,260 --> 01:26:39,300
in these models. Like, it's, it's, it's, um, yeah, I'm, I don't

1313
01:26:39,300 --> 01:26:43,860
want to, I don't want to claim that these things do not form

1314
01:26:44,140 --> 01:26:48,180
abstract things, it might not be the same abstract classes that

1315
01:26:48,180 --> 01:26:52,780
we form, but they definitely form some level of abstraction.

1316
01:26:53,100 --> 01:26:56,100
And of course, they can't transfer it because we only give

1317
01:26:56,100 --> 01:26:59,660
them the one modality, right? But they may be able to transfer

1318
01:26:59,660 --> 01:27:05,020
it between, you know, different areas of text, which they

1319
01:27:05,140 --> 01:27:10,500
sometimes do, right? And yeah, so that's, I just wouldn't be so

1320
01:27:10,540 --> 01:27:13,260
conclusive with respect to these things.

1321
01:27:13,300 --> 01:27:17,020
That that's true. I think we've gone full circle now. So after

1322
01:27:17,060 --> 01:27:21,740
speaking with Randall Ballastriro about the splines, that almost

1323
01:27:22,180 --> 01:27:25,100
results in such a cynical reading of MLPs that they're just

1324
01:27:25,100 --> 01:27:27,980
hash tables, but we're not using MLPs, we're using

1325
01:27:27,980 --> 01:27:31,140
transformers and we're using CNNs. And actually, if you think

1326
01:27:31,140 --> 01:27:34,740
of abstraction, just as being extrapolation, I think they are

1327
01:27:34,740 --> 01:27:37,980
basically synonymous, it's about being able to extrapolate

1328
01:27:37,980 --> 01:27:41,820
outside of your training set. Then those inductive priors are

1329
01:27:41,820 --> 01:27:46,460
indeed producing abstractions. But the problem is humans

1330
01:27:46,460 --> 01:27:49,780
design those inductive priors. What we want is to learn

1331
01:27:49,780 --> 01:27:52,020
abstractions. And that's the thing that I don't think is

1332
01:27:52,020 --> 01:27:57,820
happening. I'm kind of I'm kind of on the same page as Yannick

1333
01:27:57,820 --> 01:28:02,700
and Connor on the one hand, which is, hey, if an abstraction is

1334
01:28:02,700 --> 01:28:08,220
just a compression, you know, encoding of the input space, then

1335
01:28:08,220 --> 01:28:10,500
of course, they're learning abstractions, right? I mean,

1336
01:28:10,500 --> 01:28:13,740
they are throwing away, you know, information and retaining

1337
01:28:13,740 --> 01:28:17,220
some, some abstract thing. I think, but I think that just kind

1338
01:28:17,220 --> 01:28:23,580
of devolves into somewhat like, you know, bastardization, if you

1339
01:28:23,580 --> 01:28:27,140
will, of what people mean when they say abstractions, because

1340
01:28:27,500 --> 01:28:31,140
the types of abstractions that traditionally we think about as

1341
01:28:31,140 --> 01:28:34,740
abstractions are simplifications. You know, they're, they're

1342
01:28:34,740 --> 01:28:39,780
like, simplifications of more general longer range kind of

1343
01:28:39,780 --> 01:28:42,980
structures. Whereas we know, and I think we all know this for

1344
01:28:42,980 --> 01:28:47,740
sure, that a lot of the quote unquote abstractions that did a

1345
01:28:47,780 --> 01:28:52,340
neural network learns are these kind of like shortcuts, right?

1346
01:28:52,340 --> 01:28:56,540
They're like these low level borderline spurious kinds of

1347
01:28:56,580 --> 01:28:59,420
abstractions. And that's why they break so easy. That's why

1348
01:28:59,420 --> 01:29:02,340
they're so brittle. And I mean, there is this vagueness here,

1349
01:29:02,340 --> 01:29:05,020
right? Like when is an abstraction, a good abstraction,

1350
01:29:05,020 --> 01:29:07,660
I don't know. But I think it all kind of in a way misses the

1351
01:29:07,660 --> 01:29:12,660
point. Like, what we're talking about here is that, and this is a

1352
01:29:12,660 --> 01:29:16,580
lot of what Benjio said, right, which is that the goal here is

1353
01:29:16,580 --> 01:29:21,820
to figure out how to get machine learning to learn

1354
01:29:22,380 --> 01:29:28,460
structures that by virtue of their simplification, their simple

1355
01:29:28,460 --> 01:29:33,220
abstractions are more generalizable out of distribution.

1356
01:29:33,980 --> 01:29:37,540
Right, like that's, that's really kind of the goal here. And I

1357
01:29:37,540 --> 01:29:41,420
mean, so the rest of it is just semantics, pun intended. I mean,

1358
01:29:41,420 --> 01:29:41,740
the

1359
01:29:44,020 --> 01:29:49,900
if you look across the world, a lot of, let's say, cultures and

1360
01:29:49,900 --> 01:29:55,980
humans and so on must have the same abstractions, right? So it

1361
01:29:56,100 --> 01:30:00,060
must mean a little bit that it's not just something you learn

1362
01:30:00,060 --> 01:30:04,700
during your lifetime, right? So, right? Oh, absolutely.

1363
01:30:04,700 --> 01:30:08,180
Not correct. It's it's learned by the it's learned by evolution,

1364
01:30:08,220 --> 01:30:10,540
by the species, by, by life itself.

1365
01:30:10,580 --> 01:30:16,140
Exactly, right. But but is like the, the analogy to us building

1366
01:30:16,140 --> 01:30:20,380
in the correct ones as a shortcut for just evolution doing it

1367
01:30:20,380 --> 01:30:25,460
using essentially random search, right? That is, it might, right,

1368
01:30:25,460 --> 01:30:28,980
it's, it's a different, it's a different quality of we want

1369
01:30:29,020 --> 01:30:32,940
machines to learn something. Because usually we think of when

1370
01:30:32,940 --> 01:30:35,500
we say we want machines to learn something is we want him to

1371
01:30:35,500 --> 01:30:41,060
ingest data akin to maybe what a human does during its lifetime.

1372
01:30:41,540 --> 01:30:45,540
But the when you know, these sort of abstractions and the

1373
01:30:45,540 --> 01:30:48,780
ability to form abstractions, they seem to be happening on a

1374
01:30:48,780 --> 01:30:51,020
more fundamental shared level.

1375
01:30:51,300 --> 01:30:55,140
Yes, you just put the pin in the center of the bullseye. I think

1376
01:30:55,140 --> 01:30:58,060
that's exactly right. You know, there's a lot to be said for

1377
01:30:58,100 --> 01:31:01,900
me. It's an epiphenomenon. And a lot of intelligence is

1378
01:31:01,900 --> 01:31:06,060
embodied. And I agree that there's an awful lot of stuff going

1379
01:31:06,060 --> 01:31:10,540
on and unbeknown to us with this clearly something that most

1380
01:31:10,540 --> 01:31:14,420
people don't have a grasp on. Maybe this is why at the

1381
01:31:14,420 --> 01:31:17,820
population level, maybe this is why I'm frequently miscommunicating

1382
01:31:17,820 --> 01:31:21,060
with people because I never assumed that learning was about,

1383
01:31:21,540 --> 01:31:24,300
you know, what a human being learns and a human being's

1384
01:31:24,340 --> 01:31:28,140
lifetime. Like it's, to me, it's always been the evolution,

1385
01:31:28,740 --> 01:31:31,740
you know, paradigm, it's like what's encoded in your neurons,

1386
01:31:31,740 --> 01:31:35,500
what's encoded in your DNA, you know, what was learned by

1387
01:31:35,500 --> 01:31:38,940
bacteria a long time ago, and how did that translate into what

1388
01:31:38,940 --> 01:31:42,460
human beings are doing. So I don't know why, like, why so

1389
01:31:42,460 --> 01:31:46,140
many people are focused on what a human being learns in their

1390
01:31:46,140 --> 01:31:49,380
lifetime. I mean, it's more, you know, why is that the goal?

1391
01:31:49,420 --> 01:31:50,100
I'm not sure.

1392
01:31:50,300 --> 01:31:53,340
I know, but we run the risk of being very reductionist because

1393
01:31:53,380 --> 01:31:57,060
Connolly, he said that it's an open question where the humans

1394
01:31:57,060 --> 01:32:01,100
are even intelligent. And if you go down that line, very

1395
01:32:01,100 --> 01:32:04,060
quickly, you start saying, oh, human beings are just hash

1396
01:32:04,060 --> 01:32:07,660
tables like GBT three, clearly humans are intelligent in some

1397
01:32:07,660 --> 01:32:07,940
way.

1398
01:32:08,020 --> 01:32:10,420
Well, you can just take it as a, you know, matter of

1399
01:32:10,420 --> 01:32:13,780
definition, but it's not a binary thing. Like again, why are

1400
01:32:13,780 --> 01:32:16,620
we always into this black and white concept, something is or

1401
01:32:16,620 --> 01:32:19,980
is not intelligent? Like that's not how I view things. I think

1402
01:32:19,980 --> 01:32:24,780
there's a spectrum of intelligence from like zero to, I

1403
01:32:24,780 --> 01:32:27,740
don't know, maybe infinity or something, some really large

1404
01:32:27,740 --> 01:32:31,540
number beyond what what human beings are. And so it's this

1405
01:32:31,540 --> 01:32:35,300
continuum. So that's why I like chelets kind of on the measure

1406
01:32:35,300 --> 01:32:39,420
of intelligence, because even though it doesn't actually give

1407
01:32:39,420 --> 01:32:43,860
us a, you know, quantitative way yet to measure intelligence, it

1408
01:32:43,860 --> 01:32:46,580
at least is thinking along the right directions, which is how

1409
01:32:46,620 --> 01:32:51,180
do you measure intelligence? And how do you define it as a

1410
01:32:51,180 --> 01:32:56,460
category of activity? And then we can kind of get past this

1411
01:32:56,460 --> 01:32:58,220
black and white, you know, thinking.

1412
01:33:00,540 --> 01:33:01,500
Well, gentlemen,

1413
01:33:02,060 --> 01:33:05,260
always a pleasure. Absolutely. Yeah, absolutely.

