start	end	text
0	2480	Coming up later in Machine Learning Street Talk,
2480	7560	Professor Jan Le Koon, the godfather of deep learning.
7560	12840	There's been a lot of people who have been sort of saying
12840	15480	there's a limitation to deep learning, let's say,
15480	17880	or machine learning more generally.
17880	20240	Because it's obvious that those things basically
20240	22640	do curve fitting, and that only works for interpolation
22640	24400	and not for extrapolation.
24480	30680	And that kind of dismissal always sounded wrong to me.
30680	35120	Would this qualitative difference be in the form of fundamentally
35120	37960	different things from deep learning, things that
37960	40280	are like discrete symbolic reasoning
40280	42080	or things of that type?
42080	44280	And to that, my answer is clearly no.
44280	46360	I do not believe that's the case.
46360	48280	That's a limitation of supervised learning.
48280	52000	It has absolutely nothing to do with deep learning.
52000	53160	Computing on things.
53160	55480	OK, and I put a stop right there.
55480	57760	We also had a fascinating conversation
57760	62280	with Randall Bellisterio from Meta AI Research.
62280	64160	So I think it's two different things
64160	67600	to be able to interpolate or move on your manifold
67600	71240	and being in the interpolation regime from your training set.
71240	74040	So is this similar to interpolation?
74040	75440	Well, I mean, all of machine learning
75440	78240	is similar to interpolation if you want, right?
78240	81560	When you train a linear regression on scalar values,
81560	82960	you're training a model, right?
82960	85600	You're giving a bunch of pairs X and Y.
85600	89560	You're asking what are the best values of A and B for Y equals
89560	93640	AX plus B that minimizes the squared error of the prediction
93640	95520	of a line to all of the points, right?
95520	96880	That's linear regression.
96880	98320	That's interpolation.
98320	99960	All of machine learning is interpolation.
99960	101720	In a high-dimensional space, there
101720	104040	is essentially no such thing as interpolation.
104040	105840	Everything is extrapolation.
105840	108960	So imagine you are in a space of images, right?
108960	111680	So you have a car images 256 by 256.
111720	114440	So it's 200,000 dimensional input space.
114440	116000	Even if you have a million samples,
116000	120240	you're only covering a tiny portion of the dimensions
120240	121120	of that space, right?
121120	125960	Those images are in a tiny sliver of surface
125960	129200	among the space of all possible combinations
129200	130680	of values of pixels.
130680	133560	So when you show the system a new image,
133560	136240	it's very unlikely that this image is a linear combination
136240	137200	of previous images.
137200	140440	What you're doing is extra-appalation, not interpolation.
140480	142480	And in high-dimension, all of machine learning
142480	144640	is extrapolation, which is why it's high.
144640	146800	Now, that was a clip that went out on the Chalet show.
146800	149480	And I think I understand now what Lacune meant.
149480	151640	He's saying that our intuition of interpolation
151640	155200	is only correct in a very low number of dimensions.
155200	156880	It's a mathematical impossibility
156880	159640	to have statistical generalization in high dimensions.
159640	162080	So he's not saying that neural networks are really
162080	164880	extrapolating in the sense of continuing the pattern.
164880	167240	He's saying our definition of extrapolation,
167240	169760	the convex whole membership, is broken.
169800	172080	Machine learning does not and will never
172080	173440	work in high dimensions.
173440	175520	That's why we've invented so many tricks
175520	178880	to reduce the statistical and approximation complexity
178880	181120	of problems, just like we do in computer science
181120	183560	of the computational complexity of algorithms.
183560	188080	Lacune is not saying that deep learning models are clairvoyant.
188080	191240	Jan Lacune thinks that it's specious to say
191240	193840	that neural network models are interpolating
193840	198840	because in high dimensions, everything is extrapolation.
200080	201600	Oh boy.
201600	204680	We started thinking about making this show many months ago
204680	207200	when Randall Belastriro and Jerome Pesente
207200	210120	and Jan Lacune first released their paper,
210120	214280	Learning in High Dimensions Always Amounts to Extrapolation.
214280	216960	And it feels like a lot has happened since then.
216960	219200	I mean, they do say the mark of an educated mind
219200	221960	is being able to change your opinion
221960	224120	in light of new evidence.
224120	226560	Where we are now compared to where we were then,
226560	228400	I mean, let's just say, I feel like I'm standing
228400	230360	on the surface of Pluto.
231800	234960	I was initially quite skeptical of this paper
234960	236640	and I was trying to pick it apart.
236640	240000	And now it feels like we've done 180 degrees,
240000	242760	not only on the way we think about the paper,
242760	245200	but how we think about neural networks in general.
245200	247440	And we really wanna try and impart some of that knowledge
247440	248960	with you today in the introduction,
248960	250680	which might be quite ambitious.
250680	251680	So please bear with us.
251680	254320	But generally speaking, the structure of the show today,
254320	255520	it's gonna be a big show.
255520	256720	It's gonna be a big show.
256720	260440	Use the table of contents, skip around a bit.
260440	262400	But there's an intro, there's a, you know,
262400	264280	where we talk about this blind view
264280	266640	or this boundary view of neural networks,
266640	268120	which I think is very interesting.
268120	270160	There's a conversation with Yann LeCun,
270160	271720	the godfather of deep learning.
271720	275120	And we also speak with Randall Belastriro,
275120	278000	who released this beautiful paper
278000	280760	about thinking about neural networks in this new way.
280760	282000	And we have a debrief as well.
282000	283520	So I've not edited the show yet,
283520	285640	but it's gonna be about three hours.
285640	287520	Anyway, the show must go on.
287520	291880	So as I was saying, particularly speaking with Randall,
291880	293160	was a revelation.
293160	296040	I mean, I originally thought that the authors were saying
296040	298040	that people shouldn't think of neural networks
298040	301600	as interpolators because they thought neural networks
301600	303520	were doing something even more sophisticated.
303520	306480	But what I came to realize, at least in Randall's case,
306480	308360	is he's even more cynical about the behavior
308360	310440	of neural networks than Gary Marcus.
310440	313360	He doesn't think that they're fitting curves at all.
313400	315200	Reading Randall's recent spline theory
315200	318080	of deep learning paper has completely changed the way
318080	320760	that I think about their behavior.
320760	324920	So his view, basically, is that neural networks
324920	327160	recursively chop up the input space
328040	332040	into these little convex cells, or polyhedra,
332040	334600	conceptually similar to decision trees,
334600	337880	but one where the regions in the layer can share information,
337880	340880	which means that different faces of these polyhedra
340880	343480	will correspond to different hyperplanes
343480	346640	set down by different neurons in the neural network,
346640	348800	of course, ones which are topologically addressable
348800	350080	from that location.
350080	354920	So to me, this ends the notion that these neural networks
354920	356640	are learning smooth data manifolds
356640	358920	or performing smooth geometric transformations
358920	361080	in the generative setting, which I used to think of
361080	363400	as being a kind of diffeomorphism.
363400	364960	It also gave me the realization that,
364960	367720	at least in inference, each input prediction
367720	372080	is representable with a single linear affine transformation,
372080	374960	which leads directly to the next realization,
374960	377520	which is that the latent space is not homogenous.
377520	379160	It's potentially a different space
379160	381200	for every single input example.
381200	382960	I just didn't realize that before.
382960	384400	You know, suddenly a lot of the magic
384400	385600	of deep learning has vanished,
385600	388320	and I see them in the same light as things like decision trees
388320	390200	and SVMs in classical machine learning.
390200	392320	I mean, probably because I understood
392320	393720	those things very deeply.
394640	396200	So yeah, neural networks have lost
396200	397360	a little bit of their mystery,
397360	399640	but I think it's a good thing.
399640	401680	Reminds me a little bit of this parable
401680	404240	of the blind men and the elephant,
404240	406400	where, let's say you've got four blind men
406400	407680	around an elephant.
407680	409720	One's got his arm around the trunk.
409720	411520	Oh, it feels like a snake.
411520	414560	One's feeling the tail, or it looks like a rope,
414560	416080	or it feels like a rope.
416080	418720	One's on the side, or it feels like a wall.
418720	421400	And it's very similar situation with neural networks
421400	424240	that, you know, the experience of these blind men,
424240	427320	it's all the truth, but it's not the complete truth.
427320	429560	We're all just trying to understand this thing
429560	431120	from different angles.
431120	434960	Now anyway, the two key challenges in machine learning
434960	437760	are one, the curse of dimensionality.
437760	439200	And there's a corollary of that,
439200	441080	knowing what to ignore in the input space,
441080	444040	because it blows up exponentially with the dimensions.
444040	447240	And two, the extrapolation problem.
447240	449280	What happens when you extrapolate
449280	451000	outside of the training data?
451000	455400	And why does this notion of extrapolation even matter?
455400	458640	Well put simply, it matters because of the implications
458640	462240	it has for generalization in deep learning.
462240	465240	Now, why do deep learning models work at all?
465240	467600	Well, I think there's an incredible amount of engineering
467600	471220	which goes into the state of the art machine learning models,
471220	474880	which makes them work very well on specific tasks.
474880	478680	But it's easy to deceive yourself with neural networks.
478680	481320	Literally, everything about your training process
481320	483760	and predictive architecture could be leaking
483760	486360	the main specific information into your model.
486360	489000	Neural networks are not really blank slate models
489000	490480	like we've been led to believe.
490480	492880	A lot of human crafted domain knowledge
492880	495840	goes into these models.
495840	497560	By the way, this is why Francois Schollet,
497560	499480	I mean, he talks about this notion
499480	501240	of developer aware generalization,
501240	504320	which is being able to generalize to tasks
504320	506280	which the developer of the system
506280	508080	didn't know about at the time.
509060	512200	Now, weights and biases is the developer first
512240	515640	of all, MLops platform build better models faster
515640	518560	with experiment tracking, dataset versioning
518560	520160	and model management.
520160	522480	It provides a platform and a set of tools
522480	524960	which you can use for the entire life cycle
524960	526840	of your machine learning process.
526840	529600	Every team has a reliable system of record
529600	530880	for their engineering system, right?
530880	533360	Whether it's Azure DevOps for software engineering
533360	535680	or Confluence for team wikis.
535680	538560	Weights and biases is your system of record
538560	539880	for machine learning.
539880	542600	Now, for your quick link and to help out the podcast,
542600	546280	head over to 1db.me forward slash MLST.
546280	548880	I'm extremely proud actually that we've been sponsored
548880	549800	by weights and biases.
549800	552080	I've been a huge fan of what they've been doing
552080	553240	since the very beginning.
553240	555800	And also I've been a part of their community forum.
555800	557800	They are the one company who in my opinion
557800	559880	has totally changed the game
559880	562640	around what is the highly nuanced process
562640	565240	and the kind of multidisciplinary team actions
565240	568040	that are required to get machine learning models
568040	569600	safely into production.
569600	572280	I've been a chief data scientist for a major corporation
572280	573440	for the last 18 months or so.
573440	575720	And I've always been a big believer
575720	577560	in introducing engineering rigor
577560	579400	to machine learning and data science.
579400	581560	Engineering fundamentals are so important
581560	584080	when you bring machine learning systems to production
584080	587800	as is helping data scientists to become first class citizens
587800	591660	in the entire life cycle of model development and deployment.
591660	593720	It's almost a shame that MLST has become
593720	595200	quite science orientated
595200	598000	because in my day job, I'm an engineer first.
598000	600640	I would love to make more content about this stuff.
600640	602880	Anyway, weights and biases helps you go faster.
602880	604400	It makes your team more resilient
604400	606100	by increasing your knowledge sharing.
606100	607120	It helps you build models
607120	609640	which not only have better predictive performance
609640	612000	but are more safe and secure.
612000	614720	Weights and biases gives you better management information
614720	616980	allowing your company to make better decisions
616980	619840	and build data products with greater transparency
619840	621160	than ever before.
621160	623440	And critically, because weights and biases
623440	626080	orchestrates the entire machine learning process,
626080	627680	your models are reproducible
627680	630240	and important decisions are immortalized.
630240	632680	This fine level of control lets you set guardrails
632680	636480	where you need to and reduce friction wherever possible.
636480	639400	I've designed many ML DevOps systems over the years,
639400	641360	including when I worked at Microsoft.
641360	643360	The technology has really come of age now
643360	645200	and I think you should be using a platform
645200	646520	like weights and biases
646520	649120	to help you train your machine learning models
649120	651080	and to get them into production.
651960	654840	Reproducibility has always been one of the biggest problems
654840	655880	in machine learning.
655880	657840	And the reason for that is that these systems
657840	659600	are quite brittle, frankly.
659600	661920	Their runtime characteristics depend strongly
661920	664000	on the vagaries of the training regime,
664000	665720	the choice of hyperparameters,
665720	668280	even the hardware that they were trained on.
668280	669800	You know, you always get into this syndrome
669800	671800	where it works on my machine
671800	673200	but I can't put it into production
673200	675040	because I've got no idea
675040	677560	how to recreate this thing somewhere else.
677560	679680	I work in a highly regulated industry
679680	681800	and we often need to wind the clock back
681800	684560	to try and understand how a model was created
684600	686200	which decisions were made
686200	687480	and possibly go back later
687480	689440	to reason about a model's behavior.
689440	692600	This is precisely what weights and biases does.
692600	694560	Now, it's completely free for academics
694560	695880	and it's simple to get started.
695880	697880	You just have to add a few lines of code.
697880	699400	So what are you waiting for?
699400	700960	Remember to click on our special link
700960	702240	because it helps us out.
702240	704920	That's 1db.me forward slash MLST.
704920	706720	We are so grateful for the sponsorship
706720	707880	from weights and biases.
707880	709640	And also feel free to get in touch with us
709640	711600	if you're interested in sponsoring the show.
711600	713720	It'll help us scale up the operation a little bit
713720	715640	because at the moment I'm doing all the editing
715640	716680	and it takes a lot of time.
716680	718480	Anyway, thank you very much.
718480	721160	Randall's other recent work demonstrates
721160	723720	that a large class of neural networks
723720	728040	including CNNs, ResNets, RNNs and beyond,
728040	731480	those that use piecewise linear activation functions
731480	735160	like RELU can be entirely rewritten
735160	737760	as compositions of linear functions
737760	742040	arranged in polyhedra or cells in the input space.
742960	747880	It's a high resolution slice and dice into linear forms
747880	751800	like a fruit ninja set loose on a marching cubes algorithm
751800	755160	such as K-means clustering, matched filter banks
755160	757240	and vector quantitization,
757240	760640	which in plain English means locality sensitive clustering.
761520	763320	It also provides new insight
763320	767000	into the functioning of neural networks themselves.
767000	769120	For example, with this view,
769120	770800	what a neural network is doing
770800	773920	is performing an input sensitive lookup
773920	775720	for a matching filter
775720	778240	and then computing a simple inner product
778240	780760	between that filter and the input signal.
781800	783120	If that doesn't shed light
783120	786240	on Francois Chalet's characterization of neural networks
786240	788880	as locally sensitive hash tables,
788880	791120	then I don't know what will.
791120	793400	Another cool thing to come out of Randall's work
793400	795520	was a geometrically principled way
795520	798920	of devising regularization penalty terms
799000	801160	which can improve neural network performance
801160	804200	by orthogonalizing the placement
804200	806720	of those latent hyperplane boundaries
806720	810200	to increase their representational power.
810200	812520	In short, looking at neural networks
812520	816520	through this piecewise linear kaleidoscope, if you will,
816520	819480	is opening new avenues of technical understanding
819480	820680	and exploration.
820760	823760	Now, we spoke with Yannick about this as well,
823760	826600	and he commented that he had looked into these polyhedra,
826600	830400	which he called relu cells in his own research.
830400	832600	While he agreed that the boundaries between them
832600	834960	are not technically smooth,
834960	836800	there are so many of them,
836800	839920	combinatorially many possible polyhedra, in fact,
839920	843280	since each can be defined by any combination
843280	845800	of topologically addressable hyperblogics
845800	848640	that can be found in a neural network.
848760	851880	So, if you look at some of the topologically addressable
851880	854920	hyperplane boundaries, there are so many,
854920	856920	Yannick thinks that neural networks
856920	860320	can make them effectively smooth by arranging them
860320	864400	so that they change very little from neighbor to neighbor.
864400	868200	This insight came from his work in adversarial examples,
868200	871200	where the name of the game is to perturbed the input
871200	874440	as little as possible to the nearest polyhedron
874440	876240	with a different class.
876240	879720	He, Randall's work has transformed the way I think
879720	881720	about neural networks.
881720	885760	I know what they're doing at a much deeper level now.
885760	887760	Each layer of a neural network
887760	890680	contributes a new set of hyperplanes,
890680	893720	and the relu's act to toggle the hyperplanes
893720	896160	in an input sensitive way.
896160	900040	Every layer, indeed every neuron in the network,
900040	902080	not just the final layer,
902080	905640	participates in placing flat decision boundaries,
905640	909400	defining a honeycomb of affine cells.
910560	913880	Each input signal will fall into one of these cells
913880	918280	and be transformed by the combined affine transformation
918280	920400	of every neuron it activated.
922040	926320	I also used to think of a single unified latent space
926320	927840	at each layer.
927840	931360	However, it seems obvious that any particular input
931360	933960	will toggle in an input specific way
933960	935440	a set of hyperplanes.
935440	939560	By virtue of the relu's, it does or does not activate.
939560	942880	Therefore, different populations of input samples
942880	947640	will reside in different latent spaces at any one layer
947640	951960	defined by the activated set of hyperplanes.
951960	953840	So it seems to me that instead of having
953840	956080	a unified latent space,
956080	961080	we have input specific latent spaces plural at each layer.
962080	964440	There's also the matter of whether classifiers
964440	967600	are even learning the data manifold at all.
967600	969360	I have my doubts.
969360	972440	They may be learning predictably useful aspects
972440	974080	of the manifold.
974080	977160	However, neural network classifiers are optimized
977160	979480	to find class boundaries,
979480	983120	and any structure between boundaries can be ignored.
984240	986800	The idea that learning boundary manifolds
986800	991040	necessarily means learning intrinsic connection manifolds
991360	993560	is where I'm really struggling now.
993560	998000	I can't convince myself that optimizing for separation
998000	1001800	will give the same outcome as optimizing for connection.
1002720	1005360	For example, training the same neural network
1005360	1008880	in two different ways, one for classification
1008880	1012000	and once for decoding or generation,
1012000	1014880	will result in very different network weights
1014880	1016800	and latent structure.
1016800	1018880	It's even more difficult to accept now
1018880	1022920	that I'm thinking of these manifolds as piecewise linear,
1022920	1024720	where what the neural network has learned
1024720	1027800	is combinations of separating hyperplanes
1027800	1032400	that chop the space into single class polyhedra.
1032400	1036080	After all, there is nothing in the objective function
1036080	1039480	which cares about the structure within a polyhedra.
1039480	1042920	As long as the class is correct, we're good.
1042920	1045760	Nor is there anything necessarily optimizing
1045760	1048560	for global manifold structure
1048560	1051880	beyond just efficient and parsimonious use
1051880	1054080	of the available parameter space
1054080	1057480	and whatever prior structure was hard-coded
1057480	1061120	by humans and the network constraints and topology itself.
1062720	1064520	Randall commented in the interview
1064520	1066560	that in high-dimensional spaces,
1066560	1069240	you can easily separate everything
1069240	1072320	but your generalization performance might be bad.
1072320	1076520	So you want to trade off separability with dimensionality.
1076560	1079880	He thinks the merit of deep neural networks
1079880	1083320	is being able to find a nonlinear transformation
1083320	1086520	which retains separating hyperplanes
1086520	1089240	while reducing dimensionality enough
1089240	1092200	to confer generalization power.
1092200	1093760	I mean, I couldn't help but notice
1093760	1097120	that everything in the machine learning world is linear.
1097120	1099720	All the popular algorithms are linear
1099720	1102000	and any nonlinearity is a trick.
1102000	1104400	I mean, we just apply some nonlinear transformation
1104400	1107080	to the data before running it through our algorithms
1107080	1109600	just like how it is in support vector machines
1109600	1112760	or kernel ridge regression in Gaussian processors.
1112760	1114480	Deep learning models are no different.
1114480	1117520	We're just placing these relus all over the input space
1117520	1119120	to slice it and dice it.
1121040	1121880	Let's think about this.
1121880	1124560	What is the simplest mathematical model?
1124560	1126160	Linear, right?
1126160	1129120	What's the next most simple one?
1129120	1130880	Piecewise linear.
1130880	1133320	So machine learning hasn't even evolved yet
1133320	1134240	to the second order.
1134280	1136840	Neural networks in their contemporary usage
1136840	1139560	only include the minimum possible nonlinearity
1139560	1141720	of piecewise linear.
1141720	1143800	If they didn't, an entire neural network model
1143800	1147280	could be described as one monolithic linear function
1147280	1149680	with a single transformation matrix.
1149680	1152200	This view, I think, should give you a cleaner
1152200	1155240	and analytical view of neural networks.
1155240	1157480	Now, there are some reasons, in my opinion,
1157480	1159000	for the tendency towards linear.
1159000	1161200	I mean, computability is one of the reasons,
1161200	1165720	but almost every part of mathematics favours linearization,
1165720	1167920	whether it's Newton's method or calculus
1167920	1171440	or linear algebra solving PDEs,
1171440	1172520	plenty of other examples.
1172520	1175720	So it shouldn't come as a huge surprise.
1175720	1179200	Not only that, the function space of nonlinear functions
1179200	1180880	is exponentially larger.
1180880	1182360	And remember, in machine learning,
1182360	1184440	the big challenge is to reduce the size
1184440	1187000	of the approximation class.
1187000	1188680	So what about boundaries?
1188680	1190400	Now, the purpose of every value cell
1190400	1193480	is to define a boundary in the ambient space
1193480	1196360	to chop off what is no longer required.
1196360	1198920	All you have is a linear separating hyperplane.
1198920	1201840	And downstream, you work out the distance
1201840	1205040	from the hyperplanes that you're on the right side of,
1205040	1206640	you know, the ones that didn't get chopped off.
1206640	1208600	So the magic of neural networks
1208600	1212920	is actually learning what to ignore in the ambient space.
1212920	1215960	Now, I think a problem with my previous intuition
1215960	1219280	is that, like most people, I imagine to neural network,
1219280	1222080	latent space is a bit like a UMAP or a Disney projection
1222080	1225360	plot, and this leads us to misunderstand their behavior.
1225360	1228400	The latent space that these examples get projected into
1228400	1229680	is not homogeneous.
1229680	1231480	Depending on which cell you fell into
1231480	1233920	in the input space or the ambient space,
1233920	1236480	a different affine transformation will be applied,
1236480	1239160	sending you to a different region of the latent space.
1239160	1242520	So the latent space is kind of stitched together
1242520	1246440	like bits of a cosmic jigsaw puzzle in the ambient space.
1246440	1248760	And then when you run UMAP on the latent,
1248760	1250480	you see all of the clusters, but you'd
1250480	1252960	be forgiven for thinking that it was performing
1252960	1256360	some kind of smooth, diffeomorphic transformation
1256360	1259000	of the entire input space and successive layers
1259000	1261520	in the neural network, or even learning the topology
1261520	1262800	of the data manifold.
1262800	1264880	I think it's much better to think of neural networks
1264880	1268080	as quantizing the input space, much like a vector search
1268080	1271280	engine does using locality-sensitive hashing.
1271280	1274520	Now, imagine a classification problem on the Cartesian plane
1274520	1277800	where the upper right and lower left quadrants are blue,
1277800	1281400	and the upper left and lower right quadrants are orange.
1281400	1284480	Now, as a minimal example, if we trained a single layer
1284480	1287840	for neuron-relu neural network to fit this,
1287840	1290640	it will fit four diagonal hyperplanes,
1290640	1294400	two of which are reflected versions of the same hyperplane.
1294400	1296120	Now, by the way, you can play with this
1296120	1298560	on the TensorFlow playground, which we think is probably
1298560	1301520	one of the best tools for building strong intuition
1301520	1303680	on how neural networks work.
1303680	1306720	Now, what it does is it shows you how the ambient space
1306720	1309000	is being effectively subdivided by all of these
1309000	1310760	addressable hyperplanes.
1310760	1313400	But this quadrant example is unintuitive in the tool
1313400	1316800	because getting your head around how the relus act together
1316800	1320360	to combine these hyperplanes to carve up the ambient space
1320360	1322200	isn't immediately obvious.
1322200	1325040	Now, when you run the tool on a more complex example,
1325040	1327200	so for example, a spiral manifold,
1327200	1331080	you'll now see the artifacts of these hyperplanes everywhere.
1331080	1333840	When these piecewise linear chops are composed together
1333840	1336320	in the second layer, we get a decision surface
1336320	1339080	in the ambient space, which can appear smooth given
1339080	1341600	enough pieces, but it's actually a composition
1341600	1343720	of piecewise linear functions.
1343720	1346040	Now, as you can see, it's just chopping up the input space
1346040	1348280	in every neuron in the first layer,
1348280	1351080	each one with a different angle and translation.
1351080	1352440	And when you get to the second layer,
1352440	1355280	it's chopping up the space in a more sophisticated way
1355280	1358400	by composing together the chops from the previous layer.
1358400	1360160	Now, the first thing to realize here
1360160	1362520	is that for each test input example,
1362520	1364440	and every example in its vicinity,
1364440	1366600	its class projection is definable
1366600	1368560	with a single affine projection.
1368560	1370400	There's no continuous morphing going on here.
1370400	1373800	It's more like a jigsaw puzzle being pieced together
1373800	1377520	to form many parts of a bigger ambient space.
1377520	1380240	Now, oftentimes we see negative weights,
1380240	1383200	which actually allows the hyperplanes to combine
1383200	1385440	in interesting ways to kind of partially cancel
1385440	1389200	each other out to allow more complex decision boundaries.
1389200	1392920	Any hyperplane can be reused by multiple other neurons,
1392920	1394680	whichever neurons it can reach
1394680	1397480	via some topologically addressable pathway.
1397480	1399120	Every hyperplane can be the face
1399120	1401520	of multiple polyhedra in the ambient space.
1401520	1403080	This is how they share information.
1403080	1404400	And it should be obvious by now
1404400	1407080	that there are two to the N addressable polyhedra
1407080	1410040	in the ambient space where N is the number of neurons.
1410040	1411800	Now, we also observed that the neural network
1411800	1414360	is only capable of slicing and dicing the input space
1414360	1415720	with flat planes.
1415720	1418280	The only smooth nonlinearities enter the picture here
1418280	1421440	where we as data scientists linearize the data
1421440	1424200	by performing some smooth nonlinear transformation
1424200	1426560	before it even enters the neural network,
1426560	1427880	much like is the case
1427880	1429960	with other machine learning algorithms.
1429960	1432640	Now, even on the spiral data set example,
1432640	1435600	the quickest way to make the neural network fit the data
1435600	1437640	is to effectively linearize the data
1437640	1440000	by applying a nonlinear transformation
1440000	1441800	before it even gets in.
1441800	1442880	Now, we also played around
1442880	1445600	with this kind of contrived circular manifold data set
1445600	1448240	where there's a ball of data encompassed
1448240	1449320	by a circle around it.
1449320	1451400	And it's possible to do a better job on that
1451400	1453400	with one relu neuron, right?
1453400	1455160	But with two nonlinear transformations
1455160	1456920	of the input data set,
1456920	1459040	rather than fit this huge bunch
1459040	1461280	of piecewise linear relus on the ambient data.
1461280	1462560	I mean, it should be obvious, right?
1462560	1465240	But this idea that you don't need feature engineering
1465240	1467920	in neural networks, I think is nonsense.
1467920	1470280	Now, given how obvious it is that neural networks
1470280	1471840	are just chopping up the input space
1471840	1473560	with these piecewise linear functions,
1473560	1474680	it begs the question,
1474680	1477560	how could they possibly extrapolate in any meaningful way,
1477560	1479520	right, especially in the general setting?
1479520	1482840	And by the way, when we say extrapolation,
1482840	1485320	we're talking about this more general sense
1485320	1486680	of continuing the pattern,
1486680	1491240	not the convex whole notion in the given in the paper.
1491240	1493920	Regarding these piecewise linear functions,
1493920	1496320	I remember an impactful moment
1496320	1498680	for more than 20 years ago in grad school,
1499520	1502400	a visiting professor who had multiple science
1502400	1503920	and nature publications,
1503920	1506960	you know, a pinnacle of academic achievement,
1506960	1509760	was sharing his insights on how we students
1509760	1513760	might also extract such papers ourselves from nature.
1513760	1515720	And he was showing figures from his paper
1515720	1518520	and they all had two things in common.
1518520	1522320	First, they depicted very simple relationships
1522320	1525120	that were previously undiscovered, okay?
1525120	1529680	And second, all the fitted models were piecewise linear.
1530680	1533120	He even explicitly commented along the lines
1533120	1536040	that of course there's some underlying
1536040	1539040	smooth nonlinear relationship,
1539040	1542480	but absent a solid theoretical model of that,
1542480	1545920	which is often going to be the case for new discoveries,
1545920	1550840	you're better off with simple piecewise linear models.
1550840	1552200	I guess when in doubt,
1552200	1555480	Occam's razor always makes straight cuts.
1556600	1558280	And now it's clear to me
1558280	1561240	that the successful deep networks of today
1561240	1563720	are following that advice.
1563720	1567280	If you've ever wondered why piecewise linear activation
1567280	1570160	functions are dominating the field,
1570160	1573840	maybe it's because they've abandoned all pretense
1573840	1576920	at finding smooth nonlinear models
1576920	1581840	and are keeping it simple by fitting piecewise linear models,
1581840	1585800	albeit at machine scale with billions of parameters.
1586800	1591160	Randall's spline work makes that bit of philosophical insight
1591240	1594080	brutally clear in my opinion.
1594080	1596280	It conjures a scene of Darth Vader,
1596280	1599640	that cybernetic warlord towering over
1599640	1601880	a growing neural network saying,
1601880	1604780	embrace your hyperplanes.
1605920	1608040	Now, you know, I've always been skeptical
1608040	1610240	and often remind us of the limitations
1610240	1612320	of today's machine learning.
1612320	1615720	I'd say things like ML isn't magic learning,
1616600	1617680	but for a moment,
1617680	1621560	I too allowed myself to become deluded into thinking
1621560	1624760	that they are creating some kind of nonlinearity
1624760	1627880	beyond piecewise linearity.
1627880	1632840	But relu is by far the dominant activation function
1632840	1636440	because it stops pretending at anything other
1636440	1638840	than piecewise linear.
1638840	1642880	Just stick in a flat boundary threshold and the line.
1642880	1644880	A neuron puts in a hyperplane
1644880	1647120	and then lets the rest of the network
1647120	1649300	chop more as needed.
1649300	1651800	All the success of neural networks
1651800	1656380	seems explained by piecewise linear functions.
1657600	1660760	I also find it intriguing that our own brains,
1660760	1663440	our own wet neural networks,
1663440	1665520	have somehow gained access
1665520	1668940	to smooth nonlinear imagination.
1669920	1673220	Just look at the laws we've defined in physics.
1674180	1679060	Many exhibit nonlinear but smooth structure.
1679060	1680420	On the other hand,
1680420	1682820	neural networks chop up the input space
1682820	1687240	with flat boundaries and sharp edges.
1687240	1691700	How can we possibly expect them to learn or discover
1691700	1694200	the kinds of smooth relationships
1694200	1698140	that seem fundamental to science and reality?
1698140	1700200	All feature engineering and representation learning
1700200	1702380	and machine learning is about finding these
1702420	1704620	interpolative representations.
1704620	1706300	Now, I saw a really interesting example of this
1706300	1708620	when I was reading Francois Chouelet's book recently
1708620	1712920	and he spoke of pixel grids of watch faces, right?
1712920	1716780	Now, they're not interpolative in the ambient space.
1716780	1718780	If you take the average of two of these images,
1718780	1722860	you'll just get four faded clock hands
1722860	1724180	on top of each other, right?
1724180	1726020	Superposed, not very helpful.
1726020	1728340	But however, if you represent the hands
1728340	1729980	with Euclidean coordinates,
1729980	1732540	then the problem becomes more interpolated
1732540	1735300	but you're still not all the way there.
1735300	1737100	The average of two points of interest
1737100	1739540	doesn't fall on this intended circle manifold
1739540	1741580	that we're interested in.
1741580	1742900	You know, it would be interpolated
1742900	1745700	if we use some relevant distance function
1745700	1748400	which was some geodesic interpolation.
1749460	1750580	But you know, anyway,
1750580	1753180	if we encoded the problem using polar coordinates,
1753180	1755420	then it becomes linearly interpolated, right?
1755420	1756580	The problem is solved
1756580	1759400	and you can generalize perfectly to any new clock face.
1759400	1761000	But I mean, in that case,
1761000	1762480	you wouldn't even need a neural network
1762480	1765040	and the kicker is that we as data scientists
1765040	1767880	would have to figure this out ourselves, right?
1767880	1769800	The neural network wouldn't be particularly efficient
1769800	1773200	at doing so, especially if we, the data scientists,
1773200	1775900	didn't feed in the relevant nonlinear transformation
1775900	1779000	before it went into the model.
1779000	1780720	So anyway, if there exists
1780720	1782640	a nonlinear transformation of this problem,
1782640	1785080	then, you know, which makes it interpolated,
1785080	1786420	then we can say in some sense
1786420	1788840	that it's got a lower intrinsic dimension.
1788840	1790280	So crucially, deep learning models,
1790280	1793720	they can be understood as unconstrained surfaces.
1793720	1795180	They do have some structure
1795180	1796860	coming from their architectural priors,
1796860	1800520	but that structure only serves to restrict the search space
1800520	1803200	to a smaller space of unconstrained surfaces.
1803200	1806060	It doesn't provide the kind of inductive prior
1806060	1809080	that would enable stronger generalization.
1809080	1812160	A model architecture does not contain a model of the domain
1812160	1814780	except in the extremely restricted sense
1814780	1817360	of injecting priors such as, I don't know,
1817360	1820520	translational equivalents in convolutional neural networks.
1820520	1823960	If you want to generalize in a more systematic fashion,
1823960	1826600	you either need a model with a strong inductive prior
1826600	1828160	or a model which doesn't operate
1828160	1831800	by empirically slicing up the ambient space,
1831800	1834280	yeah, like a deep learning model does.
1834280	1836240	A discrete symbolic program
1836240	1837600	would be an interesting alternative.
1837600	1841480	I mean, just imagine the program Y equals X squared.
1841480	1844000	It generalizes to any arbitrary number, right?
1844000	1845320	Because it's highly structured.
1845320	1847560	You can fit it with three training points.
1847560	1849800	And once fit, it'll generalize to anything,
1849800	1852680	even things that are outside of the training range.
1852680	1854000	But the kicker, of course,
1854000	1855880	is that you wouldn't know which curve to use
1855880	1858800	if you only saw three points in the first place.
1858800	1860680	Now, let's think of another example.
1860680	1864340	And imagine a discrete staircase space.
1864340	1866000	So in this particular example,
1866000	1869680	an unconstrained curve gives you complete garbage,
1869680	1872920	but a structured model with the correct priors,
1872920	1874560	you know, it can still be a curve,
1874560	1877320	will generalize in the extrapolation regime.
1877320	1879800	And this prior basically means that you need to encode it
1879800	1880640	or linearize it
1880640	1883160	before it even gets into the neural network.
1883160	1885520	In both cases, you're making the neural network
1885520	1887800	work extremely well for a specific thing,
1887800	1891440	but not very well for generalizing between new tasks.
1891440	1892760	So anyway, when we say
1892760	1895480	that deep learning models generalize via interpolation,
1895480	1897560	what we're really saying is that these models
1897560	1898400	that we're using today,
1898400	1899640	that they work well with data
1899640	1902440	that's within the training distribution of the problem,
1902440	1906840	right, well, a problem that's intrinsically interpolative,
1906840	1910160	but they won't generalize systematically to anything else.
1910160	1912360	You won't generalize when looking at problems
1912360	1914440	that are not interpolative in nature
1914440	1918400	and problems that are outside the training distribution.
1918400	1919280	So what do we talk about
1919280	1921520	this binary notion of extrapolation?
1921520	1923760	Well, the problem with the binary convex
1923760	1925800	whole notion of extrapolation is that
1925800	1927480	we're promoting this idea
1927480	1931240	that the moment an example falls epsilon outside the hole,
1931240	1933640	that the prediction qualitatively changes.
1933640	1937040	Instead, I think it would be more of a distance question,
1937040	1939560	which is to say, the further away you get
1939560	1942760	from this convex hole, the greater the uncertainty.
1942760	1944320	I mean, it's not like you fall off a cliff
1944320	1946400	when you step outside the convex hole.
1946400	1948360	Your approximation of the latent manifold
1948360	1950080	remains valid for a little while,
1950080	1951880	although it will quickly degrade
1951880	1953480	as you move further outside,
1953480	1956040	unless your model has some strong structural prior
1956040	1958760	that matches the regularity of that latent manifold.
1958760	1960720	So this brings up an important point, right?
1960800	1963120	The relevant question is not whether you're inside
1963120	1964640	or outside the convex hole.
1964640	1968080	It's how far you are from the nearest training data points.
1968080	1970160	If you're outside the convex hole,
1970160	1972440	but you remain close to the training data,
1972440	1975160	then your model remains an accurate approximation
1975160	1976520	of the latent manifold.
1976520	1978920	And even if you're inside the convex hole,
1978920	1981200	you're in a region that wasn't densely sampled
1981200	1982720	at training time, right?
1982720	1985200	Where there's no nearby training points.
1985200	1986560	Your unconstrained model
1986560	1988960	may not accurately approximate the latent manifold.
1988960	1992960	So inside or outside doesn't really tell you very much.
1992960	1994800	It's all about the proximity.
1994800	1997520	You're performing local generalization
1997520	1999880	and the quality of your ability to generalize
1999880	2003040	to new situations depends entirely
2003040	2005960	on their proximity to known situations.
2005960	2007920	However, if you're dealing with training data
2007920	2009200	that is densely sampled
2009200	2011360	within a certain region of the problem space,
2011360	2014480	then the question, can I generalize here
2014480	2017200	becomes approximately equivalent to,
2017200	2019920	is this inside the convex hole or outside?
2019920	2021520	Now, the whole point of feature engineering
2021520	2024480	is to make data sets interpolative.
2024480	2027400	Either us data scientists design the features by hand
2027400	2031000	or neural networks learn them as part of the training process
2031000	2033800	in big air quotes for the podcast listeners.
2033800	2036280	So, I think when folks make the argument
2036280	2037920	that machine learning models generalize
2037920	2041360	via interpolation or that computer vision data sets,
2041360	2042840	that they're interpolative,
2042840	2045200	what they mean is that there's an encoding space
2045200	2048000	for which the problem becomes interpolative
2048000	2051600	or there exists a non-Euclidean pairwise distance function
2051600	2055080	between the instances that makes the problem interpolative,
2055080	2056920	which is basically the same thing.
2056920	2059120	So, if it's possible to do this,
2059120	2062320	then you can say that the problem is intrinsically interpolative
2062320	2065560	but your current representation of the problem is not.
2065560	2067480	So the key endeavor in machine learning
2067480	2069720	is to find better representations,
2069720	2073120	representations which reveal the interpolative nature
2073120	2073960	of the problem.
2074880	2077880	Now, you might just cynically write off this paper
2077880	2079360	as being a trivial finding, right?
2079360	2082480	You know, it seems trivial that the pixel space
2082480	2084720	is not linearly interpolatable.
2084720	2086320	Something that we've known about for decades.
2086320	2089560	I mean, that's why computer vision engineers in the 1980s
2089560	2091440	would write feature detectors
2091440	2093420	to create an interpolative space
2093420	2096160	for machine learning algorithms to work on, right?
2096160	2097560	If you take any two images
2097560	2100120	and you interpolate between them, what do you get?
2100120	2102240	You just get a faded copy of both.
2102280	2105720	Everyone in the field has known about this for decades, right?
2105720	2109280	That examples are not interpolative
2109280	2112160	in their original encoding space or their pixel space.
2112160	2113440	I think the most important thing though
2113440	2115400	is that this paper also shows
2115400	2117760	that the models are not interpolative
2117760	2120220	in their latent space either.
2120220	2121140	That's a shocker.
2122040	2123920	Now, what does Randall say about all of this?
2123920	2127440	Well, Randall Bellisterio thinks that the steelman
2127440	2129760	for this interpolation argument would be,
2129760	2132620	well, what if you had very low dimensional,
2132620	2134880	kind of like approximation of your data
2134880	2136520	with very few factors of variation,
2136520	2139600	which could be easily linearized in the latent space,
2139600	2142440	then it might be interpolative.
2142440	2145960	Randall thinks that the very concept of interpolation,
2145960	2148080	I mean, it was defined about 50 years ago
2148080	2150680	to describe these very small models
2150680	2153640	with very few factors of variation.
2153640	2156800	And it's just not relevant today, right?
2156800	2157800	They showed in their paper
2157920	2160140	that even in the relatively small latent space
2160140	2163280	on a popular neural network classifier,
2163280	2165320	interpolation does not occur.
2165320	2167960	He thinks that most people think of interpolation
2167960	2170160	by kind of conceptualizing their data
2170160	2172560	into a few important latent factors.
2172560	2174160	You might have dogs, for example,
2174160	2176200	and they might exhibit a latent color,
2176200	2178800	and a new color might be an interpolation
2178800	2180440	between observed colors.
2180440	2182640	But these guys think that we need to have
2182640	2185640	an entirely new definition of interpolation.
2185680	2187920	I think the main purpose behind their paper,
2187920	2190480	according to them, is to show that even though
2190480	2192800	the intuition that people have of interpolation
2192800	2194600	works well in low dimensions,
2194600	2196880	it falls down completely flat on its face
2196880	2198120	in higher dimensions.
2198120	2200720	Actually, the probability of your test data
2200720	2203520	being in the convex hull of your training data
2203520	2206720	is near zero past a certain number of dimensions.
2206720	2208680	So the current definition of interpolation,
2208680	2210400	which is to say like this notion
2210400	2211880	of convex hull membership,
2211880	2215120	it's too rigid to think about how interpolation works
2215120	2216600	in neural networks.
2216600	2220360	So Randall thinks that any new definition of extrapolation
2220360	2224040	should be directly linked to generalization itself.
2224040	2225760	This is what they're trying to get at.
2225760	2228960	This is a clip from our show with Professor Max Welling.
2228960	2229920	The first thing I wanna say,
2229920	2232240	there is no machine learning without assumptions.
2232240	2236040	It just basically, you have to interpolate between the dots,
2236040	2238520	and to interpolate means that you have to make assumptions
2238520	2240080	on smoothness or something like that.
2240080	2243000	So the machine learning doesn't exist without assumptions.
2243000	2244080	I think that's very clear.
2244080	2246960	But clearly it's a dial, right?
2246960	2249160	So you can have, on the one end,
2249160	2251480	you can have problems with a huge amount of data.
2251480	2253720	It has to be available clearly.
2253720	2257000	And there you can dial down your inductive biases.
2257000	2260600	You can basically say that the data do most of the work
2260600	2261920	in some sense.
2261920	2264960	Now, I think it's a good thing to build intuition
2264960	2266960	about machine learning principally
2266960	2269120	as an interpolation problem, right?
2269120	2270840	So we're given the training data,
2270840	2273080	and we need to cleverly interpolate
2273080	2275400	between the training examples to reason
2275400	2278600	about the statistical context of the test examples.
2278600	2280840	This is what machine learning is in a nutshell.
2280840	2284200	And actually, a researcher friend of mine
2284200	2286120	from MetaAI Research in Silicon Valley,
2286120	2288160	a guy called Dr. Thomas Lukes,
2288160	2289840	he published a paper a couple of years ago
2289840	2293000	called Interpolation of Sparse High-Dimensional Data.
2293000	2295320	And he showed that it was indeed possible
2295320	2298440	to perform competitively with multi-layer perceptrons
2298440	2302040	for a regression problem using pure play interpolation methods
2302080	2306080	like Voronoi and Dolornoi triangulation and spline methods.
2306080	2308920	I mean, think of creating simplexes
2308920	2311320	of the nearest neighbors around a training data
2311320	2313480	and then averaging the results together.
2313480	2316000	This simple method works remarkably well
2316000	2317880	up to about 30 dimensions.
2317880	2319880	I mean, obviously eventually it gets deranged
2319880	2321360	by the curse of dimensionality,
2321360	2324120	but as we'll discover slightly later,
2324120	2326800	neural networks might have a slight advantage
2326800	2329280	over pure play simplex interpolation
2329280	2330520	just because they work principally
2330520	2334120	by figuring out boundaries of the input space to exclude.
2334120	2337160	So, you know, if you're on the zero side of the ReLU, that is.
2337160	2339240	So that helps a lot in high dimensions
2339240	2340960	when you have a poor sampling density
2340960	2343040	in a particular region of the input space.
2343040	2347120	But anyway, if you do principally think of machine learning
2347120	2348960	as being an interpolation method,
2348960	2351080	it raises some interesting ideas.
2351080	2353760	You know, Thomas said to me that the crux of the problem
2353760	2355840	is that we're trying to approximate a function
2355840	2357760	that has spatially unique behavior
2357760	2359720	in more than about 20 or so dimensions.
2359720	2362240	Like in that case, it's hopeless.
2362240	2363720	There's no question about it.
2363720	2366680	He says that you can increase the data density,
2366680	2368720	you know, like in 100 dimensions,
2368720	2372120	but to get a grid with 10 points on a side, you know,
2372120	2374200	which is to say 10 to the power of 100,
2374200	2376240	there aren't enough protons in the universe
2376240	2377880	that could convexly cover it.
2377880	2379760	So, and Thomas concluded by saying,
2379760	2382600	what this means is that everything that we do successfully
2382600	2385280	approximate now with millions or billions
2385280	2387120	or even trillions of data points,
2387160	2389160	which accounts for a lot of the successes
2389160	2392080	in machine learning, these data sets
2392080	2396760	only have spatially novel behavior in very few dimensions,
2396760	2399040	right, or a varying gradient in very few dimensions,
2399040	2400240	16 or fewer.
2400240	2403120	If we suppose that the true function has a varying gradient
2403120	2405360	in more than that many dimensions,
2405360	2408800	there's simply not enough data in the world to approximate it.
2408800	2411880	The mathematics here is unequivocal.
2413120	2416520	So, what actually happens when the test examples
2416560	2419040	outside the convex hull of the training data?
2419040	2422520	What happens when we're in an extrapolative regime?
2422520	2424920	Well, it's increasingly impossible in high dimensions
2424920	2428000	for the training set to be statistically representative
2428000	2429120	of the test set.
2429120	2432000	Test set instances will often distinctly differ
2432000	2434120	from anything that we've seen during training.
2434120	2434960	In machine learning,
2434960	2437760	the test set will never fully characterize
2437760	2439560	the problem that we're interested in.
2439560	2441840	So, in high dimensional settings,
2441840	2444200	features are often linearly correlated as well,
2444200	2447160	which makes it challenging to know what information to use
2447160	2448320	and what to discard.
2448320	2451040	This is another fundamental problem of machine learning.
2451040	2454440	Many machine learning approaches depend on modeling
2454440	2456280	these local statistical relationships
2456280	2457720	between the training samples,
2457720	2458800	but in high dimensions,
2458800	2461200	the probability of a new test example
2461200	2463320	being in the convex hull of the training data
2463320	2466000	goes to zero extremely quickly.
2466000	2468360	So, making machine learning models work
2468360	2471760	in an extrapolative regime necessitates the introduction
2471840	2473480	of inductive biases,
2473480	2476880	which are a way of adding a kind of enforced smoothness
2476880	2478320	to the model predictions.
2478320	2481520	So, if you hit the right bias,
2481520	2483400	then it can be beneficial.
2483400	2486440	If you impose the wrong bias,
2486440	2487840	then it's gonna hurt you.
2487840	2489280	And this is a well-known trade-off.
2489280	2492840	So, of course, the whole endeavor of machine learning
2492840	2496240	is defining the right inductive biases
2496240	2499720	and leaving whatever you don't know to the data,
2499720	2502680	and then basically learning to focus your models
2502680	2505000	on the data that you're actually seeing.
2505000	2506760	This is how we stop models
2506760	2508600	from fitting the noise in the data
2508600	2510440	or going completely haywire
2510440	2513000	outside the convex hull of the training data.
2513000	2516480	Extrapolation requires that sensible answers
2516480	2519240	are given for all of the elements of the input space,
2519240	2522360	even in regions not seen during training.
2522360	2525040	And in particular, those within the convex hull
2525040	2527160	of, let's say, some hypothetical,
2527200	2530080	infinitely-sampled training data set.
2530080	2532240	Now, what we do in neural networks
2532240	2534200	and in many other machine learning algorithms,
2534200	2536200	for that matter, is we just take a bunch
2536200	2539720	of parameterized basis functions and we stack them
2539720	2541880	and we fit them to our training data
2541880	2543560	until it's well-described.
2543560	2546400	The most important considerations for interpolation
2546400	2549800	are the smoothness and robustness of these models.
2549800	2552520	We want our models to produce gradual changes
2552520	2554080	between the training points.
2554080	2555600	Points outside the training set
2555600	2557600	should be handled with great care,
2557600	2561400	but what exactly that means depends greatly on the problem.
2561400	2564200	In order to even know that we're in an extrapolative regime,
2564200	2565800	the basis functions must be based
2565800	2568120	near to the training examples.
2568120	2570600	Typically, neural networks do not behave sensibly
2570600	2573200	in regions where there's no training examples, right,
2573200	2574960	because their learned basis functions
2574960	2577520	have been localized around the training data
2577520	2579440	in the ambient space.
2579440	2581200	It's also worth noting that extrapolation
2581200	2583920	and interpolation are two completely different regimes.
2583920	2586240	Optimizing for one, typically,
2586240	2588800	means being worse at the other.
2588800	2590560	Now Randall said that the boundary view
2590560	2592240	of neural networks is very clear
2592240	2593840	in the discriminative setting.
2593840	2596200	You only need interpolation to try
2596200	2598160	and understand how neural networks work
2598160	2599840	in the generative setting.
2599840	2601120	This is a key point.
2601120	2604440	In the generative setting, when you interpolate the latent,
2604440	2606400	we see at first glance what looks
2606400	2608240	like geometric morphing, right?
2608240	2610480	Looking at this, you'd be forgiven for thinking
2610480	2613120	that we're traversing some smooth latent manifold,
2613120	2616280	but on closer inspection, it's not geometric
2616280	2617560	or diffeomorphic at all.
2617560	2620720	It's more of a fuzzy, high-resolution fade-in.
2620720	2623280	Now, one commonly used visual argument
2623280	2624680	for the manifold hypothesis
2624680	2628560	is the MNIST digit interpolation in a generative model.
2628560	2630800	Our own investigation shows that there's a degree
2630800	2633080	of cherry picking in some of the visual examples
2633080	2634480	used to demonstrate this.
2634480	2637240	It's not actually hard to stumble across cases
2637240	2640840	where purported manifold interpolation is no better
2640880	2643280	than ambient-space linear interpolation.
2643280	2645720	There are many examples where latent-space interpolation
2645720	2648640	gives superposed and fuzzy intermediate representations
2648640	2652200	and even crisp images, which are unrecognizable.
2652200	2654440	The blur and cutting and gluing,
2654440	2656360	which is apparent on these images,
2656360	2659120	show that it's definitely not a diffeomorphism
2659120	2660440	in the ambient space.
2660440	2662120	And even though the interpolation path
2662120	2664120	is continuous in the latent space,
2664120	2666840	it's questionable whether that path is semantically relevant
2666840	2669000	to the classification task at hand.
2669000	2671120	I mean, just think of the mean value theorem.
2671120	2672840	It tells you that if you take the value
2672840	2676240	of any continuous function at two points, right,
2676240	2678360	you can also find a point between them
2678360	2680560	where the function hits the average of those values.
2680560	2682000	So in other words,
2682000	2684720	any continuous function produces a manifold,
2684720	2686720	but that doesn't tell you anything interesting
2686720	2689640	about the function beyond what we already know, right,
2689640	2690840	that it's continuous.
2691840	2694800	Now, I'm not so sure that these manifolds are even smooth.
2694800	2696320	When we're talking about smoothness,
2696320	2698400	we're only talking about local smoothness
2698440	2700840	and inside the polyhedrus convex hole.
2700840	2702320	It's linear, right?
2702320	2704360	But there's no smooth surfaces or manifolds
2704360	2705880	anywhere to be seen.
2705880	2707840	For any particular node in a neural network,
2707840	2710640	it's actually a linear function of a subset
2710640	2713080	of the upstream linear functions, right?
2713080	2714560	Very much like a decision tree,
2714560	2716520	but with information sharing.
2716520	2718640	Now, another often used visual aid
2718640	2720280	is this idea that a neural network
2720280	2724240	is effectively uncrumpling and smoothing the ambient space,
2724240	2726600	much like you would do with a sheet of paper
2726600	2728280	with successive transformations
2728280	2730120	in the layers of the neural network.
2730120	2732640	Keith and I previously agreed with this view of neural networks
2732640	2735320	as progressively kind of flattening out the paper,
2735320	2737520	but it now seems to us like they may instead
2737520	2740480	be progressively inserting planes
2740480	2743000	aligned to the facets of the paper ball
2743000	2746680	to chop out locally affine polyhedra or cells
2746680	2749320	to cover the paper's polyhedra.
2749320	2752000	So for us, this is an entirely new way
2752000	2754280	to think about neural networks, okay?
2754280	2757320	So in a way, this is a new parlor trick, right?
2757320	2759200	Do you remember that thing, the game of life,
2759200	2760640	Conway's game of life?
2760640	2763240	It looks like the shapes are smoothly morphing,
2763240	2766480	but they're actually toggling pixels with discrete rules.
2766480	2769240	Gary Marcus talks about the parlor trick of intelligence,
2769240	2771680	but isn't it ironic that there are more parlor tricks
2771680	2773520	going on than most people realize?
2773520	2776520	Namely, that rather than doing smooth geometric morphing
2776520	2779400	via interpolation, these networks are actually chopping up
2779400	2782280	and composing linear polyhedra.
2782280	2785120	Now, if you interpolate between two latent classes,
2785120	2788760	it might traverse several polyhedra in the intermediate space.
2788760	2791320	Along the way, it would pick up characteristics
2791320	2793040	from all of those polyhedra.
2793040	2796400	Some of the black regions are impossible regions,
2796400	2799840	so it's not possible to get there from the latent space,
2799840	2801520	which is very, very interesting.
2801520	2804120	You only see the illusion of continuous morphing,
2804120	2807920	where the neighboring cells are very small and very similar.
2807920	2810560	This is Professor Michael Bronstein.
2810560	2812720	I like to think of geometric deep learning
2812720	2816560	as not a single method or architecture, but as a mindset.
2816560	2819440	It's a way of looking at machine learning problems
2819440	2823480	from the first principles of symmetry and invariance.
2823480	2827600	And symmetry is a key idea that underpins our physical world
2827600	2830920	and the data that is created by physical processes.
2830920	2833560	And accounting for this structure
2833560	2835800	allows us to beat the course of dimensionality
2835800	2837840	in machine learning problems.
2837840	2839440	So what is the manifold hypothesis?
2839440	2842760	Well, natural data falls on smooth manifolds.
2842760	2845160	The manifold hypothesis states that real-world
2845160	2849000	high-dimensional data lie on low-dimensional manifolds,
2849000	2851600	embedded in the high-dimensional space.
2851600	2853360	When people invoke this hypothesis
2853360	2854840	in a machine learning context,
2854840	2857200	they're generally suggesting that neural networks
2857200	2859560	are actually learning this data manifold,
2859560	2862080	which we now find quite hard to believe, frankly.
2862080	2863520	At best, we think they're learning
2863520	2865400	some approximate aspects of it.
2870400	2872320	So essentially, all machine learning problems
2872320	2873840	that we need to deal with nowadays
2873840	2876120	are extremely highly dimensional.
2876120	2878800	Even basic image problems live in thousands
2878800	2881040	or even millions of dimensions.
2881040	2883080	Now, I think most people have this intuition
2883080	2886080	of convex-hole membership, which is to say, in two dimensions.
2886080	2888240	As you sample more and more training data,
2888240	2891640	the convex-hole eventually fills the entire space.
2891640	2893920	But the kicker is that in higher dimensions,
2893920	2897240	the space is so vast, this will never happen.
2897760	2899840	High-dimensional learning is impossible
2899840	2902000	due to the curse of dimensionality.
2902000	2905280	It only works if we make some very strong assumptions
2905280	2907560	about the regularities in the space of functions
2907560	2909120	that we need to search through.
2909120	2911080	The classical assumptions that we make
2911080	2913720	in machine learning are no longer appropriate.
2913720	2916560	So in general, learning in high dimensions is intractable.
2916560	2918600	The number of samples grows exponentially
2918600	2919920	with the number of dimensions.
2919920	2921640	And the curse of dimensionality refers
2921640	2923680	to the various phenomena that arise
2923680	2926040	when analyzing and organizing data
2926080	2929080	in high-dimensional spaces that do not occur
2929080	2930480	in low-dimensional settings,
2930480	2934080	such as the three-dimensional physical reality
2934080	2936240	of everyday experience.
2936240	2937920	Now, the common theme of these problems
2937920	2940800	is that when the dimensionality increases,
2940800	2943720	the volume of the space increases so fast
2943720	2946280	that the available data becomes sparse.
2946280	2949200	And this sparsity is problematic for any method
2949200	2951640	that requires statistical significance.
2951640	2954440	In order to obtain a statistically sound
2954440	2956840	and reliable result, the amount of data needed
2956840	2959520	to support the result grows exponentially
2959520	2960960	with the dimensionality.
2960960	2963800	So the curse of dimensionality in a nutshell
2963800	2965960	is that the probability of a new data point
2965960	2968640	being inside the convex hull of your training data
2968640	2971680	decreases exponentially with the number of dimensions.
2971680	2974960	As an example, to estimate a standard normal density
2974960	2977920	in 10 dimensions with a relative mean square error
2977920	2981120	of 0.1 using an efficient non-parametric technique
2981120	2985440	would require more than 800,000 samples.
2985440	2987160	Now, in the last show,
2987160	2989760	we discussed geometric deep learning in great detail
2989760	2992360	and in some sense extrapolation
2992360	2994600	in the curse of dimensionality are analogous.
2994600	2995960	The reason that these folks wanted
2995960	2998120	to combat the curse of dimensionality
2998120	2999720	was they wanted to build models
2999720	3001960	which could extrapolate outside the training range
3001960	3004000	using geometrical priors.
3004000	3006840	I think it's possible to build understanding
3006840	3008640	of the curse of dimensionality
3008640	3012880	through visual analogy to familiar lower dimensional shapes
3012880	3016200	such as circles, squares, balls and cubes.
3017200	3020560	Imagine perfectly sampling an entire space
3020560	3022440	with a regular grid.
3022440	3026040	This would partition space into squares or cubes
3026040	3030440	or hyper cubes with a sample in the center of each.
3031480	3034520	Now imagine around each sample,
3034520	3037760	a disc or ball or hyper ball
3037760	3042320	representing that point's region of nearness or influence.
3042320	3045520	And let's ask how much of the total volume
3045520	3049180	of that point's grid cell is actually near the sample?
3050240	3051960	The answer is a fraction
3051960	3055200	which diminishes faster than exponentially
3055200	3056640	with increasing dimension.
3057640	3059400	First think of the 2D case,
3059400	3062520	a disc or two ball with diameter one
3062520	3066560	inscribed in a square or two cube with sides of length one.
3067480	3071040	In each corner we of course have a dart shaped chunk
3071040	3073640	that isn't covered by the disc.
3073640	3075040	A trick to think about this
3075040	3077800	which I think will help in higher dimensions
3077800	3080720	is to imagine scanning a line segment
3080720	3083680	along one dimension from side to side.
3084520	3086080	At the edge of the square,
3086080	3089200	none of the segment is covered by the disc.
3089200	3090440	At the very center,
3090440	3093640	the entire segment is covered by the disc.
3093640	3097080	And then as it scans towards the other edge of the square,
3097080	3099240	the covered section begins shrinking
3099240	3101520	and then rapidly falls to zero.
3102440	3107400	The total coverage then is just the sum over that scan.
3107400	3108480	In two dimensions,
3108480	3112280	that is just the area of a disc with diameter one
3112280	3114400	and is about 79%.
3115520	3117400	Extending to three dimensions,
3117400	3119960	imagine a ball in a cube
3119960	3123760	and we scan a square from one face through the ball
3123760	3125960	to the opposite face.
3125960	3127920	As it passes the center,
3127920	3132160	we have our familiar inscribed disc in a square.
3132160	3134880	This is already missing the corner darts
3134880	3137320	and as we scan towards the edge of the cube,
3137320	3140320	the darts expand and surround the disc
3140320	3142760	as it shrinks to zero.
3142760	3144720	So by adding a third dimension,
3144720	3147280	we've lost even more coverage
3147320	3152320	and the volume of a diameter one ball is only 52%.
3153800	3156880	In fact, if we continue on to higher dimension,
3156880	3159680	the volume of a diameter one hyperball
3159680	3162720	decays faster than exponential,
3162720	3165880	factorially fast in fact.
3165880	3168760	The volume decays so fast
3168760	3171040	that even if we allowed for sampling
3171040	3175560	with the best possible densely packed balls,
3175600	3178800	theoretical work on hypersphere packing
3178800	3181240	tells us that the volume occupied
3181240	3184080	by optimally packed hyperballs
3184080	3187760	would still decay at least exponentially
3187760	3190160	with increasing dimensions.
3190160	3192360	This is the curse.
3193280	3195400	As dimensionality grows,
3195400	3198680	space expands exponentially,
3198680	3201240	points grow further apart
3201240	3204820	and the volume near each point vanishes.
3206520	3209040	In this episode, our guests argue this curse
3209040	3212680	dunes traditional concepts of interpolation,
3212680	3215560	even if we allow for the high dimensional
3215560	3219360	transformative power of deep neural networks.
3219360	3221680	Yeah, so the course of dimensionality,
3221680	3226680	it refers generally to the inability of algorithms
3230160	3233080	to keep certifying certain performance
3233080	3234840	as the data becomes more complex
3234840	3236360	and data becoming more complex here
3236360	3238960	means that you have more and more dimensions,
3238960	3240440	more and more pixels.
3240440	3245440	And so this inability of like scaling,
3245720	3247240	basically it's like it really says
3247240	3249880	that if I scale up the input,
3249880	3251560	my algorithm is gonna have more and more trouble
3251560	3252760	to keep the pace.
3252760	3256960	And so this curse can take different flavors, right?
3256960	3260640	So this curse might have like a statistical reason
3260680	3265480	in the sense that as I make my input space bigger,
3266640	3269720	there would be many, many, many much exponentially more
3270800	3273040	functions, real functions out there
3273040	3274520	that would explain the training set
3274520	3277520	that would basically pass through the training points.
3277520	3280120	And so the more dimensions I add,
3280120	3283480	the more uncertainty I have about the true function, right?
3283480	3285680	So I would need more and more training samples
3285680	3287040	to keep the pace.
3287040	3290240	This curse can also be from the approximation side, right?
3290240	3292280	So in the sense that the number of neurons
3292280	3295440	that I'm considering to approximate my target function,
3295440	3297840	I might need to keep adding more and more neurons
3297840	3300840	at the rate that is exponentially in dimension.
3300840	3304160	And the curse can also be from the computational side, right?
3304160	3307040	The sense that if I keep adding parameters
3307040	3309520	and parameters to my training model,
3309520	3313880	I might have to optimize to solve an optimization problem
3313880	3316280	that becomes exponentially harder.
3316280	3319200	And so you can see that you are basically bombarded
3319200	3321480	by three different, by all angles.
3321480	3325160	And so an algorithm like here in the context
3325160	3328280	of statistical learning or learning theory, if you want,
3328280	3330960	having a kind of a theorem that would say,
3330960	3333760	yes, I can promise you that you can learn,
3333760	3336320	you need to actually solve these three problems at once, right?
3336320	3338680	You need to be able to say that in their condition
3338680	3340760	that you're studying, you have an algorithm
3340760	3342760	that it does not suffer from approximation
3342760	3345000	nor statistical nor computational crisis.
3346000	3348160	So as you can imagine, it's very hard, right?
3350120	3353200	Today is a big day here at Machine Learning Street Talk.
3353200	3355200	We have invited one of the Godfathers
3355200	3356920	of deep learning on the show,
3356920	3359920	none other than Professor Yan Le-Kun.
3359920	3363720	Machine learning royalty, he has met the chiefs AI scientists
3363720	3365200	and a Turing Award winner,
3365200	3368120	which I don't have to tell you is a big deal.
3368120	3371720	Wiredly recognized as the Nobel Prize of Computing.
3371720	3376720	Yan Le-Kun was born in the suburbs of Gay Pari in the 1960s.
3377640	3379920	He received his PhD in computer science
3379920	3384000	from the modern day Sir Vaughan University in 1987,
3384000	3387760	during which he proposed an early form of back propagation,
3387760	3389280	which is of course the backbone
3389280	3392000	for training all neural networks.
3392000	3393440	Whilst he was at Bell Labs,
3393440	3396080	he invented convolutional neural networks,
3396080	3397400	which again are the backbone
3397400	3401080	of most major deep learning architectures in production today.
3402440	3405520	He has been at New York University since 2003,
3405520	3407920	where he is still the professor of computer science
3407920	3409360	and neural science.
3409360	3412160	Apart from being perhaps the most known researcher
3412160	3414000	and main ambassador for deep learning,
3414000	3416280	he has championed self-supervised learning
3416280	3418200	and energy-based models.
3418200	3423040	In 2013, he created the hugely prestigious ICLR conference
3423040	3424720	with Yoshua Ben-Gio.
3427240	3429920	A lover of the green screen IC.
3429920	3430920	Well, normally,
3431680	3437200	when I use Zoom, I put a substituted background.
3437200	3440960	Well, Tim could do it for you and post, if you want.
3440960	3441920	If you want.
3441920	3443480	Well, it's never a very good cut-out.
3443480	3445960	I did hack together some Python code to do it,
3445960	3448480	but there's no substitute for the real thing.
3449360	3451240	Yeah, I attempted to do this.
3451240	3453760	I'm running Linux, so I attempted to do this
3453760	3457240	by making a fake video driver,
3457240	3459400	but introduces a little bit of delay, so.
3461920	3464360	Professor Lacune, it's an absolute honour
3464360	3465520	to have you on the show.
3465520	3469920	When I first started MLST with Yanakin and Keith and Connor,
3469920	3471920	we were discussing how long it might take
3471920	3475280	to finally get the main man himself on the show.
3475280	3478480	You've been a huge inspiration to all of us here on MLST
3478480	3481120	and also you've inspired millions around the world
3481120	3484760	to embark on successful careers in data science
3484760	3486920	and to dream about what might be possible
3486920	3488840	with artificial intelligence.
3489000	3491080	Now, we've read your recent paper,
3491080	3494480	Learning in High Dimensions Always Amounts to Extrapolation,
3494480	3496440	which you co-authored with Randall,
3496440	3498960	Belastriro and Jerome Pesente.
3498960	3500760	Let's get straight into it.
3500760	3503480	So, we were wondering,
3503480	3505440	why did you write this paper, basically?
3507640	3511680	We were thinking whether interpolation and extrapolation
3511680	3512880	is a useful dichotomy.
3512880	3514320	I mean, at the end of the day,
3514320	3516600	we measure performance of learning methods
3516600	3519000	with accepted metrics of predicted performance,
3519000	3520160	such as accuracy.
3520160	3522880	So, suppose everyone adopts your linear convention,
3522880	3525280	which is a convex whole membership,
3525280	3527400	and concludes that high-dimensional learning
3527400	3529360	is always extrapolation.
3529360	3530200	How is that useful?
3530200	3531520	What do we do with this knowledge?
3531520	3534200	And vice versa, suppose that everyone adopts
3534200	3535720	a different definition
3535720	3538840	and concludes that learning is always interpolation.
3538840	3540640	What difference would that make
3540640	3542760	in machine learning research and practice?
3542760	3545880	So, in sum, why is it important to distinguish
3545920	3547840	whether we're predicting by interpolation or extrapolation?
3547840	3550960	Okay, very interesting question.
3550960	3552320	So, first of all,
3552320	3556360	this may be the first time you have me as an interviewee,
3556360	3557920	but I've watched a bunch of your videos.
3557920	3560000	So, you've had me as an audience, at least.
3561480	3563520	Which I find really interesting, actually.
3563520	3566760	So, the answer to your question is,
3566760	3568240	the whole point of the paper is to show
3568240	3571560	that this notion of interpolation versus extrapolation
3571560	3574360	is not useful, essentially.
3575240	3577960	That, you know, there's been a lot of people
3579000	3582440	who've been sort of saying there's a limitation
3582440	3584200	to deep learning, let's say,
3584200	3586600	or machine learning more generally,
3586600	3588520	because it's obvious that those things
3588520	3589720	basically do curve fitting,
3589720	3591320	and that only works for interpolation
3591320	3593160	and not for extrapolation.
3593160	3598160	And that kind of dismissal always sounded wrong to me
3599280	3601200	because in high-dimension,
3601200	3602920	your geometry in high-dimension is very different
3602920	3606480	from the intuition that we form
3606480	3609520	with curve fitting and low-dimension, right?
3609520	3612040	So, part of the motivation for this paper
3612040	3614360	is it was to kind of, you know,
3614360	3618640	perhaps help some people gain some intuition
3618640	3621520	about what really is taking place
3621520	3624040	in machine learning and high-dimension.
3624040	3626400	And also kind of, you know,
3626400	3628280	dispel the myth, essentially,
3628280	3631520	that machine learning and deep learning, in particular,
3631520	3633120	only does interpolation.
3633120	3634120	Of course, it depends a lot
3634120	3636160	on your definition of interpolation.
3636160	3637800	So here we adopted definition,
3637800	3641920	which is, you know, an obvious and simple generalization
3641920	3645640	of interpolation in low-dimension,
3645640	3646760	which is that, you know,
3646760	3649800	you interpolate when a point is in between the points
3649800	3650640	you already know.
3652120	3655320	And the generalization of this in high-dimension
3655320	3658400	is, you know, you interpolate when a new point
3658400	3660120	is inside the convex hull of the points
3660120	3663440	that you already know.
3663440	3667040	Now, what you have to realize, of course,
3667040	3668000	is that in high-dimension,
3668000	3670520	the volume of that space is actually tiny
3670520	3673520	compared to the overall volume of space
3673520	3675960	that would be filled, you know, in high-dimension.
3675960	3679720	And so that's kind of the intuition behind this paper
3679720	3683200	that, you know, any new point,
3683200	3687160	regardless of how you sample it to some extent,
3687240	3689760	any kind of reasonable ways to sample points,
3691560	3692520	you know, in high-dimension,
3692520	3695080	new points are always going to be outside the convex hull,
3695080	3696920	almost always going to be outside the convex hull
3696920	3700480	of existing points with that definition.
3700480	3701320	Interesting.
3701320	3702480	Now, there's a second part to your question.
3702480	3703320	It's that, you know,
3703320	3705280	is there a more sensible definition
3705280	3708200	of interpolation and extrapolation?
3708200	3709880	And the answer is probably yes, you know,
3709880	3711360	the paper doesn't address it,
3711360	3716360	but there are other definitions of hulls,
3716720	3718600	if you want, they're not necessarily convex hulls,
3718600	3721600	or they're not necessarily the usual type of convex hull.
3721600	3725040	So for example, there is a definition of a hull
3725040	3726280	for a cloud of points
3726280	3729880	that would be the smallest hyperboloid,
3730640	3734040	paraboloid, or ellipsoid, I should say, actually,
3735320	3737880	the smallest ellipsoid that contains all the points, right?
3737880	3741000	So some points are going to be on the surface
3741000	3743880	of the ellipsoid, but most of them are going to be inside.
3743880	3746480	And for this type of,
3746480	3749600	and now your definition of interpolation is that,
3749600	3751880	is a new point likely to be inside the ellipsoid
3751880	3753520	of a previous point or outside?
3754720	3758000	And the answer to that is probably very different
3758000	3759880	from the one in the paper,
3759880	3761880	in the sense that it's very likely
3761880	3763600	for a lot of natural data,
3763600	3766320	new points are likely to be inside
3766320	3769000	the containing ellipsoid.
3769000	3771920	So it very much depends on what you mean,
3771920	3773920	but it's just that the notion of interpolation
3773920	3776920	in high dimensional space or intuition
3776920	3779680	are kind of biased toward low dimension
3779680	3782280	and we have to be very careful what we say.
3783240	3785440	So that was the main thing.
3785440	3790440	And we have a bunch of tons of mathematicians
3790880	3792960	that worked on these questions for many years
3792960	3795520	and there's a whole bunch of theorems about this
3795520	3797800	that we survey in the paper.
3798800	3800160	Very interesting.
3800160	3802640	Well, we'll dig more into that in a second,
3802640	3805560	but folks like Gary Marcus make the case
3805560	3809520	that deep learning models don't reason and only interpolate.
3809520	3811800	Now, I know you disagree vehemently,
3811800	3815000	but my intuition is that reasoning and extrapolation
3815000	3817240	are somewhat synonymous.
3817240	3820800	Is it, by arguing that deep learning models extrapolate,
3820800	3824160	are you kind of making the argument that they reason as well?
3824160	3825720	Because that would make a strong case
3825720	3827280	that deep learning models could scout
3827280	3828400	artificial general intelligence.
3828400	3830880	Okay, so no, the short answer is no.
3831960	3836960	But there are important questions in there.
3837960	3840560	First of all, what's our definition of reasoning?
3841960	3845280	What is the process by which we elaborate models
3845280	3847600	and is there a qualitative difference
3847600	3852520	between a models that merely performs curve feeding
3852520	3854920	as we normally know it
3854920	3857720	and a model that has a,
3857720	3861680	let's say to adopt a terminology that others have proposed
3862840	3865840	that models that establish sort of a causal model
3865840	3867520	of the data you're observing,
3868960	3871000	which can be the basis for reasoning
3871000	3872840	and things like that, right?
3874480	3876840	And the answer to this is probably no.
3876840	3878560	There is a difference, of course,
3878560	3881080	but is it an essential qualitative difference?
3881080	3882840	I'm not entirely sure.
3882880	3885440	And then there is the argument,
3885440	3886920	if there is a qualitative difference,
3886920	3888120	which I'm not sure about,
3889320	3893200	would this qualitative difference be in the form of
3893200	3895640	fundamentally different things from deep learning,
3895640	3898920	things that are like discrete symbolic reasoning
3898920	3900720	or things of that type?
3900720	3902920	And to that, my answer is clearly no.
3902920	3904680	I do not believe that's the case.
3905760	3908320	So I think reasoning is certainly,
3908320	3909800	I always list in my talks,
3910800	3913800	the ability, giving the ability to learning machines
3913800	3916600	to reason is one of the main challenges
3916600	3921600	of the next decade or perhaps couple of decades in AI.
3921880	3924920	So I'm clearly aware of the fact
3924920	3926840	that they don't do this very well at the moment.
3926840	3928360	The big question I think is,
3928360	3931920	how do we get machines to reason
3931920	3934240	in ways that are compatible with deep learning?
3934240	3936320	Because most of the criticism
3936320	3939480	that I've heard from Gary Marcus and several others
3939520	3941400	towards deep learning is not a criticism
3941400	3942440	towards deep learning.
3942440	3944840	It's a criticism towards supervised learning.
3944840	3946680	And I agree with them.
3946680	3948880	Supervised learning sucks.
3948880	3951440	I mean, it's very limited in the sense that
3951440	3954720	you can train machines to do very specific tasks.
3954720	3957200	And because they're trying to do very specific tasks,
3957200	3959160	they're going to use all the biases that are in the data
3959160	3961280	to do that task.
3961280	3964080	And if you try to get outside of that task,
3964080	3965520	they're not gonna perform very well.
3965520	3967480	That's the limitation of supervised learning.
3967480	3970640	It has absolutely nothing to do with deep learning.
3971560	3975200	So regardless of which learning techniques you're gonna use,
3975200	3976640	you're gonna have that problem.
3976640	3978600	It's a problem with supervised learning.
3978600	3981920	So I take exception with the confusion
3981920	3983400	between deep learning and supervised learning.
3983400	3985040	Now, of course today, most of supervised learning
3985040	3986200	is deep learning,
3986200	3989000	but it's the limitation of supervised learning.
3989000	3992040	And as you probably know,
3992040	3996920	I've been a very strong advocate of self-supervised learning,
3996920	4001160	sort of moving away from task-specific supervised learning
4001160	4003080	towards more kind of generic learning
4003080	4005920	followed by specialization,
4005920	4008760	using supervised or reinforcement learning.
4008760	4013680	And in that, I've kind of followed the path of Jeff Hinton,
4013680	4018680	who's been basically advocating for this for 40 years now.
4020520	4023120	And for me, it's less time.
4023120	4026040	I disagreed with him originally and changed my mind
4026720	4028120	about 20 years ago.
4028120	4033120	So that's the story, really.
4033480	4036400	It's awesome that you have the ability to change your mind
4036400	4038720	and admit that this is a good thing.
4038720	4040840	We run into so many people that are just,
4040840	4043040	they don't wanna change their mind no matter what.
4043040	4046440	It's seen as a bad thing, which it isn't at all.
4046440	4050040	No, I mean, I think the essence of being a scientist
4050040	4055040	is to be able to change your mind in the face of evidence.
4055680	4059760	You cannot be a scientist if you have preconceived ideas.
4059760	4062600	On the other hand, I've also been known
4063640	4067120	to hold very tight to ideas that I thought were true
4067120	4071600	in the face of considerable differing opinion
4071600	4072840	from my dear colleague.
4072840	4077840	So it also helps to have deeply held convictions sometimes.
4080040	4084280	So to this notion of interpolation,
4084320	4087440	I think we've also, and you mentioned this, right?
4087440	4088920	It depends on your definition.
4088920	4092440	And we've also had a little bit of the feeling
4092440	4095520	that people might be talking past one another
4095520	4097920	when they criticize, can it interpolate?
4097920	4099680	Can't it interpolate?
4099680	4101240	And we've come up with this example
4101240	4104440	of I give a classifier a dog.
4104440	4107520	And that dog is like the most doggy dog I've ever seen.
4107520	4109440	Like it's like such a dog.
4109440	4112880	It's more dog than any dog I had in the training dataset.
4112920	4117480	So clearly that dog is like outside of the training distribution,
4117480	4120200	outside of the convex hull in any space,
4120200	4124840	like be that the original space, be that the latent space.
4124840	4128760	It's like, all that matters is that it's at the correct side
4128760	4131360	of the classifying hyperplane.
4131360	4135440	So that would not be contained on sort of in the convex hull.
4135440	4138760	It would not be contained in the smallest ellipsoid
4138760	4139760	and so on.
4139760	4144040	So do you think when people talk about interpolation,
4144040	4149160	they might be talking about something maybe different?
4149160	4150720	Because you can also make the example
4150720	4153600	if I take the convex hull of all the training data points
4153600	4155800	and I take a point in the middle of it,
4155800	4158560	it's the neural network going to be pretty bad at it, right?
4158560	4162120	That's going to be like a messy blur of pixels
4162120	4165280	and it's not going to be very, very good at it.
4165280	4172240	So could you maybe make the strongest argument you could
4172240	4177240	for people saying neural networks just interpolate,
4177240	4181400	but what notion of interpolation would you substitute?
4181400	4185080	Would it be like, oh, they just nearest neighbor classify
4185080	4191360	or if you had to give the best shot at sort of doing the,
4191360	4194640	oh, neural networks just do something, what would it be?
4195520	4197160	I wouldn't give any answer of this type
4197160	4202640	because the answer would very heavily depend on the architecture.
4202640	4209800	So for example, if most of the layers in a neural net,
4209800	4212040	I mean, you can use weighted sums and sigmoids
4212040	4213480	or weighted sums and values, right?
4213480	4219040	And that effectively performs a classification separating
4219040	4224680	the input into kind of half, two halves with the hyperplanes.
4224680	4226680	But you could also use a Euclidean distance unit.
4226680	4228600	So Euclidean distance unit computes the distance
4228600	4231640	of the input vector to a weight vector,
4231640	4233680	which is not a weight vector anymore,
4233680	4235840	and then passes it through some sort of decreasing function
4235840	4237640	like an exponential.
4237640	4241040	And what that gives you is a Gaussian bump in input space
4241040	4243840	where the activity will be high
4243840	4247640	if the input is close to the vector and low if it's not.
4247640	4250520	When you have an attention layer,
4250520	4254640	where you have a whole bunch of those kind of vector comparison
4254640	4256960	where the vectors are normalized and then you do a softmax,
4256960	4260640	you're basically doing sort of a multinomial version of this.
4260640	4263640	And this is using transformers, right?
4263640	4265880	And in all kinds of architectures these days.
4265880	4269760	So those things that compare vectors with each other
4269760	4273200	and only react to the two vectors nearby
4273200	4276600	do a sort of glorified nearest neighbor
4276600	4277960	with some interpolation.
4277960	4285520	By the way, most kernel-based methods do this, right?
4285520	4290120	Kernel methods basically are one layer of such PRS comparisons.
4290120	4293240	I mean comparisons of the input with the training samples.
4293240	4295120	And then you pass that through some function,
4295120	4296960	some response function.
4296960	4300480	So Gaussian SVMs, for example, are the perfect example of this.
4300480	4302920	And then you take those cores and you compute a linear combination.
4302920	4306800	That's glorified interpolating nearest neighbor.
4306800	4308840	To some extent, transformers do this as well.
4308840	4312880	Transformers are the basic module of a transformer.
4312880	4317800	It's basically an associative memory that compares the incoming vector
4317800	4320880	to a bunch of keys and then gives you an answer
4320880	4323360	that is some linear combination of values, right?
4323360	4327640	So that is a good idea.
4327640	4329480	It learns fast.
4329480	4332640	There used to be a whole series of models that were now forgotten,
4332640	4335960	that are now forgotten in the 90s called RBF networks.
4335960	4338480	So an RBF network was basically a two-layer neural net.
4338480	4340480	Well, the first layer was very much like an SVM.
4340480	4343520	This was before SVMs that, again,
4343520	4346320	had riddle-based functions, responses, right?
4346320	4350000	Comparing an input to vectors and passing it to an exponential
4350000	4351960	or something like this.
4351960	4353840	And then you would initialize the first layer.
4353840	4354960	You could train it with backprop,
4354960	4356360	but it would get stuck in local minima.
4356360	4358160	So that wasn't a good idea.
4358160	4362600	You had to initialize the first layer with something like k-means
4362600	4365720	or mixer or Gaussian or something like that.
4365720	4368680	And then you could either just train the second layer
4368680	4372040	or fine-tune the entire thing with backprop.
4372040	4373040	And that worked pretty well.
4373040	4377280	It was actually a fairly fast learner.
4377280	4379840	They were faster than the neural nets.
4379840	4381880	So those things, for those things,
4381880	4384280	the answer to your question is of one type.
4384280	4388200	They're basically doing interpolation with kernels.
4388200	4393280	And it's very much like a smooth version of nearest neighbors.
4393280	4395240	But then for classical neural nets,
4395240	4397240	where you have either a hyperboleic tantrum,
4397240	4399800	nonlinearity, or a value, or something of that type,
4399800	4403080	something with a kink in it, or two kinks,
4403080	4404480	the answer is different.
4404480	4407800	There it's a whole cone of response
4407800	4412600	that will produce a positive response versus not
4412600	4414720	if you take a combination of units, right?
4414720	4419520	So does it make sense to talk about interpolation
4419520	4420520	in that kind of geometry?
4420520	4422000	I'm not sure.
4422000	4424640	I think people should just not use
4424640	4428680	the word interpolation for that situation.
4428680	4434720	But the fact that kind of things in a neural net,
4434720	4436120	the response of a neural net actually
4436120	4441320	are kind of half-spaced, extend beyond the training points,
4441320	4443680	perhaps has something to do with the fact
4443680	4446120	that they do extrapolate in certain ways
4446120	4448400	that may or may not be relevant for the problem,
4448400	4451640	but they do extrapolate.
4451640	4454520	Well, so we've taught some about the interpolation
4454520	4456760	versus extrapolation versus what was in the paper,
4456760	4459680	because the paper is somewhat a rigid definition.
4459680	4461720	It's like there's this convex hull,
4461720	4464040	and if you're inside, it's interpolation.
4464040	4466840	If you're outside, it's extrapolation.
4466840	4469920	And you talked about maybe we could use a ball and ellipsoid
4469920	4472000	instead, but there's kind of a key thing there,
4472000	4475200	which is that it's going across all dimensions.
4475200	4479240	So you're inside the convex hull.
4479240	4483120	It's a necessary condition that on every single dimension,
4483120	4486200	your sample data point falls within the range
4486200	4487560	of the training data.
4487560	4489680	We could kind of go the opposite extreme
4489680	4493800	and say that you're interpolating if any dimension falls
4493800	4495840	within the training domain, or rather,
4495840	4500120	you're extrapolating only if every single dimension falls
4500120	4501360	outside the training range.
4501360	4504600	And both of those are kind of these exponential extremes.
4504600	4507120	And it would seem like the truth is maybe somewhere
4507120	4509520	in between, like there's a subset of dimensions
4509520	4513400	that might be salient for any particular data point.
4513400	4516120	So why are we kind of using this very exponential extreme
4516120	4517560	definition?
4517560	4518880	It was still exponential.
4518880	4521080	Even if you, you know, you can try
4521080	4523720	to divide the set of dimensions into the ones that are useful
4523720	4526560	and the ones that are just going to use parameters that
4526560	4531480	are useless, but that are not relevant for the task at hand.
4531480	4533400	But first of all, that task is very difficult.
4533400	4535880	I mean, basically, the entire machine learning problem
4535920	4538320	is exactly that, trying to figure out
4538320	4541040	what information in the input is relevant for the task
4541040	4544000	and what part should basically be considered
4544000	4546920	as noise or useless parameters.
4546920	4549160	So solving that problem is solving the machine learning
4549160	4550360	problem, first of all.
4550360	4554000	Second of all, what we show in the paper is that regardless,
4554000	4558120	so the experiment that Randall did, as a matter of fact,
4558120	4564200	is train a ResNet 18 or 50 or whatever on a ResNet
4564200	4565680	or CIFAR or MNIST.
4565680	4568240	And then take the output representation
4568240	4576120	and see whether this sort of exponential growth of number
4576120	4579440	of data points to stay within the interpolation regime
4579440	4580040	still exists.
4580040	4581800	And it still exists.
4581800	4584400	It's just that the dimension now, instead of being
4584400	4588120	the input dimension of the data, which may be very large,
4588120	4590240	is the dimension of the embedding.
4590240	4593320	But as soon as the dimension of the embedding
4593320	4597400	is larger than 20 or so, the number of training samples
4597400	4601680	you would need to stay within the interpolation regime
4601680	4604520	is already going to be very large.
4604520	4607400	2 to the 20 is a large number.
4607400	4610760	So there is another experiment that
4610760	4612720	shows that if your entire data set is
4612720	4615440	contained within a linear subspace,
4615440	4619520	so the ambient space may be of dimension 100,
4619520	4623040	but the entire data set is within a linear subspace
4623040	4625360	of dimension 4, then what matters
4625360	4629120	is the dimension 4, not the dimension 100.
4629120	4636920	So automatically, that convex hull process, what
4636920	4639000	matters to it is not the dimension of the input space,
4639000	4642080	it's the dimension of the linear subspace that
4642080	4644760	contains all the points.
4644760	4646800	Right, but I think what I'm saying though
4646800	4651000	is that you can invert the definition so that whether or not
4651040	4653840	you're extrapolating just becomes 1 minus,
4653840	4656160	whether or not you're interpolating, or vice versa.
4656160	4658920	And so you can wind up with an equally extreme definition
4658920	4662760	that concludes everything is interpolation.
4662760	4666160	So I'm just wondering, it seems like there should be a balance.
4666160	4668000	And this kind of gets to what you said earlier.
4668000	4673120	Is there a more useful definition of interpolation
4673120	4676800	versus extrapolation that could have some utility
4676800	4677840	for machine learning?
4677840	4680880	I mean, it's not clear you would get much out of it.
4681320	4683240	I give you a potential candidate,
4683240	4687760	which is whether the points are contained in a ellipsoid that
4687760	4690080	contains all the points, you could make it a sphere.
4690080	4693360	A sphere would be, of course, bigger volume
4693360	4695360	because the data doesn't necessarily
4695360	4698280	have the same radius in all dimensions.
4698280	4702920	But I think the result would be fairly similar for both,
4702920	4708560	for both definitions that in that case,
4708560	4710120	things would be mostly interpolation.
4710120	4712800	But that would be kind of a weird definition
4712800	4717280	of interpolation in the sense that it
4717280	4720480	would rely on a false intuition about high dimensional
4720480	4722200	geometry.
4722200	4726640	OK, calling this interpolation basically
4726640	4729200	would mean that you have a completely wrong idea about how
4729200	4730840	things behave in high dimension, right,
4730840	4734200	about geometry in high dimension.
4734200	4735760	I'm trying to get some intuition about this
4735760	4739360	because we spoke to Professor Bronstein and his friends,
4739360	4741920	including Joanne Bruner, and they're
4741920	4746120	talking about geometric learning and their approach
4746120	4747920	to defeating the curse of dimensionality
4747920	4751320	is finding geometric priors to reduce the hypothesis space,
4751320	4752400	which is quite interesting.
4752400	4756000	A lot of this is about our intuitions of interpolation.
4756000	4758640	Because if I take an autoencoder and I train it on MNIST
4758640	4761360	and I start interpolating between the train
4761360	4764360	or the test examples, it is learning this continuous
4764360	4765560	geometric morphing.
4765560	4768840	And that is what people's intuition is about interpolation.
4768840	4772280	I know you would say that's extrapolation, right?
4772280	4775080	Yeah, no, I think it would be a type of interpolation,
4775080	4777240	but it would be some sort of geodesic interpolation,
4777240	4780120	interpolation on a manifold, right?
4780120	4782000	I mean, so certainly if you have some idea
4782000	4784720	about the structure of the data manifold,
4784720	4787080	you can interpolate within that manifold
4787080	4790480	without making big mistakes.
4790480	4793280	But then you're back to the original problem
4793280	4793800	of machine learning.
4793800	4794720	What is that manifold?
4794720	4795880	How do you learn that manifold?
4795880	4799320	That's one of the essential problems of machine learning.
4799320	4801600	And learning the structure of a data manifold
4801600	4806240	is a much more complex problem than learning a task
4806240	4808920	of classifying objects on that manifold.
4808920	4811800	For example, classifying points on that manifold.
4811800	4817200	So there is this old adage by which
4817200	4822320	I used to be a big fan of, which is why I was
4822320	4824200	disagreeing with Jeff Hinton about the usefulness
4824200	4826480	of unsupervised learning.
4826480	4832200	Because if you have a task at hand for which you have data
4832200	4835200	that you can use to train a supervised system,
4835200	4841920	why would you go to the trouble of pre-training a system
4841920	4846200	in unsupervised mode knowing that the unsupervised learning
4846200	4849840	problem is considerably more complicated both from every
4849840	4852920	aspect you can think of, certainly
4852920	4855320	from the theoretical point of view.
4855320	4860040	Vladimir Vapnik actually has kind of a similar opinion.
4860040	4863080	One of the few things that he and I agree on,
4863080	4865840	or agreed on, at least, which is why would you
4865840	4868280	want to solve a more complex problem than you have to?
4868280	4870520	But of course, that forgets the fact
4870520	4872680	that you don't want to solve a single problem.
4872680	4876880	You want to take advantage of multiple problems
4876880	4883920	and whatever data you have available at your disposal
4883920	4886800	to prepare for learning a task.
4886800	4894520	And we have access to considerably more unlabeled data
4894520	4897440	than we have access to labeled data.
4897440	4902320	And therefore, why not use unlabeled data in large quantity
4902320	4904240	to pre-train very large neural nets
4904240	4906120	so that we can function them for the tasks
4906520	4907720	that we are interested in?
4907720	4909480	So that's the whole idea of self-supervised learning.
4909480	4912720	And of course, we all know that that kind of strategy has
4912720	4916160	been unbelievably successful in natural language
4916160	4920760	processing with denoising autoencoder or master
4920760	4924960	autoencoder or bird-style training of transformers
4924960	4928200	followed by a supervised phase.
4928200	4930440	That success has not yet translated
4930440	4934880	in the domain of vision, although I'm sort of predicting
4934880	4937280	that it will happen very soon.
4937280	4942200	But I mean, there's a lot of interesting avenues there
4942200	4945120	and recent progress.
4945120	4947680	And then there is the cake analogy, right?
4947680	4952720	The fact that any sample in sort of a self-supervised context
4952720	4956400	give you way more information than a supervised sample,
4956400	4958400	the label from supervised learning,
4958400	4962600	a fortiori reinforcement for the context of reinforcement.
4962600	4964760	Absolutely.
4965080	4966440	We interviewed Ishan.
4966440	4968640	So we've done a show on self-supervised learning.
4968640	4970840	We're huge fans of self-supervised learning.
4970840	4974360	And I know your vision is to get to these latent predictive
4974360	4976200	models to solve that problem.
4976200	4979800	That's something else I changed my mind on the last two years.
4979800	4982000	This may be outside the scope of this particular interview.
4982000	4985640	But yeah, there's basically two major architectures, right,
4985640	4987200	for self-supervised learning, particularly
4987200	4989040	in the context of vision.
4989040	4992840	And the main characteristic of both of them
4992840	4996920	is the fact that it can handle multimodal prediction.
4996920	4998600	So if you have a system, let's say you
4998600	5000720	want to do video prediction or something like that.
5000720	5002600	So you have a piece of a video clip.
5002600	5005640	You want to predict the next video clip.
5005640	5007080	Or you just want a machine that tells you
5007080	5010240	whether a proposed video clip continuation clip is
5010240	5011920	a good continuation of the previous one.
5011920	5013240	You don't want it to predict.
5013240	5016080	You just want it to tell you whether it's a good one.
5016080	5017120	So you have two architectures.
5017120	5020000	The first one is a latent variable predictive architecture
5020000	5022120	that predicts the next video clip.
5022120	5024080	And of course, you have to parameterize the prediction
5024080	5026560	with a latent variable, because there are multiple predictions
5026560	5028040	that are plausible.
5028040	5029840	So I used to be a big fan of that.
5029840	5031640	And about two years ago, I changed my mind.
5031640	5034680	Maybe a year and a half.
5034680	5037880	The other approach is something I played with in the early 90s,
5037880	5039800	came up with some of the early models for this.
5039800	5041880	And it's called a joint embedding architecture,
5041880	5045440	where you have the first and the second video clip both going
5045440	5047600	through an neural net.
5047600	5049200	And then what you're doing is you're
5049200	5051640	training the system so that the representation of the second
5051640	5054720	video clip is easily predictable from the representation
5054720	5056760	of the first one.
5056760	5058240	So there you're not predicting pixels.
5058240	5060680	You're predicting representations.
5060680	5062480	But you still have a system that can tell you
5062480	5063840	here is a video clip and the second one
5063840	5065520	tell me if they are compatible, if one
5065520	5067960	is a good continuation of the other.
5067960	5070720	The main reason why I stayed away from those architectures,
5070720	5074120	because I knew that you had to use, in the past,
5074120	5075680	you had to use contrastive learning.
5075680	5078400	You had to have pairs of things that are compatible,
5078400	5080480	as well as pairs of things that are incompatible.
5080480	5082280	And in high dimension, there's just too many ways
5082280	5084120	two things can be incompatible.
5084120	5086000	And so that was going to do to failure.
5086000	5089040	And I played with this back in the suit
5089040	5093520	to be called Siamese networks at a paper in 1992, 1993,
5093520	5096360	on doing signature verification using those techniques.
5096360	5099560	Jeff Hinton had a slightly different method
5099560	5102680	based on maximizing mutual information with his former
5102680	5104720	student, Sue Becker.
5104720	5106240	But then in the last two years, we've
5106240	5109400	had methods that are non-contrastive
5109800	5113560	that allows us to train those joint embedding
5113560	5114120	architectures.
5114120	5116440	So I've become a big fan of them now.
5116440	5120240	And it's because of algorithms like BYUL,
5120240	5126040	from our friends at DeepBind, like Barlow Twins, whose idea came
5126040	5130080	from Stephane Denis, who's doing a postdoc with me at FAIR.
5130080	5136360	He's now a professor at the University of Alto in Finland.
5137320	5138640	And then more recently, Vic Craig,
5138640	5143920	which is a kind of improvement, if you want,
5143920	5145880	on Barlow Twins, that is also based
5145880	5147800	on the same idea that Jeff Hinton and Sue Becker
5147800	5151040	had of maximizing a measure of mutual information between the
5151040	5154600	outputs to networks, but in a slightly different way.
5154600	5156600	So I'm really excited about those things.
5156600	5159240	And again, I change my mind all the time,
5159240	5166120	whenever a good idea seems to be overcome by a better one.
5166160	5171320	So in this whole space of maybe interpolation, extrapolation,
5171320	5173480	but also data manifolds, and so on,
5173480	5176840	what's your view on things like data augmentation,
5176840	5181360	training and simulation, and domain adaptation, and so on?
5181360	5185000	Because it could be argued that these things increase
5185000	5188480	the convex hull of the training data.
5188480	5191800	They sort of make the distribution broader.
5191800	5195040	Or is that just also out of question?
5195040	5195560	Not much.
5195560	5197040	I think data augmentation does not
5197040	5202720	increase the volume of the data point cloud,
5202720	5204520	if you want, very much.
5204520	5208280	Because those augmentations are generally fairly local,
5208280	5210480	in dimensions that are already explored.
5210480	5213480	So it may increase it a little bit,
5213480	5216960	but not significantly, I think.
5216960	5218800	Obviously, data augmentation is very useful.
5218800	5221880	I mean, we've used this for decades,
5221880	5224560	so it's not a new phenomenon either.
5224560	5227120	There's a lot of ideas along those lines, right?
5227120	5232200	So it's the idea where you give two examples,
5232200	5235080	and you have two examples in your training set,
5235080	5238200	and you actually do an interpolation in input space
5238200	5240480	to generate a fake example that's in between the two,
5240480	5242600	and you try to produce the intermediate target
5242600	5244840	between the two original examples.
5244840	5245560	Mix up.
5245560	5246480	Mix up.
5246480	5247000	Mix up.
5247000	5248080	Yeah.
5248080	5248600	There is that.
5248600	5249720	There is distillation.
5249720	5252040	There is various techniques like this
5252040	5255200	are basically implicitly kind of ways
5255200	5257400	to fill in the space between samples, right,
5257400	5261160	with other kind of fake samples, or virtual samples,
5261160	5262360	if you want.
5262360	5266600	There was a paper by Patrick Simard and me,
5266600	5270400	and John Denker many years ago, when we were all at Bell Labs,
5270400	5271760	on something called tangent prop.
5271760	5275720	So the idea of tangent prop was kind of somewhat similar.
5275720	5278640	The idea was you take a training sample,
5278640	5281080	and you're going to be able to distort that training
5281080	5282760	sample in several ways.
5282760	5285800	You could generate points by data augmentation.
5285800	5288360	But the other thing you can do is just figure out
5288360	5295840	the plane in which those augmentations live, OK?
5295840	5298560	And that plane is going to be a tangent plane
5298560	5301280	to the data manifold.
5301280	5305800	What you want is your, for a given class, for example, right?
5305800	5308520	So what you want is your input output function
5308520	5310280	that the neural net learns to be invariant
5310280	5313040	to those little distortions.
5313040	5315280	And you can do this by just augmenting the data,
5315280	5317640	or you can do this by explicitly having a regularization
5317640	5320920	term that says the overall derivative of the function
5320920	5325560	in the direction of the spending vectors of that plane
5325560	5328440	should be 0, or should be whatever it is that you want it
5328440	5331280	to be, but 0 is a good target for this.
5331280	5333240	So that's called tangent prop.
5333240	5337560	And you can think of it as a regularizer that says,
5337560	5339920	I don't just want my input output function
5339920	5341200	to have this value at this point.
5341200	5343320	I also want its derivative to be 0
5343320	5346400	when I change the input in those directions.
5346400	5350400	That's another indirect way of doing data augmentation
5350400	5353800	without doing data augmentation, essentially.
5353800	5356040	And I think there's a lot of those ideas
5356040	5359960	that are very useful, certainly.
5359960	5365680	So I think there seems to be a spectrum coming back
5365680	5367320	to this interpolation, extrapolation.
5367320	5369520	You said yourself you don't believe neural networks
5369520	5372440	can do something like discrete, abstractive reasoning,
5372440	5373120	and so on.
5373120	5374720	No, no, no, I didn't say that.
5374720	5381080	Or I said, we need to do work for them to be able to do that.
5381080	5384560	But I have no doubt that eventually they will.
5384560	5386480	I mean, there's a lot of work in this area already.
5386480	5389680	And I'll give you some more specific examples if you want.
5389680	5393120	I mean, that was actually kind of the nature of my question.
5393120	5396400	Where do you think there is this spectrum of what
5396400	5399560	people think neural networks are doing now
5399560	5401480	or will be able to do later?
5401480	5404880	Where do you think the actual biggest disagreement
5404880	5408320	between the community right now lies?
5408320	5411920	And what can we do to, what experiments, what evidence
5411920	5415000	can we gather to solve these disagreements?
5415000	5415320	Right.
5415320	5420600	So I think we shouldn't talk too much about neural networks
5420600	5425200	because people have a relatively narrow picture of what
5425200	5426640	a neural network is, right?
5426640	5428600	It's a bunch of layers of neurons
5428600	5430960	that perform wetted sums and pass a result
5430960	5432400	through a nonlinear function.
5432400	5433920	And there's a fixed number of layers.
5433920	5438640	Maybe they can be recurrent, and you produce an output.
5438640	5441760	That's a very restricted view of what deep learning systems
5441760	5442920	can do.
5442920	5449720	So let me take an example of what reasoning might mean.
5449720	5452840	Reasoning might be seen at least one particular type of reasoning
5452840	5458000	might be seen as a minimization of an energy function,
5458000	5460240	not with respect to parameters in a neural net,
5460240	5462680	but with respect to latent variables.
5462680	5465520	And a lot of systems that are in use today do that.
5465520	5468240	So a lot of speech recognition systems, for example,
5468240	5472720	have a decoder on the output, which essentially given
5472720	5479920	a list of scores for what a particular segment of speech
5479920	5481800	could be in terms of what sound it could be,
5481920	5485280	or what syllable, or whatever, the decoder basically
5485280	5489080	figures out what is the best interpretation of that sequence
5489080	5495000	of sound that makes it a legal sentence in the dictionary.
5495000	5499320	Techniques like this have been used for 25 years now
5499320	5502160	in the context of speech recognition and handwriting
5502160	5505040	recognition, even before neural nets were used for those things.
5505040	5511360	So back in the old days of hidden Markov models.
5511400	5515640	And those techniques have been really developed.
5515640	5518600	Now, if you think about what those techniques do,
5518600	5521000	first of all, all the operations that are done in those systems
5521000	5521640	are differentiable.
5521640	5524000	You can backpropagate gradient through an operation
5524000	5527520	that will figure out the shortest path in a graph
5527520	5529360	using dynamic programming.
5529360	5531040	The first paper on this was in 1991
5531040	5533040	by my friend Leon Boutou and Xavier de Leoncourt.
5533040	5537360	This is not recent stuff that we're trying to do speech recognition.
5537360	5540280	You can backpropagate gradient through a lot of different modules.
5540320	5545040	What those things do is that they infer a latent variable
5545040	5547120	by basically doing energy minimization.
5547120	5548520	Finding the shortest path in a graph
5548520	5552440	is a form of energy minimization.
5552440	5554800	And so you can have, in a deep learning system,
5554800	5556920	you can have a module that has a latent variable.
5556920	5559160	And what this module will do is figure out
5559160	5561840	a value of the latent variable that minimizes some energy
5561840	5562480	function.
5562480	5563720	It could be a prediction error.
5563720	5565160	It could be something else.
5565160	5568520	It could be regularizers in it, et cetera.
5568520	5571080	But it basically performs a minimization.
5571080	5577240	And that type of operation basically
5577240	5582400	can be used to, like, you can formulate
5582400	5586720	most types of reasoning in that form.
5586720	5590080	Now, it's not necessarily in a continuous differentiable space,
5590080	5592640	but almost all reasoning, even in classical AI,
5592640	5596200	can be formulated in terms of optimizing some sort of function.
5596200	5597640	Like, you won't do a SAT problem, right?
5597640	5600360	If you have a collection of Boolean formula,
5600360	5602680	you want to find a combination of variables
5602680	5603920	that satisfy this formula.
5603920	5605040	It's an optimization problem.
5605040	5606280	It's combinatorial optimization,
5606280	5608040	but it's an optimization problem.
5608040	5608920	You want to do planning.
5608920	5611560	So planning is the best example.
5611560	5616760	So you can do a planning where the thing you're controlling
5616760	5619440	as discrete actions and discrete states,
5619440	5621240	and you have to use dynamic programming.
5621240	5624560	You can back propagate gradient through a system that
5624560	5625680	does that.
5625680	5629040	But more classical planning is in continuous space.
5629040	5632080	So planning for, like, planning the trajectory of an arm
5632080	5635600	for a robot, planning the trajectory of a rocket,
5635600	5638760	what you have is a differentiable model,
5638760	5641800	dynamical model of what is the state of the system
5641800	5644000	you're trying to control at time t plus 1
5644000	5646280	as a function of the state at time t
5646280	5647720	and as a function of the action you take.
5647720	5649920	And perhaps as a function of some random variable,
5649920	5653040	you can't measure from the environment
5653040	5656040	to something like that to make it non-deterministic.
5656040	5658880	Now, you can enroll that model.
5658880	5663360	So start with time equals 0, and then make a hypothesis
5663360	5665560	about a sequence of action, and then
5665560	5669760	apply your predictive model of the next state of the system.
5669760	5671920	And then at the end, you can measure some cost function.
5671920	5676520	Is my rocket docking with a space station or it's far away?
5676520	5678360	And how much fuel have I consumed?
5678360	5680280	Or whatever, right?
5680280	5684400	Or have I reached the goal of the arm,
5684400	5686920	avoiding all the obstacles, right?
5686920	5690000	So what you can do now is an inference process
5690000	5692200	which consists in figuring out what is the sequence of action
5692200	5695080	I should take to minimize this cost function according
5695080	5696960	to my dynamical model.
5696960	5698640	You can do this by gradient descent.
5698640	5702320	And basically, that's what the Kitty-Bison algorithm,
5702320	5705400	in optimal control, that goes back to 1962.
5705400	5707480	And it consists in basically doing backprop through time.
5707480	5708680	It's as simple as that, right?
5708720	5710240	So you do backprop through time.
5710240	5714120	So in effect, optimal control theorist invented backprop
5714120	5715280	back in the early 60s.
5715280	5717560	But nobody realized you could use this for machine
5717560	5720040	learning until the V-80s, essentially.
5720040	5722240	Or the 70s.
5722240	5724600	Just to jump in there, because there's a little caveat there,
5724600	5726840	which is that you can do that at backprop
5726840	5729560	if you have a fixed number of steps.
5729560	5731960	Like what backprop can't handle is the case
5731960	5734120	where I want some expandable number of steps.
5734120	5737520	We have no algorithms that can currently optimize.
5737520	5743120	For example, neural networks with a variable number of layers.
5743120	5744720	Like we just can't train those, right?
5744720	5745560	No, it's not true.
5745560	5746080	It's not true.
5746080	5747680	That's what recurrent nets are.
5747680	5750640	You can have a varying number of iterations in your recurrent
5750640	5750960	net.
5750960	5753560	And what I'm describing here is an unfolded recurrent net.
5753560	5755960	It's the same model that you apply every time step, right?
5755960	5757920	So it's very much like a recurrent net.
5757920	5761960	And you can very well have an unknown number of steps.
5761960	5763560	You don't know a priori how long it's
5763560	5766400	going to take for your rocket to get to the space station, right?
5766600	5770160	So you may have kind of a large potential number of steps.
5770160	5773280	And you can have a cost function which will count,
5773280	5775560	like how much time it's going to take to get there.
5775560	5777920	And this will be part of the optimization, for example.
5777920	5779480	And this is classic optimal control.
5779480	5782440	I'm not telling you anything that I came up with.
5782440	5786240	This is from at least the 60s and 70s.
5786240	5789920	No, but the backprop, that variable number of layers
5789920	5793080	has to be done in kind of a classic iterative,
5793080	5795440	try out the variable number of layers
5795480	5797360	and see what the backprop comes up with.
5797360	5800320	Or you start with a very large number of layers
5800320	5802200	and then let backprop try and find.
5802200	5805520	But still, what I'm saying is there is fundamentally
5805520	5808560	two different kinds of computation that are at play here.
5808560	5811600	One is like the finite fixed thing,
5811600	5814640	and then you do some differentiable optimization on it.
5814640	5817600	Another kind of computation is this discrete symbolic
5817600	5819280	that has like an expandable memory
5819280	5821320	and an unbounded amount of time
5821320	5823120	to sit there kind of computing on things.
5823160	5825080	Okay, I put a stop right there.
5826080	5827640	What I'm describing has nothing to do
5827640	5829560	with symbolic, discrete, or anything.
5829560	5831640	I understand that, I understand that.
5831640	5834520	But you seem to be equating a variable number
5834520	5837800	with discrete symbolic, this is two different things.
5837800	5839960	Okay, let me pose it in this form,
5839960	5842080	which is that I can very easily write down
5842080	5846760	a symbolic program that can output the arbitrary digit of pi,
5846760	5848760	like the nth digit of pi.
5848760	5851400	Nobody can train a neural network that can do that.
5852360	5855080	So what's the difference between these two types
5855080	5858360	of computation and where in the future might we go
5858360	5861480	with neural networks or by augmenting neural networks,
5861480	5864840	whether it's differentiable Turing machines or whatever,
5864840	5866840	or neural Turing machines,
5866840	5869240	to try and bridge that gap and capability?
5869240	5871040	Okay, before we bridge that gap,
5871040	5874800	the algorithm to compute the digit of pi,
5874800	5877120	there's only a tiny number of humans,
5877120	5881080	only in the last few centuries that I figured this one out.
5881560	5884960	I'm interested in how is it that a cat can plan
5884960	5888800	to jump on the table and not fall or even open a door
5888800	5890520	or do things like that, right?
5890520	5891880	Once we figure this one out,
5891880	5893920	maybe we can think about kind of more complex stuff,
5893920	5896840	like designing algorithms that involve
5896840	5898480	complex mathematical concepts.
5898480	5902920	But I think we're way, we're not there, right?
5902920	5906880	We're faced with much more fundamental issues
5906880	5910560	of how do we learn predictive models to the world
5910680	5913000	by observing it and things of that type,
5913000	5916440	which are much more basic that most animals can do
5916440	5921440	and digit of pi, it's some kind of a line.
5923560	5925640	So, because you're talking about model predictive control,
5925640	5929640	is it possible that there are different shapes of problem?
5929640	5932600	So, for example, some problems are interpolative
5932600	5936720	and are solvable using differentiable models.
5936720	5938840	Do you think there exists problems
5939560	5941400	that are quite discreet in nature
5941400	5944000	and a different type of approach would be required?
5944000	5944840	Of course, yeah.
5944840	5947320	I mean, there is certainly a lot of situations
5947320	5951920	where the mapping from action to result
5951920	5955440	is very kind of discontinuous, if you want, right?
5955440	5956920	This is qualitatively different.
5957800	5962800	And so there are many situations where
5963000	5965240	or it's somewhat continuous
5965240	5970240	and situations where you change your action a little bit
5970280	5973520	and it results in a completely different outcome.
5973520	5975240	And so the big question, I think,
5975240	5980240	is how you handle that kind of uncertainty in the search.
5980360	5982680	And you can think of sort of two extremes.
5982680	5985000	So at one extreme in the continuous case
5985000	5986680	is the case where you're planning the trajectory
5986680	5991680	of a rocket or that's pretty continuous
5992280	5994080	and differentiable and everything you want.
5994080	5995320	And you don't even need to learn the model.
5995320	5997200	You basically write it down, right?
5997200	5999480	But there are situations there where you're flying a drone
5999480	6001520	or something where you might need to learn the model
6001520	6004080	because there's so many kind of nonlinear effects
6004080	6005720	that it's probably better to learn it
6005720	6006760	and people are working on this.
6006760	6009480	Same for like working robots and stuff like that.
6009480	6011000	Then there are things that are a little intermediate
6011000	6014640	where there are sort of hard constraints on what you can do.
6014640	6017000	So you want to grab an object with a robot arm,
6017000	6018400	but there is obstacles in between
6018400	6021040	and you don't want to bump into them and things like this.
6021040	6023120	So people tend to put like penalty functions
6023120	6025120	to make this sort of more continuous,
6025120	6027360	but there's sort of qualitative difference
6027360	6030160	between using your left arm or your right arm, for example,
6030160	6035000	or going, scratching your left ear with your left hand
6035000	6037560	or with your right ear going to the back of your head, right?
6037560	6040440	Those are qualitatively different solutions.
6040440	6042840	And then all the way to the other side,
6042840	6046000	there is intrinsically discrete problems
6046000	6049400	with that may be fully observable
6049400	6053040	with some uncertainty like chess and go-playing, okay?
6053960	6057680	And those we can handle to some extent
6057680	6060680	because the number of actions is finite.
6060680	6062200	It goes exponentially, but it's finite.
6062200	6067200	And so using kind of ways to direct the search,
6067440	6070600	despite the fact that the search space is exponential,
6070600	6073080	using neural nets as basically evaluation functions
6073080	6074640	to direct the search in the right way
6074640	6078800	and doing multi-color tree search and blah, blah, blah,
6078800	6080720	will work, okay?
6080720	6082280	With sufficiently many trials,
6083760	6085320	in the completely deterministic,
6085320	6088280	fully observable, differentiable case,
6088280	6091520	that's classical model predictive control, that's fine.
6091520	6095120	Then there is some stuff that we really don't know how to do
6095120	6096160	and it's twofold.
6096160	6099520	One is the model is not given to us
6099520	6103560	by equations derived from first principles, right?
6103560	6106840	So the stuff we're trying to do is in the real world
6106840	6108160	and it's got complicated dynamics
6108160	6109960	that we can't just model from first principle.
6109960	6111360	So we have to learn the model.
6112520	6114600	That's the first issue.
6114600	6117160	Second issue, the model lives in the world
6117160	6119560	and the world is not completely predictable.
6119560	6121320	It may be deterministic,
6121320	6124640	but you don't have full observation.
6124640	6127440	So you cannot predict exactly what's gonna happen
6127440	6128960	in the world because the world is being the world
6128960	6130800	or as a consequence of your actions.
6132240	6133680	So how do you deal with the certainty?
6133680	6134920	And for that, you need predictive models
6134920	6136120	that can represent uncertainty.
6136120	6139800	And we are back to the issue I was telling you about earlier.
6139800	6141080	Do you need latent variable models
6141080	6142680	or joint embedding architectures, right?
6142680	6146240	That's, and I've changed my mind about this as I told you.
6146240	6151240	So then there is the third obstacle,
6151920	6155480	which is, is the problem we're trying to solve
6155480	6158640	of the so continuous differentiable nature
6158640	6161680	or of the kind of completely discreet,
6161680	6162800	you know, qualitatively discreet
6162800	6165160	depending on what action you take nature.
6165160	6168640	And I think most problems are some combination of the two.
6168680	6172600	So, you know, you're trying to build a box
6172600	6174120	at a wood or something like that, right?
6174120	6176640	You can make the box bigger or smaller.
6176640	6178760	You know, you can hit the nail in this way or that way.
6178760	6182040	And that may be sort of continuous and differentiable.
6182040	6183040	But then there is, you know,
6183040	6185120	you put glue or screws or nails,
6185120	6189360	do you, or use kind of more classical carpentry or whatever.
6189360	6191960	And those are kind of discreet choices.
6191960	6193960	What type of wood are you using, you know, things like that.
6193960	6196360	So I think the, you know,
6196360	6198760	human mind is able to deal with all of those situations,
6198760	6202200	have, you know, know to use differentiable continuous stuff
6202200	6206280	when they have to and use the sort of discreet exploration
6206280	6208080	when we have to as well.
6208080	6211360	But we have to realize that humans are really, really bad
6211360	6213680	at the discreet exploration stuff.
6213680	6214760	We totally suck at it.
6216360	6218560	If we didn't, then we would be better than computers
6218560	6220920	at playing chess and go, but we're not.
6220920	6222560	We're actually really, really bad.
6223560	6225560	So...
6225560	6227560	That's fascinating.
6227560	6231560	So you seem to be saying in a way that you are a fan of hybrid models.
6231560	6232560	No.
6232560	6235560	And something, well, something like AlphaGo, for example.
6235560	6238560	I mean, that's basically, there's an interpolative space
6238560	6240560	which, you know, guides a discreet search.
6240560	6242560	Let's say it like that.
6242560	6244560	Do you think that's, is that a good thing?
6244560	6246560	Or do you think that's...
6246560	6248560	So you like that kind of model?
6248560	6252560	Okay, there is something very interesting about, you know,
6252560	6254560	in the context of reinforcement running about this,
6254560	6256560	which is actual critic models, right?
6256560	6260560	And you could think of all of the stuff that actually works
6260560	6262560	in reinforcement running.
6262560	6264560	I mean, they don't work in the real world, right?
6264560	6267560	But they work in games and stuff and simulated environment.
6267560	6272560	They very often use actual critic type architectures.
6272560	6274560	And the idea of a critic, you know,
6274560	6277560	goes back to early papers by Saturn and Bartow.
6277560	6282560	And the basic idea of a critic is to have a differentiable
6282560	6285560	approximation of your value function, right?
6285560	6290560	So you train a small neural net essentially to compute,
6290560	6295560	to estimate, to predict the value function from your state.
6295560	6297560	And the reason you need this is now you can propagate
6297560	6299560	gradient through it, right?
6299560	6303560	So that's kind of the first step into sort of making
6303560	6304560	the world differentiable.
6304560	6306560	You're just making the value function differentiable.
6306560	6310560	Now, inside of the world, there's two parts in my opinion.
6310560	6313560	And this is the source of the idea behind model-based reinforcement
6313560	6319560	running is that you have the world, right?
6319560	6323560	The world is going from state to state because it wants to
6323560	6325560	or because you're taking an action.
6325560	6329560	And then there is a value function or, you know, a cost function.
6329560	6332560	I prefer to talk in terms of cost function that takes the state
6332560	6338560	of the world and gives you kind of estimate of that cost, right?
6338560	6340560	Or gives you the cost.
6340560	6346560	Now, the world itself and the value function that takes the state
6346560	6351560	of the world and gives you pain or pleasure, right, is unknown.
6351560	6355560	But what you can do is build a differentiable model of that, right?
6355560	6358560	So a differentiable model of that is what NPC is all about.
6358560	6361560	You build a model of the world that predicts the new state
6361560	6363560	of the world as a function of the previous state.
6363560	6365560	It doesn't have to be a complete state.
6365560	6369560	It has to be a state that's complete enough to contain the relevant information
6369560	6372560	about the world, you know, relative to your task, right?
6372560	6376560	And then you have a differentiable function that you learn
6376560	6381560	that learns to predict the reward or the cost from your estimate
6381560	6383560	of the state of the world.
6383560	6387560	So what you have now is, you know, a neural net inside your agent
6387560	6392560	basically can simulate the world and simulate the cost
6392560	6396560	that is going to result from the state of the world in a differentiable way.
6396560	6401560	So now you can use gradient descent or gradient based methods for two things.
6401560	6405560	One for inferring a sequence of action that will minimize a particular cost.
6405560	6408560	Okay, there's no learning there.
6408560	6414560	Two, to learn a policy that will learn to produce the right action
6414560	6418560	even the state without having to do model predictive control,
6418560	6424560	without having to do this inference by energy minimization, if you want.
6424560	6429560	And that, in my opinion, explains the process that we observe in humans
6429560	6433560	by which when you learn a new task, you go from the, you know,
6433560	6437560	Daniel Kahneman system two to Daniel Kahneman system one, right?
6437560	6439560	So, you know, you learn to drive, you're learning to drive,
6439560	6442560	you're using, of course, your entire model of the world that you've learned
6442560	6447560	in the last 18 years, if you are 18, to predict that, you know,
6447560	6450560	when you turn the wheel of the car to the right, the car will go to the right
6450560	6453560	and if there's a cliff next to you, the car is going to fall off the cliff
6453560	6455560	and, you know, you're going to die, right?
6455560	6458560	You don't have to try this to know that this is going to happen.
6458560	6464560	You can rely on your internal model to, you know, avoid yourself a lot of pain, right?
6464560	6471560	And so, but you pay attention to the situation.
6471560	6473560	You pay a lot of attention to the situation.
6473560	6476560	You're completely deliberate about it.
6476560	6480560	You imagine all kinds of scenarios and you drive slowly
6480560	6484560	so you leave yourself enough time to actually do this kind of reasoning.
6484560	6489560	And then after maybe 20, 50 hours of practice,
6489560	6493560	it becomes subconscious and automatic, you know?
6493560	6495560	That's even true for chess players.
6495560	6498560	So, that's an interesting thing about chess, right?
6498560	6501560	I played once. I'm a terrible chess player, by the way.
6501560	6505560	And I played once a simultaneous game against a grandmaster.
6505560	6510560	So, he was, you know, he was playing against like 50 other people.
6510560	6513560	And so, I had plenty of time to think about my move, right?
6513560	6518560	Because he had to kind of play with the 49 other players before getting to me.
6518560	6521560	And so, I wait for him to come and make one move.
6521560	6525560	And then, you know, in one second, or first of all, he does, like, you know,
6525560	6527560	play something stupid, which I did.
6527560	6529560	And he moves within one second.
6529560	6530560	He doesn't have to think about it, right?
6530560	6532560	It's completely subconscious to him.
6532560	6534560	You know, it's just pattern recognition, right?
6534560	6538560	He's got this, you know, covenants predicting the next move,
6538560	6540560	you know, completely instinctively.
6540560	6543560	He doesn't have to think because I'm not, you know,
6543560	6548560	I'm not good enough for him to really kind of cause his system to kick in.
6548560	6553560	And of course, you know, he beat me in 10, you know, in 10 plays, right?
6553560	6556560	I mean, as I told you, I'm terrible.
6556560	6559560	You know, I learned to drive a long time ago.
6559560	6568560	But as it turned out, very recently, I went to drive a sports car on a raceway.
6568560	6573560	And, you know, again, the first few times, you were very deliberate about it, you know?
6573560	6578560	You explain a few things, and then you basically have to integrate all of that by yourself.
6578560	6581560	Over the course of a day, you get better and better, like much better,
6581560	6587560	just by, you know, basically compiling what at first is deliberate
6587560	6591560	into something that becomes automatic and subconscious.
6591560	6595560	Indeed. And also, abstractly reusing knowledge that you've gleaned elsewhere
6595560	6596560	and applying it in a new situation.
6596560	6600560	But anyway, Professor Yanlacun, thank you so much for joining us today.
6600560	6601560	Thank you. It's been an absolute honor.
6601560	6602560	Well, it's been a pleasure.
6602560	6604560	You guys are doing great work.
6604560	6606560	So, you know, keep going.
6606560	6608560	Thank you for being a fan, too.
6608560	6610560	Thank you so much for watching some episodes.
6610560	6611560	Thank you so much.
6611560	6612560	Wonderful.
6612560	6614560	Rando, introduce yourself.
6614560	6621560	Okay, so I joined FAIR, or should I say Meta AI Research, I guess.
6621560	6628560	Last June, for a postdoc position, I was doing my PhD at Rice University
6628560	6631560	under the supervision of Professor Richard Baronyuk.
6631560	6636560	And during my PhD, I focused on basically trying to understand deep networks
6636560	6639560	from a geometrical point of view through spline theory.
6639560	6643560	And now I'm trying to, let's say, expand my horizons
6643560	6649560	and do more diversified research topics with Yanlacun.
6649560	6656560	And for this paper, basically, the main goal was to try to understand
6656560	6663560	through a lot of empirical experiments, what do we understand by interpolation?
6663560	6665560	Does it occur in practice?
6665560	6670560	And does it make sense to use interpolation as we know it,
6670560	6675560	as, let's say, a measure of generalization performance for current networks?
6675560	6680560	And the main point is really to say that the current definition of interpolation,
6680560	6688560	which uses this convex hull, might be too rigid to really provide any meaningful intuition.
6688560	6696560	And so we either need to adapt this definition or just entirely think about this in a different angle.
6696560	6703560	But yeah, the current definition is not good enough for the current data regimes that we are in right now.
6703560	6706560	I don't know if it's precise enough.
6706560	6707560	Perfect.
6707560	6708560	No, it's wonderful.
6708560	6709560	Cool.
6709560	6712560	Sorry, I need to get into the mood again.
6712560	6713560	Yeah.
6713560	6715560	Hi, Randall.
6715560	6718560	It's really cool to have you here.
6718560	6720560	We've enjoyed reading the paper.
6720560	6724560	It's quite a short and concise paper, I have to say.
6724560	6728560	And the experiments are quite, I find them to be really on point,
6728560	6734560	especially where you look at the latent space experiments.
6734560	6739560	Because a lot of people would say, of course we're not interpolating in data space.
6739560	6742560	We're interpolating in the latent space.
6742560	6749560	Yet, even in the latent space, you know, and we've talked a little bit about the notion of interpolation and extrapolation.
6749560	6755560	Is it fair to say that the paper is just sort of a negative argument?
6755560	6760560	Is it fair to say that the main point of the paper is arguing,
6760560	6768560	look, interpolation is the wrong concept you're looking for when you criticize these models?
6768560	6769560	Yes.
6769560	6771560	I don't think it's negative per se.
6771560	6779560	It's more, let's say, a call to change the definition for the current usage of machine learning models.
6779560	6783560	Because now we're not in low dimension regimes anymore.
6783560	6790560	And so using those concepts that have been defined 50 years ago when we were looking at univariate models,
6790560	6792560	does not really make sense.
6792560	6796560	So I think the intuition behind what we try to mean by interpolation is right.
6796560	6797560	We should not change that.
6797560	6802560	We should change the mathematical definition of it.
6802560	6808560	And yeah, like you say, people could argue, yes, but you have interpolation in the latent space and so on.
6808560	6812560	But we showed that even in a classifier setting, it does not happen.
6812560	6819560	And you can also show that in a generative network setting, it will not happen again or because of the dimensionality.
6819560	6823560	So it's really about going to the high dimensional setting.
6823560	6828560	Then things start to break and we have to adapt to that basically.
6828560	6831560	And we've already asked Jan this question.
6831560	6840560	But if you had to give your best shot at making the argument that these people want to make when they say,
6840560	6842560	oh, it's just interpolating.
6842560	6849560	If you had to give your best shot at making that argument successfully, you know, what would you change?
6849560	6856560	How would you change the notion of interpolation or what argument would you make for those people?
6856560	6862560	So I think what most people try to say is they try first to conceptualize the data that they have.
6862560	6864560	So for example, you have an apple, right?
6864560	6865560	You have different colors.
6865560	6873560	And if you think of this color as being your latent space, then you can say, okay, between green and red, you have a new color.
6873560	6874560	But it's in between the two.
6874560	6876560	You are in an interpolation regime, right?
6876560	6879560	So all of this point of view is from a generative perspective.
6879560	6883560	And this is because you only think of a few factors of variations like this.
6883560	6887560	And then you think, okay, everything should be in interpolation regime.
6887560	6894560	But even with that definition, if you start having a lot of factors of variation, then the course of dimensionality will kick again.
6894560	6898560	And you will never be in interpolation regime, even in a generative setting.
6898560	6911560	So for them, the best argument would be if I don't consider the real data set, but very low dimensional approximation, very rough, which can be explained only with a very few factors of variation.
6911560	6915560	And I can somehow linearize those factors in my latent space.
6915560	6919560	Then I will have more chance at being in an interpolation regime.
6919560	6924560	So we'll have to have a sort of lossy compression of your data, if you will.
6924560	6927560	And then you can try to reach there with more chances.
6927560	6931560	So I think you just still manned the argument for interpolation.
6931560	6937560	So I think that's precisely what folks do argue is happening in a deep neural network.
6937560	6939560	Do you not believe that?
6939560	6946560	Well, as soon as you have high dimensionality settings, then it does not happen almost surely.
6946560	6952560	And I mean, you could argue, so for example, let's say you take a gun or any big generative network, right?
6952560	6959560	In the latent space, you have hundreds of dimensions and you sample those guys from a Gaussian distribution.
6959560	6967560	So even if you were saying, OK, your training set is sample from a generative model and your test set is sample from the same generative model.
6967560	6970560	And in that latent space, you have interpolation.
6970560	6983560	Well, it's wrong from the beginning because in that latent space alone, you will never have a Gaussian sample that lies in an interpolation regime as soon as the dimensionality is greater than, let's say, 15 or 20.
6983560	6989560	So it does not happen because of the dimensionality, unless you have very degenerate things.
6989560	6996560	Of course, if your generative network just speeds out a constant, then in pixel space, you will have interpolation.
6996560	6999560	But this is degenerate by definition.
6999560	7004560	I'd like to pick up on that, if you don't mind.
7004560	7015560	Yeah, so what I want to pick up on is, again, all of this hinges on definition one, which is in the paper, which is membership within this convex hull.
7015560	7022560	And there's a sense in which that's an extremely rigid definition of interpolation, right?
7023560	7038560	And I think I heard you earlier say that what we need to do is redefine what we mean by interpolation in higher dimension because the intuition of interpolation is still correct, but we need to redefine what we mean by interpolation.
7038560	7048560	So if you took a shot at redefining interpolation, how would you define it? What do you think is a better definition of interpolation?
7048560	7056560	So I think it's very task dependent. So let's say you want to look at a task which is pure object classification.
7056560	7068560	In that case, I think going back to what we were saying earlier on first trying to compress some of your data so that you can explain it with as little factors of variation as possible.
7068560	7073560	And then you can use the current definition just on a compression of your data.
7073560	7084560	Then it could make sense because you don't really mind about the finicky details of your objects if you just want to differentiate between different classes of objects.
7084560	7098560	But in another setting where you might be trying, I don't know, to denoise samples or things like that, then you might want to have a very more specific definition based on what you try to denoise in your data and so on.
7098560	7113560	So I don't think there will be a general definition that works across the board. It will be really dependent on the type of task you are looking down downstream. It's really the key.
7113560	7129560	Let me throw something out there because I've been thinking about this and I tried to ask Professor Lacune about this, but I didn't communicate it clearly, which is that for something to be in the convex hull, so in dimensions,
7129560	7142560	for something to be within a convex hull, it's a necessary condition that on every single dimension, the sample data point lies within the range that was sampled.
7142560	7150560	So that's a necessary but not sufficient condition because the convex hull is actually even smaller than that space.
7150560	7159560	But it's necessary that it be within that axis-aligned bounding box. It has to be inside there on every dimension.
7159560	7173560	And so what if I were to invert this and say that, well, instead I'm going to define extrapolation as on every single dimension, it has to be outside the sample range.
7173560	7186560	So in other words, the point has to be outside the axis-aligned bounding box. That would actually come to the complete inverse conclusion because then exponentially so all machine learning would be interpolation
7186560	7191560	because it's very, very unlikely that you're outside every single dimension.
7191560	7208560	So I'm thinking somewhere in between there, there's almost like a fractional concept of interpolation versus extrapolation where we can say it's almost like on average, how many of the dimensions do you hit inside the sample point?
7208560	7216560	So maybe I'm inside 20% of the dimensions on average. Could that be like a route to any type of improved definition?
7216560	7223560	So that's actually a very good point. So basically what you are saying is instead looking at, let's say, the closest, enclosing hypercube.
7223560	7229560	And then you say, okay, if you're in that hypercube or not, you're in interpolation or extrapolation regime.
7229560	7238560	And that's actually something we're looking at right now, which is not the smallest hypercube but the smallest ellipsoid that encloses your data.
7238560	7243560	And it's somewhere in between the convex hull and the hypercube that you mentioned.
7243560	7257560	So for sure, there is some ways in between those two extremes where you will have a meaningful interpolation and extrapolation regime that does not collapse all one way or the other way.
7257560	7265560	So this is for sure one interesting route. And this could potentially be, let's say, task agnostic.
7265560	7274560	But then again, would it be precise enough to re-quantify generation performances per se? I don't know yet.
7274560	7277560	But yeah, that's something we are looking into right now.
7278560	7298560	And the reason that we kind of came to that thought process is because there is an intuition that machine learning is kind of very good at taking all these dimensions that we sample and saying, you know, it's really only a subset of those dimensions that matter, at least in some transform way.
7298560	7314560	Like for example, suppose I was doing, suppose I only had 16 dimensions of ambient space and everybody agreed, yeah, given the amount of data we have, we are interpolating, like almost all the time we're in this rigid definition of a convex hull.
7314560	7322560	And then somebody comes along and says, oh, by the way, we upgraded all our sensors now and we added 240 dimensions.
7322560	7332560	And if all those dimensions happen to not be useful for the problem that we're trying to solve or classification problem or whatever, it would be weird to say now we're all of a sudden extrapolating.
7332560	7344560	Because if the neural network is doing what it should be doing, it will ignore all those irrelevant definitions and continue just calculating exactly what it was calculating before.
7345560	7348560	Yes, that's a very good point.
7348560	7359560	But I think for this to actually work, you will need to assume that you have enough actual samples so that when you train your network understands that basically those dimensions are pure noise.
7359560	7363560	It does not need it to solve the task at hand and it disregards it.
7363560	7370560	So you have some all regularization terms that kicks in and it does not try to use them to minimize the training loss.
7370560	7373560	But I think in practice, that's not really what we see.
7373560	7391560	If you take like image task classifications, even especially now with self-supervised learning methods, we see that actually most of the information that we could think is irrelevant for the task is actually kept because it can of course help in reducing the training loss.
7391560	7398560	So I think for what you said to work, you really need to be sure that either you have the perfect regularizer,
7398560	7403560	or you have the perfect model and regime of training samples and so on.
7403560	7409560	But in a general setting, I think it will be tricky to claim it like this.
7409560	7411560	That's a very interesting point.
7411560	7418560	So you're saying when you add these extra dimensions, even to learn to ignore them, you have to have lots of data.
7418560	7424560	Yes, yes, or regularization or some mechanisms that exactly.
7424560	7426560	Yeah, very interesting.
7426560	7428560	That's one thing.
7428560	7434560	That's one reason why, let's say, add up parameterization by end is sometimes useful.
7434560	7443560	Because if you know which things are useless for your task, you can by end remove them from the data that you feed to your deep net or whatever model.
7443560	7446560	And then doing this will improve generalization.
7446560	7453560	So that's why in some cases it's still useful to do those preprocessing steps.
7453560	7454560	Right.
7454560	7456560	Could you give me a bit of intuition here?
7456560	7462560	So we've been listening to quite a few folks that make the argument about the interpolative nature of deep learning.
7462560	7468560	And you could argue that any kind of processing and machine learning, not just feature transformations,
7468560	7471560	but even things like regularization and domain randomization,
7471560	7477560	that there are all ways of making the problem more interpolative and it would be appropriate for an interpolated problem.
7477560	7482560	So I'm thinking to myself, I mean, in this example, you take pictures of clock faces,
7482560	7487560	and it's not interpolative in the ambient space, you perform some kind of feature transformation.
7487560	7491560	There exists some nonlinear transformation that transforms it into an interpolative space.
7491560	7492560	That's what these people say.
7492560	7497560	But I'm wondering, you know, I'm interested in the curse of dimensionality.
7497560	7500560	You know, why does deep learning work at all?
7500560	7507560	And I've spoken to folks that talk about creating various priors to combat the curse of dimensionality.
7507560	7511560	But why do you think that deep learning works at all in these high dimensional spaces?
7512560	7522560	Well, so first, I might say something a bit speculative or not agreed upon by everyone,
7522560	7523560	but I don't think you can...
7523560	7524560	We love it. We love it. Please do.
7524560	7525560	Exactly.
7525560	7529560	I don't think you can state so generally that deep learning works right.
7529560	7531560	You need the right architecture.
7531560	7533560	You need the right loss function.
7533560	7536560	You need everything right for it to work.
7536560	7539560	That's why it works now because we spend, I don't know,
7539560	7543560	much amount of money and manpower to get to where we are now.
7543560	7546560	But if you just take a plain MLP, you apply it on ImageNet,
7546560	7549560	I don't think you will say it's working right.
7549560	7551560	So I think right now it's working.
7551560	7555560	I mean, what we have is working because we basically, by end,
7555560	7561560	did all the cross-validation such that the network that we are using is regularized enough
7562560	7566560	to only learn the necessary or meaningful information,
7566560	7571560	or at least as best as we can to provide good generalization performances.
7571560	7579560	So, yeah, it's a bit too general of a statement to say that deep learning works out of the box for everything.
7579560	7580560	In fact, many cases...
7580560	7581560	I love that point.
7581560	7582560	Yes, yes.
7582560	7588560	Many, if you go into not just image classification, but let's say audio classification
7588560	7592560	and some maybe trickier data set where you don't have a lot of samples,
7592560	7596560	then everything starts to break quite rapidly, right?
7596560	7600560	You have always people trying to go into the medical field as well,
7600560	7604560	but since it's very hard to generalize between patients,
7604560	7608560	between recording devices and so on, things get very, very messy
7608560	7613560	and everything is ad hoc and optimization, basically.
7614560	7617560	I don't think we're at this point yet where you can say,
7617560	7619560	that's it, deep learning works out.
7619560	7621560	Yeah, can I jump in real quickly here, Tim?
7621560	7623560	Because what I want to say is, brilliant point.
7623560	7627560	I mean, and when you think about it, let's take Professor LeCun,
7627560	7632560	you know, one of his great invention, right, the CNN, Convolutional Neural Network.
7632560	7636560	That was discovered by a human being, like a machine,
7636560	7639560	and no neural network learned to do convolution.
7639560	7644560	LeCun taught machines how to do convolution.
7644560	7647560	And so we oftentimes discount this and say,
7647560	7651560	look how great machine learning works, we discount all the human engineering
7651560	7654560	that has gone into actually making machine learning work
7654560	7656560	through specific architectures.
7656560	7658560	Yes, exactly, exactly.
7658560	7662560	Basically, we are guiding where to go,
7662560	7666560	and then of course, once we guide enough, the machine knows what to do.
7666560	7671560	I mean, this is not a reduction of what we can do with deep learning, right?
7671560	7674560	But he's just saying that a generalized statement
7674560	7679560	out of the box, deep learning works even in a new task that we never tried before
7679560	7681560	is an overstatement.
7681560	7684560	But I still want to linger on this point of why you said,
7684560	7686560	or here's just a random projection of the data,
7686560	7689560	and then here's a ResNet projection, here's a trained ResNet projection.
7689560	7694560	What I took from that is irrespective, it's all in an extrapolative regime.
7694560	7697560	And there was a guy who posted an article
7697560	7700560	who is disparagingly saying that neural networks are just interpolators,
7700560	7703560	analogous to a Taylor series approximation
7703560	7706560	within a small part of the domain of a function,
7706560	7708560	and it goes haywire outside of the domain.
7708560	7711560	And there are a lot of people that would make that case,
7711560	7714560	that neural networks cannot extrapolate outside of the training range,
7714560	7717560	but they do seem to work remarkably well,
7717560	7718560	given the curse of dimensionality.
7718560	7720560	So why is that exactly?
7720560	7725560	So let's say for simplification that you have a binary classification problem.
7725560	7728560	So once you go into your Latin space,
7728560	7730560	or if you just do linear classification,
7730560	7732560	if you stay in your ambient space,
7732560	7735560	basically you say you have class 0 or 1
7735560	7738560	based on which side of an hyperplane you lie in.
7738560	7741560	Now, if you are on the good side,
7741560	7743560	you can go to the infinity, right?
7743560	7746560	You can go to extrapolation regime,
7746560	7748560	as far as you want from the training set,
7748560	7751560	as long as you are in the correct side of that hyperplane,
7751560	7753560	you will have good generalization performance.
7753560	7756560	So it's not exactly correlated.
7756560	7758560	The only thing you need,
7758560	7761560	some O is to have the orientation of the axis
7761560	7763560	where you will extrapolate
7763560	7767560	to be somewhat aligned in the orthogonal space of that hyperplane.
7767560	7769560	Once you have that, then you will generalize,
7769560	7771560	even though you can be in extrapolation regime
7771560	7774560	in ambient or feature space.
7774560	7779560	So that's one thing that is maybe proper to linear classification.
7779560	7783560	If you were doing maybe some other type of classifier on top of it,
7783560	7785560	it might change.
7785560	7787560	But given the current settings,
7787560	7790560	I don't think people should expect bad performance
7790560	7793560	because you are not in an interpolation regime.
7793560	7795560	This is quite of a shortcut,
7795560	7798560	maybe that it guided by our intuition, right?
7798560	7801560	We think it's much easier to classify something
7801560	7803560	if it's in an interpolation regime.
7803560	7806560	But if you just look at the plane in our classifier,
7806560	7809560	that's not the case.
7809560	7811560	Well, so just to steal man a bit
7811560	7815560	what say others on the interpolation side,
7815560	7818560	again, that's only if you're buying into a very rigid definition
7818560	7821560	of interpolation, which the way the paper defines it
7821560	7823560	is in a linear way.
7823560	7827560	I mean, it's defining interpolation by this linear convex hull.
7827560	7830560	And of course, neural networks, for example,
7830560	7832560	are highly nonlinear systems.
7832560	7836560	And what you just said, sort of which side of a separating hyperplane you're on,
7836560	7838560	that itself is a nonlinear function.
7838560	7842560	And so I think the point you made early on was very good,
7842560	7845560	which is we have this intuition of what interpolation is.
7845560	7849560	We haven't yet got a good definition for interpolation
7849560	7854560	in multi or high-dimensionality nonlinear cases.
7854560	7859560	But obviously the linear definition doesn't really apply.
7859560	7861560	Yeah, exactly.
7861560	7866560	It's really because, I mean, it's linear,
7866560	7870560	but again, you can consider your data as being nonlinearly transformed.
7870560	7874560	The problem is still that if you have high-dimensionality,
7874560	7878560	then to be within a convex hull becomes exponentially hard
7878560	7881560	using the convex hull of the training side.
7881560	7884560	If you define something else, like you said before the hypercube,
7884560	7886560	it could be the opposite.
7886560	7889560	It's always in interpolation regime.
7889560	7894560	And the key is basically to find a meaningful set
7894560	7898560	such that you don't go all the way in one direction or the other.
7898560	7902560	And because I think the main point also is that this should give intuition
7902560	7905560	into the generalization performance of a model, right?
7905560	7910560	Because if you can detect a sample is in an extrapolation regime,
7910560	7913560	but your model still performs very good on it.
7913560	7916560	I don't know if it has really a lot of practical use.
7916560	7920560	So what will be good as well is to have the basically
7920560	7926560	generalization performance of a model correlate with your definition of extrapolation.
7926560	7929560	And that's really what we are trying to get.
7929560	7934560	And that's why I was saying that probably you might have a task-dependent definition,
7934560	7940560	because that might be where you get the best precise definition to reach that.
7941560	7946560	I was wondering to what extent, if any of this invalidates the so-called manifold hypothesis.
7946560	7950560	So I think when most people speak of interpolation and deep learning,
7950560	7952560	their intuition is something along the lines of,
7952560	7956560	well, the model learns some manifold in the latent space,
7956560	7962560	and interpolation means I'm just kind of like traversing the geodesic on that manifold.
7962560	7965560	And when you visualize the results on an autoencoder or something,
7965560	7968560	you can see this kind of continuous geometric morphing
7968560	7971560	of, let's say, one image into another image.
7971560	7975560	And that manifold, I think you said in your paper that the actual data manifold,
7975560	7979560	it's not possible to approximate that well, but it's doing something interesting.
7979560	7983560	And a prediction on that manifold in the intermediate space,
7983560	7985560	it's not massively deranged, is it?
7985560	7987560	It's still doing something very useful, statistically.
7987560	7988560	Yes, yes, yes, totally.
7988560	7991560	So I think it does not at all contradict it.
7991560	7996560	And in fact, think of a very, very simple example where you say your data
7996560	8000560	is a linear manifold of dimension 100, let's say,
8000560	8005560	which is a relatively meaningful number of dimensions even for images.
8005560	8008560	But now suppose it's completely linear.
8008560	8012560	Well, even in that case, it will be hard to be in interpolation regime
8012560	8014560	just because you have 100 dimensions
8014560	8020560	and picking a new sample that lies in the convex of your training set is exponentially higher.
8020560	8024560	So you can have a manifold for your data.
8024560	8026560	It can even be linear.
8026560	8028560	So the simplest manifold you wish for,
8028560	8031560	the only problem is the dimensionality of that manifold.
8031560	8035560	Now that being said, again, it does not mean that you cannot do
8035560	8039560	basically moving on the geodesic of your manifold
8039560	8041560	or you cannot do some sort of interpolation
8041560	8046560	because that's what happens in most of the current state-of-the-art generative networks, right?
8046560	8050560	It's just that you don't need to be in interpolation regime
8050560	8054560	to have a correct generation process, basically.
8054560	8058560	And that's, again, one thing that is very important.
8058560	8061560	And you mentioned, you have those examples.
8061560	8064560	That's true that the interpolation is extremely nice,
8064560	8068560	but again, it does not say that the points you start with
8068560	8072560	and the point you end up with are in interpolation regime of your training set.
8072560	8077560	So I think it's two different things to be able to interpolate or move on your manifold
8077560	8081560	and being in the interpolation regime from your training set.
8081560	8086560	So what's intriguing me about, and I want to clarify something earlier,
8086560	8092560	which is that though a hyperplane is a linear construct,
8092560	8098560	the function that says you're on one side or the other of a hyperplane is a nonlinear function.
8098560	8103560	And in that case, it's a digital function.
8103560	8106560	You're either on this side or that side, it's zero or one.
8106560	8112560	What's kind of interesting is you gave that as an explanation for why machine learning works at all,
8112560	8118560	which is that for data sets that we care about, or problem spaces that we care about,
8118560	8125560	it seems to often be the case that we can do enough sequences of linear and nonlinear transformations
8125560	8132560	to arrange the data such that it falls on one side or another of a separating plane.
8132560	8139560	In a sense, it seems like real-world data is often separable if you do enough transformations on it.
8139560	8147560	And since that separability is a very digital concept, is there anything interesting in that?
8147560	8149560	Any intuition?
8149560	8152560	Yes, yes. So for this, you still have to be careful, right?
8152560	8159560	Because if you go in high-dimensional spaces, you can basically separate everything very easily.
8159560	8162560	But your generalization performance might be bad.
8162560	8169560	So basically, what you want is really to have, let's say, a good ratio of how much you expand dimension,
8169560	8174560	how much you nonlinearly transform your data, to then reach a good generalization performance.
8174560	8178560	Because otherwise, you can just fall back into a kernel regime, let's say,
8178560	8183560	and you expand so much your space that sure you can separate everything on your training set,
8183560	8186560	but then the generalization is going to be very poor.
8186560	8192560	And that's one strength of deep networks, right, is that you have so much of those nonlinear transformations
8192560	8198560	that you can somehow not expand the dimension of your space too much or even contract it,
8198560	8207560	and still have a separating hyperplane in the end where generalization is much higher than in other class of models.
8207560	8213560	So I think the key is really to have those meaningful nonlinear transformations
8213560	8216560	such that you don't have to increase your space too much.
8216560	8219560	You just shape it around, if you will.
8219560	8222560	And what you said earlier is exactly true.
8222560	8226560	If you think of which side you are in, basically, you are binarizing your data, right?
8226560	8235560	And if you have good classification, it means all the classes are assigned to the same labels, which is 1 or 0.
8235560	8242560	And if you think of it still in interpolation regime, then suddenly you are in interpolation regime, right?
8242560	8250560	You are a 1. The new sample is a 1 after binarization, and you become interpolation regime, and you have a good performance.
8250560	8256560	But this comes after this compression step, if you will, or discretization step.
8256560	8261560	And that might be another direction to explore as well.
8261560	8267560	If you start to quantize things or to compress things, as we were saying a bit at the beginning,
8267560	8273560	then you can reach the interpolation much more easily as well.
8273560	8275560	I have a couple of questions.
8275560	8281560	So again, in Table 1, I mean, your paper is making the argument that everything is extrapolation,
8281560	8284560	given this convex notion in high-dimensional space.
8284560	8292560	But if we zoom in a little bit, though, so you are using a pre-trained ResNet classifier, pre-trained on ImageNet,
8292560	8298560	and how do all of these things change the structure of the latent space in a meaningful way?
8298560	8301560	And the embedding space is also highly distributed.
8301560	8305560	And we were wondering, can you give us some intuition here?
8305560	8310560	So is the information likely to be quite evenly distributed over the latent,
8310560	8316560	or do you think it's actually quite bunched up and sparsely encoded in few of the features?
8317560	8321560	So I think this will depend on which training setting you use, right?
8321560	8324560	So for example, if you start using dropout and things like that,
8324560	8332560	you will try to have a more evenly distribution of your information to have a more stable loss when you drop those units.
8332560	8334560	So I don't think you have a general answer for that.
8334560	8340560	It will depend on the type of regularization you have or training is done, etc.
8340560	8346560	But you have to keep in mind that what you try to do with gradient descent is just minimize your loss, right?
8346560	8351560	But then with cross-ontropolis, let's say, your gradient starts to vanish as you become really good
8351560	8354560	and you stop learning where you are, basically.
8354560	8361560	So given that, depending on your initialization, you will still try to make the best of what you get.
8361560	8369560	And even if it means learning redundant dimensions, if this can reduce your loss further at a more rapid rate,
8369560	8370560	that's what you will do.
8370560	8379560	So if you don't impose any regularization or anything, there is no clear reason to assume that everything is well organized and so on.
8379560	8381560	And that's what we see even in generative networks.
8381560	8388560	You have to start putting so much regularization to try to have disentanglement and to try to make sense of those latent spaces.
8388560	8397560	Because otherwise, you just try to learn what minimizes your loss with the most short-term view of your loss landscape.
8397560	8403560	So basically, that could be built if you have some specific regularizer.
8403560	8407560	But otherwise, it will not occur naturally.
8407560	8414560	Of course, and again, if you wanted to only retain the minimal information to solve the task at hand,
8414560	8419560	then you will see much more interpolation regime in that latent space.
8419560	8428560	If you think of MNIST, for example, you will disregard the translation of your digits, the rotation of the digits, all those things.
8428560	8431560	All those things will be disregarded when you reach the latent space.
8431560	8436560	And then you will basically be in interpolation regime most of the time.
8436560	8445560	But since you keep as much information as possible to try to minimize your loss as best as you can, then you basically occupy as much as you can.
8445560	8450560	Unless, again, you have some degeneracies because of the whole architecture tricks.
8450560	8457560	For example, if you have a bottleneck layer, you will limit the dimensionality of the manifold you span in the latent space.
8457560	8461560	So you can have all those parameters that can play a big role.
8461560	8466560	So it will be in the general setting. I don't think you could assume anything.
8467560	8477560	And I think there's also a relationship, too, between the dimensionality of the latent space and, let's say, some intrinsic dimensionality of the problem.
8477560	8487560	So if the intrinsic dimensionality of the problem only takes five dimensions to solve, and yet I give a latent space of 256 dimensions,
8488560	8496560	I think what I hear you saying is that, of course, gradient descent is going to make some use of those other 251 dimensions,
8496560	8503560	but they're going to have maybe a very minuscule or diminishing effect on the latent space.
8503560	8510560	Whereas on the other hand, if I then took that same network and increased the complexity of the problem,
8510560	8515560	we could end up with, for example, it's sparsifying for any particular class.
8515560	8524560	So if we're doing some multi-class problem, we may find that it sort of arranges these seven dimensions to solve the dog versus hot dog problem,
8524560	8529560	and these 12 dimensions to dissolve the car versus motorcycle problem.
8529560	8536560	It might be forced to make more compact use of that latent information space per class. Is that fair?
8536560	8538560	Yes, so that's a very good point.
8538560	8547560	So first of the first things that you said, one thing to keep in mind, so let's say your data is even linear manifold of dimension one,
8547560	8553560	and then you go through a deep net, and then somehow it's already linearly separable.
8553560	8557560	Then you only need to learn the identity mapping with your deep net to solve the task.
8557560	8561560	But if you start from a random initialization, it's extremely hard to do that.
8561560	8564560	That's why people came up with ResNet and all those things.
8564560	8574560	So already from this point of view, it means that almost surely your linear one-dimensional manifold will become highly non-linear,
8574560	8580560	still one-dimensional, but very highly non-linear in the latent space of your classifier.
8580560	8586560	And as we showed in the figure one of the paper, even if the intrinsic dimension is one,
8586560	8591560	if you are highly non-linear in your space, then you will basically never be in interpolation regime.
8592560	8597560	So you have those sort of artifacts that come just from the fact that learning simple mapping,
8597560	8602560	let's say, with a non-linear deep network is not always simple.
8602560	8612560	And those artifacts will be introduced right away because it's so hard in your parameter space to reach that point where you have identity mapping.
8612560	8622560	So this is another effect that kicks in and that can somehow remove those assumptions that even if your data is in a low-dimensional regime
8622560	8629560	and almost linear, it will be preserved in the output space of your network.
8629560	8634560	So this is something really important to keep in mind as well.
8634560	8643560	So, Randall, I also noticed in about 2018, you were the first author on some really interesting work and it was called a spline theory of deep learning.
8643560	8646560	And then I think the next year it got into Neuribs.
8646560	8649560	So I'm just reading a bit from the abstract.
8649560	8656560	So you said you built a rigorous bridge between deep networks and approximation theory via spline functions and operators.
8656560	8665560	And you actually think that this can explain a lot about deep learning in the sense of then being a composition of these max-safine spline operators.
8665560	8668560	Has any of this work informed your view of deep learning now?
8668560	8670560	Yes, a lot actually.
8670560	8678560	So first of all, I want to be precise that of course a lot of people knew about the fact that if you use, for example,
8678560	8686560	volume activations, absolute value and so on, or max-pulling, then the whole network is continuous piecewise linear mapping.
8686560	8695560	So what we did mostly is to make this a bit more rigorous and try to understand what the partition of the input space looks like,
8695560	8702560	what the perigen mapping looks like, and how can we use that to gain some more intuitions into what's happening.
8702560	8708560	And the nice thing with this is that if you think about it, there is nothing simpler than piecewise linear mappings, right?
8708560	8718560	Basically, it says that if you are in a region of your space, a region from the partition of your mapping, then the input-output mapping will stay linear.
8718560	8724560	And this allows to do a lot of analysis, for example, to adversarial perturbation or all these type of things.
8724560	8728560	And it can also open the door to other lines of research.
8728560	8738560	For example, one thing we did, I think it was NeurIPS last year or two years ago, was to use that to derive the exact expectation maximization algorithm for the generative networks,
8738560	8744560	because now you have a cleaner, let's say, or simpler analytical form of your network.
8744560	8752560	And this really opens to me the door to derive some more interesting theoretical results.
8752560	8768560	It does not really help intuitively, because I think everyone had this intuition from before already, but it's mostly a mathematical tool that allows to derive with small is some interesting results.
8768560	8784560	And I think one important thing as well is that you have really this dichotomy right between the, let's say, old school signal processing, template matching type of academia researchers,
8784560	8788560	and the new school with deep learning, everything has to be trained and so on.
8788560	8792560	And what is interesting in this paper is that we somehow bridge the two.
8792560	8806560	We say that basically a deep net is a very smart way to build an adaptive spline, which will learn automatically its partition of the input space and the perigen affine mapping, such that it works in high dimension.
8806560	8809560	And this was not known before by anyone in the spline theory.
8809560	8825560	So I think to speak about adaptive spline in high dimension, no one has no idea what to do except dimension two or three, maybe, because a lot of PDE work uses, but outside of dimension three, no one thinks about splines.
8825560	8831560	So I think there's a very strong result is to bridge those two different fields.
8831560	8841560	That's fascinating. I think one of the issues, because I'm speaking to Juan Bruno about this, and he was saying there was a big tradition in harmonic analysis of trying to reason about the behavior of these models.
8841560	8851560	And we did have the universal function approximation theorem, which in a sense is talking about stacking basis functions, you know, to approximate an arbitrary function.
8851560	8862560	But you say in your abstract here that the spline partition of the input signal opens up in a new, you know, opens up a new geometric avenue to study how deep neural networks organize signals in a hierarchical fashion.
8862560	8869560	I don't think people really have much of an intuition on how these models behave and how to reason about them.
8869560	8871560	Yes. Yeah, that's a very good point.
8871560	8879560	So one thing that we did, for example, is to study all the partition of the mapping evolves as you go through the layers.
8879560	8885560	And what we show is that at each layer, you keep subdividing your current partition.
8885560	8898560	And this is very interesting because once you know that if you look at a binary classification, for example, you get that the decision boundary is basically linear in each region of the partition.
8898560	8908560	And so what this tells you is that as you add layer, what you have to do is to refine the regions that still contain more than one class within them.
8908560	8919560	So this kind really brings insights and maybe opens the door to building new learning techniques to how many layers you should stack, which regions should they subdivide and so on.
8919560	8924560	And this is really akin to decision trees and how they build their partition as well.
8924560	8931560	So there is a lot of work that could be done also to maybe bridge the two or use one to understand the other.
8931560	8944560	And it's really geometrical because it's in the whole field of computational geometry and so on because we have those hyperplane arrangements, those half-spaces, intersection of them, hyperplane desalation.
8944560	8947560	It's also a known system.
8947560	8954560	So it's really geometrical and it can have a lot of interesting insights to understand what's happening.
8954560	8958560	It also provides nice visualization tools.
8958560	8961560	It's really fascinating.
8961560	8967560	Would you be interested in coming back on the show for a future episode just dedicated to this topic?
8967560	8970560	That'd be amazing.
8970560	8975560	Randall also actually released a paper called Neural Decision Trees.
8975560	8982560	And in the abstract he said they propose a synergistic melting of neural networks and decision trees.
8982560	8987560	So this is something that you've been thinking about actually from many angles.
8987560	8989560	Yes, yes, yes.
8989560	8995560	And that's very, very synergic to think about one from the point of view of the other as well, right?
8995560	9008560	Because if you think of a decision tree, the real limitation of it is that all you subdivide one region by adding a node does not really tell you how to subdivide another region in another part of the space.
9008560	9014560	You don't have this, let's say, communication or friendly help between the region subdivision.
9014560	9023560	But in a deep net, what you do actually is that if you know how to subdivide one region, then it will automatically enforce or you subdivide nearby regions.
9023560	9031560	And through this, suddenly you have a mechanism that appears which is that you don't need samples in each region to know to subdivide them.
9031560	9038560	You just need samples in some of the regions and then it will guide you on how to subdivide regions of the space without samples.
9038560	9047560	And that's something that is extremely strong when you go to high dimensional settings because you cannot have samples in all parts of the space by definition.
9047560	9056560	So I think this is extremely nice to have both point of view because then you can try to use the strengths of one to maybe improve the other.
9056560	9059560	So that's really interesting to me.
9059560	9074560	I want to ask you this question that we ask a lot of our guests because it's just at least something that's kind of profound interest to me is that there is this apparent dichotomy between continuous and discreet.
9074560	9091560	It's like the human brain is at the most lowest possible level and analog kind of continuous system and yet it evolved all these sort of discreet almost computations on top of it like pulses that either fire or don't fire like that type of thing.
9091560	9107560	And in the in the learning world or let's say really in the computation world we have the fact that you know I can write a very short piece of symbolic code discreet code that that can go and calculate the nth digit of Pi or something like that.
9107560	9115560	But it's almost impossible to train or it is currently impossible to train any neural network of fixed depth to do the same thing.
9115560	9126560	And so we have this this weird you know different regimes we have the discreet kind of logic reasoning type world and then we have the continuous differentiable type of world.
9126560	9148560	Do you view those as I mean are they fundamentally different regimes and we're always going to have hybrid systems that kind of combine both types of reasoning or is it possible to just say project that discreet computation fully into an analog type space if you just have enough you know parameters or something.
9148560	9170560	Yeah I think so I think it really depends on the resources that you have right because to me at least it seems that the hybrid system might be the most efficient where you can easily let's say cluster different settings into groups and then and for this you can just have a discreet settings and then within each group.
9171560	9176560	Discreetization is not good enough anymore and you need to go into the continuous regime.
9176560	9187560	So I think it will depend on the application you have at hand or or efficient you want to be doing it either energy wise or anything else.
9187560	9208560	So I think it's not not clear if one should dominate those are necessarily like for example in the if you go back to the spline setting of a deep net you have discreet setting which is basically your partition which which which region you are in and then within that region.
9208560	9217560	You have a linear transformation of the mapping which is basically continuous and both interact if you adapt one the other one change and so on.
9217560	9231560	And I think having this type of hybrid systems and where somehow learning through the continuous part adapts the discreet part is what is extremely powerful.
9231560	9246560	And that's I think one extremely beautiful property of current networks is that they do automatically this adaptive training of their discreet path through training of their continuous parameters.
9246560	9249560	And that's why they are so efficient.
9249560	9258560	If you think of pure approximation theory and you have an adaptive spline in one day that's why you have the best convergence rate basically.
9258560	9262560	So I think you really need both systems to interact.
9262560	9269560	If they don't interact then I think it's really easy to become suboptimal and interesting.
9269560	9277560	Because we put a similar question to Lacune and he was kind of saying that in an ideal world we would have a discreet system as well.
9277560	9282560	Humans are really bad at playing chess because we don't have that discreet system built in.
9282560	9291560	But the problem I think people like Lacune have with these discrete systems is typically they're symbolic and they're statically coded.
9291560	9299560	You could start talking about getting into a discreet program search and you could even guide that program search based on some deep learning model.
9299560	9310560	But I don't think to Keith's point I don't think it's really possible to do that well inside the continuous domain because if the problem even was learnable with stochastic gradient descent.
9310560	9315560	The representation would be glitchy. It just wouldn't work.
9315560	9322560	I think it depends a lot too on what are you trying to achieve with a model you build.
9322560	9333560	If you just try to be as close as possible to let's say what the human brain is doing then you might impose yourself to have some restriction on do you want it to be discreet or not.
9333560	9339560	Or if you just want to have a model that you can deploy on a task and it can solve the task as best as you want.
9339560	9350560	So I think depending on what is your goal and what are you trying to imitate with the model would change or you answer that as well.
9350560	9354560	But what if the goal was task acquisition efficiency?
9354560	9359560	So it's like I don't know what the task is yet.
9359560	9372560	Yeah I think that's like again to me a hybrid system where you have interaction between both parts intuitively would be the most efficient.
9372560	9378560	But yeah it might not be true for all settings.
9379560	9390560	When I was looking at figure three both Tim and I were interested in the fact that if you look at the MNIST data which to a human being is kind of a simpler data set.
9390560	9394560	One, two, three we know how to do that type of thing.
9395560	9409560	That as you increase dimensionality it much more rapidly becomes extrapolation versus image net which seems to kind of more slowly transition from interpolation to image extrapolation.
9409560	9421560	And what I'm wondering is the intuition I got from that and I wonder if this is completely wrong or it's correct is that for machine learning there's a sense in which MNIST is actually a harder problem.
9421560	9432560	Because it has to look at kind of global relationships like it has to try and say well there's a circle over here that's kind of oriented with respect to a line that's kind of further away.
9432560	9446560	And so it's harder for it to do that whereas we know that with like image net very frequently ML sort of devolves to looking at these micro texture like you know well everything that has this shade of yellow is a school bus you know type of thing.
9446560	9453560	Is that an intuition one can take from that plot or what does it mean that MNIST decreases so much more quickly.
9453560	9459560	Yes, so that's actually some things that we tried to clarify with a figure that comes after this one.
9459560	9468560	Basically the thing that you really need to be careful about and keep in mind is that if you look at for example a 16 by 16 patch for MNIST.
9468560	9476560	You have basically maybe most of the information you need to solve the task and you have a lot of texture about your DG etc.
9476560	9488560	If you look at image net and you take a 16 by 16 patch you have basically no information about the class it's extremely small patch it's almost constant across the spatial dimensions right.
9488560	9493560	It's basically a very small percentage of your full image.
9493560	9502560	So that's why MNIST goes much more quickly to extrapolation regime for a fixed dimensionality in pixel space.
9502560	9508560	Because the information you have in that amount of dimension is greater than for the image net case.
9508560	9513560	And this is only because it's already much more done sample right.
9513560	9526560	If we were looking at MNIST but with a 224 by 224 spatial dimension and we look at a fixed dimensionality then you will not have this difference anymore and it might even reverse.
9526560	9538560	So the really important thing to keep in mind is that even though it's the same dimension for both it does not represent the same proportion of image that is present within that patch.
9538560	9545560	And that's why you have those differences in that's mostly why you have those differences in those curves.
9545560	9560560	It's funny because we had the opposite intuition and so in the following figure 4 you're showing then on MNIST that more of the variance is explained with fewer of the principal components on MNIST.
9560560	9566560	And as you say that that does just because it's those pixels on MNIST are more salient for the problem.
9566560	9569560	I mean just so I understand because we were debating this a little bit.
9569560	9572560	Could you just give us your articulation of figure 4?
9572560	9587560	Yes sure. So basically what we are looking for the increasing dimensionality for the three data set is we pick a number of dimension in the spatial space.
9587560	9590560	So we do this by extracting a patch.
9590560	9596560	And then what we do is that we extract of course the same patch for all the samples.
9596560	9604560	And then we are looking at the proportion of the test set patches that are in interpolation regime and we report this.
9604560	9616560	Now for the PCA plot what we do is basically we look at once you extract this patch how many principal components you need to explain to perfectly reconstruct those patches.
9616560	9620560	Or you could say to explain the variance in those patches.
9620560	9627560	And this gives direct relationship because it shows how concentrated you are on lower dimensional manifolds.
9627560	9631560	A fine one of course that learns through the principal components.
9631560	9641560	And it means that if you can encode much more information with much less principal components then you lie on a lower dimensional affine manifold.
9641560	9648560	And this coupled with figure 1 shows that basically it's much easier to be in interpolation regime.
9648560	9661560	So the whole point of using this PCA plot was to show how good low dimensional affine manifold represents the current extracted patches.
9661560	9667560	And then to use this as a way to justify the extrapolation regime curve that we see.
9667560	9671560	So because again in the PCA regime it's linear manifold.
9671560	9681560	So if only two components for example perfectly describe all your patches then you will need very few training samples to be in interpolation regime.
9681560	9683560	Okay that makes sense.
9683560	9689560	And then the staircase effect on the smooth subsample row is a function of the size of the Gaussian filter used to smooth it?
9689560	9698560	Yeah so the staircase occurs basically whenever the number of dimensions increase and we get a new bigger patch to get it.
9698560	9702560	Because there is different ways to get it right.
9702560	9712560	One would be to always extract the center patch and remove some dimensions if you don't have exact number of dimensions that you can represent with a square patch.
9712560	9718560	Another thing you could do is to first smooth subsample and then remove the dimensions.
9718560	9722560	So there is different variants on how to extract those patches.
9722560	9729560	We try to show two different ones to show that the results do not really depend on how you do this process.
9729560	9734560	But yeah you will have some little different artifacts like this.
9734560	9741560	It does not change the overall trend but yeah it can change the small trends when you change from one dimension to another.
9741560	9745560	Well let me ask a question here about figure four.
9745560	9751560	Again about the intuition that we had on leaving aside interpolation and extrapolation for the moment.
9751560	9763560	It seems that MNIST for a given amount of variance explained and for a given dimensionality MNIST requires more principal components.
9763560	9766560	For a given amount of dimension yeah.
9766560	9774560	If we fix the dimension and we fix the percent variance explained MNIST requires more principal components than ImageNet.
9774560	9778560	That seems to me to tell me that MNIST is a more difficult problem.
9778560	9780560	Is that not true?
9780560	9785560	I think it's a bit so for a specific number of dimensions you could argue that.
9785560	9792560	But the thing you have to recall is that because MNIST images are much smaller in spatial dimensions.
9792560	9798560	If you have a 16 by 16 patch you have basically the whole MNIST dataset let's say.
9798560	9804560	And so just having a few principal components is not enough to really reconstruct the whole MNIST dataset.
9804560	9810560	Now on ImageNet a 16 by 16 patch is almost a constant texture right.
9810560	9813560	You have a few different colors but you don't have a lot of variation.
9813560	9818560	It's basically through the spatial dimensions it's basically constant.
9818560	9825560	And what this means is that with only three or four principal components basically one for each color channel.
9825560	9829560	You can perfectly reconstruct all those 16 by 16 patches.
9829560	9838560	So to really get to the conclusions you are saying what you will have to do first is to either done sample ImageNet to be 28 by 28
9838560	9842560	or up sample MNIST to be 224 by 224.
9842560	9850560	And if you do that then basically I think you will have the roughly same evolution of the interpolation regime.
9850560	9860560	Because the 16 by 16 patch on this extra up sample MNIST image will be either completely black or completely white.
9860560	9864560	And in this case you will still need a few principal components.
9864560	9869560	So this is also something very interesting right because it shows that maybe for the task at hand
9869560	9874560	you might not need to have such a high resolution image you might done sample.
9874560	9881560	But because when you done sample you basically keep those in crucial information
9881560	9884560	you don't necessarily go faster to interpolation regime.
9884560	9887560	So this is another point.
9887560	9895560	It's really tricky you should think of it as how much of the image do I uncut given that number of dimensions.
9895560	9901560	And then given that this plot might be easier to understand.
9901560	9904560	16 by 16 by each.
9904560	9909560	Yeah exactly so 16 by 16 on MNIST is maybe around 60-70% of the image.
9909560	9914560	While 16 by 16 on ImageNet is maybe around maybe 5% of the image.
9914560	9918560	And that's why you have those different regimes that appear.
9918560	9920560	Incredible.
9920560	9923560	Randall thank you so much for joining us this evening.
9923560	9925560	Sure sure thanks.
9925560	9928560	So we just spoke with Randall what was your take on that Keith?
9928560	9932560	Absolutely brilliant it was a true pleasure to speak with him.
9932560	9938560	For me it cleared up a lot of thoughts and issues I was having with this paper.
9938560	9951560	So for example right up front what he says is my purpose behind this paper is to show that even though the intuition that people have of interpolation
9951560	9956560	like the intuition that we have of interpolation is good.
9956560	9962560	The mathematical definition that we have of interpolation is not useful in high dimension.
9962560	9968560	What I thought was interesting too is when we asked about well what about the manifold concept.
9968560	9972560	You know why isn't that sort of the definition of interpolation.
9972560	9980560	And he brought up a really strong argument there as he said well let's just take the simple case of suppose the problem you're trying to solve
9980560	9988560	just is linear you know like it just everything is a linear data set linear problem we're trying to solve like even in that case
9988560	9994560	this linear case in high enough dimension interpolation doesn't work.
9994560	10002560	And there your manifold is just literally a convex hull you know and so sure you can have kind of a nonlinear transformation
10002560	10007560	and a nonlinear shape and whatever you're still hit by this curse of dimensionality.
10007560	10018560	And he you know he brought up the point that like you know of course if your problem compresses down enough to where only a small number of
10018560	10026560	of transformed dimensions right latent space dimensions matter and everything then you can be said that you're you're interpolating
10026560	10032560	you know because we're not really hit by that curse of dimensionality because we stripped away all the dimensionality down to these dimensions
10032560	10039560	we've gotten lucky our problem space or data samples etc allowed us to do that okay but that's not going to be all problems
10039560	10047560	like some problems may just intrinsically have high dimensionality of interpolation that's not useful.
10047560	10057560	So we need to do something better we need to we need to come up with a definition of interpolation that maintains the intuitive notion
10057560	10062560	that we have of interpolation but that continues to work in high dimension.
10062560	10069560	And you know and he made a very you know very interesting kind of end goal here which is like if we can get a definition of interpolation
10069560	10077560	that ends the approaching this concept of generalization you know that's that's what we're trying to achieve really.
10077560	10081560	And what do you think his take on generalization is then.
10081560	10087560	Well as he made it you made a comment that you know on the one hand you could just ridiculously specify the space and make it
10087560	10090560	trivially separable but then you lose generalization.
10090560	10098560	Well yeah and I think I mean because because we did ask or you asked you know put the question what if what you're what if the problem
10098560	10105560	you're trying to solve is the ability to solve novel problems like you know what happens in of course that was in the context of our
10105560	10114560	discussion about hybrid systems where you're combining the continuous with with discreet and they're able to ping pong and kind of modify each other.
10114560	10122560	And I think what we got from him there and consistently really throughout throughout our discussion.
10122560	10130560	And what's interesting is this this totally aligns actually with Francois Chalet as well which is that this all these questions like a lot of these
10130560	10136560	very difficult questions that we're asking are problem specific and there is no.
10136560	10146560	We don't currently have some one size fits all set of concepts that fits well for it for every every problem space you know it's very task specific
10146560	10150560	very problem specific you know data specific.
10150560	10158560	So I think that's an area where we got dive deeper with him but I didn't hear anything definitive you know today.
10158560	10162560	It's a wrap we just interviewed the Godfather of deep learning.
10162560	10163560	How's that possible.
10163560	10167560	I think we can just quit now we might as well just shut the channel down.
10167560	10169560	Yeah.
10169560	10173560	I mean obviously after we've published it.
10173560	10175560	That's the singularity.
10175560	10177560	So what's your take.
10177560	10183560	I would have I would I think I think this but we also interrupted him a little bit.
10183560	10189560	Maybe the question was I would have loved for him to be more a bit unlike you know where's the actual disagreement.
10189560	10190560	Right.
10190560	10199560	Because a lot of times you know when we when we put put you know questions to him like OK other people say this he was like oh yeah I agree right.
10199560	10208560	And this seems it seems to be a general sentiment that's also when we talk to other people about you know perceived disagreements they're always very
10208560	10220560	you know being being being good being academics being also friends probably with a lot of people you always like yeah you know they have they have good point you know we essentially agree on all the
10220560	10240560	on all the things right but then I sort of want to know where actually do people disagree right if it's if it's you know in and that that that is a little bit I mean I obviously have a feeling but it's still a little bit elusive and
10240560	10250560	and if people disagree on the actual technical nature or more on the philosophical end of what do what does something mean.
10250560	10258560	I definitely think there is from hearing now and I think from his answer to that question.
10259560	10267560	I could hear a little bit in that he seems to be more optimistic on what these learning systems can do.
10267560	10280560	Then maybe other people are right because some people seem to have really kind of a hard stop on like this will never be possible with like a learning system.
10280560	10290560	Whereas it seemed it seemed it seemed that he had sort of a more optimistic outlook and said of course they can't do it now right.
10290560	10303560	However you know we work on it we modify them you know we're we're figuring out how do we need to build these systems such that you know a learning system can conceivably.
10303560	10316560	You know do many of the things that people would call a kind of reasoning and I think that's why he went into you know let me give you an example of reasoning to sort of show look here you know.
10316560	10329560	Here is here is an example of a reasoning that neural networks already do and that means that something like reasoning in general isn't too far away.
10329560	10334560	And that's ultimately where the disagreement might be with other people.
10334560	10340560	I mean I think that's that was my take on his answer to our first question like the you know why'd you write this paper.
10340560	10348560	Is I think and I may go back and watch this but I think essentially what he was saying is you know sometimes people.
10348560	10354560	They have some definition in their mind whatever it is of interpolation and extrapolation.
10354560	10366560	And they come up with kind of an argument to say see the this this machine learning is doing interpolation and in a binary sense it can't do extrapolation.
10366560	10378560	And so therefore there's this entire class of things you know that it's not capable of doing and I think that's what his objection was which is look like this is not a useful distinction to say between.
10379560	10394560	You know it's not a useful distinction to draw between quote unquote interpolation and extrapolation like here's an example of a definition right that's a relatively standard definition of interpolation and if we apply this interpolation to.
10394560	10400560	Theoretically what machine learning is doing and empirically to what machine learning is doing it's always extrapolating.
10400560	10407560	So I think that's what he's objecting to is just like look don't come to some strong conclusion about what.
10407560	10418560	Machine learning neural networks I'm not sure what the right way to phrase things is now what it's capable of doing like we just need to do more work to expand its capabilities.
10418560	10428560	Folks like Gary Marcus making this case of we need discrete models because they can abstract broadly they give a couple of examples you know binary encoding for numbers.
10428560	10436560	And there's one example with can you have a model that reverses the bits or I think there was another example in his algebraic mind which was about can you.
10436560	10446560	Generalize from the even numbers to the odd numbers and you can't do that and the reason you can't do it is the new examples are completely outside of the training space of the input data.
10446560	10451560	And probably that problem is not interpolative or it's not differentiable.
10451560	10461560	I mean what lead sub is the same thing he says in language processing in particular can like language understanding in order to have broad generalization you need to have abstract rules.
10461560	10470560	So for example being able to generalize from Mary loves John to Mary loves Jane they make the argument that with a statistical approach that just wouldn't be possible.
10470560	10489560	Well I think so I think and this is just my opinion but I think his answer to my digits a pie question kind of shed some light on what his position is on this which is that it's too early to theoretically and scientifically for us to make that that determination
10489560	10500560	because he said look okay people only been able to calculate digits of pie you know like within the last hundred years or whatever I'm not talking like we're not there yet I'm not talking about that class of problems I'm talking about these.
10500560	10501560	Try numbers as well.
10501560	10511560	Right so I'm talking about these things like a cat jumping up here and so some of these examples that Gary Marcus may bring up may fall into that even though they seem simple okay they seem deceptively simple.
10511560	10528560	They may still fall in the bucket of these kind of much more towards the discrete spectrum of capabilities which we can't currently do with with our methods and machine learning but we're going to continue and continue to get closer to that that into the spectrum.
10529560	10539560	And so it's and so it's premature to say that like machine learning and again whatever you can see that to be neural networks whatever will never be able to get there.
10539560	10563560	Like he and he and Jan is optimistic that we'll be able to get there I'm not as optimistic I think there is a very there's just a qualitative difference between you know structural topological discrete reasoning and continuous differentiable reasoning and I don't know how we're ever going to get that gap bridge to brought up some some methods to think about but.
10564560	10568560	Right but the key question is whether it's possible at all.
10568560	10578560	And there was an article by a guy called Andrew Yee I think and it really annoyed you and the Coon and he said look people are still making the argument that neural networks interpolate and in that article.
10578560	10589560	He was basically saying well neural networks are a little bit like a Taylor series approximation you take a function and you just kind of approximate it you know inside a certain range and then it goes haywire outside of the range.
10589560	10595560	And Francoise Relay came on the podcast and he said look you know it's a little bit like in Fourier analysis when you try and you kind of like.
10595560	10604560	You take these little signs and cosines and you fit it to a discrete function and it's glitchy right and actually if you look at harmonic analysis this is what.
10605560	10620560	As you and Bruno said on the show on the on the geometric show he said that what a neural network even the universal function approximation theorem it's all about stacking these basis functions together right to kind of approximate some target function well if that's all neural
10620560	10622560	networks do how could they possibly generalize.
10623560	10630560	Look I don't want to use the word like generalized I don't know that what I would say is this which is that.
10630560	10642560	No matter how discreet in appearance the human brain is you know all the signals that the neurons generate receive whatever you know at the end of the day.
10642560	10648560	Are continuous functions I mean they're you know a charge that has a continuous function.
10648560	10656560	Now somehow or another like the brain in its structure does take that continuous analog.
10656560	10672560	I should have said analog you know probably there were but it takes that analog computational environment and produces digital reasoning so I don't think it's beyond that like I just don't see how anyone could reach the conclusion that it will be impossible like to do.
10673560	10687560	Discreet reasoning with with neural networks I think it's just a question of like for me I think is more of a question is that the efficient way to do it like you know if you fast forward 2000 years from now when people have figured out all these problems and we have like you know
10688560	10707560	just walking around killing us or working for us whatever the thing may be you know are they going to be using a neural network for everything or is it going to be a neural network with you know some classic digital compute components with some other stuff like a hybrid kind of structural system that does things I think is more what I'm asking.
10708560	10725560	Like Lacan hint and they of course that that's what I was going to say they have like nature on their side in that in no matter how much we cut open the human brain we don't find like a discrete computer in there like of course the individual neural spikes are discrete like there's
10725560	10746560	sort of charge or no charge but then there is like continuous release of neurotransmitter so I agree like the brain is like a very continuous distributed machine and there is no no there's no discrete thing in there there's no part of the brain where it's like look one one brain brain
10747560	10772560	he said that and there is a level at which that's true which is there's a spike train and you know we can kind of recognize a spike because it's a maximum in this analog dimension but my point is that still an underlying analog dimension and so I don't see why in principle you couldn't build a neural network that you know has like these kind of continuous
10772560	10784560	values but still ends up with something that that's that synthesizes a a discrete you know decision sure but wouldn't it be glitchy and it still wouldn't extrapolate.
10785560	10794560	I have no idea like what you know I just don't know I think I think this kind of point is we're too early on to reach these conclusions that it cannot be done.
10794560	10809560	I do sort of feel that it's just not the efficient way to do it that you know we're going to end up with a case where we're going to have systems that have analog continuous chunks that are neural networks or whatever and we're also going to overlay that with
10809560	10826560	with digital computation that's implemented by the typical kind of digital computer that's going to be these hybrid systems working together just like you brought up with hey look you know alpha alpha zero alpha go all those things or it can be viewed in the same kind of hybrid way right.
10826560	10836560	But that's exactly I mean you said this is what you said and also what when he when he said like so first when he said you know we're far away from that.
10837560	10845560	With the digits of pie and so on but also when he said you know humans are actually pretty bad at chess or at discreet exploration in general and that is.
10846560	10851560	That's how humans do it right humans build discreet reasoning.
10852560	10865560	On top of this sort of neural continuous function and it's actually really hard like to do discreet reasoning in your head is you can do it slowly and you have to do it deliberately with attention.
10865560	10872560	If you do like multiplication of five digit number in your head right this is not a this is not a oh my god feeling.
10873560	10876560	This is like you like you sit there and you have to like keep all the stuff.
10877560	10891560	It's not a hash table look up like the multiplying numbers of the thing and we can like we can build children's toys like that for like 20 cents that multiply 10 digit numbers easily right so I like yeah I think this is.
10891560	10901560	Yeah I think this is we teach children point that way is we teach them hash table look up like memorize you know yeah all the numbers from one to a hundred multiply or something and then we teach them an algorithm.
10902560	10920560	You know we teach them this exact thing and we teach them hash table look up and and this is built on so if you and I think that if you build if you build this discreet reasoning you can probably build it on top of these models right but yes is it the most efficient way to do it.
10921560	10925560	Maybe maybe not right but then again if you do it.
10926560	10941560	If you manage to do it train it you seem at least from at least from from the perspective of humans you seem to have something insanely powerful right because you know if you just have a discrete algorithm you have the algorithm but if you have a.
10941560	10959560	Discrete if you've actually managed to train a discrete algorithm on top of this continuous function it will be able to sort of learn in the same continuous way that you know a neural network learns but you know it will be sort of be able to self modify its discreet algorithm and that.
10960560	10969560	Yeah I think you know I agree I'd also be in the maybe longer term future positive that might be possible.
10969560	10977560	I think that's because I said to him and do you think alpha goes a good thing do you think is a good thing to have a kind of.
10978560	10991560	Discreet search which is guided by a neural network and he didn't like it and I assume he didn't like it for the reason you just said you and it which is that the discrete part of it is hard coded and it's been told to do a specific task it's it's not learning.
10991560	10997560	I think look who would rather have something which is entirely differentiable right so because.
10998560	11006560	Look who wants learning end to end in a in a in a neural network right yeah and and also it's almost like he was saying before well.
11007560	11021560	Humans can't do discrete reasoning so it would actually be a more sophisticated form of intelligence if we had discrete reasoning but do we necessarily even want that in our models or is it just the fact that he wants it in the models but he wants it to be differentiable and learnable as part of the main model.
11022560	11030560	I think so because that's where he got with the you know the critic actor critic type models I think if people nowadays talk about differentiable they always.
11031560	11044560	Talk about sort of they always talk about like single shot feet forward like you know I input something into the machine and outcomes of prediction whereas he also makes a lot of arguments for you know this.
11045560	11057560	Energy minimization which which means that essentially at test time at inference time I do a minimization algorithm and you know when he when he talked about reasoning he was mentioning that.
11058560	11071560	As an example as essentially saying look I have my trained neural network and now at inference time I actually still perform an algorithm on top of that right to minimize some energy function given my trained model of the world let's say so.
11071560	11087560	I think you know we might be able to make that more learned by also learning the algorithm that we do at inference time but I don't necessarily I don't necessarily think that he's talking about you know we need.
11088560	11098560	Like we need to end to end learn like a machine where you simply input something and then out pops through forward prop out pops and all of these things are just reasoning by the back door.
11099560	11105560	It's even with a critic as I understand that that's just the way of hacking that the value the advantage function right you know with some.
11106560	11110560	I'm not an expert in reinforcement learning but I think that's what it is the same thing with the energy minimization.
11111560	11112560	Yeah I kind of agree with you there.
11113560	11121560	Yeah but the energy minimization has this particular thing of this idea which I think today is underexplored of to actually do something at inference time.
11122560	11129560	Like to not just forward prop and that I feel it's I feel it's under underexplored nowadays.
11130560	11148560	I agree with you there too but I think what kind of sometimes I feel like this is feature creep in a sense it's like you know we have neural networks and we know what deep learning is and some people now want that to be redefined it's just a general purpose research paradigm that includes all possible things that we can do with.
11148560	11166560	You know machines or chemicals it's like what use is that like what use is it defining you know all types of computation is as differentiable computation like we lose some ability to talk about these and sort of the same way that was kind of like what I was saying you know nobody I mean look.
11167560	11183560	Allowing for a variable number of layers in a neural network that's discrete computation right that's not differentiable training nobody knows how to train in like an arbitrarily unbounded number of layers in some differentiable way right.
11184560	11186560	But there's the neural ODE stuff.
11187560	11203560	Well all those types of things run into big training problems like things become really hard to train when you have these types of yeah of essentially discrete you know transitions right or combinatorial kinds of kinds of transitions.
11204560	11219560	And this is classic stuff I mean this is really classic stuff it's like even if you try to do mixed integer optimization right so you have a problem that has some combination of integer variables discrete variables and some combination of continuous variables.
11219560	11223560	There's all kinds of hacks to try to do that in differentiable ways.
11224560	11238560	They don't in general arrive at the optimal discrete solution like you do some continuous stuff and then at the end you kind of discretize it in one way or another and you wind up with a solution that's kind of approximately correct.
11238560	11247560	But you're not guaranteed that you're going to find the one that's actually correct if you discreetly combinatorially examine that that space right.
11248560	11256560	Can we finish by talking about a couple of things so first of all there's table one and the structure of the latent table one is the biggest.
11257560	11271560	The biggest argument against the most prevalent argument against this paper sorry for the triple negative but you know when when you know when the paper starts out by saying you know we build a convex hull of the training data.
11271560	11284560	If your point is not in the convex hull you know not interpolating and then people go wait wait wait wait wait but you're talking about the input space of data you're talking about the pixel space and you know for sure we're not talking about the pixel space.
11284560	11297560	Because you know the neural network is you know the data manifold and who but we're talking about you know if you go to the last layer to the latent space before the classification it's just a linear class.
11297560	11306560	You know there in that space we're talking about like interpolation right there is where the neural networks interpolate in this experiment clearly shows like.
11306560	11321560	Okay might be a rigid definition of interpolation but it clearly shows like no even in that space on a trained Resnet there is no like no interpolation going on as soon as you pick like 30 dimensions or more.
11321560	11334560	It's all outside of the convex hull of training right but but this this this gets to the punchline right because Keith was going to ask a really cool question which is like well imagine it was a very small latent and it was interpolating.
11334560	11343560	And then what if you up projected it like to one or two four dimensions and that you know with with that suddenly now be extrapolating right so that we almost need to have an information theoretical.
11343560	11356560	Theoretic way of looking at this but anyway but my intuition is that deep learning models encode the most high frequency information into the latent space right and you know this information would be encoded in a minimally distributed way.
11356560	11361560	To denoise the predictive task which is to say right you know there's a few dimensions of the latent.
11362560	11377560	You know which should be encoding the actual things that that you trained it on so my intuition is actually most of the dimensions of the latent are kind of just encoding low frequency information so you can discard them right and I know you said the other day well
11377560	11385560	Well maybe the whole point of neural networks is there a distributed representation so maybe they are distributed overall of the layers overall of the dimensions in the latent.
11385560	11403560	I mean I would I would actually argue not that neural networks are putting the data in a minimally distributed way but in like a maximally like why I think just just my intuition is that a neural if if I were a neural network and I had all of these dimensions at my disposal.
11403560	11409560	I would encode lots of information redundantly if it like if it were too much space.
11409560	11424560	I would like encode the same information pretty much redundantly in many of the dimensions that I could noisy right so you're taking a soft max and if you're noisily aggregating overall of those features that you don't want to do that.
11425560	11436560	Yeah but still if I have backprop right and the backprop path goes through each of the dimensions so for each of the dimensions I'm asking myself how can I change this one to make my prediction even better right.
11436560	11451560	It doesn't matter if over there one of the dimensions is already doing the classification for me right the backprop the nature of backprop means that you know I'm going to every single dimension independently and deciding how can I change this one.
11452560	11467560	To make it even better so that's why I think lots of information if the latent is too big lots of information will be encoded right there is if the latent is too big and so this is where all the pruning literature comes into which is that.
11467560	11473560	The majority of the time people are running neural networks and situations where the latent space is too big.
11473560	11493560	Like they're just you know we just flat out have far too many you know far too large of a latent space and so even though it may be encoded you know densely there like it may be putting all kind of little bits of extra information it's probably only adding little plus or minus you know 0.1% type things to the accuracy.
11493560	11499560	Whereas and so since it is only adding these little kind of very tiny values there right.
11499560	11513560	The only meaningful way to talk about interpolation or extrapolation because you've only got little bits of stuff you're using on that entire dimension that you've added there is really more just are you interpolating on the most salient dimensions.
11513560	11525560	You know which is again back to my question about why are we you know why are we concerned with whether or not every single dimension falls within within the sample range.
11525560	11535560	Yeah exactly but there's a few things that I mean it's really good that you bring up sparsity I think that's fantastic that you brought that in because as you say most of that information is redundant.
11535560	11546560	But then what Yannick was saying was interesting does distributing all that information rather than my intuition is it increases noise but actually I think Yannick saying it increases precision.
11546560	11556560	But in a very tiny tiny like it's a tiny tiny contribution so let's suppose the latent space is not too large right like if the latent space is not too large.
11556560	11571560	In other words it's let's say it's just barely big enough to classify your images and let's suppose you're doing multi class so we've got 10 10 10 classes you know and we just barely got enough latent space for it.
11571560	11586560	My intuition would tell me that if those classes are somewhat different from one another like it's not we're classifying brown dogs from white dogs from like every other you know simple kind of dog but they're different from each other and it's not easy to determine.
11586560	11604560	You know which is which that what would and just just to guess is it probably what would happen is you would find that you know these five bits are kind of the ones that tell you whether something is a dog and these seven bits tell you if it's a duck and these four bits tell you if it's a gun and these five
11604560	11619560	bits tell you if it's an image of the sky like I would guess that it would end up encoding it that way since they don't have a lot in common you can't really you can't really use entanglement like too much except for a few dimensions there might be some overlaps but.
11619560	11637560	Yeah I think I think what what people what people mean a little bit is when they when they start going down the interpolation road is that you know we've played with GPT three as well right and then you you do something with it you enter something and in some places you're like.
11638560	11653560	I see how you did that right you like you just you just took that that newspaper article and you just kind of replace some stuff in it right like you you sort of where whereas if you were to to talk to like a human you you sort of.
11654560	11667560	Like that stuff wouldn't happen as much even and then of course there's there's the argument I think people that say well it's just interpolating or it might also be you know it's just sort of repeating the stuff in the training data.
11668560	11684560	I think what they what I'd like to see is more like the pattern that these models extract aren't sufficiently high level for for them right and then I think the entire discussion is can we get to.
11685560	11709560	Arbitrary high abstract levels of pattern recognition with such models if we engineer then train them in the correct way or is there is there I guess some fundamental limitation to that and yes as we said that the answer the answer might be might be quite far off as as for the.
11710560	11716560	Number of you know latent dimensions and so on I I mean I agree with with Keith I think.
11717560	11727560	Having a big latent space and also having big weights and so on is might be more of a of a necessity for training.
11728560	11745560	Then it really is for for encoding like it appears to help if we have lots of weights to train such that sort of we get to combinatorically try out combinations of weights only few of which might ultimately end up being important right.
11745560	11754560	That's kind of a lottery hypothesis sort of way of going down so yeah I agree you know most of most of the information ultimately might be.
11755560	11773560	Only contributing a little bit right but my my my intuition would meet be that you know this is kind of because it's kind of redundant information right because it's like you know I'm encoding this over here I'm also encoding it in like a tiny little different way over here.
11774560	11781560	Or or some add on or some uncertainty or some one training samples a bit different so I'm going to put that right here.
11782560	11801560	Yeah that's fascinating what you said about Frank whose work that I never really heard you articulate it like that but it is actually kind of like a search problem rather than a learning problem what you're doing is you're giving it all of the possible you know you give a random initialization on a densely connected network and you're saying you know just go and find the ones that work rather than create it from first principle.
11802560	11819560	Yeah that's why initializations are so important right is is because you sort of you sort of try like your initialization essentially is is your your your buffet for SGD to like choose a good one from to then go ahead and refine.
11820560	11824560	But how much refining is it is it more is it more finding things that already work versus refining.
11825560	11846560	Yeah it's it's it's well the same but what it is not is sort of learning from scratch that that's what like people like we cannot you cannot initialize neural network from zeros and then have it have it learn well at least not today maybe that's going to come in in some form but initialization is actually pretty
11846560	11868560	important pretty crazy and that's crazy and that's yeah that's like one hint that you know we're we're not essentially we're never essentially training from scratch where we're we're sort of training we're sort of giving the choice of many combinations of good weights or of semi good weights and it has to pick sort of the good ones to continue exactly it's already pre trained you just don't know which.
11869560	11889560	Yeah I mean that the argument that a little bit of the argument against that is in in sort of the evolutionary approach where you say you know you can make the argument you know humans have sort of develop these abilities to reason to recognize super high level patterns while only having a continuous brain.
11890560	11908560	But then the other side of this is yeah but it's not like a single human that has achieved that right it's not like one single learning system that has achieved that but it's actually like this evolutionary system which is in essence a massively distributed combinatorical.
11909560	11920560	Trial and error search right and that is that is not a learning system so to say as we imagine it today it at the end you end up with a learning result but.
11921560	11937560	The evolutionary algorithm is way different than we imagine learning it's it's not even all it's not even all humans that have learned it it's it's all you know individuals of all tens of millions of species that have ever lived on earth that have that have learned.
11938560	11951560	Which is pretty much like like the lottery ticket hypothesis let's just let's just randomly train crap tons of you know weights and then slowly prune them and see what happens.
11952560	11960560	Right brilliant well gentlemen it's been an absolute pleasure I guess I guess we'll make this a Christmas special I mean it is pretty special.
11961560	11966560	Oh yeah let's be honest so yeah this is gonna warrant a hat we're gonna sign off.
11968560	11969560	Nice meeting you.
11970560	11978560	Indeed but thanks for bearing with us folks we have had a couple of months off I've had a bit of a break and you know the guys have had a bit of a break so yeah we're back.
11979560	11980560	Good to see you all again.
11981560	11982560	Peace out.
