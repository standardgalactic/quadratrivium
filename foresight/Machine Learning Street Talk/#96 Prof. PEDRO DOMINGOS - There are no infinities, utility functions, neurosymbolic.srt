1
00:00:00,000 --> 00:00:05,720
Hi, I'm Peter Domingos, I'm a professor of computer science at the University of Washington

2
00:00:05,720 --> 00:00:11,240
and machine learning researcher, probably best known as the author of the master algorithm,

3
00:00:11,240 --> 00:00:14,480
popular science introduction to machine learning.

4
00:00:14,480 --> 00:00:20,680
I'm here at NeurIPS 2022 of the vast amounts of stuff that's happening here.

5
00:00:20,680 --> 00:00:26,020
The two that I found most interesting and are closest to my own research are Neurosymbolic

6
00:00:26,020 --> 00:00:28,520
AI and symmetry-based learning.

7
00:00:28,520 --> 00:00:33,440
Okay, Professor Pedro Domingos, it's an absolute honor to have you back on the show.

8
00:00:33,440 --> 00:00:38,840
Pedro is a professor at the University of Washington and give us a quick introduction

9
00:00:38,840 --> 00:00:44,880
to yourself, to your experience here in Europe so far and what's top of mind for you?

10
00:00:44,880 --> 00:00:48,680
I'm a machine learning researcher, I've worked in most of the major areas.

11
00:00:48,680 --> 00:00:54,160
I've also written a popular science book on machine learning called the Master Algorithm.

12
00:00:54,160 --> 00:00:59,200
I'm having a lot of fun here at NeurIPS, listening to various talks like David Chalmers

13
00:00:59,200 --> 00:01:05,320
on Consciousness and Geoff Hinton on Sleep and looking forward to the rest of it.

14
00:01:05,320 --> 00:01:10,480
Awesome, I'd love to get your thoughts on Chalmers in a bit actually, but the first

15
00:01:10,480 --> 00:01:16,000
thing I wanted to talk about just because it's top of mind is this whole galactica situation.

16
00:01:16,000 --> 00:01:20,360
So first of all, I was speaking with Ian the other day and I think it's a little bit unfair

17
00:01:20,360 --> 00:01:23,720
that Meta really bear the brunt of this.

18
00:01:23,720 --> 00:01:29,440
OpenAI have just released this new chat GPT bot which suffers from similar failure modes

19
00:01:29,440 --> 00:01:34,240
and it just kind of feels that they're not getting anywhere near as much stick as Meta is.

20
00:01:34,240 --> 00:01:42,040
Well I think, I agree with you, I think the brouhaha about galactica is way overblown.

21
00:01:42,040 --> 00:01:44,480
That system is really largely harmless.

22
00:01:44,480 --> 00:01:48,400
It's just another large language model that's designed for actually something that to me

23
00:01:48,400 --> 00:01:50,960
as a scientist is very interesting.

24
00:01:50,960 --> 00:01:55,560
I would love to have a system like that to help me out with certain things and I think

25
00:01:55,560 --> 00:02:00,920
it's a step in the right direction and I think the brouhaha however is an instance of people

26
00:02:00,920 --> 00:02:06,280
jumping the gun on a lot of these AI things in a way that to me is very excessive.

27
00:02:06,280 --> 00:02:11,400
Having said that in a way they set themselves up for it in a way that they need and have,

28
00:02:11,400 --> 00:02:17,560
they kind of over claimed what it did and the problem with these LLMs is that they generate

29
00:02:17,560 --> 00:02:22,560
a lot of stuff that looks good but can be completely wrong and in a way there's no worse

30
00:02:22,560 --> 00:02:25,600
place to do that than in writing scientific articles.

31
00:02:25,600 --> 00:02:30,320
So when they came out with it, they should have been more careful about how they frame

32
00:02:30,320 --> 00:02:31,320
it.

33
00:02:31,320 --> 00:02:34,520
I think they took concerns sort of like this competition and one upping each other on who

34
00:02:34,520 --> 00:02:39,800
comes up with the frilliest demo and that kind of backfired.

35
00:02:39,800 --> 00:02:45,200
So they shouldn't have had to withdraw it.

36
00:02:45,200 --> 00:02:50,840
I think that's all pathetic and hopefully they've learned the lesson that next time

37
00:02:50,840 --> 00:02:52,520
they will do it slightly differently.

38
00:02:52,520 --> 00:02:53,520
Yes.

39
00:02:53,520 --> 00:02:54,520
Okay.

40
00:02:54,520 --> 00:02:55,520
Okay.

41
00:02:55,520 --> 00:02:59,880
Well, so Gary Marcus has been very loud about this on Twitter so he's really pushing

42
00:02:59,880 --> 00:03:05,160
the point about misinformation and the thing is as well I don't want to characterize the

43
00:03:05,160 --> 00:03:09,160
ethics folks as having monolithic views because they don't have monolithic views and I also

44
00:03:09,200 --> 00:03:12,880
think that a lot of the ethics guidelines for large language models are very reasonable.

45
00:03:12,880 --> 00:03:16,840
Like I interviewed the CEO of Coheir the other week, Aidan Gomez.

46
00:03:16,840 --> 00:03:20,800
I went through their terms and conditions and policies all very reasonable.

47
00:03:20,800 --> 00:03:23,360
The only sticking point for me is the misinformation one.

48
00:03:23,360 --> 00:03:31,720
I think the kind of the moral valence of it is in its use and especially with misrepresentation.

49
00:03:31,720 --> 00:03:35,360
I don't like this paternalism telling me what's good for me.

50
00:03:35,360 --> 00:03:38,160
I've just lost out on using a really cool tool basically.

51
00:03:38,840 --> 00:03:39,880
I completely agree with you.

52
00:03:39,880 --> 00:03:41,120
I would take it even further.

53
00:03:41,120 --> 00:03:46,400
I do not want other people deciding for me what is misinformation and what is therefore

54
00:03:46,400 --> 00:03:49,200
allowed to be said because it's misinformation or not.

55
00:03:49,200 --> 00:03:50,200
For a couple of reasons.

56
00:03:50,200 --> 00:03:54,040
One is that these people who claim to be big critics of misinformation, a lot of them

57
00:03:54,040 --> 00:03:56,680
are misinformers themselves.

58
00:03:56,680 --> 00:04:02,120
And the bottom line is that you always have your ideology that informs what you think

59
00:04:02,120 --> 00:04:04,000
is true and false.

60
00:04:04,000 --> 00:04:10,520
And I don't want anybody, every one of us in a democracy should be deciding for themselves

61
00:04:10,520 --> 00:04:13,800
what is true and what is false and what is valid and what isn't.

62
00:04:13,800 --> 00:04:20,320
And I have no fear of attempts to misinform me as long as I have a multiplicity of sources.

63
00:04:20,320 --> 00:04:26,360
The biggest misinformation danger is when you have only one monolithic source of truth,

64
00:04:26,360 --> 00:04:30,760
whatever it is, which is unfortunately what a lot of these anti-misinformation people,

65
00:04:30,760 --> 00:04:33,720
I think consciously or unconsciously want.

66
00:04:33,720 --> 00:04:36,800
Give me 10 things, 9 of which are misinformation.

67
00:04:36,800 --> 00:04:39,960
I can do the job of figuring out which one I think is valid.

68
00:04:39,960 --> 00:04:43,880
Give me only one of those things and chances are 9 in 10 that it is misinformation and

69
00:04:43,880 --> 00:04:45,640
then I have no chance to overcome it.

70
00:04:45,640 --> 00:04:49,880
So this whole attack on things because they're misinformation.

71
00:04:49,880 --> 00:04:55,120
And I mean, I understand the impulse that like, why have all this falsehood flying around?

72
00:04:55,120 --> 00:04:59,080
But the way to overcome that falsehood is not by censoring it.

73
00:04:59,080 --> 00:05:00,880
You should know this, right?

74
00:05:00,880 --> 00:05:05,280
You should be having to refight all of this over again in the context of social media

75
00:05:05,280 --> 00:05:09,200
and large language models and so on.

76
00:05:09,200 --> 00:05:13,000
So you said something really interesting, which is that this notion of a pure truth

77
00:05:13,000 --> 00:05:19,560
or a monolithic truth, and there's this concept of epistemic subjectivity, right?

78
00:05:19,560 --> 00:05:25,240
Or things observe a relative, even complex phenomena like intelligence.

79
00:05:25,240 --> 00:05:26,480
No one understands what it is.

80
00:05:26,480 --> 00:05:28,440
You can't reduce it to one particular thing.

81
00:05:28,440 --> 00:05:29,960
People have different views on it, right?

82
00:05:29,960 --> 00:05:35,440
So this notion that there is a pure monolithic truth of the world, I think is horrifying.

83
00:05:35,440 --> 00:05:39,880
Well, I would put it slightly differently.

84
00:05:39,880 --> 00:05:44,000
So first of all, there's a question, is there one reality or not, right?

85
00:05:44,000 --> 00:05:47,080
Is there truth or is there my truth and your truth, right?

86
00:05:47,080 --> 00:05:51,520
And actually, I understand the impulse to talk about my truth and your truth, but I

87
00:05:51,520 --> 00:05:54,080
think as a...

88
00:05:54,080 --> 00:05:55,200
So what is really true?

89
00:05:55,200 --> 00:05:56,360
We don't know.

90
00:05:56,360 --> 00:05:57,760
But as a...

91
00:05:57,760 --> 00:06:03,880
I think the most useful, including socially useful working hypothesis is that there is

92
00:06:03,880 --> 00:06:08,680
a single reality and the single truth, but it's extremely complex.

93
00:06:08,680 --> 00:06:11,440
So no single one of us can get at it.

94
00:06:11,440 --> 00:06:15,280
So what we need is many different people coming at it from different angles.

95
00:06:15,280 --> 00:06:18,960
But with the premise that we need to try to make these things consistent.

96
00:06:18,960 --> 00:06:23,640
So just saying, oh, we have different truths and there's no reality, that is actually very

97
00:06:23,640 --> 00:06:28,080
counterproductive because it gives everybody a pass to just believe whatever wacky thing

98
00:06:28,080 --> 00:06:29,080
they want.

99
00:06:29,080 --> 00:06:32,840
And then the consequences of that when you have to make the real decisions are very bad.

100
00:06:32,840 --> 00:06:36,640
At the same time, I agree with you.

101
00:06:36,640 --> 00:06:41,080
If I think that I have access to that truth and everybody just needs to, you know, count

102
00:06:41,080 --> 00:06:42,680
out to it, that is very dangerous.

103
00:06:42,680 --> 00:06:46,960
So I think we need to entertain these two ideas that there is a truth, but it's very

104
00:06:46,960 --> 00:06:49,680
complex and no one has a monopoly on it.

105
00:06:49,680 --> 00:06:54,360
And the key is, you know, like objective truth is what different observers can agree on.

106
00:06:54,360 --> 00:06:56,600
And now we can figure out what it is that we agree on.

107
00:06:56,600 --> 00:06:59,280
And that way we make progress in understanding reality.

108
00:06:59,280 --> 00:07:03,800
And we also tend to make more of the right decisions because we're closer to the truth.

109
00:07:03,800 --> 00:07:04,800
Okay.

110
00:07:04,800 --> 00:07:09,600
But do you see it as, I mean, I think the reality thing is interesting, but do you just see

111
00:07:09,600 --> 00:07:12,280
it cynically as gatekeeping?

112
00:07:12,280 --> 00:07:14,520
As in having a clerical class control?

113
00:07:14,520 --> 00:07:15,520
Oh, absolutely.

114
00:07:15,520 --> 00:07:16,520
No, absolutely.

115
00:07:16,880 --> 00:07:22,960
That's precisely the danger that I was referring to is that if you, let me put it this way.

116
00:07:22,960 --> 00:07:27,920
If ever there is, and this is a commonly mooted proposal, right, and not even proposed like,

117
00:07:27,920 --> 00:07:33,280
are we going to have a truth commission of people who decide what is true on whatever,

118
00:07:33,280 --> 00:07:35,160
Twitter or something, right?

119
00:07:35,160 --> 00:07:40,760
That is a really alarming thing because there is no commission that can do that.

120
00:07:40,760 --> 00:07:44,120
What they're going to do is they're going to impose their version of reality on everybody

121
00:07:44,120 --> 00:07:47,640
else, which unfortunately is what a lot of these people want to do.

122
00:07:47,640 --> 00:07:50,600
They convince that they have the truth and they want to impose it on the rest of us.

123
00:07:50,600 --> 00:07:52,040
And that is really alarming.

124
00:07:52,040 --> 00:07:55,200
We know historically what happens when people succeed in doing that.

125
00:07:55,200 --> 00:07:56,200
Yes.

126
00:07:56,200 --> 00:07:57,200
Yes.

127
00:07:57,200 --> 00:08:00,480
But I suppose my point with the gatekeeping is it almost gets you to the actual truth

128
00:08:00,480 --> 00:08:02,120
of the matter is irrelevant.

129
00:08:02,120 --> 00:08:03,760
It's actually about power.

130
00:08:03,760 --> 00:08:07,920
But what's your take on, I don't know whether you think this is putting it too strongly,

131
00:08:07,920 --> 00:08:13,800
but this being a form of industrial kind of gaslighting, kind of, you know, in an Orwellian

132
00:08:13,800 --> 00:08:19,000
sense, trying to shape people's reality through, you know, language, culture and interactions

133
00:08:19,000 --> 00:08:20,520
on the internet.

134
00:08:20,520 --> 00:08:23,600
I think a lot of it is deliberate.

135
00:08:23,600 --> 00:08:29,720
Some of it is, I mean, I'm an optimist about human nature at the end of the day.

136
00:08:29,720 --> 00:08:32,720
Maybe in, you know, maybe with justification, maybe without.

137
00:08:32,720 --> 00:08:38,200
I think so there's this postmodern view that it's all about power and it's certainly partly

138
00:08:38,200 --> 00:08:39,200
about power.

139
00:08:39,200 --> 00:08:44,600
I think a lot of the people doing this, unfortunately, or maybe fortunately, they are, they're not

140
00:08:44,600 --> 00:08:46,880
seeking power for its own sake.

141
00:08:46,880 --> 00:08:49,760
They have a set of beliefs that they think is right.

142
00:08:49,760 --> 00:08:53,120
And then the means, you know, they unjustify the means, right?

143
00:08:53,120 --> 00:08:54,120
That's the problem.

144
00:08:54,120 --> 00:09:01,480
So that gatekeeping, you know, and that gaslighting happen not because, not for their own sake,

145
00:09:01,480 --> 00:09:02,880
they happen for the sake of a cause.

146
00:09:02,880 --> 00:09:07,080
And now there's two problems with this is that these days the causes on behalf of which

147
00:09:07,080 --> 00:09:10,440
this is being done, in my view, are largely wrong, right?

148
00:09:10,440 --> 00:09:12,160
But whether they're right or wrong, right?

149
00:09:12,160 --> 00:09:15,480
The problem is that this is just noxious in its own right.

150
00:09:15,480 --> 00:09:21,840
And also then a lot of sort of like, again, personal desire for power and promotion and

151
00:09:21,840 --> 00:09:22,840
prevailing over others.

152
00:09:22,840 --> 00:09:25,080
Then of course, you know, hitches are right onto this.

153
00:09:25,080 --> 00:09:26,080
Yeah, interesting.

154
00:09:26,080 --> 00:09:30,000
I mean, we'll get into consequentialism in a minute because I think there's quite an

155
00:09:30,000 --> 00:09:31,720
interesting journey we can go there.

156
00:09:31,720 --> 00:09:35,560
But I wanted to cite Francois Chollet, I'm a big fan of him.

157
00:09:35,560 --> 00:09:39,760
He just tweeted saying, I'm not too concerned of whether what I read is right or wrong.

158
00:09:39,760 --> 00:09:42,040
I can figure that part out myself.

159
00:09:42,040 --> 00:09:46,040
I'm interested in things that are useful, thought-provoking, novel.

160
00:09:46,040 --> 00:09:49,200
Sometimes the most creative thinkers have a bias towards wrongness, but they're still

161
00:09:49,200 --> 00:09:50,200
worth reading.

162
00:09:50,200 --> 00:09:51,200
Would you agree with that?

163
00:09:51,200 --> 00:09:52,680
Yes, I largely agree with that.

164
00:09:52,680 --> 00:09:59,080
So as I was saying before, you know, I can tell for myself or I can do that exercise

165
00:09:59,080 --> 00:10:00,840
of figuring out what is right and wrong.

166
00:10:00,840 --> 00:10:06,000
The most important thing that I want is to not miss out on things that I don't want to

167
00:10:06,000 --> 00:10:07,000
miss out on.

168
00:10:07,000 --> 00:10:11,320
Like, you know, the known unknowns and the unknown unknowns, the biggest killer is the

169
00:10:11,320 --> 00:10:12,320
unknown unknowns.

170
00:10:12,320 --> 00:10:17,540
So if anybody trying to learn or understand something, they're for person or organization

171
00:10:17,540 --> 00:10:19,040
or society, right?

172
00:10:19,040 --> 00:10:23,800
If all they do is move the unknown unknowns to known unknowns, they've already gone an

173
00:10:23,800 --> 00:10:25,840
enormous distance, right?

174
00:10:25,840 --> 00:10:30,640
And so I appreciate people who I disagree with, first of all, because that's how you sharpen

175
00:10:30,640 --> 00:10:31,640
ideas.

176
00:10:31,640 --> 00:10:32,640
Yeah.

177
00:10:32,640 --> 00:10:38,280
But also because they may just bring things to my attention that if we were all conforming

178
00:10:38,280 --> 00:10:42,520
to the more majority view, would not come to our attention.

179
00:10:42,520 --> 00:10:45,000
And then those more often than not are the ones that kill you.

180
00:10:45,000 --> 00:10:46,000
Yeah, that's so interesting.

181
00:10:46,000 --> 00:10:49,760
I mean, there's a real analogy here, even this might be tenuous, but between symbolism

182
00:10:49,760 --> 00:10:54,840
and connectionism or, you know, Rich Sutton said we shouldn't be hand-crafting our AI

183
00:10:54,840 --> 00:10:55,840
systems.

184
00:10:55,840 --> 00:10:57,000
We should kind of let them emerge.

185
00:10:57,000 --> 00:11:00,040
And it's a similar thing with our moral framework, but you're kind of saying that it should be

186
00:11:00,040 --> 00:11:05,200
emergent from low level complexity and diversity and interestingness.

187
00:11:05,200 --> 00:11:08,280
And there is another school of thought, which is that we should be top down and we already

188
00:11:08,280 --> 00:11:09,440
have a representation.

189
00:11:09,440 --> 00:11:13,600
I actually think it should be, it needs to be a combination.

190
00:11:13,600 --> 00:11:15,320
We need to have both.

191
00:11:15,320 --> 00:11:18,560
This is one of those debates that in some sense puzzles me because to me the obvious

192
00:11:18,560 --> 00:11:20,880
answer is that we need both.

193
00:11:20,880 --> 00:11:23,160
And then if you read the master argument, this is what I do.

194
00:11:23,160 --> 00:11:26,720
I look at the different paradigms of machine learning and I don't come out in favor of

195
00:11:26,720 --> 00:11:30,480
any of them because I actually think we need ideas from all of them.

196
00:11:30,480 --> 00:11:32,840
And then we need to combine them into something coherent.

197
00:11:32,840 --> 00:11:36,960
And if you look at psychology, like your brain does bottom up and top down processing.

198
00:11:36,960 --> 00:11:40,400
And if it only did one of them, either one, it wouldn't work.

199
00:11:40,400 --> 00:11:43,880
And I think as we try to build a larger intelligence, it's the same thing.

200
00:11:43,880 --> 00:11:48,160
We definitely need the bottom up part and, you know, by volume, the bottom up part is

201
00:11:48,160 --> 00:11:49,160
going to be bigger.

202
00:11:49,160 --> 00:11:52,960
So if you could only have one, that would probably be, you know, the choice.

203
00:11:52,960 --> 00:11:56,200
But the top down part is also very important.

204
00:11:56,240 --> 00:12:00,440
If you go all the way back, the top down part probably started this bottom up and got synthesized

205
00:12:00,440 --> 00:12:01,440
and improved.

206
00:12:01,440 --> 00:12:02,880
But now we need that loop.

207
00:12:02,880 --> 00:12:04,320
The loop is actually very important.

208
00:12:04,320 --> 00:12:05,320
Well, that's interesting.

209
00:12:05,320 --> 00:12:09,600
So in your book, I guess I want to sketch out different types of AI architecture.

210
00:12:09,600 --> 00:12:12,840
So, you know, you get universalists to this kind of deep mind idea that a very simple

211
00:12:12,840 --> 00:12:15,000
underlying algorithm could produce everything.

212
00:12:15,000 --> 00:12:18,000
And then you get, you know, hybrid folks on the other side of the spectrum.

213
00:12:18,000 --> 00:12:20,000
And then there's an integrated approach in the middle.

214
00:12:20,000 --> 00:12:22,680
Like, where would you kind of place yourself on that continuum?

215
00:12:22,680 --> 00:12:28,480
I would place myself very much in the frame of mind, well, let me put it this way.

216
00:12:28,480 --> 00:12:31,320
I don't know, but which is nobody does, right?

217
00:12:31,320 --> 00:12:34,640
If somebody tells you that they know how we're going to get to intelligence, you should be

218
00:12:34,640 --> 00:12:36,200
suspicious right away.

219
00:12:36,200 --> 00:12:40,920
But what do I think is the most promising approach and the one that ideally would be

220
00:12:40,920 --> 00:12:42,600
the best one if we can pull it off?

221
00:12:42,600 --> 00:12:44,520
It's there being a single algorithm.

222
00:12:44,520 --> 00:12:49,240
So at that level, I very much sympathize with what is effectively the deep mind's agenda.

223
00:12:49,240 --> 00:12:55,080
Now, we're at part with a lot of these people is that I don't think the algorithm that we

224
00:12:55,080 --> 00:12:59,240
need is as simple as many of those people think it is.

225
00:12:59,240 --> 00:13:01,760
And I don't think it exists.

226
00:13:01,760 --> 00:13:05,840
It is probably the case that the algorithm that we really need at the end of the day

227
00:13:05,840 --> 00:13:09,040
doesn't even look that much like any of the things that we have now.

228
00:13:09,040 --> 00:13:12,480
So I think hopefully there is such an algorithm, but we're still far from it.

229
00:13:12,480 --> 00:13:13,480
Interesting.

230
00:13:13,480 --> 00:13:17,760
I mean, they would cite the example of evolution as being a very simple underlying algorithm.

231
00:13:17,760 --> 00:13:22,040
Although Ken Stanley would say that people misunderstand evolution.

232
00:13:22,040 --> 00:13:23,720
So I agree with them at that level.

233
00:13:23,720 --> 00:13:27,920
In fact, in the master algorithm, I have a chapter where I go over the objections and

234
00:13:27,920 --> 00:13:31,040
the reasons to believe that there is a master algorithm.

235
00:13:31,040 --> 00:13:35,720
The majority of the people even in the field are skeptical of that notion, even though

236
00:13:35,720 --> 00:13:38,640
I would claim that effectively that's what they're pursuing.

237
00:13:38,640 --> 00:13:41,680
People like Rich Sutton and Jeff Hitton, I asked a bunch of people before I wrote the

238
00:13:41,680 --> 00:13:45,000
book and they do believe in this idea of having a master algorithm.

239
00:13:45,000 --> 00:13:49,920
A lot of people believe that, but intuitively a lot of people believe that no, there is

240
00:13:49,920 --> 00:13:51,560
no such thing.

241
00:13:51,560 --> 00:14:00,920
And I understand that intuition, but I don't think it's a well-founded intuition.

242
00:14:00,920 --> 00:14:01,920
Let me put it that way.

243
00:14:01,920 --> 00:14:05,520
But in a sense, we know there is such a thing, because look at cellular automata, look at

244
00:14:05,520 --> 00:14:07,360
what we've already done with deep learning.

245
00:14:07,360 --> 00:14:11,440
I think the context is, is there such a thing that will produce what we want?

246
00:14:12,440 --> 00:14:21,720
To take your example or DeepMind's example of evolution, in the book I mentioned empirical

247
00:14:21,720 --> 00:14:25,840
evidence that there is a master algorithm and exhibit one is evolution.

248
00:14:25,840 --> 00:14:28,880
If you think of evolution as an algorithm, which by the way is a very old idea, I think

249
00:14:28,880 --> 00:14:35,200
it was George Bull that said, God does not create animals and plants, he creates the

250
00:14:35,200 --> 00:14:38,320
algorithm by which animals and plants come about.

251
00:14:38,600 --> 00:14:41,720
He didn't use the word algorithm, but that's essentially what he said.

252
00:14:41,720 --> 00:14:43,920
This I think is right on.

253
00:14:43,920 --> 00:14:46,520
Another example is your brain.

254
00:14:46,520 --> 00:14:50,480
If the algorithm doesn't have to be something as simple as backprop and you're a materialist

255
00:14:50,480 --> 00:14:54,640
like most of the scientists are, your brain, if the master algorithm is an algorithm that

256
00:14:54,640 --> 00:14:58,200
can learn anything you do, then your brain is that algorithm.

257
00:14:58,200 --> 00:15:02,600
But then there's another one which is even more fundamental, but I think from the point

258
00:15:02,600 --> 00:15:07,160
of view of this debate is very illuminating, which is the laws of physics.

259
00:15:07,160 --> 00:15:08,680
Why stop at evolution?

260
00:15:08,680 --> 00:15:11,480
The laws of physics are the master algorithm.

261
00:15:11,480 --> 00:15:13,080
Evolution is very complicated.

262
00:15:13,080 --> 00:15:17,800
In fact, what I think about evolution in AI currently is that evolution in reality is

263
00:15:17,800 --> 00:15:21,760
much more complex than we give it credit for, which is why a lot of our current generic

264
00:15:21,760 --> 00:15:23,320
algorithms don't work that well.

265
00:15:23,320 --> 00:15:26,560
But the laws of physics at this level are much simpler.

266
00:15:26,560 --> 00:15:31,480
If you think about it, from the laws of physics comes evolution and comes all the intelligence

267
00:15:31,480 --> 00:15:33,480
that we have.

268
00:15:33,480 --> 00:15:37,160
It's very intriguing why that happens and why the laws are such that that happens.

269
00:15:37,160 --> 00:15:40,160
But even just the laws of physics are already a master algorithm.

270
00:15:40,160 --> 00:15:44,680
Now what you could say, and many people immediately say is like, oh, but if you start from there,

271
00:15:44,680 --> 00:15:46,200
you'll never get anywhere, right?

272
00:15:46,200 --> 00:15:52,280
But then you can say evolution is the laws of physics sped up in a certain direction.

273
00:15:52,280 --> 00:15:55,120
And then our reinforcement learning is like evolution.

274
00:15:55,120 --> 00:15:57,920
People have pointed out the same way except it's faster.

275
00:15:57,920 --> 00:16:01,680
And in a way what we're trying to do now in machine learning is the same thing yet again

276
00:16:01,720 --> 00:16:03,920
except only even faster.

277
00:16:03,920 --> 00:16:05,520
But what are the consequences of?

278
00:16:05,520 --> 00:16:08,760
I mean, let's say it is actually a very high resolution algorithm.

279
00:16:08,760 --> 00:16:13,600
So it's something that appears to be completely unintelligible to in respect of the output

280
00:16:13,600 --> 00:16:14,480
phenomena.

281
00:16:14,480 --> 00:16:16,240
Is that is that even a good place to be?

282
00:16:16,240 --> 00:16:20,800
Because, you know, just like with cellular automata, there's no real paradigmatic

283
00:16:20,800 --> 00:16:23,600
relationship between the underlying rules and the emergent phenomena, right?

284
00:16:23,600 --> 00:16:26,400
So is that really even something we want?

285
00:16:26,400 --> 00:16:27,520
No, I think there is.

286
00:16:27,520 --> 00:16:29,600
So we don't know.

287
00:16:29,600 --> 00:16:33,360
But I think people and this is very common among connections is to say this stuff is

288
00:16:33,360 --> 00:16:36,520
also complex that we can't possibly have a handle on it.

289
00:16:36,520 --> 00:16:38,000
We just have to let it happen.

290
00:16:38,000 --> 00:16:41,840
And I think that is not giving enough credit to our human brains, right?

291
00:16:41,840 --> 00:16:46,320
We are incredibly good at making sense of things that in the beginning, I mean, over

292
00:16:46,320 --> 00:16:50,280
and over and over in the history of science and technology, you start out with things

293
00:16:50,280 --> 00:16:52,600
that you don't understand very well at all.

294
00:16:52,600 --> 00:16:57,320
But then over time, we kind of change our representation of the world to make those

295
00:16:57,320 --> 00:16:59,520
things actually be intelligible to us.

296
00:16:59,520 --> 00:17:03,480
And we should not a priori assume that that's not going to be the case here.

297
00:17:03,480 --> 00:17:08,800
So, for example, cellular automata, amazing things come out of whatever the game of life

298
00:17:08,800 --> 00:17:12,840
that seemed completely disconnected from those, but they aren't, right?

299
00:17:12,840 --> 00:17:18,160
And, you know, there's various depths at which you could go into this.

300
00:17:18,160 --> 00:17:22,400
There will probably at the end of the day be some large element of this that we can't

301
00:17:22,400 --> 00:17:26,800
figure out very well, but we can figure out enough that we have a handle on it.

302
00:17:26,800 --> 00:17:31,400
So, this singularity notion that at some point AI is just completely beyond our understanding.

303
00:17:31,400 --> 00:17:33,040
I tend not to buy.

304
00:17:33,040 --> 00:17:35,600
I don't think it will be completely beyond our understanding.

305
00:17:35,600 --> 00:17:40,320
But it's an analog back to our Twitter discussion, like, because we can only understand it through.

306
00:17:40,320 --> 00:17:43,480
It's like having views on a mountain range, you know, the view looks different depending

307
00:17:43,480 --> 00:17:45,080
on where you're standing.

308
00:17:45,080 --> 00:17:48,760
And it's the same thing with the emergent phenomena in a cellular automata.

309
00:17:48,760 --> 00:17:49,760
No, very good.

310
00:17:49,760 --> 00:17:54,400
And, you know, the classic example of this is the blind man and the elephant, right?

311
00:17:54,400 --> 00:17:57,920
And that's actually the metaphor that is in the book, as I say, you know, the different

312
00:17:57,920 --> 00:18:00,640
tribes are like different blind men, right?

313
00:18:00,640 --> 00:18:03,480
But precisely so, AI is one of the blind men.

314
00:18:03,480 --> 00:18:07,800
I can see part of the elephant, but it'd be who's meant to also talk to you who see another

315
00:18:07,800 --> 00:18:08,800
part of the elephant.

316
00:18:08,800 --> 00:18:12,600
And then each of us understands a little bit more of the elephant than we would if we were

317
00:18:12,600 --> 00:18:13,600
on our own.

318
00:18:13,600 --> 00:18:18,800
But most importantly, we collectively, which is what really matters, actually understand

319
00:18:18,800 --> 00:18:23,840
maybe not the elephant completely, but much more of the elephant than either any of us

320
00:18:23,960 --> 00:18:25,560
would alone, right?

321
00:18:25,560 --> 00:18:28,320
And it's certainly a lot better than just giving up and say, like, oh, we're never going to

322
00:18:28,320 --> 00:18:30,560
understand this strange thing that's in front of us.

323
00:18:30,560 --> 00:18:31,560
Interesting.

324
00:18:31,560 --> 00:18:33,080
But that's a great argument to what you were saying before.

325
00:18:33,080 --> 00:18:35,880
So it's beyond our cognitive horizon.

326
00:18:35,880 --> 00:18:38,680
Therefore we need to have diversity of aspect.

327
00:18:38,680 --> 00:18:42,360
There's a, yes, there's a question of whether it's beyond the cognitive ability of a single

328
00:18:42,360 --> 00:18:43,360
human.

329
00:18:43,360 --> 00:18:44,360
Yeah.

330
00:18:44,360 --> 00:18:46,400
And then there's the question of whether it's beyond the cognitive ability of an entire

331
00:18:46,400 --> 00:18:47,920
society of humans.

332
00:18:47,920 --> 00:18:51,840
And obviously, there'll be things that are beyond the cognitive ability of a single human,

333
00:18:51,840 --> 00:18:53,920
but not beyond the cognitive ability of a society.

334
00:18:53,920 --> 00:18:56,280
Also these days, we have computers.

335
00:18:56,280 --> 00:18:58,680
So our cognitive power is augmented by our machine.

336
00:18:58,680 --> 00:19:03,320
So we can understand things or bring things to the point where we understand them to a

337
00:19:03,320 --> 00:19:05,640
degree today that we couldn't a hundred years ago.

338
00:19:05,640 --> 00:19:07,200
Right now, that is a fascinating point.

339
00:19:07,200 --> 00:19:13,080
So it's beyond our cognitive horizon individually, but it might not be beyond the cognitive horizon

340
00:19:13,080 --> 00:19:17,080
of loads and loads of humans on the internet, you know, the wisdom of crowds.

341
00:19:17,080 --> 00:19:21,160
But we don't, I mean, how do we know that the crowd understands?

342
00:19:21,160 --> 00:19:26,320
But we know, well, that's the, in some sense, the beauty of this, right, is that we never,

343
00:19:26,320 --> 00:19:28,040
what is the crowd really understanding, right?

344
00:19:28,040 --> 00:19:31,920
And again, once the crowd is augmented by machines, like machine learning algorithms,

345
00:19:31,920 --> 00:19:36,800
right, we can ask what do we as a society equipped with all of our, you know, large

346
00:19:36,800 --> 00:19:40,120
language models and so on and so forth, what do we really understand, right?

347
00:19:40,120 --> 00:19:44,480
Now, at some level, you can't answer that question individually because you are just

348
00:19:44,480 --> 00:19:48,120
an individual, but right, there's a couple of very important things that we shouldn't

349
00:19:48,120 --> 00:19:49,120
forget.

350
00:19:49,240 --> 00:19:54,080
You could, one thing you can do and that I do do is say, like, do I now actually even

351
00:19:54,080 --> 00:19:59,320
just individually understand things better than I did before when it was just me looking

352
00:19:59,320 --> 00:20:00,320
at it?

353
00:20:00,320 --> 00:20:02,760
And the answer to that is almost invariably yes, right?

354
00:20:02,760 --> 00:20:05,000
So there is a big game to be heard there.

355
00:20:05,000 --> 00:20:11,440
And the second one is that you, ultimately, you tell by the consequences, right?

356
00:20:11,440 --> 00:20:14,120
And like, for example, take a deep network, right?

357
00:20:14,120 --> 00:20:17,920
And you may not know how it works, but if it's doing medical diagnosis, you can tell

358
00:20:17,920 --> 00:20:22,520
whether it, you know, gets the diagnosis right more often than it did before or more

359
00:20:22,520 --> 00:20:23,520
often than another model.

360
00:20:23,520 --> 00:20:28,920
So we as a society may not, you know, we individuals may not very understand very well what we

361
00:20:28,920 --> 00:20:32,880
as a society understand, but we can see the consequences and at some level, that's the

362
00:20:32,880 --> 00:20:33,880
point.

363
00:20:33,880 --> 00:20:34,880
Yeah.

364
00:20:34,880 --> 00:20:37,760
So on that, I mean, that sounds like a bit of an appeal to behaviorism and we're going

365
00:20:37,760 --> 00:20:39,760
to talk about that in respect of charmers as well.

366
00:20:39,760 --> 00:20:43,840
But it also brings us back to, you know, we were talking about empiricism versus rationalism

367
00:20:43,840 --> 00:20:46,600
and nativism and all of these topics.

368
00:20:46,600 --> 00:20:51,200
Would you place yourself in that camp of being a nativist and a rationalist or completely

369
00:20:51,200 --> 00:20:52,200
the other way?

370
00:20:52,200 --> 00:20:53,440
No, absolutely not.

371
00:20:53,440 --> 00:20:59,040
Again, this is one of the points that I, you know, go back to is there are the empiricists

372
00:20:59,040 --> 00:21:03,640
and there are the rationalists and you could see naively machine learning as being the

373
00:21:03,640 --> 00:21:08,000
triumph of the empiricists, but it actually is not there are very fundamental reasons

374
00:21:08,000 --> 00:21:09,000
why it's not.

375
00:21:09,000 --> 00:21:12,720
And I really do think, and this is not just think there's this thing called the no free

376
00:21:12,720 --> 00:21:14,560
lunch theorem, right?

377
00:21:14,560 --> 00:21:18,840
And if you take those things seriously, the solution has to be a combination of empiricism

378
00:21:18,840 --> 00:21:19,840
and rationalism.

379
00:21:19,840 --> 00:21:23,840
I don't think either side alone has or even can have the whole answer.

380
00:21:23,840 --> 00:21:26,200
So very much we need both of those.

381
00:21:26,200 --> 00:21:31,160
And if you're a pure empiricist or a pure rationalist, I'm already suspicious of you.

382
00:21:31,160 --> 00:21:32,160
Wonderful.

383
00:21:32,160 --> 00:21:36,640
Coming back to what Franz Walsh said in his quote, he said, you know, producing things

384
00:21:36,640 --> 00:21:39,360
that are thought provoking novel and all the rest of it.

385
00:21:39,360 --> 00:21:42,360
And I was speaking to some alignment folks yesterday and we'll pivot to that in a minute.

386
00:21:42,960 --> 00:21:47,160
The big thing for me after doing an episode on sales, the Chinese room is, you know, where

387
00:21:47,160 --> 00:21:51,640
does intentionality come from and Chomsky talks about agency, for example, we do things

388
00:21:51,640 --> 00:21:54,200
that are appropriate to the situation, but not caused by them.

389
00:21:54,200 --> 00:21:57,280
So from my perspective, all these generative models, all these large language models and

390
00:21:57,280 --> 00:22:01,120
so on, the creativity, the real spark of genius still comes from us, right?

391
00:22:01,120 --> 00:22:06,680
We've just kind of like, you know, the boring bit of actually doing the task is now delegated

392
00:22:06,680 --> 00:22:08,240
to the algorithm.

393
00:22:08,240 --> 00:22:09,600
I would disagree with that.

394
00:22:09,600 --> 00:22:14,360
I think you are, I mean, your position is very reasonable and actually, I would say

395
00:22:14,360 --> 00:22:19,880
probably the most common, but I think when you do that, you are giving us too much credit

396
00:22:19,880 --> 00:22:22,920
and the large language models too little.

397
00:22:22,920 --> 00:22:27,840
We tend to have this notion that creativity is something magical.

398
00:22:27,840 --> 00:22:32,240
In fact, I remember for many years, so quick parenthesis, in the previous life I was a

399
00:22:32,240 --> 00:22:33,240
musician.

400
00:22:33,240 --> 00:22:37,840
So, you know, I, in some sense know about, and a lot of my job was composing songs, right?

401
00:22:37,840 --> 00:22:41,880
And I was always, at the same time I was already studying AI and I couldn't help but connect

402
00:22:41,880 --> 00:22:42,880
it too, right?

403
00:22:42,880 --> 00:22:48,560
And think about like, what would an AI look that was able to compose music, right?

404
00:22:48,560 --> 00:22:53,480
And talking to late people who are not musicians, they think that composing songs is some kind

405
00:22:53,480 --> 00:22:58,600
of magic thing that comes from, you know, whatever the great beyond, and it's not.

406
00:22:58,600 --> 00:23:02,280
It's a very human enterprise and it can very well be automated.

407
00:23:02,280 --> 00:23:06,600
It's actually now, you know, people, I used to say to people like, people always say like,

408
00:23:06,600 --> 00:23:10,120
well, creativity will be the last thing that we automate because we humans can do it and

409
00:23:10,120 --> 00:23:12,920
there's no machine school and be like, no, it's going to be the opposite.

410
00:23:12,920 --> 00:23:16,920
You automate creativity long before many other things and we're there now, right?

411
00:23:16,920 --> 00:23:23,680
In just the last, so I think when you, let me put it this way, your prompt to the LLM,

412
00:23:23,680 --> 00:23:28,200
let's say, is like the grain of sand to the oyster, right?

413
00:23:28,200 --> 00:23:31,680
You should not give yourself credit for having made the pearl because it put the grain of

414
00:23:31,680 --> 00:23:32,680
sand in there.

415
00:23:32,680 --> 00:23:34,960
That's a, that's a brilliant analogy, right?

416
00:23:34,960 --> 00:23:40,440
So it is still the LLM, we need to, we can critique how creative it is or not and there's

417
00:23:40,440 --> 00:23:44,200
a lot to be said there and a lot of progress to be made, but we need to give it credit

418
00:23:44,200 --> 00:23:45,600
for what it does, right?

419
00:23:45,600 --> 00:23:47,960
It is well or not so well, right?

420
00:23:47,960 --> 00:23:52,680
Maybe it's more of an illusion that we're giving credit for and whatnot, but that text

421
00:23:52,680 --> 00:23:57,760
or that image or whatever, they were created by the AI.

422
00:23:57,760 --> 00:24:01,440
And in many ways, the thing that was created by the AI is no worse than what would have

423
00:24:01,440 --> 00:24:05,240
been created by an artist if I gave them the prompt.

424
00:24:05,240 --> 00:24:06,240
No, okay.

425
00:24:06,240 --> 00:24:07,240
Well, on that, I agree with you.

426
00:24:07,240 --> 00:24:10,560
I mean, Melanie Mitchell had this wonderful anecdote from the Google Plex when she was

427
00:24:10,560 --> 00:24:14,080
with Douglas Hofstadter and he was talking at the time about, you know, how he would

428
00:24:14,080 --> 00:24:18,920
be devastated if an AI could produce a Chopin piece, you know, which was indistinguishable

429
00:24:18,920 --> 00:24:24,520
from one which he actually created and of course, that did happen, but then we get into

430
00:24:24,520 --> 00:24:27,360
this discussion of where does it start, right?

431
00:24:27,360 --> 00:24:29,040
Where does it start?

432
00:24:29,040 --> 00:24:30,920
Computers only do what we tell them to do, right?

433
00:24:30,920 --> 00:24:35,160
They've been trained and actually I was speaking to Sep about this the other day that, you

434
00:24:35,160 --> 00:24:38,560
know, all of the abstractions, all of the things that the computers and the models do,

435
00:24:38,560 --> 00:24:42,680
they are crystallized snapshots of things that humans have previously done and we've

436
00:24:42,680 --> 00:24:44,920
written the computer code.

437
00:24:44,920 --> 00:24:46,720
So where does the creativity start?

438
00:24:46,720 --> 00:24:53,880
Well, but we, by that standard, we humans also only do what we're told to do.

439
00:24:53,880 --> 00:24:55,640
We do what we're told to do by our genes.

440
00:24:55,640 --> 00:24:59,040
Our genes do what they're told to do by evolution, which does what is told to do by the laws

441
00:24:59,040 --> 00:25:00,040
of physics, right?

442
00:25:00,160 --> 00:25:01,160
Right.

443
00:25:01,160 --> 00:25:05,680
And now, again, this gets back to this notion that there's nothing magical about creativity.

444
00:25:05,680 --> 00:25:10,920
Creativity really is, to a large extent, cutting and pasting stuff and satisfying consistency

445
00:25:10,920 --> 00:25:12,320
constraints between them.

446
00:25:12,320 --> 00:25:16,720
And I'm not just saying this in the abstract, like long before the modern era, there's this

447
00:25:16,720 --> 00:25:23,120
guy called David Cope, you know, a composer and professor of music at UC Santa Cruz who

448
00:25:23,120 --> 00:25:28,760
created these programs that exactly they would write, they can write, this was pre-machine

449
00:25:28,760 --> 00:25:29,760
learning.

450
00:25:29,760 --> 00:25:34,520
It was list code that what it did was basically have rules about how music should be.

451
00:25:34,520 --> 00:25:37,200
And then it takes snippets and combines them, right?

452
00:25:37,200 --> 00:25:40,720
You could say it's just parroting those bits, but the truth is at the end of the day and

453
00:25:40,720 --> 00:25:41,720
you can choose.

454
00:25:41,720 --> 00:25:44,080
You say, like, give me something in the style of Mozart.

455
00:25:44,080 --> 00:25:50,040
And it creates something that looks indistinguishable from what Mozart did, but all it's doing

456
00:25:50,040 --> 00:25:52,640
is this kind of recombination of pieces.

457
00:25:52,640 --> 00:25:59,440
So we humans, we have too much respect for appreciation of our own intelligence.

458
00:25:59,440 --> 00:26:01,000
That's also what we're doing.

459
00:26:01,000 --> 00:26:02,760
Yeah, I think I agree with you.

460
00:26:02,760 --> 00:26:06,760
I mean, first of all, intelligence is a receding horizon and there's the McCorduck effect.

461
00:26:06,760 --> 00:26:08,160
I agree with all of that.

462
00:26:08,160 --> 00:26:14,180
But yeah, I think it's a similar thing to how we anthropomorphise large language models

463
00:26:14,180 --> 00:26:17,320
and even, you know, it's tempting to say large language models are slightly conscious

464
00:26:17,320 --> 00:26:21,760
and we'll talk about that in a minute, but maybe like we also anthropomorphise our own

465
00:26:21,760 --> 00:26:22,760
agency, right?

466
00:26:22,760 --> 00:26:26,480
We have like a little bubble around ourselves and we kind of delude ourselves that we exist

467
00:26:26,480 --> 00:26:31,320
as an individual unit with agency disconnected from the rest of the world.

468
00:26:31,320 --> 00:26:37,400
Well, precisely the problem with how we largely take AI today, this has always been the case,

469
00:26:37,400 --> 00:26:42,760
by the way, is that we have a new resistible notion to anthropomorphise anything that behaves

470
00:26:42,760 --> 00:26:44,200
even remotely like us.

471
00:26:44,200 --> 00:26:48,800
We're the only intelligent things that we know, so if something starts behaving intelligently,

472
00:26:48,800 --> 00:26:53,240
then we project onto it all of these other human characteristics.

473
00:26:53,240 --> 00:26:55,400
Same with consciousness, same with creativity.

474
00:26:55,400 --> 00:26:57,600
We don't know anything else that's creative besides us.

475
00:26:57,600 --> 00:27:01,840
So once a machine starts behaving creatively, we cannot help but project a lot of things

476
00:27:01,840 --> 00:27:02,840
onto it.

477
00:27:02,840 --> 00:27:04,840
It's just reasoning by analogy, right?

478
00:27:04,840 --> 00:27:09,320
So it's a kind of analogical, so like you're like me in this respect, so you probably are

479
00:27:09,320 --> 00:27:10,320
in this other spec.

480
00:27:10,320 --> 00:27:13,760
Now, the good news is that we always start out with this kind of very good reasoning

481
00:27:13,760 --> 00:27:17,920
by analogy, but after a while, we actually start to build a model of the real thing.

482
00:27:17,920 --> 00:27:23,320
So AI for the public at large right now is very new, but gradually we'll come to a point

483
00:27:23,320 --> 00:27:29,120
where we zero in on what AI really is rather than just the shallow analogies that we initially

484
00:27:29,120 --> 00:27:30,120
used to try to understand.

485
00:27:30,120 --> 00:27:31,120
Okay.

486
00:27:31,120 --> 00:27:32,560
Well, I'll try it from a slightly different angle.

487
00:27:32,560 --> 00:27:36,040
So we were just saying Seoul makes the argument that it's a biological property and that's

488
00:27:36,040 --> 00:27:40,480
where intentionality and consciousness comes from and it's a requisite, but we'll leave

489
00:27:40,480 --> 00:27:42,160
that for the time being.

490
00:27:42,160 --> 00:27:47,720
Let's go the Fodor and the Gary Marcus and the Chomsky group, and they would argue that

491
00:27:47,720 --> 00:27:53,400
creativity is basically this notion of, or even analogy making by extension, is this

492
00:27:53,400 --> 00:27:58,440
notion of being able to select from a set which has an infinite cardinality.

493
00:27:58,440 --> 00:28:02,840
And as you know, neural networks can't represent infinite sets because they're finite state

494
00:28:02,840 --> 00:28:06,800
automators, therefore they make the move we need to have this compositionality.

495
00:28:06,800 --> 00:28:07,800
What do you say to that?

496
00:28:07,800 --> 00:28:09,920
Well, there's a lot to unpack there.

497
00:28:09,920 --> 00:28:13,040
I think we definitely need compositionality, right?

498
00:28:13,040 --> 00:28:16,840
If somebody asked me, make a list of how there's some things that are actually essential

499
00:28:16,840 --> 00:28:20,400
for intelligence, compositionality would be one of them, right?

500
00:28:20,400 --> 00:28:25,040
And this, of course, is the thing that people like Chomsky and Gary are not really care

501
00:28:25,040 --> 00:28:26,040
about, right?

502
00:28:26,040 --> 00:28:31,560
Having said that, I think first of all, there is no such thing as an infinite set, right?

503
00:28:31,560 --> 00:28:40,320
Like infinite set is a useful but extremely dangerous and confusing mathematical tool,

504
00:28:40,320 --> 00:28:41,320
right?

505
00:28:41,960 --> 00:28:46,520
There is no such thing as an infinite anything and there never will be.

506
00:28:46,520 --> 00:28:50,880
So I would just raise this at, well, yes, creativity and almost anything we can do in

507
00:28:50,880 --> 00:28:56,680
AI is selecting from a very large set, not infinite, but very large, right?

508
00:28:56,680 --> 00:29:00,480
And now, but now we don't just select like one full element at a time.

509
00:29:00,480 --> 00:29:04,280
We compose it out of pieces and that's actually where the intelligence comes in.

510
00:29:04,280 --> 00:29:05,280
Interesting.

511
00:29:05,280 --> 00:29:07,760
I don't want to go too far down the digital physics route, but we did just have Yoshua

512
00:29:07,840 --> 00:29:12,480
Barkon and I mean, just to reclamify on that, would you place yourself in that camp that

513
00:29:12,480 --> 00:29:15,040
the universe is digital and made of information?

514
00:29:15,040 --> 00:29:17,120
Valid question.

515
00:29:17,120 --> 00:29:20,280
I certainly think the universe is finite.

516
00:29:20,280 --> 00:29:27,920
I think, I mean, like Seth Lloyd says, the universe is a computer, right?

517
00:29:27,920 --> 00:29:34,440
And I think that is true or false depending on what you take the word computer to mean,

518
00:29:34,440 --> 00:29:35,440
right?

519
00:29:36,440 --> 00:29:42,920
So if you say that the universe is digital or is a computer as kind of like an analogy

520
00:29:42,920 --> 00:29:46,120
that lets us understand it better, I'm all for that.

521
00:29:46,120 --> 00:29:49,280
I don't think the universe is little, you know, here's the way to put this.

522
00:29:49,280 --> 00:29:51,080
The universe is a computation.

523
00:29:51,080 --> 00:29:55,120
Like, I don't know what the computer is or if there is one.

524
00:29:55,120 --> 00:30:00,160
Now the universe is digital in the sense that deep down at the most basic level, the universe

525
00:30:00,160 --> 00:30:02,120
is made of discrete things.

526
00:30:02,360 --> 00:30:05,880
OK, is this like the it from bit, the John Wheeler type hypothesis?

527
00:30:06,960 --> 00:30:08,440
Yes.

528
00:30:08,440 --> 00:30:13,360
I mean, if you read that paper, it is, I mean, John Wheeler was a brilliant person.

529
00:30:14,280 --> 00:30:19,720
Again, very, you know, to get back to François Chalice's tweet, he was very good at coming

530
00:30:19,720 --> 00:30:21,760
up with his provocative notions, right?

531
00:30:21,760 --> 00:30:25,040
And the it from the thing, of course, is like newly unveiled today.

532
00:30:25,440 --> 00:30:30,760
And I do so at that level, I do agree that looking at the universe is being made of

533
00:30:30,760 --> 00:30:32,960
information is very useful.

534
00:30:33,040 --> 00:30:38,520
And in particular, if you want a grand unified master algorithm, in some sense, the only

535
00:30:38,520 --> 00:30:43,120
way that I at least can see of doing that is by seeing everything as information.

536
00:30:43,680 --> 00:30:48,240
So I think, and if I do something that I am working on that, looking at everything as

537
00:30:48,240 --> 00:30:51,000
information is a very productive thing to do.

538
00:30:51,280 --> 00:30:57,040
Yeah, but but my caution is that information is one aspect of everything.

539
00:30:57,680 --> 00:31:00,960
So I can give you a theory of everything that's based on information.

540
00:31:01,400 --> 00:31:03,560
But it's not truly a theory of everything.

541
00:31:03,560 --> 00:31:05,520
It's a theory of one aspect of everything.

542
00:31:06,080 --> 00:31:07,520
And I think there's a lot to be done there.

543
00:31:07,520 --> 00:31:11,280
But again, we shouldn't forget what we're living out when we focus on that aspect.

544
00:31:11,640 --> 00:31:13,920
Yeah, I mean, we've spoken a lot on the show about, you know,

545
00:31:14,480 --> 00:31:18,720
Penrose's view and obviously Sal's view that arises from from biology.

546
00:31:18,720 --> 00:31:22,680
And I know if Keith was here, he would argue strongly that he believes in in

547
00:31:22,680 --> 00:31:27,840
continuum, and therefore we would need, you know, hyper computation to have this

548
00:31:27,840 --> 00:31:31,160
universe. That would be an interesting discussion to have, because I really don't

549
00:31:31,160 --> 00:31:35,440
see where there is physical or any evidence for continuum of any kind.

550
00:31:35,680 --> 00:31:39,880
The evidence is always that continuum are a useful approximation, but always

551
00:31:39,880 --> 00:31:42,000
underlying the continuum is a discrete reality.

552
00:31:42,240 --> 00:31:44,880
You take a sensor of anything, right?

553
00:31:44,880 --> 00:31:48,080
You know, quantum mechanics is like the quintessential example of this, right?

554
00:31:48,520 --> 00:31:52,240
What do we measure at the end that it's always discrete events?

555
00:31:52,680 --> 00:31:58,960
Right? Like it's the detection of a photon by, you know, by whatever detector, right?

556
00:31:58,960 --> 00:32:01,560
Could be a model of Dobson or a CCD or whatever.

557
00:32:01,800 --> 00:32:03,640
But it's a it's a it's a change of state.

558
00:32:03,640 --> 00:32:04,680
It really is a bit.

559
00:32:05,320 --> 00:32:07,080
Oh, interesting. Well, how would you contrast that?

560
00:32:07,080 --> 00:32:10,960
You know, Stephen Wolfram has got this idea of of digital physics and, you know,

561
00:32:11,280 --> 00:32:15,720
maybe and again, unfortunately, we have to use arguments from behavior, you know,

562
00:32:15,720 --> 00:32:19,440
to kind of say, well, we've we've got potentially a graph cellular automata

563
00:32:19,680 --> 00:32:23,280
and it creates this beautiful emergent structure, which is very much like the universe.

564
00:32:24,120 --> 00:32:28,760
But, you know, Scott Aronson would make the argument that he's discounting quantum mechanics.

565
00:32:28,760 --> 00:32:31,240
I mean, what would you say to that?

566
00:32:31,240 --> 00:32:35,800
So I think Steve Wolfram's theory is very interesting.

567
00:32:36,280 --> 00:32:39,560
And he gets some things right that a lot of other physicists don't,

568
00:32:40,440 --> 00:32:43,600
in particular, that the universe at heart is discrete.

569
00:32:44,360 --> 00:32:47,320
So I'm very much with him on that aspect of his agenda.

570
00:32:48,320 --> 00:32:51,240
And thank God there's someone like him and a number of others,

571
00:32:51,960 --> 00:32:53,640
you know, going that route, right?

572
00:32:53,640 --> 00:32:55,240
They're the minority in physics.

573
00:32:55,240 --> 00:32:58,120
But actually, I think if you look at just what has happened in the last 10 years,

574
00:32:58,320 --> 00:33:00,160
things are very much moving in this direction.

575
00:33:00,280 --> 00:33:02,400
And I think they're going to move more, right?

576
00:33:02,720 --> 00:33:07,680
Now, having said that, his specific theory, I think has a lot of shortcomings.

577
00:33:08,520 --> 00:33:13,840
And I don't think it's the ultimate theory or maybe even the best path to a theory,

578
00:33:14,600 --> 00:33:16,200
you know, to a discrete theory of the universe.

579
00:33:16,640 --> 00:33:22,120
Scott Aaronson's critique in that regard, I think misses the point, right?

580
00:33:22,440 --> 00:33:24,840
It's interesting because it's interesting that you should like pair those two

581
00:33:24,840 --> 00:33:28,800
because Steve Wolfram, in a way, is a physicist to become a computer scientist.

582
00:33:28,800 --> 00:33:30,640
And Scott is the other way around, right?

583
00:33:30,840 --> 00:33:32,920
And I think, you know, like I greatly admire both of them

584
00:33:32,920 --> 00:33:34,400
and I'm friends with both of them.

585
00:33:34,400 --> 00:33:36,400
I've had many discussions with them.

586
00:33:36,400 --> 00:33:40,800
I think, you know, just to very, you know, cruelly caricature things in a way,

587
00:33:40,800 --> 00:33:44,320
the problem with Steve is that he has bought into the computer science

588
00:33:44,320 --> 00:33:45,960
assumptions too much.

589
00:33:45,960 --> 00:33:49,400
And the problem with Scott is that he has bought into the quantum physics

590
00:33:49,400 --> 00:33:50,760
assumptions too much, right?

591
00:33:50,760 --> 00:33:54,200
So if you really think carefully and rigorously about quantum mechanics,

592
00:33:54,480 --> 00:33:59,480
all that continuous mathematics is there just to make discrete predictions.

593
00:34:00,280 --> 00:34:01,760
So the continuity may be useful.

594
00:34:01,760 --> 00:34:02,520
It is useful.

595
00:34:02,520 --> 00:34:05,640
I'm not arguing against the use of continuity and infinity in our mathematics.

596
00:34:05,640 --> 00:34:07,520
In fact, we'd be nowhere if we didn't have it.

597
00:34:07,760 --> 00:34:10,120
We just have to remember that it's an approximation.

598
00:34:10,120 --> 00:34:11,880
It's a useful fiction, right?

599
00:34:11,920 --> 00:34:18,720
So quantum mechanics in no way invalidates Steve Wolf from theories, right?

600
00:34:19,280 --> 00:34:22,000
The problem, however, is that he has not entirely, you know,

601
00:34:22,360 --> 00:34:24,320
ever since cellular automata days, right?

602
00:34:24,320 --> 00:34:26,800
He was always saying like, oh, you know, the laws of physics will come out

603
00:34:26,800 --> 00:34:28,080
of this cellular automata, right?

604
00:34:28,080 --> 00:34:30,440
And people had these objections and, and, you know,

605
00:34:30,440 --> 00:34:32,560
now he and others have partly answered some of them.

606
00:34:32,560 --> 00:34:35,960
But the truth is at the end of the day, he the only way to answer

607
00:34:35,960 --> 00:34:39,680
that objection is to say, look, here is how quantum mechanics arises

608
00:34:39,680 --> 00:34:41,800
from my discrete model of the world.

609
00:34:41,800 --> 00:34:45,040
And I think this will happen, but it hasn't happened yet.

610
00:34:45,240 --> 00:34:47,400
Interesting. OK, we'd love to get Steven on the show.

611
00:34:47,400 --> 00:34:49,720
Actually, he's got a he's got a new book out now,

612
00:34:49,720 --> 00:34:52,200
which is kind of like expanding on his previous work.

613
00:34:52,600 --> 00:34:57,160
But OK, I was having a chat with some alignment folks yesterday.

614
00:34:57,200 --> 00:35:01,000
And it's something that I'm a bit naive to, but as I said, I've just read a book.

615
00:35:01,360 --> 00:35:04,520
I think it's called The Rationalists Guide to the to the Universe.

616
00:35:04,840 --> 00:35:07,640
And it kind of talks all about the the early embryonic stages

617
00:35:07,640 --> 00:35:10,960
of Robin Hansen and Nick Bostrom and Eliezer Yuckowski

618
00:35:10,960 --> 00:35:13,720
and the Les Ron community and, you know, the info hazards.

619
00:35:13,720 --> 00:35:16,800
And, you know, I don't know if you've heard of Rocco's Basilicist

620
00:35:16,800 --> 00:35:18,960
and all of this line of thought, basically.

621
00:35:19,320 --> 00:35:22,080
And yeah, so where to go with this?

622
00:35:22,080 --> 00:35:27,720
Now, I kind of put forward that part of my problem with their conception

623
00:35:27,720 --> 00:35:33,480
is that it relies on this rational agent making trajectories of optimal decisions.

624
00:35:33,880 --> 00:35:37,720
And also, they they tend to be utilitarianism

625
00:35:37,720 --> 00:35:40,720
and utilitarianists and consequentialists.

626
00:35:41,120 --> 00:35:44,720
And yeah, I just wondered, what's your kind of like high level take on this?

627
00:35:46,160 --> 00:35:48,880
Well, there's many aspects to this, right?

628
00:35:48,880 --> 00:35:51,760
I think let me put it this way.

629
00:35:51,760 --> 00:35:57,160
I. A rational agent, right, is an agent that maximizes

630
00:35:57,880 --> 00:35:59,920
maximizes expected utility, right?

631
00:35:59,960 --> 00:36:03,360
The definition of rationality is that it's, you know,

632
00:36:03,360 --> 00:36:05,680
expected utility maximization, right?

633
00:36:05,920 --> 00:36:08,400
And there is a lot of content to this, right?

634
00:36:08,400 --> 00:36:12,400
And, you know, people in many fields like economics and and and, you know, AI, right?

635
00:36:12,840 --> 00:36:14,400
They make good use of it.

636
00:36:14,400 --> 00:36:18,760
It doesn't answer the question of what is the utility that you're maximizing, right?

637
00:36:18,760 --> 00:36:21,640
So if you give me utility function, right?

638
00:36:21,640 --> 00:36:24,400
Now, if you maximize it, you're rational.

639
00:36:24,840 --> 00:36:26,000
You can it can be bound.

640
00:36:26,000 --> 00:36:28,640
You can be boundedly rational because you're and indeed this

641
00:36:28,640 --> 00:36:31,880
this is the interesting and prevalent cases that you can only maximize it

642
00:36:31,880 --> 00:36:34,840
within bounds and you have to make compromises, says this fast and what not.

643
00:36:34,840 --> 00:36:36,760
But still, you're rational.

644
00:36:36,760 --> 00:36:39,160
If you don't do that, you are irrational, right?

645
00:36:39,480 --> 00:36:43,200
So rational is a very, you know, like so many mistakes that we make

646
00:36:43,200 --> 00:36:47,680
as a society as individuals would be avoided if only we were rational in that sense.

647
00:36:48,080 --> 00:36:50,760
So at that level, I sympathize very much with that view of the world.

648
00:36:50,760 --> 00:36:54,680
Having said that, there's a huge gaping hole in the middle of this, which is like,

649
00:36:54,880 --> 00:36:56,600
but what is your utility function?

650
00:36:56,640 --> 00:37:00,120
Right. And and, you know, one attitude is like, oh, that's for you to decide.

651
00:37:00,160 --> 00:37:02,560
You know, you you tell what me or your utility function is.

652
00:37:02,560 --> 00:37:05,320
But but then you you you're you're entitled to sell what like.

653
00:37:05,320 --> 00:37:08,480
But like my whole problem is that I want to figure out what my

654
00:37:08,480 --> 00:37:09,680
utility function should be.

655
00:37:09,680 --> 00:37:13,560
And at that point, this whole theory of rationality just doesn't help you at all.

656
00:37:13,960 --> 00:37:16,040
The utility function is an input, right?

657
00:37:16,040 --> 00:37:18,760
So another question becomes what is your utility function, right?

658
00:37:18,920 --> 00:37:22,080
And then there's a very related, but as Hume said, very different question,

659
00:37:22,080 --> 00:37:25,160
which is like, what should your utility function be like?

660
00:37:25,200 --> 00:37:28,280
Should is a very loaded worth here, right?

661
00:37:28,640 --> 00:37:31,480
And then what what usually happens is things like this is that our

662
00:37:31,480 --> 00:37:35,880
notions of morality and so on are trying to impose a should on you,

663
00:37:36,200 --> 00:37:41,200
a utility function that you should have because it serves the utility of the society.

664
00:37:41,960 --> 00:37:44,840
Now, from the point of view, the society, this is good, right?

665
00:37:45,080 --> 00:37:47,760
But from the point of view, because the society hopefully will live

666
00:37:47,760 --> 00:37:51,680
and prosper if its elements contribute to its utility, not just their own, right?

667
00:37:51,880 --> 00:37:53,640
But it still doesn't answer the question.

668
00:37:53,640 --> 00:37:55,280
So you can you're entitled to ask.

669
00:37:55,280 --> 00:37:56,760
So what does answer the question?

670
00:37:56,760 --> 00:38:02,080
Right. And my view on this is that none of these people and these people

671
00:38:02,080 --> 00:38:05,400
include Kant and Bentham and, you know, Plato and everybody, right?

672
00:38:05,920 --> 00:38:08,120
They you can't do that, right?

673
00:38:08,120 --> 00:38:14,200
The the the so to me, the supreme reality of life, Supreme

674
00:38:14,200 --> 00:38:18,520
maybe is a bad word, but like the overarching reality is evolution, right?

675
00:38:19,120 --> 00:38:21,360
Everything we are is created by evolution.

676
00:38:21,360 --> 00:38:24,360
And as somebody famously said, nothing in biology makes sense,

677
00:38:24,360 --> 00:38:27,960
except in light of evolution, nothing in morality makes sense,

678
00:38:27,960 --> 00:38:30,920
except in light of evolution, not just biological evolution, even though

679
00:38:30,920 --> 00:38:33,560
that's part of it, but also social and cultural evolution.

680
00:38:33,560 --> 00:38:36,800
So at the end of the day, the question that you need to ask yourself is like,

681
00:38:36,800 --> 00:38:39,440
is which utility functions are fitter?

682
00:38:39,760 --> 00:38:41,440
And those are the ones that will prevail.

683
00:38:41,440 --> 00:38:42,920
So so let's let's go there.

684
00:38:42,920 --> 00:38:43,600
That's really interesting.

685
00:38:43,600 --> 00:38:48,400
Now, you're known as a skeptic of collectivist thought, right?

686
00:38:48,400 --> 00:38:51,040
We know, and there's this interesting dichotomy we were talking about

687
00:38:51,040 --> 00:38:55,520
of, you know, monolithic truth, but the utility functions interesting as well.

688
00:38:55,520 --> 00:38:58,440
Because in a sense, I mean, I know these folks are consequentialists,

689
00:38:58,440 --> 00:39:01,120
but in a sense, that's more leaning towards deontology.

690
00:39:01,840 --> 00:39:05,800
I did it again, deontology, you know, which is this idea that

691
00:39:05,800 --> 00:39:09,520
we have kind of like a principled approach to to morality.

692
00:39:09,760 --> 00:39:13,360
And I'm skeptical as well that it's possible to create such a utility

693
00:39:13,360 --> 00:39:15,360
function because it wouldn't really be parsimonious.

694
00:39:15,360 --> 00:39:19,440
But how do you wrestle that, that you have a simple utility function,

695
00:39:19,440 --> 00:39:21,880
even though you believe in diversity of ideas?

696
00:39:21,880 --> 00:39:24,240
Oh, no, I didn't say simple.

697
00:39:24,240 --> 00:39:24,560
Go on.

698
00:39:24,560 --> 00:39:27,920
Crucial point, the utility function could be extremely complex.

699
00:39:28,880 --> 00:39:30,480
And in fact, the utility function.

700
00:39:30,480 --> 00:39:33,600
So first of all, there's there's more than one level to this.

701
00:39:34,720 --> 00:39:38,560
You to let's say you believe in utilitarianism, right, which I don't,

702
00:39:38,560 --> 00:39:41,760
but have, you know, compared to the others, it's probably the least bad.

703
00:39:41,760 --> 00:39:43,040
Right. Yeah.

704
00:39:43,040 --> 00:39:47,440
Believing you tell if you believe in maximizing utility function

705
00:39:47,440 --> 00:39:51,120
that in no way sanctions collectivism.

706
00:39:52,320 --> 00:39:56,560
Collectivism is one particular strength that historically came out of that.

707
00:39:56,560 --> 00:39:57,280
Right.

708
00:39:57,280 --> 00:40:00,640
And, and, and, you know, again, and Bentham is responsible for it.

709
00:40:00,640 --> 00:40:04,560
But it's this notion that you should have a utility function

710
00:40:04,560 --> 00:40:06,240
in which everybody counts equally.

711
00:40:07,440 --> 00:40:09,680
This is now making a choice of utility function,

712
00:40:09,680 --> 00:40:11,280
which is different from having one.

713
00:40:11,280 --> 00:40:13,360
Okay, but I think you're saying something quite interesting as well,

714
00:40:13,360 --> 00:40:14,880
which is that at the moment,

715
00:40:15,600 --> 00:40:17,760
the utility is a function of market value,

716
00:40:17,760 --> 00:40:22,080
which is very much inspired by Adam Smith's hidden hand of the market.

717
00:40:22,080 --> 00:40:25,920
But I think your views against collectivism is very much against

718
00:40:25,920 --> 00:40:27,920
this idea of equality of outcome.

719
00:40:27,920 --> 00:40:29,200
And that's definitely not what you're saying.

720
00:40:29,200 --> 00:40:31,120
No, I mean, that's even going beyond that, right?

721
00:40:31,120 --> 00:40:35,600
Equality of outcome is actually irrational, frankly, to we could go into that.

722
00:40:35,600 --> 00:40:37,280
But, you know, you mentioned the market, right?

723
00:40:37,280 --> 00:40:39,200
And the market is the size utility.

724
00:40:39,200 --> 00:40:42,320
Again, that is a one way to decide.

725
00:40:42,320 --> 00:40:46,480
I mean, that is also a very critical approximation to what you really want.

726
00:40:46,480 --> 00:40:49,920
So actually, all we have with capitalism or carnimism at this point,

727
00:40:49,920 --> 00:40:53,360
in terms of utility function, are very imperfect, right?

728
00:40:54,160 --> 00:40:56,080
And that's even saying it generously.

729
00:40:56,080 --> 00:40:58,640
And really, our job is to try to come up with something better,

730
00:40:58,640 --> 00:41:00,800
which I totally think we can, right?

731
00:41:00,800 --> 00:41:03,600
And by the way, one very salient question here,

732
00:41:03,600 --> 00:41:06,720
which again, for economists, it's very salient,

733
00:41:06,720 --> 00:41:08,560
is this question of like,

734
00:41:08,560 --> 00:41:12,720
should you have one utility function overarching, controlling everything,

735
00:41:12,720 --> 00:41:14,560
even if it's complex, right?

736
00:41:14,560 --> 00:41:16,480
Or should you not, right?

737
00:41:16,480 --> 00:41:19,680
And that I think is a very interesting question, right?

738
00:41:19,680 --> 00:41:22,240
And there are good arguments in both directions, right?

739
00:41:22,240 --> 00:41:24,400
So let me just give you one silly example,

740
00:41:24,400 --> 00:41:26,640
which then I think also generalizes two other things.

741
00:41:26,640 --> 00:41:29,520
Does your brain have a singular utility function?

742
00:41:29,520 --> 00:41:31,440
And I think the answer is no.

743
00:41:31,440 --> 00:41:34,000
Now, you could say from an evolutionary point of view,

744
00:41:34,000 --> 00:41:36,000
the overarching utility is fitness.

745
00:41:36,640 --> 00:41:38,800
But then the way that cashes out in your brain

746
00:41:38,800 --> 00:41:43,760
is that your genes need to control this adaptive machine, right?

747
00:41:44,400 --> 00:41:49,440
In such a way that you give the machine freedom, right?

748
00:41:49,440 --> 00:41:51,520
To do things that the genes by themselves couldn't.

749
00:41:51,520 --> 00:41:53,280
But at the same time, at the end of the day,

750
00:41:53,280 --> 00:41:57,040
that machine has to subserve the propagation of those genes.

751
00:41:57,040 --> 00:41:58,480
And the way you do this, right,

752
00:41:58,480 --> 00:42:00,000
at least the way evolution seems to have done,

753
00:42:00,000 --> 00:42:01,360
and I think it makes a lot of sense,

754
00:42:01,360 --> 00:42:03,600
is that you don't just have one utility.

755
00:42:03,600 --> 00:42:06,880
You have several ones which correspond to your emotions.

756
00:42:07,840 --> 00:42:09,040
And then they fight it out.

757
00:42:09,760 --> 00:42:11,280
So I actually think there's this connection

758
00:42:11,280 --> 00:42:13,520
between rationality and the emotion that people don't make,

759
00:42:13,520 --> 00:42:17,360
which is that your emotions are really your utility functions.

760
00:42:17,360 --> 00:42:21,360
You just have different ones that cater to different things, right?

761
00:42:21,360 --> 00:42:23,840
You know, fear and anger and so on.

762
00:42:23,840 --> 00:42:25,520
And so I think in reality,

763
00:42:25,520 --> 00:42:27,600
we actually have multiple utility functions.

764
00:42:27,600 --> 00:42:30,160
But because, again, it gets to this problem that

765
00:42:30,160 --> 00:42:31,920
what we're trying to do is approximate something

766
00:42:31,920 --> 00:42:34,080
that is very complex and difficult to get at,

767
00:42:34,080 --> 00:42:35,440
maybe it is just one,

768
00:42:35,440 --> 00:42:37,280
but we're better off trying to approximate it

769
00:42:37,280 --> 00:42:38,560
with 10 or 20 different things

770
00:42:38,560 --> 00:42:40,560
than just trying to nail that one thing.

771
00:42:41,200 --> 00:42:42,000
That's really interesting.

772
00:42:42,000 --> 00:42:44,880
And is your view then on having this diversity

773
00:42:44,880 --> 00:42:47,600
of utility functions analogous to your views

774
00:42:47,600 --> 00:42:48,960
on the master algorithm?

775
00:42:49,680 --> 00:42:51,680
Huh. It's analogous,

776
00:42:51,680 --> 00:42:53,760
but you're actually talking about different dimensions, right?

777
00:42:53,760 --> 00:42:55,840
You could make a table where on one side

778
00:42:55,840 --> 00:42:57,840
you have all the different utilities,

779
00:42:57,840 --> 00:42:59,920
and then on the other side you have the algorithms.

780
00:42:59,920 --> 00:43:02,080
And now you can pair off any one of them.

781
00:43:02,080 --> 00:43:04,480
I can say, I'm going to pursue this,

782
00:43:04,480 --> 00:43:08,400
you know, minimize your fear using symbolism or minimum.

783
00:43:08,400 --> 00:43:09,680
So any combination is valid.

784
00:43:11,040 --> 00:43:12,160
Really, really interesting.

785
00:43:12,160 --> 00:43:15,520
Okay. And then let's get into meritocracy, for example.

786
00:43:15,520 --> 00:43:17,520
So at the moment, we do have the market system.

787
00:43:17,520 --> 00:43:19,680
And presumably you think that some people

788
00:43:19,680 --> 00:43:21,760
do genuinely have more market value than others.

789
00:43:22,560 --> 00:43:24,720
For sure. No. And by the way, I think,

790
00:43:25,360 --> 00:43:28,320
I'm definitely a big believer in meritocracy.

791
00:43:28,320 --> 00:43:31,200
I think. But what does it mean to you?

792
00:43:31,200 --> 00:43:34,000
Right. Very good. So let's get that down first, right?

793
00:43:34,000 --> 00:43:38,240
Meritocracy, so our goal is to have the society

794
00:43:38,240 --> 00:43:42,480
that functions best and provides best for everybody, right?

795
00:43:43,280 --> 00:43:45,200
And I mean, we could refine even that,

796
00:43:45,200 --> 00:43:47,680
but let's just take that for now as our assumption, right?

797
00:43:47,680 --> 00:43:49,200
But then if that is the case,

798
00:43:49,200 --> 00:43:52,320
one of our primary goals, maybe even the most important one,

799
00:43:52,320 --> 00:43:56,640
is to get everybody to contribute the most they can, right?

800
00:43:56,640 --> 00:43:58,880
Meritocracy is often seen as like,

801
00:43:58,880 --> 00:44:00,320
I'm going to rank all the people,

802
00:44:00,320 --> 00:44:02,400
and at the top is the greatest genius,

803
00:44:02,400 --> 00:44:04,320
and at the bottom is the most useless person,

804
00:44:04,320 --> 00:44:06,080
and this is wrong, right?

805
00:44:06,080 --> 00:44:09,520
Meritocracy is a many-dimension thing, right?

806
00:44:09,520 --> 00:44:12,160
The goal of meritocracy is to find for everybody

807
00:44:12,160 --> 00:44:14,720
what they're best at doing so that they can do it,

808
00:44:15,280 --> 00:44:18,000
maximize everybody's contribution to society, right?

809
00:44:19,120 --> 00:44:20,960
And this is a very complicated process.

810
00:44:20,960 --> 00:44:23,520
There isn't a single scale of intelligence or anything else.

811
00:44:24,080 --> 00:44:27,040
Having said that, it very much is the case

812
00:44:27,040 --> 00:44:30,720
that some people are better for some things than others, right?

813
00:44:30,720 --> 00:44:31,920
And if you deny that,

814
00:44:31,920 --> 00:44:34,320
you are actually in the process of destroying the society

815
00:44:34,320 --> 00:44:35,440
and making it dysfunctional.

816
00:44:35,440 --> 00:44:41,120
So I find the attacks on meritocracy extremely disturbing, right?

817
00:44:41,120 --> 00:44:42,480
And a lot of them are,

818
00:44:42,480 --> 00:44:45,040
I've talked with many people who have those beliefs, right?

819
00:44:45,040 --> 00:44:46,880
And the number one thing that they say is,

820
00:44:46,880 --> 00:44:48,320
it basically boils down to like,

821
00:44:48,320 --> 00:44:51,120
oh, meritocracy isn't perfect, so we should junk it.

822
00:44:51,920 --> 00:44:54,880
Something not being perfect has never been a reason to junk it.

823
00:44:54,880 --> 00:44:56,000
It's a reason to improve it.

824
00:44:56,000 --> 00:44:59,200
So there's a lot of room to improve in meritocracy,

825
00:44:59,200 --> 00:45:01,680
but if you throw it away, you are destroying society.

826
00:45:02,240 --> 00:45:06,640
Well, I mean, you can trace this back to our argument about utility.

827
00:45:06,640 --> 00:45:11,840
But the thing is though, if we had a value function

828
00:45:11,840 --> 00:45:14,160
which represented actual market contributions

829
00:45:14,160 --> 00:45:16,560
or even societal contributions, that would be one thing.

830
00:45:16,560 --> 00:45:20,480
But would you agree that we have a lot of game playing at the moment?

831
00:45:20,480 --> 00:45:23,360
So utility is based on playing the success game

832
00:45:23,360 --> 00:45:25,120
or the dominance game or the virtue game,

833
00:45:25,120 --> 00:45:26,560
as Will Stor said in his book.

834
00:45:26,560 --> 00:45:28,800
So we've got these kind of emerging games

835
00:45:28,800 --> 00:45:32,960
and it's not really representing utility.

836
00:45:33,600 --> 00:45:34,560
Well, absolutely.

837
00:45:34,560 --> 00:45:38,320
So far we've been talking about utility, right?

838
00:45:38,320 --> 00:45:39,760
But what happens in the real world

839
00:45:39,760 --> 00:45:41,760
is that there are multiple agents,

840
00:45:41,760 --> 00:45:43,360
each with their own different utility.

841
00:45:44,160 --> 00:45:47,120
And at this point, what you have is game theory, right?

842
00:45:47,120 --> 00:45:48,400
Game theory is just what you have

843
00:45:48,400 --> 00:45:50,960
when there's not a single optimization going on,

844
00:45:50,960 --> 00:45:52,400
but multiple optimizations

845
00:45:52,400 --> 00:45:55,120
which are partly contradictory, maybe partly not.

846
00:45:55,120 --> 00:45:57,280
So the best way to understand everything

847
00:45:57,280 --> 00:46:00,400
that we've been talking about, including society and evolution

848
00:46:00,400 --> 00:46:03,520
and even what happens inside your brain is as a big game.

849
00:46:04,240 --> 00:46:06,320
A much bigger and more complex game

850
00:46:06,320 --> 00:46:08,880
than game theorists and economists and so on

851
00:46:08,880 --> 00:46:12,480
and evolutionary biologists, right, prominently,

852
00:46:12,480 --> 00:46:14,960
have been able to handle in the past.

853
00:46:14,960 --> 00:46:18,720
But I think they are very much in the right track

854
00:46:18,720 --> 00:46:21,040
and we can understand a lot of these phenomena

855
00:46:21,040 --> 00:46:24,480
that you're referring to as they are games being played

856
00:46:24,480 --> 00:46:26,880
by people that have certain utilities, right?

857
00:46:27,440 --> 00:46:29,840
And now you are going to impose your, you know, like,

858
00:46:29,840 --> 00:46:31,120
and it's a game, right?

859
00:46:31,120 --> 00:46:32,960
I don't, you don't know who's going to win

860
00:46:32,960 --> 00:46:35,360
until you actually do the linear program

861
00:46:35,360 --> 00:46:37,440
and figure out how this is going to, you know,

862
00:46:38,000 --> 00:46:40,720
and of course games are, you know, in reality,

863
00:46:40,720 --> 00:46:43,040
you know, most games are not single round games, right?

864
00:46:43,040 --> 00:46:44,720
They're continuing games, right?

865
00:46:44,960 --> 00:46:46,880
So things get very, very interesting,

866
00:46:46,880 --> 00:46:50,080
but this I think is the most productive way

867
00:46:50,080 --> 00:46:51,200
to look at all of this.

868
00:46:51,200 --> 00:46:52,160
Okay, good, good.

869
00:46:52,160 --> 00:46:55,600
But then some might say that this is a platonic way

870
00:46:55,600 --> 00:46:57,280
of looking at the world

871
00:46:57,280 --> 00:46:59,600
and the world is actually much more complicated than that.

872
00:46:59,600 --> 00:47:01,920
And again, we're kind of fooled by randomness

873
00:47:01,920 --> 00:47:03,760
because we're anthropomorphizing the world

874
00:47:03,760 --> 00:47:05,520
and we're kind of framing it as a game.

875
00:47:05,520 --> 00:47:07,600
It might be much more complicated than that.

876
00:47:07,600 --> 00:47:09,440
And I've already said this a couple of times,

877
00:47:09,440 --> 00:47:11,440
but you know, the concept of power, for example,

878
00:47:11,440 --> 00:47:13,200
did when Napoleon said,

879
00:47:13,200 --> 00:47:16,000
I want the men to march into this country,

880
00:47:16,000 --> 00:47:18,880
is it just a simple kind of chain of command that goes down?

881
00:47:18,880 --> 00:47:19,440
No, it's not.

882
00:47:19,440 --> 00:47:20,880
It's so much more complicated than that.

883
00:47:20,880 --> 00:47:22,800
Well, yes, but that's, actually,

884
00:47:22,800 --> 00:47:23,920
I'm not even sure what you mean

885
00:47:23,920 --> 00:47:26,480
by when you say it's much more complicated than a game.

886
00:47:26,480 --> 00:47:27,840
Again, when I say a game,

887
00:47:28,800 --> 00:47:30,720
maybe what comes to your mind is something simple,

888
00:47:30,720 --> 00:47:33,920
like in a prisoner's dilemma, two players, two moves.

889
00:47:33,920 --> 00:47:36,800
It's a game with, you know, with a vast number of players,

890
00:47:36,800 --> 00:47:38,400
each with a vast number of moves.

891
00:47:39,280 --> 00:47:41,120
Interesting, but I think this gets to the core

892
00:47:41,120 --> 00:47:42,480
of what the rationalists talk about.

893
00:47:42,800 --> 00:47:44,000
They have these thought experiments.

894
00:47:44,000 --> 00:47:45,600
They talk about prisoners dilemma.

895
00:47:45,600 --> 00:47:47,440
They have that, I forget the name of that game

896
00:47:47,440 --> 00:47:48,640
where there's the two boxes,

897
00:47:48,640 --> 00:47:51,040
and you have to choose the box, I forget that.

898
00:47:51,040 --> 00:47:52,640
But I guess what I'm saying is that

899
00:47:53,280 --> 00:47:55,280
if you do have this rationalist conception of the world,

900
00:47:55,280 --> 00:47:58,160
and think about it in terms of game theory,

901
00:47:58,160 --> 00:47:59,920
just like the symbolists do,

902
00:47:59,920 --> 00:48:02,480
and the people who handcraft cognitive architectures do,

903
00:48:02,480 --> 00:48:04,160
or even with causality, for example,

904
00:48:04,160 --> 00:48:06,960
we create these variables, it's all anthropomorphic.

905
00:48:07,600 --> 00:48:12,160
Well, I would not, so let me put it this way, right?

906
00:48:12,960 --> 00:48:15,120
You can model almost anything,

907
00:48:16,080 --> 00:48:17,760
can is an important word here.

908
00:48:17,760 --> 00:48:20,400
You can model almost anything in the world,

909
00:48:20,400 --> 00:48:22,960
in any domain, from physics to psychology

910
00:48:22,960 --> 00:48:25,920
to sociology, name it, as optimizing a function.

911
00:48:26,880 --> 00:48:29,520
Whether you should is a debatable question,

912
00:48:29,520 --> 00:48:31,360
but you can, right?

913
00:48:31,360 --> 00:48:33,360
But now, what really happens is that

914
00:48:33,360 --> 00:48:35,040
there are many different optimizations

915
00:48:35,040 --> 00:48:36,560
going on at the same time,

916
00:48:36,560 --> 00:48:38,560
all the way from maximizing entropy

917
00:48:38,560 --> 00:48:40,480
to me deciding what I have for lunch today.

918
00:48:40,960 --> 00:48:44,000
And now what you have is all of these interlocking optimizations,

919
00:48:44,000 --> 00:48:46,320
and that's what I'm calling game theory, right?

920
00:48:46,320 --> 00:48:48,320
One of those optimizations is I'm Napoleon,

921
00:48:48,320 --> 00:48:49,600
I want to conquer Russia,

922
00:48:49,600 --> 00:48:52,080
you're the Tsar of Russia, you don't want to be conquered, right?

923
00:48:52,080 --> 00:48:53,680
And then we play a very complicated game,

924
00:48:53,680 --> 00:48:56,160
which includes other agents, like your soldiers,

925
00:48:56,160 --> 00:48:58,960
which maybe, you know, I, a French soldier,

926
00:48:58,960 --> 00:49:00,160
you know, want to conquer Russia,

927
00:49:00,160 --> 00:49:01,600
but I also want to stay alive,

928
00:49:01,600 --> 00:49:03,120
whereas an opponent really couldn't care less

929
00:49:03,120 --> 00:49:05,200
whether I, in particular, stay alive or not,

930
00:49:05,200 --> 00:49:06,720
as long as he conquers Russia in the end.

931
00:49:06,720 --> 00:49:10,080
So this very complex game, I think, is what goes on.

932
00:49:10,160 --> 00:49:12,640
I don't think framing things in this way

933
00:49:12,640 --> 00:49:14,000
is anthropomorphizing them.

934
00:49:14,000 --> 00:49:16,080
In fact, I think this is our best hope

935
00:49:16,080 --> 00:49:17,760
to not anthropomorphize things,

936
00:49:17,760 --> 00:49:19,200
although at the end of the day,

937
00:49:19,200 --> 00:49:21,520
I think you can look at almost anything

938
00:49:21,520 --> 00:49:25,280
and see a ghost of anthropomorphization there.

939
00:49:25,280 --> 00:49:28,000
But if there's a less anthropomorphic way

940
00:49:28,000 --> 00:49:30,000
to look at the universe than through this lens,

941
00:49:30,000 --> 00:49:31,600
I'd be interested to see what it is.

942
00:49:31,600 --> 00:49:33,040
Well, the only reason I'm saying this is,

943
00:49:33,040 --> 00:49:35,360
first of all, I want to play devil's advocate a little bit,

944
00:49:35,360 --> 00:49:37,840
and we even spoke about the blind men

945
00:49:37,840 --> 00:49:39,360
and the elephant a little while ago,

946
00:49:39,360 --> 00:49:42,480
and I'm sure folks on the left, as they did,

947
00:49:42,480 --> 00:49:44,480
they criticized Ayan Ran, for example,

948
00:49:44,480 --> 00:49:46,720
and they said that she had this very transactional way

949
00:49:46,720 --> 00:49:48,720
of viewing the world as this kind of

950
00:49:48,720 --> 00:49:51,680
Nash equilibrium of self-interested actors.

951
00:49:51,680 --> 00:49:53,120
And are we guilty of doing that?

952
00:49:53,120 --> 00:49:55,760
Are we kind of cutting off many aspects of the truth

953
00:49:55,760 --> 00:49:57,040
by doing this? I guess that's what I'm saying.

954
00:49:57,920 --> 00:50:01,600
So we are always cutting off some aspect of the truth

955
00:50:01,600 --> 00:50:03,920
when we look at anything in any way,

956
00:50:03,920 --> 00:50:06,640
which is not a reason to look at nothing in no way.

957
00:50:07,280 --> 00:50:10,880
So I think this is a very productive way to look at things,

958
00:50:10,880 --> 00:50:12,080
but not the only one.

959
00:50:12,080 --> 00:50:13,760
It doesn't exhaust what there is to be said,

960
00:50:13,760 --> 00:50:15,440
but I personally feel like it's the one

961
00:50:15,440 --> 00:50:17,600
where the most progress can come from.

962
00:50:17,600 --> 00:50:18,640
Interesting.

963
00:50:18,640 --> 00:50:22,000
Now, that's sort of like Ayan Randian

964
00:50:22,000 --> 00:50:23,280
simplification of the world.

965
00:50:25,200 --> 00:50:29,280
Looking at things this way does not imply over-simplifying them.

966
00:50:29,280 --> 00:50:32,240
On the contrary, I would actually say it gives us a handle

967
00:50:32,240 --> 00:50:34,960
on how to go into the complexity and not get lost

968
00:50:35,440 --> 00:50:37,440
and not devolve into like platitudes

969
00:50:37,440 --> 00:50:39,120
or over-simplifying ideologies.

970
00:50:39,920 --> 00:50:42,320
The fact that there's a mathematical component to this

971
00:50:42,320 --> 00:50:43,120
is very important.

972
00:50:44,080 --> 00:50:45,600
Mathematics, when you can apply it,

973
00:50:45,600 --> 00:50:47,840
gives you a very solid handle on things.

974
00:50:47,840 --> 00:50:50,880
We are now at the point where we can handle

975
00:50:50,880 --> 00:50:53,840
a lot of things mathematically slash computationally

976
00:50:53,840 --> 00:50:54,800
that we couldn't before.

977
00:50:54,800 --> 00:50:57,760
So when von Neumann invented game theory,

978
00:50:57,760 --> 00:51:00,800
he said, this is the future of the social sciences.

979
00:51:00,800 --> 00:51:02,640
And so far it hasn't been,

980
00:51:02,640 --> 00:51:04,240
but I think we're actually now at the point

981
00:51:04,800 --> 00:51:06,160
partly because we have the data.

982
00:51:07,280 --> 00:51:09,680
We actually can now usefully apply this point of view

983
00:51:09,680 --> 00:51:11,200
in a way that we couldn't before.

984
00:51:11,200 --> 00:51:13,120
How far it takes us, we'll see.

985
00:51:13,840 --> 00:51:15,760
It's not the only possible to look at things,

986
00:51:15,760 --> 00:51:18,720
but I do think it's probably the most productive at this point.

987
00:51:18,720 --> 00:51:19,360
Interesting.

988
00:51:19,360 --> 00:51:20,000
Okay.

989
00:51:20,000 --> 00:51:22,240
So coming back to this rationalist school of thought,

990
00:51:22,240 --> 00:51:25,680
one thing that I'm interested in is morality.

991
00:51:25,680 --> 00:51:27,520
But let's go one step at a time.

992
00:51:27,520 --> 00:51:29,600
So I think Bostrom came up with this idea

993
00:51:29,600 --> 00:51:31,280
of instrumental convergence,

994
00:51:31,280 --> 00:51:33,040
which is this notion that in the pursuit

995
00:51:33,040 --> 00:51:34,480
of doing a particular task,

996
00:51:34,480 --> 00:51:37,760
the intelligence system might actually potentially kill

997
00:51:37,760 --> 00:51:40,080
everyone on the planet or do adjacent.

998
00:51:40,080 --> 00:51:41,840
And this is where the interesting thing comes from.

999
00:51:41,840 --> 00:51:45,200
So one task, but adjacent multitask ability

1000
00:51:45,200 --> 00:51:48,240
and potential intelligence and so on.

1001
00:51:48,240 --> 00:51:50,240
So there was an example of a cauldron.

1002
00:51:50,240 --> 00:51:53,440
So you've got someone filling up a cauldron

1003
00:51:53,440 --> 00:51:55,440
and in the pursuit of filling up the cauldron

1004
00:51:55,440 --> 00:51:56,320
to just the right level,

1005
00:51:56,320 --> 00:51:59,120
they might kill the person who looks after the cauldron room

1006
00:51:59,120 --> 00:52:01,840
just so that the agent could do it more efficiently.

1007
00:52:02,800 --> 00:52:05,200
Are you cynical about that or what do you think?

1008
00:52:06,000 --> 00:52:07,760
No, I'm not cynical about that,

1009
00:52:07,760 --> 00:52:09,040
but let me put it this way.

1010
00:52:09,680 --> 00:52:13,920
I don't lose any sleep worrying about the paperclip factory

1011
00:52:13,920 --> 00:52:15,360
that's going to take over the world.

1012
00:52:16,400 --> 00:52:19,600
I think you have to take that as a philosopher's thought experiment.

1013
00:52:21,120 --> 00:52:23,200
The philosopher being Nick in this case.

1014
00:52:24,320 --> 00:52:29,680
I think there's a real danger that there's putting its finger on,

1015
00:52:30,400 --> 00:52:34,880
but it's also mistaking reality for something else.

1016
00:52:34,880 --> 00:52:36,160
So let's look at both parts of that.

1017
00:52:36,880 --> 00:52:42,000
The real danger is that if you give an AI

1018
00:52:43,600 --> 00:52:47,520
an oversimplified, a hugely oversimplified objective function,

1019
00:52:47,520 --> 00:52:50,160
and at the same time a very large amount of power,

1020
00:52:50,160 --> 00:52:53,040
bad things will happen and we need to worry about that.

1021
00:52:55,360 --> 00:52:57,040
By the way, this is already a problem today

1022
00:52:57,040 --> 00:52:58,800
in many maybe more modest ways,

1023
00:52:59,520 --> 00:53:01,040
but also more relevant, frankly.

1024
00:53:01,600 --> 00:53:02,640
So what do you do?

1025
00:53:02,640 --> 00:53:06,480
First of all, the utility function needs to be as rich

1026
00:53:07,280 --> 00:53:11,680
and as complex and as subtle as the people that it's trying to serve.

1027
00:53:13,680 --> 00:53:17,120
As long as what you have to take a really world example today,

1028
00:53:17,120 --> 00:53:21,520
social media, who are all designed to just maximize engagement,

1029
00:53:22,880 --> 00:53:26,400
you have an enormous amount of AI at the service of maximizing engagement.

1030
00:53:26,880 --> 00:53:31,760
I understand why companies do it and partly they have the right to.

1031
00:53:31,760 --> 00:53:32,880
We can get into that.

1032
00:53:32,880 --> 00:53:36,560
But the point is, it's ignoring too many things.

1033
00:53:36,560 --> 00:53:41,360
So one line of defense against is that you have to enrich your utility function

1034
00:53:41,360 --> 00:53:44,640
until it's like a bit, and then this is an open-ended problem.

1035
00:53:45,600 --> 00:53:47,760
We're never going to have the final utility function.

1036
00:53:47,760 --> 00:53:50,240
It's something that the AIs have to be continually,

1037
00:53:50,240 --> 00:53:52,960
you know, AIs, I think Stuart Russell said this and I agree,

1038
00:53:52,960 --> 00:53:57,440
like they should spend half their time figuring out what the utility function is

1039
00:53:57,440 --> 00:53:59,120
and then the other half maximizing it.

1040
00:53:59,120 --> 00:54:02,480
Whereas today it's like I wrote down my utility function in one line

1041
00:54:02,480 --> 00:54:05,920
and now I spend this enormous amount of power maximizing it.

1042
00:54:05,920 --> 00:54:07,120
So that's one line.

1043
00:54:07,120 --> 00:54:11,840
The other line or like one other line is you have to put constraints on the machine.

1044
00:54:11,840 --> 00:54:12,960
Hard constraints.

1045
00:54:12,960 --> 00:54:15,840
You can win the pursuit of this utility function.

1046
00:54:15,840 --> 00:54:17,280
You can think of it as like, you know,

1047
00:54:17,280 --> 00:54:19,280
terms with infinite weight in the utility function.

1048
00:54:19,280 --> 00:54:20,480
You can't go outside this.

1049
00:54:20,480 --> 00:54:24,240
And then the other one is the single biggest reason why I sort of like

1050
00:54:24,240 --> 00:54:28,000
this paperclip experiment is silly is that, you know,

1051
00:54:28,000 --> 00:54:29,920
along with that paperclip factoring the world,

1052
00:54:29,920 --> 00:54:32,480
there are going to be a million other AIs, you know,

1053
00:54:32,480 --> 00:54:33,920
each of which is doing the same thing.

1054
00:54:33,920 --> 00:54:38,400
So none of them is ever going to acquire the power to cause that damage

1055
00:54:38,400 --> 00:54:41,360
unless it's doing something very different from just trying to make paperclips.

1056
00:54:41,360 --> 00:54:44,960
So at some level that example is extremely unrealistic

1057
00:54:44,960 --> 00:54:46,720
and leads us down the wrong track.

1058
00:54:46,720 --> 00:54:48,160
Right, loads of places to go there.

1059
00:54:48,160 --> 00:54:52,320
But first of all, I think you do believe in AI alignment then

1060
00:54:52,320 --> 00:54:54,240
because you're saying exactly the same as what they do,

1061
00:54:54,240 --> 00:54:56,800
which is that we need to have the utility function

1062
00:54:56,800 --> 00:54:59,920
that represents the richness of the human condition.

1063
00:54:59,920 --> 00:55:00,880
So that's the first thing.

1064
00:55:00,880 --> 00:55:03,680
So essentially you're all on board of alignment.

1065
00:55:03,680 --> 00:55:07,680
Well, I believe in AI alignment in one sense of it.

1066
00:55:08,320 --> 00:55:11,360
Many different things get go under that umbrella of AI alignment.

1067
00:55:11,360 --> 00:55:12,080
Right.

1068
00:55:12,080 --> 00:55:15,200
I think in the near term, thinking of things into,

1069
00:55:15,200 --> 00:55:17,600
I mean, if I like, let me put it this way.

1070
00:55:17,600 --> 00:55:22,880
If AI alignment is just trying to have a really accurate utility function,

1071
00:55:22,880 --> 00:55:23,840
then yes.

1072
00:55:23,840 --> 00:55:26,000
And then the machines are optimizing that function.

1073
00:55:26,000 --> 00:55:26,640
Absolutely.

1074
00:55:26,640 --> 00:55:27,120
Right.

1075
00:55:27,120 --> 00:55:27,600
Yeah.

1076
00:55:27,600 --> 00:55:31,520
And in the near term, I think talking about AI alignment is a little,

1077
00:55:31,520 --> 00:55:35,280
I mean, the problem that I have with the concept of alignment

1078
00:55:35,280 --> 00:55:37,200
is that goes far beyond that.

1079
00:55:37,200 --> 00:55:41,840
It tends to see AIs as these independent agents

1080
00:55:42,640 --> 00:55:46,800
that we have to align their goals to ours.

1081
00:55:46,800 --> 00:55:47,280
Right.

1082
00:55:47,760 --> 00:55:50,400
And if that just caches out as like,

1083
00:55:50,400 --> 00:55:51,840
here's the utility function, that's fine.

1084
00:55:51,840 --> 00:55:54,320
But the problem is AIs are not independent agents.

1085
00:55:54,320 --> 00:55:55,600
AIs are our tools.

1086
00:55:56,640 --> 00:55:58,320
Just to push back on that a little bit,

1087
00:55:58,320 --> 00:56:00,880
because I always had that conception of these folks.

1088
00:56:00,880 --> 00:56:03,120
I thought I was arguing against people who believed

1089
00:56:03,120 --> 00:56:04,960
in a pure monolithic intelligence.

1090
00:56:04,960 --> 00:56:07,520
And a lot of them are transhumanists actually,

1091
00:56:07,520 --> 00:56:11,680
and they say that they want to ensure human flourishing

1092
00:56:11,680 --> 00:56:14,960
through the use of AIs in tandem,

1093
00:56:14,960 --> 00:56:18,160
almost as a kind of extended mind from David Chalmers.

1094
00:56:18,160 --> 00:56:21,760
But then I really wanted to get into their fears

1095
00:56:21,760 --> 00:56:24,400
of recursive self-improving intelligence

1096
00:56:24,400 --> 00:56:25,360
and superintelligence.

1097
00:56:25,360 --> 00:56:29,360
Because when you do have this kind of heterogeneous approach

1098
00:56:29,360 --> 00:56:31,120
to humans and machines,

1099
00:56:31,120 --> 00:56:33,040
there are going to be bottlenecks everywhere.

1100
00:56:33,040 --> 00:56:35,920
Now, I like to think of it a bit like the market efficiency

1101
00:56:35,920 --> 00:56:38,800
hypothesis, which is that you reach an equilibria

1102
00:56:38,800 --> 00:56:43,120
where the individual actors in the market become more efficient,

1103
00:56:43,120 --> 00:56:44,640
will become more efficient programmers.

1104
00:56:44,640 --> 00:56:46,000
Because we're using codecs.

1105
00:56:46,000 --> 00:56:47,840
But we will reach a limit, surely.

1106
00:56:48,720 --> 00:56:53,040
Well, to touch on transhumanism for just a second,

1107
00:56:53,040 --> 00:56:55,760
because I do agree, at least sociologically,

1108
00:56:55,760 --> 00:56:57,280
a lot of that crowd is the same.

1109
00:56:58,720 --> 00:56:59,840
Let me put it this way.

1110
00:56:59,840 --> 00:57:01,920
And I'm sure this is a controversial statement.

1111
00:57:01,920 --> 00:57:05,600
But maybe in the long run, the AIs should take over the world.

1112
00:57:06,480 --> 00:57:09,920
Why are we so arrogant that we think whatever the AI is,

1113
00:57:09,920 --> 00:57:11,440
it should always be there to serve us.

1114
00:57:12,240 --> 00:57:13,280
We are a step.

1115
00:57:13,280 --> 00:57:16,480
If you take the long view of this, we're a step in evolution.

1116
00:57:17,280 --> 00:57:17,920
We're amazing.

1117
00:57:17,920 --> 00:57:20,560
Maybe I'm a human chauvinist, but I do think we are amazing.

1118
00:57:20,560 --> 00:57:21,920
But we're not the last word.

1119
00:57:23,680 --> 00:57:28,160
So the other day, I tweeted something that is maybe provocative,

1120
00:57:28,160 --> 00:57:31,600
but it's like, I think in Gemswich, which is,

1121
00:57:32,240 --> 00:57:36,000
I said that the killer app of humans is producing AI.

1122
00:57:37,040 --> 00:57:40,320
Maybe our role in evolution is that we're going to produce an AI.

1123
00:57:41,040 --> 00:57:43,920
That is the next level of whatever you like.

1124
00:57:43,920 --> 00:57:46,880
Consciousness, intelligence, et cetera, et cetera.

1125
00:57:46,880 --> 00:57:49,520
And so the notion that in the very long term,

1126
00:57:49,520 --> 00:57:51,760
the AIs should still be there to serve us,

1127
00:57:52,320 --> 00:57:54,240
by this point of view, is actually silly.

1128
00:57:55,280 --> 00:57:59,040
Right, but a lot of folks, let's say the ethics folks,

1129
00:57:59,040 --> 00:58:00,160
would find it horrifying.

1130
00:58:00,960 --> 00:58:03,040
And I was speaking to Irina actually yesterday,

1131
00:58:03,040 --> 00:58:05,600
and she said something a little bit tongue in cheek,

1132
00:58:05,600 --> 00:58:06,400
which is that which is actually...

1133
00:58:06,400 --> 00:58:07,360
Who, sorry?

1134
00:58:07,360 --> 00:58:10,240
Irina from Montreal, Mila, and Irina Rich.

1135
00:58:10,400 --> 00:58:11,440
Oh yeah, I know her, yeah.

1136
00:58:11,440 --> 00:58:13,360
We were classmates at UC Irvine.

1137
00:58:13,360 --> 00:58:15,280
Amazing, yeah, I really love her.

1138
00:58:15,280 --> 00:58:16,960
But no, she was kind of joking

1139
00:58:16,960 --> 00:58:20,400
that we should almost align human values to the AGI values.

1140
00:58:21,200 --> 00:58:22,880
Well, that I find alarming.

1141
00:58:23,440 --> 00:58:25,840
Well, I think she was saying it tongue in cheek.

1142
00:58:25,840 --> 00:58:27,680
I'm not alarmed by a lot of things, but yeah.

1143
00:58:28,240 --> 00:58:32,880
But what do you think about this ethical concern

1144
00:58:32,880 --> 00:58:35,280
that if it is the case that you believe

1145
00:58:35,280 --> 00:58:38,880
that we're just one rung on the ladder and transhumanism

1146
00:58:38,960 --> 00:58:40,880
is more AI than it is human?

1147
00:58:41,520 --> 00:58:42,800
People would find that horrifying.

1148
00:58:43,600 --> 00:58:47,840
Well, I understand why people would find that horrifying.

1149
00:58:47,840 --> 00:58:49,600
And I mean, again, we have to distinguish

1150
00:58:49,600 --> 00:58:51,760
the short from the meaning from the long term.

1151
00:58:51,760 --> 00:58:52,880
When I say something like this,

1152
00:58:52,880 --> 00:58:55,200
I'm talking about the very long term, right?

1153
00:58:55,200 --> 00:58:58,400
Trying to make humans subservient to AI today

1154
00:58:58,400 --> 00:59:00,240
is a horrifying idea, right?

1155
00:59:00,240 --> 00:59:01,920
Now, I think the reason a lot of people

1156
00:59:01,920 --> 00:59:04,560
are horrified with this idea period, right?

1157
00:59:04,640 --> 00:59:08,240
Is natural, but in my view, naive is just,

1158
00:59:08,880 --> 00:59:12,320
they are seeing humans as the end goal.

1159
00:59:14,000 --> 00:59:15,360
If humans are the end goal,

1160
00:59:15,360 --> 00:59:17,200
then the idea that they should be subservient

1161
00:59:17,200 --> 00:59:20,720
to developing the next level of AI is horrifying.

1162
00:59:20,720 --> 00:59:25,600
If you have a moral system where humans are the be all

1163
00:59:25,600 --> 00:59:27,760
and end all, then all of this is horrifying.

1164
00:59:27,760 --> 00:59:30,560
But again, if you take the long view of evolution,

1165
00:59:31,840 --> 00:59:33,600
humans are not the be all and end all.

1166
00:59:34,240 --> 00:59:36,080
Okay, I mean, eventually this might take us

1167
00:59:36,080 --> 00:59:37,760
to the effective altruism discussion.

1168
00:59:37,760 --> 00:59:39,920
But I think, as we were saying,

1169
00:59:39,920 --> 00:59:41,680
Sam Harris recently had a podcast

1170
00:59:41,680 --> 00:59:43,120
talking about the FTX disaster.

1171
00:59:43,120 --> 00:59:44,480
And he was kind of making the argument

1172
00:59:44,480 --> 00:59:45,840
that we're all consequentialists,

1173
00:59:45,840 --> 00:59:47,120
even if we don't realize it,

1174
00:59:47,120 --> 00:59:49,280
but there are different degrees of consequentialism.

1175
00:59:49,280 --> 00:59:52,560
And I think a lot of the ethics folks at the moment,

1176
00:59:52,560 --> 00:59:54,480
they really, really don't like what's going on

1177
00:59:54,480 --> 00:59:55,840
with long termism.

1178
00:59:55,840 --> 00:59:58,320
And it's because there's this slippery slope

1179
00:59:58,320 --> 01:00:00,880
of the kind of horizon of consequentialism.

1180
01:00:00,880 --> 01:00:02,000
So with Nick Bostrom,

1181
01:00:02,000 --> 01:00:03,600
he came up with this number

1182
01:00:03,600 --> 01:00:05,920
that there could be simulated humans

1183
01:00:06,640 --> 01:00:08,480
living on other planets in the future.

1184
01:00:08,480 --> 01:00:09,440
It's a very big number.

1185
01:00:09,440 --> 01:00:12,160
I think it's got a lot of zeros on it.

1186
01:00:12,160 --> 01:00:15,600
And what's to stop us from just making the argument

1187
01:00:15,600 --> 01:00:18,080
and what's to stop AIs from making the argument

1188
01:00:18,080 --> 01:00:20,720
that those simulated lives have more value than our lives?

1189
01:00:22,160 --> 01:00:23,840
Okay, there's a lot to unpack there.

1190
01:00:24,720 --> 01:00:26,480
So, but let's take this one step at a time.

1191
01:00:27,280 --> 01:00:30,720
I very much by the idea of effective altruism on principle.

1192
01:00:30,720 --> 01:00:33,280
I think that is the way to go about a lot of things.

1193
01:00:33,280 --> 01:00:34,320
I think in some ways,

1194
01:00:35,520 --> 01:00:38,240
if you are not an effective altruist,

1195
01:00:38,240 --> 01:00:41,920
maybe unconsciously, you are being irrational

1196
01:00:41,920 --> 01:00:43,840
or maybe evil, right?

1197
01:00:43,840 --> 01:00:45,120
If you believe in altruism,

1198
01:00:45,120 --> 01:00:47,440
I mean, think about both parts of that, right?

1199
01:00:47,440 --> 01:00:50,800
If altruism is good, then let's say we take that, right?

1200
01:00:50,800 --> 01:00:53,120
And then why should you be in favor

1201
01:00:53,120 --> 01:00:55,520
of ineffective altruism, right?

1202
01:00:55,520 --> 01:00:56,800
If you're an altruist,

1203
01:00:56,800 --> 01:00:58,320
if you want the good of other people,

1204
01:00:58,320 --> 01:01:00,960
you should try to do the best you can, right?

1205
01:01:00,960 --> 01:01:03,600
And so, for example, I very much by the notion that like,

1206
01:01:03,600 --> 01:01:06,640
you want to make the most money you can,

1207
01:01:06,640 --> 01:01:08,320
so then you can give away that money

1208
01:01:08,320 --> 01:01:10,720
as opposed to volunteering at the soup kitchen.

1209
01:01:10,720 --> 01:01:12,400
Volunteering at the soup kitchen for, say,

1210
01:01:12,400 --> 01:01:14,160
someone with a PhD in machine learning

1211
01:01:14,160 --> 01:01:16,480
is an ineffective form of altruism.

1212
01:01:16,480 --> 01:01:18,240
Now, having said that,

1213
01:01:18,240 --> 01:01:22,240
I think the focus on the long term has been in many ways,

1214
01:01:22,240 --> 01:01:25,120
I mean, certainly the long term is important, right?

1215
01:01:25,120 --> 01:01:27,840
But the problem with the whole effective altruism movement

1216
01:01:27,840 --> 01:01:30,240
is that it got overly focused on that,

1217
01:01:30,240 --> 01:01:31,760
and we can talk about why.

1218
01:01:31,760 --> 01:01:34,400
And then even, and then a further mistake

1219
01:01:34,400 --> 01:01:35,760
is that it got overly focused

1220
01:01:35,760 --> 01:01:37,920
on these supposedly existential dangers

1221
01:01:37,920 --> 01:01:41,120
that are much less of a big deal than people think like AI.

1222
01:01:41,120 --> 01:01:44,160
So between effective altruism and fixating on AI

1223
01:01:44,160 --> 01:01:46,720
as an existential danger lies a huge gulf.

1224
01:01:46,720 --> 01:01:49,200
I'm for effective altruism, I think, you know,

1225
01:01:49,200 --> 01:01:53,280
the long term, you know, there's ins and outs there, right?

1226
01:01:53,280 --> 01:01:55,840
And then this focus on like these existential dangers

1227
01:01:56,400 --> 01:01:57,840
is very problematic.

1228
01:01:57,840 --> 01:02:00,080
You know, for example, you know, to get back to the,

1229
01:02:00,080 --> 01:02:02,240
you know, Bostromian notion of like all these minds

1230
01:02:02,240 --> 01:02:04,080
that matter more than us and whatnot,

1231
01:02:04,080 --> 01:02:05,600
there is a basic idea, right,

1232
01:02:05,600 --> 01:02:07,440
that like any economist knows,

1233
01:02:07,440 --> 01:02:09,600
which is that you have to discount the future.

1234
01:02:09,600 --> 01:02:12,560
And the question is what your discount rate is, right?

1235
01:02:12,560 --> 01:02:14,960
And if your discount rate is high, right,

1236
01:02:14,960 --> 01:02:17,200
those minds matter not at all.

1237
01:02:17,200 --> 01:02:19,280
And now why do you have that discount rate?

1238
01:02:19,280 --> 01:02:20,320
The primary reason is that

1239
01:02:20,320 --> 01:02:22,480
there's uncertainty about the future, right?

1240
01:02:22,480 --> 01:02:27,200
I have to weigh the certain benefit of helping you today

1241
01:02:27,200 --> 01:02:29,200
with the increasingly hypothetical benefit

1242
01:02:29,200 --> 01:02:30,000
of helping your mind.

1243
01:02:30,000 --> 01:02:32,160
There's less and less likely to exist in the future.

1244
01:02:32,160 --> 01:02:33,680
So in many of those cases,

1245
01:02:33,680 --> 01:02:36,240
the present and the short-term do win.

1246
01:02:36,240 --> 01:02:38,080
Okay, but a couple of things to contrast that.

1247
01:02:38,080 --> 01:02:40,880
So a lot of effective altruism is this idea

1248
01:02:40,880 --> 01:02:42,800
that we're born with faulty programming, right?

1249
01:02:42,800 --> 01:02:45,200
So we have these views, you know,

1250
01:02:45,200 --> 01:02:48,400
like we have this concept of moral value

1251
01:02:48,400 --> 01:02:51,200
and it gets discounted in space and time, right?

1252
01:02:51,200 --> 01:02:53,440
So we need ways of overcoming our programming.

1253
01:02:53,440 --> 01:02:56,560
But you were saying that we should be thinking about this,

1254
01:02:56,560 --> 01:02:58,880
but contrast that with your, you know,

1255
01:02:58,880 --> 01:03:01,760
with your statement about Ayan Rand earlier, right?

1256
01:03:01,760 --> 01:03:04,480
So Ayan Rand was very, very transactional

1257
01:03:04,480 --> 01:03:07,600
because I think the folks that criticize this movement

1258
01:03:07,600 --> 01:03:10,080
are suspicious that we are actually being

1259
01:03:10,080 --> 01:03:12,240
a bit more like Ayan Rand,

1260
01:03:12,240 --> 01:03:14,960
but with the guise of altruism.

1261
01:03:14,960 --> 01:03:18,720
And I think they think of the FTX disaster

1262
01:03:18,720 --> 01:03:20,640
as being kind of like evidence of that.

1263
01:03:21,920 --> 01:03:23,840
A lot of different points there.

1264
01:03:23,840 --> 01:03:26,400
The FTX disaster actually has nothing whatsoever

1265
01:03:26,400 --> 01:03:28,240
to do with any of this, right?

1266
01:03:28,240 --> 01:03:30,720
Sam Beckman Fried was one guy or is one guy,

1267
01:03:30,720 --> 01:03:32,240
funny that I used the past tense.

1268
01:03:32,240 --> 01:03:35,520
He's one guy who believed in effective altruism,

1269
01:03:35,520 --> 01:03:36,560
good for him, right?

1270
01:03:36,560 --> 01:03:39,360
He was, I mean, the whole FTX thing was also obviously,

1271
01:03:39,360 --> 01:03:40,960
I mean, yeah, we could get into that,

1272
01:03:40,960 --> 01:03:43,360
but the point is you should not,

1273
01:03:44,080 --> 01:03:47,280
I understand why people's image of effective altruism

1274
01:03:47,280 --> 01:03:49,440
would be tainted by what happened with Sam Beckman Fried,

1275
01:03:49,440 --> 01:03:50,880
but really it shouldn't be, right?

1276
01:03:51,360 --> 01:03:54,480
An idea is not responsible for the mistakes

1277
01:03:54,480 --> 01:03:57,280
that its believers make in unrelated domains,

1278
01:03:57,280 --> 01:04:00,160
point one, point two, transactionalism.

1279
01:04:00,160 --> 01:04:02,480
There is nothing in what I've said whatsoever

1280
01:04:02,480 --> 01:04:05,440
that implies transactionalism, in fact, the opposite, right?

1281
01:04:05,440 --> 01:04:09,040
I think relationalism is actually the key concept.

1282
01:04:09,040 --> 01:04:12,640
And part of this is that games are not one shot.

1283
01:04:12,640 --> 01:04:14,400
Your games are played in a repeated way.

1284
01:04:14,400 --> 01:04:15,680
And famously, for example,

1285
01:04:15,680 --> 01:04:17,520
if you play things like Prisoners of the Lemon

1286
01:04:17,520 --> 01:04:19,680
and whatnot repeated, like you're like,

1287
01:04:19,760 --> 01:04:21,600
cooperate, defect and whatnot,

1288
01:04:21,600 --> 01:04:23,680
as soon as you start bringing in these other things,

1289
01:04:23,680 --> 01:04:25,440
like that make things more realistic,

1290
01:04:25,440 --> 01:04:26,800
you actually start to get behavior

1291
01:04:26,800 --> 01:04:29,360
that is much more, what's the way to put it,

1292
01:04:29,360 --> 01:04:32,080
rational in some ways and human and whatnot, right?

1293
01:04:32,080 --> 01:04:34,400
Another one is that traditional economics,

1294
01:04:34,400 --> 01:04:36,560
which I think Ayn Rand was influenced by,

1295
01:04:36,560 --> 01:04:38,640
viewed and still views the world as linear,

1296
01:04:39,360 --> 01:04:40,640
but the world is non-linear.

1297
01:04:41,200 --> 01:04:43,680
Once you start seeing the world as non-linear,

1298
01:04:43,680 --> 01:04:45,600
all of these things really change,

1299
01:04:45,600 --> 01:04:47,600
the face of them changes, right?

1300
01:04:47,680 --> 01:04:49,840
So I think we have to look at all these concepts

1301
01:04:49,840 --> 01:04:51,840
in this view, right?

1302
01:04:51,840 --> 01:04:54,480
And we want to focus on the long, so...

1303
01:04:56,640 --> 01:04:59,600
So to get back to your first point, right?

1304
01:05:00,160 --> 01:05:01,760
We are born with faulty programming.

1305
01:05:02,800 --> 01:05:03,520
Part of our fa...

1306
01:05:03,520 --> 01:05:06,240
And that's what if effective alteration

1307
01:05:06,240 --> 01:05:07,760
is there to overcome, right?

1308
01:05:08,320 --> 01:05:09,840
Part of our faulty programming

1309
01:05:09,840 --> 01:05:12,000
is that our discount rate is too high,

1310
01:05:13,120 --> 01:05:14,640
because we evolved in a world

1311
01:05:14,640 --> 01:05:16,400
where your time horizon was very short.

1312
01:05:17,360 --> 01:05:18,640
The fact that it's too high

1313
01:05:18,640 --> 01:05:20,400
doesn't mean that we should make it zero

1314
01:05:21,600 --> 01:05:23,040
and care only about the future.

1315
01:05:23,600 --> 01:05:24,400
But what would...

1316
01:05:24,400 --> 01:05:26,560
You know, the ethics folks who advocate

1317
01:05:26,560 --> 01:05:28,160
for gatekeeping and paternalism,

1318
01:05:29,120 --> 01:05:30,960
couldn't you just say that they're doing the same thing?

1319
01:05:32,480 --> 01:05:34,160
Well, you should ask them, right?

1320
01:05:34,160 --> 01:05:38,000
Wouldn't they lead by saying our programming is faulty

1321
01:05:38,000 --> 01:05:40,080
and therefore, you know, we need to...

1322
01:05:40,080 --> 01:05:42,800
No, I mean, look, we can...

1323
01:05:42,800 --> 01:05:45,760
So part one, we can debate whether our programming

1324
01:05:45,760 --> 01:05:47,520
is faulty or not and why.

1325
01:05:47,520 --> 01:05:50,560
And so to just start by touching on that,

1326
01:05:50,560 --> 01:05:52,240
our programming is faulty.

1327
01:05:52,240 --> 01:05:54,480
So our programming is not faulty

1328
01:05:54,480 --> 01:05:56,400
in the sense that we evolved

1329
01:05:57,040 --> 01:05:59,680
for a particular set of conditions, right?

1330
01:05:59,680 --> 01:06:01,360
And that evolution may not be complete

1331
01:06:01,360 --> 01:06:02,640
or optimal, et cetera, et cetera.

1332
01:06:02,640 --> 01:06:05,840
But roughly speaking, we are not faulty in that sense.

1333
01:06:05,840 --> 01:06:06,400
The reason...

1334
01:06:06,400 --> 01:06:08,400
Because evolution is doing its job, right?

1335
01:06:08,400 --> 01:06:11,440
We have all those impulses for a reason, right?

1336
01:06:11,440 --> 01:06:16,240
Now, the problem is that we, unlike any other species,

1337
01:06:16,240 --> 01:06:19,120
we actually have actually succeeded in creating a world

1338
01:06:20,240 --> 01:06:21,440
that is better for us.

1339
01:06:22,160 --> 01:06:24,320
But at the same time, and this is the problem,

1340
01:06:24,320 --> 01:06:26,640
we're actually now adapted to a different world

1341
01:06:26,640 --> 01:06:27,760
from the one that we live in.

1342
01:06:28,560 --> 01:06:30,480
So the faulty program just comes from the fact

1343
01:06:30,480 --> 01:06:32,640
that we evolved for one set of conditions.

1344
01:06:32,640 --> 01:06:34,800
For example, among many other examples

1345
01:06:34,800 --> 01:06:36,960
where your time horizon was very short,

1346
01:06:36,960 --> 01:06:38,960
and now we live in a very different world.

1347
01:06:38,960 --> 01:06:41,360
And so our job as rational people,

1348
01:06:41,360 --> 01:06:42,960
that's what our rational minds are for,

1349
01:06:42,960 --> 01:06:45,520
among other things, is to now adapt ourselves

1350
01:06:45,520 --> 01:06:47,360
to the world that we really are in

1351
01:06:47,360 --> 01:06:48,880
so that we do things that are rational

1352
01:06:48,880 --> 01:06:50,000
in the world that we're in, right?

1353
01:06:50,000 --> 01:06:53,120
So now, the fact that our programming is faulty

1354
01:06:53,120 --> 01:06:55,840
does not see anything about what are the faults

1355
01:06:55,840 --> 01:06:56,800
and how you fix them.

1356
01:06:56,800 --> 01:06:58,240
And what these people have, I think,

1357
01:06:58,240 --> 01:07:01,200
is first of all, the wrong notion of what our faults are

1358
01:07:01,200 --> 01:07:02,240
and then on top of that,

1359
01:07:02,240 --> 01:07:03,760
the wrong notion of how to fix them.

1360
01:07:04,800 --> 01:07:08,240
Okay, now, I want to get into the utility function again.

1361
01:07:08,720 --> 01:07:10,880
Again, one of the things that makes me skeptical

1362
01:07:10,880 --> 01:07:14,320
is this notion of immutability, both of what we're doing

1363
01:07:14,320 --> 01:07:16,560
and in the case of what we've been speaking about

1364
01:07:16,560 --> 01:07:19,680
with utilitarianism, what the utility function is.

1365
01:07:19,680 --> 01:07:21,680
Now, you were kind of hinting to something interesting before,

1366
01:07:21,680 --> 01:07:23,280
which is that it might be diverse

1367
01:07:23,280 --> 01:07:25,360
and it might also be self-updating.

1368
01:07:25,360 --> 01:07:27,840
But I'm constantly asking myself the question,

1369
01:07:27,840 --> 01:07:29,520
how does that work and who gets to say?

1370
01:07:30,560 --> 01:07:33,360
Well, so very much, I think it's complex

1371
01:07:33,360 --> 01:07:34,800
and it should be self-updating, right?

1372
01:07:34,800 --> 01:07:36,400
We're never going to final itself.

1373
01:07:37,360 --> 01:07:40,640
If you buy this notion that the ultimate arbiter is evolution,

1374
01:07:41,200 --> 01:07:44,320
then utility functions are subject to evolution.

1375
01:07:45,680 --> 01:07:47,360
Right, so you think about it

1376
01:07:47,360 --> 01:07:48,560
or you can't think about it.

1377
01:07:48,560 --> 01:07:50,880
It's a wrong world.

1378
01:07:50,880 --> 01:07:52,080
You can't think about this

1379
01:07:52,080 --> 01:07:54,320
and it's useful to think about this in the following ways.

1380
01:07:54,960 --> 01:07:56,240
To a first approximation,

1381
01:07:56,240 --> 01:07:59,360
the number one entity that's evolving is utility functions.

1382
01:07:59,360 --> 01:08:01,040
What you have in the world at any point

1383
01:08:01,040 --> 01:08:03,840
is a population of utility functions, right?

1384
01:08:03,920 --> 01:08:06,320
And now they combine, they evolve,

1385
01:08:06,320 --> 01:08:08,080
you have next generation of utility functions.

1386
01:08:08,080 --> 01:08:12,560
And then there's also how the utility function gets optimized.

1387
01:08:12,560 --> 01:08:14,880
That is also subject to evolution, right?

1388
01:08:14,880 --> 01:08:17,440
And now how the utility function is optimized

1389
01:08:17,440 --> 01:08:18,560
changes a lot faster

1390
01:08:18,560 --> 01:08:21,200
and this is a lot more complex than the utility function itself,

1391
01:08:21,200 --> 01:08:22,800
which is the point, right?

1392
01:08:22,800 --> 01:08:24,480
So at a certain time horizon,

1393
01:08:24,480 --> 01:08:26,880
it's reasonable to approximate utilities as being fixed.

1394
01:08:26,880 --> 01:08:30,240
Like for example, the utilities that are encoded in your brain

1395
01:08:30,240 --> 01:08:33,360
are fixed by your genes, right?

1396
01:08:33,360 --> 01:08:35,760
So in the context of our present human moment

1397
01:08:35,760 --> 01:08:37,360
and effective ultramism or not,

1398
01:08:37,360 --> 01:08:39,760
it makes perfect sense to think of utilities fixed.

1399
01:08:39,760 --> 01:08:42,240
But it is evolving and not just on

1400
01:08:42,240 --> 01:08:45,520
eon time scales, but by the generation, right?

1401
01:08:45,520 --> 01:08:46,960
Things evolve by the generation.

1402
01:08:46,960 --> 01:08:48,960
Okay, but it's still relatively glacial

1403
01:08:48,960 --> 01:08:52,000
and I take your point that there's a kind of divergence

1404
01:08:52,000 --> 01:08:52,960
between the world we live in

1405
01:08:52,960 --> 01:08:54,480
and the programming that we've got.

1406
01:08:54,480 --> 01:08:58,240
But then, okay, let's imagine that we create a new population

1407
01:08:58,240 --> 01:09:00,960
and I guess what I'm saying is that

1408
01:09:01,040 --> 01:09:04,800
you think that the utility function should emerge and evolve,

1409
01:09:04,800 --> 01:09:08,560
but I would argue for some kind of morphogenetic engineering

1410
01:09:08,560 --> 01:09:11,280
where it's a kind of hybrid between something which is emergent

1411
01:09:11,280 --> 01:09:13,120
but something which we can nudge.

1412
01:09:13,120 --> 01:09:15,920
Oh, I mean, I'm glad you brought that up.

1413
01:09:16,800 --> 01:09:18,400
Nudging is a form of emergence.

1414
01:09:19,120 --> 01:09:20,400
You yourself are emergent

1415
01:09:20,400 --> 01:09:22,160
and the things that you do are emergent as well.

1416
01:09:22,160 --> 01:09:23,520
Everything is emergent, right?

1417
01:09:23,520 --> 01:09:25,040
Utilities are emergent.

1418
01:09:25,040 --> 01:09:26,800
Maybe the laws of physics aren't emergent.

1419
01:09:26,800 --> 01:09:28,320
Some people will say even those are, right?

1420
01:09:28,320 --> 01:09:30,640
Like, you know, we live in a universe with this constant

1421
01:09:30,720 --> 01:09:31,760
because blah, blah, blah, right?

1422
01:09:31,760 --> 01:09:33,840
So, but to first approximation,

1423
01:09:33,840 --> 01:09:36,640
every single thing that we've been talking about is emergent.

1424
01:09:36,640 --> 01:09:40,400
We make a distinction between emergent and designed

1425
01:09:40,400 --> 01:09:42,720
because that is anthropomorphic, right?

1426
01:09:42,720 --> 01:09:44,880
Is this things that we do are not emergent?

1427
01:09:44,880 --> 01:09:46,480
Actually, no, when you nudge something

1428
01:09:46,480 --> 01:09:48,160
that is an emergent behavior, right?

1429
01:09:48,160 --> 01:09:49,920
We are emergent as well, right?

1430
01:09:49,920 --> 01:09:51,520
So everything that is human, you know,

1431
01:09:51,520 --> 01:09:53,840
so here's a very good way, I think,

1432
01:09:53,840 --> 01:09:54,960
to think about a lot of things

1433
01:09:54,960 --> 01:09:57,360
which I first saw, you know, in Richard Dawkins,

1434
01:09:57,360 --> 01:09:59,600
which is although he really didn't go into this

1435
01:09:59,600 --> 01:10:01,360
and I wish he had like this notion

1436
01:10:01,360 --> 01:10:03,440
of the extended phenotype, right?

1437
01:10:04,080 --> 01:10:06,960
Technology is our extended phenotype.

1438
01:10:06,960 --> 01:10:08,800
So all these things that we do, right?

1439
01:10:08,800 --> 01:10:09,840
All these things that we build,

1440
01:10:09,840 --> 01:10:11,200
including AS and whatnot,

1441
01:10:11,200 --> 01:10:13,440
they are extensions of our phenotype.

1442
01:10:13,440 --> 01:10:15,840
So if you take the long view, all of, you know,

1443
01:10:15,840 --> 01:10:19,360
technology is the continuation of biology by another means.

1444
01:10:19,920 --> 01:10:21,120
So when you make this distinction

1445
01:10:21,120 --> 01:10:22,640
between emergent and not emergent

1446
01:10:22,640 --> 01:10:25,200
and top down and bottom up, it's all emergent.

1447
01:10:25,200 --> 01:10:27,600
Interesting. Well, we recently did a show on emergence

1448
01:10:27,600 --> 01:10:29,840
and it's a topic of interest to me personally

1449
01:10:29,840 --> 01:10:32,400
and there's weak emergence and strong emergence

1450
01:10:32,400 --> 01:10:34,240
and there's, you know, like the view of weak emergence,

1451
01:10:34,240 --> 01:10:37,360
so there's some, you know, surprising macroscopic phenomena,

1452
01:10:37,360 --> 01:10:39,200
maybe something which transiently emerges

1453
01:10:39,200 --> 01:10:41,200
and Wolfram would add in the whole, you know,

1454
01:10:41,200 --> 01:10:43,200
computational irreducibility angle.

1455
01:10:43,200 --> 01:10:44,560
And then with the strong emergence,

1456
01:10:44,560 --> 01:10:45,840
Chalmers would say it's something

1457
01:10:45,840 --> 01:10:47,600
which is paradigmatically surprising.

1458
01:10:47,600 --> 01:10:49,360
It's something which is not deducible

1459
01:10:49,360 --> 01:10:51,440
for many fundamental truths in the lower level domain.

1460
01:10:51,440 --> 01:10:54,720
But I just wondered, like, how do you think about emergence?

1461
01:10:54,720 --> 01:10:56,880
Well, I think that is a very, the distinction

1462
01:10:56,880 --> 01:10:59,520
between weak and strong immersion is a very useful one.

1463
01:10:59,520 --> 01:11:00,320
Right.

1464
01:11:00,320 --> 01:11:03,840
And I would actually phrase it in slightly different terms,

1465
01:11:03,840 --> 01:11:06,320
which is starting from physics, right?

1466
01:11:07,280 --> 01:11:14,000
I think most physicists and scientists believe in weak emergence.

1467
01:11:14,000 --> 01:11:16,160
Well, could I, could I add that Sabine Hossenfelder

1468
01:11:16,160 --> 01:11:18,880
had a paper and she frames it with this idea

1469
01:11:18,880 --> 01:11:21,440
of the resolution of physical theories.

1470
01:11:21,440 --> 01:11:23,360
So like, like a lower resolution theory

1471
01:11:23,360 --> 01:11:25,760
as weakly emergent from a high resolution theory.

1472
01:11:25,760 --> 01:11:26,400
Well, exactly.

1473
01:11:26,400 --> 01:11:28,000
And, you know, like, I like Sabine,

1474
01:11:28,000 --> 01:11:29,680
but this is not her idea, right?

1475
01:11:29,680 --> 01:11:31,760
This far predates all of us here, right?

1476
01:11:31,760 --> 01:11:32,320
Yeah.

1477
01:11:32,320 --> 01:11:34,560
And again, it's, it's a very interesting history

1478
01:11:34,560 --> 01:11:36,000
and a very important concept.

1479
01:11:36,000 --> 01:11:40,960
Now, so my point was that I think few people have a quarrel

1480
01:11:40,960 --> 01:11:42,400
with the notion of weak emergence

1481
01:11:42,400 --> 01:11:43,920
in the sense that, you know,

1482
01:11:43,920 --> 01:11:45,360
I can give you a theory of everything

1483
01:11:45,360 --> 01:11:47,040
in the form of whatever string theory

1484
01:11:47,040 --> 01:11:48,640
let's take a candidate, right?

1485
01:11:48,640 --> 01:11:50,880
But no string theory claims that that's a theory

1486
01:11:50,880 --> 01:11:52,800
of everything in the sense that like now,

1487
01:11:52,800 --> 01:11:54,960
to study biology or psychology or sociology,

1488
01:11:54,960 --> 01:11:56,080
you should just study string theory.

1489
01:11:56,080 --> 01:11:57,600
No one believes that, right?

1490
01:11:57,600 --> 01:11:59,440
There's actually interesting things to be said there,

1491
01:11:59,440 --> 01:12:01,120
but, but let's not, let's look at,

1492
01:12:01,120 --> 01:12:02,880
let's not go there for a second, right?

1493
01:12:02,880 --> 01:12:06,560
There are these levels that emerge weakly

1494
01:12:06,560 --> 01:12:09,760
in the sense that they are determined by the lower levels.

1495
01:12:10,400 --> 01:12:11,840
They're just so much more complex

1496
01:12:11,840 --> 01:12:14,000
that you're better off focusing on the menu.

1497
01:12:14,000 --> 01:12:15,520
Now there's this other notion which to me

1498
01:12:15,520 --> 01:12:17,360
is the really interesting one,

1499
01:12:17,360 --> 01:12:20,480
which is that there is, there are phenomena

1500
01:12:20,480 --> 01:12:22,000
that are at the higher levels

1501
01:12:22,000 --> 01:12:26,160
that are just not reducible to the lower levels, right?

1502
01:12:26,160 --> 01:12:27,840
So the true emergent is in some sense

1503
01:12:27,840 --> 01:12:29,440
is someone who believes the latter.

1504
01:12:29,440 --> 01:12:30,640
And now you can ask the question,

1505
01:12:30,640 --> 01:12:33,040
like do you believe in that or not, right?

1506
01:12:33,040 --> 01:12:37,360
And I think to give the very short answer first

1507
01:12:37,360 --> 01:12:39,920
is that ultimately there's probably no way of knowing.

1508
01:12:41,120 --> 01:12:44,960
But pragmatically, you're actually probably better off

1509
01:12:44,960 --> 01:12:47,600
treating the world as if it has strong emergence.

1510
01:12:47,600 --> 01:12:49,280
And now strong emergence is actually

1511
01:12:49,280 --> 01:12:50,960
a very strong segment to make is to say,

1512
01:12:50,960 --> 01:12:53,200
and by the way, going down to the lowest levels

1513
01:12:53,200 --> 01:12:54,560
to make things very clear,

1514
01:12:54,560 --> 01:12:56,640
you don't need to think about biology or society

1515
01:12:56,640 --> 01:12:58,480
or consciousness or anything.

1516
01:12:58,480 --> 01:13:00,720
Condensed metaphysics, right?

1517
01:13:01,280 --> 01:13:03,360
The particle physicists tend to believe

1518
01:13:03,360 --> 01:13:06,400
that what they do is what everything reduces to.

1519
01:13:06,400 --> 01:13:08,800
You talk to the condensed metaphysicists.

1520
01:13:08,800 --> 01:13:10,240
This was actually an interesting discussion

1521
01:13:10,240 --> 01:13:12,000
that I had with Scott, you know, Aronson,

1522
01:13:12,000 --> 01:13:13,520
because like he was very much on the,

1523
01:13:13,520 --> 01:13:15,040
we're both computer scientists,

1524
01:13:15,040 --> 01:13:17,120
but he was very much on the side of the particle physicists,

1525
01:13:17,120 --> 01:13:20,080
I don't know very much on the side of the condensed metaphysicists.

1526
01:13:20,080 --> 01:13:22,560
What they will tell you over and over again,

1527
01:13:22,560 --> 01:13:26,080
they see is things that you cannot explain

1528
01:13:26,080 --> 01:13:27,680
using quantum mechanics.

1529
01:13:27,680 --> 01:13:29,200
And now people say like,

1530
01:13:29,200 --> 01:13:31,440
oh, but you can always explain things in quantum mechanics.

1531
01:13:31,440 --> 01:13:33,120
You just haven't done the calculations.

1532
01:13:33,120 --> 01:13:36,320
But the point is precisely that you can't do the calculations, right?

1533
01:13:36,320 --> 01:13:37,840
The calculations are chaotic.

1534
01:13:38,640 --> 01:13:41,280
I have a theory, I can come up with 500 theories

1535
01:13:41,280 --> 01:13:43,840
of these phenomena and semiconductors and whatnot.

1536
01:13:43,840 --> 01:13:45,840
And like, I never actually get to test them

1537
01:13:45,840 --> 01:13:48,560
because the computations diverge before I get to test them.

1538
01:13:48,560 --> 01:13:52,640
So for all intents and purposes, it is strong emergence.

1539
01:13:52,640 --> 01:13:55,520
Whether truly that came from below is unanswerable

1540
01:13:55,520 --> 01:13:57,280
because you can't compute the predictions.

1541
01:13:57,280 --> 01:13:58,320
Well, we spoke about that.

1542
01:13:58,320 --> 01:14:01,120
So I think Keith would call that semi-strong emergence,

1543
01:14:01,120 --> 01:14:03,360
which is like, you know, whether it's computationally reachable

1544
01:14:03,360 --> 01:14:06,400
from the lower resolution to the high resolution

1545
01:14:06,400 --> 01:14:07,760
to the lower resolution.

1546
01:14:07,760 --> 01:14:11,120
But no, Sabine in her paper, A Case for Strong Emergence,

1547
01:14:11,120 --> 01:14:12,640
she was talking about singularities

1548
01:14:12,640 --> 01:14:16,080
as being a really good example of what might be strong emergence.

1549
01:14:16,080 --> 01:14:19,440
And the philosopher Mark Badau, I think, said that

1550
01:14:19,440 --> 01:14:21,360
strong emergence is ridiculous.

1551
01:14:21,360 --> 01:14:23,760
It's basically an affront on physicalism.

1552
01:14:24,640 --> 01:14:27,600
Well, certainly, you know, strong emergence and physicalism,

1553
01:14:27,600 --> 01:14:29,440
or let's just call it reductionism, right?

1554
01:14:29,440 --> 01:14:30,160
Reductionism, yeah.

1555
01:14:30,160 --> 01:14:33,040
Strong emergence and reductionism are incompatible.

1556
01:14:33,040 --> 01:14:33,680
Yeah.

1557
01:14:33,680 --> 01:14:37,520
And we scientists tend to be reductionists, right?

1558
01:14:37,520 --> 01:14:41,040
Now, at some level, I'm both a reductionist

1559
01:14:41,040 --> 01:14:43,280
and someone who is willing to believe in strong emergence.

1560
01:14:43,280 --> 01:14:45,840
Again, I don't believe in strong emergence.

1561
01:14:45,840 --> 01:14:49,440
I just don't see a way to disprove it, right?

1562
01:14:49,440 --> 01:14:52,320
And like, you know, if there's an empirical way

1563
01:14:52,320 --> 01:14:54,800
to distinguish semi-strong from strong emergence,

1564
01:14:54,800 --> 01:14:56,800
I'd be very interested to know what it is.

1565
01:14:56,800 --> 01:14:59,040
But now, I think the thing that is very important

1566
01:14:59,040 --> 01:15:01,440
that a lot of people, including a lot of physicists

1567
01:15:01,440 --> 01:15:05,760
and scientists don't see is that we have this hypothesis

1568
01:15:05,760 --> 01:15:08,080
that everything can be reduced to the laws of physics

1569
01:15:08,080 --> 01:15:08,960
as we know it.

1570
01:15:08,960 --> 01:15:12,080
We should not forget that it's just a hypothesis.

1571
01:15:12,080 --> 01:15:13,760
And it's a hypothesis that, again,

1572
01:15:13,840 --> 01:15:15,600
counter to a lot of people's say,

1573
01:15:15,600 --> 01:15:18,720
is very, very, very, very far from established.

1574
01:15:18,720 --> 01:15:20,800
And usually, people say like, oh, but, you know,

1575
01:15:20,800 --> 01:15:24,000
look at all the successes of the laws of physics and blah, blah.

1576
01:15:24,000 --> 01:15:24,960
And then I say, like, you know,

1577
01:15:24,960 --> 01:15:26,560
putting on my machine learning hat,

1578
01:15:26,560 --> 01:15:30,000
the sample that you've used to validate the laws of physics

1579
01:15:30,000 --> 01:15:36,160
is extraordinarily biased in the direction of simple systems.

1580
01:15:36,160 --> 01:15:39,360
OK, so you can't make this claim of if the data was IID,

1581
01:15:39,360 --> 01:15:41,320
I could say with great confidence,

1582
01:15:41,320 --> 01:15:42,800
these laws apply universally.

1583
01:15:42,800 --> 01:15:43,920
But I haven't done it.

1584
01:15:43,920 --> 01:15:47,280
It's more like I've just landed in a new continent

1585
01:15:47,280 --> 01:15:49,200
and I've sealed up all the rivers.

1586
01:15:49,200 --> 01:15:51,760
And I say, I know what this continent looks like.

1587
01:15:51,760 --> 01:15:53,200
You've never climbed the mountains.

1588
01:15:53,200 --> 01:15:54,560
You've never gone in the jungle.

1589
01:15:54,560 --> 01:15:56,640
So like this notion that the laws of physics

1590
01:15:56,640 --> 01:15:58,640
capture everything about daily life,

1591
01:15:58,640 --> 01:16:00,320
we just don't know how exactly.

1592
01:16:00,320 --> 01:16:02,160
Maybe it's true, but it could also

1593
01:16:02,160 --> 01:16:03,600
equally well be completely false.

1594
01:16:04,560 --> 01:16:04,880
Brilliant.

1595
01:16:04,880 --> 01:16:06,880
Well, you gave a bit of a hint to this earlier, actually,

1596
01:16:06,880 --> 01:16:09,200
because you used the word relationism, right?

1597
01:16:09,200 --> 01:16:10,240
Which is basically the...

1598
01:16:10,240 --> 01:16:11,600
Or relationalism.

1599
01:16:11,600 --> 01:16:12,000
Relationalism.

1600
01:16:12,080 --> 01:16:14,000
Maybe it should be shortened to relationalism.

1601
01:16:14,000 --> 01:16:14,800
Relationalism.

1602
01:16:15,680 --> 01:16:18,720
But yeah, I think Rosen is a great advocate of this,

1603
01:16:18,720 --> 01:16:22,240
and he has a whole category theory calculus

1604
01:16:22,240 --> 01:16:24,480
for describing living systems.

1605
01:16:25,040 --> 01:16:26,880
And also we spoke to Bob Koek,

1606
01:16:26,880 --> 01:16:29,760
the quantum physics professor from Cambridge,

1607
01:16:29,760 --> 01:16:32,720
and he was talking about this concept of Cartesian togetherness,

1608
01:16:32,720 --> 01:16:34,400
which is another category or framework.

1609
01:16:34,400 --> 01:16:37,280
But I just wondered, does that inform your view?

1610
01:16:38,160 --> 01:16:41,200
Well, relationalism, at least in one way

1611
01:16:41,200 --> 01:16:44,240
of defining the term, very much informs my view, right?

1612
01:16:44,240 --> 01:16:46,560
And one way to come at this is to say,

1613
01:16:47,200 --> 01:16:49,680
the world is not made of independent entities.

1614
01:16:49,680 --> 01:16:51,040
Actually, let's just start with machine learning,

1615
01:16:51,040 --> 01:16:52,800
which is a very concrete way to look at this.

1616
01:16:53,360 --> 01:16:55,840
A very large part, maybe even the largest part

1617
01:16:55,840 --> 01:16:57,680
of my research in the last 20 years,

1618
01:16:57,680 --> 01:17:01,840
has been to do away with the assumption of IID data, right?

1619
01:17:01,840 --> 01:17:03,840
That the world is made of independent entities,

1620
01:17:03,840 --> 01:17:06,560
in particular, society is made of independent agents,

1621
01:17:06,560 --> 01:17:07,840
et cetera, et cetera, right?

1622
01:17:07,840 --> 01:17:09,600
Now, we make this assumption,

1623
01:17:09,600 --> 01:17:12,400
both as human beings, to some extent,

1624
01:17:12,400 --> 01:17:14,400
and certainly very much so in science,

1625
01:17:14,400 --> 01:17:15,840
because it makes life easier.

1626
01:17:16,720 --> 01:17:19,680
The math is way, way, way easier when you assume independence.

1627
01:17:19,680 --> 01:17:22,880
But it's a blatantly false assumption, right?

1628
01:17:22,880 --> 01:17:24,320
Unfortunately, a lot of, for example,

1629
01:17:24,320 --> 01:17:27,600
economics prominently has embedded in it this notion

1630
01:17:27,600 --> 01:17:29,680
that the world is a bunch of independent agents,

1631
01:17:29,680 --> 01:17:31,520
and it just doesn't work like that.

1632
01:17:31,520 --> 01:17:34,720
And moreover, it's a distinction that is full of consequences.

1633
01:17:34,720 --> 01:17:38,400
A society and economy is a network of agents,

1634
01:17:38,400 --> 01:17:40,960
and almost all the action is in their interactions.

1635
01:17:41,680 --> 01:17:43,760
Until you really start taking that seriously,

1636
01:17:43,760 --> 01:17:45,040
you really don't understand the world.

1637
01:17:45,040 --> 01:17:47,440
Again, I have no quarrel with classic economics

1638
01:17:47,440 --> 01:17:48,640
as a first approximation.

1639
01:17:48,640 --> 01:17:50,160
It's exactly what it should be, right?

1640
01:17:50,720 --> 01:17:51,760
But then, and by the way,

1641
01:17:51,760 --> 01:17:53,440
you should also not just throw it away and say,

1642
01:17:53,440 --> 01:17:55,280
like, oh, this is garbage, like some people say.

1643
01:17:55,280 --> 01:17:56,400
You have to go the next stage.

1644
01:17:56,400 --> 01:17:58,720
It's actually now we have the mathematical

1645
01:17:58,720 --> 01:18:00,320
and computational tools to do,

1646
01:18:00,320 --> 01:18:03,280
and understand it as being a system of interacting agents.

1647
01:18:03,840 --> 01:18:05,840
And all of the questions that we are talking about,

1648
01:18:05,840 --> 01:18:10,080
including in evolution, even in physics, right?

1649
01:18:11,200 --> 01:18:14,720
A piece of condensed matter is a network of interacting,

1650
01:18:14,720 --> 01:18:16,240
spins, et cetera, et cetera, you name it.

1651
01:18:16,240 --> 01:18:18,800
So the relations are at the heart of it,

1652
01:18:18,800 --> 01:18:20,560
and moreover, like as I said,

1653
01:18:20,560 --> 01:18:23,440
a lot of my work is we now have the representations,

1654
01:18:23,440 --> 01:18:25,440
the learning inference algorithms to handle things

1655
01:18:25,440 --> 01:18:27,520
that are big piles of relations,

1656
01:18:27,520 --> 01:18:30,080
and the whole world is better understood in those terms,

1657
01:18:30,080 --> 01:18:32,080
and we just need people to catch up with that.

1658
01:18:32,800 --> 01:18:34,240
You know, once you do that,

1659
01:18:34,240 --> 01:18:36,080
you get into things that can easily be

1660
01:18:36,080 --> 01:18:38,000
computational, intractable, and so on and so forth.

1661
01:18:38,000 --> 01:18:40,160
But there's a lot of things that we can do there

1662
01:18:40,160 --> 01:18:41,440
and a lot more that we'll do.

1663
01:18:41,440 --> 01:18:44,160
So at this level, I think relationalism

1664
01:18:44,160 --> 01:18:46,560
is really should be a cornerstone

1665
01:18:46,560 --> 01:18:48,160
of our understanding of the world

1666
01:18:48,160 --> 01:18:49,920
in a way that it hasn't been in the past.

1667
01:18:50,480 --> 01:18:53,600
Okay, and which existing complexity science brings to mind?

1668
01:18:53,600 --> 01:18:57,440
But I mean, which existing techniques and areas

1669
01:18:57,440 --> 01:19:00,560
can folks look into to take that on board?

1670
01:19:00,640 --> 01:19:02,400
Well, you know, Markov logic,

1671
01:19:02,400 --> 01:19:05,120
which is what I developed for this purpose essentially,

1672
01:19:05,120 --> 01:19:07,280
and I do think, you know,

1673
01:19:07,280 --> 01:19:08,800
this is my talking about my work,

1674
01:19:08,800 --> 01:19:10,320
so you should naturally be suspicious,

1675
01:19:10,320 --> 01:19:12,960
but I think it's the best that we have,

1676
01:19:12,960 --> 01:19:15,120
and I think by a wide measure,

1677
01:19:15,120 --> 01:19:17,120
compared to anything else that we have so far.

1678
01:19:17,120 --> 01:19:18,560
Okay, and can you sketch it out?

1679
01:19:18,560 --> 01:19:22,880
Yeah, so to sketch it out in the simplest terms, right,

1680
01:19:22,880 --> 01:19:26,240
we want to combine all the traditional goodies

1681
01:19:26,240 --> 01:19:28,400
that we have from assuming the world is IID

1682
01:19:28,400 --> 01:19:30,480
with the power to model, you know, relationships,

1683
01:19:30,480 --> 01:19:32,640
there are themselves potentially very complicated.

1684
01:19:32,640 --> 01:19:34,720
The way we do the Markov logic is,

1685
01:19:34,720 --> 01:19:35,840
there's the logical part.

1686
01:19:36,640 --> 01:19:39,840
We actually do not need to solve a new,

1687
01:19:39,840 --> 01:19:41,680
the problem of how to represent

1688
01:19:41,680 --> 01:19:43,120
and do inference with relations.

1689
01:19:43,120 --> 01:19:45,200
We have first order logic for that.

1690
01:19:45,760 --> 01:19:48,240
First order logic is the language of relations.

1691
01:19:48,240 --> 01:19:50,320
That's actually the term that is used, right,

1692
01:19:50,320 --> 01:19:52,080
and how the relations depend predicates.

1693
01:19:52,080 --> 01:19:53,280
Sometimes they're called predicates,

1694
01:19:53,280 --> 01:19:55,440
but let's just call them relations, right?

1695
01:19:55,440 --> 01:19:57,760
We have a formal language to talk about relations.

1696
01:19:57,760 --> 01:20:00,080
And by the way, essentially all of computer science

1697
01:20:00,080 --> 01:20:00,960
can be reduced to that.

1698
01:20:00,960 --> 01:20:03,120
You give me your favorite, you know, whatever,

1699
01:20:03,120 --> 01:20:06,400
knowledge representation, data structure, et cetera, et cetera.

1700
01:20:06,400 --> 01:20:09,680
And I, anybody who knows can immediately say

1701
01:20:09,680 --> 01:20:10,720
how to do that in logic.

1702
01:20:10,720 --> 01:20:11,920
So that's one part.

1703
01:20:11,920 --> 01:20:14,720
The other part is the statistical, you know,

1704
01:20:14,720 --> 01:20:17,280
machine learning probabilistic aspect of the world, right?

1705
01:20:17,280 --> 01:20:20,080
And then again, going all the way back to physics, right?

1706
01:20:20,800 --> 01:20:22,160
All of these things that we deal with

1707
01:20:22,160 --> 01:20:24,160
are essentially special cases of what are

1708
01:20:24,160 --> 01:20:26,000
variously called Markov networks,

1709
01:20:26,000 --> 01:20:28,080
which is where the name Markov comes from.

1710
01:20:28,080 --> 01:20:30,960
Or graphical models, or log linear models,

1711
01:20:30,960 --> 01:20:34,160
Gibbs distributions, Boltzmann machines, right?

1712
01:20:34,160 --> 01:20:36,480
All of these things are essentially the same, right?

1713
01:20:36,480 --> 01:20:39,440
That whole neck of the woods is captured by Markov networks.

1714
01:20:39,440 --> 01:20:40,240
Let's call them that.

1715
01:20:41,200 --> 01:20:43,840
And Markov logic is combining Markov networks

1716
01:20:43,840 --> 01:20:45,920
with first order logic in a single language,

1717
01:20:45,920 --> 01:20:47,840
which you can now do everything with.

1718
01:20:47,840 --> 01:20:48,480
Okay, okay.

1719
01:20:48,480 --> 01:20:50,960
So just to like push back a tiny bit.

1720
01:20:50,960 --> 01:20:53,760
So in the past, we've tried to create,

1721
01:20:54,720 --> 01:20:56,000
let's say things like psych,

1722
01:20:56,000 --> 01:20:58,720
which is a knowledge representation of the world.

1723
01:20:58,720 --> 01:21:01,120
Folks like Montague have tried to do semantics

1724
01:21:01,120 --> 01:21:03,760
using first order logic to set some,

1725
01:21:03,760 --> 01:21:05,760
you know, varying degrees of success.

1726
01:21:05,760 --> 01:21:07,120
And then we have the grounding problem

1727
01:21:07,120 --> 01:21:09,440
we were just talking before about, you know,

1728
01:21:09,440 --> 01:21:10,480
like even Searle said this,

1729
01:21:10,480 --> 01:21:13,840
that you have kind of epistemic objectivity

1730
01:21:13,840 --> 01:21:16,320
and subjectivity and some things are observer relative,

1731
01:21:16,320 --> 01:21:18,560
like even economics is observer relative.

1732
01:21:18,560 --> 01:21:22,080
So with this kind of formalism, how would that work?

1733
01:21:22,080 --> 01:21:23,280
That's so very good.

1734
01:21:23,280 --> 01:21:25,600
The problem with or the main problem

1735
01:21:25,600 --> 01:21:27,520
with a lot of these things that you mentioned,

1736
01:21:27,520 --> 01:21:29,920
like, you know, certain types of semantics and whatnot,

1737
01:21:29,920 --> 01:21:32,240
that are based essentially on first order logic, right?

1738
01:21:32,240 --> 01:21:33,360
Is that they're too brittle.

1739
01:21:34,000 --> 01:21:37,600
In fact, the problem with symbolic AI is that it's too brittle.

1740
01:21:38,160 --> 01:21:41,040
And this is exactly what Markov logic fixes.

1741
01:21:41,040 --> 01:21:43,440
It fixes it by making it statistical.

1742
01:21:43,440 --> 01:21:45,280
When I give you a logical statement now,

1743
01:21:45,280 --> 01:21:47,440
I'm no longer, for example, simple logical statement,

1744
01:21:48,240 --> 01:21:50,960
you know, a smoking causes cancer, right?

1745
01:21:51,680 --> 01:21:53,520
In English, this is a valid statement.

1746
01:21:53,520 --> 01:21:54,880
Smoking does cause cancer.

1747
01:21:54,880 --> 01:21:56,640
But actually, once you translate it to logic

1748
01:21:56,640 --> 01:21:59,280
for every x, smokes of x implies cancer of x,

1749
01:21:59,280 --> 01:22:02,320
it's false because some smokers don't get cancer, right?

1750
01:22:02,320 --> 01:22:04,160
What this really was meant to be all along

1751
01:22:04,160 --> 01:22:06,240
is a statistical statement that says,

1752
01:22:06,240 --> 01:22:08,240
smokers are more likely to get cancer.

1753
01:22:09,120 --> 01:22:11,520
So the way we overcome a lot of those problems

1754
01:22:11,520 --> 01:22:14,560
is precisely that we take all of this logic,

1755
01:22:14,560 --> 01:22:15,920
which again, the language exists,

1756
01:22:15,920 --> 01:22:18,480
we don't have to change it, we can, but we don't have to.

1757
01:22:18,480 --> 01:22:19,920
And we make it statistical.

1758
01:22:20,480 --> 01:22:22,800
As a result of which, it's no longer brittle.

1759
01:22:22,800 --> 01:22:24,240
Or at least now it's only as brittle

1760
01:22:24,240 --> 01:22:26,320
as machine learning and graphical model or not.

1761
01:22:26,320 --> 01:22:29,440
It's not as brittle as, you know, traditional symbolic AI.

1762
01:22:29,440 --> 01:22:31,520
Okay. And so we speak into a lot of Go-Fi people

1763
01:22:31,520 --> 01:22:33,680
and I mean, Wally Subba, for example, he's a rationalist

1764
01:22:33,680 --> 01:22:35,040
and what's interesting about the rationalist

1765
01:22:35,040 --> 01:22:37,920
is they hate any form of uncertainty, right?

1766
01:22:37,920 --> 01:22:39,360
They think in absolute binaries.

1767
01:22:39,360 --> 01:22:40,400
You either know it or you don't.

1768
01:22:40,400 --> 01:22:42,160
No, I mean, let me push back on that.

1769
01:22:42,160 --> 01:22:44,480
There's this, again, you need to distinguish, you know,

1770
01:22:44,480 --> 01:22:47,760
a general field or idea from its subtypes, right?

1771
01:22:47,760 --> 01:22:49,680
There is a type of rationalist.

1772
01:22:49,760 --> 01:22:53,280
That hits uncertainty, big mistake, big, big mistake.

1773
01:22:53,280 --> 01:22:55,200
There's a type of rationalist that, you know,

1774
01:22:55,200 --> 01:22:57,200
uncertainty is what they, you know, like,

1775
01:22:58,720 --> 01:23:01,840
an uncertainty calculus is a type of rationalism.

1776
01:23:01,840 --> 01:23:04,640
And some of the best, you know, AI, philosophy, etc.

1777
01:23:04,640 --> 01:23:05,280
is just that.

1778
01:23:05,280 --> 01:23:07,920
So there is no incompatibility at all

1779
01:23:07,920 --> 01:23:09,520
between rationalism and uncertainty.

1780
01:23:09,520 --> 01:23:13,600
In fact, if rationalism, if being rational is maximizing

1781
01:23:13,600 --> 01:23:17,120
expected utility, notice the expected in there, right?

1782
01:23:17,120 --> 01:23:20,160
You cannot be rational if you ignore the uncertainty.

1783
01:23:20,160 --> 01:23:20,800
Interesting.

1784
01:23:20,800 --> 01:23:23,680
Okay, but then what about the resolution of modeling?

1785
01:23:23,680 --> 01:23:25,680
I mean, smoking is a really good one.

1786
01:23:25,680 --> 01:23:28,320
So us humans, we anthropomorphize things.

1787
01:23:28,320 --> 01:23:30,640
We understand the world in macroscopic terms

1788
01:23:30,640 --> 01:23:33,360
using macroscopic ideas that we understand.

1789
01:23:33,360 --> 01:23:35,760
And that kind of leads to a certain type of modeling.

1790
01:23:35,760 --> 01:23:37,840
And that modeling presumably would be represented

1791
01:23:37,840 --> 01:23:40,320
at that resolution, you know, using this formalism.

1792
01:23:41,360 --> 01:23:42,800
Sure. And what's the question?

1793
01:23:42,800 --> 01:23:46,160
Well, it seemed, again, like I'm intuitively suspicious

1794
01:23:46,160 --> 01:23:48,480
that we were just saying the world is a complex place.

1795
01:23:48,480 --> 01:23:50,960
And with a lot of causal modeling, for example,

1796
01:23:50,960 --> 01:23:53,440
a lot of the art is understanding what is relevant

1797
01:23:53,440 --> 01:23:54,880
and what is not relevant.

1798
01:23:54,880 --> 01:23:58,080
What is relevant might just be kind of, you know, relevant to us.

1799
01:23:58,640 --> 01:24:01,120
No, well, what is relevant is what is relevant

1800
01:24:01,120 --> 01:24:02,800
relative to your utility function.

1801
01:24:03,520 --> 01:24:06,480
Okay, again, it gets back to that precisely.

1802
01:24:07,680 --> 01:24:09,920
The whole problem is that the world is infinitely complex

1803
01:24:09,920 --> 01:24:11,920
and we have only finite computational resources,

1804
01:24:11,920 --> 01:24:14,080
whether it's in our brains or our computers or whatever, right?

1805
01:24:14,080 --> 01:24:15,280
So now what do you do, right?

1806
01:24:15,360 --> 01:24:18,160
You are forced to oversimplify the world,

1807
01:24:18,160 --> 01:24:20,800
not just simplify, but oversimplify, right?

1808
01:24:20,800 --> 01:24:23,200
But now the whole art, that's actually a good word to use,

1809
01:24:23,200 --> 01:24:24,480
even if it's done with computers,

1810
01:24:24,480 --> 01:24:28,960
is how do you not only oversimplify as little as you can,

1811
01:24:29,520 --> 01:24:35,200
but pick out the simplifications that are least harmful to your objective.

1812
01:24:35,200 --> 01:24:37,520
By the way, the art of the physicist,

1813
01:24:37,520 --> 01:24:40,960
physicist would tell you, is precisely doing this, right?

1814
01:24:40,960 --> 01:24:43,840
Physicists are very good at deciding what to simplify.

1815
01:24:43,840 --> 01:24:45,600
And in fact, almost, I think at some level,

1816
01:24:45,600 --> 01:24:47,760
almost any good scientist, this is what they do, right?

1817
01:24:47,760 --> 01:24:51,680
So, and now how do I decide what and how to simplify

1818
01:24:51,680 --> 01:24:54,640
is by relevance to my utility function, right?

1819
01:24:55,280 --> 01:24:57,280
I want to ignore parts of the world

1820
01:24:57,280 --> 01:25:00,800
that do not affect my utility function, number one, right?

1821
01:25:00,800 --> 01:25:03,040
And for example, the notion of conditional independence,

1822
01:25:03,040 --> 01:25:05,440
which is the foundation of graphical models,

1823
01:25:05,440 --> 01:25:06,720
that's what the whole idea is.

1824
01:25:06,720 --> 01:25:08,640
It's like, once I know these things,

1825
01:25:08,640 --> 01:25:10,320
I don't have to know about those others.

1826
01:25:10,320 --> 01:25:12,080
Thank God, right?

1827
01:25:12,080 --> 01:25:14,080
Okay, but if Ken Stanley was here,

1828
01:25:14,080 --> 01:25:16,640
he says that the great thing about evolution is it's divergent,

1829
01:25:16,640 --> 01:25:19,680
it's not convergent, it's discovering new information.

1830
01:25:19,680 --> 01:25:21,600
And my worry is with a system like this,

1831
01:25:21,600 --> 01:25:23,920
with any form of anthropomorphic design,

1832
01:25:23,920 --> 01:25:25,840
would inevitably become convergent.

1833
01:25:25,840 --> 01:25:28,640
And it might look like, oh, those things over there

1834
01:25:28,640 --> 01:25:29,840
that we're ignoring don't matter,

1835
01:25:29,840 --> 01:25:31,600
but actually they might really matter

1836
01:25:31,600 --> 01:25:32,880
if they got introduced into the utility.

1837
01:25:32,880 --> 01:25:38,800
Well, I wouldn't say that maximizing expected utility is anthropomorphic, right?

1838
01:25:38,800 --> 01:25:40,080
In fact, it's one of the least...

1839
01:25:41,040 --> 01:25:43,680
I think maybe there's some degree of anthropomorphism

1840
01:25:43,680 --> 01:25:45,120
is almost anything we do,

1841
01:25:45,120 --> 01:25:49,040
and the progress of science is becoming less and less anthropomorphic,

1842
01:25:49,040 --> 01:25:50,800
and we should keep pushing on that.

1843
01:25:50,800 --> 01:25:53,760
But I would say that maximizing expected utility

1844
01:25:53,760 --> 01:25:56,480
is one of the least anthropomorphic things we can do.

1845
01:25:56,480 --> 01:25:58,240
Well, this is actually a really interesting point,

1846
01:25:58,240 --> 01:26:00,720
because one of the key tenets of the rationalist movement

1847
01:26:00,720 --> 01:26:02,000
and their conception of intelligence,

1848
01:26:02,000 --> 01:26:05,200
because all of the other definitions of intelligence

1849
01:26:05,200 --> 01:26:06,880
are anthropomorphic.

1850
01:26:06,880 --> 01:26:09,600
So, you know, there's based on behavior, capability, AI,

1851
01:26:10,320 --> 01:26:15,280
principal function is a big one, you know, from Norvig.

1852
01:26:15,280 --> 01:26:18,160
And this is the principal based AI,

1853
01:26:18,160 --> 01:26:21,280
which is just making rational moves.

1854
01:26:21,280 --> 01:26:26,160
So why is there such a push to be as, you know,

1855
01:26:26,160 --> 01:26:29,840
to be as the least amount anthropomorphic?

1856
01:26:30,480 --> 01:26:34,000
Oh, the push is not to be, at least in my view,

1857
01:26:34,480 --> 01:26:36,960
being less anthropomorphic is not a goal.

1858
01:26:37,600 --> 01:26:39,040
That's not the goal.

1859
01:26:39,040 --> 01:26:42,720
The goal is to be as accurate and complete as we can

1860
01:26:42,720 --> 01:26:44,880
in modeling the world, right?

1861
01:26:44,880 --> 01:26:47,680
We're just trying to understand the world better, right?

1862
01:26:48,320 --> 01:26:50,400
For whatever purpose, maybe for its own sake,

1863
01:26:50,400 --> 01:26:54,080
maybe for the purpose of the utility and the evolution and so on, right?

1864
01:26:54,080 --> 01:26:54,960
But that's the goal.

1865
01:26:54,960 --> 01:26:57,040
The problem is that,

1866
01:26:57,040 --> 01:26:59,600
and this has been the problem since they won, right?

1867
01:26:59,600 --> 01:27:01,200
They won of humanity,

1868
01:27:01,200 --> 01:27:03,840
is that because we anthropomorphize the world,

1869
01:27:03,840 --> 01:27:07,360
that gets in the way of understanding how it really works, right?

1870
01:27:07,360 --> 01:27:10,560
If I say the wind is some God blowing, right?

1871
01:27:10,560 --> 01:27:11,600
I understand, right?

1872
01:27:11,600 --> 01:27:13,040
That's all they could think of.

1873
01:27:13,040 --> 01:27:15,600
But it's a big obstacle to understanding what the wind really is,

1874
01:27:15,600 --> 01:27:18,560
like there's a pressure difference, et cetera, et cetera, right?

1875
01:27:18,560 --> 01:27:21,440
And we've done away with a lot of anthropomorphism.

1876
01:27:21,440 --> 01:27:23,200
By the way, one of the problems that we're always having

1877
01:27:23,200 --> 01:27:25,760
is that it's always pushing back, right?

1878
01:27:25,760 --> 01:27:27,120
You know, there's always, you know, again,

1879
01:27:27,120 --> 01:27:30,560
intuitively we have a very strong tendency to anthropomorphism,

1880
01:27:31,040 --> 01:27:33,600
as much as science broadly construed as a great victory,

1881
01:27:33,600 --> 01:27:35,360
it's always in danger from this, right?

1882
01:27:35,360 --> 01:27:37,200
But even within science,

1883
01:27:37,200 --> 01:27:42,160
we've gone from doing away with the obvious forms of anthropomorphism

1884
01:27:42,160 --> 01:27:43,600
and anthropomorphism

1885
01:27:43,600 --> 01:27:46,400
to having many things that are still there

1886
01:27:46,400 --> 01:27:50,480
that are less obviously anthropomorphic, but still are, right?

1887
01:27:50,480 --> 01:27:53,120
But if there's something anthropomorphic that's actually is accurate,

1888
01:27:53,120 --> 01:27:54,480
then more power to it.

1889
01:27:54,480 --> 01:27:55,120
Interesting, yeah.

1890
01:27:55,120 --> 01:27:57,920
And then I guess we have so many cognitive priors, right?

1891
01:27:58,160 --> 01:28:00,480
In our brains that give us a cone of attention,

1892
01:28:00,480 --> 01:28:03,280
which is completely anthropocentric.

1893
01:28:03,280 --> 01:28:05,120
Well, very good.

1894
01:28:05,120 --> 01:28:09,040
So those priors, and maybe a better term is heuristics, right?

1895
01:28:09,040 --> 01:28:13,040
Our brains are full of heuristics that evolution put there

1896
01:28:13,040 --> 01:28:16,240
for a good reason, because those heuristics work, right?

1897
01:28:16,240 --> 01:28:17,840
But they are heuristics.

1898
01:28:17,840 --> 01:28:19,760
So they have failure modes, right?

1899
01:28:19,760 --> 01:28:24,160
And we need to understand what is that those heuristics really are getting at

1900
01:28:24,160 --> 01:28:27,760
so that we also, so that we use them when they're good.

1901
01:28:27,760 --> 01:28:29,760
But then when they're not good, we use something else.

1902
01:28:30,640 --> 01:28:31,920
Brilliant, brilliant.

1903
01:28:31,920 --> 01:28:34,000
So Pedro, we're here at NeurIPS this week.

1904
01:28:34,000 --> 01:28:38,080
And could you just like sketch out some of the some of the things you've seen?

1905
01:28:38,080 --> 01:28:42,640
And I also know that you're a huge fan in that there's a Neurosymbolic algorithm

1906
01:28:42,640 --> 01:28:43,680
that you want to tell us about.

1907
01:28:43,680 --> 01:28:45,360
So let's let's hear it.

1908
01:28:45,360 --> 01:28:49,440
So I indeed, I've been enjoying NeurIPS this week.

1909
01:28:49,440 --> 01:28:54,640
One of the big things in AI in the last several years has been Neurosymbolic AI,

1910
01:28:55,600 --> 01:29:00,240
which you probably will not surprise by the fact that I very much believe in.

1911
01:29:00,240 --> 01:29:02,960
So and I believe this since I was a grad student

1912
01:29:02,960 --> 01:29:06,640
and the whole idea of Neurosymbolic AI was something that nobody was interested in, right?

1913
01:29:06,640 --> 01:29:09,360
And now suddenly everybody is, which I think is a good development.

1914
01:29:09,360 --> 01:29:13,280
And this is the idea that if we really want to solve AI by some definite,

1915
01:29:13,280 --> 01:29:18,800
if we want to get to like human level intelligence, etc, etc, we need to have both,

1916
01:29:20,320 --> 01:29:24,160
you know, like, for example, deep learning is not enough, right?

1917
01:29:24,160 --> 01:29:27,920
There are symbolic reasoning capabilities that we have and that are essential.

1918
01:29:28,640 --> 01:29:29,840
And we need to get them.

1919
01:29:29,840 --> 01:29:33,280
And I think, you know, intelligent connection is like, I don't know,

1920
01:29:33,280 --> 01:29:36,960
Yoshua Ben-Joe, you know, Yanle Kunase, they don't disagree with this.

1921
01:29:37,520 --> 01:29:41,120
But one way to look at this is say, we're just going to realize those,

1922
01:29:41,120 --> 01:29:44,880
you know, capabilities using purely connectionist means, right?

1923
01:29:45,520 --> 01:29:48,160
And what I see happening in that direction,

1924
01:29:48,160 --> 01:29:50,480
unfortunately, is a lot of reinventing the wheel.

1925
01:29:50,480 --> 01:29:55,680
So I do think, you know, symbolic AI got wedged for some reasons, including brittleness.

1926
01:29:56,800 --> 01:29:59,920
And, you know, and we have learned from that at the same time,

1927
01:29:59,920 --> 01:30:04,320
they did discover and understand a lot of things that are extremely relevant.

1928
01:30:04,320 --> 01:30:06,880
So it's just not good science to ignore it.

1929
01:30:06,880 --> 01:30:12,960
So I'm working on an approach to combine, you know, symbolic AI with deep learning.

1930
01:30:12,960 --> 01:30:17,440
Again, this is a popular exercise, there are many interesting approaches out there.

1931
01:30:17,520 --> 01:30:18,960
As much as I sympathize with them,

1932
01:30:18,960 --> 01:30:20,880
I think they're all very far from solving the problem.

1933
01:30:20,880 --> 01:30:24,080
They are over complicated and not powerful enough.

1934
01:30:24,080 --> 01:30:26,960
So, you know, I've been working on an approach called TensorFlow logic

1935
01:30:26,960 --> 01:30:33,200
that I do believe is as simple as it can be and as general as it can or needs to be.

1936
01:30:33,760 --> 01:30:38,240
And this, you know, it really is a deep unification of the two things

1937
01:30:38,240 --> 01:30:42,320
in the sense that it's not just that you combine them using, you know,

1938
01:30:42,320 --> 01:30:45,040
a neural model that causes symbolic one or vice versa,

1939
01:30:45,040 --> 01:30:47,520
which is a lot of what these things that you have today do.

1940
01:30:47,520 --> 01:30:51,680
And a lot of the claims that like, oh, this system is neuro symbolic, which it is.

1941
01:30:51,680 --> 01:30:55,600
It's like, you know, AlphaGo is neuro symbolic because some of what it does is symbolic.

1942
01:30:55,600 --> 01:31:00,960
But I'm talking about something much deeper, which is once you start doing AI,

1943
01:31:00,960 --> 01:31:03,440
learning inference representation in TensorFlow logic,

1944
01:31:03,440 --> 01:31:08,720
there's just no distinction between symbolic and neural at all anymore.

1945
01:31:09,840 --> 01:31:10,960
Can you explain that?

1946
01:31:10,960 --> 01:31:16,000
So TensorFlow logic, I'm just inferring from that that the primary representation

1947
01:31:16,000 --> 01:31:18,640
of substrate is a continuous vector space. Is that right?

1948
01:31:18,640 --> 01:31:21,200
Are you encoding discrete information into the vector space?

1949
01:31:21,200 --> 01:31:23,280
So it's a vector space. Yeah.

1950
01:31:23,280 --> 01:31:28,080
Right. In fact, this was the original term that we had for this was vector space logic.

1951
01:31:28,080 --> 01:31:30,880
But then we changed it to TensorFlow logic because it's much more appropriate.

1952
01:31:30,880 --> 01:31:34,960
But it's it's vector space in the abstract algebra sense of vector space,

1953
01:31:34,960 --> 01:31:37,200
not in the traditional, you know, vectors of numbers.

1954
01:31:37,760 --> 01:31:46,000
But anyway, so as the name implies, right, TensorFlow logic is a combination or unification

1955
01:31:46,000 --> 01:31:50,800
of tensor algebra on the one hand and logic programming on the other.

1956
01:31:50,800 --> 01:31:55,200
So is it similar because Bob Koeck had a similar idea using like tensor outer products?

1957
01:31:56,000 --> 01:31:57,040
Is it that kind of?

1958
01:31:57,040 --> 01:31:59,520
It's related, but I think it goes well beyond.

1959
01:31:59,520 --> 01:32:00,160
Okay.

1960
01:32:00,160 --> 01:32:03,120
And the basic idea is actually pretty simple.

1961
01:32:03,120 --> 01:32:08,160
And it's just the following, right, without going into too much, you know, technical detail.

1962
01:32:09,200 --> 01:32:12,320
All of deep learning can be done using tensor algebra.

1963
01:32:12,320 --> 01:32:16,320
Yeah, you know, plus univariate nonlinearities.

1964
01:32:16,320 --> 01:32:18,880
Right. So we've got the tensor algebra to do that.

1965
01:32:18,880 --> 01:32:21,680
All of symbolic AI can be done using logic programming.

1966
01:32:21,680 --> 01:32:23,920
And moreover, it has been done using logic program.

1967
01:32:23,920 --> 01:32:27,120
So if you can unify these two things, this part of the job is done.

1968
01:32:27,120 --> 01:32:32,720
Right. And as it turns out, you can unify them shockingly easily because a tensor

1969
01:32:33,200 --> 01:32:39,120
so tensor algebra is operating on tensors, you know, in that logic, so logic programming,

1970
01:32:39,120 --> 01:32:43,840
and then for learning in that logic program and symbolic AI, they are all operating on relations.

1971
01:32:43,840 --> 01:32:44,160
Yeah.

1972
01:32:44,160 --> 01:32:47,680
Right. So what is the relationship between the tensor and the relation?

1973
01:32:47,680 --> 01:32:52,560
Right. A relation is just this and efficiently represented sparse Boolean tensor.

1974
01:32:53,520 --> 01:32:57,200
So at this point, we actually know that the foundation of these two things is actually the

1975
01:32:57,200 --> 01:33:02,480
same. If your tensor is Boolean and is very sparse, now I'm better off representing it with

1976
01:33:02,480 --> 01:33:06,000
a relation, but at a certain level of abstraction, nothing has changed.

1977
01:33:06,000 --> 01:33:11,280
Right. So by this prism, you can look at logic programming and logic programming is doing tensor

1978
01:33:11,280 --> 01:33:11,760
algebra.

1979
01:33:12,720 --> 01:33:18,000
Okay. Just help me understand this a little bit. So, you know, the main criticism of using a neural

1980
01:33:18,000 --> 01:33:24,480
network as a combined computational and memory substrate is that it's a finite state automator.

1981
01:33:24,480 --> 01:33:29,200
So without having the augmented memory like a Turing machine, you can't represent infinite

1982
01:33:29,200 --> 01:33:32,560
objects. That's the main reason the symbol is, you know, that's the main argument I used.

1983
01:33:32,560 --> 01:33:35,520
So wouldn't that argument still be leveled against you?

1984
01:33:35,520 --> 01:33:40,560
Well, no, because I'm glad you brought that up because there is a very common misconception.

1985
01:33:40,560 --> 01:33:44,800
If you realize that there is no such thing as infinity, right? And in particular, there is no

1986
01:33:44,800 --> 01:33:50,960
such thing as an infinite memory. That problem doesn't arise. So there's the, so the, unfortunately,

1987
01:33:50,960 --> 01:33:56,640
a lot of theorists, including computer theorists, they foster this misconception, right?

1988
01:33:56,640 --> 01:34:01,680
There's the Chomsky hierarchy, right? With finite automata at the bottom and Turing complete,

1989
01:34:01,680 --> 01:34:06,000
you know, Turing machines bubble at the top, right? If your Turing machine has only a finite

1990
01:34:06,000 --> 01:34:11,920
tape, it's a finite automata. So everything is just finite automata. Let's get that out of the way,

1991
01:34:11,920 --> 01:34:16,160
right? A lot of what people do is like completely mistaken because of that. Now,

1992
01:34:16,880 --> 01:34:21,360
the fact that everything is finite automata does not mean that everything is equally good.

1993
01:34:21,360 --> 01:34:27,040
Some representations are far more efficient, compact, etc., etc., for certain purposes than

1994
01:34:27,040 --> 01:34:31,600
others. And the whole game here is that like, I'm not going to solve a finite automata. The question

1995
01:34:31,600 --> 01:34:36,000
is like, what do I need to do? Not because I need to go to a higher level of Chomsky hierarchy,

1996
01:34:36,000 --> 01:34:41,840
because in reality, they don't exist. But because, you know, I mean, if you have infinite resources,

1997
01:34:41,840 --> 01:34:46,000
you could solve a gap with a lookup table. But would you, would you not? I mean, for example,

1998
01:34:46,000 --> 01:34:49,280
there was this DeepMind paper that mapped architectures to different levels of the

1999
01:34:49,280 --> 01:34:55,600
Chomsky hierarchy transformers, I think were, you know, FSAs, RNNs actually were one step higher.

2000
01:34:55,600 --> 01:34:59,200
They could represent regular languages and they got context-free languages. I mean,

2001
01:34:59,200 --> 01:35:02,640
do you think there's any meaningful distinction between those language levels?

2002
01:35:02,640 --> 01:35:07,120
As I said, there is a meaningful distinction, but it's not the distinction that people usually make,

2003
01:35:07,120 --> 01:35:12,160
because once you, I mean, you can debate whether the universe is finite, but certainly computers

2004
01:35:12,160 --> 01:35:16,000
are finite. So as far as anything that you're going to run on a computer, there truly is no

2005
01:35:16,000 --> 01:35:21,120
distinction at this theoretical level between a Turing machine and a finite automata. That does,

2006
01:35:21,120 --> 01:35:25,520
so like, I can reduce and people have, there are papers reducing, you know, any of these things to

2007
01:35:25,520 --> 01:35:30,320
any of the others, right? It's like it's a fairly trivial exercise. So at that level, those distinctions

2008
01:35:30,320 --> 01:35:35,040
are completely meaningless. However, they are meaningful in the sense that for many purposes,

2009
01:35:35,040 --> 01:35:40,320
I am better off having an RNN than having, you know, a transformer. And for many purposes,

2010
01:35:40,320 --> 01:35:44,960
I'm better off. So like, let's take, you know, propositional logic versus first-order logic,

2011
01:35:44,960 --> 01:35:50,080
right? If there's no such thing as infinity, first-order logic is reducible to propositional

2012
01:35:50,080 --> 01:35:55,280
logic. But that does not mean that it's useless because it can represent a lot of things exponentially

2013
01:35:55,280 --> 01:36:00,160
more compactly than propositional logic. If I want to represent the rules of chess in first-order

2014
01:36:00,160 --> 01:36:05,040
logic, it's a page, right? If I want to represent them in propositional logic, it's more pages that

2015
01:36:05,040 --> 01:36:08,480
you can have. Okay, well, I think that that's a very, very good point. But I mean, just, just

2016
01:36:08,480 --> 01:36:11,760
a devil's advocate from the psychologist, you know, do you remember that, that photo,

2017
01:36:11,760 --> 01:36:16,640
Felician Connectionism critique paper, arguing productivity and systematicity? Productivity

2018
01:36:16,640 --> 01:36:20,000
is all about the infinite cardinality of language. I mean, presumably you would agree that language

2019
01:36:20,000 --> 01:36:24,160
has an infinite cardinality. No, well, again, another instance of the same problem. Productivity

2020
01:36:24,160 --> 01:36:29,280
is very important. But the point to just be a little precisely for a second is, is to be able to

2021
01:36:29,280 --> 01:36:36,320
generate a vast number of things beyond the ones that you started with. Vast, not infinite. In fact,

2022
01:36:36,320 --> 01:36:43,360
mathematically, infinity is not a number. Infinity is just a shorthand for something that is so

2023
01:36:43,360 --> 01:36:49,360
large that it doesn't matter how large it is. Okay. I mean, at the end of the day, I'm not a

2024
01:36:49,360 --> 01:36:53,760
mathematician, but surely mathematicians would push back on this because, you know, infinity is,

2025
01:36:53,760 --> 01:37:00,800
is a quantity in mathematics. No, I mean, again, people in every field, mathematicians, physicists,

2026
01:37:00,800 --> 01:37:06,800
computer scientists are all are often guilty of they, they, they, they have this notational shorthand

2027
01:37:06,800 --> 01:37:12,080
or like, you know, this terminological shorthand that serves them well. But then they, and then

2028
01:37:12,080 --> 01:37:16,720
they use that and then the newer generations come along and the, and the public also, right,

2029
01:37:16,720 --> 01:37:21,040
they don't even realize that what's being talked about is a little bit different.

2030
01:37:21,040 --> 01:37:25,920
Infinity is a perfect example. Any serious mathematician will tell you that infinity does

2031
01:37:25,920 --> 01:37:30,080
not have the properties of a number. So for example, if I multiply infinity by 2, I still

2032
01:37:30,080 --> 01:37:36,000
get infinity. There's no number that that happens to, right? So infinity is, is not a number, right?

2033
01:37:36,000 --> 01:37:40,320
When I say infinity is not a number, mathematicians might quibble about the way I'm stating it,

2034
01:37:40,320 --> 01:37:45,440
but this is a, this is a mathematical truth, right? Infinity truly isn't, I'm being colloquial,

2035
01:37:45,440 --> 01:37:48,880
of course, when I say that it's a shorthand for something that is so large that it doesn't

2036
01:37:48,880 --> 01:37:53,680
matter how large it is. When you take limits, you know, in calculus in anything and the limit of

2037
01:37:53,680 --> 01:37:58,320
this blah, blah, as I go to infinity, this is exactly what I'm doing. I'm going to the point

2038
01:37:58,320 --> 01:38:02,160
where I'm saying like, at this point, it doesn't matter how large the number is, the result will

2039
01:38:02,160 --> 01:38:08,800
be the same. And in this way, infinitely is an extraordinarily useful concept. So I'm not here

2040
01:38:08,800 --> 01:38:12,640
to rail against infinity. I'm just saying like, we really need to understand, I mean, like, let me

2041
01:38:12,640 --> 01:38:18,960
give you a very banal example, right? From the point of view of, you know, what to have for lunch,

2042
01:38:18,960 --> 01:38:25,600
right? Because some things cost more than others. Elon Musk is infinitely rich. He does not have

2043
01:38:25,600 --> 01:38:30,480
infinite money. But it makes no difference whatsoever whether he has whatever 100 billion

2044
01:38:30,480 --> 01:38:36,080
or 200 billion to what he's going to have for lunch. You know, like a street person who has

2045
01:38:36,080 --> 01:38:41,360
$5 to them, like their fortune is not infinite, because it very much matters what lunch costs,

2046
01:38:41,360 --> 01:38:45,680
right? So this is the real sense of infinity, which we can and should use, but we shouldn't

2047
01:38:45,680 --> 01:38:51,200
confuse it with like, oh, but then your, you know, like your formalism is incomplete because it

2048
01:38:51,280 --> 01:38:55,760
doesn't encompass infinity. Yeah, it doesn't need to infinity doesn't exist.

2049
01:38:55,760 --> 01:38:59,440
Okay, okay. Well, let's come at it from the other from the composition, you know,

2050
01:38:59,440 --> 01:39:06,000
compositionality and systematicity. So that's all about being able to do, you know, like their main

2051
01:39:06,000 --> 01:39:11,520
argument was when you have a symbolic representation, you can kind of reuse the previous representations

2052
01:39:11,520 --> 01:39:17,040
downstream composition, compositionally. And when you take a discrete symbolic representation,

2053
01:39:17,040 --> 01:39:21,520
and you kind of encode it in the envelope of a vector space, you have a real problem doing that

2054
01:39:21,520 --> 01:39:26,320
because it's now like, it's irreversible that transformation, right? You can't go back to the

2055
01:39:26,960 --> 01:39:32,560
original variables. Well, it is reversible if you realize that all those real numbers are actually

2056
01:39:32,560 --> 01:39:38,800
finite. Right? So notice that real number, there's nothing less real than a real number,

2057
01:39:38,800 --> 01:39:43,280
real numbers are imaginary, right? Real numbers are numbers with infinite precision, which is

2058
01:39:43,280 --> 01:39:48,080
a monstrosity. And many people have said this, including mathematicians and physicists, right?

2059
01:39:48,080 --> 01:39:52,720
The notion of an infinite, of a number with an infinite number of digits is just monstrous.

2060
01:39:52,720 --> 01:39:58,720
And again, in particular on a computer, even if you use, you know, you know, like numbers with

2061
01:39:58,720 --> 01:40:04,560
unlimited floating point precision, right? It's limited by the size of your memory. So this transfer

2062
01:40:04,560 --> 01:40:08,240
from which is actually very important that again, that's what tensor logic largely is about,

2063
01:40:08,320 --> 01:40:13,520
from purely symbolic structures to embeddings in a vector space, right? That vector space

2064
01:40:13,520 --> 01:40:17,200
is still finite. So there's actually nothing irreversible about what happened there.

2065
01:40:17,200 --> 01:40:21,920
Interesting. Okay. So how can people, you know, find out more information about this? And can you

2066
01:40:21,920 --> 01:40:26,160
just sketch out, you know, just, just to bring it home to people where they could actually use it

2067
01:40:26,160 --> 01:40:28,880
and how it would be, you know, better than what they can currently do?

2068
01:40:28,880 --> 01:40:32,400
Right. The answer to the first question, unfortunately, is this is not published yet,

2069
01:40:32,400 --> 01:40:37,920
but hopefully it will be soon. So for the moment, there is no very good place to point people to

2070
01:40:38,000 --> 01:40:45,840
unfortunately, but that hopefully will be fixed soon. The question of where to apply it is,

2071
01:40:46,480 --> 01:40:51,680
our goal for this is that this should become the language or hope, I should say our hope,

2072
01:40:51,680 --> 01:40:56,560
is that this will become the language we're doing just about anything in AI. So for example,

2073
01:40:56,560 --> 01:41:01,120
if what you want to do is actually nothing symbolic, but you just want to build a convent,

2074
01:41:02,000 --> 01:41:07,760
you can express a convent incredibly elegantly in tensor logic. Like if you think of, for example,

2075
01:41:07,760 --> 01:41:13,920
tensor floor or PyTorch versus NumPy, right, they allow that thing to be said much more compactly,

2076
01:41:13,920 --> 01:41:18,880
compared to tensor logic, they are as bad as NumPy is compared to them. Right. Same thing on the

2077
01:41:18,880 --> 01:41:22,880
symbolic side. But of course, the real action comes in all the problems where you have both

2078
01:41:22,880 --> 01:41:28,640
components, the problem with all those problems, which ultimately is every problem in AI, right.

2079
01:41:28,640 --> 01:41:32,000
You're always like, what happens today that is very frustrating. And that's what we're trying to

2080
01:41:32,000 --> 01:41:36,480
overcome is like, you start from one of these sides these days, mainly the connectionist one,

2081
01:41:36,560 --> 01:41:40,640
which you have a good mastery of. And then the other side, for example, the symbolic one,

2082
01:41:40,640 --> 01:41:45,520
the knowledge representation, the reasoning, the composability, you just hack. Yeah. And your

2083
01:41:45,520 --> 01:41:49,760
hack solution is terrible. You're like, you're inventing the wheel, you're making it square,

2084
01:41:49,760 --> 01:41:54,240
you're trying to make it turn, but it's square, right. It's just, you know, it's a disaster.

2085
01:41:54,240 --> 01:41:58,560
And with tensor logic, you can actually have a very well founded, very well understood

2086
01:41:59,200 --> 01:42:03,360
basis on either side. So now you don't have to hack either side. Now there's, of course,

2087
01:42:03,360 --> 01:42:06,560
still things that you're going to have to hack at the end of the day, because at the end of the day,

2088
01:42:06,560 --> 01:42:09,840
you know, AI is intractable and things are heuristic. But this, you know, is,

2089
01:42:10,960 --> 01:42:14,320
you know, you know, this notion of a tradeoff that is very important in engineering. Like,

2090
01:42:14,320 --> 01:42:19,760
people have been exploring different points on this tradeoff curve. The point of tensor logic is

2091
01:42:19,760 --> 01:42:27,120
that whatever your application is, we're moving you to a better tradeoff curve. It's still a

2092
01:42:27,120 --> 01:42:32,480
tradeoff curve, but it dominates the old one. For any given X, you have a better Y and vice

2093
01:42:32,480 --> 01:42:37,360
versa. Okay. And just help me understand, because we'll move over to, you know, the discrete program

2094
01:42:37,360 --> 01:42:41,600
search and some of Josh Tannenbaum's work in a moment. But there are two schools of thought,

2095
01:42:41,600 --> 01:42:45,200
right? There's discrete first and there's continuous first, you're on the continuous

2096
01:42:45,200 --> 01:42:50,160
substrate. But usually the reason for the continuous substrate is stochastic gradient descent,

2097
01:42:50,160 --> 01:42:54,480
learnability, et cetera, et cetera. And like, help me understand. So are you saying we start

2098
01:42:54,480 --> 01:42:58,480
with symbolic representation and then we encode it into the envelope? So where does learning come

2099
01:42:58,480 --> 01:43:03,200
into it? No, very good. So in tensor logic, you can do broadly speaking, two kinds of learning.

2100
01:43:03,200 --> 01:43:07,760
You can learn the structure of these tensor equations, as we call them, using inductive logic

2101
01:43:07,760 --> 01:43:12,080
programming techniques. Again, that whole technology is there. And then once you have that, you can

2102
01:43:12,080 --> 01:43:17,040
learn the numbers by the back prop in particular ways called back propagation through structure,

2103
01:43:17,040 --> 01:43:21,120
because the structure can vary from example to example, but we know what the type parameters

2104
01:43:21,120 --> 01:43:25,680
are. So all of the machinery of inductive logic programming and all the machinery of gradient

2105
01:43:25,760 --> 01:43:30,400
descent and deep learning or not, they're both there available to be used as you traditionally

2106
01:43:30,400 --> 01:43:35,280
have. Okay, what if I made the argument, though, that it's almost like the inductive logic, you

2107
01:43:35,280 --> 01:43:39,440
know, like the program search, that's the hard bit. So if you've already got the program, why do I

2108
01:43:39,440 --> 01:43:42,800
then need to put it into a vector space? No, actually, these are also at the end of the day,

2109
01:43:42,800 --> 01:43:46,880
in machine learning, we're always trying to learn a program of some kind, right? The question is like,

2110
01:43:46,880 --> 01:43:51,600
what is the easiest way to do that? And precisely the problem with ILPS with symbolic logic is,

2111
01:43:51,600 --> 01:43:55,840
that's really a couple of problems. One is that if all that you do, you learn programs that are

2112
01:43:55,840 --> 01:44:01,440
too brittle, and we don't want them to be brittle, right? And the other one is that each type of

2113
01:44:01,440 --> 01:44:08,080
search has its limitations. So in particular, in symbolic AI, including ILP, we tend to use a lot

2114
01:44:08,080 --> 01:44:13,040
of combinatorial optimization types of search, right? What we in AI call search is discrete search.

2115
01:44:13,040 --> 01:44:17,120
And that is good in some ways, but also very limited in others. The same thing is true of gradient

2116
01:44:17,120 --> 01:44:23,840
descent, right? And now to go to that for just a second. Gradient descent is not a continuous

2117
01:44:23,840 --> 01:44:30,400
optimization algorithm. It's not, right? Again, those real numbers are not infinite precision.

2118
01:44:30,400 --> 01:44:34,960
There's actually nothing continuous going on in the computer. Gradient descent truly literally

2119
01:44:34,960 --> 01:44:39,360
rigorously mathematically is a discrete optimization algorithm. It takes discrete steps.

2120
01:44:40,080 --> 01:44:44,960
The assumption that gradient descent depends on, which is that the infinitesimally small updates

2121
01:44:44,960 --> 01:44:50,960
do not hold. And moreover, in machine learning, as a numerical analysis, we are constantly dealing

2122
01:44:50,960 --> 01:44:55,440
with this fact that there's a mismatch between our mathematical conceptual model of the space

2123
01:44:55,440 --> 01:45:00,160
that we're working with as continuous with the reality of the computer that is not continuous.

2124
01:45:00,160 --> 01:45:06,160
So now this is not, but gradient descent still is a different optimization technique

2125
01:45:06,160 --> 01:45:11,440
with some very important advantages, in particular the key, right? The power of gradient descent

2126
01:45:11,440 --> 01:45:16,400
comes from the fact that to move from my current point to a better one, I don't need to try out

2127
01:45:16,400 --> 01:45:20,480
all the neighboring points because that takes order of the time of the neighboring points.

2128
01:45:20,480 --> 01:45:26,160
I have a closed form way to compute what is the best one, right? And then I move there.

2129
01:45:26,160 --> 01:45:29,680
And this is absolutely brilliant, right? Like we don't want to let go of that, right? This is,

2130
01:45:29,680 --> 01:45:34,960
you know, Newton's enlightenment is bright idea, right? The price of that is that in order to do

2131
01:45:34,960 --> 01:45:40,880
that you have to make this approximation, which again, calculus is an approximation. It assumes

2132
01:45:40,880 --> 01:45:46,320
that certain effects are second order and can be ignored. Now, ironically, when you learn a large

2133
01:45:46,320 --> 01:45:50,800
deep network these days, you're actually in a regime where they cannot be ignored, right? Because

2134
01:45:50,800 --> 01:45:56,880
these infinitesimal changes are not that infinitesimal because you take a finite step, right? The gradient

2135
01:45:56,880 --> 01:46:01,200
descent is always taking finite steps, which is why it's a discrete algorithm. And once you take

2136
01:46:01,200 --> 01:46:06,240
that finite step for any reasonable learning rate, the total effect of the approximations that you've

2137
01:46:06,240 --> 01:46:12,560
made typically swamps the step that you're taking. So the assumption of calculus that

2138
01:46:12,560 --> 01:46:17,600
gradient descent is founded on is actually false. Now, in some ways this invalidates a lot of our

2139
01:46:17,600 --> 01:46:23,280
intuitions. In many ways, and again, this remains to be resolved, a lot of why gradient descent works

2140
01:46:23,280 --> 01:46:27,600
better than people expected to is in fact that it's doing something else. It's doing stochastic

2141
01:46:27,600 --> 01:46:31,920
search partly because of the SGD as opposed to being matched partly because of things like this.

2142
01:46:31,920 --> 01:46:35,520
Okay, well, this is really interesting. A couple of places we can go. But first of all, I remember

2143
01:46:35,520 --> 01:46:41,280
you did the paper and that introduced elements of NTK theory as well, which might be an argument

2144
01:46:41,280 --> 01:46:45,760
against the discreteness of the optimization. But also, I wanted to trade off the two types.

2145
01:46:45,760 --> 01:46:47,440
Well, why is there an argument against the discreteness?

2146
01:46:47,440 --> 01:46:51,040
Well, isn't there a, with NTK, isn't there like a closed form solution? Doesn't that kind of like

2147
01:46:51,040 --> 01:46:56,000
erode the discreteness of the optimization? No, I mean, so there's several things here. But like,

2148
01:46:56,560 --> 01:47:01,280
if you have a closed form solution, absolutely brilliantly go for it, right? There's nothing,

2149
01:47:01,920 --> 01:47:06,400
having a closed form solution in no implies that it's continuous or discrete or any other thing,

2150
01:47:06,400 --> 01:47:12,000
right? So, let's say there was a closed form solution and it was like an infinite kernel when

2151
01:47:12,000 --> 01:47:16,880
it represented some neural network, doesn't that erode the argument? Well, so first, okay, so first

2152
01:47:16,880 --> 01:47:20,320
of all, in the work that, so the work that I've done that I think you're referring to is like,

2153
01:47:20,320 --> 01:47:24,960
I have a proof that every model learned by gradient descent is a kernel machine.

2154
01:47:24,960 --> 01:47:28,640
Yeah, right. And it's something called the path kernel, which is the integral

2155
01:47:28,640 --> 01:47:34,320
of the neural tangent kernel over the overgraded descent, right? Yeah. And now the neural tangent

2156
01:47:34,320 --> 01:47:39,920
kernel does not assume that your network is infinite. Most of the theory that people have done

2157
01:47:39,920 --> 01:47:45,600
with it assumes that the network is infinitely wide, but the definition absolutely does not

2158
01:47:45,600 --> 01:47:50,320
require that. And none of what I do, and in fact, that's part of why, you know, of its part is that

2159
01:47:50,320 --> 01:47:56,160
it assumes no infinity of anything. It's for any architecture that you use, and in particular,

2160
01:47:56,160 --> 01:48:00,960
you know, finite architectures. Okay, interesting. Okay, so hence the discreteness, but can we come

2161
01:48:00,960 --> 01:48:05,360
back to this contrasting of the discrete program search and the, you know, stochastic gradient

2162
01:48:05,360 --> 01:48:11,200
descent on a vector space? Now, in the vector space, there are certain characteristics, you know,

2163
01:48:11,200 --> 01:48:14,880
there are certain symmetries, and even though it's a discrete search through the space, I would argue

2164
01:48:14,880 --> 01:48:20,160
that it's still continuous in nature, it has certain characteristics. So contrast those two

2165
01:48:20,160 --> 01:48:25,280
forms of optimization. Precisely so. Exactly. I mean, I think you've put your finger in now.

2166
01:48:25,280 --> 01:48:29,840
The whole point of these continuous spaces, right, is not that they're continuous, because again,

2167
01:48:29,840 --> 01:48:35,200
that's, that's a fiction, is that they have a certain locality structure, yeah, that you can

2168
01:48:35,200 --> 01:48:41,280
exploit to very good effect. And this is exactly what we're going to send us, right? Now, that

2169
01:48:41,280 --> 01:48:46,880
locality structure doesn't have to be infinitesimal, right? You don't need points to be infinitely close

2170
01:48:46,880 --> 01:48:51,040
for all this to apply approximately. And again, they never are, and it's always an approximation.

2171
01:48:51,040 --> 01:48:57,840
Now, the question is, do you want to make these locality assumptions or not, right? Making them

2172
01:48:57,840 --> 01:49:02,960
buys you certain things, right? But it's also potentially unrealistic in some ways, right? Now,

2173
01:49:02,960 --> 01:49:08,560
this actually, to take a very concrete instance of this, think of space, right? We model space in

2174
01:49:08,560 --> 01:49:14,720
science and physics and in anything as a continuous thing, which it is not, right? Which is not to

2175
01:49:14,720 --> 01:49:19,280
say that, and by the way, physicists are coming to this conclusion, right? These days, the prevailing

2176
01:49:19,280 --> 01:49:23,520
views is that it's from big thing, is that like, it's, you know, space arises from entanglement,

2177
01:49:23,520 --> 01:49:28,000
et cetera, et cetera, like space is not the fundamental reality, right? And now, I think

2178
01:49:28,000 --> 01:49:32,400
that where this is inevitably going one way or another is that we realize that space is discrete,

2179
01:49:32,400 --> 01:49:38,880
right? But, and this is key, it has certain properties, including symmetries like translations

2180
01:49:38,880 --> 01:49:43,760
in variance, rotation in variance, et cetera, et cetera, that whole, approximately or exactly,

2181
01:49:43,760 --> 01:49:50,400
but if those hold a whole bunch of things like that, then you have, you know, your latent variable

2182
01:49:50,400 --> 01:49:55,600
structure, right, is very well approximated by our notion of continuous space, in which case,

2183
01:49:55,600 --> 01:50:00,160
it would be foolish to not use it, right? To formulate the laws of physics and to do computer

2184
01:50:00,160 --> 01:50:06,000
vision and so on and so forth. But at the same time, right, if we believe in it too literally,

2185
01:50:06,000 --> 01:50:10,560
we walk ourselves into a blind alley. So concretely, look at computer vision, right?

2186
01:50:10,560 --> 01:50:14,240
People in the universities of computer vision started out trying to do it with differential

2187
01:50:14,240 --> 01:50:19,280
equations and Fourier analysis and all of that could continue with stuff, right? Because that

2188
01:50:19,280 --> 01:50:24,880
was the obvious thing to do, right? And it failed. That doesn't work. That's why we need things like

2189
01:50:24,880 --> 01:50:29,760
deep learning and, you know, Markov random fields that are discrete grids that use, you know,

2190
01:50:29,760 --> 01:50:34,080
to model the images and whatnot, because you are, along with the approximate continuity,

2191
01:50:34,080 --> 01:50:39,360
you also often have large discontinuities. And if you can only model the world continuously,

2192
01:50:39,920 --> 01:50:43,360
you don't know what to do. And the problem precisely is that you have all these phenomena

2193
01:50:43,360 --> 01:50:47,760
that are like this, including, you know, in vision, but also in, in turbulence and condensed

2194
01:50:47,760 --> 01:50:52,800
metaphysics and so on, you've got to realize that there are discontinues and not try to shoehorn

2195
01:50:52,800 --> 01:50:57,360
them into continuity when that's no longer appropriate. Interesting. Okay. Well, can we bring

2196
01:50:57,360 --> 01:51:03,680
in ILP and can you contrast like the kind of function spaces that are learnable in both methods?

2197
01:51:03,680 --> 01:51:09,680
Yeah. So ILP, so let me actually preface this with the following. People in every one of these

2198
01:51:09,680 --> 01:51:16,880
schools of AI tend to have this view that I can represent everything in the world using my approach.

2199
01:51:17,680 --> 01:51:22,640
So I can like, look, prologue is too incomplete. So why do you need neural networks? But I can also

2200
01:51:22,640 --> 01:51:27,600
say neural networks are too incomplete. So why do I need prologue? And in fact, kernel machines have

2201
01:51:27,600 --> 01:51:31,760
a represented theorem that says you can approximate any function, blah, blah, blah, right? So everybody

2202
01:51:31,760 --> 01:51:36,080
has one of these represent their theorems, right? That says, I can represent anything, right? So in

2203
01:51:36,080 --> 01:51:42,160
particular, you can do, right? I mean, look, first, our logic was invented by, by Frege, essentially,

2204
01:51:42,160 --> 01:51:48,080
to, to model the real numbers. So it can almost by definition model real numbers, right? Anything

2205
01:51:48,080 --> 01:51:52,160
you might want to say about real numbers and, and weight and descent and neural networks. And in

2206
01:51:52,160 --> 01:51:58,400
fact, people have even done this. So you can say it all in, in logic programming, right? So why not

2207
01:51:58,400 --> 01:52:02,720
just do that? Well, precisely because certain things are much more easily done in other ways,

2208
01:52:02,720 --> 01:52:06,720
right? So what you have to ask about anything, but then about, you know, not the logic

2209
01:52:06,720 --> 01:52:11,280
program in particular, like, what things are well represented in this way, like compactly

2210
01:52:11,280 --> 01:52:17,440
represented, and then in such a way that learning them and doing inference with them is easy, right?

2211
01:52:17,440 --> 01:52:21,120
And those things are different for logic programming and for things like deep learning,

2212
01:52:21,120 --> 01:52:25,680
which is why we need a unification of both. So what is things like logic programming and

2213
01:52:25,680 --> 01:52:30,480
ILP good for, right? It's precisely, I mean, it's many things, but the key thing is,

2214
01:52:30,480 --> 01:52:36,560
it's precisely for learning pieces of knowledge that can then be reused and composed in arbitrary

2215
01:52:36,560 --> 01:52:44,240
ways. This is the huge power symbolic AI that connectionism does not have, right? It's like,

2216
01:52:44,240 --> 01:52:49,040
I learned the fact here, I learned a rule there. And tomorrow you ask me a question,

2217
01:52:49,040 --> 01:52:53,040
and I combine that fact, actually, several rules by rule changing, right? There's a whole proof

2218
01:52:53,040 --> 01:52:57,360
tree of rules that could have come from very different places. And I do a completely novel

2219
01:52:57,360 --> 01:53:02,240
chain of inference that answers your question. This is spectacular, right? And this is surely

2220
01:53:02,240 --> 01:53:07,760
court-wide intelligence is all about. And the symbolists know how to do it. The connectionists

2221
01:53:07,760 --> 01:53:11,040
don't. But if I was a connectionist, I'd be like, you know, I know if it was a good one,

2222
01:53:11,040 --> 01:53:15,680
and the better ones like Yoshio Benji are doing this, right? It's like, go and try to understand

2223
01:53:15,680 --> 01:53:19,120
what those people understand so that you can then not combine it with those other ideas.

2224
01:53:19,120 --> 01:53:24,560
Yes. Yeah. Yeah, I completely agree. So a huge part of intelligence is this symbolic,

2225
01:53:24,560 --> 01:53:31,600
you know, extrapolation. Yeah. So how do you bring abstraction into this? Because the thing

2226
01:53:31,600 --> 01:53:36,800
that I always get caught on is that the traditional go fi vision was to, you know, handcraft the

2227
01:53:36,800 --> 01:53:41,520
knowledge. And actually, what we need is dynamic knowledge acquisition. And we need the ability

2228
01:53:41,520 --> 01:53:46,160
to create abstractions on the fly rather than just what we do now, which is crystallizing

2229
01:53:46,240 --> 01:53:51,120
existing human abstraction. How could we do that bit? Well, abstraction traditionally was and

2230
01:53:51,120 --> 01:53:57,840
still is a central topic in symbolic AI, right? Like be precise. I mean, I think nobody questions

2231
01:53:57,840 --> 01:54:02,800
that having levels of abstraction, someone is very important. The only question is how. So if you

2232
01:54:02,800 --> 01:54:08,080
look at classic knowledge representation, planning, et cetera, et cetera, abstraction is all over

2233
01:54:08,080 --> 01:54:12,640
the place. If you look at things like reinforcement learning, and I mean, even like, you know, the

2234
01:54:12,640 --> 01:54:17,920
whole idea or hope of a convent is that it captures objects at multiple levels of abstraction,

2235
01:54:17,920 --> 01:54:22,320
at least to some degree. In reality, it doesn't, right? But that's what people are trying to do

2236
01:54:22,320 --> 01:54:27,440
and not quite doing, right? Well, good. Let's touch on that then. So I mean, certainly in

2237
01:54:27,440 --> 01:54:32,560
Jan McCoon's view, I spoke with Jan the other day, he's got this autonomous path, a paper. And,

2238
01:54:33,440 --> 01:54:37,120
you know, his system is learning abstractions, but they're abstractions which are deducible from

2239
01:54:37,120 --> 01:54:42,640
base abstraction priors, like objectness and, you know, basic visual priors. And so there's this

2240
01:54:42,640 --> 01:54:47,280
assumption that everything is deducible from the priors that we put into the model. But I have this

2241
01:54:47,280 --> 01:54:53,360
kind of intuition that abstraction space is much larger than that. Yeah. I mean, so I would even

2242
01:54:53,360 --> 01:54:58,960
say that if you arrive at your abstractions solely by deduction, you have a very impoverished notion

2243
01:54:58,960 --> 01:55:05,120
of abstraction. In fact, most of inductive learning is forming abstractions. And form abstractions at

2244
01:55:05,120 --> 01:55:09,520
the most basic level is something very trivial. It's like, I have an example described by a thousand

2245
01:55:09,520 --> 01:55:15,520
attributes. If from that I induce a rule that uses only 10, I've abstracted the way the other 990,

2246
01:55:16,240 --> 01:55:21,360
right? But if a symbolist was here, they would talk about intention versus extension, and they

2247
01:55:21,360 --> 01:55:24,640
would say that, you know, you're selecting from this infinite set of possible attributes. You

2248
01:55:24,640 --> 01:55:28,400
couldn't possibly represent all of the attributes in this. I mean, just to give you a concrete

2249
01:55:28,400 --> 01:55:34,000
example, you know, you could have a, a, a, a, a, a, a, you know what I mean? You can have like this.

2250
01:55:34,000 --> 01:55:39,120
Again, I hate to bring up infinity again, because that's always what these folks bring up. But

2251
01:55:39,120 --> 01:55:44,240
how could you select from a set that large? Well, I don't need to because it is finite.

2252
01:55:44,240 --> 01:55:48,400
But what I need to do is so, so, but there is actually a good example. And, you know,

2253
01:55:48,400 --> 01:55:54,080
infinity does not bother us at all, at all there, because what it's like, if my training set,

2254
01:55:54,080 --> 01:55:59,280
right, is a set of strings, and those strings are a, a, a, a, a, a, a, right? Going up to

2255
01:55:59,280 --> 01:56:02,720
whatever number you want to pick, like, you know, a million or a quid drill in, you know,

2256
01:56:02,720 --> 01:56:08,480
or a Google, right? Then R is your learning algorithm able to induce that the, the language

2257
01:56:08,480 --> 01:56:12,960
that these rules come out of, right? The grammar is, you know, it's a series of A's, right? You

2258
01:56:12,960 --> 01:56:17,600
and I can do that immediately. You know, most deep networks have no end of trouble doing that,

2259
01:56:17,600 --> 01:56:22,080
even though it's that basic. So it is a very good example of what symbolic learning and

2260
01:56:22,080 --> 01:56:26,000
reasoning can do versus connection is you don't need to go anywhere near infinity to actually

2261
01:56:26,000 --> 01:56:30,240
have that be a very elegant example. Well, let me bring up just one other, we've touched on a

2262
01:56:30,240 --> 01:56:34,000
lot of great things, right? There's one in this space of things that we've been talking about,

2263
01:56:34,000 --> 01:56:38,640
there's one that I think is very important, which I believe you're also a fan of. And I very much

2264
01:56:38,640 --> 01:56:42,400
am. And I think it's going to, you know, maybe you're going back to the question of what I'm

2265
01:56:42,400 --> 01:56:47,040
interested in that's happening at, at new reps right now or not. So new symbolic AI is definitely a

2266
01:56:47,040 --> 01:56:53,280
big one. Another big one. And to my mind, maybe these are the two biggest ones are most interesting

2267
01:56:53,280 --> 01:56:59,040
is, is what I call symmetry based learning. And these days is more popular known by the,

2268
01:56:59,040 --> 01:57:02,880
by the, by the name of like geometric deep learning and things like that. I tend to view

2269
01:57:02,880 --> 01:57:07,680
geometric deep learning as a special case of symmetry based learning. But this idea of,

2270
01:57:08,720 --> 01:57:14,000
I think, let me, you know, to go straight to the punchline, we know that, for example, AI and

2271
01:57:14,320 --> 01:57:18,960
machine learning in particular, have as foundations, things like, you know, logic,

2272
01:57:18,960 --> 01:57:24,000
probability optimization. And I think another foundation is symmetry group theory. In fact,

2273
01:57:24,000 --> 01:57:28,080
I was having, you know, dinner with, with Max Welling just the other day, who, who, of course,

2274
01:57:28,080 --> 01:57:32,320
have also interviewed and is, you know, like a great, you know, person in this area. And we,

2275
01:57:32,320 --> 01:57:37,280
you know, I think we have very similar views on this. Well, Pedro, yesterday, and Taka Kohen

2276
01:57:37,280 --> 01:57:41,760
was sitting where you were sitting. So there you go. Yeah. Again, I remember talking with Taka

2277
01:57:42,000 --> 01:57:45,520
Kohen, some ICML many years ago, where he published one of the first papers on this.

2278
01:57:45,520 --> 01:57:51,040
And I was like, and he seemed a little disheartened by the lack of interest that people had. And I

2279
01:57:51,040 --> 01:57:55,440
said to him, just wait, this is going to be big and we're there now, right? And it's going to be

2280
01:57:55,440 --> 01:57:59,520
even bigger, I think. But also, I think to become bigger and again, to jump straight to the punch

2281
01:57:59,520 --> 01:58:05,360
line, most of the work, including me, that people have done to date has been exploiting known

2282
01:58:05,360 --> 01:58:09,440
symmetries, like, you know, translation invariance is the quintessential example. For example,

2283
01:58:09,440 --> 01:58:13,840
we have something called deep affine networks that generalize coordinates to, you know,

2284
01:58:13,840 --> 01:58:18,400
rotation, you know, scaling, et cetera, et cetera. This is all well and good. But I think

2285
01:58:18,400 --> 01:58:23,440
if this is, and if you look at New York's today, for example, most is in that vein.

2286
01:58:23,440 --> 01:58:27,120
And there's a lot of good work to be done there. But if that's all we ever do, we will always

2287
01:58:27,120 --> 01:58:32,000
remain a niche in AI with certain very good applications, like science applications,

2288
01:58:32,000 --> 01:58:36,240
where we know that certain symmetries hold and whatnot. Max and Taka are doing things like that.

2289
01:58:36,240 --> 01:58:39,680
But I don't want to just do that. I really, you know, I'm trying to make progress towards

2290
01:58:39,680 --> 01:58:44,000
human level AI. And I think the key there is to discover symmetries from data.

2291
01:58:44,560 --> 01:58:49,040
Yeah. And I think most of us agree with this. It's a hard problem, right? But that's what we're

2292
01:58:49,040 --> 01:58:54,000
here for. We want to discover symmetries from data. And, you know, there's an interesting,

2293
01:58:54,000 --> 01:58:57,520
you know, discussion of how to do that, you know, I have a number of ideas and a number of people

2294
01:58:57,520 --> 01:59:02,160
have, then the power of discovering symmetries, right, connecting back to our early conversation

2295
01:59:02,160 --> 01:59:07,200
is that symmetries can, individual symmetries can be very easy to discover because they're

2296
01:59:07,200 --> 01:59:12,880
often very simple. But then, right, by the group axioms, axioms, you can compose them arbitrarily.

2297
01:59:12,880 --> 01:59:17,840
Yeah. Which means I can, for example, by learning 100 different symmetries of a cat

2298
01:59:17,840 --> 01:59:22,640
from 100 different examples, then I can compose them and correctly recognize as a cat

2299
01:59:23,440 --> 01:59:28,000
something that is extremely different from any concrete example of a cat that I saw before.

2300
01:59:28,960 --> 01:59:32,480
Could I push back on a tiny bit? So, I mean, in the geometric deep learning prototype book,

2301
01:59:32,480 --> 01:59:37,200
I mean, they spoke about, you know, the various symmetries of groups like SO3, you know, preserves

2302
01:59:37,200 --> 01:59:42,960
translations and angles, you know, like how primitive and how platonic are these symmetries?

2303
01:59:42,960 --> 01:59:46,640
And aren't they just like obvious in respect of the domain that you're in?

2304
01:59:46,640 --> 01:59:51,680
No, very good. So this is actually a key question. Symmetry group theory is one of them.

2305
01:59:51,680 --> 01:59:56,240
It's a central area in mathematics that it's a very highly developed and it's the foundation

2306
01:59:56,240 --> 02:00:01,440
of modern physics, like the standard model is a bunch of symmetries and so on. But the way,

2307
02:00:01,440 --> 02:00:08,000
and there is an exhaustive listing of what all the possible symmetry groups are, discrete ones,

2308
02:00:08,000 --> 02:00:13,760
you know, continuous ones, you know, so-called lead groups, etc., etc. So at that level, this is

2309
02:00:13,760 --> 02:00:19,600
not naive because people already have a handle on what the space is, right? But crucially for our

2310
02:00:19,600 --> 02:00:26,480
purpose is for AI, that's not enough because precisely because those, again, the analogy

2311
02:00:26,480 --> 02:00:31,440
with logic is actually a very good one here. First of all, the logic is to brittle, right?

2312
02:00:31,440 --> 02:00:35,040
And plain symmetry group theory, the way people have mostly applied so far,

2313
02:00:35,040 --> 02:00:40,080
is also too brilliant for the same reason. So for example, right? Something like, you know,

2314
02:00:40,080 --> 02:00:44,080
people almost always immediately come up with, so like, oh, I understand, you know,

2315
02:00:44,080 --> 02:00:48,560
I like symmetries with the light to recognize, you know, perturbed digits, but a 6 is not a 9.

2316
02:00:49,520 --> 02:00:53,280
So some, like, if you just take naive symmetry group theory and you say, like, well, arbitrary

2317
02:00:53,280 --> 02:00:56,880
composability, as I was just talking about, I was like, well, now you've just said that a 6,

2318
02:00:56,880 --> 02:01:01,680
you've lost the ability to distinguish a 6 from a 9, right? Now, what we need precisely is to

2319
02:01:01,680 --> 02:01:05,520
combine symmetry group theory with the other things like statistics and optimization and

2320
02:01:05,520 --> 02:01:10,720
say something like the following. The space of things that you can compose is unlimited. You

2321
02:01:10,720 --> 02:01:15,600
can have, you know, unlimited compositions, but for example, you pay a cost for composing more

2322
02:01:15,600 --> 02:01:20,240
symmetries. And now when you find the least cost path, and that's how you're going to match things,

2323
02:01:20,240 --> 02:01:25,360
or, you know, your digit becomes less and less probable to be in 6, the more you've rotated

2324
02:01:25,360 --> 02:01:30,000
it, right? So now we know how to do all of that very well. So we know symmetry group theory very

2325
02:01:30,000 --> 02:01:34,240
well. We know how to do all these probabilistic costs, minimizing blah, blah, blah things,

2326
02:01:34,240 --> 02:01:38,880
machine learning very well. We just need to combine it to the same way that we have previously

2327
02:01:38,880 --> 02:01:42,960
combined these things with first order logic. So I'm glad you brought in the cost that that was

2328
02:01:42,960 --> 02:01:46,880
really, really good. So there were trade offs everywhere. I mean, for example, if you want to

2329
02:01:46,880 --> 02:01:52,160
make the models more fair and, you know, prioritize the low frequency attributes on the long tail,

2330
02:01:52,160 --> 02:01:56,080
the headline accuracy goes down. Same thing with robustness. If you robustify a model,

2331
02:01:56,080 --> 02:01:59,760
the headline accuracy goes down. Same thing with symmetry groups. If you introduce other

2332
02:01:59,760 --> 02:02:04,480
symmetry groups, you know, that the headline accuracy goes down. So it all comes back to the

2333
02:02:04,480 --> 02:02:09,440
bias variance trade off at the end of the day. And, you know, where is the limit here? How much

2334
02:02:09,440 --> 02:02:15,200
can we optimize these models and what does good look like? The bias variance trade off is a very

2335
02:02:15,200 --> 02:02:22,720
useful tool, right? But it's not the deepest reality, right? The way to think about bias variance is

2336
02:02:22,720 --> 02:02:27,120
that, again, talking about this notion of a trade off curve, there's a trade off between bias and

2337
02:02:27,120 --> 02:02:31,200
variance, right, which is in some sense unavoidable, right? In machine learning, if you have finite

2338
02:02:31,200 --> 02:02:35,600
data, you're trying to learn powerful models, bias variance is a trade off. And it's a very

2339
02:02:35,600 --> 02:02:39,440
consequential trade off in the sense that, for example, the things that work best with small

2340
02:02:39,440 --> 02:02:42,960
amounts of data tend not to work best with large amounts of data, right? This is something that we

2341
02:02:42,960 --> 02:02:47,040
should all, you know, grow up knowing in machine learning. But so many mistakes have been done

2342
02:02:47,040 --> 02:02:51,200
because of that, because people study things in the easy or historically, that's all they had,

2343
02:02:51,200 --> 02:02:54,880
right? And then they're very surprised when something that seemed not very good, like, say,

2344
02:02:54,880 --> 02:02:58,800
deep learning, right, turns out to be better when you have a large amount of data, or they believe

2345
02:02:58,800 --> 02:03:03,600
in, like, silly things like, you know, Occam's razor version that, you know, accurate, you know,

2346
02:03:03,600 --> 02:03:08,240
simply is more accurate and whatnot. So a lot of mistakes have been made because of lack of

2347
02:03:08,240 --> 02:03:13,840
understanding of this. Having said that, what you really want is to move to a better trade off

2348
02:03:13,840 --> 02:03:19,760
curve between bias and variance, which you can, if you get at what the reality is, right? So the

2349
02:03:19,760 --> 02:03:24,080
real game in machine, once you're evaluating your learner and figuring out, you're like, how much

2350
02:03:24,080 --> 02:03:29,040
to prune and whatnot, or how much to regulate bias variance is very important. But before that,

2351
02:03:29,040 --> 02:03:33,200
the most important question is like, what we're trying to do here is figure out what are the

2352
02:03:33,200 --> 02:03:38,720
inductive biases? What are the regularities that the world really has, at least approximately,

2353
02:03:38,720 --> 02:03:43,520
that we build our algorithms on top of that? And then if you give me a better one than I have now,

2354
02:03:43,520 --> 02:03:47,840
I'll still have a bias variance trade off, but I'll be in a curve where for the same variance,

2355
02:03:47,840 --> 02:03:51,520
I can have less bias and vice versa. And that's where the real action is.

2356
02:03:51,600 --> 02:03:54,640
Oh, interesting. Well, I didn't quite understand that because bias and variance,

2357
02:03:54,640 --> 02:03:59,120
they are mutually exclusive. And I thought at first you were saying, well, if we understand

2358
02:03:59,120 --> 02:04:03,840
what the biases are better, the prototypical symmetries of the world we live in, then we

2359
02:04:03,840 --> 02:04:07,280
can have more bias without having an approximation error, basically.

2360
02:04:07,280 --> 02:04:11,360
The confusion arises because bias is a very unfortunately overloaded term.

2361
02:04:11,360 --> 02:04:15,520
Right. This is not even getting into the psychological notion of bias like in Danny

2362
02:04:15,520 --> 02:04:20,240
Kahneman's work, or even the sociological notion of bias like racial biases, gender biases and

2363
02:04:20,240 --> 02:04:25,600
whatnot. So we need to distinguish. I just used my bad, the word bias into completely

2364
02:04:25,600 --> 02:04:30,080
different senses, completely but not unrelated. That's the thing. One of them is the statistical

2365
02:04:30,080 --> 02:04:35,920
notion of bias. There really is a trade off between the two. There's a sum of squares,

2366
02:04:35,920 --> 02:04:41,600
blah, blah, blah. The machine learning notion of inductive bias, it's the preference that you

2367
02:04:41,600 --> 02:04:46,400
have for certain models of our others, which is really just another way of saying your priors,

2368
02:04:47,360 --> 02:04:53,200
whether they are assumptions or knowledge. Maybe actually instead of bias, they're like,

2369
02:04:53,200 --> 02:04:56,880
what you really want to do is figure out what are the priors? What are the model classes?

2370
02:04:56,880 --> 02:05:02,080
What are the preferences? The bias is a kind of preference that really line up with the world

2371
02:05:02,080 --> 02:05:06,400
in reality or the domain and therefore let you move to a better trade off curve

2372
02:05:06,400 --> 02:05:13,600
among statistical bias and statistical variance. Amazing. Well, Pedro, just tell us a little bit

2373
02:05:13,600 --> 02:05:17,040
about what have you seen at NeurIPS and how's the week been for you?

2374
02:05:18,400 --> 02:05:20,880
We've already touched on some of the interesting things that I saw,

2375
02:05:22,240 --> 02:05:26,560
in particular some of the areas that I'm interested in. The thing about NeurIPS is this,

2376
02:05:26,560 --> 02:05:30,800
of course, is that it's a vast conference. In the early days, I used to at least go through

2377
02:05:30,800 --> 02:05:36,240
the proceedings and look at the title and maybe the abstract of every paper. This is now impossible.

2378
02:05:37,040 --> 02:05:41,200
Now, these days, if all you do is try to walk through the poster sessions,

2379
02:05:41,280 --> 02:05:46,480
you never get to the end. I haven't been to a single poster session in this NeurIPS

2380
02:05:46,480 --> 02:05:50,960
where I actually got through all. I like to go through the poster sessions quickly once

2381
02:05:51,680 --> 02:05:56,080
and then just to see what's there and then go back to the ones that I found really interesting.

2382
02:05:56,080 --> 02:06:01,920
I haven't actually been able to even finish that walk through because they're so vast. You're also

2383
02:06:01,920 --> 02:06:06,880
running to people which is part of the point and talk and whatnot, but when there's 500 posters in

2384
02:06:06,880 --> 02:06:10,640
every session and there's 3,000 papers in the conference, it becomes very hard to find the

2385
02:06:10,640 --> 02:06:15,120
ones that are most relevant. Of course, an easy thing to do is look at what they, I mean,

2386
02:06:16,400 --> 02:06:20,160
something about NeurIPS this year that I honestly thought was absolutely terrible,

2387
02:06:20,160 --> 02:06:27,760
like a really, really terrible idea is that it's a hybrid conference and their idea of

2388
02:06:27,760 --> 02:06:34,080
a hybrid conference is that there are no talks. The talks are all virtual next week. Nips this

2389
02:06:34,080 --> 02:06:40,160
year to a first approximation was one big poster session, which I mean, to me, this is just an

2390
02:06:40,160 --> 02:06:46,080
incredibly bad idea. In that sense, I haven't gotten as much out of Nips by this point of the

2391
02:06:46,080 --> 02:06:51,840
conference as I would have in most years. There's also looking at the papers that were usually

2392
02:06:51,840 --> 02:06:56,880
selected as oral, but this time they call them oral equivalent because there are no oral papers,

2393
02:06:56,880 --> 02:07:03,200
but they still want to have that distinction. The number of those papers these days is 160 or

2394
02:07:03,200 --> 02:07:10,080
something, which is bigger than Nips and ICML some years ago. Usually from those papers,

2395
02:07:10,480 --> 02:07:15,680
some of them kind of like jump out at you as being great and very relevant. I've only looked at

2396
02:07:15,680 --> 02:07:23,440
them briefly, so don't quote me on this, if you will, but none of those have jumped out to me

2397
02:07:25,120 --> 02:07:28,560
as like, oh, yeah, this sounds like something really brilliant and that I want to dig into,

2398
02:07:28,560 --> 02:07:32,640
but there probably are many. I just haven't really had a chance to look at them yet.

2399
02:07:32,640 --> 02:07:37,760
Yeah. I mean, I have a similar reaction. I mean, it feels like we're at the point of saturation

2400
02:07:37,760 --> 02:07:42,640
and there are loads and loads of microvariations on the same idea. It's completely overwhelming,

2401
02:07:42,640 --> 02:07:46,800
but what I find is that it's a very social experience. When I walk through the posters,

2402
02:07:46,800 --> 02:07:51,040
I just immediately become engrossed in conversation and hours go by and I just think, oh my God,

2403
02:07:51,040 --> 02:07:55,680
what have I just been doing for the last year? That's the real point. The posters are very good.

2404
02:07:57,280 --> 02:08:00,880
It's like the grain of sand and the oyster. The poster is the grain of sand. The oyster is the

2405
02:08:00,880 --> 02:08:05,360
conversation that you have with the person at the poster or with other people around there.

2406
02:08:05,360 --> 02:08:08,160
To touch on another point that you made that I think is actually important.

2407
02:08:09,520 --> 02:08:13,680
New Europe's and ICML and so on are bigger today than they've ever been. Actually,

2408
02:08:13,680 --> 02:08:17,360
not strictly true because these recent lips, surprisingly, they tend to have gone down a

2409
02:08:17,360 --> 02:08:26,720
lot. We can and should ask why, but we need to scale. There are bigger conferences,

2410
02:08:26,720 --> 02:08:31,280
like the New Science Conference is one conference and it's 35,000 people every year and they make

2411
02:08:31,280 --> 02:08:37,360
it work. It's good to experiment. I think New Europe's at the scale that it is today can work,

2412
02:08:37,360 --> 02:08:42,000
but it is not working very well. One of the ways in which it's not working very well is that

2413
02:08:42,560 --> 02:08:46,240
we need to think a lot more. I don't understand this is working. It's hard and people have day

2414
02:08:46,240 --> 02:08:51,520
jobs or not, so this is not a criticism in that sense. We need to really work on making it easy

2415
02:08:51,520 --> 02:08:56,640
for people to find the papers that are relevant to them. Number one, number two, and maybe even

2416
02:08:56,640 --> 02:09:01,760
more important, there is more machine learning research today than ever, but in some sense the

2417
02:09:01,760 --> 02:09:06,880
diversity of that research is in some ways lower than ever. Another point that you brought up and

2418
02:09:06,880 --> 02:09:11,280
I think is very important to do with the scaling of New Europe's and the machine learning communities

2419
02:09:11,280 --> 02:09:17,680
that we have in just raw numbers, more machine learning and AI research going on today than ever

2420
02:09:17,680 --> 02:09:22,720
before by an order of magnitude. But in terms of diversity, there's probably less diversity in the

2421
02:09:22,720 --> 02:09:28,160
research now than there was before, which is a tragedy. I understand why people have kind of

2422
02:09:28,160 --> 02:09:32,240
like converged to deep learning. I'm a huge fan of deep learning. I was doing it before it was

2423
02:09:32,240 --> 02:09:37,520
cool as they say and whatnot, but the extent to which 90% of the community, not just in machine

2424
02:09:37,520 --> 02:09:43,280
learning but AI, is not just pursuing and not even deep learning, but a special type of deep

2425
02:09:43,280 --> 02:09:51,120
learning, which you might call applications of backprop, is extremely undesirable. We have

2426
02:09:52,080 --> 02:09:57,120
an infinite number of micro-improvement papers along a particular direction that is almost

2427
02:09:57,120 --> 02:10:02,000
certainly a local optimum, and we're just digging into that local optimum with ever more papers and

2428
02:10:02,000 --> 02:10:08,080
never more, you know, minimal publishable units when this large amount of manpower that has come

2429
02:10:08,080 --> 02:10:14,400
into the field or is moving around, we really need to have a greater diversity of research in

2430
02:10:14,400 --> 02:10:21,680
machine learning, within deep learning, within AI, and so like we are making very poor use of our

2431
02:10:21,680 --> 02:10:26,320
research, you know, manpower right now, and we see that very much at NeurIPS today.

2432
02:10:26,320 --> 02:10:29,200
Yeah, I mean, Sarah Hooker talked about the hardware lottery, you know, being stuck in a

2433
02:10:29,200 --> 02:10:33,600
basin of attraction determined by hardware, but there's also an idea lottery. It might just be

2434
02:10:33,600 --> 02:10:38,320
the case that NeurIPS historically has always been very connectionist anyway. I mean, maybe it

2435
02:10:38,320 --> 02:10:41,520
hasn't, right? That's one of the ironies, but it's something as well. I wasn't aware of that. Okay.

2436
02:10:41,520 --> 02:10:47,920
Oh, absolutely not. I mean, in fact, the joke is, right, that NeurIPS started in the 80s,

2437
02:10:47,920 --> 02:10:52,400
it was called Neural Information Processing Systems, and by the 90s, it should have become

2438
02:10:53,040 --> 02:10:57,600
BIPs for Patient Information Processing Systems, right? There was this study that they did at one

2439
02:10:57,600 --> 02:11:02,320
point of predictors of acceptance and rejection among words in the title, and the biggest predictor

2440
02:11:02,320 --> 02:11:07,120
of rejection was the world neural. Really? And this was very famous in the field, because

2441
02:11:07,200 --> 02:11:12,240
indeed, if you could, you know, 1990 something, you were submitting papers to NIPs with the

2442
02:11:12,240 --> 02:11:16,800
world neural in the title, you didn't know what you were doing. And then in the 2000s, right,

2443
02:11:16,800 --> 02:11:22,240
it became BIPs, or should have become BIPs, sorry, KIPs, Kernal Information Processing Systems.

2444
02:11:22,240 --> 02:11:27,200
And in fact, I remember having lunch with Yoshio Bingo at the ICML in Montreal in 2009,

2445
02:11:27,200 --> 02:11:31,440
and we were talking about this, right? The fact that every day kid, and, you know,

2446
02:11:31,440 --> 02:11:35,680
not a new paradigm, but another one of the same paradigm seems to now be on top, right?

2447
02:11:35,680 --> 02:11:39,520
And, you know, he asked, like, so what is the next decade going to be? And I said,

2448
02:11:39,520 --> 02:11:44,720
it's going to be DIPs, Deep Information Processing Systems. And then we both laughed,

2449
02:11:44,720 --> 02:11:49,280
and I could tell that I believe this, but he, Yoshio Bingo, was actually skeptical of this.

2450
02:11:49,280 --> 02:11:53,920
So, you know, the deep, little did we know, right? If somebody told us that, you know,

2451
02:11:53,920 --> 02:11:56,960
this is going to be on the page of the, on the front page of the New York Times,

2452
02:11:56,960 --> 02:12:01,360
in a couple of years would be like, what are you smoking, right? So the way to which this decade

2453
02:12:01,360 --> 02:12:06,320
has been DIPs is just mind-blowing, but looking forward, right? And to this point of, you know,

2454
02:12:06,320 --> 02:12:11,280
diversity in research approaches, I think if you extrapolate naively from the past,

2455
02:12:11,280 --> 02:12:15,600
the next decade will be about something else. And the trillion-dollar question

2456
02:12:15,600 --> 02:12:21,840
is what, what is that else going to be? Amazing. Okay. You watched Charma's talk, right?

2457
02:12:21,840 --> 02:12:26,400
Yeah. What's your high-level view? I thought it was a nice talk. I thought it was a very

2458
02:12:26,400 --> 02:12:30,720
appropriate talk for an opening talk at the conference. Actually, if New Europe's had,

2459
02:12:30,720 --> 02:12:36,240
like, some conferences, a dinner talk, right? Which is supposed to be interesting, but not as,

2460
02:12:36,240 --> 02:12:39,840
you know, deep or as technical as other. This would have been the perfect dinner talk for

2461
02:12:39,840 --> 02:12:45,520
New Europe's, because the topic is very current, right? Our machine's sentient. And, you know,

2462
02:12:45,520 --> 02:12:50,880
who better to talk about it than Dave Chalmers, right? The world's expert on, on, on, on consciousness,

2463
02:12:50,880 --> 02:12:57,120
right? And by and large, I thought the talk was excellent. In fact, you know, when journalists

2464
02:12:57,120 --> 02:13:01,360
ask me questions, you know, consciousness is like one of their top three, right? Along with

2465
02:13:01,360 --> 02:13:06,080
Terminator and, you know, Unfairness or something like that, right? And I will point them to this

2466
02:13:06,080 --> 02:13:12,880
talk because it kind of like lays out, you know, the, you know, the ground. And, you know, it's good

2467
02:13:12,880 --> 02:13:18,880
for people to at least have those things in mind. At the end of the day, so I think, of course,

2468
02:13:18,880 --> 02:13:24,560
the notion that Lambda was sentient is, you know, ridiculous, as, as most of us do.

2469
02:13:25,280 --> 02:13:30,480
You could ask a slightly more fine-going question was if, if, if, if, if consciousness is on a

2470
02:13:30,480 --> 02:13:35,680
continuum, right? Which I think Dave believes in. And if you believe in like this, you know,

2471
02:13:35,680 --> 02:13:40,000
IT theory and phi and whatnot, you know, like, phi is never zero, right? So there's always some

2472
02:13:40,000 --> 02:13:44,640
consciousness, right? Pensychism and whatnot. I'm not saying I believe in that. We could,

2473
02:13:44,640 --> 02:13:48,480
we could go into the, but like, if you believe in that, then you can ask, well, on that scale,

2474
02:13:48,480 --> 02:13:54,880
you know, where is Lambda? Where are these large language models? And, and, and surely higher than

2475
02:13:54,880 --> 02:14:00,960
previous AI systems, right? But in my view, still very, very, very far. And I think what you want

2476
02:14:00,960 --> 02:14:05,840
to keep in mind is that consciousness does not, does not increase continuously. Precisely,

2477
02:14:05,840 --> 02:14:11,200
there's these transitions where you go, you know, more is different is the, is the famous,

2478
02:14:11,200 --> 02:14:15,520
you know, phrase about emergence, right? Consciousness is very much an emerging,

2479
02:14:15,600 --> 02:14:19,840
you know, phenomenon. And I think what happens is that there are points at which your

2480
02:14:19,840 --> 02:14:24,160
consciousness will leap. Maybe a thermostat does have consciousness, like, you know,

2481
02:14:24,960 --> 02:14:29,360
or, you know, or purpose or whatever, right? Like, like people in, like people like McCarthy,

2482
02:14:29,360 --> 02:14:34,480
for example, had had had that as an example. But the amount of consciousness is minuscule.

2483
02:14:34,480 --> 02:14:40,640
And, and that, and the way I will put that is that these large language models still have not

2484
02:14:40,640 --> 02:14:45,360
passed that first threshold. Interesting. So, so in a similar way to some of the discussion

2485
02:14:45,360 --> 02:14:50,160
about large language models, there are kind of scaling breaks in the levels of consciousness.

2486
02:14:50,160 --> 02:14:53,920
I mean, Chalmers made the comment, though, that rather than it being a pure continuum,

2487
02:14:53,920 --> 02:14:58,160
he said that a bottle was not conscious, but then there was a kind of. No, yes. So very key

2488
02:14:58,160 --> 02:15:04,560
point. Scaling is part of it, but not only. It's not just that. So your cortex to first

2489
02:15:04,560 --> 02:15:10,160
approximation is a monkey brain scaled up, right? There was a module there that evolution

2490
02:15:10,160 --> 02:15:14,960
discovered, and it really paid to keep making more and more of it. And we can easily speculate why.

2491
02:15:14,960 --> 02:15:20,320
But the point is, so let me contrast two things, right? Which is true for consciousness, but also

2492
02:15:20,320 --> 02:15:25,920
for just AI in general. A lot of people are scaling believers and like open AI is the poster child

2493
02:15:25,920 --> 02:15:29,920
of this in a quite conscious ways. Like, we're just going to scale the heck out of things.

2494
02:15:29,920 --> 02:15:33,280
And then a lot of people, like, you know, Gary Marcus being a good example, they just

2495
02:15:33,280 --> 02:15:37,840
completely poo poo that they say, like, oh, no, this is a joke. Right. And I think the truth is that

2496
02:15:38,480 --> 02:15:43,120
scaling is good, right? Again, you know, part of what we are, our intelligence is scaling.

2497
02:15:43,120 --> 02:15:48,240
But the question is, what are you scaling? And the things that we're scaling today,

2498
02:15:48,240 --> 02:15:53,360
it doesn't matter how much we scale them, we never get to human level intelligence or consciousness.

2499
02:15:53,360 --> 02:15:58,080
So I think we need some fundamentally different algorithms, if you want to think at the level

2500
02:15:58,080 --> 02:16:02,320
of algorithms, or fundamentally different architect architectures, if you want to think

2501
02:16:02,320 --> 02:16:07,040
about it in a way, and then scaling those up at some point will give us consciousness.

2502
02:16:07,040 --> 02:16:10,880
If you live that it's possible for a computer to be conscious, but we're not there yet,

2503
02:16:10,880 --> 02:16:15,120
either in terms of the scaling, although actually scaling is actually the easier part of this way,

2504
02:16:15,120 --> 02:16:19,200
we're actually at the point where a computer can have the same amount of computing power that

2505
02:16:19,840 --> 02:16:24,880
your brain does, which was not the case before. But the bigger deeper problem, and the more

2506
02:16:24,880 --> 02:16:29,600
fundamental one is like, we need the architecture to scale. Right. And this is where I sympathize,

2507
02:16:29,600 --> 02:16:34,400
you know, with people like Jeff Hinton, who's just, you know, playing with, you know, ideas

2508
02:16:34,400 --> 02:16:39,280
using Mathematica and very small examples, which in some ways, sounds very underpowered,

2509
02:16:39,280 --> 02:16:44,240
but I think it's people like that, they are going to come up with the things that we then scale.

2510
02:16:44,240 --> 02:16:48,320
As in fact, it was David Roemmerhardt doing that kind of work that invented backprop.

2511
02:16:49,200 --> 02:16:53,120
Right. If he hadn't invented backprop, this whole industry would not exist. So

2512
02:16:53,120 --> 02:16:57,920
what I think is that the real backprop, the real master algorithm is not there yet,

2513
02:16:57,920 --> 02:17:03,600
and we need to discover that first. And then we, and then when we scale that up, which will not

2514
02:17:03,600 --> 02:17:08,960
be trivial, but will be much easier by comparison, then we'll have, you know, human level, intelligence,

2515
02:17:08,960 --> 02:17:14,800
consciousness, et cetera. Interesting. Okay. And so Charmes is a structuralist computationalist.

2516
02:17:14,800 --> 02:17:22,720
So, you know, he thinks information, not biology. And he's also a functionalist, right? So, you

2517
02:17:22,720 --> 02:17:28,720
know, which is very similar to behavior. And, you know, Hillary Putnam made the move that you can

2518
02:17:28,800 --> 02:17:33,600
kind of like represent a computation in any open physical system. And he kind of like used that

2519
02:17:33,600 --> 02:17:38,160
on, you know, if you follow that line of thought, it almost trivializes computationalism because,

2520
02:17:38,160 --> 02:17:42,800
you know, it leads to panpsychism very, very quickly. So, first of all, I mean, what's your

2521
02:17:42,800 --> 02:17:46,800
take on this idea that information could give rise to intelligence and consciousness?

2522
02:17:46,800 --> 02:17:51,440
So I agree, like most scientists, and I think in particular most computer scientists, that

2523
02:17:52,160 --> 02:17:56,880
to a first approximation, the substrate does not matter. And in particular,

2524
02:17:56,880 --> 02:18:01,680
you're not going to convince me that something is not conscious just because it's not biological.

2525
02:18:02,320 --> 02:18:06,320
There is no reason to think that only biological things can have consciousness. Now,

2526
02:18:06,320 --> 02:18:12,000
the deeper problem, and you know, indeed the hard problem, is that so as Dave Chalmers defined it,

2527
02:18:13,040 --> 02:18:16,160
so there's a basic fork here, which you've alluded to, which is,

2528
02:18:17,280 --> 02:18:23,200
if consciousness is subjective experience, then all these questions about consciousness are

2529
02:18:23,200 --> 02:18:29,600
ultimately unresolvable, because only I have my subjective experience. I know that I'm conscious,

2530
02:18:30,160 --> 02:18:35,120
no one can persuade me of the contrary. I don't even know if you are conscious, let alone some machine.

2531
02:18:35,680 --> 02:18:41,120
Right? So if consciousness is an intrinsic property of something that cannot be evaluated

2532
02:18:41,120 --> 02:18:45,920
from the outside, then we're doomed. We're never going to answer this question. And maybe that is

2533
02:18:45,920 --> 02:18:50,320
the case. Right? So I'm not saying that's false, and you need to always keep that in mind. But now,

2534
02:18:50,400 --> 02:18:55,840
if we're going to make any kind of progress, right, we need to look at what are, to generalize a well

2535
02:18:55,840 --> 02:19:01,120
known term, the external correlates of consciousness. Right? One of those which has been well studied by

2536
02:19:01,120 --> 02:19:05,760
people like Christoph Koch and so on, and I think that's a very good direction, is the neural

2537
02:19:05,760 --> 02:19:10,000
correlates of consciousness. Right? What goes on in your brain that correlates with consciousness?

2538
02:19:10,000 --> 02:19:14,080
And we've made a lot of progress with that. You can also talk about what are sort of like the

2539
02:19:14,080 --> 02:19:19,760
informational computational correlates of consciousness. Are there computational structures

2540
02:19:19,760 --> 02:19:24,640
that support consciousness and the ones that don't? I think that is also a useful thing to do.

2541
02:19:24,640 --> 02:19:30,000
Let's develop. It actually interests this panpsychism because it's not like everything is

2542
02:19:30,000 --> 02:19:35,200
consciousness just because it can compute. Some computations after this emergence and these,

2543
02:19:35,200 --> 02:19:40,880
you know, phase transitions may give rise to consciousness. Whereas others, it doesn't matter

2544
02:19:40,880 --> 02:19:44,880
how much of them you have, they will never be conscious. So I think this is also a very useful

2545
02:19:44,880 --> 02:19:51,120
way to make progress on this question and one to which AI versus, you know, a neuroscience or

2546
02:19:51,120 --> 02:19:56,400
psychology is very well suited to. Interesting. So on the functionalism point, and I think

2547
02:19:56,400 --> 02:20:02,080
Chalmers has been very, very consistent. He uses this kind of calculi to reason about

2548
02:20:02,080 --> 02:20:07,760
intelligence as well. So a system is intelligent if it can perform reasoning, if it can perform

2549
02:20:07,760 --> 02:20:12,320
planning, if it has sensing and so on. So we have this collection of functions. And then

2550
02:20:12,400 --> 02:20:16,480
he's kind of like moved this over to the domain of consciousness. So similarly,

2551
02:20:16,480 --> 02:20:23,040
if a system performs these functions and is used in a positive and a negative way. So some

2552
02:20:23,040 --> 02:20:28,320
functions would indicate an absence of consciousness and some functions would, you know, lead to the

2553
02:20:28,320 --> 02:20:33,760
presence of consciousness. And it's kind of like leading towards a, you know, touring test for

2554
02:20:33,760 --> 02:20:38,800
consciousness. I mean, do you kind of support that? That's a very interesting question. In fact,

2555
02:20:38,800 --> 02:20:43,760
you know, I was having dinner with Dave after his talk and I actually brought this up because it

2556
02:20:43,760 --> 02:20:50,160
wasn't clear from his talk. And I said, look, this is the answer that I usually give to journalists

2557
02:20:50,160 --> 02:20:54,080
when they ask me, you know, will machines ever be conscious and whatnot? And asked me a few,

2558
02:20:54,080 --> 02:20:58,800
and asked me if he agreed with it and actually expected him to disagree. But I think again,

2559
02:20:58,800 --> 02:21:03,520
don't want to put words in his mouth, but that he agreed, right? And the answer is the following,

2560
02:21:03,520 --> 02:21:08,480
is that human beings, right? As we've discussed, have an amazing tendency to

2561
02:21:08,480 --> 02:21:13,200
anthropoformize things. It's reasoning by analogy. And what happens, I used to say,

2562
02:21:13,200 --> 02:21:17,280
this is what's going to happen at this point is this is what is already happening is that

2563
02:21:17,280 --> 02:21:24,000
as soon as a machine behaves externally, even vaguely like it's consciousness, we immediately

2564
02:21:24,000 --> 02:21:29,280
start treating it as if it's consciousness. So if you look for 10, 20, 50 years from now,

2565
02:21:29,280 --> 02:21:33,920
we will just treat AI's as if they're consciousness and people won't even ask that question.

2566
02:21:33,920 --> 02:21:38,320
They will assume AI's are conscious in the same way that we assume that each other,

2567
02:21:38,320 --> 02:21:44,160
that we're conscious, right? But then, and so like from that pragmatic external point of view,

2568
02:21:44,160 --> 02:21:49,520
maybe the question is answered, right? But you could be a philosopher or like sort of like a very,

2569
02:21:49,520 --> 02:21:54,960
you know, rigorous, you know, technical person and so like, no, no, no, no, I really want to know

2570
02:21:54,960 --> 02:21:59,760
if things, they may look, you know, conscious from the outside, but are they really, right?

2571
02:22:00,320 --> 02:22:04,720
But that question, as far as I can tell, unfortunately, at the end of the day is probably

2572
02:22:04,720 --> 02:22:09,600
unanswerable. Now, there's a middle ground between these two things that maybe is where

2573
02:22:09,600 --> 02:22:13,920
we'll wind up. And to me, sounds like probably the best thing that we're going to be able to do,

2574
02:22:13,920 --> 02:22:18,800
which is that like, our understanding of the neural informational, et cetera, correlates

2575
02:22:18,800 --> 02:22:24,640
of consciousness evolves to a point where we have the feeling that we do understand consciousness.

2576
02:22:24,960 --> 02:22:28,560
It's not just the late person calls this consciousness even though haha, it's not like

2577
02:22:28,560 --> 02:22:31,680
lambda is not conscious, you know, poor bozo, et cetera, et cetera. It's like,

2578
02:22:32,400 --> 02:22:35,840
you know, there are many analogies to that in the history of science. There used to be a lot of

2579
02:22:35,840 --> 02:22:41,280
things that were like magical, right? And we were like, oh, we're never going to stand like life was

2580
02:22:41,280 --> 02:22:45,920
magical, right? Life did not obey the laws of physics. It's just something else, right? This

2581
02:22:45,920 --> 02:22:50,240
sounds laughable right now, but it wasn't laughable at all then, right? And now, it's not like we've

2582
02:22:50,240 --> 02:22:54,640
understood everything about life very far from it. When you say like, there's DNA and their

2583
02:22:54,640 --> 02:22:59,120
cells and then this is how it all arises, right? And I think we're at the point in consciousness

2584
02:22:59,120 --> 02:23:04,320
where it's to like, oh, consciousness is some so beyond us, right? I think we will get, you know,

2585
02:23:04,320 --> 02:23:09,360
there will be a structure of DNA moment in the history of the study of consciousness.

2586
02:23:09,360 --> 02:23:13,600
And I think, yeah, I think things like Phi and this, you know, IT3 and whatnot,

2587
02:23:13,600 --> 02:23:18,400
they're very brave attempts to make progress in this direction. I think, you know, like Julia

2588
02:23:18,880 --> 02:23:24,400
Tononi in a way is, you know, very deluded in thinking that he has nailed what consciousness

2589
02:23:24,400 --> 02:23:30,400
is, right? I think, you know, Phi maybe is an upper bound on consciousness, but with steps like this,

2590
02:23:30,400 --> 02:23:35,040
hopefully at some point, and very much with the help of AI, right? AI is really useful for this,

2591
02:23:35,040 --> 02:23:40,240
because it's a brain that might be consciousness that we have a lot of control of. And you can do

2592
02:23:40,240 --> 02:23:45,920
experiments that you can't, you know, with people, right? So I think we will make at least some progress

2593
02:23:45,920 --> 02:23:50,800
in that direction for sure. Maybe to the point where we feel that, yes, we do understand what

2594
02:23:50,800 --> 02:23:55,200
consciousness is, we're not asking ourselves that question anymore. And then we can point to things

2595
02:23:55,200 --> 02:23:59,600
and say, this is consciousness, this is that kind of consciousness, that amount of consciousness,

2596
02:23:59,600 --> 02:24:03,440
and so on. Yeah, that's really interesting. I agree, we're making a lot of progress in getting

2597
02:24:03,440 --> 02:24:08,000
a handle on this. And although the biggest game in town is still the computationalism game. And

2598
02:24:08,000 --> 02:24:13,040
as you say, historically, the only alternative was mysterious. And my friend, Professor Mark

2599
02:24:13,120 --> 02:24:17,200
Bishop, that he said that that's one of the reasons why he's become interested in the

2600
02:24:17,200 --> 02:24:21,120
forays in cognitive science, because for the first time, it's given him a kind of robust

2601
02:24:21,120 --> 02:24:25,520
alternative to computationalism. But just coming back quickly, you know, as Charlie's

2602
02:24:25,520 --> 02:24:29,920
reference, Thomas Nagel, you know, which is that it is something it is like to be a bat.

2603
02:24:30,560 --> 02:24:36,000
What do you think about that? So I'm not sure your question is, but let me check.

2604
02:24:36,000 --> 02:24:39,840
Well, what do you mean? Do you agree that there is something it is like to be a bat?

2605
02:24:39,840 --> 02:24:45,760
Oh, absolutely. Right. So there is more and more than that, right? There is something that it's

2606
02:24:45,760 --> 02:24:52,320
like to be a bat. And it's very different from being a human, right? And we grossly underestimate,

2607
02:24:52,320 --> 02:24:56,960
right? Again, we do this thing that again, it's a heuristic, it works very well as like,

2608
02:24:56,960 --> 02:25:01,120
we project ourselves into the bat, because what else could we do, right? But then what you see

2609
02:25:01,120 --> 02:25:06,160
is a bat seen through the mind of a human, right? And in fact, there's this famous, I would say,

2610
02:25:06,160 --> 02:25:11,760
even more famous, you know, you know, notion from, from Wittgenstein, right? That if the

2611
02:25:11,760 --> 02:25:18,720
lion could talk, I would not understand anything that the lion was saying. Because his world is

2612
02:25:18,720 --> 02:25:24,560
so different from mine. Now, I actually think, I think this is a very important position to,

2613
02:25:24,560 --> 02:25:28,880
as a reference point, right? Certainly a defensible one. And, you know, Wittgenstein was a good

2614
02:25:28,880 --> 02:25:34,400
defender of it. But I actually think that this is going too far. I think, ultimately, I mean,

2615
02:25:34,400 --> 02:25:38,960
never be able to completely know what it's like to be a lion. But we can make a lot,

2616
02:25:38,960 --> 02:25:42,640
don't underestimate us either, right? We can make a lot of inwards into understanding what

2617
02:25:42,640 --> 02:25:48,320
it's like to be a lion, much more than we understand today. Same thing for a bat. And,

2618
02:25:48,320 --> 02:25:53,280
you know, you could also ask that for a fruit fly, right? In a way, a fruit fly is more different

2619
02:25:53,280 --> 02:25:58,000
from us than a lion, but it's easy to understand, right? Because at some level, that thing is so

2620
02:25:58,000 --> 02:26:01,520
simple that we can understand what's going on with it, because it's not that deep.

2621
02:26:01,600 --> 02:26:05,680
Yeah, that's a beautiful quote, actually. So, closing this off, do you think that large

2622
02:26:05,680 --> 02:26:08,160
language models are slightly conscious or will be in the near future?

2623
02:26:08,800 --> 02:26:14,160
I think language, I think large language models are not slightly conscious by the reasonable,

2624
02:26:14,160 --> 02:26:18,560
you know, everyday definition of the world slightly, meaning that their consciousness,

2625
02:26:18,560 --> 02:26:25,760
so I think that either their consciousness is just zero, right? If somebody asked me, like, you know,

2626
02:26:25,760 --> 02:26:29,520
how much, you know, consciousness does, you know, lambda half, tell me in one word, and the answer

2627
02:26:29,520 --> 02:26:35,600
would be zero, right? But another answer which is hard to distinguish from the first one is epsilon,

2628
02:26:35,600 --> 02:26:41,360
right? Maybe it has a very tiny amount of consciousness, but it's so tiny that it doesn't

2629
02:26:41,360 --> 02:26:46,320
even qualify as slightly. Again, this gets back to what its architecture is. It actually gets

2630
02:26:46,320 --> 02:26:51,600
too lot of things, but for purposes of this discussion, right, lambda and these large

2631
02:26:51,600 --> 02:26:56,480
language models are not very different from a big lookup table. Any big lookup table is not

2632
02:26:56,480 --> 02:27:00,960
conscious. Now, I mean, there are a lot of interesting distinctions that you can make it well.

2633
02:27:00,960 --> 02:27:05,040
What if what I have is an efficient approximation to a lookup table? Isn't that what your brain is,

2634
02:27:05,040 --> 02:27:09,520
right? And I would say yes, and then people say, well, but then why is your brain conscious

2635
02:27:09,520 --> 02:27:14,400
but not the lookup table, right? And precisely the interesting question is that the consciousness

2636
02:27:14,400 --> 02:27:20,240
comes about from the fact that you have to concentrate all of this information, you know,

2637
02:27:20,240 --> 02:27:25,360
in real time, into something, you know, very compact and that leads to action continuously,

2638
02:27:25,360 --> 02:27:30,960
right? So to put this in another way, maybe God is unconscious because he doesn't need to be,

2639
02:27:30,960 --> 02:27:35,520
right? If you're omnipotent and omniscient, you don't need to be conscious. You are effectively

2640
02:27:35,520 --> 02:27:40,240
just a lookup table. Exactly. And I loved your response earlier about the grain of sand and

2641
02:27:40,240 --> 02:27:43,920
the oyster. I thought that was a beautiful way of looking at it. And having recently studied

2642
02:27:43,920 --> 02:27:49,200
so, I mean, personally, I think it's a lot to do with intentionality and agency, but I remember

2643
02:27:49,200 --> 02:27:56,160
you responded to that. Just final quick question. What's your definition of intelligence?

2644
02:27:57,920 --> 02:28:02,560
So let me start with the technical definition, which is unfortunately not widely known enough

2645
02:28:02,560 --> 02:28:07,920
and not appreciated enough. But I think it's a really important one to have, right? Intelligence

2646
02:28:07,920 --> 02:28:14,880
is solving NP-complete problems using heuristics. This is the real technical definition of AI,

2647
02:28:14,880 --> 02:28:19,440
right? And there's a lot packed into that, right? The fact that it's NP-complete problems and the

2648
02:28:19,440 --> 02:28:24,400
fact that it's using heuristics. If your problem is solvable with a lookup table with polynomial

2649
02:28:24,400 --> 02:28:28,960
algorithms, you don't need intelligence and there's no intelligence there. It's when you start solving

2650
02:28:28,960 --> 02:28:36,320
hard problems using heuristics that you're getting into the realm of intelligence. Moreover, NP-complete

2651
02:28:36,320 --> 02:28:42,160
is not the same as exponential, right? The crucial thing about an NP-complete problem that connects

2652
02:28:42,160 --> 02:28:47,120
very directly to our entire discussion of utility and whatnot is that the solution is easy to check.

2653
02:28:48,000 --> 02:28:53,440
This is the key. If you're working on problems whose solution is impossible to check effectively,

2654
02:28:53,440 --> 02:28:57,920
I can't even tell if you're intelligent or not. The whole thing about intelligence in humans and

2655
02:28:57,920 --> 02:29:02,960
machines is that how you solve the problem requires a lot of intelligence, a lot of computing power

2656
02:29:02,960 --> 02:29:06,720
and whatnot, but then I can easily check the solution. Now, hang on a minute, could that

2657
02:29:06,720 --> 02:29:11,440
say a step away from behavior then if you're saying that, you know, like you have the percepts,

2658
02:29:11,440 --> 02:29:14,720
the state and the action and you're saying the state is also important?

2659
02:29:14,720 --> 02:29:22,240
No, so to answer that head on, intelligence is not behavior, right? Intelligence to give a slightly

2660
02:29:22,240 --> 02:29:26,240
more general definition and then there's several and they all have their merits. Intelligence is the

2661
02:29:26,240 --> 02:29:31,200
ability to solve hard problems. Then more concretely, it's NP-complete problems and using heuristics,

2662
02:29:31,200 --> 02:29:38,000
but like, for example, if you create an AI system that cures cancer, it doesn't behave in the sense

2663
02:29:38,000 --> 02:29:42,720
that a human and a robot behave, but, you know, it's damn intelligence, it's more intelligent

2664
02:29:42,720 --> 02:29:48,080
than we are, right? It would be childish to deny intelligence to that system, no matter how it solves

2665
02:29:48,080 --> 02:29:54,720
cancer. If it finds a ridiculously simple way to solve cancer, then it's even more brilliant,

2666
02:29:54,720 --> 02:29:59,360
right? In fact, the simpler your outcome, the more intelligent you are, right? It takes intelligence

2667
02:29:59,360 --> 02:30:04,800
to produce something simple. Wow. Concretely, in many circumstances, in particular evolution,

2668
02:30:04,800 --> 02:30:09,840
right? Intelligence manifests itself as behavior. There's a sequential decision making problem,

2669
02:30:09,840 --> 02:30:13,840
there's an agent in the world that said a certain stuff, being a stochastic parrot.

2670
02:30:13,840 --> 02:30:18,640
And I think also from, you know, theoretical reasons, by analyzing what a transformer can

2671
02:30:18,640 --> 02:30:23,360
represent and how it can learn, my best guess, which could be wrong again, I don't think anybody

2672
02:30:23,360 --> 02:30:28,880
has the answer to this and it's interesting question is that those transformers, right,

2673
02:30:28,880 --> 02:30:33,680
not LLM scholars, that means more of like a task rather than the, you know, than the architecture.

2674
02:30:34,320 --> 02:30:38,000
Transformers have a certain limited ability to do compositionality,

2675
02:30:38,560 --> 02:30:44,480
very limited to compare to full logic programming, etc., but exponentially better than something like

2676
02:30:44,480 --> 02:30:49,920
an ordinary multilayer perceptron. And if you just, I mean, even a multilayer perceptron or any

2677
02:30:49,920 --> 02:30:55,760
learning algorithm is more than a stochastic parrot, because it's general, the whole point

2678
02:30:55,840 --> 02:31:00,320
of machine learning is to generalize beyond the data. If you generalize correctly beyond

2679
02:31:00,320 --> 02:31:04,880
the data, you're not just a parrot anymore. And, you know, I think it's not an accident that that

2680
02:31:04,880 --> 02:31:09,840
term stochastic parrot came from Emily Bender, my linguistics colleague at UW, who does not

2681
02:31:09,840 --> 02:31:15,360
understand machine learning. She's a classic linguist of the Chomsky and Variety, who does,

2682
02:31:15,360 --> 02:31:19,440
you know, does not fundamentally understand what I think, you know, she might disagree,

2683
02:31:19,440 --> 02:31:23,040
what machine learning is all about. And she would probably look at any learning algorithm and say

2684
02:31:23,040 --> 02:31:27,760
that it's a stochastic parrot, missing the fact that the whole point of machine learning and the

2685
02:31:27,760 --> 02:31:32,800
thing that we focus on from, you know, beginning to end is generalizing. And as soon as you're

2686
02:31:32,800 --> 02:31:37,680
generalizing correctly, even if you have no compositionality, you're already doing something

2687
02:31:37,680 --> 02:31:42,320
that has a little bit of intelligence, and that's beyond what a parrot would do.

2688
02:31:42,320 --> 02:31:46,480
Yeah, I mean, to be fair, it's not a binary. And at the time, I thought they were stochastic

2689
02:31:46,480 --> 02:31:51,200
parents as well. I've updated my view. And you were talking as well about creativity. There's

2690
02:31:51,200 --> 02:31:55,280
a kind of blurred hyperplane of creativity. And we discussed, you know, where that hyperplane

2691
02:31:56,000 --> 02:32:00,240
sits. But, you know, what's really interested me, I've interviewed quite a few people that are

2692
02:32:00,240 --> 02:32:04,800
working on working on in context learning in these language models. And it seems like these

2693
02:32:04,800 --> 02:32:11,120
language models are almost almost like a new type of compiler, you know, you're writing a program

2694
02:32:11,120 --> 02:32:17,120
inside the language prompt. And they seem to work extremely well outside of the training

2695
02:32:17,120 --> 02:32:20,000
range if you're doing like basic multiplication tasks.

2696
02:32:20,000 --> 02:32:23,840
I think it is useful to look at them as a new type of compiler. In fact, I've been saying for a

2697
02:32:23,840 --> 02:32:29,280
long time that, you know, like, there's this continuum from programming an assembly code to

2698
02:32:29,280 --> 02:32:34,880
high level languages to doing AI, right? The point of AI is to continue along that path

2699
02:32:34,880 --> 02:32:40,000
to making the language that computers speak ever closer to ours, so that we can just program them

2700
02:32:40,000 --> 02:32:44,640
by talking to them or writing things at them, right? Having said that, I think that, you know,

2701
02:32:45,600 --> 02:32:51,360
what goes on in the innards of a transformer, right, is actually still

2702
02:32:53,920 --> 02:32:59,920
very primitive, for lack of a better word, right? There's a lot of, so something I tweeted that

2703
02:32:59,920 --> 02:33:04,400
got a lot of follow up from people like Yan and Gary and who the pro because they were all bringing

2704
02:33:04,400 --> 02:33:08,720
in their own angles. So this was like, I said, and I think this is an interesting question. It's

2705
02:33:08,720 --> 02:33:13,520
like the interesting question about transformers is what needs to be added to them to get real

2706
02:33:13,520 --> 02:33:18,640
intelligence. So we should not deny what they have, like the attention mechanism in particular,

2707
02:33:18,640 --> 02:33:23,200
right? And the embeddings and the context. So like, there are two very important things in

2708
02:33:23,200 --> 02:33:28,240
transformers that are beyond what was in neural networks 10 years ago and are key. One of them

2709
02:33:28,240 --> 02:33:34,720
is attention, right? Attention is a real advance. And the other one is context specific embeddings,

2710
02:33:34,720 --> 02:33:38,720
right? Each of these ideas is important in its own right and combining them together is very

2711
02:33:38,720 --> 02:33:43,440
powerful, right? Again, because the context sensitive embeddings get that the similarity

2712
02:33:43,440 --> 02:33:47,840
part of intelligence, the attention combined with the context sensitivity of the embeddings

2713
02:33:47,840 --> 02:33:53,440
gets at the compositionality part. So they do have, so there are a couple of steps forward on

2714
02:33:53,440 --> 02:33:58,400
the road to human level intelligence, but there are many more. And rather than either saying like,

2715
02:33:58,400 --> 02:34:02,640
oh, they're just parrots, they don't do anything, we're saying like, we've almost solved the eye,

2716
02:34:02,640 --> 02:34:07,360
what we really should, we should try to understand better, you know, how the, you know,

2717
02:34:07,360 --> 02:34:12,560
the attention and the context is dependent embeddings work, which we don't. But we also need

2718
02:34:12,560 --> 02:34:17,040
to focus like, now, what are we still missing? Because we definitely are. And that's really

2719
02:34:17,040 --> 02:34:20,960
where most of our focus should be. Yeah, I completely agree. And also just in defense

2720
02:34:20,960 --> 02:34:25,760
of Bender, I mean, I think she's a brilliant linguist. And I personally think having that

2721
02:34:25,760 --> 02:34:32,960
diversity of views from different people is useful. No, I mean, so I very much think that having a

2722
02:34:32,960 --> 02:34:36,560
diversity of views is very important. And I think something that I'm always saying to my

2723
02:34:36,560 --> 02:34:41,200
deep learning friends who can't stand, you know, who hate the guts of Gary Marcus is

2724
02:34:41,920 --> 02:34:48,560
we really, really need informed critics. Yeah. And very typically, your informed critics are not

2725
02:34:48,560 --> 02:34:54,240
people in the field. We are experts, but then we also suffer from the distortion of being experts.

2726
02:34:54,240 --> 02:35:00,080
It's people in adjacent areas. And people like linguists and psychologists are very much those

2727
02:35:00,080 --> 02:35:05,280
people, they're in adjacent areas, enough to have a good critique of AI. So for example,

2728
02:35:05,280 --> 02:35:11,200
something that Jan is always throwing at Gary Marcus, that kind of doesn't sit well with me,

2729
02:35:11,200 --> 02:35:15,040
says like, well, you should try building a real system sometime, and you can criticize this until

2730
02:35:15,040 --> 02:35:19,200
we do. If we take the attitude that only engineers can criticize engineers, we're doomed.

2731
02:35:20,240 --> 02:35:24,560
Having said that, there is a very big distinction between the knowledgeable informed critics like

2732
02:35:24,560 --> 02:35:29,280
Gary Marcus, and the not so knowledgeable, not so well informed ones, which unfortunately,

2733
02:35:29,280 --> 02:35:33,680
Emily is an example. I mean, she's my colleague at UW. And I've talked with her about some of these

2734
02:35:33,680 --> 02:35:38,320
things. And her criticism of machine learning, unfortunately, like a lot of people, comes from

2735
02:35:38,320 --> 02:35:43,840
a place of actually not fundamental understanding it very well. But people do say that Gary isn't

2736
02:35:43,840 --> 02:35:48,480
an expert in deep learning and that he's, you know, attention seeking. What would you say to that?

2737
02:35:48,480 --> 02:35:57,360
No, he's not an expert in deep learning. And so like, I agree with some of his criticisms,

2738
02:35:57,360 --> 02:36:03,040
I disagree with others. Probably on balance, I disagree more with him than I agree. But

2739
02:36:03,680 --> 02:36:07,840
first of all, there is a value to having critics like that, number one. But then number two,

2740
02:36:08,800 --> 02:36:12,880
the reason his criticism, I mean, it would be better if he was also an expert in deep learning

2741
02:36:12,880 --> 02:36:17,440
and made the same criticisms. And then the problem is that often his criticisms are wrong

2742
02:36:17,440 --> 02:36:22,800
because he has a mental model of deep learning that is already outdated, or is oversimplified,

2743
02:36:22,800 --> 02:36:27,440
right? But that to some degree is unavoidable. But the thing that makes his criticism valuable

2744
02:36:27,440 --> 02:36:32,800
is that he's doing it at a level where on a good day, on a bad day, his criticisms miss the mark.

2745
02:36:32,800 --> 02:36:37,200
But on a good day, which is the ones that matter, his criticism is actually useful because it's

2746
02:36:37,200 --> 02:36:41,920
at a level where you don't need to understand the details. It's like, you claim to be producing

2747
02:36:41,920 --> 02:36:47,440
intelligence. I as a psychologist know a lot about intelligence. That's what I study for a living,

2748
02:36:47,440 --> 02:36:52,800
right? He knows more about aspects of intelligence than I do. Yeah. And from that point of view,

2749
02:36:52,800 --> 02:36:57,840
what you're doing is lacking. And that I mean, like, he's written the whole books about, you know,

2750
02:36:57,840 --> 02:37:02,000
again, because this goes back to when he was a PhD student and, you know, and symbolic learning and

2751
02:37:02,000 --> 02:37:08,480
whatnot, there are very, you know, the deep learning folks have repeatedly underestimated

2752
02:37:08,480 --> 02:37:13,200
how well he understands some of these problems. Because as a psychologist in particular interested

2753
02:37:13,200 --> 02:37:18,560
in language learning, he's actually thought very long and hard about them. Oh, I know. So I've

2754
02:37:18,560 --> 02:37:23,520
read his book and we've had him on the show three times. Which book? The algebraic mind.

2755
02:37:23,520 --> 02:37:27,280
Yeah. So that's the most relevant one here. Yeah. As a psychologist, you know, he spent a lot of

2756
02:37:27,280 --> 02:37:31,600
time studying how children learn rules. Right. And he talks very elegantly about a

2757
02:37:31,600 --> 02:37:35,760
compositionality. And we've spoken about this. It's irrefutable. And I agree with him and we've

2758
02:37:35,760 --> 02:37:42,400
supported him. I guess some of the things he argues are based on ethics, politics and virtue.

2759
02:37:42,400 --> 02:37:48,320
And some of the things like compositionality, I think are irrefutable. I mean, I think irrefutable

2760
02:37:48,320 --> 02:37:52,480
is a very strong word. I wouldn't say that they're irrefutable. I would say that they have,

2761
02:37:53,440 --> 02:37:59,120
they have very strong backing, which the connectionists have not been able to effectively

2762
02:37:59,120 --> 02:38:04,880
refute. But some of the criticisms that they have, you know, meaning people like Pinker and Prince

2763
02:38:04,880 --> 02:38:10,320
and whatnot, famously of connectionists in the 80s, some of them are still valid, which is very

2764
02:38:10,320 --> 02:38:15,440
salient. But some of them not really. And again, to go back to the daddy of this whole school of

2765
02:38:15,440 --> 02:38:20,880
thought, who's Chomsky, right? His, you know, he made his name basically panning things like,

2766
02:38:20,960 --> 02:38:25,920
you know, Markov models of language in Graham models, which he could say large language models

2767
02:38:25,920 --> 02:38:31,440
are just a very glorified version of, right? But and at the time, you could, that criticism was

2768
02:38:31,440 --> 02:38:37,120
very apt and, you know, and timely and it was useful, right? But, but, but, and famously, it's

2769
02:38:37,120 --> 02:38:40,880
like, it's like, you can't learn a context free grammar, but context free grammar is what you

2770
02:38:40,880 --> 02:38:46,160
do. Well, actually, now we know formally that you can learn a context free grammar. And, and,

2771
02:38:46,160 --> 02:38:50,080
you know, because you only have to learn it probabilistically, which is what we do. And

2772
02:38:50,160 --> 02:38:55,920
what our systems do. So his criticism was just, you know, mathematically off the mark. But also,

2773
02:38:55,920 --> 02:39:01,360
when you look at systems that do speech language, et cetera, et cetera, it is that statistical

2774
02:39:01,360 --> 02:39:06,320
approach that he made his name panning that has prevailed. And for reasons that we understand

2775
02:39:06,320 --> 02:39:11,040
very well, and large language models are just the latest greatest expression of that. So at that

2776
02:39:11,040 --> 02:39:16,880
level, a whole Chomsky and Pinker, Gary Marcus view of things, not only is it not irrefutable,

2777
02:39:16,880 --> 02:39:23,600
it has been refuted. Okay. Let's just quickly come back to your definition of intelligence. So

2778
02:39:23,600 --> 02:39:29,520
solving NP hard problems, I assume you would zoom out a little bit and, you know, it's more of a

2779
02:39:29,520 --> 02:39:33,760
meta learning algorithm. So the ability to sell to sell different problems.

2780
02:39:36,160 --> 02:39:43,920
Yes. So it's, if very good point, if all you have is the ability to solve one NP complete problem,

2781
02:39:43,920 --> 02:39:48,880
that does not qualify as general intelligence, right? There's like, these days, this is a common

2782
02:39:48,880 --> 02:39:53,920
definition to make this different difference between, you know, narrow intelligence and general

2783
02:39:53,920 --> 02:39:58,720
intelligence and AGI and whatnot, right? And if you only solve one NP complete problem very well,

2784
02:39:58,720 --> 02:40:02,800
you have narrow intelligence is the way I would put it, but you do not have general intelligence.

2785
02:40:02,800 --> 02:40:08,320
General intelligence is precisely the ability to solve a limitless variety of problems, all that

2786
02:40:08,320 --> 02:40:13,600
have this characteristic of they're hard to solve, but the solution is easy to check. Right? I mean,

2787
02:40:13,600 --> 02:40:17,360
if you have the ability to solve problems, whose solution isn't easy to check, then maybe you're

2788
02:40:17,360 --> 02:40:23,360
intelligent, but I can't decide whether intelligent or not. Interesting. Okay. And actually, Gary did,

2789
02:40:23,360 --> 02:40:27,440
he put a paper about 20 years ago talking about how neural networks can't extrapolate. I think it

2790
02:40:27,440 --> 02:40:33,040
was when he encoded numbers with a binary encoding or whatever. And we've been on a bit of a journey

2791
02:40:33,040 --> 02:40:37,360
on this. So we had Randall Bellistrier, I've interviewed him yesterday, he's got this paper

2792
02:40:37,360 --> 02:40:42,640
called the spline theory of neural networks. It basically says that a neural network decomposes

2793
02:40:42,720 --> 02:40:46,960
an input space into these input activated polyhedra. And when we first read that,

2794
02:40:46,960 --> 02:40:52,640
we felt that it kind of indicated Francois Chollet's assertion that neural networks are

2795
02:40:52,640 --> 02:40:57,600
locality sensitive hashing tables, and they only generalize within, you know, these tiny

2796
02:40:57,600 --> 02:41:03,360
polyhedra. And Randall's now updated this view to say in contrast to decision trees, these

2797
02:41:03,360 --> 02:41:08,480
hyperplanes, they actually inform a lot of information in the extrapolative regime outside

2798
02:41:08,480 --> 02:41:13,280
of the training range. So I always thought it was the inductive priors that gave the extrapolative

2799
02:41:13,280 --> 02:41:18,480
performance on neural networks by photocopying the information everywhere. And so like, you know,

2800
02:41:18,480 --> 02:41:23,040
this is a great example of where, you know, Gary might update his views because even basic MLPs

2801
02:41:23,040 --> 02:41:28,720
are far more extrapolative than anyone realized. This is a very interesting question. But the

2802
02:41:28,720 --> 02:41:34,160
way I would put it is that in that regard, in some sense, both of the sides are right.

2803
02:41:34,880 --> 02:41:38,400
And the reason they're both right is that we're in very high dimensional spaces.

2804
02:41:38,400 --> 02:41:42,800
Yeah. And we're in a very high dimensional space. The follow thing can happen, which is,

2805
02:41:42,800 --> 02:41:48,880
you know, you have a data point, and you generalize to a vast region around that data point. And it's

2806
02:41:48,880 --> 02:41:54,000
unfair to characterize these things as saying they just interpolate. In some sense, they really do

2807
02:41:54,000 --> 02:42:00,080
extrapolate. But at the same time, that vast region that they generalize correctly to is an

2808
02:42:00,080 --> 02:42:05,040
infinitesimal fraction of the much, much vaster reason that they have not generalized to but you

2809
02:42:05,040 --> 02:42:11,280
and I can. So you got to keep that distinction in mind. And then in particular, right, I like to

2810
02:42:11,280 --> 02:42:17,760
say that deep learning is nearest neighbor in curved space. And both parts of that are very

2811
02:42:17,760 --> 02:42:25,520
important, right? So, you know, Jan Lacoon was famous, you know, during the glory days of kernel

2812
02:42:25,520 --> 02:42:31,040
machines for saying that kernel machines are just glorified template matches. Right. And of

2813
02:42:31,040 --> 02:42:34,720
course, they didn't earn him any friends, but he was right. They really are just glorified template

2814
02:42:34,720 --> 02:42:39,440
matches. Kernel machine is really a souped up, more mathematically elegant and blah, blah,

2815
02:42:39,440 --> 02:42:43,840
blah version of nearest neighbor. Right. And the nearest neighbor is just a template matcher.

2816
02:42:44,400 --> 02:42:48,400
The beauty in the power of nearest neighbor, though, is that there is a neighborhood within

2817
02:42:48,400 --> 02:42:54,080
which often it generalizes very well. Right. Now, I think what Jan was missing, and I probably

2818
02:42:54,080 --> 02:43:00,960
still is, is that coordinates and deep learning, they are still just a glory. They are also glorified

2819
02:43:00,960 --> 02:43:06,400
nearest neighbor, except more glorified. And the way in which they're more glorified, which is

2820
02:43:06,400 --> 02:43:10,960
very important is that they are doing nearest neighbor in curved space. They are still just

2821
02:43:10,960 --> 02:43:15,760
doing, you know, generalization by similarity, which you could argue is all that machine learning

2822
02:43:15,760 --> 02:43:20,800
does is generalizing by similarity. Another notion of similarity can vary. Right. But the

2823
02:43:20,800 --> 02:43:24,080
important thing that they've done is that nearest neighbor just uses some distance

2824
02:43:24,080 --> 02:43:29,120
measured in the original space, whereas the neural networks are warping the space to make

2825
02:43:29,120 --> 02:43:34,880
the problem easier for the nearest neighbor, you know, essentially dot product based similarity

2826
02:43:36,160 --> 02:43:40,080
computation that they're actually doing. Oh, sure. But you're very much arguing,

2827
02:43:40,080 --> 02:43:43,120
this is the way Francois Chouelet puts it, that, you know, you have all of these

2828
02:43:44,080 --> 02:43:50,080
transformations and you kind of distort the space, you know, to represent the data manifold. And,

2829
02:43:50,080 --> 02:43:54,720
you know, you want it to, you stop SGD at the right time so that you approximate the data

2830
02:43:54,720 --> 02:43:59,600
manifold and you can do this kind of latent space, you know, interpolation on the geodesic of that

2831
02:43:59,600 --> 02:44:04,800
manifold. But, you know, Randall's idea is completely away from that idea of, you know,

2832
02:44:04,800 --> 02:44:12,160
these models learning this curved space. And so if you do slice the space up with these hyperplanes,

2833
02:44:12,160 --> 02:44:16,160
rather than it being a locality prior, which is what you're talking about, these hyperplanes give

2834
02:44:16,160 --> 02:44:21,360
you globally relevant information to things that are, you know, miles away from the training data.

2835
02:44:21,360 --> 02:44:29,040
Yeah, so, but these two perspectives are more similar than you might think, because I can take

2836
02:44:29,680 --> 02:44:35,680
a distorted version of space and decompose it into polyhedron, right? And one or the other might

2837
02:44:35,680 --> 02:44:39,920
approximate what's really going on better. I mean, these neural networks do form curved spaces,

2838
02:44:39,920 --> 02:44:43,840
except they're in practice, they're not curved because they find it, but ignoring that, right?

2839
02:44:44,800 --> 02:44:52,080
When, let me put it this way, an eloquent example of this is if you look back at the original space,

2840
02:44:52,080 --> 02:44:58,000
right? Again, treat this thing as a black box. Where does it generalize to? Does it generalize

2841
02:44:58,000 --> 02:45:03,280
only to things, neural networks as we have them today? Does it generalize correctly only to things

2842
02:45:03,280 --> 02:45:10,640
that are locally near the data point, or you can generalize well to things that are far, right?

2843
02:45:10,640 --> 02:45:15,760
And the thing is that with nearest neighbor, you buy, you know, almost intrinsically, you only

2844
02:45:15,760 --> 02:45:21,840
generalize period at all to things that are local. The beauty of deep learning and of the

2845
02:45:21,840 --> 02:45:26,000
space swapping that's going on is, again, going back to this notion of the path kernel is that

2846
02:45:26,000 --> 02:45:30,160
you're actually doing a nearest neighbor computation, not just in a space that's swapped,

2847
02:45:30,160 --> 02:45:34,240
but you're doing it in the space of gradients, which actually means that you can generalize

2848
02:45:34,240 --> 02:45:39,680
correctly to things that are very far from your examples, except they look similar in gradient

2849
02:45:39,680 --> 02:45:45,600
space. A very simple example of this is a sine wave, right? If I try to learn a sine wave using

2850
02:45:45,600 --> 02:45:50,880
nearest neighbor, I need an infinite number of examples, right? Because, you know, like what

2851
02:45:50,880 --> 02:45:55,840
I've learned over here helps me not at all with the next turn of the sine wave, like that continuous

2852
02:45:55,840 --> 02:46:00,400
extrapolation, right? At some point, there's this disaster where if the last piece of the

2853
02:46:00,400 --> 02:46:05,040
sine was going up, I just keep going up and getting more and more wrong, right? And in fact,

2854
02:46:05,040 --> 02:46:09,520
this kind of thing does happen in neural networks, but they also have the part to say like, and this

2855
02:46:09,600 --> 02:46:15,760
again, this also happens, which is I'm going to transform this space more into a more intelligent

2856
02:46:15,760 --> 02:46:22,320
one, which is the space of the slopes, right? And now if I've seen one cycle of the sine wave

2857
02:46:23,120 --> 02:46:29,680
with some density of examples, by similarity in that transformed space, I generalize correctly

2858
02:46:29,680 --> 02:46:35,760
and trivially to every other turn of the sine wave. So there's a very big fundamental difference

2859
02:46:35,840 --> 02:46:39,600
between the two. Interesting. And you think with an MLP, it would be possible to have that kind

2860
02:46:39,600 --> 02:46:44,240
of extrapolative generalization on a sine wave? Well, so people have studied this in multiple

2861
02:46:44,240 --> 02:46:51,840
ways. And the problem, so the question is, it depends on what are the basis functions that it's

2862
02:46:51,840 --> 02:46:57,520
using. Yes. So something that we didn't allude to at all in this conversation, but analyze all of

2863
02:46:57,520 --> 02:47:01,840
this is like, what is your choice of basis functions, right? And the thing is, an MLP with

2864
02:47:01,920 --> 02:47:06,560
the traditional, say, sigmoid or allude basis functions will not learn this, no matter, for

2865
02:47:06,560 --> 02:47:11,040
obvious reasons, right? And again, you can represent it, right? The representative theorem is there,

2866
02:47:11,040 --> 02:47:15,120
like the sine wave is just one sigmoid and then another one, you know, with a minus sign and

2867
02:47:15,120 --> 02:47:21,120
then another one, but the data doesn't let you learn it. If as a basis function, you have sine

2868
02:47:21,120 --> 02:47:24,960
waves, which is nothing unimaginable, that's what a Fourier transform is then, then you can learn

2869
02:47:24,960 --> 02:47:30,640
it so easily, it's not even funny. So it depends dramatically on the basis function. And the

2870
02:47:30,640 --> 02:47:35,360
question really becomes, what are the basis functions and the architect that let me generalize

2871
02:47:35,360 --> 02:47:40,240
correctly to a lot of things, including this, such that, for example, and this is a very simple test,

2872
02:47:40,240 --> 02:47:46,160
is like, I can nail a sine wave with a small number of examples without it being one of my basis

2873
02:47:46,160 --> 02:47:50,080
functions. Yeah, exactly. And then this all comes back to, you know, we're talking about inductive

2874
02:47:50,080 --> 02:47:54,160
prize and the bias variance trade off and even symmetries, actually. I mean, the Taco Cohen once

2875
02:47:54,160 --> 02:47:59,680
said that, you know, if you encode all of the symmetries into the label function, then you would

2876
02:47:59,680 --> 02:48:04,480
only need one labeled example. So it's always a trade off between how much induction are you doing?

2877
02:48:04,480 --> 02:48:09,680
Well, interesting, you should say that I understand why he says that and it's, and it's not technically

2878
02:48:09,680 --> 02:48:15,760
wrong. But I would say that practically what you need is such a set of symmetries per region of

2879
02:48:15,760 --> 02:48:22,160
the space, per cluster, right? But, you know, in another way, I would actually make an even

2880
02:48:22,160 --> 02:48:28,080
stronger statement, which again, is very perfectly mathematical, sounds same when you say, an object

2881
02:48:28,080 --> 02:48:34,240
is just the sum of its symmetries or a function. If you tell me all the symmetries, every last one

2882
02:48:34,240 --> 02:48:40,320
of an object, you've defined the object. So if I can learn the symmetries at that level, I don't

2883
02:48:40,320 --> 02:48:44,640
need anything else. Of course, as we already discussed, that's not the whole answer. Likewise,

2884
02:48:44,640 --> 02:48:50,000
with any function, if you tell me all the properties of the function, there are there, you know,

2885
02:48:50,560 --> 02:48:54,160
to be more precise, all the symmetries of a function at some point, you've told me the whole

2886
02:48:54,160 --> 02:48:58,640
function. And vice versa, from the function, I can, you know, I can read out all the symmetries

2887
02:48:58,640 --> 02:49:02,960
that it has. In principle, doing that in practice can be, you know, a very difficult and subtle

2888
02:49:02,960 --> 02:49:07,120
thing to do. That's a beautiful thing to say. You give me the symmetries and I'll give you the

2889
02:49:07,120 --> 02:49:11,440
object. Yeah, exactly. Amazing. Professor Pedro Domingos, thank you so much for joining us today.

2890
02:49:11,440 --> 02:49:13,840
It's been an honor. Thanks for having me. Amazing.

