{"text": " my fondest memory. It's usually when I discover something that I think nobody has seen before, but that happens very rarely because most of the things you think of somebody else has done before. This episode is sponsored by Numeri. Are you a data scientist looking to make a real-world impact with your skills? Do you love competing against the best minds in the world? Well, introducing Numeri, the revolutionary cutting-edge AI-driven hedge fund that's changing the game for good. Numeri combines a competitive data science tournament with powerful, clean stock market data enabling you to predict the market like never before. Sign up now, become part of the elite community, taking the stock market by storm and I'll see you on the leaderboard. Wonderful. So today is a momentous occasion. What an episode of MLST we're going to have. We're joined not by a godfather of AI, but the father of AI, you again, Schmidhuber, the researcher responsible for leading the research groups which invented much of the technology which has powered the deep learning revolution. It's long been a dream to get you on the podcast, you again. It feels like the day has finally arrived, so welcome to MLST. Thank you, Tim, for these very kind words and this very generous introduction. So on that, let's discuss the credit assignment problem in machine learning. Now, you've dedicated a significant amount of time researching and publishing the actual history of the field and there's a significant divergence between the public narrative and what actually happened. And amazingly, no one has pointed out any factual inaccuracies in your accounts, but the incorrect perceptions still persevere. Now, I particularly enjoyed reading your history of the breakthroughs in machine learning, going back to ancient times and of course even remarking on the very first computer scientist, Leibniz. And for example, you pointed out the history of who invented backprop and the CNN. And you explained that there wasn't really a neural network winter at all in the 1970s. So could you just sketch out a little bit of that history? So that's a challenge. Actually, computer science history and computing history started maybe 2000 years ago when Heron of Alexandria built the first program-controlled machine. That was 2000 years ago in the first century basically. And he basically built an automaton that was programmed through a cable which was wrapped around a rotating cylinder which had certain knobs and then there was a weight which pulled it down and the whole apparatus was able to direct the movements of little robots, of little puppets in an automatic theater. That, as far as I know, was the first program-controlled machine in the history of mankind. Even before that there were other machines. The ancient Greeks had even earlier the Antiqueterre mechanism which was kind of a clock, an astronomical clock. But then more recently we have seen many additional advances and you mentioned Leibniz, of course, who is of special interest to our field because he not only is called the first computer scientist because he had the first machine with a memory that was in the 1680s, I think. He not only had the first machine that could do all the basic arithmetic operations which are addition, multiplication, division and subtraction, then he not only had these first ideas for a universal problem solver that would solve all kinds of questions, even philosophical questions, just through computation. And he not only was the first who had this algebra of thought which is deductively equivalent to the much later Boolean algebra. In many ways he was a pioneer, but especially in our field in deep learning he contributed something essential, which is really central for this field, which is the chain rule. I think 1676, that's when he published that and that's what is now being used to train very deep artificial neural networks and also shallow neural networks and recurrent neural networks. And everything that we are using in modern AI is really in many ways depending on that early work. But then of course there was so much additional work. The first neural networks, as we know them, they came up about around 1800. That's when Gauss and Legendre had the linear neural networks, the linear perceptrons in the sense that they were linear without having any non-differential aspect to it. So these first neural networks, back then, were called method of least squares. And the training method was regression and the error function was exactly the same that we use today. And it was basically just a network with a set of inputs and a set of outputs and a linear mapping from the inputs to the outputs. And you could learn to adjust the weights of these connections. So that was the first linear neural network and many additional later developments led to what we have today. You had this beautiful statement. You said that machine learning is the science of credit assignment and we should apply that same science to the field itself. And I guess what I'm really curious about is first, if you could educate our listeners just a bit on what credit assignment is in the context of, say, machine learning and why you think it's important that that should apply to the field in general. You know, why should we care about credit assignment? Why should we study the history of the developments in the field? Why is it important? I'm interested in credit assignment, not only in machine learning, but also in the history of machine learning, because machine learning itself is the science of credit assignment. What does that mean? Suppose you have a complicated machine, which is influencing the world in a way that leads to the solution of a problem. And maybe the machine solves the problem. But then the big question is, which of the components of these many components were responsible? Some of them were active a long time ago and others later and early actions set the stage for later actions. Now, if you want to improve the performance of the machine, you should figure out how did the components contribute to the overall success. And this is what credit assignment is about. And in machine learning in general, we have a system consisting of many machine learning engineers and mathematicians and hardware builders and all kinds of people. And there you also would like to figure out which parts of the system are responsible for later successes. Yeah, and it's a brilliant point. And I completely agree with you, by the way. And I think the way I think about it is you've got this giant architecture of humanity and in it are these certain nodes that may be an individual, maybe a research group. And if they come up with things that are very helpful, right, you want to try and direct more attention, more resources, at that nodule, at that node, right, because it's likely to come up with additional very important things. And if we don't get that right, we're just not optimizing the algorithm of science as a whole. That's right, yes. Machine learning and science in general is based on this principle of credit assignment where credit usually doesn't come in form of money, sometimes also in form of money, but in form of reputation. And then the whole system is set up such that you create an incentive for people who have worked on improving some method to credit those who maybe came up with the original method and to just have these chains of credit assignment that make clear who did what, when, because the whole system is based on this incentive. And yes, those who are then credited with certain valuable contributions, they also can get reasonable jobs within the economy and so on. But that's more like the secondary consequence of the basic principle. And that's why all PhD advisors teach their PhD students to be meticulous when it comes to credit assignment to past work. So one last question, if I may, I've really enjoyed studying the history of advancement because I found that when I go back and read original source materials, let's say Einstein's first paper on diffusion or anything like that, because they're breaking new ground, they're considering a wider array of possibilities. And then over time, the field becomes more and more focused on a narrower avenue of that. And you can go back and look at the original work and actually gain a lot of inspiration for alternative approaches or alternative considerations. So in a sense, it's kind of in the sense of forgetting is as important as learning. Sometimes we need to go back to go down a different branch of the tree, if you will, and expand the breadth of the search a little bit. I'm curious if you've noticed that same phenomenon. Yes, science in general is about failure. And 99% of all scientific activity is about creating failures. But then you learn from these failures and you do backtracking. And you go back to a previous decision point where you maybe made the wrong decision and pursued the wrong avenue. But now you have a branching point and you pursue an alternative. And in a field that is rapidly moving forward, you don't go back very far usually. You just go back to a recent paper which came out five months ago. And maybe you have a little improvement there. And then maybe there's yet another little improvement there. And some parts of our field are at the moment a little bit like that, where PhD students are moving in, who just look at the most recent papers and then find a way of improving it a little bit and 2% better results on this particular benchmark. And then the same guys are also reviewing at major conferences, papers by similar students and so on. And so then sometimes what happens is that no very deep backtracking is happening, just because the actors aren't really aware of the entire search tree that has already been explored in the past. On the other hand, science has this way of healing itself. And since you can gain reputation by identifying maybe more relevant points, branching points, you have this incentive within the whole system to improve things as much as you can, sometimes by going back much further. So there's been a lot of discussion in the discourse around this concept of AI existential risk. And you again, you've published quite a few pieces about this recently, prominently in The Guardian and in Forbes actually. And one of the things I wanted to focus on is this concept of recursive self-improvement, because that seems to be one of the plausible explanations that these folks give. And of course, when it comes to recursive self-improvement, you are an expert in this field. I mean, Godel machines come to mind immediately. So I want to kind of explore asymptotes and limitations. This whole idea of recursive self-improvement is very sexy, isn't it? In fact, it is the one idea that motivated me to do all of this. So my first paper ever in 1987, that was my diploma thesis. And it was about this recursive self-improvement thing. So it was about machine that learns something in a domain. But not only that, it also learns on top of that to learn a better learning algorithm based on experience and the lower level domains. And then also recursively learns to improve the way it improves the way it learns. And then also recursively learns to improve the way it improves the way it improves the way it learns. And yeah, I called that meta-learning. And back then, I had this hierarchy with, in principle, infinite self-improvement in the recursive way, although it is always limited by the limited time that you run the system like that. And then, of course, the motivation behind that is that you don't want to have an artificial system that is stuck always with the same old human-designed learning algorithm. No, you want something that improves that learning algorithm without any limitations, except for the limitations of physics and computability. And so much of what I have been doing since then is really about that. Self-improvement in different settings where you have, on the one hand, reinforcement learning systems that learn in an environment to better interact and better create ways of learning from these interactions to learn faster and to learn to improve the way of learning faster, and so on. And then also gradient-based systems, artificial neural networks, that learn through gradient descent, which is a pre-wired human-designed learning algorithm, to come up with a better learning algorithm that works better in a given set of environments than the original human-designed one. And yeah, that started around 1992 neural networks that learned to run their own learning algorithms on the recurrent network themselves. So you have a network which has standard connections and input units and output units, but then you have these special output units which are used to address connections within the system, within this recurrent network, and they can read and write them. And suddenly, because it's a recurrent network and therefore it is a general-purpose computer, suddenly you can run arbitrary algorithms on this recurrent network, including arbitrary learning algorithms that translate incoming signals, not only the input signals, but also the evaluation signals like reinforcement signals or error signals into weight changes, fast weight changes, where the weight changes are not dictated any longer through this gradient descent method, but no, now the network itself is learning to do that. But the initial weight matrix is still learned through gradient descent, which is propagating through all these self-referential dynamics in a way that improves the learning algorithm running on the network itself. That was 1992, and back then, compute was really, really slow, it was a million times more expensive than today, and you couldn't do much with it. But now, in recent works, all of that is working out really nicely and has become popular, and we have, just if you look at the past few years, a whole series of papers just on that. So that's the fast weight programming that you're referring to? Yes, so it's fast weight programmers where you have a part of the network that learns to quickly reprogram another part of the network, or the original version of that was actually two networks, so one is a slow network, and then there's another one, a fast network, and the slow network learns to generate weight changes for the second network, and the program of the second network are its weights. So the weight matrix of the second network, that is the program of the second network, and the first one, what does it do? It generates outputs, it learns to generate outputs that cause weight changes in the second network, and these weight changes are being applied to patterns, to input patterns, to queries, for example, and then the first network essentially learns to program the second network, and essentially the first network has a learning algorithm for the second network, and the first system of that kind, 1991, that was really based on on keys and values, so the first network learns to program the second network by giving it keys and values, and it says now take second network, take this key and this value, and associate both of them through an outer product, which just means that those units are strongly active, they get connected through stronger connections, and the mathematical way of describing that is the outer product between key and value. So that's how the first network would program the second network, and the important thing was that the first network had to invent good keys and good values, depending on the context of the input stream coming in, so it used the context to generate what is today called an attention mapping, which is then being applied to queries, and this was a first step right before the most general next step, which is then really about learning a learning algorithm running on the network itself for the weights of the network itself. Could I press you a tiny bit on this concept of meta-learning and convergence and asymptotes? Now one of the reasons I think why the X-Risk people believe that it will just go on forever is they believe in this idea of a pure intelligence, one that doesn't have physical limitations in the real world, and I'm quite amenable to this ecological idea of intelligence that it does, the world is a computer basically as well as the actual brain that we're building, so surely it must hit some kind of asymptote. Do you have any intuition on what those limitations would be? So you are talking about the ongoing acceleration of computing power and limitations thereof, is that what you have in mind here? Well that's one part of it, so even if you just scale transformers I think there would be some kind of asymptote, but we're talking here about meta-learning, learning to learn, how to learn, and recursive self-improvement, and it's similar to this idea of reflection, self-reflection and language models, it actually improves the performance with successive steps of reflection and then it levels off, it reaches an asymptote. I just believe that there are asymptotes everywhere and that's the reason why I don't think recursive self-improvement will go on forever, but I just wondered if you had any intuitions on what those impressions are. Yeah, you are totally right, there are certain algorithms that we have discovered in past decades which are already optimal in a way such that you cannot really improve them any further, and no self-improvement and no fancy machine will ever be able to further improve them. There are certain sorting algorithms that under given limitations are optimal and you can further improve them. That's one of the limits. Then of course there are the fundamental limitations of what's computable, first identified by Kurt G\u00f6del in 1931, he just showed that there are certain things that no computational process can ever achieve. No computational theorem prover can prove or disprove certain theorems in a language, in a symbolic language that is powerful enough to encode certain simple principles of arithmetic and stuff like that. What he showed was that there are fundamental limitations to all of computation and therefore there are fundamental limitations to any AI based on computation. I'm glad you brought that topic up because it's one of our favorite things to discuss which is do you think the human mind ultimately reduces to just an effective computation and so subject to those same limits or do you think there's any known or unknown physics that give us some out in which the brain can do a computation that amounts to hypercomputation? Since we have no evidence that the brain can compute something that is not computable in the traditional sense, in G\u00f6del sense and torings and churches sense and everybody who has worked on this field, since we have no evidence we shouldn't assume that's the case. As soon as someone shows that people can compute certain things or prove certain theorems that machines cannot prove given the same initial conditions, we should look more closely but there are many things that might be possible in fairy tales and we are not really exploring them because the probability of coming up with interesting results is so low. Fair enough, so you mentioned so far two asymptotes, one being of the mathematical kind where there's just mathematical proofs that certain things are optimal, the other one being the limits of computation itself. What other asymptotes do you see applying to or putting bounds on recursive self-improvement? The most obvious thing is probably light speed and the limits of physical computation. We know those for several decades, we have happily enjoyed the fact that every five years compute is getting 10 times cheaper and this process started long before Moore's law was defined in the 60s I believe because even in 1941 already when Susie built the first program controlled computer this law apparently was active so back then he could compute maybe one instruction per second and since then every 10 years a factor of 100 every 30 years a factor of a million more or less until today and there's no reason to believe it won't hold for a couple of additional decades because the physical limits are much further out. The physical limits that we know are the Bremermann limit discovered I think in 1983 by Bremermann and they basically say that one kilogram of matter cannot compute more than 10 to the 51 instructions per second. So that's a lot of compute but it's limited and to give you an idea of how much compute that is I also have a kilogram of computer in here and probably it cannot compute 10 to the 20 instructions per second otherwise my head would explode because of the heat problem but maybe it can compute something that is not so far from 10 to the 20 instructions maybe 10 to the 17 something like that although most of my neurons are not active as we speak because again otherwise my head would just evaporate. Now if you have an upper limit of 10 to the 20 instructions per brain then the upper limit of all of humankind would be 10 billion times that individual limit and that would be 10 to the 30 instructions per second and you see it's still far away from the 10 to the 51 instructions per second that in principle one kilogram of matter could compute and now we have more than 10 to the 30 kilograms of matter in the solar system and there's some and so if the current trend continues at some point much of that is going to be used for computation but then it will have to slow down even if the exponential acceleration will still be with us for a couple of decades because at some point it is going to be a polynomial because due to the limits of light speed at some point it will be harder and harder to acquire additional mass once you have reached the limits of physical computation per kilogram the only way to expand is to go outwards and you know find additional stars and additional matter further away from the solar system and then you will get a polynomial acceleration or a polynomial growth at best so it will be much worse than the current exponential growth that we are still enjoying. Sure but I would say you know the existential threat that is more than sufficient to supply an existential threat and let me just put this a little bit differently which is and I agree with you on this which is you are quoted as saying that traditional humans won't play a significant role in spreading intelligence across the universe and I think you are right I think we kind of share a vision of something like the von Neumann probes that go out into space and form this star spanning civilization of machines and artificial intelligence that have transcended you know biological limitations so I guess my question to you is once that space faring star spanning you know civilization exists if it becomes misaligned with us and decides that we are in the way right isn't that an existential threat I mean might they just you know repurpose the earth regardless of whether we're here or not for for their own aims yeah I'm often getting these questions and and there is no proof that we will be safe forever or something like that on the other hand it's also very clear as far as I can judge that all of this cannot be stopped and it can be channeled in a very natural and I think good way in a way that is good for humankind now first of all at the moment we have a tremendous bias towards good AI meaning AI that is good for humans why because there is this intense commercial pressure to create stuff that humans want to buy and they like to buy only stuff they think is good for them which means that all the companies that are and that are trying to devise AI products they are maximally incentivized to generate AI products that are good for those guys who are buying them or at least where the where the customers think it's good for them so it is still 95 so it may be five percent of all AI researchers really about AI weapons and one has to be worried about that when all this has to be worried about weapons research but there's a tremendous bias towards good AI so that is one of the reasons why you can be a little bit optimistic for the future I'm always trying to point out the two types of AIs there are those who are just tools of users human human users and the others that invent their own goals and they pursue their own goals and both of them we have had for a long time now for the AI tools it's kind of clear there's a human and a human wants to achieve something and so it uses he uses or she uses that tool to achieve certain ends and and most of those are of the type let's improve healthcare and let's facilitate translation from one person to another one in another nation and just make life easier and make human lives longer and healthier okay so that that's the AI tools but then there are the other AIs which also have existed in my lab for at least 32 years which invent their own goals and they are a little bit like little scientists where you have an incentive to explore the environment through actions through experiments self-invented experiments that tell you more about how the world works such that you can become a better and better and more and more general problem solver in that world and so these AIs they have for a long time created their own goals and now of course the interesting question is these more interesting AIs what are they going to do once they are once they have been scaled up and can compete or maybe outperform humans and everything they want to achieve so on the one hand the AI tools and there the greatest worry is what are the other humans going to do to me with their AI tools so in the extreme case you have people who are using AI weapons against you and maybe your neighbor is has bought a little drone for 300 dollars and it has face recognition and it has a little gripper and it flies across the hedge and puts some poison into your coffee or something like that so then the problem is not the AI which is trying to enslave humans or something silly like that no it's your neighbor or the other human and generally speaking you have to be much more afraid of other humans than you have to be of AIs even those who define or set themselves their own goals because you must mostly worry about those with whom you share goals so if you share goals then suddenly there is a potential of conflict because maybe there is only one schnitzel over there and two persons want to eat the schnitzel and suddenly they have a reason to fight against each other generally speaking if you share goals then you can do two things you can either collaborate or compete an extreme form of collaboration would be to maybe marry another person and set up a family and master life together and an extreme form of competition would be war and and those who share goals they have many more incentives to interact than those who don't share goals and so humans are mostly interested in other humans because they share similar goals and because they give them a reason to collaborate or to compete most CEOs of certain companies are interested in other CEOs of competing companies and five-year-old girls are mostly interested in other five-year-old girls and the super smart AIs of the future who set themselves their own goals they will be mostly interested in other super smart AIs of the future who set themselves their own goals generally speaking there is not so much competition and there are not so many shared goals between biological beings such as humans and a new type of life that as you mentioned can expand into the universe and can multiply in a way that is completely infeasible for biological beings so there's a certain long-term protection at least through lack of interest on the other side okay brilliant there's a few things I wanted to touch on there we will get on to what it means for goals to emerge from systems later and you started off by saying that humans will buy products that make them feel good and Facebook is quite an interesting example to play with actually because Facebook is a little bit like an AI system which is a collective intelligence and humans use Facebook but they have some idea that it might cause them harm and the thing with population ethics is we know that our moral reasoning kind of decays over space and even more so over time and part of the reason why time is so difficult is because it's predictive we don't actually know what's going to happen in the future so our kind of reasoning about establishing what the value of something is is very very faulty and I think that's one of the reasons why these people would say that we don't really know what's good for us I do completely agree with you though that the problem I think is humans rather than AIs on their own yes these are good points feel free to uh offer some thoughts yes I mean it it would that's a whole separate discussion isn't it when you discuss the limitations of what's predictable and um and how people often fail to see what's good for them well I think maybe so you've already you've already um said that there's no proof that we'll be safe forever right like I mean there could there could come an existential risk you know from AI so I think my question to you is do you have sympathy for the folks who say we need to be putting more resources into researching alignment like we need to develop the tools um in order to allow it to be easier for people to construct AI that is aligned for the goals and to make sure that you know that it doesn't that it doesn't have unintended consequences like in other words there may not be a proof that we can go forever and be safe for AI but we at least want to develop the basic mechanics that we need to safely develop and deploy AI don't we yes and I sympathize with those who um are devoting their lives to alignment issues and trying to build AIs aligned with humans I view them as part of the evolution of all kinds of other ideas that come up as not only nations compete with other nations but companies compete with other companies and shareholders of different companies compete with shareholders of different companies and so on and so there is such a huge set of different human goals which are not aligned with each other that makes me doubt that you will come up with a general system that all humans can accept simply because if you put 10 humans in a room and ask them what is good they will give you 10 different opinions however I sympathize with with this goal and it's good that people are worried and they spend resources on solving some of these issues in the long run however I think there is no way of stopping all kinds of AIs from having all kinds of goals that have very little to do with humans the universe itself is built in a certain way that apparently derives it from very simple initial conditions to more and more complexity and now we have reached a certain stage after 13.8 billion years of evolution and it it seems clear that this cannot be the end of it because the universe is still young it's going to be much older than it is now now there is this drive built in drive of the cosmos to become more complex and it seems clear that civilization a civilization like ours is is a stepping stone on to war it's something that is more complex and could I touch on a couple of things here the bootloader example is kind of where I want to go with this so a lot of the ideas of this movement can be traced back to Derek Parfit who is a philosopher he was a moral realist so he thought there was such a thing as a moral fact and I'm a bit of a relativist myself and actually if you trace this tree of complexity and how humans evolve over time we might just be a stepping stone to a kind of rich diverse transhumanist future where we become the thing over time that we're so scared of and I think the lens that we're using here about what's right and what's wrong is kind of like I was saying before it's a snapshot of humanity now and we kind of think of it as just this monolithic single thing so does it really work when you project out to how we're going to evolve in the future but first of all humankind is not a monolithic thing so many of these arguments go like we should not do that because of that we should not do that because of that but there is no us there is no we there are only and almost 10 billion different people and they all have different ideas about what's good for them and so for thousands of years we had these evolutions of ideas and of devices and philosophies competing partially competing and partially compatible with each other which in the end led to the current values that some people agree with and other people over there they agree with different values nevertheless there are certain values that have become more popular than others more successful more evolutionary with more success during the evolution of ideas and so given this entire context of evolution of concepts and accepted ideas of what should be done or what is worth being supported and what's not worth being supported all of this has changed a lot if we look back 200 years the average people in the west had different ideas of what's good than today and and this evolution of ideas is not going to stop any time soon just a final question on this and there is a very real existential risk right now of nuclear armageddon a real risk right now and if i were a rational person i would be devoting all of my effort into that and other risks associated so do you think it's a little bit weird that so much focuses on this ai x risk to me it's indeed weird now there are all these letters coming out warning are the dangers of ai and i think some of the guys who are writing these letters they are just seeking attention because they know that ai dystopia are attracting more attention than documentaries about the benefits of ai in healthcare and stuff like that but generally speaking i am much more worried about nuclear bombs than about ai weapons a nuclear bomb a big one can wipe out 10 million people a big city within a few milliseconds without a face recognition just like that without any ai and so in that sense it's much more harmful than the comparatively harmless ai weapons than that we have today and that we can currently conceive of so yes i'm much more worried about 60 year old technology that can wipe out civilization within two hours without any ai well i guess um since we're we're not really going to worry about ai for the moment we can uh we can turn our attention back to discussing with you uh how we develop ai so um you know i'm really curious with with just the really the the vast you know breadth and depth of your of your knowledge over the the history of of ai and the state of the art i'm curious you know which current approaches you're you're most excited about and or what's on the horizon um that you know for any of our listeners out there are thinking about um going into ai research machine learning research you know what may be um alternatives that aren't getting enough attention should they should they look into studying and and perhaps choosing the research at the moment the limelight is on um language models large language models which pass the touring tests and do all kinds of things that seemed inconceivable just a couple of years ago at least to some of those who are now surprised but of course that is just a tiny part of what's going to be important to develop true ai agi artificial general intelligence um on the other hand the roots of what we need to develop true ai also come from the previous millennium they are not new and of course what you need is an environment to interact with and you need an an agent that can manipulate the environment and you need a way of learning to improve the rewards that you get from this environment as you are interacting it with it within a single lifetime so one of the important aspects of reinforcement learning what we are now talking about is that you have only one single life you don't have repeatable episodes like in most of traditional reinforcement learning no you have only one single life and in the beginning you know nothing and then after 30 percent of your life is over you know something about life and all you know is the data that you collected during these first 30 percent of your life and now there is an infinite almost infinite possibility set of possibilities of futures and from this little short experience you have to generalize somehow and try to select action sequences that lead to the most promising futures that you can shape yourself through your actions now to achieve all of that you need to build a model of the world a predictive model of the world which means that you have to be able to learn over time and to predict the consequences of your actions so that you can use this model of the world that you are acquiring there to plan to plan ahead and you want to do that in a way that isn't the naive way which we had in 1990 which is millisecond by millisecond planning where you say okay now I'm moving from A to B and the way to do it is first move that little pinky muscle a little bit and move it a little bit more and move it a little bit more and then get up and so no you want to do that in a high level way in a hierarchical way in a way that allows you to to focus on the important abstract concepts for example as you are trying to go from from your home to Beijing you decompose this whole future into a couple of sub goals you say a first important step is to go to the cap station and get a taxi to the airport and then in the airport you will find your your plane and then for nine hours nothing is going to happen and you exit in Beijing and have to find another cab and so on so you you don't do millisecond by millisecond detailed planning no you have high level planning to just reduce the computational effort and focus on the essentials of what you want to do so that is something that most current systems don't do but for a long time we have had systems like that and they are getting more sophisticated over time important you have a predictive model of the world that is not just focusing on the pixels and predicting the how does the video change as I'm moving my hand back and forth the video that I get through my camera my eyes and so on and no higher level concepts that that reflect islands of predictability many things are not predictable but certain abstract representations of these things are predictable and so how can you discover these higher level concepts that you need to efficiently think about your own future options and select those that are most promising in the single life yeah yeah this is really interesting so we've been speaking with Carl Friston for example and he talks about this collective intelligence where you have this multi-agent cybernetic framework which is causally closed and one of the things we're talking about here really is not the model itself people talk about chat gpt and it's just a model and people have configured it in arrangements that have varying degrees of autonomy and in the future we will develop these collective intelligences and they're not just predicting the actions and behaviors of other agents but even the world that we're in is a computer to some extent so when you imbue agents with this kind of creativity and autonomy that's the thing that I don't think people really understand what might emerge from that it's related to this discussion about what kind of goals might emerge from that do you have any intuition on what that would look like yeah let me give you just the simplest example that we had in 1990 or 32 years ago of a system that sets itself its own goals and it consists of two artificial neural networks and I know that Carl Friston is very interested in that and only recently for the first time in my life I was on a paper where he was co-author just a year ago and so back then it was really about a reinforcement learning agent and it interacts with the world and it generates actions that change the world and then there is another network which just is trying to predict the consequences of the actions in the environment so the reactions of the environment to these actions and so that becomes a world model and then what kind of goal was there which was different from traditional goals well in the beginning this model of the world this prediction machine which is a model of the world a world model knows nothing so it has high error as it is trying to predict the next thing as it is trying to predict the reactions of the environment to the actions of the agent so as the second network is trying to reduce its prediction error through gradient descent through back propagation essentially the other one is trying to generate actions outputs that maximize the same error so basically the goal the self-invented goal if you will of the first network is to generate an action with whose consequences cannot yet be predicted by the other network by the model of the world so the first network is generating outputs that surprise the second network so suddenly you have an incentive where the first network is trying to invent actions experiments that fool or that surprise the second network and that was called artificial curiosity so now suddenly you have a little agent which a little bit like a baby doesn't learn by imitating the parents no it learns by inventing its own little sub goals and it's trying to surprise itself and have fun by playing with the toys and and observing new unpredictable things which however become predictable over time and therefore become boring and then it has another incentive to invent the additional experiments such that it still can surprise its model of the world which in turn is improving and so on so artificial curiosity does that does that also have the effect of making the network which is trying to predict does it have the effect of making it more robust and more generalizable like almost a form of you know regularization kind of built in in this pairing yeah you can build into that network all kinds of regularizers an orthogonal concept which is also very important so that was just the first version that was really in 1990 and then we have had a we had a long string of papers just on improvements of this original concept of artificial curiosity so this old system is basically what you what you now know as GANs Generative Adversarial Networks because the first network is generating a probability distribution over outputs and the second network is then predicting the consequences of these outputs in the environment and if you if the output is an image then the consequence can be either this image is of a certain type yes or not no and then that's all that the prediction machine the world model predicts in that simple case and you minimize the first network minimizes the same error function that the second one maximizes so then you have basically a GAN but then you don't have what you just mentioned yet the regularizer as a scientist what you really want to learn is a model of the world that extracts the regularities in the environment that that that finds predictable things which are regular in the sense that there's a short explanation there of for example if you have falling objects in a video then they all fall in the same way they accelerate in the same way which means it's predictable what these objects do if you see two of the frames you can predict the third frame pretty well and the law behind that is very simple this means that you can greatly compress the video that is coming in because you can instead of storing all the pixels you can compute many of these pixels by just looking at two successive frames and predicting the third frame or maybe three successive frames and predicting the fourth frame something like that and you only have to encode the deviations from the prediction so everything else you don't have to store separately which means you once you understand gravity you can greatly greatly compress the video so that's what you really want to do and so the more advanced version of artificial curiosity is about that where you have a motivation to find a disruption of the data which is coming in of the video of the falling apples for example that is simpler than the one that you had before so before you had the simple explanation of the data you needed so many bits so many bits to um to describe the data and afterwards only so many and the difference between before and after that is the reward that you get so that's the true reward that the controller the first neural network should get in response to the improvements of the second network which are now measured in terms of compression progress so first I needed so many resources to encode the data but then I discovered this regularity gravity and I can greatly compress all kinds of videos that that are reflecting the concept of gravity and certainly I'm have a huge insight into the nature of the world and that is my true joy scientific as a scientist my my true joy as a scientist that I want to encode in a little number which is given as a reward to the guy who is inventing these experiments that lead to the data to the data with the falling apples for example right well and of course this is this has been a challenge in machine learning you know since the beginning which is okay as we add more and more parameters how do we prevent it from learning spurious information with those parameters and instead have it focus on parsimonious explanations on regular explanations on things that in this universe are more likely to generalize you know to unseen examples and so I think my question to you is does this setup that you describe is it a form of that and or what is the state of the art you know these days for helping to push or nudge neural networks towards learning parsimonious models for the world rather than highly detailed spurious susceptible to you know high frequency anomalies and adversarial examples and all this sort of thing yes what is the current state of the art in the regularizing descriptors of the data such as neural networks such that you get simple explanations of the data such that you get short programs that compute the data in other words such that the description of the data is a short program that computes the much larger raw data and and how close can we get to the limits which are given through this concept concept of algorithmic information or comagor complexity comagor complexity of any data is the length of the shortest program on some general computer that computes it since in our field the general computers are recurrent neural networks we want to find a simple recurrent network that computes all this data and given one computation of the data we want to find an even simpler one so we want to have this idea of compression progress and here I have to say although we have lots of regularizers invented throughout the past few decades there's nothing that is really convincing I think one of the very important missing things is to make that work in a way that is truly convincing that is as convincing as chat gpt is today in the much more limited domain of generating text from previously observed texts and stuff a very old idea of I think the 1980s was to have weight decay in a neural network which basically is the idea that all the weights should have an incentive to become close to zero such that you can prune them and so people built in regularizer that just punished weights for being large or being very negative but that didn't work really well and something better was flat minimum search that was 1998 and first Arthur my brilliant student set book write that back then roughly the same time when the LSTM paper came out and and there the idea is if you have if you plot the weights of a neural network on the x-axis and you plot the error on the y-axis then given the weights you have high or low error and then there is for example a sharp error function which has a sharp minimum which which goes like that can you see my finger so here here is the x-axis here's the y-axis here's the error and the error for a certain weight is really really low but then for a different weight in the environment in the vicinity it's high again which would be very different from a flat minimum which would be like this so here's the error and it's going down and for many many ways it is low the error and then it goes up again so if you are a very sharp well versus a very broad well yes a sharp well versus a broad well now if you are in a sharp well you have to specify the weights with a lot with with high precision so you have to spend many bits of information on encoding the weights of this network as opposed to a large to a flat minimum where it doesn't matter if you you know perturb the weights because the error remains low in this flat minimum so what you really want to find is is a network that has low complexity in the sense that you can describe the good network so those with low error with very few bits of information and suddenly if you maximize or if you minimize that flat minimum second order error function then suddenly you have a preference for networks that that for example do this you you have a hidden unit and the outgoing weights they have certain values but if you give a very negative weight to the hidden unit then it doesn't matter what all these outgoing weights do and flat minimum minimum search likes to find weight matrices like that where one single weight can eliminate many others which you suddenly don't need any longer such that the description complexity of the whole thing is much lower than in the beginning when you when you just had a random initialization of all these weights so that is much more general than weight decay because weight decay doesn't like these strong weights it wants to remove them but sometimes it's really good to have a very negative weight coming to a hidden unit which is switched off through that weight such that all the outgoing connections are meaningless but it's not um what you what you it's very nice it's a very nice principle but it's not as general as finding the shortest program on a university computer that computes the weight matrix that is solving your problem to the extent how do you think we're how do you think we're gonna get to that point how do you think uh what approaches are going to lead us to finding things that approach comical of complexity yeah and i think that path has again a lot to do with meta learning and as um a system is able to run its own learning algorithm on the network itself it can um suddenly speak about the um algorithms in form of weight matrices and it can discuss concepts such as the complexity of a weight matrix and then you can conduct a search um in this space of networks that generates weight matrices and then you suddenly are in the game so suddenly you are playing the right game and then it's more a question of how to um choose an initial learning algorithm such as gradient descent to come up with something that computes the simple solutions which you really want to see in the end very recent papers on that on on aspects of that came out just a while ago with my students vincent herman and louise kirch and um and francesco faccio and my poster kazuki and robert joydash also um and also imann olschlag and there the idea is really to have one network that computes an experiment and the experiment itself is the weight matrix of a recurrent network so there is a generator of an experiment which can be anything that describes a computational interaction with an environment so a program so that experiment is then executed in the real world there's a prediction machine that predicts the outcome of the experiment before the algorithm is executed and so then there's um just a yes or no question either the following outcome will occur or not either it will occur or not but now the entire setup is such that you don't have predictions all the time about every single pixel no you just have something which is very abstract and which is just about whether a certain unit of the recurrent network is going to be on or off at the end of the experiment and this internal on and off unit can represent any computational question any questions that you can ask at all and now the the task of the experiment generator which is another network which generates a recurrent network weight matrix which represents the experiment the task of this experiment generator is to again come up with something that surprises the um the prediction machine which looks at the experiment and says yeah it's going to work or not and uh and suddenly you are again in this old game uh except that now you have this world of abstractions where the abstractions can be anything that is computable interesting really cool really cool could we spend the last 10 minutes or so just talking about some of the the current ai landscape so in particular the capabilities of GPT-4 and the moat building thing and and the the power that companies like uh google and open ai have and um also the potential for open source so maybe we'll just start with the you know the very current capabilities of GPT-4 are you impressed with it what do you think i'm impressed in the sense that um i like the outcomes that you get there and um it wasn't obvious a couple of years ago that it would become so good uh on the other hand of course and it's not yet this full AGI thing and it is not really close to um to justifying those fears that some uh researchers sometimes and now um document and um in letters and public letters and so on so to me it's a little bit like a visa view because for for many decades i have um had discussions like that and people said that you are crazy when i said that within my lifetime i want to build something that is smarter than myself um and now suddenly in recent years um some of the guys who said it's never going to happen suddenly they just look at chat gbt and they think oh now we are really close to AGI and whatever uh so i i don't share these um extreme um i'm less impressed than some of those guys let me say that right uh the open source movement that you mentioned you you want to ask a specific specific question about that right well yeah there was that famous google memo that got leaked and when the waits for loma from facebook went out within about two or three weeks um it was a valing pretty similar to chat gbt you know with this um laura fine tuning and the open source community has just exploded you know you can now run it on your laptop and there is some question whether there is a significant gap between the capability you know is is it just a parlor trick is it really as good potentially or could it be as good as some of the next best models from open ai but i guess the question is do you think that we need open ai to to have the best models no of course not um no i'm very convinced of the open source movement and have um supported that some people say the open source movement is maybe six or eight months behind the large companies that are now coming out with these models and i think the best way of making sure that there won't be dominance through some large company is to support the open source movement because how can a large company compete against all these brilliant phd students around the world who are so motivated to you know within a few days create something that is a little bit better than what the last guy has um um put out there on github and whatever so i'm i'm very convinced that this open source movement is going to make sure that there won't be a huge mode for a long time i'm reading between the lines here but i would guess you would be opposed to legislation like the eu is considering where you know very tight restrictions on generative models you know onerous onerous kind of uh approval processes and things like that because that's going to have this chilling effect on on open source innovation and the little guys wouldn't it yes i have signed letters um which which support the open source movement and whenever i get a chance to um maybe influence some you um politicians then i'm trying to contribute to making sure that they don't don't shoot themselves in the foot by by by killing killing innovation through the open source movement so you certainly want to avoid that there are lots of different open source movements around the world so if one big entity fails to support open source or even makes it harder for open source there will still be lots of other entities which um won't follow follow and so no matter what's going to happen on the political level i think open source is not going away i guess just in closing you've been in this game for decades now and what is i know it's a bit of a strange question to ask but what's your fondest memory in your career my fondest memory oh it's usually when i discover something that i think nobody has seen before but that is that happens very rarely because most of the things you think are well somebody else has done before um but yeah so yeah um what usually happens is um you and and this has happened many times not many times but quite a few times in my career since the 80s as a scientist who publishes stuff but suddenly you think oh that is the solution to all these problems and now i really figured out a way of building this universal system which um learns how to improve itself and learns the way to improve the way it improves itself and so on and now we are done and now all is that's necessary is to scale it up and it's going to solve everything and then um you think a little bit longer about it and maybe you have a couple of publications but then it turns out something is missing something important is missing and and actually it's not that great and actually you have to think hard to add something important to it which then for a brief moment looks like the greatest thing since sliced bread and um and then you get excited again but then suddenly you realize oh it's still not finished something important is missing and so it goes back and forth like that i think that's the life of a scientist the greatest joys are those moments where you have an insight where suddenly things fall into place such that along the lines of what we discussed before the description length of some solution to a problem suddenly shrinks because two puzzle pieces they suddenly match and and become one or become one in the sense that they fit each other such that suddenly you have the shared line between the two puzzle pieces one is negative and the other one is positive and certainly the whole thing is much more compressible than the sum of the things separately so these these things that's what's driving um scientists like myself i guess wonderful um professor you again schmidhuber it's been an absolute honor thank you so much for coming on the show today thank you it was such a pleasure talking to you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.4, "text": " my fondest memory. It's usually when I discover something that I think nobody has seen before,", "tokens": [50364, 452, 9557, 377, 4675, 13, 467, 311, 2673, 562, 286, 4411, 746, 300, 286, 519, 5079, 575, 1612, 949, 11, 50784], "temperature": 0.0, "avg_logprob": -0.15069717831081814, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.1049831286072731}, {"id": 1, "seek": 0, "start": 9.44, "end": 15.76, "text": " but that happens very rarely because most of the things you think of somebody else has done before.", "tokens": [50836, 457, 300, 2314, 588, 13752, 570, 881, 295, 264, 721, 291, 519, 295, 2618, 1646, 575, 1096, 949, 13, 51152], "temperature": 0.0, "avg_logprob": -0.15069717831081814, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.1049831286072731}, {"id": 2, "seek": 0, "start": 16.4, "end": 21.84, "text": " This episode is sponsored by Numeri. Are you a data scientist looking to make a real-world impact", "tokens": [51184, 639, 3500, 307, 16621, 538, 426, 15583, 72, 13, 2014, 291, 257, 1412, 12662, 1237, 281, 652, 257, 957, 12, 13217, 2712, 51456], "temperature": 0.0, "avg_logprob": -0.15069717831081814, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.1049831286072731}, {"id": 3, "seek": 0, "start": 21.84, "end": 28.0, "text": " with your skills? Do you love competing against the best minds in the world? Well, introducing", "tokens": [51456, 365, 428, 3942, 30, 1144, 291, 959, 15439, 1970, 264, 1151, 9634, 294, 264, 1002, 30, 1042, 11, 15424, 51764], "temperature": 0.0, "avg_logprob": -0.15069717831081814, "compression_ratio": 1.5542168674698795, "no_speech_prob": 0.1049831286072731}, {"id": 4, "seek": 2800, "start": 28.0, "end": 34.4, "text": " Numeri, the revolutionary cutting-edge AI-driven hedge fund that's changing the game for good.", "tokens": [50364, 426, 15583, 72, 11, 264, 22687, 6492, 12, 12203, 7318, 12, 25456, 25304, 2374, 300, 311, 4473, 264, 1216, 337, 665, 13, 50684], "temperature": 0.0, "avg_logprob": -0.08356084262623506, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.00816640630364418}, {"id": 5, "seek": 2800, "start": 34.4, "end": 40.32, "text": " Numeri combines a competitive data science tournament with powerful, clean stock market data", "tokens": [50684, 426, 15583, 72, 29520, 257, 10043, 1412, 3497, 13713, 365, 4005, 11, 2541, 4127, 2142, 1412, 50980], "temperature": 0.0, "avg_logprob": -0.08356084262623506, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.00816640630364418}, {"id": 6, "seek": 2800, "start": 40.32, "end": 45.2, "text": " enabling you to predict the market like never before. Sign up now, become part of the elite", "tokens": [50980, 23148, 291, 281, 6069, 264, 2142, 411, 1128, 949, 13, 13515, 493, 586, 11, 1813, 644, 295, 264, 17801, 51224], "temperature": 0.0, "avg_logprob": -0.08356084262623506, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.00816640630364418}, {"id": 7, "seek": 2800, "start": 45.2, "end": 49.68, "text": " community, taking the stock market by storm and I'll see you on the leaderboard.", "tokens": [51224, 1768, 11, 1940, 264, 4127, 2142, 538, 7679, 293, 286, 603, 536, 291, 322, 264, 5263, 3787, 13, 51448], "temperature": 0.0, "avg_logprob": -0.08356084262623506, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.00816640630364418}, {"id": 8, "seek": 4968, "start": 49.76, "end": 59.36, "text": " Wonderful. So today is a momentous occasion. What an episode of MLST we're going to have.", "tokens": [50368, 22768, 13, 407, 965, 307, 257, 1623, 563, 9674, 13, 708, 364, 3500, 295, 376, 19198, 51, 321, 434, 516, 281, 362, 13, 50848], "temperature": 0.0, "avg_logprob": -0.1470450226978589, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.1268414556980133}, {"id": 9, "seek": 4968, "start": 59.36, "end": 65.92, "text": " We're joined not by a godfather of AI, but the father of AI, you again, Schmidhuber,", "tokens": [50848, 492, 434, 6869, 406, 538, 257, 3044, 11541, 295, 7318, 11, 457, 264, 3086, 295, 7318, 11, 291, 797, 11, 2065, 25394, 71, 10261, 11, 51176], "temperature": 0.0, "avg_logprob": -0.1470450226978589, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.1268414556980133}, {"id": 10, "seek": 4968, "start": 65.92, "end": 71.36, "text": " the researcher responsible for leading the research groups which invented much of the technology", "tokens": [51176, 264, 21751, 6250, 337, 5775, 264, 2132, 3935, 597, 14479, 709, 295, 264, 2899, 51448], "temperature": 0.0, "avg_logprob": -0.1470450226978589, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.1268414556980133}, {"id": 11, "seek": 4968, "start": 71.36, "end": 76.08, "text": " which has powered the deep learning revolution. It's long been a dream to get you on the podcast,", "tokens": [51448, 597, 575, 17786, 264, 2452, 2539, 8894, 13, 467, 311, 938, 668, 257, 3055, 281, 483, 291, 322, 264, 7367, 11, 51684], "temperature": 0.0, "avg_logprob": -0.1470450226978589, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.1268414556980133}, {"id": 12, "seek": 7608, "start": 76.16, "end": 80.96, "text": " you again. It feels like the day has finally arrived, so welcome to MLST.", "tokens": [50368, 291, 797, 13, 467, 3417, 411, 264, 786, 575, 2721, 6678, 11, 370, 2928, 281, 376, 19198, 51, 13, 50608], "temperature": 0.0, "avg_logprob": -0.0929535941073769, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.0036901056300848722}, {"id": 13, "seek": 7608, "start": 80.96, "end": 86.56, "text": " Thank you, Tim, for these very kind words and this very generous introduction.", "tokens": [50608, 1044, 291, 11, 7172, 11, 337, 613, 588, 733, 2283, 293, 341, 588, 14537, 9339, 13, 50888], "temperature": 0.0, "avg_logprob": -0.0929535941073769, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.0036901056300848722}, {"id": 14, "seek": 7608, "start": 89.84, "end": 94.8, "text": " So on that, let's discuss the credit assignment problem in machine learning. Now,", "tokens": [51052, 407, 322, 300, 11, 718, 311, 2248, 264, 5397, 15187, 1154, 294, 3479, 2539, 13, 823, 11, 51300], "temperature": 0.0, "avg_logprob": -0.0929535941073769, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.0036901056300848722}, {"id": 15, "seek": 7608, "start": 94.8, "end": 100.8, "text": " you've dedicated a significant amount of time researching and publishing the actual history", "tokens": [51300, 291, 600, 8374, 257, 4776, 2372, 295, 565, 24176, 293, 17832, 264, 3539, 2503, 51600], "temperature": 0.0, "avg_logprob": -0.0929535941073769, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.0036901056300848722}, {"id": 16, "seek": 10080, "start": 100.8, "end": 105.36, "text": " of the field and there's a significant divergence between the public narrative", "tokens": [50364, 295, 264, 2519, 293, 456, 311, 257, 4776, 47387, 1296, 264, 1908, 9977, 50592], "temperature": 0.0, "avg_logprob": -0.09448630202050302, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.3152850866317749}, {"id": 17, "seek": 10080, "start": 105.36, "end": 110.96, "text": " and what actually happened. And amazingly, no one has pointed out any factual inaccuracies in your", "tokens": [50592, 293, 437, 767, 2011, 13, 400, 31762, 11, 572, 472, 575, 10932, 484, 604, 48029, 37957, 374, 20330, 294, 428, 50872], "temperature": 0.0, "avg_logprob": -0.09448630202050302, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.3152850866317749}, {"id": 18, "seek": 10080, "start": 110.96, "end": 116.39999999999999, "text": " accounts, but the incorrect perceptions still persevere. Now, I particularly enjoyed reading", "tokens": [50872, 9402, 11, 457, 264, 18424, 35258, 920, 20607, 5887, 13, 823, 11, 286, 4098, 4626, 3760, 51144], "temperature": 0.0, "avg_logprob": -0.09448630202050302, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.3152850866317749}, {"id": 19, "seek": 10080, "start": 116.39999999999999, "end": 120.8, "text": " your history of the breakthroughs in machine learning, going back to ancient times and of course even", "tokens": [51144, 428, 2503, 295, 264, 22397, 82, 294, 3479, 2539, 11, 516, 646, 281, 7832, 1413, 293, 295, 1164, 754, 51364], "temperature": 0.0, "avg_logprob": -0.09448630202050302, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.3152850866317749}, {"id": 20, "seek": 10080, "start": 120.8, "end": 126.08, "text": " remarking on the very first computer scientist, Leibniz. And for example, you pointed out the", "tokens": [51364, 7942, 278, 322, 264, 588, 700, 3820, 12662, 11, 1456, 897, 77, 590, 13, 400, 337, 1365, 11, 291, 10932, 484, 264, 51628], "temperature": 0.0, "avg_logprob": -0.09448630202050302, "compression_ratio": 1.6124567474048443, "no_speech_prob": 0.3152850866317749}, {"id": 21, "seek": 12608, "start": 126.08, "end": 131.44, "text": " history of who invented backprop and the CNN. And you explained that there wasn't really", "tokens": [50364, 2503, 295, 567, 14479, 646, 79, 1513, 293, 264, 24859, 13, 400, 291, 8825, 300, 456, 2067, 380, 534, 50632], "temperature": 0.0, "avg_logprob": -0.08840817393678607, "compression_ratio": 1.450777202072539, "no_speech_prob": 0.0036881393752992153}, {"id": 22, "seek": 12608, "start": 131.44, "end": 136.4, "text": " a neural network winter at all in the 1970s. So could you just sketch out a little bit of that", "tokens": [50632, 257, 18161, 3209, 6355, 412, 439, 294, 264, 14577, 82, 13, 407, 727, 291, 445, 12325, 484, 257, 707, 857, 295, 300, 50880], "temperature": 0.0, "avg_logprob": -0.08840817393678607, "compression_ratio": 1.450777202072539, "no_speech_prob": 0.0036881393752992153}, {"id": 23, "seek": 12608, "start": 136.4, "end": 150.4, "text": " history? So that's a challenge. Actually, computer science history and computing history started", "tokens": [50880, 2503, 30, 407, 300, 311, 257, 3430, 13, 5135, 11, 3820, 3497, 2503, 293, 15866, 2503, 1409, 51580], "temperature": 0.0, "avg_logprob": -0.08840817393678607, "compression_ratio": 1.450777202072539, "no_speech_prob": 0.0036881393752992153}, {"id": 24, "seek": 15040, "start": 150.4, "end": 160.56, "text": " maybe 2000 years ago when Heron of Alexandria built the first program-controlled machine.", "tokens": [50364, 1310, 8132, 924, 2057, 562, 3204, 266, 295, 41943, 3094, 264, 700, 1461, 12, 49344, 3479, 13, 50872], "temperature": 0.0, "avg_logprob": -0.16130759798247238, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.006087563931941986}, {"id": 25, "seek": 15040, "start": 160.56, "end": 169.76, "text": " That was 2000 years ago in the first century basically. And he basically built an automaton", "tokens": [50872, 663, 390, 8132, 924, 2057, 294, 264, 700, 4901, 1936, 13, 400, 415, 1936, 3094, 364, 3553, 25781, 51332], "temperature": 0.0, "avg_logprob": -0.16130759798247238, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.006087563931941986}, {"id": 26, "seek": 15040, "start": 169.76, "end": 178.72, "text": " that was programmed through a cable which was wrapped around a rotating cylinder which had", "tokens": [51332, 300, 390, 31092, 807, 257, 8220, 597, 390, 14226, 926, 257, 19627, 17884, 597, 632, 51780], "temperature": 0.0, "avg_logprob": -0.16130759798247238, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.006087563931941986}, {"id": 27, "seek": 17872, "start": 178.72, "end": 186.48, "text": " certain knobs and then there was a weight which pulled it down and the whole apparatus", "tokens": [50364, 1629, 46999, 293, 550, 456, 390, 257, 3364, 597, 7373, 309, 760, 293, 264, 1379, 38573, 50752], "temperature": 0.0, "avg_logprob": -0.08840823173522949, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.001241392339579761}, {"id": 28, "seek": 17872, "start": 186.48, "end": 194.88, "text": " was able to direct the movements of little robots, of little puppets in an automatic theater.", "tokens": [50752, 390, 1075, 281, 2047, 264, 9981, 295, 707, 14733, 11, 295, 707, 17014, 1385, 294, 364, 12509, 10612, 13, 51172], "temperature": 0.0, "avg_logprob": -0.08840823173522949, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.001241392339579761}, {"id": 29, "seek": 17872, "start": 196.0, "end": 204.8, "text": " That, as far as I know, was the first program-controlled machine in the history of mankind.", "tokens": [51228, 663, 11, 382, 1400, 382, 286, 458, 11, 390, 264, 700, 1461, 12, 49344, 3479, 294, 264, 2503, 295, 21220, 13, 51668], "temperature": 0.0, "avg_logprob": -0.08840823173522949, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.001241392339579761}, {"id": 30, "seek": 20480, "start": 204.8, "end": 211.92000000000002, "text": " Even before that there were other machines. The ancient Greeks had even earlier the", "tokens": [50364, 2754, 949, 300, 456, 645, 661, 8379, 13, 440, 7832, 31029, 632, 754, 3071, 264, 50720], "temperature": 0.0, "avg_logprob": -0.19349712795681423, "compression_ratio": 1.4324324324324325, "no_speech_prob": 0.001669350080192089}, {"id": 31, "seek": 20480, "start": 212.72, "end": 220.88000000000002, "text": " Antiqueterre mechanism which was kind of a clock, an astronomical clock. But then more recently", "tokens": [50760, 5130, 3221, 2398, 265, 7513, 597, 390, 733, 295, 257, 7830, 11, 364, 49035, 7830, 13, 583, 550, 544, 3938, 51168], "temperature": 0.0, "avg_logprob": -0.19349712795681423, "compression_ratio": 1.4324324324324325, "no_speech_prob": 0.001669350080192089}, {"id": 32, "seek": 20480, "start": 223.20000000000002, "end": 229.68, "text": " we have seen many additional advances and you mentioned Leibniz, of course, who is of", "tokens": [51284, 321, 362, 1612, 867, 4497, 25297, 293, 291, 2835, 1456, 897, 77, 590, 11, 295, 1164, 11, 567, 307, 295, 51608], "temperature": 0.0, "avg_logprob": -0.19349712795681423, "compression_ratio": 1.4324324324324325, "no_speech_prob": 0.001669350080192089}, {"id": 33, "seek": 22968, "start": 229.68, "end": 236.32, "text": " special interest to our field because he not only is called the first computer scientist", "tokens": [50364, 2121, 1179, 281, 527, 2519, 570, 415, 406, 787, 307, 1219, 264, 700, 3820, 12662, 50696], "temperature": 0.0, "avg_logprob": -0.11754308977434712, "compression_ratio": 1.622754491017964, "no_speech_prob": 0.002073667710646987}, {"id": 34, "seek": 22968, "start": 236.32, "end": 245.28, "text": " because he had the first machine with a memory that was in the 1680s, I think. He not only had the", "tokens": [50696, 570, 415, 632, 264, 700, 3479, 365, 257, 4675, 300, 390, 294, 264, 3165, 4702, 82, 11, 286, 519, 13, 634, 406, 787, 632, 264, 51144], "temperature": 0.0, "avg_logprob": -0.11754308977434712, "compression_ratio": 1.622754491017964, "no_speech_prob": 0.002073667710646987}, {"id": 35, "seek": 22968, "start": 247.60000000000002, "end": 255.20000000000002, "text": " first machine that could do all the basic arithmetic operations which are addition,", "tokens": [51260, 700, 3479, 300, 727, 360, 439, 264, 3875, 42973, 7705, 597, 366, 4500, 11, 51640], "temperature": 0.0, "avg_logprob": -0.11754308977434712, "compression_ratio": 1.622754491017964, "no_speech_prob": 0.002073667710646987}, {"id": 36, "seek": 25520, "start": 255.2, "end": 265.92, "text": " multiplication, division and subtraction, then he not only had these first ideas for a universal", "tokens": [50364, 27290, 11, 10044, 293, 16390, 313, 11, 550, 415, 406, 787, 632, 613, 700, 3487, 337, 257, 11455, 50900], "temperature": 0.0, "avg_logprob": -0.11589144852201817, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.00402125483378768}, {"id": 37, "seek": 25520, "start": 265.92, "end": 272.24, "text": " problem solver that would solve all kinds of questions, even philosophical questions,", "tokens": [50900, 1154, 1404, 331, 300, 576, 5039, 439, 3685, 295, 1651, 11, 754, 25066, 1651, 11, 51216], "temperature": 0.0, "avg_logprob": -0.11589144852201817, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.00402125483378768}, {"id": 38, "seek": 25520, "start": 272.88, "end": 282.0, "text": " just through computation. And he not only was the first who had this algebra of thought which", "tokens": [51248, 445, 807, 24903, 13, 400, 415, 406, 787, 390, 264, 700, 567, 632, 341, 21989, 295, 1194, 597, 51704], "temperature": 0.0, "avg_logprob": -0.11589144852201817, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.00402125483378768}, {"id": 39, "seek": 28200, "start": 282.0, "end": 291.04, "text": " is deductively equivalent to the much later Boolean algebra. In many ways he was a pioneer,", "tokens": [50364, 307, 31513, 3413, 10344, 281, 264, 709, 1780, 23351, 28499, 21989, 13, 682, 867, 2098, 415, 390, 257, 37668, 11, 50816], "temperature": 0.0, "avg_logprob": -0.13526954935557806, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.0018311792518943548}, {"id": 40, "seek": 28200, "start": 291.04, "end": 296.24, "text": " but especially in our field in deep learning he contributed something essential, which is really", "tokens": [50816, 457, 2318, 294, 527, 2519, 294, 2452, 2539, 415, 18434, 746, 7115, 11, 597, 307, 534, 51076], "temperature": 0.0, "avg_logprob": -0.13526954935557806, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.0018311792518943548}, {"id": 41, "seek": 28200, "start": 296.24, "end": 304.64, "text": " central for this field, which is the chain rule. I think 1676, that's when he published that and", "tokens": [51076, 5777, 337, 341, 2519, 11, 597, 307, 264, 5021, 4978, 13, 286, 519, 3165, 25026, 11, 300, 311, 562, 415, 6572, 300, 293, 51496], "temperature": 0.0, "avg_logprob": -0.13526954935557806, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.0018311792518943548}, {"id": 42, "seek": 30464, "start": 304.64, "end": 315.44, "text": " that's what is now being used to train very deep artificial neural networks and also shallow", "tokens": [50364, 300, 311, 437, 307, 586, 885, 1143, 281, 3847, 588, 2452, 11677, 18161, 9590, 293, 611, 20488, 50904], "temperature": 0.0, "avg_logprob": -0.10711429196019326, "compression_ratio": 1.653179190751445, "no_speech_prob": 0.007225214969366789}, {"id": 43, "seek": 30464, "start": 315.44, "end": 321.91999999999996, "text": " neural networks and recurrent neural networks. And everything that we are using in modern AI", "tokens": [50904, 18161, 9590, 293, 18680, 1753, 18161, 9590, 13, 400, 1203, 300, 321, 366, 1228, 294, 4363, 7318, 51228], "temperature": 0.0, "avg_logprob": -0.10711429196019326, "compression_ratio": 1.653179190751445, "no_speech_prob": 0.007225214969366789}, {"id": 44, "seek": 30464, "start": 321.91999999999996, "end": 330.0, "text": " is really in many ways depending on that early work. But then of course there was so much additional", "tokens": [51228, 307, 534, 294, 867, 2098, 5413, 322, 300, 2440, 589, 13, 583, 550, 295, 1164, 456, 390, 370, 709, 4497, 51632], "temperature": 0.0, "avg_logprob": -0.10711429196019326, "compression_ratio": 1.653179190751445, "no_speech_prob": 0.007225214969366789}, {"id": 45, "seek": 33000, "start": 330.0, "end": 340.64, "text": " work. The first neural networks, as we know them, they came up about around 1800. That's when Gauss", "tokens": [50364, 589, 13, 440, 700, 18161, 9590, 11, 382, 321, 458, 552, 11, 436, 1361, 493, 466, 926, 24327, 13, 663, 311, 562, 10384, 2023, 50896], "temperature": 0.0, "avg_logprob": -0.14759504622307376, "compression_ratio": 1.6045197740112995, "no_speech_prob": 0.011306575499475002}, {"id": 46, "seek": 33000, "start": 340.64, "end": 349.68, "text": " and Legendre had the linear neural networks, the linear perceptrons in the sense that they were", "tokens": [50896, 293, 21480, 265, 632, 264, 8213, 18161, 9590, 11, 264, 8213, 43276, 13270, 294, 264, 2020, 300, 436, 645, 51348], "temperature": 0.0, "avg_logprob": -0.14759504622307376, "compression_ratio": 1.6045197740112995, "no_speech_prob": 0.011306575499475002}, {"id": 47, "seek": 33000, "start": 349.68, "end": 359.92, "text": " linear without having any non-differential aspect to it. So these first neural networks,", "tokens": [51348, 8213, 1553, 1419, 604, 2107, 12, 67, 12612, 2549, 4171, 281, 309, 13, 407, 613, 700, 18161, 9590, 11, 51860], "temperature": 0.0, "avg_logprob": -0.14759504622307376, "compression_ratio": 1.6045197740112995, "no_speech_prob": 0.011306575499475002}, {"id": 48, "seek": 36000, "start": 360.56, "end": 372.64, "text": " back then, were called method of least squares. And the training method was regression and the", "tokens": [50392, 646, 550, 11, 645, 1219, 3170, 295, 1935, 19368, 13, 400, 264, 3097, 3170, 390, 24590, 293, 264, 50996], "temperature": 0.0, "avg_logprob": -0.12557984821832002, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0006767113227397203}, {"id": 49, "seek": 36000, "start": 372.64, "end": 378.08, "text": " error function was exactly the same that we use today. And it was basically just a network with", "tokens": [50996, 6713, 2445, 390, 2293, 264, 912, 300, 321, 764, 965, 13, 400, 309, 390, 1936, 445, 257, 3209, 365, 51268], "temperature": 0.0, "avg_logprob": -0.12557984821832002, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0006767113227397203}, {"id": 50, "seek": 36000, "start": 378.08, "end": 384.56, "text": " a set of inputs and a set of outputs and a linear mapping from the inputs to the outputs. And you", "tokens": [51268, 257, 992, 295, 15743, 293, 257, 992, 295, 23930, 293, 257, 8213, 18350, 490, 264, 15743, 281, 264, 23930, 13, 400, 291, 51592], "temperature": 0.0, "avg_logprob": -0.12557984821832002, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0006767113227397203}, {"id": 51, "seek": 38456, "start": 384.56, "end": 392.96, "text": " could learn to adjust the weights of these connections. So that was the first linear neural", "tokens": [50364, 727, 1466, 281, 4369, 264, 17443, 295, 613, 9271, 13, 407, 300, 390, 264, 700, 8213, 18161, 50784], "temperature": 0.0, "avg_logprob": -0.08266649366934088, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.00057878257939592}, {"id": 52, "seek": 38456, "start": 392.96, "end": 402.8, "text": " network and many additional later developments led to what we have today. You had this beautiful", "tokens": [50784, 3209, 293, 867, 4497, 1780, 20862, 4684, 281, 437, 321, 362, 965, 13, 509, 632, 341, 2238, 51276], "temperature": 0.0, "avg_logprob": -0.08266649366934088, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.00057878257939592}, {"id": 53, "seek": 38456, "start": 402.8, "end": 407.28, "text": " statement. You said that machine learning is the science of credit assignment and we should apply", "tokens": [51276, 5629, 13, 509, 848, 300, 3479, 2539, 307, 264, 3497, 295, 5397, 15187, 293, 321, 820, 3079, 51500], "temperature": 0.0, "avg_logprob": -0.08266649366934088, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.00057878257939592}, {"id": 54, "seek": 38456, "start": 407.92, "end": 413.52, "text": " that same science to the field itself. And I guess what I'm really curious about is", "tokens": [51532, 300, 912, 3497, 281, 264, 2519, 2564, 13, 400, 286, 2041, 437, 286, 478, 534, 6369, 466, 307, 51812], "temperature": 0.0, "avg_logprob": -0.08266649366934088, "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.00057878257939592}, {"id": 55, "seek": 41352, "start": 414.24, "end": 419.28, "text": " first, if you could educate our listeners just a bit on what credit assignment is in the context", "tokens": [50400, 700, 11, 498, 291, 727, 16092, 527, 23274, 445, 257, 857, 322, 437, 5397, 15187, 307, 294, 264, 4319, 50652], "temperature": 0.0, "avg_logprob": -0.09773126515475186, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0002912451163865626}, {"id": 56, "seek": 41352, "start": 419.28, "end": 424.79999999999995, "text": " of, say, machine learning and why you think it's important that that should apply to the field", "tokens": [50652, 295, 11, 584, 11, 3479, 2539, 293, 983, 291, 519, 309, 311, 1021, 300, 300, 820, 3079, 281, 264, 2519, 50928], "temperature": 0.0, "avg_logprob": -0.09773126515475186, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0002912451163865626}, {"id": 57, "seek": 41352, "start": 424.79999999999995, "end": 429.03999999999996, "text": " in general. You know, why should we care about credit assignment? Why should we study the history", "tokens": [50928, 294, 2674, 13, 509, 458, 11, 983, 820, 321, 1127, 466, 5397, 15187, 30, 1545, 820, 321, 2979, 264, 2503, 51140], "temperature": 0.0, "avg_logprob": -0.09773126515475186, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0002912451163865626}, {"id": 58, "seek": 41352, "start": 429.03999999999996, "end": 436.56, "text": " of the developments in the field? Why is it important? I'm interested in credit assignment,", "tokens": [51140, 295, 264, 20862, 294, 264, 2519, 30, 1545, 307, 309, 1021, 30, 286, 478, 3102, 294, 5397, 15187, 11, 51516], "temperature": 0.0, "avg_logprob": -0.09773126515475186, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0002912451163865626}, {"id": 59, "seek": 43656, "start": 437.2, "end": 442.32, "text": " not only in machine learning, but also in the history of machine learning,", "tokens": [50396, 406, 787, 294, 3479, 2539, 11, 457, 611, 294, 264, 2503, 295, 3479, 2539, 11, 50652], "temperature": 0.0, "avg_logprob": -0.08120176792144776, "compression_ratio": 1.7, "no_speech_prob": 0.01078120619058609}, {"id": 60, "seek": 43656, "start": 443.12, "end": 451.2, "text": " because machine learning itself is the science of credit assignment. What does that mean? Suppose", "tokens": [50692, 570, 3479, 2539, 2564, 307, 264, 3497, 295, 5397, 15187, 13, 708, 775, 300, 914, 30, 21360, 51096], "temperature": 0.0, "avg_logprob": -0.08120176792144776, "compression_ratio": 1.7, "no_speech_prob": 0.01078120619058609}, {"id": 61, "seek": 43656, "start": 451.2, "end": 459.76, "text": " you have a complicated machine, which is influencing the world in a way that leads to the solution", "tokens": [51096, 291, 362, 257, 6179, 3479, 11, 597, 307, 40396, 264, 1002, 294, 257, 636, 300, 6689, 281, 264, 3827, 51524], "temperature": 0.0, "avg_logprob": -0.08120176792144776, "compression_ratio": 1.7, "no_speech_prob": 0.01078120619058609}, {"id": 62, "seek": 43656, "start": 459.76, "end": 465.2, "text": " of a problem. And maybe the machine solves the problem. But then the big question is,", "tokens": [51524, 295, 257, 1154, 13, 400, 1310, 264, 3479, 39890, 264, 1154, 13, 583, 550, 264, 955, 1168, 307, 11, 51796], "temperature": 0.0, "avg_logprob": -0.08120176792144776, "compression_ratio": 1.7, "no_speech_prob": 0.01078120619058609}, {"id": 63, "seek": 46520, "start": 465.2, "end": 472.71999999999997, "text": " which of the components of these many components were responsible? Some of them were active", "tokens": [50364, 597, 295, 264, 6677, 295, 613, 867, 6677, 645, 6250, 30, 2188, 295, 552, 645, 4967, 50740], "temperature": 0.0, "avg_logprob": -0.14965235176732986, "compression_ratio": 1.572289156626506, "no_speech_prob": 0.0009992492850869894}, {"id": 64, "seek": 46520, "start": 472.71999999999997, "end": 481.76, "text": " a long time ago and others later and early actions set the stage for later actions. Now,", "tokens": [50740, 257, 938, 565, 2057, 293, 2357, 1780, 293, 2440, 5909, 992, 264, 3233, 337, 1780, 5909, 13, 823, 11, 51192], "temperature": 0.0, "avg_logprob": -0.14965235176732986, "compression_ratio": 1.572289156626506, "no_speech_prob": 0.0009992492850869894}, {"id": 65, "seek": 46520, "start": 482.96, "end": 486.88, "text": " if you want to improve the performance of the machine, you should figure out how", "tokens": [51252, 498, 291, 528, 281, 3470, 264, 3389, 295, 264, 3479, 11, 291, 820, 2573, 484, 577, 51448], "temperature": 0.0, "avg_logprob": -0.14965235176732986, "compression_ratio": 1.572289156626506, "no_speech_prob": 0.0009992492850869894}, {"id": 66, "seek": 48688, "start": 487.6, "end": 496.0, "text": " did the components contribute to the overall success. And this is what credit assignment is", "tokens": [50400, 630, 264, 6677, 10586, 281, 264, 4787, 2245, 13, 400, 341, 307, 437, 5397, 15187, 307, 50820], "temperature": 0.0, "avg_logprob": -0.15518486716530538, "compression_ratio": 1.5843373493975903, "no_speech_prob": 0.0023936997167766094}, {"id": 67, "seek": 48688, "start": 496.0, "end": 502.0, "text": " about. And in machine learning in general, we have a system consisting of many", "tokens": [50820, 466, 13, 400, 294, 3479, 2539, 294, 2674, 11, 321, 362, 257, 1185, 33921, 295, 867, 51120], "temperature": 0.0, "avg_logprob": -0.15518486716530538, "compression_ratio": 1.5843373493975903, "no_speech_prob": 0.0023936997167766094}, {"id": 68, "seek": 48688, "start": 504.0, "end": 512.64, "text": " machine learning engineers and mathematicians and hardware builders and all kinds of people.", "tokens": [51220, 3479, 2539, 11955, 293, 32811, 2567, 293, 8837, 36281, 293, 439, 3685, 295, 561, 13, 51652], "temperature": 0.0, "avg_logprob": -0.15518486716530538, "compression_ratio": 1.5843373493975903, "no_speech_prob": 0.0023936997167766094}, {"id": 69, "seek": 51264, "start": 512.64, "end": 518.56, "text": " And there you also would like to figure out which parts of the system are responsible for later", "tokens": [50364, 400, 456, 291, 611, 576, 411, 281, 2573, 484, 597, 3166, 295, 264, 1185, 366, 6250, 337, 1780, 50660], "temperature": 0.0, "avg_logprob": -0.1287705521834524, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.005289988126605749}, {"id": 70, "seek": 51264, "start": 518.56, "end": 524.8, "text": " successes. Yeah, and it's a brilliant point. And I completely agree with you, by the way.", "tokens": [50660, 26101, 13, 865, 11, 293, 309, 311, 257, 10248, 935, 13, 400, 286, 2584, 3986, 365, 291, 11, 538, 264, 636, 13, 50972], "temperature": 0.0, "avg_logprob": -0.1287705521834524, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.005289988126605749}, {"id": 71, "seek": 51264, "start": 524.8, "end": 530.96, "text": " And I think the way I think about it is you've got this giant architecture of humanity and in it", "tokens": [50972, 400, 286, 519, 264, 636, 286, 519, 466, 309, 307, 291, 600, 658, 341, 7410, 9482, 295, 10243, 293, 294, 309, 51280], "temperature": 0.0, "avg_logprob": -0.1287705521834524, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.005289988126605749}, {"id": 72, "seek": 51264, "start": 530.96, "end": 535.76, "text": " are these certain nodes that may be an individual, maybe a research group. And if they come up with", "tokens": [51280, 366, 613, 1629, 13891, 300, 815, 312, 364, 2609, 11, 1310, 257, 2132, 1594, 13, 400, 498, 436, 808, 493, 365, 51520], "temperature": 0.0, "avg_logprob": -0.1287705521834524, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.005289988126605749}, {"id": 73, "seek": 51264, "start": 535.76, "end": 541.6, "text": " things that are very helpful, right, you want to try and direct more attention, more resources,", "tokens": [51520, 721, 300, 366, 588, 4961, 11, 558, 11, 291, 528, 281, 853, 293, 2047, 544, 3202, 11, 544, 3593, 11, 51812], "temperature": 0.0, "avg_logprob": -0.1287705521834524, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.005289988126605749}, {"id": 74, "seek": 54160, "start": 542.0, "end": 548.96, "text": " at that nodule, at that node, right, because it's likely to come up with additional very", "tokens": [50384, 412, 300, 15224, 2271, 11, 412, 300, 9984, 11, 558, 11, 570, 309, 311, 3700, 281, 808, 493, 365, 4497, 588, 50732], "temperature": 0.0, "avg_logprob": -0.11029091510143908, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0008824480464681983}, {"id": 75, "seek": 54160, "start": 548.96, "end": 554.08, "text": " important things. And if we don't get that right, we're just not optimizing the algorithm of science", "tokens": [50732, 1021, 721, 13, 400, 498, 321, 500, 380, 483, 300, 558, 11, 321, 434, 445, 406, 40425, 264, 9284, 295, 3497, 50988], "temperature": 0.0, "avg_logprob": -0.11029091510143908, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0008824480464681983}, {"id": 76, "seek": 54160, "start": 554.08, "end": 564.16, "text": " as a whole. That's right, yes. Machine learning and science in general is based on this principle", "tokens": [50988, 382, 257, 1379, 13, 663, 311, 558, 11, 2086, 13, 22155, 2539, 293, 3497, 294, 2674, 307, 2361, 322, 341, 8665, 51492], "temperature": 0.0, "avg_logprob": -0.11029091510143908, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0008824480464681983}, {"id": 77, "seek": 54160, "start": 564.16, "end": 571.36, "text": " of credit assignment where credit usually doesn't come in form of money, sometimes also in form", "tokens": [51492, 295, 5397, 15187, 689, 5397, 2673, 1177, 380, 808, 294, 1254, 295, 1460, 11, 2171, 611, 294, 1254, 51852], "temperature": 0.0, "avg_logprob": -0.11029091510143908, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0008824480464681983}, {"id": 78, "seek": 57136, "start": 571.36, "end": 581.44, "text": " of money, but in form of reputation. And then the whole system is set up such that you create", "tokens": [50364, 295, 1460, 11, 457, 294, 1254, 295, 13061, 13, 400, 550, 264, 1379, 1185, 307, 992, 493, 1270, 300, 291, 1884, 50868], "temperature": 0.0, "avg_logprob": -0.08765940666198731, "compression_ratio": 1.5314285714285714, "no_speech_prob": 0.0023863473907113075}, {"id": 79, "seek": 57136, "start": 581.44, "end": 592.08, "text": " an incentive for people who have worked on improving some method to credit those who", "tokens": [50868, 364, 22346, 337, 561, 567, 362, 2732, 322, 11470, 512, 3170, 281, 5397, 729, 567, 51400], "temperature": 0.0, "avg_logprob": -0.08765940666198731, "compression_ratio": 1.5314285714285714, "no_speech_prob": 0.0023863473907113075}, {"id": 80, "seek": 57136, "start": 592.08, "end": 599.2, "text": " maybe came up with the original method and to just have these chains of credit assignment", "tokens": [51400, 1310, 1361, 493, 365, 264, 3380, 3170, 293, 281, 445, 362, 613, 12626, 295, 5397, 15187, 51756], "temperature": 0.0, "avg_logprob": -0.08765940666198731, "compression_ratio": 1.5314285714285714, "no_speech_prob": 0.0023863473907113075}, {"id": 81, "seek": 59920, "start": 599.84, "end": 607.2800000000001, "text": " that make clear who did what, when, because the whole system is based on this incentive.", "tokens": [50396, 300, 652, 1850, 567, 630, 437, 11, 562, 11, 570, 264, 1379, 1185, 307, 2361, 322, 341, 22346, 13, 50768], "temperature": 0.0, "avg_logprob": -0.1138393004735311, "compression_ratio": 1.5174418604651163, "no_speech_prob": 0.0024274408351629972}, {"id": 82, "seek": 59920, "start": 607.2800000000001, "end": 616.48, "text": " And yes, those who are then credited with certain valuable contributions, they also can get", "tokens": [50768, 400, 2086, 11, 729, 567, 366, 550, 41155, 365, 1629, 8263, 15725, 11, 436, 611, 393, 483, 51228], "temperature": 0.0, "avg_logprob": -0.1138393004735311, "compression_ratio": 1.5174418604651163, "no_speech_prob": 0.0024274408351629972}, {"id": 83, "seek": 59920, "start": 616.48, "end": 623.36, "text": " reasonable jobs within the economy and so on. But that's more like the secondary", "tokens": [51228, 10585, 4782, 1951, 264, 5010, 293, 370, 322, 13, 583, 300, 311, 544, 411, 264, 11396, 51572], "temperature": 0.0, "avg_logprob": -0.1138393004735311, "compression_ratio": 1.5174418604651163, "no_speech_prob": 0.0024274408351629972}, {"id": 84, "seek": 62336, "start": 623.76, "end": 631.92, "text": " consequence of the basic principle. And that's why all PhD advisors", "tokens": [50384, 18326, 295, 264, 3875, 8665, 13, 400, 300, 311, 983, 439, 14476, 29136, 50792], "temperature": 0.0, "avg_logprob": -0.11848793531719007, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.000855601450894028}, {"id": 85, "seek": 62336, "start": 634.08, "end": 641.12, "text": " teach their PhD students to be meticulous when it comes to credit assignment to past work.", "tokens": [50900, 2924, 641, 14476, 1731, 281, 312, 41566, 6893, 562, 309, 1487, 281, 5397, 15187, 281, 1791, 589, 13, 51252], "temperature": 0.0, "avg_logprob": -0.11848793531719007, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.000855601450894028}, {"id": 86, "seek": 62336, "start": 642.32, "end": 648.5600000000001, "text": " So one last question, if I may, I've really enjoyed studying the history of advancement", "tokens": [51312, 407, 472, 1036, 1168, 11, 498, 286, 815, 11, 286, 600, 534, 4626, 7601, 264, 2503, 295, 35764, 51624], "temperature": 0.0, "avg_logprob": -0.11848793531719007, "compression_ratio": 1.3977272727272727, "no_speech_prob": 0.000855601450894028}, {"id": 87, "seek": 64856, "start": 649.52, "end": 654.3199999999999, "text": " because I found that when I go back and read original source materials, let's say", "tokens": [50412, 570, 286, 1352, 300, 562, 286, 352, 646, 293, 1401, 3380, 4009, 5319, 11, 718, 311, 584, 50652], "temperature": 0.0, "avg_logprob": -0.11041366486322313, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.006189435254782438}, {"id": 88, "seek": 64856, "start": 654.9599999999999, "end": 661.1999999999999, "text": " Einstein's first paper on diffusion or anything like that, because they're breaking new ground,", "tokens": [50684, 23486, 311, 700, 3035, 322, 25242, 420, 1340, 411, 300, 11, 570, 436, 434, 7697, 777, 2727, 11, 50996], "temperature": 0.0, "avg_logprob": -0.11041366486322313, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.006189435254782438}, {"id": 89, "seek": 64856, "start": 661.1999999999999, "end": 668.3199999999999, "text": " they're considering a wider array of possibilities. And then over time, the field becomes more and", "tokens": [50996, 436, 434, 8079, 257, 11842, 10225, 295, 12178, 13, 400, 550, 670, 565, 11, 264, 2519, 3643, 544, 293, 51352], "temperature": 0.0, "avg_logprob": -0.11041366486322313, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.006189435254782438}, {"id": 90, "seek": 64856, "start": 668.3199999999999, "end": 674.4799999999999, "text": " more focused on a narrower avenue of that. And you can go back and look at the original work", "tokens": [51352, 544, 5178, 322, 257, 46751, 39230, 295, 300, 13, 400, 291, 393, 352, 646, 293, 574, 412, 264, 3380, 589, 51660], "temperature": 0.0, "avg_logprob": -0.11041366486322313, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.006189435254782438}, {"id": 91, "seek": 67448, "start": 674.5600000000001, "end": 679.12, "text": " and actually gain a lot of inspiration for alternative approaches or alternative", "tokens": [50368, 293, 767, 6052, 257, 688, 295, 10249, 337, 8535, 11587, 420, 8535, 50596], "temperature": 0.0, "avg_logprob": -0.08248203854228175, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.013371288776397705}, {"id": 92, "seek": 67448, "start": 679.6800000000001, "end": 685.44, "text": " considerations. So in a sense, it's kind of in the sense of forgetting is as important as learning.", "tokens": [50624, 24070, 13, 407, 294, 257, 2020, 11, 309, 311, 733, 295, 294, 264, 2020, 295, 25428, 307, 382, 1021, 382, 2539, 13, 50912], "temperature": 0.0, "avg_logprob": -0.08248203854228175, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.013371288776397705}, {"id": 93, "seek": 67448, "start": 685.44, "end": 690.08, "text": " Sometimes we need to go back to go down a different branch of the tree, if you will,", "tokens": [50912, 4803, 321, 643, 281, 352, 646, 281, 352, 760, 257, 819, 9819, 295, 264, 4230, 11, 498, 291, 486, 11, 51144], "temperature": 0.0, "avg_logprob": -0.08248203854228175, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.013371288776397705}, {"id": 94, "seek": 67448, "start": 690.08, "end": 695.6800000000001, "text": " and expand the breadth of the search a little bit. I'm curious if you've noticed that same phenomenon.", "tokens": [51144, 293, 5268, 264, 35862, 295, 264, 3164, 257, 707, 857, 13, 286, 478, 6369, 498, 291, 600, 5694, 300, 912, 14029, 13, 51424], "temperature": 0.0, "avg_logprob": -0.08248203854228175, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.013371288776397705}, {"id": 95, "seek": 69568, "start": 695.8399999999999, "end": 707.4399999999999, "text": " Yes, science in general is about failure. And 99% of all scientific activity is about", "tokens": [50372, 1079, 11, 3497, 294, 2674, 307, 466, 7763, 13, 400, 11803, 4, 295, 439, 8134, 5191, 307, 466, 50952], "temperature": 0.0, "avg_logprob": -0.1720795891501687, "compression_ratio": 1.4967320261437909, "no_speech_prob": 0.0030230791307985783}, {"id": 96, "seek": 69568, "start": 709.1999999999999, "end": 712.0799999999999, "text": " creating failures. But then you learn from these", "tokens": [51040, 4084, 20774, 13, 583, 550, 291, 1466, 490, 613, 51184], "temperature": 0.0, "avg_logprob": -0.1720795891501687, "compression_ratio": 1.4967320261437909, "no_speech_prob": 0.0030230791307985783}, {"id": 97, "seek": 69568, "start": 714.2399999999999, "end": 720.0799999999999, "text": " failures and you do backtracking. And you go back to a previous decision point where you maybe", "tokens": [51292, 20774, 293, 291, 360, 646, 6903, 14134, 13, 400, 291, 352, 646, 281, 257, 3894, 3537, 935, 689, 291, 1310, 51584], "temperature": 0.0, "avg_logprob": -0.1720795891501687, "compression_ratio": 1.4967320261437909, "no_speech_prob": 0.0030230791307985783}, {"id": 98, "seek": 72008, "start": 721.0400000000001, "end": 727.6800000000001, "text": " made the wrong decision and pursued the wrong avenue. But now you have a branching point and", "tokens": [50412, 1027, 264, 2085, 3537, 293, 34893, 264, 2085, 39230, 13, 583, 586, 291, 362, 257, 9819, 278, 935, 293, 50744], "temperature": 0.0, "avg_logprob": -0.08583907105705955, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.005052120424807072}, {"id": 99, "seek": 72008, "start": 727.6800000000001, "end": 737.36, "text": " you pursue an alternative. And in a field that is rapidly moving forward, you don't go back very", "tokens": [50744, 291, 12392, 364, 8535, 13, 400, 294, 257, 2519, 300, 307, 12910, 2684, 2128, 11, 291, 500, 380, 352, 646, 588, 51228], "temperature": 0.0, "avg_logprob": -0.08583907105705955, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.005052120424807072}, {"id": 100, "seek": 72008, "start": 737.36, "end": 743.44, "text": " far usually. You just go back to a recent paper which came out five months ago. And maybe you", "tokens": [51228, 1400, 2673, 13, 509, 445, 352, 646, 281, 257, 5162, 3035, 597, 1361, 484, 1732, 2493, 2057, 13, 400, 1310, 291, 51532], "temperature": 0.0, "avg_logprob": -0.08583907105705955, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.005052120424807072}, {"id": 101, "seek": 72008, "start": 743.44, "end": 748.6400000000001, "text": " have a little improvement there. And then maybe there's yet another little improvement there.", "tokens": [51532, 362, 257, 707, 10444, 456, 13, 400, 550, 1310, 456, 311, 1939, 1071, 707, 10444, 456, 13, 51792], "temperature": 0.0, "avg_logprob": -0.08583907105705955, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.005052120424807072}, {"id": 102, "seek": 74864, "start": 749.1999999999999, "end": 753.52, "text": " And some parts of our field are at the moment a little bit like that,", "tokens": [50392, 400, 512, 3166, 295, 527, 2519, 366, 412, 264, 1623, 257, 707, 857, 411, 300, 11, 50608], "temperature": 0.0, "avg_logprob": -0.09633948082147642, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.0016201206017285585}, {"id": 103, "seek": 74864, "start": 753.52, "end": 760.3199999999999, "text": " where PhD students are moving in, who just look at the most recent papers and then find a way of", "tokens": [50608, 689, 14476, 1731, 366, 2684, 294, 11, 567, 445, 574, 412, 264, 881, 5162, 10577, 293, 550, 915, 257, 636, 295, 50948], "temperature": 0.0, "avg_logprob": -0.09633948082147642, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.0016201206017285585}, {"id": 104, "seek": 74864, "start": 760.3199999999999, "end": 769.4399999999999, "text": " improving it a little bit and 2% better results on this particular benchmark. And then the same guys", "tokens": [50948, 11470, 309, 257, 707, 857, 293, 568, 4, 1101, 3542, 322, 341, 1729, 18927, 13, 400, 550, 264, 912, 1074, 51404], "temperature": 0.0, "avg_logprob": -0.09633948082147642, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.0016201206017285585}, {"id": 105, "seek": 74864, "start": 769.4399999999999, "end": 776.3199999999999, "text": " are also reviewing at major conferences, papers by similar students and so on. And so then sometimes", "tokens": [51404, 366, 611, 19576, 412, 2563, 22032, 11, 10577, 538, 2531, 1731, 293, 370, 322, 13, 400, 370, 550, 2171, 51748], "temperature": 0.0, "avg_logprob": -0.09633948082147642, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.0016201206017285585}, {"id": 106, "seek": 77632, "start": 776.32, "end": 785.6800000000001, "text": " what happens is that no very deep backtracking is happening, just because the actors aren't really", "tokens": [50364, 437, 2314, 307, 300, 572, 588, 2452, 646, 6903, 14134, 307, 2737, 11, 445, 570, 264, 10037, 3212, 380, 534, 50832], "temperature": 0.0, "avg_logprob": -0.08303782296559167, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.0020480698440223932}, {"id": 107, "seek": 77632, "start": 786.72, "end": 793.2800000000001, "text": " aware of the entire search tree that has already been explored in the past.", "tokens": [50884, 3650, 295, 264, 2302, 3164, 4230, 300, 575, 1217, 668, 24016, 294, 264, 1791, 13, 51212], "temperature": 0.0, "avg_logprob": -0.08303782296559167, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.0020480698440223932}, {"id": 108, "seek": 77632, "start": 794.24, "end": 802.32, "text": " On the other hand, science has this way of healing itself. And since you can gain reputation by", "tokens": [51260, 1282, 264, 661, 1011, 11, 3497, 575, 341, 636, 295, 9745, 2564, 13, 400, 1670, 291, 393, 6052, 13061, 538, 51664], "temperature": 0.0, "avg_logprob": -0.08303782296559167, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.0020480698440223932}, {"id": 109, "seek": 80232, "start": 802.72, "end": 812.08, "text": " identifying maybe more relevant points, branching points, you have this incentive within the whole", "tokens": [50384, 16696, 1310, 544, 7340, 2793, 11, 9819, 278, 2793, 11, 291, 362, 341, 22346, 1951, 264, 1379, 50852], "temperature": 0.0, "avg_logprob": -0.17967286109924316, "compression_ratio": 1.4251968503937007, "no_speech_prob": 0.003632092848420143}, {"id": 110, "seek": 80232, "start": 812.08, "end": 821.36, "text": " system to improve things as much as you can, sometimes by going back much further.", "tokens": [50852, 1185, 281, 3470, 721, 382, 709, 382, 291, 393, 11, 2171, 538, 516, 646, 709, 3052, 13, 51316], "temperature": 0.0, "avg_logprob": -0.17967286109924316, "compression_ratio": 1.4251968503937007, "no_speech_prob": 0.003632092848420143}, {"id": 111, "seek": 82136, "start": 821.92, "end": 832.08, "text": " So there's been a lot of discussion in the discourse around this concept of AI existential", "tokens": [50392, 407, 456, 311, 668, 257, 688, 295, 5017, 294, 264, 23938, 926, 341, 3410, 295, 7318, 37133, 50900], "temperature": 0.0, "avg_logprob": -0.11005098169500177, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.010056167840957642}, {"id": 112, "seek": 82136, "start": 832.08, "end": 837.92, "text": " risk. And you again, you've published quite a few pieces about this recently, prominently in", "tokens": [50900, 3148, 13, 400, 291, 797, 11, 291, 600, 6572, 1596, 257, 1326, 3755, 466, 341, 3938, 11, 39225, 2276, 294, 51192], "temperature": 0.0, "avg_logprob": -0.11005098169500177, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.010056167840957642}, {"id": 113, "seek": 82136, "start": 837.92, "end": 844.16, "text": " The Guardian and in Forbes actually. And one of the things I wanted to focus on is this concept", "tokens": [51192, 440, 27684, 293, 294, 45950, 767, 13, 400, 472, 295, 264, 721, 286, 1415, 281, 1879, 322, 307, 341, 3410, 51504], "temperature": 0.0, "avg_logprob": -0.11005098169500177, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.010056167840957642}, {"id": 114, "seek": 82136, "start": 844.16, "end": 850.8000000000001, "text": " of recursive self-improvement, because that seems to be one of the plausible explanations that these", "tokens": [51504, 295, 20560, 488, 2698, 12, 332, 46955, 518, 11, 570, 300, 2544, 281, 312, 472, 295, 264, 39925, 28708, 300, 613, 51836], "temperature": 0.0, "avg_logprob": -0.11005098169500177, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.010056167840957642}, {"id": 115, "seek": 85080, "start": 850.8, "end": 855.8399999999999, "text": " folks give. And of course, when it comes to recursive self-improvement, you are an expert in", "tokens": [50364, 4024, 976, 13, 400, 295, 1164, 11, 562, 309, 1487, 281, 20560, 488, 2698, 12, 332, 46955, 518, 11, 291, 366, 364, 5844, 294, 50616], "temperature": 0.0, "avg_logprob": -0.07308992162927405, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.004308905452489853}, {"id": 116, "seek": 85080, "start": 855.8399999999999, "end": 863.1999999999999, "text": " this field. I mean, Godel machines come to mind immediately. So I want to kind of explore asymptotes", "tokens": [50616, 341, 2519, 13, 286, 914, 11, 1265, 338, 8379, 808, 281, 1575, 4258, 13, 407, 286, 528, 281, 733, 295, 6839, 35114, 17251, 50984], "temperature": 0.0, "avg_logprob": -0.07308992162927405, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.004308905452489853}, {"id": 117, "seek": 85080, "start": 863.1999999999999, "end": 871.1999999999999, "text": " and limitations. This whole idea of recursive self-improvement is very sexy, isn't it?", "tokens": [50984, 293, 15705, 13, 639, 1379, 1558, 295, 20560, 488, 2698, 12, 332, 46955, 518, 307, 588, 13701, 11, 1943, 380, 309, 30, 51384], "temperature": 0.0, "avg_logprob": -0.07308992162927405, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.004308905452489853}, {"id": 118, "seek": 87120, "start": 871.2, "end": 884.24, "text": " In fact, it is the one idea that motivated me to do all of this. So my first paper ever in 1987,", "tokens": [50364, 682, 1186, 11, 309, 307, 264, 472, 1558, 300, 14515, 385, 281, 360, 439, 295, 341, 13, 407, 452, 700, 3035, 1562, 294, 29008, 11, 51016], "temperature": 0.0, "avg_logprob": -0.11065364495301858, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.024400558322668076}, {"id": 119, "seek": 87120, "start": 884.24, "end": 890.48, "text": " that was my diploma thesis. And it was about this recursive self-improvement thing. So it was about", "tokens": [51016, 300, 390, 452, 35770, 22288, 13, 400, 309, 390, 466, 341, 20560, 488, 2698, 12, 332, 46955, 518, 551, 13, 407, 309, 390, 466, 51328], "temperature": 0.0, "avg_logprob": -0.11065364495301858, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.024400558322668076}, {"id": 120, "seek": 87120, "start": 890.48, "end": 898.88, "text": " machine that learns something in a domain. But not only that, it also learns on top of that to", "tokens": [51328, 3479, 300, 27152, 746, 294, 257, 9274, 13, 583, 406, 787, 300, 11, 309, 611, 27152, 322, 1192, 295, 300, 281, 51748], "temperature": 0.0, "avg_logprob": -0.11065364495301858, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.024400558322668076}, {"id": 121, "seek": 89888, "start": 899.68, "end": 908.4, "text": " learn a better learning algorithm based on experience and the lower level domains. And then", "tokens": [50404, 1466, 257, 1101, 2539, 9284, 2361, 322, 1752, 293, 264, 3126, 1496, 25514, 13, 400, 550, 50840], "temperature": 0.0, "avg_logprob": -0.17182471535422586, "compression_ratio": 2.2290076335877864, "no_speech_prob": 0.003586731618270278}, {"id": 122, "seek": 89888, "start": 908.4, "end": 918.4, "text": " also recursively learns to improve the way it improves the way it learns. And then also recursively", "tokens": [50840, 611, 20560, 3413, 27152, 281, 3470, 264, 636, 309, 24771, 264, 636, 309, 27152, 13, 400, 550, 611, 20560, 3413, 51340], "temperature": 0.0, "avg_logprob": -0.17182471535422586, "compression_ratio": 2.2290076335877864, "no_speech_prob": 0.003586731618270278}, {"id": 123, "seek": 89888, "start": 918.96, "end": 926.0, "text": " learns to improve the way it improves the way it improves the way it learns. And yeah, I called that", "tokens": [51368, 27152, 281, 3470, 264, 636, 309, 24771, 264, 636, 309, 24771, 264, 636, 309, 27152, 13, 400, 1338, 11, 286, 1219, 300, 51720], "temperature": 0.0, "avg_logprob": -0.17182471535422586, "compression_ratio": 2.2290076335877864, "no_speech_prob": 0.003586731618270278}, {"id": 124, "seek": 92600, "start": 926.0, "end": 935.44, "text": " meta-learning. And back then, I had this hierarchy with, in principle, infinite self-improvement", "tokens": [50364, 19616, 12, 47204, 13, 400, 646, 550, 11, 286, 632, 341, 22333, 365, 11, 294, 8665, 11, 13785, 2698, 12, 332, 46955, 518, 50836], "temperature": 0.0, "avg_logprob": -0.09046077728271484, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.003691951045766473}, {"id": 125, "seek": 92600, "start": 935.44, "end": 943.44, "text": " in the recursive way, although it is always limited by the limited time that you run the system like", "tokens": [50836, 294, 264, 20560, 488, 636, 11, 4878, 309, 307, 1009, 5567, 538, 264, 5567, 565, 300, 291, 1190, 264, 1185, 411, 51236], "temperature": 0.0, "avg_logprob": -0.09046077728271484, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.003691951045766473}, {"id": 126, "seek": 92600, "start": 943.44, "end": 954.08, "text": " that. And then, of course, the motivation behind that is that you don't want to have an artificial", "tokens": [51236, 300, 13, 400, 550, 11, 295, 1164, 11, 264, 12335, 2261, 300, 307, 300, 291, 500, 380, 528, 281, 362, 364, 11677, 51768], "temperature": 0.0, "avg_logprob": -0.09046077728271484, "compression_ratio": 1.549738219895288, "no_speech_prob": 0.003691951045766473}, {"id": 127, "seek": 95408, "start": 954.08, "end": 961.76, "text": " system that is stuck always with the same old human-designed learning algorithm. No, you want", "tokens": [50364, 1185, 300, 307, 5541, 1009, 365, 264, 912, 1331, 1952, 12, 14792, 16690, 2539, 9284, 13, 883, 11, 291, 528, 50748], "temperature": 0.0, "avg_logprob": -0.06714867055416107, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.001671736827120185}, {"id": 128, "seek": 95408, "start": 961.76, "end": 969.2, "text": " something that improves that learning algorithm without any limitations, except for the limitations", "tokens": [50748, 746, 300, 24771, 300, 2539, 9284, 1553, 604, 15705, 11, 3993, 337, 264, 15705, 51120], "temperature": 0.0, "avg_logprob": -0.06714867055416107, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.001671736827120185}, {"id": 129, "seek": 95408, "start": 969.2, "end": 980.1600000000001, "text": " of physics and computability. And so much of what I have been doing since then is really about that.", "tokens": [51120, 295, 10649, 293, 2807, 2310, 13, 400, 370, 709, 295, 437, 286, 362, 668, 884, 1670, 550, 307, 534, 466, 300, 13, 51668], "temperature": 0.0, "avg_logprob": -0.06714867055416107, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.001671736827120185}, {"id": 130, "seek": 98016, "start": 980.24, "end": 987.04, "text": " Self-improvement in different settings where you have, on the one hand, reinforcement learning", "tokens": [50368, 16348, 12, 332, 46955, 518, 294, 819, 6257, 689, 291, 362, 11, 322, 264, 472, 1011, 11, 29280, 2539, 50708], "temperature": 0.0, "avg_logprob": -0.0949292344562078, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.008420477621257305}, {"id": 131, "seek": 98016, "start": 987.04, "end": 995.4399999999999, "text": " systems that learn in an environment to better interact and better create ways of learning", "tokens": [50708, 3652, 300, 1466, 294, 364, 2823, 281, 1101, 4648, 293, 1101, 1884, 2098, 295, 2539, 51128], "temperature": 0.0, "avg_logprob": -0.0949292344562078, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.008420477621257305}, {"id": 132, "seek": 98016, "start": 995.4399999999999, "end": 1006.88, "text": " from these interactions to learn faster and to learn to improve the way of learning faster,", "tokens": [51128, 490, 613, 13280, 281, 1466, 4663, 293, 281, 1466, 281, 3470, 264, 636, 295, 2539, 4663, 11, 51700], "temperature": 0.0, "avg_logprob": -0.0949292344562078, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.008420477621257305}, {"id": 133, "seek": 100688, "start": 1006.96, "end": 1014.32, "text": " and so on. And then also gradient-based systems, artificial neural networks, that learn through", "tokens": [50368, 293, 370, 322, 13, 400, 550, 611, 16235, 12, 6032, 3652, 11, 11677, 18161, 9590, 11, 300, 1466, 807, 50736], "temperature": 0.0, "avg_logprob": -0.10419119806850657, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.0006457947310991585}, {"id": 134, "seek": 100688, "start": 1015.52, "end": 1022.48, "text": " gradient descent, which is a pre-wired human-designed learning algorithm, to come up with a better", "tokens": [50796, 16235, 23475, 11, 597, 307, 257, 659, 12, 86, 1824, 1952, 12, 14792, 16690, 2539, 9284, 11, 281, 808, 493, 365, 257, 1101, 51144], "temperature": 0.0, "avg_logprob": -0.10419119806850657, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.0006457947310991585}, {"id": 135, "seek": 100688, "start": 1022.48, "end": 1030.64, "text": " learning algorithm that works better in a given set of environments than the original human-designed", "tokens": [51144, 2539, 9284, 300, 1985, 1101, 294, 257, 2212, 992, 295, 12388, 813, 264, 3380, 1952, 12, 14792, 16690, 51552], "temperature": 0.0, "avg_logprob": -0.10419119806850657, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.0006457947310991585}, {"id": 136, "seek": 103064, "start": 1030.64, "end": 1039.44, "text": " one. And yeah, that started around 1992 neural networks that learned to run their own learning", "tokens": [50364, 472, 13, 400, 1338, 11, 300, 1409, 926, 23952, 18161, 9590, 300, 3264, 281, 1190, 641, 1065, 2539, 50804], "temperature": 0.0, "avg_logprob": -0.10642003243969332, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.007687721401453018}, {"id": 137, "seek": 103064, "start": 1039.44, "end": 1049.0400000000002, "text": " algorithms on the recurrent network themselves. So you have a network which has standard connections", "tokens": [50804, 14642, 322, 264, 18680, 1753, 3209, 2969, 13, 407, 291, 362, 257, 3209, 597, 575, 3832, 9271, 51284], "temperature": 0.0, "avg_logprob": -0.10642003243969332, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.007687721401453018}, {"id": 138, "seek": 103064, "start": 1049.0400000000002, "end": 1054.64, "text": " and input units and output units, but then you have these special output units which are used to", "tokens": [51284, 293, 4846, 6815, 293, 5598, 6815, 11, 457, 550, 291, 362, 613, 2121, 5598, 6815, 597, 366, 1143, 281, 51564], "temperature": 0.0, "avg_logprob": -0.10642003243969332, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.007687721401453018}, {"id": 139, "seek": 105464, "start": 1054.64, "end": 1063.3600000000001, "text": " address connections within the system, within this recurrent network, and they can read and", "tokens": [50364, 2985, 9271, 1951, 264, 1185, 11, 1951, 341, 18680, 1753, 3209, 11, 293, 436, 393, 1401, 293, 50800], "temperature": 0.0, "avg_logprob": -0.0809074217273343, "compression_ratio": 1.7289156626506024, "no_speech_prob": 0.005382840055972338}, {"id": 140, "seek": 105464, "start": 1063.3600000000001, "end": 1070.48, "text": " write them. And suddenly, because it's a recurrent network and therefore it is a general-purpose", "tokens": [50800, 2464, 552, 13, 400, 5800, 11, 570, 309, 311, 257, 18680, 1753, 3209, 293, 4412, 309, 307, 257, 2674, 12, 42601, 51156], "temperature": 0.0, "avg_logprob": -0.0809074217273343, "compression_ratio": 1.7289156626506024, "no_speech_prob": 0.005382840055972338}, {"id": 141, "seek": 105464, "start": 1070.48, "end": 1080.72, "text": " computer, suddenly you can run arbitrary algorithms on this recurrent network, including arbitrary", "tokens": [51156, 3820, 11, 5800, 291, 393, 1190, 23211, 14642, 322, 341, 18680, 1753, 3209, 11, 3009, 23211, 51668], "temperature": 0.0, "avg_logprob": -0.0809074217273343, "compression_ratio": 1.7289156626506024, "no_speech_prob": 0.005382840055972338}, {"id": 142, "seek": 108072, "start": 1080.72, "end": 1086.56, "text": " learning algorithms that translate incoming signals, not only the input signals, but also the", "tokens": [50364, 2539, 14642, 300, 13799, 22341, 12354, 11, 406, 787, 264, 4846, 12354, 11, 457, 611, 264, 50656], "temperature": 0.0, "avg_logprob": -0.11427852085658483, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.004197186790406704}, {"id": 143, "seek": 108072, "start": 1086.56, "end": 1094.64, "text": " evaluation signals like reinforcement signals or error signals into weight changes, fast weight", "tokens": [50656, 13344, 12354, 411, 29280, 12354, 420, 6713, 12354, 666, 3364, 2962, 11, 2370, 3364, 51060], "temperature": 0.0, "avg_logprob": -0.11427852085658483, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.004197186790406704}, {"id": 144, "seek": 108072, "start": 1094.64, "end": 1104.0, "text": " changes, where the weight changes are not dictated any longer through this gradient descent method,", "tokens": [51060, 2962, 11, 689, 264, 3364, 2962, 366, 406, 12569, 770, 604, 2854, 807, 341, 16235, 23475, 3170, 11, 51528], "temperature": 0.0, "avg_logprob": -0.11427852085658483, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.004197186790406704}, {"id": 145, "seek": 110400, "start": 1104.08, "end": 1111.84, "text": " but no, now the network itself is learning to do that. But the initial weight matrix is still", "tokens": [50368, 457, 572, 11, 586, 264, 3209, 2564, 307, 2539, 281, 360, 300, 13, 583, 264, 5883, 3364, 8141, 307, 920, 50756], "temperature": 0.0, "avg_logprob": -0.10336069833664667, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.011323210783302784}, {"id": 146, "seek": 110400, "start": 1113.2, "end": 1117.6, "text": " learned through gradient descent, which is propagating through all these self-referential", "tokens": [50824, 3264, 807, 16235, 23475, 11, 597, 307, 12425, 990, 807, 439, 613, 2698, 12, 265, 612, 2549, 51044], "temperature": 0.0, "avg_logprob": -0.10336069833664667, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.011323210783302784}, {"id": 147, "seek": 110400, "start": 1117.6, "end": 1125.52, "text": " dynamics in a way that improves the learning algorithm running on the network itself. That", "tokens": [51044, 15679, 294, 257, 636, 300, 24771, 264, 2539, 9284, 2614, 322, 264, 3209, 2564, 13, 663, 51440], "temperature": 0.0, "avg_logprob": -0.10336069833664667, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.011323210783302784}, {"id": 148, "seek": 110400, "start": 1125.52, "end": 1132.08, "text": " was 1992, and back then, compute was really, really slow, it was a million times more expensive", "tokens": [51440, 390, 23952, 11, 293, 646, 550, 11, 14722, 390, 534, 11, 534, 2964, 11, 309, 390, 257, 2459, 1413, 544, 5124, 51768], "temperature": 0.0, "avg_logprob": -0.10336069833664667, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.011323210783302784}, {"id": 149, "seek": 113208, "start": 1132.1599999999999, "end": 1138.08, "text": " than today, and you couldn't do much with it. But now, in recent works, all of that is working", "tokens": [50368, 813, 965, 11, 293, 291, 2809, 380, 360, 709, 365, 309, 13, 583, 586, 11, 294, 5162, 1985, 11, 439, 295, 300, 307, 1364, 50664], "temperature": 0.0, "avg_logprob": -0.14786043669048107, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.004003104288130999}, {"id": 150, "seek": 113208, "start": 1138.08, "end": 1145.76, "text": " out really nicely and has become popular, and we have, just if you look at the past few years,", "tokens": [50664, 484, 534, 9594, 293, 575, 1813, 3743, 11, 293, 321, 362, 11, 445, 498, 291, 574, 412, 264, 1791, 1326, 924, 11, 51048], "temperature": 0.0, "avg_logprob": -0.14786043669048107, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.004003104288130999}, {"id": 151, "seek": 113208, "start": 1145.76, "end": 1152.8799999999999, "text": " a whole series of papers just on that. So that's the fast weight programming that you're referring", "tokens": [51048, 257, 1379, 2638, 295, 10577, 445, 322, 300, 13, 407, 300, 311, 264, 2370, 3364, 9410, 300, 291, 434, 13761, 51404], "temperature": 0.0, "avg_logprob": -0.14786043669048107, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.004003104288130999}, {"id": 152, "seek": 113208, "start": 1152.8799999999999, "end": 1161.6799999999998, "text": " to? Yes, so it's fast weight programmers where you have a part of the network that", "tokens": [51404, 281, 30, 1079, 11, 370, 309, 311, 2370, 3364, 41504, 689, 291, 362, 257, 644, 295, 264, 3209, 300, 51844], "temperature": 0.0, "avg_logprob": -0.14786043669048107, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.004003104288130999}, {"id": 153, "seek": 116168, "start": 1161.68, "end": 1170.0800000000002, "text": " learns to quickly reprogram another part of the network, or the original version of that was", "tokens": [50364, 27152, 281, 2661, 35257, 1342, 1071, 644, 295, 264, 3209, 11, 420, 264, 3380, 3037, 295, 300, 390, 50784], "temperature": 0.0, "avg_logprob": -0.09525415955520258, "compression_ratio": 2.0, "no_speech_prob": 0.0014098876854404807}, {"id": 154, "seek": 116168, "start": 1170.0800000000002, "end": 1175.04, "text": " actually two networks, so one is a slow network, and then there's another one, a fast network,", "tokens": [50784, 767, 732, 9590, 11, 370, 472, 307, 257, 2964, 3209, 11, 293, 550, 456, 311, 1071, 472, 11, 257, 2370, 3209, 11, 51032], "temperature": 0.0, "avg_logprob": -0.09525415955520258, "compression_ratio": 2.0, "no_speech_prob": 0.0014098876854404807}, {"id": 155, "seek": 116168, "start": 1175.04, "end": 1182.5600000000002, "text": " and the slow network learns to generate weight changes for the second network,", "tokens": [51032, 293, 264, 2964, 3209, 27152, 281, 8460, 3364, 2962, 337, 264, 1150, 3209, 11, 51408], "temperature": 0.0, "avg_logprob": -0.09525415955520258, "compression_ratio": 2.0, "no_speech_prob": 0.0014098876854404807}, {"id": 156, "seek": 116168, "start": 1183.3600000000001, "end": 1190.88, "text": " and the program of the second network are its weights. So the weight matrix of the second", "tokens": [51448, 293, 264, 1461, 295, 264, 1150, 3209, 366, 1080, 17443, 13, 407, 264, 3364, 8141, 295, 264, 1150, 51824], "temperature": 0.0, "avg_logprob": -0.09525415955520258, "compression_ratio": 2.0, "no_speech_prob": 0.0014098876854404807}, {"id": 157, "seek": 119088, "start": 1190.88, "end": 1196.0800000000002, "text": " network, that is the program of the second network, and the first one, what does it do? It", "tokens": [50364, 3209, 11, 300, 307, 264, 1461, 295, 264, 1150, 3209, 11, 293, 264, 700, 472, 11, 437, 775, 309, 360, 30, 467, 50624], "temperature": 0.0, "avg_logprob": -0.10262892359778994, "compression_ratio": 2.1049723756906076, "no_speech_prob": 0.0037617026828229427}, {"id": 158, "seek": 119088, "start": 1196.8000000000002, "end": 1204.72, "text": " generates outputs, it learns to generate outputs that cause weight changes in the second network,", "tokens": [50660, 23815, 23930, 11, 309, 27152, 281, 8460, 23930, 300, 3082, 3364, 2962, 294, 264, 1150, 3209, 11, 51056], "temperature": 0.0, "avg_logprob": -0.10262892359778994, "compression_ratio": 2.1049723756906076, "no_speech_prob": 0.0037617026828229427}, {"id": 159, "seek": 119088, "start": 1204.72, "end": 1209.8400000000001, "text": " and these weight changes are being applied to patterns, to input patterns, to queries, for", "tokens": [51056, 293, 613, 3364, 2962, 366, 885, 6456, 281, 8294, 11, 281, 4846, 8294, 11, 281, 24109, 11, 337, 51312], "temperature": 0.0, "avg_logprob": -0.10262892359778994, "compression_ratio": 2.1049723756906076, "no_speech_prob": 0.0037617026828229427}, {"id": 160, "seek": 119088, "start": 1209.8400000000001, "end": 1219.2, "text": " example, and then the first network essentially learns to program the second network, and essentially", "tokens": [51312, 1365, 11, 293, 550, 264, 700, 3209, 4476, 27152, 281, 1461, 264, 1150, 3209, 11, 293, 4476, 51780], "temperature": 0.0, "avg_logprob": -0.10262892359778994, "compression_ratio": 2.1049723756906076, "no_speech_prob": 0.0037617026828229427}, {"id": 161, "seek": 121920, "start": 1219.2, "end": 1225.44, "text": " the first network has a learning algorithm for the second network, and the first system of that", "tokens": [50364, 264, 700, 3209, 575, 257, 2539, 9284, 337, 264, 1150, 3209, 11, 293, 264, 700, 1185, 295, 300, 50676], "temperature": 0.0, "avg_logprob": -0.08110631595958363, "compression_ratio": 1.8454106280193237, "no_speech_prob": 0.0008035866194404662}, {"id": 162, "seek": 121920, "start": 1225.44, "end": 1234.56, "text": " kind, 1991, that was really based on on keys and values, so the first network learns to program", "tokens": [50676, 733, 11, 24097, 11, 300, 390, 534, 2361, 322, 322, 9317, 293, 4190, 11, 370, 264, 700, 3209, 27152, 281, 1461, 51132], "temperature": 0.0, "avg_logprob": -0.08110631595958363, "compression_ratio": 1.8454106280193237, "no_speech_prob": 0.0008035866194404662}, {"id": 163, "seek": 121920, "start": 1234.56, "end": 1241.04, "text": " the second network by giving it keys and values, and it says now take second network, take this", "tokens": [51132, 264, 1150, 3209, 538, 2902, 309, 9317, 293, 4190, 11, 293, 309, 1619, 586, 747, 1150, 3209, 11, 747, 341, 51456], "temperature": 0.0, "avg_logprob": -0.08110631595958363, "compression_ratio": 1.8454106280193237, "no_speech_prob": 0.0008035866194404662}, {"id": 164, "seek": 121920, "start": 1241.04, "end": 1248.96, "text": " key and this value, and associate both of them through an outer product, which just means that", "tokens": [51456, 2141, 293, 341, 2158, 11, 293, 14644, 1293, 295, 552, 807, 364, 10847, 1674, 11, 597, 445, 1355, 300, 51852], "temperature": 0.0, "avg_logprob": -0.08110631595958363, "compression_ratio": 1.8454106280193237, "no_speech_prob": 0.0008035866194404662}, {"id": 165, "seek": 124896, "start": 1248.96, "end": 1255.6000000000001, "text": " those units are strongly active, they get connected through stronger connections, and", "tokens": [50364, 729, 6815, 366, 10613, 4967, 11, 436, 483, 4582, 807, 7249, 9271, 11, 293, 50696], "temperature": 0.0, "avg_logprob": -0.12562653925511744, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.00041657642577774823}, {"id": 166, "seek": 124896, "start": 1256.88, "end": 1262.32, "text": " the mathematical way of describing that is the outer product between key and value.", "tokens": [50760, 264, 18894, 636, 295, 16141, 300, 307, 264, 10847, 1674, 1296, 2141, 293, 2158, 13, 51032], "temperature": 0.0, "avg_logprob": -0.12562653925511744, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.00041657642577774823}, {"id": 167, "seek": 124896, "start": 1263.8400000000001, "end": 1268.48, "text": " So that's how the first network would program the second network, and the important thing was that", "tokens": [51108, 407, 300, 311, 577, 264, 700, 3209, 576, 1461, 264, 1150, 3209, 11, 293, 264, 1021, 551, 390, 300, 51340], "temperature": 0.0, "avg_logprob": -0.12562653925511744, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.00041657642577774823}, {"id": 168, "seek": 124896, "start": 1268.48, "end": 1275.52, "text": " the first network had to invent good keys and good values, depending on the context of the input", "tokens": [51340, 264, 700, 3209, 632, 281, 7962, 665, 9317, 293, 665, 4190, 11, 5413, 322, 264, 4319, 295, 264, 4846, 51692], "temperature": 0.0, "avg_logprob": -0.12562653925511744, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.00041657642577774823}, {"id": 169, "seek": 127552, "start": 1275.52, "end": 1283.36, "text": " stream coming in, so it used the context to generate what is today called an attention mapping,", "tokens": [50364, 4309, 1348, 294, 11, 370, 309, 1143, 264, 4319, 281, 8460, 437, 307, 965, 1219, 364, 3202, 18350, 11, 50756], "temperature": 0.0, "avg_logprob": -0.1202575441390749, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.0019848705269396305}, {"id": 170, "seek": 127552, "start": 1283.36, "end": 1292.6399999999999, "text": " which is then being applied to queries, and this was a first step right before the most general", "tokens": [50756, 597, 307, 550, 885, 6456, 281, 24109, 11, 293, 341, 390, 257, 700, 1823, 558, 949, 264, 881, 2674, 51220], "temperature": 0.0, "avg_logprob": -0.1202575441390749, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.0019848705269396305}, {"id": 171, "seek": 127552, "start": 1294.08, "end": 1300.4, "text": " next step, which is then really about learning a learning algorithm running on the network itself", "tokens": [51292, 958, 1823, 11, 597, 307, 550, 534, 466, 2539, 257, 2539, 9284, 2614, 322, 264, 3209, 2564, 51608], "temperature": 0.0, "avg_logprob": -0.1202575441390749, "compression_ratio": 1.6327683615819208, "no_speech_prob": 0.0019848705269396305}, {"id": 172, "seek": 130040, "start": 1300.4, "end": 1303.6000000000001, "text": " for the weights of the network itself.", "tokens": [50364, 337, 264, 17443, 295, 264, 3209, 2564, 13, 50524], "temperature": 0.0, "avg_logprob": -0.0912754264058946, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0006764110876247287}, {"id": 173, "seek": 130040, "start": 1306.88, "end": 1313.52, "text": " Could I press you a tiny bit on this concept of meta-learning and convergence and asymptotes?", "tokens": [50688, 7497, 286, 1886, 291, 257, 5870, 857, 322, 341, 3410, 295, 19616, 12, 47204, 293, 32181, 293, 35114, 17251, 30, 51020], "temperature": 0.0, "avg_logprob": -0.0912754264058946, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0006764110876247287}, {"id": 174, "seek": 130040, "start": 1313.52, "end": 1320.24, "text": " Now one of the reasons I think why the X-Risk people believe that it will just go on forever", "tokens": [51020, 823, 472, 295, 264, 4112, 286, 519, 983, 264, 1783, 12, 49, 7797, 561, 1697, 300, 309, 486, 445, 352, 322, 5680, 51356], "temperature": 0.0, "avg_logprob": -0.0912754264058946, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0006764110876247287}, {"id": 175, "seek": 130040, "start": 1320.24, "end": 1326.0800000000002, "text": " is they believe in this idea of a pure intelligence, one that doesn't have physical limitations in", "tokens": [51356, 307, 436, 1697, 294, 341, 1558, 295, 257, 6075, 7599, 11, 472, 300, 1177, 380, 362, 4001, 15705, 294, 51648], "temperature": 0.0, "avg_logprob": -0.0912754264058946, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0006764110876247287}, {"id": 176, "seek": 132608, "start": 1326.08, "end": 1332.3999999999999, "text": " the real world, and I'm quite amenable to this ecological idea of intelligence that it does,", "tokens": [50364, 264, 957, 1002, 11, 293, 286, 478, 1596, 18497, 712, 281, 341, 31054, 1558, 295, 7599, 300, 309, 775, 11, 50680], "temperature": 0.0, "avg_logprob": -0.09796391912253506, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.005030778236687183}, {"id": 177, "seek": 132608, "start": 1332.3999999999999, "end": 1336.3999999999999, "text": " the world is a computer basically as well as the actual brain that we're building,", "tokens": [50680, 264, 1002, 307, 257, 3820, 1936, 382, 731, 382, 264, 3539, 3567, 300, 321, 434, 2390, 11, 50880], "temperature": 0.0, "avg_logprob": -0.09796391912253506, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.005030778236687183}, {"id": 178, "seek": 132608, "start": 1337.4399999999998, "end": 1344.0, "text": " so surely it must hit some kind of asymptote. Do you have any intuition on what those limitations", "tokens": [50932, 370, 11468, 309, 1633, 2045, 512, 733, 295, 35114, 1370, 13, 1144, 291, 362, 604, 24002, 322, 437, 729, 15705, 51260], "temperature": 0.0, "avg_logprob": -0.09796391912253506, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.005030778236687183}, {"id": 179, "seek": 132608, "start": 1344.0, "end": 1355.12, "text": " would be? So you are talking about the ongoing acceleration of computing power and limitations", "tokens": [51260, 576, 312, 30, 407, 291, 366, 1417, 466, 264, 10452, 17162, 295, 15866, 1347, 293, 15705, 51816], "temperature": 0.0, "avg_logprob": -0.09796391912253506, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.005030778236687183}, {"id": 180, "seek": 135512, "start": 1355.1999999999998, "end": 1361.84, "text": " thereof, is that what you have in mind here? Well that's one part of it, so even if you", "tokens": [50368, 456, 2670, 11, 307, 300, 437, 291, 362, 294, 1575, 510, 30, 1042, 300, 311, 472, 644, 295, 309, 11, 370, 754, 498, 291, 50700], "temperature": 0.0, "avg_logprob": -0.10629201339463056, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0009458341519348323}, {"id": 181, "seek": 135512, "start": 1361.84, "end": 1366.7199999999998, "text": " just scale transformers I think there would be some kind of asymptote, but we're talking here", "tokens": [50700, 445, 4373, 4088, 433, 286, 519, 456, 576, 312, 512, 733, 295, 35114, 1370, 11, 457, 321, 434, 1417, 510, 50944], "temperature": 0.0, "avg_logprob": -0.10629201339463056, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0009458341519348323}, {"id": 182, "seek": 135512, "start": 1366.7199999999998, "end": 1372.56, "text": " about meta-learning, learning to learn, how to learn, and recursive self-improvement, and it's", "tokens": [50944, 466, 19616, 12, 47204, 11, 2539, 281, 1466, 11, 577, 281, 1466, 11, 293, 20560, 488, 2698, 12, 332, 46955, 518, 11, 293, 309, 311, 51236], "temperature": 0.0, "avg_logprob": -0.10629201339463056, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0009458341519348323}, {"id": 183, "seek": 135512, "start": 1372.56, "end": 1377.6799999999998, "text": " similar to this idea of reflection, self-reflection and language models, it actually improves the", "tokens": [51236, 2531, 281, 341, 1558, 295, 12914, 11, 2698, 12, 33115, 5450, 293, 2856, 5245, 11, 309, 767, 24771, 264, 51492], "temperature": 0.0, "avg_logprob": -0.10629201339463056, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0009458341519348323}, {"id": 184, "seek": 135512, "start": 1377.6799999999998, "end": 1383.36, "text": " performance with successive steps of reflection and then it levels off, it reaches an asymptote.", "tokens": [51492, 3389, 365, 48043, 4439, 295, 12914, 293, 550, 309, 4358, 766, 11, 309, 14235, 364, 35114, 1370, 13, 51776], "temperature": 0.0, "avg_logprob": -0.10629201339463056, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0009458341519348323}, {"id": 185, "seek": 138336, "start": 1383.52, "end": 1387.9199999999998, "text": " I just believe that there are asymptotes everywhere and that's the reason why I", "tokens": [50372, 286, 445, 1697, 300, 456, 366, 35114, 17251, 5315, 293, 300, 311, 264, 1778, 983, 286, 50592], "temperature": 0.0, "avg_logprob": -0.11230496259836051, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.001281981822103262}, {"id": 186, "seek": 138336, "start": 1387.9199999999998, "end": 1391.6799999999998, "text": " don't think recursive self-improvement will go on forever, but I just wondered if you had", "tokens": [50592, 500, 380, 519, 20560, 488, 2698, 12, 332, 46955, 518, 486, 352, 322, 5680, 11, 457, 286, 445, 17055, 498, 291, 632, 50780], "temperature": 0.0, "avg_logprob": -0.11230496259836051, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.001281981822103262}, {"id": 187, "seek": 138336, "start": 1391.6799999999998, "end": 1395.9199999999998, "text": " any intuitions on what those impressions are. Yeah, you are totally right, there are certain", "tokens": [50780, 604, 16224, 626, 322, 437, 729, 24245, 366, 13, 865, 11, 291, 366, 3879, 558, 11, 456, 366, 1629, 50992], "temperature": 0.0, "avg_logprob": -0.11230496259836051, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.001281981822103262}, {"id": 188, "seek": 138336, "start": 1397.1999999999998, "end": 1404.32, "text": " algorithms that we have discovered in past decades which are already optimal in a way", "tokens": [51056, 14642, 300, 321, 362, 6941, 294, 1791, 7878, 597, 366, 1217, 16252, 294, 257, 636, 51412], "temperature": 0.0, "avg_logprob": -0.11230496259836051, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.001281981822103262}, {"id": 189, "seek": 138336, "start": 1405.4399999999998, "end": 1411.84, "text": " such that you cannot really improve them any further, and no self-improvement and no fancy", "tokens": [51468, 1270, 300, 291, 2644, 534, 3470, 552, 604, 3052, 11, 293, 572, 2698, 12, 332, 46955, 518, 293, 572, 10247, 51788], "temperature": 0.0, "avg_logprob": -0.11230496259836051, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.001281981822103262}, {"id": 190, "seek": 141184, "start": 1412.08, "end": 1419.36, "text": " machine will ever be able to further improve them. There are certain sorting algorithms that", "tokens": [50376, 3479, 486, 1562, 312, 1075, 281, 3052, 3470, 552, 13, 821, 366, 1629, 32411, 14642, 300, 50740], "temperature": 0.0, "avg_logprob": -0.14690766334533692, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.0011485392460599542}, {"id": 191, "seek": 141184, "start": 1419.36, "end": 1426.72, "text": " under given limitations are optimal and you can further improve them. That's one of the limits.", "tokens": [50740, 833, 2212, 15705, 366, 16252, 293, 291, 393, 3052, 3470, 552, 13, 663, 311, 472, 295, 264, 10406, 13, 51108], "temperature": 0.0, "avg_logprob": -0.14690766334533692, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.0011485392460599542}, {"id": 192, "seek": 141184, "start": 1426.72, "end": 1433.9199999999998, "text": " Then of course there are the fundamental limitations of what's computable, first identified by", "tokens": [51108, 1396, 295, 1164, 456, 366, 264, 8088, 15705, 295, 437, 311, 2807, 712, 11, 700, 9234, 538, 51468], "temperature": 0.0, "avg_logprob": -0.14690766334533692, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.0011485392460599542}, {"id": 193, "seek": 143392, "start": 1433.92, "end": 1443.04, "text": " Kurt G\u00f6del in 1931, he just showed that there are certain things that no computational process", "tokens": [50364, 26168, 47894, 18105, 294, 1294, 12967, 11, 415, 445, 4712, 300, 456, 366, 1629, 721, 300, 572, 28270, 1399, 50820], "temperature": 0.0, "avg_logprob": -0.12612187660346597, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.011857791803777218}, {"id": 194, "seek": 143392, "start": 1443.6000000000001, "end": 1453.6000000000001, "text": " can ever achieve. No computational theorem prover can prove or disprove certain theorems", "tokens": [50848, 393, 1562, 4584, 13, 883, 28270, 20904, 447, 331, 393, 7081, 420, 717, 46955, 1629, 10299, 2592, 51348], "temperature": 0.0, "avg_logprob": -0.12612187660346597, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.011857791803777218}, {"id": 195, "seek": 143392, "start": 1453.6000000000001, "end": 1462.72, "text": " in a language, in a symbolic language that is powerful enough to encode", "tokens": [51348, 294, 257, 2856, 11, 294, 257, 25755, 2856, 300, 307, 4005, 1547, 281, 2058, 1429, 51804], "temperature": 0.0, "avg_logprob": -0.12612187660346597, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.011857791803777218}, {"id": 196, "seek": 146272, "start": 1462.72, "end": 1469.76, "text": " certain simple principles of arithmetic and stuff like that. What he showed was that", "tokens": [50364, 1629, 2199, 9156, 295, 42973, 293, 1507, 411, 300, 13, 708, 415, 4712, 390, 300, 50716], "temperature": 0.0, "avg_logprob": -0.1670409922014203, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0017259051091969013}, {"id": 197, "seek": 146272, "start": 1471.04, "end": 1476.8, "text": " there are fundamental limitations to all of computation and therefore there are fundamental", "tokens": [50780, 456, 366, 8088, 15705, 281, 439, 295, 24903, 293, 4412, 456, 366, 8088, 51068], "temperature": 0.0, "avg_logprob": -0.1670409922014203, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0017259051091969013}, {"id": 198, "seek": 146272, "start": 1476.8, "end": 1487.28, "text": " limitations to any AI based on computation. I'm glad you brought that topic up because it's one of", "tokens": [51068, 15705, 281, 604, 7318, 2361, 322, 24903, 13, 286, 478, 5404, 291, 3038, 300, 4829, 493, 570, 309, 311, 472, 295, 51592], "temperature": 0.0, "avg_logprob": -0.1670409922014203, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0017259051091969013}, {"id": 199, "seek": 148728, "start": 1488.24, "end": 1494.8, "text": " our favorite things to discuss which is do you think the human mind ultimately reduces to just", "tokens": [50412, 527, 2954, 721, 281, 2248, 597, 307, 360, 291, 519, 264, 1952, 1575, 6284, 18081, 281, 445, 50740], "temperature": 0.0, "avg_logprob": -0.1458632787068685, "compression_ratio": 1.6566265060240963, "no_speech_prob": 0.014240937307476997}, {"id": 200, "seek": 148728, "start": 1494.8, "end": 1500.08, "text": " an effective computation and so subject to those same limits or do you think there's any", "tokens": [50740, 364, 4942, 24903, 293, 370, 3983, 281, 729, 912, 10406, 420, 360, 291, 519, 456, 311, 604, 51004], "temperature": 0.0, "avg_logprob": -0.1458632787068685, "compression_ratio": 1.6566265060240963, "no_speech_prob": 0.014240937307476997}, {"id": 201, "seek": 148728, "start": 1501.84, "end": 1507.92, "text": " known or unknown physics that give us some out in which the brain can do a computation that", "tokens": [51092, 2570, 420, 9841, 10649, 300, 976, 505, 512, 484, 294, 597, 264, 3567, 393, 360, 257, 24903, 300, 51396], "temperature": 0.0, "avg_logprob": -0.1458632787068685, "compression_ratio": 1.6566265060240963, "no_speech_prob": 0.014240937307476997}, {"id": 202, "seek": 150792, "start": 1507.92, "end": 1518.72, "text": " amounts to hypercomputation? Since we have no evidence that the brain can compute something", "tokens": [50364, 11663, 281, 9848, 1112, 2582, 399, 30, 4162, 321, 362, 572, 4467, 300, 264, 3567, 393, 14722, 746, 50904], "temperature": 0.0, "avg_logprob": -0.1571493148803711, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.01448202133178711}, {"id": 203, "seek": 150792, "start": 1519.28, "end": 1527.52, "text": " that is not computable in the traditional sense, in G\u00f6del sense and torings and churches sense", "tokens": [50932, 300, 307, 406, 2807, 712, 294, 264, 5164, 2020, 11, 294, 47894, 18105, 2020, 293, 3930, 1109, 293, 15381, 2020, 51344], "temperature": 0.0, "avg_logprob": -0.1571493148803711, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.01448202133178711}, {"id": 204, "seek": 150792, "start": 1527.52, "end": 1536.48, "text": " and everybody who has worked on this field, since we have no evidence we shouldn't assume that's the", "tokens": [51344, 293, 2201, 567, 575, 2732, 322, 341, 2519, 11, 1670, 321, 362, 572, 4467, 321, 4659, 380, 6552, 300, 311, 264, 51792], "temperature": 0.0, "avg_logprob": -0.1571493148803711, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.01448202133178711}, {"id": 205, "seek": 153648, "start": 1536.56, "end": 1544.88, "text": " case. As soon as someone shows that people can compute certain things or prove certain theorems", "tokens": [50368, 1389, 13, 1018, 2321, 382, 1580, 3110, 300, 561, 393, 14722, 1629, 721, 420, 7081, 1629, 10299, 2592, 50784], "temperature": 0.0, "avg_logprob": -0.07911779880523681, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0028860773891210556}, {"id": 206, "seek": 153648, "start": 1544.88, "end": 1555.84, "text": " that machines cannot prove given the same initial conditions, we should look more closely but", "tokens": [50784, 300, 8379, 2644, 7081, 2212, 264, 912, 5883, 4487, 11, 321, 820, 574, 544, 8185, 457, 51332], "temperature": 0.0, "avg_logprob": -0.07911779880523681, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0028860773891210556}, {"id": 207, "seek": 153648, "start": 1556.96, "end": 1564.08, "text": " there are many things that might be possible in fairy tales and we are not really exploring them", "tokens": [51388, 456, 366, 867, 721, 300, 1062, 312, 1944, 294, 19104, 27254, 293, 321, 366, 406, 534, 12736, 552, 51744], "temperature": 0.0, "avg_logprob": -0.07911779880523681, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0028860773891210556}, {"id": 208, "seek": 156408, "start": 1564.08, "end": 1572.24, "text": " because the probability of coming up with interesting results is so low. Fair enough,", "tokens": [50364, 570, 264, 8482, 295, 1348, 493, 365, 1880, 3542, 307, 370, 2295, 13, 12157, 1547, 11, 50772], "temperature": 0.0, "avg_logprob": -0.10490124135077754, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.005132556892931461}, {"id": 209, "seek": 156408, "start": 1572.24, "end": 1578.0, "text": " so you mentioned so far two asymptotes, one being of the mathematical kind where there's just", "tokens": [50772, 370, 291, 2835, 370, 1400, 732, 35114, 17251, 11, 472, 885, 295, 264, 18894, 733, 689, 456, 311, 445, 51060], "temperature": 0.0, "avg_logprob": -0.10490124135077754, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.005132556892931461}, {"id": 210, "seek": 156408, "start": 1578.56, "end": 1582.8799999999999, "text": " mathematical proofs that certain things are optimal, the other one being the limits of", "tokens": [51088, 18894, 8177, 82, 300, 1629, 721, 366, 16252, 11, 264, 661, 472, 885, 264, 10406, 295, 51304], "temperature": 0.0, "avg_logprob": -0.10490124135077754, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.005132556892931461}, {"id": 211, "seek": 156408, "start": 1582.8799999999999, "end": 1589.6799999999998, "text": " computation itself. What other asymptotes do you see applying to or putting bounds on recursive", "tokens": [51304, 24903, 2564, 13, 708, 661, 35114, 17251, 360, 291, 536, 9275, 281, 420, 3372, 29905, 322, 20560, 488, 51644], "temperature": 0.0, "avg_logprob": -0.10490124135077754, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.005132556892931461}, {"id": 212, "seek": 158968, "start": 1589.68, "end": 1602.0800000000002, "text": " self-improvement? The most obvious thing is probably light speed and the limits of physical", "tokens": [50364, 2698, 12, 332, 46955, 518, 30, 440, 881, 6322, 551, 307, 1391, 1442, 3073, 293, 264, 10406, 295, 4001, 50984], "temperature": 0.0, "avg_logprob": -0.19373201769451762, "compression_ratio": 1.323943661971831, "no_speech_prob": 0.0026268181391060352}, {"id": 213, "seek": 158968, "start": 1602.0800000000002, "end": 1611.76, "text": " computation. We know those for several decades, we have happily enjoyed the fact that every five", "tokens": [50984, 24903, 13, 492, 458, 729, 337, 2940, 7878, 11, 321, 362, 19909, 4626, 264, 1186, 300, 633, 1732, 51468], "temperature": 0.0, "avg_logprob": -0.19373201769451762, "compression_ratio": 1.323943661971831, "no_speech_prob": 0.0026268181391060352}, {"id": 214, "seek": 161176, "start": 1611.76, "end": 1620.96, "text": " years compute is getting 10 times cheaper and this process started long before Moore's law was", "tokens": [50364, 924, 14722, 307, 1242, 1266, 1413, 12284, 293, 341, 1399, 1409, 938, 949, 21644, 311, 2101, 390, 50824], "temperature": 0.0, "avg_logprob": -0.19685460753360037, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.04133474826812744}, {"id": 215, "seek": 161176, "start": 1622.96, "end": 1631.36, "text": " defined in the 60s I believe because even in 1941 already when Susie built the first program", "tokens": [50924, 7642, 294, 264, 4060, 82, 286, 1697, 570, 754, 294, 35364, 1217, 562, 9545, 414, 3094, 264, 700, 1461, 51344], "temperature": 0.0, "avg_logprob": -0.19685460753360037, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.04133474826812744}, {"id": 216, "seek": 161176, "start": 1631.36, "end": 1639.36, "text": " controlled computer this law apparently was active so back then he could compute maybe one", "tokens": [51344, 10164, 3820, 341, 2101, 7970, 390, 4967, 370, 646, 550, 415, 727, 14722, 1310, 472, 51744], "temperature": 0.0, "avg_logprob": -0.19685460753360037, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.04133474826812744}, {"id": 217, "seek": 163936, "start": 1639.36, "end": 1645.1999999999998, "text": " instruction per second and since then every 10 years a factor of 100 every 30 years a factor of", "tokens": [50364, 10951, 680, 1150, 293, 1670, 550, 633, 1266, 924, 257, 5952, 295, 2319, 633, 2217, 924, 257, 5952, 295, 50656], "temperature": 0.0, "avg_logprob": -0.11341796288123497, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.016356514766812325}, {"id": 218, "seek": 163936, "start": 1645.1999999999998, "end": 1655.52, "text": " a million more or less until today and there's no reason to believe it won't hold for a couple", "tokens": [50656, 257, 2459, 544, 420, 1570, 1826, 965, 293, 456, 311, 572, 1778, 281, 1697, 309, 1582, 380, 1797, 337, 257, 1916, 51172], "temperature": 0.0, "avg_logprob": -0.11341796288123497, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.016356514766812325}, {"id": 219, "seek": 163936, "start": 1655.52, "end": 1663.4399999999998, "text": " of additional decades because the physical limits are much further out. The physical limits that we", "tokens": [51172, 295, 4497, 7878, 570, 264, 4001, 10406, 366, 709, 3052, 484, 13, 440, 4001, 10406, 300, 321, 51568], "temperature": 0.0, "avg_logprob": -0.11341796288123497, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.016356514766812325}, {"id": 220, "seek": 166344, "start": 1663.44, "end": 1673.1200000000001, "text": " know are the Bremermann limit discovered I think in 1983 by Bremermann and they basically say that", "tokens": [50364, 458, 366, 264, 7090, 936, 14912, 4948, 6941, 286, 519, 294, 31758, 538, 7090, 936, 14912, 293, 436, 1936, 584, 300, 50848], "temperature": 0.0, "avg_logprob": -0.10732895486495074, "compression_ratio": 1.5053763440860215, "no_speech_prob": 0.009391632862389088}, {"id": 221, "seek": 166344, "start": 1673.1200000000001, "end": 1678.64, "text": " one kilogram of matter cannot compute more than 10 to the 51 instructions per second.", "tokens": [50848, 472, 21741, 295, 1871, 2644, 14722, 544, 813, 1266, 281, 264, 18485, 9415, 680, 1150, 13, 51124], "temperature": 0.0, "avg_logprob": -0.10732895486495074, "compression_ratio": 1.5053763440860215, "no_speech_prob": 0.009391632862389088}, {"id": 222, "seek": 166344, "start": 1680.0800000000002, "end": 1687.3600000000001, "text": " So that's a lot of compute but it's limited and to give you an idea of how much compute that is", "tokens": [51196, 407, 300, 311, 257, 688, 295, 14722, 457, 309, 311, 5567, 293, 281, 976, 291, 364, 1558, 295, 577, 709, 14722, 300, 307, 51560], "temperature": 0.0, "avg_logprob": -0.10732895486495074, "compression_ratio": 1.5053763440860215, "no_speech_prob": 0.009391632862389088}, {"id": 223, "seek": 168736, "start": 1688.0, "end": 1696.24, "text": " I also have a kilogram of computer in here and probably it cannot compute 10 to the 20", "tokens": [50396, 286, 611, 362, 257, 21741, 295, 3820, 294, 510, 293, 1391, 309, 2644, 14722, 1266, 281, 264, 945, 50808], "temperature": 0.0, "avg_logprob": -0.06438296416710163, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.011675885878503323}, {"id": 224, "seek": 168736, "start": 1696.24, "end": 1704.24, "text": " instructions per second otherwise my head would explode because of the heat problem", "tokens": [50808, 9415, 680, 1150, 5911, 452, 1378, 576, 21411, 570, 295, 264, 3738, 1154, 51208], "temperature": 0.0, "avg_logprob": -0.06438296416710163, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.011675885878503323}, {"id": 225, "seek": 168736, "start": 1705.84, "end": 1711.6799999999998, "text": " but maybe it can compute something that is not so far from 10 to the 20 instructions maybe 10", "tokens": [51288, 457, 1310, 309, 393, 14722, 746, 300, 307, 406, 370, 1400, 490, 1266, 281, 264, 945, 9415, 1310, 1266, 51580], "temperature": 0.0, "avg_logprob": -0.06438296416710163, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.011675885878503323}, {"id": 226, "seek": 171168, "start": 1711.68, "end": 1719.28, "text": " to the 17 something like that although most of my neurons are not active as we speak because again", "tokens": [50364, 281, 264, 3282, 746, 411, 300, 4878, 881, 295, 452, 22027, 366, 406, 4967, 382, 321, 1710, 570, 797, 50744], "temperature": 0.0, "avg_logprob": -0.09386232006016063, "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.010806763544678688}, {"id": 227, "seek": 171168, "start": 1719.28, "end": 1727.76, "text": " otherwise my head would just evaporate. Now if you have an upper limit of 10 to the 20 instructions", "tokens": [50744, 5911, 452, 1378, 576, 445, 26315, 473, 13, 823, 498, 291, 362, 364, 6597, 4948, 295, 1266, 281, 264, 945, 9415, 51168], "temperature": 0.0, "avg_logprob": -0.09386232006016063, "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.010806763544678688}, {"id": 228, "seek": 171168, "start": 1727.76, "end": 1737.52, "text": " per brain then the upper limit of all of humankind would be 10 billion times that individual limit", "tokens": [51168, 680, 3567, 550, 264, 6597, 4948, 295, 439, 295, 1484, 40588, 576, 312, 1266, 5218, 1413, 300, 2609, 4948, 51656], "temperature": 0.0, "avg_logprob": -0.09386232006016063, "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.010806763544678688}, {"id": 229, "seek": 173752, "start": 1737.6, "end": 1744.48, "text": " and that would be 10 to the 30 instructions per second and you see it's still far away from the", "tokens": [50368, 293, 300, 576, 312, 1266, 281, 264, 2217, 9415, 680, 1150, 293, 291, 536, 309, 311, 920, 1400, 1314, 490, 264, 50712], "temperature": 0.0, "avg_logprob": -0.10395745797590776, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.002355156233534217}, {"id": 230, "seek": 173752, "start": 1744.48, "end": 1751.52, "text": " 10 to the 51 instructions per second that in principle one kilogram of matter could compute", "tokens": [50712, 1266, 281, 264, 18485, 9415, 680, 1150, 300, 294, 8665, 472, 21741, 295, 1871, 727, 14722, 51064], "temperature": 0.0, "avg_logprob": -0.10395745797590776, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.002355156233534217}, {"id": 231, "seek": 173752, "start": 1751.52, "end": 1760.6399999999999, "text": " and now we have more than 10 to the 30 kilograms of matter in the solar system and there's some", "tokens": [51064, 293, 586, 321, 362, 544, 813, 1266, 281, 264, 2217, 30690, 295, 1871, 294, 264, 7936, 1185, 293, 456, 311, 512, 51520], "temperature": 0.0, "avg_logprob": -0.10395745797590776, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.002355156233534217}, {"id": 232, "seek": 176064, "start": 1761.6000000000001, "end": 1769.1200000000001, "text": " and so if the current trend continues at some point much of that is going to be used for", "tokens": [50412, 293, 370, 498, 264, 2190, 6028, 6515, 412, 512, 935, 709, 295, 300, 307, 516, 281, 312, 1143, 337, 50788], "temperature": 0.0, "avg_logprob": -0.0796137317534416, "compression_ratio": 1.69375, "no_speech_prob": 0.0032709783408790827}, {"id": 233, "seek": 176064, "start": 1769.1200000000001, "end": 1775.8400000000001, "text": " computation but then it will have to slow down even if the exponential acceleration", "tokens": [50788, 24903, 457, 550, 309, 486, 362, 281, 2964, 760, 754, 498, 264, 21510, 17162, 51124], "temperature": 0.0, "avg_logprob": -0.0796137317534416, "compression_ratio": 1.69375, "no_speech_prob": 0.0032709783408790827}, {"id": 234, "seek": 176064, "start": 1776.88, "end": 1784.16, "text": " will still be with us for a couple of decades because at some point it is going to be a polynomial", "tokens": [51176, 486, 920, 312, 365, 505, 337, 257, 1916, 295, 7878, 570, 412, 512, 935, 309, 307, 516, 281, 312, 257, 26110, 51540], "temperature": 0.0, "avg_logprob": -0.0796137317534416, "compression_ratio": 1.69375, "no_speech_prob": 0.0032709783408790827}, {"id": 235, "seek": 178416, "start": 1784.72, "end": 1792.4, "text": " because due to the limits of light speed at some point it will be harder and harder", "tokens": [50392, 570, 3462, 281, 264, 10406, 295, 1442, 3073, 412, 512, 935, 309, 486, 312, 6081, 293, 6081, 50776], "temperature": 0.0, "avg_logprob": -0.04723863355044661, "compression_ratio": 1.6728395061728396, "no_speech_prob": 0.006484556943178177}, {"id": 236, "seek": 178416, "start": 1792.4, "end": 1797.8400000000001, "text": " to acquire additional mass once you have reached the limits of physical computation per kilogram", "tokens": [50776, 281, 20001, 4497, 2758, 1564, 291, 362, 6488, 264, 10406, 295, 4001, 24903, 680, 21741, 51048], "temperature": 0.0, "avg_logprob": -0.04723863355044661, "compression_ratio": 1.6728395061728396, "no_speech_prob": 0.006484556943178177}, {"id": 237, "seek": 178416, "start": 1797.8400000000001, "end": 1804.5600000000002, "text": " the only way to expand is to go outwards and you know find additional stars and additional", "tokens": [51048, 264, 787, 636, 281, 5268, 307, 281, 352, 484, 2015, 293, 291, 458, 915, 4497, 6105, 293, 4497, 51384], "temperature": 0.0, "avg_logprob": -0.04723863355044661, "compression_ratio": 1.6728395061728396, "no_speech_prob": 0.006484556943178177}, {"id": 238, "seek": 180456, "start": 1804.56, "end": 1816.48, "text": " matter further away from the solar system and then you will get a polynomial acceleration or", "tokens": [50364, 1871, 3052, 1314, 490, 264, 7936, 1185, 293, 550, 291, 486, 483, 257, 26110, 17162, 420, 50960], "temperature": 0.0, "avg_logprob": -0.09557002782821655, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.007230678107589483}, {"id": 239, "seek": 180456, "start": 1816.48, "end": 1823.52, "text": " a polynomial growth at best so it will be much worse than the current exponential", "tokens": [50960, 257, 26110, 4599, 412, 1151, 370, 309, 486, 312, 709, 5324, 813, 264, 2190, 21510, 51312], "temperature": 0.0, "avg_logprob": -0.09557002782821655, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.007230678107589483}, {"id": 240, "seek": 180456, "start": 1823.52, "end": 1831.44, "text": " growth that we are still enjoying. Sure but I would say you know the existential threat", "tokens": [51312, 4599, 300, 321, 366, 920, 9929, 13, 4894, 457, 286, 576, 584, 291, 458, 264, 37133, 4734, 51708], "temperature": 0.0, "avg_logprob": -0.09557002782821655, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.007230678107589483}, {"id": 241, "seek": 183144, "start": 1831.52, "end": 1837.52, "text": " that is more than sufficient to supply an existential threat and let me just put this", "tokens": [50368, 300, 307, 544, 813, 11563, 281, 5847, 364, 37133, 4734, 293, 718, 385, 445, 829, 341, 50668], "temperature": 0.0, "avg_logprob": -0.1560431586371528, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.00497839180752635}, {"id": 242, "seek": 183144, "start": 1837.52, "end": 1841.76, "text": " a little bit differently which is and I agree with you on this which is you are quoted as", "tokens": [50668, 257, 707, 857, 7614, 597, 307, 293, 286, 3986, 365, 291, 322, 341, 597, 307, 291, 366, 30047, 382, 50880], "temperature": 0.0, "avg_logprob": -0.1560431586371528, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.00497839180752635}, {"id": 243, "seek": 183144, "start": 1841.76, "end": 1846.0, "text": " saying that traditional humans won't play a significant role in spreading intelligence", "tokens": [50880, 1566, 300, 5164, 6255, 1582, 380, 862, 257, 4776, 3090, 294, 15232, 7599, 51092], "temperature": 0.0, "avg_logprob": -0.1560431586371528, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.00497839180752635}, {"id": 244, "seek": 183144, "start": 1846.0, "end": 1850.72, "text": " across the universe and I think you are right I think we kind of share a vision of something", "tokens": [51092, 2108, 264, 6445, 293, 286, 519, 291, 366, 558, 286, 519, 321, 733, 295, 2073, 257, 5201, 295, 746, 51328], "temperature": 0.0, "avg_logprob": -0.1560431586371528, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.00497839180752635}, {"id": 245, "seek": 183144, "start": 1850.72, "end": 1857.68, "text": " like the von Neumann probes that go out into space and form this star spanning civilization of", "tokens": [51328, 411, 264, 2957, 1734, 449, 969, 1239, 279, 300, 352, 484, 666, 1901, 293, 1254, 341, 3543, 47626, 18036, 295, 51676], "temperature": 0.0, "avg_logprob": -0.1560431586371528, "compression_ratio": 1.7110266159695817, "no_speech_prob": 0.00497839180752635}, {"id": 246, "seek": 185768, "start": 1857.68, "end": 1863.1200000000001, "text": " machines and artificial intelligence that have transcended you know biological limitations", "tokens": [50364, 8379, 293, 11677, 7599, 300, 362, 43800, 3502, 291, 458, 13910, 15705, 50636], "temperature": 0.0, "avg_logprob": -0.08861613273620605, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.030568135902285576}, {"id": 247, "seek": 185768, "start": 1863.1200000000001, "end": 1869.1200000000001, "text": " so I guess my question to you is once that space faring star spanning you know civilization", "tokens": [50636, 370, 286, 2041, 452, 1168, 281, 291, 307, 1564, 300, 1901, 1400, 278, 3543, 47626, 291, 458, 18036, 50936], "temperature": 0.0, "avg_logprob": -0.08861613273620605, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.030568135902285576}, {"id": 248, "seek": 185768, "start": 1869.1200000000001, "end": 1876.48, "text": " exists if it becomes misaligned with us and decides that we are in the way right isn't that", "tokens": [50936, 8198, 498, 309, 3643, 3346, 304, 16690, 365, 505, 293, 14898, 300, 321, 366, 294, 264, 636, 558, 1943, 380, 300, 51304], "temperature": 0.0, "avg_logprob": -0.08861613273620605, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.030568135902285576}, {"id": 249, "seek": 185768, "start": 1876.48, "end": 1881.28, "text": " an existential threat I mean might they just you know repurpose the earth regardless of whether", "tokens": [51304, 364, 37133, 4734, 286, 914, 1062, 436, 445, 291, 458, 1085, 31345, 264, 4120, 10060, 295, 1968, 51544], "temperature": 0.0, "avg_logprob": -0.08861613273620605, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.030568135902285576}, {"id": 250, "seek": 188128, "start": 1881.28, "end": 1888.24, "text": " we're here or not for for their own aims yeah I'm often getting these questions and", "tokens": [50364, 321, 434, 510, 420, 406, 337, 337, 641, 1065, 24683, 1338, 286, 478, 2049, 1242, 613, 1651, 293, 50712], "temperature": 0.0, "avg_logprob": -0.09293029619299847, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.005806629080325365}, {"id": 251, "seek": 188128, "start": 1889.44, "end": 1900.48, "text": " and there is no proof that we will be safe forever or something like that on the other hand it's also", "tokens": [50772, 293, 456, 307, 572, 8177, 300, 321, 486, 312, 3273, 5680, 420, 746, 411, 300, 322, 264, 661, 1011, 309, 311, 611, 51324], "temperature": 0.0, "avg_logprob": -0.09293029619299847, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.005806629080325365}, {"id": 252, "seek": 188128, "start": 1901.84, "end": 1910.3999999999999, "text": " very clear as far as I can judge that all of this cannot be stopped and it can be channeled", "tokens": [51392, 588, 1850, 382, 1400, 382, 286, 393, 6995, 300, 439, 295, 341, 2644, 312, 5936, 293, 309, 393, 312, 2269, 292, 51820], "temperature": 0.0, "avg_logprob": -0.09293029619299847, "compression_ratio": 1.6198830409356726, "no_speech_prob": 0.005806629080325365}, {"id": 253, "seek": 191040, "start": 1910.4, "end": 1920.8000000000002, "text": " in a very natural and I think good way in a way that is good for humankind now", "tokens": [50364, 294, 257, 588, 3303, 293, 286, 519, 665, 636, 294, 257, 636, 300, 307, 665, 337, 1484, 40588, 586, 50884], "temperature": 0.0, "avg_logprob": -0.06958648988178798, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.001113934675231576}, {"id": 254, "seek": 191040, "start": 1922.4, "end": 1928.88, "text": " first of all at the moment we have a tremendous bias towards good AI", "tokens": [50964, 700, 295, 439, 412, 264, 1623, 321, 362, 257, 10048, 12577, 3030, 665, 7318, 51288], "temperature": 0.0, "avg_logprob": -0.06958648988178798, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.001113934675231576}, {"id": 255, "seek": 191040, "start": 1931.2, "end": 1938.88, "text": " meaning AI that is good for humans why because there is this intense commercial pressure", "tokens": [51404, 3620, 7318, 300, 307, 665, 337, 6255, 983, 570, 456, 307, 341, 9447, 6841, 3321, 51788], "temperature": 0.0, "avg_logprob": -0.06958648988178798, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.001113934675231576}, {"id": 256, "seek": 193888, "start": 1938.88, "end": 1946.0, "text": " to create stuff that humans want to buy and they like to buy only stuff they think is good", "tokens": [50364, 281, 1884, 1507, 300, 6255, 528, 281, 2256, 293, 436, 411, 281, 2256, 787, 1507, 436, 519, 307, 665, 50720], "temperature": 0.0, "avg_logprob": -0.06767819076776505, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.0032176943495869637}, {"id": 257, "seek": 193888, "start": 1946.0, "end": 1954.0, "text": " for them which means that all the companies that are and that are trying to devise AI products", "tokens": [50720, 337, 552, 597, 1355, 300, 439, 264, 3431, 300, 366, 293, 300, 366, 1382, 281, 1905, 908, 7318, 3383, 51120], "temperature": 0.0, "avg_logprob": -0.06767819076776505, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.0032176943495869637}, {"id": 258, "seek": 193888, "start": 1954.0, "end": 1962.24, "text": " they are maximally incentivized to generate AI products that are good for those guys who are", "tokens": [51120, 436, 366, 5138, 379, 35328, 1602, 281, 8460, 7318, 3383, 300, 366, 665, 337, 729, 1074, 567, 366, 51532], "temperature": 0.0, "avg_logprob": -0.06767819076776505, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.0032176943495869637}, {"id": 259, "seek": 196224, "start": 1962.24, "end": 1970.64, "text": " buying them or at least where the where the customers think it's good for them", "tokens": [50364, 6382, 552, 420, 412, 1935, 689, 264, 689, 264, 4581, 519, 309, 311, 665, 337, 552, 50784], "temperature": 0.0, "avg_logprob": -0.14668616329330997, "compression_ratio": 1.7376237623762376, "no_speech_prob": 0.014044559560716152}, {"id": 260, "seek": 196224, "start": 1972.08, "end": 1979.92, "text": " so it is still 95 so it may be five percent of all AI researchers really about AI weapons and", "tokens": [50856, 370, 309, 307, 920, 13420, 370, 309, 815, 312, 1732, 3043, 295, 439, 7318, 10309, 534, 466, 7318, 7278, 293, 51248], "temperature": 0.0, "avg_logprob": -0.14668616329330997, "compression_ratio": 1.7376237623762376, "no_speech_prob": 0.014044559560716152}, {"id": 261, "seek": 196224, "start": 1979.92, "end": 1986.0, "text": " one has to be worried about that when all this has to be worried about weapons research but", "tokens": [51248, 472, 575, 281, 312, 5804, 466, 300, 562, 439, 341, 575, 281, 312, 5804, 466, 7278, 2132, 457, 51552], "temperature": 0.0, "avg_logprob": -0.14668616329330997, "compression_ratio": 1.7376237623762376, "no_speech_prob": 0.014044559560716152}, {"id": 262, "seek": 196224, "start": 1986.0, "end": 1990.8, "text": " there's a tremendous bias towards good AI so that is one of the reasons why you can be", "tokens": [51552, 456, 311, 257, 10048, 12577, 3030, 665, 7318, 370, 300, 307, 472, 295, 264, 4112, 983, 291, 393, 312, 51792], "temperature": 0.0, "avg_logprob": -0.14668616329330997, "compression_ratio": 1.7376237623762376, "no_speech_prob": 0.014044559560716152}, {"id": 263, "seek": 199080, "start": 1991.44, "end": 1997.9199999999998, "text": " a little bit optimistic for the future I'm always trying to point out the two types of", "tokens": [50396, 257, 707, 857, 19397, 337, 264, 2027, 286, 478, 1009, 1382, 281, 935, 484, 264, 732, 3467, 295, 50720], "temperature": 0.0, "avg_logprob": -0.08726636446439302, "compression_ratio": 1.6687116564417177, "no_speech_prob": 0.008044318296015263}, {"id": 264, "seek": 199080, "start": 1997.9199999999998, "end": 2011.84, "text": " AIs there are those who are just tools of users human human users and the others that invent", "tokens": [50720, 316, 6802, 456, 366, 729, 567, 366, 445, 3873, 295, 5022, 1952, 1952, 5022, 293, 264, 2357, 300, 7962, 51416], "temperature": 0.0, "avg_logprob": -0.08726636446439302, "compression_ratio": 1.6687116564417177, "no_speech_prob": 0.008044318296015263}, {"id": 265, "seek": 199080, "start": 2011.84, "end": 2019.52, "text": " their own goals and they pursue their own goals and both of them we have had for a long time", "tokens": [51416, 641, 1065, 5493, 293, 436, 12392, 641, 1065, 5493, 293, 1293, 295, 552, 321, 362, 632, 337, 257, 938, 565, 51800], "temperature": 0.0, "avg_logprob": -0.08726636446439302, "compression_ratio": 1.6687116564417177, "no_speech_prob": 0.008044318296015263}, {"id": 266, "seek": 201952, "start": 2020.16, "end": 2026.4, "text": " now for the AI tools it's kind of clear there's a human and a human wants to achieve something", "tokens": [50396, 586, 337, 264, 7318, 3873, 309, 311, 733, 295, 1850, 456, 311, 257, 1952, 293, 257, 1952, 2738, 281, 4584, 746, 50708], "temperature": 0.0, "avg_logprob": -0.08884648422696698, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.0023866009432822466}, {"id": 267, "seek": 201952, "start": 2026.4, "end": 2035.68, "text": " and so it uses he uses or she uses that tool to achieve certain ends and and most of those are", "tokens": [50708, 293, 370, 309, 4960, 415, 4960, 420, 750, 4960, 300, 2290, 281, 4584, 1629, 5314, 293, 293, 881, 295, 729, 366, 51172], "temperature": 0.0, "avg_logprob": -0.08884648422696698, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.0023866009432822466}, {"id": 268, "seek": 201952, "start": 2036.32, "end": 2046.0, "text": " of the type let's improve healthcare and let's facilitate translation from one person to another", "tokens": [51204, 295, 264, 2010, 718, 311, 3470, 8884, 293, 718, 311, 20207, 12853, 490, 472, 954, 281, 1071, 51688], "temperature": 0.0, "avg_logprob": -0.08884648422696698, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.0023866009432822466}, {"id": 269, "seek": 204600, "start": 2046.0, "end": 2053.36, "text": " one in another nation and just make life easier and make human lives longer and healthier", "tokens": [50364, 472, 294, 1071, 4790, 293, 445, 652, 993, 3571, 293, 652, 1952, 2909, 2854, 293, 19580, 50732], "temperature": 0.0, "avg_logprob": -0.08241815567016601, "compression_ratio": 1.5885714285714285, "no_speech_prob": 0.00432043569162488}, {"id": 270, "seek": 204600, "start": 2054.96, "end": 2062.16, "text": " okay so that that's the AI tools but then there are the other AIs which also have existed in my", "tokens": [50812, 1392, 370, 300, 300, 311, 264, 7318, 3873, 457, 550, 456, 366, 264, 661, 316, 6802, 597, 611, 362, 13135, 294, 452, 51172], "temperature": 0.0, "avg_logprob": -0.08241815567016601, "compression_ratio": 1.5885714285714285, "no_speech_prob": 0.00432043569162488}, {"id": 271, "seek": 204600, "start": 2062.16, "end": 2070.24, "text": " lab for at least 32 years which invent their own goals and they are a little bit like little", "tokens": [51172, 2715, 337, 412, 1935, 8858, 924, 597, 7962, 641, 1065, 5493, 293, 436, 366, 257, 707, 857, 411, 707, 51576], "temperature": 0.0, "avg_logprob": -0.08241815567016601, "compression_ratio": 1.5885714285714285, "no_speech_prob": 0.00432043569162488}, {"id": 272, "seek": 207024, "start": 2070.24, "end": 2079.9199999999996, "text": " scientists where you have an incentive to explore the environment through actions through", "tokens": [50364, 7708, 689, 291, 362, 364, 22346, 281, 6839, 264, 2823, 807, 5909, 807, 50848], "temperature": 0.0, "avg_logprob": -0.07518748747996795, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.0033200618345290422}, {"id": 273, "seek": 207024, "start": 2079.9199999999996, "end": 2085.2, "text": " experiments self-invented experiments that tell you more about how the world works such that you", "tokens": [50848, 12050, 2698, 12, 259, 2475, 292, 12050, 300, 980, 291, 544, 466, 577, 264, 1002, 1985, 1270, 300, 291, 51112], "temperature": 0.0, "avg_logprob": -0.07518748747996795, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.0033200618345290422}, {"id": 274, "seek": 207024, "start": 2085.2, "end": 2089.7599999999998, "text": " can become a better and better and more and more general problem solver in that world", "tokens": [51112, 393, 1813, 257, 1101, 293, 1101, 293, 544, 293, 544, 2674, 1154, 1404, 331, 294, 300, 1002, 51340], "temperature": 0.0, "avg_logprob": -0.07518748747996795, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.0033200618345290422}, {"id": 275, "seek": 207024, "start": 2090.4799999999996, "end": 2098.08, "text": " and so these AIs they have for a long time created their own goals and now of course", "tokens": [51376, 293, 370, 613, 316, 6802, 436, 362, 337, 257, 938, 565, 2942, 641, 1065, 5493, 293, 586, 295, 1164, 51756], "temperature": 0.0, "avg_logprob": -0.07518748747996795, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.0033200618345290422}, {"id": 276, "seek": 209808, "start": 2098.08, "end": 2105.92, "text": " the interesting question is these more interesting AIs what are they going to do once they are", "tokens": [50364, 264, 1880, 1168, 307, 613, 544, 1880, 316, 6802, 437, 366, 436, 516, 281, 360, 1564, 436, 366, 50756], "temperature": 0.0, "avg_logprob": -0.07663184801737467, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.0023563485592603683}, {"id": 277, "seek": 209808, "start": 2108.7999999999997, "end": 2114.3199999999997, "text": " once they have been scaled up and can compete or maybe outperform humans and everything", "tokens": [50900, 1564, 436, 362, 668, 36039, 493, 293, 393, 11831, 420, 1310, 484, 26765, 6255, 293, 1203, 51176], "temperature": 0.0, "avg_logprob": -0.07663184801737467, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.0023563485592603683}, {"id": 278, "seek": 209808, "start": 2115.44, "end": 2123.04, "text": " they want to achieve so on the one hand the AI tools and there the greatest worry is", "tokens": [51232, 436, 528, 281, 4584, 370, 322, 264, 472, 1011, 264, 7318, 3873, 293, 456, 264, 6636, 3292, 307, 51612], "temperature": 0.0, "avg_logprob": -0.07663184801737467, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.0023563485592603683}, {"id": 279, "seek": 212304, "start": 2123.2799999999997, "end": 2131.7599999999998, "text": " what are the other humans going to do to me with their AI tools so in the extreme case you have", "tokens": [50376, 437, 366, 264, 661, 6255, 516, 281, 360, 281, 385, 365, 641, 7318, 3873, 370, 294, 264, 8084, 1389, 291, 362, 50800], "temperature": 0.0, "avg_logprob": -0.15359582473982625, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.009235186502337456}, {"id": 280, "seek": 212304, "start": 2131.7599999999998, "end": 2139.6, "text": " people who are using AI weapons against you and maybe your neighbor is has bought a little drone", "tokens": [50800, 561, 567, 366, 1228, 7318, 7278, 1970, 291, 293, 1310, 428, 5987, 307, 575, 4243, 257, 707, 13852, 51192], "temperature": 0.0, "avg_logprob": -0.15359582473982625, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.009235186502337456}, {"id": 281, "seek": 212304, "start": 2139.6, "end": 2147.6, "text": " for 300 dollars and it has face recognition and it has a little gripper and it flies across the", "tokens": [51192, 337, 6641, 3808, 293, 309, 575, 1851, 11150, 293, 309, 575, 257, 707, 17865, 3717, 293, 309, 17414, 2108, 264, 51592], "temperature": 0.0, "avg_logprob": -0.15359582473982625, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.009235186502337456}, {"id": 282, "seek": 214760, "start": 2147.6, "end": 2155.7599999999998, "text": " hedge and puts some poison into your coffee or something like that so then the problem is not", "tokens": [50364, 25304, 293, 8137, 512, 10836, 666, 428, 4982, 420, 746, 411, 300, 370, 550, 264, 1154, 307, 406, 50772], "temperature": 0.0, "avg_logprob": -0.07663235068321228, "compression_ratio": 1.6608187134502923, "no_speech_prob": 0.007438707631081343}, {"id": 283, "seek": 214760, "start": 2155.7599999999998, "end": 2163.12, "text": " the AI which is trying to enslave humans or something silly like that no it's your neighbor", "tokens": [50772, 264, 7318, 597, 307, 1382, 281, 3489, 27995, 6255, 420, 746, 11774, 411, 300, 572, 309, 311, 428, 5987, 51140], "temperature": 0.0, "avg_logprob": -0.07663235068321228, "compression_ratio": 1.6608187134502923, "no_speech_prob": 0.007438707631081343}, {"id": 284, "seek": 214760, "start": 2163.12, "end": 2172.08, "text": " or the other human and generally speaking you have to be much more afraid of other humans than you", "tokens": [51140, 420, 264, 661, 1952, 293, 5101, 4124, 291, 362, 281, 312, 709, 544, 4638, 295, 661, 6255, 813, 291, 51588], "temperature": 0.0, "avg_logprob": -0.07663235068321228, "compression_ratio": 1.6608187134502923, "no_speech_prob": 0.007438707631081343}, {"id": 285, "seek": 217208, "start": 2172.08, "end": 2182.88, "text": " have to be of AIs even those who define or set themselves their own goals because you must mostly", "tokens": [50364, 362, 281, 312, 295, 316, 6802, 754, 729, 567, 6964, 420, 992, 2969, 641, 1065, 5493, 570, 291, 1633, 5240, 50904], "temperature": 0.0, "avg_logprob": -0.07168007619453198, "compression_ratio": 1.64, "no_speech_prob": 0.04397693648934364}, {"id": 286, "seek": 217208, "start": 2183.52, "end": 2191.6, "text": " worry about those with whom you share goals so if you share goals then suddenly there is a potential", "tokens": [50936, 3292, 466, 729, 365, 7101, 291, 2073, 5493, 370, 498, 291, 2073, 5493, 550, 5800, 456, 307, 257, 3995, 51340], "temperature": 0.0, "avg_logprob": -0.07168007619453198, "compression_ratio": 1.64, "no_speech_prob": 0.04397693648934364}, {"id": 287, "seek": 217208, "start": 2191.6, "end": 2198.96, "text": " of conflict because maybe there is only one schnitzel over there and two persons want to", "tokens": [51340, 295, 6596, 570, 1310, 456, 307, 787, 472, 956, 77, 6862, 338, 670, 456, 293, 732, 14453, 528, 281, 51708], "temperature": 0.0, "avg_logprob": -0.07168007619453198, "compression_ratio": 1.64, "no_speech_prob": 0.04397693648934364}, {"id": 288, "seek": 219896, "start": 2198.96, "end": 2205.6, "text": " eat the schnitzel and suddenly they have a reason to fight against each other generally speaking", "tokens": [50364, 1862, 264, 956, 77, 6862, 338, 293, 5800, 436, 362, 257, 1778, 281, 2092, 1970, 1184, 661, 5101, 4124, 50696], "temperature": 0.0, "avg_logprob": -0.06710291653871536, "compression_ratio": 1.6318681318681318, "no_speech_prob": 0.004058545920997858}, {"id": 289, "seek": 219896, "start": 2205.6, "end": 2216.0, "text": " if you share goals then you can do two things you can either collaborate or compete an extreme form", "tokens": [50696, 498, 291, 2073, 5493, 550, 291, 393, 360, 732, 721, 291, 393, 2139, 18338, 420, 11831, 364, 8084, 1254, 51216], "temperature": 0.0, "avg_logprob": -0.06710291653871536, "compression_ratio": 1.6318681318681318, "no_speech_prob": 0.004058545920997858}, {"id": 290, "seek": 219896, "start": 2216.0, "end": 2226.56, "text": " of collaboration would be to maybe marry another person and set up a family and master life together", "tokens": [51216, 295, 9363, 576, 312, 281, 1310, 9747, 1071, 954, 293, 992, 493, 257, 1605, 293, 4505, 993, 1214, 51744], "temperature": 0.0, "avg_logprob": -0.06710291653871536, "compression_ratio": 1.6318681318681318, "no_speech_prob": 0.004058545920997858}, {"id": 291, "seek": 222656, "start": 2226.96, "end": 2241.2, "text": " and an extreme form of competition would be war and and those who share goals they have many more", "tokens": [50384, 293, 364, 8084, 1254, 295, 6211, 576, 312, 1516, 293, 293, 729, 567, 2073, 5493, 436, 362, 867, 544, 51096], "temperature": 0.0, "avg_logprob": -0.16493009966473246, "compression_ratio": 1.523076923076923, "no_speech_prob": 0.001570293097756803}, {"id": 292, "seek": 222656, "start": 2241.2, "end": 2251.36, "text": " incentives to interact than those who don't share goals and so humans are mostly interested in other", "tokens": [51096, 23374, 281, 4648, 813, 729, 567, 500, 380, 2073, 5493, 293, 370, 6255, 366, 5240, 3102, 294, 661, 51604], "temperature": 0.0, "avg_logprob": -0.16493009966473246, "compression_ratio": 1.523076923076923, "no_speech_prob": 0.001570293097756803}, {"id": 293, "seek": 225136, "start": 2251.36, "end": 2258.4, "text": " humans because they share similar goals and because they give them a reason to collaborate or to", "tokens": [50364, 6255, 570, 436, 2073, 2531, 5493, 293, 570, 436, 976, 552, 257, 1778, 281, 18338, 420, 281, 50716], "temperature": 0.0, "avg_logprob": -0.07964513360000239, "compression_ratio": 1.905, "no_speech_prob": 0.04135754704475403}, {"id": 294, "seek": 225136, "start": 2258.4, "end": 2266.0, "text": " compete most CEOs of certain companies are interested in other CEOs of competing companies", "tokens": [50716, 11831, 881, 40736, 295, 1629, 3431, 366, 3102, 294, 661, 40736, 295, 15439, 3431, 51096], "temperature": 0.0, "avg_logprob": -0.07964513360000239, "compression_ratio": 1.905, "no_speech_prob": 0.04135754704475403}, {"id": 295, "seek": 225136, "start": 2266.0, "end": 2272.1600000000003, "text": " and five-year-old girls are mostly interested in other five-year-old girls and the super smart AIs", "tokens": [51096, 293, 1732, 12, 5294, 12, 2641, 4519, 366, 5240, 3102, 294, 661, 1732, 12, 5294, 12, 2641, 4519, 293, 264, 1687, 4069, 316, 6802, 51404], "temperature": 0.0, "avg_logprob": -0.07964513360000239, "compression_ratio": 1.905, "no_speech_prob": 0.04135754704475403}, {"id": 296, "seek": 225136, "start": 2272.1600000000003, "end": 2277.44, "text": " of the future who set themselves their own goals they will be mostly interested in other super", "tokens": [51404, 295, 264, 2027, 567, 992, 2969, 641, 1065, 5493, 436, 486, 312, 5240, 3102, 294, 661, 1687, 51668], "temperature": 0.0, "avg_logprob": -0.07964513360000239, "compression_ratio": 1.905, "no_speech_prob": 0.04135754704475403}, {"id": 297, "seek": 227744, "start": 2277.44, "end": 2285.6, "text": " smart AIs of the future who set themselves their own goals generally speaking there is not so much", "tokens": [50364, 4069, 316, 6802, 295, 264, 2027, 567, 992, 2969, 641, 1065, 5493, 5101, 4124, 456, 307, 406, 370, 709, 50772], "temperature": 0.0, "avg_logprob": -0.03444641431172689, "compression_ratio": 1.5698324022346368, "no_speech_prob": 0.018150728195905685}, {"id": 298, "seek": 227744, "start": 2285.6, "end": 2293.84, "text": " competition and there are not so many shared goals between biological beings such as humans", "tokens": [50772, 6211, 293, 456, 366, 406, 370, 867, 5507, 5493, 1296, 13910, 8958, 1270, 382, 6255, 51184], "temperature": 0.0, "avg_logprob": -0.03444641431172689, "compression_ratio": 1.5698324022346368, "no_speech_prob": 0.018150728195905685}, {"id": 299, "seek": 227744, "start": 2293.84, "end": 2302.32, "text": " and a new type of life that as you mentioned can expand into the universe and can multiply", "tokens": [51184, 293, 257, 777, 2010, 295, 993, 300, 382, 291, 2835, 393, 5268, 666, 264, 6445, 293, 393, 12972, 51608], "temperature": 0.0, "avg_logprob": -0.03444641431172689, "compression_ratio": 1.5698324022346368, "no_speech_prob": 0.018150728195905685}, {"id": 300, "seek": 230232, "start": 2302.4, "end": 2308.88, "text": " in a way that is completely infeasible for biological beings so there's a certain", "tokens": [50368, 294, 257, 636, 300, 307, 2584, 1536, 68, 296, 964, 337, 13910, 8958, 370, 456, 311, 257, 1629, 50692], "temperature": 0.0, "avg_logprob": -0.09421490841224546, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.0020721605978906155}, {"id": 301, "seek": 230232, "start": 2310.0, "end": 2315.6000000000004, "text": " long-term protection at least through lack of interest on the other side", "tokens": [50748, 938, 12, 7039, 6334, 412, 1935, 807, 5011, 295, 1179, 322, 264, 661, 1252, 51028], "temperature": 0.0, "avg_logprob": -0.09421490841224546, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.0020721605978906155}, {"id": 302, "seek": 230232, "start": 2318.7200000000003, "end": 2324.6400000000003, "text": " okay brilliant there's a few things I wanted to touch on there we will get on to what it means", "tokens": [51184, 1392, 10248, 456, 311, 257, 1326, 721, 286, 1415, 281, 2557, 322, 456, 321, 486, 483, 322, 281, 437, 309, 1355, 51480], "temperature": 0.0, "avg_logprob": -0.09421490841224546, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.0020721605978906155}, {"id": 303, "seek": 232464, "start": 2324.64, "end": 2333.04, "text": " for goals to emerge from systems later and you started off by saying that humans will buy", "tokens": [50364, 337, 5493, 281, 21511, 490, 3652, 1780, 293, 291, 1409, 766, 538, 1566, 300, 6255, 486, 2256, 50784], "temperature": 0.0, "avg_logprob": -0.05569953796191093, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.4885360896587372}, {"id": 304, "seek": 232464, "start": 2333.04, "end": 2338.64, "text": " products that make them feel good and Facebook is quite an interesting example to play with", "tokens": [50784, 3383, 300, 652, 552, 841, 665, 293, 4384, 307, 1596, 364, 1880, 1365, 281, 862, 365, 51064], "temperature": 0.0, "avg_logprob": -0.05569953796191093, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.4885360896587372}, {"id": 305, "seek": 232464, "start": 2338.64, "end": 2344.0, "text": " actually because Facebook is a little bit like an AI system which is a collective intelligence", "tokens": [51064, 767, 570, 4384, 307, 257, 707, 857, 411, 364, 7318, 1185, 597, 307, 257, 12590, 7599, 51332], "temperature": 0.0, "avg_logprob": -0.05569953796191093, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.4885360896587372}, {"id": 306, "seek": 232464, "start": 2344.0, "end": 2349.68, "text": " and humans use Facebook but they have some idea that it might cause them harm and the thing with", "tokens": [51332, 293, 6255, 764, 4384, 457, 436, 362, 512, 1558, 300, 309, 1062, 3082, 552, 6491, 293, 264, 551, 365, 51616], "temperature": 0.0, "avg_logprob": -0.05569953796191093, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.4885360896587372}, {"id": 307, "seek": 234968, "start": 2349.7599999999998, "end": 2355.6, "text": " population ethics is we know that our moral reasoning kind of decays over space and even more", "tokens": [50368, 4415, 19769, 307, 321, 458, 300, 527, 9723, 21577, 733, 295, 979, 3772, 670, 1901, 293, 754, 544, 50660], "temperature": 0.0, "avg_logprob": -0.046349759535356, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.1719834804534912}, {"id": 308, "seek": 234968, "start": 2355.6, "end": 2360.8799999999997, "text": " so over time and part of the reason why time is so difficult is because it's predictive we don't", "tokens": [50660, 370, 670, 565, 293, 644, 295, 264, 1778, 983, 565, 307, 370, 2252, 307, 570, 309, 311, 35521, 321, 500, 380, 50924], "temperature": 0.0, "avg_logprob": -0.046349759535356, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.1719834804534912}, {"id": 309, "seek": 234968, "start": 2360.8799999999997, "end": 2366.3999999999996, "text": " actually know what's going to happen in the future so our kind of reasoning about establishing", "tokens": [50924, 767, 458, 437, 311, 516, 281, 1051, 294, 264, 2027, 370, 527, 733, 295, 21577, 466, 22494, 51200], "temperature": 0.0, "avg_logprob": -0.046349759535356, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.1719834804534912}, {"id": 310, "seek": 234968, "start": 2366.3999999999996, "end": 2370.96, "text": " what the value of something is is very very faulty and I think that's one of the reasons why", "tokens": [51200, 437, 264, 2158, 295, 746, 307, 307, 588, 588, 2050, 5773, 293, 286, 519, 300, 311, 472, 295, 264, 4112, 983, 51428], "temperature": 0.0, "avg_logprob": -0.046349759535356, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.1719834804534912}, {"id": 311, "seek": 234968, "start": 2370.96, "end": 2376.08, "text": " these people would say that we don't really know what's good for us I do completely agree with you", "tokens": [51428, 613, 561, 576, 584, 300, 321, 500, 380, 534, 458, 437, 311, 665, 337, 505, 286, 360, 2584, 3986, 365, 291, 51684], "temperature": 0.0, "avg_logprob": -0.046349759535356, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.1719834804534912}, {"id": 312, "seek": 237608, "start": 2376.08, "end": 2384.64, "text": " though that the problem I think is humans rather than AIs on their own yes these are good points", "tokens": [50364, 1673, 300, 264, 1154, 286, 519, 307, 6255, 2831, 813, 316, 6802, 322, 641, 1065, 2086, 613, 366, 665, 2793, 50792], "temperature": 0.0, "avg_logprob": -0.18929986750825922, "compression_ratio": 1.4306569343065694, "no_speech_prob": 0.004912483040243387}, {"id": 313, "seek": 237608, "start": 2394.4, "end": 2404.48, "text": " feel free to uh offer some thoughts yes I mean it it would that's a whole separate discussion isn't", "tokens": [51280, 841, 1737, 281, 2232, 2626, 512, 4598, 2086, 286, 914, 309, 309, 576, 300, 311, 257, 1379, 4994, 5017, 1943, 380, 51784], "temperature": 0.0, "avg_logprob": -0.18929986750825922, "compression_ratio": 1.4306569343065694, "no_speech_prob": 0.004912483040243387}, {"id": 314, "seek": 240448, "start": 2404.48, "end": 2416.64, "text": " it when you discuss the limitations of what's predictable and um and how people often fail", "tokens": [50364, 309, 562, 291, 2248, 264, 15705, 295, 437, 311, 27737, 293, 1105, 293, 577, 561, 2049, 3061, 50972], "temperature": 0.0, "avg_logprob": -0.11652898788452148, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.0034785631578415632}, {"id": 315, "seek": 240448, "start": 2416.64, "end": 2425.12, "text": " to see what's good for them well I think maybe so you've already you've already um said that", "tokens": [50972, 281, 536, 437, 311, 665, 337, 552, 731, 286, 519, 1310, 370, 291, 600, 1217, 291, 600, 1217, 1105, 848, 300, 51396], "temperature": 0.0, "avg_logprob": -0.11652898788452148, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.0034785631578415632}, {"id": 316, "seek": 240448, "start": 2425.12, "end": 2431.92, "text": " there's no proof that we'll be safe forever right like I mean there could there could come an", "tokens": [51396, 456, 311, 572, 8177, 300, 321, 603, 312, 3273, 5680, 558, 411, 286, 914, 456, 727, 456, 727, 808, 364, 51736], "temperature": 0.0, "avg_logprob": -0.11652898788452148, "compression_ratio": 1.6104651162790697, "no_speech_prob": 0.0034785631578415632}, {"id": 317, "seek": 243192, "start": 2431.92, "end": 2437.76, "text": " existential risk you know from AI so I think my question to you is do you have sympathy for", "tokens": [50364, 37133, 3148, 291, 458, 490, 7318, 370, 286, 519, 452, 1168, 281, 291, 307, 360, 291, 362, 33240, 337, 50656], "temperature": 0.0, "avg_logprob": -0.07960364462315352, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.027568606659770012}, {"id": 318, "seek": 243192, "start": 2437.76, "end": 2444.16, "text": " the folks who say we need to be putting more resources into researching alignment like we need", "tokens": [50656, 264, 4024, 567, 584, 321, 643, 281, 312, 3372, 544, 3593, 666, 24176, 18515, 411, 321, 643, 50976], "temperature": 0.0, "avg_logprob": -0.07960364462315352, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.027568606659770012}, {"id": 319, "seek": 243192, "start": 2444.16, "end": 2452.7200000000003, "text": " to develop the tools um in order to allow it to be easier for people to construct AI that is aligned", "tokens": [50976, 281, 1499, 264, 3873, 1105, 294, 1668, 281, 2089, 309, 281, 312, 3571, 337, 561, 281, 7690, 7318, 300, 307, 17962, 51404], "temperature": 0.0, "avg_logprob": -0.07960364462315352, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.027568606659770012}, {"id": 320, "seek": 243192, "start": 2452.7200000000003, "end": 2458.16, "text": " for the goals and to make sure that you know that it doesn't that it doesn't have unintended", "tokens": [51404, 337, 264, 5493, 293, 281, 652, 988, 300, 291, 458, 300, 309, 1177, 380, 300, 309, 1177, 380, 362, 49902, 51676], "temperature": 0.0, "avg_logprob": -0.07960364462315352, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.027568606659770012}, {"id": 321, "seek": 245816, "start": 2458.64, "end": 2463.12, "text": " consequences like in other words there may not be a proof that we can go forever and be", "tokens": [50388, 10098, 411, 294, 661, 2283, 456, 815, 406, 312, 257, 8177, 300, 321, 393, 352, 5680, 293, 312, 50612], "temperature": 0.0, "avg_logprob": -0.07679235482517677, "compression_ratio": 1.6243902439024391, "no_speech_prob": 0.0012241234071552753}, {"id": 322, "seek": 245816, "start": 2463.12, "end": 2468.72, "text": " safe for AI but we at least want to develop the basic mechanics that we need to safely", "tokens": [50612, 3273, 337, 7318, 457, 321, 412, 1935, 528, 281, 1499, 264, 3875, 12939, 300, 321, 643, 281, 11750, 50892], "temperature": 0.0, "avg_logprob": -0.07679235482517677, "compression_ratio": 1.6243902439024391, "no_speech_prob": 0.0012241234071552753}, {"id": 323, "seek": 245816, "start": 2469.52, "end": 2477.44, "text": " develop and deploy AI don't we yes and I sympathize with those who um are devoting their", "tokens": [50932, 1499, 293, 7274, 7318, 500, 380, 321, 2086, 293, 286, 22276, 1125, 365, 729, 567, 1105, 366, 13697, 278, 641, 51328], "temperature": 0.0, "avg_logprob": -0.07679235482517677, "compression_ratio": 1.6243902439024391, "no_speech_prob": 0.0012241234071552753}, {"id": 324, "seek": 245816, "start": 2477.44, "end": 2484.0, "text": " lives to alignment issues and trying to build AIs aligned with humans", "tokens": [51328, 2909, 281, 18515, 2663, 293, 1382, 281, 1322, 316, 6802, 17962, 365, 6255, 51656], "temperature": 0.0, "avg_logprob": -0.07679235482517677, "compression_ratio": 1.6243902439024391, "no_speech_prob": 0.0012241234071552753}, {"id": 325, "seek": 248400, "start": 2484.0, "end": 2496.32, "text": " I view them as part of the evolution of all kinds of other ideas that come up as not only", "tokens": [50364, 286, 1910, 552, 382, 644, 295, 264, 9303, 295, 439, 3685, 295, 661, 3487, 300, 808, 493, 382, 406, 787, 50980], "temperature": 0.0, "avg_logprob": -0.11684500496342497, "compression_ratio": 2.046153846153846, "no_speech_prob": 0.007000945042818785}, {"id": 326, "seek": 248400, "start": 2497.04, "end": 2501.52, "text": " nations compete with other nations but companies compete with other companies", "tokens": [51016, 11035, 11831, 365, 661, 11035, 457, 3431, 11831, 365, 661, 3431, 51240], "temperature": 0.0, "avg_logprob": -0.11684500496342497, "compression_ratio": 2.046153846153846, "no_speech_prob": 0.007000945042818785}, {"id": 327, "seek": 248400, "start": 2501.52, "end": 2507.04, "text": " and shareholders of different companies compete with shareholders of different companies and so on", "tokens": [51240, 293, 33294, 295, 819, 3431, 11831, 365, 33294, 295, 819, 3431, 293, 370, 322, 51516], "temperature": 0.0, "avg_logprob": -0.11684500496342497, "compression_ratio": 2.046153846153846, "no_speech_prob": 0.007000945042818785}, {"id": 328, "seek": 250704, "start": 2508.0, "end": 2516.24, "text": " and so there is such a huge set of different human goals which are not aligned with each other", "tokens": [50412, 293, 370, 456, 307, 1270, 257, 2603, 992, 295, 819, 1952, 5493, 597, 366, 406, 17962, 365, 1184, 661, 50824], "temperature": 0.0, "avg_logprob": -0.07920880684485802, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.0012436636025086045}, {"id": 329, "seek": 250704, "start": 2516.88, "end": 2524.8, "text": " that makes me doubt that you will come up with a general system that all humans can accept", "tokens": [50856, 300, 1669, 385, 6385, 300, 291, 486, 808, 493, 365, 257, 2674, 1185, 300, 439, 6255, 393, 3241, 51252], "temperature": 0.0, "avg_logprob": -0.07920880684485802, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.0012436636025086045}, {"id": 330, "seek": 250704, "start": 2525.6, "end": 2533.44, "text": " simply because if you put 10 humans in a room and ask them what is good they will give you", "tokens": [51292, 2935, 570, 498, 291, 829, 1266, 6255, 294, 257, 1808, 293, 1029, 552, 437, 307, 665, 436, 486, 976, 291, 51684], "temperature": 0.0, "avg_logprob": -0.07920880684485802, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.0012436636025086045}, {"id": 331, "seek": 253344, "start": 2533.44, "end": 2542.88, "text": " 10 different opinions however I sympathize with with this goal and it's good that people are", "tokens": [50364, 1266, 819, 11819, 4461, 286, 22276, 1125, 365, 365, 341, 3387, 293, 309, 311, 665, 300, 561, 366, 50836], "temperature": 0.0, "avg_logprob": -0.09873816967010499, "compression_ratio": 1.579268292682927, "no_speech_prob": 0.013384911231696606}, {"id": 332, "seek": 253344, "start": 2542.88, "end": 2550.56, "text": " worried and they spend resources on solving some of these issues in the long run however", "tokens": [50836, 5804, 293, 436, 3496, 3593, 322, 12606, 512, 295, 613, 2663, 294, 264, 938, 1190, 4461, 51220], "temperature": 0.0, "avg_logprob": -0.09873816967010499, "compression_ratio": 1.579268292682927, "no_speech_prob": 0.013384911231696606}, {"id": 333, "seek": 253344, "start": 2551.68, "end": 2559.92, "text": " I think there is no way of stopping all kinds of AIs from having all kinds of", "tokens": [51276, 286, 519, 456, 307, 572, 636, 295, 12767, 439, 3685, 295, 316, 6802, 490, 1419, 439, 3685, 295, 51688], "temperature": 0.0, "avg_logprob": -0.09873816967010499, "compression_ratio": 1.579268292682927, "no_speech_prob": 0.013384911231696606}, {"id": 334, "seek": 255992, "start": 2559.92, "end": 2568.32, "text": " goals that have very little to do with humans the universe itself is built in a certain way", "tokens": [50364, 5493, 300, 362, 588, 707, 281, 360, 365, 6255, 264, 6445, 2564, 307, 3094, 294, 257, 1629, 636, 50784], "temperature": 0.0, "avg_logprob": -0.10762213170528412, "compression_ratio": 1.558011049723757, "no_speech_prob": 0.003640229580923915}, {"id": 335, "seek": 255992, "start": 2569.12, "end": 2570.08, "text": " that apparently", "tokens": [50824, 300, 7970, 50872], "temperature": 0.0, "avg_logprob": -0.10762213170528412, "compression_ratio": 1.558011049723757, "no_speech_prob": 0.003640229580923915}, {"id": 336, "seek": 255992, "start": 2573.6800000000003, "end": 2579.6800000000003, "text": " derives it from very simple initial conditions to more and more complexity", "tokens": [51052, 1163, 1539, 309, 490, 588, 2199, 5883, 4487, 281, 544, 293, 544, 14024, 51352], "temperature": 0.0, "avg_logprob": -0.10762213170528412, "compression_ratio": 1.558011049723757, "no_speech_prob": 0.003640229580923915}, {"id": 337, "seek": 255992, "start": 2580.7200000000003, "end": 2588.0, "text": " and now we have reached a certain stage after 13.8 billion years of evolution and it it seems clear", "tokens": [51404, 293, 586, 321, 362, 6488, 257, 1629, 3233, 934, 3705, 13, 23, 5218, 924, 295, 9303, 293, 309, 309, 2544, 1850, 51768], "temperature": 0.0, "avg_logprob": -0.10762213170528412, "compression_ratio": 1.558011049723757, "no_speech_prob": 0.003640229580923915}, {"id": 338, "seek": 258800, "start": 2588.0, "end": 2593.68, "text": " that this cannot be the end of it because the universe is still young it's going to be much", "tokens": [50364, 300, 341, 2644, 312, 264, 917, 295, 309, 570, 264, 6445, 307, 920, 2037, 309, 311, 516, 281, 312, 709, 50648], "temperature": 0.0, "avg_logprob": -0.08894382897069898, "compression_ratio": 1.632258064516129, "no_speech_prob": 0.00689092930406332}, {"id": 339, "seek": 258800, "start": 2593.68, "end": 2604.24, "text": " older than it is now now there is this drive built in drive of the cosmos to become more complex", "tokens": [50648, 4906, 813, 309, 307, 586, 586, 456, 307, 341, 3332, 3094, 294, 3332, 295, 264, 41794, 281, 1813, 544, 3997, 51176], "temperature": 0.0, "avg_logprob": -0.08894382897069898, "compression_ratio": 1.632258064516129, "no_speech_prob": 0.00689092930406332}, {"id": 340, "seek": 258800, "start": 2604.24, "end": 2609.36, "text": " and it seems clear that civilization a civilization like ours is", "tokens": [51176, 293, 309, 2544, 1850, 300, 18036, 257, 18036, 411, 11896, 307, 51432], "temperature": 0.0, "avg_logprob": -0.08894382897069898, "compression_ratio": 1.632258064516129, "no_speech_prob": 0.00689092930406332}, {"id": 341, "seek": 260936, "start": 2609.52, "end": 2617.6800000000003, "text": " is a stepping stone on to war it's something that is more complex and", "tokens": [50372, 307, 257, 16821, 7581, 322, 281, 1516, 309, 311, 746, 300, 307, 544, 3997, 293, 50780], "temperature": 0.0, "avg_logprob": -0.1451245821439303, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.08470170944929123}, {"id": 342, "seek": 260936, "start": 2618.48, "end": 2623.36, "text": " could I touch on a couple of things here the bootloader example is kind of where I want to go", "tokens": [50820, 727, 286, 2557, 322, 257, 1916, 295, 721, 510, 264, 11450, 2907, 260, 1365, 307, 733, 295, 689, 286, 528, 281, 352, 51064], "temperature": 0.0, "avg_logprob": -0.1451245821439303, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.08470170944929123}, {"id": 343, "seek": 260936, "start": 2623.36, "end": 2630.8, "text": " with this so a lot of the ideas of this movement can be traced back to Derek Parfit who is a", "tokens": [51064, 365, 341, 370, 257, 688, 295, 264, 3487, 295, 341, 3963, 393, 312, 38141, 646, 281, 22887, 3457, 6845, 567, 307, 257, 51436], "temperature": 0.0, "avg_logprob": -0.1451245821439303, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.08470170944929123}, {"id": 344, "seek": 260936, "start": 2630.8, "end": 2637.44, "text": " philosopher he was a moral realist so he thought there was such a thing as a moral fact and I'm", "tokens": [51436, 29805, 415, 390, 257, 9723, 957, 468, 370, 415, 1194, 456, 390, 1270, 257, 551, 382, 257, 9723, 1186, 293, 286, 478, 51768], "temperature": 0.0, "avg_logprob": -0.1451245821439303, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.08470170944929123}, {"id": 345, "seek": 263744, "start": 2637.44, "end": 2644.32, "text": " a bit of a relativist myself and actually if you trace this tree of complexity and how humans", "tokens": [50364, 257, 857, 295, 257, 21960, 468, 2059, 293, 767, 498, 291, 13508, 341, 4230, 295, 14024, 293, 577, 6255, 50708], "temperature": 0.0, "avg_logprob": -0.032299310237437755, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.02293732948601246}, {"id": 346, "seek": 263744, "start": 2644.32, "end": 2649.2000000000003, "text": " evolve over time we might just be a stepping stone to a kind of rich diverse transhumanist", "tokens": [50708, 16693, 670, 565, 321, 1062, 445, 312, 257, 16821, 7581, 281, 257, 733, 295, 4593, 9521, 1145, 18796, 468, 50952], "temperature": 0.0, "avg_logprob": -0.032299310237437755, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.02293732948601246}, {"id": 347, "seek": 263744, "start": 2649.2000000000003, "end": 2655.04, "text": " future where we become the thing over time that we're so scared of and I think the lens that", "tokens": [50952, 2027, 689, 321, 1813, 264, 551, 670, 565, 300, 321, 434, 370, 5338, 295, 293, 286, 519, 264, 6765, 300, 51244], "temperature": 0.0, "avg_logprob": -0.032299310237437755, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.02293732948601246}, {"id": 348, "seek": 263744, "start": 2655.04, "end": 2659.92, "text": " we're using here about what's right and what's wrong is kind of like I was saying before it's", "tokens": [51244, 321, 434, 1228, 510, 466, 437, 311, 558, 293, 437, 311, 2085, 307, 733, 295, 411, 286, 390, 1566, 949, 309, 311, 51488], "temperature": 0.0, "avg_logprob": -0.032299310237437755, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.02293732948601246}, {"id": 349, "seek": 263744, "start": 2659.92, "end": 2666.0, "text": " a snapshot of humanity now and we kind of think of it as just this monolithic single thing", "tokens": [51488, 257, 30163, 295, 10243, 586, 293, 321, 733, 295, 519, 295, 309, 382, 445, 341, 1108, 42878, 2167, 551, 51792], "temperature": 0.0, "avg_logprob": -0.032299310237437755, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.02293732948601246}, {"id": 350, "seek": 266600, "start": 2666.0, "end": 2672.24, "text": " so does it really work when you project out to how we're going to evolve in the future", "tokens": [50364, 370, 775, 309, 534, 589, 562, 291, 1716, 484, 281, 577, 321, 434, 516, 281, 16693, 294, 264, 2027, 50676], "temperature": 0.0, "avg_logprob": -0.09414243698120117, "compression_ratio": 1.7006802721088434, "no_speech_prob": 0.001224122243002057}, {"id": 351, "seek": 266600, "start": 2674.88, "end": 2682.4, "text": " but first of all humankind is not a monolithic thing so many of these", "tokens": [50808, 457, 700, 295, 439, 1484, 40588, 307, 406, 257, 1108, 42878, 551, 370, 867, 295, 613, 51184], "temperature": 0.0, "avg_logprob": -0.09414243698120117, "compression_ratio": 1.7006802721088434, "no_speech_prob": 0.001224122243002057}, {"id": 352, "seek": 266600, "start": 2683.52, "end": 2690.8, "text": " arguments go like we should not do that because of that we should not do that because of that", "tokens": [51240, 12869, 352, 411, 321, 820, 406, 360, 300, 570, 295, 300, 321, 820, 406, 360, 300, 570, 295, 300, 51604], "temperature": 0.0, "avg_logprob": -0.09414243698120117, "compression_ratio": 1.7006802721088434, "no_speech_prob": 0.001224122243002057}, {"id": 353, "seek": 269080, "start": 2691.6000000000004, "end": 2697.84, "text": " but there is no us there is no we there are only and almost 10 billion different people", "tokens": [50404, 457, 456, 307, 572, 505, 456, 307, 572, 321, 456, 366, 787, 293, 1920, 1266, 5218, 819, 561, 50716], "temperature": 0.0, "avg_logprob": -0.08961336612701416, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0022797398269176483}, {"id": 354, "seek": 269080, "start": 2697.84, "end": 2704.5600000000004, "text": " and they all have different ideas about what's good for them and so for thousands of years we had", "tokens": [50716, 293, 436, 439, 362, 819, 3487, 466, 437, 311, 665, 337, 552, 293, 370, 337, 5383, 295, 924, 321, 632, 51052], "temperature": 0.0, "avg_logprob": -0.08961336612701416, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0022797398269176483}, {"id": 355, "seek": 269080, "start": 2704.5600000000004, "end": 2714.0, "text": " these evolutions of ideas and of devices and philosophies competing partially competing", "tokens": [51052, 613, 1073, 15892, 295, 3487, 293, 295, 5759, 293, 14529, 530, 15439, 18886, 15439, 51524], "temperature": 0.0, "avg_logprob": -0.08961336612701416, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0022797398269176483}, {"id": 356, "seek": 271400, "start": 2714.0, "end": 2721.52, "text": " and partially compatible with each other which in the end led to the current values", "tokens": [50364, 293, 18886, 18218, 365, 1184, 661, 597, 294, 264, 917, 4684, 281, 264, 2190, 4190, 50740], "temperature": 0.0, "avg_logprob": -0.06800327450037003, "compression_ratio": 1.887005649717514, "no_speech_prob": 0.006783640943467617}, {"id": 357, "seek": 271400, "start": 2721.52, "end": 2727.68, "text": " that some people agree with and other people over there they agree with different values", "tokens": [50740, 300, 512, 561, 3986, 365, 293, 661, 561, 670, 456, 436, 3986, 365, 819, 4190, 51048], "temperature": 0.0, "avg_logprob": -0.06800327450037003, "compression_ratio": 1.887005649717514, "no_speech_prob": 0.006783640943467617}, {"id": 358, "seek": 271400, "start": 2727.68, "end": 2732.24, "text": " nevertheless there are certain values that have become more popular than others more successful", "tokens": [51048, 26924, 456, 366, 1629, 4190, 300, 362, 1813, 544, 3743, 813, 2357, 544, 4406, 51276], "temperature": 0.0, "avg_logprob": -0.06800327450037003, "compression_ratio": 1.887005649717514, "no_speech_prob": 0.006783640943467617}, {"id": 359, "seek": 271400, "start": 2732.88, "end": 2739.36, "text": " more evolutionary with more success during the evolution of ideas", "tokens": [51308, 544, 27567, 365, 544, 2245, 1830, 264, 9303, 295, 3487, 51632], "temperature": 0.0, "avg_logprob": -0.06800327450037003, "compression_ratio": 1.887005649717514, "no_speech_prob": 0.006783640943467617}, {"id": 360, "seek": 273936, "start": 2739.84, "end": 2755.76, "text": " and so given this entire context of evolution of concepts and accepted ideas of what should be done", "tokens": [50388, 293, 370, 2212, 341, 2302, 4319, 295, 9303, 295, 10392, 293, 9035, 3487, 295, 437, 820, 312, 1096, 51184], "temperature": 0.0, "avg_logprob": -0.16352877833626486, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.004666692577302456}, {"id": 361, "seek": 273936, "start": 2755.76, "end": 2764.56, "text": " or what is worth being supported and what's not worth being supported all of this has changed a lot", "tokens": [51184, 420, 437, 307, 3163, 885, 8104, 293, 437, 311, 406, 3163, 885, 8104, 439, 295, 341, 575, 3105, 257, 688, 51624], "temperature": 0.0, "avg_logprob": -0.16352877833626486, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.004666692577302456}, {"id": 362, "seek": 276456, "start": 2765.2799999999997, "end": 2772.88, "text": " if we look back 200 years the average people in the west had different ideas of what's good", "tokens": [50400, 498, 321, 574, 646, 2331, 924, 264, 4274, 561, 294, 264, 7009, 632, 819, 3487, 295, 437, 311, 665, 50780], "temperature": 0.0, "avg_logprob": -0.11904397253262794, "compression_ratio": 1.5212121212121212, "no_speech_prob": 0.004973072558641434}, {"id": 363, "seek": 276456, "start": 2772.88, "end": 2782.0, "text": " than today and and this evolution of ideas is not going to stop any time soon", "tokens": [50780, 813, 965, 293, 293, 341, 9303, 295, 3487, 307, 406, 516, 281, 1590, 604, 565, 2321, 51236], "temperature": 0.0, "avg_logprob": -0.11904397253262794, "compression_ratio": 1.5212121212121212, "no_speech_prob": 0.004973072558641434}, {"id": 364, "seek": 276456, "start": 2786.08, "end": 2792.32, "text": " just a final question on this and there is a very real existential risk right now", "tokens": [51440, 445, 257, 2572, 1168, 322, 341, 293, 456, 307, 257, 588, 957, 37133, 3148, 558, 586, 51752], "temperature": 0.0, "avg_logprob": -0.11904397253262794, "compression_ratio": 1.5212121212121212, "no_speech_prob": 0.004973072558641434}, {"id": 365, "seek": 279232, "start": 2792.4, "end": 2798.96, "text": " of nuclear armageddon a real risk right now and if i were a rational person", "tokens": [50368, 295, 8179, 3726, 2980, 13966, 257, 957, 3148, 558, 586, 293, 498, 741, 645, 257, 15090, 954, 50696], "temperature": 0.0, "avg_logprob": -0.10103569168975388, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.011489021591842175}, {"id": 366, "seek": 279232, "start": 2799.6800000000003, "end": 2806.7200000000003, "text": " i would be devoting all of my effort into that and other risks associated so do you think it's", "tokens": [50732, 741, 576, 312, 13697, 278, 439, 295, 452, 4630, 666, 300, 293, 661, 10888, 6615, 370, 360, 291, 519, 309, 311, 51084], "temperature": 0.0, "avg_logprob": -0.10103569168975388, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.011489021591842175}, {"id": 367, "seek": 279232, "start": 2806.7200000000003, "end": 2816.0800000000004, "text": " a little bit weird that so much focuses on this ai x risk to me it's indeed weird now there are all", "tokens": [51084, 257, 707, 857, 3657, 300, 370, 709, 16109, 322, 341, 9783, 2031, 3148, 281, 385, 309, 311, 6451, 3657, 586, 456, 366, 439, 51552], "temperature": 0.0, "avg_logprob": -0.10103569168975388, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.011489021591842175}, {"id": 368, "seek": 281608, "start": 2816.08, "end": 2824.48, "text": " these letters coming out warning are the dangers of ai and i think some of the guys who are writing", "tokens": [50364, 613, 7825, 1348, 484, 9164, 366, 264, 27701, 295, 9783, 293, 741, 519, 512, 295, 264, 1074, 567, 366, 3579, 50784], "temperature": 0.0, "avg_logprob": -0.0683241049448649, "compression_ratio": 1.7245508982035929, "no_speech_prob": 0.0288121048361063}, {"id": 369, "seek": 281608, "start": 2824.48, "end": 2834.88, "text": " these letters they are just seeking attention because they know that ai dystopia are attracting", "tokens": [50784, 613, 7825, 436, 366, 445, 11670, 3202, 570, 436, 458, 300, 9783, 14584, 13559, 654, 366, 36594, 51304], "temperature": 0.0, "avg_logprob": -0.0683241049448649, "compression_ratio": 1.7245508982035929, "no_speech_prob": 0.0288121048361063}, {"id": 370, "seek": 281608, "start": 2834.88, "end": 2843.2, "text": " more attention than documentaries about the benefits of ai in healthcare and stuff like that", "tokens": [51304, 544, 3202, 813, 41630, 466, 264, 5311, 295, 9783, 294, 8884, 293, 1507, 411, 300, 51720], "temperature": 0.0, "avg_logprob": -0.0683241049448649, "compression_ratio": 1.7245508982035929, "no_speech_prob": 0.0288121048361063}, {"id": 371, "seek": 284320, "start": 2844.16, "end": 2851.12, "text": " but generally speaking i am much more worried about nuclear bombs than about ai weapons", "tokens": [50412, 457, 5101, 4124, 741, 669, 709, 544, 5804, 466, 8179, 19043, 813, 466, 9783, 7278, 50760], "temperature": 0.0, "avg_logprob": -0.09449072037973712, "compression_ratio": 1.6432748538011697, "no_speech_prob": 0.001725927577354014}, {"id": 372, "seek": 284320, "start": 2854.96, "end": 2864.48, "text": " a nuclear bomb a big one can wipe out 10 million people a big city within a few milliseconds without", "tokens": [50952, 257, 8179, 7851, 257, 955, 472, 393, 14082, 484, 1266, 2459, 561, 257, 955, 2307, 1951, 257, 1326, 34184, 1553, 51428], "temperature": 0.0, "avg_logprob": -0.09449072037973712, "compression_ratio": 1.6432748538011697, "no_speech_prob": 0.001725927577354014}, {"id": 373, "seek": 284320, "start": 2864.48, "end": 2872.8799999999997, "text": " a face recognition just like that without any ai and so in that sense it's much more harmful", "tokens": [51428, 257, 1851, 11150, 445, 411, 300, 1553, 604, 9783, 293, 370, 294, 300, 2020, 309, 311, 709, 544, 19727, 51848], "temperature": 0.0, "avg_logprob": -0.09449072037973712, "compression_ratio": 1.6432748538011697, "no_speech_prob": 0.001725927577354014}, {"id": 374, "seek": 287288, "start": 2872.88, "end": 2880.0, "text": " than the comparatively harmless ai weapons than that we have today and that we can currently", "tokens": [50364, 813, 264, 6311, 19020, 40160, 9783, 7278, 813, 300, 321, 362, 965, 293, 300, 321, 393, 4362, 50720], "temperature": 0.0, "avg_logprob": -0.0753291863661546, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.0008824310498312116}, {"id": 375, "seek": 287288, "start": 2880.0, "end": 2889.6800000000003, "text": " conceive of so yes i'm much more worried about 60 year old technology that can wipe out civilization", "tokens": [50720, 48605, 295, 370, 2086, 741, 478, 709, 544, 5804, 466, 4060, 1064, 1331, 2899, 300, 393, 14082, 484, 18036, 51204], "temperature": 0.0, "avg_logprob": -0.0753291863661546, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.0008824310498312116}, {"id": 376, "seek": 287288, "start": 2889.6800000000003, "end": 2899.92, "text": " within two hours without any ai well i guess um since we're we're not really going to worry about", "tokens": [51204, 1951, 732, 2496, 1553, 604, 9783, 731, 741, 2041, 1105, 1670, 321, 434, 321, 434, 406, 534, 516, 281, 3292, 466, 51716], "temperature": 0.0, "avg_logprob": -0.0753291863661546, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.0008824310498312116}, {"id": 377, "seek": 289992, "start": 2899.92, "end": 2904.8, "text": " ai for the moment we can uh we can turn our attention back to discussing with you uh how", "tokens": [50364, 9783, 337, 264, 1623, 321, 393, 2232, 321, 393, 1261, 527, 3202, 646, 281, 10850, 365, 291, 2232, 577, 50608], "temperature": 0.0, "avg_logprob": -0.06987988948822021, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.02477405220270157}, {"id": 378, "seek": 289992, "start": 2904.8, "end": 2912.88, "text": " we develop ai so um you know i'm really curious with with just the really the the vast you know", "tokens": [50608, 321, 1499, 9783, 370, 1105, 291, 458, 741, 478, 534, 6369, 365, 365, 445, 264, 534, 264, 264, 8369, 291, 458, 51012], "temperature": 0.0, "avg_logprob": -0.06987988948822021, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.02477405220270157}, {"id": 379, "seek": 289992, "start": 2912.88, "end": 2918.88, "text": " breadth and depth of your of your knowledge over the the history of of ai and the state of the art", "tokens": [51012, 35862, 293, 7161, 295, 428, 295, 428, 3601, 670, 264, 264, 2503, 295, 295, 9783, 293, 264, 1785, 295, 264, 1523, 51312], "temperature": 0.0, "avg_logprob": -0.06987988948822021, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.02477405220270157}, {"id": 380, "seek": 289992, "start": 2918.88, "end": 2925.28, "text": " i'm curious you know which current approaches you're you're most excited about and or what's on the", "tokens": [51312, 741, 478, 6369, 291, 458, 597, 2190, 11587, 291, 434, 291, 434, 881, 2919, 466, 293, 420, 437, 311, 322, 264, 51632], "temperature": 0.0, "avg_logprob": -0.06987988948822021, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.02477405220270157}, {"id": 381, "seek": 292528, "start": 2925.28, "end": 2931.28, "text": " horizon um that you know for any of our listeners out there are thinking about um going into ai", "tokens": [50364, 18046, 1105, 300, 291, 458, 337, 604, 295, 527, 23274, 484, 456, 366, 1953, 466, 1105, 516, 666, 9783, 50664], "temperature": 0.0, "avg_logprob": -0.07265859455257268, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0016477847239002585}, {"id": 382, "seek": 292528, "start": 2931.28, "end": 2936.5600000000004, "text": " research machine learning research you know what may be um alternatives that aren't getting enough", "tokens": [50664, 2132, 3479, 2539, 2132, 291, 458, 437, 815, 312, 1105, 20478, 300, 3212, 380, 1242, 1547, 50928], "temperature": 0.0, "avg_logprob": -0.07265859455257268, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0016477847239002585}, {"id": 383, "seek": 292528, "start": 2936.5600000000004, "end": 2941.52, "text": " attention should they should they look into studying and and perhaps choosing the research", "tokens": [50928, 3202, 820, 436, 820, 436, 574, 666, 7601, 293, 293, 4317, 10875, 264, 2132, 51176], "temperature": 0.0, "avg_logprob": -0.07265859455257268, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0016477847239002585}, {"id": 384, "seek": 292528, "start": 2942.0800000000004, "end": 2949.76, "text": " at the moment the limelight is on um language models large language models which pass the", "tokens": [51204, 412, 264, 1623, 264, 2364, 338, 397, 307, 322, 1105, 2856, 5245, 2416, 2856, 5245, 597, 1320, 264, 51588], "temperature": 0.0, "avg_logprob": -0.07265859455257268, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0016477847239002585}, {"id": 385, "seek": 294976, "start": 2949.76, "end": 2957.1200000000003, "text": " touring tests and do all kinds of things that seemed inconceivable just a couple of years ago", "tokens": [50364, 32487, 6921, 293, 360, 439, 3685, 295, 721, 300, 6576, 20972, 384, 34376, 445, 257, 1916, 295, 924, 2057, 50732], "temperature": 0.0, "avg_logprob": -0.1001435479810161, "compression_ratio": 1.5705882352941176, "no_speech_prob": 0.00331869930960238}, {"id": 386, "seek": 294976, "start": 2957.1200000000003, "end": 2965.6800000000003, "text": " at least to some of those who are now surprised but of course that is just a tiny part of", "tokens": [50732, 412, 1935, 281, 512, 295, 729, 567, 366, 586, 6100, 457, 295, 1164, 300, 307, 445, 257, 5870, 644, 295, 51160], "temperature": 0.0, "avg_logprob": -0.1001435479810161, "compression_ratio": 1.5705882352941176, "no_speech_prob": 0.00331869930960238}, {"id": 387, "seek": 294976, "start": 2967.2000000000003, "end": 2974.48, "text": " what's going to be important to develop true ai agi artificial general intelligence", "tokens": [51236, 437, 311, 516, 281, 312, 1021, 281, 1499, 2074, 9783, 623, 72, 11677, 2674, 7599, 51600], "temperature": 0.0, "avg_logprob": -0.1001435479810161, "compression_ratio": 1.5705882352941176, "no_speech_prob": 0.00331869930960238}, {"id": 388, "seek": 297448, "start": 2975.44, "end": 2982.16, "text": " um on the other hand the roots of what we need to develop true ai also", "tokens": [50412, 1105, 322, 264, 661, 1011, 264, 10669, 295, 437, 321, 643, 281, 1499, 2074, 9783, 611, 50748], "temperature": 0.0, "avg_logprob": -0.08866555003796593, "compression_ratio": 1.6979865771812082, "no_speech_prob": 0.003998630680143833}, {"id": 389, "seek": 297448, "start": 2983.2, "end": 2988.88, "text": " come from the previous millennium they are not new and of course what you need is an", "tokens": [50800, 808, 490, 264, 3894, 21362, 2197, 436, 366, 406, 777, 293, 295, 1164, 437, 291, 643, 307, 364, 51084], "temperature": 0.0, "avg_logprob": -0.08866555003796593, "compression_ratio": 1.6979865771812082, "no_speech_prob": 0.003998630680143833}, {"id": 390, "seek": 297448, "start": 2988.88, "end": 2998.64, "text": " environment to interact with and you need an an agent that can manipulate the environment and you", "tokens": [51084, 2823, 281, 4648, 365, 293, 291, 643, 364, 364, 9461, 300, 393, 20459, 264, 2823, 293, 291, 51572], "temperature": 0.0, "avg_logprob": -0.08866555003796593, "compression_ratio": 1.6979865771812082, "no_speech_prob": 0.003998630680143833}, {"id": 391, "seek": 299864, "start": 2998.64, "end": 3007.2799999999997, "text": " need a way of learning to improve the rewards that you get from this environment as you are", "tokens": [50364, 643, 257, 636, 295, 2539, 281, 3470, 264, 17203, 300, 291, 483, 490, 341, 2823, 382, 291, 366, 50796], "temperature": 0.0, "avg_logprob": -0.05445144726679875, "compression_ratio": 1.8883248730964468, "no_speech_prob": 0.006483118981122971}, {"id": 392, "seek": 299864, "start": 3007.2799999999997, "end": 3015.68, "text": " interacting it with it within a single lifetime so one of the important aspects of reinforcement", "tokens": [50796, 18017, 309, 365, 309, 1951, 257, 2167, 11364, 370, 472, 295, 264, 1021, 7270, 295, 29280, 51216], "temperature": 0.0, "avg_logprob": -0.05445144726679875, "compression_ratio": 1.8883248730964468, "no_speech_prob": 0.006483118981122971}, {"id": 393, "seek": 299864, "start": 3015.68, "end": 3021.92, "text": " learning what we are now talking about is that you have only one single life you don't have", "tokens": [51216, 2539, 437, 321, 366, 586, 1417, 466, 307, 300, 291, 362, 787, 472, 2167, 993, 291, 500, 380, 362, 51528], "temperature": 0.0, "avg_logprob": -0.05445144726679875, "compression_ratio": 1.8883248730964468, "no_speech_prob": 0.006483118981122971}, {"id": 394, "seek": 299864, "start": 3021.92, "end": 3027.52, "text": " repeatable episodes like in most of traditional reinforcement learning no you have only one", "tokens": [51528, 7149, 712, 9313, 411, 294, 881, 295, 5164, 29280, 2539, 572, 291, 362, 787, 472, 51808], "temperature": 0.0, "avg_logprob": -0.05445144726679875, "compression_ratio": 1.8883248730964468, "no_speech_prob": 0.006483118981122971}, {"id": 395, "seek": 302752, "start": 3027.52, "end": 3035.68, "text": " single life and in the beginning you know nothing and then after 30 percent of your life is over", "tokens": [50364, 2167, 993, 293, 294, 264, 2863, 291, 458, 1825, 293, 550, 934, 2217, 3043, 295, 428, 993, 307, 670, 50772], "temperature": 0.0, "avg_logprob": -0.09385932078127002, "compression_ratio": 1.7898089171974523, "no_speech_prob": 0.01784709095954895}, {"id": 396, "seek": 302752, "start": 3035.68, "end": 3042.24, "text": " you know something about life and all you know is the data that you collected during these first", "tokens": [50772, 291, 458, 746, 466, 993, 293, 439, 291, 458, 307, 264, 1412, 300, 291, 11087, 1830, 613, 700, 51100], "temperature": 0.0, "avg_logprob": -0.09385932078127002, "compression_ratio": 1.7898089171974523, "no_speech_prob": 0.01784709095954895}, {"id": 397, "seek": 302752, "start": 3042.24, "end": 3050.4, "text": " 30 percent of your life and now there is an infinite almost infinite possibility set of", "tokens": [51100, 2217, 3043, 295, 428, 993, 293, 586, 456, 307, 364, 13785, 1920, 13785, 7959, 992, 295, 51508], "temperature": 0.0, "avg_logprob": -0.09385932078127002, "compression_ratio": 1.7898089171974523, "no_speech_prob": 0.01784709095954895}, {"id": 398, "seek": 305040, "start": 3050.4, "end": 3060.64, "text": " possibilities of futures and from this little short experience you have to generalize somehow", "tokens": [50364, 12178, 295, 26071, 293, 490, 341, 707, 2099, 1752, 291, 362, 281, 2674, 1125, 6063, 50876], "temperature": 0.0, "avg_logprob": -0.07976671457290649, "compression_ratio": 1.8, "no_speech_prob": 0.004464102443307638}, {"id": 399, "seek": 305040, "start": 3060.64, "end": 3067.04, "text": " and try to select action sequences that lead to the most promising futures that you can shape", "tokens": [50876, 293, 853, 281, 3048, 3069, 22978, 300, 1477, 281, 264, 881, 20257, 26071, 300, 291, 393, 3909, 51196], "temperature": 0.0, "avg_logprob": -0.07976671457290649, "compression_ratio": 1.8, "no_speech_prob": 0.004464102443307638}, {"id": 400, "seek": 305040, "start": 3067.04, "end": 3073.6800000000003, "text": " yourself through your actions now to achieve all of that you need to build a model of the world a", "tokens": [51196, 1803, 807, 428, 5909, 586, 281, 4584, 439, 295, 300, 291, 643, 281, 1322, 257, 2316, 295, 264, 1002, 257, 51528], "temperature": 0.0, "avg_logprob": -0.07976671457290649, "compression_ratio": 1.8, "no_speech_prob": 0.004464102443307638}, {"id": 401, "seek": 305040, "start": 3073.6800000000003, "end": 3080.2400000000002, "text": " predictive model of the world which means that you have to be able to learn over time and to", "tokens": [51528, 35521, 2316, 295, 264, 1002, 597, 1355, 300, 291, 362, 281, 312, 1075, 281, 1466, 670, 565, 293, 281, 51856], "temperature": 0.0, "avg_logprob": -0.07976671457290649, "compression_ratio": 1.8, "no_speech_prob": 0.004464102443307638}, {"id": 402, "seek": 308040, "start": 3080.4, "end": 3085.36, "text": " predict the consequences of your actions so that you can use this model of the world that you are", "tokens": [50364, 6069, 264, 10098, 295, 428, 5909, 370, 300, 291, 393, 764, 341, 2316, 295, 264, 1002, 300, 291, 366, 50612], "temperature": 0.0, "avg_logprob": -0.07362607251042905, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.004529057070612907}, {"id": 403, "seek": 308040, "start": 3085.36, "end": 3093.6800000000003, "text": " acquiring there to plan to plan ahead and you want to do that in a way that isn't the naive way", "tokens": [50612, 37374, 456, 281, 1393, 281, 1393, 2286, 293, 291, 528, 281, 360, 300, 294, 257, 636, 300, 1943, 380, 264, 29052, 636, 51028], "temperature": 0.0, "avg_logprob": -0.07362607251042905, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.004529057070612907}, {"id": 404, "seek": 308040, "start": 3093.6800000000003, "end": 3100.0, "text": " which we had in 1990 which is millisecond by millisecond planning where you say okay now", "tokens": [51028, 597, 321, 632, 294, 13384, 597, 307, 27940, 18882, 538, 27940, 18882, 5038, 689, 291, 584, 1392, 586, 51344], "temperature": 0.0, "avg_logprob": -0.07362607251042905, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.004529057070612907}, {"id": 405, "seek": 308040, "start": 3101.6800000000003, "end": 3109.6800000000003, "text": " I'm moving from A to B and the way to do it is first move that little pinky muscle a little bit", "tokens": [51428, 286, 478, 2684, 490, 316, 281, 363, 293, 264, 636, 281, 360, 309, 307, 700, 1286, 300, 707, 42616, 8679, 257, 707, 857, 51828], "temperature": 0.0, "avg_logprob": -0.07362607251042905, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.004529057070612907}, {"id": 406, "seek": 310968, "start": 3109.68, "end": 3114.0, "text": " and move it a little bit more and move it a little bit more and then get up and so no you want to do", "tokens": [50364, 293, 1286, 309, 257, 707, 857, 544, 293, 1286, 309, 257, 707, 857, 544, 293, 550, 483, 493, 293, 370, 572, 291, 528, 281, 360, 50580], "temperature": 0.0, "avg_logprob": -0.1289073691076162, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0017793404404073954}, {"id": 407, "seek": 310968, "start": 3114.0, "end": 3123.52, "text": " that in a high level way in a hierarchical way in a way that allows you to to focus on the important", "tokens": [50580, 300, 294, 257, 1090, 1496, 636, 294, 257, 35250, 804, 636, 294, 257, 636, 300, 4045, 291, 281, 281, 1879, 322, 264, 1021, 51056], "temperature": 0.0, "avg_logprob": -0.1289073691076162, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0017793404404073954}, {"id": 408, "seek": 310968, "start": 3124.16, "end": 3132.48, "text": " abstract concepts for example as you are trying to go from from your home to Beijing you decompose", "tokens": [51088, 12649, 10392, 337, 1365, 382, 291, 366, 1382, 281, 352, 490, 490, 428, 1280, 281, 20240, 291, 22867, 541, 51504], "temperature": 0.0, "avg_logprob": -0.1289073691076162, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0017793404404073954}, {"id": 409, "seek": 310968, "start": 3132.48, "end": 3139.44, "text": " this whole future into a couple of sub goals you say a first important step is to go to the cap", "tokens": [51504, 341, 1379, 2027, 666, 257, 1916, 295, 1422, 5493, 291, 584, 257, 700, 1021, 1823, 307, 281, 352, 281, 264, 1410, 51852], "temperature": 0.0, "avg_logprob": -0.1289073691076162, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0017793404404073954}, {"id": 410, "seek": 313944, "start": 3139.52, "end": 3147.52, "text": " station and get a taxi to the airport and then in the airport you will find your your plane", "tokens": [50368, 5214, 293, 483, 257, 18984, 281, 264, 10155, 293, 550, 294, 264, 10155, 291, 486, 915, 428, 428, 5720, 50768], "temperature": 0.0, "avg_logprob": -0.08691084662149119, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.0013647584710270166}, {"id": 411, "seek": 313944, "start": 3147.52, "end": 3152.48, "text": " and then for nine hours nothing is going to happen and you exit in Beijing and have to find another", "tokens": [50768, 293, 550, 337, 4949, 2496, 1825, 307, 516, 281, 1051, 293, 291, 11043, 294, 20240, 293, 362, 281, 915, 1071, 51016], "temperature": 0.0, "avg_logprob": -0.08691084662149119, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.0013647584710270166}, {"id": 412, "seek": 313944, "start": 3152.48, "end": 3158.96, "text": " cab and so on so you you don't do millisecond by millisecond detailed planning no you have", "tokens": [51016, 5487, 293, 370, 322, 370, 291, 291, 500, 380, 360, 27940, 18882, 538, 27940, 18882, 9942, 5038, 572, 291, 362, 51340], "temperature": 0.0, "avg_logprob": -0.08691084662149119, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.0013647584710270166}, {"id": 413, "seek": 313944, "start": 3160.08, "end": 3166.64, "text": " high level planning to just reduce the computational effort and focus on the essentials of what you", "tokens": [51396, 1090, 1496, 5038, 281, 445, 5407, 264, 28270, 4630, 293, 1879, 322, 264, 46884, 295, 437, 291, 51724], "temperature": 0.0, "avg_logprob": -0.08691084662149119, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.0013647584710270166}, {"id": 414, "seek": 316664, "start": 3166.64, "end": 3173.3599999999997, "text": " want to do so that is something that most current systems don't do but for a long time we have had", "tokens": [50364, 528, 281, 360, 370, 300, 307, 746, 300, 881, 2190, 3652, 500, 380, 360, 457, 337, 257, 938, 565, 321, 362, 632, 50700], "temperature": 0.0, "avg_logprob": -0.06870563731474034, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0031681747641414404}, {"id": 415, "seek": 316664, "start": 3173.3599999999997, "end": 3179.2, "text": " systems like that and they are getting more sophisticated over time important you have a", "tokens": [50700, 3652, 411, 300, 293, 436, 366, 1242, 544, 16950, 670, 565, 1021, 291, 362, 257, 50992], "temperature": 0.0, "avg_logprob": -0.06870563731474034, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0031681747641414404}, {"id": 416, "seek": 316664, "start": 3179.2, "end": 3184.56, "text": " predictive model of the world that is not just focusing on the pixels and predicting the how", "tokens": [50992, 35521, 2316, 295, 264, 1002, 300, 307, 406, 445, 8416, 322, 264, 18668, 293, 32884, 264, 577, 51260], "temperature": 0.0, "avg_logprob": -0.06870563731474034, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0031681747641414404}, {"id": 417, "seek": 316664, "start": 3184.56, "end": 3191.2799999999997, "text": " does the video change as I'm moving my hand back and forth the video that I get through my camera", "tokens": [51260, 775, 264, 960, 1319, 382, 286, 478, 2684, 452, 1011, 646, 293, 5220, 264, 960, 300, 286, 483, 807, 452, 2799, 51596], "temperature": 0.0, "avg_logprob": -0.06870563731474034, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0031681747641414404}, {"id": 418, "seek": 319128, "start": 3191.36, "end": 3201.0400000000004, "text": " my eyes and so on and no higher level concepts that that reflect islands of predictability many", "tokens": [50368, 452, 2575, 293, 370, 322, 293, 572, 2946, 1496, 10392, 300, 300, 5031, 17402, 295, 6069, 2310, 867, 50852], "temperature": 0.0, "avg_logprob": -0.09873310507160343, "compression_ratio": 1.86, "no_speech_prob": 0.013788311742246151}, {"id": 419, "seek": 319128, "start": 3201.0400000000004, "end": 3206.0, "text": " things are not predictable but certain abstract representations of these things are predictable", "tokens": [50852, 721, 366, 406, 27737, 457, 1629, 12649, 33358, 295, 613, 721, 366, 27737, 51100], "temperature": 0.0, "avg_logprob": -0.09873310507160343, "compression_ratio": 1.86, "no_speech_prob": 0.013788311742246151}, {"id": 420, "seek": 319128, "start": 3206.0, "end": 3211.52, "text": " and so how can you discover these higher level concepts that you need to efficiently think", "tokens": [51100, 293, 370, 577, 393, 291, 4411, 613, 2946, 1496, 10392, 300, 291, 643, 281, 19621, 519, 51376], "temperature": 0.0, "avg_logprob": -0.09873310507160343, "compression_ratio": 1.86, "no_speech_prob": 0.013788311742246151}, {"id": 421, "seek": 319128, "start": 3211.52, "end": 3217.52, "text": " about your own future options and select those that are most promising in the single life", "tokens": [51376, 466, 428, 1065, 2027, 3956, 293, 3048, 729, 300, 366, 881, 20257, 294, 264, 2167, 993, 51676], "temperature": 0.0, "avg_logprob": -0.09873310507160343, "compression_ratio": 1.86, "no_speech_prob": 0.013788311742246151}, {"id": 422, "seek": 321752, "start": 3217.84, "end": 3225.44, "text": " yeah yeah this is really interesting so we've been speaking with Carl Friston for example and he", "tokens": [50380, 1338, 1338, 341, 307, 534, 1880, 370, 321, 600, 668, 4124, 365, 14256, 1526, 47345, 337, 1365, 293, 415, 50760], "temperature": 0.0, "avg_logprob": -0.09453959797703942, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.003124015871435404}, {"id": 423, "seek": 321752, "start": 3225.44, "end": 3232.32, "text": " talks about this collective intelligence where you have this multi-agent cybernetic framework", "tokens": [50760, 6686, 466, 341, 12590, 7599, 689, 291, 362, 341, 4825, 12, 559, 317, 13411, 77, 3532, 8388, 51104], "temperature": 0.0, "avg_logprob": -0.09453959797703942, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.003124015871435404}, {"id": 424, "seek": 321752, "start": 3232.32, "end": 3238.0, "text": " which is causally closed and one of the things we're talking about here really is not the model", "tokens": [51104, 597, 307, 3302, 379, 5395, 293, 472, 295, 264, 721, 321, 434, 1417, 466, 510, 534, 307, 406, 264, 2316, 51388], "temperature": 0.0, "avg_logprob": -0.09453959797703942, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.003124015871435404}, {"id": 425, "seek": 321752, "start": 3238.0, "end": 3244.88, "text": " itself people talk about chat gpt and it's just a model and people have configured it in arrangements", "tokens": [51388, 2564, 561, 751, 466, 5081, 290, 662, 293, 309, 311, 445, 257, 2316, 293, 561, 362, 30538, 309, 294, 22435, 51732], "temperature": 0.0, "avg_logprob": -0.09453959797703942, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.003124015871435404}, {"id": 426, "seek": 324488, "start": 3244.96, "end": 3249.84, "text": " that have varying degrees of autonomy and in the future we will develop these collective", "tokens": [50368, 300, 362, 22984, 5310, 295, 27278, 293, 294, 264, 2027, 321, 486, 1499, 613, 12590, 50612], "temperature": 0.0, "avg_logprob": -0.0387957119474224, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.011058300733566284}, {"id": 427, "seek": 324488, "start": 3249.84, "end": 3256.0, "text": " intelligences and they're not just predicting the actions and behaviors of other agents but even", "tokens": [50612, 5613, 2667, 293, 436, 434, 406, 445, 32884, 264, 5909, 293, 15501, 295, 661, 12554, 457, 754, 50920], "temperature": 0.0, "avg_logprob": -0.0387957119474224, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.011058300733566284}, {"id": 428, "seek": 324488, "start": 3256.0, "end": 3263.04, "text": " the world that we're in is a computer to some extent so when you imbue agents with this kind of", "tokens": [50920, 264, 1002, 300, 321, 434, 294, 307, 257, 3820, 281, 512, 8396, 370, 562, 291, 566, 65, 622, 12554, 365, 341, 733, 295, 51272], "temperature": 0.0, "avg_logprob": -0.0387957119474224, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.011058300733566284}, {"id": 429, "seek": 324488, "start": 3263.04, "end": 3268.4, "text": " creativity and autonomy that's the thing that I don't think people really understand what might", "tokens": [51272, 12915, 293, 27278, 300, 311, 264, 551, 300, 286, 500, 380, 519, 561, 534, 1223, 437, 1062, 51540], "temperature": 0.0, "avg_logprob": -0.0387957119474224, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.011058300733566284}, {"id": 430, "seek": 324488, "start": 3268.4, "end": 3273.76, "text": " emerge from that it's related to this discussion about what kind of goals might emerge from that", "tokens": [51540, 21511, 490, 300, 309, 311, 4077, 281, 341, 5017, 466, 437, 733, 295, 5493, 1062, 21511, 490, 300, 51808], "temperature": 0.0, "avg_logprob": -0.0387957119474224, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.011058300733566284}, {"id": 431, "seek": 327376, "start": 3274.4, "end": 3279.6000000000004, "text": " do you have any intuition on what that would look like yeah let me give you just the simplest", "tokens": [50396, 360, 291, 362, 604, 24002, 322, 437, 300, 576, 574, 411, 1338, 718, 385, 976, 291, 445, 264, 22811, 50656], "temperature": 0.0, "avg_logprob": -0.12294348803433505, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.002141903853043914}, {"id": 432, "seek": 327376, "start": 3279.6000000000004, "end": 3290.0800000000004, "text": " example that we had in 1990 or 32 years ago of a system that sets itself its own goals and it", "tokens": [50656, 1365, 300, 321, 632, 294, 13384, 420, 8858, 924, 2057, 295, 257, 1185, 300, 6352, 2564, 1080, 1065, 5493, 293, 309, 51180], "temperature": 0.0, "avg_logprob": -0.12294348803433505, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.002141903853043914}, {"id": 433, "seek": 327376, "start": 3290.0800000000004, "end": 3297.5200000000004, "text": " consists of two artificial neural networks and I know that Carl Friston is very interested in that", "tokens": [51180, 14689, 295, 732, 11677, 18161, 9590, 293, 286, 458, 300, 14256, 1526, 47345, 307, 588, 3102, 294, 300, 51552], "temperature": 0.0, "avg_logprob": -0.12294348803433505, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.002141903853043914}, {"id": 434, "seek": 329752, "start": 3297.52, "end": 3305.84, "text": " and only recently for the first time in my life I was on a paper where he was co-author", "tokens": [50364, 293, 787, 3938, 337, 264, 700, 565, 294, 452, 993, 286, 390, 322, 257, 3035, 689, 415, 390, 598, 12, 34224, 50780], "temperature": 0.0, "avg_logprob": -0.06611310111151801, "compression_ratio": 1.6047904191616766, "no_speech_prob": 0.0031207941938191652}, {"id": 435, "seek": 329752, "start": 3306.56, "end": 3315.7599999999998, "text": " just a year ago and so back then it was really about a reinforcement learning agent and it", "tokens": [50816, 445, 257, 1064, 2057, 293, 370, 646, 550, 309, 390, 534, 466, 257, 29280, 2539, 9461, 293, 309, 51276], "temperature": 0.0, "avg_logprob": -0.06611310111151801, "compression_ratio": 1.6047904191616766, "no_speech_prob": 0.0031207941938191652}, {"id": 436, "seek": 329752, "start": 3317.12, "end": 3323.36, "text": " interacts with the world and it generates actions that change the world and then there is", "tokens": [51344, 43582, 365, 264, 1002, 293, 309, 23815, 5909, 300, 1319, 264, 1002, 293, 550, 456, 307, 51656], "temperature": 0.0, "avg_logprob": -0.06611310111151801, "compression_ratio": 1.6047904191616766, "no_speech_prob": 0.0031207941938191652}, {"id": 437, "seek": 332336, "start": 3323.36, "end": 3335.28, "text": " another network which just is trying to predict the consequences of the actions in the environment", "tokens": [50364, 1071, 3209, 597, 445, 307, 1382, 281, 6069, 264, 10098, 295, 264, 5909, 294, 264, 2823, 50960], "temperature": 0.0, "avg_logprob": -0.1302247852474064, "compression_ratio": 1.9427083333333333, "no_speech_prob": 0.003480352461338043}, {"id": 438, "seek": 332336, "start": 3335.28, "end": 3340.8, "text": " so the reactions of the environment to these actions and so that becomes a world model and", "tokens": [50960, 370, 264, 12215, 295, 264, 2823, 281, 613, 5909, 293, 370, 300, 3643, 257, 1002, 2316, 293, 51236], "temperature": 0.0, "avg_logprob": -0.1302247852474064, "compression_ratio": 1.9427083333333333, "no_speech_prob": 0.003480352461338043}, {"id": 439, "seek": 332336, "start": 3340.8, "end": 3348.32, "text": " then what kind of goal was there which was different from traditional goals well in the", "tokens": [51236, 550, 437, 733, 295, 3387, 390, 456, 597, 390, 819, 490, 5164, 5493, 731, 294, 264, 51612], "temperature": 0.0, "avg_logprob": -0.1302247852474064, "compression_ratio": 1.9427083333333333, "no_speech_prob": 0.003480352461338043}, {"id": 440, "seek": 332336, "start": 3348.32, "end": 3353.1200000000003, "text": " beginning this model of the world this prediction machine which is a model of the world a world", "tokens": [51612, 2863, 341, 2316, 295, 264, 1002, 341, 17630, 3479, 597, 307, 257, 2316, 295, 264, 1002, 257, 1002, 51852], "temperature": 0.0, "avg_logprob": -0.1302247852474064, "compression_ratio": 1.9427083333333333, "no_speech_prob": 0.003480352461338043}, {"id": 441, "seek": 335312, "start": 3353.12, "end": 3361.2799999999997, "text": " model knows nothing so it has high error as it is trying to predict the next thing as it is trying", "tokens": [50364, 2316, 3255, 1825, 370, 309, 575, 1090, 6713, 382, 309, 307, 1382, 281, 6069, 264, 958, 551, 382, 309, 307, 1382, 50772], "temperature": 0.0, "avg_logprob": -0.06329641576673163, "compression_ratio": 1.9102564102564104, "no_speech_prob": 0.00038587304879911244}, {"id": 442, "seek": 335312, "start": 3361.2799999999997, "end": 3372.64, "text": " to predict the reactions of the environment to the actions of the agent so as the second network", "tokens": [50772, 281, 6069, 264, 12215, 295, 264, 2823, 281, 264, 5909, 295, 264, 9461, 370, 382, 264, 1150, 3209, 51340], "temperature": 0.0, "avg_logprob": -0.06329641576673163, "compression_ratio": 1.9102564102564104, "no_speech_prob": 0.00038587304879911244}, {"id": 443, "seek": 335312, "start": 3372.64, "end": 3378.3199999999997, "text": " is trying to reduce its prediction error through gradient descent through back propagation essentially", "tokens": [51340, 307, 1382, 281, 5407, 1080, 17630, 6713, 807, 16235, 23475, 807, 646, 38377, 4476, 51624], "temperature": 0.0, "avg_logprob": -0.06329641576673163, "compression_ratio": 1.9102564102564104, "no_speech_prob": 0.00038587304879911244}, {"id": 444, "seek": 337832, "start": 3379.04, "end": 3388.2400000000002, "text": " the other one is trying to generate actions outputs that maximize the same error so basically", "tokens": [50400, 264, 661, 472, 307, 1382, 281, 8460, 5909, 23930, 300, 19874, 264, 912, 6713, 370, 1936, 50860], "temperature": 0.0, "avg_logprob": -0.09218588183003087, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.005908786319196224}, {"id": 445, "seek": 337832, "start": 3388.2400000000002, "end": 3395.1200000000003, "text": " the goal the self-invented goal if you will of the first network is to generate an action", "tokens": [50860, 264, 3387, 264, 2698, 12, 259, 2475, 292, 3387, 498, 291, 486, 295, 264, 700, 3209, 307, 281, 8460, 364, 3069, 51204], "temperature": 0.0, "avg_logprob": -0.09218588183003087, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.005908786319196224}, {"id": 446, "seek": 337832, "start": 3396.0800000000004, "end": 3402.2400000000002, "text": " with whose consequences cannot yet be predicted by the other network by the model of the world", "tokens": [51252, 365, 6104, 10098, 2644, 1939, 312, 19147, 538, 264, 661, 3209, 538, 264, 2316, 295, 264, 1002, 51560], "temperature": 0.0, "avg_logprob": -0.09218588183003087, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.005908786319196224}, {"id": 447, "seek": 340224, "start": 3403.04, "end": 3408.0, "text": " so the first network is generating outputs that surprise the second network", "tokens": [50404, 370, 264, 700, 3209, 307, 17746, 23930, 300, 6365, 264, 1150, 3209, 50652], "temperature": 0.0, "avg_logprob": -0.08296023368835449, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.003642590716481209}, {"id": 448, "seek": 340224, "start": 3409.9199999999996, "end": 3416.9599999999996, "text": " so suddenly you have an incentive where the first network is trying to invent actions", "tokens": [50748, 370, 5800, 291, 362, 364, 22346, 689, 264, 700, 3209, 307, 1382, 281, 7962, 5909, 51100], "temperature": 0.0, "avg_logprob": -0.08296023368835449, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.003642590716481209}, {"id": 449, "seek": 340224, "start": 3416.9599999999996, "end": 3426.7999999999997, "text": " experiments that fool or that surprise the second network and that was called artificial curiosity", "tokens": [51100, 12050, 300, 7979, 420, 300, 6365, 264, 1150, 3209, 293, 300, 390, 1219, 11677, 18769, 51592], "temperature": 0.0, "avg_logprob": -0.08296023368835449, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.003642590716481209}, {"id": 450, "seek": 342680, "start": 3427.2000000000003, "end": 3436.4, "text": " so now suddenly you have a little agent which a little bit like a baby doesn't learn by", "tokens": [50384, 370, 586, 5800, 291, 362, 257, 707, 9461, 597, 257, 707, 857, 411, 257, 3186, 1177, 380, 1466, 538, 50844], "temperature": 0.0, "avg_logprob": -0.09614958614110947, "compression_ratio": 1.668639053254438, "no_speech_prob": 0.006792254745960236}, {"id": 451, "seek": 342680, "start": 3436.4, "end": 3445.1200000000003, "text": " imitating the parents no it learns by inventing its own little sub goals and it's trying to surprise", "tokens": [50844, 566, 16350, 264, 3152, 572, 309, 27152, 538, 7962, 278, 1080, 1065, 707, 1422, 5493, 293, 309, 311, 1382, 281, 6365, 51280], "temperature": 0.0, "avg_logprob": -0.09614958614110947, "compression_ratio": 1.668639053254438, "no_speech_prob": 0.006792254745960236}, {"id": 452, "seek": 342680, "start": 3445.1200000000003, "end": 3453.44, "text": " itself and have fun by playing with the toys and and observing new unpredictable things which", "tokens": [51280, 2564, 293, 362, 1019, 538, 2433, 365, 264, 13753, 293, 293, 22107, 777, 31160, 721, 597, 51696], "temperature": 0.0, "avg_logprob": -0.09614958614110947, "compression_ratio": 1.668639053254438, "no_speech_prob": 0.006792254745960236}, {"id": 453, "seek": 345344, "start": 3453.44, "end": 3459.44, "text": " however become predictable over time and therefore become boring and then it has another incentive", "tokens": [50364, 4461, 1813, 27737, 670, 565, 293, 4412, 1813, 9989, 293, 550, 309, 575, 1071, 22346, 50664], "temperature": 0.0, "avg_logprob": -0.061472047170003256, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.003536572214215994}, {"id": 454, "seek": 345344, "start": 3459.44, "end": 3466.56, "text": " to invent the additional experiments such that it still can surprise its model of the world", "tokens": [50664, 281, 7962, 264, 4497, 12050, 1270, 300, 309, 920, 393, 6365, 1080, 2316, 295, 264, 1002, 51020], "temperature": 0.0, "avg_logprob": -0.061472047170003256, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.003536572214215994}, {"id": 455, "seek": 345344, "start": 3466.56, "end": 3472.8, "text": " which in turn is improving and so on so artificial curiosity does that does that also", "tokens": [51020, 597, 294, 1261, 307, 11470, 293, 370, 322, 370, 11677, 18769, 775, 300, 775, 300, 611, 51332], "temperature": 0.0, "avg_logprob": -0.061472047170003256, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.003536572214215994}, {"id": 456, "seek": 345344, "start": 3473.44, "end": 3478.0, "text": " have the effect of making the network which is trying to predict does it have the effect of", "tokens": [51364, 362, 264, 1802, 295, 1455, 264, 3209, 597, 307, 1382, 281, 6069, 775, 309, 362, 264, 1802, 295, 51592], "temperature": 0.0, "avg_logprob": -0.061472047170003256, "compression_ratio": 1.812807881773399, "no_speech_prob": 0.003536572214215994}, {"id": 457, "seek": 347800, "start": 3478.0, "end": 3483.76, "text": " making it more robust and more generalizable like almost a form of you know regularization", "tokens": [50364, 1455, 309, 544, 13956, 293, 544, 2674, 22395, 411, 1920, 257, 1254, 295, 291, 458, 3890, 2144, 50652], "temperature": 0.0, "avg_logprob": -0.0934383361065974, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.010480516590178013}, {"id": 458, "seek": 347800, "start": 3483.76, "end": 3491.84, "text": " kind of built in in this pairing yeah you can build into that network all kinds of regularizers", "tokens": [50652, 733, 295, 3094, 294, 294, 341, 32735, 1338, 291, 393, 1322, 666, 300, 3209, 439, 3685, 295, 3890, 22525, 51056], "temperature": 0.0, "avg_logprob": -0.0934383361065974, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.010480516590178013}, {"id": 459, "seek": 347800, "start": 3492.4, "end": 3501.68, "text": " an orthogonal concept which is also very important so that was just the first version that was", "tokens": [51084, 364, 41488, 3410, 597, 307, 611, 588, 1021, 370, 300, 390, 445, 264, 700, 3037, 300, 390, 51548], "temperature": 0.0, "avg_logprob": -0.0934383361065974, "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.010480516590178013}, {"id": 460, "seek": 350168, "start": 3501.68, "end": 3509.2799999999997, "text": " really in 1990 and then we have had a we had a long string of papers just on improvements of", "tokens": [50364, 534, 294, 13384, 293, 550, 321, 362, 632, 257, 321, 632, 257, 938, 6798, 295, 10577, 445, 322, 13797, 295, 50744], "temperature": 0.0, "avg_logprob": -0.08157797557551687, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.004900693893432617}, {"id": 461, "seek": 350168, "start": 3509.2799999999997, "end": 3516.3199999999997, "text": " this original concept of artificial curiosity so this old system is basically what you what you", "tokens": [50744, 341, 3380, 3410, 295, 11677, 18769, 370, 341, 1331, 1185, 307, 1936, 437, 291, 437, 291, 51096], "temperature": 0.0, "avg_logprob": -0.08157797557551687, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.004900693893432617}, {"id": 462, "seek": 350168, "start": 3516.3199999999997, "end": 3522.3199999999997, "text": " now know as GANs Generative Adversarial Networks because the first network is generating a probability", "tokens": [51096, 586, 458, 382, 460, 1770, 82, 15409, 1166, 1999, 840, 44745, 12640, 82, 570, 264, 700, 3209, 307, 17746, 257, 8482, 51396], "temperature": 0.0, "avg_logprob": -0.08157797557551687, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.004900693893432617}, {"id": 463, "seek": 350168, "start": 3522.3199999999997, "end": 3529.68, "text": " distribution over outputs and the second network is then predicting the consequences of these outputs", "tokens": [51396, 7316, 670, 23930, 293, 264, 1150, 3209, 307, 550, 32884, 264, 10098, 295, 613, 23930, 51764], "temperature": 0.0, "avg_logprob": -0.08157797557551687, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.004900693893432617}, {"id": 464, "seek": 352968, "start": 3529.68, "end": 3538.3199999999997, "text": " in the environment and if you if the output is an image then the consequence can be either this", "tokens": [50364, 294, 264, 2823, 293, 498, 291, 498, 264, 5598, 307, 364, 3256, 550, 264, 18326, 393, 312, 2139, 341, 50796], "temperature": 0.0, "avg_logprob": -0.09092042058013207, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0016463079955428839}, {"id": 465, "seek": 352968, "start": 3538.3199999999997, "end": 3544.8799999999997, "text": " image is of a certain type yes or not no and then that's all that the prediction machine the world", "tokens": [50796, 3256, 307, 295, 257, 1629, 2010, 2086, 420, 406, 572, 293, 550, 300, 311, 439, 300, 264, 17630, 3479, 264, 1002, 51124], "temperature": 0.0, "avg_logprob": -0.09092042058013207, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0016463079955428839}, {"id": 466, "seek": 352968, "start": 3544.8799999999997, "end": 3550.16, "text": " model predicts in that simple case and you minimize the first network minimizes the same", "tokens": [51124, 2316, 6069, 82, 294, 300, 2199, 1389, 293, 291, 17522, 264, 700, 3209, 4464, 5660, 264, 912, 51388], "temperature": 0.0, "avg_logprob": -0.09092042058013207, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0016463079955428839}, {"id": 467, "seek": 352968, "start": 3550.96, "end": 3556.96, "text": " error function that the second one maximizes so then you have basically a GAN but then you", "tokens": [51428, 6713, 2445, 300, 264, 1150, 472, 5138, 5660, 370, 550, 291, 362, 1936, 257, 460, 1770, 457, 550, 291, 51728], "temperature": 0.0, "avg_logprob": -0.09092042058013207, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0016463079955428839}, {"id": 468, "seek": 355696, "start": 3556.96, "end": 3564.2400000000002, "text": " don't have what you just mentioned yet the regularizer as a scientist what you really want to learn is", "tokens": [50364, 500, 380, 362, 437, 291, 445, 2835, 1939, 264, 3890, 6545, 382, 257, 12662, 437, 291, 534, 528, 281, 1466, 307, 50728], "temperature": 0.0, "avg_logprob": -0.08752966696216215, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.0014540470438078046}, {"id": 469, "seek": 355696, "start": 3564.96, "end": 3573.6, "text": " a model of the world that extracts the regularities in the environment that that", "tokens": [50764, 257, 2316, 295, 264, 1002, 300, 8947, 82, 264, 3890, 1088, 294, 264, 2823, 300, 300, 51196], "temperature": 0.0, "avg_logprob": -0.08752966696216215, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.0014540470438078046}, {"id": 470, "seek": 355696, "start": 3576.32, "end": 3583.52, "text": " that finds predictable things which are regular in the sense that there's a short explanation there", "tokens": [51332, 300, 10704, 27737, 721, 597, 366, 3890, 294, 264, 2020, 300, 456, 311, 257, 2099, 10835, 456, 51692], "temperature": 0.0, "avg_logprob": -0.08752966696216215, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.0014540470438078046}, {"id": 471, "seek": 358352, "start": 3583.52, "end": 3591.92, "text": " of for example if you have falling objects in a video then they all fall in the same way they", "tokens": [50364, 295, 337, 1365, 498, 291, 362, 7440, 6565, 294, 257, 960, 550, 436, 439, 2100, 294, 264, 912, 636, 436, 50784], "temperature": 0.0, "avg_logprob": -0.07175493240356445, "compression_ratio": 1.84, "no_speech_prob": 0.005054348614066839}, {"id": 472, "seek": 358352, "start": 3591.92, "end": 3597.04, "text": " accelerate in the same way which means it's predictable what these objects do if you see two", "tokens": [50784, 21341, 294, 264, 912, 636, 597, 1355, 309, 311, 27737, 437, 613, 6565, 360, 498, 291, 536, 732, 51040], "temperature": 0.0, "avg_logprob": -0.07175493240356445, "compression_ratio": 1.84, "no_speech_prob": 0.005054348614066839}, {"id": 473, "seek": 358352, "start": 3597.04, "end": 3604.08, "text": " of the frames you can predict the third frame pretty well and the law behind that is very simple", "tokens": [51040, 295, 264, 12083, 291, 393, 6069, 264, 2636, 3920, 1238, 731, 293, 264, 2101, 2261, 300, 307, 588, 2199, 51392], "temperature": 0.0, "avg_logprob": -0.07175493240356445, "compression_ratio": 1.84, "no_speech_prob": 0.005054348614066839}, {"id": 474, "seek": 358352, "start": 3604.88, "end": 3612.08, "text": " this means that you can greatly compress the video that is coming in because you can", "tokens": [51432, 341, 1355, 300, 291, 393, 14147, 14778, 264, 960, 300, 307, 1348, 294, 570, 291, 393, 51792], "temperature": 0.0, "avg_logprob": -0.07175493240356445, "compression_ratio": 1.84, "no_speech_prob": 0.005054348614066839}, {"id": 475, "seek": 361352, "start": 3613.52, "end": 3618.8, "text": " instead of storing all the pixels you can compute many of these pixels by just looking at two", "tokens": [50364, 2602, 295, 26085, 439, 264, 18668, 291, 393, 14722, 867, 295, 613, 18668, 538, 445, 1237, 412, 732, 50628], "temperature": 0.0, "avg_logprob": -0.06111821645422827, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.0016730019124224782}, {"id": 476, "seek": 361352, "start": 3618.8, "end": 3625.04, "text": " successive frames and predicting the third frame or maybe three successive frames and predicting the", "tokens": [50628, 48043, 12083, 293, 32884, 264, 2636, 3920, 420, 1310, 1045, 48043, 12083, 293, 32884, 264, 50940], "temperature": 0.0, "avg_logprob": -0.06111821645422827, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.0016730019124224782}, {"id": 477, "seek": 361352, "start": 3625.04, "end": 3631.6, "text": " fourth frame something like that and you only have to encode the deviations from the prediction so", "tokens": [50940, 6409, 3920, 746, 411, 300, 293, 291, 787, 362, 281, 2058, 1429, 264, 31219, 763, 490, 264, 17630, 370, 51268], "temperature": 0.0, "avg_logprob": -0.06111821645422827, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.0016730019124224782}, {"id": 478, "seek": 361352, "start": 3631.6, "end": 3637.68, "text": " everything else you don't have to store separately which means you once you understand gravity you", "tokens": [51268, 1203, 1646, 291, 500, 380, 362, 281, 3531, 14759, 597, 1355, 291, 1564, 291, 1223, 12110, 291, 51572], "temperature": 0.0, "avg_logprob": -0.06111821645422827, "compression_ratio": 1.875598086124402, "no_speech_prob": 0.0016730019124224782}, {"id": 479, "seek": 363768, "start": 3637.7599999999998, "end": 3644.96, "text": " can greatly greatly compress the video so that's what you really want to do and so the more advanced", "tokens": [50368, 393, 14147, 14147, 14778, 264, 960, 370, 300, 311, 437, 291, 534, 528, 281, 360, 293, 370, 264, 544, 7339, 50728], "temperature": 0.0, "avg_logprob": -0.11067964881658554, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.0005701545742340386}, {"id": 480, "seek": 363768, "start": 3644.96, "end": 3653.2799999999997, "text": " version of artificial curiosity is about that where you have a motivation to find a disruption", "tokens": [50728, 3037, 295, 11677, 18769, 307, 466, 300, 689, 291, 362, 257, 12335, 281, 915, 257, 28751, 51144], "temperature": 0.0, "avg_logprob": -0.11067964881658554, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.0005701545742340386}, {"id": 481, "seek": 363768, "start": 3653.8399999999997, "end": 3662.16, "text": " of the data which is coming in of the video of the falling apples for example that is simpler than", "tokens": [51172, 295, 264, 1412, 597, 307, 1348, 294, 295, 264, 960, 295, 264, 7440, 16814, 337, 1365, 300, 307, 18587, 813, 51588], "temperature": 0.0, "avg_logprob": -0.11067964881658554, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.0005701545742340386}, {"id": 482, "seek": 366216, "start": 3662.16, "end": 3668.48, "text": " the one that you had before so before you had the simple explanation of the data you needed", "tokens": [50364, 264, 472, 300, 291, 632, 949, 370, 949, 291, 632, 264, 2199, 10835, 295, 264, 1412, 291, 2978, 50680], "temperature": 0.0, "avg_logprob": -0.061431389820726615, "compression_ratio": 1.9153439153439153, "no_speech_prob": 0.0068972245790064335}, {"id": 483, "seek": 366216, "start": 3668.48, "end": 3675.92, "text": " so many bits so many bits to um to describe the data and afterwards only so many and the", "tokens": [50680, 370, 867, 9239, 370, 867, 9239, 281, 1105, 281, 6786, 264, 1412, 293, 10543, 787, 370, 867, 293, 264, 51052], "temperature": 0.0, "avg_logprob": -0.061431389820726615, "compression_ratio": 1.9153439153439153, "no_speech_prob": 0.0068972245790064335}, {"id": 484, "seek": 366216, "start": 3675.92, "end": 3682.72, "text": " difference between before and after that is the reward that you get so that's the true reward", "tokens": [51052, 2649, 1296, 949, 293, 934, 300, 307, 264, 7782, 300, 291, 483, 370, 300, 311, 264, 2074, 7782, 51392], "temperature": 0.0, "avg_logprob": -0.061431389820726615, "compression_ratio": 1.9153439153439153, "no_speech_prob": 0.0068972245790064335}, {"id": 485, "seek": 366216, "start": 3682.72, "end": 3689.04, "text": " that the controller the first neural network should get in response to the improvements", "tokens": [51392, 300, 264, 10561, 264, 700, 18161, 3209, 820, 483, 294, 4134, 281, 264, 13797, 51708], "temperature": 0.0, "avg_logprob": -0.061431389820726615, "compression_ratio": 1.9153439153439153, "no_speech_prob": 0.0068972245790064335}, {"id": 486, "seek": 368904, "start": 3690.0, "end": 3697.12, "text": " of the second network which are now measured in terms of compression progress so first I needed", "tokens": [50412, 295, 264, 1150, 3209, 597, 366, 586, 12690, 294, 2115, 295, 19355, 4205, 370, 700, 286, 2978, 50768], "temperature": 0.0, "avg_logprob": -0.08242778318474092, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.006386241875588894}, {"id": 487, "seek": 368904, "start": 3697.12, "end": 3703.52, "text": " so many resources to encode the data but then I discovered this regularity gravity and I can", "tokens": [50768, 370, 867, 3593, 281, 2058, 1429, 264, 1412, 457, 550, 286, 6941, 341, 3890, 507, 12110, 293, 286, 393, 51088], "temperature": 0.0, "avg_logprob": -0.08242778318474092, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.006386241875588894}, {"id": 488, "seek": 368904, "start": 3703.52, "end": 3709.7599999999998, "text": " greatly compress all kinds of videos that that are reflecting the concept of gravity and certainly", "tokens": [51088, 14147, 14778, 439, 3685, 295, 2145, 300, 300, 366, 23543, 264, 3410, 295, 12110, 293, 3297, 51400], "temperature": 0.0, "avg_logprob": -0.08242778318474092, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.006386241875588894}, {"id": 489, "seek": 368904, "start": 3709.7599999999998, "end": 3716.8, "text": " I'm have a huge insight into the nature of the world and that is my true joy scientific as a", "tokens": [51400, 286, 478, 362, 257, 2603, 11269, 666, 264, 3687, 295, 264, 1002, 293, 300, 307, 452, 2074, 6258, 8134, 382, 257, 51752], "temperature": 0.0, "avg_logprob": -0.08242778318474092, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.006386241875588894}, {"id": 490, "seek": 371680, "start": 3716.96, "end": 3723.76, "text": " scientist my my true joy as a scientist that I want to encode in a little number which is", "tokens": [50372, 12662, 452, 452, 2074, 6258, 382, 257, 12662, 300, 286, 528, 281, 2058, 1429, 294, 257, 707, 1230, 597, 307, 50712], "temperature": 0.0, "avg_logprob": -0.056320167504824124, "compression_ratio": 1.8359375, "no_speech_prob": 0.0011871918104588985}, {"id": 491, "seek": 371680, "start": 3723.76, "end": 3729.1200000000003, "text": " given as a reward to the guy who is inventing these experiments that lead to the data to the", "tokens": [50712, 2212, 382, 257, 7782, 281, 264, 2146, 567, 307, 7962, 278, 613, 12050, 300, 1477, 281, 264, 1412, 281, 264, 50980], "temperature": 0.0, "avg_logprob": -0.056320167504824124, "compression_ratio": 1.8359375, "no_speech_prob": 0.0011871918104588985}, {"id": 492, "seek": 371680, "start": 3729.1200000000003, "end": 3734.32, "text": " data with the falling apples for example right well and of course this is this has been a challenge", "tokens": [50980, 1412, 365, 264, 7440, 16814, 337, 1365, 558, 731, 293, 295, 1164, 341, 307, 341, 575, 668, 257, 3430, 51240], "temperature": 0.0, "avg_logprob": -0.056320167504824124, "compression_ratio": 1.8359375, "no_speech_prob": 0.0011871918104588985}, {"id": 493, "seek": 371680, "start": 3734.32, "end": 3739.36, "text": " in machine learning you know since the beginning which is okay as we add more and more parameters", "tokens": [51240, 294, 3479, 2539, 291, 458, 1670, 264, 2863, 597, 307, 1392, 382, 321, 909, 544, 293, 544, 9834, 51492], "temperature": 0.0, "avg_logprob": -0.056320167504824124, "compression_ratio": 1.8359375, "no_speech_prob": 0.0011871918104588985}, {"id": 494, "seek": 371680, "start": 3739.36, "end": 3743.92, "text": " how do we prevent it from learning spurious information with those parameters and instead", "tokens": [51492, 577, 360, 321, 4871, 309, 490, 2539, 637, 24274, 1589, 365, 729, 9834, 293, 2602, 51720], "temperature": 0.0, "avg_logprob": -0.056320167504824124, "compression_ratio": 1.8359375, "no_speech_prob": 0.0011871918104588985}, {"id": 495, "seek": 374392, "start": 3743.92, "end": 3749.92, "text": " have it focus on parsimonious explanations on regular explanations on things that in this", "tokens": [50364, 362, 309, 1879, 322, 21156, 25098, 851, 28708, 322, 3890, 28708, 322, 721, 300, 294, 341, 50664], "temperature": 0.0, "avg_logprob": -0.05173201893651208, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.00307376473210752}, {"id": 496, "seek": 374392, "start": 3749.92, "end": 3755.36, "text": " universe are more likely to generalize you know to unseen examples and so I think my question to", "tokens": [50664, 6445, 366, 544, 3700, 281, 2674, 1125, 291, 458, 281, 40608, 5110, 293, 370, 286, 519, 452, 1168, 281, 50936], "temperature": 0.0, "avg_logprob": -0.05173201893651208, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.00307376473210752}, {"id": 497, "seek": 374392, "start": 3755.36, "end": 3761.28, "text": " you is does this setup that you describe is it a form of that and or what is the state of the art", "tokens": [50936, 291, 307, 775, 341, 8657, 300, 291, 6786, 307, 309, 257, 1254, 295, 300, 293, 420, 437, 307, 264, 1785, 295, 264, 1523, 51232], "temperature": 0.0, "avg_logprob": -0.05173201893651208, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.00307376473210752}, {"id": 498, "seek": 374392, "start": 3761.28, "end": 3767.6, "text": " you know these days for helping to push or nudge neural networks towards learning parsimonious", "tokens": [51232, 291, 458, 613, 1708, 337, 4315, 281, 2944, 420, 297, 16032, 18161, 9590, 3030, 2539, 21156, 25098, 851, 51548], "temperature": 0.0, "avg_logprob": -0.05173201893651208, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.00307376473210752}, {"id": 499, "seek": 376760, "start": 3768.24, "end": 3774.16, "text": " models for the world rather than highly detailed spurious susceptible to you know", "tokens": [50396, 5245, 337, 264, 1002, 2831, 813, 5405, 9942, 637, 24274, 31249, 281, 291, 458, 50692], "temperature": 0.0, "avg_logprob": -0.08316807124925696, "compression_ratio": 1.6596858638743455, "no_speech_prob": 0.007934477180242538}, {"id": 500, "seek": 376760, "start": 3774.16, "end": 3778.72, "text": " high frequency anomalies and adversarial examples and all this sort of thing", "tokens": [50692, 1090, 7893, 24769, 48872, 293, 17641, 44745, 5110, 293, 439, 341, 1333, 295, 551, 50920], "temperature": 0.0, "avg_logprob": -0.08316807124925696, "compression_ratio": 1.6596858638743455, "no_speech_prob": 0.007934477180242538}, {"id": 501, "seek": 376760, "start": 3780.4, "end": 3784.64, "text": " yes what is the current state of the art in the regularizing", "tokens": [51004, 2086, 437, 307, 264, 2190, 1785, 295, 264, 1523, 294, 264, 3890, 3319, 51216], "temperature": 0.0, "avg_logprob": -0.08316807124925696, "compression_ratio": 1.6596858638743455, "no_speech_prob": 0.007934477180242538}, {"id": 502, "seek": 376760, "start": 3786.3199999999997, "end": 3792.88, "text": " descriptors of the data such as neural networks such that you get simple explanations of the data", "tokens": [51300, 31280, 830, 295, 264, 1412, 1270, 382, 18161, 9590, 1270, 300, 291, 483, 2199, 28708, 295, 264, 1412, 51628], "temperature": 0.0, "avg_logprob": -0.08316807124925696, "compression_ratio": 1.6596858638743455, "no_speech_prob": 0.007934477180242538}, {"id": 503, "seek": 379288, "start": 3793.76, "end": 3800.56, "text": " such that you get short programs that compute the data in other words such that the description of", "tokens": [50408, 1270, 300, 291, 483, 2099, 4268, 300, 14722, 264, 1412, 294, 661, 2283, 1270, 300, 264, 3855, 295, 50748], "temperature": 0.0, "avg_logprob": -0.106086793493052, "compression_ratio": 1.78343949044586, "no_speech_prob": 0.0036453583743423223}, {"id": 504, "seek": 379288, "start": 3800.56, "end": 3812.1600000000003, "text": " the data is a short program that computes the much larger raw data and and how close can we", "tokens": [50748, 264, 1412, 307, 257, 2099, 1461, 300, 715, 1819, 264, 709, 4833, 8936, 1412, 293, 293, 577, 1998, 393, 321, 51328], "temperature": 0.0, "avg_logprob": -0.106086793493052, "compression_ratio": 1.78343949044586, "no_speech_prob": 0.0036453583743423223}, {"id": 505, "seek": 379288, "start": 3812.7200000000003, "end": 3818.6400000000003, "text": " get to the limits which are given through this concept concept of algorithmic information", "tokens": [51356, 483, 281, 264, 10406, 597, 366, 2212, 807, 341, 3410, 3410, 295, 9284, 299, 1589, 51652], "temperature": 0.0, "avg_logprob": -0.106086793493052, "compression_ratio": 1.78343949044586, "no_speech_prob": 0.0036453583743423223}, {"id": 506, "seek": 381864, "start": 3818.64, "end": 3825.04, "text": " or comagor complexity comagor complexity of any data is the length of the shortest program", "tokens": [50364, 420, 395, 559, 284, 14024, 395, 559, 284, 14024, 295, 604, 1412, 307, 264, 4641, 295, 264, 31875, 1461, 50684], "temperature": 0.0, "avg_logprob": -0.12430276309742647, "compression_ratio": 2.066666666666667, "no_speech_prob": 0.0005192100652493536}, {"id": 507, "seek": 381864, "start": 3825.7599999999998, "end": 3831.44, "text": " on some general computer that computes it since in our field the general computers are", "tokens": [50720, 322, 512, 2674, 3820, 300, 715, 1819, 309, 1670, 294, 527, 2519, 264, 2674, 10807, 366, 51004], "temperature": 0.0, "avg_logprob": -0.12430276309742647, "compression_ratio": 2.066666666666667, "no_speech_prob": 0.0005192100652493536}, {"id": 508, "seek": 381864, "start": 3831.44, "end": 3838.4, "text": " recurrent neural networks we want to find a simple recurrent network that computes all this data", "tokens": [51004, 18680, 1753, 18161, 9590, 321, 528, 281, 915, 257, 2199, 18680, 1753, 3209, 300, 715, 1819, 439, 341, 1412, 51352], "temperature": 0.0, "avg_logprob": -0.12430276309742647, "compression_ratio": 2.066666666666667, "no_speech_prob": 0.0005192100652493536}, {"id": 509, "seek": 381864, "start": 3841.68, "end": 3847.3599999999997, "text": " and given one computation of the data we want to find an even simpler one so we want to have this", "tokens": [51516, 293, 2212, 472, 24903, 295, 264, 1412, 321, 528, 281, 915, 364, 754, 18587, 472, 370, 321, 528, 281, 362, 341, 51800], "temperature": 0.0, "avg_logprob": -0.12430276309742647, "compression_ratio": 2.066666666666667, "no_speech_prob": 0.0005192100652493536}, {"id": 510, "seek": 384736, "start": 3847.36, "end": 3855.28, "text": " idea of compression progress and here I have to say although we have lots of regularizers", "tokens": [50364, 1558, 295, 19355, 4205, 293, 510, 286, 362, 281, 584, 4878, 321, 362, 3195, 295, 3890, 22525, 50760], "temperature": 0.0, "avg_logprob": -0.09196215345148455, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.005722253583371639}, {"id": 511, "seek": 384736, "start": 3855.28, "end": 3862.08, "text": " invented throughout the past few decades there's nothing that is really convincing", "tokens": [50760, 14479, 3710, 264, 1791, 1326, 7878, 456, 311, 1825, 300, 307, 534, 24823, 51100], "temperature": 0.0, "avg_logprob": -0.09196215345148455, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.005722253583371639}, {"id": 512, "seek": 384736, "start": 3863.04, "end": 3872.08, "text": " I think one of the very important missing things is to make that work in a way that is", "tokens": [51148, 286, 519, 472, 295, 264, 588, 1021, 5361, 721, 307, 281, 652, 300, 589, 294, 257, 636, 300, 307, 51600], "temperature": 0.0, "avg_logprob": -0.09196215345148455, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.005722253583371639}, {"id": 513, "seek": 387208, "start": 3872.08, "end": 3879.2, "text": " truly convincing that is as convincing as chat gpt is today in the much more limited domain of", "tokens": [50364, 4908, 24823, 300, 307, 382, 24823, 382, 5081, 290, 662, 307, 965, 294, 264, 709, 544, 5567, 9274, 295, 50720], "temperature": 0.0, "avg_logprob": -0.09914676726810516, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.005816436372697353}, {"id": 514, "seek": 387208, "start": 3880.4, "end": 3888.96, "text": " generating text from previously observed texts and stuff a very old idea of I think the 1980s", "tokens": [50780, 17746, 2487, 490, 8046, 13095, 15765, 293, 1507, 257, 588, 1331, 1558, 295, 286, 519, 264, 13626, 82, 51208], "temperature": 0.0, "avg_logprob": -0.09914676726810516, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.005816436372697353}, {"id": 515, "seek": 387208, "start": 3889.92, "end": 3897.7599999999998, "text": " was to have weight decay in a neural network which basically is the idea that all the weights", "tokens": [51256, 390, 281, 362, 3364, 21039, 294, 257, 18161, 3209, 597, 1936, 307, 264, 1558, 300, 439, 264, 17443, 51648], "temperature": 0.0, "avg_logprob": -0.09914676726810516, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.005816436372697353}, {"id": 516, "seek": 389776, "start": 3897.76, "end": 3905.6000000000004, "text": " should have an incentive to become close to zero such that you can prune them", "tokens": [50364, 820, 362, 364, 22346, 281, 1813, 1998, 281, 4018, 1270, 300, 291, 393, 582, 2613, 552, 50756], "temperature": 0.0, "avg_logprob": -0.09591091090235217, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.0022133849561214447}, {"id": 517, "seek": 389776, "start": 3906.6400000000003, "end": 3914.5600000000004, "text": " and so people built in regularizer that just punished weights for being large or being very", "tokens": [50808, 293, 370, 561, 3094, 294, 3890, 6545, 300, 445, 22365, 17443, 337, 885, 2416, 420, 885, 588, 51204], "temperature": 0.0, "avg_logprob": -0.09591091090235217, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.0022133849561214447}, {"id": 518, "seek": 389776, "start": 3914.5600000000004, "end": 3922.2400000000002, "text": " negative but that didn't work really well and something better was flat minimum search that was", "tokens": [51204, 3671, 457, 300, 994, 380, 589, 534, 731, 293, 746, 1101, 390, 4962, 7285, 3164, 300, 390, 51588], "temperature": 0.0, "avg_logprob": -0.09591091090235217, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.0022133849561214447}, {"id": 519, "seek": 392224, "start": 3922.24, "end": 3929.8399999999997, "text": " 1998 and first Arthur my brilliant student set book write that back then roughly the same time", "tokens": [50364, 21404, 293, 700, 19624, 452, 10248, 3107, 992, 1446, 2464, 300, 646, 550, 9810, 264, 912, 565, 50744], "temperature": 0.0, "avg_logprob": -0.18455244789660816, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.014447744935750961}, {"id": 520, "seek": 392224, "start": 3929.8399999999997, "end": 3940.8799999999997, "text": " when the LSTM paper came out and and there the idea is if you have if you plot the weights of", "tokens": [50744, 562, 264, 441, 6840, 44, 3035, 1361, 484, 293, 293, 456, 264, 1558, 307, 498, 291, 362, 498, 291, 7542, 264, 17443, 295, 51296], "temperature": 0.0, "avg_logprob": -0.18455244789660816, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.014447744935750961}, {"id": 521, "seek": 392224, "start": 3940.8799999999997, "end": 3951.68, "text": " a neural network on the x-axis and you plot the error on the y-axis then given the weights you", "tokens": [51296, 257, 18161, 3209, 322, 264, 2031, 12, 24633, 293, 291, 7542, 264, 6713, 322, 264, 288, 12, 24633, 550, 2212, 264, 17443, 291, 51836], "temperature": 0.0, "avg_logprob": -0.18455244789660816, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.014447744935750961}, {"id": 522, "seek": 395168, "start": 3951.68, "end": 3960.48, "text": " have high or low error and then there is for example a sharp error function which has a sharp", "tokens": [50364, 362, 1090, 420, 2295, 6713, 293, 550, 456, 307, 337, 1365, 257, 8199, 6713, 2445, 597, 575, 257, 8199, 50804], "temperature": 0.0, "avg_logprob": -0.05656710223875184, "compression_ratio": 1.7643312101910829, "no_speech_prob": 0.0023214342072606087}, {"id": 523, "seek": 395168, "start": 3960.48, "end": 3968.0, "text": " minimum which which goes like that can you see my finger so here here is the x-axis here's the", "tokens": [50804, 7285, 597, 597, 1709, 411, 300, 393, 291, 536, 452, 5984, 370, 510, 510, 307, 264, 2031, 12, 24633, 510, 311, 264, 51180], "temperature": 0.0, "avg_logprob": -0.05656710223875184, "compression_ratio": 1.7643312101910829, "no_speech_prob": 0.0023214342072606087}, {"id": 524, "seek": 395168, "start": 3968.0, "end": 3974.48, "text": " y-axis here's the error and the error for a certain weight is really really low but then", "tokens": [51180, 288, 12, 24633, 510, 311, 264, 6713, 293, 264, 6713, 337, 257, 1629, 3364, 307, 534, 534, 2295, 457, 550, 51504], "temperature": 0.0, "avg_logprob": -0.05656710223875184, "compression_ratio": 1.7643312101910829, "no_speech_prob": 0.0023214342072606087}, {"id": 525, "seek": 397448, "start": 3974.48, "end": 3981.92, "text": " for a different weight in the environment in the vicinity it's high again which would be very", "tokens": [50364, 337, 257, 819, 3364, 294, 264, 2823, 294, 264, 42387, 309, 311, 1090, 797, 597, 576, 312, 588, 50736], "temperature": 0.0, "avg_logprob": -0.0844323968374601, "compression_ratio": 2.0, "no_speech_prob": 0.00781061640009284}, {"id": 526, "seek": 397448, "start": 3981.92, "end": 3988.8, "text": " different from a flat minimum which would be like this so here's the error and it's going down", "tokens": [50736, 819, 490, 257, 4962, 7285, 597, 576, 312, 411, 341, 370, 510, 311, 264, 6713, 293, 309, 311, 516, 760, 51080], "temperature": 0.0, "avg_logprob": -0.0844323968374601, "compression_ratio": 2.0, "no_speech_prob": 0.00781061640009284}, {"id": 527, "seek": 397448, "start": 3988.8, "end": 3995.28, "text": " and for many many ways it is low the error and then it goes up again so if you are a very sharp", "tokens": [51080, 293, 337, 867, 867, 2098, 309, 307, 2295, 264, 6713, 293, 550, 309, 1709, 493, 797, 370, 498, 291, 366, 257, 588, 8199, 51404], "temperature": 0.0, "avg_logprob": -0.0844323968374601, "compression_ratio": 2.0, "no_speech_prob": 0.00781061640009284}, {"id": 528, "seek": 397448, "start": 3995.28, "end": 4002.32, "text": " well versus a very broad well yes a sharp well versus a broad well now if you are in a sharp well", "tokens": [51404, 731, 5717, 257, 588, 4152, 731, 2086, 257, 8199, 731, 5717, 257, 4152, 731, 586, 498, 291, 366, 294, 257, 8199, 731, 51756], "temperature": 0.0, "avg_logprob": -0.0844323968374601, "compression_ratio": 2.0, "no_speech_prob": 0.00781061640009284}, {"id": 529, "seek": 400232, "start": 4002.32, "end": 4008.7200000000003, "text": " you have to specify the weights with a lot with with high precision so you have to spend many bits", "tokens": [50364, 291, 362, 281, 16500, 264, 17443, 365, 257, 688, 365, 365, 1090, 18356, 370, 291, 362, 281, 3496, 867, 9239, 50684], "temperature": 0.0, "avg_logprob": -0.06368818283081054, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.011663776822388172}, {"id": 530, "seek": 400232, "start": 4008.7200000000003, "end": 4015.6000000000004, "text": " of information on encoding the weights of this network as opposed to a large to a flat minimum", "tokens": [50684, 295, 1589, 322, 43430, 264, 17443, 295, 341, 3209, 382, 8851, 281, 257, 2416, 281, 257, 4962, 7285, 51028], "temperature": 0.0, "avg_logprob": -0.06368818283081054, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.011663776822388172}, {"id": 531, "seek": 400232, "start": 4015.6000000000004, "end": 4023.36, "text": " where it doesn't matter if you you know perturb the weights because the error remains low", "tokens": [51028, 689, 309, 1177, 380, 1871, 498, 291, 291, 458, 40468, 264, 17443, 570, 264, 6713, 7023, 2295, 51416], "temperature": 0.0, "avg_logprob": -0.06368818283081054, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.011663776822388172}, {"id": 532, "seek": 400232, "start": 4024.2400000000002, "end": 4031.04, "text": " in this flat minimum so what you really want to find is is a network that has low complexity in", "tokens": [51460, 294, 341, 4962, 7285, 370, 437, 291, 534, 528, 281, 915, 307, 307, 257, 3209, 300, 575, 2295, 14024, 294, 51800], "temperature": 0.0, "avg_logprob": -0.06368818283081054, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.011663776822388172}, {"id": 533, "seek": 403104, "start": 4031.04, "end": 4037.04, "text": " the sense that you can describe the good network so those with low error with very few bits of", "tokens": [50364, 264, 2020, 300, 291, 393, 6786, 264, 665, 3209, 370, 729, 365, 2295, 6713, 365, 588, 1326, 9239, 295, 50664], "temperature": 0.0, "avg_logprob": -0.09141085887777395, "compression_ratio": 1.680722891566265, "no_speech_prob": 0.0010985858971253037}, {"id": 534, "seek": 403104, "start": 4037.04, "end": 4045.2799999999997, "text": " information and suddenly if you maximize or if you minimize that flat minimum second order", "tokens": [50664, 1589, 293, 5800, 498, 291, 19874, 420, 498, 291, 17522, 300, 4962, 7285, 1150, 1668, 51076], "temperature": 0.0, "avg_logprob": -0.09141085887777395, "compression_ratio": 1.680722891566265, "no_speech_prob": 0.0010985858971253037}, {"id": 535, "seek": 403104, "start": 4045.2799999999997, "end": 4055.44, "text": " error function then suddenly you have a preference for networks that that for example do this", "tokens": [51076, 6713, 2445, 550, 5800, 291, 362, 257, 17502, 337, 9590, 300, 300, 337, 1365, 360, 341, 51584], "temperature": 0.0, "avg_logprob": -0.09141085887777395, "compression_ratio": 1.680722891566265, "no_speech_prob": 0.0010985858971253037}, {"id": 536, "seek": 405544, "start": 4055.76, "end": 4063.92, "text": " you you have a hidden unit and the outgoing weights they have certain values but if you", "tokens": [50380, 291, 291, 362, 257, 7633, 4985, 293, 264, 41565, 17443, 436, 362, 1629, 4190, 457, 498, 291, 50788], "temperature": 0.0, "avg_logprob": -0.078033263866718, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.005635950714349747}, {"id": 537, "seek": 405544, "start": 4063.92, "end": 4068.64, "text": " give a very negative weight to the hidden unit then it doesn't matter what all these outgoing", "tokens": [50788, 976, 257, 588, 3671, 3364, 281, 264, 7633, 4985, 550, 309, 1177, 380, 1871, 437, 439, 613, 41565, 51024], "temperature": 0.0, "avg_logprob": -0.078033263866718, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.005635950714349747}, {"id": 538, "seek": 405544, "start": 4068.64, "end": 4077.76, "text": " weights do and flat minimum minimum search likes to find weight matrices like that where one single", "tokens": [51024, 17443, 360, 293, 4962, 7285, 7285, 3164, 5902, 281, 915, 3364, 32284, 411, 300, 689, 472, 2167, 51480], "temperature": 0.0, "avg_logprob": -0.078033263866718, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.005635950714349747}, {"id": 539, "seek": 405544, "start": 4077.76, "end": 4082.8, "text": " weight can eliminate many others which you suddenly don't need any longer such that the", "tokens": [51480, 3364, 393, 13819, 867, 2357, 597, 291, 5800, 500, 380, 643, 604, 2854, 1270, 300, 264, 51732], "temperature": 0.0, "avg_logprob": -0.078033263866718, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.005635950714349747}, {"id": 540, "seek": 408280, "start": 4083.6000000000004, "end": 4090.8, "text": " description complexity of the whole thing is much lower than in the beginning when you when you", "tokens": [50404, 3855, 14024, 295, 264, 1379, 551, 307, 709, 3126, 813, 294, 264, 2863, 562, 291, 562, 291, 50764], "temperature": 0.0, "avg_logprob": -0.05626807978123794, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0005441142711788416}, {"id": 541, "seek": 408280, "start": 4090.8, "end": 4097.84, "text": " just had a random initialization of all these weights so that is much more general than weight", "tokens": [50764, 445, 632, 257, 4974, 5883, 2144, 295, 439, 613, 17443, 370, 300, 307, 709, 544, 2674, 813, 3364, 51116], "temperature": 0.0, "avg_logprob": -0.05626807978123794, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0005441142711788416}, {"id": 542, "seek": 408280, "start": 4097.84, "end": 4102.72, "text": " decay because weight decay doesn't like these strong weights it wants to remove them but sometimes", "tokens": [51116, 21039, 570, 3364, 21039, 1177, 380, 411, 613, 2068, 17443, 309, 2738, 281, 4159, 552, 457, 2171, 51360], "temperature": 0.0, "avg_logprob": -0.05626807978123794, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0005441142711788416}, {"id": 543, "seek": 408280, "start": 4102.72, "end": 4107.76, "text": " it's really good to have a very negative weight coming to a hidden unit which is switched off", "tokens": [51360, 309, 311, 534, 665, 281, 362, 257, 588, 3671, 3364, 1348, 281, 257, 7633, 4985, 597, 307, 16858, 766, 51612], "temperature": 0.0, "avg_logprob": -0.05626807978123794, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.0005441142711788416}, {"id": 544, "seek": 410776, "start": 4107.76, "end": 4116.08, "text": " through that weight such that all the outgoing connections are meaningless but it's not um what", "tokens": [50364, 807, 300, 3364, 1270, 300, 439, 264, 41565, 9271, 366, 33232, 457, 309, 311, 406, 1105, 437, 50780], "temperature": 0.0, "avg_logprob": -0.10918686001799828, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.04141070693731308}, {"id": 545, "seek": 410776, "start": 4116.08, "end": 4124.0, "text": " you what you it's very nice it's a very nice principle but it's not as general as finding the", "tokens": [50780, 291, 437, 291, 309, 311, 588, 1481, 309, 311, 257, 588, 1481, 8665, 457, 309, 311, 406, 382, 2674, 382, 5006, 264, 51176], "temperature": 0.0, "avg_logprob": -0.10918686001799828, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.04141070693731308}, {"id": 546, "seek": 410776, "start": 4124.0, "end": 4130.400000000001, "text": " shortest program on a university computer that computes the weight matrix that is solving your", "tokens": [51176, 31875, 1461, 322, 257, 5454, 3820, 300, 715, 1819, 264, 3364, 8141, 300, 307, 12606, 428, 51496], "temperature": 0.0, "avg_logprob": -0.10918686001799828, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.04141070693731308}, {"id": 547, "seek": 410776, "start": 4130.400000000001, "end": 4136.16, "text": " problem to the extent how do you think we're how do you think we're gonna get to that point", "tokens": [51496, 1154, 281, 264, 8396, 577, 360, 291, 519, 321, 434, 577, 360, 291, 519, 321, 434, 799, 483, 281, 300, 935, 51784], "temperature": 0.0, "avg_logprob": -0.10918686001799828, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.04141070693731308}, {"id": 548, "seek": 413616, "start": 4136.16, "end": 4140.96, "text": " how do you think uh what approaches are going to lead us to finding things that approach", "tokens": [50364, 577, 360, 291, 519, 2232, 437, 11587, 366, 516, 281, 1477, 505, 281, 5006, 721, 300, 3109, 50604], "temperature": 0.0, "avg_logprob": -0.20193990560678335, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006562620983459055}, {"id": 549, "seek": 413616, "start": 4140.96, "end": 4147.599999999999, "text": " comical of complexity yeah and i think that path has again a lot to do with meta learning and as", "tokens": [50604, 395, 804, 295, 14024, 1338, 293, 741, 519, 300, 3100, 575, 797, 257, 688, 281, 360, 365, 19616, 2539, 293, 382, 50936], "temperature": 0.0, "avg_logprob": -0.20193990560678335, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006562620983459055}, {"id": 550, "seek": 413616, "start": 4147.599999999999, "end": 4155.5199999999995, "text": " um a system is able to run its own learning algorithm on the network itself it can um suddenly", "tokens": [50936, 1105, 257, 1185, 307, 1075, 281, 1190, 1080, 1065, 2539, 9284, 322, 264, 3209, 2564, 309, 393, 1105, 5800, 51332], "temperature": 0.0, "avg_logprob": -0.20193990560678335, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006562620983459055}, {"id": 551, "seek": 415552, "start": 4155.6, "end": 4157.68, "text": " speak about the um", "tokens": [50368, 1710, 466, 264, 1105, 50472], "temperature": 0.0, "avg_logprob": -0.1271870239921238, "compression_ratio": 1.5338345864661653, "no_speech_prob": 0.014261340722441673}, {"id": 552, "seek": 415552, "start": 4160.080000000001, "end": 4168.320000000001, "text": " algorithms in form of weight matrices and it can discuss concepts such as the complexity of a", "tokens": [50592, 14642, 294, 1254, 295, 3364, 32284, 293, 309, 393, 2248, 10392, 1270, 382, 264, 14024, 295, 257, 51004], "temperature": 0.0, "avg_logprob": -0.1271870239921238, "compression_ratio": 1.5338345864661653, "no_speech_prob": 0.014261340722441673}, {"id": 553, "seek": 415552, "start": 4168.320000000001, "end": 4178.0, "text": " weight matrix and then you can conduct a search um in this space of networks that generates", "tokens": [51004, 3364, 8141, 293, 550, 291, 393, 6018, 257, 3164, 1105, 294, 341, 1901, 295, 9590, 300, 23815, 51488], "temperature": 0.0, "avg_logprob": -0.1271870239921238, "compression_ratio": 1.5338345864661653, "no_speech_prob": 0.014261340722441673}, {"id": 554, "seek": 417800, "start": 4178.72, "end": 4187.36, "text": " weight matrices and then you suddenly are in the game so suddenly you are playing the right game", "tokens": [50400, 3364, 32284, 293, 550, 291, 5800, 366, 294, 264, 1216, 370, 5800, 291, 366, 2433, 264, 558, 1216, 50832], "temperature": 0.0, "avg_logprob": -0.07988482316335042, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.007933461107313633}, {"id": 555, "seek": 417800, "start": 4187.36, "end": 4195.44, "text": " and then it's more a question of how to um choose an initial learning algorithm such as", "tokens": [50832, 293, 550, 309, 311, 544, 257, 1168, 295, 577, 281, 1105, 2826, 364, 5883, 2539, 9284, 1270, 382, 51236], "temperature": 0.0, "avg_logprob": -0.07988482316335042, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.007933461107313633}, {"id": 556, "seek": 417800, "start": 4195.44, "end": 4201.12, "text": " gradient descent to come up with something that computes the simple solutions which you really", "tokens": [51236, 16235, 23475, 281, 808, 493, 365, 746, 300, 715, 1819, 264, 2199, 6547, 597, 291, 534, 51520], "temperature": 0.0, "avg_logprob": -0.07988482316335042, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.007933461107313633}, {"id": 557, "seek": 420112, "start": 4201.12, "end": 4211.5199999999995, "text": " want to see in the end very recent papers on that on on aspects of that came out just a while ago", "tokens": [50364, 528, 281, 536, 294, 264, 917, 588, 5162, 10577, 322, 300, 322, 322, 7270, 295, 300, 1361, 484, 445, 257, 1339, 2057, 50884], "temperature": 0.0, "avg_logprob": -0.22531102738290462, "compression_ratio": 1.4885496183206106, "no_speech_prob": 0.028690602630376816}, {"id": 558, "seek": 420112, "start": 4211.5199999999995, "end": 4224.16, "text": " with my students vincent herman and louise kirch and um and francesco faccio and my poster kazuki", "tokens": [50884, 365, 452, 1731, 27037, 2207, 720, 1601, 293, 15185, 908, 33497, 339, 293, 1105, 293, 431, 2676, 1291, 1915, 8529, 293, 452, 17171, 30623, 11788, 51516], "temperature": 0.0, "avg_logprob": -0.22531102738290462, "compression_ratio": 1.4885496183206106, "no_speech_prob": 0.028690602630376816}, {"id": 559, "seek": 422416, "start": 4224.72, "end": 4231.36, "text": " and robert joydash also um and also imann olschlag and there the idea is really to", "tokens": [50392, 293, 744, 4290, 6258, 67, 1299, 611, 1105, 293, 611, 566, 969, 2545, 6145, 27298, 293, 456, 264, 1558, 307, 534, 281, 50724], "temperature": 0.0, "avg_logprob": -0.18711115419864655, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.02060719020664692}, {"id": 560, "seek": 422416, "start": 4232.08, "end": 4239.76, "text": " have one network that computes an experiment and the experiment itself is the weight matrix", "tokens": [50760, 362, 472, 3209, 300, 715, 1819, 364, 5120, 293, 264, 5120, 2564, 307, 264, 3364, 8141, 51144], "temperature": 0.0, "avg_logprob": -0.18711115419864655, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.02060719020664692}, {"id": 561, "seek": 422416, "start": 4239.76, "end": 4248.0, "text": " of a recurrent network so there is a generator of an experiment which can be anything that", "tokens": [51144, 295, 257, 18680, 1753, 3209, 370, 456, 307, 257, 19265, 295, 364, 5120, 597, 393, 312, 1340, 300, 51556], "temperature": 0.0, "avg_logprob": -0.18711115419864655, "compression_ratio": 1.6987179487179487, "no_speech_prob": 0.02060719020664692}, {"id": 562, "seek": 424800, "start": 4248.08, "end": 4257.6, "text": " describes a computational interaction with an environment so a program so that experiment", "tokens": [50368, 15626, 257, 28270, 9285, 365, 364, 2823, 370, 257, 1461, 370, 300, 5120, 50844], "temperature": 0.0, "avg_logprob": -0.0844015253001246, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.0020774707663804293}, {"id": 563, "seek": 424800, "start": 4257.6, "end": 4263.36, "text": " is then executed in the real world there's a prediction machine that predicts the outcome", "tokens": [50844, 307, 550, 17577, 294, 264, 957, 1002, 456, 311, 257, 17630, 3479, 300, 6069, 82, 264, 9700, 51132], "temperature": 0.0, "avg_logprob": -0.0844015253001246, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.0020774707663804293}, {"id": 564, "seek": 424800, "start": 4263.36, "end": 4273.44, "text": " of the experiment before the algorithm is executed and so then there's um just a yes or no question", "tokens": [51132, 295, 264, 5120, 949, 264, 9284, 307, 17577, 293, 370, 550, 456, 311, 1105, 445, 257, 2086, 420, 572, 1168, 51636], "temperature": 0.0, "avg_logprob": -0.0844015253001246, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.0020774707663804293}, {"id": 565, "seek": 427344, "start": 4273.44, "end": 4284.799999999999, "text": " either the following outcome will occur or not either it will occur or not but now the entire", "tokens": [50364, 2139, 264, 3480, 9700, 486, 5160, 420, 406, 2139, 309, 486, 5160, 420, 406, 457, 586, 264, 2302, 50932], "temperature": 0.0, "avg_logprob": -0.057010694481860635, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0017529188189655542}, {"id": 566, "seek": 427344, "start": 4284.799999999999, "end": 4290.16, "text": " setup is such that you don't have predictions all the time about every single pixel no you just have", "tokens": [50932, 8657, 307, 1270, 300, 291, 500, 380, 362, 21264, 439, 264, 565, 466, 633, 2167, 19261, 572, 291, 445, 362, 51200], "temperature": 0.0, "avg_logprob": -0.057010694481860635, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0017529188189655542}, {"id": 567, "seek": 427344, "start": 4290.16, "end": 4295.44, "text": " something which is very abstract and which is just about whether a certain unit of the recurrent", "tokens": [51200, 746, 597, 307, 588, 12649, 293, 597, 307, 445, 466, 1968, 257, 1629, 4985, 295, 264, 18680, 1753, 51464], "temperature": 0.0, "avg_logprob": -0.057010694481860635, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0017529188189655542}, {"id": 568, "seek": 427344, "start": 4295.44, "end": 4302.48, "text": " network is going to be on or off at the end of the experiment and this internal on and off unit", "tokens": [51464, 3209, 307, 516, 281, 312, 322, 420, 766, 412, 264, 917, 295, 264, 5120, 293, 341, 6920, 322, 293, 766, 4985, 51816], "temperature": 0.0, "avg_logprob": -0.057010694481860635, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0017529188189655542}, {"id": 569, "seek": 430248, "start": 4302.5599999999995, "end": 4311.12, "text": " can represent any computational question any questions that you can ask at all and now the", "tokens": [50368, 393, 2906, 604, 28270, 1168, 604, 1651, 300, 291, 393, 1029, 412, 439, 293, 586, 264, 50796], "temperature": 0.0, "avg_logprob": -0.07458553049299452, "compression_ratio": 1.9526315789473685, "no_speech_prob": 0.0035349931567907333}, {"id": 570, "seek": 430248, "start": 4311.12, "end": 4316.5599999999995, "text": " the task of the experiment generator which is another network which generates a recurrent", "tokens": [50796, 264, 5633, 295, 264, 5120, 19265, 597, 307, 1071, 3209, 597, 23815, 257, 18680, 1753, 51068], "temperature": 0.0, "avg_logprob": -0.07458553049299452, "compression_ratio": 1.9526315789473685, "no_speech_prob": 0.0035349931567907333}, {"id": 571, "seek": 430248, "start": 4317.599999999999, "end": 4323.599999999999, "text": " network weight matrix which represents the experiment the task of this experiment generator", "tokens": [51120, 3209, 3364, 8141, 597, 8855, 264, 5120, 264, 5633, 295, 341, 5120, 19265, 51420], "temperature": 0.0, "avg_logprob": -0.07458553049299452, "compression_ratio": 1.9526315789473685, "no_speech_prob": 0.0035349931567907333}, {"id": 572, "seek": 430248, "start": 4323.599999999999, "end": 4330.48, "text": " is to again come up with something that surprises the um the prediction machine which looks at the", "tokens": [51420, 307, 281, 797, 808, 493, 365, 746, 300, 22655, 264, 1105, 264, 17630, 3479, 597, 1542, 412, 264, 51764], "temperature": 0.0, "avg_logprob": -0.07458553049299452, "compression_ratio": 1.9526315789473685, "no_speech_prob": 0.0035349931567907333}, {"id": 573, "seek": 433048, "start": 4330.48, "end": 4337.839999999999, "text": " experiment and says yeah it's going to work or not and uh and suddenly you are again in this", "tokens": [50364, 5120, 293, 1619, 1338, 309, 311, 516, 281, 589, 420, 406, 293, 2232, 293, 5800, 291, 366, 797, 294, 341, 50732], "temperature": 0.0, "avg_logprob": -0.06238746068563806, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.001405777526088059}, {"id": 574, "seek": 433048, "start": 4337.839999999999, "end": 4346.4, "text": " old game uh except that now you have this world of abstractions where the abstractions can be", "tokens": [50732, 1331, 1216, 2232, 3993, 300, 586, 291, 362, 341, 1002, 295, 12649, 626, 689, 264, 12649, 626, 393, 312, 51160], "temperature": 0.0, "avg_logprob": -0.06238746068563806, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.001405777526088059}, {"id": 575, "seek": 433048, "start": 4346.4, "end": 4354.0, "text": " anything that is computable interesting really cool really cool could we spend the last 10 minutes", "tokens": [51160, 1340, 300, 307, 2807, 712, 1880, 534, 1627, 534, 1627, 727, 321, 3496, 264, 1036, 1266, 2077, 51540], "temperature": 0.0, "avg_logprob": -0.06238746068563806, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.001405777526088059}, {"id": 576, "seek": 433048, "start": 4354.0, "end": 4360.32, "text": " or so just talking about some of the the current ai landscape so in particular the capabilities of", "tokens": [51540, 420, 370, 445, 1417, 466, 512, 295, 264, 264, 2190, 9783, 9661, 370, 294, 1729, 264, 10862, 295, 51856], "temperature": 0.0, "avg_logprob": -0.06238746068563806, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.001405777526088059}, {"id": 577, "seek": 436048, "start": 4360.48, "end": 4368.799999999999, "text": " GPT-4 and the moat building thing and and the the power that companies like uh google and open ai", "tokens": [50364, 26039, 51, 12, 19, 293, 264, 705, 267, 2390, 551, 293, 293, 264, 264, 1347, 300, 3431, 411, 2232, 20742, 293, 1269, 9783, 50780], "temperature": 0.0, "avg_logprob": -0.11781501770019531, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.004298740066587925}, {"id": 578, "seek": 436048, "start": 4368.799999999999, "end": 4375.839999999999, "text": " have and um also the potential for open source so maybe we'll just start with the you know the", "tokens": [50780, 362, 293, 1105, 611, 264, 3995, 337, 1269, 4009, 370, 1310, 321, 603, 445, 722, 365, 264, 291, 458, 264, 51132], "temperature": 0.0, "avg_logprob": -0.11781501770019531, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.004298740066587925}, {"id": 579, "seek": 436048, "start": 4375.839999999999, "end": 4382.08, "text": " very current capabilities of GPT-4 are you impressed with it what do you think i'm impressed in the", "tokens": [51132, 588, 2190, 10862, 295, 26039, 51, 12, 19, 366, 291, 11679, 365, 309, 437, 360, 291, 519, 741, 478, 11679, 294, 264, 51444], "temperature": 0.0, "avg_logprob": -0.11781501770019531, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.004298740066587925}, {"id": 580, "seek": 438208, "start": 4382.08, "end": 4390.96, "text": " sense that um i like the outcomes that you get there and um it wasn't obvious a couple of years", "tokens": [50364, 2020, 300, 1105, 741, 411, 264, 10070, 300, 291, 483, 456, 293, 1105, 309, 2067, 380, 6322, 257, 1916, 295, 924, 50808], "temperature": 0.0, "avg_logprob": -0.09931034437367614, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.014035047963261604}, {"id": 581, "seek": 438208, "start": 4390.96, "end": 4398.64, "text": " ago that it would become so good uh on the other hand of course and it's not yet this full AGI thing", "tokens": [50808, 2057, 300, 309, 576, 1813, 370, 665, 2232, 322, 264, 661, 1011, 295, 1164, 293, 309, 311, 406, 1939, 341, 1577, 316, 26252, 551, 51192], "temperature": 0.0, "avg_logprob": -0.09931034437367614, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.014035047963261604}, {"id": 582, "seek": 438208, "start": 4399.44, "end": 4411.44, "text": " and it is not really close to um to justifying those fears that some uh researchers sometimes", "tokens": [51232, 293, 309, 307, 406, 534, 1998, 281, 1105, 281, 445, 5489, 729, 15649, 300, 512, 2232, 10309, 2171, 51832], "temperature": 0.0, "avg_logprob": -0.09931034437367614, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.014035047963261604}, {"id": 583, "seek": 441144, "start": 4411.44, "end": 4426.32, "text": " and now um document and um in letters and public letters and so on so to me it's a little bit", "tokens": [50364, 293, 586, 1105, 4166, 293, 1105, 294, 7825, 293, 1908, 7825, 293, 370, 322, 370, 281, 385, 309, 311, 257, 707, 857, 51108], "temperature": 0.0, "avg_logprob": -0.1993642969334379, "compression_ratio": 1.528, "no_speech_prob": 0.0003791695344261825}, {"id": 584, "seek": 441144, "start": 4430.0, "end": 4438.0, "text": " like a visa view because for for many decades i have um had discussions like that and people said", "tokens": [51292, 411, 257, 18589, 1910, 570, 337, 337, 867, 7878, 741, 362, 1105, 632, 11088, 411, 300, 293, 561, 848, 51692], "temperature": 0.0, "avg_logprob": -0.1993642969334379, "compression_ratio": 1.528, "no_speech_prob": 0.0003791695344261825}, {"id": 585, "seek": 443800, "start": 4438.96, "end": 4443.76, "text": " that you are crazy when i said that within my lifetime i want to build something that is smarter", "tokens": [50412, 300, 291, 366, 3219, 562, 741, 848, 300, 1951, 452, 11364, 741, 528, 281, 1322, 746, 300, 307, 20294, 50652], "temperature": 0.0, "avg_logprob": -0.1659609249659947, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.002795684849843383}, {"id": 586, "seek": 443800, "start": 4443.76, "end": 4451.28, "text": " than myself um and now suddenly in recent years um some of the guys who said it's never going to", "tokens": [50652, 813, 2059, 1105, 293, 586, 5800, 294, 5162, 924, 1105, 512, 295, 264, 1074, 567, 848, 309, 311, 1128, 516, 281, 51028], "temperature": 0.0, "avg_logprob": -0.1659609249659947, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.002795684849843383}, {"id": 587, "seek": 443800, "start": 4451.28, "end": 4456.48, "text": " happen suddenly they just look at chat gbt and they think oh now we are really close to AGI and", "tokens": [51028, 1051, 5800, 436, 445, 574, 412, 5081, 290, 4517, 293, 436, 519, 1954, 586, 321, 366, 534, 1998, 281, 316, 26252, 293, 51288], "temperature": 0.0, "avg_logprob": -0.1659609249659947, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.002795684849843383}, {"id": 588, "seek": 445648, "start": 4457.04, "end": 4465.36, "text": " whatever uh so i i don't share these um extreme um", "tokens": [50392, 2035, 2232, 370, 741, 741, 500, 380, 2073, 613, 1105, 8084, 1105, 50808], "temperature": 0.0, "avg_logprob": -0.16764089796278211, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.016733016818761826}, {"id": 589, "seek": 445648, "start": 4471.36, "end": 4478.639999999999, "text": " i'm less impressed than some of those guys let me say that right uh the open source movement", "tokens": [51108, 741, 478, 1570, 11679, 813, 512, 295, 729, 1074, 718, 385, 584, 300, 558, 2232, 264, 1269, 4009, 3963, 51472], "temperature": 0.0, "avg_logprob": -0.16764089796278211, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.016733016818761826}, {"id": 590, "seek": 445648, "start": 4478.639999999999, "end": 4482.799999999999, "text": " that you mentioned you you want to ask a specific specific question about that right", "tokens": [51472, 300, 291, 2835, 291, 291, 528, 281, 1029, 257, 2685, 2685, 1168, 466, 300, 558, 51680], "temperature": 0.0, "avg_logprob": -0.16764089796278211, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.016733016818761826}, {"id": 591, "seek": 448280, "start": 4482.96, "end": 4491.360000000001, "text": " well yeah there was that famous google memo that got leaked and when the waits for loma from", "tokens": [50372, 731, 1338, 456, 390, 300, 4618, 20742, 35900, 300, 658, 31779, 293, 562, 264, 40597, 337, 287, 6440, 490, 50792], "temperature": 0.0, "avg_logprob": -0.125143401924221, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.01378581766039133}, {"id": 592, "seek": 448280, "start": 4491.360000000001, "end": 4498.56, "text": " facebook went out within about two or three weeks um it was a valing pretty similar to chat gbt", "tokens": [50792, 23372, 1437, 484, 1951, 466, 732, 420, 1045, 3259, 1105, 309, 390, 257, 1323, 278, 1238, 2531, 281, 5081, 290, 4517, 51152], "temperature": 0.0, "avg_logprob": -0.125143401924221, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.01378581766039133}, {"id": 593, "seek": 448280, "start": 4498.56, "end": 4504.400000000001, "text": " you know with this um laura fine tuning and the open source community has just exploded you know", "tokens": [51152, 291, 458, 365, 341, 1105, 635, 2991, 2489, 15164, 293, 264, 1269, 4009, 1768, 575, 445, 27049, 291, 458, 51444], "temperature": 0.0, "avg_logprob": -0.125143401924221, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.01378581766039133}, {"id": 594, "seek": 448280, "start": 4504.400000000001, "end": 4509.76, "text": " you can now run it on your laptop and there is some question whether there is a significant", "tokens": [51444, 291, 393, 586, 1190, 309, 322, 428, 10732, 293, 456, 307, 512, 1168, 1968, 456, 307, 257, 4776, 51712], "temperature": 0.0, "avg_logprob": -0.125143401924221, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.01378581766039133}, {"id": 595, "seek": 450976, "start": 4509.76, "end": 4514.88, "text": " gap between the capability you know is is it just a parlor trick is it really as good potentially", "tokens": [50364, 7417, 1296, 264, 13759, 291, 458, 307, 307, 309, 445, 257, 971, 6746, 4282, 307, 309, 534, 382, 665, 7263, 50620], "temperature": 0.0, "avg_logprob": -0.10007150276847508, "compression_ratio": 1.7813953488372094, "no_speech_prob": 0.010607076808810234}, {"id": 596, "seek": 450976, "start": 4514.88, "end": 4519.52, "text": " or could it be as good as some of the next best models from open ai but i guess the question is", "tokens": [50620, 420, 727, 309, 312, 382, 665, 382, 512, 295, 264, 958, 1151, 5245, 490, 1269, 9783, 457, 741, 2041, 264, 1168, 307, 50852], "temperature": 0.0, "avg_logprob": -0.10007150276847508, "compression_ratio": 1.7813953488372094, "no_speech_prob": 0.010607076808810234}, {"id": 597, "seek": 450976, "start": 4519.52, "end": 4529.12, "text": " do you think that we need open ai to to have the best models no of course not um no i'm very", "tokens": [50852, 360, 291, 519, 300, 321, 643, 1269, 9783, 281, 281, 362, 264, 1151, 5245, 572, 295, 1164, 406, 1105, 572, 741, 478, 588, 51332], "temperature": 0.0, "avg_logprob": -0.10007150276847508, "compression_ratio": 1.7813953488372094, "no_speech_prob": 0.010607076808810234}, {"id": 598, "seek": 450976, "start": 4529.12, "end": 4537.280000000001, "text": " convinced of the open source movement and have um supported that some people say the open source", "tokens": [51332, 12561, 295, 264, 1269, 4009, 3963, 293, 362, 1105, 8104, 300, 512, 561, 584, 264, 1269, 4009, 51740], "temperature": 0.0, "avg_logprob": -0.10007150276847508, "compression_ratio": 1.7813953488372094, "no_speech_prob": 0.010607076808810234}, {"id": 599, "seek": 453728, "start": 4537.84, "end": 4542.639999999999, "text": " movement is maybe six or eight months behind the large companies that are now", "tokens": [50392, 3963, 307, 1310, 2309, 420, 3180, 2493, 2261, 264, 2416, 3431, 300, 366, 586, 50632], "temperature": 0.0, "avg_logprob": -0.10329493829759501, "compression_ratio": 1.6606060606060606, "no_speech_prob": 0.00755849527195096}, {"id": 600, "seek": 453728, "start": 4544.639999999999, "end": 4554.719999999999, "text": " coming out with these models and i think the best way of making sure that there won't be dominance", "tokens": [50732, 1348, 484, 365, 613, 5245, 293, 741, 519, 264, 1151, 636, 295, 1455, 988, 300, 456, 1582, 380, 312, 34987, 51236], "temperature": 0.0, "avg_logprob": -0.10329493829759501, "compression_ratio": 1.6606060606060606, "no_speech_prob": 0.00755849527195096}, {"id": 601, "seek": 453728, "start": 4554.719999999999, "end": 4562.16, "text": " through some large company is to support the open source movement because how can a large company", "tokens": [51236, 807, 512, 2416, 2237, 307, 281, 1406, 264, 1269, 4009, 3963, 570, 577, 393, 257, 2416, 2237, 51608], "temperature": 0.0, "avg_logprob": -0.10329493829759501, "compression_ratio": 1.6606060606060606, "no_speech_prob": 0.00755849527195096}, {"id": 602, "seek": 456216, "start": 4562.16, "end": 4568.72, "text": " compete against all these brilliant phd students around the world who are so", "tokens": [50364, 11831, 1970, 439, 613, 10248, 903, 67, 1731, 926, 264, 1002, 567, 366, 370, 50692], "temperature": 0.0, "avg_logprob": -0.06657245755195618, "compression_ratio": 1.5798816568047338, "no_speech_prob": 0.012775149196386337}, {"id": 603, "seek": 456216, "start": 4569.5199999999995, "end": 4574.639999999999, "text": " motivated to you know within a few days create something that is a little bit better than what", "tokens": [50732, 14515, 281, 291, 458, 1951, 257, 1326, 1708, 1884, 746, 300, 307, 257, 707, 857, 1101, 813, 437, 50988], "temperature": 0.0, "avg_logprob": -0.06657245755195618, "compression_ratio": 1.5798816568047338, "no_speech_prob": 0.012775149196386337}, {"id": 604, "seek": 456216, "start": 4574.639999999999, "end": 4585.92, "text": " the last guy has um um put out there on github and whatever so i'm i'm very convinced that this", "tokens": [50988, 264, 1036, 2146, 575, 1105, 1105, 829, 484, 456, 322, 290, 355, 836, 293, 2035, 370, 741, 478, 741, 478, 588, 12561, 300, 341, 51552], "temperature": 0.0, "avg_logprob": -0.06657245755195618, "compression_ratio": 1.5798816568047338, "no_speech_prob": 0.012775149196386337}, {"id": 605, "seek": 458592, "start": 4585.92, "end": 4595.84, "text": " open source movement is going to make sure that there won't be a huge mode for a long time", "tokens": [50364, 1269, 4009, 3963, 307, 516, 281, 652, 988, 300, 456, 1582, 380, 312, 257, 2603, 4391, 337, 257, 938, 565, 50860], "temperature": 0.0, "avg_logprob": -0.09368670137622688, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.008180993609130383}, {"id": 606, "seek": 458592, "start": 4596.88, "end": 4600.16, "text": " i'm reading between the lines here but i would guess you would be opposed to", "tokens": [50912, 741, 478, 3760, 1296, 264, 3876, 510, 457, 741, 576, 2041, 291, 576, 312, 8851, 281, 51076], "temperature": 0.0, "avg_logprob": -0.09368670137622688, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.008180993609130383}, {"id": 607, "seek": 458592, "start": 4601.04, "end": 4606.56, "text": " legislation like the eu is considering where you know very tight restrictions on", "tokens": [51120, 11329, 411, 264, 2228, 307, 8079, 689, 291, 458, 588, 4524, 14191, 322, 51396], "temperature": 0.0, "avg_logprob": -0.09368670137622688, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.008180993609130383}, {"id": 608, "seek": 458592, "start": 4606.56, "end": 4612.4800000000005, "text": " generative models you know onerous onerous kind of uh approval processes and things like that", "tokens": [51396, 1337, 1166, 5245, 291, 458, 322, 260, 563, 322, 260, 563, 733, 295, 2232, 13317, 7555, 293, 721, 411, 300, 51692], "temperature": 0.0, "avg_logprob": -0.09368670137622688, "compression_ratio": 1.6847290640394088, "no_speech_prob": 0.008180993609130383}, {"id": 609, "seek": 461248, "start": 4612.48, "end": 4618.48, "text": " because that's going to have this chilling effect on on open source innovation and the little guys", "tokens": [50364, 570, 300, 311, 516, 281, 362, 341, 31047, 1802, 322, 322, 1269, 4009, 8504, 293, 264, 707, 1074, 50664], "temperature": 0.0, "avg_logprob": -0.07723083193339998, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.007796847261488438}, {"id": 610, "seek": 461248, "start": 4618.48, "end": 4627.919999999999, "text": " wouldn't it yes i have signed letters um which which support the open source movement and whenever", "tokens": [50664, 2759, 380, 309, 2086, 741, 362, 8175, 7825, 1105, 597, 597, 1406, 264, 1269, 4009, 3963, 293, 5699, 51136], "temperature": 0.0, "avg_logprob": -0.07723083193339998, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.007796847261488438}, {"id": 611, "seek": 461248, "start": 4627.919999999999, "end": 4638.16, "text": " i get a chance to um maybe influence some you um politicians then i'm trying to contribute to", "tokens": [51136, 741, 483, 257, 2931, 281, 1105, 1310, 6503, 512, 291, 1105, 14756, 550, 741, 478, 1382, 281, 10586, 281, 51648], "temperature": 0.0, "avg_logprob": -0.07723083193339998, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.007796847261488438}, {"id": 612, "seek": 463816, "start": 4638.16, "end": 4644.639999999999, "text": " making sure that they don't don't shoot themselves in the foot by by by killing", "tokens": [50364, 1455, 988, 300, 436, 500, 380, 500, 380, 3076, 2969, 294, 264, 2671, 538, 538, 538, 8011, 50688], "temperature": 0.0, "avg_logprob": -0.09379578875256823, "compression_ratio": 1.8453608247422681, "no_speech_prob": 0.003816905664280057}, {"id": 613, "seek": 463816, "start": 4644.639999999999, "end": 4649.44, "text": " killing innovation through the open source movement so you certainly want to avoid that", "tokens": [50688, 8011, 8504, 807, 264, 1269, 4009, 3963, 370, 291, 3297, 528, 281, 5042, 300, 50928], "temperature": 0.0, "avg_logprob": -0.09379578875256823, "compression_ratio": 1.8453608247422681, "no_speech_prob": 0.003816905664280057}, {"id": 614, "seek": 463816, "start": 4652.24, "end": 4659.599999999999, "text": " there are lots of different open source movements around the world so if one big entity fails to", "tokens": [51068, 456, 366, 3195, 295, 819, 1269, 4009, 9981, 926, 264, 1002, 370, 498, 472, 955, 13977, 18199, 281, 51436], "temperature": 0.0, "avg_logprob": -0.09379578875256823, "compression_ratio": 1.8453608247422681, "no_speech_prob": 0.003816905664280057}, {"id": 615, "seek": 463816, "start": 4660.88, "end": 4667.36, "text": " support open source or even makes it harder for open source there will still be lots of other", "tokens": [51500, 1406, 1269, 4009, 420, 754, 1669, 309, 6081, 337, 1269, 4009, 456, 486, 920, 312, 3195, 295, 661, 51824], "temperature": 0.0, "avg_logprob": -0.09379578875256823, "compression_ratio": 1.8453608247422681, "no_speech_prob": 0.003816905664280057}, {"id": 616, "seek": 466736, "start": 4667.36, "end": 4675.04, "text": " entities which um won't follow follow and so no matter what's going to happen on the political", "tokens": [50364, 16667, 597, 1105, 1582, 380, 1524, 1524, 293, 370, 572, 1871, 437, 311, 516, 281, 1051, 322, 264, 3905, 50748], "temperature": 0.0, "avg_logprob": -0.06192211366035569, "compression_ratio": 1.5977653631284916, "no_speech_prob": 0.0014758020406588912}, {"id": 617, "seek": 466736, "start": 4675.04, "end": 4685.5199999999995, "text": " level i think open source is not going away i guess just in closing you've been in this game", "tokens": [50748, 1496, 741, 519, 1269, 4009, 307, 406, 516, 1314, 741, 2041, 445, 294, 10377, 291, 600, 668, 294, 341, 1216, 51272], "temperature": 0.0, "avg_logprob": -0.06192211366035569, "compression_ratio": 1.5977653631284916, "no_speech_prob": 0.0014758020406588912}, {"id": 618, "seek": 466736, "start": 4685.5199999999995, "end": 4691.92, "text": " for decades now and what is i know it's a bit of a strange question to ask but what's your fondest", "tokens": [51272, 337, 7878, 586, 293, 437, 307, 741, 458, 309, 311, 257, 857, 295, 257, 5861, 1168, 281, 1029, 457, 437, 311, 428, 9557, 377, 51592], "temperature": 0.0, "avg_logprob": -0.06192211366035569, "compression_ratio": 1.5977653631284916, "no_speech_prob": 0.0014758020406588912}, {"id": 619, "seek": 469192, "start": 4691.92, "end": 4697.92, "text": " memory in your career my fondest memory oh it's usually when i discover something that i think", "tokens": [50364, 4675, 294, 428, 3988, 452, 9557, 377, 4675, 1954, 309, 311, 2673, 562, 741, 4411, 746, 300, 741, 519, 50664], "temperature": 0.0, "avg_logprob": -0.08412290173907612, "compression_ratio": 1.4692307692307693, "no_speech_prob": 0.03558133542537689}, {"id": 620, "seek": 469192, "start": 4699.76, "end": 4706.96, "text": " nobody has seen before but that is that happens very rarely because most of the things you think", "tokens": [50756, 5079, 575, 1612, 949, 457, 300, 307, 300, 2314, 588, 13752, 570, 881, 295, 264, 721, 291, 519, 51116], "temperature": 0.0, "avg_logprob": -0.08412290173907612, "compression_ratio": 1.4692307692307693, "no_speech_prob": 0.03558133542537689}, {"id": 621, "seek": 470696, "start": 4706.96, "end": 4712.56, "text": " are well somebody else has done before um but yeah so", "tokens": [50364, 366, 731, 2618, 1646, 575, 1096, 949, 1105, 457, 1338, 370, 50644], "temperature": 0.0, "avg_logprob": -0.0979831303868975, "compression_ratio": 1.5302013422818792, "no_speech_prob": 0.029217852279543877}, {"id": 622, "seek": 470696, "start": 4719.04, "end": 4727.44, "text": " yeah um what usually happens is um you and and this has happened many times not many times but", "tokens": [50968, 1338, 1105, 437, 2673, 2314, 307, 1105, 291, 293, 293, 341, 575, 2011, 867, 1413, 406, 867, 1413, 457, 51388], "temperature": 0.0, "avg_logprob": -0.0979831303868975, "compression_ratio": 1.5302013422818792, "no_speech_prob": 0.029217852279543877}, {"id": 623, "seek": 470696, "start": 4728.08, "end": 4733.2, "text": " quite a few times in my career since the 80s as a scientist who publishes stuff", "tokens": [51420, 1596, 257, 1326, 1413, 294, 452, 3988, 1670, 264, 4688, 82, 382, 257, 12662, 567, 11374, 279, 1507, 51676], "temperature": 0.0, "avg_logprob": -0.0979831303868975, "compression_ratio": 1.5302013422818792, "no_speech_prob": 0.029217852279543877}, {"id": 624, "seek": 473320, "start": 4733.44, "end": 4742.96, "text": " but suddenly you think oh that is the solution to all these problems and now i really figured out", "tokens": [50376, 457, 5800, 291, 519, 1954, 300, 307, 264, 3827, 281, 439, 613, 2740, 293, 586, 741, 534, 8932, 484, 50852], "temperature": 0.0, "avg_logprob": -0.09280632842670787, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.0013639803510159254}, {"id": 625, "seek": 473320, "start": 4742.96, "end": 4749.599999999999, "text": " a way of building this universal system which um learns how to improve itself and learns the way", "tokens": [50852, 257, 636, 295, 2390, 341, 11455, 1185, 597, 1105, 27152, 577, 281, 3470, 2564, 293, 27152, 264, 636, 51184], "temperature": 0.0, "avg_logprob": -0.09280632842670787, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.0013639803510159254}, {"id": 626, "seek": 473320, "start": 4750.16, "end": 4755.44, "text": " to improve the way it improves itself and so on and now we are done and now all is that's", "tokens": [51212, 281, 3470, 264, 636, 309, 24771, 2564, 293, 370, 322, 293, 586, 321, 366, 1096, 293, 586, 439, 307, 300, 311, 51476], "temperature": 0.0, "avg_logprob": -0.09280632842670787, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.0013639803510159254}, {"id": 627, "seek": 473320, "start": 4755.44, "end": 4762.32, "text": " necessary is to scale it up and it's going to solve everything and then um you think a little", "tokens": [51476, 4818, 307, 281, 4373, 309, 493, 293, 309, 311, 516, 281, 5039, 1203, 293, 550, 1105, 291, 519, 257, 707, 51820], "temperature": 0.0, "avg_logprob": -0.09280632842670787, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.0013639803510159254}, {"id": 628, "seek": 476232, "start": 4762.32, "end": 4767.28, "text": " bit longer about it and maybe you have a couple of publications but then it turns out something", "tokens": [50364, 857, 2854, 466, 309, 293, 1310, 291, 362, 257, 1916, 295, 25618, 457, 550, 309, 4523, 484, 746, 50612], "temperature": 0.0, "avg_logprob": -0.07273446917533874, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.007331924047321081}, {"id": 629, "seek": 476232, "start": 4767.28, "end": 4773.44, "text": " is missing something important is missing and and actually it's not that great and actually", "tokens": [50612, 307, 5361, 746, 1021, 307, 5361, 293, 293, 767, 309, 311, 406, 300, 869, 293, 767, 50920], "temperature": 0.0, "avg_logprob": -0.07273446917533874, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.007331924047321081}, {"id": 630, "seek": 476232, "start": 4774.16, "end": 4781.92, "text": " you have to think hard to add something important to it which then for a brief moment looks like the", "tokens": [50956, 291, 362, 281, 519, 1152, 281, 909, 746, 1021, 281, 309, 597, 550, 337, 257, 5353, 1623, 1542, 411, 264, 51344], "temperature": 0.0, "avg_logprob": -0.07273446917533874, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.007331924047321081}, {"id": 631, "seek": 476232, "start": 4783.44, "end": 4791.12, "text": " greatest thing since sliced bread and um and then you get excited again but then suddenly", "tokens": [51420, 6636, 551, 1670, 27098, 5961, 293, 1105, 293, 550, 291, 483, 2919, 797, 457, 550, 5800, 51804], "temperature": 0.0, "avg_logprob": -0.07273446917533874, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.007331924047321081}, {"id": 632, "seek": 479112, "start": 4791.12, "end": 4797.5199999999995, "text": " you realize oh it's still not finished something important is missing and so it goes back and", "tokens": [50364, 291, 4325, 1954, 309, 311, 920, 406, 4335, 746, 1021, 307, 5361, 293, 370, 309, 1709, 646, 293, 50684], "temperature": 0.0, "avg_logprob": -0.07744026184082031, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.0007788455113768578}, {"id": 633, "seek": 479112, "start": 4797.5199999999995, "end": 4802.8, "text": " forth like that i think that's the life of a scientist the greatest joys are those moments", "tokens": [50684, 5220, 411, 300, 741, 519, 300, 311, 264, 993, 295, 257, 12662, 264, 6636, 1488, 749, 366, 729, 6065, 50948], "temperature": 0.0, "avg_logprob": -0.07744026184082031, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.0007788455113768578}, {"id": 634, "seek": 479112, "start": 4802.8, "end": 4809.76, "text": " where you have an insight where suddenly things fall into place such that along the lines of what", "tokens": [50948, 689, 291, 362, 364, 11269, 689, 5800, 721, 2100, 666, 1081, 1270, 300, 2051, 264, 3876, 295, 437, 51296], "temperature": 0.0, "avg_logprob": -0.07744026184082031, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.0007788455113768578}, {"id": 635, "seek": 479112, "start": 4809.76, "end": 4817.92, "text": " we discussed before the description length of some solution to a problem suddenly shrinks because", "tokens": [51296, 321, 7152, 949, 264, 3855, 4641, 295, 512, 3827, 281, 257, 1154, 5800, 9884, 16431, 570, 51704], "temperature": 0.0, "avg_logprob": -0.07744026184082031, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.0007788455113768578}, {"id": 636, "seek": 481792, "start": 4817.92, "end": 4827.4400000000005, "text": " two puzzle pieces they suddenly match and and become one or become one in the sense that", "tokens": [50364, 732, 12805, 3755, 436, 5800, 2995, 293, 293, 1813, 472, 420, 1813, 472, 294, 264, 2020, 300, 50840], "temperature": 0.0, "avg_logprob": -0.07495385722110146, "compression_ratio": 1.8870967741935485, "no_speech_prob": 0.002841173904016614}, {"id": 637, "seek": 481792, "start": 4827.4400000000005, "end": 4832.64, "text": " they fit each other such that suddenly you have the shared line between the two", "tokens": [50840, 436, 3318, 1184, 661, 1270, 300, 5800, 291, 362, 264, 5507, 1622, 1296, 264, 732, 51100], "temperature": 0.0, "avg_logprob": -0.07495385722110146, "compression_ratio": 1.8870967741935485, "no_speech_prob": 0.002841173904016614}, {"id": 638, "seek": 481792, "start": 4832.64, "end": 4837.52, "text": " puzzle pieces one is negative and the other one is positive and certainly the whole thing is", "tokens": [51100, 12805, 3755, 472, 307, 3671, 293, 264, 661, 472, 307, 3353, 293, 3297, 264, 1379, 551, 307, 51344], "temperature": 0.0, "avg_logprob": -0.07495385722110146, "compression_ratio": 1.8870967741935485, "no_speech_prob": 0.002841173904016614}, {"id": 639, "seek": 481792, "start": 4838.88, "end": 4845.36, "text": " much more compressible than the sum of the things separately so these these things that's", "tokens": [51412, 709, 544, 14778, 964, 813, 264, 2408, 295, 264, 721, 14759, 370, 613, 613, 721, 300, 311, 51736], "temperature": 0.0, "avg_logprob": -0.07495385722110146, "compression_ratio": 1.8870967741935485, "no_speech_prob": 0.002841173904016614}, {"id": 640, "seek": 484536, "start": 4845.36, "end": 4851.36, "text": " what's driving um scientists like myself i guess", "tokens": [50364, 437, 311, 4840, 1105, 7708, 411, 2059, 741, 2041, 50664], "temperature": 0.0, "avg_logprob": -0.1739802828022078, "compression_ratio": 1.4896551724137932, "no_speech_prob": 0.008792865090072155}, {"id": 641, "seek": 484536, "start": 4854.719999999999, "end": 4859.2, "text": " wonderful um professor you again schmidhuber it's been an absolute honor thank you so much", "tokens": [50832, 3715, 1105, 8304, 291, 797, 956, 25394, 71, 10261, 309, 311, 668, 364, 8236, 5968, 1309, 291, 370, 709, 51056], "temperature": 0.0, "avg_logprob": -0.1739802828022078, "compression_ratio": 1.4896551724137932, "no_speech_prob": 0.008792865090072155}, {"id": 642, "seek": 484536, "start": 4859.2, "end": 4863.36, "text": " for coming on the show today thank you it was such a pleasure talking to you", "tokens": [51056, 337, 1348, 322, 264, 855, 965, 1309, 291, 309, 390, 1270, 257, 6834, 1417, 281, 291, 51264], "temperature": 0.0, "avg_logprob": -0.1739802828022078, "compression_ratio": 1.4896551724137932, "no_speech_prob": 0.008792865090072155}], "language": "en"}