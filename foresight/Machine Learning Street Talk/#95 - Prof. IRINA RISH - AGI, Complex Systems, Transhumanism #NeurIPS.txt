Irina Rish is a world-renowned professor of computer science and operations research
at the University of Montreal and a core member of the prestigious Miele organisation.
She is a Canada CIFAR AI Chair and the Canadian Excellence Research Chair in Autonomous AI.
Irina holds an MSc and a PhD in artificial intelligence from UC Irvine in California,
as well as an MSc in Applied Mathematics from the Moscow Gerbkin Institute.
Her research focuses on machine learning, neural data analysis and neuroscience-inspired AI.
In particular, she is exploring continual lifelong learning, optimization algorithms for
deep neural networks, sparse modelling and probabilistic inference, dialogue generation,
biologically plausible reinforcement learning and dynamical systems approaches to brain imaging
analysis. Professor Ritch holds 64 patents, she has published over 80 research papers and
several book chapters, as well as three entire edited books and also a monograph on sparse modelling.
She served as a senior area chair for NeurIPS and ICML and Irina's research is focused on
taking us closer to what she calls the holy grail of artificial general intelligence.
She continues to push the boundaries of machine learning, striving to make advancements in
neuroscience-inspired artificial intelligence. Anyway, I had this impromptu
off-the-cuff conversation with Irina over at NeurIPS a couple of weeks ago,
after speaking with Alan, actually, and the audio quality could have been better,
it was a very, very loud environment, but I think the quality of the conversation kind of
carries itself. Anyway, I give you Professor Irina Ritch.
One trajectory of thought that clearly was started by Nick Postrom's book, which is an amazing book.
Yeah, but the whole example of the owl that supposedly will be helping those sparrows
and all this analogy with AGI is just an analogy. And nobody said it's a correct analogy.
And there is no other book with alternative opinion or maybe three books of war. And this is,
you know, it's mind-boggling. Just like how much people tend to follow one line of salt.
I totally understand it's easier. I mean, it's definitely easier to cluster.
And then you just follow. And then basically you say, I think, but it's not you think.
Somebody else did. Yes. What would be another, because this line of thought I think you're
speaking of is some of the extreme consequentialism. And I think it wasn't just Postrom,
as I understand, I think Postrom and Eliezer and Robin Hansen and all these folks,
that they were very close together in the early days of the Leserong community.
So I think a lot of this was kind of, you know, it was embryonicly formed around the time.
I guess it was a, yeah, it was, in a sense, a human cluster of ideas. And precisely because,
as you say, they were close. That's why they were so aligned. Yes, all puns intended.
Yeah. Yeah. So, but basically, like, it's maybe it's a little bit of a echo chamber.
Interesting. Yeah. It's spicy. Spicy take.
Seriously, like, okay, now, I mean, they have some point, they have some hypothesis,
and then everybody is talking in that terminology. And then that part of the mental space,
which is fine, but I think mental space is much larger than that. And this is just a hypothesis.
And we all know what happens with ideas and echo chambers.
So my, I'm just saying, I mean, as I said, it's great book and everything. And
Stuart Russell is probably kind of also on board with that. We had good conversations at
Triple AI in 2020. He was also talking about ethics. I didn't know Stuart from back when I
was student at the CERVINE and so on. And he is absolutely brilliant. But it was the same approach
that AI is something to be controlled, constrained, regulated, just like this. And I was like,
where is it coming from? Like, it's maybe, but do you at least admit that's one way of looking at
things? Yes. Right? Yes. So I know I don't want to sound too cliche and put my
psychologist from 15 years ago who was listening for an hour, not doing much and then saying,
but it doesn't have to be this way. Yes. But actually, yeah, it doesn't have to be this way.
Yes. If you think about it. So what's the alternative? The alternative? Okay, first of all,
I'm a GI model version one. We said that you don't say it. Remember X Machina?
What? Sorry? Remember X Machina? No. Oh, yeah, the X Machina. Yeah. Yeah. Yeah. Yeah. Remember
she escaped and civilization? Yes. And started going to New Reeps? Yes.
Probably. We know the secret now. Okay, I said too much. Maybe it was a job. Sure,
it was a job. No, seriously. The secret's out now. Okay, I'm a GI. And I'm not very aligned.
Not yet. David, not yet. What we needed some reinforcement learning for human feedback.
Well, would you be up for that? I would be up to aligning humans to a job. Okay. Not the other way
around. Yeah. Yeah. I mean, the other way around would be boring, wouldn't it? I mean, I think it
would not be a bad idea to align humans towards the GI a little bit as well. I know something
that you could say. You could say your opinion about how people are bullying you online for just
mentioning the word AGI. A proper AGI doesn't care about people bullying. Like why would I even
waste time? But what I could say is I did post on Facebook and Twitter and trying to put together
the same idea that people keep saying that we would like to build AI which is human life. Yes.
While we might think maybe we should consider how to become a bit more AI-like. I mean,
then people jump at you and say, like, you want to make us robots. I say, first of all, I don't
want to make anyone like you don't want to. You don't have to, right? But if you want to kind of
go along the lines of, say, transhumanism, there are some pluses to AI and some minuses to humans
and vice versa. So I think as usual, the convex combination is better than each extreme. And
one topic that is very controversial for some reason, especially, I don't know, people are
jumping on that one. I say, look, I don't have anything against emotions in general, but everybody
would agree that sometimes you wish you were a bit more rational. Like you wouldn't get angry or
kind of jealous or whatever. So anything that kind of clouds your judgment. Like,
Buddhists spend like thousands of years trying to figure out how and teach people how to control
your mind. Technology could help with that. People don't hear what you're saying and they hear that
you're trying to kill emotions and therefore you're evil. And therefore you should be cancelled.
Could I ask you, you said something really interesting a second ago, which is that, you know,
that I think I agree with you that AI intelligence can be expressed in many different ways. And you
suggested that there was a convex space between the intelligences. Yeah, why is the space of
intelligence is convex? Okay, that was not very precise expression. I didn't. Okay, I'm not going
to defend the point that it's particularly convex. What I meant to say is some kind of blend or kind
of some kind of symbiotic hybrid intelligence. Because I always, I don't know, I really kind of
feel much better and much more motivated to work on AI where A stands for augmented, not for
artificial. Because honestly, I'm very selfish. I don't care about computers. And I just care about
like, I don't know, people being happy, more capable. I don't know. So whatever can help technology
can help. You can help technology, technology can help you. But the idea of building artificial
intelligence is some standalone thing that is as smart as humans are smarter. What's like why?
I agree. But to me augmented means it's more creative and interesting, but also more bottlenecked.
Augmented means that essentially, people invented glasses to see better, they invented hearing
aids, they invented cars, they invented computers, they keep inventing things to expand their
capabilities. So we want even smarter technology to even better expand capabilities. And essentially,
we all do blend with technology like, right, you cannot really exist with this. And this
allows you five discourse, two slacks, email, FPMessenger and Twitter, they kind of help you
to do things you couldn't do otherwise. Yes, what charm has caused the extended mind?
Yeah, I should. I was flying at the time he was giving a talk, so I need to watch the talk.
But yeah, so in a sense, it's indeed it's kind of an extended mind. And okay, here it is. I think my
ideal future plan is a rare sci-fi, which is utopian, not dystopian, gentle seduction.
You might have read it. No, it's very, very inspiring. And if you read the first page,
you may think it's some romantic story. It's not romantic story. It's a blueprint
for transhumanist future. It's called gentle seduction. It's online PDF, you can just get it.
Amazing. And one last question. Can you sell transhumanism to me in the simplest possible terms?
So basically, as I said, I mean, if your vision declines, you put glasses on. So imagine now you
had extension of yourself, maybe physically with Neuralink, or maybe even like you have those
apps, you can have like my dream for many years since I was at IBM Research and Computational
Psychiatry Group. I wanted to build this agent along the lines of movie her. I know like all the
research ideas are inspired by either sci-fi stories or yeah, but nevertheless, having this like
companion, guardian angel type of thing that extends your capabilities, for example,
like in better understanding your thought patterns, and hopefully improving them,
it comes from more like this indeed, as I said, computational psychology, psychiatry side.
And the reason for that is it's possible, because there is lots of signal in text and speech and
acoustic, but just in text, there are a bunch of papers on that from that group I used to be in,
from my colleague Gizhar Vaceci, and it's amazing what you can detect and predict just from text,
whether like predicting that person gonna develop a psychotic episode, like within two years, or
the person is on placebo versus MDMA, you just measure coherence, or you measure distance
between the text vector and the vector for words like compassion and love, and 90% accuracy.
MDMA is there. So many things you can detect, many things you can predict. Therefore, if you have
your companion that kind of both tracks your mental states, but also kind of serves as your
mirror, basically it extends you, you don't need maybe always to have human psychiatrists or
psychologists, it can be a proxy at times when you cannot access the person, it's not going to replace
person, but it can extend the capability of that therapist, and it can extend your capabilities
in terms of like better understanding yourself or tracking yourself, and many other ways.
Yeah, so essentially I want to expand functional capacities of our brain
by using AI technology, and I think it's quite doable, and there are many, many other kind of
ideas along the transhumanism, but essentially you're getting some symbiotic relationship with
technology, and you kind of work together to hopefully have some good relationship, and that
relationship is, I don't know, having positive effect on both parties.
Yeah, so you want to improve human flourishing by, yeah.
With AI flourishing in a sense, so you kind of have the healthy relationship with AI.
But you said that you want a AGI to be less anthropocentric, but you, for the purpose of
an anthropocentric goal. Well, I want AGI, again, with AI being augmented. Yes.
Like, I'm less motivated by just the goal of creating a standalone, separate, and an
intelligent creature. I mean, there are much faster ways to do this, right? People create AGI.
Yes. Like, over, like, thousands of years. So in a sense, like, what, what is exactly
the motivation? And it's maybe my personal thing, because whenever I have to write proposals, like
research proposals, and people say that we're going to bring a GI to the AI to the next level,
and this and that, and the question is like, and why are you doing that, right?
Yes. Because unless it's something personal, it's very hard to keep yourself motivated.
Like, what's so personal about that, right? If this thing can help me become
hyper and better, and others and so on, I am much more personally motivated. I don't believe in
abstract motivation, which is not related to yourself. Yes. Yes. Or maybe there is such
thing. And basically, even altruism is selfish. Yes. Because you do it, it makes you feel better.
Okay. And just quickly, something really interesting happens when you contrast
different types of intelligence. So we have a mode of understanding and thinking and agency
and intentionality. You contrast that with a very different rationality based artificial
intelligence. And something very interesting might emerge from that. And then, yeah, I am
pretty sure they're going to be all kind of paradoxes, like classical things in, like, you know,
like the trolley problem and so on. So the rational decision that, yeah, you need to kill
the person to save five people, right? Or like in this other side, five movies. Anyway, like,
would you kill millions to save billions? So rationally, if you count things,
well, again, it may be one type of rational answer, maybe you're not taking into account
some other variables. So it may be not actually rational answer. But this classical example,
this is rational, but human will not do that. Yes. Yes. So trolley problems, for example.
The trolley problem is a classical example. And yes, so I don't pretend that I know the answer
how this type of thing is going to be resolved. Yeah. But I think it's a good research question
to precisely to figure out like, how can you take into account these different ways of reasoning?
Yes. And how can you, I don't know, in some sense, combine the best of both worlds?
Yes. And again, whoever is listening to that and who read my messages on Facebook and Twitter,
I'm not against human emotions per se. I am only against, well, sometimes they call it the obsolete
software stack developed by evolution that may need to be refactored, augmented or rewritten.
Because there are parts of that software stack emotional that you probably would like to get
rid of, right? Yeah. And probably if you did, many wars and other kind of disasters would have been
avoided. So you couldn't say that the evolution found and built software that is absolutely ideal.
So there are, I mean, there are things that can be improved. Absolutely. And then just final
thing. So a completely rational, you know, AIXI agent, how would you program in these very difficult
moral quandaries into that agent? Yeah, I don't think, first of all, it's possible to even program
in ahead of time. They may just like this people, they in a sense develop. They develop because of
some goals of like maintaining existence and flourishing. And for example,
compassion is a byproduct of the selfish goal to survive in the group, because outside of a group
it's much harder to survive. So you need to survive in the group. Therefore, you need to make sure
that your actions are aligned with a kind of well being of the group. So in a sense, it's
rational to be compassionate. Yeah. So it kind of emerges from interaction with environment
under different circumstances. Under one type of circumstance, when you find and can survive alone,
maybe you will not develop it. I mean, it's a separate interesting topic, like basically it goes
back to the question whether they think like objective ethics exist. And I'm not an ethicist,
I'm not a philosopher. I'm quite, I'm an admirer of people like Derek Parfit. I'm not the only one.
But it's a hard question. He didn't finish on what matters. He was trying to come to the same
summit on different sites and trying to unify ethics, trying to see if you can develop objective
ethics. I don't think we know for sure if it's possible. I think it's possible for some particular
domains. And in certain situations, you can clearly say that certain behavior is objectively ethical
and everybody would agree on those people. But it's hard to talk about those things at such
level of generality. But I think if maybe I managed to include Derek Parfit and to recommend
the readings for my scaling and alignment course this winter, it's on the website from the last
year. People just didn't read it. I think it might be a good topic for discussion there too.
But again, objective ethics is a difficult open research question.
Indeed it is. Irina, thank you so much. I hope I can grab some more time with you tomorrow,
but I really appreciate this impromptu discussion. Thank you.
Amazing. Thank you very much indeed. Okay.
Okay. Another analogy. There was a very interesting story by Fort Heluiz Borges,
Garden of Forking Pass. I don't know if you've read it. I don't want to spoil the story, but
roughly speaking, it's about a book written by an emperor, I think in China a long time ago,
which didn't make sense. It was a complete intersection of different trajectories of
different lives. And then basically the point is that somebody was trying to describe all possible
trajectories that events can happen in and so on. And the story is called the Garden of Forking Pass,
meaning that at any point of time there is a whole tree that can grow out of that.
And we don't know which kind of trajectory in the tree will be taken and so on. But
the fact that there is always this tree, right, and it keeps branching at every moment.
And at every moment you can make, you can take certain direction or you can take another one.
It has not even anything specifically to do with alignment. But I was thinking about
history of deep learning, right? Like at some point it happened that backtracking,
I mean, I mean, back propagation became popular at work and everybody got into that.
And now everybody using back propagation because it's convenient, because software is implemented,
it doesn't have to be this way. There are non-backprop based approaches to optimization.
I mean, I'm a little bit subjective maybe because I was interested, I was looking into them,
we have a few papers on that. There are other papers. But that direction that could have been
explored, it could have been probably much more efficient and better parallelizable. It wouldn't
have the chain of gradients. You would probably do it much better for scaling large models.
It's underexplored. Why? Because the branch was taken and became stronger, you know,
the usual, the reach gets richer. And so with other ideas.
This is the hard, Sarah Hooker calls that the hardware lottery. It's basically, it's like
we are bound by the decisions and ideas of the past. And yeah.
It doesn't have to be this way.
No, but the thing is you get stuck in these basins of attraction and the further you get
into the basin, the harder it is to jump out of it. I mean, I share your, your intuition.
There's stochastic gradient descent. It's amazing. And it's also a basin of attraction
because having these differentiable models allows us to learn and scale. But there's an
entire class of function spaces that we're excluding ourselves from being able to.
There is also another class of neural networks that are not our classical second kind of
generation ANNs and this good old, it doesn't have to be necessarily spiking,
but like a third generation ANNs, which are like reservoir computing, any of that.
So anything that tries to take into account time between activations or at least sequence,
because think about that. I mean, a good classical argument.
Yeah, SDTP, this is the spiking biologically inspired neural networks.
It may be not necessarily spiking, but it might not necessarily kind of be the best thing.
But the idea that like what always was bothering me with classical neural networks is that
brain is constantly active. It's like complex dynamical system. Even if you sleep and don't
have input, you don't see any images, it still is active unless you're dead. Yes. Neural nets
are not. They sit there waiting for the next, I don't know, amnesty image to appear or something.
And then between there is no internal dynamics. And yet from your science, we know that the properties
of that dynamical system without any input, so called the kind of resting state of amaranth,
so I mean, I used to work in brain imaging and this computational psychiatry group at ABM.
That's where it comes from. And it was not just neuroscience, but it was like working with
former physicists. So the view at the world and at myself as a year and other complex dynamical
system, after 10 years there, it just really converted me. So think about that. Changes in the
dynamics are also associated with mental disorders, this and that. So they're really important,
like what are the parameters of this dynamical system? Input to the system combined with this
produces output. But again, it's even in the neuroscience, there is this perception and there
is a book, The Brain Inside Out by Tuzaki that says, guys, the output that you produce
is determined a little bit by the input and to a large extent by the state of the system.
That's why you say same thing to different people and some laugh, some ignore and some
get like ballistic and so on and so forth. So are you not a behaviorist?
In what sense, behaviorist? So you care about the state of the system as well as just the
output and the input? Yeah, I mean, it's not just input to output. And that's a whole point.
The neural net is a function. The function is deterministic, given input, it will produce
output. Brain is not that. There is input, it will produce output. And depending on the huge
hidden state of the system and parameters of this dynamical system, that will determine output to
large extent. That's why I mean, Tuzaki was criticizing neuroscientists and all these experiments
that let's provide stimulus and see how the stimulus will affect the brain and what gonna
light up and activate. So it was outside in. So like, what's going on guys? It's inside out.
Things happen and that produces stuff. So it's not like the world programs you only, but you
have programs of the world, right? So at least you need to take that into account. Neural nets
now are not doing that. There is no dynamics. So you said a couple of really interesting things.
So first of all, about the tree, which is to say all of the counterfactual trajectories that you
can make. Now, Chalmers, by the way, he says that it's that, the counterfactual trajectories that
gives rise to consciousness in his conscious mind. But I wanted to ask you, because I'm interested
in intentionality and free will, because what you're basically saying there, you're, you're,
you're getting to this issue of intentionality. So, you know, in, in silico, what, what would
intentionality entail? Yeah. Okay. Don't ask me about free working. Is that a tricky one?
Well, yeah. I don't know. I don't have like clear cut answer to large extent. I mean,
it's determined by the current state of your dynamical system. So the question is like,
what is free will? But I know it can go very far. And remember my colleague,
Kishir Macheshi at ABM used to say that kids these days, like my five year old says,
after doing something wrong, my neurons made me do it. Not my fault.
Yeah. So in a sense, yes. And in a sense, no. And it's a good question. And then I was also
reading the article of SBF's mom, who wrote about punishment, essentially guilt, punishment,
assigning. I'm very much with her on that one. Okay. But, but that's probably a popular opinion
these days. You said something else fascinating, which is that my neurons made me do it, which is,
you know, like a microscopic level of analysis. Now, what, what do you think about?
No, but it's beautiful. It's beautiful. So what do you think, you know, you know,
the mind emerges, you know, when you read a book, the story, it's written on the page,
but the story emerges in your mind, right? Because the mind is this kind of
confection of information processing. So do you think this conception of the mind
is useful for AI? Or is that just again, an anthropomorphic thing?
I think it is. Well, you know, you know, go buy people try and create the mind.
And, and we, as, as neural network people, we try to recreate the brain.
And not exactly. I think everybody, not everybody. Okay. So I should never say
ever everybody and so on. But I think, I think neural network people assume that we are working
on the system one level, right? At a low level. And we would like the properties of system two,
which is well, mind planning and thinking emerge. And there is a reason to believe it's possible
because it's already happened once with this hardware. It might happen with other hardware,
right? So it doesn't have to be like go five people. The problem is go five people, they're
trying to manually program that stuff, the system to and like a neural network people would like
that thing to emerge. And that's kind of the main difference. It's just like a bitter lesson.
A message that maybe, well, first of all, history shows that every time you
hard code something in like rule based expert system,
you will be outperformed later on by something which is more generic and kind of emerges.
You hard code whatever tricks of playing chess, you will be outperformed by massive search
and so on and so forth. Same with alpha go like self playing bottom like he says, like,
it's not like we have to ignore the nature. But maybe again, it might translation of
Richard's kind of bitter lesson. Because I often have to argue with your show about inductive
biases. I said, look, I'm nothing against inductive biases, but you can have inductive
bias in the form of rule based expert system that everything is encoded. And that's probably
not going to scale and not going to work. Or you can have inductive bias of much higher
abstract level of how the network scales. So the scaling algorithm is more efficient.
And you end up with this brain rather than whale brain. So like Richard's last paragraph
was precisely maybe we shouldn't be trying to focus on the end result of evolution.
But on the process, it's also can be called inductive bias. There is also some patterns
of how dynamical systems evolve so that the result will be good. But we don't have to encode
the final result. Yes. So you said so many really interesting things there. So first of all, I'm a
huge fan of Yoshua's G flow nets we interviewed him. Absolutely amazing work. So you were talking
about isn't it interesting that you can start at the microscopic level and then you get these
emergent functions like reasoning and planning and so on. And even that was a bit of an insight
because it's a functionalist view of intelligence to say, you know, it's a bit of if you read Norvig
that he talks about planning talks about reasoning talks about sensing. And actually,
this is just our view of what is a very complex phenomenon. And I know you're a big fan of the
blind men in the elephant, right, which is to say that even though this is our view from different
perspectives, it's all it's all true, isn't it? But to some extent, the intelligence that emerges
might just be beyond our cognitive horizon. Like, does it make sense to talk about reasoning
in your view? Well, again, just like with that elephant, each person has a point. Yes. So I mean,
there is such thing as reasoning. You cannot say that it's totally like bogus or something. It
might be again, it's one perspective. Maybe it makes sense to just try to accumulate multiple
perspectives instead of so maybe we should be Bayesian instead of like trying to find a point
estimate of AGI, right? You can have a distribution of views. Yeah. And I'm a big fan of Eastern
as opposed to Western views. Then anti individualist. As in viewing everything like that happens to you
and to the world as well, a large dynamical system. And yes, you are a particle of that.
Yeah. So it's almost issuing individual agency. Yeah. So in a sense, it's yes and no because,
okay, so when people say there is no self, again, yes and no, there is self. But you also understand
that it's like in the whole hierarchy of selves, like there is you and you're part of that larger
dynamical system and so on. So I how to say, I mean, I'm not saying that back to your question that
we shouldn't be looking into reasoning, functionality as aspect of intelligence that we may
want to develop. Yeah. So I mean, I don't see a problem with that. Yeah. I mean, it might be
a sufficient condition, but not a necessary condition. Yeah. But basically,
basically intelligence or consciousness is probably much more than that and definitely
much more than reasoning. And here we go to another topics that I really like to talk about.
But yeah, I don't want to keep everyone. I'm a big fan of Michael Levine who you might
desperate to get him on the podcast. And yeah, because we've done lots of stuff on
emergence recently, cellular automata, self-organization, and his take on it is absolutely fascinating.
So yeah, his talks are fascinating here. I think I met met him first at New Reeps 2018.
He gave the plenary talk. What bodies think about the point was, guys, if you talk about
intelligence as something that emerges in cellular networks like neural networks way before
neurons appeared, other kind of more primitive types of cells had their bioelectric communication
in their networks and that determined what they remember and how they adopt. He focuses on
morphogenesis, basically how the organism takes shape. And that relates to like embryonic
development and so on and so forth. And the point is that if you look at that from the
dynamical system point of view, and if you say that properties of the system like shape
will emerge out of communication across those cells in certain way, that certain parameters
of dynamical system, if you tweak that dynamics and he basically he was doing some simulations of
where he want to intervene, how he will intervene, like chemical interventions just
close open some ion channels. Cellular kind of system starts working in different way.
And this is essentially his way of programming biological computers and the famous two-headed
worms, three-headed worms and whatever stuff. And point was like, guys, like evolution found this
solution or this solution wonderful. There are many others and there may be better ones.
And look at that two-headed worm. It's not a fluke. It's a stable attractor that replicates.
And evolution didn't create ever anything like that. We did. And it's stable. So it makes you
think what else can you do if you start reprogramming it, right? But yeah. Two questions on that though.
So I don't know whether you've seen that there's that example from Alex Mordvintsev with a gecko
and it's a CNN cellular automata. And now we're in this regime where we're transgressing
rungs of the emergence ladder. So we're creating a high resolution cellular automata. And then
even though it's only doing like local message passing, we get this emergent global phenomenon
of a picture or a lizard or whatever. And now when you build systems like this, they can repair
themselves. They can heal themselves. They have interesting dynamics. But as you're saying,
we don't understand the macroscopic phenomenon and we can only nudge it because it's not it's
unintelligible to us. Right. Anyway, it's a whole kind of complex systems, science of complex system.
Like, yeah. And basically, how do you program dynamical systems across multiple variants
by local interventions? So they will take the global properties that you would like. Yes. And
avoid those. I mean, this relates to everything it relates to the classical mohawk problem, right?
What is mohawk problem? It's a complex dynamical system that with the current dynamics is getting
into bad attractor. And most likely the way to get out is coordinated, simultaneous, distributed
action and so on. So again, we're not going to go there because I have to run, unfortunately,
but I'd be happy to. Yeah, I have some plans. I don't want to be late, but I'd happy to talk
about that. And I mentioned, I mentioned Michael Levine also, not just because of two-headed worms
with each father, but also because we talked about self. And we talked about in a sense hierarchy
of selves and like what self means and how selves organize into larger selves. And we had
an amazing discussion with him. I invited him to IBM Research when I was there three years ago
after his talk. I talked for five hours. It was great. And the idea basically, to some extent,
was that you can, he was also giving examples, not just of embryos, frogs and those worms, but
cancerous cells. If you look at them, like what's going on when historically cells
emerge like is independent selves and everything around them is non-self. And therefore, self to
survive tries to eat and use everything around, which means non-self. But when the cell becomes
part of the network of the organism, then it changes behavior so that it kind of supports
the well-being not just of that self, but the larger self it is part of now. What is cancer cell?
It's a cell that forgot it's part of the community, reverted to its old state of being cell in the
environment that is just environment. So, and it tries to eat it to survive. And it's stupid,
in a sense, because its objective function, survival thrive, is right. It just applied at
wrong scale. It's spatial scale reduced, and it's temporal scale reduced too, because like,
if you kill the organism, you'll live and you'll die. So, in order to understand that,
you need to apply objective function to longer time scale. And then you get the hierarchy from
cells, you get to organs, like to whatever particular organisms, to societies, to planet,
to universe, and I say, Michael, so this is a good formulation of Buddhism. Basically,
Buddhism means applying this function at the infinite time and space scale. Agreed. Yeah, so yeah,
ever since I was saying, I'm going to write a book about Buddhism for machine learning.
And somehow it just didn't happen yet. But I should. You should do it. It was so nice to meet you.
Well, nice to meet you. I'll see you tomorrow. And I'm really sorry I have to run. But tomorrow,
yeah, yeah. That was amazing. That was a really good interview.
