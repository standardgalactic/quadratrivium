WEBVTT

00:00.000 --> 00:06.080
Hattie Jo, a PhD student at the University of Montreal and Miele, has set out to understand

00:06.080 --> 00:12.000
and explain the performance of modern neural networks, believing it to be a key factor in

00:12.000 --> 00:17.920
building better, more trusted models. Having previously worked as a data scientist at Uber,

00:17.920 --> 00:24.000
a private equity analyst at Radar Capital, and an economic consultant at Cornerstone Research,

00:24.000 --> 00:28.400
she's recently released a paper in collaboration with the Google Brain team titled,

00:28.400 --> 00:34.560
Teaching Algorithmic Reasoning via InContext Learning. Now in this work, Hattie identifies and

00:34.560 --> 00:41.040
examines four key stages for successfully teaching algorithmic reasoning to large language models,

00:41.040 --> 00:47.840
formulating algorithms as skills, teaching multiple skills simultaneously, teaching how to

00:47.840 --> 00:55.120
combine skills, and finally teaching how to use skills as tools. Now through the application

00:55.120 --> 01:00.880
of algorithmic prompting, Hattie has achieved remarkable results with an order of magnitude

01:00.880 --> 01:07.280
error reduction on some tasks compared to the best available baselines. Now this breakthrough

01:07.280 --> 01:12.080
demonstrates algorithmic prompting's viability as an approach for teaching algorithmic reasoning

01:12.080 --> 01:18.480
to large language models and may have implications for other tasks requiring similar reasoning

01:18.480 --> 01:22.960
capabilities. Anyway, I hope you enjoy this conversation that I had with Hattie over at

01:22.960 --> 01:28.240
NeurIPS a couple of weeks ago. Enjoy. Amazing. Well, Hattie, we're here at NeurIPS.

01:29.280 --> 01:36.640
Welcome to MLST. Thank you. Please introduce yourself. I'm Hattie Jo. I'm currently a PhD

01:36.640 --> 01:44.160
student at NILA and a part-time researcher at Google Brain right now as well as a student

01:44.160 --> 01:51.040
researcher. Fantastic. And I've been reading your paper, Teaching Algorithmic Reasoning

01:51.120 --> 01:57.760
via In-Context Learning. Give us the elevator, bitch. The elevator, bitch. So I think

01:59.120 --> 02:03.760
typically in the past people have thought that large language models are not great at

02:04.960 --> 02:12.240
doing symbol manipulation or actually doing reasoning the way humans do. And a common example

02:12.240 --> 02:18.320
people point to for this failure is to show that models can't even do addition properly,

02:18.320 --> 02:22.720
even though it's trained on billions of tokens, billions of parameters.

02:24.480 --> 02:31.600
And in this paper, we try to teach the model how to solve these problems by learning an algorithm

02:31.600 --> 02:38.080
for them and see if it can generalize to a harder problem, generalize out of distribution,

02:38.080 --> 02:45.280
which is a common way to see if the model is actually using the correct algorithm to solve

02:45.280 --> 02:49.840
these tasks. So it's not just like fitting to the training distribution and finding

02:49.840 --> 02:57.200
spirit features, it's actually executing an algorithm. So we do that through some prompting

02:57.200 --> 03:03.120
strategies and show that actually can learn how to do addition contrary to previous belief.

03:04.080 --> 03:07.760
Yeah, I wanted to speak about the previous belief. So with large language models, I'm

03:07.760 --> 03:12.400
vacillated back and forth from being skeptical and being very optimistic. I was originally

03:12.400 --> 03:17.680
very skeptical and I'm becoming more optimistic as time goes on. I interviewed a lady from UC

03:17.680 --> 03:23.920
Irvine called Yasaman Rezegi. And she did a really interesting bit of work actually kind of comparing

03:23.920 --> 03:30.960
the term frequencies as they appeared in the corpus to the arithmetic as a function of the

03:30.960 --> 03:36.480
number of digits. And because OpenAI never released their data set statistics, so it wasn't possible

03:36.480 --> 03:41.520
to do that. And she found that there was like a linear correlation. But I think what's really

03:41.520 --> 03:50.240
interesting now in your work and some of the other work like Chain of Thoughts Prompting and

03:50.240 --> 03:55.840
Scratch Bad Prompting, that now we're in this new regime where we're kind of telling the language

03:55.840 --> 04:00.480
model. It's almost like we're treating the language model a bit like a kind of compiler

04:00.480 --> 04:06.080
and we're giving it the program. And then it's actually extrapolating and it's doing things that

04:06.080 --> 04:12.320
it wasn't directly taught. Right. Yeah. I think that's the first step, right? You want to,

04:13.120 --> 04:19.200
like if you can give the model a program, you want to see that the model can use that program

04:19.200 --> 04:25.840
and apply it to new situations. And so that's kind of the reasoning out of the distribution

04:25.840 --> 04:32.480
component. But I think also ideally you can imagine we want the model to be able to discover

04:32.480 --> 04:39.760
algorithms that we don't know ourselves. And you know, that's a whole other frontier, right?

04:39.760 --> 04:45.840
But this is the first step too on that path, I think. Okay. Well, why don't we start by defining

04:45.840 --> 04:56.240
algorithmic reasoning? Sure. So I think about it as just the way you solve a task is by using a

04:56.240 --> 05:05.040
particular algorithm. And algorithms are input independent, which means that basically for

05:05.040 --> 05:11.200
any input distribution, using that same algorithm will get you the right answer. And so performing

05:11.200 --> 05:18.880
reasoning by running an algorithm is what we refer to as algorithmic reasoning. And of course,

05:18.880 --> 05:25.840
these right now apply to tasks that can be solved by an algorithm. But you could also imagine,

05:25.840 --> 05:32.640
like, cases where you have a soft algorithm where the steps are not so rigidly defined,

05:32.640 --> 05:38.720
but there is an overarching problem, like solving structure that you can follow.

05:40.400 --> 05:45.680
Yeah. Could you refer to that? I mean, maybe I would refer to that as like inductive

05:45.680 --> 05:51.280
algorithmic reasoning. So it's where, so on one side of the spectrum, you actually have the

05:51.280 --> 05:56.880
algorithm and you write every single step explicitly using some kind of code. And now we're talking

05:56.880 --> 06:03.440
about this regime where we are giving examples of an algorithm and we're describing the steps

06:03.440 --> 06:08.720
somewhat vaguely. We're trying to be as clear as possible using language. So where are we kind

06:08.720 --> 06:15.600
of following on that continuum? Well, let's say you take an algorithm of addition.

06:15.840 --> 06:25.040
Yeah. But you actually, so you have an algorithm defined where you like start by taking the

06:25.040 --> 06:29.520
first digit, do something with it, and then get an answer and then move on to the next one.

06:32.000 --> 06:38.320
When I think about a soft algorithm, it can look something like that, except you take the digit

06:38.320 --> 06:45.920
and the something you do with it is not defined explicitly, but it might require some abstract

06:48.320 --> 06:55.120
pattern matching that large language models are particularly good at. So it can use the

06:55.120 --> 07:01.520
same process of breaking down the problem in a very specific way to generalize, but the individual

07:01.600 --> 07:12.560
steps are not actually encoded in code because it's abstract. So if you can do that, then you can

07:12.560 --> 07:20.000
use this approach to tackle tasks where you can't write an algorithm or you can't write a program

07:20.000 --> 07:26.080
for. And that's like very exciting, I think. Can you give me an example of where we can't

07:26.080 --> 07:32.720
explicitly write the algorithm for something? Well, it's hard to come up with a good example

07:32.720 --> 07:41.600
because it's supposed to be abstract in some sense, but these will not be questions of

07:44.480 --> 07:51.200
that you would normally tackle with by writing a program. So it could be a soft reasoning.

07:56.400 --> 08:02.160
I guess maybe even the grade school math word problems, there is no algorithm or a simple

08:02.160 --> 08:08.720
algorithm at least that you can write that will solve math word problems, which are like if you

08:08.720 --> 08:13.680
have four apples, I gave you twice the amount of apples you have. How many do you have now?

08:15.840 --> 08:24.080
But if you define a way to tackle these questions at each individual step, the model can decide

08:24.080 --> 08:32.800
how to apply that flexibly based on the particular question. Yeah, that might be a good thing to

08:32.800 --> 08:41.200
have. But yeah, right now, I don't know. There is no good benchmark for these kind of things.

08:42.240 --> 08:48.640
Yeah. And in your paper, so you came up with a new algorithmic prompting technique and you

08:48.640 --> 08:53.520
designed some experiments and your technique works significantly better than some of the

08:53.600 --> 08:56.640
other in context prompting techniques. Can you sketch that out for us?

08:57.360 --> 09:04.720
Yeah, so the intuition is very simple, actually. When we look at the addition algorithm, the one

09:04.720 --> 09:12.480
that we learned in school, we know that you start with the rightmost digits, you add them up,

09:12.480 --> 09:19.680
you add them up. If it's greater than 10, you have a carry of one, and then you add the carry to

09:19.680 --> 09:29.040
the next sum and so forth. So a scratch pad approach for addition will show the trace from

09:29.040 --> 09:35.440
running this algorithm. So it will show the first sum is this, and the carry is this. But it doesn't

09:35.440 --> 09:42.400
explain how those values are derived. But for us, it feels really natural because we're so familiar

09:42.400 --> 09:48.400
with it. But for a model, trying to infer the rules from seeing a couple examples of it,

09:49.280 --> 09:55.360
that's a very under specified problem. There are many rules that could explain perfectly these

09:55.360 --> 10:06.080
observations. So algorithmic prompting basically explains each step of an algorithm using some

10:06.080 --> 10:13.520
examples. And within each step tries to be as detailed as possible and tries to disambiguate

10:14.320 --> 10:20.240
as much as possible what you want the model to do. And it turns out that when you provide more

10:20.240 --> 10:26.720
details to the model, you're sort of constraining the model's interpretation of disinformation

10:26.720 --> 10:35.280
so that there's only one way to apply this. And with that, you can get more robust behavior from

10:35.360 --> 10:43.840
model and reason very well out of distribution. Yeah, because it's often said that deep learning

10:43.840 --> 10:49.040
models do not reason. And I think what people mean by that is that you get this phenomenon of

10:49.040 --> 10:55.920
shortcut learning and models do the right things for the wrong reasons. And it seems to me that

10:55.920 --> 11:01.920
what we're doing here is by imputing the kind of the structure of how to reason into the prompt,

11:01.920 --> 11:08.000
we're robustifying its behavior out of distribution. Yeah, well said. That's exactly right.

11:09.040 --> 11:15.600
Fascinating. So what do you think of this problem of shortcut learning in general? Because you know,

11:15.600 --> 11:21.520
like Melanie Mitchell said, there are two modes of understanding essentially. We have this anthropomorphic

11:21.520 --> 11:26.880
mode of understanding which is using causality and it's very sample efficient and we have a way of

11:26.880 --> 11:31.040
understanding the world. And we have a bit of an intuition that large language models

11:31.040 --> 11:37.280
don't reason the way that we do. But is it necessarily a problem? And is it cheating in

11:37.280 --> 11:41.840
your view using in-context learning? Or do you think that because we haven't had to train the

11:41.840 --> 11:51.040
model again, what's the problem, right? I mean, is it an issue? Yeah, I mean, I guess there's

11:51.040 --> 12:01.840
many forms of reasoning and algorithmic reasoning is only a subset of that. And I think if the model

12:01.840 --> 12:11.840
can output the reasoning process and show that the answer that arrives at is following the output

12:11.840 --> 12:18.880
of that process, then it's hard to argue that it's not doing reasoning. It might work differently

12:18.880 --> 12:28.000
under the hood, but it follows the similar process. Now, you know, some methods, you output a rationale,

12:28.000 --> 12:33.120
but the final answer is actually different from what the explanation suggests it should be.

12:34.160 --> 12:40.480
And maybe you can point to that and say, oh, the model isn't actually using this. It's just like

12:40.480 --> 12:45.760
giving you something that you asked for, but then the final answer is still using a shortcut.

12:46.240 --> 12:52.720
But with the algorithmic reasoning, we see that that's not the case. And so, yeah, the more you

12:52.720 --> 13:02.640
constrain things, maybe the more you remove shortcuts from the model. But an interesting

13:02.640 --> 13:08.000
question, I think, is you can get this behavior using in-context learning, which I think you,

13:08.800 --> 13:13.920
I suspect you can't really do from fine tuning or some sort of weight training.

13:14.880 --> 13:20.240
I think when you do that, you'll most likely just overfit on the training distribution.

13:20.880 --> 13:25.520
I wanted to ask you, you know, it's a bit of an open-ended question to say what's going on inside

13:25.520 --> 13:31.760
large language models. But what's so exciting to me is that you get all of this emergent strange

13:31.760 --> 13:35.920
behavior. No one would have imagined five years ago that we could do all of this prompt engineering

13:35.920 --> 13:40.880
on a kind of autoregressive language decoder. And the model is trained to do something really

13:40.880 --> 13:48.880
quite trivial, yet as a byproduct, all of this crazy stuff emerges in its internal representation.

13:48.880 --> 13:54.160
And all of this algorithmic reasoning capability seems to be like a side effect of that training.

13:55.520 --> 13:59.200
How does that even happen? Oh, that's a good question.

14:01.920 --> 14:07.600
It's possible that in order to fit that large training data set, the pre-training data set,

14:08.480 --> 14:17.360
you have to find regularities in content, I guess, that humans generate. And

14:18.960 --> 14:25.040
I think some of these abilities come out of those regularities that you learn. So the ability to

14:26.000 --> 14:37.440
refer to the pattern of a previous passage in context and, you know,

14:38.800 --> 14:43.680
see what the relationships are in that pattern and apply the same relationships when you input

14:45.680 --> 14:52.720
that circuit, I think, is just very useful as a way to summarize that large data set. And so

14:52.720 --> 15:02.000
because you're forced to compress all that data into a model set of weights, I think these regularities

15:03.120 --> 15:11.840
emerge somehow. I don't know exactly how. But I think, yeah, I mean, this is an interesting

15:11.840 --> 15:18.320
speculation because then you can say with much larger models where there is no capacity constraint

15:18.320 --> 15:27.920
at all, and you fit all the same data sets, it's not going to learn very interesting behavior

15:27.920 --> 15:33.680
because it's able to just fit the model without capturing the underlying patterns.

15:35.440 --> 15:41.120
And maybe that's why you actually need more training data and training longer rather than

15:41.120 --> 15:47.520
like the optimal scaling is not right now in the model size, the amount of data. Because you want

15:47.520 --> 15:54.640
like the right bottleneck for your representation. And I think maybe that's where these

15:56.960 --> 16:02.400
emergent capabilities are coming from. Interesting. Yeah. And also the amount of training as well as

16:02.400 --> 16:10.160
the amount of data. I wondered what is your intuition on the practical kind of computational

16:10.160 --> 16:14.400
limitations of large language models? So at the moment, they're trained to transform us.

16:14.480 --> 16:20.320
And there have been some pretty cool critiques of connectionism by Fodor and a few other people.

16:20.320 --> 16:25.920
And it's basically along the lines of neural networks can't represent infinite objects,

16:25.920 --> 16:30.720
which kind of distinguishes them from Turing machines. So they can't compute the nth digit of

16:30.720 --> 16:35.520
time is a fairly good example. But the amazing thing is that we're doing all of this stuff with

16:35.520 --> 16:40.800
algorithmic reasoning. And we haven't found the ceiling yet, it's just getting better and better.

16:40.800 --> 16:47.760
And I think it's almost creating this. Well, I mean, I'm becoming quite hopeful,

16:47.760 --> 16:51.840
actually, because I don't know where the limit is, but I suspect there is a limit quite soon.

16:51.840 --> 16:54.960
How do you think about what the realistic computational limits are?

16:56.640 --> 17:01.920
I think the fact that now we have in context learning is interesting because

17:02.880 --> 17:11.760
that allows us to have, I guess, adaptive amount of computation. And so if you have,

17:11.760 --> 17:18.960
let's say you have infinite context length, then you can sort of maybe do infinite computation

17:18.960 --> 17:26.480
in that case. Now, is infinite context length possible? Probably not. But then you can find

17:26.560 --> 17:32.080
clever ways to distill that information. You can find clever attention mechanisms.

17:32.880 --> 17:40.160
And so I think maybe there's a computational limit, but you can always find new ideas that

17:40.880 --> 17:47.600
make existing methods more efficient. And so, yeah, I have no idea when you would hit that

17:47.600 --> 17:53.040
limit, but it's probably very far into the future. Amazing. And so, Hattie, we're here in

17:53.120 --> 17:57.440
Europe. What have you taken from the conference so far? And what are you most excited about,

17:57.440 --> 18:06.320
you know, going forward? Yeah, I don't know, because I haven't checked out the posters

18:06.960 --> 18:18.160
very much yet. But I'm excited for the Math AI workshop, which is many other papers exploring

18:18.160 --> 18:27.040
this idea of doing math with language models. And yeah, there's some, you know, very impressive

18:27.040 --> 18:34.160
work there. So I'm excited for that. I'm excited to meet people and see what they're thinking about.

18:35.040 --> 18:43.200
And maybe get some ideas for what to work on next. Yeah, I'm also really interested in the

18:43.200 --> 18:48.800
math stuff I spent about an hour speaking with Marcus, is it Marcus Barb, who works under Christian

18:48.800 --> 18:54.320
Sagadia, I think he's in your group. Yeah, they're doing some, that's right, they're doing some

18:54.320 --> 18:59.920
really interesting stuff with basically doing mathematical conjecturing, you know, like representing

18:59.920 --> 19:04.560
mathematical expressions in large language models and being able to generate new ones.

19:04.560 --> 19:08.640
It's the kind of thing that you, again, would think was science fiction five years ago and like,

19:08.640 --> 19:14.400
remarkably, it works. Exactly. And then by the way, with the mathematical conjecturing, Marcus was

19:14.400 --> 19:19.600
saying that, unlike with large language models, it only has to be right one in 100 times because

19:19.600 --> 19:25.600
they can formally verify it. So it's almost like the bar is actually lower in that sense.

19:26.240 --> 19:34.160
Right. Yeah, I mean, that's where the language models informal reasonability is really useful.

19:34.880 --> 19:40.880
Right. Yeah, like the pattern actually is actually useful in a lot of cases.

19:40.880 --> 19:44.960
That's really cool. Cool. Well, actually, this has been amazing. Thank you so much for coming

19:44.960 --> 19:49.120
on the show and telling us about your research. Oh, thanks for having me. Yeah, it was fun.

19:51.760 --> 19:58.320
Looking beautiful. So Marcus, it's so nice to meet you. Can you introduce yourself?

19:59.040 --> 20:04.880
Hi, I'm Marcus. I work for Google Research together with Christian Segedy and the

20:04.880 --> 20:10.080
Antiformer team. Generally, we're working on trying to solve math,

20:12.560 --> 20:19.840
basically by trying to translate natural language mathematics into formal mathematics and

20:20.720 --> 20:23.440
in these formal representations of mathematics, we can

20:23.600 --> 20:30.480
check proofs and use that as a feedback signal for the understanding of mathematics.

20:31.120 --> 20:35.280
And most recently, I've been working on long context models like the memorizing transformer,

20:36.000 --> 20:43.760
trying to get these models, like makes models sensitive to the exact definitions and other

20:43.760 --> 20:51.360
lemmas that they might use for your improving. And that's an ongoing effort, hopefully more

20:51.360 --> 20:55.120
available soon. Can I just say, I'm so jealous that you work with Christian.

20:57.360 --> 21:01.120
I mean, folks will remember that we had a conversation with Christian. I think it was

21:01.120 --> 21:06.960
about 18 months ago. It is one of my favorite episodes of MLST. So you're a very lucky man indeed.

21:06.960 --> 21:13.520
Yes. Yes. It's great to work with Christian. It's a lot of fun. Amazing.

