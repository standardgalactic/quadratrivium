1
00:00:00,000 --> 00:00:05,280
Coming up later in today's presentation, I'm wondering at what point we're just developing

2
00:00:06,160 --> 00:00:11,040
complex math models to explain complex math models and we really haven't, you know,

3
00:00:11,040 --> 00:00:16,720
made much progress along the interpretability axis. So you have something you don't understand

4
00:00:16,720 --> 00:00:21,760
and you explain it with something you don't understand? If I have if I have some general

5
00:00:21,760 --> 00:00:26,320
formula, just some very general formula, and then I go in there and I go, you know what,

6
00:00:26,320 --> 00:00:34,240
this formula has five parameters. And if I make this 1.75 and that one one-third and this one two

7
00:00:34,240 --> 00:00:41,520
and that one zero, and I call this the Megatron activation potential, and I go and write a paper

8
00:00:41,520 --> 00:00:47,040
about it, that's really just an arbitrary selection of a bunch of numbers. And then you gave it a

9
00:00:47,040 --> 00:00:52,160
fancy mathematical passport and you got it published in some journal. And now everybody

10
00:00:52,160 --> 00:00:57,440
has to memorize that as you know, the Megatron potential and kind of learn about it. And that's

11
00:00:57,440 --> 00:01:08,080
a lot of what's going on right now is that it's really just a bunch of hacking.

12
00:01:14,960 --> 00:01:19,760
Welcome back to Street Talk. Today, we're going to be talking about interpretable machine learning.

13
00:01:19,760 --> 00:01:27,120
Enjoy. Interpretability has become one of the most important topics in machine learning.

14
00:01:27,120 --> 00:01:32,560
And it's something that every data scientist needs to be familiar with. For hundreds of years,

15
00:01:32,560 --> 00:01:39,280
we've had simple interpretable models like linear regression and rules-based systems.

16
00:01:40,320 --> 00:01:47,360
But in recent years, there's obviously been a huge rise in more complex, bigger, nonlinear models.

17
00:01:48,320 --> 00:01:55,440
And of course, predictions from these models are not always so easy to explain. So as we start to use

18
00:01:55,440 --> 00:02:03,360
these more powerful, nonlinear models to actually make decisions on real world matters, then it's

19
00:02:03,360 --> 00:02:09,520
inevitable that our attention must now turn to interpretability and explainability. When I

20
00:02:09,520 --> 00:02:15,040
first started learning about machine learning algorithms, I was told they could be dangerous.

21
00:02:15,120 --> 00:02:22,800
They were hard to understand. They were black boxes. But as Christoph lays out, it turns out

22
00:02:22,800 --> 00:02:27,440
there is a whole plethora of techniques out there to explain why a model made a certain

23
00:02:27,440 --> 00:02:33,120
prediction. Some models like low-dimensional linear regression are intrinsically interpretable.

24
00:02:33,920 --> 00:02:39,120
You can just look at the model coefficients and that tells you exactly how the model is working

25
00:02:39,120 --> 00:02:44,880
under the hood. Then there is a whole suite of methods that will actually work with any ML model.

26
00:02:45,280 --> 00:02:50,640
Like training a local surrogate or a global surrogate. There's also Shapley values,

27
00:02:50,640 --> 00:02:57,440
which is a really cool technique that allows you to distribute blame for the prediction amongst

28
00:02:57,440 --> 00:03:03,840
the input features in a really theoretically sound, really principled way. And then there are domain

29
00:03:03,840 --> 00:03:09,840
specific methods. For example, to explain image models, you can try to highlight the most relevant

30
00:03:09,840 --> 00:03:16,640
parts of an input image by making saliency maps. And there's more. You can look at things like

31
00:03:16,640 --> 00:03:23,040
example-based explanations where you try to find the smallest change in the input data that would

32
00:03:23,040 --> 00:03:29,920
cause the output prediction to change. So maybe with this awesome new interpretability toolkit,

33
00:03:29,920 --> 00:03:37,120
we can start to dispel that myth that machine learning models are all just black boxes that

34
00:03:37,120 --> 00:03:43,680
can't be understood and can't be trusted. Christoph Molnar is one of the most important people

35
00:03:43,680 --> 00:03:49,760
in the interpretable machine learning space. In 2018, he released his magnum opus,

36
00:03:49,760 --> 00:03:55,440
interpretable machine learning, a guide for making black box models explainable.

37
00:03:55,440 --> 00:04:00,880
Interpretability is often a deciding factor when a machine learning model is used in a product,

38
00:04:00,880 --> 00:04:07,040
a decision process or research. Interpretability methods can be used to discover knowledge,

39
00:04:07,120 --> 00:04:14,480
to debug or justify a model and its predictions, to control and improve the model, to reason about

40
00:04:14,480 --> 00:04:20,400
potential biases in the model, as well as increase the societal acceptance of models.

41
00:04:21,040 --> 00:04:27,200
But interpretability methods can be quite esoteric. They add an additional layer of complexity

42
00:04:27,200 --> 00:04:32,640
and the potential pitfalls require expert understanding. Machine learning models are

43
00:04:32,640 --> 00:04:38,560
inherently less interpretable than classical statistical models, but typically they have a

44
00:04:38,560 --> 00:04:43,440
better predictive performance and that's because of their ability to handle nonlinear

45
00:04:43,440 --> 00:04:49,760
relationships and also higher order feature interactions automatically. But do we have

46
00:04:49,760 --> 00:04:56,080
to suffer this implicit trade-off between the complexity of a model and the lack of our ability

47
00:04:56,080 --> 00:05:02,160
to understand it? Simplistic model approximations can often mask important information and be

48
00:05:02,160 --> 00:05:08,640
misleading as a result. In classical statistics there's an entire field called model diagnostics

49
00:05:08,640 --> 00:05:14,560
to do exactly this, to check that assumptions and simplifications have not been violated.

50
00:05:14,560 --> 00:05:18,240
This is something that does not yet exist in interpretable machine learning.

51
00:05:18,880 --> 00:05:24,800
Interpretability has exploded and matured in the last few years, in particular since the

52
00:05:24,800 --> 00:05:31,120
deep learning revolution. We now have a better understanding of the weaknesses and strengths

53
00:05:31,200 --> 00:05:36,800
of interpretability methods. A growing number of techniques are available at our fingertips

54
00:05:36,800 --> 00:05:43,680
that can lead to the wrong conclusions if applied incorrectly. Is it even possible to

55
00:05:43,680 --> 00:05:49,600
understand complex models or even humans for that matter in any meaningful way?

56
00:05:50,320 --> 00:05:53,280
That is one of the questions that we're going to be discussing this evening.

57
00:05:54,320 --> 00:06:00,480
Molnar also recently released a couple of papers where he discusses some of the important pitfalls

58
00:06:00,480 --> 00:06:04,800
of interpretable machine learning methods. So some of the things that Christoph Molnar is

59
00:06:04,800 --> 00:06:10,800
really concerned about is the lack of statistical rigor in IML methods. Molnar used to be a statistician.

60
00:06:10,800 --> 00:06:17,680
Also he is exasperated with some of the misguided causal interpretations from some of these IML

61
00:06:17,680 --> 00:06:22,480
methods. He also points out feature dependence or situations where you have shared information

62
00:06:22,480 --> 00:06:27,200
between features. It completely breaks many of the IML methods and this is something that he

63
00:06:27,200 --> 00:06:33,600
focuses on a lot. He also focuses philosophically on the broader impact of interpretability and

64
00:06:34,480 --> 00:06:40,160
what interpretability even means frankly. It's a very nebulous term. So let's have a quick flick

65
00:06:40,160 --> 00:06:44,800
through this paper, interpretable machine learning, a brief history, state of the art and challenges

66
00:06:44,800 --> 00:06:48,880
and as well as pointing out some of the history of IML methods, we'll jump straight into one of

67
00:06:48,880 --> 00:06:54,160
the challenges which is feature dependence. Molnar points out that feature dependence makes

68
00:06:54,160 --> 00:06:58,960
attribution and extrapolation problematic. This is exactly what happens in partial dependency

69
00:06:58,960 --> 00:07:04,720
plots for example. We are basically extrapolating and we are creating fictitious data points that

70
00:07:04,720 --> 00:07:09,840
didn't really exist and these fictitious data points probably exist outside of the data distribution.

71
00:07:10,480 --> 00:07:17,360
So Molnar thinks that the models that we build should reflect the causal structure in the world

72
00:07:17,360 --> 00:07:22,960
but of course that is not really the case most of the time and he points out that statistical

73
00:07:22,960 --> 00:07:28,880
learning is just reflecting surface feature correlations not the true causal structure beneath

74
00:07:28,880 --> 00:07:34,240
the scenes. Causal structures would be more robust if we could actually capture them

75
00:07:34,240 --> 00:07:39,760
and the predicted performance and learning causal factors is a conflicting goal which I think not

76
00:07:39,760 --> 00:07:44,720
many people have thought about. So we need to think about when we can make causal interpretations

77
00:07:44,720 --> 00:07:49,520
and a lot of work is underway in this field but being completely frank this is very nascent. There's

78
00:07:49,520 --> 00:07:55,920
not really much out there at the moment. Molnar also points out this lack of statistical rigor

79
00:07:56,560 --> 00:08:01,120
having been a statistician himself. He was exasperated when he came into the IML field just

80
00:08:01,120 --> 00:08:06,720
to see that most IML methods do not even give you confidence estimates something which is

81
00:08:06,720 --> 00:08:12,080
completely standard in the statistical world. Models and explanations are computed from data

82
00:08:12,080 --> 00:08:17,360
which means they are subject to uncertainty but this is something which is just not captured

83
00:08:17,360 --> 00:08:22,640
using current methods. He says that we need to be making distributional and structural assumptions.

84
00:08:22,640 --> 00:08:29,280
He points out this risk of p-hacking something which is prevalence in the natural sciences.

85
00:08:29,280 --> 00:08:34,000
This is something that could be coming to the world of IML very soon if we don't start thinking

86
00:08:34,000 --> 00:08:39,520
about this more carefully. Molnar also points out that there is no accepted definition of

87
00:08:39,520 --> 00:08:45,920
interpretable machine learning methods so it's not entirely clear how we can compare IML methods to

88
00:08:45,920 --> 00:08:51,520
machine learning models. It's really easy to assess machine learning models because we have

89
00:08:51,520 --> 00:08:56,880
benchmarks and we have ground truth labels. Those benchmarks are fraught with problems as well

90
00:08:56,880 --> 00:09:02,800
but we can't really quantify how correct an explanation is and it doesn't really help that

91
00:09:02,800 --> 00:09:07,680
there's a taxonomy of interpretability methods. There are objective methods like sparsity and

92
00:09:07,680 --> 00:09:14,000
interaction strength and there are human-centered evaluations from domain experts or from lay people

93
00:09:14,000 --> 00:09:18,640
and quite often you need to have quite a lot of technical knowledge to even understand

94
00:09:18,640 --> 00:09:24,720
these assessments. He says that the setting of machine learning is too static. It doesn't reflect

95
00:09:24,720 --> 00:09:29,360
how these models are actually used in practice and I really love this idea of thinking about a

96
00:09:29,360 --> 00:09:34,320
process rather than thinking about just the model so he says we need to have a holistic view of the

97
00:09:34,320 --> 00:09:39,520
entire process. He thinks that we need to think about how we explain predictions to folks from

98
00:09:39,520 --> 00:09:45,760
diverse backgrounds, how we have interpretability at the societal level or at the institutional level

99
00:09:45,760 --> 00:09:49,920
thinking much more broadly than we are at the moment. He also thinks that we need to reach out

100
00:09:49,920 --> 00:09:55,280
to other disciplines for example psychologists and social scientists and he thinks that there's

101
00:09:55,280 --> 00:09:59,760
lots of rich knowledge in computer science and statistics that we're just not using yet.

102
00:09:59,760 --> 00:10:05,600
So in July of last year he also released this paper pitfalls to avoid when interpreting machine

103
00:10:05,600 --> 00:10:09,920
learning models. In this paper he points out that there's a growing number of techniques

104
00:10:09,920 --> 00:10:14,800
providing model interpretations but many will lead to the wrong conclusions if used incorrectly

105
00:10:14,800 --> 00:10:19,440
and he goes on to point out many of those pitfalls. For example the first one is

106
00:10:19,440 --> 00:10:24,160
assuming that the model generalizes well so assuming that the model has been fit correctly

107
00:10:24,160 --> 00:10:29,440
if the model is underfit or overfit then the interpretation method will perform badly as well

108
00:10:29,440 --> 00:10:35,680
and interpretation can only be as good as the model underlying it. So the next pitfall he points

109
00:10:35,680 --> 00:10:41,440
out is the unnecessary use of complex models which is to say the use of opaque or complex

110
00:10:41,440 --> 00:10:46,960
machine learning models when an interpretable model would have sufficed which is to say when

111
00:10:46,960 --> 00:10:51,600
the performance of an interpretable model is only negligibly worse than one of these black box

112
00:10:51,600 --> 00:10:56,640
models and to be honest this is something I see all the time I think the gratuitous use of complex

113
00:10:56,640 --> 00:11:00,880
machine learning models is something which is really serious. One of the things I don't like

114
00:11:00,880 --> 00:11:07,120
about machine learning is the laziness. I think we should always seek to understand and simplify

115
00:11:07,120 --> 00:11:11,120
problems wherever we can it's the same thing in software engineering. We should always be trying

116
00:11:11,120 --> 00:11:17,680
to create the most elegant and simple and maintainable solution. We shouldn't be trying to over

117
00:11:17,680 --> 00:11:22,800
complicate things and I think that's a very you know the kiss principle is very generalizable here.

118
00:11:23,440 --> 00:11:28,560
So he recommends to start with simple interpretable models like generalized linear models or lasso

119
00:11:28,560 --> 00:11:34,160
models or additive models decision trees or decision rules and gradually ratcheting up the

120
00:11:34,160 --> 00:11:40,480
complexity as required. So he also points out that ignoring feature dependence is super important

121
00:11:40,480 --> 00:11:45,200
right and this is a problem that many of the IML methods have so he gives an example of

122
00:11:45,200 --> 00:11:50,640
partial dependency plots where they extrapolate in areas where the model has little training data

123
00:11:50,640 --> 00:11:55,840
and it can cause misleading interpretations. So these perturbations produce artificial data

124
00:11:55,840 --> 00:12:00,800
points that are used for model predictions which in turn are aggregated to produce global

125
00:12:00,800 --> 00:12:05,760
interpretation so that's a big problem. Another thing he points out is confusing correlation

126
00:12:05,760 --> 00:12:10,720
with dependence so he gives an example here features with a Pearson correlation coefficient

127
00:12:10,720 --> 00:12:15,280
close to zero can still be dependent and cause misleading model interpretations

128
00:12:15,280 --> 00:12:20,160
while independence between two features implies that the Pearson correlation coefficient is zero

129
00:12:20,160 --> 00:12:25,600
the converse is generally false. So there's a pretty cool example here this is a couple of features

130
00:12:25,600 --> 00:12:31,200
that absolutely have a dependence on each other you can see it visualized here but you wouldn't

131
00:12:31,200 --> 00:12:35,360
know that if you looked at the Pearson correlation it would have said that it wasn't significant.

132
00:12:35,360 --> 00:12:40,960
Another one misleading effect due to interactions so there's a couple of things here there's the

133
00:12:40,960 --> 00:12:46,320
partial dependency plots on a couple of dependent features and then he's used a simulation to kind

134
00:12:46,400 --> 00:12:52,320
of trace all of these different features to see what the predicted label was and according to

135
00:12:52,320 --> 00:12:58,560
these IML methods there is actually no clear dependency between these features and the predicted

136
00:12:58,560 --> 00:13:02,640
outcome whereas you can see that that's just blatantly false. So something I've been meaning

137
00:13:02,640 --> 00:13:08,240
to do for more than a year now is to go through Molnar's interpretability book and to make some

138
00:13:08,240 --> 00:13:12,960
bite-sized videos on every single approach well Connor and I are actually going to do that over

139
00:13:12,960 --> 00:13:18,080
on Machine Learning Dojo with the first one next week on Shapley Values so make sure you

140
00:13:18,080 --> 00:13:23,680
subscribe to Dojo and check that out. Remember to like comment and subscribe we love reading your

141
00:13:23,680 --> 00:13:31,680
comments and we'll see you back next week. Welcome back to the Machine Learning Street Talk YouTube

142
00:13:31,680 --> 00:13:38,080
channel and podcast with my two compadres Connor Tan who runs the Thomas Bay's Appreciation Society

143
00:13:38,160 --> 00:13:46,320
and MIT PhD Dr Keith Duggar. Now they say that Germans are known for beer, sausages, precision

144
00:13:46,320 --> 00:13:52,960
and these days interpretable machine learning. We have an exemplar German on the show Christoph

145
00:13:52,960 --> 00:14:00,640
Molnar. Now Christoph made waves in the community when he released his Magnum Opus interpretable

146
00:14:00,640 --> 00:14:06,160
machine learning a guide for making black box models explainable. If a machine learning model

147
00:14:06,160 --> 00:14:11,840
performs well why don't we just trust the model and ignore why it made a certain decision? Well

148
00:14:11,840 --> 00:14:17,120
the problem is that a single metric such as classification accuracy is an incomplete description

149
00:14:17,120 --> 00:14:23,680
of most real-world tasks now as according to Doshi Values and Kim in 2017. In Christoph's book he

150
00:14:23,680 --> 00:14:28,960
introduces the importance of interpretability and reports an incredibly detailed taxonomy

151
00:14:28,960 --> 00:14:34,000
of interpretability methods and his style of writing is at times entertaining and entirely

152
00:14:34,000 --> 00:14:40,320
absent of hype and nonsense. He runs the gamut of interpretability models so for example model

153
00:14:40,320 --> 00:14:45,520
agnostic methods like Lyme and Shapley values. Example-based methods such as counterfactual

154
00:14:45,520 --> 00:14:50,560
examples and adversarial examples he motivates the importance of interpretability methods

155
00:14:50,560 --> 00:14:56,080
but he's also extremely transparent about its current weaknesses and pitfalls. He's currently

156
00:14:56,080 --> 00:15:01,920
finishing his PhD in interpretable machine learning at the Ludwig Maximilians University in Munich

157
00:15:01,920 --> 00:15:06,560
after getting a stats master's from the same institution. He's recently written several

158
00:15:06,560 --> 00:15:11,040
very interesting papers on interpretable machine learning for example pitfalls to avoid when

159
00:15:11,040 --> 00:15:16,880
interpreting machine learning models in July of 2020 where Christoph detailed several problematic

160
00:15:16,880 --> 00:15:21,760
model interpretations for example ignoring estimation uncertainty feature interactions

161
00:15:21,760 --> 00:15:26,640
or confusing correlations with dependence. More recently he published a paper called

162
00:15:26,640 --> 00:15:31,600
interpretable machine learning a brief history state of the art and challenges while he acknowledged

163
00:15:31,600 --> 00:15:36,480
that the field is maturing nicely. He also spoke about some of the serious challenges in IML methods

164
00:15:36,480 --> 00:15:41,200
such as the lack of statistical uncertainty, shared information between features, lack of a

165
00:15:41,200 --> 00:15:46,160
clear definition of interpretability and the need for a more holistic view. Christoph Mulner

166
00:15:46,160 --> 00:15:51,120
it's an absolute pleasure and welcome to the show. Thank you very much for the invitation glad to be here.

167
00:15:51,760 --> 00:15:56,000
You know Christoph I have to say I really enjoyed your book. I read this actually

168
00:15:56,000 --> 00:16:01,120
some months back in preparation for a completely different show. I loved how scientific it was

169
00:16:01,680 --> 00:16:06,880
you know it was it was very much laying out essentially a survey of the facts a lay of the

170
00:16:06,880 --> 00:16:12,800
land very objective evaluation. It had both the pros and cons you know of different approaches

171
00:16:12,800 --> 00:16:17,920
examples to make them you know more understandable so kudos to you. I thought it was a great book

172
00:16:17,920 --> 00:16:23,040
very enjoyable and very informative. I also loved how it lays out the beginning you know what the

173
00:16:24,080 --> 00:16:28,560
goals that we're trying to achieve with with interpretability are especially kind of the

174
00:16:28,560 --> 00:16:35,360
human goals right like what does it mean for an explanation to be good for people what kind of

175
00:16:35,360 --> 00:16:40,400
explanations do people like and sometimes there can be conflicting conflicting goals there and

176
00:16:40,400 --> 00:16:46,080
I think one thing that that I realized from reading your book is that that actually explanations can

177
00:16:46,080 --> 00:16:53,840
be deceptively good yeah like I think I think the the the sort of cognitive bias maybe that we have

178
00:16:53,840 --> 00:17:01,200
to look for contrastive explanations or counterfactual explanations like and principle it seems good

179
00:17:01,200 --> 00:17:08,560
it's kind of like you know I'm sorry we can't give you this loan you know well why not like why

180
00:17:08,560 --> 00:17:13,120
can't you give this loan well well we've detected really that you're a that you're a deadbeat what

181
00:17:13,120 --> 00:17:17,600
do you mean I'm a deadbeat yeah you know you never pay your bills well let's see why okay let's look

182
00:17:17,600 --> 00:17:22,160
through this and we find a decision tree here and and some big decision tree and we get to this one

183
00:17:22,160 --> 00:17:28,800
little point what says here you know you didn't pay this furniture bill back in 2018 you know if

184
00:17:28,800 --> 00:17:33,600
if only you'd have paid that furniture bill like we'd be able to give you the loan right but the

185
00:17:33,600 --> 00:17:38,320
truth isn't that simple like it's actually buried all throughout yeah throughout the decision tree

186
00:17:38,320 --> 00:17:44,320
right with so many contributing points yeah I think I like this chapter that you referenced

187
00:17:44,320 --> 00:17:49,520
was about like kind of from the social view or the human view what what what people like or

188
00:17:49,520 --> 00:17:55,360
prefer is explanations and the whole chapter is based on I forgot the title of the paper it's

189
00:17:55,360 --> 00:18:01,040
like from Miller about like what we can learn from the social sciences about what a good

190
00:18:01,040 --> 00:18:07,760
explanation is and was like a paper where I learned a lot and it was super interesting also to see

191
00:18:07,760 --> 00:18:11,840
how like what people think are good explanation as you mentioned they should be contrastive

192
00:18:11,840 --> 00:18:16,560
they should be short but they should also confirm to some prior knowledge that the people have

193
00:18:17,440 --> 00:18:22,320
and I mean like objectively a lot of those things might not like you wouldn't say these are good

194
00:18:22,320 --> 00:18:29,680
explanations in some sense like maybe maybe it's not good to give an explanation that fits with

195
00:18:29,680 --> 00:18:36,800
a prior knowledge because it's not the correct one maybe so it was quite interesting to learn

196
00:18:36,800 --> 00:18:41,520
and to think about like what's the human side of it that's a very cool part of your book I thought

197
00:18:41,520 --> 00:18:46,000
the fact that it's actually quite interesting thinking about what we really want out of an

198
00:18:46,000 --> 00:18:50,640
explanation I remember first of all looking at you know sharp values that are very fair and

199
00:18:50,640 --> 00:18:55,760
will distribute the blame equally amongst all the different relevant features and then you turn

200
00:18:55,760 --> 00:19:01,360
to something else like you know selective interpretations that in a way are way less

201
00:19:01,360 --> 00:19:06,320
good because they're kind of arbitrary they'll just select a few a few a subset of the features

202
00:19:06,320 --> 00:19:10,000
and give them all the blame but then it turns out that apparently that's what people actually

203
00:19:10,000 --> 00:19:16,160
want as a useful intuptable explanation yeah so as I see there's like many many dimensions of

204
00:19:16,160 --> 00:19:23,680
explainability or like what what can be a good explanation and one of these dimensions is maybe

205
00:19:23,680 --> 00:19:28,400
sparsity that you have a short explanation with just a few facts that's something that people

206
00:19:28,400 --> 00:19:34,800
prefer maybe but this might be in conflict with other dimensions of a good explanation

207
00:19:34,880 --> 00:19:40,000
which should be maybe that all causes should be addressed by the explanation that plays some

208
00:19:40,000 --> 00:19:46,240
role at least but this is of course in conflict with sparsity if you want this full attribution

209
00:19:46,240 --> 00:19:52,880
like you get with shepley values for example so I that's why I also think there's not like just

210
00:19:52,880 --> 00:19:58,320
one correct explanation but there's like many attributes or many dimensions on which you can

211
00:19:58,320 --> 00:20:03,520
judge explanations yeah I think this is one of the problems because even machine learning is really

212
00:20:03,520 --> 00:20:08,640
difficult right because we use benchmarks and benchmarks are just something that people have

213
00:20:08,640 --> 00:20:13,440
come up with but you say in your you know you talk about one of the problems being that there's no

214
00:20:13,440 --> 00:20:18,880
definition of IML methods to start with but at least in machine learning methods we have ground

215
00:20:18,880 --> 00:20:26,160
truth which is which is significantly better in a way but if we if we can't quantify how good

216
00:20:26,160 --> 00:20:31,200
an explanation is then where are we really because you talk about a kind of taxonomy of

217
00:20:31,200 --> 00:20:35,760
interpretability methods right you say that there are objective evaluations like sparsity and

218
00:20:35,760 --> 00:20:41,520
interaction strength and fidelity and humans human-centered evaluations you know which might

219
00:20:41,520 --> 00:20:47,200
come from domain experts or lay people so I suppose you're just hitting straight on the

220
00:20:47,200 --> 00:20:53,520
fact that this is actually quite nebulous isn't it yeah so yeah so in some sense like there's this

221
00:20:53,520 --> 00:20:59,040
big criticisms okay this is not scientific or not well defined at least what interpretability is

222
00:20:59,040 --> 00:21:05,680
how can we even do research in this area but I have a bit more relaxed view I mean otherwise I

223
00:21:05,680 --> 00:21:10,560
should have stopped writing the book before I really started so I kind of see like this

224
00:21:11,680 --> 00:21:17,200
endeavor of giving interpretability or bringing interpretability to machine learning it's more

225
00:21:17,200 --> 00:21:23,520
like a first of all it's just a keyword so it's it kind of bundles all the methods together

226
00:21:23,520 --> 00:21:31,120
that kind of aim to reduce this high-dimensional function to something well mostly it's something

227
00:21:31,120 --> 00:21:36,080
in a lower dimension so we kind of just do this mapping something gets lost in a way this is fine

228
00:21:36,080 --> 00:21:42,240
and it's I think part of science to find out like or to yeah some part of analysis to find out what

229
00:21:42,240 --> 00:21:49,840
part gets lost so when you for example look at just some feature importance values for example

230
00:21:49,840 --> 00:21:58,800
of course it's a summary of your model and a lot of information gets lost but I still think

231
00:21:59,600 --> 00:22:03,680
it's useful to have obviously so many people use it but it's useful to have these

232
00:22:04,400 --> 00:22:11,440
tools and we just have to understand what they do and how to interpret the results so how do

233
00:22:11,440 --> 00:22:16,640
you interpret when like the feature importance is zero of a feature could that be quite dangerous

234
00:22:16,640 --> 00:22:21,360
though because this you gave the example of random forests when you have a lot of shared

235
00:22:21,360 --> 00:22:27,360
information between the features it would actually tell you that these correlated features have a

236
00:22:27,360 --> 00:22:33,040
higher feature importance than you might otherwise expect so does this imply that we need to have

237
00:22:33,040 --> 00:22:37,680
very detailed knowledge of how we should how we should use this information that we get from

238
00:22:37,680 --> 00:22:43,200
IML methods yeah so it's also kind of the direction in which I write papers like this pitfalls

239
00:22:44,000 --> 00:22:49,280
to avoid and stuff like this so I think so these are just tools so they do something with the

240
00:22:49,280 --> 00:22:54,640
model a kind of distill some knowledge so for example for feature importance you kind of

241
00:22:56,160 --> 00:23:00,400
measure how well does your model perform and then you measure again after you

242
00:23:00,400 --> 00:23:05,440
shuffle one of the features and and then then you get something out of it so then we can ask

243
00:23:05,440 --> 00:23:11,440
questions is this interpretable or not and it's kind of well not so relevant the question because

244
00:23:11,440 --> 00:23:16,080
you just have to understand what what happens when you shuffle feature and one is for example

245
00:23:16,720 --> 00:23:24,800
you kind of break the association between the feature and the prediction because now it doesn't

246
00:23:24,800 --> 00:23:29,120
carry the information about the target anymore because you're shuffled in randomly in your model

247
00:23:29,840 --> 00:23:35,360
so you kind of this this feature importance now measures how much performance you lose because

248
00:23:35,440 --> 00:23:41,680
of this break of information but then you also when you think about this method and want to use

249
00:23:41,680 --> 00:23:46,320
it you also have to understand that this shuffling for example breaks also association with your

250
00:23:46,320 --> 00:23:51,120
other data feature like the features in your data so this is a limitation of the method and

251
00:23:52,000 --> 00:23:58,480
what I think is needed is that we understand in which way these methods break or which

252
00:23:58,480 --> 00:24:03,600
scenarios we're allowed to use them or how we are allowed to interpret them and I think the

253
00:24:03,600 --> 00:24:09,760
situation is kind of similar to statistics where you have these models and and then you

254
00:24:09,760 --> 00:24:15,360
interpret like the coefficients of your models you still like have to learn how you do the

255
00:24:15,360 --> 00:24:19,520
interpretation what are the assumptions that have to be met that you're allowed to do this

256
00:24:19,520 --> 00:24:25,920
interpretation and I think it's we're in a similar situation here with interpretability of machine

257
00:24:25,920 --> 00:24:31,440
learning and I'm glad you mentioned sort of the old school linear models as well as dimensionality

258
00:24:31,520 --> 00:24:37,200
in the thread because you make a very good point in the book which is look even these so-called

259
00:24:37,200 --> 00:24:42,720
intrinsically interpretable models are only interpretable up to a certain dimensionality

260
00:24:42,720 --> 00:24:48,000
and you know I have I have tons of experience with with multi-linear regression right and and I can

261
00:24:48,000 --> 00:24:54,160
guarantee that beyond a very small number of dimensions those coefficients are not interpretable

262
00:24:54,160 --> 00:24:59,040
because it starts to play a bunch of games where it's inflating one coefficient and another because

263
00:24:59,040 --> 00:25:03,440
their difference is important and you know whatever else is happening a lot of correlated

264
00:25:03,440 --> 00:25:08,560
structures are all essentially getting compressed into the small number of small number of weights

265
00:25:08,560 --> 00:25:14,080
right and so as the dimensionality goes up I would say like no model is is intrinsically

266
00:25:14,080 --> 00:25:18,960
interpretable same can be said of decision trees like anybody who's looked at a decision tree that's

267
00:25:18,960 --> 00:25:23,520
come from real data you're going to find out it's not interpretable it's like oh look at this you

268
00:25:23,520 --> 00:25:28,480
know market capitalization matters oh and it matters over here too and down here and and

269
00:25:28,480 --> 00:25:33,520
actually I have to go through five checks on market capitalization before I get down to this

270
00:25:33,520 --> 00:25:39,120
decision and maybe the features aren't that intuitive either and then you have to kind of

271
00:25:39,120 --> 00:25:46,400
like mentally stack up like until you get to the decision like five decisions and then we have a

272
00:25:46,400 --> 00:25:56,560
very complex rule that led you to the prediction yeah so I agree that there's it's not like I mean

273
00:25:56,640 --> 00:26:00,800
I have I have this distinction the book like interpretable models and not so interpretable

274
00:26:00,800 --> 00:26:07,680
models um but it's as you say it's like a gray like it's a scale really people could definitely

275
00:26:07,680 --> 00:26:13,120
overhype how interpretable these white box models are right whether it's linear models

276
00:26:13,120 --> 00:26:19,200
as I'm a I've worked with many physicists who uh have had guidelines that you should only ever use

277
00:26:19,200 --> 00:26:24,880
models like a decision tree because it's possible in theory to write down on a piece of paper exactly

278
00:26:24,880 --> 00:26:30,240
how the decision is made right yeah you can trace every decision but that's never actually useful

279
00:26:30,240 --> 00:26:35,440
in practice is it since when have you ever looked at a decision tree fitted to data there's any

280
00:26:35,440 --> 00:26:40,240
complexity and the fact that in theory it's possible to go and examine how it works yeah it's

281
00:26:40,240 --> 00:26:44,560
completely irrelevant in practice isn't it yeah I mean it can be useful to have like a short decision

282
00:26:44,560 --> 00:26:49,760
tree sometimes it but in practice it will not like give you probably the best predictions

283
00:26:50,720 --> 00:26:54,640
um but it might be useful sometimes to shorten it artificially so even like

284
00:26:54,640 --> 00:26:59,680
you're throwing away some some predictive accuracy um but you shorten it so you understand

285
00:26:59,680 --> 00:27:06,240
that somehow it's manageable you can have a look at it and and see what's going on well there's also

286
00:27:06,240 --> 00:27:10,800
you know the other issue is that as I was looking through a lot of the methods that you describe

287
00:27:10,800 --> 00:27:16,720
and you survey in your book you know some of them um are not simple I mean if you start looking at

288
00:27:16,720 --> 00:27:22,560
partial dependency plots and trying to explain what those are I mean you know you have to almost

289
00:27:22,560 --> 00:27:27,360
have a deep mathematical knowledge really to appreciate them in the first place so I'm wondering

290
00:27:27,360 --> 00:27:34,560
at what point we're just developing complex math models to explain complex math models and we really

291
00:27:34,560 --> 00:27:40,720
haven't you know made much progress along the interpretability axis yeah yeah it's true it's

292
00:27:40,720 --> 00:27:45,440
also like the criticism too like um so you have something you don't understand and you explain it

293
00:27:45,440 --> 00:27:52,640
with something you don't understand um I think some methods are complex um but for some at least

294
00:27:52,640 --> 00:27:58,800
there's some intuition how they work for partial dependence plot is kind of your this um intuition

295
00:27:58,800 --> 00:28:03,760
that you do some intervention on your more or intervention on your data so you replace all your

296
00:28:04,320 --> 00:28:10,080
like for one feature you replace all the values with one fixed value and kind of look at the

297
00:28:10,080 --> 00:28:14,800
average prediction that you get afterwards and do this for a lot of points and then you connect

298
00:28:14,800 --> 00:28:20,880
the points and you have this curve so kind of gives gives you the expected change over the feature

299
00:28:20,880 --> 00:28:27,120
range maybe there was already a bit complex I don't know um maybe I'm too deep into the method

300
00:28:27,120 --> 00:28:34,560
already um but yeah of course it's it's something additional people have to learn or if they agree

301
00:28:34,560 --> 00:28:41,440
to use it of course could I get a quick take from you on saliency maps as an example because you

302
00:28:41,440 --> 00:28:46,560
said in one of your youtube videos that saliency maps are glorified edge detectors they are not

303
00:28:46,560 --> 00:28:52,320
good explanation at all and I've noticed now that many machine learning platform providers are building

304
00:28:52,320 --> 00:28:57,760
these kind of um saliency maps into their models you know into their platforms and then it becomes a

305
00:28:57,760 --> 00:29:02,320
kind of box ticking exercise where you can say okay well yeah we've done interoperability now

306
00:29:02,320 --> 00:29:07,360
that's all you need to know and that really is quite a false sense of security isn't it

307
00:29:07,360 --> 00:29:12,880
it's funny you mentioned the saliency maps because I'm writing a book chapter about it and

308
00:29:12,880 --> 00:29:19,280
actually I'm I wanted to publish it today maybe I will or at least in the next few days um it has

309
00:29:19,280 --> 00:29:23,840
been a long time in the making and it was very very frustrating like by far the most frustrating

310
00:29:23,840 --> 00:29:30,160
chapter to write um number one reason is because there's so many methods out there uh reason number

311
00:29:30,160 --> 00:29:39,040
two is I I can't judge really or if they work and it seems like they mostly don't or it's it's

312
00:29:39,040 --> 00:29:44,800
still unclear like how you say you would judge that they work so they're like dozens of these

313
00:29:44,800 --> 00:29:51,360
like integrated gradients gradients deconfinate deep tailor decomposition layer-wise relevance

314
00:29:51,360 --> 00:29:59,040
propagation in 10 variants um so and then I mean you in the end you when you apply these methods on

315
00:29:59,040 --> 00:30:03,440
they are also like for image classification and you get these nice-looking images and some areas

316
00:30:03,440 --> 00:30:08,960
are highlighted some or not sometimes you can say okay this doesn't make sense at all um but if

317
00:30:09,840 --> 00:30:15,600
if it kind of makes sense then you maybe would be inclined to trust the method um but then there's

318
00:30:15,600 --> 00:30:21,440
this uh paper which is called um sanity checks for saliency maps and they kind of found out that

319
00:30:21,440 --> 00:30:28,720
they the most of the methods are very similar to edge detectors meaning that they are kind of

320
00:30:28,720 --> 00:30:34,960
insensitive to the model and the data which is very bad of course well if you change the model um

321
00:30:34,960 --> 00:30:41,360
the explanation should obviously change um could could you expand on that a little bit so you said

322
00:30:41,360 --> 00:30:47,040
it wasn't really reflection of the model or the data but what what would a perfect saliency map

323
00:30:47,040 --> 00:30:54,240
look like well I don't know myself actually so I mean the the ideas that you saw the basic idea

324
00:30:54,240 --> 00:30:59,360
of most of these methods is that you you have your class prediction or your class score and you

325
00:30:59,360 --> 00:31:06,560
want to back propagate it not you want to back propagate it to the original image so you look

326
00:31:06,560 --> 00:31:13,120
at the gradient um with respect to your input pixels and that's there's no not not one way to

327
00:31:13,120 --> 00:31:18,480
do this but there are many different ways so that's also why we have so many different methods

328
00:31:18,480 --> 00:31:26,000
and then they highlight which pixels were relevant for the classification um but yeah

329
00:31:26,640 --> 00:31:31,680
they these these methods they have like a lot of like issues for example there's the issue of

330
00:31:31,680 --> 00:31:37,440
saturation for example because of the real unit where you have flat parts of the gradient so if

331
00:31:37,440 --> 00:31:43,280
you pass the gradient through that then um your method would say that some some neuron might not

332
00:31:43,280 --> 00:31:49,680
be uh important at all and there's a lot of these little issues that these methods have um

333
00:31:50,800 --> 00:31:55,920
yeah so but but back to your question like I think that's also the issue that I don't

334
00:31:55,920 --> 00:32:00,480
wouldn't know how to answer I mean obviously it should be some area that should be highlighted on

335
00:32:00,480 --> 00:32:05,600
this aliens method was important for the for the neural network um but then again I don't know how

336
00:32:05,600 --> 00:32:11,760
the network decides so I couldn't like if I see an image I couldn't like highlight the part I mean

337
00:32:11,760 --> 00:32:16,880
I could highlight the part where I think the network should look but then again I mean there are

338
00:32:16,880 --> 00:32:22,240
lots of papers like the clever hands paper which saw like the reveal that there are some

339
00:32:23,360 --> 00:32:29,520
sometimes it would look at watermarks on on the photo um so these are like these things that we

340
00:32:29,520 --> 00:32:35,600
just don't know uh what the neural network basis this on have if I could take a stab at that answer

341
00:32:36,240 --> 00:32:42,640
for one I think just the idea of quote a saliency map is a problem like there isn't one

342
00:32:43,520 --> 00:32:47,760
map of of the importance of the pixels it's like they're they're operating on multiple

343
00:32:48,480 --> 00:32:52,720
multiple dimensions or at least sort of multiple feature sets it's like if you ask me to tell you

344
00:32:52,720 --> 00:32:58,320
know why is this image a dog you know well for one thing it's it's the overall shape you know it has

345
00:32:58,320 --> 00:33:04,720
four legs and you know two ears sticking out over here that's one saliency another is that

346
00:33:04,720 --> 00:33:09,040
it's it's got a certain color you know and it and it's coat and that's a that's a different

347
00:33:09,040 --> 00:33:14,000
concept of what's salient and another is that there's a frisbee flying at it and its mouth is open

348
00:33:14,000 --> 00:33:19,200
and it's about to catch it and I know dogs do that so they're kind of you know when your mind

349
00:33:19,200 --> 00:33:24,640
analyzes an image it breaks it down into these many large scale kind of structural features

350
00:33:24,640 --> 00:33:29,200
and I think that gets completely lost and most of the approach is the saliency maps this is

351
00:33:30,000 --> 00:33:36,480
really important point actually because if you're just looking at the pixels on this kind of 2d

352
00:33:36,480 --> 00:33:42,240
planar manifold that's only a very it is quite literally a surface view and I think Christoph you

353
00:33:42,240 --> 00:33:48,880
said that there are all sorts of causal structures and even in the model itself right there are

354
00:33:49,520 --> 00:33:54,560
these entangled neurons and surely that's giving me more insight into what's actually happening

355
00:33:54,560 --> 00:33:58,160
just seeing a bunch of pixels and the other thing is that these models that they are completely

356
00:33:58,160 --> 00:34:02,400
lacking in robustness so probably if you changed a few of the wrong pixels your saliency map has

357
00:34:02,400 --> 00:34:09,520
just got completely broken right yeah so um but in in that vein some of these feature visualization

358
00:34:09,520 --> 00:34:14,960
techniques you know like the deep dream type stuff maybe maybe that's a better way of of

359
00:34:14,960 --> 00:34:19,920
interpreting these models yeah um so like for the one point you mentioned about the adversarial

360
00:34:19,920 --> 00:34:27,360
examples so there's also a paper I forgot to title again um which that manipulated neural networks

361
00:34:27,360 --> 00:34:32,480
so they would give the same prediction for all the images but different explanation like different

362
00:34:32,480 --> 00:34:39,200
saliency maps so this is perfectly possible to create different explanations um for these saliency

363
00:34:39,200 --> 00:34:46,320
maps um but but keeping the model like at least for the predictions the same there's another criticism

364
00:34:46,320 --> 00:34:50,320
you can throw at saliency maps where they they can be quite deceiving you think they're useful and

365
00:34:50,320 --> 00:34:54,320
they turn out not to be useful yeah there's a classic example of looking at you know comparing

366
00:34:54,320 --> 00:34:59,040
a dog to a wolf and sometimes you see it's looking at the snow in the background and that's helpful

367
00:34:59,040 --> 00:35:03,520
sometimes it highlights the animal and you think okay I understand it's looking at the face that's

368
00:35:03,520 --> 00:35:07,920
why it thinks it's a dog because it's in the face and then you look at the predicted class for something

369
00:35:07,920 --> 00:35:13,120
else like you know a cat or a frisbee or a house or a boat and it highlights the face as well yeah

370
00:35:13,120 --> 00:35:17,120
so the saliency map for all these different classes looks the same and when you realize that

371
00:35:17,120 --> 00:35:22,640
you realize this this saliency map hasn't actually told you anything about why it's gone for one class

372
00:35:22,640 --> 00:35:26,480
versus the other all it said is that it's just highlighted the thing in the middle of the picture

373
00:35:26,480 --> 00:35:31,680
yeah I think that's especially also when when you look at images you know like we're very good

374
00:35:32,640 --> 00:35:36,720
with images yeah like we were very quick to see what's happening on a scene and such

375
00:35:36,720 --> 00:35:41,360
so I think we're also very quick to make judgments oh yeah this makes sense this doesn't make sense

376
00:35:41,360 --> 00:35:46,400
it's more difficult to interpret like if you have like a graph and there's like things going on inside

377
00:35:46,400 --> 00:35:51,760
you have to like now understand what the method does and stuff like this but for an image like a

378
00:35:51,760 --> 00:35:57,200
heatmap IR this area is highlighted makes sense case closed I like the method

379
00:35:59,120 --> 00:36:05,680
yeah and that gets exactly back to the deceptively deceptively good explanations problem and explaining

380
00:36:05,680 --> 00:36:10,640
complex things with complex things we don't understand you know so I think a lot of people

381
00:36:10,640 --> 00:36:14,800
if they looked at it and again one of the points of this interpretability is really the social

382
00:36:14,800 --> 00:36:21,200
aspects of it right like being able to convince people to be at ease with machine learning models

383
00:36:21,280 --> 00:36:27,920
or to accept the results of of a machine learned you know decision process and I think if somebody

384
00:36:27,920 --> 00:36:32,320
looks at an image of a dog you know they have no problem understanding that but if you showed them a

385
00:36:32,320 --> 00:36:39,200
bunch of salience maps or or any of the other sort of you know feature projections if you will

386
00:36:39,200 --> 00:36:44,880
like you said it takes a lot of deep understanding to understand those whereas the image is kind of

387
00:36:44,880 --> 00:36:50,560
immediately obvious I think two of the main themes that you touch on is we'll get to the

388
00:36:51,360 --> 00:36:56,640
to the probabilistic stuff the the Judeo pearls stuff in in a minute but I think the main issue

389
00:36:56,640 --> 00:37:02,560
that you point out is feature dependence okay and and you say that when you have feature dependence

390
00:37:02,560 --> 00:37:07,840
it makes attribution and extrapolation problematic so a dependence just means that you got correlated

391
00:37:07,840 --> 00:37:12,560
or shared information between your features right so you say that in feature permutation methods

392
00:37:13,520 --> 00:37:17,200
these things basically break everything when you have the shared information

393
00:37:17,200 --> 00:37:22,400
and the extrapolated data points are no longer in the distribution and you say that there are

394
00:37:22,400 --> 00:37:26,960
conditional permutation schemes you know that try and and maintain that joint distribution but

395
00:37:26,960 --> 00:37:31,040
those things sometimes make it even worse right so do you think that's one of the most important

396
00:37:31,040 --> 00:37:37,120
things that people should think about when using iml methods yeah at least so that's at least like a

397
00:37:37,120 --> 00:37:45,440
very um deep issue I would say which is inherent in in most of the model agnostic methods where you

398
00:37:45,440 --> 00:37:51,280
manipulate your data see what how the model prediction changes and then create your explanations

399
00:37:51,280 --> 00:37:56,560
out of this sort of select the shadowy value line partial dependence plot feature importance

400
00:37:56,560 --> 00:38:03,600
they all work with this mechanism of manipulation of the data prediction and then kind of aggregating

401
00:38:03,600 --> 00:38:12,880
the results and most manipulations happen in isolation so that you for example when you

402
00:38:14,640 --> 00:38:19,840
for feature importance you can meet one of the features as I like said before and then

403
00:38:21,120 --> 00:38:24,240
well you break the association of target but also a few other features

404
00:38:24,960 --> 00:38:30,800
but similar things happen if you use lines or you kind of replace parts of your image

405
00:38:30,800 --> 00:38:35,440
but then again you also have to replace it with something like which is I think in line the

406
00:38:35,440 --> 00:38:41,120
defaults with just a gray image and then of course it's not like it's outside of your data

407
00:38:41,120 --> 00:38:46,640
distribution subtly because your network was not confronted with like these patchy images before

408
00:38:46,640 --> 00:38:52,080
they had like just normal photographs usually and depends on your neural network but I mean you

409
00:38:52,080 --> 00:38:57,680
certainly didn't train it on on images where parts were grayed out so it's pretty likely what the

410
00:38:57,680 --> 00:39:03,760
model should predict and what will predict at this point but but you use these images to

411
00:39:04,480 --> 00:39:08,960
create your data set like you send it through the neural network you get predictions and

412
00:39:08,960 --> 00:39:15,600
you kind of aggregate from this your explanation but you left your data distribution and your model

413
00:39:15,600 --> 00:39:21,920
can do anything then and the hope is that it doesn't do anything crazy but yeah you don't know

414
00:39:22,640 --> 00:39:28,800
like it like a simple example from the medical field would be that you know height and weight

415
00:39:28,800 --> 00:39:35,920
are highly correlated right and and on the other hand sort of the ratio or some relationship between

416
00:39:35,920 --> 00:39:41,520
your your weight to your height that actually has very important medical consequences right that's

417
00:39:41,520 --> 00:39:47,120
that's the measure of of health and so if I were to sit there and just permute say the height index

418
00:39:47,120 --> 00:39:50,800
and create a whole bunch of people that had all these bizarre combinations of

419
00:39:51,360 --> 00:39:55,920
of height and weight you know first of all those don't even probably exist in the data set and the

420
00:39:55,920 --> 00:40:02,800
ones that do exist in the data set probably had some medical issues right yeah well you actually

421
00:40:02,800 --> 00:40:06,560
gave a similar example I think you gave the example Christoph of a baby that earns a hundred

422
00:40:06,560 --> 00:40:11,040
thousand dollars a year which is which is insane but when you talk about something like lime

423
00:40:11,040 --> 00:40:16,400
maybe that's different because the cnn the what you know it shines a flashlight over the input

424
00:40:16,400 --> 00:40:21,440
space in it and it's a kind of local method so in some sense you could argue that it doesn't

425
00:40:21,440 --> 00:40:25,280
matter that you've grayed out all this other stuff because if the model was sufficiently well

426
00:40:25,280 --> 00:40:29,440
trained in the first place it should hopefully learn to ignore the background or is that just

427
00:40:29,440 --> 00:40:33,920
wishful thinking that's an interesting thought I haven't thought about it because like the property

428
00:40:33,920 --> 00:40:41,120
of like that you have these filters that trust a wander over the image yeah maybe it would make

429
00:40:41,120 --> 00:40:47,200
it more robust for these kind of interventions that we do when we create these images with lime

430
00:40:47,200 --> 00:40:54,720
and shetley um yeah I haven't thought about it it could be uh well we've mentioned all of these

431
00:40:54,720 --> 00:41:00,400
ways in which interpretability methods can go wrong right how the model might not be a realistic

432
00:41:00,400 --> 00:41:04,880
the interpretability model might not be a good approximation to the actual ml model

433
00:41:04,880 --> 00:41:09,280
so some people a bit controversially perhaps take the idea and remember that and say you're just

434
00:41:09,280 --> 00:41:14,080
working up completely the wrong tree and you should give up using interpretability

435
00:41:14,080 --> 00:41:19,520
interpretability methods to explain these black boxes this dish them instead just use an interpretable

436
00:41:19,520 --> 00:41:24,960
model to begin with use a white box model I think there's an example um from compass in the US which

437
00:41:24,960 --> 00:41:30,880
is that model to predict reoffending and I think quite famously there was a investigative journalists

438
00:41:30,880 --> 00:41:36,320
that tried to interpret this model it was a black box model because it's proprietary right it's a

439
00:41:36,320 --> 00:41:42,320
trade secret and they they fit it a proxy model a kind of a linear model and they made a report

440
00:41:42,320 --> 00:41:47,120
saying okay we think your model is racist because it looks like it's it's taking race as a factor

441
00:41:47,680 --> 00:41:51,680
and then some further work was done and they came back and said well actually you've just used a

442
00:41:51,680 --> 00:41:56,240
uh interpretability model that doesn't really fit our model very well you've made some assumptions

443
00:41:56,240 --> 00:42:00,320
that don't hold if you use a different interpretability model you get a completely different answer

444
00:42:00,320 --> 00:42:05,040
that it doesn't use race at all as a factor and so you've got to kind of a you've got to a wrong

445
00:42:05,040 --> 00:42:10,640
assumption by using a bad interpretability model and I think they were saying that this model

446
00:42:10,640 --> 00:42:15,680
instead you could get just just as good a model of reoffending with like three FL statements you

447
00:42:15,680 --> 00:42:22,160
know ditching this massive complex 100 and something features and just use three FL statements

448
00:42:22,160 --> 00:42:27,600
based on I think age and reoffending so is that was that what we should do should we just drop

449
00:42:27,600 --> 00:42:32,560
these methods and start using white boxes instead so I mean like one one thing to mention here is

450
00:42:32,560 --> 00:42:38,320
that a white box is very soon also like a gray or black box if you add interactions if you have

451
00:42:38,320 --> 00:42:44,160
many features and so on but putting that aside I would agree if you're in first place like that you

452
00:42:44,160 --> 00:42:49,840
say you should start with like a white box so if you start modeling then then you then you should

453
00:42:51,040 --> 00:42:55,360
consider these first like maybe they already solved your problem then it's perfect and you have a model

454
00:42:55,360 --> 00:43:04,880
that is quite I mean stable it's interpretable I think that would be great but then I think the

455
00:43:04,880 --> 00:43:09,520
next step would be to see like what like a black box or a machine learning model would give you in

456
00:43:09,520 --> 00:43:15,440
terms of performance and then maybe if you see the gap is really big then maybe you can try

457
00:43:15,440 --> 00:43:20,800
some feature engineering and close the gap maybe from the interpretative model to the machine learning

458
00:43:20,800 --> 00:43:27,200
model but then you're probably already infusing some features that are not so interpretable

459
00:43:27,200 --> 00:43:32,720
or maybe if you're using a linear regression model you're maybe using then splines and

460
00:43:32,720 --> 00:43:38,800
interactions so you're already moving towards more complex models usually but then if you still have

461
00:43:38,800 --> 00:43:46,240
a gap then I think you have to decide is the gap and predictive performance like worth changing to

462
00:43:46,240 --> 00:43:52,480
a black box model so I think that's your decision will be different in many cases

463
00:43:53,760 --> 00:43:56,720
as you relates back to the point you made on your paper about criticisms of using

464
00:43:57,360 --> 00:44:01,200
interpretable machine learning models that some people leap straight away at using an overly

465
00:44:01,200 --> 00:44:06,640
complex model and sometimes depending on the situation sometimes you know a linear model can

466
00:44:06,640 --> 00:44:11,360
do just as well and have all these advantages it's so much easier to explain do you have a

467
00:44:11,360 --> 00:44:17,760
philosophy from a high level here right because if it if it were a human if it were an airplane

468
00:44:17,760 --> 00:44:24,880
pilot we don't really understand how the brain works right we would just test the pilot you've

469
00:44:24,880 --> 00:44:31,680
got to fly the plane for 10 000 hours and if you don't crash then we'll let you fly so we don't

470
00:44:31,680 --> 00:44:35,520
really seek to understand how his or her brain works but with machine learning models there's

471
00:44:35,520 --> 00:44:40,720
this continuum right so if you use these complex black box models the predictive performance is

472
00:44:40,720 --> 00:44:46,160
usually better but you're trading off understandability and assuming those things are completely

473
00:44:46,160 --> 00:44:50,640
mutually exclusive what kind of decision process do you go through when you select these models

474
00:44:50,640 --> 00:44:55,600
but by the way with machine learning right the reason why we use machine learning is because

475
00:44:55,600 --> 00:45:02,640
we don't understand how to do something explicitly yeah is that a fair statement um yeah I would say

476
00:45:02,880 --> 00:45:08,960
um when when it ate us like so high-dimensional so complex many interactions and so on

477
00:45:10,240 --> 00:45:16,000
that your simple models don't cover the complex cannot cover the complexity I think then you

478
00:45:16,000 --> 00:45:22,560
need machine learning would you rather understand exactly how the plane worked or would you

479
00:45:22,560 --> 00:45:26,480
rather I mean if I was saying to you you can go and fly in a plane would you rather that you

480
00:45:26,480 --> 00:45:31,200
understood how the plane worked or would you rather that the plane was tested why not both

481
00:45:32,000 --> 00:45:40,160
so um I think we can do both it's to some degree so um of course with black box model we don't

482
00:45:40,160 --> 00:45:46,000
exactly understand how they work um but in comparison to a pilot we can test them for

483
00:45:46,000 --> 00:45:52,720
three more or less um so because I mean maybe it's not as good as an interpretable model

484
00:45:53,520 --> 00:45:59,520
but we still can use a lot of methods to at least approximate and and try to understand a few

485
00:45:59,520 --> 00:46:05,840
properties of this model so I think we are even in a situation where we don't have like these

486
00:46:05,840 --> 00:46:13,280
complete like A or B decisions but we can have so if if the machine learning works much better

487
00:46:13,280 --> 00:46:19,280
and it's like really robustly tested with lots of different data I would prefer machine learning

488
00:46:19,280 --> 00:46:28,480
model I guess um but then I would also want to like people to to apply all these methods that

489
00:46:28,480 --> 00:46:32,720
are available even if they are not perfect but still they give you something they give you some

490
00:46:32,720 --> 00:46:39,920
insights so yeah and I think so Tim one answer to your question is that a lot of people's response

491
00:46:40,640 --> 00:46:44,960
here and kind of demanding interpretability and having concerns about machine learning

492
00:46:44,960 --> 00:46:49,600
it all comes down to generalizability and we've seen through using machine learning

493
00:46:50,160 --> 00:46:57,520
that it breaks down in ways that that we don't like like for example sure maybe the soap dispenser

494
00:46:57,520 --> 00:47:04,320
you know is really great at dispensing it dispensing soap you know 87 percent of time but

495
00:47:04,320 --> 00:47:10,640
it just so happens to kind of be a race sensitive soap dispenser and just doesn't give any soap to

496
00:47:10,640 --> 00:47:15,200
people with a certain skin color like we've kind of decided is a society that are that there are

497
00:47:15,200 --> 00:47:21,360
certain generalizations or certain dimensions along which our models just have to perform

498
00:47:21,360 --> 00:47:26,720
and also because a lot of these these things that break machine learning models are things

499
00:47:26,720 --> 00:47:32,640
that happen quite quite regularly in the real world it's like a pilot you know a human being

500
00:47:32,640 --> 00:47:38,160
pilot flying around if he looks down at the ground and sees a hot air balloon with a big

501
00:47:38,160 --> 00:47:43,120
smiley face on it he's not going to crash the plane he's just going to be like oh yeah I forgot

502
00:47:43,120 --> 00:47:48,320
about the uh the hot air balloon contest that's going on today where's a machine learning model

503
00:47:48,320 --> 00:47:53,440
if it looks out a camera and sees something with a particular shape of lightning bolt you know it

504
00:47:53,440 --> 00:47:57,840
might just decide it's time to like dive for the ground right and crash the plane like that's

505
00:47:57,840 --> 00:48:03,040
sort of what these adversarial examples kind of show and I think that's why people are really

506
00:48:03,040 --> 00:48:09,280
hungering for human understandable explanations because still to this day the human brain

507
00:48:10,000 --> 00:48:16,960
is the only AGI really that that we have around yeah but deep learning models that they they

508
00:48:16,960 --> 00:48:22,960
essentially memorize lots and lots of things and they have this sparse coding so in a way it's just

509
00:48:22,960 --> 00:48:27,360
like the white box model even if we use interpretability methods we could enumerate all of the

510
00:48:27,360 --> 00:48:33,040
things that they are learning and one of those things might be a sensitivity or lack of sensitivity

511
00:48:33,040 --> 00:48:38,160
to hot air balloons or smiley faces on but even if we could enumerate all the things that they are

512
00:48:38,160 --> 00:48:43,440
learning we wouldn't understand that either in the same way we don't understand how a real human's

513
00:48:43,440 --> 00:48:48,080
brain works and I'm not sure whether we should view a human brain as a computer program and whether

514
00:48:48,080 --> 00:48:54,320
that's a good thing or a bad thing but at some I guess what I'm saying is at some point we have

515
00:48:54,320 --> 00:48:59,600
to accept that we're not going to understand a totally it's actually it's a good comparison

516
00:48:59,600 --> 00:49:04,320
I think with humans you know if you're interviewing and you want to hire let's say a software developer

517
00:49:05,040 --> 00:49:09,200
you tend to set them a coding interview you wouldn't think of taking them into surgery

518
00:49:09,200 --> 00:49:12,880
opening up their brain and trying to find the neuron that predicts what the next you know

519
00:49:12,880 --> 00:49:17,200
but if code is going to be and understanding how that works it's just why would you do it that way

520
00:49:17,200 --> 00:49:22,000
instead you learn to trust humans by working with them giving them a test seeing how they

521
00:49:22,000 --> 00:49:27,120
perform in the real world and I know maybe we're asking too much of a machine learning model if

522
00:49:27,120 --> 00:49:32,720
you want to be able to understand these complex things in terms of like a bottom-up white box

523
00:49:33,280 --> 00:49:39,280
set of rules. I think what a comparison falls a bit short is that that we have the luxury that we can

524
00:49:40,400 --> 00:49:45,200
cut off the like cut open the brain of a machine learning model without breaking it

525
00:49:45,200 --> 00:49:54,160
and without hurting it hopefully and we can do all these try out all these interpretation methods

526
00:49:54,160 --> 00:49:59,280
see how it behaves under certain situations and I also would make a distinction between

527
00:49:59,280 --> 00:50:04,800
we understand what's going on inside and doing like this kind of sensitivity analysis

528
00:50:04,800 --> 00:50:11,360
where we just try out what happens in certain scenarios so it always do that that we can like

529
00:50:12,320 --> 00:50:17,680
check like how it behaves so I mean feature importance is basically like a way to see

530
00:50:17,680 --> 00:50:21,760
like how does it behave if we break some features and then we rank the features by this

531
00:50:21,760 --> 00:50:28,000
as an importance we can do it and that's also the big difference between humans because we

532
00:50:28,000 --> 00:50:33,520
can't test in the same way and yeah shouldn't probably. I think there's one other difference

533
00:50:33,520 --> 00:50:39,200
though which is to do with the substrate of how neural networks work I think if I'm giving someone

534
00:50:39,200 --> 00:50:44,960
a job interview or something I mean of course it's a very valuable process but I'm looking at

535
00:50:44,960 --> 00:50:49,360
their values and I'm looking to try and understand how they would behave in different situations

536
00:50:49,360 --> 00:50:53,920
and I'm coming up with lots of illustrative examples but the difference is with humans

537
00:50:53,920 --> 00:51:01,120
we have that level of generalization we have a kind of guiding taxonomy of behaviors which means

538
00:51:01,120 --> 00:51:07,040
if I know if I have guiding examples of what a human will do in certain situations I expect

539
00:51:07,040 --> 00:51:12,720
that to generalize whereas my hypothesis is is that a deep neural network model is almost like

540
00:51:12,720 --> 00:51:17,440
an infinite number of rules and there's absolutely no carryover between the rules

541
00:51:17,440 --> 00:51:23,360
so knowing even some or even most of the rules doesn't really tell me about those edge cases.

542
00:51:25,920 --> 00:51:32,000
Yeah I would agree that the edge cases are quite unforeseeable probably I mean at least we know

543
00:51:32,000 --> 00:51:37,280
that they exist like with adversarial examples so even if we don't know like exactly what they

544
00:51:37,280 --> 00:51:42,800
would look like or there's even an infinite amount of uh I mean there's an infinite amount of like

545
00:51:42,800 --> 00:51:50,480
how you can change the image to make it like have a different class so we know so I think it's important

546
00:51:50,480 --> 00:51:56,480
that we know that these exist at least. Yeah I love the point you made though that we have we

547
00:51:56,480 --> 00:52:02,640
have the luxury to do analysis on these on these boxes because we can open them up and that that's

548
00:52:02,640 --> 00:52:06,880
another point I'm pretty sure that you make this in your book as well which is that part of this is

549
00:52:06,880 --> 00:52:14,240
just scientific inquiry it's it's like understanding better how to interpret and explain machine

550
00:52:14,240 --> 00:52:20,160
learning models will probably actually contribute to us being able to construct even better machine

551
00:52:20,160 --> 00:52:25,520
learning models isn't that true? So so you're basically saying that um also interpretability

552
00:52:25,520 --> 00:52:31,120
might help to to be better at like create better machine learning models themselves?

553
00:52:32,240 --> 00:52:37,200
Yeah like as we as we develop these interpretability methods because in a sense like you point out

554
00:52:37,200 --> 00:52:43,040
earlier there are statistical projections of kind of the behavior of the model and so like a saliency

555
00:52:43,040 --> 00:52:48,480
map you know if we can if we can kind of use that to learn the way in which the the neural networks

556
00:52:48,480 --> 00:52:54,720
are behaving it may it can certainly give rise to intuitions on ways to alter the model. Yeah so I

557
00:52:54,720 --> 00:53:01,920
also have seen approaches where they try like kind of fuse also these two worlds like interpretative

558
00:53:01,920 --> 00:53:06,880
models or white box models and black box models so that you try to to generate features out of

559
00:53:06,880 --> 00:53:14,960
the black box model which you then use in your more understandable white box model so um I think

560
00:53:15,040 --> 00:53:21,440
this and and this also like by using similar techniques um to which you would use for interpretability

561
00:53:21,440 --> 00:53:28,080
like detecting interactions for examples for example so um yeah these can be used also to

562
00:53:28,080 --> 00:53:34,400
to build better models and also to build better interpretable models. The other and I'll make

563
00:53:34,400 --> 00:53:40,160
one last point here which is another social good that can come out of interpretability is

564
00:53:40,160 --> 00:53:45,440
imagine we've got you know an ML model that's not trying to make any decisions but it's just

565
00:53:45,440 --> 00:53:52,080
trying to figure out what leads to happiness and success in life you know and so we analyze a whole

566
00:53:52,080 --> 00:53:57,360
bunch of data and we find out well it's really important if you graduate from high school and

567
00:53:57,360 --> 00:54:01,680
it's really important if you you know don't have children before you're married and you know all

568
00:54:01,680 --> 00:54:07,440
these other factors if we can dive in and kind of isolate those factors it actually allows people

569
00:54:07,440 --> 00:54:12,320
to have some guidance on oh look we've had this machine learning model that's analyzed a bunch of

570
00:54:12,320 --> 00:54:17,920
data and it actually has some some understandable recommendations for how to lead a healthier

571
00:54:17,920 --> 00:54:24,000
life or a better life or whatever. Yeah I think that this very good example were the fact that

572
00:54:24,000 --> 00:54:31,760
you have a prediction model doesn't solve your problem so actually it's just a means to to some

573
00:54:31,840 --> 00:54:38,640
other goal in this case understanding like what are the factors for happiness and one example I am

574
00:54:38,640 --> 00:54:42,800
from a friend who worked at a telecom company and they built like a churn prediction model

575
00:54:44,080 --> 00:54:51,920
to see like who will quit the telecom contract and then they started like the ones with the

576
00:54:51,920 --> 00:54:56,640
highest likelihood they started sending out emails say maybe offering them a better deal

577
00:54:57,440 --> 00:55:04,480
but actually the outcome was that well they they when they once they wrote to the customers they

578
00:55:04,480 --> 00:55:11,920
well left and quit their contract so it's kind of had like so this is a case where the prediction

579
00:55:11,920 --> 00:55:16,240
model actually works but then they people leave in this case probably because they

580
00:55:16,240 --> 00:55:23,920
realized ah shit I have this contract still going on time to quit now so if you knew the

581
00:55:23,920 --> 00:55:30,800
reasons why they are likely to churn then you could like better select like when you write some

582
00:55:30,800 --> 00:55:37,040
email or maybe some other campaign or when maybe not to write anything at all yeah right now um

583
00:55:37,040 --> 00:55:42,080
Christoph you have a background in stats which means you I mean you like Connor as well we take

584
00:55:42,080 --> 00:55:47,360
an incredibly dim view of machine learning and you wonder how how is it possible for us to be

585
00:55:47,360 --> 00:55:51,920
stabbing in the dark like this but you know you said that we need to be more rigorous and

586
00:55:52,000 --> 00:55:57,520
there's no quantification of uncertainty with the current IML methods and I suspect you might

587
00:55:57,520 --> 00:56:01,840
be working on some methods behind the the scenes on this but you know when you have models and

588
00:56:01,840 --> 00:56:07,280
explanations which are computed from data they are subject to uncertainty and that's just not

589
00:56:07,280 --> 00:56:11,760
modeled at all at the moment right so we need to be making some distributional and structural

590
00:56:11,760 --> 00:56:15,920
assumptions that we're not making now and you point out that there's this phenomenon of p-hacking

591
00:56:15,920 --> 00:56:20,320
which is a huge problem in the natural sciences which hasn't quite made its way to IML methods

592
00:56:20,320 --> 00:56:27,520
yet but probably will do yeah so yeah the I think and in statistics we're really good at

593
00:56:27,520 --> 00:56:32,400
quantifying uncertainty I mean this also has some darker sides with like the p-hacking and so on

594
00:56:32,400 --> 00:56:38,240
but I still would say it's better to have um not only just one number or one explanation

595
00:56:38,240 --> 00:56:45,440
but also have the distribution to do of this explanation or this number and to quantify what

596
00:56:45,440 --> 00:56:50,800
uncertainties behind computing this number so when you have a linear model then you get

597
00:56:52,160 --> 00:56:56,960
some coefficient which you interpret in the end but usually you don't just interpret the

598
00:56:56,960 --> 00:57:02,560
coefficient but you look at the confidence intervals but we don't do it at the moment for

599
00:57:02,560 --> 00:57:08,400
interpretability so you maybe get the saliency maps but how certain are you about maybe it's

600
00:57:08,400 --> 00:57:13,920
a bad example because we don't so much on it but if you have like a feature importance value and

601
00:57:13,920 --> 00:57:19,040
you get some result how like what's the range actually like how much variance is behind it

602
00:57:19,040 --> 00:57:24,480
if I were to use slightly different data or refit my model again how similar would the number be

603
00:57:25,600 --> 00:57:30,400
and I think that's something that will or should come to interpretability as well

604
00:57:31,840 --> 00:57:36,080
it's funny how when we come to machine learning it's almost like open season and forgetting

605
00:57:36,080 --> 00:57:39,680
everything you know about maths and stats you throw it all out the window okay so excited

606
00:57:39,680 --> 00:57:45,360
about these algorithms right like one example is if you take a if you're fitting a model to predict

607
00:57:45,360 --> 00:57:50,240
something that was unlikely I don't know maybe it was like a covid test for example and then

608
00:57:50,240 --> 00:57:53,760
if you know the prevalence of covid you get it back you kind of know what the false positive

609
00:57:53,760 --> 00:57:58,640
rate is going to be and so you notice that you think it's the multiple comparison issue right

610
00:57:58,640 --> 00:58:01,920
you know that you're expecting a certain level of false positives when it comes to doing something

611
00:58:01,920 --> 00:58:08,080
like feature importance or looking at interpretability from a thousand features and then five come

612
00:58:08,160 --> 00:58:12,800
through is really really important you mentioned sometimes we just forget that multiple comparison

613
00:58:12,800 --> 00:58:16,800
issue forget the fact that probably these five are going to be completely false positives and

614
00:58:16,800 --> 00:58:21,520
probably completely meaningless yeah I agree especially if you have like these high-dimensionality

615
00:58:21,520 --> 00:58:26,880
features and for the record I have to say I mean there are already approaches so especially for

616
00:58:26,880 --> 00:58:33,200
feature importance because there's like a huge community in random forests for example and they

617
00:58:33,200 --> 00:58:37,760
thought a lot about these issues and their tests for this and stuff like it but for the rest of

618
00:58:37,760 --> 00:58:42,560
interpretability I think it could gain a lot thinking more about I mean this is very simple

619
00:58:42,560 --> 00:58:48,480
stuff like multiple comparisons quantifying uncertainty does the stuff like statisticians

620
00:58:48,480 --> 00:58:55,520
think like a long time already about it and I mean if you even if you leave the area of

621
00:58:55,520 --> 00:59:00,640
interpretability and look at the benchmarks so even like if you have like accuracy like a table

622
00:59:00,640 --> 00:59:05,440
and you see accuracies in it but there's no variance attached to it then it should be like

623
00:59:05,440 --> 00:59:10,880
suspicious of it but because if you just retrain your neural network with a different seed you might

624
00:59:10,880 --> 00:59:15,440
end up with a different accuracy in the end so and if you want to say a method is better than

625
00:59:15,440 --> 00:59:22,720
another method you want to quantify how larger ranges of uncertainty do you I mean there are a

626
00:59:22,720 --> 00:59:27,200
lot of things like the choice of data choice of splitting points and training and test data

627
00:59:27,360 --> 00:59:34,800
weight initialization and so on so I think a lot of this rigor from statistics could help

628
00:59:35,360 --> 00:59:38,720
the machine learning community and and machine learning science to become better

629
00:59:39,840 --> 00:59:44,320
yeah but let's let's never forget this uh quite quite well known saying which is there are three

630
00:59:44,320 --> 00:59:51,520
kinds of lies lies damned lies and statistics so you know that that's a lot of what's going on right

631
00:59:51,520 --> 00:59:57,280
is is you know fundamentally whenever we go measure data and we have a model what we're

632
00:59:57,280 --> 01:00:03,120
actually able to extract from that data and and the model is inherently probabilistic

633
01:00:03,120 --> 01:00:08,480
it's a probability distribution right at the end of the day and we get into trouble anytime we try

634
01:00:08,480 --> 01:00:14,480
to take that probability distribution and project it to numbers i.e. statistics like as soon as we

635
01:00:14,480 --> 01:00:20,640
start trying to to generate and it doesn't matter whether it's it's a mean plus a confidence interval

636
01:00:20,640 --> 01:00:24,960
or whatever the fact is we're throwing away information the totality of the information

637
01:00:24,960 --> 01:00:31,440
sitting there in that weird multimodal you know spread out distribution right and then we we find

638
01:00:31,440 --> 01:00:36,320
some way to simplify it and project it down to a set of numbers we've got a problem like that's

639
01:00:36,320 --> 01:00:41,200
and if you forget that that's happening if you forget that you're throwing away all this information

640
01:00:41,200 --> 01:00:46,560
I think that you know the tendency to do that isn't just simplification it's that oftentimes we

641
01:00:46,560 --> 01:00:51,680
have to use these probabilistic things to reach a decision and as soon as we get to the point where

642
01:00:51,680 --> 01:00:56,800
look i've got to choose either to go left or right give the loan or not give the loan as soon as we

643
01:00:56,800 --> 01:01:02,080
get down to some point where we have to make a concrete decision we're forced you know we're

644
01:01:02,080 --> 01:01:08,240
forced to project it right but along the way it's important not to lose sight of the the fact that

645
01:01:08,240 --> 01:01:14,320
we're throwing away information fantastic well and by the way you also said something interesting

646
01:01:14,400 --> 01:01:18,800
a minute ago Christoph which is about at least in most machine learning algorithms if you change

647
01:01:18,800 --> 01:01:22,960
the random seed you know that there's enough stability there that it still gives you roughly

648
01:01:22,960 --> 01:01:28,800
the the same model every time but in reinforcement learning if you change the random seed the entire

649
01:01:28,800 --> 01:01:34,720
thing is completely broken but but yeah what Keith was saying about this this this information

650
01:01:34,720 --> 01:01:39,280
and structure in models I think that's really interesting because people have said with reinforcement

651
01:01:39,280 --> 01:01:43,680
learning you can actually learn causal factors right but that's not really true you're interacting

652
01:01:43,680 --> 01:01:47,840
with the system but what you're learning is is a surface representation of causal factors so you

653
01:01:47,840 --> 01:01:54,320
might learn that there's a causal factor between like a hose putting out fire but it wouldn't

654
01:01:54,320 --> 01:01:58,960
actually learn that it was the water that put out you know that there was a causal relationship

655
01:01:58,960 --> 01:02:03,040
between the water and the fire and this is the case with so many of our models as we were saying

656
01:02:03,040 --> 01:02:06,960
earlier that there's there's just a a surface representation which doesn't actually represent

657
01:02:06,960 --> 01:02:11,680
the reality of our world at all but this brings me on to the next point because you have a real

658
01:02:11,680 --> 01:02:17,520
problem with causal interpretations of some of these iml metrics right and you you say that models

659
01:02:17,520 --> 01:02:21,600
well the the goal of models is that they should reflect the causal structure right this is what

660
01:02:21,600 --> 01:02:26,480
we want to do in science but most statistical learning just reflects these surface feature

661
01:02:26,480 --> 01:02:30,960
correlations they they don't even scratch the surface of what we want so what are we going

662
01:02:30,960 --> 01:02:35,760
to do right are you doing some work in this field to help us out here and and why are people making

663
01:02:35,760 --> 01:02:42,800
these fallacious interpretations yeah so so i'm not not working on anything causality related at the

664
01:02:42,800 --> 01:02:50,480
moment yeah but about about causality i mean kind of like um i i studied statistics bachelor and

665
01:02:50,480 --> 01:02:58,160
master and zero i mean the only time we were talked about causality was when i heard the

666
01:02:58,160 --> 01:03:05,120
sentence correlation does not imply causality and it was really about it so um i think it's

667
01:03:05,200 --> 01:03:11,040
like really um yeah should be taught a lot more like how to think about causality like

668
01:03:11,040 --> 01:03:17,680
just super simple things like you should include confounders or um like what types of features

669
01:03:17,680 --> 01:03:23,520
if you include them in the model um like destroy your causal interpretation of another features

670
01:03:23,520 --> 01:03:28,960
these these are not super difficult things so um then you don't have to like learn any like

671
01:03:28,960 --> 01:03:35,680
difficult frameworks to work with or read like causality books on it that's like super simple

672
01:03:37,200 --> 01:03:43,520
yeah rules of thumb for your features even um yeah and i think you also have to decide

673
01:03:44,080 --> 01:03:50,800
or distinguish between like what's the goal of your model do you want a causal interpretation

674
01:03:52,640 --> 01:03:57,200
or do you want to like because in a sense and you have to also distinguish between

675
01:03:57,200 --> 01:04:01,760
two levels you have the real world level and the model level i mean once you use features for the

676
01:04:01,760 --> 01:04:06,240
model they are causal for the model prediction of course because you designed it that way

677
01:04:06,240 --> 01:04:10,000
and the question is when you are you allowed to go to the real world level where you say

678
01:04:10,000 --> 01:04:14,400
okay this um the feature importance that i see here or also the feature dependence

679
01:04:15,120 --> 01:04:23,600
plot that i see is also causal um or and may interpret it as a cause and or as a causal effect

680
01:04:23,600 --> 01:04:31,040
also for for the real world and i think that also depends like if you need this interpretation

681
01:04:31,760 --> 01:04:38,480
if you do scientific modeling for example then you probably want it um but that there can also

682
01:04:38,480 --> 01:04:42,960
be good reasons to include non-causal features into your model if your goal is really just

683
01:04:42,960 --> 01:04:48,640
prediction and and some feature might help you with the with a good prediction um but it might

684
01:04:48,720 --> 01:04:53,920
not be causal at all yes but the problem is when we're using these deep learning models

685
01:04:54,560 --> 01:05:00,720
they they will learn a structure which probably has no relationship to the real world whatsoever

686
01:05:00,720 --> 01:05:07,600
but um i think causal um factors do generalize much better there's the example of um i don't know

687
01:05:07,600 --> 01:05:13,360
with car crashes right male testosterone levels is a causal factor so that will probably generalize

688
01:05:13,360 --> 01:05:18,240
to other locations where you didn't train your data on but unfortunately models don't really do that

689
01:05:18,960 --> 01:05:24,000
well but so just real quickly on that tim like the only reason that we know testosterone

690
01:05:24,560 --> 01:05:30,400
is a causal factor is is not from that data set it's from a bunch of mechanistic you know

691
01:05:30,400 --> 01:05:37,440
scientific research and biology and and elsewhere um so you know i i'm kind of wondering how

692
01:05:38,320 --> 01:05:42,800
it would be nice if at least machine learning methods could indicate that there may be the

693
01:05:42,800 --> 01:05:49,600
possibility of a causal structure so just looking for underlying hidden structures um that that you

694
01:05:49,600 --> 01:05:55,440
know they're more generalizable that could explain large pieces of the of the data and give kind of

695
01:05:55,440 --> 01:06:01,680
a list of hey there might be a causal factor here like go investigate it but on on that there's a

696
01:06:01,680 --> 01:06:06,560
difference between a causal factor and a causal structure i think that the challenge is that we

697
01:06:06,560 --> 01:06:11,120
don't have enough fidelity in the structure that benjo by the way is doing some interesting work on

698
01:06:11,120 --> 01:06:17,680
this using data driven approaches to um you know learn causal factors but it's the structure of the

699
01:06:17,680 --> 01:06:21,360
factor graph which i think is the important thing i mean this is one of the most interesting parts

700
01:06:21,360 --> 01:06:25,840
i think of machine learning actually trying to learn causation from a data set which you can do

701
01:06:25,840 --> 01:06:30,320
right things like beta networks where you specify all your variables you connect these nodes with

702
01:06:30,320 --> 01:06:36,400
edges and you can try to learn the optimal structure like the simplest structure and that

703
01:06:36,400 --> 01:06:42,240
sometimes turns out to be the real life causal effect if you do it well but the difference is

704
01:06:42,240 --> 01:06:49,200
you as a human you you know the causal structure and you've you've created that that graph so it's

705
01:06:49,200 --> 01:06:53,760
not learned you've created it you can learn these things from data right you can actually you can

706
01:06:53,760 --> 01:06:59,200
search over the set of all possible graphs all the possible edges and you have a bit of a loss

707
01:06:59,200 --> 01:07:03,360
function you try to find a graph that fits the data well so it's got enough edges but it's not too

708
01:07:03,360 --> 01:07:08,160
complex you're not relating everything to everything and so just from data without any human input

709
01:07:08,160 --> 01:07:13,680
with structure learning you can sometimes get a model that kind of out of nothing will give you

710
01:07:13,680 --> 01:07:18,560
the cause of relationships sometimes there is redundancy right because a graph that says a

711
01:07:18,560 --> 01:07:25,600
implied near causes b equals a c that's identical to c causes b causes a right but even then with

712
01:07:25,600 --> 01:07:31,280
structure learning you you've got this adjacency matrix and all of those nodes you've already come

713
01:07:31,280 --> 01:07:36,800
up with a priori so what you want to learn is what the nodes are themselves right yeah i think

714
01:07:36,800 --> 01:07:41,200
like what kona mentioned there's lots of moda setting where you have well defined features

715
01:07:41,920 --> 01:07:47,280
and i think what tim referred to was more like the you don't even know what the features are like

716
01:07:47,280 --> 01:07:53,200
if you have a convolutional neural network and and like what's an object um what is a feature that

717
01:07:53,200 --> 01:07:59,280
like is disentangled also from other objects um so i think also there is the big issue that you

718
01:07:59,280 --> 01:08:05,120
have this entanglement between concepts that i don't know that the frisbee is always with uh on

719
01:08:05,120 --> 01:08:12,960
the same image as a as a dog so um maybe the the neural network can't even separate these two things

720
01:08:12,960 --> 01:08:19,600
then because they are too entangled in the data set to even discover the structure that that is

721
01:08:19,600 --> 01:08:24,880
really underlying the the real world in this case that's a fascinating point actually because one of

722
01:08:24,880 --> 01:08:29,600
the reasons why there's no easy solution to adversarial examples is because you you learn these

723
01:08:29,600 --> 01:08:35,120
these non robust features and you might just think to yourself well um fur is a low magnitude

724
01:08:35,120 --> 01:08:39,120
feature it's really easy just to kind of create fur on anything and for the neural network into

725
01:08:39,120 --> 01:08:43,440
thinking it's a cat and you just say well this is obvious right you just create some rules to say

726
01:08:43,440 --> 01:08:49,760
well if it's if it's not an animal and fur then ignore the fur but actually the features are entangled

727
01:08:49,760 --> 01:08:54,480
in this complex neural network so you can't do that but i wanted to move the discussion on a bit

728
01:08:54,480 --> 01:08:59,920
so you said that there are some really interesting challenges ahead in in iml and what's fascinating

729
01:08:59,920 --> 01:09:03,760
is you start talking about the process so you say that the setting of machine learning is too static

730
01:09:03,760 --> 01:09:08,400
it doesn't reflect how these models are used in reality and models are embedded in a process or a

731
01:09:08,400 --> 01:09:13,280
product or even complex people interactions and i love this right because i talk about ml devops

732
01:09:13,280 --> 01:09:17,680
and machine learning models in isolation are irrelevant it's the people in the process that's

733
01:09:17,680 --> 01:09:22,560
where the complexity is even with intelligence itself it's a process right you know intelligence

734
01:09:22,560 --> 01:09:27,680
is the interaction between a brain a body and an environment and you know within the context of

735
01:09:27,680 --> 01:09:31,760
this process you know we've got all of this rich information that we could be bringing in from other

736
01:09:31,760 --> 01:09:36,480
disciplines and you're saying we should bring in compsci and stats folks and we should be bringing

737
01:09:36,480 --> 01:09:41,440
in psychologists and social scientists and we need to also have interpretability at a higher level

738
01:09:41,440 --> 01:09:46,480
at the institutional level right or at the society level so when you kind of broaden the discussion

739
01:09:46,480 --> 01:09:53,280
out a little bit i think it adds a nice bit of flavor yeah so it's very especially as a scientist

740
01:09:53,280 --> 01:09:59,280
it's so convenient to just have this fixed model a fixed data set and then you just geek out and

741
01:09:59,280 --> 01:10:05,600
invent all these methods and so on but reality is that that you use the method some place and then

742
01:10:05,600 --> 01:10:11,600
it interacts with the institution it's built with the developers it's built by with the people it

743
01:10:11,600 --> 01:10:18,720
affects and my favorite example there is when you have this closed loop where your model makes

744
01:10:18,720 --> 01:10:25,680
predictions and these predictions generate the next generation's data so for the next generation

745
01:10:25,680 --> 01:10:31,840
of the model it produces the data so there's this example of the rent index where you have this model

746
01:10:31,840 --> 01:10:38,560
that tells you how much rent you should like pay for a certain kind of apartment and so on

747
01:10:39,040 --> 01:10:47,040
and this is actually like um legally binding so if you're a land lord you have to accept kind of

748
01:10:47,040 --> 01:10:52,800
the range that is outputted by the model which also means that the data that is produced so the

749
01:10:52,800 --> 01:10:59,760
new flats that are rented out in new apartments they all have to fit the model kind of and then

750
01:10:59,760 --> 01:11:05,040
but then you use this data again to train your model so you have this very weird feedback loop

751
01:11:05,760 --> 01:11:11,680
and I think it's also difficult to wrap your head around it and understand implications of it

752
01:11:13,600 --> 01:11:21,040
that that same thing a very similar feedback loop was a fear in the you know in our algo

753
01:11:21,040 --> 01:11:27,360
shambles video about the uk testing since they couldn't conduct the the uh what was it the

754
01:11:27,360 --> 01:11:32,320
a-level test right Tim they they built some some models around that and so it would do things like

755
01:11:32,320 --> 01:11:37,520
well you know if this school historically never had anyone in this grade bucket then we're not

756
01:11:37,520 --> 01:11:43,040
going to assign anyone to that grade bucket in that school and so it's sort of this self-perpetuating

757
01:11:43,600 --> 01:11:50,880
you know feedback loop we were reading through a lot of your work and you I mean I'm just going

758
01:11:50,880 --> 01:11:56,640
to hit this point head on um you don't really talk that much about AI ethics and you know there's the

759
01:11:56,640 --> 01:12:01,600
f word which is the fairness word and and I don't I don't recall you ever using that word

760
01:12:01,600 --> 01:12:09,040
and is that something that you've deliberately shied away from um yeah I just like um

761
01:12:10,320 --> 01:12:16,400
define it as outside of the scope like to talk to to I don't know talk about ethics so um fairness

762
01:12:16,400 --> 01:12:21,920
metrics or so on because I think there's a really big field on its own and I just don't know as much

763
01:12:21,920 --> 01:12:26,160
like about all these things so I know a little bit like about the fairness metrics that they're

764
01:12:26,160 --> 01:12:34,080
out there um I also think I mean they're kind of like research wise a little bit overlapping but

765
01:12:34,080 --> 01:12:39,040
more or less separate fields I think interpretability and fairness um but of course they have some

766
01:12:39,040 --> 01:12:46,160
commonalities that I mean when you kind of to for fairness you have to look it's not necessarily

767
01:12:46,160 --> 01:12:52,960
inside the model but you have to study how the model behaves um and that's kind of the connection

768
01:12:53,040 --> 01:12:58,800
to interpretability I would say yeah well where yeah where I see the connection is work

769
01:12:58,800 --> 01:13:06,240
like yours is helping to build the tool set that will allow people to apply human you know intuition

770
01:13:06,240 --> 01:13:12,560
and and ethics and evaluations to machine learning because at the end of the day a lot of these are

771
01:13:12,560 --> 01:13:18,640
human moral judgments or ethical judgments and it's important that people be happy with them

772
01:13:18,640 --> 01:13:24,240
because you know we have to have the population as a whole understand and accept and be able to

773
01:13:24,240 --> 01:13:30,000
move forward with the increasing role that machine learning is having in our lives and building that

774
01:13:30,000 --> 01:13:34,800
tool set is necessary so it's like you said very early in this talk you know what do we do just

775
01:13:34,800 --> 01:13:39,760
stick our heads in the sand and ignore it and just accept machine learning models are going to do

776
01:13:39,760 --> 01:13:45,120
whatever they do as long as they fly the plane or you know don't kill too many people we're okay

777
01:13:45,120 --> 01:13:49,520
like I don't think that's going to work like we have to build the tool set that you're talking

778
01:13:49,520 --> 01:13:56,080
about and continue this process of exploring how to better explain and interpret ML models so that

779
01:13:56,080 --> 01:14:01,200
human beings can have that oversight because it's the only thing that's going to give us

780
01:14:01,200 --> 01:14:06,960
comfort really as a society I suppose the reason I segue to this is we were just talking about

781
01:14:06,960 --> 01:14:12,800
the process and you you mentioned some of these feedback loops because we can have a very superficial

782
01:14:12,800 --> 01:14:18,720
discussion and you could say well we need to be able to represent reality better than we do and

783
01:14:18,720 --> 01:14:24,560
we have a whole tool set here to identify sources of bias or you know lack of robustness etc in

784
01:14:24,560 --> 01:14:29,920
models but it's so much more complex than that because these models are used in a very complex

785
01:14:29,920 --> 01:14:35,760
process and you get these very very complex dynamics emerging as a result of that and I think

786
01:14:35,760 --> 01:14:41,520
we're only really just scratching the surface of understanding those dynamics yeah I think so too

787
01:14:42,000 --> 01:14:49,360
as I said I think in science it's always very easy to to study things in isolation like study one

788
01:14:49,360 --> 01:14:56,640
type of model study one type of adjustment for a deep neural network and hopefully we will see more

789
01:14:56,640 --> 01:15:02,720
work emerge on this I think I've never read the paper like I mean of course discussed implications

790
01:15:02,720 --> 01:15:09,440
but really like analyze like what happens in terms of the data and the model when we have like

791
01:15:09,440 --> 01:15:14,640
multiple generations for example of a model and how it changes over time but this thing I mean to

792
01:15:14,640 --> 01:15:21,280
study those things also means that you have to wait for a long time until you have these dynamics

793
01:15:21,280 --> 01:15:27,120
and I think in many cases it's just starting that we use these models more extensively in our daily

794
01:15:27,120 --> 01:15:34,640
life I have a question is is anyone because look interpretability metrics whatever they are say

795
01:15:34,720 --> 01:15:39,680
saliency maps and you know could be partial dependency plots whatever you could actually

796
01:15:39,680 --> 01:15:45,360
build in some requirements of those into the objective functions when you go to train models

797
01:15:45,360 --> 01:15:49,520
so for example I'm just going to come up with a crazy idea I have no idea if this is relevant at

798
01:15:49,520 --> 01:15:55,920
all but somebody could say look I want all my saliency maps to be you know sets of of a

799
01:15:55,920 --> 01:15:59,360
bezier curves or something like that like they have to have a certain smoothness

800
01:16:00,000 --> 01:16:04,880
property and you could actually put that as a constraint in the objective function has anybody

801
01:16:04,880 --> 01:16:10,160
tried anything like that yeah there are approaches so I saw one paper they added some

802
01:16:12,080 --> 01:16:18,000
some parts to their objective function so that when you create line explanations with line that

803
01:16:18,000 --> 01:16:25,200
they were most more stable and there are a lot of things like for neural networks you have

804
01:16:25,200 --> 01:16:33,280
disentanglement that you try that the feature maps or that the nodes learn disentangled concepts

805
01:16:34,640 --> 01:16:40,320
there are ways to introduce like monotonicity so that a feature can always go into the effect of

806
01:16:40,320 --> 01:16:47,600
a feature can always be in one direction not to like zigzag around so there are approaches to do

807
01:16:47,600 --> 01:16:57,200
this to like have okay like interpretability constraints in your modeling here yeah because

808
01:16:57,200 --> 01:17:00,960
I was just thinking this can go back to Connor you know Connor was saying earlier on why don't we

809
01:17:00,960 --> 01:17:07,280
just create white box models maybe we can use if the definition to a human being of white box is

810
01:17:07,280 --> 01:17:12,000
that it's interpretable and understandable if we can build into the objective functions when we're

811
01:17:12,000 --> 01:17:17,200
actually training the network that it has these properties then we'll actually be helping to create

812
01:17:17,600 --> 01:17:23,440
more white box you know models even if they are complex that's definitely an option but I think the

813
01:17:24,480 --> 01:17:29,200
issue remains the same that you with it's similar to a white box model I mean you'll make some trade

814
01:17:29,200 --> 01:17:35,840
offs in the end you have to make the judgment whether so when you put more constraints I mean

815
01:17:35,840 --> 01:17:40,960
you can actually also help the model of course that if you I mean if you have some inductive biases

816
01:17:40,960 --> 01:17:47,760
also which which you infuse into the model which help with predicting or be more more stable

817
01:17:49,280 --> 01:17:54,960
but sometimes you might maybe also trade off with accuracy and you just have to like in the end you

818
01:17:54,960 --> 01:18:03,120
have this yeah this set of models where some are more accurate some better than this one

819
01:18:03,120 --> 01:18:09,440
interpretability dimension the other is cheaper to deploy and then you have this so this kind of

820
01:18:09,440 --> 01:18:14,160
going into direction of like automatic machine learning and you don't get like just the best

821
01:18:14,800 --> 01:18:20,000
performing one but you have this parater set like well so we have multiple objectives that you want

822
01:18:20,000 --> 01:18:24,880
to hit and then there's not one model that works best but you have a set of models that

823
01:18:24,880 --> 01:18:30,160
that have different trade-offs between these objectives and then you have to decide what

824
01:18:30,160 --> 01:18:36,960
is the trade-off that you want to do that you want to have I guess there's no getting away from it is

825
01:18:37,040 --> 01:18:41,680
the interpretability it is going to get more important and more important I think you mentioned

826
01:18:41,680 --> 01:18:46,240
Christoph that you know we've had linear models for hundreds of years and then there's been this

827
01:18:46,240 --> 01:18:52,800
big explosion and deep learning and then would you say about 2016 to 2018 that's when interpretability

828
01:18:52,800 --> 01:18:56,720
is really kicked off I what do you think do you think where's it going are we just going to get

829
01:18:56,720 --> 01:19:02,960
more and more attention paid to this area I don't know if you can get more than than this I don't

830
01:19:02,960 --> 01:19:10,560
know yeah but I think it's at least here to stay and I think it's important I mean it has been

831
01:19:10,560 --> 01:19:18,960
important before but of course with like the push from deep learning especially and that it just

832
01:19:18,960 --> 01:19:23,600
became more clear to a lot of people that we need interpretability in some sense at least

833
01:19:24,960 --> 01:19:31,120
yeah of course people have attempted it before and worked on it before it's just more urgent now

834
01:19:31,360 --> 01:19:37,760
I really like the bit in your book talking about what's changed recently how interpretability is

835
01:19:37,760 --> 01:19:41,920
coming together as a field you know with this a unification so you know in physics we love a

836
01:19:41,920 --> 01:19:45,920
big unification when you take all these different things in the past and say oh they're all just

837
01:19:45,920 --> 01:19:51,120
part of this one big framework and it was chap that chap paper was amazing wasn't it saying things

838
01:19:51,120 --> 01:19:57,520
like lime deep lift layer-wise propagation shapely ah forget all them they're all special cases of

839
01:19:57,520 --> 01:20:02,480
these additive feature attribution methods and we can prove that this is the only one that's

840
01:20:02,480 --> 01:20:07,440
theoretically valid because it has these properties of symmetry it's got the stummy property so

841
01:20:08,160 --> 01:20:11,040
you know everything that's been done before in interpretability well they're all in our

842
01:20:11,040 --> 01:20:16,080
framework now and shapely values are the way forward yeah they're quite uh quite famous to

843
01:20:16,080 --> 01:20:21,120
shapely values yeah would you believe them then I mean I guess in their paper they're kind of

844
01:20:21,120 --> 01:20:24,960
they kind of disagree with lime a bit don't they they say well lime is a count of this but

845
01:20:25,040 --> 01:20:29,760
they're going to be breaking our properties of efficiency and symmetry so lime is using the

846
01:20:29,760 --> 01:20:34,240
wrong weights right they should be using this yeah kernel shape weights rather than the line weights

847
01:20:34,240 --> 01:20:40,320
I think that's just a different approach also to think about it I mean you don't maybe you don't

848
01:20:40,320 --> 01:20:48,320
I think I think the properties are quite attractive or meaningful at least um but also also the

849
01:20:48,480 --> 01:20:55,840
line approach I'm very critical about lime because it um I think it's difficult to have the correct

850
01:20:56,560 --> 01:21:05,600
to know like how to parameterize your your local models um so I think I'm a bit more of a fan

851
01:21:05,600 --> 01:21:11,440
of shapely values because of the theoretical properties it comes with are you talking about

852
01:21:11,440 --> 01:21:15,840
that distance measure in lime where you have to be able to quantify how far away is the permutation

853
01:21:15,840 --> 01:21:23,600
yeah the like the kernel width yeah which is set to 0.75 I think so I just looked it up and

854
01:21:23,600 --> 01:21:28,960
I mean it's it's a very difficult question it goes to the heart of like what's local um

855
01:21:29,840 --> 01:21:33,840
because like I mean you have this this kernel that decides like how much you weight

856
01:21:33,840 --> 01:21:39,920
all the data points around the point you want to explain and and like how how big is this area

857
01:21:39,920 --> 01:21:45,200
I think this is very dependent on your model and your data and there's no answer no easy answer to

858
01:21:45,200 --> 01:21:51,200
how to set it yeah or even more generally than that this this whole notion of what does it mean

859
01:21:51,200 --> 01:21:56,720
to have a local interpretation method in you know in text or vision so in in vision there's this

860
01:21:56,720 --> 01:22:01,120
super pixel concept which is something that seems to make intuitive sense but but does it you know

861
01:22:01,120 --> 01:22:06,800
when you create all of these different uh maskings of different parts of the input space but um

862
01:22:06,800 --> 01:22:12,000
with shapely values as well that they they are a beautiful a beautiful technique especially

863
01:22:12,080 --> 01:22:18,000
because the the values are are quite meaningful but if you have shared information between the

864
01:22:18,000 --> 01:22:22,240
features I mean Connor and I were talking about this for example you if you had the same model

865
01:22:22,240 --> 01:22:28,080
where you were predicting someone's income and you put their I don't know let's say you had salary

866
01:22:28,080 --> 01:22:34,960
in the model twice then the shapely value would be divided between the two duplicate fields right

867
01:22:34,960 --> 01:22:41,520
so there just seems to be so much esoterica in these IML methods right are we expected to know

868
01:22:41,520 --> 01:22:46,960
all of this stuff yeah I I think I mean that's why I wrote the book um to to capture these things

869
01:22:46,960 --> 01:22:51,520
that you have to know all the these these uh disadvantages of the methods where I try to

870
01:22:51,520 --> 01:22:58,640
be very honest I mean because I'm not too invested in them but yeah I think that's worth all tools

871
01:22:58,640 --> 01:23:05,520
that we usually have um also with statistics and so on you you have to know like um these like as

872
01:23:05,520 --> 01:23:09,840
you mentioned this if you have salary twice then it will just I mean depends also on what your model

873
01:23:09,840 --> 01:23:16,160
does if it just picks one of the salary features or if it itself uses both so this also something

874
01:23:16,160 --> 01:23:22,640
that will define how the shapely value will look like later on um but you have to know these things

875
01:23:22,640 --> 01:23:27,520
if you want to use shapely values and interpret interpret them correctly yeah because I think

876
01:23:27,520 --> 01:23:32,800
philosophically we've got we've got the real behavior and then we use these interpretability

877
01:23:32,800 --> 01:23:37,760
methods and then we've got the kind of perceived behavior so we've got these these levels of

878
01:23:38,400 --> 01:23:44,720
modeling or or do you know what I mean simplification and it's all well and true if you are dealing

879
01:23:44,720 --> 01:23:51,520
with data scientists who understand how these you know methods work that's fine but invariably

880
01:23:51,520 --> 01:23:57,760
data scientists need to present this information to lay people and they are not going to understand

881
01:23:58,480 --> 01:24:02,480
all of the various different trade-offs and how information is being compressed and and lost and

882
01:24:02,480 --> 01:24:08,880
so on so do you see that as a as a serious problem uh yes but it's not a new problem it's with any

883
01:24:08,880 --> 01:24:17,920
number that you read in any newspaper uh I mean so in a sense I mean when when you look at uh

884
01:24:17,920 --> 01:24:23,040
outcomes of statistical models that uh well everyone can understand well of course not because

885
01:24:23,040 --> 01:24:28,080
you need training to understand how to interpret a linear model or any regression model yeah there's

886
01:24:28,160 --> 01:24:34,080
difficulty but I don't think it's new in any sense um because there's always I mean any number that

887
01:24:34,080 --> 01:24:40,800
you read anywhere has a very complex process um so I don't know if you have like COVID testing

888
01:24:40,800 --> 01:24:46,640
numbers it's very complex like how the number was generated maybe like how it was aggregated over many

889
01:24:46,640 --> 01:24:52,400
states and like what cases it includes and which it doesn't and so the number looks very innocent

890
01:24:52,400 --> 01:24:58,000
and simple but there's a very long process behind it to produce it um maybe this process is a bit

891
01:24:58,560 --> 01:25:04,240
uh a bit more black box or a bit more difficult if it comes out if there's some machine learning

892
01:25:04,240 --> 01:25:10,160
in between machine learning model in between to generate a number um but yeah I think this

893
01:25:11,040 --> 01:25:19,120
problem is well old well let me let me challenge a little bit here on something which is okay if

894
01:25:19,120 --> 01:25:24,960
I have if I have some general formula just some very general formula and then I go in there and I go

895
01:25:24,960 --> 01:25:31,200
you know what this formula has five parameters and if I make this one point seven five and that one

896
01:25:31,200 --> 01:25:39,840
one third and this one two and that one zero and I call this the megatron you know uh activation

897
01:25:39,840 --> 01:25:45,760
potential and I go and write a paper about it that's really just an arbitrary you know kind of

898
01:25:45,760 --> 01:25:51,120
selection of a bunch of numbers and then you gave it a fancy mathematical passport and you got it

899
01:25:51,200 --> 01:25:57,120
published in some journal and now everybody has to memorize that as you know the megatron potential

900
01:25:57,120 --> 01:26:01,920
and kind of learn about it and that's a lot of what's going on right now is that it's really just

901
01:26:01,920 --> 01:26:06,560
a bunch of hacking like it's people just they don't really know a general solution and they don't

902
01:26:06,560 --> 01:26:11,520
know how to solve like in general the problem they're trying to solve and so they just hack around and

903
01:26:11,520 --> 01:26:17,120
then the ones that are kind of famous or demonstrate some success in a particular combination you know

904
01:26:17,200 --> 01:26:21,760
competition over in this corner or something it now becomes something that's part of the lexicon

905
01:26:21,760 --> 01:26:26,400
that we all have to learn and I think like I look back on this like imagine what physics was like

906
01:26:26,400 --> 01:26:31,840
before Leibniz and Newton you know invented calculus it's like everybody memorizing a whole

907
01:26:31,840 --> 01:26:36,960
bunch of little purpose built kind of formulas and then along comes a general framework which

908
01:26:36,960 --> 01:26:42,240
now we can just learn calculus and derive the special circumstances as needed. You're onto

909
01:26:42,240 --> 01:26:48,080
something really interesting there which is that with IML methods we are we are kind of

910
01:26:48,080 --> 01:26:53,440
compressing information down into a representation you know and then that that is a transport that

911
01:26:53,440 --> 01:26:58,160
can be understood by different people but there's a trade-off right because as you said you can learn

912
01:26:58,160 --> 01:27:03,040
calculus and that's a compact framework for doing lots of stuff but it's all about the amount of

913
01:27:03,040 --> 01:27:10,240
common knowledge that is required so it's possible to compress something down just to one symbol

914
01:27:10,240 --> 01:27:13,840
and that symbol could represent all of that knowledge but it doesn't help you because I still

915
01:27:13,840 --> 01:27:20,640
need to learn all of that knowledge. Yeah but so to me calculus was a very beautiful and simple

916
01:27:20,640 --> 01:27:26,160
framework that I could learn and then once I learned that simple thing I could go and solve all

917
01:27:26,160 --> 01:27:30,320
kinds of problems with it that before I would have to memorize specific solutions or like the

918
01:27:30,320 --> 01:27:35,840
quadratic formula for example is a student I didn't actually memorize the quadratic formula I just

919
01:27:35,920 --> 01:27:40,240
learned how to complete the square and then I would just do complete the square and if somebody

920
01:27:40,240 --> 01:27:44,720
asked me what the quadratic formula was I would just quickly derive it right because it was

921
01:27:44,720 --> 01:27:50,080
easier to memorize the rule and then apply the rule to any situation rather than to memorize

922
01:27:50,080 --> 01:27:55,840
all these little one-off you know kinds of hacks that we come up with. You're not normal Keith

923
01:27:55,840 --> 01:28:00,640
right so most people won't be able to go and understand this because I think it's well no

924
01:28:00,640 --> 01:28:07,520
these IML methods are brilliant for data scientists who can it's a framework right it's a

925
01:28:07,520 --> 01:28:12,000
reference of understanding so assuming that people can understand how Shapley values work

926
01:28:12,000 --> 01:28:18,320
then this is a beautiful representation to reason about the behavior of models. Sure but when I

927
01:28:18,320 --> 01:28:23,840
first saw Shapley values I realized immediately there's a connection in you know Bayesian analysis

928
01:28:23,840 --> 01:28:29,520
to marginalization you know all we're really doing here is computing the expected marginal you know

929
01:28:29,520 --> 01:28:35,280
contribution to this value it's not a probability but it's still the same procedure being done right

930
01:28:35,280 --> 01:28:40,000
and I think I'm going to throw myself in with the lay people to a degree because the reason I'm

931
01:28:40,000 --> 01:28:46,320
always striving for simplifications is because I don't have the capacity to memorize all these little

932
01:28:46,320 --> 01:28:52,080
arbitrary kinds of hacks and but I yet I could totally understand Bayesian analysis and like I

933
01:28:52,080 --> 01:28:57,360
said you know previously in some other videos statistics made no sense to me until I learned

934
01:28:57,360 --> 01:29:04,080
the Bayesian framework because that was based on very simple rules that I could then reapply as needed.

935
01:29:05,600 --> 01:29:12,720
I think that what you refer to Keith maybe the worst situation is with the saliency maps because

936
01:29:12,720 --> 01:29:19,360
you have so many methods and they all like back propagate the gradient and to do input pixels

937
01:29:19,920 --> 01:29:27,120
and to um now do you have to like learn like how dozens of these framework works or like

938
01:29:28,240 --> 01:29:33,280
how to interpret do interpretation with all of these and they're all kind of variants of each

939
01:29:33,280 --> 01:29:41,680
other so mostly because they just there's some ambiguity how you how you back propagate the

940
01:29:41,680 --> 01:29:48,800
gradient because because of the non-linear units and stuff and a little bit differences how you

941
01:29:48,880 --> 01:29:58,320
can define this and so you have this huge like a sea of many different methods I think it would be

942
01:29:58,320 --> 01:30:03,280
nice therefore as you said to have some like simplification where you say okay this is like

943
01:30:04,480 --> 01:30:09,280
all these methods work under this one principle basically and we have these two parameters

944
01:30:10,320 --> 01:30:16,640
and that's how they differ I think that's also that some I think I wrote something in a chapter

945
01:30:16,720 --> 01:30:25,840
that the police stop inventing new methods for saliency maps so I think it's enough and we

946
01:30:25,840 --> 01:30:33,040
should focus more on like doing this consolidation to like understand the limitations of the methods

947
01:30:33,040 --> 01:30:38,720
and consolidate them to see like what's the commonalities in which ways do they differ and so on

948
01:30:38,720 --> 01:30:42,960
that's probably actually my first impression actually when I first opened the interpretable ML

949
01:30:42,960 --> 01:30:47,680
book I was amazed how many different things there are I've you know heard people say ah

950
01:30:47,680 --> 01:30:52,080
you can't use machine learning it's just a black box so many times it'd almost been drilled into my

951
01:30:52,080 --> 01:30:58,240
head then seeing all the things you know from white box models ways of training salient models

952
01:30:58,240 --> 01:31:02,400
counterfactual explanations it's what a wonderful recipe right there are so many different things

953
01:31:02,400 --> 01:31:07,840
that you can do I feel like now I trust ML models more than other kinds of things because I have

954
01:31:07,840 --> 01:31:13,520
this amazing toolbox of ways to understand them the thing that strikes me though is

955
01:31:14,400 --> 01:31:18,800
most of these methods as we were just saying they require interpretation by a human and a human who

956
01:31:18,800 --> 01:31:24,480
understands how the method works I love this concept of turning machine learning into an

957
01:31:24,480 --> 01:31:32,400
engineering discipline and being able to do a lot of these tests non-interactively and I think

958
01:31:33,120 --> 01:31:37,600
Marco Rubirio has done a lot of work around the counterfactual examples and the data grouping

959
01:31:37,600 --> 01:31:43,360
and what excites me about these methods is they seem like methods that we could actually run

960
01:31:43,360 --> 01:31:48,400
as part of an automated process we still have to set thresholds maybe we could set a threshold

961
01:31:48,400 --> 01:31:53,840
that said if this if this counterfactual example flips the switch on more than one percent of

962
01:31:53,840 --> 01:31:59,200
examples then fail the build that seems reasonable but a saliency method I mean how the hell do you

963
01:31:59,200 --> 01:32:03,840
say well if there's lots of red pixels over here then break the build I mean it's just ridiculous

964
01:32:04,720 --> 01:32:12,640
yeah yeah so I've seen interesting approaches to like using interpretability also more

965
01:32:12,640 --> 01:32:21,680
automatically like when you do model monitoring you can do things like create interpretations

966
01:32:21,680 --> 01:32:27,920
and see if they significantly change over time for example so then have thresholds that warn you

967
01:32:27,920 --> 01:32:34,880
that hey something's going on with your model so I think that's also interesting approaches there

968
01:32:35,520 --> 01:32:41,760
yeah you know Tim to your point of making this an engineering field and even making interpretability

969
01:32:41,760 --> 01:32:46,640
and and understandability an engineering field I mean I think that maybe that's why I like your

970
01:32:46,640 --> 01:32:51,520
book so much Christoph as I think it's it's a step towards that direction it's like let's

971
01:32:51,520 --> 01:32:59,280
survey everything and more importantly let's create a finite and hopefully smallish set of simple

972
01:32:59,280 --> 01:33:05,760
concepts that we can all agree on and understand that we can use to catalog you know what's out there

973
01:33:06,560 --> 01:33:11,680
so please keep up the good work you know I'm interested to see where this goes so final

974
01:33:11,680 --> 01:33:15,440
question for you Christoph then I wonder what's what's next for interpretability like are we going

975
01:33:15,440 --> 01:33:19,920
to the point where it's going to be almost a box ticking exercise where we can say yes our process

976
01:33:19,920 --> 01:33:25,840
we've done the standard interpretability step I mean is it is computing power going to change it

977
01:33:25,840 --> 01:33:31,680
I remember when Shaq the library came out it made that approach possible whereas previously you know

978
01:33:31,680 --> 01:33:36,320
it was very hard very computation infusible my friend Angem who's a wonderful data scientist

979
01:33:36,320 --> 01:33:41,520
sent me n-video rapids they've got that running on GPUs way faster than of course before is it

980
01:33:41,520 --> 01:33:46,400
just going to become a standard step do you think or is it going to be something where you need

981
01:33:46,400 --> 01:33:51,840
decent subject matter expertise and some real thought to do to really understand how a model

982
01:33:51,840 --> 01:33:58,640
works so well predictions about the future are always hard so maybe more like what I wish or

983
01:33:58,640 --> 01:34:04,320
what yeah maybe think we'll have code to happen um so I mean what we're seeing already is like a lot

984
01:34:04,320 --> 01:34:10,320
of implementations of these methods so they're kind of getting a commonality a rook one can use it

985
01:34:10,320 --> 01:34:18,720
very easily there's a lot of libraries out there in python r but also in like this machine learning

986
01:34:19,680 --> 01:34:26,960
cloud tools they also have a lot of interpretation methods available now so in that sense I think

987
01:34:27,600 --> 01:34:34,800
and it's maturing a lot I still believe that we need some expertise to understand them or at

988
01:34:34,800 --> 01:34:39,200
least some good references and there will also be hopefully more than my book maybe have some

989
01:34:40,800 --> 01:34:46,480
documentation when for these tools and people answering on stack overflow questions and whatnot

990
01:34:48,240 --> 01:34:54,240
so I think um yeah it's it's getting we're getting that everyone can use it easily

991
01:34:55,840 --> 01:35:01,840
I think it should never be a box ticking exercise it's a similar thing when if you have an AI ethics

992
01:35:02,720 --> 01:35:06,400
you know governance process or something the last thing you want is for it just to be an

993
01:35:06,400 --> 01:35:11,120
automatic response so I've just you know yeah I've thought about AI ethics um it needs to be

994
01:35:11,120 --> 01:35:16,400
something that that we really engage with I think we need to abstract away a lot of the complexity

995
01:35:16,400 --> 01:35:22,080
at the moment I think it's possible to come up with an interface to standardize the way that we

996
01:35:22,080 --> 01:35:28,800
do interpretability and we can reduce down what we have now to certain primitives which means that

997
01:35:28,800 --> 01:35:33,280
it can plug into an engineering process and it also means that we can abstract away some of the

998
01:35:33,280 --> 01:35:38,880
complexity I think that's possible yeah I also would agree that it shouldn't be like just box

999
01:35:38,880 --> 01:35:44,400
ticking but you can like for the initial um when you start interpreting a model that you just have

1000
01:35:44,400 --> 01:35:49,200
like with a click you have a report and then it shows you the most basic things but then you still

1001
01:35:49,200 --> 01:35:53,360
like should ask the like the question like does it really make sense that this feature is the most

1002
01:35:53,360 --> 01:35:57,680
important one or what's happening there with these weird interactions between the two features

1003
01:35:57,680 --> 01:36:04,720
let's dig a bit deeper here and see what's going on so I think um there's this one portion that is

1004
01:36:04,720 --> 01:36:12,160
just like this automated reporting thing um but this should then be like the starting point for

1005
01:36:12,160 --> 01:36:19,200
more critical uh questioning of the model and and and checking what's going on um for some specific

1006
01:36:20,160 --> 01:36:26,480
problems maybe so it's going to be you click on the molnar report and it gives you the the report

1007
01:36:26,480 --> 01:36:31,840
from the book right yeah there will be convenience well um christoph molnar thank you very much for

1008
01:36:31,840 --> 01:36:35,600
joining us today it's an absolute honor to have you on the show thanks for having me hey folks this

1009
01:36:35,600 --> 01:36:38,960
is tim in post script there's just a couple of thoughts that didn't come to my mind during the

1010
01:36:38,960 --> 01:36:43,600
interview that I think I'd like to quickly cover now the first thing is on the lack of fairness

1011
01:36:43,600 --> 01:36:50,160
the reason why I raised that is most folks who talk about AI ethics and fairness they use the

1012
01:36:50,160 --> 01:36:55,600
toolkit of interpretability methods quite often you know to apply their trade there are tools out

1013
01:36:55,600 --> 01:37:02,000
there to mitigate fairness and to detect fairness microsoft's fair learn is a great example of this

1014
01:37:02,000 --> 01:37:10,400
what we really need is an operating model or a set of guidelines on how to implement these tools

1015
01:37:11,120 --> 01:37:17,760
how do I identify sources of problematic correlations we need to have a database of problematic

1016
01:37:17,760 --> 01:37:23,840
correlations having a tool that allows me to identify and mitigate bias frankly is useless

1017
01:37:24,560 --> 01:37:30,160
what do I do with that as we mentioned on the show many of the machine learning cloud providers

1018
01:37:30,160 --> 01:37:36,560
whether it's data iq or azure ml and sage maker they all have these interpretability methods built

1019
01:37:36,560 --> 01:37:42,640
in now including saliency maps and it's just a box ticking exercise frankly it's completely useless

1020
01:37:42,640 --> 01:37:49,680
there is no accepted guidance on how these tools should be used right so if I'm a large company

1021
01:37:49,680 --> 01:37:55,600
and I'm building an operating model around how to implement fairness techniques just having the

1022
01:37:55,600 --> 01:38:01,120
technology is irrelevant it's about the people and the process and the the kind of operating

1023
01:38:01,120 --> 01:38:07,280
model of how we implement it and there is basically no useful information out there to help us do that

1024
01:38:07,280 --> 01:38:12,960
the other thing is we spoke about this becoming an engineering discipline which is to say what if we

1025
01:38:12,960 --> 01:38:18,720
could create an interface to abstract away some of the vagaries and esoteric of interpretability

1026
01:38:18,720 --> 01:38:24,400
methods we might come up with some primitives or some common language and then we can hide the

1027
01:38:24,400 --> 01:38:29,440
complexity behind the interface this is kind of what we do with mo dev ops already we automate

1028
01:38:29,440 --> 01:38:34,960
as much as we can then we templatize and remove friction out of the process we even create building

1029
01:38:34,960 --> 01:38:42,640
blocks using domain specific languages or yaml files and pipelines and and so on so what we do

1030
01:38:42,640 --> 01:38:48,800
is is we create a level of abstraction where people can compose together pipelines remember

1031
01:38:48,800 --> 01:38:53,600
when Conor made the comment that this might just become a box ticking exercise and this is something

1032
01:38:53,600 --> 01:39:00,640
we see in security and AI ethics already we can't really trust people to self report that the model

1033
01:39:00,640 --> 01:39:07,040
is behaving correctly or that the project has no concerns from an AI ethics point of view the whole

1034
01:39:07,040 --> 01:39:12,960
point here is process if we want to create an operating model and ensure best practices are

1035
01:39:12,960 --> 01:39:19,360
followed or any kind of standardization in a large organization we have to design a process

1036
01:39:19,360 --> 01:39:27,760
and many eyes make shallow holes so the process would mandate that a certain number of stakeholders

1037
01:39:27,760 --> 01:39:34,320
were involved in assessing the particular iml technique and validating it essentially and

1038
01:39:34,320 --> 01:39:39,760
then we would need to record that assessment so who said what when and then if the company ever

1039
01:39:39,760 --> 01:39:44,960
became audited or if god forbid there was some kind of a problem where the iml model did something

1040
01:39:44,960 --> 01:39:49,920
wrong and it caused the company lots of damage or it harmed the environment or society or something

1041
01:39:49,920 --> 01:39:55,040
like that we would then be able to rewind the clock and say okay well joe blogs said it was okay

1042
01:39:55,040 --> 01:40:02,960
because of xyz so that is an operating model it's a process and how to design such a process again

1043
01:40:02,960 --> 01:40:09,600
is completely absent speaking as a chief data scientist myself that's the kind of thing that

1044
01:40:09,600 --> 01:40:14,560
i'm interested in and it's very difficult for me to do that i really hope you've enjoyed the

1045
01:40:14,560 --> 01:40:19,680
episode today we've had so much fun making it remember to like comment and subscribe and we'll

1046
01:40:19,680 --> 01:40:25,520
see you back next week

