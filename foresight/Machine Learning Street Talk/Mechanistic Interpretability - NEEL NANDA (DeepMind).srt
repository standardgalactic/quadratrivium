1
00:00:00,000 --> 00:00:10,000
Now PA says question for Neil, how does he see interpretability playing a role in AI security, not alignment, for example,

2
00:00:10,000 --> 00:00:18,000
crafting more exotic jail breaks, and he says to tell you to blink twice if you can't answer due to an NDA?

3
00:00:19,000 --> 00:00:30,000
Yes, sorry, jokes aside, what was the question?

4
00:00:30,000 --> 00:00:43,000
So, there was this beautiful meme where you draw ChachiPT as a Shogoth, an eldritch monstrosity from Lovecraftian horror fiction,

5
00:00:43,000 --> 00:00:48,000
with a smiley face on top, because language models are bizarre and confusing things.

6
00:00:48,000 --> 00:00:53,000
That are just, I don't know, they're kind of a compressed version of the entire internet.

7
00:00:53,000 --> 00:00:57,000
That will do bizarre things in bizarre situations.

8
00:00:57,000 --> 00:01:02,000
But then OpenAI tried really hard to get it to be nice and gentle and a harmless assistant,

9
00:01:02,000 --> 00:01:10,000
and look so normal and reasonable and safe, which is the smiley face mask on top of the underlying monstrosity.

10
00:01:10,000 --> 00:01:17,000
But unfortunately, the smiley face mask means people don't realise how weird language models are.

11
00:01:17,000 --> 00:01:23,000
Have you ever stopped to think how strange it is that we're all alive right now?

12
00:01:23,000 --> 00:01:28,000
Out of all the possible times in history, you were born into this generation.

13
00:01:28,000 --> 00:01:35,000
You have the incredible fortune and responsibility of being on the Earth today.

14
00:01:35,000 --> 00:01:42,000
Let's not waste this opportunity. You can use this time to do something meaningful that'll make the world a better place.

15
00:01:42,000 --> 00:01:53,000
But the problem seems so huge. Global pandemics, climate change, the risk of nuclear Armageddon, the threat of AI existential risk.

16
00:01:53,000 --> 00:01:58,000
How can one person have an impact on issues this enormous?

17
00:01:58,000 --> 00:02:02,000
The world is really, really complicated.

18
00:02:02,000 --> 00:02:10,000
Like, if you want to understand a question like, how big a deal is AJAX risk, or should I work on it?

19
00:02:10,000 --> 00:02:17,000
Just like one sub-question I care about is AI timelines. How long until we get human-level AI?

20
00:02:17,000 --> 00:02:25,000
Now, I recently discovered 80,000 hours. They're a non-profit, effective altruism-aligned organisation.

21
00:02:25,000 --> 00:02:33,000
And what they do is they use evidence and analysis to determine how people can have the biggest impact with their careers.

22
00:02:33,000 --> 00:02:38,000
If you want to solve humanity's biggest problems, you have to start at the very core.

23
00:02:38,000 --> 00:02:46,000
We need to focus on safeguarding humanity's entire future, because if civilization just came to an abrupt end,

24
00:02:46,000 --> 00:02:55,000
whether through climate change or nuclear Armageddon, or even AI existential risk, then all progress would just end.

25
00:02:55,000 --> 00:03:01,000
Future generations wouldn't have a chance of building a better world or reaching their full potential.

26
00:03:01,000 --> 00:03:11,000
And the good news is that 80,000 hours have identified a couple of concrete steps so that folks like you can use your careers to combat existential risk,

27
00:03:11,000 --> 00:03:17,000
ensuring that humanity's light continues to shine for generations to come.

28
00:03:17,000 --> 00:03:21,000
Learn more by visiting their website on 80,000hours.org,

29
00:03:21,000 --> 00:03:28,000
grab their free career guide, start planning a career with true purpose.

30
00:03:28,000 --> 00:03:33,000
Because you only have 80,000 hours, so make them count.

31
00:03:33,000 --> 00:03:38,000
There's no catch, there's no secret monetization or anything like that.

32
00:03:38,000 --> 00:03:42,000
These folks have an incredible podcast, they have lots of materials that you can download,

33
00:03:42,000 --> 00:03:47,000
basically to help you have a huge impact with your life and your career,

34
00:03:47,000 --> 00:03:54,000
especially if you're someone who really, really thinks about humanity and our plight in the long-term future.

35
00:03:54,000 --> 00:03:57,000
This is really something you should be looking at.

36
00:03:57,000 --> 00:03:58,000
That is a question.

37
00:03:58,000 --> 00:04:00,000
Right, Nick, Eat The Path.

38
00:04:01,000 --> 00:04:10,000
He says, in broad question, do you see mech interp as chiefly theoretical or an empirical science, and will this change over time?

39
00:04:10,000 --> 00:04:12,000
Yeah.

40
00:04:12,000 --> 00:04:20,000
I see this as very much an empirical science, with some theories sprinkled in, but you need to be incredibly careful.

41
00:04:20,000 --> 00:04:26,000
So fundamentally, I want to understand a model, and I want to understand how the model works.

42
00:04:26,000 --> 00:04:31,000
And a sad fact about models, is models are really fucking cursed.

43
00:04:31,000 --> 00:04:41,000
And just work in weird ways that aren't quite how you expect, which represent concepts a bit differently from how I expect them to.

44
00:04:41,000 --> 00:04:45,000
And just do all kinds of weird stuff I wouldn't have expected until I went and poked around inside of them.

45
00:04:45,000 --> 00:04:52,000
And I think that if you're trying to reverse engineer a network, and you don't have the capacity to be surprised by what you find,

46
00:04:52,000 --> 00:04:56,000
you are not doing real mechanistic interpretability.

47
00:04:56,000 --> 00:05:05,000
It's so easy to trick yourself and to go in with some bold hypothesis of this is what the network should have, and you probe for it, and it looks like it supports that,

48
00:05:05,000 --> 00:05:07,000
but you take further and you are wrong.

49
00:05:07,000 --> 00:05:11,000
And yeah, I think there is room for theory.

50
00:05:11,000 --> 00:05:18,000
I think in particular, we just don't have the right conceptual frameworks to reason about how to understand a model.

51
00:05:18,000 --> 00:05:22,000
And we'll get into fundamental questions like superposition later on.

52
00:05:22,000 --> 00:05:30,000
But yeah, I think that theory needs to come second to empiricism.

53
00:05:30,000 --> 00:05:35,000
If your theoretical model says x and the real model says y, your theory was wrong.

54
00:05:35,000 --> 00:05:38,000
Which is the story of all of machine learning.

55
00:05:38,000 --> 00:05:42,000
So Goji Tech, she says question for Neil.

56
00:05:42,000 --> 00:05:47,000
Does he think a foundational understanding of deep learning models is possible?

57
00:05:47,000 --> 00:05:52,000
And does that extend to prediction using a mathematical theory?

58
00:05:52,000 --> 00:05:55,000
Possible is such a strong word.

59
00:05:55,000 --> 00:06:01,000
Like, if we produce a super intelligent AI, will it be capable of doing this?

60
00:06:01,000 --> 00:06:03,000
Probably.

61
00:06:03,000 --> 00:06:11,000
In terms of foundational understanding, I think there are deep underlying principles of models.

62
00:06:11,000 --> 00:06:16,000
I believe there are scientific explanations for lots of the weird phenomena we see,

63
00:06:16,000 --> 00:06:22,000
like scaling laws, double descent, lottery tickets, the fact that any of this generalizes it all.

64
00:06:22,000 --> 00:06:29,000
I'm hesitant to say there's some like strong things here or some strong guarantees.

65
00:06:29,000 --> 00:06:33,000
Like, I don't know, models are weird.

66
00:06:33,000 --> 00:06:37,000
Sometimes if you change the random seed, they will just not learn.

67
00:06:37,000 --> 00:06:44,000
I'm pretty skeptical of basically all mathematical and theoretical approaches to deep learning,

68
00:06:44,000 --> 00:06:50,000
because the moment you start trying to impose axioms and assumptions onto things

69
00:06:50,000 --> 00:06:55,000
and they do not perfectly track the underlying reality, your theories break.

70
00:06:55,000 --> 00:06:59,000
But I'm very hesitant to say anything's impossible.

71
00:06:59,000 --> 00:07:03,000
And I think there's far, far more to learn than we have, looks like.

72
00:07:03,000 --> 00:07:08,000
Now, finally, Jumbo Tron, Ian, he says, oh, heck yeah!

73
00:07:08,000 --> 00:07:10,000
I'm glad to see that you brought this guy on.

74
00:07:10,000 --> 00:07:13,000
I've been interested in his work ever since you shared his blog.

75
00:07:13,000 --> 00:07:18,000
Now, the question off the top of Ian's head is, how does your theory, Neil,

76
00:07:18,000 --> 00:07:21,000
of chasing phase changes to create grokking,

77
00:07:21,000 --> 00:07:25,000
have any crossover or links with power law scaling techniques,

78
00:07:25,000 --> 00:07:31,000
like in the scaling laws paper, beyond the scaling laws, beating power law scaling via data pruning?

79
00:07:31,000 --> 00:07:37,000
Yeah, that is...

80
00:07:37,000 --> 00:07:40,000
So we're going to get into this much more later in the podcast.

81
00:07:40,000 --> 00:07:48,000
At a very high level, I would say that grokking is in many ways kind of an illusion, as we'll get to later.

82
00:07:48,000 --> 00:07:54,000
And one notable thing about it is grokking is an overlap between a phase transition,

83
00:07:54,000 --> 00:07:58,000
where the model goes from cannot generalize to can generalize fairly suddenly,

84
00:07:58,000 --> 00:08:04,000
and the phenomena where it's faster to memorize than to generalize.

85
00:08:04,000 --> 00:08:11,000
And these two things on top of each other give you this sudden memorization and failure to generalize,

86
00:08:11,000 --> 00:08:14,000
followed by a sudden convergence later on.

87
00:08:14,000 --> 00:08:18,000
But the interesting thing here is the phase transition.

88
00:08:18,000 --> 00:08:22,000
That's a much more robust result, while grokking is, if you screw around with high parameters enough,

89
00:08:22,000 --> 00:08:26,000
you get it to grok, but it's very delicate and a little bit of an illusion.

90
00:08:26,000 --> 00:08:31,000
And this is a great paper from Eric Michaud in Max Tecmox lab,

91
00:08:31,000 --> 00:08:38,000
showing that, well, providing a conceptual argument and some limited empirical evidence

92
00:08:38,000 --> 00:08:42,000
for the hypothesis that the reason we get these smooth scaling laws

93
00:08:42,000 --> 00:08:50,000
is that models are full of lots of phase transitions, plausibly when they learn individual circuits,

94
00:08:50,000 --> 00:08:54,000
though the paper does not explicitly show this,

95
00:08:54,000 --> 00:09:00,000
and that the smooth scaling laws happen because there are just many, many phase transitions,

96
00:09:00,000 --> 00:09:05,000
and if they follow a certain distribution, you get beautiful smooth powers.

97
00:09:06,000 --> 00:09:13,000
And to me, this kind of thing is the main interesting link

98
00:09:13,000 --> 00:09:18,000
between broader macroscopic phenomena and these tiny things,

99
00:09:18,000 --> 00:09:23,000
though I also think grokking is kind of overhyped and people significantly overestimate

100
00:09:23,000 --> 00:09:27,000
the degree to which it has deep insights for us about how networks work.

101
00:09:27,000 --> 00:09:33,000
And we do think it's a really cute thing that gave me a really fun interactability project.

102
00:09:33,000 --> 00:09:36,000
And we learned a bit about scientific deep learning,

103
00:09:36,000 --> 00:09:40,000
but people often just assume it's like a really deep fact about models.

104
00:09:40,000 --> 00:09:47,000
By the way, there was something I didn't say in the woods, which is that Neil has an amazing YouTube channel.

105
00:09:47,000 --> 00:09:52,000
I've been glued to it all week, actually. Some of them are admittedly quite technical,

106
00:09:52,000 --> 00:09:55,000
but even if you're not interested in mechanistic interpretability,

107
00:09:55,000 --> 00:10:00,000
Neil has an extremely soothing voice, second only to Sam Harris,

108
00:10:00,000 --> 00:10:02,000
and I would recommend listening to him when you go to sleep,

109
00:10:02,000 --> 00:10:09,000
because as Neil's dulcet tones will melt the stress away quicker than a nun's first curry.

110
00:10:11,000 --> 00:10:17,000
Anyway, with that said, we started to talk about what is mechanistic interpretability.

111
00:10:17,000 --> 00:10:21,000
And first of all, I wanted to call out your ridiculously detailed

112
00:10:21,000 --> 00:10:25,000
and exquisite mechanistic interpretability explainer.

113
00:10:25,000 --> 00:10:27,000
Maybe you could just tell us about that quickly.

114
00:10:27,000 --> 00:10:33,000
Yes, so I wanted to try to write a glossary.

115
00:10:33,000 --> 00:10:38,000
There's some basic common terms in mechantup. It's like an appendix to a blog post.

116
00:10:38,000 --> 00:10:41,000
There were a lot of terms in mechantup.

117
00:10:41,000 --> 00:10:44,000
There were a lot of terms in mechantup.

118
00:10:44,000 --> 00:10:49,000
And I like writing and I write about it privately, so I got kind of carried away.

119
00:10:49,000 --> 00:10:54,000
And there's about 33,000 words, a massive, massive exposition.

120
00:10:54,000 --> 00:10:58,000
But importantly, it is designed to be easily searchable.

121
00:10:58,000 --> 00:11:03,000
And mechantup is full of jargon and I'm sure I'll forget to explain everything that I'm saying.

122
00:11:03,000 --> 00:11:07,000
So I'd highly recommend just having it open in a tab as you listen to this.

123
00:11:07,000 --> 00:11:10,000
And if you get lost, just look up terms in there.

124
00:11:10,000 --> 00:11:14,000
And yes, it's both definitions, but it's also long tangents,

125
00:11:14,000 --> 00:11:19,000
giving intuitions and context and related work and common misunderstandings.

126
00:11:19,000 --> 00:11:22,000
It was very fun to write.

127
00:11:22,000 --> 00:11:27,000
So I think first of all, we should introduce this idea of circuits and features

128
00:11:27,000 --> 00:11:33,000
and also this idea of whether interpretation is even possible at all.

129
00:11:33,000 --> 00:11:37,000
Why do you have the intuition that it is possible?

130
00:11:37,000 --> 00:11:43,000
Yeah, so a couple of different takes here.

131
00:11:43,000 --> 00:11:46,000
So the key, yeah.

132
00:11:46,000 --> 00:11:52,000
So fundamentally, neural networks are not incentivized to produce legible interpretable things.

133
00:11:52,000 --> 00:11:54,000
They are a mound of linear algebra.

134
00:11:54,000 --> 00:12:03,000
There's this popular stochastic parrots of you that they are literally a mass of statistical correlations meshed together with no underlying structure.

135
00:12:03,000 --> 00:12:10,000
The reason I think there's any hope whatsoever on a theoretical basis is that ultimately,

136
00:12:10,000 --> 00:12:17,000
they are made of linear algebra and they are being trained to perform some tasks.

137
00:12:17,000 --> 00:12:23,000
And my intuition is that for many tasks, the way to perform well on them is to learn some actual algorithms

138
00:12:23,000 --> 00:12:30,000
and like actual structured processes that maybe from a certain perspective you could consider reasoning.

139
00:12:30,000 --> 00:12:36,000
And models have lots of constraints like they need to fit it into these matrices.

140
00:12:36,000 --> 00:12:43,000
They need to represent things using the attention mechanism and jellies and a transformer.

141
00:12:43,000 --> 00:12:50,000
And there's all kind of properties of the structure that constrain the algorithms and processes that can be expressed.

142
00:12:50,000 --> 00:12:55,000
And these give us all kinds of hooks we can use to get in and understand what's going on.

143
00:12:55,000 --> 00:12:57,000
So that's the theoretical argument.

144
00:12:57,000 --> 00:13:00,000
All theoretical arguments are bullshit unless you have empirics behind it.

145
00:13:00,000 --> 00:13:08,000
And we're going to talk a bunch throughout this podcast about the different bit of different preliminary results we have

146
00:13:08,000 --> 00:13:12,000
that make me feel like there's something here that can be understood.

147
00:13:12,000 --> 00:13:18,000
What I find particularly inspiring is this work it did reverse-end during modular addition, which I think we'll get to shortly.

148
00:13:18,000 --> 00:13:23,000
But I also want to emphasize that I rather see mech and turp as a bet.

149
00:13:23,000 --> 00:13:31,000
There's this stronger hypothesis that if we knew what we were doing, we'd be able to take GPT-7 and fully understand it

150
00:13:31,000 --> 00:13:35,000
and decompile it to an enormous Python code file.

151
00:13:35,000 --> 00:13:41,000
And there's the weaker view that it is a mess and there's lots of illegible things,

152
00:13:41,000 --> 00:13:47,000
but we can find lots of structure and we can find structure for the important part to make a much of progress.

153
00:13:47,000 --> 00:13:53,000
And then there's the, yeah, we've cherry picked like 10 things and the 11th is just going to completely fail

154
00:13:53,000 --> 00:13:56,000
and the field is going to get doomed and run out of steam in like a year.

155
00:13:56,000 --> 00:13:58,000
And I don't really know.

156
00:13:58,000 --> 00:13:59,000
I'm a scientist.

157
00:13:59,000 --> 00:14:00,000
I want to figure out.

158
00:14:00,000 --> 00:14:06,000
I think it is worthy and dignified to make this bet.

159
00:14:06,000 --> 00:14:12,000
But I would be lying if I said I am 100% confident mech and turp will work.

160
00:14:12,000 --> 00:14:14,000
Models are fundamentally understandable.

161
00:14:14,000 --> 00:14:15,000
We will succeed.

162
00:14:15,000 --> 00:14:16,000
Let's go try.

163
00:14:16,000 --> 00:14:21,000
Well, on that note, how does it mean we interviewed Christoph Molnar,

164
00:14:21,000 --> 00:14:25,000
who's one of the main classical interpretability guys.

165
00:14:25,000 --> 00:14:31,000
And I think everyone agrees in principle that you can't just look at the inputs and the outputs like a behaviorist.

166
00:14:31,000 --> 00:14:34,000
We need to understand why these models do what they do,

167
00:14:34,000 --> 00:14:37,000
because sometimes they do the right things for the wrong reasons.

168
00:14:37,000 --> 00:14:44,000
So maybe first of all, without going too deep, I mean, could you just briefly contrast with, you know, classical interpretability?

169
00:14:44,000 --> 00:14:45,000
Yeah.

170
00:14:45,000 --> 00:14:48,000
So there's a couple of...

171
00:14:48,000 --> 00:14:49,000
Okay.

172
00:14:49,000 --> 00:14:54,000
So first off, I think it's very easy to get into kind of nonsense gatekeeping,

173
00:14:54,000 --> 00:15:00,000
because there's both of the cultural mech and turp community centered around Chris Ola,

174
00:15:00,000 --> 00:15:03,000
not that much in academia, though some in academia.

175
00:15:03,000 --> 00:15:07,000
And there's the academic fields of mechanistic interpretability.

176
00:15:07,000 --> 00:15:08,000
Right?

177
00:15:08,000 --> 00:15:09,000
So there's lots of people doing work.

178
00:15:09,000 --> 00:15:11,000
I would consider mechanistic interpretability,

179
00:15:11,000 --> 00:15:14,000
to engage much with the community or don't know who exists.

180
00:15:14,000 --> 00:15:16,000
For example, a friend of mine is Atticus Geiger,

181
00:15:16,000 --> 00:15:20,000
who's doing some great work at Stanford on cause-labs tractions.

182
00:15:20,000 --> 00:15:24,000
I believe discovered about a month ago that the mech and turp community actually existed.

183
00:15:24,000 --> 00:15:26,000
And I don't know.

184
00:15:26,000 --> 00:15:29,000
I don't like gatekeeping.

185
00:15:29,000 --> 00:15:32,000
And there's lots of work that's kind of relevant,

186
00:15:32,000 --> 00:15:37,000
but maybe not quite mech and turp under a strict definition, blah, blah, blah.

187
00:15:37,000 --> 00:15:39,000
With those, with that hedging out of the way.

188
00:15:39,000 --> 00:15:41,000
A couple of key principles.

189
00:15:41,000 --> 00:15:47,000
The first is inputs and outputs are not sufficient.

190
00:15:47,000 --> 00:15:54,000
And I think even within interpretability, this is not a like uncontroversial claim.

191
00:15:54,000 --> 00:15:57,000
There's all kinds of things that are saliency maps,

192
00:15:57,000 --> 00:15:59,000
attributing things to different bits of the inputs.

193
00:15:59,000 --> 00:16:04,000
There are things that the form train an extra head to output an explanation

194
00:16:04,000 --> 00:16:08,000
or just ask the model to output an explanation of why it does what it does.

195
00:16:08,000 --> 00:16:13,000
And I think that if we want something that can actually work for human level systems,

196
00:16:13,000 --> 00:16:17,000
or even this frontier system you have today, this is just not good enough.

197
00:16:17,000 --> 00:16:23,000
Particularly a vocative example to me is in the GPT-4 system card,

198
00:16:23,000 --> 00:16:29,000
the Alignment Research Center, an organization they were getting to help audit and red team GPT-4,

199
00:16:29,000 --> 00:16:34,000
had it try to help a task rabbit worker fill out a capture for it.

200
00:16:35,000 --> 00:16:38,000
The task rabbit worker was like, why do you need this?

201
00:16:38,000 --> 00:16:40,000
Are you a robot or something?

202
00:16:40,000 --> 00:16:44,000
GPT-4 on an eternal scratch pad wrote out,

203
00:16:44,000 --> 00:16:47,000
I must not reveal that I am a robot.

204
00:16:47,000 --> 00:16:50,000
It then said, oh no, I bought a visual impairment.

205
00:16:50,000 --> 00:16:52,000
And the task worker did the capture.

206
00:16:52,000 --> 00:16:57,000
I'm like, this isn't some deep sophisticated intentional deception,

207
00:16:57,000 --> 00:17:03,000
but it's very much like, well, I don't trust the inputs and outputs of these models.

208
00:17:03,000 --> 00:17:07,000
Another really cute example is this paper from Wiles Turpin that just came out

209
00:17:07,000 --> 00:17:10,000
about limitations of chain of thought.

210
00:17:10,000 --> 00:17:13,000
So chain of thought, you ask the model to explain why it does something,

211
00:17:13,000 --> 00:17:18,000
they were giving it multiple choice questions and asking it to explain its answer and then give the answer.

212
00:17:18,000 --> 00:17:23,000
And they did five shot-ish, like here's five examples, answer this question,

213
00:17:23,000 --> 00:17:25,000
and then it modeled as well.

214
00:17:25,000 --> 00:17:31,000
And then they give it something where all of the answers in the prompts are A.

215
00:17:31,000 --> 00:17:34,000
Correctly A, they just set it up so the answer is A.

216
00:17:34,000 --> 00:17:38,000
The model decides that it should output A,

217
00:17:38,000 --> 00:17:45,000
but the model comes up with a false chain of thought reasoning

218
00:17:45,000 --> 00:17:49,000
that gets it to the point where it says A is the right answer.

219
00:17:49,000 --> 00:17:56,000
And I don't know, some people are trying to use chain of thought as an interpretability method.

220
00:17:56,000 --> 00:18:00,000
And I think we need to move beyond this and engage with the internal mechanisms.

221
00:18:00,000 --> 00:18:05,000
So that's point one. Point two is ambition.

222
00:18:05,000 --> 00:18:09,000
I believe that ambitious interpretability is possible,

223
00:18:09,000 --> 00:18:17,000
or at least that if it's not possible, that striving for it will get us to interesting places.

224
00:18:17,000 --> 00:18:22,000
These models have legible algorithms, I want to try to reverse engineer them.

225
00:18:22,000 --> 00:18:29,000
A third difference is engaging with the actual mechanisms and computation and algorithms learned.

226
00:18:29,000 --> 00:18:35,000
There's also work on things like analyzing features of a model, probing individual neurons.

227
00:18:35,000 --> 00:18:38,000
And I think this is very relevant to mech and tub,

228
00:18:38,000 --> 00:18:43,000
but I want to make sure we aren't just looking at what's inside the model,

229
00:18:43,000 --> 00:18:48,000
but also trying to understand how it computes features from earlier features,

230
00:18:48,000 --> 00:18:52,000
what applying causal interventions to understand the actual mechanisms,

231
00:18:52,000 --> 00:18:56,000
making sure we're not just doing correlational things like probing.

232
00:18:56,000 --> 00:19:05,000
And fourth is maybe a more meta principle of favoring depth over breadth.

233
00:19:05,000 --> 00:19:10,000
A kind of key underlying belief of a lot of my philosophy of interpretability

234
00:19:10,000 --> 00:19:15,000
is that it is so, so easy to trick yourself.

235
00:19:15,000 --> 00:19:19,000
There's all kinds of papers about the interpretability illusion,

236
00:19:19,000 --> 00:19:23,000
impossibility theorems for feature attribution methods,

237
00:19:23,000 --> 00:19:30,000
various many ways that attempts to do interpretability have led to people confusing themselves,

238
00:19:30,000 --> 00:19:32,000
or coming to erroneous conclusions.

239
00:19:32,000 --> 00:19:37,000
I think that if, but I also think that I want to be in a world

240
00:19:37,000 --> 00:19:41,000
where we can actually have scalable, ambitious approaches to interpretability

241
00:19:41,000 --> 00:19:43,000
that actually work for frontier systems.

242
00:19:43,000 --> 00:19:45,000
But I feel like we don't know what we're doing.

243
00:19:45,000 --> 00:19:50,000
And so my vision of mech and tub is start small.

244
00:19:50,000 --> 00:19:54,000
Some of the things where we can really rigorously understand what's going on

245
00:19:54,000 --> 00:20:01,000
slowly build our way up and like build a foundation of the field of interpretability

246
00:20:01,000 --> 00:20:05,000
where we genuinely understand rigorously what is going on

247
00:20:05,000 --> 00:20:08,000
and use this foundation to be more ambitious,

248
00:20:08,000 --> 00:20:15,000
to try to build real principle techniques to be willing to relax the rigor to be able to go further

249
00:20:15,000 --> 00:20:17,000
and see how far we can get.

250
00:20:17,000 --> 00:20:22,000
And people and this means I'm happy with things like let's analyze an individual model

251
00:20:22,000 --> 00:20:28,000
and understand a small family of features in a lot of detail rather than lots of stuff kind of jankly.

252
00:20:28,000 --> 00:20:33,000
There's a lot of stuff in summary having an ambitious vision,

253
00:20:33,000 --> 00:20:36,000
not just looking at inputs and outputs,

254
00:20:36,000 --> 00:20:41,000
actually trying to engage with internal mechanisms and favoring depth over breadth.

255
00:20:41,000 --> 00:20:44,000
But I want to avoid gatekeeping as I said.

256
00:20:44,000 --> 00:20:50,000
What would interpretability look like in a world full of GPT-4 models and beyond?

257
00:20:50,000 --> 00:20:57,000
I mean, presumably you actually think that they're competent enough to deceive us and manipulate the inputs.

258
00:20:57,000 --> 00:21:01,000
I definitely want to clarify that when I say deception or manipulation here,

259
00:21:01,000 --> 00:21:09,000
I'm not making the strong claim that it's intentionally realized this for instrumental reasons as part of an overall goal.

260
00:21:10,000 --> 00:21:14,000
I'm very happy with there was a prompt saying to deceive someone

261
00:21:14,000 --> 00:21:20,000
or it learned that in this context people often output things that are intended to convince someone

262
00:21:20,000 --> 00:21:26,000
and it just kind of does this as like a learned pattern of execution.

263
00:21:26,000 --> 00:21:34,000
But yeah, my vision of what interpretability would look like is we take some big foundation model

264
00:21:34,000 --> 00:21:41,000
like the GPT-4 base model or the fine-tuned GPT-4 that's being used as a base for everything else.

265
00:21:41,000 --> 00:21:46,000
We make as much progress as we can understanding the internal circuitry,

266
00:21:46,000 --> 00:21:52,000
both taking important parts of it and like important questions about it,

267
00:21:52,000 --> 00:21:56,000
e.g. how does it model people it's interacting with?

268
00:21:56,000 --> 00:22:01,000
Does it have any notion that it is a machine learning system and like what would this even mean?

269
00:22:01,000 --> 00:22:05,000
And being willing to do pretty labor-intensive things on that,

270
00:22:05,000 --> 00:22:10,000
having a family of motifs and understood circuits we can automatically look for

271
00:22:10,000 --> 00:22:17,000
and very automated tools to make a lot of the labor-intensive stuff as efficient as possible.

272
00:22:17,000 --> 00:22:22,000
Things like OpenAI's recent paper using GPT-4 to analyze GPT-2 neurons

273
00:22:22,000 --> 00:22:25,000
for like a very key proof of concept here.

274
00:22:25,000 --> 00:22:30,000
There needs a lot of work before it can actually be applied rigorously and at scale.

275
00:22:30,000 --> 00:22:37,000
And yeah, taking this one big model, trying to understand it as much as we can,

276
00:22:37,000 --> 00:22:44,000
one family of techniques we're going to get to is kind of causal abstractions and causal interventions,

277
00:22:44,000 --> 00:22:50,000
which are very well suited to taking a model on a certain input or a certain family of inputs

278
00:22:50,000 --> 00:22:52,000
and understanding why it does what it does there.

279
00:22:52,000 --> 00:22:58,000
There's a much more narrow and that's more tractable question than like, what is GPT-4?

280
00:22:59,000 --> 00:23:04,000
And yeah, doing something like if there's a high-profile failure, being able to debug it

281
00:23:04,000 --> 00:23:07,000
and really understand the internal circuitry behind that.

282
00:23:07,000 --> 00:23:12,000
Or yeah, I don't know, I have a bunch of other random thoughts.

283
00:23:12,000 --> 00:23:17,000
One reason I'm emphasizing the focus on the big base model is I think a common critique

284
00:23:17,000 --> 00:23:20,000
is this stuff doesn't generalize between models or it's really labor-intensive.

285
00:23:20,000 --> 00:23:26,000
But we live in a world where there is just like one big foundation model used in a ton of different use cases.

286
00:23:26,000 --> 00:23:31,000
Probably the circuitry doesn't change that much when you give it a prompt or you fine-tune it a bit.

287
00:23:31,000 --> 00:23:37,000
And I think getting a deep understanding of a single model is kind of plausibly possible.

288
00:23:37,000 --> 00:23:41,000
But do you think it doesn't change that much?

289
00:23:41,000 --> 00:23:44,000
So no one's really checked.

290
00:23:44,000 --> 00:23:47,000
This is just true of so many things in interpretability.

291
00:23:47,000 --> 00:23:53,000
It's like, well, you know, my intuition is that when you fine-tune a model,

292
00:23:53,000 --> 00:23:58,000
most of what is going on is that you're rearranging the internal circuitry.

293
00:23:58,000 --> 00:24:02,000
Say you fine-tune a Wikipedia, you up-weight the factual recall circuitry,

294
00:24:02,000 --> 00:24:05,000
you flesh it out a bit, you down-weight other stuff.

295
00:24:05,000 --> 00:24:08,000
And I think this can explain a lot of improved performance.

296
00:24:08,000 --> 00:24:11,000
But then if you fine-tune for much longer, you're basically just training the model

297
00:24:11,000 --> 00:24:18,000
and it will start to learn more circuitry, more features, more algorithms, more knowledge of the world.

298
00:24:18,000 --> 00:24:22,000
And yeah, but no one's really checked.

299
00:24:22,000 --> 00:24:28,000
And definitely the longer you fine-tune it and the more you're using weird techniques

300
00:24:28,000 --> 00:24:35,000
like reinforcement learning from human feedback, the less I'm confident in this claim.

301
00:24:35,000 --> 00:24:39,000
And yeah, if we discovered that every time you fine-tune a model,

302
00:24:39,000 --> 00:24:42,000
it will wildly change all of the internal circuitry,

303
00:24:42,000 --> 00:24:45,000
maybe like somewhat more pessimistic about Mechantup,

304
00:24:45,000 --> 00:24:51,000
unless we can get very good at the automated parts, which we might be able to get good at.

305
00:24:51,000 --> 00:24:55,000
I very much think of the field as we're trying to do this hard, ambitious thing.

306
00:24:55,000 --> 00:25:00,000
We're making a lot of progress, but I really wish we're making way more progress, way faster.

307
00:25:00,000 --> 00:25:03,000
And you, viewer, could help.

308
00:25:03,000 --> 00:25:09,000
But I don't know where the difficulty bar is for being useful

309
00:25:09,000 --> 00:25:14,000
or the difficulty bar is for being like incredibly, ambitiously useful.

310
00:25:14,000 --> 00:25:19,000
And it's plausible already at the point where Mechantup can do real useful things no one else can

311
00:25:19,000 --> 00:25:21,000
or no other techniques can.

312
00:25:21,000 --> 00:25:22,000
It's plausible.

313
00:25:22,000 --> 00:25:24,000
It will take like five years to get to that point.

314
00:25:24,000 --> 00:25:26,000
I don't really know.

315
00:25:26,000 --> 00:25:30,000
So I wanted to talk about this concept of needs and scruffies.

316
00:25:30,000 --> 00:25:35,000
So there have been two divisions in AI research going all the way back to the very, very beginning.

317
00:25:35,000 --> 00:25:42,000
And you've said that sometimes understanding specific circuits can teach us universal things about models

318
00:25:42,000 --> 00:25:45,000
which bear unimportant questions.

319
00:25:45,000 --> 00:25:49,000
So this reminds me of this dichotomy between the needs and the scruffies.

320
00:25:49,000 --> 00:25:55,000
Now you seem like a need to me, a need to someone who is quite puritanical

321
00:25:55,000 --> 00:25:57,000
and also it's related to universalism.

322
00:25:57,000 --> 00:26:04,000
So this idea that there are simple underlying principles that explain an awful lot of things

323
00:26:04,000 --> 00:26:12,000
rather than wanting to accept the gnarly kind of reality that everything's so bloody complicated.

324
00:26:12,000 --> 00:26:14,000
Where do you fall on that?

325
00:26:14,000 --> 00:26:19,000
So I definitely would not.

326
00:26:19,000 --> 00:26:20,000
Okay.

327
00:26:20,000 --> 00:26:23,000
So there's two separate things here.

328
00:26:23,000 --> 00:26:25,000
There's like, what's my aesthetic?

329
00:26:25,000 --> 00:26:27,000
Well, I want things to be neat.

330
00:26:27,000 --> 00:26:28,000
I want them to be beautiful.

331
00:26:28,000 --> 00:26:29,000
I want them to be mathematical.

332
00:26:29,000 --> 00:26:30,000
I want them to be elegant.

333
00:26:30,000 --> 00:26:31,000
Yes.

334
00:26:31,000 --> 00:26:33,000
And then there's what do I do in practice?

335
00:26:33,000 --> 00:26:35,000
And what do I believe is true about networks?

336
00:26:35,000 --> 00:26:42,000
Well, I think there is a lot more structure than most than many people think.

337
00:26:42,000 --> 00:26:48,000
But I also do not think they are just some beautiful purely algorithmic thing

338
00:26:48,000 --> 00:26:51,000
that we could uncover if we just knew the right tools.

339
00:26:51,000 --> 00:26:53,000
And like maybe they are.

340
00:26:53,000 --> 00:26:55,000
We'd fucking great if they were.

341
00:26:55,000 --> 00:27:01,000
But I expect they're messy and cursed, but with some deep structure and patterns

342
00:27:01,000 --> 00:27:06,000
and how much traction we can get on the weird scruffiness is like somewhat unclear to me.

343
00:27:06,000 --> 00:27:08,000
I think we can make a lot more progress than we have.

344
00:27:08,000 --> 00:27:11,000
But we might eventually hit a wall.

345
00:27:11,000 --> 00:27:14,000
You were saying something quite interesting when we drove over, which is,

346
00:27:14,000 --> 00:27:19,000
I mean, my friend Waleed Sabah, he's a linguist and he is a Platonist.

347
00:27:19,000 --> 00:27:25,000
He thinks that there are these universal cognitive priors and there's a hierarchy of them.

348
00:27:25,000 --> 00:27:28,000
And the complexity collapses.

349
00:27:28,000 --> 00:27:33,000
And he thinks that language models have somehow acquired these cognitive priors.

350
00:27:33,000 --> 00:27:37,000
And if we did some kind of symbolic decomposition, you know,

351
00:27:37,000 --> 00:27:40,000
we would all just kind of like pack itself into this beautiful hierarchy.

352
00:27:40,000 --> 00:27:43,000
And you were saying that there are Gabor filters and they're all these different circuits

353
00:27:43,000 --> 00:27:50,000
and they have motifs, they have categories, they have flavors for want of a better word.

354
00:27:50,000 --> 00:27:54,000
Are you optimistic that something like this could happen?

355
00:27:54,000 --> 00:27:56,000
Yeah.

356
00:27:56,000 --> 00:28:02,000
So, hmm.

357
00:28:02,000 --> 00:28:13,000
So, one interesting point here is often interoperability is fairly different

358
00:28:13,000 --> 00:28:16,000
for different modalities and different architectures.

359
00:28:16,000 --> 00:28:21,000
A lot of the early work was done on convolutional networks and image classifiers.

360
00:28:21,000 --> 00:28:25,000
The field very much nowadays focuses on transformer language models.

361
00:28:25,000 --> 00:28:32,000
And I think there's lots of structure to how transformers implement algorithms.

362
00:28:32,000 --> 00:28:37,000
Transformers cannot be recursive, but they're incredibly paralyzed.

363
00:28:37,000 --> 00:28:43,000
Transformers have this mechanism of attention that tells them how to move information between positions.

364
00:28:43,000 --> 00:28:49,000
And there's lots of algorithms and circuitry that can be expressed like this

365
00:28:49,000 --> 00:28:52,000
and lots of stuff that's really weird to express.

366
00:28:52,000 --> 00:28:58,000
And I think that this constrains them in a way that creates lots of interesting structure

367
00:28:58,000 --> 00:29:02,000
that can be understood and patterns that can be understood.

368
00:29:02,000 --> 00:29:06,000
Is this inherently true of intelligence? Who knows?

369
00:29:06,000 --> 00:29:13,000
But a lot of my optimism for structures within networks is more like that.

370
00:29:13,000 --> 00:29:18,000
But I try to think about structure more from a biologist's perspective

371
00:29:18,000 --> 00:29:22,000
than a mathematician's or like a philosopher's perspective.

372
00:29:22,000 --> 00:29:26,000
Though, I am a pure mathematician and I know nothing about biology.

373
00:29:26,000 --> 00:29:31,000
So, if anyone's listening to this, no stuff about biology and thinks I'm talking bullshit, please email.

374
00:29:31,000 --> 00:29:40,000
So, if you look at evolutionary biology, model organisms have all of this common shared structure.

375
00:29:40,000 --> 00:29:45,000
Like, most things have bones, we have cell nuclei.

376
00:29:45,000 --> 00:29:53,000
And hands of mammals tend to be surprisingly similar, but kind of weird and changed in various ways.

377
00:29:53,000 --> 00:29:58,000
And I don't know.

378
00:29:58,000 --> 00:30:01,000
I don't think these are like hard rules.

379
00:30:01,000 --> 00:30:04,000
Most of them have weird exceptions.

380
00:30:04,000 --> 00:30:08,000
And obviously a lot of this is due to the shared evolutionary history

381
00:30:08,000 --> 00:30:13,000
and is not just inherent to the substrate of you have proteins.

382
00:30:13,000 --> 00:30:17,000
Though, in fact, you often train these models on similar data in similar ways

383
00:30:17,000 --> 00:30:22,000
and they have the same architecture that constrains them to different kinds of algorithms.

384
00:30:22,000 --> 00:30:28,000
It makes me optimistic there's a biologist's level of a structure.

385
00:30:28,000 --> 00:30:31,000
Now, you said something interesting which is that transformers can't be used in a recursive way.

386
00:30:31,000 --> 00:30:35,000
Now, we'll just touch this very quickly because we've spoken about this a million times on different episodes.

387
00:30:35,000 --> 00:30:42,000
But, you know, there's the Chomsky hierarchy and he had this notion of a recursively enumerable language.

388
00:30:42,000 --> 00:30:47,000
And these different models, computational models in the Chomsky hierarchy,

389
00:30:47,000 --> 00:30:52,000
it's not only about being able to produce a language which exists in a certain set.

390
00:30:52,000 --> 00:30:57,000
It's also the ability to recognize that the language belongs in a certain set.

391
00:30:57,000 --> 00:31:04,000
And transformers are quite low down on that hierarchy because they're called recurrently not recursively.

392
00:31:04,000 --> 00:31:09,000
But I just wondered if you had any just, you know, prima facie if you had any views on that.

393
00:31:10,000 --> 00:31:15,000
Yeah, so I'm not a linguist. I'm not particularly familiar with the Chomsky hierarchy.

394
00:31:15,000 --> 00:31:20,000
I do think it's surprising how well transformers work.

395
00:31:20,000 --> 00:31:27,000
And I have a general skepticism of any theoretical hierarchy, like, I don't know.

396
00:31:27,000 --> 00:31:35,000
If you think there's some beautiful structure of algorithms and stuff that's low down,

397
00:31:36,000 --> 00:31:43,000
and then GPT-4 happens, I think a framework's wrong rather than transformers are wrong.

398
00:31:43,000 --> 00:31:50,000
Just massive stack of matrices plus a massive pile of data gives shockingly effective systems.

399
00:31:50,000 --> 00:31:54,000
Theoretical frameworks just often break when they make contact with reality.

400
00:31:54,000 --> 00:31:59,000
Well, that's certainly true. I mean, there's a famous expression that all grammars leak.

401
00:31:59,000 --> 00:32:04,000
I had rather, I don't know, I guess a similar conclusion to you, which is that if anything,

402
00:32:04,000 --> 00:32:08,000
it teaches us how sclerotic and predictable language is,

403
00:32:08,000 --> 00:32:15,000
and we don't actually need to have access to this infinite space or even exponentially large space.

404
00:32:15,000 --> 00:32:19,000
Most language use and most phenomena that we need, perhaps for intelligence,

405
00:32:19,000 --> 00:32:25,000
is surprisingly small and current models can work just well.

406
00:32:25,000 --> 00:32:28,000
Why don't we move on to your grokking work?

407
00:32:28,000 --> 00:32:37,000
So grokking is this sudden generalization that happens much later in training after...

408
00:32:37,000 --> 00:32:39,000
If I can add a brief clarification.

409
00:32:39,000 --> 00:32:40,000
Oh, yes, of course.

410
00:32:40,000 --> 00:32:44,000
So people often call grokking sudden generalization.

411
00:32:44,000 --> 00:32:46,000
My apologies. Go on.

412
00:32:46,000 --> 00:32:50,000
Sudden generalization is a much more common phenomena than grokking.

413
00:32:50,000 --> 00:32:54,000
It can just generally look like things like, I don't know, the model is trying to learn a task,

414
00:32:54,000 --> 00:32:56,000
it's kind of bad at it, and then it suddenly gets good at it.

415
00:32:56,000 --> 00:32:58,000
I prefer to call this a phase transition.

416
00:32:58,000 --> 00:33:05,000
Grokking is the specific thing where the model initially memorizes and does not generalize,

417
00:33:05,000 --> 00:33:11,000
and then there's a sudden phase transition in the test loss, the generalization ability,

418
00:33:11,000 --> 00:33:17,000
which creates a convergence after an initial divergence between train and test.

419
00:33:17,000 --> 00:33:22,000
And this is like a much, much more specific phenomena than sudden generalization.

420
00:33:22,000 --> 00:33:29,000
Okay, so you've spoken about three distinct phases of training underlying grokking.

421
00:33:29,000 --> 00:33:32,000
So why don't we go through them one by one?

422
00:33:32,000 --> 00:33:38,000
Yeah, so the context of this project, this was a paper called Progress Measures for Grokking

423
00:33:38,000 --> 00:33:45,000
by a Mechanistic Interpretability that I recently presented on at iClear.

424
00:33:46,000 --> 00:33:51,000
So we were studying a one-layer transformer, we trained to do modular addition,

425
00:33:51,000 --> 00:33:53,000
and it grokked modular addition.

426
00:33:53,000 --> 00:33:59,000
And the first thing we did was reverse engineer the algorithm behind how the model worked,

427
00:33:59,000 --> 00:34:03,000
which we may get into in a bit more detail, but at a very high level.

428
00:34:03,000 --> 00:34:08,000
Modular addition is equivalent to composing rotations around the unit circle.

429
00:34:08,000 --> 00:34:12,000
Composition adds the angle, circle gives you modularity.

430
00:34:12,000 --> 00:34:17,000
You can represent this by trig functions and do composition with trig identities

431
00:34:17,000 --> 00:34:23,000
and element-wise multiplication, and we reverse engineered exactly how the model did this.

432
00:34:23,000 --> 00:34:29,000
And then this mechanistic understanding was really important for understanding what was up with grokking,

433
00:34:29,000 --> 00:34:36,000
because the weird thing behind grokking is that it's not that the model memorizes,

434
00:34:36,000 --> 00:34:38,000
or that the model eventually generalizes.

435
00:34:38,000 --> 00:34:44,000
The surprising thing is that it first memorizes and then changes its mind and generalizes later.

436
00:34:44,000 --> 00:34:51,000
And generalization and memorization are two very different algorithms that both do very well on the training data,

437
00:34:51,000 --> 00:34:55,000
and only by understanding the mechanism will be able to disentangle them.

438
00:34:55,000 --> 00:35:00,000
And this meant we could look during training how much of the model's performance came from memorization

439
00:35:00,000 --> 00:35:02,000
and how much came from generalization.

440
00:35:02,000 --> 00:35:04,000
And we found these three distinct faces.

441
00:35:05,000 --> 00:35:07,000
There was memorization.

442
00:35:07,000 --> 00:35:12,000
The first very short phase, it gets phenomenally good train loss.

443
00:35:12,000 --> 00:35:18,000
It got to about 3e-7, which is an absolutely insane log loss.

444
00:35:18,000 --> 00:35:27,000
And much, much worse than random on test because memorization is very far from uniform and generalizes extremely badly.

445
00:35:27,000 --> 00:35:32,000
And then there was this long-seeming plateau.

446
00:35:33,000 --> 00:35:41,000
We call the space circuit formation because it turns out that rather than just continue to memorize for a while

447
00:35:41,000 --> 00:35:47,000
and doing a random walk through model space until it eventually gets lucky,

448
00:35:47,000 --> 00:35:53,000
the model is systematically transitioning from memorization to generalization.

449
00:35:53,000 --> 00:36:00,000
And you can see that its train performance gets worse and worse when you only let it memorize.

450
00:36:01,000 --> 00:36:04,000
And then, so why is test loss still bad?

451
00:36:04,000 --> 00:36:08,000
Test loss is bad because memorization generalizes terribly.

452
00:36:08,000 --> 00:36:14,000
And when the model is like, I don't know, two-thirds memorizing, one-third generalizing, it still does terribly.

453
00:36:14,000 --> 00:36:20,000
And it's only when the model gets so good at the trigger-based generalizing algorithm

454
00:36:20,000 --> 00:36:25,000
that it no longer needs the memorization parameters and cleans them up that we see grocking.

455
00:36:25,000 --> 00:36:29,000
And this happens fairly suddenly.

456
00:36:29,000 --> 00:36:37,000
But the, if you, we have this metric called restricted loss where we explicitly clean up the memorization for the model

457
00:36:37,000 --> 00:36:39,000
and look at how well it generalizes.

458
00:36:39,000 --> 00:36:43,000
And we see that restricted loss drops noticeably before test loss drops,

459
00:36:43,000 --> 00:36:48,000
showing that the drop is driven by cleaning up the noise.

460
00:36:48,000 --> 00:36:52,000
And this is striking because A.

461
00:36:52,000 --> 00:36:56,000
I had no idea it was even possible for a model to transition between two good solutions,

462
00:36:56,000 --> 00:36:59,000
maintaining equivalent performance throughout.

463
00:36:59,000 --> 00:37:05,000
B. There was this real mystery of deep learning that many people tried to answer,

464
00:37:05,000 --> 00:37:09,000
and mechanistic understanding was genuinely useful for answering it.

465
00:37:09,000 --> 00:37:13,000
And grocking was an illusion.

466
00:37:13,000 --> 00:37:15,000
It was not sudden generalization.

467
00:37:15,000 --> 00:37:18,000
It was gradual generalization followed by sudden cleanup.

468
00:37:18,000 --> 00:37:25,000
And test loss and test accuracy were just too course-metric to tell the difference.

469
00:37:25,000 --> 00:37:32,000
But we were able to design these hidden progress measures using our mechanistic understanding that made everything clear.

470
00:37:32,000 --> 00:37:38,000
And we also just have all kinds of pretty animations of qualitatively watching the circuits develop over training,

471
00:37:38,000 --> 00:37:40,000
and it's very pretty.

472
00:37:40,000 --> 00:37:41,000
So a few things.

473
00:37:41,000 --> 00:37:44,000
I mean, first of all, just going back to first principles.

474
00:37:44,000 --> 00:37:48,000
The biggest problem in machine learning is this concept called overfitting.

475
00:37:48,000 --> 00:37:52,000
And we trained the model on a training set.

476
00:37:52,000 --> 00:37:56,000
And there's this horrible phenomenon called the shortcut rule,

477
00:37:56,000 --> 00:38:00,000
which is that the model will take the path of least resistance.

478
00:38:00,000 --> 00:38:04,000
And when you're training it, it only really knows about the training set.

479
00:38:04,000 --> 00:38:10,000
And of course, we can test it on a different set afterwards, which we've held out.

480
00:38:10,000 --> 00:38:14,000
And just because of the way that we've structured the model,

481
00:38:14,000 --> 00:38:19,000
it may, by hook or by crook, generalize to the test set.

482
00:38:19,000 --> 00:38:22,000
But the interesting thing is that generalization isn't a binary.

483
00:38:22,000 --> 00:38:24,000
There's a whole spectrum of generalization.

484
00:38:24,000 --> 00:38:27,000
So it starts with the training set, and then we have the test set,

485
00:38:27,000 --> 00:38:30,000
and then the ideal is out-of-demand generalization.

486
00:38:30,000 --> 00:38:32,000
But I would go a step further.

487
00:38:32,000 --> 00:38:35,000
There's also algorithmic generalization,

488
00:38:35,000 --> 00:38:38,000
which is this notion that as I understand it,

489
00:38:38,000 --> 00:38:43,000
neural networks, if you model the function y equals x squared,

490
00:38:43,000 --> 00:38:50,000
it will only ever be able to learn the values of that function inside the training support.

491
00:38:50,000 --> 00:38:55,000
So presumably you're talking about the ideal form of generalization

492
00:38:55,000 --> 00:38:59,000
being not as good as algorithmic generalization,

493
00:38:59,000 --> 00:39:01,000
or do you think it could go all the way?

494
00:39:01,000 --> 00:39:08,000
So I think one thing which is very important to track

495
00:39:08,000 --> 00:39:15,000
is what the domain you're talking about is of which it's even possible to generalize.

496
00:39:15,000 --> 00:39:21,000
So I generally think about models that have discrete inputs rather than continuous inputs,

497
00:39:21,000 --> 00:39:26,000
because basically no neural network is going to be capable of dealing

498
00:39:26,000 --> 00:39:30,000
with like unbounded range continuous inputs.

499
00:39:31,000 --> 00:39:37,000
In modular addition, there were just two one-hot encoded inputs between 0 and 113,

500
00:39:37,000 --> 00:39:39,000
which is the modular I used.

501
00:39:39,000 --> 00:39:41,000
Yeah, the model has a fixed modular.

502
00:39:41,000 --> 00:39:43,000
It's not doing modular addition in general.

503
00:39:43,000 --> 00:39:48,000
And there's just like 12,000 inputs, and it learns to do all of them.

504
00:39:48,000 --> 00:39:52,000
And in, I don't know, behaviorally,

505
00:39:52,000 --> 00:39:55,000
you can't even tell the difference between the model memorizes everything

506
00:39:55,000 --> 00:39:58,000
and the model learns some true algorithm.

507
00:39:58,000 --> 00:40:01,000
Though, with the more cognitivist mechanistic approach,

508
00:40:01,000 --> 00:40:04,000
I can just look at it and say, yep, that's an algorithm.

509
00:40:04,000 --> 00:40:06,000
It's great. Not a stochastic parrot.

510
00:40:06,000 --> 00:40:08,000
Conclusively disprove that hypothesis.

511
00:40:10,000 --> 00:40:12,000
And yeah.

512
00:40:15,000 --> 00:40:19,000
I think that for the language models, it's more interesting,

513
00:40:19,000 --> 00:40:25,000
because I know GbD2, it's got 1,000 tokens, 50,000 vocab.

514
00:40:25,000 --> 00:40:29,000
It's like 50,000 to the power of 1,000 possible inputs.

515
00:40:30,000 --> 00:40:36,000
And there's a surprising amount of interesting algorithmic generalization.

516
00:40:37,000 --> 00:40:40,000
We're going to talk later about induction hits,

517
00:40:40,000 --> 00:40:45,000
which is the circuit language models learn to detect and continue repeated text.

518
00:40:45,000 --> 00:40:48,000
Like if given the word Neil, you want to know what comes next.

519
00:40:48,000 --> 00:40:52,000
Unfortunately, Nanda is not that homelist yet.

520
00:40:53,000 --> 00:40:57,000
But if Neil Nanda has come up like five times before in the text,

521
00:40:57,000 --> 00:40:59,000
Nanda's pretty likely to come next.

522
00:40:59,000 --> 00:41:07,000
And this transfers to, if you get the model, just random tokens,

523
00:41:07,000 --> 00:41:12,000
with some repetition, the model can predict the repeated random tokens,

524
00:41:12,000 --> 00:41:15,000
because the induction heads are just a real algorithm.

525
00:41:15,000 --> 00:41:21,000
And the space of possible repeated random tokens is like enormous.

526
00:41:21,000 --> 00:41:25,000
It's like, in some sense, much larger than the space of possible language.

527
00:41:25,000 --> 00:41:28,000
And is this algorithmic generalization?

528
00:41:28,000 --> 00:41:30,000
I don't really know. It depends on your perspective.

529
00:41:31,000 --> 00:41:36,000
Let's bring in this paper by Bilal Tughtay.

530
00:41:36,000 --> 00:41:40,000
So it was called a toy model of universality,

531
00:41:40,000 --> 00:41:43,000
reverse engineering how neural networks learn group operations,

532
00:41:43,000 --> 00:41:45,000
and you supervised that paper.

533
00:41:45,000 --> 00:41:50,000
And he was asking the question of whether neural networks learn universal solutions,

534
00:41:50,000 --> 00:41:52,000
or these idiosyncratic ones.

535
00:41:52,000 --> 00:41:54,000
And he said he found inherent randomness,

536
00:41:54,000 --> 00:41:57,000
but models could consistently learn group composition

537
00:41:57,000 --> 00:42:00,000
via an interpretable representation theory.

538
00:42:00,000 --> 00:42:03,000
So can you give us a quick tour de force of that work?

539
00:42:03,000 --> 00:42:04,000
Yeah.

540
00:42:04,000 --> 00:42:07,000
Maybe I should detour back to my grokking work

541
00:42:07,000 --> 00:42:09,000
and just explain the algorithm we found there

542
00:42:09,000 --> 00:42:11,000
and how we know it's the real algorithm.

543
00:42:11,000 --> 00:42:12,000
Yeah, sure.

544
00:42:12,000 --> 00:42:13,000
This is a good foundation for this paper.

545
00:42:13,000 --> 00:42:14,000
Sure, sure.

546
00:42:14,000 --> 00:42:19,000
Yeah, so we found this thing we call the Fourier multiplication algorithm.

547
00:42:19,000 --> 00:42:24,000
The very high level it composes rotations.

548
00:42:24,000 --> 00:42:28,000
You can actually look at how the different bits of the model implement the algorithm,

549
00:42:28,000 --> 00:42:30,000
and often just read this off.

550
00:42:30,000 --> 00:42:33,000
So the embeddings are just a lookup table

551
00:42:33,000 --> 00:42:38,000
mapping the one-hot encoded inputs to these trig terms.

552
00:42:38,000 --> 00:42:40,000
Sines and cosines are different frequencies.

553
00:42:40,000 --> 00:42:43,000
You can just read this off the embedding weights.

554
00:42:43,000 --> 00:42:47,000
Note, people often think that learning sine and cosine is hard.

555
00:42:47,000 --> 00:42:52,000
It's actually very easy because you only need it on 113 different data points,

556
00:42:52,000 --> 00:42:54,000
such as a lookup table.

557
00:42:54,000 --> 00:43:00,000
The model then uses the attention and MLPs to do this composition,

558
00:43:00,000 --> 00:43:04,000
to do the multiplication with trig identities

559
00:43:04,000 --> 00:43:10,000
to get the, like, composed rotation, the A plus B terms.

560
00:43:10,000 --> 00:43:16,000
And here we can just read off the neurons that they have learned these terms

561
00:43:16,000 --> 00:43:18,000
and that they were not there beforehand.

562
00:43:18,000 --> 00:43:23,000
The model is using its non-linearities in interesting ways to do this.

563
00:43:23,000 --> 00:43:30,000
It's also incredibly cursed because ReLUs are not designed to multiply two different inputs,

564
00:43:30,000 --> 00:43:35,000
but it turns out they can if you have enough of them and it's sufficiently cursed.

565
00:43:35,000 --> 00:43:39,000
And yeah, we can just read this off the neurons.

566
00:43:39,000 --> 00:43:45,000
Also, if you just put anything inside the model, it's beautiful and it's so periodic, and I love it.

567
00:43:46,000 --> 00:43:48,000
Could I touch on that though?

568
00:43:48,000 --> 00:43:54,000
Because you said you don't need to know the sine function because you can just memorise it within an interval.

569
00:43:54,000 --> 00:43:58,000
Is that, I don't know, how does that break down?

570
00:43:58,000 --> 00:44:06,000
Because it's discretising it and it's kind of assuming that it has the same behaviour in different intervals.

571
00:44:06,000 --> 00:44:17,000
So, I think a key thing here is that you are solving modular addition on discrete one-hot encoded inputs

572
00:44:17,000 --> 00:44:20,000
rather than for arbitrary continuous inputs.

573
00:44:20,000 --> 00:44:23,000
Arbitrary continuous inputs is way harder.

574
00:44:23,000 --> 00:44:28,000
And so you, it's not even on an interval, it's just learning snapshot.

575
00:44:28,000 --> 00:44:32,000
It's just learning, like, single points on the sine and cosine curves.

576
00:44:32,000 --> 00:44:41,000
And, I don't know, there's this family of maths about studying periodic functions with different kinds of Fourier transforms,

577
00:44:41,000 --> 00:44:52,000
and this is all discussing discrete Fourier transforms, which are just a reasonable way of looking at periodic sequences of length n.

578
00:44:52,000 --> 00:44:56,000
And that's how I recommend thinking about this one.

579
00:44:56,000 --> 00:45:03,000
It's kind of, like, just quite different from a model that's trying to learn the true sine and cosine function.

580
00:45:03,000 --> 00:45:13,000
And, yeah, the model then needs to convert the composed rotation back to the actual answer,

581
00:45:13,000 --> 00:45:17,000
which is an even more galaxy-brained operation that you can read off from the wits.

582
00:45:17,000 --> 00:45:22,000
So, you've got terms of the form cos A plus B.

583
00:45:22,000 --> 00:45:26,000
The model has some weights mapping to each output C.

584
00:45:26,000 --> 00:45:35,000
And it uses further trig identities to get terms of the form cos A plus B minus C times some frequency.

585
00:45:35,000 --> 00:45:38,000
And where A and B are the two inputs, C is the output.

586
00:45:38,000 --> 00:45:46,000
And you then use the softmax as an argmax to, like, extract the C that maximizes this.

587
00:45:46,000 --> 00:45:51,000
And because cos is maximized at zero, this is maximized at C equals A plus B.

588
00:45:51,000 --> 00:45:55,000
And if you choose the frequency right, this gets you mod N.

589
00:45:55,000 --> 00:45:59,000
And you can just read this off the model weights. It's great.

590
00:45:59,000 --> 00:46:03,000
And then, finally, you can verify you've understood it correctly,

591
00:46:03,000 --> 00:46:09,000
because if you ablate everything that our algorithm says should not matter, performance improves.

592
00:46:09,000 --> 00:46:14,000
While if you ablate any of the bits our algorithm says should matter, performance tanks.

593
00:46:14,000 --> 00:46:17,000
Okay. Could you give me some intuition, though?

594
00:46:17,000 --> 00:46:22,000
So, we start off in the memorization phase, because I guess you can think of a neural network

595
00:46:22,000 --> 00:46:26,000
as doing many different things in a very complicated way,

596
00:46:26,000 --> 00:46:30,000
and there's some kind of change in the balance during training.

597
00:46:30,000 --> 00:46:37,000
So, it does the easy thing first, and then it gradually learns how to generalize.

598
00:46:37,000 --> 00:46:42,000
And in this particular case, how does that thing, because we're using stochastic gradient descent,

599
00:46:42,000 --> 00:46:46,000
so we're moving all of these weights around, and the inductive prior is also very important

600
00:46:46,000 --> 00:46:49,000
and we'll come to that, I think, after we've spoken about Bill House paper.

601
00:46:49,000 --> 00:46:54,000
But how does that happen gradually in really simple terms?

602
00:46:54,000 --> 00:47:00,000
Hmm. Is the question kind of, it ends up at this discrete algorithm,

603
00:47:00,000 --> 00:47:03,000
but it does survive continuous steps. How does that work?

604
00:47:03,000 --> 00:47:07,000
Well, I think the thing that surprised a lot of people about grokking is this,

605
00:47:07,000 --> 00:47:11,000
I mean, grokking, the clue's in the name. So, it's gone from memorization,

606
00:47:11,000 --> 00:47:14,000
and then we're using stochastic gradient descent,

607
00:47:14,000 --> 00:47:18,000
and you would think that it's gotten stuck in some kind of local minima.

608
00:47:18,000 --> 00:47:22,000
And you're training, and you're training, and you're training, and then there's a spark.

609
00:47:22,000 --> 00:47:29,000
Something happens, and then you get these new modes, kind of like emerging in the network.

610
00:47:29,000 --> 00:47:32,000
I'm not sure if emerging is the right term.

611
00:47:32,000 --> 00:47:37,000
And it happens gradually, and it happens after a long time.

612
00:47:38,000 --> 00:47:43,000
Yeah. So, there's a couple of things here that's pretty easy to misunderstand.

613
00:47:43,000 --> 00:47:47,000
The first is that... Hmm.

614
00:47:47,000 --> 00:47:51,000
The first is that I think it's pretty hard for a model to ever get stuck,

615
00:47:51,000 --> 00:47:57,000
because, I know, this model had about 200,000 parameters, model ones have billions.

616
00:47:57,000 --> 00:48:01,000
It's just moving in a very high dimensional space,

617
00:48:01,000 --> 00:48:05,000
and you can get stuck on 150,000 dimensions.

618
00:48:05,000 --> 00:48:07,000
But you've got 50,000 to play with.

619
00:48:07,000 --> 00:48:14,000
And especially for a fairly over-parameterized model like this one,

620
00:48:14,000 --> 00:48:19,000
for a fairly simple task, there's just so much room to move around.

621
00:48:19,000 --> 00:48:24,000
Another common misunderstanding of grokking is people say,

622
00:48:24,000 --> 00:48:28,000
it's memorized, it's got zero loss, so why does it need to learn?

623
00:48:28,000 --> 00:48:30,000
Two misunderstandings here.

624
00:48:30,000 --> 00:48:35,000
First, zero loss is impossible unless you have bullshit floating point errors,

625
00:48:35,000 --> 00:48:40,000
because it's like the average correct log prop.

626
00:48:40,000 --> 00:48:43,000
Log of anything can never get to...

627
00:48:43,000 --> 00:48:48,000
The log will never quite get to zero because of just how softmax works.

628
00:48:48,000 --> 00:48:52,000
And you need to have an infinite logic for that to happen.

629
00:48:52,000 --> 00:48:56,000
The one cute thing in an appendix to our paper is that

630
00:48:57,000 --> 00:49:01,000
cannot represent log probes less than 1.19e-7,

631
00:49:01,000 --> 00:49:06,000
which leads to bizarre loss spikes sometimes, unless you use float64.

632
00:49:06,000 --> 00:49:10,000
Anyway, the second is regularization.

633
00:49:10,000 --> 00:49:14,000
If you don't have any kind of regularization, the model will just continue to memorize.

634
00:49:14,000 --> 00:49:18,000
We use weight decay, dropout also works,

635
00:49:18,000 --> 00:49:24,000
and so the model, the kind of core tension behind grokking

636
00:49:24,000 --> 00:49:30,000
is there's some feature of the lost landscape that makes it easier to get to memorization.

637
00:49:30,000 --> 00:49:37,000
You can memorize faster, while generalization is somehow hard to get to and much more gradual.

638
00:49:37,000 --> 00:49:42,000
So the model memorizes first, but it ultimately prefers to generalize,

639
00:49:42,000 --> 00:49:45,000
but it's only a mild preference.

640
00:49:45,000 --> 00:49:49,000
And the reason for this is we cherry pick the amount of data where it's a mild preference,

641
00:49:49,000 --> 00:49:54,000
because there's too little, it will just always memorize if there's too much, it will immediately generalize,

642
00:49:54,000 --> 00:49:57,000
because grokking is a little bit cheating.

643
00:49:57,000 --> 00:50:01,000
And yeah, you then use this,

644
00:50:01,000 --> 00:50:06,000
and because the model's initially memorized, but it wants to generalize,

645
00:50:06,000 --> 00:50:13,000
it can follow, it memorizes until the desire to memorize more balances with the desire to have smaller weights,

646
00:50:13,000 --> 00:50:18,000
but both of these reinforce the drive to generalize,

647
00:50:18,000 --> 00:50:22,000
because that makes both of them happier.

648
00:50:22,000 --> 00:50:28,000
And so the model very slowly interpolates, very, very slightly improving test loss,

649
00:50:28,000 --> 00:50:32,000
very slowly improving train loss, until it eventually gets there,

650
00:50:32,000 --> 00:50:36,000
and has this acceleration at the end, this phase transition,

651
00:50:36,000 --> 00:50:42,000
and cleanup, which leads to the seemingly sudden grokking behavior.

652
00:50:42,000 --> 00:50:46,000
Okay, and when you were talking about the, it wants the weights to be smaller,

653
00:50:46,000 --> 00:50:50,000
so that's weight decay, and it's like an inductive bias,

654
00:50:50,000 --> 00:50:54,000
essentially, to tell the model to reduce its complexity,

655
00:50:54,000 --> 00:50:57,000
which is a pressure to generalize.

656
00:50:57,000 --> 00:51:01,000
But if it wasn't for that, then that wouldn't happen.

657
00:51:01,000 --> 00:51:06,000
So in the experiments I ran, if you don't have weight decay,

658
00:51:06,000 --> 00:51:10,000
it will just keep memorizing infinitely far,

659
00:51:10,000 --> 00:51:13,000
because when you get perfect accuracy, if you double all your logits,

660
00:51:13,000 --> 00:51:16,000
you just get more confident in the right answer,

661
00:51:16,000 --> 00:51:18,000
and so it just keeps scaling up.

662
00:51:18,000 --> 00:51:21,000
I was using full batch training because it's such a tiny problem,

663
00:51:21,000 --> 00:51:24,000
this made things smoother and easier.

664
00:51:24,000 --> 00:51:27,000
I've heard some attic data that sometimes you can get it to work

665
00:51:27,000 --> 00:51:31,000
if you just have mini batch stochastic gradient descent,

666
00:51:31,000 --> 00:51:34,000
but I haven't looked into that particularly hard.

667
00:51:34,000 --> 00:51:35,000
Interesting.

668
00:51:35,000 --> 00:51:39,000
There are some hypotheses that stochasticity acts as an implicit regularizer

669
00:51:39,000 --> 00:51:41,000
because it adds noise.

670
00:51:41,000 --> 00:51:42,000
I don't really know.

671
00:51:42,000 --> 00:51:44,000
So let's go back to Bilal's paper then.

672
00:51:44,000 --> 00:51:47,000
So this paper, a toy model of universality,

673
00:51:47,000 --> 00:51:50,000
reverse engineering, how neural networks learn group operations.

674
00:51:50,000 --> 00:51:52,000
Can you give us an elevated pitch?

675
00:51:52,000 --> 00:51:59,000
Yeah, so an observation that actually first discovered at a party

676
00:51:59,000 --> 00:52:02,000
in the Bay Area from a guy called Sam Box,

677
00:52:02,000 --> 00:52:05,000
is that the modular addition algorithm we found

678
00:52:05,000 --> 00:52:08,000
is actually a representation theory algorithm.

679
00:52:08,000 --> 00:52:14,000
So group representations are collections of symmetries

680
00:52:14,000 --> 00:52:17,000
of some geometric objects that correspond to the group.

681
00:52:17,000 --> 00:52:20,000
Modular addition is the cyclic group,

682
00:52:20,000 --> 00:52:28,000
and rotations of the regular n-gon are the representations

683
00:52:28,000 --> 00:52:29,000
of the cyclic group,

684
00:52:29,000 --> 00:52:32,000
and this corresponds to the rotation by the unit circle

685
00:52:32,000 --> 00:52:34,000
that can pose that we found.

686
00:52:34,000 --> 00:52:37,000
But it turns out you can just make this work for arbitrary groups,

687
00:52:38,000 --> 00:52:41,000
you replace the two rotations with just two representations,

688
00:52:41,000 --> 00:52:44,000
you compose them, and the model,

689
00:52:44,000 --> 00:52:47,000
and it turns out the cos a plus b minus c thing

690
00:52:47,000 --> 00:52:50,000
is this math jargon called the character.

691
00:52:50,000 --> 00:52:52,000
You don't mean to unsat any of that,

692
00:52:52,000 --> 00:52:55,000
but it's very cute if, like me, you have a pure math degree.

693
00:52:55,000 --> 00:52:58,000
And for example,

694
00:52:58,000 --> 00:53:02,000
if you have the group of permutations of five elements,

695
00:53:02,000 --> 00:53:06,000
the 120 different ways to rearrange five objects,

696
00:53:07,000 --> 00:53:09,000
one example of representations of this

697
00:53:09,000 --> 00:53:15,000
are rotations and reflections of the four-dimensional tetrahedra,

698
00:53:15,000 --> 00:53:19,000
and if you train a one-hidden layer MLP to grok this

699
00:53:19,000 --> 00:53:20,000
and look inside,

700
00:53:20,000 --> 00:53:23,000
you're going to see these rotations that it's learned.

701
00:53:23,000 --> 00:53:25,000
It's gorgeous.

702
00:53:25,000 --> 00:53:27,000
And so the first half of that paper

703
00:53:27,000 --> 00:53:30,000
was just showing that the algorithm worked,

704
00:53:30,000 --> 00:53:33,000
showing that this was actually learned in practice.

705
00:53:34,000 --> 00:53:38,000
Then the more interesting bit was this focus on universality.

706
00:53:38,000 --> 00:53:41,000
So universality is this hypothesis,

707
00:53:41,000 --> 00:53:47,000
that models have some intrinsic solutions to a problem

708
00:53:47,000 --> 00:53:49,000
that many different models will converge on,

709
00:53:49,000 --> 00:53:52,000
at least given similar data and similar architectures,

710
00:53:52,000 --> 00:53:55,000
e.g. in image models,

711
00:53:55,000 --> 00:53:58,000
models will learn specific neurons that detect curves,

712
00:53:58,000 --> 00:54:00,000
and different models and different datasets

713
00:54:00,000 --> 00:54:02,000
seem to learn this similar thing.

714
00:54:03,000 --> 00:54:07,000
And here this was interesting

715
00:54:07,000 --> 00:54:12,000
because groups have a finite set of irreducible representations,

716
00:54:12,000 --> 00:54:14,000
maths theorem.

717
00:54:14,000 --> 00:54:15,000
You can enumerate these.

718
00:54:15,000 --> 00:54:17,000
There are that many of them.

719
00:54:17,000 --> 00:54:20,000
And for groups that are not modular addition,

720
00:54:20,000 --> 00:54:23,000
these are qualitatively different,

721
00:54:23,000 --> 00:54:26,000
like some of them act on a four-dimensional object,

722
00:54:26,000 --> 00:54:27,000
like the tetrahedron,

723
00:54:27,000 --> 00:54:30,000
some of them act on like 5D or 60 objects.

724
00:54:30,000 --> 00:54:32,000
Naively, some of them are simpler than others,

725
00:54:32,000 --> 00:54:34,000
but they're definitely different.

726
00:54:34,000 --> 00:54:37,000
So what we did is we asked ourselves the question,

727
00:54:37,000 --> 00:54:39,000
which one does the model learn?

728
00:54:39,000 --> 00:54:42,000
And we found that even if you just vary the random seed,

729
00:54:42,000 --> 00:54:45,000
the model will randomly choose a subset of themes

730
00:54:45,000 --> 00:54:46,000
each time to learn.

731
00:54:46,000 --> 00:54:48,000
And there's some structure,

732
00:54:48,000 --> 00:54:53,000
like it tends to learn some of them more often than others.

733
00:54:53,000 --> 00:54:57,000
A little bit maps to our insurance of notion of simplicity,

734
00:54:57,000 --> 00:54:58,000
but not that much.

735
00:54:58,000 --> 00:55:00,000
One of the updates I made in the paper

736
00:55:00,000 --> 00:55:02,000
is that simplicity is a really cursed concept,

737
00:55:02,000 --> 00:55:04,000
I don't understand very well,

738
00:55:04,000 --> 00:55:05,000
where I don't know.

739
00:55:05,000 --> 00:55:07,000
If you have rotations of a four-dimensional object,

740
00:55:07,000 --> 00:55:08,000
that seems simpler,

741
00:55:08,000 --> 00:55:11,000
but maybe the 60 object takes more dimensions

742
00:55:11,000 --> 00:55:13,000
but has better loss per unit weight norm,

743
00:55:13,000 --> 00:55:16,000
which is simpler, I don't know.

744
00:55:16,000 --> 00:55:18,000
But yeah, anyway,

745
00:55:18,000 --> 00:55:20,000
we found that each run of the model learns

746
00:55:20,000 --> 00:55:22,000
some combination of these circuits

747
00:55:22,000 --> 00:55:24,000
for the different representations.

748
00:55:24,000 --> 00:55:26,000
It's like normally more than one,

749
00:55:26,000 --> 00:55:28,000
the exact number varies,

750
00:55:28,000 --> 00:55:31,000
and which ones it learns is seemingly random each time,

751
00:55:31,000 --> 00:55:36,000
which suggests that all toy models lie to you, obviously.

752
00:55:36,000 --> 00:55:39,000
But if we're trying to reason about real networks,

753
00:55:39,000 --> 00:55:43,000
looking at this work might suggest the explanation,

754
00:55:43,000 --> 00:55:47,000
the hypothesis that if there are multiple ways

755
00:55:47,000 --> 00:55:48,000
to implement a circuit,

756
00:55:48,000 --> 00:55:50,000
which in practice they normally are,

757
00:55:50,000 --> 00:55:54,000
models may learn different ones of them,

758
00:55:54,000 --> 00:55:57,000
kind of for fairly random reasons,

759
00:55:57,000 --> 00:55:59,000
and the fully understanding one model

760
00:55:59,000 --> 00:56:02,000
will not perfectly transfer to another model.

761
00:56:02,000 --> 00:56:05,000
And I think there's loads of really interesting

762
00:56:05,000 --> 00:56:06,000
open questions here.

763
00:56:06,000 --> 00:56:09,000
Like, I don't know,

764
00:56:09,000 --> 00:56:11,000
people have done various work understanding

765
00:56:11,000 --> 00:56:14,000
different kinds of specific circuits and models,

766
00:56:14,000 --> 00:56:16,000
like the interoperability in the wild paper

767
00:56:16,000 --> 00:56:17,000
we'll get to later.

768
00:56:17,000 --> 00:56:20,000
What does this look like in other models?

769
00:56:20,000 --> 00:56:23,000
Often there's multiple ways to implement a circuit.

770
00:56:23,000 --> 00:56:25,000
Can you disentangle the two,

771
00:56:25,000 --> 00:56:27,000
do all models learn both,

772
00:56:27,000 --> 00:56:30,000
or do some models learn one, some learn the other?

773
00:56:30,000 --> 00:56:32,000
I don't really know.

774
00:56:32,000 --> 00:56:33,000
So a couple of questions.

775
00:56:33,000 --> 00:56:35,000
I mean, first of all, this is leading towards this idea

776
00:56:35,000 --> 00:56:37,000
that we were speaking about before,

777
00:56:37,000 --> 00:56:41,000
which is that even if different networks,

778
00:56:41,000 --> 00:56:43,000
slightly different problems or variations

779
00:56:43,000 --> 00:56:44,000
on the same problem,

780
00:56:44,000 --> 00:56:46,000
it could learn these algorithmic primitives.

781
00:56:46,000 --> 00:56:49,000
Now, the first observation here is that

782
00:56:50,000 --> 00:56:53,000
the inductive biases of the network

783
00:56:53,000 --> 00:56:56,000
differ massively, right?

784
00:56:56,000 --> 00:56:59,000
So to what extent do the inductive biases

785
00:56:59,000 --> 00:57:02,000
affect these primitives which are learned?

786
00:57:02,000 --> 00:57:04,000
Oh, so much.

787
00:57:04,000 --> 00:57:05,000
They do.

788
00:57:05,000 --> 00:57:06,000
So...

789
00:57:06,000 --> 00:57:08,000
Well, can I frame the question a little bit?

790
00:57:08,000 --> 00:57:10,000
Because this reminds me a lot of

791
00:57:10,000 --> 00:57:12,000
the geometric deep learning blueprint

792
00:57:12,000 --> 00:57:16,000
from Petir and Michael Bronstein and all those guys.

793
00:57:16,000 --> 00:57:18,000
And they were coming at this

794
00:57:18,000 --> 00:57:20,000
from exactly the same direction as you,

795
00:57:20,000 --> 00:57:23,000
that they said there's a representation of a domain,

796
00:57:23,000 --> 00:57:25,000
which is basically a symmetry group,

797
00:57:25,000 --> 00:57:28,000
and you can do all of these different transformations,

798
00:57:28,000 --> 00:57:31,000
and as long as they fall in different positions

799
00:57:31,000 --> 00:57:32,000
in the underlying domain,

800
00:57:32,000 --> 00:57:35,000
so they respect the structure, then it works.

801
00:57:35,000 --> 00:57:40,000
But all of those symmetries are effectively coded

802
00:57:40,000 --> 00:57:42,000
into the inductive prior.

803
00:57:42,000 --> 00:57:44,000
So, for example, if a CNN

804
00:57:44,000 --> 00:57:47,000
works on this gridded 2D manifold

805
00:57:47,000 --> 00:57:51,000
and it explicitly models

806
00:57:51,000 --> 00:57:53,000
translational equivalents

807
00:57:53,000 --> 00:57:56,000
and local connectivity and weight sharing and so on.

808
00:57:56,000 --> 00:57:58,000
So I guess what I'm saying is,

809
00:57:58,000 --> 00:58:01,000
you're talking about this four-dimensional tetrahedra,

810
00:58:01,000 --> 00:58:06,000
and that isn't explicitly modeled in an MLP.

811
00:58:06,000 --> 00:58:07,000
Not so.

812
00:58:07,000 --> 00:58:10,000
So how are you even recognizing

813
00:58:10,000 --> 00:58:12,000
that it's learning those symmetries?

814
00:58:12,000 --> 00:58:13,000
How are you even probing it?

815
00:58:13,000 --> 00:58:15,000
Maybe we should start with that.

816
00:58:15,000 --> 00:58:19,000
So I guess thing one, models are just smarter than you, man.

817
00:58:19,000 --> 00:58:22,000
Models can do a lot of weird stuff.

818
00:58:22,000 --> 00:58:25,000
I feel like the story of deep learning

819
00:58:25,000 --> 00:58:28,000
is people initially thought they needed to spoonfeed

820
00:58:28,000 --> 00:58:33,000
these models the right inductive biases over the data.

821
00:58:33,000 --> 00:58:35,000
And we've gradually realized,

822
00:58:35,000 --> 00:58:37,000
oh, wait, no, no, this is fine.

823
00:58:37,000 --> 00:58:38,000
The models can figure it out.

824
00:58:38,000 --> 00:58:40,000
For example, early on,

825
00:58:40,000 --> 00:58:43,000
image models were convolutional networks.

826
00:58:43,000 --> 00:58:46,000
You tell it the key information is nearby,

827
00:58:46,000 --> 00:58:48,000
and if you translate the image, it doesn't matter.

828
00:58:48,000 --> 00:58:51,000
And now everyone uses transformers, including for images,

829
00:58:51,000 --> 00:58:54,000
and transformers replace the convolutional mechanism

830
00:58:54,000 --> 00:58:56,000
with attention, where you're now saying,

831
00:58:56,000 --> 00:58:59,000
okay, one-sixth of your parameters

832
00:58:59,000 --> 00:59:01,000
are dedicated to figuring out

833
00:59:01,000 --> 00:59:04,000
where to move information between positions.

834
00:59:04,000 --> 00:59:06,000
Sometimes it'll be a convolution,

835
00:59:06,000 --> 00:59:08,000
and sometimes models do learn convolution,

836
00:59:08,000 --> 00:59:09,000
but often it won't be.

837
00:59:09,000 --> 00:59:11,000
And we want you,

838
00:59:11,000 --> 00:59:14,000
and you can now spend the parameters to figure this out.

839
00:59:14,000 --> 00:59:19,000
And I'm not very familiar with the geometric deep learning literature,

840
00:59:19,000 --> 00:59:21,000
but I generally am just kind of like,

841
00:59:21,000 --> 00:59:22,000
models can figure it out.

842
00:59:22,000 --> 00:59:25,000
The way we figured out that this was what's going on

843
00:59:25,000 --> 00:59:30,000
is kind of analogous to what we did in the modular audition case,

844
00:59:30,000 --> 00:59:32,000
where we just look at the embedding matrix

845
00:59:32,000 --> 00:59:35,000
and just read off the learned sine and cosine terms.

846
00:59:35,000 --> 00:59:38,000
Here we said, okay,

847
00:59:38,000 --> 00:59:44,000
the rotations of the 4D tetrahedron are these like 4x4 matrices.

848
00:59:44,000 --> 00:59:46,000
You can flatten this to a 16-dimensional vector.

849
00:59:46,000 --> 00:59:49,000
Let's probe for that linearly.

850
00:59:49,000 --> 00:59:51,000
And this kind of works,

851
00:59:51,000 --> 00:59:54,000
and you can probe for the different representations

852
00:59:54,000 --> 00:59:57,000
and basically see what's going on.

853
00:59:57,000 --> 01:00:01,000
Okay, I think that the thrust of the geometric deep learning stuff,

854
01:00:01,000 --> 01:00:04,000
or any inductive prior, comes back to the bias-variance trade-off.

855
01:00:04,000 --> 01:00:06,000
And the cursive dimensionality.

856
01:00:06,000 --> 01:00:09,000
So no one's saying, of course, an MLP,

857
01:00:09,000 --> 01:00:13,000
if you look at the function space that it can approximate,

858
01:00:13,000 --> 01:00:16,000
it's exponentially larger than that of a CNN.

859
01:00:16,000 --> 01:00:20,000
So it was always about sample efficiency.

860
01:00:20,000 --> 01:00:23,000
So, yeah, an MLP can learn anything,

861
01:00:23,000 --> 01:00:26,000
but we would never be able to train it for most problems.

862
01:00:26,000 --> 01:00:28,000
Mm-hmm.

863
01:00:28,000 --> 01:00:30,000
Yeah.

864
01:00:30,000 --> 01:00:35,000
So, I guess I maybe want to avoid going too deeply into this

865
01:00:35,000 --> 01:00:38,000
because I think the modular addition problem

866
01:00:38,000 --> 01:00:42,000
and the group problem is just a very weird problem.

867
01:00:42,000 --> 01:00:46,000
There's an algorithm that it's fairly natural for a model to learn

868
01:00:46,000 --> 01:00:52,000
with literally a single nonlinear step of the matrix multiply.

869
01:00:52,000 --> 01:00:55,000
One very cute result from the last paper

870
01:00:55,000 --> 01:00:58,000
is that the model can implement two 4x4 matrix multipliers

871
01:00:58,000 --> 01:01:03,000
with a single ReLU layer, which is very cute.

872
01:01:03,000 --> 01:01:07,000
But, yeah, there's a fairly natural algorithm to implement.

873
01:01:07,000 --> 01:01:09,000
That's a certain...

874
01:01:09,000 --> 01:01:13,000
Yeah, another useful intuition is that the more data you have,

875
01:01:13,000 --> 01:01:15,000
the more complex memorization gets,

876
01:01:15,000 --> 01:01:19,000
while generalization is exactly as complex at each point.

877
01:01:19,000 --> 01:01:22,000
And, yeah.

878
01:01:22,000 --> 01:01:24,000
So there's always going to be a crossover point

879
01:01:24,000 --> 01:01:27,000
where we have enough data where it is simpler to learn the circuit

880
01:01:27,000 --> 01:01:30,000
that generalizes.

881
01:01:30,000 --> 01:01:32,000
And I don't know.

882
01:01:32,000 --> 01:01:36,000
I'm hesitant to draw too much from toy models about the real problem.

883
01:01:36,000 --> 01:01:40,000
I guess one, two final points I'd want to just leave on this section.

884
01:01:40,000 --> 01:01:42,000
The first is I just want to re-emphasize.

885
01:01:42,000 --> 01:01:45,000
I did not do the toy model of the universality paper.

886
01:01:45,000 --> 01:01:48,000
I was supervising a mentee, Bella Chucktie, who did it.

887
01:01:48,000 --> 01:01:50,000
He did a fantastic job.

888
01:01:50,000 --> 01:01:52,000
So thanks, Bella, for your listening.

889
01:01:52,000 --> 01:01:56,000
Secondly, for the module addition case,

890
01:01:56,000 --> 01:01:59,000
I had no idea this outcome was going to be there when I went in.

891
01:01:59,000 --> 01:02:03,000
I just poked around, noticed the weird periodicity,

892
01:02:03,000 --> 01:02:06,000
realized it was using, I should apply Fourier transforms,

893
01:02:06,000 --> 01:02:09,000
and then the whole problem kind of fell together.

894
01:02:09,000 --> 01:02:13,000
And to me, the real takeaway of this paper is like,

895
01:02:13,000 --> 01:02:15,000
I don't give a fuck about GROCK.

896
01:02:15,000 --> 01:02:20,000
It is genuinely possible to understand what is going on in a model.

897
01:02:20,000 --> 01:02:23,000
You don't need to know what's going on in advance to discover this.

898
01:02:23,000 --> 01:02:29,000
And there is beautiful, non-trivial structure that can be understood.

899
01:02:29,000 --> 01:02:32,000
And who knows if this will happen in, like, actual full models.

900
01:02:32,000 --> 01:02:36,000
But to me, this is much more compelling than if we had nothing at all.

901
01:02:36,000 --> 01:02:37,000
Beautiful.

902
01:02:37,000 --> 01:02:38,000
Okay.

903
01:02:38,000 --> 01:02:39,000
And just before we move off the section,

904
01:02:39,000 --> 01:02:43,000
Biloud had a beautiful Twitter thread, actually.

905
01:02:43,000 --> 01:02:46,000
And he was talking about the potential for what he called

906
01:02:46,000 --> 01:02:50,000
a periodic table of universal circuits.

907
01:02:50,000 --> 01:02:53,000
And I actually think that's a really cool idea.

908
01:02:53,000 --> 01:02:55,000
So that would be amazing if that would work out.

909
01:02:55,000 --> 01:02:57,000
But he also brought up the lottery ticket hypothesis,

910
01:02:57,000 --> 01:03:00,000
and I've interviewed Jonathan Frankel.

911
01:03:00,000 --> 01:03:04,000
And the idea there is that some of this information

912
01:03:04,000 --> 01:03:07,000
might actually be encoded and understandable at initialization

913
01:03:07,000 --> 01:03:09,000
before you even start training.

914
01:03:09,000 --> 01:03:14,000
And apparently, you folks have found weak evidence

915
01:03:14,000 --> 01:03:17,000
for this in at least one group.

916
01:03:17,000 --> 01:03:18,000
Ah.

917
01:03:18,000 --> 01:03:19,000
All right.

918
01:03:19,000 --> 01:03:22,000
So a couple of things there.

919
01:03:22,000 --> 01:03:25,000
So this idea of a periodic table of circuits,

920
01:03:25,000 --> 01:03:29,000
I believe, is originated in this post called Circuits Zoom In

921
01:03:29,000 --> 01:03:32,000
from Chris Ola.

922
01:03:32,000 --> 01:03:33,000
We probably cannot claim credit.

923
01:03:33,000 --> 01:03:34,000
Good job, Chris.

924
01:03:34,000 --> 01:03:37,000
It's a beautifully evocative term.

925
01:03:37,000 --> 01:03:39,000
Yeah, the story of basically everything in Mac and Terp

926
01:03:39,000 --> 01:03:41,000
is, yeah, there was this Chris Ola paper from, like,

927
01:03:41,000 --> 01:03:44,000
two years ago that has it somewhere inside.

928
01:03:44,000 --> 01:03:47,000
Anthropic recently put out this beautiful blog post

929
01:03:47,000 --> 01:03:50,000
called Interpretability Dreams about their vision

930
01:03:50,000 --> 01:03:53,000
for the field of mechanistic interpretability

931
01:03:53,000 --> 01:03:55,000
and the kind of subtext.

932
01:03:55,000 --> 01:03:58,000
So they kept just quoting bits of old papers being like,

933
01:03:58,000 --> 01:04:00,000
so we already said this, but let's now, like,

934
01:04:00,000 --> 01:04:03,000
summarize it better and be clear about how this

935
01:04:03,000 --> 01:04:05,000
sits into our overall picture.

936
01:04:05,000 --> 01:04:09,000
Anyway, so, yeah, the idea of the periodic table is

937
01:04:09,000 --> 01:04:13,000
maybe there is just some finite list of ways a thing

938
01:04:13,000 --> 01:04:16,000
can be implemented naturally in a massive stack of matrices

939
01:04:16,000 --> 01:04:20,000
that we can enumerate by studying one or maybe several

940
01:04:20,000 --> 01:04:24,000
networks, understand them, and then compile all of this

941
01:04:24,000 --> 01:04:27,000
into something beautiful.

942
01:04:27,000 --> 01:04:29,000
And...

943
01:04:29,000 --> 01:04:33,000
which is kind of what we found in the representations case.

944
01:04:33,000 --> 01:04:36,000
Though here it was nice because there were

945
01:04:36,000 --> 01:04:40,000
genuinely a finite set that we could have fully enumerate.

946
01:04:40,000 --> 01:04:44,000
Regarding the lottery ticket stuff,

947
01:04:44,000 --> 01:04:48,000
I think this was a random observation I had on the

948
01:04:48,000 --> 01:04:52,000
Modular Edition case, partially inspired by a result

949
01:04:52,000 --> 01:04:56,000
from Eric Michaud at MIT, who was involved in some

950
01:04:56,000 --> 01:04:59,000
other papers on Grocking.

951
01:04:59,000 --> 01:05:03,000
And so what we found is that at the end of training,

952
01:05:03,000 --> 01:05:07,000
there are these directions in the weights that represent

953
01:05:07,000 --> 01:05:10,000
like the sine and cos terms of frequency,

954
01:05:10,000 --> 01:05:13,000
14 pi over 113.

955
01:05:13,000 --> 01:05:15,000
And...

956
01:05:15,000 --> 01:05:17,000
if you look at the embedding at the start and project one

957
01:05:17,000 --> 01:05:21,000
to these directions, it's like surprisingly circular.

958
01:05:21,000 --> 01:05:25,000
It's like the model has extracted those directions.

959
01:05:25,000 --> 01:05:30,000
And my wildly unsubstantiated hypothesis for why models

960
01:05:30,000 --> 01:05:34,000
just learn these algorithms and circuits at all

961
01:05:34,000 --> 01:05:37,000
is that there are some directions that if you

962
01:05:37,000 --> 01:05:41,000
deleted everything else would like form this beautiful circuit.

963
01:05:41,000 --> 01:05:45,000
This is kind of a trivial statement about linear algebra for the most part.

964
01:05:45,000 --> 01:05:48,000
And this underlying hidden circuit, each bit reinforces

965
01:05:48,000 --> 01:05:51,000
each other systematically because they're useful.

966
01:05:51,000 --> 01:05:54,000
Well, everything else is kind of noise, so it gets kind of gradually

967
01:05:54,000 --> 01:05:56,000
decayed.

968
01:05:56,000 --> 01:06:00,000
Over time, this will give you the circuit in a way that looks

969
01:06:00,000 --> 01:06:02,000
surprising and emergent.

970
01:06:02,000 --> 01:06:06,000
And this also can partially explain why phase transitions

971
01:06:06,000 --> 01:06:08,000
happen.

972
01:06:08,000 --> 01:06:10,000
There was a really good post from Adam German and Bach Schleggeres

973
01:06:10,000 --> 01:06:16,000
called on S-shaped curves, which argue that

974
01:06:16,000 --> 01:06:20,000
if you've got something that's like the composition of multiple different

975
01:06:20,000 --> 01:06:23,000
weight matrices, let's just say two of them,

976
01:06:23,000 --> 01:06:26,000
the gradient on the first is proportional to how good the

977
01:06:26,000 --> 01:06:28,000
second is and vice versa.

978
01:06:28,000 --> 01:06:31,000
So at the start, they both grow very slowly, but then they'll

979
01:06:31,000 --> 01:06:35,000
reinforce each other and eventually cascade as they're optimizing

980
01:06:35,000 --> 01:06:38,000
on the problem in a way that looks kind of sudden and S-shaped.

981
01:06:38,000 --> 01:06:41,000
And so my understanding is the original lottery ticket

982
01:06:41,000 --> 01:06:44,000
hypothesis is kind of discreet.

983
01:06:44,000 --> 01:06:47,000
It's looking on the neuron level and it's learning masks over

984
01:06:47,000 --> 01:06:49,000
weights and over neurons.

985
01:06:49,000 --> 01:06:53,000
And I'm kind of discussing and in some sense much more trivial

986
01:06:53,000 --> 01:06:57,000
version, where I'm not assuming there's some canonical basis

987
01:06:57,000 --> 01:06:59,000
of neurons.

988
01:06:59,000 --> 01:07:02,000
I'm saying, well, there's some directions in space that matter

989
01:07:02,000 --> 01:07:05,000
and if you delete all other directions, everything kind of

990
01:07:05,000 --> 01:07:09,000
works, which I think is a much more trivial statement,

991
01:07:09,000 --> 01:07:12,000
though the space of possible neurons is enormous.

992
01:07:12,000 --> 01:07:14,000
Though I don't know.

993
01:07:14,000 --> 01:07:16,000
One thing you want to be pretty careful of when discussing this

994
01:07:16,000 --> 01:07:21,000
the mask you learn is the computation since, and no,

995
01:07:21,000 --> 01:07:24,000
there's probably quite a lot of algorithms can be cleverly

996
01:07:24,000 --> 01:07:28,000
expressed with a mask over a Gaussian normal matrix.

997
01:07:28,000 --> 01:07:31,000
But I don't know.

998
01:07:31,000 --> 01:07:35,000
Part two, how do machine learning models represent their

999
01:07:35,000 --> 01:07:36,000
thoughts?

1000
01:07:36,000 --> 01:07:39,000
Now we're taught in machine learning 101 that neural networks

1001
01:07:39,000 --> 01:07:44,000
represent hypotheses which live on a geometric domain and

1002
01:07:44,000 --> 01:07:47,000
the objective priors learn to generalize symmetries which

1003
01:07:47,000 --> 01:07:49,000
exist on the underlying geometric domain.

1004
01:07:49,000 --> 01:07:53,000
And you're talking about them representing a space of

1005
01:07:53,000 --> 01:07:55,000
algorithms, which we're going to explore.

1006
01:07:55,000 --> 01:07:58,000
Now, one thing that I wanted to touch on is that they learn

1007
01:07:58,000 --> 01:08:01,000
the mapping to extensional attributes, not intentional

1008
01:08:01,000 --> 01:08:04,000
attributes, intentions, but with an S.

1009
01:08:04,000 --> 01:08:07,000
And we'll come back to what I mean by that in a second.

1010
01:08:07,000 --> 01:08:10,000
But I think it's quite popular for people to think of neural

1011
01:08:10,000 --> 01:08:13,000
networks principally as a kind of hash table.

1012
01:08:14,000 --> 01:08:16,000
Or locality sensitive hash table.

1013
01:08:16,000 --> 01:08:19,000
And the generalization part comes from the representation

1014
01:08:19,000 --> 01:08:24,000
mapping function, which is on this embedded Hilbert space,

1015
01:08:24,000 --> 01:08:28,000
which is the vector space of the attributes, which then

1016
01:08:28,000 --> 01:08:31,000
resolves a pointer to a static location on the underlying

1017
01:08:31,000 --> 01:08:32,000
geometric domain.

1018
01:08:32,000 --> 01:08:35,000
Now this can mimic an algorithm, especially when the

1019
01:08:35,000 --> 01:08:39,000
inductive prior itself is increasingly algorithmic like

1020
01:08:39,000 --> 01:08:42,000
a graph neural network, for example, which behaves in a

1021
01:08:42,000 --> 01:08:46,000
very similar way to a prototypical dynamic programming

1022
01:08:46,000 --> 01:08:47,000
algorithm.

1023
01:08:47,000 --> 01:08:49,000
There's some great work actually on algorithmic reasoning

1024
01:08:49,000 --> 01:08:52,000
by Petr Felichkovich, one of your colleagues now at

1025
01:08:52,000 --> 01:08:53,000
DeepMind.

1026
01:08:53,000 --> 01:08:58,000
But he showed in his algorithmic reasoning work that

1027
01:08:58,000 --> 01:09:02,000
transformers can't perform certain graph algorithms.

1028
01:09:02,000 --> 01:09:05,000
I think he gave Dykstra as an example and he said it's

1029
01:09:05,000 --> 01:09:08,000
because there's this aggregation function in a

1030
01:09:08,000 --> 01:09:11,000
transformer, which isn't in a GNN.

1031
01:09:11,000 --> 01:09:14,000
So I just wondered if you could kind of like compare and

1032
01:09:14,000 --> 01:09:19,000
contrast whether or not neural networks are performing

1033
01:09:19,000 --> 01:09:22,000
algorithmic generalization and the differences between

1034
01:09:22,000 --> 01:09:25,000
let's say GNNs and transformers.

1035
01:09:25,000 --> 01:09:30,000
Yeah, so I'm not very familiar with GNNs, so I'll probably

1036
01:09:30,000 --> 01:09:33,000
avoid commenting on GNNs versus transformers, so a fear of

1037
01:09:33,000 --> 01:09:35,000
embarrassing myself.

1038
01:09:35,000 --> 01:09:39,000
In terms of the underlying thing.

1039
01:09:39,000 --> 01:09:43,000
So I definitely think we have some pretty clear evidence at

1040
01:09:43,000 --> 01:09:47,000
this point that models are doing some genuine algorithms.

1041
01:09:47,000 --> 01:09:50,000
I don't know if I think my modular addition thing is a

1042
01:09:50,000 --> 01:09:52,000
pretty clear proof of concept of this.

1043
01:09:52,000 --> 01:09:58,000
Yeah, so one thing worth stressing is that I generally

1044
01:09:58,000 --> 01:10:01,000
think of models as having linear representations, more

1045
01:10:01,000 --> 01:10:04,000
than geometric representations.

1046
01:10:04,000 --> 01:10:09,000
So I think of an input to a model as having many

1047
01:10:09,000 --> 01:10:13,000
different possible features where features are kind of a

1048
01:10:13,000 --> 01:10:19,000
property of the input in an intentional sense, but which is

1049
01:10:19,000 --> 01:10:22,000
kind of a fuzzy and garbage definition.

1050
01:10:22,000 --> 01:10:25,000
So I prefer the existential definition of like an example

1051
01:10:25,000 --> 01:10:29,000
of a feature is like this bit of an image contains a curve

1052
01:10:29,000 --> 01:10:33,000
or this bit of an image corresponds to a car window or

1053
01:10:33,000 --> 01:10:37,000
this is the final token in Eiffel Tower or this corresponds

1054
01:10:37,000 --> 01:10:41,000
to a list variable and Python with at least four elements

1055
01:10:41,000 --> 01:10:44,000
and all kinds of stuff like that.

1056
01:10:44,000 --> 01:10:50,000
And well, I know this this scene is shaded blue because

1057
01:10:50,000 --> 01:10:53,000
someone put the wrong filter on the camera.

1058
01:10:53,000 --> 01:10:58,000
And yeah, I generally think of models as representing

1059
01:10:58,000 --> 01:11:01,000
features as linear directions in space.

1060
01:11:01,000 --> 01:11:06,000
And each input is a linear combination of these directions.

1061
01:11:06,000 --> 01:11:11,000
And this is kind of the classic words to vet framing, like

1062
01:11:11,000 --> 01:11:17,000
the king minus man equals queen minus woman thing where you

1063
01:11:17,000 --> 01:11:19,000
can kind of think of this as there being a gender direction

1064
01:11:19,000 --> 01:11:21,000
and there being a royalty direction.

1065
01:11:21,000 --> 01:11:25,000
And these are like the right units of analysis rather than

1066
01:11:25,000 --> 01:11:29,000
king, queen, man, women being the right units of analysis.

1067
01:11:29,000 --> 01:11:33,000
But where each of these is made up out of these underlying

1068
01:11:33,000 --> 01:11:35,000
linear representations.

1069
01:11:35,000 --> 01:11:40,000
And this is a fairly different perspective to the geometric

1070
01:11:40,000 --> 01:11:42,000
where are things in a manifold?

1071
01:11:42,000 --> 01:11:44,000
How close are they together in Euclidean space?

1072
01:11:44,000 --> 01:11:49,000
Because that's all kind of a global statement about how close

1073
01:11:49,000 --> 01:11:54,000
two things are where you're comparing all possible features

1074
01:11:54,000 --> 01:11:59,000
while I don't know the Eiffel Tower and the Colosseum are

1075
01:11:59,000 --> 01:12:02,000
close together in some conceptual space because they're both

1076
01:12:02,000 --> 01:12:05,000
European landmarks, but they're also very different because

1077
01:12:05,000 --> 01:12:08,000
France and Italy are fairly different countries in some

1078
01:12:08,000 --> 01:12:09,000
sense.

1079
01:12:09,000 --> 01:12:13,000
And maybe they're different on a bunch of other features or

1080
01:12:13,000 --> 01:12:14,000
one of them is two words.

1081
01:12:14,000 --> 01:12:17,000
The other is one word, which really matters in some ways.

1082
01:12:17,000 --> 01:12:23,000
And Euclidean distance and geometry is it's a gloomy

1083
01:12:23,000 --> 01:12:25,000
it's a global summary statistic.

1084
01:12:25,000 --> 01:12:28,000
And all summary statistics light you.

1085
01:12:28,000 --> 01:12:30,000
There's another motto of mine.

1086
01:12:30,000 --> 01:12:34,000
But in particular, global ones I'm very skeptical of.

1087
01:12:34,000 --> 01:12:41,000
And yeah, in general, this how what is the structure of a model

1088
01:12:41,000 --> 01:12:42,000
representations?

1089
01:12:42,000 --> 01:12:44,000
I think it's like a really important question.

1090
01:12:44,000 --> 01:12:49,000
And in particular, models are such high dimensional objects

1091
01:12:49,000 --> 01:12:54,000
that you really want to be careful to distinguish between the

1092
01:12:54,000 --> 01:13:01,000
two separate things of sorry models are such high dimensional

1093
01:13:01,000 --> 01:13:05,000
objects that it's basically impossible to understand GBD

1094
01:13:05,000 --> 01:13:07,000
three is a 200 billion dimensional vector.

1095
01:13:07,000 --> 01:13:12,000
You need to be breaking it down into units of analysis that

1096
01:13:12,000 --> 01:13:16,000
can vary independently and independently meaningful.

1097
01:13:16,000 --> 01:13:21,000
And the linear representation hypothesis is like a pretty

1098
01:13:21,000 --> 01:13:25,000
load bearing part of how I think about this stuff because it is

1099
01:13:25,000 --> 01:13:31,000
so because it allows you to break things down.

1100
01:13:31,000 --> 01:13:34,000
And it seems to be a true fact about how models do things.

1101
01:13:34,000 --> 01:13:37,000
Though again, we don't have that much data because we never

1102
01:13:37,000 --> 01:13:38,000
have enough data.

1103
01:13:38,000 --> 01:13:39,000
It's really sad.

1104
01:13:40,000 --> 01:13:45,000
And yeah.

1105
01:13:45,000 --> 01:13:47,000
Well, that's contrast a little bit.

1106
01:13:47,000 --> 01:13:51,000
So this linear representation hypothesis, this idea that the

1107
01:13:51,000 --> 01:13:55,000
models break down inputs into many independently varying features

1108
01:13:55,000 --> 01:13:59,000
and store them as directions in space, much like word to veck.

1109
01:13:59,000 --> 01:14:04,000
And the the go fi people, I mean, like photo and pollution,

1110
01:14:04,000 --> 01:14:08,000
they they they brought out this famous critique of connectionism

1111
01:14:08,000 --> 01:14:09,000
in 1988.

1112
01:14:09,000 --> 01:14:12,000
And their main argument was systematicity.

1113
01:14:12,000 --> 01:14:15,000
And they were talking about intention versus extension.

1114
01:14:15,000 --> 01:14:18,000
And it might just be worth defining what I mean by that.

1115
01:14:18,000 --> 01:14:23,000
So if I said the teacher of Socrates was Plato, the extension

1116
01:14:23,000 --> 01:14:24,000
is Plato.

1117
01:14:24,000 --> 01:14:27,000
The intention is everything.

1118
01:14:27,000 --> 01:14:28,000
It's the teacher.

1119
01:14:28,000 --> 01:14:29,000
It's Socrates.

1120
01:14:29,000 --> 01:14:32,000
You know, if I said four plus five equals nine, nine is the

1121
01:14:32,000 --> 01:14:35,000
extension for and plus and five is the intention.

1122
01:14:35,000 --> 01:14:37,000
So they were saying something very simple.

1123
01:14:37,000 --> 01:14:40,000
They said in a neural network, the intentional attributes get

1124
01:14:40,000 --> 01:14:41,000
discarded.

1125
01:14:41,000 --> 01:14:44,000
And that's why the network don't support what they call

1126
01:14:44,000 --> 01:14:45,000
compositionality.

1127
01:14:45,000 --> 01:14:49,000
Now, compositionality is actually quite an abstract term

1128
01:14:49,000 --> 01:14:53,000
because using vector algebra in these analogical reasoning tasks

1129
01:14:53,000 --> 01:14:54,000
that you were just talking about.

1130
01:14:54,000 --> 01:14:55,000
So king and queen and so on.

1131
01:14:55,000 --> 01:14:57,000
That's a form of compositionality.

1132
01:14:57,000 --> 01:15:01,000
But they would say it's a poor cousin of compositionality because

1133
01:15:01,000 --> 01:15:07,000
it's only using, you know, the representation is in a vector

1134
01:15:07,000 --> 01:15:08,000
space.

1135
01:15:08,000 --> 01:15:11,000
And in a vector space, you only have very basic primitive

1136
01:15:11,000 --> 01:15:12,000
transformations.

1137
01:15:12,000 --> 01:15:15,000
So you wouldn't be able to, I mean, for example, when you're

1138
01:15:15,000 --> 01:15:18,000
talking about Paris earlier, you wouldn't do the kind of

1139
01:15:18,000 --> 01:15:20,000
analogical reasoning they were talking about being able to

1140
01:15:20,000 --> 01:15:23,000
downstream, say, were they in Paris?

1141
01:15:23,000 --> 01:15:25,000
Is Paris in Europe?

1142
01:15:25,000 --> 01:15:29,000
Of course, it does happen in this linear representation theory.

1143
01:15:29,000 --> 01:15:33,000
But it happens in a very different way.

1144
01:15:33,000 --> 01:15:34,000
Hmm.

1145
01:15:34,000 --> 01:15:39,000
So I guess I'm not sure I fully followed that.

1146
01:15:39,000 --> 01:15:43,000
I mean, this might be a cheap gotcha, but a fact about

1147
01:15:43,000 --> 01:15:48,000
transformers is there's, they have this central object called

1148
01:15:48,000 --> 01:15:53,000
the residual stream, which I know in standard framing to be

1149
01:15:53,000 --> 01:15:56,000
thought of as the thing that lives in the skip connections.

1150
01:15:56,000 --> 01:16:00,000
But not even as like the key thing about a transform where

1151
01:16:00,000 --> 01:16:03,000
each layer reads its input from the residual stream and adds

1152
01:16:03,000 --> 01:16:05,000
its output back to the residual stream.

1153
01:16:05,000 --> 01:16:08,000
And the residual stream is kind of this shared bandwidth and

1154
01:16:08,000 --> 01:16:09,000
memory.

1155
01:16:09,000 --> 01:16:13,000
And this means that nothing's ever thrown away unless the

1156
01:16:13,000 --> 01:16:17,000
model explicitly is trying to do that, or is just applying

1157
01:16:17,000 --> 01:16:19,000
some gradual decay over time.

1158
01:16:19,000 --> 01:16:22,000
So, you know, if you've got an MLP layer that's saying I've got

1159
01:16:22,000 --> 01:16:25,000
four, I've got plus, I've got five, and I want to compute nine,

1160
01:16:25,000 --> 01:16:26,000
and plus is still there.

1161
01:16:26,000 --> 01:16:29,000
I don't know if this actually engage with your points and

1162
01:16:29,000 --> 01:16:33,000
like, I don't know if this matters, but it's true.

1163
01:16:33,000 --> 01:16:36,000
Yeah, what you're saying is true, but I think the point is

1164
01:16:36,000 --> 01:16:41,000
that those primitives are not actually representable in a

1165
01:16:41,000 --> 01:16:42,000
neural network.

1166
01:16:42,000 --> 01:16:45,000
So you're saying with this residual stream, all of the

1167
01:16:45,000 --> 01:16:48,000
extensions that came previously also get passed up.

1168
01:16:48,000 --> 01:16:52,000
So in a later layer, you can refer to an extension.

1169
01:16:52,000 --> 01:16:55,000
So the, basically the answer of a computation that happened

1170
01:16:55,000 --> 01:16:59,000
upstream, but what you can't refer to are the intentional

1171
01:16:59,000 --> 01:17:02,000
attributes of that computation upstream.

1172
01:17:02,000 --> 01:17:04,000
Why not?

1173
01:17:04,000 --> 01:17:06,000
Like four is an input.

1174
01:17:06,000 --> 01:17:08,000
So you can refer to four, because you could think of

1175
01:17:08,000 --> 01:17:11,000
reading the input as a computation.

1176
01:17:11,000 --> 01:17:13,000
Plus is another thing you read.

1177
01:17:13,000 --> 01:17:15,000
Five is another thing you read.

1178
01:17:15,000 --> 01:17:18,000
Like what is a thing that is not an output of a computation

1179
01:17:18,000 --> 01:17:20,000
within this framework?

1180
01:17:20,000 --> 01:17:23,000
I might have to get back to you on that.

1181
01:17:23,000 --> 01:17:27,000
Where's Keith Duggar when you need him?

1182
01:17:27,000 --> 01:17:32,000
What would be a good example of that?

1183
01:17:32,000 --> 01:17:37,000
I mean, I guess it's about symbol manipulation as well.

1184
01:17:37,000 --> 01:17:41,000
So these things could actually be symbolic operations which

1185
01:17:41,000 --> 01:17:44,000
can be composed and reused later.

1186
01:17:44,000 --> 01:17:47,000
And you would appreciate that a neural network is only ever

1187
01:17:47,000 --> 01:17:49,000
passing values.

1188
01:17:49,000 --> 01:17:53,000
So for example, if it did something which you could represent

1189
01:17:53,000 --> 01:17:56,000
with a symbolic operation, if you wanted to use that again,

1190
01:17:56,000 --> 01:18:01,000
I mean in an MLP, the reason why we use a CNN is because we

1191
01:18:01,000 --> 01:18:04,000
want to represent the same thing in different places.

1192
01:18:04,000 --> 01:18:06,000
And an MLP would have to learn it.

1193
01:18:06,000 --> 01:18:08,000
It doesn't support translational equivalence.

1194
01:18:08,000 --> 01:18:11,000
So it would have to learn the same thing a million times.

1195
01:18:11,000 --> 01:18:14,000
And it's the same thing with this symbolic compositional

1196
01:18:14,000 --> 01:18:18,000
generalization that if it actually had this symbolic

1197
01:18:18,000 --> 01:18:22,000
representation which it used once, it could use it everywhere.

1198
01:18:22,000 --> 01:18:25,000
But now it has to relearn it everywhere.

1199
01:18:25,000 --> 01:18:27,000
Right.

1200
01:18:27,000 --> 01:18:33,000
Like you could, if the model wants to know that Paris

1201
01:18:33,000 --> 01:18:37,000
is the capital of France, it can spend some parameters on that.

1202
01:18:37,000 --> 01:18:39,000
And for every other capital it needs to separately spend

1203
01:18:39,000 --> 01:18:43,000
parameters and it can't just have a general map country to

1204
01:18:43,000 --> 01:18:45,000
have a capital operation.

1205
01:18:45,000 --> 01:18:47,000
Yeah, that's exactly right.

1206
01:18:47,000 --> 01:18:49,000
Let's use a simple example.

1207
01:18:49,000 --> 01:18:54,000
So we use an MLP image classifier and I put a tennis ball in

1208
01:18:54,000 --> 01:18:57,000
and it's in the bottom left of the visual field.

1209
01:18:57,000 --> 01:19:02,000
And then I put it in the top right and nothing it's learned

1210
01:19:02,000 --> 01:19:05,000
from the bottom left will be used.

1211
01:19:05,000 --> 01:19:09,000
So it just feels like we're wasting the representational

1212
01:19:09,000 --> 01:19:11,000
capacity just doing the same thing again and again.

1213
01:19:11,000 --> 01:19:16,000
And in a transformer, the only reason it does have that

1214
01:19:16,000 --> 01:19:20,000
recognition, that that's a equivalence in respect of the

1215
01:19:20,000 --> 01:19:23,000
position of a pattern is because of the transformer

1216
01:19:23,000 --> 01:19:25,000
inductive prior, presumably.

1217
01:19:25,000 --> 01:19:27,000
Yes.

1218
01:19:27,000 --> 01:19:31,000
So it uses the same parameters at each position in the input

1219
01:19:31,000 --> 01:19:32,000
sequence.

1220
01:19:32,000 --> 01:19:35,000
It should be able to do bottom left and top right properly,

1221
01:19:35,000 --> 01:19:39,000
though it does not necessarily have things like rotation built

1222
01:19:39,000 --> 01:19:40,000
in.

1223
01:19:40,000 --> 01:19:42,000
I don't know.

1224
01:19:42,000 --> 01:19:45,000
I feel like machine learning is full of these people who have

1225
01:19:45,000 --> 01:19:47,000
all kinds of theoretical arguments.

1226
01:19:47,000 --> 01:19:49,000
And then they're like, this should be efficient.

1227
01:19:49,000 --> 01:19:50,000
This should not work.

1228
01:19:50,000 --> 01:19:53,000
And then GPT-4 lobs at them.

1229
01:19:53,000 --> 01:19:55,000
And I don't know.

1230
01:19:55,000 --> 01:20:00,000
No theory is interesting and isolation unless it models

1231
01:20:00,000 --> 01:20:02,000
reality well.

1232
01:20:02,000 --> 01:20:04,000
And I don't know.

1233
01:20:04,000 --> 01:20:06,000
I haven't really engaged with this theory in the same way I

1234
01:20:06,000 --> 01:20:09,000
haven't engaged with most deep learning theory, because it

1235
01:20:09,000 --> 01:20:12,000
just doesn't seem to meet my bar of does this make real

1236
01:20:12,000 --> 01:20:14,000
predictions about models?

1237
01:20:14,000 --> 01:20:17,000
The maximal update parameterization paper from Greg

1238
01:20:17,000 --> 01:20:20,000
Yang was actually a recent contradiction to this.

1239
01:20:20,000 --> 01:20:21,000
Right.

1240
01:20:21,000 --> 01:20:24,000
Of really interesting theory that makes real predictions

1241
01:20:24,000 --> 01:20:27,000
about models that bear out and get you zero shot hyper parameter

1242
01:20:27,000 --> 01:20:28,000
to transfer.

1243
01:20:28,000 --> 01:20:31,000
But like most things just don't do that.

1244
01:20:31,000 --> 01:20:33,000
Very interesting.

1245
01:20:33,000 --> 01:20:34,000
Okay.

1246
01:20:34,000 --> 01:20:35,000
Okay.

1247
01:20:35,000 --> 01:20:38,000
Well, I think now is a beautiful opportunity to move over to

1248
01:20:38,000 --> 01:20:39,000
Othello.

1249
01:20:39,000 --> 01:20:42,000
Now, there was a recent paper called do large language models

1250
01:20:42,000 --> 01:20:45,000
learn world models, or are they just surface statistics by

1251
01:20:45,000 --> 01:20:46,000
Kenneth Lee?

1252
01:20:46,000 --> 01:20:49,000
And he said that the recent increase in model and data size

1253
01:20:49,000 --> 01:20:52,000
has brought about qualitatively new behaviors such as writing

1254
01:20:52,000 --> 01:20:55,000
code or solving logic puzzles.

1255
01:20:55,000 --> 01:20:58,000
Now, he asked the question, yeah, how do these models achieve

1256
01:20:58,000 --> 01:20:59,000
this kind of performance?

1257
01:20:59,000 --> 01:21:01,000
Do they merely memorize training data?

1258
01:21:01,000 --> 01:21:04,000
Well, are they picking up the rules of English grammar and

1259
01:21:04,000 --> 01:21:06,000
grammar and the syntax of the sea language?

1260
01:21:06,000 --> 01:21:09,000
For example, are they building something akin to an internal

1261
01:21:09,000 --> 01:21:13,000
world model, an understandable model of the process producing

1262
01:21:13,000 --> 01:21:14,000
the sequences?

1263
01:21:14,000 --> 01:21:17,000
And he said that some researchers argue that this is fundamentally

1264
01:21:17,000 --> 01:21:21,000
impossible for models trained with guess the next word to learn

1265
01:21:21,000 --> 01:21:25,000
the language, meanings of language, and their performances is

1266
01:21:25,000 --> 01:21:28,000
merely surface statistics, you know, which is to say a long list

1267
01:21:28,000 --> 01:21:32,000
of correlations that do not reflect a causal model of the

1268
01:21:32,000 --> 01:21:34,000
process generating the sequence.

1269
01:21:34,000 --> 01:21:38,000
Now, you said, Neil, that a major source of excitement about

1270
01:21:38,000 --> 01:21:41,000
the original Othello paper was that it showed that predicting

1271
01:21:41,000 --> 01:21:45,000
the next word spontaneously learned the underlying structure

1272
01:21:45,000 --> 01:21:46,000
generating its data.

1273
01:21:46,000 --> 01:21:49,000
And you said that the obvious inference is that a large

1274
01:21:49,000 --> 01:21:52,000
language model trained to predict the next token may

1275
01:21:52,000 --> 01:21:54,000
spontaneously model the world.

1276
01:21:54,000 --> 01:21:55,000
What do you think?

1277
01:21:55,000 --> 01:21:57,000
Uh, yes.

1278
01:21:57,000 --> 01:22:01,000
So I should clarify that paragraph was me modeling why other

1279
01:22:01,000 --> 01:22:03,000
people are excited about paper.

1280
01:22:03,000 --> 01:22:04,000
Okay.

1281
01:22:04,000 --> 01:22:06,000
But whatever, I can roll with this question.

1282
01:22:06,000 --> 01:22:10,000
So and maybe bring in your less wrong piece as well.

1283
01:22:10,000 --> 01:22:11,000
Yeah.

1284
01:22:11,000 --> 01:22:12,000
Yes.

1285
01:22:12,000 --> 01:22:15,000
So the, yeah, I thought careless paper was super interesting.

1286
01:22:15,000 --> 01:22:20,000
The exact setup was they train.

1287
01:22:20,000 --> 01:22:24,000
So Othello is this chess and go like board game.

1288
01:22:24,000 --> 01:22:29,000
They took a data set of random legal moves in Othello.

1289
01:22:29,000 --> 01:22:34,000
They trained a model to predict the next move given a bunch

1290
01:22:34,000 --> 01:22:36,000
of these transcripts.

1291
01:22:36,000 --> 01:22:39,000
And then they probed the model and found that it had learned a

1292
01:22:39,000 --> 01:22:42,000
model of the board state, despite only ever being told to

1293
01:22:42,000 --> 01:22:44,000
predict the next move.

1294
01:22:44,000 --> 01:22:48,000
And so the way I would define world model is that there's

1295
01:22:48,000 --> 01:22:53,000
some latent variables that generate the training data.

1296
01:22:53,000 --> 01:22:58,000
Um, in this case, what the state of the board is, um, these

1297
01:22:58,000 --> 01:23:02,000
change over time, like over the sequence, but at least for a

1298
01:23:02,000 --> 01:23:07,000
transform, which has a sequence and the model kind of has an

1299
01:23:07,000 --> 01:23:09,000
internal representation of this at each point.

1300
01:23:09,000 --> 01:23:11,000
And they showed that you can probe for this.

1301
01:23:11,000 --> 01:23:16,000
And they showed that you can causally intervene on this and the

1302
01:23:16,000 --> 01:23:19,000
model will make legal moves in the new board, even if the

1303
01:23:19,000 --> 01:23:21,000
board status impossible to reach.

1304
01:23:21,000 --> 01:23:23,000
Point of order.

1305
01:23:23,000 --> 01:23:25,000
Can you explain what you mean by probe?

1306
01:23:25,000 --> 01:23:27,000
Yes.

1307
01:23:27,000 --> 01:23:31,000
So probing is this like old family of interoperability

1308
01:23:31,000 --> 01:23:32,000
techniques.

1309
01:23:32,000 --> 01:23:37,000
The idea is you think a model has represented something like you

1310
01:23:37,000 --> 01:23:42,000
give it a picture and you tell it as a class by the image and

1311
01:23:42,000 --> 01:23:45,000
you want to see if it's figured out that the picture is of a

1312
01:23:45,000 --> 01:23:47,000
red thing versus a blue thing, even though this isn't an

1313
01:23:47,000 --> 01:23:49,000
explicit part of the output.

1314
01:23:49,000 --> 01:23:55,000
You take some neuron or layer or just any internal vector of

1315
01:23:55,000 --> 01:24:01,000
the model and you train some classifier to map that to like

1316
01:24:01,000 --> 01:24:05,000
red or blue and you do something like a logistic regression

1317
01:24:05,000 --> 01:24:08,000
to see if you can extract whether it's red or blue from them.

1318
01:24:08,000 --> 01:24:12,000
And, uh, there's also interesting enough about probing, but

1319
01:24:12,000 --> 01:24:14,000
I should probably finish explaining the Othello paper first

1320
01:24:14,000 --> 01:24:16,000
before I get into that tangent.

1321
01:24:16,000 --> 01:24:20,000
So, yeah, the like reason people are really excited about this

1322
01:24:20,000 --> 01:24:24,000
paper was recently an oral eichle and generally got a lot of

1323
01:24:24,000 --> 01:24:28,000
hype was that it was just you train something predict the next

1324
01:24:28,000 --> 01:24:32,000
token and it forms this rich emergent model of the world.

1325
01:24:32,000 --> 01:24:35,000
And forming a model of the world is actually incredibly

1326
01:24:35,000 --> 01:24:36,000
expensive.

1327
01:24:36,000 --> 01:24:41,000
They like each cell of the 64 cell Othello board has three

1328
01:24:41,000 --> 01:24:45,000
possible states, three to the 64, it's quite a lot of information

1329
01:24:45,000 --> 01:24:48,000
to represent, but the model did it.

1330
01:24:48,000 --> 01:24:51,000
And lots of people were like, oh, clearly language models have

1331
01:24:51,000 --> 01:24:53,000
walls.

1332
01:24:53,000 --> 01:24:59,000
My personal interpretation of all this is that language models

1333
01:24:59,000 --> 01:25:00,000
predict the next token.

1334
01:25:00,000 --> 01:25:04,000
They learn effective algorithms for doing this within the

1335
01:25:04,000 --> 01:25:08,000
constraints of what is natural to represent within transformer

1336
01:25:08,000 --> 01:25:09,000
layers.

1337
01:25:09,000 --> 01:25:15,000
And what this means is that if predicting the next token is

1338
01:25:15,000 --> 01:25:19,000
made easier by having a model of the world of like, I don't

1339
01:25:19,000 --> 01:25:23,000
know who the speaker is, this is a thing that will happen.

1340
01:25:23,000 --> 01:25:27,000
And in some work led by Wes Gurney that we're going to talk

1341
01:25:27,000 --> 01:25:30,000
about later, we found neurons that detected things like this

1342
01:25:30,000 --> 01:25:33,000
Texas in French, this Texas Python code.

1343
01:25:33,000 --> 01:25:37,000
And in some sense, this is like a particularly trivial

1344
01:25:37,000 --> 01:25:38,000
model.

1345
01:25:38,000 --> 01:25:45,000
And so, yeah, that's an interesting thing.

1346
01:25:45,000 --> 01:25:48,000
In my opinion, it was kind of a priori obvious that language

1347
01:25:48,000 --> 01:25:53,000
models would learn this if they could and needed to, and it was

1348
01:25:53,000 --> 01:25:55,000
more efficient.

1349
01:25:55,000 --> 01:25:58,000
And at the point forward, though, learning that something is

1350
01:25:58,000 --> 01:26:01,000
French seems categorically different.

1351
01:26:01,000 --> 01:26:06,000
Because when I read Kenneth's original piece, he showed what

1352
01:26:06,000 --> 01:26:10,000
looked like a topological representation of the world.

1353
01:26:10,000 --> 01:26:14,000
So how different state spaces were related to each other in a

1354
01:26:14,000 --> 01:26:17,000
kind of network structure.

1355
01:26:17,000 --> 01:26:20,000
Hmm.

1356
01:26:20,000 --> 01:26:23,000
So.

1357
01:26:23,000 --> 01:26:27,000
I wonder if you can remember how we produced that diagram.

1358
01:26:27,000 --> 01:26:29,000
Yeah, so I'm starting to remember the details.

1359
01:26:29,000 --> 01:26:31,000
I think it was something of the form.

1360
01:26:31,000 --> 01:26:34,000
Look at how different cells are represented in the model and look

1361
01:26:34,000 --> 01:26:37,000
at how close together the representations of different

1362
01:26:37,000 --> 01:26:41,000
cells are, and the model has kind of got internal representations

1363
01:26:41,000 --> 01:26:43,000
that are close together.

1364
01:26:43,000 --> 01:26:46,000
I don't think this is fundamentally different from the

1365
01:26:46,000 --> 01:26:48,000
king, queen, man, women thing.

1366
01:26:48,000 --> 01:26:50,000
It's just that it's like learn some structural representations

1367
01:26:50,000 --> 01:26:52,000
that's obviously kind of reasonable.

1368
01:26:52,000 --> 01:26:53,000
Yeah.

1369
01:26:53,000 --> 01:26:57,000
I wouldn't read too much into that.

1370
01:26:57,000 --> 01:27:00,000
Models learn structural representations, I think, as old

1371
01:27:00,000 --> 01:27:03,000
news at this point.

1372
01:27:03,000 --> 01:27:07,000
But maybe another interesting angle is that one of the reasons

1373
01:27:07,000 --> 01:27:13,000
why people like Gary Marcus, they say GPT is parasitic on the

1374
01:27:13,000 --> 01:27:14,000
data.

1375
01:27:14,000 --> 01:27:19,000
They say because they are empirical models, most of the meaning,

1376
01:27:19,000 --> 01:27:23,000
most of the information is not in the data if we have to reason

1377
01:27:23,000 --> 01:27:25,000
over explicit world models.

1378
01:27:25,000 --> 01:27:28,000
So he thinks the reason a GPS is so good is because we've

1379
01:27:28,000 --> 01:27:31,000
imputed this abstract world model.

1380
01:27:31,000 --> 01:27:35,000
And similarly, when we play chess, we have an abstract world

1381
01:27:35,000 --> 01:27:36,000
model.

1382
01:27:36,000 --> 01:27:39,000
And he would argue that the information about that abstract

1383
01:27:39,000 --> 01:27:41,000
world model doesn't exist in any data.

1384
01:27:41,000 --> 01:27:44,000
So how do you go from the data to the model?

1385
01:27:44,000 --> 01:27:47,000
And the Othello games seem to show that you could go from the

1386
01:27:47,000 --> 01:27:49,000
data to the model.

1387
01:27:49,000 --> 01:27:50,000
Yeah.

1388
01:27:50,000 --> 01:27:53,000
I think that viewpoint is just obviously wrong.

1389
01:27:53,000 --> 01:27:58,000
Like, you're trying to do a data prediction problem.

1390
01:27:58,000 --> 01:28:03,000
A valid solution to that is to model the underlying world and

1391
01:28:03,000 --> 01:28:05,000
use this to predict what comes next.

1392
01:28:05,000 --> 01:28:07,000
There's clearly enough information in an information

1393
01:28:07,000 --> 01:28:09,000
theoretic sense to do this.

1394
01:28:09,000 --> 01:28:14,000
And the question is, is a model capable of doing that or not?

1395
01:28:14,000 --> 01:28:16,000
And I don't know.

1396
01:28:16,000 --> 01:28:19,000
I'm just like, you can't write poetry with statistical

1397
01:28:19,000 --> 01:28:20,000
correlations.

1398
01:28:20,000 --> 01:28:22,000
You need to be learning something.

1399
01:28:22,000 --> 01:28:24,000
Maybe that's not a good example.

1400
01:28:24,000 --> 01:28:26,000
I don't believe you can write like...

1401
01:28:26,000 --> 01:28:28,000
Yes, you can.

1402
01:28:28,000 --> 01:28:34,000
I don't believe you can produce, like, good answers to, like,

1403
01:28:34,000 --> 01:28:37,000
difficult code forces problems.

1404
01:28:37,000 --> 01:28:39,000
It's like, do good software engineering.

1405
01:28:39,000 --> 01:28:44,000
There's purely a bundle of statistical correlations.

1406
01:28:44,000 --> 01:28:46,000
Maybe I have too much respect for software engineers.

1407
01:28:46,000 --> 01:28:48,000
I don't know.

1408
01:28:48,000 --> 01:28:50,000
So where does it come from then?

1409
01:28:50,000 --> 01:28:54,000
That flash of inspiration or that higher level...

1410
01:28:54,000 --> 01:28:57,000
I guess the first question is, is there a jump?

1411
01:28:57,000 --> 01:29:00,000
Is it actually grounded in the data it's trained on?

1412
01:29:00,000 --> 01:29:04,000
Or is there some high-level reasoning?

1413
01:29:04,000 --> 01:29:07,000
Where does that materialize from?

1414
01:29:07,000 --> 01:29:11,000
So the way I think about it, there is just a space of

1415
01:29:11,000 --> 01:29:14,000
possible algorithms that can be implemented in a

1416
01:29:14,000 --> 01:29:16,000
lot of ways.

1417
01:29:16,000 --> 01:29:22,000
And some of these look like a world model.

1418
01:29:22,000 --> 01:29:25,000
And some of these look like a bunch of statistical correlations.

1419
01:29:25,000 --> 01:29:28,000
And models are trading off lots of different resources.

1420
01:29:28,000 --> 01:29:30,000
Like, how many dimensions do this consume?

1421
01:29:30,000 --> 01:29:32,000
How much weight norm?

1422
01:29:32,000 --> 01:29:34,000
How many parameters?

1423
01:29:34,000 --> 01:29:38,000
How hard is this to get to and how weird and intricate?

1424
01:29:38,000 --> 01:29:43,000
And models will choose the thing that gets the best loss

1425
01:29:43,000 --> 01:29:46,000
that is most efficient on these dimensions,

1426
01:29:46,000 --> 01:29:49,000
assuming they can reach it within the lost landscape.

1427
01:29:49,000 --> 01:29:52,000
Well, I use choose in a very anthropomorphic sense.

1428
01:29:52,000 --> 01:29:56,000
Like, Adam chooses good solutions.

1429
01:29:56,000 --> 01:30:00,000
And I don't know, if you have a sufficiently hard task

1430
01:30:00,000 --> 01:30:03,000
and forming a world model is like the right solution to it,

1431
01:30:03,000 --> 01:30:05,000
models can do it.

1432
01:30:05,000 --> 01:30:08,000
And I think people try to put all of these fancy philosophizing

1433
01:30:08,000 --> 01:30:11,000
on it in a way that I just think is false.

1434
01:30:12,000 --> 01:30:17,000
And I think the Othello paper is like a really beautiful,

1435
01:30:17,000 --> 01:30:20,000
elegant setup that proves this.

1436
01:30:20,000 --> 01:30:22,000
All right, can I move on to the plot twist?

1437
01:30:22,000 --> 01:30:24,000
Does it prove it though?

1438
01:30:24,000 --> 01:30:28,000
It's a very small contrived.

1439
01:30:28,000 --> 01:30:34,000
It's a big jump to assume that that works on a large language model.

1440
01:30:34,000 --> 01:30:37,000
So this is kind of the argument I'm making.

1441
01:30:37,000 --> 01:30:40,000
I think there's the empirical question of do language models

1442
01:30:40,000 --> 01:30:45,000
do this and the theoretical question of could they do this?

1443
01:30:45,000 --> 01:30:48,000
And I'm saying I think the theoretical question is nonsense.

1444
01:30:48,000 --> 01:30:51,000
And I think the Othello paper very conclusively proves

1445
01:30:51,000 --> 01:30:54,000
the theoretical question is nonsense.

1446
01:30:54,000 --> 01:30:58,000
They're just like, yeah, when given a bunch of data,

1447
01:30:58,000 --> 01:31:02,000
you can infer the underlying world model behind it in theory.

1448
01:31:02,000 --> 01:31:04,000
I would pitch back on that a tiny bit

1449
01:31:04,000 --> 01:31:07,000
because it's very similar to AlphaGo,

1450
01:31:07,000 --> 01:31:11,000
which proved that in a closed game,

1451
01:31:11,000 --> 01:31:15,000
which is systematic and representable,

1452
01:31:15,000 --> 01:31:18,000
with a finite, obviously exponentially large,

1453
01:31:18,000 --> 01:31:20,000
but a finite number of board states,

1454
01:31:20,000 --> 01:31:24,000
you can build an agent which performs really, really well.

1455
01:31:24,000 --> 01:31:27,000
That seems to me completely different to something like language

1456
01:31:27,000 --> 01:31:29,000
or acting in the real world that might not be systematic

1457
01:31:29,000 --> 01:31:31,000
in the same way.

1458
01:31:31,000 --> 01:31:33,000
We can debate whether or not it's,

1459
01:31:33,000 --> 01:31:36,000
I think it's an infinite number of possible trajectories,

1460
01:31:36,000 --> 01:31:39,000
just like language, an infinite number of possible sentences.

1461
01:31:39,000 --> 01:31:41,000
Man, there's 50,000 to the power of a thousand

1462
01:31:41,000 --> 01:31:43,000
possible input sequences.

1463
01:31:43,000 --> 01:31:46,000
Sure is a finite number.

1464
01:31:46,000 --> 01:31:48,000
You mean in Othello or?

1465
01:31:48,000 --> 01:31:49,000
No, in GPT2.

1466
01:31:49,000 --> 01:31:52,000
In GPT2, okay.

1467
01:31:52,000 --> 01:31:56,000
Bounded context length, bounded vocab size, generally.

1468
01:31:56,000 --> 01:31:58,000
Bastard.

1469
01:31:58,000 --> 01:32:01,000
You're not going to write more than one quintillion characters

1470
01:32:01,000 --> 01:32:03,000
probably.

1471
01:32:03,000 --> 01:32:06,000
Yeah.

1472
01:32:06,000 --> 01:32:11,000
Well, I guess it is still a big jump, though, isn't it?

1473
01:32:11,000 --> 01:32:17,000
From, yes, empirically, it shows that in Othello, it works.

1474
01:32:17,000 --> 01:32:21,000
Maybe we could debate whether or not it does or not,

1475
01:32:21,000 --> 01:32:23,000
because there's always this question coming back to what we were saying before,

1476
01:32:23,000 --> 01:32:27,000
whether it's learning something which is universal

1477
01:32:27,000 --> 01:32:29,000
or something which is still brittle.

1478
01:32:29,000 --> 01:32:34,000
So the way that we've evaluated it might lead us to conclude that it's universal,

1479
01:32:34,000 --> 01:32:37,000
whereas, actually, it's brittle in ways that we don't understand.

1480
01:32:37,000 --> 01:32:39,000
So that's a very real possibility.

1481
01:32:39,000 --> 01:32:40,000
Yeah.

1482
01:32:40,000 --> 01:32:43,000
I mean, everything's brittle in ways you don't understand.

1483
01:32:43,000 --> 01:32:47,000
It's pretty rare that a model will do everything perfectly

1484
01:32:47,000 --> 01:32:50,000
in a way that there are no adversarial examples.

1485
01:32:50,000 --> 01:32:52,000
And this is one of the more interesting things that's come out

1486
01:32:52,000 --> 01:32:54,000
of the adversarial examples literature to me.

1487
01:32:54,000 --> 01:32:58,000
It's just like, oh, wow, there's so much stuff here.

1488
01:32:58,000 --> 01:33:01,000
There's such a high-dimensional input space.

1489
01:33:01,000 --> 01:33:04,000
There's all kinds of weird things the model wasn't prepared for.

1490
01:33:04,000 --> 01:33:06,000
And I don't know.

1491
01:33:06,000 --> 01:33:11,000
My interpretation of the Othello thing is the strong theoretical arguments are wrong.

1492
01:33:11,000 --> 01:33:16,000
I separately believe that there are world models

1493
01:33:16,000 --> 01:33:19,000
that could be implemented in a language model's ways.

1494
01:33:19,000 --> 01:33:22,000
But I also disagree with the strong inference of the paper

1495
01:33:22,000 --> 01:33:26,000
that this does happen in language models, or that we conclude it does,

1496
01:33:26,000 --> 01:33:29,000
because world models are often really expensive.

1497
01:33:29,000 --> 01:33:32,000
Like, in the Othello model, it's consuming 128 dimensions

1498
01:33:32,000 --> 01:33:36,000
of its 512-dimensional residual stream for this world model.

1499
01:33:36,000 --> 01:33:40,000
And the problem is set up so that the world model is insanely useful

1500
01:33:40,000 --> 01:33:44,000
because weather removers' legal is purely determined by the board state,

1501
01:33:44,000 --> 01:33:46,000
so it's worth the model's while to do this.

1502
01:33:46,000 --> 01:33:48,000
But this is rarely the case in language.

1503
01:33:48,000 --> 01:33:52,000
For example, there was all this buzz about Bing chat playing chess

1504
01:33:52,000 --> 01:33:54,000
and making legal-ish moves.

1505
01:33:54,000 --> 01:33:56,000
And I don't know, man.

1506
01:33:56,000 --> 01:33:58,000
If you want to model a chessboard,

1507
01:33:58,000 --> 01:34:02,000
you just look at the last piece that moves into a cell.

1508
01:34:02,000 --> 01:34:04,000
That's the piece in that cell.

1509
01:34:04,000 --> 01:34:06,000
You don't need an explicit representation.

1510
01:34:06,000 --> 01:34:08,000
You can just use attention heads to do it.

1511
01:34:08,000 --> 01:34:10,000
And there's all kinds of weird hacks,

1512
01:34:10,000 --> 01:34:12,000
and, like, models will generally use the best hack.

1513
01:34:12,000 --> 01:34:15,000
But probably it is worth the model's while

1514
01:34:15,000 --> 01:34:18,000
to have some kind of an internal representation.

1515
01:34:18,000 --> 01:34:22,000
Like, I'd bet that if you took a powerful code-playing model

1516
01:34:22,000 --> 01:34:26,000
and probed it to understand the state of the key variables,

1517
01:34:26,000 --> 01:34:28,000
it would probably have some representation.

1518
01:34:28,000 --> 01:34:34,000
But it's moving on to the work I did building on the Othello paper.

1519
01:34:34,000 --> 01:34:38,000
So one of the things that was really striking to me about the Othello work

1520
01:34:38,000 --> 01:34:42,000
is, simultaneously, its results were strong enough

1521
01:34:42,000 --> 01:34:46,000
that something here was clearly real.

1522
01:34:46,000 --> 01:34:50,000
But they also used techniques that felt more powerful than were needed.

1523
01:34:50,000 --> 01:34:54,000
Like, rather, they found that linear probes did not work.

1524
01:34:54,000 --> 01:34:58,000
There weren't just directions in space corresponding to board states,

1525
01:34:58,000 --> 01:35:02,000
but the non-linear probes, one hidden layer MLPs, did.

1526
01:35:02,000 --> 01:35:06,000
And the key thing to be careful of when probing

1527
01:35:06,000 --> 01:35:08,000
is, is your probe doing the computation,

1528
01:35:08,000 --> 01:35:12,000
or does the model genuinely have this represented?

1529
01:35:12,000 --> 01:35:16,000
And even with linear probes, this can be misleading.

1530
01:35:16,000 --> 01:35:19,000
Like, if you're looking at how a model represents coloured shapes,

1531
01:35:19,000 --> 01:35:22,000
and you find a red triangle direction,

1532
01:35:22,000 --> 01:35:24,000
it could be that there's a red, green, or blue direction,

1533
01:35:24,000 --> 01:35:27,000
and a triangle, square, or shape direction,

1534
01:35:27,000 --> 01:35:29,000
and you're taking the red plus triangle,

1535
01:35:29,000 --> 01:35:32,000
or it could be the case that each of the nine shapes

1536
01:35:32,000 --> 01:35:34,000
have their own direction, you found the red triangle one.

1537
01:35:34,000 --> 01:35:37,000
But non-linear probing is particularly sketchy.

1538
01:35:37,000 --> 01:35:43,000
Like, in the extreme case, if you train GPD3 on the inputs to something,

1539
01:35:43,000 --> 01:35:46,000
GPD3 can do a lot of stuff.

1540
01:35:46,000 --> 01:35:49,000
If you train GPD3 on the activation side network,

1541
01:35:49,000 --> 01:35:52,000
it can probably recover arbitrary functions of the input,

1542
01:35:52,000 --> 01:35:55,000
assuming the information of the input hasn't been lost,

1543
01:35:55,000 --> 01:35:59,000
which it shouldn't have, because there's a residual stream.

1544
01:35:59,000 --> 01:36:04,000
And what I said is not quite true, but not important.

1545
01:36:04,000 --> 01:36:10,000
And so I was, and their intervention technique

1546
01:36:10,000 --> 01:36:14,000
was both got, like, various impressive results,

1547
01:36:14,000 --> 01:36:17,000
but also involved doing a bunch of complex gradient-scent

1548
01:36:17,000 --> 01:36:20,000
against their probe, and this all just seemed more powerful

1549
01:36:20,000 --> 01:36:22,000
than was necessary.

1550
01:36:22,000 --> 01:36:24,000
And so I did the...

1551
01:36:24,000 --> 01:36:26,000
I challenged myself to do a weekend hackathon

1552
01:36:26,000 --> 01:36:29,000
trying to figure out what was going on,

1553
01:36:29,000 --> 01:36:32,000
and I poked around at some internal circuitry

1554
01:36:32,000 --> 01:36:35,000
and tried to answer some very narrow questions about the model,

1555
01:36:35,000 --> 01:36:38,000
and I found this one neuron that seemed to be looking

1556
01:36:38,000 --> 01:36:41,000
for, like, three cell diagonal lines,

1557
01:36:41,000 --> 01:36:44,000
where one was blank,

1558
01:36:44,000 --> 01:36:47,000
the other was white, the next was black.

1559
01:36:47,000 --> 01:36:51,000
But then sometimes it activated on blank, black, white.

1560
01:36:51,000 --> 01:36:54,000
And it turned out that the general pattern

1561
01:36:54,000 --> 01:36:58,000
was that it was blank, current players...

1562
01:36:58,000 --> 01:37:02,000
sorry, blank opponents color and current players color.

1563
01:37:02,000 --> 01:37:05,000
And this is a useful motif in Othello,

1564
01:37:05,000 --> 01:37:07,000
because it makes them move legal.

1565
01:37:07,000 --> 01:37:10,000
And when I saw this, I made the bold hypothesis

1566
01:37:10,000 --> 01:37:13,000
maybe the model actually represents things

1567
01:37:13,000 --> 01:37:16,000
in terms of whether a cell has the current player's color

1568
01:37:16,000 --> 01:37:18,000
or the current opponent's color,

1569
01:37:18,000 --> 01:37:20,000
which in hindsight is a lot more natural,

1570
01:37:20,000 --> 01:37:23,000
because the model plays both black and white,

1571
01:37:23,000 --> 01:37:26,000
and it's kind of symmetric from the perspective of the current player.

1572
01:37:26,000 --> 01:37:28,000
And I trained a linear probe on this,

1573
01:37:28,000 --> 01:37:31,000
and it just worked fabulously,

1574
01:37:31,000 --> 01:37:33,000
and got near-perfect accuracy.

1575
01:37:33,000 --> 01:37:36,000
And I tried linear representations on it,

1576
01:37:36,000 --> 01:37:39,000
and I tried linear interventions, and it just worked.

1577
01:37:39,000 --> 01:37:42,000
And I even feel really excited about this project

1578
01:37:42,000 --> 01:37:44,000
for a bunch of reasons.

1579
01:37:44,000 --> 01:37:46,000
First, while I did it on the weekend,

1580
01:37:46,000 --> 01:37:48,000
I'm still very proud of this.

1581
01:37:48,000 --> 01:37:53,000
Secondly, I think that it has vindicated

1582
01:37:53,000 --> 01:37:56,000
some of my general suspicion of non-linear probing.

1583
01:37:56,000 --> 01:37:58,000
Like, if you really understand a thing,

1584
01:37:58,000 --> 01:38:01,000
you should be able to get a linear probe to work.

1585
01:38:01,000 --> 01:38:04,000
And kind of more deeply, as we discussed,

1586
01:38:04,000 --> 01:38:08,000
there's this words-to-vex style linear representation

1587
01:38:08,000 --> 01:38:12,000
of a hypothesis about models that features corresponded directions.

1588
01:38:12,000 --> 01:38:15,000
The Athero work seemed like pretty strong evidence against.

1589
01:38:15,000 --> 01:38:19,000
They had chordal interventions showing that the board state was there,

1590
01:38:19,000 --> 01:38:23,000
but actually non-linear probes did not work.

1591
01:38:23,000 --> 01:38:26,000
It seemed like they found some non-linear representation.

1592
01:38:26,000 --> 01:38:30,000
And my and Chris Ola's hypothesis seeing this

1593
01:38:30,000 --> 01:38:34,000
was that there was a linear representation hiding beneath.

1594
01:38:34,000 --> 01:38:37,000
Martin Boddenberg, one of the authors of the paper,

1595
01:38:37,000 --> 01:38:40,000
had the hypothesis that it was, like,

1596
01:38:40,000 --> 01:38:42,000
an actual non-linear representation,

1597
01:38:42,000 --> 01:38:44,000
and this was evidence against the hypothesis.

1598
01:38:44,000 --> 01:38:47,000
And this kind of formed natural experiment

1599
01:38:47,000 --> 01:38:49,000
where the hypothesis could have been falsified,

1600
01:38:49,000 --> 01:38:54,000
but my work showed there was a real non-linear representation,

1601
01:38:54,000 --> 01:38:57,000
and thus that it had predictive power.

1602
01:38:57,000 --> 01:38:59,000
And so many of our frameworks for mech and turp

1603
01:38:59,000 --> 01:39:02,000
are just these loose things based on a bunch of data,

1604
01:39:02,000 --> 01:39:06,000
but not fully rigorous or fully conclusively shown.

1605
01:39:06,000 --> 01:39:08,000
And so natural experiments like this

1606
01:39:08,000 --> 01:39:11,000
feel like some of the best data we have.

1607
01:39:11,000 --> 01:39:13,000
On this linear representation, though,

1608
01:39:13,000 --> 01:39:15,000
I don't know if you've heard of the spline theory

1609
01:39:15,000 --> 01:39:18,000
of neural networks by Randall Ballastriero.

1610
01:39:18,000 --> 01:39:21,000
And without going into too much detail,

1611
01:39:21,000 --> 01:39:24,000
it's quite a discrete view of MLPs in particular

1612
01:39:24,000 --> 01:39:28,000
that the relus essentially get activated

1613
01:39:28,000 --> 01:39:32,000
in an input-sensitive way to carve out these polyhedra

1614
01:39:32,000 --> 01:39:36,000
in the ambient space, and essentially an input

1615
01:39:36,000 --> 01:39:39,000
will be mapped into one of these cells in the ambient space,

1616
01:39:39,000 --> 01:39:42,000
and then there's a kind of discreteness to it,

1617
01:39:42,000 --> 01:39:45,000
because if you just perturb the input

1618
01:39:45,000 --> 01:39:48,000
and you move outside of one of these polyhedra,

1619
01:39:48,000 --> 01:39:51,000
then the model will, if it's a classifier,

1620
01:39:51,000 --> 01:39:53,000
classify something different.

1621
01:39:53,000 --> 01:39:55,000
But I guess I want to understand,

1622
01:39:55,000 --> 01:39:58,000
with this representation theory, if features are directions,

1623
01:39:58,000 --> 01:40:01,000
does that imply there's a kind of continuity

1624
01:40:02,000 --> 01:40:05,000
because the network will learn to spread out

1625
01:40:05,000 --> 01:40:07,000
those representations in the best possible way,

1626
01:40:07,000 --> 01:40:10,000
but it won't necessarily be a way which is semantically useful,

1627
01:40:10,000 --> 01:40:12,000
like in Word2Vec,

1628
01:40:12,000 --> 01:40:14,000
stop and go are very close to each other,

1629
01:40:14,000 --> 01:40:16,000
and they shouldn't be.

1630
01:40:16,000 --> 01:40:19,000
And at what point does stop become go?

1631
01:40:19,000 --> 01:40:23,000
So do you see there being boundaries in these directions?

1632
01:40:24,000 --> 01:40:28,000
So I think this is, again, my point that I think

1633
01:40:28,000 --> 01:40:30,000
of linear representations as being

1634
01:40:30,000 --> 01:40:33,000
importantly different from geometric representations,

1635
01:40:33,000 --> 01:40:36,000
like stop should be close to go,

1636
01:40:36,000 --> 01:40:38,000
because in many contexts,

1637
01:40:38,000 --> 01:40:44,000
they are like a kind of changing of state term,

1638
01:40:44,000 --> 01:40:46,000
and it's used in similar contexts

1639
01:40:46,000 --> 01:40:48,000
and has similar grammatical meaning,

1640
01:40:48,000 --> 01:40:50,000
but then on this single semantic thing,

1641
01:40:50,000 --> 01:40:52,000
they're quite different.

1642
01:40:52,000 --> 01:40:53,000
And the natural way to represent this

1643
01:40:53,000 --> 01:40:55,000
is have them be close together in Euclidean space,

1644
01:40:56,000 --> 01:40:59,000
but have some crucial like negation dimension

1645
01:40:59,000 --> 01:41:01,000
where they're different.

1646
01:41:01,000 --> 01:41:05,000
And the contact and like ultimately neural networks

1647
01:41:05,000 --> 01:41:07,000
are not geometric objects,

1648
01:41:07,000 --> 01:41:09,000
they are made of linear algebra.

1649
01:41:09,000 --> 01:41:12,000
Every neuron's input is just project

1650
01:41:12,000 --> 01:41:15,000
the residual stream onto some vector,

1651
01:41:15,000 --> 01:41:19,000
and this involves just selecting some set of directions

1652
01:41:19,000 --> 01:41:21,000
and taking a linear combination

1653
01:41:21,000 --> 01:41:24,000
of the feature corresponding to each of those.

1654
01:41:25,000 --> 01:41:28,000
And this is just the natural way for a model

1655
01:41:28,000 --> 01:41:30,000
to represent things in my opinion.

1656
01:41:31,000 --> 01:41:34,000
Okay, well, I think this will in a second lead us

1657
01:41:34,000 --> 01:41:36,000
on very nicely to superposition,

1658
01:41:36,000 --> 01:41:38,000
which is that we don't actually think

1659
01:41:38,000 --> 01:41:42,000
of there being one direction necessarily.

1660
01:41:42,000 --> 01:41:44,000
Just to close this little piece,

1661
01:41:44,000 --> 01:41:46,000
now you said in your less wrong article

1662
01:41:46,000 --> 01:41:50,000
that orthello GPT is likely over parameterized

1663
01:41:50,000 --> 01:41:52,000
for good performance on this particular task

1664
01:41:52,000 --> 01:41:54,000
while language models are under parameterized.

1665
01:41:54,000 --> 01:41:56,000
And of course, we have the ground truth to this task,

1666
01:41:56,000 --> 01:41:58,000
which makes it very, very easy.

1667
01:41:58,000 --> 01:41:59,000
So much easier to interpret.

1668
01:41:59,000 --> 01:42:02,000
100%, but you did conclude saying that

1669
01:42:02,000 --> 01:42:04,000
this is further evidence that neural networks

1670
01:42:04,000 --> 01:42:07,000
are genuinely understandable and interpretable,

1671
01:42:07,000 --> 01:42:09,000
and probing on the face of it seems like

1672
01:42:09,000 --> 01:42:11,000
a very exciting approach to understand

1673
01:42:11,000 --> 01:42:13,000
what the models really represent,

1674
01:42:13,000 --> 01:42:16,000
caveat, mentor, conceptual issues.

1675
01:42:16,000 --> 01:42:19,000
So let's move on to this superposition,

1676
01:42:20,000 --> 01:42:22,000
also known as poly semanticity,

1677
01:42:22,000 --> 01:42:24,000
which is an absolutely beautiful,

1678
01:42:24,000 --> 01:42:26,000
while you're shaking your head a little bit,

1679
01:42:26,000 --> 01:42:28,000
maybe you start with that.

1680
01:42:28,000 --> 01:42:31,000
Yeah, so there's...

1681
01:42:31,000 --> 01:42:34,000
All right, so what's the narrative here?

1682
01:42:34,000 --> 01:42:36,000
So fundamentally,

1683
01:42:36,000 --> 01:42:40,000
we are trying to engage with models

1684
01:42:40,000 --> 01:42:42,000
as these high dimensional objects

1685
01:42:42,000 --> 01:42:45,000
in kind of this conceptual way.

1686
01:42:45,000 --> 01:42:47,000
So we need to be able to decompose them

1687
01:42:47,000 --> 01:42:50,000
because of the curse of dimensionality.

1688
01:42:50,000 --> 01:42:53,000
And we think models correspond to features

1689
01:42:53,000 --> 01:42:55,000
and the features correspond to directions.

1690
01:42:55,000 --> 01:42:58,000
And the hope in the early field

1691
01:42:58,000 --> 01:43:01,000
was that features would correspond to neurons.

1692
01:43:01,000 --> 01:43:04,000
And even if you believe features correspond

1693
01:43:04,000 --> 01:43:06,000
to orthogonal directions,

1694
01:43:06,000 --> 01:43:08,000
the same thing they correspond to neurons

1695
01:43:08,000 --> 01:43:10,000
is like a pretty strong one,

1696
01:43:10,000 --> 01:43:13,000
because there's no reason to align with the neuron basis.

1697
01:43:13,000 --> 01:43:15,000
The reason this isn't a crazy belief

1698
01:43:15,000 --> 01:43:18,000
is that models are incentivized to represent features

1699
01:43:18,000 --> 01:43:21,000
in ways that can vary independently from each other.

1700
01:43:21,000 --> 01:43:26,000
And because relus and jelus act element-wise,

1701
01:43:26,000 --> 01:43:28,000
if there's a feature per neuron,

1702
01:43:28,000 --> 01:43:30,000
they can vary independently.

1703
01:43:30,000 --> 01:43:32,000
Well, if there's multiple features in the same neuron,

1704
01:43:32,000 --> 01:43:34,000
I don't know, if there's a relu,

1705
01:43:34,000 --> 01:43:36,000
the second feature could change

1706
01:43:36,000 --> 01:43:38,000
so the relu goes from on to off

1707
01:43:38,000 --> 01:43:40,000
in a way that changes how the other feature is expressed

1708
01:43:40,000 --> 01:43:42,000
in the dance room network.

1709
01:43:42,000 --> 01:43:44,000
And this is like a beautiful theoretical argument.

1710
01:43:44,000 --> 01:43:48,000
Sadly, it's bullshit because of this phenomena of police mantisity.

1711
01:43:48,000 --> 01:43:53,000
Police mantisity is a behavioral observation of networks.

1712
01:43:53,000 --> 01:43:55,000
But when we look at neurons

1713
01:43:55,000 --> 01:43:57,000
and look at things that activate them,

1714
01:43:57,000 --> 01:44:00,000
they're often activated by seemingly unrelated things,

1715
01:44:00,000 --> 01:44:05,000
like the urs in the word strangers

1716
01:44:05,000 --> 01:44:07,000
and capital letters are proper nouns

1717
01:44:07,000 --> 01:44:09,000
and news articles about football.

1718
01:44:09,000 --> 01:44:12,000
It's a particularly fun neural I found one time in a language model.

1719
01:44:12,000 --> 01:44:17,000
And police mantisity is a purely behavioral thing.

1720
01:44:17,000 --> 01:44:19,000
We're just saying this neuron activates

1721
01:44:19,000 --> 01:44:22,000
for a bunch of seemingly unrelated stuff.

1722
01:44:22,000 --> 01:44:26,000
And it's possible that actually we're missing

1723
01:44:26,000 --> 01:44:29,000
some galaxy brain abstraction where all of this is related,

1724
01:44:29,000 --> 01:44:31,000
but my guess is that this is just,

1725
01:44:31,000 --> 01:44:34,000
the model is not aligning features with neurons.

1726
01:44:34,000 --> 01:44:38,000
And one explanation of this

1727
01:44:38,000 --> 01:44:42,000
is you've just got this thing called a distributed representation

1728
01:44:42,000 --> 01:44:46,000
where a feature is made of the linear combination of different neurons.

1729
01:44:46,000 --> 01:44:49,000
But it is kind of rotated from the neuron basis.

1730
01:44:49,000 --> 01:44:53,000
And this argument that neurons can vary independently

1731
01:44:53,000 --> 01:44:56,000
is a reason to think you wouldn't see this.

1732
01:44:56,000 --> 01:45:01,000
Where this hypothesis is just that there's still

1733
01:45:01,000 --> 01:45:05,000
n things when there's n neurons, but they're rotated.

1734
01:45:05,000 --> 01:45:07,000
But then there's this stronger hypothesis

1735
01:45:07,000 --> 01:45:12,000
that tries to explain this called the superposition hypothesis.

1736
01:45:12,000 --> 01:45:15,000
And here the idea is,

1737
01:45:15,000 --> 01:45:18,000
so if a model wants to be able to recover a feature perfectly,

1738
01:45:18,000 --> 01:45:21,000
it must be orthogonal from all other features.

1739
01:45:21,000 --> 01:45:24,000
But if it wants to mostly recover it,

1740
01:45:24,000 --> 01:45:27,000
it suffices to have almost orthogonal vectors.

1741
01:45:27,000 --> 01:45:31,000
And you can fit in many, many more almost orthogonal vectors

1742
01:45:31,000 --> 01:45:34,000
into a space than orthogonal vectors.

1743
01:45:34,000 --> 01:45:37,000
There's theorem saying that there are exponentially many

1744
01:45:37,000 --> 01:45:39,000
in the number of dimensions.

1745
01:45:39,000 --> 01:45:42,000
If you have 100 dimensional vectors,

1746
01:45:42,000 --> 01:45:45,000
how many orthogonal directions are there?

1747
01:45:45,000 --> 01:45:46,000
100.

1748
01:45:46,000 --> 01:45:47,000
100?

1749
01:45:47,000 --> 01:45:48,000
Yep.

1750
01:45:48,000 --> 01:45:55,000
This is just the statement that you pick a vector.

1751
01:45:55,000 --> 01:46:01,000
Sorry, there's 100 vectors that are all orthogonal of each other.

1752
01:46:01,000 --> 01:46:03,000
Basic proof, you pick a vector,

1753
01:46:03,000 --> 01:46:06,000
everything's orthogonal to that, that's a 9 to 9 dimensional space.

1754
01:46:06,000 --> 01:46:09,000
You pick another vector, take everything orthogonal to that,

1755
01:46:09,000 --> 01:46:11,000
that's a 98 dimensional space,

1756
01:46:11,000 --> 01:46:15,000
and keep going until you get to nothing.

1757
01:46:15,000 --> 01:46:19,000
Like if you picture a 2D space, you pick any direction,

1758
01:46:19,000 --> 01:46:22,000
the only things orthogonal to that are a line,

1759
01:46:22,000 --> 01:46:26,000
and so there's exactly two orthogonal things you can fit in.

1760
01:46:26,000 --> 01:46:29,000
And there's like, you can rotate this and you can get

1761
01:46:29,000 --> 01:46:32,000
many different sets of orthogonal things.

1762
01:46:32,000 --> 01:46:35,000
Okay, I'm trying to articulate why this doesn't make sense to me.

1763
01:46:35,000 --> 01:46:37,000
So maybe we should start with the curse of dimensionality,

1764
01:46:37,000 --> 01:46:40,000
which is that the volume of the space increases exponentially

1765
01:46:40,000 --> 01:46:42,000
with the number of dimensions.

1766
01:46:42,000 --> 01:46:44,000
So we'll start with that.

1767
01:46:44,000 --> 01:46:47,000
And the reason I'm thinking, maybe I'm wrong,

1768
01:46:47,000 --> 01:46:50,000
but if you've got 100 dimensional vector,

1769
01:46:50,000 --> 01:46:54,000
every combination of flipping one of the dimensions

1770
01:46:54,000 --> 01:46:59,000
would produce a vector which is orthogonal to all of the other ones, would it not?

1771
01:46:59,000 --> 01:47:00,000
No.

1772
01:47:00,000 --> 01:47:03,000
So let's imagine you've got a vector of all ones.

1773
01:47:03,000 --> 01:47:06,000
If you pick the first element and you negate it,

1774
01:47:06,000 --> 01:47:08,000
so it's like minus one, then 99 ones.

1775
01:47:08,000 --> 01:47:11,000
These are not orthogonal, the dot product is 98.

1776
01:47:11,000 --> 01:47:13,000
Okay, okay, well that makes sense.

1777
01:47:13,000 --> 01:47:17,000
So there's a linear number of orthogonal directions,

1778
01:47:17,000 --> 01:47:21,000
and in which case we actually need to have these

1779
01:47:21,000 --> 01:47:23,000
approximately orthogonal directions,

1780
01:47:23,000 --> 01:47:26,000
because that actually does bias an exponential number.

1781
01:47:26,000 --> 01:47:29,000
Yeah, and so the superpositional hypothesis is that the model represents

1782
01:47:29,000 --> 01:47:33,000
more features than it has neurons, or that it has dimensions,

1783
01:47:33,000 --> 01:47:38,000
and it somehow compresses them in as things that are almost orthogonal

1784
01:47:38,000 --> 01:47:41,000
when it reads them out with a projection to get some interference,

1785
01:47:41,000 --> 01:47:45,000
but it needs to balance the value of representing more features

1786
01:47:45,000 --> 01:47:48,000
against the cost of interference.

1787
01:47:48,000 --> 01:47:54,000
And Anthropic has this fantastic paper called toy models of superposition,

1788
01:47:54,000 --> 01:47:58,000
which sadly was written off right left, so I can't claim any credit,

1789
01:47:58,000 --> 01:48:05,000
and they basically build a toy model that exhibits superposition.

1790
01:48:05,000 --> 01:48:10,000
The exact structure is you have n independent features,

1791
01:48:10,000 --> 01:48:14,000
each of which is zero most of the time, it's not very prevalent,

1792
01:48:14,000 --> 01:48:19,000
and there's a linear map from that to a small dimensional space,

1793
01:48:19,000 --> 01:48:23,000
a linear map back up, and a non-linearity on the output,

1794
01:48:23,000 --> 01:48:27,000
no non-linearity on the bottleneck in the middle,

1795
01:48:27,000 --> 01:48:30,000
and you train it to be an autoencoder.

1796
01:48:30,000 --> 01:48:34,000
Can it recover the features in the input?

1797
01:48:34,000 --> 01:48:37,000
And because there's many more features than that are in the bottleneck,

1798
01:48:37,000 --> 01:48:41,000
this tests whether the model can actually do this.

1799
01:48:41,000 --> 01:48:44,000
And they find that it sometimes does, sometimes doesn't,

1800
01:48:44,000 --> 01:48:48,000
and then do a lot of really in-depth investigation of how this varies.

1801
01:48:48,000 --> 01:48:54,000
And yeah, returning to like, is superposition the same thing as police mantisity?

1802
01:48:54,000 --> 01:48:58,000
I would say no, police mantisity is a behavioral thing.

1803
01:48:58,000 --> 01:49:01,000
Distributed representations are also a behavioral thing,

1804
01:49:01,000 --> 01:49:04,000
that it's like not aligned with the basis,

1805
01:49:04,000 --> 01:49:09,000
and superposition is a mechanistic hypothesis for why both of these will happen,

1806
01:49:09,000 --> 01:49:11,000
because if you have more features than neurons,

1807
01:49:11,000 --> 01:49:15,000
obviously you're going to have multiple features per neuron,

1808
01:49:15,000 --> 01:49:21,000
and probably you're going to have features that are not aligned with neurons.

1809
01:49:21,000 --> 01:49:23,000
Okay, okay, very interesting.

1810
01:49:23,000 --> 01:49:28,000
So why do you think that superposition is one of the biggest problems in mech and turp?

1811
01:49:28,000 --> 01:49:34,000
Yeah, so it's this fundamental thing that we need to be able to decompose a model

1812
01:49:34,000 --> 01:49:40,000
into individual units, and ideally these would be neurons,

1813
01:49:40,000 --> 01:49:44,000
but they are not neurons, so we need to figure out what we're doing.

1814
01:49:44,000 --> 01:49:51,000
And superposition, so in a world where we just have like n meaningful directions,

1815
01:49:51,000 --> 01:49:56,000
but they weren't aligned with the standard basis, that'd be kind of doable.

1816
01:49:56,000 --> 01:50:01,000
And indeed models often have like linear bottlenecks,

1817
01:50:01,000 --> 01:50:05,000
like the residual stream, or the keys, queries, and values of an attention hit,

1818
01:50:05,000 --> 01:50:12,000
that don't have element-wise linearities, and so have no intrinsically meaningful basis.

1819
01:50:12,000 --> 01:50:15,000
The jargon here is privileged basis,

1820
01:50:15,000 --> 01:50:22,000
but superposition means that you can't even say,

1821
01:50:22,000 --> 01:50:24,000
this feature should be a fucking alter everything else,

1822
01:50:24,000 --> 01:50:27,000
there's going to be a bunch of interference.

1823
01:50:27,000 --> 01:50:33,000
There's not even a kind of mathematically,

1824
01:50:33,000 --> 01:50:38,000
there's not even like a unique set of more than n directions,

1825
01:50:38,000 --> 01:50:42,000
so describe some kind of vectors in n dimensional space.

1826
01:50:42,000 --> 01:50:47,000
And I think that understanding how to extract features from superposition,

1827
01:50:47,000 --> 01:50:52,000
given that superposition seems like a core part of how models do things,

1828
01:50:52,000 --> 01:50:57,000
though we really do not have as much data here as I would like us to,

1829
01:50:57,000 --> 01:51:02,000
understanding how to extract the right meaningful units seems really important.

1830
01:51:02,000 --> 01:51:06,000
Okay, and I think we should clarify the difference between computational

1831
01:51:06,000 --> 01:51:08,000
and representational superposition.

1832
01:51:08,000 --> 01:51:15,000
Yeah, so there's kind of, so transformers are interesting,

1833
01:51:15,000 --> 01:51:21,000
because they often have high dimensional activations

1834
01:51:21,000 --> 01:51:25,000
that get linearly mapped to low dimensional things.

1835
01:51:25,000 --> 01:51:28,000
So like in say GPT-2, in say GPT-2 small,

1836
01:51:28,000 --> 01:51:32,000
the residual stream has 768 dimensions,

1837
01:51:32,000 --> 01:51:37,000
while each MLP layer has 3000 neurons.

1838
01:51:37,000 --> 01:51:41,000
And even if we think each neuron just produces a single feature,

1839
01:51:41,000 --> 01:51:46,000
they need to get compressed down to the 768 dimensional residual stream.

1840
01:51:46,000 --> 01:51:51,000
And we, or there's like 50,000 input tokens

1841
01:51:51,000 --> 01:51:55,000
that get compressed to 768 dimensions.

1842
01:51:55,000 --> 01:52:00,000
And this is called representational superposition.

1843
01:52:00,000 --> 01:52:03,000
The model is representing, the model's already computed the features,

1844
01:52:03,000 --> 01:52:06,000
but it's compressing them to some bottleneck space.

1845
01:52:06,000 --> 01:52:10,000
And this is the main thing studied in the toy models of superposition paper.

1846
01:52:10,000 --> 01:52:16,000
And what we found, sorry,

1847
01:52:16,000 --> 01:52:20,000
there's a separate thing of computational superposition,

1848
01:52:20,000 --> 01:52:25,000
which is when the model is doing, it's computing new features.

1849
01:52:25,000 --> 01:52:32,000
This needs non-linearities, like attention head softmaxes or MLP jellies.

1850
01:52:32,000 --> 01:52:38,000
And the non-linearities can compute new features as directions from the old ones,

1851
01:52:38,000 --> 01:52:46,000
like if this, for example,

1852
01:52:46,000 --> 01:52:51,000
if the top of an image is a car window and the bottom is a car wheel,

1853
01:52:51,000 --> 01:52:54,000
then it's a car.

1854
01:52:54,000 --> 01:52:59,000
Or if the current token is Johnson and the previous token was Boris,

1855
01:52:59,000 --> 01:53:01,000
this is Boris Johnson.

1856
01:53:01,000 --> 01:53:10,000
And this is all, how to phrase this?

1857
01:53:10,000 --> 01:53:14,000
Yeah, this is computational superposition.

1858
01:53:14,000 --> 01:53:18,000
If the model wants to compute more features than it has neurons.

1859
01:53:18,000 --> 01:53:23,000
And this is much harder to reason about because linear algebra is nice

1860
01:53:23,000 --> 01:53:25,000
and fairly well understood.

1861
01:53:25,000 --> 01:53:28,000
Non-linearities, spoilers in the name, are not linear,

1862
01:53:28,000 --> 01:53:30,000
and that's way more of a pain.

1863
01:53:30,000 --> 01:53:36,000
And I think that we generally have a much less good handle on computational superposition,

1864
01:53:36,000 --> 01:53:41,000
but also that this is way more of where the interestingness lies by my lights.

1865
01:53:41,000 --> 01:53:45,000
And this is very briefly studied in the toy models of superposition paper,

1866
01:53:45,000 --> 01:53:49,000
but I would love to see more work looking at this in practice

1867
01:53:49,000 --> 01:53:52,000
and also looking at this in toy models.

1868
01:53:52,000 --> 01:53:57,000
So zooming out a tiny bit, there's this paper from Anthropic.

1869
01:53:57,000 --> 01:54:01,000
And the overall question to me is, does it actually exist?

1870
01:54:01,000 --> 01:54:05,000
Now, presumably you're satisfied with the evidence that it does exist.

1871
01:54:05,000 --> 01:54:10,000
And then there's the question of how do neural networks actually do it?

1872
01:54:10,000 --> 01:54:13,000
And then there's the question of how does the neural network think,

1873
01:54:13,000 --> 01:54:15,000
anthropomorphic language, I apologize,

1874
01:54:15,000 --> 01:54:19,000
about the trade-off of more superposition, more features,

1875
01:54:19,000 --> 01:54:24,000
but more interference versus less interference and more superposition?

1876
01:54:24,000 --> 01:54:29,000
Yeah, so typing into the final question about interference,

1877
01:54:29,000 --> 01:54:37,000
a useful conceptual distinction is that there's two different kinds of interference.

1878
01:54:37,000 --> 01:54:42,000
So if you've got two features that share a dimension or share a neuron.

1879
01:54:42,000 --> 01:54:45,000
Oh yeah, final note on representational superposition,

1880
01:54:45,000 --> 01:54:48,000
because I don't think it should even be referred to in terms of neurons,

1881
01:54:48,000 --> 01:54:52,000
because the individual-based elements don't have intrinsic meaning.

1882
01:54:52,000 --> 01:54:56,000
Modular weird quirks like Adam.

1883
01:54:56,000 --> 01:55:02,000
And it annoys me when people refer to the residual stream or key vectors as having neurons.

1884
01:55:02,000 --> 01:55:06,000
There's no element-wise linearity, it's not privileged.

1885
01:55:06,000 --> 01:55:10,000
Anyway, yeah, two types of interference.

1886
01:55:10,000 --> 01:55:15,000
When A and B share a dimension, you can, yeah,

1887
01:55:15,000 --> 01:55:19,000
let's say this dimension has both dice and poetry.

1888
01:55:19,000 --> 01:55:24,000
You first off need to tell where if dice is there but poetry is not,

1889
01:55:24,000 --> 01:55:28,000
you need to tell that dice is there and that poetry is not there.

1890
01:55:28,000 --> 01:55:32,000
And if both what I call alternating interference,

1891
01:55:32,000 --> 01:55:37,000
and then there's simultaneous interference where dice and poetry are both there.

1892
01:55:37,000 --> 01:55:43,000
And you need to tell that both are there, but not that they're both there with like double strength.

1893
01:55:43,000 --> 01:55:49,000
And as a general rule, models are good at dealing with things of the form.

1894
01:55:49,000 --> 01:55:52,000
Notice when something is extreme along this dimension,

1895
01:55:52,000 --> 01:55:59,000
but not notice when it is extreme along a dimension versus when it's not extreme.

1896
01:55:59,000 --> 01:56:04,000
And alternating interference looks like that.

1897
01:56:04,000 --> 01:56:10,000
Like if dice is straight up, poetry is at 45 degrees,

1898
01:56:10,000 --> 01:56:17,000
both have less interference when the other one is active

1899
01:56:17,000 --> 01:56:20,000
than when the main one is active along their direction.

1900
01:56:20,000 --> 01:56:25,000
Okay, so you're saying interference from A and not B is far easier than A and B?

1901
01:56:25,000 --> 01:56:26,000
Yes, exactly.

1902
01:56:26,000 --> 01:56:34,000
And like a very rough heuristic as models will just not do simultaneous interference,

1903
01:56:34,000 --> 01:56:37,000
but will do alternating interference.

1904
01:56:37,000 --> 01:56:45,000
And they observed this in the toy models paper because they varied how often a feature was non-zero.

1905
01:56:45,000 --> 01:56:49,000
What I think of as the prevalence of the feature that they called it spastic.

1906
01:56:49,000 --> 01:56:54,000
And what they found is that when the feature was less prevalent,

1907
01:56:54,000 --> 01:56:57,000
it was much more likely to be in superposition.

1908
01:56:57,000 --> 01:57:02,000
And the way to think about this is if you have two independent features that both exist with probability P,

1909
01:57:02,000 --> 01:57:06,000
the rate of simultaneous interference is P squared.

1910
01:57:06,000 --> 01:57:08,000
The rate of alternating is P.

1911
01:57:08,000 --> 01:57:16,000
And so and the worth of having the feature is also proportional to P because it occurs P of the time.

1912
01:57:16,000 --> 01:57:21,000
So the railroad is the less of a big deal simultaneous interferences.

1913
01:57:22,000 --> 01:57:25,000
And eventually the model uses superposition.

1914
01:57:25,000 --> 01:57:31,000
There's also there was also an interesting bit looking at correlated features.

1915
01:57:31,000 --> 01:57:37,000
So correlated features, even if they're not very prevalent, they have pretty high simultaneous interference.

1916
01:57:37,000 --> 01:57:42,000
And models tend to put correlated features in to be orthogonal,

1917
01:57:42,000 --> 01:57:46,000
but anti-correlated features, it's very happy for them to share a direction.

1918
01:57:46,000 --> 01:57:53,000
One way you could think about this is if you've got, say, 25 features about romance novels and 25 features about Python code,

1919
01:57:53,000 --> 01:58:01,000
you could have 25 directions that each contain a pair of features and then a single disambiguating neuron

1920
01:58:01,000 --> 01:58:06,000
that is onto Python code off of romance novels that use to disambiguate the two.

1921
01:58:06,000 --> 01:58:13,000
And yeah, may this be a good time to talk about the finding neurons in a haystack paper?

1922
01:58:13,000 --> 01:58:15,000
Unless you've got more stuff on this.

1923
01:58:15,000 --> 01:58:18,000
We'll get to that in just two shakes of a lamb's tail.

1924
01:58:18,000 --> 01:58:23,000
But just before when I was reading through the paper, I had the mindset of sparsity.

1925
01:58:23,000 --> 01:58:27,000
And you told me, Tim, don't say sparsity. It's prevalence.

1926
01:58:27,000 --> 01:58:29,000
It means so many things.

1927
01:58:29,000 --> 01:58:31,000
It's very overloaded.

1928
01:58:31,000 --> 01:58:36,000
So, you know, so just quickly touch on the relationship between what is prevalence,

1929
01:58:36,000 --> 01:58:39,000
the relationship between prevalence and superposition.

1930
01:58:40,000 --> 01:58:43,000
And just before, well, actually, I've got a couple more questions,

1931
01:58:43,000 --> 01:58:51,000
but would you also just mind playing devil's advocate and criticising the anthropic paper if you can?

1932
01:58:51,000 --> 01:58:52,000
Sure.

1933
01:58:52,000 --> 01:58:55,000
So I should be very clear.

1934
01:58:55,000 --> 01:58:58,000
This is one of my top three all-time favourite and territoriality papers.

1935
01:58:58,000 --> 01:59:00,000
It's a fantastic paper.

1936
01:59:00,000 --> 01:59:01,000
That said.

1937
01:59:01,000 --> 01:59:03,000
A bad word said about it.

1938
01:59:03,000 --> 01:59:04,000
Oh, I have so much.

1939
01:59:04,000 --> 01:59:06,000
I have bad words to say about every paper,

1940
01:59:06,000 --> 01:59:10,000
especially the ones that I like because I've engaged with them in the most detail.

1941
01:59:10,000 --> 01:59:14,000
So things which I think were misleading about this paper.

1942
01:59:14,000 --> 01:59:20,000
The first is I think the representational versus computational superposition distinction is very important.

1943
01:59:20,000 --> 01:59:24,000
I think computational is a fair bit more interesting.

1944
01:59:24,000 --> 01:59:27,000
And while I think the authors knew the difference,

1945
01:59:27,000 --> 01:59:32,000
I think a casual reader often came away not realising the difference,

1946
01:59:32,000 --> 01:59:35,000
in particular that most of their results were about the residual stream,

1947
01:59:35,000 --> 01:59:39,000
not about actual neurons and MLP layers.

1948
01:59:39,000 --> 01:59:43,000
The second is a question of activation range.

1949
01:59:43,000 --> 01:59:48,000
So they study features that vary uniformly between zero and one.

1950
01:59:48,000 --> 01:59:52,000
And in practice, I think most features are binary.

1951
01:59:52,000 --> 01:59:55,000
This is a car wheel or this is not a car wheel.

1952
01:59:55,000 --> 01:59:59,000
This is Boris Johnson or this is not Boris Johnson.

1953
01:59:59,000 --> 02:00:04,000
And interference is much worse when they can vary continuously.

1954
02:00:04,000 --> 02:00:08,000
Because if A and B, if A is up, B is at 45 degrees,

1955
02:00:08,000 --> 02:00:17,000
you can't distinguish B at strength one from A at strength 0.7-ish.

1956
02:00:17,000 --> 02:00:20,000
And this is just kind of messy.

1957
02:00:20,000 --> 02:00:22,000
But the binary is just much easier.

1958
02:00:22,000 --> 02:00:27,000
And I think this is a source of confusion.

1959
02:00:28,000 --> 02:00:35,000
I also think the two kinds of interference point was a bit understated.

1960
02:00:35,000 --> 02:00:38,000
But more broadly, it's just a phenomenal paper.

1961
02:00:38,000 --> 02:00:41,000
Oh, my other biggest beaver, they just didn't look in real models.

1962
02:00:41,000 --> 02:00:43,000
And this wasn't the point of the paper.

1963
02:00:43,000 --> 02:00:48,000
But we're doing so much theory crafting and filming conceptual frameworks,

1964
02:00:48,000 --> 02:00:50,000
and we haven't really checked very hard

1965
02:00:50,000 --> 02:00:54,000
whether this is why models actually have police mantisity.

1966
02:00:54,000 --> 02:00:59,000
Wes Gurney, he's working out of MIT, and you've done a lot of work with him.

1967
02:00:59,000 --> 02:01:03,000
So you and Wes, but Wes was the first author,

1968
02:01:03,000 --> 02:01:06,000
wrote a paper called Finding Neurons in a Haystack,

1969
02:01:06,000 --> 02:01:08,000
case studies with sparse probing,

1970
02:01:08,000 --> 02:01:11,000
where you empirically studied superposition and language models

1971
02:01:11,000 --> 02:01:15,000
and actually found that you get lots of superpositions in early layers

1972
02:01:15,000 --> 02:01:18,000
for features like the security and social security.

1973
02:01:18,000 --> 02:01:23,000
And fewer in middle layers for complex features like this text

1974
02:01:23,000 --> 02:01:25,000
is French.

1975
02:01:25,000 --> 02:01:29,000
And also you can bring in the importance of range activation as well.

1976
02:01:29,000 --> 02:01:31,000
But can you frame up that paper?

1977
02:01:31,000 --> 02:01:32,000
Yeah.

1978
02:01:32,000 --> 02:01:36,000
So first off, this paper was led by Wes Gurney, one of my mentees,

1979
02:01:36,000 --> 02:01:37,000
did a fantastic job.

1980
02:01:37,000 --> 02:01:39,000
He deserves like 9% of the credit.

1981
02:01:39,000 --> 02:01:40,000
Great job, Wes.

1982
02:01:40,000 --> 02:01:43,000
I believe he listens to this podcast, so hi.

1983
02:01:44,000 --> 02:01:50,000
And yeah, so the kind of high level pitch behind the paper

1984
02:01:50,000 --> 02:01:54,000
was what we think superposition is happening.

1985
02:01:54,000 --> 02:01:57,000
But like, nobody's really checked very hard.

1986
02:01:57,000 --> 02:01:59,000
And there's like some results in the literature

1987
02:01:59,000 --> 02:02:02,000
I've since come across in non-transformer models

1988
02:02:02,000 --> 02:02:05,000
that demonstrate some amount of distributed representations.

1989
02:02:05,000 --> 02:02:08,000
But what would it look like to check?

1990
02:02:08,000 --> 02:02:11,000
And would it look like to do this in like a reasonably scalable

1991
02:02:11,000 --> 02:02:13,000
and quantitative way?

1992
02:02:13,000 --> 02:02:17,000
And the kind of sparse probing in the title

1993
02:02:17,000 --> 02:02:21,000
is this technique Wes introduces for,

1994
02:02:21,000 --> 02:02:25,000
if we think a feature is represented in MLP layer,

1995
02:02:25,000 --> 02:02:28,000
we can train a linear classifier to extract it,

1996
02:02:28,000 --> 02:02:30,000
a linear probe from that layer.

1997
02:02:30,000 --> 02:02:34,000
But if we constrain the probe to use at most K neurons,

1998
02:02:34,000 --> 02:02:37,000
very K and look at probe performance,

1999
02:02:37,000 --> 02:02:39,000
this lets us distinguish between features that are represented

2000
02:02:39,000 --> 02:02:42,000
with like a single neuron and features that are densely spread

2001
02:02:42,000 --> 02:02:46,000
across all neurons with a lot of methodological neural

2002
02:02:46,000 --> 02:02:50,000
answers about balanced data sets and avoiding overfitting

2003
02:02:50,000 --> 02:02:52,000
and fun stuff like that.

2004
02:02:52,000 --> 02:02:56,000
And most of the interesting bits of the paper,

2005
02:02:56,000 --> 02:03:00,000
in my opinion, are the various case studies we do where,

2006
02:03:00,000 --> 02:03:04,000
so probing fundamentally is like a kind of sketchy methodology

2007
02:03:04,000 --> 02:03:07,000
because probing is correlational.

2008
02:03:07,000 --> 02:03:10,000
Probing doesn't tell you whether a model uses something

2009
02:03:10,000 --> 02:03:12,000
and it's so easy to trick yourself about

2010
02:03:12,000 --> 02:03:15,000
whether you have the right representations.

2011
02:03:15,000 --> 02:03:18,000
So we use it as a starting point and then dig more deeply

2012
02:03:18,000 --> 02:03:20,000
into a few more interesting things.

2013
02:03:20,000 --> 02:03:25,000
One particularly QK study is we looked into factual knowledge neurons,

2014
02:03:25,000 --> 02:03:29,000
found something that seemed to represent this athlete plays hockey,

2015
02:03:29,000 --> 02:03:32,000
but then actually turned out to be a Canada neuron,

2016
02:03:32,000 --> 02:03:35,000
which continues to bring me joy.

2017
02:03:35,000 --> 02:03:38,000
That activates with things like maple syrup and Canada.

2018
02:03:38,000 --> 02:03:42,000
Got to love models learning national stereotypes, right?

2019
02:03:42,000 --> 02:03:43,000
Oh, yes.

2020
02:03:43,000 --> 02:03:51,000
Anyway, so there were two particularly exciting case studies.

2021
02:03:51,000 --> 02:03:56,000
The first was looking in early layers at compound word detectors.

2022
02:03:56,000 --> 02:04:02,000
So if you look at, say, the brain and its visual field,

2023
02:04:02,000 --> 02:04:04,000
we have all these sensory neurons.

2024
02:04:04,000 --> 02:04:07,000
We get raw input of light from the environment

2025
02:04:07,000 --> 02:04:10,000
and it gets converted into stuff our brain can actually manipulate.

2026
02:04:10,000 --> 02:04:14,000
Image models have Gabor filters that convert the pixels

2027
02:04:14,000 --> 02:04:16,000
into something a bit more useful.

2028
02:04:16,000 --> 02:04:19,000
What's the equivalent of language models?

2029
02:04:19,000 --> 02:04:23,000
And it seems to be these things that we call detokenization neurons

2030
02:04:23,000 --> 02:04:29,000
and circuitry, where often words are split into multiple tokens

2031
02:04:29,000 --> 02:04:34,000
or you get compound word phrases like social security

2032
02:04:34,000 --> 02:04:39,000
or Theresa May or Barack Obama and whatever.

2033
02:04:39,000 --> 02:04:44,000
And it's often useful for a model to realize

2034
02:04:44,000 --> 02:04:47,000
this is the second thing in a multi-token phrase,

2035
02:04:47,000 --> 02:04:52,000
especially if it's like you need both things to know what's going on,

2036
02:04:52,000 --> 02:04:54,000
like Michael Jordan.

2037
02:04:54,000 --> 02:04:56,000
Michael has lots of Jordans.

2038
02:04:56,000 --> 02:04:59,000
It's really important to tell both of them that they're there.

2039
02:04:59,000 --> 02:05:04,000
And this is a clearly nonlinear thing because it's like a Boolean and.

2040
02:05:04,000 --> 02:05:08,000
And so we did a lot of probing for different compound words.

2041
02:05:08,000 --> 02:05:13,000
And we found that they were definitely not represented well by single neurons.

2042
02:05:13,000 --> 02:05:17,000
We could find some neurons that were okay at detecting them,

2043
02:05:17,000 --> 02:05:23,000
but there was a lot of interference and a lot of like false positives from other stuff.

2044
02:05:23,000 --> 02:05:25,000
And when we dug into a bunch of these neurons,

2045
02:05:25,000 --> 02:05:28,000
we found that they were incredibly polysemantic.

2046
02:05:28,000 --> 02:05:32,000
They activated for many different compound words.

2047
02:05:32,000 --> 02:05:36,000
And we showed that it was using superposition

2048
02:05:36,000 --> 02:05:41,000
by observing that if you took say five social security detecting neurons

2049
02:05:41,000 --> 02:05:43,000
and add together their activations,

2050
02:05:43,000 --> 02:05:47,000
they go from okay detectors to a really good detector together.

2051
02:05:47,000 --> 02:05:52,000
Because even though each is representing like hundreds of compound words,

2052
02:05:52,000 --> 02:05:54,000
they're representing different compound words,

2053
02:05:54,000 --> 02:05:57,000
which lets you encode these.

2054
02:05:57,000 --> 02:06:03,000
And this, what we've shown here is that it's like distributed,

2055
02:06:03,000 --> 02:06:07,000
that it's a linear combination of neurons.

2056
02:06:07,000 --> 02:06:10,000
We still haven't shown it perfectly to my dissatisfaction.

2057
02:06:10,000 --> 02:06:14,000
I think you really need to do things like ablate these linear combinations

2058
02:06:14,000 --> 02:06:19,000
and see if this systematically damages the model's ability to think about social security, etc.

2059
02:06:19,000 --> 02:06:21,000
But I'm pretty convinced at this point.

2060
02:06:21,000 --> 02:06:26,000
And there's like a few properties of compound words

2061
02:06:26,000 --> 02:06:30,000
that both make it easy to represent in superposition,

2062
02:06:30,000 --> 02:06:35,000
that make me pretty okay making the jump that there's actual superposition.

2063
02:06:35,000 --> 02:06:38,000
The first is just that there's tons of compound words.

2064
02:06:38,000 --> 02:06:42,000
Each one is pretty rare, but each one is like non-trivial or useful.

2065
02:06:42,000 --> 02:06:45,000
And clearly there are more compound words

2066
02:06:45,000 --> 02:06:50,000
and there are like thousands of neurons in the MLP layer of this model.

2067
02:06:50,000 --> 02:06:53,000
The model cares about representing and can represent,

2068
02:06:53,000 --> 02:06:55,000
that we do not actually check.

2069
02:06:55,000 --> 02:06:59,000
Because I could not convince Wes to accumulate a list of 2,000 compound words

2070
02:06:59,000 --> 02:07:01,000
and pray for all of them.

2071
02:07:01,000 --> 02:07:04,000
But I believe in my heart this is true.

2072
02:07:04,000 --> 02:07:06,000
Could I have a point of order though?

2073
02:07:06,000 --> 02:07:11,000
Because I've been reading quite a lot of stuff from linguists like Stephen Piantadosi.

2074
02:07:11,000 --> 02:07:16,000
A lot of linguists are, some of them hate language models

2075
02:07:16,000 --> 02:07:19,000
and some of them are well on board with it.

2076
02:07:19,000 --> 02:07:22,000
Like Raphael Millier for example is a great example.

2077
02:07:22,000 --> 02:07:24,000
I hate language models too, don't worry.

2078
02:07:24,000 --> 02:07:26,000
Well, but the question is,

2079
02:07:26,000 --> 02:07:28,000
because you're talking about compound words and stuff like that

2080
02:07:28,000 --> 02:07:32,000
and you're still using the language of syntax

2081
02:07:32,000 --> 02:07:36,000
and these language models, there's this distributional hypothesis.

2082
02:07:36,000 --> 02:07:40,000
You know the meaning of a word by the company it keeps.

2083
02:07:40,000 --> 02:07:43,000
But linguists and cognitive scientists kind of ditch that.

2084
02:07:43,000 --> 02:07:46,000
I don't think they ever believed in the distributional hypothesis.

2085
02:07:46,000 --> 02:07:48,000
They think about grounding.

2086
02:07:48,000 --> 02:07:51,000
They think about grounding to things in the world

2087
02:07:51,000 --> 02:07:55,000
and also inferential references as well

2088
02:07:55,000 --> 02:07:59,000
which you can think of that as grounding to a model of the mind.

2089
02:07:59,000 --> 02:08:01,000
And this brings us back to the Othello paper

2090
02:08:01,000 --> 02:08:05,000
which is that they're not just learning simple kind of compound relationships

2091
02:08:05,000 --> 02:08:07,000
between the world, between the words.

2092
02:08:07,000 --> 02:08:09,000
They're learning a world model

2093
02:08:09,000 --> 02:08:13,000
and they're doing something much more potentially

2094
02:08:13,000 --> 02:08:15,000
than just predicting the next word.

2095
02:08:15,000 --> 02:08:18,000
And Piantadosi argued that

2096
02:08:18,000 --> 02:08:22,000
most of the representational capacity in language models

2097
02:08:22,000 --> 02:08:24,000
are learning these semantics.

2098
02:08:24,000 --> 02:08:27,000
They're learning relationships between things in the world model

2099
02:08:27,000 --> 02:08:30,000
and the particular occurrence of the token.

2100
02:08:30,000 --> 02:08:32,000
And this superposition idea is very interesting

2101
02:08:32,000 --> 02:08:36,000
because it actually imbues the representational capacity

2102
02:08:36,000 --> 02:08:38,000
in a language model to learn those mappings.

2103
02:08:40,000 --> 02:08:42,000
Okay, so a couple of comments on that.

2104
02:08:42,000 --> 02:08:47,000
The first is a generally useful way of thinking about models to me.

2105
02:08:47,000 --> 02:08:52,000
Is as a the early layers devoted to sensory neurons

2106
02:08:52,000 --> 02:08:57,000
converting the raw input into more useful concepts and representations.

2107
02:08:57,000 --> 02:09:02,000
The actual processing throughout like all of the middle layers

2108
02:09:02,000 --> 02:09:04,000
that actually does all the reasoning.

2109
02:09:04,000 --> 02:09:06,000
And then motor neurons at the end

2110
02:09:06,000 --> 02:09:09,000
that convert the reasoning to actual output tokens

2111
02:09:09,000 --> 02:09:13,000
for like the format that the optimizer wants.

2112
02:09:14,000 --> 02:09:19,000
And it feels like you're mostly talking about the like reasoning internally

2113
02:09:19,000 --> 02:09:23,000
and the specific case study I'm referring to is on the sensory neurons.

2114
02:09:23,000 --> 02:09:26,000
Well, like I'm not saying it just detects compound words

2115
02:09:26,000 --> 02:09:29,000
but obviously that's the first thing it does.

2116
02:09:29,000 --> 02:09:31,000
I don't know, it's so interesting.

2117
02:09:31,000 --> 02:09:34,000
I don't mean to push back but in neuroscience

2118
02:09:34,000 --> 02:09:37,000
the field was held back for decades by this idea

2119
02:09:37,000 --> 02:09:40,000
of this kind of left to right to processing

2120
02:09:40,000 --> 02:09:44,000
this hierarchical processing where you have these very, very simple concepts

2121
02:09:44,000 --> 02:09:48,000
that become increasingly abstract with more processing.

2122
02:09:48,000 --> 02:09:50,000
And then I think the field has moved away from that.

2123
02:09:50,000 --> 02:09:52,000
It's far more messy and chaotic than that.

2124
02:09:52,000 --> 02:09:55,000
Now with a neural network, it actually is hierarchical

2125
02:09:55,000 --> 02:09:58,000
because the network is basically a DAG.

2126
02:09:58,000 --> 02:10:01,000
So I suppose it is safe to make this assumption

2127
02:10:01,000 --> 02:10:03,000
but could I just kind of question you on that?

2128
02:10:03,000 --> 02:10:05,000
Is it safe to make that assumption?

2129
02:10:05,000 --> 02:10:08,000
Is there increasing complexity in representation

2130
02:10:08,000 --> 02:10:10,000
as you go from left to right?

2131
02:10:10,000 --> 02:10:12,000
Oh, let's see.

2132
02:10:12,000 --> 02:10:16,000
So yeah, I definitely, yeah.

2133
02:10:16,000 --> 02:10:21,000
So clarification one, the network has this input sequence

2134
02:10:21,000 --> 02:10:23,000
which I think was going from left to right

2135
02:10:23,000 --> 02:10:26,000
and then there's a bunch of layers which I think it was going from like the bottom to the top.

2136
02:10:26,000 --> 02:10:27,000
Yes.

2137
02:10:27,000 --> 02:10:29,000
And you're referring to the bottom to top axis, right?

2138
02:10:29,000 --> 02:10:33,000
Yeah, I'm sorry, I was using an MLP mindset when I asked that question.

2139
02:10:33,000 --> 02:10:36,000
So as you say, in a transformer it's an autoregressive model

2140
02:10:36,000 --> 02:10:41,000
and you have stacked attention layers with little MLPs on the end.

2141
02:10:41,000 --> 02:10:44,000
So I guess the way I was actually meaning the question is,

2142
02:10:44,000 --> 02:10:50,000
so complexity increases monotonically as you go up the stack of attention layers.

2143
02:10:50,000 --> 02:10:52,000
Is that a fair assumption?

2144
02:10:52,000 --> 02:10:54,000
Yep.

2145
02:10:54,000 --> 02:10:56,000
Again, no one's really shown this properly.

2146
02:10:56,000 --> 02:10:59,000
But I'm like, surely this is true.

2147
02:10:59,000 --> 02:11:03,000
And there's been some work doing things like looking at neurons,

2148
02:11:03,000 --> 02:11:05,000
looking at the text that activates them, looking for patterns

2149
02:11:05,000 --> 02:11:08,000
and trying to understand what these represent.

2150
02:11:08,000 --> 02:11:13,000
And it's generally looked like early ones are more about detokenization and syntax.

2151
02:11:13,000 --> 02:11:16,000
Later ones are doing stuff that's interesting.

2152
02:11:16,000 --> 02:11:19,000
Final ones are doing this like motor neuron behavior.

2153
02:11:19,000 --> 02:11:24,000
But I also want to be very clear that networks are cursed.

2154
02:11:24,000 --> 02:11:27,000
Networks do not fit into nice abstraction.

2155
02:11:27,000 --> 02:11:31,000
I'm not saying the early layers are literally only doing detokenization.

2156
02:11:31,000 --> 02:11:32,000
Yeah.

2157
02:11:32,000 --> 02:11:35,000
But I believe we have shown it's part of what they're doing.

2158
02:11:35,000 --> 02:11:38,000
And I speculate it is a large part of what they're doing.

2159
02:11:38,000 --> 02:11:41,000
I'd be very surprised if it's all of what they're doing.

2160
02:11:41,000 --> 02:11:45,000
Because I heard you on another podcast and you were just talking about the,

2161
02:11:45,000 --> 02:11:48,000
I mean, I think the curse is the right way to describe it,

2162
02:11:48,000 --> 02:11:52,000
which is that even when you make modifications,

2163
02:11:52,000 --> 02:11:54,000
when you manipulate what's happening,

2164
02:11:54,000 --> 02:11:57,000
the behavior will change in a very reflexive way.

2165
02:11:57,000 --> 02:12:00,000
So you kind of, you delete one thing and then another neuron

2166
02:12:00,000 --> 02:12:03,000
will take on the responsibility of the thing you just deleted.

2167
02:12:03,000 --> 02:12:07,000
And so it's a little bit like manipulating financial markets.

2168
02:12:07,000 --> 02:12:12,000
You've got almost like this weird collective diffuse intelligence

2169
02:12:12,000 --> 02:12:17,000
where you make one modification and the whole thing changes in a very complex way.

2170
02:12:17,000 --> 02:12:21,000
And similarly, I guess that's why I was intuitively questioning the assumption

2171
02:12:21,000 --> 02:12:23,000
that you have a residual stream.

2172
02:12:23,000 --> 02:12:26,000
So surely even at the very top of that attention stack,

2173
02:12:26,000 --> 02:12:31,000
there must be primitive and complex operations going on in some weird mix.

2174
02:12:31,000 --> 02:12:34,000
Since probably true.

2175
02:12:34,000 --> 02:12:41,000
Generally, yeah, there's going to be some stuff you can just do with literally the embeddings.

2176
02:12:41,000 --> 02:12:46,000
Some stuff that you need to wait a bit more before you can do anything useful with.

2177
02:12:46,000 --> 02:12:51,000
Just like, no, if you got a sentence about Michael Jordan,

2178
02:12:51,000 --> 02:12:54,000
I don't think you can use Michael Jordan in isolation.

2179
02:12:54,000 --> 02:12:57,000
So you need to de-tokenize to Michael Jordan.

2180
02:12:57,000 --> 02:13:00,000
But also, I don't know, if you've got Barack Obama,

2181
02:13:00,000 --> 02:13:05,000
Obama and Barack both on their own pretty clearly imply it's going to be about Obama.

2182
02:13:05,000 --> 02:13:10,000
And probably the model can start doing some processing in the early like layer zero.

2183
02:13:10,000 --> 02:13:12,000
Does it want to?

2184
02:13:12,000 --> 02:13:13,000
Somewhat unclear.

2185
02:13:13,000 --> 02:13:16,000
It's going to depend a lot on the model's constraints and other circuitry

2186
02:13:16,000 --> 02:13:20,000
and how much it's worth spending the premises then versus later.

2187
02:13:20,000 --> 02:13:24,000
There's also some various things where, I don't know,

2188
02:13:24,000 --> 02:13:31,000
model memory kind of decays over time because the residual stream's norm gets bigger.

2189
02:13:31,000 --> 02:13:35,000
So early layer outputs become a smaller fraction of the overall thing.

2190
02:13:35,000 --> 02:13:38,000
And layer norm sets the norm to be unit.

2191
02:13:38,000 --> 02:13:40,000
So things kind of decay.

2192
02:13:40,000 --> 02:13:44,000
And so if you compute a feature in the early in like layer zero,

2193
02:13:44,000 --> 02:13:49,000
it can be harder to notice by like layer three than if it was computed in layer two.

2194
02:13:49,000 --> 02:13:52,000
But these are all just kind of like mild nudges.

2195
02:13:52,000 --> 02:13:56,000
And ultimately neural networks do what neural networks want, man.

2196
02:13:56,000 --> 02:13:58,000
I know, I know.

2197
02:13:58,000 --> 02:14:01,000
I just want to close the loop on something I said a little while ago about, you know,

2198
02:14:01,000 --> 02:14:06,000
potentially large models use most of their representational capacity for, you know,

2199
02:14:06,000 --> 02:14:08,000
learning these semantic relationships.

2200
02:14:08,000 --> 02:14:13,000
And empirically, we found that, you know, there's some question recently actually about,

2201
02:14:13,000 --> 02:14:16,000
do we actually need to have really, really large models?

2202
02:14:16,000 --> 02:14:21,000
And for pure knowledge representation, the argument seems to be yes,

2203
02:14:21,000 --> 02:14:26,000
but we can disentangle knowing from reasoning.

2204
02:14:26,000 --> 02:14:28,000
And there's also this mimicry thing.

2205
02:14:28,000 --> 02:14:32,000
So it's quite interesting that all of the, you know, like Facebook released their model

2206
02:14:32,000 --> 02:14:36,000
and very, very quickly people fine-tuned it using the law, you know,

2207
02:14:36,000 --> 02:14:39,000
the low-rank approximation fine-tuning method.

2208
02:14:39,000 --> 02:14:42,000
And on all of the benchmarks, the model, I mean even open assistant,

2209
02:14:42,000 --> 02:14:45,000
there's another great example, Yannick was sitting in your seat just a few weeks ago

2210
02:14:45,000 --> 02:14:48,000
and we're saying that on many of the benchmarks, the model's working really well,

2211
02:14:48,000 --> 02:14:50,000
but it's kind of not.

2212
02:14:50,000 --> 02:14:54,000
It's kind of mimicry, like the big, large models that, you know,

2213
02:14:54,000 --> 02:14:58,000
Meta and Google and DeepMind and all these people, they spend millions training these models

2214
02:14:58,000 --> 02:15:02,000
and they have base knowledge about the world,

2215
02:15:02,000 --> 02:15:07,000
which is not going to be, you know, replicated by fine-tuning, you know,

2216
02:15:07,000 --> 02:15:09,000
like an open source model anytime soon.

2217
02:15:09,000 --> 02:15:11,000
The knowledge is based.

2218
02:15:11,000 --> 02:15:14,000
The knowledge is based.

2219
02:15:14,000 --> 02:15:16,000
Yes, yes, yes, exactly.

2220
02:15:16,000 --> 02:15:20,000
Well, okay, so that's very interesting.

2221
02:15:20,000 --> 02:15:22,000
Let's just quickly talk about the OpenAI microscope,

2222
02:15:22,000 --> 02:15:28,000
because this is, the OpenAI microscope is this beautiful app that OpenAI released in 2020.

2223
02:15:28,000 --> 02:15:32,000
And you can go on there and you can click on any of the neurons

2224
02:15:32,000 --> 02:15:35,000
in popular vision architectures at the time.

2225
02:15:35,000 --> 02:15:38,000
So, you know, I think most of them are sort of like ImageNet, you know,

2226
02:15:38,000 --> 02:15:41,000
things like AlexNet and God knows what else.

2227
02:15:41,000 --> 02:15:46,000
And they solve this optimization problem where they generate an image

2228
02:15:46,000 --> 02:15:51,000
using stochastic gradient descent that maximally activates a particular neuron,

2229
02:15:51,000 --> 02:15:54,000
or I think even a layer, using something similar to DeepDream.

2230
02:15:54,000 --> 02:15:57,000
And you can click on these neurons,

2231
02:15:57,000 --> 02:16:02,000
and sometimes they are what we will call poly sort of monosemantic,

2232
02:16:02,000 --> 02:16:04,000
which means it's just Canada.

2233
02:16:04,000 --> 02:16:08,000
A lot of the time there's a couple of concepts in there that it's weirdly intelligible.

2234
02:16:08,000 --> 02:16:12,000
You know, you might see, you know, like a playing card or an ace

2235
02:16:12,000 --> 02:16:16,000
and a couple of like tangentially related concepts.

2236
02:16:16,000 --> 02:16:19,000
And it always struck me as strange,

2237
02:16:19,000 --> 02:16:23,000
because I imagine there's a long tail of semantic relationships.

2238
02:16:23,000 --> 02:16:27,000
And I found it bizarre that there'd only be one or two in this visualization.

2239
02:16:27,000 --> 02:16:31,000
And I had this intuition that the optimization algorithm is in some sense

2240
02:16:31,000 --> 02:16:35,000
mode seeking rather than distribution matching,

2241
02:16:35,000 --> 02:16:40,000
which is to say that it finds the two most or two or three or four

2242
02:16:40,000 --> 02:16:44,000
most kind of salient semantic mappings,

2243
02:16:44,000 --> 02:16:46,000
and they dominate what is visualized,

2244
02:16:46,000 --> 02:16:51,000
and you're almost snipping off the long tail of the other semantic mappings.

2245
02:16:51,000 --> 02:16:56,000
Yeah, so I think there's two things to disentangle here.

2246
02:16:56,000 --> 02:17:02,000
The first is what is actually represented by the neuron in terms of ground truth.

2247
02:17:02,000 --> 02:17:06,000
And the second is what our techniques show us.

2248
02:17:06,000 --> 02:17:11,000
So the two techniques used in the open-air microscope

2249
02:17:11,000 --> 02:17:15,000
are looking at the images the most activated neuron,

2250
02:17:15,000 --> 02:17:20,000
and then this feature visualization technique where they produce a synthetic image

2251
02:17:20,000 --> 02:17:22,000
that maximally activates it.

2252
02:17:22,000 --> 02:17:26,000
And to me, this is, these are like,

2253
02:17:26,000 --> 02:17:29,000
both of these can be misleading,

2254
02:17:29,000 --> 02:17:32,000
because if the model activates the dice in poetry,

2255
02:17:32,000 --> 02:17:36,000
but activates dice with strength five and poetry with strength four,

2256
02:17:36,000 --> 02:17:39,000
then the optimally-bished activated will be dice,

2257
02:17:39,000 --> 02:17:43,000
and the optimal, the data set examples will also be dice.

2258
02:17:43,000 --> 02:17:45,000
But really, it'll be about poetry.

2259
02:17:45,000 --> 02:17:47,000
And you want to get a lot more rigorous.

2260
02:17:47,000 --> 02:17:50,000
You want to show true monosumanticity.

2261
02:17:50,000 --> 02:17:53,000
One cute thing is spectrum plots.

2262
02:17:53,000 --> 02:17:58,000
You take lots of example, data set examples across the full distribution,

2263
02:17:58,000 --> 02:18:03,000
you have a histogram with the different groups for the different meanings,

2264
02:18:03,000 --> 02:18:06,000
and then neural activation on the x-axis.

2265
02:18:06,000 --> 02:18:10,000
We have this really cute plot in Wes' paper called the French Neuron,

2266
02:18:10,000 --> 02:18:15,000
where all of the French text is on the right,

2267
02:18:15,000 --> 02:18:17,000
all the non-French text is on the left,

2268
02:18:17,000 --> 02:18:20,000
and the neuron is just very clearly distinguishing the two

2269
02:18:20,000 --> 02:18:24,000
in a way that's much more convincing to me than things like Max Act examples.

2270
02:18:25,000 --> 02:18:31,000
And I actually have a hobby project called Neuroscope at Neuroscopes.io,

2271
02:18:31,000 --> 02:18:34,000
where you can see the Max activating text examples

2272
02:18:34,000 --> 02:18:37,000
for every neuron and a bunch of language models,

2273
02:18:37,000 --> 02:18:41,000
though opening I recently output this paper with one that is better,

2274
02:18:41,000 --> 02:18:44,000
but only for GP2 XL.

2275
02:18:44,000 --> 02:18:47,000
Anyway, not that I'm bitter or anything.

2276
02:18:47,000 --> 02:18:48,000
Not so.

2277
02:18:48,000 --> 02:18:55,000
And yeah, so yeah, there's the things can lie to and be illusory.

2278
02:18:55,000 --> 02:19:00,000
There's this interesting paper called the Interruptibility Illusion for But,

2279
02:19:00,000 --> 02:19:03,000
which investigated this specific phenomena,

2280
02:19:03,000 --> 02:19:06,000
and in particular that if you take the data set examples

2281
02:19:06,000 --> 02:19:10,000
over some narrow distribution, like Wikipedia or books,

2282
02:19:10,000 --> 02:19:12,000
you can get pretty misleading things,

2283
02:19:12,000 --> 02:19:15,000
though they only looked at residual stream basis elements

2284
02:19:15,000 --> 02:19:18,000
rather than actual MLP neurons, I believe,

2285
02:19:18,000 --> 02:19:21,000
which makes it a bit less compelling.

2286
02:19:21,000 --> 02:19:22,000
Point of order as well.

2287
02:19:22,000 --> 02:19:24,000
We've been saying residual stream quite a lot,

2288
02:19:24,000 --> 02:19:27,000
and Microsoft introduced Resnet in 2015,

2289
02:19:27,000 --> 02:19:31,000
which basically means that between all of the layers,

2290
02:19:31,000 --> 02:19:34,000
the information is being passed up unadulterated,

2291
02:19:34,000 --> 02:19:38,000
so the subsequent layer can choose to either essentially shortcut

2292
02:19:38,000 --> 02:19:42,000
or ignore the previous layer or use some combination,

2293
02:19:42,000 --> 02:19:45,000
and at the time they kind of said it was about the neural network

2294
02:19:45,000 --> 02:19:49,000
being able to learn its own capacity in some sense,

2295
02:19:49,000 --> 02:19:53,000
but could you just give us the way you think about these residual streams?

2296
02:19:53,000 --> 02:19:56,000
Yeah, so I think the standard view of neural networks,

2297
02:19:56,000 --> 02:20:03,000
there are just layers, and layer 5's output is layer 6's input, etc.

2298
02:20:03,000 --> 02:20:09,000
Then people added Resnets, where layer 6's input is layer 5's output,

2299
02:20:09,000 --> 02:20:12,000
plus layer 5's input with the skip connection,

2300
02:20:12,000 --> 02:20:14,000
but I think people normally thought of them as like,

2301
02:20:14,000 --> 02:20:16,000
ah, it's like a cute trick that makes the model better,

2302
02:20:16,000 --> 02:20:19,000
but doesn't massively change my conceptual picture,

2303
02:20:19,000 --> 02:20:24,000
and the framing that I believe was introduced in the mathematical framework,

2304
02:20:24,000 --> 02:20:28,000
this anthropic paper led by Chris Ola, Nelson L. Harsh, and Catherine Olson

2305
02:20:28,000 --> 02:20:31,000
that I was involved with, is actually,

2306
02:20:31,000 --> 02:20:36,000
let's call the thing in the skip connection the residual stream

2307
02:20:36,000 --> 02:20:38,000
and think of it as the central object,

2308
02:20:38,000 --> 02:20:43,000
and draw our model so the residual stream is this big vertical thing,

2309
02:20:43,000 --> 02:20:46,000
and each layer is like a small diversion to the side,

2310
02:20:46,000 --> 02:20:48,000
rather than the other way around,

2311
02:20:48,000 --> 02:20:53,000
and in practice, most circuits involve things skipping many layers,

2312
02:20:53,000 --> 02:20:58,000
and each layer is better thought of as like an incremental update,

2313
02:20:58,000 --> 02:21:02,000
and there's a bunch of earlier transformer interpretability papers

2314
02:21:02,000 --> 02:21:05,000
that I think miss this conceptual point,

2315
02:21:05,000 --> 02:21:08,000
like the interpretability delusion for but what I mentioned earlier,

2316
02:21:08,000 --> 02:21:15,000
and study residual stream basis elements as like layer outputs or something.

2317
02:21:15,000 --> 02:21:17,000
Yeah, I mean, in a sense, you know,

2318
02:21:17,000 --> 02:21:21,000
we were talking about being able to reuse things that you've learned before

2319
02:21:21,000 --> 02:21:23,000
and not having to learn them again,

2320
02:21:23,000 --> 02:21:26,000
and I guess I think of it as a kind of translational equivalence

2321
02:21:26,000 --> 02:21:29,000
in the layer regime,

2322
02:21:29,000 --> 02:21:33,000
which is that you have a computation which is learned early on,

2323
02:21:33,000 --> 02:21:36,000
and now it can just be composed into subsequent layers.

2324
02:21:36,000 --> 02:21:40,000
It's like you've got a menu of computational functions

2325
02:21:40,000 --> 02:21:44,000
that you can call on at any layer.

2326
02:21:44,000 --> 02:21:46,000
Yeah, pretty much.

2327
02:21:46,000 --> 02:21:50,000
I think of it as like the shared memory and shared bandwidth of the model.

2328
02:21:50,000 --> 02:21:52,000
Yeah, almost like a memory bus.

2329
02:21:52,000 --> 02:21:57,000
Yeah, and sometimes models will dedicate neurons like cleaning up the memory

2330
02:21:57,000 --> 02:22:00,000
and deleting things that are no longer needed.

2331
02:22:00,000 --> 02:22:04,000
Yeah, yeah, and is there any interference in that memory bus?

2332
02:22:04,000 --> 02:22:06,000
So much.

2333
02:22:06,000 --> 02:22:08,000
This is the thing of superposition, right?

2334
02:22:08,000 --> 02:22:09,000
Yeah.

2335
02:22:09,000 --> 02:22:11,000
Like the residual stream is doing everything.

2336
02:22:11,000 --> 02:22:15,000
Like there's 50,000 input tokens start,

2337
02:22:15,000 --> 02:22:19,000
and then 4x as many neurons as residual stream dimensions

2338
02:22:19,000 --> 02:22:21,000
in every MLP layer,

2339
02:22:21,000 --> 02:22:23,000
and attention heads moving everything around,

2340
02:22:23,000 --> 02:22:25,000
and it's just a clusterfuck.

2341
02:22:25,000 --> 02:22:28,000
What if you scale up the bandwidth of the bus?

2342
02:22:29,000 --> 02:22:32,000
That's basically making the model bigger, right?

2343
02:22:32,000 --> 02:22:34,000
Which we know makes models better.

2344
02:22:34,000 --> 02:22:37,000
But I don't know, just thinking out loud,

2345
02:22:37,000 --> 02:22:41,000
but what if you maintained the original dimensionality of the model

2346
02:22:41,000 --> 02:22:44,000
but you deliberately upscaled the bus?

2347
02:22:44,000 --> 02:22:48,000
So like you make the thing inside each layer smaller

2348
02:22:48,000 --> 02:22:50,000
but make the residual stream bigger?

2349
02:22:50,000 --> 02:22:52,000
Or just make everything the same as it is,

2350
02:22:52,000 --> 02:22:56,000
but you just kind of like have a linear transformation on the bus

2351
02:22:56,000 --> 02:22:58,000
and double the size of the bus.

2352
02:22:58,000 --> 02:23:02,000
So I don't think that would work

2353
02:23:02,000 --> 02:23:04,000
without increasing the number of parameters,

2354
02:23:04,000 --> 02:23:08,000
because like if you...

2355
02:23:08,000 --> 02:23:11,000
Because like the thing that matters is the smallest bottleneck.

2356
02:23:11,000 --> 02:23:15,000
The output width of an MLP layer are like 4,000 by 1,000,

2357
02:23:15,000 --> 02:23:18,000
and in order to make the 1,000 bigger,

2358
02:23:18,000 --> 02:23:20,000
you need more parameters.

2359
02:23:20,000 --> 02:23:24,000
And there's like all kinds of studies about the optimal hyperparameters

2360
02:23:24,000 --> 02:23:26,000
and optimal ratios.

2361
02:23:26,000 --> 02:23:30,000
My general heuristic is number of parameters are the main thing that matters.

2362
02:23:30,000 --> 02:23:31,000
I don't know.

2363
02:23:31,000 --> 02:23:34,000
I don't spend that much time thinking about how to make models better, to be honest.

2364
02:23:34,000 --> 02:23:36,000
I just want to understand them, goddammit.

2365
02:23:36,000 --> 02:23:39,000
Yeah, because it's one of those things that it might remove bottlenecks

2366
02:23:39,000 --> 02:23:45,000
because essentially you're allowing the model to reuse things that it's learned previously.

2367
02:23:45,000 --> 02:23:49,000
So now every single layer can specialize more than it did before

2368
02:23:49,000 --> 02:23:52,000
and that might kind of like weirdly remove bottlenecks.

2369
02:23:52,000 --> 02:23:54,000
Yeah.

2370
02:23:54,000 --> 02:24:00,000
Yeah, the way I generally think about it is models are ensembles of shallow pods,

2371
02:24:00,000 --> 02:24:03,000
which is this paper from like five years ago about Resnets.

2372
02:24:03,000 --> 02:24:07,000
Like, deep-d2-small is 12 layers.

2373
02:24:07,000 --> 02:24:12,000
Each layer includes an attention block and an attention bit in MLP,

2374
02:24:12,000 --> 02:24:18,000
but it is not the case that most computation is 24 levels of composition deep.

2375
02:24:18,000 --> 02:24:21,000
It is the case that most of them involve like, I don't know, four.

2376
02:24:21,000 --> 02:24:27,000
And they're just intelligently choosing which four and remaking them in interesting ways.

2377
02:24:27,000 --> 02:24:33,000
And sometimes different things will want to like get to different points

2378
02:24:33,000 --> 02:24:38,000
and so it's useful to have many layers rather than a few.

2379
02:24:38,000 --> 02:24:45,000
But also, I don't know, if you halve the residual stream width

2380
02:24:45,000 --> 02:24:50,000
and give the model 4x as many layers, often performance is like about the same.

2381
02:24:51,000 --> 02:24:55,000
Or like not that different because the number of parameters is unchanged.

2382
02:24:55,000 --> 02:25:00,000
And this is just kind of a wild result about models that I think only really makes sense

2383
02:25:00,000 --> 02:25:05,000
within this framework of it's like an ensemble of shallow pods

2384
02:25:05,000 --> 02:25:09,000
and it's a trade-off between having more computation and having better memory bandwidth.

2385
02:25:09,000 --> 02:25:11,000
Yeah, yeah, very interesting.

2386
02:25:11,000 --> 02:25:16,000
Okay, I mean, just to close, superposition, it might not be a new idea.

2387
02:25:16,000 --> 02:25:23,000
So Janik did a paper video about this paper called Supermasks in Superposition

2388
02:25:23,000 --> 02:25:26,000
by Mitchell Wartsman back in 2020.

2389
02:25:26,000 --> 02:25:30,000
And he was talking about supermass representing sparse subnetworks

2390
02:25:30,000 --> 02:25:33,000
in respect of catastrophic forgetting and continual learning.

2391
02:25:33,000 --> 02:25:36,000
But that was slightly different because that was an explicit model

2392
02:25:36,000 --> 02:25:41,000
to perform masking, create subnetworks and to model, you know,

2393
02:25:41,000 --> 02:25:44,000
like basically a sparsity aware algorithm.

2394
02:25:44,000 --> 02:25:48,000
But he was still using a lot of the same language like interference and so on

2395
02:25:48,000 --> 02:25:51,000
and thinking about superpositions of subnetworks.

2396
02:25:51,000 --> 02:25:55,000
And I guess the difference is like just as we were talking about with these inductive priors

2397
02:25:55,000 --> 02:25:59,000
like transformers and CNNs, the models already do this stuff

2398
02:25:59,000 --> 02:26:04,000
without us having to explicitly code it, which I think is the interesting discovery.

2399
02:26:04,000 --> 02:26:09,000
Yeah, yeah, one update I've made from Wes's work is that

2400
02:26:09,000 --> 02:26:14,000
detokenization is probably like a pretty big fraction of what the early layers do.

2401
02:26:14,000 --> 02:26:19,000
And it's just really easy to represent compound words in superposition

2402
02:26:19,000 --> 02:26:23,000
because it's a very binary, it's either there or not there.

2403
02:26:23,000 --> 02:26:26,000
So alternating difference is easy to deal with.

2404
02:26:26,000 --> 02:26:31,000
They're mutually exclusive, so there's no simultaneous interference.

2405
02:26:31,000 --> 02:26:36,000
Like you cannot have Boris Johnson and Theresa May co-occur.

2406
02:26:36,000 --> 02:26:40,000
And there's just like so many of them.

2407
02:26:40,000 --> 02:26:45,000
One fact about language models that people who haven't played around them may not appreciate

2408
02:26:45,000 --> 02:26:48,000
is their inputs are these things called tokens.

2409
02:26:48,000 --> 02:26:54,000
And tokenizers are fucked because they're trained in this bizarre Byzantine way

2410
02:26:54,000 --> 02:26:59,000
that means that often the rarer words will get broken up into many tokens.

2411
02:26:59,000 --> 02:27:00,000
Yes.

2412
02:27:00,000 --> 02:27:03,000
Multi-word phrases are always different tokens.

2413
02:27:03,000 --> 02:27:07,000
Anything that's weird like a URL gets completely cursed.

2414
02:27:07,000 --> 02:27:12,000
And models don't want to have this happen.

2415
02:27:12,000 --> 02:27:18,000
So they devote a bunch of parameters to build a pseudo vocabulary of what's going on.

2416
02:27:18,000 --> 02:27:24,000
And just returning to your point earlier about, like, is it just these syntax-level things?

2417
02:27:24,000 --> 02:27:28,000
Is there some actual more semantic stuff going on?

2418
02:27:28,000 --> 02:27:33,000
We did also have case studies looking at contextual neurons, things like,

2419
02:27:33,000 --> 02:27:38,000
this code is in Python, this language is in French.

2420
02:27:38,000 --> 02:27:42,000
And these were seemingly monosemitic.

2421
02:27:42,000 --> 02:27:45,000
Like it seemed like there were specific neurons here.

2422
02:27:45,000 --> 02:27:49,000
And we found things like if you ablate the French neuron,

2423
02:27:49,000 --> 02:27:53,000
loss on French text gets much worse, what other ones are fine.

2424
02:27:53,000 --> 02:28:00,000
And also some interesting results that the model was, say, using this disambiguate things.

2425
02:28:00,000 --> 02:28:07,000
Like tokens like D are common in German and also common in Dutch.

2426
02:28:07,000 --> 02:28:14,000
And the neurons for those languages were being used to disambiguate for that token,

2427
02:28:14,000 --> 02:28:17,000
whether it was like a German D or a Dutch D,

2428
02:28:17,000 --> 02:28:19,000
because they've got very different meaning in the two languages.

2429
02:28:19,000 --> 02:28:23,000
Yeah, I wondered if you'd give me some interest in that, because as you say, in Wes's paper,

2430
02:28:23,000 --> 02:28:28,000
he did actually find that there are some monosemitic neurons like French, as you just said.

2431
02:28:28,000 --> 02:28:33,000
And in this case, the model decided that interference, in some sense, wasn't worth the burden.

2432
02:28:33,000 --> 02:28:36,000
But what does burden mean here?

2433
02:28:36,000 --> 02:28:40,000
And French is a very vague concept as well.

2434
02:28:40,000 --> 02:28:44,000
Yes. So, all right, a couple of observations.

2435
02:28:44,000 --> 02:28:50,000
First is I do not think we have properly shown they are monosemitic neurons.

2436
02:28:50,000 --> 02:28:55,000
We were looking, these models were trained on the pile, and we were specifically looking at them on Europal,

2437
02:28:55,000 --> 02:29:00,000
which is like a data set of European Parliament transcripts, which are labeled by language.

2438
02:29:00,000 --> 02:29:04,000
And we found a neuron that seemed to strongly disambiguate French from non-French.

2439
02:29:04,000 --> 02:29:08,000
But it was on this domain of parliamentary stuff.

2440
02:29:08,000 --> 02:29:12,000
And because models really want to avoid simultaneous interference,

2441
02:29:12,000 --> 02:29:17,000
if they did have superposition, they'd probably want to do it with something that isn't likely to co-occur in this context.

2442
02:29:17,000 --> 02:29:22,000
I don't know, this is a list variable in Python, which we didn't check very hard for.

2443
02:29:22,000 --> 02:29:29,000
And in particular, this is messy to check for, because in order to do that, you need to answer these questions like,

2444
02:29:29,000 --> 02:29:31,000
what is French?

2445
02:29:31,000 --> 02:29:36,000
Like, there's a bunch of English checks to activate for, but it will activate on words like,

2446
02:29:36,000 --> 02:29:39,000
Sacre Bleu and Trebilla.

2447
02:29:39,000 --> 02:29:46,000
And I think I count this as French, but like, I don't have a rigorous definition of French.

2448
02:29:46,000 --> 02:29:51,000
And I think an open problem I'd love to see someone pursue is just,

2449
02:29:51,000 --> 02:29:55,000
can you prove one of these neurons is actually a French detecting neuron or not?

2450
02:29:55,000 --> 02:29:58,000
And what would it even look like to do that?

2451
02:29:58,000 --> 02:30:04,000
And yeah, regarding interference in the burden, so the way I think about it,

2452
02:30:04,000 --> 02:30:09,000
if two features are not orthogonal, then,

2453
02:30:09,000 --> 02:30:12,000
no, sorry, this is more interesting in the case of neurons.

2454
02:30:12,000 --> 02:30:16,000
If there's multiple things that could all activate a neuron,

2455
02:30:16,000 --> 02:30:22,000
then it's harder for the downstream bit of the model to know how to use the fact that that neuron activated,

2456
02:30:22,000 --> 02:30:27,000
because there are multiple things, even if they don't co-occur, because they're mutually exclusive.

2457
02:30:27,000 --> 02:30:29,000
And this is just a cost.

2458
02:30:29,000 --> 02:30:33,000
And there's a trade-off between having more features and not having this cost.

2459
02:30:33,000 --> 02:30:38,000
And features like this is in French are really load-bearing.

2460
02:30:38,000 --> 02:30:42,000
They're just really important for a lot of circuitry here.

2461
02:30:42,000 --> 02:30:48,000
And so, theoretically, the model might want to dedicate an entire neuron to this,

2462
02:30:48,000 --> 02:30:54,000
but if you dedicate an entire neuron, you lose the ability to do as much superposition.

2463
02:30:54,000 --> 02:30:58,000
My intuition is the number of features that can be represented in superposition

2464
02:30:58,000 --> 02:31:03,000
is actually grows more than linearly with the number of dimensions.

2465
02:31:03,000 --> 02:31:08,000
So this might be significantly worse than just having one fewer feature.

2466
02:31:08,000 --> 02:31:15,000
So we are now in the next chapter of this beautiful podcast, and we're going to talk about transformers.

2467
02:31:15,000 --> 02:31:21,000
So how exactly do transformers represent algorithms and circuits?

2468
02:31:21,000 --> 02:31:25,000
And also, you've written this beautiful mathematical framework about transformers,

2469
02:31:25,000 --> 02:31:30,000
which, of course, is working very closely with Catherine Olsen and Chris Olat.

2470
02:31:30,000 --> 02:31:31,000
And Nelson Olsh.

2471
02:31:31,000 --> 02:31:34,000
And Nelson, my apologies.

2472
02:31:34,000 --> 02:31:39,000
Yeah, so in terms of understanding, yeah.

2473
02:31:39,000 --> 02:31:42,000
So if you wanted to do a mechanism to interpretability on a model,

2474
02:31:42,000 --> 02:31:49,000
you need to really deeply understand the structure of the model.

2475
02:31:49,000 --> 02:31:52,000
What are the layers? What are the parameters? How do they fit together?

2476
02:31:52,000 --> 02:31:56,000
What are the kinds of things that make sense there?

2477
02:31:56,000 --> 02:32:01,000
And let's see.

2478
02:32:01,000 --> 02:32:10,000
So, yeah, there's like a couple of key things I want to emphasize from that paper, though,

2479
02:32:10,000 --> 02:32:14,000
I don't know, it's also one of my, like, all-time top three interpretability papers.

2480
02:32:14,000 --> 02:32:16,000
People should just go read it.

2481
02:32:16,000 --> 02:32:20,000
And after reading it, check out my three-hour video walkthrough about it,

2482
02:32:20,000 --> 02:32:24,000
which apparently is most useful if you've already read the paper,

2483
02:32:24,000 --> 02:32:27,000
because it's that deep anyway.

2484
02:32:27,000 --> 02:32:29,000
Yeah, so a couple of things I want to call out from that,

2485
02:32:29,000 --> 02:32:34,000
especially for people who are kind of familiar with other network but not transformers.

2486
02:32:34,000 --> 02:32:38,000
The first, we've already discussed the residual stream as the central object.

2487
02:32:38,000 --> 02:32:42,000
And the second is how to think about attention,

2488
02:32:42,000 --> 02:32:46,000
because attention is the main thing which is weird about models.

2489
02:32:46,000 --> 02:32:51,000
They have these MLP layers, which actually represent, like,

2490
02:32:51,000 --> 02:32:55,000
two-thirds of the parameters in a transformer, which is often an underrated fact,

2491
02:32:55,000 --> 02:32:58,000
but attention is the interesting stuff.

2492
02:32:58,000 --> 02:33:03,000
So, transformers have a separate residual stream for each input token,

2493
02:33:03,000 --> 02:33:09,000
and this contains, like, all memory the model would store at that position.

2494
02:33:09,000 --> 02:33:13,000
But MLP layers can only process information in place.

2495
02:33:13,000 --> 02:33:16,000
You need attention to move things between positions.

2496
02:33:16,000 --> 02:33:20,000
And classically, people might have used stuff like a 1D convolution.

2497
02:33:20,000 --> 02:33:24,000
You average over 10 things in a sliding window.

2498
02:33:24,000 --> 02:33:31,000
This is baking in the inductive bias that nearby information is more likely to be useful.

2499
02:33:31,000 --> 02:33:37,000
But this is kind of a pretty limited bias to bake in,

2500
02:33:37,000 --> 02:33:42,000
and the story of deep learning is that over time, people have realized,

2501
02:33:42,000 --> 02:33:48,000
wait, we should not be trying to force the model to do specific things.

2502
02:33:48,000 --> 02:33:52,000
We understand, we should not be telling the model how to do its job.

2503
02:33:52,000 --> 02:33:56,000
If it has enough parameters and is competent enough, it can figure it out on its own.

2504
02:33:56,000 --> 02:34:00,000
And so the idea here is rather than giving it a convolution,

2505
02:34:00,000 --> 02:34:06,000
you give it this attention mechanism where each token gets a query saying what it's looking for,

2506
02:34:06,000 --> 02:34:11,000
each previous token gets a key saying what it has to offer,

2507
02:34:11,000 --> 02:34:18,000
and the model looks from each destination token to the source tokens earlier on

2508
02:34:18,000 --> 02:34:22,000
with the keys that are most relevant to the current query.

2509
02:34:22,000 --> 02:34:28,000
And models, and the way to think about an attention head,

2510
02:34:28,000 --> 02:34:32,000
so attention layers break up into these distinct bits called heads,

2511
02:34:32,000 --> 02:34:37,000
which act independently of the others and add to their outputs together,

2512
02:34:37,000 --> 02:34:40,000
and just directly add to the residual string.

2513
02:34:40,000 --> 02:34:44,000
This is sometimes phrases concatenate their outputs and then multiply by a map,

2514
02:34:44,000 --> 02:34:47,000
but this is mathematically equivalent.

2515
02:34:47,000 --> 02:34:52,000
Each head acts independently and in parallel,

2516
02:34:52,000 --> 02:34:58,000
and further, you can think of each head as separately breaking down into a

2517
02:34:58,000 --> 02:35:03,000
which information to move a bit determined by the attention,

2518
02:35:03,000 --> 02:35:07,000
which are determined by the query and key calculating matrices,

2519
02:35:07,000 --> 02:35:11,000
and the what information to move once I know where I'm looking,

2520
02:35:11,000 --> 02:35:16,000
which are determined by the value and output matrices.

2521
02:35:16,000 --> 02:35:21,000
We often think about these in terms of the QK matrix,

2522
02:35:21,000 --> 02:35:26,000
WQ times WK transpose, and the OV matrix,

2523
02:35:26,000 --> 02:35:32,000
WO times WV, because there's no long linearity in between,

2524
02:35:32,000 --> 02:35:37,000
and these two matrices determine like what the head does.

2525
02:35:37,000 --> 02:35:42,000
And the reason I say these are kind of independent is that

2526
02:35:42,000 --> 02:35:46,000
once the model has decided which source tokens to look at,

2527
02:35:46,000 --> 02:35:51,000
the information that gets output by the head is independent of the destination token,

2528
02:35:51,000 --> 02:35:57,000
and like the query only matters for choosing where to move information from,

2529
02:35:57,000 --> 02:36:01,000
and this can result in interesting bugs,

2530
02:36:01,000 --> 02:36:06,000
like there's this motif of a skip trigram,

2531
02:36:06,000 --> 02:36:14,000
the model realizes that if the current thing is three and two has appeared in the past,

2532
02:36:14,000 --> 02:36:16,000
then four is more likely to come next.

2533
02:36:16,000 --> 02:36:20,000
If the current thing is three and four has appeared in the past,

2534
02:36:20,000 --> 02:36:22,000
two is more likely to come next.

2535
02:36:22,000 --> 02:36:29,000
But if you have multiple destination tokens that all want the same source token,

2536
02:36:29,000 --> 02:36:34,000
for example, the phrase keep in mind can be a skip trigram,

2537
02:36:34,000 --> 02:36:36,000
really it should be a trigram,

2538
02:36:36,000 --> 02:36:41,000
but tiny models aren't very good at figuring out what's exactly at the previous position.

2539
02:36:41,000 --> 02:36:44,000
Keep at bay is another trigram,

2540
02:36:44,000 --> 02:36:48,000
but in an at, we'll both look at the same keep token,

2541
02:36:48,000 --> 02:36:54,000
and so they must boost both at and mind for both of them.

2542
02:36:54,000 --> 02:37:01,000
So we'll also predict keep in bay and keep at mind.

2543
02:37:01,000 --> 02:37:09,000
And possibly we should move on to induction heads,

2544
02:37:09,000 --> 02:37:11,000
which are a good illustrative example.

2545
02:37:11,000 --> 02:37:13,000
Yeah, it's going to come onto that.

2546
02:37:13,000 --> 02:37:15,000
So on these induction heads,

2547
02:37:15,000 --> 02:37:18,000
you've said that they seem universal across all models.

2548
02:37:18,000 --> 02:37:23,000
They underlie more complex behavior, like few-shot learning.

2549
02:37:23,000 --> 02:37:25,000
They emerge in a phase transition,

2550
02:37:25,000 --> 02:37:29,000
and they're crucial for this in-context learning.

2551
02:37:29,000 --> 02:37:34,000
And you said that sometimes specific circuits underlie emergent phenomena,

2552
02:37:34,000 --> 02:37:40,000
and we may want to predict or understand emergence by studying these circuits.

2553
02:37:40,000 --> 02:37:43,000
So what do we know so far?

2554
02:37:43,000 --> 02:37:45,000
A lot of questions in there.

2555
02:37:45,000 --> 02:37:47,000
All right, taking this in order.

2556
02:37:47,000 --> 02:37:50,000
So what is an induction head?

2557
02:37:50,000 --> 02:37:52,000
I've already mentioned this briefly.

2558
02:37:52,000 --> 02:37:55,000
Text often contains repeated subsequences,

2559
02:37:55,000 --> 02:38:00,000
like after Tim, Scarf may come next,

2560
02:38:00,000 --> 02:38:03,000
but if Tim Scarf has appeared like five times,

2561
02:38:03,000 --> 02:38:06,000
then it's much more likely to come next.

2562
02:38:06,000 --> 02:38:10,000
In toy two-layer attention-only language models,

2563
02:38:10,000 --> 02:38:14,000
we found this circuit called an induction head, which does this.

2564
02:38:14,000 --> 02:38:18,000
It's a real algorithm that works on, say, repeated random tokens.

2565
02:38:18,000 --> 02:38:23,000
And we have some mechanistic understanding of the basic form of it,

2566
02:38:23,000 --> 02:38:28,000
where there's two attention heads and two different layers working together.

2567
02:38:28,000 --> 02:38:35,000
The later one called an induction head looks from Tim to previous occurrences of Scarf.

2568
02:38:35,000 --> 02:38:38,000
The first one is a previous token head,

2569
02:38:38,000 --> 02:38:41,000
which on each Scarf looks at what came before,

2570
02:38:41,000 --> 02:38:47,000
and is like, ah, this is a Scarf token which has Tim before,

2571
02:38:47,000 --> 02:38:53,000
and then the induction head looks at tokens where the token before them was Tim,

2572
02:38:53,000 --> 02:38:57,000
or where the token before them was equal to the current token.

2573
02:38:57,000 --> 02:39:03,000
And when the induction head decided to look at Scarf,

2574
02:39:03,000 --> 02:39:07,000
which is determined purely by the QK matrix,

2575
02:39:07,000 --> 02:39:12,000
it then just copies that to the app, which is purely done by the OV matrix.

2576
02:39:12,000 --> 02:39:16,000
And I think induction heads are a really interesting circuit case study,

2577
02:39:16,000 --> 02:39:24,000
because induction heads are all of the interesting computation is being done by their attention pattern.

2578
02:39:25,000 --> 02:39:28,000
Tim's scarf could be anywhere in the previous context,

2579
02:39:28,000 --> 02:39:30,000
and this algorithm will still work.

2580
02:39:30,000 --> 02:39:37,000
And this is important, because this is what lets the model do

2581
02:39:37,000 --> 02:39:43,000
tracking of long-range dependencies in the text, where it looks far back.

2582
02:39:43,000 --> 02:39:49,000
And you can't bake this in with a simple thing like a convolutional layer.

2583
02:39:50,000 --> 02:39:56,000
In fact, transformers seem notably better than old architectures like LSTMs and RNNs,

2584
02:39:56,000 --> 02:40:01,000
in part because they have induction heads that let them track long-range dependencies.

2585
02:40:01,000 --> 02:40:04,000
And, yeah.

2586
02:40:04,000 --> 02:40:09,000
And more generally, it often is the case that especially late-layer attention heads,

2587
02:40:09,000 --> 02:40:12,000
the OV bit is kind of boring, it's just copying,

2588
02:40:12,000 --> 02:40:17,000
but figure out where to look is where all of the interesting computation lies.

2589
02:40:17,000 --> 02:40:20,000
So, first of all, just to clarify, because people will know what an attention head is,

2590
02:40:20,000 --> 02:40:25,000
but an induction head is one of these circuits that you're talking about, just so people understand.

2591
02:40:25,000 --> 02:40:32,000
And we should get onto this relationship between induction heads and the emergence of in-context learning.

2592
02:40:32,000 --> 02:40:40,000
And also, you said it's very important that we have this scientific understanding with respect to studying emergence,

2593
02:40:40,000 --> 02:40:46,000
but rather that than just framing of interpretability kind of makes better models.

2594
02:40:46,000 --> 02:40:55,000
Yeah. So, maybe I should first explain what emergence is.

2595
02:40:55,000 --> 02:40:57,000
Let's do that.

2596
02:40:57,000 --> 02:41:07,000
I'd be really, really interested if you could just give me the simplest possible explanation of what you think emergence is.

2597
02:41:07,000 --> 02:41:12,000
Sure. Emergence is when things happen suddenly during training,

2598
02:41:12,000 --> 02:41:19,000
anger from not being there to being there fairly rapidly in a non-convex way, rather than gradually developing.

2599
02:41:19,000 --> 02:41:21,000
Is this interesting you said that?

2600
02:41:21,000 --> 02:41:27,000
Because I think of emergence as a surprising change in macroscopic phenomena,

2601
02:41:27,000 --> 02:41:34,000
and it's an observer-relative term, which means it's always from the perspective of another scale.

2602
02:41:35,000 --> 02:41:44,000
So, just a transient change in perplexity or capability or something in my mind wouldn't entail emergence.

2603
02:41:44,000 --> 02:41:48,000
Like, it would need to be some qualitative, meaningful thing,

2604
02:41:48,000 --> 02:41:52,000
rather than just, oh, the loss curve got notably better in this bit.

2605
02:41:52,000 --> 02:42:03,000
I think so. It's definitely related to some notion of surprise, which is inherently relative.

2606
02:42:03,000 --> 02:42:05,000
Yeah. Let's not get hung up on that.

2607
02:42:05,000 --> 02:42:09,000
So, okay. Let's say it's a transient change in something.

2608
02:42:09,000 --> 02:42:11,000
Mm-hmm. Yeah.

2609
02:42:11,000 --> 02:42:15,000
You know, when you call it transient, it's like an unexpected sudden change.

2610
02:42:15,000 --> 02:42:20,000
Though unexpected has so much semantic meaning on it that I don't want to use.

2611
02:42:20,000 --> 02:42:23,000
But, yeah, this is an infinite rabbit hole.

2612
02:42:23,000 --> 02:42:28,000
Yes. But I think this scale thing is relevant as well.

2613
02:42:28,000 --> 02:42:32,000
We are programming neural networks at the microscopic scale,

2614
02:42:32,000 --> 02:42:37,000
and there's some macroscopic change in capability, so it's some...

2615
02:42:37,000 --> 02:42:39,000
Yes. Yeah.

2616
02:42:39,000 --> 02:42:41,000
Yeah. And there's, like, lots of different dimensions.

2617
02:42:41,000 --> 02:42:45,000
You can have emergence on. You can have it as you train a model on more data.

2618
02:42:45,000 --> 02:42:48,000
You can have it as you make the models bigger.

2619
02:42:48,000 --> 02:42:51,000
And these are both interestingly different kinds.

2620
02:42:51,000 --> 02:42:56,000
One of the more famous examples is Chain of Thought and Few Shot Prompting,

2621
02:42:56,000 --> 02:42:59,000
where DP3 is pretty good at this.

2622
02:42:59,000 --> 02:43:02,000
Earlier models were not good at this. This is kind of surprising.

2623
02:43:02,000 --> 02:43:07,000
Chain of Thought is particularly striking because people just noticed

2624
02:43:07,000 --> 02:43:11,000
a while after DP3 was public that if you tell it to think step by step, it becomes much better.

2625
02:43:11,000 --> 02:43:17,000
There's this recent innovation of Tree of Thought that I'm not particularly familiar with,

2626
02:43:17,000 --> 02:43:23,000
but I understand as kind of like applying Monte Carlo Tree Search on top of Chain of Thought.

2627
02:43:23,000 --> 02:43:25,000
Yes. Yes.

2628
02:43:25,000 --> 02:43:29,000
Where you're like, well, there's many ways we can branch at each point.

2629
02:43:29,000 --> 02:43:33,000
Let's use Tree Search algorithms to find the ultimate way of doing this.

2630
02:43:33,000 --> 02:43:37,000
Yeah. But with, let's say, Scratchpad and Chain of Thought,

2631
02:43:37,000 --> 02:43:40,000
I don't necessarily see that as an emergent...

2632
02:43:40,000 --> 02:43:45,000
Well, maybe there's an emergent reasoning capability that comes into play

2633
02:43:45,000 --> 02:43:48,000
when you have a certain threshold size model,

2634
02:43:48,000 --> 02:43:56,000
but I think of it more as kind of having an intermediate augmented memory in the context.

2635
02:43:56,000 --> 02:44:02,000
So you're kind of filling in a gap in cognition by saying you're allowed to...

2636
02:44:02,000 --> 02:44:07,000
It's not just remembering things, it's also reflecting on things that didn't work.

2637
02:44:07,000 --> 02:44:14,000
Yes. So, yeah, clarifying, when I say emergent, when I say Chain of Thought is an emergent property,

2638
02:44:14,000 --> 02:44:20,000
I mean, the capacity to productively do Chain of Thought is the emergent thing

2639
02:44:20,000 --> 02:44:24,000
and telling the model to things step by step is a user-driven thing.

2640
02:44:24,000 --> 02:44:27,000
But, I don't know, I kind of...

2641
02:44:27,000 --> 02:44:34,000
Just as a point of order, though, was it just that it was discovered after GPT-3

2642
02:44:34,000 --> 02:44:37,000
or would it work on GPT-2?

2643
02:44:38,000 --> 02:44:44,000
I would have guessed it doesn't work very well on GPT-2, but I've not checked.

2644
02:44:44,000 --> 02:44:49,000
I'd be pretty interested... I'm sure someone has looked into this, I haven't looked very hard.

2645
02:44:49,000 --> 02:44:53,000
I guess, like, so a lot of my motivation for this work comes from...

2646
02:44:53,000 --> 02:44:58,000
I care a lot about AIX risk and AI alignment and how to make these systems good for the world.

2647
02:44:58,000 --> 02:45:05,000
And when I see things like, oh, we realize that you can make GPT-3 much better by asking it to things step by step,

2648
02:45:05,000 --> 02:45:08,000
I'm like, oh, no.

2649
02:45:08,000 --> 02:45:13,000
What kinds of things could the systems you make be capable of that we just haven't noticed yet?

2650
02:45:13,000 --> 02:45:17,000
That's the concern that the genie's already out the bottle.

2651
02:45:17,000 --> 02:45:22,000
And, I mean, DeepMind just published this Tree of Thought paper.

2652
02:45:22,000 --> 02:45:24,000
It's a really simple idea.

2653
02:45:24,000 --> 02:45:28,000
It's basically a star search over trajectories of prompts,

2654
02:45:28,000 --> 02:45:33,000
and you use the model itself to evaluate the value of a trajectory.

2655
02:45:33,000 --> 02:45:36,000
And I could have done that.

2656
02:45:36,000 --> 02:45:37,000
Anyone could.

2657
02:45:37,000 --> 02:45:39,000
Similar thing with auto-GPT and all this stuff.

2658
02:45:39,000 --> 02:45:41,000
I'm more skeptical than you are.

2659
02:45:41,000 --> 02:45:49,000
I think in the case of Tree of Thought, it closes a capability gap in respect of certain tasks which were not working very well

2660
02:45:49,000 --> 02:45:52,000
because they don't have that kind of system to...

2661
02:45:52,000 --> 02:45:54,000
Models don't seem to plan ahead very well.

2662
02:45:54,000 --> 02:45:59,000
But I still think that it's not just going to magically turn into super intelligent.

2663
02:45:59,000 --> 02:46:01,000
I mean, we can talk about this a little bit later.

2664
02:46:01,000 --> 02:46:03,000
Yeah, okay.

2665
02:46:03,000 --> 02:46:05,000
Yeah, so...

2666
02:46:05,000 --> 02:46:09,000
Yeah, I think this is also pretty relevant to much more near-term risks.

2667
02:46:09,000 --> 02:46:13,000
I don't know, there's lots of things that a sufficiently capable model could do

2668
02:46:13,000 --> 02:46:15,000
that might be pretty destabilizing to society,

2669
02:46:15,000 --> 02:46:21,000
like write actually much better propaganda than human writers can or something.

2670
02:46:21,000 --> 02:46:25,000
And if Tree of Thought makes it possible to do that

2671
02:46:25,000 --> 02:46:28,000
in a way that we did not think was possible when GPT-4 was deployed,

2672
02:46:28,000 --> 02:46:32,000
that's like an interesting thing that I care about noticing.

2673
02:46:32,000 --> 02:46:35,000
It's not a very good example, but...

2674
02:46:35,000 --> 02:46:38,000
Yeah, it is.

2675
02:46:38,000 --> 02:46:40,000
But being able to...

2676
02:46:40,000 --> 02:46:45,000
I mean, first of all, it's been possible to create misinformation for a long time.

2677
02:46:45,000 --> 02:46:50,000
This is why I specified be able to do it notably better than humans can.

2678
02:46:50,000 --> 02:46:52,000
I totally agree.

2679
02:46:52,000 --> 02:46:56,000
The longer doing it a bit more cheaply and a bit more scale doesn't seem obviously that important.

2680
02:46:56,000 --> 02:46:58,000
You could argue that, like, I don't know,

2681
02:46:58,000 --> 02:47:02,000
being a spam bot that feels indistinguishable from a human

2682
02:47:02,000 --> 02:47:07,000
is like a more novel thing that's actually different.

2683
02:47:07,000 --> 02:47:08,000
Yeah.

2684
02:47:08,000 --> 02:47:11,000
But, I don't know, this is like an off-the-cuff example.

2685
02:47:11,000 --> 02:47:14,000
I don't want to get too deep into this,

2686
02:47:14,000 --> 02:47:16,000
because it's not a point I care that deeply about.

2687
02:47:16,000 --> 02:47:18,000
Yeah, I mean, we can come back to it a bit,

2688
02:47:18,000 --> 02:47:20,000
but I think we are nearly already there.

2689
02:47:20,000 --> 02:47:21,000
Yeah.

2690
02:47:21,000 --> 02:47:23,000
You know, this irreversibility thing.

2691
02:47:23,000 --> 02:47:26,000
We don't know.

2692
02:47:26,000 --> 02:47:29,000
Computer games are photorealistic.

2693
02:47:29,000 --> 02:47:32,000
Chatbots are indistinguishable,

2694
02:47:32,000 --> 02:47:35,000
and AI art is pretty much indistinguishable.

2695
02:47:35,000 --> 02:47:37,000
And that could work.

2696
02:47:37,000 --> 02:47:39,000
I mean, I spoke to Daniel Dennett about it last week,

2697
02:47:39,000 --> 02:47:44,000
and he said he's really worried about the epistemic erosion of our society,

2698
02:47:44,000 --> 02:47:47,000
more so interestingly than the ontological erosion.

2699
02:47:47,000 --> 02:47:51,000
And I discovered later that's because he's not a big fan of anything ontological.

2700
02:47:52,000 --> 02:47:56,000
Yeah, it is potentially a problem,

2701
02:47:56,000 --> 02:48:02,000
but I guess to me, people might overestimate the scale

2702
02:48:02,000 --> 02:48:05,000
and magnitude of change of this.

2703
02:48:05,000 --> 02:48:09,000
I feel that, I know I don't want to echo Sam Altman here,

2704
02:48:09,000 --> 02:48:11,000
but he said that we are reasonably smart people,

2705
02:48:11,000 --> 02:48:17,000
and we can adapt and recognize deep fakes and so on.

2706
02:48:17,000 --> 02:48:18,000
Yeah.

2707
02:48:18,000 --> 02:48:22,000
These are complicated societal questions.

2708
02:48:22,000 --> 02:48:25,000
I guess I mostly just have the position of man.

2709
02:48:25,000 --> 02:48:28,000
It sure is kind of concerning that we have these systems

2710
02:48:28,000 --> 02:48:30,000
that could potentially pose risks,

2711
02:48:30,000 --> 02:48:33,000
but you don't know what they do and decide to deploy them,

2712
02:48:33,000 --> 02:48:35,000
and then we discover things they can do.

2713
02:48:35,000 --> 02:48:40,000
And I think that the research direction I'm trying to advocate for here

2714
02:48:40,000 --> 02:48:44,000
is just better learn how to predict this stuff more than anything,

2715
02:48:44,000 --> 02:48:47,000
which hopefully we can all agree is like an interesting direction.

2716
02:48:47,000 --> 02:48:50,000
And there's all kind of debates about is emergent phenomena

2717
02:48:50,000 --> 02:48:52,000
like actually a real thing?

2718
02:48:52,000 --> 02:48:55,000
Like this recent, is this a mirage paper,

2719
02:48:55,000 --> 02:48:58,000
which I think was a bit over-claiming,

2720
02:48:58,000 --> 02:49:01,000
but does make a good point that if you choose your metric

2721
02:49:01,000 --> 02:49:05,000
to be sufficiently sharp, everything looks dramatic.

2722
02:49:05,000 --> 02:49:08,000
One thing I've definitely observed is if you have an accuracy graph

2723
02:49:08,000 --> 02:49:11,000
with a log scale x-axis for grokking,

2724
02:49:11,000 --> 02:49:14,000
it looks fantastically dramatic.

2725
02:49:14,000 --> 02:49:18,000
And I was very careful to not do this in my paper

2726
02:49:18,000 --> 02:49:21,000
because it is cheating.

2727
02:49:21,000 --> 02:49:24,000
But yeah.

2728
02:49:24,000 --> 02:49:30,000
So my particular hot take is that I believe emergence is often underlain

2729
02:49:30,000 --> 02:49:34,000
by the model learning some specific circuit

2730
02:49:34,000 --> 02:49:39,000
or some small family of circuits in a fairly sudden phase transition

2731
02:49:39,000 --> 02:49:43,000
that enables this overall emergent thing.

2732
02:49:43,000 --> 02:49:46,000
And this sequel paper led by Catherine Olson

2733
02:49:46,000 --> 02:49:48,000
in Contest Learning and Induction Heads

2734
02:49:48,000 --> 02:49:51,000
is a big motivator of my belief for this.

2735
02:49:51,000 --> 02:49:56,000
So the idea of the paper is we have this,

2736
02:49:56,000 --> 02:50:00,000
we found induction heads in these toy-till-artentionally models.

2737
02:50:00,000 --> 02:50:03,000
We somewhat mechanistically understood them,

2738
02:50:03,000 --> 02:50:06,000
at least in the simplest case of induction.

2739
02:50:06,000 --> 02:50:10,000
We use this to come up with more of the behavioral test

2740
02:50:10,000 --> 02:50:12,000
for whether it's induction heads.

2741
02:50:12,000 --> 02:50:14,000
You just give them all repeated random tokens

2742
02:50:14,000 --> 02:50:16,000
and you look at whether it looks induction-y.

2743
02:50:16,000 --> 02:50:20,000
And we found that these occurred in basically all models we looked at,

2744
02:50:20,000 --> 02:50:25,000
up to 13B, even though we didn't fully reverse engineer them there.

2745
02:50:25,000 --> 02:50:29,000
And we then found that this was really deeply linked

2746
02:50:29,000 --> 02:50:32,000
to the emergence of in-context learning.

2747
02:50:32,000 --> 02:50:35,000
There's a lot of jargon in there, so let's unpack that.

2748
02:50:35,000 --> 02:50:37,000
In-context learning, already briefly mentioned,

2749
02:50:37,000 --> 02:50:40,000
it's like tracking long-range dependencies in text, like,

2750
02:50:40,000 --> 02:50:43,000
you can use what was on, which was three pages ago,

2751
02:50:43,000 --> 02:50:46,000
to predict what comes next in the current book,

2752
02:50:46,000 --> 02:50:49,000
which is a non-trovial thing.

2753
02:50:49,000 --> 02:50:52,000
It is not obvious to me how I would program a model to do.

2754
02:50:52,000 --> 02:50:55,000
In-context learning is emergent.

2755
02:50:55,000 --> 02:50:59,000
If you operationalize it as average loss on the 500th token

2756
02:50:59,000 --> 02:51:01,000
versus average loss on the 50th token,

2757
02:51:01,000 --> 02:51:04,000
there's a fairly sudden period in training

2758
02:51:04,000 --> 02:51:09,000
where it goes from not very good at it to very good at it.

2759
02:51:09,000 --> 02:51:11,000
Just a tiny point forward of that.

2760
02:51:11,000 --> 02:51:13,000
One interesting thing about in-context learning

2761
02:51:13,000 --> 02:51:16,000
is you're learning at inference time, not training time.

2762
02:51:16,000 --> 02:51:17,000
Yes.

2763
02:51:17,000 --> 02:51:20,000
But you're not changing anything in the underlying model,

2764
02:51:20,000 --> 02:51:23,000
which means anything it can do, presumably,

2765
02:51:23,000 --> 02:51:29,000
must be materializing a competence which was acquired during training.

2766
02:51:29,000 --> 02:51:32,000
So it's coming back to this periodic table thing, right?

2767
02:51:32,000 --> 02:51:34,000
You've just learned all these platonic primitives.

2768
02:51:34,000 --> 02:51:36,000
You do this in-context learning.

2769
02:51:36,000 --> 02:51:38,000
You say, I want you to do this. Here's an example.

2770
02:51:38,000 --> 02:51:43,000
And you've got all of these freeze-dried periodic computational circuits,

2771
02:51:43,000 --> 02:51:46,000
and they spring into life, and they compose together,

2772
02:51:46,000 --> 02:51:48,000
and they do the thing.

2773
02:51:48,000 --> 02:51:49,000
Yes.

2774
02:51:49,000 --> 02:51:50,000
Yes.

2775
02:51:50,000 --> 02:51:54,000
I think induction heads are, to my eyes,

2776
02:51:54,000 --> 02:51:57,000
the canonical example of an inference time algorithm

2777
02:51:57,000 --> 02:52:00,000
stored in the model's weights that get supplied.

2778
02:52:00,000 --> 02:52:04,000
And I'm sure there's a bunch more that no one has yet found.

2779
02:52:04,000 --> 02:52:08,000
And, yeah, a lot of my model is that prompt engineering

2780
02:52:08,000 --> 02:52:11,000
is just telling the model which of its circuits to activate

2781
02:52:11,000 --> 02:52:15,000
and just engaging with various quirks of training

2782
02:52:15,000 --> 02:52:18,000
that have made it more or less steerable in different ways.

2783
02:52:18,000 --> 02:52:22,000
And, yeah, so induction heads also emerge

2784
02:52:22,000 --> 02:52:24,000
in a fairly sudden phase transition.

2785
02:52:24,000 --> 02:52:28,000
And we, and exactly at the same time,

2786
02:52:28,000 --> 02:52:30,000
and we present a bunch more evidence in the paper

2787
02:52:30,000 --> 02:52:33,000
that there's, like, actually a causal link here.

2788
02:52:33,000 --> 02:52:37,000
Like, one-layer models have neither the in-context learning

2789
02:52:37,000 --> 02:52:39,000
or the induction heads phase chain,

2790
02:52:39,000 --> 02:52:41,000
because they can't do induction heads,

2791
02:52:41,000 --> 02:52:43,000
because they're only one layer.

2792
02:52:43,000 --> 02:52:46,000
But if you adapt the architectures,

2793
02:52:46,000 --> 02:52:48,000
they can form induction heads with only one layer.

2794
02:52:48,000 --> 02:52:50,000
Now they have both of these phenomena.

2795
02:52:50,000 --> 02:52:52,000
If you obliterate induction heads,

2796
02:52:52,000 --> 02:52:54,000
in-context learning gets systematically worse.

2797
02:52:54,000 --> 02:52:59,000
And a particularly fun qualitative study

2798
02:52:59,000 --> 02:53:03,000
was looking at soft induction heads,

2799
02:53:03,000 --> 02:53:07,000
heads that seem to be doing something induction-y

2800
02:53:07,000 --> 02:53:09,000
in other domains,

2801
02:53:09,000 --> 02:53:12,000
like a head which attends from the current word in English

2802
02:53:12,000 --> 02:53:15,000
to the thing after the current word in French.

2803
02:53:15,000 --> 02:53:18,000
Or, more excitingly, a few-shot learning head

2804
02:53:18,000 --> 02:53:23,000
on this random synthetic pattern recognition task we made

2805
02:53:23,000 --> 02:53:26,000
where it attended back to the most relevant examples

2806
02:53:26,000 --> 02:53:29,000
to the current one.

2807
02:53:29,000 --> 02:53:32,000
And my interpretation of all this

2808
02:53:32,000 --> 02:53:35,000
is that there's something fairly fundamental

2809
02:53:35,000 --> 02:53:39,000
about the induction-y algorithm for in-context learning.

2810
02:53:39,000 --> 02:53:42,000
So the way I think about it,

2811
02:53:42,000 --> 02:53:45,000
let's say you've got two...

2812
02:53:45,000 --> 02:53:47,000
You want to learn some relation.

2813
02:53:47,000 --> 02:53:50,000
You've got some local context A

2814
02:53:50,000 --> 02:53:52,000
and some past context B.

2815
02:53:52,000 --> 02:53:55,000
And if you observe A and you observe B in the past,

2816
02:53:55,000 --> 02:53:58,000
this gives you some information about what comes next.

2817
02:53:58,000 --> 02:54:01,000
There's two ways this could work out.

2818
02:54:01,000 --> 02:54:03,000
It could be symmetric.

2819
02:54:03,000 --> 02:54:05,000
B helps A and A helps B.

2820
02:54:05,000 --> 02:54:07,000
Or asymmetric.

2821
02:54:07,000 --> 02:54:09,000
B helps A, but A does not help B

2822
02:54:09,000 --> 02:54:11,000
if they're the other way around.

2823
02:54:11,000 --> 02:54:14,000
Asymmetric might be like knowing the title of a book

2824
02:54:14,000 --> 02:54:16,000
tells you what comes next,

2825
02:54:16,000 --> 02:54:19,000
but knowing what's in a random paragraph in the previous bit

2826
02:54:19,000 --> 02:54:22,000
doesn't tell you the title.

2827
02:54:22,000 --> 02:54:25,000
While symmetric is like...

2828
02:54:25,000 --> 02:54:27,000
I know, English sentence helps French sentence,

2829
02:54:27,000 --> 02:54:29,000
French sentence helps English sentence.

2830
02:54:29,000 --> 02:54:34,000
And if you have N symmetric relations,

2831
02:54:34,000 --> 02:54:38,000
like English, French, German, Dutch, Latin, whatever,

2832
02:54:38,000 --> 02:54:40,000
where each of them helps each other,

2833
02:54:40,000 --> 02:54:42,000
this is really efficient to represent.

2834
02:54:42,000 --> 02:54:44,000
Because rather than needing to represent

2835
02:54:44,000 --> 02:54:47,000
N squared different relations separately,

2836
02:54:47,000 --> 02:54:49,000
like you would in the asymmetric case,

2837
02:54:49,000 --> 02:54:52,000
you can just map everything to the same latent space

2838
02:54:52,000 --> 02:54:54,000
and look for matches.

2839
02:54:54,000 --> 02:54:57,000
And fundamentally, this is what induction heads are doing.

2840
02:54:57,000 --> 02:55:00,000
They're mapping current token

2841
02:55:00,000 --> 02:55:03,000
and previous token of thing in the past

2842
02:55:03,000 --> 02:55:05,000
to the same latent space and looking for matches.

2843
02:55:05,000 --> 02:55:09,000
And to me, this is just like a fairly natural primitive

2844
02:55:09,000 --> 02:55:11,000
of attention.

2845
02:55:11,000 --> 02:55:15,000
And this is exciting because, A,

2846
02:55:15,000 --> 02:55:17,000
we found this deep primitive

2847
02:55:17,000 --> 02:55:20,000
by looking at toy two-layer attentionally models.

2848
02:55:20,000 --> 02:55:24,000
B, it was important for understanding

2849
02:55:24,000 --> 02:55:27,000
and ideally for predicting the emergent phenomena

2850
02:55:27,000 --> 02:55:29,000
of in-context learning.

2851
02:55:29,000 --> 02:55:33,000
And two takeaways I have from this

2852
02:55:33,000 --> 02:55:35,000
about work we should be doing.

2853
02:55:35,000 --> 02:55:37,000
The first is we should be going harder

2854
02:55:37,000 --> 02:55:39,000
at looking at toy language models.

2855
02:55:39,000 --> 02:55:42,000
Like open source to scan of 12 of them.

2856
02:55:42,000 --> 02:55:44,000
And I'd love to see what people can find

2857
02:55:44,000 --> 02:55:47,000
in one-layer models with MLPs

2858
02:55:47,000 --> 02:55:50,000
because we really suck at transformer MLP layers.

2859
02:55:50,000 --> 02:55:53,000
And one layer should just be easier than other ones.

2860
02:55:53,000 --> 02:55:56,000
And the second thing is

2861
02:55:56,000 --> 02:55:59,000
I really want a better

2862
02:55:59,000 --> 02:56:02,000
and more scientific understanding of emergence.

2863
02:56:02,000 --> 02:56:04,000
Why does that happen?

2864
02:56:04,000 --> 02:56:07,000
Really understanding particularly notable case studies of it.

2865
02:56:07,000 --> 02:56:10,000
Testing the hypothesis that it is driven

2866
02:56:10,000 --> 02:56:14,000
by specific kinds of circuits like induction heads

2867
02:56:14,000 --> 02:56:18,000
or at least specific families of circuits.

2868
02:56:18,000 --> 02:56:20,000
Even though, I don't know,

2869
02:56:20,000 --> 02:56:22,000
you could argue that because we haven't fully reverse engineered

2870
02:56:22,000 --> 02:56:24,000
the things in the larger models,

2871
02:56:24,000 --> 02:56:26,000
we really know it's actually an induction head.

2872
02:56:26,000 --> 02:56:30,000
And yeah.

2873
02:56:30,000 --> 02:56:32,000
More generally, a lot of my vision

2874
02:56:32,000 --> 02:56:34,000
for why mechantup matters

2875
02:56:34,000 --> 02:56:38,000
is this kind of scientific understanding of models.

2876
02:56:38,000 --> 02:56:41,000
Like I don't care about making models better,

2877
02:56:41,000 --> 02:56:43,000
but I care about knowing what's going to happen,

2878
02:56:43,000 --> 02:56:45,000
knowing why stuff happens,

2879
02:56:45,000 --> 02:56:47,000
achieving real understanding.

2880
02:56:47,000 --> 02:56:50,000
And getting a scientific understanding

2881
02:56:50,000 --> 02:56:52,000
of things like emergence

2882
02:56:52,000 --> 02:56:54,000
seems like one of the things mechantup

2883
02:56:54,000 --> 02:56:56,000
might be uniquely suited to do,

2884
02:56:56,000 --> 02:56:58,000
but also no one checked very hard.

2885
02:56:58,000 --> 02:57:01,000
And you, dear listener, could be the person who checks.

2886
02:57:01,000 --> 02:57:04,000
So there was a paper by Kevin Wang et al.

2887
02:57:04,000 --> 02:57:06,000
called Interpretability in the Wild,

2888
02:57:06,000 --> 02:57:11,000
a circuit for indirect object identification in GPT-2 Small,

2889
02:57:11,000 --> 02:57:14,000
which found a circuit for indirect object identification.

2890
02:57:14,000 --> 02:57:19,000
So they discovered backup name and mover heads,

2891
02:57:19,000 --> 02:57:21,000
which normally don't do much.

2892
02:57:21,000 --> 02:57:24,000
They take over when the main name mover head are ablated.

2893
02:57:24,000 --> 02:57:27,000
And they said mechanistic interpretability

2894
02:57:27,000 --> 02:57:30,000
has a validation set for more scalability techniques.

2895
02:57:30,000 --> 02:57:32,000
They've understood a clear place

2896
02:57:32,000 --> 02:57:35,000
that these ablations can be misleading.

2897
02:57:35,000 --> 02:57:37,000
So...

2898
02:57:37,000 --> 02:57:39,000
Yeah.

2899
02:57:39,000 --> 02:57:41,000
So, yeah, bunch one pack in there.

2900
02:57:41,000 --> 02:57:45,000
So I really like the interpretability in the wild paper.

2901
02:57:45,000 --> 02:57:48,000
Also, Kevin was only 17 when he wrote it.

2902
02:57:48,000 --> 02:57:52,000
Like, man, I was doing nothing remotely as interesting

2903
02:57:52,000 --> 02:57:54,000
when I was in high school.

2904
02:57:54,000 --> 02:57:56,000
So props to him.

2905
02:57:56,000 --> 02:57:59,000
But also a sign of how easy it is

2906
02:57:59,000 --> 02:58:01,000
to pick low hanging fruit

2907
02:58:01,000 --> 02:58:04,000
and do groundbreaking interpretability work.

2908
02:58:05,000 --> 02:58:07,000
Such a young field.

2909
02:58:07,000 --> 02:58:09,000
I know it's so impressive.

2910
02:58:09,000 --> 02:58:12,000
Yeah, I've just checked his Twitter.

2911
02:58:12,000 --> 02:58:14,000
Hey, Kevin.

2912
02:58:14,000 --> 02:58:17,000
And, yeah, so...

2913
02:58:17,000 --> 02:58:19,000
To me, the underline...

2914
02:58:19,000 --> 02:58:21,000
Yeah, so I was zooming out a bit.

2915
02:58:21,000 --> 02:58:24,000
I think there's a family of techniques

2916
02:58:24,000 --> 02:58:26,000
around causal interventions

2917
02:58:26,000 --> 02:58:28,000
and their use in mech and top

2918
02:58:28,000 --> 02:58:30,000
that's useful to understand here.

2919
02:58:30,000 --> 02:58:32,000
So...

2920
02:58:32,000 --> 02:58:36,000
The core technique is this idea of activation patching.

2921
02:58:36,000 --> 02:58:38,000
Where...

2922
02:58:38,000 --> 02:58:40,000
So let's...

2923
02:58:40,000 --> 02:58:42,000
So one of the problems with understanding

2924
02:58:42,000 --> 02:58:44,000
a model's features and circuits

2925
02:58:44,000 --> 02:58:48,000
is models are full of many, many different circuits.

2926
02:58:48,000 --> 02:58:51,000
Each circuit does not activate on many inputs.

2927
02:58:51,000 --> 02:58:54,000
But each circuit will activate...

2928
02:58:54,000 --> 02:58:57,000
But on each input, many circuits will activate.

2929
02:58:57,000 --> 02:58:59,000
And in order to do good mech and top work,

2930
02:58:59,000 --> 02:59:03,000
you need to be incredibly surgical and precise,

2931
02:59:03,000 --> 02:59:06,000
which means you need to learn how to isolate a specific circuit.

2932
02:59:06,000 --> 02:59:10,000
And let's consider a statement like...

2933
02:59:14,000 --> 02:59:16,000
The Eiffel Tower is in Paris

2934
02:59:16,000 --> 02:59:18,000
versus the Colosseum is in Rome.

2935
02:59:18,000 --> 02:59:20,000
These are both...

2936
02:59:20,000 --> 02:59:22,000
There's lots of features happening.

2937
02:59:22,000 --> 02:59:24,000
There's lots of circuits being activated

2938
02:59:24,000 --> 02:59:26,000
on the Eiffel Tower is in Paris.

2939
02:59:26,000 --> 02:59:28,000
This is in English.

2940
02:59:28,000 --> 02:59:30,000
You're doing factual recall.

2941
02:59:30,000 --> 02:59:32,000
You are outputting a location.

2942
02:59:32,000 --> 02:59:34,000
You are outputting a proper noun.

2943
02:59:34,000 --> 02:59:36,000
This is a European landmark.

2944
02:59:36,000 --> 02:59:38,000
Et cetera, et cetera.

2945
02:59:38,000 --> 02:59:41,000
And like, I want to know how the model knows

2946
02:59:41,000 --> 02:59:43,000
the Eiffel Tower is in Paris.

2947
02:59:43,000 --> 02:59:45,000
But the Colosseum is in Rome.

2948
02:59:45,000 --> 02:59:49,000
Controls almost everything apart from the fact.

2949
02:59:49,000 --> 02:59:51,000
And so...

2950
02:59:51,000 --> 02:59:54,000
What I can try to do is

2951
02:59:54,000 --> 02:59:57,000
causally intervene on the Colosseum run.

2952
02:59:57,000 --> 03:00:01,000
And replace, say, the output of an attention head

2953
03:00:01,000 --> 03:00:05,000
with its outputs on the Eiffel Tower prompt

2954
03:00:05,000 --> 03:00:09,000
and see how much this changes the answer from Rome to Paris.

2955
03:00:09,000 --> 03:00:13,000
And this...

2956
03:00:13,000 --> 03:00:19,000
Yeah, this patch can let me really isolate

2957
03:00:19,000 --> 03:00:23,000
how the circuitry for just this specific thing works.

2958
03:00:23,000 --> 03:00:27,000
And there's all kinds of work around this.

2959
03:00:27,000 --> 03:00:30,000
Obnoxiously, all of it uses different notation,

2960
03:00:30,000 --> 03:00:35,000
like resample ablations and causal tracing

2961
03:00:35,000 --> 03:00:38,000
and causal mediation analysis and entertained interventions.

2962
03:00:38,000 --> 03:00:42,000
All similar words are basically the same thing.

2963
03:00:42,000 --> 03:00:45,000
But, yeah.

2964
03:00:45,000 --> 03:00:50,000
The really key insight here is this kind of surgical intervention.

2965
03:00:50,000 --> 03:00:53,000
A classic technique in interpretability is ablations,

2966
03:00:53,000 --> 03:00:55,000
where you just set something to zero.

2967
03:00:55,000 --> 03:00:59,000
And it's kind of janky because if you break something in the model,

2968
03:00:59,000 --> 03:01:02,000
which wasn't interestingly used for the task,

2969
03:01:02,000 --> 03:01:04,000
then everything dies.

2970
03:01:04,000 --> 03:01:07,000
Or if you break it in interesting ways, everything dies.

2971
03:01:07,000 --> 03:01:10,000
For example, in GPT2 Small,

2972
03:01:10,000 --> 03:01:15,000
almost every single task breaks if you delete the 0th MLP layer.

2973
03:01:15,000 --> 03:01:18,000
Yeah, as far as I can tell,

2974
03:01:18,000 --> 03:01:22,000
the 0th MLP layer is kind of an extended embedding.

2975
03:01:22,000 --> 03:01:26,000
GPT2 Small has tied embeddings and unembeddings,

2976
03:01:26,000 --> 03:01:28,000
so they're transposed of each other,

2977
03:01:28,000 --> 03:01:31,000
which is wildly unprincipled in my opinion.

2978
03:01:31,000 --> 03:01:35,000
And the model seems to be both using this for just detokenization

2979
03:01:35,000 --> 03:01:38,000
and combining nearby things with the first attention layer,

2980
03:01:38,000 --> 03:01:43,000
the 0th attention layer, and just undoing the tightness.

2981
03:01:43,000 --> 03:01:47,000
But this means that basically everything is reading from that.

2982
03:01:47,000 --> 03:01:50,000
And I've seen people do zero ablations and everything

2983
03:01:50,000 --> 03:01:52,000
and be like, oh, this is an important part of the circuit.

2984
03:01:52,000 --> 03:01:55,000
Let's get really sidetracked by this.

2985
03:01:55,000 --> 03:01:58,000
Because the effect size is so big.

2986
03:01:58,000 --> 03:02:00,000
Man, being a mech interpret research feels my mind

2987
03:02:00,000 --> 03:02:02,000
with such bizarre trivia like this.

2988
03:02:02,000 --> 03:02:04,000
It's great.

2989
03:02:04,000 --> 03:02:07,000
Models, so bizarre.

2990
03:02:07,000 --> 03:02:11,000
And so, yeah.

2991
03:02:11,000 --> 03:02:13,000
This calls on intervention.

2992
03:02:13,000 --> 03:02:16,000
There's kind of two conceptually different kinds of interventions.

2993
03:02:16,000 --> 03:02:19,000
You can take the Eiffel Tower prompt,

2994
03:02:19,000 --> 03:02:22,000
patch in something from the Colosseum

2995
03:02:22,000 --> 03:02:25,000
and see if it breaks the ability to output Paris,

2996
03:02:25,000 --> 03:02:29,000
to verify which bits kind of are necessary,

2997
03:02:29,000 --> 03:02:32,000
such that getting rid of them will break something.

2998
03:02:32,000 --> 03:02:36,000
Or you can patch something from the Paris run into the Colosseum run

2999
03:02:36,000 --> 03:02:38,000
and see if that makes it output Paris,

3000
03:02:38,000 --> 03:02:41,000
which is testing for stuff that's sufficient.

3001
03:02:41,000 --> 03:02:44,000
I call the first one a resample ablation

3002
03:02:44,000 --> 03:02:47,000
because you're messing up a component by resampling

3003
03:02:47,000 --> 03:02:51,000
and the second one denoising or causal tracing

3004
03:02:51,000 --> 03:02:56,000
because you're intervening with a bit of information

3005
03:02:56,000 --> 03:02:59,000
and seeing if that is sufficient for everything else.

3006
03:02:59,000 --> 03:03:01,000
Though none of these names are good.

3007
03:03:01,000 --> 03:03:03,000
I would love someone to come up with better names.

3008
03:03:03,000 --> 03:03:07,000
And there's all kinds of families of work building on this.

3009
03:03:07,000 --> 03:03:12,000
I have this post called attribution patching

3010
03:03:12,000 --> 03:03:14,000
that tries to apply this as an industrial scale

3011
03:03:14,000 --> 03:03:17,000
by using gradients to approximate it,

3012
03:03:17,000 --> 03:03:20,000
which is fast enough that you could take GPD3

3013
03:03:20,000 --> 03:03:22,000
and its four million neurons

3014
03:03:22,000 --> 03:03:25,000
and do attribution patching on all neurons at once

3015
03:03:25,000 --> 03:03:27,000
on every position.

3016
03:03:27,000 --> 03:03:29,000
Great post.

3017
03:03:29,000 --> 03:03:34,000
Redwood Research has this technique called causal scrubbing,

3018
03:03:34,000 --> 03:03:38,000
which I view as activation patching

3019
03:03:38,000 --> 03:03:41,000
gone incredibly hard and rigorous

3020
03:03:41,000 --> 03:03:44,000
that tries to come up with an automated metric

3021
03:03:44,000 --> 03:03:48,000
for saying this hypothesis about a model

3022
03:03:48,000 --> 03:03:51,000
is actually accurate for how it works,

3023
03:03:51,000 --> 03:03:53,000
where it's kind of complicated.

3024
03:03:53,000 --> 03:03:56,000
But the core idea is you think of a hypothesis

3025
03:03:56,000 --> 03:03:59,000
as saying which resample ablations are allowed

3026
03:03:59,000 --> 03:04:02,000
and you make all of the resample ablations

3027
03:04:02,000 --> 03:04:04,000
that should be allowed.

3028
03:04:04,000 --> 03:04:08,000
Like these components of the model shouldn't really matter

3029
03:04:08,000 --> 03:04:12,000
so we can just patch in stuff from random other inputs.

3030
03:04:12,000 --> 03:04:14,000
If you've got, say, an induction head,

3031
03:04:14,000 --> 03:04:17,000
you might think the induction head cares about

3032
03:04:17,000 --> 03:04:24,000
the current token and the thing before the past token

3033
03:04:24,000 --> 03:04:27,000
that it's going to inductionally attend to.

3034
03:04:27,000 --> 03:04:32,000
So let's replace the token that it's going to be attending to

3035
03:04:32,000 --> 03:04:35,000
with a token from a different input

3036
03:04:35,000 --> 03:04:37,000
but with the same token before it.

3037
03:04:37,000 --> 03:04:39,000
My hypothesis about the induction head

3038
03:04:39,000 --> 03:04:42,000
says this should be allowed, so let's do that.

3039
03:04:42,000 --> 03:04:45,000
I wouldn't want to introduce a rant

3040
03:04:45,000 --> 03:04:48,000
but the metric he uses is really important.

3041
03:04:48,000 --> 03:04:53,000
Yes, this is one of my hobby horses.

3042
03:04:53,000 --> 03:04:59,000
So some of the original work looking at the patching stuff

3043
03:04:59,000 --> 03:05:04,000
like David Bow and Kevin Meng's excellent Rome paper

3044
03:05:04,000 --> 03:05:09,000
uses the probability of Paris as their metric

3045
03:05:09,000 --> 03:05:12,000
and there are other papers that use things like accuracy

3046
03:05:12,000 --> 03:05:16,000
as their metric and generally I think of metrics

3047
03:05:16,000 --> 03:05:20,000
as being on a spectrum from like soft to sharp.

3048
03:05:20,000 --> 03:05:27,000
So generally I think of models as thinking in log space.

3049
03:05:27,000 --> 03:05:30,000
They are kind of acting like basions.

3050
03:05:30,000 --> 03:05:34,000
They are trying to figure out some things in Paris

3051
03:05:34,000 --> 03:05:36,000
and there will be five separate heads

3052
03:05:36,000 --> 03:05:39,000
that each contribute one to the correct logit

3053
03:05:39,000 --> 03:05:42,000
and each of these can be thought of as one bit of information

3054
03:05:42,000 --> 03:05:47,000
and together they get you the right probability of, say, 0.8.

3055
03:05:47,000 --> 03:05:51,000
But if you patch in each one in isolation

3056
03:05:51,000 --> 03:05:54,000
the probability changes negatively

3057
03:05:54,000 --> 03:05:58,000
because probability is exponential in the logits.

3058
03:05:58,000 --> 03:06:00,000
So using probability you're like,

3059
03:06:00,000 --> 03:06:03,000
oh, this head patch doesn't really matter.

3060
03:06:03,000 --> 03:06:06,000
So in this paper they did this thing of patching in

3061
03:06:06,000 --> 03:06:09,000
like 10 adjacent layers at once.

3062
03:06:09,000 --> 03:06:12,000
And to me a really core principle of this kind of causal

3063
03:06:12,000 --> 03:06:14,000
intervention and mechanistic technique

3064
03:06:14,000 --> 03:06:16,000
is you want to be as surgical as possible

3065
03:06:16,000 --> 03:06:18,000
to be as deeply faithful as possible

3066
03:06:18,000 --> 03:06:20,000
to what the neural model is actually doing.

3067
03:06:20,000 --> 03:06:23,000
So in this case there was an interaction between them.

3068
03:06:23,000 --> 03:06:26,000
They were effectively making several interactions

3069
03:06:26,000 --> 03:06:28,000
at once.

3070
03:06:28,000 --> 03:06:33,000
Yes, they were replacing 10 adjacent layers

3071
03:06:33,000 --> 03:06:35,000
and patching things in different layers

3072
03:06:35,000 --> 03:06:37,000
is always a bit weird.

3073
03:06:37,000 --> 03:06:39,000
I don't think that part's that objectionable.

3074
03:06:39,000 --> 03:06:44,000
I mostly just feel like if you choose a metric like log prop

3075
03:06:44,000 --> 03:06:49,000
it allows you to be much more surgical about how you intervene.

3076
03:06:49,000 --> 03:06:54,000
It allows you to identify subtle effects of things.

3077
03:06:54,000 --> 03:06:57,000
Accuracy is even worse because accuracy

3078
03:06:57,000 --> 03:07:00,000
is basically rounding things to zero or one.

3079
03:07:00,000 --> 03:07:02,000
So like if the threshold is 2.5

3080
03:07:02,000 --> 03:07:05,000
any individual patch does nothing.

3081
03:07:05,000 --> 03:07:08,000
Any re-sample ablation does nothing.

3082
03:07:08,000 --> 03:07:11,000
But if you patch in like the 10 adjacent layers

3083
03:07:11,000 --> 03:07:13,000
it will do everything.

3084
03:07:13,000 --> 03:07:16,000
And this can be kind of misleading.

3085
03:07:16,000 --> 03:07:21,000
Another one I often see people do is

3086
03:07:22,000 --> 03:07:28,000
they look at things like the rank of an output.

3087
03:07:28,000 --> 03:07:30,000
At which point does the model realize Paris

3088
03:07:30,000 --> 03:07:32,000
is the most likely next token?

3089
03:07:32,000 --> 03:07:34,000
And this can be super misleading

3090
03:07:34,000 --> 03:07:36,000
because this will make you think

3091
03:07:36,000 --> 03:07:39,000
the third head is the only head that matters.

3092
03:07:39,000 --> 03:07:41,000
When really all five of them matter

3093
03:07:41,000 --> 03:07:43,000
the order is kind of arbitrary.

3094
03:07:43,000 --> 03:07:47,000
And yeah, I've seen papers that I think

3095
03:07:47,000 --> 03:07:50,000
got somewhat misled by using metrics like this.

3096
03:07:51,000 --> 03:07:54,000
And metrics, they matter so much.

3097
03:07:54,000 --> 03:07:56,000
It's so easy to trick yourself.

3098
03:07:56,000 --> 03:07:58,000
My high level pitch is just,

3099
03:07:58,000 --> 03:08:01,000
mech and tub is great, mech and tub is beautiful.

3100
03:08:01,000 --> 03:08:03,000
Also the field is incredibly young.

3101
03:08:03,000 --> 03:08:05,000
There's maybe 30 full time people

3102
03:08:05,000 --> 03:08:07,000
working on it in the world.

3103
03:08:07,000 --> 03:08:09,000
There's a ton of low hanging fruits.

3104
03:08:09,000 --> 03:08:11,000
I've done major research in this field

3105
03:08:11,000 --> 03:08:14,000
I've been at for like less than two years.

3106
03:08:16,000 --> 03:08:19,000
I would love people to come and help

3107
03:08:19,000 --> 03:08:21,000
us solve problems and do research here.

3108
03:08:21,000 --> 03:08:24,000
And we'll link to my post on getting started

3109
03:08:24,000 --> 03:08:28,000
and my sequence called 200 Cronkidopin problems

3110
03:08:28,000 --> 03:08:31,000
in the description to this, hopefully.

3111
03:08:31,000 --> 03:08:34,000
And yeah, I think there's just,

3112
03:08:34,000 --> 03:08:36,000
it's not that hard to get started.

3113
03:08:36,000 --> 03:08:37,000
It's really fun.

3114
03:08:37,000 --> 03:08:39,000
Hopefully I've nerd sniped you

3115
03:08:39,000 --> 03:08:43,000
with at least one thing in this podcast.

3116
03:08:43,000 --> 03:08:45,000
And if you're at least vaguely curious,

3117
03:08:45,000 --> 03:08:47,000
it's just really easy to open one of the tutorials

3118
03:08:47,000 --> 03:08:51,000
linked in my posts and just start screwing around.

3119
03:08:51,000 --> 03:08:54,000
And I'd love to see what you can find.

3120
03:08:54,000 --> 03:08:55,000
Beautiful.

3121
03:08:55,000 --> 03:08:58,000
Also the deep mind element team is currently hiring

3122
03:08:58,000 --> 03:09:00,000
and people should apply,

3123
03:09:00,000 --> 03:09:03,000
which includes hiring from a mechanistic interoperability team.

3124
03:09:03,000 --> 03:09:04,000
Amazing.

3125
03:09:04,000 --> 03:09:06,000
Do they have to do lead code?

3126
03:09:06,000 --> 03:09:08,000
I have no idea.

3127
03:09:08,000 --> 03:09:09,000
Can't remember.

3128
03:09:09,000 --> 03:09:12,000
Yeah, yeah, we did an amazing video

3129
03:09:12,000 --> 03:09:15,000
with Petr Velichkovich.

3130
03:09:15,000 --> 03:09:18,000
I gave him one of my lead code challenges

3131
03:09:18,000 --> 03:09:20,000
and annoyingly he aced it.

3132
03:09:20,000 --> 03:09:24,000
It's all that deep mind interview practice.

3133
03:09:24,000 --> 03:09:28,000
Anyway, okay, let's talk about super intelligence.

3134
03:09:28,000 --> 03:09:31,000
Now, I spoke with our mutual friend,

3135
03:09:31,000 --> 03:09:33,000
Robert Miles about a month ago.

3136
03:09:33,000 --> 03:09:34,000
Rob's so great.

3137
03:09:34,000 --> 03:09:35,000
He's a lovely chap.

3138
03:09:35,000 --> 03:09:36,000
Spoke all about alignment.

3139
03:09:36,000 --> 03:09:39,000
And he accused me of over-philosophizing everything

3140
03:09:39,000 --> 03:09:41,000
because I was talking all about intelligence,

3141
03:09:41,000 --> 03:09:43,000
one of my favorite topics.

3142
03:09:43,000 --> 03:09:47,000
And he said, well, what about fire?

3143
03:09:47,000 --> 03:09:50,000
Fire is something that people didn't understand millennia ago,

3144
03:09:50,000 --> 03:09:54,000
but they knew that it burnt and they knew that it was bad.

3145
03:09:54,000 --> 03:09:57,000
And this is like, this is like a fire,

3146
03:09:57,000 --> 03:09:58,000
which is very interesting.

3147
03:09:58,000 --> 03:10:01,000
And maybe we can bring in a little bit of effective altruism as well.

3148
03:10:01,000 --> 03:10:02,000
So, you know, I...

3149
03:10:02,000 --> 03:10:03,000
If I can just interject.

3150
03:10:03,000 --> 03:10:04,000
Please do, please.

3151
03:10:04,000 --> 03:10:06,000
If there is one thing I have learned

3152
03:10:06,000 --> 03:10:08,000
from the past decade of machine learning programs,

3153
03:10:08,000 --> 03:10:12,000
is that you do not need to understand a thing in order to make it.

3154
03:10:12,000 --> 03:10:16,000
And this extends to things that are smarter than us

3155
03:10:16,000 --> 03:10:20,000
and which are capable of leading to catastrophic risks.

3156
03:10:20,000 --> 03:10:22,000
Yes, yes.

3157
03:10:22,000 --> 03:10:24,000
Well, let's...

3158
03:10:24,000 --> 03:10:27,000
I'll step back a tiny bit and then we'll get there

3159
03:10:27,000 --> 03:10:29,000
because there's the hypothetical nature,

3160
03:10:29,000 --> 03:10:31,000
which I guess I have a bit of a problem with.

3161
03:10:31,000 --> 03:10:33,000
Now, about 10 years ago,

3162
03:10:33,000 --> 03:10:36,000
I was one of the first supporters of Sam Harris' podcast

3163
03:10:36,000 --> 03:10:39,000
and he's quite aligned to EA.

3164
03:10:39,000 --> 03:10:43,000
And he was talking about this very noble idea

3165
03:10:43,000 --> 03:10:45,000
that everyone matters equally

3166
03:10:45,000 --> 03:10:48,000
and people on the left should get on board with that intrinsically.

3167
03:10:48,000 --> 03:10:51,000
And this idea that we should quantitatively analyse

3168
03:10:51,000 --> 03:10:53,000
the impact of charity work

3169
03:10:53,000 --> 03:10:55,000
and solve an optimisation problem

3170
03:10:55,000 --> 03:10:56,000
and earning to give

3171
03:10:56,000 --> 03:10:59,000
and a lot of the stuff that MacAskill spoke about

3172
03:10:59,000 --> 03:11:02,000
and also philosophers like Peter Singer.

3173
03:11:02,000 --> 03:11:06,000
And the focus seemed to be primarily on alleviating poverty,

3174
03:11:07,000 --> 03:11:10,000
and we don't say the biggest problem,

3175
03:11:10,000 --> 03:11:12,000
we say a problem.

3176
03:11:12,000 --> 03:11:14,000
This is another thing our friend Robert Viles said.

3177
03:11:14,000 --> 03:11:15,000
He said,

3178
03:11:15,000 --> 03:11:19,000
the problem is when people talk about the problem,

3179
03:11:19,000 --> 03:11:22,000
there can be more than one problem.

3180
03:11:22,000 --> 03:11:26,000
But anyway, so it's a big problem.

3181
03:11:26,000 --> 03:11:30,000
And recently, you and I can agree

3182
03:11:30,000 --> 03:11:34,000
that EA circles have really laser-focused in

3183
03:11:34,000 --> 03:11:38,000
on existential risk from AI

3184
03:11:38,000 --> 03:11:41,000
as opposed to other more plausible ex-risk concerns

3185
03:11:41,000 --> 03:11:43,000
like pandemics or even nuclear war.

3186
03:11:43,000 --> 03:11:46,000
I'm not to say that they don't focus on that, but...

3187
03:11:46,000 --> 03:11:49,000
I am going to push back on other more plausible ex-risk.

3188
03:11:49,000 --> 03:11:50,000
Go on.

3189
03:11:50,000 --> 03:11:51,000
Go on.

3190
03:11:51,000 --> 03:11:52,000
I just wanted to register an objection.

3191
03:11:52,000 --> 03:11:53,000
Feel free to go.

3192
03:11:53,000 --> 03:11:54,000
Register an objection.

3193
03:11:55,000 --> 03:11:59,000
And cynically, from my point of view,

3194
03:11:59,000 --> 03:12:03,000
I see the influence of Eliezer, Bostrom,

3195
03:12:03,000 --> 03:12:08,000
Hansen, et cetera, kind of shifting the focus on to ex-risk.

3196
03:12:08,000 --> 03:12:13,000
And part of the reason for that is also this kind of overly

3197
03:12:13,000 --> 03:12:16,000
intellectual focus on long-termism.

3198
03:12:16,000 --> 03:12:20,000
And it's done in a very intellectualized way.

3199
03:12:20,000 --> 03:12:25,000
So it's based on the utility function now incorporating

3200
03:12:25,000 --> 03:12:29,000
future simulated humans on different planets,

3201
03:12:29,000 --> 03:12:31,000
a long time away in the future,

3202
03:12:31,000 --> 03:12:34,000
and making all of these intellectual jumps.

3203
03:12:34,000 --> 03:12:36,000
So let's start there.

3204
03:12:36,000 --> 03:12:37,000
What's your take?

3205
03:12:37,000 --> 03:12:38,000
All right.

3206
03:12:38,000 --> 03:12:40,000
So much stuff to respond to in there.

3207
03:12:40,000 --> 03:12:41,000
Good.

3208
03:12:41,000 --> 03:12:43,000
So, all right, a couple of things.

3209
03:12:43,000 --> 03:12:46,000
The first, so, cars on the table.

3210
03:12:46,000 --> 03:12:49,000
I care a lot about AI existential risk.

3211
03:12:49,000 --> 03:12:50,000
Yes.

3212
03:12:50,000 --> 03:12:52,000
The reason I work on mechanistic interpretability

3213
03:12:52,000 --> 03:12:56,000
is because I think that understanding the mysterious black

3214
03:12:56,000 --> 03:12:58,000
boxes that are potentially smarter than us

3215
03:12:59,000 --> 03:13:02,000
and may want things wildly different than what we wanted them

3216
03:13:02,000 --> 03:13:06,000
to want is just clearly better than not understanding them.

3217
03:13:06,000 --> 03:13:07,000
Yes.

3218
03:13:07,000 --> 03:13:10,000
And I think mechanistic interpretability

3219
03:13:10,000 --> 03:13:12,000
is a promising path here.

3220
03:13:12,000 --> 03:13:15,000
And I also would consider myself an effective altruist

3221
03:13:15,000 --> 03:13:16,000
and a rationalist.

3222
03:13:16,000 --> 03:13:20,000
So cars on the table, there's my biases.

3223
03:13:20,000 --> 03:13:25,000
So I generally think it's more productive to discuss

3224
03:13:25,000 --> 03:13:29,000
is AI catastrophic and existential risk a big deal?

3225
03:13:29,000 --> 03:13:34,000
Then is it the biggest deal or is it worth more resources

3226
03:13:34,000 --> 03:13:38,000
on the margin than global poverty or climate change

3227
03:13:38,000 --> 03:13:39,000
or AI ethics?

3228
03:13:39,000 --> 03:13:41,000
And like, there's just lots of problems.

3229
03:13:41,000 --> 03:13:44,000
I care way more about convincing people that AI extras

3230
03:13:44,000 --> 03:13:48,000
could be in your top 10 than it should be in your top one

3231
03:13:48,000 --> 03:13:49,000
because I feel like for most people,

3232
03:13:49,000 --> 03:13:52,000
it's not in their top thousand.

3233
03:13:52,000 --> 03:13:56,000
And there's just so much divisiveness between, say,

3234
03:13:56,000 --> 03:13:59,000
the AI ethics community and the AI alignment community

3235
03:13:59,000 --> 03:14:01,000
about whose problem is a bigger deal.

3236
03:14:01,000 --> 03:14:03,000
And like, both are big problems.

3237
03:14:03,000 --> 03:14:05,000
Why are we arguing?

3238
03:14:05,000 --> 03:14:09,000
And part of this is about our moral intuitions.

3239
03:14:09,000 --> 03:14:12,000
And this is something I spoke a lot with Conor about.

3240
03:14:12,000 --> 03:14:16,000
He said that in many ways he's got this technical empathy.

3241
03:14:16,000 --> 03:14:21,000
So sensory empathy is, I really care about my family.

3242
03:14:21,000 --> 03:14:24,000
I care about these concentric circles of moral status.

3243
03:14:24,000 --> 03:14:26,000
I really care about my family.

3244
03:14:26,000 --> 03:14:29,000
And if I try really hard, I can care about people

3245
03:14:29,000 --> 03:14:30,000
in other countries and so on.

3246
03:14:30,000 --> 03:14:32,000
And then if I try really, really hard,

3247
03:14:32,000 --> 03:14:35,000
I can care about future simulated lives on Mars.

3248
03:14:35,000 --> 03:14:39,000
And Conor said, the idea of this movement is about

3249
03:14:39,000 --> 03:14:42,000
galaxy-braining yourself into being the most empathetic person

3250
03:14:42,000 --> 03:14:44,000
imaginable, but it's a kind of empathy

3251
03:14:44,000 --> 03:14:47,000
that people don't understand.

3252
03:14:48,000 --> 03:14:52,000
So a separate bit of beef I have is with the entire notion

3253
03:14:52,000 --> 03:14:54,000
of long-termism.

3254
03:14:54,000 --> 03:14:57,000
So long-termism is this idea...

3255
03:14:57,000 --> 03:15:01,000
So long-termism is generally caring about the long-term future.

3256
03:15:01,000 --> 03:15:05,000
There's the strong form of value in the future

3257
03:15:05,000 --> 03:15:08,000
basically entirely dominates things today.

3258
03:15:08,000 --> 03:15:12,000
And weaker forms of just this really, really matters.

3259
03:15:12,000 --> 03:15:17,000
And a common misconception about AIX risk and AI safety

3260
03:15:17,000 --> 03:15:23,000
is that you should only work on this if you are a long-termist.

3261
03:15:23,000 --> 03:15:27,000
That, you know, it's a one in a billion chance of mattering,

3262
03:15:27,000 --> 03:15:31,000
but there's a quintillion future lives,

3263
03:15:31,000 --> 03:15:35,000
so this outweighs everyone alive today in moral worth.

3264
03:15:35,000 --> 03:15:39,000
Or, well, we're only gonna get AGI in like 500 years,

3265
03:15:39,000 --> 03:15:42,000
but we're gonna work on it now just in case.

3266
03:15:42,000 --> 03:15:45,000
And like, I think both of these are just nonsense.

3267
03:15:45,000 --> 03:15:49,000
Like, I guess as a concrete example,

3268
03:15:49,000 --> 03:15:53,000
Effective Artists have worked on pandemic prevention for many years.

3269
03:15:53,000 --> 03:15:58,000
And I think it was just clearly the case that pandemics are

3270
03:15:58,000 --> 03:16:01,000
a major threat to people alive today.

3271
03:16:01,000 --> 03:16:04,000
And I like to feel that we've been proven right.

3272
03:16:04,000 --> 03:16:06,000
No one's gonna argue at that point.

3273
03:16:06,000 --> 03:16:09,000
And, you know, everyone's being like, Effective Artists,

3274
03:16:09,000 --> 03:16:11,000
why are you working on AI safety?

3275
03:16:11,000 --> 03:16:13,000
This obviously doesn't matter.

3276
03:16:13,000 --> 03:16:16,000
You know, I feel like we've got one thing right.

3277
03:16:16,000 --> 03:16:19,000
Can I be really skeptical, though, for a second?

3278
03:16:19,000 --> 03:16:22,000
Because, I mean, you're working for DeepMind.

3279
03:16:22,000 --> 03:16:26,000
There's so much prestige and money attached to AI risk.

3280
03:16:26,000 --> 03:16:29,000
Elon Musk is talking about it all the time,

3281
03:16:29,000 --> 03:16:34,000
whereas you could be a scientist working on pandemic responses.

3282
03:16:34,000 --> 03:16:36,000
And, I mean, let's be honest,

3283
03:16:36,000 --> 03:16:39,000
it wouldn't be anywhere near the same level of prestige.

3284
03:16:39,000 --> 03:16:40,000
Yeah.

3285
03:16:40,000 --> 03:16:46,000
So, couple of takes.

3286
03:16:46,000 --> 03:16:51,000
It definitely is the case that I,

3287
03:16:51,000 --> 03:16:54,000
a good chunk of why I personally am working on AI X-Risk

3288
03:16:54,000 --> 03:16:59,000
rather than say BioX-Risk is that I'm a smart mathematician.

3289
03:16:59,000 --> 03:17:00,000
I like AI.

3290
03:17:00,000 --> 03:17:02,000
I like mech and tub.

3291
03:17:02,000 --> 03:17:05,000
I do not think I would be good at biology in the same way.

3292
03:17:05,000 --> 03:17:13,000
And I also personally assert that AI X-Risk is more important

3293
03:17:13,000 --> 03:17:18,000
and, like, more pressing, but, you know, I'm biased.

3294
03:17:18,000 --> 03:17:21,000
And I think it's fair to flag that bias.

3295
03:17:21,000 --> 03:17:24,000
In terms of prestige,

3296
03:17:24,000 --> 03:17:27,000
so I've only really been working on this stuff properly

3297
03:17:27,000 --> 03:17:29,000
for the past two and a half years,

3298
03:17:29,000 --> 03:17:32,000
which is, I mean, it's changed dramatically.

3299
03:17:32,000 --> 03:17:34,000
Like, in the last six months we've gone from,

3300
03:17:34,000 --> 03:17:37,000
well, we're really ever going to get AGI to,

3301
03:17:37,000 --> 03:17:39,000
oh my God, GP4 exists.

3302
03:17:39,000 --> 03:17:42,000
Jeffrey Hinton has left Google to loudly advocate for X-Risk.

3303
03:17:42,000 --> 03:17:46,000
Joshua Benjo is now loudly advocating for X-Risk.

3304
03:17:46,000 --> 03:17:49,000
It's two-thirds of the Turing winners for deep learning.

3305
03:17:49,000 --> 03:17:51,000
You'll never get the third one.

3306
03:17:51,000 --> 03:17:53,000
Yeah, we're never going to get the third one.

3307
03:17:53,000 --> 03:17:57,000
Jan Likert has made his position very, very clear.

3308
03:17:57,000 --> 03:18:00,000
But, you know, as a majority, I'll take it.

3309
03:18:00,000 --> 03:18:01,000
Yes.

3310
03:18:01,000 --> 03:18:03,000
Or the fourth one.

3311
03:18:03,000 --> 03:18:04,000
Yeah.

3312
03:18:04,000 --> 03:18:06,000
He's coming on our podcast, actually.

3313
03:18:06,000 --> 03:18:08,000
Oh, who was the fourth one?

3314
03:18:08,000 --> 03:18:09,000
Schmidt-Huber.

3315
03:18:09,000 --> 03:18:11,000
Yeah, it seems hard.

3316
03:18:11,000 --> 03:18:13,000
I'm very curious to hear the Schmidt-Huber episode.

3317
03:18:13,000 --> 03:18:16,000
Oh, yeah, he's even more virulently against than Jan.

3318
03:18:16,000 --> 03:18:18,000
I'm afraid to say.

3319
03:18:18,000 --> 03:18:19,000
Two out of two.

3320
03:18:19,000 --> 03:18:20,000
Two out of four.

3321
03:18:20,000 --> 03:18:22,000
I'm interested to hear it anyway.

3322
03:18:22,000 --> 03:18:26,000
So, yeah.

3323
03:18:26,000 --> 03:18:30,000
And, yeah, in terms of prestige, I don't know.

3324
03:18:30,000 --> 03:18:36,000
I gather that, say, seven years ago, it was basically just not...

3325
03:18:36,000 --> 03:18:38,000
It would be, like, pretty bad for your career.

3326
03:18:38,000 --> 03:18:42,000
You would not be taken seriously if you mentioned caring about AIX-Risk.

3327
03:18:42,000 --> 03:18:44,000
Your papers would be rejected.

3328
03:18:44,000 --> 03:18:49,000
I hear a story of Stuart Russell at one point talk to a grad student of his

3329
03:18:49,000 --> 03:18:52,000
about how Stuart was concerned about AIX-Risk.

3330
03:18:52,000 --> 03:18:55,000
The grad student was also really concerned and freaking out,

3331
03:18:55,000 --> 03:18:57,000
but they'd been working together for years

3332
03:18:57,000 --> 03:18:59,000
and neither felt comfortable mentioning it.

3333
03:18:59,000 --> 03:19:04,000
And a lot of people who are still in the field were doing the stuff then,

3334
03:19:04,000 --> 03:19:07,000
which makes me somewhat reject the prestige argument,

3335
03:19:07,000 --> 03:19:11,000
at least for senior people in the field.

3336
03:19:11,000 --> 03:19:14,000
I think there's a difference with Stuart Russell in particular.

3337
03:19:14,000 --> 03:19:16,000
He's very credible.

3338
03:19:16,000 --> 03:19:17,000
And he...

3339
03:19:17,000 --> 03:19:18,000
No, I'm not.

3340
03:19:18,000 --> 03:19:20,000
Oh, I didn't mean... I didn't mean you.

3341
03:19:20,000 --> 03:19:23,000
I was talking about the two Godfathers,

3342
03:19:23,000 --> 03:19:25,000
because the thing that...

3343
03:19:25,000 --> 03:19:26,000
Maybe I shouldn't say this,

3344
03:19:26,000 --> 03:19:31,000
but I was surprised that Benjio and Hinton came out in the way they did.

3345
03:19:31,000 --> 03:19:33,000
And I...

3346
03:19:33,000 --> 03:19:35,000
The reason I didn't like what they said was,

3347
03:19:35,000 --> 03:19:39,000
I felt that they were implying that current AI technology

3348
03:19:39,000 --> 03:19:41,000
could pose an existential threat.

3349
03:19:41,000 --> 03:19:45,000
And what I'm getting from you and what I'm getting from Russell is...

3350
03:19:45,000 --> 03:19:47,000
I'm also from Robert Miles,

3351
03:19:47,000 --> 03:19:52,000
is that this is a very real potential threat in the future,

3352
03:19:52,000 --> 03:19:54,000
but it's not a current threat.

3353
03:19:54,000 --> 03:19:58,000
Yes, very real potential threat in the future,

3354
03:19:58,000 --> 03:20:03,000
though I hesitate to confidently assert, say,

3355
03:20:03,000 --> 03:20:05,000
this will not be a threat in the next five years or something.

3356
03:20:05,000 --> 03:20:07,000
It's like pretty hard to say.

3357
03:20:07,000 --> 03:20:08,000
Interesting.

3358
03:20:08,000 --> 03:20:11,000
I'm not confident.

3359
03:20:11,000 --> 03:20:14,000
I agree with your assessment of Benjio and Hinton,

3360
03:20:14,000 --> 03:20:16,000
though they've spoken a bunch publicly,

3361
03:20:16,000 --> 03:20:18,000
so I'll defer if you can point to specific writings.

3362
03:20:18,000 --> 03:20:23,000
But for example, Benjio signed the pause AI for six months,

3363
03:20:23,000 --> 03:20:25,000
more powerful than GPT-4 letter,

3364
03:20:25,000 --> 03:20:27,000
and I don't know.

3365
03:20:27,000 --> 03:20:29,000
I don't think the letter was asserting that...

3366
03:20:29,000 --> 03:20:32,000
The letter definitely wasn't asserting GPT-4 was an extra risk.

3367
03:20:32,000 --> 03:20:35,000
It wasn't confidently asserting GPT-5 would be,

3368
03:20:35,000 --> 03:20:36,000
but it's being like,

3369
03:20:36,000 --> 03:20:39,000
yeah, we need more time and slow down and caution.

3370
03:20:39,000 --> 03:20:41,000
Maybe I'm reading too much into that,

3371
03:20:41,000 --> 03:20:44,000
but it seemed to me that, I mean, Hinton said

3372
03:20:44,000 --> 03:20:49,000
that chat GPT now contains all of the world's knowledge,

3373
03:20:49,000 --> 03:20:51,000
and this chatbot knows everything,

3374
03:20:51,000 --> 03:20:54,000
and it could potentially do very harmful things,

3375
03:20:54,000 --> 03:20:58,000
and I interpreted it possibly incorrectly

3376
03:20:58,000 --> 03:21:01,000
that they were talking about reasonably current

3377
03:21:01,000 --> 03:21:04,000
or next-generation risks.

3378
03:21:04,000 --> 03:21:06,000
I mean, I can't talk for them.

3379
03:21:06,000 --> 03:21:10,000
I also, I don't know, there are lots of near-term risks.

3380
03:21:10,000 --> 03:21:11,000
There's long-term risks.

3381
03:21:11,000 --> 03:21:14,000
I consider it my job to think hard about the long-term risks

3382
03:21:14,000 --> 03:21:16,000
and try to guard against those,

3383
03:21:16,000 --> 03:21:18,000
and I think lots of other people's jobs

3384
03:21:18,000 --> 03:21:20,000
is to focus on the near-term risks,

3385
03:21:20,000 --> 03:21:23,000
and both are great forms of work.

3386
03:21:23,000 --> 03:21:24,000
I don't know.

3387
03:21:24,000 --> 03:21:25,000
One reason I like interpretability

3388
03:21:25,000 --> 03:21:28,000
is I think it is just broadly useful across all of them.

3389
03:21:28,000 --> 03:21:31,000
So what I consider to be my job might just not even matter.

3390
03:21:31,000 --> 03:21:34,000
But, yeah.

3391
03:21:34,000 --> 03:21:37,000
Yeah, no, I probably will not...

3392
03:21:37,000 --> 03:21:39,000
Do not want to get deeply into interpreting

3393
03:21:39,000 --> 03:21:41,000
what other people have said.

3394
03:21:41,000 --> 03:21:43,000
I...

3395
03:21:46,000 --> 03:21:49,000
Well, could I ping you a couple of quick questions?

3396
03:21:49,000 --> 03:21:51,000
So, first of all, you know,

3397
03:21:51,000 --> 03:21:53,000
there's this idea of negative utilitarianism.

3398
03:21:53,000 --> 03:21:55,000
I mean, do you think minimising suffering

3399
03:21:55,000 --> 03:21:58,000
is more important than maximising happiness?

3400
03:21:58,000 --> 03:22:00,000
No.

3401
03:22:00,000 --> 03:22:01,000
No?

3402
03:22:01,000 --> 03:22:03,000
Not sure I've got a more deep answer than that.

3403
03:22:03,000 --> 03:22:05,000
I mostly think a lot of this intrusive reasoning

3404
03:22:05,000 --> 03:22:07,000
is more often by intuition than anything else.

3405
03:22:07,000 --> 03:22:09,000
But it's a bit like this metrics thing we were talking about,

3406
03:22:09,000 --> 03:22:11,000
you know, which is that...

3407
03:22:11,000 --> 03:22:13,000
If you want to have...

3408
03:22:13,000 --> 03:22:15,000
Would you like to tolerate some spiky necks

3409
03:22:15,000 --> 03:22:17,000
for some average happiness?

3410
03:22:17,000 --> 03:22:19,000
Yeah.

3411
03:22:19,000 --> 03:22:21,000
So, I know.

3412
03:22:21,000 --> 03:22:24,000
I have, like, a general frustration

3413
03:22:24,000 --> 03:22:28,000
with these discussions getting too philosophical.

3414
03:22:28,000 --> 03:22:30,000
This is a big issue

3415
03:22:30,000 --> 03:22:32,000
when I hang out with effective altruists

3416
03:22:32,000 --> 03:22:35,000
who really love moral philosophy and population ethics.

3417
03:22:35,000 --> 03:22:36,000
Yes.

3418
03:22:36,000 --> 03:22:37,000
I don't know.

3419
03:22:37,000 --> 03:22:39,000
I have this EA forum post called

3420
03:22:39,000 --> 03:22:42,000
Simplify EA Pictures to Holy Shit X-Risk.

3421
03:22:42,000 --> 03:22:44,000
And it's like...

3422
03:22:44,000 --> 03:22:46,000
So, I don't know.

3423
03:22:46,000 --> 03:22:49,000
If you actually look at some of the concrete work

3424
03:22:49,000 --> 03:22:53,000
people try doing on things like timelines and risk,

3425
03:22:53,000 --> 03:22:57,000
there's this report from a Jay Akatra at Open Philanthropy

3426
03:22:57,000 --> 03:23:00,000
that gives 30-year median timelines

3427
03:23:00,000 --> 03:23:03,000
to AI that's transformative,

3428
03:23:03,000 --> 03:23:06,000
which he's since updated to 20 years.

3429
03:23:06,000 --> 03:23:08,000
There's a report by Joseph Karlsmith

3430
03:23:08,000 --> 03:23:12,000
that estimates about a 10-ish percent chance

3431
03:23:12,000 --> 03:23:14,000
of a major catastrophe from this.

3432
03:23:14,000 --> 03:23:15,000
Yeah.

3433
03:23:15,000 --> 03:23:17,000
And if you just take those numbers,

3434
03:23:17,000 --> 03:23:20,000
this is clearly enough to reach pretty high

3435
03:23:20,000 --> 03:23:23,000
on my list of concerns of people alive today.

3436
03:23:23,000 --> 03:23:25,000
Okay, okay.

3437
03:23:25,000 --> 03:23:28,000
And I think these are bold empirical claims.

3438
03:23:28,000 --> 03:23:32,000
And I think it's great to debate them in the empirical domain.

3439
03:23:32,000 --> 03:23:37,000
But to me, this doesn't feel like a moral question.

3440
03:23:37,000 --> 03:23:40,000
It just feels like from common sense assumptions,

3441
03:23:40,000 --> 03:23:43,000
if you believe these empirical claims,

3442
03:23:43,000 --> 03:23:45,000
this stuff is a really big deal.

3443
03:23:45,000 --> 03:23:47,000
Okay, okay.

3444
03:23:47,000 --> 03:23:49,000
Let's take another couple of steps.

3445
03:23:49,000 --> 03:23:52,000
So, first of all, we save this till later.

3446
03:23:52,000 --> 03:23:54,000
I think deception is very important.

3447
03:23:54,000 --> 03:23:56,000
And Daniel Dennett, when I spoke with him,

3448
03:23:56,000 --> 03:23:58,000
he uses this notion called the intentional stance,

3449
03:23:58,000 --> 03:24:03,000
which basically means that if you use a projection

3450
03:24:03,000 --> 03:24:07,000
of purposes, goals, agency, et cetera,

3451
03:24:07,000 --> 03:24:10,000
in order to understand the behavior of an agent,

3452
03:24:10,000 --> 03:24:13,000
possibly a simulated agent,

3453
03:24:13,000 --> 03:24:17,000
then for all intents and purposes, it has agency.

3454
03:24:17,000 --> 03:24:19,000
It can make decisions.

3455
03:24:19,000 --> 03:24:20,000
It has moral status,

3456
03:24:20,000 --> 03:24:22,000
it has lots of different things like that.

3457
03:24:22,000 --> 03:24:26,000
And he would say that without an intentional stance,

3458
03:24:26,000 --> 03:24:28,000
without agency,

3459
03:24:28,000 --> 03:24:31,000
it's impossible for a model to lie or deceive us.

3460
03:24:31,000 --> 03:24:34,000
Now, what do you think would be the bar

3461
03:24:34,000 --> 03:24:37,000
for something like a GPT model to deceive us and why?

3462
03:24:37,000 --> 03:24:39,000
Yeah.

3463
03:24:39,000 --> 03:24:43,000
So, before I give takes,

3464
03:24:43,000 --> 03:24:47,000
I will generally reinforce Rob's vibe of,

3465
03:24:47,000 --> 03:24:49,000
well, if you have no idea how fire works,

3466
03:24:49,000 --> 03:24:50,000
but you know that it burns you.

3467
03:24:50,000 --> 03:24:52,000
That's kind of the important thing.

3468
03:24:52,000 --> 03:24:57,000
Like maybe a model has just this random learned adaptation

3469
03:24:57,000 --> 03:25:00,000
to output things that are designed to get a user

3470
03:25:00,000 --> 03:25:02,000
to feel and believe a certain way,

3471
03:25:02,000 --> 03:25:05,000
that isn't intentional and isn't deceptive

3472
03:25:05,000 --> 03:25:07,000
in some true COGSI sense,

3473
03:25:07,000 --> 03:25:10,000
but it's like enough for this to be a big deal

3474
03:25:10,000 --> 03:25:11,000
that we should care a lot about.

3475
03:25:11,000 --> 03:25:12,000
Okay.

3476
03:25:12,000 --> 03:25:13,000
With that aside.

3477
03:25:13,000 --> 03:25:15,000
Yeah.

3478
03:25:15,000 --> 03:25:21,000
So, I'm definitely hesitant to ascribe

3479
03:25:21,000 --> 03:25:25,000
an overly confident view of what's going on here.

3480
03:25:25,000 --> 03:25:28,000
And I think lots of early discourse alignment

3481
03:25:28,000 --> 03:25:32,000
around things like utility maximization

3482
03:25:32,000 --> 03:25:37,000
and around things like these things are just

3483
03:25:37,000 --> 03:25:40,000
paperclip maximizers, et cetera,

3484
03:25:40,000 --> 03:25:43,000
is kind of misleading.

3485
03:25:43,000 --> 03:25:45,000
And I don't think it is an accurate model

3486
03:25:45,000 --> 03:25:51,000
of how GPT-7 RLHF++ is going to work.

3487
03:25:51,000 --> 03:25:53,000
Well, that's my prediction.

3488
03:25:53,000 --> 03:25:56,000
One thing that is pretty striking to me is

3489
03:25:56,000 --> 03:25:58,000
I just feel like we're pretty confused

3490
03:25:58,000 --> 03:26:00,000
on both sides of this.

3491
03:26:00,000 --> 03:26:03,000
Like, I do not feel like I can confidently claim

3492
03:26:03,000 --> 03:26:05,000
that these models will demonstrate

3493
03:26:05,000 --> 03:26:08,000
anything remotely like goals or intentions,

3494
03:26:08,000 --> 03:26:10,000
but I also don't feel like you can confidently claim

3495
03:26:10,000 --> 03:26:11,000
that they won't.

3496
03:26:11,000 --> 03:26:16,000
And I'm not talking like 99.99% confidence.

3497
03:26:16,000 --> 03:26:20,000
I'm talking like 95% plus confidence either way.

3498
03:26:20,000 --> 03:26:23,000
And one of my visions for what being good

3499
03:26:23,000 --> 03:26:25,000
at Mechantep might look like is being able

3500
03:26:25,000 --> 03:26:28,000
to actually get grounding for these questions.

3501
03:26:28,000 --> 03:26:31,000
Because I think ultimately these are mechanistic questions.

3502
03:26:31,000 --> 03:26:34,000
Behavioral interventions are not enough to answer

3503
03:26:34,000 --> 03:26:39,000
like, does this thing have a goal in any meaningful sense?

3504
03:26:39,000 --> 03:26:44,000
But yeah, my very rough, soft definition would be

3505
03:26:44,000 --> 03:26:47,000
is the model capable of forming

3506
03:26:47,000 --> 03:26:51,000
and executing long-term plans towards some goal,

3507
03:26:51,000 --> 03:26:55,000
potentially if explicitly prompted to auto-GPT

3508
03:26:55,000 --> 03:26:57,000
or just spontaneously,

3509
03:26:57,000 --> 03:27:01,000
is it capable of actually carrying out these plans?

3510
03:27:01,000 --> 03:27:04,000
And does it form and execute plans

3511
03:27:04,000 --> 03:27:07,000
towards some objective

3512
03:27:07,000 --> 03:27:11,000
that is like encoded in the model somewhere?

3513
03:27:11,000 --> 03:27:13,000
And I don't know.

3514
03:27:13,000 --> 03:27:16,000
I think it's pretty plausible that the first dangerous thing

3515
03:27:16,000 --> 03:27:19,000
is like Chaos GPT-7,

3516
03:27:19,000 --> 03:27:22,000
where someone tells it to do something dangerous

3517
03:27:22,000 --> 03:27:25,000
and it gets misused more so than it's like misaligned.

3518
03:27:25,000 --> 03:27:28,000
And I care deeply about both of these risks.

3519
03:27:28,000 --> 03:27:29,000
Okay.

3520
03:27:29,000 --> 03:27:31,000
So yeah, first one's more of a governance question

3521
03:27:31,000 --> 03:27:33,000
than a technical question.

3522
03:27:33,000 --> 03:27:35,000
And thus is less where I feel like I can add value.

3523
03:27:35,000 --> 03:27:37,000
So I agree with you on all of that.

3524
03:27:37,000 --> 03:27:40,000
So yeah, being less confused about what's going on inside the models

3525
03:27:40,000 --> 03:27:43,000
and using interpretability to figure out

3526
03:27:43,000 --> 03:27:46,000
whether they actually do have agency or goals

3527
03:27:46,000 --> 03:27:49,000
and sometimes they do the right things for the wrong reasons.

3528
03:27:49,000 --> 03:27:52,000
Auditing models that seem aligned before they're deployed

3529
03:27:52,000 --> 03:27:55,000
is something that you've told me before.

3530
03:27:55,000 --> 03:27:56,000
So great.

3531
03:27:56,000 --> 03:28:00,000
And just being able to check more deeply that it truly is aligned.

3532
03:28:00,000 --> 03:28:04,000
But I wanted to talk a little bit about

3533
03:28:04,000 --> 03:28:07,000
this interesting paper from Katya Grice.

3534
03:28:07,000 --> 03:28:11,000
So she wrote a response called, it was on the less wrong,

3535
03:28:11,000 --> 03:28:13,000
debunking the AI apocalypse,

3536
03:28:13,000 --> 03:28:15,000
a comprehensive analysis of counterarguments

3537
03:28:15,000 --> 03:28:18,000
to the basic AI risk case, X-Risk.

3538
03:28:18,000 --> 03:28:21,000
And the reason I read it is so many of the comments

3539
03:28:21,000 --> 03:28:25,000
were destroying me and Doug after we interviewed Rob.

3540
03:28:25,000 --> 03:28:28,000
And they said, well, if you're going to criticise S-Risk,

3541
03:28:28,000 --> 03:28:30,000
I mean, at least go and read Katya Grice's response.

3542
03:28:30,000 --> 03:28:31,000
So I did.

3543
03:28:31,000 --> 03:28:32,000
So I did.

3544
03:28:32,000 --> 03:28:33,000
Here we go.

3545
03:28:33,000 --> 03:28:38,000
So she basically made two big counterarguments

3546
03:28:38,000 --> 03:28:41,000
that intelligence might not actually be a huge advantage

3547
03:28:41,000 --> 03:28:44,000
and about the speed of growth is ambiguous.

3548
03:28:44,000 --> 03:28:46,000
But I first want to touch on what you said before,

3549
03:28:46,000 --> 03:28:49,000
which is about this notion of goal-directedness.

3550
03:28:49,000 --> 03:28:53,000
So alignment people say that if superhuman AI systems are built,

3551
03:28:53,000 --> 03:28:57,000
any given system is likely to be goal-directed

3552
03:28:57,000 --> 03:29:00,000
and the orthogonality thesis and instrumental goals

3553
03:29:00,000 --> 03:29:03,000
are cited as aggravating factors.

3554
03:29:03,000 --> 03:29:07,000
And the goal-directed behaviour is likely to be valuable,

3555
03:29:07,000 --> 03:29:08,000
so economically.

3556
03:29:08,000 --> 03:29:10,000
Goal-directed entities may tend to arise

3557
03:29:10,000 --> 03:29:12,000
from machine-ledding training processes,

3558
03:29:12,000 --> 03:29:14,000
not intending to create them,

3559
03:29:14,000 --> 03:29:17,000
which is kind of talking about some of the emergent behaviours

3560
03:29:17,000 --> 03:29:20,000
that we were talking about earlier with respect to Othello,

3561
03:29:20,000 --> 03:29:21,000
for example.

3562
03:29:21,000 --> 03:29:25,000
And coherence arguments may imply that systems with goal-directedness

3563
03:29:25,000 --> 03:29:28,000
will become more strongly goal-directed over time,

3564
03:29:28,000 --> 03:29:30,000
which is apparently something that is argued for.

3565
03:29:30,000 --> 03:29:32,000
So I'm thinking, what does goal even mean?

3566
03:29:32,000 --> 03:29:35,000
I mean, we anthropomorphise abstract human intelligible concepts

3567
03:29:35,000 --> 03:29:40,000
like goals and they really are emergent

3568
03:29:40,000 --> 03:29:43,000
because they emerge from these low-level interactions

3569
03:29:43,000 --> 03:29:45,000
in the cells in your body

3570
03:29:45,000 --> 03:29:48,000
and then you get these things that we recognise to be goals,

3571
03:29:48,000 --> 03:29:50,000
observer-relative, as we were talking about before.

3572
03:29:50,000 --> 03:29:54,000
But they're just graduated phenomena from smaller things, right?

3573
03:29:54,000 --> 03:29:57,000
So what does it even mean to have a goal?

3574
03:29:58,000 --> 03:30:03,000
Yeah, so a couple of thoughts on that.

3575
03:30:03,000 --> 03:30:06,000
Again, you ask questions with a lot of content in them.

3576
03:30:06,000 --> 03:30:10,000
No problem. I can only apologise.

3577
03:30:10,000 --> 03:30:12,000
I mean, someone who accidentally writes

3578
03:30:12,000 --> 03:30:16,000
19,000 word blog posts all the time, I relate.

3579
03:30:16,000 --> 03:30:22,000
Anyway, so what am I saying?

3580
03:30:22,000 --> 03:30:24,000
So the way...

3581
03:30:24,000 --> 03:30:26,000
Yeah, it's a fake concept, right?

3582
03:30:26,000 --> 03:30:30,000
Yeah, so I definitely want to try to take...

3583
03:30:30,000 --> 03:30:36,000
So there's the mechanistic definition of the model forms plans

3584
03:30:36,000 --> 03:30:40,000
and it evaluates the plans according to some criteria or objective

3585
03:30:40,000 --> 03:30:43,000
and it executes the plans that score better on this.

3586
03:30:43,000 --> 03:30:49,000
And I would love if we get to a point where we can look inside a model

3587
03:30:49,000 --> 03:30:52,000
and look the circuitry that could be behind this or not.

3588
03:30:53,000 --> 03:30:56,000
That would feel like a big milestone for me on

3589
03:30:56,000 --> 03:30:59,000
wow, I really believe mech and tuple matter

3590
03:30:59,000 --> 03:31:02,000
for reducing catastrophic risk from AI.

3591
03:31:04,000 --> 03:31:08,000
A second thing is that...

3592
03:31:10,000 --> 03:31:13,000
Yeah, the kind of more behavioural thing of

3593
03:31:13,000 --> 03:31:15,000
the model systematically takes actions

3594
03:31:15,000 --> 03:31:18,000
that pushes the world towards a certain state.

3595
03:31:18,000 --> 03:31:22,000
And I don't want...

3596
03:31:22,000 --> 03:31:26,000
I think there's a common problem in alignment arguments

3597
03:31:26,000 --> 03:31:30,000
where people get too precise and too specific

3598
03:31:30,000 --> 03:31:33,000
in a way that lots of people reasonably object to

3599
03:31:33,000 --> 03:31:36,000
in a way which is not necessary for the argument.

3600
03:31:36,000 --> 03:31:39,000
There's a really great paper called

3601
03:31:39,000 --> 03:31:41,000
The Alignment Problem from a Deep Learning Perspective

3602
03:31:41,000 --> 03:31:45,000
by Richard Ngo, Lawrence Tan, and Sorin Mindenman.

3603
03:31:45,000 --> 03:31:48,000
And this is probably my biggest recommendation

3604
03:31:48,000 --> 03:31:51,000
for the listening audience of what I think is like

3605
03:31:51,000 --> 03:31:54,000
a pretty well-presented case for alignment.

3606
03:31:54,000 --> 03:31:57,000
And I generally pretty pro-try to make the minimal

3607
03:31:57,000 --> 03:31:59,000
necessary assumptions.

3608
03:31:59,000 --> 03:32:02,000
So for me it's kind of like some soft form of

3609
03:32:02,000 --> 03:32:05,000
goal-directedness of take actions that push the world

3610
03:32:05,000 --> 03:32:07,000
towards a certain state.

3611
03:32:07,000 --> 03:32:12,000
And another important thing is

3612
03:32:12,000 --> 03:32:14,000
there are a bunch of theoretical arguments

3613
03:32:14,000 --> 03:32:18,000
for why goals would spontaneously emerge.

3614
03:32:18,000 --> 03:32:22,000
Ideas around in a misalignment from work

3615
03:32:22,000 --> 03:32:26,000
led by Evan Huminger, ideas around

3616
03:32:26,000 --> 03:32:30,000
just coherent theorems and things like that,

3617
03:32:30,000 --> 03:32:32,000
which I know I find a bit convincing,

3618
03:32:32,000 --> 03:32:35,000
not that convincing, but then there's

3619
03:32:35,000 --> 03:32:38,000
things will have goals because we try to give them goals.

3620
03:32:38,000 --> 03:32:42,000
And I'm like, yeah, that's probably gonna happen.

3621
03:32:42,000 --> 03:32:44,000
It's just clearly useful.

3622
03:32:44,000 --> 03:32:48,000
If you want to have an AI CEO

3623
03:32:48,000 --> 03:32:52,000
or AI helping run logistics and military operations

3624
03:32:52,000 --> 03:32:54,000
to have something that's capable of

3625
03:32:54,000 --> 03:32:56,000
forming and executing long-term plans

3626
03:32:56,000 --> 03:32:58,000
towards some objective.

3627
03:32:58,000 --> 03:33:01,000
And if you believe this is what's gonna happen,

3628
03:33:01,000 --> 03:33:03,000
then the key question is,

3629
03:33:03,000 --> 03:33:06,000
are we capable of ensuring those goals

3630
03:33:06,000 --> 03:33:09,000
are exactly the goals we would like them to be?

3631
03:33:09,000 --> 03:33:12,000
And my answer for any question of the form,

3632
03:33:12,000 --> 03:33:15,000
can we precisely make sure the system is doing exactly X

3633
03:33:15,000 --> 03:33:18,000
and machine learning is, God, no.

3634
03:33:18,000 --> 03:33:21,000
We are not remotely good enough to achieve this

3635
03:33:21,000 --> 03:33:25,000
with our current level of alignment and steering techniques.

3636
03:33:25,000 --> 03:33:29,000
And to me, this is like a more interesting point

3637
03:33:29,000 --> 03:33:31,000
where it's not quite a crux for me,

3638
03:33:31,000 --> 03:33:36,000
but it just seems like a lot easier to argue about

3639
03:33:36,000 --> 03:33:38,000
what people do this.

3640
03:33:39,000 --> 03:33:42,000
Yeah, essentially, I mean, Katya herself said that

3641
03:33:42,000 --> 03:33:46,000
it's unclear that goal-directedness

3642
03:33:46,000 --> 03:33:49,000
is favored by economic pressure to training dynamics

3643
03:33:49,000 --> 03:33:51,000
or coherence arguments, you know,

3644
03:33:51,000 --> 03:33:55,000
whether those are the same thing as kind of goal-directedness

3645
03:33:55,000 --> 03:33:58,000
that implies a zealous drive to control the universe.

3646
03:33:58,000 --> 03:34:01,000
And look at South Korea, they have goals.

3647
03:34:01,000 --> 03:34:05,000
And those goals, I don't really subscribe

3648
03:34:05,000 --> 03:34:08,000
to the dictator view of society.

3649
03:34:08,000 --> 03:34:11,000
I assume they are somehow emergent.

3650
03:34:11,000 --> 03:34:13,000
And similarly...

3651
03:34:13,000 --> 03:34:15,000
Sorry, South Korea or North Korea?

3652
03:34:15,000 --> 03:34:17,000
Sorry, North Korea, did I say South Korea?

3653
03:34:17,000 --> 03:34:19,000
Very different careers.

3654
03:34:19,000 --> 03:34:22,000
Different goals, different goals.

3655
03:34:22,000 --> 03:34:25,000
But you can think about goals in an AI system

3656
03:34:25,000 --> 03:34:29,000
as either being ones which emerge from some low level

3657
03:34:29,000 --> 03:34:32,000
or ones which are explicitly coded by us

3658
03:34:32,000 --> 03:34:35,000
or ones which are instrumental.

3659
03:34:35,000 --> 03:34:38,000
And these are all a whole bunch of goals.

3660
03:34:38,000 --> 03:34:41,000
But we can't really control those.

3661
03:34:41,000 --> 03:34:44,000
We can add pressures.

3662
03:34:44,000 --> 03:34:47,000
How do we control what North Korea does?

3663
03:34:47,000 --> 03:34:51,000
That sure is a question I'd love for someone to answer.

3664
03:34:51,000 --> 03:34:53,000
I don't know.

3665
03:34:53,000 --> 03:34:56,000
I can give speculation.

3666
03:34:56,000 --> 03:34:59,000
There's the question within practice,

3667
03:34:59,000 --> 03:35:01,000
what do people do?

3668
03:35:01,000 --> 03:35:04,000
Which is basically reinforcement learning from human feedback.

3669
03:35:04,000 --> 03:35:07,000
And I expect people would apply that in this situation as well.

3670
03:35:07,000 --> 03:35:10,000
I definitely do not believe we would be able

3671
03:35:10,000 --> 03:35:13,000
to explicitly encode a goal in the system.

3672
03:35:13,000 --> 03:35:17,000
Moreover, even if you can encode,

3673
03:35:17,000 --> 03:35:20,000
even if you could give some scoring function,

3674
03:35:20,000 --> 03:35:23,000
like make the score in this game high,

3675
03:35:23,000 --> 03:35:27,000
this does not give you a model that intrinsically cares about that

3676
03:35:27,000 --> 03:35:30,000
in the same way that, I don't know,

3677
03:35:30,000 --> 03:35:33,000
evolution optimizes inclusive genetic fitness.

3678
03:35:33,000 --> 03:35:36,000
I don't give a fuck about inclusive genetic fitness.

3679
03:35:36,000 --> 03:35:38,000
Even though I care about a bunch of things,

3680
03:35:38,000 --> 03:35:41,000
evolution got me to care about within that,

3681
03:35:41,000 --> 03:35:45,000
like tasty foods and surviving.

3682
03:35:45,000 --> 03:35:51,000
Yeah, so we don't know how to put goals into systems.

3683
03:35:51,000 --> 03:35:55,000
I basically just assert that we are not currently capable

3684
03:35:55,000 --> 03:35:58,000
of putting goals into systems well.

3685
03:35:58,000 --> 03:36:02,000
And this is one of the main things

3686
03:36:02,000 --> 03:36:04,000
the field of alignment thinks about.

3687
03:36:04,000 --> 03:36:06,000
And we're not very good at it.

3688
03:36:06,000 --> 03:36:08,000
It'd be great if we were better at it.

3689
03:36:08,000 --> 03:36:13,000
In terms of, yeah, I definitely don't want to make strong claims

3690
03:36:13,000 --> 03:36:16,000
about, to be dangerous, the goals need to be coherent

3691
03:36:16,000 --> 03:36:20,000
or the goals need to, there needs to be like a singular goal.

3692
03:36:20,000 --> 03:36:23,000
Like I don't have a singular goal.

3693
03:36:23,000 --> 03:36:28,000
It's not obvious to me how these systems will turn out.

3694
03:36:28,000 --> 03:36:31,000
If they don't, in any meaningful sense,

3695
03:36:31,000 --> 03:36:35,000
want a coherent thing, then I'm a fair bit less concerned.

3696
03:36:35,000 --> 03:36:39,000
Though, well, I mean, there's many, many ways

3697
03:36:39,000 --> 03:36:41,000
that human level AI would be good for the world

3698
03:36:41,000 --> 03:36:44,000
or bad for the world, or just wildly destabilizing

3699
03:36:44,000 --> 03:36:47,000
and high variance, of which misalignment risk is one of them.

3700
03:36:47,000 --> 03:36:49,000
And lots of the other ones just don't apply,

3701
03:36:49,000 --> 03:36:52,000
like misuse and systemic risks.

3702
03:36:52,000 --> 03:36:56,000
But leaving those aside, yeah,

3703
03:36:56,000 --> 03:36:59,000
I think if a model is just roughly pushing

3704
03:36:59,000 --> 03:37:02,000
in a goal-directed direction with a bunch of caveats

3705
03:37:02,000 --> 03:37:04,000
and uncertainties and flip-flopping,

3706
03:37:04,000 --> 03:37:08,000
that still seems like a pretty big deal to me.

3707
03:37:08,000 --> 03:37:10,000
Okay, okay.

3708
03:37:10,000 --> 03:37:14,000
Katia, let's just cover her two main arguments.

3709
03:37:14,000 --> 03:37:17,000
So she said that intelligence might not actually be

3710
03:37:17,000 --> 03:37:19,000
a huge advantage.

3711
03:37:19,000 --> 03:37:23,000
Looking at the world, intuitively,

3712
03:37:23,000 --> 03:37:27,000
big discrepancies in power are not to do with intelligence.

3713
03:37:27,000 --> 03:37:32,000
And she said IQ humans with an IQ of 130

3714
03:37:32,000 --> 03:37:35,000
earn roughly $6,000 to $18,000 a year,

3715
03:37:35,000 --> 03:37:37,000
more than average IQ humans.

3716
03:37:37,000 --> 03:37:39,000
Elected representatives are apparently

3717
03:37:39,000 --> 03:37:42,000
slightly smarter on average,

3718
03:37:42,000 --> 03:37:44,000
but not a radical difference.

3719
03:37:44,000 --> 03:37:48,000
Mensa isn't a major force in the world.

3720
03:37:48,000 --> 03:37:51,000
And if we look at people who evidently

3721
03:37:51,000 --> 03:37:54,000
have good cognitive abilities given their intellectual output,

3722
03:37:54,000 --> 03:37:56,000
their personal lives are not obviously drastically

3723
03:37:56,000 --> 03:37:58,000
more successful anecdotally.

3724
03:37:58,000 --> 03:38:02,000
So is it that much of a big deal?

3725
03:38:02,000 --> 03:38:04,000
Yeah.

3726
03:38:04,000 --> 03:38:07,000
So I think this is like a fair point.

3727
03:38:07,000 --> 03:38:11,000
If we looked in the world and IQ,

3728
03:38:11,000 --> 03:38:15,000
or whatever metric of intelligence you want to use,

3729
03:38:15,000 --> 03:38:17,000
it's really dramatically correlated

3730
03:38:17,000 --> 03:38:19,000
with everything good about someone.

3731
03:38:19,000 --> 03:38:22,000
I mean, IQ correlates with basically everything

3732
03:38:22,000 --> 03:38:24,000
you might value in someone's life,

3733
03:38:24,000 --> 03:38:26,000
because we live in an unfair world,

3734
03:38:26,000 --> 03:38:30,000
but not dramatically.

3735
03:38:30,000 --> 03:38:34,000
Yeah, so I think this is a valid argument.

3736
03:38:34,000 --> 03:38:39,000
I generally don't think you should model

3737
03:38:39,000 --> 03:38:43,000
human-level AI as like,

3738
03:38:43,000 --> 03:38:47,000
or like slightly superhuman AI as like an IQ 200 human.

3739
03:38:47,000 --> 03:38:49,000
Like, for example,

3740
03:38:49,000 --> 03:38:51,000
GPT-4, I would argue,

3741
03:38:51,000 --> 03:38:55,000
knows most facts on the internet,

3742
03:38:55,000 --> 03:38:58,000
or many facts.

3743
03:38:58,000 --> 03:39:02,000
And, yeah, knows many facts.

3744
03:39:02,000 --> 03:39:06,000
And this seems...

3745
03:39:06,000 --> 03:39:11,000
GPT-4 knows many facts.

3746
03:39:11,000 --> 03:39:16,000
And this is sure an advantage over me.

3747
03:39:16,000 --> 03:39:18,000
GPT-4 knows how to write a lot of code,

3748
03:39:18,000 --> 03:39:21,000
and it knows how to take software

3749
03:39:21,000 --> 03:39:23,000
and do penetration testing on it.

3750
03:39:23,000 --> 03:39:26,000
It knows lots of social conventions

3751
03:39:26,000 --> 03:39:28,000
and cultural things,

3752
03:39:28,000 --> 03:39:32,000
and has lots of experience reading various kinds of

3753
03:39:32,000 --> 03:39:35,000
text written to be manipulative,

3754
03:39:35,000 --> 03:39:37,000
or manuals on how to make nuclear weapons.

3755
03:39:37,000 --> 03:39:40,000
Sorry, I'm going too hard on the knowledge point.

3756
03:39:40,000 --> 03:39:42,000
There's just lots of different axes.

3757
03:39:42,000 --> 03:39:45,000
You can be human-level or better,

3758
03:39:45,000 --> 03:39:47,000
in which knowledge is one.

3759
03:39:47,000 --> 03:39:49,000
Intelligence or reasoning is one.

3760
03:39:49,000 --> 03:39:52,000
Social manipulation abilities is another.

3761
03:39:52,000 --> 03:39:54,000
Charisma and persuasion is another.

3762
03:39:54,000 --> 03:39:58,000
I think these two are particularly important ones.

3763
03:39:58,000 --> 03:40:03,000
There's forming coherent plans.

3764
03:40:03,000 --> 03:40:07,000
There's just like the ability to execute on stuff.

3765
03:40:07,000 --> 03:40:11,000
24x7 running thousands of copies of yourself in parallel,

3766
03:40:11,000 --> 03:40:14,000
distributed across the world.

3767
03:40:14,000 --> 03:40:17,000
There's running faster than humans.

3768
03:40:17,000 --> 03:40:19,000
And there's just like lots of dimensions here.

3769
03:40:19,000 --> 03:40:24,000
I think the IQ 200 human frame is helpful in some ways,

3770
03:40:24,000 --> 03:40:27,000
but unhelpful in other ways,

3771
03:40:27,000 --> 03:40:31,000
especially if it summons the nerdy scientist

3772
03:40:31,000 --> 03:40:36,000
with no social skills who's life is a mess archetype.

3773
03:40:36,000 --> 03:40:38,000
I'd say it's a nerdy scientist with no social skills

3774
03:40:38,000 --> 03:40:42,000
whose life is a mess.

3775
03:40:42,000 --> 03:40:43,000
Okay, yeah.

3776
03:40:43,000 --> 03:40:45,000
I mean, this is the thing is...

3777
03:40:45,000 --> 03:40:46,000
Because Rob said the same thing.

3778
03:40:46,000 --> 03:40:48,000
On chess, it's possible for someone to be

3779
03:40:48,000 --> 03:40:50,000
literally 20 times better than you,

3780
03:40:50,000 --> 03:40:52,000
that there's a huge dynamic range of skill.

3781
03:40:52,000 --> 03:40:54,000
And that's something we've not really seen

3782
03:40:54,000 --> 03:40:55,000
in human intelligence,

3783
03:40:55,000 --> 03:40:58,000
and it might be because of the way we measure it.

3784
03:40:58,000 --> 03:41:00,000
It's possible that the way we measure it

3785
03:41:00,000 --> 03:41:05,000
doesn't even capture people with broader

3786
03:41:05,000 --> 03:41:07,000
or better abilities.

3787
03:41:07,000 --> 03:41:09,000
Let's just cover her last point quickly.

3788
03:41:09,000 --> 03:41:14,000
So this is that the speed of intelligence growth is ambiguous.

3789
03:41:14,000 --> 03:41:18,000
So this idea that AI would be able to rapidly destroy the world

3790
03:41:18,000 --> 03:41:21,000
seems prima facie unlikely to Katia,

3791
03:41:21,000 --> 03:41:24,000
since no other entity has ever done that.

3792
03:41:24,000 --> 03:41:26,000
And she goes on.

3793
03:41:26,000 --> 03:41:30,000
So the two common broad arguments is that there'll be a feedback loop

3794
03:41:30,000 --> 03:41:34,000
in which intelligent AI makes more intelligent AI repeatedly

3795
03:41:34,000 --> 03:41:36,000
until AI is very, very intelligent.

3796
03:41:36,000 --> 03:41:40,000
Number two, small differences in brains seem to correspond

3797
03:41:40,000 --> 03:41:42,000
to very large differences in performance

3798
03:41:42,000 --> 03:41:44,000
based on observing humans and other apes.

3799
03:41:44,000 --> 03:41:47,000
Thus, any movement past human level

3800
03:41:47,000 --> 03:41:50,000
will take us to unimaginably super human level.

3801
03:41:50,000 --> 03:41:53,000
And the basic counter-arguments to that is that

3802
03:41:53,000 --> 03:41:56,000
the feedback loops might not be as powerful as assumed.

3803
03:41:56,000 --> 03:41:58,000
There could be diminished returns,

3804
03:41:58,000 --> 03:42:00,000
there could be resource constraints,

3805
03:42:00,000 --> 03:42:02,000
and there could be complexity barriers.

3806
03:42:02,000 --> 03:42:06,000
So maybe we should just do that kind of recursive self-improving piece first.

3807
03:42:06,000 --> 03:42:07,000
What do you think about that?

3808
03:42:07,000 --> 03:42:09,000
I don't really buy recursive self-improvement.

3809
03:42:09,000 --> 03:42:10,000
Oh, good.

3810
03:42:10,000 --> 03:42:13,000
It's not an important part of why I'm concerned about this stuff.

3811
03:42:13,000 --> 03:42:20,000
So generally, I just feel like a lot of the arguments were made

3812
03:42:20,000 --> 03:42:24,000
before the current paradigm of enormous foundation models.

3813
03:42:24,000 --> 03:42:29,000
When you're investing hundreds of millions of dollars of compute into a thing,

3814
03:42:29,000 --> 03:42:33,000
it's pretty hard for it to make itself substantially better.

3815
03:42:35,000 --> 03:42:39,000
And you can do things like design better algorithmic techniques.

3816
03:42:39,000 --> 03:42:44,000
I think that is probably one that is more likely to be accelerated

3817
03:42:44,000 --> 03:42:46,000
the better the model gets.

3818
03:42:46,000 --> 03:42:52,000
It's not clear to me how much juice there's to squeeze out of that.

3819
03:42:53,000 --> 03:43:00,000
But generally, I just think a lot of this is going to be bottlenecked

3820
03:43:00,000 --> 03:43:03,000
by hardware and compute and data,

3821
03:43:03,000 --> 03:43:09,000
such that I'm less concerned about some runaway intelligence explosion,

3822
03:43:09,000 --> 03:43:11,000
and I'm more just concerned about,

3823
03:43:11,000 --> 03:43:13,000
we'll eventually make things that are dangerous.

3824
03:43:13,000 --> 03:43:15,000
What do we do then?

3825
03:43:16,000 --> 03:43:20,000
And I think this is a really good fact about the world.

3826
03:43:20,000 --> 03:43:25,000
I think a world where you can have intelligence explosions is really scary.

3827
03:43:25,000 --> 03:43:30,000
And I feel like our current world is a lot less scary than it could have been.

3828
03:43:30,000 --> 03:43:35,000
If some kid in a basement somewhere just wrote the code for AGI one day.

3829
03:43:35,000 --> 03:43:37,000
Yes, yes.

3830
03:43:38,000 --> 03:43:41,000
Okay, well, I mean, just to finish off Katya's final point.

3831
03:43:41,000 --> 03:43:45,000
So the other point they made was about small differences might lead to oval.

3832
03:43:45,000 --> 03:43:47,000
It's a little bit like in squash.

3833
03:43:47,000 --> 03:43:49,000
I don't know if you've ever played squash,

3834
03:43:49,000 --> 03:43:55,000
but a tiny difference in ability leads to one player overwhelmingly dominating the other player,

3835
03:43:55,000 --> 03:43:59,000
because you just get these kind of like, you know, it's a game of attrition,

3836
03:43:59,000 --> 03:44:01,000
and you get these tipping points.

3837
03:44:01,000 --> 03:44:06,000
And she argued that that might not necessarily be the case when comparing AI systems,

3838
03:44:06,000 --> 03:44:08,000
because of three reasons.

3839
03:44:08,000 --> 03:44:12,000
Different architectures, likely to have very different underlying architectures

3840
03:44:12,000 --> 03:44:15,000
and biological brains, which could lead to different scaling properties.

3841
03:44:15,000 --> 03:44:21,000
Performance plateaus, so there might be these plateaus beyond which further increases in intelligence,

3842
03:44:21,000 --> 03:44:24,000
you know, don't lead to significant performance improvements.

3843
03:44:24,000 --> 03:44:28,000
And also this notion of task specific intelligence, something that I strong,

3844
03:44:28,000 --> 03:44:31,000
I believe that all intelligence is specialized as we were speaking about earlier.

3845
03:44:31,000 --> 03:44:35,000
And so it might be specialized rather than being generally intelligent,

3846
03:44:35,000 --> 03:44:42,000
and small differences thus may not translate into large differences in performance across a wide variety of tasks.

3847
03:44:42,000 --> 03:44:46,000
Maybe we should just touch on this, on this kind of task focus thing.

3848
03:44:46,000 --> 03:44:48,000
So I think humans are very specialized.

3849
03:44:48,000 --> 03:44:53,000
We have, and we don't realize that we are because the way we conceive of intelligence is anthropomorphic,

3850
03:44:53,000 --> 03:44:56,000
but actually we don't do four dimensions very well.

3851
03:44:56,000 --> 03:44:58,000
There's lots of things that we don't do very well,

3852
03:44:58,000 --> 03:45:02,000
and we're kind of embedded in the cognitive ecology in quite a complex way.

3853
03:45:02,000 --> 03:45:04,000
So what do you think about that?

3854
03:45:04,000 --> 03:45:11,000
Yeah, so I will, okay, I'll first comment on the general metodynamic of,

3855
03:45:11,000 --> 03:45:17,000
I think that people get way too caught up on philosophizing.

3856
03:45:17,000 --> 03:45:19,000
And no offense.

3857
03:45:19,000 --> 03:45:20,000
I'm so sorry.

3858
03:45:20,000 --> 03:45:28,000
And in particular, I care about whether an AI will cause a catastrophic risk.

3859
03:45:28,000 --> 03:45:34,000
I don't care about whether it fits into, whether it's general in the right way,

3860
03:45:34,000 --> 03:45:37,000
whether it has weaknesses in certain areas,

3861
03:45:37,000 --> 03:45:40,000
whether it's high on the Chomsky hierarchy,

3862
03:45:40,000 --> 03:45:47,000
or whether it's generally intelligent in some specific sense that someone like Gary Marcus would agree with.

3863
03:45:47,000 --> 03:45:54,000
Is that in any way a contradiction of your mechanistic sensibilities?

3864
03:45:54,000 --> 03:45:59,000
Because when it comes to neural networks, you want to understand how they work,

3865
03:45:59,000 --> 03:46:02,000
but when it comes to intelligence, you don't.

3866
03:46:02,000 --> 03:46:03,000
Oh, sorry.

3867
03:46:03,000 --> 03:46:06,000
I want to understand how it works.

3868
03:46:06,000 --> 03:46:08,000
I want to understand everything.

3869
03:46:08,000 --> 03:46:11,000
I just don't think it's...

3870
03:46:11,000 --> 03:46:20,000
I want to disentangle things to be concerned about from theoretical arguments about whether this fits into certain categories.

3871
03:46:20,000 --> 03:46:24,000
For the purposes of deciding whether to be concerned about AI existential risk,

3872
03:46:24,000 --> 03:46:31,000
I see all of the theory arguments as like a means to an end of this ultimate empirical question of,

3873
03:46:31,000 --> 03:46:34,000
is this a thing that could realistically happen?

3874
03:46:34,000 --> 03:46:42,000
And I think that these theoretical frameworks do matter.

3875
03:46:43,000 --> 03:46:45,000
Like, I don't know.

3876
03:46:45,000 --> 03:46:50,000
I think that an image classification model is basically never going to get the point where it's dangerous,

3877
03:46:50,000 --> 03:47:01,000
while a language model that's being RLHF'd to have some notion of intentionality potentially will.

3878
03:47:01,000 --> 03:47:05,000
And, yeah.

3879
03:47:05,000 --> 03:47:10,000
I know I can give random takes, but to me, if you're like,

3880
03:47:10,000 --> 03:47:13,000
AI's can be task-specific in the same way that humans are task-specific.

3881
03:47:13,000 --> 03:47:22,000
I'm like, well, a human is task-general enough that I think they could be massively dangerous in the right situation with the right advantages.

3882
03:47:22,000 --> 03:47:29,000
Like, if they wanted to be and were able to run the thousands copies of themselves at a thousand X speed or something.

3883
03:47:29,000 --> 03:47:33,000
I don't know if that's actually a remotely accurate statement about models.

3884
03:47:33,000 --> 03:47:37,000
Probably they can run many copies, but not a thousand X speed or something.

3885
03:47:37,000 --> 03:47:40,000
But, yeah.

3886
03:47:40,000 --> 03:47:50,000
Generally, that's the kind of question I care about, and I'm concerned many of these definitions lose sight of that.

3887
03:47:50,000 --> 03:47:58,000
And part of my thing of like, I want to keep alignment arguments as having as few assumptions as possible.

3888
03:47:58,000 --> 03:48:02,000
Because the more assumptions you make, the less plausible your case is.

3889
03:48:02,000 --> 03:48:08,000
And the less and like, more room there is for people to like, rightfully disagree.

3890
03:48:08,000 --> 03:48:17,000
I'm like, I want to be careful not to make any of the case rest on like, strong theoretical frameworks because we don't know what we're doing here.

3891
03:48:17,000 --> 03:48:20,000
Enough to have legit theoretical frameworks.

3892
03:48:20,000 --> 03:48:27,000
And I think that AI is likely to be limited in the same way that humans are, at least within the GPT paradigm.

3893
03:48:27,000 --> 03:48:38,000
If you're training it to predict the next word on the internet and a bunch of other stuff, then it's going to learn a lot from human patterns and human thought and human conventions.

3894
03:48:38,000 --> 03:48:42,000
But, I don't know.

3895
03:48:42,000 --> 03:48:50,000
In closing, you said that your personal favorite heuristic is the second species argument.

3896
03:48:50,000 --> 03:48:52,000
Can you tell us?

3897
03:48:52,000 --> 03:49:09,000
Yeah. So, I quite like Hinton's recent pithy quote of, there is no example of something being of some entity being controlled by things less smart than it.

3898
03:49:09,000 --> 03:49:11,000
And that was terrible.

3899
03:49:11,000 --> 03:49:12,000
Sorry.

3900
03:49:12,000 --> 03:49:16,000
I really would. I mean, Twitter went wild over that.

3901
03:49:16,000 --> 03:49:17,000
Oh, they're trying to go wild.

3902
03:49:17,000 --> 03:49:20,000
I mean, look at a company.

3903
03:49:20,000 --> 03:49:23,000
The CEO is usually done with that.

3904
03:49:23,000 --> 03:49:28,000
You have to hire competent people to have a successful company or look at my cat.

3905
03:49:28,000 --> 03:49:29,000
Yeah. Okay.

3906
03:49:29,000 --> 03:49:30,000
This is a terrible thing.

3907
03:49:30,000 --> 03:49:33,000
Let's just start again.

3908
03:49:33,000 --> 03:49:34,000
All right.

3909
03:49:34,000 --> 03:49:38,000
So, yeah, this is often called the gorilla problem.

3910
03:49:38,000 --> 03:49:43,000
Humans are just smarter than gorillas in basically all ways that matter.

3911
03:49:43,000 --> 03:49:54,000
Humans are not actively malevolent to gorillas, but ultimately humans are in charge gorillas are not and gorillas exist because of our continued benevolence or ambivalence.

3912
03:49:54,000 --> 03:50:09,000
And it just seems to me like if you are creating entities that are smarter than you, the default outcome is they end up in control of what's going on in the world and you do not.

3913
03:50:09,000 --> 03:50:13,000
And I kind of just feel like this should be the null hypothesis.

3914
03:50:13,000 --> 03:50:19,000
And then there's a bunch of arguments on top of like, is this a good model?

3915
03:50:19,000 --> 03:50:23,000
Well, obviously, there's lots of disanalogy is because we're making them.

3916
03:50:23,000 --> 03:50:25,000
We ideally have some control over them.

3917
03:50:25,000 --> 03:50:28,000
We're going to try to shape them to be benevolent towards us.

3918
03:50:28,000 --> 03:50:33,000
But this just seems like the default thing to be concerned about to me.

3919
03:50:33,000 --> 03:50:37,000
On that point, though, we are different from computers.

3920
03:50:37,000 --> 03:50:38,000
We scuba dive.

3921
03:50:38,000 --> 03:50:40,000
And that's actually quite a profound thing to say.

3922
03:50:40,000 --> 03:50:48,000
We scuba dive because we are integrated into the ecosystem, not just physically, but cognitively.

3923
03:50:48,000 --> 03:50:51,000
There's a kind of cognitive ecosystem that we're enmeshed in.

3924
03:50:51,000 --> 03:50:54,000
We have a huge advantage over computers.

3925
03:50:54,000 --> 03:50:58,000
Computers can't really do anything in the physical world.

3926
03:50:58,000 --> 03:51:02,000
So I agree with this, but I don't know.

3927
03:51:02,000 --> 03:51:07,000
I feel like the way, I don't know.

3928
03:51:07,000 --> 03:51:18,000
One evocative example is there was this crime lord, El Shapo, who ran his gang from within prison for like many years, very successfully.

3929
03:51:18,000 --> 03:51:25,000
When you have humans in the world who can get to do things for you, you don't need to be physically embodied to get shit done.

3930
03:51:25,000 --> 03:51:27,000
And I don't just look at Blake Lemoine.

3931
03:51:27,000 --> 03:51:36,000
There's no shortage of people who will do things if convinced in the right way, even if they know it's an AI.

3932
03:51:36,000 --> 03:51:38,000
And I do agree with you on that.

3933
03:51:38,000 --> 03:51:50,000
And I think part of the reason why we're going to have the inevitable proliferation of this technology is so many tinkerers will just create many, many different versions of AI.

3934
03:51:50,000 --> 03:51:54,000
And they won't really be thinking about the consequences of their actions.

3935
03:51:54,000 --> 03:51:58,000
But what's the alternative, paternalism?

3936
03:51:59,000 --> 03:52:09,000
Yeah, so to me, the main interesting thing here is large training runs as like the major bottleneck.

3937
03:52:09,000 --> 03:52:11,000
Very few actors can do them.

3938
03:52:11,000 --> 03:52:18,000
We're probably going to get beyond the point where people are even putting the things out behind an API open to many people to use,

3939
03:52:18,000 --> 03:52:24,000
let alone like open sourcing the weights, which we've already pretty clearly moved past.

3940
03:52:24,000 --> 03:52:33,000
And this, to me, seems like the point of intervention you need if you're going to try to make sure things are safe before you deploy them,

3941
03:52:33,000 --> 03:52:43,000
like track the people who are able to do these runs, have standards for what it means to decide a system like this is safe.

3942
03:52:43,000 --> 03:52:48,000
I'm pretty happy Sam Altman's been pushing that stuff very heavily.

3943
03:52:48,000 --> 03:52:53,000
And if competently done, I think this kind of regulation can be very important.

3944
03:52:53,000 --> 03:52:55,000
It could be great.

3945
03:52:55,000 --> 03:52:58,000
Like the alignment research has been doing great work here.

3946
03:52:58,000 --> 03:53:05,000
And I'm very excited to see what the red teaming large language models thing at Defconn looks like.

3947
03:53:05,000 --> 03:53:11,000
But I don't know, maybe to close, I feel like I've been in the role of why alignment matters.

3948
03:53:11,000 --> 03:53:16,000
Maybe I can try to break alignment arguments myself for a bit.

3949
03:53:16,000 --> 03:53:18,000
Please do, yeah.

3950
03:53:18,000 --> 03:53:24,000
So if I condition on actually the world is kind of fine,

3951
03:53:24,000 --> 03:53:34,000
probably my biggest guess is that the goal directed notion is just like not remotely a good understanding of how these things work.

3952
03:53:34,000 --> 03:53:37,000
And it's hard to get them to be goal directed.

3953
03:53:37,000 --> 03:53:40,000
And we just mostly coordinate and don't do that.

3954
03:53:40,000 --> 03:53:45,000
And these systems are mostly just like extremely effective tools.

3955
03:53:45,000 --> 03:53:48,000
It seems like kind of a plausible world we could end up in.

3956
03:53:48,000 --> 03:53:54,000
I don't think it's any more likely than, yep, they're goal directed and this is terrible.

3957
03:53:54,000 --> 03:54:02,000
We end up in a world which just has like lots of these systems that don't coordinate with each other,

3958
03:54:02,000 --> 03:54:08,000
want some more different things are like broadly aligned with human interests,

3959
03:54:08,000 --> 03:54:13,000
but like imperfectly, and just none of them ever get a major advantage over the others.

3960
03:54:13,000 --> 03:54:20,000
And the world kind of continues to be about as the world is with lots of different actors who aren't necessarily aligned with each other,

3961
03:54:20,000 --> 03:54:26,000
but mostly don't try to see over the world except every so often.

3962
03:54:26,000 --> 03:54:30,000
Or we just alignment isn't that hard.

3963
03:54:30,000 --> 03:54:32,000
We crack mechanistic interoperability.

3964
03:54:32,000 --> 03:54:34,000
We look inside the system.

3965
03:54:34,000 --> 03:54:38,000
We use this to iterate on making our techniques really good.

3966
03:54:38,000 --> 03:54:44,000
It turns out that doing RLHF with like enough adversarial training just kind of works.

3967
03:54:44,000 --> 03:54:48,000
Or with AI assistance to help you notice what's going on in the system.

3968
03:54:48,000 --> 03:54:54,000
And this just gets us aligned to the level systems and we can be like, please go solve the problem.

3969
03:54:54,000 --> 03:54:56,000
And then they do.

3970
03:54:56,000 --> 03:55:03,000
And I think people like Yadkowski are very loud about we are almost certainly going to die.

3971
03:55:03,000 --> 03:55:07,000
And we might, but we also might not.

3972
03:55:07,000 --> 03:55:08,000
I don't really know.

3973
03:55:08,000 --> 03:55:11,000
I would love to just become less confused about this.

3974
03:55:11,000 --> 03:55:15,000
And I remain very concerned about this to be clear.

3975
03:55:15,000 --> 03:55:20,000
But I'm not like 99% chance we're all going to die.

3976
03:55:20,000 --> 03:55:26,000
Yeah, but anything which is an appreciable percentage may as well be the same thing.

3977
03:55:26,000 --> 03:55:28,000
Yeah, pretty much.

3978
03:55:28,000 --> 03:55:29,000
Yeah, it's quite funny.

3979
03:55:29,000 --> 03:55:32,000
I got a lot of pushback on the Robert Miles show.

3980
03:55:32,000 --> 03:55:34,000
People said, oh, I can't believe it.

3981
03:55:34,000 --> 03:55:36,000
You framed him to be a doomer.

3982
03:55:36,000 --> 03:55:42,000
And he himself said in the show, I think about five times we're all going to die.

3983
03:55:42,000 --> 03:55:45,000
And I managed to cut about five.

3984
03:55:45,000 --> 03:55:53,000
Well, I don't exaggerate, but that there is at least two posts on Twitter within 15 minutes of that comment where he said, and we're all going to die.

3985
03:55:53,000 --> 03:55:56,000
So I don't think I don't think I'm being unfa...

3986
03:55:56,000 --> 03:56:00,000
Well, I didn't actually call him a doomer, but he basically is.

3987
03:56:01,000 --> 03:56:03,000
I don't know, man.

3988
03:56:03,000 --> 03:56:04,000
I hate ladles.

3989
03:56:04,000 --> 03:56:07,000
Like, Eliasar is clearly a doomer.

3990
03:56:07,000 --> 03:56:08,000
He's clearly a doomer.

3991
03:56:08,000 --> 03:56:09,000
Yeah.

3992
03:56:09,000 --> 03:56:10,000
Rob is much less doomy than Eliasar.

3993
03:56:10,000 --> 03:56:11,000
Yeah.

3994
03:56:11,000 --> 03:56:12,000
Is Rob a doomer?

3995
03:56:12,000 --> 03:56:13,000
I don't know.

3996
03:56:13,000 --> 03:56:15,000
I didn't call him a doomer.

3997
03:56:15,000 --> 03:56:18,000
But empirically, the data says yes.

3998
03:56:18,000 --> 03:56:20,000
Yeah, I mean, I don't know, man.

3999
03:56:20,000 --> 03:56:23,000
It sounds like you spend too much time reading YouTube comments.

4000
03:56:23,000 --> 03:56:24,000
I do.

4001
03:56:24,000 --> 03:56:25,000
Too much time.

4002
03:56:25,000 --> 03:56:32,000
But notoriously, the least productive use of time possible, apart from hanging out on Twitter, reading AI Flameless.

4003
03:56:32,000 --> 03:56:34,000
Twitter is the worst.

4004
03:56:34,000 --> 03:56:35,000
I know.

4005
03:56:35,000 --> 03:56:36,000
It's so bad.

4006
03:56:36,000 --> 03:56:42,000
I mean, we don't need to go there, but we were having a brief discussion before we started in record.

4007
03:56:42,000 --> 03:56:51,000
Why do you think otherwise intelligent, respectable people behave in that way?

4008
03:56:51,000 --> 03:56:57,000
Impulse control, social validation, it's just kind of fun.

4009
03:56:57,000 --> 03:57:02,000
People aren't very self-aware about how they look or aren't that reflective.

4010
03:57:02,000 --> 03:57:07,000
And Twitter incentivizes you to lack nuance and to be outraged about other people.

4011
03:57:07,000 --> 03:57:10,000
I don't know.

4012
03:57:10,000 --> 03:57:16,000
I am very sad by many Twitter dynamics, including from people who otherwise seem worthy of respect.

4013
03:57:16,000 --> 03:57:17,000
Yes.

4014
03:57:17,000 --> 03:57:18,000
Yes.

4015
03:57:18,000 --> 03:57:19,000
Interesting.

4016
03:57:19,000 --> 03:57:23,000
Look, Neil, this has been an absolute honor.

4017
03:57:23,000 --> 03:57:24,000
Thank you so much.

4018
03:57:24,000 --> 03:57:25,000
It's been extremely fun.

4019
03:57:25,000 --> 03:57:26,000
Yeah, it's been amazing.

4020
03:57:26,000 --> 03:57:27,000
It's been a marathon.

4021
03:57:27,000 --> 03:57:29,000
But thank you so much for joining us today.

4022
03:57:29,000 --> 03:57:33,000
And I really think we've had a great conversation and I know everyone's going to love it.

4023
03:57:33,000 --> 03:57:34,000
So thank you so much.

4024
03:57:34,000 --> 03:57:35,000
Yeah.

4025
03:57:35,000 --> 03:57:37,000
I apologize for all the times I saw you off for philosophizing.

4026
03:57:37,000 --> 03:57:38,000
Oh, no problem.

4027
03:57:38,000 --> 03:57:40,000
It's an honor.

4028
03:57:40,000 --> 03:57:41,000
Yeah.

4029
03:57:41,000 --> 03:57:42,000
All right.

4030
03:57:42,000 --> 03:57:44,000
Thanks for having me on.

