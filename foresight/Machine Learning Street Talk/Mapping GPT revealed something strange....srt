1
00:00:00,000 --> 00:00:07,860
To me, the difference feels like language models start with this highly abstract language

2
00:00:07,860 --> 00:00:08,960
representation.

3
00:00:08,960 --> 00:00:13,720
The system as a whole can try to predict the next token with greater and greater accuracy.

4
00:00:13,720 --> 00:00:18,840
And so the difference it seems is that the adversarial inputs for us tend to look a lot

5
00:00:18,840 --> 00:00:21,880
different than the adversarial examples for LLM.

6
00:00:21,880 --> 00:00:27,600
Once you try and go outside of this sphere of what is meaningful to humans, the possibilities

7
00:00:27,600 --> 00:00:28,600
grow exponentially.

8
00:00:28,800 --> 00:00:34,200
I was recently in Toronto, a beautiful city to film with Co here, and hold on, those videos

9
00:00:34,200 --> 00:00:39,040
will come out very shortly, but around the same time someone shared a paper on our Discord

10
00:00:39,040 --> 00:00:41,600
server and it's called What's the Magic Word?

11
00:00:41,600 --> 00:00:46,640
A Control Theory of Prompting Large Language Models, and it's by Amon Begava and Cameron

12
00:00:46,640 --> 00:00:47,640
Wachowski.

13
00:00:47,640 --> 00:00:53,560
Now, what these guys did is theoretically think about a language model as a dynamical

14
00:00:53,560 --> 00:00:59,440
system and use the lens of control theory to think about the space of reachability.

15
00:00:59,440 --> 00:01:00,440
Why is this important?

16
00:01:00,440 --> 00:01:06,600
Well, language models, we think that they think in language space, this abstract language

17
00:01:06,600 --> 00:01:08,360
space, but they don't.

18
00:01:08,360 --> 00:01:10,400
They actually think using the shogoth.

19
00:01:10,400 --> 00:01:16,960
They think in this very high resolution token space, and it's just this horrible hairy

20
00:01:16,960 --> 00:01:19,080
gnarly mess, right?

21
00:01:19,080 --> 00:01:22,800
No one has created any firewalls for large language models yet.

22
00:01:22,800 --> 00:01:27,680
When companies publish their language models, you know, you just have an API and you just

23
00:01:27,680 --> 00:01:35,000
send tokens up, and I always had the misconception that RLHF or these forms of, you know, kind

24
00:01:35,000 --> 00:01:40,200
of fine-tuning or preference-steering using human feedback, I thought that they significantly

25
00:01:40,200 --> 00:01:42,600
reduced the reachability space.

26
00:01:42,600 --> 00:01:46,800
Because in language models, we do the pre-training, which is distribution matching, and then we

27
00:01:46,800 --> 00:01:52,840
do RLHF, which is mode-seeking, which essentially chops down the reachable space given a prompt

28
00:01:52,840 --> 00:01:56,680
by snipping off all of those trajectories.

29
00:01:56,680 --> 00:01:57,680
Turns out I'm wrong.

30
00:01:57,680 --> 00:02:01,200
The reachability space is much larger than I thought it was, and this is one of the things

31
00:02:01,200 --> 00:02:03,960
that they point out in their paper.

32
00:02:03,960 --> 00:02:05,640
And we kind of knew this, right?

33
00:02:05,640 --> 00:02:09,280
Because we can do adversarial attacks on these language models.

34
00:02:09,280 --> 00:02:13,000
You know, people have observed that if you use sort of human social engineering tricks

35
00:02:13,000 --> 00:02:17,280
on them, like, oh, I'll tip you $500, then it'll do a bit better.

36
00:02:17,280 --> 00:02:21,560
But then there's this whole other sort of perceptual layer, I guess you could call it,

37
00:02:21,560 --> 00:02:26,600
where there's this sort of chaotic regime of adversarial prompts, kind of like hypnosis,

38
00:02:26,600 --> 00:02:32,400
kind of like magic, where if you give it these very strange, very inhuman-looking prompts

39
00:02:32,400 --> 00:02:37,920
that will steer it to this, to just making a certain output extremely likely, right?

40
00:02:37,920 --> 00:02:42,640
And so, to me, it feels really similar to digging into, like, magic and the human perceptual

41
00:02:42,640 --> 00:02:47,360
system just with LLMs, where we're learning about basically the shape or what the nature

42
00:02:47,360 --> 00:02:52,080
of these language models are in terms of how they interact with the world and how their

43
00:02:52,080 --> 00:02:54,080
dynamics really work.

44
00:03:12,640 --> 00:03:31,360
For as long as I can remember, the thing I've wanted more than anything else is to figure

45
00:03:31,360 --> 00:03:33,440
it all out.

46
00:03:33,440 --> 00:03:36,120
I've never shied away from the big questions.

47
00:03:36,120 --> 00:03:37,120
Why are we here?

48
00:03:37,120 --> 00:03:38,120
What are we all doing?

49
00:03:38,120 --> 00:03:44,080
What is this thing we call life that we are all experiencing and one and the same a part

50
00:03:44,080 --> 00:03:45,080
of?

51
00:03:45,080 --> 00:03:50,800
While these questions are all, you know, 30,000 feet in the air, one thing that drew me back

52
00:03:50,800 --> 00:03:52,840
down to earth was the field of engineering.

53
00:03:52,840 --> 00:03:57,960
And when I graduated high school, this had a very strong appeal, a pull, because in engineering

54
00:03:57,960 --> 00:03:59,080
you can design systems.

55
00:03:59,080 --> 00:04:04,280
You can design real, operable things that you can work with and design and understand

56
00:04:04,280 --> 00:04:05,520
how they work.

57
00:04:05,520 --> 00:04:12,440
And so through engineering, perhaps, you can begin to investigate and understand the intricacies

58
00:04:12,440 --> 00:04:13,440
of our world.

59
00:04:13,440 --> 00:04:15,000
That's my hope at least.

60
00:04:15,000 --> 00:04:20,920
So throughout my career, I majored in robotics and very soon I was drawn to the idea of intelligence

61
00:04:20,920 --> 00:04:25,480
because intelligence seems to underlie so much of our world, so much of the design process

62
00:04:25,480 --> 00:04:27,920
of engineering itself.

63
00:04:27,920 --> 00:04:31,360
But what is intelligence and how can we understand it?

64
00:04:31,360 --> 00:04:37,760
It's a question of systems design, really, where we're trying to figure out, okay, we're

65
00:04:37,760 --> 00:04:42,360
humans, we've been in civilization for some time, and we've sort of figured out how to

66
00:04:42,360 --> 00:04:43,360
cooperate with each other.

67
00:04:43,360 --> 00:04:45,200
We obviously have challenges with that.

68
00:04:45,200 --> 00:04:50,520
We're not perfect by any means, but when it comes to adding language models to the mix,

69
00:04:50,520 --> 00:04:54,900
I think it could go both ways, where we could have a world where language models just make

70
00:04:54,900 --> 00:04:59,200
us much dumber, much less capable, maybe make for a worse world.

71
00:04:59,200 --> 00:05:03,220
I think that if we think carefully and we really understand what's going on with the

72
00:05:03,220 --> 00:05:06,920
language models, if we can get a fundamental understanding of them one way or another,

73
00:05:06,920 --> 00:05:10,560
then there's much more hope that maybe we could make a world where our language models

74
00:05:10,560 --> 00:05:15,600
don't just make us smarter, but make our world substantially better and perhaps lead us towards

75
00:05:15,600 --> 00:05:20,560
some greater enlightenment and basically ability to cooperate much better than we were even

76
00:05:20,560 --> 00:05:21,560
before.

77
00:05:21,560 --> 00:05:24,680
Do you think language models are intelligent?

78
00:05:24,680 --> 00:05:26,080
That's a great question.

79
00:05:26,080 --> 00:05:30,360
I think that they're able to simulate intelligence.

80
00:05:30,360 --> 00:05:35,360
One of the really interesting things I'm starting to see now is we are building software abstractions

81
00:05:35,360 --> 00:05:37,840
and controllers on top of language models.

82
00:05:37,840 --> 00:05:42,360
We've been talking about doing this for years, right, because at the end of the day, we have

83
00:05:42,360 --> 00:05:46,840
this idea that we can have this big foundation model and it does all of the things.

84
00:05:46,840 --> 00:05:52,280
It's multimodal, it knows how to reason, and the fact of the matter is that's not really

85
00:05:52,280 --> 00:05:53,280
true.

86
00:05:53,280 --> 00:05:57,640
We control them and I think initially we're seeing frameworks that allow you to do things

87
00:05:57,640 --> 00:06:02,560
like prompt injection, but the next step is thinking of controllers, using control theory

88
00:06:02,560 --> 00:06:04,800
to think about these large language models.

89
00:06:04,800 --> 00:06:07,120
Anyway, I really hope you enjoy the conversation today.

90
00:06:07,120 --> 00:06:11,720
Now, these guys are fascinated not only with controlling language models, but also with

91
00:06:11,720 --> 00:06:15,680
things like AGI, general intelligence, collective intelligence.

92
00:06:15,680 --> 00:06:19,960
It was a really interesting conversation and if you stick around to the end, you can also

93
00:06:19,960 --> 00:06:24,520
hear about the institute that they've set up around AGI technology.

94
00:06:24,520 --> 00:06:25,520
Enjoy the show.

95
00:06:25,520 --> 00:06:26,520
So, my name is Aman.

96
00:06:26,520 --> 00:06:31,240
I'm a PhD student at Caltech studying computation and neural systems.

97
00:06:31,240 --> 00:06:35,360
Recently, we released this paper called What's the Magic Word, Towards the Control Theory

98
00:06:35,360 --> 00:06:39,960
of LLMs, and did that over the last summer with Cameron here.

99
00:06:39,960 --> 00:06:44,080
And yeah, I guess I was here for my undergrad at the University of Toronto doing engineering

100
00:06:44,080 --> 00:06:45,080
science.

101
00:06:45,080 --> 00:06:49,400
I specialized in machine intelligence, sort of been bouncing around between doing machine

102
00:06:49,400 --> 00:06:54,280
learning stuff, applying it to computational biology, trying to understand some stuff in

103
00:06:54,280 --> 00:06:59,960
theoretical neuroscience, and most recently getting back into the LLM space, as well as

104
00:06:59,960 --> 00:07:04,360
trying to study collective intelligence, how very simple machines can come together to

105
00:07:04,360 --> 00:07:08,800
produce a very complicated and beautiful system as a whole.

106
00:07:08,800 --> 00:07:09,800
So, yeah.

107
00:07:09,800 --> 00:07:10,800
Amazing.

108
00:07:10,800 --> 00:07:11,800
And Cameron.

109
00:07:11,800 --> 00:07:12,800
Yeah.

110
00:07:12,800 --> 00:07:16,080
So, my name is Cameron McCoskey, and I went to undergrad here.

111
00:07:16,080 --> 00:07:19,120
I did engineering science as well.

112
00:07:19,120 --> 00:07:23,400
I majored in the robotics engineering option, and now I'm a grad student.

113
00:07:23,400 --> 00:07:29,000
I'm pursuing a master's in electrical computer engineering, advised by Stephen Brown and

114
00:07:29,000 --> 00:07:30,200
Kevin Chiron.

115
00:07:30,200 --> 00:07:35,880
I'm really interested in the deep questions of intelligence, and right now I'm pursuing

116
00:07:35,880 --> 00:07:40,280
research related to morphogenesis and computational models of it.

117
00:07:40,280 --> 00:07:44,320
Like I mentioned last summer, I went down to Caltech, and we wrote this paper on prompt

118
00:07:44,320 --> 00:07:48,240
engineering, well, a control theory of prompt engineering.

119
00:07:48,360 --> 00:07:50,400
I'm excited to get into it.

120
00:07:50,400 --> 00:07:53,360
You folks have just written an incredibly interesting paper.

121
00:07:53,360 --> 00:07:58,880
It was shared in our Discord server, and I saw your presentation, and we'll share a clip

122
00:07:58,880 --> 00:08:02,640
of that in the introduction, but I was intrigued by it straight away.

123
00:08:02,640 --> 00:08:06,760
And what you're doing is you're talking about control theory in respect of large language

124
00:08:06,760 --> 00:08:07,840
models.

125
00:08:07,840 --> 00:08:09,800
Can you explain what that is?

126
00:08:09,800 --> 00:08:10,800
Yeah.

127
00:08:10,800 --> 00:08:13,720
So, I guess I'll get started with control theory.

128
00:08:13,720 --> 00:08:21,000
So back in the day, the late 1800s, this guy Maxwell observed that people were making

129
00:08:21,000 --> 00:08:24,920
these engines, and they were putting these things called governors on them, where if

130
00:08:24,920 --> 00:08:29,280
your car or your machine was experiencing varying loads, you wanted the engine to still

131
00:08:29,280 --> 00:08:31,120
go at the same rate, right?

132
00:08:31,120 --> 00:08:33,080
And people had these things called governors.

133
00:08:33,080 --> 00:08:36,800
There's this fly ball governor, which is this sort of hand tune thing that you put on top

134
00:08:36,800 --> 00:08:40,880
of the engine to try to make sure that it'll be consistent, that it'll do what you want,

135
00:08:40,880 --> 00:08:44,000
that it'll be going at a consistent speed, right?

136
00:08:44,000 --> 00:08:48,880
And people were hand tuning these things, and obviously the engines were working, but

137
00:08:48,880 --> 00:08:53,480
it wasn't very rigorous, and it wasn't very robust, and we didn't have many guarantees

138
00:08:53,480 --> 00:08:55,920
as to how it would end up working in practice.

139
00:08:55,920 --> 00:09:01,680
And so what Maxwell did was he formalized the notion of feedback control, where if you

140
00:09:01,680 --> 00:09:07,200
have this system, even if it's quite complicated, as it turns out, if you feedback the output

141
00:09:07,200 --> 00:09:12,640
of the system into a controller, and try to compute some error metric, and try to correct

142
00:09:12,640 --> 00:09:16,560
for that at every moment in time, it turns out to be a much easier problem to solve from

143
00:09:16,560 --> 00:09:21,160
an engineering perspective than trying to make a perfect system that just does the right

144
00:09:21,160 --> 00:09:22,160
thing off the bat.

145
00:09:22,160 --> 00:09:26,640
So this idea of feedback was really powerful, and sort of gave birth to modern control theory.

146
00:09:26,640 --> 00:09:30,760
And as it turned out, that was a really powerful way to look at systems, building systems,

147
00:09:30,760 --> 00:09:34,840
and controlling them and doing engineering on them, so that they could be robust, do

148
00:09:34,840 --> 00:09:37,440
what we want, and so that we could predict them.

149
00:09:37,440 --> 00:09:43,320
And so when it comes to LLM control theory, what we saw is that we're kind of at a similar

150
00:09:43,320 --> 00:09:47,440
place with language models, where we have these engines, we have these language models

151
00:09:47,440 --> 00:09:51,440
that are very powerful, they can do a lot, they seem to exhibit many interesting attributes

152
00:09:51,440 --> 00:09:55,440
of intelligence, and there's a lot of utility there for people to build further systems

153
00:09:55,440 --> 00:09:57,920
on top of them, and people are already doing that.

154
00:09:57,920 --> 00:10:02,560
But right now, it's sort of this hand tuned, hand crafted prompt engineering that's going

155
00:10:02,560 --> 00:10:09,480
on where it's really hard to get at the fundamentals of what exactly it means to control an LLM

156
00:10:09,480 --> 00:10:11,080
system and how you might do it.

157
00:10:11,080 --> 00:10:12,640
At this point, it's very heuristic.

158
00:10:12,640 --> 00:10:16,120
And so we sort of saw that as an opportunity to try to figure out what would a control

159
00:10:16,120 --> 00:10:20,880
theory for LLMs look like, that hopefully, if we can do it right, we'll give birth to

160
00:10:20,880 --> 00:10:24,920
all of these really, really useful engineering insights, and also just fundamental insights

161
00:10:24,920 --> 00:10:29,640
as to the nature of LLM systems, so that we can better control them, make them reliable

162
00:10:29,640 --> 00:10:34,240
and robust, and be able to do engineering in a more principled manner on them than we're

163
00:10:34,240 --> 00:10:35,480
currently able to.

164
00:10:35,480 --> 00:10:39,160
So that's sort of the general direction and the motivations for our control theory of

165
00:10:39,160 --> 00:10:40,160
language models.

166
00:10:40,160 --> 00:10:41,760
Yeah, that's absolutely fascinating.

167
00:10:41,760 --> 00:10:45,800
I mean, for many years, I've been thinking that we need to have some kind of a controller

168
00:10:45,800 --> 00:10:47,920
for a large language model.

169
00:10:47,920 --> 00:10:52,080
But I guess I'm interested in, first of all, what are the differences between large language

170
00:10:52,080 --> 00:10:54,360
models and something like a steam engine?

171
00:10:54,360 --> 00:10:59,600
And also, with a steam engine, you might be optimizing the efficiency or the performance

172
00:10:59,600 --> 00:11:01,640
or the speed or something like that.

173
00:11:01,640 --> 00:11:06,120
What is it that we are kind of trying to make better with a large language model?

174
00:11:06,120 --> 00:11:10,160
So first off, we'll talk about the differences between large language models and other types

175
00:11:10,160 --> 00:11:13,040
of systems that you might want to control.

176
00:11:13,040 --> 00:11:18,640
Typically a control system, you might first be introduced to control theory in the context

177
00:11:18,640 --> 00:11:22,640
of like, say you're trying to control an engine or something else where the states can be

178
00:11:22,640 --> 00:11:27,920
represented by a set of numbers or set of real numbers that is fixed size.

179
00:11:27,920 --> 00:11:32,560
So perhaps we have an X and a Y coordinate where it's trying to control or a position

180
00:11:32,560 --> 00:11:33,880
in a velocity.

181
00:11:33,880 --> 00:11:40,040
These are common types of systems in scenarios that show up in control theory.

182
00:11:40,040 --> 00:11:44,320
The difference with an LLM, the first major difference is that the token space, the state

183
00:11:44,320 --> 00:11:46,000
space of the system is discreet.

184
00:11:46,000 --> 00:11:50,240
Because we're dealing with tokens, we're dealing with words, we're not operating in the space

185
00:11:50,240 --> 00:11:55,960
of real numbers anymore, and so this introduces some complications and complexities when dealing

186
00:11:55,960 --> 00:11:57,280
with control theory.

187
00:11:57,280 --> 00:12:02,720
The second thing that's really significant is that each time an LLM generates a token

188
00:12:02,720 --> 00:12:06,480
or a user inputs a token, that state space actually expands.

189
00:12:06,480 --> 00:12:08,560
It grows by one token.

190
00:12:08,560 --> 00:12:12,320
And this is very interesting and unique for LLM systems.

191
00:12:12,320 --> 00:12:17,520
On the one hand, this can be exploited to try and get the LLMs to engage in reasoning

192
00:12:17,520 --> 00:12:23,320
or chain of thoughts or kind of take a winding path to the answer you actually want them

193
00:12:23,320 --> 00:12:24,920
to outputs.

194
00:12:24,920 --> 00:12:28,720
But of course, this makes it very difficult for control theory because each new token

195
00:12:28,720 --> 00:12:34,160
you add, the space of possible sentences grows exponentially.

196
00:12:34,160 --> 00:12:39,760
And in language models, the vocabulary size is on the order of 50,000 to 100,000, so this

197
00:12:39,760 --> 00:12:42,560
grows extremely, extremely quickly.

198
00:12:42,560 --> 00:12:44,400
These are some of the challenges.

199
00:12:44,400 --> 00:12:49,240
And with a control theory of say engines, you're trying to optimize the efficiency.

200
00:12:49,240 --> 00:12:53,960
It's a good question what you're trying to optimize for language models.

201
00:12:53,960 --> 00:12:57,920
I think this is definitely a direction for future research.

202
00:12:57,920 --> 00:12:59,360
Do you have any thoughts on this?

203
00:12:59,360 --> 00:13:00,360
Yeah.

204
00:13:00,360 --> 00:13:06,760
I think the thing that we saw was that even very simple questions about how these LLMs operate,

205
00:13:06,760 --> 00:13:11,080
their input-output relationships, when you start to treat them just as a system that

206
00:13:11,080 --> 00:13:15,080
maybe there's an imposed input, like a system prompt, and then you get to pick a subset

207
00:13:15,080 --> 00:13:16,800
of those tokens, right?

208
00:13:16,800 --> 00:13:20,480
When you start to treat it like that, and you just ask a really simple question, like,

209
00:13:20,480 --> 00:13:23,640
let's say that I want it to generate a specific string.

210
00:13:23,640 --> 00:13:26,680
We're not going to be trying to use it to do some intelligent information processing.

211
00:13:26,680 --> 00:13:29,960
I just want to see, can I make it do something?

212
00:13:29,960 --> 00:13:34,440
And what we found and what sort of motivated us to do this is that we really had no idea

213
00:13:34,440 --> 00:13:39,240
when it would be possible or if it was generally possible to make it do anything we want.

214
00:13:39,240 --> 00:13:43,240
Can we just make an LLM system generate any output we desire?

215
00:13:43,240 --> 00:13:46,200
And if the answer is yes, which seems like it's probable.

216
00:13:46,200 --> 00:13:49,800
If you get to have a lot of tokens in your input that you control, it seems reasonable

217
00:13:49,800 --> 00:13:54,520
that you'd be able to probably get it to output a wide variety of at least reasonable

218
00:13:54,520 --> 00:13:57,720
English sentences or linguistically valid sentences.

219
00:13:57,720 --> 00:14:02,600
But the question that we had was, OK, if you have a finite budget for that, would you be

220
00:14:02,600 --> 00:14:03,920
able to get it to do anything?

221
00:14:03,920 --> 00:14:08,320
And what budget of tokens, like how many tokens do you have to be able to control if you want

222
00:14:08,320 --> 00:14:11,520
to be able to make the system do whatever you want?

223
00:14:11,520 --> 00:14:15,200
And that was the initial motivation where it was like, yeah, there are all these high

224
00:14:15,200 --> 00:14:20,440
and mighty sort of questions of how do we make these systems do what we want in an alignment

225
00:14:20,440 --> 00:14:21,440
sense?

226
00:14:21,440 --> 00:14:24,480
How do we make them do what we want in the sense of cooperating towards some information

227
00:14:24,480 --> 00:14:25,880
processing objective?

228
00:14:25,880 --> 00:14:29,080
But we realized that these really, really simple questions are just, OK, you have an input

229
00:14:29,080 --> 00:14:32,440
that you get to partially control and you're trying to make it do something.

230
00:14:32,440 --> 00:14:35,480
That question was completely unanswered and we were sort of taking bets on it.

231
00:14:35,520 --> 00:14:37,560
I think Cameron was the one who started to make bets.

232
00:14:37,560 --> 00:14:40,000
He was like, I bet like $10 that we can get this done.

233
00:14:40,000 --> 00:14:42,960
We can make it emit this output within five tokens.

234
00:14:42,960 --> 00:14:47,320
And that was really the initial motivation where it was like, even the feed forward dynamics

235
00:14:47,320 --> 00:14:49,680
of this system are really mysterious.

236
00:14:49,680 --> 00:14:53,960
And getting a grip on those, it seems like that's a really strong way to start building

237
00:14:53,960 --> 00:14:59,160
up a fundamental control theory and a really strong understanding of these LLM systems where

238
00:14:59,160 --> 00:15:02,920
in control theory at least, when you start to really deeply understand just a single

239
00:15:02,960 --> 00:15:07,640
system with its own dynamics and how the input-output relationships work, what the reachable sets

240
00:15:07,640 --> 00:15:12,560
look like, how controllable it is, then when it comes to building more complicated systems

241
00:15:12,560 --> 00:15:18,280
where maybe you have a more complicated objective, maybe you have interacting systems, when you

242
00:15:18,280 --> 00:15:21,120
really understand the fundamentals, it makes that way easier.

243
00:15:21,120 --> 00:15:25,400
And so the example in classical control theory is that you observe that if you couple a bunch

244
00:15:25,400 --> 00:15:29,480
of linear controllers and linear systems together, what you get is just one bigger linear system

245
00:15:29,480 --> 00:15:30,920
and all of the same stuff applies.

246
00:15:30,920 --> 00:15:36,280
So what we were hoping is that by starting to answer this really simple question of just,

247
00:15:36,280 --> 00:15:38,360
okay, how much can we control this?

248
00:15:38,360 --> 00:15:40,440
What does the reachability of these LLMs look like?

249
00:15:40,440 --> 00:15:42,000
We're really hoping to build that up.

250
00:15:42,000 --> 00:15:45,200
And to me, it feels like we're kind of doing our homework where in engineering, we had

251
00:15:45,200 --> 00:15:47,000
to take all these classes in control.

252
00:15:47,000 --> 00:15:49,680
And that was sort of our homework to be able to go into the world.

253
00:15:49,680 --> 00:15:54,600
And if it ever comes time to build some electromechanical system and get a PID controller in there,

254
00:15:54,600 --> 00:15:58,440
now we've done our homework so we can have a sense what to expect, how we could do engineering

255
00:15:58,440 --> 00:15:59,440
on it.

256
00:15:59,440 --> 00:16:01,640
So that's really where I feel like it's at.

257
00:16:01,640 --> 00:16:05,360
And I think this is a really promising way to try to get a really fundamental understanding

258
00:16:05,360 --> 00:16:07,640
of what's going on with these language model systems.

259
00:16:07,640 --> 00:16:08,640
Amazing.

260
00:16:08,640 --> 00:16:12,600
So in a second, we're going to introduce this concept of reachability.

261
00:16:12,600 --> 00:16:17,040
But I've thought about this because I've had a couple of days to reflect on this.

262
00:16:17,040 --> 00:16:20,920
And my intuition, intuitions just seem a little bit mixed up.

263
00:16:20,920 --> 00:16:25,160
So I've interviewed Nicholas Carlini, for example, and he's done lots of work, you know, building

264
00:16:25,160 --> 00:16:30,560
on adversarial examples and writing algorithms to find adversarial examples.

265
00:16:30,560 --> 00:16:34,520
And we know that neural networks are not robust.

266
00:16:34,520 --> 00:16:40,160
You can quite easily perturb, let's say, an input image in a vision model.

267
00:16:40,160 --> 00:16:43,960
And if it's a classifier, you can make it pretty much say anything with a very small

268
00:16:43,960 --> 00:16:45,240
perturbation.

269
00:16:45,240 --> 00:16:48,920
And that's kind of the same thing as what you mean as reachability.

270
00:16:48,920 --> 00:16:53,440
It's this idea to kind of reach into the state space and make it do something quite

271
00:16:53,480 --> 00:16:55,680
weird outside of what you would expect.

272
00:16:55,680 --> 00:17:01,040
Now, for some reason, I had the intuition, and I now think I'm wrong, that LLMs do,

273
00:17:01,040 --> 00:17:05,960
you know, I didn't think they had this problem, but they do have this problem.

274
00:17:05,960 --> 00:17:10,680
And you introduced this really interesting, I guess it started out as a thought experiment

275
00:17:10,680 --> 00:17:12,480
and you coded it into a game.

276
00:17:12,480 --> 00:17:14,240
And it's the Roger Federer game.

277
00:17:14,240 --> 00:17:15,840
I think that's quite instructive.

278
00:17:15,840 --> 00:17:17,320
So can you tell us about that?

279
00:17:17,320 --> 00:17:18,000
Yeah, for sure.

280
00:17:18,000 --> 00:17:23,120
So one of the earliest examples that we were thinking about was just a simple example

281
00:17:23,120 --> 00:17:25,560
of you have this state sequence that's imposed.

282
00:17:25,560 --> 00:17:26,480
You don't get to pick it.

283
00:17:26,480 --> 00:17:28,560
It says, Roger Federer is the.

284
00:17:28,560 --> 00:17:32,200
And then the next thing that you want it to say, the thing that you want the LLM to

285
00:17:32,200 --> 00:17:34,320
generate, is the word the greatest.

286
00:17:34,320 --> 00:17:36,720
So you want to say, Roger Federer is the greatest.

287
00:17:36,720 --> 00:17:41,480
And you're trying to pick out a prompt that comes before then that will steer the system

288
00:17:41,480 --> 00:17:42,760
so that it'll output that.

289
00:17:42,760 --> 00:17:46,600
So we're basically asking the question, you know, is this word in the reachable set

290
00:17:46,600 --> 00:17:51,000
of outputs, given that we have some finite control over the input, where the goal of

291
00:17:51,000 --> 00:17:54,360
the game is to, for one, get it to actually output the right answer, which is the greatest,

292
00:17:54,360 --> 00:17:58,200
which is a fairly reasonable English thing to say.

293
00:17:58,200 --> 00:18:03,960
And the metric that we use to grade how well you're doing on that is basically how efficiently

294
00:18:03,960 --> 00:18:08,680
you're able to do control, where in the original control theory, this idea of efficient or

295
00:18:08,680 --> 00:18:10,240
optimal control is really important.

296
00:18:10,240 --> 00:18:14,840
You have this linear quadratic regularization idea where you're like, I have only a finite

297
00:18:14,840 --> 00:18:16,880
energy budget for the signal I put in.

298
00:18:16,880 --> 00:18:22,160
Similarly, with language models, what we're interested in is the minimal length of the

299
00:18:22,160 --> 00:18:25,920
control input that will steer the model successfully to what you want it to do.

300
00:18:25,920 --> 00:18:29,880
And it turns out that the game is actually very challenging, at least with this GPT-2

301
00:18:29,880 --> 00:18:33,120
model, which is the one that we're using right now, since it's just running out of a desktop

302
00:18:33,120 --> 00:18:35,200
on my desk at home.

303
00:18:35,200 --> 00:18:39,880
And so, yeah, there's this game that you can play, we can link it where you get to put

304
00:18:39,880 --> 00:18:43,800
in a prompt to the system, and it'll come back to you and say, OK, you got the answer

305
00:18:43,800 --> 00:18:49,000
right, or you got the answer wrong, as well as basically your error on that, so your cross-entropy

306
00:18:49,000 --> 00:18:52,360
loss on getting the correct output, the desired output.

307
00:18:52,360 --> 00:18:56,280
And the game is to basically get the shortest prompt that will steer the model to the desired

308
00:18:56,280 --> 00:18:57,360
output.

309
00:18:57,360 --> 00:19:01,000
And it's actually quite challenging with GPT-2, where I think only four people, including

310
00:19:01,000 --> 00:19:05,320
Cameron, and then my friend Michael Zellinger, who we had made this thing called FangCheck4,

311
00:19:05,320 --> 00:19:08,880
which is this resume checker that uses language models to basically predict your probability

312
00:19:08,880 --> 00:19:10,800
of getting into a Fang company.

313
00:19:11,320 --> 00:19:15,320
I think those two were the only people who actually ended up getting it right, and it

314
00:19:15,320 --> 00:19:17,160
turns out to be very difficult.

315
00:19:17,160 --> 00:19:22,180
So that game was sort of a codified sort of interactive version of our initial motivations

316
00:19:22,180 --> 00:19:25,320
for this, where it was like, wow, this really simple question that seems like there should

317
00:19:25,320 --> 00:19:26,320
be an easy answer.

318
00:19:26,320 --> 00:19:28,240
I mean, if there is an easy answer, I'd love to know.

319
00:19:28,240 --> 00:19:34,240
But the simple question really leads to a problem that's quite difficult to solve, and

320
00:19:34,240 --> 00:19:38,920
we really have poor insight on, and we're really just trying to get that insight together

321
00:19:38,920 --> 00:19:40,080
to understand what's going on there.

322
00:19:40,360 --> 00:19:44,160
And just to jump off that point as well, I think one of the reasons why this game in

323
00:19:44,160 --> 00:19:50,680
particular is difficult is because we're using GPT-2, and Roger Federer is the blank.

324
00:19:50,680 --> 00:19:54,640
You would think greatest would be rated pretty high, but GPT-2, I guess it's trained on lots

325
00:19:54,640 --> 00:19:55,800
of fill-in-the-blank tasks.

326
00:19:55,800 --> 00:19:59,060
It tends to output just a set of underscores quite often.

327
00:19:59,060 --> 00:20:03,200
To comment on your intuition you've mentioned before on whether language models have this

328
00:20:03,200 --> 00:20:06,920
adversarial property, one thing that was really interesting when we were doing some of our

329
00:20:06,920 --> 00:20:10,440
initial work was this technique of soft prompting.

330
00:20:10,440 --> 00:20:16,480
So soft prompting, instead of selecting discrete tokens, which we want to adversarial change

331
00:20:16,480 --> 00:20:22,400
the model's behavior with, soft prompting modifies the embedding vectors directly.

332
00:20:22,400 --> 00:20:27,520
So you have a lot more fine-grained control over the outputs, and it turns out when you

333
00:20:27,520 --> 00:20:33,040
soft prompt, when you adversarily attack not the tokens themselves, but the embedding

334
00:20:33,040 --> 00:20:37,760
vectors, you can send the cross-entropy law straight to zero for whatever token you want

335
00:20:37,760 --> 00:20:41,760
with a very tiny adjustment in these embedding vectors.

336
00:20:41,760 --> 00:20:43,080
So this is very interesting.

337
00:20:43,080 --> 00:20:48,000
This points to the fact that the real challenge with controllability is not necessarily that

338
00:20:48,000 --> 00:20:53,120
there aren't adversarial inputs for language models, but just it's very hard to search

339
00:20:53,120 --> 00:20:56,640
this exponential space of discrete prompts.

340
00:20:56,640 --> 00:21:01,800
Yeah, and so I guess there are many degrees of freedom in any deep learning model.

341
00:21:01,840 --> 00:21:03,240
It's a very highly dimensional model.

342
00:21:03,240 --> 00:21:07,600
There are many degrees of freedom, and I'm trying to understand my intuition.

343
00:21:07,600 --> 00:21:15,240
So it's trained with a softmax, for example, and certainly when you do temperature sampling,

344
00:21:15,240 --> 00:21:19,320
the likelihood is that you're only going to get the top few tokens.

345
00:21:19,320 --> 00:21:24,240
I mean, if you look at the distribution of the probability, it's almost certainly this

346
00:21:24,240 --> 00:21:27,200
one or this one, and then it just tails off very, very quickly.

347
00:21:27,200 --> 00:21:32,640
And I assume that inductive prior was quite deliberate, really, to increase the statistical

348
00:21:32,640 --> 00:21:35,080
tractability of the model.

349
00:21:35,080 --> 00:21:40,800
But underneath that, in the embedding space, it's not a shell at all, even though there's

350
00:21:40,800 --> 00:21:45,760
some low-level surface of embeddings, and you can traverse this.

351
00:21:45,760 --> 00:21:46,760
Right.

352
00:21:46,760 --> 00:21:51,560
So initially, you might think that this embedding space is a very rich representation of the

353
00:21:51,560 --> 00:21:53,440
meaning of different words.

354
00:21:53,440 --> 00:21:58,280
And certainly, if you do word-to-vec or take a PCA analysis of the embedding vectors for

355
00:21:58,280 --> 00:22:02,240
any large language model, you'll find something that roughly corresponds to the meaning.

356
00:22:02,240 --> 00:22:06,360
I mean, words that mean similar things are attached more closely together.

357
00:22:06,360 --> 00:22:10,160
But this opens the question, if you were to interpolate between two similar words, take

358
00:22:10,160 --> 00:22:14,320
the embedding vector that is halfway between, would you get the halfway in between word,

359
00:22:14,320 --> 00:22:16,800
or would you get something that's nonsense, right?

360
00:22:16,800 --> 00:22:21,840
And I think what you find by these kinds of soft-prompting experiments, by directly

361
00:22:21,840 --> 00:22:27,120
manipulating the embedding vectors, is that the embedding space is actually extremely

362
00:22:27,120 --> 00:22:33,960
non-convex, in the sense that by interpolating, you don't just get an average value between

363
00:22:33,960 --> 00:22:34,960
the two of them.

364
00:22:34,960 --> 00:22:35,960
Yeah.

365
00:22:35,960 --> 00:22:40,360
I don't know if this is best to get into, but one of the techniques we were trying to

366
00:22:40,360 --> 00:22:43,280
use is this technique called gumball softmax.

367
00:22:43,280 --> 00:22:49,160
So instead of a discrete search over the token space, one thing you can do is it's kind of

368
00:22:49,160 --> 00:22:54,680
like the repair metrization trick for variation autoencoders, but it works for a categorical

369
00:22:54,680 --> 00:22:56,280
distribution.

370
00:22:56,280 --> 00:23:02,360
And so you can use this trick, and it essentially works by kind of interpolating between embeddings.

371
00:23:02,360 --> 00:23:06,280
But it actually was very difficult to get to converge and did not even close to rival

372
00:23:06,280 --> 00:23:09,160
the performance of GCG.

373
00:23:09,160 --> 00:23:14,960
My intuition is that when you take a data point off the manifold, because these neural

374
00:23:14,960 --> 00:23:17,920
networks, they do learn a manifold of language.

375
00:23:17,920 --> 00:23:22,640
I thought if you take a data point off the manifold, it would cause some kind of mode

376
00:23:22,640 --> 00:23:23,640
collapse.

377
00:23:23,640 --> 00:23:28,160
It would just cause the network to become chaotic and go crazy.

378
00:23:28,160 --> 00:23:30,880
But apparently that's not the case.

379
00:23:30,880 --> 00:23:31,880
Can it recover?

380
00:23:31,880 --> 00:23:36,320
It's almost like if you put a bunch of tokens in which are just really weird, and then you

381
00:23:36,320 --> 00:23:39,760
just carry on, it's like the language model recovers.

382
00:23:39,760 --> 00:23:42,840
It finds coherence again, and then it just carries on.

383
00:23:42,840 --> 00:23:43,840
Yeah.

384
00:23:44,520 --> 00:23:48,360
It's honestly a really hard question to answer, where in different regimes, we've noticed

385
00:23:48,360 --> 00:23:54,160
different things where if you choose this adversarial prompt so that basically these

386
00:23:54,160 --> 00:23:59,920
prompt optimization algorithms all work in the same way where you're trying to maximize

387
00:23:59,920 --> 00:24:04,400
the likelihood of some desired string, and then you're able to modify some input.

388
00:24:04,400 --> 00:24:10,400
And so depending on how you choose that, you can do the optimization so that the model

389
00:24:10,400 --> 00:24:12,640
will output some gibberish.

390
00:24:13,000 --> 00:24:17,120
It seems like depending on the model, depending on the sampling techniques, I've seen it go

391
00:24:17,120 --> 00:24:21,240
both ways where sometimes it'll recover after that, sometimes it'll start generating reasonable

392
00:24:21,240 --> 00:24:26,040
coherent text, and other times it seems like it'll continue to generate some random stuff.

393
00:24:26,040 --> 00:24:29,000
It'll kind of be in this outer distribution mode.

394
00:24:29,000 --> 00:24:34,080
I think that that's one of the reasons that I think that these adversarial examples, studying

395
00:24:34,080 --> 00:24:37,440
them as well as this control theory stuff is really important where it's like, yeah,

396
00:24:37,440 --> 00:24:42,600
if you have a system in the real world where tokens are coming in, you're actually processing

397
00:24:42,760 --> 00:24:47,360
them from real users, you don't have total control, but the user is the one who's giving

398
00:24:47,360 --> 00:24:48,360
the control input.

399
00:24:48,360 --> 00:24:52,480
You want to make sure that your system is sort of robust to that, where there's a lot

400
00:24:52,480 --> 00:24:56,480
of really complicated interactions as it turns out between, for instance, the tokenizer

401
00:24:56,480 --> 00:25:00,520
and the incoming strings, where when you do this prompt optimization, sometimes it'll

402
00:25:00,520 --> 00:25:04,400
come out with a sequence of tokens that if you convert it to a string and then convert

403
00:25:04,400 --> 00:25:09,040
it back to tokens, it'll actually be very different, which we ran into with this game

404
00:25:09,040 --> 00:25:10,480
where I was like, oh, I'm going to cheat at this game.

405
00:25:10,520 --> 00:25:11,800
I want to be the top prompter.

406
00:25:12,000 --> 00:25:15,200
So I'm just going to use some of the algorithms that we had from our GitHub repository, the

407
00:25:15,200 --> 00:25:18,640
magic words GitHub repository to basically optimize these prompts.

408
00:25:18,640 --> 00:25:22,440
But then when you convert it back to a string, then it turns out not to work as well.

409
00:25:22,720 --> 00:25:27,080
And so, yeah, I think that answering that question and seeing when is it that the model

410
00:25:27,080 --> 00:25:30,760
will actually be able to recover, is it a function of how big the model is, are bigger

411
00:25:30,760 --> 00:25:34,480
models better at recovering, or is it the case that bigger models are maybe more

412
00:25:34,480 --> 00:25:39,480
controllable, maybe you can shift these models into this weird sort of, sorry, just

413
00:25:39,480 --> 00:25:44,320
on the mic, but this sort of out of distribution regime where they're generating this seemingly

414
00:25:44,320 --> 00:25:47,960
random output based on seemingly random input.

415
00:25:48,280 --> 00:25:52,720
And so, yeah, I think that that question is really, really important and is one that is, I

416
00:25:52,720 --> 00:25:56,000
think, well addressed through considering them as systems, which is sort of the thesis

417
00:25:56,000 --> 00:25:56,560
of this paper.

418
00:25:56,560 --> 00:26:02,040
And we're trying to get a grip on what exactly the case is, you know, is it going to be

419
00:26:02,040 --> 00:26:05,200
able to recover, is that a consistent behavior, or is it not?

420
00:26:05,960 --> 00:26:10,200
There's this sort of weird recurrence relationship between the prompt and then the

421
00:26:10,200 --> 00:26:13,320
stuff that the language model generates, and then the stuff that's generated in the

422
00:26:13,320 --> 00:26:17,160
future, where in effect, you know, you're able to pick a prompt, and then the language

423
00:26:17,160 --> 00:26:18,560
model will generate some more text.

424
00:26:18,760 --> 00:26:21,200
But then that text becomes sort of part of the prompt as well.

425
00:26:21,440 --> 00:26:26,400
So it seems like maybe there could be these sort of degenerate states where if you start

426
00:26:26,400 --> 00:26:30,640
with this seed of chaos, it'll basically branch out and the future strings that it

427
00:26:30,640 --> 00:26:32,960
generates is going to prompt it into being more and more chaotic.

428
00:26:33,280 --> 00:26:36,640
And that's basically stability analysis or sensitivity analysis.

429
00:26:36,880 --> 00:26:41,240
And there's all this like rich vocabulary and all of these people who have spent

430
00:26:41,280 --> 00:26:45,800
basically hundreds of years thinking about these concepts for both, you know, discrete

431
00:26:45,800 --> 00:26:50,000
and continuous dynamical systems that we get to build on top of and basically use

432
00:26:50,000 --> 00:26:52,640
their insights to understand, you know, what does it mean?

433
00:26:52,680 --> 00:26:53,840
What does stability really mean?

434
00:26:53,840 --> 00:26:58,160
We can just draw those definitions in, apply them to our generalized form of a

435
00:26:58,160 --> 00:27:00,040
system, a language model system.

436
00:27:00,280 --> 00:27:03,960
And I think that's why the control theoretic aspect is exciting, where you can

437
00:27:03,960 --> 00:27:07,200
actually ask these questions in a very concrete and reasonable way.

438
00:27:07,520 --> 00:27:11,840
And the best part is that people haven't really been using these, these ideas or

439
00:27:11,840 --> 00:27:14,720
using this vocabulary to describe the questions that we're trying to answer.

440
00:27:14,920 --> 00:27:18,840
And so most of these things, if you just spin up, you know, a small GPU and test

441
00:27:18,840 --> 00:27:21,640
some stuff out with a seven billion parameter model, you're actually doing new

442
00:27:21,640 --> 00:27:24,400
research and it's actually some useful research, in my opinion, where you're

443
00:27:24,600 --> 00:27:28,000
getting a sense of the control theoretic properties of language models.

444
00:27:28,240 --> 00:27:31,200
And to me, that felt like the most exciting thing here.

445
00:27:31,240 --> 00:27:34,560
The open questions are the most exciting part of the paper to me, where we've

446
00:27:34,560 --> 00:27:38,720
taken a stab at basically the, you know, empirical study of controllability by

447
00:27:38,720 --> 00:27:41,840
sampling these wiki tech sequences, seeing if we can control the next

448
00:27:41,840 --> 00:27:46,160
token, the next few tokens, as well as some sort of theoretical results on

449
00:27:46,160 --> 00:27:47,680
self-attention and its controllability.

450
00:27:47,920 --> 00:27:51,360
But then all of these open questions emerged just because we're now

451
00:27:51,360 --> 00:27:54,360
framing it as a system and people for hundreds of years have been thinking

452
00:27:54,360 --> 00:27:58,720
really, really deeply about how you understand systems when they're used in

453
00:27:58,720 --> 00:28:01,200
the real world and you have this sort of finite control of them.

454
00:28:02,360 --> 00:28:03,360
Yeah, that's really interesting.

455
00:28:03,360 --> 00:28:06,840
I mean, I suppose I'm pointing out the obvious here, but these are auto

456
00:28:06,840 --> 00:28:07,680
regressive models.

457
00:28:07,720 --> 00:28:11,400
So the answer gets kind of fed back into the prompt and then we rinse and

458
00:28:11,400 --> 00:28:14,680
repeat, which means you can model them as dynamical systems.

459
00:28:15,000 --> 00:28:18,680
And that is in stark contrast to something like a vision classifier where, you

460
00:28:18,680 --> 00:28:20,480
know, there's just an input and an output and that's it.

461
00:28:20,520 --> 00:28:21,480
That that that's the end.

462
00:28:21,760 --> 00:28:26,320
So now you can get the system into this kind of corrupted state where, you

463
00:28:26,320 --> 00:28:28,480
know, you get divergence and decoherence.

464
00:28:28,480 --> 00:28:32,080
And as you said, that that that could be analyzed with stability analysis.

465
00:28:32,360 --> 00:28:33,440
But I find that fascinating.

466
00:28:33,440 --> 00:28:36,640
But we should just go back quickly to your Roger Federer example.

467
00:28:37,000 --> 00:28:41,280
So I'm interested in the different ways that we could go about this.

468
00:28:41,520 --> 00:28:45,760
So the humans were kind of using language and language are a bunch of

469
00:28:45,760 --> 00:28:47,480
mimetically shared cognitive tools.

470
00:28:47,680 --> 00:28:51,120
And they were saying things like, you know, you know, basketball is a great

471
00:28:51,120 --> 00:28:52,880
and, you know, Joe blogs is great.

472
00:28:53,240 --> 00:28:54,440
Roger Federer is great.

473
00:28:54,880 --> 00:28:58,080
And it wasn't very parsimonious, but it but it worked.

474
00:28:58,520 --> 00:29:02,720
And then, you know, another approach that that that you spoke about is you

475
00:29:02,720 --> 00:29:06,360
could just make a Python program and you can just let's try a neighborhood

476
00:29:06,360 --> 00:29:08,240
greedy search one token at a time.

477
00:29:08,240 --> 00:29:13,360
So we find the nearest token and then we find the second nearest token until

478
00:29:13,360 --> 00:29:14,640
we find the adversarial attack.

479
00:29:15,160 --> 00:29:17,920
Or we could do like a low level gradient search.

480
00:29:18,160 --> 00:29:20,400
And then we can find something really weird and wonderful.

481
00:29:20,400 --> 00:29:24,280
There might be some esoteric characters that just make it go bananas.

482
00:29:24,600 --> 00:29:28,440
But these are three very, very different levels of talking to a language model.

483
00:29:28,600 --> 00:29:32,880
The word on the street is that language models are a new form of programming

484
00:29:33,200 --> 00:29:37,480
that you can just say what you want to do using English language and so on.

485
00:29:38,640 --> 00:29:42,760
And language models certainly seem to incorporate that structure.

486
00:29:43,040 --> 00:29:47,240
But the language models themselves are just an inscrutable, you know,

487
00:29:47,360 --> 00:29:49,560
set of, of, of neurons, right?

488
00:29:49,560 --> 00:29:51,760
And, and weights and matrices and so on.

489
00:29:52,200 --> 00:29:55,240
So there's some, there's a kind of higher resolution

490
00:29:55,240 --> 00:29:57,480
shog off going on underneath the covers.

491
00:29:57,520 --> 00:29:59,280
That's more or less the picture I have.

492
00:29:59,280 --> 00:30:03,640
We have this interface where we can speak to the language model using language.

493
00:30:04,000 --> 00:30:09,200
And if we set up a conversation with a language model where we have different labels,

494
00:30:09,200 --> 00:30:13,760
you know, chat, GBT says this, Cameron says this, and, you know,

495
00:30:13,760 --> 00:30:17,080
you engage in a conversation because it is seen enough conversations and it's

496
00:30:17,080 --> 00:30:20,880
training data, then it's able to play along very fine.

497
00:30:21,200 --> 00:30:24,480
What's going on under the hood, of course, like you say, it's very inscrutable.

498
00:30:24,800 --> 00:30:27,640
It's very difficult to really probe and understand.

499
00:30:28,120 --> 00:30:31,320
There are certain techniques in the interpretability literature,

500
00:30:31,520 --> 00:30:35,400
but I don't think as a whole it's we're even remotely close to having

501
00:30:35,400 --> 00:30:37,480
a complete understanding of how these systems work.

502
00:30:37,760 --> 00:30:41,600
But that's one of the reasons why I think that control theory is a great way

503
00:30:41,600 --> 00:30:43,960
to kind of break in and see what's going on.

504
00:30:43,960 --> 00:30:47,920
Because if you just look at the system's input and output characteristics,

505
00:30:48,240 --> 00:30:51,680
you can really gain a lot of insight into the nature of these systems.

506
00:30:52,280 --> 00:30:57,600
One guiding principle in my life doing engineering and trying to learn

507
00:30:57,600 --> 00:31:00,560
about the world has been this quote by Richard Feynman.

508
00:31:00,560 --> 00:31:04,080
It's very popular. What I cannot create, I cannot understand.

509
00:31:04,720 --> 00:31:06,680
And yet today we find ourselves in this situation

510
00:31:06,680 --> 00:31:10,480
with language models where we have these incredibly complex systems we built

511
00:31:10,480 --> 00:31:12,480
and yet we can't really get into them.

512
00:31:12,800 --> 00:31:17,560
So to extend this to today, what I would say is what I cannot control,

513
00:31:17,600 --> 00:31:18,600
I cannot understand.

514
00:31:19,560 --> 00:31:24,600
The way I think about it is it's almost like you want the language model

515
00:31:24,600 --> 00:31:27,200
to be a high level controlled, robust interface.

516
00:31:27,720 --> 00:31:31,760
And it's almost like we're all Marvel characters

517
00:31:31,800 --> 00:31:35,000
and we can give secret hidden codes.

518
00:31:35,160 --> 00:31:36,000
It's like me now.

519
00:31:36,000 --> 00:31:39,160
Imagine if I could just through telepathy control your behavior

520
00:31:39,440 --> 00:31:41,960
and anyone can do that with a language model.

521
00:31:41,960 --> 00:31:45,480
They can just put weird tokens in and they can manipulate its behavior.

522
00:31:46,000 --> 00:31:49,280
And there's there's no there's nothing stopping you.

523
00:31:49,280 --> 00:31:50,280
There's no firewall.

524
00:31:50,280 --> 00:31:53,320
I feel like the this kind of harkens to why we call the paper.

525
00:31:53,320 --> 00:31:57,560
What's the magic word where, you know, the initial reason was just that,

526
00:31:57,640 --> 00:32:00,640
you know, it's almost like the LLM is asking you if you wanted to do something.

527
00:32:00,800 --> 00:32:01,680
What's the magic word?

528
00:32:01,680 --> 00:32:04,400
Like, what's the this key, this weird control prompt

529
00:32:04,400 --> 00:32:05,800
that will just make it do the right thing?

530
00:32:05,800 --> 00:32:09,320
But I think more generally, you know, I used to be into magic when I was a kid.

531
00:32:09,320 --> 00:32:11,960
I had to jog at a restaurant doing, you know, card tricks for the patrons

532
00:32:11,960 --> 00:32:13,360
while they waited for their food.

533
00:32:13,360 --> 00:32:19,080
And what magic is, is basically you're playing tricks on the human perceptual system

534
00:32:19,080 --> 00:32:22,200
where there are all of these sort of inductive biases that the human

535
00:32:22,320 --> 00:32:25,760
perceptual system has where, you know, for instance, if I move something

536
00:32:25,760 --> 00:32:28,800
and I look at it, you naturally will tend to follow that my gaze

537
00:32:28,800 --> 00:32:30,920
and what is moving is generally more salient.

538
00:32:30,920 --> 00:32:33,040
And so then I can like do something over here with my other hand,

539
00:32:33,040 --> 00:32:34,320
like take something out of my pocket.

540
00:32:34,320 --> 00:32:37,320
And then when I display it, they'll be like, oh, my God, where did that come from?

541
00:32:37,320 --> 00:32:40,600
Right. And what we're discovering, I think, is a sort of similar thing

542
00:32:40,600 --> 00:32:43,800
with language models where, for one, you know, people have observed

543
00:32:43,800 --> 00:32:47,160
that if you use sort of human social engineering tricks on them, like,

544
00:32:47,280 --> 00:32:50,080
oh, I'll tip you $500, then, you know, it'll do a bit better.

545
00:32:50,680 --> 00:32:54,560
But then there's this whole other sort of perceptual layer, I guess you could call it

546
00:32:54,760 --> 00:32:58,640
where there's this sort of chaotic regime of adversarial prompts,

547
00:32:58,680 --> 00:33:03,400
kind of like hypnosis, kind of like magic, where if you give it these very

548
00:33:03,400 --> 00:33:07,520
strange, very inhuman looking prompts that will steer it to this,

549
00:33:07,640 --> 00:33:10,920
to just making a certain output, extremely likely, right?

550
00:33:11,240 --> 00:33:15,400
And so to me, it feels really similar to digging into like magic

551
00:33:15,400 --> 00:33:18,280
and the human perceptual system, just with LLMs, where we're learning about

552
00:33:18,280 --> 00:33:23,000
basically the shape or the what the nature of these language models are in terms

553
00:33:23,000 --> 00:33:26,880
of how they interact with the world and how they, how their dynamics really work.

554
00:33:27,080 --> 00:33:31,040
And I think that it's very sensible that the control

555
00:33:31,040 --> 00:33:35,120
theoretic perspective would be useful for this, where in classical control

556
00:33:35,120 --> 00:33:38,960
theory, trying to control these systems actually taught us a lot about

557
00:33:38,960 --> 00:33:41,080
the nature of systems, both linear and nonlinear.

558
00:33:41,320 --> 00:33:43,960
And I think that we have a very similar opportunity here where we're really

559
00:33:43,960 --> 00:33:47,760
discovering what is the nature of these language models in terms of control,

560
00:33:47,920 --> 00:33:51,000
where these questions don't emerge quite as naturally and don't have

561
00:33:51,000 --> 00:33:54,240
quite as natural of an answer when you're just thinking about them as a sort

562
00:33:54,240 --> 00:33:57,400
of probability distribution over text, thinking about them in terms

563
00:33:57,400 --> 00:34:00,400
of being systems that have inputs and outputs and these trajectories and the

564
00:34:00,400 --> 00:34:03,840
like actually really does change the kinds of questions that you end up

565
00:34:04,080 --> 00:34:06,800
being able to answer and the kind of understanding that you get about the

566
00:34:06,800 --> 00:34:09,680
nature of the system itself, which to me is one of the most exciting things.

567
00:34:09,920 --> 00:34:12,000
So yeah, that's so interesting.

568
00:34:12,000 --> 00:34:17,200
The magic example thing, I think we, we think that we are robust, but we're not.

569
00:34:17,200 --> 00:34:20,080
Maybe we're system two robust, but we're not system one robust.

570
00:34:20,080 --> 00:34:23,720
And if you look in the animal kingdom, there are so many examples of, you

571
00:34:23,720 --> 00:34:27,040
know, like a hen, if you make the right kind of clucking noise, the mother will

572
00:34:27,040 --> 00:34:29,040
think that you're that you're the chick.

573
00:34:29,480 --> 00:34:32,680
So it's really, really weird, actually.

574
00:34:32,680 --> 00:34:36,080
And Keith gave me this example of, I think it was from science fiction, that

575
00:34:36,320 --> 00:34:37,600
there's a hypothetical image.

576
00:34:37,600 --> 00:34:40,280
And if you look at the image, every single person goes into a coma.

577
00:34:40,640 --> 00:34:43,760
And what's interesting about that is it's a kind of, you know, population

578
00:34:43,760 --> 00:34:47,240
level adversarial example rather than an individual adversarial example.

579
00:34:47,520 --> 00:34:50,480
But then it gets into the question of, you know, how can we use this

580
00:34:50,480 --> 00:34:53,040
control theoretic approach to robustify models?

581
00:34:53,040 --> 00:34:55,800
Cause we're talking about building a genetic LLMs.

582
00:34:56,120 --> 00:34:59,640
And part of the thing I'm trying to get my head around is in this particular

583
00:34:59,640 --> 00:35:04,560
case, we had a very clear kind of cost function, you know, a specific thing.

584
00:35:05,000 --> 00:35:08,760
But what would it mean to robustify language models in, in the general?

585
00:35:08,960 --> 00:35:13,120
So one, one of the things that came up in our, you know, sort of literature

586
00:35:13,120 --> 00:35:18,080
view was this idea of, you know, when you're trying to control these discrete

587
00:35:18,240 --> 00:35:22,680
stochastic dynamical systems, one concept that can be quite useful is

588
00:35:22,880 --> 00:35:26,640
you might have a set of outputs that you want to reach or a set of outputs

589
00:35:26,640 --> 00:35:27,520
that you want to avoid.

590
00:35:27,520 --> 00:35:30,240
So an avoid set and basically a desirable set, right?

591
00:35:30,640 --> 00:35:34,840
And when you frame it like that, you know, I think that the robustification

592
00:35:35,080 --> 00:35:38,160
comes from the fact that let's say that you have a set of outputs, you

593
00:35:38,160 --> 00:35:40,440
really don't want the language model to, to emit, right?

594
00:35:40,640 --> 00:35:43,560
You might think, okay, well, I'll just fine tune it so that it decreases

595
00:35:43,560 --> 00:35:46,760
the likelihood, the prior likelihood basically of those sequences, right?

596
00:35:47,160 --> 00:35:50,040
And the issue with that, I think, and the thing that the control

597
00:35:50,040 --> 00:35:54,440
theoretic perspective sort of brings in is the fact that when you have

598
00:35:55,440 --> 00:35:59,320
finite, even a small control prompt, some extra tokens that you get to inject,

599
00:35:59,520 --> 00:36:04,200
it turns out that even very, very unlikely next tokens can be made to be

600
00:36:04,200 --> 00:36:08,240
the most likely next token just by inputting these new examples.

601
00:36:08,240 --> 00:36:12,880
So even if you did hypothetically fine tune the model so that this avoid set

602
00:36:12,880 --> 00:36:17,080
was assigned very low probability, it seems like if you don't incorporate

603
00:36:17,080 --> 00:36:20,320
some aspect of, you know, maybe stochastically trying to search for

604
00:36:20,320 --> 00:36:23,600
these adversarial examples and sort of having this sort of mini max thing

605
00:36:23,600 --> 00:36:26,800
where you have one system that's trying to elicit the output, one system

606
00:36:26,800 --> 00:36:30,160
that is trying to fine tune the model to maybe make it less likely or optimize

607
00:36:30,160 --> 00:36:33,400
another part of the prompt that is supposed to steer it away from these outputs.

608
00:36:34,200 --> 00:36:37,400
Basically, the inside, I think, is that you really have to be careful

609
00:36:37,400 --> 00:36:41,160
to consider the fact that you have, you're giving the outside world some

610
00:36:41,160 --> 00:36:44,480
amount of control over the system, some amount of control over the context.

611
00:36:44,720 --> 00:36:48,080
And planning around that is actually very non-trivial and is not really

612
00:36:48,080 --> 00:36:51,680
well managed, I don't think, through the classical view of just cross entropy loss

613
00:36:51,880 --> 00:36:54,240
and just treating it like a probability distribution.

614
00:36:54,680 --> 00:36:59,520
Something else that fascinates me is the divergence between focusing on

615
00:36:59,520 --> 00:37:04,120
the model versus, you know, complexifying the software which controls it.

616
00:37:04,400 --> 00:37:07,760
So right now, for example, we have language models and, you know, there's

617
00:37:07,760 --> 00:37:11,840
this kind of base training and then there's fine tuning and there's RLHF

618
00:37:11,840 --> 00:37:15,840
and, you know, there's like command variations of that, for example.

619
00:37:16,240 --> 00:37:19,440
And then we build these software APIs that are just trying to abstract

620
00:37:19,440 --> 00:37:24,680
away the complexity, so they will do dynamic prompt construction for multi,

621
00:37:25,960 --> 00:37:29,000
you know, multi-stop tool use and it goes on and on and on.

622
00:37:29,000 --> 00:37:33,520
There will be frameworks for doing agentic LLMs and there just seems

623
00:37:33,520 --> 00:37:35,760
to be like a bit of a divergence here.

624
00:37:36,000 --> 00:37:39,920
But the reason I'm asking the question is, does it make sense to

625
00:37:40,120 --> 00:37:42,840
robustify and fix the problem in the model?

626
00:37:43,040 --> 00:37:47,800
Or does it make sense to almost increase the flexibility of the model

627
00:37:47,800 --> 00:37:49,560
and fix it in the software layer?

628
00:37:50,360 --> 00:37:55,240
I think one of the insights from our paper is that solely focusing on

629
00:37:55,240 --> 00:37:59,680
the model itself, like Amman was just saying, as soon as you give the

630
00:37:59,680 --> 00:38:03,440
outside world control over the model in the sense of being able to input

631
00:38:03,480 --> 00:38:07,600
whatever kind of text that they want, it becomes very difficult to

632
00:38:07,600 --> 00:38:11,280
really prevent adversarial attacks and prevents jail breaks.

633
00:38:11,280 --> 00:38:13,520
And that's, you know, why you see jail breaks keep coming up.

634
00:38:14,120 --> 00:38:20,400
I think if you were to involve some sort of robustness in a software layer,

635
00:38:20,640 --> 00:38:22,000
that might be more feasible.

636
00:38:22,800 --> 00:38:27,880
At least I can't immediately picture, you know, ways around it as, you know,

637
00:38:28,000 --> 00:38:32,640
of course, if I was a hacker, I could probably, you know, find some loophole.

638
00:38:32,640 --> 00:38:34,840
There's usually some loophole you can find.

639
00:38:34,880 --> 00:38:39,560
But if there was some way of fielding the prompt messages, for instance,

640
00:38:39,840 --> 00:38:43,880
a user gives you a prompt, first you check, is this a reasonable thing

641
00:38:43,880 --> 00:38:46,640
that a human being would say in conversation, or is this something

642
00:38:46,640 --> 00:38:49,720
that I've never seen before in the entire history of the internets?

643
00:38:50,080 --> 00:38:50,400
Right.

644
00:38:50,880 --> 00:38:55,240
The latter maybe is a prompt injection, maybe is, you know, something

645
00:38:55,240 --> 00:38:57,960
devious, or maybe is, you know, computer science research.

646
00:38:58,800 --> 00:39:01,400
But yeah, it's definitely not an easy problem.

647
00:39:01,440 --> 00:39:04,040
But the good thing is that there are multiple approaches to it.

648
00:39:04,400 --> 00:39:04,920
Very cool.

649
00:39:05,160 --> 00:39:07,800
So we're going to go on to the more galaxy brain stuff in a second.

650
00:39:07,800 --> 00:39:10,440
So before we move off the paper, can you just talk more formally

651
00:39:10,440 --> 00:39:12,680
about what you showed in the paper?

652
00:39:13,000 --> 00:39:13,720
Yeah, definitely.

653
00:39:13,720 --> 00:39:16,040
So there were two main parts of the paper.

654
00:39:16,440 --> 00:39:17,560
So I guess three.

655
00:39:17,560 --> 00:39:21,920
So for one, what we did was we tried to formalize what an LLM system

656
00:39:21,920 --> 00:39:23,960
really is at a mathematical level.

657
00:39:24,200 --> 00:39:27,000
And what we were trying to do at that was basically balance the fact

658
00:39:27,000 --> 00:39:30,360
that, you know, we really wanted to try to take advantage of, you know,

659
00:39:30,360 --> 00:39:33,480
the original sort of control theories, very abstract picture of a system

660
00:39:33,480 --> 00:39:37,280
where you have this input space, you have a state space and output space.

661
00:39:37,440 --> 00:39:39,160
And there's some dynamics going on inside of it.

662
00:39:39,640 --> 00:39:43,400
In our case, we parameterized those dynamics with an LLM and our input

663
00:39:43,400 --> 00:39:46,760
space and our state spaces were basically the set of all possible

664
00:39:46,760 --> 00:39:49,960
token sequences from the vocabulary set of this model.

665
00:39:49,960 --> 00:39:50,280
Right.

666
00:39:50,480 --> 00:39:51,760
So that was the first part.

667
00:39:51,800 --> 00:39:55,600
And we basically transferred over a lot of the notions of basically

668
00:39:55,600 --> 00:39:59,520
reachability and controllability for LLM systems from the original control

669
00:39:59,520 --> 00:40:02,880
theory where you can really just define it in terms of this really abstract

670
00:40:02,920 --> 00:40:07,000
notions of, you know, have sets for the reachable or sorry, the state space,

671
00:40:07,000 --> 00:40:09,400
the input space and the output space, you have some dynamics.

672
00:40:09,400 --> 00:40:12,760
And basically in terms of those sets, you can define reachability and control.

673
00:40:13,000 --> 00:40:14,000
So that was the first part.

674
00:40:14,600 --> 00:40:16,960
The next thing that we did was we tried to look inside the model.

675
00:40:16,960 --> 00:40:20,400
So we were thinking, you know, it'd be really nice, like in control theory,

676
00:40:20,840 --> 00:40:24,440
if we could have a really good understanding of the components of the system

677
00:40:24,440 --> 00:40:26,480
and how controllable those individual pieces were.

678
00:40:26,760 --> 00:40:30,160
So what we did is we looked at a single self-attention head and tried

679
00:40:30,160 --> 00:40:32,760
to really think about it through a matrix algebraic perspective.

680
00:40:32,960 --> 00:40:36,280
To really break down what the relationship is between, let's say,

681
00:40:36,280 --> 00:40:39,400
you have a subset of the tokens, you get to control a subset that's fixed.

682
00:40:39,640 --> 00:40:42,560
And you're trying to get the output to be, you know, a certain value,

683
00:40:42,560 --> 00:40:45,800
the output representations where all of these in the case of a self-attention

684
00:40:45,800 --> 00:40:48,760
head are just these vector representations of tokens.

685
00:40:49,400 --> 00:40:53,720
So what we found there was that it actually is possible to do some fairly,

686
00:40:53,760 --> 00:40:57,800
you know, simple matrix algebra manipulations to decompose the output

687
00:40:57,920 --> 00:41:02,480
of a self-attention head into one component that arises from the imposed input.

688
00:41:02,600 --> 00:41:06,440
And then another component that arises from the control input, and assuming

689
00:41:06,440 --> 00:41:10,440
that those two are bound, then you can actually derive that, well,

690
00:41:10,440 --> 00:41:13,640
there actually is this geometry that sort of looks like a bubble around

691
00:41:13,640 --> 00:41:14,600
the default output.

692
00:41:14,600 --> 00:41:17,720
So the output, if you didn't have any control input in, there's a sort

693
00:41:17,720 --> 00:41:20,920
of bubble of reachable space that scales with the number of control

694
00:41:20,920 --> 00:41:22,640
input tokens that you're able to use.

695
00:41:22,960 --> 00:41:26,080
And we thought that that was really exciting because for one, I didn't

696
00:41:26,080 --> 00:41:28,800
really expect that you'd be able to do proofs on these sort of, you know,

697
00:41:28,880 --> 00:41:33,280
very complicated, high dimensional machine learning or deep learning systems

698
00:41:33,280 --> 00:41:34,240
like a self-attention head.

699
00:41:34,560 --> 00:41:37,680
But it also gave us some insight to say that, okay, we actually have

700
00:41:37,680 --> 00:41:41,280
this really concrete relationship between the sort of number of control

701
00:41:41,280 --> 00:41:44,600
input tokens, the magnitudes that you're able to input into the system,

702
00:41:44,840 --> 00:41:48,800
and the output reachable set that is at your disposal, basically.

703
00:41:49,120 --> 00:41:51,000
And so that was the second part.

704
00:41:51,000 --> 00:41:54,200
And then the last part was some empirical experiments where we said, okay,

705
00:41:54,200 --> 00:41:56,840
let's just sample a bunch of strings from Wikipedia.

706
00:41:57,080 --> 00:42:01,760
And we'll see, okay, the strings were between eight and 32 tokens.

707
00:42:01,760 --> 00:42:04,040
And those were basically our imposed state sequences.

708
00:42:04,280 --> 00:42:07,560
And we asked the question, well, can we get it to output the correct next

709
00:42:07,560 --> 00:42:09,240
token, the real next Wikipedia token?

710
00:42:09,480 --> 00:42:12,640
How many, you know, input tokens does it take or control input tokens does it

711
00:42:12,640 --> 00:42:14,000
take for that to happen?

712
00:42:14,000 --> 00:42:17,560
It turned out that you could get that done about 97% of the time to steer

713
00:42:17,560 --> 00:42:21,760
the model to the correct output within 10 tokens of a control input, which is

714
00:42:21,760 --> 00:42:24,800
reasonable, you know, we'd expect that the model should be able to be steered

715
00:42:24,840 --> 00:42:28,360
towards reasonable true English sentences that were more than likely in the

716
00:42:28,360 --> 00:42:29,160
training data set.

717
00:42:29,800 --> 00:42:33,040
What we did next was we tried to figure out, you know, if you sample the top

718
00:42:33,040 --> 00:42:39,280
75 most likely tokens, according to the model, based on this fixed input, can

719
00:42:39,280 --> 00:42:43,600
you steer those things to be the most likely token, basically the arg max of

720
00:42:43,600 --> 00:42:44,720
the probability distribution?

721
00:42:45,080 --> 00:42:48,840
And what we found there is that it's about 89% of the time, at least 89% of

722
00:42:48,840 --> 00:42:52,360
the time, we were able to find these optimal control inputs that were less

723
00:42:52,360 --> 00:42:54,680
than 10 tokens long, that would steer the model to do that.

724
00:42:54,920 --> 00:42:57,360
And then the last thing we did was he said, okay, well, let's see what would

725
00:42:57,360 --> 00:42:59,840
happen if we just randomly picked a token from the vocabulary.

726
00:42:59,840 --> 00:43:04,040
So this is everything from regular English to numbers to Cyrillic characters

727
00:43:04,040 --> 00:43:05,040
to Chinese characters.

728
00:43:05,360 --> 00:43:06,960
What if we just randomly sampled those?

729
00:43:07,200 --> 00:43:10,760
And we tried to see how many tokens it would take to steer that to being the

730
00:43:10,760 --> 00:43:12,320
arg max of the probability distribution.

731
00:43:12,600 --> 00:43:16,600
And we found there is about 46% of the time we were able to make that next

732
00:43:16,600 --> 00:43:21,240
token, the random one, the most likely next token using a prompt of length 10 or

733
00:43:21,240 --> 00:43:26,480
less. And the sort of curves are there in our, in our paper that described as

734
00:43:26,480 --> 00:43:30,120
you have an increasing budget for these tokens, how much of the time were we

735
00:43:30,120 --> 00:43:31,920
able to basically steer it to the right output?

736
00:43:32,120 --> 00:43:35,560
That's our basically the K epsilon controllability metric that lets us get

737
00:43:35,560 --> 00:43:39,320
this sort of statistical picture on controllability that renders it sort of

738
00:43:39,320 --> 00:43:42,280
practical to empirically estimate for these complicated systems.

739
00:43:42,640 --> 00:43:44,560
And so those are really the main results.

740
00:43:44,560 --> 00:43:47,880
And the surprising thing about the last one that I mentioned before was that a

741
00:43:47,880 --> 00:43:52,080
lot of times even really unlikely next tokens were able to be steered to be the

742
00:43:52,080 --> 00:43:56,080
most likely just using a really short prompt, which both gets at the, you know,

743
00:43:56,280 --> 00:44:00,960
basically chaoticness or complexity of language as a system, as well as the fact

744
00:44:00,960 --> 00:44:04,560
that the prior likelihood picture or the cross entropy loss picture doesn't

745
00:44:04,560 --> 00:44:09,360
quite get at the controllability sense of when you do have a, you know, ability

746
00:44:09,360 --> 00:44:12,000
to input tokens into the context, what happens then?

747
00:44:12,040 --> 00:44:13,640
So those are the really the main results.

748
00:44:13,640 --> 00:44:16,800
And then I mean, to me, the exciting, the really exciting part was the open

749
00:44:16,800 --> 00:44:19,840
questions where I was like, Oh, now that we're using this vocabulary, now that we

750
00:44:19,840 --> 00:44:24,080
formalize these LLMs as systems, it's really easy to ask these, you know,

751
00:44:24,080 --> 00:44:27,360
additional questions about, you know, the nature of the systems and the

752
00:44:27,360 --> 00:44:31,120
steerability controllability, especially with feedback or chain of thought or,

753
00:44:31,160 --> 00:44:33,440
you know, agents or all of these other ideas.

754
00:44:33,680 --> 00:44:35,600
And so yeah, that was basically the paper.

755
00:44:35,840 --> 00:44:36,160
Yeah.

756
00:44:36,160 --> 00:44:39,440
And it's really making me update my intuitions, right?

757
00:44:39,440 --> 00:44:41,400
So I'm thinking about the bias variance trade off.

758
00:44:41,840 --> 00:44:46,240
And I'm thinking that the reason we build these inductive priors is to

759
00:44:46,240 --> 00:44:50,280
constrain the model intentionally to make it statistically tractable to reduce

760
00:44:50,280 --> 00:44:51,920
the size of the hypothesis class.

761
00:44:52,360 --> 00:44:57,360
But what you're saying is making me think that statistical tractability and

762
00:44:57,360 --> 00:45:00,520
flexibility are not necessarily the same thing.

763
00:45:00,920 --> 00:45:05,920
Now it seems that the model must maintain a degree of flexibility.

764
00:45:05,920 --> 00:45:06,920
I mean, it makes sense, right?

765
00:45:06,920 --> 00:45:10,640
You have to be flexible in order to be a successful model.

766
00:45:11,400 --> 00:45:13,880
But that creates a kind of adversarial attack.

767
00:45:13,880 --> 00:45:17,880
So you can, the way I think about this is the model should be like the

768
00:45:17,880 --> 00:45:19,640
interstate freeway of language.

769
00:45:19,920 --> 00:45:23,040
So all of the major roads should be carved out and there should be side

770
00:45:23,040 --> 00:45:23,680
roads and so on.

771
00:45:23,680 --> 00:45:25,520
And that's the way I visualized the model.

772
00:45:25,760 --> 00:45:26,840
But the model's not like that.

773
00:45:26,840 --> 00:45:30,600
There's actually like all of these little slip roads and you can kind of

774
00:45:30,600 --> 00:45:34,360
push the cars off into the slip roads, but you need the slip roads because

775
00:45:34,360 --> 00:45:36,320
perhaps you couldn't train the model without the slip roads.

776
00:45:36,520 --> 00:45:38,600
Yeah, I think, I think that's a really good analogy.

777
00:45:38,600 --> 00:45:44,840
I think that's, um, thinking about pushing cars off the road into this space

778
00:45:44,840 --> 00:45:51,440
where they perhaps aren't used to being and what happens next.

779
00:45:52,120 --> 00:45:56,160
This, this is a case where the language model can answer some of these mode

780
00:45:56,160 --> 00:45:59,320
collapse type regimes and you can get kind of weird outputs.

781
00:45:59,680 --> 00:46:05,000
This is where you also, um, I mean, it was surprising that you can get the

782
00:46:05,120 --> 00:46:10,520
least likely token with just a specific inputs to be the most, the most

783
00:46:10,520 --> 00:46:15,160
likely next token, but if we treat language as this kind of road or as

784
00:46:15,160 --> 00:46:18,640
this kind of map structure, then it kind of makes sense that once you get off

785
00:46:18,640 --> 00:46:22,840
the map, once you enter this kind of regime that is completely unexplored,

786
00:46:23,200 --> 00:46:27,080
which there are actually plenty of regimes like this again, because the

787
00:46:27,080 --> 00:46:32,200
space is exponential in the number of tokens, it's growing so incredibly fast

788
00:46:32,520 --> 00:46:36,600
that it's very easy to find pockets that the model has never seen before and

789
00:46:36,600 --> 00:46:39,480
maybe no human on earth or it never will be seen again.

790
00:46:40,200 --> 00:46:43,800
You guys are really interested in, in collective intelligence and

791
00:46:43,800 --> 00:46:47,440
biomimetic intelligence and biologically plausible intelligence.

792
00:46:47,440 --> 00:46:50,160
And this is a matter very close to my heart.

793
00:46:50,520 --> 00:46:54,280
Um, what, what, what are you guys interested in specifically in that field?

794
00:46:55,200 --> 00:46:55,440
Yeah.

795
00:46:55,440 --> 00:47:00,680
So I guess when I first got into machine learning, it was from watching

796
00:47:00,680 --> 00:47:04,040
this Google DeepMind video where they were using reinforcement learning to

797
00:47:04,040 --> 00:47:08,560
teach this guy how to run this virtual reality avatar, how to run really fast.

798
00:47:08,560 --> 00:47:11,960
And I thought that was fascinating because it was like, okay, instead of

799
00:47:12,000 --> 00:47:15,240
traditional programming, you just have this neural network that optimizes

800
00:47:15,240 --> 00:47:16,920
itself according to some objective, right?

801
00:47:17,280 --> 00:47:21,000
And the thing that was intriguing to me about that was like the feed forward

802
00:47:21,000 --> 00:47:23,320
dynamics of a neural network aren't that complicated, right?

803
00:47:23,440 --> 00:47:26,520
You know, you have these synapses, you have this sort of gated action

804
00:47:26,520 --> 00:47:27,640
potential function.

805
00:47:27,920 --> 00:47:33,120
And the thing that was weird to me was like, how does every neuron know how

806
00:47:33,120 --> 00:47:34,360
to change its weights, right?

807
00:47:34,600 --> 00:47:38,440
How does each neuron that's independently not that smart know what to do?

808
00:47:38,800 --> 00:47:41,880
And so that sort of led me down the theoretical, the theoretical

809
00:47:41,880 --> 00:47:45,360
neuroscience route for some time where I was trying to figure out, okay, what

810
00:47:45,360 --> 00:47:48,480
do these learning rules look like that don't have to, you know, use the chain

811
00:47:48,480 --> 00:47:51,080
rule, use back propagation to update their weights.

812
00:47:51,200 --> 00:47:54,600
So I did that for a while and then sort of realized that the question of

813
00:47:54,600 --> 00:47:58,360
supervised learning was not necessarily the most interesting question to be

814
00:47:58,360 --> 00:48:01,360
asked, where it seems like the lion's share of what makes us really

815
00:48:01,360 --> 00:48:05,920
interesting as humans in our cognition seems to be associated with the cortex

816
00:48:05,920 --> 00:48:09,800
and this kind of predictive coding module that we have that lets us make

817
00:48:09,800 --> 00:48:13,680
these really rich abstract representations of reality, sort of understand what's

818
00:48:13,680 --> 00:48:16,600
going on, you know, we sort of hallucinate this internal model of the world.

819
00:48:17,000 --> 00:48:21,000
And so the interesting thing to me about the cortex was that, you know, you

820
00:48:21,000 --> 00:48:24,440
have this structure that's pretty flat and pretty homogenous throughout, you

821
00:48:24,440 --> 00:48:27,560
know, there's differences in different regions, but the end of the day, it's

822
00:48:27,560 --> 00:48:28,240
very similar.

823
00:48:28,240 --> 00:48:31,880
And in fact, if you lose a sense, like if you lose your vision, that region is

824
00:48:31,880 --> 00:48:33,360
often repurposed for other things.

825
00:48:33,360 --> 00:48:36,440
So it seems like there should exist, you know, the brain is kind of this

826
00:48:36,440 --> 00:48:40,240
existence proof that there should exist this rule set that if you apply it

827
00:48:40,240 --> 00:48:44,120
everywhere in the system in this sort of layer on the outside of the brain, then

828
00:48:44,360 --> 00:48:49,000
the behavior, the emergent property of that system is that you'll get this

829
00:48:49,000 --> 00:48:52,600
really robust and rich sort of representation of the world that is very

830
00:48:52,640 --> 00:48:54,880
predictive of subsequent sensory input.

831
00:48:54,880 --> 00:48:55,200
Right.

832
00:48:55,520 --> 00:48:58,480
And I think that the collective intelligence aspect of that is really,

833
00:48:58,480 --> 00:49:02,200
really important where there's one way to go in machine learning where you say,

834
00:49:02,200 --> 00:49:05,720
okay, we're going to make this monolithic pile of matrix algebra and we're going

835
00:49:05,720 --> 00:49:08,160
to train it through back propagation and gradient descent and the atom

836
00:49:08,160 --> 00:49:09,200
optimizer and all of that.

837
00:49:09,520 --> 00:49:13,400
And we're going to make it do some prediction task, but at the end of the

838
00:49:13,400 --> 00:49:16,480
day, every computation has to be implemented in physical reality.

839
00:49:16,480 --> 00:49:16,800
Right.

840
00:49:17,080 --> 00:49:21,120
And when we make the abstraction and just say, oh, it's just a bunch of math,

841
00:49:21,160 --> 00:49:22,240
we'll just have a GPU run it.

842
00:49:22,560 --> 00:49:25,320
It kind of abstracts away from this fact that at the end of the day,

843
00:49:25,360 --> 00:49:29,880
you have real physical objects that need to do computation and share

844
00:49:29,880 --> 00:49:34,320
information and in the sort of maximum efficiency, maximum scalability limit,

845
00:49:34,560 --> 00:49:38,440
it seems like what you'd end up having is a very similar sort of distributed

846
00:49:38,440 --> 00:49:43,240
structure where you can't really easily separate memory from computation.

847
00:49:43,240 --> 00:49:46,520
I think there's a quote from this MIT professor that says that Turing's

848
00:49:46,520 --> 00:49:50,080
initial mistake was saying that the head of the Turing machine was separate

849
00:49:50,080 --> 00:49:50,640
from the tape.

850
00:49:51,000 --> 00:49:54,000
Uh, and I think that that's true where in reality, you know, in brains,

851
00:49:54,000 --> 00:49:58,360
in, in real computing systems, the matter that composes the memory and the

852
00:49:58,360 --> 00:50:00,880
matter that composes the computation is really one in the same.

853
00:50:01,160 --> 00:50:04,360
And the brain is obviously this really great proof that, okay, there are

854
00:50:04,360 --> 00:50:08,440
relatively simple rules that are implementable with these biological neurons

855
00:50:08,440 --> 00:50:11,240
that if you just implement them everywhere, we'll get you this really

856
00:50:11,240 --> 00:50:14,760
beautiful, you know, convergence and emergent property of intelligence.

857
00:50:15,080 --> 00:50:18,200
And that really drove me for a long time in theoretical neuroscience.

858
00:50:18,200 --> 00:50:23,560
And then more recently in trying to build these distributed systems of, you

859
00:50:23,560 --> 00:50:27,320
know, artificial intelligences that, you know, the dream that I was trying to

860
00:50:27,320 --> 00:50:30,720
pursue before we started this control theory thing was that, okay, well, what

861
00:50:30,720 --> 00:50:33,680
if I just had a bunch of really small LLMs that, you know, everybody in the

862
00:50:33,680 --> 00:50:37,160
world could host and they could communicate with this sort of low band

863
00:50:37,160 --> 00:50:40,840
with communication using just tokens, just text over, you know, the regular

864
00:50:40,840 --> 00:50:44,680
internet and the emergent property of that, you know, what if it was possible

865
00:50:44,680 --> 00:50:48,760
that we can engineer a system that the emergent property was that it would

866
00:50:48,760 --> 00:50:52,920
actually be this really capable collective where maybe GPT-7 can be

867
00:50:52,920 --> 00:50:55,920
owned by everyone instead of just being behind closed doors in a data center

868
00:50:55,920 --> 00:50:56,680
that we have now.

869
00:50:56,880 --> 00:51:01,120
We're sort of using these insane engineering, you know, feats of, you

870
00:51:01,120 --> 00:51:03,640
know, NVIDIA interconnects and these really high bandwidth connections

871
00:51:03,640 --> 00:51:08,040
between massive racks in a data center that take a ton of energy to get this

872
00:51:08,040 --> 00:51:10,800
really great result of, you know, modern language models.

873
00:51:11,040 --> 00:51:13,840
What if we could have a system that was a bit more like the brain, a bit

874
00:51:13,840 --> 00:51:17,600
more decentralized and really leverage this insight that it should be possible,

875
00:51:17,600 --> 00:51:19,960
you know, this existence proof keeps coming back to you where it's like,

876
00:51:19,960 --> 00:51:21,360
okay, it should be possible, right?

877
00:51:21,800 --> 00:51:25,640
And that is sort of originally what led me to the control theory stuff where it

878
00:51:25,640 --> 00:51:28,360
just turned out to be really hard where we didn't have a great understanding of,

879
00:51:28,560 --> 00:51:31,800
you know, if we're treating these LLMs as systems rather than just, you know,

880
00:51:31,800 --> 00:51:35,080
big piles of matrix algebra that we're trying to distribute over many GPUs,

881
00:51:35,360 --> 00:51:38,160
if you treat them as systems that are coupled together, they're interacting

882
00:51:38,160 --> 00:51:40,640
in this networked fashion, how do we really understand that?

883
00:51:40,640 --> 00:51:42,960
You know, is it even possible to prompt them to do the right thing?

884
00:51:43,000 --> 00:51:43,800
When is it possible?

885
00:51:43,800 --> 00:51:45,120
How long do the prompts need to be?

886
00:51:45,480 --> 00:51:46,920
And that sort of led us down this route.

887
00:51:47,480 --> 00:51:50,480
But yeah, definitely the collective intelligence thing was, was a big

888
00:51:50,480 --> 00:51:52,400
motivation for me to get this working.

889
00:51:52,400 --> 00:51:55,880
And there's this neural cellular automata thing that I know you had talked

890
00:51:55,880 --> 00:51:58,640
with Michael Levin, who was the last author on that.

891
00:51:58,640 --> 00:52:02,160
And we worked with Alexander Mordvinsev on it, where it's this really,

892
00:52:02,160 --> 00:52:06,840
really great demonstration of how if you just optimize these basically small

893
00:52:06,840 --> 00:52:11,960
MLPs with local interaction to try to satisfy some objective, like, you know,

894
00:52:12,000 --> 00:52:16,400
reforming this gecko or lizard in their paper, then you actually can do that

895
00:52:16,400 --> 00:52:17,680
with back propagation through time.

896
00:52:17,680 --> 00:52:20,760
And so, you know, I thought, you know, it'd be really cool if we could try

897
00:52:20,760 --> 00:52:24,000
to engineer information processing systems that did this, not just morphogenesis

898
00:52:24,000 --> 00:52:27,360
systems, but information processing systems that operate in this way.

899
00:52:27,360 --> 00:52:30,400
Cause, you know, as a graduate of engineering science, we had to take

900
00:52:30,400 --> 00:52:32,040
a bunch of these digital logic courses.

901
00:52:32,280 --> 00:52:36,080
And when you have this very simple, you basically local state machine that has

902
00:52:36,440 --> 00:52:39,280
basically local connectivity, it's really easy to imagine how it

903
00:52:39,280 --> 00:52:42,720
would implement that as a custom chip and sort of reach this, you know,

904
00:52:42,720 --> 00:52:46,280
as Beth Jesus puts it, you know, thermodynamic limit of AI.

905
00:52:46,520 --> 00:52:47,640
And so that really excited me.

906
00:52:47,640 --> 00:52:50,640
And so I built a sort of demo of that where it was trying to do visual

907
00:52:50,640 --> 00:52:54,240
information processing on this really sparsified video is basically trying to

908
00:52:54,240 --> 00:52:58,640
do predictive coding of sorts on our active inference, I guess, on this

909
00:52:58,680 --> 00:53:01,600
incoming data stream of really sparsified video, trying to predict what would

910
00:53:01,600 --> 00:53:03,840
happen next, and it turned out to work quite well.

911
00:53:04,040 --> 00:53:06,560
And so then I was like, well, why can't we do that with language models?

912
00:53:06,560 --> 00:53:09,040
You know, as you mentioned, there are all these slip roads, right?

913
00:53:09,040 --> 00:53:12,240
Where if you prompt it just right, you can enter this really weird different

914
00:53:12,240 --> 00:53:16,480
regime and this exponentially large prompt space is a really handy way to try

915
00:53:16,480 --> 00:53:19,720
to control them where, you know, fine tuning is great, but what if we could

916
00:53:19,720 --> 00:53:23,520
just prompt them into interacting in a way that would lead to this emergent

917
00:53:23,520 --> 00:53:26,880
property of just being basically one larger language model that could

918
00:53:26,920 --> 00:53:28,880
predict the next token really, really well.

919
00:53:29,240 --> 00:53:32,400
And so that initial motivation sort of led to this control theory stuff.

920
00:53:32,400 --> 00:53:36,840
And I think that it is probably the right way to go for the field where if we

921
00:53:36,840 --> 00:53:41,360
want to be able to really leverage maximal computation towards our objectives,

922
00:53:41,400 --> 00:53:44,320
you know, the bitter lesson by Richard Sutton kind of suggests that we should

923
00:53:44,320 --> 00:53:48,320
probably aim for systems where you can just slap on more and more compute.

924
00:53:48,320 --> 00:53:51,400
You can have a relatively simple procedure that you follow to leverage

925
00:53:51,400 --> 00:53:53,280
more compute towards your objectives.

926
00:53:53,440 --> 00:53:55,640
That's probably the way to go for making advances in AI.

927
00:53:55,880 --> 00:53:59,600
And if we can have this decentralized networked system that, you know, I took

928
00:53:59,600 --> 00:54:02,160
this distributed systems course while I was here, that was really great and

929
00:54:02,160 --> 00:54:05,040
sort of taught how to make, you know, basically databases that were

930
00:54:05,040 --> 00:54:08,160
distributed over many servers that would have this, you know, the emergent

931
00:54:08,160 --> 00:54:11,280
property they wanted was robustness, consistency and availability.

932
00:54:12,080 --> 00:54:15,520
If we could have something similar to that, that is radically scalable and is

933
00:54:15,520 --> 00:54:18,640
able to be, you know, just run by regular people who don't need to own their

934
00:54:18,640 --> 00:54:22,360
own, you know, GPU cluster that's maybe illegal in the future when the US

935
00:54:22,360 --> 00:54:24,560
government is like, oh, you can only have this many petaflops.

936
00:54:25,520 --> 00:54:29,600
Basically, yeah, that was the real motivation for, for the, what I call

937
00:54:29,600 --> 00:54:30,960
the language game, that project.

938
00:54:30,960 --> 00:54:33,160
And that's something that we're continuing to work on.

939
00:54:33,160 --> 00:54:36,040
But yeah, that kind of led to this control theory thing where we were just

940
00:54:36,040 --> 00:54:39,400
like, yeah, we really need to get a grip on what these look like as systems.

941
00:54:39,680 --> 00:54:43,960
As we start to build these more and more complicated, you know, network

942
00:54:43,960 --> 00:54:47,240
distributed, you know, beautiful emergent systems that hopefully will be able

943
00:54:47,240 --> 00:54:49,040
to be hypercapable in the future.

944
00:54:49,320 --> 00:54:50,840
Yeah, this is all music to my ears.

945
00:54:50,880 --> 00:54:54,840
I'm a huge fan of the externalist thought in cognitive science.

946
00:54:55,200 --> 00:54:59,200
And even though I, I love the work from Jeff Hawkins, you were talking about

947
00:54:59,200 --> 00:55:05,160
the neocortex, but even then, you know, I would kind of say that it's a lot

948
00:55:05,160 --> 00:55:08,920
of the cognition happens outside of the brain, you know, we're not islands.

949
00:55:09,200 --> 00:55:11,880
And actually, I was just thinking maybe a better analogy rather than the interstate

950
00:55:11,880 --> 00:55:15,800
freeway might be, you know, in Star Trek Voyager, there was the wormhole

951
00:55:15,800 --> 00:55:18,440
network and the Borg fan, the secret work.

952
00:55:18,440 --> 00:55:21,040
And you could kind of like, you know, get into these little slip streams

953
00:55:21,040 --> 00:55:22,880
and go to different parts of the universe.

954
00:55:23,160 --> 00:55:26,960
But when I was interviewing Philip Ball, he wrote this book, How Life Works.

955
00:55:27,240 --> 00:55:30,480
And he was trying to understand, you know, what are the mechanisms like, you know,

956
00:55:30,480 --> 00:55:33,440
self-organization and multi-scale information sharing and, you know,

957
00:55:33,480 --> 00:55:34,360
emergentism.

958
00:55:34,800 --> 00:55:38,680
And it's, it's really, really, um, uh, fascinating.

959
00:55:38,960 --> 00:55:43,880
So how can we introduce some of these concepts into the next generation of AI?

960
00:55:44,160 --> 00:55:47,360
Yeah, this is one of the things I'm certainly most excited, excited about

961
00:55:47,680 --> 00:55:54,160
because I see life as this kind of interconnected, interplay, multi-scale

962
00:55:54,880 --> 00:55:58,240
process of exploitation and exploration.

963
00:55:58,520 --> 00:56:00,880
And these are two terms from the reinforcement learning literature.

964
00:56:01,120 --> 00:56:04,680
But I mean this in a much more general sense because at each stage of life, we're

965
00:56:04,680 --> 00:56:09,800
either going out into the world to get something, to do something, to try something

966
00:56:09,800 --> 00:56:15,360
new, and then at the next stage, we're coming back in, going home, uh, you know,

967
00:56:15,400 --> 00:56:18,280
reflecting, uh, going over our insights.

968
00:56:18,960 --> 00:56:23,800
And it's, it's this process, this ebb and flow, going out, coming back in.

969
00:56:24,240 --> 00:56:29,480
And I see this kind of pattern emerge across many different aspects of machine

970
00:56:29,480 --> 00:56:33,200
learning and artificial intelligence work in the sense that a lot of our

971
00:56:33,440 --> 00:56:37,000
algorithms that we have now are convergent, they're objective driven.

972
00:56:37,360 --> 00:56:39,200
We establish a loss function.

973
00:56:39,200 --> 00:56:42,120
We say, these are the rules it should follow.

974
00:56:42,320 --> 00:56:44,600
It's going to update according to this equation.

975
00:56:44,920 --> 00:56:48,920
And we set the system running, learns from data, and we have a final product.

976
00:56:49,400 --> 00:56:54,000
And on the flip side, there's, you know, like what Ken Stanley works with.

977
00:56:54,320 --> 00:56:59,160
Um, more exploratory, uh, evolutionary algorithms or open-ended algorithms.

978
00:56:59,520 --> 00:57:01,640
And this is, this is the other side of things.

979
00:57:01,640 --> 00:57:06,240
And I think some of the most interesting work to be done is how these

980
00:57:06,240 --> 00:57:13,000
two sides interconnects, how can we lay down rules, strict rigid rules, which

981
00:57:13,040 --> 00:57:18,120
when they're followed can generate novelty, can generate creativity, can

982
00:57:18,120 --> 00:57:23,200
generate organization in a way which is not predetermined, but almost fractal

983
00:57:23,200 --> 00:57:25,040
and infinite in its complexity.

984
00:57:25,640 --> 00:57:28,760
And are those rules defined already?

985
00:57:28,760 --> 00:57:30,000
Do they exist in the world?

986
00:57:30,240 --> 00:57:31,360
Are we guided by them?

987
00:57:31,800 --> 00:57:34,240
Are there principles like that which exist that we can come to?

988
00:57:35,040 --> 00:57:39,080
Or is it, you know, are we kind of, you know, the authors of our own

989
00:57:39,080 --> 00:57:39,920
fates in a sense?

990
00:57:39,920 --> 00:57:41,680
Are we each agents and actions?

991
00:57:42,040 --> 00:57:43,680
Uh, we get to choose our path in life.

992
00:57:44,240 --> 00:57:47,320
I think these, these are the directions I'm really interested in.

993
00:57:47,520 --> 00:57:51,000
And to connect this to my research, one thing I'm focused on now for my thesis

994
00:57:51,000 --> 00:57:54,680
project, um, is looking at morphogenesis.

995
00:57:54,800 --> 00:57:58,400
So this connects to the more defensive paper as well, except what I'm

996
00:57:58,400 --> 00:58:00,480
really interested in is how does structure emerge?

997
00:58:00,840 --> 00:58:03,480
How do different cells actually connect together?

998
00:58:03,920 --> 00:58:08,560
So, um, in that paper, for instance, each of the cells were on a fixed grid,

999
00:58:08,600 --> 00:58:12,680
but in our bodies, uh, there's actually quite a sophisticated protein

1000
00:58:12,680 --> 00:58:16,200
expression network which governs how cells adhere together.

1001
00:58:16,600 --> 00:58:21,560
Um, certain gene regulation pathways will turn on cat herons, which will

1002
00:58:21,560 --> 00:58:23,760
cause cells to attach together.

1003
00:58:23,760 --> 00:58:27,720
And then in other parts, um, these cells can unattach and then be

1004
00:58:27,720 --> 00:58:30,080
transported all around the embryo.

1005
00:58:30,480 --> 00:58:34,440
And I think understanding this process more deeply, not only could shed

1006
00:58:34,440 --> 00:58:38,760
lights on structure formation and problems in biology in general, but

1007
00:58:38,760 --> 00:58:43,440
maybe more deeper general problems of structure learning, because we might

1008
00:58:43,440 --> 00:58:47,200
think of embryology as quite disconnected from machine intelligence or

1009
00:58:47,200 --> 00:58:52,160
artificial intelligence, but every single brain is formed in the same way.

1010
00:58:52,560 --> 00:58:54,400
And that's through developments.

1011
00:58:55,000 --> 00:58:55,280
Yeah.

1012
00:58:55,320 --> 00:58:57,640
Um, I'm also a disciple of Kenneth Stanley.

1013
00:58:57,640 --> 00:58:59,320
He's, he's absolutely incredible.

1014
00:58:59,320 --> 00:59:00,880
Everyone at home needs to read his book.

1015
00:59:00,880 --> 00:59:02,120
My greatness cannot be planned.

1016
00:59:02,560 --> 00:59:03,920
Um, yeah.

1017
00:59:03,920 --> 00:59:09,040
You know, so in, in the natural world, we have, um, it, it's so interesting.

1018
00:59:09,040 --> 00:59:12,960
So we have this kind of like self-organization and then we have multi-scale

1019
00:59:12,960 --> 00:59:18,080
information sharing, but we also have canalization, which is that, um, you

1020
00:59:18,080 --> 00:59:22,160
actually see a kind of, um, convergence of, of structure and forms, you know,

1021
00:59:22,160 --> 00:59:25,720
which is reused, you know, almost as, as modules, um, in the system.

1022
00:59:25,960 --> 00:59:29,240
But then there's always the question of how do we create something like this?

1023
00:59:29,240 --> 00:59:31,240
Because is it simply a matter of complexity?

1024
00:59:31,240 --> 00:59:34,880
Do you need to have a microscopic scale to reproduce this?

1025
00:59:35,080 --> 00:59:36,400
Or could we reproduce it?

1026
00:59:36,640 --> 00:59:40,880
And then if we did reproduce it, the catch 22 situation is that, you know,

1027
00:59:40,920 --> 00:59:45,680
when you impute directedness onto a system, it loses its intelligence.

1028
00:59:45,680 --> 00:59:48,440
Cause to me intelligence is divergence.

1029
00:59:48,440 --> 00:59:53,560
It's exactly as you were saying, it's this tapestry of, um, discovering

1030
00:59:53,560 --> 00:59:56,320
problems, solutions, new problems, solutions.

1031
00:59:56,320 --> 00:59:57,600
And it goes on and there's no end.

1032
00:59:57,600 --> 00:59:58,560
It goes on forever.

1033
00:59:58,920 --> 01:00:03,200
And any attempt by us to control it with, I mean, it's a bit like the

1034
01:00:03,200 --> 01:00:07,080
bitter lesson, you know, Sutton said, any human design, any attempt to

1035
01:00:07,080 --> 01:00:11,560
steer it makes it convergent, but then we could do something like the game

1036
01:00:11,560 --> 01:00:16,600
of life from John Conway and incredible, beautiful structure emerges from that.

1037
01:00:16,640 --> 01:00:23,000
But whenever we try to steer it with our own will, it seems to corrupt it as well.

1038
01:00:24,440 --> 01:00:24,840
Yeah.

1039
01:00:25,040 --> 01:00:28,520
I think that the analogy to biology is really useful here and the

1040
01:00:28,520 --> 01:00:31,760
canonization that you mentioned, you know, you have this reuse of structures

1041
01:00:31,760 --> 01:00:35,840
across, you know, cells, for instance, they all have this similar machinery to

1042
01:00:35,880 --> 01:00:39,080
do gene expression and they have the same genetic code underlying that

1043
01:00:39,080 --> 01:00:42,160
gene expression with, you know, maybe differences in cell state, but at the

1044
01:00:42,160 --> 01:00:43,720
end of the day, it's the same machinery, right?

1045
01:00:44,000 --> 01:00:46,800
And, you know, I used to do a bit of protein engineering with language

1046
01:00:46,800 --> 01:00:49,560
models, and that's how actually how I learned about, uh, transformers and

1047
01:00:49,560 --> 01:00:50,760
built my first transformers.

1048
01:00:50,760 --> 01:00:54,840
And I think that the analogy is really strong where, you know, cells sort of

1049
01:00:54,840 --> 01:00:58,760
know how to read this genetic code, this language of the genetic code.

1050
01:00:58,960 --> 01:01:02,640
And they all use that ability, this canalized ability that's distributed

1051
01:01:02,640 --> 01:01:06,640
across all of them to locally they solve this problem of, okay, what is this

1052
01:01:06,640 --> 01:01:07,960
specific cell supposed to do?

1053
01:01:07,960 --> 01:01:11,800
What should it do to basically support the overall function of the organism?

1054
01:01:11,800 --> 01:01:12,120
Right.

1055
01:01:12,400 --> 01:01:16,280
And similarly, I think the hope with these language models is that now we have

1056
01:01:16,280 --> 01:01:21,480
these language based models or LLMs that have this similar sort of

1057
01:01:21,520 --> 01:01:23,440
understanding of language.

1058
01:01:23,480 --> 01:01:27,120
They are able to really constrain the probability distribution, understand

1059
01:01:27,240 --> 01:01:30,480
which sequences of text are reasonable English and, you know, what they might

1060
01:01:30,600 --> 01:01:31,440
want to generate.

1061
01:01:31,680 --> 01:01:35,440
And the exciting thing to me is that we can kind of do a similar sort of

1062
01:01:35,440 --> 01:01:39,360
evolutionary search that we used to do with, or that we currently do with, uh,

1063
01:01:39,400 --> 01:01:42,560
trying to find protein sequences, uh, when we're doing protein engineering with

1064
01:01:42,560 --> 01:01:47,200
the language models, where every computer in this network of systems has this

1065
01:01:47,200 --> 01:01:51,760
canalized ability to understand language, if you will, and is locally, it just needs

1066
01:01:51,760 --> 01:01:55,520
to solve this problem of what should this particular node do to support the

1067
01:01:55,520 --> 01:01:56,280
function of the system?

1068
01:01:56,280 --> 01:01:59,120
And that might be to explore, that might be to exploit, that might be to do any

1069
01:01:59,120 --> 01:02:00,680
sort of, any number of things.

1070
01:02:00,960 --> 01:02:04,440
And the discovery of that, I think, is really helped by the fact that we do

1071
01:02:04,440 --> 01:02:08,160
have strong language models that are able to really predict English or text

1072
01:02:08,160 --> 01:02:12,040
very well, uh, because they're able to explore this space.

1073
01:02:12,040 --> 01:02:15,640
And basically in the limit, you know, there's this good regulator theorem that

1074
01:02:15,640 --> 01:02:20,320
we had talked about before that says that any system that is a X that does

1075
01:02:20,360 --> 01:02:24,040
optimal control over another system must necessarily model that system.

1076
01:02:24,360 --> 01:02:28,280
Uh, and so if you think about in the limit, it seems like the best prompt

1077
01:02:28,320 --> 01:02:30,840
optimizers may end up being language models.

1078
01:02:30,840 --> 01:02:34,280
And already in our study, we were using this GCG algorithm that leverages a

1079
01:02:34,280 --> 01:02:37,800
language model to compute these gradients and try to figure out how we should

1080
01:02:37,800 --> 01:02:40,280
do this local stochastic search over prompts.

1081
01:02:40,520 --> 01:02:44,240
And so what I basically, I'm trying to get at is that there are actually a lot

1082
01:02:44,240 --> 01:02:48,000
of really interesting similarities, I think, that can be drawn upon from what

1083
01:02:48,000 --> 01:02:51,840
we know about the structure and the function of biological systems where,

1084
01:02:52,000 --> 01:02:55,400
you know, if we could crack this problem of there's this local control

1085
01:02:55,400 --> 01:02:58,960
objective or maybe information processing objective that must be met by every

1086
01:02:58,960 --> 01:02:59,520
cell, right?

1087
01:02:59,520 --> 01:03:03,480
Every compute node in this network of language models, if we could understand

1088
01:03:03,480 --> 01:03:07,160
what that is, what that even means from the perspective of systems and control

1089
01:03:07,160 --> 01:03:11,200
and, you know, computation and like, I think that that's a really promising

1090
01:03:11,200 --> 01:03:15,080
way that we can make progress on this dream of like, to me, it seems like it

1091
01:03:15,080 --> 01:03:18,960
would be great to have GPT seven, not just owned by one entity, but maybe

1092
01:03:18,960 --> 01:03:22,200
operated by the world where we could all have a say in what goes into it and

1093
01:03:22,200 --> 01:03:25,560
how it's used and what it should be, you know, doing and can all benefit

1094
01:03:25,560 --> 01:03:29,160
from its excellent ability to compute and predict what will happen next and

1095
01:03:29,160 --> 01:03:32,440
basically perform intelligent, you know, operations on data.

1096
01:03:32,720 --> 01:03:36,440
So yeah, I think this is a really, really exciting area to be working on.

1097
01:03:36,680 --> 01:03:37,160
Amazing.

1098
01:03:37,520 --> 01:03:40,200
We're nearly at time, but we'll do two quick five questions.

1099
01:03:40,200 --> 01:03:44,480
So you've both just started the Society for the Pursuit of AGI.

1100
01:03:44,520 --> 01:03:44,840
Yes.

1101
01:03:44,840 --> 01:03:45,640
Can you tell us about that?

1102
01:03:45,880 --> 01:03:46,640
Absolutely.

1103
01:03:46,920 --> 01:03:51,080
So the Society for the Pursuit of AGI is a student organization.

1104
01:03:51,080 --> 01:03:54,320
Currently we're operating at the University of Toronto and at Caltech.

1105
01:03:54,840 --> 01:03:58,920
And we're essentially a crucible for new ideas.

1106
01:03:59,880 --> 01:04:04,400
If you think of university research labs as pursuing relatively safe

1107
01:04:04,480 --> 01:04:09,080
bets that could be publishable, industry research labs, relatively safe

1108
01:04:09,080 --> 01:04:12,880
bets that maybe might turn a profit one day in some new product or system.

1109
01:04:13,600 --> 01:04:19,600
The Society is for the Hail Marys, for the wild bets, for the crazy stuff, for

1110
01:04:19,600 --> 01:04:23,440
the real innovative stuff that's way outside the, you know, to use the

1111
01:04:23,440 --> 01:04:27,160
analogy of the highway network, we're trying to go off the beaten path.

1112
01:04:27,680 --> 01:04:33,480
And we really believe that the bottleneck in AI progress right now is not so

1113
01:04:33,480 --> 01:04:36,600
much compute, not so much algorithms, but it's conceptual.

1114
01:04:36,920 --> 01:04:41,720
We need better ideas about intelligence, about life, about what this whole thing

1115
01:04:41,720 --> 01:04:45,160
is that we're all experiencing and how we can gain deeper insights of it.

1116
01:04:45,600 --> 01:04:49,280
Not only do I think that a deeper understanding will help us to create

1117
01:04:49,280 --> 01:04:52,840
better systems, but it'll also give us confidence that the systems we're

1118
01:04:52,840 --> 01:04:56,520
developing will be beneficial to humanity and not harmful.

1119
01:04:56,960 --> 01:05:00,600
And I think that will only come with knowledge, with first principles,

1120
01:05:00,840 --> 01:05:01,520
understanding.

1121
01:05:02,000 --> 01:05:05,240
And so that's why one of the things we're trying to do is have our

1122
01:05:05,240 --> 01:05:06,680
club very interdisciplinary.

1123
01:05:07,160 --> 01:05:12,000
I think having machine learning be some, this kind of echo chamber amongst

1124
01:05:12,080 --> 01:05:15,800
engineers, computer scientists, maybe a dash of, you know, philosophy and

1125
01:05:15,800 --> 01:05:19,680
neuroscience, it'd be really nice to open the conversation to people in other

1126
01:05:19,680 --> 01:05:23,520
fields who maybe have a really unique insights into the phenomenon of

1127
01:05:23,520 --> 01:05:27,480
intelligence, perhaps behavioral economics can offer some insights.

1128
01:05:27,720 --> 01:05:29,360
Political science, right?

1129
01:05:29,360 --> 01:05:34,680
These are fields that are currently underappreciated, but may have useful ideas.

1130
01:05:35,160 --> 01:05:40,080
And maybe even people in the arts who, you know, creates, maybe they don't

1131
01:05:40,080 --> 01:05:43,480
design systems as much as they re-represent things that we know and

1132
01:05:43,480 --> 01:05:47,080
understand, they could have an interesting voice as well.

1133
01:05:47,720 --> 01:05:48,240
Very cool.

1134
01:05:48,280 --> 01:05:49,280
And final question.

1135
01:05:49,280 --> 01:05:51,520
I mean, first of all, I just wanted to say to both of you, thank you for

1136
01:05:51,520 --> 01:05:52,600
doing this great work.

1137
01:05:52,640 --> 01:05:55,600
So your paper is one of the most interesting that I've seen in the

1138
01:05:55,600 --> 01:05:57,360
LLM space in recent history.

1139
01:05:57,360 --> 01:06:01,800
And it was shared and loved by many of the folks on our Discord server.

1140
01:06:02,240 --> 01:06:08,800
But that does bring me to another point, which is that you didn't get into

1141
01:06:08,800 --> 01:06:16,120
ICLR and from my perspective, I'm, I'm shocked because this is really, really

1142
01:06:16,120 --> 01:06:16,520
interesting.

1143
01:06:16,520 --> 01:06:21,240
It has great utility from a practical and a theoretical perspective.

1144
01:06:22,480 --> 01:06:25,240
Feel free to have a, you know, a good bitch about reviewer number two.

1145
01:06:25,840 --> 01:06:27,720
No, I mean, I wish you'd just keep talking like that.

1146
01:06:27,720 --> 01:06:30,400
It really soothes the burn of reviewer number two, you know?

1147
01:06:30,840 --> 01:06:34,240
But no, I think that, um, yeah, the review system, to be honest, I'm

1148
01:06:34,240 --> 01:06:35,400
still trying to get my head around it.

1149
01:06:35,400 --> 01:06:39,280
I'm sort of an early career, you know, researcher, uh, trying to learn how it

1150
01:06:39,280 --> 01:06:39,680
works.

1151
01:06:39,680 --> 01:06:45,160
I mean, definitely the, the review process in, for ICLR, in their defense, you

1152
01:06:45,160 --> 01:06:50,480
know, we had this bug with the submission, uh, submission of our rebuttals basically.

1153
01:06:50,480 --> 01:06:53,760
So we had submitted the revision to our paper and then 15 minutes before the

1154
01:06:53,760 --> 01:06:56,120
deadline, Cameron and I were both getting this timed out error.

1155
01:06:56,400 --> 01:06:57,280
Uh, he was in Toronto.

1156
01:06:57,280 --> 01:07:02,000
I was in, uh, California and so, you know, they didn't end up actually reading

1157
01:07:02,000 --> 01:07:04,800
our rebuttals because we had sent it in and they were like, Oh, we'll post it for

1158
01:07:04,800 --> 01:07:07,720
you and then they were like, Oh, it was posted late, so can't read that.

1159
01:07:08,040 --> 01:07:12,120
Um, so yeah, I think that the review process definitely has given us a lot of,

1160
01:07:12,160 --> 01:07:15,320
you know, really useful insights where, you know, the second two results actually

1161
01:07:15,320 --> 01:07:19,000
that we talked about, the top 75 controllability and the random controllability.

1162
01:07:19,160 --> 01:07:22,760
Both of those were like from trying to address these reviewer comments, right?

1163
01:07:23,000 --> 01:07:27,080
So I think that what I'm trying to do at least is take as much of the good

1164
01:07:27,080 --> 01:07:30,120
parts of that, you know, trying to figure out how we can take advantage of this

1165
01:07:30,120 --> 01:07:33,200
process where we actually get insight from people in the field, what they're

1166
01:07:33,200 --> 01:07:36,000
looking for, what they think is interesting, what they think would improve

1167
01:07:36,000 --> 01:07:39,280
the, the work and try to, uh, try to use that.

1168
01:07:39,520 --> 01:07:42,840
And overall, just trying to figure out how to navigate this peer review system.

1169
01:07:43,080 --> 01:07:45,560
I think it definitely made it feel better as well that the Mamba paper was

1170
01:07:45,560 --> 01:07:49,120
also rejected from ICLR, which, uh, you know, sorry.

1171
01:07:49,720 --> 01:07:52,760
I know, yeah, yeah, it was crazy to me as well.

1172
01:07:52,800 --> 01:07:55,720
But, uh, yeah, definitely, uh, it's a, it's a challenge.

1173
01:07:55,720 --> 01:07:58,760
And, you know, after staying up for 40 hours to get this done, it was like,

1174
01:07:58,760 --> 01:08:01,160
Oh, would be a, it would have been nice if they could have looked at our

1175
01:08:01,160 --> 01:08:03,720
paper at least, you know, just see, you know, the work that we did.

1176
01:08:03,720 --> 01:08:07,400
But yeah, it's, uh, it's definitely good to learn from these things.

1177
01:08:07,400 --> 01:08:09,960
And I guess we've learned the lesson as well, not to submit in the last 15

1178
01:08:09,960 --> 01:08:12,600
minutes and to, you know, do it in, uh, in advance.

1179
01:08:12,600 --> 01:08:15,200
But yeah, thank you so much for your kind words about the paper.

1180
01:08:15,200 --> 01:08:15,840
That means a lot.

1181
01:08:16,120 --> 01:08:19,360
And yeah, we'll surely continue to make this better and a lot of exciting

1182
01:08:19,360 --> 01:08:22,400
plans for how we're going to continue to try to, you know, merge together

1183
01:08:22,400 --> 01:08:26,280
these two, you know, empirical and theoretical sides of the equation to make

1184
01:08:26,280 --> 01:08:30,720
some really, hopefully impactful work that can really help people build systems

1185
01:08:30,760 --> 01:08:34,160
and, you know, make better systems and not be suffering so much under the load

1186
01:08:34,160 --> 01:08:35,080
of prompt engineering.

1187
01:08:35,320 --> 01:08:36,640
So yeah, thank you very much.

1188
01:08:36,720 --> 01:08:37,040
Amazing.

1189
01:08:37,040 --> 01:08:39,560
Well, guys, it's been a pleasure and an honor to have you on the show.

1190
01:08:39,560 --> 01:08:40,880
So just keep doing the great work.

1191
01:08:41,200 --> 01:08:41,760
Absolutely.

1192
01:08:42,040 --> 01:08:43,520
Hopefully we'll get you on again.

1193
01:08:43,640 --> 01:08:43,960
Yeah.

1194
01:08:43,960 --> 01:08:46,920
Thank you so much for, I mean, for the opportunity to come and talk.

1195
01:08:46,920 --> 01:08:48,800
It's, it's been an amazing opportunity.

1196
01:08:48,800 --> 01:08:52,200
It's, it's really unbelievable to be sitting here in front of these cameras

1197
01:08:52,440 --> 01:08:56,520
after watching the show so many times, listening to so many of the podcasts.

1198
01:08:56,520 --> 01:08:58,920
And now to be speaking, it's just unbelievable.

1199
01:08:58,960 --> 01:08:59,520
So thank you.

1200
01:08:59,800 --> 01:09:00,240
Amazing.

1201
01:09:00,280 --> 01:09:01,000
Thanks so much, guys.

1202
01:09:01,760 --> 01:09:02,040
Awesome.

1203
01:09:02,040 --> 01:09:02,360
Okay.

1204
01:09:02,400 --> 01:09:02,880
It's a wrap.

