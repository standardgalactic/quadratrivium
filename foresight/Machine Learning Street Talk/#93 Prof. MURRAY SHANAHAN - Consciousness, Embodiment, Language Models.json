{"text": " Murray Shanahan is a professor of cognitive robotics at Imperial College London and a senior research scientist at DeepMind. He graduated from Imperial College with a first in computer science in 1984 and obtained his PhD from King's College in Cambridge in 1988. He's since worked in the fields of artificial intelligence, robotics and cognitive science. He's published books such as Embodiment and the Inner Life and the Technological Singularity. His book Embodiment and the Inner Life was a significant influence on the film Ex Machina for which he was a scientific advisor. Now Professor Shanahan is a renowned researcher on sophisticated cognition and its implications for artificial intelligence. His work focuses on agents that are coupled to complex environments through sensory motor loops such as robots and animals. He's also particularly interested in the relationship between cognition and consciousness and has developed a strong understanding of the biological brain and cognitive architectures more generally. In addition Professor Shanahan is interested in the dynamics of the brain including metastability, dynamical complexity and criticality as well as the application of this understanding to machine learning. He's also fascinated by the concept of global workspace theory as proposed by Bernard Bars. We'll be talking about that on the show today which is based on a cognitive architecture comprising a set of parallel specialist processes and a global workspace. Professor Shanahan is committed to understanding the long-term implications of artificial intelligence both its potential and its risks. His research has been published extensively and he's a member of the External Advisory Board for the Cambridge Centre of the Study of Existential Risk and also on the editorial boards of Connection Science and Neuroscience of Consciousness. Conscious Exotica Professor Shanahan wrote an article called Conscious Exotica in 2016 where he invited us to explore the space of possible minds, a concept first proposed by philosopher Aaron Sloman in 1984. Now this space is comprised of all the different forms of minds which could exist from those of other animals such as chimpanzees to those of life forms that could have evolved elsewhere in the universe and indeed those of artificial intelligences. Now in order to describe the structure of this space Shanahan proposes two dimensions, the capacity for consciousness and human likeness of the behavior. According to Shanahan the space of possible minds must include forms of consciousness that are so alien that we wouldn't even recognize them. He rejects the dualistic idea that there's an impenetrable realm of the subjective experience, remember we were talking about Nagel's bat on the Charmer's show, insisting instead that nothing is hidden metaphorically speaking, citing Wittgenstein actually. Now Shanahan argues that while no artifacts exist today, which has anything even approaching human-like intelligence, the potential for variation in artificial intelligences far outstrips the potential for variation in naturally evolved intelligence. This means that the majority of the space of possible minds may be occupied by non-natural variants such as the conscious exotica of which Shanahan speaks. Now ultimately Shanahan's exploration of the space of possible minds invites us to consider the possibility for human-like minds but also for those that are radically different and inscrutable. He concludes that although we may never understand these alien forms of consciousness, we can still recognize them as part of the same reality as our own. So Professor Shanahan has just dropped a brand new paper called Talking about large language models in which he discusses the capabilities and limitations of large language models. Now in order to properly comprehend the capacities and boundaries of these models, we must first grasp the relationship between humans and these systems. Humans have evolved to survive in a common world and have cultivated a mutual understanding reflected in their ability to converse about convictions and other mental states. Conversely, AI systems lack this shared comprehension, so attributing beliefs to them should be done circumspectly. Now prompt engineering is something that we've all become very familiar with, we've discussed it a lot on this show recently, and it's almost become a fact of the matter when it comes to these large language models. It involves exploiting prompt prefixes to adjust the language models to diverse tasks without needing any supplementary training, allowing for more effective communication between humans and machines. Nevertheless, lacking a more profound understanding of the system and its relationship to the external world, it's difficult to be certain whether the arguments produced by a large language model are genuine reasoning or simply mimicry. Large language models can be integrated into a variety of embodied systems even, such as robots or virtual avatars. However, this doesn't necessarily mean that these systems possess completely human-like language abilities. Even though the robot in the SAKAN system is physically embodied and interacts with the real world, its language is still learned and used in a dramatically different manner than humans. So in summary, although Professor Shanahan concludes that large language models are formidable and versatile, they're fundamentally unlike humans and we must be wary of ascribing human-like characteristics to these systems. We must find a way to communicate the nature of these systems without resorting to simple terms. This may necessitate an extended period of interaction and experimentation with the technology, but it's a fundamental step if we are to accurately portray the capabilities and limitations of large language models. So anyway, without any further delay, I give you Professor Murray Shanahan. Professor Shanahan, it's an absolute honor to have you on MLSD. Tell me a little bit about your background. My background? Well, I've been interested in artificial intelligence for as long as I can remember since I was a child, really, and I was very much drawn to it by science fiction, by science fiction movies and books. And then I studied computer science right from when I was a teenager and got very much drawn into programming, was fascinated by programming. I really did my 10,000 hours of programming experience when I was quite young and I went on to do computer science at Imperial College London. That was my degree. And then still fascinated by artificial intelligence, I went on to Cambridge and did my PhD in AI in Cambridge, very much in the symbolic school then. And then I had a long affiliation with Imperial College, did my postdoc there and still in symbolic AI. And then at some point, I became a bit disillusioned with symbolic AI and I kind of segued into studying the brain, which was the obvious example of actual general intelligence that we have. And I think it was a good 10 years on an excursion into neuroscience and computational neuroscience and that kind of thing. And then deep learning and deep reinforcement learning happened in the early 2010s and AI started to get interesting again. And I got very much back into it that way. And I was particularly impressed by DeepMind's DQN, the system that learned to play Atari games from scratch. And I thought that was a fantastic step forward. And I really kind of went back to my roots and back to AI at that point. Yeah, and I think we'll talk about DQN when we speak about your article on consciousness. But so having such a diverse set of experiences in adjacent fields, how have they influenced each other? Yeah, well, and one thing I didn't mention is that I've also had a long standing interest in philosophy. And I very often think that what I am is a sort of weird kind of philosopher, really. And philosophical questions have had a great attraction for me. So I think there's a sort of three way into relationship between artificial intelligence, neuroscience, and the other cognitive sciences and philosophy. And I think they all kind of mutually inform each other, really. Yeah. Fantastic. So you wrote a book called Embodiment and the Inner Life. What motivated you to write that book? Yeah. So at that point, so that book was published in 2010. And it was the culmination of a sort of long excursion into thinking about consciousness and about brains, which took place after I had moved away from symbolic AI, really. So I was thinking about the biological brain. In the back of my mind, I'd always been fascinated by these philosophical questions about consciousness. And then I went a bit kind of crazy and started thinking about these things seriously. It became kind of my day job to think about neuroscience, and about consciousness. And around about that time, the science of consciousness was taking off as a serious academic discipline with proper experimental paradigms. So that was really fascinating. And I got to know Bernie Bars. Bernie Bars is the person who originated global workspace theory, global workspace theory being one of the leading contenders for a scientific theory of consciousness. And I was very drawn to global workspace theory, and partly because it was a computational sort of theory. It drew very heavily on computer science and computer architectures. There was a computer architecture at the center of the theory. So this kind of collection of interests, along with my philosophical interests, which all came together, and I wanted to put them all into a book where I expressed my kind of ideas about, first of all, from the philosophical side, very heavy influence of Wittgenstein about how we address these problems at all, then lots of global workspace theory and a certain kind of global workspace architecture, how that might be realized in the brain, drawing also on the work of Stanislaus DeHend, who was working on what he called the global neuronal workspace idea, and putting all these things together into one big book. Amazing. Well, we'll speak a lot about Wittgenstein when we speak about the language model paper and your consciousness paper. But two things that did trigger or prick my ears up, computationalism, which is quite interesting, because some folks in the cognitive science arena, especially with the fouries, like examples to escape from computationalism. We did a show on cells, Chinese room argument the other day. He's probably one of the most known people who do issue computationalism. So what do you think about that? Yeah. Well, actually, so when I was talking about global workspace theory, I mentioned that it comes out of a kind of computational architecture. But in fact, where I took it was very much moving away from that original presentation, which drew heavily on a kind of quite an old-fashioned architectural perspective, sort of boxes and how they communicate with each other and so on. And I was much more interested in taking it in a direction which is very much more connectionist and drawing much more heavily on the underlying biology and neuroscience, which in fact is also a direction that Bernie Barnes himself had moved in, because the book that originally put forward his theory is from 1988. So that was the predominant way of thinking at the time was this very computational cognitiveist perspective. So by 2010, when my book was published, I was very much more interested in a kind of more connectionist perspective on things. So that's the way that it's portrayed in the book, the theory. Fascinating. Because in this arena, some people cite penrose or the need for hypercomputation, because people talk about the church-turing hypothesis and this idea that the universe could be made of information, which is quite interesting. But do you believe that the world that we live in could be computationally represented and computed? Well, I'm not sure that I have a belief on that particular one. So I mean, I mean, Penrose's ideas about consciousness, of course, draw heavily on quantum mechanics, and he thinks that quantum effects are important for consciousness. But I mean, that's very much a minority, a tiny minority view within the people who study consciousness from a scientific standpoint. And so I don't really subscribe to that interview, I have to say. Well, I mean, coming at it from a slightly different angle, we spoke to Noam Chomsky recently, and I've just done some content on Nagel's bat, a couple of rationalists, their big argument is about the subject of experience and the limits of our cognitive horizon and the inability really for us to reduce things into a comprehensible framework of understanding. So how would you bring that in? Yeah, well, gosh, I mean, yeah, we've launched right into some really big, difficult topics here, right? So in my book, Embodiment in the Inner Life, which at the time, I thought I'd really kind of like wrapped up the problem of consciousness. But one of the big sort of outstanding things for me in one of the outstanding questions that I have not really answered, I felt in that book, is very much related to Nagel's question about bats, what does it like to be a bat? So, and it's to do with the idea that there's a sort of intuitive idea that maybe there can be very exotic entities, very exotic creatures who are completely unlike us. And yet, somehow, there's some kind of consciousness there that we could barely grasp its nature. And this is a sort of natural intuitive thought. And especially when we look at other animals, like bats, and especially if we look at an animal that's a bit different from us, then we get hints that there's some one at home, as it were, and that there's consciousness there. I think we, I'm sure all of us believe that cats and dogs, and many other animals are conscious and are capable of suffering and having awareness of the world that's like our awareness and are aware of us and each other. I mean, I take that as almost axiomatic. That's just the way we treat those creatures. But then when we think about something like a bat, it's very different from us. So the natural thing thought is that maybe what it's like is very, very different from what it's like for us, and it's a natural thought to express. And of course, Nagel takes that thought to suggest that there are something that is inaccessible to us, which is what is it like to be a bat? It's something we can never know. And this is a very un-Viconstinian thought. And I'm very much, you know, I'm very attracted to Viconstine's philosophy. But it's also a very natural thought that, you know, so it's a very un-Viconstinian thought because Viconstine says, for example, you know, nothing is hidden. So he's very, you know, and the whole private language remarks are all about sort of saying, well, this intuition that we have that there's this private realm of experience is actually just, it's just a philosophical trick of the mind to think that this sort of peculiar metaphysical realm exists of inaccessible, subjective experience in others. And that's his whole thrust of his philosophy or that aspect of it is to try and undermine that. So these two things are intention, right? So there's this natural thought that bats, you know, it must be like something to be a bat, but what is it like and how could we ever know? And then there's the Viconstinian thought, which is actually very difficult to kind of really embrace. But it's that there's a sense in which nothing is really metaphysically hidden. It's only hidden, could be hidden empirically, because maybe we don't know enough, maybe we haven't hung around with bats often enough, or maybe we haven't examined their brains, or maybe that's all empirical, right? So there's nothing metaphysically hidden, whereas Nagel's point is that there's something that's deeply, profoundly, philosophically, metaphysically hidden, which is the subjective. Now we can extend that, shall carry on. So I'm rambling now. So now we can extend that thought about bats, now, you know, especially from the perspective of the sort of thing that I'm interested in, to, well, not just bats, but what about the whole space of possible minds to use Aaron Sloman's very evocative phrase? What about, you know, extraterrestrials who are going to be, you know, who surely there is extraterrestrial intelligence out there, it's going to be very, very, very different to us. So, and then what about the things that we build? Maybe we can build things, you know, and artificial intelligence of the future, maybe, maybe, you know, we can build something that is also conscious, it's the kind of thing that's depicted in science fiction all the time. In science fiction, it's often depicted as very human-like, but there's no reason why it should be human-like at all. And so we can imagine these very, very exotic entities, and then the question is even bigger, you know, there could be something that we, we won't even be able to recognize that there was even the possibility of consciousness, but maybe it's buried there inside this complex thing somehow. So that's the, that's the kind of question that fascinated me. And I wrote this paper called Conscious Exotica, which is all about trying to, trying to make that Viconstinian perspective encompass this possibility as well. Yeah, and maybe we should talk about that before the language paper, just because it's, it's what we're talking about now. But there's a few things you said there, which are really interesting. So, you know, when Chomsky talks about ghosts in the machine, and he goes back to Galileo and Descartes, and actually it was Descartes who, you know, introduced this kind of mind-body dualism, you know, which was kind of a move away from the previous desire to have a mechanistic understanding of the world that we live in. Humans want to understand, and actually so many things in the world eludes our understanding. And then that brings us on to David Chalmers' point that the hard problem of consciousness, which I suppose is an extension of the mind-body problem. And it's, as you were saying, this little bit extra, right? So we think about, and I agree with Chalmers that intelligence and consciousness are likely entangled or would co-occur together. But he always said that there's function, dynamics, and behavior. And then there's that little subjective thing on the top. And for Chalmers' consciousness, it's almost like, what's the cash value of it? He just thinks it's just something on top. It's not really requisite for anything else. And I believe it might be requisite for intentionality and agency as so did. But what's your take? Well, it's interesting because the whole way that you put that and the whole way that people often talk about this thing is you speak about consciousness. Like, there's this thing, which, you know, there's this singular thing, which maybe it's needed, maybe it isn't, maybe it's this, maybe it's that. But I think that whole way of talking is, which is natural for us in many everyday situations. But when it comes to this kind of conversation, I think that whole way of talking is maybe not quite right, because we're thinking of consciousness as this, you know, we're reifying it, turning it into this thing. Whereas I think maybe at that point we have to take a step back and we have to say, well, when we talk about, when we use that word, conscious or consciousness, so we use it in all kinds of different ways in different contexts. And so when we talk about, you know, we might talk about it in the context of an animal, we might say, well, the animal, you know, this dog is aware of its environment. So, you know, this dog can see the bowl in front of it, it can see me, it can see the door, it can see the trees, it can see the squirrel, you know, and it can smell more like you'd smell all of these things as well. So we use consciousness, you know, we talk about consciousness in that sense. And we also talk about our self-consciousness, you know, we talk about the fact that we're aware of our own thoughts and we talk about our inner life and we use consciousness to encompass that as well. We often use consciousness in the context scientifically of a distinction between conscious and unconscious processes. And that's a very interesting distinction because when we're consciously aware of a stimulus as humans, then a whole lot of things come together. We're able to kind of like deal with novelty better, we're able to report it, we're able to remember things better. So whereas when we perhaps are unconsciously or there's a kind of unconscious processing of the stimulus, then we still can respond to it behaviorally, but and it can have queuing effects and so on, but it doesn't have all those other things. So this and that's kind of, there's a kind of integrative function for consciousness there. And then on top of all of that, there is the capacity for suffering and joy that comes with. So often there's valence to consciousness, you know, so that's another thing. So all of these things, they come as a package in humans, but when we speak about edge cases, then these things come apart and we need to speak about them separately, I think. Fascinating. I mean, there are two kind of minor digressions there. I mean, you were talking about these planes of consciousness, which is also very interesting. And maybe we could get into the integrated information theory or the global workspace theory just for the audience, just to give them some context. Yeah, sure. Or do you want me to say a few words about that? Oh, please, yeah. Okay. Yeah. So there are a number of kind of candidates for a scientific theory of consciousness. And you just mentioned two of the leading ones, which are global workspace theory and integrated information theory. And so global workspace theory. So that's, that's Bernie Baals's was originated by Bernie Baals and has been developed by Stanislaus, Dehen and colleagues. So the idea there is it's, it does rest on this sort of architectural idea, which is that, which is that we think of the brain, the biological brain as comprising, you know, a very large number of parallel processes. This is kind of a natural way to think of the brain, a large number of parallel processes. And it, and the global workspace theory posits a particular way in which these, these processes interact and communicate with each other. And that is via this global workspace. And the idea there is that, is that there are sort of two modes of processing that go on. So in one mode of processing, the, these parallel processes just do their, their own thing independently. And in the other mode of processing, they are working via this global workspace theory. So the idea is that they, you might think of them as, as, you know, depositing messages, if you like, in this global workspace, which are then broadcast out to all of the other processes. So, so it's, so there's this kind of, but I think thinking of it in terms of messages is not quite the right way of thinking of it is better to think in terms of kind of signaling and information and so on. But that's a natural way to think of it. But so the, so these, so in, in that mode, the, these processes are sort of disseminating their influence to all the other processes. And that's the global kind of broadcast aspect of it. And that's when consciousness, well, that's when information processing is conscious, according to global workspace theory, as opposed to when it's all just local and the processes are doing their own thing. That's, that's not that that processing is not conscious. So there's a dist, so it's about teasing out this distinction between conscious information processing and unconscious information processing. Now, all of those terms, by the way, are deeply philosophically problematic and to go in, you know, you have to sort of do it properly, you have to kind of unpack them all in very carefully. And that's what my book try, try, tries to do. But so essentially, it's about so the essential idea, though, is to do with broadcast and dissemination of information throughout the brain and going from like local processes and help them having global influence. And that's what consciousness is all about according to global workspace theory. Okay, so integrated information theory. So I think so integrated information theory, which is Giulio Tononi's theory, which Giulio Tononi thinks is kind of kind of incompatible in some ways with with global workspace theory. But I don't think that's, that's true. I think I think that there's a lot of synergy between the two theories, in fact. But but that's because they so they come with the same for integrated information theory has sort of two aspects to it. So according to Giulio Tononi, he really is trying to pin down a property, which is almost like a physical property, which is identical with consciousness. So you can actually speak about the amount of consciousness in any system that you that you look at phi, he could this is good, it's phi. So the phi is a number how is actually a number of how much consciousness is present in the system, like, like part of your brain, your whole brain, or you as a person, or a flock of bats, or whatever, so you can or toaster, you know, so you can give a number to how much consciousness there is, there is there according to his theory. And it's a mathematical theory based on Shannon's information theory. And it's but it and but it's all about trying to see how much information is processed by the individual parts of the system versus how much information is processed by all the parts put together. And it's and it's to do with how much the second thing, you know, exceeds the first thing. And in a sense, and that is how much consciousness there is there. And, and in a way, it actually has some synergies. If you as long as you don't think that it's necessarily measuring, you know, this property of the of the universe, which you can put a number on. But it has some synergies with global workspace theory, because they're both distinguishing between global holistic things versus local things. And the and the consciousness is in the kind of global holistic processing versus the local, you know, local processing in both those theories. So there's a kind of, you know, there's some intuitions that they have in common, I think. Interesting. And it also reminds me a little bit a little bit about what Chalmers speaks about. So he thinks that it strongly emerges from certain types of information processing. And the processing must represent causal structures as well. So it can't it's it's not an appeal to panpsychism per se. And although with with all of the things that you've just spoken about, what do they work in another universe? I mean, I guess what I'm saying is, is it just the the physical and the information processing or in a different universe might it not emerge in the same way? Yeah, which depends what you mean by a different universe, I guess. What do you mean by a different universe? Well, if the laws of nature were different. Yeah, okay. So if the laws of physics were different. Well, I guess my I guess I dislike isms. I mean, I'm an anti ismist, or rather, I'd say I'm not an ismist. But if I were to but I do sort of subscribe broadly to functionalism, I suppose. So I guess I guess I what do I mean by that? I mean, what I mean is, I mean, I really dislike saying that I subscribe to these to these isms. So what I really mean by that is that is that I imagine that a system that is organized in a particular way functionally in terms of its information processing. And if that system is in is embodied in the broadest sense, and, you know, and meets lots of other prerequisites, then it's likely to behave in a way where I'm going to naturally use the word conscious to describe it, perhaps, and where I'm going to treat it like a fellow conscious creature. So, so, so it's so, you know, ultimately, it's I think it's about the kind of organization you need to give rise to the behavior you need to talk about thing, the thing in a certain way. My question today, because I posed this question to Chalmers last week, because he's also a functionalist. And I agree with the degree of functionalism describing intelligence, but less so with consciousness, you know, there's not a Turing test for consciousness, for example. But the thing is with functionalism, we're at risk of doing what you said people do with large language models, which is anthropomorphizing them, because these functions are intelligible to us. And then our conception of intelligence becomes somewhat observer relative. Yes, do I mean, what I observe a relative so you understand these functions, so it's conscious to you, but not to someone else? Well, so, so, so in all of these cases, I mean, I think it's about the words that we use in our language to talk about the things. So, so, so if there's someone else is someone just like us, right, then we have to and if we want to use the words in different ways. So, so the large language models are a great case in point, right. So, so suddenly we're arriving at a point where somebody can describe something as conscious. And others can say that's rubbish, you know, it's not that's not true at all. And so we, so we've, we've arrived at a point where these philosophically problematic words, which, which we use in ordinary life quite, quite harmlessly. And we all, you know, we all are in agreement about how we use the word likes if somebody says, oh, you know, Fred has drank so much last night, he passed out, he was completely unconscious, you know, I mean, and, or if an anesthetist says, yes, they, you know, the patient is now unconscious, they can't feel, feel pain. Or if you say, oh, you know, I, I just wasn't aware, I didn't see the, the cyclist and you know, that's why I, I hit them, you know, I'm really, it's tragic, but I just didn't see them. And then, and we, so, you know, so you're saying I wasn't aware of it. So that didn't influence my action. So there we're using the terms in ways that we all understand. But now we're getting to a point where suddenly, these words or these concepts are being used, you know, we don't have an way, we don't have agreement about how to use these words, right? Because it's, there are these exotic edge cases. Yes. So then the question, I think that you, you're getting is, you know, is there a fact of the matter there, right? And so I'm very tempted to say the first thing I'm tempted to say is that I don't think that perhaps is a fact of the matter. Or certainly, I don't, I don't want to, I don't want to speak as if there is a fact of the matter, but rather, I think we need to arrive at a new consensus about how we use these words. So that might mean that we extend the words, we break them apart, like I was suggesting earlier, maybe we need to separate out awareness of the world from self awareness, from integration, cognitive integration from the capacity for suffering, because suddenly we have things that where that they don't all come as a package. And when we need to kind of be a bit more nuanced in the way that we use these words, we need to use them in new ways. But then there's a kind of transition period, because we don't, you know, we're all arguing about how to use these words all of a sudden, because we've got weird edge cases. So there's going to be a time when it'll take a time for language to settle back down again. So there's a kind of, you know, there's a kind of observer relativness to this for a bit, if you like, but then, but then there's a kind of consensus needs to emerge, right? But so many things to explore there. I mean, I'm, I would love it if this platonic idea of concepts were possible. And what platonic? Because what we're talking about here is reductionism and the, I mean, the parable of the blind man and the elephant comes in quite nicely. So as Chomsky said, complex phenomenon beyond our cognitive horizon. And as much as we don't want to, we use functions derived from behavior to have some common understanding of this thing. But I wasn't being reductionist, was I? Do you think I was being reductionist? Well, no. So you said that the language game converges. And in some cases, we will arrive on a common definition, but like you can bring in Hofstatter as well. Well, not a common definition, but a common usage, right? So we'll come, so we'll come to use the words, you know, with agreement, right? So that's what I, and the reason why I mean, I would, and the reason I would balk at using the word reductionist is because, and that's why I'm a bit resistant to functionalism as well to any kind of ism is because I just think that that may be the way things are organized when you take them apart. So, you know, brains, right, when you examine them on the inside, like animal brains, you might look at how an octopus's brain works. And that might inform whether you think that it suffers, can experience this pain or not. Or we might break apart, you know, an AI of the system of the future, right? You know, and we might break it apart and we may look at its functional organization. And that all is just is grist to the mill of how our language might change, right? So I'm not, I'm not subscribing to the fact that consciousness is this or this is that it with some big metaphysical capital letters on the is, right? That's really important. So the functional organization of these other things, which when we study and look at it, is all just part becomes part of a conversation that eventually is going to help us to settle on maybe new ways of talking about these things. I think we agree with each other. I think the difference is, so with the parable of the blind men and the elephant, all of the men around the elephant saw something which was part of the truth. And I think that's what we're describing with the function. So we can all agree on what perception means or what action means. The thing is, there will be many other functions that will represent a different slice of that cognitive phenomenon. Yeah, I agree. And I think that's very much true with consciousness, actually, because there's lots of people coming with kind of like new ideas and new theories. I mean, Anil Seth, for example, have you had Anil on your on your not yet being right? So Anil's written this great book called Being You. And Anil brings in a whole kind of new set of ideas. He's particularly interested in the sort of top down effects on perception, top down effects on perception. So then he brings in this kind of top down influence and perception as a big part of things. And then there's Graziano has written this book using this about his attention schema theory of consciousness. And that's, and, you know, there's a whole set of interesting ideas there. And I think you're absolutely right. I think there's, I think there's aspects of all of these things all feed into, you know, all feed into the way, you know, brains and animals work and all of them feed into the, you know, why they behave the way they do and why we use those words when we use those words, you know, conscious and aware and so fascinating. We'll get to your article in a second. But as someone who has such a diverse and interesting background, who is allowed to ask these philosophical questions? So it reminds me and Thomas Meckens who is talking about the arguments between neuroscientists and philosophers about freedom of the will. Yeah. And who gets to decide? Huh. Yeah. Well, what a great question. You know, I mean, so why should I have any right to speak about any of these things at all? Because I have no formal training in philosophy. So, so, so who gets to, who gets to dis, well, who gets to, to, I guess there are two things, right? There's, I guess, I guess there's, there's in that kind of discussion between the neuroscientists and the, and the philosophers. So there you, there you're not talking about, you know, the everyday conversation that we're all having as, as, as humanity or as English speakers, or as Chinese speakers about how we use these, these, these words. So there it's a much more kind of confined to the, to, to different, two different schools or disciplines within academia. So there, I mean, I do think that the people who work in AI and in, and in neuroscience, probably at least should be a bit familiar with, with the philosophical debates. And you know, you mentioned Descartes earlier on, and you know, you're familiar with, with just that, that basic kind of, you know, sort of stuff that it was just like philosophy 101, which people should at least be aware of Descartes arguments and then Chalmers, and the different kind of perspectives on those sorts of things before they pitch in, you know, at least, I mean, you wouldn't pitch into neuroscience just by making some up some stuff about brains. If you hadn't read, you know, the, an introduction to neuroscience. And so, so I think that the scientists need to, you know, you know, they need to kind of have a, a past to enter the conversation, which is to have, to have gone through philosophy 101. Yeah, it's so true. We have the same thing with the, with the ethics folks, actually, because, because we have a lot of them fields of expertise, and engineers should learn more about ethics. Yeah, absolutely. But when they do have an opinion about ethics quite, quite often, it's, it's, it's, you know, it can sometimes be a bit naive. And, and, and, and, you know, at least you should be familiar with the kind of, but, but that's an interesting and the difficult area in itself. Because of course, you know, you, as a scientist, it's important that you take responsibility as a scientist and the, and that you take, you know, some ethical responsibility. But at the same time, you know, you've only got so much time to become an expert. So, so it's difficult to at the same time, take ethical responsibility. And yet, you know, even though perhaps you haven't got the time to kind of read, you know, read up and become an expert on the relevant ethics. So, I mean, perhaps everybody again, should, you know, get to the entry level, you know, ethics 101. And indeed, many, many courses teach, you know, ethics, these days, many kind of science and computer science. It's part of, you know, of any degree these days. So that's a good step, I think. Yeah, there's an interesting analogy between the functionalism that we were speaking about in consciousness. I mean, even in our own research domain, we have the function of ethics, and we have linguists, and we have all sorts of different people. And that is the blind man and the elephant. And, you know, I tend to believe that even though the views from these diverse folks are inconsistent, diversity is very important. Oh, incredibly important. Intellectual diversity is, you know, every kind of diversity is important. And intellectual diversity is immensely important. And having these conversations, these interdisciplinary conversations is absolutely, you know, essential. So at least if people are talking to each other, that's a really, really positive thing, I think. Fantastic. Now, we invited Chalmers on our podcast after Ilya Sootskeva's tweet. And by the way, Chalmers took a very functionalist approach to talking about consciousness. But I guess, after that tweet, everyone in the community started thinking about and talking about consciousness. So maybe let's just start from that tweet. How did you find it? Sure. Yeah, okay. So the tweet was, so Ilya Sootskeva said, it may be that today's large language models are slightly conscious. And then I replied, tweeted back in the same sense that may be a large field of wheat is slightly pasta. And that, in fact, was actually, I mean, I've got a fair number of Twitter followers, and that was the most engaged tweet I've ever sent out. And, you know, and, you know, it got celebrity likes, Hannah Fried retweeted it, and, you know, only as my kind of comment. And so, so, but that does kind of summarize sort of what I think about, about what he said at that point. But then, but then I after tweeting my, my flippant response, I then I was violating all my own Twitter rules in in just sending back a flippant response, because I generally don't do that. I would rather kind of, you know, be professional, engage professionally. And so I thought it's very important to follow that on with a, you know, with a little explanation of why, you know, why I thought that it wasn't really appropriate to speak about today's large language models in those terms. Yeah. And for me, the number one thing is to do with embodiment. So, so as I see it, embodiment is a kind of prerequisite for for us being able to use that, that word, use words like consciousness and so on, you know, in the way that we do in the normal everyday way of talking. So, so, you know, it's only in the presence of something that that inhabits our world. And by inhabits, I don't mean just has a physical, you know, like a computer is obviously a physical thing in our world, but inhabits our world means that, you know, walks around in her own swims or jumps or flies or whatever, but is is is inhabits the same world as us and interacts with it and, and, and, and you know, and interacts with the objects in it and with other, with other creatures like ourselves. So, so that to my mind, that is, that's the, that's so, so, so in that paper conscious exotica, I think I use this phrase trans channeling Wittgenstein that that only in the context of something that that exhibits purposeful behavior, do we speak of consciousness. And the way that that's phrased, there is kind of, you know, so trying to channel Wittgenstein's style of saying things, because you notice that he's saying that he's making what he's saying is actually he's talking about the way we use the word. So he's not making a metaphysical claim. This is essential. He's saying that this is just this is the circumstances under which we use this word. So we use this word in the context of fellow creatures, basically. And so, so that's the kind of the starting point. So a large, and of course, we, of course, we talk to people on the telephone and over the internet and so on. And we don't, you know, we may not, you know, we can't see them or anything. So we, but, but, but we still, we know that there is, you know, or we assume, we've always been able to assume up to this point that there is a fellow creature at the other end. And that's the kind of grounding for kind of thinking that way and using using that word. Now that is absent in large language models. So large language models do not inhabit the world that we do. Now, I mean, we have to caveat that because, of course, it's possible to embed a large language model in a, you know, in a, we always do embed it in a larger system. It might be very simple embedding. It might be just a chatbot, or it might be much more complicated, like it might be be part of a system that enables a robot to kind of move around and interact with the world and take instructions and so on. So there was a great, some great work from Google with their Palm Seican robot, for example, where there's this embedded large language model. So, so, so there you're kind of moving in a, in a direction where maybe where these, where these words, you know, the prerequisites, you know, for, for, well, actually, I want to be careful what I say here. Sorry. Sorry. Because it's so easy to say something that's going to can be misinterpreted, right? Yes. But, but we can imagine that, that we can imagine that requirement being met for, for, for not, not, it doesn't mean it wouldn't be a sufficient condition for using those words, but at least it would, you'd meet the necessary conditions, right? Yes. But the large language models by themselves do not meet even, they're not even candidates. Yes, I agree. And we, there's so many things we can do here, because we can, we can talk about embodiment in general. I mean, as I understand it, Rodney Brooks kind of started the phenomenon of thinking about a representationalist view of artificial intelligence or, or rejecting, rejecting a representation. Rejecting. Yeah. So, so Rodney Brooks thought that we should use the world as its own best representation, which is absolutely fascinating. Yeah. And then you, maybe you might be thinking about the embodiment of you in a similar style of Wittgenstein. So it's a matter of complexity, and it's also a matter of encapsulation, which is fascinating. But, but also just to quote your paper, you said, although the language model component of SACAN provides natural language descriptions of low level actions, it doesn't take into account what the environment actually affords the robots. And there's this whole affordance thing as well. So, I mean, how do you think about embodiment? So, so the way I see it is that is that the, you know, the one exemplar we have as of, you know, the end of 2022 of something that we really can describe as, as, as, as intelligent as generally intelligent is, is the biological brain, biological brains of humans, but also of other animals. And the biological brain, you know, at its very, it's very kind of nature is it's there to help a creature to move around in the world, to move, right? It's there to move, help to guide a creature and help it move in order to help it survive and reproduce. That's what brains are for. So that's what that from an evolutionary point of view, that's that they developed in order to help a creature to move. And they are so they and they are, you know, they're the bit that's comes between the sensory input and the motor output. And as far as you can cleanly divide these things, which maybe you can't, but I mean, so and so that's that's their purpose is to intervene in the sensory motor loop in a way that benefits the organism. And everything else is on built on top of that. So, so, so the capacity to recognize objects in our environments and categorize them and the ability to kind of manipulate objects in the environment, pick them up and so on. And all of that is there, you know, initially to help the, the, the organism to survive. And, and, and, you know, and that's what brains brains are there for. And then, then when it comes to, you know, the ability to work out how the world works and to do things like figure out how to gain access to some item of food that's difficult to get hold of, then all kinds of cognitive capabilities might be required to understand how you get inside a, you know, a shell or something to get the fruit inside it or something like that, complex cognitive abilities, that sort of. And then, you know, evolutionary evolution has developed a lot of more and more and more complex cognitive cognition until we get to language and, you know, we need to interact with each other because that that's all very much a part of it, the social, social side of it. And then language is part of that. So as I see it, it's all built on top of this fundamental fact of the embodiment of the animal and the organism. So that's in the biological case. So that's sort of our starting point. So, and so that seems to me to be the, the most natural way to, to understand the very nature of intelligence. Could I frame it? I think I didn't, I didn't frame it very well. I mean, Melanie Mitchell recently had a paper out talking about the Four Misconceptions and one of them, of course, citing Drew McDermott was the wishful mnemonics and the anthropomorphization which, which you've basically spoken about. But her fourth one was about embodiment. And she spoke about this in her book as well. She said that one of the misconceptions of AI is that people have this notion of a pure intelligence, you know, something which works in isolation from the environment. And you're talking about social embeddedness and embodiment. And I guess my point with the complexity argument is I'm saying that the brain itself doesn't actually do everything. It kind of works as part of a bigger system. Oh, I see what you mean. Yes. Okay. Yeah. Yeah. So there's, so of course, there's, I mean, I noticed in one of your previous interviews with Andrew Lampinen, you mentioned the three E's frame, we're called four E's these days. And of course, that's very much part of it is the, is the idea that, you know, there's the extended mind, we use the environment, you know, as, as, as a kind of memory, for example, we deposit things in the environment, writing, you know, as an example and so on. And then there's, people talk about morphological computation, I'm sure you're familiar with that. Well, so that's the idea that the very shape of our bodies, you know, is, is, is, you know, could. So, so, so sometimes, you know, the aspects of intelligence are actually outshort outsourced into the physical shape of, of your body. So where you might think about designing a robot, where you, where you put a lot of work into the control aspect of it, so that it's, so that it can kind of walk in this very carefully engineered ways that it's always permanently stable, or alternatively, you can make a body that is naturally sort of stable, or maybe naturally unstable. And what you do is you kind of rely on the combination of the physics of it constantly falling with, with a control system that constantly restores balance. So, so, so, you know, so that's, that's, I mean, this is very much a Brooks type perspective, and people picked up on Brooks's ideas and extended them in this sort of way. So I think that's, I think that's a very natural way of thinking. But in a way that this gets to the, to the complexity argument, because I guess what I'm saying is that life is much more brittle than anyone realises. You were just pointed to some sources of brittleness that most people never would have thought of, which is, which is fascinating. So I think there's a very important relationship between embodiment and language. And this also brings us back to Wittgenstein as well. So, so for us as humans, language is inherently an embodied phenomenon. It's, it's, it's something that is, it's a social practice, something that take that it's a phenomenon that occurs in the context of other language users who inhabit the same world as we do. And where we have kind of like joint activities, we're triangulating on the same world, and we're acting on the same world together. And that's the that's what we're talking about when we use language. So there's this, so that, that's an inherently convincing view of language. And I think it's deeply profoundly correct view of language. That's, that's what, that's what language is there for us is so that we can talk about the same things together so that we can, our collective activity is, is, you know, is, is, is organised to some extent, thanks to language. So that's, so I think that's a really important perspective on language is Wittgenstein perspective. And, and embodiment is at the heart of it, embodiment and inhabiting the same world as our other language users. And, you know, that's the way we learn language, we learn language by being around other language users like our parents and carers and, and, and peers. And, and that's again a very important aspect of the nature of human language. Now large language models, they learn language in a very different way indeed, they do not inhabit the same world as us, they do not kind of sense the world in the way that we do, they don't learn language from, from other language users, from their peers in the way that we do. But rather, you know, will we know how large language models work, there's trained on a very, very large corpus of textual, of textual data. So an enormous corpus of textual data so bigger than any human is likely to encounter in a, you know, by the time they become a proficient language user at a young age. And what they're trained to do is, is not to kind of engage in activities with other language users, but to predict what the next, you know, what the next token is going to be, which is a very, very different sort of thing. So they're very, very different sorts of things. And the, and the role of embodiment is really really important in this difference, I think. Yes, absolutely. When I spoke with Andrew Lampinen, he's really, really interested in the grounding problems. I mean, would you mind just speaking about that a little bit before we go into your paper? Yeah, absolutely. Yeah, yeah. So of course, this goes back to a great paper by Stephen Harnad back in, I think, 1999 or 1998. Yeah, the one and only. Yeah, the one and only on the symbol grounding problem, it was called. And, and, and, you know, he does argue broadly that the symbols in AI systems, the kinds of AI systems he was thinking about at the time were kind of, you know, sort of expert systems say or something like that. And the symbols there are not grounded, they're provided by the human programmers and they're just sort of typed in. Whereas for us, for us, the words we use, those symbols are grounded in, in our activity in the world. So that when we use the word dog, that's because we've seen dogs. And we've talked about dogs with other people who've also seen dogs. And we've seen dogs in lots of different circumstances. And we've also seen cats and, and, and, and, and dog bowls and bones and many other things that all kind of contextualize that. But all of that, that that is kind of grounded in the real world in our and in our perception of the things in question. So that so that's this. So that's what sort of is meant by grounding or that at least that's the original meaning of the word grounding from Stephen Harled's paper. And I think that's a really, really important concept because, because, you know, in an important sense, large language models, the symbols that are used in large language models are not really grounded in that kind of way. Now this, you know, I should be absolutely clear that large language models are immensely powerful and immensely useful and, and so that, you know, so, but it's interesting that to what extent the lack of grounding here that we have in today's large language models, you know, might affect how good they are. So, so they, so they are prone to kind of, you know, hallucinating and, and, and, and confabulating and, and if you look at multimodal language models that maybe we'll talk about an image that you present to them, then, you know, they, you can have a very interesting conversation, but sometimes they'll go off pieced and start talking about things that are not in the image at all and as if they are. And that's sort of because due to a kind of, I would say lack of grounding this so that so the words are not kind of grounded in the images in, in quite the way that we would like. So that's, it's an important topic of research, I think. Yes, indeed. And although some people do believe there's this magical word called emergence and possibly some emergent symbol grounding might be possible, maybe, maybe, shall we just put that to bed before we introduce your, yeah, sure. Well, well, I mean, emergence is, is, I think is, is a really important concept. And I, and I think, you know, we do see a tremendous amount of, of very powerful emergence, I think in today's large, large language models. So, so, so, you know, even though they're, they're, so they're basically trained to do next word prediction. Or I mean, to be clear, I suppose I should have made this maybe a bit clearer in the paper, but of course, it's not always next word prediction, because there are different models learned to actually, you know, predict what's in the middle of a, of a sequence rather than kind of generally, you know, they're interested in, in, in, in, in, let's take the next word prediction case as canonical. So, so they're, so they're, so they're trained to just to do next word prediction. Now, the astonishing thing is, as I think GPT three showed us, is that, is that just that capability, if it's sufficiently powerful, can be used to do all sorts of extraordinary things. Because if you provide, you know, the prompt that describes, you know, describes some kind of complicated thing, you know, situation, like, you know, I need to tile my floor and my floor is shaped like an L and it's 20 meters long and three meters. Well, you know, you start to describe this thing, you know, and each, each tile is, is 20 centimeters square, how many tiles will I need? And, and some large language model will come back and tell you, you need 426 tiles, whatever. And it's correct, right? Well, this is astonishing, because it was only trained to do next word prediction. And so there's a kind of emergent capability there. Now, there's a sense, of course, in which it still is just doing next word prediction, because in the vast and immensely complex distribution of tokens in human text that's out there, then the correct answer is actually the thing that's most likely to come up. And that's, but it's got to discover a mechanism for producing that, right? And so that is where the emergence comes in. And I think that, you know, these capacities are astonishing, the fact that they, that it can discover mechanisms, you know, emergently that will do that sort of thing. Yes. And maybe I shouldn't have used the word magic with it with emergence, I'm a huge fan of emergence. And, and as you say, the decode is trained to predict the next token or the denoising autoencoders to, to, let's say fill in the gaps in the middle. And I guess there are different ways of thinking about emergence. So there's weak emergence, which might be thought as computational irreducibility, or a surprising macroscopic change, or strong emergence when it's not directly deducible from truths and the lower level to make, you know, lots of things. Yeah, yeah, the different senses of it, yeah. Exactly. But I guess my point is that remarkably, it's trained on something quite trivial. So all of this is about convention, right? All of this is about what's, what, what is the, what is a good way to use words, right? So I don't, so I don't think, you know, I'm not making metaphysical claims about, about, about these things. So it's all about what, you know, when is it appropriate to use words, to use certain words? And, and because when, when this becomes problematic is when they're philosophically difficult words, like beliefs and thinks and so on. Now, when it comes to reasoning, so, so I do think that we, I do think it's not unreasonable to, to, to use that term to describe what some of the, these models do today. And that's partly because of the content neutrality of, of, of reasoning. So, so, so, so a lot of the argument, or a lot of the discussion in the paper comes back to this sort of whole embodiment thing, really. And, and I'm, I'm saying, well, you know, in the kind of like ordinary way we use the word believes, well, it gets, it gets complicated because we do use the word believes in this intentional stance way to, to talk about ordinary everyday things. We say, oh, my, my, you know, my, my car clock thinks that it's British summertime, you know, you know, because we, and then, but then you'd say, then somebody says to you, what you, what you mean, your, your car clock and think, you say, no, obviously, I didn't mean that it can think, it's just a turn of phrase, you know, but when we, when we get to these large language models, we start to use the words like thinks and believes and so on, because they're so powerful, it starts to get ambiguous and yours, and your, and, you know, and some people will say, well, actually, I really didn't mean that it can think or that it believes. So I'm, so I'm, I'm interested in this, when things get difficult in this respect. And could, could you tease apart that work? So you resist anthropomorphic language in terms of belief, knowledge, understanding, self or even consciousness, but less so with reasoning. And I, my intuition is that reasoning rather depends on those things that I just said before. Well, I, so I think it doesn't because, but this is perhaps, this is just maybe in a kind of formal logic sense, because, because reason, because logic is content neutral. So if I tell you that every, could you just explain what you mean by that? Okay. So, so Lewis Carroll has all these wonderful kind of like nonsense syllogisms, right? Where he, where, you know, he says, oh, if all elephants like custard and, you know, Jonathan is an elephant, you know, Jonathan likes custard, and, you know, all kinds of things like that. And it's all sort of nonsense. And he has this big complex, complicated ones. Similarly, I could tell you that all, all sprung forths are plingy, and, and Juliet is a sprung forth. Therefore, Juliet is, is a springy, right? And I've no idea what any of those things mean, but the, the, but it's because it, because it, for the pure form of the reasoning, you don't have to know what they mean. It's just about the logic. So, so in that sense, you know, it just in the way that a theorem prover can do logic, then so can a large language model do logic. So in that sense, I think large, it is reasonable to use the word reasoning in that logical sense in the context of large language models. I don't think that's a problem. Of course, we may think that they do it badly, or they do it well, or that's a whole other thing, right? But, but at least the word is potentially applicable, right? Yes. Now, belief, I think, you know, I think at the moment is a, is a, is a different kettle of fish, because to really have a holder belief, it's, it, it's, it's not content neutral, right? So if you, if I believe to use the example in my, in, in my paper, if I believe that Barundi is to the south of, of, of Rwanda, well, whether that is the case or not, it does depend upon facts that are out there in the world. And then to, to really have a belief, at least you've got to be able to somehow try and kind of justify those facts, or at least, and you got to be at least built in such a way that you can, you know, interact with the external world and do that sort of thing, right? And, and verify that something is true or false or do an experiment or, you know, or ask someone or, you know, you've got to go outside yourself, right? We go outside of ourselves and, and in order to establish whether something a belief is true or not. And so, you've got to at least be capable of doing that. Whereas large language models, the bare bones, large language model is not capable of doing that at all, right? Now you can embed it in a larger system. This is a really important distinction that I've tried and make over and again in the paper. I talk about the bare bones, large language model. So you can take the, so, so, and whenever a large language model is used, it's not the bare bones, large language model, which just does sequence prediction, but it's embedded in a larger system. When we embed it in a larger system, well, that larger system maybe could consult Wikipedia or maybe it could be part of a robot that goes and investigates the world. So that's a whole other thing. But then you have to look at each case in point and, and, and ask yourself whether it's a, whether, you know, whether we really want to use that word in, in, in anger, you know, as in, in its full sense, rather than just in the intentional stance sense of a kind of figure of speech. So, and so in the case of, of, of, of like chatbots, for example, today's chatbots, not really appropriate, I would say. We're not using the word in the way that we, in the full blown sense that we use it, where we talk about each other. Fascinating. Okay, well, let's get on to intentional stance. So you said that it's a useful way of thinking about artificial intelligence, allowing us to view computer programs as intelligent agents, even though they may lack the same kind of understanding as a human. And then you cited the case of Bob and Bot. The, the word no was used differently in the two cases. And the word of Bob, it was used in the traditional sense. For bot, it was used in a metaphorical sense. So it kind of like, it's just distinguishing what it, what it means to know, you know, for humans and, and, and for machine. So I think it's, it's useful to think about something like Wikipedia. So, so we might ask the question, does Wikipedia know that Argentina has won the 2022 World Cup? And just immediately after the event, you know, it probably doesn't, it's not recorded in Wikipedia. And somebody might say, Oh, Wikipedia doesn't know yet that the Argentina have won. And so when we use the word like that, you know, nobody's going to kind of say to them, say to somebody who uses that word, hey, you know, I don't think you should use the word knows there. And you know, that would, you know, you should be a bit more sort of sensible. I mean, it's, it's fine to kind of use, I think, these kinds of words in this ordinary, every day sense. And we do that all the time. And that's sort of, particularly, particularly in the case of computers, that's adopting what Dandenek calls the intentional stance. So we're, so we're, we're interpreting something as, as, as, as having beliefs, desires and intentions, because it's a kind of convenient shorthand. And especially if you've got something that's a bit more complicated, like say your car sat down for something or you're, you're, you know, you're sat up on your, on your phone, then it sort of makes, makes sense to use those words. It's a, is a convenient shorthand. And it helps us to kind of talk about them, right? And without getting overly complicated, without knowing the underlying mechanisms. But there's an important sense in which we don't mean it literally. So you know, in the case of Wikipedia, you can't, you couldn't go up to Wikipedia and pat it on the shoulder and say, hey, Argentina have won. And there's no way, you know, right, I want to be a, you know, and, and, and, and, and all, and all the things that, that go with us as humans actually knowing things. And it's just a turn of phrase. Now, things get sort of interesting with large language models, and with large language model based systems and the kinds of things that we're starting to see in the world, because we're starting to get into this kind of blurry territory where it, we're blurring between the intentional stance and, and, you know, meaning the meaning it literally. And this is where we need to be really, really kind of careful, I think. So at what point does do things shade over into where it's legitimate to use that word, you know, literally, in, in the context of something that we've built, you know, I don't think we're at that point yet. And, and we need to be very careful about, about using the word as if we were using it literally. You know, that's the sort of anthropomorphization, because the problem is that we can then impute capacities to the thing and, and, and, or even, you know, empathy say that just isn't there. Yes. And I suppose we could tease apart knowledge. So it justified true belief from knows, because knows that it brings all this baggage of intentionality and agency and anthropomorphization. But you had Chomsky, you've had Chomsky on. I can tell you a story about that. I mean, the recording messed up. So when we were interviewing him, we were only getting bits and pieces. And we had to deep fake him. We had to, we had to regenerate the interview. Oh, really? And he was saying in the entire interview, how much he hated deep learning and how useless it was. And then we used deep learning to rescue his interview. And he gave us his permission to publish it. That is wonderful. So it's quite ironic. But no, he always says it's wonderful for engineering, but not a contribution to science. Yes, sure. Yeah. Yeah. He said, I like bulldozers too. They're good for clearing the snow, but they're not a contribution to science. So who else have you had? I mean, you've had a lot of people on. I listened to Andrew's one, by the way. It's Andrew Lampinen. Yes, he's great. So he's great. Andrew is somebody I do work with quite closely. So it was interesting listening to him because Andrew had quite a big influence on this paper, by the way. Oh, okay. But I think I might have had a bit of influence on him as well, to listening to him. I think so. Because that interview was just after he'd read, and he read my paper, gave me lots of comments. And we had a lot of discussion about it. And that interview, looking at the recording date, was sort of just after this. And it's interesting. I mean, he was very circumspect in some of the things he said. Yeah, it was very interesting because I think the influence has maybe gone both ways. Yes. Which is nice. I don't know. I mean, I can't be sure of that. I think there's a huge similarity. Yeah. I was thinking that, actually, just when you were speaking. But it's funny because we've spent a lot of time arguing with each other about it. And I often feel like we're coming from very different perspectives on this. But in fact, I think there's convergence, really. What are your areas of disagreement? Well, you see, I would have thought that Andrew would have been more on the side of, we can do things without embodiment, and without grounding, or to kind of take grounding in a more liberal sense. Because some people would talk about grounding, so they say that large language models, they are grounded. Prompt Engineering is the process of using prompt prefixes to allow LLMs to understand better. So the context and the purpose of a conversation in order to generate more appropriate responses. What do you think is going on with Prompt Engineering? Yeah. Well, yeah. So you let's probably let slip a phrase there. So the process of allowing the models to understand better is what you're better. Of course, I don't think guilty as charged. I don't think I don't think that's the right way of characterizing it at all. So I mean, I think the whole thing of Prompt Engineering is utterly fascinating. And it's something that's entered our world as AI researchers, very prominently, just in the last two years. And it's amazing. Of course, we have Prompt Engineering in the context of large language models. We also have Prompt Engineering in the context of the generative image models as well, like Dali and so on. And that's really fascinating as well, how by engineering the Prompt to be just the right sort of thing, you can coax the model into doing something which you might not otherwise do. And it's a great example of how alien these things are. Because if you were giving a human being the same instructions, then you wouldn't necessarily do quite what you do with either an LLM or an image model in order to get it to do the thing that you want it to do. You have to kind of get into the zone with these models and figure out kind of what strange incantations are going to make it do the things that you want it to do. Now, I think an interesting thing is that we may be looking at a moment, a very short moments in the history of the field where Prompt Engineering is relevant. Because if language models become good enough, then we're not going to need to talk to them in this weird way, engineer the Prompt to get them to do what we want them to do. It's going to be a lot easier. Anyway, so maybe that will be the case. I mean, that makes a lot of sense that that will be the case as they get better. But at the moment, you can use a strange incantation like thinking steps and suddenly the large language model will be much more effective on reasoning problems than it was if you didn't use the incantation thinking steps. So that's really fascinating. So what's going on there? Well, I mean, I think what's going on there is that we have to again bear in mind that what the model is really trained to do is next word prediction. But we have to remember that it's doing next word prediction in this unimaginably complex distribution. So we have to remember that it's not just the distribution of what a single human would, the distribution of the sequence of words that a single human will come out with, but of all the sort of text of millions of humans on the internet, plus actually a load of other stuff like code and things which we don't come out with in ordinary everyday language. Well, people do it deep mind a bit, but that's deep. So it's this unimaginably complex distribution. And so I think what's happening with prompt engineering is that you're sort of channeling it into some portion of the distribution. So you're queuing it up with a prompt. And this kind of context is putting it into some portion of this distribution. And that is what's going to enable it to do something different than it would have done if you had a different set of words. And that would have put it in a different part of the distribution. So you're kind of finding the bit of this unimaginably complex distribution. You're finding the bit of it that you want to then concentrate on. Yeah, so intuitively, I agree, because I think there's two ways of looking at this. So I agree with you that they are statistical language models. I'm also a fan of the spline theory of neural networks, which is this idea that you just kind of tessellate the ambient space into these little affine polyhedra. And it's a little bit like a locality sensitive hashing table. But that's quite, it's quite a simple way of looking at it, because you were talking about emergence before. And emergence is all about this paradigmatic surprise, a bit like the mind body dualism, if you like, there's something that happens up here, which is paradigmatically completely different to what happens down there. So on the one hand, we're kind of saying, oh, they're just simple interpolators or statistical models. But on the other hand, they really are doing something remarkable up here. So, so which is it? Which is it? Well, I mean, it's both, right? So, so, so, you know, if we want to understand these models in a in a in a more scientific way, which we surely do, you know, even if we're not, even if we're not engineering them in an old fashioned sense of engineering them, but, but rather they kind of, you know, emerge from the, from the learning process, we still want to reverse engineer them to try and get as great as as as comprehensive a scientific understanding of these things as possible. So, so we want to understand it all these levels, right? We, of course, the foundation of that understanding is that we need to understand the actual mechanisms that we've programmed in there, right? So, though, you know, so you've that's essential, you want to, you know, if you want to really understand these things, you've got to understand transformer architectures, the different kinds of transformer architectures that you've got, the, you know, what happens when you use kind of different parameter settings, whether it's sparse or dense, whether it's a decoder only architecture, or how you're doing the tokenization, how you're doing the embedding, when all of these things are essential to understanding, you know, and that's all at the absolutely at the engineering level. So we want to understand all of that. But then we can do a whole load of reverse engineering, you know, at another level, and do the sort of thing that the people at Anthropic AI have done, for example, with, with these induction heads and, and, and, and understanding in terms of transformers in terms of residual streams and induction heads, which I think is fabulous work. So that kind of thing is looking, it's still quite a low level, but it's kind of the next level up, and explaining a little bit about how these things work, and work along those lines, I think is like really essential. And then the more complex these things are, the, you know, the heart, the more we need to kind of ascend these levels of understanding and, and, and, and, you know, and I hope that we can, but I mean, there's no one that is the right one. It's, you want to understand that things are all levels. Yeah, different levels of description. You said something before, which really interested me. You said when the language models get good enough, maybe we won't need the prompts anymore. And I'd love to explore that duality, because it's a similar duality to how we talk about embodiment, you know, you can think of the language model being embodied in the prompt in, in some sense. So maybe we'll never get rid of the prompt. But just to think about these prompts, I think about them as a new type of program interpreter. And there are some remarkable examples of scratch pad and chain of thought and even algorithmic prompting for getting insane extrapolative performance on lots of, you know, standard reasoning tasks. Yeah, yeah. And, you know, these, these models are not Turing machines, they're finite state automatics. So there are limits to what we can do. But I guess what I'm saying is the prompt seems like it's not going away anytime soon. Yeah. So I think that I don't think the prompt is going to go away. But I think that the, and who knows, right? But, but I think that prompt engineering as a whole kind of thing in itself, you know, may, it may not be, you know, people talk about that as being a kind of a whole new job description as prompt engineer. And so that, that as a whole new job description, I'm not quite sure how long exactly that will last because, because prompt prompting may be just, you know, interacting with a thing in a much more natural language way in the way we would with another person, right? So, you know, I don't, I don't, when I, when I, I don't have to kind of think of some peculiar incantation in order to, you know, in order to get, you know, my colleagues to kind of help me on, on something or to, you know, to cook a meal together with somebody, we just, we just use our natural kind of forms of communication. Of course, of course, it does involve, you know, discussion and negotiation, but it's in this, it's just the same as we use with other humans, right? So, so it may be that, rather than it being a peculiar thing in itself with all these funny phrases that just work for peculiar eccentric reasons, that it may be much more natural. Amazing. Professor Shanahan, thank you so much for joining us today. Indeed, and thank you for the invitation. It's been lots of fun. Absolute honor. Absolute honor. Why?", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.36, "text": " Murray Shanahan is a professor of cognitive robotics at Imperial College London and a senior", "tokens": [50364, 27291, 25536, 21436, 307, 257, 8304, 295, 15605, 34145, 412, 21395, 6745, 7042, 293, 257, 7965, 50632], "temperature": 0.0, "avg_logprob": -0.09250611179279832, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003167447866871953}, {"id": 1, "seek": 0, "start": 5.36, "end": 11.040000000000001, "text": " research scientist at DeepMind. He graduated from Imperial College with a first in computer science", "tokens": [50632, 2132, 12662, 412, 14895, 44, 471, 13, 634, 13693, 490, 21395, 6745, 365, 257, 700, 294, 3820, 3497, 50916], "temperature": 0.0, "avg_logprob": -0.09250611179279832, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003167447866871953}, {"id": 2, "seek": 0, "start": 11.040000000000001, "end": 18.400000000000002, "text": " in 1984 and obtained his PhD from King's College in Cambridge in 1988. He's since worked in the", "tokens": [50916, 294, 27127, 293, 14879, 702, 14476, 490, 3819, 311, 6745, 294, 24876, 294, 27816, 13, 634, 311, 1670, 2732, 294, 264, 51284], "temperature": 0.0, "avg_logprob": -0.09250611179279832, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003167447866871953}, {"id": 3, "seek": 0, "start": 18.400000000000002, "end": 24.080000000000002, "text": " fields of artificial intelligence, robotics and cognitive science. He's published books such as", "tokens": [51284, 7909, 295, 11677, 7599, 11, 34145, 293, 15605, 3497, 13, 634, 311, 6572, 3642, 1270, 382, 51568], "temperature": 0.0, "avg_logprob": -0.09250611179279832, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003167447866871953}, {"id": 4, "seek": 0, "start": 24.080000000000002, "end": 29.44, "text": " Embodiment and the Inner Life and the Technological Singularity. His book Embodiment and the Inner", "tokens": [51568, 24234, 378, 2328, 293, 264, 36705, 7720, 293, 264, 8337, 4383, 7474, 1040, 507, 13, 2812, 1446, 24234, 378, 2328, 293, 264, 36705, 51836], "temperature": 0.0, "avg_logprob": -0.09250611179279832, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003167447866871953}, {"id": 5, "seek": 2944, "start": 29.44, "end": 34.56, "text": " Life was a significant influence on the film Ex Machina for which he was a scientific advisor.", "tokens": [50364, 7720, 390, 257, 4776, 6503, 322, 264, 2007, 2111, 12089, 1426, 337, 597, 415, 390, 257, 8134, 19161, 13, 50620], "temperature": 0.0, "avg_logprob": -0.06944543974740165, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0023212279193103313}, {"id": 6, "seek": 2944, "start": 35.92, "end": 40.64, "text": " Now Professor Shanahan is a renowned researcher on sophisticated cognition", "tokens": [50688, 823, 8419, 25536, 21436, 307, 257, 34065, 21751, 322, 16950, 46905, 50924], "temperature": 0.0, "avg_logprob": -0.06944543974740165, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0023212279193103313}, {"id": 7, "seek": 2944, "start": 40.64, "end": 45.92, "text": " and its implications for artificial intelligence. His work focuses on agents that are coupled to", "tokens": [50924, 293, 1080, 16602, 337, 11677, 7599, 13, 2812, 589, 16109, 322, 12554, 300, 366, 29482, 281, 51188], "temperature": 0.0, "avg_logprob": -0.06944543974740165, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0023212279193103313}, {"id": 8, "seek": 2944, "start": 45.92, "end": 51.6, "text": " complex environments through sensory motor loops such as robots and animals. He's also", "tokens": [51188, 3997, 12388, 807, 27233, 5932, 16121, 1270, 382, 14733, 293, 4882, 13, 634, 311, 611, 51472], "temperature": 0.0, "avg_logprob": -0.06944543974740165, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0023212279193103313}, {"id": 9, "seek": 2944, "start": 51.6, "end": 56.24, "text": " particularly interested in the relationship between cognition and consciousness and has", "tokens": [51472, 4098, 3102, 294, 264, 2480, 1296, 46905, 293, 10081, 293, 575, 51704], "temperature": 0.0, "avg_logprob": -0.06944543974740165, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0023212279193103313}, {"id": 10, "seek": 5624, "start": 56.24, "end": 61.92, "text": " developed a strong understanding of the biological brain and cognitive architectures more generally.", "tokens": [50364, 4743, 257, 2068, 3701, 295, 264, 13910, 3567, 293, 15605, 6331, 1303, 544, 5101, 13, 50648], "temperature": 0.0, "avg_logprob": -0.08836158280520096, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.0012443746672943234}, {"id": 11, "seek": 5624, "start": 61.92, "end": 67.36, "text": " In addition Professor Shanahan is interested in the dynamics of the brain including metastability,", "tokens": [50648, 682, 4500, 8419, 25536, 21436, 307, 3102, 294, 264, 15679, 295, 264, 3567, 3009, 1131, 525, 2310, 11, 50920], "temperature": 0.0, "avg_logprob": -0.08836158280520096, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.0012443746672943234}, {"id": 12, "seek": 5624, "start": 67.36, "end": 72.96000000000001, "text": " dynamical complexity and criticality as well as the application of this understanding to", "tokens": [50920, 5999, 804, 14024, 293, 4924, 507, 382, 731, 382, 264, 3861, 295, 341, 3701, 281, 51200], "temperature": 0.0, "avg_logprob": -0.08836158280520096, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.0012443746672943234}, {"id": 13, "seek": 5624, "start": 72.96000000000001, "end": 78.24000000000001, "text": " machine learning. He's also fascinated by the concept of global workspace theory as proposed", "tokens": [51200, 3479, 2539, 13, 634, 311, 611, 24597, 538, 264, 3410, 295, 4338, 32706, 5261, 382, 10348, 51464], "temperature": 0.0, "avg_logprob": -0.08836158280520096, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.0012443746672943234}, {"id": 14, "seek": 5624, "start": 78.24000000000001, "end": 83.04, "text": " by Bernard Bars. We'll be talking about that on the show today which is based on a cognitive", "tokens": [51464, 538, 30116, 363, 685, 13, 492, 603, 312, 1417, 466, 300, 322, 264, 855, 965, 597, 307, 2361, 322, 257, 15605, 51704], "temperature": 0.0, "avg_logprob": -0.08836158280520096, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.0012443746672943234}, {"id": 15, "seek": 8304, "start": 83.04, "end": 88.24000000000001, "text": " architecture comprising a set of parallel specialist processes and a global workspace.", "tokens": [50364, 9482, 16802, 3436, 257, 992, 295, 8952, 17008, 7555, 293, 257, 4338, 32706, 13, 50624], "temperature": 0.0, "avg_logprob": -0.07020526685212788, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.003882040735334158}, {"id": 16, "seek": 8304, "start": 88.24000000000001, "end": 92.64, "text": " Professor Shanahan is committed to understanding the long-term implications of artificial", "tokens": [50624, 8419, 25536, 21436, 307, 7784, 281, 3701, 264, 938, 12, 7039, 16602, 295, 11677, 50844], "temperature": 0.0, "avg_logprob": -0.07020526685212788, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.003882040735334158}, {"id": 17, "seek": 8304, "start": 92.64, "end": 98.96000000000001, "text": " intelligence both its potential and its risks. His research has been published extensively", "tokens": [50844, 7599, 1293, 1080, 3995, 293, 1080, 10888, 13, 2812, 2132, 575, 668, 6572, 32636, 51160], "temperature": 0.0, "avg_logprob": -0.07020526685212788, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.003882040735334158}, {"id": 18, "seek": 8304, "start": 98.96000000000001, "end": 102.80000000000001, "text": " and he's a member of the External Advisory Board for the Cambridge Centre of the Study of", "tokens": [51160, 293, 415, 311, 257, 4006, 295, 264, 48277, 39816, 10008, 337, 264, 24876, 20764, 295, 264, 27039, 295, 51352], "temperature": 0.0, "avg_logprob": -0.07020526685212788, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.003882040735334158}, {"id": 19, "seek": 8304, "start": 102.80000000000001, "end": 108.56, "text": " Existential Risk and also on the editorial boards of Connection Science and Neuroscience of Consciousness.", "tokens": [51352, 2111, 468, 2549, 45892, 293, 611, 322, 264, 33412, 13293, 295, 11653, 313, 8976, 293, 1734, 8977, 6699, 295, 6923, 4139, 1287, 13, 51640], "temperature": 0.0, "avg_logprob": -0.07020526685212788, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.003882040735334158}, {"id": 20, "seek": 10856, "start": 109.28, "end": 115.60000000000001, "text": " Conscious Exotica Professor Shanahan wrote an article called Conscious Exotica", "tokens": [50400, 6923, 4139, 2111, 310, 2262, 8419, 25536, 21436, 4114, 364, 7222, 1219, 6923, 4139, 2111, 310, 2262, 50716], "temperature": 0.0, "avg_logprob": -0.14005411933450138, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.00400614133104682}, {"id": 21, "seek": 10856, "start": 115.60000000000001, "end": 123.04, "text": " in 2016 where he invited us to explore the space of possible minds, a concept first proposed by", "tokens": [50716, 294, 6549, 689, 415, 9185, 505, 281, 6839, 264, 1901, 295, 1944, 9634, 11, 257, 3410, 700, 10348, 538, 51088], "temperature": 0.0, "avg_logprob": -0.14005411933450138, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.00400614133104682}, {"id": 22, "seek": 10856, "start": 123.04, "end": 129.76, "text": " philosopher Aaron Sloman in 1984. Now this space is comprised of all the different forms of minds", "tokens": [51088, 29805, 14018, 6187, 4277, 294, 27127, 13, 823, 341, 1901, 307, 38062, 295, 439, 264, 819, 6422, 295, 9634, 51424], "temperature": 0.0, "avg_logprob": -0.14005411933450138, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.00400614133104682}, {"id": 23, "seek": 10856, "start": 129.76, "end": 135.6, "text": " which could exist from those of other animals such as chimpanzees to those of life forms that could", "tokens": [51424, 597, 727, 2514, 490, 729, 295, 661, 4882, 1270, 382, 18375, 48410, 279, 281, 729, 295, 993, 6422, 300, 727, 51716], "temperature": 0.0, "avg_logprob": -0.14005411933450138, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.00400614133104682}, {"id": 24, "seek": 13560, "start": 135.6, "end": 141.12, "text": " have evolved elsewhere in the universe and indeed those of artificial intelligences.", "tokens": [50364, 362, 14178, 14517, 294, 264, 6445, 293, 6451, 729, 295, 11677, 5613, 2667, 13, 50640], "temperature": 0.0, "avg_logprob": -0.09209978425657595, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0009109730017371476}, {"id": 25, "seek": 13560, "start": 141.12, "end": 147.76, "text": " Now in order to describe the structure of this space Shanahan proposes two dimensions, the capacity", "tokens": [50640, 823, 294, 1668, 281, 6786, 264, 3877, 295, 341, 1901, 25536, 21436, 2365, 4201, 732, 12819, 11, 264, 6042, 50972], "temperature": 0.0, "avg_logprob": -0.09209978425657595, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0009109730017371476}, {"id": 26, "seek": 13560, "start": 147.76, "end": 154.48, "text": " for consciousness and human likeness of the behavior. According to Shanahan the space of", "tokens": [50972, 337, 10081, 293, 1952, 36946, 442, 295, 264, 5223, 13, 7328, 281, 25536, 21436, 264, 1901, 295, 51308], "temperature": 0.0, "avg_logprob": -0.09209978425657595, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0009109730017371476}, {"id": 27, "seek": 13560, "start": 154.48, "end": 160.07999999999998, "text": " possible minds must include forms of consciousness that are so alien that we wouldn't even recognize", "tokens": [51308, 1944, 9634, 1633, 4090, 6422, 295, 10081, 300, 366, 370, 12319, 300, 321, 2759, 380, 754, 5521, 51588], "temperature": 0.0, "avg_logprob": -0.09209978425657595, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0009109730017371476}, {"id": 28, "seek": 16008, "start": 160.08, "end": 167.12, "text": " them. He rejects the dualistic idea that there's an impenetrable realm of the subjective experience,", "tokens": [50364, 552, 13, 634, 8248, 82, 264, 11848, 3142, 1558, 300, 456, 311, 364, 704, 268, 302, 424, 638, 15355, 295, 264, 25972, 1752, 11, 50716], "temperature": 0.0, "avg_logprob": -0.10681741693046655, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.010326097719371319}, {"id": 29, "seek": 16008, "start": 167.12, "end": 173.84, "text": " remember we were talking about Nagel's bat on the Charmer's show, insisting instead that", "tokens": [50716, 1604, 321, 645, 1417, 466, 18913, 338, 311, 7362, 322, 264, 4327, 936, 311, 855, 11, 13466, 278, 2602, 300, 51052], "temperature": 0.0, "avg_logprob": -0.10681741693046655, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.010326097719371319}, {"id": 30, "seek": 16008, "start": 173.84, "end": 180.32000000000002, "text": " nothing is hidden metaphorically speaking, citing Wittgenstein actually. Now Shanahan argues that", "tokens": [51052, 1825, 307, 7633, 19157, 984, 4124, 11, 48749, 343, 593, 1766, 9089, 767, 13, 823, 25536, 21436, 38218, 300, 51376], "temperature": 0.0, "avg_logprob": -0.10681741693046655, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.010326097719371319}, {"id": 31, "seek": 16008, "start": 180.32000000000002, "end": 187.12, "text": " while no artifacts exist today, which has anything even approaching human-like intelligence,", "tokens": [51376, 1339, 572, 24617, 2514, 965, 11, 597, 575, 1340, 754, 14908, 1952, 12, 4092, 7599, 11, 51716], "temperature": 0.0, "avg_logprob": -0.10681741693046655, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.010326097719371319}, {"id": 32, "seek": 18712, "start": 187.20000000000002, "end": 192.16, "text": " the potential for variation in artificial intelligences far outstrips the potential", "tokens": [50368, 264, 3995, 337, 12990, 294, 11677, 5613, 2667, 1400, 484, 372, 470, 1878, 264, 3995, 50616], "temperature": 0.0, "avg_logprob": -0.049434909572849024, "compression_ratio": 1.806930693069307, "no_speech_prob": 0.00020341557683423162}, {"id": 33, "seek": 18712, "start": 192.16, "end": 198.16, "text": " for variation in naturally evolved intelligence. This means that the majority of the space of", "tokens": [50616, 337, 12990, 294, 8195, 14178, 7599, 13, 639, 1355, 300, 264, 6286, 295, 264, 1901, 295, 50916], "temperature": 0.0, "avg_logprob": -0.049434909572849024, "compression_ratio": 1.806930693069307, "no_speech_prob": 0.00020341557683423162}, {"id": 34, "seek": 18712, "start": 198.16, "end": 205.44, "text": " possible minds may be occupied by non-natural variants such as the conscious exotica of", "tokens": [50916, 1944, 9634, 815, 312, 19629, 538, 2107, 12, 16296, 21669, 1270, 382, 264, 6648, 454, 310, 2262, 295, 51280], "temperature": 0.0, "avg_logprob": -0.049434909572849024, "compression_ratio": 1.806930693069307, "no_speech_prob": 0.00020341557683423162}, {"id": 35, "seek": 18712, "start": 205.44, "end": 211.6, "text": " which Shanahan speaks. Now ultimately Shanahan's exploration of the space of possible minds invites", "tokens": [51280, 597, 25536, 21436, 10789, 13, 823, 6284, 25536, 21436, 311, 16197, 295, 264, 1901, 295, 1944, 9634, 35719, 51588], "temperature": 0.0, "avg_logprob": -0.049434909572849024, "compression_ratio": 1.806930693069307, "no_speech_prob": 0.00020341557683423162}, {"id": 36, "seek": 21160, "start": 211.6, "end": 217.6, "text": " us to consider the possibility for human-like minds but also for those that are radically different", "tokens": [50364, 505, 281, 1949, 264, 7959, 337, 1952, 12, 4092, 9634, 457, 611, 337, 729, 300, 366, 35508, 819, 50664], "temperature": 0.0, "avg_logprob": -0.07473967200831363, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.00591001333668828}, {"id": 37, "seek": 21160, "start": 217.6, "end": 224.56, "text": " and inscrutable. He concludes that although we may never understand these alien forms of consciousness,", "tokens": [50664, 293, 1028, 10757, 32148, 13, 634, 24643, 300, 4878, 321, 815, 1128, 1223, 613, 12319, 6422, 295, 10081, 11, 51012], "temperature": 0.0, "avg_logprob": -0.07473967200831363, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.00591001333668828}, {"id": 38, "seek": 21160, "start": 224.56, "end": 228.4, "text": " we can still recognize them as part of the same reality as our own.", "tokens": [51012, 321, 393, 920, 5521, 552, 382, 644, 295, 264, 912, 4103, 382, 527, 1065, 13, 51204], "temperature": 0.0, "avg_logprob": -0.07473967200831363, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.00591001333668828}, {"id": 39, "seek": 21160, "start": 230.16, "end": 235.76, "text": " So Professor Shanahan has just dropped a brand new paper called Talking about large language models", "tokens": [51292, 407, 8419, 25536, 21436, 575, 445, 8119, 257, 3360, 777, 3035, 1219, 22445, 466, 2416, 2856, 5245, 51572], "temperature": 0.0, "avg_logprob": -0.07473967200831363, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.00591001333668828}, {"id": 40, "seek": 21160, "start": 235.76, "end": 240.95999999999998, "text": " in which he discusses the capabilities and limitations of large language models.", "tokens": [51572, 294, 597, 415, 2248, 279, 264, 10862, 293, 15705, 295, 2416, 2856, 5245, 13, 51832], "temperature": 0.0, "avg_logprob": -0.07473967200831363, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.00591001333668828}, {"id": 41, "seek": 24096, "start": 240.96, "end": 245.68, "text": " Now in order to properly comprehend the capacities and boundaries of these models,", "tokens": [50364, 823, 294, 1668, 281, 6108, 38183, 264, 39396, 293, 13180, 295, 613, 5245, 11, 50600], "temperature": 0.0, "avg_logprob": -0.06769093801808912, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.00014882920368108898}, {"id": 42, "seek": 24096, "start": 245.68, "end": 249.36, "text": " we must first grasp the relationship between humans and these systems.", "tokens": [50600, 321, 1633, 700, 21743, 264, 2480, 1296, 6255, 293, 613, 3652, 13, 50784], "temperature": 0.0, "avg_logprob": -0.06769093801808912, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.00014882920368108898}, {"id": 43, "seek": 24096, "start": 249.92000000000002, "end": 255.28, "text": " Humans have evolved to survive in a common world and have cultivated a mutual understanding", "tokens": [50812, 35809, 362, 14178, 281, 7867, 294, 257, 2689, 1002, 293, 362, 46770, 257, 16917, 3701, 51080], "temperature": 0.0, "avg_logprob": -0.06769093801808912, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.00014882920368108898}, {"id": 44, "seek": 24096, "start": 255.28, "end": 261.2, "text": " reflected in their ability to converse about convictions and other mental states. Conversely,", "tokens": [51080, 15502, 294, 641, 3485, 281, 416, 4308, 466, 44757, 293, 661, 4973, 4368, 13, 33247, 736, 11, 51376], "temperature": 0.0, "avg_logprob": -0.06769093801808912, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.00014882920368108898}, {"id": 45, "seek": 24096, "start": 261.2, "end": 266.24, "text": " AI systems lack this shared comprehension, so attributing beliefs to them should be done", "tokens": [51376, 7318, 3652, 5011, 341, 5507, 44991, 11, 370, 9080, 10861, 13585, 281, 552, 820, 312, 1096, 51628], "temperature": 0.0, "avg_logprob": -0.06769093801808912, "compression_ratio": 1.633587786259542, "no_speech_prob": 0.00014882920368108898}, {"id": 46, "seek": 26624, "start": 266.32, "end": 271.04, "text": " circumspectly. Now prompt engineering is something that we've all become very familiar with,", "tokens": [50368, 7125, 82, 1043, 356, 13, 823, 12391, 7043, 307, 746, 300, 321, 600, 439, 1813, 588, 4963, 365, 11, 50604], "temperature": 0.0, "avg_logprob": -0.07699318124790384, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.004069214686751366}, {"id": 47, "seek": 26624, "start": 271.04, "end": 275.52, "text": " we've discussed it a lot on this show recently, and it's almost become a fact of the matter when", "tokens": [50604, 321, 600, 7152, 309, 257, 688, 322, 341, 855, 3938, 11, 293, 309, 311, 1920, 1813, 257, 1186, 295, 264, 1871, 562, 50828], "temperature": 0.0, "avg_logprob": -0.07699318124790384, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.004069214686751366}, {"id": 48, "seek": 26624, "start": 275.52, "end": 281.6, "text": " it comes to these large language models. It involves exploiting prompt prefixes to adjust", "tokens": [50828, 309, 1487, 281, 613, 2416, 2856, 5245, 13, 467, 11626, 12382, 1748, 12391, 18417, 36005, 281, 4369, 51132], "temperature": 0.0, "avg_logprob": -0.07699318124790384, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.004069214686751366}, {"id": 49, "seek": 26624, "start": 281.6, "end": 286.8, "text": " the language models to diverse tasks without needing any supplementary training, allowing for", "tokens": [51132, 264, 2856, 5245, 281, 9521, 9608, 1553, 18006, 604, 15436, 822, 3097, 11, 8293, 337, 51392], "temperature": 0.0, "avg_logprob": -0.07699318124790384, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.004069214686751366}, {"id": 50, "seek": 26624, "start": 286.8, "end": 293.68, "text": " more effective communication between humans and machines. Nevertheless, lacking a more profound", "tokens": [51392, 544, 4942, 6101, 1296, 6255, 293, 8379, 13, 26554, 11, 20889, 257, 544, 14382, 51736], "temperature": 0.0, "avg_logprob": -0.07699318124790384, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.004069214686751366}, {"id": 51, "seek": 29368, "start": 293.76, "end": 297.84000000000003, "text": " understanding of the system and its relationship to the external world,", "tokens": [50368, 3701, 295, 264, 1185, 293, 1080, 2480, 281, 264, 8320, 1002, 11, 50572], "temperature": 0.0, "avg_logprob": -0.0755350944843698, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0014547711471095681}, {"id": 52, "seek": 29368, "start": 297.84000000000003, "end": 302.72, "text": " it's difficult to be certain whether the arguments produced by a large language model", "tokens": [50572, 309, 311, 2252, 281, 312, 1629, 1968, 264, 12869, 7126, 538, 257, 2416, 2856, 2316, 50816], "temperature": 0.0, "avg_logprob": -0.0755350944843698, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0014547711471095681}, {"id": 53, "seek": 29368, "start": 302.72, "end": 310.08, "text": " are genuine reasoning or simply mimicry. Large language models can be integrated into a variety", "tokens": [50816, 366, 16699, 21577, 420, 2935, 31075, 627, 13, 33092, 2856, 5245, 393, 312, 10919, 666, 257, 5673, 51184], "temperature": 0.0, "avg_logprob": -0.0755350944843698, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0014547711471095681}, {"id": 54, "seek": 29368, "start": 310.08, "end": 316.24, "text": " of embodied systems even, such as robots or virtual avatars. However, this doesn't necessarily", "tokens": [51184, 295, 42046, 3652, 754, 11, 1270, 382, 14733, 420, 6374, 1305, 267, 685, 13, 2908, 11, 341, 1177, 380, 4725, 51492], "temperature": 0.0, "avg_logprob": -0.0755350944843698, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0014547711471095681}, {"id": 55, "seek": 29368, "start": 316.24, "end": 322.0, "text": " mean that these systems possess completely human-like language abilities. Even though the robot in the", "tokens": [51492, 914, 300, 613, 3652, 17490, 2584, 1952, 12, 4092, 2856, 11582, 13, 2754, 1673, 264, 7881, 294, 264, 51780], "temperature": 0.0, "avg_logprob": -0.0755350944843698, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0014547711471095681}, {"id": 56, "seek": 32200, "start": 322.0, "end": 328.08, "text": " SAKAN system is physically embodied and interacts with the real world, its language is still learned", "tokens": [50364, 318, 5340, 1770, 1185, 307, 9762, 42046, 293, 43582, 365, 264, 957, 1002, 11, 1080, 2856, 307, 920, 3264, 50668], "temperature": 0.0, "avg_logprob": -0.08766062572748974, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.004068871960043907}, {"id": 57, "seek": 32200, "start": 328.08, "end": 333.2, "text": " and used in a dramatically different manner than humans. So in summary, although Professor", "tokens": [50668, 293, 1143, 294, 257, 17548, 819, 9060, 813, 6255, 13, 407, 294, 12691, 11, 4878, 8419, 50924], "temperature": 0.0, "avg_logprob": -0.08766062572748974, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.004068871960043907}, {"id": 58, "seek": 32200, "start": 333.2, "end": 339.28, "text": " Shanahan concludes that large language models are formidable and versatile, they're fundamentally", "tokens": [50924, 25536, 21436, 24643, 300, 2416, 2856, 5245, 366, 41246, 293, 25057, 11, 436, 434, 17879, 51228], "temperature": 0.0, "avg_logprob": -0.08766062572748974, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.004068871960043907}, {"id": 59, "seek": 32200, "start": 339.28, "end": 344.8, "text": " unlike humans and we must be wary of ascribing human-like characteristics to these systems.", "tokens": [51228, 8343, 6255, 293, 321, 1633, 312, 46585, 295, 382, 39541, 1952, 12, 4092, 10891, 281, 613, 3652, 13, 51504], "temperature": 0.0, "avg_logprob": -0.08766062572748974, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.004068871960043907}, {"id": 60, "seek": 32200, "start": 344.8, "end": 350.48, "text": " We must find a way to communicate the nature of these systems without resorting to simple terms.", "tokens": [51504, 492, 1633, 915, 257, 636, 281, 7890, 264, 3687, 295, 613, 3652, 1553, 19606, 278, 281, 2199, 2115, 13, 51788], "temperature": 0.0, "avg_logprob": -0.08766062572748974, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.004068871960043907}, {"id": 61, "seek": 35048, "start": 350.48, "end": 355.36, "text": " This may necessitate an extended period of interaction and experimentation with the technology,", "tokens": [50364, 639, 815, 2688, 8086, 364, 10913, 2896, 295, 9285, 293, 37142, 365, 264, 2899, 11, 50608], "temperature": 0.0, "avg_logprob": -0.08709227121793307, "compression_ratio": 1.5906040268456376, "no_speech_prob": 0.0005786342662759125}, {"id": 62, "seek": 35048, "start": 355.36, "end": 361.44, "text": " but it's a fundamental step if we are to accurately portray the capabilities and limitations of", "tokens": [50608, 457, 309, 311, 257, 8088, 1823, 498, 321, 366, 281, 20095, 15676, 264, 10862, 293, 15705, 295, 50912], "temperature": 0.0, "avg_logprob": -0.08709227121793307, "compression_ratio": 1.5906040268456376, "no_speech_prob": 0.0005786342662759125}, {"id": 63, "seek": 35048, "start": 361.44, "end": 367.76, "text": " large language models. So anyway, without any further delay, I give you Professor Murray Shanahan.", "tokens": [50912, 2416, 2856, 5245, 13, 407, 4033, 11, 1553, 604, 3052, 8577, 11, 286, 976, 291, 8419, 27291, 25536, 21436, 13, 51228], "temperature": 0.0, "avg_logprob": -0.08709227121793307, "compression_ratio": 1.5906040268456376, "no_speech_prob": 0.0005786342662759125}, {"id": 64, "seek": 35048, "start": 368.96000000000004, "end": 374.32, "text": " Professor Shanahan, it's an absolute honor to have you on MLSD. Tell me a little bit about your", "tokens": [51288, 8419, 25536, 21436, 11, 309, 311, 364, 8236, 5968, 281, 362, 291, 322, 376, 19198, 35, 13, 5115, 385, 257, 707, 857, 466, 428, 51556], "temperature": 0.0, "avg_logprob": -0.08709227121793307, "compression_ratio": 1.5906040268456376, "no_speech_prob": 0.0005786342662759125}, {"id": 65, "seek": 35048, "start": 374.32, "end": 379.68, "text": " background. My background? Well, I've been interested in artificial intelligence for as", "tokens": [51556, 3678, 13, 1222, 3678, 30, 1042, 11, 286, 600, 668, 3102, 294, 11677, 7599, 337, 382, 51824], "temperature": 0.0, "avg_logprob": -0.08709227121793307, "compression_ratio": 1.5906040268456376, "no_speech_prob": 0.0005786342662759125}, {"id": 66, "seek": 37968, "start": 379.68, "end": 385.12, "text": " long as I can remember since I was a child, really, and I was very much drawn to it by", "tokens": [50364, 938, 382, 286, 393, 1604, 1670, 286, 390, 257, 1440, 11, 534, 11, 293, 286, 390, 588, 709, 10117, 281, 309, 538, 50636], "temperature": 0.0, "avg_logprob": -0.09279299350011916, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.012143824249505997}, {"id": 67, "seek": 37968, "start": 385.12, "end": 392.8, "text": " science fiction, by science fiction movies and books. And then I studied computer science", "tokens": [50636, 3497, 13266, 11, 538, 3497, 13266, 6233, 293, 3642, 13, 400, 550, 286, 9454, 3820, 3497, 51020], "temperature": 0.0, "avg_logprob": -0.09279299350011916, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.012143824249505997}, {"id": 68, "seek": 37968, "start": 392.8, "end": 398.4, "text": " right from when I was a teenager and got very much drawn into programming, was fascinated by", "tokens": [51020, 558, 490, 562, 286, 390, 257, 21440, 293, 658, 588, 709, 10117, 666, 9410, 11, 390, 24597, 538, 51300], "temperature": 0.0, "avg_logprob": -0.09279299350011916, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.012143824249505997}, {"id": 69, "seek": 37968, "start": 398.4, "end": 406.24, "text": " programming. I really did my 10,000 hours of programming experience when I was quite young", "tokens": [51300, 9410, 13, 286, 534, 630, 452, 1266, 11, 1360, 2496, 295, 9410, 1752, 562, 286, 390, 1596, 2037, 51692], "temperature": 0.0, "avg_logprob": -0.09279299350011916, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.012143824249505997}, {"id": 70, "seek": 40624, "start": 406.24, "end": 411.12, "text": " and I went on to do computer science at Imperial College London. That was my degree.", "tokens": [50364, 293, 286, 1437, 322, 281, 360, 3820, 3497, 412, 21395, 6745, 7042, 13, 663, 390, 452, 4314, 13, 50608], "temperature": 0.0, "avg_logprob": -0.13175312476822093, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.004397338256239891}, {"id": 71, "seek": 40624, "start": 411.92, "end": 416.56, "text": " And then still fascinated by artificial intelligence, I went on to Cambridge", "tokens": [50648, 400, 550, 920, 24597, 538, 11677, 7599, 11, 286, 1437, 322, 281, 24876, 50880], "temperature": 0.0, "avg_logprob": -0.13175312476822093, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.004397338256239891}, {"id": 72, "seek": 40624, "start": 417.52, "end": 422.88, "text": " and did my PhD in AI in Cambridge, very much in the symbolic school then.", "tokens": [50928, 293, 630, 452, 14476, 294, 7318, 294, 24876, 11, 588, 709, 294, 264, 25755, 1395, 550, 13, 51196], "temperature": 0.0, "avg_logprob": -0.13175312476822093, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.004397338256239891}, {"id": 73, "seek": 40624, "start": 424.56, "end": 430.24, "text": " And then I had a long affiliation with Imperial College, did my postdoc there and still in symbolic", "tokens": [51280, 400, 550, 286, 632, 257, 938, 14863, 399, 365, 21395, 6745, 11, 630, 452, 2183, 39966, 456, 293, 920, 294, 25755, 51564], "temperature": 0.0, "avg_logprob": -0.13175312476822093, "compression_ratio": 1.6183574879227054, "no_speech_prob": 0.004397338256239891}, {"id": 74, "seek": 43024, "start": 430.24, "end": 436.16, "text": " AI. And then at some point, I became a bit disillusioned with symbolic AI and I kind of", "tokens": [50364, 7318, 13, 400, 550, 412, 512, 935, 11, 286, 3062, 257, 857, 717, 373, 5704, 292, 365, 25755, 7318, 293, 286, 733, 295, 50660], "temperature": 0.0, "avg_logprob": -0.08079676968710763, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.04375506192445755}, {"id": 75, "seek": 43024, "start": 436.16, "end": 442.96000000000004, "text": " segued into studying the brain, which was the obvious example of actual general", "tokens": [50660, 8878, 292, 666, 7601, 264, 3567, 11, 597, 390, 264, 6322, 1365, 295, 3539, 2674, 51000], "temperature": 0.0, "avg_logprob": -0.08079676968710763, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.04375506192445755}, {"id": 76, "seek": 43024, "start": 442.96000000000004, "end": 450.96000000000004, "text": " intelligence that we have. And I think it was a good 10 years on an excursion into neuroscience", "tokens": [51000, 7599, 300, 321, 362, 13, 400, 286, 519, 309, 390, 257, 665, 1266, 924, 322, 364, 1624, 2156, 313, 666, 42762, 51400], "temperature": 0.0, "avg_logprob": -0.08079676968710763, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.04375506192445755}, {"id": 77, "seek": 43024, "start": 450.96000000000004, "end": 457.92, "text": " and computational neuroscience and that kind of thing. And then deep learning and deep", "tokens": [51400, 293, 28270, 42762, 293, 300, 733, 295, 551, 13, 400, 550, 2452, 2539, 293, 2452, 51748], "temperature": 0.0, "avg_logprob": -0.08079676968710763, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.04375506192445755}, {"id": 78, "seek": 45792, "start": 457.92, "end": 463.28000000000003, "text": " reinforcement learning happened in the early 2010s and AI started to get interesting again.", "tokens": [50364, 29280, 2539, 2011, 294, 264, 2440, 9657, 82, 293, 7318, 1409, 281, 483, 1880, 797, 13, 50632], "temperature": 0.0, "avg_logprob": -0.1114469952053494, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.012123015709221363}, {"id": 79, "seek": 45792, "start": 463.28000000000003, "end": 471.04, "text": " And I got very much back into it that way. And I was particularly impressed by DeepMind's", "tokens": [50632, 400, 286, 658, 588, 709, 646, 666, 309, 300, 636, 13, 400, 286, 390, 4098, 11679, 538, 14895, 44, 471, 311, 51020], "temperature": 0.0, "avg_logprob": -0.1114469952053494, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.012123015709221363}, {"id": 80, "seek": 45792, "start": 471.04, "end": 476.8, "text": " DQN, the system that learned to play Atari games from scratch. And I thought that was a fantastic", "tokens": [51020, 413, 48, 45, 11, 264, 1185, 300, 3264, 281, 862, 41381, 2813, 490, 8459, 13, 400, 286, 1194, 300, 390, 257, 5456, 51308], "temperature": 0.0, "avg_logprob": -0.1114469952053494, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.012123015709221363}, {"id": 81, "seek": 45792, "start": 476.8, "end": 484.0, "text": " step forward. And I really kind of went back to my roots and back to AI at that point.", "tokens": [51308, 1823, 2128, 13, 400, 286, 534, 733, 295, 1437, 646, 281, 452, 10669, 293, 646, 281, 7318, 412, 300, 935, 13, 51668], "temperature": 0.0, "avg_logprob": -0.1114469952053494, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.012123015709221363}, {"id": 82, "seek": 48400, "start": 484.0, "end": 488.08, "text": " Yeah, and I think we'll talk about DQN when we speak about your article on consciousness.", "tokens": [50364, 865, 11, 293, 286, 519, 321, 603, 751, 466, 413, 48, 45, 562, 321, 1710, 466, 428, 7222, 322, 10081, 13, 50568], "temperature": 0.0, "avg_logprob": -0.10804456144898802, "compression_ratio": 1.5230125523012552, "no_speech_prob": 0.002295262413099408}, {"id": 83, "seek": 48400, "start": 488.08, "end": 496.56, "text": " But so having such a diverse set of experiences in adjacent fields, how have they influenced each", "tokens": [50568, 583, 370, 1419, 1270, 257, 9521, 992, 295, 5235, 294, 24441, 7909, 11, 577, 362, 436, 15269, 1184, 50992], "temperature": 0.0, "avg_logprob": -0.10804456144898802, "compression_ratio": 1.5230125523012552, "no_speech_prob": 0.002295262413099408}, {"id": 84, "seek": 48400, "start": 496.56, "end": 502.88, "text": " other? Yeah, well, and one thing I didn't mention is that I've also had a long standing", "tokens": [50992, 661, 30, 865, 11, 731, 11, 293, 472, 551, 286, 994, 380, 2152, 307, 300, 286, 600, 611, 632, 257, 938, 4877, 51308], "temperature": 0.0, "avg_logprob": -0.10804456144898802, "compression_ratio": 1.5230125523012552, "no_speech_prob": 0.002295262413099408}, {"id": 85, "seek": 48400, "start": 502.88, "end": 509.28, "text": " interest in philosophy. And I very often think that what I am is a sort of weird kind of", "tokens": [51308, 1179, 294, 10675, 13, 400, 286, 588, 2049, 519, 300, 437, 286, 669, 307, 257, 1333, 295, 3657, 733, 295, 51628], "temperature": 0.0, "avg_logprob": -0.10804456144898802, "compression_ratio": 1.5230125523012552, "no_speech_prob": 0.002295262413099408}, {"id": 86, "seek": 50928, "start": 509.28, "end": 515.4399999999999, "text": " philosopher, really. And philosophical questions have had a great attraction for me. So I think", "tokens": [50364, 29805, 11, 534, 13, 400, 25066, 1651, 362, 632, 257, 869, 17672, 337, 385, 13, 407, 286, 519, 50672], "temperature": 0.0, "avg_logprob": -0.1671522013346354, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0073712957091629505}, {"id": 87, "seek": 50928, "start": 516.56, "end": 522.0799999999999, "text": " there's a sort of three way into relationship between artificial intelligence, neuroscience,", "tokens": [50728, 456, 311, 257, 1333, 295, 1045, 636, 666, 2480, 1296, 11677, 7599, 11, 42762, 11, 51004], "temperature": 0.0, "avg_logprob": -0.1671522013346354, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0073712957091629505}, {"id": 88, "seek": 50928, "start": 522.0799999999999, "end": 527.4399999999999, "text": " and the other cognitive sciences and philosophy. And I think they all kind of mutually inform", "tokens": [51004, 293, 264, 661, 15605, 17677, 293, 10675, 13, 400, 286, 519, 436, 439, 733, 295, 39144, 1356, 51272], "temperature": 0.0, "avg_logprob": -0.1671522013346354, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0073712957091629505}, {"id": 89, "seek": 50928, "start": 527.4399999999999, "end": 533.28, "text": " each other, really. Yeah. Fantastic. So you wrote a book called", "tokens": [51272, 1184, 661, 11, 534, 13, 865, 13, 21320, 13, 407, 291, 4114, 257, 1446, 1219, 51564], "temperature": 0.0, "avg_logprob": -0.1671522013346354, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0073712957091629505}, {"id": 90, "seek": 53328, "start": 533.28, "end": 537.04, "text": " Embodiment and the Inner Life. What motivated you to write that book?", "tokens": [50364, 24234, 378, 2328, 293, 264, 36705, 7720, 13, 708, 14515, 291, 281, 2464, 300, 1446, 30, 50552], "temperature": 0.0, "avg_logprob": -0.1223063912502555, "compression_ratio": 1.5, "no_speech_prob": 0.04355095326900482}, {"id": 91, "seek": 53328, "start": 537.04, "end": 546.72, "text": " Yeah. So at that point, so that book was published in 2010. And it was the culmination of a sort of", "tokens": [50552, 865, 13, 407, 412, 300, 935, 11, 370, 300, 1446, 390, 6572, 294, 9657, 13, 400, 309, 390, 264, 28583, 399, 295, 257, 1333, 295, 51036], "temperature": 0.0, "avg_logprob": -0.1223063912502555, "compression_ratio": 1.5, "no_speech_prob": 0.04355095326900482}, {"id": 92, "seek": 53328, "start": 546.72, "end": 554.3199999999999, "text": " long excursion into thinking about consciousness and about brains, which took place after I had", "tokens": [51036, 938, 1624, 2156, 313, 666, 1953, 466, 10081, 293, 466, 15442, 11, 597, 1890, 1081, 934, 286, 632, 51416], "temperature": 0.0, "avg_logprob": -0.1223063912502555, "compression_ratio": 1.5, "no_speech_prob": 0.04355095326900482}, {"id": 93, "seek": 53328, "start": 554.3199999999999, "end": 559.6, "text": " moved away from symbolic AI, really. So I was thinking about the biological brain.", "tokens": [51416, 4259, 1314, 490, 25755, 7318, 11, 534, 13, 407, 286, 390, 1953, 466, 264, 13910, 3567, 13, 51680], "temperature": 0.0, "avg_logprob": -0.1223063912502555, "compression_ratio": 1.5, "no_speech_prob": 0.04355095326900482}, {"id": 94, "seek": 55960, "start": 559.6800000000001, "end": 563.52, "text": " In the back of my mind, I'd always been fascinated by these philosophical questions about", "tokens": [50368, 682, 264, 646, 295, 452, 1575, 11, 286, 1116, 1009, 668, 24597, 538, 613, 25066, 1651, 466, 50560], "temperature": 0.0, "avg_logprob": -0.09245359521163138, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.002931562252342701}, {"id": 95, "seek": 55960, "start": 563.52, "end": 570.8000000000001, "text": " consciousness. And then I went a bit kind of crazy and started thinking about these things", "tokens": [50560, 10081, 13, 400, 550, 286, 1437, 257, 857, 733, 295, 3219, 293, 1409, 1953, 466, 613, 721, 50924], "temperature": 0.0, "avg_logprob": -0.09245359521163138, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.002931562252342701}, {"id": 96, "seek": 55960, "start": 570.8000000000001, "end": 576.16, "text": " seriously. It became kind of my day job to think about neuroscience, and about consciousness.", "tokens": [50924, 6638, 13, 467, 3062, 733, 295, 452, 786, 1691, 281, 519, 466, 42762, 11, 293, 466, 10081, 13, 51192], "temperature": 0.0, "avg_logprob": -0.09245359521163138, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.002931562252342701}, {"id": 97, "seek": 55960, "start": 576.16, "end": 582.0, "text": " And around about that time, the science of consciousness was taking off as a serious", "tokens": [51192, 400, 926, 466, 300, 565, 11, 264, 3497, 295, 10081, 390, 1940, 766, 382, 257, 3156, 51484], "temperature": 0.0, "avg_logprob": -0.09245359521163138, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.002931562252342701}, {"id": 98, "seek": 55960, "start": 582.0, "end": 586.8000000000001, "text": " academic discipline with proper experimental paradigms. So that was really fascinating.", "tokens": [51484, 7778, 13635, 365, 2296, 17069, 13480, 328, 2592, 13, 407, 300, 390, 534, 10343, 13, 51724], "temperature": 0.0, "avg_logprob": -0.09245359521163138, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.002931562252342701}, {"id": 99, "seek": 58680, "start": 587.4399999999999, "end": 594.8, "text": " And I got to know Bernie Bars. Bernie Bars is the person who originated global workspace theory,", "tokens": [50396, 400, 286, 658, 281, 458, 22426, 363, 685, 13, 22426, 363, 685, 307, 264, 954, 567, 31129, 4338, 32706, 5261, 11, 50764], "temperature": 0.0, "avg_logprob": -0.13832648176895945, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.0007424255018122494}, {"id": 100, "seek": 58680, "start": 594.8, "end": 598.9599999999999, "text": " global workspace theory being one of the leading contenders for a scientific", "tokens": [50764, 4338, 32706, 5261, 885, 472, 295, 264, 5775, 660, 16292, 337, 257, 8134, 50972], "temperature": 0.0, "avg_logprob": -0.13832648176895945, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.0007424255018122494}, {"id": 101, "seek": 58680, "start": 598.9599999999999, "end": 605.1999999999999, "text": " theory of consciousness. And I was very drawn to global workspace theory, and partly because", "tokens": [50972, 5261, 295, 10081, 13, 400, 286, 390, 588, 10117, 281, 4338, 32706, 5261, 11, 293, 17031, 570, 51284], "temperature": 0.0, "avg_logprob": -0.13832648176895945, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.0007424255018122494}, {"id": 102, "seek": 58680, "start": 606.0, "end": 611.52, "text": " it was a computational sort of theory. It drew very heavily on computer science", "tokens": [51324, 309, 390, 257, 28270, 1333, 295, 5261, 13, 467, 12804, 588, 10950, 322, 3820, 3497, 51600], "temperature": 0.0, "avg_logprob": -0.13832648176895945, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.0007424255018122494}, {"id": 103, "seek": 61152, "start": 612.48, "end": 617.6, "text": " and computer architectures. There was a computer architecture at the center of the theory.", "tokens": [50412, 293, 3820, 6331, 1303, 13, 821, 390, 257, 3820, 9482, 412, 264, 3056, 295, 264, 5261, 13, 50668], "temperature": 0.0, "avg_logprob": -0.15325262406293083, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.005619866773486137}, {"id": 104, "seek": 61152, "start": 620.3199999999999, "end": 625.52, "text": " So this kind of collection of interests, along with my philosophical interests, which all came", "tokens": [50804, 407, 341, 733, 295, 5765, 295, 8847, 11, 2051, 365, 452, 25066, 8847, 11, 597, 439, 1361, 51064], "temperature": 0.0, "avg_logprob": -0.15325262406293083, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.005619866773486137}, {"id": 105, "seek": 61152, "start": 625.52, "end": 631.52, "text": " together, and I wanted to put them all into a book where I expressed my kind of ideas about,", "tokens": [51064, 1214, 11, 293, 286, 1415, 281, 829, 552, 439, 666, 257, 1446, 689, 286, 12675, 452, 733, 295, 3487, 466, 11, 51364], "temperature": 0.0, "avg_logprob": -0.15325262406293083, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.005619866773486137}, {"id": 106, "seek": 61152, "start": 631.52, "end": 636.0799999999999, "text": " first of all, from the philosophical side, very heavy influence of Wittgenstein about how we", "tokens": [51364, 700, 295, 439, 11, 490, 264, 25066, 1252, 11, 588, 4676, 6503, 295, 343, 593, 1766, 9089, 466, 577, 321, 51592], "temperature": 0.0, "avg_logprob": -0.15325262406293083, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.005619866773486137}, {"id": 107, "seek": 63608, "start": 636.08, "end": 641.6, "text": " address these problems at all, then lots of global workspace theory and a certain kind of", "tokens": [50364, 2985, 613, 2740, 412, 439, 11, 550, 3195, 295, 4338, 32706, 5261, 293, 257, 1629, 733, 295, 50640], "temperature": 0.0, "avg_logprob": -0.11395099116306678, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0050912112928926945}, {"id": 108, "seek": 63608, "start": 641.6, "end": 647.2800000000001, "text": " global workspace architecture, how that might be realized in the brain, drawing also on the", "tokens": [50640, 4338, 32706, 9482, 11, 577, 300, 1062, 312, 5334, 294, 264, 3567, 11, 6316, 611, 322, 264, 50924], "temperature": 0.0, "avg_logprob": -0.11395099116306678, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0050912112928926945}, {"id": 109, "seek": 63608, "start": 647.2800000000001, "end": 653.6, "text": " work of Stanislaus DeHend, who was working on what he called the global neuronal workspace idea,", "tokens": [50924, 589, 295, 10061, 271, 22590, 1346, 39, 521, 11, 567, 390, 1364, 322, 437, 415, 1219, 264, 4338, 12087, 21523, 32706, 1558, 11, 51240], "temperature": 0.0, "avg_logprob": -0.11395099116306678, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0050912112928926945}, {"id": 110, "seek": 63608, "start": 654.1600000000001, "end": 657.6, "text": " and putting all these things together into one big book.", "tokens": [51268, 293, 3372, 439, 613, 721, 1214, 666, 472, 955, 1446, 13, 51440], "temperature": 0.0, "avg_logprob": -0.11395099116306678, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0050912112928926945}, {"id": 111, "seek": 63608, "start": 658.24, "end": 662.5600000000001, "text": " Amazing. Well, we'll speak a lot about Wittgenstein when we speak about the language model paper", "tokens": [51472, 14165, 13, 1042, 11, 321, 603, 1710, 257, 688, 466, 343, 593, 1766, 9089, 562, 321, 1710, 466, 264, 2856, 2316, 3035, 51688], "temperature": 0.0, "avg_logprob": -0.11395099116306678, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0050912112928926945}, {"id": 112, "seek": 66256, "start": 662.64, "end": 667.52, "text": " and your consciousness paper. But two things that did trigger or prick my ears up,", "tokens": [50368, 293, 428, 10081, 3035, 13, 583, 732, 721, 300, 630, 7875, 420, 43986, 452, 8798, 493, 11, 50612], "temperature": 0.0, "avg_logprob": -0.16658052191676864, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.039973046630620956}, {"id": 113, "seek": 66256, "start": 668.16, "end": 673.52, "text": " computationalism, which is quite interesting, because some folks in the cognitive science arena,", "tokens": [50644, 28270, 1434, 11, 597, 307, 1596, 1880, 11, 570, 512, 4024, 294, 264, 15605, 3497, 18451, 11, 50912], "temperature": 0.0, "avg_logprob": -0.16658052191676864, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.039973046630620956}, {"id": 114, "seek": 66256, "start": 673.52, "end": 681.4399999999999, "text": " especially with the fouries, like examples to escape from computationalism. We did a show on", "tokens": [50912, 2318, 365, 264, 1451, 530, 11, 411, 5110, 281, 7615, 490, 28270, 1434, 13, 492, 630, 257, 855, 322, 51308], "temperature": 0.0, "avg_logprob": -0.16658052191676864, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.039973046630620956}, {"id": 115, "seek": 66256, "start": 681.4399999999999, "end": 687.5999999999999, "text": " cells, Chinese room argument the other day. He's probably one of the most known people who do", "tokens": [51308, 5438, 11, 4649, 1808, 6770, 264, 661, 786, 13, 634, 311, 1391, 472, 295, 264, 881, 2570, 561, 567, 360, 51616], "temperature": 0.0, "avg_logprob": -0.16658052191676864, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.039973046630620956}, {"id": 116, "seek": 68760, "start": 688.4, "end": 691.2, "text": " issue computationalism. So what do you think about that?", "tokens": [50404, 2734, 28270, 1434, 13, 407, 437, 360, 291, 519, 466, 300, 30, 50544], "temperature": 0.0, "avg_logprob": -0.1572143670284387, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.031198255717754364}, {"id": 117, "seek": 68760, "start": 691.2, "end": 696.08, "text": " Yeah. Well, actually, so when I was talking about global workspace theory, I mentioned that it", "tokens": [50544, 865, 13, 1042, 11, 767, 11, 370, 562, 286, 390, 1417, 466, 4338, 32706, 5261, 11, 286, 2835, 300, 309, 50788], "temperature": 0.0, "avg_logprob": -0.1572143670284387, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.031198255717754364}, {"id": 118, "seek": 68760, "start": 698.88, "end": 706.0, "text": " comes out of a kind of computational architecture. But in fact, where I took it was very much moving", "tokens": [50928, 1487, 484, 295, 257, 733, 295, 28270, 9482, 13, 583, 294, 1186, 11, 689, 286, 1890, 309, 390, 588, 709, 2684, 51284], "temperature": 0.0, "avg_logprob": -0.1572143670284387, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.031198255717754364}, {"id": 119, "seek": 68760, "start": 706.0, "end": 713.2, "text": " away from that original presentation, which drew heavily on a kind of quite an old-fashioned", "tokens": [51284, 1314, 490, 300, 3380, 5860, 11, 597, 12804, 10950, 322, 257, 733, 295, 1596, 364, 1331, 12, 37998, 51644], "temperature": 0.0, "avg_logprob": -0.1572143670284387, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.031198255717754364}, {"id": 120, "seek": 68760, "start": 713.2, "end": 717.12, "text": " architectural perspective, sort of boxes and how they communicate with each other and so on.", "tokens": [51644, 26621, 4585, 11, 1333, 295, 9002, 293, 577, 436, 7890, 365, 1184, 661, 293, 370, 322, 13, 51840], "temperature": 0.0, "avg_logprob": -0.1572143670284387, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.031198255717754364}, {"id": 121, "seek": 71760, "start": 718.0, "end": 721.76, "text": " And I was much more interested in taking it in a direction which is very much more", "tokens": [50384, 400, 286, 390, 709, 544, 3102, 294, 1940, 309, 294, 257, 3513, 597, 307, 588, 709, 544, 50572], "temperature": 0.0, "avg_logprob": -0.1343508500319261, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.00025557223125360906}, {"id": 122, "seek": 71760, "start": 721.76, "end": 729.2, "text": " connectionist and drawing much more heavily on the underlying biology and neuroscience,", "tokens": [50572, 4984, 468, 293, 6316, 709, 544, 10950, 322, 264, 14217, 14956, 293, 42762, 11, 50944], "temperature": 0.0, "avg_logprob": -0.1343508500319261, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.00025557223125360906}, {"id": 123, "seek": 71760, "start": 729.2, "end": 733.6, "text": " which in fact is also a direction that Bernie Barnes himself had moved in, because the book", "tokens": [50944, 597, 294, 1186, 307, 611, 257, 3513, 300, 22426, 43903, 3647, 632, 4259, 294, 11, 570, 264, 1446, 51164], "temperature": 0.0, "avg_logprob": -0.1343508500319261, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.00025557223125360906}, {"id": 124, "seek": 71760, "start": 734.64, "end": 741.28, "text": " that originally put forward his theory is from 1988. So that was the predominant way of thinking", "tokens": [51216, 300, 7993, 829, 2128, 702, 5261, 307, 490, 27816, 13, 407, 300, 390, 264, 21456, 394, 636, 295, 1953, 51548], "temperature": 0.0, "avg_logprob": -0.1343508500319261, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.00025557223125360906}, {"id": 125, "seek": 74128, "start": 741.28, "end": 747.8399999999999, "text": " at the time was this very computational cognitiveist perspective. So by 2010, when my book was", "tokens": [50364, 412, 264, 565, 390, 341, 588, 28270, 15605, 468, 4585, 13, 407, 538, 9657, 11, 562, 452, 1446, 390, 50692], "temperature": 0.0, "avg_logprob": -0.16324511239694994, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.03309067711234093}, {"id": 126, "seek": 74128, "start": 747.8399999999999, "end": 754.16, "text": " published, I was very much more interested in a kind of more connectionist perspective on things.", "tokens": [50692, 6572, 11, 286, 390, 588, 709, 544, 3102, 294, 257, 733, 295, 544, 4984, 468, 4585, 322, 721, 13, 51008], "temperature": 0.0, "avg_logprob": -0.16324511239694994, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.03309067711234093}, {"id": 127, "seek": 74128, "start": 755.12, "end": 759.4399999999999, "text": " So that's the way that it's portrayed in the book, the theory.", "tokens": [51056, 407, 300, 311, 264, 636, 300, 309, 311, 29845, 294, 264, 1446, 11, 264, 5261, 13, 51272], "temperature": 0.0, "avg_logprob": -0.16324511239694994, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.03309067711234093}, {"id": 128, "seek": 74128, "start": 760.0, "end": 767.12, "text": " Fascinating. Because in this arena, some people cite penrose or the need for hypercomputation,", "tokens": [51300, 49098, 8205, 13, 1436, 294, 341, 18451, 11, 512, 561, 37771, 3435, 37841, 420, 264, 643, 337, 9848, 1112, 2582, 399, 11, 51656], "temperature": 0.0, "avg_logprob": -0.16324511239694994, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.03309067711234093}, {"id": 129, "seek": 76712, "start": 768.08, "end": 772.88, "text": " because people talk about the church-turing hypothesis and this idea that the universe", "tokens": [50412, 570, 561, 751, 466, 264, 4128, 12, 83, 1345, 17291, 293, 341, 1558, 300, 264, 6445, 50652], "temperature": 0.0, "avg_logprob": -0.11883853344207114, "compression_ratio": 1.6, "no_speech_prob": 0.003604666329920292}, {"id": 130, "seek": 76712, "start": 772.88, "end": 778.08, "text": " could be made of information, which is quite interesting. But do you believe that the world", "tokens": [50652, 727, 312, 1027, 295, 1589, 11, 597, 307, 1596, 1880, 13, 583, 360, 291, 1697, 300, 264, 1002, 50912], "temperature": 0.0, "avg_logprob": -0.11883853344207114, "compression_ratio": 1.6, "no_speech_prob": 0.003604666329920292}, {"id": 131, "seek": 76712, "start": 778.08, "end": 782.24, "text": " that we live in could be computationally represented and computed?", "tokens": [50912, 300, 321, 1621, 294, 727, 312, 24903, 379, 10379, 293, 40610, 30, 51120], "temperature": 0.0, "avg_logprob": -0.11883853344207114, "compression_ratio": 1.6, "no_speech_prob": 0.003604666329920292}, {"id": 132, "seek": 76712, "start": 784.24, "end": 787.6, "text": " Well, I'm not sure that I have a belief on that particular one.", "tokens": [51220, 1042, 11, 286, 478, 406, 988, 300, 286, 362, 257, 7107, 322, 300, 1729, 472, 13, 51388], "temperature": 0.0, "avg_logprob": -0.11883853344207114, "compression_ratio": 1.6, "no_speech_prob": 0.003604666329920292}, {"id": 133, "seek": 76712, "start": 789.76, "end": 795.52, "text": " So I mean, I mean, Penrose's ideas about consciousness, of course, draw heavily on", "tokens": [51496, 407, 286, 914, 11, 286, 914, 11, 10571, 37841, 311, 3487, 466, 10081, 11, 295, 1164, 11, 2642, 10950, 322, 51784], "temperature": 0.0, "avg_logprob": -0.11883853344207114, "compression_ratio": 1.6, "no_speech_prob": 0.003604666329920292}, {"id": 134, "seek": 79552, "start": 795.52, "end": 802.4, "text": " quantum mechanics, and he thinks that quantum effects are important for consciousness. But", "tokens": [50364, 13018, 12939, 11, 293, 415, 7309, 300, 13018, 5065, 366, 1021, 337, 10081, 13, 583, 50708], "temperature": 0.0, "avg_logprob": -0.12834601359324413, "compression_ratio": 1.625, "no_speech_prob": 0.003905689576640725}, {"id": 135, "seek": 79552, "start": 802.4, "end": 808.0, "text": " I mean, that's very much a minority, a tiny minority view within the people who study", "tokens": [50708, 286, 914, 11, 300, 311, 588, 709, 257, 16166, 11, 257, 5870, 16166, 1910, 1951, 264, 561, 567, 2979, 50988], "temperature": 0.0, "avg_logprob": -0.12834601359324413, "compression_ratio": 1.625, "no_speech_prob": 0.003905689576640725}, {"id": 136, "seek": 79552, "start": 808.0, "end": 814.48, "text": " consciousness from a scientific standpoint. And so I don't really subscribe to that", "tokens": [50988, 10081, 490, 257, 8134, 15827, 13, 400, 370, 286, 500, 380, 534, 3022, 281, 300, 51312], "temperature": 0.0, "avg_logprob": -0.12834601359324413, "compression_ratio": 1.625, "no_speech_prob": 0.003905689576640725}, {"id": 137, "seek": 79552, "start": 815.04, "end": 819.1999999999999, "text": " interview, I have to say. Well, I mean, coming at it from a slightly different angle, we spoke", "tokens": [51340, 4049, 11, 286, 362, 281, 584, 13, 1042, 11, 286, 914, 11, 1348, 412, 309, 490, 257, 4748, 819, 5802, 11, 321, 7179, 51548], "temperature": 0.0, "avg_logprob": -0.12834601359324413, "compression_ratio": 1.625, "no_speech_prob": 0.003905689576640725}, {"id": 138, "seek": 79552, "start": 819.1999999999999, "end": 824.0799999999999, "text": " to Noam Chomsky recently, and I've just done some content on Nagel's bat, a couple of rationalists,", "tokens": [51548, 281, 883, 335, 761, 4785, 4133, 3938, 11, 293, 286, 600, 445, 1096, 512, 2701, 322, 18913, 338, 311, 7362, 11, 257, 1916, 295, 15090, 1751, 11, 51792], "temperature": 0.0, "avg_logprob": -0.12834601359324413, "compression_ratio": 1.625, "no_speech_prob": 0.003905689576640725}, {"id": 139, "seek": 82408, "start": 825.0400000000001, "end": 831.2, "text": " their big argument is about the subject of experience and the limits of our cognitive", "tokens": [50412, 641, 955, 6770, 307, 466, 264, 3983, 295, 1752, 293, 264, 10406, 295, 527, 15605, 50720], "temperature": 0.0, "avg_logprob": -0.14971819691274357, "compression_ratio": 1.5274261603375527, "no_speech_prob": 0.00234047370031476}, {"id": 140, "seek": 82408, "start": 831.2, "end": 837.9200000000001, "text": " horizon and the inability really for us to reduce things into a comprehensible framework of", "tokens": [50720, 18046, 293, 264, 33162, 534, 337, 505, 281, 5407, 721, 666, 257, 10753, 30633, 8388, 295, 51056], "temperature": 0.0, "avg_logprob": -0.14971819691274357, "compression_ratio": 1.5274261603375527, "no_speech_prob": 0.00234047370031476}, {"id": 141, "seek": 82408, "start": 837.9200000000001, "end": 842.8000000000001, "text": " understanding. So how would you bring that in? Yeah, well, gosh, I mean, yeah, we've launched", "tokens": [51056, 3701, 13, 407, 577, 576, 291, 1565, 300, 294, 30, 865, 11, 731, 11, 6502, 11, 286, 914, 11, 1338, 11, 321, 600, 8730, 51300], "temperature": 0.0, "avg_logprob": -0.14971819691274357, "compression_ratio": 1.5274261603375527, "no_speech_prob": 0.00234047370031476}, {"id": 142, "seek": 82408, "start": 842.8000000000001, "end": 850.8000000000001, "text": " right into some really big, difficult topics here, right? So in my book, Embodiment in the", "tokens": [51300, 558, 666, 512, 534, 955, 11, 2252, 8378, 510, 11, 558, 30, 407, 294, 452, 1446, 11, 24234, 378, 2328, 294, 264, 51700], "temperature": 0.0, "avg_logprob": -0.14971819691274357, "compression_ratio": 1.5274261603375527, "no_speech_prob": 0.00234047370031476}, {"id": 143, "seek": 85080, "start": 850.9599999999999, "end": 855.12, "text": " Inner Life, which at the time, I thought I'd really kind of like wrapped up the problem of", "tokens": [50372, 36705, 7720, 11, 597, 412, 264, 565, 11, 286, 1194, 286, 1116, 534, 733, 295, 411, 14226, 493, 264, 1154, 295, 50580], "temperature": 0.0, "avg_logprob": -0.16020435415288453, "compression_ratio": 1.5720524017467248, "no_speech_prob": 0.01039163675159216}, {"id": 144, "seek": 85080, "start": 855.12, "end": 863.04, "text": " consciousness. But one of the big sort of outstanding things for me in one of the", "tokens": [50580, 10081, 13, 583, 472, 295, 264, 955, 1333, 295, 14485, 721, 337, 385, 294, 472, 295, 264, 50976], "temperature": 0.0, "avg_logprob": -0.16020435415288453, "compression_ratio": 1.5720524017467248, "no_speech_prob": 0.01039163675159216}, {"id": 145, "seek": 85080, "start": 863.04, "end": 867.52, "text": " outstanding questions that I have not really answered, I felt in that book, is very much", "tokens": [50976, 14485, 1651, 300, 286, 362, 406, 534, 10103, 11, 286, 2762, 294, 300, 1446, 11, 307, 588, 709, 51200], "temperature": 0.0, "avg_logprob": -0.16020435415288453, "compression_ratio": 1.5720524017467248, "no_speech_prob": 0.01039163675159216}, {"id": 146, "seek": 85080, "start": 867.52, "end": 875.52, "text": " related to Nagel's question about bats, what does it like to be a bat? So, and it's to do with the", "tokens": [51200, 4077, 281, 18913, 338, 311, 1168, 466, 26943, 11, 437, 775, 309, 411, 281, 312, 257, 7362, 30, 407, 11, 293, 309, 311, 281, 360, 365, 264, 51600], "temperature": 0.0, "avg_logprob": -0.16020435415288453, "compression_ratio": 1.5720524017467248, "no_speech_prob": 0.01039163675159216}, {"id": 147, "seek": 87552, "start": 875.52, "end": 882.3199999999999, "text": " idea that there's a sort of intuitive idea that maybe there can be very exotic entities,", "tokens": [50364, 1558, 300, 456, 311, 257, 1333, 295, 21769, 1558, 300, 1310, 456, 393, 312, 588, 27063, 16667, 11, 50704], "temperature": 0.0, "avg_logprob": -0.11411845684051514, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.027339665219187737}, {"id": 148, "seek": 87552, "start": 882.3199999999999, "end": 888.56, "text": " very exotic creatures who are completely unlike us. And yet, somehow, there's some kind of", "tokens": [50704, 588, 27063, 12281, 567, 366, 2584, 8343, 505, 13, 400, 1939, 11, 6063, 11, 456, 311, 512, 733, 295, 51016], "temperature": 0.0, "avg_logprob": -0.11411845684051514, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.027339665219187737}, {"id": 149, "seek": 87552, "start": 888.56, "end": 894.8, "text": " consciousness there that we could barely grasp its nature. And this is a sort of natural", "tokens": [51016, 10081, 456, 300, 321, 727, 10268, 21743, 1080, 3687, 13, 400, 341, 307, 257, 1333, 295, 3303, 51328], "temperature": 0.0, "avg_logprob": -0.11411845684051514, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.027339665219187737}, {"id": 150, "seek": 87552, "start": 894.8, "end": 899.1999999999999, "text": " intuitive thought. And especially when we look at other animals, like bats, and especially if we", "tokens": [51328, 21769, 1194, 13, 400, 2318, 562, 321, 574, 412, 661, 4882, 11, 411, 26943, 11, 293, 2318, 498, 321, 51548], "temperature": 0.0, "avg_logprob": -0.11411845684051514, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.027339665219187737}, {"id": 151, "seek": 87552, "start": 899.1999999999999, "end": 905.36, "text": " look at an animal that's a bit different from us, then we get hints that there's some", "tokens": [51548, 574, 412, 364, 5496, 300, 311, 257, 857, 819, 490, 505, 11, 550, 321, 483, 27271, 300, 456, 311, 512, 51856], "temperature": 0.0, "avg_logprob": -0.11411845684051514, "compression_ratio": 1.8713692946058091, "no_speech_prob": 0.027339665219187737}, {"id": 152, "seek": 90552, "start": 905.76, "end": 911.28, "text": " one at home, as it were, and that there's consciousness there. I think we, I'm sure all", "tokens": [50376, 472, 412, 1280, 11, 382, 309, 645, 11, 293, 300, 456, 311, 10081, 456, 13, 286, 519, 321, 11, 286, 478, 988, 439, 50652], "temperature": 0.0, "avg_logprob": -0.14536571502685547, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.001510998816229403}, {"id": 153, "seek": 90552, "start": 911.28, "end": 918.3199999999999, "text": " of us believe that cats and dogs, and many other animals are conscious and are capable of suffering", "tokens": [50652, 295, 505, 1697, 300, 11111, 293, 7197, 11, 293, 867, 661, 4882, 366, 6648, 293, 366, 8189, 295, 7755, 51004], "temperature": 0.0, "avg_logprob": -0.14536571502685547, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.001510998816229403}, {"id": 154, "seek": 90552, "start": 918.3199999999999, "end": 924.64, "text": " and having awareness of the world that's like our awareness and are aware of us and each other.", "tokens": [51004, 293, 1419, 8888, 295, 264, 1002, 300, 311, 411, 527, 8888, 293, 366, 3650, 295, 505, 293, 1184, 661, 13, 51320], "temperature": 0.0, "avg_logprob": -0.14536571502685547, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.001510998816229403}, {"id": 155, "seek": 90552, "start": 928.0, "end": 934.0799999999999, "text": " I mean, I take that as almost axiomatic. That's just the way we treat those creatures.", "tokens": [51488, 286, 914, 11, 286, 747, 300, 382, 1920, 6360, 72, 13143, 13, 663, 311, 445, 264, 636, 321, 2387, 729, 12281, 13, 51792], "temperature": 0.0, "avg_logprob": -0.14536571502685547, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.001510998816229403}, {"id": 156, "seek": 93408, "start": 934.08, "end": 938.64, "text": " But then when we think about something like a bat, it's very different from us. So the natural", "tokens": [50364, 583, 550, 562, 321, 519, 466, 746, 411, 257, 7362, 11, 309, 311, 588, 819, 490, 505, 13, 407, 264, 3303, 50592], "temperature": 0.0, "avg_logprob": -0.11679041903951894, "compression_ratio": 1.8090452261306533, "no_speech_prob": 0.0012644780799746513}, {"id": 157, "seek": 93408, "start": 938.64, "end": 944.32, "text": " thing thought is that maybe what it's like is very, very different from what it's like for us,", "tokens": [50592, 551, 1194, 307, 300, 1310, 437, 309, 311, 411, 307, 588, 11, 588, 819, 490, 437, 309, 311, 411, 337, 505, 11, 50876], "temperature": 0.0, "avg_logprob": -0.11679041903951894, "compression_ratio": 1.8090452261306533, "no_speech_prob": 0.0012644780799746513}, {"id": 158, "seek": 93408, "start": 944.32, "end": 952.8000000000001, "text": " and it's a natural thought to express. And of course, Nagel takes that thought", "tokens": [50876, 293, 309, 311, 257, 3303, 1194, 281, 5109, 13, 400, 295, 1164, 11, 18913, 338, 2516, 300, 1194, 51300], "temperature": 0.0, "avg_logprob": -0.11679041903951894, "compression_ratio": 1.8090452261306533, "no_speech_prob": 0.0012644780799746513}, {"id": 159, "seek": 93408, "start": 954.64, "end": 963.6, "text": " to suggest that there are something that is inaccessible to us, which is what is it like to", "tokens": [51392, 281, 3402, 300, 456, 366, 746, 300, 307, 33230, 780, 964, 281, 505, 11, 597, 307, 437, 307, 309, 411, 281, 51840], "temperature": 0.0, "avg_logprob": -0.11679041903951894, "compression_ratio": 1.8090452261306533, "no_speech_prob": 0.0012644780799746513}, {"id": 160, "seek": 96360, "start": 963.6, "end": 969.28, "text": " be a bat? It's something we can never know. And this is a very un-Viconstinian thought. And I'm", "tokens": [50364, 312, 257, 7362, 30, 467, 311, 746, 321, 393, 1128, 458, 13, 400, 341, 307, 257, 588, 517, 12, 53, 299, 4068, 259, 952, 1194, 13, 400, 286, 478, 50648], "temperature": 0.0, "avg_logprob": -0.18063321258082535, "compression_ratio": 1.82421875, "no_speech_prob": 0.002184964483603835}, {"id": 161, "seek": 96360, "start": 969.28, "end": 977.52, "text": " very much, you know, I'm very attracted to Viconstine's philosophy. But it's also a very", "tokens": [50648, 588, 709, 11, 291, 458, 11, 286, 478, 588, 15912, 281, 33316, 4068, 533, 311, 10675, 13, 583, 309, 311, 611, 257, 588, 51060], "temperature": 0.0, "avg_logprob": -0.18063321258082535, "compression_ratio": 1.82421875, "no_speech_prob": 0.002184964483603835}, {"id": 162, "seek": 96360, "start": 977.52, "end": 982.0, "text": " natural thought that, you know, so it's a very un-Viconstinian thought because Viconstine says,", "tokens": [51060, 3303, 1194, 300, 11, 291, 458, 11, 370, 309, 311, 257, 588, 517, 12, 53, 299, 4068, 259, 952, 1194, 570, 33316, 4068, 533, 1619, 11, 51284], "temperature": 0.0, "avg_logprob": -0.18063321258082535, "compression_ratio": 1.82421875, "no_speech_prob": 0.002184964483603835}, {"id": 163, "seek": 96360, "start": 982.0, "end": 987.6, "text": " for example, you know, nothing is hidden. So he's very, you know, and the whole private language", "tokens": [51284, 337, 1365, 11, 291, 458, 11, 1825, 307, 7633, 13, 407, 415, 311, 588, 11, 291, 458, 11, 293, 264, 1379, 4551, 2856, 51564], "temperature": 0.0, "avg_logprob": -0.18063321258082535, "compression_ratio": 1.82421875, "no_speech_prob": 0.002184964483603835}, {"id": 164, "seek": 96360, "start": 987.6, "end": 992.48, "text": " remarks are all about sort of saying, well, this intuition that we have that there's this", "tokens": [51564, 19151, 366, 439, 466, 1333, 295, 1566, 11, 731, 11, 341, 24002, 300, 321, 362, 300, 456, 311, 341, 51808], "temperature": 0.0, "avg_logprob": -0.18063321258082535, "compression_ratio": 1.82421875, "no_speech_prob": 0.002184964483603835}, {"id": 165, "seek": 99248, "start": 992.5600000000001, "end": 999.6, "text": " private realm of experience is actually just, it's just a philosophical trick of the mind", "tokens": [50368, 4551, 15355, 295, 1752, 307, 767, 445, 11, 309, 311, 445, 257, 25066, 4282, 295, 264, 1575, 50720], "temperature": 0.0, "avg_logprob": -0.1144971382327196, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.003868534928187728}, {"id": 166, "seek": 99248, "start": 999.6, "end": 1005.6, "text": " to think that this sort of peculiar metaphysical realm exists of inaccessible, subjective", "tokens": [50720, 281, 519, 300, 341, 1333, 295, 27149, 30946, 36280, 15355, 8198, 295, 33230, 780, 964, 11, 25972, 51020], "temperature": 0.0, "avg_logprob": -0.1144971382327196, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.003868534928187728}, {"id": 167, "seek": 99248, "start": 1005.6, "end": 1013.6800000000001, "text": " experience in others. And that's his whole thrust of his philosophy or that aspect of it", "tokens": [51020, 1752, 294, 2357, 13, 400, 300, 311, 702, 1379, 24030, 295, 702, 10675, 420, 300, 4171, 295, 309, 51424], "temperature": 0.0, "avg_logprob": -0.1144971382327196, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.003868534928187728}, {"id": 168, "seek": 99248, "start": 1013.6800000000001, "end": 1017.2, "text": " is to try and undermine that. So these two things are intention, right? So there's this", "tokens": [51424, 307, 281, 853, 293, 39257, 300, 13, 407, 613, 732, 721, 366, 7789, 11, 558, 30, 407, 456, 311, 341, 51600], "temperature": 0.0, "avg_logprob": -0.1144971382327196, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.003868534928187728}, {"id": 169, "seek": 101720, "start": 1017.2800000000001, "end": 1022.96, "text": " natural thought that bats, you know, it must be like something to be a bat, but what is it like", "tokens": [50368, 3303, 1194, 300, 26943, 11, 291, 458, 11, 309, 1633, 312, 411, 746, 281, 312, 257, 7362, 11, 457, 437, 307, 309, 411, 50652], "temperature": 0.0, "avg_logprob": -0.0906509372359472, "compression_ratio": 1.8833333333333333, "no_speech_prob": 0.06480735540390015}, {"id": 170, "seek": 101720, "start": 1022.96, "end": 1026.96, "text": " and how could we ever know? And then there's the Viconstinian thought, which is actually very", "tokens": [50652, 293, 577, 727, 321, 1562, 458, 30, 400, 550, 456, 311, 264, 33316, 4068, 259, 952, 1194, 11, 597, 307, 767, 588, 50852], "temperature": 0.0, "avg_logprob": -0.0906509372359472, "compression_ratio": 1.8833333333333333, "no_speech_prob": 0.06480735540390015}, {"id": 171, "seek": 101720, "start": 1026.96, "end": 1031.76, "text": " difficult to kind of really embrace. But it's that there's a sense in which nothing is really", "tokens": [50852, 2252, 281, 733, 295, 534, 14038, 13, 583, 309, 311, 300, 456, 311, 257, 2020, 294, 597, 1825, 307, 534, 51092], "temperature": 0.0, "avg_logprob": -0.0906509372359472, "compression_ratio": 1.8833333333333333, "no_speech_prob": 0.06480735540390015}, {"id": 172, "seek": 101720, "start": 1031.76, "end": 1036.72, "text": " metaphysically hidden. It's only hidden, could be hidden empirically, because maybe we don't", "tokens": [51092, 30946, 749, 984, 7633, 13, 467, 311, 787, 7633, 11, 727, 312, 7633, 25790, 984, 11, 570, 1310, 321, 500, 380, 51340], "temperature": 0.0, "avg_logprob": -0.0906509372359472, "compression_ratio": 1.8833333333333333, "no_speech_prob": 0.06480735540390015}, {"id": 173, "seek": 101720, "start": 1036.72, "end": 1041.28, "text": " know enough, maybe we haven't hung around with bats often enough, or maybe we haven't examined", "tokens": [51340, 458, 1547, 11, 1310, 321, 2378, 380, 5753, 926, 365, 26943, 2049, 1547, 11, 420, 1310, 321, 2378, 380, 30972, 51568], "temperature": 0.0, "avg_logprob": -0.0906509372359472, "compression_ratio": 1.8833333333333333, "no_speech_prob": 0.06480735540390015}, {"id": 174, "seek": 101720, "start": 1041.28, "end": 1045.68, "text": " their brains, or maybe that's all empirical, right? So there's nothing metaphysically hidden,", "tokens": [51568, 641, 15442, 11, 420, 1310, 300, 311, 439, 31886, 11, 558, 30, 407, 456, 311, 1825, 30946, 749, 984, 7633, 11, 51788], "temperature": 0.0, "avg_logprob": -0.0906509372359472, "compression_ratio": 1.8833333333333333, "no_speech_prob": 0.06480735540390015}, {"id": 175, "seek": 104568, "start": 1045.68, "end": 1049.28, "text": " whereas Nagel's point is that there's something that's deeply, profoundly,", "tokens": [50364, 9735, 18913, 338, 311, 935, 307, 300, 456, 311, 746, 300, 311, 8760, 11, 39954, 11, 50544], "temperature": 0.0, "avg_logprob": -0.1366939968532986, "compression_ratio": 1.738255033557047, "no_speech_prob": 0.0015828486066311598}, {"id": 176, "seek": 104568, "start": 1049.28, "end": 1053.3600000000001, "text": " philosophically, metaphysically hidden, which is the subjective. Now we can extend that,", "tokens": [50544, 14529, 984, 11, 30946, 749, 984, 7633, 11, 597, 307, 264, 25972, 13, 823, 321, 393, 10101, 300, 11, 50748], "temperature": 0.0, "avg_logprob": -0.1366939968532986, "compression_ratio": 1.738255033557047, "no_speech_prob": 0.0015828486066311598}, {"id": 177, "seek": 104568, "start": 1053.3600000000001, "end": 1060.3200000000002, "text": " shall carry on. So I'm rambling now. So now we can extend that thought about bats,", "tokens": [50748, 4393, 3985, 322, 13, 407, 286, 478, 367, 19391, 586, 13, 407, 586, 321, 393, 10101, 300, 1194, 466, 26943, 11, 51096], "temperature": 0.0, "avg_logprob": -0.1366939968532986, "compression_ratio": 1.738255033557047, "no_speech_prob": 0.0015828486066311598}, {"id": 178, "seek": 104568, "start": 1060.3200000000002, "end": 1064.16, "text": " now, you know, especially from the perspective of the sort of thing that I'm interested in,", "tokens": [51096, 586, 11, 291, 458, 11, 2318, 490, 264, 4585, 295, 264, 1333, 295, 551, 300, 286, 478, 3102, 294, 11, 51288], "temperature": 0.0, "avg_logprob": -0.1366939968532986, "compression_ratio": 1.738255033557047, "no_speech_prob": 0.0015828486066311598}, {"id": 179, "seek": 104568, "start": 1064.16, "end": 1068.4, "text": " to, well, not just bats, but what about the whole space of possible minds to use", "tokens": [51288, 281, 11, 731, 11, 406, 445, 26943, 11, 457, 437, 466, 264, 1379, 1901, 295, 1944, 9634, 281, 764, 51500], "temperature": 0.0, "avg_logprob": -0.1366939968532986, "compression_ratio": 1.738255033557047, "no_speech_prob": 0.0015828486066311598}, {"id": 180, "seek": 104568, "start": 1068.4, "end": 1073.6000000000001, "text": " Aaron Sloman's very evocative phrase? What about, you know, extraterrestrials who are going to be,", "tokens": [51500, 14018, 6187, 4277, 311, 588, 1073, 905, 1166, 9535, 30, 708, 466, 11, 291, 458, 11, 43324, 4149, 470, 1124, 567, 366, 516, 281, 312, 11, 51760], "temperature": 0.0, "avg_logprob": -0.1366939968532986, "compression_ratio": 1.738255033557047, "no_speech_prob": 0.0015828486066311598}, {"id": 181, "seek": 107360, "start": 1074.48, "end": 1079.1999999999998, "text": " you know, who surely there is extraterrestrial intelligence out there, it's going to be very,", "tokens": [50408, 291, 458, 11, 567, 11468, 456, 307, 43324, 34539, 7599, 484, 456, 11, 309, 311, 516, 281, 312, 588, 11, 50644], "temperature": 0.0, "avg_logprob": -0.10431390603383382, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.0016824413323774934}, {"id": 182, "seek": 107360, "start": 1079.1999999999998, "end": 1084.6399999999999, "text": " very, very different to us. So, and then what about the things that we build? Maybe we can build", "tokens": [50644, 588, 11, 588, 819, 281, 505, 13, 407, 11, 293, 550, 437, 466, 264, 721, 300, 321, 1322, 30, 2704, 321, 393, 1322, 50916], "temperature": 0.0, "avg_logprob": -0.10431390603383382, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.0016824413323774934}, {"id": 183, "seek": 107360, "start": 1085.52, "end": 1091.84, "text": " things, you know, and artificial intelligence of the future, maybe, maybe, you know, we can build", "tokens": [50960, 721, 11, 291, 458, 11, 293, 11677, 7599, 295, 264, 2027, 11, 1310, 11, 1310, 11, 291, 458, 11, 321, 393, 1322, 51276], "temperature": 0.0, "avg_logprob": -0.10431390603383382, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.0016824413323774934}, {"id": 184, "seek": 107360, "start": 1091.84, "end": 1095.4399999999998, "text": " something that is also conscious, it's the kind of thing that's depicted in science fiction all", "tokens": [51276, 746, 300, 307, 611, 6648, 11, 309, 311, 264, 733, 295, 551, 300, 311, 30207, 294, 3497, 13266, 439, 51456], "temperature": 0.0, "avg_logprob": -0.10431390603383382, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.0016824413323774934}, {"id": 185, "seek": 107360, "start": 1095.4399999999998, "end": 1099.9199999999998, "text": " the time. In science fiction, it's often depicted as very human-like, but there's no reason why it", "tokens": [51456, 264, 565, 13, 682, 3497, 13266, 11, 309, 311, 2049, 30207, 382, 588, 1952, 12, 4092, 11, 457, 456, 311, 572, 1778, 983, 309, 51680], "temperature": 0.0, "avg_logprob": -0.10431390603383382, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.0016824413323774934}, {"id": 186, "seek": 109992, "start": 1099.92, "end": 1104.88, "text": " should be human-like at all. And so we can imagine these very, very exotic entities, and then the", "tokens": [50364, 820, 312, 1952, 12, 4092, 412, 439, 13, 400, 370, 321, 393, 3811, 613, 588, 11, 588, 27063, 16667, 11, 293, 550, 264, 50612], "temperature": 0.0, "avg_logprob": -0.11817822574583953, "compression_ratio": 1.7, "no_speech_prob": 0.0037296307273209095}, {"id": 187, "seek": 109992, "start": 1104.88, "end": 1109.2, "text": " question is even bigger, you know, there could be something that we, we won't even be able to recognize", "tokens": [50612, 1168, 307, 754, 3801, 11, 291, 458, 11, 456, 727, 312, 746, 300, 321, 11, 321, 1582, 380, 754, 312, 1075, 281, 5521, 50828], "temperature": 0.0, "avg_logprob": -0.11817822574583953, "compression_ratio": 1.7, "no_speech_prob": 0.0037296307273209095}, {"id": 188, "seek": 109992, "start": 1109.2, "end": 1113.76, "text": " that there was even the possibility of consciousness, but maybe it's buried there inside this complex", "tokens": [50828, 300, 456, 390, 754, 264, 7959, 295, 10081, 11, 457, 1310, 309, 311, 14101, 456, 1854, 341, 3997, 51056], "temperature": 0.0, "avg_logprob": -0.11817822574583953, "compression_ratio": 1.7, "no_speech_prob": 0.0037296307273209095}, {"id": 189, "seek": 109992, "start": 1113.76, "end": 1118.72, "text": " thing somehow. So that's the, that's the kind of question that fascinated me. And I wrote this", "tokens": [51056, 551, 6063, 13, 407, 300, 311, 264, 11, 300, 311, 264, 733, 295, 1168, 300, 24597, 385, 13, 400, 286, 4114, 341, 51304], "temperature": 0.0, "avg_logprob": -0.11817822574583953, "compression_ratio": 1.7, "no_speech_prob": 0.0037296307273209095}, {"id": 190, "seek": 109992, "start": 1118.72, "end": 1124.24, "text": " paper called Conscious Exotica, which is all about trying to, trying to make that Viconstinian", "tokens": [51304, 3035, 1219, 6923, 4139, 2111, 310, 2262, 11, 597, 307, 439, 466, 1382, 281, 11, 1382, 281, 652, 300, 33316, 4068, 259, 952, 51580], "temperature": 0.0, "avg_logprob": -0.11817822574583953, "compression_ratio": 1.7, "no_speech_prob": 0.0037296307273209095}, {"id": 191, "seek": 112424, "start": 1124.24, "end": 1130.64, "text": " perspective encompass this possibility as well. Yeah, and maybe we should talk about that before", "tokens": [50364, 4585, 28268, 341, 7959, 382, 731, 13, 865, 11, 293, 1310, 321, 820, 751, 466, 300, 949, 50684], "temperature": 0.0, "avg_logprob": -0.1209932041168213, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.018583348020911217}, {"id": 192, "seek": 112424, "start": 1130.64, "end": 1136.64, "text": " the language paper, just because it's, it's what we're talking about now. But there's a few things", "tokens": [50684, 264, 2856, 3035, 11, 445, 570, 309, 311, 11, 309, 311, 437, 321, 434, 1417, 466, 586, 13, 583, 456, 311, 257, 1326, 721, 50984], "temperature": 0.0, "avg_logprob": -0.1209932041168213, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.018583348020911217}, {"id": 193, "seek": 112424, "start": 1136.64, "end": 1141.44, "text": " you said there, which are really interesting. So, you know, when Chomsky talks about ghosts in the", "tokens": [50984, 291, 848, 456, 11, 597, 366, 534, 1880, 13, 407, 11, 291, 458, 11, 562, 761, 4785, 4133, 6686, 466, 21744, 294, 264, 51224], "temperature": 0.0, "avg_logprob": -0.1209932041168213, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.018583348020911217}, {"id": 194, "seek": 112424, "start": 1141.44, "end": 1148.32, "text": " machine, and he goes back to Galileo and Descartes, and actually it was Descartes who, you know,", "tokens": [51224, 3479, 11, 293, 415, 1709, 646, 281, 46576, 78, 293, 3885, 44672, 279, 11, 293, 767, 309, 390, 3885, 44672, 279, 567, 11, 291, 458, 11, 51568], "temperature": 0.0, "avg_logprob": -0.1209932041168213, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.018583348020911217}, {"id": 195, "seek": 114832, "start": 1148.32, "end": 1154.6399999999999, "text": " introduced this kind of mind-body dualism, you know, which was kind of a move away from", "tokens": [50364, 7268, 341, 733, 295, 1575, 12, 1067, 11848, 1434, 11, 291, 458, 11, 597, 390, 733, 295, 257, 1286, 1314, 490, 50680], "temperature": 0.0, "avg_logprob": -0.0977297001056843, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.10978145152330399}, {"id": 196, "seek": 114832, "start": 1154.6399999999999, "end": 1158.3999999999999, "text": " the previous desire to have a mechanistic understanding of the world that we live in.", "tokens": [50680, 264, 3894, 7516, 281, 362, 257, 4236, 3142, 3701, 295, 264, 1002, 300, 321, 1621, 294, 13, 50868], "temperature": 0.0, "avg_logprob": -0.0977297001056843, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.10978145152330399}, {"id": 197, "seek": 114832, "start": 1158.3999999999999, "end": 1164.3999999999999, "text": " Humans want to understand, and actually so many things in the world eludes our understanding.", "tokens": [50868, 35809, 528, 281, 1223, 11, 293, 767, 370, 867, 721, 294, 264, 1002, 806, 10131, 527, 3701, 13, 51168], "temperature": 0.0, "avg_logprob": -0.0977297001056843, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.10978145152330399}, {"id": 198, "seek": 114832, "start": 1164.3999999999999, "end": 1169.52, "text": " And then that brings us on to David Chalmers' point that the hard problem of consciousness,", "tokens": [51168, 400, 550, 300, 5607, 505, 322, 281, 4389, 761, 304, 18552, 6, 935, 300, 264, 1152, 1154, 295, 10081, 11, 51424], "temperature": 0.0, "avg_logprob": -0.0977297001056843, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.10978145152330399}, {"id": 199, "seek": 114832, "start": 1169.52, "end": 1175.28, "text": " which I suppose is an extension of the mind-body problem. And it's, as you were saying, this", "tokens": [51424, 597, 286, 7297, 307, 364, 10320, 295, 264, 1575, 12, 1067, 1154, 13, 400, 309, 311, 11, 382, 291, 645, 1566, 11, 341, 51712], "temperature": 0.0, "avg_logprob": -0.0977297001056843, "compression_ratio": 1.7251908396946565, "no_speech_prob": 0.10978145152330399}, {"id": 200, "seek": 117528, "start": 1175.28, "end": 1179.92, "text": " little bit extra, right? So we think about, and I agree with Chalmers that intelligence and", "tokens": [50364, 707, 857, 2857, 11, 558, 30, 407, 321, 519, 466, 11, 293, 286, 3986, 365, 761, 304, 18552, 300, 7599, 293, 50596], "temperature": 0.0, "avg_logprob": -0.10382103722942762, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.01917550340294838}, {"id": 201, "seek": 117528, "start": 1179.92, "end": 1184.8, "text": " consciousness are likely entangled or would co-occur together. But he always said that there's", "tokens": [50596, 10081, 366, 3700, 948, 39101, 420, 576, 598, 12, 905, 14112, 1214, 13, 583, 415, 1009, 848, 300, 456, 311, 50840], "temperature": 0.0, "avg_logprob": -0.10382103722942762, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.01917550340294838}, {"id": 202, "seek": 117528, "start": 1184.8, "end": 1190.56, "text": " function, dynamics, and behavior. And then there's that little subjective thing on the top. And for", "tokens": [50840, 2445, 11, 15679, 11, 293, 5223, 13, 400, 550, 456, 311, 300, 707, 25972, 551, 322, 264, 1192, 13, 400, 337, 51128], "temperature": 0.0, "avg_logprob": -0.10382103722942762, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.01917550340294838}, {"id": 203, "seek": 117528, "start": 1190.56, "end": 1195.28, "text": " Chalmers' consciousness, it's almost like, what's the cash value of it? He just thinks it's just", "tokens": [51128, 761, 304, 18552, 6, 10081, 11, 309, 311, 1920, 411, 11, 437, 311, 264, 6388, 2158, 295, 309, 30, 634, 445, 7309, 309, 311, 445, 51364], "temperature": 0.0, "avg_logprob": -0.10382103722942762, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.01917550340294838}, {"id": 204, "seek": 117528, "start": 1195.28, "end": 1199.76, "text": " something on top. It's not really requisite for anything else. And I believe it might be requisite", "tokens": [51364, 746, 322, 1192, 13, 467, 311, 406, 534, 49878, 642, 337, 1340, 1646, 13, 400, 286, 1697, 309, 1062, 312, 49878, 642, 51588], "temperature": 0.0, "avg_logprob": -0.10382103722942762, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.01917550340294838}, {"id": 205, "seek": 119976, "start": 1199.76, "end": 1204.8799999999999, "text": " for intentionality and agency as so did. But what's your take?", "tokens": [50364, 337, 7789, 1860, 293, 7934, 382, 370, 630, 13, 583, 437, 311, 428, 747, 30, 50620], "temperature": 0.0, "avg_logprob": -0.12920873815363104, "compression_ratio": 1.962686567164179, "no_speech_prob": 0.06450125575065613}, {"id": 206, "seek": 119976, "start": 1204.8799999999999, "end": 1208.48, "text": " Well, it's interesting because the whole way that you put that and the whole way that", "tokens": [50620, 1042, 11, 309, 311, 1880, 570, 264, 1379, 636, 300, 291, 829, 300, 293, 264, 1379, 636, 300, 50800], "temperature": 0.0, "avg_logprob": -0.12920873815363104, "compression_ratio": 1.962686567164179, "no_speech_prob": 0.06450125575065613}, {"id": 207, "seek": 119976, "start": 1208.48, "end": 1213.68, "text": " people often talk about this thing is you speak about consciousness. Like, there's this thing,", "tokens": [50800, 561, 2049, 751, 466, 341, 551, 307, 291, 1710, 466, 10081, 13, 1743, 11, 456, 311, 341, 551, 11, 51060], "temperature": 0.0, "avg_logprob": -0.12920873815363104, "compression_ratio": 1.962686567164179, "no_speech_prob": 0.06450125575065613}, {"id": 208, "seek": 119976, "start": 1213.68, "end": 1217.92, "text": " which, you know, there's this singular thing, which maybe it's needed, maybe it isn't, maybe", "tokens": [51060, 597, 11, 291, 458, 11, 456, 311, 341, 20010, 551, 11, 597, 1310, 309, 311, 2978, 11, 1310, 309, 1943, 380, 11, 1310, 51272], "temperature": 0.0, "avg_logprob": -0.12920873815363104, "compression_ratio": 1.962686567164179, "no_speech_prob": 0.06450125575065613}, {"id": 209, "seek": 119976, "start": 1217.92, "end": 1225.36, "text": " it's this, maybe it's that. But I think that whole way of talking is, which is natural for us", "tokens": [51272, 309, 311, 341, 11, 1310, 309, 311, 300, 13, 583, 286, 519, 300, 1379, 636, 295, 1417, 307, 11, 597, 307, 3303, 337, 505, 51644], "temperature": 0.0, "avg_logprob": -0.12920873815363104, "compression_ratio": 1.962686567164179, "no_speech_prob": 0.06450125575065613}, {"id": 210, "seek": 119976, "start": 1225.36, "end": 1229.2, "text": " in many everyday situations. But when it comes to this kind of conversation, I think that whole", "tokens": [51644, 294, 867, 7429, 6851, 13, 583, 562, 309, 1487, 281, 341, 733, 295, 3761, 11, 286, 519, 300, 1379, 51836], "temperature": 0.0, "avg_logprob": -0.12920873815363104, "compression_ratio": 1.962686567164179, "no_speech_prob": 0.06450125575065613}, {"id": 211, "seek": 122920, "start": 1229.2, "end": 1235.44, "text": " way of talking is maybe not quite right, because we're thinking of consciousness as this, you know,", "tokens": [50364, 636, 295, 1417, 307, 1310, 406, 1596, 558, 11, 570, 321, 434, 1953, 295, 10081, 382, 341, 11, 291, 458, 11, 50676], "temperature": 0.0, "avg_logprob": -0.10646891593933105, "compression_ratio": 1.9344262295081966, "no_speech_prob": 0.004312918055802584}, {"id": 212, "seek": 122920, "start": 1235.44, "end": 1239.2, "text": " we're reifying it, turning it into this thing. Whereas I think maybe at that point we have to", "tokens": [50676, 321, 434, 319, 5489, 309, 11, 6246, 309, 666, 341, 551, 13, 13813, 286, 519, 1310, 412, 300, 935, 321, 362, 281, 50864], "temperature": 0.0, "avg_logprob": -0.10646891593933105, "compression_ratio": 1.9344262295081966, "no_speech_prob": 0.004312918055802584}, {"id": 213, "seek": 122920, "start": 1239.2, "end": 1244.16, "text": " take a step back and we have to say, well, when we talk about, when we use that word,", "tokens": [50864, 747, 257, 1823, 646, 293, 321, 362, 281, 584, 11, 731, 11, 562, 321, 751, 466, 11, 562, 321, 764, 300, 1349, 11, 51112], "temperature": 0.0, "avg_logprob": -0.10646891593933105, "compression_ratio": 1.9344262295081966, "no_speech_prob": 0.004312918055802584}, {"id": 214, "seek": 122920, "start": 1244.16, "end": 1248.72, "text": " conscious or consciousness, so we use it in all kinds of different ways in different contexts.", "tokens": [51112, 6648, 420, 10081, 11, 370, 321, 764, 309, 294, 439, 3685, 295, 819, 2098, 294, 819, 30628, 13, 51340], "temperature": 0.0, "avg_logprob": -0.10646891593933105, "compression_ratio": 1.9344262295081966, "no_speech_prob": 0.004312918055802584}, {"id": 215, "seek": 122920, "start": 1248.72, "end": 1255.2, "text": " And so when we talk about, you know, we might talk about it in the context of an animal, we might", "tokens": [51340, 400, 370, 562, 321, 751, 466, 11, 291, 458, 11, 321, 1062, 751, 466, 309, 294, 264, 4319, 295, 364, 5496, 11, 321, 1062, 51664], "temperature": 0.0, "avg_logprob": -0.10646891593933105, "compression_ratio": 1.9344262295081966, "no_speech_prob": 0.004312918055802584}, {"id": 216, "seek": 125520, "start": 1255.2, "end": 1261.52, "text": " say, well, the animal, you know, this dog is aware of its environment. So, you know, this dog can see", "tokens": [50364, 584, 11, 731, 11, 264, 5496, 11, 291, 458, 11, 341, 3000, 307, 3650, 295, 1080, 2823, 13, 407, 11, 291, 458, 11, 341, 3000, 393, 536, 50680], "temperature": 0.0, "avg_logprob": -0.12444516538663675, "compression_ratio": 2.1327433628318584, "no_speech_prob": 0.023844053968787193}, {"id": 217, "seek": 125520, "start": 1262.0800000000002, "end": 1267.1200000000001, "text": " the bowl in front of it, it can see me, it can see the door, it can see the trees, it can see", "tokens": [50708, 264, 6571, 294, 1868, 295, 309, 11, 309, 393, 536, 385, 11, 309, 393, 536, 264, 2853, 11, 309, 393, 536, 264, 5852, 11, 309, 393, 536, 50960], "temperature": 0.0, "avg_logprob": -0.12444516538663675, "compression_ratio": 2.1327433628318584, "no_speech_prob": 0.023844053968787193}, {"id": 218, "seek": 125520, "start": 1267.1200000000001, "end": 1271.8400000000001, "text": " the squirrel, you know, and it can smell more like you'd smell all of these things as well.", "tokens": [50960, 264, 28565, 11, 291, 458, 11, 293, 309, 393, 4316, 544, 411, 291, 1116, 4316, 439, 295, 613, 721, 382, 731, 13, 51196], "temperature": 0.0, "avg_logprob": -0.12444516538663675, "compression_ratio": 2.1327433628318584, "no_speech_prob": 0.023844053968787193}, {"id": 219, "seek": 125520, "start": 1272.4, "end": 1278.0800000000002, "text": " So we use consciousness, you know, we talk about consciousness in that sense. And we also talk", "tokens": [51224, 407, 321, 764, 10081, 11, 291, 458, 11, 321, 751, 466, 10081, 294, 300, 2020, 13, 400, 321, 611, 751, 51508], "temperature": 0.0, "avg_logprob": -0.12444516538663675, "compression_ratio": 2.1327433628318584, "no_speech_prob": 0.023844053968787193}, {"id": 220, "seek": 125520, "start": 1278.0800000000002, "end": 1283.52, "text": " about our self-consciousness, you know, we talk about the fact that we're aware of our own thoughts", "tokens": [51508, 466, 527, 2698, 12, 19877, 1287, 11, 291, 458, 11, 321, 751, 466, 264, 1186, 300, 321, 434, 3650, 295, 527, 1065, 4598, 51780], "temperature": 0.0, "avg_logprob": -0.12444516538663675, "compression_ratio": 2.1327433628318584, "no_speech_prob": 0.023844053968787193}, {"id": 221, "seek": 128352, "start": 1283.52, "end": 1290.56, "text": " and we talk about our inner life and we use consciousness to encompass that as well.", "tokens": [50364, 293, 321, 751, 466, 527, 7284, 993, 293, 321, 764, 10081, 281, 28268, 300, 382, 731, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1408327685462104, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.00045349070569500327}, {"id": 222, "seek": 128352, "start": 1291.52, "end": 1297.92, "text": " We often use consciousness in the context scientifically of a distinction between", "tokens": [50764, 492, 2049, 764, 10081, 294, 264, 4319, 39719, 295, 257, 16844, 1296, 51084], "temperature": 0.0, "avg_logprob": -0.1408327685462104, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.00045349070569500327}, {"id": 223, "seek": 128352, "start": 1298.72, "end": 1302.6399999999999, "text": " conscious and unconscious processes. And that's a very interesting distinction because", "tokens": [51124, 6648, 293, 18900, 7555, 13, 400, 300, 311, 257, 588, 1880, 16844, 570, 51320], "temperature": 0.0, "avg_logprob": -0.1408327685462104, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.00045349070569500327}, {"id": 224, "seek": 128352, "start": 1303.36, "end": 1308.6399999999999, "text": " when we're consciously aware of a stimulus as humans, then a whole lot of things come together.", "tokens": [51356, 562, 321, 434, 32538, 3650, 295, 257, 21366, 382, 6255, 11, 550, 257, 1379, 688, 295, 721, 808, 1214, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1408327685462104, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.00045349070569500327}, {"id": 225, "seek": 130864, "start": 1308.88, "end": 1315.0400000000002, "text": " We're able to kind of like deal with novelty better, we're able to report it, we're able to", "tokens": [50376, 492, 434, 1075, 281, 733, 295, 411, 2028, 365, 44805, 1101, 11, 321, 434, 1075, 281, 2275, 309, 11, 321, 434, 1075, 281, 50684], "temperature": 0.0, "avg_logprob": -0.11948813189257372, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.006335421930998564}, {"id": 226, "seek": 130864, "start": 1315.0400000000002, "end": 1321.2800000000002, "text": " remember things better. So whereas when we perhaps are unconsciously or there's a kind", "tokens": [50684, 1604, 721, 1101, 13, 407, 9735, 562, 321, 4317, 366, 18900, 356, 420, 456, 311, 257, 733, 50996], "temperature": 0.0, "avg_logprob": -0.11948813189257372, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.006335421930998564}, {"id": 227, "seek": 130864, "start": 1321.2800000000002, "end": 1325.68, "text": " of unconscious processing of the stimulus, then we still can respond to it behaviorally, but", "tokens": [50996, 295, 18900, 9007, 295, 264, 21366, 11, 550, 321, 920, 393, 4196, 281, 309, 5223, 379, 11, 457, 51216], "temperature": 0.0, "avg_logprob": -0.11948813189257372, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.006335421930998564}, {"id": 228, "seek": 130864, "start": 1326.5600000000002, "end": 1330.3200000000002, "text": " and it can have queuing effects and so on, but it doesn't have all those other things.", "tokens": [51260, 293, 309, 393, 362, 631, 9635, 5065, 293, 370, 322, 11, 457, 309, 1177, 380, 362, 439, 729, 661, 721, 13, 51448], "temperature": 0.0, "avg_logprob": -0.11948813189257372, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.006335421930998564}, {"id": 229, "seek": 130864, "start": 1330.3200000000002, "end": 1336.3200000000002, "text": " So this and that's kind of, there's a kind of integrative function for consciousness there.", "tokens": [51448, 407, 341, 293, 300, 311, 733, 295, 11, 456, 311, 257, 733, 295, 3572, 1166, 2445, 337, 10081, 456, 13, 51748], "temperature": 0.0, "avg_logprob": -0.11948813189257372, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.006335421930998564}, {"id": 230, "seek": 133632, "start": 1336.32, "end": 1342.24, "text": " And then on top of all of that, there is the capacity for suffering and joy that comes with.", "tokens": [50364, 400, 550, 322, 1192, 295, 439, 295, 300, 11, 456, 307, 264, 6042, 337, 7755, 293, 6258, 300, 1487, 365, 13, 50660], "temperature": 0.0, "avg_logprob": -0.125277269970287, "compression_ratio": 1.671497584541063, "no_speech_prob": 0.00048509862972423434}, {"id": 231, "seek": 133632, "start": 1342.24, "end": 1349.28, "text": " So often there's valence to consciousness, you know, so that's another thing.", "tokens": [50660, 407, 2049, 456, 311, 1323, 655, 281, 10081, 11, 291, 458, 11, 370, 300, 311, 1071, 551, 13, 51012], "temperature": 0.0, "avg_logprob": -0.125277269970287, "compression_ratio": 1.671497584541063, "no_speech_prob": 0.00048509862972423434}, {"id": 232, "seek": 133632, "start": 1349.28, "end": 1352.8, "text": " So all of these things, they come as a package in humans, but when we speak about", "tokens": [51012, 407, 439, 295, 613, 721, 11, 436, 808, 382, 257, 7372, 294, 6255, 11, 457, 562, 321, 1710, 466, 51188], "temperature": 0.0, "avg_logprob": -0.125277269970287, "compression_ratio": 1.671497584541063, "no_speech_prob": 0.00048509862972423434}, {"id": 233, "seek": 133632, "start": 1353.36, "end": 1361.2, "text": " edge cases, then these things come apart and we need to speak about them separately, I think.", "tokens": [51216, 4691, 3331, 11, 550, 613, 721, 808, 4936, 293, 321, 643, 281, 1710, 466, 552, 14759, 11, 286, 519, 13, 51608], "temperature": 0.0, "avg_logprob": -0.125277269970287, "compression_ratio": 1.671497584541063, "no_speech_prob": 0.00048509862972423434}, {"id": 234, "seek": 136120, "start": 1361.2, "end": 1366.24, "text": " Fascinating. I mean, there are two kind of minor digressions there. I mean,", "tokens": [50364, 49098, 8205, 13, 286, 914, 11, 456, 366, 732, 733, 295, 6696, 2528, 735, 626, 456, 13, 286, 914, 11, 50616], "temperature": 0.0, "avg_logprob": -0.17970844930853724, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.023512203246355057}, {"id": 235, "seek": 136120, "start": 1366.24, "end": 1370.0, "text": " you were talking about these planes of consciousness, which is also very interesting.", "tokens": [50616, 291, 645, 1417, 466, 613, 14952, 295, 10081, 11, 597, 307, 611, 588, 1880, 13, 50804], "temperature": 0.0, "avg_logprob": -0.17970844930853724, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.023512203246355057}, {"id": 236, "seek": 136120, "start": 1370.0, "end": 1374.64, "text": " And maybe we could get into the integrated information theory or the global workspace", "tokens": [50804, 400, 1310, 321, 727, 483, 666, 264, 10919, 1589, 5261, 420, 264, 4338, 32706, 51036], "temperature": 0.0, "avg_logprob": -0.17970844930853724, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.023512203246355057}, {"id": 237, "seek": 136120, "start": 1374.64, "end": 1377.76, "text": " theory just for the audience, just to give them some context.", "tokens": [51036, 5261, 445, 337, 264, 4034, 11, 445, 281, 976, 552, 512, 4319, 13, 51192], "temperature": 0.0, "avg_logprob": -0.17970844930853724, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.023512203246355057}, {"id": 238, "seek": 136120, "start": 1377.76, "end": 1380.56, "text": " Yeah, sure. Or do you want me to say a few words about that?", "tokens": [51192, 865, 11, 988, 13, 1610, 360, 291, 528, 385, 281, 584, 257, 1326, 2283, 466, 300, 30, 51332], "temperature": 0.0, "avg_logprob": -0.17970844930853724, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.023512203246355057}, {"id": 239, "seek": 136120, "start": 1380.56, "end": 1381.52, "text": " Oh, please, yeah.", "tokens": [51332, 876, 11, 1767, 11, 1338, 13, 51380], "temperature": 0.0, "avg_logprob": -0.17970844930853724, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.023512203246355057}, {"id": 240, "seek": 136120, "start": 1381.52, "end": 1387.92, "text": " Okay. Yeah. So there are a number of kind of candidates for a scientific theory of consciousness.", "tokens": [51380, 1033, 13, 865, 13, 407, 456, 366, 257, 1230, 295, 733, 295, 11255, 337, 257, 8134, 5261, 295, 10081, 13, 51700], "temperature": 0.0, "avg_logprob": -0.17970844930853724, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.023512203246355057}, {"id": 241, "seek": 138792, "start": 1388.72, "end": 1392.16, "text": " And you just mentioned two of the leading ones, which are global workspace theory and", "tokens": [50404, 400, 291, 445, 2835, 732, 295, 264, 5775, 2306, 11, 597, 366, 4338, 32706, 5261, 293, 50576], "temperature": 0.0, "avg_logprob": -0.15442132512363818, "compression_ratio": 1.7698412698412698, "no_speech_prob": 0.004970105364918709}, {"id": 242, "seek": 138792, "start": 1392.16, "end": 1397.6000000000001, "text": " integrated information theory. And so global workspace theory. So that's, that's Bernie", "tokens": [50576, 10919, 1589, 5261, 13, 400, 370, 4338, 32706, 5261, 13, 407, 300, 311, 11, 300, 311, 22426, 50848], "temperature": 0.0, "avg_logprob": -0.15442132512363818, "compression_ratio": 1.7698412698412698, "no_speech_prob": 0.004970105364918709}, {"id": 243, "seek": 138792, "start": 1397.6000000000001, "end": 1403.3600000000001, "text": " Baals's was originated by Bernie Baals and has been developed by Stanislaus, Dehen and colleagues.", "tokens": [50848, 6777, 1124, 311, 390, 31129, 538, 22426, 6777, 1124, 293, 575, 668, 4743, 538, 10061, 271, 22590, 11, 1346, 2932, 293, 7734, 13, 51136], "temperature": 0.0, "avg_logprob": -0.15442132512363818, "compression_ratio": 1.7698412698412698, "no_speech_prob": 0.004970105364918709}, {"id": 244, "seek": 138792, "start": 1403.3600000000001, "end": 1408.24, "text": " So the idea there is it's, it does rest on this sort of architectural idea, which is that,", "tokens": [51136, 407, 264, 1558, 456, 307, 309, 311, 11, 309, 775, 1472, 322, 341, 1333, 295, 26621, 1558, 11, 597, 307, 300, 11, 51380], "temperature": 0.0, "avg_logprob": -0.15442132512363818, "compression_ratio": 1.7698412698412698, "no_speech_prob": 0.004970105364918709}, {"id": 245, "seek": 138792, "start": 1410.16, "end": 1415.44, "text": " which is that we think of the brain, the biological brain as comprising, you know,", "tokens": [51476, 597, 307, 300, 321, 519, 295, 264, 3567, 11, 264, 13910, 3567, 382, 16802, 3436, 11, 291, 458, 11, 51740], "temperature": 0.0, "avg_logprob": -0.15442132512363818, "compression_ratio": 1.7698412698412698, "no_speech_prob": 0.004970105364918709}, {"id": 246, "seek": 141544, "start": 1415.44, "end": 1419.6000000000001, "text": " a very large number of parallel processes. This is kind of a natural way to think of the brain,", "tokens": [50364, 257, 588, 2416, 1230, 295, 8952, 7555, 13, 639, 307, 733, 295, 257, 3303, 636, 281, 519, 295, 264, 3567, 11, 50572], "temperature": 0.0, "avg_logprob": -0.08570402162569063, "compression_ratio": 1.962809917355372, "no_speech_prob": 0.0019369556102901697}, {"id": 247, "seek": 141544, "start": 1419.6000000000001, "end": 1425.1200000000001, "text": " a large number of parallel processes. And it, and the global workspace theory posits a particular", "tokens": [50572, 257, 2416, 1230, 295, 8952, 7555, 13, 400, 309, 11, 293, 264, 4338, 32706, 5261, 1366, 1208, 257, 1729, 50848], "temperature": 0.0, "avg_logprob": -0.08570402162569063, "compression_ratio": 1.962809917355372, "no_speech_prob": 0.0019369556102901697}, {"id": 248, "seek": 141544, "start": 1425.1200000000001, "end": 1430.48, "text": " way in which these, these processes interact and communicate with each other. And that is via", "tokens": [50848, 636, 294, 597, 613, 11, 613, 7555, 4648, 293, 7890, 365, 1184, 661, 13, 400, 300, 307, 5766, 51116], "temperature": 0.0, "avg_logprob": -0.08570402162569063, "compression_ratio": 1.962809917355372, "no_speech_prob": 0.0019369556102901697}, {"id": 249, "seek": 141544, "start": 1430.48, "end": 1436.0, "text": " this global workspace. And the idea there is that, is that there are sort of two modes of", "tokens": [51116, 341, 4338, 32706, 13, 400, 264, 1558, 456, 307, 300, 11, 307, 300, 456, 366, 1333, 295, 732, 14068, 295, 51392], "temperature": 0.0, "avg_logprob": -0.08570402162569063, "compression_ratio": 1.962809917355372, "no_speech_prob": 0.0019369556102901697}, {"id": 250, "seek": 141544, "start": 1436.0, "end": 1443.04, "text": " processing that go on. So in one mode of processing, the, these parallel processes just do their,", "tokens": [51392, 9007, 300, 352, 322, 13, 407, 294, 472, 4391, 295, 9007, 11, 264, 11, 613, 8952, 7555, 445, 360, 641, 11, 51744], "temperature": 0.0, "avg_logprob": -0.08570402162569063, "compression_ratio": 1.962809917355372, "no_speech_prob": 0.0019369556102901697}, {"id": 251, "seek": 144304, "start": 1443.04, "end": 1449.44, "text": " their own thing independently. And in the other mode of processing, they are working via this", "tokens": [50364, 641, 1065, 551, 21761, 13, 400, 294, 264, 661, 4391, 295, 9007, 11, 436, 366, 1364, 5766, 341, 50684], "temperature": 0.0, "avg_logprob": -0.07238728949364195, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0029026626143604517}, {"id": 252, "seek": 144304, "start": 1449.44, "end": 1454.6399999999999, "text": " global workspace theory. So the idea is that they, you might think of them as, as, you know,", "tokens": [50684, 4338, 32706, 5261, 13, 407, 264, 1558, 307, 300, 436, 11, 291, 1062, 519, 295, 552, 382, 11, 382, 11, 291, 458, 11, 50944], "temperature": 0.0, "avg_logprob": -0.07238728949364195, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0029026626143604517}, {"id": 253, "seek": 144304, "start": 1454.6399999999999, "end": 1459.68, "text": " depositing messages, if you like, in this global workspace, which are then broadcast out to all", "tokens": [50944, 19930, 1748, 7897, 11, 498, 291, 411, 11, 294, 341, 4338, 32706, 11, 597, 366, 550, 9975, 484, 281, 439, 51196], "temperature": 0.0, "avg_logprob": -0.07238728949364195, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0029026626143604517}, {"id": 254, "seek": 144304, "start": 1459.68, "end": 1463.44, "text": " of the other processes. So, so it's, so there's this kind of, but I think thinking of it in", "tokens": [51196, 295, 264, 661, 7555, 13, 407, 11, 370, 309, 311, 11, 370, 456, 311, 341, 733, 295, 11, 457, 286, 519, 1953, 295, 309, 294, 51384], "temperature": 0.0, "avg_logprob": -0.07238728949364195, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0029026626143604517}, {"id": 255, "seek": 144304, "start": 1463.44, "end": 1467.6, "text": " terms of messages is not quite the right way of thinking of it is better to think in terms of", "tokens": [51384, 2115, 295, 7897, 307, 406, 1596, 264, 558, 636, 295, 1953, 295, 309, 307, 1101, 281, 519, 294, 2115, 295, 51592], "temperature": 0.0, "avg_logprob": -0.07238728949364195, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0029026626143604517}, {"id": 256, "seek": 144304, "start": 1467.6, "end": 1471.84, "text": " kind of signaling and information and so on. But that's a natural way to think of it. But", "tokens": [51592, 733, 295, 38639, 293, 1589, 293, 370, 322, 13, 583, 300, 311, 257, 3303, 636, 281, 519, 295, 309, 13, 583, 51804], "temperature": 0.0, "avg_logprob": -0.07238728949364195, "compression_ratio": 1.8979591836734695, "no_speech_prob": 0.0029026626143604517}, {"id": 257, "seek": 147184, "start": 1471.84, "end": 1479.6, "text": " so the, so these, so in, in that mode, the, these processes are sort of disseminating their influence", "tokens": [50364, 370, 264, 11, 370, 613, 11, 370, 294, 11, 294, 300, 4391, 11, 264, 11, 613, 7555, 366, 1333, 295, 34585, 990, 641, 6503, 50752], "temperature": 0.0, "avg_logprob": -0.13751663010695886, "compression_ratio": 1.9508196721311475, "no_speech_prob": 0.002604251727461815}, {"id": 258, "seek": 147184, "start": 1479.6, "end": 1485.1999999999998, "text": " to all the other processes. And that's the global kind of broadcast aspect of it. And that's when", "tokens": [50752, 281, 439, 264, 661, 7555, 13, 400, 300, 311, 264, 4338, 733, 295, 9975, 4171, 295, 309, 13, 400, 300, 311, 562, 51032], "temperature": 0.0, "avg_logprob": -0.13751663010695886, "compression_ratio": 1.9508196721311475, "no_speech_prob": 0.002604251727461815}, {"id": 259, "seek": 147184, "start": 1485.1999999999998, "end": 1490.6399999999999, "text": " consciousness, well, that's when information processing is conscious, according to global", "tokens": [51032, 10081, 11, 731, 11, 300, 311, 562, 1589, 9007, 307, 6648, 11, 4650, 281, 4338, 51304], "temperature": 0.0, "avg_logprob": -0.13751663010695886, "compression_ratio": 1.9508196721311475, "no_speech_prob": 0.002604251727461815}, {"id": 260, "seek": 147184, "start": 1490.6399999999999, "end": 1494.56, "text": " workspace theory, as opposed to when it's all just local and the processes are doing their own", "tokens": [51304, 32706, 5261, 11, 382, 8851, 281, 562, 309, 311, 439, 445, 2654, 293, 264, 7555, 366, 884, 641, 1065, 51500], "temperature": 0.0, "avg_logprob": -0.13751663010695886, "compression_ratio": 1.9508196721311475, "no_speech_prob": 0.002604251727461815}, {"id": 261, "seek": 147184, "start": 1494.56, "end": 1500.48, "text": " thing. That's, that's not that that processing is not conscious. So there's a dist, so it's", "tokens": [51500, 551, 13, 663, 311, 11, 300, 311, 406, 300, 300, 9007, 307, 406, 6648, 13, 407, 456, 311, 257, 1483, 11, 370, 309, 311, 51796], "temperature": 0.0, "avg_logprob": -0.13751663010695886, "compression_ratio": 1.9508196721311475, "no_speech_prob": 0.002604251727461815}, {"id": 262, "seek": 150048, "start": 1500.48, "end": 1505.2, "text": " about teasing out this distinction between conscious information processing and unconscious", "tokens": [50364, 466, 37720, 484, 341, 16844, 1296, 6648, 1589, 9007, 293, 18900, 50600], "temperature": 0.0, "avg_logprob": -0.1039399499968281, "compression_ratio": 1.832258064516129, "no_speech_prob": 0.00108798046130687}, {"id": 263, "seek": 150048, "start": 1505.2, "end": 1511.04, "text": " information processing. Now, all of those terms, by the way, are deeply philosophically problematic", "tokens": [50600, 1589, 9007, 13, 823, 11, 439, 295, 729, 2115, 11, 538, 264, 636, 11, 366, 8760, 14529, 984, 19011, 50892], "temperature": 0.0, "avg_logprob": -0.1039399499968281, "compression_ratio": 1.832258064516129, "no_speech_prob": 0.00108798046130687}, {"id": 264, "seek": 150048, "start": 1511.04, "end": 1515.3600000000001, "text": " and to go in, you know, you have to sort of do it properly, you have to kind of unpack them all", "tokens": [50892, 293, 281, 352, 294, 11, 291, 458, 11, 291, 362, 281, 1333, 295, 360, 309, 6108, 11, 291, 362, 281, 733, 295, 26699, 552, 439, 51108], "temperature": 0.0, "avg_logprob": -0.1039399499968281, "compression_ratio": 1.832258064516129, "no_speech_prob": 0.00108798046130687}, {"id": 265, "seek": 150048, "start": 1515.3600000000001, "end": 1520.32, "text": " in very carefully. And that's what my book try, try, tries to do. But so essentially, it's about", "tokens": [51108, 294, 588, 7500, 13, 400, 300, 311, 437, 452, 1446, 853, 11, 853, 11, 9898, 281, 360, 13, 583, 370, 4476, 11, 309, 311, 466, 51356], "temperature": 0.0, "avg_logprob": -0.1039399499968281, "compression_ratio": 1.832258064516129, "no_speech_prob": 0.00108798046130687}, {"id": 266, "seek": 150048, "start": 1520.32, "end": 1524.8, "text": " so the essential idea, though, is to do with broadcast and dissemination of information", "tokens": [51356, 370, 264, 7115, 1558, 11, 1673, 11, 307, 281, 360, 365, 9975, 293, 34585, 399, 295, 1589, 51580], "temperature": 0.0, "avg_logprob": -0.1039399499968281, "compression_ratio": 1.832258064516129, "no_speech_prob": 0.00108798046130687}, {"id": 267, "seek": 150048, "start": 1524.8, "end": 1529.1200000000001, "text": " throughout the brain and going from like local processes and help them having global influence.", "tokens": [51580, 3710, 264, 3567, 293, 516, 490, 411, 2654, 7555, 293, 854, 552, 1419, 4338, 6503, 13, 51796], "temperature": 0.0, "avg_logprob": -0.1039399499968281, "compression_ratio": 1.832258064516129, "no_speech_prob": 0.00108798046130687}, {"id": 268, "seek": 152912, "start": 1529.1999999999998, "end": 1532.2399999999998, "text": " And that's what consciousness is all about according to global workspace theory.", "tokens": [50368, 400, 300, 311, 437, 10081, 307, 439, 466, 4650, 281, 4338, 32706, 5261, 13, 50520], "temperature": 0.0, "avg_logprob": -0.11821477229778583, "compression_ratio": 1.8722466960352422, "no_speech_prob": 0.006723918952047825}, {"id": 269, "seek": 152912, "start": 1533.1999999999998, "end": 1537.9199999999998, "text": " Okay, so integrated information theory. So I think so integrated information theory,", "tokens": [50568, 1033, 11, 370, 10919, 1589, 5261, 13, 407, 286, 519, 370, 10919, 1589, 5261, 11, 50804], "temperature": 0.0, "avg_logprob": -0.11821477229778583, "compression_ratio": 1.8722466960352422, "no_speech_prob": 0.006723918952047825}, {"id": 270, "seek": 152912, "start": 1537.9199999999998, "end": 1548.0, "text": " which is Giulio Tononi's theory, which Giulio Tononi thinks is kind of kind of incompatible", "tokens": [50804, 597, 307, 38679, 1004, 11385, 17049, 311, 5261, 11, 597, 38679, 1004, 11385, 17049, 7309, 307, 733, 295, 733, 295, 40393, 267, 964, 51308], "temperature": 0.0, "avg_logprob": -0.11821477229778583, "compression_ratio": 1.8722466960352422, "no_speech_prob": 0.006723918952047825}, {"id": 271, "seek": 152912, "start": 1548.0, "end": 1552.6399999999999, "text": " in some ways with with global workspace theory. But I don't think that's, that's true. I think", "tokens": [51308, 294, 512, 2098, 365, 365, 4338, 32706, 5261, 13, 583, 286, 500, 380, 519, 300, 311, 11, 300, 311, 2074, 13, 286, 519, 51540], "temperature": 0.0, "avg_logprob": -0.11821477229778583, "compression_ratio": 1.8722466960352422, "no_speech_prob": 0.006723918952047825}, {"id": 272, "seek": 152912, "start": 1552.6399999999999, "end": 1556.32, "text": " I think that there's a lot of synergy between the two theories, in fact.", "tokens": [51540, 286, 519, 300, 456, 311, 257, 688, 295, 50163, 1296, 264, 732, 13667, 11, 294, 1186, 13, 51724], "temperature": 0.0, "avg_logprob": -0.11821477229778583, "compression_ratio": 1.8722466960352422, "no_speech_prob": 0.006723918952047825}, {"id": 273, "seek": 155632, "start": 1556.8, "end": 1563.9199999999998, "text": " But but that's because they so they come with the same for integrated information theory", "tokens": [50388, 583, 457, 300, 311, 570, 436, 370, 436, 808, 365, 264, 912, 337, 10919, 1589, 5261, 50744], "temperature": 0.0, "avg_logprob": -0.14294744974159332, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.004634378477931023}, {"id": 274, "seek": 155632, "start": 1564.6399999999999, "end": 1571.04, "text": " has sort of two aspects to it. So according to Giulio Tononi, he really is trying to pin down", "tokens": [50780, 575, 1333, 295, 732, 7270, 281, 309, 13, 407, 4650, 281, 38679, 1004, 11385, 17049, 11, 415, 534, 307, 1382, 281, 5447, 760, 51100], "temperature": 0.0, "avg_logprob": -0.14294744974159332, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.004634378477931023}, {"id": 275, "seek": 155632, "start": 1571.04, "end": 1577.12, "text": " a property, which is almost like a physical property, which is identical with consciousness.", "tokens": [51100, 257, 4707, 11, 597, 307, 1920, 411, 257, 4001, 4707, 11, 597, 307, 14800, 365, 10081, 13, 51404], "temperature": 0.0, "avg_logprob": -0.14294744974159332, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.004634378477931023}, {"id": 276, "seek": 155632, "start": 1577.12, "end": 1582.8, "text": " So you can actually speak about the amount of consciousness in any system that you that you", "tokens": [51404, 407, 291, 393, 767, 1710, 466, 264, 2372, 295, 10081, 294, 604, 1185, 300, 291, 300, 291, 51688], "temperature": 0.0, "avg_logprob": -0.14294744974159332, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.004634378477931023}, {"id": 277, "seek": 158280, "start": 1582.8, "end": 1588.08, "text": " look at phi, he could this is good, it's phi. So the phi is a number how is actually a number of", "tokens": [50364, 574, 412, 13107, 11, 415, 727, 341, 307, 665, 11, 309, 311, 13107, 13, 407, 264, 13107, 307, 257, 1230, 577, 307, 767, 257, 1230, 295, 50628], "temperature": 0.0, "avg_logprob": -0.19552013066809948, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.007859480567276478}, {"id": 278, "seek": 158280, "start": 1588.08, "end": 1593.68, "text": " how much consciousness is present in the system, like, like part of your brain, your whole brain,", "tokens": [50628, 577, 709, 10081, 307, 1974, 294, 264, 1185, 11, 411, 11, 411, 644, 295, 428, 3567, 11, 428, 1379, 3567, 11, 50908], "temperature": 0.0, "avg_logprob": -0.19552013066809948, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.007859480567276478}, {"id": 279, "seek": 158280, "start": 1593.68, "end": 1599.44, "text": " or you as a person, or a flock of bats, or whatever, so you can or toaster, you know,", "tokens": [50908, 420, 291, 382, 257, 954, 11, 420, 257, 34819, 295, 26943, 11, 420, 2035, 11, 370, 291, 393, 420, 281, 1727, 11, 291, 458, 11, 51196], "temperature": 0.0, "avg_logprob": -0.19552013066809948, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.007859480567276478}, {"id": 280, "seek": 158280, "start": 1599.44, "end": 1604.72, "text": " so you can give a number to how much consciousness there is, there is there according to his theory.", "tokens": [51196, 370, 291, 393, 976, 257, 1230, 281, 577, 709, 10081, 456, 307, 11, 456, 307, 456, 4650, 281, 702, 5261, 13, 51460], "temperature": 0.0, "avg_logprob": -0.19552013066809948, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.007859480567276478}, {"id": 281, "seek": 158280, "start": 1604.72, "end": 1611.36, "text": " And it's a mathematical theory based on Shannon's information theory. And it's but it and but it's", "tokens": [51460, 400, 309, 311, 257, 18894, 5261, 2361, 322, 28974, 311, 1589, 5261, 13, 400, 309, 311, 457, 309, 293, 457, 309, 311, 51792], "temperature": 0.0, "avg_logprob": -0.19552013066809948, "compression_ratio": 1.9123505976095618, "no_speech_prob": 0.007859480567276478}, {"id": 282, "seek": 161136, "start": 1611.4399999999998, "end": 1617.12, "text": " all about trying to see how much information is processed by the individual parts of the system", "tokens": [50368, 439, 466, 1382, 281, 536, 577, 709, 1589, 307, 18846, 538, 264, 2609, 3166, 295, 264, 1185, 50652], "temperature": 0.0, "avg_logprob": -0.08773509577700966, "compression_ratio": 1.8037383177570094, "no_speech_prob": 0.0025413220282644033}, {"id": 283, "seek": 161136, "start": 1617.76, "end": 1624.24, "text": " versus how much information is processed by all the parts put together. And it's and it's to do", "tokens": [50684, 5717, 577, 709, 1589, 307, 18846, 538, 439, 264, 3166, 829, 1214, 13, 400, 309, 311, 293, 309, 311, 281, 360, 51008], "temperature": 0.0, "avg_logprob": -0.08773509577700966, "compression_ratio": 1.8037383177570094, "no_speech_prob": 0.0025413220282644033}, {"id": 284, "seek": 161136, "start": 1624.24, "end": 1630.32, "text": " with how much the second thing, you know, exceeds the first thing. And in a sense, and that is how", "tokens": [51008, 365, 577, 709, 264, 1150, 551, 11, 291, 458, 11, 43305, 264, 700, 551, 13, 400, 294, 257, 2020, 11, 293, 300, 307, 577, 51312], "temperature": 0.0, "avg_logprob": -0.08773509577700966, "compression_ratio": 1.8037383177570094, "no_speech_prob": 0.0025413220282644033}, {"id": 285, "seek": 161136, "start": 1630.32, "end": 1637.12, "text": " much consciousness there is there. And, and in a way, it actually has some synergies. If you as", "tokens": [51312, 709, 10081, 456, 307, 456, 13, 400, 11, 293, 294, 257, 636, 11, 309, 767, 575, 512, 33781, 25480, 13, 759, 291, 382, 51652], "temperature": 0.0, "avg_logprob": -0.08773509577700966, "compression_ratio": 1.8037383177570094, "no_speech_prob": 0.0025413220282644033}, {"id": 286, "seek": 163712, "start": 1637.12, "end": 1642.1599999999999, "text": " long as you don't think that it's necessarily measuring, you know, this property of the of", "tokens": [50364, 938, 382, 291, 500, 380, 519, 300, 309, 311, 4725, 13389, 11, 291, 458, 11, 341, 4707, 295, 264, 295, 50616], "temperature": 0.0, "avg_logprob": -0.08920396698845758, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0065628234297037125}, {"id": 287, "seek": 163712, "start": 1642.1599999999999, "end": 1646.56, "text": " the universe, which you can put a number on. But it has some synergies with global workspace theory,", "tokens": [50616, 264, 6445, 11, 597, 291, 393, 829, 257, 1230, 322, 13, 583, 309, 575, 512, 33781, 25480, 365, 4338, 32706, 5261, 11, 50836], "temperature": 0.0, "avg_logprob": -0.08920396698845758, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0065628234297037125}, {"id": 288, "seek": 163712, "start": 1646.56, "end": 1654.0, "text": " because they're both distinguishing between global holistic things versus local things. And the", "tokens": [50836, 570, 436, 434, 1293, 11365, 3807, 1296, 4338, 30334, 721, 5717, 2654, 721, 13, 400, 264, 51208], "temperature": 0.0, "avg_logprob": -0.08920396698845758, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0065628234297037125}, {"id": 289, "seek": 163712, "start": 1654.0, "end": 1660.6399999999999, "text": " and the consciousness is in the kind of global holistic processing versus the local, you know,", "tokens": [51208, 293, 264, 10081, 307, 294, 264, 733, 295, 4338, 30334, 9007, 5717, 264, 2654, 11, 291, 458, 11, 51540], "temperature": 0.0, "avg_logprob": -0.08920396698845758, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0065628234297037125}, {"id": 290, "seek": 163712, "start": 1660.6399999999999, "end": 1664.6399999999999, "text": " local processing in both those theories. So there's a kind of, you know, there's some", "tokens": [51540, 2654, 9007, 294, 1293, 729, 13667, 13, 407, 456, 311, 257, 733, 295, 11, 291, 458, 11, 456, 311, 512, 51740], "temperature": 0.0, "avg_logprob": -0.08920396698845758, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0065628234297037125}, {"id": 291, "seek": 166464, "start": 1664.64, "end": 1668.64, "text": " intuitions that they have in common, I think. Interesting. And it also reminds me a little", "tokens": [50364, 16224, 626, 300, 436, 362, 294, 2689, 11, 286, 519, 13, 14711, 13, 400, 309, 611, 12025, 385, 257, 707, 50564], "temperature": 0.0, "avg_logprob": -0.13232576240927485, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.004335544537752867}, {"id": 292, "seek": 166464, "start": 1668.64, "end": 1675.1200000000001, "text": " bit a little bit about what Chalmers speaks about. So he thinks that it strongly emerges from certain", "tokens": [50564, 857, 257, 707, 857, 466, 437, 761, 304, 18552, 10789, 466, 13, 407, 415, 7309, 300, 309, 10613, 38965, 490, 1629, 50888], "temperature": 0.0, "avg_logprob": -0.13232576240927485, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.004335544537752867}, {"id": 293, "seek": 166464, "start": 1675.1200000000001, "end": 1681.5200000000002, "text": " types of information processing. And the processing must represent causal structures as well. So it", "tokens": [50888, 3467, 295, 1589, 9007, 13, 400, 264, 9007, 1633, 2906, 38755, 9227, 382, 731, 13, 407, 309, 51208], "temperature": 0.0, "avg_logprob": -0.13232576240927485, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.004335544537752867}, {"id": 294, "seek": 166464, "start": 1681.5200000000002, "end": 1687.3600000000001, "text": " can't it's it's not an appeal to panpsychism per se. And although with with all of the things", "tokens": [51208, 393, 380, 309, 311, 309, 311, 406, 364, 13668, 281, 2462, 1878, 16384, 1434, 680, 369, 13, 400, 4878, 365, 365, 439, 295, 264, 721, 51500], "temperature": 0.0, "avg_logprob": -0.13232576240927485, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.004335544537752867}, {"id": 295, "seek": 166464, "start": 1687.3600000000001, "end": 1691.5200000000002, "text": " that you've just spoken about, what do they work in another universe? I mean, I guess what I'm", "tokens": [51500, 300, 291, 600, 445, 10759, 466, 11, 437, 360, 436, 589, 294, 1071, 6445, 30, 286, 914, 11, 286, 2041, 437, 286, 478, 51708], "temperature": 0.0, "avg_logprob": -0.13232576240927485, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.004335544537752867}, {"id": 296, "seek": 169152, "start": 1691.52, "end": 1697.52, "text": " saying is, is it just the the physical and the information processing or in a different universe", "tokens": [50364, 1566, 307, 11, 307, 309, 445, 264, 264, 4001, 293, 264, 1589, 9007, 420, 294, 257, 819, 6445, 50664], "temperature": 0.0, "avg_logprob": -0.17346976785098805, "compression_ratio": 1.8644067796610169, "no_speech_prob": 0.004399044904857874}, {"id": 297, "seek": 169152, "start": 1697.52, "end": 1702.0, "text": " might it not emerge in the same way? Yeah, which depends what you mean by a different universe,", "tokens": [50664, 1062, 309, 406, 21511, 294, 264, 912, 636, 30, 865, 11, 597, 5946, 437, 291, 914, 538, 257, 819, 6445, 11, 50888], "temperature": 0.0, "avg_logprob": -0.17346976785098805, "compression_ratio": 1.8644067796610169, "no_speech_prob": 0.004399044904857874}, {"id": 298, "seek": 169152, "start": 1702.0, "end": 1705.28, "text": " I guess. What do you mean by a different universe? Well, if the laws of nature were different.", "tokens": [50888, 286, 2041, 13, 708, 360, 291, 914, 538, 257, 819, 6445, 30, 1042, 11, 498, 264, 6064, 295, 3687, 645, 819, 13, 51052], "temperature": 0.0, "avg_logprob": -0.17346976785098805, "compression_ratio": 1.8644067796610169, "no_speech_prob": 0.004399044904857874}, {"id": 299, "seek": 169152, "start": 1705.84, "end": 1708.8, "text": " Yeah, okay. So if the laws of physics were different.", "tokens": [51080, 865, 11, 1392, 13, 407, 498, 264, 6064, 295, 10649, 645, 819, 13, 51228], "temperature": 0.0, "avg_logprob": -0.17346976785098805, "compression_ratio": 1.8644067796610169, "no_speech_prob": 0.004399044904857874}, {"id": 300, "seek": 169152, "start": 1710.56, "end": 1718.8, "text": " Well, I guess my I guess I dislike isms. I mean, I'm an anti ismist, or rather, I'd say I'm not an", "tokens": [51316, 1042, 11, 286, 2041, 452, 286, 2041, 286, 26006, 307, 2592, 13, 286, 914, 11, 286, 478, 364, 6061, 307, 76, 468, 11, 420, 2831, 11, 286, 1116, 584, 286, 478, 406, 364, 51728], "temperature": 0.0, "avg_logprob": -0.17346976785098805, "compression_ratio": 1.8644067796610169, "no_speech_prob": 0.004399044904857874}, {"id": 301, "seek": 171880, "start": 1718.8, "end": 1727.44, "text": " ismist. But if I were to but I do sort of subscribe broadly to functionalism, I suppose. So I guess", "tokens": [50364, 307, 76, 468, 13, 583, 498, 286, 645, 281, 457, 286, 360, 1333, 295, 3022, 19511, 281, 11745, 1434, 11, 286, 7297, 13, 407, 286, 2041, 50796], "temperature": 0.0, "avg_logprob": -0.1394339061918713, "compression_ratio": 1.7650602409638554, "no_speech_prob": 0.007697715424001217}, {"id": 302, "seek": 171880, "start": 1728.72, "end": 1738.1599999999999, "text": " I guess I what do I mean by that? I mean, what I mean is, I mean, I really dislike saying that I", "tokens": [50860, 286, 2041, 286, 437, 360, 286, 914, 538, 300, 30, 286, 914, 11, 437, 286, 914, 307, 11, 286, 914, 11, 286, 534, 26006, 1566, 300, 286, 51332], "temperature": 0.0, "avg_logprob": -0.1394339061918713, "compression_ratio": 1.7650602409638554, "no_speech_prob": 0.007697715424001217}, {"id": 303, "seek": 171880, "start": 1738.1599999999999, "end": 1747.52, "text": " subscribe to these to these isms. So what I really mean by that is that is that I imagine that a", "tokens": [51332, 3022, 281, 613, 281, 613, 307, 2592, 13, 407, 437, 286, 534, 914, 538, 300, 307, 300, 307, 300, 286, 3811, 300, 257, 51800], "temperature": 0.0, "avg_logprob": -0.1394339061918713, "compression_ratio": 1.7650602409638554, "no_speech_prob": 0.007697715424001217}, {"id": 304, "seek": 174752, "start": 1747.52, "end": 1752.56, "text": " system that is organized in a particular way functionally in terms of its information processing.", "tokens": [50364, 1185, 300, 307, 9983, 294, 257, 1729, 636, 2445, 379, 294, 2115, 295, 1080, 1589, 9007, 13, 50616], "temperature": 0.0, "avg_logprob": -0.09349159484213972, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002037188271060586}, {"id": 305, "seek": 174752, "start": 1752.56, "end": 1758.72, "text": " And if that system is in is embodied in the broadest sense, and, you know, and meets lots of", "tokens": [50616, 400, 498, 300, 1185, 307, 294, 307, 42046, 294, 264, 4152, 377, 2020, 11, 293, 11, 291, 458, 11, 293, 13961, 3195, 295, 50924], "temperature": 0.0, "avg_logprob": -0.09349159484213972, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002037188271060586}, {"id": 306, "seek": 174752, "start": 1758.72, "end": 1764.56, "text": " other prerequisites, then it's likely to behave in a way where I'm going to naturally use the word", "tokens": [50924, 661, 38333, 15398, 3324, 11, 550, 309, 311, 3700, 281, 15158, 294, 257, 636, 689, 286, 478, 516, 281, 8195, 764, 264, 1349, 51216], "temperature": 0.0, "avg_logprob": -0.09349159484213972, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002037188271060586}, {"id": 307, "seek": 174752, "start": 1764.56, "end": 1770.24, "text": " conscious to describe it, perhaps, and where I'm going to treat it like a fellow conscious creature.", "tokens": [51216, 6648, 281, 6786, 309, 11, 4317, 11, 293, 689, 286, 478, 516, 281, 2387, 309, 411, 257, 7177, 6648, 12797, 13, 51500], "temperature": 0.0, "avg_logprob": -0.09349159484213972, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002037188271060586}, {"id": 308, "seek": 177024, "start": 1770.8, "end": 1777.2, "text": " So, so, so it's so, you know, ultimately, it's I think it's about the kind of organization you need", "tokens": [50392, 407, 11, 370, 11, 370, 309, 311, 370, 11, 291, 458, 11, 6284, 11, 309, 311, 286, 519, 309, 311, 466, 264, 733, 295, 4475, 291, 643, 50712], "temperature": 0.0, "avg_logprob": -0.15824683099730402, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.025994595140218735}, {"id": 309, "seek": 177024, "start": 1777.76, "end": 1782.4, "text": " to give rise to the behavior you need to talk about thing, the thing in a certain way.", "tokens": [50740, 281, 976, 6272, 281, 264, 5223, 291, 643, 281, 751, 466, 551, 11, 264, 551, 294, 257, 1629, 636, 13, 50972], "temperature": 0.0, "avg_logprob": -0.15824683099730402, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.025994595140218735}, {"id": 310, "seek": 177024, "start": 1783.52, "end": 1787.1200000000001, "text": " My question today, because I posed this question to Chalmers last week, because he's also a", "tokens": [51028, 1222, 1168, 965, 11, 570, 286, 31399, 341, 1168, 281, 761, 304, 18552, 1036, 1243, 11, 570, 415, 311, 611, 257, 51208], "temperature": 0.0, "avg_logprob": -0.15824683099730402, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.025994595140218735}, {"id": 311, "seek": 177024, "start": 1787.1200000000001, "end": 1791.6, "text": " functionalist. And I agree with the degree of functionalism describing intelligence,", "tokens": [51208, 11745, 468, 13, 400, 286, 3986, 365, 264, 4314, 295, 11745, 1434, 16141, 7599, 11, 51432], "temperature": 0.0, "avg_logprob": -0.15824683099730402, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.025994595140218735}, {"id": 312, "seek": 177024, "start": 1791.6, "end": 1795.76, "text": " but less so with consciousness, you know, there's not a Turing test for consciousness, for example.", "tokens": [51432, 457, 1570, 370, 365, 10081, 11, 291, 458, 11, 456, 311, 406, 257, 314, 1345, 1500, 337, 10081, 11, 337, 1365, 13, 51640], "temperature": 0.0, "avg_logprob": -0.15824683099730402, "compression_ratio": 1.7671755725190839, "no_speech_prob": 0.025994595140218735}, {"id": 313, "seek": 179576, "start": 1796.24, "end": 1801.44, "text": " But the thing is with functionalism, we're at risk of doing what you said people do with", "tokens": [50388, 583, 264, 551, 307, 365, 11745, 1434, 11, 321, 434, 412, 3148, 295, 884, 437, 291, 848, 561, 360, 365, 50648], "temperature": 0.0, "avg_logprob": -0.17995730842032084, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.011350753717124462}, {"id": 314, "seek": 179576, "start": 1801.44, "end": 1805.12, "text": " large language models, which is anthropomorphizing them, because these functions are intelligible", "tokens": [50648, 2416, 2856, 5245, 11, 597, 307, 22727, 32702, 3319, 552, 11, 570, 613, 6828, 366, 5613, 964, 50832], "temperature": 0.0, "avg_logprob": -0.17995730842032084, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.011350753717124462}, {"id": 315, "seek": 179576, "start": 1805.12, "end": 1809.36, "text": " to us. And then our conception of intelligence becomes somewhat observer relative.", "tokens": [50832, 281, 505, 13, 400, 550, 527, 30698, 295, 7599, 3643, 8344, 27878, 4972, 13, 51044], "temperature": 0.0, "avg_logprob": -0.17995730842032084, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.011350753717124462}, {"id": 316, "seek": 179576, "start": 1813.76, "end": 1821.84, "text": " Yes, do I mean, what I observe a relative so you understand these functions, so it's conscious", "tokens": [51264, 1079, 11, 360, 286, 914, 11, 437, 286, 11441, 257, 4972, 370, 291, 1223, 613, 6828, 11, 370, 309, 311, 6648, 51668], "temperature": 0.0, "avg_logprob": -0.17995730842032084, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.011350753717124462}, {"id": 317, "seek": 182184, "start": 1821.84, "end": 1830.24, "text": " to you, but not to someone else? Well, so, so, so in all of these cases, I mean,", "tokens": [50364, 281, 291, 11, 457, 406, 281, 1580, 1646, 30, 1042, 11, 370, 11, 370, 11, 370, 294, 439, 295, 613, 3331, 11, 286, 914, 11, 50784], "temperature": 0.0, "avg_logprob": -0.15018843687497652, "compression_ratio": 1.75, "no_speech_prob": 0.009355295449495316}, {"id": 318, "seek": 182184, "start": 1830.24, "end": 1835.9199999999998, "text": " I think it's about the words that we use in our language to talk about the things. So, so,", "tokens": [50784, 286, 519, 309, 311, 466, 264, 2283, 300, 321, 764, 294, 527, 2856, 281, 751, 466, 264, 721, 13, 407, 11, 370, 11, 51068], "temperature": 0.0, "avg_logprob": -0.15018843687497652, "compression_ratio": 1.75, "no_speech_prob": 0.009355295449495316}, {"id": 319, "seek": 182184, "start": 1835.9199999999998, "end": 1841.4399999999998, "text": " so if there's someone else is someone just like us, right, then we have to and if we want to use", "tokens": [51068, 370, 498, 456, 311, 1580, 1646, 307, 1580, 445, 411, 505, 11, 558, 11, 550, 321, 362, 281, 293, 498, 321, 528, 281, 764, 51344], "temperature": 0.0, "avg_logprob": -0.15018843687497652, "compression_ratio": 1.75, "no_speech_prob": 0.009355295449495316}, {"id": 320, "seek": 182184, "start": 1841.4399999999998, "end": 1846.1599999999999, "text": " the words in different ways. So, so the large language models are a great case in point, right.", "tokens": [51344, 264, 2283, 294, 819, 2098, 13, 407, 11, 370, 264, 2416, 2856, 5245, 366, 257, 869, 1389, 294, 935, 11, 558, 13, 51580], "temperature": 0.0, "avg_logprob": -0.15018843687497652, "compression_ratio": 1.75, "no_speech_prob": 0.009355295449495316}, {"id": 321, "seek": 184616, "start": 1846.5600000000002, "end": 1853.28, "text": " So, so suddenly we're arriving at a point where somebody can describe something as conscious.", "tokens": [50384, 407, 11, 370, 5800, 321, 434, 22436, 412, 257, 935, 689, 2618, 393, 6786, 746, 382, 6648, 13, 50720], "temperature": 0.0, "avg_logprob": -0.15692403761007018, "compression_ratio": 1.7874015748031495, "no_speech_prob": 0.0152664789929986}, {"id": 322, "seek": 184616, "start": 1854.24, "end": 1858.48, "text": " And others can say that's rubbish, you know, it's not that's not true at all. And so we,", "tokens": [50768, 400, 2357, 393, 584, 300, 311, 29978, 11, 291, 458, 11, 309, 311, 406, 300, 311, 406, 2074, 412, 439, 13, 400, 370, 321, 11, 50980], "temperature": 0.0, "avg_logprob": -0.15692403761007018, "compression_ratio": 1.7874015748031495, "no_speech_prob": 0.0152664789929986}, {"id": 323, "seek": 184616, "start": 1858.48, "end": 1863.28, "text": " so we've, we've arrived at a point where these philosophically problematic words, which,", "tokens": [50980, 370, 321, 600, 11, 321, 600, 6678, 412, 257, 935, 689, 613, 14529, 984, 19011, 2283, 11, 597, 11, 51220], "temperature": 0.0, "avg_logprob": -0.15692403761007018, "compression_ratio": 1.7874015748031495, "no_speech_prob": 0.0152664789929986}, {"id": 324, "seek": 184616, "start": 1864.24, "end": 1870.4, "text": " which we use in ordinary life quite, quite harmlessly. And we all, you know, we all are in", "tokens": [51268, 597, 321, 764, 294, 10547, 993, 1596, 11, 1596, 6491, 12048, 13, 400, 321, 439, 11, 291, 458, 11, 321, 439, 366, 294, 51576], "temperature": 0.0, "avg_logprob": -0.15692403761007018, "compression_ratio": 1.7874015748031495, "no_speech_prob": 0.0152664789929986}, {"id": 325, "seek": 184616, "start": 1870.4, "end": 1874.96, "text": " agreement about how we use the word likes if somebody says, oh, you know, Fred has drank so", "tokens": [51576, 8106, 466, 577, 321, 764, 264, 1349, 5902, 498, 2618, 1619, 11, 1954, 11, 291, 458, 11, 10112, 575, 21011, 370, 51804], "temperature": 0.0, "avg_logprob": -0.15692403761007018, "compression_ratio": 1.7874015748031495, "no_speech_prob": 0.0152664789929986}, {"id": 326, "seek": 187496, "start": 1874.96, "end": 1878.96, "text": " much last night, he passed out, he was completely unconscious, you know, I mean, and, or if an", "tokens": [50364, 709, 1036, 1818, 11, 415, 4678, 484, 11, 415, 390, 2584, 18900, 11, 291, 458, 11, 286, 914, 11, 293, 11, 420, 498, 364, 50564], "temperature": 0.0, "avg_logprob": -0.16812214630329056, "compression_ratio": 1.8984375, "no_speech_prob": 0.016850460320711136}, {"id": 327, "seek": 187496, "start": 1878.96, "end": 1883.52, "text": " anesthetist says, yes, they, you know, the patient is now unconscious, they can't feel, feel pain.", "tokens": [50564, 31750, 9092, 468, 1619, 11, 2086, 11, 436, 11, 291, 458, 11, 264, 4537, 307, 586, 18900, 11, 436, 393, 380, 841, 11, 841, 1822, 13, 50792], "temperature": 0.0, "avg_logprob": -0.16812214630329056, "compression_ratio": 1.8984375, "no_speech_prob": 0.016850460320711136}, {"id": 328, "seek": 187496, "start": 1884.88, "end": 1891.76, "text": " Or if you say, oh, you know, I, I just wasn't aware, I didn't see the, the cyclist and you know,", "tokens": [50860, 1610, 498, 291, 584, 11, 1954, 11, 291, 458, 11, 286, 11, 286, 445, 2067, 380, 3650, 11, 286, 994, 380, 536, 264, 11, 264, 19474, 468, 293, 291, 458, 11, 51204], "temperature": 0.0, "avg_logprob": -0.16812214630329056, "compression_ratio": 1.8984375, "no_speech_prob": 0.016850460320711136}, {"id": 329, "seek": 187496, "start": 1891.76, "end": 1896.96, "text": " that's why I, I hit them, you know, I'm really, it's tragic, but I just didn't see them. And then,", "tokens": [51204, 300, 311, 983, 286, 11, 286, 2045, 552, 11, 291, 458, 11, 286, 478, 534, 11, 309, 311, 20385, 11, 457, 286, 445, 994, 380, 536, 552, 13, 400, 550, 11, 51464], "temperature": 0.0, "avg_logprob": -0.16812214630329056, "compression_ratio": 1.8984375, "no_speech_prob": 0.016850460320711136}, {"id": 330, "seek": 187496, "start": 1897.76, "end": 1902.88, "text": " and we, so, you know, so you're saying I wasn't aware of it. So that didn't influence my action.", "tokens": [51504, 293, 321, 11, 370, 11, 291, 458, 11, 370, 291, 434, 1566, 286, 2067, 380, 3650, 295, 309, 13, 407, 300, 994, 380, 6503, 452, 3069, 13, 51760], "temperature": 0.0, "avg_logprob": -0.16812214630329056, "compression_ratio": 1.8984375, "no_speech_prob": 0.016850460320711136}, {"id": 331, "seek": 190288, "start": 1902.88, "end": 1907.3600000000001, "text": " So there we're using the terms in ways that we all understand. But now we're getting to a point", "tokens": [50364, 407, 456, 321, 434, 1228, 264, 2115, 294, 2098, 300, 321, 439, 1223, 13, 583, 586, 321, 434, 1242, 281, 257, 935, 50588], "temperature": 0.0, "avg_logprob": -0.11285047978162766, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.002405451610684395}, {"id": 332, "seek": 190288, "start": 1907.3600000000001, "end": 1912.64, "text": " where suddenly, these words or these concepts are being used, you know, we don't have an way,", "tokens": [50588, 689, 5800, 11, 613, 2283, 420, 613, 10392, 366, 885, 1143, 11, 291, 458, 11, 321, 500, 380, 362, 364, 636, 11, 50852], "temperature": 0.0, "avg_logprob": -0.11285047978162766, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.002405451610684395}, {"id": 333, "seek": 190288, "start": 1912.64, "end": 1917.44, "text": " we don't have agreement about how to use these words, right? Because it's, there are these exotic", "tokens": [50852, 321, 500, 380, 362, 8106, 466, 577, 281, 764, 613, 2283, 11, 558, 30, 1436, 309, 311, 11, 456, 366, 613, 27063, 51092], "temperature": 0.0, "avg_logprob": -0.11285047978162766, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.002405451610684395}, {"id": 334, "seek": 190288, "start": 1917.44, "end": 1922.4, "text": " edge cases. Yes. So then the question, I think that you, you're getting is, you know, is there", "tokens": [51092, 4691, 3331, 13, 1079, 13, 407, 550, 264, 1168, 11, 286, 519, 300, 291, 11, 291, 434, 1242, 307, 11, 291, 458, 11, 307, 456, 51340], "temperature": 0.0, "avg_logprob": -0.11285047978162766, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.002405451610684395}, {"id": 335, "seek": 190288, "start": 1922.4, "end": 1931.8400000000001, "text": " a fact of the matter there, right? And so I'm very tempted to say the first thing I'm tempted to say", "tokens": [51340, 257, 1186, 295, 264, 1871, 456, 11, 558, 30, 400, 370, 286, 478, 588, 29941, 281, 584, 264, 700, 551, 286, 478, 29941, 281, 584, 51812], "temperature": 0.0, "avg_logprob": -0.11285047978162766, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.002405451610684395}, {"id": 336, "seek": 193184, "start": 1931.84, "end": 1936.48, "text": " is that I don't think that perhaps is a fact of the matter. Or certainly, I don't, I don't want to,", "tokens": [50364, 307, 300, 286, 500, 380, 519, 300, 4317, 307, 257, 1186, 295, 264, 1871, 13, 1610, 3297, 11, 286, 500, 380, 11, 286, 500, 380, 528, 281, 11, 50596], "temperature": 0.0, "avg_logprob": -0.0886790672938029, "compression_ratio": 1.8671875, "no_speech_prob": 0.005712659098207951}, {"id": 337, "seek": 193184, "start": 1937.12, "end": 1943.36, "text": " I don't want to speak as if there is a fact of the matter, but rather, I think we need to arrive", "tokens": [50628, 286, 500, 380, 528, 281, 1710, 382, 498, 456, 307, 257, 1186, 295, 264, 1871, 11, 457, 2831, 11, 286, 519, 321, 643, 281, 8881, 50940], "temperature": 0.0, "avg_logprob": -0.0886790672938029, "compression_ratio": 1.8671875, "no_speech_prob": 0.005712659098207951}, {"id": 338, "seek": 193184, "start": 1943.36, "end": 1947.6, "text": " at a new consensus about how we use these words. So that might mean that we extend the words,", "tokens": [50940, 412, 257, 777, 19115, 466, 577, 321, 764, 613, 2283, 13, 407, 300, 1062, 914, 300, 321, 10101, 264, 2283, 11, 51152], "temperature": 0.0, "avg_logprob": -0.0886790672938029, "compression_ratio": 1.8671875, "no_speech_prob": 0.005712659098207951}, {"id": 339, "seek": 193184, "start": 1947.6, "end": 1952.9599999999998, "text": " we break them apart, like I was suggesting earlier, maybe we need to separate out awareness of the", "tokens": [51152, 321, 1821, 552, 4936, 11, 411, 286, 390, 18094, 3071, 11, 1310, 321, 643, 281, 4994, 484, 8888, 295, 264, 51420], "temperature": 0.0, "avg_logprob": -0.0886790672938029, "compression_ratio": 1.8671875, "no_speech_prob": 0.005712659098207951}, {"id": 340, "seek": 193184, "start": 1952.9599999999998, "end": 1958.56, "text": " world from self awareness, from integration, cognitive integration from the capacity for", "tokens": [51420, 1002, 490, 2698, 8888, 11, 490, 10980, 11, 15605, 10980, 490, 264, 6042, 337, 51700], "temperature": 0.0, "avg_logprob": -0.0886790672938029, "compression_ratio": 1.8671875, "no_speech_prob": 0.005712659098207951}, {"id": 341, "seek": 195856, "start": 1958.56, "end": 1963.28, "text": " suffering, because suddenly we have things that where that they don't all come as a package. And", "tokens": [50364, 7755, 11, 570, 5800, 321, 362, 721, 300, 689, 300, 436, 500, 380, 439, 808, 382, 257, 7372, 13, 400, 50600], "temperature": 0.0, "avg_logprob": -0.0979908453790765, "compression_ratio": 1.910299003322259, "no_speech_prob": 0.012080729007720947}, {"id": 342, "seek": 195856, "start": 1963.9199999999998, "end": 1967.6, "text": " when we need to kind of be a bit more nuanced in the way that we use these words, we need to use", "tokens": [50632, 562, 321, 643, 281, 733, 295, 312, 257, 857, 544, 45115, 294, 264, 636, 300, 321, 764, 613, 2283, 11, 321, 643, 281, 764, 50816], "temperature": 0.0, "avg_logprob": -0.0979908453790765, "compression_ratio": 1.910299003322259, "no_speech_prob": 0.012080729007720947}, {"id": 343, "seek": 195856, "start": 1967.6, "end": 1972.3999999999999, "text": " them in new ways. But then there's a kind of transition period, because we don't, you know,", "tokens": [50816, 552, 294, 777, 2098, 13, 583, 550, 456, 311, 257, 733, 295, 6034, 2896, 11, 570, 321, 500, 380, 11, 291, 458, 11, 51056], "temperature": 0.0, "avg_logprob": -0.0979908453790765, "compression_ratio": 1.910299003322259, "no_speech_prob": 0.012080729007720947}, {"id": 344, "seek": 195856, "start": 1972.3999999999999, "end": 1976.6399999999999, "text": " we're all arguing about how to use these words all of a sudden, because we've got weird edge cases.", "tokens": [51056, 321, 434, 439, 19697, 466, 577, 281, 764, 613, 2283, 439, 295, 257, 3990, 11, 570, 321, 600, 658, 3657, 4691, 3331, 13, 51268], "temperature": 0.0, "avg_logprob": -0.0979908453790765, "compression_ratio": 1.910299003322259, "no_speech_prob": 0.012080729007720947}, {"id": 345, "seek": 195856, "start": 1976.6399999999999, "end": 1981.28, "text": " So there's going to be a time when it'll take a time for language to settle back down again.", "tokens": [51268, 407, 456, 311, 516, 281, 312, 257, 565, 562, 309, 603, 747, 257, 565, 337, 2856, 281, 11852, 646, 760, 797, 13, 51500], "temperature": 0.0, "avg_logprob": -0.0979908453790765, "compression_ratio": 1.910299003322259, "no_speech_prob": 0.012080729007720947}, {"id": 346, "seek": 195856, "start": 1981.28, "end": 1987.28, "text": " So there's a kind of, you know, there's a kind of observer relativness to this for a bit, if you", "tokens": [51500, 407, 456, 311, 257, 733, 295, 11, 291, 458, 11, 456, 311, 257, 733, 295, 27878, 21960, 1287, 281, 341, 337, 257, 857, 11, 498, 291, 51800], "temperature": 0.0, "avg_logprob": -0.0979908453790765, "compression_ratio": 1.910299003322259, "no_speech_prob": 0.012080729007720947}, {"id": 347, "seek": 198728, "start": 1987.28, "end": 1994.8, "text": " like, but then, but then there's a kind of consensus needs to emerge, right? But so many", "tokens": [50364, 411, 11, 457, 550, 11, 457, 550, 456, 311, 257, 733, 295, 19115, 2203, 281, 21511, 11, 558, 30, 583, 370, 867, 50740], "temperature": 0.0, "avg_logprob": -0.15331959123371028, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.011208117008209229}, {"id": 348, "seek": 198728, "start": 1994.8, "end": 2001.12, "text": " things to explore there. I mean, I'm, I would love it if this platonic idea of concepts were possible.", "tokens": [50740, 721, 281, 6839, 456, 13, 286, 914, 11, 286, 478, 11, 286, 576, 959, 309, 498, 341, 3403, 11630, 1558, 295, 10392, 645, 1944, 13, 51056], "temperature": 0.0, "avg_logprob": -0.15331959123371028, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.011208117008209229}, {"id": 349, "seek": 198728, "start": 2001.68, "end": 2007.68, "text": " And what platonic? Because what we're talking about here is reductionism and the, I mean,", "tokens": [51084, 400, 437, 3403, 11630, 30, 1436, 437, 321, 434, 1417, 466, 510, 307, 11004, 1434, 293, 264, 11, 286, 914, 11, 51384], "temperature": 0.0, "avg_logprob": -0.15331959123371028, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.011208117008209229}, {"id": 350, "seek": 198728, "start": 2007.68, "end": 2012.0, "text": " the parable of the blind man and the elephant comes in quite nicely. So as Chomsky said,", "tokens": [51384, 264, 971, 712, 295, 264, 6865, 587, 293, 264, 19791, 1487, 294, 1596, 9594, 13, 407, 382, 761, 4785, 4133, 848, 11, 51600], "temperature": 0.0, "avg_logprob": -0.15331959123371028, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.011208117008209229}, {"id": 351, "seek": 198728, "start": 2012.0, "end": 2016.3999999999999, "text": " complex phenomenon beyond our cognitive horizon. And as much as we don't want to,", "tokens": [51600, 3997, 14029, 4399, 527, 15605, 18046, 13, 400, 382, 709, 382, 321, 500, 380, 528, 281, 11, 51820], "temperature": 0.0, "avg_logprob": -0.15331959123371028, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.011208117008209229}, {"id": 352, "seek": 201640, "start": 2016.4, "end": 2020.72, "text": " we use functions derived from behavior to have some common understanding of this thing.", "tokens": [50364, 321, 764, 6828, 18949, 490, 5223, 281, 362, 512, 2689, 3701, 295, 341, 551, 13, 50580], "temperature": 0.0, "avg_logprob": -0.1527104377746582, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.003866587532684207}, {"id": 353, "seek": 201640, "start": 2020.72, "end": 2024.5600000000002, "text": " But I wasn't being reductionist, was I? Do you think I was being reductionist?", "tokens": [50580, 583, 286, 2067, 380, 885, 11004, 468, 11, 390, 286, 30, 1144, 291, 519, 286, 390, 885, 11004, 468, 30, 50772], "temperature": 0.0, "avg_logprob": -0.1527104377746582, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.003866587532684207}, {"id": 354, "seek": 201640, "start": 2024.5600000000002, "end": 2031.1200000000001, "text": " Well, no. So you said that the language game converges. And in some cases, we will arrive on", "tokens": [50772, 1042, 11, 572, 13, 407, 291, 848, 300, 264, 2856, 1216, 9652, 2880, 13, 400, 294, 512, 3331, 11, 321, 486, 8881, 322, 51100], "temperature": 0.0, "avg_logprob": -0.1527104377746582, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.003866587532684207}, {"id": 355, "seek": 201640, "start": 2031.1200000000001, "end": 2035.3600000000001, "text": " a common definition, but like you can bring in Hofstatter as well. Well, not a common definition,", "tokens": [51100, 257, 2689, 7123, 11, 457, 411, 291, 393, 1565, 294, 37379, 372, 1161, 382, 731, 13, 1042, 11, 406, 257, 2689, 7123, 11, 51312], "temperature": 0.0, "avg_logprob": -0.1527104377746582, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.003866587532684207}, {"id": 356, "seek": 201640, "start": 2035.3600000000001, "end": 2041.68, "text": " but a common usage, right? So we'll come, so we'll come to use the words, you know, with agreement,", "tokens": [51312, 457, 257, 2689, 14924, 11, 558, 30, 407, 321, 603, 808, 11, 370, 321, 603, 808, 281, 764, 264, 2283, 11, 291, 458, 11, 365, 8106, 11, 51628], "temperature": 0.0, "avg_logprob": -0.1527104377746582, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.003866587532684207}, {"id": 357, "seek": 204168, "start": 2041.68, "end": 2046.88, "text": " right? So that's what I, and the reason why I mean, I would, and the reason I would balk at", "tokens": [50364, 558, 30, 407, 300, 311, 437, 286, 11, 293, 264, 1778, 983, 286, 914, 11, 286, 576, 11, 293, 264, 1778, 286, 576, 272, 667, 412, 50624], "temperature": 0.0, "avg_logprob": -0.12769595130545194, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.025516996160149574}, {"id": 358, "seek": 204168, "start": 2046.88, "end": 2051.92, "text": " using the word reductionist is because, and that's why I'm a bit resistant to functionalism as well", "tokens": [50624, 1228, 264, 1349, 11004, 468, 307, 570, 11, 293, 300, 311, 983, 286, 478, 257, 857, 20383, 281, 11745, 1434, 382, 731, 50876], "temperature": 0.0, "avg_logprob": -0.12769595130545194, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.025516996160149574}, {"id": 359, "seek": 204168, "start": 2051.92, "end": 2058.64, "text": " to any kind of ism is because I just think that that may be the way things are organized when", "tokens": [50876, 281, 604, 733, 295, 307, 76, 307, 570, 286, 445, 519, 300, 300, 815, 312, 264, 636, 721, 366, 9983, 562, 51212], "temperature": 0.0, "avg_logprob": -0.12769595130545194, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.025516996160149574}, {"id": 360, "seek": 204168, "start": 2058.64, "end": 2064.64, "text": " you take them apart. So, you know, brains, right, when you examine them on the inside,", "tokens": [51212, 291, 747, 552, 4936, 13, 407, 11, 291, 458, 11, 15442, 11, 558, 11, 562, 291, 17496, 552, 322, 264, 1854, 11, 51512], "temperature": 0.0, "avg_logprob": -0.12769595130545194, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.025516996160149574}, {"id": 361, "seek": 204168, "start": 2064.64, "end": 2070.8, "text": " like animal brains, you might look at how an octopus's brain works. And that might inform", "tokens": [51512, 411, 5496, 15442, 11, 291, 1062, 574, 412, 577, 364, 27962, 311, 3567, 1985, 13, 400, 300, 1062, 1356, 51820], "temperature": 0.0, "avg_logprob": -0.12769595130545194, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.025516996160149574}, {"id": 362, "seek": 207080, "start": 2070.8, "end": 2075.28, "text": " whether you think that it suffers, can experience this pain or not. Or we might break apart, you", "tokens": [50364, 1968, 291, 519, 300, 309, 33776, 11, 393, 1752, 341, 1822, 420, 406, 13, 1610, 321, 1062, 1821, 4936, 11, 291, 50588], "temperature": 0.0, "avg_logprob": -0.12666996901597433, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.010018974542617798}, {"id": 363, "seek": 207080, "start": 2075.28, "end": 2079.92, "text": " know, an AI of the system of the future, right? You know, and we might break it apart and we may", "tokens": [50588, 458, 11, 364, 7318, 295, 264, 1185, 295, 264, 2027, 11, 558, 30, 509, 458, 11, 293, 321, 1062, 1821, 309, 4936, 293, 321, 815, 50820], "temperature": 0.0, "avg_logprob": -0.12666996901597433, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.010018974542617798}, {"id": 364, "seek": 207080, "start": 2079.92, "end": 2085.44, "text": " look at its functional organization. And that all is just is grist to the mill of how our language", "tokens": [50820, 574, 412, 1080, 11745, 4475, 13, 400, 300, 439, 307, 445, 307, 677, 468, 281, 264, 1728, 295, 577, 527, 2856, 51096], "temperature": 0.0, "avg_logprob": -0.12666996901597433, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.010018974542617798}, {"id": 365, "seek": 207080, "start": 2085.44, "end": 2091.36, "text": " might change, right? So I'm not, I'm not subscribing to the fact that consciousness is this or this", "tokens": [51096, 1062, 1319, 11, 558, 30, 407, 286, 478, 406, 11, 286, 478, 406, 19981, 281, 264, 1186, 300, 10081, 307, 341, 420, 341, 51392], "temperature": 0.0, "avg_logprob": -0.12666996901597433, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.010018974542617798}, {"id": 366, "seek": 207080, "start": 2091.36, "end": 2097.6000000000004, "text": " is that it with some big metaphysical capital letters on the is, right? That's really important.", "tokens": [51392, 307, 300, 309, 365, 512, 955, 30946, 36280, 4238, 7825, 322, 264, 307, 11, 558, 30, 663, 311, 534, 1021, 13, 51704], "temperature": 0.0, "avg_logprob": -0.12666996901597433, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.010018974542617798}, {"id": 367, "seek": 209760, "start": 2098.3199999999997, "end": 2103.68, "text": " So the functional organization of these other things, which when we study and look at it,", "tokens": [50400, 407, 264, 11745, 4475, 295, 613, 661, 721, 11, 597, 562, 321, 2979, 293, 574, 412, 309, 11, 50668], "temperature": 0.0, "avg_logprob": -0.11028100778390695, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.004565161187201738}, {"id": 368, "seek": 209760, "start": 2103.68, "end": 2109.12, "text": " is all just part becomes part of a conversation that eventually is going to help us to settle on", "tokens": [50668, 307, 439, 445, 644, 3643, 644, 295, 257, 3761, 300, 4728, 307, 516, 281, 854, 505, 281, 11852, 322, 50940], "temperature": 0.0, "avg_logprob": -0.11028100778390695, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.004565161187201738}, {"id": 369, "seek": 209760, "start": 2109.12, "end": 2114.56, "text": " maybe new ways of talking about these things. I think we agree with each other. I think the", "tokens": [50940, 1310, 777, 2098, 295, 1417, 466, 613, 721, 13, 286, 519, 321, 3986, 365, 1184, 661, 13, 286, 519, 264, 51212], "temperature": 0.0, "avg_logprob": -0.11028100778390695, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.004565161187201738}, {"id": 370, "seek": 209760, "start": 2114.56, "end": 2118.88, "text": " difference is, so with the parable of the blind men and the elephant, all of the men around the", "tokens": [51212, 2649, 307, 11, 370, 365, 264, 971, 712, 295, 264, 6865, 1706, 293, 264, 19791, 11, 439, 295, 264, 1706, 926, 264, 51428], "temperature": 0.0, "avg_logprob": -0.11028100778390695, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.004565161187201738}, {"id": 371, "seek": 209760, "start": 2118.88, "end": 2124.48, "text": " elephant saw something which was part of the truth. And I think that's what we're describing with", "tokens": [51428, 19791, 1866, 746, 597, 390, 644, 295, 264, 3494, 13, 400, 286, 519, 300, 311, 437, 321, 434, 16141, 365, 51708], "temperature": 0.0, "avg_logprob": -0.11028100778390695, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.004565161187201738}, {"id": 372, "seek": 212448, "start": 2124.48, "end": 2131.44, "text": " the function. So we can all agree on what perception means or what action means. The thing is,", "tokens": [50364, 264, 2445, 13, 407, 321, 393, 439, 3986, 322, 437, 12860, 1355, 420, 437, 3069, 1355, 13, 440, 551, 307, 11, 50712], "temperature": 0.0, "avg_logprob": -0.1660553993435081, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.048754703253507614}, {"id": 373, "seek": 212448, "start": 2131.44, "end": 2135.44, "text": " there will be many other functions that will represent a different slice of that cognitive", "tokens": [50712, 456, 486, 312, 867, 661, 6828, 300, 486, 2906, 257, 819, 13153, 295, 300, 15605, 50912], "temperature": 0.0, "avg_logprob": -0.1660553993435081, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.048754703253507614}, {"id": 374, "seek": 212448, "start": 2135.44, "end": 2139.52, "text": " phenomenon. Yeah, I agree. And I think that's very much true with consciousness, actually, because", "tokens": [50912, 14029, 13, 865, 11, 286, 3986, 13, 400, 286, 519, 300, 311, 588, 709, 2074, 365, 10081, 11, 767, 11, 570, 51116], "temperature": 0.0, "avg_logprob": -0.1660553993435081, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.048754703253507614}, {"id": 375, "seek": 212448, "start": 2139.52, "end": 2143.2, "text": " there's lots of people coming with kind of like new ideas and new theories. I mean,", "tokens": [51116, 456, 311, 3195, 295, 561, 1348, 365, 733, 295, 411, 777, 3487, 293, 777, 13667, 13, 286, 914, 11, 51300], "temperature": 0.0, "avg_logprob": -0.1660553993435081, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.048754703253507614}, {"id": 376, "seek": 212448, "start": 2144.4, "end": 2148.32, "text": " Anil Seth, for example, have you had Anil on your on your not yet being right?", "tokens": [51360, 1107, 388, 25353, 11, 337, 1365, 11, 362, 291, 632, 1107, 388, 322, 428, 322, 428, 406, 1939, 885, 558, 30, 51556], "temperature": 0.0, "avg_logprob": -0.1660553993435081, "compression_ratio": 1.6373626373626373, "no_speech_prob": 0.048754703253507614}, {"id": 377, "seek": 214832, "start": 2149.2000000000003, "end": 2154.96, "text": " So Anil's written this great book called Being You. And Anil brings in a whole kind of new set", "tokens": [50408, 407, 1107, 388, 311, 3720, 341, 869, 1446, 1219, 8891, 509, 13, 400, 1107, 388, 5607, 294, 257, 1379, 733, 295, 777, 992, 50696], "temperature": 0.0, "avg_logprob": -0.13335008621215821, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.005642845295369625}, {"id": 378, "seek": 214832, "start": 2154.96, "end": 2162.0800000000004, "text": " of ideas. He's particularly interested in the sort of top down effects on perception, top", "tokens": [50696, 295, 3487, 13, 634, 311, 4098, 3102, 294, 264, 1333, 295, 1192, 760, 5065, 322, 12860, 11, 1192, 51052], "temperature": 0.0, "avg_logprob": -0.13335008621215821, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.005642845295369625}, {"id": 379, "seek": 214832, "start": 2162.0800000000004, "end": 2166.1600000000003, "text": " down effects on perception. So then he brings in this kind of top down influence and perception", "tokens": [51052, 760, 5065, 322, 12860, 13, 407, 550, 415, 5607, 294, 341, 733, 295, 1192, 760, 6503, 293, 12860, 51256], "temperature": 0.0, "avg_logprob": -0.13335008621215821, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.005642845295369625}, {"id": 380, "seek": 214832, "start": 2166.1600000000003, "end": 2171.52, "text": " as a big part of things. And then there's Graziano has written this book using this about", "tokens": [51256, 382, 257, 955, 644, 295, 721, 13, 400, 550, 456, 311, 8985, 89, 6254, 575, 3720, 341, 1446, 1228, 341, 466, 51524], "temperature": 0.0, "avg_logprob": -0.13335008621215821, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.005642845295369625}, {"id": 381, "seek": 217152, "start": 2171.6, "end": 2178.8, "text": " his attention schema theory of consciousness. And that's, and, you know, there's a whole set", "tokens": [50368, 702, 3202, 34078, 5261, 295, 10081, 13, 400, 300, 311, 11, 293, 11, 291, 458, 11, 456, 311, 257, 1379, 992, 50728], "temperature": 0.0, "avg_logprob": -0.11183043607731455, "compression_ratio": 1.898989898989899, "no_speech_prob": 0.01574920304119587}, {"id": 382, "seek": 217152, "start": 2178.8, "end": 2182.64, "text": " of interesting ideas there. And I think you're absolutely right. I think there's, I think there's", "tokens": [50728, 295, 1880, 3487, 456, 13, 400, 286, 519, 291, 434, 3122, 558, 13, 286, 519, 456, 311, 11, 286, 519, 456, 311, 50920], "temperature": 0.0, "avg_logprob": -0.11183043607731455, "compression_ratio": 1.898989898989899, "no_speech_prob": 0.01574920304119587}, {"id": 383, "seek": 217152, "start": 2182.64, "end": 2191.12, "text": " aspects of all of these things all feed into, you know, all feed into the way, you know,", "tokens": [50920, 7270, 295, 439, 295, 613, 721, 439, 3154, 666, 11, 291, 458, 11, 439, 3154, 666, 264, 636, 11, 291, 458, 11, 51344], "temperature": 0.0, "avg_logprob": -0.11183043607731455, "compression_ratio": 1.898989898989899, "no_speech_prob": 0.01574920304119587}, {"id": 384, "seek": 217152, "start": 2192.32, "end": 2200.08, "text": " brains and animals work and all of them feed into the, you know, why they behave the way they do", "tokens": [51404, 15442, 293, 4882, 589, 293, 439, 295, 552, 3154, 666, 264, 11, 291, 458, 11, 983, 436, 15158, 264, 636, 436, 360, 51792], "temperature": 0.0, "avg_logprob": -0.11183043607731455, "compression_ratio": 1.898989898989899, "no_speech_prob": 0.01574920304119587}, {"id": 385, "seek": 220008, "start": 2200.08, "end": 2204.56, "text": " and why we use those words when we use those words, you know, conscious and aware and so", "tokens": [50364, 293, 983, 321, 764, 729, 2283, 562, 321, 764, 729, 2283, 11, 291, 458, 11, 6648, 293, 3650, 293, 370, 50588], "temperature": 0.0, "avg_logprob": -0.157079794111043, "compression_ratio": 1.7057057057057057, "no_speech_prob": 0.005648349411785603}, {"id": 386, "seek": 220008, "start": 2204.56, "end": 2210.56, "text": " fascinating. We'll get to your article in a second. But as someone who has such a diverse", "tokens": [50588, 10343, 13, 492, 603, 483, 281, 428, 7222, 294, 257, 1150, 13, 583, 382, 1580, 567, 575, 1270, 257, 9521, 50888], "temperature": 0.0, "avg_logprob": -0.157079794111043, "compression_ratio": 1.7057057057057057, "no_speech_prob": 0.005648349411785603}, {"id": 387, "seek": 220008, "start": 2210.56, "end": 2215.12, "text": " and interesting background, who is allowed to ask these philosophical questions? So it reminds me", "tokens": [50888, 293, 1880, 3678, 11, 567, 307, 4350, 281, 1029, 613, 25066, 1651, 30, 407, 309, 12025, 385, 51116], "temperature": 0.0, "avg_logprob": -0.157079794111043, "compression_ratio": 1.7057057057057057, "no_speech_prob": 0.005648349411785603}, {"id": 388, "seek": 220008, "start": 2215.12, "end": 2220.24, "text": " and Thomas Meckens who is talking about the arguments between neuroscientists and philosophers", "tokens": [51116, 293, 8500, 1923, 547, 694, 567, 307, 1417, 466, 264, 12869, 1296, 28813, 5412, 1751, 293, 36839, 51372], "temperature": 0.0, "avg_logprob": -0.157079794111043, "compression_ratio": 1.7057057057057057, "no_speech_prob": 0.005648349411785603}, {"id": 389, "seek": 220008, "start": 2220.24, "end": 2225.6, "text": " about freedom of the will. Yeah. And who gets to decide? Huh. Yeah. Well, what a great question.", "tokens": [51372, 466, 5645, 295, 264, 486, 13, 865, 13, 400, 567, 2170, 281, 4536, 30, 8063, 13, 865, 13, 1042, 11, 437, 257, 869, 1168, 13, 51640], "temperature": 0.0, "avg_logprob": -0.157079794111043, "compression_ratio": 1.7057057057057057, "no_speech_prob": 0.005648349411785603}, {"id": 390, "seek": 220008, "start": 2225.6, "end": 2229.36, "text": " You know, I mean, so why should I have any right to speak about any of these things at all? Because", "tokens": [51640, 509, 458, 11, 286, 914, 11, 370, 983, 820, 286, 362, 604, 558, 281, 1710, 466, 604, 295, 613, 721, 412, 439, 30, 1436, 51828], "temperature": 0.0, "avg_logprob": -0.157079794111043, "compression_ratio": 1.7057057057057057, "no_speech_prob": 0.005648349411785603}, {"id": 391, "seek": 222936, "start": 2229.36, "end": 2238.0, "text": " I have no formal training in philosophy. So, so, so who gets to, who gets to dis, well, who gets to,", "tokens": [50364, 286, 362, 572, 9860, 3097, 294, 10675, 13, 407, 11, 370, 11, 370, 567, 2170, 281, 11, 567, 2170, 281, 717, 11, 731, 11, 567, 2170, 281, 11, 50796], "temperature": 0.0, "avg_logprob": -0.19248255472334605, "compression_ratio": 1.8365758754863812, "no_speech_prob": 0.0008233787375502288}, {"id": 392, "seek": 222936, "start": 2238.0, "end": 2241.6, "text": " to, I guess there are two things, right? There's, I guess, I guess there's, there's", "tokens": [50796, 281, 11, 286, 2041, 456, 366, 732, 721, 11, 558, 30, 821, 311, 11, 286, 2041, 11, 286, 2041, 456, 311, 11, 456, 311, 50976], "temperature": 0.0, "avg_logprob": -0.19248255472334605, "compression_ratio": 1.8365758754863812, "no_speech_prob": 0.0008233787375502288}, {"id": 393, "seek": 222936, "start": 2241.6, "end": 2246.6400000000003, "text": " in that kind of discussion between the neuroscientists and the, and the philosophers. So there you,", "tokens": [50976, 294, 300, 733, 295, 5017, 1296, 264, 28813, 5412, 1751, 293, 264, 11, 293, 264, 36839, 13, 407, 456, 291, 11, 51228], "temperature": 0.0, "avg_logprob": -0.19248255472334605, "compression_ratio": 1.8365758754863812, "no_speech_prob": 0.0008233787375502288}, {"id": 394, "seek": 222936, "start": 2246.6400000000003, "end": 2250.1600000000003, "text": " there you're not talking about, you know, the everyday conversation that we're all having as,", "tokens": [51228, 456, 291, 434, 406, 1417, 466, 11, 291, 458, 11, 264, 7429, 3761, 300, 321, 434, 439, 1419, 382, 11, 51404], "temperature": 0.0, "avg_logprob": -0.19248255472334605, "compression_ratio": 1.8365758754863812, "no_speech_prob": 0.0008233787375502288}, {"id": 395, "seek": 222936, "start": 2250.1600000000003, "end": 2255.84, "text": " as, as humanity or as English speakers, or as Chinese speakers about how we use these, these,", "tokens": [51404, 382, 11, 382, 10243, 420, 382, 3669, 9518, 11, 420, 382, 4649, 9518, 466, 577, 321, 764, 613, 11, 613, 11, 51688], "temperature": 0.0, "avg_logprob": -0.19248255472334605, "compression_ratio": 1.8365758754863812, "no_speech_prob": 0.0008233787375502288}, {"id": 396, "seek": 225584, "start": 2255.92, "end": 2260.7200000000003, "text": " these words. So there it's a much more kind of confined to the, to, to different, two different", "tokens": [50368, 613, 2283, 13, 407, 456, 309, 311, 257, 709, 544, 733, 295, 31745, 281, 264, 11, 281, 11, 281, 819, 11, 732, 819, 50608], "temperature": 0.0, "avg_logprob": -0.13847586573386678, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.014845621772110462}, {"id": 397, "seek": 225584, "start": 2260.7200000000003, "end": 2270.4, "text": " schools or disciplines within academia. So there, I mean, I do think that the people who work in AI", "tokens": [50608, 4656, 420, 21919, 1951, 28937, 13, 407, 456, 11, 286, 914, 11, 286, 360, 519, 300, 264, 561, 567, 589, 294, 7318, 51092], "temperature": 0.0, "avg_logprob": -0.13847586573386678, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.014845621772110462}, {"id": 398, "seek": 225584, "start": 2270.4, "end": 2276.0, "text": " and in, and in neuroscience, probably at least should be a bit familiar with, with the philosophical", "tokens": [51092, 293, 294, 11, 293, 294, 42762, 11, 1391, 412, 1935, 820, 312, 257, 857, 4963, 365, 11, 365, 264, 25066, 51372], "temperature": 0.0, "avg_logprob": -0.13847586573386678, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.014845621772110462}, {"id": 399, "seek": 225584, "start": 2276.0, "end": 2279.84, "text": " debates. And you know, you mentioned Descartes earlier on, and you know, you're familiar with,", "tokens": [51372, 24203, 13, 400, 291, 458, 11, 291, 2835, 3885, 44672, 279, 3071, 322, 11, 293, 291, 458, 11, 291, 434, 4963, 365, 11, 51564], "temperature": 0.0, "avg_logprob": -0.13847586573386678, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.014845621772110462}, {"id": 400, "seek": 227984, "start": 2279.84, "end": 2286.2400000000002, "text": " with just that, that basic kind of, you know, sort of stuff that it was just like philosophy 101,", "tokens": [50364, 365, 445, 300, 11, 300, 3875, 733, 295, 11, 291, 458, 11, 1333, 295, 1507, 300, 309, 390, 445, 411, 10675, 21055, 11, 50684], "temperature": 0.0, "avg_logprob": -0.1605529144030659, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.06743471324443817}, {"id": 401, "seek": 227984, "start": 2286.88, "end": 2291.44, "text": " which people should at least be aware of Descartes arguments and then Chalmers, and the different", "tokens": [50716, 597, 561, 820, 412, 1935, 312, 3650, 295, 3885, 44672, 279, 12869, 293, 550, 761, 304, 18552, 11, 293, 264, 819, 50944], "temperature": 0.0, "avg_logprob": -0.1605529144030659, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.06743471324443817}, {"id": 402, "seek": 227984, "start": 2291.44, "end": 2295.92, "text": " kind of perspectives on those sorts of things before they pitch in, you know, at least, I mean,", "tokens": [50944, 733, 295, 16766, 322, 729, 7527, 295, 721, 949, 436, 7293, 294, 11, 291, 458, 11, 412, 1935, 11, 286, 914, 11, 51168], "temperature": 0.0, "avg_logprob": -0.1605529144030659, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.06743471324443817}, {"id": 403, "seek": 227984, "start": 2295.92, "end": 2300.1600000000003, "text": " you wouldn't pitch into neuroscience just by making some up some stuff about brains. If you", "tokens": [51168, 291, 2759, 380, 7293, 666, 42762, 445, 538, 1455, 512, 493, 512, 1507, 466, 15442, 13, 759, 291, 51380], "temperature": 0.0, "avg_logprob": -0.1605529144030659, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.06743471324443817}, {"id": 404, "seek": 227984, "start": 2300.1600000000003, "end": 2305.92, "text": " hadn't read, you know, the, an introduction to neuroscience. And so, so I think that the scientists", "tokens": [51380, 8782, 380, 1401, 11, 291, 458, 11, 264, 11, 364, 9339, 281, 42762, 13, 400, 370, 11, 370, 286, 519, 300, 264, 7708, 51668], "temperature": 0.0, "avg_logprob": -0.1605529144030659, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.06743471324443817}, {"id": 405, "seek": 230592, "start": 2305.92, "end": 2312.16, "text": " need to, you know, you know, they need to kind of have a, a past to enter the conversation,", "tokens": [50364, 643, 281, 11, 291, 458, 11, 291, 458, 11, 436, 643, 281, 733, 295, 362, 257, 11, 257, 1791, 281, 3242, 264, 3761, 11, 50676], "temperature": 0.0, "avg_logprob": -0.13648494209830217, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0013347856001928449}, {"id": 406, "seek": 230592, "start": 2312.16, "end": 2317.76, "text": " which is to have, to have gone through philosophy 101. Yeah, it's so true. We have the same thing with", "tokens": [50676, 597, 307, 281, 362, 11, 281, 362, 2780, 807, 10675, 21055, 13, 865, 11, 309, 311, 370, 2074, 13, 492, 362, 264, 912, 551, 365, 50956], "temperature": 0.0, "avg_logprob": -0.13648494209830217, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0013347856001928449}, {"id": 407, "seek": 230592, "start": 2317.76, "end": 2322.56, "text": " the, with the ethics folks, actually, because, because we have a lot of them fields of expertise,", "tokens": [50956, 264, 11, 365, 264, 19769, 4024, 11, 767, 11, 570, 11, 570, 321, 362, 257, 688, 295, 552, 7909, 295, 11769, 11, 51196], "temperature": 0.0, "avg_logprob": -0.13648494209830217, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0013347856001928449}, {"id": 408, "seek": 230592, "start": 2323.12, "end": 2326.7200000000003, "text": " and engineers should learn more about ethics. Yeah, absolutely. But when they do have an", "tokens": [51224, 293, 11955, 820, 1466, 544, 466, 19769, 13, 865, 11, 3122, 13, 583, 562, 436, 360, 362, 364, 51404], "temperature": 0.0, "avg_logprob": -0.13648494209830217, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0013347856001928449}, {"id": 409, "seek": 230592, "start": 2326.7200000000003, "end": 2331.28, "text": " opinion about ethics quite, quite often, it's, it's, it's, you know, it can sometimes be a bit", "tokens": [51404, 4800, 466, 19769, 1596, 11, 1596, 2049, 11, 309, 311, 11, 309, 311, 11, 309, 311, 11, 291, 458, 11, 309, 393, 2171, 312, 257, 857, 51632], "temperature": 0.0, "avg_logprob": -0.13648494209830217, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0013347856001928449}, {"id": 410, "seek": 233128, "start": 2331.28, "end": 2337.0400000000004, "text": " naive. And, and, and, and, you know, at least you should be familiar with the kind of, but, but", "tokens": [50364, 29052, 13, 400, 11, 293, 11, 293, 11, 293, 11, 291, 458, 11, 412, 1935, 291, 820, 312, 4963, 365, 264, 733, 295, 11, 457, 11, 457, 50652], "temperature": 0.0, "avg_logprob": -0.11577543398229087, "compression_ratio": 1.953781512605042, "no_speech_prob": 0.0064414991065859795}, {"id": 411, "seek": 233128, "start": 2337.0400000000004, "end": 2341.52, "text": " that's an interesting and the difficult area in itself. Because of course, you know, you,", "tokens": [50652, 300, 311, 364, 1880, 293, 264, 2252, 1859, 294, 2564, 13, 1436, 295, 1164, 11, 291, 458, 11, 291, 11, 50876], "temperature": 0.0, "avg_logprob": -0.11577543398229087, "compression_ratio": 1.953781512605042, "no_speech_prob": 0.0064414991065859795}, {"id": 412, "seek": 233128, "start": 2343.1200000000003, "end": 2348.2400000000002, "text": " as a scientist, it's important that you take responsibility as a scientist and the, and that", "tokens": [50956, 382, 257, 12662, 11, 309, 311, 1021, 300, 291, 747, 6357, 382, 257, 12662, 293, 264, 11, 293, 300, 51212], "temperature": 0.0, "avg_logprob": -0.11577543398229087, "compression_ratio": 1.953781512605042, "no_speech_prob": 0.0064414991065859795}, {"id": 413, "seek": 233128, "start": 2348.2400000000002, "end": 2352.5600000000004, "text": " you take, you know, some ethical responsibility. But at the same time, you know, you've only got", "tokens": [51212, 291, 747, 11, 291, 458, 11, 512, 18890, 6357, 13, 583, 412, 264, 912, 565, 11, 291, 458, 11, 291, 600, 787, 658, 51428], "temperature": 0.0, "avg_logprob": -0.11577543398229087, "compression_ratio": 1.953781512605042, "no_speech_prob": 0.0064414991065859795}, {"id": 414, "seek": 233128, "start": 2352.5600000000004, "end": 2358.2400000000002, "text": " so much time to become an expert. So, so it's difficult to at the same time, take ethical", "tokens": [51428, 370, 709, 565, 281, 1813, 364, 5844, 13, 407, 11, 370, 309, 311, 2252, 281, 412, 264, 912, 565, 11, 747, 18890, 51712], "temperature": 0.0, "avg_logprob": -0.11577543398229087, "compression_ratio": 1.953781512605042, "no_speech_prob": 0.0064414991065859795}, {"id": 415, "seek": 235824, "start": 2358.3199999999997, "end": 2364.8799999999997, "text": " responsibility. And yet, you know, even though perhaps you haven't got the time to kind of read,", "tokens": [50368, 6357, 13, 400, 1939, 11, 291, 458, 11, 754, 1673, 4317, 291, 2378, 380, 658, 264, 565, 281, 733, 295, 1401, 11, 50696], "temperature": 0.0, "avg_logprob": -0.09476047338441361, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.010158688761293888}, {"id": 416, "seek": 235824, "start": 2364.8799999999997, "end": 2371.2799999999997, "text": " you know, read up and become an expert on the relevant ethics. So, I mean, perhaps everybody", "tokens": [50696, 291, 458, 11, 1401, 493, 293, 1813, 364, 5844, 322, 264, 7340, 19769, 13, 407, 11, 286, 914, 11, 4317, 2201, 51016], "temperature": 0.0, "avg_logprob": -0.09476047338441361, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.010158688761293888}, {"id": 417, "seek": 235824, "start": 2371.2799999999997, "end": 2377.3599999999997, "text": " again, should, you know, get to the entry level, you know, ethics 101. And indeed, many, many courses", "tokens": [51016, 797, 11, 820, 11, 291, 458, 11, 483, 281, 264, 8729, 1496, 11, 291, 458, 11, 19769, 21055, 13, 400, 6451, 11, 867, 11, 867, 7712, 51320], "temperature": 0.0, "avg_logprob": -0.09476047338441361, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.010158688761293888}, {"id": 418, "seek": 235824, "start": 2377.3599999999997, "end": 2382.3999999999996, "text": " teach, you know, ethics, these days, many kind of science and computer science. It's part of,", "tokens": [51320, 2924, 11, 291, 458, 11, 19769, 11, 613, 1708, 11, 867, 733, 295, 3497, 293, 3820, 3497, 13, 467, 311, 644, 295, 11, 51572], "temperature": 0.0, "avg_logprob": -0.09476047338441361, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.010158688761293888}, {"id": 419, "seek": 235824, "start": 2382.3999999999996, "end": 2386.7999999999997, "text": " you know, of any degree these days. So that's a good step, I think. Yeah, there's an interesting", "tokens": [51572, 291, 458, 11, 295, 604, 4314, 613, 1708, 13, 407, 300, 311, 257, 665, 1823, 11, 286, 519, 13, 865, 11, 456, 311, 364, 1880, 51792], "temperature": 0.0, "avg_logprob": -0.09476047338441361, "compression_ratio": 1.8120300751879699, "no_speech_prob": 0.010158688761293888}, {"id": 420, "seek": 238680, "start": 2386.8, "end": 2390.96, "text": " analogy between the functionalism that we were speaking about in consciousness. I mean, even", "tokens": [50364, 21663, 1296, 264, 11745, 1434, 300, 321, 645, 4124, 466, 294, 10081, 13, 286, 914, 11, 754, 50572], "temperature": 0.0, "avg_logprob": -0.09171031188964844, "compression_ratio": 1.9340277777777777, "no_speech_prob": 0.014734171330928802}, {"id": 421, "seek": 238680, "start": 2390.96, "end": 2395.36, "text": " in our own research domain, we have the function of ethics, and we have linguists, and we have", "tokens": [50572, 294, 527, 1065, 2132, 9274, 11, 321, 362, 264, 2445, 295, 19769, 11, 293, 321, 362, 21766, 1751, 11, 293, 321, 362, 50792], "temperature": 0.0, "avg_logprob": -0.09171031188964844, "compression_ratio": 1.9340277777777777, "no_speech_prob": 0.014734171330928802}, {"id": 422, "seek": 238680, "start": 2395.36, "end": 2399.36, "text": " all sorts of different people. And that is the blind man and the elephant. And, you know, I", "tokens": [50792, 439, 7527, 295, 819, 561, 13, 400, 300, 307, 264, 6865, 587, 293, 264, 19791, 13, 400, 11, 291, 458, 11, 286, 50992], "temperature": 0.0, "avg_logprob": -0.09171031188964844, "compression_ratio": 1.9340277777777777, "no_speech_prob": 0.014734171330928802}, {"id": 423, "seek": 238680, "start": 2399.36, "end": 2405.04, "text": " tend to believe that even though the views from these diverse folks are inconsistent, diversity", "tokens": [50992, 3928, 281, 1697, 300, 754, 1673, 264, 6809, 490, 613, 9521, 4024, 366, 36891, 11, 8811, 51276], "temperature": 0.0, "avg_logprob": -0.09171031188964844, "compression_ratio": 1.9340277777777777, "no_speech_prob": 0.014734171330928802}, {"id": 424, "seek": 238680, "start": 2405.04, "end": 2410.0800000000004, "text": " is very important. Oh, incredibly important. Intellectual diversity is, you know, every", "tokens": [51276, 307, 588, 1021, 13, 876, 11, 6252, 1021, 13, 18762, 557, 901, 8811, 307, 11, 291, 458, 11, 633, 51528], "temperature": 0.0, "avg_logprob": -0.09171031188964844, "compression_ratio": 1.9340277777777777, "no_speech_prob": 0.014734171330928802}, {"id": 425, "seek": 238680, "start": 2410.0800000000004, "end": 2414.7200000000003, "text": " kind of diversity is important. And intellectual diversity is immensely important. And having", "tokens": [51528, 733, 295, 8811, 307, 1021, 13, 400, 12576, 8811, 307, 38674, 1021, 13, 400, 1419, 51760], "temperature": 0.0, "avg_logprob": -0.09171031188964844, "compression_ratio": 1.9340277777777777, "no_speech_prob": 0.014734171330928802}, {"id": 426, "seek": 241472, "start": 2414.7999999999997, "end": 2419.68, "text": " these conversations, these interdisciplinary conversations is absolutely, you know, essential.", "tokens": [50368, 613, 7315, 11, 613, 38280, 7315, 307, 3122, 11, 291, 458, 11, 7115, 13, 50612], "temperature": 0.0, "avg_logprob": -0.09152261167764664, "compression_ratio": 1.7483870967741935, "no_speech_prob": 0.00228545512072742}, {"id": 427, "seek": 241472, "start": 2419.68, "end": 2424.7999999999997, "text": " So at least if people are talking to each other, that's a really, really positive thing, I think.", "tokens": [50612, 407, 412, 1935, 498, 561, 366, 1417, 281, 1184, 661, 11, 300, 311, 257, 534, 11, 534, 3353, 551, 11, 286, 519, 13, 50868], "temperature": 0.0, "avg_logprob": -0.09152261167764664, "compression_ratio": 1.7483870967741935, "no_speech_prob": 0.00228545512072742}, {"id": 428, "seek": 241472, "start": 2424.7999999999997, "end": 2429.8399999999997, "text": " Fantastic. Now, we invited Chalmers on our podcast after Ilya Sootskeva's tweet. And by the way,", "tokens": [50868, 21320, 13, 823, 11, 321, 9185, 761, 304, 18552, 322, 527, 7367, 934, 286, 45106, 407, 1971, 330, 2757, 311, 15258, 13, 400, 538, 264, 636, 11, 51120], "temperature": 0.0, "avg_logprob": -0.09152261167764664, "compression_ratio": 1.7483870967741935, "no_speech_prob": 0.00228545512072742}, {"id": 429, "seek": 241472, "start": 2429.8399999999997, "end": 2435.12, "text": " Chalmers took a very functionalist approach to talking about consciousness. But I guess,", "tokens": [51120, 761, 304, 18552, 1890, 257, 588, 11745, 468, 3109, 281, 1417, 466, 10081, 13, 583, 286, 2041, 11, 51384], "temperature": 0.0, "avg_logprob": -0.09152261167764664, "compression_ratio": 1.7483870967741935, "no_speech_prob": 0.00228545512072742}, {"id": 430, "seek": 241472, "start": 2435.9199999999996, "end": 2440.08, "text": " after that tweet, everyone in the community started thinking about and talking about", "tokens": [51424, 934, 300, 15258, 11, 1518, 294, 264, 1768, 1409, 1953, 466, 293, 1417, 466, 51632], "temperature": 0.0, "avg_logprob": -0.09152261167764664, "compression_ratio": 1.7483870967741935, "no_speech_prob": 0.00228545512072742}, {"id": 431, "seek": 241472, "start": 2440.08, "end": 2443.8399999999997, "text": " consciousness. So maybe let's just start from that tweet. How did you find it?", "tokens": [51632, 10081, 13, 407, 1310, 718, 311, 445, 722, 490, 300, 15258, 13, 1012, 630, 291, 915, 309, 30, 51820], "temperature": 0.0, "avg_logprob": -0.09152261167764664, "compression_ratio": 1.7483870967741935, "no_speech_prob": 0.00228545512072742}, {"id": 432, "seek": 244384, "start": 2443.92, "end": 2451.28, "text": " Sure. Yeah, okay. So the tweet was, so Ilya Sootskeva said, it may be that today's large", "tokens": [50368, 4894, 13, 865, 11, 1392, 13, 407, 264, 15258, 390, 11, 370, 286, 45106, 407, 1971, 330, 2757, 848, 11, 309, 815, 312, 300, 965, 311, 2416, 50736], "temperature": 0.0, "avg_logprob": -0.16384843383172545, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.002729592379182577}, {"id": 433, "seek": 244384, "start": 2451.28, "end": 2460.2400000000002, "text": " language models are slightly conscious. And then I replied, tweeted back in the same sense that", "tokens": [50736, 2856, 5245, 366, 4748, 6648, 13, 400, 550, 286, 20345, 11, 25646, 646, 294, 264, 912, 2020, 300, 51184], "temperature": 0.0, "avg_logprob": -0.16384843383172545, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.002729592379182577}, {"id": 434, "seek": 244384, "start": 2460.8, "end": 2467.2000000000003, "text": " may be a large field of wheat is slightly pasta. And that, in fact, was actually, I mean,", "tokens": [51212, 815, 312, 257, 2416, 2519, 295, 16691, 307, 4748, 13296, 13, 400, 300, 11, 294, 1186, 11, 390, 767, 11, 286, 914, 11, 51532], "temperature": 0.0, "avg_logprob": -0.16384843383172545, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.002729592379182577}, {"id": 435, "seek": 244384, "start": 2467.2000000000003, "end": 2471.84, "text": " I've got a fair number of Twitter followers, and that was the most engaged tweet I've ever sent", "tokens": [51532, 286, 600, 658, 257, 3143, 1230, 295, 5794, 13071, 11, 293, 300, 390, 264, 881, 8237, 15258, 286, 600, 1562, 2279, 51764], "temperature": 0.0, "avg_logprob": -0.16384843383172545, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.002729592379182577}, {"id": 436, "seek": 247184, "start": 2471.84, "end": 2477.92, "text": " out. And, you know, and, you know, it got celebrity likes, Hannah Fried retweeted it, and, you", "tokens": [50364, 484, 13, 400, 11, 291, 458, 11, 293, 11, 291, 458, 11, 309, 658, 18597, 5902, 11, 21754, 17605, 1533, 10354, 292, 309, 11, 293, 11, 291, 50668], "temperature": 0.0, "avg_logprob": -0.18870212918236143, "compression_ratio": 1.625, "no_speech_prob": 0.01168884988874197}, {"id": 437, "seek": 247184, "start": 2477.92, "end": 2488.4, "text": " know, only as my kind of comment. And so, so, but that does kind of summarize sort of what I think", "tokens": [50668, 458, 11, 787, 382, 452, 733, 295, 2871, 13, 400, 370, 11, 370, 11, 457, 300, 775, 733, 295, 20858, 1333, 295, 437, 286, 519, 51192], "temperature": 0.0, "avg_logprob": -0.18870212918236143, "compression_ratio": 1.625, "no_speech_prob": 0.01168884988874197}, {"id": 438, "seek": 247184, "start": 2488.4, "end": 2496.08, "text": " about, about what he said at that point. But then, but then I after tweeting my, my flippant", "tokens": [51192, 466, 11, 466, 437, 415, 848, 412, 300, 935, 13, 583, 550, 11, 457, 550, 286, 934, 40090, 452, 11, 452, 932, 2488, 394, 51576], "temperature": 0.0, "avg_logprob": -0.18870212918236143, "compression_ratio": 1.625, "no_speech_prob": 0.01168884988874197}, {"id": 439, "seek": 249608, "start": 2496.16, "end": 2502.88, "text": " response, I then I was violating all my own Twitter rules in in just sending back a flippant", "tokens": [50368, 4134, 11, 286, 550, 286, 390, 42201, 439, 452, 1065, 5794, 4474, 294, 294, 445, 7750, 646, 257, 932, 2488, 394, 50704], "temperature": 0.0, "avg_logprob": -0.09079950138673944, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.25580981373786926}, {"id": 440, "seek": 249608, "start": 2502.88, "end": 2509.04, "text": " response, because I generally don't do that. I would rather kind of, you know, be professional,", "tokens": [50704, 4134, 11, 570, 286, 5101, 500, 380, 360, 300, 13, 286, 576, 2831, 733, 295, 11, 291, 458, 11, 312, 4843, 11, 51012], "temperature": 0.0, "avg_logprob": -0.09079950138673944, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.25580981373786926}, {"id": 441, "seek": 249608, "start": 2509.04, "end": 2513.36, "text": " engage professionally. And so I thought it's very important to follow that on with a, you know,", "tokens": [51012, 4683, 27941, 13, 400, 370, 286, 1194, 309, 311, 588, 1021, 281, 1524, 300, 322, 365, 257, 11, 291, 458, 11, 51228], "temperature": 0.0, "avg_logprob": -0.09079950138673944, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.25580981373786926}, {"id": 442, "seek": 249608, "start": 2513.36, "end": 2518.72, "text": " with a little explanation of why, you know, why I thought that it wasn't really appropriate to", "tokens": [51228, 365, 257, 707, 10835, 295, 983, 11, 291, 458, 11, 983, 286, 1194, 300, 309, 2067, 380, 534, 6854, 281, 51496], "temperature": 0.0, "avg_logprob": -0.09079950138673944, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.25580981373786926}, {"id": 443, "seek": 249608, "start": 2518.72, "end": 2524.16, "text": " speak about today's large language models in those terms. Yeah. And for me, the number one thing is", "tokens": [51496, 1710, 466, 965, 311, 2416, 2856, 5245, 294, 729, 2115, 13, 865, 13, 400, 337, 385, 11, 264, 1230, 472, 551, 307, 51768], "temperature": 0.0, "avg_logprob": -0.09079950138673944, "compression_ratio": 1.6866197183098592, "no_speech_prob": 0.25580981373786926}, {"id": 444, "seek": 252416, "start": 2524.16, "end": 2532.56, "text": " to do with embodiment. So, so as I see it, embodiment is a kind of prerequisite for for us", "tokens": [50364, 281, 360, 365, 28935, 2328, 13, 407, 11, 370, 382, 286, 536, 309, 11, 28935, 2328, 307, 257, 733, 295, 38333, 34152, 337, 337, 505, 50784], "temperature": 0.0, "avg_logprob": -0.08828028043111165, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.027542708441615105}, {"id": 445, "seek": 252416, "start": 2532.56, "end": 2538.3999999999996, "text": " being able to use that, that word, use words like consciousness and so on, you know, in the way that", "tokens": [50784, 885, 1075, 281, 764, 300, 11, 300, 1349, 11, 764, 2283, 411, 10081, 293, 370, 322, 11, 291, 458, 11, 294, 264, 636, 300, 51076], "temperature": 0.0, "avg_logprob": -0.08828028043111165, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.027542708441615105}, {"id": 446, "seek": 252416, "start": 2538.3999999999996, "end": 2544.64, "text": " we do in the normal everyday way of talking. So, so, you know, it's only in the presence of something", "tokens": [51076, 321, 360, 294, 264, 2710, 7429, 636, 295, 1417, 13, 407, 11, 370, 11, 291, 458, 11, 309, 311, 787, 294, 264, 6814, 295, 746, 51388], "temperature": 0.0, "avg_logprob": -0.08828028043111165, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.027542708441615105}, {"id": 447, "seek": 252416, "start": 2546.16, "end": 2551.3599999999997, "text": " that that inhabits our world. And by inhabits, I don't mean just has a physical, you know,", "tokens": [51464, 300, 300, 16934, 1208, 527, 1002, 13, 400, 538, 16934, 1208, 11, 286, 500, 380, 914, 445, 575, 257, 4001, 11, 291, 458, 11, 51724], "temperature": 0.0, "avg_logprob": -0.08828028043111165, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.027542708441615105}, {"id": 448, "seek": 255136, "start": 2551.36, "end": 2554.96, "text": " like a computer is obviously a physical thing in our world, but inhabits our world means that,", "tokens": [50364, 411, 257, 3820, 307, 2745, 257, 4001, 551, 294, 527, 1002, 11, 457, 16934, 1208, 527, 1002, 1355, 300, 11, 50544], "temperature": 0.0, "avg_logprob": -0.195628191973712, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.011748077347874641}, {"id": 449, "seek": 255136, "start": 2555.6800000000003, "end": 2562.48, "text": " you know, walks around in her own swims or jumps or flies or whatever, but is is is inhabits the", "tokens": [50580, 291, 458, 11, 12896, 926, 294, 720, 1065, 42357, 420, 16704, 420, 17414, 420, 2035, 11, 457, 307, 307, 307, 16934, 1208, 264, 50920], "temperature": 0.0, "avg_logprob": -0.195628191973712, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.011748077347874641}, {"id": 450, "seek": 255136, "start": 2562.48, "end": 2570.32, "text": " same world as us and interacts with it and, and, and, and you know, and interacts with the objects", "tokens": [50920, 912, 1002, 382, 505, 293, 43582, 365, 309, 293, 11, 293, 11, 293, 11, 293, 291, 458, 11, 293, 43582, 365, 264, 6565, 51312], "temperature": 0.0, "avg_logprob": -0.195628191973712, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.011748077347874641}, {"id": 451, "seek": 257032, "start": 2570.32, "end": 2578.8, "text": " in it and with other, with other creatures like ourselves. So, so that to my mind, that is,", "tokens": [50364, 294, 309, 293, 365, 661, 11, 365, 661, 12281, 411, 4175, 13, 407, 11, 370, 300, 281, 452, 1575, 11, 300, 307, 11, 50788], "temperature": 0.0, "avg_logprob": -0.20600677573162576, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.4001595079898834}, {"id": 452, "seek": 257032, "start": 2578.8, "end": 2584.56, "text": " that's the, that's so, so, so in that paper conscious exotica, I think I use this phrase", "tokens": [50788, 300, 311, 264, 11, 300, 311, 370, 11, 370, 11, 370, 294, 300, 3035, 6648, 454, 310, 2262, 11, 286, 519, 286, 764, 341, 9535, 51076], "temperature": 0.0, "avg_logprob": -0.20600677573162576, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.4001595079898834}, {"id": 453, "seek": 257032, "start": 2584.56, "end": 2589.28, "text": " trans channeling Wittgenstein that that only in the context of something that", "tokens": [51076, 1145, 2269, 278, 343, 593, 1766, 9089, 300, 300, 787, 294, 264, 4319, 295, 746, 300, 51312], "temperature": 0.0, "avg_logprob": -0.20600677573162576, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.4001595079898834}, {"id": 454, "seek": 257032, "start": 2591.04, "end": 2597.44, "text": " that exhibits purposeful behavior, do we speak of consciousness. And the way that that's", "tokens": [51400, 300, 39205, 4334, 906, 5223, 11, 360, 321, 1710, 295, 10081, 13, 400, 264, 636, 300, 300, 311, 51720], "temperature": 0.0, "avg_logprob": -0.20600677573162576, "compression_ratio": 1.6763285024154588, "no_speech_prob": 0.4001595079898834}, {"id": 455, "seek": 259744, "start": 2597.44, "end": 2602.96, "text": " phrased, there is kind of, you know, so trying to channel Wittgenstein's style of saying things,", "tokens": [50364, 7636, 1937, 11, 456, 307, 733, 295, 11, 291, 458, 11, 370, 1382, 281, 2269, 343, 593, 1766, 9089, 311, 3758, 295, 1566, 721, 11, 50640], "temperature": 0.0, "avg_logprob": -0.1305228356392153, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.01807350292801857}, {"id": 456, "seek": 259744, "start": 2602.96, "end": 2607.84, "text": " because you notice that he's saying that he's making what he's saying is actually he's talking", "tokens": [50640, 570, 291, 3449, 300, 415, 311, 1566, 300, 415, 311, 1455, 437, 415, 311, 1566, 307, 767, 415, 311, 1417, 50884], "temperature": 0.0, "avg_logprob": -0.1305228356392153, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.01807350292801857}, {"id": 457, "seek": 259744, "start": 2607.84, "end": 2612.7200000000003, "text": " about the way we use the word. So he's not making a metaphysical claim. This is essential. He's", "tokens": [50884, 466, 264, 636, 321, 764, 264, 1349, 13, 407, 415, 311, 406, 1455, 257, 30946, 36280, 3932, 13, 639, 307, 7115, 13, 634, 311, 51128], "temperature": 0.0, "avg_logprob": -0.1305228356392153, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.01807350292801857}, {"id": 458, "seek": 259744, "start": 2612.7200000000003, "end": 2618.8, "text": " saying that this is just this is the circumstances under which we use this word. So we use this word", "tokens": [51128, 1566, 300, 341, 307, 445, 341, 307, 264, 9121, 833, 597, 321, 764, 341, 1349, 13, 407, 321, 764, 341, 1349, 51432], "temperature": 0.0, "avg_logprob": -0.1305228356392153, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.01807350292801857}, {"id": 459, "seek": 259744, "start": 2618.8, "end": 2624.7200000000003, "text": " in the context of fellow creatures, basically. And so, so that's the kind of the starting point.", "tokens": [51432, 294, 264, 4319, 295, 7177, 12281, 11, 1936, 13, 400, 370, 11, 370, 300, 311, 264, 733, 295, 264, 2891, 935, 13, 51728], "temperature": 0.0, "avg_logprob": -0.1305228356392153, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.01807350292801857}, {"id": 460, "seek": 262472, "start": 2624.72, "end": 2629.04, "text": " So a large, and of course, we, of course, we talk to people on the telephone and over the", "tokens": [50364, 407, 257, 2416, 11, 293, 295, 1164, 11, 321, 11, 295, 1164, 11, 321, 751, 281, 561, 322, 264, 19800, 293, 670, 264, 50580], "temperature": 0.0, "avg_logprob": -0.13942144856308447, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.004845740273594856}, {"id": 461, "seek": 262472, "start": 2629.04, "end": 2634.3999999999996, "text": " internet and so on. And we don't, you know, we may not, you know, we can't see them or anything.", "tokens": [50580, 4705, 293, 370, 322, 13, 400, 321, 500, 380, 11, 291, 458, 11, 321, 815, 406, 11, 291, 458, 11, 321, 393, 380, 536, 552, 420, 1340, 13, 50848], "temperature": 0.0, "avg_logprob": -0.13942144856308447, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.004845740273594856}, {"id": 462, "seek": 262472, "start": 2634.3999999999996, "end": 2640.8799999999997, "text": " So we, but, but, but we still, we know that there is, you know, or we assume, we've always been", "tokens": [50848, 407, 321, 11, 457, 11, 457, 11, 457, 321, 920, 11, 321, 458, 300, 456, 307, 11, 291, 458, 11, 420, 321, 6552, 11, 321, 600, 1009, 668, 51172], "temperature": 0.0, "avg_logprob": -0.13942144856308447, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.004845740273594856}, {"id": 463, "seek": 262472, "start": 2640.8799999999997, "end": 2644.72, "text": " able to assume up to this point that there is a fellow creature at the other end. And that's the", "tokens": [51172, 1075, 281, 6552, 493, 281, 341, 935, 300, 456, 307, 257, 7177, 12797, 412, 264, 661, 917, 13, 400, 300, 311, 264, 51364], "temperature": 0.0, "avg_logprob": -0.13942144856308447, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.004845740273594856}, {"id": 464, "seek": 262472, "start": 2644.72, "end": 2651.04, "text": " kind of grounding for kind of thinking that way and using using that word. Now that is absent", "tokens": [51364, 733, 295, 46727, 337, 733, 295, 1953, 300, 636, 293, 1228, 1228, 300, 1349, 13, 823, 300, 307, 25185, 51680], "temperature": 0.0, "avg_logprob": -0.13942144856308447, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.004845740273594856}, {"id": 465, "seek": 265104, "start": 2651.04, "end": 2655.68, "text": " in large language models. So large language models do not inhabit the world that we do.", "tokens": [50364, 294, 2416, 2856, 5245, 13, 407, 2416, 2856, 5245, 360, 406, 21863, 264, 1002, 300, 321, 360, 13, 50596], "temperature": 0.0, "avg_logprob": -0.1252829074859619, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.006522330921143293}, {"id": 466, "seek": 265104, "start": 2659.2799999999997, "end": 2664.72, "text": " Now, I mean, we have to caveat that because, of course, it's possible to embed a large language", "tokens": [50776, 823, 11, 286, 914, 11, 321, 362, 281, 43012, 300, 570, 11, 295, 1164, 11, 309, 311, 1944, 281, 12240, 257, 2416, 2856, 51048], "temperature": 0.0, "avg_logprob": -0.1252829074859619, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.006522330921143293}, {"id": 467, "seek": 265104, "start": 2664.72, "end": 2670.8, "text": " model in a, you know, in a, we always do embed it in a larger system. It might be very simple", "tokens": [51048, 2316, 294, 257, 11, 291, 458, 11, 294, 257, 11, 321, 1009, 360, 12240, 309, 294, 257, 4833, 1185, 13, 467, 1062, 312, 588, 2199, 51352], "temperature": 0.0, "avg_logprob": -0.1252829074859619, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.006522330921143293}, {"id": 468, "seek": 265104, "start": 2670.8, "end": 2675.2, "text": " embedding. It might be just a chatbot, or it might be much more complicated, like it might", "tokens": [51352, 12240, 3584, 13, 467, 1062, 312, 445, 257, 5081, 18870, 11, 420, 309, 1062, 312, 709, 544, 6179, 11, 411, 309, 1062, 51572], "temperature": 0.0, "avg_logprob": -0.1252829074859619, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.006522330921143293}, {"id": 469, "seek": 265104, "start": 2675.2, "end": 2680.56, "text": " be be part of a system that enables a robot to kind of move around and interact with the world", "tokens": [51572, 312, 312, 644, 295, 257, 1185, 300, 17077, 257, 7881, 281, 733, 295, 1286, 926, 293, 4648, 365, 264, 1002, 51840], "temperature": 0.0, "avg_logprob": -0.1252829074859619, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.006522330921143293}, {"id": 470, "seek": 268056, "start": 2680.56, "end": 2687.12, "text": " and take instructions and so on. So there was a great, some great work from Google with their", "tokens": [50364, 293, 747, 9415, 293, 370, 322, 13, 407, 456, 390, 257, 869, 11, 512, 869, 589, 490, 3329, 365, 641, 50692], "temperature": 0.0, "avg_logprob": -0.1634957752530537, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.00125123281031847}, {"id": 471, "seek": 268056, "start": 2687.12, "end": 2691.7599999999998, "text": " Palm Seican robot, for example, where there's this embedded large language model. So, so,", "tokens": [50692, 32668, 1100, 8914, 7881, 11, 337, 1365, 11, 689, 456, 311, 341, 16741, 2416, 2856, 2316, 13, 407, 11, 370, 11, 50924], "temperature": 0.0, "avg_logprob": -0.1634957752530537, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.00125123281031847}, {"id": 472, "seek": 268056, "start": 2691.7599999999998, "end": 2697.12, "text": " so there you're kind of moving in a, in a direction where maybe where these, where these words, you", "tokens": [50924, 370, 456, 291, 434, 733, 295, 2684, 294, 257, 11, 294, 257, 3513, 689, 1310, 689, 613, 11, 689, 613, 2283, 11, 291, 51192], "temperature": 0.0, "avg_logprob": -0.1634957752530537, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.00125123281031847}, {"id": 473, "seek": 268056, "start": 2697.12, "end": 2704.08, "text": " know, the prerequisites, you know, for, for, well, actually, I want to be careful what I say here.", "tokens": [51192, 458, 11, 264, 38333, 15398, 3324, 11, 291, 458, 11, 337, 11, 337, 11, 731, 11, 767, 11, 286, 528, 281, 312, 5026, 437, 286, 584, 510, 13, 51540], "temperature": 0.0, "avg_logprob": -0.1634957752530537, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.00125123281031847}, {"id": 474, "seek": 268056, "start": 2704.08, "end": 2708.7999999999997, "text": " Sorry. Sorry. Because it's so easy to say something that's going to can be misinterpreted,", "tokens": [51540, 4919, 13, 4919, 13, 1436, 309, 311, 370, 1858, 281, 584, 746, 300, 311, 516, 281, 393, 312, 3346, 41935, 292, 11, 51776], "temperature": 0.0, "avg_logprob": -0.1634957752530537, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.00125123281031847}, {"id": 475, "seek": 270880, "start": 2708.8, "end": 2714.5600000000004, "text": " right? Yes. But, but we can imagine that, that we can imagine that requirement being met for, for,", "tokens": [50364, 558, 30, 1079, 13, 583, 11, 457, 321, 393, 3811, 300, 11, 300, 321, 393, 3811, 300, 11695, 885, 1131, 337, 11, 337, 11, 50652], "temperature": 0.0, "avg_logprob": -0.13403372610768965, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0024998823646456003}, {"id": 476, "seek": 270880, "start": 2714.5600000000004, "end": 2719.6000000000004, "text": " for not, not, it doesn't mean it wouldn't be a sufficient condition for using those words,", "tokens": [50652, 337, 406, 11, 406, 11, 309, 1177, 380, 914, 309, 2759, 380, 312, 257, 11563, 4188, 337, 1228, 729, 2283, 11, 50904], "temperature": 0.0, "avg_logprob": -0.13403372610768965, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0024998823646456003}, {"id": 477, "seek": 270880, "start": 2719.6000000000004, "end": 2723.36, "text": " but at least it would, you'd meet the necessary conditions, right? Yes. But the large language", "tokens": [50904, 457, 412, 1935, 309, 576, 11, 291, 1116, 1677, 264, 4818, 4487, 11, 558, 30, 1079, 13, 583, 264, 2416, 2856, 51092], "temperature": 0.0, "avg_logprob": -0.13403372610768965, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0024998823646456003}, {"id": 478, "seek": 270880, "start": 2723.36, "end": 2730.2400000000002, "text": " models by themselves do not meet even, they're not even candidates. Yes, I agree. And we,", "tokens": [51092, 5245, 538, 2969, 360, 406, 1677, 754, 11, 436, 434, 406, 754, 11255, 13, 1079, 11, 286, 3986, 13, 400, 321, 11, 51436], "temperature": 0.0, "avg_logprob": -0.13403372610768965, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0024998823646456003}, {"id": 479, "seek": 270880, "start": 2730.2400000000002, "end": 2734.0, "text": " there's so many things we can do here, because we can, we can talk about embodiment in general. I", "tokens": [51436, 456, 311, 370, 867, 721, 321, 393, 360, 510, 11, 570, 321, 393, 11, 321, 393, 751, 466, 28935, 2328, 294, 2674, 13, 286, 51624], "temperature": 0.0, "avg_logprob": -0.13403372610768965, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0024998823646456003}, {"id": 480, "seek": 273400, "start": 2734.0, "end": 2739.36, "text": " mean, as I understand it, Rodney Brooks kind of started the phenomenon of thinking about a", "tokens": [50364, 914, 11, 382, 286, 1223, 309, 11, 11097, 2397, 33493, 733, 295, 1409, 264, 14029, 295, 1953, 466, 257, 50632], "temperature": 0.0, "avg_logprob": -0.13974359456230612, "compression_ratio": 1.8263665594855305, "no_speech_prob": 0.007857940159738064}, {"id": 481, "seek": 273400, "start": 2739.36, "end": 2744.16, "text": " representationalist view of artificial intelligence or, or rejecting, rejecting a representation.", "tokens": [50632, 2906, 1478, 468, 1910, 295, 11677, 7599, 420, 11, 420, 45401, 11, 45401, 257, 10290, 13, 50872], "temperature": 0.0, "avg_logprob": -0.13974359456230612, "compression_ratio": 1.8263665594855305, "no_speech_prob": 0.007857940159738064}, {"id": 482, "seek": 273400, "start": 2744.16, "end": 2748.08, "text": " Rejecting. Yeah. So, so Rodney Brooks thought that we should use the world as its own best", "tokens": [50872, 1300, 1020, 278, 13, 865, 13, 407, 11, 370, 11097, 2397, 33493, 1194, 300, 321, 820, 764, 264, 1002, 382, 1080, 1065, 1151, 51068], "temperature": 0.0, "avg_logprob": -0.13974359456230612, "compression_ratio": 1.8263665594855305, "no_speech_prob": 0.007857940159738064}, {"id": 483, "seek": 273400, "start": 2748.08, "end": 2752.88, "text": " representation, which is absolutely fascinating. Yeah. And then you, maybe you might be thinking", "tokens": [51068, 10290, 11, 597, 307, 3122, 10343, 13, 865, 13, 400, 550, 291, 11, 1310, 291, 1062, 312, 1953, 51308], "temperature": 0.0, "avg_logprob": -0.13974359456230612, "compression_ratio": 1.8263665594855305, "no_speech_prob": 0.007857940159738064}, {"id": 484, "seek": 273400, "start": 2752.88, "end": 2757.6, "text": " about the embodiment of you in a similar style of Wittgenstein. So it's a matter of complexity,", "tokens": [51308, 466, 264, 28935, 2328, 295, 291, 294, 257, 2531, 3758, 295, 343, 593, 1766, 9089, 13, 407, 309, 311, 257, 1871, 295, 14024, 11, 51544], "temperature": 0.0, "avg_logprob": -0.13974359456230612, "compression_ratio": 1.8263665594855305, "no_speech_prob": 0.007857940159738064}, {"id": 485, "seek": 273400, "start": 2757.6, "end": 2761.76, "text": " and it's also a matter of encapsulation, which is fascinating. But, but also just to quote your", "tokens": [51544, 293, 309, 311, 611, 257, 1871, 295, 38745, 2776, 11, 597, 307, 10343, 13, 583, 11, 457, 611, 445, 281, 6513, 428, 51752], "temperature": 0.0, "avg_logprob": -0.13974359456230612, "compression_ratio": 1.8263665594855305, "no_speech_prob": 0.007857940159738064}, {"id": 486, "seek": 276176, "start": 2761.76, "end": 2766.0800000000004, "text": " paper, you said, although the language model component of SACAN provides natural language", "tokens": [50364, 3035, 11, 291, 848, 11, 4878, 264, 2856, 2316, 6542, 295, 318, 4378, 1770, 6417, 3303, 2856, 50580], "temperature": 0.0, "avg_logprob": -0.10631168775322024, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.002197963884100318}, {"id": 487, "seek": 276176, "start": 2766.0800000000004, "end": 2770.48, "text": " descriptions of low level actions, it doesn't take into account what the environment actually", "tokens": [50580, 24406, 295, 2295, 1496, 5909, 11, 309, 1177, 380, 747, 666, 2696, 437, 264, 2823, 767, 50800], "temperature": 0.0, "avg_logprob": -0.10631168775322024, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.002197963884100318}, {"id": 488, "seek": 276176, "start": 2770.48, "end": 2775.28, "text": " affords the robots. And there's this whole affordance thing as well. So, I mean, how do you", "tokens": [50800, 2096, 5703, 264, 14733, 13, 400, 456, 311, 341, 1379, 6157, 719, 551, 382, 731, 13, 407, 11, 286, 914, 11, 577, 360, 291, 51040], "temperature": 0.0, "avg_logprob": -0.10631168775322024, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.002197963884100318}, {"id": 489, "seek": 276176, "start": 2775.28, "end": 2782.8, "text": " think about embodiment? So, so the way I see it is that is that the, you know, the one exemplar we", "tokens": [51040, 519, 466, 28935, 2328, 30, 407, 11, 370, 264, 636, 286, 536, 309, 307, 300, 307, 300, 264, 11, 291, 458, 11, 264, 472, 24112, 289, 321, 51416], "temperature": 0.0, "avg_logprob": -0.10631168775322024, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.002197963884100318}, {"id": 490, "seek": 276176, "start": 2782.8, "end": 2790.7200000000003, "text": " have as of, you know, the end of 2022 of something that we really can describe as, as, as, as", "tokens": [51416, 362, 382, 295, 11, 291, 458, 11, 264, 917, 295, 20229, 295, 746, 300, 321, 534, 393, 6786, 382, 11, 382, 11, 382, 11, 382, 51812], "temperature": 0.0, "avg_logprob": -0.10631168775322024, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.002197963884100318}, {"id": 491, "seek": 279072, "start": 2790.72, "end": 2797.12, "text": " intelligent as generally intelligent is, is the biological brain, biological brains of humans,", "tokens": [50364, 13232, 382, 5101, 13232, 307, 11, 307, 264, 13910, 3567, 11, 13910, 15442, 295, 6255, 11, 50684], "temperature": 0.0, "avg_logprob": -0.09045483203644449, "compression_ratio": 1.8439024390243903, "no_speech_prob": 0.004406413994729519}, {"id": 492, "seek": 279072, "start": 2797.12, "end": 2804.9599999999996, "text": " but also of other animals. And the biological brain, you know, at its very, it's very kind of", "tokens": [50684, 457, 611, 295, 661, 4882, 13, 400, 264, 13910, 3567, 11, 291, 458, 11, 412, 1080, 588, 11, 309, 311, 588, 733, 295, 51076], "temperature": 0.0, "avg_logprob": -0.09045483203644449, "compression_ratio": 1.8439024390243903, "no_speech_prob": 0.004406413994729519}, {"id": 493, "seek": 279072, "start": 2804.9599999999996, "end": 2812.3199999999997, "text": " nature is it's there to help a creature to move around in the world, to move, right? It's there", "tokens": [51076, 3687, 307, 309, 311, 456, 281, 854, 257, 12797, 281, 1286, 926, 294, 264, 1002, 11, 281, 1286, 11, 558, 30, 467, 311, 456, 51444], "temperature": 0.0, "avg_logprob": -0.09045483203644449, "compression_ratio": 1.8439024390243903, "no_speech_prob": 0.004406413994729519}, {"id": 494, "seek": 279072, "start": 2812.3199999999997, "end": 2819.52, "text": " to move, help to guide a creature and help it move in order to help it survive and reproduce.", "tokens": [51444, 281, 1286, 11, 854, 281, 5934, 257, 12797, 293, 854, 309, 1286, 294, 1668, 281, 854, 309, 7867, 293, 29501, 13, 51804], "temperature": 0.0, "avg_logprob": -0.09045483203644449, "compression_ratio": 1.8439024390243903, "no_speech_prob": 0.004406413994729519}, {"id": 495, "seek": 281952, "start": 2819.52, "end": 2823.92, "text": " That's what brains are for. So that's what that from an evolutionary point of view, that's that", "tokens": [50364, 663, 311, 437, 15442, 366, 337, 13, 407, 300, 311, 437, 300, 490, 364, 27567, 935, 295, 1910, 11, 300, 311, 300, 50584], "temperature": 0.0, "avg_logprob": -0.1767851196893371, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.0019761642906814814}, {"id": 496, "seek": 281952, "start": 2823.92, "end": 2832.16, "text": " they developed in order to help a creature to move. And they are so they and they are, you know,", "tokens": [50584, 436, 4743, 294, 1668, 281, 854, 257, 12797, 281, 1286, 13, 400, 436, 366, 370, 436, 293, 436, 366, 11, 291, 458, 11, 50996], "temperature": 0.0, "avg_logprob": -0.1767851196893371, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.0019761642906814814}, {"id": 497, "seek": 281952, "start": 2832.16, "end": 2839.36, "text": " they're the bit that's comes between the sensory input and the motor output. And as far as you", "tokens": [50996, 436, 434, 264, 857, 300, 311, 1487, 1296, 264, 27233, 4846, 293, 264, 5932, 5598, 13, 400, 382, 1400, 382, 291, 51356], "temperature": 0.0, "avg_logprob": -0.1767851196893371, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.0019761642906814814}, {"id": 498, "seek": 281952, "start": 2839.36, "end": 2844.64, "text": " can cleanly divide these things, which maybe you can't, but I mean, so and so that's that's their", "tokens": [51356, 393, 2541, 356, 9845, 613, 721, 11, 597, 1310, 291, 393, 380, 11, 457, 286, 914, 11, 370, 293, 370, 300, 311, 300, 311, 641, 51620], "temperature": 0.0, "avg_logprob": -0.1767851196893371, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.0019761642906814814}, {"id": 499, "seek": 284464, "start": 2844.64, "end": 2850.48, "text": " purpose is to intervene in the sensory motor loop in a way that benefits the organism. And", "tokens": [50364, 4334, 307, 281, 30407, 294, 264, 27233, 5932, 6367, 294, 257, 636, 300, 5311, 264, 24128, 13, 400, 50656], "temperature": 0.0, "avg_logprob": -0.09122880299886067, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.01587413065135479}, {"id": 500, "seek": 284464, "start": 2850.48, "end": 2858.56, "text": " everything else is on built on top of that. So, so, so the capacity to recognize objects in our", "tokens": [50656, 1203, 1646, 307, 322, 3094, 322, 1192, 295, 300, 13, 407, 11, 370, 11, 370, 264, 6042, 281, 5521, 6565, 294, 527, 51060], "temperature": 0.0, "avg_logprob": -0.09122880299886067, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.01587413065135479}, {"id": 501, "seek": 284464, "start": 2858.56, "end": 2865.2, "text": " environments and categorize them and the ability to kind of manipulate objects in the environment,", "tokens": [51060, 12388, 293, 19250, 1125, 552, 293, 264, 3485, 281, 733, 295, 20459, 6565, 294, 264, 2823, 11, 51392], "temperature": 0.0, "avg_logprob": -0.09122880299886067, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.01587413065135479}, {"id": 502, "seek": 284464, "start": 2865.2, "end": 2872.4, "text": " pick them up and so on. And all of that is there, you know, initially to help the, the, the organism", "tokens": [51392, 1888, 552, 493, 293, 370, 322, 13, 400, 439, 295, 300, 307, 456, 11, 291, 458, 11, 9105, 281, 854, 264, 11, 264, 11, 264, 24128, 51752], "temperature": 0.0, "avg_logprob": -0.09122880299886067, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.01587413065135479}, {"id": 503, "seek": 287240, "start": 2872.48, "end": 2881.84, "text": " to survive. And, and, and, you know, and that's what brains brains are there for. And then,", "tokens": [50368, 281, 7867, 13, 400, 11, 293, 11, 293, 11, 291, 458, 11, 293, 300, 311, 437, 15442, 15442, 366, 456, 337, 13, 400, 550, 11, 50836], "temperature": 0.0, "avg_logprob": -0.09287899732589722, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.008923170156776905}, {"id": 504, "seek": 287240, "start": 2881.84, "end": 2889.6, "text": " then when it comes to, you know, the ability to work out how the world works and to do things", "tokens": [50836, 550, 562, 309, 1487, 281, 11, 291, 458, 11, 264, 3485, 281, 589, 484, 577, 264, 1002, 1985, 293, 281, 360, 721, 51224], "temperature": 0.0, "avg_logprob": -0.09287899732589722, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.008923170156776905}, {"id": 505, "seek": 287240, "start": 2889.6, "end": 2895.04, "text": " like figure out how to gain access to some item of food that's difficult to get hold of, then all", "tokens": [51224, 411, 2573, 484, 577, 281, 6052, 2105, 281, 512, 3174, 295, 1755, 300, 311, 2252, 281, 483, 1797, 295, 11, 550, 439, 51496], "temperature": 0.0, "avg_logprob": -0.09287899732589722, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.008923170156776905}, {"id": 506, "seek": 287240, "start": 2895.04, "end": 2901.84, "text": " kinds of cognitive capabilities might be required to understand how you get inside a, you know,", "tokens": [51496, 3685, 295, 15605, 10862, 1062, 312, 4739, 281, 1223, 577, 291, 483, 1854, 257, 11, 291, 458, 11, 51836], "temperature": 0.0, "avg_logprob": -0.09287899732589722, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.008923170156776905}, {"id": 507, "seek": 290240, "start": 2902.48, "end": 2909.28, "text": " a shell or something to get the fruit inside it or something like that, complex cognitive", "tokens": [50368, 257, 8720, 420, 746, 281, 483, 264, 6773, 1854, 309, 420, 746, 411, 300, 11, 3997, 15605, 50708], "temperature": 0.0, "avg_logprob": -0.16647916091115852, "compression_ratio": 1.8704453441295548, "no_speech_prob": 0.0033199219033122063}, {"id": 508, "seek": 290240, "start": 2909.28, "end": 2913.76, "text": " abilities, that sort of. And then, you know, evolutionary evolution has developed a lot of", "tokens": [50708, 11582, 11, 300, 1333, 295, 13, 400, 550, 11, 291, 458, 11, 27567, 9303, 575, 4743, 257, 688, 295, 50932], "temperature": 0.0, "avg_logprob": -0.16647916091115852, "compression_ratio": 1.8704453441295548, "no_speech_prob": 0.0033199219033122063}, {"id": 509, "seek": 290240, "start": 2914.32, "end": 2918.64, "text": " more and more and more complex cognitive cognition until we get to language and, you know,", "tokens": [50960, 544, 293, 544, 293, 544, 3997, 15605, 46905, 1826, 321, 483, 281, 2856, 293, 11, 291, 458, 11, 51176], "temperature": 0.0, "avg_logprob": -0.16647916091115852, "compression_ratio": 1.8704453441295548, "no_speech_prob": 0.0033199219033122063}, {"id": 510, "seek": 290240, "start": 2918.64, "end": 2923.36, "text": " we need to interact with each other because that that's all very much a part of it, the social,", "tokens": [51176, 321, 643, 281, 4648, 365, 1184, 661, 570, 300, 300, 311, 439, 588, 709, 257, 644, 295, 309, 11, 264, 2093, 11, 51412], "temperature": 0.0, "avg_logprob": -0.16647916091115852, "compression_ratio": 1.8704453441295548, "no_speech_prob": 0.0033199219033122063}, {"id": 511, "seek": 290240, "start": 2923.36, "end": 2928.32, "text": " social side of it. And then language is part of that. So as I see it, it's all built on top of", "tokens": [51412, 2093, 1252, 295, 309, 13, 400, 550, 2856, 307, 644, 295, 300, 13, 407, 382, 286, 536, 309, 11, 309, 311, 439, 3094, 322, 1192, 295, 51660], "temperature": 0.0, "avg_logprob": -0.16647916091115852, "compression_ratio": 1.8704453441295548, "no_speech_prob": 0.0033199219033122063}, {"id": 512, "seek": 292832, "start": 2929.28, "end": 2934.1600000000003, "text": " this fundamental fact of the embodiment of the animal and the organism. So that's in the", "tokens": [50412, 341, 8088, 1186, 295, 264, 28935, 2328, 295, 264, 5496, 293, 264, 24128, 13, 407, 300, 311, 294, 264, 50656], "temperature": 0.0, "avg_logprob": -0.12325858225864647, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.011968966573476791}, {"id": 513, "seek": 292832, "start": 2934.1600000000003, "end": 2941.28, "text": " biological case. So that's sort of our starting point. So, and so that seems to me to be the,", "tokens": [50656, 13910, 1389, 13, 407, 300, 311, 1333, 295, 527, 2891, 935, 13, 407, 11, 293, 370, 300, 2544, 281, 385, 281, 312, 264, 11, 51012], "temperature": 0.0, "avg_logprob": -0.12325858225864647, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.011968966573476791}, {"id": 514, "seek": 292832, "start": 2941.28, "end": 2946.7200000000003, "text": " the most natural way to, to understand the very nature of intelligence.", "tokens": [51012, 264, 881, 3303, 636, 281, 11, 281, 1223, 264, 588, 3687, 295, 7599, 13, 51284], "temperature": 0.0, "avg_logprob": -0.12325858225864647, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.011968966573476791}, {"id": 515, "seek": 292832, "start": 2947.52, "end": 2950.6400000000003, "text": " Could I frame it? I think I didn't, I didn't frame it very well. I mean, Melanie Mitchell", "tokens": [51324, 7497, 286, 3920, 309, 30, 286, 519, 286, 994, 380, 11, 286, 994, 380, 3920, 309, 588, 731, 13, 286, 914, 11, 42798, 27582, 51480], "temperature": 0.0, "avg_logprob": -0.12325858225864647, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.011968966573476791}, {"id": 516, "seek": 292832, "start": 2950.6400000000003, "end": 2954.6400000000003, "text": " recently had a paper out talking about the Four Misconceptions and one of them, of course,", "tokens": [51480, 3938, 632, 257, 3035, 484, 1417, 466, 264, 7451, 23240, 1671, 22393, 293, 472, 295, 552, 11, 295, 1164, 11, 51680], "temperature": 0.0, "avg_logprob": -0.12325858225864647, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.011968966573476791}, {"id": 517, "seek": 295464, "start": 2954.64, "end": 2959.44, "text": " citing Drew McDermott was the wishful mnemonics and the anthropomorphization which, which you've", "tokens": [50364, 48749, 25550, 49269, 966, 1521, 390, 264, 3172, 906, 275, 25989, 266, 1167, 293, 264, 22727, 32702, 2144, 597, 11, 597, 291, 600, 50604], "temperature": 0.0, "avg_logprob": -0.11010264605283737, "compression_ratio": 1.7024539877300613, "no_speech_prob": 0.00798026379197836}, {"id": 518, "seek": 295464, "start": 2959.44, "end": 2964.16, "text": " basically spoken about. But her fourth one was about embodiment. And she spoke about this in", "tokens": [50604, 1936, 10759, 466, 13, 583, 720, 6409, 472, 390, 466, 28935, 2328, 13, 400, 750, 7179, 466, 341, 294, 50840], "temperature": 0.0, "avg_logprob": -0.11010264605283737, "compression_ratio": 1.7024539877300613, "no_speech_prob": 0.00798026379197836}, {"id": 519, "seek": 295464, "start": 2964.16, "end": 2968.16, "text": " her book as well. She said that one of the misconceptions of AI is that people have this", "tokens": [50840, 720, 1446, 382, 731, 13, 1240, 848, 300, 472, 295, 264, 50012, 295, 7318, 307, 300, 561, 362, 341, 51040], "temperature": 0.0, "avg_logprob": -0.11010264605283737, "compression_ratio": 1.7024539877300613, "no_speech_prob": 0.00798026379197836}, {"id": 520, "seek": 295464, "start": 2968.16, "end": 2972.08, "text": " notion of a pure intelligence, you know, something which works in isolation from the", "tokens": [51040, 10710, 295, 257, 6075, 7599, 11, 291, 458, 11, 746, 597, 1985, 294, 16001, 490, 264, 51236], "temperature": 0.0, "avg_logprob": -0.11010264605283737, "compression_ratio": 1.7024539877300613, "no_speech_prob": 0.00798026379197836}, {"id": 521, "seek": 295464, "start": 2972.08, "end": 2976.72, "text": " environment. And you're talking about social embeddedness and embodiment. And I guess my", "tokens": [51236, 2823, 13, 400, 291, 434, 1417, 466, 2093, 16741, 1287, 293, 28935, 2328, 13, 400, 286, 2041, 452, 51468], "temperature": 0.0, "avg_logprob": -0.11010264605283737, "compression_ratio": 1.7024539877300613, "no_speech_prob": 0.00798026379197836}, {"id": 522, "seek": 295464, "start": 2976.72, "end": 2981.92, "text": " point with the complexity argument is I'm saying that the brain itself doesn't actually do everything.", "tokens": [51468, 935, 365, 264, 14024, 6770, 307, 286, 478, 1566, 300, 264, 3567, 2564, 1177, 380, 767, 360, 1203, 13, 51728], "temperature": 0.0, "avg_logprob": -0.11010264605283737, "compression_ratio": 1.7024539877300613, "no_speech_prob": 0.00798026379197836}, {"id": 523, "seek": 298192, "start": 2982.0, "end": 2987.44, "text": " It kind of works as part of a bigger system. Oh, I see what you mean. Yes. Okay. Yeah. Yeah. So", "tokens": [50368, 467, 733, 295, 1985, 382, 644, 295, 257, 3801, 1185, 13, 876, 11, 286, 536, 437, 291, 914, 13, 1079, 13, 1033, 13, 865, 13, 865, 13, 407, 50640], "temperature": 0.0, "avg_logprob": -0.1600273506981986, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0015128763625398278}, {"id": 524, "seek": 298192, "start": 2987.44, "end": 2994.4, "text": " there's, so of course, there's, I mean, I noticed in one of your previous interviews with Andrew", "tokens": [50640, 456, 311, 11, 370, 295, 1164, 11, 456, 311, 11, 286, 914, 11, 286, 5694, 294, 472, 295, 428, 3894, 12318, 365, 10110, 50988], "temperature": 0.0, "avg_logprob": -0.1600273506981986, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0015128763625398278}, {"id": 525, "seek": 298192, "start": 2994.4, "end": 3000.96, "text": " Lampinen, you mentioned the three E's frame, we're called four E's these days. And of course,", "tokens": [50988, 441, 1215, 5636, 11, 291, 2835, 264, 1045, 462, 311, 3920, 11, 321, 434, 1219, 1451, 462, 311, 613, 1708, 13, 400, 295, 1164, 11, 51316], "temperature": 0.0, "avg_logprob": -0.1600273506981986, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0015128763625398278}, {"id": 526, "seek": 298192, "start": 3000.96, "end": 3005.28, "text": " that's very much part of it is the, is the idea that, you know, there's the extended mind, we use", "tokens": [51316, 300, 311, 588, 709, 644, 295, 309, 307, 264, 11, 307, 264, 1558, 300, 11, 291, 458, 11, 456, 311, 264, 10913, 1575, 11, 321, 764, 51532], "temperature": 0.0, "avg_logprob": -0.1600273506981986, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0015128763625398278}, {"id": 527, "seek": 300528, "start": 3005.28, "end": 3013.6000000000004, "text": " the environment, you know, as, as, as a kind of memory, for example, we deposit things in the", "tokens": [50364, 264, 2823, 11, 291, 458, 11, 382, 11, 382, 11, 382, 257, 733, 295, 4675, 11, 337, 1365, 11, 321, 19107, 721, 294, 264, 50780], "temperature": 0.0, "avg_logprob": -0.13923521496000743, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.014753918163478374}, {"id": 528, "seek": 300528, "start": 3013.6000000000004, "end": 3020.0, "text": " environment, writing, you know, as an example and so on. And then there's, people talk about", "tokens": [50780, 2823, 11, 3579, 11, 291, 458, 11, 382, 364, 1365, 293, 370, 322, 13, 400, 550, 456, 311, 11, 561, 751, 466, 51100], "temperature": 0.0, "avg_logprob": -0.13923521496000743, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.014753918163478374}, {"id": 529, "seek": 300528, "start": 3020.0, "end": 3024.7200000000003, "text": " morphological computation, I'm sure you're familiar with that. Well, so that's the idea", "tokens": [51100, 25778, 4383, 24903, 11, 286, 478, 988, 291, 434, 4963, 365, 300, 13, 1042, 11, 370, 300, 311, 264, 1558, 51336], "temperature": 0.0, "avg_logprob": -0.13923521496000743, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.014753918163478374}, {"id": 530, "seek": 300528, "start": 3024.7200000000003, "end": 3031.0400000000004, "text": " that the very shape of our bodies, you know, is, is, is, you know, could. So, so, so sometimes,", "tokens": [51336, 300, 264, 588, 3909, 295, 527, 7510, 11, 291, 458, 11, 307, 11, 307, 11, 307, 11, 291, 458, 11, 727, 13, 407, 11, 370, 11, 370, 2171, 11, 51652], "temperature": 0.0, "avg_logprob": -0.13923521496000743, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.014753918163478374}, {"id": 531, "seek": 303104, "start": 3031.04, "end": 3038.56, "text": " you know, the aspects of intelligence are actually outshort outsourced into the physical shape of,", "tokens": [50364, 291, 458, 11, 264, 7270, 295, 7599, 366, 767, 484, 2716, 477, 14758, 396, 1232, 666, 264, 4001, 3909, 295, 11, 50740], "temperature": 0.0, "avg_logprob": -0.12584360714616447, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.005751247052103281}, {"id": 532, "seek": 303104, "start": 3038.56, "end": 3044.4, "text": " of your body. So where you might think about designing a robot, where you, where you put a lot", "tokens": [50740, 295, 428, 1772, 13, 407, 689, 291, 1062, 519, 466, 14685, 257, 7881, 11, 689, 291, 11, 689, 291, 829, 257, 688, 51032], "temperature": 0.0, "avg_logprob": -0.12584360714616447, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.005751247052103281}, {"id": 533, "seek": 303104, "start": 3044.4, "end": 3049.04, "text": " of work into the control aspect of it, so that it's, so that it can kind of walk in this very", "tokens": [51032, 295, 589, 666, 264, 1969, 4171, 295, 309, 11, 370, 300, 309, 311, 11, 370, 300, 309, 393, 733, 295, 1792, 294, 341, 588, 51264], "temperature": 0.0, "avg_logprob": -0.12584360714616447, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.005751247052103281}, {"id": 534, "seek": 303104, "start": 3049.04, "end": 3053.84, "text": " carefully engineered ways that it's always permanently stable, or alternatively, you can", "tokens": [51264, 7500, 38648, 2098, 300, 309, 311, 1009, 24042, 8351, 11, 420, 8535, 356, 11, 291, 393, 51504], "temperature": 0.0, "avg_logprob": -0.12584360714616447, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.005751247052103281}, {"id": 535, "seek": 303104, "start": 3053.84, "end": 3060.16, "text": " make a body that is naturally sort of stable, or maybe naturally unstable. And what you do is you", "tokens": [51504, 652, 257, 1772, 300, 307, 8195, 1333, 295, 8351, 11, 420, 1310, 8195, 23742, 13, 400, 437, 291, 360, 307, 291, 51820], "temperature": 0.0, "avg_logprob": -0.12584360714616447, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.005751247052103281}, {"id": 536, "seek": 306016, "start": 3060.16, "end": 3066.08, "text": " kind of rely on the combination of the physics of it constantly falling with, with a control system", "tokens": [50364, 733, 295, 10687, 322, 264, 6562, 295, 264, 10649, 295, 309, 6460, 7440, 365, 11, 365, 257, 1969, 1185, 50660], "temperature": 0.0, "avg_logprob": -0.12855908870697022, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0038351353723555803}, {"id": 537, "seek": 306016, "start": 3066.08, "end": 3072.0, "text": " that constantly restores balance. So, so, so, you know, so that's, that's, I mean, this is very", "tokens": [50660, 300, 6460, 1472, 2706, 4772, 13, 407, 11, 370, 11, 370, 11, 291, 458, 11, 370, 300, 311, 11, 300, 311, 11, 286, 914, 11, 341, 307, 588, 50956], "temperature": 0.0, "avg_logprob": -0.12855908870697022, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0038351353723555803}, {"id": 538, "seek": 306016, "start": 3072.0, "end": 3077.8399999999997, "text": " much a Brooks type perspective, and people picked up on Brooks's ideas and extended them in this", "tokens": [50956, 709, 257, 33493, 2010, 4585, 11, 293, 561, 6183, 493, 322, 33493, 311, 3487, 293, 10913, 552, 294, 341, 51248], "temperature": 0.0, "avg_logprob": -0.12855908870697022, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0038351353723555803}, {"id": 539, "seek": 306016, "start": 3077.8399999999997, "end": 3081.92, "text": " sort of way. So I think that's, I think that's a very natural way of thinking.", "tokens": [51248, 1333, 295, 636, 13, 407, 286, 519, 300, 311, 11, 286, 519, 300, 311, 257, 588, 3303, 636, 295, 1953, 13, 51452], "temperature": 0.0, "avg_logprob": -0.12855908870697022, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0038351353723555803}, {"id": 540, "seek": 306016, "start": 3082.48, "end": 3086.08, "text": " But in a way that this gets to the, to the complexity argument, because I guess what I'm", "tokens": [51480, 583, 294, 257, 636, 300, 341, 2170, 281, 264, 11, 281, 264, 14024, 6770, 11, 570, 286, 2041, 437, 286, 478, 51660], "temperature": 0.0, "avg_logprob": -0.12855908870697022, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0038351353723555803}, {"id": 541, "seek": 308608, "start": 3086.08, "end": 3092.48, "text": " saying is that life is much more brittle than anyone realises. You were just pointed to some", "tokens": [50364, 1566, 307, 300, 993, 307, 709, 544, 49325, 813, 2878, 957, 3598, 13, 509, 645, 445, 10932, 281, 512, 50684], "temperature": 0.0, "avg_logprob": -0.10527638767076575, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.007423529867082834}, {"id": 542, "seek": 308608, "start": 3092.48, "end": 3097.2, "text": " sources of brittleness that most people never would have thought of, which is, which is fascinating.", "tokens": [50684, 7139, 295, 738, 593, 45887, 300, 881, 561, 1128, 576, 362, 1194, 295, 11, 597, 307, 11, 597, 307, 10343, 13, 50920], "temperature": 0.0, "avg_logprob": -0.10527638767076575, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.007423529867082834}, {"id": 543, "seek": 308608, "start": 3097.2, "end": 3103.2799999999997, "text": " So I think there's a very important relationship between embodiment and language. And this also", "tokens": [50920, 407, 286, 519, 456, 311, 257, 588, 1021, 2480, 1296, 28935, 2328, 293, 2856, 13, 400, 341, 611, 51224], "temperature": 0.0, "avg_logprob": -0.10527638767076575, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.007423529867082834}, {"id": 544, "seek": 308608, "start": 3103.2799999999997, "end": 3111.84, "text": " brings us back to Wittgenstein as well. So, so for us as humans, language is inherently an embodied", "tokens": [51224, 5607, 505, 646, 281, 343, 593, 1766, 9089, 382, 731, 13, 407, 11, 370, 337, 505, 382, 6255, 11, 2856, 307, 27993, 364, 42046, 51652], "temperature": 0.0, "avg_logprob": -0.10527638767076575, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.007423529867082834}, {"id": 545, "seek": 311184, "start": 3111.84, "end": 3119.36, "text": " phenomenon. It's, it's, it's something that is, it's a social practice, something that take that", "tokens": [50364, 14029, 13, 467, 311, 11, 309, 311, 11, 309, 311, 746, 300, 307, 11, 309, 311, 257, 2093, 3124, 11, 746, 300, 747, 300, 50740], "temperature": 0.0, "avg_logprob": -0.10477459307798405, "compression_ratio": 1.8605769230769231, "no_speech_prob": 0.030242452397942543}, {"id": 546, "seek": 311184, "start": 3119.36, "end": 3125.52, "text": " it's a phenomenon that occurs in the context of other language users who inhabit the same world", "tokens": [50740, 309, 311, 257, 14029, 300, 11843, 294, 264, 4319, 295, 661, 2856, 5022, 567, 21863, 264, 912, 1002, 51048], "temperature": 0.0, "avg_logprob": -0.10477459307798405, "compression_ratio": 1.8605769230769231, "no_speech_prob": 0.030242452397942543}, {"id": 547, "seek": 311184, "start": 3125.52, "end": 3130.6400000000003, "text": " as we do. And where we have kind of like joint activities, we're triangulating on the same world,", "tokens": [51048, 382, 321, 360, 13, 400, 689, 321, 362, 733, 295, 411, 7225, 5354, 11, 321, 434, 19335, 12162, 322, 264, 912, 1002, 11, 51304], "temperature": 0.0, "avg_logprob": -0.10477459307798405, "compression_ratio": 1.8605769230769231, "no_speech_prob": 0.030242452397942543}, {"id": 548, "seek": 311184, "start": 3130.6400000000003, "end": 3135.52, "text": " and we're acting on the same world together. And that's the that's what we're talking about when", "tokens": [51304, 293, 321, 434, 6577, 322, 264, 912, 1002, 1214, 13, 400, 300, 311, 264, 300, 311, 437, 321, 434, 1417, 466, 562, 51548], "temperature": 0.0, "avg_logprob": -0.10477459307798405, "compression_ratio": 1.8605769230769231, "no_speech_prob": 0.030242452397942543}, {"id": 549, "seek": 313552, "start": 3135.52, "end": 3142.4, "text": " we use language. So there's this, so that, that's an inherently convincing view of language. And I", "tokens": [50364, 321, 764, 2856, 13, 407, 456, 311, 341, 11, 370, 300, 11, 300, 311, 364, 27993, 24823, 1910, 295, 2856, 13, 400, 286, 50708], "temperature": 0.0, "avg_logprob": -0.16944707608690449, "compression_ratio": 1.853080568720379, "no_speech_prob": 0.11151126772165298}, {"id": 550, "seek": 313552, "start": 3142.4, "end": 3148.64, "text": " think it's deeply profoundly correct view of language. That's, that's what, that's what language", "tokens": [50708, 519, 309, 311, 8760, 39954, 3006, 1910, 295, 2856, 13, 663, 311, 11, 300, 311, 437, 11, 300, 311, 437, 2856, 51020], "temperature": 0.0, "avg_logprob": -0.16944707608690449, "compression_ratio": 1.853080568720379, "no_speech_prob": 0.11151126772165298}, {"id": 551, "seek": 313552, "start": 3148.64, "end": 3153.68, "text": " is there for us is so that we can talk about the same things together so that we can, our collective", "tokens": [51020, 307, 456, 337, 505, 307, 370, 300, 321, 393, 751, 466, 264, 912, 721, 1214, 370, 300, 321, 393, 11, 527, 12590, 51272], "temperature": 0.0, "avg_logprob": -0.16944707608690449, "compression_ratio": 1.853080568720379, "no_speech_prob": 0.11151126772165298}, {"id": 552, "seek": 313552, "start": 3153.68, "end": 3162.64, "text": " activity is, is, you know, is, is, is organised to some extent, thanks to language. So that's,", "tokens": [51272, 5191, 307, 11, 307, 11, 291, 458, 11, 307, 11, 307, 11, 307, 36866, 281, 512, 8396, 11, 3231, 281, 2856, 13, 407, 300, 311, 11, 51720], "temperature": 0.0, "avg_logprob": -0.16944707608690449, "compression_ratio": 1.853080568720379, "no_speech_prob": 0.11151126772165298}, {"id": 553, "seek": 316264, "start": 3162.64, "end": 3168.0, "text": " so I think that's a really important perspective on language is Wittgenstein perspective. And, and", "tokens": [50364, 370, 286, 519, 300, 311, 257, 534, 1021, 4585, 322, 2856, 307, 343, 593, 1766, 9089, 4585, 13, 400, 11, 293, 50632], "temperature": 0.0, "avg_logprob": -0.12759977517668733, "compression_ratio": 1.8373205741626795, "no_speech_prob": 0.0018442384898662567}, {"id": 554, "seek": 316264, "start": 3168.0, "end": 3172.8799999999997, "text": " embodiment is at the heart of it, embodiment and inhabiting the same world as our other language", "tokens": [50632, 28935, 2328, 307, 412, 264, 1917, 295, 309, 11, 28935, 2328, 293, 16934, 1748, 264, 912, 1002, 382, 527, 661, 2856, 50876], "temperature": 0.0, "avg_logprob": -0.12759977517668733, "compression_ratio": 1.8373205741626795, "no_speech_prob": 0.0018442384898662567}, {"id": 555, "seek": 316264, "start": 3172.8799999999997, "end": 3178.8799999999997, "text": " users. And, you know, that's the way we learn language, we learn language by being around", "tokens": [50876, 5022, 13, 400, 11, 291, 458, 11, 300, 311, 264, 636, 321, 1466, 2856, 11, 321, 1466, 2856, 538, 885, 926, 51176], "temperature": 0.0, "avg_logprob": -0.12759977517668733, "compression_ratio": 1.8373205741626795, "no_speech_prob": 0.0018442384898662567}, {"id": 556, "seek": 316264, "start": 3178.8799999999997, "end": 3186.48, "text": " other language users like our parents and carers and, and, and peers. And, and that's again a very", "tokens": [51176, 661, 2856, 5022, 411, 527, 3152, 293, 1032, 433, 293, 11, 293, 11, 293, 16739, 13, 400, 11, 293, 300, 311, 797, 257, 588, 51556], "temperature": 0.0, "avg_logprob": -0.12759977517668733, "compression_ratio": 1.8373205741626795, "no_speech_prob": 0.0018442384898662567}, {"id": 557, "seek": 318648, "start": 3186.48, "end": 3193.28, "text": " important aspect of the nature of human language. Now large language models, they learn language in", "tokens": [50364, 1021, 4171, 295, 264, 3687, 295, 1952, 2856, 13, 823, 2416, 2856, 5245, 11, 436, 1466, 2856, 294, 50704], "temperature": 0.0, "avg_logprob": -0.12189037516965705, "compression_ratio": 1.978902953586498, "no_speech_prob": 0.4416765868663788}, {"id": 558, "seek": 318648, "start": 3193.28, "end": 3198.88, "text": " a very different way indeed, they do not inhabit the same world as us, they do not kind of sense", "tokens": [50704, 257, 588, 819, 636, 6451, 11, 436, 360, 406, 21863, 264, 912, 1002, 382, 505, 11, 436, 360, 406, 733, 295, 2020, 50984], "temperature": 0.0, "avg_logprob": -0.12189037516965705, "compression_ratio": 1.978902953586498, "no_speech_prob": 0.4416765868663788}, {"id": 559, "seek": 318648, "start": 3199.44, "end": 3204.64, "text": " the world in the way that we do, they don't learn language from, from other language users,", "tokens": [51012, 264, 1002, 294, 264, 636, 300, 321, 360, 11, 436, 500, 380, 1466, 2856, 490, 11, 490, 661, 2856, 5022, 11, 51272], "temperature": 0.0, "avg_logprob": -0.12189037516965705, "compression_ratio": 1.978902953586498, "no_speech_prob": 0.4416765868663788}, {"id": 560, "seek": 318648, "start": 3204.64, "end": 3209.68, "text": " from their peers in the way that we do. But rather, you know, will we know how large language", "tokens": [51272, 490, 641, 16739, 294, 264, 636, 300, 321, 360, 13, 583, 2831, 11, 291, 458, 11, 486, 321, 458, 577, 2416, 2856, 51524], "temperature": 0.0, "avg_logprob": -0.12189037516965705, "compression_ratio": 1.978902953586498, "no_speech_prob": 0.4416765868663788}, {"id": 561, "seek": 318648, "start": 3210.8, "end": 3215.76, "text": " models work, there's trained on a very, very large corpus of textual, of textual data.", "tokens": [51580, 5245, 589, 11, 456, 311, 8895, 322, 257, 588, 11, 588, 2416, 1181, 31624, 295, 2487, 901, 11, 295, 2487, 901, 1412, 13, 51828], "temperature": 0.0, "avg_logprob": -0.12189037516965705, "compression_ratio": 1.978902953586498, "no_speech_prob": 0.4416765868663788}, {"id": 562, "seek": 321576, "start": 3215.76, "end": 3220.7200000000003, "text": " So an enormous corpus of textual data so bigger than any human is likely to encounter in a, you", "tokens": [50364, 407, 364, 11322, 1181, 31624, 295, 2487, 901, 1412, 370, 3801, 813, 604, 1952, 307, 3700, 281, 8593, 294, 257, 11, 291, 50612], "temperature": 0.0, "avg_logprob": -0.08379244995117187, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.0005107464967295527}, {"id": 563, "seek": 321576, "start": 3220.7200000000003, "end": 3227.92, "text": " know, by the time they become a proficient language user at a young age. And what they're trained to", "tokens": [50612, 458, 11, 538, 264, 565, 436, 1813, 257, 1740, 24549, 2856, 4195, 412, 257, 2037, 3205, 13, 400, 437, 436, 434, 8895, 281, 50972], "temperature": 0.0, "avg_logprob": -0.08379244995117187, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.0005107464967295527}, {"id": 564, "seek": 321576, "start": 3227.92, "end": 3234.7200000000003, "text": " do is, is not to kind of engage in activities with other language users, but to predict what the", "tokens": [50972, 360, 307, 11, 307, 406, 281, 733, 295, 4683, 294, 5354, 365, 661, 2856, 5022, 11, 457, 281, 6069, 437, 264, 51312], "temperature": 0.0, "avg_logprob": -0.08379244995117187, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.0005107464967295527}, {"id": 565, "seek": 321576, "start": 3234.7200000000003, "end": 3239.5200000000004, "text": " next, you know, what the next token is going to be, which is a very, very different sort of thing.", "tokens": [51312, 958, 11, 291, 458, 11, 437, 264, 958, 14862, 307, 516, 281, 312, 11, 597, 307, 257, 588, 11, 588, 819, 1333, 295, 551, 13, 51552], "temperature": 0.0, "avg_logprob": -0.08379244995117187, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.0005107464967295527}, {"id": 566, "seek": 321576, "start": 3239.5200000000004, "end": 3244.1600000000003, "text": " So they're very, very different sorts of things. And the, and the role of embodiment is really", "tokens": [51552, 407, 436, 434, 588, 11, 588, 819, 7527, 295, 721, 13, 400, 264, 11, 293, 264, 3090, 295, 28935, 2328, 307, 534, 51784], "temperature": 0.0, "avg_logprob": -0.08379244995117187, "compression_ratio": 1.8104089219330854, "no_speech_prob": 0.0005107464967295527}, {"id": 567, "seek": 324416, "start": 3244.24, "end": 3250.48, "text": " really important in this difference, I think. Yes, absolutely. When I spoke with Andrew Lampinen,", "tokens": [50368, 534, 1021, 294, 341, 2649, 11, 286, 519, 13, 1079, 11, 3122, 13, 1133, 286, 7179, 365, 10110, 441, 1215, 5636, 11, 50680], "temperature": 0.0, "avg_logprob": -0.1700631595048748, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.03731166943907738}, {"id": 568, "seek": 324416, "start": 3250.48, "end": 3253.8399999999997, "text": " he's really, really interested in the grounding problems. I mean, would you mind just speaking", "tokens": [50680, 415, 311, 534, 11, 534, 3102, 294, 264, 46727, 2740, 13, 286, 914, 11, 576, 291, 1575, 445, 4124, 50848], "temperature": 0.0, "avg_logprob": -0.1700631595048748, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.03731166943907738}, {"id": 569, "seek": 324416, "start": 3253.8399999999997, "end": 3258.72, "text": " about that a little bit before we go into your paper? Yeah, absolutely. Yeah, yeah. So of course,", "tokens": [50848, 466, 300, 257, 707, 857, 949, 321, 352, 666, 428, 3035, 30, 865, 11, 3122, 13, 865, 11, 1338, 13, 407, 295, 1164, 11, 51092], "temperature": 0.0, "avg_logprob": -0.1700631595048748, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.03731166943907738}, {"id": 570, "seek": 324416, "start": 3258.72, "end": 3265.7599999999998, "text": " this goes back to a great paper by Stephen Harnad back in, I think, 1999 or 1998.", "tokens": [51092, 341, 1709, 646, 281, 257, 869, 3035, 538, 13391, 389, 1083, 345, 646, 294, 11, 286, 519, 11, 19952, 420, 21404, 13, 51444], "temperature": 0.0, "avg_logprob": -0.1700631595048748, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.03731166943907738}, {"id": 571, "seek": 324416, "start": 3265.7599999999998, "end": 3271.2, "text": " Yeah, the one and only. Yeah, the one and only on the symbol grounding problem, it was called.", "tokens": [51444, 865, 11, 264, 472, 293, 787, 13, 865, 11, 264, 472, 293, 787, 322, 264, 5986, 46727, 1154, 11, 309, 390, 1219, 13, 51716], "temperature": 0.0, "avg_logprob": -0.1700631595048748, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.03731166943907738}, {"id": 572, "seek": 327120, "start": 3271.2, "end": 3280.64, "text": " And, and, and, you know, he does argue broadly that the symbols in AI systems,", "tokens": [50364, 400, 11, 293, 11, 293, 11, 291, 458, 11, 415, 775, 9695, 19511, 300, 264, 16944, 294, 7318, 3652, 11, 50836], "temperature": 0.0, "avg_logprob": -0.11527527130401886, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.0010351903038099408}, {"id": 573, "seek": 327120, "start": 3282.16, "end": 3285.12, "text": " the kinds of AI systems he was thinking about at the time were kind of, you know,", "tokens": [50912, 264, 3685, 295, 7318, 3652, 415, 390, 1953, 466, 412, 264, 565, 645, 733, 295, 11, 291, 458, 11, 51060], "temperature": 0.0, "avg_logprob": -0.11527527130401886, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.0010351903038099408}, {"id": 574, "seek": 327120, "start": 3285.12, "end": 3289.04, "text": " sort of expert systems say or something like that. And the symbols there are not grounded,", "tokens": [51060, 1333, 295, 5844, 3652, 584, 420, 746, 411, 300, 13, 400, 264, 16944, 456, 366, 406, 23535, 11, 51256], "temperature": 0.0, "avg_logprob": -0.11527527130401886, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.0010351903038099408}, {"id": 575, "seek": 327120, "start": 3289.04, "end": 3293.68, "text": " they're provided by the human programmers and they're just sort of typed in. Whereas for us,", "tokens": [51256, 436, 434, 5649, 538, 264, 1952, 41504, 293, 436, 434, 445, 1333, 295, 33941, 294, 13, 13813, 337, 505, 11, 51488], "temperature": 0.0, "avg_logprob": -0.11527527130401886, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.0010351903038099408}, {"id": 576, "seek": 327120, "start": 3293.68, "end": 3299.52, "text": " for us, the words we use, those symbols are grounded in, in our activity in the world. So", "tokens": [51488, 337, 505, 11, 264, 2283, 321, 764, 11, 729, 16944, 366, 23535, 294, 11, 294, 527, 5191, 294, 264, 1002, 13, 407, 51780], "temperature": 0.0, "avg_logprob": -0.11527527130401886, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.0010351903038099408}, {"id": 577, "seek": 329952, "start": 3299.52, "end": 3305.44, "text": " that when we use the word dog, that's because we've seen dogs. And we've talked about dogs with", "tokens": [50364, 300, 562, 321, 764, 264, 1349, 3000, 11, 300, 311, 570, 321, 600, 1612, 7197, 13, 400, 321, 600, 2825, 466, 7197, 365, 50660], "temperature": 0.0, "avg_logprob": -0.11579279395622935, "compression_ratio": 1.9308943089430894, "no_speech_prob": 0.002568610943853855}, {"id": 578, "seek": 329952, "start": 3305.44, "end": 3309.2, "text": " other people who've also seen dogs. And we've seen dogs in lots of different circumstances. And", "tokens": [50660, 661, 561, 567, 600, 611, 1612, 7197, 13, 400, 321, 600, 1612, 7197, 294, 3195, 295, 819, 9121, 13, 400, 50848], "temperature": 0.0, "avg_logprob": -0.11579279395622935, "compression_ratio": 1.9308943089430894, "no_speech_prob": 0.002568610943853855}, {"id": 579, "seek": 329952, "start": 3309.2, "end": 3316.4, "text": " we've also seen cats and, and, and, and, and dog bowls and bones and many other things that all", "tokens": [50848, 321, 600, 611, 1612, 11111, 293, 11, 293, 11, 293, 11, 293, 11, 293, 3000, 28513, 293, 10491, 293, 867, 661, 721, 300, 439, 51208], "temperature": 0.0, "avg_logprob": -0.11579279395622935, "compression_ratio": 1.9308943089430894, "no_speech_prob": 0.002568610943853855}, {"id": 580, "seek": 329952, "start": 3316.4, "end": 3321.7599999999998, "text": " kind of contextualize that. But all of that, that that is kind of grounded in the real world in", "tokens": [51208, 733, 295, 35526, 1125, 300, 13, 583, 439, 295, 300, 11, 300, 300, 307, 733, 295, 23535, 294, 264, 957, 1002, 294, 51476], "temperature": 0.0, "avg_logprob": -0.11579279395622935, "compression_ratio": 1.9308943089430894, "no_speech_prob": 0.002568610943853855}, {"id": 581, "seek": 329952, "start": 3321.7599999999998, "end": 3327.36, "text": " our and in our perception of the things in question. So that so that's this. So that's what", "tokens": [51476, 527, 293, 294, 527, 12860, 295, 264, 721, 294, 1168, 13, 407, 300, 370, 300, 311, 341, 13, 407, 300, 311, 437, 51756], "temperature": 0.0, "avg_logprob": -0.11579279395622935, "compression_ratio": 1.9308943089430894, "no_speech_prob": 0.002568610943853855}, {"id": 582, "seek": 332736, "start": 3327.36, "end": 3332.48, "text": " sort of is meant by grounding or that at least that's the original meaning of the word grounding", "tokens": [50364, 1333, 295, 307, 4140, 538, 46727, 420, 300, 412, 1935, 300, 311, 264, 3380, 3620, 295, 264, 1349, 46727, 50620], "temperature": 0.0, "avg_logprob": -0.10915767812283239, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.0035741906613111496}, {"id": 583, "seek": 332736, "start": 3332.48, "end": 3337.92, "text": " from Stephen Harled's paper. And I think that's a really, really important concept because,", "tokens": [50620, 490, 13391, 3653, 1493, 311, 3035, 13, 400, 286, 519, 300, 311, 257, 534, 11, 534, 1021, 3410, 570, 11, 50892], "temperature": 0.0, "avg_logprob": -0.10915767812283239, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.0035741906613111496}, {"id": 584, "seek": 332736, "start": 3338.88, "end": 3344.8, "text": " because, you know, in an important sense, large language models, the symbols that are used in", "tokens": [50940, 570, 11, 291, 458, 11, 294, 364, 1021, 2020, 11, 2416, 2856, 5245, 11, 264, 16944, 300, 366, 1143, 294, 51236], "temperature": 0.0, "avg_logprob": -0.10915767812283239, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.0035741906613111496}, {"id": 585, "seek": 332736, "start": 3344.8, "end": 3350.48, "text": " large language models are not really grounded in that kind of way. Now this, you know, I should", "tokens": [51236, 2416, 2856, 5245, 366, 406, 534, 23535, 294, 300, 733, 295, 636, 13, 823, 341, 11, 291, 458, 11, 286, 820, 51520], "temperature": 0.0, "avg_logprob": -0.10915767812283239, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.0035741906613111496}, {"id": 586, "seek": 332736, "start": 3351.1200000000003, "end": 3356.6400000000003, "text": " be absolutely clear that large language models are immensely powerful and immensely useful and,", "tokens": [51552, 312, 3122, 1850, 300, 2416, 2856, 5245, 366, 38674, 4005, 293, 38674, 4420, 293, 11, 51828], "temperature": 0.0, "avg_logprob": -0.10915767812283239, "compression_ratio": 1.9036144578313252, "no_speech_prob": 0.0035741906613111496}, {"id": 587, "seek": 335664, "start": 3356.7999999999997, "end": 3362.24, "text": " and so that, you know, so, but it's interesting that to what extent the lack of grounding here", "tokens": [50372, 293, 370, 300, 11, 291, 458, 11, 370, 11, 457, 309, 311, 1880, 300, 281, 437, 8396, 264, 5011, 295, 46727, 510, 50644], "temperature": 0.0, "avg_logprob": -0.12372450735054764, "compression_ratio": 1.784037558685446, "no_speech_prob": 0.0003447931376285851}, {"id": 588, "seek": 335664, "start": 3362.96, "end": 3369.2, "text": " that we have in today's large language models, you know, might affect how good they are. So,", "tokens": [50680, 300, 321, 362, 294, 965, 311, 2416, 2856, 5245, 11, 291, 458, 11, 1062, 3345, 577, 665, 436, 366, 13, 407, 11, 50992], "temperature": 0.0, "avg_logprob": -0.12372450735054764, "compression_ratio": 1.784037558685446, "no_speech_prob": 0.0003447931376285851}, {"id": 589, "seek": 335664, "start": 3369.7599999999998, "end": 3376.56, "text": " so they, so they are prone to kind of, you know, hallucinating and, and, and, and confabulating", "tokens": [51020, 370, 436, 11, 370, 436, 366, 25806, 281, 733, 295, 11, 291, 458, 11, 35212, 8205, 293, 11, 293, 11, 293, 11, 293, 1497, 455, 12162, 51360], "temperature": 0.0, "avg_logprob": -0.12372450735054764, "compression_ratio": 1.784037558685446, "no_speech_prob": 0.0003447931376285851}, {"id": 590, "seek": 335664, "start": 3376.56, "end": 3383.04, "text": " and, and if you look at multimodal language models that maybe we'll talk about an image that you", "tokens": [51360, 293, 11, 293, 498, 291, 574, 412, 32972, 378, 304, 2856, 5245, 300, 1310, 321, 603, 751, 466, 364, 3256, 300, 291, 51684], "temperature": 0.0, "avg_logprob": -0.12372450735054764, "compression_ratio": 1.784037558685446, "no_speech_prob": 0.0003447931376285851}, {"id": 591, "seek": 338304, "start": 3383.04, "end": 3387.84, "text": " present to them, then, you know, they, you can have a very interesting conversation, but sometimes", "tokens": [50364, 1974, 281, 552, 11, 550, 11, 291, 458, 11, 436, 11, 291, 393, 362, 257, 588, 1880, 3761, 11, 457, 2171, 50604], "temperature": 0.0, "avg_logprob": -0.12936817932128905, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.029146013781428337}, {"id": 592, "seek": 338304, "start": 3387.84, "end": 3392.64, "text": " they'll go off pieced and start talking about things that are not in the image at all and as", "tokens": [50604, 436, 603, 352, 766, 1730, 1232, 293, 722, 1417, 466, 721, 300, 366, 406, 294, 264, 3256, 412, 439, 293, 382, 50844], "temperature": 0.0, "avg_logprob": -0.12936817932128905, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.029146013781428337}, {"id": 593, "seek": 338304, "start": 3392.64, "end": 3399.7599999999998, "text": " if they are. And that's sort of because due to a kind of, I would say lack of grounding this so that", "tokens": [50844, 498, 436, 366, 13, 400, 300, 311, 1333, 295, 570, 3462, 281, 257, 733, 295, 11, 286, 576, 584, 5011, 295, 46727, 341, 370, 300, 51200], "temperature": 0.0, "avg_logprob": -0.12936817932128905, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.029146013781428337}, {"id": 594, "seek": 338304, "start": 3399.7599999999998, "end": 3405.44, "text": " so the words are not kind of grounded in the images in, in quite the way that we would like. So", "tokens": [51200, 370, 264, 2283, 366, 406, 733, 295, 23535, 294, 264, 5267, 294, 11, 294, 1596, 264, 636, 300, 321, 576, 411, 13, 407, 51484], "temperature": 0.0, "avg_logprob": -0.12936817932128905, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.029146013781428337}, {"id": 595, "seek": 338304, "start": 3405.44, "end": 3409.36, "text": " that's, it's an important topic of research, I think. Yes, indeed. And although some people do", "tokens": [51484, 300, 311, 11, 309, 311, 364, 1021, 4829, 295, 2132, 11, 286, 519, 13, 1079, 11, 6451, 13, 400, 4878, 512, 561, 360, 51680], "temperature": 0.0, "avg_logprob": -0.12936817932128905, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.029146013781428337}, {"id": 596, "seek": 340936, "start": 3409.36, "end": 3413.52, "text": " believe there's this magical word called emergence and possibly some emergent symbol", "tokens": [50364, 1697, 456, 311, 341, 12066, 1349, 1219, 36211, 293, 6264, 512, 4345, 6930, 5986, 50572], "temperature": 0.0, "avg_logprob": -0.14837557983398436, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.017352791503071785}, {"id": 597, "seek": 340936, "start": 3413.52, "end": 3417.28, "text": " grounding might be possible, maybe, maybe, shall we just put that to bed before we introduce", "tokens": [50572, 46727, 1062, 312, 1944, 11, 1310, 11, 1310, 11, 4393, 321, 445, 829, 300, 281, 2901, 949, 321, 5366, 50760], "temperature": 0.0, "avg_logprob": -0.14837557983398436, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.017352791503071785}, {"id": 598, "seek": 340936, "start": 3417.28, "end": 3423.28, "text": " your, yeah, sure. Well, well, I mean, emergence is, is, I think is, is a really important concept.", "tokens": [50760, 428, 11, 1338, 11, 988, 13, 1042, 11, 731, 11, 286, 914, 11, 36211, 307, 11, 307, 11, 286, 519, 307, 11, 307, 257, 534, 1021, 3410, 13, 51060], "temperature": 0.0, "avg_logprob": -0.14837557983398436, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.017352791503071785}, {"id": 599, "seek": 340936, "start": 3423.28, "end": 3430.4, "text": " And I, and I think, you know, we do see a tremendous amount of, of very powerful emergence, I think", "tokens": [51060, 400, 286, 11, 293, 286, 519, 11, 291, 458, 11, 321, 360, 536, 257, 10048, 2372, 295, 11, 295, 588, 4005, 36211, 11, 286, 519, 51416], "temperature": 0.0, "avg_logprob": -0.14837557983398436, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.017352791503071785}, {"id": 600, "seek": 340936, "start": 3430.4, "end": 3436.48, "text": " in today's large, large language models. So, so, so, you know, even though they're, they're, so", "tokens": [51416, 294, 965, 311, 2416, 11, 2416, 2856, 5245, 13, 407, 11, 370, 11, 370, 11, 291, 458, 11, 754, 1673, 436, 434, 11, 436, 434, 11, 370, 51720], "temperature": 0.0, "avg_logprob": -0.14837557983398436, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.017352791503071785}, {"id": 601, "seek": 343648, "start": 3436.56, "end": 3442.0, "text": " they're basically trained to do next word prediction. Or I mean, to be clear, I suppose I", "tokens": [50368, 436, 434, 1936, 8895, 281, 360, 958, 1349, 17630, 13, 1610, 286, 914, 11, 281, 312, 1850, 11, 286, 7297, 286, 50640], "temperature": 0.0, "avg_logprob": -0.1334249742569462, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0015177635941654444}, {"id": 602, "seek": 343648, "start": 3442.0, "end": 3445.76, "text": " should have made this maybe a bit clearer in the paper, but of course, it's not always next word", "tokens": [50640, 820, 362, 1027, 341, 1310, 257, 857, 26131, 294, 264, 3035, 11, 457, 295, 1164, 11, 309, 311, 406, 1009, 958, 1349, 50828], "temperature": 0.0, "avg_logprob": -0.1334249742569462, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0015177635941654444}, {"id": 603, "seek": 343648, "start": 3445.76, "end": 3450.16, "text": " prediction, because there are different models learned to actually, you know, predict what's", "tokens": [50828, 17630, 11, 570, 456, 366, 819, 5245, 3264, 281, 767, 11, 291, 458, 11, 6069, 437, 311, 51048], "temperature": 0.0, "avg_logprob": -0.1334249742569462, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0015177635941654444}, {"id": 604, "seek": 343648, "start": 3450.16, "end": 3455.12, "text": " in the middle of a, of a sequence rather than kind of generally, you know, they're interested in,", "tokens": [51048, 294, 264, 2808, 295, 257, 11, 295, 257, 8310, 2831, 813, 733, 295, 5101, 11, 291, 458, 11, 436, 434, 3102, 294, 11, 51296], "temperature": 0.0, "avg_logprob": -0.1334249742569462, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0015177635941654444}, {"id": 605, "seek": 343648, "start": 3455.12, "end": 3462.08, "text": " in, in, in, in, let's take the next word prediction case as canonical. So, so they're, so they're,", "tokens": [51296, 294, 11, 294, 11, 294, 11, 294, 11, 718, 311, 747, 264, 958, 1349, 17630, 1389, 382, 46491, 13, 407, 11, 370, 436, 434, 11, 370, 436, 434, 11, 51644], "temperature": 0.0, "avg_logprob": -0.1334249742569462, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0015177635941654444}, {"id": 606, "seek": 346208, "start": 3462.08, "end": 3466.56, "text": " so they're trained to just to do next word prediction. Now, the astonishing thing is,", "tokens": [50364, 370, 436, 434, 8895, 281, 445, 281, 360, 958, 1349, 17630, 13, 823, 11, 264, 35264, 551, 307, 11, 50588], "temperature": 0.0, "avg_logprob": -0.09806627697414821, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.004101107362657785}, {"id": 607, "seek": 346208, "start": 3466.56, "end": 3473.92, "text": " as I think GPT three showed us, is that, is that just that capability, if it's sufficiently powerful,", "tokens": [50588, 382, 286, 519, 26039, 51, 1045, 4712, 505, 11, 307, 300, 11, 307, 300, 445, 300, 13759, 11, 498, 309, 311, 31868, 4005, 11, 50956], "temperature": 0.0, "avg_logprob": -0.09806627697414821, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.004101107362657785}, {"id": 608, "seek": 346208, "start": 3474.56, "end": 3481.52, "text": " can be used to do all sorts of extraordinary things. Because if you provide, you know,", "tokens": [50988, 393, 312, 1143, 281, 360, 439, 7527, 295, 10581, 721, 13, 1436, 498, 291, 2893, 11, 291, 458, 11, 51336], "temperature": 0.0, "avg_logprob": -0.09806627697414821, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.004101107362657785}, {"id": 609, "seek": 346208, "start": 3481.52, "end": 3487.2799999999997, "text": " the prompt that describes, you know, describes some kind of complicated thing, you know,", "tokens": [51336, 264, 12391, 300, 15626, 11, 291, 458, 11, 15626, 512, 733, 295, 6179, 551, 11, 291, 458, 11, 51624], "temperature": 0.0, "avg_logprob": -0.09806627697414821, "compression_ratio": 1.6425339366515836, "no_speech_prob": 0.004101107362657785}, {"id": 610, "seek": 348728, "start": 3487.28, "end": 3495.6800000000003, "text": " situation, like, you know, I need to tile my floor and my floor is shaped like an L and it's", "tokens": [50364, 2590, 11, 411, 11, 291, 458, 11, 286, 643, 281, 20590, 452, 4123, 293, 452, 4123, 307, 13475, 411, 364, 441, 293, 309, 311, 50784], "temperature": 0.0, "avg_logprob": -0.145307522553664, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.027372047305107117}, {"id": 611, "seek": 348728, "start": 3495.6800000000003, "end": 3500.32, "text": " 20 meters long and three meters. Well, you know, you start to describe this thing, you know, and", "tokens": [50784, 945, 8146, 938, 293, 1045, 8146, 13, 1042, 11, 291, 458, 11, 291, 722, 281, 6786, 341, 551, 11, 291, 458, 11, 293, 51016], "temperature": 0.0, "avg_logprob": -0.145307522553664, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.027372047305107117}, {"id": 612, "seek": 348728, "start": 3500.32, "end": 3506.88, "text": " each, each tile is, is 20 centimeters square, how many tiles will I need? And, and some large", "tokens": [51016, 1184, 11, 1184, 20590, 307, 11, 307, 945, 23300, 3732, 11, 577, 867, 21982, 486, 286, 643, 30, 400, 11, 293, 512, 2416, 51344], "temperature": 0.0, "avg_logprob": -0.145307522553664, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.027372047305107117}, {"id": 613, "seek": 348728, "start": 3506.88, "end": 3512.7200000000003, "text": " language model will come back and tell you, you need 426 tiles, whatever. And it's correct, right?", "tokens": [51344, 2856, 2316, 486, 808, 646, 293, 980, 291, 11, 291, 643, 1017, 10880, 21982, 11, 2035, 13, 400, 309, 311, 3006, 11, 558, 30, 51636], "temperature": 0.0, "avg_logprob": -0.145307522553664, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.027372047305107117}, {"id": 614, "seek": 351272, "start": 3512.72, "end": 3518.56, "text": " Well, this is astonishing, because it was only trained to do next word prediction. And so there's", "tokens": [50364, 1042, 11, 341, 307, 35264, 11, 570, 309, 390, 787, 8895, 281, 360, 958, 1349, 17630, 13, 400, 370, 456, 311, 50656], "temperature": 0.0, "avg_logprob": -0.10213545535473113, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0031586731784045696}, {"id": 615, "seek": 351272, "start": 3518.56, "end": 3523.68, "text": " a kind of emergent capability there. Now, there's a sense, of course, in which it still is just doing", "tokens": [50656, 257, 733, 295, 4345, 6930, 13759, 456, 13, 823, 11, 456, 311, 257, 2020, 11, 295, 1164, 11, 294, 597, 309, 920, 307, 445, 884, 50912], "temperature": 0.0, "avg_logprob": -0.10213545535473113, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0031586731784045696}, {"id": 616, "seek": 351272, "start": 3523.68, "end": 3530.7999999999997, "text": " next word prediction, because in the vast and immensely complex distribution of tokens in human", "tokens": [50912, 958, 1349, 17630, 11, 570, 294, 264, 8369, 293, 38674, 3997, 7316, 295, 22667, 294, 1952, 51268], "temperature": 0.0, "avg_logprob": -0.10213545535473113, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0031586731784045696}, {"id": 617, "seek": 351272, "start": 3531.3599999999997, "end": 3539.4399999999996, "text": " text that's out there, then the correct answer is actually the thing that's most likely to come up.", "tokens": [51296, 2487, 300, 311, 484, 456, 11, 550, 264, 3006, 1867, 307, 767, 264, 551, 300, 311, 881, 3700, 281, 808, 493, 13, 51700], "temperature": 0.0, "avg_logprob": -0.10213545535473113, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0031586731784045696}, {"id": 618, "seek": 353944, "start": 3539.52, "end": 3544.48, "text": " And that's, but it's got to discover a mechanism for producing that, right? And so that is where", "tokens": [50368, 400, 300, 311, 11, 457, 309, 311, 658, 281, 4411, 257, 7513, 337, 10501, 300, 11, 558, 30, 400, 370, 300, 307, 689, 50616], "temperature": 0.0, "avg_logprob": -0.12483997218656225, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.0007007502717897296}, {"id": 619, "seek": 353944, "start": 3544.48, "end": 3549.84, "text": " the emergence comes in. And I think that, you know, these capacities are astonishing, the fact", "tokens": [50616, 264, 36211, 1487, 294, 13, 400, 286, 519, 300, 11, 291, 458, 11, 613, 39396, 366, 35264, 11, 264, 1186, 50884], "temperature": 0.0, "avg_logprob": -0.12483997218656225, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.0007007502717897296}, {"id": 620, "seek": 353944, "start": 3549.84, "end": 3554.7200000000003, "text": " that they, that it can discover mechanisms, you know, emergently that will do that sort of thing.", "tokens": [50884, 300, 436, 11, 300, 309, 393, 4411, 15902, 11, 291, 458, 11, 4345, 70, 2276, 300, 486, 360, 300, 1333, 295, 551, 13, 51128], "temperature": 0.0, "avg_logprob": -0.12483997218656225, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.0007007502717897296}, {"id": 621, "seek": 353944, "start": 3554.7200000000003, "end": 3558.56, "text": " Yes. And maybe I shouldn't have used the word magic with it with emergence, I'm a huge fan", "tokens": [51128, 1079, 13, 400, 1310, 286, 4659, 380, 362, 1143, 264, 1349, 5585, 365, 309, 365, 36211, 11, 286, 478, 257, 2603, 3429, 51320], "temperature": 0.0, "avg_logprob": -0.12483997218656225, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.0007007502717897296}, {"id": 622, "seek": 353944, "start": 3558.56, "end": 3563.52, "text": " of emergence. And, and as you say, the decode is trained to predict the next token or the", "tokens": [51320, 295, 36211, 13, 400, 11, 293, 382, 291, 584, 11, 264, 979, 1429, 307, 8895, 281, 6069, 264, 958, 14862, 420, 264, 51568], "temperature": 0.0, "avg_logprob": -0.12483997218656225, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.0007007502717897296}, {"id": 623, "seek": 353944, "start": 3563.52, "end": 3568.64, "text": " denoising autoencoders to, to, let's say fill in the gaps in the middle. And I guess there are", "tokens": [51568, 1441, 78, 3436, 8399, 22660, 378, 433, 281, 11, 281, 11, 718, 311, 584, 2836, 294, 264, 15031, 294, 264, 2808, 13, 400, 286, 2041, 456, 366, 51824], "temperature": 0.0, "avg_logprob": -0.12483997218656225, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.0007007502717897296}, {"id": 624, "seek": 356864, "start": 3568.64, "end": 3572.48, "text": " different ways of thinking about emergence. So there's weak emergence, which might be thought as", "tokens": [50364, 819, 2098, 295, 1953, 466, 36211, 13, 407, 456, 311, 5336, 36211, 11, 597, 1062, 312, 1194, 382, 50556], "temperature": 0.0, "avg_logprob": -0.170892386600889, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0034855108242481947}, {"id": 625, "seek": 356864, "start": 3573.3599999999997, "end": 3578.56, "text": " computational irreducibility, or a surprising macroscopic change, or strong emergence when", "tokens": [50600, 28270, 16014, 769, 537, 39802, 11, 420, 257, 8830, 7912, 38006, 299, 1319, 11, 420, 2068, 36211, 562, 50860], "temperature": 0.0, "avg_logprob": -0.170892386600889, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0034855108242481947}, {"id": 626, "seek": 356864, "start": 3578.56, "end": 3581.52, "text": " it's not directly deducible from truths and the lower level to make, you know, lots of things.", "tokens": [50860, 309, 311, 406, 3838, 4172, 84, 32128, 490, 30079, 293, 264, 3126, 1496, 281, 652, 11, 291, 458, 11, 3195, 295, 721, 13, 51008], "temperature": 0.0, "avg_logprob": -0.170892386600889, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0034855108242481947}, {"id": 627, "seek": 356864, "start": 3581.52, "end": 3586.96, "text": " Yeah, yeah, the different senses of it, yeah. Exactly. But I guess my point is that remarkably,", "tokens": [51008, 865, 11, 1338, 11, 264, 819, 17057, 295, 309, 11, 1338, 13, 7587, 13, 583, 286, 2041, 452, 935, 307, 300, 37381, 11, 51280], "temperature": 0.0, "avg_logprob": -0.170892386600889, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0034855108242481947}, {"id": 628, "seek": 356864, "start": 3587.6, "end": 3592.3199999999997, "text": " it's trained on something quite trivial. So all of this is about convention, right? All of this is", "tokens": [51312, 309, 311, 8895, 322, 746, 1596, 26703, 13, 407, 439, 295, 341, 307, 466, 10286, 11, 558, 30, 1057, 295, 341, 307, 51548], "temperature": 0.0, "avg_logprob": -0.170892386600889, "compression_ratio": 1.6505190311418685, "no_speech_prob": 0.0034855108242481947}, {"id": 629, "seek": 359232, "start": 3592.32, "end": 3597.44, "text": " about what's, what, what is the, what is a good way to use words, right? So I don't, so I don't", "tokens": [50364, 466, 437, 311, 11, 437, 11, 437, 307, 264, 11, 437, 307, 257, 665, 636, 281, 764, 2283, 11, 558, 30, 407, 286, 500, 380, 11, 370, 286, 500, 380, 50620], "temperature": 0.0, "avg_logprob": -0.09438420820605847, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.10814218968153}, {"id": 630, "seek": 359232, "start": 3597.44, "end": 3602.88, "text": " think, you know, I'm not making metaphysical claims about, about, about these things. So it's", "tokens": [50620, 519, 11, 291, 458, 11, 286, 478, 406, 1455, 30946, 36280, 9441, 466, 11, 466, 11, 466, 613, 721, 13, 407, 309, 311, 50892], "temperature": 0.0, "avg_logprob": -0.09438420820605847, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.10814218968153}, {"id": 631, "seek": 359232, "start": 3602.88, "end": 3608.1600000000003, "text": " all about what, you know, when is it appropriate to use words, to use certain words? And, and", "tokens": [50892, 439, 466, 437, 11, 291, 458, 11, 562, 307, 309, 6854, 281, 764, 2283, 11, 281, 764, 1629, 2283, 30, 400, 11, 293, 51156], "temperature": 0.0, "avg_logprob": -0.09438420820605847, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.10814218968153}, {"id": 632, "seek": 359232, "start": 3608.1600000000003, "end": 3612.0, "text": " because when, when this becomes problematic is when they're philosophically difficult words,", "tokens": [51156, 570, 562, 11, 562, 341, 3643, 19011, 307, 562, 436, 434, 14529, 984, 2252, 2283, 11, 51348], "temperature": 0.0, "avg_logprob": -0.09438420820605847, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.10814218968153}, {"id": 633, "seek": 359232, "start": 3612.0, "end": 3620.0, "text": " like beliefs and thinks and so on. Now, when it comes to reasoning, so, so I do think that we,", "tokens": [51348, 411, 13585, 293, 7309, 293, 370, 322, 13, 823, 11, 562, 309, 1487, 281, 21577, 11, 370, 11, 370, 286, 360, 519, 300, 321, 11, 51748], "temperature": 0.0, "avg_logprob": -0.09438420820605847, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.10814218968153}, {"id": 634, "seek": 362000, "start": 3620.0, "end": 3625.76, "text": " I do think it's not unreasonable to, to, to use that term to describe what some of the,", "tokens": [50364, 286, 360, 519, 309, 311, 406, 41730, 281, 11, 281, 11, 281, 764, 300, 1433, 281, 6786, 437, 512, 295, 264, 11, 50652], "temperature": 0.0, "avg_logprob": -0.09262804465718788, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.0023452439345419407}, {"id": 635, "seek": 362000, "start": 3625.76, "end": 3630.64, "text": " these models do today. And that's partly because of the content neutrality of,", "tokens": [50652, 613, 5245, 360, 965, 13, 400, 300, 311, 17031, 570, 295, 264, 2701, 39913, 1860, 295, 11, 50896], "temperature": 0.0, "avg_logprob": -0.09262804465718788, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.0023452439345419407}, {"id": 636, "seek": 362000, "start": 3631.6, "end": 3637.36, "text": " of, of reasoning. So, so, so, so a lot of the argument, or a lot of the discussion in the", "tokens": [50944, 295, 11, 295, 21577, 13, 407, 11, 370, 11, 370, 11, 370, 257, 688, 295, 264, 6770, 11, 420, 257, 688, 295, 264, 5017, 294, 264, 51232], "temperature": 0.0, "avg_logprob": -0.09262804465718788, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.0023452439345419407}, {"id": 637, "seek": 362000, "start": 3637.36, "end": 3643.44, "text": " paper comes back to this sort of whole embodiment thing, really. And, and I'm, I'm saying, well,", "tokens": [51232, 3035, 1487, 646, 281, 341, 1333, 295, 1379, 28935, 2328, 551, 11, 534, 13, 400, 11, 293, 286, 478, 11, 286, 478, 1566, 11, 731, 11, 51536], "temperature": 0.0, "avg_logprob": -0.09262804465718788, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.0023452439345419407}, {"id": 638, "seek": 364344, "start": 3643.52, "end": 3651.84, "text": " you know, in the kind of like ordinary way we use the word believes, well, it gets, it gets", "tokens": [50368, 291, 458, 11, 294, 264, 733, 295, 411, 10547, 636, 321, 764, 264, 1349, 12307, 11, 731, 11, 309, 2170, 11, 309, 2170, 50784], "temperature": 0.0, "avg_logprob": -0.16659861863261521, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.14153529703617096}, {"id": 639, "seek": 364344, "start": 3651.84, "end": 3658.56, "text": " complicated because we do use the word believes in this intentional stance way to, to talk about", "tokens": [50784, 6179, 570, 321, 360, 764, 264, 1349, 12307, 294, 341, 21935, 21033, 636, 281, 11, 281, 751, 466, 51120], "temperature": 0.0, "avg_logprob": -0.16659861863261521, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.14153529703617096}, {"id": 640, "seek": 364344, "start": 3658.56, "end": 3665.28, "text": " ordinary everyday things. We say, oh, my, my, you know, my, my car clock thinks that it's", "tokens": [51120, 10547, 7429, 721, 13, 492, 584, 11, 1954, 11, 452, 11, 452, 11, 291, 458, 11, 452, 11, 452, 1032, 7830, 7309, 300, 309, 311, 51456], "temperature": 0.0, "avg_logprob": -0.16659861863261521, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.14153529703617096}, {"id": 641, "seek": 364344, "start": 3665.28, "end": 3670.56, "text": " British summertime, you know, you know, because we, and then, but then you'd say, then somebody", "tokens": [51456, 6221, 43785, 11, 291, 458, 11, 291, 458, 11, 570, 321, 11, 293, 550, 11, 457, 550, 291, 1116, 584, 11, 550, 2618, 51720], "temperature": 0.0, "avg_logprob": -0.16659861863261521, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.14153529703617096}, {"id": 642, "seek": 367056, "start": 3670.56, "end": 3675.2799999999997, "text": " says to you, what you, what you mean, your, your car clock and think, you say, no, obviously,", "tokens": [50364, 1619, 281, 291, 11, 437, 291, 11, 437, 291, 914, 11, 428, 11, 428, 1032, 7830, 293, 519, 11, 291, 584, 11, 572, 11, 2745, 11, 50600], "temperature": 0.0, "avg_logprob": -0.17408677596080152, "compression_ratio": 1.935593220338983, "no_speech_prob": 0.00244508171454072}, {"id": 643, "seek": 367056, "start": 3675.2799999999997, "end": 3679.2799999999997, "text": " I didn't mean that it can think, it's just a turn of phrase, you know, but when we, when we get to", "tokens": [50600, 286, 994, 380, 914, 300, 309, 393, 519, 11, 309, 311, 445, 257, 1261, 295, 9535, 11, 291, 458, 11, 457, 562, 321, 11, 562, 321, 483, 281, 50800], "temperature": 0.0, "avg_logprob": -0.17408677596080152, "compression_ratio": 1.935593220338983, "no_speech_prob": 0.00244508171454072}, {"id": 644, "seek": 367056, "start": 3679.2799999999997, "end": 3684.32, "text": " these large language models, we start to use the words like thinks and believes and so on,", "tokens": [50800, 613, 2416, 2856, 5245, 11, 321, 722, 281, 764, 264, 2283, 411, 7309, 293, 12307, 293, 370, 322, 11, 51052], "temperature": 0.0, "avg_logprob": -0.17408677596080152, "compression_ratio": 1.935593220338983, "no_speech_prob": 0.00244508171454072}, {"id": 645, "seek": 367056, "start": 3684.32, "end": 3688.7999999999997, "text": " because they're so powerful, it starts to get ambiguous and yours, and your, and, you know,", "tokens": [51052, 570, 436, 434, 370, 4005, 11, 309, 3719, 281, 483, 39465, 293, 6342, 11, 293, 428, 11, 293, 11, 291, 458, 11, 51276], "temperature": 0.0, "avg_logprob": -0.17408677596080152, "compression_ratio": 1.935593220338983, "no_speech_prob": 0.00244508171454072}, {"id": 646, "seek": 367056, "start": 3689.52, "end": 3693.36, "text": " and some people will say, well, actually, I really didn't mean that it can think or that it believes.", "tokens": [51312, 293, 512, 561, 486, 584, 11, 731, 11, 767, 11, 286, 534, 994, 380, 914, 300, 309, 393, 519, 420, 300, 309, 12307, 13, 51504], "temperature": 0.0, "avg_logprob": -0.17408677596080152, "compression_ratio": 1.935593220338983, "no_speech_prob": 0.00244508171454072}, {"id": 647, "seek": 367056, "start": 3693.36, "end": 3699.7599999999998, "text": " So I'm, so I'm, I'm interested in this, when things get difficult in this respect. And could,", "tokens": [51504, 407, 286, 478, 11, 370, 286, 478, 11, 286, 478, 3102, 294, 341, 11, 562, 721, 483, 2252, 294, 341, 3104, 13, 400, 727, 11, 51824], "temperature": 0.0, "avg_logprob": -0.17408677596080152, "compression_ratio": 1.935593220338983, "no_speech_prob": 0.00244508171454072}, {"id": 648, "seek": 369976, "start": 3699.76, "end": 3704.2400000000002, "text": " could you tease apart that work? So you resist anthropomorphic language in terms of belief,", "tokens": [50364, 727, 291, 30444, 4936, 300, 589, 30, 407, 291, 4597, 22727, 32702, 299, 2856, 294, 2115, 295, 7107, 11, 50588], "temperature": 0.0, "avg_logprob": -0.16446566785502637, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.004548649303615093}, {"id": 649, "seek": 369976, "start": 3704.2400000000002, "end": 3709.92, "text": " knowledge, understanding, self or even consciousness, but less so with reasoning. And I, my intuition", "tokens": [50588, 3601, 11, 3701, 11, 2698, 420, 754, 10081, 11, 457, 1570, 370, 365, 21577, 13, 400, 286, 11, 452, 24002, 50872], "temperature": 0.0, "avg_logprob": -0.16446566785502637, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.004548649303615093}, {"id": 650, "seek": 369976, "start": 3709.92, "end": 3715.44, "text": " is that reasoning rather depends on those things that I just said before. Well, I, so I think it", "tokens": [50872, 307, 300, 21577, 2831, 5946, 322, 729, 721, 300, 286, 445, 848, 949, 13, 1042, 11, 286, 11, 370, 286, 519, 309, 51148], "temperature": 0.0, "avg_logprob": -0.16446566785502637, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.004548649303615093}, {"id": 651, "seek": 369976, "start": 3715.44, "end": 3722.88, "text": " doesn't because, but this is perhaps, this is just maybe in a kind of formal logic sense, because,", "tokens": [51148, 1177, 380, 570, 11, 457, 341, 307, 4317, 11, 341, 307, 445, 1310, 294, 257, 733, 295, 9860, 9952, 2020, 11, 570, 11, 51520], "temperature": 0.0, "avg_logprob": -0.16446566785502637, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.004548649303615093}, {"id": 652, "seek": 369976, "start": 3722.88, "end": 3729.44, "text": " because reason, because logic is content neutral. So if I tell you that every, could you just explain", "tokens": [51520, 570, 1778, 11, 570, 9952, 307, 2701, 10598, 13, 407, 498, 286, 980, 291, 300, 633, 11, 727, 291, 445, 2903, 51848], "temperature": 0.0, "avg_logprob": -0.16446566785502637, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.004548649303615093}, {"id": 653, "seek": 372944, "start": 3729.52, "end": 3734.88, "text": " what you mean by that? Okay. So, so Lewis Carroll has all these wonderful kind of like nonsense", "tokens": [50368, 437, 291, 914, 538, 300, 30, 1033, 13, 407, 11, 370, 17412, 48456, 575, 439, 613, 3715, 733, 295, 411, 14925, 50636], "temperature": 0.0, "avg_logprob": -0.15503725351071826, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004209776874631643}, {"id": 654, "seek": 372944, "start": 3734.88, "end": 3742.7200000000003, "text": " syllogisms, right? Where he, where, you know, he says, oh, if all elephants like custard and,", "tokens": [50636, 20223, 664, 13539, 11, 558, 30, 2305, 415, 11, 689, 11, 291, 458, 11, 415, 1619, 11, 1954, 11, 498, 439, 33015, 411, 46972, 293, 11, 51028], "temperature": 0.0, "avg_logprob": -0.15503725351071826, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004209776874631643}, {"id": 655, "seek": 372944, "start": 3742.7200000000003, "end": 3746.7200000000003, "text": " you know, Jonathan is an elephant, you know, Jonathan likes custard, and, you know, all kinds", "tokens": [51028, 291, 458, 11, 15471, 307, 364, 19791, 11, 291, 458, 11, 15471, 5902, 46972, 11, 293, 11, 291, 458, 11, 439, 3685, 51228], "temperature": 0.0, "avg_logprob": -0.15503725351071826, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004209776874631643}, {"id": 656, "seek": 372944, "start": 3746.7200000000003, "end": 3752.0, "text": " of things like that. And it's all sort of nonsense. And he has this big complex, complicated ones.", "tokens": [51228, 295, 721, 411, 300, 13, 400, 309, 311, 439, 1333, 295, 14925, 13, 400, 415, 575, 341, 955, 3997, 11, 6179, 2306, 13, 51492], "temperature": 0.0, "avg_logprob": -0.15503725351071826, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.004209776874631643}, {"id": 657, "seek": 375200, "start": 3752.0, "end": 3760.88, "text": " Similarly, I could tell you that all, all sprung forths are plingy, and, and Juliet is a sprung", "tokens": [50364, 13157, 11, 286, 727, 980, 291, 300, 439, 11, 439, 6103, 1063, 5220, 82, 366, 499, 278, 88, 11, 293, 11, 293, 33532, 307, 257, 6103, 1063, 50808], "temperature": 0.0, "avg_logprob": -0.16726782383062902, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.017444852739572525}, {"id": 658, "seek": 375200, "start": 3760.88, "end": 3766.48, "text": " forth. Therefore, Juliet is, is a springy, right? And I've no idea what any of those things mean,", "tokens": [50808, 5220, 13, 7504, 11, 33532, 307, 11, 307, 257, 5587, 88, 11, 558, 30, 400, 286, 600, 572, 1558, 437, 604, 295, 729, 721, 914, 11, 51088], "temperature": 0.0, "avg_logprob": -0.16726782383062902, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.017444852739572525}, {"id": 659, "seek": 375200, "start": 3766.48, "end": 3772.72, "text": " but the, the, but it's because it, because it, for the pure form of the reasoning, you don't have", "tokens": [51088, 457, 264, 11, 264, 11, 457, 309, 311, 570, 309, 11, 570, 309, 11, 337, 264, 6075, 1254, 295, 264, 21577, 11, 291, 500, 380, 362, 51400], "temperature": 0.0, "avg_logprob": -0.16726782383062902, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.017444852739572525}, {"id": 660, "seek": 375200, "start": 3772.72, "end": 3780.0, "text": " to know what they mean. It's just about the logic. So, so in that sense, you know, it just in the way", "tokens": [51400, 281, 458, 437, 436, 914, 13, 467, 311, 445, 466, 264, 9952, 13, 407, 11, 370, 294, 300, 2020, 11, 291, 458, 11, 309, 445, 294, 264, 636, 51764], "temperature": 0.0, "avg_logprob": -0.16726782383062902, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.017444852739572525}, {"id": 661, "seek": 378000, "start": 3780.0, "end": 3785.36, "text": " that a theorem prover can do logic, then so can a large language model do logic. So in that sense,", "tokens": [50364, 300, 257, 20904, 447, 331, 393, 360, 9952, 11, 550, 370, 393, 257, 2416, 2856, 2316, 360, 9952, 13, 407, 294, 300, 2020, 11, 50632], "temperature": 0.0, "avg_logprob": -0.07679716996320589, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.01057297084480524}, {"id": 662, "seek": 378000, "start": 3785.36, "end": 3790.24, "text": " I think large, it is reasonable to use the word reasoning in that logical sense in the", "tokens": [50632, 286, 519, 2416, 11, 309, 307, 10585, 281, 764, 264, 1349, 21577, 294, 300, 14978, 2020, 294, 264, 50876], "temperature": 0.0, "avg_logprob": -0.07679716996320589, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.01057297084480524}, {"id": 663, "seek": 378000, "start": 3790.24, "end": 3794.08, "text": " context of large language models. I don't think that's a problem. Of course, we may think that", "tokens": [50876, 4319, 295, 2416, 2856, 5245, 13, 286, 500, 380, 519, 300, 311, 257, 1154, 13, 2720, 1164, 11, 321, 815, 519, 300, 51068], "temperature": 0.0, "avg_logprob": -0.07679716996320589, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.01057297084480524}, {"id": 664, "seek": 378000, "start": 3794.08, "end": 3798.48, "text": " they do it badly, or they do it well, or that's a whole other thing, right? But, but at least the", "tokens": [51068, 436, 360, 309, 13425, 11, 420, 436, 360, 309, 731, 11, 420, 300, 311, 257, 1379, 661, 551, 11, 558, 30, 583, 11, 457, 412, 1935, 264, 51288], "temperature": 0.0, "avg_logprob": -0.07679716996320589, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.01057297084480524}, {"id": 665, "seek": 378000, "start": 3798.48, "end": 3805.28, "text": " word is potentially applicable, right? Yes. Now, belief, I think, you know, I think at the moment", "tokens": [51288, 1349, 307, 7263, 21142, 11, 558, 30, 1079, 13, 823, 11, 7107, 11, 286, 519, 11, 291, 458, 11, 286, 519, 412, 264, 1623, 51628], "temperature": 0.0, "avg_logprob": -0.07679716996320589, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.01057297084480524}, {"id": 666, "seek": 380528, "start": 3805.28, "end": 3812.88, "text": " is a, is a, is a different kettle of fish, because to really have a holder belief, it's, it, it's,", "tokens": [50364, 307, 257, 11, 307, 257, 11, 307, 257, 819, 39088, 295, 3506, 11, 570, 281, 534, 362, 257, 20349, 7107, 11, 309, 311, 11, 309, 11, 309, 311, 11, 50744], "temperature": 0.0, "avg_logprob": -0.1331199804941813, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.03590543195605278}, {"id": 667, "seek": 380528, "start": 3812.88, "end": 3818.48, "text": " it's not content neutral, right? So if you, if I believe to use the example in my, in, in my paper,", "tokens": [50744, 309, 311, 406, 2701, 10598, 11, 558, 30, 407, 498, 291, 11, 498, 286, 1697, 281, 764, 264, 1365, 294, 452, 11, 294, 11, 294, 452, 3035, 11, 51024], "temperature": 0.0, "avg_logprob": -0.1331199804941813, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.03590543195605278}, {"id": 668, "seek": 380528, "start": 3819.2000000000003, "end": 3826.88, "text": " if I believe that Barundi is to the south of, of, of Rwanda, well, whether that is the case or not,", "tokens": [51060, 498, 286, 1697, 300, 4156, 997, 72, 307, 281, 264, 7377, 295, 11, 295, 11, 295, 497, 86, 5575, 11, 731, 11, 1968, 300, 307, 264, 1389, 420, 406, 11, 51444], "temperature": 0.0, "avg_logprob": -0.1331199804941813, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.03590543195605278}, {"id": 669, "seek": 380528, "start": 3826.88, "end": 3832.5600000000004, "text": " it does depend upon facts that are out there in the world. And then to, to really have a belief,", "tokens": [51444, 309, 775, 5672, 3564, 9130, 300, 366, 484, 456, 294, 264, 1002, 13, 400, 550, 281, 11, 281, 534, 362, 257, 7107, 11, 51728], "temperature": 0.0, "avg_logprob": -0.1331199804941813, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.03590543195605278}, {"id": 670, "seek": 383256, "start": 3832.64, "end": 3839.68, "text": " at least you've got to be able to somehow try and kind of justify those facts, or at least, and you", "tokens": [50368, 412, 1935, 291, 600, 658, 281, 312, 1075, 281, 6063, 853, 293, 733, 295, 20833, 729, 9130, 11, 420, 412, 1935, 11, 293, 291, 50720], "temperature": 0.0, "avg_logprob": -0.09475060552358627, "compression_ratio": 1.8884615384615384, "no_speech_prob": 0.005862811580300331}, {"id": 671, "seek": 383256, "start": 3839.68, "end": 3845.2, "text": " got to be at least built in such a way that you can, you know, interact with the external world", "tokens": [50720, 658, 281, 312, 412, 1935, 3094, 294, 1270, 257, 636, 300, 291, 393, 11, 291, 458, 11, 4648, 365, 264, 8320, 1002, 50996], "temperature": 0.0, "avg_logprob": -0.09475060552358627, "compression_ratio": 1.8884615384615384, "no_speech_prob": 0.005862811580300331}, {"id": 672, "seek": 383256, "start": 3845.2, "end": 3850.96, "text": " and do that sort of thing, right? And, and verify that something is true or false or do an experiment", "tokens": [50996, 293, 360, 300, 1333, 295, 551, 11, 558, 30, 400, 11, 293, 16888, 300, 746, 307, 2074, 420, 7908, 420, 360, 364, 5120, 51284], "temperature": 0.0, "avg_logprob": -0.09475060552358627, "compression_ratio": 1.8884615384615384, "no_speech_prob": 0.005862811580300331}, {"id": 673, "seek": 383256, "start": 3850.96, "end": 3856.4, "text": " or, you know, or ask someone or, you know, you've got to go outside yourself, right? We go outside", "tokens": [51284, 420, 11, 291, 458, 11, 420, 1029, 1580, 420, 11, 291, 458, 11, 291, 600, 658, 281, 352, 2380, 1803, 11, 558, 30, 492, 352, 2380, 51556], "temperature": 0.0, "avg_logprob": -0.09475060552358627, "compression_ratio": 1.8884615384615384, "no_speech_prob": 0.005862811580300331}, {"id": 674, "seek": 383256, "start": 3856.4, "end": 3861.36, "text": " of ourselves and, and in order to establish whether something a belief is true or not. And so,", "tokens": [51556, 295, 4175, 293, 11, 293, 294, 1668, 281, 8327, 1968, 746, 257, 7107, 307, 2074, 420, 406, 13, 400, 370, 11, 51804], "temperature": 0.0, "avg_logprob": -0.09475060552358627, "compression_ratio": 1.8884615384615384, "no_speech_prob": 0.005862811580300331}, {"id": 675, "seek": 386136, "start": 3862.08, "end": 3866.88, "text": " you've got to at least be capable of doing that. Whereas large language models, the bare bones,", "tokens": [50400, 291, 600, 658, 281, 412, 1935, 312, 8189, 295, 884, 300, 13, 13813, 2416, 2856, 5245, 11, 264, 6949, 10491, 11, 50640], "temperature": 0.0, "avg_logprob": -0.1074541732147857, "compression_ratio": 2.132841328413284, "no_speech_prob": 0.00015267744311131537}, {"id": 676, "seek": 386136, "start": 3866.88, "end": 3872.56, "text": " large language model is not capable of doing that at all, right? Now you can embed it in a larger", "tokens": [50640, 2416, 2856, 2316, 307, 406, 8189, 295, 884, 300, 412, 439, 11, 558, 30, 823, 291, 393, 12240, 309, 294, 257, 4833, 50924], "temperature": 0.0, "avg_logprob": -0.1074541732147857, "compression_ratio": 2.132841328413284, "no_speech_prob": 0.00015267744311131537}, {"id": 677, "seek": 386136, "start": 3872.56, "end": 3876.08, "text": " system. This is a really important distinction that I've tried and make over and again in the", "tokens": [50924, 1185, 13, 639, 307, 257, 534, 1021, 16844, 300, 286, 600, 3031, 293, 652, 670, 293, 797, 294, 264, 51100], "temperature": 0.0, "avg_logprob": -0.1074541732147857, "compression_ratio": 2.132841328413284, "no_speech_prob": 0.00015267744311131537}, {"id": 678, "seek": 386136, "start": 3876.08, "end": 3881.36, "text": " paper. I talk about the bare bones, large language model. So you can take the, so, so, and whenever", "tokens": [51100, 3035, 13, 286, 751, 466, 264, 6949, 10491, 11, 2416, 2856, 2316, 13, 407, 291, 393, 747, 264, 11, 370, 11, 370, 11, 293, 5699, 51364], "temperature": 0.0, "avg_logprob": -0.1074541732147857, "compression_ratio": 2.132841328413284, "no_speech_prob": 0.00015267744311131537}, {"id": 679, "seek": 386136, "start": 3881.36, "end": 3886.0, "text": " a large language model is used, it's not the bare bones, large language model, which just does", "tokens": [51364, 257, 2416, 2856, 2316, 307, 1143, 11, 309, 311, 406, 264, 6949, 10491, 11, 2416, 2856, 2316, 11, 597, 445, 775, 51596], "temperature": 0.0, "avg_logprob": -0.1074541732147857, "compression_ratio": 2.132841328413284, "no_speech_prob": 0.00015267744311131537}, {"id": 680, "seek": 386136, "start": 3886.0, "end": 3890.56, "text": " sequence prediction, but it's embedded in a larger system. When we embed it in a larger system,", "tokens": [51596, 8310, 17630, 11, 457, 309, 311, 16741, 294, 257, 4833, 1185, 13, 1133, 321, 12240, 309, 294, 257, 4833, 1185, 11, 51824], "temperature": 0.0, "avg_logprob": -0.1074541732147857, "compression_ratio": 2.132841328413284, "no_speech_prob": 0.00015267744311131537}, {"id": 681, "seek": 389056, "start": 3890.56, "end": 3895.2799999999997, "text": " well, that larger system maybe could consult Wikipedia or maybe it could be part of a robot", "tokens": [50364, 731, 11, 300, 4833, 1185, 1310, 727, 7189, 28999, 420, 1310, 309, 727, 312, 644, 295, 257, 7881, 50600], "temperature": 0.0, "avg_logprob": -0.10223012377125348, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.001307824277319014}, {"id": 682, "seek": 389056, "start": 3895.2799999999997, "end": 3899.04, "text": " that goes and investigates the world. So that's a whole other thing. But then you have to look at", "tokens": [50600, 300, 1709, 293, 4557, 1024, 264, 1002, 13, 407, 300, 311, 257, 1379, 661, 551, 13, 583, 550, 291, 362, 281, 574, 412, 50788], "temperature": 0.0, "avg_logprob": -0.10223012377125348, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.001307824277319014}, {"id": 683, "seek": 389056, "start": 3899.04, "end": 3904.7999999999997, "text": " each case in point and, and, and ask yourself whether it's a, whether, you know, whether we", "tokens": [50788, 1184, 1389, 294, 935, 293, 11, 293, 11, 293, 1029, 1803, 1968, 309, 311, 257, 11, 1968, 11, 291, 458, 11, 1968, 321, 51076], "temperature": 0.0, "avg_logprob": -0.10223012377125348, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.001307824277319014}, {"id": 684, "seek": 389056, "start": 3904.7999999999997, "end": 3910.56, "text": " really want to use that word in, in, in anger, you know, as in, in its full sense, rather than just", "tokens": [51076, 534, 528, 281, 764, 300, 1349, 294, 11, 294, 11, 294, 10240, 11, 291, 458, 11, 382, 294, 11, 294, 1080, 1577, 2020, 11, 2831, 813, 445, 51364], "temperature": 0.0, "avg_logprob": -0.10223012377125348, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.001307824277319014}, {"id": 685, "seek": 389056, "start": 3910.56, "end": 3915.7599999999998, "text": " in the intentional stance sense of a kind of figure of speech. So, and so in the case of, of, of,", "tokens": [51364, 294, 264, 21935, 21033, 2020, 295, 257, 733, 295, 2573, 295, 6218, 13, 407, 11, 293, 370, 294, 264, 1389, 295, 11, 295, 11, 295, 11, 51624], "temperature": 0.0, "avg_logprob": -0.10223012377125348, "compression_ratio": 1.794007490636704, "no_speech_prob": 0.001307824277319014}, {"id": 686, "seek": 391576, "start": 3915.84, "end": 3922.88, "text": " of like chatbots, for example, today's chatbots, not really appropriate, I would say. We're not", "tokens": [50368, 295, 411, 5081, 65, 1971, 11, 337, 1365, 11, 965, 311, 5081, 65, 1971, 11, 406, 534, 6854, 11, 286, 576, 584, 13, 492, 434, 406, 50720], "temperature": 0.0, "avg_logprob": -0.09816402685446818, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.0017958233365789056}, {"id": 687, "seek": 391576, "start": 3922.88, "end": 3928.5600000000004, "text": " using the word in the way that we, in the full blown sense that we use it, where we talk about", "tokens": [50720, 1228, 264, 1349, 294, 264, 636, 300, 321, 11, 294, 264, 1577, 16479, 2020, 300, 321, 764, 309, 11, 689, 321, 751, 466, 51004], "temperature": 0.0, "avg_logprob": -0.09816402685446818, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.0017958233365789056}, {"id": 688, "seek": 391576, "start": 3928.5600000000004, "end": 3934.48, "text": " each other. Fascinating. Okay, well, let's get on to intentional stance. So you said that it's a useful", "tokens": [51004, 1184, 661, 13, 49098, 8205, 13, 1033, 11, 731, 11, 718, 311, 483, 322, 281, 21935, 21033, 13, 407, 291, 848, 300, 309, 311, 257, 4420, 51300], "temperature": 0.0, "avg_logprob": -0.09816402685446818, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.0017958233365789056}, {"id": 689, "seek": 391576, "start": 3934.48, "end": 3939.1200000000003, "text": " way of thinking about artificial intelligence, allowing us to view computer programs as intelligent", "tokens": [51300, 636, 295, 1953, 466, 11677, 7599, 11, 8293, 505, 281, 1910, 3820, 4268, 382, 13232, 51532], "temperature": 0.0, "avg_logprob": -0.09816402685446818, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.0017958233365789056}, {"id": 690, "seek": 391576, "start": 3939.1200000000003, "end": 3942.88, "text": " agents, even though they may lack the same kind of understanding as a human. And then you cited", "tokens": [51532, 12554, 11, 754, 1673, 436, 815, 5011, 264, 912, 733, 295, 3701, 382, 257, 1952, 13, 400, 550, 291, 30134, 51720], "temperature": 0.0, "avg_logprob": -0.09816402685446818, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.0017958233365789056}, {"id": 691, "seek": 394288, "start": 3942.96, "end": 3948.1600000000003, "text": " the case of Bob and Bot. The, the word no was used differently in the two cases. And the word of", "tokens": [50368, 264, 1389, 295, 6085, 293, 25486, 13, 440, 11, 264, 1349, 572, 390, 1143, 7614, 294, 264, 732, 3331, 13, 400, 264, 1349, 295, 50628], "temperature": 0.0, "avg_logprob": -0.13922648131847382, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.00992193166166544}, {"id": 692, "seek": 394288, "start": 3948.1600000000003, "end": 3952.56, "text": " Bob, it was used in the traditional sense. For bot, it was used in a metaphorical sense. So it kind", "tokens": [50628, 6085, 11, 309, 390, 1143, 294, 264, 5164, 2020, 13, 1171, 10592, 11, 309, 390, 1143, 294, 257, 19157, 804, 2020, 13, 407, 309, 733, 50848], "temperature": 0.0, "avg_logprob": -0.13922648131847382, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.00992193166166544}, {"id": 693, "seek": 394288, "start": 3952.56, "end": 3958.32, "text": " of like, it's just distinguishing what it, what it means to know, you know, for humans and, and,", "tokens": [50848, 295, 411, 11, 309, 311, 445, 11365, 3807, 437, 309, 11, 437, 309, 1355, 281, 458, 11, 291, 458, 11, 337, 6255, 293, 11, 293, 11, 51136], "temperature": 0.0, "avg_logprob": -0.13922648131847382, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.00992193166166544}, {"id": 694, "seek": 394288, "start": 3958.32, "end": 3963.6, "text": " and for machine. So I think it's, it's useful to think about something like Wikipedia. So,", "tokens": [51136, 293, 337, 3479, 13, 407, 286, 519, 309, 311, 11, 309, 311, 4420, 281, 519, 466, 746, 411, 28999, 13, 407, 11, 51400], "temperature": 0.0, "avg_logprob": -0.13922648131847382, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.00992193166166544}, {"id": 695, "seek": 394288, "start": 3964.7200000000003, "end": 3971.6, "text": " so we might ask the question, does Wikipedia know that Argentina has won the 2022 World Cup?", "tokens": [51456, 370, 321, 1062, 1029, 264, 1168, 11, 775, 28999, 458, 300, 18336, 575, 1582, 264, 20229, 3937, 13751, 30, 51800], "temperature": 0.0, "avg_logprob": -0.13922648131847382, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.00992193166166544}, {"id": 696, "seek": 397160, "start": 3971.6, "end": 3976.96, "text": " And just immediately after the event, you know, it probably doesn't, it's not recorded in Wikipedia.", "tokens": [50364, 400, 445, 4258, 934, 264, 2280, 11, 291, 458, 11, 309, 1391, 1177, 380, 11, 309, 311, 406, 8287, 294, 28999, 13, 50632], "temperature": 0.0, "avg_logprob": -0.10688534618294152, "compression_ratio": 1.9233716475095786, "no_speech_prob": 0.002023555338382721}, {"id": 697, "seek": 397160, "start": 3976.96, "end": 3982.96, "text": " And somebody might say, Oh, Wikipedia doesn't know yet that the Argentina have won. And so when we", "tokens": [50632, 400, 2618, 1062, 584, 11, 876, 11, 28999, 1177, 380, 458, 1939, 300, 264, 18336, 362, 1582, 13, 400, 370, 562, 321, 50932], "temperature": 0.0, "avg_logprob": -0.10688534618294152, "compression_ratio": 1.9233716475095786, "no_speech_prob": 0.002023555338382721}, {"id": 698, "seek": 397160, "start": 3982.96, "end": 3987.2799999999997, "text": " use the word like that, you know, nobody's going to kind of say to them, say to somebody who uses", "tokens": [50932, 764, 264, 1349, 411, 300, 11, 291, 458, 11, 5079, 311, 516, 281, 733, 295, 584, 281, 552, 11, 584, 281, 2618, 567, 4960, 51148], "temperature": 0.0, "avg_logprob": -0.10688534618294152, "compression_ratio": 1.9233716475095786, "no_speech_prob": 0.002023555338382721}, {"id": 699, "seek": 397160, "start": 3987.2799999999997, "end": 3991.8399999999997, "text": " that word, hey, you know, I don't think you should use the word knows there. And you know, that would,", "tokens": [51148, 300, 1349, 11, 4177, 11, 291, 458, 11, 286, 500, 380, 519, 291, 820, 764, 264, 1349, 3255, 456, 13, 400, 291, 458, 11, 300, 576, 11, 51376], "temperature": 0.0, "avg_logprob": -0.10688534618294152, "compression_ratio": 1.9233716475095786, "no_speech_prob": 0.002023555338382721}, {"id": 700, "seek": 397160, "start": 3991.8399999999997, "end": 3996.0, "text": " you know, you should be a bit more sort of sensible. I mean, it's, it's fine to kind of use, I think,", "tokens": [51376, 291, 458, 11, 291, 820, 312, 257, 857, 544, 1333, 295, 25380, 13, 286, 914, 11, 309, 311, 11, 309, 311, 2489, 281, 733, 295, 764, 11, 286, 519, 11, 51584], "temperature": 0.0, "avg_logprob": -0.10688534618294152, "compression_ratio": 1.9233716475095786, "no_speech_prob": 0.002023555338382721}, {"id": 701, "seek": 399600, "start": 3996.0, "end": 4002.4, "text": " these kinds of words in this ordinary, every day sense. And we do that all the time. And that's", "tokens": [50364, 613, 3685, 295, 2283, 294, 341, 10547, 11, 633, 786, 2020, 13, 400, 321, 360, 300, 439, 264, 565, 13, 400, 300, 311, 50684], "temperature": 0.0, "avg_logprob": -0.14965858780035451, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.20852932333946228}, {"id": 702, "seek": 399600, "start": 4002.4, "end": 4008.64, "text": " sort of, particularly, particularly in the case of computers, that's adopting what Dandenek calls", "tokens": [50684, 1333, 295, 11, 4098, 11, 4098, 294, 264, 1389, 295, 10807, 11, 300, 311, 32328, 437, 413, 28762, 916, 5498, 50996], "temperature": 0.0, "avg_logprob": -0.14965858780035451, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.20852932333946228}, {"id": 703, "seek": 399600, "start": 4008.64, "end": 4016.88, "text": " the intentional stance. So we're, so we're, we're interpreting something as, as, as, as having beliefs,", "tokens": [50996, 264, 21935, 21033, 13, 407, 321, 434, 11, 370, 321, 434, 11, 321, 434, 37395, 746, 382, 11, 382, 11, 382, 11, 382, 1419, 13585, 11, 51408], "temperature": 0.0, "avg_logprob": -0.14965858780035451, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.20852932333946228}, {"id": 704, "seek": 399600, "start": 4016.88, "end": 4022.08, "text": " desires and intentions, because it's a kind of convenient shorthand. And especially if you've", "tokens": [51408, 18005, 293, 19354, 11, 570, 309, 311, 257, 733, 295, 10851, 402, 2652, 474, 13, 400, 2318, 498, 291, 600, 51668], "temperature": 0.0, "avg_logprob": -0.14965858780035451, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.20852932333946228}, {"id": 705, "seek": 399600, "start": 4022.08, "end": 4025.84, "text": " got something that's a bit more complicated, like say your car sat down for something or", "tokens": [51668, 658, 746, 300, 311, 257, 857, 544, 6179, 11, 411, 584, 428, 1032, 3227, 760, 337, 746, 420, 51856], "temperature": 0.0, "avg_logprob": -0.14965858780035451, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.20852932333946228}, {"id": 706, "seek": 402584, "start": 4025.84, "end": 4031.52, "text": " you're, you're, you know, you're sat up on your, on your phone, then it sort of makes, makes sense", "tokens": [50364, 291, 434, 11, 291, 434, 11, 291, 458, 11, 291, 434, 3227, 493, 322, 428, 11, 322, 428, 2593, 11, 550, 309, 1333, 295, 1669, 11, 1669, 2020, 50648], "temperature": 0.0, "avg_logprob": -0.1768649785946577, "compression_ratio": 1.7968253968253969, "no_speech_prob": 0.00044733661343343556}, {"id": 707, "seek": 402584, "start": 4031.52, "end": 4036.7200000000003, "text": " to use those words. It's a, is a convenient shorthand. And it helps us to kind of talk about them,", "tokens": [50648, 281, 764, 729, 2283, 13, 467, 311, 257, 11, 307, 257, 10851, 402, 2652, 474, 13, 400, 309, 3665, 505, 281, 733, 295, 751, 466, 552, 11, 50908], "temperature": 0.0, "avg_logprob": -0.1768649785946577, "compression_ratio": 1.7968253968253969, "no_speech_prob": 0.00044733661343343556}, {"id": 708, "seek": 402584, "start": 4036.7200000000003, "end": 4042.08, "text": " right? And without getting overly complicated, without knowing the underlying mechanisms. But", "tokens": [50908, 558, 30, 400, 1553, 1242, 24324, 6179, 11, 1553, 5276, 264, 14217, 15902, 13, 583, 51176], "temperature": 0.0, "avg_logprob": -0.1768649785946577, "compression_ratio": 1.7968253968253969, "no_speech_prob": 0.00044733661343343556}, {"id": 709, "seek": 402584, "start": 4042.08, "end": 4045.28, "text": " there's an important sense in which we don't mean it literally. So you know, in the case of", "tokens": [51176, 456, 311, 364, 1021, 2020, 294, 597, 321, 500, 380, 914, 309, 3736, 13, 407, 291, 458, 11, 294, 264, 1389, 295, 51336], "temperature": 0.0, "avg_logprob": -0.1768649785946577, "compression_ratio": 1.7968253968253969, "no_speech_prob": 0.00044733661343343556}, {"id": 710, "seek": 402584, "start": 4045.28, "end": 4048.88, "text": " Wikipedia, you can't, you couldn't go up to Wikipedia and pat it on the shoulder and say,", "tokens": [51336, 28999, 11, 291, 393, 380, 11, 291, 2809, 380, 352, 493, 281, 28999, 293, 1947, 309, 322, 264, 7948, 293, 584, 11, 51516], "temperature": 0.0, "avg_logprob": -0.1768649785946577, "compression_ratio": 1.7968253968253969, "no_speech_prob": 0.00044733661343343556}, {"id": 711, "seek": 402584, "start": 4048.88, "end": 4053.2000000000003, "text": " hey, Argentina have won. And there's no way, you know, right, I want to be a, you know, and,", "tokens": [51516, 4177, 11, 18336, 362, 1582, 13, 400, 456, 311, 572, 636, 11, 291, 458, 11, 558, 11, 286, 528, 281, 312, 257, 11, 291, 458, 11, 293, 11, 51732], "temperature": 0.0, "avg_logprob": -0.1768649785946577, "compression_ratio": 1.7968253968253969, "no_speech_prob": 0.00044733661343343556}, {"id": 712, "seek": 405320, "start": 4053.2, "end": 4060.0, "text": " and, and, and, and all, and all the things that, that go with us as humans actually knowing things.", "tokens": [50364, 293, 11, 293, 11, 293, 11, 293, 439, 11, 293, 439, 264, 721, 300, 11, 300, 352, 365, 505, 382, 6255, 767, 5276, 721, 13, 50704], "temperature": 0.0, "avg_logprob": -0.12894052074801537, "compression_ratio": 1.8309178743961352, "no_speech_prob": 0.0025994053576141596}, {"id": 713, "seek": 405320, "start": 4060.0, "end": 4067.12, "text": " And it's just a turn of phrase. Now, things get sort of interesting with large language models,", "tokens": [50704, 400, 309, 311, 445, 257, 1261, 295, 9535, 13, 823, 11, 721, 483, 1333, 295, 1880, 365, 2416, 2856, 5245, 11, 51060], "temperature": 0.0, "avg_logprob": -0.12894052074801537, "compression_ratio": 1.8309178743961352, "no_speech_prob": 0.0025994053576141596}, {"id": 714, "seek": 405320, "start": 4067.12, "end": 4072.56, "text": " and with large language model based systems and the kinds of things that we're starting to see", "tokens": [51060, 293, 365, 2416, 2856, 2316, 2361, 3652, 293, 264, 3685, 295, 721, 300, 321, 434, 2891, 281, 536, 51332], "temperature": 0.0, "avg_logprob": -0.12894052074801537, "compression_ratio": 1.8309178743961352, "no_speech_prob": 0.0025994053576141596}, {"id": 715, "seek": 405320, "start": 4072.56, "end": 4077.2799999999997, "text": " in the world, because we're starting to get into this kind of blurry territory where it,", "tokens": [51332, 294, 264, 1002, 11, 570, 321, 434, 2891, 281, 483, 666, 341, 733, 295, 37644, 11360, 689, 309, 11, 51568], "temperature": 0.0, "avg_logprob": -0.12894052074801537, "compression_ratio": 1.8309178743961352, "no_speech_prob": 0.0025994053576141596}, {"id": 716, "seek": 407728, "start": 4077.76, "end": 4083.6000000000004, "text": " we're blurring between the intentional stance and, and, you know, meaning the meaning it literally.", "tokens": [50388, 321, 434, 14257, 2937, 1296, 264, 21935, 21033, 293, 11, 293, 11, 291, 458, 11, 3620, 264, 3620, 309, 3736, 13, 50680], "temperature": 0.0, "avg_logprob": -0.11739640142403397, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.008523243479430676}, {"id": 717, "seek": 407728, "start": 4083.6000000000004, "end": 4087.92, "text": " And this is where we need to be really, really kind of careful, I think. So at what point does", "tokens": [50680, 400, 341, 307, 689, 321, 643, 281, 312, 534, 11, 534, 733, 295, 5026, 11, 286, 519, 13, 407, 412, 437, 935, 775, 50896], "temperature": 0.0, "avg_logprob": -0.11739640142403397, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.008523243479430676}, {"id": 718, "seek": 407728, "start": 4087.92, "end": 4095.52, "text": " do things shade over into where it's legitimate to use that word, you know, literally, in, in the", "tokens": [50896, 360, 721, 11466, 670, 666, 689, 309, 311, 17956, 281, 764, 300, 1349, 11, 291, 458, 11, 3736, 11, 294, 11, 294, 264, 51276], "temperature": 0.0, "avg_logprob": -0.11739640142403397, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.008523243479430676}, {"id": 719, "seek": 407728, "start": 4095.52, "end": 4101.68, "text": " context of something that we've built, you know, I don't think we're at that point yet. And, and", "tokens": [51276, 4319, 295, 746, 300, 321, 600, 3094, 11, 291, 458, 11, 286, 500, 380, 519, 321, 434, 412, 300, 935, 1939, 13, 400, 11, 293, 51584], "temperature": 0.0, "avg_logprob": -0.11739640142403397, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.008523243479430676}, {"id": 720, "seek": 410168, "start": 4101.68, "end": 4108.320000000001, "text": " we need to be very careful about, about using the word as if we were using it literally.", "tokens": [50364, 321, 643, 281, 312, 588, 5026, 466, 11, 466, 1228, 264, 1349, 382, 498, 321, 645, 1228, 309, 3736, 13, 50696], "temperature": 0.0, "avg_logprob": -0.11581692644344863, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.006754667032510042}, {"id": 721, "seek": 410168, "start": 4109.92, "end": 4114.72, "text": " You know, that's the sort of anthropomorphization, because the problem is that we can then", "tokens": [50776, 509, 458, 11, 300, 311, 264, 1333, 295, 22727, 32702, 2144, 11, 570, 264, 1154, 307, 300, 321, 393, 550, 51016], "temperature": 0.0, "avg_logprob": -0.11581692644344863, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.006754667032510042}, {"id": 722, "seek": 410168, "start": 4115.6, "end": 4123.280000000001, "text": " impute capacities to the thing and, and, and, or even, you know, empathy say that just isn't there.", "tokens": [51060, 704, 1169, 39396, 281, 264, 551, 293, 11, 293, 11, 293, 11, 420, 754, 11, 291, 458, 11, 18701, 584, 300, 445, 1943, 380, 456, 13, 51444], "temperature": 0.0, "avg_logprob": -0.11581692644344863, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.006754667032510042}, {"id": 723, "seek": 410168, "start": 4124.0, "end": 4128.56, "text": " Yes. And I suppose we could tease apart knowledge. So it justified true belief from", "tokens": [51480, 1079, 13, 400, 286, 7297, 321, 727, 30444, 4936, 3601, 13, 407, 309, 27808, 2074, 7107, 490, 51708], "temperature": 0.0, "avg_logprob": -0.11581692644344863, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.006754667032510042}, {"id": 724, "seek": 412856, "start": 4128.56, "end": 4134.160000000001, "text": " knows, because knows that it brings all this baggage of intentionality and agency and anthropomorphization.", "tokens": [50364, 3255, 11, 570, 3255, 300, 309, 5607, 439, 341, 41567, 295, 7789, 1860, 293, 7934, 293, 22727, 32702, 2144, 13, 50644], "temperature": 0.0, "avg_logprob": -0.142557352998831, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.02242165617644787}, {"id": 725, "seek": 412856, "start": 4134.160000000001, "end": 4136.080000000001, "text": " But you had Chomsky, you've had Chomsky on.", "tokens": [50644, 583, 291, 632, 761, 4785, 4133, 11, 291, 600, 632, 761, 4785, 4133, 322, 13, 50740], "temperature": 0.0, "avg_logprob": -0.142557352998831, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.02242165617644787}, {"id": 726, "seek": 412856, "start": 4136.8, "end": 4141.52, "text": " I can tell you a story about that. I mean, the recording messed up. So when we were interviewing", "tokens": [50776, 286, 393, 980, 291, 257, 1657, 466, 300, 13, 286, 914, 11, 264, 6613, 16507, 493, 13, 407, 562, 321, 645, 26524, 51012], "temperature": 0.0, "avg_logprob": -0.142557352998831, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.02242165617644787}, {"id": 727, "seek": 412856, "start": 4141.52, "end": 4145.68, "text": " him, we were only getting bits and pieces. And we had to deep fake him. We had to, we had to", "tokens": [51012, 796, 11, 321, 645, 787, 1242, 9239, 293, 3755, 13, 400, 321, 632, 281, 2452, 7592, 796, 13, 492, 632, 281, 11, 321, 632, 281, 51220], "temperature": 0.0, "avg_logprob": -0.142557352998831, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.02242165617644787}, {"id": 728, "seek": 412856, "start": 4145.68, "end": 4150.080000000001, "text": " regenerate the interview. Oh, really? And he was saying in the entire interview, how much he hated", "tokens": [51220, 26358, 473, 264, 4049, 13, 876, 11, 534, 30, 400, 415, 390, 1566, 294, 264, 2302, 4049, 11, 577, 709, 415, 17398, 51440], "temperature": 0.0, "avg_logprob": -0.142557352998831, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.02242165617644787}, {"id": 729, "seek": 412856, "start": 4150.080000000001, "end": 4155.120000000001, "text": " deep learning and how useless it was. And then we used deep learning to rescue his interview.", "tokens": [51440, 2452, 2539, 293, 577, 14115, 309, 390, 13, 400, 550, 321, 1143, 2452, 2539, 281, 13283, 702, 4049, 13, 51692], "temperature": 0.0, "avg_logprob": -0.142557352998831, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.02242165617644787}, {"id": 730, "seek": 415512, "start": 4155.44, "end": 4161.04, "text": " And he gave us his permission to publish it. That is wonderful.", "tokens": [50380, 400, 415, 2729, 505, 702, 11226, 281, 11374, 309, 13, 663, 307, 3715, 13, 50660], "temperature": 0.0, "avg_logprob": -0.18581608179453257, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.030181271955370903}, {"id": 731, "seek": 415512, "start": 4161.04, "end": 4165.12, "text": " So it's quite ironic. But no, he always says it's wonderful for engineering,", "tokens": [50660, 407, 309, 311, 1596, 33719, 13, 583, 572, 11, 415, 1009, 1619, 309, 311, 3715, 337, 7043, 11, 50864], "temperature": 0.0, "avg_logprob": -0.18581608179453257, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.030181271955370903}, {"id": 732, "seek": 415512, "start": 4165.12, "end": 4167.84, "text": " but not a contribution to science. Yes, sure. Yeah.", "tokens": [50864, 457, 406, 257, 13150, 281, 3497, 13, 1079, 11, 988, 13, 865, 13, 51000], "temperature": 0.0, "avg_logprob": -0.18581608179453257, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.030181271955370903}, {"id": 733, "seek": 415512, "start": 4169.599999999999, "end": 4172.88, "text": " Yeah. He said, I like bulldozers too. They're good for clearing the snow, but they're not a", "tokens": [51088, 865, 13, 634, 848, 11, 286, 411, 4693, 2595, 41698, 886, 13, 814, 434, 665, 337, 23937, 264, 5756, 11, 457, 436, 434, 406, 257, 51252], "temperature": 0.0, "avg_logprob": -0.18581608179453257, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.030181271955370903}, {"id": 734, "seek": 415512, "start": 4172.88, "end": 4176.5599999999995, "text": " contribution to science. So who else have you had? I mean, you've had a lot of people on.", "tokens": [51252, 13150, 281, 3497, 13, 407, 567, 1646, 362, 291, 632, 30, 286, 914, 11, 291, 600, 632, 257, 688, 295, 561, 322, 13, 51436], "temperature": 0.0, "avg_logprob": -0.18581608179453257, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.030181271955370903}, {"id": 735, "seek": 415512, "start": 4176.5599999999995, "end": 4181.12, "text": " I listened to Andrew's one, by the way. It's Andrew Lampinen. Yes, he's great.", "tokens": [51436, 286, 13207, 281, 10110, 311, 472, 11, 538, 264, 636, 13, 467, 311, 10110, 441, 1215, 5636, 13, 1079, 11, 415, 311, 869, 13, 51664], "temperature": 0.0, "avg_logprob": -0.18581608179453257, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.030181271955370903}, {"id": 736, "seek": 415512, "start": 4181.12, "end": 4184.96, "text": " So he's great. Andrew is somebody I do work with quite closely.", "tokens": [51664, 407, 415, 311, 869, 13, 10110, 307, 2618, 286, 360, 589, 365, 1596, 8185, 13, 51856], "temperature": 0.0, "avg_logprob": -0.18581608179453257, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.030181271955370903}, {"id": 737, "seek": 418496, "start": 4185.84, "end": 4190.96, "text": " So it was interesting listening to him because Andrew had quite a big influence on this paper,", "tokens": [50408, 407, 309, 390, 1880, 4764, 281, 796, 570, 10110, 632, 1596, 257, 955, 6503, 322, 341, 3035, 11, 50664], "temperature": 0.0, "avg_logprob": -0.1639974871252337, "compression_ratio": 1.812, "no_speech_prob": 0.0007322862511500716}, {"id": 738, "seek": 418496, "start": 4190.96, "end": 4196.64, "text": " by the way. Oh, okay. But I think I might have had a bit of influence on him as well,", "tokens": [50664, 538, 264, 636, 13, 876, 11, 1392, 13, 583, 286, 519, 286, 1062, 362, 632, 257, 857, 295, 6503, 322, 796, 382, 731, 11, 50948], "temperature": 0.0, "avg_logprob": -0.1639974871252337, "compression_ratio": 1.812, "no_speech_prob": 0.0007322862511500716}, {"id": 739, "seek": 418496, "start": 4196.64, "end": 4202.64, "text": " to listening to him. I think so. Because that interview was just after he'd read,", "tokens": [50948, 281, 4764, 281, 796, 13, 286, 519, 370, 13, 1436, 300, 4049, 390, 445, 934, 415, 1116, 1401, 11, 51248], "temperature": 0.0, "avg_logprob": -0.1639974871252337, "compression_ratio": 1.812, "no_speech_prob": 0.0007322862511500716}, {"id": 740, "seek": 418496, "start": 4202.64, "end": 4206.64, "text": " and he read my paper, gave me lots of comments. And we had a lot of discussion about it.", "tokens": [51248, 293, 415, 1401, 452, 3035, 11, 2729, 385, 3195, 295, 3053, 13, 400, 321, 632, 257, 688, 295, 5017, 466, 309, 13, 51448], "temperature": 0.0, "avg_logprob": -0.1639974871252337, "compression_ratio": 1.812, "no_speech_prob": 0.0007322862511500716}, {"id": 741, "seek": 418496, "start": 4207.12, "end": 4214.4800000000005, "text": " And that interview, looking at the recording date, was sort of just after this. And it's interesting.", "tokens": [51472, 400, 300, 4049, 11, 1237, 412, 264, 6613, 4002, 11, 390, 1333, 295, 445, 934, 341, 13, 400, 309, 311, 1880, 13, 51840], "temperature": 0.0, "avg_logprob": -0.1639974871252337, "compression_ratio": 1.812, "no_speech_prob": 0.0007322862511500716}, {"id": 742, "seek": 421448, "start": 4214.5599999999995, "end": 4218.799999999999, "text": " I mean, he was very circumspect in some of the things he said. Yeah, it was very interesting", "tokens": [50368, 286, 914, 11, 415, 390, 588, 7125, 82, 1043, 294, 512, 295, 264, 721, 415, 848, 13, 865, 11, 309, 390, 588, 1880, 50580], "temperature": 0.0, "avg_logprob": -0.12816257164126538, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0010232807835564017}, {"id": 743, "seek": 421448, "start": 4218.799999999999, "end": 4226.24, "text": " because I think the influence has maybe gone both ways. Yes. Which is nice. I don't know.", "tokens": [50580, 570, 286, 519, 264, 6503, 575, 1310, 2780, 1293, 2098, 13, 1079, 13, 3013, 307, 1481, 13, 286, 500, 380, 458, 13, 50952], "temperature": 0.0, "avg_logprob": -0.12816257164126538, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0010232807835564017}, {"id": 744, "seek": 421448, "start": 4226.24, "end": 4231.599999999999, "text": " I mean, I can't be sure of that. I think there's a huge similarity. Yeah. I was thinking that,", "tokens": [50952, 286, 914, 11, 286, 393, 380, 312, 988, 295, 300, 13, 286, 519, 456, 311, 257, 2603, 32194, 13, 865, 13, 286, 390, 1953, 300, 11, 51220], "temperature": 0.0, "avg_logprob": -0.12816257164126538, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0010232807835564017}, {"id": 745, "seek": 421448, "start": 4231.599999999999, "end": 4236.48, "text": " actually, just when you were speaking. But it's funny because we've spent a lot of time arguing", "tokens": [51220, 767, 11, 445, 562, 291, 645, 4124, 13, 583, 309, 311, 4074, 570, 321, 600, 4418, 257, 688, 295, 565, 19697, 51464], "temperature": 0.0, "avg_logprob": -0.12816257164126538, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0010232807835564017}, {"id": 746, "seek": 421448, "start": 4236.48, "end": 4242.639999999999, "text": " with each other about it. And I often feel like we're coming from very different perspectives", "tokens": [51464, 365, 1184, 661, 466, 309, 13, 400, 286, 2049, 841, 411, 321, 434, 1348, 490, 588, 819, 16766, 51772], "temperature": 0.0, "avg_logprob": -0.12816257164126538, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0010232807835564017}, {"id": 747, "seek": 424264, "start": 4243.200000000001, "end": 4249.84, "text": " on this. But in fact, I think there's convergence, really. What are your areas of disagreement?", "tokens": [50392, 322, 341, 13, 583, 294, 1186, 11, 286, 519, 456, 311, 32181, 11, 534, 13, 708, 366, 428, 3179, 295, 38947, 30, 50724], "temperature": 0.0, "avg_logprob": -0.1401970454624721, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.009862184524536133}, {"id": 748, "seek": 424264, "start": 4253.360000000001, "end": 4260.08, "text": " Well, you see, I would have thought that Andrew would have been more on the side of,", "tokens": [50900, 1042, 11, 291, 536, 11, 286, 576, 362, 1194, 300, 10110, 576, 362, 668, 544, 322, 264, 1252, 295, 11, 51236], "temperature": 0.0, "avg_logprob": -0.1401970454624721, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.009862184524536133}, {"id": 749, "seek": 424264, "start": 4260.08, "end": 4267.280000000001, "text": " we can do things without embodiment, and without grounding, or to kind of take grounding in a", "tokens": [51236, 321, 393, 360, 721, 1553, 28935, 2328, 11, 293, 1553, 46727, 11, 420, 281, 733, 295, 747, 46727, 294, 257, 51596], "temperature": 0.0, "avg_logprob": -0.1401970454624721, "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.009862184524536133}, {"id": 750, "seek": 426728, "start": 4268.08, "end": 4278.96, "text": " more liberal sense. Because some people would talk about grounding, so they say that large", "tokens": [50404, 544, 13767, 2020, 13, 1436, 512, 561, 576, 751, 466, 46727, 11, 370, 436, 584, 300, 2416, 50948], "temperature": 0.0, "avg_logprob": -0.13796892283875264, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.003989007323980331}, {"id": 751, "seek": 426728, "start": 4278.96, "end": 4286.16, "text": " language models, they are grounded. Prompt Engineering is the process of using prompt", "tokens": [50948, 2856, 5245, 11, 436, 366, 23535, 13, 15833, 662, 16215, 307, 264, 1399, 295, 1228, 12391, 51308], "temperature": 0.0, "avg_logprob": -0.13796892283875264, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.003989007323980331}, {"id": 752, "seek": 426728, "start": 4286.16, "end": 4292.32, "text": " prefixes to allow LLMs to understand better. So the context and the purpose of a conversation", "tokens": [51308, 18417, 36005, 281, 2089, 441, 43, 26386, 281, 1223, 1101, 13, 407, 264, 4319, 293, 264, 4334, 295, 257, 3761, 51616], "temperature": 0.0, "avg_logprob": -0.13796892283875264, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.003989007323980331}, {"id": 753, "seek": 426728, "start": 4292.32, "end": 4296.8, "text": " in order to generate more appropriate responses. What do you think is going on with Prompt", "tokens": [51616, 294, 1668, 281, 8460, 544, 6854, 13019, 13, 708, 360, 291, 519, 307, 516, 322, 365, 15833, 662, 51840], "temperature": 0.0, "avg_logprob": -0.13796892283875264, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.003989007323980331}, {"id": 754, "seek": 429680, "start": 4296.8, "end": 4302.56, "text": " Engineering? Yeah. Well, yeah. So you let's probably let slip a phrase there. So the process", "tokens": [50364, 16215, 30, 865, 13, 1042, 11, 1338, 13, 407, 291, 718, 311, 1391, 718, 11140, 257, 9535, 456, 13, 407, 264, 1399, 50652], "temperature": 0.0, "avg_logprob": -0.18194064264712126, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0018296890193596482}, {"id": 755, "seek": 429680, "start": 4302.56, "end": 4307.12, "text": " of allowing the models to understand better is what you're better. Of course, I don't think", "tokens": [50652, 295, 8293, 264, 5245, 281, 1223, 1101, 307, 437, 291, 434, 1101, 13, 2720, 1164, 11, 286, 500, 380, 519, 50880], "temperature": 0.0, "avg_logprob": -0.18194064264712126, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0018296890193596482}, {"id": 756, "seek": 429680, "start": 4307.12, "end": 4312.0, "text": " guilty as charged. I don't think I don't think that's the right way of characterizing it at all.", "tokens": [50880, 12341, 382, 11109, 13, 286, 500, 380, 519, 286, 500, 380, 519, 300, 311, 264, 558, 636, 295, 2517, 3319, 309, 412, 439, 13, 51124], "temperature": 0.0, "avg_logprob": -0.18194064264712126, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0018296890193596482}, {"id": 757, "seek": 429680, "start": 4313.28, "end": 4319.76, "text": " So I mean, I think the whole thing of Prompt Engineering is utterly fascinating. And it's", "tokens": [51188, 407, 286, 914, 11, 286, 519, 264, 1379, 551, 295, 15833, 662, 16215, 307, 30251, 10343, 13, 400, 309, 311, 51512], "temperature": 0.0, "avg_logprob": -0.18194064264712126, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0018296890193596482}, {"id": 758, "seek": 429680, "start": 4319.76, "end": 4324.56, "text": " something that's entered our world as AI researchers, very prominently, just in the", "tokens": [51512, 746, 300, 311, 9065, 527, 1002, 382, 7318, 10309, 11, 588, 39225, 2276, 11, 445, 294, 264, 51752], "temperature": 0.0, "avg_logprob": -0.18194064264712126, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0018296890193596482}, {"id": 759, "seek": 432456, "start": 4324.56, "end": 4330.0, "text": " last two years. And it's amazing. Of course, we have Prompt Engineering in the context of", "tokens": [50364, 1036, 732, 924, 13, 400, 309, 311, 2243, 13, 2720, 1164, 11, 321, 362, 15833, 662, 16215, 294, 264, 4319, 295, 50636], "temperature": 0.0, "avg_logprob": -0.11122510721395304, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.004203242249786854}, {"id": 760, "seek": 432456, "start": 4330.0, "end": 4335.76, "text": " large language models. We also have Prompt Engineering in the context of the generative", "tokens": [50636, 2416, 2856, 5245, 13, 492, 611, 362, 15833, 662, 16215, 294, 264, 4319, 295, 264, 1337, 1166, 50924], "temperature": 0.0, "avg_logprob": -0.11122510721395304, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.004203242249786854}, {"id": 761, "seek": 432456, "start": 4337.360000000001, "end": 4344.320000000001, "text": " image models as well, like Dali and so on. And that's really fascinating as well, how by", "tokens": [51004, 3256, 5245, 382, 731, 11, 411, 413, 5103, 293, 370, 322, 13, 400, 300, 311, 534, 10343, 382, 731, 11, 577, 538, 51352], "temperature": 0.0, "avg_logprob": -0.11122510721395304, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.004203242249786854}, {"id": 762, "seek": 432456, "start": 4345.52, "end": 4350.64, "text": " engineering the Prompt to be just the right sort of thing, you can coax the model into doing", "tokens": [51412, 7043, 264, 15833, 662, 281, 312, 445, 264, 558, 1333, 295, 551, 11, 291, 393, 598, 2797, 264, 2316, 666, 884, 51668], "temperature": 0.0, "avg_logprob": -0.11122510721395304, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.004203242249786854}, {"id": 763, "seek": 435064, "start": 4350.64, "end": 4356.400000000001, "text": " something which you might not otherwise do. And it's a great example of how alien these things", "tokens": [50364, 746, 597, 291, 1062, 406, 5911, 360, 13, 400, 309, 311, 257, 869, 1365, 295, 577, 12319, 613, 721, 50652], "temperature": 0.0, "avg_logprob": -0.07559992164693853, "compression_ratio": 1.625, "no_speech_prob": 0.00971293542534113}, {"id": 764, "seek": 435064, "start": 4356.400000000001, "end": 4361.04, "text": " are. Because if you were giving a human being the same instructions, then you wouldn't necessarily", "tokens": [50652, 366, 13, 1436, 498, 291, 645, 2902, 257, 1952, 885, 264, 912, 9415, 11, 550, 291, 2759, 380, 4725, 50884], "temperature": 0.0, "avg_logprob": -0.07559992164693853, "compression_ratio": 1.625, "no_speech_prob": 0.00971293542534113}, {"id": 765, "seek": 435064, "start": 4361.04, "end": 4369.04, "text": " do quite what you do with either an LLM or an image model in order to get it to do the", "tokens": [50884, 360, 1596, 437, 291, 360, 365, 2139, 364, 441, 43, 44, 420, 364, 3256, 2316, 294, 1668, 281, 483, 309, 281, 360, 264, 51284], "temperature": 0.0, "avg_logprob": -0.07559992164693853, "compression_ratio": 1.625, "no_speech_prob": 0.00971293542534113}, {"id": 766, "seek": 435064, "start": 4369.04, "end": 4375.68, "text": " thing that you want it to do. You have to kind of get into the zone with these models and figure", "tokens": [51284, 551, 300, 291, 528, 309, 281, 360, 13, 509, 362, 281, 733, 295, 483, 666, 264, 6668, 365, 613, 5245, 293, 2573, 51616], "temperature": 0.0, "avg_logprob": -0.07559992164693853, "compression_ratio": 1.625, "no_speech_prob": 0.00971293542534113}, {"id": 767, "seek": 437568, "start": 4375.68, "end": 4380.8, "text": " out kind of what strange incantations are going to make it do the things that you want it to do.", "tokens": [50364, 484, 733, 295, 437, 5861, 834, 394, 763, 366, 516, 281, 652, 309, 360, 264, 721, 300, 291, 528, 309, 281, 360, 13, 50620], "temperature": 0.0, "avg_logprob": -0.09080755104452877, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.04584436118602753}, {"id": 768, "seek": 437568, "start": 4380.8, "end": 4386.320000000001, "text": " Now, I think an interesting thing is that we may be looking at a moment, a very short moments in", "tokens": [50620, 823, 11, 286, 519, 364, 1880, 551, 307, 300, 321, 815, 312, 1237, 412, 257, 1623, 11, 257, 588, 2099, 6065, 294, 50896], "temperature": 0.0, "avg_logprob": -0.09080755104452877, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.04584436118602753}, {"id": 769, "seek": 437568, "start": 4386.320000000001, "end": 4392.88, "text": " the history of the field where Prompt Engineering is relevant. Because if language models become", "tokens": [50896, 264, 2503, 295, 264, 2519, 689, 15833, 662, 16215, 307, 7340, 13, 1436, 498, 2856, 5245, 1813, 51224], "temperature": 0.0, "avg_logprob": -0.09080755104452877, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.04584436118602753}, {"id": 770, "seek": 437568, "start": 4392.88, "end": 4398.08, "text": " good enough, then we're not going to need to talk to them in this weird way,", "tokens": [51224, 665, 1547, 11, 550, 321, 434, 406, 516, 281, 643, 281, 751, 281, 552, 294, 341, 3657, 636, 11, 51484], "temperature": 0.0, "avg_logprob": -0.09080755104452877, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.04584436118602753}, {"id": 771, "seek": 437568, "start": 4398.8, "end": 4403.68, "text": " engineer the Prompt to get them to do what we want them to do. It's going to be a lot easier.", "tokens": [51520, 11403, 264, 15833, 662, 281, 483, 552, 281, 360, 437, 321, 528, 552, 281, 360, 13, 467, 311, 516, 281, 312, 257, 688, 3571, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09080755104452877, "compression_ratio": 1.7201492537313432, "no_speech_prob": 0.04584436118602753}, {"id": 772, "seek": 440368, "start": 4403.92, "end": 4407.4400000000005, "text": " Anyway, so maybe that will be the case. I mean, that makes a lot of sense that that will be the", "tokens": [50376, 5684, 11, 370, 1310, 300, 486, 312, 264, 1389, 13, 286, 914, 11, 300, 1669, 257, 688, 295, 2020, 300, 300, 486, 312, 264, 50552], "temperature": 0.0, "avg_logprob": -0.11500869813512583, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0063381618820130825}, {"id": 773, "seek": 440368, "start": 4407.4400000000005, "end": 4416.4800000000005, "text": " case as they get better. But at the moment, you can use a strange incantation like thinking steps", "tokens": [50552, 1389, 382, 436, 483, 1101, 13, 583, 412, 264, 1623, 11, 291, 393, 764, 257, 5861, 834, 394, 399, 411, 1953, 4439, 51004], "temperature": 0.0, "avg_logprob": -0.11500869813512583, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0063381618820130825}, {"id": 774, "seek": 440368, "start": 4416.4800000000005, "end": 4421.84, "text": " and suddenly the large language model will be much more effective on reasoning problems than it", "tokens": [51004, 293, 5800, 264, 2416, 2856, 2316, 486, 312, 709, 544, 4942, 322, 21577, 2740, 813, 309, 51272], "temperature": 0.0, "avg_logprob": -0.11500869813512583, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0063381618820130825}, {"id": 775, "seek": 440368, "start": 4421.84, "end": 4425.92, "text": " was if you didn't use the incantation thinking steps. So that's really fascinating. So what's", "tokens": [51272, 390, 498, 291, 994, 380, 764, 264, 834, 394, 399, 1953, 4439, 13, 407, 300, 311, 534, 10343, 13, 407, 437, 311, 51476], "temperature": 0.0, "avg_logprob": -0.11500869813512583, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0063381618820130825}, {"id": 776, "seek": 440368, "start": 4425.92, "end": 4430.96, "text": " going on there? Well, I mean, I think what's going on there is that we have to again bear in mind that", "tokens": [51476, 516, 322, 456, 30, 1042, 11, 286, 914, 11, 286, 519, 437, 311, 516, 322, 456, 307, 300, 321, 362, 281, 797, 6155, 294, 1575, 300, 51728], "temperature": 0.0, "avg_logprob": -0.11500869813512583, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0063381618820130825}, {"id": 777, "seek": 443096, "start": 4430.96, "end": 4437.68, "text": " what the model is really trained to do is next word prediction. But we have to remember that", "tokens": [50364, 437, 264, 2316, 307, 534, 8895, 281, 360, 307, 958, 1349, 17630, 13, 583, 321, 362, 281, 1604, 300, 50700], "temperature": 0.0, "avg_logprob": -0.12017535054406454, "compression_ratio": 1.9119170984455958, "no_speech_prob": 0.012775112874805927}, {"id": 778, "seek": 443096, "start": 4437.68, "end": 4446.24, "text": " it's doing next word prediction in this unimaginably complex distribution. So we have to remember", "tokens": [50700, 309, 311, 884, 958, 1349, 17630, 294, 341, 517, 44976, 1188, 3997, 7316, 13, 407, 321, 362, 281, 1604, 51128], "temperature": 0.0, "avg_logprob": -0.12017535054406454, "compression_ratio": 1.9119170984455958, "no_speech_prob": 0.012775112874805927}, {"id": 779, "seek": 443096, "start": 4446.24, "end": 4451.76, "text": " that it's not just the distribution of what a single human would, the distribution of", "tokens": [51128, 300, 309, 311, 406, 445, 264, 7316, 295, 437, 257, 2167, 1952, 576, 11, 264, 7316, 295, 51404], "temperature": 0.0, "avg_logprob": -0.12017535054406454, "compression_ratio": 1.9119170984455958, "no_speech_prob": 0.012775112874805927}, {"id": 780, "seek": 443096, "start": 4451.76, "end": 4456.4800000000005, "text": " the sequence of words that a single human will come out with, but of all the sort of text of", "tokens": [51404, 264, 8310, 295, 2283, 300, 257, 2167, 1952, 486, 808, 484, 365, 11, 457, 295, 439, 264, 1333, 295, 2487, 295, 51640], "temperature": 0.0, "avg_logprob": -0.12017535054406454, "compression_ratio": 1.9119170984455958, "no_speech_prob": 0.012775112874805927}, {"id": 781, "seek": 445648, "start": 4456.799999999999, "end": 4462.639999999999, "text": " millions of humans on the internet, plus actually a load of other stuff like code and", "tokens": [50380, 6803, 295, 6255, 322, 264, 4705, 11, 1804, 767, 257, 3677, 295, 661, 1507, 411, 3089, 293, 50672], "temperature": 0.0, "avg_logprob": -0.15671937921074, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.003365960670635104}, {"id": 782, "seek": 445648, "start": 4462.639999999999, "end": 4468.16, "text": " things which we don't come out with in ordinary everyday language. Well, people do it deep", "tokens": [50672, 721, 597, 321, 500, 380, 808, 484, 365, 294, 10547, 7429, 2856, 13, 1042, 11, 561, 360, 309, 2452, 50948], "temperature": 0.0, "avg_logprob": -0.15671937921074, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.003365960670635104}, {"id": 783, "seek": 445648, "start": 4468.16, "end": 4476.16, "text": " mind a bit, but that's deep. So it's this unimaginably complex distribution. And so I think what's", "tokens": [50948, 1575, 257, 857, 11, 457, 300, 311, 2452, 13, 407, 309, 311, 341, 517, 44976, 1188, 3997, 7316, 13, 400, 370, 286, 519, 437, 311, 51348], "temperature": 0.0, "avg_logprob": -0.15671937921074, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.003365960670635104}, {"id": 784, "seek": 445648, "start": 4476.16, "end": 4485.759999999999, "text": " happening with prompt engineering is that you're sort of channeling it into some portion of the", "tokens": [51348, 2737, 365, 12391, 7043, 307, 300, 291, 434, 1333, 295, 2269, 278, 309, 666, 512, 8044, 295, 264, 51828], "temperature": 0.0, "avg_logprob": -0.15671937921074, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.003365960670635104}, {"id": 785, "seek": 448576, "start": 4485.76, "end": 4494.64, "text": " distribution. So you're queuing it up with a prompt. And this kind of context is putting it", "tokens": [50364, 7316, 13, 407, 291, 434, 631, 9635, 309, 493, 365, 257, 12391, 13, 400, 341, 733, 295, 4319, 307, 3372, 309, 50808], "temperature": 0.0, "avg_logprob": -0.09428071975708008, "compression_ratio": 2.0, "no_speech_prob": 0.002485735807567835}, {"id": 786, "seek": 448576, "start": 4494.64, "end": 4499.6, "text": " into some portion of this distribution. And that is what's going to enable it to do something", "tokens": [50808, 666, 512, 8044, 295, 341, 7316, 13, 400, 300, 307, 437, 311, 516, 281, 9528, 309, 281, 360, 746, 51056], "temperature": 0.0, "avg_logprob": -0.09428071975708008, "compression_ratio": 2.0, "no_speech_prob": 0.002485735807567835}, {"id": 787, "seek": 448576, "start": 4499.6, "end": 4504.56, "text": " different than it would have done if you had a different set of words. And that would have", "tokens": [51056, 819, 813, 309, 576, 362, 1096, 498, 291, 632, 257, 819, 992, 295, 2283, 13, 400, 300, 576, 362, 51304], "temperature": 0.0, "avg_logprob": -0.09428071975708008, "compression_ratio": 2.0, "no_speech_prob": 0.002485735807567835}, {"id": 788, "seek": 448576, "start": 4504.56, "end": 4510.24, "text": " put it in a different part of the distribution. So you're kind of finding the bit of this unimaginably", "tokens": [51304, 829, 309, 294, 257, 819, 644, 295, 264, 7316, 13, 407, 291, 434, 733, 295, 5006, 264, 857, 295, 341, 517, 44976, 1188, 51588], "temperature": 0.0, "avg_logprob": -0.09428071975708008, "compression_ratio": 2.0, "no_speech_prob": 0.002485735807567835}, {"id": 789, "seek": 448576, "start": 4510.24, "end": 4515.04, "text": " complex distribution. You're finding the bit of it that you want to then concentrate on.", "tokens": [51588, 3997, 7316, 13, 509, 434, 5006, 264, 857, 295, 309, 300, 291, 528, 281, 550, 18089, 322, 13, 51828], "temperature": 0.0, "avg_logprob": -0.09428071975708008, "compression_ratio": 2.0, "no_speech_prob": 0.002485735807567835}, {"id": 790, "seek": 451504, "start": 4515.12, "end": 4519.28, "text": " Yeah, so intuitively, I agree, because I think there's two ways of looking at this. So I agree", "tokens": [50368, 865, 11, 370, 46506, 11, 286, 3986, 11, 570, 286, 519, 456, 311, 732, 2098, 295, 1237, 412, 341, 13, 407, 286, 3986, 50576], "temperature": 0.0, "avg_logprob": -0.09319151364839993, "compression_ratio": 1.7400611620795108, "no_speech_prob": 0.004188902210444212}, {"id": 791, "seek": 451504, "start": 4519.28, "end": 4522.96, "text": " with you that they are statistical language models. I'm also a fan of the spline theory of neural", "tokens": [50576, 365, 291, 300, 436, 366, 22820, 2856, 5245, 13, 286, 478, 611, 257, 3429, 295, 264, 4732, 533, 5261, 295, 18161, 50760], "temperature": 0.0, "avg_logprob": -0.09319151364839993, "compression_ratio": 1.7400611620795108, "no_speech_prob": 0.004188902210444212}, {"id": 792, "seek": 451504, "start": 4522.96, "end": 4528.4, "text": " networks, which is this idea that you just kind of tessellate the ambient space into these little", "tokens": [50760, 9590, 11, 597, 307, 341, 1558, 300, 291, 445, 733, 295, 256, 7357, 285, 473, 264, 22997, 1901, 666, 613, 707, 51032], "temperature": 0.0, "avg_logprob": -0.09319151364839993, "compression_ratio": 1.7400611620795108, "no_speech_prob": 0.004188902210444212}, {"id": 793, "seek": 451504, "start": 4529.04, "end": 4534.4, "text": " affine polyhedra. And it's a little bit like a locality sensitive hashing table. But that's", "tokens": [51064, 2096, 533, 6754, 27096, 424, 13, 400, 309, 311, 257, 707, 857, 411, 257, 1628, 1860, 9477, 575, 571, 3199, 13, 583, 300, 311, 51332], "temperature": 0.0, "avg_logprob": -0.09319151364839993, "compression_ratio": 1.7400611620795108, "no_speech_prob": 0.004188902210444212}, {"id": 794, "seek": 451504, "start": 4534.4, "end": 4538.8, "text": " quite, it's quite a simple way of looking at it, because you were talking about emergence before.", "tokens": [51332, 1596, 11, 309, 311, 1596, 257, 2199, 636, 295, 1237, 412, 309, 11, 570, 291, 645, 1417, 466, 36211, 949, 13, 51552], "temperature": 0.0, "avg_logprob": -0.09319151364839993, "compression_ratio": 1.7400611620795108, "no_speech_prob": 0.004188902210444212}, {"id": 795, "seek": 451504, "start": 4538.8, "end": 4543.6, "text": " And emergence is all about this paradigmatic surprise, a bit like the mind body dualism,", "tokens": [51552, 400, 36211, 307, 439, 466, 341, 24709, 2399, 6365, 11, 257, 857, 411, 264, 1575, 1772, 11848, 1434, 11, 51792], "temperature": 0.0, "avg_logprob": -0.09319151364839993, "compression_ratio": 1.7400611620795108, "no_speech_prob": 0.004188902210444212}, {"id": 796, "seek": 454360, "start": 4543.6, "end": 4547.200000000001, "text": " if you like, there's something that happens up here, which is paradigmatically", "tokens": [50364, 498, 291, 411, 11, 456, 311, 746, 300, 2314, 493, 510, 11, 597, 307, 24709, 5030, 50544], "temperature": 0.0, "avg_logprob": -0.08402250478933523, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.004021258093416691}, {"id": 797, "seek": 454360, "start": 4547.200000000001, "end": 4550.88, "text": " completely different to what happens down there. So on the one hand, we're kind of saying, oh,", "tokens": [50544, 2584, 819, 281, 437, 2314, 760, 456, 13, 407, 322, 264, 472, 1011, 11, 321, 434, 733, 295, 1566, 11, 1954, 11, 50728], "temperature": 0.0, "avg_logprob": -0.08402250478933523, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.004021258093416691}, {"id": 798, "seek": 454360, "start": 4550.88, "end": 4555.68, "text": " they're just simple interpolators or statistical models. But on the other hand,", "tokens": [50728, 436, 434, 445, 2199, 44902, 3391, 420, 22820, 5245, 13, 583, 322, 264, 661, 1011, 11, 50968], "temperature": 0.0, "avg_logprob": -0.08402250478933523, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.004021258093416691}, {"id": 799, "seek": 454360, "start": 4555.68, "end": 4559.6, "text": " they really are doing something remarkable up here. So, so which is it?", "tokens": [50968, 436, 534, 366, 884, 746, 12802, 493, 510, 13, 407, 11, 370, 597, 307, 309, 30, 51164], "temperature": 0.0, "avg_logprob": -0.08402250478933523, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.004021258093416691}, {"id": 800, "seek": 454360, "start": 4560.4800000000005, "end": 4567.76, "text": " Which is it? Well, I mean, it's both, right? So, so, so, you know, if we want to understand", "tokens": [51208, 3013, 307, 309, 30, 1042, 11, 286, 914, 11, 309, 311, 1293, 11, 558, 30, 407, 11, 370, 11, 370, 11, 291, 458, 11, 498, 321, 528, 281, 1223, 51572], "temperature": 0.0, "avg_logprob": -0.08402250478933523, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.004021258093416691}, {"id": 801, "seek": 456776, "start": 4568.72, "end": 4575.04, "text": " these models in a in a in a more scientific way, which we surely do, you know, even if we're not,", "tokens": [50412, 613, 5245, 294, 257, 294, 257, 294, 257, 544, 8134, 636, 11, 597, 321, 11468, 360, 11, 291, 458, 11, 754, 498, 321, 434, 406, 11, 50728], "temperature": 0.0, "avg_logprob": -0.17431591628888332, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.008960133418440819}, {"id": 802, "seek": 456776, "start": 4575.04, "end": 4579.280000000001, "text": " even if we're not engineering them in an old fashioned sense of engineering them,", "tokens": [50728, 754, 498, 321, 434, 406, 7043, 552, 294, 364, 1331, 40646, 2020, 295, 7043, 552, 11, 50940], "temperature": 0.0, "avg_logprob": -0.17431591628888332, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.008960133418440819}, {"id": 803, "seek": 456776, "start": 4579.280000000001, "end": 4584.4800000000005, "text": " but, but rather they kind of, you know, emerge from the, from the learning process,", "tokens": [50940, 457, 11, 457, 2831, 436, 733, 295, 11, 291, 458, 11, 21511, 490, 264, 11, 490, 264, 2539, 1399, 11, 51200], "temperature": 0.0, "avg_logprob": -0.17431591628888332, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.008960133418440819}, {"id": 804, "seek": 456776, "start": 4584.4800000000005, "end": 4590.0, "text": " we still want to reverse engineer them to try and get as great as as as comprehensive", "tokens": [51200, 321, 920, 528, 281, 9943, 11403, 552, 281, 853, 293, 483, 382, 869, 382, 382, 382, 13914, 51476], "temperature": 0.0, "avg_logprob": -0.17431591628888332, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.008960133418440819}, {"id": 805, "seek": 456776, "start": 4590.8, "end": 4595.52, "text": " a scientific understanding of these things as possible. So, so we want to understand it all", "tokens": [51516, 257, 8134, 3701, 295, 613, 721, 382, 1944, 13, 407, 11, 370, 321, 528, 281, 1223, 309, 439, 51752], "temperature": 0.0, "avg_logprob": -0.17431591628888332, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.008960133418440819}, {"id": 806, "seek": 459552, "start": 4595.52, "end": 4599.360000000001, "text": " these levels, right? We, of course, the foundation of that understanding is that we", "tokens": [50364, 613, 4358, 11, 558, 30, 492, 11, 295, 1164, 11, 264, 7030, 295, 300, 3701, 307, 300, 321, 50556], "temperature": 0.0, "avg_logprob": -0.12657949166704519, "compression_ratio": 2.0456273764258555, "no_speech_prob": 0.010736299678683281}, {"id": 807, "seek": 459552, "start": 4599.360000000001, "end": 4602.72, "text": " need to understand the actual mechanisms that we've programmed in there, right? So,", "tokens": [50556, 643, 281, 1223, 264, 3539, 15902, 300, 321, 600, 31092, 294, 456, 11, 558, 30, 407, 11, 50724], "temperature": 0.0, "avg_logprob": -0.12657949166704519, "compression_ratio": 2.0456273764258555, "no_speech_prob": 0.010736299678683281}, {"id": 808, "seek": 459552, "start": 4602.72, "end": 4606.320000000001, "text": " though, you know, so you've that's essential, you want to, you know, if you want to really", "tokens": [50724, 1673, 11, 291, 458, 11, 370, 291, 600, 300, 311, 7115, 11, 291, 528, 281, 11, 291, 458, 11, 498, 291, 528, 281, 534, 50904], "temperature": 0.0, "avg_logprob": -0.12657949166704519, "compression_ratio": 2.0456273764258555, "no_speech_prob": 0.010736299678683281}, {"id": 809, "seek": 459552, "start": 4606.320000000001, "end": 4611.6, "text": " understand these things, you've got to understand transformer architectures, the different kinds", "tokens": [50904, 1223, 613, 721, 11, 291, 600, 658, 281, 1223, 31782, 6331, 1303, 11, 264, 819, 3685, 51168], "temperature": 0.0, "avg_logprob": -0.12657949166704519, "compression_ratio": 2.0456273764258555, "no_speech_prob": 0.010736299678683281}, {"id": 810, "seek": 459552, "start": 4611.6, "end": 4616.320000000001, "text": " of transformer architectures that you've got, the, you know, what happens when you use kind of", "tokens": [51168, 295, 31782, 6331, 1303, 300, 291, 600, 658, 11, 264, 11, 291, 458, 11, 437, 2314, 562, 291, 764, 733, 295, 51404], "temperature": 0.0, "avg_logprob": -0.12657949166704519, "compression_ratio": 2.0456273764258555, "no_speech_prob": 0.010736299678683281}, {"id": 811, "seek": 459552, "start": 4616.320000000001, "end": 4621.68, "text": " different parameter settings, whether it's sparse or dense, whether it's a decoder only", "tokens": [51404, 819, 13075, 6257, 11, 1968, 309, 311, 637, 11668, 420, 18011, 11, 1968, 309, 311, 257, 979, 19866, 787, 51672], "temperature": 0.0, "avg_logprob": -0.12657949166704519, "compression_ratio": 2.0456273764258555, "no_speech_prob": 0.010736299678683281}, {"id": 812, "seek": 462168, "start": 4621.76, "end": 4626.0, "text": " architecture, or how you're doing the tokenization, how you're doing the embedding,", "tokens": [50368, 9482, 11, 420, 577, 291, 434, 884, 264, 14862, 2144, 11, 577, 291, 434, 884, 264, 12240, 3584, 11, 50580], "temperature": 0.0, "avg_logprob": -0.1098422860740719, "compression_ratio": 1.975, "no_speech_prob": 0.0028509811963886023}, {"id": 813, "seek": 462168, "start": 4626.0, "end": 4629.76, "text": " when all of these things are essential to understanding, you know, and that's all at the", "tokens": [50580, 562, 439, 295, 613, 721, 366, 7115, 281, 3701, 11, 291, 458, 11, 293, 300, 311, 439, 412, 264, 50768], "temperature": 0.0, "avg_logprob": -0.1098422860740719, "compression_ratio": 1.975, "no_speech_prob": 0.0028509811963886023}, {"id": 814, "seek": 462168, "start": 4629.76, "end": 4634.72, "text": " absolutely at the engineering level. So we want to understand all of that. But then we can do a", "tokens": [50768, 3122, 412, 264, 7043, 1496, 13, 407, 321, 528, 281, 1223, 439, 295, 300, 13, 583, 550, 321, 393, 360, 257, 51016], "temperature": 0.0, "avg_logprob": -0.1098422860740719, "compression_ratio": 1.975, "no_speech_prob": 0.0028509811963886023}, {"id": 815, "seek": 462168, "start": 4634.72, "end": 4638.64, "text": " whole load of reverse engineering, you know, at another level, and do the sort of thing that", "tokens": [51016, 1379, 3677, 295, 9943, 7043, 11, 291, 458, 11, 412, 1071, 1496, 11, 293, 360, 264, 1333, 295, 551, 300, 51212], "temperature": 0.0, "avg_logprob": -0.1098422860740719, "compression_ratio": 1.975, "no_speech_prob": 0.0028509811963886023}, {"id": 816, "seek": 462168, "start": 4638.64, "end": 4645.200000000001, "text": " the people at Anthropic AI have done, for example, with, with these induction heads and, and, and,", "tokens": [51212, 264, 561, 412, 12727, 39173, 7318, 362, 1096, 11, 337, 1365, 11, 365, 11, 365, 613, 33371, 8050, 293, 11, 293, 11, 293, 11, 51540], "temperature": 0.0, "avg_logprob": -0.1098422860740719, "compression_ratio": 1.975, "no_speech_prob": 0.0028509811963886023}, {"id": 817, "seek": 462168, "start": 4645.200000000001, "end": 4650.320000000001, "text": " and understanding in terms of transformers in terms of residual streams and induction heads,", "tokens": [51540, 293, 3701, 294, 2115, 295, 4088, 433, 294, 2115, 295, 27980, 15842, 293, 33371, 8050, 11, 51796], "temperature": 0.0, "avg_logprob": -0.1098422860740719, "compression_ratio": 1.975, "no_speech_prob": 0.0028509811963886023}, {"id": 818, "seek": 465032, "start": 4650.32, "end": 4653.92, "text": " which I think is fabulous work. So that kind of thing is looking, it's still", "tokens": [50364, 597, 286, 519, 307, 17692, 589, 13, 407, 300, 733, 295, 551, 307, 1237, 11, 309, 311, 920, 50544], "temperature": 0.0, "avg_logprob": -0.12917017167614353, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.005488347727805376}, {"id": 819, "seek": 465032, "start": 4654.5599999999995, "end": 4659.5199999999995, "text": " quite a low level, but it's kind of the next level up, and explaining a little bit about how these", "tokens": [50576, 1596, 257, 2295, 1496, 11, 457, 309, 311, 733, 295, 264, 958, 1496, 493, 11, 293, 13468, 257, 707, 857, 466, 577, 613, 50824], "temperature": 0.0, "avg_logprob": -0.12917017167614353, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.005488347727805376}, {"id": 820, "seek": 465032, "start": 4659.5199999999995, "end": 4665.36, "text": " things work, and work along those lines, I think is like really essential. And then the more complex", "tokens": [50824, 721, 589, 11, 293, 589, 2051, 729, 3876, 11, 286, 519, 307, 411, 534, 7115, 13, 400, 550, 264, 544, 3997, 51116], "temperature": 0.0, "avg_logprob": -0.12917017167614353, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.005488347727805376}, {"id": 821, "seek": 465032, "start": 4665.36, "end": 4670.639999999999, "text": " these things are, the, you know, the heart, the more we need to kind of ascend these levels of", "tokens": [51116, 613, 721, 366, 11, 264, 11, 291, 458, 11, 264, 1917, 11, 264, 544, 321, 643, 281, 733, 295, 41604, 613, 4358, 295, 51380], "temperature": 0.0, "avg_logprob": -0.12917017167614353, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.005488347727805376}, {"id": 822, "seek": 465032, "start": 4670.639999999999, "end": 4677.12, "text": " understanding and, and, and, and, you know, and I hope that we can, but I mean, there's no one", "tokens": [51380, 3701, 293, 11, 293, 11, 293, 11, 293, 11, 291, 458, 11, 293, 286, 1454, 300, 321, 393, 11, 457, 286, 914, 11, 456, 311, 572, 472, 51704], "temperature": 0.0, "avg_logprob": -0.12917017167614353, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.005488347727805376}, {"id": 823, "seek": 467712, "start": 4677.12, "end": 4681.12, "text": " that is the right one. It's, you want to understand that things are all levels.", "tokens": [50364, 300, 307, 264, 558, 472, 13, 467, 311, 11, 291, 528, 281, 1223, 300, 721, 366, 439, 4358, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08215432380562399, "compression_ratio": 1.7380191693290734, "no_speech_prob": 0.009973536245524883}, {"id": 824, "seek": 467712, "start": 4681.12, "end": 4684.88, "text": " Yeah, different levels of description. You said something before, which really", "tokens": [50564, 865, 11, 819, 4358, 295, 3855, 13, 509, 848, 746, 949, 11, 597, 534, 50752], "temperature": 0.0, "avg_logprob": -0.08215432380562399, "compression_ratio": 1.7380191693290734, "no_speech_prob": 0.009973536245524883}, {"id": 825, "seek": 467712, "start": 4684.88, "end": 4689.76, "text": " interested me. You said when the language models get good enough, maybe we won't need the prompts", "tokens": [50752, 3102, 385, 13, 509, 848, 562, 264, 2856, 5245, 483, 665, 1547, 11, 1310, 321, 1582, 380, 643, 264, 41095, 50996], "temperature": 0.0, "avg_logprob": -0.08215432380562399, "compression_ratio": 1.7380191693290734, "no_speech_prob": 0.009973536245524883}, {"id": 826, "seek": 467712, "start": 4689.76, "end": 4694.64, "text": " anymore. And I'd love to explore that duality, because it's a similar duality to how we talk", "tokens": [50996, 3602, 13, 400, 286, 1116, 959, 281, 6839, 300, 11848, 507, 11, 570, 309, 311, 257, 2531, 11848, 507, 281, 577, 321, 751, 51240], "temperature": 0.0, "avg_logprob": -0.08215432380562399, "compression_ratio": 1.7380191693290734, "no_speech_prob": 0.009973536245524883}, {"id": 827, "seek": 467712, "start": 4694.64, "end": 4698.72, "text": " about embodiment, you know, you can think of the language model being embodied in the prompt in,", "tokens": [51240, 466, 28935, 2328, 11, 291, 458, 11, 291, 393, 519, 295, 264, 2856, 2316, 885, 42046, 294, 264, 12391, 294, 11, 51444], "temperature": 0.0, "avg_logprob": -0.08215432380562399, "compression_ratio": 1.7380191693290734, "no_speech_prob": 0.009973536245524883}, {"id": 828, "seek": 467712, "start": 4698.72, "end": 4703.599999999999, "text": " in some sense. So maybe we'll never get rid of the prompt. But just to think about these prompts,", "tokens": [51444, 294, 512, 2020, 13, 407, 1310, 321, 603, 1128, 483, 3973, 295, 264, 12391, 13, 583, 445, 281, 519, 466, 613, 41095, 11, 51688], "temperature": 0.0, "avg_logprob": -0.08215432380562399, "compression_ratio": 1.7380191693290734, "no_speech_prob": 0.009973536245524883}, {"id": 829, "seek": 470360, "start": 4703.6, "end": 4708.96, "text": " I think about them as a new type of program interpreter. And there are some remarkable", "tokens": [50364, 286, 519, 466, 552, 382, 257, 777, 2010, 295, 1461, 34132, 13, 400, 456, 366, 512, 12802, 50632], "temperature": 0.0, "avg_logprob": -0.1137079646569172, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.009898149408400059}, {"id": 830, "seek": 470360, "start": 4708.96, "end": 4713.92, "text": " examples of scratch pad and chain of thought and even algorithmic prompting for getting", "tokens": [50632, 5110, 295, 8459, 6887, 293, 5021, 295, 1194, 293, 754, 9284, 299, 12391, 278, 337, 1242, 50880], "temperature": 0.0, "avg_logprob": -0.1137079646569172, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.009898149408400059}, {"id": 831, "seek": 470360, "start": 4713.92, "end": 4719.200000000001, "text": " insane extrapolative performance on lots of, you know, standard reasoning tasks. Yeah, yeah.", "tokens": [50880, 10838, 48224, 1166, 3389, 322, 3195, 295, 11, 291, 458, 11, 3832, 21577, 9608, 13, 865, 11, 1338, 13, 51144], "temperature": 0.0, "avg_logprob": -0.1137079646569172, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.009898149408400059}, {"id": 832, "seek": 470360, "start": 4719.200000000001, "end": 4723.360000000001, "text": " And, you know, these, these models are not Turing machines, they're finite state", "tokens": [51144, 400, 11, 291, 458, 11, 613, 11, 613, 5245, 366, 406, 314, 1345, 8379, 11, 436, 434, 19362, 1785, 51352], "temperature": 0.0, "avg_logprob": -0.1137079646569172, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.009898149408400059}, {"id": 833, "seek": 470360, "start": 4723.360000000001, "end": 4727.68, "text": " automatics. So there are limits to what we can do. But I guess what I'm saying is the prompt", "tokens": [51352, 28034, 1167, 13, 407, 456, 366, 10406, 281, 437, 321, 393, 360, 13, 583, 286, 2041, 437, 286, 478, 1566, 307, 264, 12391, 51568], "temperature": 0.0, "avg_logprob": -0.1137079646569172, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.009898149408400059}, {"id": 834, "seek": 470360, "start": 4727.68, "end": 4732.160000000001, "text": " seems like it's not going away anytime soon. Yeah. So I think that I don't think the prompt is", "tokens": [51568, 2544, 411, 309, 311, 406, 516, 1314, 13038, 2321, 13, 865, 13, 407, 286, 519, 300, 286, 500, 380, 519, 264, 12391, 307, 51792], "temperature": 0.0, "avg_logprob": -0.1137079646569172, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.009898149408400059}, {"id": 835, "seek": 473216, "start": 4732.16, "end": 4737.599999999999, "text": " going to go away. But I think that the, and who knows, right? But, but I think that prompt", "tokens": [50364, 516, 281, 352, 1314, 13, 583, 286, 519, 300, 264, 11, 293, 567, 3255, 11, 558, 30, 583, 11, 457, 286, 519, 300, 12391, 50636], "temperature": 0.0, "avg_logprob": -0.1433750684144067, "compression_ratio": 1.8884462151394423, "no_speech_prob": 0.0037294048815965652}, {"id": 836, "seek": 473216, "start": 4737.599999999999, "end": 4746.0, "text": " engineering as a whole kind of thing in itself, you know, may, it may not be, you know, people", "tokens": [50636, 7043, 382, 257, 1379, 733, 295, 551, 294, 2564, 11, 291, 458, 11, 815, 11, 309, 815, 406, 312, 11, 291, 458, 11, 561, 51056], "temperature": 0.0, "avg_logprob": -0.1433750684144067, "compression_ratio": 1.8884462151394423, "no_speech_prob": 0.0037294048815965652}, {"id": 837, "seek": 473216, "start": 4746.0, "end": 4750.32, "text": " talk about that as being a kind of a whole new job description as prompt engineer. And so that,", "tokens": [51056, 751, 466, 300, 382, 885, 257, 733, 295, 257, 1379, 777, 1691, 3855, 382, 12391, 11403, 13, 400, 370, 300, 11, 51272], "temperature": 0.0, "avg_logprob": -0.1433750684144067, "compression_ratio": 1.8884462151394423, "no_speech_prob": 0.0037294048815965652}, {"id": 838, "seek": 473216, "start": 4750.32, "end": 4754.639999999999, "text": " that as a whole new job description, I'm not quite sure how long exactly that will last because,", "tokens": [51272, 300, 382, 257, 1379, 777, 1691, 3855, 11, 286, 478, 406, 1596, 988, 577, 938, 2293, 300, 486, 1036, 570, 11, 51488], "temperature": 0.0, "avg_logprob": -0.1433750684144067, "compression_ratio": 1.8884462151394423, "no_speech_prob": 0.0037294048815965652}, {"id": 839, "seek": 473216, "start": 4755.76, "end": 4760.4, "text": " because prompt prompting may be just, you know, interacting with a thing in a much more natural", "tokens": [51544, 570, 12391, 12391, 278, 815, 312, 445, 11, 291, 458, 11, 18017, 365, 257, 551, 294, 257, 709, 544, 3303, 51776], "temperature": 0.0, "avg_logprob": -0.1433750684144067, "compression_ratio": 1.8884462151394423, "no_speech_prob": 0.0037294048815965652}, {"id": 840, "seek": 476040, "start": 4761.36, "end": 4766.08, "text": " language way in the way we would with another person, right? So, you know, I don't, I don't,", "tokens": [50412, 2856, 636, 294, 264, 636, 321, 576, 365, 1071, 954, 11, 558, 30, 407, 11, 291, 458, 11, 286, 500, 380, 11, 286, 500, 380, 11, 50648], "temperature": 0.0, "avg_logprob": -0.09353356049439618, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.014542749151587486}, {"id": 841, "seek": 476040, "start": 4766.08, "end": 4771.28, "text": " when I, when I, I don't have to kind of think of some peculiar incantation in order to,", "tokens": [50648, 562, 286, 11, 562, 286, 11, 286, 500, 380, 362, 281, 733, 295, 519, 295, 512, 27149, 834, 394, 399, 294, 1668, 281, 11, 50908], "temperature": 0.0, "avg_logprob": -0.09353356049439618, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.014542749151587486}, {"id": 842, "seek": 476040, "start": 4772.639999999999, "end": 4780.08, "text": " you know, in order to get, you know, my colleagues to kind of help me on, on something or to, you", "tokens": [50976, 291, 458, 11, 294, 1668, 281, 483, 11, 291, 458, 11, 452, 7734, 281, 733, 295, 854, 385, 322, 11, 322, 746, 420, 281, 11, 291, 51348], "temperature": 0.0, "avg_logprob": -0.09353356049439618, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.014542749151587486}, {"id": 843, "seek": 476040, "start": 4780.08, "end": 4786.96, "text": " know, to cook a meal together with somebody, we just, we just use our natural kind of forms", "tokens": [51348, 458, 11, 281, 2543, 257, 6791, 1214, 365, 2618, 11, 321, 445, 11, 321, 445, 764, 527, 3303, 733, 295, 6422, 51692], "temperature": 0.0, "avg_logprob": -0.09353356049439618, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.014542749151587486}, {"id": 844, "seek": 478696, "start": 4786.96, "end": 4791.52, "text": " of communication. Of course, of course, it does involve, you know, discussion and negotiation,", "tokens": [50364, 295, 6101, 13, 2720, 1164, 11, 295, 1164, 11, 309, 775, 9494, 11, 291, 458, 11, 5017, 293, 27573, 11, 50592], "temperature": 0.0, "avg_logprob": -0.1494728670281879, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.019454924389719963}, {"id": 845, "seek": 478696, "start": 4791.52, "end": 4795.68, "text": " but it's in this, it's just the same as we use with other humans, right? So, so it may be that,", "tokens": [50592, 457, 309, 311, 294, 341, 11, 309, 311, 445, 264, 912, 382, 321, 764, 365, 661, 6255, 11, 558, 30, 407, 11, 370, 309, 815, 312, 300, 11, 50800], "temperature": 0.0, "avg_logprob": -0.1494728670281879, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.019454924389719963}, {"id": 846, "seek": 478696, "start": 4796.24, "end": 4801.52, "text": " rather than it being a peculiar thing in itself with all these funny phrases that just work", "tokens": [50828, 2831, 813, 309, 885, 257, 27149, 551, 294, 2564, 365, 439, 613, 4074, 20312, 300, 445, 589, 51092], "temperature": 0.0, "avg_logprob": -0.1494728670281879, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.019454924389719963}, {"id": 847, "seek": 478696, "start": 4801.52, "end": 4807.6, "text": " for peculiar eccentric reasons, that it may be much more natural. Amazing. Professor Shanahan,", "tokens": [51092, 337, 27149, 42629, 4112, 11, 300, 309, 815, 312, 709, 544, 3303, 13, 14165, 13, 8419, 25536, 21436, 11, 51396], "temperature": 0.0, "avg_logprob": -0.1494728670281879, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.019454924389719963}, {"id": 848, "seek": 478696, "start": 4808.4800000000005, "end": 4812.4, "text": " thank you so much for joining us today. Indeed, and thank you for the invitation. It's been lots", "tokens": [51440, 1309, 291, 370, 709, 337, 5549, 505, 965, 13, 15061, 11, 293, 1309, 291, 337, 264, 17890, 13, 467, 311, 668, 3195, 51636], "temperature": 0.0, "avg_logprob": -0.1494728670281879, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.019454924389719963}, {"id": 849, "seek": 481240, "start": 4812.4, "end": 4814.4, "text": " of fun. Absolute honor. Absolute honor. Why?", "tokens": [50364, 295, 1019, 13, 43965, 1169, 5968, 13, 43965, 1169, 5968, 13, 1545, 30, 50464], "temperature": 0.0, "avg_logprob": -0.4954492151737213, "compression_ratio": 1.2222222222222223, "no_speech_prob": 0.27693668007850647}], "language": "en"}