Peter Velichkovich is a staff research scientist at DeepMind.
He's firmly established himself as one of the most significant and up-and-coming researchers
in the deep learning space.
He invented graph attention networks in 2017 and he's been a leading light in the field
ever since, pioneering research in graph neural networks, geometric deep learning and also
neural algorithmic reasoning.
Recently he's been applying category theory to take the geometric deep learning ideas
one step further.
If you haven't already, you should check out our show that we did on the geometric deep
learning blueprint, which of course featured Peter and I caught up with him last week
at NeurIPS.
Enjoy.
Peter, it's fantastic to see you again.
So this is the first time that I've actually met you in person.
We did that really cool show together on geometric deep learning with your proto book with Takko.
I spoke with Takko yesterday, but also Michael Bronstein and Joanne Brunner.
So anyway, it's been a little while since we've really synced.
Now you've had this really, really interesting category theory series.
Can you start by just letting us know what you've been doing there?
Yeah, that's a great point and great to finally meet you in person, Tim.
It's really great to catch up after some time has passed.
And yeah, I mean, I like to think that all four of us, myself, Michael, Joanne and Takko
have a greater understanding of the implications of these methods since the last time we spoke.
If you remember back when we did our conversation, I kind of hinted at the fact that category
theory might hold some of the answers to maybe generalize some of these geometric concepts
beyond the notion of just pure symmetries.
And we believe that now we have a sufficient understanding of these kinds of things that
we were able to make this kind of mini course on categories for deep learning.
And to me, it really feels like the natural continuation of these concepts of geometric
deep learning into the realm beyond.
And I'll explain that in a moment.
But one other kind of very related point is that here at NeurIPS, we're actually presenting
a full conference paper which deals with using category theoretic tools to invent new kinds
of graph neural networks.
So basically, it's not just that we're throwing a bunch of new theory, it actually leads to
empirical findings that we can actionably use in our models day to day.
So that's one point.
That is incredible.
Can you sketch out the paper?
Yeah, sure.
So basically, maybe I'll first take a step back to explain why do we think categories
are important and in what sense they're kind of a step further from what geometric deep
learning already gives us.
So geometric deep learning concerns itself with giving us these equivalent layers, right?
So layers that are in some sense resistant to operations of these symmetry transformations
is that fundamentally change an object, but the object is still the same.
We still have all of it, right?
And this immediately implies that these symmetries have to be composable, invertible, all that
sort of stuff.
And yeah, essentially, the category theory framework is in some sense mindful of the
fact that while symmetries are a very nice way to reason about things that happen and
that we see in nature, they're often not completely an accurate representation of what happens.
Very often there are operations both in nature, but especially in general computation, like
say in algorithmic stuff, where an operation of an algorithm might destroy half of your
data.
So that is no longer a symmetry.
You cannot invert it.
But you might still be interested in building a neural network model that is in some sense
resistant to the operations of say this algorithm or this natural phenomenon that you're studying.
One simple example that maybe predates our work a little bit is building some kind of
equivalence to scaling operations.
Obviously if you scale or course in something, these are not always invertible transformations
because if you course in the pixels of your image, you cannot perfectly reconstruct where
you came from.
Yet you still might want to build a model that will give you the same answer as regardless
of how you scale up your input, right?
So these are obviously things that are going to be very important as we move to more generic
domains than ones that can be described purely through geometric and symmetry transformations,
right?
And in that sense, the same way we had groups, representations, and equivalence in geometric
deep learning, these are all special cases of categorical concepts like categories,
functors, and natural transformations, which basically generalize all the stuff of geometric
deep learning into their own beyond.
And in our paper, we try to use exactly these kinds of category theoretic tools to study
what it would mean to build a say graph neural network that is capable of behaving like a
classical computer science algorithm.
In the sense that if you have some data that's transformed by an algorithm, you may imagine,
say, a path finding algorithm where at every step in every node, you have your knowledge
of how far away is that node from the source vertex.
And one step of the algorithm kind of looks at all the immediate neighbors and updates
those beliefs of how far away you are.
And I'll say you want to have a GNN that simulates that, like we typically do in algorithmic
reasoning.
You take your algorithmic state, you encode it with a neural network into this high dimensional
space.
Your GNN then processes it to update the latent space.
And now you want to be able to decode it so that you predict whatever the next state
is going to be.
So you have something which in category theory we use a lot is known as a commutative diagram.
So basically it's saying you can either take the step of the algorithm or you can encode,
process, decode, and hopefully end up in the same place.
So category theory seems like a very nice language to study.
These kinds of, I won't call them symmetries, they're basically like interchangeable sequences
of operations because the step of an algorithm might not be invertible.
You might not be able to go back after you do one step of, you know, shortest path algorithm
because it's a contraction map, right?
When you find the final solution of a shortest path algorithm, you won't necessarily know
which previous state led you there because there could be many equivalent states that
could lead you to the same contracted solution, right?
So our method, using these category theory frameworks, try to characterize how these
graph neural networks align with a target algorithm that we might want to simulate.
Then we detect various ways not only to explain the code of graph neural networks from this
kind of perspective, but also it gives us a very interesting sort of, if you've done
any functional programming, a type checker of sorts to kind of detect whenever we're
using our representations in slightly broken ways.
So specifically to give you one very concrete example, in a categorical framework, just
like in functional programming, you expect your transformations to be functions.
That is, for every input there should be a unique output.
However, one thing that people very often do in graph representation learning, when
they want to predict outputs not only in the nodes, but also in the edges, is to reuse
the edge messages both as edge outputs and integrated overall the other messages to get
node outputs, right?
But this is a problem from the categorical perspective because this is no longer a function.
You cannot get a function that takes, you know, edges to edges plus nodes without sending
the same thing into two different places in this case, right?
And, you know, just because it mathematically breaks doesn't mean you cannot implement it.
In fact, 99% of the GNN implementations you'll find online will do this exactly in this particular
way.
DeepMindsGraphNet's library does this, for example.
However, you know, just because you can implement it doesn't mean that there's something not
potentially a bit tricky going on in the sense that you're putting a bit of representational
pressure on that edge message, right?
Because now it has to be used for two potentially very different things, both for some output
in the edges, but also it needs to be integratable into nodes where it predicts something potentially
wildly different.
And, you know, while gradient descent can take care of this and give you a model that
fits your training distribution well, you're not like, to deal with this pressure, it's
probably going to have to learn something which has nothing to do with the algorithm
that you want to align to.
And as a result, you're out of distribution, extrapolation performance is going to be much,
much worse.
And any self-respecting algorithm should extrapolate well.
That's the main property of algorithmic reasoning, right?
And we find that just by, you know, splitting this message function into two streams, one
which goes into the edges and one which goes into the nodes, we get basically significant
empirical benefits when extrapolating on edge-centric algorithms.
Yeah.
Amazing.
So, Epeta has just produced this incredible series which is available on YouTube.
Where can folks find it?
Yeah.
So, basically, if you just go to cats.4.ai, you can see all of the main series lectures
from our course, which starts off with assuming kind of a foundational knowledge of deep learning
with neural networks, back propagation, and so on, and then also tries to introduce these
concepts of category theory and how we can use them to rethink the way we might go about
some of our standard ideas in deep learning like compositionality or functional structure
of deep learning pipelines, or even how can we reinterpret back propagation from the
perspective of categorical theory.
And each lecture basically deals with one particular aspect and we try to keep it grounded
from the beginning to keep it motivated so every single lecture is aligning itself with
one particular top-tier paper that one of us has published on one of these venues like
in Europe.
And one thing I'll also mention is that the course is actually, in principle, still ongoing
because besides the main series of five lectures that myself, Bruno, Pym, and Andrew have given,
we also have several interesting guest lectures where we try to bring in other influential
people at the intersection of popularizing category theory with deep learning concepts
in a way that can bring an even wider area of views once you're kind of trained in the
basics of these techniques, how they're applied to various other things like causality.
We had Taco Cohen tell us about how he uses these concepts to reimagine causality through
a categorical lens.
We're going to have Tydenay Bradley, she's a very popular mathematics educator generally.
She will show how she used some of these concepts to explain transformers.
And one thing I'm very excited about early next year, we will have actually a guest talk
from David Spivak, which is one of the co-authors of the very famous Seven Sketches in Compositionality
book, which is what initially one of the things that got me really excited about category
theory in the first place.
So I'm really keen to hear all these perspectives as well.
The man is a legend.
And also on Taco, I interviewed him yesterday and his work on causality is really, really
exciting.
What would you say to people who might be intimidated or scared by category theory?
So one thing that I should mention here is that one point about being intimidated or
scared about category theory is that to really be able to utilize these ideas in how you
do research or build your models or anything, it does require a reasonably significant buy-in.
So this is not something that you can just read one blog post and suddenly you're empowered
to do it.
This is like one key thing.
But I would say the main thing that might make people a bit scared to do it is the fact
that many category theory resources out there are a bit guided towards mathematicians.
So they will tend to use the kind of language and the kind of examples that will be quite
attractive to someone who has studied, say, various kinds of differential geometry or
topology or something like this.
And these kinds of areas tend to generally scare off people who come from a more computer
science style background.
And basically I would say the answer to that is you need to find the right resource for
you.
Category theory is no more or no less than a way to take a bird's eye view of the phenomena
that you try to study.
And when you study these phenomena from high in the sky, details become invisible, but
you suddenly get a much better feel for the structure and you can utilize kind of the
nice patterns that reappear across various fields.
And this you would argue is kind of the essence of what we're trying to do in deep learning.
We have a lot of analogical way in which these architectures are constructed, right?
So cats for AI is one possible answer to that.
It's our way to kind of, as half of us are deep learners and half of us are category
theorists, trying to apply these techniques to deep learning.
We believe we have a sort of unique perspective of we and like we understand what makes people
afraid to try to talk about these things because some of us had to go through it ourselves
to deal with the way in which the materials are arranged online right now.
So yeah, maybe just these kinds of resources, starting with them and basically trying as
much as possible not to descend into the depths of NCAT lab as the very first thing that you
do can be a good way to maybe stay sane during the first few weeks or months of trying to
explore this field.
Wonderful.
I wondered if you could give a couple of examples of where category theory has been used in
an adjacent field.
I can think of too.
I can think of Rosen using category theory to describe, you know, sort of ecosystems
and life.
I can also think of some quantum mechanics folks that have come up with a category theoretical
conception of quantum mechanics.
Right.
Are there any other ones?
Yeah.
So I mean, I can start by giving the examples that I know about closest in terms of just
deep learning.
So one particular example that I think could be quite interesting is the work that was
published at NeurIPS two years ago, which I think is one of the first papers that really
tried to use categorical concepts to build these structures, is the natural graph networks
paper from Pimdehan, Tapocoin and Max Swelling, which effectively realizes the fact that the
way we build graph neural networks very often we have this one shared message function that's
applied everywhere on every single edge on every single graph that you get.
But in reality, is this necessary for it to be a legitimate graph neural network?
That's actually not the case because if I give you two completely non-isomorphic graphs,
if I choose to have completely different message functions in those two graphs, that's totally
fine because it's still a valid graph net.
If I permute any of those graphs, I'll get the permutation equivalent function for the
two of them separately.
There needs to be no weight sharing between them and naturally concepts like these.
So this kind of requires taking a step above the group theoretic view of geometric deep
learning and into the realm of what is known as a group poid.
You kind of imagine every single graph structure, isomorphic graph structure, living on a sort
of island of possible adjacency matrix representations of it.
And for those graphs living on those islands, you need to have some weight sharing.
But for separate islands, you don't need to have any weight sharing whatsoever.
Of course, in practice, these kinds of layers, you would need to have some kind of sharing
of weights in order to make them scalable to arbitrary new graph structures you haven't
seen at training time, but it allows you a lot more flexibility about how you go about
building your functions.
And you're no longer constrained to have just one function everywhere repeated, right?
So that's maybe one example that, at least to me, was what first motivated me and made
me realize that there's more to this stuff than just to say what group theory will give
us.
Amazing.
I'm really interested in your work in algorithmic reasoning, and I know you were just discussing
it as an adjacent thing, and very soon we want to make a show, actually, on your work
on that.
But if you wouldn't mind, could you just sketch out algorithmic reasoning?
Yes, wonderful.
So, very happy to.
Basically, what are we interested in algorithmic reasoning is building neural networks.
They tend to be graph neural networks, but generally speaking, neural networks that are
capable of executing algorithmic computation.
So if I give you some context on what is the state of a particular algorithm, can my network
somehow learn to execute that algorithm ideally in some latent space such that at every single
step of the way, I could if I wanted to decode the states of that algorithm.
So that's basically the main premise.
Why do we care about this?
Well, basically, I think of algorithms as a sort of basic foundational building block
of reasoning, and it's kind of a timeless principle where a software engineer reads
through one of these textbooks on algorithms and learns these 30 or 40 basis algorithms,
and then that knowledge serves them for life in a whole career of software engineering.
So basically, we have this hypothesis that you have this nice basis of algorithms that
if you can master how to do them robustly, you can try to mimic any kind of at least
polynomial time reasoning behavior.
And that's really nice because if you look at the way current state-of-the-art large-scale
models tend to have shortcomings, it's usually in those kinds of robust extrapolation problems.
Basically, if we want to have a really good AI scientist that's able to not just make
great sense of a bunch of training data from the internet, but also use that training data
to derive new knowledge, you need some robustified way to apply rules to get infinite knowledge
from finite means.
So basically, that's what we want to do.
We want to find ways inductive biases or training procedures to build neural networks
that are more algorithmically capable.
And in algorithmic reasoning, we obviously spent a lot of time trying to make this happen,
just building better graph neural networks that align better with target algorithms so
that you can execute them better, but then the really exciting part comes where we've
actually taken some of these graph neural networks that have been pre-trained to execute
one particular algorithm, and then we deployed it in a real-world problem where that algorithm
is required, and we achieved, say, significant representational benefits in terms of downstream
accuracy.
So the idea behind this, and I'll give an example from Google Maps.
This is an application that I worked on at DeepMind, so it's something that I've thought
about quite a bit.
We've invented these algorithms, like Dijkstra's algorithm, to be able to resolve these kinds
of real-world routing problems.
That's the kind of motivation for why you want to build the shortest path algorithm.
And it comes as a little surprise that when you have real-world traffic data, you might
be tempted to apply Dijkstra's algorithm to solve it, to route agents in traffic.
However, what is the actual data that, say, Google Maps has access to?
It's not this nice, abstractified graph with a single scalar in every edge where you can
just go ahead and apply an algorithm.
In fact, there's a huge bridge that must be built between the real data and the input
to the algorithm.
In fact, Google Maps data is typically people's cell phones in their cars, and the cars move,
the phones move, and then based on the movement of the phones, you somehow infer how fast the
car is going or something like that.
And this is very noisy, not very well-structured, and you have to somehow go from there to a
graph where you can apply this heuristic.
Previously, it was always done exclusively by humans, like feature engineers, effectively.
And whenever there's a human feature engineer in the loop like this, you are almost certainly
going to drop a lot of information that you might need to solve the problem.
So basically, you have a huge kind of bridge to cross there.
And with algorithmic reasoning, we now don't use Dijkstra's algorithm.
We use a high-dimensional graph net that was pre-trained to execute Dijkstra's algorithm
in a latent space.
So now this gives us a differentiable component that we can hook up to any encoder and decoder
function we want to, so we can go straight from raw data and code it into the GNN's latent
space, run the algorithm there, and then decode whatever it is that you need, like routing
the vehicles in traffic.
So now purely through backprop, this encoder function now learns to do what the human feature
engineer did.
It learns how to most effectively map that complicated, noisy, real-world data into the
latent space where this GNN can best do its thing.
That really is software 2.0.
But I wanted to ask you about the computational limitations, because you just said something
interesting about representing infinite objects with a finite memory.
So neural networks are not Turing machines, but they can extrapolate, of course.
What's the realistic limitation?
Let's say you're trying to learn an algorithm, how far can you go with a neural network?
So the thing is, there are cases where you can go very far.
We do have theory that is very robust about this, and I think it's theory that is actually
quite easily understandable.
So let me try to kind of visualize it.
When you have a real UMLP, your standard universal approximator, it's basically a piecewise
linear function.
So as you go far enough away from the training data, you're going to hit that level of extrapolation
where you hit the linear part of the piecewise linear.
And at that point, if your target function is not linear, no extrapolation is going to
happen.
You're not going to fit the function properly.
So what's one outcome of this theory is that if you use real UMLPs, this was a great paper
from MIT a few years back, which showed that basically you need to line up parts of your
neural network such that they learn linear functions in the target.
And that's the reason why, say, when you want to imitate a pathfinding algorithm, you want
to use a max aggregation, your GNN, and not sum, where sum is universal.
It can fit anything.
But the function you have to learn, because pathfinding is like minimum overall neighbors
of distance to neighbor plus the edge weight, suddenly when you put max in there, it's a
linear function.
When you put sum in there, it's a highly nonlinear function, so it's going to extrapolate much
worse.
Now, there's been some great follow-up work on this from Beatrice Bevilacqua, Bruno
Ribeiro from Purdue University.
That was at ICML a few years back, which showed that this idea with, like, you want linear
targets with real UMLPs, it's really just a special case of a more general idea that
if you want to extrapolate, say, on different sizes of graphs, you need to have some implicit
causal model of what your test data is going to look like.
This linear algorithmic alignment is just one special case of a causal model like that.
So basically, if you line things up properly from a causal perspective, you should, in
principle, be able to extrapolate.
I mean, we have a clear nonparametric evidence that you can extrapolate is the algorithm
itself, right?
Now, the key is to find the right sweet spot between full universal approximator MLPs and
algorithms on the other side, right?
Interesting.
I spoke to Jan the other day.
He had a paper a couple of years ago about extrapolation in neural networks, saying they
always extrapolate.
Yes.
And speaking with Randall Belastriero, and he's got this paper, the Spline Theory of
Neural Networks, which is about, you know, these input sensitive polyhedra in the ambient
space.
And I always took that to mean why they're quite interpolative and it's just an affine
transformation for a single input.
But what he's shown, though, is that actually, even an MLP with relus is extremely extrapolative
because you can remove a whole bunch of data and, depending on how you've designed the
network architecture, it will still inform that region that you've taken away.
So, I mean, are you familiar with the Spline Theory and do you think it's a useful framework?
Yes.
So, one thing I would say, the way I understand Jan's paper, it could be that I missed some
detail, but the way I understand it is that here we're talking about interpolation and
extrapolation with respect to the geometry of the data.
So, like, you take, say, the convex hull of all the training points and then, yeah, it's
very common, especially in these high dimensional image spaces, right?
It's very easy to push one dimension sufficiently to escape the convex hull of what you've seen
so far.
So, I guess when I say extrapolation out of distribution, I'm actually maybe thinking
of a more probabilistic argument, so something like if you think of the probability distribution
induced by the training set, which obviously allows you to extrapolate away from the convex
hull, right?
But if you go sufficiently far from the modes of that distribution, so you explore a part
of the space that hasn't really been covered, you know, from a probabilistic mass point
of view in the training data, that is what we're actually thinking of when we say out
of distribution generalization.
But, yeah, I fully agree with you, like, in terms of just convex hull arguments, we very
often ask these regular MOPs to go beyond the convex hull, and they seem to work quite
well in those regimes.
But here, I'm talking really about going, like, significantly beyond the convex hull
to, like, some region that really wasn't touched.
And what we do, for example, in our papers is we train on 16 node graphs to execute these
algorithms, and then we test it on four times larger, 64 node graphs.
And what this means, because an algorithm might have, say, n-cubed time complexity, it means
the trajectory over which you have to roll it out is also much, much longer than what
you've seen in training time.
So it's really a test of, like, very different conditions than what you've seen in training
time, right?
That's interesting.
And first of all, I completely agree with you that this binary convex hull notion of
extrapolation probably isn't particularly useful.
But, you know, folks like Francois Relais describe the way Neuron Network's work is kind of bending
the space, you know, progressively with layers.
I really like this polyhedra idea.
Contrast the algorithmic reasoning with GNNs, so, I mean, I spoke with Hattie from a Google
brain team the other day, she's doing the in-consex prompting, you know, sort of algorithm
learning.
How would you contrast those two approaches?
So basically, I would really like these approaches to be reconciled going forward in the sense
that, like, I don't see them as going one without the other, if that makes sense.
So on one side, and I'm going to invoke the same principles I mentioned during our MLST
episode, you know, Daniel Kahneman's book, System 1 and System 2, right?
I think you cannot have one without the other.
So you have these amazing large-scale perceptive models that are really amazing at, you know,
taking the complexities of the real world and somehow getting interpretable enough concepts
out of there that they can, you know, make sense of what's going on and, like, drive many
interesting real-world decision-making problems, although they might lose a little bit on having
to do something like what an AI scientist would be expected to do, which is, like, extrapolate
and generate new concepts out of what they've seen.
And as you said, these kinds of specifically tailored prompts might enable the model to
take things a step or two further, but it's always, like, it's kind of, in spirit, it's
the same thing as algorithmic reasoning, because we teach a model to execute an algorithm by
forcing it to imitate the algorithm step by step.
Here you prompt a language model by telling it what are some of the steps, like, just
like you're trying to teach a student how to solve a homework, right, telling them the
individual steps they need to do, and then letting the language model go off on its own
to solve it.
But where I see the real future of these two methods converging is you're going to have
your system one component that gets your concepts out very nicely, cleanly.
And then those concepts, because we're working with transformers nowadays anyway most of
the time, are going to be very slot-based.
So that plays very nicely with GNNs, which expect nodes as input, right, so you can maybe
hook up in some nice way those concepts into a graph neural network that was trained to
execute a bunch of algorithms, and then, you know, kind of get the best of both worlds.
So have your perceptual component do the perception, and maybe prompt it as well to kind of do
it in a particularly step-by-step manner, and then further have a robust component that
makes you not have to relearn all those things that neural networks we know theoretically
cannot learn to do that well because of these extrapolation arguments.
Maybe one last point I would make to kind of cement this.
If you've been around the archive recently, you might have seen our paper on a generalist
neural algorithmic learner where we have actually used GATO-style ideas to train one graph neural
network that can execute 30 very diverse algorithms all in the same architecture with a single
set of weights, so sorting, searching, pathfinding, dynamic programming, comics, hauls, all those
kinds of nice things, very diverse ways of reasoning.
We believe something like that could maybe be a basis of, say, a foundation model of
reasoning in the future that could nicely hook up to the foundation models we already
know and love in the realm of perception.
Amazing.
And what's the biggest research challenge for you next year?
So next year, I would really like to show to what extent these things can scale in the
real world.
So we already have several isolated papers that showed that these ideas can work on
real problems.
We have Excelvin where we applied it to reinforcement learning.
That was in Europe Spotlight last year.
We have RMR where we applied it to self-supervision problems.
We also have one paper currently under review at iClear where we successfully applied to
supervised learning.
So we say pre-trained on a flow algorithm and we deploy it on brain vessel segmentation
tasks and stuff like that.
So we have many isolated cases where you learn a particular algorithm and it works really
well in a real world scenario.
I would like to see how can we take this idea and truly put it to the test at larger scales,
both in terms of number of problems we attack or number of nodes that we support or anything
in between.
Amazing.
Dr. Patovaličković, let's just, we'll get a shaking handshot.
All right.
Thank you so much for joining us.
Thank you for having me.
I really appreciate it.
Dr. Ishan Mizra of Meta and Lex Friedman fame came over and had a chat with us.
Ishan is one of the world's leading experts in computer vision.
So what was your paper about?
Yeah, basically we try to have global propagation, the likes of which you see in transformers,
not like with sparse costs.
So but in a way that will still allow you to have nice global communication properties
and no bottlenecks and stuff like that.
So we basically have this idea of you could generate these expander graphs which allow
you to have nice sparsity properties.
So basically every node I think has degree four in the graphs we compute and you need
only logarithmically many steps to traverse the graph, which means you can still do it
efficiently with a small number of steps.
And yeah, it seems to empirically work well on a bunch of graph benchmarks.
So yeah, it's a, I think it's only scratching the surface of what we can do because we literally
just generate a graph at random and slap it onto like mask the computations, but yeah,
it's an interesting start.
Very nice.
Yeah.
How about your conference?
How's it been?
So it's been pretty good.
We're organizing the self supervised learning workshop tomorrow, which is going to be probably,
I hope like useful to a lot of people, we're going to have a bunch of speakers coming from
vision, language, NLP, like speech and so on.
And yeah, we're also presenting a poster there, which is about learning joint image and video
representations, which are state of the art across image and video benchmarks using a
single model.
On the final day of the conference, I caught up with Petra again at the poster session for
new reps, which is the symmetry and geometry and neural representations group.
And his paper was selected by all of the reviewers at the conference as being in the
top 10, which is super impressive, but this is Petra talking about his paper.
So in the expander graph propagation work, we are trying to solve what is, in my opinion,
one of the most important problems in graph representation learning currently unsolved,
which is the oversplashing problem.
And effectively it is a task, which it's a problem which plagues graph neural networks
regardless of which parameters you choose or which model you choose.
It's really something that often depends on the topology of the graph, and it's a situation
where no matter how hard you try, no matter which parameters you set, the amount of features
you would need to compute, so the size of your latent space would have to be exponential
in the number of layers for the pairs of nodes to efficiently communicate.
We don't always know when it happens, but very often it tends to happen around these
bottlenecks.
So basically in this particular graph, you have these two communities that are tightly
connected, and you have this just one critical edge connecting them, and this edge is now
under a lot of pressure.
If you want data from these nodes to travel to these nodes and vice versa, this edge has
to be mindful of a lot of things, so the size of the feature space required for this edge
grows exponentially, and things get even worse when you look at trees.
Trees are like the canonical worst case example, where cutting off this edge would really trigger
all sorts of bottleneck cases, and essentially you need basically a number of, to store information
about a number of nodes that goes exponentially in the number of steps, just to be able to
travel to the other side of the tree.
So this is a fundamental problem of propagating data, which has nothing to do with the choice
of model, just topology.
And what do we try to do to fix this problem?
You would ideally want, so first we start off with the assumption that this kind of global
talking is actually beneficial.
Of course there are some tasks where you might not want data to travel in this way, because
if it's a highly homophilus data-driven problem, then you might want information to stay in
the community, to not get diluted.
But we assume in many tasks, like say molecular property prediction tasks, you actually want
data to travel globally, so that's exactly what we do.
That's our first assumption.
As we just described, we don't really want these bottlenecks to exist, because if there's
a bottleneck, no matter what you do with the model, it's not going to work well.
We would ideally want the complexity to be scalable, so we can apply this to graphs of
arbitrary sizes.
One simple solution to this problem is to use a graph transformer, which would connect
every node to every other node and give you a trivially setting with no bottlenecks.
However, as we will show later, these fully connected graphs are trivially dense expanders,
actually.
So they fit our theory, but they are dense and they won't scale.
So we don't necessarily want that.
And lastly, because it's often quite computationally painful to clear these bottlenecks in an input
data-driven way, especially if you have lots of online graphs coming into your problem,
we might ideally want a method that doesn't have to do like dedicated pre-processing of
the input graph.
And actually, satisfying all four of these at the same time turns out to be quite tricky.
We actually have done a literature survey of a bunch of related works, and it seems
really hard to tick all four of these boxes.
And our method, the expander graph propagation, tends to tick all four of them.
So how do we do it?
Basically, we propose to propagate information over these expander graphs, which are known
constructs from graph theory.
Specifically, expander graphs have mathematical properties of a high-chigger constant, so
a very low bottleneck, which is good, a low diameter, meaning you'll get global information
propagation very efficiently.
However, additionally, we can build expanders in a sparse manner using this standard mathematical
construction from the special linear group.
And that actually guarantees us that the degree of every node will be four, therefore the
graph will be sparse.
And actually, the only generative parameter of these graphs is the size of the group,
this N over here.
So it's very easy to generate an expander for a particular number of nodes.
You just tell me what N you want, and I'll give you a graph.
So when you look at an expander, it looks something like this.
It is basically, what I like to say, it looks a bit like the human brain, right?
Every node kind of has this very local connectivity to its four immediate neighbors.
But as you go far away, like log N steps, you get a lot of cycles being closed very quickly,
and the global communication properties get like really good.
So that's our proposal.
Take basically, you know, your state-of-the-art graph net that you care about.
We literally just take the code actively available implementation.
We switch the graph neural network connectivity in every even layer to operate over one of
these guys rather than the input graph.
So basically, you kind of alternate input graph, expander graph, input graph, expander
graph, so that the input graphs layers are responsible for the usual local computations
that a GNN wants to do.
And the expander layers are responsible within diffusing that information globally in a sparse
and scalable way.
And this seems to work well.
So on all the data sets we tried this construction, it was better than the baseline.
As I said, all we did was change the connectivity, so the number of parameters is exactly the
same.
It's really like an apples-to-apples comparison, and it led to statistically significant results.
One last point I would like to make is, you know, we're not the only group that tried
to study this problem, concurrently to us, the group of Michael Bronstein with Jake
Topping and Francesco DiGiovanni had this great paper on curvature analysis, which was
actually one of the best paper awardees at iClear 2022.
And basically in this paper, they claim that if you have negatively curved edges, so edges
with very negative curvature, those tend to be the ones responsible for the formation
of bottlenecks and therefore over-squashing.
So naturally we wanted to connect our expander to this theory, so we computed the curvature
of our graphs.
But we found that actually the graphs that we built are negatively curved everywhere.
So it has a curvature of negative 1 very quickly as you increase the size of the graph, right?
So obviously, you know, we built a negatively curved graph everywhere, yet it still seems
to work well.
So what gives, right?
We try to analyze this a bit further.
First we show that the curvature of negative 1 is actually not that small.
Like the theorem in this paper is only invoked when the curvature is close to minus 2.
So in our case with curvature of negative 1, it's actually not sufficiently negative
to trigger that failure case of this theorem.
And additionally, we took it a step further and we actually tried to analyze how easy
it is to satisfy these three properties at once.
So to have sparsity, we said sparsity is good for scalability.
To have a low bottleneck, so a high trigger constant, which would mean you don't have
these kinds of pathological propagation problems.
And thirdly, to have positive curvature, which seems to be a good idea based on the analysis
of this paper.
And we actually proved, there is a theorem in our paper that proves that these three
things are incompatible with each other, in that there's only finitely many graphs
that satisfy these three properties simultaneously.
So as you go to large enough input graphs to be sparsed and to have no bottlenecks,
you have to be negatively curved somewhere.
It's impossible to avoid it.
So while we don't study the implications of this any further, we do believe that it calls
on the community in the future to study what happens in this gray area where the curvature
is negative but not too negative.
Because it seems like something like that might be critical to having the most optimal
message passing possible.
And that is basically the rough summary of our work.
