1
00:00:00,000 --> 00:00:06,080
Peter Velichkovich is a staff research scientist at DeepMind.

2
00:00:06,080 --> 00:00:11,040
He's firmly established himself as one of the most significant and up-and-coming researchers

3
00:00:11,040 --> 00:00:13,280
in the deep learning space.

4
00:00:13,280 --> 00:00:18,720
He invented graph attention networks in 2017 and he's been a leading light in the field

5
00:00:18,720 --> 00:00:24,800
ever since, pioneering research in graph neural networks, geometric deep learning and also

6
00:00:24,800 --> 00:00:26,800
neural algorithmic reasoning.

7
00:00:26,800 --> 00:00:31,640
Recently he's been applying category theory to take the geometric deep learning ideas

8
00:00:31,640 --> 00:00:33,400
one step further.

9
00:00:33,400 --> 00:00:36,760
If you haven't already, you should check out our show that we did on the geometric deep

10
00:00:36,760 --> 00:00:41,480
learning blueprint, which of course featured Peter and I caught up with him last week

11
00:00:41,480 --> 00:00:42,480
at NeurIPS.

12
00:00:42,480 --> 00:00:43,480
Enjoy.

13
00:00:43,480 --> 00:00:45,720
Peter, it's fantastic to see you again.

14
00:00:45,720 --> 00:00:48,600
So this is the first time that I've actually met you in person.

15
00:00:48,600 --> 00:00:52,680
We did that really cool show together on geometric deep learning with your proto book with Takko.

16
00:00:52,680 --> 00:00:58,400
I spoke with Takko yesterday, but also Michael Bronstein and Joanne Brunner.

17
00:00:58,400 --> 00:01:02,480
So anyway, it's been a little while since we've really synced.

18
00:01:02,480 --> 00:01:07,040
Now you've had this really, really interesting category theory series.

19
00:01:07,040 --> 00:01:10,160
Can you start by just letting us know what you've been doing there?

20
00:01:10,160 --> 00:01:15,640
Yeah, that's a great point and great to finally meet you in person, Tim.

21
00:01:15,640 --> 00:01:19,200
It's really great to catch up after some time has passed.

22
00:01:19,200 --> 00:01:25,440
And yeah, I mean, I like to think that all four of us, myself, Michael, Joanne and Takko

23
00:01:25,440 --> 00:01:29,280
have a greater understanding of the implications of these methods since the last time we spoke.

24
00:01:29,280 --> 00:01:33,520
If you remember back when we did our conversation, I kind of hinted at the fact that category

25
00:01:33,520 --> 00:01:39,280
theory might hold some of the answers to maybe generalize some of these geometric concepts

26
00:01:39,280 --> 00:01:42,560
beyond the notion of just pure symmetries.

27
00:01:42,560 --> 00:01:47,200
And we believe that now we have a sufficient understanding of these kinds of things that

28
00:01:47,280 --> 00:01:50,720
we were able to make this kind of mini course on categories for deep learning.

29
00:01:50,720 --> 00:01:55,680
And to me, it really feels like the natural continuation of these concepts of geometric

30
00:01:55,680 --> 00:01:58,040
deep learning into the realm beyond.

31
00:01:58,040 --> 00:02:00,720
And I'll explain that in a moment.

32
00:02:00,720 --> 00:02:05,720
But one other kind of very related point is that here at NeurIPS, we're actually presenting

33
00:02:05,720 --> 00:02:10,920
a full conference paper which deals with using category theoretic tools to invent new kinds

34
00:02:10,920 --> 00:02:11,920
of graph neural networks.

35
00:02:11,920 --> 00:02:15,920
So basically, it's not just that we're throwing a bunch of new theory, it actually leads to

36
00:02:15,920 --> 00:02:20,120
empirical findings that we can actionably use in our models day to day.

37
00:02:20,120 --> 00:02:21,560
So that's one point.

38
00:02:21,560 --> 00:02:22,560
That is incredible.

39
00:02:22,560 --> 00:02:24,040
Can you sketch out the paper?

40
00:02:24,040 --> 00:02:25,040
Yeah, sure.

41
00:02:25,040 --> 00:02:30,920
So basically, maybe I'll first take a step back to explain why do we think categories

42
00:02:30,920 --> 00:02:34,760
are important and in what sense they're kind of a step further from what geometric deep

43
00:02:34,760 --> 00:02:36,200
learning already gives us.

44
00:02:36,200 --> 00:02:40,840
So geometric deep learning concerns itself with giving us these equivalent layers, right?

45
00:02:40,840 --> 00:02:45,640
So layers that are in some sense resistant to operations of these symmetry transformations

46
00:02:45,640 --> 00:02:48,860
is that fundamentally change an object, but the object is still the same.

47
00:02:48,860 --> 00:02:50,400
We still have all of it, right?

48
00:02:50,400 --> 00:02:54,680
And this immediately implies that these symmetries have to be composable, invertible, all that

49
00:02:54,680 --> 00:02:56,200
sort of stuff.

50
00:02:56,200 --> 00:03:03,600
And yeah, essentially, the category theory framework is in some sense mindful of the

51
00:03:03,600 --> 00:03:08,560
fact that while symmetries are a very nice way to reason about things that happen and

52
00:03:08,560 --> 00:03:14,800
that we see in nature, they're often not completely an accurate representation of what happens.

53
00:03:14,800 --> 00:03:19,240
Very often there are operations both in nature, but especially in general computation, like

54
00:03:19,240 --> 00:03:24,080
say in algorithmic stuff, where an operation of an algorithm might destroy half of your

55
00:03:24,080 --> 00:03:25,080
data.

56
00:03:25,080 --> 00:03:26,080
So that is no longer a symmetry.

57
00:03:26,080 --> 00:03:27,080
You cannot invert it.

58
00:03:27,080 --> 00:03:31,360
But you might still be interested in building a neural network model that is in some sense

59
00:03:31,360 --> 00:03:37,640
resistant to the operations of say this algorithm or this natural phenomenon that you're studying.

60
00:03:37,640 --> 00:03:42,440
One simple example that maybe predates our work a little bit is building some kind of

61
00:03:42,560 --> 00:03:45,520
equivalence to scaling operations.

62
00:03:45,520 --> 00:03:49,480
Obviously if you scale or course in something, these are not always invertible transformations

63
00:03:49,480 --> 00:03:53,920
because if you course in the pixels of your image, you cannot perfectly reconstruct where

64
00:03:53,920 --> 00:03:54,920
you came from.

65
00:03:54,920 --> 00:03:58,280
Yet you still might want to build a model that will give you the same answer as regardless

66
00:03:58,280 --> 00:04:00,560
of how you scale up your input, right?

67
00:04:00,560 --> 00:04:04,360
So these are obviously things that are going to be very important as we move to more generic

68
00:04:04,360 --> 00:04:09,640
domains than ones that can be described purely through geometric and symmetry transformations,

69
00:04:09,640 --> 00:04:10,640
right?

70
00:04:10,640 --> 00:04:16,400
And in that sense, the same way we had groups, representations, and equivalence in geometric

71
00:04:16,400 --> 00:04:21,040
deep learning, these are all special cases of categorical concepts like categories,

72
00:04:21,040 --> 00:04:25,920
functors, and natural transformations, which basically generalize all the stuff of geometric

73
00:04:25,920 --> 00:04:28,000
deep learning into their own beyond.

74
00:04:28,000 --> 00:04:33,360
And in our paper, we try to use exactly these kinds of category theoretic tools to study

75
00:04:33,360 --> 00:04:38,240
what it would mean to build a say graph neural network that is capable of behaving like a

76
00:04:38,240 --> 00:04:40,480
classical computer science algorithm.

77
00:04:40,480 --> 00:04:44,920
In the sense that if you have some data that's transformed by an algorithm, you may imagine,

78
00:04:44,920 --> 00:04:49,080
say, a path finding algorithm where at every step in every node, you have your knowledge

79
00:04:49,080 --> 00:04:52,160
of how far away is that node from the source vertex.

80
00:04:52,160 --> 00:04:55,400
And one step of the algorithm kind of looks at all the immediate neighbors and updates

81
00:04:55,400 --> 00:04:57,360
those beliefs of how far away you are.

82
00:04:57,360 --> 00:05:00,720
And I'll say you want to have a GNN that simulates that, like we typically do in algorithmic

83
00:05:00,720 --> 00:05:01,720
reasoning.

84
00:05:01,720 --> 00:05:05,880
You take your algorithmic state, you encode it with a neural network into this high dimensional

85
00:05:05,880 --> 00:05:06,880
space.

86
00:05:06,880 --> 00:05:09,680
Your GNN then processes it to update the latent space.

87
00:05:09,680 --> 00:05:13,600
And now you want to be able to decode it so that you predict whatever the next state

88
00:05:13,600 --> 00:05:14,600
is going to be.

89
00:05:14,600 --> 00:05:19,080
So you have something which in category theory we use a lot is known as a commutative diagram.

90
00:05:19,080 --> 00:05:22,640
So basically it's saying you can either take the step of the algorithm or you can encode,

91
00:05:22,640 --> 00:05:25,360
process, decode, and hopefully end up in the same place.

92
00:05:25,360 --> 00:05:28,400
So category theory seems like a very nice language to study.

93
00:05:28,400 --> 00:05:34,320
These kinds of, I won't call them symmetries, they're basically like interchangeable sequences

94
00:05:34,320 --> 00:05:39,040
of operations because the step of an algorithm might not be invertible.

95
00:05:39,040 --> 00:05:43,240
You might not be able to go back after you do one step of, you know, shortest path algorithm

96
00:05:43,240 --> 00:05:45,280
because it's a contraction map, right?

97
00:05:45,280 --> 00:05:48,880
When you find the final solution of a shortest path algorithm, you won't necessarily know

98
00:05:48,880 --> 00:05:53,560
which previous state led you there because there could be many equivalent states that

99
00:05:53,560 --> 00:05:56,340
could lead you to the same contracted solution, right?

100
00:05:56,340 --> 00:06:01,280
So our method, using these category theory frameworks, try to characterize how these

101
00:06:01,280 --> 00:06:06,520
graph neural networks align with a target algorithm that we might want to simulate.

102
00:06:06,520 --> 00:06:10,520
Then we detect various ways not only to explain the code of graph neural networks from this

103
00:06:10,520 --> 00:06:14,360
kind of perspective, but also it gives us a very interesting sort of, if you've done

104
00:06:14,360 --> 00:06:18,760
any functional programming, a type checker of sorts to kind of detect whenever we're

105
00:06:18,760 --> 00:06:22,040
using our representations in slightly broken ways.

106
00:06:22,040 --> 00:06:27,160
So specifically to give you one very concrete example, in a categorical framework, just

107
00:06:27,160 --> 00:06:30,760
like in functional programming, you expect your transformations to be functions.

108
00:06:30,760 --> 00:06:33,840
That is, for every input there should be a unique output.

109
00:06:33,840 --> 00:06:37,600
However, one thing that people very often do in graph representation learning, when

110
00:06:37,600 --> 00:06:42,040
they want to predict outputs not only in the nodes, but also in the edges, is to reuse

111
00:06:42,040 --> 00:06:47,520
the edge messages both as edge outputs and integrated overall the other messages to get

112
00:06:47,520 --> 00:06:49,040
node outputs, right?

113
00:06:49,040 --> 00:06:53,000
But this is a problem from the categorical perspective because this is no longer a function.

114
00:06:53,000 --> 00:06:58,640
You cannot get a function that takes, you know, edges to edges plus nodes without sending

115
00:06:58,640 --> 00:07:01,760
the same thing into two different places in this case, right?

116
00:07:01,760 --> 00:07:05,200
And, you know, just because it mathematically breaks doesn't mean you cannot implement it.

117
00:07:05,200 --> 00:07:12,000
In fact, 99% of the GNN implementations you'll find online will do this exactly in this particular

118
00:07:12,000 --> 00:07:13,000
way.

119
00:07:13,000 --> 00:07:15,280
DeepMindsGraphNet's library does this, for example.

120
00:07:15,280 --> 00:07:18,800
However, you know, just because you can implement it doesn't mean that there's something not

121
00:07:18,800 --> 00:07:22,840
potentially a bit tricky going on in the sense that you're putting a bit of representational

122
00:07:22,840 --> 00:07:25,120
pressure on that edge message, right?

123
00:07:25,120 --> 00:07:29,560
Because now it has to be used for two potentially very different things, both for some output

124
00:07:29,560 --> 00:07:34,320
in the edges, but also it needs to be integratable into nodes where it predicts something potentially

125
00:07:34,320 --> 00:07:35,320
wildly different.

126
00:07:35,320 --> 00:07:39,360
And, you know, while gradient descent can take care of this and give you a model that

127
00:07:39,360 --> 00:07:43,520
fits your training distribution well, you're not like, to deal with this pressure, it's

128
00:07:43,520 --> 00:07:46,240
probably going to have to learn something which has nothing to do with the algorithm

129
00:07:46,240 --> 00:07:47,840
that you want to align to.

130
00:07:47,840 --> 00:07:51,440
And as a result, you're out of distribution, extrapolation performance is going to be much,

131
00:07:51,440 --> 00:07:52,440
much worse.

132
00:07:52,440 --> 00:07:55,120
And any self-respecting algorithm should extrapolate well.

133
00:07:55,120 --> 00:07:57,400
That's the main property of algorithmic reasoning, right?

134
00:07:57,400 --> 00:08:01,160
And we find that just by, you know, splitting this message function into two streams, one

135
00:08:01,160 --> 00:08:04,920
which goes into the edges and one which goes into the nodes, we get basically significant

136
00:08:04,920 --> 00:08:08,240
empirical benefits when extrapolating on edge-centric algorithms.

137
00:08:08,240 --> 00:08:09,240
Yeah.

138
00:08:09,240 --> 00:08:10,240
Amazing.

139
00:08:10,240 --> 00:08:13,520
So, Epeta has just produced this incredible series which is available on YouTube.

140
00:08:13,520 --> 00:08:14,720
Where can folks find it?

141
00:08:14,720 --> 00:08:15,720
Yeah.

142
00:08:15,720 --> 00:08:23,160
So, basically, if you just go to cats.4.ai, you can see all of the main series lectures

143
00:08:23,160 --> 00:08:30,560
from our course, which starts off with assuming kind of a foundational knowledge of deep learning

144
00:08:30,560 --> 00:08:35,600
with neural networks, back propagation, and so on, and then also tries to introduce these

145
00:08:35,600 --> 00:08:41,000
concepts of category theory and how we can use them to rethink the way we might go about

146
00:08:41,000 --> 00:08:45,480
some of our standard ideas in deep learning like compositionality or functional structure

147
00:08:45,480 --> 00:08:50,560
of deep learning pipelines, or even how can we reinterpret back propagation from the

148
00:08:50,560 --> 00:08:53,200
perspective of categorical theory.

149
00:08:53,200 --> 00:08:58,960
And each lecture basically deals with one particular aspect and we try to keep it grounded

150
00:08:58,960 --> 00:09:03,200
from the beginning to keep it motivated so every single lecture is aligning itself with

151
00:09:03,200 --> 00:09:07,840
one particular top-tier paper that one of us has published on one of these venues like

152
00:09:07,840 --> 00:09:09,080
in Europe.

153
00:09:09,080 --> 00:09:12,520
And one thing I'll also mention is that the course is actually, in principle, still ongoing

154
00:09:12,520 --> 00:09:18,520
because besides the main series of five lectures that myself, Bruno, Pym, and Andrew have given,

155
00:09:18,520 --> 00:09:24,160
we also have several interesting guest lectures where we try to bring in other influential

156
00:09:24,160 --> 00:09:30,120
people at the intersection of popularizing category theory with deep learning concepts

157
00:09:30,120 --> 00:09:35,640
in a way that can bring an even wider area of views once you're kind of trained in the

158
00:09:35,640 --> 00:09:39,480
basics of these techniques, how they're applied to various other things like causality.

159
00:09:39,480 --> 00:09:45,200
We had Taco Cohen tell us about how he uses these concepts to reimagine causality through

160
00:09:45,200 --> 00:09:46,760
a categorical lens.

161
00:09:47,720 --> 00:09:52,760
We're going to have Tydenay Bradley, she's a very popular mathematics educator generally.

162
00:09:52,760 --> 00:09:57,040
She will show how she used some of these concepts to explain transformers.

163
00:09:57,040 --> 00:10:01,280
And one thing I'm very excited about early next year, we will have actually a guest talk

164
00:10:01,280 --> 00:10:07,600
from David Spivak, which is one of the co-authors of the very famous Seven Sketches in Compositionality

165
00:10:07,600 --> 00:10:11,040
book, which is what initially one of the things that got me really excited about category

166
00:10:11,040 --> 00:10:12,880
theory in the first place.

167
00:10:12,880 --> 00:10:15,480
So I'm really keen to hear all these perspectives as well.

168
00:10:15,640 --> 00:10:16,520
The man is a legend.

169
00:10:16,520 --> 00:10:20,920
And also on Taco, I interviewed him yesterday and his work on causality is really, really

170
00:10:20,920 --> 00:10:22,120
exciting.

171
00:10:22,120 --> 00:10:26,560
What would you say to people who might be intimidated or scared by category theory?

172
00:10:26,560 --> 00:10:31,600
So one thing that I should mention here is that one point about being intimidated or

173
00:10:31,600 --> 00:10:36,360
scared about category theory is that to really be able to utilize these ideas in how you

174
00:10:36,360 --> 00:10:42,160
do research or build your models or anything, it does require a reasonably significant buy-in.

175
00:10:42,200 --> 00:10:46,360
So this is not something that you can just read one blog post and suddenly you're empowered

176
00:10:46,360 --> 00:10:47,360
to do it.

177
00:10:47,360 --> 00:10:48,920
This is like one key thing.

178
00:10:48,920 --> 00:10:53,440
But I would say the main thing that might make people a bit scared to do it is the fact

179
00:10:53,440 --> 00:10:59,520
that many category theory resources out there are a bit guided towards mathematicians.

180
00:10:59,520 --> 00:11:03,960
So they will tend to use the kind of language and the kind of examples that will be quite

181
00:11:03,960 --> 00:11:08,560
attractive to someone who has studied, say, various kinds of differential geometry or

182
00:11:08,560 --> 00:11:10,080
topology or something like this.

183
00:11:10,080 --> 00:11:14,520
And these kinds of areas tend to generally scare off people who come from a more computer

184
00:11:14,520 --> 00:11:16,520
science style background.

185
00:11:16,520 --> 00:11:21,280
And basically I would say the answer to that is you need to find the right resource for

186
00:11:21,280 --> 00:11:22,280
you.

187
00:11:22,280 --> 00:11:26,680
Category theory is no more or no less than a way to take a bird's eye view of the phenomena

188
00:11:26,680 --> 00:11:28,200
that you try to study.

189
00:11:28,200 --> 00:11:33,120
And when you study these phenomena from high in the sky, details become invisible, but

190
00:11:33,120 --> 00:11:36,960
you suddenly get a much better feel for the structure and you can utilize kind of the

191
00:11:36,960 --> 00:11:40,560
nice patterns that reappear across various fields.

192
00:11:40,560 --> 00:11:44,320
And this you would argue is kind of the essence of what we're trying to do in deep learning.

193
00:11:44,320 --> 00:11:48,080
We have a lot of analogical way in which these architectures are constructed, right?

194
00:11:48,080 --> 00:11:51,200
So cats for AI is one possible answer to that.

195
00:11:51,200 --> 00:11:55,520
It's our way to kind of, as half of us are deep learners and half of us are category

196
00:11:55,520 --> 00:11:58,480
theorists, trying to apply these techniques to deep learning.

197
00:11:58,480 --> 00:12:03,280
We believe we have a sort of unique perspective of we and like we understand what makes people

198
00:12:03,280 --> 00:12:07,480
afraid to try to talk about these things because some of us had to go through it ourselves

199
00:12:07,480 --> 00:12:11,760
to deal with the way in which the materials are arranged online right now.

200
00:12:11,760 --> 00:12:17,880
So yeah, maybe just these kinds of resources, starting with them and basically trying as

201
00:12:17,880 --> 00:12:23,440
much as possible not to descend into the depths of NCAT lab as the very first thing that you

202
00:12:23,440 --> 00:12:29,320
do can be a good way to maybe stay sane during the first few weeks or months of trying to

203
00:12:29,320 --> 00:12:30,320
explore this field.

204
00:12:30,320 --> 00:12:31,320
Wonderful.

205
00:12:31,320 --> 00:12:34,800
I wondered if you could give a couple of examples of where category theory has been used in

206
00:12:34,800 --> 00:12:35,800
an adjacent field.

207
00:12:35,800 --> 00:12:36,800
I can think of too.

208
00:12:36,800 --> 00:12:41,120
I can think of Rosen using category theory to describe, you know, sort of ecosystems

209
00:12:41,120 --> 00:12:42,120
and life.

210
00:12:42,120 --> 00:12:46,920
I can also think of some quantum mechanics folks that have come up with a category theoretical

211
00:12:46,920 --> 00:12:49,120
conception of quantum mechanics.

212
00:12:49,120 --> 00:12:50,120
Right.

213
00:12:50,120 --> 00:12:51,120
Are there any other ones?

214
00:12:51,120 --> 00:12:52,120
Yeah.

215
00:12:52,120 --> 00:12:56,480
So I mean, I can start by giving the examples that I know about closest in terms of just

216
00:12:56,480 --> 00:12:57,480
deep learning.

217
00:12:57,480 --> 00:13:03,560
So one particular example that I think could be quite interesting is the work that was

218
00:13:03,560 --> 00:13:07,000
published at NeurIPS two years ago, which I think is one of the first papers that really

219
00:13:07,000 --> 00:13:12,000
tried to use categorical concepts to build these structures, is the natural graph networks

220
00:13:12,000 --> 00:13:18,800
paper from Pimdehan, Tapocoin and Max Swelling, which effectively realizes the fact that the

221
00:13:18,800 --> 00:13:23,200
way we build graph neural networks very often we have this one shared message function that's

222
00:13:23,200 --> 00:13:27,760
applied everywhere on every single edge on every single graph that you get.

223
00:13:27,760 --> 00:13:31,520
But in reality, is this necessary for it to be a legitimate graph neural network?

224
00:13:31,520 --> 00:13:36,780
That's actually not the case because if I give you two completely non-isomorphic graphs,

225
00:13:36,780 --> 00:13:40,400
if I choose to have completely different message functions in those two graphs, that's totally

226
00:13:40,400 --> 00:13:42,440
fine because it's still a valid graph net.

227
00:13:42,440 --> 00:13:46,240
If I permute any of those graphs, I'll get the permutation equivalent function for the

228
00:13:46,240 --> 00:13:47,560
two of them separately.

229
00:13:47,560 --> 00:13:51,760
There needs to be no weight sharing between them and naturally concepts like these.

230
00:13:51,760 --> 00:13:57,720
So this kind of requires taking a step above the group theoretic view of geometric deep

231
00:13:57,720 --> 00:14:00,840
learning and into the realm of what is known as a group poid.

232
00:14:00,840 --> 00:14:06,160
You kind of imagine every single graph structure, isomorphic graph structure, living on a sort

233
00:14:06,160 --> 00:14:10,120
of island of possible adjacency matrix representations of it.

234
00:14:10,120 --> 00:14:14,320
And for those graphs living on those islands, you need to have some weight sharing.

235
00:14:14,320 --> 00:14:18,080
But for separate islands, you don't need to have any weight sharing whatsoever.

236
00:14:18,080 --> 00:14:22,080
Of course, in practice, these kinds of layers, you would need to have some kind of sharing

237
00:14:22,080 --> 00:14:25,680
of weights in order to make them scalable to arbitrary new graph structures you haven't

238
00:14:25,680 --> 00:14:30,480
seen at training time, but it allows you a lot more flexibility about how you go about

239
00:14:30,480 --> 00:14:31,480
building your functions.

240
00:14:31,480 --> 00:14:35,080
And you're no longer constrained to have just one function everywhere repeated, right?

241
00:14:35,080 --> 00:14:39,240
So that's maybe one example that, at least to me, was what first motivated me and made

242
00:14:39,240 --> 00:14:43,640
me realize that there's more to this stuff than just to say what group theory will give

243
00:14:43,640 --> 00:14:44,640
us.

244
00:14:44,640 --> 00:14:45,640
Amazing.

245
00:14:45,640 --> 00:14:48,840
I'm really interested in your work in algorithmic reasoning, and I know you were just discussing

246
00:14:48,840 --> 00:14:54,040
it as an adjacent thing, and very soon we want to make a show, actually, on your work

247
00:14:54,040 --> 00:14:55,040
on that.

248
00:14:55,040 --> 00:14:57,480
But if you wouldn't mind, could you just sketch out algorithmic reasoning?

249
00:14:57,480 --> 00:14:58,480
Yes, wonderful.

250
00:14:58,480 --> 00:15:00,200
So, very happy to.

251
00:15:00,200 --> 00:15:05,480
Basically, what are we interested in algorithmic reasoning is building neural networks.

252
00:15:05,480 --> 00:15:08,840
They tend to be graph neural networks, but generally speaking, neural networks that are

253
00:15:08,840 --> 00:15:12,480
capable of executing algorithmic computation.

254
00:15:12,480 --> 00:15:17,240
So if I give you some context on what is the state of a particular algorithm, can my network

255
00:15:17,240 --> 00:15:22,320
somehow learn to execute that algorithm ideally in some latent space such that at every single

256
00:15:22,320 --> 00:15:26,920
step of the way, I could if I wanted to decode the states of that algorithm.

257
00:15:26,920 --> 00:15:29,200
So that's basically the main premise.

258
00:15:29,200 --> 00:15:30,680
Why do we care about this?

259
00:15:30,680 --> 00:15:35,680
Well, basically, I think of algorithms as a sort of basic foundational building block

260
00:15:35,680 --> 00:15:40,760
of reasoning, and it's kind of a timeless principle where a software engineer reads

261
00:15:40,760 --> 00:15:45,760
through one of these textbooks on algorithms and learns these 30 or 40 basis algorithms,

262
00:15:45,760 --> 00:15:50,040
and then that knowledge serves them for life in a whole career of software engineering.

263
00:15:50,040 --> 00:15:54,800
So basically, we have this hypothesis that you have this nice basis of algorithms that

264
00:15:54,800 --> 00:15:59,760
if you can master how to do them robustly, you can try to mimic any kind of at least

265
00:15:59,760 --> 00:16:02,080
polynomial time reasoning behavior.

266
00:16:02,080 --> 00:16:06,560
And that's really nice because if you look at the way current state-of-the-art large-scale

267
00:16:06,560 --> 00:16:12,320
models tend to have shortcomings, it's usually in those kinds of robust extrapolation problems.

268
00:16:12,320 --> 00:16:18,680
Basically, if we want to have a really good AI scientist that's able to not just make

269
00:16:18,680 --> 00:16:23,280
great sense of a bunch of training data from the internet, but also use that training data

270
00:16:23,280 --> 00:16:28,640
to derive new knowledge, you need some robustified way to apply rules to get infinite knowledge

271
00:16:28,640 --> 00:16:30,440
from finite means.

272
00:16:30,440 --> 00:16:32,320
So basically, that's what we want to do.

273
00:16:32,320 --> 00:16:36,440
We want to find ways inductive biases or training procedures to build neural networks

274
00:16:36,440 --> 00:16:39,200
that are more algorithmically capable.

275
00:16:39,200 --> 00:16:44,240
And in algorithmic reasoning, we obviously spent a lot of time trying to make this happen,

276
00:16:44,240 --> 00:16:48,600
just building better graph neural networks that align better with target algorithms so

277
00:16:48,600 --> 00:16:53,400
that you can execute them better, but then the really exciting part comes where we've

278
00:16:53,400 --> 00:16:57,400
actually taken some of these graph neural networks that have been pre-trained to execute

279
00:16:57,400 --> 00:17:02,480
one particular algorithm, and then we deployed it in a real-world problem where that algorithm

280
00:17:02,480 --> 00:17:07,260
is required, and we achieved, say, significant representational benefits in terms of downstream

281
00:17:07,260 --> 00:17:08,260
accuracy.

282
00:17:08,260 --> 00:17:11,440
So the idea behind this, and I'll give an example from Google Maps.

283
00:17:11,440 --> 00:17:16,040
This is an application that I worked on at DeepMind, so it's something that I've thought

284
00:17:16,040 --> 00:17:18,040
about quite a bit.

285
00:17:18,040 --> 00:17:23,420
We've invented these algorithms, like Dijkstra's algorithm, to be able to resolve these kinds

286
00:17:23,420 --> 00:17:25,840
of real-world routing problems.

287
00:17:25,840 --> 00:17:29,440
That's the kind of motivation for why you want to build the shortest path algorithm.

288
00:17:29,440 --> 00:17:33,480
And it comes as a little surprise that when you have real-world traffic data, you might

289
00:17:33,480 --> 00:17:39,000
be tempted to apply Dijkstra's algorithm to solve it, to route agents in traffic.

290
00:17:39,000 --> 00:17:43,280
However, what is the actual data that, say, Google Maps has access to?

291
00:17:43,280 --> 00:17:47,280
It's not this nice, abstractified graph with a single scalar in every edge where you can

292
00:17:47,280 --> 00:17:49,320
just go ahead and apply an algorithm.

293
00:17:49,320 --> 00:17:53,360
In fact, there's a huge bridge that must be built between the real data and the input

294
00:17:53,360 --> 00:17:54,360
to the algorithm.

295
00:17:54,360 --> 00:17:59,160
In fact, Google Maps data is typically people's cell phones in their cars, and the cars move,

296
00:17:59,160 --> 00:18:03,440
the phones move, and then based on the movement of the phones, you somehow infer how fast the

297
00:18:03,440 --> 00:18:05,880
car is going or something like that.

298
00:18:05,880 --> 00:18:11,240
And this is very noisy, not very well-structured, and you have to somehow go from there to a

299
00:18:11,240 --> 00:18:13,600
graph where you can apply this heuristic.

300
00:18:13,600 --> 00:18:18,640
Previously, it was always done exclusively by humans, like feature engineers, effectively.

301
00:18:18,640 --> 00:18:22,760
And whenever there's a human feature engineer in the loop like this, you are almost certainly

302
00:18:22,760 --> 00:18:26,280
going to drop a lot of information that you might need to solve the problem.

303
00:18:26,280 --> 00:18:29,560
So basically, you have a huge kind of bridge to cross there.

304
00:18:29,560 --> 00:18:32,600
And with algorithmic reasoning, we now don't use Dijkstra's algorithm.

305
00:18:32,600 --> 00:18:37,440
We use a high-dimensional graph net that was pre-trained to execute Dijkstra's algorithm

306
00:18:37,440 --> 00:18:38,600
in a latent space.

307
00:18:38,600 --> 00:18:42,640
So now this gives us a differentiable component that we can hook up to any encoder and decoder

308
00:18:42,640 --> 00:18:47,520
function we want to, so we can go straight from raw data and code it into the GNN's latent

309
00:18:47,520 --> 00:18:51,920
space, run the algorithm there, and then decode whatever it is that you need, like routing

310
00:18:51,920 --> 00:18:53,680
the vehicles in traffic.

311
00:18:53,680 --> 00:18:58,600
So now purely through backprop, this encoder function now learns to do what the human feature

312
00:18:58,600 --> 00:18:59,600
engineer did.

313
00:18:59,600 --> 00:19:04,560
It learns how to most effectively map that complicated, noisy, real-world data into the

314
00:19:04,560 --> 00:19:07,280
latent space where this GNN can best do its thing.

315
00:19:07,280 --> 00:19:08,440
That really is software 2.0.

316
00:19:08,440 --> 00:19:13,120
But I wanted to ask you about the computational limitations, because you just said something

317
00:19:13,120 --> 00:19:17,640
interesting about representing infinite objects with a finite memory.

318
00:19:17,640 --> 00:19:23,960
So neural networks are not Turing machines, but they can extrapolate, of course.

319
00:19:23,960 --> 00:19:25,520
What's the realistic limitation?

320
00:19:25,520 --> 00:19:30,560
Let's say you're trying to learn an algorithm, how far can you go with a neural network?

321
00:19:30,560 --> 00:19:33,880
So the thing is, there are cases where you can go very far.

322
00:19:33,880 --> 00:19:38,600
We do have theory that is very robust about this, and I think it's theory that is actually

323
00:19:38,600 --> 00:19:40,600
quite easily understandable.

324
00:19:40,600 --> 00:19:44,200
So let me try to kind of visualize it.

325
00:19:44,200 --> 00:19:48,360
When you have a real UMLP, your standard universal approximator, it's basically a piecewise

326
00:19:48,360 --> 00:19:49,800
linear function.

327
00:19:49,800 --> 00:19:54,920
So as you go far enough away from the training data, you're going to hit that level of extrapolation

328
00:19:54,920 --> 00:19:57,680
where you hit the linear part of the piecewise linear.

329
00:19:57,680 --> 00:20:01,840
And at that point, if your target function is not linear, no extrapolation is going to

330
00:20:01,840 --> 00:20:02,840
happen.

331
00:20:02,840 --> 00:20:04,640
You're not going to fit the function properly.

332
00:20:04,640 --> 00:20:08,880
So what's one outcome of this theory is that if you use real UMLPs, this was a great paper

333
00:20:08,880 --> 00:20:14,800
from MIT a few years back, which showed that basically you need to line up parts of your

334
00:20:14,800 --> 00:20:18,440
neural network such that they learn linear functions in the target.

335
00:20:18,440 --> 00:20:21,880
And that's the reason why, say, when you want to imitate a pathfinding algorithm, you want

336
00:20:21,880 --> 00:20:26,760
to use a max aggregation, your GNN, and not sum, where sum is universal.

337
00:20:26,760 --> 00:20:28,120
It can fit anything.

338
00:20:28,120 --> 00:20:31,960
But the function you have to learn, because pathfinding is like minimum overall neighbors

339
00:20:31,960 --> 00:20:36,080
of distance to neighbor plus the edge weight, suddenly when you put max in there, it's a

340
00:20:36,080 --> 00:20:37,080
linear function.

341
00:20:37,080 --> 00:20:40,320
When you put sum in there, it's a highly nonlinear function, so it's going to extrapolate much

342
00:20:40,320 --> 00:20:41,320
worse.

343
00:20:41,320 --> 00:20:45,800
Now, there's been some great follow-up work on this from Beatrice Bevilacqua, Bruno

344
00:20:45,800 --> 00:20:47,920
Ribeiro from Purdue University.

345
00:20:47,920 --> 00:20:52,040
That was at ICML a few years back, which showed that this idea with, like, you want linear

346
00:20:52,040 --> 00:20:57,160
targets with real UMLPs, it's really just a special case of a more general idea that

347
00:20:57,160 --> 00:21:02,960
if you want to extrapolate, say, on different sizes of graphs, you need to have some implicit

348
00:21:02,960 --> 00:21:06,640
causal model of what your test data is going to look like.

349
00:21:06,640 --> 00:21:10,560
This linear algorithmic alignment is just one special case of a causal model like that.

350
00:21:10,560 --> 00:21:15,720
So basically, if you line things up properly from a causal perspective, you should, in

351
00:21:15,720 --> 00:21:17,280
principle, be able to extrapolate.

352
00:21:17,280 --> 00:21:21,200
I mean, we have a clear nonparametric evidence that you can extrapolate is the algorithm

353
00:21:21,200 --> 00:21:22,200
itself, right?

354
00:21:22,200 --> 00:21:26,800
Now, the key is to find the right sweet spot between full universal approximator MLPs and

355
00:21:26,800 --> 00:21:28,280
algorithms on the other side, right?

356
00:21:28,280 --> 00:21:29,280
Interesting.

357
00:21:29,280 --> 00:21:30,280
I spoke to Jan the other day.

358
00:21:30,280 --> 00:21:34,000
He had a paper a couple of years ago about extrapolation in neural networks, saying they

359
00:21:34,000 --> 00:21:35,000
always extrapolate.

360
00:21:35,000 --> 00:21:36,000
Yes.

361
00:21:36,000 --> 00:21:39,080
And speaking with Randall Belastriero, and he's got this paper, the Spline Theory of

362
00:21:39,080 --> 00:21:43,680
Neural Networks, which is about, you know, these input sensitive polyhedra in the ambient

363
00:21:43,680 --> 00:21:44,680
space.

364
00:21:44,680 --> 00:21:48,520
And I always took that to mean why they're quite interpolative and it's just an affine

365
00:21:48,520 --> 00:21:50,160
transformation for a single input.

366
00:21:50,160 --> 00:21:55,080
But what he's shown, though, is that actually, even an MLP with relus is extremely extrapolative

367
00:21:55,080 --> 00:22:00,560
because you can remove a whole bunch of data and, depending on how you've designed the

368
00:22:00,560 --> 00:22:04,320
network architecture, it will still inform that region that you've taken away.

369
00:22:04,320 --> 00:22:08,080
So, I mean, are you familiar with the Spline Theory and do you think it's a useful framework?

370
00:22:08,080 --> 00:22:09,080
Yes.

371
00:22:09,080 --> 00:22:13,320
So, one thing I would say, the way I understand Jan's paper, it could be that I missed some

372
00:22:13,320 --> 00:22:17,040
detail, but the way I understand it is that here we're talking about interpolation and

373
00:22:17,040 --> 00:22:19,600
extrapolation with respect to the geometry of the data.

374
00:22:19,600 --> 00:22:23,840
So, like, you take, say, the convex hull of all the training points and then, yeah, it's

375
00:22:23,840 --> 00:22:27,760
very common, especially in these high dimensional image spaces, right?

376
00:22:27,760 --> 00:22:31,840
It's very easy to push one dimension sufficiently to escape the convex hull of what you've seen

377
00:22:31,840 --> 00:22:32,840
so far.

378
00:22:32,840 --> 00:22:36,720
So, I guess when I say extrapolation out of distribution, I'm actually maybe thinking

379
00:22:36,720 --> 00:22:41,640
of a more probabilistic argument, so something like if you think of the probability distribution

380
00:22:41,640 --> 00:22:45,480
induced by the training set, which obviously allows you to extrapolate away from the convex

381
00:22:45,480 --> 00:22:46,600
hull, right?

382
00:22:46,600 --> 00:22:50,920
But if you go sufficiently far from the modes of that distribution, so you explore a part

383
00:22:50,920 --> 00:22:55,960
of the space that hasn't really been covered, you know, from a probabilistic mass point

384
00:22:55,960 --> 00:23:00,440
of view in the training data, that is what we're actually thinking of when we say out

385
00:23:00,440 --> 00:23:01,440
of distribution generalization.

386
00:23:01,440 --> 00:23:05,840
But, yeah, I fully agree with you, like, in terms of just convex hull arguments, we very

387
00:23:05,840 --> 00:23:09,360
often ask these regular MOPs to go beyond the convex hull, and they seem to work quite

388
00:23:09,360 --> 00:23:11,280
well in those regimes.

389
00:23:11,280 --> 00:23:15,920
But here, I'm talking really about going, like, significantly beyond the convex hull

390
00:23:15,920 --> 00:23:19,160
to, like, some region that really wasn't touched.

391
00:23:19,160 --> 00:23:23,960
And what we do, for example, in our papers is we train on 16 node graphs to execute these

392
00:23:23,960 --> 00:23:28,240
algorithms, and then we test it on four times larger, 64 node graphs.

393
00:23:28,240 --> 00:23:31,920
And what this means, because an algorithm might have, say, n-cubed time complexity, it means

394
00:23:31,920 --> 00:23:35,560
the trajectory over which you have to roll it out is also much, much longer than what

395
00:23:35,560 --> 00:23:36,680
you've seen in training time.

396
00:23:36,680 --> 00:23:41,240
So it's really a test of, like, very different conditions than what you've seen in training

397
00:23:41,240 --> 00:23:42,240
time, right?

398
00:23:42,240 --> 00:23:43,240
That's interesting.

399
00:23:43,240 --> 00:23:45,520
And first of all, I completely agree with you that this binary convex hull notion of

400
00:23:45,520 --> 00:23:48,720
extrapolation probably isn't particularly useful.

401
00:23:48,720 --> 00:23:53,320
But, you know, folks like Francois Relais describe the way Neuron Network's work is kind of bending

402
00:23:53,320 --> 00:23:55,960
the space, you know, progressively with layers.

403
00:23:56,080 --> 00:23:59,600
I really like this polyhedra idea.

404
00:23:59,600 --> 00:24:03,600
Contrast the algorithmic reasoning with GNNs, so, I mean, I spoke with Hattie from a Google

405
00:24:03,600 --> 00:24:08,280
brain team the other day, she's doing the in-consex prompting, you know, sort of algorithm

406
00:24:08,280 --> 00:24:09,280
learning.

407
00:24:09,280 --> 00:24:11,720
How would you contrast those two approaches?

408
00:24:11,720 --> 00:24:17,520
So basically, I would really like these approaches to be reconciled going forward in the sense

409
00:24:17,520 --> 00:24:22,520
that, like, I don't see them as going one without the other, if that makes sense.

410
00:24:22,520 --> 00:24:28,720
So on one side, and I'm going to invoke the same principles I mentioned during our MLST

411
00:24:28,720 --> 00:24:32,400
episode, you know, Daniel Kahneman's book, System 1 and System 2, right?

412
00:24:32,400 --> 00:24:34,080
I think you cannot have one without the other.

413
00:24:34,080 --> 00:24:38,680
So you have these amazing large-scale perceptive models that are really amazing at, you know,

414
00:24:38,680 --> 00:24:42,880
taking the complexities of the real world and somehow getting interpretable enough concepts

415
00:24:42,880 --> 00:24:46,760
out of there that they can, you know, make sense of what's going on and, like, drive many

416
00:24:46,760 --> 00:24:51,440
interesting real-world decision-making problems, although they might lose a little bit on having

417
00:24:51,480 --> 00:24:56,000
to do something like what an AI scientist would be expected to do, which is, like, extrapolate

418
00:24:56,000 --> 00:24:58,520
and generate new concepts out of what they've seen.

419
00:24:58,520 --> 00:25:03,640
And as you said, these kinds of specifically tailored prompts might enable the model to

420
00:25:03,640 --> 00:25:08,800
take things a step or two further, but it's always, like, it's kind of, in spirit, it's

421
00:25:08,800 --> 00:25:12,920
the same thing as algorithmic reasoning, because we teach a model to execute an algorithm by

422
00:25:12,920 --> 00:25:15,480
forcing it to imitate the algorithm step by step.

423
00:25:15,480 --> 00:25:19,720
Here you prompt a language model by telling it what are some of the steps, like, just

424
00:25:19,760 --> 00:25:23,280
like you're trying to teach a student how to solve a homework, right, telling them the

425
00:25:23,280 --> 00:25:26,640
individual steps they need to do, and then letting the language model go off on its own

426
00:25:26,640 --> 00:25:27,640
to solve it.

427
00:25:27,640 --> 00:25:32,040
But where I see the real future of these two methods converging is you're going to have

428
00:25:32,040 --> 00:25:36,560
your system one component that gets your concepts out very nicely, cleanly.

429
00:25:36,560 --> 00:25:40,040
And then those concepts, because we're working with transformers nowadays anyway most of

430
00:25:40,040 --> 00:25:42,440
the time, are going to be very slot-based.

431
00:25:42,440 --> 00:25:46,640
So that plays very nicely with GNNs, which expect nodes as input, right, so you can maybe

432
00:25:46,640 --> 00:25:51,520
hook up in some nice way those concepts into a graph neural network that was trained to

433
00:25:51,520 --> 00:25:55,680
execute a bunch of algorithms, and then, you know, kind of get the best of both worlds.

434
00:25:55,680 --> 00:26:00,320
So have your perceptual component do the perception, and maybe prompt it as well to kind of do

435
00:26:00,320 --> 00:26:04,960
it in a particularly step-by-step manner, and then further have a robust component that

436
00:26:04,960 --> 00:26:08,880
makes you not have to relearn all those things that neural networks we know theoretically

437
00:26:08,880 --> 00:26:12,400
cannot learn to do that well because of these extrapolation arguments.

438
00:26:12,400 --> 00:26:15,200
Maybe one last point I would make to kind of cement this.

439
00:26:15,200 --> 00:26:18,880
If you've been around the archive recently, you might have seen our paper on a generalist

440
00:26:18,880 --> 00:26:25,080
neural algorithmic learner where we have actually used GATO-style ideas to train one graph neural

441
00:26:25,080 --> 00:26:29,800
network that can execute 30 very diverse algorithms all in the same architecture with a single

442
00:26:29,800 --> 00:26:34,480
set of weights, so sorting, searching, pathfinding, dynamic programming, comics, hauls, all those

443
00:26:34,480 --> 00:26:37,240
kinds of nice things, very diverse ways of reasoning.

444
00:26:37,240 --> 00:26:41,120
We believe something like that could maybe be a basis of, say, a foundation model of

445
00:26:41,120 --> 00:26:45,160
reasoning in the future that could nicely hook up to the foundation models we already

446
00:26:45,160 --> 00:26:47,280
know and love in the realm of perception.

447
00:26:47,280 --> 00:26:48,280
Amazing.

448
00:26:48,280 --> 00:26:50,760
And what's the biggest research challenge for you next year?

449
00:26:50,760 --> 00:26:56,120
So next year, I would really like to show to what extent these things can scale in the

450
00:26:56,120 --> 00:26:57,120
real world.

451
00:26:57,120 --> 00:27:02,000
So we already have several isolated papers that showed that these ideas can work on

452
00:27:02,000 --> 00:27:03,000
real problems.

453
00:27:03,000 --> 00:27:05,660
We have Excelvin where we applied it to reinforcement learning.

454
00:27:05,660 --> 00:27:07,560
That was in Europe Spotlight last year.

455
00:27:07,560 --> 00:27:10,800
We have RMR where we applied it to self-supervision problems.

456
00:27:10,800 --> 00:27:15,760
We also have one paper currently under review at iClear where we successfully applied to

457
00:27:15,760 --> 00:27:16,760
supervised learning.

458
00:27:16,760 --> 00:27:21,040
So we say pre-trained on a flow algorithm and we deploy it on brain vessel segmentation

459
00:27:21,040 --> 00:27:22,680
tasks and stuff like that.

460
00:27:22,680 --> 00:27:26,640
So we have many isolated cases where you learn a particular algorithm and it works really

461
00:27:26,640 --> 00:27:29,400
well in a real world scenario.

462
00:27:29,400 --> 00:27:34,240
I would like to see how can we take this idea and truly put it to the test at larger scales,

463
00:27:34,240 --> 00:27:39,640
both in terms of number of problems we attack or number of nodes that we support or anything

464
00:27:39,640 --> 00:27:40,640
in between.

465
00:27:40,640 --> 00:27:41,640
Amazing.

466
00:27:41,640 --> 00:27:45,640
Dr. Patovaličković, let's just, we'll get a shaking handshot.

467
00:27:45,640 --> 00:27:46,640
All right.

468
00:27:46,640 --> 00:27:49,120
Thank you so much for joining us.

469
00:27:49,120 --> 00:27:50,120
Thank you for having me.

470
00:27:50,120 --> 00:27:51,120
I really appreciate it.

471
00:27:51,120 --> 00:27:57,920
Dr. Ishan Mizra of Meta and Lex Friedman fame came over and had a chat with us.

472
00:27:57,920 --> 00:28:01,640
Ishan is one of the world's leading experts in computer vision.

473
00:28:01,640 --> 00:28:03,880
So what was your paper about?

474
00:28:03,880 --> 00:28:09,440
Yeah, basically we try to have global propagation, the likes of which you see in transformers,

475
00:28:09,440 --> 00:28:13,360
not like with sparse costs.

476
00:28:13,360 --> 00:28:17,960
So but in a way that will still allow you to have nice global communication properties

477
00:28:17,960 --> 00:28:19,720
and no bottlenecks and stuff like that.

478
00:28:19,720 --> 00:28:25,400
So we basically have this idea of you could generate these expander graphs which allow

479
00:28:25,400 --> 00:28:28,400
you to have nice sparsity properties.

480
00:28:28,400 --> 00:28:33,000
So basically every node I think has degree four in the graphs we compute and you need

481
00:28:33,000 --> 00:28:36,400
only logarithmically many steps to traverse the graph, which means you can still do it

482
00:28:36,400 --> 00:28:39,480
efficiently with a small number of steps.

483
00:28:39,480 --> 00:28:42,240
And yeah, it seems to empirically work well on a bunch of graph benchmarks.

484
00:28:42,240 --> 00:28:47,240
So yeah, it's a, I think it's only scratching the surface of what we can do because we literally

485
00:28:47,240 --> 00:28:52,040
just generate a graph at random and slap it onto like mask the computations, but yeah,

486
00:28:52,040 --> 00:28:53,040
it's an interesting start.

487
00:28:53,040 --> 00:28:54,040
Very nice.

488
00:28:54,040 --> 00:28:55,040
Yeah.

489
00:28:55,040 --> 00:28:56,040
How about your conference?

490
00:28:56,040 --> 00:28:57,040
How's it been?

491
00:28:57,040 --> 00:28:58,040
So it's been pretty good.

492
00:28:58,040 --> 00:29:01,280
We're organizing the self supervised learning workshop tomorrow, which is going to be probably,

493
00:29:01,280 --> 00:29:05,600
I hope like useful to a lot of people, we're going to have a bunch of speakers coming from

494
00:29:05,600 --> 00:29:09,880
vision, language, NLP, like speech and so on.

495
00:29:09,880 --> 00:29:13,920
And yeah, we're also presenting a poster there, which is about learning joint image and video

496
00:29:13,920 --> 00:29:17,800
representations, which are state of the art across image and video benchmarks using a

497
00:29:17,800 --> 00:29:19,300
single model.

498
00:29:19,300 --> 00:29:23,120
On the final day of the conference, I caught up with Petra again at the poster session for

499
00:29:23,120 --> 00:29:28,440
new reps, which is the symmetry and geometry and neural representations group.

500
00:29:28,440 --> 00:29:32,720
And his paper was selected by all of the reviewers at the conference as being in the

501
00:29:32,720 --> 00:29:39,440
top 10, which is super impressive, but this is Petra talking about his paper.

502
00:29:39,440 --> 00:29:45,080
So in the expander graph propagation work, we are trying to solve what is, in my opinion,

503
00:29:45,080 --> 00:29:49,360
one of the most important problems in graph representation learning currently unsolved,

504
00:29:49,360 --> 00:29:51,320
which is the oversplashing problem.

505
00:29:51,320 --> 00:29:56,160
And effectively it is a task, which it's a problem which plagues graph neural networks

506
00:29:56,160 --> 00:29:59,900
regardless of which parameters you choose or which model you choose.

507
00:29:59,900 --> 00:30:04,020
It's really something that often depends on the topology of the graph, and it's a situation

508
00:30:04,020 --> 00:30:09,940
where no matter how hard you try, no matter which parameters you set, the amount of features

509
00:30:09,940 --> 00:30:13,760
you would need to compute, so the size of your latent space would have to be exponential

510
00:30:13,760 --> 00:30:17,780
in the number of layers for the pairs of nodes to efficiently communicate.

511
00:30:17,780 --> 00:30:21,760
We don't always know when it happens, but very often it tends to happen around these

512
00:30:21,760 --> 00:30:22,760
bottlenecks.

513
00:30:22,760 --> 00:30:26,420
So basically in this particular graph, you have these two communities that are tightly

514
00:30:26,420 --> 00:30:31,040
connected, and you have this just one critical edge connecting them, and this edge is now

515
00:30:31,040 --> 00:30:32,640
under a lot of pressure.

516
00:30:32,640 --> 00:30:37,860
If you want data from these nodes to travel to these nodes and vice versa, this edge has

517
00:30:37,860 --> 00:30:44,180
to be mindful of a lot of things, so the size of the feature space required for this edge

518
00:30:44,180 --> 00:30:47,540
grows exponentially, and things get even worse when you look at trees.

519
00:30:47,540 --> 00:30:52,840
Trees are like the canonical worst case example, where cutting off this edge would really trigger

520
00:30:52,840 --> 00:30:58,260
all sorts of bottleneck cases, and essentially you need basically a number of, to store information

521
00:30:58,260 --> 00:31:04,240
about a number of nodes that goes exponentially in the number of steps, just to be able to

522
00:31:04,240 --> 00:31:05,960
travel to the other side of the tree.

523
00:31:05,960 --> 00:31:09,960
So this is a fundamental problem of propagating data, which has nothing to do with the choice

524
00:31:09,960 --> 00:31:13,100
of model, just topology.

525
00:31:13,100 --> 00:31:15,860
And what do we try to do to fix this problem?

526
00:31:15,860 --> 00:31:19,720
You would ideally want, so first we start off with the assumption that this kind of global

527
00:31:19,720 --> 00:31:21,580
talking is actually beneficial.

528
00:31:21,580 --> 00:31:25,520
Of course there are some tasks where you might not want data to travel in this way, because

529
00:31:25,520 --> 00:31:30,560
if it's a highly homophilus data-driven problem, then you might want information to stay in

530
00:31:30,560 --> 00:31:32,760
the community, to not get diluted.

531
00:31:32,760 --> 00:31:36,800
But we assume in many tasks, like say molecular property prediction tasks, you actually want

532
00:31:36,800 --> 00:31:40,560
data to travel globally, so that's exactly what we do.

533
00:31:40,560 --> 00:31:42,000
That's our first assumption.

534
00:31:42,000 --> 00:31:45,280
As we just described, we don't really want these bottlenecks to exist, because if there's

535
00:31:45,280 --> 00:31:49,800
a bottleneck, no matter what you do with the model, it's not going to work well.

536
00:31:49,800 --> 00:31:53,800
We would ideally want the complexity to be scalable, so we can apply this to graphs of

537
00:31:53,800 --> 00:31:55,680
arbitrary sizes.

538
00:31:55,680 --> 00:31:59,840
One simple solution to this problem is to use a graph transformer, which would connect

539
00:31:59,840 --> 00:32:05,280
every node to every other node and give you a trivially setting with no bottlenecks.

540
00:32:05,280 --> 00:32:11,400
However, as we will show later, these fully connected graphs are trivially dense expanders,

541
00:32:11,400 --> 00:32:12,400
actually.

542
00:32:12,400 --> 00:32:14,840
So they fit our theory, but they are dense and they won't scale.

543
00:32:14,840 --> 00:32:16,600
So we don't necessarily want that.

544
00:32:16,600 --> 00:32:23,080
And lastly, because it's often quite computationally painful to clear these bottlenecks in an input

545
00:32:23,080 --> 00:32:28,120
data-driven way, especially if you have lots of online graphs coming into your problem,

546
00:32:28,120 --> 00:32:31,720
we might ideally want a method that doesn't have to do like dedicated pre-processing of

547
00:32:31,720 --> 00:32:32,880
the input graph.

548
00:32:32,880 --> 00:32:37,000
And actually, satisfying all four of these at the same time turns out to be quite tricky.

549
00:32:37,000 --> 00:32:41,600
We actually have done a literature survey of a bunch of related works, and it seems

550
00:32:41,600 --> 00:32:44,720
really hard to tick all four of these boxes.

551
00:32:44,720 --> 00:32:48,180
And our method, the expander graph propagation, tends to tick all four of them.

552
00:32:48,180 --> 00:32:49,400
So how do we do it?

553
00:32:49,400 --> 00:32:53,800
Basically, we propose to propagate information over these expander graphs, which are known

554
00:32:53,800 --> 00:32:55,720
constructs from graph theory.

555
00:32:55,720 --> 00:33:00,480
Specifically, expander graphs have mathematical properties of a high-chigger constant, so

556
00:33:00,480 --> 00:33:05,120
a very low bottleneck, which is good, a low diameter, meaning you'll get global information

557
00:33:05,120 --> 00:33:06,720
propagation very efficiently.

558
00:33:06,720 --> 00:33:11,960
However, additionally, we can build expanders in a sparse manner using this standard mathematical

559
00:33:11,960 --> 00:33:14,840
construction from the special linear group.

560
00:33:14,840 --> 00:33:19,240
And that actually guarantees us that the degree of every node will be four, therefore the

561
00:33:19,240 --> 00:33:21,200
graph will be sparse.

562
00:33:21,200 --> 00:33:26,240
And actually, the only generative parameter of these graphs is the size of the group,

563
00:33:26,240 --> 00:33:27,600
this N over here.

564
00:33:27,600 --> 00:33:31,040
So it's very easy to generate an expander for a particular number of nodes.

565
00:33:31,040 --> 00:33:34,760
You just tell me what N you want, and I'll give you a graph.

566
00:33:34,760 --> 00:33:37,640
So when you look at an expander, it looks something like this.

567
00:33:37,640 --> 00:33:41,400
It is basically, what I like to say, it looks a bit like the human brain, right?

568
00:33:41,400 --> 00:33:45,560
Every node kind of has this very local connectivity to its four immediate neighbors.

569
00:33:45,560 --> 00:33:50,560
But as you go far away, like log N steps, you get a lot of cycles being closed very quickly,

570
00:33:50,560 --> 00:33:53,920
and the global communication properties get like really good.

571
00:33:53,920 --> 00:33:55,520
So that's our proposal.

572
00:33:55,520 --> 00:33:59,480
Take basically, you know, your state-of-the-art graph net that you care about.

573
00:33:59,480 --> 00:34:02,920
We literally just take the code actively available implementation.

574
00:34:02,920 --> 00:34:08,280
We switch the graph neural network connectivity in every even layer to operate over one of

575
00:34:08,280 --> 00:34:10,640
these guys rather than the input graph.

576
00:34:10,640 --> 00:34:14,400
So basically, you kind of alternate input graph, expander graph, input graph, expander

577
00:34:14,400 --> 00:34:19,320
graph, so that the input graphs layers are responsible for the usual local computations

578
00:34:19,320 --> 00:34:21,080
that a GNN wants to do.

579
00:34:21,080 --> 00:34:25,240
And the expander layers are responsible within diffusing that information globally in a sparse

580
00:34:25,240 --> 00:34:26,760
and scalable way.

581
00:34:26,760 --> 00:34:27,760
And this seems to work well.

582
00:34:27,760 --> 00:34:31,880
So on all the data sets we tried this construction, it was better than the baseline.

583
00:34:31,960 --> 00:34:35,600
As I said, all we did was change the connectivity, so the number of parameters is exactly the

584
00:34:35,600 --> 00:34:36,600
same.

585
00:34:36,600 --> 00:34:41,200
It's really like an apples-to-apples comparison, and it led to statistically significant results.

586
00:34:41,200 --> 00:34:44,760
One last point I would like to make is, you know, we're not the only group that tried

587
00:34:44,760 --> 00:34:49,840
to study this problem, concurrently to us, the group of Michael Bronstein with Jake

588
00:34:49,840 --> 00:34:54,120
Topping and Francesco DiGiovanni had this great paper on curvature analysis, which was

589
00:34:54,120 --> 00:34:58,120
actually one of the best paper awardees at iClear 2022.

590
00:34:58,120 --> 00:35:02,840
And basically in this paper, they claim that if you have negatively curved edges, so edges

591
00:35:02,840 --> 00:35:08,200
with very negative curvature, those tend to be the ones responsible for the formation

592
00:35:08,200 --> 00:35:10,720
of bottlenecks and therefore over-squashing.

593
00:35:10,720 --> 00:35:15,920
So naturally we wanted to connect our expander to this theory, so we computed the curvature

594
00:35:15,920 --> 00:35:17,120
of our graphs.

595
00:35:17,120 --> 00:35:21,000
But we found that actually the graphs that we built are negatively curved everywhere.

596
00:35:21,000 --> 00:35:26,480
So it has a curvature of negative 1 very quickly as you increase the size of the graph, right?

597
00:35:26,480 --> 00:35:31,280
So obviously, you know, we built a negatively curved graph everywhere, yet it still seems

598
00:35:31,280 --> 00:35:32,280
to work well.

599
00:35:32,280 --> 00:35:33,720
So what gives, right?

600
00:35:33,720 --> 00:35:35,480
We try to analyze this a bit further.

601
00:35:35,480 --> 00:35:39,200
First we show that the curvature of negative 1 is actually not that small.

602
00:35:39,200 --> 00:35:44,860
Like the theorem in this paper is only invoked when the curvature is close to minus 2.

603
00:35:44,860 --> 00:35:48,520
So in our case with curvature of negative 1, it's actually not sufficiently negative

604
00:35:48,520 --> 00:35:51,440
to trigger that failure case of this theorem.

605
00:35:51,440 --> 00:35:55,840
And additionally, we took it a step further and we actually tried to analyze how easy

606
00:35:55,840 --> 00:35:58,160
it is to satisfy these three properties at once.

607
00:35:58,160 --> 00:36:01,400
So to have sparsity, we said sparsity is good for scalability.

608
00:36:01,400 --> 00:36:05,520
To have a low bottleneck, so a high trigger constant, which would mean you don't have

609
00:36:05,520 --> 00:36:08,560
these kinds of pathological propagation problems.

610
00:36:08,560 --> 00:36:13,560
And thirdly, to have positive curvature, which seems to be a good idea based on the analysis

611
00:36:13,560 --> 00:36:14,560
of this paper.

612
00:36:14,560 --> 00:36:17,560
And we actually proved, there is a theorem in our paper that proves that these three

613
00:36:17,560 --> 00:36:22,080
things are incompatible with each other, in that there's only finitely many graphs

614
00:36:22,080 --> 00:36:25,700
that satisfy these three properties simultaneously.

615
00:36:25,700 --> 00:36:30,860
So as you go to large enough input graphs to be sparsed and to have no bottlenecks,

616
00:36:30,860 --> 00:36:32,820
you have to be negatively curved somewhere.

617
00:36:32,820 --> 00:36:34,540
It's impossible to avoid it.

618
00:36:34,540 --> 00:36:39,260
So while we don't study the implications of this any further, we do believe that it calls

619
00:36:39,260 --> 00:36:45,160
on the community in the future to study what happens in this gray area where the curvature

620
00:36:45,160 --> 00:36:47,220
is negative but not too negative.

621
00:36:47,220 --> 00:36:50,860
Because it seems like something like that might be critical to having the most optimal

622
00:36:50,860 --> 00:36:52,660
message passing possible.

623
00:36:52,660 --> 00:36:54,980
And that is basically the rough summary of our work.

