1
00:00:00,000 --> 00:00:06,080
Hattie Jo, a PhD student at the University of Montreal and Miele, has set out to understand

2
00:00:06,080 --> 00:00:12,000
and explain the performance of modern neural networks, believing it to be a key factor in

3
00:00:12,000 --> 00:00:17,920
building better, more trusted models. Having previously worked as a data scientist at Uber,

4
00:00:17,920 --> 00:00:24,000
a private equity analyst at Radar Capital, and an economic consultant at Cornerstone Research,

5
00:00:24,000 --> 00:00:28,400
she's recently released a paper in collaboration with the Google Brain team titled,

6
00:00:28,400 --> 00:00:34,560
Teaching Algorithmic Reasoning via InContext Learning. Now in this work, Hattie identifies and

7
00:00:34,560 --> 00:00:41,040
examines four key stages for successfully teaching algorithmic reasoning to large language models,

8
00:00:41,040 --> 00:00:47,840
formulating algorithms as skills, teaching multiple skills simultaneously, teaching how to

9
00:00:47,840 --> 00:00:55,120
combine skills, and finally teaching how to use skills as tools. Now through the application

10
00:00:55,120 --> 00:01:00,880
of algorithmic prompting, Hattie has achieved remarkable results with an order of magnitude

11
00:01:00,880 --> 00:01:07,280
error reduction on some tasks compared to the best available baselines. Now this breakthrough

12
00:01:07,280 --> 00:01:12,080
demonstrates algorithmic prompting's viability as an approach for teaching algorithmic reasoning

13
00:01:12,080 --> 00:01:18,480
to large language models and may have implications for other tasks requiring similar reasoning

14
00:01:18,480 --> 00:01:22,960
capabilities. Anyway, I hope you enjoy this conversation that I had with Hattie over at

15
00:01:22,960 --> 00:01:28,240
NeurIPS a couple of weeks ago. Enjoy. Amazing. Well, Hattie, we're here at NeurIPS.

16
00:01:29,280 --> 00:01:36,640
Welcome to MLST. Thank you. Please introduce yourself. I'm Hattie Jo. I'm currently a PhD

17
00:01:36,640 --> 00:01:44,160
student at NILA and a part-time researcher at Google Brain right now as well as a student

18
00:01:44,160 --> 00:01:51,040
researcher. Fantastic. And I've been reading your paper, Teaching Algorithmic Reasoning

19
00:01:51,120 --> 00:01:57,760
via In-Context Learning. Give us the elevator, bitch. The elevator, bitch. So I think

20
00:01:59,120 --> 00:02:03,760
typically in the past people have thought that large language models are not great at

21
00:02:04,960 --> 00:02:12,240
doing symbol manipulation or actually doing reasoning the way humans do. And a common example

22
00:02:12,240 --> 00:02:18,320
people point to for this failure is to show that models can't even do addition properly,

23
00:02:18,320 --> 00:02:22,720
even though it's trained on billions of tokens, billions of parameters.

24
00:02:24,480 --> 00:02:31,600
And in this paper, we try to teach the model how to solve these problems by learning an algorithm

25
00:02:31,600 --> 00:02:38,080
for them and see if it can generalize to a harder problem, generalize out of distribution,

26
00:02:38,080 --> 00:02:45,280
which is a common way to see if the model is actually using the correct algorithm to solve

27
00:02:45,280 --> 00:02:49,840
these tasks. So it's not just like fitting to the training distribution and finding

28
00:02:49,840 --> 00:02:57,200
spirit features, it's actually executing an algorithm. So we do that through some prompting

29
00:02:57,200 --> 00:03:03,120
strategies and show that actually can learn how to do addition contrary to previous belief.

30
00:03:04,080 --> 00:03:07,760
Yeah, I wanted to speak about the previous belief. So with large language models, I'm

31
00:03:07,760 --> 00:03:12,400
vacillated back and forth from being skeptical and being very optimistic. I was originally

32
00:03:12,400 --> 00:03:17,680
very skeptical and I'm becoming more optimistic as time goes on. I interviewed a lady from UC

33
00:03:17,680 --> 00:03:23,920
Irvine called Yasaman Rezegi. And she did a really interesting bit of work actually kind of comparing

34
00:03:23,920 --> 00:03:30,960
the term frequencies as they appeared in the corpus to the arithmetic as a function of the

35
00:03:30,960 --> 00:03:36,480
number of digits. And because OpenAI never released their data set statistics, so it wasn't possible

36
00:03:36,480 --> 00:03:41,520
to do that. And she found that there was like a linear correlation. But I think what's really

37
00:03:41,520 --> 00:03:50,240
interesting now in your work and some of the other work like Chain of Thoughts Prompting and

38
00:03:50,240 --> 00:03:55,840
Scratch Bad Prompting, that now we're in this new regime where we're kind of telling the language

39
00:03:55,840 --> 00:04:00,480
model. It's almost like we're treating the language model a bit like a kind of compiler

40
00:04:00,480 --> 00:04:06,080
and we're giving it the program. And then it's actually extrapolating and it's doing things that

41
00:04:06,080 --> 00:04:12,320
it wasn't directly taught. Right. Yeah. I think that's the first step, right? You want to,

42
00:04:13,120 --> 00:04:19,200
like if you can give the model a program, you want to see that the model can use that program

43
00:04:19,200 --> 00:04:25,840
and apply it to new situations. And so that's kind of the reasoning out of the distribution

44
00:04:25,840 --> 00:04:32,480
component. But I think also ideally you can imagine we want the model to be able to discover

45
00:04:32,480 --> 00:04:39,760
algorithms that we don't know ourselves. And you know, that's a whole other frontier, right?

46
00:04:39,760 --> 00:04:45,840
But this is the first step too on that path, I think. Okay. Well, why don't we start by defining

47
00:04:45,840 --> 00:04:56,240
algorithmic reasoning? Sure. So I think about it as just the way you solve a task is by using a

48
00:04:56,240 --> 00:05:05,040
particular algorithm. And algorithms are input independent, which means that basically for

49
00:05:05,040 --> 00:05:11,200
any input distribution, using that same algorithm will get you the right answer. And so performing

50
00:05:11,200 --> 00:05:18,880
reasoning by running an algorithm is what we refer to as algorithmic reasoning. And of course,

51
00:05:18,880 --> 00:05:25,840
these right now apply to tasks that can be solved by an algorithm. But you could also imagine,

52
00:05:25,840 --> 00:05:32,640
like, cases where you have a soft algorithm where the steps are not so rigidly defined,

53
00:05:32,640 --> 00:05:38,720
but there is an overarching problem, like solving structure that you can follow.

54
00:05:40,400 --> 00:05:45,680
Yeah. Could you refer to that? I mean, maybe I would refer to that as like inductive

55
00:05:45,680 --> 00:05:51,280
algorithmic reasoning. So it's where, so on one side of the spectrum, you actually have the

56
00:05:51,280 --> 00:05:56,880
algorithm and you write every single step explicitly using some kind of code. And now we're talking

57
00:05:56,880 --> 00:06:03,440
about this regime where we are giving examples of an algorithm and we're describing the steps

58
00:06:03,440 --> 00:06:08,720
somewhat vaguely. We're trying to be as clear as possible using language. So where are we kind

59
00:06:08,720 --> 00:06:15,600
of following on that continuum? Well, let's say you take an algorithm of addition.

60
00:06:15,840 --> 00:06:25,040
Yeah. But you actually, so you have an algorithm defined where you like start by taking the

61
00:06:25,040 --> 00:06:29,520
first digit, do something with it, and then get an answer and then move on to the next one.

62
00:06:32,000 --> 00:06:38,320
When I think about a soft algorithm, it can look something like that, except you take the digit

63
00:06:38,320 --> 00:06:45,920
and the something you do with it is not defined explicitly, but it might require some abstract

64
00:06:48,320 --> 00:06:55,120
pattern matching that large language models are particularly good at. So it can use the

65
00:06:55,120 --> 00:07:01,520
same process of breaking down the problem in a very specific way to generalize, but the individual

66
00:07:01,600 --> 00:07:12,560
steps are not actually encoded in code because it's abstract. So if you can do that, then you can

67
00:07:12,560 --> 00:07:20,000
use this approach to tackle tasks where you can't write an algorithm or you can't write a program

68
00:07:20,000 --> 00:07:26,080
for. And that's like very exciting, I think. Can you give me an example of where we can't

69
00:07:26,080 --> 00:07:32,720
explicitly write the algorithm for something? Well, it's hard to come up with a good example

70
00:07:32,720 --> 00:07:41,600
because it's supposed to be abstract in some sense, but these will not be questions of

71
00:07:44,480 --> 00:07:51,200
that you would normally tackle with by writing a program. So it could be a soft reasoning.

72
00:07:56,400 --> 00:08:02,160
I guess maybe even the grade school math word problems, there is no algorithm or a simple

73
00:08:02,160 --> 00:08:08,720
algorithm at least that you can write that will solve math word problems, which are like if you

74
00:08:08,720 --> 00:08:13,680
have four apples, I gave you twice the amount of apples you have. How many do you have now?

75
00:08:15,840 --> 00:08:24,080
But if you define a way to tackle these questions at each individual step, the model can decide

76
00:08:24,080 --> 00:08:32,800
how to apply that flexibly based on the particular question. Yeah, that might be a good thing to

77
00:08:32,800 --> 00:08:41,200
have. But yeah, right now, I don't know. There is no good benchmark for these kind of things.

78
00:08:42,240 --> 00:08:48,640
Yeah. And in your paper, so you came up with a new algorithmic prompting technique and you

79
00:08:48,640 --> 00:08:53,520
designed some experiments and your technique works significantly better than some of the

80
00:08:53,600 --> 00:08:56,640
other in context prompting techniques. Can you sketch that out for us?

81
00:08:57,360 --> 00:09:04,720
Yeah, so the intuition is very simple, actually. When we look at the addition algorithm, the one

82
00:09:04,720 --> 00:09:12,480
that we learned in school, we know that you start with the rightmost digits, you add them up,

83
00:09:12,480 --> 00:09:19,680
you add them up. If it's greater than 10, you have a carry of one, and then you add the carry to

84
00:09:19,680 --> 00:09:29,040
the next sum and so forth. So a scratch pad approach for addition will show the trace from

85
00:09:29,040 --> 00:09:35,440
running this algorithm. So it will show the first sum is this, and the carry is this. But it doesn't

86
00:09:35,440 --> 00:09:42,400
explain how those values are derived. But for us, it feels really natural because we're so familiar

87
00:09:42,400 --> 00:09:48,400
with it. But for a model, trying to infer the rules from seeing a couple examples of it,

88
00:09:49,280 --> 00:09:55,360
that's a very under specified problem. There are many rules that could explain perfectly these

89
00:09:55,360 --> 00:10:06,080
observations. So algorithmic prompting basically explains each step of an algorithm using some

90
00:10:06,080 --> 00:10:13,520
examples. And within each step tries to be as detailed as possible and tries to disambiguate

91
00:10:14,320 --> 00:10:20,240
as much as possible what you want the model to do. And it turns out that when you provide more

92
00:10:20,240 --> 00:10:26,720
details to the model, you're sort of constraining the model's interpretation of disinformation

93
00:10:26,720 --> 00:10:35,280
so that there's only one way to apply this. And with that, you can get more robust behavior from

94
00:10:35,360 --> 00:10:43,840
model and reason very well out of distribution. Yeah, because it's often said that deep learning

95
00:10:43,840 --> 00:10:49,040
models do not reason. And I think what people mean by that is that you get this phenomenon of

96
00:10:49,040 --> 00:10:55,920
shortcut learning and models do the right things for the wrong reasons. And it seems to me that

97
00:10:55,920 --> 00:11:01,920
what we're doing here is by imputing the kind of the structure of how to reason into the prompt,

98
00:11:01,920 --> 00:11:08,000
we're robustifying its behavior out of distribution. Yeah, well said. That's exactly right.

99
00:11:09,040 --> 00:11:15,600
Fascinating. So what do you think of this problem of shortcut learning in general? Because you know,

100
00:11:15,600 --> 00:11:21,520
like Melanie Mitchell said, there are two modes of understanding essentially. We have this anthropomorphic

101
00:11:21,520 --> 00:11:26,880
mode of understanding which is using causality and it's very sample efficient and we have a way of

102
00:11:26,880 --> 00:11:31,040
understanding the world. And we have a bit of an intuition that large language models

103
00:11:31,040 --> 00:11:37,280
don't reason the way that we do. But is it necessarily a problem? And is it cheating in

104
00:11:37,280 --> 00:11:41,840
your view using in-context learning? Or do you think that because we haven't had to train the

105
00:11:41,840 --> 00:11:51,040
model again, what's the problem, right? I mean, is it an issue? Yeah, I mean, I guess there's

106
00:11:51,040 --> 00:12:01,840
many forms of reasoning and algorithmic reasoning is only a subset of that. And I think if the model

107
00:12:01,840 --> 00:12:11,840
can output the reasoning process and show that the answer that arrives at is following the output

108
00:12:11,840 --> 00:12:18,880
of that process, then it's hard to argue that it's not doing reasoning. It might work differently

109
00:12:18,880 --> 00:12:28,000
under the hood, but it follows the similar process. Now, you know, some methods, you output a rationale,

110
00:12:28,000 --> 00:12:33,120
but the final answer is actually different from what the explanation suggests it should be.

111
00:12:34,160 --> 00:12:40,480
And maybe you can point to that and say, oh, the model isn't actually using this. It's just like

112
00:12:40,480 --> 00:12:45,760
giving you something that you asked for, but then the final answer is still using a shortcut.

113
00:12:46,240 --> 00:12:52,720
But with the algorithmic reasoning, we see that that's not the case. And so, yeah, the more you

114
00:12:52,720 --> 00:13:02,640
constrain things, maybe the more you remove shortcuts from the model. But an interesting

115
00:13:02,640 --> 00:13:08,000
question, I think, is you can get this behavior using in-context learning, which I think you,

116
00:13:08,800 --> 00:13:13,920
I suspect you can't really do from fine tuning or some sort of weight training.

117
00:13:14,880 --> 00:13:20,240
I think when you do that, you'll most likely just overfit on the training distribution.

118
00:13:20,880 --> 00:13:25,520
I wanted to ask you, you know, it's a bit of an open-ended question to say what's going on inside

119
00:13:25,520 --> 00:13:31,760
large language models. But what's so exciting to me is that you get all of this emergent strange

120
00:13:31,760 --> 00:13:35,920
behavior. No one would have imagined five years ago that we could do all of this prompt engineering

121
00:13:35,920 --> 00:13:40,880
on a kind of autoregressive language decoder. And the model is trained to do something really

122
00:13:40,880 --> 00:13:48,880
quite trivial, yet as a byproduct, all of this crazy stuff emerges in its internal representation.

123
00:13:48,880 --> 00:13:54,160
And all of this algorithmic reasoning capability seems to be like a side effect of that training.

124
00:13:55,520 --> 00:13:59,200
How does that even happen? Oh, that's a good question.

125
00:14:01,920 --> 00:14:07,600
It's possible that in order to fit that large training data set, the pre-training data set,

126
00:14:08,480 --> 00:14:17,360
you have to find regularities in content, I guess, that humans generate. And

127
00:14:18,960 --> 00:14:25,040
I think some of these abilities come out of those regularities that you learn. So the ability to

128
00:14:26,000 --> 00:14:37,440
refer to the pattern of a previous passage in context and, you know,

129
00:14:38,800 --> 00:14:43,680
see what the relationships are in that pattern and apply the same relationships when you input

130
00:14:45,680 --> 00:14:52,720
that circuit, I think, is just very useful as a way to summarize that large data set. And so

131
00:14:52,720 --> 00:15:02,000
because you're forced to compress all that data into a model set of weights, I think these regularities

132
00:15:03,120 --> 00:15:11,840
emerge somehow. I don't know exactly how. But I think, yeah, I mean, this is an interesting

133
00:15:11,840 --> 00:15:18,320
speculation because then you can say with much larger models where there is no capacity constraint

134
00:15:18,320 --> 00:15:27,920
at all, and you fit all the same data sets, it's not going to learn very interesting behavior

135
00:15:27,920 --> 00:15:33,680
because it's able to just fit the model without capturing the underlying patterns.

136
00:15:35,440 --> 00:15:41,120
And maybe that's why you actually need more training data and training longer rather than

137
00:15:41,120 --> 00:15:47,520
like the optimal scaling is not right now in the model size, the amount of data. Because you want

138
00:15:47,520 --> 00:15:54,640
like the right bottleneck for your representation. And I think maybe that's where these

139
00:15:56,960 --> 00:16:02,400
emergent capabilities are coming from. Interesting. Yeah. And also the amount of training as well as

140
00:16:02,400 --> 00:16:10,160
the amount of data. I wondered what is your intuition on the practical kind of computational

141
00:16:10,160 --> 00:16:14,400
limitations of large language models? So at the moment, they're trained to transform us.

142
00:16:14,480 --> 00:16:20,320
And there have been some pretty cool critiques of connectionism by Fodor and a few other people.

143
00:16:20,320 --> 00:16:25,920
And it's basically along the lines of neural networks can't represent infinite objects,

144
00:16:25,920 --> 00:16:30,720
which kind of distinguishes them from Turing machines. So they can't compute the nth digit of

145
00:16:30,720 --> 00:16:35,520
time is a fairly good example. But the amazing thing is that we're doing all of this stuff with

146
00:16:35,520 --> 00:16:40,800
algorithmic reasoning. And we haven't found the ceiling yet, it's just getting better and better.

147
00:16:40,800 --> 00:16:47,760
And I think it's almost creating this. Well, I mean, I'm becoming quite hopeful,

148
00:16:47,760 --> 00:16:51,840
actually, because I don't know where the limit is, but I suspect there is a limit quite soon.

149
00:16:51,840 --> 00:16:54,960
How do you think about what the realistic computational limits are?

150
00:16:56,640 --> 00:17:01,920
I think the fact that now we have in context learning is interesting because

151
00:17:02,880 --> 00:17:11,760
that allows us to have, I guess, adaptive amount of computation. And so if you have,

152
00:17:11,760 --> 00:17:18,960
let's say you have infinite context length, then you can sort of maybe do infinite computation

153
00:17:18,960 --> 00:17:26,480
in that case. Now, is infinite context length possible? Probably not. But then you can find

154
00:17:26,560 --> 00:17:32,080
clever ways to distill that information. You can find clever attention mechanisms.

155
00:17:32,880 --> 00:17:40,160
And so I think maybe there's a computational limit, but you can always find new ideas that

156
00:17:40,880 --> 00:17:47,600
make existing methods more efficient. And so, yeah, I have no idea when you would hit that

157
00:17:47,600 --> 00:17:53,040
limit, but it's probably very far into the future. Amazing. And so, Hattie, we're here in

158
00:17:53,120 --> 00:17:57,440
Europe. What have you taken from the conference so far? And what are you most excited about,

159
00:17:57,440 --> 00:18:06,320
you know, going forward? Yeah, I don't know, because I haven't checked out the posters

160
00:18:06,960 --> 00:18:18,160
very much yet. But I'm excited for the Math AI workshop, which is many other papers exploring

161
00:18:18,160 --> 00:18:27,040
this idea of doing math with language models. And yeah, there's some, you know, very impressive

162
00:18:27,040 --> 00:18:34,160
work there. So I'm excited for that. I'm excited to meet people and see what they're thinking about.

163
00:18:35,040 --> 00:18:43,200
And maybe get some ideas for what to work on next. Yeah, I'm also really interested in the

164
00:18:43,200 --> 00:18:48,800
math stuff I spent about an hour speaking with Marcus, is it Marcus Barb, who works under Christian

165
00:18:48,800 --> 00:18:54,320
Sagadia, I think he's in your group. Yeah, they're doing some, that's right, they're doing some

166
00:18:54,320 --> 00:18:59,920
really interesting stuff with basically doing mathematical conjecturing, you know, like representing

167
00:18:59,920 --> 00:19:04,560
mathematical expressions in large language models and being able to generate new ones.

168
00:19:04,560 --> 00:19:08,640
It's the kind of thing that you, again, would think was science fiction five years ago and like,

169
00:19:08,640 --> 00:19:14,400
remarkably, it works. Exactly. And then by the way, with the mathematical conjecturing, Marcus was

170
00:19:14,400 --> 00:19:19,600
saying that, unlike with large language models, it only has to be right one in 100 times because

171
00:19:19,600 --> 00:19:25,600
they can formally verify it. So it's almost like the bar is actually lower in that sense.

172
00:19:26,240 --> 00:19:34,160
Right. Yeah, I mean, that's where the language models informal reasonability is really useful.

173
00:19:34,880 --> 00:19:40,880
Right. Yeah, like the pattern actually is actually useful in a lot of cases.

174
00:19:40,880 --> 00:19:44,960
That's really cool. Cool. Well, actually, this has been amazing. Thank you so much for coming

175
00:19:44,960 --> 00:19:49,120
on the show and telling us about your research. Oh, thanks for having me. Yeah, it was fun.

176
00:19:51,760 --> 00:19:58,320
Looking beautiful. So Marcus, it's so nice to meet you. Can you introduce yourself?

177
00:19:59,040 --> 00:20:04,880
Hi, I'm Marcus. I work for Google Research together with Christian Segedy and the

178
00:20:04,880 --> 00:20:10,080
Antiformer team. Generally, we're working on trying to solve math,

179
00:20:12,560 --> 00:20:19,840
basically by trying to translate natural language mathematics into formal mathematics and

180
00:20:20,720 --> 00:20:23,440
in these formal representations of mathematics, we can

181
00:20:23,600 --> 00:20:30,480
check proofs and use that as a feedback signal for the understanding of mathematics.

182
00:20:31,120 --> 00:20:35,280
And most recently, I've been working on long context models like the memorizing transformer,

183
00:20:36,000 --> 00:20:43,760
trying to get these models, like makes models sensitive to the exact definitions and other

184
00:20:43,760 --> 00:20:51,360
lemmas that they might use for your improving. And that's an ongoing effort, hopefully more

185
00:20:51,360 --> 00:20:55,120
available soon. Can I just say, I'm so jealous that you work with Christian.

186
00:20:57,360 --> 00:21:01,120
I mean, folks will remember that we had a conversation with Christian. I think it was

187
00:21:01,120 --> 00:21:06,960
about 18 months ago. It is one of my favorite episodes of MLST. So you're a very lucky man indeed.

188
00:21:06,960 --> 00:21:13,520
Yes. Yes. It's great to work with Christian. It's a lot of fun. Amazing.

