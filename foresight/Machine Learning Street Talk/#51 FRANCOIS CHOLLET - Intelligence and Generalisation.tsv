start	end	text
0	20640	Past a certain level of complexity, every system starts looking like a living organism.
20640	27280	In order to build a general intelligence, you need to be optimising for generality itself.
30240	37920	We are surrounded by isomorphisms, just like a kaleidoscope. It creates a remarkable richness
37920	45280	of patterns from a tiny little bit of information. Generalisation is the ability to mine previous
45280	52640	experience to make sense of future novel situations. Generalisation describes a knowledge
52640	60160	differential. It characterises the ratio between known information and the space of possible future
60160	67440	situations. To what extent can we analyse the knowledge that we already have into simulacrums
67440	74880	that apply widely across experienced space? So intelligence, which is to say generalisation
74880	82880	power, is literally sensitivity to abstract analysis, and that's in fact all there is to it.
82880	90080	In today's show we are joined by Francois Chollet. I have been using the Keras Library for many years.
90080	96320	I also read his Deep Learning with Python book, which was inspiring, and I discovered his racy
96320	101520	Twitter feed. When I worked for Microsoft I used to run machine learning seminars and workshops
101520	108080	and hackathons. I used to travel around the world and I always had a copy of Francois's book
108080	114720	Under My Arm. It never left my side. I used to force everyone to read the first four chapters
114720	119680	of that book and of course the chapter on the limitations of deep learning before we did anything.
120320	127760	Francois has a clarity of thought, which is unparalleled I think in any other human being
127760	134320	on the planet. It's really quite incredible. Indeed even our own Dr. Duggar, who normally has
135120	140720	no trouble at all finding holes in some of our guests' work, had this to say while prepping
140720	146320	for the show. I'm working on it. It turned out to be a little bit more difficult than I thought.
146960	152400	Chalet is a little bit too reasonable. Yeah, do you like my Duggar accent? He would enjoy me
152400	157120	doing that. But anyway Chalet is extremely controversial to some people actually, but
157120	163360	he's not controversial to us. Our discussion today lies at the intersection of machine learning
163360	169040	and reasoning. Now Chalet has made his vision completely clear about what he thinks the future
169040	174320	of machine learning is. Make no mistake, what you should take from today's episode is that the
174320	180640	future of artificial intelligence is going to be discrete as well as continuous. Actually the two
180640	188640	are going to be enmeshed. The future of AI will almost certainly involve a large degree of program
188640	194880	synthesis. Deep learning has its limits. You can use deep learning for continuous problems
194880	200800	where the data is interpolative and has a learnable manifold and where you have a dense
200800	207200	sampling across the entire surface of the manifold between which you need to make predictions.
207760	215200	For Chalet, generalization itself is by far the most important feature of intelligence
215200	222240	and of developing strong AI. He describes a spectrum of generalization starting with,
222240	229360	for example, a chess algorithm where there is no novelty to adapt to whatsoever. The task is fixed.
230080	237200	The machine learning we have today confers some adaptation within a known domain of tasks. For
237200	244400	example, being able to recognize dogs or cats within a variety of different poses and lighting
244400	252400	conditions. What's not been robustly demonstrated so far is broad generalization, adaptation to
252400	258960	unknown unknowns within a known but broad domain. It's certainly true that we're knocking on the
258960	265280	door of this now with GPT-3, where the subtask, if you like, is given at test time. Although
265280	270960	Chalet would make the argument that the subtask isn't learned at test time, everything that GPT-3
270960	276320	knows was learned on the vast amounts of training data that we trained it on, the poet algorithm
276320	281840	from Kenneth Stanley et al. That appears to be meta-learning tasks as part of the training
281840	286400	process, which is very, very interesting. It's creating new problems and new solutions as part
286400	291200	of the training process. But broadly speaking in the machine learning space at the moment,
291200	297120	the task that we are doing is fixed and not generalizable. The other thing is that the real
297120	303680	world does not have a static distribution. We need systems that can adapt dynamically.
303680	309360	Intelligence requires that you adapt to novelty without the help of the engineer who helped you
309360	314880	write the system. Chalet has come up with a formalism of intelligence that balances the task
314880	321840	skill, the difficulty, the knowledge, and experience to effectively quantify and normalise
321840	328560	an algorithmic information conversion ratio. It's the ability to convert experience into future skill
328560	333040	that is Chalet's measure of intelligence. At the end of his measure of intelligence paper,
333040	338560	Francois introduced the ARC challenge. It became a Kaggle competition as well and it introduced a
338640	344640	massive diversity of tasks. The reason we have a diversity of tasks is for developer-aware
344640	350240	generalisation. Any model that we have needs to generalise to tasks that the developer was
350240	356240	unaware of. And Chalet thinks that intelligence is specialised. It needs to be human-centric or
356240	362480	anthropocentric. So the kind of priors that you need to solve these intelligence tasks need to
362480	367360	represent the kind of priors that us humans have. Now machine learning algorithms are completely
367360	372480	ineffective against the ARC challenge because it's so challenging to generalise from a few
372480	378640	examples. The only solutions that were effective in the ARC challenge were programme synthesis.
378640	383280	The manifold hypothesis is that natural data forms lower-dimensional manifolds
383280	389920	in its embedding space. There are both theoretical and experimental reasons to believe this is true.
389920	394640	If you believe this, then the task of a classification algorithm is fundamentally
394640	400480	to separate a bunch of tangled manifolds. The only way deep learning models can generalise
400480	406960	is via interpolation. Most perception problems in particular, according to Francois, are
406960	413120	interpolative. Neural networks not only have to represent the manifold of the data that they're
413120	418640	learning, the manifold also needs to be learnable. And that's an even tougher constraint.
418640	423760	Gradient descent will not learn data that has challenging discontinuities in its manifold.
423760	430080	It'll just resort to memorising the data. Deep learning allows you to represent complex programmes
430080	435920	that you couldn't write by hand, but on the other side of the coin it also fails to represent
435920	442000	very simple programmes that you could write by hand. Discrete programmes. So there are some
442000	446400	problems where deep learning is a great fit and there are other problems where deep learning
446400	451760	is a disaster. And the reason for that is that they are not interpolative in nature. These tend
451760	457680	to be algorithmic reasoning problems. Francois thinks that 99% of software written today,
457680	463120	with code, is not interpolative in nature and therefore it's a bad fit for deep learning.
463120	468160	The only answer to these problems is discrete programme search. To use deep learning for these
468160	473840	problems requires a lot of data. It's hard to train and the representation will be glitchy.
473840	479840	It'll be brittle. Neural networks cannot even extrapolate the scalar identity function,
479920	487200	f of x equals x. They can only interpolate given the existence of a smooth manifold in the latent
487200	494000	space. Jan Lacune recently said to Alfredo that all high dimensional machine learning is extrapolation.
494560	499440	So is this similar to interpolation? Well, I mean, all of machine learning is similar to
499440	504880	interpolation if you want, right? When you train a linear regression on scalar values,
504880	510000	you're training a model, right? You're giving a bunch of pairs x and y. You're asking what are the
510000	516960	best values of A and B for y equals A x plus B that minimizes the square error of the prediction
516960	521040	of a line to all of the points, right? That's linear regression. That's interpolation.
521600	526320	All of machine learning is interpolation. In a high dimensional space, there is essentially no such
526320	532240	thing as interpolation. Everything is extrapolation. So imagine you are in a space of images, right?
532240	538240	So you have a core images 256 by 256. So it's 200,000 dimensional input space. Even if you have
538240	544560	a million samples, you're only covering a tiny portion of the dimensions of that space, right?
544560	552480	Those images are in a tiny sliver of surface among the space of all possible combinations
552480	558400	of values of pixels. So when you show the system a new image, it's very unlikely that this image
558400	563360	is a linear combination of previous images. What you're doing is extrapolation, not interpolation,
563360	567840	okay? And in high dimension, all of machine learning is extrapolation, which is why it's hard.
567840	573760	I'm being brave calling out Jan Lacoon, the godfather of deep learning, but hear me out.
573760	580480	It's certainly true that interpolation on the native data domain is useless, right? We need to
580480	585680	pull some useful information out of the data and the model architecture and training method matter
585680	592320	a lot here. We can all agree that interpolation on the learned manifold would seem like extrapolation
592320	598000	in the original space of the data, right? Chalet is quite clear that neural networks only
598880	605040	generalize through interpolation. You might argue that you can go a tiny step outside of the convex
605040	611280	hull of your data, even by a tiny little bit, and you can technically extrapolate. Well, I would argue
611280	616240	that if the manifold doesn't give you any useful information outside of the training range, then
616240	620880	it wouldn't be any better than finding your nearest training example and just adding a bit of random
620880	628320	noise. If you train again, for example, you can interpolate on the latent manifold, but interestingly,
628320	634880	you can extrapolate. But the reason for that is the natural manifold that the data of faces sits on
634880	640800	might be shaped like a football or a sphere, which means if you go outside of the training range,
640800	646720	you actually have some information about those data points. The scalar identity function might seem
646720	651280	like a contrived example, but it's a really interesting one. When you go outside of the
651280	657760	training range, nothing about the manifold is known, right? Think about the manifold. It's just a
657760	663920	string that goes on forever. We don't know anything about that manifold outside of the training range.
663920	668800	This is not true for most perceptual problems in deep learning. And this is why image models,
668880	674000	for example, suffer greatly drawing straight lines. What are your thoughts about this? Why don't you
674000	679120	let us know in the comments section on YouTube? So there's a real interesting dichotomy of continuous
679120	684080	problems versus discrete problems that we're going to be exploring in the show today. It's very
684080	689200	interesting that brittleness works both ways, depending on the discreteness of the problem.
689200	696320	Program synthesis would be extremely brittle in classifying cats versus dogs or even M-nist,
696320	702560	and deep learning would be extremely brittle predicting the digits of pi or prime numbers
702560	708480	or sorting a list. So brittleness here means the overall fit of your model or your program,
708480	715200	so accuracy and robustness. Imagine if every single bug you experienced with computer software
715200	720720	was entirely unique to you and the development team wouldn't even be able to reproduce it.
720720	724880	This is what would happen if software was written entirely with neural networks.
724880	730560	It would be more, not less brittle. Sholey thinks that motivated thinking is the primary
730560	735840	obstacle to getting people to wake up to the fact that neural networks are poorly suited
735840	741600	to discrete problems. The people who are good enough at deep learning to realize its limitations
741600	747600	are too invested in its success to say so. Sholey fundamentally thinks that there are two types of
747600	754880	thinking, type one and type two. He thinks that every single thought in our minds is not simply
754880	762240	one or the other, rather it's a combination of both types. Type one and type two, they are
762240	770560	enmeshed together in everything you think and in everything you do. Even our reasoning is guided
770560	778560	by intuition, which is interpolative in nature. Sholey thinks that abstraction is key to generalization
778560	785920	and the way we perform abstraction is different in continuous versus discrete space. We need to
785920	791680	find analogies and those analogies will be found differently in both of those different spaces.
791680	797840	Program search allows us to generalize broadly from just a few examples. It marks a significant
797920	802640	deviation from traditional machine learning. Rather than trying to interpolate between the
802640	808720	examples you have, you're constructing an entire search space from scratch and testing
808720	814640	if it fits our training data. It all started with the flash fill feature in Microsoft Excel.
814640	819600	Do you remember that? You give a few examples of some transformation that you want to perform
819600	824400	and it will generate a piece of programming code for you, which means it can generalize
824400	830000	that transformation across an entire spreadsheet. It's quite a revolutionary idea. It's been around
830000	835680	for about 20 years actually, but what's really making it work now is the idea of using neural
835680	843040	networks or a neural engine to guide the discrete program search. We spoke about GPT-3. He thinks
843040	849360	that GPT-3 hasn't expanded his knowledge of the world. He says that GPT-3 is not learning any new
849360	855520	algorithms on the fly. It's already learned continuous and often glitchy representations
855520	860800	of existing tasks during its training. It's completely ineffective against his arc challenge
860800	869360	tasks. People often claim that neural networks are turing complete. No, they're not. A model has
869360	875520	a bounded number of nodes and a bounded runtime. It cannot execute algorithms that require unbounded
875520	882400	space or unbounded time. For example, could you train a neural network to predict the nth digit of
882400	888000	pi? No, you couldn't. You could write a computer program to do it, but you couldn't train a neural
888000	893200	network to do it. A simple turing machine program can do just that and that is because a turing machine
893200	898960	can access unbounded memory and time. The best thing that neural networks can do is approximate
898960	905200	unbounded algorithms, but doing so will introduce glitches. For example, one can train a neural
905200	910640	network to approximately multiply integers together. Yet, even when learning to multiply
910640	915920	fixed-width integers, practically-sized neural networks introduce errors occasionally,
915920	921760	and for a fixed-sized neural network, these errors grow more common as the size of the input grows.
921760	927280	That said, neural networks are finite state machines, and just as finite state machines
927280	932640	can be augmented with unbounded memory and iteration to yield a turing machine, neural
932640	938400	networks can also be automated in the same way to produce a turing-complete computational model.
938400	943280	If you want to see a concrete example of the kind of discrete program search that
943280	949360	Chalet is talking about, look no further than the recent DreamCoder paper. Yannick just made a video
949360	954480	about it. So yeah, it feels like today is the culmination of a year of really hard work and
954480	959840	passion from the MLST team. We've worked with so many fascinating people. We've had so many
959840	965600	amazing guests on. It really means a lot to us. Today is a very, very special episode. It was
965600	970560	my dream from the beginning to get Chalet on the show. I know that Chalet is going to say
970560	975840	lots of interesting things that will trigger some people and inspire others, and please take to the
975840	982480	comment section and tell us exactly what you think. Anyway, enjoy the show. See you next week. Peace out.
982560	986320	Welcome back to the Machine Learning Street Talk YouTube channel and podcast
986320	992960	with my two compadres, MIT, PhD, Dr. Keith Duggar and Yannick Lightspeed Kiltcher.
993600	999920	Now today we have a very special guest, Francois Chalet. Francois is one of the few leaders in
999920	1004800	the machine learning space who's caused a massive stir in my thinking, the only other notable one
1004800	1008880	actually being Kenneth Stanley, who we had on recently. My ultimate goal with Street Talk was
1008880	1013680	always to get Francois on the show, and I can't believe that it's actually happened. We actually
1013680	1017920	have a rule, by the way, that I'm only allowed to invoke Francois's name about once per show,
1017920	1024160	but that rule will not apply today. Yannick and I have made more content on Francois Chalet actually
1024160	1028160	than anyone else by a wide margin, and it's because his work is very thought-provoking
1028160	1033360	and disruptive. I spent many weeks actually studying his measure of intelligence paper last year,
1033360	1038000	and of course his recent New York's workshop was fascinating as well. Almost every single word in
1038000	1042400	my opinion that comes out of Francois's mouth deserves rigorous study, and I seriously mean that.
1043440	1048960	Francois thinks that intelligence is embodied, it's a process, and it's not just a brain. He's
1048960	1053680	skeptical of the so-called intelligence explosion, and he thinks there's no such thing as general
1053680	1060080	intelligence. All intelligence is specialized. Critically, he thinks that generalization,
1060080	1066960	the ability to deal with novelty and uncertainty is the most important concept in intelligence.
1066960	1072800	He thinks that task-specific skills tells you nothing about intelligence. He thinks that deep
1072800	1077920	learning only works for problems where the manifold hypothesis applies. For example,
1077920	1083520	problems which are interpolative in nature and when a sufficiently dense sampling of your
1083520	1091040	distribution is obtained. Otherwise, deep learning cannot generalize. Deep learning can only memorize,
1091040	1096400	but it cannot always generalize. And in his recent New York's presentation, he introduced the concept
1096400	1101120	of program-centric and value-centric generalization, which we'll get into in the show today.
1101120	1106800	But I wanted to move straight on to this concept of deep learning being a hash table,
1106800	1113200	because this is what Francois thinks. He says that a deep learning model is a high-dimensional
1113200	1119280	curve with some constraints on its structure given by inductive priors, and that curve has
1119280	1124000	enough parameters that it could fit almost anything. If you train your model for long enough,
1124000	1130080	it'll simply memorize your data. And because of SGD, your manifold fit is found progressively,
1130080	1134560	and at some point, the manifold will approximate the natural manifold between underfitting and
1134560	1140640	overfitting. And at this point, you'll be able to make sense of novel inputs by interpolating
1140640	1145680	on that manifold. So the power of the model to generalize is actually a consequence of the
1145680	1151680	structure of the data and the gradual process of SGD, according to Francois, rather than any property
1151680	1157600	of the model itself. Last week, Francois, we were talking to Christian Sergeidi, and he takes a
1157600	1161680	rather different view, because one school of thought is that deep learning models are kind of like
1162400	1168000	searching for a space of possible programs, and advocates of GPT-3 make this argument quite
1168000	1172560	strongly. And presumably, Christian Sergeidi, he wouldn't be doing what he's doing, which is
1173680	1178960	interpolating between mathematical conjectures, assuming that interpolation space would actually
1178960	1184480	give us new information about mathematics, if he thought that that space wasn't interpolatable.
1184480	1191440	What do you think Francois? Right, I think you've already summarized it, really. Yeah, so interpolation
1191440	1197600	is the origin of generalization in deep learning models, and that's very much by construction,
1197600	1203360	by nature, right? Like a deep learning model is a very large, differentiable,
1203360	1209120	parametric model, trained with gradient descent. And so the only way it's ever going to be
1209120	1215040	generalizing is your interpolation. This is literally, this is what it is, this is what it does.
1215040	1220240	So I think the question, you know, are all deep learning models, interpolators or not,
1220240	1224160	is not a super interesting question, because it's not an open question. We know they are.
1224160	1228880	But the more interesting question, I think, is what can you actually achieve with the
1228880	1233680	stored of interpolation on this very complex, very high-dimensional manifold,
1233680	1238000	that they're deep learning models and implementing. We're telling you the properties of this generalization,
1239120	1242720	the tasks for which it will perform well, the tasks for which it will not perform well.
1242720	1248720	I guess one example I could give you is encoding data with the Fourier transform,
1248720	1253120	like you know about the Fourier transform. And maybe, you know, some people will play around
1253120	1257840	with it and they will be like, hey, you know, actually the Fourier transform can draw much
1257840	1262160	more than curves. Look, I made a square with it, right? And then you would have to point out that,
1262160	1266640	no, actually the square, you've made it by supposing lots of tiny curves. And it's not,
1266640	1270880	in fact, a perfect square, right? Because it is made of this, with the other supposition of lots
1270880	1276320	of tiny curves. And that's really, this is true by nature, by construction. This is where the
1276320	1281600	Fourier transform starts, right? And the more interesting question is, you know, what sort of
1281600	1286880	data is a good fit for encoding the Fourier transform? And what sort of data is not a good
1286880	1291760	fit? Like if you try to encode the t-square fractal with the Fourier transform, you're going to have
1291760	1297360	a bad time. And if you try to encode the drawing, that's mostly just, you know, nice, smooth curves,
1297360	1302000	then it's going to be a very, very efficient encoding at a good idea. And deep learning is
1302000	1306160	very much like that. We should ask, you know, what are its strong points, what are its weak points?
1306160	1310880	Yeah, so I, by the way, so I don't believe that deep learning models are hash tables, plus there,
1310880	1315840	I usually say there, localities are sensitive hash tables, meaning that kind of like a hash table
1315840	1320960	with some amount of generalization power, because they have some notion of distance
1320960	1326160	between parts. They're capable of comparing points by measuring the distance between them, right?
1326160	1331120	And this, this is what would enable this kind of hash table to actually generalize, as opposed to
1331120	1334560	the classic kind of hash table, which is just memorizing the data.
1334560	1339760	It's very interesting that you allude to the fact that, you know, what kind of data is the model
1339760	1346720	good for, and so on. And now, deep learning models being essentially like really, as Tim said,
1346720	1351040	like big interpolators of arbitrary manifolds, do you think there is something
1351680	1358480	common across the types of data we choose deep learning for? Or, you know, could we in fact
1358480	1365280	use deep learning for most kinds of manifold dish data? Or do you think there is some kind of
1365360	1370720	specialness about natural signals that makes deep learning very attuned to them?
1371440	1377040	So I think most things are to some extent interpolative, which is why you can actually do
1377040	1380960	lots of things with deep learning models. Doesn't necessarily mean it's always a good idea,
1380960	1385200	but it's going to kind of work, right? You know, when people hear the word interpolation,
1385200	1390720	they tend to think about linear interpolation, that's what pops up in their mind. That's not
1390880	1395440	always what deep learning models are doing, right? They're interpolating on this very complex,
1395440	1401680	very high dimensional manifold. And this enables very, you know, arbitrarily complex behavior.
1402400	1408960	And in practice, it's always possible to an arbitrary discrete algorithm in a continuous
1408960	1414080	manifold, right? It's not necessarily a good idea, but it's always possible, at least in theory.
1414080	1419680	So for any program, you can imagine, you can ask, you know, is there a deep learning model that
1419680	1424480	will encode some kind of approximation of it? And the answer is always yes, right?
1425120	1431040	Similar to how you can always encode an arbitrary shape with the Fourier transform, right? But
1431040	1435120	there are, if you try to do that, actually, there are some issues with that. There are very much,
1435120	1439520	you know, some problems for which deep learning is good fit, some problems for which deep learning
1439520	1446400	is not a good fit. In the limit, the extreme point is a space that is not interpolative at all,
1446400	1450400	which is quite right, actually. You know, most spaces, even very discrete kind of spaces,
1450400	1456960	do have, you know, some amounts of interpolativeness. So like, but one example would be, for instance,
1456960	1464400	trying to train a deep learning model to predict the next prime number, right? Or to tell whether
1464400	1470480	a number is a prime number. As you cannot actually do that, the best you can do is memorize the
1470480	1476800	train data point, because the space of prime numbers is not interpreted at all. So your deep
1476800	1483200	learning model will always have zero generalization power. But that's actually quite rare. This is
1483200	1489680	kind of an extreme case. Most problems, even problems that are binary, discrete, algorithmic
1489680	1494240	problems, there will be some amount of interpolation that you can do, right? But that doesn't necessarily
1494240	1499120	mean that it's a good idea to try to solve, you know, such problems with deep learning models
1499120	1504000	for deep learning to be a good idea. You need a very, you need very much the manifold
1504000	1509360	level as it is to apply. So it works best for perception problems. Any problem that humans
1509360	1514320	can solve via pure intuition or perception is probably a good fit for deep learning. But any
1514320	1519600	problem where you need, you know, high level explicit step by step reasoning is probably
1519600	1524240	a bad fit for deep learning. And, you know, 99% of what today software engineers solve,
1524960	1530000	the writing code is going to be a bad fit for deep learning. That doesn't mean that there
1530000	1535440	wouldn't be, you know, theoretically, a deep learning model that can embed the same algorithm
1535440	1541040	in a smooth manifold. This is always possible to some extent, right? But there are very significant
1541040	1546080	issues with attempting to do this. I like just because something is theoretically possible doesn't
1546080	1552240	mean you should actually do it. I think we might be not being careful enough when we say what we
1552240	1559200	mean by program. Because, for example, if I take program to be the universal sense like a program
1559200	1564240	is something that can run on a Turing machine, for example, because of the fact that that type
1564240	1571200	of program actually has access to unbounded time and memory computation. It's impossible in the
1571200	1577280	general sense to encode that in any finite neural network, like I can write a very short piece of
1577280	1582960	code theoretical Turing machine can output, you know, the nth digit of Pi. It's impossible to do
1582960	1587600	that with any finite neural network. Would you agree? Yeah, absolutely. Absolutely. Okay, because I
1587600	1591360	think that's like a big source of confusion often time with these statements that like, you know,
1591920	1596480	oh, neural networks are Turing complete. Well, no, they're not. You know, if you have a neural
1596480	1602480	Turing machine, which is a neural network that's the finite state machine piece of a Turing machine,
1603040	1607600	that can be Turing complete. But in the general case, you know, finite neural networks, which is
1607600	1612320	what everyone means by neural networks, are not Turing complete. And it actually has practical
1612320	1617840	effects, right? This is why we see this sort of explosion and the number of parameters to kind of,
1617840	1623520	you know, start to accomplish. Yeah, absolutely. 100%. You're entirely right. So we're only
1623520	1627360	interested in realistic programs, like the sort of programs that start to engineer with right,
1627360	1631360	for instance. And we're only interested in realistic neural networks. And by the way,
1631440	1635360	the constraints that we have on neural networks are actually much stronger than asking,
1635360	1641280	given this program that I have, is there a neural network that could embed it in a continuous
1641280	1646240	manifold? The constraint is actually, is there a neural network that could not only represent it,
1646240	1652480	but that could learn this embedding of the program from there. And this is a several orders of
1652480	1659360	magnitude harder, right? Learnability is a big problem because you're fitting your manifold via
1659360	1667360	gradient descent, right? And if the structure you're trying to fit is too discrete, with too
1667360	1673840	big discontinuities, gradient descent will not work at all. And the best you can do is, again,
1673840	1680960	just memorize the train data. So I can maybe give you a concrete example to kind of ground
1680960	1688480	our discussion here. So in 2015, some friend of mine, so his name is, he used Keras to do
1688480	1694160	something pretty cool, which actually became a cool example on the Keras website. He used a LSTM
1694160	1702160	model to multiply numbers, but not like numbers multiplied by value, but the input of the model
1702160	1707360	would be strings, like two strings, strings of digits. And the LSTM will actually learn the
1707360	1712240	multiplication algorithm for like multiplying three digits and three digits numbers, kind of the
1712240	1718000	sort of algorithm we would learn in primary school, right, to do multiplication. And remarkably,
1718080	1722800	that worked, right? It works just fine. So you can train a deep learning model to learn this
1722800	1726880	algorithm. And you could, of course, train a transformer model to do the same. It will actually
1726880	1732880	be probably significantly more efficient. So that works. That comes with a number of downsides.
1732880	1736880	So first, in order to train that algorithm, which is very simple, you're going to need
1736880	1743280	thousands and thousands of examples of different strategic numbers. And once you've trained your
1743280	1750400	algorithm, because the actual algorithm was embedded in the neural network, it does generalize to
1750400	1755600	never see before digits, right? So it's actually learning the algorithm. It's not just learning,
1755600	1761520	I'm just not memorizing the data. But the thing is, because the embedding of an algorithm,
1761520	1765840	the embedding of a discrete structure in the continuous space, is not the same thing as the
1765840	1771120	original discrete object. There are glitches in your deep learning network, unless that's something
1771120	1775520	you could have found via program synthesis, for instance, it's not going to be correct 100% of
1775520	1781520	the time, it's going to be correct 95% of the time. In much the same way that if you try to
1781520	1786560	encode a very discrete object via the Fourier transform, it's not going to be correct. 100%
1786560	1790720	of the time is going to be an approximation and around sharp angles, it's actually going to be
1790720	1796000	wrong. And very importantly, and this is really like the algorithm that you've painstakingly
1796080	1802800	embedded into your deep learning model via exposure to data, does only, it does not generalize very
1802800	1808000	well, it only does local generalization, meaning that if you train it with three, to multiply
1808000	1812480	three digit numbers, and then you send it a five digit number, is it going to work? No,
1812480	1816400	absolutely not. And not only is it not going to work, but you could not in fact,
1818240	1823760	few shots fine tune your algorithm to learn to handle five digits, seven digits and so on.
1823760	1827760	If you want to fine tune your algorithm, you're going to need thousands, maybe millions
1828640	1835200	of examples, right? So it's all local generalization. And lastly, it's super inefficient,
1835200	1840080	like I think we can all agree with this, that multiplication is not like it's not
1841440	1846400	a clever use of an LSTM, it's you're burning tons of resources for something that's actually
1846400	1851920	super easy. And you can compare that, like since we are talking about pros and cons of deep learning,
1851920	1856480	you can compare that to what you could get with a program synthesis engine. Like I don't want to
1856480	1860480	compare to what you could get with a human written algorithm, because kind of the point of deep
1860480	1865920	learning is that it enables you to develop programs that you could not otherwise write by hand.
1865920	1870400	So the right point of comparison is actually what you could do with deep learning versus what you
1870400	1877280	could do with discrete program synthesis based on discrete search and the DSL. And if you were to
1877280	1883760	use a program synthesis to solve the multiplication problem, so you would find a solution, even a
1883760	1890240	very neat engine that does just like maybe a plus operation, maybe a loop. And this DSL is going to
1890240	1895120	find it, it can find it with a handful of examples, you're not going to need thousands of examples,
1895120	1899280	like in the deep learning case, you're going to need maybe five. And the program you get out of it
1899280	1905040	is going to be exact, because it is the exact discrete algorithm, it is not a continuous
1905040	1909760	embedding of it. So it does not have glitches, it outputs the correct answer. It will be lightweight,
1909760	1913680	so it will be very efficient, you know, and like the LCM or transformer model,
1913680	1919280	and crucially, it's going to generalize. So if you develop it only from three digit numbers,
1919280	1923040	maybe there will be something inside it that will hardcore the assumption that they're dealing with
1923040	1928240	three digit numbers. But even if that's the case, you can take it and automatically learn
1929200	1933840	a generalized form of it if you just start giving it seven digit numbers. Very easy because it's
1933840	1938880	just modifying probably a couple lines of code. So it is capable of strong generalization. So here
1938880	1943840	you start seeing how for a problem that's fundamentally a discrete algorithmic reasoning
1943840	1950160	problem, discrete search is the correct answer. Deep learning, it's possible, it works, but with
1950160	1956320	extremely stark limitations, right? It's very hard to train it, you need tons of data. The resulting
1956320	1960640	embedding, because it's not, it's not discrete, we'll have glitches. It's not going to work on
1960640	1964640	a person at a time, it's going to be pretty long. It's only going to be capable of hardcore
1964640	1973520	generalization, right? Because again, there is a huge difference in representational flexibility
1973520	1979040	between your very simple, discrete algorithm and some kind of very complex, high dimensional
1979040	1984400	continuous embedding. And then there's also the efficiency consideration. So clearly for
1984400	1989600	if you're dealing, and the reverse is also true, right? Like if you're dealing with a problem that's
1990160	1996800	perception problem, where you have data points that fit on a nice and smooth manifold, then
1996800	2004640	deep learning is actually the right answer. And if you tried to train a discrete program to develop
2004640	2009360	your program synthesis, an actual algorithm to classify MNIST digits, for instance.
2010800	2016240	Everything I just said would be true, but in reverse, your program would be brittle. The
2016240	2020960	deep learning model would be robust, and so on. So there are really problems where deep learning
2020960	2025120	is a pretty idea. It's a great fit. Problems where it's a terrible idea. Like try sorting a list
2025120	2030640	with deep learning model. Can it be done? Yes, actually it can. But with all these caveats applying.
2031200	2036720	It is possible to sort a list of deep learning with some hacky inductive priors and probably
2036720	2041440	memorizing most of the training data. And it's not a binary, is it? You said yourself, there's lots
2041440	2046880	of problems that fall in the middle, where there is a semi continuous structure and some
2046880	2052000	regularity, but it's still a discrete problem. And you're saying in that situation, we should
2052000	2056400	still use program search, but maybe we can use deep learning, maybe something about the shape
2056400	2060720	of the manifold, even though it's semi continuous, could actually tell us about how to do that
2060720	2066320	program search more efficiently. But it seems to me that if there are problems out there,
2066320	2072320	let's say adding numbers up in GBT3, when I read the stuff that you've been talking about here,
2072320	2078880	it seems obvious to me. Why are people not picking up on this? I think most people are not necessarily
2078880	2084000	paying a lot of attention to the nature of deep learning, why it works, why it doesn't work.
2084000	2089520	I also think the people, they are basically two categories of people. They are like laypeople,
2089520	2095200	and they are people with deep expertise. And the big problem we have here is that the people with
2095200	2101760	a lot of expertise are going to be a lot of the time driven by motivated thinking. Because
2103680	2107360	like I do, they work in the field of deep learning, and so they're going to have this vested
2107360	2112960	interest in deep learning being potentially more powerful, more general at the nature is. I think
2112960	2119440	if you want to think clearly, the primary obstacle is motivated thinking. It's fighting
2119520	2126160	against what you want to be true. So I tend to have super boring opinions in that sense,
2126160	2131440	because I do my best to try to forget kind of what I would like the world to be in my
2131440	2136240	best interest and try to look at it as it really is. And that will tend to actually diminish
2136240	2141840	the importance of my own work. So yeah, but you know, I've been doing deep learning for
2141840	2147360	almost a decade. Of course, I would want it to be like this incredible world changing thing that
2147360	2151040	leads to human level intelligence, right off the bat, that would be that would be awesome,
2151040	2156240	that would be amazing, and that would be right in the middle of it. But that's not that's not
2156240	2162960	actually what's going on. You said you tend to be what was the word, not not controversial ideas
2162960	2167600	or something because you try to stick to the way the world is rather than the way you want the
2167600	2173200	world to be. But we just had Yannick produce an interesting video about how if you think that
2173280	2177360	machine learning models essentially attempt to do the same thing, right? I mean, they're not human
2177360	2182160	beings, they don't really have wants per se, they're just modeling reality as it is. It turns out
2182160	2188640	reality itself really annoys a lot of people, like they just don't like reality, and they don't like
2188640	2193280	the way the world is, and they wish it was something different. And that infects like every mode of
2193280	2197840	their thinking, actually. Yeah, no, absolutely. Most people, you know, and that's that's true for
2197840	2205600	me as well. I'm not saying I'm an exception, I'm trying to do my best to resist this trend.
2205600	2211280	But I have no exception. Most people have opinions not because they've seen evidence in
2211280	2216400	support of their opinion, but because it's in their interest for this opinion to be true,
2216400	2221360	or they just want it to be true. I guess one example is, you know, we were mentioning GPT-3
2221360	2227120	and so on and proponents of GPT-3. I was actually super excited when I initially saw the claim
2227120	2231680	that the pre-trained language model could perform few short generalizations. I thought that's
2231680	2237600	super fascinating. I'm always super excited if I hear about something that's really challenging
2238320	2245120	my initial kind of mental model of how the world works, you know, it's like a few years back,
2245120	2249920	and there was this claim that a neutrino was measured going faster than speed of flight.
2249920	2254960	I mean, that's exciting, right? That's like new physics, you want it to be true, at least you
2254960	2259280	want to get to the bottom of it. And then it turned out to be a measurement error, right? So that's
2259280	2264240	disappointing. So I think GPT-3 is kind of the same for me. I really wanted it to be something,
2264240	2268400	something novel, and that would really challenge what they thought to be true by deep learning
2268400	2274960	models. And I regret to say that everything I've seen close has actually confirmed. In my view,
2274960	2280720	that basically deep learning models, they can learn to embed algorithms given sufficient exposure
2280720	2288080	to data, but they cannot really, like, few short synthesize novel algorithms that represent a pattern
2288080	2293120	they haven't seen in a train yet, which is why, by the way, GPT-3 is entirely ineffective on ARC,
2293120	2300960	for instance. And that's kind of sad to me. I kind of regret it, because it means I haven't actually
2300960	2308240	learned anything from it. It hasn't expanded my view of the world, which is too bad. Like,
2308240	2314560	I wish it did. I wish it did. So in the case of GPT-3, what's really going on is that the model
2314560	2320560	is exposed to many patterns. You could call them algorithms, for instance, in many different contexts.
2320560	2325440	And so it has memorized these patterns. And now it's able to take these patterns and apply them
2325440	2329680	to new data in much the same way that the multiplication algorithm we are talking about.
2329680	2334800	Because it's an actual algorithm, it can process new digits. It's not just memorizing the digits
2334800	2340560	in the train. It's an actual algorithm. In the same way, GPT-3 contains tons of small algorithms
2340560	2346560	like that. But the model is not synthesizing these algorithms on the fly. They are in the model already.
2347280	2352880	And if you try to apply GPT-3 to something for which a new algorithm would need to be produced,
2352880	2356000	like an ARC task, for instance, it has just completed anything.
2356000	2361280	It seems to all build up what you're saying, because there is this strong generalization
2361280	2367760	versus local generalization. And then you make a case that in order to do strong generalization,
2367760	2373360	we need maybe something like program synthesis approach. So deep learning can't necessarily
2373360	2379680	get us there in most problems. And you make an interesting case that something like graph
2379680	2387440	isomorphism search could play a core role in that. Could you briefly connect all of these
2387520	2391680	terms together of the case you're making there? Because it's super interesting.
2391680	2400240	So going back to it, Tim was saying it's rarely the case that you have problems that are fully
2400240	2404560	interpretive or fully discrete. There are definitely such problems. In fact, most perception
2404560	2410160	problems are almost entirely interpretive. And most programs, the kind of programs that you
2410720	2416400	write there, they're largely discrete, not interpretive. But most tasks actually are best
2416400	2422960	solved via a combination of both. And I actually believe that's true for the way humans think.
2422960	2428560	You know, there's type 1 thinking and type 2 thinking. I strongly believe that almost every
2428560	2436560	thought you have and everything you do with your mind is not one or the other. It's a combination
2436800	2442240	of both. That type 1 and type 2 are really unmatched into each other in everything you
2442240	2448720	think and everything you do. Like, for instance, perception. That looks like something very
2448720	2453840	instant. So very much the sort of continuous, interpolative thing. In fact, there's a lot
2453840	2458960	of reasoning that's embedded into perception. And the reverse is true, for instance. If you
2458960	2463760	look at a mathematician, for instance, proving a theorem, where they're writing down on the
2463760	2469600	sheet of paper is really step-by-step, discrete reasoning type thing. But it's very much guided
2469600	2474800	by high-level intuition, which is very much interpreted. They know where they're going,
2474800	2482240	without having to derive the exact sequence of steps to get there. So they have this high-level
2482240	2486960	kind of view. Kind of like, you know, if you're driving, you have to make discrete decisions
2486960	2493280	because you are driving on network frauds. But if you have a bird, a GPS, for instance,
2493280	2497760	you can kind of see the direction in which you are going, which is interpolated. If you're talking
2497760	2502080	about direction, you're talking about distances, you're talking about geometric spaces. And
2502080	2509040	everything in the human mind kind of follows this model of type 1 and type 2 thinking at the same
2509040	2515920	time. If you go back to first principles, intelligence is about abstraction. So intelligence
2515920	2525360	is about the ability to face the future, given things you've seen in the past. And the way you do
2525360	2532240	that is, yeah, abstraction. You extract from the past some construct. Maybe it's a template,
2532240	2537120	maybe it's an algorithm that will actually be effective in terms of explaining the future. And
2537120	2543760	that's why it makes it abstract, is that it can handle multiple instances of some kind of thing,
2544000	2548960	that thing is an abstraction. And if it's abstract enough, it can actually handle instances
2548960	2556080	you've never seen before, right? It does generalization power. And all abstraction is worn
2556080	2563200	from analogy. Abstraction starts when you make an analogy between two things. Like you say,
2563200	2567520	time is like a river, if you want to get philosophical or something. But in general,
2567520	2571520	you can just say this apple looks similar to this other apple. So there is such a thing as
2571520	2577440	the concept of an apple, for instance. And the part that is shared between the two things that
2577440	2582960	you're relating to each other, the subject of the analogy that that's the part that can be said to
2582960	2587760	be abstract, that is the part that will help you make sense of the future, like you encounter a
2587760	2592400	third apple in the future, you know, it's an apple. Because you don't even need to relate this to
2592400	2596640	the apple should have memorized, you just need to, you just need to relate it to the template,
2596640	2601120	the abstract template of an apple that you've formed by from exposure to different kinds of
2601120	2606800	apples in the past. And if you think about what's what's an analogy, really, like how do you find
2606800	2613440	an analogy, it's a way to compare two things to each other. And there are only really two ways
2613440	2623120	to compare things. You can, you can basically ask how similar are they in terms of distance,
2623120	2628640	like you can say implicitly, there's you're looking at the space of points, there's a distance
2628640	2634000	between any two points. That's, that's the type one, a subject analogy that leads to type one
2634000	2640880	abstractions, which leads to a type one thinking, right? So a type one analogy is like your things,
2640880	2646400	you say to what degree they're similar to each other. So you read them by distance, you, so
2646400	2653120	implicitly, it means you put your things on in a geometric space, right? And type one abstraction
2653120	2657760	is going to be a template. It's like you're going to have clusters of things, you can take the average
2657760	2663360	and say everything that is within a certain distance of that template belongs to this category.
2663360	2669120	That's that's type one. It's very much the way deep learning models work. And then you and then
2669120	2673680	you start adding perception and intuition on top of that, which is very much the type one thing.
2673680	2680080	And the other way you can compare two things is the discrete way, right? You can say these two
2680080	2685920	things are exactly the same. They have exactly the same structure. Or maybe the structure of this
2685920	2691280	thing is a subset of the structure of this bigger thing. So this creates topology grounded
2691280	2696880	comparisons. So you have the geometry grounded comparison. It's all about distances and templates.
2696880	2702720	And then you have the topology grounded way of comparing things. That's all about exact comparison
2702720	2709760	or finding a sub graph isomorphism. So in the first case, your objects are very much
2709760	2714720	points in geometric spaces. So they are vectors. And deep learning is always a great fit for this
2714720	2719440	sort of stuff. And in the second case, your objects are going to be graphs, right? And you're
2719440	2722640	and you're really looking at the structure of these graphs and substructure and so on.
2722640	2730160	And you're doing always you're doing exact comparisons. And in practice, most thinking is
2730160	2735440	actually kind of some some combination of these two atoms, right? Of these two poles.
2736240	2742320	You're very rarely just going to say, yeah, this airport is exactly this close to my template
2742320	2746640	of an airport. So it's an airport. You're going to have basically layers upon layers of thinking.
2746640	2750880	And some of them are going to be intuitive. Some of them are going to be more about, you know,
2750880	2755440	comparing structures and so on. What you're saying is really interesting, right? Because you invoke
2755440	2762160	the kaleidoscope hypothesis in your paper. And the idea there is that a tiny bit of information,
2762160	2770160	just like in a kaleidoscope, could be represented widely across experience space. So you say that
2770160	2776000	intelligence is literally having some kind of sensitivity to abstract analogies.
2776000	2783280	So the intelligence is about being able to face the future unknown future, given your past experience.
2783280	2789200	And that's fundamentally requires the future to share some commonalities with the past. And
2789200	2794800	that's that's the idea of the kaleidoscope hypothesis that the universe and our lives
2794880	2801280	are made of lots of repeated atoms of structure. And in fact, if you look at the source,
2801280	2804640	there are very few things that are that are unique that are kind of like
2804640	2809360	the grains of sand that are at the origin of all the different kinds of moving patterns you
2809360	2814080	can see in the kaleidoscope, right? So the kind of like intrinsic structure contained in the universe
2814080	2821920	is very small, but it is repeated in all kinds of variants, right? And the idea is that if you see
2821920	2827360	two things in the universe that look similar to each other or that share some commonalities,
2827360	2833360	a subgraph, maybe, it fundamentally means that they come from the same thing. And that thing is
2833360	2837280	going to be is going to be an abstraction. We'll be one of these grains of sand in your
2837280	2844800	in your kaleidoscope or grains of glass, actually. And intelligence is all about reverse engineering
2844800	2850400	the universe to get back to this source of intrinsic complexity in the universe to get
2850400	2855360	back to these abstractions. I think the heart of this conversation goes back thousands of years
2855360	2859120	because what we're talking about right now is a lot of say, Platonism, right? Which is that there
2859120	2864400	are these ideal abstract structures. And of course, they they really thought of them as actually
2864400	2870720	existing in some universe. But you know, even if they don't exist in some reality, they at least
2870720	2876160	exist in concept. And it strikes at the heart of this duality that's always been a very
2876880	2880960	that's been one of the central mystery, really, of a lot of human thinking, which is
2881600	2887120	particle versus wave, you know, discrete versus continuous abstract versus the real versus the
2887120	2892160	messy. And you know, I think you pointed out, you definitely pointed this out in this call. But
2892160	2897040	I think also in some of your papers that in your view, you know, let's say the ultimate solution
2897040	2903120	or whatever of creating artificial intelligence or synthetic intelligence or whatever is a
2903200	2908400	hybrid system that can do both of these types of reasoning, maybe in kind of multiple layers.
2909040	2914880	And, you know, I'm kind of curious, where is the state of the art now with actually implementing
2915680	2919760	hybrid systems, you know, something like, I don't know, is it capsule networks? Is it the
2919760	2925840	topological neural networks that we talked about? Where where lies the direction of some type of a
2925840	2933600	hybrid system that in a unified way is capable of doing both of these modes of reasoning, if you
2933600	2939280	will? Yeah, that's a great question. So I think this is definitely an active field of research,
2939280	2945360	but I think the most promising direction right now is going to be discrete search very much. So
2945360	2949760	a system that is discrete search centric that has a DSA and so on. And that's one of the
2949760	2955360	it's basically just problems in this engine. But it is getting lots of help from deep learning
2955360	2961680	models. And there are two ways in which you can incorporate this type one sort of thinking into
2961680	2970080	a phenomenally type two centric system. So one way is so basically, you want to apply deep learning
2970080	2976960	to any sorts of data sets where you have an abundance of data, and your data is interpreted.
2976960	2981840	One example would be being able to easily play models to generate a sort of like perception
2981840	2990080	DSL that your discrete search process can build upon. So look at art, art tasks, for instance,
2990080	2994160	a human that is looking at art tasks, the very first layer through which they're approaching
2994160	2999600	the art task is by applying basically perception primitives to the grid they're looking at. They
2999600	3005040	are not actually analyzing the grid in a in a discrete way like cell by cell, object by object,
3005280	3010320	they're approaching it holistically, like what do they see? And these outputs can be discrete
3010320	3014560	concepts. And then you can start you can start applying the script reasoning to them. So generating
3014560	3020160	the DSL. And by the way, the reason it's possible is because humans have access to tons of visual
3020160	3025840	data and these different frames share lots of commonalities, right? So it is an interpolative
3025840	3029680	space where deep learning is relevant, where intuition and perception are relevant. And the
3029680	3035920	other way, which is is is much more difficult and much, much more subtle thing is basically being
3035920	3043040	able to provide guidance to the discrete search process, basically, because even though one single
3043040	3047600	program, so learning one single problem, for instance, for an art task is not a good fit
3047600	3052560	for deep learning model at all, because you only have a handful of examples to learn from.
3053360	3059920	And the program is super discrete. It's not really easily embeddable in this movement.
3059920	3064800	However, here's the thing, the space of all possible programs, for instance, the space of
3064800	3069120	all possible art tasks and all possible programs that solve art tasks is actually
3069120	3074800	very likely going to be interpolative, at least to some extent. And so you can imagine a deep
3074800	3081040	learning model that has enough experience with with these problems and the algorithmic solution
3081040	3086240	that it can it can start providing directions to the search to the discrete system. So
3087600	3092880	basically, you're in a kind of like you have, yeah, you have like layers of
3094000	3099120	of learning the lowest layer is going to be perceptive. It's going to be learned across many
3099120	3104080	different tasks and many different environments. It's going to be type type one, then you're going
3104080	3110480	to have the context specific on the fly problem solving system that's going to be type two.
3111200	3115280	And the reason is going to be possible and efficient is because it's going to be guided
3115280	3119840	by this upper layer, which is going to be type one, which is also going to be trained
3119840	3124960	from a very, very long experience across many different problems and tasks. And it is able
3124960	3132320	to do interpolation between different tasks. So can I challenge you a little bit maybe because
3132320	3137760	you say maybe, you know, all of these problems and what humans do is a bit of an interpolate
3137760	3145200	like an interpolation between the interpolative systems and the discrete systems. And I see that
3145200	3152000	going for, you know, something like an arc task or or if you really write code. But if you really
3152000	3158000	come to let's say, let's say the highest levels of human intelligence, which to me seems to be
3158000	3166880	navigating social situations, which is is is ultimately is super complex. And I can imagine
3166960	3173840	something like the graph structure you're referring to be that being, let's say I come into a room
3173840	3181440	and I see the graphs as, you know, what kind of social dynamics exist in this room, you know,
3181440	3187200	this is the father of this person, and that person's kind of angry at me. And so I need to,
3187200	3195360	you know, do something. And my question is, how often is that really a disk like how often can
3195360	3202000	you really map this in a discrete way to another graph? Isn't isn't every situation going to be
3202000	3209120	a little bit different, even in terms of its graph structure? And, you know, even if in an arc task,
3209120	3216640	a line is just like a little bit squiggled, any program synthesis approach would have a hard
3216640	3222160	time with it, I feel, or do you think, or do you think I'm misunderstanding something here? Like
3222640	3229840	how discrete is really discrete? That's the purpose of abstraction. The purpose of abstraction
3229840	3236880	is to erase the irrelevant differences between different instances of the thing and focus on
3237920	3243760	the commonalities that matter. So like if the squiggled in your line is not relevant,
3243760	3248240	then the proper abstraction for a line should abstract it away. I was going to pick up on that
3248240	3253360	because your main point basically is that program based abstraction is more powerful
3253360	3256880	than geometric based abstraction, because topology is robust to small perturbations,
3256880	3261840	but it's more than that. It comes back to these analogies, right? So we actually have functions
3261840	3267440	and abstractions in our mind that as you say, will take away all of the relevant differences,
3267440	3274320	but focus on what's salient and what's generalizable. Yeah, exactly. So in in the big sense, do you
3274320	3281040	think the type one and type two reasoning are really different or is there also a continuum
3281040	3286400	between them? Like you say we need we need hybrid systems, but is there something,
3287680	3291280	right? Because they're both they're both in the brain, they're both on the same neurons,
3291280	3297360	like is there a continuum? So right, so yes and no, I do believe they are they are very
3297360	3302240	qualitatively different. These are the two poles of cognition, but there are there are, you know,
3302240	3306800	most most things we do with our mind are a combination of both. That doesn't mean it's
3306800	3310720	it lies somewhere in between. It means it's a direct combination of one pole with the other,
3310720	3316160	kind of like what I described with with the arc solver with three layers, with two layers of
3316160	3321920	that type one and one layer in the middle of type two. But in very much the same way that you can
3322560	3329360	embed discrete programs in a smooth manifold, you can also do the reverse. And when you're
3329360	3335360	meaning you can basically encode an approximation of a geometric space using discrete constructs. In
3335360	3339760	fact, if you've done any sort of linear algebra on a computer, that's exactly what you're doing,
3339760	3344800	you're actually manipulating ones and zeros. But somehow somehow you're able to have vectors
3344800	3349440	of seemingly constant new numbers, you can compute a distance between two vectors and so
3349440	3354640	all of this is an approximation that's actually grounded in discrete programs. So you can you
3354640	3359280	can actually kind of merge the two together. It's not necessarily always a good idea. In
3359280	3365120	particular, I think it's often not a good idea to try to embed an overly complex or overly
3365120	3372400	discreet program in a constant new space. As I was mentioning earlier, the reverse is actually
3372400	3377920	usually way more tractable. And by the way, my I think this is something that came up before
3377920	3383920	in our conversation, but my kind of subjective totally not backed by any evidence opinion of
3384000	3390240	how the brain works is that fundamentally it's doing type one on type two, using a discrete
3390240	3395920	system, because it's actually much easier to do to type one via an approximation of a geometric
3395920	3399840	space that's encoded in a district structure than it is to do the reverse. Yeah, and if I can,
3400400	3404640	if I just for the benefit of the reader, the listeners, if I can give some other examples,
3404640	3409600	you know, for example, and mixed integer optimization, it's often the case that you
3409600	3414480	take that problem. And instead of having these discrete values, you project it into a continuous
3414480	3420160	space, do a continuous optimization. And then as you get sort of close to a good optimization,
3420160	3425520	you discretize it back over into the, the discrete variables, you know, to, to kind of,
3425520	3430960	you know, flesh out the most optimal path within that discrete space, or an example to is the
3430960	3435920	gamma function, you know, which is a continuous generalization of the factorial, right? And
3435920	3441760	it kind of provides some cool and interesting behavior in between those, those poles that
3441760	3446400	show up very clearly on the graph as these discrete points. And this is this bizarre
3447040	3451440	duality between the continuous and the discrete that we see like throughout the universe. And
3451440	3456960	it's kind of one of the strangest things we have to deal with. Yeah, exactly. I just wonder what
3456960	3460880	some of the transformers folks must be saying now, because Max Welling, we had him on and
3461600	3466640	folks have done topological applications using transformers or using graph neural
3466640	3472960	networks and the alpha fold, the thing from DeepMind, that was looking at graph isomorphisms,
3472960	3478880	right? It was looking at different types of equivariance in topological space. Is it a naive
3478880	3485440	thing to say that we could make it continuous or are we on a hiding to nothing? Right. So I guess,
3485520	3492080	I guess the question is, is there like one approach that's going to end up being universal? And it's,
3492080	3497200	it's like, can you actually scale deep learning to handle arbitrary district programs? It's kind
3497200	3503360	of, it's kind of the question. And the answer is no, actually, like, by, by construction, do,
3503360	3509600	due to the very nature of what deep learning is, it's like parametric continuous parametric models
3509840	3515200	in fact, smooths, because they're differentiable, sure. And it's quite undecent. That is never
3515200	3521760	actually going to be a good fit for most discrete programs. So, and, and the reverse is true as
3521760	3526320	well. I don't think, so you have basically two engines that you can use to learn problems. You
3526320	3531680	have quite undecent and you have discrete search. And I think the reverse is also true that discrete
3531680	3537200	search is not going to be this universal approach that's going to beat everything. I truly believe
3537280	3542640	that the AIs of the future will be truly hybrid in the sense that they will have these two engines
3542640	3546720	inside them, they will be able to do this, they will be able to do this quick search.
3546720	3550560	Right. And then, and then, and they will set, you, is that appropriate? You said, by the way,
3550560	3554640	in your measure of intelligence paper that there are three types of priors, right? Low level,
3554640	3558960	sensory motor priors and meta learning priors. That's the interesting one. I think that's got
3558960	3563840	intelligences and high level knowledge. And then we get over to the ARC challenge and, and as you
3563840	3568720	said in your presentation last year, the two winning folks on that Kaggle challenge, one was
3568720	3573200	doing a genetic algorithm over a DSL. So doing what you're talking about, a kind of program
3573200	3579120	search, and actually the winner who got about 20% accuracy. And that was, that was just, yeah, that
3579120	3585200	was just doing a brute force, you know, selecting combinations of, of operations on this DSL.
3586000	3591200	So this absolutely fascinates me. So at the moment, that seems like a horrific solution,
3591200	3595520	but clearly no one could do it using deep learning. So, but, but this is what you're
3595520	3599840	advocating for. So you're saying for these discrete problems, get, get a DSL. Now,
3599840	3603280	all the stuff you're talking about, presumably they haven't done yet, you're saying, well,
3603280	3607840	software engineering, the beauty of software engineering is being able to modularize things
3607840	3612560	into building blocks. And in fact, I love citing this thing actually from Patrice Simhard. But
3612560	3617280	he said, the reason why software engineering is so good is if I ask you, how long will it take
3617280	3622960	you to build the game of Tetris? You will say not long at all. And if you look at the number of
3622960	3628640	state spaces in Tetris, it's, it's huge. But the reason you'll be confident to build it in a couple
3628640	3634000	of weeks is because you know that you can modularize it into, into blocks, you can't say the same for
3634000	3638160	deep learning, right? But they don't appear to have done that on the arc challenge yet.
3638880	3643440	Yeah, so the, the solutions we've seen on the accident so far have been incredibly,
3643440	3648320	incredibly primitive. And so it's, it's actually quite interesting that you can get to 20%.
3649360	3654640	It's very primitive solutions. I think you can, even with today's technology, you can go much
3654640	3660560	further. Like the, what I was describing before about learning a DSL that is perceptive and then
3660560	3665840	guiding discrete program search. Yeah, intuition about program space. This is already something
3665840	3670640	that you can try today. So there's one approach that I was very excited about. And that I thought
3670960	3676880	was very cool. And I really like it's, it's called Dreamcoder by Dr. Kevin Ellis and, and folks.
3677920	3681760	So check it out if you, if you have incidents, it's very good. I think that they're trying to
3681760	3687680	play to arc now, but it's generally like, is this kind of like hybrid deep learning programs into
3687680	3693440	this engine? And I think that's really to me, that that is the sort of direction that is the most
3693440	3699360	promising to that. So you have a paper that's fairly long on, it's called on the measure of
3699360	3705120	intelligence. And you make the case that intelligence is something like the efficiency
3705120	3711440	with which we transform prior information and experience into task solutions, as, as you have
3711440	3718240	said before. And in that same paper, the arc challenge is presented. So, you know, a naive
3718240	3724160	reader like me assumes there is some connection between, you know, what you say about intelligence
3724240	3731280	and solving this arc challenge. So my question is, if tomorrow, you know, a new team comes and
3731280	3737440	gives you a solution, you evaluate it, it gets whatever 95% correct, it solves the arc challenge.
3738160	3745840	Is it immediately intelligent? Or what would you ask of that system for, for you to say,
3745840	3750800	yes, that's intelligent, or it's, it's intelligent is, is high or something like this.
3750880	3758000	So you, you, you would be able to make that, that conclusion, if and only if arc was a,
3758000	3763280	was a perfect benchmark, but it's not, it's actually very much flawed. So if you solve arc,
3763280	3770080	are you, are you intelligent? Well, no, because arc is potentially flawed. That's, that's the
3770080	3776560	thing. So the thing you need to really understand about arc is that it's not kind of the end state
3776560	3782240	of the intelligence benchmark. It is very much a work in progress. And there will be new iterations,
3782240	3787680	especially as we learn more about the flows. And by the way, so last year, we ran a Kaggle
3787680	3793200	challenge on arc, and we learned a ton, not necessarily a ton about program synthesis approaches
3793200	3798160	although there were some cool stuff we still learned about and so on. But mostly we learned about
3798160	3803200	the flows of arc. So there will be future additions and so on. So I will tell you this,
3804080	3809600	if you solve the specific test set of arc as it exists today, you're not necessarily intelligent
3809600	3816000	because it is not perfect because it has its laws. But if more generally speaking, you give me a system
3816000	3823280	that is such that any new arc task I throw at it, like I can, I can make some new ones tomorrow,
3823280	3827520	for instance, I give them to your system. If it's always solving them, I will say,
3827520	3831280	yeah, it's looking like you've got a system that's, that's got, you know, pretty close to
3831280	3837760	human level fluid intelligence. This is one of the things that, look, and I like the paper a lot,
3837760	3843520	I think, I think it serves as a really good, you know, foundation for us to think differently
3843520	3848000	about how to build intelligence. But, but I have some, some issues with it too as well. And one
3848000	3854080	of them is this sort of necessity that it requires kind of white box analysis of things in order to
3854080	3857920	figure out whether or not they're intelligent. Because for example, suppose time travel is
3858000	3863920	actually possible. And you know, somebody like 100 years from now looks back on your arc thing and
3863920	3868480	writes an algorithm that, that solves all, all them in there because it actually knows about them
3868480	3873040	already and then ships it back into the past and we enter it into the competition. And no matter
3873040	3877200	what new arc thing you throw at it, it sort of does well. And you say, well, yeah, you know,
3877200	3882160	this thing's like kind of intelligent, but, but we'd be wrong because in the sense in the paper,
3882160	3886240	it's actually just encoded, you know, prior knowledge from the future. So we have to,
3886240	3890160	we always have to kind of be able to look into the box, right, in order to evaluate
3890720	3894400	intelligence in the way that you define in the paper. And so my question is one,
3894960	3901200	isn't that a bit of a undesirable feature? And two, do you have any hopes for a more black box
3901200	3906160	measure of intelligence? So basically, the fundamental issue is that if intelligence
3906160	3913040	is this conversion ratio, then computing it requires knowing where you start from. And
3913040	3917600	you don't really have a way around it. So the thing to keep in mind is that the
3917600	3923280	under measure of intelligence stuff is not so much meant to provide like a sort of like
3923840	3928480	golden measure of tape to measure anyone's intelligence or anything's intelligence.
3928480	3936080	It is more meant as a sort of cognitive device to help you think about what the actual challenges are
3936800	3941680	to help you kind of kind of reframe AI because they think they have been pretty deep and
3941680	3946320	longstanding conceptual misunderstandings. So that is really being, that's being holding the
3946320	3954400	feedback. So it's very much meant as a cognitive device. If you take a step back and you ask,
3954400	3959200	why are we even trying to define intelligence and measure intelligence in the first place,
3959200	3965280	why is it useful at all? I think it's useful to the extent that it is actionable, right,
3965280	3970800	a good definition and a good measure should be actionable. So meaning it should help you
3971600	3977280	think, it should help you find solutions and it should help you make progress. In particular,
3977280	3982640	a good definition is a definition that will highlight the key challenges and help you think
3982640	3987200	about it. And I think that's what the paper does. And a good measure is a measure that gives you an
3987200	3993760	actionable feedback signal towards building the right kind of system, right in the sense that
3993760	3999840	it will be capable of doing more. And so that's part of the feedback signal is what ARC is trying
3999840	4008720	to achieve. And the way it's trying to control for priors and experience is by assuming a fixed
4008720	4013680	set of priors. And you're going to see, you know, every test taker can have such priors.
4013680	4018800	This is the core knowledge priors. And then it controls for experience by only giving you a very
4018800	4025360	small number of input examples. And also by making sure the tasks are sufficiently novel and
4025360	4031440	surprising that you're unlikely to have seen a very similar instance before. So now, of course,
4031440	4036240	it's super flawed. So this is not 100% true, of course, but this is kind of like the the
4036240	4040880	planning ideal that we're trying to get to. So that for the record, that's a fascinating point to
4040880	4045200	me is that you view this more as a cognitive device to help guide us to produce better,
4046000	4052880	better intelligent agents. It is not an input. It's not like ARC is like the measure of intelligence
4052880	4059200	and all we need to do is solve ARC. This is not at all the point. It's like it's one.
4059200	4063280	Oh, darn, because I was doing pretty well on some of the examples. I was hoping that would
4063280	4067280	mean I was intelligent. But another interesting point, because Keith and I were looking at the
4067280	4071840	paper again yesterday, because it's been, I haven't properly studied it since last year. But
4072880	4076320	we were starting to talk about an alien that comes in from outer space. And, you know,
4076960	4082640	we don't know the priors and the experience. And then I was thinking in a way, it might be a
4082640	4088240	kind of lower bound on intelligence, right? Because, you know, if I play chess, and if I beat
4088240	4092720	someone with a higher elo than me, then only really tells me that I'm better, you know, as
4092720	4098320	good as that person that I just beat. And similarly, this measure of intelligence, it only gives you
4098320	4103600	a reading in the situation when you know what the conversion was. So if they are not converting
4103600	4109120	anything, then you don't know. And another interesting byproduct of this is the more
4109120	4116480	experienced you get, the less intelligent you get. So I would push back against that last claim
4116480	4121520	that the measure of intelligence as I define it is dependent on how much experience you have.
4123360	4127120	Because the amount of initial experience you have does not actually change at the conversion
4127120	4134240	ratio if you measure it via the right task. So you might need, so if you have a fixed set of tasks,
4134240	4139520	then yes, it does affect it. But if you're able to renew your set of tasks and come up with
4139520	4144000	styles that are orthogonal to the experience that you have, then it's not the actual effect,
4144000	4151280	the definition. So, but yeah, you're definitely right that if you take a pure black box approach,
4151280	4157600	and all you're looking at, the only thing you can really measure is the behavior of a system.
4157600	4163520	And unless you know how that behavior is achieved, you can't really tell immediately
4163520	4169040	how much intelligence was involved in producing this behavior. If you look at an insect,
4169040	4173920	they're capable of super complex behavior. Are they crazy intelligent? Well, actually,
4173920	4179360	you know, probably not. And the way you can really tell is by putting these systems out of
4179360	4184800	their comfort zone, getting them to face novel situations and see how they adapt. And that's
4184800	4192160	the measure of intelligence. It's adaptability, the ability to deal with novel and unknown
4192160	4198960	situations. But in order to give your system a novel and unknown situation, you need to have
4198960	4205280	this white box understanding of what it already knows about. And that's not really something
4205280	4212400	you can work on. So can I ask about the generalization difficulty? Because I sort of had
4212400	4217760	some difficulty intuitively with some of, let's say, it's limiting cases. So for example,
4218720	4223760	you know, the algorithmic complexity is highest. Let's just suppose we're dealing with problems
4223760	4230320	tasks where we have whatever sets of integers mapped to zero, one values, you know, the algorithmic
4230320	4236080	complexity will be greatest when that's just a random mapping, like I just assigned zero and one
4236080	4241280	randomly to every single integer. And if I go to look at that generalization difficulty,
4241280	4246320	it's going to be super high, because the length of the program for any set is basically going to
4246720	4252320	be, you'd have to encode the entire set as a hash table, right? So how does like this measure
4252320	4259120	account for or help us avoid problems where we're confusing generalization difficulty with just
4259120	4264880	increasing random, you know, randomness? Well, I mean, increasing randomness is a part of
4264880	4270800	the realization difficulty, right? Generalization is really the ability to deal with the stuff you
4270800	4275840	don't know about the stuff you don't expect, the stuff you haven't seen before. And randomness is
4275920	4280400	a part of it. But you're right that if you just add randomness to a system, you're increasing
4280400	4285680	the generalization difficulty, but you're not increasing it in a very interesting way, right?
4285680	4291440	Because you're increasing it in a way that's kind of orthogonal to an integration system's ability
4291440	4297520	to deal with it, right? The best you can do is modify the system to be more robust to very much
4297520	4303120	randomness. But that's not super interesting. What's really interesting is to test the system's
4303120	4310320	sensitivity to subtle analogies, is to make the system face novel and unexpected situations that
4310320	4316160	are actually derived from the past, but in interesting ways, right? Not just random ways.
4316160	4324720	You've run this Kaggle challenge on ARC. And, you know, we know from systems such as Alpha Go and
4324720	4332000	so on that bootstrapping intelligent, like bootstrapping AI systems can be very valuable,
4332000	4338640	like playing them against each other and so on. And also, we know that something like markets can
4338640	4349200	be very efficient and valuable. And I imagine a system where you'd have agents creating ARC tasks
4349200	4355040	and other agents solving ARC tasks, and they're going some kind of money around and so on. And
4355040	4361120	this could be kind of a powerful engine for research teams to research anything like this.
4361120	4367600	And, you know, given that you have, I don't know how much, but you do have the backing of Google
4367600	4378160	with a bit of capital in hand. Could you imagine there being a push for this kind of thing? Or is
4378160	4388880	it, as of now, an intellectual curiosity? Yeah, so I don't have that much backing you from Google
4388880	4395440	around this kind of project. But, yeah, so it would be super interesting to have this kind of
4395440	4400640	two-part system where one part is generating the task and one part is learning to solve them.
4400640	4406720	And you could get them to do some kind of curriculum optimization, like the task generator network
4406720	4413040	would not just be trying to generate tasks that look like ARC tasks. It would be trying to
4413040	4419120	generate tasks that correspond to level of generalization, difficulty and complexity that is
4420080	4425440	right below the limits of the student system that's trying to solve them. Kind of like, you know,
4425440	4432720	the way a teacher would provide exercises that are solvable, but challenging. They shouldn't be.
4432720	4436720	They shouldn't be easy. They shouldn't be impossible. They should be solvable. Because
4436720	4441760	that's how you get the most growth. So it's actually a system that's described at the very end
4442480	4447680	of the paper on the measure of contagions. And I think one thing I point out in the paper is
4447680	4453360	kind of like the pitfall you should avoid falling into is that this system is circular,
4454000	4460000	right? And the complexity you're going to see in your task, it needs to come from somewhere, right?
4461200	4466720	It's like conservation of complexity. So the system, this two-part system needs to have
4467680	4473840	a source of intrinsic complexity. It needs to be grounded in the real world.
4473840	4479360	And one way we can achieve that grounding, and I've been thinking about it, is I think we should,
4479360	4485120	you know, like ARC tasks, as they are today, they're made by me and this is not a good setup
4485120	4490400	because it's going to be biased. It's going to be very bottlenecked as well. I think we should
4490400	4495920	start crowdsourcing our task. There should definitely be, you know, a filtering system so
4495920	4500000	that we make sure that we're only keeping our tasks that are interesting, that are not too easy,
4500000	4506480	that are not difficult, and that are only grounded in core knowledge priors. But if we have, like,
4506480	4512000	this stream of novel ARC tasks that contain intrinsic complexity and novel information,
4512000	4517040	because they come from the real world, they come from human brains, that have experienced the
4517040	4523120	real world, and you use that as a way to ground your task generator, then you're starting to get
4523120	4529440	a very interesting three-part system, right? So I would love to actually get that started,
4529440	4536240	to actually produce a V2 of ARC as soon as possible, let's include, you know, 10x more tasks
4536240	4541440	that will be crowdsourced, and maybe something that will take the form of a continuous challenge
4541440	4546240	where you have an API where you can draw a new ARC task, and every time you draw a task, it's
4546240	4552400	actually a different one because you have so many of them. Gamify it, that'll make a fun game
4552480	4557440	on a mobile app. There are actually a few people who have created, because ARC is open source,
4557440	4561120	and they're totally free licensed, there are a few people who have created mobile apps where
4561120	4565120	users sort of ARC tasks, and apparently it's popular. So there's also the other angle you
4565120	4568960	mentioned in the paper, which was, which is pretty fascinating, you're talking about it almost right
4568960	4577440	now, which is that, okay, let's start thinking about how to map ARC performance to psychometric,
4577440	4581680	you know, classic kind of psychometric tests. Are there any efforts that you're aware of
4581680	4590000	underway right now to do that? Are you involved in any ETAs? Yeah, ETAs, I'm not sure. So we did a
4590000	4595600	workshop at AAAI the other day, and there were two presentations about efforts that teams of people,
4595600	4601280	so there are people who do neuropsychology, and they're using ARC in very interesting ways. So
4601280	4606960	there's a group at NYU, and there's a group at MIT, and yeah, so they're using ARC for neuropsychology
4606960	4613280	experiments, and it's it's super cool. Amazing. I want to switch over a little bit, because of
4613280	4618640	course, you know, other than the measurement of intelligence, you are also famous for a small
4618640	4627280	library you wrote once in a while called Keras. And I wish I wrote it, and then that was that.
4628640	4632560	No, I yeah, it's been very much an ongoing project for the past six years.
4633520	4639040	It was because I remember, you know, the days of TensorFlow one and and Theano,
4639840	4646320	and things like this. And Keras was just, I think, so helpful to a lot of people, because it just
4646960	4653360	simplified all of this, you know, graph construction, whatnot, and so on. It just made it accessible to
4653360	4660240	so many people. And now with the development of, you know, things like PyTorch and TensorFlow two,
4660320	4667760	it almost seems like Keras is it has been kind of absorbed by TensorFlow two, right, there is TF.Keras.
4667760	4674240	And now I think the newest APIs are even sort of vanishing that a little bit. Do you do you see
4675280	4680720	Keras going away? Do you see it changing? Where do you see it? Where do you see Keras going?
4681360	4686880	Yeah, so going away, definitely not. I mean, we have we have more users than ever before. And we
4686880	4691760	are still growing very nicely, both inside Google, like one more teams that Google are moving away
4691760	4697040	from TensorFlow one and adopting Keras and outside Google as well. It's a big market out there,
4697040	4703760	and there's definitely room for multiple frameworks. Evolving absolutely, I mean, Keras is constantly
4703760	4710080	evolving, but evolving with continuity. Like if you look at Keras from 2016 or 2015, you look at
4710080	4716240	Keras now, you recognize, is it the same thing? And it's the same API. And yet it's actually a very
4716240	4722560	different and much, much bigger set of features and things you can do it. So evolving, definitely.
4722560	4729440	And there are so several, so you, I think you asked, you know, about, yeah, like,
4729440	4733520	Keras is getting kind of merged into TensorFlow, does it mean it's like failing away?
4733520	4739840	So definitely not. So merging with TensorFlow was a good idea because it starts enabling
4739920	4746480	a spectrum of workflows from the very high level, like scikit-learn like, to the very low level,
4746480	4753760	numpy like, and everything in between. In the early days, because Keras had to interact with
4753760	4760080	multiple backends via backend interface, it means you had this kind of like a barrier where as long
4760080	4765520	as you use the Keras APIs, everything was super simple. It was scikit-learn like, so very easy,
4765520	4771520	very proactive, very fast. But if you wanted more customization, at some point, you would hit
4771520	4777440	that backend barrier. And you had to reverse to TensorFlow base or piano base workflow,
4777440	4782640	that was low level, but when, where you couldn't really leverage Keras effectively,
4782640	4787920	by removing the backend thing and just saying the flow together in one spectrum,
4787920	4793680	then you get really this progressive disclosure of complexity when you can start out with the
4793840	4798880	very high level thing, but then you need to customize your training step. You have an API for
4798880	4805520	that. And you can just mix and match seamlessly the low level TensorFlow stuff with the high
4805520	4810000	level Keras step. And that way you can achieve any, can work with Keras and TensorFlow at the
4810000	4815680	level of abstraction that you want. Very, very easy high level or very, very low level full
4815680	4821520	flexibility. It's up to you. I'm going to point out the temptation here to analogize connecting
4821520	4827840	type one with type two reason. Yeah, why not? I was just about to do that. At least Francois
4827840	4832880	has great form for this, because not only does he talk about having powerful and useful interfaces
4832880	4838160	and abstractions in deep learning, he's been playing this game in the library world for quite
4838160	4842960	some time. But I wanted to touch on this quickly. We had a couple of people in our community asking
4842960	4850560	you about Keras, actually. And Robert Lange and Ivan Finnell said that apparently Theano has returned
4850560	4855120	with Jax and XLA underneath and he wants to know are there any plans to add it as a Keras back end
4855120	4860000	and Robert Lange also says, you know, just Jax on its own. Would you add that as a back end?
4860000	4863200	We've also had a couple of questions about PyTorch as well. Is there anything on the
4863200	4867920	roadmap for that? Okay, so let's talk about Jax. I think Jax is an awesome project and the
4867920	4872880	developers have really done a very, very interesting and very good job with it. And lots of people,
4872880	4878240	I like Jax actually. So that said, adoption is not super high. I think Google is probably the
4878240	4882880	company where it's the most adopted, where you will find the most users. And even then,
4882880	4888640	it's like a tiny, tiny, tiny fraction of total machine usage at Google. But I think as a project,
4888640	4895040	it's a beautiful project. It's elegant. It's powerful. It's great. So would I like to add
4895040	4901200	Jax back end to Keras or PyTorch back end to Keras? So I want to say we've really moved away
4901200	4909680	from this like interface back end kind of model. So precisely for the reason I was describing,
4909680	4913680	because you want to achieve this spectrum of workflows, with that, I think this cliff where
4913680	4919280	you go, you fall from the high level down to the low level. We don't want the cliffs. We don't
4919280	4923520	because cliffs create silos of users where you have the high level users. You want a gradient.
4923520	4929040	Yeah, you want the gradient. Exactly. So that said, I think it would be super cool to have a
4929040	4935040	sort of like re-implementation of the Keras API on top of Jax that will also achieve this screening
4935600	4939920	and that will still follow the Keras API spec. It would still be the same thing,
4941600	4945840	but on top of Jax. That said, so I would love to see something like this. This is also a very
4945840	4951120	low priority for us because we have the actual current Keras, which I wish we need to work on,
4951120	4955840	which has lots of users. So we don't really have time to do this. But in theory, would it be cool?
4955840	4960400	Yeah, sure. I would love to see something like this. So if I had tons of free time, I would
4960400	4963760	probably build it, but in practice, I don't. Fantastic. Well, we've got another question
4963760	4968400	from Giovanni actually. He says, what does Francois think of Dr. Kenneth Stanley's book on the myth
4968400	4972640	of the objective? Are you familiar with Kenneth Stanley's work about the tyranny of objectives
4972640	4980400	and open-endedness? So I'm vaguely familiar with the name. I'm not really familiar with the book.
4981120	4986560	Oh, okay. Well, sorry, not to worry, but it's Kenneth has been a huge inspiration for me.
4986560	4994560	And he talks a lot about objectives leading to deception. So sometimes following an objective
4994560	4999520	monotonically sends you in the wrong direction. And his solution to that is either quality,
4999520	5004000	diversity, or more recently, open-endedness, which is that if you have an infinitude of
5004000	5009280	objectives, in a sense, the system has no objective. And you can also with diversity,
5009280	5013520	preservation, you can overcome deceptive search spaces. But yeah, you might have heard of the
5013520	5018480	poet algorithm, which he was involved in. Yeah, absolutely. No, I'm aware. And so when it comes
5018480	5024720	to your description of the problem's objectives, I completely agree that one thing I mentioned
5024720	5031280	in the paper, it's like the shortcut rule, which is that if you try to achieve one thing, one
5031280	5035680	objective, you're going to achieve it. But the thing is, you're going to take every shortcut
5035680	5039920	along the way for things that we are not actually incorporated in your objective.
5039920	5044560	And this leads to systems that are not actually doing what you wanted them to do. Like for instance,
5044560	5050880	we built chess playing systems, because we hoped that a system that could play chess would have to
5050880	5056640	be able to feature reasoning, book learning, creativity, and so on. Turns out it just plays
5056640	5063200	chess. That's what it does. The same is true with challenges and Kaggle. The winning systems,
5063200	5068160	they just optimize for the leaderboard ranking and they achieve it. But they achieve it at the
5068160	5074320	expense of everything else that you might care about the system. Like, is the code base readable?
5074320	5079520	No. Is it computationally efficient? No, it's actually terrible. You could never put it in
5079520	5084080	production. Is it explainable? No, and so on. Yeah, so it's like, if you if you optimize for
5084080	5089280	something, you get it, but you take shortcuts. Yeah, exactly. And that's very much what Kenneth
5089280	5093440	says as well. I love what you said about shortcuts. You said in your New York's presentation that if
5093440	5098320	you optimize for a specific metric, then you'll take shortcuts on every other dimension, not
5098320	5102800	captured by your metric. And you said in a machine learning context, it's similar to overfitting,
5102800	5107760	right? Because on task specific skills, you actually lose generalization if you get good at
5107760	5112080	a particular task. So it's completely orthogonal to what you want. I know you're very well known
5112080	5116640	for your skepticism of the intelligence explosion. And what I love about your conception of
5116640	5121840	intelligence is that you think of it as a system or as a process, you say that intelligence is
5121840	5128240	embodied, right? So you have a brain in a body acting in an environment. And in that context,
5128240	5132880	it makes sense that you would think that there are environmental kind of rate limiting steps to
5132880	5137920	any kind of super intelligence, right? But I spoke to someone the other day who is of the other
5137920	5142880	persuasion, shall we say, and this person was saying, Well, what if you had a super, super
5142880	5149120	smart bunch of scientists? I know you said in your rebuttal that if you look at the IQ of a
5149120	5155600	scientist who is Richard Feynman, for example, the same IQ as a mediocre scientist, turns out
5155600	5161040	that IQ only helps up to about 125. And then it stops helping you. But these people would say,
5161040	5165280	Oh, well, you know, what if what if every single scientist was an Einstein and intelligence is
5165280	5169360	just making better decisions, they would consistently make better decisions and science
5169360	5174720	would accelerate. A chimp doesn't understand how good a human is. So how would we understand what a
5174720	5178480	super intelligent person would do? You know, they'd invent nanotech, they'd upload themselves into
5178480	5182960	the matrix, they'd do all of this stuff, and somehow they would miraculously overcome. Do you
5182960	5187440	know what I mean? How would you respond to that? Yeah, if every scientist was super intelligent
5187440	5192400	in human terms, that would in fact accelerate science. But it would not really like accelerate
5192400	5199200	science in a linear fashion and very much not in an exponential fashion. So I guess the main
5199360	5206080	conceptual differences I have with these folks is that they tend to credit everything humans can do
5206080	5212000	to the human brain. And they have this vision of intelligence as you know, a brain in a jar
5212000	5215920	kind of thing. And if you tweak the brain, it gets more intelligent and intelligence
5216560	5222080	is directly expressed as power. If you're more intelligent, if you have a hierarchy, you can
5222080	5227040	do more things, you can solve more problems and so on. And in particular, you can build a better
5227120	5232560	brain. And by the way, there is not really any practical evidence that this is true. But
5232560	5238160	I view intelligence here more as this holistic thing that okay, you have the brain, but actually
5238160	5244640	the brain is in a body which gives it access to a certain set of actions it can do and set
5244640	5250320	up a perception primitives. And this body is an environment which gives it access to a set of
5250320	5258160	experiences, a set of problems it can solve. And to a very large extent, you know, the brain is just,
5258160	5264400	it's not so much a problem solving algorithm, like a problem center descending, as it is a
5264400	5270080	big sport. And you put it in an environment to absorb experiences from that environment. And
5271280	5276000	one thing that's super important to understand if you're on issue, if you really think deeply about
5276080	5282880	intelligence, is that most of our expressed intelligence does not come from here, it is
5282880	5286800	externalized intelligence. So externalized intelligence can be can be many things.
5288480	5294400	If I look up something online, that's externalized intelligence, Google is part of my brain. If I
5294400	5300240	write a Python script to test some idea, that's externalized intelligence, my laptop is part of
5300240	5307600	my cognition, and so on. But it's actually, it goes much further than that. Most of our cognition
5307600	5315360	is crystallized, the crystallized output of someone, someone else's thinking. And the process
5315360	5320880	through which we get access to all these accumulated outputs of people's thinking is civilization,
5320880	5330720	right? And like 99% of the things you think are the behaviors you act, the behaviors you execute,
5330720	5337840	you did not invent them. You did not solve the underlying problem yourself. You're just copying
5338880	5344400	a solution. You've seen like, we're in the middle of a pandemic, you're probably washing your hands
5344400	5349840	after you went outside. And that's a very smart behavior. But did you invent it? Did you come
5349840	5355680	up with that? No, actually, other people came up with that. You did not also come up with the
5355680	5360480	infrastructure that enables you to do it in the first place. And so, and this is true, you know,
5360480	5367920	for even the most intimate of your thoughts, you're thinking with words that you did not invent,
5367920	5374400	you're thinking with concepts that you did not invent or that you did not derive from your own
5374480	5381520	experience. They really come from other people, from this accumulation of past generations.
5381520	5389040	And if you want to enhance the expressed intelligence of people, then this is actually the
5389040	5394880	system you need to tweak and improve, not the human brain, but civilization, right?
5394880	5400880	In a way, that seems like a contradiction, because you're talking about the externalization of knowledge,
5401440	5407760	not intelligence. So by your own definition, isn't that the opposite of intelligence?
5408400	5414000	That's a great point. So I'm relating expressed intelligence. So I was specifically saying
5414000	5418000	expressed intelligence as opposed to fluid intelligence. And what expressed intelligence
5418720	5422720	means in this context is something very different from what we talk about in the measure of
5422720	5428640	intelligence. It means intelligence behavior. And in particular, I think the ability to solve
5428720	5433120	problems that you encounter as an individual. Typically, when you solve a problem as an
5433120	5438640	individual, you're actually using a solution you found somewhere else. There are not that many
5438640	5445040	problems that as an individual, you solve from scratch in your own lifetime. But here's the
5445040	5450400	thing is that if you're able to actually solve something novel yourself, you have the ability
5450400	5455120	to write about it, you have the ability to communicate it, and then the next generation can
5455120	5462560	benefit from it. So let me just pose a kind of a counter argument to this. So suppose you're
5462560	5468160	reading a novel about, I don't know, a kind of planet of the apes or something, which was a
5468160	5476240	planet that had a life form similar to ours, but with a significantly lower IQ. And a human being
5476240	5481360	shows up there one day, and these things start writing about this, hey, this weird alien just
5481360	5486480	showed up here, and we captured it, we ran some tests on it, and we figured out it's really
5486480	5492720	intelligent. It's much more intelligent than any of us are. And we're worried what's going to happen
5492720	5499040	when 100 of them show up instead of just this initial explorer. And some other of these guys
5499040	5504320	were like, ah, don't worry about it. They've got two legs and two arms like us, and most of what
5504320	5510480	they are is kind of outside of their brain. So I'm not really worried about it. We would be
5510480	5516640	reading that with trepidation, right, because we know that when this more intelligent species
5516640	5521920	with more fluid intelligence, more externalized intelligence, better technology, all this kind
5521920	5527040	of stuff shows up, those guys are going to get wiped out. And it's actually happened like many
5527040	5532640	times throughout human history, not that humans were more fluid intelligence showed up and killed
5532640	5538560	off, you know, other people, but humans that had more externalized intelligence or more, you know,
5538640	5542000	represented intelligence and technology certainly showed up and dominated.
5542000	5547120	Absolutely. You're saying it yourself that when it has happened in history, it was not
5547120	5554160	fundamentally about one people having smarter brains, but one people having higher technology.
5554160	5558800	But that that is not something that is attributable to intelligence itself, right?
5558800	5562320	There's a connection there. If you did have a group of species or whatever,
5562320	5567440	that was much more intelligent, they will have advanced technologically much faster and further
5567520	5570720	in any given amount of time, all else being equal, right?
5571520	5576960	It depends on many factors. And that's kind of my point is that is your brain a factor? Yes,
5577600	5582480	absolutely, it is. But there are other factors like we are just talking about the development
5582480	5588880	of technology. So in that case, the critical factor was not the brain, but the superstructure
5588880	5593520	in particular communication and environmental constraints around it. The direction in which
5593840	5600720	civilization develops is a direct function of the specific challenges it encounters that
5600720	5605280	come from its environment, that comes from its surrounding enemies, and so on. And
5605920	5612720	technological development advances the fastest when you have a civilization that are dealing with
5612720	5617040	very harsh challenges, but that are not quite fortunate to work them out.
5618000	5624400	Because that's what forces them to develop as fast as it can survive. So this is actually a
5624400	5629680	very good example where the critical factor was the superstructure that guided the development
5629680	5634000	civilization was not actually the brain. But of course, yeah, if a one is smaller,
5634000	5641520	then civilization will advance faster. But my point is that there are many factors and that
5641600	5647200	by tweaking one factor, the brain, if the brain stops being the bottleneck, then immediately
5647200	5654240	some other factor will be the bottleneck. There are civilizations that have not actually advanced
5654240	5660560	very much at all because they simply did not face any changes. And did they have worse brains? No,
5660560	5665680	actually, they had exactly the same brain. But somehow the outcome was different because
5665680	5671920	something else, then the brain turned out to be the bottleneck like lack of environmental change.
5672960	5678080	I'm fascinated by scale and bottlenecks in systems. Actually, I work in a large corporation and
5678080	5682880	when you have role fragmentation and lots of different businesses and lots of different
5682880	5688720	organization or structures, some people might decide to structure themselves based on data
5688720	5694480	domain or based on organization or based on something else. And you can think of it topologically.
5694480	5700160	And I think human society is very similar to this. And I'm not sure whether evolution
5700160	5705200	would lead itself to one particular topology. But the environmental structures and the ways
5705200	5711680	that we organize ourselves can create incredible bottlenecks. And that seems to be where the real
5711680	5717120	interesting stuff goes on rather than the individuals. And I think you would agree with that,
5717120	5722800	Francois. Yeah, absolutely. If you take two companies, and in one company, the average IQ
5722800	5728320	is like 15 points higher, but it has a terrible organizational structure and terrible incentives
5728320	5734160	and the promo process is super broken or something. And that company is actually going to perform worse
5734160	5739360	than the more progressive innovation encouraging company that has a very nice organizational
5739360	5745280	structure and where people are actually more mediocre. Maybe they have on average 15 points
5745280	5750000	less in IQ, but they're actually going to do a better job because they have the better superstructure.
5750640	5755680	Yeah, it's fascinating that the problem is in most corporations, you can't actually design the
5755680	5761600	information architecture to be more efficient, because everything is so decentralized and
5761600	5766720	fractionated, you can only do it in pockets. And if you try and fix something in one part
5766720	5770160	of the organization, everyone else will say, well, my requirements are different. I'm not going to
5770160	5774240	wait for you. I'm going to do it my own way. And it's actually a really, really difficult thing
5774240	5780640	to do well. To sum up the whole like intelligence explosion thing, the point is really that it's
5780640	5786000	a system you have to look at holistically to get it holistically. And just by tweaking one factor,
5786000	5791280	which is the intelligence of an individual human brain, then what this means is this factor starts
5791280	5795680	being the bottleneck. But that means some other factor in the system, because there's an infinite
5795680	5800480	factor that will become the bottleneck. And by just focusing on one factor, you're not going to
5800480	5806000	actually lift all the votes. Yeah, and I actually agree with you. However,
5806960	5811040	I do want to say, I think we just don't know. I think both sides of the intelligence,
5811040	5817360	quote unquote, explosion really can't say for certain that it will or will not pose a mortal
5817360	5822160	threat to humanity. I think we have to accept that it's at least a risk factor. And we have
5822160	5829280	to be very careful about, in the future, when we start embodying, if we find general intelligence,
5829280	5833760	we need to be cautious. If we come up with something that looks like general intelligence,
5833760	5839600	there is absolutely some risk potential around it. However, I've never seen anything coming
5839600	5845520	anywhere close to that. In fact, the systems that we have today, they fit your almost no
5845520	5849920	intelligence whatsoever. So I think it's a bit early to start banning them.
5849920	5853840	And even if we get into that conversation, I think Francois would say that intelligence
5853840	5857360	must be specialized, right, because of the no free lunch theorems.
5857360	5862560	If you define intelligence as your ability to solve problems, then yeah, it's going to be
5862560	5869680	specific to a scope of problems, a kind of problems. And like, yeah, what the no free lunch
5869680	5875200	theorem is saying is basically, if you want to learn something from data, you have to make assumptions
5875200	5880000	about it. Which is why you know a convent, for instance, is a great fit for image data. It's
5880000	5884880	not really a great fit for natural language processing. And because it makes different
5884880	5889440	assumptions about destruction. It doesn't give me a lot of comfort, though, because I'm fairly
5889440	5894560	certain that whatever the first AGI that gets created, it's going to be highly specialized
5894560	5899920	for killing other people, because it's going to be a military, you know, secret project,
5899920	5906480	probably that finds it. You know, it's, I don't know. But what I know is that right now, we don't
5906480	5914640	have anything coming close to AGI. It's probably going to be actually a system that just displays
5914640	5921200	you ads. Like if, like, if, you know, if you, if we see where the most money is right now, the
5921200	5926960	first AGI is probably just going to like write, not only display, but write the perfect ad for
5926960	5932800	you on the fly. You know, it knows what you ate and you know, I know you're joking with
5932960	5938240	actually think on the, on the more serious, I think that's highly unlikely because of the
5938240	5943920	short code of the story because of the short patrol. I don't think a general intelligence is
5943920	5948720	going to be created by the military is not going to be created by a system that's trying to show
5948720	5954640	you ads because these are specific goals. And so if you try to optimize those specific goals,
5954640	5959920	you're going to end up with a very specialized system in order to build a general intelligence,
5960000	5967360	you need to be optimizing for generality itself. So it's going to come from, if it comes from the
5967360	5971360	applied, either it's going to come from the academic side, where you have researchers who are
5971360	5975920	actually optimizing for generality itself, who said generality as they are going. Or if it's
5975920	5980240	come from the applied side, it's going to come from people who have problems where they have to
5980240	5985760	deal with extreme novelty, uncertainty, and unpredictability. So it's not going to be ads,
5985760	5988800	it's not going to be the military. I don't know where this is going to be.
5988880	5994240	One of the things that interested me about Kenneth Stanley was that he says the reason we
5994240	5999280	can't monotonically optimize on objectives is because of deception, which means sometimes you
5999280	6004160	need to get a lot worse before you get better. His original conception was quality diversity,
6004160	6008960	which basically means if you optimize for novelty, that's something that you can optimize on
6008960	6014480	monotonically. And also, if you look at evolution, where there is a cacophony of problems and
6014480	6021120	solutions divergently being generated, then as an information accumulator, you can optimize
6021120	6026320	on that monotonically. And your conception of intelligence is generality. And that also appears
6026320	6031360	to be a monotonic increase throughout advancing levels of intelligence. So I think that's quite
6031360	6036320	interesting. Anyway, Francois Chollet, this has been my dream come true to have you on the show.
6036320	6040160	Thank you so much. It really means a lot to us. And yeah, I appreciate it. Thank you.
6040240	6044960	Thanks for having me on the podcast. It's really my pleasure. This was super fun.
6044960	6048640	Thanks. And thank you for Keras, by the way. Thanks. I'm glad it's useful.
6048640	6052080	We're going to jump straight into the post-show analysis.
6052080	6056000	Okay, well, I'm going to mention you did really well, Tim, that trickle sweat
6056000	6061200	that this was running down your face the whole time. Not very noticeable. So I think you can
6061200	6067120	relax. That was fun. I think it went pretty well. Yeah, it was a dream come true.
6067680	6073760	I was actually I was very pleasantly kind of interested in how he he framed, you know,
6073760	6077840	the measure of intelligence paper like, look, it's not really about the measure per se. It's just
6077840	6085440	that this is this is a cognitive framework, a cognitive tool for thinking about where to go
6085440	6091840	and a guidepost for building more generalizable or more general intelligences say like that,
6091840	6096800	I totally, totally agree to. And it's quite, you know, quite a fascinating goal, which is like,
6096800	6100320	here's a framework to help us think more in the direction we need to be thinking.
6100880	6108160	Yeah. And it's so surprising that like the arc challenge is at like 20% solved only because
6108880	6115360	you know, he self admits that it's flawed, right? Because he like, he makes the tasks.
6115360	6120960	And, you know, there's only finitely many and and you know, you kind of you see the kind of tasks
6121040	6127600	he makes, you know, in the public set, you would think that not someone will come up with an
6127600	6133520	intelligent thing, but someone will come up with like a smart set of shortcuts to like solve that
6133520	6140320	sucker, right? But it's still at 20%. I don't know whether that's due to just, you know, not too many
6140320	6149200	people investigating it. Or whether it's really actually a hard problem. And if it is a problem,
6149200	6154720	you know, well, it's fascinating too, because if he if he achieves what he wanted, which was
6154720	6158720	getting it more outsourced, right, like getting all the intelligent people all around the world
6158720	6165440	contributing to arc problems and refining them over time, I think actually that community project
6165440	6170720	would help the core knowledge people in that line of research and figuring out, okay, what,
6170720	6175840	what is a catalog of all the core knowledge, right? It's, again, back in school, we used to call these
6175840	6180480	prime thoughts, because we would, we would play these brain teasers all the time. And we realized
6180480	6185840	that there were patterns, right? Like, well, this brain teaser requires the concept of coloring,
6185840	6189600	like with a red black tree, where you add an additional variable that kind of lets you
6190240	6195600	solve the problem. And if we could really have a nice catalog of, here's all the core knowledge,
6195600	6200560	here's all the like problem solving techniques, I think that would be really powerful. I mean,
6200560	6205440	well, we kind of have that. So this woman, Elizabeth Spellke, she came up with about
6205440	6210240	six core knowledge systems, right? And that and the arc challenge uses four of them. So
6210240	6216000	objectness and intuitive physics, one, agentness to elementary geometry, anthropology, three,
6216000	6220800	numbers, counting, quantitative comparisons. So the two that weren't in there are places
6220800	6224640	and social partners. Now, the thing is, I think we may discover new ones.
6225200	6230480	Well, we may be real, but I'm surprised that we did as well as 20%. Because if you think about it,
6230720	6236000	imagine if you just guessed the classification on ImageNet when you've got 1000 classes,
6236000	6241920	20% would be amazing, wouldn't it? And we've got a similar amount of diversity of tasks on arc,
6241920	6247440	right? And what's interesting as well is that all of those different tasks that have been created
6247440	6253760	by Francois, they all tie back to just four priors, right? Which means, I don't know whether
6253760	6260400	it's uniformly distributed. But 20% seems really good for just guessing ops on a DSM.
6260400	6264880	Yeah, there's, there's two things. So first, I would have thought that if someone,
6264880	6269840	if someone came up with something that solves more than 5%, it's going to be like immediately at
6269840	6275600	95%. Like just because they've sort of cracked the problem. And then, you know, there might be
6275600	6280880	a few outliers. But you know, if I would guess that's kind of a task that if you hit the correct
6280880	6286320	solution, it's going to be like, boom, you're, you're there. And that's not, which is surprising.
6286320	6293760	And the other thing is, I, I don't, I don't feel it's surprising that there's so few priors. What I
6293760	6300320	do think is that the space of these priors is still way too large. Like, so if you just think
6300320	6308720	about something like object, because in, in these arc tasks, there are, I feel so many more priors
6308800	6315280	than just the core knowledge things. Because so one of them is like, you have the, you have like
6316240	6320960	this thing, and then you have this thing. And the solution is like, it goes, right? It
6320960	6326160	could go, it like bounces. But this is election. Yeah. But, but like the fact that we recognize
6326160	6332000	like this is a wall or something, but there is no, there's no, no prior to says like a wall
6332000	6338480	needs to be straight, the wall could be like any, you know, any old, any shape at all. And the fact
6338480	6344960	that this is much more core knowledge, right? Like in, you know, we build stuff out of straight
6344960	6349360	walls. And I think, I think I agree with you, which is I think, I think what you're getting at,
6349360	6353360	correct me if I'm wrong, but it's that the way in which the core knowledge is kind of specified
6353360	6359040	right now is vague, right? There's a vagueness to it. And I think if we actually start to try and
6359040	6364720	codify that more and some type of a mathematical language, Tim, I think it's going to expand
6364720	6369520	like the scope of that, we're going to end up with more core knowledge concepts really than,
6369520	6372880	than just six, we'll need to make them finer grained. And I'm really excited,
6373440	6378320	you know, to see that develop because this has been for me a long wonder, right, which is
6378960	6385760	what are the in, in a rigorously defined way? What are these core concepts, these core bits
6385760	6390880	of knowledge that make human cognition so powerful? Yeah. And there's also,
6391440	6396400	because Yannick made the point about brittleness, right, even in topological space, you still have
6396400	6402880	brittleness, but, but the solution was to create powerful abstractions, right? But how would that
6402880	6408480	work with the priors? Because if you think about it, you can recombine many of the priors to come
6408480	6412720	up with powerful abstractions. And you might find that it doesn't actually filter down to, to that
6412720	6417440	many. But the question is, how many things are there? Remember when we spoke to Walid Saber,
6417440	6421760	and he was talking about, he's got them somewhere in a PowerPoint deck, you just wouldn't give them
6421760	6426800	to us. But you know, part of, part of why, why I agree with Yannick that they're finer grained
6426800	6430720	concepts are more important. I think probably stems from a lot of the computer science
6431840	6437040	education that I had where, where when we were devising algorithms to do one thing or another,
6437040	6443280	you get these little hints that kind of like clever bits of core knowledge that was used to
6443280	6447440	solve this problem. Like when you study quicksort, and it's like, you know what, like, I'm just going
6447440	6451840	to randomly choose an element. Well, random selection is kind of a bit of core knowledge.
6451840	6455840	And then I'm just going to partition by that, and then repeat, you know, or things like,
6455840	6460000	I don't know how to balance this tree the way it is. But if I color stuff, like add in red,
6460000	6465040	black nodes, I can now overlay a computation that, you know, so there's all these little bits,
6465040	6469600	you know, that's what's fascinating about computer programming is it, is it really strikes at the
6469600	6474640	heart of this cognition and this core knowledge and how to read, and you have to do it rigorously,
6474640	6479120	right? You can't just vaguely go, Oh, you know, just kind of sort it and merge them. You got to
6479120	6484560	define like what that means. And it's fascinating dynamic programming. I'm always, I'm always a
6484560	6491600	bit amazed by people who have just kind of sort of learned programming, because it's, it's almost
6491600	6497520	like a different world in that they'll, they'll, they'll do, it's like, Oh, okay, I need to solve
6497520	6503680	this problem. Can I can I copy paste this code here? And it works like 20% of the time, but not
6503680	6509120	fully. Yeah. But then on the other side of the coin to that. So when I was working in,
6510800	6515520	you know, quantitative trading, right, we had these these massive globally integrated,
6515520	6521440	automated trading systems. And I mean, some of the bizarre, I don't want to call them hacks,
6521440	6528640	but some of the bizarre sort of piecewise linear equations slash hacks, whatever that actually
6528640	6533280	work in reality. You know, you sit there and you look at them and go, when I first went in there,
6533280	6537200	as fresh out of academia, and I started seeing things like, Oh, this is crap, like, I'm going to
6537200	6541920	figure out some continuous equation that, you know, fits this piecewise linear thing, and it's
6541920	6546960	going to do better. Nope, like it didn't do better. I couldn't find any continuous thing to do better.
6546960	6552240	It's like, you know, options pay off, right, is this this piecewise linear thing. And, and you're
6552240	6556800	like, Oh, that's, well, there should be some continuous like thing in there. Like all these
6556800	6562880	weird, you know, piecewise discrete, like kind of hybrid things between continuous and discrete
6563440	6567520	work. And, and that's weird. It was weird to me and still weird to me.
6568080	6575040	Interesting. But I've got to say, so my main three take homes from Sholay today. I really love Sholay.
6575040	6582480	So one, intelligence is generalization. I think that's super powerful to his idea that deep learning
6582480	6589200	is really good for value centric abstraction. And because of the manifold hypothesis, lots of
6589200	6595200	natural data has some kind of manifold, which you can interpolate on, but lots of discrete
6595200	6600880	problems do not have that. Right. And my mind was thinking, Well, does that mean that we can just
6600880	6605520	use, because it's because of SGD, you can't even learn the manifold, even if it did exist. But
6605520	6609360	he's saying that it doesn't exist for discrete problems. The manifold might be there or it might
6609360	6613840	only be there in parts. So that was interesting. And then the third thing that fascinated me about
6613840	6618880	Sholay is he talks about these systems and bottlenecks in systems. And we shouldn't be
6618880	6624160	thinking about individual brains, we should be thinking about the externalization of knowledge.
6624800	6631760	Yeah. And the way he described this, what he thinks like a hybrid system should look like,
6631760	6639040	which is sort of you have a perception layer and then a discrete search layer. And then on top of
6639040	6646400	that kind of another fuzzy layer that guides the search that can be deep learning again. And I
6646400	6652960	think we're like halfway there on the top with the top very much looks like alpha zero, right,
6652960	6658720	which is kind of a discrete search that is guided by a neural networks. And the bottom
6658720	6664960	layer we have to because that's just our, you know, regular neural networks. I think we have
6664960	6671760	big trouble in how to connect the two in a in a single unified way such that we can learn them,
6671760	6678640	right? Because the best we can do right now is is right, we can, we can plug a pre train network
6679600	6684640	onto alpha zero or something like this, but we can't really, we don't really have it figured
6684640	6689360	out yet how to connect the all the stuff. A good example of that is the neural Turing machines,
6689360	6694720	like how it's so hard to to optimize them, right? And I think not only do we need these kind of
6694720	6700720	three components that that nicely integrate and are optimal, we have to be able to modularize
6700720	6707360	and componentize and connect multiple instances of those things together. And some, you know,
6707360	6712320	weird topological network to really achieve like kind of the capsule network kind of vision
6712320	6717840	where each of the capsules is maybe one of these units. And then they're part of it's like a fractal,
6717840	6722640	you know, kind of these fractal layers of those pieces. I don't know whether I was
6722640	6728880	misunderstanding you before, Janne, but with the alpha zero thing, my conception is that has
6728880	6733840	been quite hard coded. So you're, you're searching through, let's say, a bunch of deep learning
6733840	6738880	models and the way you search is quite opinionated. What you're always talking about is have a very
6738880	6746080	basic DSL and in that topological space, you just search and you start to modularize and you start
6746080	6749920	to create functions and abstractions. And you have from a software engineering point of view,
6749920	6755760	you start to build a library of functions that have been written in code that do certain things,
6755760	6759040	right? And that's that's different, isn't it to alpha zero?
6760000	6767200	Well, the alpha zero is made specifically to search over actions in some kind of RL space.
6768000	6775360	Yeah, I mean, what he describes is certainly much more abstract in that you search over applications
6775360	6784160	of the DSL. And the DSL itself is not is like a perceptive DSL that in itself is described by
6784160	6790080	these lower level neural networks. But I mean, in S, I just, that just came to my mind when he
6790080	6795840	described the system, I'm like, oh, the top part looks very much like, you know, alpha zero, because
6795840	6802480	that's essentially neural network guided search is something we, we already, already do though.
6803280	6810880	Yeah, I, I think, I'm not sure. I think just that the reality is even a bit more fuzzy, because
6811600	6817520	what you do as a, as a human, there's also some part of hierarchical system to it,
6817520	6823920	in that you can, you can do this, but you can do it hierarchically, right? You can, you can be like,
6823920	6830000	okay, I'm gonna, I have to solve, you know, I have this high layer search, and then each of the
6830000	6837760	search things goes through maybe a fuzzy thing, but then you, you again, search to solve the sub
6837760	6844800	problem. And there is also, you can do it at will too, by the way, like you can, you can scan an
6844800	6849600	image, and you get this type one that sort of finds a bunch of objects, and then you do this type two
6849600	6853920	thinking where you start reason about those. And in your mind, you can kind of zoom in on one, let
6853920	6859200	me like zoom in on that tree. And now like, now I've got the bark, you know, pieces of the bark
6859200	6865120	is objects and bugs and reason about so you have this ability to transcend the process and tune it
6865120	6871200	and move it around. Yeah, this self like the, that's the whole consciousness aspect, right? That's
6871200	6877280	even like, apart from intelligence, you have the ability to, to introspect the whole thing.
6878000	6884160	And that probably is a big part of intelligence. I mean, I guess you could have intelligence
6884160	6889360	without consciousness, but you know, there is an argument to be made that the fact that you can
6889360	6895040	introspect your own processes contributes in big part to the furthering of intelligence.
6896400	6902800	Yeah, I would separate consciousness and intelligence, but the thing that hit me the most on
6902800	6907520	his newest presentation was when he said intelligence is literally sensitivity to abstract
6907520	6913200	analogies. So we were talking about the kaleidoscope. The main thing here with intelligence is that
6913200	6919440	there is so much repetition in the universe. Right, but it's repetition in this funny way
6919440	6925920	where it's sort of fuzzy repetition. Like, yeah, sure, the solar system kind of resembles galaxies,
6925920	6931440	kind of resembles, you know, but, but then there are these little weird differences, these asymmetries,
6931440	6936800	and you know, like the universe is a fascinating place. And I don't know, something, yeah.
6936800	6941840	Right, that's not what when you say you have to make analogies, which is I can, I can absolutely
6941840	6946320	see, you know, this and me, I think my question was formulated a bit dumb where I said, you know,
6946320	6953440	if the line is squiggly, what I more meant is that, you know, in that case, it's not a line,
6953440	6958640	it's a squiggly line. And the same with the social situations, you know, that is like, okay,
6958640	6963280	that that person over there kind of doesn't like me. But then in the next social situation, it's
6963280	6969520	kind of a person that doesn't like you and has a gun, or something like this. I almost feel like
6969600	6975360	or a group of people that you don't consider as a single single sure they are similar in some way,
6975360	6980960	but it's never the exact same thing. So this reasoning by analogy does work, but you always
6980960	6986560	do your little modifications on top specific to the situation. And I'm sure there there's a place
6986560	6993280	in his framework for this, but it's it's just, again, it's it's like a lot more complex than
6993280	6998240	yeah. I think that's what he call I think that's abstraction, at least that, you know, that with
6998240	7003280	prior to today, my my concept of abstraction was similar to that, which it's removing the
7003280	7008640	insignificant details. So you're able you're you're able to take whatever, you know, some,
7008640	7013680	you know, object thing situation doesn't matter, and kind of strip away all the stuff that doesn't
7013680	7018800	matter for whatever your purpose is, that's abstraction. And, you know, I think one of
7018800	7023920	the weird things is that, and this is kind of the unreasonable effectiveness of mathematics,
7023920	7031040	right, is that abstracting actually produces things that are useful, you know, that abstraction,
7031040	7036240	I think the fact that abstraction helps with generalization is a very not well understood
7036240	7041920	kind of mystery in a sense, like, why should abstraction help generalize, but it does, like
7041920	7048880	in the real world, that's what happens. Though the yet abstraction in though abstraction has to
7048880	7055120	be somehow specific to what what you want to do, like, like, you're right, an apple is an apple only
7055120	7060880	if, you know, you're looking for food or non food, but when it comes to this fear, if you want to
7060880	7066560	shoot it out of out of a potato can, exactly, but when it comes to, you know, separating fruit by
7066560	7071440	ripeness, then it's not an apple is an apple, then all of a sudden, this apple has much more in
7071440	7077600	common with this orange, right, so that even the way how you abstract, it's not like, it's not like
7077600	7082880	we can just, you know, plug in our ResNet 50, and then boom, we get an embedding vector, and that's
7082880	7089840	our abstraction, but the how you abstract is also incredibly specific to what you want to do.
7090960	7095600	Yeah, and that's what, and I agree with Saba that this is an empirical question, right,
7096400	7100080	you know, like he's kind of like these concepts or whatever, it's an empirical question, and
7100080	7105920	Shelley's, I think the art project, if it ever becomes this crowdsource thing, is going to give
7105920	7110880	us lots of data to start thinking about this empirically, and it's going to be really fascinating.
7110880	7117680	I mean, this needs to be on, like this is a, this is a prime blockchain project, because you can,
7117680	7123840	you can probably, like you can probably even zero, you can zero knowledge prove that you can solve
7124880	7130320	a given set of arc problems, right, you can probably create zero knowledge, so you wouldn't
7130320	7136320	even have to show your solution, and if they're, you know, people would put up arc problems,
7136320	7142400	and they, you know, if you want to try them, you will have to put up some money, and if you can
7142400	7148240	solve it, you know, the creator of the challenge gives you some money or something like this,
7148240	7153920	like this, this is going to be fascinating. Maybe you could do, you know, a homomorphic,
7153920	7158160	like arc, right, or like you don't even, you somehow, like you're saying, you can just prove
7158160	7162480	you can solve the problem without ever you having seen the problem, but just an encryption of it.
7163360	7167920	Yeah. Yeah, no, normally homomorphic encryption comes after blockchain in the same sentence.
7170800	7175920	And we make a nifty, we make a nifty of it. Yeah, what else can we get in there 10 weeks?
7175920	7181520	So we got blockchain, homomorphic encryption, what else? What can we throw in there? Bitcoin,
7181520	7186080	can't we just say people should have to pay through Bitcoin, if they, if somebody wins
7186080	7191760	the challenge on ARC? We'll get our own token. ARC, ARC coin. Oh, God, hold on, I got to get that
7191760	7199760	domain. I want to know, by the way, so the whole point of the arc, diversity of tasks for developer
7199760	7205520	aware generalization, which means the developer could not have conceived of the task. But if all
7205520	7212720	of the tasks are representing four human priors, then how is that developer aware of generalization?
7212800	7215200	Because the developer would be aware of all of those priors.
7216000	7222320	Of the priors, right? But not, not of the task, right? That's the control is the control, like
7222320	7227760	that's what he said, you have to know the start of where your, your white box analyzing from. And
7227760	7233600	the start is not clean slate, but the start here is these four priors. So it's, it's kind of the
7233600	7239840	diff between you give the developer those four priors, what can the developer come up with,
7239840	7245600	just from that, right? Yeah, because I think, I think there's a lot of information leakage there.
7245600	7251520	And you implicitly said the same thing, because you said, once you solve it, you know, once you
7251520	7256480	solve some of them, you've solved all of them. Okay, artcoin.com is available for the, it's,
7256480	7261200	but it's, it's a premium domain. So it's 300 bucks. Should we get it? Because it has coin in it?
7262160	7268880	I guess. We need to figure out something cooler.
7268880	7274240	Like, no art coin. Okay. I don't care enough to grab it.
7275440	7278240	Right. Anyway, we should draw this to a close, ladies and gentlemen. But yeah,
7278240	7283040	thank you very much for listening. Yep. Thank you so much. It's been, it's been emotional.
7283040	7287440	We've recently reached 10k subscribers actually. So yeah, thank you very much.
7287440	7290800	We're still going to continue the show now that we've had Shaleo.
7292320	7295920	Oh, yeah. I thought this was the end. I thought we were going to cap it with show. I mean,
7295920	7303760	to be honest, we might as well just stop now. Anyway, see you folks. Thanks, Bob. Bye.
7303760	7308640	I really hope you've enjoyed the episode today. Remember to like, comment and subscribe.
7308640	7312960	We love reading your comments and we'll see you back next week.
