WEBVTT

00:00.000 --> 00:20.640
Past a certain level of complexity, every system starts looking like a living organism.

00:20.640 --> 00:27.280
In order to build a general intelligence, you need to be optimising for generality itself.

00:30.240 --> 00:37.920
We are surrounded by isomorphisms, just like a kaleidoscope. It creates a remarkable richness

00:37.920 --> 00:45.280
of patterns from a tiny little bit of information. Generalisation is the ability to mine previous

00:45.280 --> 00:52.640
experience to make sense of future novel situations. Generalisation describes a knowledge

00:52.640 --> 01:00.160
differential. It characterises the ratio between known information and the space of possible future

01:00.160 --> 01:07.440
situations. To what extent can we analyse the knowledge that we already have into simulacrums

01:07.440 --> 01:14.880
that apply widely across experienced space? So intelligence, which is to say generalisation

01:14.880 --> 01:22.880
power, is literally sensitivity to abstract analysis, and that's in fact all there is to it.

01:22.880 --> 01:30.080
In today's show we are joined by Francois Chollet. I have been using the Keras Library for many years.

01:30.080 --> 01:36.320
I also read his Deep Learning with Python book, which was inspiring, and I discovered his racy

01:36.320 --> 01:41.520
Twitter feed. When I worked for Microsoft I used to run machine learning seminars and workshops

01:41.520 --> 01:48.080
and hackathons. I used to travel around the world and I always had a copy of Francois's book

01:48.080 --> 01:54.720
Under My Arm. It never left my side. I used to force everyone to read the first four chapters

01:54.720 --> 01:59.680
of that book and of course the chapter on the limitations of deep learning before we did anything.

02:00.320 --> 02:07.760
Francois has a clarity of thought, which is unparalleled I think in any other human being

02:07.760 --> 02:14.320
on the planet. It's really quite incredible. Indeed even our own Dr. Duggar, who normally has

02:15.120 --> 02:20.720
no trouble at all finding holes in some of our guests' work, had this to say while prepping

02:20.720 --> 02:26.320
for the show. I'm working on it. It turned out to be a little bit more difficult than I thought.

02:26.960 --> 02:32.400
Chalet is a little bit too reasonable. Yeah, do you like my Duggar accent? He would enjoy me

02:32.400 --> 02:37.120
doing that. But anyway Chalet is extremely controversial to some people actually, but

02:37.120 --> 02:43.360
he's not controversial to us. Our discussion today lies at the intersection of machine learning

02:43.360 --> 02:49.040
and reasoning. Now Chalet has made his vision completely clear about what he thinks the future

02:49.040 --> 02:54.320
of machine learning is. Make no mistake, what you should take from today's episode is that the

02:54.320 --> 03:00.640
future of artificial intelligence is going to be discrete as well as continuous. Actually the two

03:00.640 --> 03:08.640
are going to be enmeshed. The future of AI will almost certainly involve a large degree of program

03:08.640 --> 03:14.880
synthesis. Deep learning has its limits. You can use deep learning for continuous problems

03:14.880 --> 03:20.800
where the data is interpolative and has a learnable manifold and where you have a dense

03:20.800 --> 03:27.200
sampling across the entire surface of the manifold between which you need to make predictions.

03:27.760 --> 03:35.200
For Chalet, generalization itself is by far the most important feature of intelligence

03:35.200 --> 03:42.240
and of developing strong AI. He describes a spectrum of generalization starting with,

03:42.240 --> 03:49.360
for example, a chess algorithm where there is no novelty to adapt to whatsoever. The task is fixed.

03:50.080 --> 03:57.200
The machine learning we have today confers some adaptation within a known domain of tasks. For

03:57.200 --> 04:04.400
example, being able to recognize dogs or cats within a variety of different poses and lighting

04:04.400 --> 04:12.400
conditions. What's not been robustly demonstrated so far is broad generalization, adaptation to

04:12.400 --> 04:18.960
unknown unknowns within a known but broad domain. It's certainly true that we're knocking on the

04:18.960 --> 04:25.280
door of this now with GPT-3, where the subtask, if you like, is given at test time. Although

04:25.280 --> 04:30.960
Chalet would make the argument that the subtask isn't learned at test time, everything that GPT-3

04:30.960 --> 04:36.320
knows was learned on the vast amounts of training data that we trained it on, the poet algorithm

04:36.320 --> 04:41.840
from Kenneth Stanley et al. That appears to be meta-learning tasks as part of the training

04:41.840 --> 04:46.400
process, which is very, very interesting. It's creating new problems and new solutions as part

04:46.400 --> 04:51.200
of the training process. But broadly speaking in the machine learning space at the moment,

04:51.200 --> 04:57.120
the task that we are doing is fixed and not generalizable. The other thing is that the real

04:57.120 --> 05:03.680
world does not have a static distribution. We need systems that can adapt dynamically.

05:03.680 --> 05:09.360
Intelligence requires that you adapt to novelty without the help of the engineer who helped you

05:09.360 --> 05:14.880
write the system. Chalet has come up with a formalism of intelligence that balances the task

05:14.880 --> 05:21.840
skill, the difficulty, the knowledge, and experience to effectively quantify and normalise

05:21.840 --> 05:28.560
an algorithmic information conversion ratio. It's the ability to convert experience into future skill

05:28.560 --> 05:33.040
that is Chalet's measure of intelligence. At the end of his measure of intelligence paper,

05:33.040 --> 05:38.560
Francois introduced the ARC challenge. It became a Kaggle competition as well and it introduced a

05:38.640 --> 05:44.640
massive diversity of tasks. The reason we have a diversity of tasks is for developer-aware

05:44.640 --> 05:50.240
generalisation. Any model that we have needs to generalise to tasks that the developer was

05:50.240 --> 05:56.240
unaware of. And Chalet thinks that intelligence is specialised. It needs to be human-centric or

05:56.240 --> 06:02.480
anthropocentric. So the kind of priors that you need to solve these intelligence tasks need to

06:02.480 --> 06:07.360
represent the kind of priors that us humans have. Now machine learning algorithms are completely

06:07.360 --> 06:12.480
ineffective against the ARC challenge because it's so challenging to generalise from a few

06:12.480 --> 06:18.640
examples. The only solutions that were effective in the ARC challenge were programme synthesis.

06:18.640 --> 06:23.280
The manifold hypothesis is that natural data forms lower-dimensional manifolds

06:23.280 --> 06:29.920
in its embedding space. There are both theoretical and experimental reasons to believe this is true.

06:29.920 --> 06:34.640
If you believe this, then the task of a classification algorithm is fundamentally

06:34.640 --> 06:40.480
to separate a bunch of tangled manifolds. The only way deep learning models can generalise

06:40.480 --> 06:46.960
is via interpolation. Most perception problems in particular, according to Francois, are

06:46.960 --> 06:53.120
interpolative. Neural networks not only have to represent the manifold of the data that they're

06:53.120 --> 06:58.640
learning, the manifold also needs to be learnable. And that's an even tougher constraint.

06:58.640 --> 07:03.760
Gradient descent will not learn data that has challenging discontinuities in its manifold.

07:03.760 --> 07:10.080
It'll just resort to memorising the data. Deep learning allows you to represent complex programmes

07:10.080 --> 07:15.920
that you couldn't write by hand, but on the other side of the coin it also fails to represent

07:15.920 --> 07:22.000
very simple programmes that you could write by hand. Discrete programmes. So there are some

07:22.000 --> 07:26.400
problems where deep learning is a great fit and there are other problems where deep learning

07:26.400 --> 07:31.760
is a disaster. And the reason for that is that they are not interpolative in nature. These tend

07:31.760 --> 07:37.680
to be algorithmic reasoning problems. Francois thinks that 99% of software written today,

07:37.680 --> 07:43.120
with code, is not interpolative in nature and therefore it's a bad fit for deep learning.

07:43.120 --> 07:48.160
The only answer to these problems is discrete programme search. To use deep learning for these

07:48.160 --> 07:53.840
problems requires a lot of data. It's hard to train and the representation will be glitchy.

07:53.840 --> 07:59.840
It'll be brittle. Neural networks cannot even extrapolate the scalar identity function,

07:59.920 --> 08:07.200
f of x equals x. They can only interpolate given the existence of a smooth manifold in the latent

08:07.200 --> 08:14.000
space. Jan Lacune recently said to Alfredo that all high dimensional machine learning is extrapolation.

08:14.560 --> 08:19.440
So is this similar to interpolation? Well, I mean, all of machine learning is similar to

08:19.440 --> 08:24.880
interpolation if you want, right? When you train a linear regression on scalar values,

08:24.880 --> 08:30.000
you're training a model, right? You're giving a bunch of pairs x and y. You're asking what are the

08:30.000 --> 08:36.960
best values of A and B for y equals A x plus B that minimizes the square error of the prediction

08:36.960 --> 08:41.040
of a line to all of the points, right? That's linear regression. That's interpolation.

08:41.600 --> 08:46.320
All of machine learning is interpolation. In a high dimensional space, there is essentially no such

08:46.320 --> 08:52.240
thing as interpolation. Everything is extrapolation. So imagine you are in a space of images, right?

08:52.240 --> 08:58.240
So you have a core images 256 by 256. So it's 200,000 dimensional input space. Even if you have

08:58.240 --> 09:04.560
a million samples, you're only covering a tiny portion of the dimensions of that space, right?

09:04.560 --> 09:12.480
Those images are in a tiny sliver of surface among the space of all possible combinations

09:12.480 --> 09:18.400
of values of pixels. So when you show the system a new image, it's very unlikely that this image

09:18.400 --> 09:23.360
is a linear combination of previous images. What you're doing is extrapolation, not interpolation,

09:23.360 --> 09:27.840
okay? And in high dimension, all of machine learning is extrapolation, which is why it's hard.

09:27.840 --> 09:33.760
I'm being brave calling out Jan Lacoon, the godfather of deep learning, but hear me out.

09:33.760 --> 09:40.480
It's certainly true that interpolation on the native data domain is useless, right? We need to

09:40.480 --> 09:45.680
pull some useful information out of the data and the model architecture and training method matter

09:45.680 --> 09:52.320
a lot here. We can all agree that interpolation on the learned manifold would seem like extrapolation

09:52.320 --> 09:58.000
in the original space of the data, right? Chalet is quite clear that neural networks only

09:58.880 --> 10:05.040
generalize through interpolation. You might argue that you can go a tiny step outside of the convex

10:05.040 --> 10:11.280
hull of your data, even by a tiny little bit, and you can technically extrapolate. Well, I would argue

10:11.280 --> 10:16.240
that if the manifold doesn't give you any useful information outside of the training range, then

10:16.240 --> 10:20.880
it wouldn't be any better than finding your nearest training example and just adding a bit of random

10:20.880 --> 10:28.320
noise. If you train again, for example, you can interpolate on the latent manifold, but interestingly,

10:28.320 --> 10:34.880
you can extrapolate. But the reason for that is the natural manifold that the data of faces sits on

10:34.880 --> 10:40.800
might be shaped like a football or a sphere, which means if you go outside of the training range,

10:40.800 --> 10:46.720
you actually have some information about those data points. The scalar identity function might seem

10:46.720 --> 10:51.280
like a contrived example, but it's a really interesting one. When you go outside of the

10:51.280 --> 10:57.760
training range, nothing about the manifold is known, right? Think about the manifold. It's just a

10:57.760 --> 11:03.920
string that goes on forever. We don't know anything about that manifold outside of the training range.

11:03.920 --> 11:08.800
This is not true for most perceptual problems in deep learning. And this is why image models,

11:08.880 --> 11:14.000
for example, suffer greatly drawing straight lines. What are your thoughts about this? Why don't you

11:14.000 --> 11:19.120
let us know in the comments section on YouTube? So there's a real interesting dichotomy of continuous

11:19.120 --> 11:24.080
problems versus discrete problems that we're going to be exploring in the show today. It's very

11:24.080 --> 11:29.200
interesting that brittleness works both ways, depending on the discreteness of the problem.

11:29.200 --> 11:36.320
Program synthesis would be extremely brittle in classifying cats versus dogs or even M-nist,

11:36.320 --> 11:42.560
and deep learning would be extremely brittle predicting the digits of pi or prime numbers

11:42.560 --> 11:48.480
or sorting a list. So brittleness here means the overall fit of your model or your program,

11:48.480 --> 11:55.200
so accuracy and robustness. Imagine if every single bug you experienced with computer software

11:55.200 --> 12:00.720
was entirely unique to you and the development team wouldn't even be able to reproduce it.

12:00.720 --> 12:04.880
This is what would happen if software was written entirely with neural networks.

12:04.880 --> 12:10.560
It would be more, not less brittle. Sholey thinks that motivated thinking is the primary

12:10.560 --> 12:15.840
obstacle to getting people to wake up to the fact that neural networks are poorly suited

12:15.840 --> 12:21.600
to discrete problems. The people who are good enough at deep learning to realize its limitations

12:21.600 --> 12:27.600
are too invested in its success to say so. Sholey fundamentally thinks that there are two types of

12:27.600 --> 12:34.880
thinking, type one and type two. He thinks that every single thought in our minds is not simply

12:34.880 --> 12:42.240
one or the other, rather it's a combination of both types. Type one and type two, they are

12:42.240 --> 12:50.560
enmeshed together in everything you think and in everything you do. Even our reasoning is guided

12:50.560 --> 12:58.560
by intuition, which is interpolative in nature. Sholey thinks that abstraction is key to generalization

12:58.560 --> 13:05.920
and the way we perform abstraction is different in continuous versus discrete space. We need to

13:05.920 --> 13:11.680
find analogies and those analogies will be found differently in both of those different spaces.

13:11.680 --> 13:17.840
Program search allows us to generalize broadly from just a few examples. It marks a significant

13:17.920 --> 13:22.640
deviation from traditional machine learning. Rather than trying to interpolate between the

13:22.640 --> 13:28.720
examples you have, you're constructing an entire search space from scratch and testing

13:28.720 --> 13:34.640
if it fits our training data. It all started with the flash fill feature in Microsoft Excel.

13:34.640 --> 13:39.600
Do you remember that? You give a few examples of some transformation that you want to perform

13:39.600 --> 13:44.400
and it will generate a piece of programming code for you, which means it can generalize

13:44.400 --> 13:50.000
that transformation across an entire spreadsheet. It's quite a revolutionary idea. It's been around

13:50.000 --> 13:55.680
for about 20 years actually, but what's really making it work now is the idea of using neural

13:55.680 --> 14:03.040
networks or a neural engine to guide the discrete program search. We spoke about GPT-3. He thinks

14:03.040 --> 14:09.360
that GPT-3 hasn't expanded his knowledge of the world. He says that GPT-3 is not learning any new

14:09.360 --> 14:15.520
algorithms on the fly. It's already learned continuous and often glitchy representations

14:15.520 --> 14:20.800
of existing tasks during its training. It's completely ineffective against his arc challenge

14:20.800 --> 14:29.360
tasks. People often claim that neural networks are turing complete. No, they're not. A model has

14:29.360 --> 14:35.520
a bounded number of nodes and a bounded runtime. It cannot execute algorithms that require unbounded

14:35.520 --> 14:42.400
space or unbounded time. For example, could you train a neural network to predict the nth digit of

14:42.400 --> 14:48.000
pi? No, you couldn't. You could write a computer program to do it, but you couldn't train a neural

14:48.000 --> 14:53.200
network to do it. A simple turing machine program can do just that and that is because a turing machine

14:53.200 --> 14:58.960
can access unbounded memory and time. The best thing that neural networks can do is approximate

14:58.960 --> 15:05.200
unbounded algorithms, but doing so will introduce glitches. For example, one can train a neural

15:05.200 --> 15:10.640
network to approximately multiply integers together. Yet, even when learning to multiply

15:10.640 --> 15:15.920
fixed-width integers, practically-sized neural networks introduce errors occasionally,

15:15.920 --> 15:21.760
and for a fixed-sized neural network, these errors grow more common as the size of the input grows.

15:21.760 --> 15:27.280
That said, neural networks are finite state machines, and just as finite state machines

15:27.280 --> 15:32.640
can be augmented with unbounded memory and iteration to yield a turing machine, neural

15:32.640 --> 15:38.400
networks can also be automated in the same way to produce a turing-complete computational model.

15:38.400 --> 15:43.280
If you want to see a concrete example of the kind of discrete program search that

15:43.280 --> 15:49.360
Chalet is talking about, look no further than the recent DreamCoder paper. Yannick just made a video

15:49.360 --> 15:54.480
about it. So yeah, it feels like today is the culmination of a year of really hard work and

15:54.480 --> 15:59.840
passion from the MLST team. We've worked with so many fascinating people. We've had so many

15:59.840 --> 16:05.600
amazing guests on. It really means a lot to us. Today is a very, very special episode. It was

16:05.600 --> 16:10.560
my dream from the beginning to get Chalet on the show. I know that Chalet is going to say

16:10.560 --> 16:15.840
lots of interesting things that will trigger some people and inspire others, and please take to the

16:15.840 --> 16:22.480
comment section and tell us exactly what you think. Anyway, enjoy the show. See you next week. Peace out.

16:22.560 --> 16:26.320
Welcome back to the Machine Learning Street Talk YouTube channel and podcast

16:26.320 --> 16:32.960
with my two compadres, MIT, PhD, Dr. Keith Duggar and Yannick Lightspeed Kiltcher.

16:33.600 --> 16:39.920
Now today we have a very special guest, Francois Chalet. Francois is one of the few leaders in

16:39.920 --> 16:44.800
the machine learning space who's caused a massive stir in my thinking, the only other notable one

16:44.800 --> 16:48.880
actually being Kenneth Stanley, who we had on recently. My ultimate goal with Street Talk was

16:48.880 --> 16:53.680
always to get Francois on the show, and I can't believe that it's actually happened. We actually

16:53.680 --> 16:57.920
have a rule, by the way, that I'm only allowed to invoke Francois's name about once per show,

16:57.920 --> 17:04.160
but that rule will not apply today. Yannick and I have made more content on Francois Chalet actually

17:04.160 --> 17:08.160
than anyone else by a wide margin, and it's because his work is very thought-provoking

17:08.160 --> 17:13.360
and disruptive. I spent many weeks actually studying his measure of intelligence paper last year,

17:13.360 --> 17:18.000
and of course his recent New York's workshop was fascinating as well. Almost every single word in

17:18.000 --> 17:22.400
my opinion that comes out of Francois's mouth deserves rigorous study, and I seriously mean that.

17:23.440 --> 17:28.960
Francois thinks that intelligence is embodied, it's a process, and it's not just a brain. He's

17:28.960 --> 17:33.680
skeptical of the so-called intelligence explosion, and he thinks there's no such thing as general

17:33.680 --> 17:40.080
intelligence. All intelligence is specialized. Critically, he thinks that generalization,

17:40.080 --> 17:46.960
the ability to deal with novelty and uncertainty is the most important concept in intelligence.

17:46.960 --> 17:52.800
He thinks that task-specific skills tells you nothing about intelligence. He thinks that deep

17:52.800 --> 17:57.920
learning only works for problems where the manifold hypothesis applies. For example,

17:57.920 --> 18:03.520
problems which are interpolative in nature and when a sufficiently dense sampling of your

18:03.520 --> 18:11.040
distribution is obtained. Otherwise, deep learning cannot generalize. Deep learning can only memorize,

18:11.040 --> 18:16.400
but it cannot always generalize. And in his recent New York's presentation, he introduced the concept

18:16.400 --> 18:21.120
of program-centric and value-centric generalization, which we'll get into in the show today.

18:21.120 --> 18:26.800
But I wanted to move straight on to this concept of deep learning being a hash table,

18:26.800 --> 18:33.200
because this is what Francois thinks. He says that a deep learning model is a high-dimensional

18:33.200 --> 18:39.280
curve with some constraints on its structure given by inductive priors, and that curve has

18:39.280 --> 18:44.000
enough parameters that it could fit almost anything. If you train your model for long enough,

18:44.000 --> 18:50.080
it'll simply memorize your data. And because of SGD, your manifold fit is found progressively,

18:50.080 --> 18:54.560
and at some point, the manifold will approximate the natural manifold between underfitting and

18:54.560 --> 19:00.640
overfitting. And at this point, you'll be able to make sense of novel inputs by interpolating

19:00.640 --> 19:05.680
on that manifold. So the power of the model to generalize is actually a consequence of the

19:05.680 --> 19:11.680
structure of the data and the gradual process of SGD, according to Francois, rather than any property

19:11.680 --> 19:17.600
of the model itself. Last week, Francois, we were talking to Christian Sergeidi, and he takes a

19:17.600 --> 19:21.680
rather different view, because one school of thought is that deep learning models are kind of like

19:22.400 --> 19:28.000
searching for a space of possible programs, and advocates of GPT-3 make this argument quite

19:28.000 --> 19:32.560
strongly. And presumably, Christian Sergeidi, he wouldn't be doing what he's doing, which is

19:33.680 --> 19:38.960
interpolating between mathematical conjectures, assuming that interpolation space would actually

19:38.960 --> 19:44.480
give us new information about mathematics, if he thought that that space wasn't interpolatable.

19:44.480 --> 19:51.440
What do you think Francois? Right, I think you've already summarized it, really. Yeah, so interpolation

19:51.440 --> 19:57.600
is the origin of generalization in deep learning models, and that's very much by construction,

19:57.600 --> 20:03.360
by nature, right? Like a deep learning model is a very large, differentiable,

20:03.360 --> 20:09.120
parametric model, trained with gradient descent. And so the only way it's ever going to be

20:09.120 --> 20:15.040
generalizing is your interpolation. This is literally, this is what it is, this is what it does.

20:15.040 --> 20:20.240
So I think the question, you know, are all deep learning models, interpolators or not,

20:20.240 --> 20:24.160
is not a super interesting question, because it's not an open question. We know they are.

20:24.160 --> 20:28.880
But the more interesting question, I think, is what can you actually achieve with the

20:28.880 --> 20:33.680
stored of interpolation on this very complex, very high-dimensional manifold,

20:33.680 --> 20:38.000
that they're deep learning models and implementing. We're telling you the properties of this generalization,

20:39.120 --> 20:42.720
the tasks for which it will perform well, the tasks for which it will not perform well.

20:42.720 --> 20:48.720
I guess one example I could give you is encoding data with the Fourier transform,

20:48.720 --> 20:53.120
like you know about the Fourier transform. And maybe, you know, some people will play around

20:53.120 --> 20:57.840
with it and they will be like, hey, you know, actually the Fourier transform can draw much

20:57.840 --> 21:02.160
more than curves. Look, I made a square with it, right? And then you would have to point out that,

21:02.160 --> 21:06.640
no, actually the square, you've made it by supposing lots of tiny curves. And it's not,

21:06.640 --> 21:10.880
in fact, a perfect square, right? Because it is made of this, with the other supposition of lots

21:10.880 --> 21:16.320
of tiny curves. And that's really, this is true by nature, by construction. This is where the

21:16.320 --> 21:21.600
Fourier transform starts, right? And the more interesting question is, you know, what sort of

21:21.600 --> 21:26.880
data is a good fit for encoding the Fourier transform? And what sort of data is not a good

21:26.880 --> 21:31.760
fit? Like if you try to encode the t-square fractal with the Fourier transform, you're going to have

21:31.760 --> 21:37.360
a bad time. And if you try to encode the drawing, that's mostly just, you know, nice, smooth curves,

21:37.360 --> 21:42.000
then it's going to be a very, very efficient encoding at a good idea. And deep learning is

21:42.000 --> 21:46.160
very much like that. We should ask, you know, what are its strong points, what are its weak points?

21:46.160 --> 21:50.880
Yeah, so I, by the way, so I don't believe that deep learning models are hash tables, plus there,

21:50.880 --> 21:55.840
I usually say there, localities are sensitive hash tables, meaning that kind of like a hash table

21:55.840 --> 22:00.960
with some amount of generalization power, because they have some notion of distance

22:00.960 --> 22:06.160
between parts. They're capable of comparing points by measuring the distance between them, right?

22:06.160 --> 22:11.120
And this, this is what would enable this kind of hash table to actually generalize, as opposed to

22:11.120 --> 22:14.560
the classic kind of hash table, which is just memorizing the data.

22:14.560 --> 22:19.760
It's very interesting that you allude to the fact that, you know, what kind of data is the model

22:19.760 --> 22:26.720
good for, and so on. And now, deep learning models being essentially like really, as Tim said,

22:26.720 --> 22:31.040
like big interpolators of arbitrary manifolds, do you think there is something

22:31.680 --> 22:38.480
common across the types of data we choose deep learning for? Or, you know, could we in fact

22:38.480 --> 22:45.280
use deep learning for most kinds of manifold dish data? Or do you think there is some kind of

22:45.360 --> 22:50.720
specialness about natural signals that makes deep learning very attuned to them?

22:51.440 --> 22:57.040
So I think most things are to some extent interpolative, which is why you can actually do

22:57.040 --> 23:00.960
lots of things with deep learning models. Doesn't necessarily mean it's always a good idea,

23:00.960 --> 23:05.200
but it's going to kind of work, right? You know, when people hear the word interpolation,

23:05.200 --> 23:10.720
they tend to think about linear interpolation, that's what pops up in their mind. That's not

23:10.880 --> 23:15.440
always what deep learning models are doing, right? They're interpolating on this very complex,

23:15.440 --> 23:21.680
very high dimensional manifold. And this enables very, you know, arbitrarily complex behavior.

23:22.400 --> 23:28.960
And in practice, it's always possible to an arbitrary discrete algorithm in a continuous

23:28.960 --> 23:34.080
manifold, right? It's not necessarily a good idea, but it's always possible, at least in theory.

23:34.080 --> 23:39.680
So for any program, you can imagine, you can ask, you know, is there a deep learning model that

23:39.680 --> 23:44.480
will encode some kind of approximation of it? And the answer is always yes, right?

23:45.120 --> 23:51.040
Similar to how you can always encode an arbitrary shape with the Fourier transform, right? But

23:51.040 --> 23:55.120
there are, if you try to do that, actually, there are some issues with that. There are very much,

23:55.120 --> 23:59.520
you know, some problems for which deep learning is good fit, some problems for which deep learning

23:59.520 --> 24:06.400
is not a good fit. In the limit, the extreme point is a space that is not interpolative at all,

24:06.400 --> 24:10.400
which is quite right, actually. You know, most spaces, even very discrete kind of spaces,

24:10.400 --> 24:16.960
do have, you know, some amounts of interpolativeness. So like, but one example would be, for instance,

24:16.960 --> 24:24.400
trying to train a deep learning model to predict the next prime number, right? Or to tell whether

24:24.400 --> 24:30.480
a number is a prime number. As you cannot actually do that, the best you can do is memorize the

24:30.480 --> 24:36.800
train data point, because the space of prime numbers is not interpreted at all. So your deep

24:36.800 --> 24:43.200
learning model will always have zero generalization power. But that's actually quite rare. This is

24:43.200 --> 24:49.680
kind of an extreme case. Most problems, even problems that are binary, discrete, algorithmic

24:49.680 --> 24:54.240
problems, there will be some amount of interpolation that you can do, right? But that doesn't necessarily

24:54.240 --> 24:59.120
mean that it's a good idea to try to solve, you know, such problems with deep learning models

24:59.120 --> 25:04.000
for deep learning to be a good idea. You need a very, you need very much the manifold

25:04.000 --> 25:09.360
level as it is to apply. So it works best for perception problems. Any problem that humans

25:09.360 --> 25:14.320
can solve via pure intuition or perception is probably a good fit for deep learning. But any

25:14.320 --> 25:19.600
problem where you need, you know, high level explicit step by step reasoning is probably

25:19.600 --> 25:24.240
a bad fit for deep learning. And, you know, 99% of what today software engineers solve,

25:24.960 --> 25:30.000
the writing code is going to be a bad fit for deep learning. That doesn't mean that there

25:30.000 --> 25:35.440
wouldn't be, you know, theoretically, a deep learning model that can embed the same algorithm

25:35.440 --> 25:41.040
in a smooth manifold. This is always possible to some extent, right? But there are very significant

25:41.040 --> 25:46.080
issues with attempting to do this. I like just because something is theoretically possible doesn't

25:46.080 --> 25:52.240
mean you should actually do it. I think we might be not being careful enough when we say what we

25:52.240 --> 25:59.200
mean by program. Because, for example, if I take program to be the universal sense like a program

25:59.200 --> 26:04.240
is something that can run on a Turing machine, for example, because of the fact that that type

26:04.240 --> 26:11.200
of program actually has access to unbounded time and memory computation. It's impossible in the

26:11.200 --> 26:17.280
general sense to encode that in any finite neural network, like I can write a very short piece of

26:17.280 --> 26:22.960
code theoretical Turing machine can output, you know, the nth digit of Pi. It's impossible to do

26:22.960 --> 26:27.600
that with any finite neural network. Would you agree? Yeah, absolutely. Absolutely. Okay, because I

26:27.600 --> 26:31.360
think that's like a big source of confusion often time with these statements that like, you know,

26:31.920 --> 26:36.480
oh, neural networks are Turing complete. Well, no, they're not. You know, if you have a neural

26:36.480 --> 26:42.480
Turing machine, which is a neural network that's the finite state machine piece of a Turing machine,

26:43.040 --> 26:47.600
that can be Turing complete. But in the general case, you know, finite neural networks, which is

26:47.600 --> 26:52.320
what everyone means by neural networks, are not Turing complete. And it actually has practical

26:52.320 --> 26:57.840
effects, right? This is why we see this sort of explosion and the number of parameters to kind of,

26:57.840 --> 27:03.520
you know, start to accomplish. Yeah, absolutely. 100%. You're entirely right. So we're only

27:03.520 --> 27:07.360
interested in realistic programs, like the sort of programs that start to engineer with right,

27:07.360 --> 27:11.360
for instance. And we're only interested in realistic neural networks. And by the way,

27:11.440 --> 27:15.360
the constraints that we have on neural networks are actually much stronger than asking,

27:15.360 --> 27:21.280
given this program that I have, is there a neural network that could embed it in a continuous

27:21.280 --> 27:26.240
manifold? The constraint is actually, is there a neural network that could not only represent it,

27:26.240 --> 27:32.480
but that could learn this embedding of the program from there. And this is a several orders of

27:32.480 --> 27:39.360
magnitude harder, right? Learnability is a big problem because you're fitting your manifold via

27:39.360 --> 27:47.360
gradient descent, right? And if the structure you're trying to fit is too discrete, with too

27:47.360 --> 27:53.840
big discontinuities, gradient descent will not work at all. And the best you can do is, again,

27:53.840 --> 28:00.960
just memorize the train data. So I can maybe give you a concrete example to kind of ground

28:00.960 --> 28:08.480
our discussion here. So in 2015, some friend of mine, so his name is, he used Keras to do

28:08.480 --> 28:14.160
something pretty cool, which actually became a cool example on the Keras website. He used a LSTM

28:14.160 --> 28:22.160
model to multiply numbers, but not like numbers multiplied by value, but the input of the model

28:22.160 --> 28:27.360
would be strings, like two strings, strings of digits. And the LSTM will actually learn the

28:27.360 --> 28:32.240
multiplication algorithm for like multiplying three digits and three digits numbers, kind of the

28:32.240 --> 28:38.000
sort of algorithm we would learn in primary school, right, to do multiplication. And remarkably,

28:38.080 --> 28:42.800
that worked, right? It works just fine. So you can train a deep learning model to learn this

28:42.800 --> 28:46.880
algorithm. And you could, of course, train a transformer model to do the same. It will actually

28:46.880 --> 28:52.880
be probably significantly more efficient. So that works. That comes with a number of downsides.

28:52.880 --> 28:56.880
So first, in order to train that algorithm, which is very simple, you're going to need

28:56.880 --> 29:03.280
thousands and thousands of examples of different strategic numbers. And once you've trained your

29:03.280 --> 29:10.400
algorithm, because the actual algorithm was embedded in the neural network, it does generalize to

29:10.400 --> 29:15.600
never see before digits, right? So it's actually learning the algorithm. It's not just learning,

29:15.600 --> 29:21.520
I'm just not memorizing the data. But the thing is, because the embedding of an algorithm,

29:21.520 --> 29:25.840
the embedding of a discrete structure in the continuous space, is not the same thing as the

29:25.840 --> 29:31.120
original discrete object. There are glitches in your deep learning network, unless that's something

29:31.120 --> 29:35.520
you could have found via program synthesis, for instance, it's not going to be correct 100% of

29:35.520 --> 29:41.520
the time, it's going to be correct 95% of the time. In much the same way that if you try to

29:41.520 --> 29:46.560
encode a very discrete object via the Fourier transform, it's not going to be correct. 100%

29:46.560 --> 29:50.720
of the time is going to be an approximation and around sharp angles, it's actually going to be

29:50.720 --> 29:56.000
wrong. And very importantly, and this is really like the algorithm that you've painstakingly

29:56.080 --> 30:02.800
embedded into your deep learning model via exposure to data, does only, it does not generalize very

30:02.800 --> 30:08.000
well, it only does local generalization, meaning that if you train it with three, to multiply

30:08.000 --> 30:12.480
three digit numbers, and then you send it a five digit number, is it going to work? No,

30:12.480 --> 30:16.400
absolutely not. And not only is it not going to work, but you could not in fact,

30:18.240 --> 30:23.760
few shots fine tune your algorithm to learn to handle five digits, seven digits and so on.

30:23.760 --> 30:27.760
If you want to fine tune your algorithm, you're going to need thousands, maybe millions

30:28.640 --> 30:35.200
of examples, right? So it's all local generalization. And lastly, it's super inefficient,

30:35.200 --> 30:40.080
like I think we can all agree with this, that multiplication is not like it's not

30:41.440 --> 30:46.400
a clever use of an LSTM, it's you're burning tons of resources for something that's actually

30:46.400 --> 30:51.920
super easy. And you can compare that, like since we are talking about pros and cons of deep learning,

30:51.920 --> 30:56.480
you can compare that to what you could get with a program synthesis engine. Like I don't want to

30:56.480 --> 31:00.480
compare to what you could get with a human written algorithm, because kind of the point of deep

31:00.480 --> 31:05.920
learning is that it enables you to develop programs that you could not otherwise write by hand.

31:05.920 --> 31:10.400
So the right point of comparison is actually what you could do with deep learning versus what you

31:10.400 --> 31:17.280
could do with discrete program synthesis based on discrete search and the DSL. And if you were to

31:17.280 --> 31:23.760
use a program synthesis to solve the multiplication problem, so you would find a solution, even a

31:23.760 --> 31:30.240
very neat engine that does just like maybe a plus operation, maybe a loop. And this DSL is going to

31:30.240 --> 31:35.120
find it, it can find it with a handful of examples, you're not going to need thousands of examples,

31:35.120 --> 31:39.280
like in the deep learning case, you're going to need maybe five. And the program you get out of it

31:39.280 --> 31:45.040
is going to be exact, because it is the exact discrete algorithm, it is not a continuous

31:45.040 --> 31:49.760
embedding of it. So it does not have glitches, it outputs the correct answer. It will be lightweight,

31:49.760 --> 31:53.680
so it will be very efficient, you know, and like the LCM or transformer model,

31:53.680 --> 31:59.280
and crucially, it's going to generalize. So if you develop it only from three digit numbers,

31:59.280 --> 32:03.040
maybe there will be something inside it that will hardcore the assumption that they're dealing with

32:03.040 --> 32:08.240
three digit numbers. But even if that's the case, you can take it and automatically learn

32:09.200 --> 32:13.840
a generalized form of it if you just start giving it seven digit numbers. Very easy because it's

32:13.840 --> 32:18.880
just modifying probably a couple lines of code. So it is capable of strong generalization. So here

32:18.880 --> 32:23.840
you start seeing how for a problem that's fundamentally a discrete algorithmic reasoning

32:23.840 --> 32:30.160
problem, discrete search is the correct answer. Deep learning, it's possible, it works, but with

32:30.160 --> 32:36.320
extremely stark limitations, right? It's very hard to train it, you need tons of data. The resulting

32:36.320 --> 32:40.640
embedding, because it's not, it's not discrete, we'll have glitches. It's not going to work on

32:40.640 --> 32:44.640
a person at a time, it's going to be pretty long. It's only going to be capable of hardcore

32:44.640 --> 32:53.520
generalization, right? Because again, there is a huge difference in representational flexibility

32:53.520 --> 32:59.040
between your very simple, discrete algorithm and some kind of very complex, high dimensional

32:59.040 --> 33:04.400
continuous embedding. And then there's also the efficiency consideration. So clearly for

33:04.400 --> 33:09.600
if you're dealing, and the reverse is also true, right? Like if you're dealing with a problem that's

33:10.160 --> 33:16.800
perception problem, where you have data points that fit on a nice and smooth manifold, then

33:16.800 --> 33:24.640
deep learning is actually the right answer. And if you tried to train a discrete program to develop

33:24.640 --> 33:29.360
your program synthesis, an actual algorithm to classify MNIST digits, for instance.

33:30.800 --> 33:36.240
Everything I just said would be true, but in reverse, your program would be brittle. The

33:36.240 --> 33:40.960
deep learning model would be robust, and so on. So there are really problems where deep learning

33:40.960 --> 33:45.120
is a pretty idea. It's a great fit. Problems where it's a terrible idea. Like try sorting a list

33:45.120 --> 33:50.640
with deep learning model. Can it be done? Yes, actually it can. But with all these caveats applying.

33:51.200 --> 33:56.720
It is possible to sort a list of deep learning with some hacky inductive priors and probably

33:56.720 --> 34:01.440
memorizing most of the training data. And it's not a binary, is it? You said yourself, there's lots

34:01.440 --> 34:06.880
of problems that fall in the middle, where there is a semi continuous structure and some

34:06.880 --> 34:12.000
regularity, but it's still a discrete problem. And you're saying in that situation, we should

34:12.000 --> 34:16.400
still use program search, but maybe we can use deep learning, maybe something about the shape

34:16.400 --> 34:20.720
of the manifold, even though it's semi continuous, could actually tell us about how to do that

34:20.720 --> 34:26.320
program search more efficiently. But it seems to me that if there are problems out there,

34:26.320 --> 34:32.320
let's say adding numbers up in GBT3, when I read the stuff that you've been talking about here,

34:32.320 --> 34:38.880
it seems obvious to me. Why are people not picking up on this? I think most people are not necessarily

34:38.880 --> 34:44.000
paying a lot of attention to the nature of deep learning, why it works, why it doesn't work.

34:44.000 --> 34:49.520
I also think the people, they are basically two categories of people. They are like laypeople,

34:49.520 --> 34:55.200
and they are people with deep expertise. And the big problem we have here is that the people with

34:55.200 --> 35:01.760
a lot of expertise are going to be a lot of the time driven by motivated thinking. Because

35:03.680 --> 35:07.360
like I do, they work in the field of deep learning, and so they're going to have this vested

35:07.360 --> 35:12.960
interest in deep learning being potentially more powerful, more general at the nature is. I think

35:12.960 --> 35:19.440
if you want to think clearly, the primary obstacle is motivated thinking. It's fighting

35:19.520 --> 35:26.160
against what you want to be true. So I tend to have super boring opinions in that sense,

35:26.160 --> 35:31.440
because I do my best to try to forget kind of what I would like the world to be in my

35:31.440 --> 35:36.240
best interest and try to look at it as it really is. And that will tend to actually diminish

35:36.240 --> 35:41.840
the importance of my own work. So yeah, but you know, I've been doing deep learning for

35:41.840 --> 35:47.360
almost a decade. Of course, I would want it to be like this incredible world changing thing that

35:47.360 --> 35:51.040
leads to human level intelligence, right off the bat, that would be that would be awesome,

35:51.040 --> 35:56.240
that would be amazing, and that would be right in the middle of it. But that's not that's not

35:56.240 --> 36:02.960
actually what's going on. You said you tend to be what was the word, not not controversial ideas

36:02.960 --> 36:07.600
or something because you try to stick to the way the world is rather than the way you want the

36:07.600 --> 36:13.200
world to be. But we just had Yannick produce an interesting video about how if you think that

36:13.280 --> 36:17.360
machine learning models essentially attempt to do the same thing, right? I mean, they're not human

36:17.360 --> 36:22.160
beings, they don't really have wants per se, they're just modeling reality as it is. It turns out

36:22.160 --> 36:28.640
reality itself really annoys a lot of people, like they just don't like reality, and they don't like

36:28.640 --> 36:33.280
the way the world is, and they wish it was something different. And that infects like every mode of

36:33.280 --> 36:37.840
their thinking, actually. Yeah, no, absolutely. Most people, you know, and that's that's true for

36:37.840 --> 36:45.600
me as well. I'm not saying I'm an exception, I'm trying to do my best to resist this trend.

36:45.600 --> 36:51.280
But I have no exception. Most people have opinions not because they've seen evidence in

36:51.280 --> 36:56.400
support of their opinion, but because it's in their interest for this opinion to be true,

36:56.400 --> 37:01.360
or they just want it to be true. I guess one example is, you know, we were mentioning GPT-3

37:01.360 --> 37:07.120
and so on and proponents of GPT-3. I was actually super excited when I initially saw the claim

37:07.120 --> 37:11.680
that the pre-trained language model could perform few short generalizations. I thought that's

37:11.680 --> 37:17.600
super fascinating. I'm always super excited if I hear about something that's really challenging

37:18.320 --> 37:25.120
my initial kind of mental model of how the world works, you know, it's like a few years back,

37:25.120 --> 37:29.920
and there was this claim that a neutrino was measured going faster than speed of flight.

37:29.920 --> 37:34.960
I mean, that's exciting, right? That's like new physics, you want it to be true, at least you

37:34.960 --> 37:39.280
want to get to the bottom of it. And then it turned out to be a measurement error, right? So that's

37:39.280 --> 37:44.240
disappointing. So I think GPT-3 is kind of the same for me. I really wanted it to be something,

37:44.240 --> 37:48.400
something novel, and that would really challenge what they thought to be true by deep learning

37:48.400 --> 37:54.960
models. And I regret to say that everything I've seen close has actually confirmed. In my view,

37:54.960 --> 38:00.720
that basically deep learning models, they can learn to embed algorithms given sufficient exposure

38:00.720 --> 38:08.080
to data, but they cannot really, like, few short synthesize novel algorithms that represent a pattern

38:08.080 --> 38:13.120
they haven't seen in a train yet, which is why, by the way, GPT-3 is entirely ineffective on ARC,

38:13.120 --> 38:20.960
for instance. And that's kind of sad to me. I kind of regret it, because it means I haven't actually

38:20.960 --> 38:28.240
learned anything from it. It hasn't expanded my view of the world, which is too bad. Like,

38:28.240 --> 38:34.560
I wish it did. I wish it did. So in the case of GPT-3, what's really going on is that the model

38:34.560 --> 38:40.560
is exposed to many patterns. You could call them algorithms, for instance, in many different contexts.

38:40.560 --> 38:45.440
And so it has memorized these patterns. And now it's able to take these patterns and apply them

38:45.440 --> 38:49.680
to new data in much the same way that the multiplication algorithm we are talking about.

38:49.680 --> 38:54.800
Because it's an actual algorithm, it can process new digits. It's not just memorizing the digits

38:54.800 --> 39:00.560
in the train. It's an actual algorithm. In the same way, GPT-3 contains tons of small algorithms

39:00.560 --> 39:06.560
like that. But the model is not synthesizing these algorithms on the fly. They are in the model already.

39:07.280 --> 39:12.880
And if you try to apply GPT-3 to something for which a new algorithm would need to be produced,

39:12.880 --> 39:16.000
like an ARC task, for instance, it has just completed anything.

39:16.000 --> 39:21.280
It seems to all build up what you're saying, because there is this strong generalization

39:21.280 --> 39:27.760
versus local generalization. And then you make a case that in order to do strong generalization,

39:27.760 --> 39:33.360
we need maybe something like program synthesis approach. So deep learning can't necessarily

39:33.360 --> 39:39.680
get us there in most problems. And you make an interesting case that something like graph

39:39.680 --> 39:47.440
isomorphism search could play a core role in that. Could you briefly connect all of these

39:47.520 --> 39:51.680
terms together of the case you're making there? Because it's super interesting.

39:51.680 --> 40:00.240
So going back to it, Tim was saying it's rarely the case that you have problems that are fully

40:00.240 --> 40:04.560
interpretive or fully discrete. There are definitely such problems. In fact, most perception

40:04.560 --> 40:10.160
problems are almost entirely interpretive. And most programs, the kind of programs that you

40:10.720 --> 40:16.400
write there, they're largely discrete, not interpretive. But most tasks actually are best

40:16.400 --> 40:22.960
solved via a combination of both. And I actually believe that's true for the way humans think.

40:22.960 --> 40:28.560
You know, there's type 1 thinking and type 2 thinking. I strongly believe that almost every

40:28.560 --> 40:36.560
thought you have and everything you do with your mind is not one or the other. It's a combination

40:36.800 --> 40:42.240
of both. That type 1 and type 2 are really unmatched into each other in everything you

40:42.240 --> 40:48.720
think and everything you do. Like, for instance, perception. That looks like something very

40:48.720 --> 40:53.840
instant. So very much the sort of continuous, interpolative thing. In fact, there's a lot

40:53.840 --> 40:58.960
of reasoning that's embedded into perception. And the reverse is true, for instance. If you

40:58.960 --> 41:03.760
look at a mathematician, for instance, proving a theorem, where they're writing down on the

41:03.760 --> 41:09.600
sheet of paper is really step-by-step, discrete reasoning type thing. But it's very much guided

41:09.600 --> 41:14.800
by high-level intuition, which is very much interpreted. They know where they're going,

41:14.800 --> 41:22.240
without having to derive the exact sequence of steps to get there. So they have this high-level

41:22.240 --> 41:26.960
kind of view. Kind of like, you know, if you're driving, you have to make discrete decisions

41:26.960 --> 41:33.280
because you are driving on network frauds. But if you have a bird, a GPS, for instance,

41:33.280 --> 41:37.760
you can kind of see the direction in which you are going, which is interpolated. If you're talking

41:37.760 --> 41:42.080
about direction, you're talking about distances, you're talking about geometric spaces. And

41:42.080 --> 41:49.040
everything in the human mind kind of follows this model of type 1 and type 2 thinking at the same

41:49.040 --> 41:55.920
time. If you go back to first principles, intelligence is about abstraction. So intelligence

41:55.920 --> 42:05.360
is about the ability to face the future, given things you've seen in the past. And the way you do

42:05.360 --> 42:12.240
that is, yeah, abstraction. You extract from the past some construct. Maybe it's a template,

42:12.240 --> 42:17.120
maybe it's an algorithm that will actually be effective in terms of explaining the future. And

42:17.120 --> 42:23.760
that's why it makes it abstract, is that it can handle multiple instances of some kind of thing,

42:24.000 --> 42:28.960
that thing is an abstraction. And if it's abstract enough, it can actually handle instances

42:28.960 --> 42:36.080
you've never seen before, right? It does generalization power. And all abstraction is worn

42:36.080 --> 42:43.200
from analogy. Abstraction starts when you make an analogy between two things. Like you say,

42:43.200 --> 42:47.520
time is like a river, if you want to get philosophical or something. But in general,

42:47.520 --> 42:51.520
you can just say this apple looks similar to this other apple. So there is such a thing as

42:51.520 --> 42:57.440
the concept of an apple, for instance. And the part that is shared between the two things that

42:57.440 --> 43:02.960
you're relating to each other, the subject of the analogy that that's the part that can be said to

43:02.960 --> 43:07.760
be abstract, that is the part that will help you make sense of the future, like you encounter a

43:07.760 --> 43:12.400
third apple in the future, you know, it's an apple. Because you don't even need to relate this to

43:12.400 --> 43:16.640
the apple should have memorized, you just need to, you just need to relate it to the template,

43:16.640 --> 43:21.120
the abstract template of an apple that you've formed by from exposure to different kinds of

43:21.120 --> 43:26.800
apples in the past. And if you think about what's what's an analogy, really, like how do you find

43:26.800 --> 43:33.440
an analogy, it's a way to compare two things to each other. And there are only really two ways

43:33.440 --> 43:43.120
to compare things. You can, you can basically ask how similar are they in terms of distance,

43:43.120 --> 43:48.640
like you can say implicitly, there's you're looking at the space of points, there's a distance

43:48.640 --> 43:54.000
between any two points. That's, that's the type one, a subject analogy that leads to type one

43:54.000 --> 44:00.880
abstractions, which leads to a type one thinking, right? So a type one analogy is like your things,

44:00.880 --> 44:06.400
you say to what degree they're similar to each other. So you read them by distance, you, so

44:06.400 --> 44:13.120
implicitly, it means you put your things on in a geometric space, right? And type one abstraction

44:13.120 --> 44:17.760
is going to be a template. It's like you're going to have clusters of things, you can take the average

44:17.760 --> 44:23.360
and say everything that is within a certain distance of that template belongs to this category.

44:23.360 --> 44:29.120
That's that's type one. It's very much the way deep learning models work. And then you and then

44:29.120 --> 44:33.680
you start adding perception and intuition on top of that, which is very much the type one thing.

44:33.680 --> 44:40.080
And the other way you can compare two things is the discrete way, right? You can say these two

44:40.080 --> 44:45.920
things are exactly the same. They have exactly the same structure. Or maybe the structure of this

44:45.920 --> 44:51.280
thing is a subset of the structure of this bigger thing. So this creates topology grounded

44:51.280 --> 44:56.880
comparisons. So you have the geometry grounded comparison. It's all about distances and templates.

44:56.880 --> 45:02.720
And then you have the topology grounded way of comparing things. That's all about exact comparison

45:02.720 --> 45:09.760
or finding a sub graph isomorphism. So in the first case, your objects are very much

45:09.760 --> 45:14.720
points in geometric spaces. So they are vectors. And deep learning is always a great fit for this

45:14.720 --> 45:19.440
sort of stuff. And in the second case, your objects are going to be graphs, right? And you're

45:19.440 --> 45:22.640
and you're really looking at the structure of these graphs and substructure and so on.

45:22.640 --> 45:30.160
And you're doing always you're doing exact comparisons. And in practice, most thinking is

45:30.160 --> 45:35.440
actually kind of some some combination of these two atoms, right? Of these two poles.

45:36.240 --> 45:42.320
You're very rarely just going to say, yeah, this airport is exactly this close to my template

45:42.320 --> 45:46.640
of an airport. So it's an airport. You're going to have basically layers upon layers of thinking.

45:46.640 --> 45:50.880
And some of them are going to be intuitive. Some of them are going to be more about, you know,

45:50.880 --> 45:55.440
comparing structures and so on. What you're saying is really interesting, right? Because you invoke

45:55.440 --> 46:02.160
the kaleidoscope hypothesis in your paper. And the idea there is that a tiny bit of information,

46:02.160 --> 46:10.160
just like in a kaleidoscope, could be represented widely across experience space. So you say that

46:10.160 --> 46:16.000
intelligence is literally having some kind of sensitivity to abstract analogies.

46:16.000 --> 46:23.280
So the intelligence is about being able to face the future unknown future, given your past experience.

46:23.280 --> 46:29.200
And that's fundamentally requires the future to share some commonalities with the past. And

46:29.200 --> 46:34.800
that's that's the idea of the kaleidoscope hypothesis that the universe and our lives

46:34.880 --> 46:41.280
are made of lots of repeated atoms of structure. And in fact, if you look at the source,

46:41.280 --> 46:44.640
there are very few things that are that are unique that are kind of like

46:44.640 --> 46:49.360
the grains of sand that are at the origin of all the different kinds of moving patterns you

46:49.360 --> 46:54.080
can see in the kaleidoscope, right? So the kind of like intrinsic structure contained in the universe

46:54.080 --> 47:01.920
is very small, but it is repeated in all kinds of variants, right? And the idea is that if you see

47:01.920 --> 47:07.360
two things in the universe that look similar to each other or that share some commonalities,

47:07.360 --> 47:13.360
a subgraph, maybe, it fundamentally means that they come from the same thing. And that thing is

47:13.360 --> 47:17.280
going to be is going to be an abstraction. We'll be one of these grains of sand in your

47:17.280 --> 47:24.800
in your kaleidoscope or grains of glass, actually. And intelligence is all about reverse engineering

47:24.800 --> 47:30.400
the universe to get back to this source of intrinsic complexity in the universe to get

47:30.400 --> 47:35.360
back to these abstractions. I think the heart of this conversation goes back thousands of years

47:35.360 --> 47:39.120
because what we're talking about right now is a lot of say, Platonism, right? Which is that there

47:39.120 --> 47:44.400
are these ideal abstract structures. And of course, they they really thought of them as actually

47:44.400 --> 47:50.720
existing in some universe. But you know, even if they don't exist in some reality, they at least

47:50.720 --> 47:56.160
exist in concept. And it strikes at the heart of this duality that's always been a very

47:56.880 --> 48:00.960
that's been one of the central mystery, really, of a lot of human thinking, which is

48:01.600 --> 48:07.120
particle versus wave, you know, discrete versus continuous abstract versus the real versus the

48:07.120 --> 48:12.160
messy. And you know, I think you pointed out, you definitely pointed this out in this call. But

48:12.160 --> 48:17.040
I think also in some of your papers that in your view, you know, let's say the ultimate solution

48:17.040 --> 48:23.120
or whatever of creating artificial intelligence or synthetic intelligence or whatever is a

48:23.200 --> 48:28.400
hybrid system that can do both of these types of reasoning, maybe in kind of multiple layers.

48:29.040 --> 48:34.880
And, you know, I'm kind of curious, where is the state of the art now with actually implementing

48:35.680 --> 48:39.760
hybrid systems, you know, something like, I don't know, is it capsule networks? Is it the

48:39.760 --> 48:45.840
topological neural networks that we talked about? Where where lies the direction of some type of a

48:45.840 --> 48:53.600
hybrid system that in a unified way is capable of doing both of these modes of reasoning, if you

48:53.600 --> 48:59.280
will? Yeah, that's a great question. So I think this is definitely an active field of research,

48:59.280 --> 49:05.360
but I think the most promising direction right now is going to be discrete search very much. So

49:05.360 --> 49:09.760
a system that is discrete search centric that has a DSA and so on. And that's one of the

49:09.760 --> 49:15.360
it's basically just problems in this engine. But it is getting lots of help from deep learning

49:15.360 --> 49:21.680
models. And there are two ways in which you can incorporate this type one sort of thinking into

49:21.680 --> 49:30.080
a phenomenally type two centric system. So one way is so basically, you want to apply deep learning

49:30.080 --> 49:36.960
to any sorts of data sets where you have an abundance of data, and your data is interpreted.

49:36.960 --> 49:41.840
One example would be being able to easily play models to generate a sort of like perception

49:41.840 --> 49:50.080
DSL that your discrete search process can build upon. So look at art, art tasks, for instance,

49:50.080 --> 49:54.160
a human that is looking at art tasks, the very first layer through which they're approaching

49:54.160 --> 49:59.600
the art task is by applying basically perception primitives to the grid they're looking at. They

49:59.600 --> 50:05.040
are not actually analyzing the grid in a in a discrete way like cell by cell, object by object,

50:05.280 --> 50:10.320
they're approaching it holistically, like what do they see? And these outputs can be discrete

50:10.320 --> 50:14.560
concepts. And then you can start you can start applying the script reasoning to them. So generating

50:14.560 --> 50:20.160
the DSL. And by the way, the reason it's possible is because humans have access to tons of visual

50:20.160 --> 50:25.840
data and these different frames share lots of commonalities, right? So it is an interpolative

50:25.840 --> 50:29.680
space where deep learning is relevant, where intuition and perception are relevant. And the

50:29.680 --> 50:35.920
other way, which is is is much more difficult and much, much more subtle thing is basically being

50:35.920 --> 50:43.040
able to provide guidance to the discrete search process, basically, because even though one single

50:43.040 --> 50:47.600
program, so learning one single problem, for instance, for an art task is not a good fit

50:47.600 --> 50:52.560
for deep learning model at all, because you only have a handful of examples to learn from.

50:53.360 --> 50:59.920
And the program is super discrete. It's not really easily embeddable in this movement.

50:59.920 --> 51:04.800
However, here's the thing, the space of all possible programs, for instance, the space of

51:04.800 --> 51:09.120
all possible art tasks and all possible programs that solve art tasks is actually

51:09.120 --> 51:14.800
very likely going to be interpolative, at least to some extent. And so you can imagine a deep

51:14.800 --> 51:21.040
learning model that has enough experience with with these problems and the algorithmic solution

51:21.040 --> 51:26.240
that it can it can start providing directions to the search to the discrete system. So

51:27.600 --> 51:32.880
basically, you're in a kind of like you have, yeah, you have like layers of

51:34.000 --> 51:39.120
of learning the lowest layer is going to be perceptive. It's going to be learned across many

51:39.120 --> 51:44.080
different tasks and many different environments. It's going to be type type one, then you're going

51:44.080 --> 51:50.480
to have the context specific on the fly problem solving system that's going to be type two.

51:51.200 --> 51:55.280
And the reason is going to be possible and efficient is because it's going to be guided

51:55.280 --> 51:59.840
by this upper layer, which is going to be type one, which is also going to be trained

51:59.840 --> 52:04.960
from a very, very long experience across many different problems and tasks. And it is able

52:04.960 --> 52:12.320
to do interpolation between different tasks. So can I challenge you a little bit maybe because

52:12.320 --> 52:17.760
you say maybe, you know, all of these problems and what humans do is a bit of an interpolate

52:17.760 --> 52:25.200
like an interpolation between the interpolative systems and the discrete systems. And I see that

52:25.200 --> 52:32.000
going for, you know, something like an arc task or or if you really write code. But if you really

52:32.000 --> 52:38.000
come to let's say, let's say the highest levels of human intelligence, which to me seems to be

52:38.000 --> 52:46.880
navigating social situations, which is is is ultimately is super complex. And I can imagine

52:46.960 --> 52:53.840
something like the graph structure you're referring to be that being, let's say I come into a room

52:53.840 --> 53:01.440
and I see the graphs as, you know, what kind of social dynamics exist in this room, you know,

53:01.440 --> 53:07.200
this is the father of this person, and that person's kind of angry at me. And so I need to,

53:07.200 --> 53:15.360
you know, do something. And my question is, how often is that really a disk like how often can

53:15.360 --> 53:22.000
you really map this in a discrete way to another graph? Isn't isn't every situation going to be

53:22.000 --> 53:29.120
a little bit different, even in terms of its graph structure? And, you know, even if in an arc task,

53:29.120 --> 53:36.640
a line is just like a little bit squiggled, any program synthesis approach would have a hard

53:36.640 --> 53:42.160
time with it, I feel, or do you think, or do you think I'm misunderstanding something here? Like

53:42.640 --> 53:49.840
how discrete is really discrete? That's the purpose of abstraction. The purpose of abstraction

53:49.840 --> 53:56.880
is to erase the irrelevant differences between different instances of the thing and focus on

53:57.920 --> 54:03.760
the commonalities that matter. So like if the squiggled in your line is not relevant,

54:03.760 --> 54:08.240
then the proper abstraction for a line should abstract it away. I was going to pick up on that

54:08.240 --> 54:13.360
because your main point basically is that program based abstraction is more powerful

54:13.360 --> 54:16.880
than geometric based abstraction, because topology is robust to small perturbations,

54:16.880 --> 54:21.840
but it's more than that. It comes back to these analogies, right? So we actually have functions

54:21.840 --> 54:27.440
and abstractions in our mind that as you say, will take away all of the relevant differences,

54:27.440 --> 54:34.320
but focus on what's salient and what's generalizable. Yeah, exactly. So in in the big sense, do you

54:34.320 --> 54:41.040
think the type one and type two reasoning are really different or is there also a continuum

54:41.040 --> 54:46.400
between them? Like you say we need we need hybrid systems, but is there something,

54:47.680 --> 54:51.280
right? Because they're both they're both in the brain, they're both on the same neurons,

54:51.280 --> 54:57.360
like is there a continuum? So right, so yes and no, I do believe they are they are very

54:57.360 --> 55:02.240
qualitatively different. These are the two poles of cognition, but there are there are, you know,

55:02.240 --> 55:06.800
most most things we do with our mind are a combination of both. That doesn't mean it's

55:06.800 --> 55:10.720
it lies somewhere in between. It means it's a direct combination of one pole with the other,

55:10.720 --> 55:16.160
kind of like what I described with with the arc solver with three layers, with two layers of

55:16.160 --> 55:21.920
that type one and one layer in the middle of type two. But in very much the same way that you can

55:22.560 --> 55:29.360
embed discrete programs in a smooth manifold, you can also do the reverse. And when you're

55:29.360 --> 55:35.360
meaning you can basically encode an approximation of a geometric space using discrete constructs. In

55:35.360 --> 55:39.760
fact, if you've done any sort of linear algebra on a computer, that's exactly what you're doing,

55:39.760 --> 55:44.800
you're actually manipulating ones and zeros. But somehow somehow you're able to have vectors

55:44.800 --> 55:49.440
of seemingly constant new numbers, you can compute a distance between two vectors and so

55:49.440 --> 55:54.640
all of this is an approximation that's actually grounded in discrete programs. So you can you

55:54.640 --> 55:59.280
can actually kind of merge the two together. It's not necessarily always a good idea. In

55:59.280 --> 56:05.120
particular, I think it's often not a good idea to try to embed an overly complex or overly

56:05.120 --> 56:12.400
discreet program in a constant new space. As I was mentioning earlier, the reverse is actually

56:12.400 --> 56:17.920
usually way more tractable. And by the way, my I think this is something that came up before

56:17.920 --> 56:23.920
in our conversation, but my kind of subjective totally not backed by any evidence opinion of

56:24.000 --> 56:30.240
how the brain works is that fundamentally it's doing type one on type two, using a discrete

56:30.240 --> 56:35.920
system, because it's actually much easier to do to type one via an approximation of a geometric

56:35.920 --> 56:39.840
space that's encoded in a district structure than it is to do the reverse. Yeah, and if I can,

56:40.400 --> 56:44.640
if I just for the benefit of the reader, the listeners, if I can give some other examples,

56:44.640 --> 56:49.600
you know, for example, and mixed integer optimization, it's often the case that you

56:49.600 --> 56:54.480
take that problem. And instead of having these discrete values, you project it into a continuous

56:54.480 --> 57:00.160
space, do a continuous optimization. And then as you get sort of close to a good optimization,

57:00.160 --> 57:05.520
you discretize it back over into the, the discrete variables, you know, to, to kind of,

57:05.520 --> 57:10.960
you know, flesh out the most optimal path within that discrete space, or an example to is the

57:10.960 --> 57:15.920
gamma function, you know, which is a continuous generalization of the factorial, right? And

57:15.920 --> 57:21.760
it kind of provides some cool and interesting behavior in between those, those poles that

57:21.760 --> 57:26.400
show up very clearly on the graph as these discrete points. And this is this bizarre

57:27.040 --> 57:31.440
duality between the continuous and the discrete that we see like throughout the universe. And

57:31.440 --> 57:36.960
it's kind of one of the strangest things we have to deal with. Yeah, exactly. I just wonder what

57:36.960 --> 57:40.880
some of the transformers folks must be saying now, because Max Welling, we had him on and

57:41.600 --> 57:46.640
folks have done topological applications using transformers or using graph neural

57:46.640 --> 57:52.960
networks and the alpha fold, the thing from DeepMind, that was looking at graph isomorphisms,

57:52.960 --> 57:58.880
right? It was looking at different types of equivariance in topological space. Is it a naive

57:58.880 --> 58:05.440
thing to say that we could make it continuous or are we on a hiding to nothing? Right. So I guess,

58:05.520 --> 58:12.080
I guess the question is, is there like one approach that's going to end up being universal? And it's,

58:12.080 --> 58:17.200
it's like, can you actually scale deep learning to handle arbitrary district programs? It's kind

58:17.200 --> 58:23.360
of, it's kind of the question. And the answer is no, actually, like, by, by construction, do,

58:23.360 --> 58:29.600
due to the very nature of what deep learning is, it's like parametric continuous parametric models

58:29.840 --> 58:35.200
in fact, smooths, because they're differentiable, sure. And it's quite undecent. That is never

58:35.200 --> 58:41.760
actually going to be a good fit for most discrete programs. So, and, and the reverse is true as

58:41.760 --> 58:46.320
well. I don't think, so you have basically two engines that you can use to learn problems. You

58:46.320 --> 58:51.680
have quite undecent and you have discrete search. And I think the reverse is also true that discrete

58:51.680 --> 58:57.200
search is not going to be this universal approach that's going to beat everything. I truly believe

58:57.280 --> 59:02.640
that the AIs of the future will be truly hybrid in the sense that they will have these two engines

59:02.640 --> 59:06.720
inside them, they will be able to do this, they will be able to do this quick search.

59:06.720 --> 59:10.560
Right. And then, and then, and they will set, you, is that appropriate? You said, by the way,

59:10.560 --> 59:14.640
in your measure of intelligence paper that there are three types of priors, right? Low level,

59:14.640 --> 59:18.960
sensory motor priors and meta learning priors. That's the interesting one. I think that's got

59:18.960 --> 59:23.840
intelligences and high level knowledge. And then we get over to the ARC challenge and, and as you

59:23.840 --> 59:28.720
said in your presentation last year, the two winning folks on that Kaggle challenge, one was

59:28.720 --> 59:33.200
doing a genetic algorithm over a DSL. So doing what you're talking about, a kind of program

59:33.200 --> 59:39.120
search, and actually the winner who got about 20% accuracy. And that was, that was just, yeah, that

59:39.120 --> 59:45.200
was just doing a brute force, you know, selecting combinations of, of operations on this DSL.

59:46.000 --> 59:51.200
So this absolutely fascinates me. So at the moment, that seems like a horrific solution,

59:51.200 --> 59:55.520
but clearly no one could do it using deep learning. So, but, but this is what you're

59:55.520 --> 59:59.840
advocating for. So you're saying for these discrete problems, get, get a DSL. Now,

59:59.840 --> 01:00:03.280
all the stuff you're talking about, presumably they haven't done yet, you're saying, well,

01:00:03.280 --> 01:00:07.840
software engineering, the beauty of software engineering is being able to modularize things

01:00:07.840 --> 01:00:12.560
into building blocks. And in fact, I love citing this thing actually from Patrice Simhard. But

01:00:12.560 --> 01:00:17.280
he said, the reason why software engineering is so good is if I ask you, how long will it take

01:00:17.280 --> 01:00:22.960
you to build the game of Tetris? You will say not long at all. And if you look at the number of

01:00:22.960 --> 01:00:28.640
state spaces in Tetris, it's, it's huge. But the reason you'll be confident to build it in a couple

01:00:28.640 --> 01:00:34.000
of weeks is because you know that you can modularize it into, into blocks, you can't say the same for

01:00:34.000 --> 01:00:38.160
deep learning, right? But they don't appear to have done that on the arc challenge yet.

01:00:38.880 --> 01:00:43.440
Yeah, so the, the solutions we've seen on the accident so far have been incredibly,

01:00:43.440 --> 01:00:48.320
incredibly primitive. And so it's, it's actually quite interesting that you can get to 20%.

01:00:49.360 --> 01:00:54.640
It's very primitive solutions. I think you can, even with today's technology, you can go much

01:00:54.640 --> 01:01:00.560
further. Like the, what I was describing before about learning a DSL that is perceptive and then

01:01:00.560 --> 01:01:05.840
guiding discrete program search. Yeah, intuition about program space. This is already something

01:01:05.840 --> 01:01:10.640
that you can try today. So there's one approach that I was very excited about. And that I thought

01:01:10.960 --> 01:01:16.880
was very cool. And I really like it's, it's called Dreamcoder by Dr. Kevin Ellis and, and folks.

01:01:17.920 --> 01:01:21.760
So check it out if you, if you have incidents, it's very good. I think that they're trying to

01:01:21.760 --> 01:01:27.680
play to arc now, but it's generally like, is this kind of like hybrid deep learning programs into

01:01:27.680 --> 01:01:33.440
this engine? And I think that's really to me, that that is the sort of direction that is the most

01:01:33.440 --> 01:01:39.360
promising to that. So you have a paper that's fairly long on, it's called on the measure of

01:01:39.360 --> 01:01:45.120
intelligence. And you make the case that intelligence is something like the efficiency

01:01:45.120 --> 01:01:51.440
with which we transform prior information and experience into task solutions, as, as you have

01:01:51.440 --> 01:01:58.240
said before. And in that same paper, the arc challenge is presented. So, you know, a naive

01:01:58.240 --> 01:02:04.160
reader like me assumes there is some connection between, you know, what you say about intelligence

01:02:04.240 --> 01:02:11.280
and solving this arc challenge. So my question is, if tomorrow, you know, a new team comes and

01:02:11.280 --> 01:02:17.440
gives you a solution, you evaluate it, it gets whatever 95% correct, it solves the arc challenge.

01:02:18.160 --> 01:02:25.840
Is it immediately intelligent? Or what would you ask of that system for, for you to say,

01:02:25.840 --> 01:02:30.800
yes, that's intelligent, or it's, it's intelligent is, is high or something like this.

01:02:30.880 --> 01:02:38.000
So you, you, you would be able to make that, that conclusion, if and only if arc was a,

01:02:38.000 --> 01:02:43.280
was a perfect benchmark, but it's not, it's actually very much flawed. So if you solve arc,

01:02:43.280 --> 01:02:50.080
are you, are you intelligent? Well, no, because arc is potentially flawed. That's, that's the

01:02:50.080 --> 01:02:56.560
thing. So the thing you need to really understand about arc is that it's not kind of the end state

01:02:56.560 --> 01:03:02.240
of the intelligence benchmark. It is very much a work in progress. And there will be new iterations,

01:03:02.240 --> 01:03:07.680
especially as we learn more about the flows. And by the way, so last year, we ran a Kaggle

01:03:07.680 --> 01:03:13.200
challenge on arc, and we learned a ton, not necessarily a ton about program synthesis approaches

01:03:13.200 --> 01:03:18.160
although there were some cool stuff we still learned about and so on. But mostly we learned about

01:03:18.160 --> 01:03:23.200
the flows of arc. So there will be future additions and so on. So I will tell you this,

01:03:24.080 --> 01:03:29.600
if you solve the specific test set of arc as it exists today, you're not necessarily intelligent

01:03:29.600 --> 01:03:36.000
because it is not perfect because it has its laws. But if more generally speaking, you give me a system

01:03:36.000 --> 01:03:43.280
that is such that any new arc task I throw at it, like I can, I can make some new ones tomorrow,

01:03:43.280 --> 01:03:47.520
for instance, I give them to your system. If it's always solving them, I will say,

01:03:47.520 --> 01:03:51.280
yeah, it's looking like you've got a system that's, that's got, you know, pretty close to

01:03:51.280 --> 01:03:57.760
human level fluid intelligence. This is one of the things that, look, and I like the paper a lot,

01:03:57.760 --> 01:04:03.520
I think, I think it serves as a really good, you know, foundation for us to think differently

01:04:03.520 --> 01:04:08.000
about how to build intelligence. But, but I have some, some issues with it too as well. And one

01:04:08.000 --> 01:04:14.080
of them is this sort of necessity that it requires kind of white box analysis of things in order to

01:04:14.080 --> 01:04:17.920
figure out whether or not they're intelligent. Because for example, suppose time travel is

01:04:18.000 --> 01:04:23.920
actually possible. And you know, somebody like 100 years from now looks back on your arc thing and

01:04:23.920 --> 01:04:28.480
writes an algorithm that, that solves all, all them in there because it actually knows about them

01:04:28.480 --> 01:04:33.040
already and then ships it back into the past and we enter it into the competition. And no matter

01:04:33.040 --> 01:04:37.200
what new arc thing you throw at it, it sort of does well. And you say, well, yeah, you know,

01:04:37.200 --> 01:04:42.160
this thing's like kind of intelligent, but, but we'd be wrong because in the sense in the paper,

01:04:42.160 --> 01:04:46.240
it's actually just encoded, you know, prior knowledge from the future. So we have to,

01:04:46.240 --> 01:04:50.160
we always have to kind of be able to look into the box, right, in order to evaluate

01:04:50.720 --> 01:04:54.400
intelligence in the way that you define in the paper. And so my question is one,

01:04:54.960 --> 01:05:01.200
isn't that a bit of a undesirable feature? And two, do you have any hopes for a more black box

01:05:01.200 --> 01:05:06.160
measure of intelligence? So basically, the fundamental issue is that if intelligence

01:05:06.160 --> 01:05:13.040
is this conversion ratio, then computing it requires knowing where you start from. And

01:05:13.040 --> 01:05:17.600
you don't really have a way around it. So the thing to keep in mind is that the

01:05:17.600 --> 01:05:23.280
under measure of intelligence stuff is not so much meant to provide like a sort of like

01:05:23.840 --> 01:05:28.480
golden measure of tape to measure anyone's intelligence or anything's intelligence.

01:05:28.480 --> 01:05:36.080
It is more meant as a sort of cognitive device to help you think about what the actual challenges are

01:05:36.800 --> 01:05:41.680
to help you kind of kind of reframe AI because they think they have been pretty deep and

01:05:41.680 --> 01:05:46.320
longstanding conceptual misunderstandings. So that is really being, that's being holding the

01:05:46.320 --> 01:05:54.400
feedback. So it's very much meant as a cognitive device. If you take a step back and you ask,

01:05:54.400 --> 01:05:59.200
why are we even trying to define intelligence and measure intelligence in the first place,

01:05:59.200 --> 01:06:05.280
why is it useful at all? I think it's useful to the extent that it is actionable, right,

01:06:05.280 --> 01:06:10.800
a good definition and a good measure should be actionable. So meaning it should help you

01:06:11.600 --> 01:06:17.280
think, it should help you find solutions and it should help you make progress. In particular,

01:06:17.280 --> 01:06:22.640
a good definition is a definition that will highlight the key challenges and help you think

01:06:22.640 --> 01:06:27.200
about it. And I think that's what the paper does. And a good measure is a measure that gives you an

01:06:27.200 --> 01:06:33.760
actionable feedback signal towards building the right kind of system, right in the sense that

01:06:33.760 --> 01:06:39.840
it will be capable of doing more. And so that's part of the feedback signal is what ARC is trying

01:06:39.840 --> 01:06:48.720
to achieve. And the way it's trying to control for priors and experience is by assuming a fixed

01:06:48.720 --> 01:06:53.680
set of priors. And you're going to see, you know, every test taker can have such priors.

01:06:53.680 --> 01:06:58.800
This is the core knowledge priors. And then it controls for experience by only giving you a very

01:06:58.800 --> 01:07:05.360
small number of input examples. And also by making sure the tasks are sufficiently novel and

01:07:05.360 --> 01:07:11.440
surprising that you're unlikely to have seen a very similar instance before. So now, of course,

01:07:11.440 --> 01:07:16.240
it's super flawed. So this is not 100% true, of course, but this is kind of like the the

01:07:16.240 --> 01:07:20.880
planning ideal that we're trying to get to. So that for the record, that's a fascinating point to

01:07:20.880 --> 01:07:25.200
me is that you view this more as a cognitive device to help guide us to produce better,

01:07:26.000 --> 01:07:32.880
better intelligent agents. It is not an input. It's not like ARC is like the measure of intelligence

01:07:32.880 --> 01:07:39.200
and all we need to do is solve ARC. This is not at all the point. It's like it's one.

01:07:39.200 --> 01:07:43.280
Oh, darn, because I was doing pretty well on some of the examples. I was hoping that would

01:07:43.280 --> 01:07:47.280
mean I was intelligent. But another interesting point, because Keith and I were looking at the

01:07:47.280 --> 01:07:51.840
paper again yesterday, because it's been, I haven't properly studied it since last year. But

01:07:52.880 --> 01:07:56.320
we were starting to talk about an alien that comes in from outer space. And, you know,

01:07:56.960 --> 01:08:02.640
we don't know the priors and the experience. And then I was thinking in a way, it might be a

01:08:02.640 --> 01:08:08.240
kind of lower bound on intelligence, right? Because, you know, if I play chess, and if I beat

01:08:08.240 --> 01:08:12.720
someone with a higher elo than me, then only really tells me that I'm better, you know, as

01:08:12.720 --> 01:08:18.320
good as that person that I just beat. And similarly, this measure of intelligence, it only gives you

01:08:18.320 --> 01:08:23.600
a reading in the situation when you know what the conversion was. So if they are not converting

01:08:23.600 --> 01:08:29.120
anything, then you don't know. And another interesting byproduct of this is the more

01:08:29.120 --> 01:08:36.480
experienced you get, the less intelligent you get. So I would push back against that last claim

01:08:36.480 --> 01:08:41.520
that the measure of intelligence as I define it is dependent on how much experience you have.

01:08:43.360 --> 01:08:47.120
Because the amount of initial experience you have does not actually change at the conversion

01:08:47.120 --> 01:08:54.240
ratio if you measure it via the right task. So you might need, so if you have a fixed set of tasks,

01:08:54.240 --> 01:08:59.520
then yes, it does affect it. But if you're able to renew your set of tasks and come up with

01:08:59.520 --> 01:09:04.000
styles that are orthogonal to the experience that you have, then it's not the actual effect,

01:09:04.000 --> 01:09:11.280
the definition. So, but yeah, you're definitely right that if you take a pure black box approach,

01:09:11.280 --> 01:09:17.600
and all you're looking at, the only thing you can really measure is the behavior of a system.

01:09:17.600 --> 01:09:23.520
And unless you know how that behavior is achieved, you can't really tell immediately

01:09:23.520 --> 01:09:29.040
how much intelligence was involved in producing this behavior. If you look at an insect,

01:09:29.040 --> 01:09:33.920
they're capable of super complex behavior. Are they crazy intelligent? Well, actually,

01:09:33.920 --> 01:09:39.360
you know, probably not. And the way you can really tell is by putting these systems out of

01:09:39.360 --> 01:09:44.800
their comfort zone, getting them to face novel situations and see how they adapt. And that's

01:09:44.800 --> 01:09:52.160
the measure of intelligence. It's adaptability, the ability to deal with novel and unknown

01:09:52.160 --> 01:09:58.960
situations. But in order to give your system a novel and unknown situation, you need to have

01:09:58.960 --> 01:10:05.280
this white box understanding of what it already knows about. And that's not really something

01:10:05.280 --> 01:10:12.400
you can work on. So can I ask about the generalization difficulty? Because I sort of had

01:10:12.400 --> 01:10:17.760
some difficulty intuitively with some of, let's say, it's limiting cases. So for example,

01:10:18.720 --> 01:10:23.760
you know, the algorithmic complexity is highest. Let's just suppose we're dealing with problems

01:10:23.760 --> 01:10:30.320
tasks where we have whatever sets of integers mapped to zero, one values, you know, the algorithmic

01:10:30.320 --> 01:10:36.080
complexity will be greatest when that's just a random mapping, like I just assigned zero and one

01:10:36.080 --> 01:10:41.280
randomly to every single integer. And if I go to look at that generalization difficulty,

01:10:41.280 --> 01:10:46.320
it's going to be super high, because the length of the program for any set is basically going to

01:10:46.720 --> 01:10:52.320
be, you'd have to encode the entire set as a hash table, right? So how does like this measure

01:10:52.320 --> 01:10:59.120
account for or help us avoid problems where we're confusing generalization difficulty with just

01:10:59.120 --> 01:11:04.880
increasing random, you know, randomness? Well, I mean, increasing randomness is a part of

01:11:04.880 --> 01:11:10.800
the realization difficulty, right? Generalization is really the ability to deal with the stuff you

01:11:10.800 --> 01:11:15.840
don't know about the stuff you don't expect, the stuff you haven't seen before. And randomness is

01:11:15.920 --> 01:11:20.400
a part of it. But you're right that if you just add randomness to a system, you're increasing

01:11:20.400 --> 01:11:25.680
the generalization difficulty, but you're not increasing it in a very interesting way, right?

01:11:25.680 --> 01:11:31.440
Because you're increasing it in a way that's kind of orthogonal to an integration system's ability

01:11:31.440 --> 01:11:37.520
to deal with it, right? The best you can do is modify the system to be more robust to very much

01:11:37.520 --> 01:11:43.120
randomness. But that's not super interesting. What's really interesting is to test the system's

01:11:43.120 --> 01:11:50.320
sensitivity to subtle analogies, is to make the system face novel and unexpected situations that

01:11:50.320 --> 01:11:56.160
are actually derived from the past, but in interesting ways, right? Not just random ways.

01:11:56.160 --> 01:12:04.720
You've run this Kaggle challenge on ARC. And, you know, we know from systems such as Alpha Go and

01:12:04.720 --> 01:12:12.000
so on that bootstrapping intelligent, like bootstrapping AI systems can be very valuable,

01:12:12.000 --> 01:12:18.640
like playing them against each other and so on. And also, we know that something like markets can

01:12:18.640 --> 01:12:29.200
be very efficient and valuable. And I imagine a system where you'd have agents creating ARC tasks

01:12:29.200 --> 01:12:35.040
and other agents solving ARC tasks, and they're going some kind of money around and so on. And

01:12:35.040 --> 01:12:41.120
this could be kind of a powerful engine for research teams to research anything like this.

01:12:41.120 --> 01:12:47.600
And, you know, given that you have, I don't know how much, but you do have the backing of Google

01:12:47.600 --> 01:12:58.160
with a bit of capital in hand. Could you imagine there being a push for this kind of thing? Or is

01:12:58.160 --> 01:13:08.880
it, as of now, an intellectual curiosity? Yeah, so I don't have that much backing you from Google

01:13:08.880 --> 01:13:15.440
around this kind of project. But, yeah, so it would be super interesting to have this kind of

01:13:15.440 --> 01:13:20.640
two-part system where one part is generating the task and one part is learning to solve them.

01:13:20.640 --> 01:13:26.720
And you could get them to do some kind of curriculum optimization, like the task generator network

01:13:26.720 --> 01:13:33.040
would not just be trying to generate tasks that look like ARC tasks. It would be trying to

01:13:33.040 --> 01:13:39.120
generate tasks that correspond to level of generalization, difficulty and complexity that is

01:13:40.080 --> 01:13:45.440
right below the limits of the student system that's trying to solve them. Kind of like, you know,

01:13:45.440 --> 01:13:52.720
the way a teacher would provide exercises that are solvable, but challenging. They shouldn't be.

01:13:52.720 --> 01:13:56.720
They shouldn't be easy. They shouldn't be impossible. They should be solvable. Because

01:13:56.720 --> 01:14:01.760
that's how you get the most growth. So it's actually a system that's described at the very end

01:14:02.480 --> 01:14:07.680
of the paper on the measure of contagions. And I think one thing I point out in the paper is

01:14:07.680 --> 01:14:13.360
kind of like the pitfall you should avoid falling into is that this system is circular,

01:14:14.000 --> 01:14:20.000
right? And the complexity you're going to see in your task, it needs to come from somewhere, right?

01:14:21.200 --> 01:14:26.720
It's like conservation of complexity. So the system, this two-part system needs to have

01:14:27.680 --> 01:14:33.840
a source of intrinsic complexity. It needs to be grounded in the real world.

01:14:33.840 --> 01:14:39.360
And one way we can achieve that grounding, and I've been thinking about it, is I think we should,

01:14:39.360 --> 01:14:45.120
you know, like ARC tasks, as they are today, they're made by me and this is not a good setup

01:14:45.120 --> 01:14:50.400
because it's going to be biased. It's going to be very bottlenecked as well. I think we should

01:14:50.400 --> 01:14:55.920
start crowdsourcing our task. There should definitely be, you know, a filtering system so

01:14:55.920 --> 01:15:00.000
that we make sure that we're only keeping our tasks that are interesting, that are not too easy,

01:15:00.000 --> 01:15:06.480
that are not difficult, and that are only grounded in core knowledge priors. But if we have, like,

01:15:06.480 --> 01:15:12.000
this stream of novel ARC tasks that contain intrinsic complexity and novel information,

01:15:12.000 --> 01:15:17.040
because they come from the real world, they come from human brains, that have experienced the

01:15:17.040 --> 01:15:23.120
real world, and you use that as a way to ground your task generator, then you're starting to get

01:15:23.120 --> 01:15:29.440
a very interesting three-part system, right? So I would love to actually get that started,

01:15:29.440 --> 01:15:36.240
to actually produce a V2 of ARC as soon as possible, let's include, you know, 10x more tasks

01:15:36.240 --> 01:15:41.440
that will be crowdsourced, and maybe something that will take the form of a continuous challenge

01:15:41.440 --> 01:15:46.240
where you have an API where you can draw a new ARC task, and every time you draw a task, it's

01:15:46.240 --> 01:15:52.400
actually a different one because you have so many of them. Gamify it, that'll make a fun game

01:15:52.480 --> 01:15:57.440
on a mobile app. There are actually a few people who have created, because ARC is open source,

01:15:57.440 --> 01:16:01.120
and they're totally free licensed, there are a few people who have created mobile apps where

01:16:01.120 --> 01:16:05.120
users sort of ARC tasks, and apparently it's popular. So there's also the other angle you

01:16:05.120 --> 01:16:08.960
mentioned in the paper, which was, which is pretty fascinating, you're talking about it almost right

01:16:08.960 --> 01:16:17.440
now, which is that, okay, let's start thinking about how to map ARC performance to psychometric,

01:16:17.440 --> 01:16:21.680
you know, classic kind of psychometric tests. Are there any efforts that you're aware of

01:16:21.680 --> 01:16:30.000
underway right now to do that? Are you involved in any ETAs? Yeah, ETAs, I'm not sure. So we did a

01:16:30.000 --> 01:16:35.600
workshop at AAAI the other day, and there were two presentations about efforts that teams of people,

01:16:35.600 --> 01:16:41.280
so there are people who do neuropsychology, and they're using ARC in very interesting ways. So

01:16:41.280 --> 01:16:46.960
there's a group at NYU, and there's a group at MIT, and yeah, so they're using ARC for neuropsychology

01:16:46.960 --> 01:16:53.280
experiments, and it's it's super cool. Amazing. I want to switch over a little bit, because of

01:16:53.280 --> 01:16:58.640
course, you know, other than the measurement of intelligence, you are also famous for a small

01:16:58.640 --> 01:17:07.280
library you wrote once in a while called Keras. And I wish I wrote it, and then that was that.

01:17:08.640 --> 01:17:12.560
No, I yeah, it's been very much an ongoing project for the past six years.

01:17:13.520 --> 01:17:19.040
It was because I remember, you know, the days of TensorFlow one and and Theano,

01:17:19.840 --> 01:17:26.320
and things like this. And Keras was just, I think, so helpful to a lot of people, because it just

01:17:26.960 --> 01:17:33.360
simplified all of this, you know, graph construction, whatnot, and so on. It just made it accessible to

01:17:33.360 --> 01:17:40.240
so many people. And now with the development of, you know, things like PyTorch and TensorFlow two,

01:17:40.320 --> 01:17:47.760
it almost seems like Keras is it has been kind of absorbed by TensorFlow two, right, there is TF.Keras.

01:17:47.760 --> 01:17:54.240
And now I think the newest APIs are even sort of vanishing that a little bit. Do you do you see

01:17:55.280 --> 01:18:00.720
Keras going away? Do you see it changing? Where do you see it? Where do you see Keras going?

01:18:01.360 --> 01:18:06.880
Yeah, so going away, definitely not. I mean, we have we have more users than ever before. And we

01:18:06.880 --> 01:18:11.760
are still growing very nicely, both inside Google, like one more teams that Google are moving away

01:18:11.760 --> 01:18:17.040
from TensorFlow one and adopting Keras and outside Google as well. It's a big market out there,

01:18:17.040 --> 01:18:23.760
and there's definitely room for multiple frameworks. Evolving absolutely, I mean, Keras is constantly

01:18:23.760 --> 01:18:30.080
evolving, but evolving with continuity. Like if you look at Keras from 2016 or 2015, you look at

01:18:30.080 --> 01:18:36.240
Keras now, you recognize, is it the same thing? And it's the same API. And yet it's actually a very

01:18:36.240 --> 01:18:42.560
different and much, much bigger set of features and things you can do it. So evolving, definitely.

01:18:42.560 --> 01:18:49.440
And there are so several, so you, I think you asked, you know, about, yeah, like,

01:18:49.440 --> 01:18:53.520
Keras is getting kind of merged into TensorFlow, does it mean it's like failing away?

01:18:53.520 --> 01:18:59.840
So definitely not. So merging with TensorFlow was a good idea because it starts enabling

01:18:59.920 --> 01:19:06.480
a spectrum of workflows from the very high level, like scikit-learn like, to the very low level,

01:19:06.480 --> 01:19:13.760
numpy like, and everything in between. In the early days, because Keras had to interact with

01:19:13.760 --> 01:19:20.080
multiple backends via backend interface, it means you had this kind of like a barrier where as long

01:19:20.080 --> 01:19:25.520
as you use the Keras APIs, everything was super simple. It was scikit-learn like, so very easy,

01:19:25.520 --> 01:19:31.520
very proactive, very fast. But if you wanted more customization, at some point, you would hit

01:19:31.520 --> 01:19:37.440
that backend barrier. And you had to reverse to TensorFlow base or piano base workflow,

01:19:37.440 --> 01:19:42.640
that was low level, but when, where you couldn't really leverage Keras effectively,

01:19:42.640 --> 01:19:47.920
by removing the backend thing and just saying the flow together in one spectrum,

01:19:47.920 --> 01:19:53.680
then you get really this progressive disclosure of complexity when you can start out with the

01:19:53.840 --> 01:19:58.880
very high level thing, but then you need to customize your training step. You have an API for

01:19:58.880 --> 01:20:05.520
that. And you can just mix and match seamlessly the low level TensorFlow stuff with the high

01:20:05.520 --> 01:20:10.000
level Keras step. And that way you can achieve any, can work with Keras and TensorFlow at the

01:20:10.000 --> 01:20:15.680
level of abstraction that you want. Very, very easy high level or very, very low level full

01:20:15.680 --> 01:20:21.520
flexibility. It's up to you. I'm going to point out the temptation here to analogize connecting

01:20:21.520 --> 01:20:27.840
type one with type two reason. Yeah, why not? I was just about to do that. At least Francois

01:20:27.840 --> 01:20:32.880
has great form for this, because not only does he talk about having powerful and useful interfaces

01:20:32.880 --> 01:20:38.160
and abstractions in deep learning, he's been playing this game in the library world for quite

01:20:38.160 --> 01:20:42.960
some time. But I wanted to touch on this quickly. We had a couple of people in our community asking

01:20:42.960 --> 01:20:50.560
you about Keras, actually. And Robert Lange and Ivan Finnell said that apparently Theano has returned

01:20:50.560 --> 01:20:55.120
with Jax and XLA underneath and he wants to know are there any plans to add it as a Keras back end

01:20:55.120 --> 01:21:00.000
and Robert Lange also says, you know, just Jax on its own. Would you add that as a back end?

01:21:00.000 --> 01:21:03.200
We've also had a couple of questions about PyTorch as well. Is there anything on the

01:21:03.200 --> 01:21:07.920
roadmap for that? Okay, so let's talk about Jax. I think Jax is an awesome project and the

01:21:07.920 --> 01:21:12.880
developers have really done a very, very interesting and very good job with it. And lots of people,

01:21:12.880 --> 01:21:18.240
I like Jax actually. So that said, adoption is not super high. I think Google is probably the

01:21:18.240 --> 01:21:22.880
company where it's the most adopted, where you will find the most users. And even then,

01:21:22.880 --> 01:21:28.640
it's like a tiny, tiny, tiny fraction of total machine usage at Google. But I think as a project,

01:21:28.640 --> 01:21:35.040
it's a beautiful project. It's elegant. It's powerful. It's great. So would I like to add

01:21:35.040 --> 01:21:41.200
Jax back end to Keras or PyTorch back end to Keras? So I want to say we've really moved away

01:21:41.200 --> 01:21:49.680
from this like interface back end kind of model. So precisely for the reason I was describing,

01:21:49.680 --> 01:21:53.680
because you want to achieve this spectrum of workflows, with that, I think this cliff where

01:21:53.680 --> 01:21:59.280
you go, you fall from the high level down to the low level. We don't want the cliffs. We don't

01:21:59.280 --> 01:22:03.520
because cliffs create silos of users where you have the high level users. You want a gradient.

01:22:03.520 --> 01:22:09.040
Yeah, you want the gradient. Exactly. So that said, I think it would be super cool to have a

01:22:09.040 --> 01:22:15.040
sort of like re-implementation of the Keras API on top of Jax that will also achieve this screening

01:22:15.600 --> 01:22:19.920
and that will still follow the Keras API spec. It would still be the same thing,

01:22:21.600 --> 01:22:25.840
but on top of Jax. That said, so I would love to see something like this. This is also a very

01:22:25.840 --> 01:22:31.120
low priority for us because we have the actual current Keras, which I wish we need to work on,

01:22:31.120 --> 01:22:35.840
which has lots of users. So we don't really have time to do this. But in theory, would it be cool?

01:22:35.840 --> 01:22:40.400
Yeah, sure. I would love to see something like this. So if I had tons of free time, I would

01:22:40.400 --> 01:22:43.760
probably build it, but in practice, I don't. Fantastic. Well, we've got another question

01:22:43.760 --> 01:22:48.400
from Giovanni actually. He says, what does Francois think of Dr. Kenneth Stanley's book on the myth

01:22:48.400 --> 01:22:52.640
of the objective? Are you familiar with Kenneth Stanley's work about the tyranny of objectives

01:22:52.640 --> 01:23:00.400
and open-endedness? So I'm vaguely familiar with the name. I'm not really familiar with the book.

01:23:01.120 --> 01:23:06.560
Oh, okay. Well, sorry, not to worry, but it's Kenneth has been a huge inspiration for me.

01:23:06.560 --> 01:23:14.560
And he talks a lot about objectives leading to deception. So sometimes following an objective

01:23:14.560 --> 01:23:19.520
monotonically sends you in the wrong direction. And his solution to that is either quality,

01:23:19.520 --> 01:23:24.000
diversity, or more recently, open-endedness, which is that if you have an infinitude of

01:23:24.000 --> 01:23:29.280
objectives, in a sense, the system has no objective. And you can also with diversity,

01:23:29.280 --> 01:23:33.520
preservation, you can overcome deceptive search spaces. But yeah, you might have heard of the

01:23:33.520 --> 01:23:38.480
poet algorithm, which he was involved in. Yeah, absolutely. No, I'm aware. And so when it comes

01:23:38.480 --> 01:23:44.720
to your description of the problem's objectives, I completely agree that one thing I mentioned

01:23:44.720 --> 01:23:51.280
in the paper, it's like the shortcut rule, which is that if you try to achieve one thing, one

01:23:51.280 --> 01:23:55.680
objective, you're going to achieve it. But the thing is, you're going to take every shortcut

01:23:55.680 --> 01:23:59.920
along the way for things that we are not actually incorporated in your objective.

01:23:59.920 --> 01:24:04.560
And this leads to systems that are not actually doing what you wanted them to do. Like for instance,

01:24:04.560 --> 01:24:10.880
we built chess playing systems, because we hoped that a system that could play chess would have to

01:24:10.880 --> 01:24:16.640
be able to feature reasoning, book learning, creativity, and so on. Turns out it just plays

01:24:16.640 --> 01:24:23.200
chess. That's what it does. The same is true with challenges and Kaggle. The winning systems,

01:24:23.200 --> 01:24:28.160
they just optimize for the leaderboard ranking and they achieve it. But they achieve it at the

01:24:28.160 --> 01:24:34.320
expense of everything else that you might care about the system. Like, is the code base readable?

01:24:34.320 --> 01:24:39.520
No. Is it computationally efficient? No, it's actually terrible. You could never put it in

01:24:39.520 --> 01:24:44.080
production. Is it explainable? No, and so on. Yeah, so it's like, if you if you optimize for

01:24:44.080 --> 01:24:49.280
something, you get it, but you take shortcuts. Yeah, exactly. And that's very much what Kenneth

01:24:49.280 --> 01:24:53.440
says as well. I love what you said about shortcuts. You said in your New York's presentation that if

01:24:53.440 --> 01:24:58.320
you optimize for a specific metric, then you'll take shortcuts on every other dimension, not

01:24:58.320 --> 01:25:02.800
captured by your metric. And you said in a machine learning context, it's similar to overfitting,

01:25:02.800 --> 01:25:07.760
right? Because on task specific skills, you actually lose generalization if you get good at

01:25:07.760 --> 01:25:12.080
a particular task. So it's completely orthogonal to what you want. I know you're very well known

01:25:12.080 --> 01:25:16.640
for your skepticism of the intelligence explosion. And what I love about your conception of

01:25:16.640 --> 01:25:21.840
intelligence is that you think of it as a system or as a process, you say that intelligence is

01:25:21.840 --> 01:25:28.240
embodied, right? So you have a brain in a body acting in an environment. And in that context,

01:25:28.240 --> 01:25:32.880
it makes sense that you would think that there are environmental kind of rate limiting steps to

01:25:32.880 --> 01:25:37.920
any kind of super intelligence, right? But I spoke to someone the other day who is of the other

01:25:37.920 --> 01:25:42.880
persuasion, shall we say, and this person was saying, Well, what if you had a super, super

01:25:42.880 --> 01:25:49.120
smart bunch of scientists? I know you said in your rebuttal that if you look at the IQ of a

01:25:49.120 --> 01:25:55.600
scientist who is Richard Feynman, for example, the same IQ as a mediocre scientist, turns out

01:25:55.600 --> 01:26:01.040
that IQ only helps up to about 125. And then it stops helping you. But these people would say,

01:26:01.040 --> 01:26:05.280
Oh, well, you know, what if what if every single scientist was an Einstein and intelligence is

01:26:05.280 --> 01:26:09.360
just making better decisions, they would consistently make better decisions and science

01:26:09.360 --> 01:26:14.720
would accelerate. A chimp doesn't understand how good a human is. So how would we understand what a

01:26:14.720 --> 01:26:18.480
super intelligent person would do? You know, they'd invent nanotech, they'd upload themselves into

01:26:18.480 --> 01:26:22.960
the matrix, they'd do all of this stuff, and somehow they would miraculously overcome. Do you

01:26:22.960 --> 01:26:27.440
know what I mean? How would you respond to that? Yeah, if every scientist was super intelligent

01:26:27.440 --> 01:26:32.400
in human terms, that would in fact accelerate science. But it would not really like accelerate

01:26:32.400 --> 01:26:39.200
science in a linear fashion and very much not in an exponential fashion. So I guess the main

01:26:39.360 --> 01:26:46.080
conceptual differences I have with these folks is that they tend to credit everything humans can do

01:26:46.080 --> 01:26:52.000
to the human brain. And they have this vision of intelligence as you know, a brain in a jar

01:26:52.000 --> 01:26:55.920
kind of thing. And if you tweak the brain, it gets more intelligent and intelligence

01:26:56.560 --> 01:27:02.080
is directly expressed as power. If you're more intelligent, if you have a hierarchy, you can

01:27:02.080 --> 01:27:07.040
do more things, you can solve more problems and so on. And in particular, you can build a better

01:27:07.120 --> 01:27:12.560
brain. And by the way, there is not really any practical evidence that this is true. But

01:27:12.560 --> 01:27:18.160
I view intelligence here more as this holistic thing that okay, you have the brain, but actually

01:27:18.160 --> 01:27:24.640
the brain is in a body which gives it access to a certain set of actions it can do and set

01:27:24.640 --> 01:27:30.320
up a perception primitives. And this body is an environment which gives it access to a set of

01:27:30.320 --> 01:27:38.160
experiences, a set of problems it can solve. And to a very large extent, you know, the brain is just,

01:27:38.160 --> 01:27:44.400
it's not so much a problem solving algorithm, like a problem center descending, as it is a

01:27:44.400 --> 01:27:50.080
big sport. And you put it in an environment to absorb experiences from that environment. And

01:27:51.280 --> 01:27:56.000
one thing that's super important to understand if you're on issue, if you really think deeply about

01:27:56.080 --> 01:28:02.880
intelligence, is that most of our expressed intelligence does not come from here, it is

01:28:02.880 --> 01:28:06.800
externalized intelligence. So externalized intelligence can be can be many things.

01:28:08.480 --> 01:28:14.400
If I look up something online, that's externalized intelligence, Google is part of my brain. If I

01:28:14.400 --> 01:28:20.240
write a Python script to test some idea, that's externalized intelligence, my laptop is part of

01:28:20.240 --> 01:28:27.600
my cognition, and so on. But it's actually, it goes much further than that. Most of our cognition

01:28:27.600 --> 01:28:35.360
is crystallized, the crystallized output of someone, someone else's thinking. And the process

01:28:35.360 --> 01:28:40.880
through which we get access to all these accumulated outputs of people's thinking is civilization,

01:28:40.880 --> 01:28:50.720
right? And like 99% of the things you think are the behaviors you act, the behaviors you execute,

01:28:50.720 --> 01:28:57.840
you did not invent them. You did not solve the underlying problem yourself. You're just copying

01:28:58.880 --> 01:29:04.400
a solution. You've seen like, we're in the middle of a pandemic, you're probably washing your hands

01:29:04.400 --> 01:29:09.840
after you went outside. And that's a very smart behavior. But did you invent it? Did you come

01:29:09.840 --> 01:29:15.680
up with that? No, actually, other people came up with that. You did not also come up with the

01:29:15.680 --> 01:29:20.480
infrastructure that enables you to do it in the first place. And so, and this is true, you know,

01:29:20.480 --> 01:29:27.920
for even the most intimate of your thoughts, you're thinking with words that you did not invent,

01:29:27.920 --> 01:29:34.400
you're thinking with concepts that you did not invent or that you did not derive from your own

01:29:34.480 --> 01:29:41.520
experience. They really come from other people, from this accumulation of past generations.

01:29:41.520 --> 01:29:49.040
And if you want to enhance the expressed intelligence of people, then this is actually the

01:29:49.040 --> 01:29:54.880
system you need to tweak and improve, not the human brain, but civilization, right?

01:29:54.880 --> 01:30:00.880
In a way, that seems like a contradiction, because you're talking about the externalization of knowledge,

01:30:01.440 --> 01:30:07.760
not intelligence. So by your own definition, isn't that the opposite of intelligence?

01:30:08.400 --> 01:30:14.000
That's a great point. So I'm relating expressed intelligence. So I was specifically saying

01:30:14.000 --> 01:30:18.000
expressed intelligence as opposed to fluid intelligence. And what expressed intelligence

01:30:18.720 --> 01:30:22.720
means in this context is something very different from what we talk about in the measure of

01:30:22.720 --> 01:30:28.640
intelligence. It means intelligence behavior. And in particular, I think the ability to solve

01:30:28.720 --> 01:30:33.120
problems that you encounter as an individual. Typically, when you solve a problem as an

01:30:33.120 --> 01:30:38.640
individual, you're actually using a solution you found somewhere else. There are not that many

01:30:38.640 --> 01:30:45.040
problems that as an individual, you solve from scratch in your own lifetime. But here's the

01:30:45.040 --> 01:30:50.400
thing is that if you're able to actually solve something novel yourself, you have the ability

01:30:50.400 --> 01:30:55.120
to write about it, you have the ability to communicate it, and then the next generation can

01:30:55.120 --> 01:31:02.560
benefit from it. So let me just pose a kind of a counter argument to this. So suppose you're

01:31:02.560 --> 01:31:08.160
reading a novel about, I don't know, a kind of planet of the apes or something, which was a

01:31:08.160 --> 01:31:16.240
planet that had a life form similar to ours, but with a significantly lower IQ. And a human being

01:31:16.240 --> 01:31:21.360
shows up there one day, and these things start writing about this, hey, this weird alien just

01:31:21.360 --> 01:31:26.480
showed up here, and we captured it, we ran some tests on it, and we figured out it's really

01:31:26.480 --> 01:31:32.720
intelligent. It's much more intelligent than any of us are. And we're worried what's going to happen

01:31:32.720 --> 01:31:39.040
when 100 of them show up instead of just this initial explorer. And some other of these guys

01:31:39.040 --> 01:31:44.320
were like, ah, don't worry about it. They've got two legs and two arms like us, and most of what

01:31:44.320 --> 01:31:50.480
they are is kind of outside of their brain. So I'm not really worried about it. We would be

01:31:50.480 --> 01:31:56.640
reading that with trepidation, right, because we know that when this more intelligent species

01:31:56.640 --> 01:32:01.920
with more fluid intelligence, more externalized intelligence, better technology, all this kind

01:32:01.920 --> 01:32:07.040
of stuff shows up, those guys are going to get wiped out. And it's actually happened like many

01:32:07.040 --> 01:32:12.640
times throughout human history, not that humans were more fluid intelligence showed up and killed

01:32:12.640 --> 01:32:18.560
off, you know, other people, but humans that had more externalized intelligence or more, you know,

01:32:18.640 --> 01:32:22.000
represented intelligence and technology certainly showed up and dominated.

01:32:22.000 --> 01:32:27.120
Absolutely. You're saying it yourself that when it has happened in history, it was not

01:32:27.120 --> 01:32:34.160
fundamentally about one people having smarter brains, but one people having higher technology.

01:32:34.160 --> 01:32:38.800
But that that is not something that is attributable to intelligence itself, right?

01:32:38.800 --> 01:32:42.320
There's a connection there. If you did have a group of species or whatever,

01:32:42.320 --> 01:32:47.440
that was much more intelligent, they will have advanced technologically much faster and further

01:32:47.520 --> 01:32:50.720
in any given amount of time, all else being equal, right?

01:32:51.520 --> 01:32:56.960
It depends on many factors. And that's kind of my point is that is your brain a factor? Yes,

01:32:57.600 --> 01:33:02.480
absolutely, it is. But there are other factors like we are just talking about the development

01:33:02.480 --> 01:33:08.880
of technology. So in that case, the critical factor was not the brain, but the superstructure

01:33:08.880 --> 01:33:13.520
in particular communication and environmental constraints around it. The direction in which

01:33:13.840 --> 01:33:20.720
civilization develops is a direct function of the specific challenges it encounters that

01:33:20.720 --> 01:33:25.280
come from its environment, that comes from its surrounding enemies, and so on. And

01:33:25.920 --> 01:33:32.720
technological development advances the fastest when you have a civilization that are dealing with

01:33:32.720 --> 01:33:37.040
very harsh challenges, but that are not quite fortunate to work them out.

01:33:38.000 --> 01:33:44.400
Because that's what forces them to develop as fast as it can survive. So this is actually a

01:33:44.400 --> 01:33:49.680
very good example where the critical factor was the superstructure that guided the development

01:33:49.680 --> 01:33:54.000
civilization was not actually the brain. But of course, yeah, if a one is smaller,

01:33:54.000 --> 01:34:01.520
then civilization will advance faster. But my point is that there are many factors and that

01:34:01.600 --> 01:34:07.200
by tweaking one factor, the brain, if the brain stops being the bottleneck, then immediately

01:34:07.200 --> 01:34:14.240
some other factor will be the bottleneck. There are civilizations that have not actually advanced

01:34:14.240 --> 01:34:20.560
very much at all because they simply did not face any changes. And did they have worse brains? No,

01:34:20.560 --> 01:34:25.680
actually, they had exactly the same brain. But somehow the outcome was different because

01:34:25.680 --> 01:34:31.920
something else, then the brain turned out to be the bottleneck like lack of environmental change.

01:34:32.960 --> 01:34:38.080
I'm fascinated by scale and bottlenecks in systems. Actually, I work in a large corporation and

01:34:38.080 --> 01:34:42.880
when you have role fragmentation and lots of different businesses and lots of different

01:34:42.880 --> 01:34:48.720
organization or structures, some people might decide to structure themselves based on data

01:34:48.720 --> 01:34:54.480
domain or based on organization or based on something else. And you can think of it topologically.

01:34:54.480 --> 01:35:00.160
And I think human society is very similar to this. And I'm not sure whether evolution

01:35:00.160 --> 01:35:05.200
would lead itself to one particular topology. But the environmental structures and the ways

01:35:05.200 --> 01:35:11.680
that we organize ourselves can create incredible bottlenecks. And that seems to be where the real

01:35:11.680 --> 01:35:17.120
interesting stuff goes on rather than the individuals. And I think you would agree with that,

01:35:17.120 --> 01:35:22.800
Francois. Yeah, absolutely. If you take two companies, and in one company, the average IQ

01:35:22.800 --> 01:35:28.320
is like 15 points higher, but it has a terrible organizational structure and terrible incentives

01:35:28.320 --> 01:35:34.160
and the promo process is super broken or something. And that company is actually going to perform worse

01:35:34.160 --> 01:35:39.360
than the more progressive innovation encouraging company that has a very nice organizational

01:35:39.360 --> 01:35:45.280
structure and where people are actually more mediocre. Maybe they have on average 15 points

01:35:45.280 --> 01:35:50.000
less in IQ, but they're actually going to do a better job because they have the better superstructure.

01:35:50.640 --> 01:35:55.680
Yeah, it's fascinating that the problem is in most corporations, you can't actually design the

01:35:55.680 --> 01:36:01.600
information architecture to be more efficient, because everything is so decentralized and

01:36:01.600 --> 01:36:06.720
fractionated, you can only do it in pockets. And if you try and fix something in one part

01:36:06.720 --> 01:36:10.160
of the organization, everyone else will say, well, my requirements are different. I'm not going to

01:36:10.160 --> 01:36:14.240
wait for you. I'm going to do it my own way. And it's actually a really, really difficult thing

01:36:14.240 --> 01:36:20.640
to do well. To sum up the whole like intelligence explosion thing, the point is really that it's

01:36:20.640 --> 01:36:26.000
a system you have to look at holistically to get it holistically. And just by tweaking one factor,

01:36:26.000 --> 01:36:31.280
which is the intelligence of an individual human brain, then what this means is this factor starts

01:36:31.280 --> 01:36:35.680
being the bottleneck. But that means some other factor in the system, because there's an infinite

01:36:35.680 --> 01:36:40.480
factor that will become the bottleneck. And by just focusing on one factor, you're not going to

01:36:40.480 --> 01:36:46.000
actually lift all the votes. Yeah, and I actually agree with you. However,

01:36:46.960 --> 01:36:51.040
I do want to say, I think we just don't know. I think both sides of the intelligence,

01:36:51.040 --> 01:36:57.360
quote unquote, explosion really can't say for certain that it will or will not pose a mortal

01:36:57.360 --> 01:37:02.160
threat to humanity. I think we have to accept that it's at least a risk factor. And we have

01:37:02.160 --> 01:37:09.280
to be very careful about, in the future, when we start embodying, if we find general intelligence,

01:37:09.280 --> 01:37:13.760
we need to be cautious. If we come up with something that looks like general intelligence,

01:37:13.760 --> 01:37:19.600
there is absolutely some risk potential around it. However, I've never seen anything coming

01:37:19.600 --> 01:37:25.520
anywhere close to that. In fact, the systems that we have today, they fit your almost no

01:37:25.520 --> 01:37:29.920
intelligence whatsoever. So I think it's a bit early to start banning them.

01:37:29.920 --> 01:37:33.840
And even if we get into that conversation, I think Francois would say that intelligence

01:37:33.840 --> 01:37:37.360
must be specialized, right, because of the no free lunch theorems.

01:37:37.360 --> 01:37:42.560
If you define intelligence as your ability to solve problems, then yeah, it's going to be

01:37:42.560 --> 01:37:49.680
specific to a scope of problems, a kind of problems. And like, yeah, what the no free lunch

01:37:49.680 --> 01:37:55.200
theorem is saying is basically, if you want to learn something from data, you have to make assumptions

01:37:55.200 --> 01:38:00.000
about it. Which is why you know a convent, for instance, is a great fit for image data. It's

01:38:00.000 --> 01:38:04.880
not really a great fit for natural language processing. And because it makes different

01:38:04.880 --> 01:38:09.440
assumptions about destruction. It doesn't give me a lot of comfort, though, because I'm fairly

01:38:09.440 --> 01:38:14.560
certain that whatever the first AGI that gets created, it's going to be highly specialized

01:38:14.560 --> 01:38:19.920
for killing other people, because it's going to be a military, you know, secret project,

01:38:19.920 --> 01:38:26.480
probably that finds it. You know, it's, I don't know. But what I know is that right now, we don't

01:38:26.480 --> 01:38:34.640
have anything coming close to AGI. It's probably going to be actually a system that just displays

01:38:34.640 --> 01:38:41.200
you ads. Like if, like, if, you know, if you, if we see where the most money is right now, the

01:38:41.200 --> 01:38:46.960
first AGI is probably just going to like write, not only display, but write the perfect ad for

01:38:46.960 --> 01:38:52.800
you on the fly. You know, it knows what you ate and you know, I know you're joking with

01:38:52.960 --> 01:38:58.240
actually think on the, on the more serious, I think that's highly unlikely because of the

01:38:58.240 --> 01:39:03.920
short code of the story because of the short patrol. I don't think a general intelligence is

01:39:03.920 --> 01:39:08.720
going to be created by the military is not going to be created by a system that's trying to show

01:39:08.720 --> 01:39:14.640
you ads because these are specific goals. And so if you try to optimize those specific goals,

01:39:14.640 --> 01:39:19.920
you're going to end up with a very specialized system in order to build a general intelligence,

01:39:20.000 --> 01:39:27.360
you need to be optimizing for generality itself. So it's going to come from, if it comes from the

01:39:27.360 --> 01:39:31.360
applied, either it's going to come from the academic side, where you have researchers who are

01:39:31.360 --> 01:39:35.920
actually optimizing for generality itself, who said generality as they are going. Or if it's

01:39:35.920 --> 01:39:40.240
come from the applied side, it's going to come from people who have problems where they have to

01:39:40.240 --> 01:39:45.760
deal with extreme novelty, uncertainty, and unpredictability. So it's not going to be ads,

01:39:45.760 --> 01:39:48.800
it's not going to be the military. I don't know where this is going to be.

01:39:48.880 --> 01:39:54.240
One of the things that interested me about Kenneth Stanley was that he says the reason we

01:39:54.240 --> 01:39:59.280
can't monotonically optimize on objectives is because of deception, which means sometimes you

01:39:59.280 --> 01:40:04.160
need to get a lot worse before you get better. His original conception was quality diversity,

01:40:04.160 --> 01:40:08.960
which basically means if you optimize for novelty, that's something that you can optimize on

01:40:08.960 --> 01:40:14.480
monotonically. And also, if you look at evolution, where there is a cacophony of problems and

01:40:14.480 --> 01:40:21.120
solutions divergently being generated, then as an information accumulator, you can optimize

01:40:21.120 --> 01:40:26.320
on that monotonically. And your conception of intelligence is generality. And that also appears

01:40:26.320 --> 01:40:31.360
to be a monotonic increase throughout advancing levels of intelligence. So I think that's quite

01:40:31.360 --> 01:40:36.320
interesting. Anyway, Francois Chollet, this has been my dream come true to have you on the show.

01:40:36.320 --> 01:40:40.160
Thank you so much. It really means a lot to us. And yeah, I appreciate it. Thank you.

01:40:40.240 --> 01:40:44.960
Thanks for having me on the podcast. It's really my pleasure. This was super fun.

01:40:44.960 --> 01:40:48.640
Thanks. And thank you for Keras, by the way. Thanks. I'm glad it's useful.

01:40:48.640 --> 01:40:52.080
We're going to jump straight into the post-show analysis.

01:40:52.080 --> 01:40:56.000
Okay, well, I'm going to mention you did really well, Tim, that trickle sweat

01:40:56.000 --> 01:41:01.200
that this was running down your face the whole time. Not very noticeable. So I think you can

01:41:01.200 --> 01:41:07.120
relax. That was fun. I think it went pretty well. Yeah, it was a dream come true.

01:41:07.680 --> 01:41:13.760
I was actually I was very pleasantly kind of interested in how he he framed, you know,

01:41:13.760 --> 01:41:17.840
the measure of intelligence paper like, look, it's not really about the measure per se. It's just

01:41:17.840 --> 01:41:25.440
that this is this is a cognitive framework, a cognitive tool for thinking about where to go

01:41:25.440 --> 01:41:31.840
and a guidepost for building more generalizable or more general intelligences say like that,

01:41:31.840 --> 01:41:36.800
I totally, totally agree to. And it's quite, you know, quite a fascinating goal, which is like,

01:41:36.800 --> 01:41:40.320
here's a framework to help us think more in the direction we need to be thinking.

01:41:40.880 --> 01:41:48.160
Yeah. And it's so surprising that like the arc challenge is at like 20% solved only because

01:41:48.880 --> 01:41:55.360
you know, he self admits that it's flawed, right? Because he like, he makes the tasks.

01:41:55.360 --> 01:42:00.960
And, you know, there's only finitely many and and you know, you kind of you see the kind of tasks

01:42:01.040 --> 01:42:07.600
he makes, you know, in the public set, you would think that not someone will come up with an

01:42:07.600 --> 01:42:13.520
intelligent thing, but someone will come up with like a smart set of shortcuts to like solve that

01:42:13.520 --> 01:42:20.320
sucker, right? But it's still at 20%. I don't know whether that's due to just, you know, not too many

01:42:20.320 --> 01:42:29.200
people investigating it. Or whether it's really actually a hard problem. And if it is a problem,

01:42:29.200 --> 01:42:34.720
you know, well, it's fascinating too, because if he if he achieves what he wanted, which was

01:42:34.720 --> 01:42:38.720
getting it more outsourced, right, like getting all the intelligent people all around the world

01:42:38.720 --> 01:42:45.440
contributing to arc problems and refining them over time, I think actually that community project

01:42:45.440 --> 01:42:50.720
would help the core knowledge people in that line of research and figuring out, okay, what,

01:42:50.720 --> 01:42:55.840
what is a catalog of all the core knowledge, right? It's, again, back in school, we used to call these

01:42:55.840 --> 01:43:00.480
prime thoughts, because we would, we would play these brain teasers all the time. And we realized

01:43:00.480 --> 01:43:05.840
that there were patterns, right? Like, well, this brain teaser requires the concept of coloring,

01:43:05.840 --> 01:43:09.600
like with a red black tree, where you add an additional variable that kind of lets you

01:43:10.240 --> 01:43:15.600
solve the problem. And if we could really have a nice catalog of, here's all the core knowledge,

01:43:15.600 --> 01:43:20.560
here's all the like problem solving techniques, I think that would be really powerful. I mean,

01:43:20.560 --> 01:43:25.440
well, we kind of have that. So this woman, Elizabeth Spellke, she came up with about

01:43:25.440 --> 01:43:30.240
six core knowledge systems, right? And that and the arc challenge uses four of them. So

01:43:30.240 --> 01:43:36.000
objectness and intuitive physics, one, agentness to elementary geometry, anthropology, three,

01:43:36.000 --> 01:43:40.800
numbers, counting, quantitative comparisons. So the two that weren't in there are places

01:43:40.800 --> 01:43:44.640
and social partners. Now, the thing is, I think we may discover new ones.

01:43:45.200 --> 01:43:50.480
Well, we may be real, but I'm surprised that we did as well as 20%. Because if you think about it,

01:43:50.720 --> 01:43:56.000
imagine if you just guessed the classification on ImageNet when you've got 1000 classes,

01:43:56.000 --> 01:44:01.920
20% would be amazing, wouldn't it? And we've got a similar amount of diversity of tasks on arc,

01:44:01.920 --> 01:44:07.440
right? And what's interesting as well is that all of those different tasks that have been created

01:44:07.440 --> 01:44:13.760
by Francois, they all tie back to just four priors, right? Which means, I don't know whether

01:44:13.760 --> 01:44:20.400
it's uniformly distributed. But 20% seems really good for just guessing ops on a DSM.

01:44:20.400 --> 01:44:24.880
Yeah, there's, there's two things. So first, I would have thought that if someone,

01:44:24.880 --> 01:44:29.840
if someone came up with something that solves more than 5%, it's going to be like immediately at

01:44:29.840 --> 01:44:35.600
95%. Like just because they've sort of cracked the problem. And then, you know, there might be

01:44:35.600 --> 01:44:40.880
a few outliers. But you know, if I would guess that's kind of a task that if you hit the correct

01:44:40.880 --> 01:44:46.320
solution, it's going to be like, boom, you're, you're there. And that's not, which is surprising.

01:44:46.320 --> 01:44:53.760
And the other thing is, I, I don't, I don't feel it's surprising that there's so few priors. What I

01:44:53.760 --> 01:45:00.320
do think is that the space of these priors is still way too large. Like, so if you just think

01:45:00.320 --> 01:45:08.720
about something like object, because in, in these arc tasks, there are, I feel so many more priors

01:45:08.800 --> 01:45:15.280
than just the core knowledge things. Because so one of them is like, you have the, you have like

01:45:16.240 --> 01:45:20.960
this thing, and then you have this thing. And the solution is like, it goes, right? It

01:45:20.960 --> 01:45:26.160
could go, it like bounces. But this is election. Yeah. But, but like the fact that we recognize

01:45:26.160 --> 01:45:32.000
like this is a wall or something, but there is no, there's no, no prior to says like a wall

01:45:32.000 --> 01:45:38.480
needs to be straight, the wall could be like any, you know, any old, any shape at all. And the fact

01:45:38.480 --> 01:45:44.960
that this is much more core knowledge, right? Like in, you know, we build stuff out of straight

01:45:44.960 --> 01:45:49.360
walls. And I think, I think I agree with you, which is I think, I think what you're getting at,

01:45:49.360 --> 01:45:53.360
correct me if I'm wrong, but it's that the way in which the core knowledge is kind of specified

01:45:53.360 --> 01:45:59.040
right now is vague, right? There's a vagueness to it. And I think if we actually start to try and

01:45:59.040 --> 01:46:04.720
codify that more and some type of a mathematical language, Tim, I think it's going to expand

01:46:04.720 --> 01:46:09.520
like the scope of that, we're going to end up with more core knowledge concepts really than,

01:46:09.520 --> 01:46:12.880
than just six, we'll need to make them finer grained. And I'm really excited,

01:46:13.440 --> 01:46:18.320
you know, to see that develop because this has been for me a long wonder, right, which is

01:46:18.960 --> 01:46:25.760
what are the in, in a rigorously defined way? What are these core concepts, these core bits

01:46:25.760 --> 01:46:30.880
of knowledge that make human cognition so powerful? Yeah. And there's also,

01:46:31.440 --> 01:46:36.400
because Yannick made the point about brittleness, right, even in topological space, you still have

01:46:36.400 --> 01:46:42.880
brittleness, but, but the solution was to create powerful abstractions, right? But how would that

01:46:42.880 --> 01:46:48.480
work with the priors? Because if you think about it, you can recombine many of the priors to come

01:46:48.480 --> 01:46:52.720
up with powerful abstractions. And you might find that it doesn't actually filter down to, to that

01:46:52.720 --> 01:46:57.440
many. But the question is, how many things are there? Remember when we spoke to Walid Saber,

01:46:57.440 --> 01:47:01.760
and he was talking about, he's got them somewhere in a PowerPoint deck, you just wouldn't give them

01:47:01.760 --> 01:47:06.800
to us. But you know, part of, part of why, why I agree with Yannick that they're finer grained

01:47:06.800 --> 01:47:10.720
concepts are more important. I think probably stems from a lot of the computer science

01:47:11.840 --> 01:47:17.040
education that I had where, where when we were devising algorithms to do one thing or another,

01:47:17.040 --> 01:47:23.280
you get these little hints that kind of like clever bits of core knowledge that was used to

01:47:23.280 --> 01:47:27.440
solve this problem. Like when you study quicksort, and it's like, you know what, like, I'm just going

01:47:27.440 --> 01:47:31.840
to randomly choose an element. Well, random selection is kind of a bit of core knowledge.

01:47:31.840 --> 01:47:35.840
And then I'm just going to partition by that, and then repeat, you know, or things like,

01:47:35.840 --> 01:47:40.000
I don't know how to balance this tree the way it is. But if I color stuff, like add in red,

01:47:40.000 --> 01:47:45.040
black nodes, I can now overlay a computation that, you know, so there's all these little bits,

01:47:45.040 --> 01:47:49.600
you know, that's what's fascinating about computer programming is it, is it really strikes at the

01:47:49.600 --> 01:47:54.640
heart of this cognition and this core knowledge and how to read, and you have to do it rigorously,

01:47:54.640 --> 01:47:59.120
right? You can't just vaguely go, Oh, you know, just kind of sort it and merge them. You got to

01:47:59.120 --> 01:48:04.560
define like what that means. And it's fascinating dynamic programming. I'm always, I'm always a

01:48:04.560 --> 01:48:11.600
bit amazed by people who have just kind of sort of learned programming, because it's, it's almost

01:48:11.600 --> 01:48:17.520
like a different world in that they'll, they'll, they'll do, it's like, Oh, okay, I need to solve

01:48:17.520 --> 01:48:23.680
this problem. Can I can I copy paste this code here? And it works like 20% of the time, but not

01:48:23.680 --> 01:48:29.120
fully. Yeah. But then on the other side of the coin to that. So when I was working in,

01:48:30.800 --> 01:48:35.520
you know, quantitative trading, right, we had these these massive globally integrated,

01:48:35.520 --> 01:48:41.440
automated trading systems. And I mean, some of the bizarre, I don't want to call them hacks,

01:48:41.440 --> 01:48:48.640
but some of the bizarre sort of piecewise linear equations slash hacks, whatever that actually

01:48:48.640 --> 01:48:53.280
work in reality. You know, you sit there and you look at them and go, when I first went in there,

01:48:53.280 --> 01:48:57.200
as fresh out of academia, and I started seeing things like, Oh, this is crap, like, I'm going to

01:48:57.200 --> 01:49:01.920
figure out some continuous equation that, you know, fits this piecewise linear thing, and it's

01:49:01.920 --> 01:49:06.960
going to do better. Nope, like it didn't do better. I couldn't find any continuous thing to do better.

01:49:06.960 --> 01:49:12.240
It's like, you know, options pay off, right, is this this piecewise linear thing. And, and you're

01:49:12.240 --> 01:49:16.800
like, Oh, that's, well, there should be some continuous like thing in there. Like all these

01:49:16.800 --> 01:49:22.880
weird, you know, piecewise discrete, like kind of hybrid things between continuous and discrete

01:49:23.440 --> 01:49:27.520
work. And, and that's weird. It was weird to me and still weird to me.

01:49:28.080 --> 01:49:35.040
Interesting. But I've got to say, so my main three take homes from Sholay today. I really love Sholay.

01:49:35.040 --> 01:49:42.480
So one, intelligence is generalization. I think that's super powerful to his idea that deep learning

01:49:42.480 --> 01:49:49.200
is really good for value centric abstraction. And because of the manifold hypothesis, lots of

01:49:49.200 --> 01:49:55.200
natural data has some kind of manifold, which you can interpolate on, but lots of discrete

01:49:55.200 --> 01:50:00.880
problems do not have that. Right. And my mind was thinking, Well, does that mean that we can just

01:50:00.880 --> 01:50:05.520
use, because it's because of SGD, you can't even learn the manifold, even if it did exist. But

01:50:05.520 --> 01:50:09.360
he's saying that it doesn't exist for discrete problems. The manifold might be there or it might

01:50:09.360 --> 01:50:13.840
only be there in parts. So that was interesting. And then the third thing that fascinated me about

01:50:13.840 --> 01:50:18.880
Sholay is he talks about these systems and bottlenecks in systems. And we shouldn't be

01:50:18.880 --> 01:50:24.160
thinking about individual brains, we should be thinking about the externalization of knowledge.

01:50:24.800 --> 01:50:31.760
Yeah. And the way he described this, what he thinks like a hybrid system should look like,

01:50:31.760 --> 01:50:39.040
which is sort of you have a perception layer and then a discrete search layer. And then on top of

01:50:39.040 --> 01:50:46.400
that kind of another fuzzy layer that guides the search that can be deep learning again. And I

01:50:46.400 --> 01:50:52.960
think we're like halfway there on the top with the top very much looks like alpha zero, right,

01:50:52.960 --> 01:50:58.720
which is kind of a discrete search that is guided by a neural networks. And the bottom

01:50:58.720 --> 01:51:04.960
layer we have to because that's just our, you know, regular neural networks. I think we have

01:51:04.960 --> 01:51:11.760
big trouble in how to connect the two in a in a single unified way such that we can learn them,

01:51:11.760 --> 01:51:18.640
right? Because the best we can do right now is is right, we can, we can plug a pre train network

01:51:19.600 --> 01:51:24.640
onto alpha zero or something like this, but we can't really, we don't really have it figured

01:51:24.640 --> 01:51:29.360
out yet how to connect the all the stuff. A good example of that is the neural Turing machines,

01:51:29.360 --> 01:51:34.720
like how it's so hard to to optimize them, right? And I think not only do we need these kind of

01:51:34.720 --> 01:51:40.720
three components that that nicely integrate and are optimal, we have to be able to modularize

01:51:40.720 --> 01:51:47.360
and componentize and connect multiple instances of those things together. And some, you know,

01:51:47.360 --> 01:51:52.320
weird topological network to really achieve like kind of the capsule network kind of vision

01:51:52.320 --> 01:51:57.840
where each of the capsules is maybe one of these units. And then they're part of it's like a fractal,

01:51:57.840 --> 01:52:02.640
you know, kind of these fractal layers of those pieces. I don't know whether I was

01:52:02.640 --> 01:52:08.880
misunderstanding you before, Janne, but with the alpha zero thing, my conception is that has

01:52:08.880 --> 01:52:13.840
been quite hard coded. So you're, you're searching through, let's say, a bunch of deep learning

01:52:13.840 --> 01:52:18.880
models and the way you search is quite opinionated. What you're always talking about is have a very

01:52:18.880 --> 01:52:26.080
basic DSL and in that topological space, you just search and you start to modularize and you start

01:52:26.080 --> 01:52:29.920
to create functions and abstractions. And you have from a software engineering point of view,

01:52:29.920 --> 01:52:35.760
you start to build a library of functions that have been written in code that do certain things,

01:52:35.760 --> 01:52:39.040
right? And that's that's different, isn't it to alpha zero?

01:52:40.000 --> 01:52:47.200
Well, the alpha zero is made specifically to search over actions in some kind of RL space.

01:52:48.000 --> 01:52:55.360
Yeah, I mean, what he describes is certainly much more abstract in that you search over applications

01:52:55.360 --> 01:53:04.160
of the DSL. And the DSL itself is not is like a perceptive DSL that in itself is described by

01:53:04.160 --> 01:53:10.080
these lower level neural networks. But I mean, in S, I just, that just came to my mind when he

01:53:10.080 --> 01:53:15.840
described the system, I'm like, oh, the top part looks very much like, you know, alpha zero, because

01:53:15.840 --> 01:53:22.480
that's essentially neural network guided search is something we, we already, already do though.

01:53:23.280 --> 01:53:30.880
Yeah, I, I think, I'm not sure. I think just that the reality is even a bit more fuzzy, because

01:53:31.600 --> 01:53:37.520
what you do as a, as a human, there's also some part of hierarchical system to it,

01:53:37.520 --> 01:53:43.920
in that you can, you can do this, but you can do it hierarchically, right? You can, you can be like,

01:53:43.920 --> 01:53:50.000
okay, I'm gonna, I have to solve, you know, I have this high layer search, and then each of the

01:53:50.000 --> 01:53:57.760
search things goes through maybe a fuzzy thing, but then you, you again, search to solve the sub

01:53:57.760 --> 01:54:04.800
problem. And there is also, you can do it at will too, by the way, like you can, you can scan an

01:54:04.800 --> 01:54:09.600
image, and you get this type one that sort of finds a bunch of objects, and then you do this type two

01:54:09.600 --> 01:54:13.920
thinking where you start reason about those. And in your mind, you can kind of zoom in on one, let

01:54:13.920 --> 01:54:19.200
me like zoom in on that tree. And now like, now I've got the bark, you know, pieces of the bark

01:54:19.200 --> 01:54:25.120
is objects and bugs and reason about so you have this ability to transcend the process and tune it

01:54:25.120 --> 01:54:31.200
and move it around. Yeah, this self like the, that's the whole consciousness aspect, right? That's

01:54:31.200 --> 01:54:37.280
even like, apart from intelligence, you have the ability to, to introspect the whole thing.

01:54:38.000 --> 01:54:44.160
And that probably is a big part of intelligence. I mean, I guess you could have intelligence

01:54:44.160 --> 01:54:49.360
without consciousness, but you know, there is an argument to be made that the fact that you can

01:54:49.360 --> 01:54:55.040
introspect your own processes contributes in big part to the furthering of intelligence.

01:54:56.400 --> 01:55:02.800
Yeah, I would separate consciousness and intelligence, but the thing that hit me the most on

01:55:02.800 --> 01:55:07.520
his newest presentation was when he said intelligence is literally sensitivity to abstract

01:55:07.520 --> 01:55:13.200
analogies. So we were talking about the kaleidoscope. The main thing here with intelligence is that

01:55:13.200 --> 01:55:19.440
there is so much repetition in the universe. Right, but it's repetition in this funny way

01:55:19.440 --> 01:55:25.920
where it's sort of fuzzy repetition. Like, yeah, sure, the solar system kind of resembles galaxies,

01:55:25.920 --> 01:55:31.440
kind of resembles, you know, but, but then there are these little weird differences, these asymmetries,

01:55:31.440 --> 01:55:36.800
and you know, like the universe is a fascinating place. And I don't know, something, yeah.

01:55:36.800 --> 01:55:41.840
Right, that's not what when you say you have to make analogies, which is I can, I can absolutely

01:55:41.840 --> 01:55:46.320
see, you know, this and me, I think my question was formulated a bit dumb where I said, you know,

01:55:46.320 --> 01:55:53.440
if the line is squiggly, what I more meant is that, you know, in that case, it's not a line,

01:55:53.440 --> 01:55:58.640
it's a squiggly line. And the same with the social situations, you know, that is like, okay,

01:55:58.640 --> 01:56:03.280
that that person over there kind of doesn't like me. But then in the next social situation, it's

01:56:03.280 --> 01:56:09.520
kind of a person that doesn't like you and has a gun, or something like this. I almost feel like

01:56:09.600 --> 01:56:15.360
or a group of people that you don't consider as a single single sure they are similar in some way,

01:56:15.360 --> 01:56:20.960
but it's never the exact same thing. So this reasoning by analogy does work, but you always

01:56:20.960 --> 01:56:26.560
do your little modifications on top specific to the situation. And I'm sure there there's a place

01:56:26.560 --> 01:56:33.280
in his framework for this, but it's it's just, again, it's it's like a lot more complex than

01:56:33.280 --> 01:56:38.240
yeah. I think that's what he call I think that's abstraction, at least that, you know, that with

01:56:38.240 --> 01:56:43.280
prior to today, my my concept of abstraction was similar to that, which it's removing the

01:56:43.280 --> 01:56:48.640
insignificant details. So you're able you're you're able to take whatever, you know, some,

01:56:48.640 --> 01:56:53.680
you know, object thing situation doesn't matter, and kind of strip away all the stuff that doesn't

01:56:53.680 --> 01:56:58.800
matter for whatever your purpose is, that's abstraction. And, you know, I think one of

01:56:58.800 --> 01:57:03.920
the weird things is that, and this is kind of the unreasonable effectiveness of mathematics,

01:57:03.920 --> 01:57:11.040
right, is that abstracting actually produces things that are useful, you know, that abstraction,

01:57:11.040 --> 01:57:16.240
I think the fact that abstraction helps with generalization is a very not well understood

01:57:16.240 --> 01:57:21.920
kind of mystery in a sense, like, why should abstraction help generalize, but it does, like

01:57:21.920 --> 01:57:28.880
in the real world, that's what happens. Though the yet abstraction in though abstraction has to

01:57:28.880 --> 01:57:35.120
be somehow specific to what what you want to do, like, like, you're right, an apple is an apple only

01:57:35.120 --> 01:57:40.880
if, you know, you're looking for food or non food, but when it comes to this fear, if you want to

01:57:40.880 --> 01:57:46.560
shoot it out of out of a potato can, exactly, but when it comes to, you know, separating fruit by

01:57:46.560 --> 01:57:51.440
ripeness, then it's not an apple is an apple, then all of a sudden, this apple has much more in

01:57:51.440 --> 01:57:57.600
common with this orange, right, so that even the way how you abstract, it's not like, it's not like

01:57:57.600 --> 01:58:02.880
we can just, you know, plug in our ResNet 50, and then boom, we get an embedding vector, and that's

01:58:02.880 --> 01:58:09.840
our abstraction, but the how you abstract is also incredibly specific to what you want to do.

01:58:10.960 --> 01:58:15.600
Yeah, and that's what, and I agree with Saba that this is an empirical question, right,

01:58:16.400 --> 01:58:20.080
you know, like he's kind of like these concepts or whatever, it's an empirical question, and

01:58:20.080 --> 01:58:25.920
Shelley's, I think the art project, if it ever becomes this crowdsource thing, is going to give

01:58:25.920 --> 01:58:30.880
us lots of data to start thinking about this empirically, and it's going to be really fascinating.

01:58:30.880 --> 01:58:37.680
I mean, this needs to be on, like this is a, this is a prime blockchain project, because you can,

01:58:37.680 --> 01:58:43.840
you can probably, like you can probably even zero, you can zero knowledge prove that you can solve

01:58:44.880 --> 01:58:50.320
a given set of arc problems, right, you can probably create zero knowledge, so you wouldn't

01:58:50.320 --> 01:58:56.320
even have to show your solution, and if they're, you know, people would put up arc problems,

01:58:56.320 --> 01:59:02.400
and they, you know, if you want to try them, you will have to put up some money, and if you can

01:59:02.400 --> 01:59:08.240
solve it, you know, the creator of the challenge gives you some money or something like this,

01:59:08.240 --> 01:59:13.920
like this, this is going to be fascinating. Maybe you could do, you know, a homomorphic,

01:59:13.920 --> 01:59:18.160
like arc, right, or like you don't even, you somehow, like you're saying, you can just prove

01:59:18.160 --> 01:59:22.480
you can solve the problem without ever you having seen the problem, but just an encryption of it.

01:59:23.360 --> 01:59:27.920
Yeah. Yeah, no, normally homomorphic encryption comes after blockchain in the same sentence.

01:59:30.800 --> 01:59:35.920
And we make a nifty, we make a nifty of it. Yeah, what else can we get in there 10 weeks?

01:59:35.920 --> 01:59:41.520
So we got blockchain, homomorphic encryption, what else? What can we throw in there? Bitcoin,

01:59:41.520 --> 01:59:46.080
can't we just say people should have to pay through Bitcoin, if they, if somebody wins

01:59:46.080 --> 01:59:51.760
the challenge on ARC? We'll get our own token. ARC, ARC coin. Oh, God, hold on, I got to get that

01:59:51.760 --> 01:59:59.760
domain. I want to know, by the way, so the whole point of the arc, diversity of tasks for developer

01:59:59.760 --> 02:00:05.520
aware generalization, which means the developer could not have conceived of the task. But if all

02:00:05.520 --> 02:00:12.720
of the tasks are representing four human priors, then how is that developer aware of generalization?

02:00:12.800 --> 02:00:15.200
Because the developer would be aware of all of those priors.

02:00:16.000 --> 02:00:22.320
Of the priors, right? But not, not of the task, right? That's the control is the control, like

02:00:22.320 --> 02:00:27.760
that's what he said, you have to know the start of where your, your white box analyzing from. And

02:00:27.760 --> 02:00:33.600
the start is not clean slate, but the start here is these four priors. So it's, it's kind of the

02:00:33.600 --> 02:00:39.840
diff between you give the developer those four priors, what can the developer come up with,

02:00:39.840 --> 02:00:45.600
just from that, right? Yeah, because I think, I think there's a lot of information leakage there.

02:00:45.600 --> 02:00:51.520
And you implicitly said the same thing, because you said, once you solve it, you know, once you

02:00:51.520 --> 02:00:56.480
solve some of them, you've solved all of them. Okay, artcoin.com is available for the, it's,

02:00:56.480 --> 02:01:01.200
but it's, it's a premium domain. So it's 300 bucks. Should we get it? Because it has coin in it?

02:01:02.160 --> 02:01:08.880
I guess. We need to figure out something cooler.

02:01:08.880 --> 02:01:14.240
Like, no art coin. Okay. I don't care enough to grab it.

02:01:15.440 --> 02:01:18.240
Right. Anyway, we should draw this to a close, ladies and gentlemen. But yeah,

02:01:18.240 --> 02:01:23.040
thank you very much for listening. Yep. Thank you so much. It's been, it's been emotional.

02:01:23.040 --> 02:01:27.440
We've recently reached 10k subscribers actually. So yeah, thank you very much.

02:01:27.440 --> 02:01:30.800
We're still going to continue the show now that we've had Shaleo.

02:01:32.320 --> 02:01:35.920
Oh, yeah. I thought this was the end. I thought we were going to cap it with show. I mean,

02:01:35.920 --> 02:01:43.760
to be honest, we might as well just stop now. Anyway, see you folks. Thanks, Bob. Bye.

02:01:43.760 --> 02:01:48.640
I really hope you've enjoyed the episode today. Remember to like, comment and subscribe.

02:01:48.640 --> 02:01:52.960
We love reading your comments and we'll see you back next week.

