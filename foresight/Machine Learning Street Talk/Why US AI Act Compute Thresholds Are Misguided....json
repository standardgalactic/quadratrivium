{"text": " Um, Sarah, it's amazing to have you back on MLST. It's so lovely to be here. It's been a year and a half or something since our last conversation. Yes, it has. Yeah, because I think, um, we met at NeurIPS and then, and then I came in filming to be in the London office, which is really good. Um, but fans of the show, of course, will know that our first interview was about your hardware lottery paper. Yeah. And that was your first grumpy essay. That was a very grumpy essay. You know, I lead career for AI, so it's a research lab. We do a lot of fundamental research, uh, and we, a lot of my work is on efficiency, um, reliability, and building these models that scale the next generation models. So you can go to Co-Here for AI and take a look at some of our work. Sarah Hooker is VP of research at Co-Here, and she leads Co-Here for AI, a research lab, which seeks to solve complex machine learning problems. Co-Here for AI supports fundamental research, which explores the unknown. She leads a team of researchers and engineers working on making large language models more efficient, safe, and grounded. In this conversation, Sarah discusses her recent work on multilingual AI and the challenges of developing language models, which work across many different languages. She provides insights into the limitations of current approaches like RLHF, especially for low resource languages. Sarah also talks about her recent paper critiquing the use of compute thresholds as an AI governance strategy, explaining why simple measures like flops are inadequate for assessing AI capabilities and risks. Sarah emphasizes the importance of understanding the relationship between compute, data, and model architectures. She advocates for a more nuanced approach to AI development and governance, which considers the complexities of language, culture, and the representational long tail, where all the low frequency data lives, which is so often neglected in current models. Sarah's work aims to make AI more globally representative and equitable, as these technologies become increasingly integrated into society. Enjoy the show. Your most recent grumpy paper is called On the limitations of compute thresholds as a governance strategy. Can you give us the elevator pitch? So this paper is, it has a very boring title. And at face value, it's just about this kind of odd, known to not many people in the public, compute thresholds that have actually been widely adopted. They were adopted by the executive order on AI, they were adopted by the EU AI Act. And what's fascinating is that these are kind of the key policies that have come out on AI. Why did I write a paper about this very, very deep topic of compute thresholds? Because it's at the heart of really what our field is asking right now, which is that compute thresholds are based on an idea that models at a future size, so it doesn't apply to models in the well now, are going to trigger some difference in risk profile that deserves scrutiny. And this question of, does scale trigger this moment where models have these properties that are fundamentally different from models before that? It is actually very much being at the core of our field for the last two decades. Because in the last two decades, we've had this philosophy of bigger is better, we scale data and we scale model size. So this essay is really about, is that true? As we look and stand and look at the last decade, what do we know about the relationship between compute and risk? And what do we think is the feasibility of these compute thresholds actually mitigating risk? And that was the starting point. Yeah, so in the beginning, you were talking about how historically we have tried to estimate and control and respond to risk. Can you give us a couple of examples? Mostly as a society, we have tried to grapple with this idea that we want to proactively control our future for the better. And this is actually recent as well. So it's very typical of modern society that we have this notion of planning and anticipating risks and being able to mitigate. There's examples where me and you do this every day, right? We could put on sunscreen if we're knowing we're going to the sun. We avoid working in dark areas. There's also areas where governments have done this, you know, even in this modern era of the last 300, 400 years. And it requires two things to do well. One is that you have to understand where risk comes from. So you have to understand what is the kind of lever of risk. A good example of where that's failed is something like the Black Death, where for example, a lot of the protocols around the time didn't realize that rats were the main vector of the disease. And so because of that, many of the mitigation techniques were unsuccessful. But the second crucial aspect is that once you've identified the lever of risk, you have to form a proportionate response. And we also have examples where that's failed historically. So for example, the London fire is a great example where it was known that this was a risk, but the fail to curb it early on in the expansion of the fire led to the destruction of a large part of London. So these are the two challenges that policymakers face. And what compounds it for something like technology is that typically, the idea of identifying the lever of risk is very difficult, because most technology breakthroughs, by the nature of being a breakthrough, you're in a kind of rather than a proactive setting, you're in a retroactive setting. What do we do now that this is changing the world? And that's a very difficult position for someone to form a response to. Yes, exactly. I mean, you know, one of the themes of the paper is we're super bad at predicting the future. And maybe we should just linger just, you know, just for a second on the the executive order and the EU AI Act. Now, they used this notion called flops. And please explain what flops are in a second. But the in America, they set the limit to I think 10 to the 26. Is that right? And then in the EU, it was they wanted to be a little bit more strict. So they just went down to 25. Yeah. Tell me about that. So flops, by the way, is this measure by which this compute threshold is done? I think flops is just a it's a way of counting. So typically, when you train a model, you're doing many different operations, you're doing additions, subtraction, multiplication, famously matrix multiplies, dominate our modern networks. And so that can be decomposed into all these operations. So flops is just a tally, it just counts it up. And these thresholds, 10 to the 26 and 10 to the 25 are this idea that at that moment, that's when you kick in scrutiny. And it's important to realize that doesn't apply to models in the wild right now. For the executive order, for the EU AI Act, when it comes into effect next year, it might hit a handful of models. But this is a for looking policy. It is not based on current risk in the wild. And so that's interesting to think about, because that creates the question of, well, are we good at predicting what risks emerge? And is that the right number to do it at? And that's where it starts to get very interesting. Yeah, yes. So they have a tally, they kind of estimate how much computation is happening in the models. And then they've set this threshold. So they don't care about anything below that number. So there's lots of real risk now that they presumably don't care about. And then they're saying above this number, there's a problem. And I think did they set the number roughly commensurate to the size of a GPT-4 model? So it's difficult because they haven't formally justified why they set the number there. But anecdotally, my understanding is that guided it. So it's interesting. And it's worth thinking about, well, it's this interesting aspect of, well, firstly, there's a notion of, is this number valid tally of risk? Like, is training compute the number that you care about if you wanted to do this tally, if you believed in this future risk? But secondly, if are we good at predicting like that number? And that's kind of interesting to think about. Yeah, I mean, to me, it was a bit crazy on its face. And what's going through my mind is, have they got anyone working in the government that actually know what they're talking about? Because presumably, if they asked you, you would have thrown this thing out straight away. And you gave some examples, actually. So you said, there are things that have a normal distribution, like the weight of babies when they're born or blood pressure or certain things like that. And then there are other things that are significantly more complex, like if I'm buying a house, what does the estate agent do? Well, they have a complex model where they look at the neighborhood, they look at various different factors. Some people have indexes and they have things that can shift over time. So having this one absolutist number just seems a bit ridiculous. I actually think I feel for policymakers because I think it will put pressure on them to continually adapt the number. So I will say there were benefits in the thinking of this number. I think it's unfortunate it got so far without scientific input. But one reason that people like Flops is, for example, it's hardware agnostic, you can measure the same way across different types of hardware. And also, it's fairly easy to measure because all it's doing is a tally of operations. So it also avoids specifying maybe what risk you care about. So it gives a degree of, I would say, flexibility there for governments to adapt over time. I would say that is probably one of the larger shortcomings is that by not specifying, you can end up with something which is evading your Flops threshold but a highly risky model. So I think that's actually one of the crucial shortcomings. But I do understand that the motivation of a lot of policymakers are what else? Like what else could I use? I would argue if you're going to stick with this measure and it has been formalized in several policies, you have to understand that this can be manipulated as a measure. And there's many ways, and I list some out in the paper, but to your point, a single number puts a lot of pressure on policymakers to constantly adjust this and have the technical information to adjust it because this is a rapidly changing distribution. The notion of compute has been highly unstable. Just looking at the last decade, we know this. And so it will quickly have an expiration date. And I would argue what you're saying is excellent as an example. One is that you need a reference class of what are you comparing against? So you mentioned the kind of real estate agent who compares the pool of houses. Each of these domains, like biology models, which are very interesting to certain researchers because of bio risk, language models, multimodal models, they have different distributions of compute requirements. And so it has to be done relative to your reference class, but also it should be done dynamically. The same way that a real estate agent does it based on a percentile of surrounding houses, the notion of a single inflection point for risk is not a viable policy tool because you just are changing things all the time. Yes. I mean, there are so many things to get into here because you went through a wonderful list of examples in your paper. But one of the elephants in the room is that it supposes that there is some kind of linear commensurate relationship between compute and capabilities. And of course, you're working in multilingual. I mean, it actually penalizes you because you need to do more compute just to have a model that works at all in many different languages. And this thing just isn't working for you. Yeah. I mean, what you're pointing out is that once you do something like multilingual, you're basically trying to learn a new distribution each time that's as vast as English. And so you typically need a lot. It's called the curse of multilinguality. And so you need more compute. There's other things there which are very tricky is that how do you flops and how does training compute account for the vast amount of change and how we optimize after training? So we talked about RLHF. There's also instruction fine-tuning. There's also things like synthetic data distillation which shortens training times. So these are all what we call inference time optimization. So you spend time after training. You pay for it in compute. Like you can do best event sampling, which is what you refer to with the Francois Chollet where you sample a lot of completions and you choose the best. That all has very pronounced benefits for models. So typically your model performance alone, just using a subset of these techniques is two to six times more powerful. And that's not reflected in flops. Yes. Because I'm really interested in when you look at the model life cycle or the predictive life cycle, there are so many places where you can spend computation. So you can do dataset generation and you can do, obviously there's the training of the model and then you can do like inference time optimization and active inference and a whole bunch of stuff like that. And they are only taking into account the model training. But then there's the further issue of training provenance. So for example, I can download a model from Huggingface and I can fine tune it and do a bunch of stuff on it. And like, how do you know, right? It's just an inscrutable bunch of weights. Like you have no idea how much training has gone into it. This is the idea of tracing flops across the life cycle. I think this is also going to be formidable because increasingly the most popular models on Huggingface, by the way, are models which haven't been instruction fine-tuned. They're base models. And why? Because people want to do continued pre-training. They want to overlay their own optimization techniques, which suggests that people are using this as one step in their optimization process. It's going to be a formidable challenge to tally it in a reasonable way, especially when sometimes the way that we measure these, think about something like mixture of experts or a classic ensemble. What counts then? Because you may have many different experts in your mixture of experts, but you're only using two at the end. Classic ensembling is even more nuanced because technically you didn't even optimize all the models together. You just show up and you ensemble them at the end and you get one model at the end. So how do you handle that? It's very interesting and it's very related to this challenge of people are likely taking some level of compute already and they're doing some changes at the end of training that make it more performant. Yeah. And then there's this matter of Good Heart's Law, which is that when a target becomes a measure, it ceases to be a good measure. And there are so many examples of this. For example, the banks have these arbitrary limits on the amount of money that you can send, which is why it's set at, let's say, $10,000 and then you see loads and loads of transactions at like $9999 because they know what the limit is. And it must be the same here, right? It's going to gamify the system. People are going to evade it in so many ways. I think that the main advice I have about this is that if policy makers have decided on this and they're going to go for it, they need to complement it with an auxiliary measure of the actual risk that they care about. It has to be an index because if you just stick with compute, it is too easy to evade because there's too many different things you can do post training to gain percentage points. And there's too many ways of essentially shortening your training time or reducing your flops while still arriving at a highly performant model. And so that's the other key recommendation I have is that you need something that is anchored to the downstream risk you actually care about. And compute is not. It just reflects our belief that more compute is better. And that simply is too simplistic of you to account for all the ways in which smaller models, if they're very targeted, can be extremely risky. Yes. So I think maybe we should bring Rich Sutton in. So he wrote this essay called The Bitter Lesson. And I'll let you bring it in. But he was partially responsible for this idea that compute is all you need. Yeah, bring that in. Yeah. And by the way, I think that's a fantastic essay. It really is this idea that history tells us in computer science in particular that all efforts to codify our expertise, to work on very fancy ways of imparting what we think is the right way to learn to a model have been particularly futile. Like he's really saying we're not very good as computer scientists. And the biggest ingredient of success that's driven things is being adding compute to the mix. And that we can do things that are algorithmic, but it has to play well with compute. So anyway, he is kind of get this idea of hardware. But it's more general. It's this idea that it's not specific to a different sort of type of hardware. It's just compute. If you play well with compute, if it scales well, it's going to be the winning variant. And yeah, go for it. Well, I mean, I just I have an intuition that he's he's right and wrong at the same time. So I mean, in terms of system one models that just memorize things better and better, he's kind of right. Because there is a commensurate relationship, you know, as we memorize more of the long tail, the models get better and better. But I still think that there might be a fundamental break between compute and capabilities when and ironically, a regulation like this might incentivize to find such a break. So you know, we might design system two models that actually do reasoning and have a, you know, Neurosymbolic architecture or whatever. And now all of a sudden we've got like really good capabilities with less compute. Yeah, I mean, you're hitting on the head, I don't disagree with you. I think where I agree with Rich said it is that for given architecture, say, transformers, you can throw more compute at it up until a certain point where it's saturated, but you're going to see all the things equal, your data sets equal, compute is better because these are greedy learners, they, they're, you know, our deep neural networks are frequency counters, you're going to see gains on the long tail performance and overall gains. Where it misses the point is that really there's a few things going on. One is that because our current representations are so inefficient, there's ways to really change the algorithm itself and bend the, the rule of compute. So, and the rate at which compute is needed to unlock gains. And deep neural networks in particular are a great example of this because they're so painfully inefficient because we have to show all the data the same amount of times because we have to do these global updates. And so we're seeing all these tricks. For example, now we care about data again, and we care about data quality. So we condition that space better to represent what we want to model downstream. That means we have to train far less because all the features in the data set, the ones we want to learn was you just train on the internet. There's a lot you don't want to learn. So you have to kind of unlearn it afterwards and spend a lot of compute just trying to find what you want within that. So that's where you bend the rule and it becomes more nuanced, which is that the other thing that that misses is that, or at least that wasn't a core part of this essay, I actually think Rich might agree with me on this, is that your rate of compute is really determined more than anything by the prior of your algorithm. So yes, if your algorithm plays well with compute, if it's scalable, compute unlocks a lot. But the rate and the saturation point is determined by the algorithm. And what do we mean by this? Convolutional neural networks are a great example. Introduced in 2012, really unlocked scalability. Why? Because convolutional filters and patches made it possible to model high dimensional images at the time. Why? Because you move your patch over your image. This takes advantage of local relationships. You can really reduce your dimensionality. Max max pooling layers, which Jeffrey Hinton is famously grumpy about, and rightly so, just throws away everything except for the max. You reduce the amount of features, you unlock the ability to model images, you have scalability up to your point. This is what we famously know about image models. Now there's been a saturation point. Everyone who switched to transformers because there's a new arc. So what I mean by this is your algorithm is kind of your most heavy prior on your search space. And it's Richard's right that what plays well with compute is the one we're going to default to. But the question becomes your scaling laws and your ability to predict the future are essentially limited to algorithm and compute. And that's what's interesting is that it means we're not very good at predicting the future because it means we're too locked in to this narrow arc of this architecture we use combined with compute. Yeah. I mean, even the CNN example is, I think an example that proves Richard wrong, because he said in the bitter lesson that any attempt to impute hand-designed priors like symmetry or CNN is a symmetry. So in the CNN, it encodes a symmetry and scale invariance. And essentially, all it's doing is a shortcut because it's still in MLP at the end. So what it's doing is it's basically it's building an MLP as if you didn't have the scale and symmetry in there. But it's just kind of like doing this thing and it's like, you know, it's embedding it all into the MLP. But it's basically still an MLP with a symmetry shortcut, which was hand-designed. So yeah, yeah, but there's still this notion though that there's, you know, connectionists think that, I mean, like Neil Nanda said to me, he's like a rationalist guide from DeepMind and now, yeah, DeepMind. And he said, these things are just smarter than you, man. How did that make you feel? Well, I mean, not great, not great, but you know, there's this whole like mech-interp thing. And I think that they genuinely believe that there's just some deep form of inscrutable intelligence going on. There was that like monosomanticity paper from Anthropic recently. And you know, there's this deep belief that there's something really interesting going on. What do you think? I think that it is, there are persuasive, here's what I will say. Language is very powerful, which is why we connect and with this technology so much, because language is how we connect with each other emotionally. It's very tied to how we, as humans, are quite tribal. And so whenever a model learns a distribution that is indistinguishable from humans, I think that it gives pause. And it is, I do think that these conversations are useful because it gives worthy pause to how this technology is used. And for example, I'm very against many of the efforts to sometimes deploy these algorithms without notifying humans that it's an algorithm. I think that you should always be aware when you're talking to an algorithm. And that's because these are quite convincing sometimes. And so it's very important that we always communicate what is the role of the model and the human. Do I think that there's a higher reasoning here? I don't. I think that in many ways, whenever you have persuasive interpolation of a space between ideas, it's going to be surprising to us. I think what delights us is the creativity and the surprise and element. But is this ability to reason? I don't think so. I think that there is a clear relationship with the type of architecture we have of a memorization relationship. And we know this. We know that when we increase scale, we learn a given architecture. Or when we do different tricks to compensate for scale, so we can go smaller and still learn things, what we're really doing is we're just trying to induce good memorization and good steering towards part of the distribution we care about. Frankly, while why all these optimization tricks have worked beyond compute to reduce compute has been we're largely training on a distribution we don't want at the end of the day. So we start by training on the internet. And we actually don't want the internet when we engage with these models. We want something that's very chatty and philosophical and wise. And so a lot of what we're doing is we're trying to steer things towards the part the tiny sliver of the distribution that training data that we care about. And that's why we have so many optimization tricks before we get to the end. But that's fascinating because what's that that's really telling you is that unlike a traditional machine learning problem where you're training to the data set is the distribution you want to learn, a lot of what we're doing with language is we're unlearning. We're just trying to steer and unlearning nor and then focus on what we want. So it's really interesting. Yeah, machine unlearning. That's fascinating. I'm also a big fan of the externalist tradition in cognitive science, you know, like for recognition. And in that sense, I think it doesn't make sense to draw a boundary around the model, because I think, you know, our sense making semantics, situated knowledge and so on, it's kind of observe a relative anyway, you know, these things are embedded in our culture. And sense making humans put prompts in there and they interpret the outputs. And actually, even the data generating process that went into building these things was, you know, originated from the universe, we're all agents and we're all in the physical world and the social world. And we, you know, we're doing the effect of computation, both in how the models are built and how they are used and interpreted and evaluated and so on. So what are you going to do? Like draw a big, should you include the flops of the universe as well? Oh, that's fascinating. Yeah, there is this interesting, yeah, it does spark something else with flops, which is that, so typically the final model that you deliver is only one of the possible models, right? So in fact, typically, even at massive scale, you train many candidate models, and then you choose the best one. And so it's interesting because these are not optimized together, but they implicitly optimized through the selection process. And it is really interesting because we kind of steer towards what we want. So yeah, it's a fascinating dynamic. I didn't think of it like that, though, that's an even bigger meta approach for thinking about this. Hardware Lottery Paper, which we talked about, and that was a really fun conversation because I remember it was when you were doing the trio kind of the group of ML Street talk, the earlier version. And I think you originally invited me onto the show because there was this idea that I wrote about, which is that most of computer science history has been driven by whether your idea works with available hardware or not. And I think that resonated with a lot of people at the time, because what it's really saying is that we may be in another hardware lottery right now, that something like Transformers, which we all use, has become increasingly locked in to GPUs and to TPUs, which have all been built to accelerate this one hardware. So it raises the question of what next and how do we make sure that the next brilliant idea isn't stuck in purgatory for decades, because that's what happened to deep neural networks. It simply didn't work until TPUs were converted from video game use, which was really not the intended purpose of how they were converted to work for machine learning workloads. And that happened over the course of a decade. It was a very slow conversion process, but that turned out to be the key for deep neural networks. What we now identify as 2012, the moment that this explosion of interest and funding and acceleration happened. People identify that with convolutional neural networks or the algorithm, but really it was both. It was the hardware making the algorithm feasible. And that's when you first had the empirical proof that deep neural networks were viable. To what extent do you think there is an algorithm lottery as well? Oh, what do you mean by that? Well, as in now, your paper was about the basin of attraction of hardware. But is there a basin of attraction of algorithms as well? Absolutely. I mean, you just have to look at optimizers to see that. So what I mean by that is an algorithm is really how you learn from data. This is the essence of an algorithm. And what we've been locked into is this idea that it has to be gradient based optimization. It's really hard to do something that's a non-differentiable objective. And what that means kind of in accessible terms is that we're stuck doing these global updates. So the way our models train is that we kind of send through shovel through data, and then the update to the weights is based on an average of all the data that's seen. Why is that tricky for a few reasons? Because why does it mean that we overfit to the average? And that's why we need so much training data. Because essentially, if you're just overfitting to the average, it takes ages to learn the rare patterns. So you train for longer, you need more data. But the other thing that's very tricky is that it means that models forget. So every time you shovel in new data, the model forgets the old data because you're updating everything at once. A nice point of contrast is that as humans, we typically have long-term memory and short-term memory. These are different ways of learning, and the rate of learning is different. And so when you process information, some are stored in your long-term memory. You may have a distinctive memory from a child that you think is like your first memory from a child. And it may be mutated over time. That's the nature of memory. But this ability to preserve two states of what you did today, what you did years ago, that's very different from gradient updates. And somehow, because we haven't found an alternative way, even though a lot of people have worked on it, we are in this algorithm based on where it's very tricky to propose an algorithm that doesn't rely on a differentiable objective. Yes. And I think we'll talk about this later. But part of the problem is people think of this paradigm as a form of general abstract pure intelligence. And the reality is that certainly with your multilingual work, that we're dealing with just this long tail of complexity, heterogeneous data sets. But maybe that's a good segue because you just released this primer paper called the AI language gap. Can you tell us about that? So it's quite fun because in some ways, what you're talking about these themes leads so nice into the AI language gap. Really, when we have built these models, we've overfitted to what is weighted most importantly to those who built it. And these models have been built in a few places. We're in London. London is a very big hub of where researchers have been. So is the US and Europe and China. And because some of the first impressive large language models were built in the US and the UK with DeepMind, and in the US with places like Coher and places like OpenAI, I think that that has necessarily reflected the nature of the researchers who built them. They wanted to work in English. The tricky thing is that when you try and make AI actually work for the world, you're talking about this vast array of different languages. So there's 7,000 languages in the world. 80% of those have no text data. So it's truly not even a language problem. It's also a multimodal problem. The second part is that even with the top 101 languages, no models except for Io101 currently cover it. So there's this vast amount of the world that simply isn't reflected in the way that AI works and who AI serves. The primer about the language gap is really calling attention to this. But at the root of this problem and what you're getting at with this theme of how does models work with the long tail is that the fundamental issue is our models really overfit to high frequency patterns. And so the key difficulty with the language gap is that, one, these languages typically are underserved by available data on the internet. The internet kind of reflects early patterns of adoption, not necessarily humanity as it is. So that means that there's way more English on the internet than there is people who speak English. So 5% of homes speak English, but 50% of the internet is in English. In contrast, something like Yoruba, spoken by 40 million people, is really underserved. And so it's a long tail problem. But here's the other thing, it's a pattern where the rich get richer and the poor get poorer because we're now in a synthetic data era. So as models get much better at generating English and Chinese in particular, these are the two high resource languages that are well served, you're going to see more content in those two languages. And that makes it even harder if you're relying on large data to properly represent the languages that are currently underserved. Yeah, so interesting because we're moving away from the material world into the information world. And right now in the material world, there is a kind of a commensurate relationship between the number of people who speak English and the amount of data on the internet. And as you say, we're now moving to this place where we are generating data of language and the polarization is going to increase. So you're talking about there's this kind of North American tech based inequality, which is getting worse. And you said that there are safety implications for this. I was interested in this word safety, we spoke about this last night. Because when I think of AI safety, I think of X risk in Silicon Valley and stuff like that. And I've noticed over the years, the conflation of the two communities in terms of ethics and existential risk. And how do you feel about that? I think it's, I mean, I feel grumpy about that. But here's the thing. So, you know, subfields are always like this. I think there's always this notion of subfields, which are extremely, you know, actually people caring about the same objectives, trying to distinguish themselves over time. AI safety encompasses a large array of perspectives and expertise and people who care about different things. I think that this shift towards talking about from response for AI to AI safety is a fascinating one, because it's been a bit intentional from communities who want to maybe suggest that response for AI is distinct from what they're doing. And instead saying AI safety is about these profound risks, these like fundamental issues of our time. And response for AI is, okay, great, you're doing that, but keep going. And so I do think there's a very interesting thing with how we name things and how we really have precision in our conversations. Increasingly, I think AI safety encompasses both of these, and you need more precise language with both. And I actually think my main ask is, we need to be precise about what our objective is with AI safety. Because it can be, it is in many ways the same goals as response for AI. But the degree of precision when this is articulated is a sign of accountability for the objective. And I think sometimes the use of that word lacks accountability. Yes, exactly. And when I hear some ex-risk folks talk about AI, it feels to be in the abstract. And what I mean by that is they are just thinking about, if we scale this technology up, it learns these abstract representations, which work in any situation, and it's just a matter of scale. And it feels unmoored from the research, because when I read your work about multilingual models, you're clearly pointing out that when we have what they call low resource languages, the models don't work very well. They're not learning these abstractions that just automatically work in other languages. There's a specificity to it. That seems to be the difference to me. Yeah, there's this big question right now, what you're getting at is there's this idea of, there's a mystique that some people are attributing to scale. It's been called different things. It's this question of, are there emergent properties? Are there properties that appear from nowhere that we unlock with scale? By the way, multilingual is originally proposed as one of them. Like in the first paper about emergent properties, multilingual was there. It's like, wow, how did this appear? We didn't even have this in our training data. But it's very interesting. Now there's been subsequent work which is shown. It was there all along. It just wasn't documented in the training data. So scale is just really learning your long tail. It's learning the low frequency. We just get surprised because I think there's a big disconnect between what we think we know about the vast amounts of data that we train on and what's actually in that mix. And so there is often certain properties where it takes scale to unlock because it's very relate to this question of memorization. I think how this conversation has become a bigger theme beyond this scientific question of when do properties emerge and what to scale and lock, it's become this thing of kind of creating a myth around these models. That there's a lack of ability to understand what scale gives. And then that is used to kind of impart a degree of anxiety that because we don't know precisely when this property will emerge, there should be anxiety about this. And there should be a sense of real danger about the use of these models. And I would say that that is actually the wrong framing for this. The right framing is that one is the notion that we're just going to keep on scaling I think is flawed. I think there's very clear evidence that you know bigger is not always better that we're kind of reaching the limits of how we scale with something like transformers and it's very architecture bound. But the second thing that I would say is it really kind of ignores the mounting evidence that these kind of properties are surprising only because we're not going to predict in what emerges at scale. Yes, yes. I spoke with David Chalmers recently and he bemoans the fact that whenever we have a complex system, we say, oh, it's emergent. And there is something interesting going on as you say that when you memorize more and more of the long tail, you do see this qualitative increase in capabilities. And it's quite easy as an observer just to say, oh, you know, it's an emergent property. And people ascribe things like you know, divergent intentionality and reasoning and all of these kind of anthropomorphic qualities to the models even though they probably don't really exist. But one interesting thing though is that, you know, when you memorize all of these surface statistics at scale, you can use the language model as an idea generator. And like on Francois Chalet's arc challenge, you know, Ryan Greenblatt generated about 30,000 completions for all of the tasks. And the remarkable thing is in terms of sensitivity, the correct answer is in those completions. And then you can do some neuro symbolic evaluation and selection and you can pull the thing out, you know, so you can build an architecture that does really well. But I think people underestimate the amount of human selection kind of like smoothing out the brittleness. Yeah, well, right now, I agree, there's a huge amount of creativity that's unlocked for these models. So this iteration, and actually, by the way, this idea of like, you can create a lot of different options, and then you can verify which are correct, you see this in a lot of different kind of states of progress right now, that's how code is currently done, like you can create a really nice code data set by running code and seeing which one passed the test and kind of do formal verification of which ones passed. So it's not that these models are not capable of generating insensible answers is just that the probability on every single turn consistency is what you're putting out consistency is sometimes not there. And I also think part of what is beautiful from the creativity perspective, iteration of ideas is that sometimes you actually don't want consistency. So the objective may be different in different settings. So for example, for code, we always want code that passes. So that's a good example where we sample a lot just to get the subset. But sometimes I've talked to people who use it as a way to seed ideas or things like that. And actually there the diversity is the important part and gain very different responses each time. And so I think over time, we'll actually have different models for different things and be able to this is the core of the challenge of steerability of control, which right now is not good, frankly, like why do we have prompt engineering and why does everyone love it? Like this is a this is a symptom of a problem, not a symptom of a solution. The fact that we spend so much time prompt engineering the perfect thing to steer. So hopefully we have better tools in the future. But I see that as one key thing that will change is that we'll be able to steer towards the mode we want to use. Do we want consistency? Do we want exploration? And how does it fit into our iteration pattern? We won't spend too long on this because I asked everyone about this. But you know, where are the sources of creativity? So as we memorize more of the long tail, and the models can extrapolate, and the human prompters can, you know, mix novel combinations of things together. So there's this potential extrapolative space and whatnot that's how creative can they be? Yeah, I've been so one of the recent papers that we released was a paper about what we call active inheritance. It's this idea that we can start to steer how we sample data to sampling different parts of the distributions from different models. So so far the paradigm of like sampling data, either for human or for another model, has been very much like there's a single teacher, you're the student, or there's another student or your co creators with a single model. But if you think about it, one, that's a kind of passive inheritance, you're just trying a single prompt, you're not really kind of enforcing any criteria. Active inheritance is where you sample different parts of the problem you want to solve from a variety of different models. And that diversity actually spurs really interesting patterns where you increase the realm of what's possible and kind of spur higher quality that transcends the quality of any one model. And I see that as a very important step that we're building a lot of work on, including a multilingual, but also in this fundamental area of we actually used it to in the paper that we just released, we used it to steer towards non-differentiable objectives. So you know, going back to what you were talking about the algorithm basin, this idea, and I was saying everything is dependent on gradient descent, it's very hard to steer towards non-differentiable objectives. Before deep neural networks, there was decades of research on just these non-differentiable objectives. There are things like, how do you compute the perplexity of like a given, like, what is the reading grade level of a given sentence? So there's these scores that are kind of codified, but you can't really use it because they're not differentiable. And we actually show that you can use that as part of active inheritance where you steer towards models that are better at a reading grade level. And then you use that to kind of form your basis of your data set. So I think that's fascinating. And I think that's really going to spur creativity beyond just this more static notion of you just sample from a single teacher. Yeah, that's fascinating because there's so much of your research has been on the tyranny of forgetting the long tail or not paying attention to it. And of course, you can solve that with better optimization and, you know, federated learning and a gentile, you know, kind of multimodal systems that share information and query and almost like an adversary or setup. Yeah, it's a more dynamic pool. And so it's this idea that you can actually, and actually the long tail is a perfect example of where I find active inheritance most promising is that because the long pool, the long tail, you typically have many weak teachers. No one's very good at the long tail. But sampling effectively and doing this active inheritance rather than passive of just choosing a single teacher, but choosing a variety of teachers and then comparing and optimizing, this is fascinating. And I suspect it will benefit most the long tail. So you said in your language gap paper that language models are going to become integral to modern societies. How do you see that panning out? It's already happening in different ways. Like I would call it the high low way. So we can talk about high level themes, which is there'll be a ability to communicate much more easily. And so you'll just see much more proliferation of things like art or people writing or kind of taking away some of the difficult parts of how we communicate. I think the low way is just the more granular ways that you're using it right now, which is I use it typically for very basic things throughout my day. We write a lot of papers, so I'll do my citation reformatting using a language model. So there's both the mundane, but there's also the profound. I think the profound is that it changes the ease of communication. And so it changes the rate of inflammation flow. And this can be really powerful. It can mean that we can be more creative and experiment more the space. It can also bring new risks. And so I think this is also important to think about. Interesting. And you said that this North American bias in language model training, you said that it affects the design, the outputs and the behavior of the models. What did you mean by that? Well, there's two things. I mean, when I say design outputs and the behavior of the models, I think that there's optimization bias in the models itself against different languages. So tokenizes is a great example. So Roman scripts are things like French, Italian. We also have a Latin based scripts. This is also English. Whenever you deviate from Latin based scripts, you have something like Hindi, Korean, and these do not play well with tokenizers. So there's a lot of work which shows not only do tokenizers not work very well for these languages, but also it ends up being at this double tax because not only does the models perform worse, it also takes more tokens to represent these languages. So it's higher latency, higher cost for users outside of English to use APIs right now. So that's an example of like an optimization bias. The other, frankly, the issue is that whenever you're trying to have a model that represents many different parts of a distribution, typically our solution right now is we've got to give it more capacity. So I1 and 1 was an interesting example of this. We released I1 and 1. It represented 101 languages, and you can start to think about how many that is when you try and list more than 20. So you'll probably get to 10, and then you'll start struggling. And 101 is nuts. It includes things like we had Welsh, we had Irish, but we also had Telegu, we had many African languages, and we had very much these underrepresented like Haitian things, the variety and the complexity as well as dialect. So 101 is probably like preparing for the space race. It's like at the most extreme of the problem. And what's interesting is everything you learn there trickles down to less severe settings. But one of the things that we learned there is that you have to be very careful about how you use capacity because we had this 13 billion parameter model, and we were stuck with it because there was no pre-training data that covered 101. So this model was actually from 2019, which is crazy given how much has happened since then. But because of that, we were stuck with this model, and it meant that everything we had to do was try and make the best user capacity. We had to wait properly. We had to do data processing, data cleaning, but also we had to do a lot of work with synthetic data and the manipulation of how we did the optimization time. So you can do this two ways. We could have even increased it to 103 billion parameter model, and then we would have to retrain because right now models, unless they're trained with the day from the beginning, you can't just add it at the end. But also there's a secondary way, which is we get much more clever about the optimization and the data creation. And so this is really the issue is that when you go multilingual, all your problems in a given language are kind of multiplied out. And so you have to be very careful about all the details. I wonder what's the relationship between language and capabilities? And the reason I asked this is there was a great book I read called The Language Game by Morton Christensen and Nick Chater. And that very much led me to this idea of situated knowledge, I guess. So actually a lot of our cognition and thinking is quite specific to the culture and the language that we are in. And that seems to go against the grain of the idea that these things are learning general patterns of reasoning across languages. So then it rather kind of leads you to this conclusion that you actually need to be within the language and the culture in order to do the kind of thinking that they do inside that culture. So how does that work then when you're mixing all of these together into one language model? I think it doesn't work that well right now. So I would say this is like one of the core problems because you're precisely right. So we actually, so there's a few things I would say here. One is that we already see this with things like dialect. So the notion of dialect, which isn't really a counter for any models, including Aya, I think that we all go as mainly just to be the first next step in state of art. But even ours doesn't do this nuance of dialect. We do have various dialects of Arabic and some other dialects, but take something like Portuguese for example. Portuguese is spoken in many different places of the world. I spent part of my childhood in Mozambique. The Mozambique Portuguese is very different from, you know, I guess the most extreme would be Brazilian Portuguese. But also Portuguese in Portugal has its own nuances. And actually when we did Aya, we had researchers all over the world who were part of this project. And we would frequently have these little riffs between the Portuguese contributors in Brazil and the Portuguese contributors in Portugal because they were asked to review within a single pool. And so because the Brazilians outnumbered the Portuguese in Portugal, they would all correct their submissions to Brazilian Portuguese. This is a very interesting concept. And this is just on the notion of dialect. But your wider point is this idea that language is a tool for communication. And there's actually this very interesting concept about whether we even use language to think or if we use it as a utilitarian tool. Why is that relevant here? Because the way that we achieve an objective is going to depend upon where we are in the world. And the way that technology should serve us is going to depend on where we are with the world. This has come out recently. We just released a paper which I'm quite proud of, which is thinking about this idea of local versus global harms. At any one moment, we have multiple facets of our identity. So there's notions of what is insensitive to us as part of a notion of being global citizens. And that probably gets to things like there's a universal agreement that some types of harms, like harms to world children are particularly egregious. And most of, almost universally, our legal systems reflect this. But there's also notions of very particular harms which are cultural and very specific to how we live. And that is reflected in things like wording. So we just released this paper, which I think is important for safety, but also part of this broader move and in the field, which is that most of our models right now are trained with a single objective, a single decision boundary. What that means is all the data gets flattened to this one decision boundary. I'm very interested in multi-objective optimization. And this changes it so that you can hold multiple objectives at once. And that perhaps you can even adapt these objectives on the fly, which is very interesting. Yeah, a couple of things. I mean, you're talking, I guess, about the interplay between having a relativistic worldview and having some global norms. And in general, the way we do model alignment with our LHF and so on, it tends to de-complexify the reality of the world that we live in. And in ethical frameworks, there are deontology people who think there are just guiding principles and there are virtue ethics people who think there are certain virtues that we should emphasize. And there's consequentialism that there are certain consequences that are bad. And as you were just pointing to, it's very, very difficult to have a hybrid ethical framework that encapsulates all of these things together. What kind of work are people doing and what are you thinking about it? Well, we recently, the paper we just released is this really, this paper called, we call the Multilingual Prism, which is this idea that for safety, we collected both local examples of red teaming safety with really this very nuanced collection process across multiple languages, as well as harms that were considered global. From there, you can go into something like our LHF and you can change the notion of a single reward model. So this is an area I'm quite interested in. Like, how do you have multiple reward models? And then how do you balance them? This is the crux of the problem. And that's what you're getting at. So how do you have these two things in unison? And I suspect what we're going to see there is this notion of adaptation of our models in a more nimble way than previously. So typically, in a production setting, you spend months doing this model, you release it, cool, thumbs up, enjoy, and it's not as dynamic, but a true production model is refreshed and is more nimble and is deployed in different ways to different places. Like Netflix famously does this with its recommendation systems. I think here, this is actually a much more profound way of doing this because you can have these models, which essentially the way that they're steered is adapted. And this is both interesting as well as profoundly challenging because the tricky thing is, is you want to be sensitive to how the preferences of users change around the world, but you cannot overfit to too granular a preference because this is a philosophical tension you're actually getting at, which is that, you know, a libertarian view would say every person here has a list of preferences and those should be respected in their rank order. But as a society, we typically say we have this group of preferences, but we also adhere and kind of subsume some of our preferences for the common grid. And so there's this notion as well, when you articulate that as an algorithm, how do you get that balance somewhere in the middle, like where you are basically not adhering completely to a societal view. I think that's one of the concerns about algorithms and tokenizers being used in certain states where there's a state influence on how algorithms are deployed, but also not being used a total libertarian view where we don't want objectives that essentially amplify how a person thinks about the world without balancing and introducing different viewpoints. Yeah, it's so fascinating because, you know, even things like polarization on their face seem like an incredibly bad thing, but some kind of diversity preservation might actually lead to a pluralistic society that, you know, gains information and, you know, like a degree of health actually that we need. But with safetyism in general though, there's always this notion of, I think we probably agree on this a little bit, that if you leave people to their own devices, then that can be bad, but you also need a little bit of that because otherwise the society might become quite sclerotic. And these decisions presumably need to be baked into the way that we build these models in some way. Yeah, and currently then not. I would say currently the way that we approach safety, it's quite, we have this notion of refusals. So when you've engaged with a model, you typically will see refusals for certain what I would call the more black and white type cases. There's this really interesting opportunity I see in the evolution of how we think about safety, which is that instead of just saying I can't answer this to kind of provide more nuance or provide links to additional support. And I think that's very interesting because there's a different type of discussion. But I would say your perspective, what you're talking about, which is really this part in the middle where you have some values as like how you build your algorithm, but also you realize that this is someone who's engaging with an algorithm and like this is, you know, the algorithm should not reflect perfectly a single view of the world. You need more ways I also think within the UI for the person to influence and provide feedback and to something as course hallucinations is really interesting because hallucinations is not, you can't, I'm very skeptical we're going to eliminate hallucinations because they're also what we really like about these models. It's the creativity. So for me, this is not just, we often fixate a lot on the model in these conversations. The model has to solve this, but I think there's also a notion of the system. And I think that some things that will be interesting to play within the system is how does the user express when they think that steering isn't aligned with what they think is reasonable? A good example, this is for example a question about sexual health. There's valid reasons to ask those questions. There's valid reasons to want to understand like parts of your biology or things like that. Wikipedia has whole pages about sexual health. And so it's very interesting that a lot of systems refuse to answer this right now. So there's this nuance where we need to make sure that we are updating these binary decision boundaries where it's outright refusal and move towards something which is instead steering towards resources. Yeah, and I think so much of this is about when you fix something as it is now, it can go both ways as well. So maybe you can explain to the model, no in this situation I think you really should tell me. And likewise the model can say no, actually I think the reason I'm not allowing you to do this is because of this and maybe you should shift your viewpoint a little bit. It has to be done subtly because people don't like re-education. So that actually creates, they say the road to hell is paved with good intentions, it creates an equal and opposite reaction when you try to re-educate people. But coming on to RLHF, I mean we've spoken about this for years, you've always been a bit grumpy about RLHF and I read your paper, unfortunately I don't have an internet connection so I'm doing this from memory. Can you just remind me the multilingual paper that you've just released where you're trying to remove translation artifacts? Oh yes, RLHF speaks many languages. So this is a really nice paper, it was led by John. This idea that we were the first to extend a lot of the RLHF techniques from many different languages. So I think actually there's a wider view of RLHF and I have been grumpy about it, but you go first. Yeah, what were you going to... Well I mean even in this paper you were saying that, I mean obviously the broader conversation we've just had is that we need perhaps you know some kind of a more systems approach where we have a multitude of different models and optimizers and datasets and all that good stuff. But even within RLHF you are saying that it's hideously complex and inefficient and you have to have this separate reward model and it can't be optimized very well and you are saying that sometimes using DPO or even just basic reinforce is better. Yeah, so there's an excellent paper which we also recently released back to basics where we actually do a much more profound question of this, not even specific to multilingual, we take a step back. And we say okay, it's really interesting. All the kind of most cited papers originally on RLHF are papers which really take this canonical method within RL, PPO, and apply it to the language setting. And PPO really evolved in the RL, traditional RL space to address and mitigate a lot of the issues with traditional RL. RL is typically over a large expansive search space, incredibly noisy, and the trickiest part right is that your errors compound. So it's almost like thinking about well what if I bet incorrectly at a game table and then tried to bet again and did it incorrectly and your losses just compound the more that your estimates are off. So PPO is heavily what I would call kind of regularized or conditioned to limit the impact of an incorrect estimate. What that means is that often it's quite memory intensive, you kind of have four models and play at any one time, and it also means that it's quite sensitive. So typically PPO to train, it takes longer. And showed the language setting. And so the initial adoption of PPO and the success of it was taken at least value. This is incredible, let's go with this. But the language space is also an enormous search space because if you think about it, you're trying to predict the next token, how many tokens, how many possible tokens are there in the world to represent language. But by the time you have a trained model, and by the time you've done all this pre-training, the search space is much narrower. And actually it's quite interesting because the likelihood and the probability of what the next token will be is actually very concentrated. And when you have this pre-trained base, it's only going to be a few different tokens that you would likely predict, which means that this was overkill for the setting. And what we show convincingly and back to basics is that you can strip a lot, a lot of the components of PPO out. You can propose something like RLU, which is still an RL method, but that works effectively and even surpasses it. And RLU is also what we used in the RLHF speaks many languages. And we showed that this is very impactful. And because it's online, it does beat things like DPO, which is offline. So RLU is still an RL method. But what it's really saying is that we are in a well-conditioned search space. And because of that, we can be a lot more nimble about how we explore it. Yeah. Well, in the RLHF on many languages one, because obviously you've had this huge focus on multilingual. And I suppose there's the problem of getting diverse data, because this is super heterogeneous data when we're doing multilingual language training. And of course, even the preference completions, they needed to be generated as well. And I think you generated some with translations and then you had a strong model and you had a setup there. Can you tell us about that? That's fun, because it's part of this wider issue where multilingual relies a lot traditionally on translations. You don't have data, so you translate your good English data, your gold standard data, or your good Mandarin Chinese data into many different languages. Here is where it gets interesting. Translation models typically have what we call translation ease. There's these weird artifacts that pop up. So you might have like, odd enumeration where instead of like the one, two, three, it spells out one, two, three. So it's just, and it's very annoying for people who have to experience it because it gets imparted to the model, the downstream model. So we did something which I think is very fun with this paper where we said, well, the whole goal of RLHF is to steer away from certain parts of the distribution, steer towards other parts. And so what we did for our preference pairs, so let's think about the normal way preference pairs are done. It's quite expensive and time intensive. You have to go get annotators and you're asking humans which one do you prefer. We did this really fun, I would say, trick here where we said, well, we know we have translation pairs. We generate synthetic pair, the other pair, with what is a very high-performance model. In this case, we use Command R+, which is super-performance, does very well in many different languages. And then we compare the two and we ask an Alem is a judge, which is better, the translated English or the sampled in the other language. And what we found was this actually helped with translation artifacts because it steered the model away from the bad, translated and towards the more versatile, fluid Command R+, generation. So really interesting. And there were some percentage of time where the translated was better. And so you got that nuance too. So very, very interesting. Yeah, amazing. And then that removed a lot of the translation artifacts. Yeah. Amazing. Sarah, this has been incredible. Where would you like to point people to as well for your later stuff? Feel free. So, you know, I lead Co-Here4AI. So it's a research that we do a lot of fundamental research. And we, a lot of my work is on efficiency, reliability and building these models that scale the next generation models. So you can go to Co-Here4AI and take a look at some of our work and just a lovely being here again. It's really nice catching up. Amazing. Sarah, thank you so much. Yeah, thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.2, "text": " Um, Sarah, it's amazing to have you back on MLST.", "tokens": [50364, 3301, 11, 9519, 11, 309, 311, 2243, 281, 362, 291, 646, 322, 376, 19198, 51, 13, 50524], "temperature": 0.0, "avg_logprob": -0.21186378232894404, "compression_ratio": 1.6417445482866044, "no_speech_prob": 0.06936383992433548}, {"id": 1, "seek": 0, "start": 3.2, "end": 8.4, "text": " It's so lovely to be here. It's been a year and a half or something since our last conversation.", "tokens": [50524, 467, 311, 370, 7496, 281, 312, 510, 13, 467, 311, 668, 257, 1064, 293, 257, 1922, 420, 746, 1670, 527, 1036, 3761, 13, 50784], "temperature": 0.0, "avg_logprob": -0.21186378232894404, "compression_ratio": 1.6417445482866044, "no_speech_prob": 0.06936383992433548}, {"id": 2, "seek": 0, "start": 8.4, "end": 13.6, "text": " Yes, it has. Yeah, because I think, um, we met at NeurIPS and then, and then I came in", "tokens": [50784, 1079, 11, 309, 575, 13, 865, 11, 570, 286, 519, 11, 1105, 11, 321, 1131, 412, 1734, 374, 40, 6273, 293, 550, 11, 293, 550, 286, 1361, 294, 51044], "temperature": 0.0, "avg_logprob": -0.21186378232894404, "compression_ratio": 1.6417445482866044, "no_speech_prob": 0.06936383992433548}, {"id": 3, "seek": 0, "start": 13.6, "end": 17.44, "text": " filming to be in the London office, which is really good. Um, but fans of the show, of course,", "tokens": [51044, 8869, 281, 312, 294, 264, 7042, 3398, 11, 597, 307, 534, 665, 13, 3301, 11, 457, 4499, 295, 264, 855, 11, 295, 1164, 11, 51236], "temperature": 0.0, "avg_logprob": -0.21186378232894404, "compression_ratio": 1.6417445482866044, "no_speech_prob": 0.06936383992433548}, {"id": 4, "seek": 0, "start": 17.44, "end": 20.72, "text": " will know that our first interview was about your hardware lottery paper.", "tokens": [51236, 486, 458, 300, 527, 700, 4049, 390, 466, 428, 8837, 27391, 3035, 13, 51400], "temperature": 0.0, "avg_logprob": -0.21186378232894404, "compression_ratio": 1.6417445482866044, "no_speech_prob": 0.06936383992433548}, {"id": 5, "seek": 0, "start": 20.72, "end": 21.28, "text": " Yeah.", "tokens": [51400, 865, 13, 51428], "temperature": 0.0, "avg_logprob": -0.21186378232894404, "compression_ratio": 1.6417445482866044, "no_speech_prob": 0.06936383992433548}, {"id": 6, "seek": 0, "start": 21.28, "end": 23.52, "text": " And that was your first grumpy essay.", "tokens": [51428, 400, 300, 390, 428, 700, 677, 36142, 16238, 13, 51540], "temperature": 0.0, "avg_logprob": -0.21186378232894404, "compression_ratio": 1.6417445482866044, "no_speech_prob": 0.06936383992433548}, {"id": 7, "seek": 0, "start": 23.52, "end": 28.240000000000002, "text": " That was a very grumpy essay. You know, I lead career for AI, so it's a research", "tokens": [51540, 663, 390, 257, 588, 677, 36142, 16238, 13, 509, 458, 11, 286, 1477, 3988, 337, 7318, 11, 370, 309, 311, 257, 2132, 51776], "temperature": 0.0, "avg_logprob": -0.21186378232894404, "compression_ratio": 1.6417445482866044, "no_speech_prob": 0.06936383992433548}, {"id": 8, "seek": 2824, "start": 28.24, "end": 34.56, "text": " lab. We do a lot of fundamental research, uh, and we, a lot of my work is on efficiency, um,", "tokens": [50364, 2715, 13, 492, 360, 257, 688, 295, 8088, 2132, 11, 2232, 11, 293, 321, 11, 257, 688, 295, 452, 589, 307, 322, 10493, 11, 1105, 11, 50680], "temperature": 0.0, "avg_logprob": -0.13671281508037023, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.015870295464992523}, {"id": 9, "seek": 2824, "start": 34.56, "end": 37.92, "text": " reliability, and building these models that scale the next generation models.", "tokens": [50680, 24550, 11, 293, 2390, 613, 5245, 300, 4373, 264, 958, 5125, 5245, 13, 50848], "temperature": 0.0, "avg_logprob": -0.13671281508037023, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.015870295464992523}, {"id": 10, "seek": 2824, "start": 37.92, "end": 41.36, "text": " So you can go to Co-Here for AI and take a look at some of our work.", "tokens": [50848, 407, 291, 393, 352, 281, 3066, 12, 17685, 337, 7318, 293, 747, 257, 574, 412, 512, 295, 527, 589, 13, 51020], "temperature": 0.0, "avg_logprob": -0.13671281508037023, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.015870295464992523}, {"id": 11, "seek": 2824, "start": 42.16, "end": 48.239999999999995, "text": " Sarah Hooker is VP of research at Co-Here, and she leads Co-Here for AI, a research lab,", "tokens": [51060, 9519, 33132, 260, 307, 35812, 295, 2132, 412, 3066, 12, 17685, 11, 293, 750, 6689, 3066, 12, 17685, 337, 7318, 11, 257, 2132, 2715, 11, 51364], "temperature": 0.0, "avg_logprob": -0.13671281508037023, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.015870295464992523}, {"id": 12, "seek": 2824, "start": 48.239999999999995, "end": 53.519999999999996, "text": " which seeks to solve complex machine learning problems. Co-Here for AI supports fundamental", "tokens": [51364, 597, 28840, 281, 5039, 3997, 3479, 2539, 2740, 13, 3066, 12, 17685, 337, 7318, 9346, 8088, 51628], "temperature": 0.0, "avg_logprob": -0.13671281508037023, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.015870295464992523}, {"id": 13, "seek": 5352, "start": 53.52, "end": 59.52, "text": " research, which explores the unknown. She leads a team of researchers and engineers working on", "tokens": [50364, 2132, 11, 597, 45473, 264, 9841, 13, 1240, 6689, 257, 1469, 295, 10309, 293, 11955, 1364, 322, 50664], "temperature": 0.0, "avg_logprob": -0.05199213027954101, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.125543475151062}, {"id": 14, "seek": 5352, "start": 59.52, "end": 65.92, "text": " making large language models more efficient, safe, and grounded. In this conversation, Sarah", "tokens": [50664, 1455, 2416, 2856, 5245, 544, 7148, 11, 3273, 11, 293, 23535, 13, 682, 341, 3761, 11, 9519, 50984], "temperature": 0.0, "avg_logprob": -0.05199213027954101, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.125543475151062}, {"id": 15, "seek": 5352, "start": 65.92, "end": 71.2, "text": " discusses her recent work on multilingual AI and the challenges of developing language models,", "tokens": [50984, 2248, 279, 720, 5162, 589, 322, 2120, 38219, 7318, 293, 264, 4759, 295, 6416, 2856, 5245, 11, 51248], "temperature": 0.0, "avg_logprob": -0.05199213027954101, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.125543475151062}, {"id": 16, "seek": 5352, "start": 71.2, "end": 76.48, "text": " which work across many different languages. She provides insights into the limitations", "tokens": [51248, 597, 589, 2108, 867, 819, 8650, 13, 1240, 6417, 14310, 666, 264, 15705, 51512], "temperature": 0.0, "avg_logprob": -0.05199213027954101, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.125543475151062}, {"id": 17, "seek": 5352, "start": 76.48, "end": 82.72, "text": " of current approaches like RLHF, especially for low resource languages. Sarah also talks about", "tokens": [51512, 295, 2190, 11587, 411, 497, 43, 39, 37, 11, 2318, 337, 2295, 7684, 8650, 13, 9519, 611, 6686, 466, 51824], "temperature": 0.0, "avg_logprob": -0.05199213027954101, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.125543475151062}, {"id": 18, "seek": 8272, "start": 82.72, "end": 88.32, "text": " her recent paper critiquing the use of compute thresholds as an AI governance strategy,", "tokens": [50364, 720, 5162, 3035, 3113, 3221, 278, 264, 764, 295, 14722, 14678, 82, 382, 364, 7318, 17449, 5206, 11, 50644], "temperature": 0.0, "avg_logprob": -0.07578263680140178, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.0050559393130242825}, {"id": 19, "seek": 8272, "start": 88.32, "end": 94.24, "text": " explaining why simple measures like flops are inadequate for assessing AI capabilities and", "tokens": [50644, 13468, 983, 2199, 8000, 411, 932, 3370, 366, 42107, 337, 34348, 7318, 10862, 293, 50940], "temperature": 0.0, "avg_logprob": -0.07578263680140178, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.0050559393130242825}, {"id": 20, "seek": 8272, "start": 94.24, "end": 100.48, "text": " risks. Sarah emphasizes the importance of understanding the relationship between compute,", "tokens": [50940, 10888, 13, 9519, 48856, 264, 7379, 295, 3701, 264, 2480, 1296, 14722, 11, 51252], "temperature": 0.0, "avg_logprob": -0.07578263680140178, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.0050559393130242825}, {"id": 21, "seek": 8272, "start": 100.48, "end": 107.52, "text": " data, and model architectures. She advocates for a more nuanced approach to AI development", "tokens": [51252, 1412, 11, 293, 2316, 6331, 1303, 13, 1240, 25160, 337, 257, 544, 45115, 3109, 281, 7318, 3250, 51604], "temperature": 0.0, "avg_logprob": -0.07578263680140178, "compression_ratio": 1.5407725321888412, "no_speech_prob": 0.0050559393130242825}, {"id": 22, "seek": 10752, "start": 107.52, "end": 113.28, "text": " and governance, which considers the complexities of language, culture, and the representational", "tokens": [50364, 293, 17449, 11, 597, 33095, 264, 48705, 295, 2856, 11, 3713, 11, 293, 264, 2906, 1478, 50652], "temperature": 0.0, "avg_logprob": -0.08298185136583117, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.1963622272014618}, {"id": 23, "seek": 10752, "start": 113.28, "end": 119.11999999999999, "text": " long tail, where all the low frequency data lives, which is so often neglected in current models.", "tokens": [50652, 938, 6838, 11, 689, 439, 264, 2295, 7893, 1412, 2909, 11, 597, 307, 370, 2049, 32701, 294, 2190, 5245, 13, 50944], "temperature": 0.0, "avg_logprob": -0.08298185136583117, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.1963622272014618}, {"id": 24, "seek": 10752, "start": 120.0, "end": 125.6, "text": " Sarah's work aims to make AI more globally representative and equitable, as these technologies", "tokens": [50988, 9519, 311, 589, 24683, 281, 652, 7318, 544, 18958, 12424, 293, 33730, 11, 382, 613, 7943, 51268], "temperature": 0.0, "avg_logprob": -0.08298185136583117, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.1963622272014618}, {"id": 25, "seek": 10752, "start": 125.6, "end": 130.16, "text": " become increasingly integrated into society. Enjoy the show.", "tokens": [51268, 1813, 12980, 10919, 666, 4086, 13, 15411, 264, 855, 13, 51496], "temperature": 0.0, "avg_logprob": -0.08298185136583117, "compression_ratio": 1.5442477876106195, "no_speech_prob": 0.1963622272014618}, {"id": 26, "seek": 13016, "start": 130.16, "end": 139.04, "text": " Your most recent grumpy paper is called On the limitations of compute thresholds as a", "tokens": [50364, 2260, 881, 5162, 677, 36142, 3035, 307, 1219, 1282, 264, 15705, 295, 14722, 14678, 82, 382, 257, 50808], "temperature": 0.0, "avg_logprob": -0.15132541656494142, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.025423413142561913}, {"id": 27, "seek": 13016, "start": 139.04, "end": 142.48, "text": " governance strategy. Can you give us the elevator pitch?", "tokens": [50808, 17449, 5206, 13, 1664, 291, 976, 505, 264, 18782, 7293, 30, 50980], "temperature": 0.0, "avg_logprob": -0.15132541656494142, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.025423413142561913}, {"id": 28, "seek": 13016, "start": 143.76, "end": 153.76, "text": " So this paper is, it has a very boring title. And at face value, it's just about this kind of", "tokens": [51044, 407, 341, 3035, 307, 11, 309, 575, 257, 588, 9989, 4876, 13, 400, 412, 1851, 2158, 11, 309, 311, 445, 466, 341, 733, 295, 51544], "temperature": 0.0, "avg_logprob": -0.15132541656494142, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.025423413142561913}, {"id": 29, "seek": 13016, "start": 154.48, "end": 159.68, "text": " odd, known to not many people in the public, compute thresholds that have actually been", "tokens": [51580, 7401, 11, 2570, 281, 406, 867, 561, 294, 264, 1908, 11, 14722, 14678, 82, 300, 362, 767, 668, 51840], "temperature": 0.0, "avg_logprob": -0.15132541656494142, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.025423413142561913}, {"id": 30, "seek": 15968, "start": 159.68, "end": 165.36, "text": " widely adopted. They were adopted by the executive order on AI, they were adopted by the EU AI Act.", "tokens": [50364, 13371, 12175, 13, 814, 645, 12175, 538, 264, 10140, 1668, 322, 7318, 11, 436, 645, 12175, 538, 264, 10887, 7318, 3251, 13, 50648], "temperature": 0.0, "avg_logprob": -0.08578488032023111, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.009837549179792404}, {"id": 31, "seek": 15968, "start": 165.92000000000002, "end": 171.04000000000002, "text": " And what's fascinating is that these are kind of the key policies that have come out on AI.", "tokens": [50676, 400, 437, 311, 10343, 307, 300, 613, 366, 733, 295, 264, 2141, 7657, 300, 362, 808, 484, 322, 7318, 13, 50932], "temperature": 0.0, "avg_logprob": -0.08578488032023111, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.009837549179792404}, {"id": 32, "seek": 15968, "start": 172.56, "end": 178.32, "text": " Why did I write a paper about this very, very deep topic of compute thresholds?", "tokens": [51008, 1545, 630, 286, 2464, 257, 3035, 466, 341, 588, 11, 588, 2452, 4829, 295, 14722, 14678, 82, 30, 51296], "temperature": 0.0, "avg_logprob": -0.08578488032023111, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.009837549179792404}, {"id": 33, "seek": 15968, "start": 179.44, "end": 185.12, "text": " Because it's at the heart of really what our field is asking right now, which is that compute", "tokens": [51352, 1436, 309, 311, 412, 264, 1917, 295, 534, 437, 527, 2519, 307, 3365, 558, 586, 11, 597, 307, 300, 14722, 51636], "temperature": 0.0, "avg_logprob": -0.08578488032023111, "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.009837549179792404}, {"id": 34, "seek": 18512, "start": 185.12, "end": 192.56, "text": " thresholds are based on an idea that models at a future size, so it doesn't apply to models in the", "tokens": [50364, 14678, 82, 366, 2361, 322, 364, 1558, 300, 5245, 412, 257, 2027, 2744, 11, 370, 309, 1177, 380, 3079, 281, 5245, 294, 264, 50736], "temperature": 0.0, "avg_logprob": -0.10990173476082939, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.0078096892684698105}, {"id": 35, "seek": 18512, "start": 192.56, "end": 199.52, "text": " well now, are going to trigger some difference in risk profile that deserves scrutiny. And this", "tokens": [50736, 731, 586, 11, 366, 516, 281, 7875, 512, 2649, 294, 3148, 7964, 300, 17037, 38615, 13, 400, 341, 51084], "temperature": 0.0, "avg_logprob": -0.10990173476082939, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.0078096892684698105}, {"id": 36, "seek": 18512, "start": 199.52, "end": 208.0, "text": " question of, does scale trigger this moment where models have these properties that are", "tokens": [51084, 1168, 295, 11, 775, 4373, 7875, 341, 1623, 689, 5245, 362, 613, 7221, 300, 366, 51508], "temperature": 0.0, "avg_logprob": -0.10990173476082939, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.0078096892684698105}, {"id": 37, "seek": 18512, "start": 208.0, "end": 213.84, "text": " fundamentally different from models before that? It is actually very much being at the core of our", "tokens": [51508, 17879, 819, 490, 5245, 949, 300, 30, 467, 307, 767, 588, 709, 885, 412, 264, 4965, 295, 527, 51800], "temperature": 0.0, "avg_logprob": -0.10990173476082939, "compression_ratio": 1.6565217391304348, "no_speech_prob": 0.0078096892684698105}, {"id": 38, "seek": 21384, "start": 213.84, "end": 219.20000000000002, "text": " field for the last two decades. Because in the last two decades, we've had this philosophy of", "tokens": [50364, 2519, 337, 264, 1036, 732, 7878, 13, 1436, 294, 264, 1036, 732, 7878, 11, 321, 600, 632, 341, 10675, 295, 50632], "temperature": 0.0, "avg_logprob": -0.07570513239446676, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.0023155352100729942}, {"id": 39, "seek": 21384, "start": 219.20000000000002, "end": 225.36, "text": " bigger is better, we scale data and we scale model size. So this essay is really about,", "tokens": [50632, 3801, 307, 1101, 11, 321, 4373, 1412, 293, 321, 4373, 2316, 2744, 13, 407, 341, 16238, 307, 534, 466, 11, 50940], "temperature": 0.0, "avg_logprob": -0.07570513239446676, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.0023155352100729942}, {"id": 40, "seek": 21384, "start": 225.36, "end": 231.6, "text": " is that true? As we look and stand and look at the last decade, what do we know about the relationship", "tokens": [50940, 307, 300, 2074, 30, 1018, 321, 574, 293, 1463, 293, 574, 412, 264, 1036, 10378, 11, 437, 360, 321, 458, 466, 264, 2480, 51252], "temperature": 0.0, "avg_logprob": -0.07570513239446676, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.0023155352100729942}, {"id": 41, "seek": 21384, "start": 231.6, "end": 238.0, "text": " between compute and risk? And what do we think is the feasibility of these compute thresholds", "tokens": [51252, 1296, 14722, 293, 3148, 30, 400, 437, 360, 321, 519, 307, 264, 21781, 2841, 295, 613, 14722, 14678, 82, 51572], "temperature": 0.0, "avg_logprob": -0.07570513239446676, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.0023155352100729942}, {"id": 42, "seek": 21384, "start": 238.0, "end": 241.92000000000002, "text": " actually mitigating risk? And that was the starting point.", "tokens": [51572, 767, 15699, 990, 3148, 30, 400, 300, 390, 264, 2891, 935, 13, 51768], "temperature": 0.0, "avg_logprob": -0.07570513239446676, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.0023155352100729942}, {"id": 43, "seek": 24192, "start": 241.92, "end": 246.79999999999998, "text": " Yeah, so in the beginning, you were talking about how historically we have tried to estimate and", "tokens": [50364, 865, 11, 370, 294, 264, 2863, 11, 291, 645, 1417, 466, 577, 16180, 321, 362, 3031, 281, 12539, 293, 50608], "temperature": 0.0, "avg_logprob": -0.06704595202491397, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.0027499892748892307}, {"id": 44, "seek": 24192, "start": 246.79999999999998, "end": 251.92, "text": " control and respond to risk. Can you give us a couple of examples?", "tokens": [50608, 1969, 293, 4196, 281, 3148, 13, 1664, 291, 976, 505, 257, 1916, 295, 5110, 30, 50864], "temperature": 0.0, "avg_logprob": -0.06704595202491397, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.0027499892748892307}, {"id": 45, "seek": 24192, "start": 251.92, "end": 259.28, "text": " Mostly as a society, we have tried to grapple with this idea that we want to proactively control", "tokens": [50864, 29035, 382, 257, 4086, 11, 321, 362, 3031, 281, 27165, 306, 365, 341, 1558, 300, 321, 528, 281, 447, 45679, 1969, 51232], "temperature": 0.0, "avg_logprob": -0.06704595202491397, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.0027499892748892307}, {"id": 46, "seek": 24192, "start": 259.28, "end": 264.32, "text": " our future for the better. And this is actually recent as well. So it's very typical of modern", "tokens": [51232, 527, 2027, 337, 264, 1101, 13, 400, 341, 307, 767, 5162, 382, 731, 13, 407, 309, 311, 588, 7476, 295, 4363, 51484], "temperature": 0.0, "avg_logprob": -0.06704595202491397, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.0027499892748892307}, {"id": 47, "seek": 24192, "start": 264.32, "end": 270.32, "text": " society that we have this notion of planning and anticipating risks and being able to mitigate.", "tokens": [51484, 4086, 300, 321, 362, 341, 10710, 295, 5038, 293, 40568, 10888, 293, 885, 1075, 281, 27336, 13, 51784], "temperature": 0.0, "avg_logprob": -0.06704595202491397, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.0027499892748892307}, {"id": 48, "seek": 27032, "start": 270.32, "end": 277.04, "text": " There's examples where me and you do this every day, right? We could put on sunscreen if we're", "tokens": [50364, 821, 311, 5110, 689, 385, 293, 291, 360, 341, 633, 786, 11, 558, 30, 492, 727, 829, 322, 30304, 498, 321, 434, 50700], "temperature": 0.0, "avg_logprob": -0.09541323720192423, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.006553956773132086}, {"id": 49, "seek": 27032, "start": 277.04, "end": 283.28, "text": " knowing we're going to the sun. We avoid working in dark areas. There's also areas where governments", "tokens": [50700, 5276, 321, 434, 516, 281, 264, 3295, 13, 492, 5042, 1364, 294, 2877, 3179, 13, 821, 311, 611, 3179, 689, 11280, 51012], "temperature": 0.0, "avg_logprob": -0.09541323720192423, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.006553956773132086}, {"id": 50, "seek": 27032, "start": 283.28, "end": 292.0, "text": " have done this, you know, even in this modern era of the last 300, 400 years. And it requires two", "tokens": [51012, 362, 1096, 341, 11, 291, 458, 11, 754, 294, 341, 4363, 4249, 295, 264, 1036, 6641, 11, 8423, 924, 13, 400, 309, 7029, 732, 51448], "temperature": 0.0, "avg_logprob": -0.09541323720192423, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.006553956773132086}, {"id": 51, "seek": 27032, "start": 292.0, "end": 296.48, "text": " things to do well. One is that you have to understand where risk comes from. So you have to", "tokens": [51448, 721, 281, 360, 731, 13, 1485, 307, 300, 291, 362, 281, 1223, 689, 3148, 1487, 490, 13, 407, 291, 362, 281, 51672], "temperature": 0.0, "avg_logprob": -0.09541323720192423, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.006553956773132086}, {"id": 52, "seek": 29648, "start": 296.48, "end": 303.36, "text": " understand what is the kind of lever of risk. A good example of where that's failed is something", "tokens": [50364, 1223, 437, 307, 264, 733, 295, 12451, 295, 3148, 13, 316, 665, 1365, 295, 689, 300, 311, 7612, 307, 746, 50708], "temperature": 0.0, "avg_logprob": -0.05921788051210601, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.003700326196849346}, {"id": 53, "seek": 29648, "start": 303.36, "end": 310.08000000000004, "text": " like the Black Death, where for example, a lot of the protocols around the time didn't realize", "tokens": [50708, 411, 264, 4076, 13703, 11, 689, 337, 1365, 11, 257, 688, 295, 264, 20618, 926, 264, 565, 994, 380, 4325, 51044], "temperature": 0.0, "avg_logprob": -0.05921788051210601, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.003700326196849346}, {"id": 54, "seek": 29648, "start": 310.08000000000004, "end": 314.64000000000004, "text": " that rats were the main vector of the disease. And so because of that, many of the mitigation", "tokens": [51044, 300, 25691, 645, 264, 2135, 8062, 295, 264, 4752, 13, 400, 370, 570, 295, 300, 11, 867, 295, 264, 32649, 51272], "temperature": 0.0, "avg_logprob": -0.05921788051210601, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.003700326196849346}, {"id": 55, "seek": 29648, "start": 314.64000000000004, "end": 320.40000000000003, "text": " techniques were unsuccessful. But the second crucial aspect is that once you've identified the lever", "tokens": [51272, 7512, 645, 46258, 13, 583, 264, 1150, 11462, 4171, 307, 300, 1564, 291, 600, 9234, 264, 12451, 51560], "temperature": 0.0, "avg_logprob": -0.05921788051210601, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.003700326196849346}, {"id": 56, "seek": 32040, "start": 320.4, "end": 326.79999999999995, "text": " of risk, you have to form a proportionate response. And we also have examples where that's failed", "tokens": [50364, 295, 3148, 11, 291, 362, 281, 1254, 257, 16068, 473, 4134, 13, 400, 321, 611, 362, 5110, 689, 300, 311, 7612, 50684], "temperature": 0.0, "avg_logprob": -0.06481298509534898, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.19382187724113464}, {"id": 57, "seek": 32040, "start": 326.79999999999995, "end": 332.88, "text": " historically. So for example, the London fire is a great example where it was known that this was a", "tokens": [50684, 16180, 13, 407, 337, 1365, 11, 264, 7042, 2610, 307, 257, 869, 1365, 689, 309, 390, 2570, 300, 341, 390, 257, 50988], "temperature": 0.0, "avg_logprob": -0.06481298509534898, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.19382187724113464}, {"id": 58, "seek": 32040, "start": 332.88, "end": 338.15999999999997, "text": " risk, but the fail to curb it early on in the expansion of the fire led to the destruction of", "tokens": [50988, 3148, 11, 457, 264, 3061, 281, 33731, 309, 2440, 322, 294, 264, 11260, 295, 264, 2610, 4684, 281, 264, 13563, 295, 51252], "temperature": 0.0, "avg_logprob": -0.06481298509534898, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.19382187724113464}, {"id": 59, "seek": 32040, "start": 338.15999999999997, "end": 344.08, "text": " a large part of London. So these are the two challenges that policymakers face. And what", "tokens": [51252, 257, 2416, 644, 295, 7042, 13, 407, 613, 366, 264, 732, 4759, 300, 47325, 1851, 13, 400, 437, 51548], "temperature": 0.0, "avg_logprob": -0.06481298509534898, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.19382187724113464}, {"id": 60, "seek": 34408, "start": 344.15999999999997, "end": 350.0, "text": " compounds it for something like technology is that typically, the idea of identifying the", "tokens": [50368, 21810, 309, 337, 746, 411, 2899, 307, 300, 5850, 11, 264, 1558, 295, 16696, 264, 50660], "temperature": 0.0, "avg_logprob": -0.09671410807856808, "compression_ratio": 1.772549019607843, "no_speech_prob": 0.01912197843194008}, {"id": 61, "seek": 34408, "start": 350.0, "end": 355.44, "text": " lever of risk is very difficult, because most technology breakthroughs, by the nature of", "tokens": [50660, 12451, 295, 3148, 307, 588, 2252, 11, 570, 881, 2899, 22397, 82, 11, 538, 264, 3687, 295, 50932], "temperature": 0.0, "avg_logprob": -0.09671410807856808, "compression_ratio": 1.772549019607843, "no_speech_prob": 0.01912197843194008}, {"id": 62, "seek": 34408, "start": 355.44, "end": 361.68, "text": " being a breakthrough, you're in a kind of rather than a proactive setting, you're in a retroactive", "tokens": [50932, 885, 257, 22397, 11, 291, 434, 294, 257, 733, 295, 2831, 813, 257, 28028, 3287, 11, 291, 434, 294, 257, 18820, 12596, 51244], "temperature": 0.0, "avg_logprob": -0.09671410807856808, "compression_ratio": 1.772549019607843, "no_speech_prob": 0.01912197843194008}, {"id": 63, "seek": 34408, "start": 361.68, "end": 366.4, "text": " setting. What do we do now that this is changing the world? And that's a very difficult position", "tokens": [51244, 3287, 13, 708, 360, 321, 360, 586, 300, 341, 307, 4473, 264, 1002, 30, 400, 300, 311, 257, 588, 2252, 2535, 51480], "temperature": 0.0, "avg_logprob": -0.09671410807856808, "compression_ratio": 1.772549019607843, "no_speech_prob": 0.01912197843194008}, {"id": 64, "seek": 34408, "start": 366.4, "end": 372.24, "text": " for someone to form a response to. Yes, exactly. I mean, you know, one of the", "tokens": [51480, 337, 1580, 281, 1254, 257, 4134, 281, 13, 1079, 11, 2293, 13, 286, 914, 11, 291, 458, 11, 472, 295, 264, 51772], "temperature": 0.0, "avg_logprob": -0.09671410807856808, "compression_ratio": 1.772549019607843, "no_speech_prob": 0.01912197843194008}, {"id": 65, "seek": 37224, "start": 372.24, "end": 378.08, "text": " themes of the paper is we're super bad at predicting the future. And maybe we should just", "tokens": [50364, 13544, 295, 264, 3035, 307, 321, 434, 1687, 1578, 412, 32884, 264, 2027, 13, 400, 1310, 321, 820, 445, 50656], "temperature": 0.0, "avg_logprob": -0.12231031258900961, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.023885969072580338}, {"id": 66, "seek": 37224, "start": 378.08, "end": 383.6, "text": " linger just, you know, just for a second on the the executive order and the EU AI Act. Now,", "tokens": [50656, 45657, 445, 11, 291, 458, 11, 445, 337, 257, 1150, 322, 264, 264, 10140, 1668, 293, 264, 10887, 7318, 3251, 13, 823, 11, 50932], "temperature": 0.0, "avg_logprob": -0.12231031258900961, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.023885969072580338}, {"id": 67, "seek": 37224, "start": 383.6, "end": 390.16, "text": " they used this notion called flops. And please explain what flops are in a second. But the", "tokens": [50932, 436, 1143, 341, 10710, 1219, 932, 3370, 13, 400, 1767, 2903, 437, 932, 3370, 366, 294, 257, 1150, 13, 583, 264, 51260], "temperature": 0.0, "avg_logprob": -0.12231031258900961, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.023885969072580338}, {"id": 68, "seek": 37224, "start": 390.72, "end": 395.44, "text": " in America, they set the limit to I think 10 to the 26. Is that right? And then in the EU,", "tokens": [51288, 294, 3374, 11, 436, 992, 264, 4948, 281, 286, 519, 1266, 281, 264, 7551, 13, 1119, 300, 558, 30, 400, 550, 294, 264, 10887, 11, 51524], "temperature": 0.0, "avg_logprob": -0.12231031258900961, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.023885969072580338}, {"id": 69, "seek": 37224, "start": 395.44, "end": 400.16, "text": " it was they wanted to be a little bit more strict. So they just went down to 25.", "tokens": [51524, 309, 390, 436, 1415, 281, 312, 257, 707, 857, 544, 10910, 13, 407, 436, 445, 1437, 760, 281, 3552, 13, 51760], "temperature": 0.0, "avg_logprob": -0.12231031258900961, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.023885969072580338}, {"id": 70, "seek": 40016, "start": 400.40000000000003, "end": 406.96000000000004, "text": " Yeah. Tell me about that. So flops, by the way, is this measure by which this compute threshold", "tokens": [50376, 865, 13, 5115, 385, 466, 300, 13, 407, 932, 3370, 11, 538, 264, 636, 11, 307, 341, 3481, 538, 597, 341, 14722, 14678, 50704], "temperature": 0.0, "avg_logprob": -0.11968048413594563, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0016959590138867497}, {"id": 71, "seek": 40016, "start": 406.96000000000004, "end": 411.84000000000003, "text": " is done? I think flops is just a it's a way of counting. So typically, when you train a model,", "tokens": [50704, 307, 1096, 30, 286, 519, 932, 3370, 307, 445, 257, 309, 311, 257, 636, 295, 13251, 13, 407, 5850, 11, 562, 291, 3847, 257, 2316, 11, 50948], "temperature": 0.0, "avg_logprob": -0.11968048413594563, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0016959590138867497}, {"id": 72, "seek": 40016, "start": 411.84000000000003, "end": 416.24, "text": " you're doing many different operations, you're doing additions, subtraction, multiplication,", "tokens": [50948, 291, 434, 884, 867, 819, 7705, 11, 291, 434, 884, 35113, 11, 16390, 313, 11, 27290, 11, 51168], "temperature": 0.0, "avg_logprob": -0.11968048413594563, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0016959590138867497}, {"id": 73, "seek": 40016, "start": 416.24, "end": 421.36, "text": " famously matrix multiplies, dominate our modern networks. And so that can be decomposed into", "tokens": [51168, 34360, 8141, 12788, 530, 11, 28246, 527, 4363, 9590, 13, 400, 370, 300, 393, 312, 22867, 1744, 666, 51424], "temperature": 0.0, "avg_logprob": -0.11968048413594563, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0016959590138867497}, {"id": 74, "seek": 40016, "start": 421.36, "end": 427.12, "text": " all these operations. So flops is just a tally, it just counts it up. And these thresholds,", "tokens": [51424, 439, 613, 7705, 13, 407, 932, 3370, 307, 445, 257, 256, 379, 11, 309, 445, 14893, 309, 493, 13, 400, 613, 14678, 82, 11, 51712], "temperature": 0.0, "avg_logprob": -0.11968048413594563, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0016959590138867497}, {"id": 75, "seek": 42712, "start": 428.08, "end": 434.48, "text": " 10 to the 26 and 10 to the 25 are this idea that at that moment, that's when you kick in scrutiny.", "tokens": [50412, 1266, 281, 264, 7551, 293, 1266, 281, 264, 3552, 366, 341, 1558, 300, 412, 300, 1623, 11, 300, 311, 562, 291, 4437, 294, 38615, 13, 50732], "temperature": 0.0, "avg_logprob": -0.09915466797657502, "compression_ratio": 1.6156583629893237, "no_speech_prob": 0.00581575371325016}, {"id": 76, "seek": 42712, "start": 434.48, "end": 438.8, "text": " And it's important to realize that doesn't apply to models in the wild right now.", "tokens": [50732, 400, 309, 311, 1021, 281, 4325, 300, 1177, 380, 3079, 281, 5245, 294, 264, 4868, 558, 586, 13, 50948], "temperature": 0.0, "avg_logprob": -0.09915466797657502, "compression_ratio": 1.6156583629893237, "no_speech_prob": 0.00581575371325016}, {"id": 77, "seek": 42712, "start": 439.44, "end": 443.36, "text": " For the executive order, for the EU AI Act, when it comes into effect next year,", "tokens": [50980, 1171, 264, 10140, 1668, 11, 337, 264, 10887, 7318, 3251, 11, 562, 309, 1487, 666, 1802, 958, 1064, 11, 51176], "temperature": 0.0, "avg_logprob": -0.09915466797657502, "compression_ratio": 1.6156583629893237, "no_speech_prob": 0.00581575371325016}, {"id": 78, "seek": 42712, "start": 443.36, "end": 449.36, "text": " it might hit a handful of models. But this is a for looking policy. It is not based on current", "tokens": [51176, 309, 1062, 2045, 257, 16458, 295, 5245, 13, 583, 341, 307, 257, 337, 1237, 3897, 13, 467, 307, 406, 2361, 322, 2190, 51476], "temperature": 0.0, "avg_logprob": -0.09915466797657502, "compression_ratio": 1.6156583629893237, "no_speech_prob": 0.00581575371325016}, {"id": 79, "seek": 42712, "start": 449.36, "end": 454.08, "text": " risk in the wild. And so that's interesting to think about, because that creates the question of,", "tokens": [51476, 3148, 294, 264, 4868, 13, 400, 370, 300, 311, 1880, 281, 519, 466, 11, 570, 300, 7829, 264, 1168, 295, 11, 51712], "temperature": 0.0, "avg_logprob": -0.09915466797657502, "compression_ratio": 1.6156583629893237, "no_speech_prob": 0.00581575371325016}, {"id": 80, "seek": 45408, "start": 454.08, "end": 458.32, "text": " well, are we good at predicting what risks emerge? And is that the right number to do it at? And", "tokens": [50364, 731, 11, 366, 321, 665, 412, 32884, 437, 10888, 21511, 30, 400, 307, 300, 264, 558, 1230, 281, 360, 309, 412, 30, 400, 50576], "temperature": 0.0, "avg_logprob": -0.10143431027730306, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.001959409099072218}, {"id": 81, "seek": 45408, "start": 458.32, "end": 465.12, "text": " that's where it starts to get very interesting. Yeah, yes. So they have a tally, they kind of", "tokens": [50576, 300, 311, 689, 309, 3719, 281, 483, 588, 1880, 13, 865, 11, 2086, 13, 407, 436, 362, 257, 256, 379, 11, 436, 733, 295, 50916], "temperature": 0.0, "avg_logprob": -0.10143431027730306, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.001959409099072218}, {"id": 82, "seek": 45408, "start": 465.12, "end": 470.47999999999996, "text": " estimate how much computation is happening in the models. And then they've set this threshold. So", "tokens": [50916, 12539, 577, 709, 24903, 307, 2737, 294, 264, 5245, 13, 400, 550, 436, 600, 992, 341, 14678, 13, 407, 51184], "temperature": 0.0, "avg_logprob": -0.10143431027730306, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.001959409099072218}, {"id": 83, "seek": 45408, "start": 470.47999999999996, "end": 474.96, "text": " they don't care about anything below that number. So there's lots of real risk now that they", "tokens": [51184, 436, 500, 380, 1127, 466, 1340, 2507, 300, 1230, 13, 407, 456, 311, 3195, 295, 957, 3148, 586, 300, 436, 51408], "temperature": 0.0, "avg_logprob": -0.10143431027730306, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.001959409099072218}, {"id": 84, "seek": 45408, "start": 474.96, "end": 479.36, "text": " presumably don't care about. And then they're saying above this number, there's a problem. And I", "tokens": [51408, 26742, 500, 380, 1127, 466, 13, 400, 550, 436, 434, 1566, 3673, 341, 1230, 11, 456, 311, 257, 1154, 13, 400, 286, 51628], "temperature": 0.0, "avg_logprob": -0.10143431027730306, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.001959409099072218}, {"id": 85, "seek": 47936, "start": 479.36, "end": 483.76, "text": " think did they set the number roughly commensurate to the size of a GPT-4 model?", "tokens": [50364, 519, 630, 436, 992, 264, 1230, 9810, 800, 694, 33144, 281, 264, 2744, 295, 257, 26039, 51, 12, 19, 2316, 30, 50584], "temperature": 0.0, "avg_logprob": -0.09570431202015978, "compression_ratio": 1.625, "no_speech_prob": 0.0036875184159725904}, {"id": 86, "seek": 47936, "start": 483.76, "end": 490.24, "text": " So it's difficult because they haven't formally justified why they set the number there. But", "tokens": [50584, 407, 309, 311, 2252, 570, 436, 2378, 380, 25983, 27808, 983, 436, 992, 264, 1230, 456, 13, 583, 50908], "temperature": 0.0, "avg_logprob": -0.09570431202015978, "compression_ratio": 1.625, "no_speech_prob": 0.0036875184159725904}, {"id": 87, "seek": 47936, "start": 491.12, "end": 497.44, "text": " anecdotally, my understanding is that guided it. So it's interesting. And it's worth thinking about,", "tokens": [50952, 26652, 310, 379, 11, 452, 3701, 307, 300, 19663, 309, 13, 407, 309, 311, 1880, 13, 400, 309, 311, 3163, 1953, 466, 11, 51268], "temperature": 0.0, "avg_logprob": -0.09570431202015978, "compression_ratio": 1.625, "no_speech_prob": 0.0036875184159725904}, {"id": 88, "seek": 47936, "start": 498.64, "end": 504.8, "text": " well, it's this interesting aspect of, well, firstly, there's a notion of, is this number", "tokens": [51328, 731, 11, 309, 311, 341, 1880, 4171, 295, 11, 731, 11, 27376, 11, 456, 311, 257, 10710, 295, 11, 307, 341, 1230, 51636], "temperature": 0.0, "avg_logprob": -0.09570431202015978, "compression_ratio": 1.625, "no_speech_prob": 0.0036875184159725904}, {"id": 89, "seek": 50480, "start": 504.88, "end": 510.72, "text": " valid tally of risk? Like, is training compute the number that you care about if you wanted to", "tokens": [50368, 7363, 256, 379, 295, 3148, 30, 1743, 11, 307, 3097, 14722, 264, 1230, 300, 291, 1127, 466, 498, 291, 1415, 281, 50660], "temperature": 0.0, "avg_logprob": -0.07434375286102295, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0061194621957838535}, {"id": 90, "seek": 50480, "start": 510.72, "end": 517.84, "text": " do this tally, if you believed in this future risk? But secondly, if are we good at predicting", "tokens": [50660, 360, 341, 256, 379, 11, 498, 291, 7847, 294, 341, 2027, 3148, 30, 583, 26246, 11, 498, 366, 321, 665, 412, 32884, 51016], "temperature": 0.0, "avg_logprob": -0.07434375286102295, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0061194621957838535}, {"id": 91, "seek": 50480, "start": 517.84, "end": 522.5600000000001, "text": " like that number? And that's kind of interesting to think about. Yeah, I mean, to me, it was", "tokens": [51016, 411, 300, 1230, 30, 400, 300, 311, 733, 295, 1880, 281, 519, 466, 13, 865, 11, 286, 914, 11, 281, 385, 11, 309, 390, 51252], "temperature": 0.0, "avg_logprob": -0.07434375286102295, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0061194621957838535}, {"id": 92, "seek": 50480, "start": 522.5600000000001, "end": 528.8, "text": " a bit crazy on its face. And what's going through my mind is, have they got anyone working in the", "tokens": [51252, 257, 857, 3219, 322, 1080, 1851, 13, 400, 437, 311, 516, 807, 452, 1575, 307, 11, 362, 436, 658, 2878, 1364, 294, 264, 51564], "temperature": 0.0, "avg_logprob": -0.07434375286102295, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0061194621957838535}, {"id": 93, "seek": 50480, "start": 528.8, "end": 532.96, "text": " government that actually know what they're talking about? Because presumably, if they asked you,", "tokens": [51564, 2463, 300, 767, 458, 437, 436, 434, 1417, 466, 30, 1436, 26742, 11, 498, 436, 2351, 291, 11, 51772], "temperature": 0.0, "avg_logprob": -0.07434375286102295, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0061194621957838535}, {"id": 94, "seek": 53296, "start": 532.96, "end": 537.44, "text": " you would have thrown this thing out straight away. And you gave some examples, actually. So you", "tokens": [50364, 291, 576, 362, 11732, 341, 551, 484, 2997, 1314, 13, 400, 291, 2729, 512, 5110, 11, 767, 13, 407, 291, 50588], "temperature": 0.0, "avg_logprob": -0.09385548371535081, "compression_ratio": 1.7955974842767295, "no_speech_prob": 0.01151969563215971}, {"id": 95, "seek": 53296, "start": 537.44, "end": 541.84, "text": " said, there are things that have a normal distribution, like the weight of babies when", "tokens": [50588, 848, 11, 456, 366, 721, 300, 362, 257, 2710, 7316, 11, 411, 264, 3364, 295, 10917, 562, 50808], "temperature": 0.0, "avg_logprob": -0.09385548371535081, "compression_ratio": 1.7955974842767295, "no_speech_prob": 0.01151969563215971}, {"id": 96, "seek": 53296, "start": 541.84, "end": 546.1600000000001, "text": " they're born or blood pressure or certain things like that. And then there are other things that", "tokens": [50808, 436, 434, 4232, 420, 3390, 3321, 420, 1629, 721, 411, 300, 13, 400, 550, 456, 366, 661, 721, 300, 51024], "temperature": 0.0, "avg_logprob": -0.09385548371535081, "compression_ratio": 1.7955974842767295, "no_speech_prob": 0.01151969563215971}, {"id": 97, "seek": 53296, "start": 546.1600000000001, "end": 550.4000000000001, "text": " are significantly more complex, like if I'm buying a house, what does the estate agent do? Well,", "tokens": [51024, 366, 10591, 544, 3997, 11, 411, 498, 286, 478, 6382, 257, 1782, 11, 437, 775, 264, 9749, 9461, 360, 30, 1042, 11, 51236], "temperature": 0.0, "avg_logprob": -0.09385548371535081, "compression_ratio": 1.7955974842767295, "no_speech_prob": 0.01151969563215971}, {"id": 98, "seek": 53296, "start": 550.4000000000001, "end": 554.4000000000001, "text": " they have a complex model where they look at the neighborhood, they look at various different factors.", "tokens": [51236, 436, 362, 257, 3997, 2316, 689, 436, 574, 412, 264, 7630, 11, 436, 574, 412, 3683, 819, 6771, 13, 51436], "temperature": 0.0, "avg_logprob": -0.09385548371535081, "compression_ratio": 1.7955974842767295, "no_speech_prob": 0.01151969563215971}, {"id": 99, "seek": 53296, "start": 555.52, "end": 561.6800000000001, "text": " Some people have indexes and they have things that can shift over time. So having this one", "tokens": [51492, 2188, 561, 362, 8186, 279, 293, 436, 362, 721, 300, 393, 5513, 670, 565, 13, 407, 1419, 341, 472, 51800], "temperature": 0.0, "avg_logprob": -0.09385548371535081, "compression_ratio": 1.7955974842767295, "no_speech_prob": 0.01151969563215971}, {"id": 100, "seek": 56168, "start": 561.68, "end": 568.4799999999999, "text": " absolutist number just seems a bit ridiculous. I actually think I feel for policymakers because", "tokens": [50364, 18757, 468, 1230, 445, 2544, 257, 857, 11083, 13, 286, 767, 519, 286, 841, 337, 47325, 570, 50704], "temperature": 0.0, "avg_logprob": -0.08202225821358818, "compression_ratio": 1.6090534979423867, "no_speech_prob": 0.0018066366901621222}, {"id": 101, "seek": 56168, "start": 568.4799999999999, "end": 575.3599999999999, "text": " I think it will put pressure on them to continually adapt the number. So I will say there were benefits", "tokens": [50704, 286, 519, 309, 486, 829, 3321, 322, 552, 281, 22277, 6231, 264, 1230, 13, 407, 286, 486, 584, 456, 645, 5311, 51048], "temperature": 0.0, "avg_logprob": -0.08202225821358818, "compression_ratio": 1.6090534979423867, "no_speech_prob": 0.0018066366901621222}, {"id": 102, "seek": 56168, "start": 575.3599999999999, "end": 580.8, "text": " in the thinking of this number. I think it's unfortunate it got so far without scientific", "tokens": [51048, 294, 264, 1953, 295, 341, 1230, 13, 286, 519, 309, 311, 17843, 309, 658, 370, 1400, 1553, 8134, 51320], "temperature": 0.0, "avg_logprob": -0.08202225821358818, "compression_ratio": 1.6090534979423867, "no_speech_prob": 0.0018066366901621222}, {"id": 103, "seek": 56168, "start": 580.8, "end": 586.4799999999999, "text": " input. But one reason that people like Flops is, for example, it's hardware agnostic, you can measure", "tokens": [51320, 4846, 13, 583, 472, 1778, 300, 561, 411, 3235, 3370, 307, 11, 337, 1365, 11, 309, 311, 8837, 623, 77, 19634, 11, 291, 393, 3481, 51604], "temperature": 0.0, "avg_logprob": -0.08202225821358818, "compression_ratio": 1.6090534979423867, "no_speech_prob": 0.0018066366901621222}, {"id": 104, "seek": 58648, "start": 586.5600000000001, "end": 592.48, "text": " the same way across different types of hardware. And also, it's fairly easy to measure because all", "tokens": [50368, 264, 912, 636, 2108, 819, 3467, 295, 8837, 13, 400, 611, 11, 309, 311, 6457, 1858, 281, 3481, 570, 439, 50664], "temperature": 0.0, "avg_logprob": -0.08013570551969568, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.10901516675949097}, {"id": 105, "seek": 58648, "start": 592.48, "end": 600.16, "text": " it's doing is a tally of operations. So it also avoids specifying maybe what risk you care about.", "tokens": [50664, 309, 311, 884, 307, 257, 256, 379, 295, 7705, 13, 407, 309, 611, 3641, 3742, 1608, 5489, 1310, 437, 3148, 291, 1127, 466, 13, 51048], "temperature": 0.0, "avg_logprob": -0.08013570551969568, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.10901516675949097}, {"id": 106, "seek": 58648, "start": 600.16, "end": 606.8000000000001, "text": " So it gives a degree of, I would say, flexibility there for governments to adapt over time. I would", "tokens": [51048, 407, 309, 2709, 257, 4314, 295, 11, 286, 576, 584, 11, 12635, 456, 337, 11280, 281, 6231, 670, 565, 13, 286, 576, 51380], "temperature": 0.0, "avg_logprob": -0.08013570551969568, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.10901516675949097}, {"id": 107, "seek": 58648, "start": 606.8000000000001, "end": 614.8000000000001, "text": " say that is probably one of the larger shortcomings is that by not specifying, you can end up with", "tokens": [51380, 584, 300, 307, 1391, 472, 295, 264, 4833, 2099, 49886, 307, 300, 538, 406, 1608, 5489, 11, 291, 393, 917, 493, 365, 51780], "temperature": 0.0, "avg_logprob": -0.08013570551969568, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.10901516675949097}, {"id": 108, "seek": 61480, "start": 614.8, "end": 620.8, "text": " something which is evading your Flops threshold but a highly risky model. So I think that's actually", "tokens": [50364, 746, 597, 307, 1073, 8166, 428, 3235, 3370, 14678, 457, 257, 5405, 21137, 2316, 13, 407, 286, 519, 300, 311, 767, 50664], "temperature": 0.0, "avg_logprob": -0.08893461550696422, "compression_ratio": 1.6321070234113713, "no_speech_prob": 0.008836767636239529}, {"id": 109, "seek": 61480, "start": 620.8, "end": 626.9599999999999, "text": " one of the crucial shortcomings. But I do understand that the motivation of a lot of policymakers are", "tokens": [50664, 472, 295, 264, 11462, 2099, 49886, 13, 583, 286, 360, 1223, 300, 264, 12335, 295, 257, 688, 295, 47325, 366, 50972], "temperature": 0.0, "avg_logprob": -0.08893461550696422, "compression_ratio": 1.6321070234113713, "no_speech_prob": 0.008836767636239529}, {"id": 110, "seek": 61480, "start": 626.9599999999999, "end": 632.56, "text": " what else? Like what else could I use? I would argue if you're going to stick with this measure", "tokens": [50972, 437, 1646, 30, 1743, 437, 1646, 727, 286, 764, 30, 286, 576, 9695, 498, 291, 434, 516, 281, 2897, 365, 341, 3481, 51252], "temperature": 0.0, "avg_logprob": -0.08893461550696422, "compression_ratio": 1.6321070234113713, "no_speech_prob": 0.008836767636239529}, {"id": 111, "seek": 61480, "start": 632.56, "end": 638.4, "text": " and it has been formalized in several policies, you have to understand that this can be manipulated", "tokens": [51252, 293, 309, 575, 668, 9860, 1602, 294, 2940, 7657, 11, 291, 362, 281, 1223, 300, 341, 393, 312, 37161, 51544], "temperature": 0.0, "avg_logprob": -0.08893461550696422, "compression_ratio": 1.6321070234113713, "no_speech_prob": 0.008836767636239529}, {"id": 112, "seek": 61480, "start": 638.4, "end": 643.5999999999999, "text": " as a measure. And there's many ways, and I list some out in the paper, but to your point,", "tokens": [51544, 382, 257, 3481, 13, 400, 456, 311, 867, 2098, 11, 293, 286, 1329, 512, 484, 294, 264, 3035, 11, 457, 281, 428, 935, 11, 51804], "temperature": 0.0, "avg_logprob": -0.08893461550696422, "compression_ratio": 1.6321070234113713, "no_speech_prob": 0.008836767636239529}, {"id": 113, "seek": 64360, "start": 643.6, "end": 651.12, "text": " a single number puts a lot of pressure on policymakers to constantly adjust this and have", "tokens": [50364, 257, 2167, 1230, 8137, 257, 688, 295, 3321, 322, 47325, 281, 6460, 4369, 341, 293, 362, 50740], "temperature": 0.0, "avg_logprob": -0.0903090238571167, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0015958129661157727}, {"id": 114, "seek": 64360, "start": 651.12, "end": 656.64, "text": " the technical information to adjust it because this is a rapidly changing distribution. The notion of", "tokens": [50740, 264, 6191, 1589, 281, 4369, 309, 570, 341, 307, 257, 12910, 4473, 7316, 13, 440, 10710, 295, 51016], "temperature": 0.0, "avg_logprob": -0.0903090238571167, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0015958129661157727}, {"id": 115, "seek": 64360, "start": 656.64, "end": 662.96, "text": " compute has been highly unstable. Just looking at the last decade, we know this. And so it will quickly", "tokens": [51016, 14722, 575, 668, 5405, 23742, 13, 1449, 1237, 412, 264, 1036, 10378, 11, 321, 458, 341, 13, 400, 370, 309, 486, 2661, 51332], "temperature": 0.0, "avg_logprob": -0.0903090238571167, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0015958129661157727}, {"id": 116, "seek": 64360, "start": 663.76, "end": 668.48, "text": " have an expiration date. And I would argue what you're saying is excellent as an example.", "tokens": [51372, 362, 364, 39657, 4002, 13, 400, 286, 576, 9695, 437, 291, 434, 1566, 307, 7103, 382, 364, 1365, 13, 51608], "temperature": 0.0, "avg_logprob": -0.0903090238571167, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0015958129661157727}, {"id": 117, "seek": 66848, "start": 668.48, "end": 674.08, "text": " One is that you need a reference class of what are you comparing against? So you mentioned the", "tokens": [50364, 1485, 307, 300, 291, 643, 257, 6408, 1508, 295, 437, 366, 291, 15763, 1970, 30, 407, 291, 2835, 264, 50644], "temperature": 0.0, "avg_logprob": -0.1038023570798478, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.004674243740737438}, {"id": 118, "seek": 66848, "start": 675.2, "end": 681.36, "text": " kind of real estate agent who compares the pool of houses. Each of these domains, like biology models,", "tokens": [50700, 733, 295, 957, 9749, 9461, 567, 38334, 264, 7005, 295, 8078, 13, 6947, 295, 613, 25514, 11, 411, 14956, 5245, 11, 51008], "temperature": 0.0, "avg_logprob": -0.1038023570798478, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.004674243740737438}, {"id": 119, "seek": 66848, "start": 681.36, "end": 687.28, "text": " which are very interesting to certain researchers because of bio risk, language models, multimodal", "tokens": [51008, 597, 366, 588, 1880, 281, 1629, 10309, 570, 295, 12198, 3148, 11, 2856, 5245, 11, 32972, 378, 304, 51304], "temperature": 0.0, "avg_logprob": -0.1038023570798478, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.004674243740737438}, {"id": 120, "seek": 66848, "start": 687.28, "end": 691.76, "text": " models, they have different distributions of compute requirements. And so it has to be done", "tokens": [51304, 5245, 11, 436, 362, 819, 37870, 295, 14722, 7728, 13, 400, 370, 309, 575, 281, 312, 1096, 51528], "temperature": 0.0, "avg_logprob": -0.1038023570798478, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.004674243740737438}, {"id": 121, "seek": 66848, "start": 691.76, "end": 696.96, "text": " relative to your reference class, but also it should be done dynamically. The same way that a", "tokens": [51528, 4972, 281, 428, 6408, 1508, 11, 457, 611, 309, 820, 312, 1096, 43492, 13, 440, 912, 636, 300, 257, 51788], "temperature": 0.0, "avg_logprob": -0.1038023570798478, "compression_ratio": 1.6971830985915493, "no_speech_prob": 0.004674243740737438}, {"id": 122, "seek": 69696, "start": 696.96, "end": 703.12, "text": " real estate agent does it based on a percentile of surrounding houses, the notion of a single", "tokens": [50364, 957, 9749, 9461, 775, 309, 2361, 322, 257, 3043, 794, 295, 11498, 8078, 11, 264, 10710, 295, 257, 2167, 50672], "temperature": 0.0, "avg_logprob": -0.08045562108357747, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.0026582858990877867}, {"id": 123, "seek": 69696, "start": 703.12, "end": 709.52, "text": " inflection point for risk is not a viable policy tool because you just are changing things all the", "tokens": [50672, 1536, 5450, 935, 337, 3148, 307, 406, 257, 22024, 3897, 2290, 570, 291, 445, 366, 4473, 721, 439, 264, 50992], "temperature": 0.0, "avg_logprob": -0.08045562108357747, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.0026582858990877867}, {"id": 124, "seek": 69696, "start": 709.52, "end": 714.48, "text": " time. Yes. I mean, there are so many things to get into here because you went through a wonderful", "tokens": [50992, 565, 13, 1079, 13, 286, 914, 11, 456, 366, 370, 867, 721, 281, 483, 666, 510, 570, 291, 1437, 807, 257, 3715, 51240], "temperature": 0.0, "avg_logprob": -0.08045562108357747, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.0026582858990877867}, {"id": 125, "seek": 69696, "start": 714.48, "end": 719.6800000000001, "text": " list of examples in your paper. But one of the elephants in the room is that it supposes that", "tokens": [51240, 1329, 295, 5110, 294, 428, 3035, 13, 583, 472, 295, 264, 33015, 294, 264, 1808, 307, 300, 309, 1003, 4201, 300, 51500], "temperature": 0.0, "avg_logprob": -0.08045562108357747, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.0026582858990877867}, {"id": 126, "seek": 69696, "start": 719.6800000000001, "end": 725.6, "text": " there is some kind of linear commensurate relationship between compute and capabilities.", "tokens": [51500, 456, 307, 512, 733, 295, 8213, 800, 694, 33144, 2480, 1296, 14722, 293, 10862, 13, 51796], "temperature": 0.0, "avg_logprob": -0.08045562108357747, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.0026582858990877867}, {"id": 127, "seek": 72560, "start": 725.6, "end": 730.08, "text": " And of course, you're working in multilingual. I mean, it actually penalizes you because", "tokens": [50364, 400, 295, 1164, 11, 291, 434, 1364, 294, 2120, 38219, 13, 286, 914, 11, 309, 767, 13661, 5660, 291, 570, 50588], "temperature": 0.0, "avg_logprob": -0.07595826811709647, "compression_ratio": 1.7292418772563176, "no_speech_prob": 0.000726016703993082}, {"id": 128, "seek": 72560, "start": 730.08, "end": 736.4, "text": " you need to do more compute just to have a model that works at all in many different languages.", "tokens": [50588, 291, 643, 281, 360, 544, 14722, 445, 281, 362, 257, 2316, 300, 1985, 412, 439, 294, 867, 819, 8650, 13, 50904], "temperature": 0.0, "avg_logprob": -0.07595826811709647, "compression_ratio": 1.7292418772563176, "no_speech_prob": 0.000726016703993082}, {"id": 129, "seek": 72560, "start": 736.4, "end": 741.12, "text": " And this thing just isn't working for you. Yeah. I mean, what you're pointing out is that once you", "tokens": [50904, 400, 341, 551, 445, 1943, 380, 1364, 337, 291, 13, 865, 13, 286, 914, 11, 437, 291, 434, 12166, 484, 307, 300, 1564, 291, 51140], "temperature": 0.0, "avg_logprob": -0.07595826811709647, "compression_ratio": 1.7292418772563176, "no_speech_prob": 0.000726016703993082}, {"id": 130, "seek": 72560, "start": 741.12, "end": 746.4, "text": " do something like multilingual, you're basically trying to learn a new distribution each time", "tokens": [51140, 360, 746, 411, 2120, 38219, 11, 291, 434, 1936, 1382, 281, 1466, 257, 777, 7316, 1184, 565, 51404], "temperature": 0.0, "avg_logprob": -0.07595826811709647, "compression_ratio": 1.7292418772563176, "no_speech_prob": 0.000726016703993082}, {"id": 131, "seek": 72560, "start": 746.4, "end": 752.08, "text": " that's as vast as English. And so you typically need a lot. It's called the curse of multilinguality.", "tokens": [51404, 300, 311, 382, 8369, 382, 3669, 13, 400, 370, 291, 5850, 643, 257, 688, 13, 467, 311, 1219, 264, 17139, 295, 2120, 38219, 507, 13, 51688], "temperature": 0.0, "avg_logprob": -0.07595826811709647, "compression_ratio": 1.7292418772563176, "no_speech_prob": 0.000726016703993082}, {"id": 132, "seek": 75208, "start": 752.08, "end": 758.0, "text": " And so you need more compute. There's other things there which are very tricky is that", "tokens": [50364, 400, 370, 291, 643, 544, 14722, 13, 821, 311, 661, 721, 456, 597, 366, 588, 12414, 307, 300, 50660], "temperature": 0.0, "avg_logprob": -0.09400094537174, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.021261505782604218}, {"id": 133, "seek": 75208, "start": 758.0, "end": 762.64, "text": " how do you flops and how does training compute account for the vast amount of", "tokens": [50660, 577, 360, 291, 932, 3370, 293, 577, 775, 3097, 14722, 2696, 337, 264, 8369, 2372, 295, 50892], "temperature": 0.0, "avg_logprob": -0.09400094537174, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.021261505782604218}, {"id": 134, "seek": 75208, "start": 763.84, "end": 769.9200000000001, "text": " change and how we optimize after training? So we talked about RLHF. There's also instruction", "tokens": [50952, 1319, 293, 577, 321, 19719, 934, 3097, 30, 407, 321, 2825, 466, 497, 43, 39, 37, 13, 821, 311, 611, 10951, 51256], "temperature": 0.0, "avg_logprob": -0.09400094537174, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.021261505782604218}, {"id": 135, "seek": 75208, "start": 769.9200000000001, "end": 776.1600000000001, "text": " fine-tuning. There's also things like synthetic data distillation which shortens training times.", "tokens": [51256, 2489, 12, 83, 37726, 13, 821, 311, 611, 721, 411, 23420, 1412, 42923, 399, 597, 2099, 694, 3097, 1413, 13, 51568], "temperature": 0.0, "avg_logprob": -0.09400094537174, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.021261505782604218}, {"id": 136, "seek": 77616, "start": 776.16, "end": 782.48, "text": " So these are all what we call inference time optimization. So you spend time after training.", "tokens": [50364, 407, 613, 366, 439, 437, 321, 818, 38253, 565, 19618, 13, 407, 291, 3496, 565, 934, 3097, 13, 50680], "temperature": 0.0, "avg_logprob": -0.14045734916414535, "compression_ratio": 1.6047297297297298, "no_speech_prob": 0.006170196458697319}, {"id": 137, "seek": 77616, "start": 782.48, "end": 786.8, "text": " You pay for it in compute. Like you can do best event sampling, which is what you refer to with", "tokens": [50680, 509, 1689, 337, 309, 294, 14722, 13, 1743, 291, 393, 360, 1151, 2280, 21179, 11, 597, 307, 437, 291, 2864, 281, 365, 50896], "temperature": 0.0, "avg_logprob": -0.14045734916414535, "compression_ratio": 1.6047297297297298, "no_speech_prob": 0.006170196458697319}, {"id": 138, "seek": 77616, "start": 786.8, "end": 793.6, "text": " the Francois Chollet where you sample a lot of completions and you choose the best. That all has", "tokens": [50896, 264, 34695, 271, 761, 1833, 302, 689, 291, 6889, 257, 688, 295, 1557, 626, 293, 291, 2826, 264, 1151, 13, 663, 439, 575, 51236], "temperature": 0.0, "avg_logprob": -0.14045734916414535, "compression_ratio": 1.6047297297297298, "no_speech_prob": 0.006170196458697319}, {"id": 139, "seek": 77616, "start": 793.6, "end": 799.76, "text": " very pronounced benefits for models. So typically your model performance alone, just using a subset", "tokens": [51236, 588, 23155, 5311, 337, 5245, 13, 407, 5850, 428, 2316, 3389, 3312, 11, 445, 1228, 257, 25993, 51544], "temperature": 0.0, "avg_logprob": -0.14045734916414535, "compression_ratio": 1.6047297297297298, "no_speech_prob": 0.006170196458697319}, {"id": 140, "seek": 77616, "start": 799.76, "end": 805.36, "text": " of these techniques is two to six times more powerful. And that's not reflected in flops.", "tokens": [51544, 295, 613, 7512, 307, 732, 281, 2309, 1413, 544, 4005, 13, 400, 300, 311, 406, 15502, 294, 932, 3370, 13, 51824], "temperature": 0.0, "avg_logprob": -0.14045734916414535, "compression_ratio": 1.6047297297297298, "no_speech_prob": 0.006170196458697319}, {"id": 141, "seek": 80536, "start": 805.36, "end": 810.5600000000001, "text": " Yes. Because I'm really interested in when you look at the model life cycle or the predictive", "tokens": [50364, 1079, 13, 1436, 286, 478, 534, 3102, 294, 562, 291, 574, 412, 264, 2316, 993, 6586, 420, 264, 35521, 50624], "temperature": 0.0, "avg_logprob": -0.12192895093302089, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.0023177664261311293}, {"id": 142, "seek": 80536, "start": 810.5600000000001, "end": 815.76, "text": " life cycle, there are so many places where you can spend computation. So you can do dataset", "tokens": [50624, 993, 6586, 11, 456, 366, 370, 867, 3190, 689, 291, 393, 3496, 24903, 13, 407, 291, 393, 360, 28872, 50884], "temperature": 0.0, "avg_logprob": -0.12192895093302089, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.0023177664261311293}, {"id": 143, "seek": 80536, "start": 815.76, "end": 820.0, "text": " generation and you can do, obviously there's the training of the model and then you can do", "tokens": [50884, 5125, 293, 291, 393, 360, 11, 2745, 456, 311, 264, 3097, 295, 264, 2316, 293, 550, 291, 393, 360, 51096], "temperature": 0.0, "avg_logprob": -0.12192895093302089, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.0023177664261311293}, {"id": 144, "seek": 80536, "start": 820.0, "end": 823.92, "text": " like inference time optimization and active inference and a whole bunch of stuff like that.", "tokens": [51096, 411, 38253, 565, 19618, 293, 4967, 38253, 293, 257, 1379, 3840, 295, 1507, 411, 300, 13, 51292], "temperature": 0.0, "avg_logprob": -0.12192895093302089, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.0023177664261311293}, {"id": 145, "seek": 80536, "start": 823.92, "end": 828.48, "text": " And they are only taking into account the model training. But then there's the further issue", "tokens": [51292, 400, 436, 366, 787, 1940, 666, 2696, 264, 2316, 3097, 13, 583, 550, 456, 311, 264, 3052, 2734, 51520], "temperature": 0.0, "avg_logprob": -0.12192895093302089, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.0023177664261311293}, {"id": 146, "seek": 80536, "start": 828.48, "end": 832.88, "text": " of training provenance. So for example, I can download a model from Huggingface and I can fine", "tokens": [51520, 295, 3097, 12785, 719, 13, 407, 337, 1365, 11, 286, 393, 5484, 257, 2316, 490, 46892, 3249, 2868, 293, 286, 393, 2489, 51740], "temperature": 0.0, "avg_logprob": -0.12192895093302089, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.0023177664261311293}, {"id": 147, "seek": 83288, "start": 832.88, "end": 837.12, "text": " tune it and do a bunch of stuff on it. And like, how do you know, right? It's just an inscrutable", "tokens": [50364, 10864, 309, 293, 360, 257, 3840, 295, 1507, 322, 309, 13, 400, 411, 11, 577, 360, 291, 458, 11, 558, 30, 467, 311, 445, 364, 1028, 10757, 32148, 50576], "temperature": 0.0, "avg_logprob": -0.10523624557385342, "compression_ratio": 1.6930091185410334, "no_speech_prob": 0.006966503337025642}, {"id": 148, "seek": 83288, "start": 837.12, "end": 842.16, "text": " bunch of weights. Like you have no idea how much training has gone into it. This is the idea of", "tokens": [50576, 3840, 295, 17443, 13, 1743, 291, 362, 572, 1558, 577, 709, 3097, 575, 2780, 666, 309, 13, 639, 307, 264, 1558, 295, 50828], "temperature": 0.0, "avg_logprob": -0.10523624557385342, "compression_ratio": 1.6930091185410334, "no_speech_prob": 0.006966503337025642}, {"id": 149, "seek": 83288, "start": 842.16, "end": 846.8, "text": " tracing flops across the life cycle. I think this is also going to be formidable because", "tokens": [50828, 25262, 932, 3370, 2108, 264, 993, 6586, 13, 286, 519, 341, 307, 611, 516, 281, 312, 41246, 570, 51060], "temperature": 0.0, "avg_logprob": -0.10523624557385342, "compression_ratio": 1.6930091185410334, "no_speech_prob": 0.006966503337025642}, {"id": 150, "seek": 83288, "start": 846.8, "end": 852.32, "text": " increasingly the most popular models on Huggingface, by the way, are models which haven't been", "tokens": [51060, 12980, 264, 881, 3743, 5245, 322, 46892, 3249, 2868, 11, 538, 264, 636, 11, 366, 5245, 597, 2378, 380, 668, 51336], "temperature": 0.0, "avg_logprob": -0.10523624557385342, "compression_ratio": 1.6930091185410334, "no_speech_prob": 0.006966503337025642}, {"id": 151, "seek": 83288, "start": 852.32, "end": 856.56, "text": " instruction fine-tuned. They're base models. And why? Because people want to do continued", "tokens": [51336, 10951, 2489, 12, 83, 43703, 13, 814, 434, 3096, 5245, 13, 400, 983, 30, 1436, 561, 528, 281, 360, 7014, 51548], "temperature": 0.0, "avg_logprob": -0.10523624557385342, "compression_ratio": 1.6930091185410334, "no_speech_prob": 0.006966503337025642}, {"id": 152, "seek": 83288, "start": 856.56, "end": 861.04, "text": " pre-training. They want to overlay their own optimization techniques, which suggests that", "tokens": [51548, 659, 12, 17227, 1760, 13, 814, 528, 281, 31741, 641, 1065, 19618, 7512, 11, 597, 13409, 300, 51772], "temperature": 0.0, "avg_logprob": -0.10523624557385342, "compression_ratio": 1.6930091185410334, "no_speech_prob": 0.006966503337025642}, {"id": 153, "seek": 86104, "start": 861.04, "end": 865.68, "text": " people are using this as one step in their optimization process. It's going to be a formidable", "tokens": [50364, 561, 366, 1228, 341, 382, 472, 1823, 294, 641, 19618, 1399, 13, 467, 311, 516, 281, 312, 257, 41246, 50596], "temperature": 0.0, "avg_logprob": -0.08790384292602539, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.00348108378238976}, {"id": 154, "seek": 86104, "start": 865.68, "end": 871.52, "text": " challenge to tally it in a reasonable way, especially when sometimes the way that we", "tokens": [50596, 3430, 281, 256, 379, 309, 294, 257, 10585, 636, 11, 2318, 562, 2171, 264, 636, 300, 321, 50888], "temperature": 0.0, "avg_logprob": -0.08790384292602539, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.00348108378238976}, {"id": 155, "seek": 86104, "start": 872.24, "end": 876.7199999999999, "text": " measure these, think about something like mixture of experts or a classic ensemble.", "tokens": [50924, 3481, 613, 11, 519, 466, 746, 411, 9925, 295, 8572, 420, 257, 7230, 19492, 13, 51148], "temperature": 0.0, "avg_logprob": -0.08790384292602539, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.00348108378238976}, {"id": 156, "seek": 86104, "start": 877.52, "end": 882.24, "text": " What counts then? Because you may have many different experts in your mixture of experts,", "tokens": [51188, 708, 14893, 550, 30, 1436, 291, 815, 362, 867, 819, 8572, 294, 428, 9925, 295, 8572, 11, 51424], "temperature": 0.0, "avg_logprob": -0.08790384292602539, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.00348108378238976}, {"id": 157, "seek": 86104, "start": 882.24, "end": 888.8, "text": " but you're only using two at the end. Classic ensembling is even more nuanced because technically", "tokens": [51424, 457, 291, 434, 787, 1228, 732, 412, 264, 917, 13, 25008, 465, 15750, 1688, 307, 754, 544, 45115, 570, 12120, 51752], "temperature": 0.0, "avg_logprob": -0.08790384292602539, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.00348108378238976}, {"id": 158, "seek": 88880, "start": 888.8, "end": 894.24, "text": " you didn't even optimize all the models together. You just show up and you ensemble them at the end", "tokens": [50364, 291, 994, 380, 754, 19719, 439, 264, 5245, 1214, 13, 509, 445, 855, 493, 293, 291, 19492, 552, 412, 264, 917, 50636], "temperature": 0.0, "avg_logprob": -0.06085535227242163, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.002746670739725232}, {"id": 159, "seek": 88880, "start": 894.24, "end": 898.88, "text": " and you get one model at the end. So how do you handle that? It's very interesting and it's very", "tokens": [50636, 293, 291, 483, 472, 2316, 412, 264, 917, 13, 407, 577, 360, 291, 4813, 300, 30, 467, 311, 588, 1880, 293, 309, 311, 588, 50868], "temperature": 0.0, "avg_logprob": -0.06085535227242163, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.002746670739725232}, {"id": 160, "seek": 88880, "start": 898.88, "end": 904.7199999999999, "text": " related to this challenge of people are likely taking some level of compute already and they're", "tokens": [50868, 4077, 281, 341, 3430, 295, 561, 366, 3700, 1940, 512, 1496, 295, 14722, 1217, 293, 436, 434, 51160], "temperature": 0.0, "avg_logprob": -0.06085535227242163, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.002746670739725232}, {"id": 161, "seek": 88880, "start": 904.7199999999999, "end": 909.76, "text": " doing some changes at the end of training that make it more performant. Yeah. And then there's", "tokens": [51160, 884, 512, 2962, 412, 264, 917, 295, 3097, 300, 652, 309, 544, 2042, 394, 13, 865, 13, 400, 550, 456, 311, 51412], "temperature": 0.0, "avg_logprob": -0.06085535227242163, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.002746670739725232}, {"id": 162, "seek": 88880, "start": 909.76, "end": 915.76, "text": " this matter of Good Heart's Law, which is that when a target becomes a measure, it ceases to be", "tokens": [51412, 341, 1871, 295, 2205, 13569, 311, 7744, 11, 597, 307, 300, 562, 257, 3779, 3643, 257, 3481, 11, 309, 1769, 1957, 281, 312, 51712], "temperature": 0.0, "avg_logprob": -0.06085535227242163, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.002746670739725232}, {"id": 163, "seek": 91576, "start": 915.76, "end": 921.04, "text": " a good measure. And there are so many examples of this. For example, the banks have these arbitrary", "tokens": [50364, 257, 665, 3481, 13, 400, 456, 366, 370, 867, 5110, 295, 341, 13, 1171, 1365, 11, 264, 10237, 362, 613, 23211, 50628], "temperature": 0.0, "avg_logprob": -0.11091034228985126, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.030912457033991814}, {"id": 164, "seek": 91576, "start": 921.04, "end": 925.84, "text": " limits on the amount of money that you can send, which is why it's set at, let's say, $10,000 and", "tokens": [50628, 10406, 322, 264, 2372, 295, 1460, 300, 291, 393, 2845, 11, 597, 307, 983, 309, 311, 992, 412, 11, 718, 311, 584, 11, 1848, 3279, 11, 1360, 293, 50868], "temperature": 0.0, "avg_logprob": -0.11091034228985126, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.030912457033991814}, {"id": 165, "seek": 91576, "start": 925.84, "end": 930.72, "text": " then you see loads and loads of transactions at like $9999 because they know what the limit is.", "tokens": [50868, 550, 291, 536, 12668, 293, 12668, 295, 16856, 412, 411, 1848, 24, 8494, 24, 570, 436, 458, 437, 264, 4948, 307, 13, 51112], "temperature": 0.0, "avg_logprob": -0.11091034228985126, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.030912457033991814}, {"id": 166, "seek": 91576, "start": 930.72, "end": 935.04, "text": " And it must be the same here, right? It's going to gamify the system. People are going to evade", "tokens": [51112, 400, 309, 1633, 312, 264, 912, 510, 11, 558, 30, 467, 311, 516, 281, 8019, 2505, 264, 1185, 13, 3432, 366, 516, 281, 1073, 762, 51328], "temperature": 0.0, "avg_logprob": -0.11091034228985126, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.030912457033991814}, {"id": 167, "seek": 91576, "start": 935.04, "end": 941.68, "text": " it in so many ways. I think that the main advice I have about this is that if policy makers have", "tokens": [51328, 309, 294, 370, 867, 2098, 13, 286, 519, 300, 264, 2135, 5192, 286, 362, 466, 341, 307, 300, 498, 3897, 19323, 362, 51660], "temperature": 0.0, "avg_logprob": -0.11091034228985126, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.030912457033991814}, {"id": 168, "seek": 94168, "start": 941.68, "end": 946.3199999999999, "text": " decided on this and they're going to go for it, they need to complement it with an auxiliary", "tokens": [50364, 3047, 322, 341, 293, 436, 434, 516, 281, 352, 337, 309, 11, 436, 643, 281, 17103, 309, 365, 364, 43741, 50596], "temperature": 0.0, "avg_logprob": -0.053393408104225444, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.04878527671098709}, {"id": 169, "seek": 94168, "start": 946.3199999999999, "end": 951.04, "text": " measure of the actual risk that they care about. It has to be an index because if you just stick", "tokens": [50596, 3481, 295, 264, 3539, 3148, 300, 436, 1127, 466, 13, 467, 575, 281, 312, 364, 8186, 570, 498, 291, 445, 2897, 50832], "temperature": 0.0, "avg_logprob": -0.053393408104225444, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.04878527671098709}, {"id": 170, "seek": 94168, "start": 951.04, "end": 957.12, "text": " with compute, it is too easy to evade because there's too many different things you can do post", "tokens": [50832, 365, 14722, 11, 309, 307, 886, 1858, 281, 1073, 762, 570, 456, 311, 886, 867, 819, 721, 291, 393, 360, 2183, 51136], "temperature": 0.0, "avg_logprob": -0.053393408104225444, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.04878527671098709}, {"id": 171, "seek": 94168, "start": 957.12, "end": 963.04, "text": " training to gain percentage points. And there's too many ways of essentially shortening your", "tokens": [51136, 3097, 281, 6052, 9668, 2793, 13, 400, 456, 311, 886, 867, 2098, 295, 4476, 2099, 4559, 428, 51432], "temperature": 0.0, "avg_logprob": -0.053393408104225444, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.04878527671098709}, {"id": 172, "seek": 94168, "start": 963.04, "end": 967.4399999999999, "text": " training time or reducing your flops while still arriving at a highly performant model.", "tokens": [51432, 3097, 565, 420, 12245, 428, 932, 3370, 1339, 920, 22436, 412, 257, 5405, 2042, 394, 2316, 13, 51652], "temperature": 0.0, "avg_logprob": -0.053393408104225444, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.04878527671098709}, {"id": 173, "seek": 96744, "start": 968.1600000000001, "end": 973.36, "text": " And so that's the other key recommendation I have is that you need something that is", "tokens": [50400, 400, 370, 300, 311, 264, 661, 2141, 11879, 286, 362, 307, 300, 291, 643, 746, 300, 307, 50660], "temperature": 0.0, "avg_logprob": -0.07050229254223052, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.015100793913006783}, {"id": 174, "seek": 96744, "start": 973.36, "end": 979.12, "text": " anchored to the downstream risk you actually care about. And compute is not. It just reflects", "tokens": [50660, 12723, 2769, 281, 264, 30621, 3148, 291, 767, 1127, 466, 13, 400, 14722, 307, 406, 13, 467, 445, 18926, 50948], "temperature": 0.0, "avg_logprob": -0.07050229254223052, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.015100793913006783}, {"id": 175, "seek": 96744, "start": 979.12, "end": 988.08, "text": " our belief that more compute is better. And that simply is too simplistic of you to account for", "tokens": [50948, 527, 7107, 300, 544, 14722, 307, 1101, 13, 400, 300, 2935, 307, 886, 44199, 295, 291, 281, 2696, 337, 51396], "temperature": 0.0, "avg_logprob": -0.07050229254223052, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.015100793913006783}, {"id": 176, "seek": 96744, "start": 988.72, "end": 993.6800000000001, "text": " all the ways in which smaller models, if they're very targeted, can be extremely risky.", "tokens": [51428, 439, 264, 2098, 294, 597, 4356, 5245, 11, 498, 436, 434, 588, 15045, 11, 393, 312, 4664, 21137, 13, 51676], "temperature": 0.0, "avg_logprob": -0.07050229254223052, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.015100793913006783}, {"id": 177, "seek": 99368, "start": 994.4, "end": 1002.7199999999999, "text": " Yes. So I think maybe we should bring Rich Sutton in. So he wrote this essay called The", "tokens": [50400, 1079, 13, 407, 286, 519, 1310, 321, 820, 1565, 6781, 40492, 1756, 294, 13, 407, 415, 4114, 341, 16238, 1219, 440, 50816], "temperature": 0.0, "avg_logprob": -0.12299597632024706, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.008104951120913029}, {"id": 178, "seek": 99368, "start": 1002.7199999999999, "end": 1008.9599999999999, "text": " Bitter Lesson. And I'll let you bring it in. But he was partially responsible for this idea that", "tokens": [50816, 363, 3904, 18649, 266, 13, 400, 286, 603, 718, 291, 1565, 309, 294, 13, 583, 415, 390, 18886, 6250, 337, 341, 1558, 300, 51128], "temperature": 0.0, "avg_logprob": -0.12299597632024706, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.008104951120913029}, {"id": 179, "seek": 99368, "start": 1008.9599999999999, "end": 1014.9599999999999, "text": " compute is all you need. Yeah, bring that in. Yeah. And by the way, I think that's a fantastic", "tokens": [51128, 14722, 307, 439, 291, 643, 13, 865, 11, 1565, 300, 294, 13, 865, 13, 400, 538, 264, 636, 11, 286, 519, 300, 311, 257, 5456, 51428], "temperature": 0.0, "avg_logprob": -0.12299597632024706, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.008104951120913029}, {"id": 180, "seek": 99368, "start": 1014.9599999999999, "end": 1023.28, "text": " essay. It really is this idea that history tells us in computer science in particular that all", "tokens": [51428, 16238, 13, 467, 534, 307, 341, 1558, 300, 2503, 5112, 505, 294, 3820, 3497, 294, 1729, 300, 439, 51844], "temperature": 0.0, "avg_logprob": -0.12299597632024706, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.008104951120913029}, {"id": 181, "seek": 102328, "start": 1023.36, "end": 1031.84, "text": " efforts to codify our expertise, to work on very fancy ways of imparting what we think is the right", "tokens": [50368, 6484, 281, 17656, 2505, 527, 11769, 11, 281, 589, 322, 588, 10247, 2098, 295, 32177, 278, 437, 321, 519, 307, 264, 558, 50792], "temperature": 0.0, "avg_logprob": -0.0897764390514743, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.006551405880600214}, {"id": 182, "seek": 102328, "start": 1031.84, "end": 1038.72, "text": " way to learn to a model have been particularly futile. Like he's really saying we're not very good", "tokens": [50792, 636, 281, 1466, 281, 257, 2316, 362, 668, 4098, 1877, 794, 13, 1743, 415, 311, 534, 1566, 321, 434, 406, 588, 665, 51136], "temperature": 0.0, "avg_logprob": -0.0897764390514743, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.006551405880600214}, {"id": 183, "seek": 102328, "start": 1038.72, "end": 1043.6, "text": " as computer scientists. And the biggest ingredient of success that's driven things is being adding", "tokens": [51136, 382, 3820, 7708, 13, 400, 264, 3880, 14751, 295, 2245, 300, 311, 9555, 721, 307, 885, 5127, 51380], "temperature": 0.0, "avg_logprob": -0.0897764390514743, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.006551405880600214}, {"id": 184, "seek": 102328, "start": 1043.6, "end": 1048.3999999999999, "text": " compute to the mix. And that we can do things that are algorithmic, but it has to play well", "tokens": [51380, 14722, 281, 264, 2890, 13, 400, 300, 321, 393, 360, 721, 300, 366, 9284, 299, 11, 457, 309, 575, 281, 862, 731, 51620], "temperature": 0.0, "avg_logprob": -0.0897764390514743, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.006551405880600214}, {"id": 185, "seek": 104840, "start": 1048.4, "end": 1054.48, "text": " with compute. So anyway, he is kind of get this idea of hardware. But it's more general. It's this", "tokens": [50364, 365, 14722, 13, 407, 4033, 11, 415, 307, 733, 295, 483, 341, 1558, 295, 8837, 13, 583, 309, 311, 544, 2674, 13, 467, 311, 341, 50668], "temperature": 0.0, "avg_logprob": -0.09614570517289012, "compression_ratio": 1.7981072555205047, "no_speech_prob": 0.07638049870729446}, {"id": 186, "seek": 104840, "start": 1054.48, "end": 1058.0, "text": " idea that it's not specific to a different sort of type of hardware. It's just compute. If you", "tokens": [50668, 1558, 300, 309, 311, 406, 2685, 281, 257, 819, 1333, 295, 2010, 295, 8837, 13, 467, 311, 445, 14722, 13, 759, 291, 50844], "temperature": 0.0, "avg_logprob": -0.09614570517289012, "compression_ratio": 1.7981072555205047, "no_speech_prob": 0.07638049870729446}, {"id": 187, "seek": 104840, "start": 1058.0, "end": 1063.3600000000001, "text": " play well with compute, if it scales well, it's going to be the winning variant. And yeah, go for", "tokens": [50844, 862, 731, 365, 14722, 11, 498, 309, 17408, 731, 11, 309, 311, 516, 281, 312, 264, 8224, 17501, 13, 400, 1338, 11, 352, 337, 51112], "temperature": 0.0, "avg_logprob": -0.09614570517289012, "compression_ratio": 1.7981072555205047, "no_speech_prob": 0.07638049870729446}, {"id": 188, "seek": 104840, "start": 1063.3600000000001, "end": 1068.0, "text": " it. Well, I mean, I just I have an intuition that he's he's right and wrong at the same time. So", "tokens": [51112, 309, 13, 1042, 11, 286, 914, 11, 286, 445, 286, 362, 364, 24002, 300, 415, 311, 415, 311, 558, 293, 2085, 412, 264, 912, 565, 13, 407, 51344], "temperature": 0.0, "avg_logprob": -0.09614570517289012, "compression_ratio": 1.7981072555205047, "no_speech_prob": 0.07638049870729446}, {"id": 189, "seek": 104840, "start": 1068.0, "end": 1072.0800000000002, "text": " I mean, in terms of system one models that just memorize things better and better, he's kind", "tokens": [51344, 286, 914, 11, 294, 2115, 295, 1185, 472, 5245, 300, 445, 27478, 721, 1101, 293, 1101, 11, 415, 311, 733, 51548], "temperature": 0.0, "avg_logprob": -0.09614570517289012, "compression_ratio": 1.7981072555205047, "no_speech_prob": 0.07638049870729446}, {"id": 190, "seek": 104840, "start": 1072.0800000000002, "end": 1075.76, "text": " of right. Because there is a commensurate relationship, you know, as we memorize more of", "tokens": [51548, 295, 558, 13, 1436, 456, 307, 257, 800, 694, 33144, 2480, 11, 291, 458, 11, 382, 321, 27478, 544, 295, 51732], "temperature": 0.0, "avg_logprob": -0.09614570517289012, "compression_ratio": 1.7981072555205047, "no_speech_prob": 0.07638049870729446}, {"id": 191, "seek": 107576, "start": 1075.76, "end": 1081.04, "text": " the long tail, the models get better and better. But I still think that there might be a fundamental", "tokens": [50364, 264, 938, 6838, 11, 264, 5245, 483, 1101, 293, 1101, 13, 583, 286, 920, 519, 300, 456, 1062, 312, 257, 8088, 50628], "temperature": 0.0, "avg_logprob": -0.13197096403654632, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.010429180227220058}, {"id": 192, "seek": 107576, "start": 1081.04, "end": 1086.24, "text": " break between compute and capabilities when and ironically, a regulation like this might", "tokens": [50628, 1821, 1296, 14722, 293, 10862, 562, 293, 41082, 11, 257, 15062, 411, 341, 1062, 50888], "temperature": 0.0, "avg_logprob": -0.13197096403654632, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.010429180227220058}, {"id": 193, "seek": 107576, "start": 1086.24, "end": 1090.96, "text": " incentivize to find such a break. So you know, we might design system two models that actually do", "tokens": [50888, 35328, 1125, 281, 915, 1270, 257, 1821, 13, 407, 291, 458, 11, 321, 1062, 1715, 1185, 732, 5245, 300, 767, 360, 51124], "temperature": 0.0, "avg_logprob": -0.13197096403654632, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.010429180227220058}, {"id": 194, "seek": 107576, "start": 1090.96, "end": 1094.64, "text": " reasoning and have a, you know, Neurosymbolic architecture or whatever. And now all of a", "tokens": [51124, 21577, 293, 362, 257, 11, 291, 458, 11, 1734, 8977, 88, 5612, 299, 9482, 420, 2035, 13, 400, 586, 439, 295, 257, 51308], "temperature": 0.0, "avg_logprob": -0.13197096403654632, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.010429180227220058}, {"id": 195, "seek": 107576, "start": 1094.64, "end": 1100.96, "text": " sudden we've got like really good capabilities with less compute. Yeah, I mean, you're hitting on", "tokens": [51308, 3990, 321, 600, 658, 411, 534, 665, 10862, 365, 1570, 14722, 13, 865, 11, 286, 914, 11, 291, 434, 8850, 322, 51624], "temperature": 0.0, "avg_logprob": -0.13197096403654632, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.010429180227220058}, {"id": 196, "seek": 110096, "start": 1101.68, "end": 1106.56, "text": " the head, I don't disagree with you. I think where I agree with Rich said it is that for", "tokens": [50400, 264, 1378, 11, 286, 500, 380, 14091, 365, 291, 13, 286, 519, 689, 286, 3986, 365, 6781, 848, 309, 307, 300, 337, 50644], "temperature": 0.0, "avg_logprob": -0.16635588912276533, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.01716603897511959}, {"id": 197, "seek": 110096, "start": 1106.56, "end": 1113.92, "text": " given architecture, say, transformers, you can throw more compute at it up until a certain", "tokens": [50644, 2212, 9482, 11, 584, 11, 4088, 433, 11, 291, 393, 3507, 544, 14722, 412, 309, 493, 1826, 257, 1629, 51012], "temperature": 0.0, "avg_logprob": -0.16635588912276533, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.01716603897511959}, {"id": 198, "seek": 110096, "start": 1113.92, "end": 1118.96, "text": " point where it's saturated, but you're going to see all the things equal, your data sets equal,", "tokens": [51012, 935, 689, 309, 311, 25408, 11, 457, 291, 434, 516, 281, 536, 439, 264, 721, 2681, 11, 428, 1412, 6352, 2681, 11, 51264], "temperature": 0.0, "avg_logprob": -0.16635588912276533, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.01716603897511959}, {"id": 199, "seek": 110096, "start": 1118.96, "end": 1123.28, "text": " compute is better because these are greedy learners, they, they're, you know, our deep", "tokens": [51264, 14722, 307, 1101, 570, 613, 366, 28228, 23655, 11, 436, 11, 436, 434, 11, 291, 458, 11, 527, 2452, 51480], "temperature": 0.0, "avg_logprob": -0.16635588912276533, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.01716603897511959}, {"id": 200, "seek": 110096, "start": 1123.28, "end": 1127.28, "text": " neural networks are frequency counters, you're going to see gains on the long tail performance", "tokens": [51480, 18161, 9590, 366, 7893, 39338, 11, 291, 434, 516, 281, 536, 16823, 322, 264, 938, 6838, 3389, 51680], "temperature": 0.0, "avg_logprob": -0.16635588912276533, "compression_ratio": 1.7245283018867925, "no_speech_prob": 0.01716603897511959}, {"id": 201, "seek": 112728, "start": 1127.28, "end": 1134.8799999999999, "text": " and overall gains. Where it misses the point is that really there's a few things going on.", "tokens": [50364, 293, 4787, 16823, 13, 2305, 309, 29394, 264, 935, 307, 300, 534, 456, 311, 257, 1326, 721, 516, 322, 13, 50744], "temperature": 0.0, "avg_logprob": -0.09599751517886207, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.008438808843493462}, {"id": 202, "seek": 112728, "start": 1134.8799999999999, "end": 1140.08, "text": " One is that because our current representations are so inefficient, there's ways to", "tokens": [50744, 1485, 307, 300, 570, 527, 2190, 33358, 366, 370, 43495, 11, 456, 311, 2098, 281, 51004], "temperature": 0.0, "avg_logprob": -0.09599751517886207, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.008438808843493462}, {"id": 203, "seek": 112728, "start": 1142.16, "end": 1149.84, "text": " really change the algorithm itself and bend the, the rule of compute. So, and the rate at which", "tokens": [51108, 534, 1319, 264, 9284, 2564, 293, 11229, 264, 11, 264, 4978, 295, 14722, 13, 407, 11, 293, 264, 3314, 412, 597, 51492], "temperature": 0.0, "avg_logprob": -0.09599751517886207, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.008438808843493462}, {"id": 204, "seek": 112728, "start": 1149.84, "end": 1155.76, "text": " compute is needed to unlock gains. And deep neural networks in particular are a great example of", "tokens": [51492, 14722, 307, 2978, 281, 11634, 16823, 13, 400, 2452, 18161, 9590, 294, 1729, 366, 257, 869, 1365, 295, 51788], "temperature": 0.0, "avg_logprob": -0.09599751517886207, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.008438808843493462}, {"id": 205, "seek": 115576, "start": 1155.76, "end": 1159.44, "text": " this because they're so painfully inefficient because we have to show all the data the same", "tokens": [50364, 341, 570, 436, 434, 370, 1822, 2277, 43495, 570, 321, 362, 281, 855, 439, 264, 1412, 264, 912, 50548], "temperature": 0.0, "avg_logprob": -0.10482362387836844, "compression_ratio": 1.888157894736842, "no_speech_prob": 0.021252473816275597}, {"id": 206, "seek": 115576, "start": 1159.44, "end": 1163.6, "text": " amount of times because we have to do these global updates. And so we're seeing all these tricks.", "tokens": [50548, 2372, 295, 1413, 570, 321, 362, 281, 360, 613, 4338, 9205, 13, 400, 370, 321, 434, 2577, 439, 613, 11733, 13, 50756], "temperature": 0.0, "avg_logprob": -0.10482362387836844, "compression_ratio": 1.888157894736842, "no_speech_prob": 0.021252473816275597}, {"id": 207, "seek": 115576, "start": 1164.24, "end": 1168.48, "text": " For example, now we care about data again, and we care about data quality. So we condition that", "tokens": [50788, 1171, 1365, 11, 586, 321, 1127, 466, 1412, 797, 11, 293, 321, 1127, 466, 1412, 3125, 13, 407, 321, 4188, 300, 51000], "temperature": 0.0, "avg_logprob": -0.10482362387836844, "compression_ratio": 1.888157894736842, "no_speech_prob": 0.021252473816275597}, {"id": 208, "seek": 115576, "start": 1168.48, "end": 1173.84, "text": " space better to represent what we want to model downstream. That means we have to train far less", "tokens": [51000, 1901, 1101, 281, 2906, 437, 321, 528, 281, 2316, 30621, 13, 663, 1355, 321, 362, 281, 3847, 1400, 1570, 51268], "temperature": 0.0, "avg_logprob": -0.10482362387836844, "compression_ratio": 1.888157894736842, "no_speech_prob": 0.021252473816275597}, {"id": 209, "seek": 115576, "start": 1173.84, "end": 1178.08, "text": " because all the features in the data set, the ones we want to learn was you just train on the", "tokens": [51268, 570, 439, 264, 4122, 294, 264, 1412, 992, 11, 264, 2306, 321, 528, 281, 1466, 390, 291, 445, 3847, 322, 264, 51480], "temperature": 0.0, "avg_logprob": -0.10482362387836844, "compression_ratio": 1.888157894736842, "no_speech_prob": 0.021252473816275597}, {"id": 210, "seek": 115576, "start": 1178.08, "end": 1181.84, "text": " internet. There's a lot you don't want to learn. So you have to kind of unlearn it afterwards and", "tokens": [51480, 4705, 13, 821, 311, 257, 688, 291, 500, 380, 528, 281, 1466, 13, 407, 291, 362, 281, 733, 295, 25272, 1083, 309, 10543, 293, 51668], "temperature": 0.0, "avg_logprob": -0.10482362387836844, "compression_ratio": 1.888157894736842, "no_speech_prob": 0.021252473816275597}, {"id": 211, "seek": 118184, "start": 1181.84, "end": 1187.4399999999998, "text": " spend a lot of compute just trying to find what you want within that. So that's where you bend", "tokens": [50364, 3496, 257, 688, 295, 14722, 445, 1382, 281, 915, 437, 291, 528, 1951, 300, 13, 407, 300, 311, 689, 291, 11229, 50644], "temperature": 0.0, "avg_logprob": -0.09053983007158552, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.005727180279791355}, {"id": 212, "seek": 118184, "start": 1187.4399999999998, "end": 1192.24, "text": " the rule and it becomes more nuanced, which is that the other thing that that misses is that,", "tokens": [50644, 264, 4978, 293, 309, 3643, 544, 45115, 11, 597, 307, 300, 264, 661, 551, 300, 300, 29394, 307, 300, 11, 50884], "temperature": 0.0, "avg_logprob": -0.09053983007158552, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.005727180279791355}, {"id": 213, "seek": 118184, "start": 1192.8799999999999, "end": 1196.48, "text": " or at least that wasn't a core part of this essay, I actually think Rich might agree with", "tokens": [50916, 420, 412, 1935, 300, 2067, 380, 257, 4965, 644, 295, 341, 16238, 11, 286, 767, 519, 6781, 1062, 3986, 365, 51096], "temperature": 0.0, "avg_logprob": -0.09053983007158552, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.005727180279791355}, {"id": 214, "seek": 118184, "start": 1196.48, "end": 1203.76, "text": " me on this, is that your rate of compute is really determined more than anything by the", "tokens": [51096, 385, 322, 341, 11, 307, 300, 428, 3314, 295, 14722, 307, 534, 9540, 544, 813, 1340, 538, 264, 51460], "temperature": 0.0, "avg_logprob": -0.09053983007158552, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.005727180279791355}, {"id": 215, "seek": 118184, "start": 1203.76, "end": 1208.9599999999998, "text": " prior of your algorithm. So yes, if your algorithm plays well with compute, if it's scalable,", "tokens": [51460, 4059, 295, 428, 9284, 13, 407, 2086, 11, 498, 428, 9284, 5749, 731, 365, 14722, 11, 498, 309, 311, 38481, 11, 51720], "temperature": 0.0, "avg_logprob": -0.09053983007158552, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.005727180279791355}, {"id": 216, "seek": 120896, "start": 1209.04, "end": 1214.4, "text": " compute unlocks a lot. But the rate and the saturation point is determined by the algorithm.", "tokens": [50368, 14722, 517, 34896, 257, 688, 13, 583, 264, 3314, 293, 264, 27090, 935, 307, 9540, 538, 264, 9284, 13, 50636], "temperature": 0.0, "avg_logprob": -0.09229078916745766, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.005908488295972347}, {"id": 217, "seek": 120896, "start": 1214.4, "end": 1219.6000000000001, "text": " And what do we mean by this? Convolutional neural networks are a great example. Introduced in 2012,", "tokens": [50636, 400, 437, 360, 321, 914, 538, 341, 30, 2656, 85, 3386, 304, 18161, 9590, 366, 257, 869, 1365, 13, 27193, 1232, 294, 9125, 11, 50896], "temperature": 0.0, "avg_logprob": -0.09229078916745766, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.005908488295972347}, {"id": 218, "seek": 120896, "start": 1220.56, "end": 1225.28, "text": " really unlocked scalability. Why? Because convolutional filters and patches made it", "tokens": [50944, 534, 30180, 15664, 2310, 13, 1545, 30, 1436, 45216, 304, 15995, 293, 26531, 1027, 309, 51180], "temperature": 0.0, "avg_logprob": -0.09229078916745766, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.005908488295972347}, {"id": 219, "seek": 120896, "start": 1225.28, "end": 1230.4, "text": " possible to model high dimensional images at the time. Why? Because you move your patch over your", "tokens": [51180, 1944, 281, 2316, 1090, 18795, 5267, 412, 264, 565, 13, 1545, 30, 1436, 291, 1286, 428, 9972, 670, 428, 51436], "temperature": 0.0, "avg_logprob": -0.09229078916745766, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.005908488295972347}, {"id": 220, "seek": 120896, "start": 1230.4, "end": 1236.96, "text": " image. This takes advantage of local relationships. You can really reduce your dimensionality. Max", "tokens": [51436, 3256, 13, 639, 2516, 5002, 295, 2654, 6159, 13, 509, 393, 534, 5407, 428, 10139, 1860, 13, 7402, 51764], "temperature": 0.0, "avg_logprob": -0.09229078916745766, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.005908488295972347}, {"id": 221, "seek": 123696, "start": 1237.8400000000001, "end": 1243.3600000000001, "text": " max pooling layers, which Jeffrey Hinton is famously grumpy about, and rightly so,", "tokens": [50408, 11469, 7005, 278, 7914, 11, 597, 28721, 389, 12442, 307, 34360, 677, 36142, 466, 11, 293, 32879, 370, 11, 50684], "temperature": 0.0, "avg_logprob": -0.1310749407167788, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.0034275008365511894}, {"id": 222, "seek": 123696, "start": 1243.3600000000001, "end": 1247.2, "text": " just throws away everything except for the max. You reduce the amount of features,", "tokens": [50684, 445, 19251, 1314, 1203, 3993, 337, 264, 11469, 13, 509, 5407, 264, 2372, 295, 4122, 11, 50876], "temperature": 0.0, "avg_logprob": -0.1310749407167788, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.0034275008365511894}, {"id": 223, "seek": 123696, "start": 1247.2, "end": 1252.96, "text": " you unlock the ability to model images, you have scalability up to your point. This is what we", "tokens": [50876, 291, 11634, 264, 3485, 281, 2316, 5267, 11, 291, 362, 15664, 2310, 493, 281, 428, 935, 13, 639, 307, 437, 321, 51164], "temperature": 0.0, "avg_logprob": -0.1310749407167788, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.0034275008365511894}, {"id": 224, "seek": 123696, "start": 1252.96, "end": 1257.68, "text": " famously know about image models. Now there's been a saturation point. Everyone who switched", "tokens": [51164, 34360, 458, 466, 3256, 5245, 13, 823, 456, 311, 668, 257, 27090, 935, 13, 5198, 567, 16858, 51400], "temperature": 0.0, "avg_logprob": -0.1310749407167788, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.0034275008365511894}, {"id": 225, "seek": 123696, "start": 1257.68, "end": 1262.88, "text": " to transformers because there's a new arc. So what I mean by this is your algorithm is kind of your", "tokens": [51400, 281, 4088, 433, 570, 456, 311, 257, 777, 10346, 13, 407, 437, 286, 914, 538, 341, 307, 428, 9284, 307, 733, 295, 428, 51660], "temperature": 0.0, "avg_logprob": -0.1310749407167788, "compression_ratio": 1.6236559139784945, "no_speech_prob": 0.0034275008365511894}, {"id": 226, "seek": 126288, "start": 1262.96, "end": 1272.0800000000002, "text": " most heavy prior on your search space. And it's Richard's right that what plays well with compute", "tokens": [50368, 881, 4676, 4059, 322, 428, 3164, 1901, 13, 400, 309, 311, 9809, 311, 558, 300, 437, 5749, 731, 365, 14722, 50824], "temperature": 0.0, "avg_logprob": -0.09960890925207803, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.0049002463929355145}, {"id": 227, "seek": 126288, "start": 1272.0800000000002, "end": 1278.24, "text": " is the one we're going to default to. But the question becomes your scaling laws and your ability", "tokens": [50824, 307, 264, 472, 321, 434, 516, 281, 7576, 281, 13, 583, 264, 1168, 3643, 428, 21589, 6064, 293, 428, 3485, 51132], "temperature": 0.0, "avg_logprob": -0.09960890925207803, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.0049002463929355145}, {"id": 228, "seek": 126288, "start": 1278.24, "end": 1284.16, "text": " to predict the future are essentially limited to algorithm and compute. And that's what's", "tokens": [51132, 281, 6069, 264, 2027, 366, 4476, 5567, 281, 9284, 293, 14722, 13, 400, 300, 311, 437, 311, 51428], "temperature": 0.0, "avg_logprob": -0.09960890925207803, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.0049002463929355145}, {"id": 229, "seek": 126288, "start": 1284.16, "end": 1288.0, "text": " interesting is that it means we're not very good at predicting the future because it means we're", "tokens": [51428, 1880, 307, 300, 309, 1355, 321, 434, 406, 588, 665, 412, 32884, 264, 2027, 570, 309, 1355, 321, 434, 51620], "temperature": 0.0, "avg_logprob": -0.09960890925207803, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.0049002463929355145}, {"id": 230, "seek": 128800, "start": 1288.0, "end": 1293.52, "text": " too locked in to this narrow arc of this architecture we use combined with compute.", "tokens": [50364, 886, 9376, 294, 281, 341, 9432, 10346, 295, 341, 9482, 321, 764, 9354, 365, 14722, 13, 50640], "temperature": 0.0, "avg_logprob": -0.16086731969782736, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.004991195164620876}, {"id": 231, "seek": 128800, "start": 1293.52, "end": 1298.0, "text": " Yeah. I mean, even the CNN example is, I think an example that proves Richard wrong,", "tokens": [50640, 865, 13, 286, 914, 11, 754, 264, 24859, 1365, 307, 11, 286, 519, 364, 1365, 300, 25019, 9809, 2085, 11, 50864], "temperature": 0.0, "avg_logprob": -0.16086731969782736, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.004991195164620876}, {"id": 232, "seek": 128800, "start": 1298.0, "end": 1303.68, "text": " because he said in the bitter lesson that any attempt to impute hand-designed priors like", "tokens": [50864, 570, 415, 848, 294, 264, 13871, 6898, 300, 604, 5217, 281, 704, 1169, 1011, 12, 14792, 16690, 1790, 830, 411, 51148], "temperature": 0.0, "avg_logprob": -0.16086731969782736, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.004991195164620876}, {"id": 233, "seek": 128800, "start": 1303.68, "end": 1310.48, "text": " symmetry or CNN is a symmetry. So in the CNN, it encodes a symmetry and scale invariance.", "tokens": [51148, 25440, 420, 24859, 307, 257, 25440, 13, 407, 294, 264, 24859, 11, 309, 2058, 4789, 257, 25440, 293, 4373, 33270, 719, 13, 51488], "temperature": 0.0, "avg_logprob": -0.16086731969782736, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.004991195164620876}, {"id": 234, "seek": 128800, "start": 1310.48, "end": 1315.2, "text": " And essentially, all it's doing is a shortcut because it's still in MLP at the end. So what", "tokens": [51488, 400, 4476, 11, 439, 309, 311, 884, 307, 257, 24822, 570, 309, 311, 920, 294, 21601, 47, 412, 264, 917, 13, 407, 437, 51724], "temperature": 0.0, "avg_logprob": -0.16086731969782736, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.004991195164620876}, {"id": 235, "seek": 131520, "start": 1315.2, "end": 1321.52, "text": " it's doing is it's basically it's building an MLP as if you didn't have the scale and symmetry in", "tokens": [50364, 309, 311, 884, 307, 309, 311, 1936, 309, 311, 2390, 364, 21601, 47, 382, 498, 291, 994, 380, 362, 264, 4373, 293, 25440, 294, 50680], "temperature": 0.0, "avg_logprob": -0.1299084636652581, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.002545480150729418}, {"id": 236, "seek": 131520, "start": 1321.52, "end": 1325.52, "text": " there. But it's just kind of like doing this thing and it's like, you know, it's embedding it", "tokens": [50680, 456, 13, 583, 309, 311, 445, 733, 295, 411, 884, 341, 551, 293, 309, 311, 411, 11, 291, 458, 11, 309, 311, 12240, 3584, 309, 50880], "temperature": 0.0, "avg_logprob": -0.1299084636652581, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.002545480150729418}, {"id": 237, "seek": 131520, "start": 1325.52, "end": 1329.28, "text": " all into the MLP. But it's basically still an MLP with a symmetry shortcut, which was hand-designed.", "tokens": [50880, 439, 666, 264, 21601, 47, 13, 583, 309, 311, 1936, 920, 364, 21601, 47, 365, 257, 25440, 24822, 11, 597, 390, 1011, 12, 14792, 16690, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1299084636652581, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.002545480150729418}, {"id": 238, "seek": 131520, "start": 1330.88, "end": 1337.68, "text": " So yeah, yeah, but there's still this notion though that there's, you know, connectionists think", "tokens": [51148, 407, 1338, 11, 1338, 11, 457, 456, 311, 920, 341, 10710, 1673, 300, 456, 311, 11, 291, 458, 11, 4984, 1751, 519, 51488], "temperature": 0.0, "avg_logprob": -0.1299084636652581, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.002545480150729418}, {"id": 239, "seek": 133768, "start": 1337.68, "end": 1345.04, "text": " that, I mean, like Neil Nanda said to me, he's like a rationalist guide from DeepMind and now,", "tokens": [50364, 300, 11, 286, 914, 11, 411, 18615, 426, 5575, 848, 281, 385, 11, 415, 311, 411, 257, 15090, 468, 5934, 490, 14895, 44, 471, 293, 586, 11, 50732], "temperature": 0.0, "avg_logprob": -0.19534861852252294, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.08809932321310043}, {"id": 240, "seek": 133768, "start": 1345.04, "end": 1347.92, "text": " yeah, DeepMind. And he said, these things are just smarter than you, man.", "tokens": [50732, 1338, 11, 14895, 44, 471, 13, 400, 415, 848, 11, 613, 721, 366, 445, 20294, 813, 291, 11, 587, 13, 50876], "temperature": 0.0, "avg_logprob": -0.19534861852252294, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.08809932321310043}, {"id": 241, "seek": 133768, "start": 1350.0800000000002, "end": 1354.72, "text": " How did that make you feel? Well, I mean, not great, not great, but you know, there's this whole", "tokens": [50984, 1012, 630, 300, 652, 291, 841, 30, 1042, 11, 286, 914, 11, 406, 869, 11, 406, 869, 11, 457, 291, 458, 11, 456, 311, 341, 1379, 51216], "temperature": 0.0, "avg_logprob": -0.19534861852252294, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.08809932321310043}, {"id": 242, "seek": 133768, "start": 1354.72, "end": 1362.48, "text": " like mech-interp thing. And I think that they genuinely believe that there's just some deep", "tokens": [51216, 411, 385, 339, 12, 5106, 79, 551, 13, 400, 286, 519, 300, 436, 17839, 1697, 300, 456, 311, 445, 512, 2452, 51604], "temperature": 0.0, "avg_logprob": -0.19534861852252294, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.08809932321310043}, {"id": 243, "seek": 133768, "start": 1362.48, "end": 1367.04, "text": " form of inscrutable intelligence going on. There was that like monosomanticity paper from", "tokens": [51604, 1254, 295, 1028, 10757, 32148, 7599, 516, 322, 13, 821, 390, 300, 411, 1108, 329, 298, 7128, 507, 3035, 490, 51832], "temperature": 0.0, "avg_logprob": -0.19534861852252294, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.08809932321310043}, {"id": 244, "seek": 136704, "start": 1367.12, "end": 1370.8799999999999, "text": " Anthropic recently. And you know, there's this deep belief that there's something", "tokens": [50368, 12727, 39173, 3938, 13, 400, 291, 458, 11, 456, 311, 341, 2452, 7107, 300, 456, 311, 746, 50556], "temperature": 0.0, "avg_logprob": -0.11112237183944039, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.0008760071941651404}, {"id": 245, "seek": 136704, "start": 1370.8799999999999, "end": 1376.6399999999999, "text": " really interesting going on. What do you think? I think that it is, there are persuasive,", "tokens": [50556, 534, 1880, 516, 322, 13, 708, 360, 291, 519, 30, 286, 519, 300, 309, 307, 11, 456, 366, 16336, 23686, 11, 50844], "temperature": 0.0, "avg_logprob": -0.11112237183944039, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.0008760071941651404}, {"id": 246, "seek": 136704, "start": 1378.56, "end": 1385.04, "text": " here's what I will say. Language is very powerful, which is why we connect and with this technology", "tokens": [50940, 510, 311, 437, 286, 486, 584, 13, 24445, 307, 588, 4005, 11, 597, 307, 983, 321, 1745, 293, 365, 341, 2899, 51264], "temperature": 0.0, "avg_logprob": -0.11112237183944039, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.0008760071941651404}, {"id": 247, "seek": 136704, "start": 1385.04, "end": 1390.24, "text": " so much, because language is how we connect with each other emotionally. It's very tied to how we,", "tokens": [51264, 370, 709, 11, 570, 2856, 307, 577, 321, 1745, 365, 1184, 661, 17991, 13, 467, 311, 588, 9601, 281, 577, 321, 11, 51524], "temperature": 0.0, "avg_logprob": -0.11112237183944039, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.0008760071941651404}, {"id": 248, "seek": 136704, "start": 1390.24, "end": 1395.52, "text": " as humans, are quite tribal. And so whenever a model learns a distribution that is indistinguishable", "tokens": [51524, 382, 6255, 11, 366, 1596, 20958, 13, 400, 370, 5699, 257, 2316, 27152, 257, 7316, 300, 307, 1016, 468, 7050, 742, 712, 51788], "temperature": 0.0, "avg_logprob": -0.11112237183944039, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.0008760071941651404}, {"id": 249, "seek": 139552, "start": 1395.52, "end": 1402.0, "text": " from humans, I think that it gives pause. And it is, I do think that these conversations are", "tokens": [50364, 490, 6255, 11, 286, 519, 300, 309, 2709, 10465, 13, 400, 309, 307, 11, 286, 360, 519, 300, 613, 7315, 366, 50688], "temperature": 0.0, "avg_logprob": -0.050667888816745804, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.011313351802527905}, {"id": 250, "seek": 139552, "start": 1402.0, "end": 1408.4, "text": " useful because it gives worthy pause to how this technology is used. And for example, I'm very", "tokens": [50688, 4420, 570, 309, 2709, 14829, 10465, 281, 577, 341, 2899, 307, 1143, 13, 400, 337, 1365, 11, 286, 478, 588, 51008], "temperature": 0.0, "avg_logprob": -0.050667888816745804, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.011313351802527905}, {"id": 251, "seek": 139552, "start": 1408.4, "end": 1417.2, "text": " against many of the efforts to sometimes deploy these algorithms without notifying humans that", "tokens": [51008, 1970, 867, 295, 264, 6484, 281, 2171, 7274, 613, 14642, 1553, 406, 5489, 6255, 300, 51448], "temperature": 0.0, "avg_logprob": -0.050667888816745804, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.011313351802527905}, {"id": 252, "seek": 139552, "start": 1417.2, "end": 1421.6, "text": " it's an algorithm. I think that you should always be aware when you're talking to an algorithm.", "tokens": [51448, 309, 311, 364, 9284, 13, 286, 519, 300, 291, 820, 1009, 312, 3650, 562, 291, 434, 1417, 281, 364, 9284, 13, 51668], "temperature": 0.0, "avg_logprob": -0.050667888816745804, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.011313351802527905}, {"id": 253, "seek": 142160, "start": 1421.6, "end": 1426.8, "text": " And that's because these are quite convincing sometimes. And so it's very important that we", "tokens": [50364, 400, 300, 311, 570, 613, 366, 1596, 24823, 2171, 13, 400, 370, 309, 311, 588, 1021, 300, 321, 50624], "temperature": 0.0, "avg_logprob": -0.07930202179766715, "compression_ratio": 1.6329113924050633, "no_speech_prob": 0.003879435360431671}, {"id": 254, "seek": 142160, "start": 1426.8, "end": 1431.9199999999998, "text": " always communicate what is the role of the model and the human. Do I think that there's a higher", "tokens": [50624, 1009, 7890, 437, 307, 264, 3090, 295, 264, 2316, 293, 264, 1952, 13, 1144, 286, 519, 300, 456, 311, 257, 2946, 50880], "temperature": 0.0, "avg_logprob": -0.07930202179766715, "compression_ratio": 1.6329113924050633, "no_speech_prob": 0.003879435360431671}, {"id": 255, "seek": 142160, "start": 1431.9199999999998, "end": 1440.56, "text": " reasoning here? I don't. I think that in many ways, whenever you have persuasive interpolation of a", "tokens": [50880, 21577, 510, 30, 286, 500, 380, 13, 286, 519, 300, 294, 867, 2098, 11, 5699, 291, 362, 16336, 23686, 44902, 399, 295, 257, 51312], "temperature": 0.0, "avg_logprob": -0.07930202179766715, "compression_ratio": 1.6329113924050633, "no_speech_prob": 0.003879435360431671}, {"id": 256, "seek": 142160, "start": 1440.56, "end": 1446.56, "text": " space between ideas, it's going to be surprising to us. I think what delights us is the creativity", "tokens": [51312, 1901, 1296, 3487, 11, 309, 311, 516, 281, 312, 8830, 281, 505, 13, 286, 519, 437, 1103, 5761, 505, 307, 264, 12915, 51612], "temperature": 0.0, "avg_logprob": -0.07930202179766715, "compression_ratio": 1.6329113924050633, "no_speech_prob": 0.003879435360431671}, {"id": 257, "seek": 144656, "start": 1446.56, "end": 1453.52, "text": " and the surprise and element. But is this ability to reason? I don't think so. I think that there", "tokens": [50364, 293, 264, 6365, 293, 4478, 13, 583, 307, 341, 3485, 281, 1778, 30, 286, 500, 380, 519, 370, 13, 286, 519, 300, 456, 50712], "temperature": 0.0, "avg_logprob": -0.07069190640315831, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.07989205420017242}, {"id": 258, "seek": 144656, "start": 1455.28, "end": 1460.6399999999999, "text": " is a clear relationship with the type of architecture we have of a memorization", "tokens": [50800, 307, 257, 1850, 2480, 365, 264, 2010, 295, 9482, 321, 362, 295, 257, 10560, 2144, 51068], "temperature": 0.0, "avg_logprob": -0.07069190640315831, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.07989205420017242}, {"id": 259, "seek": 144656, "start": 1460.6399999999999, "end": 1466.6399999999999, "text": " relationship. And we know this. We know that when we increase scale, we learn a given architecture.", "tokens": [51068, 2480, 13, 400, 321, 458, 341, 13, 492, 458, 300, 562, 321, 3488, 4373, 11, 321, 1466, 257, 2212, 9482, 13, 51368], "temperature": 0.0, "avg_logprob": -0.07069190640315831, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.07989205420017242}, {"id": 260, "seek": 144656, "start": 1466.6399999999999, "end": 1471.12, "text": " Or when we do different tricks to compensate for scale, so we can go smaller and still learn", "tokens": [51368, 1610, 562, 321, 360, 819, 11733, 281, 29458, 337, 4373, 11, 370, 321, 393, 352, 4356, 293, 920, 1466, 51592], "temperature": 0.0, "avg_logprob": -0.07069190640315831, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.07989205420017242}, {"id": 261, "seek": 144656, "start": 1471.12, "end": 1474.72, "text": " things, what we're really doing is we're just trying to induce good memorization", "tokens": [51592, 721, 11, 437, 321, 434, 534, 884, 307, 321, 434, 445, 1382, 281, 41263, 665, 10560, 2144, 51772], "temperature": 0.0, "avg_logprob": -0.07069190640315831, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.07989205420017242}, {"id": 262, "seek": 147472, "start": 1475.52, "end": 1479.6000000000001, "text": " and good steering towards part of the distribution we care about. Frankly, while", "tokens": [50404, 293, 665, 14823, 3030, 644, 295, 264, 7316, 321, 1127, 466, 13, 41344, 11, 1339, 50608], "temperature": 0.0, "avg_logprob": -0.09604073339892973, "compression_ratio": 1.8566552901023892, "no_speech_prob": 0.008974261581897736}, {"id": 263, "seek": 147472, "start": 1479.6000000000001, "end": 1486.08, "text": " why all these optimization tricks have worked beyond compute to reduce compute has been", "tokens": [50608, 983, 439, 613, 19618, 11733, 362, 2732, 4399, 14722, 281, 5407, 14722, 575, 668, 50932], "temperature": 0.0, "avg_logprob": -0.09604073339892973, "compression_ratio": 1.8566552901023892, "no_speech_prob": 0.008974261581897736}, {"id": 264, "seek": 147472, "start": 1486.08, "end": 1489.76, "text": " we're largely training on a distribution we don't want at the end of the day. So we start by", "tokens": [50932, 321, 434, 11611, 3097, 322, 257, 7316, 321, 500, 380, 528, 412, 264, 917, 295, 264, 786, 13, 407, 321, 722, 538, 51116], "temperature": 0.0, "avg_logprob": -0.09604073339892973, "compression_ratio": 1.8566552901023892, "no_speech_prob": 0.008974261581897736}, {"id": 265, "seek": 147472, "start": 1489.76, "end": 1496.0, "text": " training on the internet. And we actually don't want the internet when we engage with these models.", "tokens": [51116, 3097, 322, 264, 4705, 13, 400, 321, 767, 500, 380, 528, 264, 4705, 562, 321, 4683, 365, 613, 5245, 13, 51428], "temperature": 0.0, "avg_logprob": -0.09604073339892973, "compression_ratio": 1.8566552901023892, "no_speech_prob": 0.008974261581897736}, {"id": 266, "seek": 147472, "start": 1496.0, "end": 1500.16, "text": " We want something that's very chatty and philosophical and wise. And so a lot of what", "tokens": [51428, 492, 528, 746, 300, 311, 588, 5081, 874, 293, 25066, 293, 10829, 13, 400, 370, 257, 688, 295, 437, 51636], "temperature": 0.0, "avg_logprob": -0.09604073339892973, "compression_ratio": 1.8566552901023892, "no_speech_prob": 0.008974261581897736}, {"id": 267, "seek": 147472, "start": 1500.16, "end": 1504.32, "text": " we're doing is we're trying to steer things towards the part the tiny sliver of the distribution", "tokens": [51636, 321, 434, 884, 307, 321, 434, 1382, 281, 30814, 721, 3030, 264, 644, 264, 5870, 1061, 1837, 295, 264, 7316, 51844], "temperature": 0.0, "avg_logprob": -0.09604073339892973, "compression_ratio": 1.8566552901023892, "no_speech_prob": 0.008974261581897736}, {"id": 268, "seek": 150432, "start": 1504.32, "end": 1509.12, "text": " that training data that we care about. And that's why we have so many optimization tricks", "tokens": [50364, 300, 3097, 1412, 300, 321, 1127, 466, 13, 400, 300, 311, 983, 321, 362, 370, 867, 19618, 11733, 50604], "temperature": 0.0, "avg_logprob": -0.11542271492176485, "compression_ratio": 1.840531561461794, "no_speech_prob": 0.0027096744161099195}, {"id": 269, "seek": 150432, "start": 1509.76, "end": 1514.1599999999999, "text": " before we get to the end. But that's fascinating because what's that that's really telling you is", "tokens": [50636, 949, 321, 483, 281, 264, 917, 13, 583, 300, 311, 10343, 570, 437, 311, 300, 300, 311, 534, 3585, 291, 307, 50856], "temperature": 0.0, "avg_logprob": -0.11542271492176485, "compression_ratio": 1.840531561461794, "no_speech_prob": 0.0027096744161099195}, {"id": 270, "seek": 150432, "start": 1514.1599999999999, "end": 1518.1599999999999, "text": " that unlike a traditional machine learning problem where you're training to the data set is the", "tokens": [50856, 300, 8343, 257, 5164, 3479, 2539, 1154, 689, 291, 434, 3097, 281, 264, 1412, 992, 307, 264, 51056], "temperature": 0.0, "avg_logprob": -0.11542271492176485, "compression_ratio": 1.840531561461794, "no_speech_prob": 0.0027096744161099195}, {"id": 271, "seek": 150432, "start": 1518.1599999999999, "end": 1523.4399999999998, "text": " distribution you want to learn, a lot of what we're doing with language is we're unlearning.", "tokens": [51056, 7316, 291, 528, 281, 1466, 11, 257, 688, 295, 437, 321, 434, 884, 365, 2856, 307, 321, 434, 25272, 2341, 13, 51320], "temperature": 0.0, "avg_logprob": -0.11542271492176485, "compression_ratio": 1.840531561461794, "no_speech_prob": 0.0027096744161099195}, {"id": 272, "seek": 150432, "start": 1523.4399999999998, "end": 1527.6, "text": " We're just trying to steer and unlearning nor and then focus on what we want. So it's really", "tokens": [51320, 492, 434, 445, 1382, 281, 30814, 293, 25272, 2341, 6051, 293, 550, 1879, 322, 437, 321, 528, 13, 407, 309, 311, 534, 51528], "temperature": 0.0, "avg_logprob": -0.11542271492176485, "compression_ratio": 1.840531561461794, "no_speech_prob": 0.0027096744161099195}, {"id": 273, "seek": 150432, "start": 1527.6, "end": 1532.3999999999999, "text": " interesting. Yeah, machine unlearning. That's fascinating. I'm also a big fan of the", "tokens": [51528, 1880, 13, 865, 11, 3479, 25272, 2341, 13, 663, 311, 10343, 13, 286, 478, 611, 257, 955, 3429, 295, 264, 51768], "temperature": 0.0, "avg_logprob": -0.11542271492176485, "compression_ratio": 1.840531561461794, "no_speech_prob": 0.0027096744161099195}, {"id": 274, "seek": 153240, "start": 1532.4, "end": 1536.48, "text": " externalist tradition in cognitive science, you know, like for recognition. And in that sense,", "tokens": [50364, 8320, 468, 6994, 294, 15605, 3497, 11, 291, 458, 11, 411, 337, 11150, 13, 400, 294, 300, 2020, 11, 50568], "temperature": 0.0, "avg_logprob": -0.13192538973651355, "compression_ratio": 1.898876404494382, "no_speech_prob": 0.003859593067318201}, {"id": 275, "seek": 153240, "start": 1536.48, "end": 1539.92, "text": " I think it doesn't make sense to draw a boundary around the model, because I think, you know,", "tokens": [50568, 286, 519, 309, 1177, 380, 652, 2020, 281, 2642, 257, 12866, 926, 264, 2316, 11, 570, 286, 519, 11, 291, 458, 11, 50740], "temperature": 0.0, "avg_logprob": -0.13192538973651355, "compression_ratio": 1.898876404494382, "no_speech_prob": 0.003859593067318201}, {"id": 276, "seek": 153240, "start": 1539.92, "end": 1544.3200000000002, "text": " our sense making semantics, situated knowledge and so on, it's kind of observe a relative anyway,", "tokens": [50740, 527, 2020, 1455, 4361, 45298, 11, 30143, 3601, 293, 370, 322, 11, 309, 311, 733, 295, 11441, 257, 4972, 4033, 11, 50960], "temperature": 0.0, "avg_logprob": -0.13192538973651355, "compression_ratio": 1.898876404494382, "no_speech_prob": 0.003859593067318201}, {"id": 277, "seek": 153240, "start": 1544.3200000000002, "end": 1548.96, "text": " you know, these things are embedded in our culture. And sense making humans put prompts in there and", "tokens": [50960, 291, 458, 11, 613, 721, 366, 16741, 294, 527, 3713, 13, 400, 2020, 1455, 6255, 829, 41095, 294, 456, 293, 51192], "temperature": 0.0, "avg_logprob": -0.13192538973651355, "compression_ratio": 1.898876404494382, "no_speech_prob": 0.003859593067318201}, {"id": 278, "seek": 153240, "start": 1548.96, "end": 1552.72, "text": " they interpret the outputs. And actually, even the data generating process that went into building", "tokens": [51192, 436, 7302, 264, 23930, 13, 400, 767, 11, 754, 264, 1412, 17746, 1399, 300, 1437, 666, 2390, 51380], "temperature": 0.0, "avg_logprob": -0.13192538973651355, "compression_ratio": 1.898876404494382, "no_speech_prob": 0.003859593067318201}, {"id": 279, "seek": 153240, "start": 1552.72, "end": 1556.4, "text": " these things was, you know, originated from the universe, we're all agents and we're all in the", "tokens": [51380, 613, 721, 390, 11, 291, 458, 11, 31129, 490, 264, 6445, 11, 321, 434, 439, 12554, 293, 321, 434, 439, 294, 264, 51564], "temperature": 0.0, "avg_logprob": -0.13192538973651355, "compression_ratio": 1.898876404494382, "no_speech_prob": 0.003859593067318201}, {"id": 280, "seek": 153240, "start": 1556.4, "end": 1560.4, "text": " physical world and the social world. And we, you know, we're doing the effect of computation,", "tokens": [51564, 4001, 1002, 293, 264, 2093, 1002, 13, 400, 321, 11, 291, 458, 11, 321, 434, 884, 264, 1802, 295, 24903, 11, 51764], "temperature": 0.0, "avg_logprob": -0.13192538973651355, "compression_ratio": 1.898876404494382, "no_speech_prob": 0.003859593067318201}, {"id": 281, "seek": 156040, "start": 1560.4, "end": 1564.4, "text": " both in how the models are built and how they are used and interpreted and evaluated and so on.", "tokens": [50364, 1293, 294, 577, 264, 5245, 366, 3094, 293, 577, 436, 366, 1143, 293, 26749, 293, 25509, 293, 370, 322, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12248197283063617, "compression_ratio": 1.76, "no_speech_prob": 0.017685724422335625}, {"id": 282, "seek": 156040, "start": 1564.4, "end": 1568.3200000000002, "text": " So what are you going to do? Like draw a big, should you include the flops of the universe as", "tokens": [50564, 407, 437, 366, 291, 516, 281, 360, 30, 1743, 2642, 257, 955, 11, 820, 291, 4090, 264, 932, 3370, 295, 264, 6445, 382, 50760], "temperature": 0.0, "avg_logprob": -0.12248197283063617, "compression_ratio": 1.76, "no_speech_prob": 0.017685724422335625}, {"id": 283, "seek": 156040, "start": 1568.3200000000002, "end": 1574.0, "text": " well? Oh, that's fascinating. Yeah, there is this interesting, yeah, it does spark something else with", "tokens": [50760, 731, 30, 876, 11, 300, 311, 10343, 13, 865, 11, 456, 307, 341, 1880, 11, 1338, 11, 309, 775, 9908, 746, 1646, 365, 51044], "temperature": 0.0, "avg_logprob": -0.12248197283063617, "compression_ratio": 1.76, "no_speech_prob": 0.017685724422335625}, {"id": 284, "seek": 156040, "start": 1574.64, "end": 1578.96, "text": " flops, which is that, so typically the final model that you deliver is only one of the possible", "tokens": [51076, 932, 3370, 11, 597, 307, 300, 11, 370, 5850, 264, 2572, 2316, 300, 291, 4239, 307, 787, 472, 295, 264, 1944, 51292], "temperature": 0.0, "avg_logprob": -0.12248197283063617, "compression_ratio": 1.76, "no_speech_prob": 0.017685724422335625}, {"id": 285, "seek": 156040, "start": 1578.96, "end": 1585.0400000000002, "text": " models, right? So in fact, typically, even at massive scale, you train many candidate models,", "tokens": [51292, 5245, 11, 558, 30, 407, 294, 1186, 11, 5850, 11, 754, 412, 5994, 4373, 11, 291, 3847, 867, 11532, 5245, 11, 51596], "temperature": 0.0, "avg_logprob": -0.12248197283063617, "compression_ratio": 1.76, "no_speech_prob": 0.017685724422335625}, {"id": 286, "seek": 156040, "start": 1585.0400000000002, "end": 1588.5600000000002, "text": " and then you choose the best one. And so it's interesting because these are not optimized", "tokens": [51596, 293, 550, 291, 2826, 264, 1151, 472, 13, 400, 370, 309, 311, 1880, 570, 613, 366, 406, 26941, 51772], "temperature": 0.0, "avg_logprob": -0.12248197283063617, "compression_ratio": 1.76, "no_speech_prob": 0.017685724422335625}, {"id": 287, "seek": 158856, "start": 1588.56, "end": 1592.96, "text": " together, but they implicitly optimized through the selection process. And it is really interesting", "tokens": [50364, 1214, 11, 457, 436, 26947, 356, 26941, 807, 264, 9450, 1399, 13, 400, 309, 307, 534, 1880, 50584], "temperature": 0.0, "avg_logprob": -0.14831302040501645, "compression_ratio": 1.6125827814569536, "no_speech_prob": 0.022199120372533798}, {"id": 288, "seek": 158856, "start": 1592.96, "end": 1597.28, "text": " because we kind of steer towards what we want. So yeah, it's a fascinating dynamic. I didn't think", "tokens": [50584, 570, 321, 733, 295, 30814, 3030, 437, 321, 528, 13, 407, 1338, 11, 309, 311, 257, 10343, 8546, 13, 286, 994, 380, 519, 50800], "temperature": 0.0, "avg_logprob": -0.14831302040501645, "compression_ratio": 1.6125827814569536, "no_speech_prob": 0.022199120372533798}, {"id": 289, "seek": 158856, "start": 1597.28, "end": 1602.48, "text": " of it like that, though, that's an even bigger meta approach for thinking about this. Hardware", "tokens": [50800, 295, 309, 411, 300, 11, 1673, 11, 300, 311, 364, 754, 3801, 19616, 3109, 337, 1953, 466, 341, 13, 11817, 3039, 51060], "temperature": 0.0, "avg_logprob": -0.14831302040501645, "compression_ratio": 1.6125827814569536, "no_speech_prob": 0.022199120372533798}, {"id": 290, "seek": 158856, "start": 1602.48, "end": 1606.6399999999999, "text": " Lottery Paper, which we talked about, and that was a really fun conversation because I remember it", "tokens": [51060, 20131, 12733, 24990, 11, 597, 321, 2825, 466, 11, 293, 300, 390, 257, 534, 1019, 3761, 570, 286, 1604, 309, 51268], "temperature": 0.0, "avg_logprob": -0.14831302040501645, "compression_ratio": 1.6125827814569536, "no_speech_prob": 0.022199120372533798}, {"id": 291, "seek": 158856, "start": 1606.6399999999999, "end": 1614.56, "text": " was when you were doing the trio kind of the group of ML Street talk, the earlier version. And", "tokens": [51268, 390, 562, 291, 645, 884, 264, 37274, 733, 295, 264, 1594, 295, 21601, 7638, 751, 11, 264, 3071, 3037, 13, 400, 51664], "temperature": 0.0, "avg_logprob": -0.14831302040501645, "compression_ratio": 1.6125827814569536, "no_speech_prob": 0.022199120372533798}, {"id": 292, "seek": 161456, "start": 1615.36, "end": 1620.96, "text": " I think you originally invited me onto the show because there was this idea that I wrote about,", "tokens": [50404, 286, 519, 291, 7993, 9185, 385, 3911, 264, 855, 570, 456, 390, 341, 1558, 300, 286, 4114, 466, 11, 50684], "temperature": 0.0, "avg_logprob": -0.11637469372117376, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.003532283240929246}, {"id": 293, "seek": 161456, "start": 1620.96, "end": 1628.0, "text": " which is that most of computer science history has been driven by whether your idea works with", "tokens": [50684, 597, 307, 300, 881, 295, 3820, 3497, 2503, 575, 668, 9555, 538, 1968, 428, 1558, 1985, 365, 51036], "temperature": 0.0, "avg_logprob": -0.11637469372117376, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.003532283240929246}, {"id": 294, "seek": 161456, "start": 1628.0, "end": 1633.84, "text": " available hardware or not. And I think that resonated with a lot of people at the time,", "tokens": [51036, 2435, 8837, 420, 406, 13, 400, 286, 519, 300, 47957, 365, 257, 688, 295, 561, 412, 264, 565, 11, 51328], "temperature": 0.0, "avg_logprob": -0.11637469372117376, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.003532283240929246}, {"id": 295, "seek": 161456, "start": 1633.84, "end": 1638.8, "text": " because what it's really saying is that we may be in another hardware lottery right now,", "tokens": [51328, 570, 437, 309, 311, 534, 1566, 307, 300, 321, 815, 312, 294, 1071, 8837, 27391, 558, 586, 11, 51576], "temperature": 0.0, "avg_logprob": -0.11637469372117376, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.003532283240929246}, {"id": 296, "seek": 163880, "start": 1638.8, "end": 1645.12, "text": " that something like Transformers, which we all use, has become increasingly locked in to GPUs", "tokens": [50364, 300, 746, 411, 27938, 433, 11, 597, 321, 439, 764, 11, 575, 1813, 12980, 9376, 294, 281, 18407, 82, 50680], "temperature": 0.0, "avg_logprob": -0.12612031520098105, "compression_ratio": 1.5147679324894514, "no_speech_prob": 0.09631936252117157}, {"id": 297, "seek": 163880, "start": 1645.12, "end": 1649.9199999999998, "text": " and to TPUs, which have all been built to accelerate this one hardware. So it raises the", "tokens": [50680, 293, 281, 314, 8115, 82, 11, 597, 362, 439, 668, 3094, 281, 21341, 341, 472, 8837, 13, 407, 309, 19658, 264, 50920], "temperature": 0.0, "avg_logprob": -0.12612031520098105, "compression_ratio": 1.5147679324894514, "no_speech_prob": 0.09631936252117157}, {"id": 298, "seek": 163880, "start": 1649.9199999999998, "end": 1655.84, "text": " question of what next and how do we make sure that the next brilliant idea isn't stuck in", "tokens": [50920, 1168, 295, 437, 958, 293, 577, 360, 321, 652, 988, 300, 264, 958, 10248, 1558, 1943, 380, 5541, 294, 51216], "temperature": 0.0, "avg_logprob": -0.12612031520098105, "compression_ratio": 1.5147679324894514, "no_speech_prob": 0.09631936252117157}, {"id": 299, "seek": 163880, "start": 1655.84, "end": 1660.1599999999999, "text": " purgatory for decades, because that's what happened to deep neural networks. It simply", "tokens": [51216, 1864, 70, 4745, 337, 7878, 11, 570, 300, 311, 437, 2011, 281, 2452, 18161, 9590, 13, 467, 2935, 51432], "temperature": 0.0, "avg_logprob": -0.12612031520098105, "compression_ratio": 1.5147679324894514, "no_speech_prob": 0.09631936252117157}, {"id": 300, "seek": 166016, "start": 1660.16, "end": 1669.2, "text": " didn't work until TPUs were converted from video game use, which was really not the intended", "tokens": [50364, 994, 380, 589, 1826, 314, 8115, 82, 645, 16424, 490, 960, 1216, 764, 11, 597, 390, 534, 406, 264, 10226, 50816], "temperature": 0.0, "avg_logprob": -0.06332666434130622, "compression_ratio": 1.667844522968198, "no_speech_prob": 0.29618945717811584}, {"id": 301, "seek": 166016, "start": 1669.2, "end": 1673.3600000000001, "text": " purpose of how they were converted to work for machine learning workloads. And that happened", "tokens": [50816, 4334, 295, 577, 436, 645, 16424, 281, 589, 337, 3479, 2539, 32452, 13, 400, 300, 2011, 51024], "temperature": 0.0, "avg_logprob": -0.06332666434130622, "compression_ratio": 1.667844522968198, "no_speech_prob": 0.29618945717811584}, {"id": 302, "seek": 166016, "start": 1673.3600000000001, "end": 1677.68, "text": " over the course of a decade. It was a very slow conversion process, but that turned out to be", "tokens": [51024, 670, 264, 1164, 295, 257, 10378, 13, 467, 390, 257, 588, 2964, 14298, 1399, 11, 457, 300, 3574, 484, 281, 312, 51240], "temperature": 0.0, "avg_logprob": -0.06332666434130622, "compression_ratio": 1.667844522968198, "no_speech_prob": 0.29618945717811584}, {"id": 303, "seek": 166016, "start": 1677.68, "end": 1683.28, "text": " the key for deep neural networks. What we now identify as 2012, the moment that this explosion", "tokens": [51240, 264, 2141, 337, 2452, 18161, 9590, 13, 708, 321, 586, 5876, 382, 9125, 11, 264, 1623, 300, 341, 15673, 51520], "temperature": 0.0, "avg_logprob": -0.06332666434130622, "compression_ratio": 1.667844522968198, "no_speech_prob": 0.29618945717811584}, {"id": 304, "seek": 166016, "start": 1683.28, "end": 1689.3600000000001, "text": " of interest and funding and acceleration happened. People identify that with convolutional neural", "tokens": [51520, 295, 1179, 293, 6137, 293, 17162, 2011, 13, 3432, 5876, 300, 365, 45216, 304, 18161, 51824], "temperature": 0.0, "avg_logprob": -0.06332666434130622, "compression_ratio": 1.667844522968198, "no_speech_prob": 0.29618945717811584}, {"id": 305, "seek": 168936, "start": 1689.36, "end": 1694.8, "text": " networks or the algorithm, but really it was both. It was the hardware making the algorithm", "tokens": [50364, 9590, 420, 264, 9284, 11, 457, 534, 309, 390, 1293, 13, 467, 390, 264, 8837, 1455, 264, 9284, 50636], "temperature": 0.0, "avg_logprob": -0.11113339883309824, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.004708963446319103}, {"id": 306, "seek": 168936, "start": 1694.8, "end": 1699.84, "text": " feasible. And that's when you first had the empirical proof that deep neural networks were viable.", "tokens": [50636, 26648, 13, 400, 300, 311, 562, 291, 700, 632, 264, 31886, 8177, 300, 2452, 18161, 9590, 645, 22024, 13, 50888], "temperature": 0.0, "avg_logprob": -0.11113339883309824, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.004708963446319103}, {"id": 307, "seek": 168936, "start": 1700.6399999999999, "end": 1703.36, "text": " To what extent do you think there is an algorithm lottery as well?", "tokens": [50928, 1407, 437, 8396, 360, 291, 519, 456, 307, 364, 9284, 27391, 382, 731, 30, 51064], "temperature": 0.0, "avg_logprob": -0.11113339883309824, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.004708963446319103}, {"id": 308, "seek": 168936, "start": 1704.4799999999998, "end": 1710.08, "text": " Oh, what do you mean by that? Well, as in now, your paper was about the basin of attraction of", "tokens": [51120, 876, 11, 437, 360, 291, 914, 538, 300, 30, 1042, 11, 382, 294, 586, 11, 428, 3035, 390, 466, 264, 34863, 295, 17672, 295, 51400], "temperature": 0.0, "avg_logprob": -0.11113339883309824, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.004708963446319103}, {"id": 309, "seek": 168936, "start": 1710.08, "end": 1715.84, "text": " hardware. But is there a basin of attraction of algorithms as well? Absolutely. I mean, you just", "tokens": [51400, 8837, 13, 583, 307, 456, 257, 34863, 295, 17672, 295, 14642, 382, 731, 30, 7021, 13, 286, 914, 11, 291, 445, 51688], "temperature": 0.0, "avg_logprob": -0.11113339883309824, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.004708963446319103}, {"id": 310, "seek": 171584, "start": 1715.84, "end": 1723.9199999999998, "text": " have to look at optimizers to see that. So what I mean by that is an algorithm is really how you", "tokens": [50364, 362, 281, 574, 412, 5028, 22525, 281, 536, 300, 13, 407, 437, 286, 914, 538, 300, 307, 364, 9284, 307, 534, 577, 291, 50768], "temperature": 0.0, "avg_logprob": -0.07592232694330904, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.012020266614854336}, {"id": 311, "seek": 171584, "start": 1723.9199999999998, "end": 1730.8, "text": " learn from data. This is the essence of an algorithm. And what we've been locked into is this idea that", "tokens": [50768, 1466, 490, 1412, 13, 639, 307, 264, 12801, 295, 364, 9284, 13, 400, 437, 321, 600, 668, 9376, 666, 307, 341, 1558, 300, 51112], "temperature": 0.0, "avg_logprob": -0.07592232694330904, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.012020266614854336}, {"id": 312, "seek": 171584, "start": 1730.8, "end": 1736.6399999999999, "text": " it has to be gradient based optimization. It's really hard to do something that's a non-differentiable", "tokens": [51112, 309, 575, 281, 312, 16235, 2361, 19618, 13, 467, 311, 534, 1152, 281, 360, 746, 300, 311, 257, 2107, 12, 67, 15790, 9364, 51404], "temperature": 0.0, "avg_logprob": -0.07592232694330904, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.012020266614854336}, {"id": 313, "seek": 171584, "start": 1736.6399999999999, "end": 1743.52, "text": " objective. And what that means kind of in accessible terms is that we're stuck doing these", "tokens": [51404, 10024, 13, 400, 437, 300, 1355, 733, 295, 294, 9515, 2115, 307, 300, 321, 434, 5541, 884, 613, 51748], "temperature": 0.0, "avg_logprob": -0.07592232694330904, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.012020266614854336}, {"id": 314, "seek": 174352, "start": 1743.6, "end": 1749.2, "text": " global updates. So the way our models train is that we kind of send through shovel through data,", "tokens": [50368, 4338, 9205, 13, 407, 264, 636, 527, 5245, 3847, 307, 300, 321, 733, 295, 2845, 807, 29789, 807, 1412, 11, 50648], "temperature": 0.0, "avg_logprob": -0.09446400862473708, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.015168958343565464}, {"id": 315, "seek": 174352, "start": 1749.2, "end": 1753.92, "text": " and then the update to the weights is based on an average of all the data that's seen.", "tokens": [50648, 293, 550, 264, 5623, 281, 264, 17443, 307, 2361, 322, 364, 4274, 295, 439, 264, 1412, 300, 311, 1612, 13, 50884], "temperature": 0.0, "avg_logprob": -0.09446400862473708, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.015168958343565464}, {"id": 316, "seek": 174352, "start": 1754.48, "end": 1759.44, "text": " Why is that tricky for a few reasons? Because why does it mean that we overfit to the average?", "tokens": [50912, 1545, 307, 300, 12414, 337, 257, 1326, 4112, 30, 1436, 983, 775, 309, 914, 300, 321, 670, 6845, 281, 264, 4274, 30, 51160], "temperature": 0.0, "avg_logprob": -0.09446400862473708, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.015168958343565464}, {"id": 317, "seek": 174352, "start": 1759.44, "end": 1763.76, "text": " And that's why we need so much training data. Because essentially, if you're just overfitting", "tokens": [51160, 400, 300, 311, 983, 321, 643, 370, 709, 3097, 1412, 13, 1436, 4476, 11, 498, 291, 434, 445, 670, 69, 2414, 51376], "temperature": 0.0, "avg_logprob": -0.09446400862473708, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.015168958343565464}, {"id": 318, "seek": 174352, "start": 1763.76, "end": 1768.96, "text": " to the average, it takes ages to learn the rare patterns. So you train for longer, you need more", "tokens": [51376, 281, 264, 4274, 11, 309, 2516, 12357, 281, 1466, 264, 5892, 8294, 13, 407, 291, 3847, 337, 2854, 11, 291, 643, 544, 51636], "temperature": 0.0, "avg_logprob": -0.09446400862473708, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.015168958343565464}, {"id": 319, "seek": 176896, "start": 1768.96, "end": 1775.52, "text": " data. But the other thing that's very tricky is that it means that models forget. So every time", "tokens": [50364, 1412, 13, 583, 264, 661, 551, 300, 311, 588, 12414, 307, 300, 309, 1355, 300, 5245, 2870, 13, 407, 633, 565, 50692], "temperature": 0.0, "avg_logprob": -0.05463628354279891, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.03619551286101341}, {"id": 320, "seek": 176896, "start": 1775.52, "end": 1780.0, "text": " you shovel in new data, the model forgets the old data because you're updating everything at once.", "tokens": [50692, 291, 29789, 294, 777, 1412, 11, 264, 2316, 2870, 82, 264, 1331, 1412, 570, 291, 434, 25113, 1203, 412, 1564, 13, 50916], "temperature": 0.0, "avg_logprob": -0.05463628354279891, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.03619551286101341}, {"id": 321, "seek": 176896, "start": 1780.0, "end": 1785.3600000000001, "text": " A nice point of contrast is that as humans, we typically have long-term memory and short-term", "tokens": [50916, 316, 1481, 935, 295, 8712, 307, 300, 382, 6255, 11, 321, 5850, 362, 938, 12, 7039, 4675, 293, 2099, 12, 7039, 51184], "temperature": 0.0, "avg_logprob": -0.05463628354279891, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.03619551286101341}, {"id": 322, "seek": 176896, "start": 1785.3600000000001, "end": 1789.1200000000001, "text": " memory. These are different ways of learning, and the rate of learning is different. And so when", "tokens": [51184, 4675, 13, 1981, 366, 819, 2098, 295, 2539, 11, 293, 264, 3314, 295, 2539, 307, 819, 13, 400, 370, 562, 51372], "temperature": 0.0, "avg_logprob": -0.05463628354279891, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.03619551286101341}, {"id": 323, "seek": 176896, "start": 1789.1200000000001, "end": 1794.24, "text": " you process information, some are stored in your long-term memory. You may have a distinctive memory", "tokens": [51372, 291, 1399, 1589, 11, 512, 366, 12187, 294, 428, 938, 12, 7039, 4675, 13, 509, 815, 362, 257, 27766, 4675, 51628], "temperature": 0.0, "avg_logprob": -0.05463628354279891, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.03619551286101341}, {"id": 324, "seek": 179424, "start": 1794.32, "end": 1800.08, "text": " from a child that you think is like your first memory from a child. And it may be mutated over", "tokens": [50368, 490, 257, 1440, 300, 291, 519, 307, 411, 428, 700, 4675, 490, 257, 1440, 13, 400, 309, 815, 312, 5839, 770, 670, 50656], "temperature": 0.0, "avg_logprob": -0.08218475839366084, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.0323043093085289}, {"id": 325, "seek": 179424, "start": 1800.08, "end": 1804.8, "text": " time. That's the nature of memory. But this ability to preserve two states of what you did today,", "tokens": [50656, 565, 13, 663, 311, 264, 3687, 295, 4675, 13, 583, 341, 3485, 281, 15665, 732, 4368, 295, 437, 291, 630, 965, 11, 50892], "temperature": 0.0, "avg_logprob": -0.08218475839366084, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.0323043093085289}, {"id": 326, "seek": 179424, "start": 1804.8, "end": 1810.72, "text": " what you did years ago, that's very different from gradient updates. And somehow, because we", "tokens": [50892, 437, 291, 630, 924, 2057, 11, 300, 311, 588, 819, 490, 16235, 9205, 13, 400, 6063, 11, 570, 321, 51188], "temperature": 0.0, "avg_logprob": -0.08218475839366084, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.0323043093085289}, {"id": 327, "seek": 179424, "start": 1810.72, "end": 1814.88, "text": " haven't found an alternative way, even though a lot of people have worked on it, we are in this", "tokens": [51188, 2378, 380, 1352, 364, 8535, 636, 11, 754, 1673, 257, 688, 295, 561, 362, 2732, 322, 309, 11, 321, 366, 294, 341, 51396], "temperature": 0.0, "avg_logprob": -0.08218475839366084, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.0323043093085289}, {"id": 328, "seek": 179424, "start": 1814.88, "end": 1820.4, "text": " algorithm based on where it's very tricky to propose an algorithm that doesn't rely on a", "tokens": [51396, 9284, 2361, 322, 689, 309, 311, 588, 12414, 281, 17421, 364, 9284, 300, 1177, 380, 10687, 322, 257, 51672], "temperature": 0.0, "avg_logprob": -0.08218475839366084, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.0323043093085289}, {"id": 329, "seek": 182040, "start": 1820.4, "end": 1826.5600000000002, "text": " differentiable objective. Yes. And I think we'll talk about this later. But part of the problem", "tokens": [50364, 819, 9364, 10024, 13, 1079, 13, 400, 286, 519, 321, 603, 751, 466, 341, 1780, 13, 583, 644, 295, 264, 1154, 50672], "temperature": 0.0, "avg_logprob": -0.10041956718151386, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.006797799840569496}, {"id": 330, "seek": 182040, "start": 1826.5600000000002, "end": 1831.44, "text": " is people think of this paradigm as a form of general abstract pure intelligence. And the", "tokens": [50672, 307, 561, 519, 295, 341, 24709, 382, 257, 1254, 295, 2674, 12649, 6075, 7599, 13, 400, 264, 50916], "temperature": 0.0, "avg_logprob": -0.10041956718151386, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.006797799840569496}, {"id": 331, "seek": 182040, "start": 1831.44, "end": 1837.3600000000001, "text": " reality is that certainly with your multilingual work, that we're dealing with just this long tail", "tokens": [50916, 4103, 307, 300, 3297, 365, 428, 2120, 38219, 589, 11, 300, 321, 434, 6260, 365, 445, 341, 938, 6838, 51212], "temperature": 0.0, "avg_logprob": -0.10041956718151386, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.006797799840569496}, {"id": 332, "seek": 182040, "start": 1837.3600000000001, "end": 1843.3600000000001, "text": " of complexity, heterogeneous data sets. But maybe that's a good segue because you just released", "tokens": [51212, 295, 14024, 11, 20789, 31112, 1412, 6352, 13, 583, 1310, 300, 311, 257, 665, 33850, 570, 291, 445, 4736, 51512], "temperature": 0.0, "avg_logprob": -0.10041956718151386, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.006797799840569496}, {"id": 333, "seek": 182040, "start": 1844.72, "end": 1848.8000000000002, "text": " this primer paper called the AI language gap. Can you tell us about that?", "tokens": [51580, 341, 12595, 3035, 1219, 264, 7318, 2856, 7417, 13, 1664, 291, 980, 505, 466, 300, 30, 51784], "temperature": 0.0, "avg_logprob": -0.10041956718151386, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.006797799840569496}, {"id": 334, "seek": 184880, "start": 1849.68, "end": 1856.32, "text": " So it's quite fun because in some ways, what you're talking about these themes leads so nice into", "tokens": [50408, 407, 309, 311, 1596, 1019, 570, 294, 512, 2098, 11, 437, 291, 434, 1417, 466, 613, 13544, 6689, 370, 1481, 666, 50740], "temperature": 0.0, "avg_logprob": -0.10936738064414576, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0039190324023365974}, {"id": 335, "seek": 184880, "start": 1856.32, "end": 1864.48, "text": " the AI language gap. Really, when we have built these models, we've overfitted to what is weighted", "tokens": [50740, 264, 7318, 2856, 7417, 13, 4083, 11, 562, 321, 362, 3094, 613, 5245, 11, 321, 600, 670, 69, 3944, 281, 437, 307, 32807, 51148], "temperature": 0.0, "avg_logprob": -0.10936738064414576, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0039190324023365974}, {"id": 336, "seek": 184880, "start": 1864.48, "end": 1868.6399999999999, "text": " most importantly to those who built it. And these models have been built in a few places.", "tokens": [51148, 881, 8906, 281, 729, 567, 3094, 309, 13, 400, 613, 5245, 362, 668, 3094, 294, 257, 1326, 3190, 13, 51356], "temperature": 0.0, "avg_logprob": -0.10936738064414576, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0039190324023365974}, {"id": 337, "seek": 184880, "start": 1869.36, "end": 1875.6, "text": " We're in London. London is a very big hub of where researchers have been. So is the US and", "tokens": [51392, 492, 434, 294, 7042, 13, 7042, 307, 257, 588, 955, 11838, 295, 689, 10309, 362, 668, 13, 407, 307, 264, 2546, 293, 51704], "temperature": 0.0, "avg_logprob": -0.10936738064414576, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0039190324023365974}, {"id": 338, "seek": 187560, "start": 1875.6799999999998, "end": 1882.0, "text": " Europe and China. And because some of the first impressive large language models were built in", "tokens": [50368, 3315, 293, 3533, 13, 400, 570, 512, 295, 264, 700, 8992, 2416, 2856, 5245, 645, 3094, 294, 50684], "temperature": 0.0, "avg_logprob": -0.08963718197562477, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0028657028451561928}, {"id": 339, "seek": 187560, "start": 1882.0, "end": 1889.84, "text": " the US and the UK with DeepMind, and in the US with places like Coher and places like OpenAI,", "tokens": [50684, 264, 2546, 293, 264, 7051, 365, 14895, 44, 471, 11, 293, 294, 264, 2546, 365, 3190, 411, 3066, 511, 293, 3190, 411, 7238, 48698, 11, 51076], "temperature": 0.0, "avg_logprob": -0.08963718197562477, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0028657028451561928}, {"id": 340, "seek": 187560, "start": 1889.84, "end": 1893.84, "text": " I think that that has necessarily reflected the nature of the researchers who built them.", "tokens": [51076, 286, 519, 300, 300, 575, 4725, 15502, 264, 3687, 295, 264, 10309, 567, 3094, 552, 13, 51276], "temperature": 0.0, "avg_logprob": -0.08963718197562477, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0028657028451561928}, {"id": 341, "seek": 187560, "start": 1893.84, "end": 1900.8799999999999, "text": " They wanted to work in English. The tricky thing is that when you try and make AI actually work", "tokens": [51276, 814, 1415, 281, 589, 294, 3669, 13, 440, 12414, 551, 307, 300, 562, 291, 853, 293, 652, 7318, 767, 589, 51628], "temperature": 0.0, "avg_logprob": -0.08963718197562477, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0028657028451561928}, {"id": 342, "seek": 190088, "start": 1900.96, "end": 1906.4, "text": " for the world, you're talking about this vast array of different languages. So there's 7,000", "tokens": [50368, 337, 264, 1002, 11, 291, 434, 1417, 466, 341, 8369, 10225, 295, 819, 8650, 13, 407, 456, 311, 1614, 11, 1360, 50640], "temperature": 0.0, "avg_logprob": -0.07321831192633119, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.11040324717760086}, {"id": 343, "seek": 190088, "start": 1906.4, "end": 1913.0400000000002, "text": " languages in the world. 80% of those have no text data. So it's truly not even a language problem.", "tokens": [50640, 8650, 294, 264, 1002, 13, 4688, 4, 295, 729, 362, 572, 2487, 1412, 13, 407, 309, 311, 4908, 406, 754, 257, 2856, 1154, 13, 50972], "temperature": 0.0, "avg_logprob": -0.07321831192633119, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.11040324717760086}, {"id": 344, "seek": 190088, "start": 1913.0400000000002, "end": 1920.4, "text": " It's also a multimodal problem. The second part is that even with the top 101 languages, no models", "tokens": [50972, 467, 311, 611, 257, 32972, 378, 304, 1154, 13, 440, 1150, 644, 307, 300, 754, 365, 264, 1192, 21055, 8650, 11, 572, 5245, 51340], "temperature": 0.0, "avg_logprob": -0.07321831192633119, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.11040324717760086}, {"id": 345, "seek": 190088, "start": 1920.4, "end": 1926.48, "text": " except for Io101 currently cover it. So there's this vast amount of the world that simply isn't", "tokens": [51340, 3993, 337, 19239, 47520, 4362, 2060, 309, 13, 407, 456, 311, 341, 8369, 2372, 295, 264, 1002, 300, 2935, 1943, 380, 51644], "temperature": 0.0, "avg_logprob": -0.07321831192633119, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.11040324717760086}, {"id": 346, "seek": 192648, "start": 1926.48, "end": 1933.68, "text": " reflected in the way that AI works and who AI serves. The primer about the language gap is", "tokens": [50364, 15502, 294, 264, 636, 300, 7318, 1985, 293, 567, 7318, 13451, 13, 440, 12595, 466, 264, 2856, 7417, 307, 50724], "temperature": 0.0, "avg_logprob": -0.11272346542542239, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.04926180839538574}, {"id": 347, "seek": 192648, "start": 1933.68, "end": 1937.92, "text": " really calling attention to this. But at the root of this problem and what you're getting at with", "tokens": [50724, 534, 5141, 3202, 281, 341, 13, 583, 412, 264, 5593, 295, 341, 1154, 293, 437, 291, 434, 1242, 412, 365, 50936], "temperature": 0.0, "avg_logprob": -0.11272346542542239, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.04926180839538574}, {"id": 348, "seek": 192648, "start": 1937.92, "end": 1943.3600000000001, "text": " this theme of how does models work with the long tail is that the fundamental issue is", "tokens": [50936, 341, 6314, 295, 577, 775, 5245, 589, 365, 264, 938, 6838, 307, 300, 264, 8088, 2734, 307, 51208], "temperature": 0.0, "avg_logprob": -0.11272346542542239, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.04926180839538574}, {"id": 349, "seek": 192648, "start": 1944.0, "end": 1950.88, "text": " our models really overfit to high frequency patterns. And so the key difficulty with the", "tokens": [51240, 527, 5245, 534, 670, 6845, 281, 1090, 7893, 8294, 13, 400, 370, 264, 2141, 10360, 365, 264, 51584], "temperature": 0.0, "avg_logprob": -0.11272346542542239, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.04926180839538574}, {"id": 350, "seek": 195088, "start": 1950.88, "end": 1957.44, "text": " language gap is that, one, these languages typically are underserved by available data on", "tokens": [50364, 2856, 7417, 307, 300, 11, 472, 11, 613, 8650, 5850, 366, 16692, 6913, 538, 2435, 1412, 322, 50692], "temperature": 0.0, "avg_logprob": -0.0834191974840666, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.04259670898318291}, {"id": 351, "seek": 195088, "start": 1957.44, "end": 1962.4, "text": " the internet. The internet kind of reflects early patterns of adoption, not necessarily humanity", "tokens": [50692, 264, 4705, 13, 440, 4705, 733, 295, 18926, 2440, 8294, 295, 19215, 11, 406, 4725, 10243, 50940], "temperature": 0.0, "avg_logprob": -0.0834191974840666, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.04259670898318291}, {"id": 352, "seek": 195088, "start": 1962.4, "end": 1967.2800000000002, "text": " as it is. So that means that there's way more English on the internet than there is people who", "tokens": [50940, 382, 309, 307, 13, 407, 300, 1355, 300, 456, 311, 636, 544, 3669, 322, 264, 4705, 813, 456, 307, 561, 567, 51184], "temperature": 0.0, "avg_logprob": -0.0834191974840666, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.04259670898318291}, {"id": 353, "seek": 195088, "start": 1967.2800000000002, "end": 1973.8400000000001, "text": " speak English. So 5% of homes speak English, but 50% of the internet is in English. In contrast,", "tokens": [51184, 1710, 3669, 13, 407, 1025, 4, 295, 7388, 1710, 3669, 11, 457, 2625, 4, 295, 264, 4705, 307, 294, 3669, 13, 682, 8712, 11, 51512], "temperature": 0.0, "avg_logprob": -0.0834191974840666, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.04259670898318291}, {"id": 354, "seek": 195088, "start": 1973.8400000000001, "end": 1979.3600000000001, "text": " something like Yoruba, spoken by 40 million people, is really underserved. And so it's a long tail", "tokens": [51512, 746, 411, 398, 284, 12584, 11, 10759, 538, 3356, 2459, 561, 11, 307, 534, 16692, 6913, 13, 400, 370, 309, 311, 257, 938, 6838, 51788], "temperature": 0.0, "avg_logprob": -0.0834191974840666, "compression_ratio": 1.7220216606498195, "no_speech_prob": 0.04259670898318291}, {"id": 355, "seek": 197936, "start": 1979.36, "end": 1984.32, "text": " problem. But here's the other thing, it's a pattern where the rich get richer and the poor get poorer", "tokens": [50364, 1154, 13, 583, 510, 311, 264, 661, 551, 11, 309, 311, 257, 5102, 689, 264, 4593, 483, 29021, 293, 264, 4716, 483, 49740, 50612], "temperature": 0.0, "avg_logprob": -0.08137861887613933, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.007075286004692316}, {"id": 356, "seek": 197936, "start": 1984.32, "end": 1990.32, "text": " because we're now in a synthetic data era. So as models get much better at generating English and", "tokens": [50612, 570, 321, 434, 586, 294, 257, 23420, 1412, 4249, 13, 407, 382, 5245, 483, 709, 1101, 412, 17746, 3669, 293, 50912], "temperature": 0.0, "avg_logprob": -0.08137861887613933, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.007075286004692316}, {"id": 357, "seek": 197936, "start": 1990.32, "end": 1994.8799999999999, "text": " Chinese in particular, these are the two high resource languages that are well served, you're", "tokens": [50912, 4649, 294, 1729, 11, 613, 366, 264, 732, 1090, 7684, 8650, 300, 366, 731, 7584, 11, 291, 434, 51140], "temperature": 0.0, "avg_logprob": -0.08137861887613933, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.007075286004692316}, {"id": 358, "seek": 197936, "start": 1994.8799999999999, "end": 2000.7199999999998, "text": " going to see more content in those two languages. And that makes it even harder if you're relying on", "tokens": [51140, 516, 281, 536, 544, 2701, 294, 729, 732, 8650, 13, 400, 300, 1669, 309, 754, 6081, 498, 291, 434, 24140, 322, 51432], "temperature": 0.0, "avg_logprob": -0.08137861887613933, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.007075286004692316}, {"id": 359, "seek": 197936, "start": 2000.7199999999998, "end": 2006.4799999999998, "text": " large data to properly represent the languages that are currently underserved. Yeah, so interesting", "tokens": [51432, 2416, 1412, 281, 6108, 2906, 264, 8650, 300, 366, 4362, 16692, 6913, 13, 865, 11, 370, 1880, 51720], "temperature": 0.0, "avg_logprob": -0.08137861887613933, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.007075286004692316}, {"id": 360, "seek": 200648, "start": 2006.48, "end": 2012.24, "text": " because we're moving away from the material world into the information world. And right now in the", "tokens": [50364, 570, 321, 434, 2684, 1314, 490, 264, 2527, 1002, 666, 264, 1589, 1002, 13, 400, 558, 586, 294, 264, 50652], "temperature": 0.0, "avg_logprob": -0.062515774288693, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.013660425320267677}, {"id": 361, "seek": 200648, "start": 2012.24, "end": 2016.16, "text": " material world, there is a kind of a commensurate relationship between the number of people who", "tokens": [50652, 2527, 1002, 11, 456, 307, 257, 733, 295, 257, 800, 694, 33144, 2480, 1296, 264, 1230, 295, 561, 567, 50848], "temperature": 0.0, "avg_logprob": -0.062515774288693, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.013660425320267677}, {"id": 362, "seek": 200648, "start": 2016.16, "end": 2020.0, "text": " speak English and the amount of data on the internet. And as you say, we're now moving to this place", "tokens": [50848, 1710, 3669, 293, 264, 2372, 295, 1412, 322, 264, 4705, 13, 400, 382, 291, 584, 11, 321, 434, 586, 2684, 281, 341, 1081, 51040], "temperature": 0.0, "avg_logprob": -0.062515774288693, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.013660425320267677}, {"id": 363, "seek": 200648, "start": 2020.0, "end": 2025.92, "text": " where we are generating data of language and the polarization is going to increase. So you're talking", "tokens": [51040, 689, 321, 366, 17746, 1412, 295, 2856, 293, 264, 37736, 307, 516, 281, 3488, 13, 407, 291, 434, 1417, 51336], "temperature": 0.0, "avg_logprob": -0.062515774288693, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.013660425320267677}, {"id": 364, "seek": 200648, "start": 2025.92, "end": 2032.24, "text": " about there's this kind of North American tech based inequality, which is getting worse. And you", "tokens": [51336, 466, 456, 311, 341, 733, 295, 4067, 2665, 7553, 2361, 16970, 11, 597, 307, 1242, 5324, 13, 400, 291, 51652], "temperature": 0.0, "avg_logprob": -0.062515774288693, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.013660425320267677}, {"id": 365, "seek": 203224, "start": 2032.24, "end": 2036.56, "text": " said that there are safety implications for this. I was interested in this word safety, we spoke", "tokens": [50364, 848, 300, 456, 366, 4514, 16602, 337, 341, 13, 286, 390, 3102, 294, 341, 1349, 4514, 11, 321, 7179, 50580], "temperature": 0.0, "avg_logprob": -0.10484172859970405, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.05444762855768204}, {"id": 366, "seek": 203224, "start": 2036.56, "end": 2043.6, "text": " about this last night. Because when I think of AI safety, I think of X risk in Silicon Valley and", "tokens": [50580, 466, 341, 1036, 1818, 13, 1436, 562, 286, 519, 295, 7318, 4514, 11, 286, 519, 295, 1783, 3148, 294, 25351, 10666, 293, 50932], "temperature": 0.0, "avg_logprob": -0.10484172859970405, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.05444762855768204}, {"id": 367, "seek": 203224, "start": 2043.6, "end": 2050.16, "text": " stuff like that. And I've noticed over the years, the conflation of the two communities in terms of", "tokens": [50932, 1507, 411, 300, 13, 400, 286, 600, 5694, 670, 264, 924, 11, 264, 1497, 24278, 295, 264, 732, 4456, 294, 2115, 295, 51260], "temperature": 0.0, "avg_logprob": -0.10484172859970405, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.05444762855768204}, {"id": 368, "seek": 203224, "start": 2050.16, "end": 2058.48, "text": " ethics and existential risk. And how do you feel about that? I think it's, I mean, I feel grumpy", "tokens": [51260, 19769, 293, 37133, 3148, 13, 400, 577, 360, 291, 841, 466, 300, 30, 286, 519, 309, 311, 11, 286, 914, 11, 286, 841, 677, 36142, 51676], "temperature": 0.0, "avg_logprob": -0.10484172859970405, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.05444762855768204}, {"id": 369, "seek": 205848, "start": 2058.56, "end": 2063.04, "text": " about that. But here's the thing. So, you know, subfields are always like this. I think there's", "tokens": [50368, 466, 300, 13, 583, 510, 311, 264, 551, 13, 407, 11, 291, 458, 11, 1422, 7610, 82, 366, 1009, 411, 341, 13, 286, 519, 456, 311, 50592], "temperature": 0.0, "avg_logprob": -0.10462681631023964, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.07459576427936554}, {"id": 370, "seek": 205848, "start": 2063.04, "end": 2069.36, "text": " always this notion of subfields, which are extremely, you know, actually people caring about the same", "tokens": [50592, 1009, 341, 10710, 295, 1422, 7610, 82, 11, 597, 366, 4664, 11, 291, 458, 11, 767, 561, 15365, 466, 264, 912, 50908], "temperature": 0.0, "avg_logprob": -0.10462681631023964, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.07459576427936554}, {"id": 371, "seek": 205848, "start": 2069.36, "end": 2075.92, "text": " objectives, trying to distinguish themselves over time. AI safety encompasses a large array of", "tokens": [50908, 15961, 11, 1382, 281, 20206, 2969, 670, 565, 13, 7318, 4514, 49866, 257, 2416, 10225, 295, 51236], "temperature": 0.0, "avg_logprob": -0.10462681631023964, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.07459576427936554}, {"id": 372, "seek": 205848, "start": 2075.92, "end": 2083.6, "text": " perspectives and expertise and people who care about different things. I think that this shift", "tokens": [51236, 16766, 293, 11769, 293, 561, 567, 1127, 466, 819, 721, 13, 286, 519, 300, 341, 5513, 51620], "temperature": 0.0, "avg_logprob": -0.10462681631023964, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.07459576427936554}, {"id": 373, "seek": 208360, "start": 2083.68, "end": 2088.7999999999997, "text": " towards talking about from response for AI to AI safety is a fascinating one,", "tokens": [50368, 3030, 1417, 466, 490, 4134, 337, 7318, 281, 7318, 4514, 307, 257, 10343, 472, 11, 50624], "temperature": 0.0, "avg_logprob": -0.1357709582489316, "compression_ratio": 1.75, "no_speech_prob": 0.07461079955101013}, {"id": 374, "seek": 208360, "start": 2088.7999999999997, "end": 2094.64, "text": " because it's been a bit intentional from communities who want to maybe suggest that", "tokens": [50624, 570, 309, 311, 668, 257, 857, 21935, 490, 4456, 567, 528, 281, 1310, 3402, 300, 50916], "temperature": 0.0, "avg_logprob": -0.1357709582489316, "compression_ratio": 1.75, "no_speech_prob": 0.07461079955101013}, {"id": 375, "seek": 208360, "start": 2095.52, "end": 2101.68, "text": " response for AI is distinct from what they're doing. And instead saying AI safety is about", "tokens": [50960, 4134, 337, 7318, 307, 10644, 490, 437, 436, 434, 884, 13, 400, 2602, 1566, 7318, 4514, 307, 466, 51268], "temperature": 0.0, "avg_logprob": -0.1357709582489316, "compression_ratio": 1.75, "no_speech_prob": 0.07461079955101013}, {"id": 376, "seek": 208360, "start": 2101.68, "end": 2107.36, "text": " these profound risks, these like fundamental issues of our time. And response for AI is,", "tokens": [51268, 613, 14382, 10888, 11, 613, 411, 8088, 2663, 295, 527, 565, 13, 400, 4134, 337, 7318, 307, 11, 51552], "temperature": 0.0, "avg_logprob": -0.1357709582489316, "compression_ratio": 1.75, "no_speech_prob": 0.07461079955101013}, {"id": 377, "seek": 208360, "start": 2108.08, "end": 2112.4, "text": " okay, great, you're doing that, but keep going. And so I do think there's a very interesting", "tokens": [51588, 1392, 11, 869, 11, 291, 434, 884, 300, 11, 457, 1066, 516, 13, 400, 370, 286, 360, 519, 456, 311, 257, 588, 1880, 51804], "temperature": 0.0, "avg_logprob": -0.1357709582489316, "compression_ratio": 1.75, "no_speech_prob": 0.07461079955101013}, {"id": 378, "seek": 211240, "start": 2112.4, "end": 2117.76, "text": " thing with how we name things and how we really have precision in our conversations.", "tokens": [50364, 551, 365, 577, 321, 1315, 721, 293, 577, 321, 534, 362, 18356, 294, 527, 7315, 13, 50632], "temperature": 0.0, "avg_logprob": -0.08284088734830364, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.008274529129266739}, {"id": 379, "seek": 211240, "start": 2119.12, "end": 2123.76, "text": " Increasingly, I think AI safety encompasses both of these, and you need more precise", "tokens": [50700, 30367, 3349, 356, 11, 286, 519, 7318, 4514, 49866, 1293, 295, 613, 11, 293, 291, 643, 544, 13600, 50932], "temperature": 0.0, "avg_logprob": -0.08284088734830364, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.008274529129266739}, {"id": 380, "seek": 211240, "start": 2123.76, "end": 2130.1600000000003, "text": " language with both. And I actually think my main ask is, we need to be precise about what our", "tokens": [50932, 2856, 365, 1293, 13, 400, 286, 767, 519, 452, 2135, 1029, 307, 11, 321, 643, 281, 312, 13600, 466, 437, 527, 51252], "temperature": 0.0, "avg_logprob": -0.08284088734830364, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.008274529129266739}, {"id": 381, "seek": 211240, "start": 2130.1600000000003, "end": 2138.4, "text": " objective is with AI safety. Because it can be, it is in many ways the same goals as response for AI.", "tokens": [51252, 10024, 307, 365, 7318, 4514, 13, 1436, 309, 393, 312, 11, 309, 307, 294, 867, 2098, 264, 912, 5493, 382, 4134, 337, 7318, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08284088734830364, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.008274529129266739}, {"id": 382, "seek": 213840, "start": 2139.28, "end": 2144.32, "text": " But the degree of precision when this is articulated is a sign of accountability for", "tokens": [50408, 583, 264, 4314, 295, 18356, 562, 341, 307, 43322, 307, 257, 1465, 295, 19380, 337, 50660], "temperature": 0.0, "avg_logprob": -0.10620295326664762, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.003866547951474786}, {"id": 383, "seek": 213840, "start": 2144.32, "end": 2147.6, "text": " the objective. And I think sometimes the use of that word lacks accountability.", "tokens": [50660, 264, 10024, 13, 400, 286, 519, 2171, 264, 764, 295, 300, 1349, 31132, 19380, 13, 50824], "temperature": 0.0, "avg_logprob": -0.10620295326664762, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.003866547951474786}, {"id": 384, "seek": 213840, "start": 2148.32, "end": 2155.12, "text": " Yes, exactly. And when I hear some ex-risk folks talk about AI, it feels to be in the abstract.", "tokens": [50860, 1079, 11, 2293, 13, 400, 562, 286, 1568, 512, 454, 12, 33263, 4024, 751, 466, 7318, 11, 309, 3417, 281, 312, 294, 264, 12649, 13, 51200], "temperature": 0.0, "avg_logprob": -0.10620295326664762, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.003866547951474786}, {"id": 385, "seek": 213840, "start": 2155.12, "end": 2161.84, "text": " And what I mean by that is they are just thinking about, if we scale this technology up,", "tokens": [51200, 400, 437, 286, 914, 538, 300, 307, 436, 366, 445, 1953, 466, 11, 498, 321, 4373, 341, 2899, 493, 11, 51536], "temperature": 0.0, "avg_logprob": -0.10620295326664762, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.003866547951474786}, {"id": 386, "seek": 213840, "start": 2161.84, "end": 2167.36, "text": " it learns these abstract representations, which work in any situation, and it's just a matter of", "tokens": [51536, 309, 27152, 613, 12649, 33358, 11, 597, 589, 294, 604, 2590, 11, 293, 309, 311, 445, 257, 1871, 295, 51812], "temperature": 0.0, "avg_logprob": -0.10620295326664762, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.003866547951474786}, {"id": 387, "seek": 216736, "start": 2167.36, "end": 2172.7200000000003, "text": " scale. And it feels unmoored from the research, because when I read your work about multilingual", "tokens": [50364, 4373, 13, 400, 309, 3417, 517, 3280, 2769, 490, 264, 2132, 11, 570, 562, 286, 1401, 428, 589, 466, 2120, 38219, 50632], "temperature": 0.0, "avg_logprob": -0.09590507608599368, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.003556717885658145}, {"id": 388, "seek": 216736, "start": 2172.7200000000003, "end": 2177.92, "text": " models, you're clearly pointing out that when we have what they call low resource languages,", "tokens": [50632, 5245, 11, 291, 434, 4448, 12166, 484, 300, 562, 321, 362, 437, 436, 818, 2295, 7684, 8650, 11, 50892], "temperature": 0.0, "avg_logprob": -0.09590507608599368, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.003556717885658145}, {"id": 389, "seek": 216736, "start": 2177.92, "end": 2182.0, "text": " the models don't work very well. They're not learning these abstractions that just", "tokens": [50892, 264, 5245, 500, 380, 589, 588, 731, 13, 814, 434, 406, 2539, 613, 12649, 626, 300, 445, 51096], "temperature": 0.0, "avg_logprob": -0.09590507608599368, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.003556717885658145}, {"id": 390, "seek": 216736, "start": 2182.0, "end": 2187.6, "text": " automatically work in other languages. There's a specificity to it. That seems to be the difference", "tokens": [51096, 6772, 589, 294, 661, 8650, 13, 821, 311, 257, 2685, 507, 281, 309, 13, 663, 2544, 281, 312, 264, 2649, 51376], "temperature": 0.0, "avg_logprob": -0.09590507608599368, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.003556717885658145}, {"id": 391, "seek": 216736, "start": 2187.6, "end": 2193.04, "text": " to me. Yeah, there's this big question right now, what you're getting at is there's this idea of,", "tokens": [51376, 281, 385, 13, 865, 11, 456, 311, 341, 955, 1168, 558, 586, 11, 437, 291, 434, 1242, 412, 307, 456, 311, 341, 1558, 295, 11, 51648], "temperature": 0.0, "avg_logprob": -0.09590507608599368, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.003556717885658145}, {"id": 392, "seek": 219304, "start": 2193.92, "end": 2198.24, "text": " there's a mystique that some people are attributing to scale. It's been called different", "tokens": [50408, 456, 311, 257, 9111, 1925, 300, 512, 561, 366, 9080, 10861, 281, 4373, 13, 467, 311, 668, 1219, 819, 50624], "temperature": 0.0, "avg_logprob": -0.08527882202811864, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.004130291286855936}, {"id": 393, "seek": 219304, "start": 2198.24, "end": 2202.64, "text": " things. It's this question of, are there emergent properties? Are there properties that appear from", "tokens": [50624, 721, 13, 467, 311, 341, 1168, 295, 11, 366, 456, 4345, 6930, 7221, 30, 2014, 456, 7221, 300, 4204, 490, 50844], "temperature": 0.0, "avg_logprob": -0.08527882202811864, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.004130291286855936}, {"id": 394, "seek": 219304, "start": 2202.64, "end": 2207.36, "text": " nowhere that we unlock with scale? By the way, multilingual is originally proposed as one of", "tokens": [50844, 11159, 300, 321, 11634, 365, 4373, 30, 3146, 264, 636, 11, 2120, 38219, 307, 7993, 10348, 382, 472, 295, 51080], "temperature": 0.0, "avg_logprob": -0.08527882202811864, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.004130291286855936}, {"id": 395, "seek": 219304, "start": 2207.36, "end": 2211.36, "text": " them. Like in the first paper about emergent properties, multilingual was there. It's like,", "tokens": [51080, 552, 13, 1743, 294, 264, 700, 3035, 466, 4345, 6930, 7221, 11, 2120, 38219, 390, 456, 13, 467, 311, 411, 11, 51280], "temperature": 0.0, "avg_logprob": -0.08527882202811864, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.004130291286855936}, {"id": 396, "seek": 219304, "start": 2211.36, "end": 2216.8, "text": " wow, how did this appear? We didn't even have this in our training data. But it's very interesting.", "tokens": [51280, 6076, 11, 577, 630, 341, 4204, 30, 492, 994, 380, 754, 362, 341, 294, 527, 3097, 1412, 13, 583, 309, 311, 588, 1880, 13, 51552], "temperature": 0.0, "avg_logprob": -0.08527882202811864, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.004130291286855936}, {"id": 397, "seek": 219304, "start": 2216.8, "end": 2221.44, "text": " Now there's been subsequent work which is shown. It was there all along. It just wasn't documented", "tokens": [51552, 823, 456, 311, 668, 19962, 589, 597, 307, 4898, 13, 467, 390, 456, 439, 2051, 13, 467, 445, 2067, 380, 23007, 51784], "temperature": 0.0, "avg_logprob": -0.08527882202811864, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.004130291286855936}, {"id": 398, "seek": 222144, "start": 2221.44, "end": 2226.08, "text": " in the training data. So scale is just really learning your long tail. It's learning the low", "tokens": [50364, 294, 264, 3097, 1412, 13, 407, 4373, 307, 445, 534, 2539, 428, 938, 6838, 13, 467, 311, 2539, 264, 2295, 50596], "temperature": 0.0, "avg_logprob": -0.10960792359851655, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.005632809363305569}, {"id": 399, "seek": 222144, "start": 2226.08, "end": 2231.52, "text": " frequency. We just get surprised because I think there's a big disconnect between what we think", "tokens": [50596, 7893, 13, 492, 445, 483, 6100, 570, 286, 519, 456, 311, 257, 955, 14299, 1296, 437, 321, 519, 50868], "temperature": 0.0, "avg_logprob": -0.10960792359851655, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.005632809363305569}, {"id": 400, "seek": 222144, "start": 2231.52, "end": 2236.32, "text": " we know about the vast amounts of data that we train on and what's actually in that mix.", "tokens": [50868, 321, 458, 466, 264, 8369, 11663, 295, 1412, 300, 321, 3847, 322, 293, 437, 311, 767, 294, 300, 2890, 13, 51108], "temperature": 0.0, "avg_logprob": -0.10960792359851655, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.005632809363305569}, {"id": 401, "seek": 222144, "start": 2236.32, "end": 2241.44, "text": " And so there is often certain properties where it takes scale to unlock because it's very", "tokens": [51108, 400, 370, 456, 307, 2049, 1629, 7221, 689, 309, 2516, 4373, 281, 11634, 570, 309, 311, 588, 51364], "temperature": 0.0, "avg_logprob": -0.10960792359851655, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.005632809363305569}, {"id": 402, "seek": 222144, "start": 2241.44, "end": 2246.48, "text": " relate to this question of memorization. I think how this conversation has become a bigger theme", "tokens": [51364, 10961, 281, 341, 1168, 295, 10560, 2144, 13, 286, 519, 577, 341, 3761, 575, 1813, 257, 3801, 6314, 51616], "temperature": 0.0, "avg_logprob": -0.10960792359851655, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.005632809363305569}, {"id": 403, "seek": 224648, "start": 2246.56, "end": 2252.32, "text": " beyond this scientific question of when do properties emerge and what to scale and lock,", "tokens": [50368, 4399, 341, 8134, 1168, 295, 562, 360, 7221, 21511, 293, 437, 281, 4373, 293, 4017, 11, 50656], "temperature": 0.0, "avg_logprob": -0.13305655232182256, "compression_ratio": 1.8262548262548262, "no_speech_prob": 0.003641603048890829}, {"id": 404, "seek": 224648, "start": 2252.32, "end": 2258.64, "text": " it's become this thing of kind of creating a myth around these models. That there's a lack of", "tokens": [50656, 309, 311, 1813, 341, 551, 295, 733, 295, 4084, 257, 9474, 926, 613, 5245, 13, 663, 456, 311, 257, 5011, 295, 50972], "temperature": 0.0, "avg_logprob": -0.13305655232182256, "compression_ratio": 1.8262548262548262, "no_speech_prob": 0.003641603048890829}, {"id": 405, "seek": 224648, "start": 2259.92, "end": 2265.92, "text": " ability to understand what scale gives. And then that is used to kind of impart a degree of anxiety", "tokens": [51036, 3485, 281, 1223, 437, 4373, 2709, 13, 400, 550, 300, 307, 1143, 281, 733, 295, 32177, 257, 4314, 295, 9119, 51336], "temperature": 0.0, "avg_logprob": -0.13305655232182256, "compression_ratio": 1.8262548262548262, "no_speech_prob": 0.003641603048890829}, {"id": 406, "seek": 224648, "start": 2265.92, "end": 2270.64, "text": " that because we don't know precisely when this property will emerge, there should be anxiety", "tokens": [51336, 300, 570, 321, 500, 380, 458, 13402, 562, 341, 4707, 486, 21511, 11, 456, 820, 312, 9119, 51572], "temperature": 0.0, "avg_logprob": -0.13305655232182256, "compression_ratio": 1.8262548262548262, "no_speech_prob": 0.003641603048890829}, {"id": 407, "seek": 224648, "start": 2270.64, "end": 2276.0, "text": " about this. And there should be a sense of real danger about the use of these models. And I would", "tokens": [51572, 466, 341, 13, 400, 456, 820, 312, 257, 2020, 295, 957, 4330, 466, 264, 764, 295, 613, 5245, 13, 400, 286, 576, 51840], "temperature": 0.0, "avg_logprob": -0.13305655232182256, "compression_ratio": 1.8262548262548262, "no_speech_prob": 0.003641603048890829}, {"id": 408, "seek": 227600, "start": 2276.0, "end": 2284.48, "text": " say that that is actually the wrong framing for this. The right framing is that one is the notion", "tokens": [50364, 584, 300, 300, 307, 767, 264, 2085, 28971, 337, 341, 13, 440, 558, 28971, 307, 300, 472, 307, 264, 10710, 50788], "temperature": 0.0, "avg_logprob": -0.08182319362511796, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0034173133317381144}, {"id": 409, "seek": 227600, "start": 2284.48, "end": 2290.48, "text": " that we're just going to keep on scaling I think is flawed. I think there's very clear evidence that", "tokens": [50788, 300, 321, 434, 445, 516, 281, 1066, 322, 21589, 286, 519, 307, 38823, 13, 286, 519, 456, 311, 588, 1850, 4467, 300, 51088], "temperature": 0.0, "avg_logprob": -0.08182319362511796, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0034173133317381144}, {"id": 410, "seek": 227600, "start": 2290.48, "end": 2295.44, "text": " you know bigger is not always better that we're kind of reaching the limits of how we scale with", "tokens": [51088, 291, 458, 3801, 307, 406, 1009, 1101, 300, 321, 434, 733, 295, 9906, 264, 10406, 295, 577, 321, 4373, 365, 51336], "temperature": 0.0, "avg_logprob": -0.08182319362511796, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0034173133317381144}, {"id": 411, "seek": 227600, "start": 2295.44, "end": 2299.28, "text": " something like transformers and it's very architecture bound. But the second thing that I", "tokens": [51336, 746, 411, 4088, 433, 293, 309, 311, 588, 9482, 5472, 13, 583, 264, 1150, 551, 300, 286, 51528], "temperature": 0.0, "avg_logprob": -0.08182319362511796, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0034173133317381144}, {"id": 412, "seek": 229928, "start": 2299.28, "end": 2308.1600000000003, "text": " would say is it really kind of ignores the mounting evidence that these kind of properties", "tokens": [50364, 576, 584, 307, 309, 534, 733, 295, 5335, 2706, 264, 22986, 4467, 300, 613, 733, 295, 7221, 50808], "temperature": 0.0, "avg_logprob": -0.14360202043906026, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.005467585287988186}, {"id": 413, "seek": 229928, "start": 2308.1600000000003, "end": 2311.36, "text": " are surprising only because we're not going to predict in what emerges at scale.", "tokens": [50808, 366, 8830, 787, 570, 321, 434, 406, 516, 281, 6069, 294, 437, 38965, 412, 4373, 13, 50968], "temperature": 0.0, "avg_logprob": -0.14360202043906026, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.005467585287988186}, {"id": 414, "seek": 229928, "start": 2312.1600000000003, "end": 2318.48, "text": " Yes, yes. I spoke with David Chalmers recently and he bemoans the fact that whenever we have", "tokens": [51008, 1079, 11, 2086, 13, 286, 7179, 365, 4389, 761, 304, 18552, 3938, 293, 415, 312, 3280, 599, 264, 1186, 300, 5699, 321, 362, 51324], "temperature": 0.0, "avg_logprob": -0.14360202043906026, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.005467585287988186}, {"id": 415, "seek": 229928, "start": 2318.48, "end": 2325.6000000000004, "text": " a complex system, we say, oh, it's emergent. And there is something interesting going on", "tokens": [51324, 257, 3997, 1185, 11, 321, 584, 11, 1954, 11, 309, 311, 4345, 6930, 13, 400, 456, 307, 746, 1880, 516, 322, 51680], "temperature": 0.0, "avg_logprob": -0.14360202043906026, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.005467585287988186}, {"id": 416, "seek": 232560, "start": 2325.6, "end": 2330.48, "text": " as you say that when you memorize more and more of the long tail, you do see this qualitative", "tokens": [50364, 382, 291, 584, 300, 562, 291, 27478, 544, 293, 544, 295, 264, 938, 6838, 11, 291, 360, 536, 341, 31312, 50608], "temperature": 0.0, "avg_logprob": -0.08123312498393812, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.007883592508733273}, {"id": 417, "seek": 232560, "start": 2330.48, "end": 2335.36, "text": " increase in capabilities. And it's quite easy as an observer just to say, oh, you know, it's an", "tokens": [50608, 3488, 294, 10862, 13, 400, 309, 311, 1596, 1858, 382, 364, 27878, 445, 281, 584, 11, 1954, 11, 291, 458, 11, 309, 311, 364, 50852], "temperature": 0.0, "avg_logprob": -0.08123312498393812, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.007883592508733273}, {"id": 418, "seek": 232560, "start": 2335.36, "end": 2342.16, "text": " emergent property. And people ascribe things like you know, divergent intentionality and reasoning", "tokens": [50852, 4345, 6930, 4707, 13, 400, 561, 382, 8056, 721, 411, 291, 458, 11, 18558, 6930, 7789, 1860, 293, 21577, 51192], "temperature": 0.0, "avg_logprob": -0.08123312498393812, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.007883592508733273}, {"id": 419, "seek": 232560, "start": 2342.16, "end": 2346.3199999999997, "text": " and all of these kind of anthropomorphic qualities to the models even though they probably don't", "tokens": [51192, 293, 439, 295, 613, 733, 295, 22727, 32702, 299, 16477, 281, 264, 5245, 754, 1673, 436, 1391, 500, 380, 51400], "temperature": 0.0, "avg_logprob": -0.08123312498393812, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.007883592508733273}, {"id": 420, "seek": 232560, "start": 2346.3199999999997, "end": 2351.7599999999998, "text": " really exist. But one interesting thing though is that, you know, when you memorize all of these", "tokens": [51400, 534, 2514, 13, 583, 472, 1880, 551, 1673, 307, 300, 11, 291, 458, 11, 562, 291, 27478, 439, 295, 613, 51672], "temperature": 0.0, "avg_logprob": -0.08123312498393812, "compression_ratio": 1.79182156133829, "no_speech_prob": 0.007883592508733273}, {"id": 421, "seek": 235176, "start": 2351.76, "end": 2357.1200000000003, "text": " surface statistics at scale, you can use the language model as an idea generator. And like on", "tokens": [50364, 3753, 12523, 412, 4373, 11, 291, 393, 764, 264, 2856, 2316, 382, 364, 1558, 19265, 13, 400, 411, 322, 50632], "temperature": 0.0, "avg_logprob": -0.09498143551954583, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.020090961828827858}, {"id": 422, "seek": 235176, "start": 2357.1200000000003, "end": 2362.1600000000003, "text": " Francois Chalet's arc challenge, you know, Ryan Greenblatt generated about 30,000 completions for", "tokens": [50632, 34695, 271, 761, 49744, 311, 10346, 3430, 11, 291, 458, 11, 9116, 6969, 5199, 1591, 10833, 466, 2217, 11, 1360, 1557, 626, 337, 50884], "temperature": 0.0, "avg_logprob": -0.09498143551954583, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.020090961828827858}, {"id": 423, "seek": 235176, "start": 2362.1600000000003, "end": 2367.28, "text": " all of the tasks. And the remarkable thing is in terms of sensitivity, the correct answer is in", "tokens": [50884, 439, 295, 264, 9608, 13, 400, 264, 12802, 551, 307, 294, 2115, 295, 19392, 11, 264, 3006, 1867, 307, 294, 51140], "temperature": 0.0, "avg_logprob": -0.09498143551954583, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.020090961828827858}, {"id": 424, "seek": 235176, "start": 2367.28, "end": 2372.5600000000004, "text": " those completions. And then you can do some neuro symbolic evaluation and selection and you can", "tokens": [51140, 729, 1557, 626, 13, 400, 550, 291, 393, 360, 512, 16499, 25755, 13344, 293, 9450, 293, 291, 393, 51404], "temperature": 0.0, "avg_logprob": -0.09498143551954583, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.020090961828827858}, {"id": 425, "seek": 235176, "start": 2372.5600000000004, "end": 2376.1600000000003, "text": " pull the thing out, you know, so you can build an architecture that does really well. But I think", "tokens": [51404, 2235, 264, 551, 484, 11, 291, 458, 11, 370, 291, 393, 1322, 364, 9482, 300, 775, 534, 731, 13, 583, 286, 519, 51584], "temperature": 0.0, "avg_logprob": -0.09498143551954583, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.020090961828827858}, {"id": 426, "seek": 235176, "start": 2376.1600000000003, "end": 2381.36, "text": " people underestimate the amount of human selection kind of like smoothing out the brittleness.", "tokens": [51584, 561, 35826, 264, 2372, 295, 1952, 9450, 733, 295, 411, 899, 6259, 571, 484, 264, 738, 593, 45887, 13, 51844], "temperature": 0.0, "avg_logprob": -0.09498143551954583, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.020090961828827858}, {"id": 427, "seek": 238176, "start": 2382.0800000000004, "end": 2387.0400000000004, "text": " Yeah, well, right now, I agree, there's a huge amount of creativity that's unlocked for these", "tokens": [50380, 865, 11, 731, 11, 558, 586, 11, 286, 3986, 11, 456, 311, 257, 2603, 2372, 295, 12915, 300, 311, 30180, 337, 613, 50628], "temperature": 0.0, "avg_logprob": -0.10332167559656603, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0010151235619559884}, {"id": 428, "seek": 238176, "start": 2387.0400000000004, "end": 2392.0, "text": " models. So this iteration, and actually, by the way, this idea of like, you can create a lot of", "tokens": [50628, 5245, 13, 407, 341, 24784, 11, 293, 767, 11, 538, 264, 636, 11, 341, 1558, 295, 411, 11, 291, 393, 1884, 257, 688, 295, 50876], "temperature": 0.0, "avg_logprob": -0.10332167559656603, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0010151235619559884}, {"id": 429, "seek": 238176, "start": 2392.0, "end": 2396.4, "text": " different options, and then you can verify which are correct, you see this in a lot of different", "tokens": [50876, 819, 3956, 11, 293, 550, 291, 393, 16888, 597, 366, 3006, 11, 291, 536, 341, 294, 257, 688, 295, 819, 51096], "temperature": 0.0, "avg_logprob": -0.10332167559656603, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0010151235619559884}, {"id": 430, "seek": 238176, "start": 2397.76, "end": 2401.84, "text": " kind of states of progress right now, that's how code is currently done, like you can", "tokens": [51164, 733, 295, 4368, 295, 4205, 558, 586, 11, 300, 311, 577, 3089, 307, 4362, 1096, 11, 411, 291, 393, 51368], "temperature": 0.0, "avg_logprob": -0.10332167559656603, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0010151235619559884}, {"id": 431, "seek": 238176, "start": 2401.84, "end": 2407.84, "text": " create a really nice code data set by running code and seeing which one passed the test and kind", "tokens": [51368, 1884, 257, 534, 1481, 3089, 1412, 992, 538, 2614, 3089, 293, 2577, 597, 472, 4678, 264, 1500, 293, 733, 51668], "temperature": 0.0, "avg_logprob": -0.10332167559656603, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0010151235619559884}, {"id": 432, "seek": 240784, "start": 2407.92, "end": 2413.6000000000004, "text": " of do formal verification of which ones passed. So it's not that these models are not capable of", "tokens": [50368, 295, 360, 9860, 30206, 295, 597, 2306, 4678, 13, 407, 309, 311, 406, 300, 613, 5245, 366, 406, 8189, 295, 50652], "temperature": 0.0, "avg_logprob": -0.080022540518908, "compression_ratio": 1.8295819935691318, "no_speech_prob": 0.022251777350902557}, {"id": 433, "seek": 240784, "start": 2413.6000000000004, "end": 2418.56, "text": " generating insensible answers is just that the probability on every single turn consistency", "tokens": [50652, 17746, 1028, 30633, 6338, 307, 445, 300, 264, 8482, 322, 633, 2167, 1261, 14416, 50900], "temperature": 0.0, "avg_logprob": -0.080022540518908, "compression_ratio": 1.8295819935691318, "no_speech_prob": 0.022251777350902557}, {"id": 434, "seek": 240784, "start": 2418.56, "end": 2423.76, "text": " is what you're putting out consistency is sometimes not there. And I also think part of what is", "tokens": [50900, 307, 437, 291, 434, 3372, 484, 14416, 307, 2171, 406, 456, 13, 400, 286, 611, 519, 644, 295, 437, 307, 51160], "temperature": 0.0, "avg_logprob": -0.080022540518908, "compression_ratio": 1.8295819935691318, "no_speech_prob": 0.022251777350902557}, {"id": 435, "seek": 240784, "start": 2423.76, "end": 2428.32, "text": " beautiful from the creativity perspective, iteration of ideas is that sometimes you actually", "tokens": [51160, 2238, 490, 264, 12915, 4585, 11, 24784, 295, 3487, 307, 300, 2171, 291, 767, 51388], "temperature": 0.0, "avg_logprob": -0.080022540518908, "compression_ratio": 1.8295819935691318, "no_speech_prob": 0.022251777350902557}, {"id": 436, "seek": 240784, "start": 2428.32, "end": 2432.56, "text": " don't want consistency. So the objective may be different in different settings. So for example,", "tokens": [51388, 500, 380, 528, 14416, 13, 407, 264, 10024, 815, 312, 819, 294, 819, 6257, 13, 407, 337, 1365, 11, 51600], "temperature": 0.0, "avg_logprob": -0.080022540518908, "compression_ratio": 1.8295819935691318, "no_speech_prob": 0.022251777350902557}, {"id": 437, "seek": 240784, "start": 2432.56, "end": 2437.52, "text": " for code, we always want code that passes. So that's a good example where we sample a lot just", "tokens": [51600, 337, 3089, 11, 321, 1009, 528, 3089, 300, 11335, 13, 407, 300, 311, 257, 665, 1365, 689, 321, 6889, 257, 688, 445, 51848], "temperature": 0.0, "avg_logprob": -0.080022540518908, "compression_ratio": 1.8295819935691318, "no_speech_prob": 0.022251777350902557}, {"id": 438, "seek": 243752, "start": 2437.52, "end": 2443.28, "text": " to get the subset. But sometimes I've talked to people who use it as a way to seed ideas or", "tokens": [50364, 281, 483, 264, 25993, 13, 583, 2171, 286, 600, 2825, 281, 561, 567, 764, 309, 382, 257, 636, 281, 8871, 3487, 420, 50652], "temperature": 0.0, "avg_logprob": -0.1079681413667696, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.01588418520987034}, {"id": 439, "seek": 243752, "start": 2443.28, "end": 2447.44, "text": " things like that. And actually there the diversity is the important part and gain very different", "tokens": [50652, 721, 411, 300, 13, 400, 767, 456, 264, 8811, 307, 264, 1021, 644, 293, 6052, 588, 819, 50860], "temperature": 0.0, "avg_logprob": -0.1079681413667696, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.01588418520987034}, {"id": 440, "seek": 243752, "start": 2447.44, "end": 2452.64, "text": " responses each time. And so I think over time, we'll actually have different models for different", "tokens": [50860, 13019, 1184, 565, 13, 400, 370, 286, 519, 670, 565, 11, 321, 603, 767, 362, 819, 5245, 337, 819, 51120], "temperature": 0.0, "avg_logprob": -0.1079681413667696, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.01588418520987034}, {"id": 441, "seek": 243752, "start": 2452.64, "end": 2457.84, "text": " things and be able to this is the core of the challenge of steerability of control, which right", "tokens": [51120, 721, 293, 312, 1075, 281, 341, 307, 264, 4965, 295, 264, 3430, 295, 30814, 2310, 295, 1969, 11, 597, 558, 51380], "temperature": 0.0, "avg_logprob": -0.1079681413667696, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.01588418520987034}, {"id": 442, "seek": 243752, "start": 2457.84, "end": 2462.88, "text": " now is not good, frankly, like why do we have prompt engineering and why does everyone love it?", "tokens": [51380, 586, 307, 406, 665, 11, 11939, 11, 411, 983, 360, 321, 362, 12391, 7043, 293, 983, 775, 1518, 959, 309, 30, 51632], "temperature": 0.0, "avg_logprob": -0.1079681413667696, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.01588418520987034}, {"id": 443, "seek": 246288, "start": 2462.88, "end": 2467.84, "text": " Like this is a this is a symptom of a problem, not a symptom of a solution. The fact that we", "tokens": [50364, 1743, 341, 307, 257, 341, 307, 257, 29370, 295, 257, 1154, 11, 406, 257, 29370, 295, 257, 3827, 13, 440, 1186, 300, 321, 50612], "temperature": 0.0, "avg_logprob": -0.07878114851258641, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.008003275841474533}, {"id": 444, "seek": 246288, "start": 2467.84, "end": 2472.32, "text": " spend so much time prompt engineering the perfect thing to steer. So hopefully we have better tools", "tokens": [50612, 3496, 370, 709, 565, 12391, 7043, 264, 2176, 551, 281, 30814, 13, 407, 4696, 321, 362, 1101, 3873, 50836], "temperature": 0.0, "avg_logprob": -0.07878114851258641, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.008003275841474533}, {"id": 445, "seek": 246288, "start": 2472.32, "end": 2476.56, "text": " in the future. But I see that as one key thing that will change is that we'll be able to steer", "tokens": [50836, 294, 264, 2027, 13, 583, 286, 536, 300, 382, 472, 2141, 551, 300, 486, 1319, 307, 300, 321, 603, 312, 1075, 281, 30814, 51048], "temperature": 0.0, "avg_logprob": -0.07878114851258641, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.008003275841474533}, {"id": 446, "seek": 246288, "start": 2476.56, "end": 2481.36, "text": " towards the mode we want to use. Do we want consistency? Do we want exploration? And how", "tokens": [51048, 3030, 264, 4391, 321, 528, 281, 764, 13, 1144, 321, 528, 14416, 30, 1144, 321, 528, 16197, 30, 400, 577, 51288], "temperature": 0.0, "avg_logprob": -0.07878114851258641, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.008003275841474533}, {"id": 447, "seek": 246288, "start": 2481.36, "end": 2485.12, "text": " does it fit into our iteration pattern? We won't spend too long on this because I asked everyone", "tokens": [51288, 775, 309, 3318, 666, 527, 24784, 5102, 30, 492, 1582, 380, 3496, 886, 938, 322, 341, 570, 286, 2351, 1518, 51476], "temperature": 0.0, "avg_logprob": -0.07878114851258641, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.008003275841474533}, {"id": 448, "seek": 246288, "start": 2485.12, "end": 2489.12, "text": " about this. But you know, where are the sources of creativity? So as we memorize more of the", "tokens": [51476, 466, 341, 13, 583, 291, 458, 11, 689, 366, 264, 7139, 295, 12915, 30, 407, 382, 321, 27478, 544, 295, 264, 51676], "temperature": 0.0, "avg_logprob": -0.07878114851258641, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.008003275841474533}, {"id": 449, "seek": 248912, "start": 2489.12, "end": 2493.52, "text": " long tail, and the models can extrapolate, and the human prompters can, you know, mix novel", "tokens": [50364, 938, 6838, 11, 293, 264, 5245, 393, 48224, 473, 11, 293, 264, 1952, 12391, 433, 393, 11, 291, 458, 11, 2890, 7613, 50584], "temperature": 0.0, "avg_logprob": -0.12961979599686357, "compression_ratio": 1.75, "no_speech_prob": 0.0021246226970106363}, {"id": 450, "seek": 248912, "start": 2493.52, "end": 2498.24, "text": " combinations of things together. So there's this potential extrapolative space and whatnot that's", "tokens": [50584, 21267, 295, 721, 1214, 13, 407, 456, 311, 341, 3995, 48224, 1166, 1901, 293, 25882, 300, 311, 50820], "temperature": 0.0, "avg_logprob": -0.12961979599686357, "compression_ratio": 1.75, "no_speech_prob": 0.0021246226970106363}, {"id": 451, "seek": 248912, "start": 2499.44, "end": 2505.12, "text": " how creative can they be? Yeah, I've been so one of the recent papers that we released was a paper", "tokens": [50880, 577, 5880, 393, 436, 312, 30, 865, 11, 286, 600, 668, 370, 472, 295, 264, 5162, 10577, 300, 321, 4736, 390, 257, 3035, 51164], "temperature": 0.0, "avg_logprob": -0.12961979599686357, "compression_ratio": 1.75, "no_speech_prob": 0.0021246226970106363}, {"id": 452, "seek": 248912, "start": 2505.12, "end": 2513.12, "text": " about what we call active inheritance. It's this idea that we can start to steer how we sample data", "tokens": [51164, 466, 437, 321, 818, 4967, 32122, 13, 467, 311, 341, 1558, 300, 321, 393, 722, 281, 30814, 577, 321, 6889, 1412, 51564], "temperature": 0.0, "avg_logprob": -0.12961979599686357, "compression_ratio": 1.75, "no_speech_prob": 0.0021246226970106363}, {"id": 453, "seek": 248912, "start": 2513.12, "end": 2518.24, "text": " to sampling different parts of the distributions from different models. So so far the paradigm", "tokens": [51564, 281, 21179, 819, 3166, 295, 264, 37870, 490, 819, 5245, 13, 407, 370, 1400, 264, 24709, 51820], "temperature": 0.0, "avg_logprob": -0.12961979599686357, "compression_ratio": 1.75, "no_speech_prob": 0.0021246226970106363}, {"id": 454, "seek": 251824, "start": 2518.24, "end": 2523.4399999999996, "text": " of like sampling data, either for human or for another model, has been very much like there's", "tokens": [50364, 295, 411, 21179, 1412, 11, 2139, 337, 1952, 420, 337, 1071, 2316, 11, 575, 668, 588, 709, 411, 456, 311, 50624], "temperature": 0.0, "avg_logprob": -0.07522525618561601, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.002355943899601698}, {"id": 455, "seek": 251824, "start": 2523.4399999999996, "end": 2527.9199999999996, "text": " a single teacher, you're the student, or there's another student or your co creators with a single", "tokens": [50624, 257, 2167, 5027, 11, 291, 434, 264, 3107, 11, 420, 456, 311, 1071, 3107, 420, 428, 598, 16039, 365, 257, 2167, 50848], "temperature": 0.0, "avg_logprob": -0.07522525618561601, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.002355943899601698}, {"id": 456, "seek": 251824, "start": 2527.9199999999996, "end": 2533.68, "text": " model. But if you think about it, one, that's a kind of passive inheritance, you're just trying a", "tokens": [50848, 2316, 13, 583, 498, 291, 519, 466, 309, 11, 472, 11, 300, 311, 257, 733, 295, 14975, 32122, 11, 291, 434, 445, 1382, 257, 51136], "temperature": 0.0, "avg_logprob": -0.07522525618561601, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.002355943899601698}, {"id": 457, "seek": 251824, "start": 2533.68, "end": 2539.9199999999996, "text": " single prompt, you're not really kind of enforcing any criteria. Active inheritance is where you", "tokens": [51136, 2167, 12391, 11, 291, 434, 406, 534, 733, 295, 25495, 2175, 604, 11101, 13, 26635, 32122, 307, 689, 291, 51448], "temperature": 0.0, "avg_logprob": -0.07522525618561601, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.002355943899601698}, {"id": 458, "seek": 251824, "start": 2539.9199999999996, "end": 2544.56, "text": " sample different parts of the problem you want to solve from a variety of different models.", "tokens": [51448, 6889, 819, 3166, 295, 264, 1154, 291, 528, 281, 5039, 490, 257, 5673, 295, 819, 5245, 13, 51680], "temperature": 0.0, "avg_logprob": -0.07522525618561601, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.002355943899601698}, {"id": 459, "seek": 254456, "start": 2544.56, "end": 2549.84, "text": " And that diversity actually spurs really interesting patterns where you increase", "tokens": [50364, 400, 300, 8811, 767, 637, 2156, 534, 1880, 8294, 689, 291, 3488, 50628], "temperature": 0.0, "avg_logprob": -0.0896418210372184, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.0014981694985181093}, {"id": 460, "seek": 254456, "start": 2549.84, "end": 2555.7599999999998, "text": " the realm of what's possible and kind of spur higher quality that transcends the quality of", "tokens": [50628, 264, 15355, 295, 437, 311, 1944, 293, 733, 295, 35657, 2946, 3125, 300, 43800, 2581, 264, 3125, 295, 50924], "temperature": 0.0, "avg_logprob": -0.0896418210372184, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.0014981694985181093}, {"id": 461, "seek": 254456, "start": 2555.7599999999998, "end": 2561.2799999999997, "text": " any one model. And I see that as a very important step that we're building a lot of work on,", "tokens": [50924, 604, 472, 2316, 13, 400, 286, 536, 300, 382, 257, 588, 1021, 1823, 300, 321, 434, 2390, 257, 688, 295, 589, 322, 11, 51200], "temperature": 0.0, "avg_logprob": -0.0896418210372184, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.0014981694985181093}, {"id": 462, "seek": 254456, "start": 2561.2799999999997, "end": 2567.2, "text": " including a multilingual, but also in this fundamental area of we actually used it to", "tokens": [51200, 3009, 257, 2120, 38219, 11, 457, 611, 294, 341, 8088, 1859, 295, 321, 767, 1143, 309, 281, 51496], "temperature": 0.0, "avg_logprob": -0.0896418210372184, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.0014981694985181093}, {"id": 463, "seek": 254456, "start": 2567.2, "end": 2571.44, "text": " in the paper that we just released, we used it to steer towards non-differentiable objectives.", "tokens": [51496, 294, 264, 3035, 300, 321, 445, 4736, 11, 321, 1143, 309, 281, 30814, 3030, 2107, 12, 67, 15790, 9364, 15961, 13, 51708], "temperature": 0.0, "avg_logprob": -0.0896418210372184, "compression_ratio": 1.6958174904942966, "no_speech_prob": 0.0014981694985181093}, {"id": 464, "seek": 257144, "start": 2571.44, "end": 2574.8, "text": " So you know, going back to what you were talking about the algorithm basin,", "tokens": [50364, 407, 291, 458, 11, 516, 646, 281, 437, 291, 645, 1417, 466, 264, 9284, 34863, 11, 50532], "temperature": 0.0, "avg_logprob": -0.09234634312716397, "compression_ratio": 1.731012658227848, "no_speech_prob": 0.004710209555923939}, {"id": 465, "seek": 257144, "start": 2574.8, "end": 2578.64, "text": " this idea, and I was saying everything is dependent on gradient descent, it's very hard", "tokens": [50532, 341, 1558, 11, 293, 286, 390, 1566, 1203, 307, 12334, 322, 16235, 23475, 11, 309, 311, 588, 1152, 50724], "temperature": 0.0, "avg_logprob": -0.09234634312716397, "compression_ratio": 1.731012658227848, "no_speech_prob": 0.004710209555923939}, {"id": 466, "seek": 257144, "start": 2578.64, "end": 2583.68, "text": " to steer towards non-differentiable objectives. Before deep neural networks, there was decades", "tokens": [50724, 281, 30814, 3030, 2107, 12, 67, 15790, 9364, 15961, 13, 4546, 2452, 18161, 9590, 11, 456, 390, 7878, 50976], "temperature": 0.0, "avg_logprob": -0.09234634312716397, "compression_ratio": 1.731012658227848, "no_speech_prob": 0.004710209555923939}, {"id": 467, "seek": 257144, "start": 2583.68, "end": 2589.36, "text": " of research on just these non-differentiable objectives. There are things like, how do you", "tokens": [50976, 295, 2132, 322, 445, 613, 2107, 12, 67, 15790, 9364, 15961, 13, 821, 366, 721, 411, 11, 577, 360, 291, 51260], "temperature": 0.0, "avg_logprob": -0.09234634312716397, "compression_ratio": 1.731012658227848, "no_speech_prob": 0.004710209555923939}, {"id": 468, "seek": 257144, "start": 2589.36, "end": 2596.08, "text": " compute the perplexity of like a given, like, what is the reading grade level of a given sentence?", "tokens": [51260, 14722, 264, 680, 18945, 507, 295, 411, 257, 2212, 11, 411, 11, 437, 307, 264, 3760, 7204, 1496, 295, 257, 2212, 8174, 30, 51596], "temperature": 0.0, "avg_logprob": -0.09234634312716397, "compression_ratio": 1.731012658227848, "no_speech_prob": 0.004710209555923939}, {"id": 469, "seek": 257144, "start": 2596.08, "end": 2600.56, "text": " So there's these scores that are kind of codified, but you can't really use it because they're not", "tokens": [51596, 407, 456, 311, 613, 13444, 300, 366, 733, 295, 17656, 2587, 11, 457, 291, 393, 380, 534, 764, 309, 570, 436, 434, 406, 51820], "temperature": 0.0, "avg_logprob": -0.09234634312716397, "compression_ratio": 1.731012658227848, "no_speech_prob": 0.004710209555923939}, {"id": 470, "seek": 260056, "start": 2600.56, "end": 2604.7999999999997, "text": " differentiable. And we actually show that you can use that as part of active inheritance where", "tokens": [50364, 819, 9364, 13, 400, 321, 767, 855, 300, 291, 393, 764, 300, 382, 644, 295, 4967, 32122, 689, 50576], "temperature": 0.0, "avg_logprob": -0.06985219080645338, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0016267126193270087}, {"id": 471, "seek": 260056, "start": 2604.7999999999997, "end": 2609.84, "text": " you steer towards models that are better at a reading grade level. And then you use that to", "tokens": [50576, 291, 30814, 3030, 5245, 300, 366, 1101, 412, 257, 3760, 7204, 1496, 13, 400, 550, 291, 764, 300, 281, 50828], "temperature": 0.0, "avg_logprob": -0.06985219080645338, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0016267126193270087}, {"id": 472, "seek": 260056, "start": 2609.84, "end": 2615.04, "text": " kind of form your basis of your data set. So I think that's fascinating. And I think that's", "tokens": [50828, 733, 295, 1254, 428, 5143, 295, 428, 1412, 992, 13, 407, 286, 519, 300, 311, 10343, 13, 400, 286, 519, 300, 311, 51088], "temperature": 0.0, "avg_logprob": -0.06985219080645338, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0016267126193270087}, {"id": 473, "seek": 260056, "start": 2615.04, "end": 2619.84, "text": " really going to spur creativity beyond just this more static notion of you just sample from a single", "tokens": [51088, 534, 516, 281, 35657, 12915, 4399, 445, 341, 544, 13437, 10710, 295, 291, 445, 6889, 490, 257, 2167, 51328], "temperature": 0.0, "avg_logprob": -0.06985219080645338, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0016267126193270087}, {"id": 474, "seek": 260056, "start": 2619.84, "end": 2624.0, "text": " teacher. Yeah, that's fascinating because there's so much of your research has been on the tyranny", "tokens": [51328, 5027, 13, 865, 11, 300, 311, 10343, 570, 456, 311, 370, 709, 295, 428, 2132, 575, 668, 322, 264, 41108, 11612, 51536], "temperature": 0.0, "avg_logprob": -0.06985219080645338, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0016267126193270087}, {"id": 475, "seek": 260056, "start": 2624.0, "end": 2627.6, "text": " of forgetting the long tail or not paying attention to it. And of course, you can solve that with", "tokens": [51536, 295, 25428, 264, 938, 6838, 420, 406, 6229, 3202, 281, 309, 13, 400, 295, 1164, 11, 291, 393, 5039, 300, 365, 51716], "temperature": 0.0, "avg_logprob": -0.06985219080645338, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.0016267126193270087}, {"id": 476, "seek": 262760, "start": 2627.6, "end": 2632.56, "text": " better optimization and, you know, federated learning and a gentile, you know, kind of", "tokens": [50364, 1101, 19618, 293, 11, 291, 458, 11, 38024, 770, 2539, 293, 257, 16108, 794, 11, 291, 458, 11, 733, 295, 50612], "temperature": 0.0, "avg_logprob": -0.13779524889859288, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.0038567848969250917}, {"id": 477, "seek": 262760, "start": 2632.56, "end": 2637.6, "text": " multimodal systems that share information and query and almost like an adversary or setup.", "tokens": [50612, 32972, 378, 304, 3652, 300, 2073, 1589, 293, 14581, 293, 1920, 411, 364, 48222, 420, 8657, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13779524889859288, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.0038567848969250917}, {"id": 478, "seek": 262760, "start": 2637.6, "end": 2641.68, "text": " Yeah, it's a more dynamic pool. And so it's this idea that you can actually, and actually", "tokens": [50864, 865, 11, 309, 311, 257, 544, 8546, 7005, 13, 400, 370, 309, 311, 341, 1558, 300, 291, 393, 767, 11, 293, 767, 51068], "temperature": 0.0, "avg_logprob": -0.13779524889859288, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.0038567848969250917}, {"id": 479, "seek": 262760, "start": 2642.3199999999997, "end": 2647.52, "text": " the long tail is a perfect example of where I find active inheritance most promising is that", "tokens": [51100, 264, 938, 6838, 307, 257, 2176, 1365, 295, 689, 286, 915, 4967, 32122, 881, 20257, 307, 300, 51360], "temperature": 0.0, "avg_logprob": -0.13779524889859288, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.0038567848969250917}, {"id": 480, "seek": 262760, "start": 2647.52, "end": 2652.08, "text": " because the long pool, the long tail, you typically have many weak teachers. No one's very good at", "tokens": [51360, 570, 264, 938, 7005, 11, 264, 938, 6838, 11, 291, 5850, 362, 867, 5336, 6023, 13, 883, 472, 311, 588, 665, 412, 51588], "temperature": 0.0, "avg_logprob": -0.13779524889859288, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.0038567848969250917}, {"id": 481, "seek": 265208, "start": 2652.08, "end": 2657.6, "text": " the long tail. But sampling effectively and doing this active inheritance rather than passive of", "tokens": [50364, 264, 938, 6838, 13, 583, 21179, 8659, 293, 884, 341, 4967, 32122, 2831, 813, 14975, 295, 50640], "temperature": 0.0, "avg_logprob": -0.09266561391402264, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.011594145558774471}, {"id": 482, "seek": 265208, "start": 2657.6, "end": 2662.64, "text": " just choosing a single teacher, but choosing a variety of teachers and then comparing and optimizing,", "tokens": [50640, 445, 10875, 257, 2167, 5027, 11, 457, 10875, 257, 5673, 295, 6023, 293, 550, 15763, 293, 40425, 11, 50892], "temperature": 0.0, "avg_logprob": -0.09266561391402264, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.011594145558774471}, {"id": 483, "seek": 265208, "start": 2662.64, "end": 2666.3199999999997, "text": " this is fascinating. And I suspect it will benefit most the long tail.", "tokens": [50892, 341, 307, 10343, 13, 400, 286, 9091, 309, 486, 5121, 881, 264, 938, 6838, 13, 51076], "temperature": 0.0, "avg_logprob": -0.09266561391402264, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.011594145558774471}, {"id": 484, "seek": 265208, "start": 2667.04, "end": 2671.52, "text": " So you said in your language gap paper that language models are going to become integral to", "tokens": [51112, 407, 291, 848, 294, 428, 2856, 7417, 3035, 300, 2856, 5245, 366, 516, 281, 1813, 11573, 281, 51336], "temperature": 0.0, "avg_logprob": -0.09266561391402264, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.011594145558774471}, {"id": 485, "seek": 265208, "start": 2671.52, "end": 2678.64, "text": " modern societies. How do you see that panning out? It's already happening in different ways.", "tokens": [51336, 4363, 19329, 13, 1012, 360, 291, 536, 300, 2462, 773, 484, 30, 467, 311, 1217, 2737, 294, 819, 2098, 13, 51692], "temperature": 0.0, "avg_logprob": -0.09266561391402264, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.011594145558774471}, {"id": 486, "seek": 267864, "start": 2678.72, "end": 2684.96, "text": " Like I would call it the high low way. So we can talk about high level themes, which is there'll be", "tokens": [50368, 1743, 286, 576, 818, 309, 264, 1090, 2295, 636, 13, 407, 321, 393, 751, 466, 1090, 1496, 13544, 11, 597, 307, 456, 603, 312, 50680], "temperature": 0.0, "avg_logprob": -0.08685498185210176, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00894290953874588}, {"id": 487, "seek": 267864, "start": 2684.96, "end": 2692.16, "text": " a ability to communicate much more easily. And so you'll just see much more proliferation of", "tokens": [50680, 257, 3485, 281, 7890, 709, 544, 3612, 13, 400, 370, 291, 603, 445, 536, 709, 544, 24398, 44987, 295, 51040], "temperature": 0.0, "avg_logprob": -0.08685498185210176, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00894290953874588}, {"id": 488, "seek": 267864, "start": 2692.16, "end": 2698.96, "text": " things like art or people writing or kind of taking away some of the difficult parts of", "tokens": [51040, 721, 411, 1523, 420, 561, 3579, 420, 733, 295, 1940, 1314, 512, 295, 264, 2252, 3166, 295, 51380], "temperature": 0.0, "avg_logprob": -0.08685498185210176, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00894290953874588}, {"id": 489, "seek": 267864, "start": 2698.96, "end": 2704.0, "text": " how we communicate. I think the low way is just the more granular ways that you're using it right", "tokens": [51380, 577, 321, 7890, 13, 286, 519, 264, 2295, 636, 307, 445, 264, 544, 39962, 2098, 300, 291, 434, 1228, 309, 558, 51632], "temperature": 0.0, "avg_logprob": -0.08685498185210176, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00894290953874588}, {"id": 490, "seek": 270400, "start": 2704.0, "end": 2711.44, "text": " now, which is I use it typically for very basic things throughout my day. We write a lot of papers,", "tokens": [50364, 586, 11, 597, 307, 286, 764, 309, 5850, 337, 588, 3875, 721, 3710, 452, 786, 13, 492, 2464, 257, 688, 295, 10577, 11, 50736], "temperature": 0.0, "avg_logprob": -0.07103688089471115, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.03400065004825592}, {"id": 491, "seek": 270400, "start": 2711.44, "end": 2716.8, "text": " so I'll do my citation reformatting using a language model. So there's both the mundane,", "tokens": [50736, 370, 286, 603, 360, 452, 45590, 8290, 267, 783, 1228, 257, 2856, 2316, 13, 407, 456, 311, 1293, 264, 43497, 11, 51004], "temperature": 0.0, "avg_logprob": -0.07103688089471115, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.03400065004825592}, {"id": 492, "seek": 270400, "start": 2716.8, "end": 2722.4, "text": " but there's also the profound. I think the profound is that it changes the ease of communication.", "tokens": [51004, 457, 456, 311, 611, 264, 14382, 13, 286, 519, 264, 14382, 307, 300, 309, 2962, 264, 12708, 295, 6101, 13, 51284], "temperature": 0.0, "avg_logprob": -0.07103688089471115, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.03400065004825592}, {"id": 493, "seek": 270400, "start": 2722.4, "end": 2727.76, "text": " And so it changes the rate of inflammation flow. And this can be really powerful. It can mean that", "tokens": [51284, 400, 370, 309, 2962, 264, 3314, 295, 21613, 3095, 13, 400, 341, 393, 312, 534, 4005, 13, 467, 393, 914, 300, 51552], "temperature": 0.0, "avg_logprob": -0.07103688089471115, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.03400065004825592}, {"id": 494, "seek": 272776, "start": 2727.76, "end": 2734.5600000000004, "text": " we can be more creative and experiment more the space. It can also bring new risks. And so", "tokens": [50364, 321, 393, 312, 544, 5880, 293, 5120, 544, 264, 1901, 13, 467, 393, 611, 1565, 777, 10888, 13, 400, 370, 50704], "temperature": 0.0, "avg_logprob": -0.09495453167987127, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.020337916910648346}, {"id": 495, "seek": 272776, "start": 2734.5600000000004, "end": 2740.7200000000003, "text": " I think this is also important to think about. Interesting. And you said that this North American", "tokens": [50704, 286, 519, 341, 307, 611, 1021, 281, 519, 466, 13, 14711, 13, 400, 291, 848, 300, 341, 4067, 2665, 51012], "temperature": 0.0, "avg_logprob": -0.09495453167987127, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.020337916910648346}, {"id": 496, "seek": 272776, "start": 2740.7200000000003, "end": 2747.28, "text": " bias in language model training, you said that it affects the design, the outputs and the behavior", "tokens": [51012, 12577, 294, 2856, 2316, 3097, 11, 291, 848, 300, 309, 11807, 264, 1715, 11, 264, 23930, 293, 264, 5223, 51340], "temperature": 0.0, "avg_logprob": -0.09495453167987127, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.020337916910648346}, {"id": 497, "seek": 272776, "start": 2747.28, "end": 2755.1200000000003, "text": " of the models. What did you mean by that? Well, there's two things. I mean, when I say design", "tokens": [51340, 295, 264, 5245, 13, 708, 630, 291, 914, 538, 300, 30, 1042, 11, 456, 311, 732, 721, 13, 286, 914, 11, 562, 286, 584, 1715, 51732], "temperature": 0.0, "avg_logprob": -0.09495453167987127, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.020337916910648346}, {"id": 498, "seek": 275512, "start": 2755.2, "end": 2762.0, "text": " outputs and the behavior of the models, I think that there's optimization bias in the models", "tokens": [50368, 23930, 293, 264, 5223, 295, 264, 5245, 11, 286, 519, 300, 456, 311, 19618, 12577, 294, 264, 5245, 50708], "temperature": 0.0, "avg_logprob": -0.1086675928927016, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.014236665330827236}, {"id": 499, "seek": 275512, "start": 2762.0, "end": 2767.12, "text": " itself against different languages. So tokenizes is a great example. So Roman scripts are things like", "tokens": [50708, 2564, 1970, 819, 8650, 13, 407, 14862, 5660, 307, 257, 869, 1365, 13, 407, 8566, 23294, 366, 721, 411, 50964], "temperature": 0.0, "avg_logprob": -0.1086675928927016, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.014236665330827236}, {"id": 500, "seek": 275512, "start": 2768.08, "end": 2775.6, "text": " French, Italian. We also have a Latin based scripts. This is also English. Whenever you deviate", "tokens": [51012, 5522, 11, 10003, 13, 492, 611, 362, 257, 10803, 2361, 23294, 13, 639, 307, 611, 3669, 13, 14159, 291, 1905, 13024, 51388], "temperature": 0.0, "avg_logprob": -0.1086675928927016, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.014236665330827236}, {"id": 501, "seek": 275512, "start": 2775.6, "end": 2782.96, "text": " from Latin based scripts, you have something like Hindi, Korean, and these do not play well", "tokens": [51388, 490, 10803, 2361, 23294, 11, 291, 362, 746, 411, 36225, 11, 6933, 11, 293, 613, 360, 406, 862, 731, 51756], "temperature": 0.0, "avg_logprob": -0.1086675928927016, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.014236665330827236}, {"id": 502, "seek": 278296, "start": 2782.96, "end": 2788.48, "text": " with tokenizers. So there's a lot of work which shows not only do tokenizers not work very well", "tokens": [50364, 365, 14862, 22525, 13, 407, 456, 311, 257, 688, 295, 589, 597, 3110, 406, 787, 360, 14862, 22525, 406, 589, 588, 731, 50640], "temperature": 0.0, "avg_logprob": -0.06460680041396827, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.020290276035666466}, {"id": 503, "seek": 278296, "start": 2788.48, "end": 2794.32, "text": " for these languages, but also it ends up being at this double tax because not only does the models", "tokens": [50640, 337, 613, 8650, 11, 457, 611, 309, 5314, 493, 885, 412, 341, 3834, 3366, 570, 406, 787, 775, 264, 5245, 50932], "temperature": 0.0, "avg_logprob": -0.06460680041396827, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.020290276035666466}, {"id": 504, "seek": 278296, "start": 2794.32, "end": 2798.8, "text": " perform worse, it also takes more tokens to represent these languages. So it's higher latency,", "tokens": [50932, 2042, 5324, 11, 309, 611, 2516, 544, 22667, 281, 2906, 613, 8650, 13, 407, 309, 311, 2946, 27043, 11, 51156], "temperature": 0.0, "avg_logprob": -0.06460680041396827, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.020290276035666466}, {"id": 505, "seek": 278296, "start": 2798.8, "end": 2804.4, "text": " higher cost for users outside of English to use APIs right now. So that's an example of like an", "tokens": [51156, 2946, 2063, 337, 5022, 2380, 295, 3669, 281, 764, 21445, 558, 586, 13, 407, 300, 311, 364, 1365, 295, 411, 364, 51436], "temperature": 0.0, "avg_logprob": -0.06460680041396827, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.020290276035666466}, {"id": 506, "seek": 278296, "start": 2804.4, "end": 2812.16, "text": " optimization bias. The other, frankly, the issue is that whenever you're trying to have a model", "tokens": [51436, 19618, 12577, 13, 440, 661, 11, 11939, 11, 264, 2734, 307, 300, 5699, 291, 434, 1382, 281, 362, 257, 2316, 51824], "temperature": 0.0, "avg_logprob": -0.06460680041396827, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.020290276035666466}, {"id": 507, "seek": 281216, "start": 2812.16, "end": 2818.3999999999996, "text": " that represents many different parts of a distribution, typically our solution right now is", "tokens": [50364, 300, 8855, 867, 819, 3166, 295, 257, 7316, 11, 5850, 527, 3827, 558, 586, 307, 50676], "temperature": 0.0, "avg_logprob": -0.12976023408233142, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.008942166343331337}, {"id": 508, "seek": 281216, "start": 2818.3999999999996, "end": 2823.12, "text": " we've got to give it more capacity. So I1 and 1 was an interesting example of this. We released", "tokens": [50676, 321, 600, 658, 281, 976, 309, 544, 6042, 13, 407, 286, 16, 293, 502, 390, 364, 1880, 1365, 295, 341, 13, 492, 4736, 50912], "temperature": 0.0, "avg_logprob": -0.12976023408233142, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.008942166343331337}, {"id": 509, "seek": 281216, "start": 2823.12, "end": 2828.0, "text": " I1 and 1. It represented 101 languages, and you can start to think about how many that is when you", "tokens": [50912, 286, 16, 293, 502, 13, 467, 10379, 21055, 8650, 11, 293, 291, 393, 722, 281, 519, 466, 577, 867, 300, 307, 562, 291, 51156], "temperature": 0.0, "avg_logprob": -0.12976023408233142, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.008942166343331337}, {"id": 510, "seek": 281216, "start": 2828.0, "end": 2833.6, "text": " try and list more than 20. So you'll probably get to 10, and then you'll start struggling. And 101", "tokens": [51156, 853, 293, 1329, 544, 813, 945, 13, 407, 291, 603, 1391, 483, 281, 1266, 11, 293, 550, 291, 603, 722, 9314, 13, 400, 21055, 51436], "temperature": 0.0, "avg_logprob": -0.12976023408233142, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.008942166343331337}, {"id": 511, "seek": 281216, "start": 2833.6, "end": 2841.2, "text": " is nuts. It includes things like we had Welsh, we had Irish, but we also had Telegu, we had many", "tokens": [51436, 307, 10483, 13, 467, 5974, 721, 411, 321, 632, 27129, 11, 321, 632, 16801, 11, 457, 321, 611, 632, 14889, 2794, 11, 321, 632, 867, 51816], "temperature": 0.0, "avg_logprob": -0.12976023408233142, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.008942166343331337}, {"id": 512, "seek": 284120, "start": 2841.2, "end": 2849.8399999999997, "text": " African languages, and we had very much these underrepresented like Haitian things, the variety", "tokens": [50364, 7312, 8650, 11, 293, 321, 632, 588, 709, 613, 833, 38293, 411, 25752, 952, 721, 11, 264, 5673, 50796], "temperature": 0.0, "avg_logprob": -0.09538083788992345, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.0030021320562809706}, {"id": 513, "seek": 284120, "start": 2849.8399999999997, "end": 2856.0, "text": " and the complexity as well as dialect. So 101 is probably like preparing for the space race. It's", "tokens": [50796, 293, 264, 14024, 382, 731, 382, 24652, 13, 407, 21055, 307, 1391, 411, 10075, 337, 264, 1901, 4569, 13, 467, 311, 51104], "temperature": 0.0, "avg_logprob": -0.09538083788992345, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.0030021320562809706}, {"id": 514, "seek": 284120, "start": 2856.0, "end": 2861.4399999999996, "text": " like at the most extreme of the problem. And what's interesting is everything you learn there", "tokens": [51104, 411, 412, 264, 881, 8084, 295, 264, 1154, 13, 400, 437, 311, 1880, 307, 1203, 291, 1466, 456, 51376], "temperature": 0.0, "avg_logprob": -0.09538083788992345, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.0030021320562809706}, {"id": 515, "seek": 284120, "start": 2862.16, "end": 2867.9199999999996, "text": " trickles down to less severe settings. But one of the things that we learned there is that you", "tokens": [51412, 4282, 904, 760, 281, 1570, 8922, 6257, 13, 583, 472, 295, 264, 721, 300, 321, 3264, 456, 307, 300, 291, 51700], "temperature": 0.0, "avg_logprob": -0.09538083788992345, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.0030021320562809706}, {"id": 516, "seek": 286792, "start": 2868.0, "end": 2872.64, "text": " have to be very careful about how you use capacity because we had this 13 billion parameter model,", "tokens": [50368, 362, 281, 312, 588, 5026, 466, 577, 291, 764, 6042, 570, 321, 632, 341, 3705, 5218, 13075, 2316, 11, 50600], "temperature": 0.0, "avg_logprob": -0.08174309488070214, "compression_ratio": 1.7661870503597121, "no_speech_prob": 0.010765415616333485}, {"id": 517, "seek": 286792, "start": 2873.28, "end": 2878.56, "text": " and we were stuck with it because there was no pre-training data that covered 101. So this model", "tokens": [50632, 293, 321, 645, 5541, 365, 309, 570, 456, 390, 572, 659, 12, 17227, 1760, 1412, 300, 5343, 21055, 13, 407, 341, 2316, 50896], "temperature": 0.0, "avg_logprob": -0.08174309488070214, "compression_ratio": 1.7661870503597121, "no_speech_prob": 0.010765415616333485}, {"id": 518, "seek": 286792, "start": 2878.56, "end": 2884.7200000000003, "text": " was actually from 2019, which is crazy given how much has happened since then. But because of that,", "tokens": [50896, 390, 767, 490, 6071, 11, 597, 307, 3219, 2212, 577, 709, 575, 2011, 1670, 550, 13, 583, 570, 295, 300, 11, 51204], "temperature": 0.0, "avg_logprob": -0.08174309488070214, "compression_ratio": 1.7661870503597121, "no_speech_prob": 0.010765415616333485}, {"id": 519, "seek": 286792, "start": 2884.7200000000003, "end": 2890.08, "text": " we were stuck with this model, and it meant that everything we had to do was try and make the best", "tokens": [51204, 321, 645, 5541, 365, 341, 2316, 11, 293, 309, 4140, 300, 1203, 321, 632, 281, 360, 390, 853, 293, 652, 264, 1151, 51472], "temperature": 0.0, "avg_logprob": -0.08174309488070214, "compression_ratio": 1.7661870503597121, "no_speech_prob": 0.010765415616333485}, {"id": 520, "seek": 286792, "start": 2890.08, "end": 2895.6800000000003, "text": " user capacity. We had to wait properly. We had to do data processing, data cleaning, but also we", "tokens": [51472, 4195, 6042, 13, 492, 632, 281, 1699, 6108, 13, 492, 632, 281, 360, 1412, 9007, 11, 1412, 8924, 11, 457, 611, 321, 51752], "temperature": 0.0, "avg_logprob": -0.08174309488070214, "compression_ratio": 1.7661870503597121, "no_speech_prob": 0.010765415616333485}, {"id": 521, "seek": 289568, "start": 2895.68, "end": 2900.72, "text": " had to do a lot of work with synthetic data and the manipulation of how we did the optimization time.", "tokens": [50364, 632, 281, 360, 257, 688, 295, 589, 365, 23420, 1412, 293, 264, 26475, 295, 577, 321, 630, 264, 19618, 565, 13, 50616], "temperature": 0.0, "avg_logprob": -0.08036020067003039, "compression_ratio": 1.6746575342465753, "no_speech_prob": 0.002965799765661359}, {"id": 522, "seek": 289568, "start": 2901.3599999999997, "end": 2907.8399999999997, "text": " So you can do this two ways. We could have even increased it to 103 billion parameter model,", "tokens": [50648, 407, 291, 393, 360, 341, 732, 2098, 13, 492, 727, 362, 754, 6505, 309, 281, 48784, 5218, 13075, 2316, 11, 50972], "temperature": 0.0, "avg_logprob": -0.08036020067003039, "compression_ratio": 1.6746575342465753, "no_speech_prob": 0.002965799765661359}, {"id": 523, "seek": 289568, "start": 2907.8399999999997, "end": 2911.44, "text": " and then we would have to retrain because right now models, unless they're trained with the day", "tokens": [50972, 293, 550, 321, 576, 362, 281, 1533, 7146, 570, 558, 586, 5245, 11, 5969, 436, 434, 8895, 365, 264, 786, 51152], "temperature": 0.0, "avg_logprob": -0.08036020067003039, "compression_ratio": 1.6746575342465753, "no_speech_prob": 0.002965799765661359}, {"id": 524, "seek": 289568, "start": 2911.44, "end": 2917.52, "text": " from the beginning, you can't just add it at the end. But also there's a secondary way, which is we", "tokens": [51152, 490, 264, 2863, 11, 291, 393, 380, 445, 909, 309, 412, 264, 917, 13, 583, 611, 456, 311, 257, 11396, 636, 11, 597, 307, 321, 51456], "temperature": 0.0, "avg_logprob": -0.08036020067003039, "compression_ratio": 1.6746575342465753, "no_speech_prob": 0.002965799765661359}, {"id": 525, "seek": 289568, "start": 2917.52, "end": 2922.64, "text": " get much more clever about the optimization and the data creation. And so this is really the issue", "tokens": [51456, 483, 709, 544, 13494, 466, 264, 19618, 293, 264, 1412, 8016, 13, 400, 370, 341, 307, 534, 264, 2734, 51712], "temperature": 0.0, "avg_logprob": -0.08036020067003039, "compression_ratio": 1.6746575342465753, "no_speech_prob": 0.002965799765661359}, {"id": 526, "seek": 292264, "start": 2922.72, "end": 2926.4, "text": " is that when you go multilingual, all your problems in a given language", "tokens": [50368, 307, 300, 562, 291, 352, 2120, 38219, 11, 439, 428, 2740, 294, 257, 2212, 2856, 50552], "temperature": 0.0, "avg_logprob": -0.11452720321227457, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.008936312980949879}, {"id": 527, "seek": 292264, "start": 2927.52, "end": 2930.96, "text": " are kind of multiplied out. And so you have to be very careful about all the details.", "tokens": [50608, 366, 733, 295, 17207, 484, 13, 400, 370, 291, 362, 281, 312, 588, 5026, 466, 439, 264, 4365, 13, 50780], "temperature": 0.0, "avg_logprob": -0.11452720321227457, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.008936312980949879}, {"id": 528, "seek": 292264, "start": 2932.16, "end": 2936.48, "text": " I wonder what's the relationship between language and capabilities? And the reason I asked this is", "tokens": [50840, 286, 2441, 437, 311, 264, 2480, 1296, 2856, 293, 10862, 30, 400, 264, 1778, 286, 2351, 341, 307, 51056], "temperature": 0.0, "avg_logprob": -0.11452720321227457, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.008936312980949879}, {"id": 529, "seek": 292264, "start": 2936.48, "end": 2940.96, "text": " there was a great book I read called The Language Game by Morton Christensen and Nick Chater.", "tokens": [51056, 456, 390, 257, 869, 1446, 286, 1401, 1219, 440, 24445, 7522, 538, 24977, 266, 2040, 32934, 293, 9449, 761, 771, 13, 51280], "temperature": 0.0, "avg_logprob": -0.11452720321227457, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.008936312980949879}, {"id": 530, "seek": 292264, "start": 2940.96, "end": 2946.4, "text": " And that very much led me to this idea of situated knowledge, I guess. So actually a lot of our", "tokens": [51280, 400, 300, 588, 709, 4684, 385, 281, 341, 1558, 295, 30143, 3601, 11, 286, 2041, 13, 407, 767, 257, 688, 295, 527, 51552], "temperature": 0.0, "avg_logprob": -0.11452720321227457, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.008936312980949879}, {"id": 531, "seek": 294640, "start": 2946.4, "end": 2953.28, "text": " cognition and thinking is quite specific to the culture and the language that we are in.", "tokens": [50364, 46905, 293, 1953, 307, 1596, 2685, 281, 264, 3713, 293, 264, 2856, 300, 321, 366, 294, 13, 50708], "temperature": 0.0, "avg_logprob": -0.05200186814412032, "compression_ratio": 1.8693877551020408, "no_speech_prob": 0.02552955225110054}, {"id": 532, "seek": 294640, "start": 2953.28, "end": 2957.52, "text": " And that seems to go against the grain of the idea that these things are learning", "tokens": [50708, 400, 300, 2544, 281, 352, 1970, 264, 12837, 295, 264, 1558, 300, 613, 721, 366, 2539, 50920], "temperature": 0.0, "avg_logprob": -0.05200186814412032, "compression_ratio": 1.8693877551020408, "no_speech_prob": 0.02552955225110054}, {"id": 533, "seek": 294640, "start": 2957.52, "end": 2962.88, "text": " general patterns of reasoning across languages. So then it rather kind of leads you to this", "tokens": [50920, 2674, 8294, 295, 21577, 2108, 8650, 13, 407, 550, 309, 2831, 733, 295, 6689, 291, 281, 341, 51188], "temperature": 0.0, "avg_logprob": -0.05200186814412032, "compression_ratio": 1.8693877551020408, "no_speech_prob": 0.02552955225110054}, {"id": 534, "seek": 294640, "start": 2962.88, "end": 2969.04, "text": " conclusion that you actually need to be within the language and the culture in order to do the", "tokens": [51188, 10063, 300, 291, 767, 643, 281, 312, 1951, 264, 2856, 293, 264, 3713, 294, 1668, 281, 360, 264, 51496], "temperature": 0.0, "avg_logprob": -0.05200186814412032, "compression_ratio": 1.8693877551020408, "no_speech_prob": 0.02552955225110054}, {"id": 535, "seek": 294640, "start": 2969.04, "end": 2973.92, "text": " kind of thinking that they do inside that culture. So how does that work then when you're mixing all", "tokens": [51496, 733, 295, 1953, 300, 436, 360, 1854, 300, 3713, 13, 407, 577, 775, 300, 589, 550, 562, 291, 434, 11983, 439, 51740], "temperature": 0.0, "avg_logprob": -0.05200186814412032, "compression_ratio": 1.8693877551020408, "no_speech_prob": 0.02552955225110054}, {"id": 536, "seek": 297392, "start": 2973.92, "end": 2980.08, "text": " of these together into one language model? I think it doesn't work that well right now. So I would", "tokens": [50364, 295, 613, 1214, 666, 472, 2856, 2316, 30, 286, 519, 309, 1177, 380, 589, 300, 731, 558, 586, 13, 407, 286, 576, 50672], "temperature": 0.0, "avg_logprob": -0.09934807586669922, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.016621509566903114}, {"id": 537, "seek": 297392, "start": 2980.08, "end": 2983.92, "text": " say this is like one of the core problems because you're precisely right. So we actually, so there's", "tokens": [50672, 584, 341, 307, 411, 472, 295, 264, 4965, 2740, 570, 291, 434, 13402, 558, 13, 407, 321, 767, 11, 370, 456, 311, 50864], "temperature": 0.0, "avg_logprob": -0.09934807586669922, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.016621509566903114}, {"id": 538, "seek": 297392, "start": 2983.92, "end": 2988.7200000000003, "text": " a few things I would say here. One is that we already see this with things like dialect. So the", "tokens": [50864, 257, 1326, 721, 286, 576, 584, 510, 13, 1485, 307, 300, 321, 1217, 536, 341, 365, 721, 411, 24652, 13, 407, 264, 51104], "temperature": 0.0, "avg_logprob": -0.09934807586669922, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.016621509566903114}, {"id": 539, "seek": 297392, "start": 2988.7200000000003, "end": 2994.08, "text": " notion of dialect, which isn't really a counter for any models, including Aya, I think that we all go", "tokens": [51104, 10710, 295, 24652, 11, 597, 1943, 380, 534, 257, 5682, 337, 604, 5245, 11, 3009, 316, 3016, 11, 286, 519, 300, 321, 439, 352, 51372], "temperature": 0.0, "avg_logprob": -0.09934807586669922, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.016621509566903114}, {"id": 540, "seek": 297392, "start": 2994.08, "end": 2999.76, "text": " as mainly just to be the first next step in state of art. But even ours doesn't do this nuance of", "tokens": [51372, 382, 8704, 445, 281, 312, 264, 700, 958, 1823, 294, 1785, 295, 1523, 13, 583, 754, 11896, 1177, 380, 360, 341, 42625, 295, 51656], "temperature": 0.0, "avg_logprob": -0.09934807586669922, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.016621509566903114}, {"id": 541, "seek": 299976, "start": 2999.76, "end": 3004.4, "text": " dialect. We do have various dialects of Arabic and some other dialects, but take something like", "tokens": [50364, 24652, 13, 492, 360, 362, 3683, 24652, 82, 295, 19938, 293, 512, 661, 24652, 82, 11, 457, 747, 746, 411, 50596], "temperature": 0.0, "avg_logprob": -0.0805586090794316, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.13585413992404938}, {"id": 542, "seek": 299976, "start": 3004.4, "end": 3010.48, "text": " Portuguese for example. Portuguese is spoken in many different places of the world. I spent part", "tokens": [50596, 22759, 337, 1365, 13, 22759, 307, 10759, 294, 867, 819, 3190, 295, 264, 1002, 13, 286, 4418, 644, 50900], "temperature": 0.0, "avg_logprob": -0.0805586090794316, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.13585413992404938}, {"id": 543, "seek": 299976, "start": 3010.48, "end": 3015.28, "text": " of my childhood in Mozambique. The Mozambique Portuguese is very different from, you know,", "tokens": [50900, 295, 452, 9278, 294, 30208, 2173, 1925, 13, 440, 30208, 2173, 1925, 22759, 307, 588, 819, 490, 11, 291, 458, 11, 51140], "temperature": 0.0, "avg_logprob": -0.0805586090794316, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.13585413992404938}, {"id": 544, "seek": 299976, "start": 3015.28, "end": 3020.2400000000002, "text": " I guess the most extreme would be Brazilian Portuguese. But also Portuguese in Portugal", "tokens": [51140, 286, 2041, 264, 881, 8084, 576, 312, 23435, 22759, 13, 583, 611, 22759, 294, 23011, 51388], "temperature": 0.0, "avg_logprob": -0.0805586090794316, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.13585413992404938}, {"id": 545, "seek": 299976, "start": 3020.2400000000002, "end": 3025.44, "text": " has its own nuances. And actually when we did Aya, we had researchers all over the world who were", "tokens": [51388, 575, 1080, 1065, 38775, 13, 400, 767, 562, 321, 630, 316, 3016, 11, 321, 632, 10309, 439, 670, 264, 1002, 567, 645, 51648], "temperature": 0.0, "avg_logprob": -0.0805586090794316, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.13585413992404938}, {"id": 546, "seek": 302544, "start": 3025.44, "end": 3031.04, "text": " part of this project. And we would frequently have these little riffs between the Portuguese", "tokens": [50364, 644, 295, 341, 1716, 13, 400, 321, 576, 10374, 362, 613, 707, 367, 17643, 1296, 264, 22759, 50644], "temperature": 0.0, "avg_logprob": -0.06841672068894511, "compression_ratio": 1.83203125, "no_speech_prob": 0.05689866840839386}, {"id": 547, "seek": 302544, "start": 3031.04, "end": 3036.2400000000002, "text": " contributors in Brazil and the Portuguese contributors in Portugal because they were asked", "tokens": [50644, 45627, 294, 9435, 293, 264, 22759, 45627, 294, 23011, 570, 436, 645, 2351, 50904], "temperature": 0.0, "avg_logprob": -0.06841672068894511, "compression_ratio": 1.83203125, "no_speech_prob": 0.05689866840839386}, {"id": 548, "seek": 302544, "start": 3036.2400000000002, "end": 3041.52, "text": " to review within a single pool. And so because the Brazilians outnumbered the Portuguese in Portugal,", "tokens": [50904, 281, 3131, 1951, 257, 2167, 7005, 13, 400, 370, 570, 264, 4991, 89, 21738, 484, 41261, 292, 264, 22759, 294, 23011, 11, 51168], "temperature": 0.0, "avg_logprob": -0.06841672068894511, "compression_ratio": 1.83203125, "no_speech_prob": 0.05689866840839386}, {"id": 549, "seek": 302544, "start": 3041.52, "end": 3047.12, "text": " they would all correct their submissions to Brazilian Portuguese. This is a very interesting", "tokens": [51168, 436, 576, 439, 3006, 641, 40429, 281, 23435, 22759, 13, 639, 307, 257, 588, 1880, 51448], "temperature": 0.0, "avg_logprob": -0.06841672068894511, "compression_ratio": 1.83203125, "no_speech_prob": 0.05689866840839386}, {"id": 550, "seek": 302544, "start": 3047.12, "end": 3052.16, "text": " concept. And this is just on the notion of dialect. But your wider point is this idea that", "tokens": [51448, 3410, 13, 400, 341, 307, 445, 322, 264, 10710, 295, 24652, 13, 583, 428, 11842, 935, 307, 341, 1558, 300, 51700], "temperature": 0.0, "avg_logprob": -0.06841672068894511, "compression_ratio": 1.83203125, "no_speech_prob": 0.05689866840839386}, {"id": 551, "seek": 305216, "start": 3052.96, "end": 3059.3599999999997, "text": " language is a tool for communication. And there's actually this very interesting concept about", "tokens": [50404, 2856, 307, 257, 2290, 337, 6101, 13, 400, 456, 311, 767, 341, 588, 1880, 3410, 466, 50724], "temperature": 0.0, "avg_logprob": -0.05797237632548915, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.021996332332491875}, {"id": 552, "seek": 305216, "start": 3059.3599999999997, "end": 3065.12, "text": " whether we even use language to think or if we use it as a utilitarian tool. Why is that relevant", "tokens": [50724, 1968, 321, 754, 764, 2856, 281, 519, 420, 498, 321, 764, 309, 382, 257, 4976, 13707, 2290, 13, 1545, 307, 300, 7340, 51012], "temperature": 0.0, "avg_logprob": -0.05797237632548915, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.021996332332491875}, {"id": 553, "seek": 305216, "start": 3065.12, "end": 3070.72, "text": " here? Because the way that we achieve an objective is going to depend upon where we are in the world.", "tokens": [51012, 510, 30, 1436, 264, 636, 300, 321, 4584, 364, 10024, 307, 516, 281, 5672, 3564, 689, 321, 366, 294, 264, 1002, 13, 51292], "temperature": 0.0, "avg_logprob": -0.05797237632548915, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.021996332332491875}, {"id": 554, "seek": 305216, "start": 3070.72, "end": 3074.8799999999997, "text": " And the way that technology should serve us is going to depend on where we are with the world.", "tokens": [51292, 400, 264, 636, 300, 2899, 820, 4596, 505, 307, 516, 281, 5672, 322, 689, 321, 366, 365, 264, 1002, 13, 51500], "temperature": 0.0, "avg_logprob": -0.05797237632548915, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.021996332332491875}, {"id": 555, "seek": 305216, "start": 3074.8799999999997, "end": 3079.2, "text": " This has come out recently. We just released a paper which I'm quite proud of, which is thinking", "tokens": [51500, 639, 575, 808, 484, 3938, 13, 492, 445, 4736, 257, 3035, 597, 286, 478, 1596, 4570, 295, 11, 597, 307, 1953, 51716], "temperature": 0.0, "avg_logprob": -0.05797237632548915, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.021996332332491875}, {"id": 556, "seek": 307920, "start": 3079.2, "end": 3086.0, "text": " about this idea of local versus global harms. At any one moment, we have multiple facets of our", "tokens": [50364, 466, 341, 1558, 295, 2654, 5717, 4338, 48505, 13, 1711, 604, 472, 1623, 11, 321, 362, 3866, 49752, 295, 527, 50704], "temperature": 0.0, "avg_logprob": -0.09345010693153638, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.051514897495508194}, {"id": 557, "seek": 307920, "start": 3086.0, "end": 3094.3199999999997, "text": " identity. So there's notions of what is insensitive to us as part of a notion of being global citizens.", "tokens": [50704, 6575, 13, 407, 456, 311, 35799, 295, 437, 307, 1028, 34465, 281, 505, 382, 644, 295, 257, 10710, 295, 885, 4338, 7180, 13, 51120], "temperature": 0.0, "avg_logprob": -0.09345010693153638, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.051514897495508194}, {"id": 558, "seek": 307920, "start": 3094.3199999999997, "end": 3098.8799999999997, "text": " And that probably gets to things like there's a universal agreement that some types of harms,", "tokens": [51120, 400, 300, 1391, 2170, 281, 721, 411, 456, 311, 257, 11455, 8106, 300, 512, 3467, 295, 48505, 11, 51348], "temperature": 0.0, "avg_logprob": -0.09345010693153638, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.051514897495508194}, {"id": 559, "seek": 307920, "start": 3098.8799999999997, "end": 3105.2, "text": " like harms to world children are particularly egregious. And most of, almost universally,", "tokens": [51348, 411, 48505, 281, 1002, 2227, 366, 4098, 308, 11027, 851, 13, 400, 881, 295, 11, 1920, 43995, 11, 51664], "temperature": 0.0, "avg_logprob": -0.09345010693153638, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.051514897495508194}, {"id": 560, "seek": 310520, "start": 3105.2, "end": 3110.3199999999997, "text": " our legal systems reflect this. But there's also notions of very particular harms which are", "tokens": [50364, 527, 5089, 3652, 5031, 341, 13, 583, 456, 311, 611, 35799, 295, 588, 1729, 48505, 597, 366, 50620], "temperature": 0.0, "avg_logprob": -0.06892271041870117, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.0071090711280703545}, {"id": 561, "seek": 310520, "start": 3111.04, "end": 3116.24, "text": " cultural and very specific to how we live. And that is reflected in things like wording. So", "tokens": [50656, 6988, 293, 588, 2685, 281, 577, 321, 1621, 13, 400, 300, 307, 15502, 294, 721, 411, 47602, 13, 407, 50916], "temperature": 0.0, "avg_logprob": -0.06892271041870117, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.0071090711280703545}, {"id": 562, "seek": 310520, "start": 3116.96, "end": 3122.3199999999997, "text": " we just released this paper, which I think is important for safety, but also part of this", "tokens": [50952, 321, 445, 4736, 341, 3035, 11, 597, 286, 519, 307, 1021, 337, 4514, 11, 457, 611, 644, 295, 341, 51220], "temperature": 0.0, "avg_logprob": -0.06892271041870117, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.0071090711280703545}, {"id": 563, "seek": 310520, "start": 3122.3199999999997, "end": 3128.16, "text": " broader move and in the field, which is that most of our models right now are trained with a", "tokens": [51220, 13227, 1286, 293, 294, 264, 2519, 11, 597, 307, 300, 881, 295, 527, 5245, 558, 586, 366, 8895, 365, 257, 51512], "temperature": 0.0, "avg_logprob": -0.06892271041870117, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.0071090711280703545}, {"id": 564, "seek": 310520, "start": 3128.16, "end": 3133.12, "text": " single objective, a single decision boundary. What that means is all the data gets flattened", "tokens": [51512, 2167, 10024, 11, 257, 2167, 3537, 12866, 13, 708, 300, 1355, 307, 439, 264, 1412, 2170, 24183, 292, 51760], "temperature": 0.0, "avg_logprob": -0.06892271041870117, "compression_ratio": 1.6510791366906474, "no_speech_prob": 0.0071090711280703545}, {"id": 565, "seek": 313312, "start": 3133.12, "end": 3138.88, "text": " to this one decision boundary. I'm very interested in multi-objective optimization. And this changes", "tokens": [50364, 281, 341, 472, 3537, 12866, 13, 286, 478, 588, 3102, 294, 4825, 12, 41070, 488, 19618, 13, 400, 341, 2962, 50652], "temperature": 0.0, "avg_logprob": -0.11931688595662075, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0022906330414116383}, {"id": 566, "seek": 313312, "start": 3138.88, "end": 3144.4, "text": " it so that you can hold multiple objectives at once. And that perhaps you can even adapt", "tokens": [50652, 309, 370, 300, 291, 393, 1797, 3866, 15961, 412, 1564, 13, 400, 300, 4317, 291, 393, 754, 6231, 50928], "temperature": 0.0, "avg_logprob": -0.11931688595662075, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0022906330414116383}, {"id": 567, "seek": 313312, "start": 3144.4, "end": 3149.44, "text": " these objectives on the fly, which is very interesting. Yeah, a couple of things. I mean,", "tokens": [50928, 613, 15961, 322, 264, 3603, 11, 597, 307, 588, 1880, 13, 865, 11, 257, 1916, 295, 721, 13, 286, 914, 11, 51180], "temperature": 0.0, "avg_logprob": -0.11931688595662075, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0022906330414116383}, {"id": 568, "seek": 313312, "start": 3149.44, "end": 3154.64, "text": " you're talking, I guess, about the interplay between having a relativistic worldview and", "tokens": [51180, 291, 434, 1417, 11, 286, 2041, 11, 466, 264, 728, 2858, 1296, 1419, 257, 21960, 3142, 41141, 293, 51440], "temperature": 0.0, "avg_logprob": -0.11931688595662075, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0022906330414116383}, {"id": 569, "seek": 313312, "start": 3154.64, "end": 3160.48, "text": " having some global norms. And in general, the way we do model alignment with our LHF and so on,", "tokens": [51440, 1419, 512, 4338, 24357, 13, 400, 294, 2674, 11, 264, 636, 321, 360, 2316, 18515, 365, 527, 441, 39, 37, 293, 370, 322, 11, 51732], "temperature": 0.0, "avg_logprob": -0.11931688595662075, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.0022906330414116383}, {"id": 570, "seek": 316048, "start": 3160.48, "end": 3167.36, "text": " it tends to de-complexify the reality of the world that we live in. And in ethical frameworks,", "tokens": [50364, 309, 12258, 281, 368, 12, 1112, 18945, 2505, 264, 4103, 295, 264, 1002, 300, 321, 1621, 294, 13, 400, 294, 18890, 29834, 11, 50708], "temperature": 0.0, "avg_logprob": -0.10125158679100775, "compression_ratio": 1.93006993006993, "no_speech_prob": 0.006501322612166405}, {"id": 571, "seek": 316048, "start": 3167.36, "end": 3171.6, "text": " there are deontology people who think there are just guiding principles and there are", "tokens": [50708, 456, 366, 368, 896, 1793, 561, 567, 519, 456, 366, 445, 25061, 9156, 293, 456, 366, 50920], "temperature": 0.0, "avg_logprob": -0.10125158679100775, "compression_ratio": 1.93006993006993, "no_speech_prob": 0.006501322612166405}, {"id": 572, "seek": 316048, "start": 3171.6, "end": 3175.76, "text": " virtue ethics people who think there are certain virtues that we should emphasize. And there's", "tokens": [50920, 20816, 19769, 561, 567, 519, 456, 366, 1629, 41106, 300, 321, 820, 16078, 13, 400, 456, 311, 51128], "temperature": 0.0, "avg_logprob": -0.10125158679100775, "compression_ratio": 1.93006993006993, "no_speech_prob": 0.006501322612166405}, {"id": 573, "seek": 316048, "start": 3175.76, "end": 3180.16, "text": " consequentialism that there are certain consequences that are bad. And as you were just", "tokens": [51128, 7242, 2549, 1434, 300, 456, 366, 1629, 10098, 300, 366, 1578, 13, 400, 382, 291, 645, 445, 51348], "temperature": 0.0, "avg_logprob": -0.10125158679100775, "compression_ratio": 1.93006993006993, "no_speech_prob": 0.006501322612166405}, {"id": 574, "seek": 316048, "start": 3180.16, "end": 3185.68, "text": " pointing to, it's very, very difficult to have a hybrid ethical framework that encapsulates", "tokens": [51348, 12166, 281, 11, 309, 311, 588, 11, 588, 2252, 281, 362, 257, 13051, 18890, 8388, 300, 38745, 26192, 51624], "temperature": 0.0, "avg_logprob": -0.10125158679100775, "compression_ratio": 1.93006993006993, "no_speech_prob": 0.006501322612166405}, {"id": 575, "seek": 316048, "start": 3185.68, "end": 3190.2400000000002, "text": " all of these things together. What kind of work are people doing and what are you thinking about", "tokens": [51624, 439, 295, 613, 721, 1214, 13, 708, 733, 295, 589, 366, 561, 884, 293, 437, 366, 291, 1953, 466, 51852], "temperature": 0.0, "avg_logprob": -0.10125158679100775, "compression_ratio": 1.93006993006993, "no_speech_prob": 0.006501322612166405}, {"id": 576, "seek": 319024, "start": 3191.04, "end": 3196.16, "text": " it? Well, we recently, the paper we just released is this really, this paper called,", "tokens": [50404, 309, 30, 1042, 11, 321, 3938, 11, 264, 3035, 321, 445, 4736, 307, 341, 534, 11, 341, 3035, 1219, 11, 50660], "temperature": 0.0, "avg_logprob": -0.14940405758944425, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.001569710555486381}, {"id": 577, "seek": 319024, "start": 3196.16, "end": 3202.4799999999996, "text": " we call the Multilingual Prism, which is this idea that for safety, we collected both local", "tokens": [50660, 321, 818, 264, 14665, 38219, 2114, 1434, 11, 597, 307, 341, 1558, 300, 337, 4514, 11, 321, 11087, 1293, 2654, 50976], "temperature": 0.0, "avg_logprob": -0.14940405758944425, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.001569710555486381}, {"id": 578, "seek": 319024, "start": 3202.4799999999996, "end": 3209.2799999999997, "text": " examples of red teaming safety with really this very nuanced collection process across multiple", "tokens": [50976, 5110, 295, 2182, 1469, 278, 4514, 365, 534, 341, 588, 45115, 5765, 1399, 2108, 3866, 51316], "temperature": 0.0, "avg_logprob": -0.14940405758944425, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.001569710555486381}, {"id": 579, "seek": 319024, "start": 3209.2799999999997, "end": 3215.04, "text": " languages, as well as harms that were considered global. From there, you can go into something", "tokens": [51316, 8650, 11, 382, 731, 382, 48505, 300, 645, 4888, 4338, 13, 3358, 456, 11, 291, 393, 352, 666, 746, 51604], "temperature": 0.0, "avg_logprob": -0.14940405758944425, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.001569710555486381}, {"id": 580, "seek": 319024, "start": 3215.04, "end": 3219.9199999999996, "text": " like our LHF and you can change the notion of a single reward model. So this is an area I'm", "tokens": [51604, 411, 527, 441, 39, 37, 293, 291, 393, 1319, 264, 10710, 295, 257, 2167, 7782, 2316, 13, 407, 341, 307, 364, 1859, 286, 478, 51848], "temperature": 0.0, "avg_logprob": -0.14940405758944425, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.001569710555486381}, {"id": 581, "seek": 321992, "start": 3219.92, "end": 3224.08, "text": " quite interested in. Like, how do you have multiple reward models? And then how do you", "tokens": [50364, 1596, 3102, 294, 13, 1743, 11, 577, 360, 291, 362, 3866, 7782, 5245, 30, 400, 550, 577, 360, 291, 50572], "temperature": 0.0, "avg_logprob": -0.08007063658341118, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.004461255855858326}, {"id": 582, "seek": 321992, "start": 3224.08, "end": 3227.6800000000003, "text": " balance them? This is the crux of the problem. And that's what you're getting at. So how do", "tokens": [50572, 4772, 552, 30, 639, 307, 264, 5140, 87, 295, 264, 1154, 13, 400, 300, 311, 437, 291, 434, 1242, 412, 13, 407, 577, 360, 50752], "temperature": 0.0, "avg_logprob": -0.08007063658341118, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.004461255855858326}, {"id": 583, "seek": 321992, "start": 3227.6800000000003, "end": 3232.7200000000003, "text": " you have these two things in unison? And I suspect what we're going to see there is", "tokens": [50752, 291, 362, 613, 732, 721, 294, 517, 2770, 30, 400, 286, 9091, 437, 321, 434, 516, 281, 536, 456, 307, 51004], "temperature": 0.0, "avg_logprob": -0.08007063658341118, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.004461255855858326}, {"id": 584, "seek": 321992, "start": 3233.6, "end": 3239.2000000000003, "text": " this notion of adaptation of our models in a more nimble way than previously. So typically,", "tokens": [51048, 341, 10710, 295, 21549, 295, 527, 5245, 294, 257, 544, 24887, 638, 636, 813, 8046, 13, 407, 5850, 11, 51328], "temperature": 0.0, "avg_logprob": -0.08007063658341118, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.004461255855858326}, {"id": 585, "seek": 321992, "start": 3239.2000000000003, "end": 3246.8, "text": " in a production setting, you spend months doing this model, you release it, cool, thumbs up,", "tokens": [51328, 294, 257, 4265, 3287, 11, 291, 3496, 2493, 884, 341, 2316, 11, 291, 4374, 309, 11, 1627, 11, 8838, 493, 11, 51708], "temperature": 0.0, "avg_logprob": -0.08007063658341118, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.004461255855858326}, {"id": 586, "seek": 324680, "start": 3246.88, "end": 3253.76, "text": " enjoy, and it's not as dynamic, but a true production model is refreshed and is more nimble", "tokens": [50368, 2103, 11, 293, 309, 311, 406, 382, 8546, 11, 457, 257, 2074, 4265, 2316, 307, 46330, 293, 307, 544, 24887, 638, 50712], "temperature": 0.0, "avg_logprob": -0.0880576769510905, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.041185203939676285}, {"id": 587, "seek": 324680, "start": 3253.76, "end": 3257.6000000000004, "text": " and is deployed in different ways to different places. Like Netflix famously does this with", "tokens": [50712, 293, 307, 17826, 294, 819, 2098, 281, 819, 3190, 13, 1743, 12778, 34360, 775, 341, 365, 50904], "temperature": 0.0, "avg_logprob": -0.0880576769510905, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.041185203939676285}, {"id": 588, "seek": 324680, "start": 3257.6000000000004, "end": 3263.2000000000003, "text": " its recommendation systems. I think here, this is actually a much more profound way of doing this", "tokens": [50904, 1080, 11879, 3652, 13, 286, 519, 510, 11, 341, 307, 767, 257, 709, 544, 14382, 636, 295, 884, 341, 51184], "temperature": 0.0, "avg_logprob": -0.0880576769510905, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.041185203939676285}, {"id": 589, "seek": 324680, "start": 3263.2000000000003, "end": 3270.32, "text": " because you can have these models, which essentially the way that they're steered is adapted. And this", "tokens": [51184, 570, 291, 393, 362, 613, 5245, 11, 597, 4476, 264, 636, 300, 436, 434, 2126, 4073, 307, 20871, 13, 400, 341, 51540], "temperature": 0.0, "avg_logprob": -0.0880576769510905, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.041185203939676285}, {"id": 590, "seek": 327032, "start": 3270.32, "end": 3275.6000000000004, "text": " is both interesting as well as profoundly challenging because the tricky thing is,", "tokens": [50364, 307, 1293, 1880, 382, 731, 382, 39954, 7595, 570, 264, 12414, 551, 307, 11, 50628], "temperature": 0.0, "avg_logprob": -0.11628690361976624, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.15154558420181274}, {"id": 591, "seek": 327032, "start": 3276.2400000000002, "end": 3282.32, "text": " is you want to be sensitive to how the preferences of users change around the world,", "tokens": [50660, 307, 291, 528, 281, 312, 9477, 281, 577, 264, 21910, 295, 5022, 1319, 926, 264, 1002, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11628690361976624, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.15154558420181274}, {"id": 592, "seek": 327032, "start": 3282.32, "end": 3287.2000000000003, "text": " but you cannot overfit to too granular a preference because this is a philosophical", "tokens": [50964, 457, 291, 2644, 670, 6845, 281, 886, 39962, 257, 17502, 570, 341, 307, 257, 25066, 51208], "temperature": 0.0, "avg_logprob": -0.11628690361976624, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.15154558420181274}, {"id": 593, "seek": 327032, "start": 3287.2000000000003, "end": 3291.2000000000003, "text": " tension you're actually getting at, which is that, you know, a libertarian view would say", "tokens": [51208, 8980, 291, 434, 767, 1242, 412, 11, 597, 307, 300, 11, 291, 458, 11, 257, 18058, 10652, 1910, 576, 584, 51408], "temperature": 0.0, "avg_logprob": -0.11628690361976624, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.15154558420181274}, {"id": 594, "seek": 327032, "start": 3291.92, "end": 3297.36, "text": " every person here has a list of preferences and those should be respected in their rank order.", "tokens": [51444, 633, 954, 510, 575, 257, 1329, 295, 21910, 293, 729, 820, 312, 20020, 294, 641, 6181, 1668, 13, 51716], "temperature": 0.0, "avg_logprob": -0.11628690361976624, "compression_ratio": 1.7301587301587302, "no_speech_prob": 0.15154558420181274}, {"id": 595, "seek": 329736, "start": 3297.44, "end": 3303.36, "text": " But as a society, we typically say we have this group of preferences, but we also adhere and kind", "tokens": [50368, 583, 382, 257, 4086, 11, 321, 5850, 584, 321, 362, 341, 1594, 295, 21910, 11, 457, 321, 611, 33584, 293, 733, 50664], "temperature": 0.0, "avg_logprob": -0.07815907452557538, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.002928289817646146}, {"id": 596, "seek": 329736, "start": 3303.36, "end": 3309.36, "text": " of subsume some of our preferences for the common grid. And so there's this notion as well, when", "tokens": [50664, 295, 2090, 2540, 512, 295, 527, 21910, 337, 264, 2689, 10748, 13, 400, 370, 456, 311, 341, 10710, 382, 731, 11, 562, 50964], "temperature": 0.0, "avg_logprob": -0.07815907452557538, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.002928289817646146}, {"id": 597, "seek": 329736, "start": 3309.36, "end": 3315.44, "text": " you articulate that as an algorithm, how do you get that balance somewhere in the middle, like where", "tokens": [50964, 291, 30305, 300, 382, 364, 9284, 11, 577, 360, 291, 483, 300, 4772, 4079, 294, 264, 2808, 11, 411, 689, 51268], "temperature": 0.0, "avg_logprob": -0.07815907452557538, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.002928289817646146}, {"id": 598, "seek": 329736, "start": 3315.44, "end": 3321.52, "text": " you are basically not adhering completely to a societal view. I think that's one of the concerns", "tokens": [51268, 291, 366, 1936, 406, 30106, 278, 2584, 281, 257, 33472, 1910, 13, 286, 519, 300, 311, 472, 295, 264, 7389, 51572], "temperature": 0.0, "avg_logprob": -0.07815907452557538, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.002928289817646146}, {"id": 599, "seek": 329736, "start": 3321.52, "end": 3326.48, "text": " about algorithms and tokenizers being used in certain states where there's a state influence", "tokens": [51572, 466, 14642, 293, 14862, 22525, 885, 1143, 294, 1629, 4368, 689, 456, 311, 257, 1785, 6503, 51820], "temperature": 0.0, "avg_logprob": -0.07815907452557538, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.002928289817646146}, {"id": 600, "seek": 332648, "start": 3326.48, "end": 3332.32, "text": " on how algorithms are deployed, but also not being used a total libertarian view where we don't want", "tokens": [50364, 322, 577, 14642, 366, 17826, 11, 457, 611, 406, 885, 1143, 257, 3217, 18058, 10652, 1910, 689, 321, 500, 380, 528, 50656], "temperature": 0.0, "avg_logprob": -0.07971037707282501, "compression_ratio": 1.6816608996539792, "no_speech_prob": 0.00034533062716946006}, {"id": 601, "seek": 332648, "start": 3332.32, "end": 3338.88, "text": " objectives that essentially amplify how a person thinks about the world without balancing and", "tokens": [50656, 15961, 300, 4476, 41174, 577, 257, 954, 7309, 466, 264, 1002, 1553, 22495, 293, 50984], "temperature": 0.0, "avg_logprob": -0.07971037707282501, "compression_ratio": 1.6816608996539792, "no_speech_prob": 0.00034533062716946006}, {"id": 602, "seek": 332648, "start": 3338.88, "end": 3342.4, "text": " introducing different viewpoints. Yeah, it's so fascinating because, you know, even things like", "tokens": [50984, 15424, 819, 1910, 20552, 13, 865, 11, 309, 311, 370, 10343, 570, 11, 291, 458, 11, 754, 721, 411, 51160], "temperature": 0.0, "avg_logprob": -0.07971037707282501, "compression_ratio": 1.6816608996539792, "no_speech_prob": 0.00034533062716946006}, {"id": 603, "seek": 332648, "start": 3342.4, "end": 3346.88, "text": " polarization on their face seem like an incredibly bad thing, but some kind of diversity preservation", "tokens": [51160, 37736, 322, 641, 1851, 1643, 411, 364, 6252, 1578, 551, 11, 457, 512, 733, 295, 8811, 27257, 51384], "temperature": 0.0, "avg_logprob": -0.07971037707282501, "compression_ratio": 1.6816608996539792, "no_speech_prob": 0.00034533062716946006}, {"id": 604, "seek": 332648, "start": 3346.88, "end": 3352.64, "text": " might actually lead to a pluralistic society that, you know, gains information and, you know,", "tokens": [51384, 1062, 767, 1477, 281, 257, 25377, 3142, 4086, 300, 11, 291, 458, 11, 16823, 1589, 293, 11, 291, 458, 11, 51672], "temperature": 0.0, "avg_logprob": -0.07971037707282501, "compression_ratio": 1.6816608996539792, "no_speech_prob": 0.00034533062716946006}, {"id": 605, "seek": 335264, "start": 3352.64, "end": 3356.7999999999997, "text": " like a degree of health actually that we need. But with safetyism in general though, there's", "tokens": [50364, 411, 257, 4314, 295, 1585, 767, 300, 321, 643, 13, 583, 365, 4514, 1434, 294, 2674, 1673, 11, 456, 311, 50572], "temperature": 0.0, "avg_logprob": -0.09727959840194039, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012276205234229565}, {"id": 606, "seek": 335264, "start": 3356.7999999999997, "end": 3362.48, "text": " always this notion of, I think we probably agree on this a little bit, that if you leave people", "tokens": [50572, 1009, 341, 10710, 295, 11, 286, 519, 321, 1391, 3986, 322, 341, 257, 707, 857, 11, 300, 498, 291, 1856, 561, 50856], "temperature": 0.0, "avg_logprob": -0.09727959840194039, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012276205234229565}, {"id": 607, "seek": 335264, "start": 3362.48, "end": 3368.56, "text": " to their own devices, then that can be bad, but you also need a little bit of that because otherwise", "tokens": [50856, 281, 641, 1065, 5759, 11, 550, 300, 393, 312, 1578, 11, 457, 291, 611, 643, 257, 707, 857, 295, 300, 570, 5911, 51160], "temperature": 0.0, "avg_logprob": -0.09727959840194039, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012276205234229565}, {"id": 608, "seek": 335264, "start": 3368.56, "end": 3374.08, "text": " the society might become quite sclerotic. And these decisions presumably need to be baked into", "tokens": [51160, 264, 4086, 1062, 1813, 1596, 795, 1918, 9411, 13, 400, 613, 5327, 26742, 643, 281, 312, 19453, 666, 51436], "temperature": 0.0, "avg_logprob": -0.09727959840194039, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012276205234229565}, {"id": 609, "seek": 335264, "start": 3374.08, "end": 3379.6, "text": " the way that we build these models in some way. Yeah, and currently then not. I would say currently", "tokens": [51436, 264, 636, 300, 321, 1322, 613, 5245, 294, 512, 636, 13, 865, 11, 293, 4362, 550, 406, 13, 286, 576, 584, 4362, 51712], "temperature": 0.0, "avg_logprob": -0.09727959840194039, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.012276205234229565}, {"id": 610, "seek": 337960, "start": 3379.6, "end": 3385.36, "text": " the way that we approach safety, it's quite, we have this notion of refusals. So when you've", "tokens": [50364, 264, 636, 300, 321, 3109, 4514, 11, 309, 311, 1596, 11, 321, 362, 341, 10710, 295, 1895, 301, 1124, 13, 407, 562, 291, 600, 50652], "temperature": 0.0, "avg_logprob": -0.08660996067631352, "compression_ratio": 1.68, "no_speech_prob": 0.017945580184459686}, {"id": 611, "seek": 337960, "start": 3385.36, "end": 3390.72, "text": " engaged with a model, you typically will see refusals for certain what I would call the more", "tokens": [50652, 8237, 365, 257, 2316, 11, 291, 5850, 486, 536, 1895, 301, 1124, 337, 1629, 437, 286, 576, 818, 264, 544, 50920], "temperature": 0.0, "avg_logprob": -0.08660996067631352, "compression_ratio": 1.68, "no_speech_prob": 0.017945580184459686}, {"id": 612, "seek": 337960, "start": 3390.72, "end": 3395.7599999999998, "text": " black and white type cases. There's this really interesting opportunity I see in the evolution", "tokens": [50920, 2211, 293, 2418, 2010, 3331, 13, 821, 311, 341, 534, 1880, 2650, 286, 536, 294, 264, 9303, 51172], "temperature": 0.0, "avg_logprob": -0.08660996067631352, "compression_ratio": 1.68, "no_speech_prob": 0.017945580184459686}, {"id": 613, "seek": 337960, "start": 3395.7599999999998, "end": 3400.4, "text": " of how we think about safety, which is that instead of just saying I can't answer this to kind of", "tokens": [51172, 295, 577, 321, 519, 466, 4514, 11, 597, 307, 300, 2602, 295, 445, 1566, 286, 393, 380, 1867, 341, 281, 733, 295, 51404], "temperature": 0.0, "avg_logprob": -0.08660996067631352, "compression_ratio": 1.68, "no_speech_prob": 0.017945580184459686}, {"id": 614, "seek": 337960, "start": 3401.52, "end": 3407.52, "text": " provide more nuance or provide links to additional support. And I think that's very", "tokens": [51460, 2893, 544, 42625, 420, 2893, 6123, 281, 4497, 1406, 13, 400, 286, 519, 300, 311, 588, 51760], "temperature": 0.0, "avg_logprob": -0.08660996067631352, "compression_ratio": 1.68, "no_speech_prob": 0.017945580184459686}, {"id": 615, "seek": 340752, "start": 3407.52, "end": 3412.56, "text": " interesting because there's a different type of discussion. But I would say your perspective,", "tokens": [50364, 1880, 570, 456, 311, 257, 819, 2010, 295, 5017, 13, 583, 286, 576, 584, 428, 4585, 11, 50616], "temperature": 0.0, "avg_logprob": -0.0873034290064161, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.0176385585218668}, {"id": 616, "seek": 340752, "start": 3412.56, "end": 3417.7599999999998, "text": " what you're talking about, which is really this part in the middle where you have some values as", "tokens": [50616, 437, 291, 434, 1417, 466, 11, 597, 307, 534, 341, 644, 294, 264, 2808, 689, 291, 362, 512, 4190, 382, 50876], "temperature": 0.0, "avg_logprob": -0.0873034290064161, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.0176385585218668}, {"id": 617, "seek": 340752, "start": 3417.7599999999998, "end": 3422.4, "text": " like how you build your algorithm, but also you realize that this is someone who's engaging with", "tokens": [50876, 411, 577, 291, 1322, 428, 9284, 11, 457, 611, 291, 4325, 300, 341, 307, 1580, 567, 311, 11268, 365, 51108], "temperature": 0.0, "avg_logprob": -0.0873034290064161, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.0176385585218668}, {"id": 618, "seek": 340752, "start": 3422.4, "end": 3428.0, "text": " an algorithm and like this is, you know, the algorithm should not reflect perfectly a single", "tokens": [51108, 364, 9284, 293, 411, 341, 307, 11, 291, 458, 11, 264, 9284, 820, 406, 5031, 6239, 257, 2167, 51388], "temperature": 0.0, "avg_logprob": -0.0873034290064161, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.0176385585218668}, {"id": 619, "seek": 340752, "start": 3428.0, "end": 3434.32, "text": " view of the world. You need more ways I also think within the UI for the person to influence and", "tokens": [51388, 1910, 295, 264, 1002, 13, 509, 643, 544, 2098, 286, 611, 519, 1951, 264, 15682, 337, 264, 954, 281, 6503, 293, 51704], "temperature": 0.0, "avg_logprob": -0.0873034290064161, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.0176385585218668}, {"id": 620, "seek": 343432, "start": 3434.32, "end": 3439.1200000000003, "text": " provide feedback and to something as course hallucinations is really interesting because", "tokens": [50364, 2893, 5824, 293, 281, 746, 382, 1164, 35212, 10325, 307, 534, 1880, 570, 50604], "temperature": 0.0, "avg_logprob": -0.14012017121186127, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014243360608816147}, {"id": 621, "seek": 343432, "start": 3439.1200000000003, "end": 3444.48, "text": " hallucinations is not, you can't, I'm very skeptical we're going to eliminate hallucinations", "tokens": [50604, 35212, 10325, 307, 406, 11, 291, 393, 380, 11, 286, 478, 588, 28601, 321, 434, 516, 281, 13819, 35212, 10325, 50872], "temperature": 0.0, "avg_logprob": -0.14012017121186127, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014243360608816147}, {"id": 622, "seek": 343432, "start": 3444.48, "end": 3448.88, "text": " because they're also what we really like about these models. It's the creativity. So for me,", "tokens": [50872, 570, 436, 434, 611, 437, 321, 534, 411, 466, 613, 5245, 13, 467, 311, 264, 12915, 13, 407, 337, 385, 11, 51092], "temperature": 0.0, "avg_logprob": -0.14012017121186127, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014243360608816147}, {"id": 623, "seek": 343432, "start": 3448.88, "end": 3453.44, "text": " this is not just, we often fixate a lot on the model in these conversations. The model has to", "tokens": [51092, 341, 307, 406, 445, 11, 321, 2049, 3191, 473, 257, 688, 322, 264, 2316, 294, 613, 7315, 13, 440, 2316, 575, 281, 51320], "temperature": 0.0, "avg_logprob": -0.14012017121186127, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014243360608816147}, {"id": 624, "seek": 343432, "start": 3453.44, "end": 3457.92, "text": " solve this, but I think there's also a notion of the system. And I think that some things that will", "tokens": [51320, 5039, 341, 11, 457, 286, 519, 456, 311, 611, 257, 10710, 295, 264, 1185, 13, 400, 286, 519, 300, 512, 721, 300, 486, 51544], "temperature": 0.0, "avg_logprob": -0.14012017121186127, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014243360608816147}, {"id": 625, "seek": 345792, "start": 3457.92, "end": 3464.64, "text": " be interesting to play within the system is how does the user express when they think that", "tokens": [50364, 312, 1880, 281, 862, 1951, 264, 1185, 307, 577, 775, 264, 4195, 5109, 562, 436, 519, 300, 50700], "temperature": 0.0, "avg_logprob": -0.06967316889295391, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.2901996076107025}, {"id": 626, "seek": 345792, "start": 3464.64, "end": 3470.2400000000002, "text": " steering isn't aligned with what they think is reasonable? A good example, this is for example", "tokens": [50700, 14823, 1943, 380, 17962, 365, 437, 436, 519, 307, 10585, 30, 316, 665, 1365, 11, 341, 307, 337, 1365, 50980], "temperature": 0.0, "avg_logprob": -0.06967316889295391, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.2901996076107025}, {"id": 627, "seek": 345792, "start": 3470.8, "end": 3475.52, "text": " a question about sexual health. There's valid reasons to ask those questions. There's valid", "tokens": [51008, 257, 1168, 466, 6701, 1585, 13, 821, 311, 7363, 4112, 281, 1029, 729, 1651, 13, 821, 311, 7363, 51244], "temperature": 0.0, "avg_logprob": -0.06967316889295391, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.2901996076107025}, {"id": 628, "seek": 345792, "start": 3475.52, "end": 3481.52, "text": " reasons to want to understand like parts of your biology or things like that. Wikipedia has whole", "tokens": [51244, 4112, 281, 528, 281, 1223, 411, 3166, 295, 428, 14956, 420, 721, 411, 300, 13, 28999, 575, 1379, 51544], "temperature": 0.0, "avg_logprob": -0.06967316889295391, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.2901996076107025}, {"id": 629, "seek": 345792, "start": 3481.52, "end": 3486.4, "text": " pages about sexual health. And so it's very interesting that a lot of systems refuse to", "tokens": [51544, 7183, 466, 6701, 1585, 13, 400, 370, 309, 311, 588, 1880, 300, 257, 688, 295, 3652, 16791, 281, 51788], "temperature": 0.0, "avg_logprob": -0.06967316889295391, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.2901996076107025}, {"id": 630, "seek": 348640, "start": 3486.4, "end": 3492.0, "text": " answer this right now. So there's this nuance where we need to make sure that we are updating these", "tokens": [50364, 1867, 341, 558, 586, 13, 407, 456, 311, 341, 42625, 689, 321, 643, 281, 652, 988, 300, 321, 366, 25113, 613, 50644], "temperature": 0.0, "avg_logprob": -0.0962081524863172, "compression_ratio": 1.78328173374613, "no_speech_prob": 0.004913488868623972}, {"id": 631, "seek": 348640, "start": 3492.0, "end": 3496.32, "text": " binary decision boundaries where it's outright refusal and move towards something which is", "tokens": [50644, 17434, 3537, 13180, 689, 309, 311, 35189, 48948, 293, 1286, 3030, 746, 597, 307, 50860], "temperature": 0.0, "avg_logprob": -0.0962081524863172, "compression_ratio": 1.78328173374613, "no_speech_prob": 0.004913488868623972}, {"id": 632, "seek": 348640, "start": 3496.32, "end": 3502.1600000000003, "text": " instead steering towards resources. Yeah, and I think so much of this is about when you fix something", "tokens": [50860, 2602, 14823, 3030, 3593, 13, 865, 11, 293, 286, 519, 370, 709, 295, 341, 307, 466, 562, 291, 3191, 746, 51152], "temperature": 0.0, "avg_logprob": -0.0962081524863172, "compression_ratio": 1.78328173374613, "no_speech_prob": 0.004913488868623972}, {"id": 633, "seek": 348640, "start": 3502.1600000000003, "end": 3506.32, "text": " as it is now, it can go both ways as well. So maybe you can explain to the model, no in this", "tokens": [51152, 382, 309, 307, 586, 11, 309, 393, 352, 1293, 2098, 382, 731, 13, 407, 1310, 291, 393, 2903, 281, 264, 2316, 11, 572, 294, 341, 51360], "temperature": 0.0, "avg_logprob": -0.0962081524863172, "compression_ratio": 1.78328173374613, "no_speech_prob": 0.004913488868623972}, {"id": 634, "seek": 348640, "start": 3506.32, "end": 3511.6, "text": " situation I think you really should tell me. And likewise the model can say no, actually I think", "tokens": [51360, 2590, 286, 519, 291, 534, 820, 980, 385, 13, 400, 32407, 264, 2316, 393, 584, 572, 11, 767, 286, 519, 51624], "temperature": 0.0, "avg_logprob": -0.0962081524863172, "compression_ratio": 1.78328173374613, "no_speech_prob": 0.004913488868623972}, {"id": 635, "seek": 348640, "start": 3511.6, "end": 3515.04, "text": " the reason I'm not allowing you to do this is because of this and maybe you should shift your", "tokens": [51624, 264, 1778, 286, 478, 406, 8293, 291, 281, 360, 341, 307, 570, 295, 341, 293, 1310, 291, 820, 5513, 428, 51796], "temperature": 0.0, "avg_logprob": -0.0962081524863172, "compression_ratio": 1.78328173374613, "no_speech_prob": 0.004913488868623972}, {"id": 636, "seek": 351504, "start": 3515.04, "end": 3520.08, "text": " viewpoint a little bit. It has to be done subtly because people don't like re-education. So that", "tokens": [50364, 35248, 257, 707, 857, 13, 467, 575, 281, 312, 1096, 7257, 356, 570, 561, 500, 380, 411, 319, 12, 32604, 399, 13, 407, 300, 50616], "temperature": 0.0, "avg_logprob": -0.10973712000353583, "compression_ratio": 1.7032640949554896, "no_speech_prob": 0.02391056902706623}, {"id": 637, "seek": 351504, "start": 3520.08, "end": 3524.32, "text": " actually creates, they say the road to hell is paved with good intentions, it creates an equal", "tokens": [50616, 767, 7829, 11, 436, 584, 264, 3060, 281, 4921, 307, 42989, 365, 665, 19354, 11, 309, 7829, 364, 2681, 50828], "temperature": 0.0, "avg_logprob": -0.10973712000353583, "compression_ratio": 1.7032640949554896, "no_speech_prob": 0.02391056902706623}, {"id": 638, "seek": 351504, "start": 3524.32, "end": 3529.2, "text": " and opposite reaction when you try to re-educate people. But coming on to RLHF, I mean we've", "tokens": [50828, 293, 6182, 5480, 562, 291, 853, 281, 319, 12, 32604, 473, 561, 13, 583, 1348, 322, 281, 497, 43, 39, 37, 11, 286, 914, 321, 600, 51072], "temperature": 0.0, "avg_logprob": -0.10973712000353583, "compression_ratio": 1.7032640949554896, "no_speech_prob": 0.02391056902706623}, {"id": 639, "seek": 351504, "start": 3529.2, "end": 3534.88, "text": " spoken about this for years, you've always been a bit grumpy about RLHF and I read your paper,", "tokens": [51072, 10759, 466, 341, 337, 924, 11, 291, 600, 1009, 668, 257, 857, 677, 36142, 466, 497, 43, 39, 37, 293, 286, 1401, 428, 3035, 11, 51356], "temperature": 0.0, "avg_logprob": -0.10973712000353583, "compression_ratio": 1.7032640949554896, "no_speech_prob": 0.02391056902706623}, {"id": 640, "seek": 351504, "start": 3534.88, "end": 3538.24, "text": " unfortunately I don't have an internet connection so I'm doing this from memory. Can you just remind", "tokens": [51356, 7015, 286, 500, 380, 362, 364, 4705, 4984, 370, 286, 478, 884, 341, 490, 4675, 13, 1664, 291, 445, 4160, 51524], "temperature": 0.0, "avg_logprob": -0.10973712000353583, "compression_ratio": 1.7032640949554896, "no_speech_prob": 0.02391056902706623}, {"id": 641, "seek": 351504, "start": 3538.24, "end": 3542.72, "text": " me the multilingual paper that you've just released where you're trying to remove translation", "tokens": [51524, 385, 264, 2120, 38219, 3035, 300, 291, 600, 445, 4736, 689, 291, 434, 1382, 281, 4159, 12853, 51748], "temperature": 0.0, "avg_logprob": -0.10973712000353583, "compression_ratio": 1.7032640949554896, "no_speech_prob": 0.02391056902706623}, {"id": 642, "seek": 354272, "start": 3542.72, "end": 3549.9199999999996, "text": " artifacts? Oh yes, RLHF speaks many languages. So this is a really nice paper, it was led by", "tokens": [50364, 24617, 30, 876, 2086, 11, 497, 43, 39, 37, 10789, 867, 8650, 13, 407, 341, 307, 257, 534, 1481, 3035, 11, 309, 390, 4684, 538, 50724], "temperature": 0.0, "avg_logprob": -0.12155812180887057, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.008976190350949764}, {"id": 643, "seek": 354272, "start": 3549.9199999999996, "end": 3555.6, "text": " John. This idea that we were the first to extend a lot of the RLHF techniques from many different", "tokens": [50724, 2619, 13, 639, 1558, 300, 321, 645, 264, 700, 281, 10101, 257, 688, 295, 264, 497, 43, 39, 37, 7512, 490, 867, 819, 51008], "temperature": 0.0, "avg_logprob": -0.12155812180887057, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.008976190350949764}, {"id": 644, "seek": 354272, "start": 3555.6, "end": 3562.8799999999997, "text": " languages. So I think actually there's a wider view of RLHF and I have been grumpy about it,", "tokens": [51008, 8650, 13, 407, 286, 519, 767, 456, 311, 257, 11842, 1910, 295, 497, 43, 39, 37, 293, 286, 362, 668, 677, 36142, 466, 309, 11, 51372], "temperature": 0.0, "avg_logprob": -0.12155812180887057, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.008976190350949764}, {"id": 645, "seek": 354272, "start": 3562.8799999999997, "end": 3567.2, "text": " but you go first. Yeah, what were you going to... Well I mean even in this paper you were saying", "tokens": [51372, 457, 291, 352, 700, 13, 865, 11, 437, 645, 291, 516, 281, 485, 1042, 286, 914, 754, 294, 341, 3035, 291, 645, 1566, 51588], "temperature": 0.0, "avg_logprob": -0.12155812180887057, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.008976190350949764}, {"id": 646, "seek": 354272, "start": 3567.2, "end": 3571.8399999999997, "text": " that, I mean obviously the broader conversation we've just had is that we need perhaps you know", "tokens": [51588, 300, 11, 286, 914, 2745, 264, 13227, 3761, 321, 600, 445, 632, 307, 300, 321, 643, 4317, 291, 458, 51820], "temperature": 0.0, "avg_logprob": -0.12155812180887057, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.008976190350949764}, {"id": 647, "seek": 357184, "start": 3571.84, "end": 3577.44, "text": " some kind of a more systems approach where we have a multitude of different models and optimizers", "tokens": [50364, 512, 733, 295, 257, 544, 3652, 3109, 689, 321, 362, 257, 36358, 295, 819, 5245, 293, 5028, 22525, 50644], "temperature": 0.0, "avg_logprob": -0.10759951129104152, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0011667503276839852}, {"id": 648, "seek": 357184, "start": 3577.44, "end": 3582.1600000000003, "text": " and datasets and all that good stuff. But even within RLHF you are saying that it's hideously", "tokens": [50644, 293, 42856, 293, 439, 300, 665, 1507, 13, 583, 754, 1951, 497, 43, 39, 37, 291, 366, 1566, 300, 309, 311, 6479, 5098, 50880], "temperature": 0.0, "avg_logprob": -0.10759951129104152, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0011667503276839852}, {"id": 649, "seek": 357184, "start": 3582.88, "end": 3587.28, "text": " complex and inefficient and you have to have this separate reward model and it can't be optimized", "tokens": [50916, 3997, 293, 43495, 293, 291, 362, 281, 362, 341, 4994, 7782, 2316, 293, 309, 393, 380, 312, 26941, 51136], "temperature": 0.0, "avg_logprob": -0.10759951129104152, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0011667503276839852}, {"id": 650, "seek": 357184, "start": 3587.28, "end": 3592.2400000000002, "text": " very well and you are saying that sometimes using DPO or even just basic reinforce is better.", "tokens": [51136, 588, 731, 293, 291, 366, 1566, 300, 2171, 1228, 413, 34885, 420, 754, 445, 3875, 22634, 307, 1101, 13, 51384], "temperature": 0.0, "avg_logprob": -0.10759951129104152, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0011667503276839852}, {"id": 651, "seek": 357184, "start": 3592.2400000000002, "end": 3597.28, "text": " Yeah, so there's an excellent paper which we also recently released back to basics where we actually", "tokens": [51384, 865, 11, 370, 456, 311, 364, 7103, 3035, 597, 321, 611, 3938, 4736, 646, 281, 14688, 689, 321, 767, 51636], "temperature": 0.0, "avg_logprob": -0.10759951129104152, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0011667503276839852}, {"id": 652, "seek": 357184, "start": 3597.28, "end": 3601.36, "text": " do a much more profound question of this, not even specific to multilingual, we take a step back.", "tokens": [51636, 360, 257, 709, 544, 14382, 1168, 295, 341, 11, 406, 754, 2685, 281, 2120, 38219, 11, 321, 747, 257, 1823, 646, 13, 51840], "temperature": 0.0, "avg_logprob": -0.10759951129104152, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0011667503276839852}, {"id": 653, "seek": 360184, "start": 3602.0, "end": 3611.84, "text": " And we say okay, it's really interesting. All the kind of most cited papers originally on RLHF", "tokens": [50372, 400, 321, 584, 1392, 11, 309, 311, 534, 1880, 13, 1057, 264, 733, 295, 881, 30134, 10577, 7993, 322, 497, 43, 39, 37, 50864], "temperature": 0.0, "avg_logprob": -0.10834739605585735, "compression_ratio": 1.4866310160427807, "no_speech_prob": 0.0013006735825911164}, {"id": 654, "seek": 360184, "start": 3612.4, "end": 3619.6000000000004, "text": " are papers which really take this canonical method within RL, PPO, and apply it to the", "tokens": [50892, 366, 10577, 597, 534, 747, 341, 46491, 3170, 1951, 497, 43, 11, 430, 34885, 11, 293, 3079, 309, 281, 264, 51252], "temperature": 0.0, "avg_logprob": -0.10834739605585735, "compression_ratio": 1.4866310160427807, "no_speech_prob": 0.0013006735825911164}, {"id": 655, "seek": 360184, "start": 3619.6000000000004, "end": 3628.6400000000003, "text": " language setting. And PPO really evolved in the RL, traditional RL space to address and mitigate", "tokens": [51252, 2856, 3287, 13, 400, 430, 34885, 534, 14178, 294, 264, 497, 43, 11, 5164, 497, 43, 1901, 281, 2985, 293, 27336, 51704], "temperature": 0.0, "avg_logprob": -0.10834739605585735, "compression_ratio": 1.4866310160427807, "no_speech_prob": 0.0013006735825911164}, {"id": 656, "seek": 362864, "start": 3628.64, "end": 3634.48, "text": " a lot of the issues with traditional RL. RL is typically over a large expansive search space,", "tokens": [50364, 257, 688, 295, 264, 2663, 365, 5164, 497, 43, 13, 497, 43, 307, 5850, 670, 257, 2416, 46949, 3164, 1901, 11, 50656], "temperature": 0.0, "avg_logprob": -0.12119933234320747, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.025121822953224182}, {"id": 657, "seek": 362864, "start": 3634.48, "end": 3641.04, "text": " incredibly noisy, and the trickiest part right is that your errors compound. So it's almost like", "tokens": [50656, 6252, 24518, 11, 293, 264, 4282, 6495, 644, 558, 307, 300, 428, 13603, 14154, 13, 407, 309, 311, 1920, 411, 50984], "temperature": 0.0, "avg_logprob": -0.12119933234320747, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.025121822953224182}, {"id": 658, "seek": 362864, "start": 3641.04, "end": 3645.92, "text": " thinking about well what if I bet incorrectly at a game table and then tried to bet again and", "tokens": [50984, 1953, 466, 731, 437, 498, 286, 778, 42892, 412, 257, 1216, 3199, 293, 550, 3031, 281, 778, 797, 293, 51228], "temperature": 0.0, "avg_logprob": -0.12119933234320747, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.025121822953224182}, {"id": 659, "seek": 362864, "start": 3645.92, "end": 3653.3599999999997, "text": " did it incorrectly and your losses just compound the more that your estimates are off. So PPO is", "tokens": [51228, 630, 309, 42892, 293, 428, 15352, 445, 14154, 264, 544, 300, 428, 20561, 366, 766, 13, 407, 430, 34885, 307, 51600], "temperature": 0.0, "avg_logprob": -0.12119933234320747, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.025121822953224182}, {"id": 660, "seek": 365336, "start": 3653.44, "end": 3662.2400000000002, "text": " heavily what I would call kind of regularized or conditioned to limit the impact of an incorrect", "tokens": [50368, 10950, 437, 286, 576, 818, 733, 295, 3890, 1602, 420, 35833, 281, 4948, 264, 2712, 295, 364, 18424, 50808], "temperature": 0.0, "avg_logprob": -0.10291509313897772, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.017138902097940445}, {"id": 661, "seek": 365336, "start": 3662.2400000000002, "end": 3667.52, "text": " estimate. What that means is that often it's quite memory intensive, you kind of have four", "tokens": [50808, 12539, 13, 708, 300, 1355, 307, 300, 2049, 309, 311, 1596, 4675, 18957, 11, 291, 733, 295, 362, 1451, 51072], "temperature": 0.0, "avg_logprob": -0.10291509313897772, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.017138902097940445}, {"id": 662, "seek": 365336, "start": 3667.52, "end": 3673.6800000000003, "text": " models and play at any one time, and it also means that it's quite sensitive. So typically PPO to", "tokens": [51072, 5245, 293, 862, 412, 604, 472, 565, 11, 293, 309, 611, 1355, 300, 309, 311, 1596, 9477, 13, 407, 5850, 430, 34885, 281, 51380], "temperature": 0.0, "avg_logprob": -0.10291509313897772, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.017138902097940445}, {"id": 663, "seek": 365336, "start": 3673.6800000000003, "end": 3681.6, "text": " train, it takes longer. And showed the language setting. And so the initial adoption of PPO and", "tokens": [51380, 3847, 11, 309, 2516, 2854, 13, 400, 4712, 264, 2856, 3287, 13, 400, 370, 264, 5883, 19215, 295, 430, 34885, 293, 51776], "temperature": 0.0, "avg_logprob": -0.10291509313897772, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.017138902097940445}, {"id": 664, "seek": 368160, "start": 3681.6, "end": 3687.2799999999997, "text": " the success of it was taken at least value. This is incredible, let's go with this. But", "tokens": [50364, 264, 2245, 295, 309, 390, 2726, 412, 1935, 2158, 13, 639, 307, 4651, 11, 718, 311, 352, 365, 341, 13, 583, 50648], "temperature": 0.0, "avg_logprob": -0.08674714461616848, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0044655525125563145}, {"id": 665, "seek": 368160, "start": 3688.08, "end": 3691.8399999999997, "text": " the language space is also an enormous search space because if you think about it, you're trying to", "tokens": [50688, 264, 2856, 1901, 307, 611, 364, 11322, 3164, 1901, 570, 498, 291, 519, 466, 309, 11, 291, 434, 1382, 281, 50876], "temperature": 0.0, "avg_logprob": -0.08674714461616848, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0044655525125563145}, {"id": 666, "seek": 368160, "start": 3691.8399999999997, "end": 3697.44, "text": " predict the next token, how many tokens, how many possible tokens are there in the world to represent", "tokens": [50876, 6069, 264, 958, 14862, 11, 577, 867, 22667, 11, 577, 867, 1944, 22667, 366, 456, 294, 264, 1002, 281, 2906, 51156], "temperature": 0.0, "avg_logprob": -0.08674714461616848, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0044655525125563145}, {"id": 667, "seek": 368160, "start": 3697.44, "end": 3703.8399999999997, "text": " language. But by the time you have a trained model, and by the time you've done all this pre-training,", "tokens": [51156, 2856, 13, 583, 538, 264, 565, 291, 362, 257, 8895, 2316, 11, 293, 538, 264, 565, 291, 600, 1096, 439, 341, 659, 12, 17227, 1760, 11, 51476], "temperature": 0.0, "avg_logprob": -0.08674714461616848, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0044655525125563145}, {"id": 668, "seek": 368160, "start": 3703.8399999999997, "end": 3708.96, "text": " the search space is much narrower. And actually it's quite interesting because the likelihood", "tokens": [51476, 264, 3164, 1901, 307, 709, 46751, 13, 400, 767, 309, 311, 1596, 1880, 570, 264, 22119, 51732], "temperature": 0.0, "avg_logprob": -0.08674714461616848, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0044655525125563145}, {"id": 669, "seek": 370896, "start": 3708.96, "end": 3713.52, "text": " and the probability of what the next token will be is actually very concentrated.", "tokens": [50364, 293, 264, 8482, 295, 437, 264, 958, 14862, 486, 312, 307, 767, 588, 21321, 13, 50592], "temperature": 0.0, "avg_logprob": -0.10238392599697771, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.009257174097001553}, {"id": 670, "seek": 370896, "start": 3713.52, "end": 3718.08, "text": " And when you have this pre-trained base, it's only going to be a few different tokens that you", "tokens": [50592, 400, 562, 291, 362, 341, 659, 12, 17227, 2001, 3096, 11, 309, 311, 787, 516, 281, 312, 257, 1326, 819, 22667, 300, 291, 50820], "temperature": 0.0, "avg_logprob": -0.10238392599697771, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.009257174097001553}, {"id": 671, "seek": 370896, "start": 3718.08, "end": 3723.2, "text": " would likely predict, which means that this was overkill for the setting. And what we show", "tokens": [50820, 576, 3700, 6069, 11, 597, 1355, 300, 341, 390, 670, 34213, 337, 264, 3287, 13, 400, 437, 321, 855, 51076], "temperature": 0.0, "avg_logprob": -0.10238392599697771, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.009257174097001553}, {"id": 672, "seek": 370896, "start": 3723.2, "end": 3729.44, "text": " convincingly and back to basics is that you can strip a lot, a lot of the components of PPO out.", "tokens": [51076, 24823, 356, 293, 646, 281, 14688, 307, 300, 291, 393, 12828, 257, 688, 11, 257, 688, 295, 264, 6677, 295, 430, 34885, 484, 13, 51388], "temperature": 0.0, "avg_logprob": -0.10238392599697771, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.009257174097001553}, {"id": 673, "seek": 370896, "start": 3730.08, "end": 3736.88, "text": " You can propose something like RLU, which is still an RL method, but that works effectively and even", "tokens": [51420, 509, 393, 17421, 746, 411, 497, 43, 52, 11, 597, 307, 920, 364, 497, 43, 3170, 11, 457, 300, 1985, 8659, 293, 754, 51760], "temperature": 0.0, "avg_logprob": -0.10238392599697771, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.009257174097001553}, {"id": 674, "seek": 373688, "start": 3736.88, "end": 3744.4, "text": " surpasses it. And RLU is also what we used in the RLHF speaks many languages. And we showed that", "tokens": [50364, 27650, 279, 309, 13, 400, 497, 43, 52, 307, 611, 437, 321, 1143, 294, 264, 497, 43, 39, 37, 10789, 867, 8650, 13, 400, 321, 4712, 300, 50740], "temperature": 0.0, "avg_logprob": -0.06636017886075106, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.013995318673551083}, {"id": 675, "seek": 373688, "start": 3744.4, "end": 3750.08, "text": " this is very impactful. And because it's online, it does beat things like DPO, which is offline.", "tokens": [50740, 341, 307, 588, 30842, 13, 400, 570, 309, 311, 2950, 11, 309, 775, 4224, 721, 411, 413, 34885, 11, 597, 307, 21857, 13, 51024], "temperature": 0.0, "avg_logprob": -0.06636017886075106, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.013995318673551083}, {"id": 676, "seek": 373688, "start": 3750.08, "end": 3757.36, "text": " So RLU is still an RL method. But what it's really saying is that we are in a well-conditioned", "tokens": [51024, 407, 497, 43, 52, 307, 920, 364, 497, 43, 3170, 13, 583, 437, 309, 311, 534, 1566, 307, 300, 321, 366, 294, 257, 731, 12, 18882, 849, 292, 51388], "temperature": 0.0, "avg_logprob": -0.06636017886075106, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.013995318673551083}, {"id": 677, "seek": 373688, "start": 3757.36, "end": 3762.4, "text": " search space. And because of that, we can be a lot more nimble about how we explore it.", "tokens": [51388, 3164, 1901, 13, 400, 570, 295, 300, 11, 321, 393, 312, 257, 688, 544, 24887, 638, 466, 577, 321, 6839, 309, 13, 51640], "temperature": 0.0, "avg_logprob": -0.06636017886075106, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.013995318673551083}, {"id": 678, "seek": 376240, "start": 3762.4, "end": 3768.4, "text": " Yeah. Well, in the RLHF on many languages one, because obviously you've had this huge focus on", "tokens": [50364, 865, 13, 1042, 11, 294, 264, 497, 43, 39, 37, 322, 867, 8650, 472, 11, 570, 2745, 291, 600, 632, 341, 2603, 1879, 322, 50664], "temperature": 0.0, "avg_logprob": -0.1012454953110009, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.009625861421227455}, {"id": 679, "seek": 376240, "start": 3768.4, "end": 3773.44, "text": " multilingual. And I suppose there's the problem of getting diverse data, because this is super", "tokens": [50664, 2120, 38219, 13, 400, 286, 7297, 456, 311, 264, 1154, 295, 1242, 9521, 1412, 11, 570, 341, 307, 1687, 50916], "temperature": 0.0, "avg_logprob": -0.1012454953110009, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.009625861421227455}, {"id": 680, "seek": 376240, "start": 3773.44, "end": 3778.48, "text": " heterogeneous data when we're doing multilingual language training. And of course, even the", "tokens": [50916, 20789, 31112, 1412, 562, 321, 434, 884, 2120, 38219, 2856, 3097, 13, 400, 295, 1164, 11, 754, 264, 51168], "temperature": 0.0, "avg_logprob": -0.1012454953110009, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.009625861421227455}, {"id": 681, "seek": 376240, "start": 3778.48, "end": 3784.08, "text": " preference completions, they needed to be generated as well. And I think you generated some with", "tokens": [51168, 17502, 1557, 626, 11, 436, 2978, 281, 312, 10833, 382, 731, 13, 400, 286, 519, 291, 10833, 512, 365, 51448], "temperature": 0.0, "avg_logprob": -0.1012454953110009, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.009625861421227455}, {"id": 682, "seek": 376240, "start": 3784.08, "end": 3788.48, "text": " translations and then you had a strong model and you had a setup there. Can you tell us about that?", "tokens": [51448, 37578, 293, 550, 291, 632, 257, 2068, 2316, 293, 291, 632, 257, 8657, 456, 13, 1664, 291, 980, 505, 466, 300, 30, 51668], "temperature": 0.0, "avg_logprob": -0.1012454953110009, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.009625861421227455}, {"id": 683, "seek": 378848, "start": 3788.48, "end": 3793.2, "text": " That's fun, because it's part of this wider issue where multilingual relies a lot traditionally on", "tokens": [50364, 663, 311, 1019, 11, 570, 309, 311, 644, 295, 341, 11842, 2734, 689, 2120, 38219, 30910, 257, 688, 19067, 322, 50600], "temperature": 0.0, "avg_logprob": -0.10850787162780762, "compression_ratio": 1.7355072463768115, "no_speech_prob": 0.014904976822435856}, {"id": 684, "seek": 378848, "start": 3793.2, "end": 3797.76, "text": " translations. You don't have data, so you translate your good English data, your gold standard data,", "tokens": [50600, 37578, 13, 509, 500, 380, 362, 1412, 11, 370, 291, 13799, 428, 665, 3669, 1412, 11, 428, 3821, 3832, 1412, 11, 50828], "temperature": 0.0, "avg_logprob": -0.10850787162780762, "compression_ratio": 1.7355072463768115, "no_speech_prob": 0.014904976822435856}, {"id": 685, "seek": 378848, "start": 3797.76, "end": 3803.68, "text": " or your good Mandarin Chinese data into many different languages. Here is where it gets", "tokens": [50828, 420, 428, 665, 42292, 4649, 1412, 666, 867, 819, 8650, 13, 1692, 307, 689, 309, 2170, 51124], "temperature": 0.0, "avg_logprob": -0.10850787162780762, "compression_ratio": 1.7355072463768115, "no_speech_prob": 0.014904976822435856}, {"id": 686, "seek": 378848, "start": 3803.68, "end": 3808.8, "text": " interesting. Translation models typically have what we call translation ease. There's these weird", "tokens": [51124, 1880, 13, 6531, 24278, 5245, 5850, 362, 437, 321, 818, 12853, 12708, 13, 821, 311, 613, 3657, 51380], "temperature": 0.0, "avg_logprob": -0.10850787162780762, "compression_ratio": 1.7355072463768115, "no_speech_prob": 0.014904976822435856}, {"id": 687, "seek": 378848, "start": 3808.8, "end": 3814.88, "text": " artifacts that pop up. So you might have like, odd enumeration where instead of like the one,", "tokens": [51380, 24617, 300, 1665, 493, 13, 407, 291, 1062, 362, 411, 11, 7401, 465, 449, 5053, 689, 2602, 295, 411, 264, 472, 11, 51684], "temperature": 0.0, "avg_logprob": -0.10850787162780762, "compression_ratio": 1.7355072463768115, "no_speech_prob": 0.014904976822435856}, {"id": 688, "seek": 381488, "start": 3814.88, "end": 3819.6800000000003, "text": " two, three, it spells out one, two, three. So it's just, and it's very annoying for people who", "tokens": [50364, 732, 11, 1045, 11, 309, 25053, 484, 472, 11, 732, 11, 1045, 13, 407, 309, 311, 445, 11, 293, 309, 311, 588, 11304, 337, 561, 567, 50604], "temperature": 0.0, "avg_logprob": -0.09073965890066964, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.025898916646838188}, {"id": 689, "seek": 381488, "start": 3819.6800000000003, "end": 3825.28, "text": " have to experience it because it gets imparted to the model, the downstream model. So we did", "tokens": [50604, 362, 281, 1752, 309, 570, 309, 2170, 32177, 292, 281, 264, 2316, 11, 264, 30621, 2316, 13, 407, 321, 630, 50884], "temperature": 0.0, "avg_logprob": -0.09073965890066964, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.025898916646838188}, {"id": 690, "seek": 381488, "start": 3825.28, "end": 3831.52, "text": " something which I think is very fun with this paper where we said, well, the whole goal of RLHF", "tokens": [50884, 746, 597, 286, 519, 307, 588, 1019, 365, 341, 3035, 689, 321, 848, 11, 731, 11, 264, 1379, 3387, 295, 497, 43, 39, 37, 51196], "temperature": 0.0, "avg_logprob": -0.09073965890066964, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.025898916646838188}, {"id": 691, "seek": 381488, "start": 3831.52, "end": 3837.52, "text": " is to steer away from certain parts of the distribution, steer towards other parts. And so", "tokens": [51196, 307, 281, 30814, 1314, 490, 1629, 3166, 295, 264, 7316, 11, 30814, 3030, 661, 3166, 13, 400, 370, 51496], "temperature": 0.0, "avg_logprob": -0.09073965890066964, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.025898916646838188}, {"id": 692, "seek": 381488, "start": 3837.52, "end": 3842.2400000000002, "text": " what we did for our preference pairs, so let's think about the normal way preference pairs are", "tokens": [51496, 437, 321, 630, 337, 527, 17502, 15494, 11, 370, 718, 311, 519, 466, 264, 2710, 636, 17502, 15494, 366, 51732], "temperature": 0.0, "avg_logprob": -0.09073965890066964, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.025898916646838188}, {"id": 693, "seek": 384224, "start": 3842.24, "end": 3848.16, "text": " done. It's quite expensive and time intensive. You have to go get annotators and you're asking", "tokens": [50364, 1096, 13, 467, 311, 1596, 5124, 293, 565, 18957, 13, 509, 362, 281, 352, 483, 25339, 3391, 293, 291, 434, 3365, 50660], "temperature": 0.0, "avg_logprob": -0.1138150630853115, "compression_ratio": 1.6202090592334495, "no_speech_prob": 0.012019569054245949}, {"id": 694, "seek": 384224, "start": 3848.16, "end": 3855.4399999999996, "text": " humans which one do you prefer. We did this really fun, I would say, trick here where we said, well,", "tokens": [50660, 6255, 597, 472, 360, 291, 4382, 13, 492, 630, 341, 534, 1019, 11, 286, 576, 584, 11, 4282, 510, 689, 321, 848, 11, 731, 11, 51024], "temperature": 0.0, "avg_logprob": -0.1138150630853115, "compression_ratio": 1.6202090592334495, "no_speech_prob": 0.012019569054245949}, {"id": 695, "seek": 384224, "start": 3855.4399999999996, "end": 3860.8799999999997, "text": " we know we have translation pairs. We generate synthetic pair, the other pair, with what is", "tokens": [51024, 321, 458, 321, 362, 12853, 15494, 13, 492, 8460, 23420, 6119, 11, 264, 661, 6119, 11, 365, 437, 307, 51296], "temperature": 0.0, "avg_logprob": -0.1138150630853115, "compression_ratio": 1.6202090592334495, "no_speech_prob": 0.012019569054245949}, {"id": 696, "seek": 384224, "start": 3860.8799999999997, "end": 3866.4799999999996, "text": " a very high-performance model. In this case, we use Command R+, which is super-performance,", "tokens": [51296, 257, 588, 1090, 12, 50242, 2316, 13, 682, 341, 1389, 11, 321, 764, 17901, 497, 46797, 597, 307, 1687, 12, 50242, 11, 51576], "temperature": 0.0, "avg_logprob": -0.1138150630853115, "compression_ratio": 1.6202090592334495, "no_speech_prob": 0.012019569054245949}, {"id": 697, "seek": 384224, "start": 3866.4799999999996, "end": 3871.4399999999996, "text": " does very well in many different languages. And then we compare the two and we ask an", "tokens": [51576, 775, 588, 731, 294, 867, 819, 8650, 13, 400, 550, 321, 6794, 264, 732, 293, 321, 1029, 364, 51824], "temperature": 0.0, "avg_logprob": -0.1138150630853115, "compression_ratio": 1.6202090592334495, "no_speech_prob": 0.012019569054245949}, {"id": 698, "seek": 387144, "start": 3871.44, "end": 3877.52, "text": " Alem is a judge, which is better, the translated English or the sampled in the other language.", "tokens": [50364, 9366, 76, 307, 257, 6995, 11, 597, 307, 1101, 11, 264, 16805, 3669, 420, 264, 3247, 15551, 294, 264, 661, 2856, 13, 50668], "temperature": 0.0, "avg_logprob": -0.14031385301469682, "compression_ratio": 1.7572463768115942, "no_speech_prob": 0.004810189828276634}, {"id": 699, "seek": 387144, "start": 3877.52, "end": 3882.16, "text": " And what we found was this actually helped with translation artifacts because it steered the model", "tokens": [50668, 400, 437, 321, 1352, 390, 341, 767, 4254, 365, 12853, 24617, 570, 309, 2126, 4073, 264, 2316, 50900], "temperature": 0.0, "avg_logprob": -0.14031385301469682, "compression_ratio": 1.7572463768115942, "no_speech_prob": 0.004810189828276634}, {"id": 700, "seek": 387144, "start": 3882.16, "end": 3890.4, "text": " away from the bad, translated and towards the more versatile, fluid Command R+, generation. So", "tokens": [50900, 1314, 490, 264, 1578, 11, 16805, 293, 3030, 264, 544, 25057, 11, 9113, 17901, 497, 46797, 5125, 13, 407, 51312], "temperature": 0.0, "avg_logprob": -0.14031385301469682, "compression_ratio": 1.7572463768115942, "no_speech_prob": 0.004810189828276634}, {"id": 701, "seek": 387144, "start": 3890.4, "end": 3895.28, "text": " really interesting. And there were some percentage of time where the translated was better. And so", "tokens": [51312, 534, 1880, 13, 400, 456, 645, 512, 9668, 295, 565, 689, 264, 16805, 390, 1101, 13, 400, 370, 51556], "temperature": 0.0, "avg_logprob": -0.14031385301469682, "compression_ratio": 1.7572463768115942, "no_speech_prob": 0.004810189828276634}, {"id": 702, "seek": 387144, "start": 3895.28, "end": 3900.56, "text": " you got that nuance too. So very, very interesting. Yeah, amazing. And then that removed a lot of", "tokens": [51556, 291, 658, 300, 42625, 886, 13, 407, 588, 11, 588, 1880, 13, 865, 11, 2243, 13, 400, 550, 300, 7261, 257, 688, 295, 51820], "temperature": 0.0, "avg_logprob": -0.14031385301469682, "compression_ratio": 1.7572463768115942, "no_speech_prob": 0.004810189828276634}, {"id": 703, "seek": 390056, "start": 3900.56, "end": 3906.88, "text": " the translation artifacts. Yeah. Amazing. Sarah, this has been incredible. Where would you like", "tokens": [50364, 264, 12853, 24617, 13, 865, 13, 14165, 13, 9519, 11, 341, 575, 668, 4651, 13, 2305, 576, 291, 411, 50680], "temperature": 0.0, "avg_logprob": -0.16874015142047216, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.0070471325889229774}, {"id": 704, "seek": 390056, "start": 3906.88, "end": 3912.88, "text": " to point people to as well for your later stuff? Feel free. So, you know, I lead Co-Here4AI. So", "tokens": [50680, 281, 935, 561, 281, 382, 731, 337, 428, 1780, 1507, 30, 14113, 1737, 13, 407, 11, 291, 458, 11, 286, 1477, 3066, 12, 17685, 19, 48698, 13, 407, 50980], "temperature": 0.0, "avg_logprob": -0.16874015142047216, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.0070471325889229774}, {"id": 705, "seek": 390056, "start": 3912.88, "end": 3918.72, "text": " it's a research that we do a lot of fundamental research. And we, a lot of my work is on efficiency,", "tokens": [50980, 309, 311, 257, 2132, 300, 321, 360, 257, 688, 295, 8088, 2132, 13, 400, 321, 11, 257, 688, 295, 452, 589, 307, 322, 10493, 11, 51272], "temperature": 0.0, "avg_logprob": -0.16874015142047216, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.0070471325889229774}, {"id": 706, "seek": 390056, "start": 3919.92, "end": 3924.08, "text": " reliability and building these models that scale the next generation models. So you can go to", "tokens": [51332, 24550, 293, 2390, 613, 5245, 300, 4373, 264, 958, 5125, 5245, 13, 407, 291, 393, 352, 281, 51540], "temperature": 0.0, "avg_logprob": -0.16874015142047216, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.0070471325889229774}, {"id": 707, "seek": 390056, "start": 3924.08, "end": 3928.72, "text": " Co-Here4AI and take a look at some of our work and just a lovely being here again. It's really", "tokens": [51540, 3066, 12, 17685, 19, 48698, 293, 747, 257, 574, 412, 512, 295, 527, 589, 293, 445, 257, 7496, 885, 510, 797, 13, 467, 311, 534, 51772], "temperature": 0.0, "avg_logprob": -0.16874015142047216, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.0070471325889229774}, {"id": 708, "seek": 392872, "start": 3928.72, "end": 3941.4399999999996, "text": " nice catching up. Amazing. Sarah, thank you so much. Yeah, thank you.", "tokens": [50364, 1481, 16124, 493, 13, 14165, 13, 9519, 11, 1309, 291, 370, 709, 13, 865, 11, 1309, 291, 13, 51000], "temperature": 0.0, "avg_logprob": -0.29013411204020184, "compression_ratio": 1.078125, "no_speech_prob": 0.010917669162154198}], "language": "en"}