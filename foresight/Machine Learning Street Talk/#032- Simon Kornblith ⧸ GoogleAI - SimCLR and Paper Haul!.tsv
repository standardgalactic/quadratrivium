start	end	text
0	14600	Welcome back. Today we're talking with Dr Simon Cornblith, a research scientist in the
14600	20640	Google Brain team. Simon is most famous for being one of the authors on Simclear, the computer
20640	25360	vision paper that used self-supervised learning and the contrastive loss with loads of cool
25360	30760	image augmentations. Simon also used to be a neuroscientist.
30760	35920	When I was pretty young, I was interested in consciousness and how we create this kind
35920	41640	of impression of the external world inside our heads. And so I guess it's pretty obvious
41640	46560	how that translates into an interest in brains and how the brain works.
46560	52240	Turns out the neuroscience is really difficult. Progress is really slow and tedious. Simon's
52320	58160	goal is to understand the inner workings of neural networks, both in meat space and
58160	65640	in silicon. He initially thought that the artificial variety might be easier to understand.
65640	67360	He was in for a rude awakening.
67360	71200	So in a neural network, we can record all the neurons, which is extremely challenging
71200	77160	in a biological organism. And we can also manipulate the system in any kind of way that
77160	82080	we can imagine. But it still seems really hard to understand neural networks. I think there
82080	88800	are a lot of ideas from machine learning that will ultimately help us understand brains.
88800	93920	Maybe we could make some headway that might eventually translate back to brains. And so
93920	95800	that's how I ended up in machine learning.
95800	99360	People often try and anthropomorphise neural networks.
99360	103720	People try to relate whatever neural network they've built back to a brain and they say
103720	106800	that it works like the brain, but it doesn't work like the brain.
106800	111780	So Simon was involved in this paper, do wide and deep networks learn the same things, uncovering
111780	115460	how neural network representations vary with width and depth.
115460	121300	Simon pioneered this really fascinating way of comparing representations by comparing
121300	126820	features. And what this essentially amounts to is we need to have a similarity function
126820	133420	so that we can compare the representations in layers to themselves in different parts
133420	135780	of the network or indeed to other networks.
135780	139540	And so for this similarity measure to work well, the first thing Simon did was take two
139540	144580	architecturally identical networks, A and B, trained from different random initialisations
144580	152180	and just ensure that the third convolution layer is more self-similar to its counterpart
152180	154460	than any of the other layers.
154460	158420	If that works, then you're onto something. Turns out that's not super simple to do, but
158420	161740	Simon came up with this concept called the centred kernel alignment, which we'll talk
161740	163180	about on the call.
163180	169300	But this is actually super fascinating. We're talking about this idea here of using self-similarity
169300	175020	to reason about the evolution of representations throughout successive layers in the neural
175020	180940	network. And what Simon found is that you get this kind of characteristic blockiness.
180940	186380	So when you see these large blocks, what it means is that the representations are no longer
186380	188460	evolving in respect of time.
188460	193940	So it's showing here the representational similarity of all of the layers against themselves
193940	196140	and against all of the other layers.
196140	200260	So clearly there's this characteristic diagonal down the matrix, as you would see with any
200260	202260	self-similarity matrix.
202260	207700	And because this blockiness appears, it means that nothing is happening.
207700	211620	And what Simon realised is you can actually delete these layers from the neural network
211620	215100	and it wouldn't make any difference because it hasn't learned anything new.
215100	219900	But it's also a really interesting way of reasoning about a kind of pathology, a weird
219900	223060	thing that happens when you saturate a neural network.
223060	227100	So he said that this presence of this block structure is an indicator of the halting of
227100	231420	evolution and a strong indicator of over-parameterisation.
231420	236020	And he actually shows that this blockiness appears on deeper networks and wider networks.
236020	239220	But this concept of self-similarity analysis is not new to me.
239220	246060	On my PhD, I was fascinated in segmenting DJ-mixed music shows and I actually used the same techniques
246060	250260	for learning regimes in financial datasets later on.
250620	253900	This is an example of a DJ-mix which I segmented.
253900	259260	I came up with a dynamic programming algorithm which would essentially sum up all of the
259260	265260	tiles along this diagonal and compute the lowest costs contiguous segmentation.
265260	266580	And it's super interesting.
266580	271260	So here are two music tracks and you can see that they are more self-similar to each other
271260	275820	than they are any of the other tracks just because of the tone of the colour here.
275820	279700	And if you zoom into a track, you can even see that there are symmetries.
279700	284420	This part of the track here is a repetition from this part of the track here.
284420	289420	And you can tell that from this kind of symmetry pattern on the diagonal.
289420	292780	And you can see that there's a little bit in the track in the middle here which is not
292780	295740	similar to any other part of the track.
295740	300660	You see some really interesting stuff here and essentially I'm a huge fan of anyone using
300660	305420	self-similarity matrices for reasoning about the evolution of representations.
305420	307140	I think it's a fascinating idea.
307140	309980	So how did Simon come up with this measure of similarity?
309980	312580	The centred kernel alignment.
312580	317900	Jeff Hinton had another idea and I tried the idea that it worked but then we wondered is
317900	320540	there a simpler thing that worked.
320540	323140	And that's how we ended up with centred kernel alignment.
323140	327940	The blockiness in these matrices is absolutely fascinating but how much can we read into
327940	328940	it?
328940	333460	It's not clear what we should really expect in terms of how a neural network representation
333460	335060	evolves through the layers.
335060	340260	I think there's kind of some theory on what we should expect if all the layers are linear.
340260	344740	But like obviously the neural networks that we train are nonlinear and it's really important
344740	347340	to have a nonlinearity in between the layers.
347340	352780	If we see that nothing is changing from one layer to the next that's a really bad sign.
352780	356860	If the neural network representation isn't changing then obviously nothing's happening.
356860	361540	We couldn't have predicted this ahead of time based on what we know about neural network
361620	366940	theory and we couldn't have predicted it ahead of time based on the accuracy of the network.
366940	368740	Does this apply to ResNets though?
368740	371460	I thought that they could learn their own capacity.
371460	375780	You can either look at networks without residual connections where you do actually find that
375780	381580	at some depth the accuracy will start going down and in networks without residual connections
381580	388140	we find that the depth where accuracy starts to go down is like around the same depth where
388180	393020	you begin seeing this kind of block structure where many successive layers have similar
393020	397780	representations and it looks like the representation is no longer getting refined through the network.
397780	402420	Once you start getting these blocks making the network deeper, making the network wider
402420	405660	no longer really gives you any improvement in accuracy.
405660	410220	So it seems like this is basically telling you that the network has fit the data as much
410220	416820	as it can and there's no real advantage to using something bigger.
416820	422020	Next we move on to Simon's paper about using different loss functions on image classifiers
422020	425980	and he made some really interesting findings actually so the loss functions only really
425980	430780	seem to affect the penultimate layers in the neural network.
430780	434580	This also gives us some pretty useful insight into transfer learning.
434580	439500	The last third of the network is setting up the penultimate layer representation in a
439500	444700	way that is good for your loss function but the first two thirds of the network are somehow
444860	447420	just learning general features.
447420	452180	I think this also corresponds with the success of transfer learning where we can take features
452180	456260	that we've learned on one task and transfer them to some other task.
456260	457980	What's the implication though?
457980	464460	It seems, is the implication that the loss function is not having any impact on the representations
464460	465660	early on in the network?
465660	469340	That seems like quite a big implication.
469340	472460	Ultimately we're asking the network to do the same thing just in a slightly different
472460	473460	way.
473500	479940	Like some inverse correlation between the gains you get from a loss function and how
479940	481900	good it is for transfer learning.
481900	489900	If you use loss functions that give you higher accuracy on ImageNet, you tend to learn representations
489900	494060	that transfer substantially worse in that setting.
494060	499100	The loss functions that perform better lead classes to become more separated in the penultimate
499100	500100	layers.
500140	505340	To standard softmax loss, actually the classes are not that separated from each other in
505340	508180	the penultimate layer representation.
508180	513780	Right now on whatever TensorFlow Hub or Hugging Face repositories and so on, we have these
513780	518660	pre-trend models and the pre-trend models, they're like full stack models and people
518660	526820	usually take some sort of last or next to last hidden layer but maybe we should much
526900	532820	more focus on actually providing like half of a network to share, like determining which
532820	538220	are actually the best, good or general representations from a data set and so on.
538220	540060	It's a really interesting question.
540060	544740	If we just want to turn an image into a vector that we could then train a linear classifier
544740	547900	on top of, what is the best way of doing that?
547900	552340	Self-supervised pre-training, just like word vectors, gives us a really great starting
552340	557780	point to vectorize an image into a semantically relevant geometric space.
557780	561820	It's been a real game changer in the computer vision world since about 2018.
561820	565900	We want that neural network to learn a representation such that when we then just train a linear
565900	570860	classifier on top of that representation to classify image net, it's going to do well.
570860	575780	But we want to learn the initial representation without using any kind of labels.
575780	578780	So what is self-supervised pre-training for vision?
578780	582460	People came up with these kinds of tasks that you could try to train a neural network to
582460	586260	do so that it would learn some kind of good representation.
586260	593140	You're trying to learn some kind of representation space where you've got different patches from
593140	597340	an image or different augmentations from an image, just different representations of the
597340	602940	same image and you want to learn a representation space where these representations of the same
602940	608740	image are all close together in that representation space and they're far apart from the representations
609380	611340	of other images.
611340	616220	This surprisingly seems to lead to very good representations.
616220	623060	I was very fascinated by all of these different tricks that you apparently have to get and
623060	628140	so big kudos to figuring all of this out for the rest of us.
628140	631500	Data augmentation is absolutely key to making this work.
631500	635300	The important part of the recipe is data augmentation.
635300	640380	There are really only two super important data augmentations that we need.
640380	645860	So we have to take two different crops from the same image and then we have to do some
645860	648020	kind of color distortion.
648020	650380	Turns out though the architecture isn't that important.
650380	656780	You don't have to worry about architecture, engineering specifically or contrastive learning.
656780	658660	What was new in the CIM CLR paper?
658660	664060	We introduced the idea of this projection head in CIM Clear and we also spend a lot
664060	666220	of time studying the augmentation.
666220	668340	And what about the bring your own latent paper?
668340	678380	I don't really have any insight into how either BYOL or the more recent papers actually
678380	681860	are learning a representation that doesn't end up collapsing.
681860	686700	Why it doesn't happen relates to some mysteries about neural network training dynamics that
686700	688940	we still don't entirely understand.
688940	691620	We dive deep into data augmentation in general.
691620	695940	The data augmentation that you need for contrastive learning is different from the data augmentation
695940	698980	that you need for supervised learning because the task is different.
698980	704220	When you have contrastive learning, you have this problem that if there's just one feature
704220	710580	in your data that can be used to do the contrastive task to get images of the same example or
710580	715020	views of the same example close together and far apart from views of all the other examples.
715020	718980	If you could do that with one feature, that would be the only feature the network would
718980	722660	ever learn or it might be the only feature the network would ever learn.
722660	726780	And so with the augmentation, you're making the task harder so that the network actually
726780	730420	has to learn many different kinds of features.
730420	736580	We find that this color distortion actually is very important for self-supervised learning,
736580	741300	for contrastive learning, or as it doesn't really matter for supervised learning.
741300	746420	There seems to be this fascinating universality of representations, especially in vision.
746420	750740	I'm not trying to be flippant when I say this because practitioners have used ImageNet
750740	752740	on a variety of downstream tasks.
752740	756580	For example, they might use it for classifying circuit boards or something.
756580	759300	And the miraculous thing is it just seems to work quite well.
759300	763500	So do you think in your opinion that there is some kind of universality?
763500	769100	I'm very skeptical about universality of ImageNet for different tasks.
769100	772980	Even though there are lots of cars in ImageNet, if you pre-train on ImageNet and you fine
772980	778500	tune on that data set, you will learn to classify it faster and fewer steps than if you had
778500	783100	trained from scratch on the Stanford cars data set.
783100	786940	But you won't actually perform any better at the end.
786940	789860	Representations of images are not that universal.
789860	796140	And at least what works for natural images like those in ImageNet may not work on other
796140	797780	data sets.
797780	805860	Taking a bit from the universality of representations to the universality of augmentations, since
805860	812620	this is such a crucial part, do you think that there is a systematic way how we can
812620	815060	discover augmentations?
815060	817340	Right now, it seems to be kind of a whack-a-mole, right?
817340	818340	It's okay.
818340	820300	We just feed images and say, no, that's too easy.
820300	821300	We crop them.
821300	822580	Oh, no, it's the color histogram.
822580	825420	So we like whack on the color and then it works better.
825500	830740	Maybe someone finds out, oh, there is still this easy feature that the network, every
830740	836340	noun, then pays attention to, so we design a method to whack on that a bit.
836340	841700	Do you think there is a systematic way or will this kind of philosophically always rely
841700	847580	on us humans having a higher level inside of what we want to do with the data set?
847580	850460	So what comes next after data augmentation?
850460	852460	So would the next step be some simulation?
853180	854180	Do you know what I mean?
854180	857580	Where we impute physics and we impute some world knowledge and then, I don't know, whether
857580	859660	we train a machine learning model from that?
859660	866100	Yeah, I think there are definitely shortcomings in our current machine learning models, understandings
866100	867100	of the world.
867100	872340	There are probably things that we can't just solve by throwing more static images at them.
872340	878100	I think maybe the next step, rather than trying to immediately situate the machine
878100	883420	learning model in a simulated world, we could just think about video.
883420	889180	I think probably representation learning from video is going to be a big thing next year
889180	893420	or the year after, something sometime in the near future.
893420	895740	Finally, we talk about Simon's paper.
895740	900460	Big self-supervised models are strong semi-supervised learners.
900460	905980	What is a practical problem is the situation where you have a lot of unlabeled data and
905980	909340	then a very small amount of labeled data.
909340	915180	What I find fascinating is how many ideas come together in this paper.
915180	920580	You probably didn't sit down after a SimClear one and be like, all right, what do we do
920580	921580	for SimClear two?
921580	922580	Okay, let's do this.
922580	924220	So it tells me there was this process.
924220	930340	Could you, if you can, maybe elaborate a bit on how did you going to build up the system
930340	932820	towards the final output?
932820	937540	We also tried the approach of first fine-tuning the big network and then distilling it.
937540	940900	It turned out that worked a lot better.
940900	946540	What we found was this approach of pre-training, then fine-tuning, then distilling works a
946540	950660	lot better than pre-training, then distilling, then fine-tuning.
950660	956100	We probably shouldn't expect distillation of the kind that we do in SimClear v2 to work
956100	962660	substantially better than supervised distillation, which has been around for quite a while now.
962660	969820	I think what's impressive is that in the self-supervised case, in the contrastive case, distillation
969820	974460	basically allows you to recover the same accuracy that you would get from training supervised
974460	978980	from scratch, whereas without it, the accuracy is a lot worse.
978980	983940	So it seems like it maybe matters more in this contrastive case.
983940	989300	But I think generally when you do distillation in the supervised case, you can get maybe
989300	993580	a percentage point gain, maybe a couple of percentage points.
993580	998180	And I think that's probably about the limit in terms of the improvement that you could
998180	1004820	get from any kind of distillation-based approach over supervised training from scratch.
1004820	1010580	Can you use GANs, Generative Adversarial Neural Networks, to do data augmentation?
1010580	1012780	Or is that just a myth?
1012780	1015100	Simon certainly seems to think so.
1015100	1019260	Using a GAN to do data augmentation, you have this problem that you still don't actually
1019260	1021140	have more data.
1021140	1023620	You have a GAN that's trained on the same data.
1023620	1028460	And so it might help you because your way of encoding inductive bias into the GAN is
1028460	1032500	different from your way of encoding inductive bias into the neural network.
1032500	1037540	And maybe by having more inductive bias, you can learn a better function.
1037540	1041660	You still don't have more data, and it seems like without having more data, there's no
1041660	1046460	reason to expect a priority that you will be able to learn a better function.
1046460	1052820	Ironically, when you do the simple data augmentation, you do have more data because you put all
1052820	1058620	the knowledge in there as a human of what makes two images dissimilar visually, but
1058620	1064500	still equivalent semantically, which, again, is exactly the opposite.
1064500	1071180	It gives you images that are visually similar, but it has no intuition of what the semantic
1071180	1072980	similarity is.
1072980	1077180	We round off the show by talking about Simon's love of the Julia language.
1077180	1082380	Julia is a much better programming language than Python in many ways.
1082380	1087180	Julia is designed for these situations where maybe beyond just matrices, you have these
1087180	1091500	funny types of structured matrices, you have sparse matrices, and you can define special
1091500	1096700	methods for the product of a sparse matrix in a vector, or all sorts of things where
1096700	1100500	you might want different methods depending on the types.
1100500	1102300	I really hope you've enjoyed the show today.
1102300	1104380	We've had so much fun making it.
1104380	1107940	Remember to like, comment, and subscribe.
1107940	1113820	We love reading your comments, every single one of them, and we'll see you back next
1113820	1114820	week.
1114820	1119820	Welcome back to the Machine Learning Street Talk YouTube channel and podcast with my
1119820	1126100	two compadre, Syac, the neural network pruner, Paul, and Yannick, the Lightspeed protein
1126100	1129180	folder, Kiltcher.
1129180	1133780	Today we have an incredibly special guest, Simon Cornblith, and Simon got his PhD in
1133780	1136980	brain and cognitive sciences from MIT.
1136980	1140740	His undergrad was from Caltech, and he's a research scientist at Google Brain.
1140740	1143660	He's been there since about 2017.
1143660	1148900	He's been cited nearly 2,000 times, which for someone quite early in career is seriously
1148900	1149900	impressive.
1149900	1156500	He's got a keen interest in the digital humanities, in philosophy, computer science, machine learning,
1156500	1159140	computer vision, and neuroscience.
1159140	1162900	He used to be a neuroscientist before we started doing machine learning, and he tells us that
1162900	1167180	he's got some very strong opinions about neuroscience and machine learning, which we certainly will
1167180	1168860	be getting on to later.
1168860	1173300	He's a huge lover of the Julia language, so if you Google Simon's name, you'll see him
1173300	1178820	talking at about a million Julia conferences, so definitely check that out as well.
1178820	1183740	Simon pioneered the use of centered kernel alignment as a way of analyzing the evolution
1183740	1189180	of representations in layers, in network, and between networks of different architectures.
1189180	1193980	Now Simon, like me, is a lover of similarity matrices, and what can be gleaned from them?
1193980	1199220	On my own PhD, I worked with them a lot for music segmentation, and also for detecting
1199220	1201780	regimes in financial data sets.
1201780	1207220	When a block in a ResNet is no longer self-similar to previous layers early on, you might intuit
1207220	1212700	that it's moving into a new representational regime, or maybe it's just started hallucinating.
1212780	1216540	All of this stuff was covered in his paper, Do Wide and Deep Neural Networks Learn the
1216540	1221900	Same Things, and I find it fascinating that representation of self-similarity can reveal
1221900	1223540	network pathology.
1223540	1227580	Now in his paper, What's in a Loss Function for Image Classification, he noted that different
1227580	1233500	losses and regularizers have similar accuracies on several data sets, but using the same representational
1233500	1239500	evolution analysis, Simon gleaned that these losses and regularizers only affected the
1239500	1243340	penultimate layers in the neural network, revealing inherent limitations in what can
1243340	1246500	be achieved in manipulating the loss on a network.
1246500	1250140	Now next in the session today, we're going to talk about the Simclear paper, and this
1250140	1255940	was an incredibly exciting paper for unsupervised contrastive image learning with augmentations.
1255940	1260740	It introduced a learnable nonlinear transformation between the representations and the contrastive
1260740	1264100	loss, which massively improved the representations.
1264100	1268740	The composition of augmentations is super important, and whenever anyone asks me about
1268980	1272100	what are the different data augmentations in computer vision, I always point them to
1272100	1276940	the SimCLR paper because it's got this wonderful matrix, and in that matrix it was shown that
1276940	1281740	the crop and the color I think were the most effective augmentations, but Simon also noted
1281740	1285580	that the batch sizes were super important, and the paper improved over the state of the
1285580	1290020	art on the ImageNet top one, and actually matched unsupervised methods for the first
1290020	1292420	time, albeit with many more parameters.
1292420	1295820	But the final paper we're going to talk about today is Big Self-Supervised Models, a strong
1295900	1300060	semi-supervised learners, and this is where you can learn from fewer labeled examples while
1300060	1304940	making use of a large amount of unlabeled data, and with unsupervised pre-training
1304940	1310380	on SimCLR v2, supervised fine-tuning on a few labeled examples, and then distillation
1310380	1314900	with unlabeled examples, this approach improved the label efficiency over previous state-of-the-art
1314900	1315900	methods.
1315900	1320140	I remember Yannick Lightspeed Kilcher made a video on this one, which I watched a few
1320140	1323900	months ago, so Yannick will have all of that completely fresh in his mind.
1323980	1326980	Anyway, Simon, it's an absolute pleasure to welcome you to the show.
1326980	1327980	Thank you so much for coming.
1327980	1328980	It's great to be here.
1328980	1329980	Amazing.
1329980	1331780	How did you get into machine learning?
1331780	1338540	So I guess first I got into neuroscience, and then I got disillusioned with neuroscience.
1338540	1343700	When I was pretty young, I was interested in consciousness and how we create this kind
1343700	1348060	of impression of the external world inside our heads.
1348060	1352500	And so I guess it's pretty obvious how that translates into an interest in brains and
1352500	1354300	how the brain works.
1354300	1360460	So I spent both four years as an undergraduate doing neuroscience research, and then seven
1360460	1366900	years working with monkeys at MIT trying to figure out how monkey brains work.
1366900	1373460	And then after that, I felt like we weren't getting very far by trying to record from
1373460	1378100	neurons in monkeys' brains and figure out how those neurons work.
1378100	1383780	So I thought about what other ways are there approaching this problem?
1383780	1390380	How could we think about how to understand how the brain is doing tasks?
1390380	1396500	And it seemed like maybe by building systems that can do those tasks well that are not
1396500	1398740	biological, we could learn more.
1398740	1401060	So that's how I got into machine learning.
1401060	1405820	I joined the Google AI residency program, which is like this great program that Google
1405820	1412660	has to take people who have extensive background in some field that is not machine learning
1412660	1415580	and train them to do machine learning.
1415580	1420660	And I ended up at Google, and initially I thought I'm going to spend a year here learning
1420660	1425140	about machine learning related stuff, and then maybe I'll go back to neuroscience and
1425140	1429820	I'll decide the tools for machine learning could be applied back to brains, and maybe
1429820	1434580	we can learn more about brains by applying the tools of machine learning there.
1434580	1439260	But ultimately I decided I was more interested in just looking at how the neural networks
1439260	1444500	work and also in the engineering challenges of building better neural networks, which
1444500	1446500	I actually think are fun.
1446500	1451020	One of the thoughts that came to my mind is it's fascinating looking at the kind of introspective
1451020	1455380	analysis that you've been conducting with neural networks, but could you contrast that
1455380	1456380	with neuroscience?
1456380	1462300	Because as I understand, you have MRI scans and you have different ways of trying to visualize
1462300	1466620	and reason about the behavior of a brain, but you can't really tweak the architecture
1466620	1470780	and tweak all of the knobs and the levers in quite the same way you do in machine learning.
1470780	1477020	Yeah, so like in neuroscience people also use this analysis across different individuals
1477020	1479740	or different organisms or whatever.
1479740	1484620	It is a tool that people use in neuroscience as well, but I guess they're limited in the
1484620	1490060	ways in which they could manipulate the systems that are providing these representations.
1490060	1495900	So in neuroscience, you're always constrained by data, so you can compare representations
1495900	1500340	of images across individuals by doing MRI scans.
1500340	1505020	But first of all, you might not get a very good idea of how the brain is representing
1505020	1508580	those images because there's a lot of noise in the MRI scan and there's a limit to how
1508580	1513980	long you can scan each person, whereas I guess in a neural network, noise is not a problem.
1513980	1518340	The entire system's deterministic, we just pass in the image and we get the representation
1518340	1519340	vector.
1519460	1524020	And you also have these kinds of limits of, like, we can't see what happens if people
1524020	1529500	have bigger brains, like we can't manipulate the architecture in those kinds of ways.
1529500	1534660	So even though we can look at how intact brains are working, we can't see how representations
1534660	1538380	change when we manipulate them all that easily.
1538380	1542340	And I guess, again, in machine learning, like we can do all of those things.
1542340	1545220	We can look at what happens when we change the loss function.
1545220	1548700	We can look at what happens when we make the network deeper or wider.
1548700	1554140	So I think there are, like, some really cool ways that even the same techniques can be
1554140	1559420	applied in machine learning that they couldn't be applied in neuroscience.
1559420	1565700	I felt like we weren't getting very far by trying to record from neurons in monkey's
1565700	1569140	brains and figure out how those neurons work.
1569140	1575020	Like it didn't really seem like a very effective way of figuring out how the brain constructs
1575220	1578900	this kind of internal representation of the world.
1578900	1584460	So from there, I thought about what could we actually do to understand this?
1584460	1589940	And it seemed like the most promising thing to do was to look at what happens in simpler
1589940	1595540	systems that we can construct ourselves and where we can analyze the behavior of everything
1595540	1596980	inside the system.
1596980	1601020	So in a neural network, we can record all the neurons, which is extremely challenging
1601020	1602900	in a biological organism.
1602940	1608100	And we can also manipulate the system in any kind of way that we can imagine.
1608100	1611700	But it still seems like really hard to understand neural networks.
1611700	1617500	So it seemed like maybe this was a more tractable challenge and a challenge where maybe we could
1617500	1621740	make some headway that might eventually translate back to brains.
1621740	1623780	And so that's how I ended up in machine learning.
1623780	1626740	I guess there are, like, other great things about machine learning.
1626740	1630940	I guess the pay is much better than in, like, academic neuroscience.
1630980	1636180	But really, I think, like, it's a logical progression based on the ideas that I was
1636180	1637020	interested in.
1637020	1641540	And I am still interested in the same sorts of ideas.
1641540	1647700	Do you still think now that you're in machine learning and have made some progress here that
1647700	1653660	there is a good chance that we're going to map our knowledge that we gain back to the brain?
1653660	1658380	Or do you think there is a bit of a disconnect?
1658380	1660740	I think that's a really good question.
1660740	1665380	I think there is definitely some knowledge that we're going to get from machine learning
1665380	1667020	that will map back to the brain.
1667020	1672380	I think, like, in terms of general principles and ways of looking at how, like, information
1672380	1678220	processing systems work, I think there are a lot of ideas from machine learning that
1678220	1681420	will ultimately help us understand brains.
1681420	1686780	I'm a little less sure whether we're going to build, like, a machine learning system
1686780	1687780	that is a brain.
1687780	1692380	I think there's a disconnect between the way that the systems that we build work and
1692380	1693820	the way that biology works.
1693820	1697980	And I think that's insurmountable just because there's differences between what you can
1697980	1703220	build efficiently with cells and what you can build efficiently in silicon.
1703220	1708860	But in terms of approaches to understanding, in terms of building tools to understand things,
1708860	1714660	the tools that we build in machine learning, I think will eventually be useful in neuroscience.
1714660	1720460	So people make a lot of analogies and they make a lot of claims about neuroscience in
1720460	1723540	connections with neural networks.
1723540	1728100	Is there a statement or a bunch of statements that you hear over and over again where you
1728100	1731580	just cringe because they're so wrong?
1731580	1733380	Is that something that happens to you?
1733380	1735300	I can imagine it would.
1735300	1736300	Yeah.
1736300	1737300	Yeah.
1737300	1741140	So I think there's this, like, kind of basic fact that neural networks are inspired by
1741140	1743060	brains, which is true.
1743060	1747940	Then there's all this other stuff where people try to relate whatever neural network they've
1747940	1751700	built back to a brain and they say that it works like the brain, but it doesn't work
1751700	1752700	like the brain.
1752700	1757940	There's still this huge kind of disconnect in how the system is actually operating.
1757940	1760500	The brain is not literally doing back prop.
1760500	1762820	It might be doing something that's like back prop.
1762820	1769100	We still don't really know, but it's not literally computing gradients by automatic differentiation.
1769100	1773340	And I'm fascinated to talk about this line of reasoning that you have because you're
1773340	1778020	clearly the kind of guy that you want to reason about the behavior of models and in particular
1778020	1780220	the evolution of representations.
1780220	1783380	And I watched one of your presentations on YouTube where you were talking about how you
1783380	1786620	can compare the representations by comparing features.
1786620	1790620	And of course, the naive way of doing is the dot product or some variations of that.
1790620	1792100	Turns out that doesn't work very well.
1792100	1797420	And you came up with this wonderful metric called the centered kernel alignment.
1797420	1800020	So how did that all come about?
1800020	1806420	The way we came up with that idea was that Jeff Hinton had another idea and I tried the
1806420	1811860	idea and it worked, but then we wondered, is there a simpler thing that worked?
1811860	1814300	And that's how we ended up with centered kernel alignment.
1814300	1819620	I guess the problem that we had in trying to come up with a way of comparing similarity
1819620	1824820	of neural network representations is that it's really hard to know what is a good way.
1824820	1828780	Like it's not something where you can really develop a good benchmark.
1828780	1834300	So like in the paper, we came up with this simple sanity check where the idea is basically
1834300	1839460	we've got two architecturally identical neural networks and we just train them from different
1839460	1841260	random initializations.
1841260	1845700	And so we want it to be the case that if you measure like the similarity between a layer
1845700	1851100	from network A and all the layers from network B, that the most similar layer in network
1851100	1853820	B is going to be the architecturally corresponding layer.
1853820	1859540	So if we have layer two from network A, it should be more similar to layer two from network
1859540	1862420	B than layer three or layer four.
1862420	1868860	And so like basically we found that what people had been doing before didn't always pass that
1868860	1870380	sanity check.
1870380	1874620	And we basically tried to come up with the simplest way of building a similarity index
1874620	1877580	that did actually pass that sanity check.
1877580	1880020	And that's how we ended up with centered kernel alignment.
1881020	1885260	Yeah, because I think you showed that the canonical correlation analysis only worked
1885260	1887940	about, I think at an accuracy of about 1.4%.
1887940	1890220	So it's complete apples and oranges.
1890220	1894180	But this absolutely fascinates me though, because when you plot this thing in this kind
1894180	1900100	of self similarity matrix, you can glean so much about the evolution as a function of
1900100	1901100	time.
1901100	1904260	And because you talk about this in one of your other papers as well, that there's this
1904260	1906700	characteristic blockiness.
1906700	1912780	And when you see blockiness, that successive layers are similar to versions of themselves
1912780	1914140	in the past.
1914140	1917700	And that kind of means that they're not evolving anymore.
1917700	1922260	And you then made the intuition in your paper that, well, essentially it's redundant information.
1922260	1925380	If it's not learning anything new, I can just delete that block.
1925380	1928740	I can just delete those layers from the neural network and it won't make any difference.
1928740	1930540	And indeed it didn't.
1930540	1933540	Yeah.
1933540	1941100	Could you, for people listening, explain the similarity measure you came up with in principle,
1941100	1945180	just so we can imagine something, how that should even work?
1945180	1946180	Yeah.
1946180	1951620	So I guess the idea is you've got a neural network and you feed some set of examples,
1951620	1953900	like multiple examples through the neural network.
1953900	1960100	And now you've got some matrix where the rows of the matrix are different examples and the
1960100	1962820	columns are different neurons.
1962820	1967940	So yeah, you can imagine this as if you have vectors of activations for each example, you've
1967940	1970460	stacked them real wise.
1970460	1974900	So now what do we do with that to compare two neural networks trained from different
1974900	1976500	random initializations?
1976500	1981140	The problem is if we were to just take the square difference between those matrices,
1981140	1986780	we have this problem that the neurons between these two different networks aren't necessarily
1986780	1990500	aligned in any way if they're trained from different random initializations.
1990500	1994660	Even if we had exactly the same neurons, we shouldn't expect that neuron one would be
1994660	1998420	the same, representing the same thing in both networks.
1998420	2001180	So we need some way to get around that problem.
2001180	2008860	One way around this problem is instead of comparing these original matrices, we're going
2008860	2014820	to make matrices that measure the similarity of each example to each other example for
2014820	2017420	one particular network.
2017420	2022740	So if we've got example A and example B, we can measure their similarity very simply
2022740	2026260	just by taking the dot product between those two vectors.
2026260	2030060	And now because we're measuring similarity from the same network, we don't have to worry
2030060	2032180	about this alignment problem.
2032180	2037860	And we get some idea of how similar different examples are to each other according to the
2037860	2043100	representation in network A. So if we do that for all the examples, we get some examples
2043100	2045300	by examples matrix.
2045300	2049660	And then we can do that both for our first network and for our second network.
2049660	2054620	So after we've done that, we've got these two examples by examples matrices.
2054620	2059780	And then the easy way to compare those matrices is we just reshape them to vectors and we
2059780	2062620	take the dot product again between those vectors.
2062620	2068660	So now we've measured the similarities between the similarities of the examples.
2068660	2073660	And this doesn't have this problem of aligning the neurons because instead of measuring similarities
2073660	2077780	of neurons, we're measuring similarities of examples and then we're comparing those
2077780	2078780	similarities.
2078780	2082620	So ultimately, we do that, we take that dot product and then we normalize it in a way
2082620	2085020	that makes it invariant to scaling.
2085020	2090420	So if you just took the dot product, you'd have this problem that scaling all of the
2090420	2095420	features by some number, if you scale everything by a factor of two, the dot product will go
2095420	2097380	up by a factor of two.
2097380	2101860	And so we just apply some normalization so that kind of scaling will not affect the similarity
2101860	2108420	index and we get centered kernel alignment, which gives us a similarity score between
2108420	2110060	zero and one.
2110060	2113460	The fascinating thing is that you can replace that dot product with a kernel because it's
2113460	2114580	a gram matrix.
2114580	2115580	Yeah.
2115580	2119900	So did you find that it made a difference if you use, let's say, the RBF kernel?
2119900	2120900	Yeah.
2120900	2121900	Yeah.
2121900	2125700	So, yeah, basically when we're measuring the similarities between examples, we can just
2125700	2129180	instead of taking the dot product between the representations of the different examples,
2129180	2133900	we can take the kernel between one example and another example because the kernel is
2133900	2136460	also a way of measuring similarity.
2136460	2138620	And so we tried that.
2138620	2141940	It turns out that like for CNNs, it didn't really make a difference.
2141940	2146340	Like the RBF kernel worked, but sort of just taking a regular dot product.
2146340	2152260	But we did find in the appendix of that paper that if you instead use an RBF kernel with
2152260	2157260	a transformer, it actually does work better than taking a normal dot product.
2157260	2162300	And I think like part of what's going on is that sometimes you want it to be the case
2162300	2168300	that when you're measuring similarity, you care more about the distances between the
2168300	2173540	examples that you're close to than the distances to the examples that you're far away from.
2173540	2178260	Like once you're really far away from something, maybe it doesn't matter so much if you're
2178260	2184900	10 times as far away because like you're already so far, you're already not going to...
2184900	2190380	You don't really care how far away something is once you're far enough.
2190380	2194980	And the RBF kernel takes that into account in a way that a linear dot product wouldn't.
2194980	2203660	The linear dot product is like very sensitive to the global distances in the space.
2203660	2207020	What I find fascinating is that you can glean so much from the blockiness, right?
2207020	2211380	So you are saying that as it becomes blockier, it might be an indication that it's become
2211380	2213700	saturated in some sense.
2213700	2219940	And I'm also interested in a way, we already know that the representations in neural networks
2219940	2223500	are increasingly abstract, so they don't necessarily bear any resemblance to the beginning.
2223500	2228260	So when we're looking at the cell similarity matrix, we don't necessarily want the representations
2228260	2232340	on the final penultimate layers to be similar to the ones at the beginning.
2232340	2235260	We want there to be a continuous evolution.
2235260	2240180	We don't want to have a stalled evolution because that would correspond to this blockiness.
2240180	2245940	But is it when you've stalled for a long time, is that when it becomes pathological?
2245940	2249380	Because we want it to evolve in stops and starts, don't we?
2249380	2250380	Yeah.
2250380	2255500	I think it's not clear what we should really expect in terms of how a neural network representation
2255500	2257060	evolves through the layers.
2257060	2262340	I think there's kind of some theory on what we should expect if all the layers are linear.
2262340	2266780	But obviously, the neural networks that we train are nonlinear, and it's really important
2266780	2269380	to have a nonlinearity in between the layers.
2269380	2274180	And so at that point, it's really hard to reason about what the optimal thing for a
2274180	2276700	neural network to do actually is.
2276700	2280180	I think it's something that we can really only study empirically.
2280180	2284220	On the other hand, I do think if we see that nothing is changing from one layer to the
2284220	2286820	next, that's a really bad sign.
2286820	2290700	If the neural network representation isn't changing, then obviously nothing's happening.
2290700	2295540	But I guess it's unclear whether we should expect abrupt shifts or we want things to
2295540	2297380	happen slowly between the layers.
2297380	2302700	I'm not sure whether we really have the theoretical knowledge to say what is best.
2302700	2303700	Yeah.
2303700	2307940	I'd love to see this as a kind of tool in our toolbox that we could use on different network
2307940	2308940	architectures.
2308940	2312780	But you said that the other learn features are shared across different initializations
2312780	2316300	and architectures, particularly across the depths of the network.
2316300	2322380	So it almost seems as if this blockiness is separate to your work in wide and deep neural
2322380	2325900	networks because you showed that the width and the depth have got different effects on
2325900	2329620	network predictions at the example level or at the class level.
2329620	2332140	But the blockiness almost seems to be an orthogonal thing.
2332140	2336420	That's just when you have this kind of saturation of the network, you see the blockiness.
2336420	2337420	Yeah.
2337420	2338420	Yeah.
2338420	2342980	So initially we had hoped that we could look at other similarities between wide networks
2342980	2346180	and deep networks in their representations.
2346180	2349620	But like when we did those experiments, we actually just found that if you make the network
2349620	2354140	really wide, you get this kind of blockiness in the representations and those blocks are
2354140	2357220	like dissimilar across different initializations.
2357220	2359700	And then the same thing happens if you make the network really deep.
2359700	2363200	We see these like big blocks in the representations.
2363200	2369380	So that made it hard to study these very wide and very deep networks from the representational
2369380	2371020	similarity perspective.
2371060	2374380	But at the same time, I think it's like a really interesting observation.
2374380	2379260	Like it's something where we couldn't have predicted this ahead of time based on what
2379260	2383980	we know about neural network theory and we couldn't have predicted it ahead of time based
2383980	2386100	on the accuracy of the network.
2386100	2390860	It's something where we really needed like these techniques for looking at the internal
2390860	2395620	representations of neural networks to see what was happening inside of them.
2395700	2400820	There's this whole literature that takes a look at a network's expressibility with
2400820	2403100	regards to its depth and width.
2403100	2407980	So could you just explain it to us whether or not we should be able to meaningfully quantify
2407980	2413220	or formulate the expressibility of a neural network with regards to your analysis made
2413220	2414220	on that?
2414220	2415220	Yeah.
2415220	2418780	So there's this work that looks at like kind of the functions that can be expressed by
2418780	2422100	wide networks and the functions that can be expressed by deep networks.
2422100	2428180	And I guess like the neural networks seem to become exponentially more expressive as
2428180	2429900	you make them deeper.
2429900	2433460	So it seems like in that sense depth is more important than width.
2433460	2438100	But on the other hand, like the neural networks that we actually train in this paper, like
2438100	2442220	both the wide networks and the deep networks are big enough that they can overfit the entire
2442220	2443540	training set.
2443540	2448460	So in this case, like the expressibility of the network is not really important.
2448660	2452260	Important is the function that the network actually ends up learning.
2452260	2457740	So I guess even though networks could express more functions when they're deep, what we're
2457740	2463140	really studying is the function that you actually get when you train the neural network by gradient
2463140	2467260	descent on some data, what the optimization process actually finds.
2467260	2472060	One thing as well in that paper, you talked about the network pathology, right?
2472060	2478260	You said that two times depth accuracy 95%, four times 93.2, eight times 91.9.
2478260	2481460	So because this is this runs counter to what a lot of us would intuit.
2481460	2483460	We think that you can have as much depth as you want.
2483460	2487260	And architectures like ResNet in some sense, they learn their own capacity.
2487260	2489340	There is a pathology there happening clearly.
2489340	2493780	And how would you determine that from this visualization?
2493780	2496500	Yeah, so I guess there are two kind of results.
2496500	2502020	So like you can either look at networks without residual connections, where you do actually
2502100	2505780	find that at some depth, the accuracy will start going down.
2505780	2511820	And in networks without residual connections, we find that like that the depth where accuracy
2511820	2516860	starts to go down is like around the same depth where you begin seeing this kind of block
2516860	2520820	structure where many successive layers have similar representations.
2520820	2524740	And it looks like the representation is no longer getting refined through the network.
2524740	2527740	Yeah, I mean, with ResNets, you can make them much deeper.
2527740	2532860	And it seems like it doesn't hurt accuracy as much even once you start getting these blocks.
2533100	2537260	But it also seems once you start getting these blocks, making the network deeper, making
2537260	2541580	that work wider, no longer really gives you any improvement in accuracy.
2541580	2547020	So it seems like this is basically telling you that the network has fit the data as much as it can.
2547020	2551940	And and there's no real advantage to using something bigger.
2553180	2557340	Fascinating. Let's move on to another paper that you've done, which is quite related in
2557340	2560020	terms of you've used the same analysis to reason about it.
2560020	2563260	But you had a paper called What's in a Loss Function for Image Classification.
2563420	2567820	And you looked at a whole bunch of different label smoothing and regularizers, which are
2567820	2569580	things that you do on the end of the network.
2569580	2574820	And you identified differences in accuracy and calibration and out of domain distribution.
2575020	2577620	And you made some really interesting observations.
2577620	2580900	So by the way, we're talking about things like do we use the softmax or the squared
2580900	2583780	area or dropout or label smoothing or logic penalty.
2583940	2587540	But you noticed using the same analysis technique that only affected the
2587540	2590700	representations on the penultimate layers of the neural network.
2590900	2592580	What's going on there?
2592580	2595020	Yeah, so it's not just the penultimate layer.
2595020	2600500	It's like the the last maybe third of the network is affected by the loss function.
2600500	2605460	But then the first two thirds of the network, it seems like you learn the same representation
2605460	2607460	no matter what loss function you use.
2607460	2609860	So it doesn't change if you use label smoothing.
2610100	2615300	It doesn't even change if you use mean squared error instead of using softmax cross entropy.
2615300	2620260	You still basically learn the same representation for the first two thirds of the network.
2620260	2624020	And I think it's still it's a bit of a puzzle to us why this happens.
2624020	2627540	Clearly, it matters that you're training the network with the loss function.
2627540	2632820	There's those layers in the first two thirds of the network do change from the initialization.
2632820	2638260	But I guess it seems that the last third of the network is setting up the penultimate
2638260	2642180	layer representation in a way that is good for your loss function.
2642180	2647540	But the first two thirds of the network are somehow just learning general features.
2647540	2651300	I think this also like corresponds with the success of transfer learning,
2651300	2655700	where we can take features that we've learned on one task and transfer them to some other task.
2656500	2661940	What's the implication that it seems is the implication that the loss function
2661940	2665780	is not having any impact on the representations early on in the network?
2666500	2668180	That seems like quite a big implication.
2669140	2674500	Yeah, I think the loss function must have some impact because if you don't train
2674500	2680180	the network, if you don't have any loss function at all, then the representation
2680180	2684820	in that first two thirds of the network is actually quite different.
2684820	2688980	I think what's really happening is there are these differences among the loss functions,
2688980	2692660	which don't really matter except later in the network.
2692740	2698500	Although they will give you a slight change in accuracy and slight changes in robustness,
2698500	2701940	they don't matter for this general feature learning process.
2701940	2707540	I guess maybe it's what we should expect because ultimately we're asking the network
2707540	2709700	to do the same thing just in a slightly different way.
2709700	2712660	We're still asking the network to classify images.
2712660	2717060	We're just asking it to provide slightly different outputs to produce a slightly
2717060	2720020	different representation at the penultimate layer.
2720100	2726900	Maybe we should expect that those earlier features that are just trying to
2727540	2733540	represent general things about images, those will be the same no matter what loss function we pick.
2734820	2739380	In your experiments, did you find whether or not model capacity has anything to do with it?
2739380	2744580	Yeah, so we didn't really investigate different model capacities in that paper.
2744580	2749380	I would expect that the same thing holds for a wide range of model capacities.
2749700	2753860	There's no indication from the experiments that if you use a bigger network or a slightly
2753860	2755860	smaller network that things would change all that much.
2757060	2761300	Yeah, I think it's still an open question how model capacity changes things.
2761300	2766180	I guess in the Sinclair paper, we found that model capacity can matter quite a bit.
2766180	2769300	Yeah, so the general hypothesis there should also hold,
2769300	2774180	even though your model is bigger or smaller, no matter how big or smaller your network is,
2774180	2779060	the general feature learning regime or paradigm should still hold no matter what
2779060	2781140	loss function you would end up using.
2781140	2785860	Yeah, that would be my guess. I think if you're in a regime where it's really hard for you to fit
2785860	2790500	the training data, if you have a very small network, it might be the case that you see more
2790500	2796740	differences in the earlier layers because it might be that the loss function really affects
2796740	2802100	what features are best there in a way that it wouldn't if the network is a bit bigger and if
2802100	2807940	it's more capable of fitting your training data. But I don't really know. I think this
2807940	2811540	is something that is probably worth looking at in some follow-up work.
2812420	2817220	And you found also there's implications for transfer learning with respect to the loss
2817220	2823540	function. There seems to be some inverse correlation between the gains you get from a loss
2823540	2829220	function and how good it is for transfer learning. Or is there a connection between
2829220	2831940	loss functions and regularizers and all of that?
2832660	2837380	Yeah, we look at just linear transfer in the paper. So if we take the features from the
2837380	2842420	penultimate layer and we try to use them directly for some other task, how good are those features
2842420	2850900	going to be? And what we found was that if you use loss functions that give you higher accuracy
2850900	2857940	on ImageNet, you tend to learn representations that transfer substantially worse in that setting.
2858900	2864340	And our intuition is you could learn many different kinds of representations in that
2864340	2870980	penultimate layer and still do a reasonable job of classifying ImageNet. But what seems to happen
2870980	2876820	is that the loss functions that perform better lead classes to become more separated in the
2876820	2883220	penultimate layer. So like class A and class B will be farther apart relative to the variability
2883220	2888820	within the class. And when you have a situation like that, you have this penultimate layer
2888820	2894260	representation that's specialized for the classes in ImageNet. Like you've got a thousand clusters
2894260	2900180	corresponding to the thousand classes in ImageNet. And so then if you want to use like those kinds
2900180	2906100	of representations for some other task, it will only really work well if you have exactly the
2906100	2912260	same classes that are in ImageNet because they're already organized by the ImageNet classes.
2912340	2915780	On the other hand, what we found is if you just use standard softmax loss,
2915780	2920500	actually the classes are not that separated from each other in the penultimate layer
2920500	2925860	representation. And because they're not that separated, there are these features that you
2925860	2931620	could use to classify things that are not ImageNet that still convey some kind of
2931620	2937060	useful information about the images that are not just their ImageNet class labels.
2937940	2943700	It hints at a bit of a future where, you know, like right now on whatever TensorFlow Hub or
2943700	2949220	HuggingFace repositories and so on, we have these pre-trend models. And the pre-trend models,
2949220	2956100	they're like full stack models and people usually take some sort of last or next to last hidden layer.
2956820	2964500	But maybe we should much more focus on actually providing like half of a network to share. Like
2964500	2970580	determining which are actually the best good or general representations from a data set and so on.
2970580	2973380	Do you have any of this in mind when you do work like this?
2974500	2981060	Yeah, at Google, what is generally best for us to do is just to fine-tune the whole network.
2981060	2984660	And if you fine-tune the whole network, it eliminates some of these issues with
2985300	2990260	the actual form of the representation in the penultimate layer. Because even if you have this
2990260	2994900	kind of highly specialized penultimate layer, when if you're allowed to change all the other
2994900	2998580	weights in the network, you can fix that and you can specialize the rest of the network
2999140	3004900	for some other task. But yeah, I think like it's a really interesting question. If we just want
3004900	3009780	to turn an image into a vector that we could then train a linear classifier on top of,
3009780	3014100	what is the best way of doing that? How should we approach that problem? And how should we approach
3014100	3019620	that problem if we want this very like general universal vector representation of an image that
3019620	3024020	would work well for a lot of different tasks? And I think we don't really have good ways of
3024020	3030820	doing that because basically this is all empirical, right? Like, we don't know what makes a good
3030820	3035620	universal representation of an image. We've just got to try a bunch of things and figure out what
3035620	3040420	works best. And I guess, yeah, the insight from this paper is like actually the loss function
3040420	3045940	that you use to train the network can make a huge difference there. Fascinating. I guess without
3045940	3052180	any further ado, we should move on to SIMCLEAR, a simple framework for contrastive learning of
3052180	3057540	visual representations. We've been absolutely fascinated by this concept of unsupervised
3057540	3062820	contrastive image representation learning algorithms. We've seen such a huge kind of step
3062820	3069060	forward, haven't we, over the last couple of years in this area? Yeah, it's pretty amazing to me.
3069620	3073860	Could you just go back to real basics? Imagine that people out there have been living in a
3073860	3078580	cave. They don't know what contrastive learning is. They don't know about image augmentation.
3078580	3083940	How would you frame the whole thing up? The self-supervised learning setup is we've got a
3083940	3091540	bunch of images and at least the initial like historical self-supervised learning setup is
3091540	3096980	we've got a bunch of images. We want to train some kind of neural network on it. And we want
3096980	3100980	that neural network to learn a representation such that when we then just train a linear
3100980	3105780	classifier on top of that representation to classify ImageNet, it's going to do well. But
3105780	3111780	we want to learn the initial representation without using any kind of labels. And yeah,
3111780	3116980	I guess there are a lot of different approaches that people tried for this problem. Like people
3116980	3124660	tried things like let's train a neural network so that we can cut up the image into just a grid
3124660	3129860	and shuffle the grid. And then the neural network has to figure out how to assemble these puzzle
3129860	3134900	pieces back into the original image. And maybe that'll give us a good representation. Or let's try
3135540	3141460	just rotating the images so we can have images that are rotated 90, 180, 270 degrees. And then
3141460	3147380	we'll have the neural network try to classify what rotation we fed into it. And so people came up
3147380	3152340	with these kinds of tasks that you could try to train a neural network to do so that it would learn
3152340	3157620	some kind of good representation. They were defined in this ad hoc way. Let's come up with some kind
3157620	3163300	of funny thing where you don't need a label. You can have the neural network trained to do
3163300	3169700	this kind of thing. And maybe it'll learn something about images. Starting in around 2018, there are
3169700	3177380	a few papers that basically suggested this alternative approach where you're trying to learn
3177380	3183780	some kind of representation space where you've got different patches from an image or different
3183860	3188420	augmentations from an image, just different representations of the same image. And you want
3188420	3194180	to learn a representation space where these representations of the same image are all close
3194180	3199860	together in that representation space. And they're far apart from the representations of other images.
3200580	3207220	This surprisingly seems to lead to very good representations. But it turns out there are a
3207220	3212980	lot of very important details to get this to work well. So it's really like a situation where the
3212980	3219300	basic idea is very simple. Let's create multiple views of an image and try to get them close to
3219300	3225140	each other and far away from everything else. But things like augmentation and things like the
3225140	3230820	exact way we set up the network end up being very important to learning a good representation with
3230820	3235780	this kind of technique. How does the negative sampling work? People have done this in different
3235780	3241620	ways. So in Simclear, our way of doing negative sampling is very simple. So basically,
3242340	3251140	we are attracting two views of the same image. And then we have a mini batch that has 4,096
3251140	3258420	images in it and two augmentations of each image. And so we are repelling using a softmax from all
3258420	3267460	of the other 8,190 views in that mini batch. Basically, we want our two augmentations of
3267460	3273140	the same image close and we want them to be far from the other 8,190 images.
3274420	3281140	Yeah, it's a bit of a throwback to work to VEC. I think it's pretty cool how these ideas just come
3281140	3289060	up through the eras and through the different models and so on. And there is seemingly always
3289060	3297540	another layer on top of these ideas. Pretty cool. Yeah. So if you only consider, you know,
3297540	3302900	two views that are coming out of the same image as the positive pair, so to speak, and all the other
3302900	3308740	views are coming out of the different images located in the same mini batch, wouldn't this hurt
3308740	3315780	the representation space to some extent? Let's say you have multiple images of dogs in a mini batch
3316180	3323220	of 4,096 samples. We would essentially want the representations of different dogs to map together
3323220	3328500	as closer as possible in the representation space while representations of cats from dogs would
3328500	3334580	get further away. Wouldn't we expect this? But how does Simclear ensure this rigorously? I guess
3334580	3339620	it's because of the larger batch sizes you use, but I still wanted to know from you.
3339620	3346420	Yeah, one thing is, even if we've got other kinds of images that we want to be close together in the
3346420	3352100	mini batch, even if we've got like a dog image and then another dog image and ultimately we want to
3352100	3358500	learn a representation space where maybe they're not so far apart, like on average, most of the
3358500	3364340	images in the mini batch are things that we want to be really far apart from. So maybe it doesn't
3364340	3372180	hurt that much if we're repelling from everything as opposed to just repelling from images that are
3372180	3378020	part of other classes. I think this actually is something that hurts current self-supervised
3378020	3382260	learning techniques and hurts contrastive techniques because we also know when you do the
3382260	3389220	contrastive loss, if you don't contrast against examples that are very close to you, that actually
3389300	3394420	improves things a little bit. So if you don't contrast against the very hard negatives,
3394420	3399860	we've found that gives you slightly higher accuracy when you do this linear evaluation.
3399860	3404340	That kind of suggests that this really is a problem with these techniques that maybe sometimes you
3404340	3409860	don't want to be as far apart from other images as the losses encouraging you to be. Now there's
3409860	3415300	one other aspect which is that in Simclear, we don't actually use the representation that's
3415300	3421780	feeding into the loss function. Like we have this projection head, an MLP on top of the network,
3421780	3426900	and instead of using that representation at the end of the network, we use a representation
3426900	3434100	that's two layers back. And so by using a representation that's two layers back, even if
3434100	3439300	in the final layer we're pushing things apart, we kind of figure that this earlier representation
3439300	3444020	might not have pushed apart the things that really are semantically similar. And indeed,
3444020	3449860	we find that using this earlier representation in the network leads to higher linear evaluation
3449860	3457140	accuracy. So it works better. I was very fascinated by all of these different tricks that you apparently
3457140	3464500	have to get. And so big kudos to figuring all of this out for the rest of us. There has been a
3464500	3472180	lot of follow-up work on this idea. A lot of modifications. There is this bootstrap your own
3472180	3479060	latent where they completely leave out the negative sampling. Then I think just like one or two weeks
3479060	3487460	ago, there was a paper saying if you build in a stop gradient into the contrastive loss,
3487460	3493220	you also apparently don't need a negative and so on. Do you have maybe from your own work or
3493220	3501300	from work of others, do you have any sort of current? If I were to build a self-supervised
3501380	3508740	contrastive representation learner today, what is the top things I should do? What is my recipe?
3508740	3515380	How do I go about it? The most important part of the recipe is data augmentation. So we're
3515380	3520420	going to use two views from the same image and it's very important how those two views are
3520420	3527460	constructed. But they're really only two super important data augmentations that we need. So
3527460	3533540	we have to take two different crops from the same image and then we have to do some kind of color
3533540	3541220	distortion. So in SimClear we use very aggressive color distortion. So that is probably the most
3541220	3546900	important part of the recipe. Then I guess we feed that representation into a neural network
3546900	3552420	and fortunately we found that you can just use a regular ResNet 50 for this part. You don't have
3552420	3559060	to worry about architecture, engineering, specifically for contrastive learning. Then
3559060	3564500	I think all of the work since SimClear also uses this idea of putting an MLP on top of the end of
3564500	3571060	the network and then using that to get whatever representation goes into the loss function,
3571060	3577300	but then discarding part of the MLP when we later just want the representation for a downstream
3577300	3585540	task. All of those pieces are pieces that are shared by all of these modern self-supervised
3585540	3591780	learning techniques. So like we introduced the idea of this projection head in SimClear and we
3591780	3596660	also spend a lot of time studying the augmentation although we were not the first people to
3597620	3602660	come up with the idea that the augmentation was important. Yeah, in terms of what the loss function
3602660	3608180	is, I guess it's surprising that there are so many things that work that we use this contrastive
3608180	3613060	loss in SimClear because it was what previous work had done and it's like intuitive that you might
3613060	3618740	want to learn a space where you're explicitly pushing away representations of other examples,
3619540	3626020	but I guess like in BYOL they aren't explicitly contrasting against representations of other
3626020	3632740	examples. So instead they have a network where they're taking a moving average of
3633620	3638580	the weights that they've been learning and they try to match the representation that's coming
3638580	3643860	out of the network that they're training to this representation of this moving average network
3644420	3650740	and somehow magically that works and I guess it doesn't even have to be a moving average. I think
3650740	3657140	you were referring to earlier like you can just match the representation of one network to
3657860	3661940	stop gradient of the same network as long as you're matching the representation in an earlier
3661940	3669700	layer and I think like it's still like mysterious why that should work. I don't really have any
3669700	3679700	insight into how either BYOL or the more recent papers actually are learning a representation
3679700	3684580	that doesn't end up collapsing. The problem is if you're trying to match some earlier representation
3685140	3689140	you could just collapse to the point where all of your representations are the same and then
3689700	3696180	like you would trivially be matching the earlier representation, but this doesn't happen and I
3696180	3701300	think why it doesn't happen relates to some mysteries about neural network training dynamics
3701300	3706820	that we still don't entirely understand. I'm absolutely fascinated by this concept of data
3706820	3711220	augmentation. Early on in my neural network career I just imagined it as being a way of
3711220	3716820	increasing the size of your training set, but in a sense you're not really adding new information.
3717460	3723380	You are creating semantically equivalent noise perturbations or examples similar to how BERT
3723380	3728340	works the NLP model it's like a denoising autoencoder and you're creating noise diversions of the
3728340	3732580	same thing and pushing the examples off the manifold. So there seems to be a dichotomy between
3732580	3736260	on the one hand augmenting your data and it's almost like you're stopping the neural network
3736260	3741780	from overfitting on things like the color or some specific feature you don't want to. You want to
3741780	3747140	have a bit of generalization, but at the same time you are saying those things over there it's
3747140	3751700	definitely nothing like that. The data augmentation that you need for contrastive learning is
3751700	3755780	different from the data augmentation that you need for supervised learning because the task is
3755780	3761220	different. When you have contrastive learning you have this problem that if there's just one
3761220	3767780	feature in your data that can be used to do the contrastive task to get images of the same example
3767780	3772260	or views of the same example close together and far apart from views of all the other examples.
3772260	3776340	If you could do that with one feature that would be the only feature the network would
3776340	3781460	ever learn or it might be the only feature the network would ever learn. And so with the augmentation
3781460	3784820	you're making the task harder so that the network actually has to learn
3785780	3791220	many different kinds of features. So I guess we find that this color distortion
3791220	3795780	actually is very important for self-supervised learning, for contrastive learning,
3795780	3801700	whereas it doesn't really matter for supervised learning. And what we think is going on is that
3801700	3807700	if you have two crops from the same image, generally their color histograms are surprisingly
3807700	3813940	similar. If you just plot out the intensity histogram of the image you can see that the
3813940	3819540	crops came from the same image. And that's a trick that the network is very good at doing
3819540	3824100	because I guess if you have ReLU activations they're very good at computing histograms.
3824820	3830500	And so by doing the color distortion we basically we don't let the network just learn
3831140	3835620	the color histograms in order to do the contrastive task. We force the network
3835620	3840660	to actually use other sorts of information and that ends up being like critical to the
3840660	3844980	performance of these contrastive methods. Like it basically doesn't work unless you do
3845700	3850180	that kind of aggressive color distortion. Because that seems to be the key thing then. So you're
3850180	3856580	not telling it to learn things, you're telling it not to learn things. We're telling it to learn
3856580	3862180	one thing. We're telling it to learn figure out which views came from the same image. But then,
3862180	3868740	yeah, we have to make sure that it learns to do that with a diverse set of features instead
3868740	3874180	of just doing it in one way. Because I guess it's like a task that's actually pretty easy to do if
3874180	3881060	you don't have this kind of aggressive augmentation. Yeah, I think in a way it helps the network to
3881060	3886580	also differentiate what actually what is the thing that differentiates two images. I think
3887300	3893380	it helps the network to learn, you know, pick up on that signal. To that end, I also wanted to ask
3893380	3899780	for a custom dataset, if I wanted to, you know, apply a sim clear, what pointers should I take
3899780	3904820	into consideration while designing my augmentation policy? I'm sure you have been asked about this
3904820	3909700	question quite a few times. But yeah, I think it's a good question. Like I think like we actually
3909700	3914740	still don't really know how generalizable these contrastive learning techniques are beyond image
3914740	3920900	net. Like we know they work super well on image net. But like image net is like a boring dataset to
3920900	3924980	apply contrastive learning to because we actually already have all the labels and we could just be
3924980	3932660	doing supervised learning. But I think starting with the crop and color augmentation is definitely a
3932660	3937780	good idea, at least like for datasets that have color, I guess if you don't have color, then maybe
3937780	3945780	think about distorting intensities instead of colors. But beyond that, I think it depends on the
3945860	3951460	specific task and what you really want the neural network to pick up out of the dataset.
3952020	3956820	I feel like there are probably some sorts of data where I wouldn't really expect
3957620	3964340	contrastive learning to work well. So for example, like if you try to do contrastive learning on a
3964340	3971620	dataset of medical images where you've just got healthy patients, and then you want to translate
3971620	3977620	that to like some sort of dataset of people with some kind of pathology, you might never pick up the
3977620	3982900	features that are important for detecting the pathology. But yeah, I think this question of how
3982900	3987860	do you design the augmentation, what augmentation works well for datasets that maybe aren't natural
3987860	3994020	images like these kinds of medical images or maybe like satellite images. That's an important
3994020	3999300	question that we haven't addressed yet. There seems to be this fascinating universality of
3999300	4003860	representations, especially in vision. This is exactly the kind of thing you can test with your
4003860	4009700	wonderful similarity matrix idea. I'm not trying to be flippant when I say this because
4009700	4015300	practitioners have used ImageNet on a variety of downstream tasks. For example, they might use it for
4015300	4020020	classifying circuit boards or something. And the miraculous thing is it just seems to work quite
4020020	4025780	well. So do you think in your opinion that there is some kind of universality? I'm very skeptical
4025780	4032980	about like universality of ImageNet for different tasks. Like in the past, we did some work where
4032980	4039380	we looked at how well ImageNet networks transfer to other tasks. And it seems like
4040180	4047060	there are actually some tasks which are just like datasets of natural images where pre-training an
4047060	4053380	ImageNet doesn't really help at all. And those datasets just seem to be too different from ImageNet.
4053380	4060100	They're things like this Stanford cars dataset where you have to like classify different cars
4060100	4065140	according to their make, model, and year. It turns out even though there are lots of cars in ImageNet,
4065140	4070420	if you pre-train on ImageNet and you fine-tune on that dataset, you will learn to classify it
4070420	4076100	faster in fewer steps than if you had trained from scratch on the Stanford cars dataset.
4076820	4081700	But you won't actually perform any better at the end. And that's true even though the
4081780	4088660	Stanford cars dataset has 10,000 images. So it's tiny compared to ImageNet. So I think like actually
4088660	4096020	representations of images are not that universal. And at least what works for natural images for
4096020	4102820	images like those in ImageNet may not work on other datasets. I think there's also like limited
4102820	4109220	evidence for transfer from ImageNet to medical datasets. It seems if you don't like work really
4109220	4115380	hard at tuning hyperparameters or if you don't train for long enough, you will get better accuracy
4115380	4120180	by starting with an ImageNet pre-trained network. But if you do very thorough experiments and you
4120180	4124820	train for long enough, you try different learning rates and weight decay parameters, like actually
4124820	4130500	it seems like training from scratch on most medical datasets will give you the same accuracy as if
4130500	4136260	you started from a network that's pre-trained on some other giant dataset. Maybe this makes sense
4136260	4142660	because if you think about radiologists, like it's not like a radiologist can just like at the
4142660	4148100	beginning of their education, they can't just look at an MRI or an X-ray image and say this is
4148100	4153860	where the tumor is. It's something that takes them years of training to learn how to do. And so maybe
4153860	4159540	it also makes sense that like our neural networks can't just immediately easily without lots of
4159540	4167220	training pick up on very different image distributions. It does seem to make sense,
4167220	4174340	going a bit from the universality of representations to the universality of
4174340	4181780	augmentation, since this is such a crucial part. Do you think that there is a systematic way how
4181780	4187300	we can discover augmentations? Because it seems right now, it seems to be kind of a whack-a-mole,
4187300	4192180	right? It's okay, we just feed images and it's no, that's too easy. We crop them. Oh no, it's the
4192180	4197620	color histogram. So we like whack on the color and then it works better. But maybe someone finds out
4197620	4202900	oh, there is still this easy feature that the network every now and then pays attention to. So
4202900	4210020	we design a method to whack on that a bit. Do you think there is a systematic way or will this
4210020	4216340	kind of philosophically always rely on us humans having a higher level inside of what we want to
4216340	4224100	do with the dataset? Yeah, so I think actually I'm hopeful that at least for natural images,
4224100	4229780	just like crops and color distortions are enough, because I guess like what we found is
4229780	4235540	you combine those two augmentations and if you do that, like that gets you most of the way to
4235540	4241140	supervised accuracy. So maybe we shouldn't expect huge gains from adding additional augmentations on
4241140	4246260	top of that, even though there are like in the Sinclair paper, we add like Gaussian blur, which
4246260	4252100	gives slight gains on top of that. I guess in the BYL paper, they add even more augmentations on top
4252100	4258180	of that. So you can get small gains, but it seems like the gains are much smaller once you've got
4258180	4263860	the crops and the color distortions there. In terms of systematic ways of discovering
4264420	4269620	what set of augmentations we should be using, I guess there's a paper that I saw where
4270420	4276420	they basically use linear evaluation on the rotation prediction task to see whether the
4276420	4281860	augmentations are good and they claim that actually works for evaluating the augmentations.
4281860	4288340	So maybe that's one way, I don't know. There are all sorts of ways of designing augmentations
4288340	4295540	for supervised learning that could conceivably be applied to the self-supervised learning setting,
4295620	4301060	to the contrastive setting. There are these meta-learning based approaches for learning
4301060	4307700	data augmentation. I'm not sure, those techniques tend to be pretty complicated. I'm not sure whether
4307700	4313140	it's actually easier to deal with those techniques than just like trying a bunch of things, but I
4313140	4319460	think that maybe it doesn't matter that much. Maybe like just, at least if you're dealing with natural
4319460	4327300	images, maybe crop and color distortion is enough. I guess if you think about other images,
4327300	4331220	I don't really have any idea. I guess it depends on what the images look like.
4331940	4337300	There are lots of things that you could be expressing as images like a spectrogram or
4337300	4342420	like some kind of chart or whatever, where you could be applying a neural network to it,
4342420	4347380	but the further you get from natural images, the less clear it is what kind of augmentations
4347460	4352740	you should be working with. It is a fascinating thought though, this universality of augmentations.
4352740	4359780	When you said cropping and color, that made me think it seems to be related to the inductive
4359780	4365940	priors in the CNN architecture that we use and also to things like its regularly sampled, gridded,
4366500	4370580	discrete image data, because we're speaking with Max Welling the other week and as he's
4370580	4376100	created lots of other interesting inductive priors for computer vision models. It does set
4376180	4380100	my mind racing a little bit because presumably there's a continuum. On the one hand, we don't do
4380100	4384740	any augmentation and we just learn from examples. In the middle, we do the augmentation and then
4384740	4388580	maybe in the future, because some people have said that computer vision systems don't have seen
4388580	4392900	understanding. They don't understand physics. The ball might be on the table, but we don't know that
4392900	4397780	it's not falling and so on. There's a lot of missing information. Would the next step be some
4397780	4402420	simulation? Do you know what I mean? Where we impute physics and we impute some world knowledge
4402420	4404980	and then I don't know whether we train a machine learning model from that?
4405620	4411380	Yeah, I think there are definitely shortcomings in our current machine learning models,
4411380	4416660	understandings of the world. There are probably things that we can't just solve by throwing more
4416660	4423380	static images at them. I think maybe the next step, rather than trying to immediately situate
4423380	4429780	the machine learning model in a simulated world, we could just think about video. I think there's
4429780	4435380	already a lot of additional information in video that a neural network could use to learn
4435380	4442100	interesting representations. It seems like if you just see static images, it's hard to learn
4442100	4448500	how to segment objects. It's hard to learn where the object's boundaries are, but once you have
4448500	4453300	video, it's like the stuff that's moving together is an object and you can tell that because it's
4453300	4458980	moving together. I think there's a lot of potential for learning better visual representations
4460740	4466020	and maybe eventually from these kinds of interactions in simulated environments. I
4466020	4472020	think ultimately it becomes a computational headache. Even video is a computational headache
4472020	4476820	because suddenly you've got all of these frames that you have to deal with. You probably want to be
4478260	4485460	thinking about how representations change over time and video data is just huge. It's especially
4485460	4493620	huge if you have to process many frames at once on your accelerators. I think that's why this hasn't
4493620	4500100	taken off yet, but I think probably representation learning from video is going to be a big thing
4500100	4507780	next year or the year after or sometime in the near future. We would love to talk about your big
4507780	4512180	self-supervised models, our strong semi-supervised learners. This is super interesting because
4512180	4516020	you're combining the unsupervised stuff that we've been talking about in Simclear, but now
4516020	4521540	we're in the semi-supervised domain where the label efficiency becomes super important. What's
4521540	4528580	the deal there? Yeah. I guess in Simclear we focus on this question of linear evaluation
4528580	4534100	accuracy. We're just learning a representation without any labels and then training a linear
4534100	4539700	classifier on top of that representation on the same data, but now with all the labels. It turns
4539780	4546260	out that's not really a very practical problem if you have all the labels. There's not necessarily any
4546260	4552580	reason in practice that you would want to first learn this representation and then train the
4552580	4558660	classifier versus just doing standard supervised end-to-end training. What is a practical problem
4558660	4565460	is the situation where you have a lot of unlabeled data and then a very small amount of labeled
4565460	4574260	data. That's the situation that we look at in Simclear v2 in that paper. What we find there is
4574260	4582180	that you can train this network fully unsupervised without using the labels on all the data and then
4582180	4587780	you can fine-tune it on just the subset where you've got the labels. If you do that, it's possible to
4587780	4595140	get very high accuracy, especially if the network is very big. Basically, we find if you have a
4595140	4600100	really big ResNet, if you have ResNet 152 and then you make the layers three times wider,
4600660	4607940	when you do that, you can get accuracy when you fine-tune on 10% of the labels that's substantially
4607940	4614020	better than if you trained ResNet 50 from scratch with all the labels. Once you have that really big
4614020	4618180	network, it turns out you don't have to put the really big network into production. You can take
4618180	4624900	the really big network and you can then distill it back into a standard ResNet 50 and you can retain
4624900	4629540	almost all of the accuracy when you do that. I guess what's important about this distillation
4629540	4634980	process is we're not just going to distill on the labeled dataset. We're going to also use
4635860	4642740	the labels that this giant network, which we fine-tuned on a small subset of the data,
4642740	4648180	we're going to use the labels that it gives on all of our unlabeled data. We're going to use it to
4648180	4653460	generate labels and then we're just going to use those labels to train a much smaller network. If
4653460	4659700	we do that, we get accuracy that's similar to or maybe even slightly better than standard supervised
4659700	4666980	training from scratch. This becomes a highly practically relatable approach toward doing
4667540	4674420	computer vision related things. We see all the times that folks have a huge corpus of unlabeled
4674420	4682020	images, but they only have maybe 5% or 10% labeled images. This immediately becomes a practically
4682100	4687460	applicable recipe for them. I definitely am looking forward to seeing this thing
4687460	4693300	implemented at scale at different companies. That's there. If I understood it correctly,
4693300	4700100	just for the viewers, you folks used a variant of distillation here, which is more popularly
4700100	4704340	referred to as self-training. I don't think there's really a difference between
4705140	4711380	what we call distillation and what other people call self-training. I guess the idea is basically
4711460	4717540	we will pass information into the network. We get its output probabilities and then we train
4717540	4721380	another neural network with those output probabilities as the targets.
4722180	4728660	What I find fascinating is how many ideas come together in this paper. There's first,
4729380	4736100	there's this, let's do representation learning and then we have these just small labels. We find
4736100	4742580	tune and then there's big networks, small networks. Then you label, but you also apply
4742580	4749060	some noise, if I understand correctly, in the process of transferring. Maybe I'm misremembering
4749060	4755060	that, but there's a lot of ideas that come together. Yannick, you probably confused it with
4755060	4760260	noisy student training, where they impose noise during the student training.
4760820	4767060	Sorry, maybe not, but there is a lot of ideas that come together. Something tells me that
4767060	4775300	there was a process behind going, you probably didn't sit down after SimClear1 and be like,
4775300	4780100	all right, what do we do for SimClear2? Okay, let's do this. It tells me there was this process.
4780740	4788020	If you can maybe elaborate a bit on how did you going to build up the system towards the final
4788020	4794420	output? Was there dead ends or was it like, let's build up until we can no longer make it better?
4794420	4803460	How was that? Yeah, I guess there was a bit of a process. After the original SimClear paper,
4803460	4808340	I guess it's clear from the SimClear paper that when we have this bigger network, we get much
4808340	4816020	higher linear evaluation accuracy than we do if we just train SimClear with a ResNet50. Then the
4816020	4822740	question was, is there some way that we can somehow eliminate this dependence on this giant
4822740	4827140	network? Because the giant network is annoying to work with, it's computationally expensive,
4827140	4835220	it's big. We first tried what happens if you distill the unsupervised network. We basically
4835220	4842020	have this task that is set up as a form of cross entropy loss when we're doing the contrast of
4842020	4850420	learning. You can also think about distilling on that task where you have a probability distribution
4850420	4855060	that corresponds to the similarity between an image and all the other images. You could use
4855060	4860900	those kinds of targets to distill. We tried that and that kind of worked. Then we also tried the
4860900	4866180	approach of first fine-tuning the big network and then distilling it. It turned out that worked
4866260	4874580	a lot better. I guess we jumped straight to distillation because we knew that we could get
4874580	4881620	much better results by using a giant network with SimClear. Then once you realize that distillation
4881620	4886740	is going to be important, the only thing you've got to figure out is what kind of distillation
4886740	4891940	should you be doing. What we found was this approach of pre-training, then fine-tuning,
4891940	4897060	then distilling works a lot better than pre-training, then distilling, then fine-tuning.
4897620	4905860	How far down do you think one can go with this final distillation step? Is this something that is
4905860	4911860	conceivably going to be available on, let's say, edge devices at some point, like that
4912980	4919140	our glasses or something run with similar accuracies to these giant networks? Or
4920020	4923700	is it more a factor of four, a factor of 10 kind of stuff?
4924660	4930980	I think there are clearly some limits to distillation. I guess we probably shouldn't
4930980	4936980	expect distillation of the kind that we do in SimClear v2 to work substantially better than
4937620	4943780	supervised distillation, which has been around for quite a while now. I think what's impressive is
4943860	4950260	that in the self-supervised case, in the contrastive case, distillation basically allows you to
4950260	4955220	recover the same accuracy that you would get from training supervised from scratch, whereas without
4955220	4962340	it, the accuracy is a lot worse. It seems like it maybe matters more in this contrastive case,
4962980	4968820	but I think generally when you do distillation in the supervised case, you can get maybe a
4968820	4974980	percentage point gain, maybe a couple of percentage points. I think that's probably about the limit
4974980	4980900	in terms of the improvement that you could get from any kind of distillation-based approach
4980900	4986900	over supervised training from scratch. Fascinating. I don't know if you know that we've been playing
4986900	4991860	with GPT-3 and you said something quite interesting just now. You said that they're deterministic,
4991860	4996020	but in GPT-3, that's not really the case. If you sample from it deterministically,
4996020	5003220	it gets stuck in cycles. You have to do some kind of trickery, some kind of random sampling from
5003220	5008740	this distribution. It might be the case in future computer vision models as well, that we have to
5008740	5012740	randomly sample from them in some way, because otherwise it would get into some pathological
5012740	5017780	behavior. Maybe to do that, we need to have some kind of controller on the top. I suppose my question
5017780	5022260	is in the future, maybe we will be in the stochastic regime. What do you think about that?
5023140	5029620	With GPT-3, you're trying to generate data. When you generate data, there has to be some
5029620	5034500	kind of noise that's coming from somewhere. The process of generating data is like turning
5034500	5040980	noise into data. For image classification, we have the data and we just want to turn it into
5040980	5047380	a label. Maybe there's this implicit notion of stochasticity in that the network gives some
5047860	5053540	output distribution. I think we still want everything to be deterministic if it can be
5053540	5059620	deterministic. We basically want the network to say, this is a dog with probability x.
5060340	5065860	If there's some way to improve it with stochasticity, I don't know. I guess dropout used to be very
5065860	5070980	popular, but it seems like it's not so popular anymore and it doesn't really help us very much
5070980	5077860	with vision models. Also, even dropout is generally only used when we train the neural
5077860	5086260	networks. On the other hand, the brain is very stochastic. The brain has lots of noise. I guess
5086260	5090900	that suggests that maybe there's some way to leverage noise to learn better representations
5090900	5094340	in neural networks as well and we just don't quite know the right way yet.
5094900	5099140	That's right. Max Welling said to us that he thinks that the future of AI will have a
5099140	5104660	generative component. He thinks that we have the matrix in our minds. We have these simulations
5104660	5111220	going on all the time and we're generating new scenarios. It's related to the data
5111220	5116340	augmentation thing as well. Some people have said to me in the past that using a GAN might be a way
5116340	5121940	of doing data augmentation. Presumably, that would require some kind of stochastic sampling as well.
5121940	5125380	I suppose it's just quite interesting to see where these two things might meet in the middle at
5125380	5130900	some point. Max Yeah. I don't know. I guess like with a GAN, using a GAN to do data augmentation,
5130900	5135940	you have this problem that you still don't actually have more data. You have a GAN that's
5135940	5142340	trained on the same data. It might help you because your way of encoding inductive bias into the GAN
5142340	5148100	is different from your way of encoding inductive bias into the neural network. Maybe by having
5148100	5154500	more inductive bias, you can learn a better function. You still don't have more data at it.
5155300	5161300	Without having more data, there's no reason to expect a priority that you will be able to learn
5161300	5166340	a better function. Paul I'm so glad you said that. It was always my intuition. The amount of people
5166340	5169700	that have said to me that you should use a GAN for data augmentation. Anyway.
5169700	5174660	Max What's the first thing you think about if you're like, oh, what could I use a GAN for?
5174660	5180020	And then you learn about data, and they're like, wait, this is so much more data. Yeah,
5180020	5186020	but conceptually, yes, you don't have more data. And ironically, when you do the simple
5186020	5191780	data augmentation, you do have more data because you put all the knowledge in there
5191780	5199060	as a human of what makes two images dissimilar visually, but still equivalent semantically,
5199060	5205140	which again, is exactly the opposite. It gives you images that are visually similar,
5205140	5211700	but it has no intuition of what the semantic similarity is. For my last question, I actually
5211700	5217460	want to switch topics just a little bit to what Tim said at the beginning, namely your love of Julia.
5218260	5225540	So I have seen a number of especially data scientists be strong advocates for Julia as a
5225540	5232180	language and so on. Do you want to give anyone sort of the pitch? Why should we even consider this?
5233060	5239300	Yeah, so I think Julia is a much better programming language than Python in many ways.
5239940	5244980	I guess one thing and I guess the thing that first attracted me to Julia is that it's really
5244980	5250820	fast, like you can write Julia code and with very little work, you will end up running as
5250820	5256820	fast as equivalent C code. So that's something that you can't get out of standard Python code.
5256820	5261300	If you're just writing a for loop in Python, it's going to be super slow. But in Julia,
5261300	5264900	you don't have to worry about all of that. And you don't have to worry about like Cython or
5264900	5269860	number all of this other stuff that people have hacked on top of Python. Julia is just
5269860	5277620	designed to be fast and it works. I think there are other advantages to Julia as a language
5277620	5284820	beyond that. I guess it's built on this idea of generic functions where you have a function that
5285860	5290100	can take multiple types and you can define the function differently for the different types.
5290660	5295220	And this is something that we do all the time when we're doing like machine learning, like we
5295220	5300500	have matrix multiplication, which is a different form of multiplication that takes matrices and
5300500	5308020	produces something. And I guess like in Python, it's so it used to be that you had to type dot dot
5309300	5314980	to multiply things, but now it's like they have this add symbol that does matrix multiplication.
5314980	5319700	But Julia is designed for these situations where maybe beyond just matrices,
5319700	5324180	you have these funny types of structured matrices, you have sparse matrices, and you can define
5324180	5329860	special methods for the product of a sparse matrix and a vector or all sorts of things where you
5329860	5335940	might want different methods depending on the types. And even though it seems like this is
5335940	5340900	complicated and you might have some trouble picking which version of the function is going to be called
5340900	5348980	at runtime, because Julia is ultimately compiling everything when you call it. And because it has
5348980	5356580	this kind of strong type system, it can ultimately pick which method is going to be used and compile
5356580	5361620	that method call in and you don't have to worry about picking which one. And so it still ends up
5361620	5369620	being fast. I also think like there's this question of whether like the object oriented Python or
5369620	5376900	the object oriented paradigm and Python is really like the best paradigm for machine learning.
5376900	5382980	Because I guess like it's like we have data and then we have functions that operate on the data.
5382980	5386740	But in the object oriented paradigm, you want the functions that operate on the data to be
5386740	5392820	attached to the data, which is like a weird way of setting things up. And that Julia is not set up
5392820	5398820	that way. You have these data structures. And then because you are able to create functions that
5398820	5404260	specialize on the data structures, you don't have to worry about attaching those functions to the
5404340	5410660	data structures themselves. Amazing. Dr. Simon Cornblith. Thank you very much for joining us
5410660	5415780	this evening. It's been an absolute pleasure. Thanks for having me. Thank you. I really hope
5415780	5421140	you've enjoyed the show today. We've had so much fun making it. Remember to like, comment,
5421140	5426180	and subscribe. We love reading your comments, every single one of them. And we'll see you back
5426180	5429060	next week.
