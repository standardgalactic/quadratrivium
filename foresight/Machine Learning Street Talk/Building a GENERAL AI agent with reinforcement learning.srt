1
00:00:00,000 --> 00:00:05,880
Open-endedness is essentially, you know, we're studying systems that can generate their own data in an infinite capacity

2
00:00:05,880 --> 00:00:10,520
And so it's systems that essentially if you run it for longer and longer they get more and more complex

3
00:00:10,520 --> 00:00:13,760
They generate more and more quote-unquote interestingness or interesting data

4
00:00:14,240 --> 00:00:19,040
And so if we can actually, you know, crack this nut of how do we actually come up with a

5
00:00:19,560 --> 00:00:22,520
Self-improving system in the sense that it keeps generating interesting data

6
00:00:23,040 --> 00:00:28,100
We can then use that data to train further train our models

7
00:00:28,140 --> 00:00:34,220
But of course you get into this perpetual data machine type of idea where obviously, you know

8
00:00:34,220 --> 00:00:36,220
There's how do you generate more data?

9
00:00:36,380 --> 00:00:40,540
If you know the data is ultimately coming from a model that you probably trained on previous data

10
00:00:40,540 --> 00:00:42,180
How do you get net new information from that?

11
00:00:42,180 --> 00:00:47,420
Well, I think a lot of this is actually just resolved purely again going back to this idea of the reward function

12
00:00:47,420 --> 00:00:53,580
Right or a preference function where there is outside information coming in through some sort of filtering criteria

13
00:00:53,580 --> 00:01:01,820
For example human designers in the loop or designers designing some sort of preference model that could essentially automatically rate the kinds of automatic

14
00:01:02,460 --> 00:01:06,980
Data that's being generated by these open-ended systems. What does waker stand for?

15
00:01:07,060 --> 00:01:12,460
Right, so waker stands for a weighted acquisition of knowledge across environments for robustness

16
00:01:13,420 --> 00:01:15,420
Fantastic, and what was the title of the paper?

17
00:01:16,420 --> 00:01:19,420
Oh, right. Yeah reward free curricula. Oh girl. What was the title?

18
00:01:20,420 --> 00:01:25,140
reward free curricula for training robust world models, that was it. Okay, so

19
00:01:26,860 --> 00:01:31,460
Give us the elevator pitch. Yeah, totally. So basically like the overarching

20
00:01:32,620 --> 00:01:37,620
Question that we're trying to answer with this paper is like how should we go about training like very general agents?

21
00:01:37,820 --> 00:01:43,300
So in the context of the paper, we think of a general agent as being one that's able to perform a lot of different tasks

22
00:01:43,380 --> 00:01:47,860
So we might think of these as different reward functions or for thinking of it from a reinforcement learning perspective

23
00:01:48,100 --> 00:01:50,940
But also be able to perform those tasks in lots of different environments

24
00:01:50,940 --> 00:01:56,860
So, you know, we don't want a robot to just be able to do you know pick up tasks do tasks in my like my kitchen

25
00:01:56,860 --> 00:01:58,860
specifically we want the robot to be able to go into like arbitrary

26
00:01:59,420 --> 00:02:04,320
Apartments and also be able to do those tasks in like arbitrary environments. And so we kind of thought about like, yeah

27
00:02:04,340 --> 00:02:07,100
How do we want to create an agent that can do such a thing?

28
00:02:07,100 --> 00:02:12,540
And we argue in the paper that a good way of doing it would be to have an agent that has a very general world model

29
00:02:12,900 --> 00:02:18,740
So a world model meaning that it can predict the outcome of sequences of actions and predict what will happen if it does certain actions

30
00:02:18,740 --> 00:02:22,740
And so we argue if we have a very general world model that can lead to a very general agent

31
00:02:22,740 --> 00:02:26,460
That's able to perform, you know, a variety of tasks in different environments

32
00:02:26,460 --> 00:02:31,620
And so then, you know, once we've established that we kind of ask the question of how do we get a very general world model?

33
00:02:31,620 --> 00:02:33,860
And what does it mean to have a good world model that works?

34
00:02:34,260 --> 00:02:40,140
While in a very general setting across different environments and different tasks like how do we define that and how should we gather data to do that?

35
00:02:41,100 --> 00:02:46,700
Beautiful. So I really enjoyed reading the paper and it reminded me a lot of Kenneth Stanley's poet paper

36
00:02:46,860 --> 00:02:53,220
So he was doing this thing called curriculum learning and it's really related to machine teaching as well

37
00:02:53,220 --> 00:02:58,300
There's quite a few things in machine learning where you say well if we had a really principled way of

38
00:02:58,700 --> 00:03:03,300
Selecting the best training data and presenting it to the learner in the best possible order

39
00:03:03,340 --> 00:03:10,060
Could the learner be better and in that poet paper Stanley was kind of generating a diverse set of

40
00:03:10,300 --> 00:03:17,420
Environments and like training a learner on those things and you're doing something very similar and you're using this mini max regret

41
00:03:17,420 --> 00:03:21,620
Which is a concept from decision theory. Can you bring that in? Yeah, absolutely. So

42
00:03:22,620 --> 00:03:25,260
So I guess we have this notion of like wanting to be

43
00:03:25,740 --> 00:03:28,460
To perform well across a wide range of scenarios, right?

44
00:03:28,500 --> 00:03:33,180
So scenarios in our context mean like different environments and different tasks and kind of like the most

45
00:03:33,780 --> 00:03:38,820
Standard way of thinking about that, especially in reinforcement learning or a machine learning in general as you think about like the average performance

46
00:03:38,820 --> 00:03:43,220
So so how do I optimize like the expected reward across all of these different scenarios?

47
00:03:43,780 --> 00:03:47,100
and a lot of the work that that munchies done as well kind of argues that

48
00:03:47,660 --> 00:03:50,900
Just just optimizing for expectation isn't necessarily the best

49
00:03:51,700 --> 00:03:55,180
The best objective so, you know, we can imagine in the real world

50
00:03:55,220 --> 00:03:58,620
We don't really know like the distribution over possible tasks or or anything

51
00:03:58,620 --> 00:04:02,860
Well, you know in most situations, we don't know things like that and so maybe a better objective is to try and be

52
00:04:03,220 --> 00:04:07,220
robust instead and robust basically we can think of that as meaning like we should do

53
00:04:07,700 --> 00:04:12,220
Reasonably well in every situation we could be in and that that's kind of what a robust objective is

54
00:04:12,940 --> 00:04:16,660
And one of the ways that you can define a robust objective is via mini max regret

55
00:04:16,660 --> 00:04:22,300
And so regret means like suboptimality like how well that I do relative to the best I could have possibly done

56
00:04:22,340 --> 00:04:25,220
So that means basically the same thing as it does in normal English

57
00:04:25,620 --> 00:04:30,660
And so the mini max regret objective basically says across all possible situations. I want to try and do

58
00:04:31,700 --> 00:04:37,020
Minimize the regret across all possible situations minimize the maximum regret I should say so that means in all possible situations

59
00:04:37,020 --> 00:04:40,380
We should do almost as well as the best we could have possibly done

60
00:04:40,900 --> 00:04:44,300
And I guess just to contrast this against the standard objective for robustness

61
00:04:44,300 --> 00:04:51,300
So the more common objective for robustness at least traditionally is like a maximum performance. That means maximize the performance

62
00:04:51,820 --> 00:04:56,900
While the environments like minimizing and choosing the most adversarial environment or the most adversarial scenario

63
00:04:57,820 --> 00:05:02,860
But but the problem with kind of the maximum objective is that in some environments you just can't do anything

64
00:05:02,860 --> 00:05:05,060
Let's say it's like some such situations is too hard

65
00:05:05,060 --> 00:05:10,620
You're doomed and so if in some situations you're doomed and you always get like zero reward or negative infinity reward

66
00:05:10,700 --> 00:05:16,420
That means there's no incentive to try and do better in any other environment because your maximum reward is always going to be zero

67
00:05:16,420 --> 00:05:22,140
And so therefore I think like minty argues as well as Michael Dennis and a lot of these recent papers argue that mini max regret

68
00:05:22,140 --> 00:05:28,100
So minimizing the maximum self-optimality is actually like a better objective for a general agent. That's robust fascinating. So

69
00:05:28,900 --> 00:05:32,980
If I understand correctly is it a way of saying I want to have the best case

70
00:05:33,540 --> 00:05:35,260
worst

71
00:05:35,260 --> 00:05:41,140
Expected regret. Yes. So basically mini max regret is saying that if you assume that you know

72
00:05:41,140 --> 00:05:46,420
The environment is adversarial to you in some way like when you're training or at inference time when you're actually

73
00:05:46,980 --> 00:05:48,980
Testing your policy out in the real world

74
00:05:49,460 --> 00:05:56,380
Mini max regret is saying the agent should behave the model should behave in a way that minimizes its worst case possible regret

75
00:05:56,660 --> 00:06:00,700
Over all the possible conditions of the world that this adversary could choose

76
00:06:00,900 --> 00:06:04,740
What's really interesting about this paper is we are talking about the reward free

77
00:06:05,300 --> 00:06:11,420
Exploration phase and we're also talking about the domain of model based reinforcement learning as opposed to

78
00:06:11,980 --> 00:06:14,660
You know, let's say value based reinforcement learning where

79
00:06:16,220 --> 00:06:19,860
You get this entanglement, right? So the dynamics the model of the world

80
00:06:19,860 --> 00:06:23,380
It's still in there, but it's kind of in meshed with this with this value model

81
00:06:23,420 --> 00:06:25,900
Whereas in model based reinforcement learning in a principal way

82
00:06:25,900 --> 00:06:31,220
We kind of separate out the parts so that we can do explicit planning and imagination and simulations and stuff like that

83
00:06:31,300 --> 00:06:34,340
So we're very much in this model based domain, right? Yeah, absolutely

84
00:06:34,340 --> 00:06:40,100
Yes, we focused on yeah model based reinforcement learning or some people like to call this like the world model setting more recently

85
00:06:40,340 --> 00:06:43,700
But yeah, like you said we you know in typical like model free reinforcement learning

86
00:06:43,780 --> 00:06:46,740
We we typically aim to learn a policy and a value function and yeah

87
00:06:46,740 --> 00:06:52,900
As you said like that value function is kind of implicitly encoding the dynamics through the fact that we learn the value function using the bellman equation

88
00:06:52,980 --> 00:06:58,900
So so the bellman equation kind of propagates the information between like transition and the environments through the value function

89
00:06:58,900 --> 00:07:01,780
So so the value function will like implicitly have the dynamics in it

90
00:07:02,580 --> 00:07:06,420
But in model based reinforcement learning we want to very explicitly model the dynamics of the environment

91
00:07:06,820 --> 00:07:10,580
And so what I mean by that is we want to be able to take some previous sequence of observations

92
00:07:10,820 --> 00:07:13,940
Perhaps those are images and then also condition on the next action

93
00:07:13,940 --> 00:07:18,420
We want to take in the environment and then be able to predict the distribution over the next observation or states

94
00:07:18,420 --> 00:07:20,740
We're very explicitly modeling the dynamics of the environment

95
00:07:21,060 --> 00:07:25,380
Okay, now this is really interesting because you know people think about reinforcement learning and in reinforcement learning

96
00:07:25,380 --> 00:07:28,740
You don't so much care about having a model of the world

97
00:07:29,060 --> 00:07:35,220
You care about building trajectories that lead to some you know task or goal or whatever that you're interested in so like

98
00:07:35,940 --> 00:07:39,860
I mean just just in broader terms. What what what do we get from explicitly modeling the world?

99
00:07:40,260 --> 00:07:43,380
So there are there are a few arguments for why we would want to explicitly model the environment

100
00:07:43,380 --> 00:07:48,740
So so one of which is um a lot of people would argue that you get better sample efficiency by modeling the environment

101
00:07:48,980 --> 00:07:52,180
And the argument for this is you know the reward function might be quite sparse

102
00:07:52,500 --> 00:07:56,900
And so if you're just relying on like the propagation of rewards backwards to try and learn the optimal behavior

103
00:07:57,140 --> 00:08:02,820
That might not be as efficient as actually learning the dynamics because the dynamics can be learned from every single transition that you have

104
00:08:02,900 --> 00:08:05,460
It's kind of like a standard supervised or unsupervised learning problem

105
00:08:05,460 --> 00:08:10,900
So so you kind of have like a richer signal to learn from which might arguably lead to better sample efficiency

106
00:08:11,460 --> 00:08:12,900
um, but I think like

107
00:08:12,900 --> 00:08:17,060
More concrete arguments that I would argue for or that if you have a model of the environment

108
00:08:17,140 --> 00:08:22,260
It's it's some kind of more general thing that you can then use to develop better decision making later on

109
00:08:22,260 --> 00:08:24,260
So so if you just learn a value function

110
00:08:24,900 --> 00:08:30,180
You're kind of only learning how to optimally do that specific reward function or optimize that specific reward function

111
00:08:30,660 --> 00:08:34,820
Um, but if we have a model of the environment, we can kind of arbitrarily be given some task later down

112
00:08:34,820 --> 00:08:40,340
Whether it be a reward function or a goal state or something like that and we can then plan to optimize that task later down the road

113
00:08:40,660 --> 00:08:42,500
so I would think that um

114
00:08:42,500 --> 00:08:45,940
You know, it's kind of a much more general way of having a powerful decision making agent

115
00:08:46,260 --> 00:08:50,340
Rather than just specifying like one task and learning the optimal kind of policy for one task

116
00:08:50,660 --> 00:08:52,660
and I guess another thing that I'll add to that is um

117
00:08:53,460 --> 00:08:57,460
Rather than only learning like a feedforward policy like you wouldn't reinforcement learning

118
00:08:57,540 --> 00:08:59,540
So something that maps directly to actions

119
00:08:59,620 --> 00:09:02,580
The other thing that a world model allows you to do is also to do online planning

120
00:09:02,660 --> 00:09:06,100
So you can imagine at test time we're trying to deploy it in the environment

121
00:09:06,260 --> 00:09:10,340
But we can actually do a bit more further planning through the world model to then work out what the best action is

122
00:09:10,740 --> 00:09:13,780
Rather than relying on just a neural network to immediately output an action

123
00:09:14,180 --> 00:09:17,380
And there's kind of a lot of work showing that if you can do this like planning at test time

124
00:09:17,700 --> 00:09:21,300
You can kind of get a lot of a better performance on a lot of environments, especially things that

125
00:09:21,700 --> 00:09:25,460
That really rely on um search to do well things like go and like these kind of games

126
00:09:25,460 --> 00:09:28,100
We do have to think explicitly ahead in the environment

127
00:09:28,500 --> 00:09:32,580
And so I would think those are the main reasons you would want to consider um a learning a world model

128
00:09:32,820 --> 00:09:35,700
And maybe a last point I'll just add is that I think this is kind of a um

129
00:09:36,340 --> 00:09:38,420
Again, like unclear whether this is true necessarily

130
00:09:38,420 --> 00:09:43,220
But but I think some people would argue that a world model will generalize better than learning a value function

131
00:09:43,540 --> 00:09:47,380
So you can imagine like a world model is learning things like you know state transitions

132
00:09:47,540 --> 00:09:49,940
So you can imagine if you if you're training on straight transitions

133
00:09:50,020 --> 00:09:53,380
The model is kind of implicitly being forced to learn something like physics or something like that

134
00:09:53,620 --> 00:09:56,500
And so if you're like very explicitly forcing the model to learn something like physics

135
00:09:56,820 --> 00:09:57,620
You could argue, you know

136
00:09:57,620 --> 00:10:01,300
We'll go to some new state and the rules of physics will still hold and therefore the world model will still be

137
00:10:01,620 --> 00:10:04,820
Quite good at the new state potentially whereas if you learn a value function

138
00:10:05,220 --> 00:10:08,580
I guess it's a little bit less clear as to whether you're putting a new situation

139
00:10:08,740 --> 00:10:12,100
Will the same kind of structure of that value hold as it would a model

140
00:10:12,340 --> 00:10:15,060
Anyway, so that was a bit of a long answer, but no, no, it's fascinating

141
00:10:15,060 --> 00:10:20,020
I mean when I was reading the paper that one of the reads I got is um in machine learning

142
00:10:20,020 --> 00:10:25,460
We are often overcoming the curse of sparsity. So of course like in trajectories and reinforcement learning that that's quite intuitive

143
00:10:25,700 --> 00:10:28,740
But even in learning the world model itself the model

144
00:10:29,300 --> 00:10:34,260
Just because of the way they're trained it tends to compress the world into small little motifs and

145
00:10:34,900 --> 00:10:40,100
Actually, the world is quite complicated and we need to combine the motifs together in lots of interesting and rich ways

146
00:10:40,500 --> 00:10:45,860
And by exploring through the world model, we're almost kind of like make it we're forcing it to make those connections

147
00:10:46,020 --> 00:10:48,740
Yeah, and I think um, you know to follow up on mark's um

148
00:10:49,540 --> 00:10:50,260
Mark's point

149
00:10:50,260 --> 00:10:54,580
I think it's also interesting because especially in the waker paper, uh, the world model setting

150
00:10:54,580 --> 00:10:57,060
We're looking at specifically reward free world models

151
00:10:57,300 --> 00:11:02,660
And so essentially there's this explicit decision to separate separate out the two components of world model

152
00:11:02,900 --> 00:11:07,380
Which is essentially the dynamics function, which tells you how things transition from state to state

153
00:11:07,380 --> 00:11:11,460
How does a state transition state of the world transition to the next state of the world?

154
00:11:11,700 --> 00:11:16,500
Given an action that the model or the agent is taking in that world and the reward that it receives

155
00:11:16,580 --> 00:11:19,460
So the slider part the reward is defined by the reward function

156
00:11:19,780 --> 00:11:25,940
And so, uh, you know, I think mark was uh to follow up on his point a lot of the benefits of the world model is

157
00:11:26,260 --> 00:11:28,660
In this design arrangement is that you can

158
00:11:29,300 --> 00:11:33,220
compositionally separate out this dynamics aspect from the reward aspect

159
00:11:33,460 --> 00:11:38,900
So the general idea would be why shouldn't agent train in such a world model be able to generalize to a new setting?

160
00:11:39,060 --> 00:11:43,860
Well, maybe if that setting shares a lot of the underlying dynamics in that version of the world

161
00:11:43,940 --> 00:11:48,820
for example rules of physics and the agent has learned how to exploit those to accomplish, um

162
00:11:49,460 --> 00:11:52,500
Navigation around that environment or reach different types of tasks

163
00:11:52,900 --> 00:11:54,900
Achieve different kinds of tasks in that environment

164
00:11:55,060 --> 00:11:58,260
Then you can um sort of superimpose a different reward function

165
00:11:58,580 --> 00:12:03,700
That essentially defines a different task because the reward function defines what task success is

166
00:12:03,940 --> 00:12:09,220
so you can essentially superimpose different tasks on top of that dynamics model and you would

167
00:12:09,540 --> 00:12:14,980
You know, you could expect that the agent could learn more quickly because it's already mastered sort of the foundational skills of

168
00:12:15,220 --> 00:12:19,460
navigating or manipulating different aspects of the dynamics of that world

169
00:12:19,780 --> 00:12:24,020
We've been on a bit of a journey here. I think over the last few years in the literature of

170
00:12:24,100 --> 00:12:25,220
um

171
00:12:25,220 --> 00:12:32,820
We we want to have robust models and we're doing that by kind of perturbing and you know making a bunch of manipulations to the environment

172
00:12:33,060 --> 00:12:37,300
And there there was this domain randomization and there's like unsupervised environment design

173
00:12:37,300 --> 00:12:40,660
And of course your your iteration now is doing this in in the domain of

174
00:12:41,380 --> 00:12:46,900
Reward-free exploration, but can you take us on on that journey sort of maybe starting with um domain randomization?

175
00:12:47,140 --> 00:12:52,340
kind of just to uh elaborate on something that mark was previously talking about which is that the typical

176
00:12:52,660 --> 00:12:55,620
you know standard setup in machine learning is to

177
00:12:56,180 --> 00:13:00,500
Uh, essentially optimize a model's performance uh over a uniform distribution

178
00:13:00,980 --> 00:13:04,340
Over the data points and so this is really just randomly sampling data points

179
00:13:04,340 --> 00:13:08,340
And we try to minimize the loss over those data points for whatever objective

180
00:13:08,420 --> 00:13:12,180
We're trying to minimize or maximize in reinforcement learning. Um

181
00:13:12,740 --> 00:13:15,620
We want to train agents that can perform well in lots of different

182
00:13:16,180 --> 00:13:18,180
versions of the environment and so

183
00:13:18,580 --> 00:13:20,580
You can think of each environment

184
00:13:20,580 --> 00:13:24,580
Almost as a bundle of data points, right? It's kind of the set of trajectories that the agent can

185
00:13:25,860 --> 00:13:31,460
Can encounter within that version of the world and we essentially in reinforcement learning we want to learn to maximize

186
00:13:32,100 --> 00:13:36,260
The reward of the agent uh in that set of trajectories

187
00:13:36,260 --> 00:13:38,260
So we want to specifically start to

188
00:13:39,060 --> 00:13:45,060
actively pursue those trajectories that give us the highest reward and we learn from the reward signal as the feedback signal for

189
00:13:45,380 --> 00:13:49,940
Figuring out, you know, which actions and therefore which trajectories will lead to maximizing that reward

190
00:13:50,340 --> 00:13:57,620
and so typically um when we operate in the multitask setting, uh, we essentially randomly sample different versions of the environment

191
00:13:57,860 --> 00:14:04,100
And essentially have the agent try to maximize its performance its reward on that random sample of environments

192
00:14:04,740 --> 00:14:05,780
uniformly

193
00:14:05,780 --> 00:14:08,180
Sampled from, you know, the set of possible environments

194
00:14:08,980 --> 00:14:10,980
And this is essentially

195
00:14:10,980 --> 00:14:18,500
Causing the agent it'll cause the agent to learn a policy that's optimal for essentially uniform distribution over those environments

196
00:14:18,900 --> 00:14:25,780
Um, but of course this is kind of a naive assumption because we essentially are assuming that every possible version of the environment is equally likely

197
00:14:26,020 --> 00:14:30,900
Which is obviously not true because some versions of the world will not be as likely as other as others

198
00:14:31,140 --> 00:14:34,740
Uh, for example, like if you walk outside the sky is usually blue and not green

199
00:14:34,980 --> 00:14:39,220
And so, you know, when the sky is orange, maybe that happens if you're in california

200
00:14:39,300 --> 00:14:41,460
There's a wildfire, but that's not usually the case

201
00:14:41,780 --> 00:14:46,580
And so instead what we can do is we can turn to decision theory and think of

202
00:14:47,220 --> 00:14:50,820
Sort of more sensible approaches to what it means to act optimally

203
00:14:51,300 --> 00:14:58,420
When you're uncertain about uh, what state of the world the world will be in and so the thing that we focus on in this paper

204
00:14:59,380 --> 00:15:03,140
Is this idea of minimax regret where it is this idea again of

205
00:15:03,780 --> 00:15:08,340
Having the agent act in a way that essentially minimizes its worst case regret

206
00:15:09,860 --> 00:15:11,860
In any possible, uh, state of the world

207
00:15:12,100 --> 00:15:18,020
So largely, you know, this is a shift from randomly sample what it means in practice is you want to shift from randomly sampling

208
00:15:18,420 --> 00:15:24,660
environments during training to essentially, uh, sampling environments that maximize the agent's regret

209
00:15:25,220 --> 00:15:31,060
And what this means is you're now actively sampling for those environment settings where the agents, um,

210
00:15:32,260 --> 00:15:38,980
Experiencing the most regret and here regret is defined just simply as what does the optimal agent do in that version of the environment?

211
00:15:39,300 --> 00:15:42,260
And what did this current agent that's learning do in that environment?

212
00:15:42,260 --> 00:15:47,380
And so there's this gap in performance and you want to actively find those environments where that gap is maximal

213
00:15:47,780 --> 00:15:53,940
And if you view this as this adversarial game now between, you know, uh, an adversary like nature

214
00:15:53,940 --> 00:16:00,980
That's choosing the environment and the agent that's learning to solve the environment. Um, you can think of the adversary as, you know, having a

215
00:16:01,780 --> 00:16:05,220
Payoff function in that game or it's rewarded for the

216
00:16:05,700 --> 00:16:10,260
Based on the regret that the agent experiences and the agent is trying to shrink that regret

217
00:16:10,260 --> 00:16:16,100
So the agent you can think of as being rewarded for, you know, um, the the negative of that reward

218
00:16:16,100 --> 00:16:19,780
So the agent's reward signal is you can think of as the negative of the regret

219
00:16:20,180 --> 00:16:24,100
And so now you have the setting where you can essentially view this training process

220
00:16:24,500 --> 00:16:29,060
this active sampling process as a two player zero sum game where the

221
00:16:29,460 --> 00:16:36,340
Adversary is, you know, rewarded for the regret of the agent in each environment it chooses and the agent is rewarded based on the

222
00:16:37,060 --> 00:16:43,380
The agent receives the negative regret as its payoff. And so, um, we know that into player zero sum games

223
00:16:43,380 --> 00:16:47,460
There's always a this there's always a solution called a Nash equilibrium

224
00:16:47,540 --> 00:16:50,740
and so this is an idea in game theory where basically this is

225
00:16:51,140 --> 00:16:57,620
um, a choice of behaviors on both parties or a choice of strategies on both parties in the game such that, um

226
00:16:58,500 --> 00:17:01,620
No player can do better unless the other player changes their strategy

227
00:17:01,780 --> 00:17:05,060
And so you can think of this as a situation where, you know, I'm not

228
00:17:05,620 --> 00:17:11,460
Neither player is incentivized to deviate from their behavior. Uh, once they reach this choice of mutual strategies

229
00:17:11,940 --> 00:17:16,980
And so we know that all two player zero sum games have a Nash equilibrium

230
00:17:17,380 --> 00:17:20,820
A set of strategies between the two players and in this case

231
00:17:21,300 --> 00:17:23,860
We know there's additional theorem called the mini max theorem

232
00:17:24,100 --> 00:17:29,620
Which says that when in a two player zero sum game specifically two players and zero sum when, um,

233
00:17:30,020 --> 00:17:35,860
You are at the Nash equilibrium setting then each player must be playing what's called the mini max

234
00:17:36,740 --> 00:17:41,060
The mini max strategy, which means that each player is minimizing the maximum

235
00:17:42,420 --> 00:17:45,540
Minimizing the maximum reward for the other player

236
00:17:45,860 --> 00:17:52,420
And so here the reward again is the regret and therefore just based on this known, you know, theorem about two player zero sum games

237
00:17:52,660 --> 00:17:57,140
We know that, um, the agent which is, you know, receiving the payoff of negative regret

238
00:17:57,220 --> 00:18:01,700
It's the min player. It must be implementing the min and max regret strategy

239
00:18:02,020 --> 00:18:11,540
And so this is how we essentially can shape the training process to essentially, um, arrive at an agent that performs mini max regret decision making

240
00:18:11,860 --> 00:18:16,500
Rather than decision making that optimizes, um, just a uniform sample of environments

241
00:18:16,820 --> 00:18:20,420
Okay, so kind of play back, um, some of those things as I understand it

242
00:18:20,740 --> 00:18:27,460
So, um, essentially we we are we're building a model which will learn to select the environments where we perform badly on

243
00:18:27,780 --> 00:18:32,980
And then we fine-tune on those environments because we're leaning into the gaps. We're saying where where do I perform badly?

244
00:18:33,140 --> 00:18:39,380
Let's fine-tune on that and then you're saying that if we continue to do this as a kind of adversarial sampling game

245
00:18:39,620 --> 00:18:43,220
That we will reach a Nash equilibrium. So it will converge in a good place

246
00:18:43,540 --> 00:18:46,020
But help me understand that why would it

247
00:18:46,740 --> 00:18:52,020
You know, it seems to me intuitively that it might be unstable or it might not quite why does it converge?

248
00:18:52,500 --> 00:18:54,500
So there's no guarantees around convergence

249
00:18:55,140 --> 00:18:58,820
And so I think this is an area where there's a lot of room for innovation

250
00:18:59,220 --> 00:19:05,060
Uh around these methods a lot of this is um, this is more I would say like theoretical motivation around why we think

251
00:19:05,460 --> 00:19:12,260
actively sampling environment settings based on, um, estimates of regret is a good idea and another point related to that

252
00:19:12,500 --> 00:19:18,740
Around sort of this gap between the theory. I I just um explained and in practice is that

253
00:19:19,460 --> 00:19:21,940
Regret itself is a pretty hard quantity to actually

254
00:19:22,500 --> 00:19:26,900
Measure in practice because you know knowing regrets defined as what's optimal performance

255
00:19:27,780 --> 00:19:29,700
minus my agents performance

256
00:19:29,700 --> 00:19:34,500
So you kind of have to know what optimal performance is and in general you don't know the optimal behavior

257
00:19:34,500 --> 00:19:39,940
Therefore you don't really know the optimal performance on any environment unless it's like a very toy setting and so

258
00:19:40,820 --> 00:19:43,300
In practice, we also use approximations for the regret

259
00:19:43,940 --> 00:19:46,900
in order to do this kind of active sampling and so

260
00:19:47,940 --> 00:19:50,340
There's a lot of deviations between theory and practice

261
00:19:51,380 --> 00:19:52,340
So

262
00:19:52,340 --> 00:19:56,100
There's no guarantees, you know that different forms of gradient based optimization

263
00:19:56,660 --> 00:20:00,180
For rl training would actually lead to converging to Nash equilibria

264
00:20:00,580 --> 00:20:06,180
A lot of the theory is just stating that if you were to run the system the learning system for a long time if we make the assumption that

265
00:20:07,380 --> 00:20:09,380
the optimization algorithm is

266
00:20:10,020 --> 00:20:13,780
fairly good at producing, you know an improved response to the

267
00:20:14,500 --> 00:20:20,980
Other player in this type of zero sum game you if you're assuming that if the successive sort of series of best responses

268
00:20:21,620 --> 00:20:24,020
That the optimization algorithm is generating

269
00:20:24,900 --> 00:20:31,300
Continues to improve over the previous ones you could make the assumption that maybe eventually it does get to that equilibrium

270
00:20:31,380 --> 00:20:34,580
But there is no mathematical guarantee that this actually happens

271
00:20:35,380 --> 00:20:37,380
what we want to do is

272
00:20:37,780 --> 00:20:40,180
You know build this latent dynamics

273
00:20:41,140 --> 00:20:45,620
You know a predictive model which is a simulacrum of what the idealized version is

274
00:20:45,940 --> 00:20:49,620
But we don't have a way of directly computing the regrets. So we kind of perform

275
00:20:50,420 --> 00:20:53,460
You know, we learn a proxy for that regret. How does that work?

276
00:20:53,700 --> 00:20:57,140
So we think of regret in the following way. So so there's kind of this um

277
00:20:57,780 --> 00:21:04,180
Old school result from like um mdp theory or maybe it's not that old but like 20 years ago or something like that called the simulation lemma

278
00:21:04,500 --> 00:21:06,500
and that basically says that you know

279
00:21:06,740 --> 00:21:09,700
If we let's assume for now that we we have like an optimal planner

280
00:21:09,780 --> 00:21:13,860
So we can give our like model of the world to this optimal planner and end some reward function

281
00:21:13,940 --> 00:21:16,260
Let's say later down the road we get given some reward function

282
00:21:16,740 --> 00:21:21,300
And so we give the model and the reward function to our optimal planner and we assume that this planner can return

283
00:21:21,700 --> 00:21:23,700
The optimal policy in our model

284
00:21:24,260 --> 00:21:26,260
So we kind of have this, you know planning oracle

285
00:21:26,980 --> 00:21:28,820
And if we assume that we can do that

286
00:21:28,820 --> 00:21:32,180
Then we can think about the difference between like how good the policy would be from

287
00:21:32,740 --> 00:21:36,660
Our planning oracle in the model versus the truly optimal policy in the real world

288
00:21:37,460 --> 00:21:41,940
And so what the simulation lemma tells us is that you know the difference between these two policies

289
00:21:41,940 --> 00:21:44,980
So the one found by acting optimally in the model versus the truly optimal one

290
00:21:45,460 --> 00:21:49,060
Is bounded essentially by the error between the model and the real world

291
00:21:49,780 --> 00:21:53,220
Under the distribution of states that the policy would generate

292
00:21:53,460 --> 00:21:58,340
So so, you know, it only it only matters that we have low error where the policy would go essentially because you know

293
00:21:58,580 --> 00:22:01,060
If there are some states that are just completely irrelevant what the policy is going to do

294
00:22:01,060 --> 00:22:03,060
It's not really going to matter if the if the model is not

295
00:22:03,540 --> 00:22:04,820
Accurate and there

296
00:22:04,820 --> 00:22:06,820
So we kind of use this result to think about the regret

297
00:22:06,900 --> 00:22:09,540
So that that gives us like, you know, if we have like one

298
00:22:10,340 --> 00:22:13,700
One true mdp and one model of an mdp and one reward function

299
00:22:14,340 --> 00:22:16,180
The simulation lemma can tell us, you know

300
00:22:16,180 --> 00:22:20,100
What would kind of be the regret if we did this optimal planning within this one model of the

301
00:22:20,980 --> 00:22:22,260
Of the mdp

302
00:22:22,260 --> 00:22:26,180
But then in our work, we're not really interested in the setting of like one mdp one reward function

303
00:22:26,900 --> 00:22:32,820
Um, so we start to think about, you know, what happens if we have arbitrarily many environments as well as arbitrarily many reward functions

304
00:22:32,980 --> 00:22:34,500
Which we don't know in advance

305
00:22:34,500 --> 00:22:38,420
And then I guess the other thing that I should say like you you alluded to like latent dynamics is

306
00:22:38,660 --> 00:22:44,100
You know, these existing results are assuming that we have an mdp. That's fully observable meaning, you know exactly what the state of the environment is

307
00:22:44,660 --> 00:22:48,980
Um, but usually when we think about like world models or even or just maybe more modern reinforcement learning

308
00:22:49,620 --> 00:22:52,900
We're really interested in learning from like quite high dimensional signals. So

309
00:22:53,460 --> 00:22:55,220
images or maybe

310
00:22:55,220 --> 00:22:58,500
Probably images, but maybe there are the high high dimensional signals we want to reason about

311
00:22:59,300 --> 00:23:03,780
And because we're just using image observations, this means that the world is like partially observable

312
00:23:03,780 --> 00:23:07,300
Like we can't infer everything we need to know about the world just from one image, you know

313
00:23:08,420 --> 00:23:13,220
For basically any physical task like the velocity of objects is important, but you can't infer that just from one image

314
00:23:13,940 --> 00:23:17,140
Um, so in this partially observable environments, we really want to take

315
00:23:17,700 --> 00:23:22,660
A sequence of observations because we need to to use those sequence of observations to infer what the state is

316
00:23:22,980 --> 00:23:26,100
So, you know viewing a sequence of images will help me to infer what the um

317
00:23:26,820 --> 00:23:30,340
The velocities are for example, and so we can think of this as inferring like a belief

318
00:23:31,060 --> 00:23:33,940
A belief over what the state is and a partially observable mdp

319
00:23:34,660 --> 00:23:39,300
Um, so we need this full sequence of images and we need to use the full sequence of images to then to be able to predict ahead

320
00:23:39,300 --> 00:23:43,700
What the next observation will be and that's kind of what you know, most world models are attempting to do

321
00:23:44,260 --> 00:23:49,540
Um, but if we just like taken a bunch of images and then try and directly predict images again, that's like quite a hard problem

322
00:23:50,260 --> 00:23:52,980
Um to just like just predict straight an image space

323
00:23:53,460 --> 00:23:56,900
And so the most common thing to do is kind of to take your previous sequence of images

324
00:23:57,300 --> 00:24:01,940
And then try and get like some compressed representation of the history of images into like the latent state

325
00:24:02,660 --> 00:24:04,660
And then predict the dynamics in the latent state

326
00:24:05,380 --> 00:24:07,300
So yeah, so I have my sequence of images

327
00:24:07,380 --> 00:24:09,940
I kind of compress these somehow into some vector

328
00:24:10,420 --> 00:24:15,540
And then I give it a new new action and I try and predict what the next kind of latent vector will be given this new action

329
00:24:15,860 --> 00:24:18,500
And this now represents my prediction of the dynamics in the world

330
00:24:18,900 --> 00:24:22,420
And then if I want to um, you know predict what the next observation would be an image space

331
00:24:22,420 --> 00:24:24,100
Then I can also decode that back to an image

332
00:24:24,660 --> 00:24:29,060
Um, but then a lot of works also argue that maybe we don't want to actually learn to predict the entire image

333
00:24:29,060 --> 00:24:31,300
So maybe you don't want to actually decode the entire image

334
00:24:31,300 --> 00:24:37,140
But that's that's another aspect that we might want to get into but there's this whole broad story of of um working in the latent space

335
00:24:37,540 --> 00:24:41,860
And um in reinforcement learning there was that paper called world models by you know, david haran and schmidhuber

336
00:24:42,340 --> 00:24:48,500
And it also I think has a relationship with you know, what lakoon's doing with jepper and these like you know joint embedding prediction architecture

337
00:24:48,500 --> 00:24:51,060
So there seems to be something magical about working in in the latent space

338
00:24:51,380 --> 00:24:55,460
And also you were talking about um, you know partially observable markoff decision processors

339
00:24:55,780 --> 00:24:59,140
And you know, that seems to be this idea that we need to have a modeling framework for the world

340
00:24:59,540 --> 00:25:04,260
And I guess like the ideal situation would be is that like we just we we knew exactly what would happen

341
00:25:04,660 --> 00:25:07,140
You know every single time step in every single state

342
00:25:07,700 --> 00:25:10,820
Um, but we don't you know, so so we model it as a partially observable

343
00:25:11,060 --> 00:25:13,940
Markov decision process and the markov bit is quite interesting as well

344
00:25:13,940 --> 00:25:17,860
I mean maybe and you guys can just sort of introduce what why do we use that as a model?

345
00:25:18,260 --> 00:25:23,620
So markovian basically just means you only need to look at like the current state to be able to infer all the information about the system

346
00:25:24,340 --> 00:25:26,420
um, so so in a markov decision process

347
00:25:26,580 --> 00:25:31,620
We have some state and then we assume that we're able to take some actions and given some state in some action

348
00:25:31,620 --> 00:25:33,780
We get some distribution over next states of the system

349
00:25:34,100 --> 00:25:37,380
And then the the system will transition according to that distribution to the next state

350
00:25:37,620 --> 00:25:41,780
And this is just like kind of a general framework for modeling like systems that we might want to control

351
00:25:41,940 --> 00:25:45,060
So, you know, it kind of dates back to like early work and control theory

352
00:25:45,060 --> 00:25:47,380
But then it's also the main framework used in reinforcement learning

353
00:25:47,860 --> 00:25:48,020
um

354
00:25:48,020 --> 00:25:50,740
Yeah, and the reinforcement learning setting because it's the decision process

355
00:25:50,820 --> 00:25:56,660
We we also add an reward function which tells us how good it is to be in a certain state or to execute a certain state action pair

356
00:25:57,300 --> 00:26:00,900
Um, but yeah, as you said with relating to like partial observability and a lot of like systems

357
00:26:00,980 --> 00:26:03,460
We we don't actually know what the true like state of the world is

358
00:26:03,460 --> 00:26:08,420
So so you can imagine, you know, if we want to think of the entire world as a partially observable mdp

359
00:26:09,220 --> 00:26:14,900
We can't just have some vector telling us exactly what the true configuration of the world is or maybe that exists

360
00:26:14,900 --> 00:26:19,220
But we can't we definitely can't just know that and so we usually think of it as being a partially observable system

361
00:26:19,940 --> 00:26:24,660
Um, so this means that like given given the state, um, you know at each step

362
00:26:24,660 --> 00:26:28,580
We'll basically get some distribution over observations and we just get to observe that observation

363
00:26:28,980 --> 00:26:34,740
So, you know, the state of the world could be what it currently is in here and maybe my um observation is like a camera image

364
00:26:34,820 --> 00:26:36,580
so I only get some

365
00:26:36,580 --> 00:26:39,700
Camera image of the world that allows me to infer a bit of information about the state

366
00:26:40,340 --> 00:26:42,820
Um, and because it only allows me to infer a bit of information about the state

367
00:26:42,820 --> 00:26:44,260
It doesn't tell me the whole state

368
00:26:44,260 --> 00:26:48,740
It really you need to keep track of all of the observations you have to be able to keep track of all the information

369
00:26:48,740 --> 00:26:49,700
You have about the world

370
00:26:49,700 --> 00:26:55,700
So, you know, you can imagine um, if the task is for me to remember how to get out the door a while ago

371
00:26:55,940 --> 00:27:01,460
Um, you know, I don't just need to be able to like look at my current image of the world to be able to infer that information

372
00:27:01,460 --> 00:27:04,020
I need to have kept track of like all my previous information as well

373
00:27:04,580 --> 00:27:10,100
Um, so that's kind of why we think about often want to think about like partially observable environments as opposed to fully observable ones

374
00:27:10,340 --> 00:27:15,140
Amazing amazing. So so minci, maybe you can um bring in this this latent idea

375
00:27:15,860 --> 00:27:18,660
And and sort of contrast that to what lacuna is doing as well

376
00:27:19,060 --> 00:27:24,740
Sure, I mean, so I think in machine learning and deep learning, uh, there's this general paradigm that's been around

377
00:27:24,980 --> 00:27:26,980
You know since the inception which is learning

378
00:27:27,940 --> 00:27:33,540
latent latent representations of data and one of the benefits of learning latent representation is that

379
00:27:34,340 --> 00:27:38,660
You know, ideally your objective, uh, that leads to learning these latent representations

380
00:27:38,820 --> 00:27:45,540
Is that you are ultimately learning a lower dimensional representation of the data or dynamics that you're modeling like in our case with the world model

381
00:27:45,860 --> 00:27:52,900
Um, that captures just what is necessary. It's a more compact representation of just the information that's necessary to predict

382
00:27:53,300 --> 00:27:57,860
The task you're trying to predict and so um with uh with our case

383
00:27:58,340 --> 00:28:01,940
Or latent space world models a lot of the benefit of working in the latent space

384
00:28:02,020 --> 00:28:04,980
Is that if as opposed to working in the full image space?

385
00:28:05,060 --> 00:28:05,540
for example

386
00:28:05,540 --> 00:28:10,580
If your observations are images like in a video game is that there could be a lot of spurious features

387
00:28:10,820 --> 00:28:17,220
Or you know a lot of additional information that you could be expending lots of compute and um, you know gradient updates

388
00:28:17,460 --> 00:28:22,820
Just to learn those patterns when they don't actually impact the ultimate um transition dynamics

389
00:28:22,900 --> 00:28:26,740
Or reward dynamics that you need to learn in order to do well in that environment

390
00:28:26,900 --> 00:28:30,820
So one example is if you have a game where, you know, maybe the background is different

391
00:28:31,140 --> 00:28:36,820
Uh, because it's daytime or nighttime or it's close to sunset. Um, but ultimately, you know, the background

392
00:28:37,380 --> 00:28:39,380
doesn't really impact

393
00:28:39,460 --> 00:28:45,140
How the player moves around in the environment or whether they've reached the end goal of the task and so

394
00:28:45,780 --> 00:28:49,780
If you're training a model where it needs to compress a lot of this information

395
00:28:49,860 --> 00:28:53,540
First into a smaller dimensional latent vector or latent representation

396
00:28:53,940 --> 00:28:58,260
Um, you don't really need you would expect that latent representation not to actually capture

397
00:28:58,500 --> 00:29:03,300
It would start to ignore the background color and it might only capture certain features of the environment that can

398
00:29:03,860 --> 00:29:08,260
Essentially if you were to decode it back out it might only capture certain information about the environment

399
00:29:08,340 --> 00:29:10,900
That's predictive of the actual task that you want to solve

400
00:29:11,300 --> 00:29:14,500
Um, so maybe if the task is to say reach a coin at the end of a level

401
00:29:14,740 --> 00:29:19,620
Then maybe the latent representation would capture the presence of the coin or whether the the proximity of the character

402
00:29:19,780 --> 00:29:21,780
You're controlling to the coin

403
00:29:22,180 --> 00:29:23,540
and so

404
00:29:23,540 --> 00:29:24,980
With the jeppa related work

405
00:29:24,980 --> 00:29:30,660
I think a lot of this is also, you know, motivated with this idea where if we can learn a better latent space representation

406
00:29:30,980 --> 00:29:34,260
Um of images or videos or whatever modality we're trying to model

407
00:29:34,500 --> 00:29:39,380
Um, it's a much lower dimensional computationally efficient representation. Uh that you can

408
00:29:40,260 --> 00:29:43,220
You can effectively use for downstream tasks. Um

409
00:29:44,180 --> 00:29:48,740
I'm not I'm actually not super familiar with exactly, you know, the the visual jeppa

410
00:29:50,740 --> 00:29:56,740
Objective so you don't think I can say too much about that. Oh, that's okay. Yeah. I mean, but yeah, I mean you pretty much nailed it

411
00:29:56,980 --> 00:29:58,340
so, um, I mean

412
00:29:58,340 --> 00:30:01,460
Lacune even gives the example of like, um, you know in self-driving cars

413
00:30:01,780 --> 00:30:04,580
You might not be interested in the leaves on on on the road, you know

414
00:30:04,580 --> 00:30:07,700
So like with increasing levels of of nesting you kind of like learn to

415
00:30:08,100 --> 00:30:11,620
Ignore the things that are not relevant and focus on the things that that are relevant

416
00:30:12,020 --> 00:30:17,220
But we're almost getting to the center of the bulls I hear so intelligence to me is all about model building

417
00:30:17,380 --> 00:30:22,180
And and that's what these abstractions are. They're they're models that kind of are predictive about the thing that that that's relevant

418
00:30:22,260 --> 00:30:24,900
and kind of like ignoring what is not relevant and

419
00:30:25,380 --> 00:30:30,660
We build better models when we have a curriculum. I mean apparently this happens in nature as well. Max Bennett

420
00:30:30,660 --> 00:30:34,340
I was talking to him the other day and he said, you know, our genome doesn't encode all of our skills

421
00:30:34,580 --> 00:30:39,060
Um explicitly because it would be too inefficient to do so, but they do encode a kind of curriculum

422
00:30:39,300 --> 00:30:43,700
So we teach babies. Yeah, we babble with babies and we teach babies how to talk and stuff like that

423
00:30:43,940 --> 00:30:49,380
So so the curricula is is really important and then we're getting to the center of the bull's eye

424
00:30:49,460 --> 00:30:51,700
Which is intelligence in in general now

425
00:30:52,260 --> 00:30:58,260
I think Lacune thinks that it's specialized and and what that means is that there are there are motifs

426
00:30:58,740 --> 00:31:01,700
That's statistically generalized and what that means is that

427
00:31:02,580 --> 00:31:03,780
You do need

428
00:31:03,780 --> 00:31:07,620
environments you need to find motifs that are present in

429
00:31:08,260 --> 00:31:12,900
In as many environments as possible and those are the generalizing features. Do would you agree with that?

430
00:31:13,380 --> 00:31:15,380
Yeah, definitely. I think that a lot of um

431
00:31:16,020 --> 00:31:20,260
So a lot of really powerful machine learning methods, for example, uh are trained in simulation

432
00:31:20,660 --> 00:31:24,500
And when you're training in simulation, there's a concept in control from control literature

433
00:31:24,900 --> 00:31:29,460
Called the sim 2 real gap and essentially this is essentially quantifying a performance difference between

434
00:31:30,020 --> 00:31:37,460
Well, it's quantifying a few things one is just how different is the are the actual physical or other other kinds of dynamics captured by your simulator

435
00:31:37,540 --> 00:31:44,180
Compared to reality. So if you have a physics simulator, how accurate are for example the friction dynamics or different kinds of contact dynamics?

436
00:31:44,660 --> 00:31:49,380
In your robotic simulator compared to those actual dynamics in the real world with a real robot

437
00:31:49,700 --> 00:31:52,900
Um, and this also leads to a sim 2 real gap in terms of performance

438
00:31:53,060 --> 00:31:59,460
So if you train in the simulator, you know, a lot of times what machine learning is really good at is it's really good at learning to exploit

439
00:31:59,620 --> 00:32:05,220
Whatever system you're training this the model in and so it's fairly um common for

440
00:32:05,620 --> 00:32:10,900
You know systems that are models that are trained within a simulator to learn to eventually exploit the simulator

441
00:32:11,140 --> 00:32:13,140
and so actually like one big area of um

442
00:32:13,300 --> 00:32:18,420
Games ai is using is actually leveraging this idea where they essentially use ml models

443
00:32:18,580 --> 00:32:25,060
They optimize ml models to within a certain game environment to try to find bugs within that environment to look for exploits automatically

444
00:32:25,300 --> 00:32:28,500
Um, so ml systems are very good at finding exploits in whatever system you have

445
00:32:28,740 --> 00:32:34,580
But then the issue is those exploits are usually where exactly where the gap between your simulator and reality resides

446
00:32:34,820 --> 00:32:40,980
And so you actually don't want your model to learn to exploit these differences between the simulator and reality to get a high performance

447
00:32:41,300 --> 00:32:45,220
Uh, because that kind of defeats the purpose of then later transferring your model

448
00:32:45,540 --> 00:32:52,500
That's trained in simulation to reality because now in reality, obviously the model can't exploit those same those same glitches within the simulator

449
00:32:52,900 --> 00:32:54,900
Um, yeah, so yeah

450
00:32:54,900 --> 00:33:00,180
Yeah, I mean because the reason this is really interesting is is that the the premise of your paper is that

451
00:33:00,900 --> 00:33:04,020
It is possible to build a generalist agent

452
00:33:04,420 --> 00:33:09,940
Which means it's an agent that can be fine tuned and work really well on a whole bunch of downstream tasks

453
00:33:10,260 --> 00:33:15,300
And to me that implies that at least in our physical world in any situation

454
00:33:15,380 --> 00:33:21,780
You might use this agent that there are general motifs that it could have learned during free training that it could like, you know

455
00:33:22,020 --> 00:33:28,820
Become activated in any situation. Um, does that is that fair? Yeah, maybe I can say something about um

456
00:33:29,140 --> 00:33:32,900
Just the way that we should could think about like the different like latent dynamic subjective

457
00:33:32,980 --> 00:33:37,540
So so I think I agree that like at least when I try and think about how I think or how people think

458
00:33:37,620 --> 00:33:38,820
I think I agree that like

459
00:33:38,820 --> 00:33:43,380
You know a truly intelligent system should kind of think through the world and like a very compressed representation of the world

460
00:33:43,380 --> 00:33:45,940
Like if I'm trying to like think through how to go to the airport

461
00:33:46,020 --> 00:33:51,140
Like I'm definitely not like predicting ahead in terms of like the raw image space of trying to predict every image

462
00:33:51,140 --> 00:33:55,700
I might observe on the way the airport and things like this. And so I think we have this kind of like trade-off between, you know

463
00:33:56,500 --> 00:34:00,020
Um, like we said with the bgf of paper like should should be just try and like

464
00:34:00,500 --> 00:34:03,940
Kind of basically model like the minimum information we need about the world to try and you know

465
00:34:04,020 --> 00:34:08,340
Do the do the relevant task in the world? I think what you're saying. I think that probably is

466
00:34:09,140 --> 00:34:12,500
Maybe more what we think about when we think about like human intelligence or something like that

467
00:34:13,060 --> 00:34:15,780
Um, but then there's also this other way where we just say we're going to just like

468
00:34:16,500 --> 00:34:18,820
Enforce the model to be able to predict ahead every single image

469
00:34:19,220 --> 00:34:23,140
And so in our paper, we do actually enforce that the model has to predict the next image

470
00:34:24,020 --> 00:34:26,020
um, and so um

471
00:34:26,340 --> 00:34:28,740
Basically what this might mean is yeah, like maybe the model does

472
00:34:29,300 --> 00:34:34,580
You know, hopefully it does like like you said like kind of capture the underlying like true things that matter in the environment

473
00:34:34,820 --> 00:34:37,380
But it might also mean like what we're saying with like the leaves example

474
00:34:37,380 --> 00:34:40,580
Like this might force the model to kind of capture a lot of irrelevant details

475
00:34:40,580 --> 00:34:42,820
That don't really matter like the leaves on the ground and things like this

476
00:34:43,220 --> 00:34:46,100
And so, you know, maybe that means it isn't actually capturing the underlying motifs

477
00:34:46,100 --> 00:34:50,180
It's actually just getting good at image generation. Um, but then I've or image prediction I should say

478
00:34:50,980 --> 00:34:55,940
Um, but then I've also heard arguments kind of saying, you know, so what if people don't really think in terms of like image prediction

479
00:34:56,100 --> 00:34:58,980
You know, I you know, we think in terms of like more like these high level motifs

480
00:34:59,300 --> 00:35:01,380
But people have other people would argue that you know

481
00:35:01,620 --> 00:35:04,660
Kind of the machine learning machinery is there to do really good image prediction

482
00:35:05,060 --> 00:35:09,060
So so if if we if we can get a model that can actually just like predict images ahead really well

483
00:35:09,540 --> 00:35:12,900
Um, and not really worry so much about whether it's reasoning about these like high level features

484
00:35:13,220 --> 00:35:17,940
You know, if you can predict images ahead really well, you know, that's enough to make to do good decision making a lot of context

485
00:35:18,180 --> 00:35:19,940
So I think there's this kind of like

486
00:35:19,940 --> 00:35:22,740
Contrasting ways of thinking about, you know, image prediction is good enough

487
00:35:22,740 --> 00:35:26,580
We'll just predict like really visually good scenes and that will be good enough for decision making

488
00:35:26,900 --> 00:35:30,340
Or do we want to force the model to try and reason about like more abstract features of the environment?

489
00:35:30,340 --> 00:35:35,380
And that's kind of a more intelligent way of reasoning about the world. Um, and yeah, I think that's a very interesting trade-off

490
00:35:36,420 --> 00:35:38,420
Yeah, yeah, I mean like it's um

491
00:35:39,140 --> 00:35:41,220
Like the biggest problem in machine learning is overfitting, you know

492
00:35:41,220 --> 00:35:43,940
So as you say like that, there are all of these statistically generalizing features

493
00:35:43,940 --> 00:35:48,100
But they generalize within the domain and the domain might be like your your simulator or like, you know

494
00:35:48,100 --> 00:35:50,820
How you're training it rather than how it's being used in in production

495
00:35:51,060 --> 00:35:55,780
And then as you say that there's also this um almost human chauvinistic or puritanical view on this which is that well

496
00:35:56,340 --> 00:36:00,420
You know, it does the right thing for the wrong reasons or I use different motifs to do the reasoning

497
00:36:00,420 --> 00:36:02,900
So that thing must be doing it wrong. Do you know what I mean?

498
00:36:03,220 --> 00:36:07,860
And um, I was talking with chris bishop at msr the other day and and you know, he's um big on

499
00:36:08,100 --> 00:36:13,460
Symmetries and yeah, you know, the kind of stuff that like max welling and takako hen and bronstein and um

500
00:36:13,460 --> 00:36:15,380
The deep mind has done loads of cool stuff on on this

501
00:36:15,620 --> 00:36:20,900
But it's this idea that like we know the world um has a certain geometry. It has certain physical priors

502
00:36:20,900 --> 00:36:25,780
So like we can deliberately um, you know, kind of construct the approximation class in machine learning

503
00:36:26,260 --> 00:36:30,580
Methods so so that like we make it an easier problem, right? Because we because we know we know the thing is in there

504
00:36:31,700 --> 00:36:33,700
Yeah, so I mean, I guess sort of the uh

505
00:36:34,260 --> 00:36:36,260
Slight tangent I went into around the sim to real gap

506
00:36:36,420 --> 00:36:39,540
I guess part of the point I wanted to make there is that um, you know

507
00:36:39,540 --> 00:36:42,420
One way around the sim to real gap is you could try to train

508
00:36:43,060 --> 00:36:47,060
You could try to parametrize a very large space of possible versions of reality

509
00:36:47,300 --> 00:36:53,380
And this is kind of the motivation behind this method of domain randomization where you sort of say this is the you know

510
00:36:53,380 --> 00:36:56,660
This is the specific task domain I care about I can parametrize the different

511
00:36:56,980 --> 00:36:59,380
Uh versions of the task with a few parameters

512
00:36:59,620 --> 00:37:06,020
And I basically want to search over the space of parameters and train my model or my agent on all possible variations of this world

513
00:37:06,180 --> 00:37:10,820
But obviously that's not very sample efficient because that design space could be huge could be massive

514
00:37:11,060 --> 00:37:14,820
And so instead we like these active sampling strategies like we were talking about earlier

515
00:37:15,380 --> 00:37:17,380
around mini max regret style

516
00:37:17,540 --> 00:37:22,340
Active sampling where you sample those environments that maximize your regret or some other type of objective

517
00:37:22,420 --> 00:37:25,940
Maybe like uncertainty uh similar to what we do in the waker paper

518
00:37:26,500 --> 00:37:29,860
But ultimately these things these active sampling process it leads to

519
00:37:30,660 --> 00:37:33,300
What we like to call an auto curriculum automatic curriculum

520
00:37:33,940 --> 00:37:37,060
And this is in contrast to prior curriculum learning works because here

521
00:37:37,380 --> 00:37:42,500
This is an automatically generated curriculum. So you you can kind of not have any pre-defined notion of what is

522
00:37:42,900 --> 00:37:48,900
Easy or hard it's purely fixed to what is easier or hard for the model in terms of how good the model is at

523
00:37:49,060 --> 00:37:51,780
Performing at those tasks. And so it's nice. It's an automatic curriculum

524
00:37:51,940 --> 00:37:56,660
So you can think of it as almost like weaving a path through this high-dimensional design space

525
00:37:57,380 --> 00:37:59,380
automatically such that if the

526
00:37:59,380 --> 00:38:05,620
Agent or model were to train on data along this path of environments through its experiences in this path of environments during the training curriculum

527
00:38:05,860 --> 00:38:09,060
It'll basically be maximizing some sort of information gain objective

528
00:38:09,380 --> 00:38:10,100
um

529
00:38:10,100 --> 00:38:14,500
Because you know, for example regret if there's a high regret that's that means there's a high

530
00:38:15,300 --> 00:38:18,420
Ceiling there's a high gap in terms of how much the agent can improve

531
00:38:18,660 --> 00:38:21,140
Which implies that there's a lot more for the agent to learn in those environments

532
00:38:21,300 --> 00:38:26,740
So it's sort of this like optimal you want to find this optimal path weaving through the high-dimensional design space of environments

533
00:38:27,060 --> 00:38:34,820
Now the danger here is that as you do this, uh, auto curriculum the auto curriculum, uh, could also go haywire very easily because

534
00:38:35,060 --> 00:38:40,580
The design space is so big if you're training in simulation, which we have to do because these methods are so sample inefficient

535
00:38:40,580 --> 00:38:43,780
We need so much data to train them. Um, you want to train in simulation

536
00:38:43,780 --> 00:38:46,820
But if you're doing the auto curriculum in the simulation design space

537
00:38:47,220 --> 00:38:53,380
It could start to veer very easily and quickly into different corners or niches of the design space where

538
00:38:53,940 --> 00:39:00,260
You know the parameters no longer really make sense in terms of mapping to a physical reality or a real world scenario

539
00:39:00,580 --> 00:39:02,580
That we as human users

540
00:39:02,660 --> 00:39:05,620
Uh actually care about and so kind of it would be you know

541
00:39:05,780 --> 00:39:09,860
It would defeat the purpose of spending all this compute to train this model that could then help us in the real world

542
00:39:09,940 --> 00:39:14,420
Because now it's veering off into parts of the design space that don't really matter for humans

543
00:39:14,420 --> 00:39:20,020
It's kind of noisy parts of the design space. And so this kind of leads us to this question of grounding

544
00:39:20,020 --> 00:39:23,620
How do we ground curricula? How do we align the curricula such that you know?

545
00:39:23,620 --> 00:39:28,420
They can still do their exploration through this active sampling type of procedure over the environment design space

546
00:39:28,580 --> 00:39:35,220
But at the same still at the same time maintain at least some proximity to the parts of that design space that are relevant to

547
00:39:35,700 --> 00:39:38,340
What humans care about in terms of the actual tasks they represent

548
00:39:38,500 --> 00:39:41,940
I mean i've been speaking with kenneth stanley a lot recently and we're talking about open-endedness

549
00:39:42,340 --> 00:39:45,860
And in general i've been trying to come at this problem from multiple angles

550
00:39:45,940 --> 00:39:51,060
And i've been using the lens of agency because i think agency is something that happens in the real world

551
00:39:51,060 --> 00:39:55,140
And that's why we have this divergent process because we have multiple agents, you know, kind of like

552
00:39:55,540 --> 00:39:59,540
You know undirected following their own gradient of interestingness. So in in evolution

553
00:39:59,540 --> 00:40:02,980
That's a great example that it is this divergent process, but it's also grounded

554
00:40:02,980 --> 00:40:08,340
It's physically grounded, you know, so it's like the physical world creates some kind of constraints on on the things that are found

555
00:40:08,660 --> 00:40:12,900
And i mean, you know clune called this ai generating algorithms. There's quite a few different takes on this

556
00:40:12,900 --> 00:40:16,020
But the idea is that um to search this complex search space

557
00:40:16,100 --> 00:40:21,940
We we need to have a divergent search and that's like we actually need to create the problems and the solutions

558
00:40:21,940 --> 00:40:23,220
So like in the real world

559
00:40:23,220 --> 00:40:27,620
The the you know the giraffes had the problem of like eating the leaves from from from the trees

560
00:40:27,780 --> 00:40:30,260
And the problems and the solutions get generated in tandem

561
00:40:30,420 --> 00:40:32,500
And this whole thing just kind of grows and grows and grows

562
00:40:32,660 --> 00:40:39,460
And that seems to be the most important feature that is missing in current ai systems and the grounding or the

563
00:40:40,660 --> 00:40:43,940
Stanley calls it the gradient of interestingness. I'm not sure whether you'd agree with that

564
00:40:44,020 --> 00:40:49,060
But um, i mean what mark what what what do you think about the importance of like this divergence in ai?

565
00:40:49,700 --> 00:40:52,020
kind of the current paradigm of machine learning

566
00:40:52,660 --> 00:40:58,100
Of kind of like, you know gathering some data set beforehand or specifying some simulated beforehand if it's reinforcement learning

567
00:40:58,580 --> 00:41:02,580
Is kind of good enough to do like a lot of reasonable tasks that we might care about

568
00:41:03,060 --> 00:41:03,860
um, you know

569
00:41:03,860 --> 00:41:08,900
Like obviously like predicting language or generating simulated language or performing very well at some simulated task in rl

570
00:41:09,140 --> 00:41:12,660
But it definitely seems like the next step towards like very general agents that are kind of

571
00:41:13,460 --> 00:41:16,100
You know, I guess maybe I don't know if we want to use the term agi

572
00:41:16,100 --> 00:41:19,540
But there's something something more along the lines of a general agent that's kind of you know

573
00:41:19,860 --> 00:41:23,140
able to kind of self improve and learn in more diverse environments

574
00:41:23,620 --> 00:41:27,620
Um, it definitely seems like that's kind of the next step of where machine learning will go

575
00:41:28,180 --> 00:41:30,820
And if we're going to get to that point, I kind of agree with the idea that

576
00:41:31,540 --> 00:41:36,900
You know, it certainly doesn't make sense to have some agent that just randomly trying to gather completely random new knowledge

577
00:41:37,220 --> 00:41:40,580
Like it certainly seems to make sense that you know, you know, even as a human

578
00:41:41,220 --> 00:41:46,100
To improve your intelligence you kind of selectively try and find out the areas in which like you can gather more

579
00:41:46,500 --> 00:41:49,540
More information or more knowledge and things like this and this is kind of what you know

580
00:41:49,700 --> 00:41:54,340
Leads to this kind of I guess branching or you know, like you said like the diverse set of things um

581
00:41:55,140 --> 00:41:57,540
That you might want to learn more about and so yeah

582
00:41:57,540 --> 00:42:01,620
I think like it clearly seems to make sense that like this kind of more open-ended this thinking is probably going to be like

583
00:42:02,020 --> 00:42:04,340
The next paradigm of how we think about these kinds of systems

584
00:42:04,580 --> 00:42:06,580
But I'll I think mentally we'll have more to say about this

585
00:42:06,980 --> 00:42:12,820
I think the reason open-endedness is so interesting now is I think we're uh, there's there's a few reasons why I think it's like

586
00:42:12,900 --> 00:42:19,700
newly relevant to this current era of machine learning because these ideas have been around for quite a while like, um, Ken Stanley, Joe Lehmann

587
00:42:20,020 --> 00:42:22,020
um, Jeff Klune, uh

588
00:42:22,020 --> 00:42:24,180
Lisa Soros these a lot of these researchers, they've

589
00:42:24,900 --> 00:42:30,180
They've been thinking about open-endedness and novelty based search divergent search for decades. Um

590
00:42:30,740 --> 00:42:35,060
I think it's really interesting to think about why there's sort of this resurgence of these ideas now

591
00:42:35,300 --> 00:42:37,300
and I think a lot of it is because

592
00:42:37,380 --> 00:42:39,700
It is again, you know, it's it's sort of following the same

593
00:42:40,100 --> 00:42:40,820
um

594
00:42:40,820 --> 00:42:43,700
Sort of uh tailwinds that have been driving a lot of the ml industry

595
00:42:43,700 --> 00:42:46,980
Which is just like much better compute much larger datasets

596
00:42:47,300 --> 00:42:49,780
And I think what we're seeing now is that we know that

597
00:42:50,260 --> 00:42:55,140
Modern deep learning methods work best when we can scale up the compute and the data. That's how you get them to work

598
00:42:55,540 --> 00:42:59,380
Um to the to their maximal capabilities. Um at some point

599
00:42:59,380 --> 00:43:02,420
We're going to run out of data and a lot of people are now starting to talk about

600
00:43:02,660 --> 00:43:04,740
You know this as sort of a pending issue on the horizon

601
00:43:04,740 --> 00:43:09,140
Which is you know at the current rate of consuming data for training our foundation models at some point

602
00:43:09,140 --> 00:43:12,500
We're going to run out of data. We're going to where are we going to get the next trillion tokens from?

603
00:43:12,900 --> 00:43:15,620
Um, and so I think a lot of this uh now

604
00:43:16,020 --> 00:43:20,340
points a lot of the interest to open-endedness because open-endedness is essentially, you know

605
00:43:20,580 --> 00:43:23,460
We're studying systems that can generate their own data in an infinite

606
00:43:24,020 --> 00:43:27,700
Capacity and so it's systems that essentially if you run it for longer and longer

607
00:43:28,020 --> 00:43:32,420
They get more and more complex. They generate more and more quote-unquote interestingness or interesting data

608
00:43:32,820 --> 00:43:33,220
um

609
00:43:33,220 --> 00:43:36,580
And so if we can actually, you know crack this nut of how do we actually

610
00:43:36,980 --> 00:43:41,220
Come up with a self-improving system in the sense that it keeps generating interesting data

611
00:43:41,940 --> 00:43:46,740
We can then use that data to train further train our models

612
00:43:46,980 --> 00:43:50,900
But of course you get into this perpetual data machine type of

613
00:43:51,460 --> 00:43:54,500
Idea where obviously, you know, there's how do you generate more data?

614
00:43:55,380 --> 00:43:59,140
If you know the data is ultimately coming from a model that you probably trained on previous data

615
00:43:59,220 --> 00:44:00,980
How do you get net new information from that?

616
00:44:00,980 --> 00:44:05,940
Well, I think a lot of this is actually just resolved purely again going back to this idea of the reward function

617
00:44:06,100 --> 00:44:12,180
Right or a preference function where there is outside information coming in through some sort of filtering criteria

618
00:44:12,420 --> 00:44:14,420
For example human designers in the loop

619
00:44:14,820 --> 00:44:20,420
Or designers designing some sort of preference model that could essentially automatically rate the kinds of automatic

620
00:44:21,300 --> 00:44:23,540
Data that's being generated by these open-ended systems

621
00:44:23,700 --> 00:44:28,580
And if we can do this kind of filtering we can essentially automatically find start to automatically find

622
00:44:29,140 --> 00:44:33,060
Useful net new data net new trajectories net new even, you know, maybe

623
00:44:33,940 --> 00:44:38,020
Sentences like tokens or net new content to train our models on

624
00:44:38,340 --> 00:44:44,820
I've been thinking a lot about creativity recently and I think creativity is is is the other half of the coin of intelligence

625
00:44:45,220 --> 00:44:49,700
So in the world we live in I think that the intelligent process is is us

626
00:44:50,100 --> 00:44:52,100
We are a divergent search and we are

627
00:44:52,580 --> 00:44:56,420
And basically tackling a complex search space and we are building knowledge

628
00:44:56,740 --> 00:44:58,820
And we are memetically sharing them in our society

629
00:44:58,820 --> 00:45:02,660
We're embedding them in our language and then language models come and like acquire all of that knowledge

630
00:45:02,900 --> 00:45:06,900
So the cynical take is that ai today doesn't you know generalize and

631
00:45:07,700 --> 00:45:10,260
It doesn't it doesn't creatively find new knowledge

632
00:45:10,500 --> 00:45:13,300
It just is a representation of the knowledge that we have found

633
00:45:13,620 --> 00:45:18,260
But it's not black and white is it so the the work that you're doing is a great example of no no no

634
00:45:18,660 --> 00:45:26,660
You can generate new knowledge by exploring these complex search spaces and even though you're exploring existing models

635
00:45:27,060 --> 00:45:31,860
You're discovering interesting and novel combinations of those models that have not been found before

636
00:45:31,860 --> 00:45:36,100
So it's creating a novel margin on something that was not there before

637
00:45:36,420 --> 00:45:40,660
But I suppose the ideal future we want to get into is that we really can just

638
00:45:41,460 --> 00:45:44,340
From a far deeper level generate new knowledge

639
00:45:45,140 --> 00:45:50,580
Yeah, I think one interesting thing that I've been thinking about more recently, you know is that um sort of the you know

640
00:45:50,580 --> 00:45:52,340
The high level question is just

641
00:45:52,340 --> 00:45:59,220
Right now all of the state of the rai systems from chat gbt to stable diffusion style models for text image generation

642
00:45:59,380 --> 00:46:03,700
All of these systems they're they're amazing very impressive, you know

643
00:46:03,780 --> 00:46:08,260
Like five years ago. I would not have believed that these systems could exist at this level of performance today

644
00:46:08,660 --> 00:46:13,460
But uh, ultimately, uh, what they do is they're in the they're they're in the q&a business

645
00:46:13,620 --> 00:46:18,100
So I basically ask these systems a question or I give them a command and they give me an answer

646
00:46:18,740 --> 00:46:23,300
Um, and so I think the next frontier of ai is really how do we design systems that don't just

647
00:46:23,940 --> 00:46:27,460
Answer questions, but they actually are the ones that start to ask the questions

648
00:46:28,020 --> 00:46:32,100
And I think once we can have ai systems that start to ask interesting questions

649
00:46:32,740 --> 00:46:38,820
Um, that's when we start to get closer to I think traditional notions of what uh strong agi might be

650
00:46:39,140 --> 00:46:41,140
Okay, so so again really really interesting now

651
00:46:41,460 --> 00:46:45,460
So we're getting into agency and and people think that oh you could give a language model agency

652
00:46:45,460 --> 00:46:48,020
You just like you know run it in a loop and interesting things will happen

653
00:46:48,180 --> 00:46:51,540
Well, well, that's not true because the whole point of open-endedness is to prove that

654
00:46:51,860 --> 00:46:55,460
Existing systems converge so they don't diverge so they don't accumulate information

655
00:46:55,700 --> 00:46:59,060
So we would need to create a kind of agent that like, you know, it would just keep running

656
00:46:59,060 --> 00:47:03,060
And it would just keep doing interesting and novel things that would keep accumulating information

657
00:47:03,460 --> 00:47:08,180
And I think that the reason why language models don't have agency is because they are essentially

658
00:47:08,740 --> 00:47:14,420
A low entropy model and what that means is during training a lot of the the sort of like the unnecessary

659
00:47:14,980 --> 00:47:20,100
Um, you know complexity was snipped off. So the models only know about relevant things in the next step

660
00:47:20,100 --> 00:47:26,180
What's the next best token and it feels like we would need to have not only a higher entropy search

661
00:47:26,420 --> 00:47:30,900
But we would also need to have um a diverse set of models that are actively

662
00:47:31,620 --> 00:47:36,740
Continually learning and and diverging from from each other, but that's just my take. I mean, what do you guys think about that?

663
00:47:37,060 --> 00:47:38,740
Yeah, I think that

664
00:47:38,740 --> 00:47:43,540
So I guess this relates quite a lot to this idea of like intrinsic motivation, which is something that we utilize in our paper and I guess

665
00:47:44,340 --> 00:47:46,340
I guess the idea with that is like

666
00:47:46,820 --> 00:47:51,700
You know, if we're trying to like gather new data in the environment, like we shouldn't necessarily be constrained to just try and

667
00:47:51,940 --> 00:47:54,580
Gather new data that's like good for a specific task

668
00:47:56,180 --> 00:48:02,020
And so I guess this kind of you know, so intrinsic motivation basically says I should just gather new information because it's novel

669
00:48:03,460 --> 00:48:07,540
And things like this and so we can basically like specifically try and gather information that you know

670
00:48:08,020 --> 00:48:14,340
Reduces our uncertainty about the environment and and or similar objectives that that don't rely on some external reward signal

671
00:48:14,660 --> 00:48:19,940
And I think we when you get to the situation where the model is able to like self-improve in the absence of an external reward signal

672
00:48:20,020 --> 00:48:24,420
So intrinsic meaning that the the signal for what you should get is just purely generated by the model

673
00:48:24,420 --> 00:48:26,420
So it's purely intrinsic to the model

674
00:48:26,660 --> 00:48:32,660
Um, so I think the situation where you know, you have the model that's able to self-improve without any external signal without a human

675
00:48:32,660 --> 00:48:36,580
Having to define what the reward is or what the objective is or this was good data. This was bad data

676
00:48:37,140 --> 00:48:42,020
Um, I feel like that does feel like a lot closer to the notion of agency because of the fact you don't have kind of some

677
00:48:42,340 --> 00:48:44,580
External person defining what's good and what's bad?

678
00:48:45,140 --> 00:48:49,780
And so yeah, I think like this the like and you also mentioned the word like creativity because I think

679
00:48:50,180 --> 00:48:52,180
At least in the context of things that

680
00:48:52,260 --> 00:48:58,260
I've done in terms of machine learning or reinforcement learning. I think like intrinsic motivation feels like the closest thing related to creativity

681
00:48:58,580 --> 00:49:03,940
So you're basically like trying to gather information because it's novel or because you think it's or the model thinks it's interesting

682
00:49:04,340 --> 00:49:05,780
rather than because

683
00:49:05,780 --> 00:49:07,780
You know, it satisfies some objective

684
00:49:08,020 --> 00:49:13,540
And so I think we could maybe say like intrinsic motivation is in some sense like an objective for being creative as well

685
00:49:14,180 --> 00:49:17,540
Um, I don't know if you have any thoughts about this. Yeah, I think I think that uh

686
00:49:18,260 --> 00:49:23,940
It's I think there's definitely a hugely deep connection between intrinsic motivation and creativity. Um

687
00:49:24,500 --> 00:49:27,860
In the literature intrinsic motivations also sometimes called artificial curiosity

688
00:49:27,940 --> 00:49:30,420
So this is a term that was coined by Juergen Schmidt-Huber

689
00:49:30,900 --> 00:49:33,540
Could you could you explain it just what it is? Yeah, so oh, yeah

690
00:49:33,540 --> 00:49:37,940
So taking a step back intrinsic motivation is essentially um in reinforcement learning

691
00:49:37,940 --> 00:49:43,300
We train on reward signals and as mark was saying, um, we typically train on external reward signal by external

692
00:49:43,300 --> 00:49:49,140
We mean that this is a task based reward. So this is um external in the sense that something outside of the agent

693
00:49:49,140 --> 00:49:53,780
That's learning like the human system designer decided that this is what the reward signal is for the task

694
00:49:54,100 --> 00:49:58,740
Uh intrinsic means that we want to we don't design directly the reward signal

695
00:49:58,820 --> 00:50:01,780
But we're actually using some aspect of the model itself

696
00:50:02,180 --> 00:50:06,900
In order to drive the models learning forward. And so one example of this could be prediction error

697
00:50:07,060 --> 00:50:12,420
So if the model, uh has a large prediction error on a certain task like averaged over each time step

698
00:50:12,660 --> 00:50:19,300
We can use that as a reward signal and say, hey, you want to visit more parts of the environment where you're bad at predicting

699
00:50:19,620 --> 00:50:23,700
Um, how the state will transition when you act in that part of the environment. And so

700
00:50:24,260 --> 00:50:28,260
Uh, as you can see, this is very similar to maybe like intuitive notions of what curiosity is

701
00:50:28,900 --> 00:50:30,900
Curiosity and different forms of play

702
00:50:30,980 --> 00:50:35,460
Um in the psychology literature, a lot of people actually argue that, you know, different forms of play

703
00:50:35,940 --> 00:50:41,860
In curiosity really they they amount to you can model these behaviors as essentially a person trying to

704
00:50:42,500 --> 00:50:45,780
Engage in activities where, you know, they're not very good at predicting the outcome

705
00:50:45,940 --> 00:50:47,540
And that's kind of what makes you could argue

706
00:50:47,540 --> 00:50:53,860
That's kind of what makes certain kinds of entertainment fun because or entertaining because you can't actually predict what will happen

707
00:50:54,180 --> 00:50:55,140
um

708
00:50:55,140 --> 00:50:59,300
You know in a few frames of the movie like a movie wouldn't be very interesting or a book would not be very interesting

709
00:50:59,540 --> 00:51:03,460
If you can predict what will happen in the rest of the book just by reading the first few pages

710
00:51:03,860 --> 00:51:08,340
Uh, and so intrinsic motivation is really saying let's guide the model towards parts of the environment or the world

711
00:51:08,580 --> 00:51:11,300
Or experiences where it's similarly unpredictable

712
00:51:11,780 --> 00:51:16,180
Stanley speaks about this this concept of deception or we call it the false compass

713
00:51:16,500 --> 00:51:22,820
Which is this idea that any objective and even you you could say exploring all of the search space is an objective

714
00:51:22,820 --> 00:51:26,260
So he said every every objective has deception and if you monotonically

715
00:51:26,660 --> 00:51:29,700
Optimize any objective you will always lead into you know, like a

716
00:51:30,340 --> 00:51:37,300
Deceptive part of the search space, but then like the counter argument is okay. Well, let's let's not um, let's not have any principles for doing the

717
00:51:38,020 --> 00:51:43,220
You know the the exploration. Let's just do something completely random and that doesn't seem very good

718
00:51:43,380 --> 00:51:51,220
So so then, you know, there's this concept of well, how do I how do I imbue some concept of what's interesting without falling victim to deception?

719
00:51:51,300 --> 00:51:57,220
Yes, so ken stanley, uh has a famous essay in the realm of open-endedness where he points out

720
00:51:58,580 --> 00:52:00,580
That this notion of interestingness

721
00:52:01,220 --> 00:52:06,100
Is uh, ultimately a subjective concept and so even in the case of intrinsic motivation

722
00:52:06,100 --> 00:52:09,300
Which I think is you know in practice we can get a lot of mileage out of this

723
00:52:09,940 --> 00:52:14,500
And we've seen this in a lot of domains where exploration helps a lot like even in the wakeer paper

724
00:52:14,500 --> 00:52:19,620
it's largely founded on this idea on how we exploit intrinsic motivation for learning world models, but

725
00:52:20,420 --> 00:52:23,140
Ultimately, you know, these these model based

726
00:52:23,780 --> 00:52:30,340
Measures of intrinsic motivation. They are by definition based on the particular model at play and so

727
00:52:31,220 --> 00:52:36,740
At some point, you know, you're you're starting to over fit to what that specific model finds interesting

728
00:52:37,220 --> 00:52:42,020
And of course what that model finds interesting if your measure of interestingness is something like a prediction error

729
00:52:43,140 --> 00:52:49,460
Is going to be a function of you know, the specific architecture of the model the actual inductive biases of that model

730
00:52:50,100 --> 00:52:53,860
The capacity of that model to learn and so you could imagine a model where you know

731
00:52:54,260 --> 00:52:57,780
At the beginning it's looking for lots of interesting parts of a particular video game environment

732
00:52:57,860 --> 00:53:00,820
But at some point, you know, it might saturate what it can represent

733
00:53:01,060 --> 00:53:04,020
And what it can learn and at some point it might start to find things

734
00:53:04,100 --> 00:53:07,940
It's explored before interesting just because it's starting to forget those parts of the environment

735
00:53:08,100 --> 00:53:12,020
You know if you have like a very rich stream of different kinds of environments that it's exploring

736
00:53:12,180 --> 00:53:18,180
So ultimately this is like an example of deception because now it's like I I think that my model is the model thinks it's exploring

737
00:53:18,260 --> 00:53:21,300
Parts of the environment that it finds interesting based on this prediction error

738
00:53:21,460 --> 00:53:26,740
But ultimately it might actually start to go back to other parts of the environment because of issues of model capacity

739
00:53:26,820 --> 00:53:29,860
And another really famous example of this issue would be like the noisy tv

740
00:53:30,020 --> 00:53:33,220
So like if your environment has you know, this this uh

741
00:53:33,860 --> 00:53:38,820
Noisy tv where it's just showing random noise random rgb pixels. Um, you know, that's

742
00:53:39,460 --> 00:53:42,260
You know, that's not something you can actually predict because it's just noise

743
00:53:42,500 --> 00:53:48,100
And so the model if your intrinsic motivation is really just to search for novelty in the form of prediction error

744
00:53:48,260 --> 00:53:53,860
It might just start staring at this tv forever because it's something that it just can't predict and I know just by looking at that tv

745
00:53:53,940 --> 00:54:00,500
It'll be maximizing its prediction error. Yeah. Yeah, it's so interesting. Um, so so just coming into rich stuff in a little bit

746
00:54:00,500 --> 00:54:06,420
So he had this idea called um reward is enough and and essentially that doesn't make in the case that you know

747
00:54:06,500 --> 00:54:11,700
Just using um implicit uh motivation all the stuff that that you've just been speaking about using this trajectory

748
00:54:12,180 --> 00:54:15,940
You know optimization process that we can do everything we need to do

749
00:54:16,260 --> 00:54:22,020
And in in your paper, you're kind of making an argument similar to what lakuna has been making for years about self supervised image learning

750
00:54:22,020 --> 00:54:25,700
That what we should do guys is let's let's kind of pre-train a base model

751
00:54:26,020 --> 00:54:29,060
So this model um understands environmental dynamics really well

752
00:54:29,380 --> 00:54:32,820
And then we stick a reward in there and and we build um agents after that

753
00:54:32,900 --> 00:54:38,500
So does it in any way reinforce or pun intended uh satan or or do you think it's still complimentary?

754
00:54:39,140 --> 00:54:44,500
I think it's still complimentary at least if I understand the the meaning of the reward is enough paper because my understanding of that

755
00:54:45,060 --> 00:54:48,580
Um line of thought is basically saying that you know, we can kind of specify

756
00:54:49,140 --> 00:54:54,500
You know any tasks that we might want an intelligent agent to do as optimizing a reward in some like mdp or promdp

757
00:54:54,580 --> 00:54:58,740
So market decision process or something like that and I think our work isn't contrary to that in the sense of like

758
00:54:59,540 --> 00:55:05,620
You know, I do think that that probably is a sufficient framework to be able to model any any kind of behavior that we might want an agent to

759
00:55:05,620 --> 00:55:10,100
Do but I think when it comes to actually like practically implementing that idea. There's a lot of difficulties

760
00:55:10,500 --> 00:55:15,540
So the first one might be um, you know, how do we even specify that reward function?

761
00:55:15,860 --> 00:55:18,180
so, you know, if the reward function is to um

762
00:55:18,820 --> 00:55:21,380
Have a good life or something like this like there's obviously like

763
00:55:21,940 --> 00:55:25,780
You know, maybe there is some like numerical way of defining that in terms of an mdp

764
00:55:26,580 --> 00:55:32,100
But there's like not actually a good way of writing down that function that maps what I do to whether I'm getting good rewards

765
00:55:32,420 --> 00:55:36,500
And so I think there's this kind of like, you know, I think that's a good framework for like thinking about any problem

766
00:55:36,740 --> 00:55:40,340
But then you have these kind of like practical issues of how do you actually define rewards?

767
00:55:40,340 --> 00:55:42,340
And how do you how do you say like?

768
00:55:42,580 --> 00:55:47,460
Were there an agents doing well and not doing well and things like this? Um, and so I think that's still um

769
00:55:48,020 --> 00:55:51,540
Even with the world models lines of work. I think that's still like kind of quite a difficult issue

770
00:55:51,940 --> 00:55:56,900
So so so the world models lines of work kind of, you know, allow you to model, you know, predicting ahead in the environment

771
00:55:57,460 --> 00:55:59,700
Which is a very useful thing for doing a lot of tasks

772
00:56:00,340 --> 00:56:02,980
Um, but then if you actually want to optimize some specific task

773
00:56:03,380 --> 00:56:05,780
You still have this problem of like, how do you define the reward?

774
00:56:06,020 --> 00:56:09,940
And so we eventually want to get to this point of being able to like inject a reward into the world model

775
00:56:10,020 --> 00:56:12,980
So we're kind of in agreement with that kind of line of thinking in a sense

776
00:56:13,060 --> 00:56:16,980
We're eventually going to use a reward to derive the the the desired intelligent behavior

777
00:56:16,980 --> 00:56:18,660
So I don't think there's any conflict in that sense

778
00:56:18,900 --> 00:56:22,740
But we still have this kind of problem of how do we inject that reward into the the world model?

779
00:56:22,740 --> 00:56:24,740
How do we define what that reward should be?

780
00:56:24,980 --> 00:56:26,980
um and the case of um

781
00:56:27,380 --> 00:56:29,380
You know one of the easiest things to do for example

782
00:56:29,460 --> 00:56:32,340
Would just be to label each image with reward and then you can kind of

783
00:56:32,660 --> 00:56:37,300
Encode that image into the latent space of the world model and then use that to define how good a certain thing is

784
00:56:37,620 --> 00:56:40,020
And that's kind of the style of thinking what we think of in our work

785
00:56:40,500 --> 00:56:44,180
Um, but I don't think that overcomes this like overarching issue of in general

786
00:56:44,260 --> 00:56:49,060
It's you know rewards can define everything, but how do you in practice like get that function is pretty hard

787
00:56:50,180 --> 00:56:54,500
Yeah, I mean in a sense reward is enough is sort of a tontology because once you know the reward

788
00:56:54,580 --> 00:56:55,700
um

789
00:56:55,700 --> 00:56:57,700
If you know the reward function for your environment

790
00:56:57,860 --> 00:57:00,980
You can essentially compute the value function, which gives you the optimal policy

791
00:57:01,220 --> 00:57:01,540
um

792
00:57:01,540 --> 00:57:04,820
And so reward has to be enough if you know the reward function and so

793
00:57:05,460 --> 00:57:08,900
Uh, I think the more interesting question is definitely like what is enough for the reward?

794
00:57:09,140 --> 00:57:16,340
What is enough to actually have a system automatically figure out what are interesting new rewards for us to train new agents?

795
00:57:16,340 --> 00:57:22,020
We're new models on or continue training existing models on and I think this goes back to the question of environment design

796
00:57:22,020 --> 00:57:27,700
This is largely the motivation of that line of work this auto curricula environment design where essentially if we can automatically

797
00:57:27,780 --> 00:57:31,780
Weave through this path of possible environments of the design space of the environments

798
00:57:32,180 --> 00:57:38,660
The design space clearly will encompass like a big part of the design space is also encompassing the reward for those tasks

799
00:57:38,980 --> 00:57:44,500
And so essentially we want to find a curriculum automatic curriculum or path through the possible reward functions

800
00:57:44,820 --> 00:57:47,460
In which we can start to train a more and more general agent

801
00:57:47,700 --> 00:57:49,700
But then the interesting question is again

802
00:57:49,780 --> 00:57:55,940
Like what exactly is the right notion of interestingness in order to drive that curriculum that path through the design space

803
00:57:56,100 --> 00:58:01,300
of possible things we could be training our model or agent on and um, and that's I think

804
00:58:01,700 --> 00:58:07,060
One of the most interesting open questions and it relates to the question as well of how do we get the model to ask the questions?

805
00:58:07,700 --> 00:58:11,220
Because really what drives humans in terms of asking further questions

806
00:58:12,020 --> 00:58:17,220
Is our own implicit notion of interestingness which is informed by things like the scientific method and you know

807
00:58:17,300 --> 00:58:20,340
being able to create explanations about the world

808
00:58:20,740 --> 00:58:25,460
And we find things interesting when we can't actually explain some phenomenon about the world

809
00:58:25,940 --> 00:58:27,940
Based on existing theories or explanations

810
00:58:28,260 --> 00:58:33,700
And so I think what's really missing for a well-grounded, you know, human interpretable version of

811
00:58:34,180 --> 00:58:40,100
Interestingness is having models that can essentially come up with their own theories about the world and start to probe those theories

812
00:58:40,260 --> 00:58:43,620
For where there's mismatch between, you know, the their learned theory of the world

813
00:58:44,020 --> 00:58:47,700
And evidence that new evidence that they find from experiences in the world

814
00:58:48,020 --> 00:58:52,900
Yeah, it's so interesting and and um, I mean when I make the argument that agent should be physically and socially embedded

815
00:58:52,980 --> 00:58:58,100
It's it's actually quite a simple argument, which is just the guardrails. It's that interesting this thing

816
00:58:58,180 --> 00:59:05,060
I think that that is how, you know, having um, uh agency but with the guardrails of our physical and social embedding

817
00:59:05,140 --> 00:59:08,180
So, you know, we're we're sampling things that make sense because they're already there

818
00:59:08,420 --> 00:59:12,100
That, you know, but but but obviously we can go off piece to little bit as individual agents

819
00:59:12,340 --> 00:59:15,860
I I feel that that's what helps that process just coming back to Sutton

820
00:59:15,940 --> 00:59:18,100
It's entirely possible that I've misunderstood Sutton, by the way

821
00:59:18,100 --> 00:59:24,420
So my my interpretation of reward is enough and it might be true as you say that it's tautological given that if you already knew

822
00:59:24,740 --> 00:59:28,740
The reward function for a particular environment then it could do everything that it needed to do

823
00:59:28,820 --> 00:59:34,900
But my interpretation of reward is enough is that it would lead to um, a general intelligence and you know

824
00:59:34,980 --> 00:59:39,380
General in in the kind of magical sense that it would work in in any possible situation

825
00:59:39,700 --> 00:59:44,900
But if it is specialized in the way that we agreed earlier that there exists a reward function

826
00:59:44,980 --> 00:59:46,980
Which would inco you know codify

827
00:59:47,140 --> 00:59:52,740
Motifs and things that you know, you need to know or optimize in a particular environment or set of environments

828
00:59:53,140 --> 00:59:57,220
Then to me that's still specialized intelligence and I would agree. Yeah. Yeah, yeah

829
00:59:57,300 --> 01:00:02,260
That's that I think that aligns with my take as well where I think if you have a reward function

830
01:00:03,140 --> 01:00:05,140
It's already sort of applying

831
01:00:05,620 --> 01:00:09,220
Largely applies to at least the examples in that position paper about reward is enough

832
01:00:09,220 --> 01:00:12,100
It seems like most of the reward functions they discuss are largely

833
01:00:12,980 --> 01:00:14,980
Grounded in a specific task

834
01:00:15,060 --> 01:00:17,620
And I think that if you have the reward function for a specific task

835
01:00:17,940 --> 01:00:22,020
Then it definitely seems that you can have some optimization or learning algorithm

836
01:00:22,260 --> 01:00:25,540
That essentially learns to optimize that reward and therefore achieve that task

837
01:00:26,420 --> 01:00:28,500
So I do think sort of the open question that

838
01:00:28,980 --> 01:00:31,700
Uh, I think same reward is enough

839
01:00:31,780 --> 01:00:36,500
I think it kind of passes the buck up further one level to the question of where that reward comes from

840
01:00:36,660 --> 01:00:41,940
And I do think that having systems that can automatically design interesting new rewards. That seems like the frontier

841
01:00:42,180 --> 01:00:47,220
Yeah, I agree and and you know because to me intelligence is about discovering the knowledge and the knowledge is the reward function

842
01:00:47,220 --> 01:00:51,060
So if it was like kind of baking the knowledge in into the system, um, okay

843
01:00:51,060 --> 01:00:56,900
so another sort of galaxy brain take is um, I was talking to bishop about this the other day and um

844
01:00:57,620 --> 01:01:00,180
Do you think of like deep learning models as one model?

845
01:01:00,180 --> 01:01:06,420
Or do you think of them as a sort of like intrinsic ensemble of models because they they get they behave differently in an input sensitive way

846
01:01:06,740 --> 01:01:10,340
So, you know, like depending on the prompts you put into language into a language model

847
01:01:10,580 --> 01:01:15,620
You might find that like a different part of the weight space gets activated and and essentially it's like retrieving a mini program

848
01:01:15,860 --> 01:01:20,180
And that program is being run, but it's not it's not model building. It's like model

849
01:01:20,900 --> 01:01:22,900
Retrieving, but we would would you agree of that?

850
01:01:23,140 --> 01:01:24,180
Hmm

851
01:01:24,180 --> 01:01:26,180
I guess i'm not sure about that like

852
01:01:26,180 --> 01:01:28,500
Like within like subsets of a single homogeneous model

853
01:01:28,500 --> 01:01:32,500
But I guess the thing that I like to think about that's I think quite related to this is this idea of like

854
01:01:33,140 --> 01:01:36,740
And I think yamakun also kind of well a lot of people have laid out like a similar architecture

855
01:01:36,740 --> 01:01:41,300
It's like, you know, should we think of intelligent agents as having kind of like separate subsystems that can maybe

856
01:01:41,860 --> 01:01:45,060
Like be thought of as different neural networks. And so, you know, we could have like, you know

857
01:01:45,860 --> 01:01:49,060
Um, the standard notion of a policy which is like outputting actions

858
01:01:49,380 --> 01:01:52,820
And maybe we also want to have the notion of like a prediction model more like a world model that predicts

859
01:01:52,820 --> 01:01:57,540
What might go ahead in the world as well as maybe like a planner that is somehow good at like optimizing in that model

860
01:01:57,540 --> 01:02:01,860
And so we could kind of think of all these things as like separate subcomponents that we assume an intelligent

861
01:02:02,100 --> 01:02:03,700
You know an intelligent

862
01:02:03,700 --> 01:02:06,660
Thing would have like an intelligent thing should be able to predict ahead in the world

863
01:02:06,660 --> 01:02:08,660
It should also be able to output actions

864
01:02:08,660 --> 01:02:12,260
It should hopefully maybe be able to infer like why other things happened and things like this

865
01:02:12,740 --> 01:02:16,980
And so I guess as to whether we think that should you know be just like one homogeneous model

866
01:02:17,780 --> 01:02:21,380
For which maybe you query it and maybe you know different aspects of that model are kind of um

867
01:02:22,020 --> 01:02:25,700
You know handle different aspects of the query or whether we should think of those as separate components

868
01:02:25,700 --> 01:02:29,620
I'm not I'm not really sure as to whether it matters whether they're separate components or not because yeah

869
01:02:29,620 --> 01:02:32,580
I agree that you probably could just have like one massive model that does all these things

870
01:02:32,980 --> 01:02:35,140
And I think at least from the the trend that I've been seeing

871
01:02:36,340 --> 01:02:40,580
In kind of the world models literature and and also just like I guess the rl literature

872
01:02:40,580 --> 01:02:42,580
Or maybe just we should call it the foundation model literature

873
01:02:42,740 --> 01:02:46,820
Is you kind of don't want to have like a separate model that does the prediction for actions and a separate model

874
01:02:46,820 --> 01:02:50,020
That does the prediction observations like why not just have one massive model

875
01:02:50,260 --> 01:02:54,020
That's jointly trained to predict everything you might want to query and then depending on the different query

876
01:02:54,100 --> 01:02:59,780
You know it will just either predict an action or a predictive video sequence or it can be conditioned on actions or conditioned on language

877
01:03:00,020 --> 01:03:04,340
So I think in this sense like this kind of model like you said is more like just one massive model

878
01:03:04,340 --> 01:03:07,220
But it kind of has like a lots of different sub tasks that it's able to do

879
01:03:07,780 --> 01:03:08,340
um

880
01:03:08,340 --> 01:03:12,340
And so maybe this is actually like the more effective way of training a model because then you kind of get generalization

881
01:03:12,340 --> 01:03:14,340
Across these different sub tasks as well

882
01:03:14,500 --> 01:03:16,500
Well, yeah, and the reason I'm asking the question is um

883
01:03:17,140 --> 01:03:21,860
It seemed I mean like you know for for an outsider coming in it looks like statistics has broken

884
01:03:22,100 --> 01:03:25,140
You know in the olden days we used to talk about the no free lunch theorem used to say like you know

885
01:03:25,140 --> 01:03:27,620
You need to have specialized models for different situations

886
01:03:27,860 --> 01:03:30,100
And now the narrative is that we have generalist models

887
01:03:30,100 --> 01:03:34,260
We have foundation models and and they are better than the specialized models in a strong sense

888
01:03:34,660 --> 01:03:37,780
And you know and I like to sort of push on this a little bit and see well

889
01:03:37,860 --> 01:03:43,700
When when does it break because we know that there are like these physics inspired models with inductive priors that you know

890
01:03:43,780 --> 01:03:47,940
Know about invariance of you know like molecules and drug discovery and stuff like that

891
01:03:47,940 --> 01:03:50,260
And surely they would be better than a language model

892
01:03:50,260 --> 01:03:54,420
But no no no now they're training language models on mathematical conjecturing and like you know like

893
01:03:55,060 --> 01:03:59,700
Drug formulation using tokens and and so on so you know as an outsider you might just think well

894
01:03:59,700 --> 01:04:01,860
We can just use a big transformers model for everything

895
01:04:02,820 --> 01:04:06,260
I I think a lot of this does come from um well, so

896
01:04:07,220 --> 01:04:09,540
I think the attention-based transformer architecture is

897
01:04:10,100 --> 01:04:15,140
Proven empirically to just be highly scalable highly effective at learning lots of different kinds of data distributions

898
01:04:15,780 --> 01:04:19,300
But I think also part of it is just that we're just starting to enter this regime

899
01:04:19,300 --> 01:04:24,500
When we're just training these models on an insanely large amount of data, and I think that a lot of times

900
01:04:25,380 --> 01:04:29,780
We need to sort of take a step back and really consider the amazing performances on different tasks

901
01:04:30,180 --> 01:04:33,620
And really think about you know how much information was actually leaked into

902
01:04:34,340 --> 01:04:36,340
This task in the training data because

903
01:04:37,300 --> 01:04:39,300
Right now. We're really just training

904
01:04:40,020 --> 01:04:42,020
these huge models on

905
01:04:42,020 --> 01:04:45,700
I think I would say that we're largely training them on the test distribution in many cases

906
01:04:46,420 --> 01:04:49,700
I do there I have seen like lots of examples of

907
01:04:50,180 --> 01:04:53,540
Truly impressive behaviors from these models that that do seem like

908
01:04:54,340 --> 01:04:57,220
Truly novel like zero shot generalization to unseen tasks

909
01:04:57,700 --> 01:05:00,820
Like there was a recent example. I saw on twitter or someone

910
01:05:01,380 --> 01:05:04,580
Had like a very low resource like rare language and they gave it a few

911
01:05:04,980 --> 01:05:10,980
They gave I think the cloud 3 model a few examples and it was able to essentially perfectly reproduce new utterances in that language

912
01:05:11,540 --> 01:05:13,540
So that does seem very impressive

913
01:05:13,780 --> 01:05:19,140
But it does seem at the same time, you know a lot of the performances for example on elsat or like ap biology exams

914
01:05:19,300 --> 01:05:21,380
I imagine a lot of that is really a function of just

915
01:05:22,340 --> 01:05:27,780
Literally giving the model the test domain in terms of information during the training step

916
01:05:28,180 --> 01:05:29,220
Okay, okay

917
01:05:29,220 --> 01:05:32,260
So there are like two schools of thought on this when we talk about world models

918
01:05:32,260 --> 01:05:36,980
You know people are talking about sorrow and is it building a world model and and it certainly seems to be it seems to be doing

919
01:05:37,060 --> 01:05:38,420
I mean, obviously it's not doing navier stokes

920
01:05:38,420 --> 01:05:41,220
It's not doing like fluid dynamics, but it seems to be doing something like that

921
01:05:41,460 --> 01:05:45,140
So like one extreme view is that it is just a hash table

922
01:05:45,540 --> 01:05:52,180
And you know, it's it's kind of doing some diffused approximate retrieval or whatever another school of thought is that it's like a simulator

923
01:05:52,420 --> 01:05:57,220
And you know people talk about the simulator's view of large language models and you know, like it's like it's modeling

924
01:05:57,540 --> 01:06:00,660
Not only, you know, just to just just the words and the language

925
01:06:00,740 --> 01:06:04,500
But it's also implicitly learned to model the world and the people and all of us

926
01:06:04,980 --> 01:06:09,700
So that's the spectrum. I mean like uh mark, where do you think these things are on that spectrum?

927
01:06:09,940 --> 01:06:14,340
Yeah, I think we have like it would be great to be able to play around with it and kind of see what we can get out of it

928
01:06:14,420 --> 01:06:16,900
But I think I think if you can for example

929
01:06:17,860 --> 01:06:20,900
You know after each kind of you know, so it's a language condition model

930
01:06:20,980 --> 01:06:26,740
So if after each kind of frame you could for example put in a different language language kind of conditioning and say like

931
01:06:27,220 --> 01:06:32,820
You know, what happens here if you know, the mug was pushed off the table instead of whatever else was originally happening in the video

932
01:06:33,060 --> 01:06:37,380
And so if you can basically do this kind of like counterfactual like interventional predictions where you kind of

933
01:06:37,860 --> 01:06:41,940
Give some new action and then you're able to see like the alternative outcome of that new action

934
01:06:42,500 --> 01:06:48,340
I think if the model is able to do that then I would think that it does have a pretty good understanding of how the world works in the sense of

935
01:06:48,740 --> 01:06:54,100
You know, I really think like if you can predict the outcome of any action given some sequence of observations

936
01:06:54,100 --> 01:06:58,580
I do think that's a pretty good proxy for being able to say if you can do that you really do understand how the world works

937
01:07:00,980 --> 01:07:02,660
And so I think if the model can do that

938
01:07:02,660 --> 01:07:06,820
I would be kind of inclined to say that it does have a kind of world model in the sense of understanding

939
01:07:07,220 --> 01:07:09,540
The underlying world but then there might also be a chance that you know

940
01:07:10,340 --> 01:07:15,700
You know these models aren't like you said it's more just like a diffuse retrieval and perhaps if you try and do like a very

941
01:07:16,100 --> 01:07:19,540
Fine grain conditioning on a slightly different outcome different like conditioning

942
01:07:19,540 --> 01:07:22,260
Maybe it won't actually give you the correct kind of counterfactual prediction

943
01:07:22,660 --> 01:07:27,460
And so I think maybe we'd have to see how good these models are at generalizing to slightly different inputs and things like that

944
01:07:27,700 --> 01:07:32,500
To really see if it understands things well, or it is just like kind of generating some arbitrary video

945
01:07:32,740 --> 01:07:37,700
Yeah, I think it's a double whammy because our colloquial use of language and like you know use of models and intelligence

946
01:07:37,700 --> 01:07:42,980
It's so static that like, you know, we we um, we think of that as being intelligence

947
01:07:43,380 --> 01:07:46,980
But but we're still going like we're now create we're creating knowledge right now

948
01:07:46,980 --> 01:07:49,940
We're creating models because we're exploring we're doing exactly what you said Minshew

949
01:07:49,940 --> 01:07:53,780
We're like we're exploring the search space and we're building models and we're combining them together

950
01:07:54,100 --> 01:07:57,780
And you know, presumably we would diverge quite quickly from from from the language models

951
01:07:57,780 --> 01:08:02,900
But I mean what what's your take on on this idea that they are, you know, potentially world simulators? Yeah

952
01:08:03,300 --> 01:08:08,740
um, so just regarding the the sort of lookup analogy for these large models, I think it's

953
01:08:09,460 --> 01:08:15,060
So my mental model is similar to that. Um, although I think it's it's very close to um, I think a really good write-up of

954
01:08:15,620 --> 01:08:17,620
of the of this alternative take

955
01:08:18,100 --> 01:08:19,380
Which is more like

956
01:08:19,380 --> 01:08:21,940
There's an alternative take which is that it is kind of like a lookup table

957
01:08:22,180 --> 01:08:28,420
But the prompt itself is a key that maps not to a specific sort of response, but to potentially like a function

958
01:08:28,740 --> 01:08:32,660
Yeah, and a vast space of functions and france wash relay had a really good

959
01:08:33,220 --> 01:08:36,180
Sort of blog post where he kind of goes more into the details of this viewpoint

960
01:08:36,340 --> 01:08:42,420
But I think that that really, you know resonates with my intuition of how these things behave where it's not literally looking up like

961
01:08:42,660 --> 01:08:44,660
A key value in a hash table

962
01:08:44,740 --> 01:08:49,140
It seems more like it's these models have learned over tremendous amounts of data to compress that data

963
01:08:49,140 --> 01:08:54,260
They have to learn, I think more abstract functions that help to explain that data and therefore they're learning functions

964
01:08:54,420 --> 01:08:56,420
So they're approximating some kind of function

965
01:08:56,660 --> 01:08:58,180
Or a vast family of functions

966
01:08:58,500 --> 01:09:02,260
And I think the prompt really acts like as a key that essentially activates a particular function

967
01:09:02,500 --> 01:09:08,100
And so you can kind of think of you know in the classical world where one neural network equals one function like basically it's mapping from

968
01:09:08,500 --> 01:09:13,060
Images to image net labels now like foundation model in the foundation model regime

969
01:09:13,140 --> 01:09:18,260
It's like one foundation model is essentially kind of like a giant database of lots and lots of different functions

970
01:09:19,300 --> 01:09:22,340
That's basically activated selectively based on the input with prompt

971
01:09:22,820 --> 01:09:25,380
Um, and I do think that you know based on this

972
01:09:25,460 --> 01:09:29,780
I think it's definitely possible that with enough data from the world enough experiential data

973
01:09:30,020 --> 01:09:36,420
That these foundation models can learn sort of a basis set of dynamics and transitions that explain how the world works

974
01:09:37,060 --> 01:09:42,980
And essentially if it does learn these transitions, um, for example in like the massive amount of video data that swore is trained on

975
01:09:43,220 --> 01:09:48,660
Um, I would say that yeah, I would agree that they are essentially starting to approximate, uh world models

976
01:09:49,060 --> 01:09:51,860
Sure. Yeah, so yeah, these are two um separate papers. So

977
01:09:52,420 --> 01:09:59,620
So the first one being dreamer led by like Dan and jar Haffner. So this is um, you know example of work in the space of world models and so

978
01:10:00,580 --> 01:10:03,620
Basically what dreamer involves doing is like a way of training a world model

979
01:10:03,860 --> 01:10:10,820
And then also showing that you can just generate synthetic data in the small model and then optimize decision making like purely using the synthetic data

980
01:10:11,540 --> 01:10:12,740
um

981
01:10:12,740 --> 01:10:18,580
So we talked a little bit earlier about like partially observable mdps. So we want to like take kind of the sequence of observations

982
01:10:19,380 --> 01:10:24,100
Um, and then be able to predict like the next a distribution of the next observation given some action

983
01:10:24,900 --> 01:10:28,500
and so we also talked about how you might want to like compress this into like a um

984
01:10:29,300 --> 01:10:31,860
More compressed representation of your of the previous observation

985
01:10:31,940 --> 01:10:37,460
So basically what dreamer proposes to do and a lot of works on world modeling is to take your previous sequence of observations

986
01:10:37,860 --> 01:10:40,100
And then you map them to some compressed representation

987
01:10:40,900 --> 01:10:44,740
And then could predict ahead in this latent space. Um, the next uh, latent

988
01:10:45,540 --> 01:10:50,740
Latent state condition on the action and then yeah, the really interesting thing about this is that now, um, you know

989
01:10:51,220 --> 01:10:54,420
We can in general predict what's going to happen to condition on different actions

990
01:10:54,660 --> 01:10:57,700
So now if you want to get like interesting behavior out of something like dreamer

991
01:10:57,780 --> 01:11:01,540
You can then go ahead and generate a lot of synthetic data using dreamer

992
01:11:02,260 --> 01:11:05,380
Or the dreamer world model and then use that to optimize behavior

993
01:11:05,780 --> 01:11:10,420
And so in dreamer basically the way it's done is by doing like on policy reinforcement learning in the world model

994
01:11:10,500 --> 01:11:13,380
So a lot of people call this like reinforcement learning and imagination

995
01:11:13,700 --> 01:11:19,940
So it's basically, you know, you're imagining a bunch of synthetic data then using that to like use some standard reinforcement learning algorithm and then optimize

996
01:11:21,060 --> 01:11:22,580
behavior in some sense

997
01:11:22,980 --> 01:11:25,860
And then you could also do other things like Monte Carlo tree search

998
01:11:25,940 --> 01:11:28,980
Which is like closer to like the works on on mu zero and things like this

999
01:11:29,540 --> 01:11:35,300
Creativity is a little bit like a cloud and all the creativity only happens on the surface of the cloud

1000
01:11:35,540 --> 01:11:39,940
So there's this interesting thing that like creative discovery depends on the history of all the things that I discovered before

1001
01:11:40,180 --> 01:11:44,020
And typically like new discovery only happens at the end of the chain not back in in the middle

1002
01:11:45,060 --> 01:11:48,980
Exactly and and there's also this notion that creativity happens through knowledge

1003
01:11:49,060 --> 01:11:52,420
So like knowledge new knowledge doesn't come from the ether. It's kind of

1004
01:11:53,380 --> 01:11:56,020
There's some creative component to it, but it's it's on the

1005
01:11:56,900 --> 01:12:02,180
The the trodden path of existing knowledge that we already have. Yeah, that wasn't a very good question

1006
01:12:02,500 --> 01:12:06,020
But you see I mean so so when we talk about imagination through like, you know

1007
01:12:06,100 --> 01:12:09,140
Like reinforcement learning policies and and so on what we're saying is like, you know

1008
01:12:09,220 --> 01:12:12,820
I'm I'm imagining all of these like possible, you know worlds and so on

1009
01:12:12,980 --> 01:12:17,140
But I'm using the cognitive primitives of all of the stuff that I already know

1010
01:12:18,020 --> 01:12:20,820
Yeah, I think knowledge is definitely a compounding

1011
01:12:21,780 --> 01:12:23,780
compounding

1012
01:12:23,780 --> 01:12:25,620
artifact

1013
01:12:25,620 --> 01:12:29,700
That's basically like the culmination of everything all the experiences that we

1014
01:12:30,660 --> 01:12:34,420
That we encounter like throughout our whole life and through also like beyond, you know

1015
01:12:34,500 --> 01:12:42,180
Going backwards beyond like even our individual lives into like the cultural knowledge that's shared and what's really cool about language models

1016
01:12:42,900 --> 01:12:44,900
is that they are essentially a

1017
01:12:44,900 --> 01:12:47,220
codification of cultural knowledge and so

1018
01:12:47,620 --> 01:12:53,300
Jeff Klune has this concept of AI generating AI and so he's got multiple pillars of essentially what it takes for

1019
01:12:53,940 --> 01:12:55,940
You to have AI systems that generate

1020
01:12:56,020 --> 01:13:01,060
General AI systems and he recently added actually like as a fundamental piece of this in in his framework

1021
01:13:01,380 --> 01:13:03,540
This idea of building on top of foundation models

1022
01:13:03,940 --> 01:13:07,460
And so he says he calls it like standing on the shoulders of giant foundation models

1023
01:13:08,100 --> 01:13:12,980
Which is I think really just sort of the ml equivalent of building on top of cultural knowledge

1024
01:13:13,460 --> 01:13:15,860
There's there's a real shift recently towards talking about

1025
01:13:16,340 --> 01:13:18,020
um synthetic data

1026
01:13:18,020 --> 01:13:21,380
And as we were just saying like, you know synthetic data, it doesn't come from the ether

1027
01:13:21,700 --> 01:13:28,740
So we already know stuff about the world. We we build simulators and we kind of generate new

1028
01:13:29,300 --> 01:13:35,860
Information but in the neighborhood of things that we already know and then we kind of like iterate and fine-tune on the generated data

1029
01:13:36,340 --> 01:13:37,700
um

1030
01:13:37,700 --> 01:13:39,540
What what do you think about that process?

1031
01:13:39,540 --> 01:13:43,940
Yeah, no, I think yeah, maybe I'll bring it back to this like the plan to explore line of work. So, yeah

1032
01:13:44,900 --> 01:13:48,180
um, so so basically like the motivation of that kind of work is like

1033
01:13:48,740 --> 01:13:51,700
Kind of saying, you know, we might have some like previous data set or something

1034
01:13:51,700 --> 01:13:53,620
And we've trained our world model on that data set

1035
01:13:53,780 --> 01:13:57,700
But we really want to go out and like gather more data and then like improve the world model

1036
01:13:58,580 --> 01:13:59,540
um

1037
01:13:59,540 --> 01:14:01,540
By gathering more data

1038
01:14:01,540 --> 01:14:06,020
And so we can use things like intrinsic motivation to then give us like a reward signal within the world model

1039
01:14:06,340 --> 01:14:09,620
So in the sense of something like prediction error, which mentioned earlier

1040
01:14:09,620 --> 01:14:14,100
So now we can basically like train a policy in the world model that's now not trained for a specific task

1041
01:14:14,420 --> 01:14:17,380
But it's trained to go out and gather information in the world

1042
01:14:18,100 --> 01:14:21,700
So basically now, you know, you do this imagining in the world model to imagine ahead

1043
01:14:21,860 --> 01:14:24,900
But instead of imagining ahead, how do I do a task? Well, you're imagining ahead

1044
01:14:25,140 --> 01:14:31,540
How do I get to states that I don't know what happens and therefore we'll learn more and that's basically like the motivation behind plan to explore

1045
01:14:32,580 --> 01:14:34,580
um, and then and our um

1046
01:14:34,740 --> 01:14:39,780
Paper waker it's it's kind of like inspired by plan to explore as well as works on like auto-curricular

1047
01:14:40,580 --> 01:14:42,580
and so basically what we're trying to say is

1048
01:14:43,220 --> 01:14:47,060
You know plan to explore is good for for getting an agent to go out and gather data

1049
01:14:47,300 --> 01:14:52,500
Um within a single environment and you know and presumably once you've gathered enough data within a single environment

1050
01:14:52,500 --> 01:14:55,620
Then you can generate a bunch of synthetic data in that single environment

1051
01:14:56,020 --> 01:15:00,900
And then do what we discuss with dreamer in terms of like optimizing a policy for that very specific environment

1052
01:15:01,780 --> 01:15:04,020
um, but what we're really interested in is saying, you know

1053
01:15:04,660 --> 01:15:07,620
Let's not assume that we have like one specific environment beforehand

1054
01:15:07,860 --> 01:15:11,700
Let's assume that you know, there's some space of you know broad range of scenarios

1055
01:15:11,780 --> 01:15:13,380
Like we want a very like general agents

1056
01:15:13,380 --> 01:15:17,300
There might be a bunch of different environments and then within that what those different environments

1057
01:15:17,300 --> 01:15:20,260
We kind of want to be able to to handle absolutely any task

1058
01:15:21,140 --> 01:15:26,580
And so in the waker paper, we're basically saying like, you know, how should we gather the data within um

1059
01:15:27,300 --> 01:15:32,420
Within this like broad space of possible environments and tasks such that we can train a very good world model

1060
01:15:32,820 --> 01:15:36,900
And then once we have that world model, that's kind of like capable across environments and tasks

1061
01:15:37,220 --> 01:15:41,300
You know the assumption is that we can then use that to generate good synthetic data, which we can then

1062
01:15:41,700 --> 01:15:43,700
Um use to optimize behavior

1063
01:15:44,580 --> 01:15:47,620
And so maybe to talk a little bit about like how we formalize this problem

1064
01:15:48,020 --> 01:15:51,780
Um, so, you know, we mentioned earlier this idea of like the simulation lemma

1065
01:15:51,860 --> 01:15:56,260
So we basically say that or an existing work that says like in a single environment

1066
01:15:56,580 --> 01:15:59,380
We can bound the gap between the optimal policy

1067
01:15:59,860 --> 01:16:04,580
That's trained in the world model so trained in the synthetic data to the to the truly optimal policy

1068
01:16:04,980 --> 01:16:08,820
By the error in the world model and the distribution of states generated by that policy

1069
01:16:09,460 --> 01:16:14,500
So it's kind of intuitive like the world model should have, you know, low error and then we will get a good policy out of it

1070
01:16:15,220 --> 01:16:19,700
But then what we're trying to say is like now, let's assume we don't know what the environment is beforehand

1071
01:16:19,700 --> 01:16:22,100
And we also don't know what the task is beforehand

1072
01:16:22,340 --> 01:16:25,940
So how do we get like a good world model that can handle like all of those situations?

1073
01:16:26,660 --> 01:16:28,900
When we later want to go ahead and optimize some task

1074
01:16:29,620 --> 01:16:30,740
um

1075
01:16:30,740 --> 01:16:32,740
And so the way that we do this is we basically yeah

1076
01:16:33,220 --> 01:16:39,540
We then use this notion of mini max regret to say that the policy should have like low maximum regret across this hot entire space of environments

1077
01:16:40,100 --> 01:16:42,580
And then using the simulation lemma we can basically say now

1078
01:16:43,220 --> 01:16:47,300
Now the um the world model has to have low error across all environments

1079
01:16:47,940 --> 01:16:53,220
Under the distribution of states generated by the optimal policy for any future task

1080
01:16:53,860 --> 01:16:57,700
Um, so we're going to say like yeah, the world model has to be good for any environment and

1081
01:16:58,260 --> 01:17:02,100
Under, you know in any area that the policy might go to that's relevant to the future tasks

1082
01:17:03,140 --> 01:17:06,740
And then what we kind of say in the paper is, you know, if we want a truly general agent

1083
01:17:06,980 --> 01:17:09,300
We're not going to know what the distribution of tasks is beforehand

1084
01:17:09,300 --> 01:17:12,900
So we don't know we don't know what the reward function is. We don't have a set of reward functions

1085
01:17:13,620 --> 01:17:17,380
Um, you know, we're just going to kind of assume the agent has to do anything later down the line

1086
01:17:17,380 --> 01:17:20,900
And this is kind of like related to this idea of like open-endedness that we've talked a lot about

1087
01:17:21,780 --> 01:17:25,060
And so if we don't know what the task is going to be like later down the line

1088
01:17:26,020 --> 01:17:30,740
Then the best assumption we can do is say that, you know, it could be any reward function later down the line

1089
01:17:31,380 --> 01:17:35,540
Which is maybe not the best assumption because as we talked a bit earlier if you're just kind of

1090
01:17:36,180 --> 01:17:39,300
You know, we talked about a bit about intrinsic motivation and interestingness

1091
01:17:39,620 --> 01:17:42,660
And if you kind of assume the task can be absolutely anything later down the line

1092
01:17:43,060 --> 01:17:47,300
You're kind of assuming that, you know, the agent might want to do something completely ridiculous later like it

1093
01:17:48,100 --> 01:17:52,340
If you do this in robotics, that might mean the task is just to do like backflips later or something like that

1094
01:17:52,340 --> 01:17:56,340
But you have no interest in doing that. So it's it's not clear if that's really a good assumption about

1095
01:17:56,740 --> 01:18:00,180
How we should think about what tasks might be interesting later, but that's the assumption we make

1096
01:18:00,180 --> 01:18:02,580
So we assume the task can be absolutely anything later down the line

1097
01:18:04,340 --> 01:18:07,220
So so now we have to get a to the point where we have the world model

1098
01:18:07,700 --> 01:18:13,140
Which is good for any environment and under the distribution of states generated for any task or any optimal reward function

1099
01:18:15,300 --> 01:18:18,340
And to do this we basically like leverage two different techniques

1100
01:18:19,300 --> 01:18:21,300
So to generate this state um

1101
01:18:21,620 --> 01:18:25,940
So to handle the aspect that we don't know what the task is later down the line. We assumed that um

1102
01:18:26,820 --> 01:18:31,940
We have an intrinsically motivated policy that's basically seeking out the maximum uncertainty in any single environment

1103
01:18:32,500 --> 01:18:34,500
And so basically if if this um

1104
01:18:34,980 --> 01:18:38,900
If this intrinsically motivated policy is seeking out the maximum uncertainty in every environment

1105
01:18:39,380 --> 01:18:46,020
Um, it's kind of like estimating for us what the maximum uncertainty is in every environment because it's like actively finding uncertainty in every environment

1106
01:18:46,820 --> 01:18:50,180
So now we have a policy that's finding like the maximum uncertainty in every environment

1107
01:18:50,820 --> 01:18:54,420
And then if we want to optimize this like mini max criterion across environments

1108
01:18:54,820 --> 01:18:58,260
We kind of need the maximum uncertainty to be low across all environments. So

1109
01:18:58,980 --> 01:19:04,340
So we kind of have to have like um, you know, this policy isn't able to find like lots of big errors across all different environments

1110
01:19:05,300 --> 01:19:06,260
um

1111
01:19:06,260 --> 01:19:07,220
And so

1112
01:19:07,220 --> 01:19:11,540
Basically, you know, what we could think like what what happened in practice is, you know

1113
01:19:11,540 --> 01:19:13,140
You can imagine there are a bunch of different environments

1114
01:19:13,140 --> 01:19:16,020
Some which are like a low complexity and some of which are high complexity

1115
01:19:16,900 --> 01:19:20,420
And if we just kind of naively sample from those two different environments data, you know

1116
01:19:20,500 --> 01:19:23,300
Our world model is going to very quickly get good at the low complexity environment

1117
01:19:23,940 --> 01:19:29,060
And then it's going to leave a lot more data from that high complexity environment to eventually get the errors low in the high complexity environment

1118
01:19:29,940 --> 01:19:34,660
So to bring it back to the title of the paper, which is weighted acquisition of knowledge across environments for a bussiness

1119
01:19:34,980 --> 01:19:37,140
So the idea here is that we're basically going to

1120
01:19:38,100 --> 01:19:44,180
Change how we sample that distribution of data across environments to make sure that maximum uncertainty stays low across environments

1121
01:19:44,580 --> 01:19:49,860
So what this ends up looking like is, you know, we're going to sample less data from the environment that has lower complexity

1122
01:19:50,500 --> 01:19:54,340
And then we're going to actively sample more data from the environment that has higher complexity

1123
01:19:54,660 --> 01:19:57,620
Such that we we bring those errors down on the higher complexity environments

1124
01:19:58,180 --> 01:20:03,540
And I guess it's a little bit different to existing works on curricula because normally in curricula like automatic curriculum learning

1125
01:20:04,180 --> 01:20:06,100
You kind of assume that you have some reward function

1126
01:20:06,100 --> 01:20:12,740
Which is telling you how well the policy is doing in each environment and use use that specific like metric of how well the policy is doing

1127
01:20:13,540 --> 01:20:17,060
To determine, um, you know, where the policy has more potential to learn

1128
01:20:17,380 --> 01:20:20,580
But because we're making this assumption that, you know, we don't know what the reward function is

1129
01:20:20,740 --> 01:20:23,860
We're trying to get a general agent that can kind of do any task any reward function

1130
01:20:24,740 --> 01:20:27,700
Um, we don't assume that we know that reward function beforehand

1131
01:20:27,700 --> 01:20:31,620
So we can't use reward as a metric of saying like I need more data from here or I need more data from here

1132
01:20:32,420 --> 01:20:35,540
But then kind of the main argument of the paper is showing that, you know

1133
01:20:35,620 --> 01:20:38,820
If we just think about this in terms of prediction error in the world model

1134
01:20:39,540 --> 01:20:43,140
Like we can actually use that as like an intrinsic motivation signal to say, you know

1135
01:20:43,220 --> 01:20:48,180
Does the agent need to gather more data from this environment or from this environment without access to reward function

1136
01:20:48,740 --> 01:20:50,740
and so we could kind of think of um

1137
01:20:51,060 --> 01:20:57,300
This work as kind of a more general approach to automatic curriculum learning in the sense of like we're not assuming that you have a reward function beforehand

1138
01:20:57,300 --> 01:20:59,300
We're kind of agnostic to what the task is

1139
01:21:00,100 --> 01:21:04,100
And because and to kind of distill that knowledge that's that's gathered without the reward function

1140
01:21:04,180 --> 01:21:06,980
We use the world model as a mechanism to like distill that knowledge

1141
01:21:07,300 --> 01:21:11,300
Because if you just like naively have an agent gathering information with no reward function

1142
01:21:12,340 --> 01:21:14,820
You know, how do you how do you kind of put that knowledge into the agent?

1143
01:21:14,820 --> 01:21:17,140
And we kind of argue the best way of doing that is the world model

1144
01:21:18,260 --> 01:21:22,980
So that's kind of a summary of like the waker paper and what like what the ultimate algorithm ends up doing

1145
01:21:23,540 --> 01:21:28,340
So I mean essentially you're doing a high entropy search. So you're you're leaning into

1146
01:21:29,140 --> 01:21:32,020
Areas of complexity and you're building a higher complexity model

1147
01:21:32,020 --> 01:21:36,500
Which goes against the grain of the intuition of like Occam's razor that should have simple models

1148
01:21:36,500 --> 01:21:42,020
So you're you're almost deliberately saying no, I want I want to model the the complexity and have more of that

1149
01:21:42,340 --> 01:21:45,620
And then the other interesting thing is like from from a curriculum learning point of view

1150
01:21:45,620 --> 01:21:50,420
I think traditionally we did explicit curriculum learning and you know, we might have some

1151
01:21:50,980 --> 01:21:54,580
Principles around having a monotonically increasing curriculum of complexity

1152
01:21:54,980 --> 01:21:56,980
Whereas here by leaning into

1153
01:21:57,940 --> 01:22:01,300
Environments where we do worse on so we're selecting them based on prediction error

1154
01:22:01,460 --> 01:22:06,740
We're actually implicitly getting a kind of monotonically increasing complexity, which just happens to work really well

1155
01:22:07,540 --> 01:22:11,460
Yeah, I guess actually it actually almost ends up being in the opposite direction

1156
01:22:11,460 --> 01:22:15,140
So so by leaning into the the the higher complexity environments more

1157
01:22:15,140 --> 01:22:18,420
We're kind of saying let's prioritize the harder environments more to begin with

1158
01:22:18,420 --> 01:22:21,700
So let's like gather more data in the higher complexity environments

1159
01:22:22,660 --> 01:22:27,140
Um, you know, because I guess intuitively if you kind of want to be good across all environments

1160
01:22:27,620 --> 01:22:30,180
You kind of need more data from the higher complexity environments

1161
01:22:30,500 --> 01:22:34,260
And we don't really explicitly think about an ordering of going first from easy to hard

1162
01:22:34,740 --> 01:22:37,540
Um, I guess that maybe there is a something to look into there because

1163
01:22:38,180 --> 01:22:39,060
You know

1164
01:22:39,060 --> 01:22:42,660
Like a lot of these works go from low complexity to high complexity because it's kind of easier to learn

1165
01:22:42,660 --> 01:22:47,540
An initial policy that can kind of do something in the low complexity environment and then you build up the complexity

1166
01:22:47,940 --> 01:22:52,100
Gradually, um, but I think that that idea is most useful when you know what the task is

1167
01:22:52,100 --> 01:22:54,980
So you could imagine if the task is like low commotion if it's walking

1168
01:22:55,380 --> 01:22:58,180
You kind of want to first learn a policy that's able to walk on flat ground

1169
01:22:58,260 --> 01:23:02,420
And then maybe gradually build up the complexity like add and bumps and then eventually it can walk on like a very

1170
01:23:03,060 --> 01:23:05,940
Complicated terrain so it kind of makes sense to go from low to high complexity

1171
01:23:06,500 --> 01:23:08,500
um, but in this work we're focusing on

1172
01:23:09,300 --> 01:23:13,540
purely intrinsic motivation meaning that the policy is not trying to learn a specific task

1173
01:23:13,860 --> 01:23:17,860
It's trying to just seek out um uncertainty and like reduce uncertainty

1174
01:23:18,260 --> 01:23:20,180
And so we don't really have the notion of you know

1175
01:23:20,180 --> 01:23:22,820
You first need to be able to learn how to do something on an easy

1176
01:23:23,380 --> 01:23:28,340
An easy environment and then move towards harder environments because there is no specific task that we're trying to learn

1177
01:23:28,660 --> 01:23:33,860
And so I think for this reason, you know, we wouldn't didn't really focus on this notion of moving from easier to harder environments

1178
01:23:33,860 --> 01:23:37,140
So that actually, you know, we're consistently something more data from the hard environments

1179
01:23:37,780 --> 01:23:41,700
And I guess I think this relates or I think this is something that you brought up when we when we worked on this is like

1180
01:23:42,420 --> 01:23:48,260
You know, I think we can really relate this idea to like a lot of different contexts including things like like language models, for example

1181
01:23:49,140 --> 01:23:50,020
um

1182
01:23:50,020 --> 01:23:54,740
So, you know, you can imagine if I'm training an llm. I don't really necessarily have this, you know

1183
01:23:54,820 --> 01:23:57,300
Not really a reward function in some sense. You're just trying to

1184
01:23:58,100 --> 01:24:00,100
Do like unsupervised prediction

1185
01:24:00,340 --> 01:24:07,060
And so, you know, we could for example take the prediction error of like a language model and a bunch of different domains and say, you know, the language model is

1186
01:24:08,020 --> 01:24:11,460
Not very good at predicting a language about some certain task or something like that

1187
01:24:11,940 --> 01:24:16,340
And you know, we could say, you know, and intuitively the same thing kinds of holds if it's not very good at predicting, you know

1188
01:24:17,380 --> 01:24:20,900
What the next token is in french like we should presumably gather more data in french

1189
01:24:21,460 --> 01:24:25,140
And that so that kind of gives us a way of like actively gathering the appropriate data

1190
01:24:25,620 --> 01:24:29,620
Um, and so yeah, I think this idea of like gathering more data based on certainty

1191
01:24:29,620 --> 01:24:32,340
Obviously is a very general idea like the idea of like active learning

1192
01:24:32,900 --> 01:24:34,500
Um, but we kind of like

1193
01:24:34,500 --> 01:24:38,260
Specialized that into thinking about how do we think about this in terms of the reinforcement learning setting?

1194
01:24:38,740 --> 01:24:42,900
It might be interesting to talk about as well like sort of because we looked at some of the metrics as well, right?

1195
01:24:42,900 --> 01:24:46,580
The environment complexity metrics. Yeah, we don't have the external notion of difficulty

1196
01:24:46,660 --> 01:24:53,140
But we we also did look at sort of the emergent, uh, curriculum. Yeah. Yeah. Yeah. Gotcha. Yeah, so I guess um

1197
01:24:53,780 --> 01:24:55,220
So it kind of depended on the environment

1198
01:24:55,220 --> 01:24:59,220
So in some environments, you just kind of got this like very straightforward behavior of like, you know

1199
01:24:59,300 --> 01:25:01,860
Consistently gather more data in the more complex environment

1200
01:25:02,660 --> 01:25:06,180
um, but because we're we're actively trying to gather data, um

1201
01:25:06,900 --> 01:25:12,900
Of the the environments for which the uncertainty is the highest kind of this curriculum could change over over the course of training

1202
01:25:12,980 --> 01:25:15,220
So so what happened in some of the other environments?

1203
01:25:15,220 --> 01:25:18,740
For example, is that initially all the environments are just like high uncertainty

1204
01:25:19,060 --> 01:25:24,420
Like there's like all environments are kind of misunderstood therefore like sample all environments like equally more or less

1205
01:25:24,580 --> 01:25:26,580
To just get a rough understanding

1206
01:25:26,740 --> 01:25:29,540
And then you know as as the model would improve on the simplest environments

1207
01:25:29,540 --> 01:25:33,380
Then we would see like more and more emphasis towards sampling the highest complexity environments

1208
01:25:33,700 --> 01:25:37,780
So I guess in that sense we would get something to more like kind of what you said in terms of like a standard curriculum

1209
01:25:38,020 --> 01:25:41,140
But a bit different in the sense of like initially everything is uncertain

1210
01:25:41,300 --> 01:25:43,300
So we're just going to sample everything uniformly

1211
01:25:43,780 --> 01:25:46,340
Um, but then we kind of get a better understanding of which of the environments

1212
01:25:46,420 --> 01:25:51,780
You know the uncertainty remains high on these higher complexity ones and those are the ones we need to like go out and gather more data

1213
01:25:52,100 --> 01:25:52,820
Yeah

1214
01:25:52,820 --> 01:25:54,100
I mean I can see this both ways

1215
01:25:54,100 --> 01:25:57,380
I mean certainly from like a Bayesian optimization point of view that there's something to be said for

1216
01:25:57,540 --> 01:26:02,340
Um, you know, this is where I'm uncertain going gather more data where where I have highest uncertainty

1217
01:26:02,820 --> 01:26:05,300
And uh, as you say like traditionally in curriculum learning

1218
01:26:05,540 --> 01:26:08,500
We are told that we need to have monotonic increasing complexity

1219
01:26:08,500 --> 01:26:12,500
But as you just said that's when we have a particular task in mind now neural networks

1220
01:26:12,500 --> 01:26:15,780
They're a little bit like a block of clay aren't they so you know, it starts off with

1221
01:26:16,260 --> 01:26:21,460
Abject complexity and then we do stand, you know, we do um stochastic gradient descent and we chip away at the clay

1222
01:26:21,540 --> 01:26:27,300
And we kind of build we sculpt a statue that that that we want to build and I'm just trying to get an intuition here

1223
01:26:27,300 --> 01:26:29,300
So like with this maximum entropy

1224
01:26:29,540 --> 01:26:31,540
Search, you know like high entropy search

1225
01:26:31,620 --> 01:26:33,060
What we're doing is is we're saying okay

1226
01:26:33,060 --> 01:26:38,660
Well, here are some complex models and these models must contain motifs that tell us a lot of information

1227
01:26:39,060 --> 01:26:40,900
It's a little bit like the elo algorithm in chess

1228
01:26:40,980 --> 01:26:44,740
You know, you actually get information gain when something surprising happened

1229
01:26:45,060 --> 01:26:49,700
So here's a big block of complexity and I'm going to try and infer

1230
01:26:50,260 --> 01:26:53,940
What the motifs are in that complexity that that explain the information that I'm missing

1231
01:26:54,180 --> 01:26:59,220
I think that a lot of this ultimately traces back to sort of there's like this like fundamental pattern

1232
01:26:59,620 --> 01:27:03,140
towards uh, I think that like ties a lot of these ideas around active

1233
01:27:03,620 --> 01:27:10,340
Um active experiment design or like active sampling, which is and all these autocurricular methods, which is you essentially want to devise

1234
01:27:10,900 --> 01:27:16,340
Uh, what you know nowadays we call a self supervised objective or self self supervised training algorithm

1235
01:27:16,740 --> 01:27:21,860
Um, where essentially you have the system essentially use signals. It produces itself

1236
01:27:22,340 --> 01:27:28,420
Um during the training or evaluation process in order to drive itself forward in terms of deciding what future data to train on

1237
01:27:28,740 --> 01:27:35,060
And so, you know, we sometimes call these kinds of systems autocurricular as well because it's automatically generating this curriculum of

1238
01:27:35,380 --> 01:27:39,220
Tasks to train on and I think the sort of like the fundamental connecting

1239
01:27:40,260 --> 01:27:44,020
Uh pattern here is just that this the signal that we use to drive the training

1240
01:27:44,180 --> 01:27:48,660
It's always going to be based on something like, uh, an uncertainty signal or, um

1241
01:27:49,060 --> 01:27:53,300
Going back to the open-endedness literature something like a classic notion of interestingness

1242
01:27:53,780 --> 01:27:58,580
And I think there's just a lot of different possible choices for this metric and so

1243
01:27:59,380 --> 01:28:01,380
One for example, we talked a lot about

1244
01:28:01,380 --> 01:28:02,260
Minimax regret

1245
01:28:02,260 --> 01:28:07,620
So regret could be one of these driving signals because it measures the existence of a performance gap and therefore

1246
01:28:07,700 --> 01:28:11,780
Probably an information gap as well in terms of learning to master those tasks with high regret

1247
01:28:12,260 --> 01:28:18,340
But also uncertainty is also another one it ties back to novelty because novel environments you will be more uncertain within

1248
01:28:18,740 --> 01:28:23,540
And so there's fundamentally lots of different sort of branches of these autocurricular that you could use

1249
01:28:23,540 --> 01:28:27,780
Depending on this search objective that you use to drive this exploration process

1250
01:28:28,900 --> 01:28:32,820
Can we contrast this to you know, like, um, large language models that they are self-supervised learning

1251
01:28:32,980 --> 01:28:38,340
So, you know, we do this self-supervised objective, you know, which is like, you know, typically predict in the next word

1252
01:28:38,580 --> 01:28:41,460
And it's a similar thing with, um, self-supervised, um image

1253
01:28:42,260 --> 01:28:49,300
Learning now the difference is with that is you're talking about a principled way of, you know, seeking specific information

1254
01:28:49,620 --> 01:28:54,900
You know with, um, let's say high entropy and that would lead to an implicit curricula

1255
01:28:55,220 --> 01:28:58,980
Whereas with language modeling language modeling, there is no implicit curricula

1256
01:28:59,220 --> 01:29:05,780
But I might argue that there kind of is because the way the model does this continual learning, um, it might regularize itself

1257
01:29:05,780 --> 01:29:09,860
So if you give it sort of surprising and weird information, the language model might just kind of brush it off

1258
01:29:10,020 --> 01:29:14,500
And if you reinforce things that it already knows then it's almost like a stream of channels, you know

1259
01:29:14,500 --> 01:29:18,420
It'll say, okay, you know go and go and pay attention to that. So it's almost like it's implicit

1260
01:29:18,660 --> 01:29:22,660
Yeah, and I would say that in some ways it's almost explicit in terms of how we design these systems

1261
01:29:23,220 --> 01:29:26,580
A lot of times like if you look at, for example, open ai's job listings

1262
01:29:26,740 --> 01:29:31,860
They're actually hiring specifically for experts in different domains to essentially create the next

1263
01:29:32,500 --> 01:29:36,500
Batch of supervised data to train or instruction tune their models on

1264
01:29:36,900 --> 01:29:41,060
For example, they hire biologists or they hire people with legal expertise to generate this data

1265
01:29:41,380 --> 01:29:47,220
And you can think of this essentially as a human steered or human driven version of this active sampling process, right?

1266
01:29:47,460 --> 01:29:54,340
Because essentially they know that the model tends to get high perplexity or they don't it doesn't perform as well on this domain of tasks

1267
01:29:54,500 --> 01:30:00,740
It doesn't get as high of an LSAT score as it could and so you can essentially, you know, it's it's beyond an algorithm at this point

1268
01:30:00,820 --> 01:30:06,100
Right, it's kind of the super algorithm where you have the system designers now also being part of the data collection process

1269
01:30:06,580 --> 01:30:08,580
and in a way

1270
01:30:08,580 --> 01:30:13,540
supervised learning is really just sort of one point in a continual learning process where, you know

1271
01:30:14,100 --> 01:30:18,500
Classically, we just looked at one step of this which is here's a batch of data train on that but really

1272
01:30:19,380 --> 01:30:23,460
Building machine learning systems, especially nowadays. Everything's in production. These are all live systems

1273
01:30:23,700 --> 01:30:27,540
You have to keep it up to date. You have to keep it continually generalizing to new knowledge

1274
01:30:28,820 --> 01:30:36,100
Like chat gpt or clod or gemini and so really it's sort of this pattern over and over again in sequence where you collect a batch of data

1275
01:30:36,500 --> 01:30:38,820
Train your model on that collect the next batch of data

1276
01:30:39,620 --> 01:30:41,620
You know continue training your model on that

1277
01:30:42,180 --> 01:30:48,660
And really you want to be selective about what the next batch of data is because obviously if you just retrain it on the previous batch of data

1278
01:30:49,220 --> 01:30:51,060
It's going to overfit to that data

1279
01:30:51,060 --> 01:30:56,980
Beyond a few epochs or it's not going to you know get as much novel information from it just because it's already trained on it

1280
01:30:57,220 --> 01:31:01,140
So you do want to selectively actively collect the data

1281
01:31:01,540 --> 01:31:06,020
And so I think we kind of almost explicitly already do this at a systems level

1282
01:31:06,820 --> 01:31:12,340
And I think the next frontier is really just having systems that self-improve in this way where they can start to guide

1283
01:31:12,660 --> 01:31:18,660
More of their own active data collection. I love this way of thinking about it. You know like gbt4 is a memetic intelligence

1284
01:31:18,660 --> 01:31:22,020
It's not just like you know a bunch of weights on on a on a server somewhere

1285
01:31:22,420 --> 01:31:26,020
And so you could argue, you know, there's this concept called graduate student descent

1286
01:31:26,020 --> 01:31:29,940
Which is what happens in academia or even as you just articulated with open ai

1287
01:31:29,940 --> 01:31:33,060
It's a little bit like an epic mechanical turk right where you know

1288
01:31:33,780 --> 01:31:34,900
They are monitoring the logs

1289
01:31:34,900 --> 01:31:40,100
They know when things go go badly and then they lean into it in the same way you are they go in higher experts

1290
01:31:40,100 --> 01:31:42,980
And they kind of like add more and more data in all of the holes

1291
01:31:43,140 --> 01:31:46,180
And eventually there are no more pockets of like abject failure

1292
01:31:46,180 --> 01:31:51,460
It just it just appears to work really well for everyone and people start to say that it's you know, generally intelligent

1293
01:31:51,700 --> 01:31:54,980
So yeah, so there's this interesting systems view of of intelligence

1294
01:31:55,220 --> 01:31:57,860
Yeah, it kind of starts to mimic just the scientific process in a way

1295
01:31:58,420 --> 01:32:02,260
Where we're sort of we were putting a lot of hope in the model to basically be able to distill

1296
01:32:02,820 --> 01:32:05,700
information from sort of the net news batch of data that we collect

1297
01:32:06,580 --> 01:32:08,820
You know that we know the model currently doesn't explain well

1298
01:32:09,060 --> 01:32:15,140
And we we we put a lot of faith and gradient descent in order to basically be able to come up with updates to the weights

1299
01:32:15,300 --> 01:32:21,140
That better explain that data. So we're kind of we're kind of already treating the system as almost like an automated

1300
01:32:21,860 --> 01:32:25,140
Scientist or an automated version of this like continual

1301
01:32:25,860 --> 01:32:28,420
process of creating theories and explanations about the world

1302
01:32:29,300 --> 01:32:31,220
But of course, you know

1303
01:32:31,220 --> 01:32:35,860
Humans are still much better at language models at doing this or large models at doing this

1304
01:32:35,860 --> 01:32:39,220
So I do think there clearly seems like a huge gap in terms of

1305
01:32:39,540 --> 01:32:44,260
Well, we still have work that needs to be done in order to build systems that can actually build much more robust theories

1306
01:32:45,140 --> 01:32:48,180
Based on like net do new data and even seeking that out as humans do

1307
01:32:48,580 --> 01:32:54,100
Interesting and certainly, you know in in this broader memetic intelligence. We are still the sources of agency

1308
01:32:54,820 --> 01:32:59,300
But um, we were just sort of talking a minute ago about there being two types of ai

1309
01:32:59,380 --> 01:33:02,980
You know, there's there's an ai where we are the generating sources of agency

1310
01:33:02,980 --> 01:33:07,620
But there might potentially be another ai in the future where that that is the generating source of agency

1311
01:33:09,220 --> 01:33:11,700
Yeah, I so I think that um

1312
01:33:12,340 --> 01:33:14,820
This kind of ties into my my the framework

1313
01:33:14,820 --> 01:33:17,380
I personally used to think about open-ended systems as well

1314
01:33:18,020 --> 01:33:23,780
Where I think that you know at a high level you can you can study ai sort of in silico

1315
01:33:23,860 --> 01:33:30,420
You can study it in systems that you control that you design and that you try to like have the ai model self-improve within

1316
01:33:30,740 --> 01:33:32,740
And so you can try to build

1317
01:33:32,980 --> 01:33:38,100
Systems that self-improve within silico and that's going to lead to potentially some issues around like the grounding problem

1318
01:33:38,260 --> 01:33:44,900
Where essentially it starts to the auto the auto curricular exploratory process starts to veer into parts pockets of the design space

1319
01:33:44,900 --> 01:33:47,060
That are not relevant to tasks you care about

1320
01:33:47,460 --> 01:33:52,020
Um, and so this is kind of the danger of like generating open-ended systems in silico

1321
01:33:52,100 --> 01:33:55,860
And I think it's very similar to potential dangers of generating agi in silico

1322
01:33:56,260 --> 01:33:57,300
um

1323
01:33:57,300 --> 01:34:01,700
And I think the alternative is really just what are existing intelligent systems

1324
01:34:02,100 --> 01:34:07,860
And how do we actually amplify the efficiency the efficacy of those systems the intelligence within those systems?

1325
01:34:08,100 --> 01:34:14,180
And so you can kind of think of like sort of the entire enterprise of ai research as do we want to generate like ai or intelligence from scratch

1326
01:34:14,500 --> 01:34:16,500
Or do we want to build tools?

1327
01:34:16,500 --> 01:34:21,300
You know motivated or inspired by human intelligence and other intelligent systems and use that to further amplify

1328
01:34:21,620 --> 01:34:24,260
existing intelligence like human creativity human intelligence

1329
01:34:24,740 --> 01:34:28,180
Could you argue because if intelligence is a divergent search process?

1330
01:34:28,900 --> 01:34:32,900
You might be tempted to think that well if we had loads of tools to help us share

1331
01:34:33,220 --> 01:34:38,500
The models and help other people discover the models that i've created that that will help us generally be more intelligent

1332
01:34:38,660 --> 01:34:45,380
But could you make the counter argument that i'm actually sequestering agency or stealing agency from other people because rather than thinking for themselves

1333
01:34:45,620 --> 01:34:48,020
And discovering novel models. They're just going to use my model

1334
01:34:48,420 --> 01:34:53,300
Yeah, I mean I think that in the best case scenario you're building systems that essentially, you know

1335
01:34:54,100 --> 01:35:00,340
Not you know to to think about how you know as existing systems nowadays can build on the shoulders of foundation models

1336
01:35:00,580 --> 01:35:06,740
You really want the to build models where even humans can stand on their shoulders where the humans can basically leverage the

1337
01:35:07,060 --> 01:35:08,420
existing expertise or

1338
01:35:08,420 --> 01:35:14,180
Automative capabilities of those models to then like move further beyond what they're naturally capable of doing

1339
01:35:14,500 --> 01:35:18,740
And really that pushes the frontier of the knowledge that we can create as a civilization

1340
01:35:18,980 --> 01:35:26,020
And so you're already starting to see this where there's some recent studies that show for example like junior software engineers that use systems like

1341
01:35:26,580 --> 01:35:28,900
Chat gpt to help them with coding at work

1342
01:35:29,140 --> 01:35:32,740
They actually now are starting to match the performance of more senior engineers

1343
01:35:33,300 --> 01:35:34,900
Because it sort of levels the playing field

1344
01:35:35,060 --> 01:35:40,980
But that also translates into just like net more productivity per software engineer. And so

1345
01:35:41,940 --> 01:35:44,100
I think that it's more just unlocking sort of

1346
01:35:44,740 --> 01:35:49,940
Existing bottleneck and how productive each individual can be and really just means that each individual can create a lot more value

1347
01:35:50,020 --> 01:35:52,020
Can discover a lot more knowledge

1348
01:35:52,340 --> 01:35:53,540
Than before

1349
01:35:53,540 --> 01:35:57,780
Okay, but I mean do you think that it creates a tendency towards boilerplate though

1350
01:35:57,780 --> 01:36:00,660
So we're more we're more efficient at doing things that exist

1351
01:36:01,060 --> 01:36:04,420
But you know like on on the frontier we might have a slowdown

1352
01:36:04,580 --> 01:36:08,020
There's definitely the danger that it can lock you in to certain patterns

1353
01:36:08,180 --> 01:36:12,500
Right. So basically if chat gpt always returns a certain boilerplate that might have an anti pattern in it

1354
01:36:12,820 --> 01:36:19,700
Um, if that stays around it could self-amplify and then future generations of programmers might just adopt that by default because it's what's already

1355
01:36:20,100 --> 01:36:24,420
Generated by autocomplete. So I think that that's also another really interesting realm of questions

1356
01:36:24,500 --> 01:36:29,940
Which is basically how do you um, how do you avoid these kinds of uh, these local optima?

1357
01:36:30,340 --> 01:36:33,300
When you start to train a model on its own outputs

1358
01:36:33,620 --> 01:36:38,420
And I think again like sort of the solution will start to look like some form of novelty search or exploration

1359
01:36:38,980 --> 01:36:43,860
Makes sense. Okay. Um, what do you guys think about like, um, you know academic academia versus industry and

1360
01:36:44,580 --> 01:36:48,980
Some say there's a bit of a brain drain from academia. Totally. Yeah, I think there's like a very

1361
01:36:49,700 --> 01:36:53,780
Very clear trade-off between the two and they said they both have like fantastic things going for them

1362
01:36:54,260 --> 01:36:57,060
And I guess the trade-off being you know academic freedom

1363
01:36:57,780 --> 01:37:02,260
An academia and be able to like individually pursue ideas like purely for curiosity's sake

1364
01:37:03,140 --> 01:37:06,020
And um, you know, that's something I've really loved about academia

1365
01:37:06,020 --> 01:37:11,620
But I guess you know, I guess the general trend and machine learning research at the moment is kind of towards like larger scale projects

1366
01:37:11,700 --> 01:37:12,660
especially

1367
01:37:12,660 --> 01:37:18,020
You know a lot of the properties that we might want to see kind of only emerge when you expend a lot of compute and therefore

1368
01:37:18,900 --> 01:37:20,820
You know a lot of interesting research can kind of

1369
01:37:22,180 --> 01:37:26,180
Maybe not only be done in an industry, but it's a lot easier to do some kinds of research in industry

1370
01:37:26,660 --> 01:37:29,540
And so I think this kind of leads this trade-off of do you want freedom?

1371
01:37:29,540 --> 01:37:32,580
Or do you want to be on these like larger projects that are potentially more impactful?

1372
01:37:32,980 --> 01:37:35,700
And so yeah, I've really struggled with that trade-off. I think they they both

1373
01:37:36,180 --> 01:37:41,060
Have big pros and cons. I don't know what you think minty. Yeah, I I think that um

1374
01:37:42,180 --> 01:37:49,700
Industry is I I think I like at a very like first word rough approximation would be to say that industry focuses much more on

1375
01:37:50,180 --> 01:37:52,180
um exploitation and

1376
01:37:52,660 --> 01:37:56,500
academia is where you know in principle you should get a lot more exploration

1377
01:37:57,460 --> 01:37:59,460
But I do think that currently

1378
01:37:59,780 --> 01:38:00,660
both

1379
01:38:00,660 --> 01:38:05,460
Systems are kind of like entwined in the same sort of reward function at a high level where essentially

1380
01:38:06,900 --> 01:38:09,700
You know if if if you're if you care a lot about

1381
01:38:10,340 --> 01:38:16,820
Citations and a short-term greedy algorithm for maximizing citations would be to focus your research efforts on

1382
01:38:17,540 --> 01:38:19,540
sort of whatever topic is

1383
01:38:19,860 --> 01:38:22,180
Trendy or hyped at the current time

1384
01:38:22,260 --> 01:38:24,900
And so like I think you see tons of people obviously

1385
01:38:25,380 --> 01:38:28,660
Working on language models partly because it really is a fascinating subject

1386
01:38:28,740 --> 01:38:34,260
And it really is like the most powerful form of deep learning we have so I understand why everyone's working on it

1387
01:38:34,420 --> 01:38:36,260
but I also think that um

1388
01:38:36,260 --> 01:38:40,180
A lot of it is kind of you do get this sort of rich gets richer effect around

1389
01:38:40,580 --> 01:38:47,140
Different topics that people tend to gravitate towards and you lose a lot of the exploration that you should otherwise have

1390
01:38:47,620 --> 01:38:48,340
um

1391
01:38:48,340 --> 01:38:53,860
And that's partly because you know like both industry and academia are at some level optimizing for a similar

1392
01:38:55,460 --> 01:38:58,020
Sort of reputational status or citation count sort of metric

1393
01:38:59,060 --> 01:39:02,740
And so I think that's an issue, but I also think that in some ways

1394
01:39:03,140 --> 01:39:05,620
Uh industry you could say has

1395
01:39:06,340 --> 01:39:11,300
Additional benefit where I do think that from like a short-term point of view industry is better poised to

1396
01:39:12,020 --> 01:39:13,700
make certain

1397
01:39:13,700 --> 01:39:19,220
Higher impact research not just because of the resources available to industry, but also partly because

1398
01:39:19,780 --> 01:39:21,140
um

1399
01:39:21,140 --> 01:39:23,460
Sort of industry, uh, you know

1400
01:39:23,460 --> 01:39:26,660
Rides or dies based on whether the actual research artifact you produce

1401
01:39:27,380 --> 01:39:33,300
Is useful and so I think that's like a very powerful reward function that is not necessarily true for academia

1402
01:39:33,540 --> 01:39:36,340
Um, and then sort of on the to take the counter position

1403
01:39:36,340 --> 01:39:42,020
I think academia obviously, you know, you have a lot more freedom to just explore ideas that don't need to be on that critical path

1404
01:39:42,020 --> 01:39:46,660
For value creation immediately and so it gives you a lot more scope to potentially find like the next big thing

1405
01:39:47,060 --> 01:39:51,540
And so I think really it's about like if you want to if you want to take the bet that you can

1406
01:39:52,180 --> 01:39:54,260
You know play a part in discovering the next big thing

1407
01:39:54,740 --> 01:39:58,660
Then and that's that's suited to your taste for research then academia makes more sense

1408
01:39:59,060 --> 01:40:01,060
but if you know, um, you want to

1409
01:40:01,620 --> 01:40:06,580
You want to maximize the probability you'll have a higher impact in sort of like a near horizon line of work

1410
01:40:06,820 --> 01:40:11,780
Then industry is definitely I think a better bet rich Sutton, you know, he had this bitter lesson essay

1411
01:40:12,100 --> 01:40:17,220
And he made the argument that it's just all computation and there are no shortcuts and you can even think of you know

1412
01:40:17,780 --> 01:40:19,780
Maybe we're not very intelligent

1413
01:40:19,860 --> 01:40:24,820
Evolution has just been running for a very very long time and we are the result of that

1414
01:40:25,060 --> 01:40:30,180
So in in a sense, do you think that we could make strides in intelligence?

1415
01:40:30,580 --> 01:40:34,900
You know just through ingenuity or are we always going to need loads of computer power?

1416
01:40:36,260 --> 01:40:41,220
This definitely like makes me think of like the recent trend that we've been seeing even in like kind of the reinforcement learning

1417
01:40:41,220 --> 01:40:43,780
Literature lately, which is like these kind of large scale

1418
01:40:44,900 --> 01:40:50,740
Like mostly industry projects that are kind of they're even ditching the idea of doing like sequential decision making so

1419
01:40:51,860 --> 01:40:55,700
You know you have all these algorithms that are like, you know optimal planning and so forth

1420
01:40:56,020 --> 01:40:58,020
But we're kind of seeing

1421
01:40:58,020 --> 01:41:03,940
A trend towards you know, even ditching that complexity of algorithm and just going straight to just copy what the human did

1422
01:41:04,260 --> 01:41:06,740
and so kind of reducing the problem to you know

1423
01:41:07,620 --> 01:41:09,620
essentially no real algorithmic

1424
01:41:10,340 --> 01:41:13,220
Innovation and more just like can you gather enough expert data?

1425
01:41:13,700 --> 01:41:16,580
And I think yeah, I guess the reason why that trend is occurring is

1426
01:41:17,140 --> 01:41:19,140
Is I guess like you said there's kind of been

1427
01:41:19,700 --> 01:41:22,020
You know the bit lesson kind of said that you know

1428
01:41:22,420 --> 01:41:26,180
Just being able to scale with more data and more compute is kind of the most important thing

1429
01:41:26,660 --> 01:41:31,540
And a lot of the more complex algorithms, especially around like reinforcement learning are actually like quite challenging to scale up

1430
01:41:32,020 --> 01:41:34,980
especially like online reinforcement learning if you want to go out and like

1431
01:41:35,780 --> 01:41:39,220
Actually have an agent like actively collecting data in a bunch of different environments

1432
01:41:39,620 --> 01:41:43,140
And updating itself online like that's so much like engineering infrastructure to set up

1433
01:41:43,540 --> 01:41:46,900
And so I think there's this this trend towards just like the simplest algorithm possible

1434
01:41:46,900 --> 01:41:51,220
Which is like not even reinforcement learning not even planning just copy an expert

1435
01:41:51,700 --> 01:41:53,460
but I think that

1436
01:41:53,460 --> 01:41:57,460
That's like you kind of said earlier with like this kind of like short-term exploitation

1437
01:41:57,940 --> 01:42:02,340
I think this is you know, it it kind of makes sense to exploit this now and push it as far as possible because

1438
01:42:02,660 --> 01:42:06,900
You know, it's very easy to just train a large transformer and then gather as much data as possible

1439
01:42:07,220 --> 01:42:10,980
And I think in areas like robotics, we haven't really seen like how far can that go like

1440
01:42:11,300 --> 01:42:17,780
Can you actually get a generally useful robotics platform just by gathering more expert demonstrations and training a larger and larger transformer?

1441
01:42:18,100 --> 01:42:22,980
And so I think it does kind of make sense that why like a lot of industry projects are pursuing that because we don't really know

1442
01:42:23,380 --> 01:42:27,380
You know, will will that actually hit a bottleneck or or if you just gather enough data

1443
01:42:27,780 --> 01:42:29,540
Will that will that kind of be sufficient?

1444
01:42:30,100 --> 01:42:31,300
And I guess like

1445
01:42:31,300 --> 01:42:35,460
You know, you could argue that I think it's probably true that there must be a better algorithm out there

1446
01:42:35,700 --> 01:42:37,860
That can and principle do this in a more efficient way

1447
01:42:38,340 --> 01:42:42,100
But I guess if it's just easier to just gather more data and just do imitation learning

1448
01:42:42,180 --> 01:42:44,660
I can see that there's at least a business case for trying that

1449
01:42:45,540 --> 01:42:51,460
So I guess I'm on the the opinion of like, you know, there must be a more efficient way of getting to like a more intelligent system

1450
01:42:51,780 --> 01:42:56,660
But it's not necessarily clear that just scaling like raw supervised learning or unsupervised learning

1451
01:42:56,980 --> 01:43:00,180
Like won't get you there and so it does make sense to pursue that first

1452
01:43:00,420 --> 01:43:05,780
But kind of what I hope and expect to see is that eventually pure imitation learning or pure unsupervised learning will kind of

1453
01:43:06,180 --> 01:43:08,980
Run out of steam and everything will plateau and I think at that point

1454
01:43:09,460 --> 01:43:14,020
You know, then these like more complicated algorithms about gathering more data reinforcement learning planning, etc

1455
01:43:14,020 --> 01:43:15,620
Will really come into their own

1456
01:43:15,620 --> 01:43:18,820
And so I guess this again relates back to like the academia industry trade-off like, you know

1457
01:43:18,820 --> 01:43:22,260
A lot of the projects in the industry are just going to kind of be exploiting gathering data right now

1458
01:43:22,740 --> 01:43:25,140
Whereas maybe there's a lot of scope to do these kind of more

1459
01:43:25,620 --> 01:43:30,660
Exploratory exploratory projects where maybe that will get you to like the next frontier a few years down the line

1460
01:43:31,140 --> 01:43:32,740
I don't know what you think about this

1461
01:43:32,740 --> 01:43:34,820
Yeah, I definitely think that um

1462
01:43:34,820 --> 01:43:39,460
Yeah, just like treating everything as just supervised learning it does tend to work because we have large data sets

1463
01:43:39,540 --> 01:43:43,940
But um, I think again like the challenge is just at some point we will run out of tokens

1464
01:43:43,940 --> 01:43:50,420
We'll run out of data to train on and so that's why the self-improving more self exploratory systems will be more and more

1465
01:43:50,740 --> 01:43:53,460
I think paramount to like driving performance even further

1466
01:43:53,620 --> 01:43:57,780
So if we want to sort of break beyond sort of the token limit of like the data that's available now

1467
01:43:58,020 --> 01:44:01,060
We actually need these systems to generate their own tokens their own synthetic data

1468
01:44:01,700 --> 01:44:06,420
And that's that's where like the self play auto curricula exploration types of algorithms will start to

1469
01:44:07,620 --> 01:44:11,860
Become more and more prominent and obviously you need an environment in which to do that exploration

1470
01:44:12,180 --> 01:44:14,180
And that's where the world model

1471
01:44:14,580 --> 01:44:20,020
Line of research is going to be very powerful just because that allows you to really sort of milk all of the value within

1472
01:44:20,580 --> 01:44:27,060
The existing previous data you have seen by creating these role models where you might be able to do like counterfactual trajectories and really learn much more

1473
01:44:27,460 --> 01:44:29,460
Um, amplify the existing data you had

1474
01:44:30,020 --> 01:44:35,460
Yeah, I mean, I think one of one of the key things for me, um is modeling dynamics. So, um

1475
01:44:36,340 --> 01:44:40,420
It's quite interesting actually with the human knowledge things or even looking at the innovations from from deep mind

1476
01:44:40,420 --> 01:44:43,540
You know early versions of alpha go were bootstrapped with human knowledge

1477
01:44:43,780 --> 01:44:46,260
And then there was the alpha zero. So it was actually doing what we're talking about

1478
01:44:46,260 --> 01:44:50,340
It was actually discovering knowledge on its own and um in principle. That's a great idea

1479
01:44:50,420 --> 01:44:56,420
But of course like an irrestricted domain, it's tractable but in the real world it isn't and I'm not sure whether it makes sense to use

1480
01:44:56,500 --> 01:45:02,260
The the computation and you know information metaphor for the real world and humans and so on but but the basic idea is that

1481
01:45:02,420 --> 01:45:05,220
We are all real agents the universe is a massive computer

1482
01:45:05,540 --> 01:45:10,340
We're discovering all of this knowledge and then we're bootstrapping that into a machine learning algorithm

1483
01:45:10,660 --> 01:45:16,180
And then the question is well, if you kind of just capture the thing now without the dynamics that produced it

1484
01:45:16,420 --> 01:45:20,180
Um, will the system be robust and could you still um, you know, kind of

1485
01:45:20,260 --> 01:45:24,660
Carry on as we were in the real world if that makes sense. So um, but yeah

1486
01:45:24,660 --> 01:45:27,860
The interesting thing with the work you've done is is that you are modeling agential systems

1487
01:45:27,860 --> 01:45:34,500
You are modeling dynamics, but could that be used for you know, much more complex tasks like the real world

1488
01:45:35,140 --> 01:45:40,260
Like simulating much more complex systems in the real world. Yeah, I think that if you if you

1489
01:45:41,380 --> 01:45:46,020
So I think that just purely imitation learning alone is not really going to get you there

1490
01:45:46,900 --> 01:45:50,260
But I think that if you can if you can imitate

1491
01:45:51,380 --> 01:45:54,020
So one is sort of finding the set of tasks

1492
01:45:54,100 --> 01:45:58,660
I think that if you find the set of tasks or reward functions that could be relevant

1493
01:45:58,820 --> 01:46:04,900
Then you can start to simulate things that are otherwise really hard to capture by just purely imitating historical trajectories

1494
01:46:05,380 --> 01:46:12,740
So for example strategic adaptation type of behaviors are really hard because those are sort of an open-ended space of behaviors where

1495
01:46:13,460 --> 01:46:16,740
If you basically have like a stock market, for example, that's a really good example

1496
01:46:17,060 --> 01:46:19,620
Where if you have a stock market, that's a very open-ended system

1497
01:46:19,780 --> 01:46:23,380
And like different traders will have different strategies that are best responses to each other

1498
01:46:23,620 --> 01:46:27,300
And then over time the set of strategies evolves over time in an open-ended way

1499
01:46:27,620 --> 01:46:33,540
Um, you know trading strategies that worked 10 years ago probably won't work very well today because people have sort of um

1500
01:46:34,340 --> 01:46:39,700
They've sort of figured out those strategies. And so they won't be very competitive. And so um

1501
01:46:40,340 --> 01:46:43,460
I don't see an an imitation learning system being able to sort of

1502
01:46:44,100 --> 01:46:48,740
Um, generalize to that level of complexity just because by definition it's imitating previous

1503
01:46:49,220 --> 01:46:55,060
Uh trajectories and therefore strategies. So I think you need some notion of like a um a more uh

1504
01:46:55,540 --> 01:46:59,300
More interactive trial and error learning that allows for strategic adaptation

1505
01:46:59,380 --> 01:47:05,460
And that requires some notion of a payoff or a reward. And so you kind of need to have this this idea of um

1506
01:47:06,900 --> 01:47:08,900
You you can't just purely I think learn

1507
01:47:08,980 --> 01:47:10,180
Uh

1508
01:47:10,180 --> 01:47:13,060
A model of something like the stock market just based on previous data

1509
01:47:13,060 --> 01:47:17,060
You really need to have more inductive biases around uh, sort of you know

1510
01:47:17,380 --> 01:47:23,620
What creates a payoff or what the actual reward function is for each of the traders? Uh, but that might be something that you could

1511
01:47:24,100 --> 01:47:25,300
um

1512
01:47:25,300 --> 01:47:27,300
You could learn over time, but I

1513
01:47:27,700 --> 01:47:31,940
But maybe not in the yeah, so this is kind of like this is not very coherent

1514
01:47:31,940 --> 01:47:36,260
But I feel like uh, you might need something that looks more like learning over space of programs

1515
01:47:36,500 --> 01:47:39,860
That starts to encompass different kinds of uh tasks

1516
01:47:40,020 --> 01:47:44,660
And then you can basically simulate those tasks to completion with agents that can essentially

1517
01:47:45,060 --> 01:47:47,140
Uh try to self-improve against other agents

1518
01:47:47,860 --> 01:47:53,700
The stock market I think is a wonderful metaphor for we're talking about and for for two reasons first of all from the grounding reason

1519
01:47:53,940 --> 01:47:56,980
Because you know like the the the the the memetic world is very ungrounded

1520
01:47:57,060 --> 01:48:01,460
And that's why we develop as humans lots of weird shared delusions about things because it's actually like you know

1521
01:48:01,700 --> 01:48:03,780
It can go in it can go in almost any direction

1522
01:48:04,020 --> 01:48:09,300
And also the concept of alpha I think is really important because a trading strategy works really well today

1523
01:48:09,620 --> 01:48:14,740
And then when other people learn about it it no longer provides an advantage because everyone else knows about it

1524
01:48:14,900 --> 01:48:19,540
And I feel it's the same with language models. So, you know, like gbt4 pros was really novel and cool

1525
01:48:19,860 --> 01:48:23,300
It was great to you know, have like a ted talk speech when it came out

1526
01:48:23,620 --> 01:48:26,500
And now it doesn't seem cool anymore because everyone's using it on linkedin

1527
01:48:26,740 --> 01:48:30,340
So it's almost like that we need to have this in like continuous

1528
01:48:30,900 --> 01:48:34,420
creative evolving process producing new sources of alpha

1529
01:48:34,820 --> 01:48:40,180
And the paradox is that if everyone has access to the same model it can't be a source of alpha by definition

1530
01:48:40,340 --> 01:48:44,500
Yeah, I guess on that like topic because we kind of talked about like synthetic data earlier

1531
01:48:44,580 --> 01:48:50,100
And you kind of said like you know one one mechanism towards getting like a kind of self-improving system that is able to kind of

1532
01:48:50,260 --> 01:48:51,060
You know

1533
01:48:51,060 --> 01:48:54,260
Continue to improve is to kind of like filter the synthetic data for example

1534
01:48:54,340 --> 01:48:58,020
So you might kind of you know have the the new system and then we generate some more data

1535
01:48:58,020 --> 01:49:01,940
And then we kind of have some like filtering mechanism to say that you know in the current stock market

1536
01:49:01,940 --> 01:49:05,540
This is this is good data or what you know, whatever system we're thinking about and then we can kind of like

1537
01:49:06,340 --> 01:49:08,340
Use that to enable the model to improve

1538
01:49:08,980 --> 01:49:10,980
You know and adapt to the new system

1539
01:49:11,060 --> 01:49:14,020
But something I've always like why like thought about is like well

1540
01:49:14,020 --> 01:49:18,660
I guess one is is it really trivial to be able to like filter that you know new synthetic data

1541
01:49:19,140 --> 01:49:24,340
And then two it feels like if you're just relying on like filtering existing synthetic data

1542
01:49:24,900 --> 01:49:29,220
Like isn't that a never to go into kind of plateau and so I guess eventually

1543
01:49:29,220 --> 01:49:34,260
You know we talked about how you kind of said that you do actually actively not need to go out and get real more real data

1544
01:49:34,580 --> 01:49:39,460
But I guess I'm kind of asking you do you think this idea of just like filtering synthetic data from a model is kind of

1545
01:49:40,100 --> 01:49:44,900
Sufficient to always be able to adapt and improve or is it always going to be a mixture of like more real data

1546
01:49:45,300 --> 01:49:50,020
Plus synthetic data filtering. I think it's the latter just because um at some point you would expect that

1547
01:49:50,820 --> 01:49:55,620
The synthetic data you do generate it'll start to sort of saturate like what's already in the model

1548
01:49:56,180 --> 01:50:01,220
Just because the model is trained on a finite amount of information. So at some point you're just going to start to see more and more

1549
01:50:02,340 --> 01:50:06,900
Especially like the more likely trajectories or sequences of samples. You'll start to see that

1550
01:50:07,620 --> 01:50:12,820
More and more and so you're not really going to be very sample efficient in terms of searching for the synthetic data

1551
01:50:13,540 --> 01:50:18,900
So can you can you tell us about the results of the paper? Totally. Yeah, so basically we evaluate this um

1552
01:50:19,620 --> 01:50:24,660
This algorithm on a bunch of like synthetic simulated domains kind of like robotics related tasks

1553
01:50:25,140 --> 01:50:28,340
Um and kind of yet environments where there's like varying levels of complexity

1554
01:50:28,420 --> 01:50:32,260
So, you know, you might have a robot pushing around a variable number of like objects

1555
01:50:32,340 --> 01:50:35,460
Or maybe you have different terrain that the robot might want to um

1556
01:50:36,580 --> 01:50:39,540
Learn to kind of you know do locomotion over and things like this

1557
01:50:40,180 --> 01:50:45,860
Um and so kind of you know, the main comparison we make is like how well does waker work relative to like naive domain randomization

1558
01:50:45,860 --> 01:50:51,620
So how well does it work if you just like uniformly sample the space of environments versus if you do actively seek out

1559
01:50:51,940 --> 01:50:54,260
The environments that have this like higher uncertainty

1560
01:50:54,980 --> 01:50:58,740
Um and so basically what we show is that you know, if we do the waker approach

1561
01:50:59,060 --> 01:51:03,540
We still do like very well on average, but we consistently do better in terms of robustness

1562
01:51:03,940 --> 01:51:08,420
And so robustness by robustness. I mean here that that we do better in terms of the worst environment

1563
01:51:08,980 --> 01:51:11,780
That the agent is evaluated under and so this kind of means, you know

1564
01:51:11,780 --> 01:51:16,260
If the agent is able to do well in the worst environments that it that it is evaluated under

1565
01:51:16,420 --> 01:51:19,300
That kind of shows that it's able to do well across all environments because

1566
01:51:19,700 --> 01:51:24,900
Its worst performance is still good. Um, so we kind of this this shows that we we achieve this like robustness property

1567
01:51:25,460 --> 01:51:27,460
Which we talked about in terms of like mini max regret

1568
01:51:28,020 --> 01:51:33,380
But we evaluate we don't evaluate it in terms of like the true notion of mini max regret because as we talked about earlier

1569
01:51:33,700 --> 01:51:39,380
Actually evaluating regret exactly as difficult because that that requires knowing the exact true optimal performance

1570
01:51:39,940 --> 01:51:42,980
Which isn't something we can really know. So instead we we just show that you know

1571
01:51:43,060 --> 01:51:48,980
The agent performs well across all environments more so than if you just like naively sampled the environments uniformly

1572
01:51:49,780 --> 01:51:54,580
And in terms of decomposing the performance across the spectrum of possible environments

1573
01:51:54,580 --> 01:51:59,540
So like, you know the ideal situation is that we have a very simple model which just generalizes

1574
01:51:59,540 --> 01:52:04,980
So we happen to have found the golden motif, you know, there's a spectrum of correlations almost all of them are spurious

1575
01:52:05,060 --> 01:52:08,420
But we've just you know, just by through some sheer magic

1576
01:52:08,660 --> 01:52:12,340
We found the best motif to work in all situations. Probably that's not quite true

1577
01:52:12,420 --> 01:52:17,460
Probably there are some good generalizing motifs and the model has also kind of like memorized the long tail

1578
01:52:17,940 --> 01:52:22,980
And that there's some degree of like, you know, it works really well on on the test set that might not out of domain distribution

1579
01:52:23,140 --> 01:52:26,100
Do you have any like way of reasoning about what that is?

1580
01:52:26,980 --> 01:52:28,260
um

1581
01:52:28,260 --> 01:52:29,540
so

1582
01:52:29,540 --> 01:52:33,620
Yeah, I agree. I guess there's like not necessarily it's not necessarily the case that by like

1583
01:52:34,100 --> 01:52:36,420
Focusing more on these like long tail examples

1584
01:52:36,500 --> 01:52:41,620
That's necessarily the best way of training the best model because like you said like maybe it happens to be the case that

1585
01:52:42,020 --> 01:52:46,020
If the model is trained on some certain subset of the tasks like that will actually generalize better

1586
01:52:46,100 --> 01:52:48,820
but but I think in practice, that's not something we can really um

1587
01:52:49,780 --> 01:52:55,220
Really know how to you know, like optimally select the best kind of set of tasks that will generalize well

1588
01:52:55,540 --> 01:52:59,060
And so we do focus more on on like, you know, these these kind of long tail tasks

1589
01:52:59,060 --> 01:53:02,260
Or like the ones that we might see rarely and therefore have high uncertainty about

1590
01:53:02,980 --> 01:53:03,780
um

1591
01:53:03,780 --> 01:53:06,500
In terms of like the the out of distribution generalization

1592
01:53:06,580 --> 01:53:10,980
So so we do also do some experiments like looking at how well does the model generalize out of distribution?

1593
01:53:11,860 --> 01:53:14,900
And basically what we show is that if we train the model in this way

1594
01:53:15,220 --> 01:53:18,420
And then we give it some more environments that hasn't seen at test time

1595
01:53:18,900 --> 01:53:23,300
Um, if the environments are more complex then then we've seen sorry hasn't seen at training time

1596
01:53:24,180 --> 01:53:28,500
Basically like this model then generalizes better to out of distribution environments that are like more complex

1597
01:53:28,500 --> 01:53:32,260
Which is kind of what you'd expect because we've kind of biased something towards more complexity

1598
01:53:32,420 --> 01:53:35,540
We're able to generalize better to out of distribution environments that have higher complexity

1599
01:53:35,940 --> 01:53:40,420
And then I guess the question is like do we care about out of distribution environments that have higher complexity?

1600
01:53:40,420 --> 01:53:42,900
Like what about the out of distribution environments that have lower complexity?

1601
01:53:43,540 --> 01:53:45,540
and I would argue that you know

1602
01:53:45,940 --> 01:53:50,180
Basically the lower environment out of distribution distribution environments that have lower complexity

1603
01:53:50,180 --> 01:53:55,540
Like we would already expect that the model is able to do very well at so so there's not really much of a difference there because you know

1604
01:53:56,020 --> 01:53:59,300
Almost any reasonably trained model can handle the very simplest environment

1605
01:53:59,460 --> 01:54:03,780
So what we really care about is can we generalize out of distribution to like higher complexity environments?

1606
01:54:04,180 --> 01:54:07,140
And so by biasing the something towards the higher complexity environments

1607
01:54:07,140 --> 01:54:11,060
We do show that we're able to generalize further out of distribution to even higher complexity environments

1608
01:54:11,300 --> 01:54:14,420
Okay, but is there any way of knowing whether it's kind of like

1609
01:54:14,820 --> 01:54:19,860
Memorizing the high complexity instances or whether it's still learning abstract motifs and generalizing between them

1610
01:54:20,180 --> 01:54:26,260
Yeah, that's a great question. I think that's a really interesting question generally for ml as a field right now

1611
01:54:26,260 --> 01:54:28,260
Which is better evaluation benchmarks for

1612
01:54:28,900 --> 01:54:31,460
Generalization within different kinds of models

1613
01:54:31,780 --> 01:54:37,940
Um, and like we we alluded to earlier. There's kind of this issue of data leakage between training and test set

1614
01:54:38,100 --> 01:54:43,540
Which is um, which is definitely an issue that is currently happening with large language models

1615
01:54:43,940 --> 01:54:48,820
Um, it doesn't take away from the impressiveness of these models because clearly there is a strong generalization

1616
01:54:49,140 --> 01:54:54,100
aspect to their behavior, but I do think that in terms of measuring performance on specific benchmarks

1617
01:54:54,420 --> 01:54:57,940
Um, we really need to solve this problem. How do we have these clean data sets?

1618
01:54:58,500 --> 01:55:00,260
That allow us to

1619
01:55:00,260 --> 01:55:05,620
To truly test on inputs that the model hasn't seen at training. Um, I think in the case of

1620
01:55:06,420 --> 01:55:08,420
reinforcement learning

1621
01:55:08,420 --> 01:55:12,500
That's a bit more difficult just because usually we focus on a particular task domain

1622
01:55:12,660 --> 01:55:15,060
And so there's always going to be some shared similarities within the task

1623
01:55:15,060 --> 01:55:20,980
But obviously, uh, we didn't do this in this paper, but we could try things where we have more um more controlled

1624
01:55:21,620 --> 01:55:27,940
Settings where we you know change one aspect of the environment and really see if it's learning specific causal relationships between

1625
01:55:28,580 --> 01:55:30,580
Things that have to be accomplished in that task

1626
01:55:30,900 --> 01:55:34,820
But we didn't do that. Um, that I actually think would be a really interesting idea for

1627
01:55:35,460 --> 01:55:37,460
A new evaluation environment for rl

1628
01:55:37,940 --> 01:55:41,460
Yeah, I mean the benchmarks thing is just a huge challenge in in machine learning

1629
01:55:41,940 --> 01:55:45,220
In general, but just just to kind of round off off the interview

1630
01:55:45,220 --> 01:55:49,140
I mean minchie you you were talking about you're doing some work with um edgreff instead and it is amazing

1631
01:55:49,140 --> 01:55:54,260
I'm getting edg back on and um, you said that um, you've been looking into this kind of the interface

1632
01:55:54,580 --> 01:55:56,740
Between humans and machine learning. Can you tell me about that?

1633
01:55:56,980 --> 01:56:02,100
Yeah, so just to not say too much about it because um, it's related to current work that's happening at DeepMind

1634
01:56:02,500 --> 01:56:06,340
Um is just that you know, I think from personally from a high level point of view

1635
01:56:06,660 --> 01:56:10,660
I'm very interested, you know talking about this divide sort of this fork in the road in terms of

1636
01:56:10,980 --> 01:56:16,100
What's the path to open studying open-endedness studying it in silico or studying it in

1637
01:56:16,660 --> 01:56:20,580
situ in the setting of an actual open-ended system like a user

1638
01:56:21,460 --> 01:56:26,500
App interaction or you know the interaction between a user and a piece of software on the web

1639
01:56:27,380 --> 01:56:30,100
Or potentially with many other users. There are such rich

1640
01:56:30,900 --> 01:56:37,540
Existing systems online that are already open-ended because they amplify or connect the creativity and knowledge of humans

1641
01:56:38,180 --> 01:56:44,180
To create more knowledge and more creative artifacts. And so I think what's really uh, interesting in my mind now is sort of studying

1642
01:56:44,900 --> 01:56:49,060
Systems or algorithms that allow us to better steer the creativity of humans

1643
01:56:49,700 --> 01:56:51,700
As they are mediated by software

1644
01:56:52,420 --> 01:56:55,380
And basically allow us to essentially amplify

1645
01:56:55,940 --> 01:57:03,060
Existing intelligent or creative systems that are open-ended so amplify existing open open-endedness rather than try to build it from scratch

1646
01:57:04,020 --> 01:57:08,580
Amazing guys. It's been an honor to have you on MLS T. Thank you so much. Thanks. Thank you. Yeah

1647
01:57:09,380 --> 01:57:11,380
Great cool. Yeah, we're done

