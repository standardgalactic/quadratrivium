WEBVTT

00:00.000 --> 00:05.360
Murray Shanahan is a professor of cognitive robotics at Imperial College London and a senior

00:05.360 --> 00:11.040
research scientist at DeepMind. He graduated from Imperial College with a first in computer science

00:11.040 --> 00:18.400
in 1984 and obtained his PhD from King's College in Cambridge in 1988. He's since worked in the

00:18.400 --> 00:24.080
fields of artificial intelligence, robotics and cognitive science. He's published books such as

00:24.080 --> 00:29.440
Embodiment and the Inner Life and the Technological Singularity. His book Embodiment and the Inner

00:29.440 --> 00:34.560
Life was a significant influence on the film Ex Machina for which he was a scientific advisor.

00:35.920 --> 00:40.640
Now Professor Shanahan is a renowned researcher on sophisticated cognition

00:40.640 --> 00:45.920
and its implications for artificial intelligence. His work focuses on agents that are coupled to

00:45.920 --> 00:51.600
complex environments through sensory motor loops such as robots and animals. He's also

00:51.600 --> 00:56.240
particularly interested in the relationship between cognition and consciousness and has

00:56.240 --> 01:01.920
developed a strong understanding of the biological brain and cognitive architectures more generally.

01:01.920 --> 01:07.360
In addition Professor Shanahan is interested in the dynamics of the brain including metastability,

01:07.360 --> 01:12.960
dynamical complexity and criticality as well as the application of this understanding to

01:12.960 --> 01:18.240
machine learning. He's also fascinated by the concept of global workspace theory as proposed

01:18.240 --> 01:23.040
by Bernard Bars. We'll be talking about that on the show today which is based on a cognitive

01:23.040 --> 01:28.240
architecture comprising a set of parallel specialist processes and a global workspace.

01:28.240 --> 01:32.640
Professor Shanahan is committed to understanding the long-term implications of artificial

01:32.640 --> 01:38.960
intelligence both its potential and its risks. His research has been published extensively

01:38.960 --> 01:42.800
and he's a member of the External Advisory Board for the Cambridge Centre of the Study of

01:42.800 --> 01:48.560
Existential Risk and also on the editorial boards of Connection Science and Neuroscience of Consciousness.

01:49.280 --> 01:55.600
Conscious Exotica Professor Shanahan wrote an article called Conscious Exotica

01:55.600 --> 02:03.040
in 2016 where he invited us to explore the space of possible minds, a concept first proposed by

02:03.040 --> 02:09.760
philosopher Aaron Sloman in 1984. Now this space is comprised of all the different forms of minds

02:09.760 --> 02:15.600
which could exist from those of other animals such as chimpanzees to those of life forms that could

02:15.600 --> 02:21.120
have evolved elsewhere in the universe and indeed those of artificial intelligences.

02:21.120 --> 02:27.760
Now in order to describe the structure of this space Shanahan proposes two dimensions, the capacity

02:27.760 --> 02:34.480
for consciousness and human likeness of the behavior. According to Shanahan the space of

02:34.480 --> 02:40.080
possible minds must include forms of consciousness that are so alien that we wouldn't even recognize

02:40.080 --> 02:47.120
them. He rejects the dualistic idea that there's an impenetrable realm of the subjective experience,

02:47.120 --> 02:53.840
remember we were talking about Nagel's bat on the Charmer's show, insisting instead that

02:53.840 --> 03:00.320
nothing is hidden metaphorically speaking, citing Wittgenstein actually. Now Shanahan argues that

03:00.320 --> 03:07.120
while no artifacts exist today, which has anything even approaching human-like intelligence,

03:07.200 --> 03:12.160
the potential for variation in artificial intelligences far outstrips the potential

03:12.160 --> 03:18.160
for variation in naturally evolved intelligence. This means that the majority of the space of

03:18.160 --> 03:25.440
possible minds may be occupied by non-natural variants such as the conscious exotica of

03:25.440 --> 03:31.600
which Shanahan speaks. Now ultimately Shanahan's exploration of the space of possible minds invites

03:31.600 --> 03:37.600
us to consider the possibility for human-like minds but also for those that are radically different

03:37.600 --> 03:44.560
and inscrutable. He concludes that although we may never understand these alien forms of consciousness,

03:44.560 --> 03:48.400
we can still recognize them as part of the same reality as our own.

03:50.160 --> 03:55.760
So Professor Shanahan has just dropped a brand new paper called Talking about large language models

03:55.760 --> 04:00.960
in which he discusses the capabilities and limitations of large language models.

04:00.960 --> 04:05.680
Now in order to properly comprehend the capacities and boundaries of these models,

04:05.680 --> 04:09.360
we must first grasp the relationship between humans and these systems.

04:09.920 --> 04:15.280
Humans have evolved to survive in a common world and have cultivated a mutual understanding

04:15.280 --> 04:21.200
reflected in their ability to converse about convictions and other mental states. Conversely,

04:21.200 --> 04:26.240
AI systems lack this shared comprehension, so attributing beliefs to them should be done

04:26.320 --> 04:31.040
circumspectly. Now prompt engineering is something that we've all become very familiar with,

04:31.040 --> 04:35.520
we've discussed it a lot on this show recently, and it's almost become a fact of the matter when

04:35.520 --> 04:41.600
it comes to these large language models. It involves exploiting prompt prefixes to adjust

04:41.600 --> 04:46.800
the language models to diverse tasks without needing any supplementary training, allowing for

04:46.800 --> 04:53.680
more effective communication between humans and machines. Nevertheless, lacking a more profound

04:53.760 --> 04:57.840
understanding of the system and its relationship to the external world,

04:57.840 --> 05:02.720
it's difficult to be certain whether the arguments produced by a large language model

05:02.720 --> 05:10.080
are genuine reasoning or simply mimicry. Large language models can be integrated into a variety

05:10.080 --> 05:16.240
of embodied systems even, such as robots or virtual avatars. However, this doesn't necessarily

05:16.240 --> 05:22.000
mean that these systems possess completely human-like language abilities. Even though the robot in the

05:22.000 --> 05:28.080
SAKAN system is physically embodied and interacts with the real world, its language is still learned

05:28.080 --> 05:33.200
and used in a dramatically different manner than humans. So in summary, although Professor

05:33.200 --> 05:39.280
Shanahan concludes that large language models are formidable and versatile, they're fundamentally

05:39.280 --> 05:44.800
unlike humans and we must be wary of ascribing human-like characteristics to these systems.

05:44.800 --> 05:50.480
We must find a way to communicate the nature of these systems without resorting to simple terms.

05:50.480 --> 05:55.360
This may necessitate an extended period of interaction and experimentation with the technology,

05:55.360 --> 06:01.440
but it's a fundamental step if we are to accurately portray the capabilities and limitations of

06:01.440 --> 06:07.760
large language models. So anyway, without any further delay, I give you Professor Murray Shanahan.

06:08.960 --> 06:14.320
Professor Shanahan, it's an absolute honor to have you on MLSD. Tell me a little bit about your

06:14.320 --> 06:19.680
background. My background? Well, I've been interested in artificial intelligence for as

06:19.680 --> 06:25.120
long as I can remember since I was a child, really, and I was very much drawn to it by

06:25.120 --> 06:32.800
science fiction, by science fiction movies and books. And then I studied computer science

06:32.800 --> 06:38.400
right from when I was a teenager and got very much drawn into programming, was fascinated by

06:38.400 --> 06:46.240
programming. I really did my 10,000 hours of programming experience when I was quite young

06:46.240 --> 06:51.120
and I went on to do computer science at Imperial College London. That was my degree.

06:51.920 --> 06:56.560
And then still fascinated by artificial intelligence, I went on to Cambridge

06:57.520 --> 07:02.880
and did my PhD in AI in Cambridge, very much in the symbolic school then.

07:04.560 --> 07:10.240
And then I had a long affiliation with Imperial College, did my postdoc there and still in symbolic

07:10.240 --> 07:16.160
AI. And then at some point, I became a bit disillusioned with symbolic AI and I kind of

07:16.160 --> 07:22.960
segued into studying the brain, which was the obvious example of actual general

07:22.960 --> 07:30.960
intelligence that we have. And I think it was a good 10 years on an excursion into neuroscience

07:30.960 --> 07:37.920
and computational neuroscience and that kind of thing. And then deep learning and deep

07:37.920 --> 07:43.280
reinforcement learning happened in the early 2010s and AI started to get interesting again.

07:43.280 --> 07:51.040
And I got very much back into it that way. And I was particularly impressed by DeepMind's

07:51.040 --> 07:56.800
DQN, the system that learned to play Atari games from scratch. And I thought that was a fantastic

07:56.800 --> 08:04.000
step forward. And I really kind of went back to my roots and back to AI at that point.

08:04.000 --> 08:08.080
Yeah, and I think we'll talk about DQN when we speak about your article on consciousness.

08:08.080 --> 08:16.560
But so having such a diverse set of experiences in adjacent fields, how have they influenced each

08:16.560 --> 08:22.880
other? Yeah, well, and one thing I didn't mention is that I've also had a long standing

08:22.880 --> 08:29.280
interest in philosophy. And I very often think that what I am is a sort of weird kind of

08:29.280 --> 08:35.440
philosopher, really. And philosophical questions have had a great attraction for me. So I think

08:36.560 --> 08:42.080
there's a sort of three way into relationship between artificial intelligence, neuroscience,

08:42.080 --> 08:47.440
and the other cognitive sciences and philosophy. And I think they all kind of mutually inform

08:47.440 --> 08:53.280
each other, really. Yeah. Fantastic. So you wrote a book called

08:53.280 --> 08:57.040
Embodiment and the Inner Life. What motivated you to write that book?

08:57.040 --> 09:06.720
Yeah. So at that point, so that book was published in 2010. And it was the culmination of a sort of

09:06.720 --> 09:14.320
long excursion into thinking about consciousness and about brains, which took place after I had

09:14.320 --> 09:19.600
moved away from symbolic AI, really. So I was thinking about the biological brain.

09:19.680 --> 09:23.520
In the back of my mind, I'd always been fascinated by these philosophical questions about

09:23.520 --> 09:30.800
consciousness. And then I went a bit kind of crazy and started thinking about these things

09:30.800 --> 09:36.160
seriously. It became kind of my day job to think about neuroscience, and about consciousness.

09:36.160 --> 09:42.000
And around about that time, the science of consciousness was taking off as a serious

09:42.000 --> 09:46.800
academic discipline with proper experimental paradigms. So that was really fascinating.

09:47.440 --> 09:54.800
And I got to know Bernie Bars. Bernie Bars is the person who originated global workspace theory,

09:54.800 --> 09:58.960
global workspace theory being one of the leading contenders for a scientific

09:58.960 --> 10:05.200
theory of consciousness. And I was very drawn to global workspace theory, and partly because

10:06.000 --> 10:11.520
it was a computational sort of theory. It drew very heavily on computer science

10:12.480 --> 10:17.600
and computer architectures. There was a computer architecture at the center of the theory.

10:20.320 --> 10:25.520
So this kind of collection of interests, along with my philosophical interests, which all came

10:25.520 --> 10:31.520
together, and I wanted to put them all into a book where I expressed my kind of ideas about,

10:31.520 --> 10:36.080
first of all, from the philosophical side, very heavy influence of Wittgenstein about how we

10:36.080 --> 10:41.600
address these problems at all, then lots of global workspace theory and a certain kind of

10:41.600 --> 10:47.280
global workspace architecture, how that might be realized in the brain, drawing also on the

10:47.280 --> 10:53.600
work of Stanislaus DeHend, who was working on what he called the global neuronal workspace idea,

10:54.160 --> 10:57.600
and putting all these things together into one big book.

10:58.240 --> 11:02.560
Amazing. Well, we'll speak a lot about Wittgenstein when we speak about the language model paper

11:02.640 --> 11:07.520
and your consciousness paper. But two things that did trigger or prick my ears up,

11:08.160 --> 11:13.520
computationalism, which is quite interesting, because some folks in the cognitive science arena,

11:13.520 --> 11:21.440
especially with the fouries, like examples to escape from computationalism. We did a show on

11:21.440 --> 11:27.600
cells, Chinese room argument the other day. He's probably one of the most known people who do

11:28.400 --> 11:31.200
issue computationalism. So what do you think about that?

11:31.200 --> 11:36.080
Yeah. Well, actually, so when I was talking about global workspace theory, I mentioned that it

11:38.880 --> 11:46.000
comes out of a kind of computational architecture. But in fact, where I took it was very much moving

11:46.000 --> 11:53.200
away from that original presentation, which drew heavily on a kind of quite an old-fashioned

11:53.200 --> 11:57.120
architectural perspective, sort of boxes and how they communicate with each other and so on.

11:58.000 --> 12:01.760
And I was much more interested in taking it in a direction which is very much more

12:01.760 --> 12:09.200
connectionist and drawing much more heavily on the underlying biology and neuroscience,

12:09.200 --> 12:13.600
which in fact is also a direction that Bernie Barnes himself had moved in, because the book

12:14.640 --> 12:21.280
that originally put forward his theory is from 1988. So that was the predominant way of thinking

12:21.280 --> 12:27.840
at the time was this very computational cognitiveist perspective. So by 2010, when my book was

12:27.840 --> 12:34.160
published, I was very much more interested in a kind of more connectionist perspective on things.

12:35.120 --> 12:39.440
So that's the way that it's portrayed in the book, the theory.

12:40.000 --> 12:47.120
Fascinating. Because in this arena, some people cite penrose or the need for hypercomputation,

12:48.080 --> 12:52.880
because people talk about the church-turing hypothesis and this idea that the universe

12:52.880 --> 12:58.080
could be made of information, which is quite interesting. But do you believe that the world

12:58.080 --> 13:02.240
that we live in could be computationally represented and computed?

13:04.240 --> 13:07.600
Well, I'm not sure that I have a belief on that particular one.

13:09.760 --> 13:15.520
So I mean, I mean, Penrose's ideas about consciousness, of course, draw heavily on

13:15.520 --> 13:22.400
quantum mechanics, and he thinks that quantum effects are important for consciousness. But

13:22.400 --> 13:28.000
I mean, that's very much a minority, a tiny minority view within the people who study

13:28.000 --> 13:34.480
consciousness from a scientific standpoint. And so I don't really subscribe to that

13:35.040 --> 13:39.200
interview, I have to say. Well, I mean, coming at it from a slightly different angle, we spoke

13:39.200 --> 13:44.080
to Noam Chomsky recently, and I've just done some content on Nagel's bat, a couple of rationalists,

13:45.040 --> 13:51.200
their big argument is about the subject of experience and the limits of our cognitive

13:51.200 --> 13:57.920
horizon and the inability really for us to reduce things into a comprehensible framework of

13:57.920 --> 14:02.800
understanding. So how would you bring that in? Yeah, well, gosh, I mean, yeah, we've launched

14:02.800 --> 14:10.800
right into some really big, difficult topics here, right? So in my book, Embodiment in the

14:10.960 --> 14:15.120
Inner Life, which at the time, I thought I'd really kind of like wrapped up the problem of

14:15.120 --> 14:23.040
consciousness. But one of the big sort of outstanding things for me in one of the

14:23.040 --> 14:27.520
outstanding questions that I have not really answered, I felt in that book, is very much

14:27.520 --> 14:35.520
related to Nagel's question about bats, what does it like to be a bat? So, and it's to do with the

14:35.520 --> 14:42.320
idea that there's a sort of intuitive idea that maybe there can be very exotic entities,

14:42.320 --> 14:48.560
very exotic creatures who are completely unlike us. And yet, somehow, there's some kind of

14:48.560 --> 14:54.800
consciousness there that we could barely grasp its nature. And this is a sort of natural

14:54.800 --> 14:59.200
intuitive thought. And especially when we look at other animals, like bats, and especially if we

14:59.200 --> 15:05.360
look at an animal that's a bit different from us, then we get hints that there's some

15:05.760 --> 15:11.280
one at home, as it were, and that there's consciousness there. I think we, I'm sure all

15:11.280 --> 15:18.320
of us believe that cats and dogs, and many other animals are conscious and are capable of suffering

15:18.320 --> 15:24.640
and having awareness of the world that's like our awareness and are aware of us and each other.

15:28.000 --> 15:34.080
I mean, I take that as almost axiomatic. That's just the way we treat those creatures.

15:34.080 --> 15:38.640
But then when we think about something like a bat, it's very different from us. So the natural

15:38.640 --> 15:44.320
thing thought is that maybe what it's like is very, very different from what it's like for us,

15:44.320 --> 15:52.800
and it's a natural thought to express. And of course, Nagel takes that thought

15:54.640 --> 16:03.600
to suggest that there are something that is inaccessible to us, which is what is it like to

16:03.600 --> 16:09.280
be a bat? It's something we can never know. And this is a very un-Viconstinian thought. And I'm

16:09.280 --> 16:17.520
very much, you know, I'm very attracted to Viconstine's philosophy. But it's also a very

16:17.520 --> 16:22.000
natural thought that, you know, so it's a very un-Viconstinian thought because Viconstine says,

16:22.000 --> 16:27.600
for example, you know, nothing is hidden. So he's very, you know, and the whole private language

16:27.600 --> 16:32.480
remarks are all about sort of saying, well, this intuition that we have that there's this

16:32.560 --> 16:39.600
private realm of experience is actually just, it's just a philosophical trick of the mind

16:39.600 --> 16:45.600
to think that this sort of peculiar metaphysical realm exists of inaccessible, subjective

16:45.600 --> 16:53.680
experience in others. And that's his whole thrust of his philosophy or that aspect of it

16:53.680 --> 16:57.200
is to try and undermine that. So these two things are intention, right? So there's this

16:57.280 --> 17:02.960
natural thought that bats, you know, it must be like something to be a bat, but what is it like

17:02.960 --> 17:06.960
and how could we ever know? And then there's the Viconstinian thought, which is actually very

17:06.960 --> 17:11.760
difficult to kind of really embrace. But it's that there's a sense in which nothing is really

17:11.760 --> 17:16.720
metaphysically hidden. It's only hidden, could be hidden empirically, because maybe we don't

17:16.720 --> 17:21.280
know enough, maybe we haven't hung around with bats often enough, or maybe we haven't examined

17:21.280 --> 17:25.680
their brains, or maybe that's all empirical, right? So there's nothing metaphysically hidden,

17:25.680 --> 17:29.280
whereas Nagel's point is that there's something that's deeply, profoundly,

17:29.280 --> 17:33.360
philosophically, metaphysically hidden, which is the subjective. Now we can extend that,

17:33.360 --> 17:40.320
shall carry on. So I'm rambling now. So now we can extend that thought about bats,

17:40.320 --> 17:44.160
now, you know, especially from the perspective of the sort of thing that I'm interested in,

17:44.160 --> 17:48.400
to, well, not just bats, but what about the whole space of possible minds to use

17:48.400 --> 17:53.600
Aaron Sloman's very evocative phrase? What about, you know, extraterrestrials who are going to be,

17:54.480 --> 17:59.200
you know, who surely there is extraterrestrial intelligence out there, it's going to be very,

17:59.200 --> 18:04.640
very, very different to us. So, and then what about the things that we build? Maybe we can build

18:05.520 --> 18:11.840
things, you know, and artificial intelligence of the future, maybe, maybe, you know, we can build

18:11.840 --> 18:15.440
something that is also conscious, it's the kind of thing that's depicted in science fiction all

18:15.440 --> 18:19.920
the time. In science fiction, it's often depicted as very human-like, but there's no reason why it

18:19.920 --> 18:24.880
should be human-like at all. And so we can imagine these very, very exotic entities, and then the

18:24.880 --> 18:29.200
question is even bigger, you know, there could be something that we, we won't even be able to recognize

18:29.200 --> 18:33.760
that there was even the possibility of consciousness, but maybe it's buried there inside this complex

18:33.760 --> 18:38.720
thing somehow. So that's the, that's the kind of question that fascinated me. And I wrote this

18:38.720 --> 18:44.240
paper called Conscious Exotica, which is all about trying to, trying to make that Viconstinian

18:44.240 --> 18:50.640
perspective encompass this possibility as well. Yeah, and maybe we should talk about that before

18:50.640 --> 18:56.640
the language paper, just because it's, it's what we're talking about now. But there's a few things

18:56.640 --> 19:01.440
you said there, which are really interesting. So, you know, when Chomsky talks about ghosts in the

19:01.440 --> 19:08.320
machine, and he goes back to Galileo and Descartes, and actually it was Descartes who, you know,

19:08.320 --> 19:14.640
introduced this kind of mind-body dualism, you know, which was kind of a move away from

19:14.640 --> 19:18.400
the previous desire to have a mechanistic understanding of the world that we live in.

19:18.400 --> 19:24.400
Humans want to understand, and actually so many things in the world eludes our understanding.

19:24.400 --> 19:29.520
And then that brings us on to David Chalmers' point that the hard problem of consciousness,

19:29.520 --> 19:35.280
which I suppose is an extension of the mind-body problem. And it's, as you were saying, this

19:35.280 --> 19:39.920
little bit extra, right? So we think about, and I agree with Chalmers that intelligence and

19:39.920 --> 19:44.800
consciousness are likely entangled or would co-occur together. But he always said that there's

19:44.800 --> 19:50.560
function, dynamics, and behavior. And then there's that little subjective thing on the top. And for

19:50.560 --> 19:55.280
Chalmers' consciousness, it's almost like, what's the cash value of it? He just thinks it's just

19:55.280 --> 19:59.760
something on top. It's not really requisite for anything else. And I believe it might be requisite

19:59.760 --> 20:04.880
for intentionality and agency as so did. But what's your take?

20:04.880 --> 20:08.480
Well, it's interesting because the whole way that you put that and the whole way that

20:08.480 --> 20:13.680
people often talk about this thing is you speak about consciousness. Like, there's this thing,

20:13.680 --> 20:17.920
which, you know, there's this singular thing, which maybe it's needed, maybe it isn't, maybe

20:17.920 --> 20:25.360
it's this, maybe it's that. But I think that whole way of talking is, which is natural for us

20:25.360 --> 20:29.200
in many everyday situations. But when it comes to this kind of conversation, I think that whole

20:29.200 --> 20:35.440
way of talking is maybe not quite right, because we're thinking of consciousness as this, you know,

20:35.440 --> 20:39.200
we're reifying it, turning it into this thing. Whereas I think maybe at that point we have to

20:39.200 --> 20:44.160
take a step back and we have to say, well, when we talk about, when we use that word,

20:44.160 --> 20:48.720
conscious or consciousness, so we use it in all kinds of different ways in different contexts.

20:48.720 --> 20:55.200
And so when we talk about, you know, we might talk about it in the context of an animal, we might

20:55.200 --> 21:01.520
say, well, the animal, you know, this dog is aware of its environment. So, you know, this dog can see

21:02.080 --> 21:07.120
the bowl in front of it, it can see me, it can see the door, it can see the trees, it can see

21:07.120 --> 21:11.840
the squirrel, you know, and it can smell more like you'd smell all of these things as well.

21:12.400 --> 21:18.080
So we use consciousness, you know, we talk about consciousness in that sense. And we also talk

21:18.080 --> 21:23.520
about our self-consciousness, you know, we talk about the fact that we're aware of our own thoughts

21:23.520 --> 21:30.560
and we talk about our inner life and we use consciousness to encompass that as well.

21:31.520 --> 21:37.920
We often use consciousness in the context scientifically of a distinction between

21:38.720 --> 21:42.640
conscious and unconscious processes. And that's a very interesting distinction because

21:43.360 --> 21:48.640
when we're consciously aware of a stimulus as humans, then a whole lot of things come together.

21:48.880 --> 21:55.040
We're able to kind of like deal with novelty better, we're able to report it, we're able to

21:55.040 --> 22:01.280
remember things better. So whereas when we perhaps are unconsciously or there's a kind

22:01.280 --> 22:05.680
of unconscious processing of the stimulus, then we still can respond to it behaviorally, but

22:06.560 --> 22:10.320
and it can have queuing effects and so on, but it doesn't have all those other things.

22:10.320 --> 22:16.320
So this and that's kind of, there's a kind of integrative function for consciousness there.

22:16.320 --> 22:22.240
And then on top of all of that, there is the capacity for suffering and joy that comes with.

22:22.240 --> 22:29.280
So often there's valence to consciousness, you know, so that's another thing.

22:29.280 --> 22:32.800
So all of these things, they come as a package in humans, but when we speak about

22:33.360 --> 22:41.200
edge cases, then these things come apart and we need to speak about them separately, I think.

22:41.200 --> 22:46.240
Fascinating. I mean, there are two kind of minor digressions there. I mean,

22:46.240 --> 22:50.000
you were talking about these planes of consciousness, which is also very interesting.

22:50.000 --> 22:54.640
And maybe we could get into the integrated information theory or the global workspace

22:54.640 --> 22:57.760
theory just for the audience, just to give them some context.

22:57.760 --> 23:00.560
Yeah, sure. Or do you want me to say a few words about that?

23:00.560 --> 23:01.520
Oh, please, yeah.

23:01.520 --> 23:07.920
Okay. Yeah. So there are a number of kind of candidates for a scientific theory of consciousness.

23:08.720 --> 23:12.160
And you just mentioned two of the leading ones, which are global workspace theory and

23:12.160 --> 23:17.600
integrated information theory. And so global workspace theory. So that's, that's Bernie

23:17.600 --> 23:23.360
Baals's was originated by Bernie Baals and has been developed by Stanislaus, Dehen and colleagues.

23:23.360 --> 23:28.240
So the idea there is it's, it does rest on this sort of architectural idea, which is that,

23:30.160 --> 23:35.440
which is that we think of the brain, the biological brain as comprising, you know,

23:35.440 --> 23:39.600
a very large number of parallel processes. This is kind of a natural way to think of the brain,

23:39.600 --> 23:45.120
a large number of parallel processes. And it, and the global workspace theory posits a particular

23:45.120 --> 23:50.480
way in which these, these processes interact and communicate with each other. And that is via

23:50.480 --> 23:56.000
this global workspace. And the idea there is that, is that there are sort of two modes of

23:56.000 --> 24:03.040
processing that go on. So in one mode of processing, the, these parallel processes just do their,

24:03.040 --> 24:09.440
their own thing independently. And in the other mode of processing, they are working via this

24:09.440 --> 24:14.640
global workspace theory. So the idea is that they, you might think of them as, as, you know,

24:14.640 --> 24:19.680
depositing messages, if you like, in this global workspace, which are then broadcast out to all

24:19.680 --> 24:23.440
of the other processes. So, so it's, so there's this kind of, but I think thinking of it in

24:23.440 --> 24:27.600
terms of messages is not quite the right way of thinking of it is better to think in terms of

24:27.600 --> 24:31.840
kind of signaling and information and so on. But that's a natural way to think of it. But

24:31.840 --> 24:39.600
so the, so these, so in, in that mode, the, these processes are sort of disseminating their influence

24:39.600 --> 24:45.200
to all the other processes. And that's the global kind of broadcast aspect of it. And that's when

24:45.200 --> 24:50.640
consciousness, well, that's when information processing is conscious, according to global

24:50.640 --> 24:54.560
workspace theory, as opposed to when it's all just local and the processes are doing their own

24:54.560 --> 25:00.480
thing. That's, that's not that that processing is not conscious. So there's a dist, so it's

25:00.480 --> 25:05.200
about teasing out this distinction between conscious information processing and unconscious

25:05.200 --> 25:11.040
information processing. Now, all of those terms, by the way, are deeply philosophically problematic

25:11.040 --> 25:15.360
and to go in, you know, you have to sort of do it properly, you have to kind of unpack them all

25:15.360 --> 25:20.320
in very carefully. And that's what my book try, try, tries to do. But so essentially, it's about

25:20.320 --> 25:24.800
so the essential idea, though, is to do with broadcast and dissemination of information

25:24.800 --> 25:29.120
throughout the brain and going from like local processes and help them having global influence.

25:29.200 --> 25:32.240
And that's what consciousness is all about according to global workspace theory.

25:33.200 --> 25:37.920
Okay, so integrated information theory. So I think so integrated information theory,

25:37.920 --> 25:48.000
which is Giulio Tononi's theory, which Giulio Tononi thinks is kind of kind of incompatible

25:48.000 --> 25:52.640
in some ways with with global workspace theory. But I don't think that's, that's true. I think

25:52.640 --> 25:56.320
I think that there's a lot of synergy between the two theories, in fact.

25:56.800 --> 26:03.920
But but that's because they so they come with the same for integrated information theory

26:04.640 --> 26:11.040
has sort of two aspects to it. So according to Giulio Tononi, he really is trying to pin down

26:11.040 --> 26:17.120
a property, which is almost like a physical property, which is identical with consciousness.

26:17.120 --> 26:22.800
So you can actually speak about the amount of consciousness in any system that you that you

26:22.800 --> 26:28.080
look at phi, he could this is good, it's phi. So the phi is a number how is actually a number of

26:28.080 --> 26:33.680
how much consciousness is present in the system, like, like part of your brain, your whole brain,

26:33.680 --> 26:39.440
or you as a person, or a flock of bats, or whatever, so you can or toaster, you know,

26:39.440 --> 26:44.720
so you can give a number to how much consciousness there is, there is there according to his theory.

26:44.720 --> 26:51.360
And it's a mathematical theory based on Shannon's information theory. And it's but it and but it's

26:51.440 --> 26:57.120
all about trying to see how much information is processed by the individual parts of the system

26:57.760 --> 27:04.240
versus how much information is processed by all the parts put together. And it's and it's to do

27:04.240 --> 27:10.320
with how much the second thing, you know, exceeds the first thing. And in a sense, and that is how

27:10.320 --> 27:17.120
much consciousness there is there. And, and in a way, it actually has some synergies. If you as

27:17.120 --> 27:22.160
long as you don't think that it's necessarily measuring, you know, this property of the of

27:22.160 --> 27:26.560
the universe, which you can put a number on. But it has some synergies with global workspace theory,

27:26.560 --> 27:34.000
because they're both distinguishing between global holistic things versus local things. And the

27:34.000 --> 27:40.640
and the consciousness is in the kind of global holistic processing versus the local, you know,

27:40.640 --> 27:44.640
local processing in both those theories. So there's a kind of, you know, there's some

27:44.640 --> 27:48.640
intuitions that they have in common, I think. Interesting. And it also reminds me a little

27:48.640 --> 27:55.120
bit a little bit about what Chalmers speaks about. So he thinks that it strongly emerges from certain

27:55.120 --> 28:01.520
types of information processing. And the processing must represent causal structures as well. So it

28:01.520 --> 28:07.360
can't it's it's not an appeal to panpsychism per se. And although with with all of the things

28:07.360 --> 28:11.520
that you've just spoken about, what do they work in another universe? I mean, I guess what I'm

28:11.520 --> 28:17.520
saying is, is it just the the physical and the information processing or in a different universe

28:17.520 --> 28:22.000
might it not emerge in the same way? Yeah, which depends what you mean by a different universe,

28:22.000 --> 28:25.280
I guess. What do you mean by a different universe? Well, if the laws of nature were different.

28:25.840 --> 28:28.800
Yeah, okay. So if the laws of physics were different.

28:30.560 --> 28:38.800
Well, I guess my I guess I dislike isms. I mean, I'm an anti ismist, or rather, I'd say I'm not an

28:38.800 --> 28:47.440
ismist. But if I were to but I do sort of subscribe broadly to functionalism, I suppose. So I guess

28:48.720 --> 28:58.160
I guess I what do I mean by that? I mean, what I mean is, I mean, I really dislike saying that I

28:58.160 --> 29:07.520
subscribe to these to these isms. So what I really mean by that is that is that I imagine that a

29:07.520 --> 29:12.560
system that is organized in a particular way functionally in terms of its information processing.

29:12.560 --> 29:18.720
And if that system is in is embodied in the broadest sense, and, you know, and meets lots of

29:18.720 --> 29:24.560
other prerequisites, then it's likely to behave in a way where I'm going to naturally use the word

29:24.560 --> 29:30.240
conscious to describe it, perhaps, and where I'm going to treat it like a fellow conscious creature.

29:30.800 --> 29:37.200
So, so, so it's so, you know, ultimately, it's I think it's about the kind of organization you need

29:37.760 --> 29:42.400
to give rise to the behavior you need to talk about thing, the thing in a certain way.

29:43.520 --> 29:47.120
My question today, because I posed this question to Chalmers last week, because he's also a

29:47.120 --> 29:51.600
functionalist. And I agree with the degree of functionalism describing intelligence,

29:51.600 --> 29:55.760
but less so with consciousness, you know, there's not a Turing test for consciousness, for example.

29:56.240 --> 30:01.440
But the thing is with functionalism, we're at risk of doing what you said people do with

30:01.440 --> 30:05.120
large language models, which is anthropomorphizing them, because these functions are intelligible

30:05.120 --> 30:09.360
to us. And then our conception of intelligence becomes somewhat observer relative.

30:13.760 --> 30:21.840
Yes, do I mean, what I observe a relative so you understand these functions, so it's conscious

30:21.840 --> 30:30.240
to you, but not to someone else? Well, so, so, so in all of these cases, I mean,

30:30.240 --> 30:35.920
I think it's about the words that we use in our language to talk about the things. So, so,

30:35.920 --> 30:41.440
so if there's someone else is someone just like us, right, then we have to and if we want to use

30:41.440 --> 30:46.160
the words in different ways. So, so the large language models are a great case in point, right.

30:46.560 --> 30:53.280
So, so suddenly we're arriving at a point where somebody can describe something as conscious.

30:54.240 --> 30:58.480
And others can say that's rubbish, you know, it's not that's not true at all. And so we,

30:58.480 --> 31:03.280
so we've, we've arrived at a point where these philosophically problematic words, which,

31:04.240 --> 31:10.400
which we use in ordinary life quite, quite harmlessly. And we all, you know, we all are in

31:10.400 --> 31:14.960
agreement about how we use the word likes if somebody says, oh, you know, Fred has drank so

31:14.960 --> 31:18.960
much last night, he passed out, he was completely unconscious, you know, I mean, and, or if an

31:18.960 --> 31:23.520
anesthetist says, yes, they, you know, the patient is now unconscious, they can't feel, feel pain.

31:24.880 --> 31:31.760
Or if you say, oh, you know, I, I just wasn't aware, I didn't see the, the cyclist and you know,

31:31.760 --> 31:36.960
that's why I, I hit them, you know, I'm really, it's tragic, but I just didn't see them. And then,

31:37.760 --> 31:42.880
and we, so, you know, so you're saying I wasn't aware of it. So that didn't influence my action.

31:42.880 --> 31:47.360
So there we're using the terms in ways that we all understand. But now we're getting to a point

31:47.360 --> 31:52.640
where suddenly, these words or these concepts are being used, you know, we don't have an way,

31:52.640 --> 31:57.440
we don't have agreement about how to use these words, right? Because it's, there are these exotic

31:57.440 --> 32:02.400
edge cases. Yes. So then the question, I think that you, you're getting is, you know, is there

32:02.400 --> 32:11.840
a fact of the matter there, right? And so I'm very tempted to say the first thing I'm tempted to say

32:11.840 --> 32:16.480
is that I don't think that perhaps is a fact of the matter. Or certainly, I don't, I don't want to,

32:17.120 --> 32:23.360
I don't want to speak as if there is a fact of the matter, but rather, I think we need to arrive

32:23.360 --> 32:27.600
at a new consensus about how we use these words. So that might mean that we extend the words,

32:27.600 --> 32:32.960
we break them apart, like I was suggesting earlier, maybe we need to separate out awareness of the

32:32.960 --> 32:38.560
world from self awareness, from integration, cognitive integration from the capacity for

32:38.560 --> 32:43.280
suffering, because suddenly we have things that where that they don't all come as a package. And

32:43.920 --> 32:47.600
when we need to kind of be a bit more nuanced in the way that we use these words, we need to use

32:47.600 --> 32:52.400
them in new ways. But then there's a kind of transition period, because we don't, you know,

32:52.400 --> 32:56.640
we're all arguing about how to use these words all of a sudden, because we've got weird edge cases.

32:56.640 --> 33:01.280
So there's going to be a time when it'll take a time for language to settle back down again.

33:01.280 --> 33:07.280
So there's a kind of, you know, there's a kind of observer relativness to this for a bit, if you

33:07.280 --> 33:14.800
like, but then, but then there's a kind of consensus needs to emerge, right? But so many

33:14.800 --> 33:21.120
things to explore there. I mean, I'm, I would love it if this platonic idea of concepts were possible.

33:21.680 --> 33:27.680
And what platonic? Because what we're talking about here is reductionism and the, I mean,

33:27.680 --> 33:32.000
the parable of the blind man and the elephant comes in quite nicely. So as Chomsky said,

33:32.000 --> 33:36.400
complex phenomenon beyond our cognitive horizon. And as much as we don't want to,

33:36.400 --> 33:40.720
we use functions derived from behavior to have some common understanding of this thing.

33:40.720 --> 33:44.560
But I wasn't being reductionist, was I? Do you think I was being reductionist?

33:44.560 --> 33:51.120
Well, no. So you said that the language game converges. And in some cases, we will arrive on

33:51.120 --> 33:55.360
a common definition, but like you can bring in Hofstatter as well. Well, not a common definition,

33:55.360 --> 34:01.680
but a common usage, right? So we'll come, so we'll come to use the words, you know, with agreement,

34:01.680 --> 34:06.880
right? So that's what I, and the reason why I mean, I would, and the reason I would balk at

34:06.880 --> 34:11.920
using the word reductionist is because, and that's why I'm a bit resistant to functionalism as well

34:11.920 --> 34:18.640
to any kind of ism is because I just think that that may be the way things are organized when

34:18.640 --> 34:24.640
you take them apart. So, you know, brains, right, when you examine them on the inside,

34:24.640 --> 34:30.800
like animal brains, you might look at how an octopus's brain works. And that might inform

34:30.800 --> 34:35.280
whether you think that it suffers, can experience this pain or not. Or we might break apart, you

34:35.280 --> 34:39.920
know, an AI of the system of the future, right? You know, and we might break it apart and we may

34:39.920 --> 34:45.440
look at its functional organization. And that all is just is grist to the mill of how our language

34:45.440 --> 34:51.360
might change, right? So I'm not, I'm not subscribing to the fact that consciousness is this or this

34:51.360 --> 34:57.600
is that it with some big metaphysical capital letters on the is, right? That's really important.

34:58.320 --> 35:03.680
So the functional organization of these other things, which when we study and look at it,

35:03.680 --> 35:09.120
is all just part becomes part of a conversation that eventually is going to help us to settle on

35:09.120 --> 35:14.560
maybe new ways of talking about these things. I think we agree with each other. I think the

35:14.560 --> 35:18.880
difference is, so with the parable of the blind men and the elephant, all of the men around the

35:18.880 --> 35:24.480
elephant saw something which was part of the truth. And I think that's what we're describing with

35:24.480 --> 35:31.440
the function. So we can all agree on what perception means or what action means. The thing is,

35:31.440 --> 35:35.440
there will be many other functions that will represent a different slice of that cognitive

35:35.440 --> 35:39.520
phenomenon. Yeah, I agree. And I think that's very much true with consciousness, actually, because

35:39.520 --> 35:43.200
there's lots of people coming with kind of like new ideas and new theories. I mean,

35:44.400 --> 35:48.320
Anil Seth, for example, have you had Anil on your on your not yet being right?

35:49.200 --> 35:54.960
So Anil's written this great book called Being You. And Anil brings in a whole kind of new set

35:54.960 --> 36:02.080
of ideas. He's particularly interested in the sort of top down effects on perception, top

36:02.080 --> 36:06.160
down effects on perception. So then he brings in this kind of top down influence and perception

36:06.160 --> 36:11.520
as a big part of things. And then there's Graziano has written this book using this about

36:11.600 --> 36:18.800
his attention schema theory of consciousness. And that's, and, you know, there's a whole set

36:18.800 --> 36:22.640
of interesting ideas there. And I think you're absolutely right. I think there's, I think there's

36:22.640 --> 36:31.120
aspects of all of these things all feed into, you know, all feed into the way, you know,

36:32.320 --> 36:40.080
brains and animals work and all of them feed into the, you know, why they behave the way they do

36:40.080 --> 36:44.560
and why we use those words when we use those words, you know, conscious and aware and so

36:44.560 --> 36:50.560
fascinating. We'll get to your article in a second. But as someone who has such a diverse

36:50.560 --> 36:55.120
and interesting background, who is allowed to ask these philosophical questions? So it reminds me

36:55.120 --> 37:00.240
and Thomas Meckens who is talking about the arguments between neuroscientists and philosophers

37:00.240 --> 37:05.600
about freedom of the will. Yeah. And who gets to decide? Huh. Yeah. Well, what a great question.

37:05.600 --> 37:09.360
You know, I mean, so why should I have any right to speak about any of these things at all? Because

37:09.360 --> 37:18.000
I have no formal training in philosophy. So, so, so who gets to, who gets to dis, well, who gets to,

37:18.000 --> 37:21.600
to, I guess there are two things, right? There's, I guess, I guess there's, there's

37:21.600 --> 37:26.640
in that kind of discussion between the neuroscientists and the, and the philosophers. So there you,

37:26.640 --> 37:30.160
there you're not talking about, you know, the everyday conversation that we're all having as,

37:30.160 --> 37:35.840
as, as humanity or as English speakers, or as Chinese speakers about how we use these, these,

37:35.920 --> 37:40.720
these words. So there it's a much more kind of confined to the, to, to different, two different

37:40.720 --> 37:50.400
schools or disciplines within academia. So there, I mean, I do think that the people who work in AI

37:50.400 --> 37:56.000
and in, and in neuroscience, probably at least should be a bit familiar with, with the philosophical

37:56.000 --> 37:59.840
debates. And you know, you mentioned Descartes earlier on, and you know, you're familiar with,

37:59.840 --> 38:06.240
with just that, that basic kind of, you know, sort of stuff that it was just like philosophy 101,

38:06.880 --> 38:11.440
which people should at least be aware of Descartes arguments and then Chalmers, and the different

38:11.440 --> 38:15.920
kind of perspectives on those sorts of things before they pitch in, you know, at least, I mean,

38:15.920 --> 38:20.160
you wouldn't pitch into neuroscience just by making some up some stuff about brains. If you

38:20.160 --> 38:25.920
hadn't read, you know, the, an introduction to neuroscience. And so, so I think that the scientists

38:25.920 --> 38:32.160
need to, you know, you know, they need to kind of have a, a past to enter the conversation,

38:32.160 --> 38:37.760
which is to have, to have gone through philosophy 101. Yeah, it's so true. We have the same thing with

38:37.760 --> 38:42.560
the, with the ethics folks, actually, because, because we have a lot of them fields of expertise,

38:43.120 --> 38:46.720
and engineers should learn more about ethics. Yeah, absolutely. But when they do have an

38:46.720 --> 38:51.280
opinion about ethics quite, quite often, it's, it's, it's, you know, it can sometimes be a bit

38:51.280 --> 38:57.040
naive. And, and, and, and, you know, at least you should be familiar with the kind of, but, but

38:57.040 --> 39:01.520
that's an interesting and the difficult area in itself. Because of course, you know, you,

39:03.120 --> 39:08.240
as a scientist, it's important that you take responsibility as a scientist and the, and that

39:08.240 --> 39:12.560
you take, you know, some ethical responsibility. But at the same time, you know, you've only got

39:12.560 --> 39:18.240
so much time to become an expert. So, so it's difficult to at the same time, take ethical

39:18.320 --> 39:24.880
responsibility. And yet, you know, even though perhaps you haven't got the time to kind of read,

39:24.880 --> 39:31.280
you know, read up and become an expert on the relevant ethics. So, I mean, perhaps everybody

39:31.280 --> 39:37.360
again, should, you know, get to the entry level, you know, ethics 101. And indeed, many, many courses

39:37.360 --> 39:42.400
teach, you know, ethics, these days, many kind of science and computer science. It's part of,

39:42.400 --> 39:46.800
you know, of any degree these days. So that's a good step, I think. Yeah, there's an interesting

39:46.800 --> 39:50.960
analogy between the functionalism that we were speaking about in consciousness. I mean, even

39:50.960 --> 39:55.360
in our own research domain, we have the function of ethics, and we have linguists, and we have

39:55.360 --> 39:59.360
all sorts of different people. And that is the blind man and the elephant. And, you know, I

39:59.360 --> 40:05.040
tend to believe that even though the views from these diverse folks are inconsistent, diversity

40:05.040 --> 40:10.080
is very important. Oh, incredibly important. Intellectual diversity is, you know, every

40:10.080 --> 40:14.720
kind of diversity is important. And intellectual diversity is immensely important. And having

40:14.800 --> 40:19.680
these conversations, these interdisciplinary conversations is absolutely, you know, essential.

40:19.680 --> 40:24.800
So at least if people are talking to each other, that's a really, really positive thing, I think.

40:24.800 --> 40:29.840
Fantastic. Now, we invited Chalmers on our podcast after Ilya Sootskeva's tweet. And by the way,

40:29.840 --> 40:35.120
Chalmers took a very functionalist approach to talking about consciousness. But I guess,

40:35.920 --> 40:40.080
after that tweet, everyone in the community started thinking about and talking about

40:40.080 --> 40:43.840
consciousness. So maybe let's just start from that tweet. How did you find it?

40:43.920 --> 40:51.280
Sure. Yeah, okay. So the tweet was, so Ilya Sootskeva said, it may be that today's large

40:51.280 --> 41:00.240
language models are slightly conscious. And then I replied, tweeted back in the same sense that

41:00.800 --> 41:07.200
may be a large field of wheat is slightly pasta. And that, in fact, was actually, I mean,

41:07.200 --> 41:11.840
I've got a fair number of Twitter followers, and that was the most engaged tweet I've ever sent

41:11.840 --> 41:17.920
out. And, you know, and, you know, it got celebrity likes, Hannah Fried retweeted it, and, you

41:17.920 --> 41:28.400
know, only as my kind of comment. And so, so, but that does kind of summarize sort of what I think

41:28.400 --> 41:36.080
about, about what he said at that point. But then, but then I after tweeting my, my flippant

41:36.160 --> 41:42.880
response, I then I was violating all my own Twitter rules in in just sending back a flippant

41:42.880 --> 41:49.040
response, because I generally don't do that. I would rather kind of, you know, be professional,

41:49.040 --> 41:53.360
engage professionally. And so I thought it's very important to follow that on with a, you know,

41:53.360 --> 41:58.720
with a little explanation of why, you know, why I thought that it wasn't really appropriate to

41:58.720 --> 42:04.160
speak about today's large language models in those terms. Yeah. And for me, the number one thing is

42:04.160 --> 42:12.560
to do with embodiment. So, so as I see it, embodiment is a kind of prerequisite for for us

42:12.560 --> 42:18.400
being able to use that, that word, use words like consciousness and so on, you know, in the way that

42:18.400 --> 42:24.640
we do in the normal everyday way of talking. So, so, you know, it's only in the presence of something

42:26.160 --> 42:31.360
that that inhabits our world. And by inhabits, I don't mean just has a physical, you know,

42:31.360 --> 42:34.960
like a computer is obviously a physical thing in our world, but inhabits our world means that,

42:35.680 --> 42:42.480
you know, walks around in her own swims or jumps or flies or whatever, but is is is inhabits the

42:42.480 --> 42:50.320
same world as us and interacts with it and, and, and, and you know, and interacts with the objects

42:50.320 --> 42:58.800
in it and with other, with other creatures like ourselves. So, so that to my mind, that is,

42:58.800 --> 43:04.560
that's the, that's so, so, so in that paper conscious exotica, I think I use this phrase

43:04.560 --> 43:09.280
trans channeling Wittgenstein that that only in the context of something that

43:11.040 --> 43:17.440
that exhibits purposeful behavior, do we speak of consciousness. And the way that that's

43:17.440 --> 43:22.960
phrased, there is kind of, you know, so trying to channel Wittgenstein's style of saying things,

43:22.960 --> 43:27.840
because you notice that he's saying that he's making what he's saying is actually he's talking

43:27.840 --> 43:32.720
about the way we use the word. So he's not making a metaphysical claim. This is essential. He's

43:32.720 --> 43:38.800
saying that this is just this is the circumstances under which we use this word. So we use this word

43:38.800 --> 43:44.720
in the context of fellow creatures, basically. And so, so that's the kind of the starting point.

43:44.720 --> 43:49.040
So a large, and of course, we, of course, we talk to people on the telephone and over the

43:49.040 --> 43:54.400
internet and so on. And we don't, you know, we may not, you know, we can't see them or anything.

43:54.400 --> 44:00.880
So we, but, but, but we still, we know that there is, you know, or we assume, we've always been

44:00.880 --> 44:04.720
able to assume up to this point that there is a fellow creature at the other end. And that's the

44:04.720 --> 44:11.040
kind of grounding for kind of thinking that way and using using that word. Now that is absent

44:11.040 --> 44:15.680
in large language models. So large language models do not inhabit the world that we do.

44:19.280 --> 44:24.720
Now, I mean, we have to caveat that because, of course, it's possible to embed a large language

44:24.720 --> 44:30.800
model in a, you know, in a, we always do embed it in a larger system. It might be very simple

44:30.800 --> 44:35.200
embedding. It might be just a chatbot, or it might be much more complicated, like it might

44:35.200 --> 44:40.560
be be part of a system that enables a robot to kind of move around and interact with the world

44:40.560 --> 44:47.120
and take instructions and so on. So there was a great, some great work from Google with their

44:47.120 --> 44:51.760
Palm Seican robot, for example, where there's this embedded large language model. So, so,

44:51.760 --> 44:57.120
so there you're kind of moving in a, in a direction where maybe where these, where these words, you

44:57.120 --> 45:04.080
know, the prerequisites, you know, for, for, well, actually, I want to be careful what I say here.

45:04.080 --> 45:08.800
Sorry. Sorry. Because it's so easy to say something that's going to can be misinterpreted,

45:08.800 --> 45:14.560
right? Yes. But, but we can imagine that, that we can imagine that requirement being met for, for,

45:14.560 --> 45:19.600
for not, not, it doesn't mean it wouldn't be a sufficient condition for using those words,

45:19.600 --> 45:23.360
but at least it would, you'd meet the necessary conditions, right? Yes. But the large language

45:23.360 --> 45:30.240
models by themselves do not meet even, they're not even candidates. Yes, I agree. And we,

45:30.240 --> 45:34.000
there's so many things we can do here, because we can, we can talk about embodiment in general. I

45:34.000 --> 45:39.360
mean, as I understand it, Rodney Brooks kind of started the phenomenon of thinking about a

45:39.360 --> 45:44.160
representationalist view of artificial intelligence or, or rejecting, rejecting a representation.

45:44.160 --> 45:48.080
Rejecting. Yeah. So, so Rodney Brooks thought that we should use the world as its own best

45:48.080 --> 45:52.880
representation, which is absolutely fascinating. Yeah. And then you, maybe you might be thinking

45:52.880 --> 45:57.600
about the embodiment of you in a similar style of Wittgenstein. So it's a matter of complexity,

45:57.600 --> 46:01.760
and it's also a matter of encapsulation, which is fascinating. But, but also just to quote your

46:01.760 --> 46:06.080
paper, you said, although the language model component of SACAN provides natural language

46:06.080 --> 46:10.480
descriptions of low level actions, it doesn't take into account what the environment actually

46:10.480 --> 46:15.280
affords the robots. And there's this whole affordance thing as well. So, I mean, how do you

46:15.280 --> 46:22.800
think about embodiment? So, so the way I see it is that is that the, you know, the one exemplar we

46:22.800 --> 46:30.720
have as of, you know, the end of 2022 of something that we really can describe as, as, as, as

46:30.720 --> 46:37.120
intelligent as generally intelligent is, is the biological brain, biological brains of humans,

46:37.120 --> 46:44.960
but also of other animals. And the biological brain, you know, at its very, it's very kind of

46:44.960 --> 46:52.320
nature is it's there to help a creature to move around in the world, to move, right? It's there

46:52.320 --> 46:59.520
to move, help to guide a creature and help it move in order to help it survive and reproduce.

46:59.520 --> 47:03.920
That's what brains are for. So that's what that from an evolutionary point of view, that's that

47:03.920 --> 47:12.160
they developed in order to help a creature to move. And they are so they and they are, you know,

47:12.160 --> 47:19.360
they're the bit that's comes between the sensory input and the motor output. And as far as you

47:19.360 --> 47:24.640
can cleanly divide these things, which maybe you can't, but I mean, so and so that's that's their

47:24.640 --> 47:30.480
purpose is to intervene in the sensory motor loop in a way that benefits the organism. And

47:30.480 --> 47:38.560
everything else is on built on top of that. So, so, so the capacity to recognize objects in our

47:38.560 --> 47:45.200
environments and categorize them and the ability to kind of manipulate objects in the environment,

47:45.200 --> 47:52.400
pick them up and so on. And all of that is there, you know, initially to help the, the, the organism

47:52.480 --> 48:01.840
to survive. And, and, and, you know, and that's what brains brains are there for. And then,

48:01.840 --> 48:09.600
then when it comes to, you know, the ability to work out how the world works and to do things

48:09.600 --> 48:15.040
like figure out how to gain access to some item of food that's difficult to get hold of, then all

48:15.040 --> 48:21.840
kinds of cognitive capabilities might be required to understand how you get inside a, you know,

48:22.480 --> 48:29.280
a shell or something to get the fruit inside it or something like that, complex cognitive

48:29.280 --> 48:33.760
abilities, that sort of. And then, you know, evolutionary evolution has developed a lot of

48:34.320 --> 48:38.640
more and more and more complex cognitive cognition until we get to language and, you know,

48:38.640 --> 48:43.360
we need to interact with each other because that that's all very much a part of it, the social,

48:43.360 --> 48:48.320
social side of it. And then language is part of that. So as I see it, it's all built on top of

48:49.280 --> 48:54.160
this fundamental fact of the embodiment of the animal and the organism. So that's in the

48:54.160 --> 49:01.280
biological case. So that's sort of our starting point. So, and so that seems to me to be the,

49:01.280 --> 49:06.720
the most natural way to, to understand the very nature of intelligence.

49:07.520 --> 49:10.640
Could I frame it? I think I didn't, I didn't frame it very well. I mean, Melanie Mitchell

49:10.640 --> 49:14.640
recently had a paper out talking about the Four Misconceptions and one of them, of course,

49:14.640 --> 49:19.440
citing Drew McDermott was the wishful mnemonics and the anthropomorphization which, which you've

49:19.440 --> 49:24.160
basically spoken about. But her fourth one was about embodiment. And she spoke about this in

49:24.160 --> 49:28.160
her book as well. She said that one of the misconceptions of AI is that people have this

49:28.160 --> 49:32.080
notion of a pure intelligence, you know, something which works in isolation from the

49:32.080 --> 49:36.720
environment. And you're talking about social embeddedness and embodiment. And I guess my

49:36.720 --> 49:41.920
point with the complexity argument is I'm saying that the brain itself doesn't actually do everything.

49:42.000 --> 49:47.440
It kind of works as part of a bigger system. Oh, I see what you mean. Yes. Okay. Yeah. Yeah. So

49:47.440 --> 49:54.400
there's, so of course, there's, I mean, I noticed in one of your previous interviews with Andrew

49:54.400 --> 50:00.960
Lampinen, you mentioned the three E's frame, we're called four E's these days. And of course,

50:00.960 --> 50:05.280
that's very much part of it is the, is the idea that, you know, there's the extended mind, we use

50:05.280 --> 50:13.600
the environment, you know, as, as, as a kind of memory, for example, we deposit things in the

50:13.600 --> 50:20.000
environment, writing, you know, as an example and so on. And then there's, people talk about

50:20.000 --> 50:24.720
morphological computation, I'm sure you're familiar with that. Well, so that's the idea

50:24.720 --> 50:31.040
that the very shape of our bodies, you know, is, is, is, you know, could. So, so, so sometimes,

50:31.040 --> 50:38.560
you know, the aspects of intelligence are actually outshort outsourced into the physical shape of,

50:38.560 --> 50:44.400
of your body. So where you might think about designing a robot, where you, where you put a lot

50:44.400 --> 50:49.040
of work into the control aspect of it, so that it's, so that it can kind of walk in this very

50:49.040 --> 50:53.840
carefully engineered ways that it's always permanently stable, or alternatively, you can

50:53.840 --> 51:00.160
make a body that is naturally sort of stable, or maybe naturally unstable. And what you do is you

51:00.160 --> 51:06.080
kind of rely on the combination of the physics of it constantly falling with, with a control system

51:06.080 --> 51:12.000
that constantly restores balance. So, so, so, you know, so that's, that's, I mean, this is very

51:12.000 --> 51:17.840
much a Brooks type perspective, and people picked up on Brooks's ideas and extended them in this

51:17.840 --> 51:21.920
sort of way. So I think that's, I think that's a very natural way of thinking.

51:22.480 --> 51:26.080
But in a way that this gets to the, to the complexity argument, because I guess what I'm

51:26.080 --> 51:32.480
saying is that life is much more brittle than anyone realises. You were just pointed to some

51:32.480 --> 51:37.200
sources of brittleness that most people never would have thought of, which is, which is fascinating.

51:37.200 --> 51:43.280
So I think there's a very important relationship between embodiment and language. And this also

51:43.280 --> 51:51.840
brings us back to Wittgenstein as well. So, so for us as humans, language is inherently an embodied

51:51.840 --> 51:59.360
phenomenon. It's, it's, it's something that is, it's a social practice, something that take that

51:59.360 --> 52:05.520
it's a phenomenon that occurs in the context of other language users who inhabit the same world

52:05.520 --> 52:10.640
as we do. And where we have kind of like joint activities, we're triangulating on the same world,

52:10.640 --> 52:15.520
and we're acting on the same world together. And that's the that's what we're talking about when

52:15.520 --> 52:22.400
we use language. So there's this, so that, that's an inherently convincing view of language. And I

52:22.400 --> 52:28.640
think it's deeply profoundly correct view of language. That's, that's what, that's what language

52:28.640 --> 52:33.680
is there for us is so that we can talk about the same things together so that we can, our collective

52:33.680 --> 52:42.640
activity is, is, you know, is, is, is organised to some extent, thanks to language. So that's,

52:42.640 --> 52:48.000
so I think that's a really important perspective on language is Wittgenstein perspective. And, and

52:48.000 --> 52:52.880
embodiment is at the heart of it, embodiment and inhabiting the same world as our other language

52:52.880 --> 52:58.880
users. And, you know, that's the way we learn language, we learn language by being around

52:58.880 --> 53:06.480
other language users like our parents and carers and, and, and peers. And, and that's again a very

53:06.480 --> 53:13.280
important aspect of the nature of human language. Now large language models, they learn language in

53:13.280 --> 53:18.880
a very different way indeed, they do not inhabit the same world as us, they do not kind of sense

53:19.440 --> 53:24.640
the world in the way that we do, they don't learn language from, from other language users,

53:24.640 --> 53:29.680
from their peers in the way that we do. But rather, you know, will we know how large language

53:30.800 --> 53:35.760
models work, there's trained on a very, very large corpus of textual, of textual data.

53:35.760 --> 53:40.720
So an enormous corpus of textual data so bigger than any human is likely to encounter in a, you

53:40.720 --> 53:47.920
know, by the time they become a proficient language user at a young age. And what they're trained to

53:47.920 --> 53:54.720
do is, is not to kind of engage in activities with other language users, but to predict what the

53:54.720 --> 53:59.520
next, you know, what the next token is going to be, which is a very, very different sort of thing.

53:59.520 --> 54:04.160
So they're very, very different sorts of things. And the, and the role of embodiment is really

54:04.240 --> 54:10.480
really important in this difference, I think. Yes, absolutely. When I spoke with Andrew Lampinen,

54:10.480 --> 54:13.840
he's really, really interested in the grounding problems. I mean, would you mind just speaking

54:13.840 --> 54:18.720
about that a little bit before we go into your paper? Yeah, absolutely. Yeah, yeah. So of course,

54:18.720 --> 54:25.760
this goes back to a great paper by Stephen Harnad back in, I think, 1999 or 1998.

54:25.760 --> 54:31.200
Yeah, the one and only. Yeah, the one and only on the symbol grounding problem, it was called.

54:31.200 --> 54:40.640
And, and, and, you know, he does argue broadly that the symbols in AI systems,

54:42.160 --> 54:45.120
the kinds of AI systems he was thinking about at the time were kind of, you know,

54:45.120 --> 54:49.040
sort of expert systems say or something like that. And the symbols there are not grounded,

54:49.040 --> 54:53.680
they're provided by the human programmers and they're just sort of typed in. Whereas for us,

54:53.680 --> 54:59.520
for us, the words we use, those symbols are grounded in, in our activity in the world. So

54:59.520 --> 55:05.440
that when we use the word dog, that's because we've seen dogs. And we've talked about dogs with

55:05.440 --> 55:09.200
other people who've also seen dogs. And we've seen dogs in lots of different circumstances. And

55:09.200 --> 55:16.400
we've also seen cats and, and, and, and, and dog bowls and bones and many other things that all

55:16.400 --> 55:21.760
kind of contextualize that. But all of that, that that is kind of grounded in the real world in

55:21.760 --> 55:27.360
our and in our perception of the things in question. So that so that's this. So that's what

55:27.360 --> 55:32.480
sort of is meant by grounding or that at least that's the original meaning of the word grounding

55:32.480 --> 55:37.920
from Stephen Harled's paper. And I think that's a really, really important concept because,

55:38.880 --> 55:44.800
because, you know, in an important sense, large language models, the symbols that are used in

55:44.800 --> 55:50.480
large language models are not really grounded in that kind of way. Now this, you know, I should

55:51.120 --> 55:56.640
be absolutely clear that large language models are immensely powerful and immensely useful and,

55:56.800 --> 56:02.240
and so that, you know, so, but it's interesting that to what extent the lack of grounding here

56:02.960 --> 56:09.200
that we have in today's large language models, you know, might affect how good they are. So,

56:09.760 --> 56:16.560
so they, so they are prone to kind of, you know, hallucinating and, and, and, and confabulating

56:16.560 --> 56:23.040
and, and if you look at multimodal language models that maybe we'll talk about an image that you

56:23.040 --> 56:27.840
present to them, then, you know, they, you can have a very interesting conversation, but sometimes

56:27.840 --> 56:32.640
they'll go off pieced and start talking about things that are not in the image at all and as

56:32.640 --> 56:39.760
if they are. And that's sort of because due to a kind of, I would say lack of grounding this so that

56:39.760 --> 56:45.440
so the words are not kind of grounded in the images in, in quite the way that we would like. So

56:45.440 --> 56:49.360
that's, it's an important topic of research, I think. Yes, indeed. And although some people do

56:49.360 --> 56:53.520
believe there's this magical word called emergence and possibly some emergent symbol

56:53.520 --> 56:57.280
grounding might be possible, maybe, maybe, shall we just put that to bed before we introduce

56:57.280 --> 57:03.280
your, yeah, sure. Well, well, I mean, emergence is, is, I think is, is a really important concept.

57:03.280 --> 57:10.400
And I, and I think, you know, we do see a tremendous amount of, of very powerful emergence, I think

57:10.400 --> 57:16.480
in today's large, large language models. So, so, so, you know, even though they're, they're, so

57:16.560 --> 57:22.000
they're basically trained to do next word prediction. Or I mean, to be clear, I suppose I

57:22.000 --> 57:25.760
should have made this maybe a bit clearer in the paper, but of course, it's not always next word

57:25.760 --> 57:30.160
prediction, because there are different models learned to actually, you know, predict what's

57:30.160 --> 57:35.120
in the middle of a, of a sequence rather than kind of generally, you know, they're interested in,

57:35.120 --> 57:42.080
in, in, in, in, let's take the next word prediction case as canonical. So, so they're, so they're,

57:42.080 --> 57:46.560
so they're trained to just to do next word prediction. Now, the astonishing thing is,

57:46.560 --> 57:53.920
as I think GPT three showed us, is that, is that just that capability, if it's sufficiently powerful,

57:54.560 --> 58:01.520
can be used to do all sorts of extraordinary things. Because if you provide, you know,

58:01.520 --> 58:07.280
the prompt that describes, you know, describes some kind of complicated thing, you know,

58:07.280 --> 58:15.680
situation, like, you know, I need to tile my floor and my floor is shaped like an L and it's

58:15.680 --> 58:20.320
20 meters long and three meters. Well, you know, you start to describe this thing, you know, and

58:20.320 --> 58:26.880
each, each tile is, is 20 centimeters square, how many tiles will I need? And, and some large

58:26.880 --> 58:32.720
language model will come back and tell you, you need 426 tiles, whatever. And it's correct, right?

58:32.720 --> 58:38.560
Well, this is astonishing, because it was only trained to do next word prediction. And so there's

58:38.560 --> 58:43.680
a kind of emergent capability there. Now, there's a sense, of course, in which it still is just doing

58:43.680 --> 58:50.800
next word prediction, because in the vast and immensely complex distribution of tokens in human

58:51.360 --> 58:59.440
text that's out there, then the correct answer is actually the thing that's most likely to come up.

58:59.520 --> 59:04.480
And that's, but it's got to discover a mechanism for producing that, right? And so that is where

59:04.480 --> 59:09.840
the emergence comes in. And I think that, you know, these capacities are astonishing, the fact

59:09.840 --> 59:14.720
that they, that it can discover mechanisms, you know, emergently that will do that sort of thing.

59:14.720 --> 59:18.560
Yes. And maybe I shouldn't have used the word magic with it with emergence, I'm a huge fan

59:18.560 --> 59:23.520
of emergence. And, and as you say, the decode is trained to predict the next token or the

59:23.520 --> 59:28.640
denoising autoencoders to, to, let's say fill in the gaps in the middle. And I guess there are

59:28.640 --> 59:32.480
different ways of thinking about emergence. So there's weak emergence, which might be thought as

59:33.360 --> 59:38.560
computational irreducibility, or a surprising macroscopic change, or strong emergence when

59:38.560 --> 59:41.520
it's not directly deducible from truths and the lower level to make, you know, lots of things.

59:41.520 --> 59:46.960
Yeah, yeah, the different senses of it, yeah. Exactly. But I guess my point is that remarkably,

59:47.600 --> 59:52.320
it's trained on something quite trivial. So all of this is about convention, right? All of this is

59:52.320 --> 59:57.440
about what's, what, what is the, what is a good way to use words, right? So I don't, so I don't

59:57.440 --> 01:00:02.880
think, you know, I'm not making metaphysical claims about, about, about these things. So it's

01:00:02.880 --> 01:00:08.160
all about what, you know, when is it appropriate to use words, to use certain words? And, and

01:00:08.160 --> 01:00:12.000
because when, when this becomes problematic is when they're philosophically difficult words,

01:00:12.000 --> 01:00:20.000
like beliefs and thinks and so on. Now, when it comes to reasoning, so, so I do think that we,

01:00:20.000 --> 01:00:25.760
I do think it's not unreasonable to, to, to use that term to describe what some of the,

01:00:25.760 --> 01:00:30.640
these models do today. And that's partly because of the content neutrality of,

01:00:31.600 --> 01:00:37.360
of, of reasoning. So, so, so, so a lot of the argument, or a lot of the discussion in the

01:00:37.360 --> 01:00:43.440
paper comes back to this sort of whole embodiment thing, really. And, and I'm, I'm saying, well,

01:00:43.520 --> 01:00:51.840
you know, in the kind of like ordinary way we use the word believes, well, it gets, it gets

01:00:51.840 --> 01:00:58.560
complicated because we do use the word believes in this intentional stance way to, to talk about

01:00:58.560 --> 01:01:05.280
ordinary everyday things. We say, oh, my, my, you know, my, my car clock thinks that it's

01:01:05.280 --> 01:01:10.560
British summertime, you know, you know, because we, and then, but then you'd say, then somebody

01:01:10.560 --> 01:01:15.280
says to you, what you, what you mean, your, your car clock and think, you say, no, obviously,

01:01:15.280 --> 01:01:19.280
I didn't mean that it can think, it's just a turn of phrase, you know, but when we, when we get to

01:01:19.280 --> 01:01:24.320
these large language models, we start to use the words like thinks and believes and so on,

01:01:24.320 --> 01:01:28.800
because they're so powerful, it starts to get ambiguous and yours, and your, and, you know,

01:01:29.520 --> 01:01:33.360
and some people will say, well, actually, I really didn't mean that it can think or that it believes.

01:01:33.360 --> 01:01:39.760
So I'm, so I'm, I'm interested in this, when things get difficult in this respect. And could,

01:01:39.760 --> 01:01:44.240
could you tease apart that work? So you resist anthropomorphic language in terms of belief,

01:01:44.240 --> 01:01:49.920
knowledge, understanding, self or even consciousness, but less so with reasoning. And I, my intuition

01:01:49.920 --> 01:01:55.440
is that reasoning rather depends on those things that I just said before. Well, I, so I think it

01:01:55.440 --> 01:02:02.880
doesn't because, but this is perhaps, this is just maybe in a kind of formal logic sense, because,

01:02:02.880 --> 01:02:09.440
because reason, because logic is content neutral. So if I tell you that every, could you just explain

01:02:09.520 --> 01:02:14.880
what you mean by that? Okay. So, so Lewis Carroll has all these wonderful kind of like nonsense

01:02:14.880 --> 01:02:22.720
syllogisms, right? Where he, where, you know, he says, oh, if all elephants like custard and,

01:02:22.720 --> 01:02:26.720
you know, Jonathan is an elephant, you know, Jonathan likes custard, and, you know, all kinds

01:02:26.720 --> 01:02:32.000
of things like that. And it's all sort of nonsense. And he has this big complex, complicated ones.

01:02:32.000 --> 01:02:40.880
Similarly, I could tell you that all, all sprung forths are plingy, and, and Juliet is a sprung

01:02:40.880 --> 01:02:46.480
forth. Therefore, Juliet is, is a springy, right? And I've no idea what any of those things mean,

01:02:46.480 --> 01:02:52.720
but the, the, but it's because it, because it, for the pure form of the reasoning, you don't have

01:02:52.720 --> 01:03:00.000
to know what they mean. It's just about the logic. So, so in that sense, you know, it just in the way

01:03:00.000 --> 01:03:05.360
that a theorem prover can do logic, then so can a large language model do logic. So in that sense,

01:03:05.360 --> 01:03:10.240
I think large, it is reasonable to use the word reasoning in that logical sense in the

01:03:10.240 --> 01:03:14.080
context of large language models. I don't think that's a problem. Of course, we may think that

01:03:14.080 --> 01:03:18.480
they do it badly, or they do it well, or that's a whole other thing, right? But, but at least the

01:03:18.480 --> 01:03:25.280
word is potentially applicable, right? Yes. Now, belief, I think, you know, I think at the moment

01:03:25.280 --> 01:03:32.880
is a, is a, is a different kettle of fish, because to really have a holder belief, it's, it, it's,

01:03:32.880 --> 01:03:38.480
it's not content neutral, right? So if you, if I believe to use the example in my, in, in my paper,

01:03:39.200 --> 01:03:46.880
if I believe that Barundi is to the south of, of, of Rwanda, well, whether that is the case or not,

01:03:46.880 --> 01:03:52.560
it does depend upon facts that are out there in the world. And then to, to really have a belief,

01:03:52.640 --> 01:03:59.680
at least you've got to be able to somehow try and kind of justify those facts, or at least, and you

01:03:59.680 --> 01:04:05.200
got to be at least built in such a way that you can, you know, interact with the external world

01:04:05.200 --> 01:04:10.960
and do that sort of thing, right? And, and verify that something is true or false or do an experiment

01:04:10.960 --> 01:04:16.400
or, you know, or ask someone or, you know, you've got to go outside yourself, right? We go outside

01:04:16.400 --> 01:04:21.360
of ourselves and, and in order to establish whether something a belief is true or not. And so,

01:04:22.080 --> 01:04:26.880
you've got to at least be capable of doing that. Whereas large language models, the bare bones,

01:04:26.880 --> 01:04:32.560
large language model is not capable of doing that at all, right? Now you can embed it in a larger

01:04:32.560 --> 01:04:36.080
system. This is a really important distinction that I've tried and make over and again in the

01:04:36.080 --> 01:04:41.360
paper. I talk about the bare bones, large language model. So you can take the, so, so, and whenever

01:04:41.360 --> 01:04:46.000
a large language model is used, it's not the bare bones, large language model, which just does

01:04:46.000 --> 01:04:50.560
sequence prediction, but it's embedded in a larger system. When we embed it in a larger system,

01:04:50.560 --> 01:04:55.280
well, that larger system maybe could consult Wikipedia or maybe it could be part of a robot

01:04:55.280 --> 01:04:59.040
that goes and investigates the world. So that's a whole other thing. But then you have to look at

01:04:59.040 --> 01:05:04.800
each case in point and, and, and ask yourself whether it's a, whether, you know, whether we

01:05:04.800 --> 01:05:10.560
really want to use that word in, in, in anger, you know, as in, in its full sense, rather than just

01:05:10.560 --> 01:05:15.760
in the intentional stance sense of a kind of figure of speech. So, and so in the case of, of, of,

01:05:15.840 --> 01:05:22.880
of like chatbots, for example, today's chatbots, not really appropriate, I would say. We're not

01:05:22.880 --> 01:05:28.560
using the word in the way that we, in the full blown sense that we use it, where we talk about

01:05:28.560 --> 01:05:34.480
each other. Fascinating. Okay, well, let's get on to intentional stance. So you said that it's a useful

01:05:34.480 --> 01:05:39.120
way of thinking about artificial intelligence, allowing us to view computer programs as intelligent

01:05:39.120 --> 01:05:42.880
agents, even though they may lack the same kind of understanding as a human. And then you cited

01:05:42.960 --> 01:05:48.160
the case of Bob and Bot. The, the word no was used differently in the two cases. And the word of

01:05:48.160 --> 01:05:52.560
Bob, it was used in the traditional sense. For bot, it was used in a metaphorical sense. So it kind

01:05:52.560 --> 01:05:58.320
of like, it's just distinguishing what it, what it means to know, you know, for humans and, and,

01:05:58.320 --> 01:06:03.600
and for machine. So I think it's, it's useful to think about something like Wikipedia. So,

01:06:04.720 --> 01:06:11.600
so we might ask the question, does Wikipedia know that Argentina has won the 2022 World Cup?

01:06:11.600 --> 01:06:16.960
And just immediately after the event, you know, it probably doesn't, it's not recorded in Wikipedia.

01:06:16.960 --> 01:06:22.960
And somebody might say, Oh, Wikipedia doesn't know yet that the Argentina have won. And so when we

01:06:22.960 --> 01:06:27.280
use the word like that, you know, nobody's going to kind of say to them, say to somebody who uses

01:06:27.280 --> 01:06:31.840
that word, hey, you know, I don't think you should use the word knows there. And you know, that would,

01:06:31.840 --> 01:06:36.000
you know, you should be a bit more sort of sensible. I mean, it's, it's fine to kind of use, I think,

01:06:36.000 --> 01:06:42.400
these kinds of words in this ordinary, every day sense. And we do that all the time. And that's

01:06:42.400 --> 01:06:48.640
sort of, particularly, particularly in the case of computers, that's adopting what Dandenek calls

01:06:48.640 --> 01:06:56.880
the intentional stance. So we're, so we're, we're interpreting something as, as, as, as having beliefs,

01:06:56.880 --> 01:07:02.080
desires and intentions, because it's a kind of convenient shorthand. And especially if you've

01:07:02.080 --> 01:07:05.840
got something that's a bit more complicated, like say your car sat down for something or

01:07:05.840 --> 01:07:11.520
you're, you're, you know, you're sat up on your, on your phone, then it sort of makes, makes sense

01:07:11.520 --> 01:07:16.720
to use those words. It's a, is a convenient shorthand. And it helps us to kind of talk about them,

01:07:16.720 --> 01:07:22.080
right? And without getting overly complicated, without knowing the underlying mechanisms. But

01:07:22.080 --> 01:07:25.280
there's an important sense in which we don't mean it literally. So you know, in the case of

01:07:25.280 --> 01:07:28.880
Wikipedia, you can't, you couldn't go up to Wikipedia and pat it on the shoulder and say,

01:07:28.880 --> 01:07:33.200
hey, Argentina have won. And there's no way, you know, right, I want to be a, you know, and,

01:07:33.200 --> 01:07:40.000
and, and, and, and all, and all the things that, that go with us as humans actually knowing things.

01:07:40.000 --> 01:07:47.120
And it's just a turn of phrase. Now, things get sort of interesting with large language models,

01:07:47.120 --> 01:07:52.560
and with large language model based systems and the kinds of things that we're starting to see

01:07:52.560 --> 01:07:57.280
in the world, because we're starting to get into this kind of blurry territory where it,

01:07:57.760 --> 01:08:03.600
we're blurring between the intentional stance and, and, you know, meaning the meaning it literally.

01:08:03.600 --> 01:08:07.920
And this is where we need to be really, really kind of careful, I think. So at what point does

01:08:07.920 --> 01:08:15.520
do things shade over into where it's legitimate to use that word, you know, literally, in, in the

01:08:15.520 --> 01:08:21.680
context of something that we've built, you know, I don't think we're at that point yet. And, and

01:08:21.680 --> 01:08:28.320
we need to be very careful about, about using the word as if we were using it literally.

01:08:29.920 --> 01:08:34.720
You know, that's the sort of anthropomorphization, because the problem is that we can then

01:08:35.600 --> 01:08:43.280
impute capacities to the thing and, and, and, or even, you know, empathy say that just isn't there.

01:08:44.000 --> 01:08:48.560
Yes. And I suppose we could tease apart knowledge. So it justified true belief from

01:08:48.560 --> 01:08:54.160
knows, because knows that it brings all this baggage of intentionality and agency and anthropomorphization.

01:08:54.160 --> 01:08:56.080
But you had Chomsky, you've had Chomsky on.

01:08:56.800 --> 01:09:01.520
I can tell you a story about that. I mean, the recording messed up. So when we were interviewing

01:09:01.520 --> 01:09:05.680
him, we were only getting bits and pieces. And we had to deep fake him. We had to, we had to

01:09:05.680 --> 01:09:10.080
regenerate the interview. Oh, really? And he was saying in the entire interview, how much he hated

01:09:10.080 --> 01:09:15.120
deep learning and how useless it was. And then we used deep learning to rescue his interview.

01:09:15.440 --> 01:09:21.040
And he gave us his permission to publish it. That is wonderful.

01:09:21.040 --> 01:09:25.120
So it's quite ironic. But no, he always says it's wonderful for engineering,

01:09:25.120 --> 01:09:27.840
but not a contribution to science. Yes, sure. Yeah.

01:09:29.600 --> 01:09:32.880
Yeah. He said, I like bulldozers too. They're good for clearing the snow, but they're not a

01:09:32.880 --> 01:09:36.560
contribution to science. So who else have you had? I mean, you've had a lot of people on.

01:09:36.560 --> 01:09:41.120
I listened to Andrew's one, by the way. It's Andrew Lampinen. Yes, he's great.

01:09:41.120 --> 01:09:44.960
So he's great. Andrew is somebody I do work with quite closely.

01:09:45.840 --> 01:09:50.960
So it was interesting listening to him because Andrew had quite a big influence on this paper,

01:09:50.960 --> 01:09:56.640
by the way. Oh, okay. But I think I might have had a bit of influence on him as well,

01:09:56.640 --> 01:10:02.640
to listening to him. I think so. Because that interview was just after he'd read,

01:10:02.640 --> 01:10:06.640
and he read my paper, gave me lots of comments. And we had a lot of discussion about it.

01:10:07.120 --> 01:10:14.480
And that interview, looking at the recording date, was sort of just after this. And it's interesting.

01:10:14.560 --> 01:10:18.800
I mean, he was very circumspect in some of the things he said. Yeah, it was very interesting

01:10:18.800 --> 01:10:26.240
because I think the influence has maybe gone both ways. Yes. Which is nice. I don't know.

01:10:26.240 --> 01:10:31.600
I mean, I can't be sure of that. I think there's a huge similarity. Yeah. I was thinking that,

01:10:31.600 --> 01:10:36.480
actually, just when you were speaking. But it's funny because we've spent a lot of time arguing

01:10:36.480 --> 01:10:42.640
with each other about it. And I often feel like we're coming from very different perspectives

01:10:43.200 --> 01:10:49.840
on this. But in fact, I think there's convergence, really. What are your areas of disagreement?

01:10:53.360 --> 01:11:00.080
Well, you see, I would have thought that Andrew would have been more on the side of,

01:11:00.080 --> 01:11:07.280
we can do things without embodiment, and without grounding, or to kind of take grounding in a

01:11:08.080 --> 01:11:18.960
more liberal sense. Because some people would talk about grounding, so they say that large

01:11:18.960 --> 01:11:26.160
language models, they are grounded. Prompt Engineering is the process of using prompt

01:11:26.160 --> 01:11:32.320
prefixes to allow LLMs to understand better. So the context and the purpose of a conversation

01:11:32.320 --> 01:11:36.800
in order to generate more appropriate responses. What do you think is going on with Prompt

01:11:36.800 --> 01:11:42.560
Engineering? Yeah. Well, yeah. So you let's probably let slip a phrase there. So the process

01:11:42.560 --> 01:11:47.120
of allowing the models to understand better is what you're better. Of course, I don't think

01:11:47.120 --> 01:11:52.000
guilty as charged. I don't think I don't think that's the right way of characterizing it at all.

01:11:53.280 --> 01:11:59.760
So I mean, I think the whole thing of Prompt Engineering is utterly fascinating. And it's

01:11:59.760 --> 01:12:04.560
something that's entered our world as AI researchers, very prominently, just in the

01:12:04.560 --> 01:12:10.000
last two years. And it's amazing. Of course, we have Prompt Engineering in the context of

01:12:10.000 --> 01:12:15.760
large language models. We also have Prompt Engineering in the context of the generative

01:12:17.360 --> 01:12:24.320
image models as well, like Dali and so on. And that's really fascinating as well, how by

01:12:25.520 --> 01:12:30.640
engineering the Prompt to be just the right sort of thing, you can coax the model into doing

01:12:30.640 --> 01:12:36.400
something which you might not otherwise do. And it's a great example of how alien these things

01:12:36.400 --> 01:12:41.040
are. Because if you were giving a human being the same instructions, then you wouldn't necessarily

01:12:41.040 --> 01:12:49.040
do quite what you do with either an LLM or an image model in order to get it to do the

01:12:49.040 --> 01:12:55.680
thing that you want it to do. You have to kind of get into the zone with these models and figure

01:12:55.680 --> 01:13:00.800
out kind of what strange incantations are going to make it do the things that you want it to do.

01:13:00.800 --> 01:13:06.320
Now, I think an interesting thing is that we may be looking at a moment, a very short moments in

01:13:06.320 --> 01:13:12.880
the history of the field where Prompt Engineering is relevant. Because if language models become

01:13:12.880 --> 01:13:18.080
good enough, then we're not going to need to talk to them in this weird way,

01:13:18.800 --> 01:13:23.680
engineer the Prompt to get them to do what we want them to do. It's going to be a lot easier.

01:13:23.920 --> 01:13:27.440
Anyway, so maybe that will be the case. I mean, that makes a lot of sense that that will be the

01:13:27.440 --> 01:13:36.480
case as they get better. But at the moment, you can use a strange incantation like thinking steps

01:13:36.480 --> 01:13:41.840
and suddenly the large language model will be much more effective on reasoning problems than it

01:13:41.840 --> 01:13:45.920
was if you didn't use the incantation thinking steps. So that's really fascinating. So what's

01:13:45.920 --> 01:13:50.960
going on there? Well, I mean, I think what's going on there is that we have to again bear in mind that

01:13:50.960 --> 01:13:57.680
what the model is really trained to do is next word prediction. But we have to remember that

01:13:57.680 --> 01:14:06.240
it's doing next word prediction in this unimaginably complex distribution. So we have to remember

01:14:06.240 --> 01:14:11.760
that it's not just the distribution of what a single human would, the distribution of

01:14:11.760 --> 01:14:16.480
the sequence of words that a single human will come out with, but of all the sort of text of

01:14:16.800 --> 01:14:22.640
millions of humans on the internet, plus actually a load of other stuff like code and

01:14:22.640 --> 01:14:28.160
things which we don't come out with in ordinary everyday language. Well, people do it deep

01:14:28.160 --> 01:14:36.160
mind a bit, but that's deep. So it's this unimaginably complex distribution. And so I think what's

01:14:36.160 --> 01:14:45.760
happening with prompt engineering is that you're sort of channeling it into some portion of the

01:14:45.760 --> 01:14:54.640
distribution. So you're queuing it up with a prompt. And this kind of context is putting it

01:14:54.640 --> 01:14:59.600
into some portion of this distribution. And that is what's going to enable it to do something

01:14:59.600 --> 01:15:04.560
different than it would have done if you had a different set of words. And that would have

01:15:04.560 --> 01:15:10.240
put it in a different part of the distribution. So you're kind of finding the bit of this unimaginably

01:15:10.240 --> 01:15:15.040
complex distribution. You're finding the bit of it that you want to then concentrate on.

01:15:15.120 --> 01:15:19.280
Yeah, so intuitively, I agree, because I think there's two ways of looking at this. So I agree

01:15:19.280 --> 01:15:22.960
with you that they are statistical language models. I'm also a fan of the spline theory of neural

01:15:22.960 --> 01:15:28.400
networks, which is this idea that you just kind of tessellate the ambient space into these little

01:15:29.040 --> 01:15:34.400
affine polyhedra. And it's a little bit like a locality sensitive hashing table. But that's

01:15:34.400 --> 01:15:38.800
quite, it's quite a simple way of looking at it, because you were talking about emergence before.

01:15:38.800 --> 01:15:43.600
And emergence is all about this paradigmatic surprise, a bit like the mind body dualism,

01:15:43.600 --> 01:15:47.200
if you like, there's something that happens up here, which is paradigmatically

01:15:47.200 --> 01:15:50.880
completely different to what happens down there. So on the one hand, we're kind of saying, oh,

01:15:50.880 --> 01:15:55.680
they're just simple interpolators or statistical models. But on the other hand,

01:15:55.680 --> 01:15:59.600
they really are doing something remarkable up here. So, so which is it?

01:16:00.480 --> 01:16:07.760
Which is it? Well, I mean, it's both, right? So, so, so, you know, if we want to understand

01:16:08.720 --> 01:16:15.040
these models in a in a in a more scientific way, which we surely do, you know, even if we're not,

01:16:15.040 --> 01:16:19.280
even if we're not engineering them in an old fashioned sense of engineering them,

01:16:19.280 --> 01:16:24.480
but, but rather they kind of, you know, emerge from the, from the learning process,

01:16:24.480 --> 01:16:30.000
we still want to reverse engineer them to try and get as great as as as comprehensive

01:16:30.800 --> 01:16:35.520
a scientific understanding of these things as possible. So, so we want to understand it all

01:16:35.520 --> 01:16:39.360
these levels, right? We, of course, the foundation of that understanding is that we

01:16:39.360 --> 01:16:42.720
need to understand the actual mechanisms that we've programmed in there, right? So,

01:16:42.720 --> 01:16:46.320
though, you know, so you've that's essential, you want to, you know, if you want to really

01:16:46.320 --> 01:16:51.600
understand these things, you've got to understand transformer architectures, the different kinds

01:16:51.600 --> 01:16:56.320
of transformer architectures that you've got, the, you know, what happens when you use kind of

01:16:56.320 --> 01:17:01.680
different parameter settings, whether it's sparse or dense, whether it's a decoder only

01:17:01.760 --> 01:17:06.000
architecture, or how you're doing the tokenization, how you're doing the embedding,

01:17:06.000 --> 01:17:09.760
when all of these things are essential to understanding, you know, and that's all at the

01:17:09.760 --> 01:17:14.720
absolutely at the engineering level. So we want to understand all of that. But then we can do a

01:17:14.720 --> 01:17:18.640
whole load of reverse engineering, you know, at another level, and do the sort of thing that

01:17:18.640 --> 01:17:25.200
the people at Anthropic AI have done, for example, with, with these induction heads and, and, and,

01:17:25.200 --> 01:17:30.320
and understanding in terms of transformers in terms of residual streams and induction heads,

01:17:30.320 --> 01:17:33.920
which I think is fabulous work. So that kind of thing is looking, it's still

01:17:34.560 --> 01:17:39.520
quite a low level, but it's kind of the next level up, and explaining a little bit about how these

01:17:39.520 --> 01:17:45.360
things work, and work along those lines, I think is like really essential. And then the more complex

01:17:45.360 --> 01:17:50.640
these things are, the, you know, the heart, the more we need to kind of ascend these levels of

01:17:50.640 --> 01:17:57.120
understanding and, and, and, and, you know, and I hope that we can, but I mean, there's no one

01:17:57.120 --> 01:18:01.120
that is the right one. It's, you want to understand that things are all levels.

01:18:01.120 --> 01:18:04.880
Yeah, different levels of description. You said something before, which really

01:18:04.880 --> 01:18:09.760
interested me. You said when the language models get good enough, maybe we won't need the prompts

01:18:09.760 --> 01:18:14.640
anymore. And I'd love to explore that duality, because it's a similar duality to how we talk

01:18:14.640 --> 01:18:18.720
about embodiment, you know, you can think of the language model being embodied in the prompt in,

01:18:18.720 --> 01:18:23.600
in some sense. So maybe we'll never get rid of the prompt. But just to think about these prompts,

01:18:23.600 --> 01:18:28.960
I think about them as a new type of program interpreter. And there are some remarkable

01:18:28.960 --> 01:18:33.920
examples of scratch pad and chain of thought and even algorithmic prompting for getting

01:18:33.920 --> 01:18:39.200
insane extrapolative performance on lots of, you know, standard reasoning tasks. Yeah, yeah.

01:18:39.200 --> 01:18:43.360
And, you know, these, these models are not Turing machines, they're finite state

01:18:43.360 --> 01:18:47.680
automatics. So there are limits to what we can do. But I guess what I'm saying is the prompt

01:18:47.680 --> 01:18:52.160
seems like it's not going away anytime soon. Yeah. So I think that I don't think the prompt is

01:18:52.160 --> 01:18:57.600
going to go away. But I think that the, and who knows, right? But, but I think that prompt

01:18:57.600 --> 01:19:06.000
engineering as a whole kind of thing in itself, you know, may, it may not be, you know, people

01:19:06.000 --> 01:19:10.320
talk about that as being a kind of a whole new job description as prompt engineer. And so that,

01:19:10.320 --> 01:19:14.640
that as a whole new job description, I'm not quite sure how long exactly that will last because,

01:19:15.760 --> 01:19:20.400
because prompt prompting may be just, you know, interacting with a thing in a much more natural

01:19:21.360 --> 01:19:26.080
language way in the way we would with another person, right? So, you know, I don't, I don't,

01:19:26.080 --> 01:19:31.280
when I, when I, I don't have to kind of think of some peculiar incantation in order to,

01:19:32.640 --> 01:19:40.080
you know, in order to get, you know, my colleagues to kind of help me on, on something or to, you

01:19:40.080 --> 01:19:46.960
know, to cook a meal together with somebody, we just, we just use our natural kind of forms

01:19:46.960 --> 01:19:51.520
of communication. Of course, of course, it does involve, you know, discussion and negotiation,

01:19:51.520 --> 01:19:55.680
but it's in this, it's just the same as we use with other humans, right? So, so it may be that,

01:19:56.240 --> 01:20:01.520
rather than it being a peculiar thing in itself with all these funny phrases that just work

01:20:01.520 --> 01:20:07.600
for peculiar eccentric reasons, that it may be much more natural. Amazing. Professor Shanahan,

01:20:08.480 --> 01:20:12.400
thank you so much for joining us today. Indeed, and thank you for the invitation. It's been lots

01:20:12.400 --> 01:20:14.400
of fun. Absolute honor. Absolute honor. Why?

