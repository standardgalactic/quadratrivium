WEBVTT

00:00.000 --> 00:05.680
He moved to Indiana University in 1989, where he obtained his PhD in philosophy and cognitive

00:05.680 --> 00:11.360
science, working for the legendary Douglas Hofstadter, by the way. I've got his book here

00:11.360 --> 00:16.880
in the Center for Research on Concepts and Cognition. Douglas, by the way, is one of the most

00:16.880 --> 00:21.360
legendary figures in the AI space. We also had the pleasure of interviewing one of his other

00:21.360 --> 00:27.120
PhD students, Professor Melanie Mitchell. Now, David recently wrote this fascinating book called

00:27.120 --> 00:34.960
Reality Plus. In that book, he discussed the three central philosophical questions, actually.

00:34.960 --> 00:40.320
The reality question, which is to say, are virtual worlds real? His answer to that is yes.

00:40.960 --> 00:45.600
The knowledge question, which is to say, can we know whether or not we're in a virtual world?

00:45.600 --> 00:53.440
His answer to that is no. Also, the value question, which is to say, can you lead a good life

00:53.440 --> 00:59.280
in a virtual world? And his answer to that is a resounding yes. Now, you probably also heard

00:59.280 --> 01:04.800
of this notion of an extended mind, something which David formulated with Professor Andy Clark,

01:04.800 --> 01:11.360
and they described the idea as active externalism, based on the active role of the environment in

01:11.360 --> 01:16.400
driving cognitive processes, or put simply, you might think of your phone as being an extension

01:16.400 --> 01:21.680
of your mind, for example. What was it like to work with Douglas during your PhD? Oh, he was great.

01:21.680 --> 01:31.200
He was so interested in so many things. It was officially, it was an AI lab, and most of the

01:31.200 --> 01:36.640
people there were AI researchers. You mentioned Melanie Mitchell. She was my colleague there.

01:36.640 --> 01:44.800
Bob French, who's done important work on AI, Gary McGraw, Jim Marshall, Wong-Pay, and others.

01:45.760 --> 01:49.920
I was the only philosopher. People were interested in so many things, whether it was,

01:50.640 --> 01:59.920
we had workshops on humor, or creativity, on mathematics, on politics, on everything,

01:59.920 --> 02:06.800
as well as all the stuff on AI, analogy, concepts. It was also a very exciting time to be there,

02:06.800 --> 02:17.440
because this was one of the, in the boom and bust cycle of machine learning and of neural

02:17.440 --> 02:24.240
networks. This was one of the boom periods, the early 1990s. The PDP books had just come out,

02:24.240 --> 02:31.360
parallel distributed processing by Rubble Hart, McClelland, and Hinton. There was so much excitement

02:31.360 --> 02:39.120
about the capacities of neural networks. I ended up writing a few papers on machine learning

02:39.120 --> 02:44.720
back then. One was on the evolution of learning systems. One was on getting machines to learn

02:45.360 --> 02:54.080
structural generalizations. All with fairly basic neural networks. A few years after that,

02:54.080 --> 02:59.760
the bottom fell out of neural networks for another 15 years or so, but this was a very

02:59.760 --> 03:04.480
exciting period to be there. One thing about Doug that you don't quite get from his books,

03:06.720 --> 03:11.280
his books, he's so enthusiastic about everything. He's not into all these ideas and so on, but it

03:11.280 --> 03:18.000
turns out there's 5% of things in the world or 1% that he's enthusiastic about. The other 99% he hates.

03:20.400 --> 03:27.040
He's like, he likes certain approaches to AI he loved, but even neural networks, he was like,

03:28.160 --> 03:35.520
he was not a big fan of most research in neural networks back then. He said,

03:35.520 --> 03:40.560
I think it was a, this is a bandwagon or even he said a bad wagon. Likewise for philosophy,

03:40.560 --> 03:44.320
there's bits of philosophy he loves. There's a lot of it. There's a lot of it he doesn't like.

03:44.320 --> 03:50.160
So maybe in person, you get more of a, more of that very opinionated side, but really he's a,

03:51.520 --> 03:56.080
he's just, you know, that book Go to Lesha Bark was just, I mean, it was what drew me into philosophy

03:56.080 --> 04:01.680
and AI as a teenager and it's so rich. And you go back to that book, there's still so many ideas

04:01.680 --> 04:05.920
there. The Mind's Eye is another book he edited with Dan Dennett. It was reading the Mind's Eye

04:05.920 --> 04:11.280
that really first got me thinking about issues about consciousness and the mind-body problem.

04:11.280 --> 04:15.840
So yeah, he's a very rich thinker. Yeah. Well, thank God for Douglas Tostata.

04:15.840 --> 04:21.040
Yeah, I love the Mind's Eye. It has one of the, one of the eeriest stories of all. What was it?

04:21.040 --> 04:26.080
The riddle of the universe and its solution where there's, there's some image or some text that if

04:26.080 --> 04:31.600
you read it, it causes your brain basically to core dump. And so it becomes this infectious thing

04:31.600 --> 04:35.920
where anybody who goes into that office and reads that thing, they just crash and they never,

04:35.920 --> 04:40.320
they go into a coma and they never wake up again. And so, you know, there's a huge investigation

04:40.320 --> 04:44.560
to figure out what's going on and more and more people keep going into comas, you know, because

04:44.560 --> 04:50.400
of this crazy ideas in that book. Could I just pick up on this point because you were saying that

04:50.400 --> 04:54.640
Douglas Tostata turned his nose up at Neural Networks. And I hope he still would actually,

04:54.640 --> 04:58.480
and Melanie Mitchell certainly does. And I mean, Douglas had that paper out called

04:58.480 --> 05:03.520
The Core of Cognition. He was always huge about this idea of the primacy of analogy making.

05:03.520 --> 05:08.000
And actually, there's a few researchers today that are mirroring that, that idea that Francois

05:08.000 --> 05:12.880
Chollet, for example, he says intelligence is literally sensitivity to abstract analogies.

05:12.880 --> 05:17.040
It's not memorizing the internet. So I think, thank God that we do have people out there

05:17.040 --> 05:21.680
thinking slightly differently because the modus operandi today is that we need to build a big

05:21.680 --> 05:26.240
hash table of everything. And even the way we formalize intelligence, as we were just saying

05:26.240 --> 05:31.840
previously, we're not really paying attention to why or how the things do what they do. We're

05:31.840 --> 05:35.760
just looking at the behavioral output bit like the Turing test, as long as it looks and smells

05:35.760 --> 05:42.720
like a duck, then it must be a duck. So thank God for Douglas Tostata. Yeah, I think he's always been

05:43.360 --> 05:48.880
at the same time an enthusiast about the possibility of AI while being somewhat skeptical

05:48.880 --> 05:56.000
about the capacities of existing AI and about the kind of hype that suggests that AI might just be

05:56.480 --> 06:01.680
10 or 20 years around the corner. So in the early 90s, I think, yeah, that was,

06:03.040 --> 06:08.080
that was especially, especially natural. Back around then, people would say a year spent working

06:08.080 --> 06:13.360
in AI is enough to make you believe in God. It was so hard to get anything even like

06:15.680 --> 06:20.560
any kind of intelligence out of an AI system. And I think, you know, Doug was equally skeptical

06:20.560 --> 06:26.720
of both the symbolic and the connectionist on neural network approaches back then.

06:27.840 --> 06:32.880
I think ultimately his sympathies lay with it on the neural network side of things,

06:32.880 --> 06:37.200
with the idea that intelligence could in principle bubble up from a million, you know,

06:37.200 --> 06:42.000
from 100 billion separate little interactions, intelligence could bubble up from there. But

06:42.000 --> 06:47.520
I still think he'd be inclined to think that current approaches are too statistical, too simple,

06:47.520 --> 06:53.760
and so on. That said, you have to look at the, at the progress in machine learning over the last

06:54.800 --> 06:58.400
10 years. And it's been amazing and surprising. And I think even somebody like,

06:59.280 --> 07:02.800
even people like Melanie or like Doug are going to have to say this has been

07:02.800 --> 07:08.400
something they did not expect. And that they did not predict. So I actually, I was back in

07:08.400 --> 07:14.480
Indiana just over two years ago, just before the pandemic got going February 2020. And

07:15.440 --> 07:18.720
I don't know, maybe that was before GBT three, but still there've been all these amazing

07:19.840 --> 07:24.880
developments in machine learning over the last few years. I asked Doug about this,

07:24.880 --> 07:28.720
and what do you make of this? And because he's on the record of saying, you know,

07:28.720 --> 07:33.360
there will be AI eventually, but it's going to have to be involve all these new kinds of complexity,

07:33.360 --> 07:37.680
not not something simple like this. And he says, Yeah, well, this is, this is troubling.

07:37.680 --> 07:41.760
This is concerning, you know, it could be, I don't know yet, but it could be that I was wrong.

07:42.320 --> 07:47.440
It could be there are simpler passes, paths to AI. And his attitude was that would be very

07:47.440 --> 07:52.400
disappointing. It turned out that you could actually train up an AGI, just using those

07:53.120 --> 07:57.440
simple methods to human levels. That would make, I think Doug's view was that would make kind of

07:57.440 --> 08:04.560
human level intelligence less, less grand and remarkable than, than he had thought. Well,

08:04.560 --> 08:09.360
actually, back in, back in Goethe-les-Sherbach, I think he said that even to have a machine that

08:09.360 --> 08:14.000
could beat a human at chess, it would have to be good at everything would be a good composer,

08:14.000 --> 08:19.040
it could tell jokes and so on. Okay, that one, that view got rolled out back in the,

08:19.040 --> 08:27.920
back in the 90s. So the question is, you know, is this, is this ever growing progress of just,

08:27.920 --> 08:31.200
of the kind of machine learning that says just throw a whole lot of compute,

08:31.200 --> 08:36.240
and a whole lot of data at it, and see what happens. Is that eventually going to get us to

08:36.320 --> 08:39.840
human level intelligence? Or is it, is it just going to get us so far

08:40.800 --> 08:44.560
with, and there's going to be principled limitations? I've always been on the side of,

08:45.360 --> 08:50.560
they'll probably only get us so far. But I have to say those principled limitations,

08:50.560 --> 08:57.840
those obstacles that have not yet been conquered are getting smaller and smaller. And the progress,

08:57.840 --> 09:01.120
if the progress of the last five or 10 years continues for another five or 10 years,

09:01.680 --> 09:06.960
then who's to say what's going to be left? Yeah, there was a fascinating anecdote in,

09:06.960 --> 09:12.400
in Melanie's book about how she and Hofstadter went to the Googleplex one time. And basically,

09:12.400 --> 09:17.360
as you said, Douglas was terrified that intelligence might be disappointingly simple to

09:17.360 --> 09:22.800
mechanize because he felt of the mind of Chopin as being infinitely nuanced. And just,

09:22.800 --> 09:26.960
just the incredible process that must have gone through his mind when he, when he produced his

09:26.960 --> 09:32.640
music. But I wanted to, and just quickly, by the way, you said that there was a conception in,

09:32.640 --> 09:38.080
in the 70s that task specific skill was what was required for intelligence or a collection of,

09:38.080 --> 09:44.160
of specific skills. And, and now the mindset is much more towards task acquisition efficiency

09:44.160 --> 09:49.440
and generalization. But I wanted to just quickly pick you up on the so-called intelligence

09:49.440 --> 09:54.400
explosion question. So this is a subject which Nick Bostrom has popularized after his book,

09:54.480 --> 09:59.680
Superintelligence. Personally speaking, we're not particularly sympathetic to this view. And

09:59.680 --> 10:04.560
Saigre Francois-Labe, he said in a blog post recently that this line of reasoning represents

10:04.560 --> 10:09.040
a misunderstanding of intelligence. He said that in his opinion, intelligence is situational.

10:09.680 --> 10:13.840
He said that our environment puts a hard limit on our individual intelligences. He said that

10:13.840 --> 10:18.720
most of our intelligence is not in the brain, it's externalized as civilization. And that an

10:18.720 --> 10:23.840
individual brain cannot implement recursive intelligence augmentation like a Godel machine.

10:23.840 --> 10:28.560
He also said that there are already many examples of recursively self-improving systems.

10:28.560 --> 10:32.880
Even personal investing, for example, is a recursively self-improving system. The more

10:32.880 --> 10:38.640
money you have, the more money you make. Anyway, so Bostrom described a thought experiment in 2003.

10:39.280 --> 10:43.280
I'm sure you've heard of this. The scenario describes an artificial, you know, like a very

10:43.280 --> 10:47.840
advanced artificial intelligence task with manufacturing paper clips. If such a machine

10:47.840 --> 10:52.000
were not programmed to value human life, then given enough power over its environment,

10:52.000 --> 10:56.240
it would try to turn all the matter in the universe, including human beings,

10:56.240 --> 11:01.040
into paper clips or machines which could manufacture paper clips. Do you think we might

11:01.040 --> 11:05.440
be on the precipice of being turned into paper clips, as Bostrom famously described in his

11:05.440 --> 11:10.800
thought experiment? Yeah, look, it's there's two different issues here. One is, will we get to

11:10.800 --> 11:17.520
some kind of much greater than human superintelligence relatively soon by some kind of intelligence

11:17.520 --> 11:23.680
explosion process? And second, if that happens, are there major dangers around? Yeah, I wrote

11:23.680 --> 11:29.360
about both of these things back in 2009. I had an article called, yeah, the Singularity

11:29.360 --> 11:35.360
of Philosophical Analysis, where I tried to take this line of reasoning for an intelligence

11:35.360 --> 11:42.240
explosion through recursive, through basically through recursive design of ever more sophisticated

11:42.240 --> 11:46.480
AIs. I tried to take that and turn it into an argument. I mean, the classic statement of this

11:46.480 --> 11:53.520
comes from I.J. Goode, the statistician and philosopher back in 1965 on the design of an

11:53.520 --> 11:58.800
ultra intelligent machine where he puts the basic idea right there that once you've got a machine

11:58.800 --> 12:06.560
which is smarter than a human, it will be able to design a machine which is smarter still,

12:06.560 --> 12:12.400
and then you're going to get recursive, runaway explosion of intelligence. I tried to analyze

12:12.400 --> 12:18.480
that to set out that article, that argument in as much detail as I could, analyze where it could

12:18.480 --> 12:22.880
go right, where it could go wrong, what the possible obstacles would be, and it's a long story.

12:23.840 --> 12:30.160
If anyone wants to look it up, it's out there on my website. But I in the end became convinced

12:30.160 --> 12:36.640
this is a pretty powerful argument. There's only so many ways it could go wrong. I think it's

12:36.640 --> 12:41.600
important that not every recursive augmentation process is going to lead to an intelligence

12:41.680 --> 12:46.880
explosion. It could easily bottom out, could asymptote before human intelligence. But I do

12:46.880 --> 12:51.200
think that once we start from greater than human intelligence, we have to find some way to get

12:51.200 --> 12:56.080
to greater than human intelligence first. This explosion won't get you that. But once you get

12:56.080 --> 13:02.240
there, then there's pretty good reason to think things in principle can take off from there.

13:02.240 --> 13:08.880
If intelligence is extended, I'm a big fan of the idea that intelligence is extended into the

13:08.880 --> 13:15.280
environment. But as far as I can tell, all that can in principle be augmented too. We develop

13:15.280 --> 13:21.760
extended systems, which are smarter than humans, and then they'll be able to design even better

13:21.760 --> 13:29.200
extended systems. And we could then have an intelligence explosion of extended intelligences.

13:29.200 --> 13:34.720
So I'm actually, nothing about this gets you to human level intelligence. But once we get to human

13:34.720 --> 13:40.560
level intelligence and a little bit beyond, then I think there's a pretty good case that there's some

13:40.560 --> 13:46.160
kind of potential explosion in the offing. Then the other issue you mentioned Bostrom and the paper

13:46.160 --> 13:52.800
clips is, yeah, what does this mean for the future of humanity? I guess I don't know what I'd say

13:52.800 --> 13:58.480
about the probabilities, but I'd say, yeah, once you have greater than human artificial general

13:58.480 --> 14:04.320
intelligence, then there's many ways that can go wrong for the obvious reasons that such a being

14:04.320 --> 14:10.000
is going to be extremely powerful. The most intelligent beings in the universe tend to be

14:10.000 --> 14:16.400
the most powerful for obvious reasons. Whatever they want, they have the capacity to get. So it's

14:16.400 --> 14:23.600
going to be extremely important that our AGI systems want the right things. That is, they have

14:23.600 --> 14:29.120
the right kind of goals. Or as people put it, these days that they are aligned with human goals.

14:30.000 --> 14:35.760
Because if they're even a little bit misaligned, then there's going to be the capacity for things

14:35.760 --> 14:39.840
to go very badly wrong. I know there are some people who think that the alignment is going to

14:39.840 --> 14:45.680
have to be so precise that, you know, missed by just the tiniest bit and will destroy the universe,

14:45.680 --> 14:51.600
whereas others think it's extremely robust. It may be more robust than that. I'm not

14:51.600 --> 14:55.200
totally sure about that. But I'm certainly on the side of people who think we have to take this

14:55.200 --> 15:01.200
issue extremely seriously. And there is at least potential existential risks here that if AGI is

15:01.200 --> 15:07.200
produced in an unthinking way, perhaps say in a military or a financial context where there's

15:07.200 --> 15:17.920
an AI arms race, and we suddenly have greater than human AIs that can achieve arbitrary goals,

15:17.920 --> 15:24.960
then suddenly it becomes an extremely sensitive matter what their goals are. So I'm certainly on

15:24.960 --> 15:28.160
Bostrom's side when it comes to, yeah, this is something we should take seriously.

15:28.880 --> 15:32.560
But isn't there a bit of, you know, it's a big distance to go from

15:33.760 --> 15:39.280
superior to human intelligence and achieve anything you want. I mean, I'm relatively

15:39.280 --> 15:46.320
intelligent, but I can't achieve flight, you know, by myself without, you know, apparatus to do that

15:46.320 --> 15:51.040
and airplane wings, whatever. I mean, there are physical limitations in the world. And I think

15:51.040 --> 15:55.200
sometimes there's this assumption that intelligence can kind of go to infinity,

15:55.200 --> 16:00.800
where in fact, maybe intelligence itself kind of bottoms out at IQ 1000 or something, there's

16:00.800 --> 16:05.360
just not much, you know, you can do beyond that certain IQ. I mean, isn't there a degree of

16:06.160 --> 16:12.400
kind of speculative, you know, extrapolation that we need to account for there?

16:13.840 --> 16:17.920
I would say this is certainly one way that things could, that the argument could fail,

16:17.920 --> 16:21.440
is if it turns out that basically there are diminishing, there's some kind of intelligence

16:21.440 --> 16:27.680
ceiling, and there's some kind of diminishing returns towards this. Just there is such a

16:27.680 --> 16:33.120
ceiling that we might find that when we make a being which is 10% smarter than us on some scale,

16:33.680 --> 16:40.240
it could only make a being which is 5% smarter than it. And that being will make a machine,

16:40.240 --> 16:45.920
make a being which is only 2.5% smarter than it. And all this will kind of asymptote to some

16:46.880 --> 16:52.000
intelligence ceiling. And I don't know, this turns on very subtle issues about the structure

16:52.000 --> 16:57.680
of intelligence space. I'm rather doubtful there is such an intelligence ceiling, or if there is

16:57.680 --> 17:02.400
one, maybe it's something like, you know, the limits of computability compared to, you know,

17:02.400 --> 17:07.840
hypercomputation that an infinite system could do. But I think that ceiling is so high that there's

17:07.840 --> 17:14.080
room for an awful lot of super intelligence before we get there. But in any case, I would say that,

17:14.080 --> 17:19.040
you know, for the purposes of, say, caution and thinking about the future, I would just turn the

17:19.040 --> 17:23.280
point back on you and say that the thought that there is such an intelligence ceiling is itself

17:23.280 --> 17:28.400
an extremely speculative one. I wouldn't want to rely on this, on this extremely speculative thought

17:28.400 --> 17:33.840
to kind of protect us from, from the, you know, potential risks of AGI in the future. If there's

17:33.840 --> 17:39.840
only a 20% chance there's not such an intelligence ceiling, then this is something that we very

17:39.840 --> 17:44.720
much need to be, to be worrying about. Yeah, I mean, fair enough, it's certainly a risk factor.

17:44.720 --> 17:50.400
It's certainly something that we need to need to keep a handle on. Well, let me ask you about one

17:50.400 --> 17:56.320
specific time there. So I'm thinking you're probably familiar with Carl Friston and, you know,

17:56.320 --> 18:00.960
his free energy principle. And he sends his regards, by the way, we talked to him a couple

18:00.960 --> 18:06.160
weeks back. And, and he wanted to ask you about kind of one line of thinking that he's been exploring

18:06.800 --> 18:12.720
lately. And I want to give you a quote from his 2018 article and my self conscious,

18:12.720 --> 18:18.800
or does self organization entail self consciousness. And he said, the proposal on offer here

18:19.520 --> 18:24.960
is that the mind comes into being when self evidencing has a temporal thickness,

18:25.680 --> 18:31.760
or counterfactual depth, which grounds inferences about the consequences of my action.

18:32.480 --> 18:38.720
On this view, consciousness is nothing more than the inference about my future, namely,

18:38.720 --> 18:45.120
self evidencing consequences of what I could do. What do you think about that, that perspective?

18:46.080 --> 18:50.560
Yeah, I'd have to know more about the connection to consciousness. I know that yeah, Friston is

18:51.280 --> 18:57.040
very has developed very deeply the idea of the mind as a prediction machine, a mind which is

18:57.040 --> 19:05.200
basically set up to, you know, predict whatever signal is coming next. And that's with that one

19:05.200 --> 19:10.640
basic key loss, you know, predict what's next, what's next, what's next, then you get to build these

19:10.640 --> 19:17.760
amazing models of the world with all of these, all of these, these capacities. And that's a

19:17.760 --> 19:23.440
really interesting perspective thinking about the mind and intelligence in general. And it's

19:23.440 --> 19:29.760
got to be at least one huge part of the story, even if it's not the whole story as Friston thinks

19:29.760 --> 19:36.080
it is, but I've never really understood the distinct what this kind of predictive approach has to say

19:36.080 --> 19:42.240
distinctively about consciousness. Because presumably there's a whole lot of different

19:42.240 --> 19:48.640
predictive processes at all kinds of levels of the hierarchy, including at the very early vision

19:48.640 --> 19:54.400
and very late cognition, and the whole mind is engaged in coming up with these predictions,

19:54.400 --> 20:01.600
but only some limited part of it is conscious. What you just said about, yeah, trying to figure out

20:01.600 --> 20:05.920
the predictions consequent on our actions, sounds to me like a very general statement of

20:07.040 --> 20:10.880
what the predictive approach says about the mind in general. And I haven't yet heard what is the

20:10.880 --> 20:16.160
part that corresponds to consciousness. Why, for example, or some representations get to be

20:16.160 --> 20:21.040
conscious where so much of it in the brain is not, I can give you I can give you a bit more

20:21.040 --> 20:27.040
detail, which may be helpful, because we did dig into him with a on a bit. And he said, for one thing,

20:27.040 --> 20:32.800
he expected that perhaps part of your response might might entail or talk about the meta hard

20:32.800 --> 20:38.320
problem. You know, why is it that certain beings, i.e. things like philosophers, and people like you

20:38.320 --> 20:42.960
and me puzzle so much about our qualitative experience. And the argument he makes there,

20:42.960 --> 20:48.320
he says that if we are inference machines that are built to actively self evidence,

20:48.320 --> 20:53.840
then that necessarily entails we need to have a generative model about our experienced world.

20:54.480 --> 20:59.760
And if we have that that generative model about our experience world, our experience world,

20:59.760 --> 21:06.640
then we have to entertain the hypothesis that we are things having a qualitative experience,

21:06.640 --> 21:11.760
along with the alternate to that hypothesis, which is that we're not having qualitative

21:11.760 --> 21:17.040
experiences. And so essentially that the capability to model the world generatively

21:17.600 --> 21:22.960
really requires that we entertain this hypothesis that we're actually having qualitative experiences

21:22.960 --> 21:28.640
or maybe not. And that's why we pontificate about it. Yeah, it's interesting. And I think the meta

21:28.640 --> 21:34.480
problem is a, yeah, as a promising approach is the meta problem is your why do we say and think

21:34.480 --> 21:39.040
the things we do about consciousness, instead of explaining consciousness directly,

21:39.040 --> 21:46.320
let's explain, you know, our internal model of consciousness. And yeah, there's got to be

21:46.320 --> 21:52.000
such a model. So I think this is a promising approach to take. I still don't fully, I mean,

21:52.000 --> 21:56.480
I think if you take the predictive approach, so what you would expect is, is the system would have

21:56.480 --> 22:02.240
many different models, you know, a big complex model of the world at all levels, it doesn't

22:02.240 --> 22:09.360
just correspond to experienced reality, but the models the world way beyond what's experienced,

22:09.360 --> 22:16.640
it would also you'd also expect the model to have a model of the mind to have a model of ourselves

22:16.640 --> 22:22.000
and relation to the world. But what actually happens in the in the human mind is we have,

22:23.120 --> 22:27.280
we have models at all levels, you know, there's like so many different levels of say of

22:27.280 --> 22:34.960
representation, even in the visual hierarchy. And somehow, though, only one of those

22:34.960 --> 22:38.720
levels seems to correspond to consciousness. The question is, why now do we need a distinctive

22:38.720 --> 22:46.560
model of those representations in us, which correspond to conscious experience? One idea,

22:46.560 --> 22:51.040
I think, one idea I quite like is that this could be like a simplification. In fact,

22:51.040 --> 22:56.160
we have millions of layers of representation of the world. But to build all that into our model

22:56.160 --> 23:01.360
of ourselves, and our relation of the world is going to be too complex. So we basically,

23:02.000 --> 23:07.280
we oversimplify by saying, ah, there's this one special relationship we have to the world,

23:07.280 --> 23:13.120
we call it consciousness or experience. And yet we experience certain things and then we use them

23:13.120 --> 23:18.880
to reason about them. And this is massively oversimplified as a model of the mind. But it could

23:18.880 --> 23:24.240
be that that simplification is then what actually gives us the sense that we have this special

23:24.240 --> 23:28.560
thing called consciousness. At least maybe that could explain why it seems to us that we have

23:28.560 --> 23:35.280
some special representations of the world. It's a further question why those conscious

23:35.280 --> 23:41.360
representations should seem to be so ineffable and subjective and hard to explain. And what

23:41.360 --> 23:46.160
in what Carl has written about this, I think he and Andy Clark had some ideas about the meta

23:46.160 --> 23:51.120
problem to try and push on this. Maybe that maybe there'd be certain representations that

23:51.200 --> 23:55.520
we'd have to be especially certain that we have them. Maybe that would give rise to

23:56.320 --> 24:01.680
the Descartes idea that, well, I'm not sure about the world, but I know that I'm thinking.

24:02.640 --> 24:07.840
I think, therefore, I am. And they had some kind of story about how this could get the whole,

24:07.840 --> 24:13.600
I think, therefore, I am certainty in one's own mind going. Anyway, I think it's an interesting

24:13.600 --> 24:16.720
approach and I'll be very cool to see if they can develop it further.

24:16.960 --> 24:21.600
Fascinating. I wanted to dig into this modeling thing. I was even thinking a second ago when

24:21.600 --> 24:26.080
you were talking about intelligence, that straight away you did the Hutter thing and

24:26.080 --> 24:31.040
we're talking about agents performing in environments and so on. And even that is a model.

24:31.040 --> 24:35.280
And of course, we're talking about complex phenomena and the way we model things depends

24:35.280 --> 24:40.640
on the level of analysis. But I'm really fascinated by this idea that some phenomena is so complex

24:40.640 --> 24:45.520
that it cannot be formalized or communicated, almost as if there's a representation problem.

24:45.520 --> 24:49.200
Now, you discussed in your consciousness book whether consciousness itself could be

24:49.200 --> 24:54.000
reductively explained and your knowledge argument, you spoke of this neuroscientist Mary

24:54.000 --> 24:57.600
that had been brought up in a black and white room. She's never seen any colors except for

24:57.600 --> 25:01.440
black and white and shades of gray. She's nevertheless one of the world's leading

25:01.440 --> 25:05.600
neuroscientists specializing in neurophysiology of color vision. She knows everything there is

25:05.600 --> 25:10.240
to know about neural processes involved in visual information processing, about the physics of

25:10.240 --> 25:14.960
optical processes, about the physical makeup of objects in the environment. But she doesn't know

25:15.040 --> 25:19.600
what it's like to see red. No amount of reasoning from physical facts alone will give her this

25:19.600 --> 25:25.520
knowledge. Physical facts about systems do not tell us what their conscious experiences are like.

25:25.520 --> 25:30.800
Now, you're speaking about this phenomenon in respect of the conscious or the phenomenological

25:30.800 --> 25:35.280
experience. But I think it's a much bigger problem of representation with any complex system, right?

25:35.280 --> 25:39.360
So what I find fascinating is that all of us have a conscious experience, but it's completely

25:39.360 --> 25:43.680
ineffable, as you just said, it's impossible for us to communicate it to others. And whenever

25:43.680 --> 25:48.080
we try to do so, we're reaching, right? Just like the blind men in the elephant, we end up defining

25:48.080 --> 25:53.440
some weird abstract motif, right? Chopping off 90% of the truth. The thing that fascinates me is

25:53.440 --> 25:58.080
that we need to have some kind of formalism or reduction in order to communicate, you know,

25:58.080 --> 26:02.560
in order to know or even understand anything. But so often is the case that all of the nuance

26:02.560 --> 26:08.080
and richness of the phenomena is lost in doing so. So I suspect that any formalism of a complex

26:08.080 --> 26:12.160
system might blind us from discovering a much better and richer formalism later because it

26:12.160 --> 26:17.440
kind of frames our thinking in quite a pernicious way in your book. So as I said, you were trying

26:17.440 --> 26:22.480
to separate the phenomenological experience as something that couldn't be described. But do

26:22.480 --> 26:30.240
you think it could be extended to any complex system? Well, we don't. As far as we know,

26:30.240 --> 26:35.920
you know, some complex systems actually have conscious subjective experience. But, you know,

26:36.000 --> 26:42.880
most of them don't. You know, this Mac that I'm using right now is a very complex system,

26:42.880 --> 26:47.840
but not much reason to think that it's conscious despite the complexity of what's going on

26:47.840 --> 26:53.360
within it. So certain kinds of complexity go along with consciousness. But if we were to kind of

26:53.360 --> 26:58.640
return to that meta problem approach for a moment, maybe there are certain kinds of properties

26:59.520 --> 27:04.960
of a complex system that tend to produce reports, for example, that the system

27:04.960 --> 27:12.560
is conscious. So maybe some complex systems have the capacity for a certain kind of direct

27:12.560 --> 27:17.520
self-modeling that corresponds to what we think of as introspection. We have introspection,

27:17.520 --> 27:22.400
which is a way of saying, this is what I'm perceiving right now. This is what I'm thinking

27:22.480 --> 27:28.320
right now. This is what I'm feeling right now. And we build a model of ourselves,

27:28.320 --> 27:32.960
and it may well be that that model is highly oversimplified. You don't have access

27:32.960 --> 27:39.600
to all these facts about ourselves. So perhaps you could tell a story where the kinds of complex

27:39.600 --> 27:44.800
systems that give rise at least to this capacity for introspection are then at least going to report

27:44.800 --> 27:53.200
themselves as being conscious. And maybe that could get at some element, maybe sort of the

27:53.200 --> 28:01.840
ineffability of consciousness. You'd expect to build these very simplified self-models. We wouldn't

28:01.840 --> 28:10.640
know immediately how to extend to other people. I mean, I still think, in principle, you could

28:10.640 --> 28:15.520
take Mary, who knows all about the human brain, and she could come to know all about those models

28:16.080 --> 28:21.680
in other people. But it still seems that she's never actually experienced red for herself.

28:21.680 --> 28:26.080
There's still something really crucial about this objective experience that she doesn't know.

28:26.080 --> 28:30.800
She doesn't know what it's like to experience red, and knowing all about the details of the model

28:30.800 --> 28:36.400
still hasn't told her that. So I think that's still something that needs explaining. Some people at

28:36.400 --> 28:43.120
this point just say that sense of something extra is an illusion. Something extra that the model

28:43.120 --> 28:47.440
hasn't explained is an illusion. But that's really where a lot of the action is at then.

28:50.080 --> 28:53.920
Just quickly, do you think there could be a sense of something extra to intelligence,

28:53.920 --> 29:01.600
as well as consciousness? Probably, yeah, we model our own intelligence with massively

29:01.600 --> 29:10.400
oversimplified self-models that were programmed into us by nature that model us as these agents

29:10.400 --> 29:20.320
with incredible capacities, free will, rationality, reason. It probably, again, it will, yeah, maybe

29:20.960 --> 29:24.800
I talked about consciousness is involving the subjective elements, intelligence is involving

29:24.800 --> 29:30.400
the objective elements. But yeah, we probably have oversimplified models of those conscious

29:32.400 --> 29:38.800
of those behavioral elements as well, perhaps that make us out to be more rational, or more free,

29:39.760 --> 29:47.920
or more capable than we actually are. I wanted to ask, what is an interesting simulation? Is

29:47.920 --> 29:53.280
our universe interesting or not? Because we represent just a pinprick of intelligence. So

29:53.280 --> 29:58.800
should intelligence be more spread out in the eyes of the simulator? Or in the vast majority of

29:58.800 --> 30:03.520
instances, would there just be gas everywhere or a singularity? Maybe stars can't form.

30:04.400 --> 30:10.320
Maybe the interesting phenomena itself is on the boundary between chaos and order,

30:11.040 --> 30:15.440
or between order and disorder, I should say, which is just a tiny sliver. So what do you think

30:15.440 --> 30:20.720
makes an interesting simulation? I don't know. I think it probably depends on your perspective,

30:20.720 --> 30:26.960
and it might, for example, depend on the perspective of the simulators, what they're after. One thing

30:26.960 --> 30:32.400
that a simulator might be doing is just create a whole lot of different universes with different

30:32.400 --> 30:39.600
potential laws of physics that they're simulating just to see what happens. And maybe if they're

30:39.600 --> 30:43.840
interested in, say, life or intelligence, then it could be that they're going to find that, okay,

30:43.840 --> 30:49.360
well, 99% of these simulations don't produce anything like life or intelligence. And yeah,

30:49.360 --> 30:56.080
1% of them produce life, and 0.01% lead to intelligence. So if that's what they're interested in,

30:57.040 --> 31:02.720
in studying, fantastic. But they might be interested in who's to say the laws of physics or

31:02.720 --> 31:08.560
galaxy formation, more generally, totally independent of life and intelligence. So I don't

31:08.560 --> 31:14.640
think there's any single standard of what's interesting. I mean, to me, as a philosopher

31:14.640 --> 31:19.440
interested in consciousness, I'm especially interested in this question of what kinds of

31:19.440 --> 31:26.240
simulations might actually develop conscious beings within them, not least because that's

31:26.240 --> 31:30.880
going to be especially relevant to our situation. If we're in a simulation, it seems we're conscious.

31:31.520 --> 31:37.840
So there's a question about just how this kind of simulation might get set up. But I think this

31:37.840 --> 31:43.440
whole, I mean, already simulations are used in actual practice for a million different purposes

31:43.440 --> 31:48.160
by scientists studying this phenomenon or that phenomenon, by people doing entertainment, by

31:48.160 --> 31:53.040
people doing prediction of the future, by people doing simulation of the past. And I guess when

31:53.040 --> 31:58.640
it comes to simulated universes, all of those sources of interest may themselves be present.

