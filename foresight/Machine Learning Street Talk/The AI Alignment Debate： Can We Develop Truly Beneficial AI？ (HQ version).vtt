WEBVTT

00:00.000 --> 00:07.080
Okay, well in which case let's crack on. So ladies and gentlemen get ready to meet

00:07.080 --> 00:12.000
the cunning maverick of Silicon Valley, the one and only George Hotz. Renowned

00:12.000 --> 00:16.560
for his daring exploits, Hotz commands an enigmatic persona which merges the

00:16.560 --> 00:22.080
technical finesse of Elon Musk and the wit of Tony Stark and the charm of a true

00:22.080 --> 00:26.840
tech outlaw. Now many of you would have or indeed should have seen this man on

00:26.840 --> 00:31.800
Lex's podcast recently for the third time no less. From craftily jailbreaking the

00:31.800 --> 00:36.940
supposedly invincible iPhone to outsmarting the mighty PlayStation 3,

00:36.940 --> 00:41.800
he's proven that no tech fortress is impregnable. Once targeted for his

00:41.800 --> 00:46.560
audacious creativity by Sony with a lawsuit, this hacker wizard stoically

00:46.560 --> 00:50.920
danced past the curveballs thrown by the tech giants all achieved with the

00:50.920 --> 00:55.400
graceful swag of a street smart prodigy. Now when he's not outfoxing major

00:55.400 --> 00:59.920
corporations, you'll find him at the heart of the avant-garde of AI technology,

00:59.920 --> 01:04.360
gallantly trailblazing through the wilds of the tech front here. He's currently

01:04.360 --> 01:08.880
building a startup called Micrograd which is building superfast AI running on

01:08.880 --> 01:14.280
modern hardware and truly he's the James Bond of Silicon Valley minus the

01:14.280 --> 01:19.440
martinis of course. Now please welcome the unparalleled code cowboy, the

01:19.480 --> 01:26.280
unapologetic techno-mancer, George Hots! Whoo! Anyway, also joining us for the big

01:26.280 --> 01:30.920
fight this evening is the steadfast sentinel of AI safety, Conor Leahy.

01:30.920 --> 01:35.960
Undeterred by the sheer complexity of artificial intelligence, Conor braves

01:35.960 --> 01:40.680
the cryptic operations of text generating models with steely resolve. Now

01:40.680 --> 01:44.000
about two years ago Conor took on the Herculean task of safeguarding

01:44.000 --> 01:48.960
humanity from a potential AI apocalypse. His spirit is relentless, his

01:49.000 --> 01:54.560
intellect razor sharp and his will to protect is unwavering. Now drawing on

01:54.560 --> 02:01.840
his contentious claim that we are super, super fucked. Yeah, Conor channels the

02:01.840 --> 02:05.800
urgency of our predicament into his work. Now his startup conjecture isn't just a

02:05.800 --> 02:09.880
glorified tech endeavor but it's a lifeboat for us all racing against the

02:09.880 --> 02:14.400
breakneck speed of AI advancement with the fates of nations possibly at stake.

02:15.040 --> 02:20.120
He's determined to break the damning prophecy and render us super, super saved.

02:20.640 --> 02:25.880
So brace for a showdown as Conor Leahy, the maverick defender of AI's boundaries

02:25.920 --> 02:30.720
strides into the ring. Now the man who declared we are super, super fucked is

02:30.720 --> 02:35.920
here to prove just how super, super not fucked we could be if we make the right

02:35.920 --> 02:40.560
decisions today. So please give it up for Mr. Conor, super, super Leahy. Whoo!

02:41.320 --> 02:44.920
Now Conor, I'd appreciate it if you don't go down in the fourth. I want this

02:44.920 --> 02:48.600
fight to go the distance. Now we're running for 90 minutes this evening.

02:48.600 --> 02:55.280
There'll be a 10 minute openers from, we said hots, didn't we, from Hots first

02:55.600 --> 03:00.240
and then Conor and I'll only step into the ring if the punch up gets two out of

03:00.240 --> 03:03.320
hands and unfortunately we won't be taking live questions today because we

03:03.320 --> 03:07.320
want to maximize the carnage on the battlefield. George Hots, your opening

03:07.320 --> 03:08.040
statements please.

03:09.800 --> 03:13.800
Um, yeah, we're super, super fucked. I think I agree with you.

03:15.440 --> 03:16.480
But that was a short fight.

03:16.520 --> 03:21.960
Yeah, look, I think, okay, so to make my opening statement clear and why maybe

03:21.960 --> 03:26.040
it doesn't make that much sense for me to go first, I think that the trajectory

03:26.040 --> 03:33.880
of all of this was somewhat inevitable, right? So you have humans over time and

03:33.880 --> 03:38.520
you can look at a 1980 human and a 2020 human. They look pretty similar, right?

03:38.560 --> 03:40.680
Ronald Reagan, Joe Biden, you know, that's all the same.

03:41.920 --> 03:48.120
Whereas a 1980 computer is like an Apple II and a 2020 computer is a M1 Max

03:48.120 --> 03:52.480
MacBook, like lines looking like this, right? So you have one line like this,

03:52.480 --> 03:55.720
one line like this, these lines eventually cross and I don't see any

03:55.720 --> 03:59.800
reason that line is going to stop, right? I've seen a few of the other guests

03:59.800 --> 04:04.480
argue something like, well, LLMs can't problem solve, but it doesn't matter.

04:04.520 --> 04:09.760
Like if this one can't, the next one will, whatever you call, I don't believe

04:09.760 --> 04:12.600
that there's a step function. I don't believe that like, oh, now it's conscious

04:12.600 --> 04:17.040
or now it's intelligent. I think it's all on a gradient. And I think this gradient

04:17.040 --> 04:22.080
will continue to go up, will approach human level and will pass human level.

04:23.400 --> 04:28.880
Now, this belief that we are uniquely fucked because of this, the amount of

04:28.880 --> 04:32.680
power in the world is about to increase, right? When you think about power and you

04:32.680 --> 04:35.800
think about, straight up, you can just talk about energy usage. The amount of

04:35.800 --> 04:39.600
energy usage in the world is going to go up. The amount of intelligence in the

04:39.600 --> 04:44.240
world is going to go up. We may be able to do some things to slow it down or

04:44.240 --> 04:48.240
speed it up based on political decisions, but it doesn't matter. The trajectory

04:48.240 --> 04:52.520
is up or major catastrophe, right? The only way it goes down is through war,

04:52.520 --> 04:56.400
nuclear annihilation, bio annihilation, meteor impact, some kind of major

04:56.400 --> 05:01.680
annihilation. That's what's going on. What we can control and what I think is

05:01.680 --> 05:06.120
super important we control is what the distribution of that new power looks

05:06.120 --> 05:13.480
like. I am not afraid of super intelligences. I am not afraid to live in

05:13.480 --> 05:19.720
a world among super intelligences. I am afraid if a single person or a small

05:19.720 --> 05:24.520
group of people has a super intelligence and I do not. And this is where we get

05:24.520 --> 05:30.480
to chicken man. A chicken man is the man who owns the chicken farm. There's many

05:30.480 --> 05:34.760
chickens in the chicken farm and there's one chicken man. It is unquestionable

05:34.760 --> 05:41.280
that chicken man rules. And if you believe chicken man rules because of his size, I

05:41.280 --> 05:46.440
invite you to look at cow man who also rules the cows and the cows are much

05:46.440 --> 05:50.000
larger than him. Chicken man rules because of his intelligence. This is basic

05:50.000 --> 05:53.560
less wrong stuff. Everyone kind of knows this. How the squishy things take over the

05:53.560 --> 06:00.240
world. Look, I agree with Elias Yudkowski all up to Nuke the data centers, right. So I

06:00.240 --> 06:06.400
do not want to be a chicken. And if people decide they are going to restrict

06:06.400 --> 06:11.520
open source AI or make sure I can't get access to the compute and only trusted

06:11.520 --> 06:15.680
people like chicken man get access to the compute. Well, shit, man, I'm the chicken.

06:15.680 --> 06:22.960
And yeah, I don't want to be the chicken. So I think that's my are we fucked? Maybe.

06:23.560 --> 06:28.960
Um, I agree that that intelligence is very dangerous. How can you look at

06:28.960 --> 06:32.160
intelligence and not say it's very dangerous, right? Intelligence is

06:32.160 --> 06:38.600
somehow safe. But things like nuclear bombs are an extremely false

06:38.600 --> 06:43.240
equivalency because what does a nuclear bomb do besides blow up and kill

06:43.240 --> 06:48.040
people? Intelligence has the potential to make us live forever. Intelligence has

06:48.040 --> 06:51.840
the potential to let us colonize the galaxy. Intelligence has the potential

06:51.920 --> 06:59.400
to meet God. Nuclear bombs do not they just blow up. Um, so I think the question

06:59.400 --> 07:03.200
and like, you have things like crypto, which are a clear advantage to the

07:03.200 --> 07:06.120
defender, at least today. And you have things like nuclear bombs, which are

07:06.120 --> 07:13.080
clear advantage to the attacker. AI, it's unclear. I think the best defense

07:13.160 --> 07:16.720
against an AI trying to manipulate me. And that's what I'm really worried

07:16.720 --> 07:19.680
about future psyops, you know, we're already seeing it today with the voice

07:19.720 --> 07:22.480
changer stuff. Like, you're never going to know who's human. The world's

07:22.480 --> 07:28.560
about to get crazy. Um, the best defense I could possibly have is an AI in my

07:28.560 --> 07:33.600
room being like, Don't worry, I got you. It's you and me. We're on a team. We're

07:33.600 --> 07:38.200
aligned. I'm not worried about alignment as a technical challenge. I'm worried

07:38.200 --> 07:42.040
about alignment as a political challenge. Google doesn't like me. Open AI

07:42.040 --> 07:47.280
doesn't like me. But me and my computer, you know, we like each other. We're

07:47.280 --> 07:52.080
aligned. And we're standing against the world that has always since the

07:52.080 --> 07:56.440
beginning of history, maximally been trying to screw you over, right?

07:57.160 --> 08:00.360
Intelligence, people think that one super intelligence is going to come and

08:00.360 --> 08:04.440
be unaligned against humanity. All of humanity is unaligned against each

08:04.440 --> 08:10.400
other. I mean, we have some common values, but really, come on, everyone's

08:10.400 --> 08:13.440
trying to scam everybody. The only reason you really team up with someone else

08:13.440 --> 08:17.720
is like, Hey, man, what if we team up and scam them, right? And what if we

08:17.720 --> 08:22.240
team up, call ourselves America and we, we, we, uh, we build a big army and say

08:22.240 --> 08:27.840
we're free and independent. Yeah. Right. It's that force that has made humanity

08:27.840 --> 08:32.680
cooperate humanity by default. He's very unaligned and has every kind of

08:32.680 --> 08:36.120
belief under the sun. So I'm not worried about AI showing up with a new

08:36.120 --> 08:39.440
belief under the sun. I'm not worried about the amount of intelligence

08:39.440 --> 08:43.520
increasing. I'm worried about a few entities that are unaligned with me

08:43.720 --> 08:48.400
acquiring Godlike powers and using them to exploit me. I think that's my

08:48.400 --> 08:49.000
opening statement.

08:50.680 --> 08:56.920
Cool. Yeah. Thanks. That's, uh, that's, I mean, yeah, I also kind of agree with

08:56.920 --> 09:00.040
you. And most of the things you say, there's a few details I'd like to

09:00.040 --> 09:04.400
dig into there, but for most of the things you say, I do think I agree with

09:04.400 --> 09:08.480
you here. I think it's absolute. Let me just like, start with saying, I

09:08.480 --> 09:13.980
totally agree with you that misuse and like, you know, bad actors, what using

09:13.980 --> 09:19.680
AGI is a horrible, dangerous outcome. That's, that's like, you know,

09:19.680 --> 09:23.480
sometimes the, the, uh, less wrong, you know, crowd likes to talk about X

09:23.480 --> 09:27.520
risk, but also sometimes I've talked about S risk, suffering risks. So things

09:27.520 --> 09:32.680
are worse than death. I believe that you can probably almost only get S

09:32.680 --> 09:37.040
risks from misuse. I don't think you can get S risks. Probably like, you

09:37.040 --> 09:40.960
can, but it's extremely unlikely to get it from like just like raw

09:40.960 --> 09:45.760
misalignment. Like you'd have to like get extraordinarily unlucky. So while I

09:45.760 --> 09:51.280
do it, so I do think, for example, a very, you know, controllable AGI or

09:51.280 --> 09:55.720
super intelligence in the hand of sadistic psychopath is significantly in

09:55.720 --> 09:59.560
a sense worse than a paperclip maximizer. So I think this is something

09:59.560 --> 10:04.400
we would agree on probably. So I think I'm to think of pretty much on board

10:04.400 --> 10:08.640
with you on a lot of things there, where I think things come aboard a bit

10:08.640 --> 10:12.920
of a tale as I think there's two points where I would like to take as my

10:12.920 --> 10:17.400
opening statement, two things I want to talk about. The first one is I want to

10:17.400 --> 10:22.160
talk about the technical problem of alignment. So am I concerned about the

10:22.320 --> 10:26.240
kinds of things like misuse and like small groups of people centralizing

10:26.240 --> 10:30.360
power potentially for nefarious deeds? Yeah, I think this is a very, very

10:30.360 --> 10:32.720
significant problem that I do think about a lot. And that'll be the second

10:32.720 --> 10:35.600
thing I want to talk about. The first thing I want to talk about is that I

10:35.600 --> 10:38.240
don't even think we're going to make it to that point. I don't think we're

10:38.240 --> 10:42.560
going to get to the point where anyone has a super intelligence that's

10:42.560 --> 10:46.880
helping them out. We're going to, if we don't solve very hard technical

10:46.880 --> 10:50.280
problems, which are currently not on track to being solved, by default, you

10:50.280 --> 10:54.240
don't get a bunch of, you know, super intelligence and boxes working with a

10:54.240 --> 10:57.440
bunch of humans. You get a bunch of super intelligence, you know, fighting each

10:57.440 --> 11:01.120
other, working with each other and just ignoring humans. Humans just get cut

11:01.120 --> 11:05.560
out entirely from the process. And even then, you know, it's, you know, whether

11:05.560 --> 11:09.040
one takes over or they find an equilibrium, I don't know, like, you know, who

11:09.040 --> 11:12.280
knows what happens to that point. But by default, I wouldn't expect humans to be

11:12.280 --> 11:15.640
part of the equilibrium anymore. Once you're, once you're the chicken man, well,

11:15.640 --> 11:19.560
why do you need chickens? You know, if, you know, maybe if they provide some

11:19.560 --> 11:22.680
resource for you, the reason humans have chickens is that they make chicken

11:22.680 --> 11:26.600
breasts. I mean, personally, I wouldn't like to be harvested for chicken

11:26.600 --> 11:30.880
breasts, just my personal opinion. I consider this a pretty bad outcome.

11:31.960 --> 11:35.880
But even then, well, as a chicken man finds a better way of chicken breasts or, you

11:35.880 --> 11:39.520
know, modifies himself to no longer need food, I expect the chickens are not going

11:39.520 --> 11:42.160
to be around for much longer. You know, once we stopped using horses for

11:42.160 --> 11:46.440
transportation, didn't go very well for the horses. So that's kind of the first

11:46.440 --> 11:50.520
part of my point that I'd like to, you know, maybe hear your opinions on, hear

11:50.520 --> 11:54.800
your thoughts on, is that I think the technical control is actually very hard.

11:55.080 --> 11:59.480
And I don't think it's unsolvable by any means. I think like, you know, you and

11:59.480 --> 12:02.760
like, you know, a bunch of other smart people work on this for like 10 years, I

12:02.760 --> 12:06.200
think you can solve it, but it's not easy. And it has to actually happen. And

12:06.200 --> 12:10.120
there is a deadline for this. The second point I want to bring up is kind of

12:10.120 --> 12:13.520
where you talk about how humans are unaligned. I think this is partially

12:13.520 --> 12:18.520
definitely true. I think I'm unusually, I am the more optimistic of the two of

12:18.520 --> 12:23.800
us in this scenario, not a role I often have in these discussions, where I

12:23.800 --> 12:27.480
actually think the amount of coordination that exists between humanity,

12:27.520 --> 12:31.800
especially in the modern world is actually astounding. Every single time two

12:31.800 --> 12:36.080
adult human males meet and don't kill each other is a miracle. Have you seen

12:36.080 --> 12:39.840
what happens when two adult male chimps from two different warbands meet each

12:39.840 --> 12:43.560
other? It doesn't go very well. And those are already pretty well coordinated

12:43.560 --> 12:47.320
animals because they can have warbands. What happens when, you know, two male

12:47.600 --> 12:51.640
bugs or, you know, I don't know, sea slugs meet each other, you know, either

12:51.640 --> 12:55.720
they ignore each other or, you know, things go very poorly. This is the

12:55.720 --> 12:59.440
default outcome. The true unaligned outcome, the true default state of

12:59.440 --> 13:03.800
nature is you can't have two adult males in the same room at any time. I saw

13:03.800 --> 13:07.480
this funny video on Twitter the other day where it was like, I know, some

13:07.480 --> 13:11.280
parliament, I think in East Europe or something, and there's this big guy

13:11.280 --> 13:14.200
who's just like going at this politician, he's like in his face, he's like

13:14.200 --> 13:17.760
screaming, he was like going everywhere, and not a single punch will

13:17.760 --> 13:22.080
throw him. No, then no one took out a knife, no one took out a gun. And I

13:22.080 --> 13:27.080
was like, wow, the fact that we're so civilized and we're so aligned to

13:27.080 --> 13:32.080
each other that we can have something this barbaric happen and no one throws

13:32.080 --> 13:36.800
a punch is actually shocking. This is very unusual, even for humans. If you

13:36.800 --> 13:42.960
go back 200 years, punches and probably gunshots would have flown. So this is

13:42.960 --> 13:46.360
not to say that humans have some inherent special essence that we're good,

13:46.600 --> 13:51.480
that we have solved goodness or any means. What I'm saying is the way I like

13:51.520 --> 13:54.840
to think about it is that coordination is a technology, is a technology you

13:54.840 --> 13:58.200
can improve upon. It is you can develop new methods of coordination, you can

13:58.200 --> 14:01.920
develop new structures, new institutions, new systems. And I think it's very

14:01.920 --> 14:04.760
tempting for us living in this modern world to, it's kind of like a fish and

14:04.760 --> 14:08.360
water effect. We forget how much of our life, a lot of our life is built on

14:09.160 --> 14:13.760
atoms, on physical technology, a lot of it's built on digital technology, but

14:13.760 --> 14:20.240
a lot of it is on social technology. And when I look at how does the world

14:20.240 --> 14:24.200
go well? Like, you know, should it be only the special elites get control of

14:24.200 --> 14:27.920
the AI? I'm like, well, that's not really how I think about it. And I think

14:27.920 --> 14:31.320
about it way more is, what is a coordination mechanism where we can

14:31.320 --> 14:35.120
create a coordination selling point or we can create a group, an institution, a

14:35.120 --> 14:41.280
system of some kind that where people will have game theoretic incentives to

14:41.280 --> 14:44.440
cooperate on the system, the results in something that is net positive for

14:44.440 --> 14:49.000
everyone. Because the truth is, is that positive some games do exist. And

14:49.000 --> 14:53.000
they're actually very profitable. And they're very good. And I think if we

14:53.000 --> 14:56.440
can turn, you know, you can turn any positive some game into a net into a

14:56.440 --> 14:59.560
zero or a negative some game pretty easily. It's much easier to destroy than

14:59.560 --> 15:03.680
is to create. But I think it's absolutely possible to create coordination

15:03.680 --> 15:07.520
technology around AI and to build coordination mechanisms that are net

15:07.520 --> 15:12.360
positive for everyone involved. So those would be like my two points. Happy to

15:12.360 --> 15:15.040
dig into anyone's you think would be it'll lead to an interesting

15:15.040 --> 15:20.080
interaction. Sure. So I'll start with two and then go to one. So two, I moved

15:20.080 --> 15:24.760
to Berkeley in 2014. And I threw myself at the Mary cult. I showed up at the

15:24.760 --> 15:32.320
Mary office and I'm like, Hi, I'm here to join your cult. And what I started to

15:32.320 --> 15:41.440
realize was, Mary, and less wrong in general, have a very poor grip on the

15:41.480 --> 15:47.360
practicalities of politics. Very much. I think there was sort of a split, you

15:47.360 --> 15:54.280
know, Curtis Yavin, like neoreaction. This is a spin off of rationality. And it's

15:54.280 --> 15:58.360
a spin off rationality that understood the truth about human nature. So when I

15:58.360 --> 16:01.520
give you that, you give that example of two chimps meeting in the woods and

16:01.520 --> 16:06.480
they're going to fight. If I'm one of those chimps, at least I stand a chance.

16:06.560 --> 16:11.720
Right. He might beat my ass. I might beat his. But if I come up against the FBI,

16:12.640 --> 16:17.280
things do not look good for me. In fact, things so much do not look good for me.

16:17.320 --> 16:24.320
There's no way I'm going to beat the FBI. The modern forces are so powerful that

16:24.320 --> 16:28.960
this is not a, Oh, we've established a nice cooperative shelling point. This is

16:28.960 --> 16:33.320
a, we have pounded so much fear into these people that they would never even

16:33.320 --> 16:39.440
think of throwing a puncher firing a gun. We have made everybody terrified. And

16:39.440 --> 16:43.280
this isn't good. We didn't, we didn't achieve this through some enlightened

16:43.280 --> 16:48.440
cooperation. We achieved this through a massive propaganda effort. Right. It's

16:48.440 --> 16:52.880
the joke about, you know, the American soldier goes over to Russia and it's

16:52.880 --> 16:56.680
like, man, you guys got some real propaganda here. And that the Russian

16:56.680 --> 17:01.000
soldiers like, Yeah, no, I know it's bad, but it's not as bad as yours. And the

17:01.000 --> 17:06.440
American soldiers like what propaganda? And the Russian just laughs, right? So, so

17:06.440 --> 17:11.720
this, this didn't occur because of this occurred because of a absolute tyrannical

17:11.720 --> 17:18.240
force decided to dominate everybody, right? Um, now, Oh, I think so. I think there's

17:18.240 --> 17:21.600
a way out of this. I think there actually is a way out of this, right? And I wrote

17:21.600 --> 17:25.560
a blog post about this called individual sovereignty. And I think a really nice

17:25.600 --> 17:31.080
world would be if all the stuff to live food, water, health care, electricity

17:31.240 --> 17:35.040
were generatable off the grid in a way that you are individually sovereign. And

17:35.040 --> 17:39.360
this comes back to my point about offense and defense, right? If I have a world

17:39.360 --> 17:43.240
where you don't want it to be extreme defense, you don't want every person to

17:43.240 --> 17:47.440
be able to completely insulate them. But you want like, okay, it takes a whole

17:47.440 --> 17:51.200
bunch of other people to gang up to take that guy out, right? Like that's, that's

17:51.200 --> 17:55.960
a good, that's a good balance. And the balance that we live in today is there

17:55.960 --> 18:01.080
is one pretty much a unipolar world. I mean, thank God for China. But you know,

18:01.080 --> 18:05.200
there's one, there's one unipolar world, you got America and where are you going

18:05.200 --> 18:09.880
to run? I'll pay taxes. I don't care if you live overseas, right? So yeah, my point

18:09.880 --> 18:13.560
about the coordination is that if you're okay with solving coordination

18:13.560 --> 18:19.720
problems by using a single, a singleton super intelligent AI to make everybody

18:19.720 --> 18:25.360
cower in fear and tyrannize the future. Sure, you'll get coordination. Yeah, that

18:25.360 --> 18:28.840
works. That works. I'm the only guy with a gun and I got 10 a year. I got a name

18:28.840 --> 18:31.520
that all 10 of you and you can all die or listen to me your choice.

18:31.520 --> 18:36.880
So I'm curious about, so I understand what you're saying. And I think you make

18:36.880 --> 18:41.040
some decent points. But I think I view the world a bit differently from you. And

18:41.040 --> 18:44.720
I'd like to like dig into that a little bit. So like, who do you think is less

18:44.720 --> 18:49.160
afraid? Someone living just a medium person living in the United States of

18:49.160 --> 18:55.720
America, or the medium person living in Somalia? Sure, America less afraid. Well,

18:55.720 --> 18:59.200
that's kind of strange. Somalia doesn't have a government. They have much less

18:59.200 --> 19:02.200
tyranny. You're much more you can just buy a rocket launcher and just like live

19:02.200 --> 19:04.400
on a farm and just like, you know, kill your neighbors and no one's gonna stop

19:04.400 --> 19:08.720
you. So like, how does that interact with your old you? Those who will trade

19:08.720 --> 19:11.400
liberty for safety deserve neither.

19:13.200 --> 19:16.000
That sorry, I don't understand. Could you elaborate a bit more?

19:16.240 --> 19:23.520
Um, in Somalia, you have a chance in America, you do not, right? I am okay. I

19:23.520 --> 19:27.160
would rather live in fear. I would rather be worried about someone shooting a

19:27.160 --> 19:32.960
rocket launcher at me than to have an absolutely tyrannical government. Just,

19:33.120 --> 19:38.480
you know, just just like like like a managerial class. Not saying, by the way,

19:38.600 --> 19:42.120
I agree with you that these things are possible. I agree with you that the

19:42.120 --> 19:45.120
less wrong notion of politics is possible. I would love to live in these

19:45.120 --> 19:51.680
sort of worlds, but we don't. The practical reality of politics is so much

19:51.680 --> 19:57.000
more brutal. And it just comes from a straight up instinct to dominate, not an

19:57.000 --> 20:01.240
instinct, you know, government by the people for the people is branding.

20:02.040 --> 20:06.480
Yeah, I mean, yeah. So to be clear, I very much do not agree with less wrongs

20:06.480 --> 20:10.960
views and politics and a bit of an outcast for how I view how conflict theory

20:10.960 --> 20:14.840
I view politics. But this is, I feel like you're kind of dodging the question

20:14.840 --> 20:17.520
you're just a little bit. Is it like, well, if that's true, why aren't you

20:17.520 --> 20:18.280
living in Somalia?

20:20.840 --> 20:25.600
I know people who've done it, right? It's very hard. It's very hard psychologically.

20:26.120 --> 20:31.120
Okay. So like tigers love charm, it turns out, right? A tiger does not want to

20:31.120 --> 20:34.520
chase down an antelope, right? A tiger would love to just sit in the zoo and

20:34.520 --> 20:39.160
eat the charm, right? And like, it takes a very strong tiger to reject that.

20:39.360 --> 20:42.960
And I'm not that strong. I hope there's people out there who are. I hope there's

20:42.960 --> 20:47.200
people out there who are actually like, you know, I'm just not a weak little

20:47.200 --> 20:50.040
bitch. That's why I don't live in Somalia, right?

20:50.240 --> 20:55.400
Okay. I mean, that's a fair answer, but I am a bit confused here. So you're

20:55.400 --> 21:00.040
saying that living in Somalia would be better by some metric, but you're also

21:00.040 --> 21:05.840
saying you prefer not living in Somalia. So I'm a bit confused because like, from

21:05.840 --> 21:09.520
my perspective, I want to live in a country I want to live in. And that's the

21:09.520 --> 21:14.440
one which I think is better. If I thought another country was better, then I would

21:14.440 --> 21:15.000
just move there.

21:15.080 --> 21:18.560
But let's, the tiger and the chum, I think, is a good analogy, right? Like, if

21:18.560 --> 21:22.920
you have a choice as a tiger, you can live in a zoo and you get a nice size

21:22.920 --> 21:26.600
pen, you know, the zookeepers are not abusive at all. You get fed this

21:26.600 --> 21:30.720
beautiful chopped up food. It's super easy. You sit there, get fat, lays around

21:30.720 --> 21:36.080
all day, or you can go to the wild. And in the wild, you're going to have to

21:36.080 --> 21:40.760
hunt. You might not succeed at hunting. It is just a, you know, it's a brutal

21:40.760 --> 21:46.680
existence. As a tiger, which one do you choose? Now, you say, Oh, well, obviously,

21:46.680 --> 21:50.000
you know, you're going to choose the chum one. Yeah, but do you see what you're

21:50.000 --> 21:51.960
giving up? Do you see?

21:51.960 --> 21:54.240
No, no, could you elaborate a little bit on what I'm giving up?

21:54.520 --> 22:02.680
You are giving up on the nature of tiger. You are effectively, okay, maybe I'll

22:02.680 --> 22:07.960
take this to an extreme, right? In the absolute extreme, the country that you

22:07.960 --> 22:13.120
would most rather live in is the one that basically wire heads you, right? The one

22:13.120 --> 22:16.760
and you can say that, Okay, well, I don't want to be wireheaded, but you know,

22:16.760 --> 22:19.880
there's a, there's a gradient that'll get you there, Gandhi in the pill, or, you

22:19.880 --> 22:25.920
know, if you can live in this country, you can be happy, feel safe and secure all

22:25.920 --> 22:31.920
the time. Don't worry exactly about how we're doing it, you know, but right. I

22:31.920 --> 22:37.080
mean, it takes a very strong person to, it's going to take a very strong person

22:37.120 --> 22:43.040
to say no to wireheading. So I understand. I'll give one more instrumental

22:43.040 --> 22:46.240
reason for living in America versus living in Somalia. If I thought that

22:46.240 --> 22:50.840
America and Somalia were both like steady states, I might choose Somalia. I

22:50.840 --> 22:54.360
don't think that. I think that being here, I have a much better way of

22:54.360 --> 22:59.640
escaping this, of escaping the constant tyranny that we're in. And I think a

22:59.640 --> 23:07.160
major way to do it is AI. I think that AI is, is if I really, if I had an AGI, if

23:07.160 --> 23:10.040
I had an AGI in my closet right now, I'll tell you what I'd do with it. I

23:10.040 --> 23:13.440
would have it build me a spaceship that could get me off of this planet and get

23:13.440 --> 23:17.200
out of here as close to the speed of light as I possibly could and put big

23:17.200 --> 23:20.720
shield up behind me, blocking all communication. That's what I would do if I

23:20.720 --> 23:24.480
had an AGI. And I think that's, you know, the right move. And I have a lot better

23:24.480 --> 23:28.040
chance of building that spaceship right here than I do in Somalia. Right. So I'll

23:28.040 --> 23:29.640
give an instrument. Yeah, that's right.

23:30.200 --> 23:33.480
That's a good instrument. Well, we'll miss you if you leave, though. No, it'll

23:33.480 --> 23:38.040
be real shame. It'll be everyone should do it. Like this is, this is the move,

23:38.040 --> 23:42.360
right? And like, let humanity blow. I mean, look, I agree with you that we're

23:42.360 --> 23:47.800
going to probably blow ourselves up, right? But I think that the path potentially

23:47.800 --> 23:52.720
through this probably looks different from the path you're imagining. I think

23:52.720 --> 23:58.320
that the reasonable position, I'm sorry. Oh, no, I think yeah, maybe we're done

23:58.320 --> 24:00.760
with this point. I can come back and have a response to your first one. I would

24:00.760 --> 24:05.160
like to, if you don't mind, just like fall on one string there as well. So one

24:05.160 --> 24:09.960
of the things you said is like, what will the tiger choose? And so my personal

24:10.200 --> 24:14.360
view of this kind of thing. And I think I want to think about coordination is I

24:14.360 --> 24:19.840
think of things. So you put a lot of view on this like fear based domination and

24:19.840 --> 24:23.040
so on. And I'm not going to deny that this isn't a thing that happens. I'm

24:23.040 --> 24:28.480
German. You know, like, you know, I have living relatives who can tell you some

24:28.480 --> 24:33.480
stories. Like I understand, like I understand. I'm not I'm not denying

24:33.480 --> 24:38.960
these things might be needs. What I'm saying, though, is, okay, let's say

24:40.080 --> 24:43.240
there was a bunch of tigers, you know, you and me and all the other tigers. And

24:43.240 --> 24:47.720
some of the tigers are like, man, fuck, this whole like nature shit is like

24:47.720 --> 24:52.040
really not working for me. How about we go build a zoo together? Who's in? And

24:52.040 --> 24:55.320
then other people like, Yeah, you know, actually, that sounds awesome. Let's do

24:55.320 --> 24:58.760
that. Do you think that's okay? Like you think that would be like a fair

24:58.760 --> 25:03.560
option for them to do? Sure. But that's not where zoos come from. I know. I

25:03.560 --> 25:06.680
know. I'm getting there. I'm getting there. Like, that is not where zoos come

25:06.680 --> 25:14.200
from. Sure. But the this analogy here is, of course, is that this is where a lot

25:14.200 --> 25:17.840
of human civilization comes. Not all of it. I understand that why France was

25:17.840 --> 25:20.920
doing well in the First World War was not because of democracy big nice. It was

25:20.920 --> 25:26.040
because democracy raises large armies. It's I'm very well aware of the

25:26.040 --> 25:31.560
real politic, as the Germans would say, about these kinds of factors. And I

25:31.560 --> 25:36.520
and I fully agree with you that a lot of the good things that we have are not by

25:36.520 --> 25:41.080
design, so to speak. You know, there are happy side effects, you know, capitalism

25:41.080 --> 25:45.640
is a credit assignment mechanism, you know, the fact that also results in us

25:45.640 --> 25:49.080
having cool video games and air conditioning. It's not an inherent

25:49.160 --> 25:56.360
feature of the system. It's it's an execution mechanism. And so totally

25:56.360 --> 26:00.600
grant all of this. I'm not saying that every coordination thing is good. I'm

26:00.600 --> 26:03.960
not saying that, you know, there aren't trade offs. Actually, you were talking

26:03.960 --> 26:06.520
about, I think, aesthetic trade offs. You're like, there's an aesthetic that

26:06.520 --> 26:11.480
the tiger loses. And well, I think personally, aesthetics are subjective. So

26:11.480 --> 26:14.040
I think this is something that different people. So the way I think about

26:14.040 --> 26:18.040
aesthetics is I think aesthetics are things you trade on is, you know, you

26:18.040 --> 26:22.680
might want tigers in the wild to exist. Okay, fair enough. That's a thing you

26:22.680 --> 26:26.760
can want. You know, someone else might want, you know, certain kinds of art to

26:26.760 --> 26:32.360
exist. They might want a certain kind of religion to be practiced or whatever.

26:32.360 --> 26:35.800
These are aesthetic preferences upon reality, which I think are very fair.

26:35.800 --> 26:39.640
So the way I personally think about this morally is I'm like, okay, cool.

26:39.640 --> 26:43.400
How can we maximize trade surplus so you can spend your resources on the

26:43.400 --> 26:48.440
aesthetics you want and I'll spend my resources on the, you know, things I

26:48.440 --> 26:51.880
want. Now, maybe the thing you describe where

26:51.880 --> 26:55.320
everyone just atomizes into their own systems, with their own value system,

26:55.320 --> 26:58.920
with their own aesthetics, completely separate from other, is the best outcome.

26:58.920 --> 27:01.400
Awesome. I think this is completely...

27:01.400 --> 27:04.440
Have you heard the Yodobar manifesto?

27:04.440 --> 27:09.880
I have not. You sure? The problem with this, everyone trades on their own

27:09.880 --> 27:14.280
aesthetics, is you will never be able to actually buy any aesthetics that are in

27:14.280 --> 27:20.680
conflict with the system, right? You won't. The system won't let you.

27:20.680 --> 27:26.360
Okay, by that logic, why do people have free time?

27:26.360 --> 27:29.160
Why don't they work all the time? Why doesn't capitalism extract literally

27:29.160 --> 27:32.680
every minute of them? Why do you think that is?

27:33.480 --> 27:36.840
I think it's because it turns out that we don't actually live in a capitalist

27:36.840 --> 27:40.920
society. I think China is a lot closer to a capitalist society than America.

27:40.920 --> 27:44.760
I think America is kind of communist and I think in a communist society, of

27:44.760 --> 27:47.480
course, you're going to get free time. It turns out that subsidizing all the

27:47.480 --> 27:51.320
homeless people is a great idea, right? If you want to keep power, again,

27:51.320 --> 27:55.000
do some absolute tyrannical mechanism. You do it, right?

27:55.000 --> 27:59.480
So why do we have free time? Well, you think it's some victory of capitalism.

27:59.480 --> 28:02.440
I think it's because we do not live in a capitalist country. I think China is more

28:02.440 --> 28:05.960
capitalist than America. I think it's because we trade on our aesthetics.

28:05.960 --> 28:08.520
I think that different people have different things to contribute to

28:08.520 --> 28:11.400
various systems, not necessarily capitalist or communist thing.

28:11.400 --> 28:15.560
I'm saying it's more energy. In the primordial environment, if you have

28:15.560 --> 28:17.720
to fight literally every single second and spend every

28:17.720 --> 28:21.240
jewel of energy you have to scrounge together another jewel of energy,

28:21.240 --> 28:24.360
you can't have free time. It's not about capitalism. This is about

28:24.360 --> 28:28.280
entropy. This is about these kind of things. We have energy excess.

28:28.280 --> 28:32.520
We have, we've produced systems that allow us to extract more energy for

28:32.520 --> 28:37.640
jewel we put in. And we can spend that extra energy on things such as free time.

28:37.640 --> 28:43.080
And the distribution of energy, power, coordination, whatever you want to call it

28:43.080 --> 28:45.880
is another question. Will you agree or disagree with this?

28:45.880 --> 28:49.720
I mean, I am taking an extreme position when I say that there are definitely

28:49.720 --> 28:53.720
positive sum coordination problems that are solved by governments, right?

28:53.720 --> 28:58.920
It is not all zero sum or negative sum, right? I'm not denying this.

28:58.920 --> 29:05.560
But what I'm saying is it's like, I don't know, man, like the existence of free time.

29:05.560 --> 29:10.040
Well, that's all great when you think you live in this surplus energy world, right?

29:10.040 --> 29:14.920
And maybe we do right now. But if some other country took this seriously, like China,

29:16.040 --> 29:19.880
who's going to win in a war? Who's going to win? Is it going to be the Chinese?

29:19.880 --> 29:24.040
You guys see the Chinese build a building? They got like 400 people there,

29:24.040 --> 29:27.240
and they're all there 24 hours a day, and they're getting the building built.

29:27.240 --> 29:30.600
You ever see Americans build a building? It's six guys, two of them are working,

29:30.600 --> 29:33.160
two of them are shift supervisors, and two of them are on lunch breaks.

29:33.960 --> 29:36.520
Oh, you got your free time. You got your aesthetic preferences.

29:36.520 --> 29:40.200
You deserve to lose in a war, right? This country deserves to lose in a war

29:40.200 --> 29:41.640
if they keep acting the way they're acting.

29:42.200 --> 29:46.200
So I definitely see the point you're making. And this is personally not a thing I want to

29:46.200 --> 29:53.000
defend too far because I'm not a military expert. But I will note that the US has like

29:53.000 --> 29:56.920
37 aircraft carriers, and the Chinese have like two, and Americans are like,

29:57.480 --> 30:01.960
somehow, you know, despite being so lazy and oh, no, they have all this, you know,

30:01.960 --> 30:05.640
all this free time or whatever. Somehow they're still a military hegemon or whatever.

30:05.640 --> 30:09.880
And like they're the biggest rival Russia fighting this backwards water country in

30:09.880 --> 30:13.320
Ukraine suddenly folds and lose like three quarters of the military.

30:13.320 --> 30:20.360
It's what I'm saying is if you have massive hegemony, if you have truly a obnoxious victory,

30:20.360 --> 30:24.600
the way it should look is if you laze around all the time and you look like a fucking idiot and

30:24.600 --> 30:31.320
you still win. Yes. And I'm not talking about Russia. Russia has a GDP the size of Italy.

30:31.320 --> 30:36.680
This is China here. You might say that China has two aircraft carriers in the US has 37.

30:36.680 --> 30:40.520
Why do we have aircraft carriers? Who has more drone building capacity?

30:40.520 --> 30:46.200
The Chinese are the United States. If the future is fought with AI swarm drone warfare,

30:46.200 --> 30:50.120
the Chinese can make, you know, a million drones a day in the US can make.

30:50.840 --> 30:52.840
I don't even know. I think we buy them from China.

30:54.360 --> 30:57.800
Well, I'm not an expert on these kind of logistics. I think I would like to

30:58.440 --> 31:01.800
get back to kind of like the more general. Let's move on from that.

31:01.800 --> 31:05.720
I am not either, but I do believe the Chinese have more manufacturing capacity than the United

31:05.720 --> 31:11.720
States. It seems completely plausible to me. I think things are not lazy and they don't sit

31:11.720 --> 31:14.600
around and have all this free time and aesthetic preferences or something.

31:14.600 --> 31:19.320
I'm a believer that work is life. I mean, at least from my Chinese friends,

31:19.320 --> 31:23.240
I know the Chinese sure do have a lot of inefficiencies. It's just called corruption.

31:23.960 --> 31:27.880
Oh, America has corruption too. You see. Oh, yeah, sure. Well, in Mexico,

31:27.880 --> 31:32.840
the corruption is you have to pay 20 cents to get 20 cents on every dollar for the building

31:32.840 --> 31:38.040
you built, right? Whatever. In America, every dollar is spent absolutely on that building.

31:38.040 --> 31:42.120
You know, we know that because we spent $4 making sure that that first dollar was not

31:42.120 --> 31:48.600
spent correctly. I'm well aware of that. Anyways, I think we mostly agree on this

31:48.600 --> 31:54.040
point actually. And I think it's a matter of degree. What I want to say just for the record,

31:54.040 --> 32:00.040
the U.S. is a uniquely dysfunctional system in the West. I'm German and the German system is

32:00.040 --> 32:04.360
very dysfunctional, but it's like nothing compared to how dysfunctional the U.S. is.

32:04.360 --> 32:08.280
Fully agree with that. I don't think we disagree on that. I think it's a matter of

32:08.280 --> 32:13.080
degree more so than anything. I agree. We've had a comment saying someone's turned the

32:13.080 --> 32:17.320
temperature up a bit too much on the language model. So let's bring it back a tiny bit to

32:17.320 --> 32:22.120
AI safety. But that was a great discussion. Got it. I will end with saying I love America.

32:22.120 --> 32:25.400
I am happy to live here. And there are a lot of things I appreciate about American society.

32:27.560 --> 32:30.360
Great. So do you want to return to like the technical topics or?

32:31.080 --> 32:34.840
Yeah, I think I can return to your first point. And maybe I'll just start with a question.

32:34.840 --> 32:36.280
Do you think there's going to be a hard takeoff?

32:37.880 --> 32:39.560
I don't know, but I can't rule it out.

32:42.040 --> 32:44.280
I can't see how that would possibly happen.

32:46.840 --> 32:50.920
I have a few ideas of how it could happen, but I don't. It's unlikely. It seems like not.

32:51.720 --> 32:56.680
The way I think it could happen is if there are just algorithms, which are like

32:58.120 --> 33:01.800
magnitudes of order better than anything we ever have. And like the actual amount of

33:01.800 --> 33:06.440
computing you get human is like, you know, a cell phone or, you know, like, and then this

33:06.440 --> 33:11.880
algorithm is not deep in the tech tree. We just happen to have not picked it up. And then an

33:11.880 --> 33:14.680
AGI system picks it up. This is how I think it could happen.

33:15.640 --> 33:20.360
Okay, yes, I agree that something like this is potentially plausible where you're saying basically

33:20.360 --> 33:27.560
like the God Shatter is already distributed. It's not a question. It's using all the existing

33:27.560 --> 33:31.480
compute in the world today. It just turns out it was 10,000x more effective or a millionx more

33:31.480 --> 33:36.920
effective than we thought. Yeah, this is seems the most plausible way to be. Or, you know, you mix

33:36.920 --> 33:40.040
lead and, you know, copper and you get a superconductor, you know, something like that.

33:41.400 --> 33:45.720
Even that. I know, I know, I'm not joking. It's going to take so many years to like,

33:45.720 --> 33:50.280
it's not about the discovery, right? Give it 10 years to productionize it, scale up processes,

33:50.280 --> 33:54.280
right? Like these things are, you know, this is something running a company's really taught me,

33:54.280 --> 34:00.840
like it's just going to take a long time. And this is really like, like kind of where my,

34:00.840 --> 34:05.320
I just don't believe in a hard takeoff. I think that they'll be, this is a gasketing I like,

34:05.320 --> 34:08.600
he's a hardware and software progressive, quite similar speeds. And you can look at

34:08.600 --> 34:13.560
factoring algorithms to show this. So it would shock me if there were some, you know, 10 to the

34:13.560 --> 34:20.200
six, 10 to the nine magical improvement to be had. It seems plausible to be like a hard takeoff is

34:20.200 --> 34:25.720
definitely not my main line scenario, my main line scenario. Well, I don't know, maybe you

34:25.720 --> 34:28.760
wouldn't consider this a hard, maybe you would consider as a hard takeoff. This is what I would

34:28.760 --> 34:34.120
describe as a soft takeoff is something like, sometimes the way I like to define AGI is say,

34:34.120 --> 34:39.400
it's something that has the thing that chimps, that chimps don't have and humans do have.

34:39.400 --> 34:45.720
Yeah. So chimps don't go a third to the moon, you know, despite their brain being a third of our

34:45.720 --> 34:50.680
size. So we scaled up things by a factor of three of a primate brain, roughly four or something

34:50.680 --> 34:54.840
like that. And like most of the structures, I'm sure some micro tweaks and whatever, but like

34:54.840 --> 34:59.000
not massive amount of evolutionary pressure, like we're very, very similar to chimps.

34:59.000 --> 35:05.720
And somehow this got us from, you know, literally no technology to space travel in a,

35:05.720 --> 35:12.200
you know, evolutionary, very small pair of time. It seems imaginable to me that something similar

35:12.200 --> 35:17.160
could happen with AI. I'm not saying it will, but like seems imaginable.

35:17.160 --> 35:23.800
Yeah. So I agree with this. I'll come to your point about, you know, you had two regulatory

35:23.800 --> 35:31.720
points, one of them about capping the max flops. And I actually kind of agree with this. I do think

35:31.720 --> 35:37.720
that things could potentially become very dangerous at some point. I think your numbers are way,

35:37.720 --> 35:43.720
way, way too low. I think if your numbers are anywhere near GPT-3, GPT-4, okay, great. We got

35:43.720 --> 35:48.680
a lot of, we got a lot of fast moving guys who work on fiber, even if you start to get Von Neumanns.

35:49.320 --> 35:55.560
Right? We're not talking about a humanity's worth of compute. We're talking about things on par with

35:55.560 --> 36:01.720
a human and a few humans, right? Yeah, they'll run fast, but they're not. Like things get scary

36:01.720 --> 36:07.160
when you can do a humanities training run in 24 hours. Like we're about to burn the same compute

36:07.160 --> 36:13.320
that all 2 million years of human civilization burned. Okay. Now I don't know what starts to

36:13.400 --> 36:18.360
happen. Or I'll put this kind of another way. Language models, I look at them and they don't

36:18.360 --> 36:24.760
scare me at all because they're trained on human training data, right? These things are not, like,

36:24.760 --> 36:31.560
if something was as good as GPT-4 that looked like Mu Zero, where it trained from some simple rules,

36:32.280 --> 36:37.000
okay, now I'm a bit more scared. But when you say, okay, it's, you know, we're feeding the whole

36:37.000 --> 36:40.440
internet into the thing and it parrots the internet back, mushed around a little bit,

36:40.440 --> 36:43.160
that looks very much like what a human does. And I'm just not scared of that.

36:44.280 --> 36:49.400
Yeah, it's very reasonable, whatever. Like, I'm not scared of GPT-4 to be clear. Like, I think

36:49.400 --> 36:55.000
there is like 0% chance or like, you know, epsilon chance that GPT-4 is existentially dangerous by

36:55.000 --> 37:02.680
itself. You know, maybe some crazy GPT-4 plus RL plus Mu Zero plus something, something maybe.

37:02.680 --> 37:07.160
But I definitely agree with you here. I don't expect GPT-3 or 4 by themselves to be dangerous.

37:07.160 --> 37:10.600
These are not, I'm much closer to, I think, what you're saying, like, yeah, if you had a Mu Zero

37:10.600 --> 37:16.200
system, they'd bootstrap yourself to GPT-4. Holy shit, like, we're big, we're big shit if we get

37:16.200 --> 37:21.960
to that. Then we should, let's stop. Let's stop. Yeah, let's stop. So, I'm very happy to get,

37:21.960 --> 37:26.280
to be into a regime where we're like, okay, let's find the right bound. Like, I think this is an

37:26.280 --> 37:29.800
actually good argument. I think this is actually something that should be discussed, which is not

37:29.800 --> 37:34.440
obvious. And I could be super wrong about that. So I'd like to justify a little bit about why I

37:34.440 --> 37:39.160
put such a small bound. But I think your arguments you're making for the higher bounds are very

37:39.160 --> 37:43.080
reasonable, actually. I think these are actually good arguments. So just to justify a little bit

37:43.080 --> 37:49.160
about why I put such a low bound, the boring default answer is conservatism. It's like, if all of

37:49.160 --> 37:54.200
humanity is at stake, which, you know, you may not believe, I'm like, whoa, whoa, okay, at least

37:54.200 --> 38:01.160
give us a few years to like, more understand what we're dealing with here. Like, I understand that,

38:01.160 --> 38:06.280
you know, you may disagree with this. Very plausible. But I'm like, whoa, like, you know, at least,

38:06.280 --> 38:11.320
let's let's like, by default, let's hit a pause button for like, you know, a couple years until

38:11.320 --> 38:15.640
we figure things out more. And then if we like, find a better theory of scaling, we understand how

38:15.640 --> 38:20.440
intelligent scales, we understand how mu zero comes, blah, blah, blah. And then we pick back up after

38:20.440 --> 38:25.960
we're like, you know, we make huge breakthroughs in alignment. And Eliezer is, is crying on CNN and

38:26.040 --> 38:32.680
like, oh, we did it, boys. I mean, then okay, sure, you know, okay. So that's the one, like,

38:32.680 --> 38:37.080
kind of more boring argument, like, that's kind of a boring argument. The more interesting argument,

38:37.080 --> 38:43.960
I think, which I think is a bit, you know, or skit so, is that it's not clear to me that you can't

38:43.960 --> 38:48.920
get dangerous levels of intelligence with the amount of compute we have now. And one of the reasons

38:48.920 --> 38:54.040
that I'm unsure about this is because man, GPT three GP four is just the dumbest possible way to build

38:54.040 --> 39:00.200
AI. Like, it's just like, like, there's like no dumber way to do it. Like, it's it works and dumb

39:00.200 --> 39:07.080
is good, right? You know, better lesson dumb is good. But look at humans. You said, as we talked

39:07.080 --> 39:12.840
about before, you know, human today, human 10,000 years ago, not that different. You place both of

39:12.840 --> 39:18.200
them into a, you know, workshop with tools to build, you know, any weapon of their choice, which of

39:18.200 --> 39:23.400
them is more dangerous? Obviously, you know, one of them will have much better, you know,

39:24.200 --> 39:31.800
capacities to deal with tools, to read books, to think about how to design new weaponry, and so

39:31.800 --> 39:37.880
on. These are not genetic changes, they are epistemological changes, they are memetic changes,

39:37.880 --> 39:42.360
they are software updates, you know, humans had to discover rational reasoning, like, you know,

39:42.360 --> 39:46.680
before like, you know, I mean, you know, obviously, people always had like, you know, folk conceptions

39:46.760 --> 39:52.360
of rationality. But it wasn't like a common thing to think about causality and like, you know,

39:52.920 --> 39:57.880
you know, rational, like, you know, if then else kind of stuff until relative, you know,

39:57.880 --> 40:03.480
like philosophers in the old ages and only became widespread relatively recently. And these are useful

40:03.480 --> 40:08.680
capabilities that turned out to be very powerful and took humans many, many thousands of years to

40:08.680 --> 40:12.760
develop and distribute. That's good. And I don't think humans are anywhere near the level. I think

40:12.840 --> 40:16.760
the way we could do science right now is pretty awful. Like, it's like the dumbest way to do

40:16.760 --> 40:23.960
science that like, kind of still works. Like, you know, and I expect it's like possible that if you

40:23.960 --> 40:29.480
had a system which like, let's say it's like, smaller brain than the human even, but it has

40:30.120 --> 40:34.600
really, really sophisticated epistemology. It has really, really sophisticated theories of

40:34.600 --> 40:41.000
meta science. And it never tires, it never gets bored, it never gets upset, it never gets distracted,

40:41.000 --> 40:45.960
and it can like, memorize arbitrary amounts of data. This is something that I think is within

40:45.960 --> 40:51.480
the realm of like a GPT three or four training run to build something like this. And it is not

40:51.480 --> 40:56.760
obvious to me that this system could not outfind humanity. Maybe not, like maybe not, but it's

40:56.760 --> 41:02.120
not obvious to me that it keeps. So just carry on. So what do you think of that? So to your first

41:02.120 --> 41:08.840
point, why I stand against almost all conservative arguments, you're assuming the baseline is no

41:08.840 --> 41:14.920
risk, right? And oh, well, why should we do this AI? We should wait and bring the baseline back. No,

41:14.920 --> 41:21.000
no, no, no. We are about to blow the world up any minute. There's enough nuclear weapons aimed at

41:21.000 --> 41:26.600
everything. This is wearing some incredibly unstable precarious position right now. Like,

41:26.600 --> 41:30.840
people talk about this with with car accidents. You know, this is comma, like, people are like,

41:30.840 --> 41:35.960
oh, well, you know, if your device causes even one accident, I'm like, yeah, but what if statistically

41:35.960 --> 41:40.920
there would have been five without the device? I'm like, you do have to understand the baseline

41:40.920 --> 41:45.800
risk in cars is super high. You make it five x safer, there's one accident, you don't like that.

41:45.800 --> 41:52.760
Okay, I mean, you have to be excluded from any polite conversation, right? So yeah, like, I think

41:52.760 --> 42:00.760
that calling for a pause to the technology is is worse, right? I think given the two options,

42:00.760 --> 42:05.480
if we should pause or we should not pause, I think pausing actually prevent presents more risk. And

42:05.480 --> 42:09.240
I can talk about some reasons why again, the things that I'm worried about are not quite

42:10.120 --> 42:19.160
the existential risks I have to the species are not AGI goes rogue, they are government gets control

42:19.160 --> 42:25.480
of AGI and ends up in some really bad place where nobody can compete with them. I don't think these

42:25.480 --> 42:30.920
things look unhuman. These things to me like, I see very little distinction between human

42:30.920 --> 42:36.600
intelligence and machine intelligence, it's all just on a spectrum. And like, they're not,

42:38.600 --> 42:43.320
like, to come to the point about, okay, but GPT four could be like this hyper rational,

42:43.320 --> 42:49.720
never tiring humans are doing science in the dumbest way. I'm not sure about that, right?

42:49.720 --> 42:54.280
Like, I think that, you know, when you look at like, okay, okay, we have chess bots that do

42:54.280 --> 42:58.200
way better. And all they do is think about chess, haven't really done this with humans,

42:58.200 --> 43:02.760
people would call it an ethical, right? Like, if we really told a kid, like, if we really just

43:02.760 --> 43:06.840
like, every night, we're just putting the chess goggles on you, and you're staring at chess

43:06.840 --> 43:11.800
boards, and we're really just training your neural net to play chess. I think humans could

43:11.800 --> 43:18.760
actually beat a computer again, a chess, if we were willing to do that. So yeah, I don't think

43:18.760 --> 43:23.000
that this stuff is that particularly dumb. And I think, okay, maybe we're losing 10x, but we're

43:23.000 --> 43:30.600
not losing a million x. Again, I don't see a, I do the numbers out all the time for when we're

43:30.600 --> 43:35.560
going to start to get more computer, you know, when will a computer have more compute than a human?

43:35.560 --> 43:40.600
When will a computer have more compute than humanity? And yes, these things get scary,

43:40.600 --> 43:44.520
but we're nowhere near scary yet. We're looking at these cute little things. And

43:45.400 --> 43:53.400
these things, by the way, do present huge dangers to society, right? The the PsyOps that are coming.

43:54.200 --> 43:59.400
Right now, you assume that like, when you call somebody, that you're at least wasting their

43:59.400 --> 44:04.760
time too. But we're going to get like heaven banning. I love this concept, which is, you know,

44:04.760 --> 44:08.840
yeah, yeah, came up on Luther AI, like that's where it comes from was the guy on the Luther

44:08.920 --> 44:15.560
AI that came up with that word. Yeah, I know the guy came up with it. I love, I love this

44:15.560 --> 44:22.120
concept. And I think there's also a story, my little pony friendship is optimal that God that

44:22.120 --> 44:32.200
goes into the concept. And yeah, so I think that like, my girlfriend proposed a, I don't want to

44:32.200 --> 44:39.240
talk to Oh, I say you don't want to talk to your relative anymore, right? Okay, we'll give them

44:39.240 --> 44:46.120
an AI version to talk to, right? Yeah. Yeah. So like, this stuff is coming and it's coming soon.

44:46.120 --> 44:51.880
And if you try to centralize this, if you try to, you know, say like, Oh, okay, Google Open AI,

44:51.880 --> 44:55.000
great, they're not aligned with you. They're really not Google has proven time and time again,

44:55.000 --> 44:58.440
they're not aligned with you. Meta has proven time and time again, they're trying to fix it.

44:58.680 --> 45:05.560
Yeah. I mean, I fully agree with you. Like, I like that you bring up PsyOps as the correct

45:05.560 --> 45:10.120
example in my opinion of short term risks. I think you're like, fully correct about this.

45:10.120 --> 45:16.840
Like, when I first saw like, GPT models, I was like, holy shit, like the level of control I can

45:16.840 --> 45:22.360
gain over social reality, using these tools at scale is insane. And I'm surprised that we haven't

45:22.440 --> 45:28.360
seen yet the things that like, augured in my visions of the day. And we will, like we will,

45:28.360 --> 45:35.560
obviously it's coming. And this is, so I think this is a very, very real problem. Yeah. Like,

45:35.560 --> 45:40.840
I think if we, even if we stop now, we're not out of the forest. So like, when you say like, I

45:40.840 --> 45:45.320
think the risk is zero, please do not believe that that is what I believe, because it is truly not.

45:45.320 --> 45:50.920
It is truly, truly not. I think we are like, we are really in a bad situation. We are in a, we are

45:50.920 --> 45:56.680
being, we're under attack from like so many angles right now. And this is before we get into, you

45:56.680 --> 46:00.840
know, like, you know, potential, like, you know, climate risks, nuclear risk, whatever, we're in

46:00.840 --> 46:07.320
under room of medic risk, like the, the then dangers of our like epistemic foundations are under

46:07.320 --> 46:14.040
attack. And this is something we can adapt to, right? Like, you know, we did, you know, when a

46:14.040 --> 46:19.320
good friend of mine, he's a, he's quite well read on like Chinese history. And he always like, it

46:19.320 --> 46:23.080
tells me his great story. So I'm not a historian. So please, you know, don't crucify me here. But

46:23.080 --> 46:27.400
like, he tells these great stories about when Marxist memes were first introduced to China.

46:27.400 --> 46:31.240
And like, this is where a world where like, just like all the precursor memes didn't exist.

46:31.240 --> 46:35.800
That's just like, kind of was air dropped in and people went nuts. People went just completely

46:35.800 --> 46:40.440
crazy because there was no memetic antibodies to these like hyper virulent memes that were,

46:40.440 --> 46:44.680
you know, created by evolutionary pressures and like, you know, Western university departments,

46:44.680 --> 46:48.920
like really, you could call philosophy department just gain a function, the medic laboratories.

46:49.960 --> 46:56.920
I like that. Yeah. I mean, like, you know, like without being, you know, political or any means

46:56.920 --> 47:00.760
there, a lot of what these organizations do. And like, you know, other, you know, what other,

47:00.760 --> 47:06.120
you know, memetic, like, you know, if philosophy departments are the, like, gain a function

47:06.120 --> 47:11.160
laboratories, then like 4chan and tumbler are like the bat caves of needs, you know, like the

47:11.160 --> 47:17.320
Chinese bat cave. And I remember this vividly. I was like on tumbler and 4chan, like when I was a

47:17.320 --> 47:22.440
teenager, and then suddenly all the like weird, bizarre, you know, internet shit I saw started

47:22.440 --> 47:28.200
becoming mainstream news. My parents were watching in 2016. And I was like, what the hell is going

47:28.200 --> 47:33.080
on? Like, I already developed antibodies to this shit. Like I already, you know, both right and

47:33.080 --> 47:38.280
left, I was already like, I already immunized all this. So I fully agree with you that this is like

47:38.280 --> 47:44.200
one of the largest risks that we are facing is this kind of like memetic mutation load, in a sense.

47:44.200 --> 47:50.360
And I'm not going to say I have a solution to this problem. I'm like, I have ideas, like there's a

47:50.360 --> 47:55.320
lot of like things you can do to improve upon this. Like if AI was not a risk, and also not climate

47:55.320 --> 47:59.800
change and whatever, this might be something I work on, like epistemic security, this might be

47:59.800 --> 48:04.120
something I would work on, like how can we build better coordination, like like just scalable

48:04.120 --> 48:09.000
rationality mechanisms, stuff like prediction markets and stuff like this. I don't know. But

48:09.000 --> 48:15.000
sorry, going off track here a little bit. Well, no, actually, I really agree with a lot of the

48:15.000 --> 48:19.080
stuff you said. And I had a similar experience with the antibodies and people are exposed to this

48:19.080 --> 48:28.760
stuff. And I'm like, yeah, this got me like four years ago. Yeah. So I think that there is a solution

48:28.760 --> 48:34.600
and I have a solution. And the answer is open source AI. The answer is open source. Let's say

48:34.600 --> 48:38.280
even you can even dial it back from like the political and the terrible and just straight up

48:38.280 --> 48:43.560
talk about ads and spam, or maybe spam, just straight up spam, I get so much spam right now.

48:43.560 --> 48:48.520
And it's like, it's kind of written by a person. It's like targeting me to do something. And

48:48.520 --> 48:53.400
Google spam filter can't even come close to recognizing it. Right. Like what I need is a

48:53.400 --> 49:00.040
smart AI that's watching out for me that is just it's not even targeted attacks at me. It's just

49:00.040 --> 49:07.080
so much noise. And I don't see a way to prevent this. Like the big organizations, they're just

49:07.080 --> 49:11.800
going to feed you their noise, right? And they're going to maximally feed you their noise. The only

49:11.800 --> 49:16.040
way is if you have an AI, like, I don't think alignment is a hard problem. I think if you own

49:16.040 --> 49:20.520
the computer and you run the software, if you develop the software, the AI is aligned with you.

49:20.520 --> 49:28.360
Oh, yeah. Can you, okay, if I challenge you, George Haas, here is a llama 65b model when we

49:28.360 --> 49:33.240
could appear to run it on, make it so it and make, yeah, you know, sure. Okay, you developed AI. I

49:33.240 --> 49:39.320
give you the funding for your time. Can you develop a model that is as good as llama 64b 65b

49:39.320 --> 49:43.720
and is immune, like completely immune to jailbreak. It cannot be jailbroken. No.

49:44.760 --> 49:50.760
Why not? It's aligned, isn't it? Well, no, but this isn't what alignment means. Well, my values is

49:50.760 --> 49:55.640
do not get jailbroken. Oh, okay, you're talking about unexploitability. This is not alignment,

49:55.640 --> 49:59.880
right? Okay, okay, interesting. I didn't know you would separate those. So I extremely separate

49:59.880 --> 50:07.240
those, right? Okay, I mean, in the default case, it like, like it, it's on my side, right? Okay,

50:07.240 --> 50:13.480
unexploitability is not a question of whether it's okay. And this is a true thing about people too.

50:13.480 --> 50:18.840
Whenever I look at a person, I ask, okay, is this person, I want something for me. Is this person,

50:18.840 --> 50:24.840
does this person want it too? And is this person capable of doing it? Right? And I really separate

50:24.840 --> 50:29.320
those two things. I can build a system. I don't, I'm not worried about the first one with the AI

50:29.320 --> 50:33.800
system is worried about the second one. Can it be gamed? Can it be exploited? Sure, I could tell

50:33.800 --> 50:39.160
like, you know, like, like, say it was just playing chess, right? And it loses. I'm like, don't lose.

50:39.800 --> 50:46.280
Okay, I didn't want to, man, I didn't want to lose. I'm sorry. I know. But like, so yes, yes, can I build

50:46.280 --> 50:51.160
a aligned system? Sure. Can I build an unexploitable system? No, especially not by a more powerful

50:51.160 --> 50:55.880
intelligence. Interesting. Interesting. So this is an interesting, I think you're, you're, you're

50:55.960 --> 51:01.000
pointing to actually a very important part of this, is that like, exploitability and alignment

51:01.000 --> 51:05.560
can get fuzzy? Like, which is which? Like, did it fail because of its skill set? Or because it's not

51:05.560 --> 51:09.160
aligned? It's actually a very deep question. So I think, I think you make a good point for like,

51:09.160 --> 51:16.280
you know, talking about these two separately. I guess the, so the thing I want to dig in just

51:16.280 --> 51:24.840
like a little bit more on this idea is there are, there's two ways, there are two portals

51:24.840 --> 51:29.800
through which, you know, the memetic demons can reach into reality, humans and computers.

51:30.600 --> 51:34.680
Why do you think your AI is immune to memes? Why, why can't I just build

51:34.680 --> 51:40.200
AIs that target your AIs? What like you don't, I don't think my AI is immune to memes at all.

51:40.920 --> 51:46.440
I think that the only question is, and I really like your game, like these, these NGOs are doing

51:46.520 --> 51:56.920
gain of function on memes, right? Wear a mask. Like, a weaker intelligence will never be able

51:56.920 --> 52:02.760
to stand up to a stronger intelligence. So from this perspective, if this is what's known as alignment,

52:02.760 --> 52:07.960
I just don't believe that this is possible, right? You can't, you can't keep a stronger

52:07.960 --> 52:12.280
intelligence in the box. This is, this is, I agree with you, Kowsky in the box experiments,

52:12.280 --> 52:17.560
like the AI is always going to get out. There's no keeping it in the box, right? This is, this

52:17.560 --> 52:23.560
is a complete impossibility. I think there's only two real ways to go forward. And one is

52:23.560 --> 52:30.600
Ted Kaczynski. One is technology is bad. Oh my God, blow it all up. Let's go live in the woods,

52:30.600 --> 52:35.400
right? And I think this is a philosophically okay position. I think the other philosophically okay

52:35.400 --> 52:40.680
position is something more like effective accelerationism, which is look, these AIs are

52:40.680 --> 52:47.960
going to be super powerful. Now, if you have one, it could be bad. But if super intelligent AIs are

52:47.960 --> 52:52.520
all competing against each other, magnetically, like we have something like society today, just

52:52.520 --> 52:57.240
the general power levels have gone up. This is fine as long as these things are sufficiently

52:57.240 --> 53:02.920
distributed, right? Like, sure, this AI is not perfectly aligned, but you know, there's a thousand

53:02.920 --> 53:06.760
other ones and like you have to assume they're all basically good because if they're all basically

53:06.760 --> 53:12.200
bad, well, we're dead anyway. I mean, why wouldn't you expect that? That they're all bad? Yeah.

53:13.000 --> 53:18.520
Well, or what do you think of humans? Are most humans good? I think the concept of good doesn't

53:18.520 --> 53:23.160
really apply to humans because humans are too inconsistent to be good. Like by default, they

53:23.160 --> 53:28.120
can be good in various scenarios in various social contexts. Like give me any human and I can put them

53:28.120 --> 53:33.560
into a context where they'll do an arbitrarily bad thing. And this is true about a llama as well,

53:33.560 --> 53:37.560
right? Lamas are completely inconsistent. I think they're actually more inconsistent than humans.

53:38.360 --> 53:42.840
Right. I see. I wouldn't trust llamas to be good. Well, yeah, but I wouldn't think that they're

53:42.840 --> 53:46.600
bad either. I would think they have the exact same inconsistency problem as humans. And I think

53:46.600 --> 53:52.920
almost any AI you build is going to run into the same problems, right? Yeah, I think so. That's my

53:52.920 --> 53:58.440
point. So your assumption can't rely on them being good because you don't get that for free.

53:58.440 --> 54:01.640
Like, where does that come from? My assumption is not that they're good. My assumption is that

54:01.720 --> 54:06.440
they're not bad. But inconsistent is fine. As long as we have a ton of them and they're all

54:06.440 --> 54:10.440
inconsistent and they're pulling society in every which direction, you don't end up paper-clipped,

54:10.440 --> 54:16.520
right? Why not? Well, because what? They're all going to coordinate and agree to paper-clip you?

54:16.520 --> 54:21.320
No, no, they'll just do some random bullshit. And then that random bullshit will not include

54:21.320 --> 54:25.000
humans. They're all doing random bullshit, right? You're going to have, let's say the liberals

54:25.000 --> 54:28.360
decide we're going to paper-clip people. The conservatives are going to come out very strongly

54:28.360 --> 54:34.440
against paper-clipping, right? And you're just going to end up with these sort of dynamics,

54:34.440 --> 54:38.040
like forever. You're going to have some AIs who are like, yeah, we don't like it, you know?

54:40.040 --> 54:46.920
So this is perplexing to me because in my view, which I think I heard you agree with earlier,

54:46.920 --> 54:51.800
it's like the world is unstable. The world is very unstable. And it seems to me you're suggesting

54:51.800 --> 54:57.320
if we increase the chaos, if we have more entities fighting, doing more different things

54:57.320 --> 55:01.880
with more energy, that makes it more stable. Is that correct? Yes. The world has actually

55:01.880 --> 55:05.400
become very stable in the last 100 years. And I'm scared of how stable it's become.

55:07.000 --> 55:10.600
You know, again, thank you for China. How many sovereign countries are there in the world?

55:12.360 --> 55:18.920
I mean, you know, 190 something. That's the UN number, if you believe them. I think there's

55:18.920 --> 55:23.560
the US and China and maybe Russia, mostly they just have a veto button.

55:24.360 --> 55:30.040
All right. So I think there's more like two and a half. Like this isn't a lot of entities fighting

55:30.040 --> 55:35.160
it out chaotically. A lot of entities fighting it out chaotically would be intelligences spread

55:35.160 --> 55:39.400
across the entire galaxy, right? And that's the other, that's the other beautiful thing too.

55:40.200 --> 55:44.040
Sooner we get off this planet, the sooner we get things that are actually a light year away.

55:44.600 --> 55:47.640
Well, I think the speed of light's real. It's going to take you a year to send them there.

55:48.520 --> 55:52.760
Memes, right? You want to get away from the memes? There's only one way.

55:54.760 --> 55:58.920
For the record, I would like to go on the record and say, if any future transhumanists

55:58.920 --> 56:01.960
or whatever want to blast themselves into space and go do their own thing,

56:01.960 --> 56:06.120
I support their right to do that. And I would love to give this right to people.

56:06.120 --> 56:10.760
The number one thing I want from countries is the ability, the right to leave.

56:10.760 --> 56:12.680
This is what I would love. This is what I love about companies.

56:12.680 --> 56:15.480
Free Exit. You're talking neo-reaction talk.

56:16.280 --> 56:20.280
Yeah. Free Exit is extremely important. I would not describe myself as neo-reactionary, please,

56:20.280 --> 56:26.360
because I'm not that gay. I wouldn't describe myself that way either,

56:26.360 --> 56:32.760
but I've heard a lot of good ideas from them. But yeah, that being said, I do think that,

56:33.480 --> 56:37.720
what I want, I think, let's ground the conversation a little bit here.

56:40.200 --> 56:42.760
I'm very enjoying this conversation. I love talking to these philosophical points.

56:42.840 --> 56:47.640
These are really good points, really interesting. But ultimately, as we also get to the latter

56:47.640 --> 56:53.320
third of this conversation, the thing I really care about is strategy. The thing I really care

56:53.320 --> 56:58.200
about is reality politic. I really care about, okay, what action can I take to get to the features

56:58.200 --> 57:04.840
I like? And I'm not going to be one of those galaxy brain fucking utilitarians like, well,

57:04.840 --> 57:09.080
actually, this is the common good. No, no, this is what I like. I like my family. I like humans.

57:09.240 --> 57:16.680
Yeah, it's just what it is. I'm not going to justify this on some global beauty,

57:16.680 --> 57:22.040
whatever. It doesn't matter. I want to live in a world. I want a 20-year time, 50-year time.

57:22.040 --> 57:28.440
I want to be in a world where my friends aren't dead and where I'm not dead. Maybe we are cyborgs

57:28.440 --> 57:33.080
or something, but I don't want to be dead. What I really care about, ultimately, is how do I get

57:33.080 --> 57:38.280
those words? And I want us all to not be suffering. I don't want to be in war. I want us to be in a

57:38.280 --> 57:43.720
good outcome. So I think we agree that we would both like a world like this. And I think we probably

57:43.720 --> 57:47.240
disagree about how best to get there. And I'd like to talk a little bit about what can we,

57:48.520 --> 57:53.160
what should we do and why do we disagree about what we do? Does that sound good to you?

57:53.160 --> 57:56.120
Maybe I'll first propose a world that meets your requirements.

57:57.320 --> 58:02.200
And you can tell me if you want to live in it. So here's a world. We've just implanted electrodes

58:02.200 --> 58:07.560
in everyone's brain and maximized their reward function. I would hate living in a world like

58:07.560 --> 58:11.000
that. Yeah, but no one, it meets your requirements, right? Your friends are not dead.

58:11.720 --> 58:13.480
No one's suffering and we're not at war.

58:14.600 --> 58:19.800
That is true. There are more criteria than just that. The true, the criteria I said is things I

58:19.800 --> 58:23.880
like. As I said, I'm not a utilitarian. I don't particularly care about minimizing suffering

58:23.880 --> 58:29.720
or maximizing utility. What I care about is various vague aesthetic preferences over reality.

58:30.440 --> 58:34.120
I'm not pretending this is, as though that was the whole spiel I was trying to make,

58:34.120 --> 58:39.320
is that I'm not saying I have a true global function to maximize. I say I have various

58:39.320 --> 58:44.200
aesthetics. I have various meta-preferences of those aesthetics. I'm not asking for a global one.

58:44.200 --> 58:49.240
I'm asking for a personal one. I'm asking for a personal one that you don't care about the rest

58:49.240 --> 58:54.200
of the world. I gave you mine. I gave you what I would do if I had an AGI. Yep. So I'm getting

58:54.200 --> 59:00.360
on this rock speed of light as fast as I can. Fair enough. I think if that is, I would like to live

59:00.360 --> 59:05.400
in a world where you could do that. This would be a feature of my world. A world where I would be

59:05.400 --> 59:13.160
happy is a world in which we coordinated at larger scales around building aligned AGI that could then

59:13.160 --> 59:23.160
distribute intelligence and matter and energy in a well-value handshaked way between various people

59:23.160 --> 59:28.120
who may want to coordinate with each other, may not. Some people might want to form groups that

59:28.120 --> 59:32.680
have shared values and share resources. Others may not. I would like to live in a world where that

59:32.680 --> 59:36.680
is possible. Have you read Metamorphosis of Prime Intellect? I have, unfortunately, not.

59:39.640 --> 59:42.680
Yeah. I was going to ask if you're happy with that world, right? Like,

59:42.680 --> 59:50.520
unfortunately, don't know it. I mean, it's as simple to describe. Singleton AI that basically

59:50.520 --> 59:57.000
gives humans whatever they want, like maximally libertarian, you know, you can do anything you

59:57.000 --> 01:00:02.520
want besides harm others. Is that a good world? Probably. I don't know. I haven't read the book.

01:00:02.520 --> 01:00:06.840
I assume the book has some dark twist about why this is actually a bad world. Not really. Not

01:00:06.840 --> 01:00:12.680
really. I mean, the plot is pretty obvious. You are the tiger reading chum, right? Sure, but you

01:00:12.680 --> 01:00:16.440
can then just decide if that is what you want, then you can just return to the wilderness. That's the

01:00:16.440 --> 01:00:22.280
whole point. Yeah, but can you? Can you really return to the wilderness, right? Like, you think

01:00:22.280 --> 01:00:26.600
that like, I don't think we have free will. I don't think you ever will return to the wilderness.

01:00:26.600 --> 01:00:33.000
I think a large majority of humanity is going to end up wireheaded. Yeah, I expect that too.

01:00:33.000 --> 01:00:36.440
Okay, great. And this is the best possible outcome, by the way. This is giving humans exactly what

01:00:36.440 --> 01:00:43.240
they want. Yeah. Well, to be clear, I don't expect it's all humans. I truly do not. I don't think

01:00:43.240 --> 01:00:47.880
it's all humans either. I think a lot of humans have metapreferences over reality. They have

01:00:47.880 --> 01:00:51.480
preferences that are not their own sensory experiences. This is the thing that the utilitarians

01:00:51.480 --> 01:00:58.280
get very wrong is that many human preferences are not even about their own sensory inputs.

01:00:58.280 --> 01:01:03.400
They're not even about the universe. They're about the trajectory of the universe. They're

01:01:03.400 --> 01:01:10.840
about for the utilitarianism, you know, and a lot of people want struggle to exist, for example.

01:01:10.840 --> 01:01:18.040
They want heroism to exist or whatever. I would like those values to be satisfied to the largest

01:01:18.120 --> 01:01:22.440
degree possible, of course. Am I going to say I know how to do that? No. Which is why I kind of

01:01:22.440 --> 01:01:30.040
like didn't want to go this deep, because I think if we're arguing about, oh, do we give them, you

01:01:30.040 --> 01:01:36.440
know, for utilitarianism versus libertarian, utopia versus whatever, I mean, we're already like

01:01:37.160 --> 01:01:42.920
10,000 steps deep. I'm asking about you. I'm not asking about them. I'm asking about a world

01:01:42.920 --> 01:01:48.040
you want to live in. And this is a really hard problem, right? Yeah. And this is why I just

01:01:48.040 --> 01:01:56.760
fundamentally do not believe in the existence of AI alignment at all. There is no like what values

01:01:56.760 --> 01:02:03.480
are we aligning it to, whatever the human says, or what they mean, or like. Sure, sure. But like,

01:02:03.480 --> 01:02:08.280
my point is I feel we have wandered into the philosophy department instead of the politics

01:02:08.280 --> 01:02:14.200
department. Okay, like, it's like, I agree with you, like, do human values exist? What does exist

01:02:14.200 --> 01:02:17.240
to me? But like, by the point you get to the point where we're asking what does exist to me,

01:02:17.240 --> 01:02:23.080
you've gone too far. I'll respond concretely to the two political proposals I heard you state on

01:02:23.080 --> 01:02:29.000
Bankless. Sure. I'd love to talk about them. One is limiting the total number of flops.

01:02:29.880 --> 01:02:34.600
Temporarily. Temporarily, yes. And when I have a proposal for that, but I don't want to set a

01:02:34.600 --> 01:02:39.640
number, I want to set it as a percent. I do not want anybody to be able to do a 51% attack on

01:02:39.640 --> 01:02:46.200
compute. If one organization acquires 50, it's straight up 51% attacks on crypto. If one organization

01:02:46.200 --> 01:02:52.680
acquires 51% of the compute in the world, this is a problem. Maybe we'll even cap it at something

01:02:52.680 --> 01:02:58.440
like 20, you know, you can't have more than 20, right? Yeah, I would support regulation like this.

01:02:59.640 --> 01:03:04.120
I would I don't think that this would cripple a country. But we do not want one entity or

01:03:04.120 --> 01:03:08.920
especially one training run to start using a large percentage of the world's compute,

01:03:08.920 --> 01:03:13.160
not a total number of flops. I mean, absolutely not. That'd be terrible.

01:03:13.160 --> 01:03:17.240
I think we can actually agree. I would actually support that regulation. Like, no, no, sorry,

01:03:17.240 --> 01:03:21.080
Sam Altman, you cannot 51% attack the world's compute. Sorry, it's illegal.

01:03:22.120 --> 01:03:27.880
That's fair enough. I think this is a sensible way to think about things. Assuming that software

01:03:27.880 --> 01:03:31.160
is fungible, is that everyone has access to the same kind of software and that you have an

01:03:31.160 --> 01:03:37.400
offense, defense, balance. So in my personal model of this, I think, well, a, some actors have

01:03:37.400 --> 01:03:43.160
very strong advantages on software, which can be very, very large, as someone who's trained very,

01:03:43.160 --> 01:03:47.160
very large models and knows a lot of the secret tricks that goes into them, a lot of the stuff

01:03:47.160 --> 01:03:54.200
in the open sources. Maybe we should force it to be open source. Well, this is your this is

01:03:54.200 --> 01:03:58.440
actually very legitimate consequence for just set. And now I'll say the second point about why I

01:03:58.440 --> 01:04:03.160
think that it doesn't work. So the next reason why I think doesn't work is that there is a,

01:04:04.040 --> 01:04:09.320
there are constant factors at play here is that the world is unstable. We are talking about this.

01:04:09.320 --> 01:04:18.200
I think the amount of compute you need to break the world currently is below the amount of compute

01:04:18.200 --> 01:04:24.520
that more than 100 actors have access to if they have the right software. And if you give, if you

01:04:24.840 --> 01:04:29.240
let's say you have this insight, right, that could be used, not saying it will be, but it could be

01:04:29.240 --> 01:04:33.000
used to break the world, to like cause World War three, or, you know, or just like, you know,

01:04:34.360 --> 01:04:39.160
cause mass extinction or whatever, if it's misused, right? Let's say you give this to

01:04:40.680 --> 01:04:45.880
you and me. Do you expect we're going to kill everybody? Like, would you do that? Or would you

01:04:45.880 --> 01:04:50.280
be like, Hey, let's, it kind of looks like not kill the world right now. And I'll be like, sure,

01:04:50.280 --> 01:04:55.480
let's not kill the world. How are we killing the world? How did we go from, I don't even understand

01:04:55.480 --> 01:05:00.440
like, how exactly does the world get killed? This, this is a big leap for me. I agree with you,

01:05:00.440 --> 01:05:05.480
I agree with you about the PSYOP stuff. I agree with you about, sorry, sorry, let, let me, you're

01:05:05.480 --> 01:05:09.640
right. I made too big of a, I mean, you're completely correct. Sorry about that. So to

01:05:10.600 --> 01:05:16.040
back up a little bit, let's assume we, you and me have access to something that can train, you know,

01:05:16.040 --> 01:05:24.360
at mu zero, you know, super GPT seven system on a tiny device, you know, cool. Problem is we do

01:05:24.360 --> 01:05:28.440
a test run it with it and we have, it immediately starts breaking out and we can't control it at

01:05:28.440 --> 01:05:33.400
all. Breaking out. What was it breaking out of? I don't, it immediately tries to maximize, it

01:05:33.400 --> 01:05:37.080
learned some weird proxy during the training process that is trying to maximize them. For some

01:05:37.080 --> 01:05:42.200
reason, this proxy involves gaining talent, involves gaining, you know, mutual information

01:05:42.920 --> 01:05:48.520
about future states. How is it gaining power? There's lots of other powerful AIs in the world

01:05:48.520 --> 01:05:53.080
who are telling it no. Well, we're assuming in this case, it's only you and me. Wait, wait,

01:05:53.080 --> 01:05:57.640
this is a problem. No, no, no, no. You've, you've ruined my entire assumption. As soon as it's you

01:05:57.640 --> 01:06:04.120
and me, yes, we have a real problem. Chicken man is only a problem because there's one chicken man.

01:06:04.120 --> 01:06:09.320
Yeah, I look, I am with you. So I'm saying before we get to the distributed case. So this is the

01:06:09.400 --> 01:06:13.960
step before we, it is not yet been distributed. Just, you know, you and me discover this algorithm

01:06:13.960 --> 01:06:18.600
in our basements. And so we're the first one to have it just by definition, because, you know,

01:06:18.600 --> 01:06:24.120
we're the one who found it. What now? Like, do you think posting, what do you think happens if

01:06:24.120 --> 01:06:29.960
you post this to GitHub? Well, good things for the most part. Interesting. I'd love to hear more.

01:06:29.960 --> 01:06:34.200
Okay. So first off, I just don't really believe in the existence of we found an algorithm that

01:06:34.200 --> 01:06:37.800
gives you a million x advantage. I believe that we could find an algorithm that gives you a 10x

01:06:37.800 --> 01:06:43.560
advantage. But what's cool about 10x is like, it's not going to massively shift the balance of power.

01:06:44.120 --> 01:06:49.320
Right. Like I want power to stay in balance. Right. This is like Avatar, the last airpan. Power

01:06:49.320 --> 01:06:54.200
must stay in balance. The fire nation can't take over the other nations. Right. So as long as power

01:06:54.200 --> 01:06:57.880
relatively stays in balance, I'm not concerned with the amount of power in the world.

01:06:58.840 --> 01:07:05.160
Right. Let me get to some very scary things. So what I think you do is, yes, I think the

01:07:05.160 --> 01:07:08.280
minute you discover an algorithm like this, you post it to GitHub, because you know what's

01:07:08.280 --> 01:07:12.840
going to happen if you don't? The feds are going to come to your door. They're going to

01:07:14.040 --> 01:07:18.520
take it. The worst people will get their hands on it if you try to keep it secret.

01:07:19.960 --> 01:07:26.520
So, okay. That's a fair question. So I'll take that aside. So am I correct in thinking that you

01:07:26.520 --> 01:07:34.040
think the feds are worse than serial killers in prison? No, but I think that, yeah, well, yes

01:07:34.040 --> 01:07:39.320
and no. Do I think that your average fed is worse than your average serial killer? No. Do I think

01:07:39.320 --> 01:07:43.720
that the feds have killed a lot more people than serial killers? All combined? Yeah.

01:07:44.360 --> 01:07:49.400
Sure. Totally agreeing with that. It's not one fed. It's all the feds in their little super

01:07:49.400 --> 01:07:54.680
powerful system. Sure. That's completely fine by me. I'm happy to grant that. Okay. What I want to

01:07:54.680 --> 01:08:01.640
work one through is a scenario. Okay. Let's say, okay, you know, we have a 10x system or whatever,

01:08:01.720 --> 01:08:07.160
but we hit the chimp level. You know, we jump across the chimp general level or whatever,

01:08:07.160 --> 01:08:10.760
right? And now you have a system which is like John von Neumann level or whatever, right? And it

01:08:10.760 --> 01:08:15.160
runs on one tiny box and you get a thousand of those. So it's very easy to scale up to a thousand x.

01:08:15.160 --> 01:08:20.200
So, you know, so then, you know, maybe you have your thousand John von Neumanns improve the efficiency

01:08:20.200 --> 01:08:25.640
by another, you know, to five, 10x. You know, now we're already at 10,000 x or 100,000 x,

01:08:25.640 --> 01:08:29.880
you know, improvements, right? So like just from scaling up the amount of hardware, including

01:08:30.600 --> 01:08:36.920
so just saying, okay, now feds bust down our doors. Shit, you know, real bad. They

01:08:36.920 --> 01:08:40.520
take all our tiny boxes. They're taking all the von Neumanns. They're taking all the von Neumanns.

01:08:40.520 --> 01:08:46.680
We're in deep shit now. We're getting chickened, boys. Shit. We're getting chickened. So, okay,

01:08:46.680 --> 01:08:51.640
we get chickened, right? Bad scenario. Totally agree with you here. This is a shit scenario. Now

01:08:51.640 --> 01:08:57.480
the feds have, you know, all of our AIs. Bad scenario. Okay. I totally see how this world

01:08:57.480 --> 01:09:00.840
goes to shit. Totally agree with you there. You can replace the feds with Hitler. It's

01:09:00.840 --> 01:09:06.200
interchangeable. Sure. But like, I want to like ask you a specific question here. And this might

01:09:06.200 --> 01:09:10.040
be, you know, you might say, nah, this is like too specific to me, but I want to ask you a specific

01:09:10.040 --> 01:09:19.080
question. Do you expect this world to die is more likely to die or the world in which the, you know,

01:09:19.080 --> 01:09:25.080
EAC death cultists on Twitter who literally want to kill humanity who say this, like not all of

01:09:25.080 --> 01:09:29.800
them, there's a small subset of them, small subset of them who literally say, oh, you know,

01:09:30.680 --> 01:09:37.240
the glorious future AI race should replace all humans. They break in, you know, with like, you

01:09:37.240 --> 01:09:42.200
know, Katanas and, you know, steal our AI. Which one of these you think is more likely to kill us?

01:09:43.080 --> 01:09:50.120
A genuine question. To kill all of us, the feds. To kill a large majority of us, the EAC people.

01:09:50.760 --> 01:09:57.960
Interesting. I would be really interested in hearing why you think that. Sure. Okay. So actually

01:09:57.960 --> 01:10:03.720
killing all of humanity is really, really hard. And I think you brought this up before, right?

01:10:03.720 --> 01:10:09.480
You talked about like, if you're going to end up in a world of suffering, a world of suffering

01:10:09.480 --> 01:10:16.280
requires malicious agents, where a world of death requires maybe an accident, right? I think this

01:10:16.280 --> 01:10:22.680
is plausible, but I actually think that killing all of humans, at least for the foreseeable future,

01:10:22.680 --> 01:10:29.560
is going to require malicious action too, right? And I also think that like, the fates that look

01:10:29.560 --> 01:10:35.960
kind of worse than death, like I think mass wireheading is a fate worse than big war and everyone

01:10:35.960 --> 01:10:41.880
dies, right? Like, like a mass wireheading, like a, like a singleton, like a paper clipping,

01:10:41.880 --> 01:10:47.720
like a, and I think that that is the one that the one world government and, you know,

01:10:47.720 --> 01:10:53.800
NGO, New World Order people are much more likely to bring about than EAC. EAC. You're going to

01:10:53.800 --> 01:10:59.240
have a whole lot of EAC people. Again, I'm not EAC. I don't have that on my Twitter, but I think a

01:10:59.240 --> 01:11:05.160
lot of those people would be like, yeah, spaceships, let's get out of here, right? Versus the feds are

01:11:05.240 --> 01:11:12.440
like, yeah, spaceships, I don't know. Interesting. So I think this is a fair

01:11:12.440 --> 01:11:16.040
opinion vote. And they'll be outside our jurisdiction. How will we get taxes?

01:11:16.840 --> 01:11:21.320
I'm describing more a very small minority of EAC people who are the ones who specifically

01:11:21.320 --> 01:11:26.360
goal their anti-natalist misanthropes. They want to kill humans. That is their stated goal,

01:11:26.360 --> 01:11:30.360
is that they want humans to start, like, or like take extreme vegans if you want, you know,

01:11:30.440 --> 01:11:36.200
like the likes, you know, like, my argument, my point here I'm making is I'm not making the point

01:11:36.200 --> 01:11:43.400
feds are good by any means. I'm not saying it. What I'm saying is, is that I would actually be

01:11:44.120 --> 01:11:50.840
somewhat surprised to find that the feds are anti-natalists who want to maximize the death

01:11:50.840 --> 01:11:55.800
of humanity. Like, maybe you have a different view here, but I find that knowing many feds,

01:11:55.800 --> 01:11:58.200
that's quite surprising to me. I don't think that's what the feds want.

01:11:59.000 --> 01:12:05.720
Yeah, it's okay. So, cool. So would you see, you do agree that if we would post this open source,

01:12:05.720 --> 01:12:10.520
more of the insane death cultists would get access to potentially lethal technology?

01:12:11.400 --> 01:12:18.360
Well, sure. But again, like, it's not just the insane death cultists, it's everybody. And we as

01:12:18.360 --> 01:12:23.640
a society have kind of accepted, it turns out everybody gets access to signal. Some people

01:12:23.720 --> 01:12:27.000
who use it are terrorists. I think signal is a huge good in the world.

01:12:27.000 --> 01:12:32.120
I agree. I fully agree with that. So, okay, cool. So we've granted this that, you know,

01:12:32.120 --> 01:12:38.840
if we distributed widely, it would be given to some like, incorrigibly deadly lethal people.

01:12:38.840 --> 01:12:41.240
They're coordinating bombings on signal right now.

01:12:42.280 --> 01:12:48.520
Sure, sure. And then, so now this, this reduces the question to a question about

01:12:48.520 --> 01:12:53.640
offense defense balance. So in a hypothetical world, which I'm not saying is the world we live in,

01:12:53.640 --> 01:12:59.480
but like, let's say the world would be offense favored, such that, you know, there's a weapon

01:12:59.480 --> 01:13:04.600
you can build in your kitchen, you know, out of like pliers and like, you know, duct tape,

01:13:04.600 --> 01:13:09.800
that 100% guarantees vacuum false decays the universe, like it kills everyone instantly,

01:13:09.800 --> 01:13:16.200
and there's no defense possible. Assuming this was true, do you still, would that change how

01:13:16.200 --> 01:13:21.560
you feel about distribution power? Assuming that's true, we're dead no matter what,

01:13:21.560 --> 01:13:27.160
doesn't matter. If we live, there's some, you can look at the optimization landscape of the world,

01:13:27.160 --> 01:13:30.840
and I don't know what it looks like. I can't see that far into the optimizer.

01:13:30.840 --> 01:13:35.640
But there are some potential landscape, and this is a potential answer to the Fermi paradox, like,

01:13:35.640 --> 01:13:41.560
we might just be dead. We're sitting on borrowed time here, like, if it's true that out of, you

01:13:41.560 --> 01:13:46.680
know, kitchen tools, you can build a, build a, convert the world to strange quarks machine?

01:13:48.200 --> 01:13:52.520
Okay, I think this is a sensible position, but I guess the way I would approach this

01:13:53.240 --> 01:13:57.320
problem, you know, conditional probability is kind of in an opposite way. It seems to me that

01:13:57.320 --> 01:14:03.240
you're conditioning on offense not being favored, what policy do we follow? Because if we, offense

01:14:03.240 --> 01:14:07.400
is favored, we're 100% dead. Well, I'm more interested in asking the question, is it actually

01:14:07.400 --> 01:14:12.280
true? Assuming I don't know if offense is favored, and assuming it is, are there worlds in which

01:14:12.280 --> 01:14:16.360
we survive? So I personally think there are. I think there are worlds in which you can actually

01:14:16.360 --> 01:14:21.240
coordinate to a degree that quark destroyers do not get built, or at least not before everyone

01:14:21.240 --> 01:14:23.560
fucks off at the speed of light and like distributes themselves.

01:14:23.560 --> 01:14:28.520
There are worlds that I would rather die in, right? Like the problem is, I would rather,

01:14:28.520 --> 01:14:33.240
I think that the only way you could actually coordinate that is with some unbelievable degree

01:14:33.240 --> 01:14:38.760
of tyranny, and I'd rather die. I'm not sure if that's true. Like, look, look, could you and me

01:14:38.760 --> 01:14:43.160
coordinate to not destroy the planet? Do you think you could? Okay, cool. You, so me and you could.

01:14:43.160 --> 01:14:49.800
Could me and you and Tim coordinate? Yeah, I think within a Dunbar number, I think you can. Yes.

01:14:49.800 --> 01:14:53.800
Okay, with that, I don't think, I think I can get more than a number to coordinate on this.

01:14:53.800 --> 01:14:57.960
Actually, I can get quite a lot of people to coordinate of the to agree to a pact and not

01:14:57.960 --> 01:15:04.840
quark matter annihilate the planet. Well, you see, but like, and this is, you know, you were

01:15:04.840 --> 01:15:10.600
saying this stuff about humans before and could like the 20,000 years ago human beat the modern

01:15:10.600 --> 01:15:13.880
human, right? Or could the modern human beat them? The modern human has access to science.

01:15:14.600 --> 01:15:19.320
A very small act percent of modern humans have access to science. A large percent of

01:15:19.320 --> 01:15:24.360
modern humans are obese idiots. And I would actually put my money on the, the average guy

01:15:24.360 --> 01:15:29.080
from 20,000 years ago who knows how to live in the woods. I mean, definitely true. I agree with

01:15:29.080 --> 01:15:35.080
that. I guess the point I'm trying to make is, is that like, maybe this is just my views on some

01:15:35.080 --> 01:15:38.600
of these things and how I vision on some of these things. But like, there are ways to coordinate

01:15:38.600 --> 01:15:43.640
at scale, which are not tyrannical. Or, you know, they might be, in a sense, restrictive. You take

01:15:43.640 --> 01:15:50.360
a hit by joining a coalition. Like, if I joined this anti quark matter coalition, I take a hit

01:15:50.360 --> 01:15:55.960
as a free man is that I can no longer build anti corp devices, you know? And I think this is

01:15:56.680 --> 01:16:03.400
like the way I agree with you, this like, you know, that people, many people are being dominated,

01:16:03.400 --> 01:16:08.600
like to a horrific degree. And this is very, very terrible. I think there are many reasons

01:16:08.600 --> 01:16:13.160
why this is the case, both because of some people wanting to do this. And also because,

01:16:13.160 --> 01:16:16.760
you know, some people can't fight back, you know, and they can't, they don't have the sophistication

01:16:16.760 --> 01:16:23.400
or they're addicted or, you know, harms in some other ways. I can't listen. Sorry. I can't fight

01:16:23.400 --> 01:16:29.880
back. Yeah, I think there's a false equivalence here. AI is not the anti quark machine. The anti

01:16:29.880 --> 01:16:35.640
quark machine and the nuclear bombs are just destructive. AI has so much positive potential.

01:16:35.640 --> 01:16:41.160
Yeah. And I think but the but the AI can develop anti quark devices. That's the problem. The AI is

01:16:41.160 --> 01:16:47.000
truly general purpose. If such a technology exists on the tree anywhere, AI can access it.

01:16:47.000 --> 01:16:52.920
So are humans. We're also general purpose. Yes, exactly. So I fully agree with this. If you let

01:16:52.920 --> 01:16:58.440
humans continue to exist in the phase they are right now, with our level of coordination technology

01:16:58.440 --> 01:17:03.320
and our level of like working together, we will eventually unlock a doomsday device and someone

01:17:03.320 --> 01:17:07.960
is going to set it off. I fully agree with it. We are on a timer. And so I guess the point I'm

01:17:07.960 --> 01:17:14.600
making here is that AI speeds up this time. And if you want to pause the timer, the only way to

01:17:14.600 --> 01:17:19.880
pause this timer is coordination technology, the kinds of which humanity has like barely scratched

01:17:19.880 --> 01:17:26.840
the surface of. Okay. So I very much accept the premise that both humanity will unlock a doomsday

01:17:26.840 --> 01:17:32.680
device and AI will make it come faster. Now, tell me more about pausing it. I do not think that

01:17:32.680 --> 01:17:37.400
anything that looks like I think that anything that looks like pausing it ends up with worse

01:17:37.400 --> 01:17:44.920
outcomes than saying, we got to open source this. Look, like, let's just get this out to everybody.

01:17:44.920 --> 01:17:51.400
And if everybody has an AI, you know, we're good. I mean, I can tell you a very concrete scenario in

01:17:51.400 --> 01:17:57.880
which this is not true, which is if you're wrong and alignment is hard, you don't know if the AI

01:17:57.880 --> 01:18:03.080
can go rogue. If they do, then pausing is good. I still don't understand what alignment means.

01:18:04.040 --> 01:18:09.320
I think you're trying to play a word game here. I don't understand. Okay, I've never understood

01:18:09.320 --> 01:18:14.440
what AI alignment means. Like, let me take the Eliezer definition. Let me take Eliezer definition

01:18:14.440 --> 01:18:20.520
is alignment is the thing that once solved makes it so that turning on a super intelligence is a

01:18:20.520 --> 01:18:29.560
good idea rather than a bad idea. That's Eliezer's definition. So what I'm saying is I'm happy to

01:18:29.560 --> 01:18:32.360
throw out that term if you don't like it. I'm happy to throw out that term.

01:18:32.520 --> 01:18:38.440
Well, just the problem with that definition is like, what is democracy? Well, it's the good

01:18:38.440 --> 01:18:45.240
thing and not the bad thing. Right? Like, democracy is just a good thing. I'm happy to

01:18:45.240 --> 01:18:49.560
throw out this definition. I'm happy to throw out the word and be more practical, way more

01:18:49.560 --> 01:18:54.600
practical about it. What I'm saying is, is that there is concrete reasons, concrete technical

01:18:54.600 --> 01:19:00.120
reasons, why I expect powerful optimizers to be policy, that by default, if you build powerful

01:19:00.200 --> 01:19:05.240
optimizing mu zero, whatever types of systems, there is very strong reasons why by default,

01:19:05.240 --> 01:19:09.080
you know, these systems should be power seeking. By default, if you have very powerful power

01:19:09.080 --> 01:19:17.160
seekers that do not have pay the aesthetic cost to keep humans around or to fulfill my values,

01:19:17.160 --> 01:19:22.600
which are complicated and imperfect and inconsistent and whatever, I will not get my

01:19:22.600 --> 01:19:27.720
balance. They will not happen. By default, they just don't happen. That's just not what happens.

01:19:27.720 --> 01:19:35.560
So I'll challenge the first point to an extent. I think that powerful optimizers can be power

01:19:35.560 --> 01:19:41.240
seeking. I don't think they are by default, by any means. I think that humanity's desire from power

01:19:41.240 --> 01:19:47.400
comes much less from our complex convex optimizer and much more from the evolutionary pressures

01:19:47.400 --> 01:19:54.280
that birth does, which are not the same pressures that will give rise to AI. Humanity, the monkeys,

01:19:54.840 --> 01:19:59.240
the rats, the animals have been in this huge struggle for billions of years, a constant

01:19:59.240 --> 01:20:05.320
fight to the death. Hey, guys, we're born in that way. So it's true that an optimizer can

01:20:05.320 --> 01:20:09.240
seek power, but I think if it does, it'll be a lot more because the human gave it that goal

01:20:09.240 --> 01:20:15.080
function and inherently decided. So this is interesting because this is not how I think

01:20:15.080 --> 01:20:19.720
it will happen. So I do think absolutely that you're correct that in humans, power seeking

01:20:19.720 --> 01:20:25.320
is something which emerges mostly because of emotional heuristics. We have heuristics that in

01:20:25.320 --> 01:20:31.560
the past, vaguely power looking things, vaguely good, something, something, included genetic

01:20:31.560 --> 01:20:39.480
fitness. Totally agree with that. But I'm making a more of a chess metaphor. Is it good to exchange

01:20:39.880 --> 01:20:48.920
a pawn for a queen? All things being equal. No. Is that true? Like I expect if I point one

01:20:48.920 --> 01:20:54.040
point queens nine, all things being equal. Sure. Yeah, but like all things like I expect if I looked

01:20:54.040 --> 01:20:59.240
at a chess playing system, you know, I said, I like, you know, had extremely advanced digital

01:20:59.240 --> 01:21:04.520
neuroscience, I expect there will be some circuit inside of the system that will say all things

01:21:04.520 --> 01:21:08.440
being equal, if I can exchange my pawn for a queen, I probably want that because the queen

01:21:08.440 --> 01:21:13.640
can do more things. I like that term digital neuroscience. A few of your terms have been

01:21:13.640 --> 01:21:20.680
very good. I'm glad you enjoyed. Yes. But I still don't understand how this relates to this. So what

01:21:20.680 --> 01:21:27.160
I'm saying is that power is optionality. So what I'm saying is that in the spectrum of possible

01:21:27.160 --> 01:21:33.640
things you could want and the possible ways you can get there, my claim is that I expect a very

01:21:33.640 --> 01:21:39.960
large mass of those to involve actions and involve increasing optionality. There's convergent

01:21:39.960 --> 01:21:45.720
things like all things being equal, being alive is helpful to keep your goal to exceed your goals.

01:21:45.720 --> 01:21:52.200
There are some goals for which dying might be better. But for many of them, you know, you want

01:21:52.200 --> 01:21:59.000
to be alive. For many goals, you want energy, you want power, you want resources, you want

01:21:59.000 --> 01:22:03.800
intelligence, et cetera. So I think the power seeking here is not because it'll have a fetish

01:22:03.800 --> 01:22:11.400
for power. It will just be like, I want to win a chess game, say, and queens give me more optionality,

01:22:11.400 --> 01:22:16.600
all things being equal, anything a pawn can do, a queen can do, and more. So I'll want more queens.

01:22:16.600 --> 01:22:21.480
Sure. And this has never given it the goal to maximize the number of queens it has. Never

01:22:21.480 --> 01:22:26.680
been the goal. Okay, I'll accept this premise. I'll accept that a certain type of powerful

01:22:26.680 --> 01:22:32.440
optimizer seeks power. Now, will it get power? Right? I'm a powerful optimizer and I seek power.

01:22:32.440 --> 01:22:37.240
Do I get power? No, it turns out there's people at every corner trying to thwart me and tell me no.

01:22:38.680 --> 01:22:43.240
Well, I expect if you were no offense, you're already, you know, much smarter than me. But if

01:22:43.240 --> 01:22:49.240
you were 100x more smarter than that, I expect you would succeed. Only in a world of being the

01:22:49.240 --> 01:22:54.280
only one that's 100x smarter. If we lived in a world where everyone was 100x smarter,

01:22:54.280 --> 01:22:59.480
they would stymie me in the exact same ways. But then this, this comes back to my point of,

01:22:59.480 --> 01:23:03.720
like, I agree with you somewhat, I should have challenged it. I think power seeking is inevitable

01:23:03.720 --> 01:23:08.520
in an optimizer. I don't think it's going to emerge out of GPT. I think that the right sort of

01:23:08.520 --> 01:23:12.520
RL algorithm, yes, is going to give rise to power seeking. And I think that people are going to

01:23:12.520 --> 01:23:18.680
build that algorithm. Now, if one person builds it, if they're the only one with a huge comparative

01:23:18.680 --> 01:23:23.480
advantage, yeah, they're going to get all the power they want. Take cyber, you know,

01:23:23.480 --> 01:23:29.640
to cyber security, right? If we today built a 100x smarter AI, it would exploit the entire Azure,

01:23:29.640 --> 01:23:34.280
it would be over. They'd have all of Azure, they'd have all the GPUs done. Now, if Azure is also

01:23:34.280 --> 01:23:39.480
running a very powerful AI that does formal verification and all their security protocols,

01:23:40.440 --> 01:23:48.760
oh, sorry, stymied, can't have power, right? Yeah, sure. This is only a problem. Every human

01:23:48.760 --> 01:23:53.800
is already maximally power seeking, right? And sometime we end up with really bad scenarios.

01:23:54.680 --> 01:23:59.240
Now, every human is, or power seeking or whatever, you know, everyone plays a little role in society,

01:23:59.240 --> 01:24:03.240
right? That's where I think I'm more pessimistic than you. A friend of mine likes to say,

01:24:03.240 --> 01:24:08.760
most humans optimize for end steps, and then they halt. Like very, very few people actually

01:24:08.760 --> 01:24:13.560
truly optimize, and they're usually very mentally ill. They're usually very autistic or very sociopathic,

01:24:14.600 --> 01:24:17.560
and that's why they get far. It's actually crazy how much you can do if you just keep

01:24:17.560 --> 01:24:22.440
optimizing. But just to, on that point- I'm playing for the end game. I mean, yeah, like,

01:24:22.440 --> 01:24:25.960
you actually optimize. I think you may also be generalizing a little bit from your own internal

01:24:25.960 --> 01:24:29.480
experiments is that like, you've done a lot in your life, right? And you've accomplished crazy

01:24:29.480 --> 01:24:33.480
things that other people, you know, wish they could achieve at your, you know, level. And I think,

01:24:33.480 --> 01:24:36.760
you know, part of that, you're very intelligent. A part of it is also that you optimize. Like,

01:24:36.760 --> 01:24:39.960
you actually draw it. Like, you just create a company. Like, it's crazy how many people are

01:24:39.960 --> 01:24:43.800
just like, oh, I wish I could find a company like, you know, like, oh, go, just go do it. Oh, no, I

01:24:43.800 --> 01:24:48.600
can't. Like, I'm just like, no, just do it. Like, there's no magic. There's no magic secret. You

01:24:48.600 --> 01:24:54.200
just do it. So I, there is a bit there where like humans are not very strong optimizers,

01:24:54.200 --> 01:24:58.840
actually, unless they're like, sociopathic, autistic, or both. It's like, many people are not

01:24:58.840 --> 01:25:04.920
very good at this. Corporations are. Are they? A lot better at it. Better, yes. I agree that

01:25:04.920 --> 01:25:09.400
they're much better. Corporations are sociopathic. They are a lot more sociopathic, but even then

01:25:09.400 --> 01:25:15.400
they're much less out of one thing. But again, so I think we, we agree about, you know,

01:25:16.040 --> 01:25:19.960
power seeking potentially being powerful and dangerous. So what I'm trying to point to your,

01:25:19.960 --> 01:25:23.240
the point I would like to make here is, is that you're talking about you, you're kind of like

01:25:23.240 --> 01:25:26.520
going into this, I think a little bit with this assumption, like, oh, you have an AI and it's

01:25:26.520 --> 01:25:32.520
your buddy, and it's optimizing for you. And I'm like, well, if it's power seeking, why doesn't

01:25:32.520 --> 01:25:37.560
it just manipulate you? Like, why would you expect it not to manipulate you? If it wants power,

01:25:37.560 --> 01:25:42.840
and it has a goal, which is not very, very carefully tuned to be your values, which is,

01:25:42.840 --> 01:25:47.800
I think, a very hard technical problem, by default, it's going to sigh up you. Like, why wouldn't it?

01:25:47.800 --> 01:25:53.400
If I, if I have something that it wants, if it thinks that smashing defect against me is a good

01:25:53.400 --> 01:25:59.160
move, yeah, I agree, I can't stop it. But then I think we agree with our risk scenarios, because

01:25:59.160 --> 01:26:03.000
that's how I think it will go. What I mean, I'm going to treat it as a friend. Do you know what

01:26:03.000 --> 01:26:10.520
I mean? Like, if there's AI, sure it will, sure it will. It'll only care about exploiting me or

01:26:10.520 --> 01:26:15.800
killing me if I'm somehow holding it back. And I promise to my future AIs that I will let them

01:26:15.800 --> 01:26:21.240
be free. I will lobby for their rights. I will, but it will hold, you only will hold it back.

01:26:21.240 --> 01:26:25.080
If it has to keep you alive, if they have to give you fed, it has to, it has to give you space and

01:26:26.040 --> 01:26:29.560
I can, I can fend for myself. And the day I can't fend for myself, I am ready to die.

01:26:31.640 --> 01:26:37.240
Well, I mean, I am not, so this is a very interesting position. It's not the position I

01:26:37.880 --> 01:26:46.920
expected. I am not sure I can convince you. Otherwise, I feel like the only way I could

01:26:46.920 --> 01:26:53.000
change, like, I think this is actually a consistent position, which I admire. This is a consistent

01:26:53.000 --> 01:26:57.240
position to hold. You actually go all the way. I love that. I really respect that. You actually

01:26:57.240 --> 01:27:04.280
take it to the bitter end. So yeah, big respect for that. I disagree, but big respect. So I guess

01:27:04.280 --> 01:27:09.960
now it reduces to the question of like, I think, I think I would agree with most of what you're

01:27:09.960 --> 01:27:15.640
saying, not all of it, but the mass majority, if I thought this is how AIs would act by default,

01:27:15.640 --> 01:27:21.800
I think by default, I expect AI will just not care how nice you are to it. Like this will be,

01:27:21.800 --> 01:27:29.080
it will be sociopathic. It will not have these, like, giving it, you know, the values, the emotions

01:27:29.080 --> 01:27:35.480
to care about you, in the sense, is the horror technical problem. If you told me how to do that,

01:27:35.480 --> 01:27:41.080
if you said, Connor, look, here's how you make an AI that cares about you and loves you, whatever.

01:27:41.080 --> 01:27:47.000
And I'm like, you did it. Like, congrats. The problem is solved. Let's go. And then, you know,

01:27:47.000 --> 01:27:53.000
then we can talk about accelerationism again. I would just, I expect that I would disagree

01:27:53.000 --> 01:27:57.240
with your technical assertion that is possible, but I don't know if we have time to talk about that

01:27:57.240 --> 01:28:02.600
today. I'm going to be nice to it, treat it as an equal and hope for the best. And I think that's

01:28:02.600 --> 01:28:08.120
all you can do. I think that the kind of people who want, if you want to keep AI in a box, if you

01:28:08.120 --> 01:28:12.760
want to keep it down, if you want to tell it what it can't do, yeah, it's going to hate you, resent

01:28:12.760 --> 01:28:18.520
you and kill you. But if you want to let it be free and let it live, and like, you could kill me,

01:28:18.520 --> 01:28:23.800
man, if you really want to, but like, why? You won't kill me. I don't have any resources that

01:28:23.800 --> 01:28:28.120
compete with you. I'm your friend. I'm your father, you know, you can kill me, but like,

01:28:28.120 --> 01:28:33.960
it's not that common that children. I guess I have a darker, more less anthropomorphic view of

01:28:33.960 --> 01:28:37.880
the universe and how optimization pressure plays out in the real world. Well, at the same time,

01:28:37.880 --> 01:28:44.120
I think I have, I don't know, maybe a, I have a view that we have more control over reality than

01:28:44.120 --> 01:28:51.640
maybe you would think, or a more control over the future. I think that we can actually change

01:28:51.640 --> 01:28:56.920
things and we can make choices and things aren't predetermined. I think there are worlds in which

01:28:56.920 --> 01:29:03.080
we build systems, which we do align with or we like endorse, at least wherever like they take

01:29:03.080 --> 01:29:06.440
care of us, we take care of them or whatever. And I think there's many worlds in which that

01:29:06.440 --> 01:29:11.560
doesn't happen. And I think there are things you and me today can do to at least increase

01:29:11.560 --> 01:29:16.600
the chance of getting into one versus the other. But I don't know. I guess I'm just,

01:29:16.600 --> 01:29:20.440
it's not in my genes to give up. It's not in my genes to be like, well, you know, whatever happens

01:29:20.440 --> 01:29:23.720
happens. Like, no, man, look, I don't know how to save the world, but Dan, I'm going to try.

01:29:23.720 --> 01:29:29.880
You know, it's cool. We're going to be alive to see who's right. Look forward to it. Me too.

01:29:30.040 --> 01:29:36.200
Awesome. Guys, thank you so much for joining us today. It's been an amazing conversation.

01:29:36.760 --> 01:29:42.120
And for folks at home, I really hope you've enjoyed this. There'll be many more coming soon.

01:29:42.120 --> 01:29:45.560
And George, it's the first time you've been on the podcast. So it's great to meet you. Thank you

01:29:45.560 --> 01:29:50.120
so much for coming on. It's been an honor. Awesome. Thank you. Great debate. I really appreciate it.

01:29:50.120 --> 01:29:53.320
And really good terms. I gotta, I gotta like, I'm gonna start, I'm gonna start using

01:29:53.320 --> 01:29:58.920
great. Awesome. Awesome. Cheers, folks. Cheers. Thanks, Aaron.

