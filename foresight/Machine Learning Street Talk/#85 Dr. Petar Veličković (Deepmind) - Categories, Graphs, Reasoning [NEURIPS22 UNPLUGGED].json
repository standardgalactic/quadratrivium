{"text": " Peter Velichkovich is a staff research scientist at DeepMind. He's firmly established himself as one of the most significant and up-and-coming researchers in the deep learning space. He invented graph attention networks in 2017 and he's been a leading light in the field ever since, pioneering research in graph neural networks, geometric deep learning and also neural algorithmic reasoning. Recently he's been applying category theory to take the geometric deep learning ideas one step further. If you haven't already, you should check out our show that we did on the geometric deep learning blueprint, which of course featured Peter and I caught up with him last week at NeurIPS. Enjoy. Peter, it's fantastic to see you again. So this is the first time that I've actually met you in person. We did that really cool show together on geometric deep learning with your proto book with Takko. I spoke with Takko yesterday, but also Michael Bronstein and Joanne Brunner. So anyway, it's been a little while since we've really synced. Now you've had this really, really interesting category theory series. Can you start by just letting us know what you've been doing there? Yeah, that's a great point and great to finally meet you in person, Tim. It's really great to catch up after some time has passed. And yeah, I mean, I like to think that all four of us, myself, Michael, Joanne and Takko have a greater understanding of the implications of these methods since the last time we spoke. If you remember back when we did our conversation, I kind of hinted at the fact that category theory might hold some of the answers to maybe generalize some of these geometric concepts beyond the notion of just pure symmetries. And we believe that now we have a sufficient understanding of these kinds of things that we were able to make this kind of mini course on categories for deep learning. And to me, it really feels like the natural continuation of these concepts of geometric deep learning into the realm beyond. And I'll explain that in a moment. But one other kind of very related point is that here at NeurIPS, we're actually presenting a full conference paper which deals with using category theoretic tools to invent new kinds of graph neural networks. So basically, it's not just that we're throwing a bunch of new theory, it actually leads to empirical findings that we can actionably use in our models day to day. So that's one point. That is incredible. Can you sketch out the paper? Yeah, sure. So basically, maybe I'll first take a step back to explain why do we think categories are important and in what sense they're kind of a step further from what geometric deep learning already gives us. So geometric deep learning concerns itself with giving us these equivalent layers, right? So layers that are in some sense resistant to operations of these symmetry transformations is that fundamentally change an object, but the object is still the same. We still have all of it, right? And this immediately implies that these symmetries have to be composable, invertible, all that sort of stuff. And yeah, essentially, the category theory framework is in some sense mindful of the fact that while symmetries are a very nice way to reason about things that happen and that we see in nature, they're often not completely an accurate representation of what happens. Very often there are operations both in nature, but especially in general computation, like say in algorithmic stuff, where an operation of an algorithm might destroy half of your data. So that is no longer a symmetry. You cannot invert it. But you might still be interested in building a neural network model that is in some sense resistant to the operations of say this algorithm or this natural phenomenon that you're studying. One simple example that maybe predates our work a little bit is building some kind of equivalence to scaling operations. Obviously if you scale or course in something, these are not always invertible transformations because if you course in the pixels of your image, you cannot perfectly reconstruct where you came from. Yet you still might want to build a model that will give you the same answer as regardless of how you scale up your input, right? So these are obviously things that are going to be very important as we move to more generic domains than ones that can be described purely through geometric and symmetry transformations, right? And in that sense, the same way we had groups, representations, and equivalence in geometric deep learning, these are all special cases of categorical concepts like categories, functors, and natural transformations, which basically generalize all the stuff of geometric deep learning into their own beyond. And in our paper, we try to use exactly these kinds of category theoretic tools to study what it would mean to build a say graph neural network that is capable of behaving like a classical computer science algorithm. In the sense that if you have some data that's transformed by an algorithm, you may imagine, say, a path finding algorithm where at every step in every node, you have your knowledge of how far away is that node from the source vertex. And one step of the algorithm kind of looks at all the immediate neighbors and updates those beliefs of how far away you are. And I'll say you want to have a GNN that simulates that, like we typically do in algorithmic reasoning. You take your algorithmic state, you encode it with a neural network into this high dimensional space. Your GNN then processes it to update the latent space. And now you want to be able to decode it so that you predict whatever the next state is going to be. So you have something which in category theory we use a lot is known as a commutative diagram. So basically it's saying you can either take the step of the algorithm or you can encode, process, decode, and hopefully end up in the same place. So category theory seems like a very nice language to study. These kinds of, I won't call them symmetries, they're basically like interchangeable sequences of operations because the step of an algorithm might not be invertible. You might not be able to go back after you do one step of, you know, shortest path algorithm because it's a contraction map, right? When you find the final solution of a shortest path algorithm, you won't necessarily know which previous state led you there because there could be many equivalent states that could lead you to the same contracted solution, right? So our method, using these category theory frameworks, try to characterize how these graph neural networks align with a target algorithm that we might want to simulate. Then we detect various ways not only to explain the code of graph neural networks from this kind of perspective, but also it gives us a very interesting sort of, if you've done any functional programming, a type checker of sorts to kind of detect whenever we're using our representations in slightly broken ways. So specifically to give you one very concrete example, in a categorical framework, just like in functional programming, you expect your transformations to be functions. That is, for every input there should be a unique output. However, one thing that people very often do in graph representation learning, when they want to predict outputs not only in the nodes, but also in the edges, is to reuse the edge messages both as edge outputs and integrated overall the other messages to get node outputs, right? But this is a problem from the categorical perspective because this is no longer a function. You cannot get a function that takes, you know, edges to edges plus nodes without sending the same thing into two different places in this case, right? And, you know, just because it mathematically breaks doesn't mean you cannot implement it. In fact, 99% of the GNN implementations you'll find online will do this exactly in this particular way. DeepMindsGraphNet's library does this, for example. However, you know, just because you can implement it doesn't mean that there's something not potentially a bit tricky going on in the sense that you're putting a bit of representational pressure on that edge message, right? Because now it has to be used for two potentially very different things, both for some output in the edges, but also it needs to be integratable into nodes where it predicts something potentially wildly different. And, you know, while gradient descent can take care of this and give you a model that fits your training distribution well, you're not like, to deal with this pressure, it's probably going to have to learn something which has nothing to do with the algorithm that you want to align to. And as a result, you're out of distribution, extrapolation performance is going to be much, much worse. And any self-respecting algorithm should extrapolate well. That's the main property of algorithmic reasoning, right? And we find that just by, you know, splitting this message function into two streams, one which goes into the edges and one which goes into the nodes, we get basically significant empirical benefits when extrapolating on edge-centric algorithms. Yeah. Amazing. So, Epeta has just produced this incredible series which is available on YouTube. Where can folks find it? Yeah. So, basically, if you just go to cats.4.ai, you can see all of the main series lectures from our course, which starts off with assuming kind of a foundational knowledge of deep learning with neural networks, back propagation, and so on, and then also tries to introduce these concepts of category theory and how we can use them to rethink the way we might go about some of our standard ideas in deep learning like compositionality or functional structure of deep learning pipelines, or even how can we reinterpret back propagation from the perspective of categorical theory. And each lecture basically deals with one particular aspect and we try to keep it grounded from the beginning to keep it motivated so every single lecture is aligning itself with one particular top-tier paper that one of us has published on one of these venues like in Europe. And one thing I'll also mention is that the course is actually, in principle, still ongoing because besides the main series of five lectures that myself, Bruno, Pym, and Andrew have given, we also have several interesting guest lectures where we try to bring in other influential people at the intersection of popularizing category theory with deep learning concepts in a way that can bring an even wider area of views once you're kind of trained in the basics of these techniques, how they're applied to various other things like causality. We had Taco Cohen tell us about how he uses these concepts to reimagine causality through a categorical lens. We're going to have Tydenay Bradley, she's a very popular mathematics educator generally. She will show how she used some of these concepts to explain transformers. And one thing I'm very excited about early next year, we will have actually a guest talk from David Spivak, which is one of the co-authors of the very famous Seven Sketches in Compositionality book, which is what initially one of the things that got me really excited about category theory in the first place. So I'm really keen to hear all these perspectives as well. The man is a legend. And also on Taco, I interviewed him yesterday and his work on causality is really, really exciting. What would you say to people who might be intimidated or scared by category theory? So one thing that I should mention here is that one point about being intimidated or scared about category theory is that to really be able to utilize these ideas in how you do research or build your models or anything, it does require a reasonably significant buy-in. So this is not something that you can just read one blog post and suddenly you're empowered to do it. This is like one key thing. But I would say the main thing that might make people a bit scared to do it is the fact that many category theory resources out there are a bit guided towards mathematicians. So they will tend to use the kind of language and the kind of examples that will be quite attractive to someone who has studied, say, various kinds of differential geometry or topology or something like this. And these kinds of areas tend to generally scare off people who come from a more computer science style background. And basically I would say the answer to that is you need to find the right resource for you. Category theory is no more or no less than a way to take a bird's eye view of the phenomena that you try to study. And when you study these phenomena from high in the sky, details become invisible, but you suddenly get a much better feel for the structure and you can utilize kind of the nice patterns that reappear across various fields. And this you would argue is kind of the essence of what we're trying to do in deep learning. We have a lot of analogical way in which these architectures are constructed, right? So cats for AI is one possible answer to that. It's our way to kind of, as half of us are deep learners and half of us are category theorists, trying to apply these techniques to deep learning. We believe we have a sort of unique perspective of we and like we understand what makes people afraid to try to talk about these things because some of us had to go through it ourselves to deal with the way in which the materials are arranged online right now. So yeah, maybe just these kinds of resources, starting with them and basically trying as much as possible not to descend into the depths of NCAT lab as the very first thing that you do can be a good way to maybe stay sane during the first few weeks or months of trying to explore this field. Wonderful. I wondered if you could give a couple of examples of where category theory has been used in an adjacent field. I can think of too. I can think of Rosen using category theory to describe, you know, sort of ecosystems and life. I can also think of some quantum mechanics folks that have come up with a category theoretical conception of quantum mechanics. Right. Are there any other ones? Yeah. So I mean, I can start by giving the examples that I know about closest in terms of just deep learning. So one particular example that I think could be quite interesting is the work that was published at NeurIPS two years ago, which I think is one of the first papers that really tried to use categorical concepts to build these structures, is the natural graph networks paper from Pimdehan, Tapocoin and Max Swelling, which effectively realizes the fact that the way we build graph neural networks very often we have this one shared message function that's applied everywhere on every single edge on every single graph that you get. But in reality, is this necessary for it to be a legitimate graph neural network? That's actually not the case because if I give you two completely non-isomorphic graphs, if I choose to have completely different message functions in those two graphs, that's totally fine because it's still a valid graph net. If I permute any of those graphs, I'll get the permutation equivalent function for the two of them separately. There needs to be no weight sharing between them and naturally concepts like these. So this kind of requires taking a step above the group theoretic view of geometric deep learning and into the realm of what is known as a group poid. You kind of imagine every single graph structure, isomorphic graph structure, living on a sort of island of possible adjacency matrix representations of it. And for those graphs living on those islands, you need to have some weight sharing. But for separate islands, you don't need to have any weight sharing whatsoever. Of course, in practice, these kinds of layers, you would need to have some kind of sharing of weights in order to make them scalable to arbitrary new graph structures you haven't seen at training time, but it allows you a lot more flexibility about how you go about building your functions. And you're no longer constrained to have just one function everywhere repeated, right? So that's maybe one example that, at least to me, was what first motivated me and made me realize that there's more to this stuff than just to say what group theory will give us. Amazing. I'm really interested in your work in algorithmic reasoning, and I know you were just discussing it as an adjacent thing, and very soon we want to make a show, actually, on your work on that. But if you wouldn't mind, could you just sketch out algorithmic reasoning? Yes, wonderful. So, very happy to. Basically, what are we interested in algorithmic reasoning is building neural networks. They tend to be graph neural networks, but generally speaking, neural networks that are capable of executing algorithmic computation. So if I give you some context on what is the state of a particular algorithm, can my network somehow learn to execute that algorithm ideally in some latent space such that at every single step of the way, I could if I wanted to decode the states of that algorithm. So that's basically the main premise. Why do we care about this? Well, basically, I think of algorithms as a sort of basic foundational building block of reasoning, and it's kind of a timeless principle where a software engineer reads through one of these textbooks on algorithms and learns these 30 or 40 basis algorithms, and then that knowledge serves them for life in a whole career of software engineering. So basically, we have this hypothesis that you have this nice basis of algorithms that if you can master how to do them robustly, you can try to mimic any kind of at least polynomial time reasoning behavior. And that's really nice because if you look at the way current state-of-the-art large-scale models tend to have shortcomings, it's usually in those kinds of robust extrapolation problems. Basically, if we want to have a really good AI scientist that's able to not just make great sense of a bunch of training data from the internet, but also use that training data to derive new knowledge, you need some robustified way to apply rules to get infinite knowledge from finite means. So basically, that's what we want to do. We want to find ways inductive biases or training procedures to build neural networks that are more algorithmically capable. And in algorithmic reasoning, we obviously spent a lot of time trying to make this happen, just building better graph neural networks that align better with target algorithms so that you can execute them better, but then the really exciting part comes where we've actually taken some of these graph neural networks that have been pre-trained to execute one particular algorithm, and then we deployed it in a real-world problem where that algorithm is required, and we achieved, say, significant representational benefits in terms of downstream accuracy. So the idea behind this, and I'll give an example from Google Maps. This is an application that I worked on at DeepMind, so it's something that I've thought about quite a bit. We've invented these algorithms, like Dijkstra's algorithm, to be able to resolve these kinds of real-world routing problems. That's the kind of motivation for why you want to build the shortest path algorithm. And it comes as a little surprise that when you have real-world traffic data, you might be tempted to apply Dijkstra's algorithm to solve it, to route agents in traffic. However, what is the actual data that, say, Google Maps has access to? It's not this nice, abstractified graph with a single scalar in every edge where you can just go ahead and apply an algorithm. In fact, there's a huge bridge that must be built between the real data and the input to the algorithm. In fact, Google Maps data is typically people's cell phones in their cars, and the cars move, the phones move, and then based on the movement of the phones, you somehow infer how fast the car is going or something like that. And this is very noisy, not very well-structured, and you have to somehow go from there to a graph where you can apply this heuristic. Previously, it was always done exclusively by humans, like feature engineers, effectively. And whenever there's a human feature engineer in the loop like this, you are almost certainly going to drop a lot of information that you might need to solve the problem. So basically, you have a huge kind of bridge to cross there. And with algorithmic reasoning, we now don't use Dijkstra's algorithm. We use a high-dimensional graph net that was pre-trained to execute Dijkstra's algorithm in a latent space. So now this gives us a differentiable component that we can hook up to any encoder and decoder function we want to, so we can go straight from raw data and code it into the GNN's latent space, run the algorithm there, and then decode whatever it is that you need, like routing the vehicles in traffic. So now purely through backprop, this encoder function now learns to do what the human feature engineer did. It learns how to most effectively map that complicated, noisy, real-world data into the latent space where this GNN can best do its thing. That really is software 2.0. But I wanted to ask you about the computational limitations, because you just said something interesting about representing infinite objects with a finite memory. So neural networks are not Turing machines, but they can extrapolate, of course. What's the realistic limitation? Let's say you're trying to learn an algorithm, how far can you go with a neural network? So the thing is, there are cases where you can go very far. We do have theory that is very robust about this, and I think it's theory that is actually quite easily understandable. So let me try to kind of visualize it. When you have a real UMLP, your standard universal approximator, it's basically a piecewise linear function. So as you go far enough away from the training data, you're going to hit that level of extrapolation where you hit the linear part of the piecewise linear. And at that point, if your target function is not linear, no extrapolation is going to happen. You're not going to fit the function properly. So what's one outcome of this theory is that if you use real UMLPs, this was a great paper from MIT a few years back, which showed that basically you need to line up parts of your neural network such that they learn linear functions in the target. And that's the reason why, say, when you want to imitate a pathfinding algorithm, you want to use a max aggregation, your GNN, and not sum, where sum is universal. It can fit anything. But the function you have to learn, because pathfinding is like minimum overall neighbors of distance to neighbor plus the edge weight, suddenly when you put max in there, it's a linear function. When you put sum in there, it's a highly nonlinear function, so it's going to extrapolate much worse. Now, there's been some great follow-up work on this from Beatrice Bevilacqua, Bruno Ribeiro from Purdue University. That was at ICML a few years back, which showed that this idea with, like, you want linear targets with real UMLPs, it's really just a special case of a more general idea that if you want to extrapolate, say, on different sizes of graphs, you need to have some implicit causal model of what your test data is going to look like. This linear algorithmic alignment is just one special case of a causal model like that. So basically, if you line things up properly from a causal perspective, you should, in principle, be able to extrapolate. I mean, we have a clear nonparametric evidence that you can extrapolate is the algorithm itself, right? Now, the key is to find the right sweet spot between full universal approximator MLPs and algorithms on the other side, right? Interesting. I spoke to Jan the other day. He had a paper a couple of years ago about extrapolation in neural networks, saying they always extrapolate. Yes. And speaking with Randall Belastriero, and he's got this paper, the Spline Theory of Neural Networks, which is about, you know, these input sensitive polyhedra in the ambient space. And I always took that to mean why they're quite interpolative and it's just an affine transformation for a single input. But what he's shown, though, is that actually, even an MLP with relus is extremely extrapolative because you can remove a whole bunch of data and, depending on how you've designed the network architecture, it will still inform that region that you've taken away. So, I mean, are you familiar with the Spline Theory and do you think it's a useful framework? Yes. So, one thing I would say, the way I understand Jan's paper, it could be that I missed some detail, but the way I understand it is that here we're talking about interpolation and extrapolation with respect to the geometry of the data. So, like, you take, say, the convex hull of all the training points and then, yeah, it's very common, especially in these high dimensional image spaces, right? It's very easy to push one dimension sufficiently to escape the convex hull of what you've seen so far. So, I guess when I say extrapolation out of distribution, I'm actually maybe thinking of a more probabilistic argument, so something like if you think of the probability distribution induced by the training set, which obviously allows you to extrapolate away from the convex hull, right? But if you go sufficiently far from the modes of that distribution, so you explore a part of the space that hasn't really been covered, you know, from a probabilistic mass point of view in the training data, that is what we're actually thinking of when we say out of distribution generalization. But, yeah, I fully agree with you, like, in terms of just convex hull arguments, we very often ask these regular MOPs to go beyond the convex hull, and they seem to work quite well in those regimes. But here, I'm talking really about going, like, significantly beyond the convex hull to, like, some region that really wasn't touched. And what we do, for example, in our papers is we train on 16 node graphs to execute these algorithms, and then we test it on four times larger, 64 node graphs. And what this means, because an algorithm might have, say, n-cubed time complexity, it means the trajectory over which you have to roll it out is also much, much longer than what you've seen in training time. So it's really a test of, like, very different conditions than what you've seen in training time, right? That's interesting. And first of all, I completely agree with you that this binary convex hull notion of extrapolation probably isn't particularly useful. But, you know, folks like Francois Relais describe the way Neuron Network's work is kind of bending the space, you know, progressively with layers. I really like this polyhedra idea. Contrast the algorithmic reasoning with GNNs, so, I mean, I spoke with Hattie from a Google brain team the other day, she's doing the in-consex prompting, you know, sort of algorithm learning. How would you contrast those two approaches? So basically, I would really like these approaches to be reconciled going forward in the sense that, like, I don't see them as going one without the other, if that makes sense. So on one side, and I'm going to invoke the same principles I mentioned during our MLST episode, you know, Daniel Kahneman's book, System 1 and System 2, right? I think you cannot have one without the other. So you have these amazing large-scale perceptive models that are really amazing at, you know, taking the complexities of the real world and somehow getting interpretable enough concepts out of there that they can, you know, make sense of what's going on and, like, drive many interesting real-world decision-making problems, although they might lose a little bit on having to do something like what an AI scientist would be expected to do, which is, like, extrapolate and generate new concepts out of what they've seen. And as you said, these kinds of specifically tailored prompts might enable the model to take things a step or two further, but it's always, like, it's kind of, in spirit, it's the same thing as algorithmic reasoning, because we teach a model to execute an algorithm by forcing it to imitate the algorithm step by step. Here you prompt a language model by telling it what are some of the steps, like, just like you're trying to teach a student how to solve a homework, right, telling them the individual steps they need to do, and then letting the language model go off on its own to solve it. But where I see the real future of these two methods converging is you're going to have your system one component that gets your concepts out very nicely, cleanly. And then those concepts, because we're working with transformers nowadays anyway most of the time, are going to be very slot-based. So that plays very nicely with GNNs, which expect nodes as input, right, so you can maybe hook up in some nice way those concepts into a graph neural network that was trained to execute a bunch of algorithms, and then, you know, kind of get the best of both worlds. So have your perceptual component do the perception, and maybe prompt it as well to kind of do it in a particularly step-by-step manner, and then further have a robust component that makes you not have to relearn all those things that neural networks we know theoretically cannot learn to do that well because of these extrapolation arguments. Maybe one last point I would make to kind of cement this. If you've been around the archive recently, you might have seen our paper on a generalist neural algorithmic learner where we have actually used GATO-style ideas to train one graph neural network that can execute 30 very diverse algorithms all in the same architecture with a single set of weights, so sorting, searching, pathfinding, dynamic programming, comics, hauls, all those kinds of nice things, very diverse ways of reasoning. We believe something like that could maybe be a basis of, say, a foundation model of reasoning in the future that could nicely hook up to the foundation models we already know and love in the realm of perception. Amazing. And what's the biggest research challenge for you next year? So next year, I would really like to show to what extent these things can scale in the real world. So we already have several isolated papers that showed that these ideas can work on real problems. We have Excelvin where we applied it to reinforcement learning. That was in Europe Spotlight last year. We have RMR where we applied it to self-supervision problems. We also have one paper currently under review at iClear where we successfully applied to supervised learning. So we say pre-trained on a flow algorithm and we deploy it on brain vessel segmentation tasks and stuff like that. So we have many isolated cases where you learn a particular algorithm and it works really well in a real world scenario. I would like to see how can we take this idea and truly put it to the test at larger scales, both in terms of number of problems we attack or number of nodes that we support or anything in between. Amazing. Dr. Patovali\u010dkovi\u0107, let's just, we'll get a shaking handshot. All right. Thank you so much for joining us. Thank you for having me. I really appreciate it. Dr. Ishan Mizra of Meta and Lex Friedman fame came over and had a chat with us. Ishan is one of the world's leading experts in computer vision. So what was your paper about? Yeah, basically we try to have global propagation, the likes of which you see in transformers, not like with sparse costs. So but in a way that will still allow you to have nice global communication properties and no bottlenecks and stuff like that. So we basically have this idea of you could generate these expander graphs which allow you to have nice sparsity properties. So basically every node I think has degree four in the graphs we compute and you need only logarithmically many steps to traverse the graph, which means you can still do it efficiently with a small number of steps. And yeah, it seems to empirically work well on a bunch of graph benchmarks. So yeah, it's a, I think it's only scratching the surface of what we can do because we literally just generate a graph at random and slap it onto like mask the computations, but yeah, it's an interesting start. Very nice. Yeah. How about your conference? How's it been? So it's been pretty good. We're organizing the self supervised learning workshop tomorrow, which is going to be probably, I hope like useful to a lot of people, we're going to have a bunch of speakers coming from vision, language, NLP, like speech and so on. And yeah, we're also presenting a poster there, which is about learning joint image and video representations, which are state of the art across image and video benchmarks using a single model. On the final day of the conference, I caught up with Petra again at the poster session for new reps, which is the symmetry and geometry and neural representations group. And his paper was selected by all of the reviewers at the conference as being in the top 10, which is super impressive, but this is Petra talking about his paper. So in the expander graph propagation work, we are trying to solve what is, in my opinion, one of the most important problems in graph representation learning currently unsolved, which is the oversplashing problem. And effectively it is a task, which it's a problem which plagues graph neural networks regardless of which parameters you choose or which model you choose. It's really something that often depends on the topology of the graph, and it's a situation where no matter how hard you try, no matter which parameters you set, the amount of features you would need to compute, so the size of your latent space would have to be exponential in the number of layers for the pairs of nodes to efficiently communicate. We don't always know when it happens, but very often it tends to happen around these bottlenecks. So basically in this particular graph, you have these two communities that are tightly connected, and you have this just one critical edge connecting them, and this edge is now under a lot of pressure. If you want data from these nodes to travel to these nodes and vice versa, this edge has to be mindful of a lot of things, so the size of the feature space required for this edge grows exponentially, and things get even worse when you look at trees. Trees are like the canonical worst case example, where cutting off this edge would really trigger all sorts of bottleneck cases, and essentially you need basically a number of, to store information about a number of nodes that goes exponentially in the number of steps, just to be able to travel to the other side of the tree. So this is a fundamental problem of propagating data, which has nothing to do with the choice of model, just topology. And what do we try to do to fix this problem? You would ideally want, so first we start off with the assumption that this kind of global talking is actually beneficial. Of course there are some tasks where you might not want data to travel in this way, because if it's a highly homophilus data-driven problem, then you might want information to stay in the community, to not get diluted. But we assume in many tasks, like say molecular property prediction tasks, you actually want data to travel globally, so that's exactly what we do. That's our first assumption. As we just described, we don't really want these bottlenecks to exist, because if there's a bottleneck, no matter what you do with the model, it's not going to work well. We would ideally want the complexity to be scalable, so we can apply this to graphs of arbitrary sizes. One simple solution to this problem is to use a graph transformer, which would connect every node to every other node and give you a trivially setting with no bottlenecks. However, as we will show later, these fully connected graphs are trivially dense expanders, actually. So they fit our theory, but they are dense and they won't scale. So we don't necessarily want that. And lastly, because it's often quite computationally painful to clear these bottlenecks in an input data-driven way, especially if you have lots of online graphs coming into your problem, we might ideally want a method that doesn't have to do like dedicated pre-processing of the input graph. And actually, satisfying all four of these at the same time turns out to be quite tricky. We actually have done a literature survey of a bunch of related works, and it seems really hard to tick all four of these boxes. And our method, the expander graph propagation, tends to tick all four of them. So how do we do it? Basically, we propose to propagate information over these expander graphs, which are known constructs from graph theory. Specifically, expander graphs have mathematical properties of a high-chigger constant, so a very low bottleneck, which is good, a low diameter, meaning you'll get global information propagation very efficiently. However, additionally, we can build expanders in a sparse manner using this standard mathematical construction from the special linear group. And that actually guarantees us that the degree of every node will be four, therefore the graph will be sparse. And actually, the only generative parameter of these graphs is the size of the group, this N over here. So it's very easy to generate an expander for a particular number of nodes. You just tell me what N you want, and I'll give you a graph. So when you look at an expander, it looks something like this. It is basically, what I like to say, it looks a bit like the human brain, right? Every node kind of has this very local connectivity to its four immediate neighbors. But as you go far away, like log N steps, you get a lot of cycles being closed very quickly, and the global communication properties get like really good. So that's our proposal. Take basically, you know, your state-of-the-art graph net that you care about. We literally just take the code actively available implementation. We switch the graph neural network connectivity in every even layer to operate over one of these guys rather than the input graph. So basically, you kind of alternate input graph, expander graph, input graph, expander graph, so that the input graphs layers are responsible for the usual local computations that a GNN wants to do. And the expander layers are responsible within diffusing that information globally in a sparse and scalable way. And this seems to work well. So on all the data sets we tried this construction, it was better than the baseline. As I said, all we did was change the connectivity, so the number of parameters is exactly the same. It's really like an apples-to-apples comparison, and it led to statistically significant results. One last point I would like to make is, you know, we're not the only group that tried to study this problem, concurrently to us, the group of Michael Bronstein with Jake Topping and Francesco DiGiovanni had this great paper on curvature analysis, which was actually one of the best paper awardees at iClear 2022. And basically in this paper, they claim that if you have negatively curved edges, so edges with very negative curvature, those tend to be the ones responsible for the formation of bottlenecks and therefore over-squashing. So naturally we wanted to connect our expander to this theory, so we computed the curvature of our graphs. But we found that actually the graphs that we built are negatively curved everywhere. So it has a curvature of negative 1 very quickly as you increase the size of the graph, right? So obviously, you know, we built a negatively curved graph everywhere, yet it still seems to work well. So what gives, right? We try to analyze this a bit further. First we show that the curvature of negative 1 is actually not that small. Like the theorem in this paper is only invoked when the curvature is close to minus 2. So in our case with curvature of negative 1, it's actually not sufficiently negative to trigger that failure case of this theorem. And additionally, we took it a step further and we actually tried to analyze how easy it is to satisfy these three properties at once. So to have sparsity, we said sparsity is good for scalability. To have a low bottleneck, so a high trigger constant, which would mean you don't have these kinds of pathological propagation problems. And thirdly, to have positive curvature, which seems to be a good idea based on the analysis of this paper. And we actually proved, there is a theorem in our paper that proves that these three things are incompatible with each other, in that there's only finitely many graphs that satisfy these three properties simultaneously. So as you go to large enough input graphs to be sparsed and to have no bottlenecks, you have to be negatively curved somewhere. It's impossible to avoid it. So while we don't study the implications of this any further, we do believe that it calls on the community in the future to study what happens in this gray area where the curvature is negative but not too negative. Because it seems like something like that might be critical to having the most optimal message passing possible. And that is basically the rough summary of our work.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.08, "text": " Peter Velichkovich is a staff research scientist at DeepMind.", "tokens": [50364, 6508, 17814, 480, 33516, 480, 307, 257, 3525, 2132, 12662, 412, 14895, 44, 471, 13, 50668], "temperature": 0.0, "avg_logprob": -0.18323253449939547, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.09879732131958008}, {"id": 1, "seek": 0, "start": 6.08, "end": 11.040000000000001, "text": " He's firmly established himself as one of the most significant and up-and-coming researchers", "tokens": [50668, 634, 311, 20031, 7545, 3647, 382, 472, 295, 264, 881, 4776, 293, 493, 12, 474, 12, 6590, 10309, 50916], "temperature": 0.0, "avg_logprob": -0.18323253449939547, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.09879732131958008}, {"id": 2, "seek": 0, "start": 11.040000000000001, "end": 13.280000000000001, "text": " in the deep learning space.", "tokens": [50916, 294, 264, 2452, 2539, 1901, 13, 51028], "temperature": 0.0, "avg_logprob": -0.18323253449939547, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.09879732131958008}, {"id": 3, "seek": 0, "start": 13.280000000000001, "end": 18.72, "text": " He invented graph attention networks in 2017 and he's been a leading light in the field", "tokens": [51028, 634, 14479, 4295, 3202, 9590, 294, 6591, 293, 415, 311, 668, 257, 5775, 1442, 294, 264, 2519, 51300], "temperature": 0.0, "avg_logprob": -0.18323253449939547, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.09879732131958008}, {"id": 4, "seek": 0, "start": 18.72, "end": 24.8, "text": " ever since, pioneering research in graph neural networks, geometric deep learning and also", "tokens": [51300, 1562, 1670, 11, 19761, 1794, 2132, 294, 4295, 18161, 9590, 11, 33246, 2452, 2539, 293, 611, 51604], "temperature": 0.0, "avg_logprob": -0.18323253449939547, "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.09879732131958008}, {"id": 5, "seek": 2480, "start": 24.8, "end": 26.8, "text": " neural algorithmic reasoning.", "tokens": [50364, 18161, 9284, 299, 21577, 13, 50464], "temperature": 0.0, "avg_logprob": -0.19646890235669684, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.06161420792341232}, {"id": 6, "seek": 2480, "start": 26.8, "end": 31.64, "text": " Recently he's been applying category theory to take the geometric deep learning ideas", "tokens": [50464, 20072, 415, 311, 668, 9275, 7719, 5261, 281, 747, 264, 33246, 2452, 2539, 3487, 50706], "temperature": 0.0, "avg_logprob": -0.19646890235669684, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.06161420792341232}, {"id": 7, "seek": 2480, "start": 31.64, "end": 33.4, "text": " one step further.", "tokens": [50706, 472, 1823, 3052, 13, 50794], "temperature": 0.0, "avg_logprob": -0.19646890235669684, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.06161420792341232}, {"id": 8, "seek": 2480, "start": 33.4, "end": 36.760000000000005, "text": " If you haven't already, you should check out our show that we did on the geometric deep", "tokens": [50794, 759, 291, 2378, 380, 1217, 11, 291, 820, 1520, 484, 527, 855, 300, 321, 630, 322, 264, 33246, 2452, 50962], "temperature": 0.0, "avg_logprob": -0.19646890235669684, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.06161420792341232}, {"id": 9, "seek": 2480, "start": 36.760000000000005, "end": 41.480000000000004, "text": " learning blueprint, which of course featured Peter and I caught up with him last week", "tokens": [50962, 2539, 35868, 11, 597, 295, 1164, 13822, 6508, 293, 286, 5415, 493, 365, 796, 1036, 1243, 51198], "temperature": 0.0, "avg_logprob": -0.19646890235669684, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.06161420792341232}, {"id": 10, "seek": 2480, "start": 41.480000000000004, "end": 42.480000000000004, "text": " at NeurIPS.", "tokens": [51198, 412, 1734, 374, 40, 6273, 13, 51248], "temperature": 0.0, "avg_logprob": -0.19646890235669684, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.06161420792341232}, {"id": 11, "seek": 2480, "start": 42.480000000000004, "end": 43.480000000000004, "text": " Enjoy.", "tokens": [51248, 15411, 13, 51298], "temperature": 0.0, "avg_logprob": -0.19646890235669684, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.06161420792341232}, {"id": 12, "seek": 2480, "start": 43.480000000000004, "end": 45.72, "text": " Peter, it's fantastic to see you again.", "tokens": [51298, 6508, 11, 309, 311, 5456, 281, 536, 291, 797, 13, 51410], "temperature": 0.0, "avg_logprob": -0.19646890235669684, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.06161420792341232}, {"id": 13, "seek": 2480, "start": 45.72, "end": 48.6, "text": " So this is the first time that I've actually met you in person.", "tokens": [51410, 407, 341, 307, 264, 700, 565, 300, 286, 600, 767, 1131, 291, 294, 954, 13, 51554], "temperature": 0.0, "avg_logprob": -0.19646890235669684, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.06161420792341232}, {"id": 14, "seek": 2480, "start": 48.6, "end": 52.68, "text": " We did that really cool show together on geometric deep learning with your proto book with Takko.", "tokens": [51554, 492, 630, 300, 534, 1627, 855, 1214, 322, 33246, 2452, 2539, 365, 428, 47896, 1446, 365, 9118, 4093, 13, 51758], "temperature": 0.0, "avg_logprob": -0.19646890235669684, "compression_ratio": 1.7032258064516128, "no_speech_prob": 0.06161420792341232}, {"id": 15, "seek": 5268, "start": 52.68, "end": 58.4, "text": " I spoke with Takko yesterday, but also Michael Bronstein and Joanne Brunner.", "tokens": [50364, 286, 7179, 365, 9118, 4093, 5186, 11, 457, 611, 5116, 19544, 9089, 293, 3139, 12674, 1603, 409, 1193, 13, 50650], "temperature": 0.0, "avg_logprob": -0.24078121891728155, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.2006341516971588}, {"id": 16, "seek": 5268, "start": 58.4, "end": 62.480000000000004, "text": " So anyway, it's been a little while since we've really synced.", "tokens": [50650, 407, 4033, 11, 309, 311, 668, 257, 707, 1339, 1670, 321, 600, 534, 5451, 1232, 13, 50854], "temperature": 0.0, "avg_logprob": -0.24078121891728155, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.2006341516971588}, {"id": 17, "seek": 5268, "start": 62.480000000000004, "end": 67.03999999999999, "text": " Now you've had this really, really interesting category theory series.", "tokens": [50854, 823, 291, 600, 632, 341, 534, 11, 534, 1880, 7719, 5261, 2638, 13, 51082], "temperature": 0.0, "avg_logprob": -0.24078121891728155, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.2006341516971588}, {"id": 18, "seek": 5268, "start": 67.03999999999999, "end": 70.16, "text": " Can you start by just letting us know what you've been doing there?", "tokens": [51082, 1664, 291, 722, 538, 445, 8295, 505, 458, 437, 291, 600, 668, 884, 456, 30, 51238], "temperature": 0.0, "avg_logprob": -0.24078121891728155, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.2006341516971588}, {"id": 19, "seek": 5268, "start": 70.16, "end": 75.64, "text": " Yeah, that's a great point and great to finally meet you in person, Tim.", "tokens": [51238, 865, 11, 300, 311, 257, 869, 935, 293, 869, 281, 2721, 1677, 291, 294, 954, 11, 7172, 13, 51512], "temperature": 0.0, "avg_logprob": -0.24078121891728155, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.2006341516971588}, {"id": 20, "seek": 5268, "start": 75.64, "end": 79.2, "text": " It's really great to catch up after some time has passed.", "tokens": [51512, 467, 311, 534, 869, 281, 3745, 493, 934, 512, 565, 575, 4678, 13, 51690], "temperature": 0.0, "avg_logprob": -0.24078121891728155, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.2006341516971588}, {"id": 21, "seek": 7920, "start": 79.2, "end": 85.44, "text": " And yeah, I mean, I like to think that all four of us, myself, Michael, Joanne and Takko", "tokens": [50364, 400, 1338, 11, 286, 914, 11, 286, 411, 281, 519, 300, 439, 1451, 295, 505, 11, 2059, 11, 5116, 11, 3139, 12674, 293, 9118, 4093, 50676], "temperature": 0.0, "avg_logprob": -0.09898144511853234, "compression_ratio": 1.6925675675675675, "no_speech_prob": 0.034033361822366714}, {"id": 22, "seek": 7920, "start": 85.44, "end": 89.28, "text": " have a greater understanding of the implications of these methods since the last time we spoke.", "tokens": [50676, 362, 257, 5044, 3701, 295, 264, 16602, 295, 613, 7150, 1670, 264, 1036, 565, 321, 7179, 13, 50868], "temperature": 0.0, "avg_logprob": -0.09898144511853234, "compression_ratio": 1.6925675675675675, "no_speech_prob": 0.034033361822366714}, {"id": 23, "seek": 7920, "start": 89.28, "end": 93.52000000000001, "text": " If you remember back when we did our conversation, I kind of hinted at the fact that category", "tokens": [50868, 759, 291, 1604, 646, 562, 321, 630, 527, 3761, 11, 286, 733, 295, 12075, 292, 412, 264, 1186, 300, 7719, 51080], "temperature": 0.0, "avg_logprob": -0.09898144511853234, "compression_ratio": 1.6925675675675675, "no_speech_prob": 0.034033361822366714}, {"id": 24, "seek": 7920, "start": 93.52000000000001, "end": 99.28, "text": " theory might hold some of the answers to maybe generalize some of these geometric concepts", "tokens": [51080, 5261, 1062, 1797, 512, 295, 264, 6338, 281, 1310, 2674, 1125, 512, 295, 613, 33246, 10392, 51368], "temperature": 0.0, "avg_logprob": -0.09898144511853234, "compression_ratio": 1.6925675675675675, "no_speech_prob": 0.034033361822366714}, {"id": 25, "seek": 7920, "start": 99.28, "end": 102.56, "text": " beyond the notion of just pure symmetries.", "tokens": [51368, 4399, 264, 10710, 295, 445, 6075, 14232, 302, 2244, 13, 51532], "temperature": 0.0, "avg_logprob": -0.09898144511853234, "compression_ratio": 1.6925675675675675, "no_speech_prob": 0.034033361822366714}, {"id": 26, "seek": 7920, "start": 102.56, "end": 107.2, "text": " And we believe that now we have a sufficient understanding of these kinds of things that", "tokens": [51532, 400, 321, 1697, 300, 586, 321, 362, 257, 11563, 3701, 295, 613, 3685, 295, 721, 300, 51764], "temperature": 0.0, "avg_logprob": -0.09898144511853234, "compression_ratio": 1.6925675675675675, "no_speech_prob": 0.034033361822366714}, {"id": 27, "seek": 10720, "start": 107.28, "end": 110.72, "text": " we were able to make this kind of mini course on categories for deep learning.", "tokens": [50368, 321, 645, 1075, 281, 652, 341, 733, 295, 8382, 1164, 322, 10479, 337, 2452, 2539, 13, 50540], "temperature": 0.0, "avg_logprob": -0.12928872515064802, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.009407375007867813}, {"id": 28, "seek": 10720, "start": 110.72, "end": 115.68, "text": " And to me, it really feels like the natural continuation of these concepts of geometric", "tokens": [50540, 400, 281, 385, 11, 309, 534, 3417, 411, 264, 3303, 29357, 295, 613, 10392, 295, 33246, 50788], "temperature": 0.0, "avg_logprob": -0.12928872515064802, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.009407375007867813}, {"id": 29, "seek": 10720, "start": 115.68, "end": 118.04, "text": " deep learning into the realm beyond.", "tokens": [50788, 2452, 2539, 666, 264, 15355, 4399, 13, 50906], "temperature": 0.0, "avg_logprob": -0.12928872515064802, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.009407375007867813}, {"id": 30, "seek": 10720, "start": 118.04, "end": 120.72, "text": " And I'll explain that in a moment.", "tokens": [50906, 400, 286, 603, 2903, 300, 294, 257, 1623, 13, 51040], "temperature": 0.0, "avg_logprob": -0.12928872515064802, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.009407375007867813}, {"id": 31, "seek": 10720, "start": 120.72, "end": 125.72, "text": " But one other kind of very related point is that here at NeurIPS, we're actually presenting", "tokens": [51040, 583, 472, 661, 733, 295, 588, 4077, 935, 307, 300, 510, 412, 1734, 374, 40, 6273, 11, 321, 434, 767, 15578, 51290], "temperature": 0.0, "avg_logprob": -0.12928872515064802, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.009407375007867813}, {"id": 32, "seek": 10720, "start": 125.72, "end": 130.92000000000002, "text": " a full conference paper which deals with using category theoretic tools to invent new kinds", "tokens": [51290, 257, 1577, 7586, 3035, 597, 11215, 365, 1228, 7719, 14308, 299, 3873, 281, 7962, 777, 3685, 51550], "temperature": 0.0, "avg_logprob": -0.12928872515064802, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.009407375007867813}, {"id": 33, "seek": 10720, "start": 130.92000000000002, "end": 131.92000000000002, "text": " of graph neural networks.", "tokens": [51550, 295, 4295, 18161, 9590, 13, 51600], "temperature": 0.0, "avg_logprob": -0.12928872515064802, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.009407375007867813}, {"id": 34, "seek": 10720, "start": 131.92000000000002, "end": 135.92000000000002, "text": " So basically, it's not just that we're throwing a bunch of new theory, it actually leads to", "tokens": [51600, 407, 1936, 11, 309, 311, 406, 445, 300, 321, 434, 10238, 257, 3840, 295, 777, 5261, 11, 309, 767, 6689, 281, 51800], "temperature": 0.0, "avg_logprob": -0.12928872515064802, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.009407375007867813}, {"id": 35, "seek": 13592, "start": 135.92, "end": 140.11999999999998, "text": " empirical findings that we can actionably use in our models day to day.", "tokens": [50364, 31886, 16483, 300, 321, 393, 3069, 1188, 764, 294, 527, 5245, 786, 281, 786, 13, 50574], "temperature": 0.0, "avg_logprob": -0.14908215380090428, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.003318021772429347}, {"id": 36, "seek": 13592, "start": 140.11999999999998, "end": 141.55999999999997, "text": " So that's one point.", "tokens": [50574, 407, 300, 311, 472, 935, 13, 50646], "temperature": 0.0, "avg_logprob": -0.14908215380090428, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.003318021772429347}, {"id": 37, "seek": 13592, "start": 141.55999999999997, "end": 142.56, "text": " That is incredible.", "tokens": [50646, 663, 307, 4651, 13, 50696], "temperature": 0.0, "avg_logprob": -0.14908215380090428, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.003318021772429347}, {"id": 38, "seek": 13592, "start": 142.56, "end": 144.04, "text": " Can you sketch out the paper?", "tokens": [50696, 1664, 291, 12325, 484, 264, 3035, 30, 50770], "temperature": 0.0, "avg_logprob": -0.14908215380090428, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.003318021772429347}, {"id": 39, "seek": 13592, "start": 144.04, "end": 145.04, "text": " Yeah, sure.", "tokens": [50770, 865, 11, 988, 13, 50820], "temperature": 0.0, "avg_logprob": -0.14908215380090428, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.003318021772429347}, {"id": 40, "seek": 13592, "start": 145.04, "end": 150.92, "text": " So basically, maybe I'll first take a step back to explain why do we think categories", "tokens": [50820, 407, 1936, 11, 1310, 286, 603, 700, 747, 257, 1823, 646, 281, 2903, 983, 360, 321, 519, 10479, 51114], "temperature": 0.0, "avg_logprob": -0.14908215380090428, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.003318021772429347}, {"id": 41, "seek": 13592, "start": 150.92, "end": 154.76, "text": " are important and in what sense they're kind of a step further from what geometric deep", "tokens": [51114, 366, 1021, 293, 294, 437, 2020, 436, 434, 733, 295, 257, 1823, 3052, 490, 437, 33246, 2452, 51306], "temperature": 0.0, "avg_logprob": -0.14908215380090428, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.003318021772429347}, {"id": 42, "seek": 13592, "start": 154.76, "end": 156.2, "text": " learning already gives us.", "tokens": [51306, 2539, 1217, 2709, 505, 13, 51378], "temperature": 0.0, "avg_logprob": -0.14908215380090428, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.003318021772429347}, {"id": 43, "seek": 13592, "start": 156.2, "end": 160.83999999999997, "text": " So geometric deep learning concerns itself with giving us these equivalent layers, right?", "tokens": [51378, 407, 33246, 2452, 2539, 7389, 2564, 365, 2902, 505, 613, 10344, 7914, 11, 558, 30, 51610], "temperature": 0.0, "avg_logprob": -0.14908215380090428, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.003318021772429347}, {"id": 44, "seek": 13592, "start": 160.83999999999997, "end": 165.64, "text": " So layers that are in some sense resistant to operations of these symmetry transformations", "tokens": [51610, 407, 7914, 300, 366, 294, 512, 2020, 20383, 281, 7705, 295, 613, 25440, 34852, 51850], "temperature": 0.0, "avg_logprob": -0.14908215380090428, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.003318021772429347}, {"id": 45, "seek": 16564, "start": 165.64, "end": 168.85999999999999, "text": " is that fundamentally change an object, but the object is still the same.", "tokens": [50364, 307, 300, 17879, 1319, 364, 2657, 11, 457, 264, 2657, 307, 920, 264, 912, 13, 50525], "temperature": 0.0, "avg_logprob": -0.11802852564844592, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0035919498186558485}, {"id": 46, "seek": 16564, "start": 168.85999999999999, "end": 170.39999999999998, "text": " We still have all of it, right?", "tokens": [50525, 492, 920, 362, 439, 295, 309, 11, 558, 30, 50602], "temperature": 0.0, "avg_logprob": -0.11802852564844592, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0035919498186558485}, {"id": 47, "seek": 16564, "start": 170.39999999999998, "end": 174.67999999999998, "text": " And this immediately implies that these symmetries have to be composable, invertible, all that", "tokens": [50602, 400, 341, 4258, 18779, 300, 613, 14232, 302, 2244, 362, 281, 312, 10199, 712, 11, 33966, 964, 11, 439, 300, 50816], "temperature": 0.0, "avg_logprob": -0.11802852564844592, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0035919498186558485}, {"id": 48, "seek": 16564, "start": 174.67999999999998, "end": 176.2, "text": " sort of stuff.", "tokens": [50816, 1333, 295, 1507, 13, 50892], "temperature": 0.0, "avg_logprob": -0.11802852564844592, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0035919498186558485}, {"id": 49, "seek": 16564, "start": 176.2, "end": 183.6, "text": " And yeah, essentially, the category theory framework is in some sense mindful of the", "tokens": [50892, 400, 1338, 11, 4476, 11, 264, 7719, 5261, 8388, 307, 294, 512, 2020, 14618, 295, 264, 51262], "temperature": 0.0, "avg_logprob": -0.11802852564844592, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0035919498186558485}, {"id": 50, "seek": 16564, "start": 183.6, "end": 188.56, "text": " fact that while symmetries are a very nice way to reason about things that happen and", "tokens": [51262, 1186, 300, 1339, 14232, 302, 2244, 366, 257, 588, 1481, 636, 281, 1778, 466, 721, 300, 1051, 293, 51510], "temperature": 0.0, "avg_logprob": -0.11802852564844592, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0035919498186558485}, {"id": 51, "seek": 16564, "start": 188.56, "end": 194.79999999999998, "text": " that we see in nature, they're often not completely an accurate representation of what happens.", "tokens": [51510, 300, 321, 536, 294, 3687, 11, 436, 434, 2049, 406, 2584, 364, 8559, 10290, 295, 437, 2314, 13, 51822], "temperature": 0.0, "avg_logprob": -0.11802852564844592, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0035919498186558485}, {"id": 52, "seek": 19480, "start": 194.8, "end": 199.24, "text": " Very often there are operations both in nature, but especially in general computation, like", "tokens": [50364, 4372, 2049, 456, 366, 7705, 1293, 294, 3687, 11, 457, 2318, 294, 2674, 24903, 11, 411, 50586], "temperature": 0.0, "avg_logprob": -0.13010743948129508, "compression_ratio": 1.72, "no_speech_prob": 0.028422711417078972}, {"id": 53, "seek": 19480, "start": 199.24, "end": 204.08, "text": " say in algorithmic stuff, where an operation of an algorithm might destroy half of your", "tokens": [50586, 584, 294, 9284, 299, 1507, 11, 689, 364, 6916, 295, 364, 9284, 1062, 5293, 1922, 295, 428, 50828], "temperature": 0.0, "avg_logprob": -0.13010743948129508, "compression_ratio": 1.72, "no_speech_prob": 0.028422711417078972}, {"id": 54, "seek": 19480, "start": 204.08, "end": 205.08, "text": " data.", "tokens": [50828, 1412, 13, 50878], "temperature": 0.0, "avg_logprob": -0.13010743948129508, "compression_ratio": 1.72, "no_speech_prob": 0.028422711417078972}, {"id": 55, "seek": 19480, "start": 205.08, "end": 206.08, "text": " So that is no longer a symmetry.", "tokens": [50878, 407, 300, 307, 572, 2854, 257, 25440, 13, 50928], "temperature": 0.0, "avg_logprob": -0.13010743948129508, "compression_ratio": 1.72, "no_speech_prob": 0.028422711417078972}, {"id": 56, "seek": 19480, "start": 206.08, "end": 207.08, "text": " You cannot invert it.", "tokens": [50928, 509, 2644, 33966, 309, 13, 50978], "temperature": 0.0, "avg_logprob": -0.13010743948129508, "compression_ratio": 1.72, "no_speech_prob": 0.028422711417078972}, {"id": 57, "seek": 19480, "start": 207.08, "end": 211.36, "text": " But you might still be interested in building a neural network model that is in some sense", "tokens": [50978, 583, 291, 1062, 920, 312, 3102, 294, 2390, 257, 18161, 3209, 2316, 300, 307, 294, 512, 2020, 51192], "temperature": 0.0, "avg_logprob": -0.13010743948129508, "compression_ratio": 1.72, "no_speech_prob": 0.028422711417078972}, {"id": 58, "seek": 19480, "start": 211.36, "end": 217.64000000000001, "text": " resistant to the operations of say this algorithm or this natural phenomenon that you're studying.", "tokens": [51192, 20383, 281, 264, 7705, 295, 584, 341, 9284, 420, 341, 3303, 14029, 300, 291, 434, 7601, 13, 51506], "temperature": 0.0, "avg_logprob": -0.13010743948129508, "compression_ratio": 1.72, "no_speech_prob": 0.028422711417078972}, {"id": 59, "seek": 19480, "start": 217.64000000000001, "end": 222.44, "text": " One simple example that maybe predates our work a little bit is building some kind of", "tokens": [51506, 1485, 2199, 1365, 300, 1310, 3852, 1024, 527, 589, 257, 707, 857, 307, 2390, 512, 733, 295, 51746], "temperature": 0.0, "avg_logprob": -0.13010743948129508, "compression_ratio": 1.72, "no_speech_prob": 0.028422711417078972}, {"id": 60, "seek": 22244, "start": 222.56, "end": 225.52, "text": " equivalence to scaling operations.", "tokens": [50370, 9052, 655, 281, 21589, 7705, 13, 50518], "temperature": 0.0, "avg_logprob": -0.13998848392117408, "compression_ratio": 1.803225806451613, "no_speech_prob": 0.08142470568418503}, {"id": 61, "seek": 22244, "start": 225.52, "end": 229.48, "text": " Obviously if you scale or course in something, these are not always invertible transformations", "tokens": [50518, 7580, 498, 291, 4373, 420, 1164, 294, 746, 11, 613, 366, 406, 1009, 33966, 964, 34852, 50716], "temperature": 0.0, "avg_logprob": -0.13998848392117408, "compression_ratio": 1.803225806451613, "no_speech_prob": 0.08142470568418503}, {"id": 62, "seek": 22244, "start": 229.48, "end": 233.92, "text": " because if you course in the pixels of your image, you cannot perfectly reconstruct where", "tokens": [50716, 570, 498, 291, 1164, 294, 264, 18668, 295, 428, 3256, 11, 291, 2644, 6239, 31499, 689, 50938], "temperature": 0.0, "avg_logprob": -0.13998848392117408, "compression_ratio": 1.803225806451613, "no_speech_prob": 0.08142470568418503}, {"id": 63, "seek": 22244, "start": 233.92, "end": 234.92, "text": " you came from.", "tokens": [50938, 291, 1361, 490, 13, 50988], "temperature": 0.0, "avg_logprob": -0.13998848392117408, "compression_ratio": 1.803225806451613, "no_speech_prob": 0.08142470568418503}, {"id": 64, "seek": 22244, "start": 234.92, "end": 238.28, "text": " Yet you still might want to build a model that will give you the same answer as regardless", "tokens": [50988, 10890, 291, 920, 1062, 528, 281, 1322, 257, 2316, 300, 486, 976, 291, 264, 912, 1867, 382, 10060, 51156], "temperature": 0.0, "avg_logprob": -0.13998848392117408, "compression_ratio": 1.803225806451613, "no_speech_prob": 0.08142470568418503}, {"id": 65, "seek": 22244, "start": 238.28, "end": 240.56, "text": " of how you scale up your input, right?", "tokens": [51156, 295, 577, 291, 4373, 493, 428, 4846, 11, 558, 30, 51270], "temperature": 0.0, "avg_logprob": -0.13998848392117408, "compression_ratio": 1.803225806451613, "no_speech_prob": 0.08142470568418503}, {"id": 66, "seek": 22244, "start": 240.56, "end": 244.36, "text": " So these are obviously things that are going to be very important as we move to more generic", "tokens": [51270, 407, 613, 366, 2745, 721, 300, 366, 516, 281, 312, 588, 1021, 382, 321, 1286, 281, 544, 19577, 51460], "temperature": 0.0, "avg_logprob": -0.13998848392117408, "compression_ratio": 1.803225806451613, "no_speech_prob": 0.08142470568418503}, {"id": 67, "seek": 22244, "start": 244.36, "end": 249.64, "text": " domains than ones that can be described purely through geometric and symmetry transformations,", "tokens": [51460, 25514, 813, 2306, 300, 393, 312, 7619, 17491, 807, 33246, 293, 25440, 34852, 11, 51724], "temperature": 0.0, "avg_logprob": -0.13998848392117408, "compression_ratio": 1.803225806451613, "no_speech_prob": 0.08142470568418503}, {"id": 68, "seek": 22244, "start": 249.64, "end": 250.64, "text": " right?", "tokens": [51724, 558, 30, 51774], "temperature": 0.0, "avg_logprob": -0.13998848392117408, "compression_ratio": 1.803225806451613, "no_speech_prob": 0.08142470568418503}, {"id": 69, "seek": 25064, "start": 250.64, "end": 256.4, "text": " And in that sense, the same way we had groups, representations, and equivalence in geometric", "tokens": [50364, 400, 294, 300, 2020, 11, 264, 912, 636, 321, 632, 3935, 11, 33358, 11, 293, 9052, 655, 294, 33246, 50652], "temperature": 0.0, "avg_logprob": -0.1336720673934273, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.0009396735113114119}, {"id": 70, "seek": 25064, "start": 256.4, "end": 261.03999999999996, "text": " deep learning, these are all special cases of categorical concepts like categories,", "tokens": [50652, 2452, 2539, 11, 613, 366, 439, 2121, 3331, 295, 19250, 804, 10392, 411, 10479, 11, 50884], "temperature": 0.0, "avg_logprob": -0.1336720673934273, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.0009396735113114119}, {"id": 71, "seek": 25064, "start": 261.03999999999996, "end": 265.91999999999996, "text": " functors, and natural transformations, which basically generalize all the stuff of geometric", "tokens": [50884, 1019, 5547, 11, 293, 3303, 34852, 11, 597, 1936, 2674, 1125, 439, 264, 1507, 295, 33246, 51128], "temperature": 0.0, "avg_logprob": -0.1336720673934273, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.0009396735113114119}, {"id": 72, "seek": 25064, "start": 265.91999999999996, "end": 268.0, "text": " deep learning into their own beyond.", "tokens": [51128, 2452, 2539, 666, 641, 1065, 4399, 13, 51232], "temperature": 0.0, "avg_logprob": -0.1336720673934273, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.0009396735113114119}, {"id": 73, "seek": 25064, "start": 268.0, "end": 273.36, "text": " And in our paper, we try to use exactly these kinds of category theoretic tools to study", "tokens": [51232, 400, 294, 527, 3035, 11, 321, 853, 281, 764, 2293, 613, 3685, 295, 7719, 14308, 299, 3873, 281, 2979, 51500], "temperature": 0.0, "avg_logprob": -0.1336720673934273, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.0009396735113114119}, {"id": 74, "seek": 25064, "start": 273.36, "end": 278.24, "text": " what it would mean to build a say graph neural network that is capable of behaving like a", "tokens": [51500, 437, 309, 576, 914, 281, 1322, 257, 584, 4295, 18161, 3209, 300, 307, 8189, 295, 35263, 411, 257, 51744], "temperature": 0.0, "avg_logprob": -0.1336720673934273, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.0009396735113114119}, {"id": 75, "seek": 25064, "start": 278.24, "end": 280.47999999999996, "text": " classical computer science algorithm.", "tokens": [51744, 13735, 3820, 3497, 9284, 13, 51856], "temperature": 0.0, "avg_logprob": -0.1336720673934273, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.0009396735113114119}, {"id": 76, "seek": 28048, "start": 280.48, "end": 284.92, "text": " In the sense that if you have some data that's transformed by an algorithm, you may imagine,", "tokens": [50364, 682, 264, 2020, 300, 498, 291, 362, 512, 1412, 300, 311, 16894, 538, 364, 9284, 11, 291, 815, 3811, 11, 50586], "temperature": 0.0, "avg_logprob": -0.13153643454274824, "compression_ratio": 1.8294117647058823, "no_speech_prob": 0.037865087389945984}, {"id": 77, "seek": 28048, "start": 284.92, "end": 289.08000000000004, "text": " say, a path finding algorithm where at every step in every node, you have your knowledge", "tokens": [50586, 584, 11, 257, 3100, 5006, 9284, 689, 412, 633, 1823, 294, 633, 9984, 11, 291, 362, 428, 3601, 50794], "temperature": 0.0, "avg_logprob": -0.13153643454274824, "compression_ratio": 1.8294117647058823, "no_speech_prob": 0.037865087389945984}, {"id": 78, "seek": 28048, "start": 289.08000000000004, "end": 292.16, "text": " of how far away is that node from the source vertex.", "tokens": [50794, 295, 577, 1400, 1314, 307, 300, 9984, 490, 264, 4009, 28162, 13, 50948], "temperature": 0.0, "avg_logprob": -0.13153643454274824, "compression_ratio": 1.8294117647058823, "no_speech_prob": 0.037865087389945984}, {"id": 79, "seek": 28048, "start": 292.16, "end": 295.40000000000003, "text": " And one step of the algorithm kind of looks at all the immediate neighbors and updates", "tokens": [50948, 400, 472, 1823, 295, 264, 9284, 733, 295, 1542, 412, 439, 264, 11629, 12512, 293, 9205, 51110], "temperature": 0.0, "avg_logprob": -0.13153643454274824, "compression_ratio": 1.8294117647058823, "no_speech_prob": 0.037865087389945984}, {"id": 80, "seek": 28048, "start": 295.40000000000003, "end": 297.36, "text": " those beliefs of how far away you are.", "tokens": [51110, 729, 13585, 295, 577, 1400, 1314, 291, 366, 13, 51208], "temperature": 0.0, "avg_logprob": -0.13153643454274824, "compression_ratio": 1.8294117647058823, "no_speech_prob": 0.037865087389945984}, {"id": 81, "seek": 28048, "start": 297.36, "end": 300.72, "text": " And I'll say you want to have a GNN that simulates that, like we typically do in algorithmic", "tokens": [51208, 400, 286, 603, 584, 291, 528, 281, 362, 257, 46411, 45, 300, 1034, 26192, 300, 11, 411, 321, 5850, 360, 294, 9284, 299, 51376], "temperature": 0.0, "avg_logprob": -0.13153643454274824, "compression_ratio": 1.8294117647058823, "no_speech_prob": 0.037865087389945984}, {"id": 82, "seek": 28048, "start": 300.72, "end": 301.72, "text": " reasoning.", "tokens": [51376, 21577, 13, 51426], "temperature": 0.0, "avg_logprob": -0.13153643454274824, "compression_ratio": 1.8294117647058823, "no_speech_prob": 0.037865087389945984}, {"id": 83, "seek": 28048, "start": 301.72, "end": 305.88, "text": " You take your algorithmic state, you encode it with a neural network into this high dimensional", "tokens": [51426, 509, 747, 428, 9284, 299, 1785, 11, 291, 2058, 1429, 309, 365, 257, 18161, 3209, 666, 341, 1090, 18795, 51634], "temperature": 0.0, "avg_logprob": -0.13153643454274824, "compression_ratio": 1.8294117647058823, "no_speech_prob": 0.037865087389945984}, {"id": 84, "seek": 28048, "start": 305.88, "end": 306.88, "text": " space.", "tokens": [51634, 1901, 13, 51684], "temperature": 0.0, "avg_logprob": -0.13153643454274824, "compression_ratio": 1.8294117647058823, "no_speech_prob": 0.037865087389945984}, {"id": 85, "seek": 28048, "start": 306.88, "end": 309.68, "text": " Your GNN then processes it to update the latent space.", "tokens": [51684, 2260, 46411, 45, 550, 7555, 309, 281, 5623, 264, 48994, 1901, 13, 51824], "temperature": 0.0, "avg_logprob": -0.13153643454274824, "compression_ratio": 1.8294117647058823, "no_speech_prob": 0.037865087389945984}, {"id": 86, "seek": 30968, "start": 309.68, "end": 313.6, "text": " And now you want to be able to decode it so that you predict whatever the next state", "tokens": [50364, 400, 586, 291, 528, 281, 312, 1075, 281, 979, 1429, 309, 370, 300, 291, 6069, 2035, 264, 958, 1785, 50560], "temperature": 0.0, "avg_logprob": -0.11669481717623197, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.09384498000144958}, {"id": 87, "seek": 30968, "start": 313.6, "end": 314.6, "text": " is going to be.", "tokens": [50560, 307, 516, 281, 312, 13, 50610], "temperature": 0.0, "avg_logprob": -0.11669481717623197, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.09384498000144958}, {"id": 88, "seek": 30968, "start": 314.6, "end": 319.08, "text": " So you have something which in category theory we use a lot is known as a commutative diagram.", "tokens": [50610, 407, 291, 362, 746, 597, 294, 7719, 5261, 321, 764, 257, 688, 307, 2570, 382, 257, 800, 325, 1166, 10686, 13, 50834], "temperature": 0.0, "avg_logprob": -0.11669481717623197, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.09384498000144958}, {"id": 89, "seek": 30968, "start": 319.08, "end": 322.64, "text": " So basically it's saying you can either take the step of the algorithm or you can encode,", "tokens": [50834, 407, 1936, 309, 311, 1566, 291, 393, 2139, 747, 264, 1823, 295, 264, 9284, 420, 291, 393, 2058, 1429, 11, 51012], "temperature": 0.0, "avg_logprob": -0.11669481717623197, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.09384498000144958}, {"id": 90, "seek": 30968, "start": 322.64, "end": 325.36, "text": " process, decode, and hopefully end up in the same place.", "tokens": [51012, 1399, 11, 979, 1429, 11, 293, 4696, 917, 493, 294, 264, 912, 1081, 13, 51148], "temperature": 0.0, "avg_logprob": -0.11669481717623197, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.09384498000144958}, {"id": 91, "seek": 30968, "start": 325.36, "end": 328.4, "text": " So category theory seems like a very nice language to study.", "tokens": [51148, 407, 7719, 5261, 2544, 411, 257, 588, 1481, 2856, 281, 2979, 13, 51300], "temperature": 0.0, "avg_logprob": -0.11669481717623197, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.09384498000144958}, {"id": 92, "seek": 30968, "start": 328.4, "end": 334.32, "text": " These kinds of, I won't call them symmetries, they're basically like interchangeable sequences", "tokens": [51300, 1981, 3685, 295, 11, 286, 1582, 380, 818, 552, 14232, 302, 2244, 11, 436, 434, 1936, 411, 30358, 712, 22978, 51596], "temperature": 0.0, "avg_logprob": -0.11669481717623197, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.09384498000144958}, {"id": 93, "seek": 30968, "start": 334.32, "end": 339.04, "text": " of operations because the step of an algorithm might not be invertible.", "tokens": [51596, 295, 7705, 570, 264, 1823, 295, 364, 9284, 1062, 406, 312, 33966, 964, 13, 51832], "temperature": 0.0, "avg_logprob": -0.11669481717623197, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.09384498000144958}, {"id": 94, "seek": 33904, "start": 339.04, "end": 343.24, "text": " You might not be able to go back after you do one step of, you know, shortest path algorithm", "tokens": [50364, 509, 1062, 406, 312, 1075, 281, 352, 646, 934, 291, 360, 472, 1823, 295, 11, 291, 458, 11, 31875, 3100, 9284, 50574], "temperature": 0.0, "avg_logprob": -0.10145906674659859, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.004069729242473841}, {"id": 95, "seek": 33904, "start": 343.24, "end": 345.28000000000003, "text": " because it's a contraction map, right?", "tokens": [50574, 570, 309, 311, 257, 37372, 4471, 11, 558, 30, 50676], "temperature": 0.0, "avg_logprob": -0.10145906674659859, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.004069729242473841}, {"id": 96, "seek": 33904, "start": 345.28000000000003, "end": 348.88, "text": " When you find the final solution of a shortest path algorithm, you won't necessarily know", "tokens": [50676, 1133, 291, 915, 264, 2572, 3827, 295, 257, 31875, 3100, 9284, 11, 291, 1582, 380, 4725, 458, 50856], "temperature": 0.0, "avg_logprob": -0.10145906674659859, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.004069729242473841}, {"id": 97, "seek": 33904, "start": 348.88, "end": 353.56, "text": " which previous state led you there because there could be many equivalent states that", "tokens": [50856, 597, 3894, 1785, 4684, 291, 456, 570, 456, 727, 312, 867, 10344, 4368, 300, 51090], "temperature": 0.0, "avg_logprob": -0.10145906674659859, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.004069729242473841}, {"id": 98, "seek": 33904, "start": 353.56, "end": 356.34000000000003, "text": " could lead you to the same contracted solution, right?", "tokens": [51090, 727, 1477, 291, 281, 264, 912, 37629, 3827, 11, 558, 30, 51229], "temperature": 0.0, "avg_logprob": -0.10145906674659859, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.004069729242473841}, {"id": 99, "seek": 33904, "start": 356.34000000000003, "end": 361.28000000000003, "text": " So our method, using these category theory frameworks, try to characterize how these", "tokens": [51229, 407, 527, 3170, 11, 1228, 613, 7719, 5261, 29834, 11, 853, 281, 38463, 577, 613, 51476], "temperature": 0.0, "avg_logprob": -0.10145906674659859, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.004069729242473841}, {"id": 100, "seek": 33904, "start": 361.28000000000003, "end": 366.52000000000004, "text": " graph neural networks align with a target algorithm that we might want to simulate.", "tokens": [51476, 4295, 18161, 9590, 7975, 365, 257, 3779, 9284, 300, 321, 1062, 528, 281, 27817, 13, 51738], "temperature": 0.0, "avg_logprob": -0.10145906674659859, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.004069729242473841}, {"id": 101, "seek": 36652, "start": 366.52, "end": 370.52, "text": " Then we detect various ways not only to explain the code of graph neural networks from this", "tokens": [50364, 1396, 321, 5531, 3683, 2098, 406, 787, 281, 2903, 264, 3089, 295, 4295, 18161, 9590, 490, 341, 50564], "temperature": 0.0, "avg_logprob": -0.12483925024668376, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.16219861805438995}, {"id": 102, "seek": 36652, "start": 370.52, "end": 374.35999999999996, "text": " kind of perspective, but also it gives us a very interesting sort of, if you've done", "tokens": [50564, 733, 295, 4585, 11, 457, 611, 309, 2709, 505, 257, 588, 1880, 1333, 295, 11, 498, 291, 600, 1096, 50756], "temperature": 0.0, "avg_logprob": -0.12483925024668376, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.16219861805438995}, {"id": 103, "seek": 36652, "start": 374.35999999999996, "end": 378.76, "text": " any functional programming, a type checker of sorts to kind of detect whenever we're", "tokens": [50756, 604, 11745, 9410, 11, 257, 2010, 1520, 260, 295, 7527, 281, 733, 295, 5531, 5699, 321, 434, 50976], "temperature": 0.0, "avg_logprob": -0.12483925024668376, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.16219861805438995}, {"id": 104, "seek": 36652, "start": 378.76, "end": 382.03999999999996, "text": " using our representations in slightly broken ways.", "tokens": [50976, 1228, 527, 33358, 294, 4748, 5463, 2098, 13, 51140], "temperature": 0.0, "avg_logprob": -0.12483925024668376, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.16219861805438995}, {"id": 105, "seek": 36652, "start": 382.03999999999996, "end": 387.15999999999997, "text": " So specifically to give you one very concrete example, in a categorical framework, just", "tokens": [51140, 407, 4682, 281, 976, 291, 472, 588, 9859, 1365, 11, 294, 257, 19250, 804, 8388, 11, 445, 51396], "temperature": 0.0, "avg_logprob": -0.12483925024668376, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.16219861805438995}, {"id": 106, "seek": 36652, "start": 387.15999999999997, "end": 390.76, "text": " like in functional programming, you expect your transformations to be functions.", "tokens": [51396, 411, 294, 11745, 9410, 11, 291, 2066, 428, 34852, 281, 312, 6828, 13, 51576], "temperature": 0.0, "avg_logprob": -0.12483925024668376, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.16219861805438995}, {"id": 107, "seek": 36652, "start": 390.76, "end": 393.84, "text": " That is, for every input there should be a unique output.", "tokens": [51576, 663, 307, 11, 337, 633, 4846, 456, 820, 312, 257, 3845, 5598, 13, 51730], "temperature": 0.0, "avg_logprob": -0.12483925024668376, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.16219861805438995}, {"id": 108, "seek": 39384, "start": 393.84, "end": 397.59999999999997, "text": " However, one thing that people very often do in graph representation learning, when", "tokens": [50364, 2908, 11, 472, 551, 300, 561, 588, 2049, 360, 294, 4295, 10290, 2539, 11, 562, 50552], "temperature": 0.0, "avg_logprob": -0.11176000436147054, "compression_ratio": 1.782312925170068, "no_speech_prob": 0.007344208192080259}, {"id": 109, "seek": 39384, "start": 397.59999999999997, "end": 402.03999999999996, "text": " they want to predict outputs not only in the nodes, but also in the edges, is to reuse", "tokens": [50552, 436, 528, 281, 6069, 23930, 406, 787, 294, 264, 13891, 11, 457, 611, 294, 264, 8819, 11, 307, 281, 26225, 50774], "temperature": 0.0, "avg_logprob": -0.11176000436147054, "compression_ratio": 1.782312925170068, "no_speech_prob": 0.007344208192080259}, {"id": 110, "seek": 39384, "start": 402.03999999999996, "end": 407.52, "text": " the edge messages both as edge outputs and integrated overall the other messages to get", "tokens": [50774, 264, 4691, 7897, 1293, 382, 4691, 23930, 293, 10919, 4787, 264, 661, 7897, 281, 483, 51048], "temperature": 0.0, "avg_logprob": -0.11176000436147054, "compression_ratio": 1.782312925170068, "no_speech_prob": 0.007344208192080259}, {"id": 111, "seek": 39384, "start": 407.52, "end": 409.03999999999996, "text": " node outputs, right?", "tokens": [51048, 9984, 23930, 11, 558, 30, 51124], "temperature": 0.0, "avg_logprob": -0.11176000436147054, "compression_ratio": 1.782312925170068, "no_speech_prob": 0.007344208192080259}, {"id": 112, "seek": 39384, "start": 409.03999999999996, "end": 413.0, "text": " But this is a problem from the categorical perspective because this is no longer a function.", "tokens": [51124, 583, 341, 307, 257, 1154, 490, 264, 19250, 804, 4585, 570, 341, 307, 572, 2854, 257, 2445, 13, 51322], "temperature": 0.0, "avg_logprob": -0.11176000436147054, "compression_ratio": 1.782312925170068, "no_speech_prob": 0.007344208192080259}, {"id": 113, "seek": 39384, "start": 413.0, "end": 418.64, "text": " You cannot get a function that takes, you know, edges to edges plus nodes without sending", "tokens": [51322, 509, 2644, 483, 257, 2445, 300, 2516, 11, 291, 458, 11, 8819, 281, 8819, 1804, 13891, 1553, 7750, 51604], "temperature": 0.0, "avg_logprob": -0.11176000436147054, "compression_ratio": 1.782312925170068, "no_speech_prob": 0.007344208192080259}, {"id": 114, "seek": 39384, "start": 418.64, "end": 421.76, "text": " the same thing into two different places in this case, right?", "tokens": [51604, 264, 912, 551, 666, 732, 819, 3190, 294, 341, 1389, 11, 558, 30, 51760], "temperature": 0.0, "avg_logprob": -0.11176000436147054, "compression_ratio": 1.782312925170068, "no_speech_prob": 0.007344208192080259}, {"id": 115, "seek": 42176, "start": 421.76, "end": 425.2, "text": " And, you know, just because it mathematically breaks doesn't mean you cannot implement it.", "tokens": [50364, 400, 11, 291, 458, 11, 445, 570, 309, 44003, 9857, 1177, 380, 914, 291, 2644, 4445, 309, 13, 50536], "temperature": 0.0, "avg_logprob": -0.14672785219938858, "compression_ratio": 1.7625, "no_speech_prob": 0.006588367745280266}, {"id": 116, "seek": 42176, "start": 425.2, "end": 432.0, "text": " In fact, 99% of the GNN implementations you'll find online will do this exactly in this particular", "tokens": [50536, 682, 1186, 11, 11803, 4, 295, 264, 46411, 45, 4445, 763, 291, 603, 915, 2950, 486, 360, 341, 2293, 294, 341, 1729, 50876], "temperature": 0.0, "avg_logprob": -0.14672785219938858, "compression_ratio": 1.7625, "no_speech_prob": 0.006588367745280266}, {"id": 117, "seek": 42176, "start": 432.0, "end": 433.0, "text": " way.", "tokens": [50876, 636, 13, 50926], "temperature": 0.0, "avg_logprob": -0.14672785219938858, "compression_ratio": 1.7625, "no_speech_prob": 0.006588367745280266}, {"id": 118, "seek": 42176, "start": 433.0, "end": 435.28, "text": " DeepMindsGraphNet's library does this, for example.", "tokens": [50926, 14895, 44, 471, 82, 38, 2662, 31890, 311, 6405, 775, 341, 11, 337, 1365, 13, 51040], "temperature": 0.0, "avg_logprob": -0.14672785219938858, "compression_ratio": 1.7625, "no_speech_prob": 0.006588367745280266}, {"id": 119, "seek": 42176, "start": 435.28, "end": 438.8, "text": " However, you know, just because you can implement it doesn't mean that there's something not", "tokens": [51040, 2908, 11, 291, 458, 11, 445, 570, 291, 393, 4445, 309, 1177, 380, 914, 300, 456, 311, 746, 406, 51216], "temperature": 0.0, "avg_logprob": -0.14672785219938858, "compression_ratio": 1.7625, "no_speech_prob": 0.006588367745280266}, {"id": 120, "seek": 42176, "start": 438.8, "end": 442.84, "text": " potentially a bit tricky going on in the sense that you're putting a bit of representational", "tokens": [51216, 7263, 257, 857, 12414, 516, 322, 294, 264, 2020, 300, 291, 434, 3372, 257, 857, 295, 2906, 1478, 51418], "temperature": 0.0, "avg_logprob": -0.14672785219938858, "compression_ratio": 1.7625, "no_speech_prob": 0.006588367745280266}, {"id": 121, "seek": 42176, "start": 442.84, "end": 445.12, "text": " pressure on that edge message, right?", "tokens": [51418, 3321, 322, 300, 4691, 3636, 11, 558, 30, 51532], "temperature": 0.0, "avg_logprob": -0.14672785219938858, "compression_ratio": 1.7625, "no_speech_prob": 0.006588367745280266}, {"id": 122, "seek": 42176, "start": 445.12, "end": 449.56, "text": " Because now it has to be used for two potentially very different things, both for some output", "tokens": [51532, 1436, 586, 309, 575, 281, 312, 1143, 337, 732, 7263, 588, 819, 721, 11, 1293, 337, 512, 5598, 51754], "temperature": 0.0, "avg_logprob": -0.14672785219938858, "compression_ratio": 1.7625, "no_speech_prob": 0.006588367745280266}, {"id": 123, "seek": 44956, "start": 449.56, "end": 454.32, "text": " in the edges, but also it needs to be integratable into nodes where it predicts something potentially", "tokens": [50364, 294, 264, 8819, 11, 457, 611, 309, 2203, 281, 312, 3572, 31415, 666, 13891, 689, 309, 6069, 82, 746, 7263, 50602], "temperature": 0.0, "avg_logprob": -0.12512762922989695, "compression_ratio": 1.7834757834757835, "no_speech_prob": 0.07469338923692703}, {"id": 124, "seek": 44956, "start": 454.32, "end": 455.32, "text": " wildly different.", "tokens": [50602, 34731, 819, 13, 50652], "temperature": 0.0, "avg_logprob": -0.12512762922989695, "compression_ratio": 1.7834757834757835, "no_speech_prob": 0.07469338923692703}, {"id": 125, "seek": 44956, "start": 455.32, "end": 459.36, "text": " And, you know, while gradient descent can take care of this and give you a model that", "tokens": [50652, 400, 11, 291, 458, 11, 1339, 16235, 23475, 393, 747, 1127, 295, 341, 293, 976, 291, 257, 2316, 300, 50854], "temperature": 0.0, "avg_logprob": -0.12512762922989695, "compression_ratio": 1.7834757834757835, "no_speech_prob": 0.07469338923692703}, {"id": 126, "seek": 44956, "start": 459.36, "end": 463.52, "text": " fits your training distribution well, you're not like, to deal with this pressure, it's", "tokens": [50854, 9001, 428, 3097, 7316, 731, 11, 291, 434, 406, 411, 11, 281, 2028, 365, 341, 3321, 11, 309, 311, 51062], "temperature": 0.0, "avg_logprob": -0.12512762922989695, "compression_ratio": 1.7834757834757835, "no_speech_prob": 0.07469338923692703}, {"id": 127, "seek": 44956, "start": 463.52, "end": 466.24, "text": " probably going to have to learn something which has nothing to do with the algorithm", "tokens": [51062, 1391, 516, 281, 362, 281, 1466, 746, 597, 575, 1825, 281, 360, 365, 264, 9284, 51198], "temperature": 0.0, "avg_logprob": -0.12512762922989695, "compression_ratio": 1.7834757834757835, "no_speech_prob": 0.07469338923692703}, {"id": 128, "seek": 44956, "start": 466.24, "end": 467.84000000000003, "text": " that you want to align to.", "tokens": [51198, 300, 291, 528, 281, 7975, 281, 13, 51278], "temperature": 0.0, "avg_logprob": -0.12512762922989695, "compression_ratio": 1.7834757834757835, "no_speech_prob": 0.07469338923692703}, {"id": 129, "seek": 44956, "start": 467.84000000000003, "end": 471.44, "text": " And as a result, you're out of distribution, extrapolation performance is going to be much,", "tokens": [51278, 400, 382, 257, 1874, 11, 291, 434, 484, 295, 7316, 11, 48224, 399, 3389, 307, 516, 281, 312, 709, 11, 51458], "temperature": 0.0, "avg_logprob": -0.12512762922989695, "compression_ratio": 1.7834757834757835, "no_speech_prob": 0.07469338923692703}, {"id": 130, "seek": 44956, "start": 471.44, "end": 472.44, "text": " much worse.", "tokens": [51458, 709, 5324, 13, 51508], "temperature": 0.0, "avg_logprob": -0.12512762922989695, "compression_ratio": 1.7834757834757835, "no_speech_prob": 0.07469338923692703}, {"id": 131, "seek": 44956, "start": 472.44, "end": 475.12, "text": " And any self-respecting algorithm should extrapolate well.", "tokens": [51508, 400, 604, 2698, 12, 19575, 278, 9284, 820, 48224, 473, 731, 13, 51642], "temperature": 0.0, "avg_logprob": -0.12512762922989695, "compression_ratio": 1.7834757834757835, "no_speech_prob": 0.07469338923692703}, {"id": 132, "seek": 44956, "start": 475.12, "end": 477.4, "text": " That's the main property of algorithmic reasoning, right?", "tokens": [51642, 663, 311, 264, 2135, 4707, 295, 9284, 299, 21577, 11, 558, 30, 51756], "temperature": 0.0, "avg_logprob": -0.12512762922989695, "compression_ratio": 1.7834757834757835, "no_speech_prob": 0.07469338923692703}, {"id": 133, "seek": 47740, "start": 477.4, "end": 481.15999999999997, "text": " And we find that just by, you know, splitting this message function into two streams, one", "tokens": [50364, 400, 321, 915, 300, 445, 538, 11, 291, 458, 11, 30348, 341, 3636, 2445, 666, 732, 15842, 11, 472, 50552], "temperature": 0.0, "avg_logprob": -0.17681620915730795, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.035627029836177826}, {"id": 134, "seek": 47740, "start": 481.15999999999997, "end": 484.91999999999996, "text": " which goes into the edges and one which goes into the nodes, we get basically significant", "tokens": [50552, 597, 1709, 666, 264, 8819, 293, 472, 597, 1709, 666, 264, 13891, 11, 321, 483, 1936, 4776, 50740], "temperature": 0.0, "avg_logprob": -0.17681620915730795, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.035627029836177826}, {"id": 135, "seek": 47740, "start": 484.91999999999996, "end": 488.23999999999995, "text": " empirical benefits when extrapolating on edge-centric algorithms.", "tokens": [50740, 31886, 5311, 562, 48224, 990, 322, 4691, 12, 45300, 14642, 13, 50906], "temperature": 0.0, "avg_logprob": -0.17681620915730795, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.035627029836177826}, {"id": 136, "seek": 47740, "start": 488.23999999999995, "end": 489.23999999999995, "text": " Yeah.", "tokens": [50906, 865, 13, 50956], "temperature": 0.0, "avg_logprob": -0.17681620915730795, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.035627029836177826}, {"id": 137, "seek": 47740, "start": 489.23999999999995, "end": 490.23999999999995, "text": " Amazing.", "tokens": [50956, 14165, 13, 51006], "temperature": 0.0, "avg_logprob": -0.17681620915730795, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.035627029836177826}, {"id": 138, "seek": 47740, "start": 490.23999999999995, "end": 493.52, "text": " So, Epeta has just produced this incredible series which is available on YouTube.", "tokens": [51006, 407, 11, 462, 7275, 64, 575, 445, 7126, 341, 4651, 2638, 597, 307, 2435, 322, 3088, 13, 51170], "temperature": 0.0, "avg_logprob": -0.17681620915730795, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.035627029836177826}, {"id": 139, "seek": 47740, "start": 493.52, "end": 494.71999999999997, "text": " Where can folks find it?", "tokens": [51170, 2305, 393, 4024, 915, 309, 30, 51230], "temperature": 0.0, "avg_logprob": -0.17681620915730795, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.035627029836177826}, {"id": 140, "seek": 47740, "start": 494.71999999999997, "end": 495.71999999999997, "text": " Yeah.", "tokens": [51230, 865, 13, 51280], "temperature": 0.0, "avg_logprob": -0.17681620915730795, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.035627029836177826}, {"id": 141, "seek": 47740, "start": 495.71999999999997, "end": 503.15999999999997, "text": " So, basically, if you just go to cats.4.ai, you can see all of the main series lectures", "tokens": [51280, 407, 11, 1936, 11, 498, 291, 445, 352, 281, 11111, 13, 19, 13, 1301, 11, 291, 393, 536, 439, 295, 264, 2135, 2638, 16564, 51652], "temperature": 0.0, "avg_logprob": -0.17681620915730795, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.035627029836177826}, {"id": 142, "seek": 50316, "start": 503.16, "end": 510.56, "text": " from our course, which starts off with assuming kind of a foundational knowledge of deep learning", "tokens": [50364, 490, 527, 1164, 11, 597, 3719, 766, 365, 11926, 733, 295, 257, 32195, 3601, 295, 2452, 2539, 50734], "temperature": 0.0, "avg_logprob": -0.12990110296952098, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.3985905647277832}, {"id": 143, "seek": 50316, "start": 510.56, "end": 515.6, "text": " with neural networks, back propagation, and so on, and then also tries to introduce these", "tokens": [50734, 365, 18161, 9590, 11, 646, 38377, 11, 293, 370, 322, 11, 293, 550, 611, 9898, 281, 5366, 613, 50986], "temperature": 0.0, "avg_logprob": -0.12990110296952098, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.3985905647277832}, {"id": 144, "seek": 50316, "start": 515.6, "end": 521.0, "text": " concepts of category theory and how we can use them to rethink the way we might go about", "tokens": [50986, 10392, 295, 7719, 5261, 293, 577, 321, 393, 764, 552, 281, 34595, 264, 636, 321, 1062, 352, 466, 51256], "temperature": 0.0, "avg_logprob": -0.12990110296952098, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.3985905647277832}, {"id": 145, "seek": 50316, "start": 521.0, "end": 525.48, "text": " some of our standard ideas in deep learning like compositionality or functional structure", "tokens": [51256, 512, 295, 527, 3832, 3487, 294, 2452, 2539, 411, 12686, 1860, 420, 11745, 3877, 51480], "temperature": 0.0, "avg_logprob": -0.12990110296952098, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.3985905647277832}, {"id": 146, "seek": 50316, "start": 525.48, "end": 530.5600000000001, "text": " of deep learning pipelines, or even how can we reinterpret back propagation from the", "tokens": [51480, 295, 2452, 2539, 40168, 11, 420, 754, 577, 393, 321, 319, 41935, 646, 38377, 490, 264, 51734], "temperature": 0.0, "avg_logprob": -0.12990110296952098, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.3985905647277832}, {"id": 147, "seek": 53056, "start": 530.56, "end": 533.1999999999999, "text": " perspective of categorical theory.", "tokens": [50364, 4585, 295, 19250, 804, 5261, 13, 50496], "temperature": 0.0, "avg_logprob": -0.1317091651584791, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.10506434738636017}, {"id": 148, "seek": 53056, "start": 533.1999999999999, "end": 538.9599999999999, "text": " And each lecture basically deals with one particular aspect and we try to keep it grounded", "tokens": [50496, 400, 1184, 7991, 1936, 11215, 365, 472, 1729, 4171, 293, 321, 853, 281, 1066, 309, 23535, 50784], "temperature": 0.0, "avg_logprob": -0.1317091651584791, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.10506434738636017}, {"id": 149, "seek": 53056, "start": 538.9599999999999, "end": 543.1999999999999, "text": " from the beginning to keep it motivated so every single lecture is aligning itself with", "tokens": [50784, 490, 264, 2863, 281, 1066, 309, 14515, 370, 633, 2167, 7991, 307, 419, 9676, 2564, 365, 50996], "temperature": 0.0, "avg_logprob": -0.1317091651584791, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.10506434738636017}, {"id": 150, "seek": 53056, "start": 543.1999999999999, "end": 547.8399999999999, "text": " one particular top-tier paper that one of us has published on one of these venues like", "tokens": [50996, 472, 1729, 1192, 12, 25402, 3035, 300, 472, 295, 505, 575, 6572, 322, 472, 295, 613, 32882, 411, 51228], "temperature": 0.0, "avg_logprob": -0.1317091651584791, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.10506434738636017}, {"id": 151, "seek": 53056, "start": 547.8399999999999, "end": 549.0799999999999, "text": " in Europe.", "tokens": [51228, 294, 3315, 13, 51290], "temperature": 0.0, "avg_logprob": -0.1317091651584791, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.10506434738636017}, {"id": 152, "seek": 53056, "start": 549.0799999999999, "end": 552.52, "text": " And one thing I'll also mention is that the course is actually, in principle, still ongoing", "tokens": [51290, 400, 472, 551, 286, 603, 611, 2152, 307, 300, 264, 1164, 307, 767, 11, 294, 8665, 11, 920, 10452, 51462], "temperature": 0.0, "avg_logprob": -0.1317091651584791, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.10506434738636017}, {"id": 153, "seek": 53056, "start": 552.52, "end": 558.52, "text": " because besides the main series of five lectures that myself, Bruno, Pym, and Andrew have given,", "tokens": [51462, 570, 11868, 264, 2135, 2638, 295, 1732, 16564, 300, 2059, 11, 23046, 11, 430, 4199, 11, 293, 10110, 362, 2212, 11, 51762], "temperature": 0.0, "avg_logprob": -0.1317091651584791, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.10506434738636017}, {"id": 154, "seek": 55852, "start": 558.52, "end": 564.16, "text": " we also have several interesting guest lectures where we try to bring in other influential", "tokens": [50364, 321, 611, 362, 2940, 1880, 8341, 16564, 689, 321, 853, 281, 1565, 294, 661, 22215, 50646], "temperature": 0.0, "avg_logprob": -0.1316346561207491, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.005637824069708586}, {"id": 155, "seek": 55852, "start": 564.16, "end": 570.12, "text": " people at the intersection of popularizing category theory with deep learning concepts", "tokens": [50646, 561, 412, 264, 15236, 295, 3743, 3319, 7719, 5261, 365, 2452, 2539, 10392, 50944], "temperature": 0.0, "avg_logprob": -0.1316346561207491, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.005637824069708586}, {"id": 156, "seek": 55852, "start": 570.12, "end": 575.64, "text": " in a way that can bring an even wider area of views once you're kind of trained in the", "tokens": [50944, 294, 257, 636, 300, 393, 1565, 364, 754, 11842, 1859, 295, 6809, 1564, 291, 434, 733, 295, 8895, 294, 264, 51220], "temperature": 0.0, "avg_logprob": -0.1316346561207491, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.005637824069708586}, {"id": 157, "seek": 55852, "start": 575.64, "end": 579.48, "text": " basics of these techniques, how they're applied to various other things like causality.", "tokens": [51220, 14688, 295, 613, 7512, 11, 577, 436, 434, 6456, 281, 3683, 661, 721, 411, 3302, 1860, 13, 51412], "temperature": 0.0, "avg_logprob": -0.1316346561207491, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.005637824069708586}, {"id": 158, "seek": 55852, "start": 579.48, "end": 585.1999999999999, "text": " We had Taco Cohen tell us about how he uses these concepts to reimagine causality through", "tokens": [51412, 492, 632, 37992, 32968, 980, 505, 466, 577, 415, 4960, 613, 10392, 281, 33433, 10260, 3302, 1860, 807, 51698], "temperature": 0.0, "avg_logprob": -0.1316346561207491, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.005637824069708586}, {"id": 159, "seek": 55852, "start": 585.1999999999999, "end": 586.76, "text": " a categorical lens.", "tokens": [51698, 257, 19250, 804, 6765, 13, 51776], "temperature": 0.0, "avg_logprob": -0.1316346561207491, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.005637824069708586}, {"id": 160, "seek": 58676, "start": 587.72, "end": 592.76, "text": " We're going to have Tydenay Bradley, she's a very popular mathematics educator generally.", "tokens": [50412, 492, 434, 516, 281, 362, 5569, 1556, 320, 36607, 11, 750, 311, 257, 588, 3743, 18666, 31237, 5101, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1871338909818926, "compression_ratio": 1.665625, "no_speech_prob": 0.07895515114068985}, {"id": 161, "seek": 58676, "start": 592.76, "end": 597.04, "text": " She will show how she used some of these concepts to explain transformers.", "tokens": [50664, 1240, 486, 855, 577, 750, 1143, 512, 295, 613, 10392, 281, 2903, 4088, 433, 13, 50878], "temperature": 0.0, "avg_logprob": -0.1871338909818926, "compression_ratio": 1.665625, "no_speech_prob": 0.07895515114068985}, {"id": 162, "seek": 58676, "start": 597.04, "end": 601.28, "text": " And one thing I'm very excited about early next year, we will have actually a guest talk", "tokens": [50878, 400, 472, 551, 286, 478, 588, 2919, 466, 2440, 958, 1064, 11, 321, 486, 362, 767, 257, 8341, 751, 51090], "temperature": 0.0, "avg_logprob": -0.1871338909818926, "compression_ratio": 1.665625, "no_speech_prob": 0.07895515114068985}, {"id": 163, "seek": 58676, "start": 601.28, "end": 607.6, "text": " from David Spivak, which is one of the co-authors of the very famous Seven Sketches in Compositionality", "tokens": [51090, 490, 4389, 1738, 592, 514, 11, 597, 307, 472, 295, 264, 598, 12, 40198, 830, 295, 264, 588, 4618, 14868, 45012, 3781, 294, 6620, 5830, 1860, 51406], "temperature": 0.0, "avg_logprob": -0.1871338909818926, "compression_ratio": 1.665625, "no_speech_prob": 0.07895515114068985}, {"id": 164, "seek": 58676, "start": 607.6, "end": 611.04, "text": " book, which is what initially one of the things that got me really excited about category", "tokens": [51406, 1446, 11, 597, 307, 437, 9105, 472, 295, 264, 721, 300, 658, 385, 534, 2919, 466, 7719, 51578], "temperature": 0.0, "avg_logprob": -0.1871338909818926, "compression_ratio": 1.665625, "no_speech_prob": 0.07895515114068985}, {"id": 165, "seek": 58676, "start": 611.04, "end": 612.88, "text": " theory in the first place.", "tokens": [51578, 5261, 294, 264, 700, 1081, 13, 51670], "temperature": 0.0, "avg_logprob": -0.1871338909818926, "compression_ratio": 1.665625, "no_speech_prob": 0.07895515114068985}, {"id": 166, "seek": 58676, "start": 612.88, "end": 615.48, "text": " So I'm really keen to hear all these perspectives as well.", "tokens": [51670, 407, 286, 478, 534, 20297, 281, 1568, 439, 613, 16766, 382, 731, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1871338909818926, "compression_ratio": 1.665625, "no_speech_prob": 0.07895515114068985}, {"id": 167, "seek": 61548, "start": 615.64, "end": 616.52, "text": " The man is a legend.", "tokens": [50372, 440, 587, 307, 257, 9451, 13, 50416], "temperature": 0.0, "avg_logprob": -0.16026804468653225, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.04792284592986107}, {"id": 168, "seek": 61548, "start": 616.52, "end": 620.9200000000001, "text": " And also on Taco, I interviewed him yesterday and his work on causality is really, really", "tokens": [50416, 400, 611, 322, 37992, 11, 286, 19770, 796, 5186, 293, 702, 589, 322, 3302, 1860, 307, 534, 11, 534, 50636], "temperature": 0.0, "avg_logprob": -0.16026804468653225, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.04792284592986107}, {"id": 169, "seek": 61548, "start": 620.9200000000001, "end": 622.12, "text": " exciting.", "tokens": [50636, 4670, 13, 50696], "temperature": 0.0, "avg_logprob": -0.16026804468653225, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.04792284592986107}, {"id": 170, "seek": 61548, "start": 622.12, "end": 626.5600000000001, "text": " What would you say to people who might be intimidated or scared by category theory?", "tokens": [50696, 708, 576, 291, 584, 281, 561, 567, 1062, 312, 40234, 420, 5338, 538, 7719, 5261, 30, 50918], "temperature": 0.0, "avg_logprob": -0.16026804468653225, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.04792284592986107}, {"id": 171, "seek": 61548, "start": 626.5600000000001, "end": 631.6, "text": " So one thing that I should mention here is that one point about being intimidated or", "tokens": [50918, 407, 472, 551, 300, 286, 820, 2152, 510, 307, 300, 472, 935, 466, 885, 40234, 420, 51170], "temperature": 0.0, "avg_logprob": -0.16026804468653225, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.04792284592986107}, {"id": 172, "seek": 61548, "start": 631.6, "end": 636.36, "text": " scared about category theory is that to really be able to utilize these ideas in how you", "tokens": [51170, 5338, 466, 7719, 5261, 307, 300, 281, 534, 312, 1075, 281, 16117, 613, 3487, 294, 577, 291, 51408], "temperature": 0.0, "avg_logprob": -0.16026804468653225, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.04792284592986107}, {"id": 173, "seek": 61548, "start": 636.36, "end": 642.16, "text": " do research or build your models or anything, it does require a reasonably significant buy-in.", "tokens": [51408, 360, 2132, 420, 1322, 428, 5245, 420, 1340, 11, 309, 775, 3651, 257, 23551, 4776, 2256, 12, 259, 13, 51698], "temperature": 0.0, "avg_logprob": -0.16026804468653225, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.04792284592986107}, {"id": 174, "seek": 64216, "start": 642.1999999999999, "end": 646.36, "text": " So this is not something that you can just read one blog post and suddenly you're empowered", "tokens": [50366, 407, 341, 307, 406, 746, 300, 291, 393, 445, 1401, 472, 6968, 2183, 293, 5800, 291, 434, 27898, 50574], "temperature": 0.0, "avg_logprob": -0.07368436673792397, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00723046250641346}, {"id": 175, "seek": 64216, "start": 646.36, "end": 647.36, "text": " to do it.", "tokens": [50574, 281, 360, 309, 13, 50624], "temperature": 0.0, "avg_logprob": -0.07368436673792397, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00723046250641346}, {"id": 176, "seek": 64216, "start": 647.36, "end": 648.92, "text": " This is like one key thing.", "tokens": [50624, 639, 307, 411, 472, 2141, 551, 13, 50702], "temperature": 0.0, "avg_logprob": -0.07368436673792397, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00723046250641346}, {"id": 177, "seek": 64216, "start": 648.92, "end": 653.4399999999999, "text": " But I would say the main thing that might make people a bit scared to do it is the fact", "tokens": [50702, 583, 286, 576, 584, 264, 2135, 551, 300, 1062, 652, 561, 257, 857, 5338, 281, 360, 309, 307, 264, 1186, 50928], "temperature": 0.0, "avg_logprob": -0.07368436673792397, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00723046250641346}, {"id": 178, "seek": 64216, "start": 653.4399999999999, "end": 659.52, "text": " that many category theory resources out there are a bit guided towards mathematicians.", "tokens": [50928, 300, 867, 7719, 5261, 3593, 484, 456, 366, 257, 857, 19663, 3030, 32811, 2567, 13, 51232], "temperature": 0.0, "avg_logprob": -0.07368436673792397, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00723046250641346}, {"id": 179, "seek": 64216, "start": 659.52, "end": 663.9599999999999, "text": " So they will tend to use the kind of language and the kind of examples that will be quite", "tokens": [51232, 407, 436, 486, 3928, 281, 764, 264, 733, 295, 2856, 293, 264, 733, 295, 5110, 300, 486, 312, 1596, 51454], "temperature": 0.0, "avg_logprob": -0.07368436673792397, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00723046250641346}, {"id": 180, "seek": 64216, "start": 663.9599999999999, "end": 668.56, "text": " attractive to someone who has studied, say, various kinds of differential geometry or", "tokens": [51454, 12609, 281, 1580, 567, 575, 9454, 11, 584, 11, 3683, 3685, 295, 15756, 18426, 420, 51684], "temperature": 0.0, "avg_logprob": -0.07368436673792397, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00723046250641346}, {"id": 181, "seek": 64216, "start": 668.56, "end": 670.0799999999999, "text": " topology or something like this.", "tokens": [51684, 1192, 1793, 420, 746, 411, 341, 13, 51760], "temperature": 0.0, "avg_logprob": -0.07368436673792397, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00723046250641346}, {"id": 182, "seek": 67008, "start": 670.08, "end": 674.5200000000001, "text": " And these kinds of areas tend to generally scare off people who come from a more computer", "tokens": [50364, 400, 613, 3685, 295, 3179, 3928, 281, 5101, 17185, 766, 561, 567, 808, 490, 257, 544, 3820, 50586], "temperature": 0.0, "avg_logprob": -0.11058412504590247, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.007344215176999569}, {"id": 183, "seek": 67008, "start": 674.5200000000001, "end": 676.5200000000001, "text": " science style background.", "tokens": [50586, 3497, 3758, 3678, 13, 50686], "temperature": 0.0, "avg_logprob": -0.11058412504590247, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.007344215176999569}, {"id": 184, "seek": 67008, "start": 676.5200000000001, "end": 681.2800000000001, "text": " And basically I would say the answer to that is you need to find the right resource for", "tokens": [50686, 400, 1936, 286, 576, 584, 264, 1867, 281, 300, 307, 291, 643, 281, 915, 264, 558, 7684, 337, 50924], "temperature": 0.0, "avg_logprob": -0.11058412504590247, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.007344215176999569}, {"id": 185, "seek": 67008, "start": 681.2800000000001, "end": 682.2800000000001, "text": " you.", "tokens": [50924, 291, 13, 50974], "temperature": 0.0, "avg_logprob": -0.11058412504590247, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.007344215176999569}, {"id": 186, "seek": 67008, "start": 682.2800000000001, "end": 686.6800000000001, "text": " Category theory is no more or no less than a way to take a bird's eye view of the phenomena", "tokens": [50974, 383, 48701, 5261, 307, 572, 544, 420, 572, 1570, 813, 257, 636, 281, 747, 257, 5255, 311, 3313, 1910, 295, 264, 22004, 51194], "temperature": 0.0, "avg_logprob": -0.11058412504590247, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.007344215176999569}, {"id": 187, "seek": 67008, "start": 686.6800000000001, "end": 688.2, "text": " that you try to study.", "tokens": [51194, 300, 291, 853, 281, 2979, 13, 51270], "temperature": 0.0, "avg_logprob": -0.11058412504590247, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.007344215176999569}, {"id": 188, "seek": 67008, "start": 688.2, "end": 693.12, "text": " And when you study these phenomena from high in the sky, details become invisible, but", "tokens": [51270, 400, 562, 291, 2979, 613, 22004, 490, 1090, 294, 264, 5443, 11, 4365, 1813, 14603, 11, 457, 51516], "temperature": 0.0, "avg_logprob": -0.11058412504590247, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.007344215176999569}, {"id": 189, "seek": 67008, "start": 693.12, "end": 696.96, "text": " you suddenly get a much better feel for the structure and you can utilize kind of the", "tokens": [51516, 291, 5800, 483, 257, 709, 1101, 841, 337, 264, 3877, 293, 291, 393, 16117, 733, 295, 264, 51708], "temperature": 0.0, "avg_logprob": -0.11058412504590247, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.007344215176999569}, {"id": 190, "seek": 69696, "start": 696.96, "end": 700.5600000000001, "text": " nice patterns that reappear across various fields.", "tokens": [50364, 1481, 8294, 300, 35638, 14881, 2108, 3683, 7909, 13, 50544], "temperature": 0.0, "avg_logprob": -0.11810849204895989, "compression_ratio": 1.7348993288590604, "no_speech_prob": 0.009855356067419052}, {"id": 191, "seek": 69696, "start": 700.5600000000001, "end": 704.32, "text": " And this you would argue is kind of the essence of what we're trying to do in deep learning.", "tokens": [50544, 400, 341, 291, 576, 9695, 307, 733, 295, 264, 12801, 295, 437, 321, 434, 1382, 281, 360, 294, 2452, 2539, 13, 50732], "temperature": 0.0, "avg_logprob": -0.11810849204895989, "compression_ratio": 1.7348993288590604, "no_speech_prob": 0.009855356067419052}, {"id": 192, "seek": 69696, "start": 704.32, "end": 708.08, "text": " We have a lot of analogical way in which these architectures are constructed, right?", "tokens": [50732, 492, 362, 257, 688, 295, 16660, 804, 636, 294, 597, 613, 6331, 1303, 366, 17083, 11, 558, 30, 50920], "temperature": 0.0, "avg_logprob": -0.11810849204895989, "compression_ratio": 1.7348993288590604, "no_speech_prob": 0.009855356067419052}, {"id": 193, "seek": 69696, "start": 708.08, "end": 711.2, "text": " So cats for AI is one possible answer to that.", "tokens": [50920, 407, 11111, 337, 7318, 307, 472, 1944, 1867, 281, 300, 13, 51076], "temperature": 0.0, "avg_logprob": -0.11810849204895989, "compression_ratio": 1.7348993288590604, "no_speech_prob": 0.009855356067419052}, {"id": 194, "seek": 69696, "start": 711.2, "end": 715.52, "text": " It's our way to kind of, as half of us are deep learners and half of us are category", "tokens": [51076, 467, 311, 527, 636, 281, 733, 295, 11, 382, 1922, 295, 505, 366, 2452, 23655, 293, 1922, 295, 505, 366, 7719, 51292], "temperature": 0.0, "avg_logprob": -0.11810849204895989, "compression_ratio": 1.7348993288590604, "no_speech_prob": 0.009855356067419052}, {"id": 195, "seek": 69696, "start": 715.52, "end": 718.48, "text": " theorists, trying to apply these techniques to deep learning.", "tokens": [51292, 27423, 1751, 11, 1382, 281, 3079, 613, 7512, 281, 2452, 2539, 13, 51440], "temperature": 0.0, "avg_logprob": -0.11810849204895989, "compression_ratio": 1.7348993288590604, "no_speech_prob": 0.009855356067419052}, {"id": 196, "seek": 69696, "start": 718.48, "end": 723.2800000000001, "text": " We believe we have a sort of unique perspective of we and like we understand what makes people", "tokens": [51440, 492, 1697, 321, 362, 257, 1333, 295, 3845, 4585, 295, 321, 293, 411, 321, 1223, 437, 1669, 561, 51680], "temperature": 0.0, "avg_logprob": -0.11810849204895989, "compression_ratio": 1.7348993288590604, "no_speech_prob": 0.009855356067419052}, {"id": 197, "seek": 72328, "start": 723.28, "end": 727.48, "text": " afraid to try to talk about these things because some of us had to go through it ourselves", "tokens": [50364, 4638, 281, 853, 281, 751, 466, 613, 721, 570, 512, 295, 505, 632, 281, 352, 807, 309, 4175, 50574], "temperature": 0.0, "avg_logprob": -0.11819691128200954, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.06930828094482422}, {"id": 198, "seek": 72328, "start": 727.48, "end": 731.76, "text": " to deal with the way in which the materials are arranged online right now.", "tokens": [50574, 281, 2028, 365, 264, 636, 294, 597, 264, 5319, 366, 18721, 2950, 558, 586, 13, 50788], "temperature": 0.0, "avg_logprob": -0.11819691128200954, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.06930828094482422}, {"id": 199, "seek": 72328, "start": 731.76, "end": 737.88, "text": " So yeah, maybe just these kinds of resources, starting with them and basically trying as", "tokens": [50788, 407, 1338, 11, 1310, 445, 613, 3685, 295, 3593, 11, 2891, 365, 552, 293, 1936, 1382, 382, 51094], "temperature": 0.0, "avg_logprob": -0.11819691128200954, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.06930828094482422}, {"id": 200, "seek": 72328, "start": 737.88, "end": 743.4399999999999, "text": " much as possible not to descend into the depths of NCAT lab as the very first thing that you", "tokens": [51094, 709, 382, 1944, 406, 281, 16333, 666, 264, 28439, 295, 20786, 2218, 2715, 382, 264, 588, 700, 551, 300, 291, 51372], "temperature": 0.0, "avg_logprob": -0.11819691128200954, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.06930828094482422}, {"id": 201, "seek": 72328, "start": 743.4399999999999, "end": 749.3199999999999, "text": " do can be a good way to maybe stay sane during the first few weeks or months of trying to", "tokens": [51372, 360, 393, 312, 257, 665, 636, 281, 1310, 1754, 45610, 1830, 264, 700, 1326, 3259, 420, 2493, 295, 1382, 281, 51666], "temperature": 0.0, "avg_logprob": -0.11819691128200954, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.06930828094482422}, {"id": 202, "seek": 72328, "start": 749.3199999999999, "end": 750.3199999999999, "text": " explore this field.", "tokens": [51666, 6839, 341, 2519, 13, 51716], "temperature": 0.0, "avg_logprob": -0.11819691128200954, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.06930828094482422}, {"id": 203, "seek": 75032, "start": 750.32, "end": 751.32, "text": " Wonderful.", "tokens": [50364, 22768, 13, 50414], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 204, "seek": 75032, "start": 751.32, "end": 754.8000000000001, "text": " I wondered if you could give a couple of examples of where category theory has been used in", "tokens": [50414, 286, 17055, 498, 291, 727, 976, 257, 1916, 295, 5110, 295, 689, 7719, 5261, 575, 668, 1143, 294, 50588], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 205, "seek": 75032, "start": 754.8000000000001, "end": 755.8000000000001, "text": " an adjacent field.", "tokens": [50588, 364, 24441, 2519, 13, 50638], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 206, "seek": 75032, "start": 755.8000000000001, "end": 756.8000000000001, "text": " I can think of too.", "tokens": [50638, 286, 393, 519, 295, 886, 13, 50688], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 207, "seek": 75032, "start": 756.8000000000001, "end": 761.12, "text": " I can think of Rosen using category theory to describe, you know, sort of ecosystems", "tokens": [50688, 286, 393, 519, 295, 33630, 1228, 7719, 5261, 281, 6786, 11, 291, 458, 11, 1333, 295, 32647, 50904], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 208, "seek": 75032, "start": 761.12, "end": 762.12, "text": " and life.", "tokens": [50904, 293, 993, 13, 50954], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 209, "seek": 75032, "start": 762.12, "end": 766.9200000000001, "text": " I can also think of some quantum mechanics folks that have come up with a category theoretical", "tokens": [50954, 286, 393, 611, 519, 295, 512, 13018, 12939, 4024, 300, 362, 808, 493, 365, 257, 7719, 20864, 51194], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 210, "seek": 75032, "start": 766.9200000000001, "end": 769.12, "text": " conception of quantum mechanics.", "tokens": [51194, 30698, 295, 13018, 12939, 13, 51304], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 211, "seek": 75032, "start": 769.12, "end": 770.12, "text": " Right.", "tokens": [51304, 1779, 13, 51354], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 212, "seek": 75032, "start": 770.12, "end": 771.12, "text": " Are there any other ones?", "tokens": [51354, 2014, 456, 604, 661, 2306, 30, 51404], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 213, "seek": 75032, "start": 771.12, "end": 772.12, "text": " Yeah.", "tokens": [51404, 865, 13, 51454], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 214, "seek": 75032, "start": 772.12, "end": 776.48, "text": " So I mean, I can start by giving the examples that I know about closest in terms of just", "tokens": [51454, 407, 286, 914, 11, 286, 393, 722, 538, 2902, 264, 5110, 300, 286, 458, 466, 13699, 294, 2115, 295, 445, 51672], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 215, "seek": 75032, "start": 776.48, "end": 777.48, "text": " deep learning.", "tokens": [51672, 2452, 2539, 13, 51722], "temperature": 0.0, "avg_logprob": -0.15951976919532718, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.014400942251086235}, {"id": 216, "seek": 77748, "start": 777.48, "end": 783.5600000000001, "text": " So one particular example that I think could be quite interesting is the work that was", "tokens": [50364, 407, 472, 1729, 1365, 300, 286, 519, 727, 312, 1596, 1880, 307, 264, 589, 300, 390, 50668], "temperature": 0.0, "avg_logprob": -0.1771390438079834, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.2015465497970581}, {"id": 217, "seek": 77748, "start": 783.5600000000001, "end": 787.0, "text": " published at NeurIPS two years ago, which I think is one of the first papers that really", "tokens": [50668, 6572, 412, 1734, 374, 40, 6273, 732, 924, 2057, 11, 597, 286, 519, 307, 472, 295, 264, 700, 10577, 300, 534, 50840], "temperature": 0.0, "avg_logprob": -0.1771390438079834, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.2015465497970581}, {"id": 218, "seek": 77748, "start": 787.0, "end": 792.0, "text": " tried to use categorical concepts to build these structures, is the natural graph networks", "tokens": [50840, 3031, 281, 764, 19250, 804, 10392, 281, 1322, 613, 9227, 11, 307, 264, 3303, 4295, 9590, 51090], "temperature": 0.0, "avg_logprob": -0.1771390438079834, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.2015465497970581}, {"id": 219, "seek": 77748, "start": 792.0, "end": 798.8000000000001, "text": " paper from Pimdehan, Tapocoin and Max Swelling, which effectively realizes the fact that the", "tokens": [51090, 3035, 490, 430, 332, 1479, 3451, 11, 13445, 11198, 259, 293, 7402, 29918, 2669, 11, 597, 8659, 29316, 264, 1186, 300, 264, 51430], "temperature": 0.0, "avg_logprob": -0.1771390438079834, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.2015465497970581}, {"id": 220, "seek": 77748, "start": 798.8000000000001, "end": 803.2, "text": " way we build graph neural networks very often we have this one shared message function that's", "tokens": [51430, 636, 321, 1322, 4295, 18161, 9590, 588, 2049, 321, 362, 341, 472, 5507, 3636, 2445, 300, 311, 51650], "temperature": 0.0, "avg_logprob": -0.1771390438079834, "compression_ratio": 1.6532846715328466, "no_speech_prob": 0.2015465497970581}, {"id": 221, "seek": 80320, "start": 803.2, "end": 807.76, "text": " applied everywhere on every single edge on every single graph that you get.", "tokens": [50364, 6456, 5315, 322, 633, 2167, 4691, 322, 633, 2167, 4295, 300, 291, 483, 13, 50592], "temperature": 0.0, "avg_logprob": -0.11533470153808593, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.4568309187889099}, {"id": 222, "seek": 80320, "start": 807.76, "end": 811.5200000000001, "text": " But in reality, is this necessary for it to be a legitimate graph neural network?", "tokens": [50592, 583, 294, 4103, 11, 307, 341, 4818, 337, 309, 281, 312, 257, 17956, 4295, 18161, 3209, 30, 50780], "temperature": 0.0, "avg_logprob": -0.11533470153808593, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.4568309187889099}, {"id": 223, "seek": 80320, "start": 811.5200000000001, "end": 816.7800000000001, "text": " That's actually not the case because if I give you two completely non-isomorphic graphs,", "tokens": [50780, 663, 311, 767, 406, 264, 1389, 570, 498, 286, 976, 291, 732, 2584, 2107, 12, 271, 32702, 299, 24877, 11, 51043], "temperature": 0.0, "avg_logprob": -0.11533470153808593, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.4568309187889099}, {"id": 224, "seek": 80320, "start": 816.7800000000001, "end": 820.4000000000001, "text": " if I choose to have completely different message functions in those two graphs, that's totally", "tokens": [51043, 498, 286, 2826, 281, 362, 2584, 819, 3636, 6828, 294, 729, 732, 24877, 11, 300, 311, 3879, 51224], "temperature": 0.0, "avg_logprob": -0.11533470153808593, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.4568309187889099}, {"id": 225, "seek": 80320, "start": 820.4000000000001, "end": 822.44, "text": " fine because it's still a valid graph net.", "tokens": [51224, 2489, 570, 309, 311, 920, 257, 7363, 4295, 2533, 13, 51326], "temperature": 0.0, "avg_logprob": -0.11533470153808593, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.4568309187889099}, {"id": 226, "seek": 80320, "start": 822.44, "end": 826.24, "text": " If I permute any of those graphs, I'll get the permutation equivalent function for the", "tokens": [51326, 759, 286, 4784, 1169, 604, 295, 729, 24877, 11, 286, 603, 483, 264, 4784, 11380, 10344, 2445, 337, 264, 51516], "temperature": 0.0, "avg_logprob": -0.11533470153808593, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.4568309187889099}, {"id": 227, "seek": 80320, "start": 826.24, "end": 827.5600000000001, "text": " two of them separately.", "tokens": [51516, 732, 295, 552, 14759, 13, 51582], "temperature": 0.0, "avg_logprob": -0.11533470153808593, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.4568309187889099}, {"id": 228, "seek": 80320, "start": 827.5600000000001, "end": 831.76, "text": " There needs to be no weight sharing between them and naturally concepts like these.", "tokens": [51582, 821, 2203, 281, 312, 572, 3364, 5414, 1296, 552, 293, 8195, 10392, 411, 613, 13, 51792], "temperature": 0.0, "avg_logprob": -0.11533470153808593, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.4568309187889099}, {"id": 229, "seek": 83176, "start": 831.76, "end": 837.72, "text": " So this kind of requires taking a step above the group theoretic view of geometric deep", "tokens": [50364, 407, 341, 733, 295, 7029, 1940, 257, 1823, 3673, 264, 1594, 14308, 299, 1910, 295, 33246, 2452, 50662], "temperature": 0.0, "avg_logprob": -0.11445086592927985, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.003763392334803939}, {"id": 230, "seek": 83176, "start": 837.72, "end": 840.84, "text": " learning and into the realm of what is known as a group poid.", "tokens": [50662, 2539, 293, 666, 264, 15355, 295, 437, 307, 2570, 382, 257, 1594, 714, 327, 13, 50818], "temperature": 0.0, "avg_logprob": -0.11445086592927985, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.003763392334803939}, {"id": 231, "seek": 83176, "start": 840.84, "end": 846.16, "text": " You kind of imagine every single graph structure, isomorphic graph structure, living on a sort", "tokens": [50818, 509, 733, 295, 3811, 633, 2167, 4295, 3877, 11, 307, 32702, 299, 4295, 3877, 11, 2647, 322, 257, 1333, 51084], "temperature": 0.0, "avg_logprob": -0.11445086592927985, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.003763392334803939}, {"id": 232, "seek": 83176, "start": 846.16, "end": 850.12, "text": " of island of possible adjacency matrix representations of it.", "tokens": [51084, 295, 6077, 295, 1944, 22940, 3020, 8141, 33358, 295, 309, 13, 51282], "temperature": 0.0, "avg_logprob": -0.11445086592927985, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.003763392334803939}, {"id": 233, "seek": 83176, "start": 850.12, "end": 854.3199999999999, "text": " And for those graphs living on those islands, you need to have some weight sharing.", "tokens": [51282, 400, 337, 729, 24877, 2647, 322, 729, 17402, 11, 291, 643, 281, 362, 512, 3364, 5414, 13, 51492], "temperature": 0.0, "avg_logprob": -0.11445086592927985, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.003763392334803939}, {"id": 234, "seek": 83176, "start": 854.3199999999999, "end": 858.08, "text": " But for separate islands, you don't need to have any weight sharing whatsoever.", "tokens": [51492, 583, 337, 4994, 17402, 11, 291, 500, 380, 643, 281, 362, 604, 3364, 5414, 17076, 13, 51680], "temperature": 0.0, "avg_logprob": -0.11445086592927985, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.003763392334803939}, {"id": 235, "seek": 85808, "start": 858.08, "end": 862.08, "text": " Of course, in practice, these kinds of layers, you would need to have some kind of sharing", "tokens": [50364, 2720, 1164, 11, 294, 3124, 11, 613, 3685, 295, 7914, 11, 291, 576, 643, 281, 362, 512, 733, 295, 5414, 50564], "temperature": 0.0, "avg_logprob": -0.13117875772364, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0018333502812311053}, {"id": 236, "seek": 85808, "start": 862.08, "end": 865.6800000000001, "text": " of weights in order to make them scalable to arbitrary new graph structures you haven't", "tokens": [50564, 295, 17443, 294, 1668, 281, 652, 552, 38481, 281, 23211, 777, 4295, 9227, 291, 2378, 380, 50744], "temperature": 0.0, "avg_logprob": -0.13117875772364, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0018333502812311053}, {"id": 237, "seek": 85808, "start": 865.6800000000001, "end": 870.48, "text": " seen at training time, but it allows you a lot more flexibility about how you go about", "tokens": [50744, 1612, 412, 3097, 565, 11, 457, 309, 4045, 291, 257, 688, 544, 12635, 466, 577, 291, 352, 466, 50984], "temperature": 0.0, "avg_logprob": -0.13117875772364, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0018333502812311053}, {"id": 238, "seek": 85808, "start": 870.48, "end": 871.48, "text": " building your functions.", "tokens": [50984, 2390, 428, 6828, 13, 51034], "temperature": 0.0, "avg_logprob": -0.13117875772364, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0018333502812311053}, {"id": 239, "seek": 85808, "start": 871.48, "end": 875.08, "text": " And you're no longer constrained to have just one function everywhere repeated, right?", "tokens": [51034, 400, 291, 434, 572, 2854, 38901, 281, 362, 445, 472, 2445, 5315, 10477, 11, 558, 30, 51214], "temperature": 0.0, "avg_logprob": -0.13117875772364, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0018333502812311053}, {"id": 240, "seek": 85808, "start": 875.08, "end": 879.24, "text": " So that's maybe one example that, at least to me, was what first motivated me and made", "tokens": [51214, 407, 300, 311, 1310, 472, 1365, 300, 11, 412, 1935, 281, 385, 11, 390, 437, 700, 14515, 385, 293, 1027, 51422], "temperature": 0.0, "avg_logprob": -0.13117875772364, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0018333502812311053}, {"id": 241, "seek": 85808, "start": 879.24, "end": 883.6400000000001, "text": " me realize that there's more to this stuff than just to say what group theory will give", "tokens": [51422, 385, 4325, 300, 456, 311, 544, 281, 341, 1507, 813, 445, 281, 584, 437, 1594, 5261, 486, 976, 51642], "temperature": 0.0, "avg_logprob": -0.13117875772364, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0018333502812311053}, {"id": 242, "seek": 85808, "start": 883.6400000000001, "end": 884.6400000000001, "text": " us.", "tokens": [51642, 505, 13, 51692], "temperature": 0.0, "avg_logprob": -0.13117875772364, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0018333502812311053}, {"id": 243, "seek": 85808, "start": 884.6400000000001, "end": 885.6400000000001, "text": " Amazing.", "tokens": [51692, 14165, 13, 51742], "temperature": 0.0, "avg_logprob": -0.13117875772364, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0018333502812311053}, {"id": 244, "seek": 88564, "start": 885.64, "end": 888.84, "text": " I'm really interested in your work in algorithmic reasoning, and I know you were just discussing", "tokens": [50364, 286, 478, 534, 3102, 294, 428, 589, 294, 9284, 299, 21577, 11, 293, 286, 458, 291, 645, 445, 10850, 50524], "temperature": 0.0, "avg_logprob": -0.19229424844576617, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.016344647854566574}, {"id": 245, "seek": 88564, "start": 888.84, "end": 894.04, "text": " it as an adjacent thing, and very soon we want to make a show, actually, on your work", "tokens": [50524, 309, 382, 364, 24441, 551, 11, 293, 588, 2321, 321, 528, 281, 652, 257, 855, 11, 767, 11, 322, 428, 589, 50784], "temperature": 0.0, "avg_logprob": -0.19229424844576617, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.016344647854566574}, {"id": 246, "seek": 88564, "start": 894.04, "end": 895.04, "text": " on that.", "tokens": [50784, 322, 300, 13, 50834], "temperature": 0.0, "avg_logprob": -0.19229424844576617, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.016344647854566574}, {"id": 247, "seek": 88564, "start": 895.04, "end": 897.48, "text": " But if you wouldn't mind, could you just sketch out algorithmic reasoning?", "tokens": [50834, 583, 498, 291, 2759, 380, 1575, 11, 727, 291, 445, 12325, 484, 9284, 299, 21577, 30, 50956], "temperature": 0.0, "avg_logprob": -0.19229424844576617, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.016344647854566574}, {"id": 248, "seek": 88564, "start": 897.48, "end": 898.48, "text": " Yes, wonderful.", "tokens": [50956, 1079, 11, 3715, 13, 51006], "temperature": 0.0, "avg_logprob": -0.19229424844576617, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.016344647854566574}, {"id": 249, "seek": 88564, "start": 898.48, "end": 900.1999999999999, "text": " So, very happy to.", "tokens": [51006, 407, 11, 588, 2055, 281, 13, 51092], "temperature": 0.0, "avg_logprob": -0.19229424844576617, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.016344647854566574}, {"id": 250, "seek": 88564, "start": 900.1999999999999, "end": 905.48, "text": " Basically, what are we interested in algorithmic reasoning is building neural networks.", "tokens": [51092, 8537, 11, 437, 366, 321, 3102, 294, 9284, 299, 21577, 307, 2390, 18161, 9590, 13, 51356], "temperature": 0.0, "avg_logprob": -0.19229424844576617, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.016344647854566574}, {"id": 251, "seek": 88564, "start": 905.48, "end": 908.84, "text": " They tend to be graph neural networks, but generally speaking, neural networks that are", "tokens": [51356, 814, 3928, 281, 312, 4295, 18161, 9590, 11, 457, 5101, 4124, 11, 18161, 9590, 300, 366, 51524], "temperature": 0.0, "avg_logprob": -0.19229424844576617, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.016344647854566574}, {"id": 252, "seek": 88564, "start": 908.84, "end": 912.48, "text": " capable of executing algorithmic computation.", "tokens": [51524, 8189, 295, 32368, 9284, 299, 24903, 13, 51706], "temperature": 0.0, "avg_logprob": -0.19229424844576617, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.016344647854566574}, {"id": 253, "seek": 91248, "start": 912.48, "end": 917.24, "text": " So if I give you some context on what is the state of a particular algorithm, can my network", "tokens": [50364, 407, 498, 286, 976, 291, 512, 4319, 322, 437, 307, 264, 1785, 295, 257, 1729, 9284, 11, 393, 452, 3209, 50602], "temperature": 0.0, "avg_logprob": -0.1047143289598368, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.02929660677909851}, {"id": 254, "seek": 91248, "start": 917.24, "end": 922.32, "text": " somehow learn to execute that algorithm ideally in some latent space such that at every single", "tokens": [50602, 6063, 1466, 281, 14483, 300, 9284, 22915, 294, 512, 48994, 1901, 1270, 300, 412, 633, 2167, 50856], "temperature": 0.0, "avg_logprob": -0.1047143289598368, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.02929660677909851}, {"id": 255, "seek": 91248, "start": 922.32, "end": 926.9200000000001, "text": " step of the way, I could if I wanted to decode the states of that algorithm.", "tokens": [50856, 1823, 295, 264, 636, 11, 286, 727, 498, 286, 1415, 281, 979, 1429, 264, 4368, 295, 300, 9284, 13, 51086], "temperature": 0.0, "avg_logprob": -0.1047143289598368, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.02929660677909851}, {"id": 256, "seek": 91248, "start": 926.9200000000001, "end": 929.2, "text": " So that's basically the main premise.", "tokens": [51086, 407, 300, 311, 1936, 264, 2135, 22045, 13, 51200], "temperature": 0.0, "avg_logprob": -0.1047143289598368, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.02929660677909851}, {"id": 257, "seek": 91248, "start": 929.2, "end": 930.6800000000001, "text": " Why do we care about this?", "tokens": [51200, 1545, 360, 321, 1127, 466, 341, 30, 51274], "temperature": 0.0, "avg_logprob": -0.1047143289598368, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.02929660677909851}, {"id": 258, "seek": 91248, "start": 930.6800000000001, "end": 935.6800000000001, "text": " Well, basically, I think of algorithms as a sort of basic foundational building block", "tokens": [51274, 1042, 11, 1936, 11, 286, 519, 295, 14642, 382, 257, 1333, 295, 3875, 32195, 2390, 3461, 51524], "temperature": 0.0, "avg_logprob": -0.1047143289598368, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.02929660677909851}, {"id": 259, "seek": 91248, "start": 935.6800000000001, "end": 940.76, "text": " of reasoning, and it's kind of a timeless principle where a software engineer reads", "tokens": [51524, 295, 21577, 11, 293, 309, 311, 733, 295, 257, 41200, 8665, 689, 257, 4722, 11403, 15700, 51778], "temperature": 0.0, "avg_logprob": -0.1047143289598368, "compression_ratio": 1.6972789115646258, "no_speech_prob": 0.02929660677909851}, {"id": 260, "seek": 94076, "start": 940.76, "end": 945.76, "text": " through one of these textbooks on algorithms and learns these 30 or 40 basis algorithms,", "tokens": [50364, 807, 472, 295, 613, 33587, 322, 14642, 293, 27152, 613, 2217, 420, 3356, 5143, 14642, 11, 50614], "temperature": 0.0, "avg_logprob": -0.13782200512585338, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.06651942431926727}, {"id": 261, "seek": 94076, "start": 945.76, "end": 950.04, "text": " and then that knowledge serves them for life in a whole career of software engineering.", "tokens": [50614, 293, 550, 300, 3601, 13451, 552, 337, 993, 294, 257, 1379, 3988, 295, 4722, 7043, 13, 50828], "temperature": 0.0, "avg_logprob": -0.13782200512585338, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.06651942431926727}, {"id": 262, "seek": 94076, "start": 950.04, "end": 954.8, "text": " So basically, we have this hypothesis that you have this nice basis of algorithms that", "tokens": [50828, 407, 1936, 11, 321, 362, 341, 17291, 300, 291, 362, 341, 1481, 5143, 295, 14642, 300, 51066], "temperature": 0.0, "avg_logprob": -0.13782200512585338, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.06651942431926727}, {"id": 263, "seek": 94076, "start": 954.8, "end": 959.76, "text": " if you can master how to do them robustly, you can try to mimic any kind of at least", "tokens": [51066, 498, 291, 393, 4505, 577, 281, 360, 552, 13956, 356, 11, 291, 393, 853, 281, 31075, 604, 733, 295, 412, 1935, 51314], "temperature": 0.0, "avg_logprob": -0.13782200512585338, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.06651942431926727}, {"id": 264, "seek": 94076, "start": 959.76, "end": 962.08, "text": " polynomial time reasoning behavior.", "tokens": [51314, 26110, 565, 21577, 5223, 13, 51430], "temperature": 0.0, "avg_logprob": -0.13782200512585338, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.06651942431926727}, {"id": 265, "seek": 94076, "start": 962.08, "end": 966.56, "text": " And that's really nice because if you look at the way current state-of-the-art large-scale", "tokens": [51430, 400, 300, 311, 534, 1481, 570, 498, 291, 574, 412, 264, 636, 2190, 1785, 12, 2670, 12, 3322, 12, 446, 2416, 12, 20033, 51654], "temperature": 0.0, "avg_logprob": -0.13782200512585338, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.06651942431926727}, {"id": 266, "seek": 96656, "start": 966.56, "end": 972.3199999999999, "text": " models tend to have shortcomings, it's usually in those kinds of robust extrapolation problems.", "tokens": [50364, 5245, 3928, 281, 362, 2099, 49886, 11, 309, 311, 2673, 294, 729, 3685, 295, 13956, 48224, 399, 2740, 13, 50652], "temperature": 0.0, "avg_logprob": -0.11726769197334364, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.20679110288619995}, {"id": 267, "seek": 96656, "start": 972.3199999999999, "end": 978.68, "text": " Basically, if we want to have a really good AI scientist that's able to not just make", "tokens": [50652, 8537, 11, 498, 321, 528, 281, 362, 257, 534, 665, 7318, 12662, 300, 311, 1075, 281, 406, 445, 652, 50970], "temperature": 0.0, "avg_logprob": -0.11726769197334364, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.20679110288619995}, {"id": 268, "seek": 96656, "start": 978.68, "end": 983.28, "text": " great sense of a bunch of training data from the internet, but also use that training data", "tokens": [50970, 869, 2020, 295, 257, 3840, 295, 3097, 1412, 490, 264, 4705, 11, 457, 611, 764, 300, 3097, 1412, 51200], "temperature": 0.0, "avg_logprob": -0.11726769197334364, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.20679110288619995}, {"id": 269, "seek": 96656, "start": 983.28, "end": 988.64, "text": " to derive new knowledge, you need some robustified way to apply rules to get infinite knowledge", "tokens": [51200, 281, 28446, 777, 3601, 11, 291, 643, 512, 13956, 2587, 636, 281, 3079, 4474, 281, 483, 13785, 3601, 51468], "temperature": 0.0, "avg_logprob": -0.11726769197334364, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.20679110288619995}, {"id": 270, "seek": 96656, "start": 988.64, "end": 990.4399999999999, "text": " from finite means.", "tokens": [51468, 490, 19362, 1355, 13, 51558], "temperature": 0.0, "avg_logprob": -0.11726769197334364, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.20679110288619995}, {"id": 271, "seek": 96656, "start": 990.4399999999999, "end": 992.3199999999999, "text": " So basically, that's what we want to do.", "tokens": [51558, 407, 1936, 11, 300, 311, 437, 321, 528, 281, 360, 13, 51652], "temperature": 0.0, "avg_logprob": -0.11726769197334364, "compression_ratio": 1.691699604743083, "no_speech_prob": 0.20679110288619995}, {"id": 272, "seek": 99232, "start": 992.32, "end": 996.44, "text": " We want to find ways inductive biases or training procedures to build neural networks", "tokens": [50364, 492, 528, 281, 915, 2098, 31612, 488, 32152, 420, 3097, 13846, 281, 1322, 18161, 9590, 50570], "temperature": 0.0, "avg_logprob": -0.12902211207969516, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.005301003344357014}, {"id": 273, "seek": 99232, "start": 996.44, "end": 999.2, "text": " that are more algorithmically capable.", "tokens": [50570, 300, 366, 544, 9284, 984, 8189, 13, 50708], "temperature": 0.0, "avg_logprob": -0.12902211207969516, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.005301003344357014}, {"id": 274, "seek": 99232, "start": 999.2, "end": 1004.24, "text": " And in algorithmic reasoning, we obviously spent a lot of time trying to make this happen,", "tokens": [50708, 400, 294, 9284, 299, 21577, 11, 321, 2745, 4418, 257, 688, 295, 565, 1382, 281, 652, 341, 1051, 11, 50960], "temperature": 0.0, "avg_logprob": -0.12902211207969516, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.005301003344357014}, {"id": 275, "seek": 99232, "start": 1004.24, "end": 1008.6, "text": " just building better graph neural networks that align better with target algorithms so", "tokens": [50960, 445, 2390, 1101, 4295, 18161, 9590, 300, 7975, 1101, 365, 3779, 14642, 370, 51178], "temperature": 0.0, "avg_logprob": -0.12902211207969516, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.005301003344357014}, {"id": 276, "seek": 99232, "start": 1008.6, "end": 1013.4000000000001, "text": " that you can execute them better, but then the really exciting part comes where we've", "tokens": [51178, 300, 291, 393, 14483, 552, 1101, 11, 457, 550, 264, 534, 4670, 644, 1487, 689, 321, 600, 51418], "temperature": 0.0, "avg_logprob": -0.12902211207969516, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.005301003344357014}, {"id": 277, "seek": 99232, "start": 1013.4000000000001, "end": 1017.4000000000001, "text": " actually taken some of these graph neural networks that have been pre-trained to execute", "tokens": [51418, 767, 2726, 512, 295, 613, 4295, 18161, 9590, 300, 362, 668, 659, 12, 17227, 2001, 281, 14483, 51618], "temperature": 0.0, "avg_logprob": -0.12902211207969516, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.005301003344357014}, {"id": 278, "seek": 101740, "start": 1017.4, "end": 1022.48, "text": " one particular algorithm, and then we deployed it in a real-world problem where that algorithm", "tokens": [50364, 472, 1729, 9284, 11, 293, 550, 321, 17826, 309, 294, 257, 957, 12, 13217, 1154, 689, 300, 9284, 50618], "temperature": 0.0, "avg_logprob": -0.1249610839351531, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.4144841134548187}, {"id": 279, "seek": 101740, "start": 1022.48, "end": 1027.26, "text": " is required, and we achieved, say, significant representational benefits in terms of downstream", "tokens": [50618, 307, 4739, 11, 293, 321, 11042, 11, 584, 11, 4776, 2906, 1478, 5311, 294, 2115, 295, 30621, 50857], "temperature": 0.0, "avg_logprob": -0.1249610839351531, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.4144841134548187}, {"id": 280, "seek": 101740, "start": 1027.26, "end": 1028.26, "text": " accuracy.", "tokens": [50857, 14170, 13, 50907], "temperature": 0.0, "avg_logprob": -0.1249610839351531, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.4144841134548187}, {"id": 281, "seek": 101740, "start": 1028.26, "end": 1031.44, "text": " So the idea behind this, and I'll give an example from Google Maps.", "tokens": [50907, 407, 264, 1558, 2261, 341, 11, 293, 286, 603, 976, 364, 1365, 490, 3329, 28978, 13, 51066], "temperature": 0.0, "avg_logprob": -0.1249610839351531, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.4144841134548187}, {"id": 282, "seek": 101740, "start": 1031.44, "end": 1036.04, "text": " This is an application that I worked on at DeepMind, so it's something that I've thought", "tokens": [51066, 639, 307, 364, 3861, 300, 286, 2732, 322, 412, 14895, 44, 471, 11, 370, 309, 311, 746, 300, 286, 600, 1194, 51296], "temperature": 0.0, "avg_logprob": -0.1249610839351531, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.4144841134548187}, {"id": 283, "seek": 101740, "start": 1036.04, "end": 1038.04, "text": " about quite a bit.", "tokens": [51296, 466, 1596, 257, 857, 13, 51396], "temperature": 0.0, "avg_logprob": -0.1249610839351531, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.4144841134548187}, {"id": 284, "seek": 101740, "start": 1038.04, "end": 1043.42, "text": " We've invented these algorithms, like Dijkstra's algorithm, to be able to resolve these kinds", "tokens": [51396, 492, 600, 14479, 613, 14642, 11, 411, 413, 6940, 19639, 311, 9284, 11, 281, 312, 1075, 281, 14151, 613, 3685, 51665], "temperature": 0.0, "avg_logprob": -0.1249610839351531, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.4144841134548187}, {"id": 285, "seek": 101740, "start": 1043.42, "end": 1045.84, "text": " of real-world routing problems.", "tokens": [51665, 295, 957, 12, 13217, 32722, 2740, 13, 51786], "temperature": 0.0, "avg_logprob": -0.1249610839351531, "compression_ratio": 1.6405228758169934, "no_speech_prob": 0.4144841134548187}, {"id": 286, "seek": 104584, "start": 1045.84, "end": 1049.4399999999998, "text": " That's the kind of motivation for why you want to build the shortest path algorithm.", "tokens": [50364, 663, 311, 264, 733, 295, 12335, 337, 983, 291, 528, 281, 1322, 264, 31875, 3100, 9284, 13, 50544], "temperature": 0.0, "avg_logprob": -0.1269390106201172, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.06556766480207443}, {"id": 287, "seek": 104584, "start": 1049.4399999999998, "end": 1053.48, "text": " And it comes as a little surprise that when you have real-world traffic data, you might", "tokens": [50544, 400, 309, 1487, 382, 257, 707, 6365, 300, 562, 291, 362, 957, 12, 13217, 6419, 1412, 11, 291, 1062, 50746], "temperature": 0.0, "avg_logprob": -0.1269390106201172, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.06556766480207443}, {"id": 288, "seek": 104584, "start": 1053.48, "end": 1059.0, "text": " be tempted to apply Dijkstra's algorithm to solve it, to route agents in traffic.", "tokens": [50746, 312, 29941, 281, 3079, 413, 6940, 19639, 311, 9284, 281, 5039, 309, 11, 281, 7955, 12554, 294, 6419, 13, 51022], "temperature": 0.0, "avg_logprob": -0.1269390106201172, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.06556766480207443}, {"id": 289, "seek": 104584, "start": 1059.0, "end": 1063.28, "text": " However, what is the actual data that, say, Google Maps has access to?", "tokens": [51022, 2908, 11, 437, 307, 264, 3539, 1412, 300, 11, 584, 11, 3329, 28978, 575, 2105, 281, 30, 51236], "temperature": 0.0, "avg_logprob": -0.1269390106201172, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.06556766480207443}, {"id": 290, "seek": 104584, "start": 1063.28, "end": 1067.28, "text": " It's not this nice, abstractified graph with a single scalar in every edge where you can", "tokens": [51236, 467, 311, 406, 341, 1481, 11, 12649, 2587, 4295, 365, 257, 2167, 39684, 294, 633, 4691, 689, 291, 393, 51436], "temperature": 0.0, "avg_logprob": -0.1269390106201172, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.06556766480207443}, {"id": 291, "seek": 104584, "start": 1067.28, "end": 1069.32, "text": " just go ahead and apply an algorithm.", "tokens": [51436, 445, 352, 2286, 293, 3079, 364, 9284, 13, 51538], "temperature": 0.0, "avg_logprob": -0.1269390106201172, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.06556766480207443}, {"id": 292, "seek": 104584, "start": 1069.32, "end": 1073.36, "text": " In fact, there's a huge bridge that must be built between the real data and the input", "tokens": [51538, 682, 1186, 11, 456, 311, 257, 2603, 7283, 300, 1633, 312, 3094, 1296, 264, 957, 1412, 293, 264, 4846, 51740], "temperature": 0.0, "avg_logprob": -0.1269390106201172, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.06556766480207443}, {"id": 293, "seek": 104584, "start": 1073.36, "end": 1074.36, "text": " to the algorithm.", "tokens": [51740, 281, 264, 9284, 13, 51790], "temperature": 0.0, "avg_logprob": -0.1269390106201172, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.06556766480207443}, {"id": 294, "seek": 107436, "start": 1074.36, "end": 1079.1599999999999, "text": " In fact, Google Maps data is typically people's cell phones in their cars, and the cars move,", "tokens": [50364, 682, 1186, 11, 3329, 28978, 1412, 307, 5850, 561, 311, 2815, 10216, 294, 641, 5163, 11, 293, 264, 5163, 1286, 11, 50604], "temperature": 0.0, "avg_logprob": -0.14039848790024267, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.03160158544778824}, {"id": 295, "seek": 107436, "start": 1079.1599999999999, "end": 1083.4399999999998, "text": " the phones move, and then based on the movement of the phones, you somehow infer how fast the", "tokens": [50604, 264, 10216, 1286, 11, 293, 550, 2361, 322, 264, 3963, 295, 264, 10216, 11, 291, 6063, 13596, 577, 2370, 264, 50818], "temperature": 0.0, "avg_logprob": -0.14039848790024267, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.03160158544778824}, {"id": 296, "seek": 107436, "start": 1083.4399999999998, "end": 1085.8799999999999, "text": " car is going or something like that.", "tokens": [50818, 1032, 307, 516, 420, 746, 411, 300, 13, 50940], "temperature": 0.0, "avg_logprob": -0.14039848790024267, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.03160158544778824}, {"id": 297, "seek": 107436, "start": 1085.8799999999999, "end": 1091.24, "text": " And this is very noisy, not very well-structured, and you have to somehow go from there to a", "tokens": [50940, 400, 341, 307, 588, 24518, 11, 406, 588, 731, 12, 372, 46847, 11, 293, 291, 362, 281, 6063, 352, 490, 456, 281, 257, 51208], "temperature": 0.0, "avg_logprob": -0.14039848790024267, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.03160158544778824}, {"id": 298, "seek": 107436, "start": 1091.24, "end": 1093.6, "text": " graph where you can apply this heuristic.", "tokens": [51208, 4295, 689, 291, 393, 3079, 341, 415, 374, 3142, 13, 51326], "temperature": 0.0, "avg_logprob": -0.14039848790024267, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.03160158544778824}, {"id": 299, "seek": 107436, "start": 1093.6, "end": 1098.6399999999999, "text": " Previously, it was always done exclusively by humans, like feature engineers, effectively.", "tokens": [51326, 33606, 11, 309, 390, 1009, 1096, 20638, 538, 6255, 11, 411, 4111, 11955, 11, 8659, 13, 51578], "temperature": 0.0, "avg_logprob": -0.14039848790024267, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.03160158544778824}, {"id": 300, "seek": 107436, "start": 1098.6399999999999, "end": 1102.76, "text": " And whenever there's a human feature engineer in the loop like this, you are almost certainly", "tokens": [51578, 400, 5699, 456, 311, 257, 1952, 4111, 11403, 294, 264, 6367, 411, 341, 11, 291, 366, 1920, 3297, 51784], "temperature": 0.0, "avg_logprob": -0.14039848790024267, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.03160158544778824}, {"id": 301, "seek": 110276, "start": 1102.76, "end": 1106.28, "text": " going to drop a lot of information that you might need to solve the problem.", "tokens": [50364, 516, 281, 3270, 257, 688, 295, 1589, 300, 291, 1062, 643, 281, 5039, 264, 1154, 13, 50540], "temperature": 0.0, "avg_logprob": -0.09594031123371867, "compression_ratio": 1.7492625368731562, "no_speech_prob": 0.01261527743190527}, {"id": 302, "seek": 110276, "start": 1106.28, "end": 1109.56, "text": " So basically, you have a huge kind of bridge to cross there.", "tokens": [50540, 407, 1936, 11, 291, 362, 257, 2603, 733, 295, 7283, 281, 3278, 456, 13, 50704], "temperature": 0.0, "avg_logprob": -0.09594031123371867, "compression_ratio": 1.7492625368731562, "no_speech_prob": 0.01261527743190527}, {"id": 303, "seek": 110276, "start": 1109.56, "end": 1112.6, "text": " And with algorithmic reasoning, we now don't use Dijkstra's algorithm.", "tokens": [50704, 400, 365, 9284, 299, 21577, 11, 321, 586, 500, 380, 764, 413, 6940, 19639, 311, 9284, 13, 50856], "temperature": 0.0, "avg_logprob": -0.09594031123371867, "compression_ratio": 1.7492625368731562, "no_speech_prob": 0.01261527743190527}, {"id": 304, "seek": 110276, "start": 1112.6, "end": 1117.44, "text": " We use a high-dimensional graph net that was pre-trained to execute Dijkstra's algorithm", "tokens": [50856, 492, 764, 257, 1090, 12, 18759, 4295, 2533, 300, 390, 659, 12, 17227, 2001, 281, 14483, 413, 6940, 19639, 311, 9284, 51098], "temperature": 0.0, "avg_logprob": -0.09594031123371867, "compression_ratio": 1.7492625368731562, "no_speech_prob": 0.01261527743190527}, {"id": 305, "seek": 110276, "start": 1117.44, "end": 1118.6, "text": " in a latent space.", "tokens": [51098, 294, 257, 48994, 1901, 13, 51156], "temperature": 0.0, "avg_logprob": -0.09594031123371867, "compression_ratio": 1.7492625368731562, "no_speech_prob": 0.01261527743190527}, {"id": 306, "seek": 110276, "start": 1118.6, "end": 1122.64, "text": " So now this gives us a differentiable component that we can hook up to any encoder and decoder", "tokens": [51156, 407, 586, 341, 2709, 505, 257, 819, 9364, 6542, 300, 321, 393, 6328, 493, 281, 604, 2058, 19866, 293, 979, 19866, 51358], "temperature": 0.0, "avg_logprob": -0.09594031123371867, "compression_ratio": 1.7492625368731562, "no_speech_prob": 0.01261527743190527}, {"id": 307, "seek": 110276, "start": 1122.64, "end": 1127.52, "text": " function we want to, so we can go straight from raw data and code it into the GNN's latent", "tokens": [51358, 2445, 321, 528, 281, 11, 370, 321, 393, 352, 2997, 490, 8936, 1412, 293, 3089, 309, 666, 264, 46411, 45, 311, 48994, 51602], "temperature": 0.0, "avg_logprob": -0.09594031123371867, "compression_ratio": 1.7492625368731562, "no_speech_prob": 0.01261527743190527}, {"id": 308, "seek": 110276, "start": 1127.52, "end": 1131.92, "text": " space, run the algorithm there, and then decode whatever it is that you need, like routing", "tokens": [51602, 1901, 11, 1190, 264, 9284, 456, 11, 293, 550, 979, 1429, 2035, 309, 307, 300, 291, 643, 11, 411, 32722, 51822], "temperature": 0.0, "avg_logprob": -0.09594031123371867, "compression_ratio": 1.7492625368731562, "no_speech_prob": 0.01261527743190527}, {"id": 309, "seek": 113192, "start": 1131.92, "end": 1133.68, "text": " the vehicles in traffic.", "tokens": [50364, 264, 8948, 294, 6419, 13, 50452], "temperature": 0.0, "avg_logprob": -0.15585150285200638, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.09178568422794342}, {"id": 310, "seek": 113192, "start": 1133.68, "end": 1138.6000000000001, "text": " So now purely through backprop, this encoder function now learns to do what the human feature", "tokens": [50452, 407, 586, 17491, 807, 646, 79, 1513, 11, 341, 2058, 19866, 2445, 586, 27152, 281, 360, 437, 264, 1952, 4111, 50698], "temperature": 0.0, "avg_logprob": -0.15585150285200638, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.09178568422794342}, {"id": 311, "seek": 113192, "start": 1138.6000000000001, "end": 1139.6000000000001, "text": " engineer did.", "tokens": [50698, 11403, 630, 13, 50748], "temperature": 0.0, "avg_logprob": -0.15585150285200638, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.09178568422794342}, {"id": 312, "seek": 113192, "start": 1139.6000000000001, "end": 1144.5600000000002, "text": " It learns how to most effectively map that complicated, noisy, real-world data into the", "tokens": [50748, 467, 27152, 577, 281, 881, 8659, 4471, 300, 6179, 11, 24518, 11, 957, 12, 13217, 1412, 666, 264, 50996], "temperature": 0.0, "avg_logprob": -0.15585150285200638, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.09178568422794342}, {"id": 313, "seek": 113192, "start": 1144.5600000000002, "end": 1147.28, "text": " latent space where this GNN can best do its thing.", "tokens": [50996, 48994, 1901, 689, 341, 46411, 45, 393, 1151, 360, 1080, 551, 13, 51132], "temperature": 0.0, "avg_logprob": -0.15585150285200638, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.09178568422794342}, {"id": 314, "seek": 113192, "start": 1147.28, "end": 1148.44, "text": " That really is software 2.0.", "tokens": [51132, 663, 534, 307, 4722, 568, 13, 15, 13, 51190], "temperature": 0.0, "avg_logprob": -0.15585150285200638, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.09178568422794342}, {"id": 315, "seek": 113192, "start": 1148.44, "end": 1153.1200000000001, "text": " But I wanted to ask you about the computational limitations, because you just said something", "tokens": [51190, 583, 286, 1415, 281, 1029, 291, 466, 264, 28270, 15705, 11, 570, 291, 445, 848, 746, 51424], "temperature": 0.0, "avg_logprob": -0.15585150285200638, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.09178568422794342}, {"id": 316, "seek": 113192, "start": 1153.1200000000001, "end": 1157.64, "text": " interesting about representing infinite objects with a finite memory.", "tokens": [51424, 1880, 466, 13460, 13785, 6565, 365, 257, 19362, 4675, 13, 51650], "temperature": 0.0, "avg_logprob": -0.15585150285200638, "compression_ratio": 1.5910652920962198, "no_speech_prob": 0.09178568422794342}, {"id": 317, "seek": 115764, "start": 1157.64, "end": 1163.96, "text": " So neural networks are not Turing machines, but they can extrapolate, of course.", "tokens": [50364, 407, 18161, 9590, 366, 406, 314, 1345, 8379, 11, 457, 436, 393, 48224, 473, 11, 295, 1164, 13, 50680], "temperature": 0.0, "avg_logprob": -0.15713937932794744, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.2352050244808197}, {"id": 318, "seek": 115764, "start": 1163.96, "end": 1165.5200000000002, "text": " What's the realistic limitation?", "tokens": [50680, 708, 311, 264, 12465, 27432, 30, 50758], "temperature": 0.0, "avg_logprob": -0.15713937932794744, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.2352050244808197}, {"id": 319, "seek": 115764, "start": 1165.5200000000002, "end": 1170.5600000000002, "text": " Let's say you're trying to learn an algorithm, how far can you go with a neural network?", "tokens": [50758, 961, 311, 584, 291, 434, 1382, 281, 1466, 364, 9284, 11, 577, 1400, 393, 291, 352, 365, 257, 18161, 3209, 30, 51010], "temperature": 0.0, "avg_logprob": -0.15713937932794744, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.2352050244808197}, {"id": 320, "seek": 115764, "start": 1170.5600000000002, "end": 1173.88, "text": " So the thing is, there are cases where you can go very far.", "tokens": [51010, 407, 264, 551, 307, 11, 456, 366, 3331, 689, 291, 393, 352, 588, 1400, 13, 51176], "temperature": 0.0, "avg_logprob": -0.15713937932794744, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.2352050244808197}, {"id": 321, "seek": 115764, "start": 1173.88, "end": 1178.6000000000001, "text": " We do have theory that is very robust about this, and I think it's theory that is actually", "tokens": [51176, 492, 360, 362, 5261, 300, 307, 588, 13956, 466, 341, 11, 293, 286, 519, 309, 311, 5261, 300, 307, 767, 51412], "temperature": 0.0, "avg_logprob": -0.15713937932794744, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.2352050244808197}, {"id": 322, "seek": 115764, "start": 1178.6000000000001, "end": 1180.6000000000001, "text": " quite easily understandable.", "tokens": [51412, 1596, 3612, 25648, 13, 51512], "temperature": 0.0, "avg_logprob": -0.15713937932794744, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.2352050244808197}, {"id": 323, "seek": 115764, "start": 1180.6000000000001, "end": 1184.2, "text": " So let me try to kind of visualize it.", "tokens": [51512, 407, 718, 385, 853, 281, 733, 295, 23273, 309, 13, 51692], "temperature": 0.0, "avg_logprob": -0.15713937932794744, "compression_ratio": 1.6254826254826256, "no_speech_prob": 0.2352050244808197}, {"id": 324, "seek": 118420, "start": 1184.2, "end": 1188.3600000000001, "text": " When you have a real UMLP, your standard universal approximator, it's basically a piecewise", "tokens": [50364, 1133, 291, 362, 257, 957, 624, 12683, 47, 11, 428, 3832, 11455, 8542, 1639, 11, 309, 311, 1936, 257, 2522, 3711, 50572], "temperature": 0.0, "avg_logprob": -0.10641315166766827, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.27183571457862854}, {"id": 325, "seek": 118420, "start": 1188.3600000000001, "end": 1189.8, "text": " linear function.", "tokens": [50572, 8213, 2445, 13, 50644], "temperature": 0.0, "avg_logprob": -0.10641315166766827, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.27183571457862854}, {"id": 326, "seek": 118420, "start": 1189.8, "end": 1194.92, "text": " So as you go far enough away from the training data, you're going to hit that level of extrapolation", "tokens": [50644, 407, 382, 291, 352, 1400, 1547, 1314, 490, 264, 3097, 1412, 11, 291, 434, 516, 281, 2045, 300, 1496, 295, 48224, 399, 50900], "temperature": 0.0, "avg_logprob": -0.10641315166766827, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.27183571457862854}, {"id": 327, "seek": 118420, "start": 1194.92, "end": 1197.68, "text": " where you hit the linear part of the piecewise linear.", "tokens": [50900, 689, 291, 2045, 264, 8213, 644, 295, 264, 2522, 3711, 8213, 13, 51038], "temperature": 0.0, "avg_logprob": -0.10641315166766827, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.27183571457862854}, {"id": 328, "seek": 118420, "start": 1197.68, "end": 1201.8400000000001, "text": " And at that point, if your target function is not linear, no extrapolation is going to", "tokens": [51038, 400, 412, 300, 935, 11, 498, 428, 3779, 2445, 307, 406, 8213, 11, 572, 48224, 399, 307, 516, 281, 51246], "temperature": 0.0, "avg_logprob": -0.10641315166766827, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.27183571457862854}, {"id": 329, "seek": 118420, "start": 1201.8400000000001, "end": 1202.8400000000001, "text": " happen.", "tokens": [51246, 1051, 13, 51296], "temperature": 0.0, "avg_logprob": -0.10641315166766827, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.27183571457862854}, {"id": 330, "seek": 118420, "start": 1202.8400000000001, "end": 1204.64, "text": " You're not going to fit the function properly.", "tokens": [51296, 509, 434, 406, 516, 281, 3318, 264, 2445, 6108, 13, 51386], "temperature": 0.0, "avg_logprob": -0.10641315166766827, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.27183571457862854}, {"id": 331, "seek": 118420, "start": 1204.64, "end": 1208.88, "text": " So what's one outcome of this theory is that if you use real UMLPs, this was a great paper", "tokens": [51386, 407, 437, 311, 472, 9700, 295, 341, 5261, 307, 300, 498, 291, 764, 957, 624, 12683, 23043, 11, 341, 390, 257, 869, 3035, 51598], "temperature": 0.0, "avg_logprob": -0.10641315166766827, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.27183571457862854}, {"id": 332, "seek": 120888, "start": 1208.88, "end": 1214.8000000000002, "text": " from MIT a few years back, which showed that basically you need to line up parts of your", "tokens": [50364, 490, 13100, 257, 1326, 924, 646, 11, 597, 4712, 300, 1936, 291, 643, 281, 1622, 493, 3166, 295, 428, 50660], "temperature": 0.0, "avg_logprob": -0.12396581032696892, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.15198764204978943}, {"id": 333, "seek": 120888, "start": 1214.8000000000002, "end": 1218.44, "text": " neural network such that they learn linear functions in the target.", "tokens": [50660, 18161, 3209, 1270, 300, 436, 1466, 8213, 6828, 294, 264, 3779, 13, 50842], "temperature": 0.0, "avg_logprob": -0.12396581032696892, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.15198764204978943}, {"id": 334, "seek": 120888, "start": 1218.44, "end": 1221.88, "text": " And that's the reason why, say, when you want to imitate a pathfinding algorithm, you want", "tokens": [50842, 400, 300, 311, 264, 1778, 983, 11, 584, 11, 562, 291, 528, 281, 35556, 257, 3100, 69, 9245, 9284, 11, 291, 528, 51014], "temperature": 0.0, "avg_logprob": -0.12396581032696892, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.15198764204978943}, {"id": 335, "seek": 120888, "start": 1221.88, "end": 1226.7600000000002, "text": " to use a max aggregation, your GNN, and not sum, where sum is universal.", "tokens": [51014, 281, 764, 257, 11469, 16743, 399, 11, 428, 46411, 45, 11, 293, 406, 2408, 11, 689, 2408, 307, 11455, 13, 51258], "temperature": 0.0, "avg_logprob": -0.12396581032696892, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.15198764204978943}, {"id": 336, "seek": 120888, "start": 1226.7600000000002, "end": 1228.1200000000001, "text": " It can fit anything.", "tokens": [51258, 467, 393, 3318, 1340, 13, 51326], "temperature": 0.0, "avg_logprob": -0.12396581032696892, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.15198764204978943}, {"id": 337, "seek": 120888, "start": 1228.1200000000001, "end": 1231.96, "text": " But the function you have to learn, because pathfinding is like minimum overall neighbors", "tokens": [51326, 583, 264, 2445, 291, 362, 281, 1466, 11, 570, 3100, 69, 9245, 307, 411, 7285, 4787, 12512, 51518], "temperature": 0.0, "avg_logprob": -0.12396581032696892, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.15198764204978943}, {"id": 338, "seek": 120888, "start": 1231.96, "end": 1236.0800000000002, "text": " of distance to neighbor plus the edge weight, suddenly when you put max in there, it's a", "tokens": [51518, 295, 4560, 281, 5987, 1804, 264, 4691, 3364, 11, 5800, 562, 291, 829, 11469, 294, 456, 11, 309, 311, 257, 51724], "temperature": 0.0, "avg_logprob": -0.12396581032696892, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.15198764204978943}, {"id": 339, "seek": 120888, "start": 1236.0800000000002, "end": 1237.0800000000002, "text": " linear function.", "tokens": [51724, 8213, 2445, 13, 51774], "temperature": 0.0, "avg_logprob": -0.12396581032696892, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.15198764204978943}, {"id": 340, "seek": 123708, "start": 1237.08, "end": 1240.32, "text": " When you put sum in there, it's a highly nonlinear function, so it's going to extrapolate much", "tokens": [50364, 1133, 291, 829, 2408, 294, 456, 11, 309, 311, 257, 5405, 2107, 28263, 2445, 11, 370, 309, 311, 516, 281, 48224, 473, 709, 50526], "temperature": 0.0, "avg_logprob": -0.13963294673610377, "compression_ratio": 1.625, "no_speech_prob": 0.0013667712919414043}, {"id": 341, "seek": 123708, "start": 1240.32, "end": 1241.32, "text": " worse.", "tokens": [50526, 5324, 13, 50576], "temperature": 0.0, "avg_logprob": -0.13963294673610377, "compression_ratio": 1.625, "no_speech_prob": 0.0013667712919414043}, {"id": 342, "seek": 123708, "start": 1241.32, "end": 1245.8, "text": " Now, there's been some great follow-up work on this from Beatrice Bevilacqua, Bruno", "tokens": [50576, 823, 11, 456, 311, 668, 512, 869, 1524, 12, 1010, 589, 322, 341, 490, 16031, 21299, 879, 20202, 326, 34787, 11, 23046, 50800], "temperature": 0.0, "avg_logprob": -0.13963294673610377, "compression_ratio": 1.625, "no_speech_prob": 0.0013667712919414043}, {"id": 343, "seek": 123708, "start": 1245.8, "end": 1247.9199999999998, "text": " Ribeiro from Purdue University.", "tokens": [50800, 33668, 650, 5182, 490, 42506, 3535, 13, 50906], "temperature": 0.0, "avg_logprob": -0.13963294673610377, "compression_ratio": 1.625, "no_speech_prob": 0.0013667712919414043}, {"id": 344, "seek": 123708, "start": 1247.9199999999998, "end": 1252.04, "text": " That was at ICML a few years back, which showed that this idea with, like, you want linear", "tokens": [50906, 663, 390, 412, 14360, 12683, 257, 1326, 924, 646, 11, 597, 4712, 300, 341, 1558, 365, 11, 411, 11, 291, 528, 8213, 51112], "temperature": 0.0, "avg_logprob": -0.13963294673610377, "compression_ratio": 1.625, "no_speech_prob": 0.0013667712919414043}, {"id": 345, "seek": 123708, "start": 1252.04, "end": 1257.1599999999999, "text": " targets with real UMLPs, it's really just a special case of a more general idea that", "tokens": [51112, 12911, 365, 957, 624, 12683, 23043, 11, 309, 311, 534, 445, 257, 2121, 1389, 295, 257, 544, 2674, 1558, 300, 51368], "temperature": 0.0, "avg_logprob": -0.13963294673610377, "compression_ratio": 1.625, "no_speech_prob": 0.0013667712919414043}, {"id": 346, "seek": 123708, "start": 1257.1599999999999, "end": 1262.96, "text": " if you want to extrapolate, say, on different sizes of graphs, you need to have some implicit", "tokens": [51368, 498, 291, 528, 281, 48224, 473, 11, 584, 11, 322, 819, 11602, 295, 24877, 11, 291, 643, 281, 362, 512, 26947, 51658], "temperature": 0.0, "avg_logprob": -0.13963294673610377, "compression_ratio": 1.625, "no_speech_prob": 0.0013667712919414043}, {"id": 347, "seek": 123708, "start": 1262.96, "end": 1266.6399999999999, "text": " causal model of what your test data is going to look like.", "tokens": [51658, 38755, 2316, 295, 437, 428, 1500, 1412, 307, 516, 281, 574, 411, 13, 51842], "temperature": 0.0, "avg_logprob": -0.13963294673610377, "compression_ratio": 1.625, "no_speech_prob": 0.0013667712919414043}, {"id": 348, "seek": 126664, "start": 1266.64, "end": 1270.5600000000002, "text": " This linear algorithmic alignment is just one special case of a causal model like that.", "tokens": [50364, 639, 8213, 9284, 299, 18515, 307, 445, 472, 2121, 1389, 295, 257, 38755, 2316, 411, 300, 13, 50560], "temperature": 0.0, "avg_logprob": -0.19857889646059507, "compression_ratio": 1.7204610951008645, "no_speech_prob": 0.20613937079906464}, {"id": 349, "seek": 126664, "start": 1270.5600000000002, "end": 1275.72, "text": " So basically, if you line things up properly from a causal perspective, you should, in", "tokens": [50560, 407, 1936, 11, 498, 291, 1622, 721, 493, 6108, 490, 257, 38755, 4585, 11, 291, 820, 11, 294, 50818], "temperature": 0.0, "avg_logprob": -0.19857889646059507, "compression_ratio": 1.7204610951008645, "no_speech_prob": 0.20613937079906464}, {"id": 350, "seek": 126664, "start": 1275.72, "end": 1277.2800000000002, "text": " principle, be able to extrapolate.", "tokens": [50818, 8665, 11, 312, 1075, 281, 48224, 473, 13, 50896], "temperature": 0.0, "avg_logprob": -0.19857889646059507, "compression_ratio": 1.7204610951008645, "no_speech_prob": 0.20613937079906464}, {"id": 351, "seek": 126664, "start": 1277.2800000000002, "end": 1281.2, "text": " I mean, we have a clear nonparametric evidence that you can extrapolate is the algorithm", "tokens": [50896, 286, 914, 11, 321, 362, 257, 1850, 2107, 2181, 335, 17475, 4467, 300, 291, 393, 48224, 473, 307, 264, 9284, 51092], "temperature": 0.0, "avg_logprob": -0.19857889646059507, "compression_ratio": 1.7204610951008645, "no_speech_prob": 0.20613937079906464}, {"id": 352, "seek": 126664, "start": 1281.2, "end": 1282.2, "text": " itself, right?", "tokens": [51092, 2564, 11, 558, 30, 51142], "temperature": 0.0, "avg_logprob": -0.19857889646059507, "compression_ratio": 1.7204610951008645, "no_speech_prob": 0.20613937079906464}, {"id": 353, "seek": 126664, "start": 1282.2, "end": 1286.8000000000002, "text": " Now, the key is to find the right sweet spot between full universal approximator MLPs and", "tokens": [51142, 823, 11, 264, 2141, 307, 281, 915, 264, 558, 3844, 4008, 1296, 1577, 11455, 8542, 1639, 21601, 23043, 293, 51372], "temperature": 0.0, "avg_logprob": -0.19857889646059507, "compression_ratio": 1.7204610951008645, "no_speech_prob": 0.20613937079906464}, {"id": 354, "seek": 126664, "start": 1286.8000000000002, "end": 1288.2800000000002, "text": " algorithms on the other side, right?", "tokens": [51372, 14642, 322, 264, 661, 1252, 11, 558, 30, 51446], "temperature": 0.0, "avg_logprob": -0.19857889646059507, "compression_ratio": 1.7204610951008645, "no_speech_prob": 0.20613937079906464}, {"id": 355, "seek": 126664, "start": 1288.2800000000002, "end": 1289.2800000000002, "text": " Interesting.", "tokens": [51446, 14711, 13, 51496], "temperature": 0.0, "avg_logprob": -0.19857889646059507, "compression_ratio": 1.7204610951008645, "no_speech_prob": 0.20613937079906464}, {"id": 356, "seek": 126664, "start": 1289.2800000000002, "end": 1290.2800000000002, "text": " I spoke to Jan the other day.", "tokens": [51496, 286, 7179, 281, 4956, 264, 661, 786, 13, 51546], "temperature": 0.0, "avg_logprob": -0.19857889646059507, "compression_ratio": 1.7204610951008645, "no_speech_prob": 0.20613937079906464}, {"id": 357, "seek": 126664, "start": 1290.2800000000002, "end": 1294.0, "text": " He had a paper a couple of years ago about extrapolation in neural networks, saying they", "tokens": [51546, 634, 632, 257, 3035, 257, 1916, 295, 924, 2057, 466, 48224, 399, 294, 18161, 9590, 11, 1566, 436, 51732], "temperature": 0.0, "avg_logprob": -0.19857889646059507, "compression_ratio": 1.7204610951008645, "no_speech_prob": 0.20613937079906464}, {"id": 358, "seek": 126664, "start": 1294.0, "end": 1295.0, "text": " always extrapolate.", "tokens": [51732, 1009, 48224, 473, 13, 51782], "temperature": 0.0, "avg_logprob": -0.19857889646059507, "compression_ratio": 1.7204610951008645, "no_speech_prob": 0.20613937079906464}, {"id": 359, "seek": 126664, "start": 1295.0, "end": 1296.0, "text": " Yes.", "tokens": [51782, 1079, 13, 51832], "temperature": 0.0, "avg_logprob": -0.19857889646059507, "compression_ratio": 1.7204610951008645, "no_speech_prob": 0.20613937079906464}, {"id": 360, "seek": 129600, "start": 1296.0, "end": 1299.08, "text": " And speaking with Randall Belastriero, and he's got this paper, the Spline Theory of", "tokens": [50364, 400, 4124, 365, 23614, 336, 6248, 525, 470, 2032, 11, 293, 415, 311, 658, 341, 3035, 11, 264, 19788, 533, 29009, 295, 50518], "temperature": 0.0, "avg_logprob": -0.20394733034331222, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.0036623822525143623}, {"id": 361, "seek": 129600, "start": 1299.08, "end": 1303.68, "text": " Neural Networks, which is about, you know, these input sensitive polyhedra in the ambient", "tokens": [50518, 1734, 1807, 12640, 82, 11, 597, 307, 466, 11, 291, 458, 11, 613, 4846, 9477, 6754, 27096, 424, 294, 264, 22997, 50748], "temperature": 0.0, "avg_logprob": -0.20394733034331222, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.0036623822525143623}, {"id": 362, "seek": 129600, "start": 1303.68, "end": 1304.68, "text": " space.", "tokens": [50748, 1901, 13, 50798], "temperature": 0.0, "avg_logprob": -0.20394733034331222, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.0036623822525143623}, {"id": 363, "seek": 129600, "start": 1304.68, "end": 1308.52, "text": " And I always took that to mean why they're quite interpolative and it's just an affine", "tokens": [50798, 400, 286, 1009, 1890, 300, 281, 914, 983, 436, 434, 1596, 44902, 1166, 293, 309, 311, 445, 364, 2096, 533, 50990], "temperature": 0.0, "avg_logprob": -0.20394733034331222, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.0036623822525143623}, {"id": 364, "seek": 129600, "start": 1308.52, "end": 1310.16, "text": " transformation for a single input.", "tokens": [50990, 9887, 337, 257, 2167, 4846, 13, 51072], "temperature": 0.0, "avg_logprob": -0.20394733034331222, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.0036623822525143623}, {"id": 365, "seek": 129600, "start": 1310.16, "end": 1315.08, "text": " But what he's shown, though, is that actually, even an MLP with relus is extremely extrapolative", "tokens": [51072, 583, 437, 415, 311, 4898, 11, 1673, 11, 307, 300, 767, 11, 754, 364, 21601, 47, 365, 1039, 301, 307, 4664, 48224, 1166, 51318], "temperature": 0.0, "avg_logprob": -0.20394733034331222, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.0036623822525143623}, {"id": 366, "seek": 129600, "start": 1315.08, "end": 1320.56, "text": " because you can remove a whole bunch of data and, depending on how you've designed the", "tokens": [51318, 570, 291, 393, 4159, 257, 1379, 3840, 295, 1412, 293, 11, 5413, 322, 577, 291, 600, 4761, 264, 51592], "temperature": 0.0, "avg_logprob": -0.20394733034331222, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.0036623822525143623}, {"id": 367, "seek": 129600, "start": 1320.56, "end": 1324.32, "text": " network architecture, it will still inform that region that you've taken away.", "tokens": [51592, 3209, 9482, 11, 309, 486, 920, 1356, 300, 4458, 300, 291, 600, 2726, 1314, 13, 51780], "temperature": 0.0, "avg_logprob": -0.20394733034331222, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.0036623822525143623}, {"id": 368, "seek": 132432, "start": 1324.32, "end": 1328.08, "text": " So, I mean, are you familiar with the Spline Theory and do you think it's a useful framework?", "tokens": [50364, 407, 11, 286, 914, 11, 366, 291, 4963, 365, 264, 19788, 533, 29009, 293, 360, 291, 519, 309, 311, 257, 4420, 8388, 30, 50552], "temperature": 0.0, "avg_logprob": -0.16495359860933745, "compression_ratio": 1.7927927927927927, "no_speech_prob": 0.1344309002161026}, {"id": 369, "seek": 132432, "start": 1328.08, "end": 1329.08, "text": " Yes.", "tokens": [50552, 1079, 13, 50602], "temperature": 0.0, "avg_logprob": -0.16495359860933745, "compression_ratio": 1.7927927927927927, "no_speech_prob": 0.1344309002161026}, {"id": 370, "seek": 132432, "start": 1329.08, "end": 1333.32, "text": " So, one thing I would say, the way I understand Jan's paper, it could be that I missed some", "tokens": [50602, 407, 11, 472, 551, 286, 576, 584, 11, 264, 636, 286, 1223, 4956, 311, 3035, 11, 309, 727, 312, 300, 286, 6721, 512, 50814], "temperature": 0.0, "avg_logprob": -0.16495359860933745, "compression_ratio": 1.7927927927927927, "no_speech_prob": 0.1344309002161026}, {"id": 371, "seek": 132432, "start": 1333.32, "end": 1337.04, "text": " detail, but the way I understand it is that here we're talking about interpolation and", "tokens": [50814, 2607, 11, 457, 264, 636, 286, 1223, 309, 307, 300, 510, 321, 434, 1417, 466, 44902, 399, 293, 51000], "temperature": 0.0, "avg_logprob": -0.16495359860933745, "compression_ratio": 1.7927927927927927, "no_speech_prob": 0.1344309002161026}, {"id": 372, "seek": 132432, "start": 1337.04, "end": 1339.6, "text": " extrapolation with respect to the geometry of the data.", "tokens": [51000, 48224, 399, 365, 3104, 281, 264, 18426, 295, 264, 1412, 13, 51128], "temperature": 0.0, "avg_logprob": -0.16495359860933745, "compression_ratio": 1.7927927927927927, "no_speech_prob": 0.1344309002161026}, {"id": 373, "seek": 132432, "start": 1339.6, "end": 1343.84, "text": " So, like, you take, say, the convex hull of all the training points and then, yeah, it's", "tokens": [51128, 407, 11, 411, 11, 291, 747, 11, 584, 11, 264, 42432, 32335, 295, 439, 264, 3097, 2793, 293, 550, 11, 1338, 11, 309, 311, 51340], "temperature": 0.0, "avg_logprob": -0.16495359860933745, "compression_ratio": 1.7927927927927927, "no_speech_prob": 0.1344309002161026}, {"id": 374, "seek": 132432, "start": 1343.84, "end": 1347.76, "text": " very common, especially in these high dimensional image spaces, right?", "tokens": [51340, 588, 2689, 11, 2318, 294, 613, 1090, 18795, 3256, 7673, 11, 558, 30, 51536], "temperature": 0.0, "avg_logprob": -0.16495359860933745, "compression_ratio": 1.7927927927927927, "no_speech_prob": 0.1344309002161026}, {"id": 375, "seek": 132432, "start": 1347.76, "end": 1351.84, "text": " It's very easy to push one dimension sufficiently to escape the convex hull of what you've seen", "tokens": [51536, 467, 311, 588, 1858, 281, 2944, 472, 10139, 31868, 281, 7615, 264, 42432, 32335, 295, 437, 291, 600, 1612, 51740], "temperature": 0.0, "avg_logprob": -0.16495359860933745, "compression_ratio": 1.7927927927927927, "no_speech_prob": 0.1344309002161026}, {"id": 376, "seek": 132432, "start": 1351.84, "end": 1352.84, "text": " so far.", "tokens": [51740, 370, 1400, 13, 51790], "temperature": 0.0, "avg_logprob": -0.16495359860933745, "compression_ratio": 1.7927927927927927, "no_speech_prob": 0.1344309002161026}, {"id": 377, "seek": 135284, "start": 1352.84, "end": 1356.72, "text": " So, I guess when I say extrapolation out of distribution, I'm actually maybe thinking", "tokens": [50364, 407, 11, 286, 2041, 562, 286, 584, 48224, 399, 484, 295, 7316, 11, 286, 478, 767, 1310, 1953, 50558], "temperature": 0.0, "avg_logprob": -0.09768276921025029, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0014547455357387662}, {"id": 378, "seek": 135284, "start": 1356.72, "end": 1361.6399999999999, "text": " of a more probabilistic argument, so something like if you think of the probability distribution", "tokens": [50558, 295, 257, 544, 31959, 3142, 6770, 11, 370, 746, 411, 498, 291, 519, 295, 264, 8482, 7316, 50804], "temperature": 0.0, "avg_logprob": -0.09768276921025029, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0014547455357387662}, {"id": 379, "seek": 135284, "start": 1361.6399999999999, "end": 1365.48, "text": " induced by the training set, which obviously allows you to extrapolate away from the convex", "tokens": [50804, 33991, 538, 264, 3097, 992, 11, 597, 2745, 4045, 291, 281, 48224, 473, 1314, 490, 264, 42432, 50996], "temperature": 0.0, "avg_logprob": -0.09768276921025029, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0014547455357387662}, {"id": 380, "seek": 135284, "start": 1365.48, "end": 1366.6, "text": " hull, right?", "tokens": [50996, 32335, 11, 558, 30, 51052], "temperature": 0.0, "avg_logprob": -0.09768276921025029, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0014547455357387662}, {"id": 381, "seek": 135284, "start": 1366.6, "end": 1370.9199999999998, "text": " But if you go sufficiently far from the modes of that distribution, so you explore a part", "tokens": [51052, 583, 498, 291, 352, 31868, 1400, 490, 264, 14068, 295, 300, 7316, 11, 370, 291, 6839, 257, 644, 51268], "temperature": 0.0, "avg_logprob": -0.09768276921025029, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0014547455357387662}, {"id": 382, "seek": 135284, "start": 1370.9199999999998, "end": 1375.9599999999998, "text": " of the space that hasn't really been covered, you know, from a probabilistic mass point", "tokens": [51268, 295, 264, 1901, 300, 6132, 380, 534, 668, 5343, 11, 291, 458, 11, 490, 257, 31959, 3142, 2758, 935, 51520], "temperature": 0.0, "avg_logprob": -0.09768276921025029, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0014547455357387662}, {"id": 383, "seek": 135284, "start": 1375.9599999999998, "end": 1380.4399999999998, "text": " of view in the training data, that is what we're actually thinking of when we say out", "tokens": [51520, 295, 1910, 294, 264, 3097, 1412, 11, 300, 307, 437, 321, 434, 767, 1953, 295, 562, 321, 584, 484, 51744], "temperature": 0.0, "avg_logprob": -0.09768276921025029, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0014547455357387662}, {"id": 384, "seek": 135284, "start": 1380.4399999999998, "end": 1381.4399999999998, "text": " of distribution generalization.", "tokens": [51744, 295, 7316, 2674, 2144, 13, 51794], "temperature": 0.0, "avg_logprob": -0.09768276921025029, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0014547455357387662}, {"id": 385, "seek": 138144, "start": 1381.44, "end": 1385.8400000000001, "text": " But, yeah, I fully agree with you, like, in terms of just convex hull arguments, we very", "tokens": [50364, 583, 11, 1338, 11, 286, 4498, 3986, 365, 291, 11, 411, 11, 294, 2115, 295, 445, 42432, 32335, 12869, 11, 321, 588, 50584], "temperature": 0.0, "avg_logprob": -0.14972544825354286, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.013012800365686417}, {"id": 386, "seek": 138144, "start": 1385.8400000000001, "end": 1389.3600000000001, "text": " often ask these regular MOPs to go beyond the convex hull, and they seem to work quite", "tokens": [50584, 2049, 1029, 613, 3890, 376, 12059, 82, 281, 352, 4399, 264, 42432, 32335, 11, 293, 436, 1643, 281, 589, 1596, 50760], "temperature": 0.0, "avg_logprob": -0.14972544825354286, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.013012800365686417}, {"id": 387, "seek": 138144, "start": 1389.3600000000001, "end": 1391.28, "text": " well in those regimes.", "tokens": [50760, 731, 294, 729, 45738, 13, 50856], "temperature": 0.0, "avg_logprob": -0.14972544825354286, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.013012800365686417}, {"id": 388, "seek": 138144, "start": 1391.28, "end": 1395.92, "text": " But here, I'm talking really about going, like, significantly beyond the convex hull", "tokens": [50856, 583, 510, 11, 286, 478, 1417, 534, 466, 516, 11, 411, 11, 10591, 4399, 264, 42432, 32335, 51088], "temperature": 0.0, "avg_logprob": -0.14972544825354286, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.013012800365686417}, {"id": 389, "seek": 138144, "start": 1395.92, "end": 1399.16, "text": " to, like, some region that really wasn't touched.", "tokens": [51088, 281, 11, 411, 11, 512, 4458, 300, 534, 2067, 380, 9828, 13, 51250], "temperature": 0.0, "avg_logprob": -0.14972544825354286, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.013012800365686417}, {"id": 390, "seek": 138144, "start": 1399.16, "end": 1403.96, "text": " And what we do, for example, in our papers is we train on 16 node graphs to execute these", "tokens": [51250, 400, 437, 321, 360, 11, 337, 1365, 11, 294, 527, 10577, 307, 321, 3847, 322, 3165, 9984, 24877, 281, 14483, 613, 51490], "temperature": 0.0, "avg_logprob": -0.14972544825354286, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.013012800365686417}, {"id": 391, "seek": 138144, "start": 1403.96, "end": 1408.24, "text": " algorithms, and then we test it on four times larger, 64 node graphs.", "tokens": [51490, 14642, 11, 293, 550, 321, 1500, 309, 322, 1451, 1413, 4833, 11, 12145, 9984, 24877, 13, 51704], "temperature": 0.0, "avg_logprob": -0.14972544825354286, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.013012800365686417}, {"id": 392, "seek": 140824, "start": 1408.24, "end": 1411.92, "text": " And what this means, because an algorithm might have, say, n-cubed time complexity, it means", "tokens": [50364, 400, 437, 341, 1355, 11, 570, 364, 9284, 1062, 362, 11, 584, 11, 297, 12, 66, 836, 292, 565, 14024, 11, 309, 1355, 50548], "temperature": 0.0, "avg_logprob": -0.18884345376567477, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.022394724190235138}, {"id": 393, "seek": 140824, "start": 1411.92, "end": 1415.56, "text": " the trajectory over which you have to roll it out is also much, much longer than what", "tokens": [50548, 264, 21512, 670, 597, 291, 362, 281, 3373, 309, 484, 307, 611, 709, 11, 709, 2854, 813, 437, 50730], "temperature": 0.0, "avg_logprob": -0.18884345376567477, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.022394724190235138}, {"id": 394, "seek": 140824, "start": 1415.56, "end": 1416.68, "text": " you've seen in training time.", "tokens": [50730, 291, 600, 1612, 294, 3097, 565, 13, 50786], "temperature": 0.0, "avg_logprob": -0.18884345376567477, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.022394724190235138}, {"id": 395, "seek": 140824, "start": 1416.68, "end": 1421.24, "text": " So it's really a test of, like, very different conditions than what you've seen in training", "tokens": [50786, 407, 309, 311, 534, 257, 1500, 295, 11, 411, 11, 588, 819, 4487, 813, 437, 291, 600, 1612, 294, 3097, 51014], "temperature": 0.0, "avg_logprob": -0.18884345376567477, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.022394724190235138}, {"id": 396, "seek": 140824, "start": 1421.24, "end": 1422.24, "text": " time, right?", "tokens": [51014, 565, 11, 558, 30, 51064], "temperature": 0.0, "avg_logprob": -0.18884345376567477, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.022394724190235138}, {"id": 397, "seek": 140824, "start": 1422.24, "end": 1423.24, "text": " That's interesting.", "tokens": [51064, 663, 311, 1880, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18884345376567477, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.022394724190235138}, {"id": 398, "seek": 140824, "start": 1423.24, "end": 1425.52, "text": " And first of all, I completely agree with you that this binary convex hull notion of", "tokens": [51114, 400, 700, 295, 439, 11, 286, 2584, 3986, 365, 291, 300, 341, 17434, 42432, 32335, 10710, 295, 51228], "temperature": 0.0, "avg_logprob": -0.18884345376567477, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.022394724190235138}, {"id": 399, "seek": 140824, "start": 1425.52, "end": 1428.72, "text": " extrapolation probably isn't particularly useful.", "tokens": [51228, 48224, 399, 1391, 1943, 380, 4098, 4420, 13, 51388], "temperature": 0.0, "avg_logprob": -0.18884345376567477, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.022394724190235138}, {"id": 400, "seek": 140824, "start": 1428.72, "end": 1433.32, "text": " But, you know, folks like Francois Relais describe the way Neuron Network's work is kind of bending", "tokens": [51388, 583, 11, 291, 458, 11, 4024, 411, 34695, 271, 8738, 1527, 6786, 264, 636, 1734, 374, 266, 12640, 311, 589, 307, 733, 295, 22487, 51618], "temperature": 0.0, "avg_logprob": -0.18884345376567477, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.022394724190235138}, {"id": 401, "seek": 140824, "start": 1433.32, "end": 1435.96, "text": " the space, you know, progressively with layers.", "tokens": [51618, 264, 1901, 11, 291, 458, 11, 46667, 365, 7914, 13, 51750], "temperature": 0.0, "avg_logprob": -0.18884345376567477, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.022394724190235138}, {"id": 402, "seek": 143596, "start": 1436.08, "end": 1439.6000000000001, "text": " I really like this polyhedra idea.", "tokens": [50370, 286, 534, 411, 341, 6754, 27096, 424, 1558, 13, 50546], "temperature": 0.0, "avg_logprob": -0.2526268805226972, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.06228218600153923}, {"id": 403, "seek": 143596, "start": 1439.6000000000001, "end": 1443.6000000000001, "text": " Contrast the algorithmic reasoning with GNNs, so, I mean, I spoke with Hattie from a Google", "tokens": [50546, 4839, 4148, 264, 9284, 299, 21577, 365, 46411, 45, 82, 11, 370, 11, 286, 914, 11, 286, 7179, 365, 389, 1591, 414, 490, 257, 3329, 50746], "temperature": 0.0, "avg_logprob": -0.2526268805226972, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.06228218600153923}, {"id": 404, "seek": 143596, "start": 1443.6000000000001, "end": 1448.28, "text": " brain team the other day, she's doing the in-consex prompting, you know, sort of algorithm", "tokens": [50746, 3567, 1469, 264, 661, 786, 11, 750, 311, 884, 264, 294, 12, 1671, 405, 87, 12391, 278, 11, 291, 458, 11, 1333, 295, 9284, 50980], "temperature": 0.0, "avg_logprob": -0.2526268805226972, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.06228218600153923}, {"id": 405, "seek": 143596, "start": 1448.28, "end": 1449.28, "text": " learning.", "tokens": [50980, 2539, 13, 51030], "temperature": 0.0, "avg_logprob": -0.2526268805226972, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.06228218600153923}, {"id": 406, "seek": 143596, "start": 1449.28, "end": 1451.72, "text": " How would you contrast those two approaches?", "tokens": [51030, 1012, 576, 291, 8712, 729, 732, 11587, 30, 51152], "temperature": 0.0, "avg_logprob": -0.2526268805226972, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.06228218600153923}, {"id": 407, "seek": 143596, "start": 1451.72, "end": 1457.52, "text": " So basically, I would really like these approaches to be reconciled going forward in the sense", "tokens": [51152, 407, 1936, 11, 286, 576, 534, 411, 613, 11587, 281, 312, 9993, 3208, 292, 516, 2128, 294, 264, 2020, 51442], "temperature": 0.0, "avg_logprob": -0.2526268805226972, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.06228218600153923}, {"id": 408, "seek": 143596, "start": 1457.52, "end": 1462.52, "text": " that, like, I don't see them as going one without the other, if that makes sense.", "tokens": [51442, 300, 11, 411, 11, 286, 500, 380, 536, 552, 382, 516, 472, 1553, 264, 661, 11, 498, 300, 1669, 2020, 13, 51692], "temperature": 0.0, "avg_logprob": -0.2526268805226972, "compression_ratio": 1.6943396226415095, "no_speech_prob": 0.06228218600153923}, {"id": 409, "seek": 146252, "start": 1462.52, "end": 1468.72, "text": " So on one side, and I'm going to invoke the same principles I mentioned during our MLST", "tokens": [50364, 407, 322, 472, 1252, 11, 293, 286, 478, 516, 281, 41117, 264, 912, 9156, 286, 2835, 1830, 527, 376, 43, 6840, 50674], "temperature": 0.0, "avg_logprob": -0.13718458632348288, "compression_ratio": 1.6714697406340058, "no_speech_prob": 0.022961826995015144}, {"id": 410, "seek": 146252, "start": 1468.72, "end": 1472.4, "text": " episode, you know, Daniel Kahneman's book, System 1 and System 2, right?", "tokens": [50674, 3500, 11, 291, 458, 11, 8033, 591, 12140, 15023, 311, 1446, 11, 8910, 502, 293, 8910, 568, 11, 558, 30, 50858], "temperature": 0.0, "avg_logprob": -0.13718458632348288, "compression_ratio": 1.6714697406340058, "no_speech_prob": 0.022961826995015144}, {"id": 411, "seek": 146252, "start": 1472.4, "end": 1474.08, "text": " I think you cannot have one without the other.", "tokens": [50858, 286, 519, 291, 2644, 362, 472, 1553, 264, 661, 13, 50942], "temperature": 0.0, "avg_logprob": -0.13718458632348288, "compression_ratio": 1.6714697406340058, "no_speech_prob": 0.022961826995015144}, {"id": 412, "seek": 146252, "start": 1474.08, "end": 1478.68, "text": " So you have these amazing large-scale perceptive models that are really amazing at, you know,", "tokens": [50942, 407, 291, 362, 613, 2243, 2416, 12, 20033, 43276, 488, 5245, 300, 366, 534, 2243, 412, 11, 291, 458, 11, 51172], "temperature": 0.0, "avg_logprob": -0.13718458632348288, "compression_ratio": 1.6714697406340058, "no_speech_prob": 0.022961826995015144}, {"id": 413, "seek": 146252, "start": 1478.68, "end": 1482.8799999999999, "text": " taking the complexities of the real world and somehow getting interpretable enough concepts", "tokens": [51172, 1940, 264, 48705, 295, 264, 957, 1002, 293, 6063, 1242, 7302, 712, 1547, 10392, 51382], "temperature": 0.0, "avg_logprob": -0.13718458632348288, "compression_ratio": 1.6714697406340058, "no_speech_prob": 0.022961826995015144}, {"id": 414, "seek": 146252, "start": 1482.8799999999999, "end": 1486.76, "text": " out of there that they can, you know, make sense of what's going on and, like, drive many", "tokens": [51382, 484, 295, 456, 300, 436, 393, 11, 291, 458, 11, 652, 2020, 295, 437, 311, 516, 322, 293, 11, 411, 11, 3332, 867, 51576], "temperature": 0.0, "avg_logprob": -0.13718458632348288, "compression_ratio": 1.6714697406340058, "no_speech_prob": 0.022961826995015144}, {"id": 415, "seek": 146252, "start": 1486.76, "end": 1491.44, "text": " interesting real-world decision-making problems, although they might lose a little bit on having", "tokens": [51576, 1880, 957, 12, 13217, 3537, 12, 12402, 2740, 11, 4878, 436, 1062, 3624, 257, 707, 857, 322, 1419, 51810], "temperature": 0.0, "avg_logprob": -0.13718458632348288, "compression_ratio": 1.6714697406340058, "no_speech_prob": 0.022961826995015144}, {"id": 416, "seek": 149144, "start": 1491.48, "end": 1496.0, "text": " to do something like what an AI scientist would be expected to do, which is, like, extrapolate", "tokens": [50366, 281, 360, 746, 411, 437, 364, 7318, 12662, 576, 312, 5176, 281, 360, 11, 597, 307, 11, 411, 11, 48224, 473, 50592], "temperature": 0.0, "avg_logprob": -0.1112824327805463, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.250450998544693}, {"id": 417, "seek": 149144, "start": 1496.0, "end": 1498.52, "text": " and generate new concepts out of what they've seen.", "tokens": [50592, 293, 8460, 777, 10392, 484, 295, 437, 436, 600, 1612, 13, 50718], "temperature": 0.0, "avg_logprob": -0.1112824327805463, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.250450998544693}, {"id": 418, "seek": 149144, "start": 1498.52, "end": 1503.64, "text": " And as you said, these kinds of specifically tailored prompts might enable the model to", "tokens": [50718, 400, 382, 291, 848, 11, 613, 3685, 295, 4682, 34858, 41095, 1062, 9528, 264, 2316, 281, 50974], "temperature": 0.0, "avg_logprob": -0.1112824327805463, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.250450998544693}, {"id": 419, "seek": 149144, "start": 1503.64, "end": 1508.8, "text": " take things a step or two further, but it's always, like, it's kind of, in spirit, it's", "tokens": [50974, 747, 721, 257, 1823, 420, 732, 3052, 11, 457, 309, 311, 1009, 11, 411, 11, 309, 311, 733, 295, 11, 294, 3797, 11, 309, 311, 51232], "temperature": 0.0, "avg_logprob": -0.1112824327805463, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.250450998544693}, {"id": 420, "seek": 149144, "start": 1508.8, "end": 1512.92, "text": " the same thing as algorithmic reasoning, because we teach a model to execute an algorithm by", "tokens": [51232, 264, 912, 551, 382, 9284, 299, 21577, 11, 570, 321, 2924, 257, 2316, 281, 14483, 364, 9284, 538, 51438], "temperature": 0.0, "avg_logprob": -0.1112824327805463, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.250450998544693}, {"id": 421, "seek": 149144, "start": 1512.92, "end": 1515.48, "text": " forcing it to imitate the algorithm step by step.", "tokens": [51438, 19030, 309, 281, 35556, 264, 9284, 1823, 538, 1823, 13, 51566], "temperature": 0.0, "avg_logprob": -0.1112824327805463, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.250450998544693}, {"id": 422, "seek": 149144, "start": 1515.48, "end": 1519.72, "text": " Here you prompt a language model by telling it what are some of the steps, like, just", "tokens": [51566, 1692, 291, 12391, 257, 2856, 2316, 538, 3585, 309, 437, 366, 512, 295, 264, 4439, 11, 411, 11, 445, 51778], "temperature": 0.0, "avg_logprob": -0.1112824327805463, "compression_ratio": 1.7889610389610389, "no_speech_prob": 0.250450998544693}, {"id": 423, "seek": 151972, "start": 1519.76, "end": 1523.28, "text": " like you're trying to teach a student how to solve a homework, right, telling them the", "tokens": [50366, 411, 291, 434, 1382, 281, 2924, 257, 3107, 577, 281, 5039, 257, 14578, 11, 558, 11, 3585, 552, 264, 50542], "temperature": 0.0, "avg_logprob": -0.11738453044757977, "compression_ratio": 1.7311178247734138, "no_speech_prob": 0.008982311002910137}, {"id": 424, "seek": 151972, "start": 1523.28, "end": 1526.64, "text": " individual steps they need to do, and then letting the language model go off on its own", "tokens": [50542, 2609, 4439, 436, 643, 281, 360, 11, 293, 550, 8295, 264, 2856, 2316, 352, 766, 322, 1080, 1065, 50710], "temperature": 0.0, "avg_logprob": -0.11738453044757977, "compression_ratio": 1.7311178247734138, "no_speech_prob": 0.008982311002910137}, {"id": 425, "seek": 151972, "start": 1526.64, "end": 1527.64, "text": " to solve it.", "tokens": [50710, 281, 5039, 309, 13, 50760], "temperature": 0.0, "avg_logprob": -0.11738453044757977, "compression_ratio": 1.7311178247734138, "no_speech_prob": 0.008982311002910137}, {"id": 426, "seek": 151972, "start": 1527.64, "end": 1532.04, "text": " But where I see the real future of these two methods converging is you're going to have", "tokens": [50760, 583, 689, 286, 536, 264, 957, 2027, 295, 613, 732, 7150, 9652, 3249, 307, 291, 434, 516, 281, 362, 50980], "temperature": 0.0, "avg_logprob": -0.11738453044757977, "compression_ratio": 1.7311178247734138, "no_speech_prob": 0.008982311002910137}, {"id": 427, "seek": 151972, "start": 1532.04, "end": 1536.56, "text": " your system one component that gets your concepts out very nicely, cleanly.", "tokens": [50980, 428, 1185, 472, 6542, 300, 2170, 428, 10392, 484, 588, 9594, 11, 2541, 356, 13, 51206], "temperature": 0.0, "avg_logprob": -0.11738453044757977, "compression_ratio": 1.7311178247734138, "no_speech_prob": 0.008982311002910137}, {"id": 428, "seek": 151972, "start": 1536.56, "end": 1540.04, "text": " And then those concepts, because we're working with transformers nowadays anyway most of", "tokens": [51206, 400, 550, 729, 10392, 11, 570, 321, 434, 1364, 365, 4088, 433, 13434, 4033, 881, 295, 51380], "temperature": 0.0, "avg_logprob": -0.11738453044757977, "compression_ratio": 1.7311178247734138, "no_speech_prob": 0.008982311002910137}, {"id": 429, "seek": 151972, "start": 1540.04, "end": 1542.44, "text": " the time, are going to be very slot-based.", "tokens": [51380, 264, 565, 11, 366, 516, 281, 312, 588, 14747, 12, 6032, 13, 51500], "temperature": 0.0, "avg_logprob": -0.11738453044757977, "compression_ratio": 1.7311178247734138, "no_speech_prob": 0.008982311002910137}, {"id": 430, "seek": 151972, "start": 1542.44, "end": 1546.64, "text": " So that plays very nicely with GNNs, which expect nodes as input, right, so you can maybe", "tokens": [51500, 407, 300, 5749, 588, 9594, 365, 46411, 45, 82, 11, 597, 2066, 13891, 382, 4846, 11, 558, 11, 370, 291, 393, 1310, 51710], "temperature": 0.0, "avg_logprob": -0.11738453044757977, "compression_ratio": 1.7311178247734138, "no_speech_prob": 0.008982311002910137}, {"id": 431, "seek": 154664, "start": 1546.64, "end": 1551.5200000000002, "text": " hook up in some nice way those concepts into a graph neural network that was trained to", "tokens": [50364, 6328, 493, 294, 512, 1481, 636, 729, 10392, 666, 257, 4295, 18161, 3209, 300, 390, 8895, 281, 50608], "temperature": 0.0, "avg_logprob": -0.0993350346883138, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.0038830724079161882}, {"id": 432, "seek": 154664, "start": 1551.5200000000002, "end": 1555.68, "text": " execute a bunch of algorithms, and then, you know, kind of get the best of both worlds.", "tokens": [50608, 14483, 257, 3840, 295, 14642, 11, 293, 550, 11, 291, 458, 11, 733, 295, 483, 264, 1151, 295, 1293, 13401, 13, 50816], "temperature": 0.0, "avg_logprob": -0.0993350346883138, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.0038830724079161882}, {"id": 433, "seek": 154664, "start": 1555.68, "end": 1560.3200000000002, "text": " So have your perceptual component do the perception, and maybe prompt it as well to kind of do", "tokens": [50816, 407, 362, 428, 43276, 901, 6542, 360, 264, 12860, 11, 293, 1310, 12391, 309, 382, 731, 281, 733, 295, 360, 51048], "temperature": 0.0, "avg_logprob": -0.0993350346883138, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.0038830724079161882}, {"id": 434, "seek": 154664, "start": 1560.3200000000002, "end": 1564.96, "text": " it in a particularly step-by-step manner, and then further have a robust component that", "tokens": [51048, 309, 294, 257, 4098, 1823, 12, 2322, 12, 16792, 9060, 11, 293, 550, 3052, 362, 257, 13956, 6542, 300, 51280], "temperature": 0.0, "avg_logprob": -0.0993350346883138, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.0038830724079161882}, {"id": 435, "seek": 154664, "start": 1564.96, "end": 1568.88, "text": " makes you not have to relearn all those things that neural networks we know theoretically", "tokens": [51280, 1669, 291, 406, 362, 281, 2951, 1083, 439, 729, 721, 300, 18161, 9590, 321, 458, 29400, 51476], "temperature": 0.0, "avg_logprob": -0.0993350346883138, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.0038830724079161882}, {"id": 436, "seek": 154664, "start": 1568.88, "end": 1572.4, "text": " cannot learn to do that well because of these extrapolation arguments.", "tokens": [51476, 2644, 1466, 281, 360, 300, 731, 570, 295, 613, 48224, 399, 12869, 13, 51652], "temperature": 0.0, "avg_logprob": -0.0993350346883138, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.0038830724079161882}, {"id": 437, "seek": 154664, "start": 1572.4, "end": 1575.2, "text": " Maybe one last point I would make to kind of cement this.", "tokens": [51652, 2704, 472, 1036, 935, 286, 576, 652, 281, 733, 295, 19729, 341, 13, 51792], "temperature": 0.0, "avg_logprob": -0.0993350346883138, "compression_ratio": 1.7753846153846153, "no_speech_prob": 0.0038830724079161882}, {"id": 438, "seek": 157520, "start": 1575.2, "end": 1578.88, "text": " If you've been around the archive recently, you might have seen our paper on a generalist", "tokens": [50364, 759, 291, 600, 668, 926, 264, 23507, 3938, 11, 291, 1062, 362, 1612, 527, 3035, 322, 257, 2674, 468, 50548], "temperature": 0.0, "avg_logprob": -0.12099480431927137, "compression_ratio": 1.6528662420382165, "no_speech_prob": 0.008060443215072155}, {"id": 439, "seek": 157520, "start": 1578.88, "end": 1585.0800000000002, "text": " neural algorithmic learner where we have actually used GATO-style ideas to train one graph neural", "tokens": [50548, 18161, 9284, 299, 33347, 689, 321, 362, 767, 1143, 460, 2218, 46, 12, 15014, 3487, 281, 3847, 472, 4295, 18161, 50858], "temperature": 0.0, "avg_logprob": -0.12099480431927137, "compression_ratio": 1.6528662420382165, "no_speech_prob": 0.008060443215072155}, {"id": 440, "seek": 157520, "start": 1585.0800000000002, "end": 1589.8, "text": " network that can execute 30 very diverse algorithms all in the same architecture with a single", "tokens": [50858, 3209, 300, 393, 14483, 2217, 588, 9521, 14642, 439, 294, 264, 912, 9482, 365, 257, 2167, 51094], "temperature": 0.0, "avg_logprob": -0.12099480431927137, "compression_ratio": 1.6528662420382165, "no_speech_prob": 0.008060443215072155}, {"id": 441, "seek": 157520, "start": 1589.8, "end": 1594.48, "text": " set of weights, so sorting, searching, pathfinding, dynamic programming, comics, hauls, all those", "tokens": [51094, 992, 295, 17443, 11, 370, 32411, 11, 10808, 11, 3100, 69, 9245, 11, 8546, 9410, 11, 18756, 11, 324, 9468, 11, 439, 729, 51328], "temperature": 0.0, "avg_logprob": -0.12099480431927137, "compression_ratio": 1.6528662420382165, "no_speech_prob": 0.008060443215072155}, {"id": 442, "seek": 157520, "start": 1594.48, "end": 1597.24, "text": " kinds of nice things, very diverse ways of reasoning.", "tokens": [51328, 3685, 295, 1481, 721, 11, 588, 9521, 2098, 295, 21577, 13, 51466], "temperature": 0.0, "avg_logprob": -0.12099480431927137, "compression_ratio": 1.6528662420382165, "no_speech_prob": 0.008060443215072155}, {"id": 443, "seek": 157520, "start": 1597.24, "end": 1601.1200000000001, "text": " We believe something like that could maybe be a basis of, say, a foundation model of", "tokens": [51466, 492, 1697, 746, 411, 300, 727, 1310, 312, 257, 5143, 295, 11, 584, 11, 257, 7030, 2316, 295, 51660], "temperature": 0.0, "avg_logprob": -0.12099480431927137, "compression_ratio": 1.6528662420382165, "no_speech_prob": 0.008060443215072155}, {"id": 444, "seek": 160112, "start": 1601.12, "end": 1605.1599999999999, "text": " reasoning in the future that could nicely hook up to the foundation models we already", "tokens": [50364, 21577, 294, 264, 2027, 300, 727, 9594, 6328, 493, 281, 264, 7030, 5245, 321, 1217, 50566], "temperature": 0.0, "avg_logprob": -0.14626317784406137, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.030169140547513962}, {"id": 445, "seek": 160112, "start": 1605.1599999999999, "end": 1607.28, "text": " know and love in the realm of perception.", "tokens": [50566, 458, 293, 959, 294, 264, 15355, 295, 12860, 13, 50672], "temperature": 0.0, "avg_logprob": -0.14626317784406137, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.030169140547513962}, {"id": 446, "seek": 160112, "start": 1607.28, "end": 1608.28, "text": " Amazing.", "tokens": [50672, 14165, 13, 50722], "temperature": 0.0, "avg_logprob": -0.14626317784406137, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.030169140547513962}, {"id": 447, "seek": 160112, "start": 1608.28, "end": 1610.76, "text": " And what's the biggest research challenge for you next year?", "tokens": [50722, 400, 437, 311, 264, 3880, 2132, 3430, 337, 291, 958, 1064, 30, 50846], "temperature": 0.0, "avg_logprob": -0.14626317784406137, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.030169140547513962}, {"id": 448, "seek": 160112, "start": 1610.76, "end": 1616.12, "text": " So next year, I would really like to show to what extent these things can scale in the", "tokens": [50846, 407, 958, 1064, 11, 286, 576, 534, 411, 281, 855, 281, 437, 8396, 613, 721, 393, 4373, 294, 264, 51114], "temperature": 0.0, "avg_logprob": -0.14626317784406137, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.030169140547513962}, {"id": 449, "seek": 160112, "start": 1616.12, "end": 1617.12, "text": " real world.", "tokens": [51114, 957, 1002, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14626317784406137, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.030169140547513962}, {"id": 450, "seek": 160112, "start": 1617.12, "end": 1622.0, "text": " So we already have several isolated papers that showed that these ideas can work on", "tokens": [51164, 407, 321, 1217, 362, 2940, 14621, 10577, 300, 4712, 300, 613, 3487, 393, 589, 322, 51408], "temperature": 0.0, "avg_logprob": -0.14626317784406137, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.030169140547513962}, {"id": 451, "seek": 160112, "start": 1622.0, "end": 1623.0, "text": " real problems.", "tokens": [51408, 957, 2740, 13, 51458], "temperature": 0.0, "avg_logprob": -0.14626317784406137, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.030169140547513962}, {"id": 452, "seek": 160112, "start": 1623.0, "end": 1625.6599999999999, "text": " We have Excelvin where we applied it to reinforcement learning.", "tokens": [51458, 492, 362, 19060, 4796, 689, 321, 6456, 309, 281, 29280, 2539, 13, 51591], "temperature": 0.0, "avg_logprob": -0.14626317784406137, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.030169140547513962}, {"id": 453, "seek": 160112, "start": 1625.6599999999999, "end": 1627.56, "text": " That was in Europe Spotlight last year.", "tokens": [51591, 663, 390, 294, 3315, 19102, 2764, 1036, 1064, 13, 51686], "temperature": 0.0, "avg_logprob": -0.14626317784406137, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.030169140547513962}, {"id": 454, "seek": 160112, "start": 1627.56, "end": 1630.8, "text": " We have RMR where we applied it to self-supervision problems.", "tokens": [51686, 492, 362, 497, 21173, 689, 321, 6456, 309, 281, 2698, 12, 48172, 6763, 2740, 13, 51848], "temperature": 0.0, "avg_logprob": -0.14626317784406137, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.030169140547513962}, {"id": 455, "seek": 163080, "start": 1630.8, "end": 1635.76, "text": " We also have one paper currently under review at iClear where we successfully applied to", "tokens": [50364, 492, 611, 362, 472, 3035, 4362, 833, 3131, 412, 741, 34, 5797, 689, 321, 10727, 6456, 281, 50612], "temperature": 0.0, "avg_logprob": -0.11881150974063423, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.010163741186261177}, {"id": 456, "seek": 163080, "start": 1635.76, "end": 1636.76, "text": " supervised learning.", "tokens": [50612, 46533, 2539, 13, 50662], "temperature": 0.0, "avg_logprob": -0.11881150974063423, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.010163741186261177}, {"id": 457, "seek": 163080, "start": 1636.76, "end": 1641.04, "text": " So we say pre-trained on a flow algorithm and we deploy it on brain vessel segmentation", "tokens": [50662, 407, 321, 584, 659, 12, 17227, 2001, 322, 257, 3095, 9284, 293, 321, 7274, 309, 322, 3567, 18098, 9469, 399, 50876], "temperature": 0.0, "avg_logprob": -0.11881150974063423, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.010163741186261177}, {"id": 458, "seek": 163080, "start": 1641.04, "end": 1642.68, "text": " tasks and stuff like that.", "tokens": [50876, 9608, 293, 1507, 411, 300, 13, 50958], "temperature": 0.0, "avg_logprob": -0.11881150974063423, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.010163741186261177}, {"id": 459, "seek": 163080, "start": 1642.68, "end": 1646.6399999999999, "text": " So we have many isolated cases where you learn a particular algorithm and it works really", "tokens": [50958, 407, 321, 362, 867, 14621, 3331, 689, 291, 1466, 257, 1729, 9284, 293, 309, 1985, 534, 51156], "temperature": 0.0, "avg_logprob": -0.11881150974063423, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.010163741186261177}, {"id": 460, "seek": 163080, "start": 1646.6399999999999, "end": 1649.3999999999999, "text": " well in a real world scenario.", "tokens": [51156, 731, 294, 257, 957, 1002, 9005, 13, 51294], "temperature": 0.0, "avg_logprob": -0.11881150974063423, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.010163741186261177}, {"id": 461, "seek": 163080, "start": 1649.3999999999999, "end": 1654.24, "text": " I would like to see how can we take this idea and truly put it to the test at larger scales,", "tokens": [51294, 286, 576, 411, 281, 536, 577, 393, 321, 747, 341, 1558, 293, 4908, 829, 309, 281, 264, 1500, 412, 4833, 17408, 11, 51536], "temperature": 0.0, "avg_logprob": -0.11881150974063423, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.010163741186261177}, {"id": 462, "seek": 163080, "start": 1654.24, "end": 1659.6399999999999, "text": " both in terms of number of problems we attack or number of nodes that we support or anything", "tokens": [51536, 1293, 294, 2115, 295, 1230, 295, 2740, 321, 2690, 420, 1230, 295, 13891, 300, 321, 1406, 420, 1340, 51806], "temperature": 0.0, "avg_logprob": -0.11881150974063423, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.010163741186261177}, {"id": 463, "seek": 165964, "start": 1659.64, "end": 1660.64, "text": " in between.", "tokens": [50364, 294, 1296, 13, 50414], "temperature": 0.0, "avg_logprob": -0.33001689634461334, "compression_ratio": 1.5, "no_speech_prob": 0.6272650957107544}, {"id": 464, "seek": 165964, "start": 1660.64, "end": 1661.64, "text": " Amazing.", "tokens": [50414, 14165, 13, 50464], "temperature": 0.0, "avg_logprob": -0.33001689634461334, "compression_ratio": 1.5, "no_speech_prob": 0.6272650957107544}, {"id": 465, "seek": 165964, "start": 1661.64, "end": 1665.64, "text": " Dr. Patovali\u010dkovi\u0107, let's just, we'll get a shaking handshot.", "tokens": [50464, 2491, 13, 430, 2513, 3337, 72, 10236, 4093, 4917, 2162, 11, 718, 311, 445, 11, 321, 603, 483, 257, 15415, 2377, 12194, 13, 50664], "temperature": 0.0, "avg_logprob": -0.33001689634461334, "compression_ratio": 1.5, "no_speech_prob": 0.6272650957107544}, {"id": 466, "seek": 165964, "start": 1665.64, "end": 1666.64, "text": " All right.", "tokens": [50664, 1057, 558, 13, 50714], "temperature": 0.0, "avg_logprob": -0.33001689634461334, "compression_ratio": 1.5, "no_speech_prob": 0.6272650957107544}, {"id": 467, "seek": 165964, "start": 1666.64, "end": 1669.1200000000001, "text": " Thank you so much for joining us.", "tokens": [50714, 1044, 291, 370, 709, 337, 5549, 505, 13, 50838], "temperature": 0.0, "avg_logprob": -0.33001689634461334, "compression_ratio": 1.5, "no_speech_prob": 0.6272650957107544}, {"id": 468, "seek": 165964, "start": 1669.1200000000001, "end": 1670.1200000000001, "text": " Thank you for having me.", "tokens": [50838, 1044, 291, 337, 1419, 385, 13, 50888], "temperature": 0.0, "avg_logprob": -0.33001689634461334, "compression_ratio": 1.5, "no_speech_prob": 0.6272650957107544}, {"id": 469, "seek": 165964, "start": 1670.1200000000001, "end": 1671.1200000000001, "text": " I really appreciate it.", "tokens": [50888, 286, 534, 4449, 309, 13, 50938], "temperature": 0.0, "avg_logprob": -0.33001689634461334, "compression_ratio": 1.5, "no_speech_prob": 0.6272650957107544}, {"id": 470, "seek": 165964, "start": 1671.1200000000001, "end": 1677.92, "text": " Dr. Ishan Mizra of Meta and Lex Friedman fame came over and had a chat with us.", "tokens": [50938, 2491, 13, 1119, 3451, 37793, 424, 295, 6377, 64, 293, 24086, 17605, 1601, 16874, 1361, 670, 293, 632, 257, 5081, 365, 505, 13, 51278], "temperature": 0.0, "avg_logprob": -0.33001689634461334, "compression_ratio": 1.5, "no_speech_prob": 0.6272650957107544}, {"id": 471, "seek": 165964, "start": 1677.92, "end": 1681.64, "text": " Ishan is one of the world's leading experts in computer vision.", "tokens": [51278, 1119, 3451, 307, 472, 295, 264, 1002, 311, 5775, 8572, 294, 3820, 5201, 13, 51464], "temperature": 0.0, "avg_logprob": -0.33001689634461334, "compression_ratio": 1.5, "no_speech_prob": 0.6272650957107544}, {"id": 472, "seek": 165964, "start": 1681.64, "end": 1683.88, "text": " So what was your paper about?", "tokens": [51464, 407, 437, 390, 428, 3035, 466, 30, 51576], "temperature": 0.0, "avg_logprob": -0.33001689634461334, "compression_ratio": 1.5, "no_speech_prob": 0.6272650957107544}, {"id": 473, "seek": 165964, "start": 1683.88, "end": 1689.44, "text": " Yeah, basically we try to have global propagation, the likes of which you see in transformers,", "tokens": [51576, 865, 11, 1936, 321, 853, 281, 362, 4338, 38377, 11, 264, 5902, 295, 597, 291, 536, 294, 4088, 433, 11, 51854], "temperature": 0.0, "avg_logprob": -0.33001689634461334, "compression_ratio": 1.5, "no_speech_prob": 0.6272650957107544}, {"id": 474, "seek": 168944, "start": 1689.44, "end": 1693.3600000000001, "text": " not like with sparse costs.", "tokens": [50364, 406, 411, 365, 637, 11668, 5497, 13, 50560], "temperature": 0.0, "avg_logprob": -0.16312781086674444, "compression_ratio": 1.779527559055118, "no_speech_prob": 0.04785698652267456}, {"id": 475, "seek": 168944, "start": 1693.3600000000001, "end": 1697.96, "text": " So but in a way that will still allow you to have nice global communication properties", "tokens": [50560, 407, 457, 294, 257, 636, 300, 486, 920, 2089, 291, 281, 362, 1481, 4338, 6101, 7221, 50790], "temperature": 0.0, "avg_logprob": -0.16312781086674444, "compression_ratio": 1.779527559055118, "no_speech_prob": 0.04785698652267456}, {"id": 476, "seek": 168944, "start": 1697.96, "end": 1699.72, "text": " and no bottlenecks and stuff like that.", "tokens": [50790, 293, 572, 44641, 2761, 293, 1507, 411, 300, 13, 50878], "temperature": 0.0, "avg_logprob": -0.16312781086674444, "compression_ratio": 1.779527559055118, "no_speech_prob": 0.04785698652267456}, {"id": 477, "seek": 168944, "start": 1699.72, "end": 1705.4, "text": " So we basically have this idea of you could generate these expander graphs which allow", "tokens": [50878, 407, 321, 1936, 362, 341, 1558, 295, 291, 727, 8460, 613, 1278, 4483, 24877, 597, 2089, 51162], "temperature": 0.0, "avg_logprob": -0.16312781086674444, "compression_ratio": 1.779527559055118, "no_speech_prob": 0.04785698652267456}, {"id": 478, "seek": 168944, "start": 1705.4, "end": 1708.4, "text": " you to have nice sparsity properties.", "tokens": [51162, 291, 281, 362, 1481, 637, 685, 507, 7221, 13, 51312], "temperature": 0.0, "avg_logprob": -0.16312781086674444, "compression_ratio": 1.779527559055118, "no_speech_prob": 0.04785698652267456}, {"id": 479, "seek": 168944, "start": 1708.4, "end": 1713.0, "text": " So basically every node I think has degree four in the graphs we compute and you need", "tokens": [51312, 407, 1936, 633, 9984, 286, 519, 575, 4314, 1451, 294, 264, 24877, 321, 14722, 293, 291, 643, 51542], "temperature": 0.0, "avg_logprob": -0.16312781086674444, "compression_ratio": 1.779527559055118, "no_speech_prob": 0.04785698652267456}, {"id": 480, "seek": 168944, "start": 1713.0, "end": 1716.4, "text": " only logarithmically many steps to traverse the graph, which means you can still do it", "tokens": [51542, 787, 41473, 32674, 984, 867, 4439, 281, 45674, 264, 4295, 11, 597, 1355, 291, 393, 920, 360, 309, 51712], "temperature": 0.0, "avg_logprob": -0.16312781086674444, "compression_ratio": 1.779527559055118, "no_speech_prob": 0.04785698652267456}, {"id": 481, "seek": 171640, "start": 1716.4, "end": 1719.48, "text": " efficiently with a small number of steps.", "tokens": [50364, 19621, 365, 257, 1359, 1230, 295, 4439, 13, 50518], "temperature": 0.0, "avg_logprob": -0.18321429558520047, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.0704803541302681}, {"id": 482, "seek": 171640, "start": 1719.48, "end": 1722.24, "text": " And yeah, it seems to empirically work well on a bunch of graph benchmarks.", "tokens": [50518, 400, 1338, 11, 309, 2544, 281, 25790, 984, 589, 731, 322, 257, 3840, 295, 4295, 43751, 13, 50656], "temperature": 0.0, "avg_logprob": -0.18321429558520047, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.0704803541302681}, {"id": 483, "seek": 171640, "start": 1722.24, "end": 1727.24, "text": " So yeah, it's a, I think it's only scratching the surface of what we can do because we literally", "tokens": [50656, 407, 1338, 11, 309, 311, 257, 11, 286, 519, 309, 311, 787, 29699, 264, 3753, 295, 437, 321, 393, 360, 570, 321, 3736, 50906], "temperature": 0.0, "avg_logprob": -0.18321429558520047, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.0704803541302681}, {"id": 484, "seek": 171640, "start": 1727.24, "end": 1732.0400000000002, "text": " just generate a graph at random and slap it onto like mask the computations, but yeah,", "tokens": [50906, 445, 8460, 257, 4295, 412, 4974, 293, 21075, 309, 3911, 411, 6094, 264, 2807, 763, 11, 457, 1338, 11, 51146], "temperature": 0.0, "avg_logprob": -0.18321429558520047, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.0704803541302681}, {"id": 485, "seek": 171640, "start": 1732.0400000000002, "end": 1733.0400000000002, "text": " it's an interesting start.", "tokens": [51146, 309, 311, 364, 1880, 722, 13, 51196], "temperature": 0.0, "avg_logprob": -0.18321429558520047, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.0704803541302681}, {"id": 486, "seek": 171640, "start": 1733.0400000000002, "end": 1734.0400000000002, "text": " Very nice.", "tokens": [51196, 4372, 1481, 13, 51246], "temperature": 0.0, "avg_logprob": -0.18321429558520047, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.0704803541302681}, {"id": 487, "seek": 171640, "start": 1734.0400000000002, "end": 1735.0400000000002, "text": " Yeah.", "tokens": [51246, 865, 13, 51296], "temperature": 0.0, "avg_logprob": -0.18321429558520047, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.0704803541302681}, {"id": 488, "seek": 171640, "start": 1735.0400000000002, "end": 1736.0400000000002, "text": " How about your conference?", "tokens": [51296, 1012, 466, 428, 7586, 30, 51346], "temperature": 0.0, "avg_logprob": -0.18321429558520047, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.0704803541302681}, {"id": 489, "seek": 171640, "start": 1736.0400000000002, "end": 1737.0400000000002, "text": " How's it been?", "tokens": [51346, 1012, 311, 309, 668, 30, 51396], "temperature": 0.0, "avg_logprob": -0.18321429558520047, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.0704803541302681}, {"id": 490, "seek": 171640, "start": 1737.0400000000002, "end": 1738.0400000000002, "text": " So it's been pretty good.", "tokens": [51396, 407, 309, 311, 668, 1238, 665, 13, 51446], "temperature": 0.0, "avg_logprob": -0.18321429558520047, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.0704803541302681}, {"id": 491, "seek": 171640, "start": 1738.0400000000002, "end": 1741.2800000000002, "text": " We're organizing the self supervised learning workshop tomorrow, which is going to be probably,", "tokens": [51446, 492, 434, 17608, 264, 2698, 46533, 2539, 13541, 4153, 11, 597, 307, 516, 281, 312, 1391, 11, 51608], "temperature": 0.0, "avg_logprob": -0.18321429558520047, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.0704803541302681}, {"id": 492, "seek": 171640, "start": 1741.2800000000002, "end": 1745.6000000000001, "text": " I hope like useful to a lot of people, we're going to have a bunch of speakers coming from", "tokens": [51608, 286, 1454, 411, 4420, 281, 257, 688, 295, 561, 11, 321, 434, 516, 281, 362, 257, 3840, 295, 9518, 1348, 490, 51824], "temperature": 0.0, "avg_logprob": -0.18321429558520047, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.0704803541302681}, {"id": 493, "seek": 174560, "start": 1745.6, "end": 1749.8799999999999, "text": " vision, language, NLP, like speech and so on.", "tokens": [50364, 5201, 11, 2856, 11, 426, 45196, 11, 411, 6218, 293, 370, 322, 13, 50578], "temperature": 0.0, "avg_logprob": -0.1973235627524873, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.058239392936229706}, {"id": 494, "seek": 174560, "start": 1749.8799999999999, "end": 1753.9199999999998, "text": " And yeah, we're also presenting a poster there, which is about learning joint image and video", "tokens": [50578, 400, 1338, 11, 321, 434, 611, 15578, 257, 17171, 456, 11, 597, 307, 466, 2539, 7225, 3256, 293, 960, 50780], "temperature": 0.0, "avg_logprob": -0.1973235627524873, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.058239392936229706}, {"id": 495, "seek": 174560, "start": 1753.9199999999998, "end": 1757.8, "text": " representations, which are state of the art across image and video benchmarks using a", "tokens": [50780, 33358, 11, 597, 366, 1785, 295, 264, 1523, 2108, 3256, 293, 960, 43751, 1228, 257, 50974], "temperature": 0.0, "avg_logprob": -0.1973235627524873, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.058239392936229706}, {"id": 496, "seek": 174560, "start": 1757.8, "end": 1759.3, "text": " single model.", "tokens": [50974, 2167, 2316, 13, 51049], "temperature": 0.0, "avg_logprob": -0.1973235627524873, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.058239392936229706}, {"id": 497, "seek": 174560, "start": 1759.3, "end": 1763.12, "text": " On the final day of the conference, I caught up with Petra again at the poster session for", "tokens": [51049, 1282, 264, 2572, 786, 295, 264, 7586, 11, 286, 5415, 493, 365, 10472, 424, 797, 412, 264, 17171, 5481, 337, 51240], "temperature": 0.0, "avg_logprob": -0.1973235627524873, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.058239392936229706}, {"id": 498, "seek": 174560, "start": 1763.12, "end": 1768.4399999999998, "text": " new reps, which is the symmetry and geometry and neural representations group.", "tokens": [51240, 777, 27007, 11, 597, 307, 264, 25440, 293, 18426, 293, 18161, 33358, 1594, 13, 51506], "temperature": 0.0, "avg_logprob": -0.1973235627524873, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.058239392936229706}, {"id": 499, "seek": 174560, "start": 1768.4399999999998, "end": 1772.7199999999998, "text": " And his paper was selected by all of the reviewers at the conference as being in the", "tokens": [51506, 400, 702, 3035, 390, 8209, 538, 439, 295, 264, 45837, 412, 264, 7586, 382, 885, 294, 264, 51720], "temperature": 0.0, "avg_logprob": -0.1973235627524873, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.058239392936229706}, {"id": 500, "seek": 177272, "start": 1772.72, "end": 1779.44, "text": " top 10, which is super impressive, but this is Petra talking about his paper.", "tokens": [50364, 1192, 1266, 11, 597, 307, 1687, 8992, 11, 457, 341, 307, 10472, 424, 1417, 466, 702, 3035, 13, 50700], "temperature": 0.0, "avg_logprob": -0.16442931038992745, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.17946557700634003}, {"id": 501, "seek": 177272, "start": 1779.44, "end": 1785.08, "text": " So in the expander graph propagation work, we are trying to solve what is, in my opinion,", "tokens": [50700, 407, 294, 264, 1278, 4483, 4295, 38377, 589, 11, 321, 366, 1382, 281, 5039, 437, 307, 11, 294, 452, 4800, 11, 50982], "temperature": 0.0, "avg_logprob": -0.16442931038992745, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.17946557700634003}, {"id": 502, "seek": 177272, "start": 1785.08, "end": 1789.3600000000001, "text": " one of the most important problems in graph representation learning currently unsolved,", "tokens": [50982, 472, 295, 264, 881, 1021, 2740, 294, 4295, 10290, 2539, 4362, 2693, 29110, 11, 51196], "temperature": 0.0, "avg_logprob": -0.16442931038992745, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.17946557700634003}, {"id": 503, "seek": 177272, "start": 1789.3600000000001, "end": 1791.32, "text": " which is the oversplashing problem.", "tokens": [51196, 597, 307, 264, 15488, 564, 11077, 1154, 13, 51294], "temperature": 0.0, "avg_logprob": -0.16442931038992745, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.17946557700634003}, {"id": 504, "seek": 177272, "start": 1791.32, "end": 1796.16, "text": " And effectively it is a task, which it's a problem which plagues graph neural networks", "tokens": [51294, 400, 8659, 309, 307, 257, 5633, 11, 597, 309, 311, 257, 1154, 597, 499, 7063, 4295, 18161, 9590, 51536], "temperature": 0.0, "avg_logprob": -0.16442931038992745, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.17946557700634003}, {"id": 505, "seek": 177272, "start": 1796.16, "end": 1799.9, "text": " regardless of which parameters you choose or which model you choose.", "tokens": [51536, 10060, 295, 597, 9834, 291, 2826, 420, 597, 2316, 291, 2826, 13, 51723], "temperature": 0.0, "avg_logprob": -0.16442931038992745, "compression_ratio": 1.6996197718631179, "no_speech_prob": 0.17946557700634003}, {"id": 506, "seek": 179990, "start": 1799.9, "end": 1804.02, "text": " It's really something that often depends on the topology of the graph, and it's a situation", "tokens": [50364, 467, 311, 534, 746, 300, 2049, 5946, 322, 264, 1192, 1793, 295, 264, 4295, 11, 293, 309, 311, 257, 2590, 50570], "temperature": 0.0, "avg_logprob": -0.12116692705852229, "compression_ratio": 1.697452229299363, "no_speech_prob": 0.09126097708940506}, {"id": 507, "seek": 179990, "start": 1804.02, "end": 1809.94, "text": " where no matter how hard you try, no matter which parameters you set, the amount of features", "tokens": [50570, 689, 572, 1871, 577, 1152, 291, 853, 11, 572, 1871, 597, 9834, 291, 992, 11, 264, 2372, 295, 4122, 50866], "temperature": 0.0, "avg_logprob": -0.12116692705852229, "compression_ratio": 1.697452229299363, "no_speech_prob": 0.09126097708940506}, {"id": 508, "seek": 179990, "start": 1809.94, "end": 1813.76, "text": " you would need to compute, so the size of your latent space would have to be exponential", "tokens": [50866, 291, 576, 643, 281, 14722, 11, 370, 264, 2744, 295, 428, 48994, 1901, 576, 362, 281, 312, 21510, 51057], "temperature": 0.0, "avg_logprob": -0.12116692705852229, "compression_ratio": 1.697452229299363, "no_speech_prob": 0.09126097708940506}, {"id": 509, "seek": 179990, "start": 1813.76, "end": 1817.7800000000002, "text": " in the number of layers for the pairs of nodes to efficiently communicate.", "tokens": [51057, 294, 264, 1230, 295, 7914, 337, 264, 15494, 295, 13891, 281, 19621, 7890, 13, 51258], "temperature": 0.0, "avg_logprob": -0.12116692705852229, "compression_ratio": 1.697452229299363, "no_speech_prob": 0.09126097708940506}, {"id": 510, "seek": 179990, "start": 1817.7800000000002, "end": 1821.76, "text": " We don't always know when it happens, but very often it tends to happen around these", "tokens": [51258, 492, 500, 380, 1009, 458, 562, 309, 2314, 11, 457, 588, 2049, 309, 12258, 281, 1051, 926, 613, 51457], "temperature": 0.0, "avg_logprob": -0.12116692705852229, "compression_ratio": 1.697452229299363, "no_speech_prob": 0.09126097708940506}, {"id": 511, "seek": 179990, "start": 1821.76, "end": 1822.76, "text": " bottlenecks.", "tokens": [51457, 44641, 2761, 13, 51507], "temperature": 0.0, "avg_logprob": -0.12116692705852229, "compression_ratio": 1.697452229299363, "no_speech_prob": 0.09126097708940506}, {"id": 512, "seek": 179990, "start": 1822.76, "end": 1826.42, "text": " So basically in this particular graph, you have these two communities that are tightly", "tokens": [51507, 407, 1936, 294, 341, 1729, 4295, 11, 291, 362, 613, 732, 4456, 300, 366, 21952, 51690], "temperature": 0.0, "avg_logprob": -0.12116692705852229, "compression_ratio": 1.697452229299363, "no_speech_prob": 0.09126097708940506}, {"id": 513, "seek": 182642, "start": 1826.42, "end": 1831.04, "text": " connected, and you have this just one critical edge connecting them, and this edge is now", "tokens": [50364, 4582, 11, 293, 291, 362, 341, 445, 472, 4924, 4691, 11015, 552, 11, 293, 341, 4691, 307, 586, 50595], "temperature": 0.0, "avg_logprob": -0.1347315202065564, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.22751988470554352}, {"id": 514, "seek": 182642, "start": 1831.04, "end": 1832.64, "text": " under a lot of pressure.", "tokens": [50595, 833, 257, 688, 295, 3321, 13, 50675], "temperature": 0.0, "avg_logprob": -0.1347315202065564, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.22751988470554352}, {"id": 515, "seek": 182642, "start": 1832.64, "end": 1837.8600000000001, "text": " If you want data from these nodes to travel to these nodes and vice versa, this edge has", "tokens": [50675, 759, 291, 528, 1412, 490, 613, 13891, 281, 3147, 281, 613, 13891, 293, 11964, 25650, 11, 341, 4691, 575, 50936], "temperature": 0.0, "avg_logprob": -0.1347315202065564, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.22751988470554352}, {"id": 516, "seek": 182642, "start": 1837.8600000000001, "end": 1844.18, "text": " to be mindful of a lot of things, so the size of the feature space required for this edge", "tokens": [50936, 281, 312, 14618, 295, 257, 688, 295, 721, 11, 370, 264, 2744, 295, 264, 4111, 1901, 4739, 337, 341, 4691, 51252], "temperature": 0.0, "avg_logprob": -0.1347315202065564, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.22751988470554352}, {"id": 517, "seek": 182642, "start": 1844.18, "end": 1847.54, "text": " grows exponentially, and things get even worse when you look at trees.", "tokens": [51252, 13156, 37330, 11, 293, 721, 483, 754, 5324, 562, 291, 574, 412, 5852, 13, 51420], "temperature": 0.0, "avg_logprob": -0.1347315202065564, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.22751988470554352}, {"id": 518, "seek": 182642, "start": 1847.54, "end": 1852.8400000000001, "text": " Trees are like the canonical worst case example, where cutting off this edge would really trigger", "tokens": [51420, 314, 4856, 366, 411, 264, 46491, 5855, 1389, 1365, 11, 689, 6492, 766, 341, 4691, 576, 534, 7875, 51685], "temperature": 0.0, "avg_logprob": -0.1347315202065564, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.22751988470554352}, {"id": 519, "seek": 185284, "start": 1852.84, "end": 1858.26, "text": " all sorts of bottleneck cases, and essentially you need basically a number of, to store information", "tokens": [50364, 439, 7527, 295, 44641, 547, 3331, 11, 293, 4476, 291, 643, 1936, 257, 1230, 295, 11, 281, 3531, 1589, 50635], "temperature": 0.0, "avg_logprob": -0.12093774811560366, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.00897662527859211}, {"id": 520, "seek": 185284, "start": 1858.26, "end": 1864.24, "text": " about a number of nodes that goes exponentially in the number of steps, just to be able to", "tokens": [50635, 466, 257, 1230, 295, 13891, 300, 1709, 37330, 294, 264, 1230, 295, 4439, 11, 445, 281, 312, 1075, 281, 50934], "temperature": 0.0, "avg_logprob": -0.12093774811560366, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.00897662527859211}, {"id": 521, "seek": 185284, "start": 1864.24, "end": 1865.9599999999998, "text": " travel to the other side of the tree.", "tokens": [50934, 3147, 281, 264, 661, 1252, 295, 264, 4230, 13, 51020], "temperature": 0.0, "avg_logprob": -0.12093774811560366, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.00897662527859211}, {"id": 522, "seek": 185284, "start": 1865.9599999999998, "end": 1869.9599999999998, "text": " So this is a fundamental problem of propagating data, which has nothing to do with the choice", "tokens": [51020, 407, 341, 307, 257, 8088, 1154, 295, 12425, 990, 1412, 11, 597, 575, 1825, 281, 360, 365, 264, 3922, 51220], "temperature": 0.0, "avg_logprob": -0.12093774811560366, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.00897662527859211}, {"id": 523, "seek": 185284, "start": 1869.9599999999998, "end": 1873.1, "text": " of model, just topology.", "tokens": [51220, 295, 2316, 11, 445, 1192, 1793, 13, 51377], "temperature": 0.0, "avg_logprob": -0.12093774811560366, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.00897662527859211}, {"id": 524, "seek": 185284, "start": 1873.1, "end": 1875.86, "text": " And what do we try to do to fix this problem?", "tokens": [51377, 400, 437, 360, 321, 853, 281, 360, 281, 3191, 341, 1154, 30, 51515], "temperature": 0.0, "avg_logprob": -0.12093774811560366, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.00897662527859211}, {"id": 525, "seek": 185284, "start": 1875.86, "end": 1879.72, "text": " You would ideally want, so first we start off with the assumption that this kind of global", "tokens": [51515, 509, 576, 22915, 528, 11, 370, 700, 321, 722, 766, 365, 264, 15302, 300, 341, 733, 295, 4338, 51708], "temperature": 0.0, "avg_logprob": -0.12093774811560366, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.00897662527859211}, {"id": 526, "seek": 187972, "start": 1879.72, "end": 1881.58, "text": " talking is actually beneficial.", "tokens": [50364, 1417, 307, 767, 14072, 13, 50457], "temperature": 0.0, "avg_logprob": -0.14398838013641593, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.12571801245212555}, {"id": 527, "seek": 187972, "start": 1881.58, "end": 1885.52, "text": " Of course there are some tasks where you might not want data to travel in this way, because", "tokens": [50457, 2720, 1164, 456, 366, 512, 9608, 689, 291, 1062, 406, 528, 1412, 281, 3147, 294, 341, 636, 11, 570, 50654], "temperature": 0.0, "avg_logprob": -0.14398838013641593, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.12571801245212555}, {"id": 528, "seek": 187972, "start": 1885.52, "end": 1890.56, "text": " if it's a highly homophilus data-driven problem, then you might want information to stay in", "tokens": [50654, 498, 309, 311, 257, 5405, 3655, 5317, 388, 301, 1412, 12, 25456, 1154, 11, 550, 291, 1062, 528, 1589, 281, 1754, 294, 50906], "temperature": 0.0, "avg_logprob": -0.14398838013641593, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.12571801245212555}, {"id": 529, "seek": 187972, "start": 1890.56, "end": 1892.76, "text": " the community, to not get diluted.", "tokens": [50906, 264, 1768, 11, 281, 406, 483, 11504, 4866, 13, 51016], "temperature": 0.0, "avg_logprob": -0.14398838013641593, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.12571801245212555}, {"id": 530, "seek": 187972, "start": 1892.76, "end": 1896.8, "text": " But we assume in many tasks, like say molecular property prediction tasks, you actually want", "tokens": [51016, 583, 321, 6552, 294, 867, 9608, 11, 411, 584, 19046, 4707, 17630, 9608, 11, 291, 767, 528, 51218], "temperature": 0.0, "avg_logprob": -0.14398838013641593, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.12571801245212555}, {"id": 531, "seek": 187972, "start": 1896.8, "end": 1900.56, "text": " data to travel globally, so that's exactly what we do.", "tokens": [51218, 1412, 281, 3147, 18958, 11, 370, 300, 311, 2293, 437, 321, 360, 13, 51406], "temperature": 0.0, "avg_logprob": -0.14398838013641593, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.12571801245212555}, {"id": 532, "seek": 187972, "start": 1900.56, "end": 1902.0, "text": " That's our first assumption.", "tokens": [51406, 663, 311, 527, 700, 15302, 13, 51478], "temperature": 0.0, "avg_logprob": -0.14398838013641593, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.12571801245212555}, {"id": 533, "seek": 187972, "start": 1902.0, "end": 1905.28, "text": " As we just described, we don't really want these bottlenecks to exist, because if there's", "tokens": [51478, 1018, 321, 445, 7619, 11, 321, 500, 380, 534, 528, 613, 44641, 2761, 281, 2514, 11, 570, 498, 456, 311, 51642], "temperature": 0.0, "avg_logprob": -0.14398838013641593, "compression_ratio": 1.7290969899665551, "no_speech_prob": 0.12571801245212555}, {"id": 534, "seek": 190528, "start": 1905.28, "end": 1909.8, "text": " a bottleneck, no matter what you do with the model, it's not going to work well.", "tokens": [50364, 257, 44641, 547, 11, 572, 1871, 437, 291, 360, 365, 264, 2316, 11, 309, 311, 406, 516, 281, 589, 731, 13, 50590], "temperature": 0.0, "avg_logprob": -0.1594799156475784, "compression_ratio": 1.7203947368421053, "no_speech_prob": 0.03955238312482834}, {"id": 535, "seek": 190528, "start": 1909.8, "end": 1913.8, "text": " We would ideally want the complexity to be scalable, so we can apply this to graphs of", "tokens": [50590, 492, 576, 22915, 528, 264, 14024, 281, 312, 38481, 11, 370, 321, 393, 3079, 341, 281, 24877, 295, 50790], "temperature": 0.0, "avg_logprob": -0.1594799156475784, "compression_ratio": 1.7203947368421053, "no_speech_prob": 0.03955238312482834}, {"id": 536, "seek": 190528, "start": 1913.8, "end": 1915.68, "text": " arbitrary sizes.", "tokens": [50790, 23211, 11602, 13, 50884], "temperature": 0.0, "avg_logprob": -0.1594799156475784, "compression_ratio": 1.7203947368421053, "no_speech_prob": 0.03955238312482834}, {"id": 537, "seek": 190528, "start": 1915.68, "end": 1919.84, "text": " One simple solution to this problem is to use a graph transformer, which would connect", "tokens": [50884, 1485, 2199, 3827, 281, 341, 1154, 307, 281, 764, 257, 4295, 31782, 11, 597, 576, 1745, 51092], "temperature": 0.0, "avg_logprob": -0.1594799156475784, "compression_ratio": 1.7203947368421053, "no_speech_prob": 0.03955238312482834}, {"id": 538, "seek": 190528, "start": 1919.84, "end": 1925.28, "text": " every node to every other node and give you a trivially setting with no bottlenecks.", "tokens": [51092, 633, 9984, 281, 633, 661, 9984, 293, 976, 291, 257, 1376, 85, 2270, 3287, 365, 572, 44641, 2761, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1594799156475784, "compression_ratio": 1.7203947368421053, "no_speech_prob": 0.03955238312482834}, {"id": 539, "seek": 190528, "start": 1925.28, "end": 1931.3999999999999, "text": " However, as we will show later, these fully connected graphs are trivially dense expanders,", "tokens": [51364, 2908, 11, 382, 321, 486, 855, 1780, 11, 613, 4498, 4582, 24877, 366, 1376, 85, 2270, 18011, 5268, 433, 11, 51670], "temperature": 0.0, "avg_logprob": -0.1594799156475784, "compression_ratio": 1.7203947368421053, "no_speech_prob": 0.03955238312482834}, {"id": 540, "seek": 190528, "start": 1931.3999999999999, "end": 1932.3999999999999, "text": " actually.", "tokens": [51670, 767, 13, 51720], "temperature": 0.0, "avg_logprob": -0.1594799156475784, "compression_ratio": 1.7203947368421053, "no_speech_prob": 0.03955238312482834}, {"id": 541, "seek": 190528, "start": 1932.3999999999999, "end": 1934.84, "text": " So they fit our theory, but they are dense and they won't scale.", "tokens": [51720, 407, 436, 3318, 527, 5261, 11, 457, 436, 366, 18011, 293, 436, 1582, 380, 4373, 13, 51842], "temperature": 0.0, "avg_logprob": -0.1594799156475784, "compression_ratio": 1.7203947368421053, "no_speech_prob": 0.03955238312482834}, {"id": 542, "seek": 193484, "start": 1934.84, "end": 1936.6, "text": " So we don't necessarily want that.", "tokens": [50364, 407, 321, 500, 380, 4725, 528, 300, 13, 50452], "temperature": 0.0, "avg_logprob": -0.1226334015528361, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.10651461780071259}, {"id": 543, "seek": 193484, "start": 1936.6, "end": 1943.08, "text": " And lastly, because it's often quite computationally painful to clear these bottlenecks in an input", "tokens": [50452, 400, 16386, 11, 570, 309, 311, 2049, 1596, 24903, 379, 11697, 281, 1850, 613, 44641, 2761, 294, 364, 4846, 50776], "temperature": 0.0, "avg_logprob": -0.1226334015528361, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.10651461780071259}, {"id": 544, "seek": 193484, "start": 1943.08, "end": 1948.12, "text": " data-driven way, especially if you have lots of online graphs coming into your problem,", "tokens": [50776, 1412, 12, 25456, 636, 11, 2318, 498, 291, 362, 3195, 295, 2950, 24877, 1348, 666, 428, 1154, 11, 51028], "temperature": 0.0, "avg_logprob": -0.1226334015528361, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.10651461780071259}, {"id": 545, "seek": 193484, "start": 1948.12, "end": 1951.72, "text": " we might ideally want a method that doesn't have to do like dedicated pre-processing of", "tokens": [51028, 321, 1062, 22915, 528, 257, 3170, 300, 1177, 380, 362, 281, 360, 411, 8374, 659, 12, 41075, 278, 295, 51208], "temperature": 0.0, "avg_logprob": -0.1226334015528361, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.10651461780071259}, {"id": 546, "seek": 193484, "start": 1951.72, "end": 1952.8799999999999, "text": " the input graph.", "tokens": [51208, 264, 4846, 4295, 13, 51266], "temperature": 0.0, "avg_logprob": -0.1226334015528361, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.10651461780071259}, {"id": 547, "seek": 193484, "start": 1952.8799999999999, "end": 1957.0, "text": " And actually, satisfying all four of these at the same time turns out to be quite tricky.", "tokens": [51266, 400, 767, 11, 18348, 439, 1451, 295, 613, 412, 264, 912, 565, 4523, 484, 281, 312, 1596, 12414, 13, 51472], "temperature": 0.0, "avg_logprob": -0.1226334015528361, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.10651461780071259}, {"id": 548, "seek": 193484, "start": 1957.0, "end": 1961.6, "text": " We actually have done a literature survey of a bunch of related works, and it seems", "tokens": [51472, 492, 767, 362, 1096, 257, 10394, 8984, 295, 257, 3840, 295, 4077, 1985, 11, 293, 309, 2544, 51702], "temperature": 0.0, "avg_logprob": -0.1226334015528361, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.10651461780071259}, {"id": 549, "seek": 196160, "start": 1961.6, "end": 1964.7199999999998, "text": " really hard to tick all four of these boxes.", "tokens": [50364, 534, 1152, 281, 5204, 439, 1451, 295, 613, 9002, 13, 50520], "temperature": 0.0, "avg_logprob": -0.17674220841506433, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.21434849500656128}, {"id": 550, "seek": 196160, "start": 1964.7199999999998, "end": 1968.1799999999998, "text": " And our method, the expander graph propagation, tends to tick all four of them.", "tokens": [50520, 400, 527, 3170, 11, 264, 1278, 4483, 4295, 38377, 11, 12258, 281, 5204, 439, 1451, 295, 552, 13, 50693], "temperature": 0.0, "avg_logprob": -0.17674220841506433, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.21434849500656128}, {"id": 551, "seek": 196160, "start": 1968.1799999999998, "end": 1969.3999999999999, "text": " So how do we do it?", "tokens": [50693, 407, 577, 360, 321, 360, 309, 30, 50754], "temperature": 0.0, "avg_logprob": -0.17674220841506433, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.21434849500656128}, {"id": 552, "seek": 196160, "start": 1969.3999999999999, "end": 1973.8, "text": " Basically, we propose to propagate information over these expander graphs, which are known", "tokens": [50754, 8537, 11, 321, 17421, 281, 48256, 1589, 670, 613, 1278, 4483, 24877, 11, 597, 366, 2570, 50974], "temperature": 0.0, "avg_logprob": -0.17674220841506433, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.21434849500656128}, {"id": 553, "seek": 196160, "start": 1973.8, "end": 1975.7199999999998, "text": " constructs from graph theory.", "tokens": [50974, 7690, 82, 490, 4295, 5261, 13, 51070], "temperature": 0.0, "avg_logprob": -0.17674220841506433, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.21434849500656128}, {"id": 554, "seek": 196160, "start": 1975.7199999999998, "end": 1980.48, "text": " Specifically, expander graphs have mathematical properties of a high-chigger constant, so", "tokens": [51070, 26058, 11, 1278, 4483, 24877, 362, 18894, 7221, 295, 257, 1090, 12, 339, 6812, 5754, 11, 370, 51308], "temperature": 0.0, "avg_logprob": -0.17674220841506433, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.21434849500656128}, {"id": 555, "seek": 196160, "start": 1980.48, "end": 1985.12, "text": " a very low bottleneck, which is good, a low diameter, meaning you'll get global information", "tokens": [51308, 257, 588, 2295, 44641, 547, 11, 597, 307, 665, 11, 257, 2295, 14196, 11, 3620, 291, 603, 483, 4338, 1589, 51540], "temperature": 0.0, "avg_logprob": -0.17674220841506433, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.21434849500656128}, {"id": 556, "seek": 196160, "start": 1985.12, "end": 1986.7199999999998, "text": " propagation very efficiently.", "tokens": [51540, 38377, 588, 19621, 13, 51620], "temperature": 0.0, "avg_logprob": -0.17674220841506433, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.21434849500656128}, {"id": 557, "seek": 198672, "start": 1986.72, "end": 1991.96, "text": " However, additionally, we can build expanders in a sparse manner using this standard mathematical", "tokens": [50364, 2908, 11, 43181, 11, 321, 393, 1322, 1278, 41430, 294, 257, 637, 11668, 9060, 1228, 341, 3832, 18894, 50626], "temperature": 0.0, "avg_logprob": -0.16425473903252827, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.1822671890258789}, {"id": 558, "seek": 198672, "start": 1991.96, "end": 1994.84, "text": " construction from the special linear group.", "tokens": [50626, 6435, 490, 264, 2121, 8213, 1594, 13, 50770], "temperature": 0.0, "avg_logprob": -0.16425473903252827, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.1822671890258789}, {"id": 559, "seek": 198672, "start": 1994.84, "end": 1999.24, "text": " And that actually guarantees us that the degree of every node will be four, therefore the", "tokens": [50770, 400, 300, 767, 32567, 505, 300, 264, 4314, 295, 633, 9984, 486, 312, 1451, 11, 4412, 264, 50990], "temperature": 0.0, "avg_logprob": -0.16425473903252827, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.1822671890258789}, {"id": 560, "seek": 198672, "start": 1999.24, "end": 2001.2, "text": " graph will be sparse.", "tokens": [50990, 4295, 486, 312, 637, 11668, 13, 51088], "temperature": 0.0, "avg_logprob": -0.16425473903252827, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.1822671890258789}, {"id": 561, "seek": 198672, "start": 2001.2, "end": 2006.24, "text": " And actually, the only generative parameter of these graphs is the size of the group,", "tokens": [51088, 400, 767, 11, 264, 787, 1337, 1166, 13075, 295, 613, 24877, 307, 264, 2744, 295, 264, 1594, 11, 51340], "temperature": 0.0, "avg_logprob": -0.16425473903252827, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.1822671890258789}, {"id": 562, "seek": 198672, "start": 2006.24, "end": 2007.6000000000001, "text": " this N over here.", "tokens": [51340, 341, 426, 670, 510, 13, 51408], "temperature": 0.0, "avg_logprob": -0.16425473903252827, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.1822671890258789}, {"id": 563, "seek": 198672, "start": 2007.6000000000001, "end": 2011.04, "text": " So it's very easy to generate an expander for a particular number of nodes.", "tokens": [51408, 407, 309, 311, 588, 1858, 281, 8460, 364, 1278, 4483, 337, 257, 1729, 1230, 295, 13891, 13, 51580], "temperature": 0.0, "avg_logprob": -0.16425473903252827, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.1822671890258789}, {"id": 564, "seek": 198672, "start": 2011.04, "end": 2014.76, "text": " You just tell me what N you want, and I'll give you a graph.", "tokens": [51580, 509, 445, 980, 385, 437, 426, 291, 528, 11, 293, 286, 603, 976, 291, 257, 4295, 13, 51766], "temperature": 0.0, "avg_logprob": -0.16425473903252827, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.1822671890258789}, {"id": 565, "seek": 201476, "start": 2014.76, "end": 2017.64, "text": " So when you look at an expander, it looks something like this.", "tokens": [50364, 407, 562, 291, 574, 412, 364, 1278, 4483, 11, 309, 1542, 746, 411, 341, 13, 50508], "temperature": 0.0, "avg_logprob": -0.12840845945070115, "compression_ratio": 1.6757575757575758, "no_speech_prob": 0.452144980430603}, {"id": 566, "seek": 201476, "start": 2017.64, "end": 2021.4, "text": " It is basically, what I like to say, it looks a bit like the human brain, right?", "tokens": [50508, 467, 307, 1936, 11, 437, 286, 411, 281, 584, 11, 309, 1542, 257, 857, 411, 264, 1952, 3567, 11, 558, 30, 50696], "temperature": 0.0, "avg_logprob": -0.12840845945070115, "compression_ratio": 1.6757575757575758, "no_speech_prob": 0.452144980430603}, {"id": 567, "seek": 201476, "start": 2021.4, "end": 2025.56, "text": " Every node kind of has this very local connectivity to its four immediate neighbors.", "tokens": [50696, 2048, 9984, 733, 295, 575, 341, 588, 2654, 21095, 281, 1080, 1451, 11629, 12512, 13, 50904], "temperature": 0.0, "avg_logprob": -0.12840845945070115, "compression_ratio": 1.6757575757575758, "no_speech_prob": 0.452144980430603}, {"id": 568, "seek": 201476, "start": 2025.56, "end": 2030.56, "text": " But as you go far away, like log N steps, you get a lot of cycles being closed very quickly,", "tokens": [50904, 583, 382, 291, 352, 1400, 1314, 11, 411, 3565, 426, 4439, 11, 291, 483, 257, 688, 295, 17796, 885, 5395, 588, 2661, 11, 51154], "temperature": 0.0, "avg_logprob": -0.12840845945070115, "compression_ratio": 1.6757575757575758, "no_speech_prob": 0.452144980430603}, {"id": 569, "seek": 201476, "start": 2030.56, "end": 2033.92, "text": " and the global communication properties get like really good.", "tokens": [51154, 293, 264, 4338, 6101, 7221, 483, 411, 534, 665, 13, 51322], "temperature": 0.0, "avg_logprob": -0.12840845945070115, "compression_ratio": 1.6757575757575758, "no_speech_prob": 0.452144980430603}, {"id": 570, "seek": 201476, "start": 2033.92, "end": 2035.52, "text": " So that's our proposal.", "tokens": [51322, 407, 300, 311, 527, 11494, 13, 51402], "temperature": 0.0, "avg_logprob": -0.12840845945070115, "compression_ratio": 1.6757575757575758, "no_speech_prob": 0.452144980430603}, {"id": 571, "seek": 201476, "start": 2035.52, "end": 2039.48, "text": " Take basically, you know, your state-of-the-art graph net that you care about.", "tokens": [51402, 3664, 1936, 11, 291, 458, 11, 428, 1785, 12, 2670, 12, 3322, 12, 446, 4295, 2533, 300, 291, 1127, 466, 13, 51600], "temperature": 0.0, "avg_logprob": -0.12840845945070115, "compression_ratio": 1.6757575757575758, "no_speech_prob": 0.452144980430603}, {"id": 572, "seek": 201476, "start": 2039.48, "end": 2042.92, "text": " We literally just take the code actively available implementation.", "tokens": [51600, 492, 3736, 445, 747, 264, 3089, 13022, 2435, 11420, 13, 51772], "temperature": 0.0, "avg_logprob": -0.12840845945070115, "compression_ratio": 1.6757575757575758, "no_speech_prob": 0.452144980430603}, {"id": 573, "seek": 204292, "start": 2042.92, "end": 2048.28, "text": " We switch the graph neural network connectivity in every even layer to operate over one of", "tokens": [50364, 492, 3679, 264, 4295, 18161, 3209, 21095, 294, 633, 754, 4583, 281, 9651, 670, 472, 295, 50632], "temperature": 0.0, "avg_logprob": -0.11113787772960233, "compression_ratio": 1.8911564625850341, "no_speech_prob": 0.0035927153658121824}, {"id": 574, "seek": 204292, "start": 2048.28, "end": 2050.64, "text": " these guys rather than the input graph.", "tokens": [50632, 613, 1074, 2831, 813, 264, 4846, 4295, 13, 50750], "temperature": 0.0, "avg_logprob": -0.11113787772960233, "compression_ratio": 1.8911564625850341, "no_speech_prob": 0.0035927153658121824}, {"id": 575, "seek": 204292, "start": 2050.64, "end": 2054.4, "text": " So basically, you kind of alternate input graph, expander graph, input graph, expander", "tokens": [50750, 407, 1936, 11, 291, 733, 295, 18873, 4846, 4295, 11, 1278, 4483, 4295, 11, 4846, 4295, 11, 1278, 4483, 50938], "temperature": 0.0, "avg_logprob": -0.11113787772960233, "compression_ratio": 1.8911564625850341, "no_speech_prob": 0.0035927153658121824}, {"id": 576, "seek": 204292, "start": 2054.4, "end": 2059.32, "text": " graph, so that the input graphs layers are responsible for the usual local computations", "tokens": [50938, 4295, 11, 370, 300, 264, 4846, 24877, 7914, 366, 6250, 337, 264, 7713, 2654, 2807, 763, 51184], "temperature": 0.0, "avg_logprob": -0.11113787772960233, "compression_ratio": 1.8911564625850341, "no_speech_prob": 0.0035927153658121824}, {"id": 577, "seek": 204292, "start": 2059.32, "end": 2061.08, "text": " that a GNN wants to do.", "tokens": [51184, 300, 257, 46411, 45, 2738, 281, 360, 13, 51272], "temperature": 0.0, "avg_logprob": -0.11113787772960233, "compression_ratio": 1.8911564625850341, "no_speech_prob": 0.0035927153658121824}, {"id": 578, "seek": 204292, "start": 2061.08, "end": 2065.2400000000002, "text": " And the expander layers are responsible within diffusing that information globally in a sparse", "tokens": [51272, 400, 264, 1278, 4483, 7914, 366, 6250, 1951, 7593, 7981, 300, 1589, 18958, 294, 257, 637, 11668, 51480], "temperature": 0.0, "avg_logprob": -0.11113787772960233, "compression_ratio": 1.8911564625850341, "no_speech_prob": 0.0035927153658121824}, {"id": 579, "seek": 204292, "start": 2065.2400000000002, "end": 2066.76, "text": " and scalable way.", "tokens": [51480, 293, 38481, 636, 13, 51556], "temperature": 0.0, "avg_logprob": -0.11113787772960233, "compression_ratio": 1.8911564625850341, "no_speech_prob": 0.0035927153658121824}, {"id": 580, "seek": 204292, "start": 2066.76, "end": 2067.76, "text": " And this seems to work well.", "tokens": [51556, 400, 341, 2544, 281, 589, 731, 13, 51606], "temperature": 0.0, "avg_logprob": -0.11113787772960233, "compression_ratio": 1.8911564625850341, "no_speech_prob": 0.0035927153658121824}, {"id": 581, "seek": 204292, "start": 2067.76, "end": 2071.88, "text": " So on all the data sets we tried this construction, it was better than the baseline.", "tokens": [51606, 407, 322, 439, 264, 1412, 6352, 321, 3031, 341, 6435, 11, 309, 390, 1101, 813, 264, 20518, 13, 51812], "temperature": 0.0, "avg_logprob": -0.11113787772960233, "compression_ratio": 1.8911564625850341, "no_speech_prob": 0.0035927153658121824}, {"id": 582, "seek": 207188, "start": 2071.96, "end": 2075.6, "text": " As I said, all we did was change the connectivity, so the number of parameters is exactly the", "tokens": [50368, 1018, 286, 848, 11, 439, 321, 630, 390, 1319, 264, 21095, 11, 370, 264, 1230, 295, 9834, 307, 2293, 264, 50550], "temperature": 0.0, "avg_logprob": -0.15839291346892145, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.1991310864686966}, {"id": 583, "seek": 207188, "start": 2075.6, "end": 2076.6, "text": " same.", "tokens": [50550, 912, 13, 50600], "temperature": 0.0, "avg_logprob": -0.15839291346892145, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.1991310864686966}, {"id": 584, "seek": 207188, "start": 2076.6, "end": 2081.2000000000003, "text": " It's really like an apples-to-apples comparison, and it led to statistically significant results.", "tokens": [50600, 467, 311, 534, 411, 364, 16814, 12, 1353, 12, 1746, 904, 9660, 11, 293, 309, 4684, 281, 36478, 4776, 3542, 13, 50830], "temperature": 0.0, "avg_logprob": -0.15839291346892145, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.1991310864686966}, {"id": 585, "seek": 207188, "start": 2081.2000000000003, "end": 2084.76, "text": " One last point I would like to make is, you know, we're not the only group that tried", "tokens": [50830, 1485, 1036, 935, 286, 576, 411, 281, 652, 307, 11, 291, 458, 11, 321, 434, 406, 264, 787, 1594, 300, 3031, 51008], "temperature": 0.0, "avg_logprob": -0.15839291346892145, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.1991310864686966}, {"id": 586, "seek": 207188, "start": 2084.76, "end": 2089.84, "text": " to study this problem, concurrently to us, the group of Michael Bronstein with Jake", "tokens": [51008, 281, 2979, 341, 1154, 11, 37702, 356, 281, 505, 11, 264, 1594, 295, 5116, 19544, 9089, 365, 15822, 51262], "temperature": 0.0, "avg_logprob": -0.15839291346892145, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.1991310864686966}, {"id": 587, "seek": 207188, "start": 2089.84, "end": 2094.12, "text": " Topping and Francesco DiGiovanni had this great paper on curvature analysis, which was", "tokens": [51262, 8840, 3381, 293, 31441, 1291, 8789, 38, 20675, 35832, 632, 341, 869, 3035, 322, 37638, 5215, 11, 597, 390, 51476], "temperature": 0.0, "avg_logprob": -0.15839291346892145, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.1991310864686966}, {"id": 588, "seek": 207188, "start": 2094.12, "end": 2098.12, "text": " actually one of the best paper awardees at iClear 2022.", "tokens": [51476, 767, 472, 295, 264, 1151, 3035, 7130, 4031, 412, 741, 34, 5797, 20229, 13, 51676], "temperature": 0.0, "avg_logprob": -0.15839291346892145, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.1991310864686966}, {"id": 589, "seek": 209812, "start": 2098.12, "end": 2102.8399999999997, "text": " And basically in this paper, they claim that if you have negatively curved edges, so edges", "tokens": [50364, 400, 1936, 294, 341, 3035, 11, 436, 3932, 300, 498, 291, 362, 29519, 24991, 8819, 11, 370, 8819, 50600], "temperature": 0.0, "avg_logprob": -0.15875255552112547, "compression_ratio": 1.8049645390070923, "no_speech_prob": 0.398873507976532}, {"id": 590, "seek": 209812, "start": 2102.8399999999997, "end": 2108.2, "text": " with very negative curvature, those tend to be the ones responsible for the formation", "tokens": [50600, 365, 588, 3671, 37638, 11, 729, 3928, 281, 312, 264, 2306, 6250, 337, 264, 11723, 50868], "temperature": 0.0, "avg_logprob": -0.15875255552112547, "compression_ratio": 1.8049645390070923, "no_speech_prob": 0.398873507976532}, {"id": 591, "seek": 209812, "start": 2108.2, "end": 2110.72, "text": " of bottlenecks and therefore over-squashing.", "tokens": [50868, 295, 44641, 2761, 293, 4412, 670, 12, 33292, 11077, 13, 50994], "temperature": 0.0, "avg_logprob": -0.15875255552112547, "compression_ratio": 1.8049645390070923, "no_speech_prob": 0.398873507976532}, {"id": 592, "seek": 209812, "start": 2110.72, "end": 2115.92, "text": " So naturally we wanted to connect our expander to this theory, so we computed the curvature", "tokens": [50994, 407, 8195, 321, 1415, 281, 1745, 527, 1278, 4483, 281, 341, 5261, 11, 370, 321, 40610, 264, 37638, 51254], "temperature": 0.0, "avg_logprob": -0.15875255552112547, "compression_ratio": 1.8049645390070923, "no_speech_prob": 0.398873507976532}, {"id": 593, "seek": 209812, "start": 2115.92, "end": 2117.12, "text": " of our graphs.", "tokens": [51254, 295, 527, 24877, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15875255552112547, "compression_ratio": 1.8049645390070923, "no_speech_prob": 0.398873507976532}, {"id": 594, "seek": 209812, "start": 2117.12, "end": 2121.0, "text": " But we found that actually the graphs that we built are negatively curved everywhere.", "tokens": [51314, 583, 321, 1352, 300, 767, 264, 24877, 300, 321, 3094, 366, 29519, 24991, 5315, 13, 51508], "temperature": 0.0, "avg_logprob": -0.15875255552112547, "compression_ratio": 1.8049645390070923, "no_speech_prob": 0.398873507976532}, {"id": 595, "seek": 209812, "start": 2121.0, "end": 2126.48, "text": " So it has a curvature of negative 1 very quickly as you increase the size of the graph, right?", "tokens": [51508, 407, 309, 575, 257, 37638, 295, 3671, 502, 588, 2661, 382, 291, 3488, 264, 2744, 295, 264, 4295, 11, 558, 30, 51782], "temperature": 0.0, "avg_logprob": -0.15875255552112547, "compression_ratio": 1.8049645390070923, "no_speech_prob": 0.398873507976532}, {"id": 596, "seek": 212648, "start": 2126.48, "end": 2131.28, "text": " So obviously, you know, we built a negatively curved graph everywhere, yet it still seems", "tokens": [50364, 407, 2745, 11, 291, 458, 11, 321, 3094, 257, 29519, 24991, 4295, 5315, 11, 1939, 309, 920, 2544, 50604], "temperature": 0.0, "avg_logprob": -0.11879555981858332, "compression_ratio": 1.765472312703583, "no_speech_prob": 0.0025104135274887085}, {"id": 597, "seek": 212648, "start": 2131.28, "end": 2132.28, "text": " to work well.", "tokens": [50604, 281, 589, 731, 13, 50654], "temperature": 0.0, "avg_logprob": -0.11879555981858332, "compression_ratio": 1.765472312703583, "no_speech_prob": 0.0025104135274887085}, {"id": 598, "seek": 212648, "start": 2132.28, "end": 2133.72, "text": " So what gives, right?", "tokens": [50654, 407, 437, 2709, 11, 558, 30, 50726], "temperature": 0.0, "avg_logprob": -0.11879555981858332, "compression_ratio": 1.765472312703583, "no_speech_prob": 0.0025104135274887085}, {"id": 599, "seek": 212648, "start": 2133.72, "end": 2135.48, "text": " We try to analyze this a bit further.", "tokens": [50726, 492, 853, 281, 12477, 341, 257, 857, 3052, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11879555981858332, "compression_ratio": 1.765472312703583, "no_speech_prob": 0.0025104135274887085}, {"id": 600, "seek": 212648, "start": 2135.48, "end": 2139.2, "text": " First we show that the curvature of negative 1 is actually not that small.", "tokens": [50814, 2386, 321, 855, 300, 264, 37638, 295, 3671, 502, 307, 767, 406, 300, 1359, 13, 51000], "temperature": 0.0, "avg_logprob": -0.11879555981858332, "compression_ratio": 1.765472312703583, "no_speech_prob": 0.0025104135274887085}, {"id": 601, "seek": 212648, "start": 2139.2, "end": 2144.86, "text": " Like the theorem in this paper is only invoked when the curvature is close to minus 2.", "tokens": [51000, 1743, 264, 20904, 294, 341, 3035, 307, 787, 1048, 9511, 562, 264, 37638, 307, 1998, 281, 3175, 568, 13, 51283], "temperature": 0.0, "avg_logprob": -0.11879555981858332, "compression_ratio": 1.765472312703583, "no_speech_prob": 0.0025104135274887085}, {"id": 602, "seek": 212648, "start": 2144.86, "end": 2148.52, "text": " So in our case with curvature of negative 1, it's actually not sufficiently negative", "tokens": [51283, 407, 294, 527, 1389, 365, 37638, 295, 3671, 502, 11, 309, 311, 767, 406, 31868, 3671, 51466], "temperature": 0.0, "avg_logprob": -0.11879555981858332, "compression_ratio": 1.765472312703583, "no_speech_prob": 0.0025104135274887085}, {"id": 603, "seek": 212648, "start": 2148.52, "end": 2151.44, "text": " to trigger that failure case of this theorem.", "tokens": [51466, 281, 7875, 300, 7763, 1389, 295, 341, 20904, 13, 51612], "temperature": 0.0, "avg_logprob": -0.11879555981858332, "compression_ratio": 1.765472312703583, "no_speech_prob": 0.0025104135274887085}, {"id": 604, "seek": 212648, "start": 2151.44, "end": 2155.84, "text": " And additionally, we took it a step further and we actually tried to analyze how easy", "tokens": [51612, 400, 43181, 11, 321, 1890, 309, 257, 1823, 3052, 293, 321, 767, 3031, 281, 12477, 577, 1858, 51832], "temperature": 0.0, "avg_logprob": -0.11879555981858332, "compression_ratio": 1.765472312703583, "no_speech_prob": 0.0025104135274887085}, {"id": 605, "seek": 215584, "start": 2155.84, "end": 2158.1600000000003, "text": " it is to satisfy these three properties at once.", "tokens": [50364, 309, 307, 281, 19319, 613, 1045, 7221, 412, 1564, 13, 50480], "temperature": 0.0, "avg_logprob": -0.10282589012468366, "compression_ratio": 1.796875, "no_speech_prob": 0.004979931283742189}, {"id": 606, "seek": 215584, "start": 2158.1600000000003, "end": 2161.4, "text": " So to have sparsity, we said sparsity is good for scalability.", "tokens": [50480, 407, 281, 362, 637, 685, 507, 11, 321, 848, 637, 685, 507, 307, 665, 337, 15664, 2310, 13, 50642], "temperature": 0.0, "avg_logprob": -0.10282589012468366, "compression_ratio": 1.796875, "no_speech_prob": 0.004979931283742189}, {"id": 607, "seek": 215584, "start": 2161.4, "end": 2165.52, "text": " To have a low bottleneck, so a high trigger constant, which would mean you don't have", "tokens": [50642, 1407, 362, 257, 2295, 44641, 547, 11, 370, 257, 1090, 7875, 5754, 11, 597, 576, 914, 291, 500, 380, 362, 50848], "temperature": 0.0, "avg_logprob": -0.10282589012468366, "compression_ratio": 1.796875, "no_speech_prob": 0.004979931283742189}, {"id": 608, "seek": 215584, "start": 2165.52, "end": 2168.56, "text": " these kinds of pathological propagation problems.", "tokens": [50848, 613, 3685, 295, 3100, 4383, 38377, 2740, 13, 51000], "temperature": 0.0, "avg_logprob": -0.10282589012468366, "compression_ratio": 1.796875, "no_speech_prob": 0.004979931283742189}, {"id": 609, "seek": 215584, "start": 2168.56, "end": 2173.56, "text": " And thirdly, to have positive curvature, which seems to be a good idea based on the analysis", "tokens": [51000, 400, 2636, 356, 11, 281, 362, 3353, 37638, 11, 597, 2544, 281, 312, 257, 665, 1558, 2361, 322, 264, 5215, 51250], "temperature": 0.0, "avg_logprob": -0.10282589012468366, "compression_ratio": 1.796875, "no_speech_prob": 0.004979931283742189}, {"id": 610, "seek": 215584, "start": 2173.56, "end": 2174.56, "text": " of this paper.", "tokens": [51250, 295, 341, 3035, 13, 51300], "temperature": 0.0, "avg_logprob": -0.10282589012468366, "compression_ratio": 1.796875, "no_speech_prob": 0.004979931283742189}, {"id": 611, "seek": 215584, "start": 2174.56, "end": 2177.56, "text": " And we actually proved, there is a theorem in our paper that proves that these three", "tokens": [51300, 400, 321, 767, 14617, 11, 456, 307, 257, 20904, 294, 527, 3035, 300, 25019, 300, 613, 1045, 51450], "temperature": 0.0, "avg_logprob": -0.10282589012468366, "compression_ratio": 1.796875, "no_speech_prob": 0.004979931283742189}, {"id": 612, "seek": 215584, "start": 2177.56, "end": 2182.08, "text": " things are incompatible with each other, in that there's only finitely many graphs", "tokens": [51450, 721, 366, 40393, 267, 964, 365, 1184, 661, 11, 294, 300, 456, 311, 787, 962, 1959, 867, 24877, 51676], "temperature": 0.0, "avg_logprob": -0.10282589012468366, "compression_ratio": 1.796875, "no_speech_prob": 0.004979931283742189}, {"id": 613, "seek": 215584, "start": 2182.08, "end": 2185.7000000000003, "text": " that satisfy these three properties simultaneously.", "tokens": [51676, 300, 19319, 613, 1045, 7221, 16561, 13, 51857], "temperature": 0.0, "avg_logprob": -0.10282589012468366, "compression_ratio": 1.796875, "no_speech_prob": 0.004979931283742189}, {"id": 614, "seek": 218570, "start": 2185.7, "end": 2190.8599999999997, "text": " So as you go to large enough input graphs to be sparsed and to have no bottlenecks,", "tokens": [50364, 407, 382, 291, 352, 281, 2416, 1547, 4846, 24877, 281, 312, 637, 685, 292, 293, 281, 362, 572, 44641, 2761, 11, 50622], "temperature": 0.0, "avg_logprob": -0.10654647596951189, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.004750515799969435}, {"id": 615, "seek": 218570, "start": 2190.8599999999997, "end": 2192.8199999999997, "text": " you have to be negatively curved somewhere.", "tokens": [50622, 291, 362, 281, 312, 29519, 24991, 4079, 13, 50720], "temperature": 0.0, "avg_logprob": -0.10654647596951189, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.004750515799969435}, {"id": 616, "seek": 218570, "start": 2192.8199999999997, "end": 2194.54, "text": " It's impossible to avoid it.", "tokens": [50720, 467, 311, 6243, 281, 5042, 309, 13, 50806], "temperature": 0.0, "avg_logprob": -0.10654647596951189, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.004750515799969435}, {"id": 617, "seek": 218570, "start": 2194.54, "end": 2199.2599999999998, "text": " So while we don't study the implications of this any further, we do believe that it calls", "tokens": [50806, 407, 1339, 321, 500, 380, 2979, 264, 16602, 295, 341, 604, 3052, 11, 321, 360, 1697, 300, 309, 5498, 51042], "temperature": 0.0, "avg_logprob": -0.10654647596951189, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.004750515799969435}, {"id": 618, "seek": 218570, "start": 2199.2599999999998, "end": 2205.16, "text": " on the community in the future to study what happens in this gray area where the curvature", "tokens": [51042, 322, 264, 1768, 294, 264, 2027, 281, 2979, 437, 2314, 294, 341, 10855, 1859, 689, 264, 37638, 51337], "temperature": 0.0, "avg_logprob": -0.10654647596951189, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.004750515799969435}, {"id": 619, "seek": 218570, "start": 2205.16, "end": 2207.22, "text": " is negative but not too negative.", "tokens": [51337, 307, 3671, 457, 406, 886, 3671, 13, 51440], "temperature": 0.0, "avg_logprob": -0.10654647596951189, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.004750515799969435}, {"id": 620, "seek": 218570, "start": 2207.22, "end": 2210.8599999999997, "text": " Because it seems like something like that might be critical to having the most optimal", "tokens": [51440, 1436, 309, 2544, 411, 746, 411, 300, 1062, 312, 4924, 281, 1419, 264, 881, 16252, 51622], "temperature": 0.0, "avg_logprob": -0.10654647596951189, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.004750515799969435}, {"id": 621, "seek": 218570, "start": 2210.8599999999997, "end": 2212.66, "text": " message passing possible.", "tokens": [51622, 3636, 8437, 1944, 13, 51712], "temperature": 0.0, "avg_logprob": -0.10654647596951189, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.004750515799969435}, {"id": 622, "seek": 221266, "start": 2212.66, "end": 2214.98, "text": " And that is basically the rough summary of our work.", "tokens": [50364, 400, 300, 307, 1936, 264, 5903, 12691, 295, 527, 589, 13, 50480], "temperature": 0.0, "avg_logprob": -0.3866102695465088, "compression_ratio": 0.8813559322033898, "no_speech_prob": 0.5675889253616333}], "language": "en"}