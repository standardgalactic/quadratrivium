start	end	text
0	6080	Peter Velichkovich is a staff research scientist at DeepMind.
6080	11040	He's firmly established himself as one of the most significant and up-and-coming researchers
11040	13280	in the deep learning space.
13280	18720	He invented graph attention networks in 2017 and he's been a leading light in the field
18720	24800	ever since, pioneering research in graph neural networks, geometric deep learning and also
24800	26800	neural algorithmic reasoning.
26800	31640	Recently he's been applying category theory to take the geometric deep learning ideas
31640	33400	one step further.
33400	36760	If you haven't already, you should check out our show that we did on the geometric deep
36760	41480	learning blueprint, which of course featured Peter and I caught up with him last week
41480	42480	at NeurIPS.
42480	43480	Enjoy.
43480	45720	Peter, it's fantastic to see you again.
45720	48600	So this is the first time that I've actually met you in person.
48600	52680	We did that really cool show together on geometric deep learning with your proto book with Takko.
52680	58400	I spoke with Takko yesterday, but also Michael Bronstein and Joanne Brunner.
58400	62480	So anyway, it's been a little while since we've really synced.
62480	67040	Now you've had this really, really interesting category theory series.
67040	70160	Can you start by just letting us know what you've been doing there?
70160	75640	Yeah, that's a great point and great to finally meet you in person, Tim.
75640	79200	It's really great to catch up after some time has passed.
79200	85440	And yeah, I mean, I like to think that all four of us, myself, Michael, Joanne and Takko
85440	89280	have a greater understanding of the implications of these methods since the last time we spoke.
89280	93520	If you remember back when we did our conversation, I kind of hinted at the fact that category
93520	99280	theory might hold some of the answers to maybe generalize some of these geometric concepts
99280	102560	beyond the notion of just pure symmetries.
102560	107200	And we believe that now we have a sufficient understanding of these kinds of things that
107280	110720	we were able to make this kind of mini course on categories for deep learning.
110720	115680	And to me, it really feels like the natural continuation of these concepts of geometric
115680	118040	deep learning into the realm beyond.
118040	120720	And I'll explain that in a moment.
120720	125720	But one other kind of very related point is that here at NeurIPS, we're actually presenting
125720	130920	a full conference paper which deals with using category theoretic tools to invent new kinds
130920	131920	of graph neural networks.
131920	135920	So basically, it's not just that we're throwing a bunch of new theory, it actually leads to
135920	140120	empirical findings that we can actionably use in our models day to day.
140120	141560	So that's one point.
141560	142560	That is incredible.
142560	144040	Can you sketch out the paper?
144040	145040	Yeah, sure.
145040	150920	So basically, maybe I'll first take a step back to explain why do we think categories
150920	154760	are important and in what sense they're kind of a step further from what geometric deep
154760	156200	learning already gives us.
156200	160840	So geometric deep learning concerns itself with giving us these equivalent layers, right?
160840	165640	So layers that are in some sense resistant to operations of these symmetry transformations
165640	168860	is that fundamentally change an object, but the object is still the same.
168860	170400	We still have all of it, right?
170400	174680	And this immediately implies that these symmetries have to be composable, invertible, all that
174680	176200	sort of stuff.
176200	183600	And yeah, essentially, the category theory framework is in some sense mindful of the
183600	188560	fact that while symmetries are a very nice way to reason about things that happen and
188560	194800	that we see in nature, they're often not completely an accurate representation of what happens.
194800	199240	Very often there are operations both in nature, but especially in general computation, like
199240	204080	say in algorithmic stuff, where an operation of an algorithm might destroy half of your
204080	205080	data.
205080	206080	So that is no longer a symmetry.
206080	207080	You cannot invert it.
207080	211360	But you might still be interested in building a neural network model that is in some sense
211360	217640	resistant to the operations of say this algorithm or this natural phenomenon that you're studying.
217640	222440	One simple example that maybe predates our work a little bit is building some kind of
222560	225520	equivalence to scaling operations.
225520	229480	Obviously if you scale or course in something, these are not always invertible transformations
229480	233920	because if you course in the pixels of your image, you cannot perfectly reconstruct where
233920	234920	you came from.
234920	238280	Yet you still might want to build a model that will give you the same answer as regardless
238280	240560	of how you scale up your input, right?
240560	244360	So these are obviously things that are going to be very important as we move to more generic
244360	249640	domains than ones that can be described purely through geometric and symmetry transformations,
249640	250640	right?
250640	256400	And in that sense, the same way we had groups, representations, and equivalence in geometric
256400	261040	deep learning, these are all special cases of categorical concepts like categories,
261040	265920	functors, and natural transformations, which basically generalize all the stuff of geometric
265920	268000	deep learning into their own beyond.
268000	273360	And in our paper, we try to use exactly these kinds of category theoretic tools to study
273360	278240	what it would mean to build a say graph neural network that is capable of behaving like a
278240	280480	classical computer science algorithm.
280480	284920	In the sense that if you have some data that's transformed by an algorithm, you may imagine,
284920	289080	say, a path finding algorithm where at every step in every node, you have your knowledge
289080	292160	of how far away is that node from the source vertex.
292160	295400	And one step of the algorithm kind of looks at all the immediate neighbors and updates
295400	297360	those beliefs of how far away you are.
297360	300720	And I'll say you want to have a GNN that simulates that, like we typically do in algorithmic
300720	301720	reasoning.
301720	305880	You take your algorithmic state, you encode it with a neural network into this high dimensional
305880	306880	space.
306880	309680	Your GNN then processes it to update the latent space.
309680	313600	And now you want to be able to decode it so that you predict whatever the next state
313600	314600	is going to be.
314600	319080	So you have something which in category theory we use a lot is known as a commutative diagram.
319080	322640	So basically it's saying you can either take the step of the algorithm or you can encode,
322640	325360	process, decode, and hopefully end up in the same place.
325360	328400	So category theory seems like a very nice language to study.
328400	334320	These kinds of, I won't call them symmetries, they're basically like interchangeable sequences
334320	339040	of operations because the step of an algorithm might not be invertible.
339040	343240	You might not be able to go back after you do one step of, you know, shortest path algorithm
343240	345280	because it's a contraction map, right?
345280	348880	When you find the final solution of a shortest path algorithm, you won't necessarily know
348880	353560	which previous state led you there because there could be many equivalent states that
353560	356340	could lead you to the same contracted solution, right?
356340	361280	So our method, using these category theory frameworks, try to characterize how these
361280	366520	graph neural networks align with a target algorithm that we might want to simulate.
366520	370520	Then we detect various ways not only to explain the code of graph neural networks from this
370520	374360	kind of perspective, but also it gives us a very interesting sort of, if you've done
374360	378760	any functional programming, a type checker of sorts to kind of detect whenever we're
378760	382040	using our representations in slightly broken ways.
382040	387160	So specifically to give you one very concrete example, in a categorical framework, just
387160	390760	like in functional programming, you expect your transformations to be functions.
390760	393840	That is, for every input there should be a unique output.
393840	397600	However, one thing that people very often do in graph representation learning, when
397600	402040	they want to predict outputs not only in the nodes, but also in the edges, is to reuse
402040	407520	the edge messages both as edge outputs and integrated overall the other messages to get
407520	409040	node outputs, right?
409040	413000	But this is a problem from the categorical perspective because this is no longer a function.
413000	418640	You cannot get a function that takes, you know, edges to edges plus nodes without sending
418640	421760	the same thing into two different places in this case, right?
421760	425200	And, you know, just because it mathematically breaks doesn't mean you cannot implement it.
425200	432000	In fact, 99% of the GNN implementations you'll find online will do this exactly in this particular
432000	433000	way.
433000	435280	DeepMindsGraphNet's library does this, for example.
435280	438800	However, you know, just because you can implement it doesn't mean that there's something not
438800	442840	potentially a bit tricky going on in the sense that you're putting a bit of representational
442840	445120	pressure on that edge message, right?
445120	449560	Because now it has to be used for two potentially very different things, both for some output
449560	454320	in the edges, but also it needs to be integratable into nodes where it predicts something potentially
454320	455320	wildly different.
455320	459360	And, you know, while gradient descent can take care of this and give you a model that
459360	463520	fits your training distribution well, you're not like, to deal with this pressure, it's
463520	466240	probably going to have to learn something which has nothing to do with the algorithm
466240	467840	that you want to align to.
467840	471440	And as a result, you're out of distribution, extrapolation performance is going to be much,
471440	472440	much worse.
472440	475120	And any self-respecting algorithm should extrapolate well.
475120	477400	That's the main property of algorithmic reasoning, right?
477400	481160	And we find that just by, you know, splitting this message function into two streams, one
481160	484920	which goes into the edges and one which goes into the nodes, we get basically significant
484920	488240	empirical benefits when extrapolating on edge-centric algorithms.
488240	489240	Yeah.
489240	490240	Amazing.
490240	493520	So, Epeta has just produced this incredible series which is available on YouTube.
493520	494720	Where can folks find it?
494720	495720	Yeah.
495720	503160	So, basically, if you just go to cats.4.ai, you can see all of the main series lectures
503160	510560	from our course, which starts off with assuming kind of a foundational knowledge of deep learning
510560	515600	with neural networks, back propagation, and so on, and then also tries to introduce these
515600	521000	concepts of category theory and how we can use them to rethink the way we might go about
521000	525480	some of our standard ideas in deep learning like compositionality or functional structure
525480	530560	of deep learning pipelines, or even how can we reinterpret back propagation from the
530560	533200	perspective of categorical theory.
533200	538960	And each lecture basically deals with one particular aspect and we try to keep it grounded
538960	543200	from the beginning to keep it motivated so every single lecture is aligning itself with
543200	547840	one particular top-tier paper that one of us has published on one of these venues like
547840	549080	in Europe.
549080	552520	And one thing I'll also mention is that the course is actually, in principle, still ongoing
552520	558520	because besides the main series of five lectures that myself, Bruno, Pym, and Andrew have given,
558520	564160	we also have several interesting guest lectures where we try to bring in other influential
564160	570120	people at the intersection of popularizing category theory with deep learning concepts
570120	575640	in a way that can bring an even wider area of views once you're kind of trained in the
575640	579480	basics of these techniques, how they're applied to various other things like causality.
579480	585200	We had Taco Cohen tell us about how he uses these concepts to reimagine causality through
585200	586760	a categorical lens.
587720	592760	We're going to have Tydenay Bradley, she's a very popular mathematics educator generally.
592760	597040	She will show how she used some of these concepts to explain transformers.
597040	601280	And one thing I'm very excited about early next year, we will have actually a guest talk
601280	607600	from David Spivak, which is one of the co-authors of the very famous Seven Sketches in Compositionality
607600	611040	book, which is what initially one of the things that got me really excited about category
611040	612880	theory in the first place.
612880	615480	So I'm really keen to hear all these perspectives as well.
615640	616520	The man is a legend.
616520	620920	And also on Taco, I interviewed him yesterday and his work on causality is really, really
620920	622120	exciting.
622120	626560	What would you say to people who might be intimidated or scared by category theory?
626560	631600	So one thing that I should mention here is that one point about being intimidated or
631600	636360	scared about category theory is that to really be able to utilize these ideas in how you
636360	642160	do research or build your models or anything, it does require a reasonably significant buy-in.
642200	646360	So this is not something that you can just read one blog post and suddenly you're empowered
646360	647360	to do it.
647360	648920	This is like one key thing.
648920	653440	But I would say the main thing that might make people a bit scared to do it is the fact
653440	659520	that many category theory resources out there are a bit guided towards mathematicians.
659520	663960	So they will tend to use the kind of language and the kind of examples that will be quite
663960	668560	attractive to someone who has studied, say, various kinds of differential geometry or
668560	670080	topology or something like this.
670080	674520	And these kinds of areas tend to generally scare off people who come from a more computer
674520	676520	science style background.
676520	681280	And basically I would say the answer to that is you need to find the right resource for
681280	682280	you.
682280	686680	Category theory is no more or no less than a way to take a bird's eye view of the phenomena
686680	688200	that you try to study.
688200	693120	And when you study these phenomena from high in the sky, details become invisible, but
693120	696960	you suddenly get a much better feel for the structure and you can utilize kind of the
696960	700560	nice patterns that reappear across various fields.
700560	704320	And this you would argue is kind of the essence of what we're trying to do in deep learning.
704320	708080	We have a lot of analogical way in which these architectures are constructed, right?
708080	711200	So cats for AI is one possible answer to that.
711200	715520	It's our way to kind of, as half of us are deep learners and half of us are category
715520	718480	theorists, trying to apply these techniques to deep learning.
718480	723280	We believe we have a sort of unique perspective of we and like we understand what makes people
723280	727480	afraid to try to talk about these things because some of us had to go through it ourselves
727480	731760	to deal with the way in which the materials are arranged online right now.
731760	737880	So yeah, maybe just these kinds of resources, starting with them and basically trying as
737880	743440	much as possible not to descend into the depths of NCAT lab as the very first thing that you
743440	749320	do can be a good way to maybe stay sane during the first few weeks or months of trying to
749320	750320	explore this field.
750320	751320	Wonderful.
751320	754800	I wondered if you could give a couple of examples of where category theory has been used in
754800	755800	an adjacent field.
755800	756800	I can think of too.
756800	761120	I can think of Rosen using category theory to describe, you know, sort of ecosystems
761120	762120	and life.
762120	766920	I can also think of some quantum mechanics folks that have come up with a category theoretical
766920	769120	conception of quantum mechanics.
769120	770120	Right.
770120	771120	Are there any other ones?
771120	772120	Yeah.
772120	776480	So I mean, I can start by giving the examples that I know about closest in terms of just
776480	777480	deep learning.
777480	783560	So one particular example that I think could be quite interesting is the work that was
783560	787000	published at NeurIPS two years ago, which I think is one of the first papers that really
787000	792000	tried to use categorical concepts to build these structures, is the natural graph networks
792000	798800	paper from Pimdehan, Tapocoin and Max Swelling, which effectively realizes the fact that the
798800	803200	way we build graph neural networks very often we have this one shared message function that's
803200	807760	applied everywhere on every single edge on every single graph that you get.
807760	811520	But in reality, is this necessary for it to be a legitimate graph neural network?
811520	816780	That's actually not the case because if I give you two completely non-isomorphic graphs,
816780	820400	if I choose to have completely different message functions in those two graphs, that's totally
820400	822440	fine because it's still a valid graph net.
822440	826240	If I permute any of those graphs, I'll get the permutation equivalent function for the
826240	827560	two of them separately.
827560	831760	There needs to be no weight sharing between them and naturally concepts like these.
831760	837720	So this kind of requires taking a step above the group theoretic view of geometric deep
837720	840840	learning and into the realm of what is known as a group poid.
840840	846160	You kind of imagine every single graph structure, isomorphic graph structure, living on a sort
846160	850120	of island of possible adjacency matrix representations of it.
850120	854320	And for those graphs living on those islands, you need to have some weight sharing.
854320	858080	But for separate islands, you don't need to have any weight sharing whatsoever.
858080	862080	Of course, in practice, these kinds of layers, you would need to have some kind of sharing
862080	865680	of weights in order to make them scalable to arbitrary new graph structures you haven't
865680	870480	seen at training time, but it allows you a lot more flexibility about how you go about
870480	871480	building your functions.
871480	875080	And you're no longer constrained to have just one function everywhere repeated, right?
875080	879240	So that's maybe one example that, at least to me, was what first motivated me and made
879240	883640	me realize that there's more to this stuff than just to say what group theory will give
883640	884640	us.
884640	885640	Amazing.
885640	888840	I'm really interested in your work in algorithmic reasoning, and I know you were just discussing
888840	894040	it as an adjacent thing, and very soon we want to make a show, actually, on your work
894040	895040	on that.
895040	897480	But if you wouldn't mind, could you just sketch out algorithmic reasoning?
897480	898480	Yes, wonderful.
898480	900200	So, very happy to.
900200	905480	Basically, what are we interested in algorithmic reasoning is building neural networks.
905480	908840	They tend to be graph neural networks, but generally speaking, neural networks that are
908840	912480	capable of executing algorithmic computation.
912480	917240	So if I give you some context on what is the state of a particular algorithm, can my network
917240	922320	somehow learn to execute that algorithm ideally in some latent space such that at every single
922320	926920	step of the way, I could if I wanted to decode the states of that algorithm.
926920	929200	So that's basically the main premise.
929200	930680	Why do we care about this?
930680	935680	Well, basically, I think of algorithms as a sort of basic foundational building block
935680	940760	of reasoning, and it's kind of a timeless principle where a software engineer reads
940760	945760	through one of these textbooks on algorithms and learns these 30 or 40 basis algorithms,
945760	950040	and then that knowledge serves them for life in a whole career of software engineering.
950040	954800	So basically, we have this hypothesis that you have this nice basis of algorithms that
954800	959760	if you can master how to do them robustly, you can try to mimic any kind of at least
959760	962080	polynomial time reasoning behavior.
962080	966560	And that's really nice because if you look at the way current state-of-the-art large-scale
966560	972320	models tend to have shortcomings, it's usually in those kinds of robust extrapolation problems.
972320	978680	Basically, if we want to have a really good AI scientist that's able to not just make
978680	983280	great sense of a bunch of training data from the internet, but also use that training data
983280	988640	to derive new knowledge, you need some robustified way to apply rules to get infinite knowledge
988640	990440	from finite means.
990440	992320	So basically, that's what we want to do.
992320	996440	We want to find ways inductive biases or training procedures to build neural networks
996440	999200	that are more algorithmically capable.
999200	1004240	And in algorithmic reasoning, we obviously spent a lot of time trying to make this happen,
1004240	1008600	just building better graph neural networks that align better with target algorithms so
1008600	1013400	that you can execute them better, but then the really exciting part comes where we've
1013400	1017400	actually taken some of these graph neural networks that have been pre-trained to execute
1017400	1022480	one particular algorithm, and then we deployed it in a real-world problem where that algorithm
1022480	1027260	is required, and we achieved, say, significant representational benefits in terms of downstream
1027260	1028260	accuracy.
1028260	1031440	So the idea behind this, and I'll give an example from Google Maps.
1031440	1036040	This is an application that I worked on at DeepMind, so it's something that I've thought
1036040	1038040	about quite a bit.
1038040	1043420	We've invented these algorithms, like Dijkstra's algorithm, to be able to resolve these kinds
1043420	1045840	of real-world routing problems.
1045840	1049440	That's the kind of motivation for why you want to build the shortest path algorithm.
1049440	1053480	And it comes as a little surprise that when you have real-world traffic data, you might
1053480	1059000	be tempted to apply Dijkstra's algorithm to solve it, to route agents in traffic.
1059000	1063280	However, what is the actual data that, say, Google Maps has access to?
1063280	1067280	It's not this nice, abstractified graph with a single scalar in every edge where you can
1067280	1069320	just go ahead and apply an algorithm.
1069320	1073360	In fact, there's a huge bridge that must be built between the real data and the input
1073360	1074360	to the algorithm.
1074360	1079160	In fact, Google Maps data is typically people's cell phones in their cars, and the cars move,
1079160	1083440	the phones move, and then based on the movement of the phones, you somehow infer how fast the
1083440	1085880	car is going or something like that.
1085880	1091240	And this is very noisy, not very well-structured, and you have to somehow go from there to a
1091240	1093600	graph where you can apply this heuristic.
1093600	1098640	Previously, it was always done exclusively by humans, like feature engineers, effectively.
1098640	1102760	And whenever there's a human feature engineer in the loop like this, you are almost certainly
1102760	1106280	going to drop a lot of information that you might need to solve the problem.
1106280	1109560	So basically, you have a huge kind of bridge to cross there.
1109560	1112600	And with algorithmic reasoning, we now don't use Dijkstra's algorithm.
1112600	1117440	We use a high-dimensional graph net that was pre-trained to execute Dijkstra's algorithm
1117440	1118600	in a latent space.
1118600	1122640	So now this gives us a differentiable component that we can hook up to any encoder and decoder
1122640	1127520	function we want to, so we can go straight from raw data and code it into the GNN's latent
1127520	1131920	space, run the algorithm there, and then decode whatever it is that you need, like routing
1131920	1133680	the vehicles in traffic.
1133680	1138600	So now purely through backprop, this encoder function now learns to do what the human feature
1138600	1139600	engineer did.
1139600	1144560	It learns how to most effectively map that complicated, noisy, real-world data into the
1144560	1147280	latent space where this GNN can best do its thing.
1147280	1148440	That really is software 2.0.
1148440	1153120	But I wanted to ask you about the computational limitations, because you just said something
1153120	1157640	interesting about representing infinite objects with a finite memory.
1157640	1163960	So neural networks are not Turing machines, but they can extrapolate, of course.
1163960	1165520	What's the realistic limitation?
1165520	1170560	Let's say you're trying to learn an algorithm, how far can you go with a neural network?
1170560	1173880	So the thing is, there are cases where you can go very far.
1173880	1178600	We do have theory that is very robust about this, and I think it's theory that is actually
1178600	1180600	quite easily understandable.
1180600	1184200	So let me try to kind of visualize it.
1184200	1188360	When you have a real UMLP, your standard universal approximator, it's basically a piecewise
1188360	1189800	linear function.
1189800	1194920	So as you go far enough away from the training data, you're going to hit that level of extrapolation
1194920	1197680	where you hit the linear part of the piecewise linear.
1197680	1201840	And at that point, if your target function is not linear, no extrapolation is going to
1201840	1202840	happen.
1202840	1204640	You're not going to fit the function properly.
1204640	1208880	So what's one outcome of this theory is that if you use real UMLPs, this was a great paper
1208880	1214800	from MIT a few years back, which showed that basically you need to line up parts of your
1214800	1218440	neural network such that they learn linear functions in the target.
1218440	1221880	And that's the reason why, say, when you want to imitate a pathfinding algorithm, you want
1221880	1226760	to use a max aggregation, your GNN, and not sum, where sum is universal.
1226760	1228120	It can fit anything.
1228120	1231960	But the function you have to learn, because pathfinding is like minimum overall neighbors
1231960	1236080	of distance to neighbor plus the edge weight, suddenly when you put max in there, it's a
1236080	1237080	linear function.
1237080	1240320	When you put sum in there, it's a highly nonlinear function, so it's going to extrapolate much
1240320	1241320	worse.
1241320	1245800	Now, there's been some great follow-up work on this from Beatrice Bevilacqua, Bruno
1245800	1247920	Ribeiro from Purdue University.
1247920	1252040	That was at ICML a few years back, which showed that this idea with, like, you want linear
1252040	1257160	targets with real UMLPs, it's really just a special case of a more general idea that
1257160	1262960	if you want to extrapolate, say, on different sizes of graphs, you need to have some implicit
1262960	1266640	causal model of what your test data is going to look like.
1266640	1270560	This linear algorithmic alignment is just one special case of a causal model like that.
1270560	1275720	So basically, if you line things up properly from a causal perspective, you should, in
1275720	1277280	principle, be able to extrapolate.
1277280	1281200	I mean, we have a clear nonparametric evidence that you can extrapolate is the algorithm
1281200	1282200	itself, right?
1282200	1286800	Now, the key is to find the right sweet spot between full universal approximator MLPs and
1286800	1288280	algorithms on the other side, right?
1288280	1289280	Interesting.
1289280	1290280	I spoke to Jan the other day.
1290280	1294000	He had a paper a couple of years ago about extrapolation in neural networks, saying they
1294000	1295000	always extrapolate.
1295000	1296000	Yes.
1296000	1299080	And speaking with Randall Belastriero, and he's got this paper, the Spline Theory of
1299080	1303680	Neural Networks, which is about, you know, these input sensitive polyhedra in the ambient
1303680	1304680	space.
1304680	1308520	And I always took that to mean why they're quite interpolative and it's just an affine
1308520	1310160	transformation for a single input.
1310160	1315080	But what he's shown, though, is that actually, even an MLP with relus is extremely extrapolative
1315080	1320560	because you can remove a whole bunch of data and, depending on how you've designed the
1320560	1324320	network architecture, it will still inform that region that you've taken away.
1324320	1328080	So, I mean, are you familiar with the Spline Theory and do you think it's a useful framework?
1328080	1329080	Yes.
1329080	1333320	So, one thing I would say, the way I understand Jan's paper, it could be that I missed some
1333320	1337040	detail, but the way I understand it is that here we're talking about interpolation and
1337040	1339600	extrapolation with respect to the geometry of the data.
1339600	1343840	So, like, you take, say, the convex hull of all the training points and then, yeah, it's
1343840	1347760	very common, especially in these high dimensional image spaces, right?
1347760	1351840	It's very easy to push one dimension sufficiently to escape the convex hull of what you've seen
1351840	1352840	so far.
1352840	1356720	So, I guess when I say extrapolation out of distribution, I'm actually maybe thinking
1356720	1361640	of a more probabilistic argument, so something like if you think of the probability distribution
1361640	1365480	induced by the training set, which obviously allows you to extrapolate away from the convex
1365480	1366600	hull, right?
1366600	1370920	But if you go sufficiently far from the modes of that distribution, so you explore a part
1370920	1375960	of the space that hasn't really been covered, you know, from a probabilistic mass point
1375960	1380440	of view in the training data, that is what we're actually thinking of when we say out
1380440	1381440	of distribution generalization.
1381440	1385840	But, yeah, I fully agree with you, like, in terms of just convex hull arguments, we very
1385840	1389360	often ask these regular MOPs to go beyond the convex hull, and they seem to work quite
1389360	1391280	well in those regimes.
1391280	1395920	But here, I'm talking really about going, like, significantly beyond the convex hull
1395920	1399160	to, like, some region that really wasn't touched.
1399160	1403960	And what we do, for example, in our papers is we train on 16 node graphs to execute these
1403960	1408240	algorithms, and then we test it on four times larger, 64 node graphs.
1408240	1411920	And what this means, because an algorithm might have, say, n-cubed time complexity, it means
1411920	1415560	the trajectory over which you have to roll it out is also much, much longer than what
1415560	1416680	you've seen in training time.
1416680	1421240	So it's really a test of, like, very different conditions than what you've seen in training
1421240	1422240	time, right?
1422240	1423240	That's interesting.
1423240	1425520	And first of all, I completely agree with you that this binary convex hull notion of
1425520	1428720	extrapolation probably isn't particularly useful.
1428720	1433320	But, you know, folks like Francois Relais describe the way Neuron Network's work is kind of bending
1433320	1435960	the space, you know, progressively with layers.
1436080	1439600	I really like this polyhedra idea.
1439600	1443600	Contrast the algorithmic reasoning with GNNs, so, I mean, I spoke with Hattie from a Google
1443600	1448280	brain team the other day, she's doing the in-consex prompting, you know, sort of algorithm
1448280	1449280	learning.
1449280	1451720	How would you contrast those two approaches?
1451720	1457520	So basically, I would really like these approaches to be reconciled going forward in the sense
1457520	1462520	that, like, I don't see them as going one without the other, if that makes sense.
1462520	1468720	So on one side, and I'm going to invoke the same principles I mentioned during our MLST
1468720	1472400	episode, you know, Daniel Kahneman's book, System 1 and System 2, right?
1472400	1474080	I think you cannot have one without the other.
1474080	1478680	So you have these amazing large-scale perceptive models that are really amazing at, you know,
1478680	1482880	taking the complexities of the real world and somehow getting interpretable enough concepts
1482880	1486760	out of there that they can, you know, make sense of what's going on and, like, drive many
1486760	1491440	interesting real-world decision-making problems, although they might lose a little bit on having
1491480	1496000	to do something like what an AI scientist would be expected to do, which is, like, extrapolate
1496000	1498520	and generate new concepts out of what they've seen.
1498520	1503640	And as you said, these kinds of specifically tailored prompts might enable the model to
1503640	1508800	take things a step or two further, but it's always, like, it's kind of, in spirit, it's
1508800	1512920	the same thing as algorithmic reasoning, because we teach a model to execute an algorithm by
1512920	1515480	forcing it to imitate the algorithm step by step.
1515480	1519720	Here you prompt a language model by telling it what are some of the steps, like, just
1519760	1523280	like you're trying to teach a student how to solve a homework, right, telling them the
1523280	1526640	individual steps they need to do, and then letting the language model go off on its own
1526640	1527640	to solve it.
1527640	1532040	But where I see the real future of these two methods converging is you're going to have
1532040	1536560	your system one component that gets your concepts out very nicely, cleanly.
1536560	1540040	And then those concepts, because we're working with transformers nowadays anyway most of
1540040	1542440	the time, are going to be very slot-based.
1542440	1546640	So that plays very nicely with GNNs, which expect nodes as input, right, so you can maybe
1546640	1551520	hook up in some nice way those concepts into a graph neural network that was trained to
1551520	1555680	execute a bunch of algorithms, and then, you know, kind of get the best of both worlds.
1555680	1560320	So have your perceptual component do the perception, and maybe prompt it as well to kind of do
1560320	1564960	it in a particularly step-by-step manner, and then further have a robust component that
1564960	1568880	makes you not have to relearn all those things that neural networks we know theoretically
1568880	1572400	cannot learn to do that well because of these extrapolation arguments.
1572400	1575200	Maybe one last point I would make to kind of cement this.
1575200	1578880	If you've been around the archive recently, you might have seen our paper on a generalist
1578880	1585080	neural algorithmic learner where we have actually used GATO-style ideas to train one graph neural
1585080	1589800	network that can execute 30 very diverse algorithms all in the same architecture with a single
1589800	1594480	set of weights, so sorting, searching, pathfinding, dynamic programming, comics, hauls, all those
1594480	1597240	kinds of nice things, very diverse ways of reasoning.
1597240	1601120	We believe something like that could maybe be a basis of, say, a foundation model of
1601120	1605160	reasoning in the future that could nicely hook up to the foundation models we already
1605160	1607280	know and love in the realm of perception.
1607280	1608280	Amazing.
1608280	1610760	And what's the biggest research challenge for you next year?
1610760	1616120	So next year, I would really like to show to what extent these things can scale in the
1616120	1617120	real world.
1617120	1622000	So we already have several isolated papers that showed that these ideas can work on
1622000	1623000	real problems.
1623000	1625660	We have Excelvin where we applied it to reinforcement learning.
1625660	1627560	That was in Europe Spotlight last year.
1627560	1630800	We have RMR where we applied it to self-supervision problems.
1630800	1635760	We also have one paper currently under review at iClear where we successfully applied to
1635760	1636760	supervised learning.
1636760	1641040	So we say pre-trained on a flow algorithm and we deploy it on brain vessel segmentation
1641040	1642680	tasks and stuff like that.
1642680	1646640	So we have many isolated cases where you learn a particular algorithm and it works really
1646640	1649400	well in a real world scenario.
1649400	1654240	I would like to see how can we take this idea and truly put it to the test at larger scales,
1654240	1659640	both in terms of number of problems we attack or number of nodes that we support or anything
1659640	1660640	in between.
1660640	1661640	Amazing.
1661640	1665640	Dr. Patovaličković, let's just, we'll get a shaking handshot.
1665640	1666640	All right.
1666640	1669120	Thank you so much for joining us.
1669120	1670120	Thank you for having me.
1670120	1671120	I really appreciate it.
1671120	1677920	Dr. Ishan Mizra of Meta and Lex Friedman fame came over and had a chat with us.
1677920	1681640	Ishan is one of the world's leading experts in computer vision.
1681640	1683880	So what was your paper about?
1683880	1689440	Yeah, basically we try to have global propagation, the likes of which you see in transformers,
1689440	1693360	not like with sparse costs.
1693360	1697960	So but in a way that will still allow you to have nice global communication properties
1697960	1699720	and no bottlenecks and stuff like that.
1699720	1705400	So we basically have this idea of you could generate these expander graphs which allow
1705400	1708400	you to have nice sparsity properties.
1708400	1713000	So basically every node I think has degree four in the graphs we compute and you need
1713000	1716400	only logarithmically many steps to traverse the graph, which means you can still do it
1716400	1719480	efficiently with a small number of steps.
1719480	1722240	And yeah, it seems to empirically work well on a bunch of graph benchmarks.
1722240	1727240	So yeah, it's a, I think it's only scratching the surface of what we can do because we literally
1727240	1732040	just generate a graph at random and slap it onto like mask the computations, but yeah,
1732040	1733040	it's an interesting start.
1733040	1734040	Very nice.
1734040	1735040	Yeah.
1735040	1736040	How about your conference?
1736040	1737040	How's it been?
1737040	1738040	So it's been pretty good.
1738040	1741280	We're organizing the self supervised learning workshop tomorrow, which is going to be probably,
1741280	1745600	I hope like useful to a lot of people, we're going to have a bunch of speakers coming from
1745600	1749880	vision, language, NLP, like speech and so on.
1749880	1753920	And yeah, we're also presenting a poster there, which is about learning joint image and video
1753920	1757800	representations, which are state of the art across image and video benchmarks using a
1757800	1759300	single model.
1759300	1763120	On the final day of the conference, I caught up with Petra again at the poster session for
1763120	1768440	new reps, which is the symmetry and geometry and neural representations group.
1768440	1772720	And his paper was selected by all of the reviewers at the conference as being in the
1772720	1779440	top 10, which is super impressive, but this is Petra talking about his paper.
1779440	1785080	So in the expander graph propagation work, we are trying to solve what is, in my opinion,
1785080	1789360	one of the most important problems in graph representation learning currently unsolved,
1789360	1791320	which is the oversplashing problem.
1791320	1796160	And effectively it is a task, which it's a problem which plagues graph neural networks
1796160	1799900	regardless of which parameters you choose or which model you choose.
1799900	1804020	It's really something that often depends on the topology of the graph, and it's a situation
1804020	1809940	where no matter how hard you try, no matter which parameters you set, the amount of features
1809940	1813760	you would need to compute, so the size of your latent space would have to be exponential
1813760	1817780	in the number of layers for the pairs of nodes to efficiently communicate.
1817780	1821760	We don't always know when it happens, but very often it tends to happen around these
1821760	1822760	bottlenecks.
1822760	1826420	So basically in this particular graph, you have these two communities that are tightly
1826420	1831040	connected, and you have this just one critical edge connecting them, and this edge is now
1831040	1832640	under a lot of pressure.
1832640	1837860	If you want data from these nodes to travel to these nodes and vice versa, this edge has
1837860	1844180	to be mindful of a lot of things, so the size of the feature space required for this edge
1844180	1847540	grows exponentially, and things get even worse when you look at trees.
1847540	1852840	Trees are like the canonical worst case example, where cutting off this edge would really trigger
1852840	1858260	all sorts of bottleneck cases, and essentially you need basically a number of, to store information
1858260	1864240	about a number of nodes that goes exponentially in the number of steps, just to be able to
1864240	1865960	travel to the other side of the tree.
1865960	1869960	So this is a fundamental problem of propagating data, which has nothing to do with the choice
1869960	1873100	of model, just topology.
1873100	1875860	And what do we try to do to fix this problem?
1875860	1879720	You would ideally want, so first we start off with the assumption that this kind of global
1879720	1881580	talking is actually beneficial.
1881580	1885520	Of course there are some tasks where you might not want data to travel in this way, because
1885520	1890560	if it's a highly homophilus data-driven problem, then you might want information to stay in
1890560	1892760	the community, to not get diluted.
1892760	1896800	But we assume in many tasks, like say molecular property prediction tasks, you actually want
1896800	1900560	data to travel globally, so that's exactly what we do.
1900560	1902000	That's our first assumption.
1902000	1905280	As we just described, we don't really want these bottlenecks to exist, because if there's
1905280	1909800	a bottleneck, no matter what you do with the model, it's not going to work well.
1909800	1913800	We would ideally want the complexity to be scalable, so we can apply this to graphs of
1913800	1915680	arbitrary sizes.
1915680	1919840	One simple solution to this problem is to use a graph transformer, which would connect
1919840	1925280	every node to every other node and give you a trivially setting with no bottlenecks.
1925280	1931400	However, as we will show later, these fully connected graphs are trivially dense expanders,
1931400	1932400	actually.
1932400	1934840	So they fit our theory, but they are dense and they won't scale.
1934840	1936600	So we don't necessarily want that.
1936600	1943080	And lastly, because it's often quite computationally painful to clear these bottlenecks in an input
1943080	1948120	data-driven way, especially if you have lots of online graphs coming into your problem,
1948120	1951720	we might ideally want a method that doesn't have to do like dedicated pre-processing of
1951720	1952880	the input graph.
1952880	1957000	And actually, satisfying all four of these at the same time turns out to be quite tricky.
1957000	1961600	We actually have done a literature survey of a bunch of related works, and it seems
1961600	1964720	really hard to tick all four of these boxes.
1964720	1968180	And our method, the expander graph propagation, tends to tick all four of them.
1968180	1969400	So how do we do it?
1969400	1973800	Basically, we propose to propagate information over these expander graphs, which are known
1973800	1975720	constructs from graph theory.
1975720	1980480	Specifically, expander graphs have mathematical properties of a high-chigger constant, so
1980480	1985120	a very low bottleneck, which is good, a low diameter, meaning you'll get global information
1985120	1986720	propagation very efficiently.
1986720	1991960	However, additionally, we can build expanders in a sparse manner using this standard mathematical
1991960	1994840	construction from the special linear group.
1994840	1999240	And that actually guarantees us that the degree of every node will be four, therefore the
1999240	2001200	graph will be sparse.
2001200	2006240	And actually, the only generative parameter of these graphs is the size of the group,
2006240	2007600	this N over here.
2007600	2011040	So it's very easy to generate an expander for a particular number of nodes.
2011040	2014760	You just tell me what N you want, and I'll give you a graph.
2014760	2017640	So when you look at an expander, it looks something like this.
2017640	2021400	It is basically, what I like to say, it looks a bit like the human brain, right?
2021400	2025560	Every node kind of has this very local connectivity to its four immediate neighbors.
2025560	2030560	But as you go far away, like log N steps, you get a lot of cycles being closed very quickly,
2030560	2033920	and the global communication properties get like really good.
2033920	2035520	So that's our proposal.
2035520	2039480	Take basically, you know, your state-of-the-art graph net that you care about.
2039480	2042920	We literally just take the code actively available implementation.
2042920	2048280	We switch the graph neural network connectivity in every even layer to operate over one of
2048280	2050640	these guys rather than the input graph.
2050640	2054400	So basically, you kind of alternate input graph, expander graph, input graph, expander
2054400	2059320	graph, so that the input graphs layers are responsible for the usual local computations
2059320	2061080	that a GNN wants to do.
2061080	2065240	And the expander layers are responsible within diffusing that information globally in a sparse
2065240	2066760	and scalable way.
2066760	2067760	And this seems to work well.
2067760	2071880	So on all the data sets we tried this construction, it was better than the baseline.
2071960	2075600	As I said, all we did was change the connectivity, so the number of parameters is exactly the
2075600	2076600	same.
2076600	2081200	It's really like an apples-to-apples comparison, and it led to statistically significant results.
2081200	2084760	One last point I would like to make is, you know, we're not the only group that tried
2084760	2089840	to study this problem, concurrently to us, the group of Michael Bronstein with Jake
2089840	2094120	Topping and Francesco DiGiovanni had this great paper on curvature analysis, which was
2094120	2098120	actually one of the best paper awardees at iClear 2022.
2098120	2102840	And basically in this paper, they claim that if you have negatively curved edges, so edges
2102840	2108200	with very negative curvature, those tend to be the ones responsible for the formation
2108200	2110720	of bottlenecks and therefore over-squashing.
2110720	2115920	So naturally we wanted to connect our expander to this theory, so we computed the curvature
2115920	2117120	of our graphs.
2117120	2121000	But we found that actually the graphs that we built are negatively curved everywhere.
2121000	2126480	So it has a curvature of negative 1 very quickly as you increase the size of the graph, right?
2126480	2131280	So obviously, you know, we built a negatively curved graph everywhere, yet it still seems
2131280	2132280	to work well.
2132280	2133720	So what gives, right?
2133720	2135480	We try to analyze this a bit further.
2135480	2139200	First we show that the curvature of negative 1 is actually not that small.
2139200	2144860	Like the theorem in this paper is only invoked when the curvature is close to minus 2.
2144860	2148520	So in our case with curvature of negative 1, it's actually not sufficiently negative
2148520	2151440	to trigger that failure case of this theorem.
2151440	2155840	And additionally, we took it a step further and we actually tried to analyze how easy
2155840	2158160	it is to satisfy these three properties at once.
2158160	2161400	So to have sparsity, we said sparsity is good for scalability.
2161400	2165520	To have a low bottleneck, so a high trigger constant, which would mean you don't have
2165520	2168560	these kinds of pathological propagation problems.
2168560	2173560	And thirdly, to have positive curvature, which seems to be a good idea based on the analysis
2173560	2174560	of this paper.
2174560	2177560	And we actually proved, there is a theorem in our paper that proves that these three
2177560	2182080	things are incompatible with each other, in that there's only finitely many graphs
2182080	2185700	that satisfy these three properties simultaneously.
2185700	2190860	So as you go to large enough input graphs to be sparsed and to have no bottlenecks,
2190860	2192820	you have to be negatively curved somewhere.
2192820	2194540	It's impossible to avoid it.
2194540	2199260	So while we don't study the implications of this any further, we do believe that it calls
2199260	2205160	on the community in the future to study what happens in this gray area where the curvature
2205160	2207220	is negative but not too negative.
2207220	2210860	Because it seems like something like that might be critical to having the most optimal
2210860	2212660	message passing possible.
2212660	2214980	And that is basically the rough summary of our work.
