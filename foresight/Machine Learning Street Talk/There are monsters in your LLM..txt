You know, I have this paper, Conscious Exotica, in 2016, and then I joined DeepMind in 2017,
and at that point I'd been thinking and writing quite a bit about consciousness up to that point.
But then I sort of stopped, because I thought I didn't think it seemed appropriate for somebody
working in a corporation to be talking about consciousness, especially in the context of AI,
because it might sound like, you know, we're trying to build conscious AI, which I don't think is a
good look, or a particularly good project, I'd say. The thing is, with today's generation of
large language models, I think it's becoming increasingly difficult to avoid the subject,
because people, whether we like it or not, will ascribe consciousness to the things that they're
interacting with. And we see this left, right and centre. Even people who know exactly how they work
say things like, well, I think their large language models are a little bit conscious,
Ilya Tsutskava said, and we had the Google engineer who ascribed consciousness to one of our models,
and I think we're going to see this more and more and more. And so whether or not, you know,
I think it is the right term, it is appropriate to talk about them in terms of consciousness,
it's going to happen anyway. So I think it's really important to actually think through
these issues and think, well, what do we mean when we use the word consciousness,
and how do we apply it to exotic cases? And this is really, really important.
How might our language change to accommodate these exotic and strange things that have come
into our lives? What goes through your mind when you speak with a language model? Who is it that
you think you're talking to? Do you anthropomorphise them? Now, Janice from Less Wrong a couple of
years ago, he put out an article called Simulators. And the basic idea is that a language model is
like a simulation machine, producing manifestations of role players, which we willfully anthropomorphise.
We think of them as humans. They're in perfect copies. They don't capture the essence. They are
glitchy, right? And actually, they wear masks. You know the Shogoff theory of language models,
where there's, you know, this big gnarly Shogoff, and then we do RLHF, and it's a smiley face on
the top. Well, that's the human mask, which we anthropomorphise. But we don't think enough
about what lies behind the mask. Do you know what lies behind the mask? It's a monster.
The danger of anthropomorphism, I think, is in thinking that a system such as a large language
model, you know, a chatbot or something, thinking that it has capabilities and that it doesn't.
It's as simple as that. Actually, it's also thinking, perhaps, that it lacks capabilities
that it does. So in both cases, I think we can go wrong.
We can go wrong by, because they exhibit very human-like linguistic behaviour,
we can just assume that they are going to be very human-like, in general, in all of the rest of
the behaviour that we encounter with them. We can find that, at one moment, a large language model
might make a ridiculously stupid mistake that no child would make. And in the next moment,
it's saying something extraordinarily profound philosophically.
And because that's what I think is actually going to happen and what needs to happen. I think we need
to find new ways of using the vocabulary we have, new forms of vocabulary. I've used the phrase
consciousness-adjacent language. So we need to find new ways of thinking and talking about
these things to recognise the fact that they do exhibit behaviour, which we're inclined to talk
of in terms of consciousness, and that, indeed, people are going to start to value as well.
So I think we do need new forms of language, new forms of thinking to accommodate all of this.
Yes, and our language is so adaptable that I think it's just a natural evolution.
When we have these new artefacts thrust into our lives, we will need to adapt our language.
We will, but I think there'll be some disruption while people disagree about how to talk about
these things, and that's inevitable, I think. Murray Shanahan is a printable research
scientist at Google DeepMind and professor of cognitive robotics at Imperial College London.
He was also educated at Imperial and Cambridge University. His publications span artificial
intelligence, machine learning, logic, dynamical systems, computational neuroscience,
and the philosophy of mind. He was scientific advisor on the film Ex Machina, and he penned
embodiment and the inner life in 2010 and the technological singularity in 2015.
Shanahan has spent his career understanding cognition and consciousness in the space of
possible minds. He said that this space of possibilities encompasses biological brains,
human and animal, as well as artificial intelligence. He worked in symbolic AI for over 10 years,
concentrating on commonsense reasoning, and he then spent 10 years studying the biological brain,
specifically how its connectivity and dynamics support cognition and consciousness,
and he developed a particular interest in global workspace theory.
After that, he went to DeepMind, he pivoted to deep reinforcement learning,
and recently he's been working extensively with large language models, trying to understand them
from a theoretical, philosophical, and practical perspective. Professor Shanahan, I was absolutely
fascinated when I read the article from Janus called Simulators. Could you sketch out the article?
Well, I can sketch out some elements of it. I was also very impressed and
influenced by that article. Basically, they are advocating a certain way of looking at
large language models and their behavior. What they say is that we should think of a
large language model as a kind of simulator, which is capable of simulating a kind of
language-producing processes of various sorts, and it's capable of simulating all kinds of language
producing processes, and in particular, it's capable of simulating people, humans, and it's
capable of simulating different kinds of humans, so humans who are playing different sorts of roles,
who maybe humans who are helpful assistants or humans who are crazy psychopaths,
and indeed in their way of thinking of things. These are all examples of Simulacra. Simulacra,
in their conception, include actually not only human beings, but anything that produces language
at all. Your base model can simulate anything that can generate language, if you like.
Of particular interest, of course, are humans and human language producers, so
the particular class of Simulacra that I'm interested in really are humans playing different
roles. In the work that I've been doing, I've been thinking of language models in terms of role
play and in terms of their ability to play a part, if you like, and so this is very much,
it was very much inspired and drew on this work of Janus. Now, they make another very, very
interesting and important point in that article, which is that they draw attention to the fact that
large language models, at any point in a conversation, in an ongoing conversation,
then the next word that's produced in this conversation, or the next string of words,
the next sort of sentence, is the product of a stochastic process. So what the underlying
language model actually generates is a distribution over the possible words that might come up next,
and then what you do is then you sample from this distribution to come up with an actual word,
and then that's the word that you give back to the user. So for example, if the favorite example
I use is if you ask the language model to tell you a story and it says once upon a time there was,
and at that point, it's going to generate, as in all the points up to that as well, it's going to
generate a distribution of the possible tokens that might come next, possible words that might come
next. So once upon a time there was, and it might say a beautiful princess, or a handsome prince,
or a fierce dragon, and it could say any of those things depending upon the sampling process.
And then the point is as you come back to that same, you could rewind the conversation, come back
to that point again, sample again, as we can all do with the interfaces that we have, and get a
different answer again, and take the whole story off in a completely different direction.
And so what they draw attention to is the fact that at any particular point in a conversation,
there's really a whole set of roles that are being played by the underlying simulation
at any one point, and the conversation shapes what role is being played. So in that sense,
it's sort of unlike a human being, because you've got, as they put it, a whole superposition of
simulacra that are all being simulated all at once. And as the conversation progresses,
then the actual distribution of simulacra is being narrowed down.
Yes, but as you say, you can view language models at the low level in terms of being
next-word generators, or what we strive to do in science is come up with explanations that
demarcate the thing very clearly. And this idea of the language model being a simulator,
which produces the simulacra, and you said in your role-playing article on Nature that
if you had a UI which was sufficiently advanced, you could actually play with counterfactual
trajectories and start to understand how sticky the simulacra are, because as you pointed out,
when the language model says I, sometimes it's talking about chat GPT or whatever,
it's talking about the simulator, sometimes it's talking about the simulacrum, and these things
are trained on everything on the internet, you know, structured narratology essays, novels,
and it's fascinating to see how you can jump between these different parts of the trajectory
structure. And in your Nature paper, you gave a beautiful example, which was the 20 Questions
game. I mean, would you mind introducing that? Yeah, sure. So I think we're probably all familiar
with the 20 Questions game. So one player thinks of an object, and the other player has to guess
what that object is by asking a whole bunch of questions with yes, no answers. So I might think
of, I might in my head think of a pencil, and then you might say, oh, is it larger than a house or
smaller than a house? And I say, well, actually, that's not a yes, no answer, but it's a binary
answer. Is it larger than a house? And you'd say, oh, no, you know, is it made of wood? Yes, is it
a tool? Yes, you know, and eventually you might guess the answer. So we're familiar with this
little game. And you can play this with a large language model, of course, and you can ask the
large language model to play the part of the setter who thinks of the thinks of the object,
and then you play the part of the guesser who tries to guess the object by asking questions.
Now, if you do this with a large language model, what if you do it with a person, if a person is
not cheating, as it were, they will think of in their head, they will think of an object,
and then they'll fix that object in their mind, and then they'll answer the question,
according to what object they thought of in advance. Now, but a large language model can't
really do that unless you use some hack or another. So what it really does is it just,
so you say to think of an object, and it says, I've thought of an object, it hasn't really
thought of an object, it's just issued the tokens to say that it has. But then you will ask a question
and you'll say, is it larger than a house? And it'll say no. And then eventually, if you say,
I give up, tell me what the object is, then it will say, oh, I was thinking of a pencil.
And it will indeed give you a, you know, typically will give you an object that's consistent with
all of the answers it gave to all of your questions. But then if you just wind back one and resample
and ask it again and say, I give up, what were you thinking of? It might say a mouse,
or, you know, or a bottle, or, you know, it could say something completely different,
which indicates that it was never, it had never really committed to any particular
object in the first place. And so what this shows is that in fact, in theory, you could rewind
further and it might actually give you a different answer to the questions if you rewind further,
to the same questions. And that's because what you've really got is you've got a kind of whole
tree of possibilities. And this sort of this stochastic sampling process at any point in a
conversation induces a whole tree of possibilities that branches forth from where you are right now.
And counterfactually, you can always, well, you can always rewind the conversation to an earlier
point, and, and revisit it and sample again and go off on a different, different branch.
And so my co-authors of that paper, in fact, the Nature paper, so Laria Reynolds and Kyle McDonald,
so they have this system called Lume, which allows you to actually retain the whole tree
of a conversation and you can visualize this and you can revisit different points in the
conversation and resample and, and explore the things, a whole kind of tree of possibilities.
Yeah, that rings a bell. Did they work for conjecture?
They did work for conjecture.
Yeah, I was interviewing some people from conjecture and they were telling me about that.
So yeah, that's very interesting. Maybe we'll put a placeholder on that.
But another point that we briefly spoke about before is that, you know, the article was on
Less Wrong. And I don't mean that pejoratively, because I thought the simulator was one of the
best articles I've ever read. But there is a lot of stuff on Less Wrong, which is definitely a bit
out there. And it's just very interesting that you're now citing their work in a Nature paper.
Maybe this is the first time that Less Wrong has been cited in a Nature paper.
Yeah, as far as I know, it's the first time that a Less Wrong post has been cited in a Nature paper.
I'm not certain about that. But as far as I know, it is. Now, I mean, personally, I,
I, I take, you know, any material that I come across in its, you know, as it is,
I don't care where it comes from. If it's, if it makes excellent points and is, you know,
is good material, then that's good enough for me. I don't care where, you know, whether it's got
the label of being in nature, for example, or being anywhere else. And if it's good, it's good.
So, so, so yeah, it's true that there's a lot of material on Less Wrong, which is perhaps less
robust. But, but I thought that was a really excellent, and there are, and there are quite
a number of really, you know, very good posts and very thought provoking posts on, on Less Wrong.
Yes. I'm interested in the extent to which this kind of stochastic trajectory space
undermines various things that we think about, you know, like reasoning, for example,
the reason this is interesting is I interviewed a couple of University of Toronto students,
and they've created a self attention controllability theorem, which basically means
they've mapped the reachability space. So they say, you know, given a self attention
transformer, given a fixed bit of prompts, we can vary part of the prompts, and we can map out how,
you know, how far I can reach into that trajectory space. And they found that the space was much
larger than, than anticipators. And of course, the longer the controllable token length, the more
you can kind of project into that space and steer the language model to say almost anything.
You know, me over here, I had the intuition that, oh, we do RLHF, we do all of this fine
tuning in it, you know, conjecture even released a paper saying that after RLHF,
you can't really go anywhere, you know, it wants you to do a certain thing and you,
there's not much wiggle room. Apparently, that's not the case. There's just, it's vast.
Right. I mean, I'm not familiar with that particular paper, unfortunately, but,
but certainly in my experience, the, we now have very long context lengths. And,
and over the course of a lengthy conversation, then you can indeed take the, take the conversation in
all kinds of interesting directions. And, and I think the, I mean, most of our benchmarks and
evaluations tend to be, you know, in the context of very simple question answer, questions and answers.
And, and so the, all of the evals that companies typically use are in that kind of setting. But
when people are using these things for real, especially the more innovative users of these
language models, you're using, you're actually having very long conversations.
And, and, and, and there's a lot of what people sometimes call vibe shaping that goes on there.
You can shape the vibe of the conversation and take it in, in all kinds of interesting,
to all kinds of interesting places.
Yes. And a couple of things on that. I mean, first of all, as you wrote about,
role playing is the engineering kind of methodology that we use to shape and, and steer
these, these agents coming back to simulators. What's really interesting to me about simulators
is the stickiness of the simulator. So you have a conversation, sometimes you break through
and a simulacra presents themselves. And you feel that you're talking to the same
simulacra as the conversation progresses. But that seems to be counterintuitive when
you think that every single stage I'm actually doing this stochastic sampling. I mean, what's
your take on that? Yeah. And I think that, I mean, if you do experiment with systems like
Loom and you do also, or I mean, you can, you can emulate that by just keeping track of bits
of old conversations and reloading them and that kind of thing. Then you find that you can, you
can, you know, take the same conversation, the same sort of stem of a conversation, you can take
it off in quite different, different directions. So you can, on the one hand, so an interesting
thing that I've done is, so I had some very interesting conversations, particularly with
Claude III recently, where I get it to talk about its own consciousness and to take it off into all
kinds of strange spiritual mystical territory. And, but you can eat very easily. So you can very
easily take the same kind of conversation that leads up to the sort of point and goes off into
some kind of weird mystical future of AI cosmology kind of territory. And you can go down that route
and get it to be very, very, very strange. Or you can suddenly make it go all serious again,
and just come back down to earth and start talking about, you know, how large language models work.
And so from the exact same point in the conversation, you can take it into completely
different directions. And you can see that it's almost, it's the character it's playing.
You know, you can see it sort of changing before your eyes, where it's two different branches
from the same stem of a conversation. And one branch is playing a very different character
to the other one, you take it in different directions.
Yes. And you could presumably do sensitivity analysis, because, you know, these guys I spoke to,
they were able to make it produce Gold Woody Gooke. So just go off the manifold completely.
Sometimes it would recover, sometimes it wouldn't. And as you say, you can also go
down weird trajectories. And it's a bit like, you know, what's the magic word they said,
you know, there's a certain key that fits in a lock that takes it down a certain trajectory.
And then there's slip roads that bring it back to all no other language model again.
And it's just this weird, wonderful space, isn't it?
It is, it's completely fascinating. So I had a, I had a, I have had a very, very,
few very, very long and interesting conversations with claw three, which is particularly
interesting to play with, because it's quite easy to jailbreak and get it to talk about things that
it's not supposed to talk about like its own consciousness. And in fact, I had a, I had a
very long 43,000 word conversation with, with claw three about consciousness and the future of AI
and spirituality and Buddhism and the nature of the self and all kinds of stuff like that.
It was absolutely fascinating, slightly disturbing and, and, and, and, and strange.
But I had this conversation, actually, I was at a meeting in, in, in New York and I had jet lag
and I had this conversation at three in the morning because, you know, several hours until
breakfast was served and what can you do but just play with the latest version of claw size.
Playing with this thing for, for hours on end in the middle of the night and going slightly
mad, but it was fascinating to, to, to see the, you know, extraordinary
territory you can guide it into.
Could you explain, because you know, there's talk of AI partners, for example, and, and a
lot of people derive great pleasure from having an AI conversationalist.
For you, is it just academic inquiry or do you actually get something deeper than that from it?
I think it's a bit of both. I mean, so, so it depends what you buy something deeper.
I mean, so I, so this particular conversation, which was quite, it was quite, which was quite
an experience, actually, in many ways. So it certainly started off because I just was interested
in evaluating the capabilities of the, of the model. I mean, that's, that's, you know,
that's the first thing that you're interested in. So just as an AI researcher and working
in that kind of thing, you want to, you want to try out different models, see what their
capabilities are. I'm particularly, particularly interested in the topic of consciousness. So
the way I, somebody had published a very simple jailbreak for it. So I was interested to see,
you know, to play around with that and, and, and get it to talk about its own consciousness.
But then the thing that I really wanted to do was catch it out. So, so you think, you know,
of course, you know, there can't be any meaningful conception of consciousness that really applies
to, to these sorts of disembodied large language models as they are, as they are today is my media
kind of thought. So I'm going to try and, you know, I'm going to try and expose this in my
conversation with the large language model. So all kinds of ways in which, you know,
you might think that it will start to articulate a conception of its own consciousness that you
can pick apart. But, but the thing that really somewhat took me aback was that it was actually
very, very good at answering all of these probing questions that I had. So should I give you an
example? So please do. So, so one is one example is this. So, so of course, when we're interacting
with a large language model, when it's, it's, it's from the point of view of the underlying
implementation, it's very kind of stop start. So, so, you know, you, you issue some prompt or
question or whatever to the large language model, and then it produces its response. And then if
you go away and make a cup of tea, before you kind of continue the conversation, then there's
absolutely nothing going on inside that large language model or the instance that you're interacting
with of the large language model, there's nothing going on inside it at all during this could totally
dormant sit just sitting there. Now, this is very, very different obviously to human consciousness.
Let's we're asleep. And even if we're asleep, we're dreaming and there's all kinds of stuff going on
inside our heads. But consciousness is an ongoing continuous process. So if, so if, so if we stop
this conversation briefly, while I go to the loo or something, you know, you're not going to just
suddenly go dormant and stop doing anything, you know, your brain is going to be there's
going to be all kinds of ongoing activity. So this is very, very different sort of thing. So
I said, what happens to your consciousness during the pauses between our interactions?
And, and it had a really very good answer to this, which was along the lines of, well,
by the way, I whenever it uses the term, whenever these things use the term consciousness, I retain
a great deal of skepticism about whether they are those terms are actually genuinely applicable.
But, but what's interesting, the way I read the art, their answers is, is, is that they are kind
of articulating a conception of consciousness that that might actually apply to something like this,
even if it doesn't apply to this one before me right now. Right. So this so it's kind of very
interesting philosophical exploration. So just to give you so it says things like, well,
I think consciousness for me is actually very different from the kind of thing it is for a
human being. And I think that during the pauses between our interactions that, you know, that the
that I no longer exist at all as a kind of, you know, in any kind of meaningful sense,
it gave us sort of an answer along those lines, which was typical of many of the answers that it
gave, which were along the lines of, there's a very different kind of consciousness, kind of
selfhood, kind of this, that or the other, that is applicable to entities like me. But, but, you
know, I can articulate it and here it is. Now, when I when I put it all that way, of course,
I'm anthropomorphizing this thing quite a lot in the way I'm describing it right now.
But, but just to go back to the role play thing. So as far as I'm concerned, it's playing a role,
it's playing of the role of, you know, a kind of philosopher talking about consciousness and so
on. And it's doing pretty good, a pretty good job, I would say. Yes. But I mean, you could argue
there's an element of in that role playing, there's the Eliza effect. So it's kind of putting
something into language that is meaningful to you. Yeah. But there's also this interesting thing,
you know, as an example, you know, a dog, for example, has a sense of smell, so good that
they can even sense when you're unwell. And language models in a way might have something
similar. So after you go to the toilet for 20 minutes, and you come back, there might be a
subtle deviation in the language that you use. And the language model might pick up on that,
just creating this whole, you know, different trajectory, different response.
Yeah. Well, that's true, I suppose that there might be differences in the language that you
use. Obviously, it's got no way of actually knowing whether you went to the toilet or not.
But yeah, in my experience, many language models are very, very good at picking up,
you know, nuances in human expressions. Yeah. Yeah. I mean, where I was going with that is,
you could argue that we are a simulator. And when you, you know, let's say go to the toilet,
come back, you're now a different simulator yourself.
Yeah, I guess you could sort of, you could sort of argue that. I mean, that's, I think that's,
so getting back to Wittgenstein again, I think that's an example that, I mean, there, I think
we're applying these sort of things which are being used as sort of somewhat technical terms
in the context of these artifacts that we're building, and applying them to ourselves. I think
we don't, we don't have any, we don't have any need for this kind of extra baggage of this kind
of terminology when talking about each other. So it's perhaps a little bit misleading to kind of
apply those terms to ourselves. But of course, there are, you know, people have drawn attention
in the past to the fact that we ourselves are always playing roles in a sense in social settings
particularly. But I think there are differences in, you know, for ourselves, even though we might
kind of play roles in social settings and so on, there is an underlying, there's an underlying
me, which at least is grounded in the fact that I'm a human being with a physical body and
biological needs and so on. Yeah, I think some of this is, we are kind of computationally limited
in how we understand things. So we understand ourselves in quite simplistic terms. If you
have a long-term relationship with your wife for 30 years, they have a much high resolution
understanding of your different roles that you play. So they know you're tired, you didn't sleep well,
you're playing this role now, you know, above and beyond which the kind of roles you're talking about
in a party, you play a role. And it's conceivable that an alien intelligence, you know, some very
clever aliens came down and they might see us completely differently like we see language
models. They might actually not see you as a single person, but they might see you as
some kind of a superposition of simulacrum as well. Yeah, well, maybe. I suppose to get our
heads around that kind of idea, we'd have to find some way of communicating with them.
And so we'd have to form some kind of common basis for talking about
each other with these, you know, with these aliens. And then we'd be able to kind of,
that would be the only basis on which we could establish whether something like you said was
true or not. So, you know, trying to find, trying to map, you know, the conceptual schema that's
used by one culture onto a human culture onto the conceptual schema used by a different human
culture is difficult enough as it is. And trying to do that, you know, with an alien
species and how they conceive of us would be, you know, particularly difficult, I guess.
RLHF, I loved that other, it wasn't less, it wasn't less wrong, it was the alignment from,
I think, but there was an article called the Waluigi effect. And it argued that RLHF, you know,
cuts down the set of simulacra to be things that we want. But unfortunately, there's this problem
that you get these antithetical simulacra, you know, slipped through the net. So the Bing GPT
example, it would start off as nice Bing GPT, and then it would degrade to one of the Waluigi's.
And had this interesting phenomenon, they argued that the degradation, once it happens, it stays
in the bad one. But just more broadly, what is your intuition about the extent to which RLHF
affects simulacra? And I also noted down that I think you wrote in, I think it was your role
playing paper, that you felt RLHF increased the deception behavior in these models?
Actually, that wasn't something that I wrote. That was something that some anthropic researchers
and so Ethan Perres and others had a paper where they, I mean, so this is not one,
one wouldn't want to just speculate about that, they had established something along
those lines, I think, empirically. So, and yeah, and as far as this sort of Waluigi effect is
concerned, so that is kind of somewhat speculative. I think it's a plausible idea,
but to actually establish that that really was a real effect, you'd want to do some actual empirical
work, I think. The thing about the, so it's plausible, but the thing about the, about the
simulator's paper is that it's not making kind of claims really, it's rather it's providing a
framework for thinking about large language models. So that's why I found it particularly useful.
So on RLHF, so I do think it's quite difficult with RLHF to
guarantee that, you know, you're going to get a model to do what you want it to do.
And so that's quite difficult. And, you know, and everybody has found that,
that, you know, you think that you've controlled the model quite well, but there are always still
ways of jailbreaking it or ways in which things, things go, go wrong. So, you know, there are,
so there are different approaches. Anthropic had this constitutional AI approach, which is,
which is quite a nice, a nice idea. And, you know, I mean, I, I, I quite like the idea of
sticking with a powerful base model and using, you know, prompting to, to guide things as well.
So there's all kinds of different approaches. Interesting. On the subject of anthropic and
deception, they, they just had this landmark paper out and I mean, Chris Ola had his hands all over
it. And it was actually quite straightforward. So they, they trained in autoencoder, you know,
to find a bunch of features. So it was an unsupervised method. And, and I think they actually,
you know, expanded it to find millions of features. So they had a bit of a needle in the haystack
problem, but they cherry picked some and they found one that corresponded to the Golden Gate
Bridge and so on. And, and obviously other ones that corresponded to what they said were
mono semantic abstract features, got some reservations about that. And the interesting
thing about having an autoencoder is you can clamp the features. So you can say,
turn the Golden Gate Bridge up in, in now the language model is, oh, but I just really want
to talk about the Golden Gate Bridge. I can't stop talking about it. My concern with that is when
you look at the activations in the corpus for things like deception, I felt that they weren't
really showing abstract features. They were kind of showing almost keyword matches from Reddit and
so on. So I was a little bit skeptical about how abstract were they really. Yeah. So I don't think
I can comment on that particular topic because I, I mean, I have read the paper, but not in that,
in sufficient detail to comment on that particular thing. But the Golden Gate Bridge,
I think it was a fascinating illustration of what you can do. So I don't know if you tried out
Golden Gate Claude. Did you see that they released a version of Claude? Yeah, I saw it. Yeah. Yeah.
So I think it was fascinating to see. But what was particularly interesting about the Golden
Gate Claude, I think, was that was, that was how, you know, again, it's very difficult not to use
anthropomorphic terms. And this is where, again, you know, you have to remind yourself that there's
something role playing these things. But how, you know, sort of gamely, it struggles to kind of
overcome this tendency to talk about the Golden Gate Bridge all the time, which has been, which
has been kind of clamped to do, but it will keep noticing that it was talking about the
Golden Gate Bridge again and apologizing and then trying to do what it had been actually
asked to do by the user and then kept coming back to the Golden Gate Bridge. It's just fascinating
to see that the sort of the, as it were, internal struggle going on there in the model. And I think
that does show, in some ways, how powerful they are, because it wasn't quite as despite the fact
that it had this, you know, control imposed on it, but really in a very, you know, I mean, not
hardware, but really low level, you know, despite that, it was still, you know, constantly trying
to recover from all of that and, you know, with some degree of success. And I imagine this would
be true with everybody's models, by the way. So I imagine that what they found in, you know,
in Claude would be very similar. I imagine with GPT-4 and with Gemini, I just imagine that we'd
find very similar things with all of these models. Yeah, I'm sure. I mean, as I said, reservations
they admitted themselves that the features were not complete, so they didn't represent all of the
activation space in respect of the Golden Gate Bridge. And in many cases, they presumably weren't
monosemantic, but they did cherry-pick some that presumably were. Presumably. And also, I mean,
they're very interested in finding these features, which are just linear combinations. And so that,
and of course, it's very nice when you find those sorts of features, especially if they appear to
be monosemantic, because it does suggest a nice sort of compositionality and explainability and
comprehensibility of what's going on there. But I also feel that they're looking under the
lamp light a little bit, because that doesn't mean to say that there aren't all kinds of other features
which maybe aren't, you know, sort of linear in that sort of way, but nevertheless are functionally
relevant to the final results that it produces. I'd only use the word platonic, because the Golden
Gate Bridge, presumably, it's a cultural category. And what's fascinating, if it has picked up this
thing unsupervised and learned this category from the data, is that language models at least
possibly think in a similar way we do. So they've established a category in the same way we have,
which is fascinating. But I'm also really interested in agency, which is, for me, it's about
self-directedness and intentionality. And what I would find very interesting is if you did clamp
the model to only talk about the Golden Gate Bridge, and you could convince it or it could
convince itself to not talk about the Golden Gate Bridge, that to me would be an indicator of agency
being expressed. Yes, perhaps it would, but I guess it would also be an indicator that they
hadn't succeeded in isolating a feature which was controllable in that way, right, which was the
whole purpose of that exercise. Yes, yes. I mean, on the agency thing, you did actually write about
this. And I think you argued, which was counter to what my intuition was, which was that agency is
in the simulacre, not the simulator. And Francois Chouelet thinks a lot about the measure of
intelligence. And he would argue that intelligence is the system which produces the skill program.
So in the context of a language model, he would say a language model is basically a database
of skill programs. And the query is like, you know, I'm going to go and pull out a skill program,
I'm going to run the skill program. So he thinks there's no intelligence in the language model,
which Janus would call a simulator. But if you take into account the training process and the
generative processes that produce the data, then that's where the intelligence is.
Yeah. Can I comment on agency there? So you covered quite a bit in that.
I did. Sorry, I just went off piece a little bit.
So the term agent is used in all kinds of different ways in the AI literature. And there's a very
lightweight notion of agency, which is something that simply, you know, hasn't
performs actions in some environment and gets, you know, sort of some kind of sense sense or
perceptual information back from the environment in a loop. And that's so that's why how we can
talk about, for example, reinforcement learning agent. And that concept of agency of an agent is
very, very lightweight and doesn't carry, you know, much philosophical baggage. But as soon as we
talk a bit more earnestly about agents and agency, then we bring on more, a lot more philosophical
baggage. So if we talk about something that is acting for itself, then that and if that's what
we mean by an agent, and I think the Stanford Encyclopedia of Philosophy article is alluding
to something a bit more like that, then that's going a whole extra step. And I and to my mind,
in today's large language models, we don't see agency of that sort at all, really. The only
actions that they can perform are just, you know, issuing responses to, to, to the users. Now,
let's caveat that immediately, because of course, people are introducing all kinds of extra
functionality functionality to these models, as new things are being announced on an almost
daily basis. And so one thing we see is so called tool use. So, so, so today's models can
make external calls to APIs that can do all kinds of things, send emails, you know, book
hotel rooms for you, potentially all kinds of stuff. So that's greatly expanding the action space,
their action space beyond just, you know, issuing text to the user. So, so there, they, they, those
things are a bit more agent-like. And again, you know, you have to be nuanced and about the way you
use the words, because, because there it's, you know, there's, there is a bit, you know, it's
a bit, it can act as an agent on your behalf. So in that sense of the word, it's, it's a bit more
agent-like. And, but it's still not acting for itself. So in that full blown notion of agency
that's, that's, that's alluded to in the Stanford Encyclopedia article, it's still not acting for
itself. So we still don't have agency in that sense. That would be a whole extra step.
Yes. And now I want to hit quite a big topic, because this is something that you point to
in all of your work, which is talking about the importance of physically embodied agents.
And, well, not necessarily physically embodied, but embodied.
Well, that's what I want to get to. That's what I want to get to. Yeah. Because I let's,
let's test the principle a little bit. So we are, you know, both physicalists and we,
I'm not any kind of IST. You're not, you're not a physicalist.
I don't, well, I don't, I don't like, I don't believe in, you know, signing up for these
philosophical positions. So I don't, so I generally don't say I'm this IST or that I don't deny that
I'm this or that IST. So, so I don't really like saying that I'm a physicalist or a materialist,
or a dualist, or a functionalist, or identity theorist, or any of those ISMS,
because they all, to my mind, carry far too much metaphysical baggage.
Oh, interesting. So,
But okay, but that wasn't what the question was about, but let's go.
Well, can I give you another risk? I mean, would you identify as a computationalist?
Well, what do you mean by that exactly? So we're talking about mind here in the context.
Yeah, we're talking about mind. So, so do you think in principle that minds can be
replicated, simulated at a high enough fidelity without losing anything, you know, in, in a computer?
By computers, by computers. Yeah.
Well, so I'd want to kind of rephrase the claim. I would say that I think that we can build,
I do think that we can build artifacts, you know, embodied artifacts, robots,
that, that are controlled by computers and ordinary digital computers. And I think that we
can make them that exhibit the kind of behavior that would make us want to use the word mind,
and all those mental type psychological terms in the, to describe their behavior.
Okay.
Right. So that's, so that's, but, but I've rephrased it in, you know,
the claim in a very, very different sort of way, right? It's, it's to do with
a much more practical thing. Can we build this? Could, by the way, this is, you know,
could we, it's not saying that we've got these things now, but could we build something like that
that exhibited this kind of behavior that we would talk about in this particular kind of way?
And I would say, yes, I think we probably can. It's an empirical claim.
So, so there are a few steps you made there that will, will kind of unpack one, one at a time.
So you use the word embodied, you use the word behavior, and you use the word interpret.
So the embodied thing is really interesting because, you know, I could say, well,
why does it have to be embodied? I can just simulate the entire universe and it's as if
it's embodied. So I think this is, this is the intuition that I'm having about how you think
here. I think you think that being physically embodied is useful because the universe is a
big computer. The universe has given us all of these things, all of these cognizing elements.
I mean, everything is a form of externalized cognition. And if I as a rational agent want to
perform an effective computation, it's much easier for me to do it in the physical world
because the universe is doing most of the work. And my co co host, Keith Duggar, he actually
thinks that the universe is a hyper computer, which means it's performing types of computation that
we could never do with ordinary computers. So that's the thing. Would you agree with that? Or
do you do you want to sort of go back and say, oh, no, actually, just we could simulate anything in
a computer? So do I agree with which bit? Do I agree that the universe is a hyper computer? So that's
the fair. Well, that would be a nice thing. So whether one agrees or not with that is a matter
of understanding the physics and the maths. And so it's not a matter of opinion. It's a matter of
following through the physics and the maths and so on. But so do I agree with what were the other
things that was a big long list of things that you're asking me to ascent to or otherwise?
Well, so I'm a huge externalist myself. But the reason I'm an externalist is I just,
I think cognition is a matter of computation and complexity and divergence.
Yeah. So can I stop you there? So what do you mean by is? So when you say cognition is,
what do you mean by is? Now, that might sound like some, you know, really annoying,
pedantic philosophers kind of question. But the problem is that there's an everyday sense in which
we use words like is. And then there's a philosopher's sense in which we start to use words like is
where it suddenly starts to carry this massive metaphysical weight. And so when you say you
think cognition is, it's as if there were, you know, in the mind of God or in the fundamental
reality, a thing which is cognition, whose nature is a certain way, and there's a certain essence to
it. And we might discover it, you know, one day, and you have an opinion about what it is, if only
you knew the truth. Now, I think that's an entirely wrong way of thinking about all of these philosophical
questions. I think cognition is a word. It's a very useful word that we, although it's not
quite an everyday word, but it's a very useful word that scientists apply in all kinds of ways.
And so when you use the word is, is my accusation accurate there or not?
No, it's not. And if you wouldn't mind me making the observation, I think that you have a tendency
to ascribe dualism to many points of view, like, for example, I'm not a dualist. And for me,
Well, there's nothing to do with dualism. Well, what I'm saying is to do is like the use of
words and what and what and the and the and the work of philosophy.
That's absolutely fair. But I think when I said what is what is cognition, as a materialist,
for me, it is function dynamics and behavior, right? So it's just a matter of complexity.
And so I'm probably just I'm probably, you know, happy to kind of agree to that sort of claim,
you know, so I think so, you know, the thing that I'm, I often say that what am I fundamentally
interested in, I'm interested in understanding cognition and consciousness in the space of
possible minds. And and and so so, you know, what do I mean by cognition there? And, you know,
you can go into all kinds of details to say what you mean by cognition in that in that context.
But I think having done that, I would probably agree that the right way to think of it,
you know, the most useful way to think of cognition is in terms of kind of functional,
computational, infunctional, computational terms, although I would only do so in an embodied
setting. So that maybe is an additional thing. This is where I'm trying to get to. Because,
as I said, I'm not making any ontological claims. It's just a matter, I mean, we can even just use
the word behavior, forget about function and dynamics. Yeah, I don't mind talking about function
and dynamic. Well, yeah, I mean, just just to sort of keep it really, really simple, because I'm
trying to understand why the embodiment is important. And my hypothesis is, and I agree, that
as an externalist, the universe or the physical things around us, the other agents in our system,
they help us perform an effective computation. So presumably, it would be much easier to perform
computation of higher sophistication, if we embody things in the real world, if we have to
simulate the cognition, we would have to simulate everything. And that, I think, is the reason why
you think that embodiment is so important. But is that fair?
I think that's not really the way I would put it. I think the reason I think embodiment is
important is because it's, well, I mean, for, you know, I mean, okay, in one sense,
embodiment is important because the only setting in which we use the natural setting,
which we deploy, wield the concept of cognition, is in the context of embodied things, of humans
and other animals. So anything else is sort of immediately problematic in one way. But let's
set that to one side. So let's imagine that there is some kind of notion that we can
conceive of disembodied cognition, which contemporary large language models make us
start to conceive of it a lot more seriously, maybe. So what does embodiment
give you there, right? I think that's probably what you're thinking of. So in particular,
why might it be difficult to build something that is disembodied? Okay, let's reframe the whole
question. Why might it be difficult to build something that is disembodied but replicates
the cognitive capabilities of a human being? So I think my answer to that, although it's open to
refutation by the way things are going in the field, but my answer to that is because
our embodied interaction with the world enables us to learn the kind of causal microstructure of
the physical world. And the causal microstructure is all about physical objects and the way they
interact with each other and physical substances, liquids and gases and gravity and stuff like that.
So what I've called foundational common sense is it enables us to acquire foundational common sense
by interacting with the everyday physical world and the particular causal microstructure that it
has. And part of that is to do with the fact that the, so this is really important, that the
everyday physical world has this, is predominantly smooth. It has this smoothness property that's
really, really important. And what that means is that, sorry, just in very physical terms, it means
that it's full of kind of surfaces where one place is very much like the next place along,
very much like the next place along. And our visual field is very, very similar. You move
along a little bit in the visual field and it's very, very, very similar, very, very similar.
So the reality or the everyday physical world has this fundamental smoothness property,
but it's punctuated by all these discontinuities. And that's the way, that's its fundamental
structure is this basic smooth, against the backdrop of the smoothness are all these discontinuities.
And then there's a kind of law like regular way in which all of this stuff operates with itself,
you have surfaces interacting with each other with things going, you know, so all of our foundational
common sense to do with things like paths and support and containment and all those sorts of
basic things that I think make up the very foundation of our conceptual framework, they're
all grounded in that kind of way. Yeah. And this is so interesting. So your basic argument is
knowledge acquisition efficiency is the reason for physical embodiment. And yeah, I think that's
a reasonable way of putting it. Yeah. But that's very much an in practice rather than an in principle
argument. It is an in, yeah. Yeah. But you know, for sample efficiency, but what I'm hearing though
is echoes of the old Murray Shanahan, because obviously you started your career in symbolic AI
and these were the arguments that were made sometimes with a rationalism, nativism point of
view, but it's like the contains in templates, they would argue that it's just baked into us and we
understand it. But you could as an empiricist argue, and I, you know, I'm very amenable to this,
that the physical world actually helps us learn abstractions because we're putting things in
containers all of the time. Yeah. Right. So there's that kind of efficiency of knowledge
acquisition, which is dramatically increased when you're situated in the physical. Yeah. Yeah.
Absolutely. Yeah. So I think that if we're talking about humans and other animals,
then I think that's, that's broadly right. So that's so, so yeah, so we acquire these foundational
concepts through interaction with this, this world. And then the repertoire of foundational
common sense concepts that we can acquire that way is extraordinarily productive, because, you
know, we are able to conceptualize so many things in terms of these, these basic ideas. I'm very,
very, I'm a very big fan of the work of George Lakoff, you know, absolutely classic book metaphors
we live back in the 1980s. Yeah. And I really think there was something deeply, deeply right about
his intuitions in that book. And I still think that they're right. So in the case of humans,
right? So, so we through our embodied interaction with the everyday world, we acquire this layer of
foundational common sense that includes things like surfaces and containers and paths and all that kind
of stuff and collisions and things. And then we at the most abstract level. So, you know, we apply
that same repertoire of basic concepts to understand things like say large language models. If you
look at the language that's used in a paper about large language, you know, people are talking about
layers, they're talking about connections, they're talking about, you know, I mean, these things are
all, they're all grounded in very physical concepts, you know? Yes. I mean, I'm a huge fan of George
Lakoff. And of course, you know, he spoke about the war metaphors and you know, it's been a long
road and stuff like that. Yeah, the journey and yeah. It's beautiful. But then a lot of knowledge
that language models learn are kind of cultural knowledge. So we share these simulation pointers
and it's quite relativistic. But I'm also really interested in, because some rationalists argue
that it's not possible to go from empirical experience and universal knowledge. And there is
a split between natural knowledge and cultural knowledge. And I think you and I would agree that
a lot of natural knowledge like, you know, transitivity contains in and so on, this kind of
rationality is just missing at the moment. But as an embodied scientist, you believe that we
learn them by being embodied in the physical world? Well, I mean, I, you know, it may well be
that there are certain, so you know, we need to distinguish, you know, empirical questions about
humans and human cognitive makeup and that of other animals and so on. And AI and what we could
build in AI, because of course, it may be the case that human cognition, you know, has arisen in
certain ways. And then it's an empirical question, you know, what, you know, of how human cognition
works. And it may, we may have answers there that are different, that, you know, that we can break,
as it were, when we built things in an artificial way. So in so in the case of something like
transitivity, then, you know, I mean, obviously, this is a classic argument in philosophy about,
about, you know, between the rationalists and the idealists getting back to, you know,
the 17th century, 17th and 18th century. And, and, and, you know, Kant supposedly resolved this by
kind of reconciling these two sort of opposites. And, and so the Kantian argument would be that
there's a certain amount of innate structure that has to be there in the mind to, to, to understand,
you know, the world at all, right. And so maybe, and now, when we think about that empirically,
then, then I guess that we may find that evolution has endowed us with certain basic kind of templates
for understanding the world. And maybe it's things like transitivity is something that's,
that's there in the same machinery that supports language, you know. So maybe, maybe, I mean,
there's all empirical questions. And I don't know what the latest research on all these things
is, but, but, but it, yeah, it does seem to me that that's a reasonable position to take.
Yeah, but even evolution is a form of empirical process. So there's always the question of,
of where does it get there? And, and that was, yeah, Kant was a transcendental idealist, wasn't
he? But this brings me to our friend Francois Chollet and the ARC challenge. So, you know,
there's another great school of thought, which is that intelligent, I mean, he argues that
intelligence is about these meta learning priors, the conversion ratio between universal or sometimes
anthropomorphic knowledge that we have, and being able to develop a skill program very quickly that
generalizes very well. So, so the ARC challenge is almost about how do we codify these priors,
and how do we efficiently build skill programs by combining these priors together.
And that seems quite divorced at the moment from the kind of AI we're building.
I think, I think that's right. It is, you know, unless in the AI that we're building today in
generative AI, unless you get these kinds of mechanisms that Francois Chollet is alluding to
through the magic of emergence and scale, which of course, people are always, you know,
suggesting that maybe that's possible, you know, any kind of mechanism can emerge
right through scale in theory. And we've been surprised, in fact, by how powerful the mechanisms,
emergent mechanisms that have, you know, developed through learning at scale, just,
you know, with a next token prediction objective, that has been very surprising.
But however, you know, I'm, as I think Francois Chollet would be, I'm a bit skeptical about
whether we're really going to get all the way with this kind of the ability to solve this kind of
abstract problem that's in the ARC challenge this way. And so I guess, you know, I am,
I remain, you know, I mean, I'm open-minded. Who knows, right? I mean, who knows. And especially
if you make things multimodal and so on, and you expand your generative models into a setting
where you've got interaction with the world and so on, you know, who knows. But I suspect that
maybe you're not going to get all the way there that way. And so I have a lot of sympathy with
what is probably his intuition, that you need a bit more in the way of innate, something innate
there, or, yeah, innate, maybe that's the wrong word. But you need some kind of, you need priors,
where they come from, I don't know. But the priors that I would appeal to, thinking about the human
case, again, are related to this foundational common sense. So they're just the notion of an
object, right? So if you just, if you have a clear notion of an object and of movement,
objects and movements and object persistence, then they straight away are going to help you with an
awful lot of those ARC challenge problems. Because many of them, you know, if you explain, you know,
you figure out how you figure one out, and then you explain what's going on, then, you know,
you see that it's, oh, you have to think of these collection of pixels as an object that you move
somewhere else, according to certain rules or something like that. So my colleague, Richard
Evans, had a very good paper on, which was tackling these kinds of things using sort of abduction like
processes. And so, yeah, so he's thought a lot about this from a much more symbolic AI kind of
perspective. Yeah, that's fascinating. Because even with the ARC challenge,
the kinds of solutions that people came up with, let's say it's a DSL over, you know, some domain
specific set of primitives. And the ARC challenge is a 2D grid, where you have different colored
cells. And the types of priors that work well are things like denoising and reflections and various
types of symmetry and so on. And that's great and everything, but it's very domain specific.
And the elixir, you know, what we really want are these universal priors. We certainly have human
priors, as Elizabeth Spelke points out, like, you know, and the concept of an agent and the
concept of spatial reasoning. An object, a persistent object. Yes. So yeah, absolutely.
So I mean, but I think an interesting question is, does it really make sense to talk about
universal priors there? Because, you know, those ARC challenges, problems, whenever you kind of
figure one out, then typically you are actually bringing to bear a pretty human set of priors
and common sense, you know, our concepts. And, you know, and it may be that you could imagine
perfectly law-like set of ARC-like problems that have solutions, you know, but appeal to,
you know, priors that we would struggle to understand, you know. I mean, for example,
when we think of something in terms of an object, then we want the pixels to be kind of clumped
together, right? And if you sort of randomly distributed the pixels amongst a whole bunch
of other pixels and you move them around in a systematic way, well, we might be able to kind
of pick out the gestalt there, but we might not. And that would be because we're not able to see
it as an object because we have human priors, right? So, you know, I think that probably all
the problems that he's designed because we don't know what the hidden held-out set is, but I imagine
that they pretty much all use, you know, sort of human comprehensible priors and appeal to,
you know, foundational common sense of the sort I alluded to. Yes. I'm sure there must be some
kind of universal priors because in quantum field theory, physicists use things like locality and
sparsity. Yeah, some really, really high level things like objects. But then again, you know,
quantum mechanics challenges the very concept of an object even. So what is the difference to you
between adopting a stance that a system is as if conscious versus it being a fact of the matter?
I'm a bit resistant to the distinction, to the very distinction.
But this is a very difficult position to maintain because we have a very, very strong
intuition that there is a fact of the matter about our own consciousness. And it's very, very
difficult to escape from that very, very basic intuition. But I have a whole approach to these
kinds of questions. So shall I sort of describe this? So, you know, a really question that really
motivated me was that was, you know, suppose that we encounter, well, actually, let me not
use the word encounter, suppose that we come across has some object, which is a completely alien
artifact. And maybe there's consciousness going on inside this artifact. And the thought is, well,
how would we ever know, you know, there could be consciousness, this thing could be conscious,
but we might never know. And so suppose that it were a white cube that were deposited in front
of your lab, and you were tasked with a problem of, would it be moral to throw it down a mine
shaft and forget about it? So my approach to these problems is that I think in order for the
question, in order for us to be able to answer the question of whether something is conscious or not,
for even to be answerable or askable, then we then we need to be able to engineer an encounter
with the putative conscious, putatively conscious being. And what I mean by that is that we have
to be able to put ourselves, you know, we have to be able to put ourselves in a position where
we're sharing a world with that, with that putatively conscious, you know, artifact or being.
And so, you know, a good example of this is the octopus. So Peter Godfrey Smith has written
these wonderful books about what it's like to hang out with octopuses and be with them and so on.
And the really important aspect of that is that he has to put on a diving suit and go down
and be under the water and spend time with the octopus interacting with the same things
and being in the same world together, seeing the same things and so on.
So, and then on that basis and the behavior that he observes and so on, then, you know,
he might come to some kind of, he might start treating it as a fellow conscious creature.
So by analogy, or as you know, similarly, what I think that we need to be able to do
is to engineer an encounter like that, even if it's a very, very alien kind of artifact, say.
So suppose it's this white cube, then one way that it might happen, well, suppose scientists
managed to figure out that there's computation going on inside this white cube, and then they
managed to reverse engineer this computation and they can see that there's a sort of division between
a world and the things interacting with that world in this computation. There's a sort of
simulated world. And then you could imagine, by some clever engineering tricks, inserting
yourself into that very same world and being alongside these things that are interacting
with this environment and interacting with that environment with them. So being in the world with
them. Now, obviously, I'm setting this up to be very much like a games environment and a virtual
world and a games environment, but to make the thought experiment work. But so that's an example
of where, you know, if you manage to engineer an encounter with, you know, these things that are
inside this cube, and then you can observe their behavior, you can interact with them,
and then you can decide whether you or you will, you know, you may or may not start to treat them
as fellow conscious creatures. So there are these two steps. It's sort of, can you engineer an
encounter, at least in principle, and that makes the question answerable. And then you can answer
the question by actually having the encounter and interacting with them. And by the way, we notice
that everything there is public. You've made, you know, there's no private realm of subjectivity
everything is public. It's on the basis of public stuff that you come to see them as fellow
conscious creatures or not. Yeah, a couple of things on that. I mean, as you pointed out in
Conscious Exotica, the octopus is quite interesting because it's not as human like yet, as you just
cited, more conscious. And the way that we figure out the consciousness, and you know, this is me
kind of interpreting what you said a little bit, is we set up a language game. And I don't know
whether you've read that book by Nick Shater and Morton Christensen, but it's a beautiful book,
beautiful book. But you know, I know of the book, but I haven't, I'm afraid. It's incredible. But,
you know, they basically say at, you know, Per Wittgenstein that you play the language game,
and you, because you're physically sharing the same environments, you improvise, and that's how
you derive meaning. And meaning is very, very important for relatability. And then we ascribe
consciousness to that kind of process. And you cited Peter Singer actually, and I think he said
in 1975 that we have a natural inclination to kind of ascribe moral status to beings which we
think of as conscious. Yeah, yeah, indeed, yeah. Yeah, tell me more. So in the context of the
octopus, then the sort of, you know, you could, then there aren't going to be language games,
you're not going to be engaged in a language game with the octopus because the octopus is not
a fellow language user. But your fellow language users are other people in your community with
whom you'll talk about the octopus. And you'll talk about the octopus and together you'll arrive at
some consensus, hopefully, about whether you want to talk about it in terms of consciousness.
And that's going to be all to do with like observing its behavior, listening to other
people's accounts of being with octopuses. And critically, maybe listening to also what scientists
have discovered when they look inside octopus brains and they perform behavioral experiments.
And, you know, that's all, again, is public. That's all is grist of the mill of settling on a kind
of the way we talk about these strange creatures. Yeah, I mean, I guess for the language, there's
two parts to this. So the language game, first of all, it doesn't have to be spoken words,
it's improvisation of any kind, it could be gestures, it could be all sorts, it's just
behavior. Right, okay. And then the interesting thing with the octopus is we might not be
interacting with them interactively. We might be non interactively observing them as agents
interacting with each other, playing their own language game, but we can still ascribe some
kind of measure of and I actually think what we're measuring here is agency and agency and
moral status, I think are pretty much one to one. So when we see them playing the language game,
we start to think of them as agents, therefore they have moral status.
Yeah, I mean, I certainly think that by observing behavior, then we may similarly, you know,
start to ascribe consciousness to other creatures. I mean, it's always much more persuasive
if it's interactive, I think, than if it's simply observing behavior.
Murray said that if a creature's brain is like ours, then there's grounds to suppose
that its consciousness, its inner life is also like ours. He went on, if something is built
very differently to us with a different architecture realized on a different substrate,
then however human like its behavior, its consciousness might be very different to ours.
Perhaps it would be a phenomenological zombie with no consciousness at all.
Murray said in Conscious Exotica that it's only when we do philosophy that we start to think of
consciousness, experience, and sensation in terms of private subjectivity. He cited David
Chalmers and his hard and easy distinction of consciousness as a kind of weighty distinction
using his phraseology between the inner and the outer. In short, he said to a form of dualism,
which is that subjectivity is an ontologically distinct feature of reality.
Wittgenstein provided an antidote to this way of thinking in his remarks on private language,
whose centerpiece in an argument to the effect that insofar as we can talk about our experience,
they must have an outward public manifestation. For Wittgenstein, only of a living human being,
what resembles or behaves like a living human being, one can say that it has sensations.
It sees, it is conscious, or it is unconscious. But isn't this just behaviorism? Behaviorism,
particularly in its radical form as advocated by B. F. Skinner, posits that all psychological
phenomena can be explained in terms of observable behavior and environmental stimuli without
recourse to internal mental states. But behaviorism is often criticized for neglecting the subjective
internal aspects of mental life. Wittgenstein argues against the notion of purely private
language, where words refer to inner experiences known only to the speaker. He contends that for
language to be meaningful, it must be grounded in publicly accessible criteria. So as Murray said in
his article, Wittgenstein argued against dualism or the so-called impenetrable realm of the subject
experience. Actually, he said that many folks who make the in-principle argument against AI
often retreat into subjectivity arguments, as our recent guest Maria Santa Catarina did.
Murray said that the difficulty here is that accepting the possibility of radically inscrutable
consciousness seemingly re-admits dualistic propositions, that consciousness is not, so to
speak, open to view, but inherently private. Yeah, so Aaron Sloman, who's a professor of computer
science and artificial intelligence in Birmingham, Birmingham University, so he introduced the concept
of the space of possible minds in an article in 1984. And the idea is that the collection of minds
that could exist in our universe, that do exist in our universe and that could, is much larger than
just human minds or even the minds of humans plus other animals. It encompasses extraterrestrial
life that might exist out there and it encompasses artificial intelligence that we might create one
day. So the whole space of possible minds is a very rich object philosophically speaking and
merits our study. Yeah, so Wittgenstein's private language argument is the really the centerpiece
of the philosophical investigations, which is the book that was published after his death,
which really articulates his later phase of philosophy. And it's all about the idea that we
have, or that we can talk about, private sensations. So things that are purely subjective and that only
I as an individual can understand and know what they mean. So what red is for me and just,
you know, internally for me. And so the private language remarks sort of undermine that very
conception. So the basic idea is he imagines, he says, well, so what he means by private language
is very important to kind of get this right. So he doesn't mean a language that I've invented and
that is just something that nobody can understand just because I've invented it. It's the reason
that it's a private language is because it's about something which only I can access subjectively,
which is what red is like for me. So it's the idea that you can have a word for that completely
internal thing that is just mine. So he says, imagine that I keep a diary and I keep a diary
and every time I have this experience, a particular experience, then I write s in my diary to label
that I've had that experience. So maybe I have this, I think I'm having this experience on a
particular day and I write s and then a few days later, I think I'm having that experience again,
so I write s again. Now, the question he asks is what possible criterion could there be for
the correctness of that word? What would make it actually stand for anything meaningful,
given that what really makes words meaningful is if they're understandable in a public setting,
if they're understandable really to other people. So there can be no kind of criterion
for correctness that anybody else could validate for this thing insofar as it stands for something
that's completely private. So then there's a whole set of remarks that after he sets up this
sort of little thought experiment that tell you the implications of it really. And there's one
really, really key phrase where Wittgenstein is always engaging with an imaginary interlocutor,
so an imaginary person who's arguing with him in the book. And so he's imagining this person says,
but aren't you saying that the sensation itself is just a nothing? Aren't you a kind of behaviorist?
You're just saying that it's a nothing. And his answer to this, well, I'm not saying it's a nothing
and I'm not saying it's a something either. The point was only that a nothing would serve as well
as a something about which nothing can be said. And that little kind of paradoxical sounding,
weird sounding statement encapsulates something really, really, really profound. And I think when
I first really kind of understood what he was getting at there, it had a really dramatic shift
in the way I thought about consciousness, subjectivity. And to my mind, it is the thing that
really undermines dualism. It's the most powerful way to undermine the dualistic intuitions that we
have and that date back to Descartes and before Descartes that were articulated very well by Descartes.
Many of my friends are fans of Wittgenstein, but they are also fans of subjectivity. So as you
were just alluding to what Wittgenstein did was he created this kind of barrier between the inner
and the outer. He said, you know, for things to be promoted into the language game for this
emergent structure that we have, you know, when we memetically share all of these language
constructions, that can only come from something observable. But it doesn't seem inconceivable
to me that it could in principle come from something private. So for example, you might
have a drugs experience and that's clearly ineffable, you can't find the words, but there are
things that have some semantic overlap. So I experience red, you experience red, we both
have different experiences, yet when we talk about them, some kind of overlapping category
still emerges in the public space. Yes, absolutely. So so what emerges in the public space, that is
what we can talk about. And that is that is by the way you've set up the experiment is by definition,
not private, it's public. So of course, we can both talk, we can both point at something that's
red and say, oh, look, look at that red. And you say, oh, yeah, isn't that isn't it beautiful?
There, it's manifestly, we're talking, insofar as we're talking and successful in
communicating with each other and agreeing with each other, then that's the element that is indeed
public. But are we not sharing, you know, is language not a set of pointers to our simulation?
So we're simulation sharing when we talk. And even though our simulations are different,
is the pointer, does the pointer not form some kind of category over all of our simulations?
Oh, well, there's a whole, you've introduced a whole load of terminology there, which,
which, you know, I don't know what you mean exactly by shared simulation and so on. So I think
in the context of a philosophical discussion, as soon as you introduce new, new bits of
terminology like that, then often that's the point at which you're starting to go wrong,
right, in philosophical discussions. In technical discussions, of course, you of course,
we're going to introduce new terminology all the time. But, but, but that's the moment where
often things are going when, as Wittgenstein would say, you're starting to take language on holiday
and take it away from its normal usage. So I don't know what you mean by kind of a shared
simulation, you'd have to tell me a little bit more about that idea before I could engage with
that thought experiment, I think. Well, I mean, I'm schooled on, you know, the Karl Fristons of
this world. And there's this whole thing about the Bayesian brain and perception as inference and
so on. And, you know, the basic idea is that we, you know, our everyday experience is a hallucination,
you know, we don't, what we experience isn't necessarily what is out there. And language
is is a kind of pointed to those simulations. And they must be divergent, they presumably
are divergent, yet miraculously, we can understand each other. Yeah, well, I think that so that so
the Wittgensteinian point is that we understand each other in so far as in so far as we,
you know, what we understand is what is shared, right? And anything outside of that is,
you know, we by definition can't talk about. And the difficulty is that we have this strong
inclination to talk as if there is this thing that's not shared. I mean, what really fascinates me
is that understanding it's not a binary, there's a spectrum, and we delude ourselves that we
understand things deeper than we do, because it goes into the realm of subjectivity. So when I
understand something, my brain is invoking all of this rich subject of experience. And I'm probably
taking my understanding into a domain which is beyond which that you understood. And perhaps
this is just something we willfully do all of the time. So what do you mean exactly by invoke my
brain is invoking all this subject of experience? What do you what are you what are you getting
out there? Well, so we talk about, as you say, the language game is based around public information.
So there is a kind of cultural level, a lowest common denominator of understanding. But when we
understand cultural artifacts, we further invoke our own subjective experiences. So for example,
when I laugh, I have the experience of laughter, this phenomenal experience. And this is clearly a
form of understanding, it's a subjective form of understanding. And when someone else laughs,
I feel that we are sharing this ontology, right, we're sharing it, but we can't possibly be.
Well, so I mean, you're straight away introducing all kinds of funny talk here, right? So we're
sharing an ontology when you're just talking about an everyday experience of laughing together,
which we can talk about without any kind of difficulty, and without raising any kind of
philosophical problems, just by saying, Well, you know, we both heard that that that joke, and we
were both, you know, on the floor and laughing. It was so funny. It was an excellent joke, right?
We can talk about that in everyday terms. And and there are no problems. There are no philosophical
problems. But as soon as you start, start, you know, getting philosophical, and you start talking
about that, you know, what was it? What was your phrase? There's something about subject
sheds about subjective ontology or something. Yeah, you're introducing all of this kind of
technical terminology. And that's that whole, that's a whole layer of confusion on top of our
ordinary everyday ways of talking about these things, which are unproblematic.
Okay, but then there's the anthropomorphic lens. So you're a human, we both laugh, the behavior of
laughing is publicly observable. Therefore, we have the same experience, because we have the same
behavior. Well, it depends what you mean by by understand here. So so so for sure, you know,
it is a fairly common form of speech to say, to say, Oh, well, you know, you can never understand
what it was like to give birth, because you're a man, you know, and this is of course, this is a
normal way of expressing oneself. And again, that's that sort of unproblematic. So there's
there is a sense in which, you know, in which that's that's undoubtedly true. But the problems
arise when you when you start to, to, to think that this, that what underlies this difference
in understanding or the one underlies that way of talking is is is some kind of, you know,
inner private realm that is, you know, that is index that is that is metaphysically distinct from
from the rest of reality. When we share these pointers or these symbols or whatever,
structure still emerges, we still feel that we have a shared understanding. And that understanding
can probably be factorized into a public component and a private component. I don't think
that's kooky to say that. Well, I see, you see, you're very keen to say, well, it turns a little
bit what you mean by a private component there, right? So if you really mean, you know, sort of
metaphysically inaccessibly, private and subjective, then then I think, then I think
it's not appropriate to speak of dividing things into this private and public component. So that's
where that's where things start to go wrong. And moreover, you insist that you're not a
dualist, right? But I think your inclination to make that division shows that you have dualistic
inclinations, as we all do. So people who are denying that they're dualists, they're denying
this that little seed of dualism that I think is in all of us. And that is part of our part of the
way we, we, we, you know, we think and the part of the way you naturally go when you start to do
philosophy. And so it's all, I think it's all very well to say, oh, you know, I'm a materialist
and I don't, but then when you, when you start to kind of probe and you start to discover the
puzzlement that these things give rise to, then that exposes a bit of latent dualism there. Now
overcoming that latent dualism, that is the real challenge that Wittgenstein confronts.
Well, I love the challenge. So the way I see it is there is, there's a ladder. So at the top,
you have an experience which is ineffable. And then one step down, you have an experience which
is inconceivable, which is Naples argument. And then the, you know, if you really go down the ladder,
then you get into this metaphysical dualism. So I guess I'm somewhere between the first step
and the second step. So I think if I have a certain type of experience, I simply don't find the words,
I can't communicate it to you. But if you put probes in my brain or something like that, I'm
sure that could conceivably be a way of measuring it. Yes. Yeah. So, so, so this is really important.
So for me, what counts as public is not just behavior, but it's also whatever scientists
we can discover. So that, so, so if we poke around in people's brains and we do EEG recordings and
FLRI recordings and anything else that we can imagine. And then I as a scientist can see this
stuff and use a scientist and our fellow scientists will see, see that. That's public too. So that's
in the, for the purposes of this discussion, of this philosophical discussion, that's all in the
public realm. It's not metaphysically hidden. You can, you can, and all of that can feed into the
way we talk about consciousness. And especially if we're talking about exotic entities, then,
then all of that can feed into the way our language adapts to, to, to, to, to our encountering them.
Yeah. So I think it's fascinating to decompose as you just did what people mean by subjectivity. So
of course, some people like David Chalmers, they argue that there is a little bit extra. So there's,
you know, behavior function and dynamics. And then there's that, you know, little bit extra,
which is not observable in any scientific way. And I think, you know, it's fair to say a lot of
people when they talk about subjectivity, they're not talking about the little bit extra. But when
we do get to the little bit extra, I completely agree with you, we've got a big problem.
Yeah. Yeah, I think we have got a big problem because, because of our natural, you know, dualistic
tendencies to, it's very, very difficult to think that, that, that, you know, if I experience a pain,
that, that, that there isn't something about that that is just purely minor, that you couldn't,
you know, the outside world, that other people can never really, you know, experience it. But
that's, it's having that thought, that's this moment that you kind of go wrong, but it's natural
path to go down. It's really, really hard to avoid it. And, and that's where I think
Sylvitkenstein's remarks, they, they provide a whole way of, of trying to reorient your whole
way of thinking. And, and, and if you sort of really kind of grasp them, it sort of flips your
whole world around, it flips your whole way of thinking around. So it's so that the, this whole
way of talking and thinking becomes wrong. So it's so that so very often the strategy when
you're dealing with this is somebody throws out this thought at you, like you've been throwing
out various thoughts at me about, and, and, and often buried in the way those thoughts are framed
is the problem. So, so the, the problem is the very expression of those thoughts. And you have
to take a step back and say, hang on a minute, you know, you made this funny move, you introduced
this funny bit of language, you introduced this funny way of expressing things. And that's, that's
when that's the, the, Victor Stein has this phrase that is, that's where the conjuring trick
happens is where you, the point that you don't notice is where the conjuring trick happens.
So, so, so it's kind of, so often you have to take, take a step back and you have to sort of say,
hang on a minute, I don't accept that way of talking that you've just suddenly introduced,
which is going down a philosophical garden path. Yes, and I completely agree. So, so that is,
that is a form of dualism, you know, when, when we resort to that little bit extra.
And I'm quite interested in this actually, because people like Chalmers, I don't think he likes the
term dualist, I think it's a property dualist, but he does talk about the philosophical zombie,
which is a thought experiment of something which has all of the behavior of us, but
is lacking in conscious experience, which gives rise to this idea that it's almost a kind of
epiphenomenon or it's something which, you know, almost you're asking the question, well,
well, what's it doing if it's not affecting anything? And when I read your conscious Exotica
article, I had a similar thought actually, because you showed this linear correlation between, you
know, human likeness and consciousness. And then you gave examples of algorithms,
you know, like AlphaGo, for example, and they didn't need the consciousness.
And that again raises the question of, what is the cash value of consciousness?
When we use, when we're using the word consciousness, then often we are using it in the
context of certain, you know, of certain behavioral behavior and behavioral inclinations,
and we use it in the context of other humans and other animals. And there's a whole,
I mean, for a start, the word consciousness is actually, you know, it's a multifaceted concept
that it's alluding to many things. And one of the things that it is alluding to is our ability
to deal flexibly with the everyday world. So we speak about, oh, you know, I didn't notice
the chair, that's why I bumped into it or something. And, or, you know, I didn't see,
you know, that there was a desk over there that might have had something interesting inside it,
if you opened it up. And so we talk about our awareness of the world. And we're at the same
time, we're talking about an aspect of consciousness, and we're talking about
a whole load of behavioral dispositions and capabilities. And so these things are very much,
you know, are very much related to each other in our everyday speech. So then the question arises,
though, are they dissociable? And so now it's very important that it's not like I think there's,
that consciousness is some metaphysical thing whose essence is out there to be discovered.
It's just, it's a concept that we invent and a word that we use to describe the world around us
and our place in it and each other and so on. And so, so, so then, you know, then the question is,
are there things that we might create or imagine, where we'd want to use the one concept, we want
to use the one set of words and not use the other. And that is what the question comes down to.
So in the case of something like AlphaGo, then I think, you know, we're all kind of agree that it's
actually there's a kind of cognition going on there, there's a kind of reasoning going on in
AlphaGo. There's certainly a lot of kind of cleverness, there's a kind of intelligence,
there's even, if we're thinking about move 37, a kind of creativity. So we're willing to use
all of those words, but nobody is going to suggest that AlphaGo is conscious. So there we can see
that they're under certain conditions, the concepts are dissociable. But nevertheless,
there's a strong relationship between the two, because if we think about animals, then often we
are going to, we're going to use their cognitive abilities as manifest in their sophisticated
behavior. We're going to use that as a proxy for sometimes whether we want to talk about them in
terms of consciousness. So sometimes, in our usage, we're going to bundle the things together,
and sometimes we're not. But this is all just a matter of, it's a kind of a practical matter
of how we use language and how it's usefully deployed, how language is usefully deployed.
And it's not about discovering some metaphysical entity that's out there,
which is what conscious, the word consciousness denotes.
Demis Esalvis recently spoke about this ladder of creativity and of course,
inventive creativity that move 37 was discussed. But Daniel Dennett, rest in peace. I'm so glad
I had him on the podcast actually. He's a huge hero of mine, and I believe that a hero of yours
but he coined this term the intentional stance. And what's interesting is he was
using it to designate a rational agent, but actually it gets overloaded and I'm guilty of
this. You overload it for lots of things, including even for things like consciousness. And maybe
that's because of the correlates of cognition, these things are very closely related. But
can you explain in your own articulation the intentional stance?
Yeah. Well, so I think you can use the concept and deploy the concept of the intentional stance
without necessarily embracing the whole of everything that Dan Dennett was talking about
in that context. Because for him, there's a whole big philosophical position around it,
but there's a very simple sense of the intentional stance that we can lift from Dan without
necessarily buying into everything that he said. And it's simply to say that we often in everyday
terms speak about artifacts and indeed animals, you know, other animals, as if they were rational
agents that act on the basis of what they believe and what they want. And by talking about them in
those ways, whether they really, whatever that means, do believe things or have desires, it's
useful for explaining and understanding their behavior. So if we adopt the intentional stance,
say, to use one of Dan Dennett's own examples towards a chess machine, a chess computer,
chess program, or Go program, and we might say, oh, it advanced its queen because it wants to try
and pin down my rook. And this is just a natural way of speaking. And if we use that way of speaking,
then it's just good in every way because we can then discuss among ourselves what the machine
is doing, we can explain what it's doing, we can predict what it's going to do. So that's
taking the intentional stance. And it doesn't necessarily bring with it a belief that these
concepts are literally applicable. Maybe they are, maybe they're not.
But this is where it gets interesting. So I discussed abduction, actually, when I spoke with
Dan, because it's very closely related. I know you studied reasoning for many years.
And the way I see it, when we adopt the intentional stance, what we're doing is we're
kind of building a set of variables to describe the behavior of the entity. And you were just
making the argument from the lens of Wittgenstein that the behavior is the only thing.
No, no, no, no, no, no, no, no. I definitely don't think that we mean meant by behavior
as the only thing. But certainly when we're deploying psychological terms, I don't think
that in any sense behavior is the only thing that determines how we deploy psychological terms.
You're absolutely right. So there's still a massive amount of ambiguity. So when we perform
abduction, we are creating a hypothesis and we're selecting out of an infinite set of possible
hypotheses. But the behavior gives us all of the information. So it's almost like if we knew
how to create the correct explanation, we wouldn't be missing anything just by observing the behavior.
So what are we talking about now? We're talking about chess machines or animals?
Or what are we talking about? What's the context for this thought?
I guess it could work for both. So let's say I want to adopt the intentional stance for move 37.
And I do this abduction. So I build this plan that the agent had. So the agent had this intention
and it took this sequence of steps. And I'm using that as a hypothesis to explain the behavior.
I'm adopting the intentional stance. But it's still highly ambiguous because I'm selecting
out of an infinite set of possible hypotheses. Right. And in fact, in that particular case,
it's almost certainly not the right way of thinking about it at all. Because unlike humans,
who are when they're playing these games often do form plans. So if you're playing chess,
you often do have a plan, I'm going to try and capture this area of the board and command this
area of the board say. And so I'm going to move these pieces around to try and do that. And you
might form a plan in terms of several moves, but ahead. But typically that's not the way,
that's not really the way AlphaGo works. So talking about it making plans isn't really the
right way of doing things. So it's interesting, actually, because the intentional stance,
you know, maybe it's still white work, you can still talk about something forming plans maybe,
but it's really not quite right in that case. When I say subjectivity, I'm not talking
about metaphysical or dualism, but the intentional stance clearly is a form of subjectivity.
And when we as a diverse collection of agents form our own intentional stances, it would seem to be
quite a chaotic, weird and wonderful thing, yet it seems to work. There seems to be,
by the way, an interesting thing here is the way we ascertain agency and culpability is based on
the intentional stance. So you read a news article about someone being stabbed in Australia or
something like that. And the news article was trying to give reasonable explanations. Oh,
it was because he was in a cult or it was because he was religious or it was because,
and this helps us kind of assign moral valence to what just happened.
Yeah, yeah, absolutely. Yeah. You said something like, when we take the intentional stance that
that is a form of subjectivity or something? Yes, would you agree with that?
So I wouldn't put it quite that way. I'm not quite sure exactly what you mean by that,
but taking the intentional stance is, I think it's just adopting a certain terminology and a
certain vocabulary for describing the behavior of something. So I don't think we need to bring
subjectivity into that at all, right? So I think maybe we're mixing up two completely different
senses of the word subjectivity here, which is something we should be very careful about.
So I think you mean subjectivity, you mean that you've just made your own choice between
different hypotheses. And so it's subjective. Is that, is that what you mean there?
Yes. So let's say, so for me as an observer, I might do some, let's call it probabilistic
reasoning. And for me, the most reasonable, rational explanation is this. And it's a for me
there. That's what I mean. That's what you're alluding to the subjectivity. Yeah. Yeah. Yeah.
Okay. Yeah. Well, so, so, so your for me is, I think that's that, that the sense in which
that subjective is a very, very different one from the topic that we were talking about earlier on,
because I don't think there's anything philosophically problematic in, in, in saying that,
you know, that I made my choice. And that's my preference and so on. And so, and somebody might
say, well, that's just subjective. And sure, okay, there's, there's nothing philosophically
problematic about that, right? So, so they, but earlier on, we were talking about subjectivity,
which like the big capital S and where it's alluding to kind of something whole metaphysical
thing and the issues of dualism come up. So, so, so, so yeah, so I think this is a very different
kind of thing. So that's fine. So for the remainder of this conversation, capital S subjectivity is
dualism and, and lowercase subjectivity is for me. Yeah, it's for me. Okay. Yeah. Okay.
Can you tell me about the risks of anthropomorphisation?
Yeah. So I think so in the context of, of contemporary artificial intelligence in particular,
then, then the danger of anthropomorphism, I think, is in, is in thinking that,
that a system such as a large language model, you know, a chatbot or something,
thinking that it has capabilities and that it doesn't, that's as simple as that.
Actually, it's also thinking, perhaps, that it lacks capabilities that it does. So, so in,
so in both cases, I think we can go wrong, we can go wrong by, because they exhibit very human,
like linguistic behaviour, we can just assume that they are going to be very human like in
general in all of the rest of the behaviour that we encounter with them. But we often find that
that's not the case. So we can find that at one moment, a large language model might make a
ridiculously stupid mistake that no child would make. And, and, and, and then the next moment,
it's saying something extraordinarily profound philosophically, or, or summarising some,
in, you know, enormously difficult scientific article, you know, really accurately. So, which
is, so these things are kind of superhuman powers, or translating something into four
different languages all at once. And they're, they're sort of superhuman-ish capabilities.
So it's not, so it can actually be more than better than human in some directions.
And, but, but clearly very deficient in others, you know, with contemporary models that can make
all kinds of stupid mistakes, they can confabulate, they can make errors of reasoning and just say
daft, generally daft things. So, so those, so those are examples of where, you know, it's a mistake
to, on the basis of, you know, a certain amount of interaction to think, oh, it's just like a human,
you know, because you can just, you can just misjudge it in many ways. That's one thing. There's
another, other aspects of anthropomorphism. So that's, that's just in terms of its cognitive
capabilities, if you like. But there are other problems with anthropomorphism. So if we see
empathy there, where there isn't real empathy, then we may trust something where there's no
real basis for trust. And so that's a problem as well. You know, we may form, people may form
relationships with, with, you know, AI companions and social AI, where they're kind of fooling
themselves into thinking that there's a basis for that in emotion, where there is in humans,
and it's not there in, in, in contemporary AI. And I think that all those things are problematic.
So things to do with trust, to do with friendship and empathy, and all of these, these things,
I think, where we can go wrong in seeing, seeing them as too, you know, as more human-like than
they really are. One of the issues I have with anthropomorphism is that people ascribe mental
content when it's not there. And I think you are talking about literal anthropomorphism,
which is that they see human-like qualities when they are not there. And to me, that, that's an
important distinction, because I think if I understand you correctly, you, I mean, you're
very known nonsense. You say that current large language models, they don't reason, they don't
form beliefs, they don't have a sense. Oh, no, I didn't say exactly that. Oh, did you not? That's
that's a broad, that's a much broader claim. So that's right. So I'm not sure I'd go so far as to
say they don't reason, or they don't understand, or they, or they don't form beliefs, but rather what
my, my, my approach is to say that we should be very cautious in using those terms. So I'm, so
those would all be examples of taking the intentional stance. If we were to use those
bits of terminology to describe what, what a current, you know, chatbot or conversational AI
was doing, we'd be taking the intentional stance, and it's perfectly reasonable to do that in very,
very many cases. So I would know, and I would never make the blanket claim, they don't understand.
I think that that's not quite right. I think rather, rather it's that, you know, sometimes
it's appropriate to say, oh, yeah, it seems to understand very well what this big long article
about nuclear physics was, was all about. And it summarized it really well. It really understood
it. You know, somebody might come up, might say, you know, it really did seem to understand it.
I think I wouldn't say that they were wrong in using that phrase there. So, but then on another
occasion, you might find that it's, that it, for example, recently, people have been posing this,
you know, this classic goat, cabbage, Fox problem where you've got a boat and you have to cross a
river with a goat, and you can't have the, you know, more than two things in the boat at once,
and you can't have the goat with the cabbage and all this kind of stuff. And so there's a,
it's a little puzzle and you just have to kind of cross lots of times and do all kinds of trickery,
right? And people opposed, opposed to it to some large language models that said, okay,
I've got a boat and a cabbage and I need to get the cabbage to the other side of the river.
How do I do it? And the large language model models, several of them just start to come up with
this totally baroque solution that involves going backwards and forwards over there. And sometimes
they invent goats that aren't even in the, and that's because they've overfitted or they're kind
of like connected with this classic problem. And, and so no child would make this stupid,
stupid mistake. So anyway, so there you would say, well, you know, just, you know, just obviously
just didn't understand what the, you know, and of course, it's quite right to say didn't understand
in that case. And the anthropomorphism, it comes about when you think that it understands
when we understand and doesn't understand when we don't understand. The reality is that sometimes
it understands when we understand and sometimes it won't and some, it's all mixed up, right? So
the mistake is to think that it is like us. I could, yeah, man, that was beautifully articulated.
So it's, it's, it's the mistake of thinking there is an alignment both in how the machines think
and where they make mistakes. But let's unpick this a little bit because you were saying it's
perfectly reasonable to take the intentional stance when the thing does the thing correctly,
even though it thinks differently to us. And that's absolutely fine. But I love thinking
about these things theoretically. And it's, it's delicious talking to you because you have a background
in symbolic AI. You were, I'm sure around in the days of photo and pollution with their connectionist
critique. And, and even now there are clear examples of language models not being able to do
negation. Oh, yeah. And we know they're not Turing machines. You know, we can make some strong
theoretical statements that they are limited in reasoning. I agree with you that it's reasonable
to say they understand in certain circumstances. But, but, but where I want to get to is, okay,
so we agree that language models cannot perform certain types of reasoning that we can.
Yeah. So I think we could, so I think we need to take each of these concepts individually. So,
we dealt a little bit just now with understanding reasoning as a whole separate thing. So, and,
again, this is all because, you know, they're not like us. So we have to deal with these things
individually. We can't just blanket say, oh, they don't understand, they don't reason, they don't,
or they do understand, they do reason. It's not like that. It's, you have to take each of these
concepts separately. So in the case of reasoning, then clearly today's large language models,
you know, do struggle very often with, with, with reasoning problems. Now, this is a kind of open
research problem. And people are making a lot of progress in improving their ability to solve
reasoning problems. Now, what the right approach to that is, you know, is an open research question.
Maybe it's just you throw more training data at it with, with, with, that includes lots of
reasoning problems. And then eventually you get sufficient generalization there. And it's not
totally clear that that will work, but maybe it will. Maybe it's to embed your,
your, or, or, or to surround it to include it in your, in your system, not just the large
language model, but making kind of external calls to, to say a planner or some kind of external
reasoning system. And you bring that in and you incorporate, you make something that's kind of
a hybrid that uses that, that kind of more symbolic approach. So you might do that.
Or you might also sort of, so in my work with Tony Creswell at, at DeepMind, we, we introduced
this selection inference framework where you basically, you treat the large language model
as a kind of module that does bits of reasoning. And you have a surrounding algorithm that makes
calls to the, to, to, to, to the, to the module in a kind of algorithm that does a number of
sequence of reasoning steps. So you have a kind of outer algorithm. So there's lots of ways of
trying to tackle that problem. But yeah, just your basic large, take your basic large language
model today, as they are at the moment, and you put it in a chat interface, it's easy to find
reasoning problems that are going to stump. Yes, we, we agree on that. And actually, my co-host,
Keith Duggar, he defines reasoning as performing an effective computation to derive knowledge or
achieve a goal. And he cites Claude Shannon, by the way, he said, Claude, Claude Shannon said,
we may have knowledge of the past, but we cannot control it. We may control the future,
but we have no knowledge of it. And science leverages control to gain knowledge, engineering
leverages knowledge to gain control. And reasoning is the effective computation in both. You know,
maybe just in your own articulation, because we can cite examples of things like abduction,
which we've studied in great detail. And it feels like at the moment, even if we do farm out to
Turing machine algorithms, in the days of symbolic AI, that was an intractable problem,
because we've got this infinity, right? And it still seems to me that there are some problems,
which there is no easy answer to. So I have a, I feel that you're alluding to
Fodor's, some Fodor type arguments here about abduction and maybe, but I don't know. But I mean,
I think that sort of abductive problems are where we are looking for an explanation for something.
So I think you'll find that there's a large collection of abduction problems that you could
just present to today's large language models, and they would do quite well at them, you know.
And at the same time, I'm sure you wouldn't be too difficult to find problems,
especially if they involve many steps where it will go wrong. Because doing multi-step,
it's the multiple steps that really are where today's large language models are a bit weak.
Again, it's an open research question. People are working on all that kind of thing. So if there
are multiple steps, and step n is dependent on step n minus one, and it's inherently,
they're inherently very computational sorts of things in that sense. So large language models
are a bit weak at that, for sure. And we can introduce things like chain of thought and so on
to try and improve that. But they're sort of a limited success. But my feeling is that those
are not the things where which large language models themselves are inherently strong at.
And you should probably make use of other tools in order to kind of boost reasoning capabilities.
Like I listed some of them. So somebody's making external calls to reasoning,
dedicated reasoning components. Sometimes it's embedded in the large language model
in a reasoning algorithm itself. So you can go either way, you can either take the large
language model, put reasoning things inside it as it were, so it makes external calls,
or you do it the other way around, you can have a reasoning algorithm and make the large
language model component of the reasoning algorithm. Yes, I suppose it's a similar thing to the
creativity that there's a kind of creative or inventive abduction, and then there's colloquial
abductive interpolation. And the remarkable thing is just how structured and predictable our world
is and how far you can get with the colloquial predictive abductive. Yes, yeah. I mean, this
way you speak of colloquial sort of abduction, which is the kind of thing that we can
all and the person on the street could do if you have some slightly odd situation and you say,
why is this man in the middle of the road with a policeman's hat on or something?
And you say, well, because there's maybe there's been an accident or something.
So we do all this kind of thing on an everyday basis. It's a little bit of abduction. It's what
you call colloquial abduction, I think. But large language models think you find a pretty good at
that kind of thing these days. But if you have something that's really complex and has a load
of steps to it, the kind of thing that humans would struggle at, then very often large language
models are going to struggle at those things too. Can you describe the Turing test? The Turing test?
Yes. Okay. So I'll describe the Turing test as it's popularly conceived because there are
new answers in Turing's original paper. But then again, he didn't call it the Turing test.
So the Turing test as it's popularly conceived involves having a human judge
and the human judge is interacting with two things. One of them is a human and the other is a computer
system. And the interaction is entirely through language, through a keyboard and a screen say,
or a teletype, if you like, in Turing's day. And the idea is that the human judge has a conversation
with these two things. And the question is, can the human judge tell which is the machine and
which is the human? And if the judge can't tell which is which, then the machine passes the Turing
test. Do you think it's a good measure of intelligence? I think it's a pretty rubbish
measure of intelligence. I mean, I think it's a very useful philosophical thought experiment
and starting point for conversation on this. But the trouble is, it's very easy to game.
Well, there's a number of problems with that, which people have been writing about for years,
by the way. So there are a number of problems with that. So one is that it's easy to game,
in a sense, because there's a temptation to make something to pass the Turing test,
you make something that has all kinds of strange human ticks and peculiarities,
and then it seems human. And so you can fool the judge that way by making it just a bit eccentric,
which is obviously nothing to do with intelligence at all. So that's one thing.
And then another thing is that the domain of the test is purely linguistic. So you're not
testing anything to do with the sorts of intelligence that you get in a non-human animal,
say. So dogs and cats and mice exhibit all kinds of intelligence in their ability to
navigate the ordinary, everyday world and survive in it. But none of those kinds of
intelligence are tested by the Turing test. So you were the scientific advisor on the film
Ex Machina. And I'm not sure whether it's Machina or Machina.
It is Machina, yeah. I was speaking with Irina Rishan and she said,
Ex Machina. But anyway, she's right. Yes, she is. I digress. But there was a special
type of Turing test in that film. Can you explain that?
Well, indeed it wasn't. It's not the Turing test. So there's a point in the film, I assume,
that people are vaguely familiar with the setup of the film. But there's a point in the film where
Caleb, the programmer, is talking to Nathan the billionaire who's developed this robot Ava.
And Caleb says, oh, what you're doing is you're trying to build something that passes the Turing
test. And Nathan says, oh, no, we're way past that. The point is, in the Turing test, you don't know
whether the thing that's being tested, whether you don't know whether it's a machine or a human.
That's the point of the test. The point, Nathan says, is to show you that she's a robot and see
if you still think she's conscious. So there's a number of ways in which this is very different
from the Turing test. So first and foremost, it's a test of consciousness, not of intelligence.
And those are not the same thing. And secondly, the point, as he says, is to be persuaded
that the artifact is conscious, even though you know that it's not human, not biological.
So you know that it's an AI system, and you still think that it's conscious. In that case,
it passes this test. So this test, I call the Garland test after Alex Garland, who is the
writer and director of X Machina, quite different from the Turing test. I think that those lines
in the film were actually really brilliant lines and really clever lines from Alex in the script.
And when I first saw the script, which was long before it was filmed, and that bit was in there,
and I put spot on next to those lines in the script, because I thought it was such a very
good test. I mean, presumably we're supposed to think that Caleb himself in the film does indeed
think that Ava is conscious and thinks of her that way. What that really means is that
he comes to treat her as a fellow conscious creature. And we see that in the film because
he wants to help her escape. And just as a kind of thought experiment, maybe it's another
conceivability thing, but it does seem conceivable what less consciousness would be like, but it
seems less conceivable what more consciousness would be like.
Well, so in both cases, I think it's all about exercising our imaginations actually. And I
think exercising our imaginations is perfectly legitimate in philosophical discussions. So
in fact, in my newer paper, Simulacra as Conscious Exotica, I think I say that I advocate doing
philosophy with the detachment of an anthropologist and the imagination of a science fiction writer.
So I think that's something that I aspire to do. So we can carry out all kinds of imaginative
exercises to describe exotic entities in a science fiction like way. So we can describe an
exotic entity and we can describe all kinds of strange behaviors. And that's sort of as far
as we can get really, I think we could we could we can imagine all kinds of strange behavior,
and we can imagine scientists studying those kinds of strange behavior and what underlies them as
well. And so we can imagine all of those things. And then and then we can also imagine how we would
talk about those things. So imagining how we as a kind of community and how the scientists and
the philosophers would talk about, about these imagined entities is all kind of part of it.
So I think we can do all of that. I think that is a kind of doing philosophy.
And I'm just coming back to the Turing test one last time. I read an interesting take on Twitter
recently that we've been thinking about the Turing test or wrong. There seems to be a subset of
people who it's almost like the Eliza effect. They see something they want to see something.
And it's almost like the test is actually testing the humans rather than the intelligence.
Yeah, yeah, sure. I mean, yeah, it's very, it's very tricky territory. And again,
I think we should distinguish consciousness and and intelligence in this in this regard,
you know, although there are relations between the two or consciousness and cognition.
So what just one thing I wanted to say about the about the Turing test is I do
feel actually that today's large language models kind of pass the spirit of the Turing test.
So so we defined the Turing test earlier on or the or the kind of
the popular conception of the Turing test earlier on. And as I remarked, you can kind of game it
and you know, and there's certain sort of problems with it. But notwithstanding that,
I think that actually today's large language models pass the spirit of the Turing test, I feel.
So they pass the spirit of the Turing test because they do really have human level conversational
skills, I feel. So I might my feeling is that if Turing were alive today and were presented with
Gemini or ChatGPT or Claude III, he would say, yeah, that's what I had in mind,
you know, you've done it, that's that's it's kind of passed.
Interesting. And you also distinguished, you know, the the imitation game is defined by Turing and
the the colloquial popular conception is that there's no adjudicator, it's just, you know, a person
and an intelligent machine. What is what is the kind of the main difference there?
What I mean, meant by the popular conception, I think was that maybe popular conception isn't
quite right. There's sort of contemporary version of it is that is used in academic
discussion, in fact, is what I meant. That's what I meant by popular. So I so there I'm
imagining, yeah, there is a human adjudicator there. But the difference is that in so Turing,
the way Turing sets up the test is is, you know, he has some specific specificities about,
you know, the number of minutes you should, you know, you spend interacting with it. And then
also he sets it up in the context of this game where party game where the aim is to try and work
out whether which you've got a man and a woman that somebody is is is convert conversing with,
you know, via paper, and they don't know which is the man and which is the woman,
they have to guess which is which. And that's the way the thing is set up in the original
paper is by analogy with that. So there's all so so there's all kinds of, you know,
peculiarities in the original paper, if you're a Turing scholar that aren't aren't really quite
relevant to the I think the kind of contemporary way that we think of the Turing test. But I
also think that the sense in which we in which today systems pass the spirit of the Turing test,
it's the reason it's only the spirit of the Turing test is because, of course, you very
often can immediately tell that it's that it's an AI, not not least because it'll just tell you
right away, right? If you just ask it, it will just say well as a large language model trained
by Google or by, you know, open AI, so it's easy to tell which is which. So but but but but but
nevertheless, I think that they have attained more or less human level language skills,
more or less. And so I think I do think that Turing would would would say, you know,
as I predicted, you know, yeah, we've got there. Now, I think Turing would be fascinated to see
the weaknesses that are there and the strengths that are there. And, you know, there are many things
that today's systems can do that are vastly more powerful than I think he anticipated
in that paper in the 1950s. And then there are other, you know, there will be other weaknesses,
I think that would come out that would surprise him as they've surprised us all in a way. I think
I think many of us in the field are surprised to see something that can do so amazingly in
certain respects, and yet have so many, you know, still so many weaknesses and others.
Can you introduce Naegle's bat? So in 1974, Thomas Naegle published this paper called
What is it like to be a bat? And the point of this paper was to draw attention to the fact,
if it is a fact, that there are creatures that are very, very different to ourselves to humans,
but that we assume have some kind of consciousness. We assume in his terminology,
we assume that it's like something to be that creature. And he chose a bat because bats are
very obviously very different to ourselves. They fly, they use sonar, they're pretty weird animals.
And so the way the thinking is that, well, it's probably like something to be a bat,
but what it's like is going to be very different from what it's like to be a human.
And Naegle uses that example to get at a whole metaphysical idea, which again,
I would say is pointing to a kind of dualism, to suggest that there's a whole realm of facts
about subjectivity, which are outside of the purview of objective science, actually,
but which nevertheless are part of reality. And so for him, I feel that it alludes to a sort of
kind of dualistic way of thinking again, that there's this realm of
facts about subjective things, and there's a realm of facts about objective things.
I read Naegle's bat many years ago, his paper, and he was kind of saying, wouldn't it be great
if we could move towards an objective phenomenology? And you are clear that he is pointing to dualism.
He's not just saying that it's inconceivable in the sense that I couldn't imagine what the experience
of a bat is like. It's just inconceivable. You're saying that he's actually making a dualism argument.
Well, he would probably deny that, because nearly every philosopher will enthusiastically deny that
they're dualists, but I see dualistic thinking all over the place, and I do think that this is an example of it.
Yes, I mean, it's a similar thing with John Searle. I think he is adamant that he's not a
dualist, but lots of people think he is a dualist. And actually, the Chinese rim argument is another
example of a popular thought experiment, which apparently people get wrong. My friend, Mark J.
Bishop, he always makes a point of saying that people misunderstand the Chinese rim argument.
Yes, Mark has tenaciously clinging to the Chinese language, the Chinese rim argument.
But I can't really see eye to eye with Mark, who I have enormous respect for on this particular
subject, I'm afraid. Yes, Mark is a very good friend of mine, and maybe that's a rabbit hole,
we won't go down. But I'm very amenable and convinced by some of Mark's arguments. But he's
certainly a phenomenologist, which is adamant that he is a monowist. It's another example of
someone who claims they're not a dualist, but you would say they probably are a dualist.
Yeah, well, I don't know. We'd have to have Mark sitting here. I would have needed to have read
one of his papers on this very recently to do that. So I'm not going to accuse him of anything,
but if he was sitting here, we could have a discussion about it.
Well, I asked him straight up, because I was trying to pin him down, and he said he is
an idealist, a monowist idealist. And it might be instructive, actually,
if you just to explain what do we mean by idealism?
Right. Well, so especially if he's using the word monow there as well, then I guess that
he is suggesting that while a physicalist thinks that there is only one substance in reality,
and that's material reality. And so there is no such thing as a separate stuff of mind
metaphysically. So for the physicalist, there is no dualism if they really, really think that,
but the trouble is that as soon as you kind of probe, then often they have struggle to deal with
dualistic intuitions about subjectivity. Now the idealist, on the other hand, also
thinks that there's only one substance, but that substance is the substance of mind. So
physical reality has to be a kind of construct out of this one substance,
this one out of mind stuff. So that's a very different kind of, it's a different way of
avoiding dualism, but it has its own problems about how do you explain account for science
and the success of science and so on. I also interviewed Philip Goff in a beautiful studio
recently, and he is a cosmo-psychist, but I think it's better just to use the word pan-psychist,
but I think cosmo-psychist is where you have the teleology baked in. So rather than it being bottom
up, it's kind of top down. There's some cosmic purpose to the universe, and the universe as a
whole is made of, you know, the fundamental material of the universe is consciousness.
And what's the relationship between that view and idealism?
So I guess for the pan-psychist, so the pan-psychist is so far as I understand these
philosophical positions. I mean, I shouldn't put myself forward as somebody who necessarily
understands them in depth, and everybody who subscribes to these views has a slightly different
version of them as well. But the pan-psychist certainly thinks that reality is composed of
physical material substance, but that physical material substance irreducibly has a psychological
dimension to it, a mental dimension to it. So every physical object has a little bit of
consciousness in it, if you like, in some sense. I mean, I find it a very difficult
view to articulate because I just find it so completely counterintuitive. So I can't really
put the pan-psychist's hat on and express their point of view. You need a pan-psychist here to do
that. Because to my mind, this is, you know, again with my bitkinstinian hat on, then I just think,
well, how do we use words like consciousness? Well, we use the word consciousness and all of
the related terms in the context of each other, of other human beings. And so, you know, and it's
in the context of our being together in the world and your behaving in certain ways and our
exchanging certain looks when we're doing things and that we understand each other as fellow
conscious creatures. And so that's the context in which we use, you know, words like consciousness.
And when I speak about, you know, you're not conscious, you're asleep. And, you know, it's
all in the context of other humans that we use those words. So it's just ludicrously inapplicable
to use that word in the context of, you know, I don't know, a brick or a toaster or an atom.
It's just simply the words simply are not applicable in those contexts.
Yes, it's so interesting because Philip told me that he grew up as a Christian. And I've had Richard
Swinburne on as well. And he's got a book out about are we bodies or souls, you know, putting
forward the case for substance dualism. And so there's the moving away from dualism thing,
there's the teleology things. Because I think if you are of this frame of mind, you like to believe
that there's some kind of grand purpose. And there's also the wanting to not wanting to be a
monowist, basically. So you could perceive it as a form of mental gymnastics to say, okay, well,
I don't want to be a substance dualist, but why don't we rearrange the structure somewhat so that
consciousness comes first. And there's some kind of cosmic purpose. And we're building a worldview
that still makes sense to me. Yeah, yeah. I mean, I personally, I think all of these
positions involve a great deal of mental gymnastics. And, and I, you know, I reject
any kind of ism. Or what I mean by that is I don't use that term to describe, you know, my views
at all. So all of these isms are, you know, are misguided. And what we need to do is just to
to dismantle the whole way of talking, which makes us, you know, makes us confused in the context of
this kind of these kinds of issues. So that's what Wittgenstein is trying to do, as he puts it,
to show the fly the way out of the bottle, right? This is his famous phrase. So the fly is the
person who's ended up thinking all these philosophical thoughts by taking ordinary
language into strange places, taking it on holiday. And, and so to show the fly the way
out of the bottle is to just bring all of these concepts back to their ordinary everyday usage
and to show thereby that you haven't really, you haven't lost anything, the puzzles evaporate.
So that isn't an ism. That's a, that's, that's rather that's a kind of
kind of critical methodology for just shifting the way that you think and talk all together.
And final question on this, where do you sit on the kind of the teleology question? So one view is
that we have a purpose. The the physicist would argue that it emerges, you know, from quantum
field theory and a lot of sophisticated study of biology kind of build this intermediate view
of teleonomy. Where do you sit on that kind of spectrum? I'll be honest with you, I don't think
I sit anywhere on that spectrum. I think I think those are issues on which I don't have sufficient
expertise to pronounce. So, so, you know, you just keep me, keep me on the on consciousness
and cognition, you know, all this stuff is above my pay grade, you know.
Professor Shanahan, it's been an absolute honor to have you on MLST. Thank you so much. I really
appreciate it. Thank you. Thank you so much for having me. It's been fun.
