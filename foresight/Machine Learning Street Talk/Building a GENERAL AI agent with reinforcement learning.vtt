WEBVTT

00:00.000 --> 00:05.880
Open-endedness is essentially, you know, we're studying systems that can generate their own data in an infinite capacity

00:05.880 --> 00:10.520
And so it's systems that essentially if you run it for longer and longer they get more and more complex

00:10.520 --> 00:13.760
They generate more and more quote-unquote interestingness or interesting data

00:14.240 --> 00:19.040
And so if we can actually, you know, crack this nut of how do we actually come up with a

00:19.560 --> 00:22.520
Self-improving system in the sense that it keeps generating interesting data

00:23.040 --> 00:28.100
We can then use that data to train further train our models

00:28.140 --> 00:34.220
But of course you get into this perpetual data machine type of idea where obviously, you know

00:34.220 --> 00:36.220
There's how do you generate more data?

00:36.380 --> 00:40.540
If you know the data is ultimately coming from a model that you probably trained on previous data

00:40.540 --> 00:42.180
How do you get net new information from that?

00:42.180 --> 00:47.420
Well, I think a lot of this is actually just resolved purely again going back to this idea of the reward function

00:47.420 --> 00:53.580
Right or a preference function where there is outside information coming in through some sort of filtering criteria

00:53.580 --> 01:01.820
For example human designers in the loop or designers designing some sort of preference model that could essentially automatically rate the kinds of automatic

01:02.460 --> 01:06.980
Data that's being generated by these open-ended systems. What does waker stand for?

01:07.060 --> 01:12.460
Right, so waker stands for a weighted acquisition of knowledge across environments for robustness

01:13.420 --> 01:15.420
Fantastic, and what was the title of the paper?

01:16.420 --> 01:19.420
Oh, right. Yeah reward free curricula. Oh girl. What was the title?

01:20.420 --> 01:25.140
reward free curricula for training robust world models, that was it. Okay, so

01:26.860 --> 01:31.460
Give us the elevator pitch. Yeah, totally. So basically like the overarching

01:32.620 --> 01:37.620
Question that we're trying to answer with this paper is like how should we go about training like very general agents?

01:37.820 --> 01:43.300
So in the context of the paper, we think of a general agent as being one that's able to perform a lot of different tasks

01:43.380 --> 01:47.860
So we might think of these as different reward functions or for thinking of it from a reinforcement learning perspective

01:48.100 --> 01:50.940
But also be able to perform those tasks in lots of different environments

01:50.940 --> 01:56.860
So, you know, we don't want a robot to just be able to do you know pick up tasks do tasks in my like my kitchen

01:56.860 --> 01:58.860
specifically we want the robot to be able to go into like arbitrary

01:59.420 --> 02:04.320
Apartments and also be able to do those tasks in like arbitrary environments. And so we kind of thought about like, yeah

02:04.340 --> 02:07.100
How do we want to create an agent that can do such a thing?

02:07.100 --> 02:12.540
And we argue in the paper that a good way of doing it would be to have an agent that has a very general world model

02:12.900 --> 02:18.740
So a world model meaning that it can predict the outcome of sequences of actions and predict what will happen if it does certain actions

02:18.740 --> 02:22.740
And so we argue if we have a very general world model that can lead to a very general agent

02:22.740 --> 02:26.460
That's able to perform, you know, a variety of tasks in different environments

02:26.460 --> 02:31.620
And so then, you know, once we've established that we kind of ask the question of how do we get a very general world model?

02:31.620 --> 02:33.860
And what does it mean to have a good world model that works?

02:34.260 --> 02:40.140
While in a very general setting across different environments and different tasks like how do we define that and how should we gather data to do that?

02:41.100 --> 02:46.700
Beautiful. So I really enjoyed reading the paper and it reminded me a lot of Kenneth Stanley's poet paper

02:46.860 --> 02:53.220
So he was doing this thing called curriculum learning and it's really related to machine teaching as well

02:53.220 --> 02:58.300
There's quite a few things in machine learning where you say well if we had a really principled way of

02:58.700 --> 03:03.300
Selecting the best training data and presenting it to the learner in the best possible order

03:03.340 --> 03:10.060
Could the learner be better and in that poet paper Stanley was kind of generating a diverse set of

03:10.300 --> 03:17.420
Environments and like training a learner on those things and you're doing something very similar and you're using this mini max regret

03:17.420 --> 03:21.620
Which is a concept from decision theory. Can you bring that in? Yeah, absolutely. So

03:22.620 --> 03:25.260
So I guess we have this notion of like wanting to be

03:25.740 --> 03:28.460
To perform well across a wide range of scenarios, right?

03:28.500 --> 03:33.180
So scenarios in our context mean like different environments and different tasks and kind of like the most

03:33.780 --> 03:38.820
Standard way of thinking about that, especially in reinforcement learning or a machine learning in general as you think about like the average performance

03:38.820 --> 03:43.220
So so how do I optimize like the expected reward across all of these different scenarios?

03:43.780 --> 03:47.100
and a lot of the work that that munchies done as well kind of argues that

03:47.660 --> 03:50.900
Just just optimizing for expectation isn't necessarily the best

03:51.700 --> 03:55.180
The best objective so, you know, we can imagine in the real world

03:55.220 --> 03:58.620
We don't really know like the distribution over possible tasks or or anything

03:58.620 --> 04:02.860
Well, you know in most situations, we don't know things like that and so maybe a better objective is to try and be

04:03.220 --> 04:07.220
robust instead and robust basically we can think of that as meaning like we should do

04:07.700 --> 04:12.220
Reasonably well in every situation we could be in and that that's kind of what a robust objective is

04:12.940 --> 04:16.660
And one of the ways that you can define a robust objective is via mini max regret

04:16.660 --> 04:22.300
And so regret means like suboptimality like how well that I do relative to the best I could have possibly done

04:22.340 --> 04:25.220
So that means basically the same thing as it does in normal English

04:25.620 --> 04:30.660
And so the mini max regret objective basically says across all possible situations. I want to try and do

04:31.700 --> 04:37.020
Minimize the regret across all possible situations minimize the maximum regret I should say so that means in all possible situations

04:37.020 --> 04:40.380
We should do almost as well as the best we could have possibly done

04:40.900 --> 04:44.300
And I guess just to contrast this against the standard objective for robustness

04:44.300 --> 04:51.300
So the more common objective for robustness at least traditionally is like a maximum performance. That means maximize the performance

04:51.820 --> 04:56.900
While the environments like minimizing and choosing the most adversarial environment or the most adversarial scenario

04:57.820 --> 05:02.860
But but the problem with kind of the maximum objective is that in some environments you just can't do anything

05:02.860 --> 05:05.060
Let's say it's like some such situations is too hard

05:05.060 --> 05:10.620
You're doomed and so if in some situations you're doomed and you always get like zero reward or negative infinity reward

05:10.700 --> 05:16.420
That means there's no incentive to try and do better in any other environment because your maximum reward is always going to be zero

05:16.420 --> 05:22.140
And so therefore I think like minty argues as well as Michael Dennis and a lot of these recent papers argue that mini max regret

05:22.140 --> 05:28.100
So minimizing the maximum self-optimality is actually like a better objective for a general agent. That's robust fascinating. So

05:28.900 --> 05:32.980
If I understand correctly is it a way of saying I want to have the best case

05:33.540 --> 05:35.260
worst

05:35.260 --> 05:41.140
Expected regret. Yes. So basically mini max regret is saying that if you assume that you know

05:41.140 --> 05:46.420
The environment is adversarial to you in some way like when you're training or at inference time when you're actually

05:46.980 --> 05:48.980
Testing your policy out in the real world

05:49.460 --> 05:56.380
Mini max regret is saying the agent should behave the model should behave in a way that minimizes its worst case possible regret

05:56.660 --> 06:00.700
Over all the possible conditions of the world that this adversary could choose

06:00.900 --> 06:04.740
What's really interesting about this paper is we are talking about the reward free

06:05.300 --> 06:11.420
Exploration phase and we're also talking about the domain of model based reinforcement learning as opposed to

06:11.980 --> 06:14.660
You know, let's say value based reinforcement learning where

06:16.220 --> 06:19.860
You get this entanglement, right? So the dynamics the model of the world

06:19.860 --> 06:23.380
It's still in there, but it's kind of in meshed with this with this value model

06:23.420 --> 06:25.900
Whereas in model based reinforcement learning in a principal way

06:25.900 --> 06:31.220
We kind of separate out the parts so that we can do explicit planning and imagination and simulations and stuff like that

06:31.300 --> 06:34.340
So we're very much in this model based domain, right? Yeah, absolutely

06:34.340 --> 06:40.100
Yes, we focused on yeah model based reinforcement learning or some people like to call this like the world model setting more recently

06:40.340 --> 06:43.700
But yeah, like you said we you know in typical like model free reinforcement learning

06:43.780 --> 06:46.740
We we typically aim to learn a policy and a value function and yeah

06:46.740 --> 06:52.900
As you said like that value function is kind of implicitly encoding the dynamics through the fact that we learn the value function using the bellman equation

06:52.980 --> 06:58.900
So so the bellman equation kind of propagates the information between like transition and the environments through the value function

06:58.900 --> 07:01.780
So so the value function will like implicitly have the dynamics in it

07:02.580 --> 07:06.420
But in model based reinforcement learning we want to very explicitly model the dynamics of the environment

07:06.820 --> 07:10.580
And so what I mean by that is we want to be able to take some previous sequence of observations

07:10.820 --> 07:13.940
Perhaps those are images and then also condition on the next action

07:13.940 --> 07:18.420
We want to take in the environment and then be able to predict the distribution over the next observation or states

07:18.420 --> 07:20.740
We're very explicitly modeling the dynamics of the environment

07:21.060 --> 07:25.380
Okay, now this is really interesting because you know people think about reinforcement learning and in reinforcement learning

07:25.380 --> 07:28.740
You don't so much care about having a model of the world

07:29.060 --> 07:35.220
You care about building trajectories that lead to some you know task or goal or whatever that you're interested in so like

07:35.940 --> 07:39.860
I mean just just in broader terms. What what what do we get from explicitly modeling the world?

07:40.260 --> 07:43.380
So there are there are a few arguments for why we would want to explicitly model the environment

07:43.380 --> 07:48.740
So so one of which is um a lot of people would argue that you get better sample efficiency by modeling the environment

07:48.980 --> 07:52.180
And the argument for this is you know the reward function might be quite sparse

07:52.500 --> 07:56.900
And so if you're just relying on like the propagation of rewards backwards to try and learn the optimal behavior

07:57.140 --> 08:02.820
That might not be as efficient as actually learning the dynamics because the dynamics can be learned from every single transition that you have

08:02.900 --> 08:05.460
It's kind of like a standard supervised or unsupervised learning problem

08:05.460 --> 08:10.900
So so you kind of have like a richer signal to learn from which might arguably lead to better sample efficiency

08:11.460 --> 08:12.900
um, but I think like

08:12.900 --> 08:17.060
More concrete arguments that I would argue for or that if you have a model of the environment

08:17.140 --> 08:22.260
It's it's some kind of more general thing that you can then use to develop better decision making later on

08:22.260 --> 08:24.260
So so if you just learn a value function

08:24.900 --> 08:30.180
You're kind of only learning how to optimally do that specific reward function or optimize that specific reward function

08:30.660 --> 08:34.820
Um, but if we have a model of the environment, we can kind of arbitrarily be given some task later down

08:34.820 --> 08:40.340
Whether it be a reward function or a goal state or something like that and we can then plan to optimize that task later down the road

08:40.660 --> 08:42.500
so I would think that um

08:42.500 --> 08:45.940
You know, it's kind of a much more general way of having a powerful decision making agent

08:46.260 --> 08:50.340
Rather than just specifying like one task and learning the optimal kind of policy for one task

08:50.660 --> 08:52.660
and I guess another thing that I'll add to that is um

08:53.460 --> 08:57.460
Rather than only learning like a feedforward policy like you wouldn't reinforcement learning

08:57.540 --> 08:59.540
So something that maps directly to actions

08:59.620 --> 09:02.580
The other thing that a world model allows you to do is also to do online planning

09:02.660 --> 09:06.100
So you can imagine at test time we're trying to deploy it in the environment

09:06.260 --> 09:10.340
But we can actually do a bit more further planning through the world model to then work out what the best action is

09:10.740 --> 09:13.780
Rather than relying on just a neural network to immediately output an action

09:14.180 --> 09:17.380
And there's kind of a lot of work showing that if you can do this like planning at test time

09:17.700 --> 09:21.300
You can kind of get a lot of a better performance on a lot of environments, especially things that

09:21.700 --> 09:25.460
That really rely on um search to do well things like go and like these kind of games

09:25.460 --> 09:28.100
We do have to think explicitly ahead in the environment

09:28.500 --> 09:32.580
And so I would think those are the main reasons you would want to consider um a learning a world model

09:32.820 --> 09:35.700
And maybe a last point I'll just add is that I think this is kind of a um

09:36.340 --> 09:38.420
Again, like unclear whether this is true necessarily

09:38.420 --> 09:43.220
But but I think some people would argue that a world model will generalize better than learning a value function

09:43.540 --> 09:47.380
So you can imagine like a world model is learning things like you know state transitions

09:47.540 --> 09:49.940
So you can imagine if you if you're training on straight transitions

09:50.020 --> 09:53.380
The model is kind of implicitly being forced to learn something like physics or something like that

09:53.620 --> 09:56.500
And so if you're like very explicitly forcing the model to learn something like physics

09:56.820 --> 09:57.620
You could argue, you know

09:57.620 --> 10:01.300
We'll go to some new state and the rules of physics will still hold and therefore the world model will still be

10:01.620 --> 10:04.820
Quite good at the new state potentially whereas if you learn a value function

10:05.220 --> 10:08.580
I guess it's a little bit less clear as to whether you're putting a new situation

10:08.740 --> 10:12.100
Will the same kind of structure of that value hold as it would a model

10:12.340 --> 10:15.060
Anyway, so that was a bit of a long answer, but no, no, it's fascinating

10:15.060 --> 10:20.020
I mean when I was reading the paper that one of the reads I got is um in machine learning

10:20.020 --> 10:25.460
We are often overcoming the curse of sparsity. So of course like in trajectories and reinforcement learning that that's quite intuitive

10:25.700 --> 10:28.740
But even in learning the world model itself the model

10:29.300 --> 10:34.260
Just because of the way they're trained it tends to compress the world into small little motifs and

10:34.900 --> 10:40.100
Actually, the world is quite complicated and we need to combine the motifs together in lots of interesting and rich ways

10:40.500 --> 10:45.860
And by exploring through the world model, we're almost kind of like make it we're forcing it to make those connections

10:46.020 --> 10:48.740
Yeah, and I think um, you know to follow up on mark's um

10:49.540 --> 10:50.260
Mark's point

10:50.260 --> 10:54.580
I think it's also interesting because especially in the waker paper, uh, the world model setting

10:54.580 --> 10:57.060
We're looking at specifically reward free world models

10:57.300 --> 11:02.660
And so essentially there's this explicit decision to separate separate out the two components of world model

11:02.900 --> 11:07.380
Which is essentially the dynamics function, which tells you how things transition from state to state

11:07.380 --> 11:11.460
How does a state transition state of the world transition to the next state of the world?

11:11.700 --> 11:16.500
Given an action that the model or the agent is taking in that world and the reward that it receives

11:16.580 --> 11:19.460
So the slider part the reward is defined by the reward function

11:19.780 --> 11:25.940
And so, uh, you know, I think mark was uh to follow up on his point a lot of the benefits of the world model is

11:26.260 --> 11:28.660
In this design arrangement is that you can

11:29.300 --> 11:33.220
compositionally separate out this dynamics aspect from the reward aspect

11:33.460 --> 11:38.900
So the general idea would be why shouldn't agent train in such a world model be able to generalize to a new setting?

11:39.060 --> 11:43.860
Well, maybe if that setting shares a lot of the underlying dynamics in that version of the world

11:43.940 --> 11:48.820
for example rules of physics and the agent has learned how to exploit those to accomplish, um

11:49.460 --> 11:52.500
Navigation around that environment or reach different types of tasks

11:52.900 --> 11:54.900
Achieve different kinds of tasks in that environment

11:55.060 --> 11:58.260
Then you can um sort of superimpose a different reward function

11:58.580 --> 12:03.700
That essentially defines a different task because the reward function defines what task success is

12:03.940 --> 12:09.220
so you can essentially superimpose different tasks on top of that dynamics model and you would

12:09.540 --> 12:14.980
You know, you could expect that the agent could learn more quickly because it's already mastered sort of the foundational skills of

12:15.220 --> 12:19.460
navigating or manipulating different aspects of the dynamics of that world

12:19.780 --> 12:24.020
We've been on a bit of a journey here. I think over the last few years in the literature of

12:24.100 --> 12:25.220
um

12:25.220 --> 12:32.820
We we want to have robust models and we're doing that by kind of perturbing and you know making a bunch of manipulations to the environment

12:33.060 --> 12:37.300
And there there was this domain randomization and there's like unsupervised environment design

12:37.300 --> 12:40.660
And of course your your iteration now is doing this in in the domain of

12:41.380 --> 12:46.900
Reward-free exploration, but can you take us on on that journey sort of maybe starting with um domain randomization?

12:47.140 --> 12:52.340
kind of just to uh elaborate on something that mark was previously talking about which is that the typical

12:52.660 --> 12:55.620
you know standard setup in machine learning is to

12:56.180 --> 13:00.500
Uh, essentially optimize a model's performance uh over a uniform distribution

13:00.980 --> 13:04.340
Over the data points and so this is really just randomly sampling data points

13:04.340 --> 13:08.340
And we try to minimize the loss over those data points for whatever objective

13:08.420 --> 13:12.180
We're trying to minimize or maximize in reinforcement learning. Um

13:12.740 --> 13:15.620
We want to train agents that can perform well in lots of different

13:16.180 --> 13:18.180
versions of the environment and so

13:18.580 --> 13:20.580
You can think of each environment

13:20.580 --> 13:24.580
Almost as a bundle of data points, right? It's kind of the set of trajectories that the agent can

13:25.860 --> 13:31.460
Can encounter within that version of the world and we essentially in reinforcement learning we want to learn to maximize

13:32.100 --> 13:36.260
The reward of the agent uh in that set of trajectories

13:36.260 --> 13:38.260
So we want to specifically start to

13:39.060 --> 13:45.060
actively pursue those trajectories that give us the highest reward and we learn from the reward signal as the feedback signal for

13:45.380 --> 13:49.940
Figuring out, you know, which actions and therefore which trajectories will lead to maximizing that reward

13:50.340 --> 13:57.620
and so typically um when we operate in the multitask setting, uh, we essentially randomly sample different versions of the environment

13:57.860 --> 14:04.100
And essentially have the agent try to maximize its performance its reward on that random sample of environments

14:04.740 --> 14:05.780
uniformly

14:05.780 --> 14:08.180
Sampled from, you know, the set of possible environments

14:08.980 --> 14:10.980
And this is essentially

14:10.980 --> 14:18.500
Causing the agent it'll cause the agent to learn a policy that's optimal for essentially uniform distribution over those environments

14:18.900 --> 14:25.780
Um, but of course this is kind of a naive assumption because we essentially are assuming that every possible version of the environment is equally likely

14:26.020 --> 14:30.900
Which is obviously not true because some versions of the world will not be as likely as other as others

14:31.140 --> 14:34.740
Uh, for example, like if you walk outside the sky is usually blue and not green

14:34.980 --> 14:39.220
And so, you know, when the sky is orange, maybe that happens if you're in california

14:39.300 --> 14:41.460
There's a wildfire, but that's not usually the case

14:41.780 --> 14:46.580
And so instead what we can do is we can turn to decision theory and think of

14:47.220 --> 14:50.820
Sort of more sensible approaches to what it means to act optimally

14:51.300 --> 14:58.420
When you're uncertain about uh, what state of the world the world will be in and so the thing that we focus on in this paper

14:59.380 --> 15:03.140
Is this idea of minimax regret where it is this idea again of

15:03.780 --> 15:08.340
Having the agent act in a way that essentially minimizes its worst case regret

15:09.860 --> 15:11.860
In any possible, uh, state of the world

15:12.100 --> 15:18.020
So largely, you know, this is a shift from randomly sample what it means in practice is you want to shift from randomly sampling

15:18.420 --> 15:24.660
environments during training to essentially, uh, sampling environments that maximize the agent's regret

15:25.220 --> 15:31.060
And what this means is you're now actively sampling for those environment settings where the agents, um,

15:32.260 --> 15:38.980
Experiencing the most regret and here regret is defined just simply as what does the optimal agent do in that version of the environment?

15:39.300 --> 15:42.260
And what did this current agent that's learning do in that environment?

15:42.260 --> 15:47.380
And so there's this gap in performance and you want to actively find those environments where that gap is maximal

15:47.780 --> 15:53.940
And if you view this as this adversarial game now between, you know, uh, an adversary like nature

15:53.940 --> 16:00.980
That's choosing the environment and the agent that's learning to solve the environment. Um, you can think of the adversary as, you know, having a

16:01.780 --> 16:05.220
Payoff function in that game or it's rewarded for the

16:05.700 --> 16:10.260
Based on the regret that the agent experiences and the agent is trying to shrink that regret

16:10.260 --> 16:16.100
So the agent you can think of as being rewarded for, you know, um, the the negative of that reward

16:16.100 --> 16:19.780
So the agent's reward signal is you can think of as the negative of the regret

16:20.180 --> 16:24.100
And so now you have the setting where you can essentially view this training process

16:24.500 --> 16:29.060
this active sampling process as a two player zero sum game where the

16:29.460 --> 16:36.340
Adversary is, you know, rewarded for the regret of the agent in each environment it chooses and the agent is rewarded based on the

16:37.060 --> 16:43.380
The agent receives the negative regret as its payoff. And so, um, we know that into player zero sum games

16:43.380 --> 16:47.460
There's always a this there's always a solution called a Nash equilibrium

16:47.540 --> 16:50.740
and so this is an idea in game theory where basically this is

16:51.140 --> 16:57.620
um, a choice of behaviors on both parties or a choice of strategies on both parties in the game such that, um

16:58.500 --> 17:01.620
No player can do better unless the other player changes their strategy

17:01.780 --> 17:05.060
And so you can think of this as a situation where, you know, I'm not

17:05.620 --> 17:11.460
Neither player is incentivized to deviate from their behavior. Uh, once they reach this choice of mutual strategies

17:11.940 --> 17:16.980
And so we know that all two player zero sum games have a Nash equilibrium

17:17.380 --> 17:20.820
A set of strategies between the two players and in this case

17:21.300 --> 17:23.860
We know there's additional theorem called the mini max theorem

17:24.100 --> 17:29.620
Which says that when in a two player zero sum game specifically two players and zero sum when, um,

17:30.020 --> 17:35.860
You are at the Nash equilibrium setting then each player must be playing what's called the mini max

17:36.740 --> 17:41.060
The mini max strategy, which means that each player is minimizing the maximum

17:42.420 --> 17:45.540
Minimizing the maximum reward for the other player

17:45.860 --> 17:52.420
And so here the reward again is the regret and therefore just based on this known, you know, theorem about two player zero sum games

17:52.660 --> 17:57.140
We know that, um, the agent which is, you know, receiving the payoff of negative regret

17:57.220 --> 18:01.700
It's the min player. It must be implementing the min and max regret strategy

18:02.020 --> 18:11.540
And so this is how we essentially can shape the training process to essentially, um, arrive at an agent that performs mini max regret decision making

18:11.860 --> 18:16.500
Rather than decision making that optimizes, um, just a uniform sample of environments

18:16.820 --> 18:20.420
Okay, so kind of play back, um, some of those things as I understand it

18:20.740 --> 18:27.460
So, um, essentially we we are we're building a model which will learn to select the environments where we perform badly on

18:27.780 --> 18:32.980
And then we fine-tune on those environments because we're leaning into the gaps. We're saying where where do I perform badly?

18:33.140 --> 18:39.380
Let's fine-tune on that and then you're saying that if we continue to do this as a kind of adversarial sampling game

18:39.620 --> 18:43.220
That we will reach a Nash equilibrium. So it will converge in a good place

18:43.540 --> 18:46.020
But help me understand that why would it

18:46.740 --> 18:52.020
You know, it seems to me intuitively that it might be unstable or it might not quite why does it converge?

18:52.500 --> 18:54.500
So there's no guarantees around convergence

18:55.140 --> 18:58.820
And so I think this is an area where there's a lot of room for innovation

18:59.220 --> 19:05.060
Uh around these methods a lot of this is um, this is more I would say like theoretical motivation around why we think

19:05.460 --> 19:12.260
actively sampling environment settings based on, um, estimates of regret is a good idea and another point related to that

19:12.500 --> 19:18.740
Around sort of this gap between the theory. I I just um explained and in practice is that

19:19.460 --> 19:21.940
Regret itself is a pretty hard quantity to actually

19:22.500 --> 19:26.900
Measure in practice because you know knowing regrets defined as what's optimal performance

19:27.780 --> 19:29.700
minus my agents performance

19:29.700 --> 19:34.500
So you kind of have to know what optimal performance is and in general you don't know the optimal behavior

19:34.500 --> 19:39.940
Therefore you don't really know the optimal performance on any environment unless it's like a very toy setting and so

19:40.820 --> 19:43.300
In practice, we also use approximations for the regret

19:43.940 --> 19:46.900
in order to do this kind of active sampling and so

19:47.940 --> 19:50.340
There's a lot of deviations between theory and practice

19:51.380 --> 19:52.340
So

19:52.340 --> 19:56.100
There's no guarantees, you know that different forms of gradient based optimization

19:56.660 --> 20:00.180
For rl training would actually lead to converging to Nash equilibria

20:00.580 --> 20:06.180
A lot of the theory is just stating that if you were to run the system the learning system for a long time if we make the assumption that

20:07.380 --> 20:09.380
the optimization algorithm is

20:10.020 --> 20:13.780
fairly good at producing, you know an improved response to the

20:14.500 --> 20:20.980
Other player in this type of zero sum game you if you're assuming that if the successive sort of series of best responses

20:21.620 --> 20:24.020
That the optimization algorithm is generating

20:24.900 --> 20:31.300
Continues to improve over the previous ones you could make the assumption that maybe eventually it does get to that equilibrium

20:31.380 --> 20:34.580
But there is no mathematical guarantee that this actually happens

20:35.380 --> 20:37.380
what we want to do is

20:37.780 --> 20:40.180
You know build this latent dynamics

20:41.140 --> 20:45.620
You know a predictive model which is a simulacrum of what the idealized version is

20:45.940 --> 20:49.620
But we don't have a way of directly computing the regrets. So we kind of perform

20:50.420 --> 20:53.460
You know, we learn a proxy for that regret. How does that work?

20:53.700 --> 20:57.140
So we think of regret in the following way. So so there's kind of this um

20:57.780 --> 21:04.180
Old school result from like um mdp theory or maybe it's not that old but like 20 years ago or something like that called the simulation lemma

21:04.500 --> 21:06.500
and that basically says that you know

21:06.740 --> 21:09.700
If we let's assume for now that we we have like an optimal planner

21:09.780 --> 21:13.860
So we can give our like model of the world to this optimal planner and end some reward function

21:13.940 --> 21:16.260
Let's say later down the road we get given some reward function

21:16.740 --> 21:21.300
And so we give the model and the reward function to our optimal planner and we assume that this planner can return

21:21.700 --> 21:23.700
The optimal policy in our model

21:24.260 --> 21:26.260
So we kind of have this, you know planning oracle

21:26.980 --> 21:28.820
And if we assume that we can do that

21:28.820 --> 21:32.180
Then we can think about the difference between like how good the policy would be from

21:32.740 --> 21:36.660
Our planning oracle in the model versus the truly optimal policy in the real world

21:37.460 --> 21:41.940
And so what the simulation lemma tells us is that you know the difference between these two policies

21:41.940 --> 21:44.980
So the one found by acting optimally in the model versus the truly optimal one

21:45.460 --> 21:49.060
Is bounded essentially by the error between the model and the real world

21:49.780 --> 21:53.220
Under the distribution of states that the policy would generate

21:53.460 --> 21:58.340
So so, you know, it only it only matters that we have low error where the policy would go essentially because you know

21:58.580 --> 22:01.060
If there are some states that are just completely irrelevant what the policy is going to do

22:01.060 --> 22:03.060
It's not really going to matter if the if the model is not

22:03.540 --> 22:04.820
Accurate and there

22:04.820 --> 22:06.820
So we kind of use this result to think about the regret

22:06.900 --> 22:09.540
So that that gives us like, you know, if we have like one

22:10.340 --> 22:13.700
One true mdp and one model of an mdp and one reward function

22:14.340 --> 22:16.180
The simulation lemma can tell us, you know

22:16.180 --> 22:20.100
What would kind of be the regret if we did this optimal planning within this one model of the

22:20.980 --> 22:22.260
Of the mdp

22:22.260 --> 22:26.180
But then in our work, we're not really interested in the setting of like one mdp one reward function

22:26.900 --> 22:32.820
Um, so we start to think about, you know, what happens if we have arbitrarily many environments as well as arbitrarily many reward functions

22:32.980 --> 22:34.500
Which we don't know in advance

22:34.500 --> 22:38.420
And then I guess the other thing that I should say like you you alluded to like latent dynamics is

22:38.660 --> 22:44.100
You know, these existing results are assuming that we have an mdp. That's fully observable meaning, you know exactly what the state of the environment is

22:44.660 --> 22:48.980
Um, but usually when we think about like world models or even or just maybe more modern reinforcement learning

22:49.620 --> 22:52.900
We're really interested in learning from like quite high dimensional signals. So

22:53.460 --> 22:55.220
images or maybe

22:55.220 --> 22:58.500
Probably images, but maybe there are the high high dimensional signals we want to reason about

22:59.300 --> 23:03.780
And because we're just using image observations, this means that the world is like partially observable

23:03.780 --> 23:07.300
Like we can't infer everything we need to know about the world just from one image, you know

23:08.420 --> 23:13.220
For basically any physical task like the velocity of objects is important, but you can't infer that just from one image

23:13.940 --> 23:17.140
Um, so in this partially observable environments, we really want to take

23:17.700 --> 23:22.660
A sequence of observations because we need to to use those sequence of observations to infer what the state is

23:22.980 --> 23:26.100
So, you know viewing a sequence of images will help me to infer what the um

23:26.820 --> 23:30.340
The velocities are for example, and so we can think of this as inferring like a belief

23:31.060 --> 23:33.940
A belief over what the state is and a partially observable mdp

23:34.660 --> 23:39.300
Um, so we need this full sequence of images and we need to use the full sequence of images to then to be able to predict ahead

23:39.300 --> 23:43.700
What the next observation will be and that's kind of what you know, most world models are attempting to do

23:44.260 --> 23:49.540
Um, but if we just like taken a bunch of images and then try and directly predict images again, that's like quite a hard problem

23:50.260 --> 23:52.980
Um to just like just predict straight an image space

23:53.460 --> 23:56.900
And so the most common thing to do is kind of to take your previous sequence of images

23:57.300 --> 24:01.940
And then try and get like some compressed representation of the history of images into like the latent state

24:02.660 --> 24:04.660
And then predict the dynamics in the latent state

24:05.380 --> 24:07.300
So yeah, so I have my sequence of images

24:07.380 --> 24:09.940
I kind of compress these somehow into some vector

24:10.420 --> 24:15.540
And then I give it a new new action and I try and predict what the next kind of latent vector will be given this new action

24:15.860 --> 24:18.500
And this now represents my prediction of the dynamics in the world

24:18.900 --> 24:22.420
And then if I want to um, you know predict what the next observation would be an image space

24:22.420 --> 24:24.100
Then I can also decode that back to an image

24:24.660 --> 24:29.060
Um, but then a lot of works also argue that maybe we don't want to actually learn to predict the entire image

24:29.060 --> 24:31.300
So maybe you don't want to actually decode the entire image

24:31.300 --> 24:37.140
But that's that's another aspect that we might want to get into but there's this whole broad story of of um working in the latent space

24:37.540 --> 24:41.860
And um in reinforcement learning there was that paper called world models by you know, david haran and schmidhuber

24:42.340 --> 24:48.500
And it also I think has a relationship with you know, what lakoon's doing with jepper and these like you know joint embedding prediction architecture

24:48.500 --> 24:51.060
So there seems to be something magical about working in in the latent space

24:51.380 --> 24:55.460
And also you were talking about um, you know partially observable markoff decision processors

24:55.780 --> 24:59.140
And you know, that seems to be this idea that we need to have a modeling framework for the world

24:59.540 --> 25:04.260
And I guess like the ideal situation would be is that like we just we we knew exactly what would happen

25:04.660 --> 25:07.140
You know every single time step in every single state

25:07.700 --> 25:10.820
Um, but we don't you know, so so we model it as a partially observable

25:11.060 --> 25:13.940
Markov decision process and the markov bit is quite interesting as well

25:13.940 --> 25:17.860
I mean maybe and you guys can just sort of introduce what why do we use that as a model?

25:18.260 --> 25:23.620
So markovian basically just means you only need to look at like the current state to be able to infer all the information about the system

25:24.340 --> 25:26.420
um, so so in a markov decision process

25:26.580 --> 25:31.620
We have some state and then we assume that we're able to take some actions and given some state in some action

25:31.620 --> 25:33.780
We get some distribution over next states of the system

25:34.100 --> 25:37.380
And then the the system will transition according to that distribution to the next state

25:37.620 --> 25:41.780
And this is just like kind of a general framework for modeling like systems that we might want to control

25:41.940 --> 25:45.060
So, you know, it kind of dates back to like early work and control theory

25:45.060 --> 25:47.380
But then it's also the main framework used in reinforcement learning

25:47.860 --> 25:48.020
um

25:48.020 --> 25:50.740
Yeah, and the reinforcement learning setting because it's the decision process

25:50.820 --> 25:56.660
We we also add an reward function which tells us how good it is to be in a certain state or to execute a certain state action pair

25:57.300 --> 26:00.900
Um, but yeah, as you said with relating to like partial observability and a lot of like systems

26:00.980 --> 26:03.460
We we don't actually know what the true like state of the world is

26:03.460 --> 26:08.420
So so you can imagine, you know, if we want to think of the entire world as a partially observable mdp

26:09.220 --> 26:14.900
We can't just have some vector telling us exactly what the true configuration of the world is or maybe that exists

26:14.900 --> 26:19.220
But we can't we definitely can't just know that and so we usually think of it as being a partially observable system

26:19.940 --> 26:24.660
Um, so this means that like given given the state, um, you know at each step

26:24.660 --> 26:28.580
We'll basically get some distribution over observations and we just get to observe that observation

26:28.980 --> 26:34.740
So, you know, the state of the world could be what it currently is in here and maybe my um observation is like a camera image

26:34.820 --> 26:36.580
so I only get some

26:36.580 --> 26:39.700
Camera image of the world that allows me to infer a bit of information about the state

26:40.340 --> 26:42.820
Um, and because it only allows me to infer a bit of information about the state

26:42.820 --> 26:44.260
It doesn't tell me the whole state

26:44.260 --> 26:48.740
It really you need to keep track of all of the observations you have to be able to keep track of all the information

26:48.740 --> 26:49.700
You have about the world

26:49.700 --> 26:55.700
So, you know, you can imagine um, if the task is for me to remember how to get out the door a while ago

26:55.940 --> 27:01.460
Um, you know, I don't just need to be able to like look at my current image of the world to be able to infer that information

27:01.460 --> 27:04.020
I need to have kept track of like all my previous information as well

27:04.580 --> 27:10.100
Um, so that's kind of why we think about often want to think about like partially observable environments as opposed to fully observable ones

27:10.340 --> 27:15.140
Amazing amazing. So so minci, maybe you can um bring in this this latent idea

27:15.860 --> 27:18.660
And and sort of contrast that to what lacuna is doing as well

27:19.060 --> 27:24.740
Sure, I mean, so I think in machine learning and deep learning, uh, there's this general paradigm that's been around

27:24.980 --> 27:26.980
You know since the inception which is learning

27:27.940 --> 27:33.540
latent latent representations of data and one of the benefits of learning latent representation is that

27:34.340 --> 27:38.660
You know, ideally your objective, uh, that leads to learning these latent representations

27:38.820 --> 27:45.540
Is that you are ultimately learning a lower dimensional representation of the data or dynamics that you're modeling like in our case with the world model

27:45.860 --> 27:52.900
Um, that captures just what is necessary. It's a more compact representation of just the information that's necessary to predict

27:53.300 --> 27:57.860
The task you're trying to predict and so um with uh with our case

27:58.340 --> 28:01.940
Or latent space world models a lot of the benefit of working in the latent space

28:02.020 --> 28:04.980
Is that if as opposed to working in the full image space?

28:05.060 --> 28:05.540
for example

28:05.540 --> 28:10.580
If your observations are images like in a video game is that there could be a lot of spurious features

28:10.820 --> 28:17.220
Or you know a lot of additional information that you could be expending lots of compute and um, you know gradient updates

28:17.460 --> 28:22.820
Just to learn those patterns when they don't actually impact the ultimate um transition dynamics

28:22.900 --> 28:26.740
Or reward dynamics that you need to learn in order to do well in that environment

28:26.900 --> 28:30.820
So one example is if you have a game where, you know, maybe the background is different

28:31.140 --> 28:36.820
Uh, because it's daytime or nighttime or it's close to sunset. Um, but ultimately, you know, the background

28:37.380 --> 28:39.380
doesn't really impact

28:39.460 --> 28:45.140
How the player moves around in the environment or whether they've reached the end goal of the task and so

28:45.780 --> 28:49.780
If you're training a model where it needs to compress a lot of this information

28:49.860 --> 28:53.540
First into a smaller dimensional latent vector or latent representation

28:53.940 --> 28:58.260
Um, you don't really need you would expect that latent representation not to actually capture

28:58.500 --> 29:03.300
It would start to ignore the background color and it might only capture certain features of the environment that can

29:03.860 --> 29:08.260
Essentially if you were to decode it back out it might only capture certain information about the environment

29:08.340 --> 29:10.900
That's predictive of the actual task that you want to solve

29:11.300 --> 29:14.500
Um, so maybe if the task is to say reach a coin at the end of a level

29:14.740 --> 29:19.620
Then maybe the latent representation would capture the presence of the coin or whether the the proximity of the character

29:19.780 --> 29:21.780
You're controlling to the coin

29:22.180 --> 29:23.540
and so

29:23.540 --> 29:24.980
With the jeppa related work

29:24.980 --> 29:30.660
I think a lot of this is also, you know, motivated with this idea where if we can learn a better latent space representation

29:30.980 --> 29:34.260
Um of images or videos or whatever modality we're trying to model

29:34.500 --> 29:39.380
Um, it's a much lower dimensional computationally efficient representation. Uh that you can

29:40.260 --> 29:43.220
You can effectively use for downstream tasks. Um

29:44.180 --> 29:48.740
I'm not I'm actually not super familiar with exactly, you know, the the visual jeppa

29:50.740 --> 29:56.740
Objective so you don't think I can say too much about that. Oh, that's okay. Yeah. I mean, but yeah, I mean you pretty much nailed it

29:56.980 --> 29:58.340
so, um, I mean

29:58.340 --> 30:01.460
Lacune even gives the example of like, um, you know in self-driving cars

30:01.780 --> 30:04.580
You might not be interested in the leaves on on on the road, you know

30:04.580 --> 30:07.700
So like with increasing levels of of nesting you kind of like learn to

30:08.100 --> 30:11.620
Ignore the things that are not relevant and focus on the things that that are relevant

30:12.020 --> 30:17.220
But we're almost getting to the center of the bulls I hear so intelligence to me is all about model building

30:17.380 --> 30:22.180
And and that's what these abstractions are. They're they're models that kind of are predictive about the thing that that that's relevant

30:22.260 --> 30:24.900
and kind of like ignoring what is not relevant and

30:25.380 --> 30:30.660
We build better models when we have a curriculum. I mean apparently this happens in nature as well. Max Bennett

30:30.660 --> 30:34.340
I was talking to him the other day and he said, you know, our genome doesn't encode all of our skills

30:34.580 --> 30:39.060
Um explicitly because it would be too inefficient to do so, but they do encode a kind of curriculum

30:39.300 --> 30:43.700
So we teach babies. Yeah, we babble with babies and we teach babies how to talk and stuff like that

30:43.940 --> 30:49.380
So so the curricula is is really important and then we're getting to the center of the bull's eye

30:49.460 --> 30:51.700
Which is intelligence in in general now

30:52.260 --> 30:58.260
I think Lacune thinks that it's specialized and and what that means is that there are there are motifs

30:58.740 --> 31:01.700
That's statistically generalized and what that means is that

31:02.580 --> 31:03.780
You do need

31:03.780 --> 31:07.620
environments you need to find motifs that are present in

31:08.260 --> 31:12.900
In as many environments as possible and those are the generalizing features. Do would you agree with that?

31:13.380 --> 31:15.380
Yeah, definitely. I think that a lot of um

31:16.020 --> 31:20.260
So a lot of really powerful machine learning methods, for example, uh are trained in simulation

31:20.660 --> 31:24.500
And when you're training in simulation, there's a concept in control from control literature

31:24.900 --> 31:29.460
Called the sim 2 real gap and essentially this is essentially quantifying a performance difference between

31:30.020 --> 31:37.460
Well, it's quantifying a few things one is just how different is the are the actual physical or other other kinds of dynamics captured by your simulator

31:37.540 --> 31:44.180
Compared to reality. So if you have a physics simulator, how accurate are for example the friction dynamics or different kinds of contact dynamics?

31:44.660 --> 31:49.380
In your robotic simulator compared to those actual dynamics in the real world with a real robot

31:49.700 --> 31:52.900
Um, and this also leads to a sim 2 real gap in terms of performance

31:53.060 --> 31:59.460
So if you train in the simulator, you know, a lot of times what machine learning is really good at is it's really good at learning to exploit

31:59.620 --> 32:05.220
Whatever system you're training this the model in and so it's fairly um common for

32:05.620 --> 32:10.900
You know systems that are models that are trained within a simulator to learn to eventually exploit the simulator

32:11.140 --> 32:13.140
and so actually like one big area of um

32:13.300 --> 32:18.420
Games ai is using is actually leveraging this idea where they essentially use ml models

32:18.580 --> 32:25.060
They optimize ml models to within a certain game environment to try to find bugs within that environment to look for exploits automatically

32:25.300 --> 32:28.500
Um, so ml systems are very good at finding exploits in whatever system you have

32:28.740 --> 32:34.580
But then the issue is those exploits are usually where exactly where the gap between your simulator and reality resides

32:34.820 --> 32:40.980
And so you actually don't want your model to learn to exploit these differences between the simulator and reality to get a high performance

32:41.300 --> 32:45.220
Uh, because that kind of defeats the purpose of then later transferring your model

32:45.540 --> 32:52.500
That's trained in simulation to reality because now in reality, obviously the model can't exploit those same those same glitches within the simulator

32:52.900 --> 32:54.900
Um, yeah, so yeah

32:54.900 --> 33:00.180
Yeah, I mean because the reason this is really interesting is is that the the premise of your paper is that

33:00.900 --> 33:04.020
It is possible to build a generalist agent

33:04.420 --> 33:09.940
Which means it's an agent that can be fine tuned and work really well on a whole bunch of downstream tasks

33:10.260 --> 33:15.300
And to me that implies that at least in our physical world in any situation

33:15.380 --> 33:21.780
You might use this agent that there are general motifs that it could have learned during free training that it could like, you know

33:22.020 --> 33:28.820
Become activated in any situation. Um, does that is that fair? Yeah, maybe I can say something about um

33:29.140 --> 33:32.900
Just the way that we should could think about like the different like latent dynamic subjective

33:32.980 --> 33:37.540
So so I think I agree that like at least when I try and think about how I think or how people think

33:37.620 --> 33:38.820
I think I agree that like

33:38.820 --> 33:43.380
You know a truly intelligent system should kind of think through the world and like a very compressed representation of the world

33:43.380 --> 33:45.940
Like if I'm trying to like think through how to go to the airport

33:46.020 --> 33:51.140
Like I'm definitely not like predicting ahead in terms of like the raw image space of trying to predict every image

33:51.140 --> 33:55.700
I might observe on the way the airport and things like this. And so I think we have this kind of like trade-off between, you know

33:56.500 --> 34:00.020
Um, like we said with the bgf of paper like should should be just try and like

34:00.500 --> 34:03.940
Kind of basically model like the minimum information we need about the world to try and you know

34:04.020 --> 34:08.340
Do the do the relevant task in the world? I think what you're saying. I think that probably is

34:09.140 --> 34:12.500
Maybe more what we think about when we think about like human intelligence or something like that

34:13.060 --> 34:15.780
Um, but then there's also this other way where we just say we're going to just like

34:16.500 --> 34:18.820
Enforce the model to be able to predict ahead every single image

34:19.220 --> 34:23.140
And so in our paper, we do actually enforce that the model has to predict the next image

34:24.020 --> 34:26.020
um, and so um

34:26.340 --> 34:28.740
Basically what this might mean is yeah, like maybe the model does

34:29.300 --> 34:34.580
You know, hopefully it does like like you said like kind of capture the underlying like true things that matter in the environment

34:34.820 --> 34:37.380
But it might also mean like what we're saying with like the leaves example

34:37.380 --> 34:40.580
Like this might force the model to kind of capture a lot of irrelevant details

34:40.580 --> 34:42.820
That don't really matter like the leaves on the ground and things like this

34:43.220 --> 34:46.100
And so, you know, maybe that means it isn't actually capturing the underlying motifs

34:46.100 --> 34:50.180
It's actually just getting good at image generation. Um, but then I've or image prediction I should say

34:50.980 --> 34:55.940
Um, but then I've also heard arguments kind of saying, you know, so what if people don't really think in terms of like image prediction

34:56.100 --> 34:58.980
You know, I you know, we think in terms of like more like these high level motifs

34:59.300 --> 35:01.380
But people have other people would argue that you know

35:01.620 --> 35:04.660
Kind of the machine learning machinery is there to do really good image prediction

35:05.060 --> 35:09.060
So so if if we if we can get a model that can actually just like predict images ahead really well

35:09.540 --> 35:12.900
Um, and not really worry so much about whether it's reasoning about these like high level features

35:13.220 --> 35:17.940
You know, if you can predict images ahead really well, you know, that's enough to make to do good decision making a lot of context

35:18.180 --> 35:19.940
So I think there's this kind of like

35:19.940 --> 35:22.740
Contrasting ways of thinking about, you know, image prediction is good enough

35:22.740 --> 35:26.580
We'll just predict like really visually good scenes and that will be good enough for decision making

35:26.900 --> 35:30.340
Or do we want to force the model to try and reason about like more abstract features of the environment?

35:30.340 --> 35:35.380
And that's kind of a more intelligent way of reasoning about the world. Um, and yeah, I think that's a very interesting trade-off

35:36.420 --> 35:38.420
Yeah, yeah, I mean like it's um

35:39.140 --> 35:41.220
Like the biggest problem in machine learning is overfitting, you know

35:41.220 --> 35:43.940
So as you say like that, there are all of these statistically generalizing features

35:43.940 --> 35:48.100
But they generalize within the domain and the domain might be like your your simulator or like, you know

35:48.100 --> 35:50.820
How you're training it rather than how it's being used in in production

35:51.060 --> 35:55.780
And then as you say that there's also this um almost human chauvinistic or puritanical view on this which is that well

35:56.340 --> 36:00.420
You know, it does the right thing for the wrong reasons or I use different motifs to do the reasoning

36:00.420 --> 36:02.900
So that thing must be doing it wrong. Do you know what I mean?

36:03.220 --> 36:07.860
And um, I was talking with chris bishop at msr the other day and and you know, he's um big on

36:08.100 --> 36:13.460
Symmetries and yeah, you know, the kind of stuff that like max welling and takako hen and bronstein and um

36:13.460 --> 36:15.380
The deep mind has done loads of cool stuff on on this

36:15.620 --> 36:20.900
But it's this idea that like we know the world um has a certain geometry. It has certain physical priors

36:20.900 --> 36:25.780
So like we can deliberately um, you know, kind of construct the approximation class in machine learning

36:26.260 --> 36:30.580
Methods so so that like we make it an easier problem, right? Because we because we know we know the thing is in there

36:31.700 --> 36:33.700
Yeah, so I mean, I guess sort of the uh

36:34.260 --> 36:36.260
Slight tangent I went into around the sim to real gap

36:36.420 --> 36:39.540
I guess part of the point I wanted to make there is that um, you know

36:39.540 --> 36:42.420
One way around the sim to real gap is you could try to train

36:43.060 --> 36:47.060
You could try to parametrize a very large space of possible versions of reality

36:47.300 --> 36:53.380
And this is kind of the motivation behind this method of domain randomization where you sort of say this is the you know

36:53.380 --> 36:56.660
This is the specific task domain I care about I can parametrize the different

36:56.980 --> 36:59.380
Uh versions of the task with a few parameters

36:59.620 --> 37:06.020
And I basically want to search over the space of parameters and train my model or my agent on all possible variations of this world

37:06.180 --> 37:10.820
But obviously that's not very sample efficient because that design space could be huge could be massive

37:11.060 --> 37:14.820
And so instead we like these active sampling strategies like we were talking about earlier

37:15.380 --> 37:17.380
around mini max regret style

37:17.540 --> 37:22.340
Active sampling where you sample those environments that maximize your regret or some other type of objective

37:22.420 --> 37:25.940
Maybe like uncertainty uh similar to what we do in the waker paper

37:26.500 --> 37:29.860
But ultimately these things these active sampling process it leads to

37:30.660 --> 37:33.300
What we like to call an auto curriculum automatic curriculum

37:33.940 --> 37:37.060
And this is in contrast to prior curriculum learning works because here

37:37.380 --> 37:42.500
This is an automatically generated curriculum. So you you can kind of not have any pre-defined notion of what is

37:42.900 --> 37:48.900
Easy or hard it's purely fixed to what is easier or hard for the model in terms of how good the model is at

37:49.060 --> 37:51.780
Performing at those tasks. And so it's nice. It's an automatic curriculum

37:51.940 --> 37:56.660
So you can think of it as almost like weaving a path through this high-dimensional design space

37:57.380 --> 37:59.380
automatically such that if the

37:59.380 --> 38:05.620
Agent or model were to train on data along this path of environments through its experiences in this path of environments during the training curriculum

38:05.860 --> 38:09.060
It'll basically be maximizing some sort of information gain objective

38:09.380 --> 38:10.100
um

38:10.100 --> 38:14.500
Because you know, for example regret if there's a high regret that's that means there's a high

38:15.300 --> 38:18.420
Ceiling there's a high gap in terms of how much the agent can improve

38:18.660 --> 38:21.140
Which implies that there's a lot more for the agent to learn in those environments

38:21.300 --> 38:26.740
So it's sort of this like optimal you want to find this optimal path weaving through the high-dimensional design space of environments

38:27.060 --> 38:34.820
Now the danger here is that as you do this, uh, auto curriculum the auto curriculum, uh, could also go haywire very easily because

38:35.060 --> 38:40.580
The design space is so big if you're training in simulation, which we have to do because these methods are so sample inefficient

38:40.580 --> 38:43.780
We need so much data to train them. Um, you want to train in simulation

38:43.780 --> 38:46.820
But if you're doing the auto curriculum in the simulation design space

38:47.220 --> 38:53.380
It could start to veer very easily and quickly into different corners or niches of the design space where

38:53.940 --> 39:00.260
You know the parameters no longer really make sense in terms of mapping to a physical reality or a real world scenario

39:00.580 --> 39:02.580
That we as human users

39:02.660 --> 39:05.620
Uh actually care about and so kind of it would be you know

39:05.780 --> 39:09.860
It would defeat the purpose of spending all this compute to train this model that could then help us in the real world

39:09.940 --> 39:14.420
Because now it's veering off into parts of the design space that don't really matter for humans

39:14.420 --> 39:20.020
It's kind of noisy parts of the design space. And so this kind of leads us to this question of grounding

39:20.020 --> 39:23.620
How do we ground curricula? How do we align the curricula such that you know?

39:23.620 --> 39:28.420
They can still do their exploration through this active sampling type of procedure over the environment design space

39:28.580 --> 39:35.220
But at the same still at the same time maintain at least some proximity to the parts of that design space that are relevant to

39:35.700 --> 39:38.340
What humans care about in terms of the actual tasks they represent

39:38.500 --> 39:41.940
I mean i've been speaking with kenneth stanley a lot recently and we're talking about open-endedness

39:42.340 --> 39:45.860
And in general i've been trying to come at this problem from multiple angles

39:45.940 --> 39:51.060
And i've been using the lens of agency because i think agency is something that happens in the real world

39:51.060 --> 39:55.140
And that's why we have this divergent process because we have multiple agents, you know, kind of like

39:55.540 --> 39:59.540
You know undirected following their own gradient of interestingness. So in in evolution

39:59.540 --> 40:02.980
That's a great example that it is this divergent process, but it's also grounded

40:02.980 --> 40:08.340
It's physically grounded, you know, so it's like the physical world creates some kind of constraints on on the things that are found

40:08.660 --> 40:12.900
And i mean, you know clune called this ai generating algorithms. There's quite a few different takes on this

40:12.900 --> 40:16.020
But the idea is that um to search this complex search space

40:16.100 --> 40:21.940
We we need to have a divergent search and that's like we actually need to create the problems and the solutions

40:21.940 --> 40:23.220
So like in the real world

40:23.220 --> 40:27.620
The the you know the giraffes had the problem of like eating the leaves from from from the trees

40:27.780 --> 40:30.260
And the problems and the solutions get generated in tandem

40:30.420 --> 40:32.500
And this whole thing just kind of grows and grows and grows

40:32.660 --> 40:39.460
And that seems to be the most important feature that is missing in current ai systems and the grounding or the

40:40.660 --> 40:43.940
Stanley calls it the gradient of interestingness. I'm not sure whether you'd agree with that

40:44.020 --> 40:49.060
But um, i mean what mark what what what do you think about the importance of like this divergence in ai?

40:49.700 --> 40:52.020
kind of the current paradigm of machine learning

40:52.660 --> 40:58.100
Of kind of like, you know gathering some data set beforehand or specifying some simulated beforehand if it's reinforcement learning

40:58.580 --> 41:02.580
Is kind of good enough to do like a lot of reasonable tasks that we might care about

41:03.060 --> 41:03.860
um, you know

41:03.860 --> 41:08.900
Like obviously like predicting language or generating simulated language or performing very well at some simulated task in rl

41:09.140 --> 41:12.660
But it definitely seems like the next step towards like very general agents that are kind of

41:13.460 --> 41:16.100
You know, I guess maybe I don't know if we want to use the term agi

41:16.100 --> 41:19.540
But there's something something more along the lines of a general agent that's kind of you know

41:19.860 --> 41:23.140
able to kind of self improve and learn in more diverse environments

41:23.620 --> 41:27.620
Um, it definitely seems like that's kind of the next step of where machine learning will go

41:28.180 --> 41:30.820
And if we're going to get to that point, I kind of agree with the idea that

41:31.540 --> 41:36.900
You know, it certainly doesn't make sense to have some agent that just randomly trying to gather completely random new knowledge

41:37.220 --> 41:40.580
Like it certainly seems to make sense that you know, you know, even as a human

41:41.220 --> 41:46.100
To improve your intelligence you kind of selectively try and find out the areas in which like you can gather more

41:46.500 --> 41:49.540
More information or more knowledge and things like this and this is kind of what you know

41:49.700 --> 41:54.340
Leads to this kind of I guess branching or you know, like you said like the diverse set of things um

41:55.140 --> 41:57.540
That you might want to learn more about and so yeah

41:57.540 --> 42:01.620
I think like it clearly seems to make sense that like this kind of more open-ended this thinking is probably going to be like

42:02.020 --> 42:04.340
The next paradigm of how we think about these kinds of systems

42:04.580 --> 42:06.580
But I'll I think mentally we'll have more to say about this

42:06.980 --> 42:12.820
I think the reason open-endedness is so interesting now is I think we're uh, there's there's a few reasons why I think it's like

42:12.900 --> 42:19.700
newly relevant to this current era of machine learning because these ideas have been around for quite a while like, um, Ken Stanley, Joe Lehmann

42:20.020 --> 42:22.020
um, Jeff Klune, uh

42:22.020 --> 42:24.180
Lisa Soros these a lot of these researchers, they've

42:24.900 --> 42:30.180
They've been thinking about open-endedness and novelty based search divergent search for decades. Um

42:30.740 --> 42:35.060
I think it's really interesting to think about why there's sort of this resurgence of these ideas now

42:35.300 --> 42:37.300
and I think a lot of it is because

42:37.380 --> 42:39.700
It is again, you know, it's it's sort of following the same

42:40.100 --> 42:40.820
um

42:40.820 --> 42:43.700
Sort of uh tailwinds that have been driving a lot of the ml industry

42:43.700 --> 42:46.980
Which is just like much better compute much larger datasets

42:47.300 --> 42:49.780
And I think what we're seeing now is that we know that

42:50.260 --> 42:55.140
Modern deep learning methods work best when we can scale up the compute and the data. That's how you get them to work

42:55.540 --> 42:59.380
Um to the to their maximal capabilities. Um at some point

42:59.380 --> 43:02.420
We're going to run out of data and a lot of people are now starting to talk about

43:02.660 --> 43:04.740
You know this as sort of a pending issue on the horizon

43:04.740 --> 43:09.140
Which is you know at the current rate of consuming data for training our foundation models at some point

43:09.140 --> 43:12.500
We're going to run out of data. We're going to where are we going to get the next trillion tokens from?

43:12.900 --> 43:15.620
Um, and so I think a lot of this uh now

43:16.020 --> 43:20.340
points a lot of the interest to open-endedness because open-endedness is essentially, you know

43:20.580 --> 43:23.460
We're studying systems that can generate their own data in an infinite

43:24.020 --> 43:27.700
Capacity and so it's systems that essentially if you run it for longer and longer

43:28.020 --> 43:32.420
They get more and more complex. They generate more and more quote-unquote interestingness or interesting data

43:32.820 --> 43:33.220
um

43:33.220 --> 43:36.580
And so if we can actually, you know crack this nut of how do we actually

43:36.980 --> 43:41.220
Come up with a self-improving system in the sense that it keeps generating interesting data

43:41.940 --> 43:46.740
We can then use that data to train further train our models

43:46.980 --> 43:50.900
But of course you get into this perpetual data machine type of

43:51.460 --> 43:54.500
Idea where obviously, you know, there's how do you generate more data?

43:55.380 --> 43:59.140
If you know the data is ultimately coming from a model that you probably trained on previous data

43:59.220 --> 44:00.980
How do you get net new information from that?

44:00.980 --> 44:05.940
Well, I think a lot of this is actually just resolved purely again going back to this idea of the reward function

44:06.100 --> 44:12.180
Right or a preference function where there is outside information coming in through some sort of filtering criteria

44:12.420 --> 44:14.420
For example human designers in the loop

44:14.820 --> 44:20.420
Or designers designing some sort of preference model that could essentially automatically rate the kinds of automatic

44:21.300 --> 44:23.540
Data that's being generated by these open-ended systems

44:23.700 --> 44:28.580
And if we can do this kind of filtering we can essentially automatically find start to automatically find

44:29.140 --> 44:33.060
Useful net new data net new trajectories net new even, you know, maybe

44:33.940 --> 44:38.020
Sentences like tokens or net new content to train our models on

44:38.340 --> 44:44.820
I've been thinking a lot about creativity recently and I think creativity is is is the other half of the coin of intelligence

44:45.220 --> 44:49.700
So in the world we live in I think that the intelligent process is is us

44:50.100 --> 44:52.100
We are a divergent search and we are

44:52.580 --> 44:56.420
And basically tackling a complex search space and we are building knowledge

44:56.740 --> 44:58.820
And we are memetically sharing them in our society

44:58.820 --> 45:02.660
We're embedding them in our language and then language models come and like acquire all of that knowledge

45:02.900 --> 45:06.900
So the cynical take is that ai today doesn't you know generalize and

45:07.700 --> 45:10.260
It doesn't it doesn't creatively find new knowledge

45:10.500 --> 45:13.300
It just is a representation of the knowledge that we have found

45:13.620 --> 45:18.260
But it's not black and white is it so the the work that you're doing is a great example of no no no

45:18.660 --> 45:26.660
You can generate new knowledge by exploring these complex search spaces and even though you're exploring existing models

45:27.060 --> 45:31.860
You're discovering interesting and novel combinations of those models that have not been found before

45:31.860 --> 45:36.100
So it's creating a novel margin on something that was not there before

45:36.420 --> 45:40.660
But I suppose the ideal future we want to get into is that we really can just

45:41.460 --> 45:44.340
From a far deeper level generate new knowledge

45:45.140 --> 45:50.580
Yeah, I think one interesting thing that I've been thinking about more recently, you know is that um sort of the you know

45:50.580 --> 45:52.340
The high level question is just

45:52.340 --> 45:59.220
Right now all of the state of the rai systems from chat gbt to stable diffusion style models for text image generation

45:59.380 --> 46:03.700
All of these systems they're they're amazing very impressive, you know

46:03.780 --> 46:08.260
Like five years ago. I would not have believed that these systems could exist at this level of performance today

46:08.660 --> 46:13.460
But uh, ultimately, uh, what they do is they're in the they're they're in the q&a business

46:13.620 --> 46:18.100
So I basically ask these systems a question or I give them a command and they give me an answer

46:18.740 --> 46:23.300
Um, and so I think the next frontier of ai is really how do we design systems that don't just

46:23.940 --> 46:27.460
Answer questions, but they actually are the ones that start to ask the questions

46:28.020 --> 46:32.100
And I think once we can have ai systems that start to ask interesting questions

46:32.740 --> 46:38.820
Um, that's when we start to get closer to I think traditional notions of what uh strong agi might be

46:39.140 --> 46:41.140
Okay, so so again really really interesting now

46:41.460 --> 46:45.460
So we're getting into agency and and people think that oh you could give a language model agency

46:45.460 --> 46:48.020
You just like you know run it in a loop and interesting things will happen

46:48.180 --> 46:51.540
Well, well, that's not true because the whole point of open-endedness is to prove that

46:51.860 --> 46:55.460
Existing systems converge so they don't diverge so they don't accumulate information

46:55.700 --> 46:59.060
So we would need to create a kind of agent that like, you know, it would just keep running

46:59.060 --> 47:03.060
And it would just keep doing interesting and novel things that would keep accumulating information

47:03.460 --> 47:08.180
And I think that the reason why language models don't have agency is because they are essentially

47:08.740 --> 47:14.420
A low entropy model and what that means is during training a lot of the the sort of like the unnecessary

47:14.980 --> 47:20.100
Um, you know complexity was snipped off. So the models only know about relevant things in the next step

47:20.100 --> 47:26.180
What's the next best token and it feels like we would need to have not only a higher entropy search

47:26.420 --> 47:30.900
But we would also need to have um a diverse set of models that are actively

47:31.620 --> 47:36.740
Continually learning and and diverging from from each other, but that's just my take. I mean, what do you guys think about that?

47:37.060 --> 47:38.740
Yeah, I think that

47:38.740 --> 47:43.540
So I guess this relates quite a lot to this idea of like intrinsic motivation, which is something that we utilize in our paper and I guess

47:44.340 --> 47:46.340
I guess the idea with that is like

47:46.820 --> 47:51.700
You know, if we're trying to like gather new data in the environment, like we shouldn't necessarily be constrained to just try and

47:51.940 --> 47:54.580
Gather new data that's like good for a specific task

47:56.180 --> 48:02.020
And so I guess this kind of you know, so intrinsic motivation basically says I should just gather new information because it's novel

48:03.460 --> 48:07.540
And things like this and so we can basically like specifically try and gather information that you know

48:08.020 --> 48:14.340
Reduces our uncertainty about the environment and and or similar objectives that that don't rely on some external reward signal

48:14.660 --> 48:19.940
And I think we when you get to the situation where the model is able to like self-improve in the absence of an external reward signal

48:20.020 --> 48:24.420
So intrinsic meaning that the the signal for what you should get is just purely generated by the model

48:24.420 --> 48:26.420
So it's purely intrinsic to the model

48:26.660 --> 48:32.660
Um, so I think the situation where you know, you have the model that's able to self-improve without any external signal without a human

48:32.660 --> 48:36.580
Having to define what the reward is or what the objective is or this was good data. This was bad data

48:37.140 --> 48:42.020
Um, I feel like that does feel like a lot closer to the notion of agency because of the fact you don't have kind of some

48:42.340 --> 48:44.580
External person defining what's good and what's bad?

48:45.140 --> 48:49.780
And so yeah, I think like this the like and you also mentioned the word like creativity because I think

48:50.180 --> 48:52.180
At least in the context of things that

48:52.260 --> 48:58.260
I've done in terms of machine learning or reinforcement learning. I think like intrinsic motivation feels like the closest thing related to creativity

48:58.580 --> 49:03.940
So you're basically like trying to gather information because it's novel or because you think it's or the model thinks it's interesting

49:04.340 --> 49:05.780
rather than because

49:05.780 --> 49:07.780
You know, it satisfies some objective

49:08.020 --> 49:13.540
And so I think we could maybe say like intrinsic motivation is in some sense like an objective for being creative as well

49:14.180 --> 49:17.540
Um, I don't know if you have any thoughts about this. Yeah, I think I think that uh

49:18.260 --> 49:23.940
It's I think there's definitely a hugely deep connection between intrinsic motivation and creativity. Um

49:24.500 --> 49:27.860
In the literature intrinsic motivations also sometimes called artificial curiosity

49:27.940 --> 49:30.420
So this is a term that was coined by Juergen Schmidt-Huber

49:30.900 --> 49:33.540
Could you could you explain it just what it is? Yeah, so oh, yeah

49:33.540 --> 49:37.940
So taking a step back intrinsic motivation is essentially um in reinforcement learning

49:37.940 --> 49:43.300
We train on reward signals and as mark was saying, um, we typically train on external reward signal by external

49:43.300 --> 49:49.140
We mean that this is a task based reward. So this is um external in the sense that something outside of the agent

49:49.140 --> 49:53.780
That's learning like the human system designer decided that this is what the reward signal is for the task

49:54.100 --> 49:58.740
Uh intrinsic means that we want to we don't design directly the reward signal

49:58.820 --> 50:01.780
But we're actually using some aspect of the model itself

50:02.180 --> 50:06.900
In order to drive the models learning forward. And so one example of this could be prediction error

50:07.060 --> 50:12.420
So if the model, uh has a large prediction error on a certain task like averaged over each time step

50:12.660 --> 50:19.300
We can use that as a reward signal and say, hey, you want to visit more parts of the environment where you're bad at predicting

50:19.620 --> 50:23.700
Um, how the state will transition when you act in that part of the environment. And so

50:24.260 --> 50:28.260
Uh, as you can see, this is very similar to maybe like intuitive notions of what curiosity is

50:28.900 --> 50:30.900
Curiosity and different forms of play

50:30.980 --> 50:35.460
Um in the psychology literature, a lot of people actually argue that, you know, different forms of play

50:35.940 --> 50:41.860
In curiosity really they they amount to you can model these behaviors as essentially a person trying to

50:42.500 --> 50:45.780
Engage in activities where, you know, they're not very good at predicting the outcome

50:45.940 --> 50:47.540
And that's kind of what makes you could argue

50:47.540 --> 50:53.860
That's kind of what makes certain kinds of entertainment fun because or entertaining because you can't actually predict what will happen

50:54.180 --> 50:55.140
um

50:55.140 --> 50:59.300
You know in a few frames of the movie like a movie wouldn't be very interesting or a book would not be very interesting

50:59.540 --> 51:03.460
If you can predict what will happen in the rest of the book just by reading the first few pages

51:03.860 --> 51:08.340
Uh, and so intrinsic motivation is really saying let's guide the model towards parts of the environment or the world

51:08.580 --> 51:11.300
Or experiences where it's similarly unpredictable

51:11.780 --> 51:16.180
Stanley speaks about this this concept of deception or we call it the false compass

51:16.500 --> 51:22.820
Which is this idea that any objective and even you you could say exploring all of the search space is an objective

51:22.820 --> 51:26.260
So he said every every objective has deception and if you monotonically

51:26.660 --> 51:29.700
Optimize any objective you will always lead into you know, like a

51:30.340 --> 51:37.300
Deceptive part of the search space, but then like the counter argument is okay. Well, let's let's not um, let's not have any principles for doing the

51:38.020 --> 51:43.220
You know the the exploration. Let's just do something completely random and that doesn't seem very good

51:43.380 --> 51:51.220
So so then, you know, there's this concept of well, how do I how do I imbue some concept of what's interesting without falling victim to deception?

51:51.300 --> 51:57.220
Yes, so ken stanley, uh has a famous essay in the realm of open-endedness where he points out

51:58.580 --> 52:00.580
That this notion of interestingness

52:01.220 --> 52:06.100
Is uh, ultimately a subjective concept and so even in the case of intrinsic motivation

52:06.100 --> 52:09.300
Which I think is you know in practice we can get a lot of mileage out of this

52:09.940 --> 52:14.500
And we've seen this in a lot of domains where exploration helps a lot like even in the wakeer paper

52:14.500 --> 52:19.620
it's largely founded on this idea on how we exploit intrinsic motivation for learning world models, but

52:20.420 --> 52:23.140
Ultimately, you know, these these model based

52:23.780 --> 52:30.340
Measures of intrinsic motivation. They are by definition based on the particular model at play and so

52:31.220 --> 52:36.740
At some point, you know, you're you're starting to over fit to what that specific model finds interesting

52:37.220 --> 52:42.020
And of course what that model finds interesting if your measure of interestingness is something like a prediction error

52:43.140 --> 52:49.460
Is going to be a function of you know, the specific architecture of the model the actual inductive biases of that model

52:50.100 --> 52:53.860
The capacity of that model to learn and so you could imagine a model where you know

52:54.260 --> 52:57.780
At the beginning it's looking for lots of interesting parts of a particular video game environment

52:57.860 --> 53:00.820
But at some point, you know, it might saturate what it can represent

53:01.060 --> 53:04.020
And what it can learn and at some point it might start to find things

53:04.100 --> 53:07.940
It's explored before interesting just because it's starting to forget those parts of the environment

53:08.100 --> 53:12.020
You know if you have like a very rich stream of different kinds of environments that it's exploring

53:12.180 --> 53:18.180
So ultimately this is like an example of deception because now it's like I I think that my model is the model thinks it's exploring

53:18.260 --> 53:21.300
Parts of the environment that it finds interesting based on this prediction error

53:21.460 --> 53:26.740
But ultimately it might actually start to go back to other parts of the environment because of issues of model capacity

53:26.820 --> 53:29.860
And another really famous example of this issue would be like the noisy tv

53:30.020 --> 53:33.220
So like if your environment has you know, this this uh

53:33.860 --> 53:38.820
Noisy tv where it's just showing random noise random rgb pixels. Um, you know, that's

53:39.460 --> 53:42.260
You know, that's not something you can actually predict because it's just noise

53:42.500 --> 53:48.100
And so the model if your intrinsic motivation is really just to search for novelty in the form of prediction error

53:48.260 --> 53:53.860
It might just start staring at this tv forever because it's something that it just can't predict and I know just by looking at that tv

53:53.940 --> 54:00.500
It'll be maximizing its prediction error. Yeah. Yeah, it's so interesting. Um, so so just coming into rich stuff in a little bit

54:00.500 --> 54:06.420
So he had this idea called um reward is enough and and essentially that doesn't make in the case that you know

54:06.500 --> 54:11.700
Just using um implicit uh motivation all the stuff that that you've just been speaking about using this trajectory

54:12.180 --> 54:15.940
You know optimization process that we can do everything we need to do

54:16.260 --> 54:22.020
And in in your paper, you're kind of making an argument similar to what lakuna has been making for years about self supervised image learning

54:22.020 --> 54:25.700
That what we should do guys is let's let's kind of pre-train a base model

54:26.020 --> 54:29.060
So this model um understands environmental dynamics really well

54:29.380 --> 54:32.820
And then we stick a reward in there and and we build um agents after that

54:32.900 --> 54:38.500
So does it in any way reinforce or pun intended uh satan or or do you think it's still complimentary?

54:39.140 --> 54:44.500
I think it's still complimentary at least if I understand the the meaning of the reward is enough paper because my understanding of that

54:45.060 --> 54:48.580
Um line of thought is basically saying that you know, we can kind of specify

54:49.140 --> 54:54.500
You know any tasks that we might want an intelligent agent to do as optimizing a reward in some like mdp or promdp

54:54.580 --> 54:58.740
So market decision process or something like that and I think our work isn't contrary to that in the sense of like

54:59.540 --> 55:05.620
You know, I do think that that probably is a sufficient framework to be able to model any any kind of behavior that we might want an agent to

55:05.620 --> 55:10.100
Do but I think when it comes to actually like practically implementing that idea. There's a lot of difficulties

55:10.500 --> 55:15.540
So the first one might be um, you know, how do we even specify that reward function?

55:15.860 --> 55:18.180
so, you know, if the reward function is to um

55:18.820 --> 55:21.380
Have a good life or something like this like there's obviously like

55:21.940 --> 55:25.780
You know, maybe there is some like numerical way of defining that in terms of an mdp

55:26.580 --> 55:32.100
But there's like not actually a good way of writing down that function that maps what I do to whether I'm getting good rewards

55:32.420 --> 55:36.500
And so I think there's this kind of like, you know, I think that's a good framework for like thinking about any problem

55:36.740 --> 55:40.340
But then you have these kind of like practical issues of how do you actually define rewards?

55:40.340 --> 55:42.340
And how do you how do you say like?

55:42.580 --> 55:47.460
Were there an agents doing well and not doing well and things like this? Um, and so I think that's still um

55:48.020 --> 55:51.540
Even with the world models lines of work. I think that's still like kind of quite a difficult issue

55:51.940 --> 55:56.900
So so so the world models lines of work kind of, you know, allow you to model, you know, predicting ahead in the environment

55:57.460 --> 55:59.700
Which is a very useful thing for doing a lot of tasks

56:00.340 --> 56:02.980
Um, but then if you actually want to optimize some specific task

56:03.380 --> 56:05.780
You still have this problem of like, how do you define the reward?

56:06.020 --> 56:09.940
And so we eventually want to get to this point of being able to like inject a reward into the world model

56:10.020 --> 56:12.980
So we're kind of in agreement with that kind of line of thinking in a sense

56:13.060 --> 56:16.980
We're eventually going to use a reward to derive the the the desired intelligent behavior

56:16.980 --> 56:18.660
So I don't think there's any conflict in that sense

56:18.900 --> 56:22.740
But we still have this kind of problem of how do we inject that reward into the the world model?

56:22.740 --> 56:24.740
How do we define what that reward should be?

56:24.980 --> 56:26.980
um and the case of um

56:27.380 --> 56:29.380
You know one of the easiest things to do for example

56:29.460 --> 56:32.340
Would just be to label each image with reward and then you can kind of

56:32.660 --> 56:37.300
Encode that image into the latent space of the world model and then use that to define how good a certain thing is

56:37.620 --> 56:40.020
And that's kind of the style of thinking what we think of in our work

56:40.500 --> 56:44.180
Um, but I don't think that overcomes this like overarching issue of in general

56:44.260 --> 56:49.060
It's you know rewards can define everything, but how do you in practice like get that function is pretty hard

56:50.180 --> 56:54.500
Yeah, I mean in a sense reward is enough is sort of a tontology because once you know the reward

56:54.580 --> 56:55.700
um

56:55.700 --> 56:57.700
If you know the reward function for your environment

56:57.860 --> 57:00.980
You can essentially compute the value function, which gives you the optimal policy

57:01.220 --> 57:01.540
um

57:01.540 --> 57:04.820
And so reward has to be enough if you know the reward function and so

57:05.460 --> 57:08.900
Uh, I think the more interesting question is definitely like what is enough for the reward?

57:09.140 --> 57:16.340
What is enough to actually have a system automatically figure out what are interesting new rewards for us to train new agents?

57:16.340 --> 57:22.020
We're new models on or continue training existing models on and I think this goes back to the question of environment design

57:22.020 --> 57:27.700
This is largely the motivation of that line of work this auto curricula environment design where essentially if we can automatically

57:27.780 --> 57:31.780
Weave through this path of possible environments of the design space of the environments

57:32.180 --> 57:38.660
The design space clearly will encompass like a big part of the design space is also encompassing the reward for those tasks

57:38.980 --> 57:44.500
And so essentially we want to find a curriculum automatic curriculum or path through the possible reward functions

57:44.820 --> 57:47.460
In which we can start to train a more and more general agent

57:47.700 --> 57:49.700
But then the interesting question is again

57:49.780 --> 57:55.940
Like what exactly is the right notion of interestingness in order to drive that curriculum that path through the design space

57:56.100 --> 58:01.300
of possible things we could be training our model or agent on and um, and that's I think

58:01.700 --> 58:07.060
One of the most interesting open questions and it relates to the question as well of how do we get the model to ask the questions?

58:07.700 --> 58:11.220
Because really what drives humans in terms of asking further questions

58:12.020 --> 58:17.220
Is our own implicit notion of interestingness which is informed by things like the scientific method and you know

58:17.300 --> 58:20.340
being able to create explanations about the world

58:20.740 --> 58:25.460
And we find things interesting when we can't actually explain some phenomenon about the world

58:25.940 --> 58:27.940
Based on existing theories or explanations

58:28.260 --> 58:33.700
And so I think what's really missing for a well-grounded, you know, human interpretable version of

58:34.180 --> 58:40.100
Interestingness is having models that can essentially come up with their own theories about the world and start to probe those theories

58:40.260 --> 58:43.620
For where there's mismatch between, you know, the their learned theory of the world

58:44.020 --> 58:47.700
And evidence that new evidence that they find from experiences in the world

58:48.020 --> 58:52.900
Yeah, it's so interesting and and um, I mean when I make the argument that agent should be physically and socially embedded

58:52.980 --> 58:58.100
It's it's actually quite a simple argument, which is just the guardrails. It's that interesting this thing

58:58.180 --> 59:05.060
I think that that is how, you know, having um, uh agency but with the guardrails of our physical and social embedding

59:05.140 --> 59:08.180
So, you know, we're we're sampling things that make sense because they're already there

59:08.420 --> 59:12.100
That, you know, but but but obviously we can go off piece to little bit as individual agents

59:12.340 --> 59:15.860
I I feel that that's what helps that process just coming back to Sutton

59:15.940 --> 59:18.100
It's entirely possible that I've misunderstood Sutton, by the way

59:18.100 --> 59:24.420
So my my interpretation of reward is enough and it might be true as you say that it's tautological given that if you already knew

59:24.740 --> 59:28.740
The reward function for a particular environment then it could do everything that it needed to do

59:28.820 --> 59:34.900
But my interpretation of reward is enough is that it would lead to um, a general intelligence and you know

59:34.980 --> 59:39.380
General in in the kind of magical sense that it would work in in any possible situation

59:39.700 --> 59:44.900
But if it is specialized in the way that we agreed earlier that there exists a reward function

59:44.980 --> 59:46.980
Which would inco you know codify

59:47.140 --> 59:52.740
Motifs and things that you know, you need to know or optimize in a particular environment or set of environments

59:53.140 --> 59:57.220
Then to me that's still specialized intelligence and I would agree. Yeah. Yeah, yeah

59:57.300 --> 01:00:02.260
That's that I think that aligns with my take as well where I think if you have a reward function

01:00:03.140 --> 01:00:05.140
It's already sort of applying

01:00:05.620 --> 01:00:09.220
Largely applies to at least the examples in that position paper about reward is enough

01:00:09.220 --> 01:00:12.100
It seems like most of the reward functions they discuss are largely

01:00:12.980 --> 01:00:14.980
Grounded in a specific task

01:00:15.060 --> 01:00:17.620
And I think that if you have the reward function for a specific task

01:00:17.940 --> 01:00:22.020
Then it definitely seems that you can have some optimization or learning algorithm

01:00:22.260 --> 01:00:25.540
That essentially learns to optimize that reward and therefore achieve that task

01:00:26.420 --> 01:00:28.500
So I do think sort of the open question that

01:00:28.980 --> 01:00:31.700
Uh, I think same reward is enough

01:00:31.780 --> 01:00:36.500
I think it kind of passes the buck up further one level to the question of where that reward comes from

01:00:36.660 --> 01:00:41.940
And I do think that having systems that can automatically design interesting new rewards. That seems like the frontier

01:00:42.180 --> 01:00:47.220
Yeah, I agree and and you know because to me intelligence is about discovering the knowledge and the knowledge is the reward function

01:00:47.220 --> 01:00:51.060
So if it was like kind of baking the knowledge in into the system, um, okay

01:00:51.060 --> 01:00:56.900
so another sort of galaxy brain take is um, I was talking to bishop about this the other day and um

01:00:57.620 --> 01:01:00.180
Do you think of like deep learning models as one model?

01:01:00.180 --> 01:01:06.420
Or do you think of them as a sort of like intrinsic ensemble of models because they they get they behave differently in an input sensitive way

01:01:06.740 --> 01:01:10.340
So, you know, like depending on the prompts you put into language into a language model

01:01:10.580 --> 01:01:15.620
You might find that like a different part of the weight space gets activated and and essentially it's like retrieving a mini program

01:01:15.860 --> 01:01:20.180
And that program is being run, but it's not it's not model building. It's like model

01:01:20.900 --> 01:01:22.900
Retrieving, but we would would you agree of that?

01:01:23.140 --> 01:01:24.180
Hmm

01:01:24.180 --> 01:01:26.180
I guess i'm not sure about that like

01:01:26.180 --> 01:01:28.500
Like within like subsets of a single homogeneous model

01:01:28.500 --> 01:01:32.500
But I guess the thing that I like to think about that's I think quite related to this is this idea of like

01:01:33.140 --> 01:01:36.740
And I think yamakun also kind of well a lot of people have laid out like a similar architecture

01:01:36.740 --> 01:01:41.300
It's like, you know, should we think of intelligent agents as having kind of like separate subsystems that can maybe

01:01:41.860 --> 01:01:45.060
Like be thought of as different neural networks. And so, you know, we could have like, you know

01:01:45.860 --> 01:01:49.060
Um, the standard notion of a policy which is like outputting actions

01:01:49.380 --> 01:01:52.820
And maybe we also want to have the notion of like a prediction model more like a world model that predicts

01:01:52.820 --> 01:01:57.540
What might go ahead in the world as well as maybe like a planner that is somehow good at like optimizing in that model

01:01:57.540 --> 01:02:01.860
And so we could kind of think of all these things as like separate subcomponents that we assume an intelligent

01:02:02.100 --> 01:02:03.700
You know an intelligent

01:02:03.700 --> 01:02:06.660
Thing would have like an intelligent thing should be able to predict ahead in the world

01:02:06.660 --> 01:02:08.660
It should also be able to output actions

01:02:08.660 --> 01:02:12.260
It should hopefully maybe be able to infer like why other things happened and things like this

01:02:12.740 --> 01:02:16.980
And so I guess as to whether we think that should you know be just like one homogeneous model

01:02:17.780 --> 01:02:21.380
For which maybe you query it and maybe you know different aspects of that model are kind of um

01:02:22.020 --> 01:02:25.700
You know handle different aspects of the query or whether we should think of those as separate components

01:02:25.700 --> 01:02:29.620
I'm not I'm not really sure as to whether it matters whether they're separate components or not because yeah

01:02:29.620 --> 01:02:32.580
I agree that you probably could just have like one massive model that does all these things

01:02:32.980 --> 01:02:35.140
And I think at least from the the trend that I've been seeing

01:02:36.340 --> 01:02:40.580
In kind of the world models literature and and also just like I guess the rl literature

01:02:40.580 --> 01:02:42.580
Or maybe just we should call it the foundation model literature

01:02:42.740 --> 01:02:46.820
Is you kind of don't want to have like a separate model that does the prediction for actions and a separate model

01:02:46.820 --> 01:02:50.020
That does the prediction observations like why not just have one massive model

01:02:50.260 --> 01:02:54.020
That's jointly trained to predict everything you might want to query and then depending on the different query

01:02:54.100 --> 01:02:59.780
You know it will just either predict an action or a predictive video sequence or it can be conditioned on actions or conditioned on language

01:03:00.020 --> 01:03:04.340
So I think in this sense like this kind of model like you said is more like just one massive model

01:03:04.340 --> 01:03:07.220
But it kind of has like a lots of different sub tasks that it's able to do

01:03:07.780 --> 01:03:08.340
um

01:03:08.340 --> 01:03:12.340
And so maybe this is actually like the more effective way of training a model because then you kind of get generalization

01:03:12.340 --> 01:03:14.340
Across these different sub tasks as well

01:03:14.500 --> 01:03:16.500
Well, yeah, and the reason I'm asking the question is um

01:03:17.140 --> 01:03:21.860
It seemed I mean like you know for for an outsider coming in it looks like statistics has broken

01:03:22.100 --> 01:03:25.140
You know in the olden days we used to talk about the no free lunch theorem used to say like you know

01:03:25.140 --> 01:03:27.620
You need to have specialized models for different situations

01:03:27.860 --> 01:03:30.100
And now the narrative is that we have generalist models

01:03:30.100 --> 01:03:34.260
We have foundation models and and they are better than the specialized models in a strong sense

01:03:34.660 --> 01:03:37.780
And you know and I like to sort of push on this a little bit and see well

01:03:37.860 --> 01:03:43.700
When when does it break because we know that there are like these physics inspired models with inductive priors that you know

01:03:43.780 --> 01:03:47.940
Know about invariance of you know like molecules and drug discovery and stuff like that

01:03:47.940 --> 01:03:50.260
And surely they would be better than a language model

01:03:50.260 --> 01:03:54.420
But no no no now they're training language models on mathematical conjecturing and like you know like

01:03:55.060 --> 01:03:59.700
Drug formulation using tokens and and so on so you know as an outsider you might just think well

01:03:59.700 --> 01:04:01.860
We can just use a big transformers model for everything

01:04:02.820 --> 01:04:06.260
I I think a lot of this does come from um well, so

01:04:07.220 --> 01:04:09.540
I think the attention-based transformer architecture is

01:04:10.100 --> 01:04:15.140
Proven empirically to just be highly scalable highly effective at learning lots of different kinds of data distributions

01:04:15.780 --> 01:04:19.300
But I think also part of it is just that we're just starting to enter this regime

01:04:19.300 --> 01:04:24.500
When we're just training these models on an insanely large amount of data, and I think that a lot of times

01:04:25.380 --> 01:04:29.780
We need to sort of take a step back and really consider the amazing performances on different tasks

01:04:30.180 --> 01:04:33.620
And really think about you know how much information was actually leaked into

01:04:34.340 --> 01:04:36.340
This task in the training data because

01:04:37.300 --> 01:04:39.300
Right now. We're really just training

01:04:40.020 --> 01:04:42.020
these huge models on

01:04:42.020 --> 01:04:45.700
I think I would say that we're largely training them on the test distribution in many cases

01:04:46.420 --> 01:04:49.700
I do there I have seen like lots of examples of

01:04:50.180 --> 01:04:53.540
Truly impressive behaviors from these models that that do seem like

01:04:54.340 --> 01:04:57.220
Truly novel like zero shot generalization to unseen tasks

01:04:57.700 --> 01:05:00.820
Like there was a recent example. I saw on twitter or someone

01:05:01.380 --> 01:05:04.580
Had like a very low resource like rare language and they gave it a few

01:05:04.980 --> 01:05:10.980
They gave I think the cloud 3 model a few examples and it was able to essentially perfectly reproduce new utterances in that language

01:05:11.540 --> 01:05:13.540
So that does seem very impressive

01:05:13.780 --> 01:05:19.140
But it does seem at the same time, you know a lot of the performances for example on elsat or like ap biology exams

01:05:19.300 --> 01:05:21.380
I imagine a lot of that is really a function of just

01:05:22.340 --> 01:05:27.780
Literally giving the model the test domain in terms of information during the training step

01:05:28.180 --> 01:05:29.220
Okay, okay

01:05:29.220 --> 01:05:32.260
So there are like two schools of thought on this when we talk about world models

01:05:32.260 --> 01:05:36.980
You know people are talking about sorrow and is it building a world model and and it certainly seems to be it seems to be doing

01:05:37.060 --> 01:05:38.420
I mean, obviously it's not doing navier stokes

01:05:38.420 --> 01:05:41.220
It's not doing like fluid dynamics, but it seems to be doing something like that

01:05:41.460 --> 01:05:45.140
So like one extreme view is that it is just a hash table

01:05:45.540 --> 01:05:52.180
And you know, it's it's kind of doing some diffused approximate retrieval or whatever another school of thought is that it's like a simulator

01:05:52.420 --> 01:05:57.220
And you know people talk about the simulator's view of large language models and you know, like it's like it's modeling

01:05:57.540 --> 01:06:00.660
Not only, you know, just to just just the words and the language

01:06:00.740 --> 01:06:04.500
But it's also implicitly learned to model the world and the people and all of us

01:06:04.980 --> 01:06:09.700
So that's the spectrum. I mean like uh mark, where do you think these things are on that spectrum?

01:06:09.940 --> 01:06:14.340
Yeah, I think we have like it would be great to be able to play around with it and kind of see what we can get out of it

01:06:14.420 --> 01:06:16.900
But I think I think if you can for example

01:06:17.860 --> 01:06:20.900
You know after each kind of you know, so it's a language condition model

01:06:20.980 --> 01:06:26.740
So if after each kind of frame you could for example put in a different language language kind of conditioning and say like

01:06:27.220 --> 01:06:32.820
You know, what happens here if you know, the mug was pushed off the table instead of whatever else was originally happening in the video

01:06:33.060 --> 01:06:37.380
And so if you can basically do this kind of like counterfactual like interventional predictions where you kind of

01:06:37.860 --> 01:06:41.940
Give some new action and then you're able to see like the alternative outcome of that new action

01:06:42.500 --> 01:06:48.340
I think if the model is able to do that then I would think that it does have a pretty good understanding of how the world works in the sense of

01:06:48.740 --> 01:06:54.100
You know, I really think like if you can predict the outcome of any action given some sequence of observations

01:06:54.100 --> 01:06:58.580
I do think that's a pretty good proxy for being able to say if you can do that you really do understand how the world works

01:07:00.980 --> 01:07:02.660
And so I think if the model can do that

01:07:02.660 --> 01:07:06.820
I would be kind of inclined to say that it does have a kind of world model in the sense of understanding

01:07:07.220 --> 01:07:09.540
The underlying world but then there might also be a chance that you know

01:07:10.340 --> 01:07:15.700
You know these models aren't like you said it's more just like a diffuse retrieval and perhaps if you try and do like a very

01:07:16.100 --> 01:07:19.540
Fine grain conditioning on a slightly different outcome different like conditioning

01:07:19.540 --> 01:07:22.260
Maybe it won't actually give you the correct kind of counterfactual prediction

01:07:22.660 --> 01:07:27.460
And so I think maybe we'd have to see how good these models are at generalizing to slightly different inputs and things like that

01:07:27.700 --> 01:07:32.500
To really see if it understands things well, or it is just like kind of generating some arbitrary video

01:07:32.740 --> 01:07:37.700
Yeah, I think it's a double whammy because our colloquial use of language and like you know use of models and intelligence

01:07:37.700 --> 01:07:42.980
It's so static that like, you know, we we um, we think of that as being intelligence

01:07:43.380 --> 01:07:46.980
But but we're still going like we're now create we're creating knowledge right now

01:07:46.980 --> 01:07:49.940
We're creating models because we're exploring we're doing exactly what you said Minshew

01:07:49.940 --> 01:07:53.780
We're like we're exploring the search space and we're building models and we're combining them together

01:07:54.100 --> 01:07:57.780
And you know, presumably we would diverge quite quickly from from from the language models

01:07:57.780 --> 01:08:02.900
But I mean what what's your take on on this idea that they are, you know, potentially world simulators? Yeah

01:08:03.300 --> 01:08:08.740
um, so just regarding the the sort of lookup analogy for these large models, I think it's

01:08:09.460 --> 01:08:15.060
So my mental model is similar to that. Um, although I think it's it's very close to um, I think a really good write-up of

01:08:15.620 --> 01:08:17.620
of the of this alternative take

01:08:18.100 --> 01:08:19.380
Which is more like

01:08:19.380 --> 01:08:21.940
There's an alternative take which is that it is kind of like a lookup table

01:08:22.180 --> 01:08:28.420
But the prompt itself is a key that maps not to a specific sort of response, but to potentially like a function

01:08:28.740 --> 01:08:32.660
Yeah, and a vast space of functions and france wash relay had a really good

01:08:33.220 --> 01:08:36.180
Sort of blog post where he kind of goes more into the details of this viewpoint

01:08:36.340 --> 01:08:42.420
But I think that that really, you know resonates with my intuition of how these things behave where it's not literally looking up like

01:08:42.660 --> 01:08:44.660
A key value in a hash table

01:08:44.740 --> 01:08:49.140
It seems more like it's these models have learned over tremendous amounts of data to compress that data

01:08:49.140 --> 01:08:54.260
They have to learn, I think more abstract functions that help to explain that data and therefore they're learning functions

01:08:54.420 --> 01:08:56.420
So they're approximating some kind of function

01:08:56.660 --> 01:08:58.180
Or a vast family of functions

01:08:58.500 --> 01:09:02.260
And I think the prompt really acts like as a key that essentially activates a particular function

01:09:02.500 --> 01:09:08.100
And so you can kind of think of you know in the classical world where one neural network equals one function like basically it's mapping from

01:09:08.500 --> 01:09:13.060
Images to image net labels now like foundation model in the foundation model regime

01:09:13.140 --> 01:09:18.260
It's like one foundation model is essentially kind of like a giant database of lots and lots of different functions

01:09:19.300 --> 01:09:22.340
That's basically activated selectively based on the input with prompt

01:09:22.820 --> 01:09:25.380
Um, and I do think that you know based on this

01:09:25.460 --> 01:09:29.780
I think it's definitely possible that with enough data from the world enough experiential data

01:09:30.020 --> 01:09:36.420
That these foundation models can learn sort of a basis set of dynamics and transitions that explain how the world works

01:09:37.060 --> 01:09:42.980
And essentially if it does learn these transitions, um, for example in like the massive amount of video data that swore is trained on

01:09:43.220 --> 01:09:48.660
Um, I would say that yeah, I would agree that they are essentially starting to approximate, uh world models

01:09:49.060 --> 01:09:51.860
Sure. Yeah, so yeah, these are two um separate papers. So

01:09:52.420 --> 01:09:59.620
So the first one being dreamer led by like Dan and jar Haffner. So this is um, you know example of work in the space of world models and so

01:10:00.580 --> 01:10:03.620
Basically what dreamer involves doing is like a way of training a world model

01:10:03.860 --> 01:10:10.820
And then also showing that you can just generate synthetic data in the small model and then optimize decision making like purely using the synthetic data

01:10:11.540 --> 01:10:12.740
um

01:10:12.740 --> 01:10:18.580
So we talked a little bit earlier about like partially observable mdps. So we want to like take kind of the sequence of observations

01:10:19.380 --> 01:10:24.100
Um, and then be able to predict like the next a distribution of the next observation given some action

01:10:24.900 --> 01:10:28.500
and so we also talked about how you might want to like compress this into like a um

01:10:29.300 --> 01:10:31.860
More compressed representation of your of the previous observation

01:10:31.940 --> 01:10:37.460
So basically what dreamer proposes to do and a lot of works on world modeling is to take your previous sequence of observations

01:10:37.860 --> 01:10:40.100
And then you map them to some compressed representation

01:10:40.900 --> 01:10:44.740
And then could predict ahead in this latent space. Um, the next uh, latent

01:10:45.540 --> 01:10:50.740
Latent state condition on the action and then yeah, the really interesting thing about this is that now, um, you know

01:10:51.220 --> 01:10:54.420
We can in general predict what's going to happen to condition on different actions

01:10:54.660 --> 01:10:57.700
So now if you want to get like interesting behavior out of something like dreamer

01:10:57.780 --> 01:11:01.540
You can then go ahead and generate a lot of synthetic data using dreamer

01:11:02.260 --> 01:11:05.380
Or the dreamer world model and then use that to optimize behavior

01:11:05.780 --> 01:11:10.420
And so in dreamer basically the way it's done is by doing like on policy reinforcement learning in the world model

01:11:10.500 --> 01:11:13.380
So a lot of people call this like reinforcement learning and imagination

01:11:13.700 --> 01:11:19.940
So it's basically, you know, you're imagining a bunch of synthetic data then using that to like use some standard reinforcement learning algorithm and then optimize

01:11:21.060 --> 01:11:22.580
behavior in some sense

01:11:22.980 --> 01:11:25.860
And then you could also do other things like Monte Carlo tree search

01:11:25.940 --> 01:11:28.980
Which is like closer to like the works on on mu zero and things like this

01:11:29.540 --> 01:11:35.300
Creativity is a little bit like a cloud and all the creativity only happens on the surface of the cloud

01:11:35.540 --> 01:11:39.940
So there's this interesting thing that like creative discovery depends on the history of all the things that I discovered before

01:11:40.180 --> 01:11:44.020
And typically like new discovery only happens at the end of the chain not back in in the middle

01:11:45.060 --> 01:11:48.980
Exactly and and there's also this notion that creativity happens through knowledge

01:11:49.060 --> 01:11:52.420
So like knowledge new knowledge doesn't come from the ether. It's kind of

01:11:53.380 --> 01:11:56.020
There's some creative component to it, but it's it's on the

01:11:56.900 --> 01:12:02.180
The the trodden path of existing knowledge that we already have. Yeah, that wasn't a very good question

01:12:02.500 --> 01:12:06.020
But you see I mean so so when we talk about imagination through like, you know

01:12:06.100 --> 01:12:09.140
Like reinforcement learning policies and and so on what we're saying is like, you know

01:12:09.220 --> 01:12:12.820
I'm I'm imagining all of these like possible, you know worlds and so on

01:12:12.980 --> 01:12:17.140
But I'm using the cognitive primitives of all of the stuff that I already know

01:12:18.020 --> 01:12:20.820
Yeah, I think knowledge is definitely a compounding

01:12:21.780 --> 01:12:23.780
compounding

01:12:23.780 --> 01:12:25.620
artifact

01:12:25.620 --> 01:12:29.700
That's basically like the culmination of everything all the experiences that we

01:12:30.660 --> 01:12:34.420
That we encounter like throughout our whole life and through also like beyond, you know

01:12:34.500 --> 01:12:42.180
Going backwards beyond like even our individual lives into like the cultural knowledge that's shared and what's really cool about language models

01:12:42.900 --> 01:12:44.900
is that they are essentially a

01:12:44.900 --> 01:12:47.220
codification of cultural knowledge and so

01:12:47.620 --> 01:12:53.300
Jeff Klune has this concept of AI generating AI and so he's got multiple pillars of essentially what it takes for

01:12:53.940 --> 01:12:55.940
You to have AI systems that generate

01:12:56.020 --> 01:13:01.060
General AI systems and he recently added actually like as a fundamental piece of this in in his framework

01:13:01.380 --> 01:13:03.540
This idea of building on top of foundation models

01:13:03.940 --> 01:13:07.460
And so he says he calls it like standing on the shoulders of giant foundation models

01:13:08.100 --> 01:13:12.980
Which is I think really just sort of the ml equivalent of building on top of cultural knowledge

01:13:13.460 --> 01:13:15.860
There's there's a real shift recently towards talking about

01:13:16.340 --> 01:13:18.020
um synthetic data

01:13:18.020 --> 01:13:21.380
And as we were just saying like, you know synthetic data, it doesn't come from the ether

01:13:21.700 --> 01:13:28.740
So we already know stuff about the world. We we build simulators and we kind of generate new

01:13:29.300 --> 01:13:35.860
Information but in the neighborhood of things that we already know and then we kind of like iterate and fine-tune on the generated data

01:13:36.340 --> 01:13:37.700
um

01:13:37.700 --> 01:13:39.540
What what do you think about that process?

01:13:39.540 --> 01:13:43.940
Yeah, no, I think yeah, maybe I'll bring it back to this like the plan to explore line of work. So, yeah

01:13:44.900 --> 01:13:48.180
um, so so basically like the motivation of that kind of work is like

01:13:48.740 --> 01:13:51.700
Kind of saying, you know, we might have some like previous data set or something

01:13:51.700 --> 01:13:53.620
And we've trained our world model on that data set

01:13:53.780 --> 01:13:57.700
But we really want to go out and like gather more data and then like improve the world model

01:13:58.580 --> 01:13:59.540
um

01:13:59.540 --> 01:14:01.540
By gathering more data

01:14:01.540 --> 01:14:06.020
And so we can use things like intrinsic motivation to then give us like a reward signal within the world model

01:14:06.340 --> 01:14:09.620
So in the sense of something like prediction error, which mentioned earlier

01:14:09.620 --> 01:14:14.100
So now we can basically like train a policy in the world model that's now not trained for a specific task

01:14:14.420 --> 01:14:17.380
But it's trained to go out and gather information in the world

01:14:18.100 --> 01:14:21.700
So basically now, you know, you do this imagining in the world model to imagine ahead

01:14:21.860 --> 01:14:24.900
But instead of imagining ahead, how do I do a task? Well, you're imagining ahead

01:14:25.140 --> 01:14:31.540
How do I get to states that I don't know what happens and therefore we'll learn more and that's basically like the motivation behind plan to explore

01:14:32.580 --> 01:14:34.580
um, and then and our um

01:14:34.740 --> 01:14:39.780
Paper waker it's it's kind of like inspired by plan to explore as well as works on like auto-curricular

01:14:40.580 --> 01:14:42.580
and so basically what we're trying to say is

01:14:43.220 --> 01:14:47.060
You know plan to explore is good for for getting an agent to go out and gather data

01:14:47.300 --> 01:14:52.500
Um within a single environment and you know and presumably once you've gathered enough data within a single environment

01:14:52.500 --> 01:14:55.620
Then you can generate a bunch of synthetic data in that single environment

01:14:56.020 --> 01:15:00.900
And then do what we discuss with dreamer in terms of like optimizing a policy for that very specific environment

01:15:01.780 --> 01:15:04.020
um, but what we're really interested in is saying, you know

01:15:04.660 --> 01:15:07.620
Let's not assume that we have like one specific environment beforehand

01:15:07.860 --> 01:15:11.700
Let's assume that you know, there's some space of you know broad range of scenarios

01:15:11.780 --> 01:15:13.380
Like we want a very like general agents

01:15:13.380 --> 01:15:17.300
There might be a bunch of different environments and then within that what those different environments

01:15:17.300 --> 01:15:20.260
We kind of want to be able to to handle absolutely any task

01:15:21.140 --> 01:15:26.580
And so in the waker paper, we're basically saying like, you know, how should we gather the data within um

01:15:27.300 --> 01:15:32.420
Within this like broad space of possible environments and tasks such that we can train a very good world model

01:15:32.820 --> 01:15:36.900
And then once we have that world model, that's kind of like capable across environments and tasks

01:15:37.220 --> 01:15:41.300
You know the assumption is that we can then use that to generate good synthetic data, which we can then

01:15:41.700 --> 01:15:43.700
Um use to optimize behavior

01:15:44.580 --> 01:15:47.620
And so maybe to talk a little bit about like how we formalize this problem

01:15:48.020 --> 01:15:51.780
Um, so, you know, we mentioned earlier this idea of like the simulation lemma

01:15:51.860 --> 01:15:56.260
So we basically say that or an existing work that says like in a single environment

01:15:56.580 --> 01:15:59.380
We can bound the gap between the optimal policy

01:15:59.860 --> 01:16:04.580
That's trained in the world model so trained in the synthetic data to the to the truly optimal policy

01:16:04.980 --> 01:16:08.820
By the error in the world model and the distribution of states generated by that policy

01:16:09.460 --> 01:16:14.500
So it's kind of intuitive like the world model should have, you know, low error and then we will get a good policy out of it

01:16:15.220 --> 01:16:19.700
But then what we're trying to say is like now, let's assume we don't know what the environment is beforehand

01:16:19.700 --> 01:16:22.100
And we also don't know what the task is beforehand

01:16:22.340 --> 01:16:25.940
So how do we get like a good world model that can handle like all of those situations?

01:16:26.660 --> 01:16:28.900
When we later want to go ahead and optimize some task

01:16:29.620 --> 01:16:30.740
um

01:16:30.740 --> 01:16:32.740
And so the way that we do this is we basically yeah

01:16:33.220 --> 01:16:39.540
We then use this notion of mini max regret to say that the policy should have like low maximum regret across this hot entire space of environments

01:16:40.100 --> 01:16:42.580
And then using the simulation lemma we can basically say now

01:16:43.220 --> 01:16:47.300
Now the um the world model has to have low error across all environments

01:16:47.940 --> 01:16:53.220
Under the distribution of states generated by the optimal policy for any future task

01:16:53.860 --> 01:16:57.700
Um, so we're going to say like yeah, the world model has to be good for any environment and

01:16:58.260 --> 01:17:02.100
Under, you know in any area that the policy might go to that's relevant to the future tasks

01:17:03.140 --> 01:17:06.740
And then what we kind of say in the paper is, you know, if we want a truly general agent

01:17:06.980 --> 01:17:09.300
We're not going to know what the distribution of tasks is beforehand

01:17:09.300 --> 01:17:12.900
So we don't know we don't know what the reward function is. We don't have a set of reward functions

01:17:13.620 --> 01:17:17.380
Um, you know, we're just going to kind of assume the agent has to do anything later down the line

01:17:17.380 --> 01:17:20.900
And this is kind of like related to this idea of like open-endedness that we've talked a lot about

01:17:21.780 --> 01:17:25.060
And so if we don't know what the task is going to be like later down the line

01:17:26.020 --> 01:17:30.740
Then the best assumption we can do is say that, you know, it could be any reward function later down the line

01:17:31.380 --> 01:17:35.540
Which is maybe not the best assumption because as we talked a bit earlier if you're just kind of

01:17:36.180 --> 01:17:39.300
You know, we talked about a bit about intrinsic motivation and interestingness

01:17:39.620 --> 01:17:42.660
And if you kind of assume the task can be absolutely anything later down the line

01:17:43.060 --> 01:17:47.300
You're kind of assuming that, you know, the agent might want to do something completely ridiculous later like it

01:17:48.100 --> 01:17:52.340
If you do this in robotics, that might mean the task is just to do like backflips later or something like that

01:17:52.340 --> 01:17:56.340
But you have no interest in doing that. So it's it's not clear if that's really a good assumption about

01:17:56.740 --> 01:18:00.180
How we should think about what tasks might be interesting later, but that's the assumption we make

01:18:00.180 --> 01:18:02.580
So we assume the task can be absolutely anything later down the line

01:18:04.340 --> 01:18:07.220
So so now we have to get a to the point where we have the world model

01:18:07.700 --> 01:18:13.140
Which is good for any environment and under the distribution of states generated for any task or any optimal reward function

01:18:15.300 --> 01:18:18.340
And to do this we basically like leverage two different techniques

01:18:19.300 --> 01:18:21.300
So to generate this state um

01:18:21.620 --> 01:18:25.940
So to handle the aspect that we don't know what the task is later down the line. We assumed that um

01:18:26.820 --> 01:18:31.940
We have an intrinsically motivated policy that's basically seeking out the maximum uncertainty in any single environment

01:18:32.500 --> 01:18:34.500
And so basically if if this um

01:18:34.980 --> 01:18:38.900
If this intrinsically motivated policy is seeking out the maximum uncertainty in every environment

01:18:39.380 --> 01:18:46.020
Um, it's kind of like estimating for us what the maximum uncertainty is in every environment because it's like actively finding uncertainty in every environment

01:18:46.820 --> 01:18:50.180
So now we have a policy that's finding like the maximum uncertainty in every environment

01:18:50.820 --> 01:18:54.420
And then if we want to optimize this like mini max criterion across environments

01:18:54.820 --> 01:18:58.260
We kind of need the maximum uncertainty to be low across all environments. So

01:18:58.980 --> 01:19:04.340
So we kind of have to have like um, you know, this policy isn't able to find like lots of big errors across all different environments

01:19:05.300 --> 01:19:06.260
um

01:19:06.260 --> 01:19:07.220
And so

01:19:07.220 --> 01:19:11.540
Basically, you know, what we could think like what what happened in practice is, you know

01:19:11.540 --> 01:19:13.140
You can imagine there are a bunch of different environments

01:19:13.140 --> 01:19:16.020
Some which are like a low complexity and some of which are high complexity

01:19:16.900 --> 01:19:20.420
And if we just kind of naively sample from those two different environments data, you know

01:19:20.500 --> 01:19:23.300
Our world model is going to very quickly get good at the low complexity environment

01:19:23.940 --> 01:19:29.060
And then it's going to leave a lot more data from that high complexity environment to eventually get the errors low in the high complexity environment

01:19:29.940 --> 01:19:34.660
So to bring it back to the title of the paper, which is weighted acquisition of knowledge across environments for a bussiness

01:19:34.980 --> 01:19:37.140
So the idea here is that we're basically going to

01:19:38.100 --> 01:19:44.180
Change how we sample that distribution of data across environments to make sure that maximum uncertainty stays low across environments

01:19:44.580 --> 01:19:49.860
So what this ends up looking like is, you know, we're going to sample less data from the environment that has lower complexity

01:19:50.500 --> 01:19:54.340
And then we're going to actively sample more data from the environment that has higher complexity

01:19:54.660 --> 01:19:57.620
Such that we we bring those errors down on the higher complexity environments

01:19:58.180 --> 01:20:03.540
And I guess it's a little bit different to existing works on curricula because normally in curricula like automatic curriculum learning

01:20:04.180 --> 01:20:06.100
You kind of assume that you have some reward function

01:20:06.100 --> 01:20:12.740
Which is telling you how well the policy is doing in each environment and use use that specific like metric of how well the policy is doing

01:20:13.540 --> 01:20:17.060
To determine, um, you know, where the policy has more potential to learn

01:20:17.380 --> 01:20:20.580
But because we're making this assumption that, you know, we don't know what the reward function is

01:20:20.740 --> 01:20:23.860
We're trying to get a general agent that can kind of do any task any reward function

01:20:24.740 --> 01:20:27.700
Um, we don't assume that we know that reward function beforehand

01:20:27.700 --> 01:20:31.620
So we can't use reward as a metric of saying like I need more data from here or I need more data from here

01:20:32.420 --> 01:20:35.540
But then kind of the main argument of the paper is showing that, you know

01:20:35.620 --> 01:20:38.820
If we just think about this in terms of prediction error in the world model

01:20:39.540 --> 01:20:43.140
Like we can actually use that as like an intrinsic motivation signal to say, you know

01:20:43.220 --> 01:20:48.180
Does the agent need to gather more data from this environment or from this environment without access to reward function

01:20:48.740 --> 01:20:50.740
and so we could kind of think of um

01:20:51.060 --> 01:20:57.300
This work as kind of a more general approach to automatic curriculum learning in the sense of like we're not assuming that you have a reward function beforehand

01:20:57.300 --> 01:20:59.300
We're kind of agnostic to what the task is

01:21:00.100 --> 01:21:04.100
And because and to kind of distill that knowledge that's that's gathered without the reward function

01:21:04.180 --> 01:21:06.980
We use the world model as a mechanism to like distill that knowledge

01:21:07.300 --> 01:21:11.300
Because if you just like naively have an agent gathering information with no reward function

01:21:12.340 --> 01:21:14.820
You know, how do you how do you kind of put that knowledge into the agent?

01:21:14.820 --> 01:21:17.140
And we kind of argue the best way of doing that is the world model

01:21:18.260 --> 01:21:22.980
So that's kind of a summary of like the waker paper and what like what the ultimate algorithm ends up doing

01:21:23.540 --> 01:21:28.340
So I mean essentially you're doing a high entropy search. So you're you're leaning into

01:21:29.140 --> 01:21:32.020
Areas of complexity and you're building a higher complexity model

01:21:32.020 --> 01:21:36.500
Which goes against the grain of the intuition of like Occam's razor that should have simple models

01:21:36.500 --> 01:21:42.020
So you're you're almost deliberately saying no, I want I want to model the the complexity and have more of that

01:21:42.340 --> 01:21:45.620
And then the other interesting thing is like from from a curriculum learning point of view

01:21:45.620 --> 01:21:50.420
I think traditionally we did explicit curriculum learning and you know, we might have some

01:21:50.980 --> 01:21:54.580
Principles around having a monotonically increasing curriculum of complexity

01:21:54.980 --> 01:21:56.980
Whereas here by leaning into

01:21:57.940 --> 01:22:01.300
Environments where we do worse on so we're selecting them based on prediction error

01:22:01.460 --> 01:22:06.740
We're actually implicitly getting a kind of monotonically increasing complexity, which just happens to work really well

01:22:07.540 --> 01:22:11.460
Yeah, I guess actually it actually almost ends up being in the opposite direction

01:22:11.460 --> 01:22:15.140
So so by leaning into the the the higher complexity environments more

01:22:15.140 --> 01:22:18.420
We're kind of saying let's prioritize the harder environments more to begin with

01:22:18.420 --> 01:22:21.700
So let's like gather more data in the higher complexity environments

01:22:22.660 --> 01:22:27.140
Um, you know, because I guess intuitively if you kind of want to be good across all environments

01:22:27.620 --> 01:22:30.180
You kind of need more data from the higher complexity environments

01:22:30.500 --> 01:22:34.260
And we don't really explicitly think about an ordering of going first from easy to hard

01:22:34.740 --> 01:22:37.540
Um, I guess that maybe there is a something to look into there because

01:22:38.180 --> 01:22:39.060
You know

01:22:39.060 --> 01:22:42.660
Like a lot of these works go from low complexity to high complexity because it's kind of easier to learn

01:22:42.660 --> 01:22:47.540
An initial policy that can kind of do something in the low complexity environment and then you build up the complexity

01:22:47.940 --> 01:22:52.100
Gradually, um, but I think that that idea is most useful when you know what the task is

01:22:52.100 --> 01:22:54.980
So you could imagine if the task is like low commotion if it's walking

01:22:55.380 --> 01:22:58.180
You kind of want to first learn a policy that's able to walk on flat ground

01:22:58.260 --> 01:23:02.420
And then maybe gradually build up the complexity like add and bumps and then eventually it can walk on like a very

01:23:03.060 --> 01:23:05.940
Complicated terrain so it kind of makes sense to go from low to high complexity

01:23:06.500 --> 01:23:08.500
um, but in this work we're focusing on

01:23:09.300 --> 01:23:13.540
purely intrinsic motivation meaning that the policy is not trying to learn a specific task

01:23:13.860 --> 01:23:17.860
It's trying to just seek out um uncertainty and like reduce uncertainty

01:23:18.260 --> 01:23:20.180
And so we don't really have the notion of you know

01:23:20.180 --> 01:23:22.820
You first need to be able to learn how to do something on an easy

01:23:23.380 --> 01:23:28.340
An easy environment and then move towards harder environments because there is no specific task that we're trying to learn

01:23:28.660 --> 01:23:33.860
And so I think for this reason, you know, we wouldn't didn't really focus on this notion of moving from easier to harder environments

01:23:33.860 --> 01:23:37.140
So that actually, you know, we're consistently something more data from the hard environments

01:23:37.780 --> 01:23:41.700
And I guess I think this relates or I think this is something that you brought up when we when we worked on this is like

01:23:42.420 --> 01:23:48.260
You know, I think we can really relate this idea to like a lot of different contexts including things like like language models, for example

01:23:49.140 --> 01:23:50.020
um

01:23:50.020 --> 01:23:54.740
So, you know, you can imagine if I'm training an llm. I don't really necessarily have this, you know

01:23:54.820 --> 01:23:57.300
Not really a reward function in some sense. You're just trying to

01:23:58.100 --> 01:24:00.100
Do like unsupervised prediction

01:24:00.340 --> 01:24:07.060
And so, you know, we could for example take the prediction error of like a language model and a bunch of different domains and say, you know, the language model is

01:24:08.020 --> 01:24:11.460
Not very good at predicting a language about some certain task or something like that

01:24:11.940 --> 01:24:16.340
And you know, we could say, you know, and intuitively the same thing kinds of holds if it's not very good at predicting, you know

01:24:17.380 --> 01:24:20.900
What the next token is in french like we should presumably gather more data in french

01:24:21.460 --> 01:24:25.140
And that so that kind of gives us a way of like actively gathering the appropriate data

01:24:25.620 --> 01:24:29.620
Um, and so yeah, I think this idea of like gathering more data based on certainty

01:24:29.620 --> 01:24:32.340
Obviously is a very general idea like the idea of like active learning

01:24:32.900 --> 01:24:34.500
Um, but we kind of like

01:24:34.500 --> 01:24:38.260
Specialized that into thinking about how do we think about this in terms of the reinforcement learning setting?

01:24:38.740 --> 01:24:42.900
It might be interesting to talk about as well like sort of because we looked at some of the metrics as well, right?

01:24:42.900 --> 01:24:46.580
The environment complexity metrics. Yeah, we don't have the external notion of difficulty

01:24:46.660 --> 01:24:53.140
But we we also did look at sort of the emergent, uh, curriculum. Yeah. Yeah. Yeah. Gotcha. Yeah, so I guess um

01:24:53.780 --> 01:24:55.220
So it kind of depended on the environment

01:24:55.220 --> 01:24:59.220
So in some environments, you just kind of got this like very straightforward behavior of like, you know

01:24:59.300 --> 01:25:01.860
Consistently gather more data in the more complex environment

01:25:02.660 --> 01:25:06.180
um, but because we're we're actively trying to gather data, um

01:25:06.900 --> 01:25:12.900
Of the the environments for which the uncertainty is the highest kind of this curriculum could change over over the course of training

01:25:12.980 --> 01:25:15.220
So so what happened in some of the other environments?

01:25:15.220 --> 01:25:18.740
For example, is that initially all the environments are just like high uncertainty

01:25:19.060 --> 01:25:24.420
Like there's like all environments are kind of misunderstood therefore like sample all environments like equally more or less

01:25:24.580 --> 01:25:26.580
To just get a rough understanding

01:25:26.740 --> 01:25:29.540
And then you know as as the model would improve on the simplest environments

01:25:29.540 --> 01:25:33.380
Then we would see like more and more emphasis towards sampling the highest complexity environments

01:25:33.700 --> 01:25:37.780
So I guess in that sense we would get something to more like kind of what you said in terms of like a standard curriculum

01:25:38.020 --> 01:25:41.140
But a bit different in the sense of like initially everything is uncertain

01:25:41.300 --> 01:25:43.300
So we're just going to sample everything uniformly

01:25:43.780 --> 01:25:46.340
Um, but then we kind of get a better understanding of which of the environments

01:25:46.420 --> 01:25:51.780
You know the uncertainty remains high on these higher complexity ones and those are the ones we need to like go out and gather more data

01:25:52.100 --> 01:25:52.820
Yeah

01:25:52.820 --> 01:25:54.100
I mean I can see this both ways

01:25:54.100 --> 01:25:57.380
I mean certainly from like a Bayesian optimization point of view that there's something to be said for

01:25:57.540 --> 01:26:02.340
Um, you know, this is where I'm uncertain going gather more data where where I have highest uncertainty

01:26:02.820 --> 01:26:05.300
And uh, as you say like traditionally in curriculum learning

01:26:05.540 --> 01:26:08.500
We are told that we need to have monotonic increasing complexity

01:26:08.500 --> 01:26:12.500
But as you just said that's when we have a particular task in mind now neural networks

01:26:12.500 --> 01:26:15.780
They're a little bit like a block of clay aren't they so you know, it starts off with

01:26:16.260 --> 01:26:21.460
Abject complexity and then we do stand, you know, we do um stochastic gradient descent and we chip away at the clay

01:26:21.540 --> 01:26:27.300
And we kind of build we sculpt a statue that that that we want to build and I'm just trying to get an intuition here

01:26:27.300 --> 01:26:29.300
So like with this maximum entropy

01:26:29.540 --> 01:26:31.540
Search, you know like high entropy search

01:26:31.620 --> 01:26:33.060
What we're doing is is we're saying okay

01:26:33.060 --> 01:26:38.660
Well, here are some complex models and these models must contain motifs that tell us a lot of information

01:26:39.060 --> 01:26:40.900
It's a little bit like the elo algorithm in chess

01:26:40.980 --> 01:26:44.740
You know, you actually get information gain when something surprising happened

01:26:45.060 --> 01:26:49.700
So here's a big block of complexity and I'm going to try and infer

01:26:50.260 --> 01:26:53.940
What the motifs are in that complexity that that explain the information that I'm missing

01:26:54.180 --> 01:26:59.220
I think that a lot of this ultimately traces back to sort of there's like this like fundamental pattern

01:26:59.620 --> 01:27:03.140
towards uh, I think that like ties a lot of these ideas around active

01:27:03.620 --> 01:27:10.340
Um active experiment design or like active sampling, which is and all these autocurricular methods, which is you essentially want to devise

01:27:10.900 --> 01:27:16.340
Uh, what you know nowadays we call a self supervised objective or self self supervised training algorithm

01:27:16.740 --> 01:27:21.860
Um, where essentially you have the system essentially use signals. It produces itself

01:27:22.340 --> 01:27:28.420
Um during the training or evaluation process in order to drive itself forward in terms of deciding what future data to train on

01:27:28.740 --> 01:27:35.060
And so, you know, we sometimes call these kinds of systems autocurricular as well because it's automatically generating this curriculum of

01:27:35.380 --> 01:27:39.220
Tasks to train on and I think the sort of like the fundamental connecting

01:27:40.260 --> 01:27:44.020
Uh pattern here is just that this the signal that we use to drive the training

01:27:44.180 --> 01:27:48.660
It's always going to be based on something like, uh, an uncertainty signal or, um

01:27:49.060 --> 01:27:53.300
Going back to the open-endedness literature something like a classic notion of interestingness

01:27:53.780 --> 01:27:58.580
And I think there's just a lot of different possible choices for this metric and so

01:27:59.380 --> 01:28:01.380
One for example, we talked a lot about

01:28:01.380 --> 01:28:02.260
Minimax regret

01:28:02.260 --> 01:28:07.620
So regret could be one of these driving signals because it measures the existence of a performance gap and therefore

01:28:07.700 --> 01:28:11.780
Probably an information gap as well in terms of learning to master those tasks with high regret

01:28:12.260 --> 01:28:18.340
But also uncertainty is also another one it ties back to novelty because novel environments you will be more uncertain within

01:28:18.740 --> 01:28:23.540
And so there's fundamentally lots of different sort of branches of these autocurricular that you could use

01:28:23.540 --> 01:28:27.780
Depending on this search objective that you use to drive this exploration process

01:28:28.900 --> 01:28:32.820
Can we contrast this to you know, like, um, large language models that they are self-supervised learning

01:28:32.980 --> 01:28:38.340
So, you know, we do this self-supervised objective, you know, which is like, you know, typically predict in the next word

01:28:38.580 --> 01:28:41.460
And it's a similar thing with, um, self-supervised, um image

01:28:42.260 --> 01:28:49.300
Learning now the difference is with that is you're talking about a principled way of, you know, seeking specific information

01:28:49.620 --> 01:28:54.900
You know with, um, let's say high entropy and that would lead to an implicit curricula

01:28:55.220 --> 01:28:58.980
Whereas with language modeling language modeling, there is no implicit curricula

01:28:59.220 --> 01:29:05.780
But I might argue that there kind of is because the way the model does this continual learning, um, it might regularize itself

01:29:05.780 --> 01:29:09.860
So if you give it sort of surprising and weird information, the language model might just kind of brush it off

01:29:10.020 --> 01:29:14.500
And if you reinforce things that it already knows then it's almost like a stream of channels, you know

01:29:14.500 --> 01:29:18.420
It'll say, okay, you know go and go and pay attention to that. So it's almost like it's implicit

01:29:18.660 --> 01:29:22.660
Yeah, and I would say that in some ways it's almost explicit in terms of how we design these systems

01:29:23.220 --> 01:29:26.580
A lot of times like if you look at, for example, open ai's job listings

01:29:26.740 --> 01:29:31.860
They're actually hiring specifically for experts in different domains to essentially create the next

01:29:32.500 --> 01:29:36.500
Batch of supervised data to train or instruction tune their models on

01:29:36.900 --> 01:29:41.060
For example, they hire biologists or they hire people with legal expertise to generate this data

01:29:41.380 --> 01:29:47.220
And you can think of this essentially as a human steered or human driven version of this active sampling process, right?

01:29:47.460 --> 01:29:54.340
Because essentially they know that the model tends to get high perplexity or they don't it doesn't perform as well on this domain of tasks

01:29:54.500 --> 01:30:00.740
It doesn't get as high of an LSAT score as it could and so you can essentially, you know, it's it's beyond an algorithm at this point

01:30:00.820 --> 01:30:06.100
Right, it's kind of the super algorithm where you have the system designers now also being part of the data collection process

01:30:06.580 --> 01:30:08.580
and in a way

01:30:08.580 --> 01:30:13.540
supervised learning is really just sort of one point in a continual learning process where, you know

01:30:14.100 --> 01:30:18.500
Classically, we just looked at one step of this which is here's a batch of data train on that but really

01:30:19.380 --> 01:30:23.460
Building machine learning systems, especially nowadays. Everything's in production. These are all live systems

01:30:23.700 --> 01:30:27.540
You have to keep it up to date. You have to keep it continually generalizing to new knowledge

01:30:28.820 --> 01:30:36.100
Like chat gpt or clod or gemini and so really it's sort of this pattern over and over again in sequence where you collect a batch of data

01:30:36.500 --> 01:30:38.820
Train your model on that collect the next batch of data

01:30:39.620 --> 01:30:41.620
You know continue training your model on that

01:30:42.180 --> 01:30:48.660
And really you want to be selective about what the next batch of data is because obviously if you just retrain it on the previous batch of data

01:30:49.220 --> 01:30:51.060
It's going to overfit to that data

01:30:51.060 --> 01:30:56.980
Beyond a few epochs or it's not going to you know get as much novel information from it just because it's already trained on it

01:30:57.220 --> 01:31:01.140
So you do want to selectively actively collect the data

01:31:01.540 --> 01:31:06.020
And so I think we kind of almost explicitly already do this at a systems level

01:31:06.820 --> 01:31:12.340
And I think the next frontier is really just having systems that self-improve in this way where they can start to guide

01:31:12.660 --> 01:31:18.660
More of their own active data collection. I love this way of thinking about it. You know like gbt4 is a memetic intelligence

01:31:18.660 --> 01:31:22.020
It's not just like you know a bunch of weights on on a on a server somewhere

01:31:22.420 --> 01:31:26.020
And so you could argue, you know, there's this concept called graduate student descent

01:31:26.020 --> 01:31:29.940
Which is what happens in academia or even as you just articulated with open ai

01:31:29.940 --> 01:31:33.060
It's a little bit like an epic mechanical turk right where you know

01:31:33.780 --> 01:31:34.900
They are monitoring the logs

01:31:34.900 --> 01:31:40.100
They know when things go go badly and then they lean into it in the same way you are they go in higher experts

01:31:40.100 --> 01:31:42.980
And they kind of like add more and more data in all of the holes

01:31:43.140 --> 01:31:46.180
And eventually there are no more pockets of like abject failure

01:31:46.180 --> 01:31:51.460
It just it just appears to work really well for everyone and people start to say that it's you know, generally intelligent

01:31:51.700 --> 01:31:54.980
So yeah, so there's this interesting systems view of of intelligence

01:31:55.220 --> 01:31:57.860
Yeah, it kind of starts to mimic just the scientific process in a way

01:31:58.420 --> 01:32:02.260
Where we're sort of we were putting a lot of hope in the model to basically be able to distill

01:32:02.820 --> 01:32:05.700
information from sort of the net news batch of data that we collect

01:32:06.580 --> 01:32:08.820
You know that we know the model currently doesn't explain well

01:32:09.060 --> 01:32:15.140
And we we we put a lot of faith and gradient descent in order to basically be able to come up with updates to the weights

01:32:15.300 --> 01:32:21.140
That better explain that data. So we're kind of we're kind of already treating the system as almost like an automated

01:32:21.860 --> 01:32:25.140
Scientist or an automated version of this like continual

01:32:25.860 --> 01:32:28.420
process of creating theories and explanations about the world

01:32:29.300 --> 01:32:31.220
But of course, you know

01:32:31.220 --> 01:32:35.860
Humans are still much better at language models at doing this or large models at doing this

01:32:35.860 --> 01:32:39.220
So I do think there clearly seems like a huge gap in terms of

01:32:39.540 --> 01:32:44.260
Well, we still have work that needs to be done in order to build systems that can actually build much more robust theories

01:32:45.140 --> 01:32:48.180
Based on like net do new data and even seeking that out as humans do

01:32:48.580 --> 01:32:54.100
Interesting and certainly, you know in in this broader memetic intelligence. We are still the sources of agency

01:32:54.820 --> 01:32:59.300
But um, we were just sort of talking a minute ago about there being two types of ai

01:32:59.380 --> 01:33:02.980
You know, there's there's an ai where we are the generating sources of agency

01:33:02.980 --> 01:33:07.620
But there might potentially be another ai in the future where that that is the generating source of agency

01:33:09.220 --> 01:33:11.700
Yeah, I so I think that um

01:33:12.340 --> 01:33:14.820
This kind of ties into my my the framework

01:33:14.820 --> 01:33:17.380
I personally used to think about open-ended systems as well

01:33:18.020 --> 01:33:23.780
Where I think that you know at a high level you can you can study ai sort of in silico

01:33:23.860 --> 01:33:30.420
You can study it in systems that you control that you design and that you try to like have the ai model self-improve within

01:33:30.740 --> 01:33:32.740
And so you can try to build

01:33:32.980 --> 01:33:38.100
Systems that self-improve within silico and that's going to lead to potentially some issues around like the grounding problem

01:33:38.260 --> 01:33:44.900
Where essentially it starts to the auto the auto curricular exploratory process starts to veer into parts pockets of the design space

01:33:44.900 --> 01:33:47.060
That are not relevant to tasks you care about

01:33:47.460 --> 01:33:52.020
Um, and so this is kind of the danger of like generating open-ended systems in silico

01:33:52.100 --> 01:33:55.860
And I think it's very similar to potential dangers of generating agi in silico

01:33:56.260 --> 01:33:57.300
um

01:33:57.300 --> 01:34:01.700
And I think the alternative is really just what are existing intelligent systems

01:34:02.100 --> 01:34:07.860
And how do we actually amplify the efficiency the efficacy of those systems the intelligence within those systems?

01:34:08.100 --> 01:34:14.180
And so you can kind of think of like sort of the entire enterprise of ai research as do we want to generate like ai or intelligence from scratch

01:34:14.500 --> 01:34:16.500
Or do we want to build tools?

01:34:16.500 --> 01:34:21.300
You know motivated or inspired by human intelligence and other intelligent systems and use that to further amplify

01:34:21.620 --> 01:34:24.260
existing intelligence like human creativity human intelligence

01:34:24.740 --> 01:34:28.180
Could you argue because if intelligence is a divergent search process?

01:34:28.900 --> 01:34:32.900
You might be tempted to think that well if we had loads of tools to help us share

01:34:33.220 --> 01:34:38.500
The models and help other people discover the models that i've created that that will help us generally be more intelligent

01:34:38.660 --> 01:34:45.380
But could you make the counter argument that i'm actually sequestering agency or stealing agency from other people because rather than thinking for themselves

01:34:45.620 --> 01:34:48.020
And discovering novel models. They're just going to use my model

01:34:48.420 --> 01:34:53.300
Yeah, I mean I think that in the best case scenario you're building systems that essentially, you know

01:34:54.100 --> 01:35:00.340
Not you know to to think about how you know as existing systems nowadays can build on the shoulders of foundation models

01:35:00.580 --> 01:35:06.740
You really want the to build models where even humans can stand on their shoulders where the humans can basically leverage the

01:35:07.060 --> 01:35:08.420
existing expertise or

01:35:08.420 --> 01:35:14.180
Automative capabilities of those models to then like move further beyond what they're naturally capable of doing

01:35:14.500 --> 01:35:18.740
And really that pushes the frontier of the knowledge that we can create as a civilization

01:35:18.980 --> 01:35:26.020
And so you're already starting to see this where there's some recent studies that show for example like junior software engineers that use systems like

01:35:26.580 --> 01:35:28.900
Chat gpt to help them with coding at work

01:35:29.140 --> 01:35:32.740
They actually now are starting to match the performance of more senior engineers

01:35:33.300 --> 01:35:34.900
Because it sort of levels the playing field

01:35:35.060 --> 01:35:40.980
But that also translates into just like net more productivity per software engineer. And so

01:35:41.940 --> 01:35:44.100
I think that it's more just unlocking sort of

01:35:44.740 --> 01:35:49.940
Existing bottleneck and how productive each individual can be and really just means that each individual can create a lot more value

01:35:50.020 --> 01:35:52.020
Can discover a lot more knowledge

01:35:52.340 --> 01:35:53.540
Than before

01:35:53.540 --> 01:35:57.780
Okay, but I mean do you think that it creates a tendency towards boilerplate though

01:35:57.780 --> 01:36:00.660
So we're more we're more efficient at doing things that exist

01:36:01.060 --> 01:36:04.420
But you know like on on the frontier we might have a slowdown

01:36:04.580 --> 01:36:08.020
There's definitely the danger that it can lock you in to certain patterns

01:36:08.180 --> 01:36:12.500
Right. So basically if chat gpt always returns a certain boilerplate that might have an anti pattern in it

01:36:12.820 --> 01:36:19.700
Um, if that stays around it could self-amplify and then future generations of programmers might just adopt that by default because it's what's already

01:36:20.100 --> 01:36:24.420
Generated by autocomplete. So I think that that's also another really interesting realm of questions

01:36:24.500 --> 01:36:29.940
Which is basically how do you um, how do you avoid these kinds of uh, these local optima?

01:36:30.340 --> 01:36:33.300
When you start to train a model on its own outputs

01:36:33.620 --> 01:36:38.420
And I think again like sort of the solution will start to look like some form of novelty search or exploration

01:36:38.980 --> 01:36:43.860
Makes sense. Okay. Um, what do you guys think about like, um, you know academic academia versus industry and

01:36:44.580 --> 01:36:48.980
Some say there's a bit of a brain drain from academia. Totally. Yeah, I think there's like a very

01:36:49.700 --> 01:36:53.780
Very clear trade-off between the two and they said they both have like fantastic things going for them

01:36:54.260 --> 01:36:57.060
And I guess the trade-off being you know academic freedom

01:36:57.780 --> 01:37:02.260
An academia and be able to like individually pursue ideas like purely for curiosity's sake

01:37:03.140 --> 01:37:06.020
And um, you know, that's something I've really loved about academia

01:37:06.020 --> 01:37:11.620
But I guess you know, I guess the general trend and machine learning research at the moment is kind of towards like larger scale projects

01:37:11.700 --> 01:37:12.660
especially

01:37:12.660 --> 01:37:18.020
You know a lot of the properties that we might want to see kind of only emerge when you expend a lot of compute and therefore

01:37:18.900 --> 01:37:20.820
You know a lot of interesting research can kind of

01:37:22.180 --> 01:37:26.180
Maybe not only be done in an industry, but it's a lot easier to do some kinds of research in industry

01:37:26.660 --> 01:37:29.540
And so I think this kind of leads this trade-off of do you want freedom?

01:37:29.540 --> 01:37:32.580
Or do you want to be on these like larger projects that are potentially more impactful?

01:37:32.980 --> 01:37:35.700
And so yeah, I've really struggled with that trade-off. I think they they both

01:37:36.180 --> 01:37:41.060
Have big pros and cons. I don't know what you think minty. Yeah, I I think that um

01:37:42.180 --> 01:37:49.700
Industry is I I think I like at a very like first word rough approximation would be to say that industry focuses much more on

01:37:50.180 --> 01:37:52.180
um exploitation and

01:37:52.660 --> 01:37:56.500
academia is where you know in principle you should get a lot more exploration

01:37:57.460 --> 01:37:59.460
But I do think that currently

01:37:59.780 --> 01:38:00.660
both

01:38:00.660 --> 01:38:05.460
Systems are kind of like entwined in the same sort of reward function at a high level where essentially

01:38:06.900 --> 01:38:09.700
You know if if if you're if you care a lot about

01:38:10.340 --> 01:38:16.820
Citations and a short-term greedy algorithm for maximizing citations would be to focus your research efforts on

01:38:17.540 --> 01:38:19.540
sort of whatever topic is

01:38:19.860 --> 01:38:22.180
Trendy or hyped at the current time

01:38:22.260 --> 01:38:24.900
And so like I think you see tons of people obviously

01:38:25.380 --> 01:38:28.660
Working on language models partly because it really is a fascinating subject

01:38:28.740 --> 01:38:34.260
And it really is like the most powerful form of deep learning we have so I understand why everyone's working on it

01:38:34.420 --> 01:38:36.260
but I also think that um

01:38:36.260 --> 01:38:40.180
A lot of it is kind of you do get this sort of rich gets richer effect around

01:38:40.580 --> 01:38:47.140
Different topics that people tend to gravitate towards and you lose a lot of the exploration that you should otherwise have

01:38:47.620 --> 01:38:48.340
um

01:38:48.340 --> 01:38:53.860
And that's partly because you know like both industry and academia are at some level optimizing for a similar

01:38:55.460 --> 01:38:58.020
Sort of reputational status or citation count sort of metric

01:38:59.060 --> 01:39:02.740
And so I think that's an issue, but I also think that in some ways

01:39:03.140 --> 01:39:05.620
Uh industry you could say has

01:39:06.340 --> 01:39:11.300
Additional benefit where I do think that from like a short-term point of view industry is better poised to

01:39:12.020 --> 01:39:13.700
make certain

01:39:13.700 --> 01:39:19.220
Higher impact research not just because of the resources available to industry, but also partly because

01:39:19.780 --> 01:39:21.140
um

01:39:21.140 --> 01:39:23.460
Sort of industry, uh, you know

01:39:23.460 --> 01:39:26.660
Rides or dies based on whether the actual research artifact you produce

01:39:27.380 --> 01:39:33.300
Is useful and so I think that's like a very powerful reward function that is not necessarily true for academia

01:39:33.540 --> 01:39:36.340
Um, and then sort of on the to take the counter position

01:39:36.340 --> 01:39:42.020
I think academia obviously, you know, you have a lot more freedom to just explore ideas that don't need to be on that critical path

01:39:42.020 --> 01:39:46.660
For value creation immediately and so it gives you a lot more scope to potentially find like the next big thing

01:39:47.060 --> 01:39:51.540
And so I think really it's about like if you want to if you want to take the bet that you can

01:39:52.180 --> 01:39:54.260
You know play a part in discovering the next big thing

01:39:54.740 --> 01:39:58.660
Then and that's that's suited to your taste for research then academia makes more sense

01:39:59.060 --> 01:40:01.060
but if you know, um, you want to

01:40:01.620 --> 01:40:06.580
You want to maximize the probability you'll have a higher impact in sort of like a near horizon line of work

01:40:06.820 --> 01:40:11.780
Then industry is definitely I think a better bet rich Sutton, you know, he had this bitter lesson essay

01:40:12.100 --> 01:40:17.220
And he made the argument that it's just all computation and there are no shortcuts and you can even think of you know

01:40:17.780 --> 01:40:19.780
Maybe we're not very intelligent

01:40:19.860 --> 01:40:24.820
Evolution has just been running for a very very long time and we are the result of that

01:40:25.060 --> 01:40:30.180
So in in a sense, do you think that we could make strides in intelligence?

01:40:30.580 --> 01:40:34.900
You know just through ingenuity or are we always going to need loads of computer power?

01:40:36.260 --> 01:40:41.220
This definitely like makes me think of like the recent trend that we've been seeing even in like kind of the reinforcement learning

01:40:41.220 --> 01:40:43.780
Literature lately, which is like these kind of large scale

01:40:44.900 --> 01:40:50.740
Like mostly industry projects that are kind of they're even ditching the idea of doing like sequential decision making so

01:40:51.860 --> 01:40:55.700
You know you have all these algorithms that are like, you know optimal planning and so forth

01:40:56.020 --> 01:40:58.020
But we're kind of seeing

01:40:58.020 --> 01:41:03.940
A trend towards you know, even ditching that complexity of algorithm and just going straight to just copy what the human did

01:41:04.260 --> 01:41:06.740
and so kind of reducing the problem to you know

01:41:07.620 --> 01:41:09.620
essentially no real algorithmic

01:41:10.340 --> 01:41:13.220
Innovation and more just like can you gather enough expert data?

01:41:13.700 --> 01:41:16.580
And I think yeah, I guess the reason why that trend is occurring is

01:41:17.140 --> 01:41:19.140
Is I guess like you said there's kind of been

01:41:19.700 --> 01:41:22.020
You know the bit lesson kind of said that you know

01:41:22.420 --> 01:41:26.180
Just being able to scale with more data and more compute is kind of the most important thing

01:41:26.660 --> 01:41:31.540
And a lot of the more complex algorithms, especially around like reinforcement learning are actually like quite challenging to scale up

01:41:32.020 --> 01:41:34.980
especially like online reinforcement learning if you want to go out and like

01:41:35.780 --> 01:41:39.220
Actually have an agent like actively collecting data in a bunch of different environments

01:41:39.620 --> 01:41:43.140
And updating itself online like that's so much like engineering infrastructure to set up

01:41:43.540 --> 01:41:46.900
And so I think there's this this trend towards just like the simplest algorithm possible

01:41:46.900 --> 01:41:51.220
Which is like not even reinforcement learning not even planning just copy an expert

01:41:51.700 --> 01:41:53.460
but I think that

01:41:53.460 --> 01:41:57.460
That's like you kind of said earlier with like this kind of like short-term exploitation

01:41:57.940 --> 01:42:02.340
I think this is you know, it it kind of makes sense to exploit this now and push it as far as possible because

01:42:02.660 --> 01:42:06.900
You know, it's very easy to just train a large transformer and then gather as much data as possible

01:42:07.220 --> 01:42:10.980
And I think in areas like robotics, we haven't really seen like how far can that go like

01:42:11.300 --> 01:42:17.780
Can you actually get a generally useful robotics platform just by gathering more expert demonstrations and training a larger and larger transformer?

01:42:18.100 --> 01:42:22.980
And so I think it does kind of make sense that why like a lot of industry projects are pursuing that because we don't really know

01:42:23.380 --> 01:42:27.380
You know, will will that actually hit a bottleneck or or if you just gather enough data

01:42:27.780 --> 01:42:29.540
Will that will that kind of be sufficient?

01:42:30.100 --> 01:42:31.300
And I guess like

01:42:31.300 --> 01:42:35.460
You know, you could argue that I think it's probably true that there must be a better algorithm out there

01:42:35.700 --> 01:42:37.860
That can and principle do this in a more efficient way

01:42:38.340 --> 01:42:42.100
But I guess if it's just easier to just gather more data and just do imitation learning

01:42:42.180 --> 01:42:44.660
I can see that there's at least a business case for trying that

01:42:45.540 --> 01:42:51.460
So I guess I'm on the the opinion of like, you know, there must be a more efficient way of getting to like a more intelligent system

01:42:51.780 --> 01:42:56.660
But it's not necessarily clear that just scaling like raw supervised learning or unsupervised learning

01:42:56.980 --> 01:43:00.180
Like won't get you there and so it does make sense to pursue that first

01:43:00.420 --> 01:43:05.780
But kind of what I hope and expect to see is that eventually pure imitation learning or pure unsupervised learning will kind of

01:43:06.180 --> 01:43:08.980
Run out of steam and everything will plateau and I think at that point

01:43:09.460 --> 01:43:14.020
You know, then these like more complicated algorithms about gathering more data reinforcement learning planning, etc

01:43:14.020 --> 01:43:15.620
Will really come into their own

01:43:15.620 --> 01:43:18.820
And so I guess this again relates back to like the academia industry trade-off like, you know

01:43:18.820 --> 01:43:22.260
A lot of the projects in the industry are just going to kind of be exploiting gathering data right now

01:43:22.740 --> 01:43:25.140
Whereas maybe there's a lot of scope to do these kind of more

01:43:25.620 --> 01:43:30.660
Exploratory exploratory projects where maybe that will get you to like the next frontier a few years down the line

01:43:31.140 --> 01:43:32.740
I don't know what you think about this

01:43:32.740 --> 01:43:34.820
Yeah, I definitely think that um

01:43:34.820 --> 01:43:39.460
Yeah, just like treating everything as just supervised learning it does tend to work because we have large data sets

01:43:39.540 --> 01:43:43.940
But um, I think again like the challenge is just at some point we will run out of tokens

01:43:43.940 --> 01:43:50.420
We'll run out of data to train on and so that's why the self-improving more self exploratory systems will be more and more

01:43:50.740 --> 01:43:53.460
I think paramount to like driving performance even further

01:43:53.620 --> 01:43:57.780
So if we want to sort of break beyond sort of the token limit of like the data that's available now

01:43:58.020 --> 01:44:01.060
We actually need these systems to generate their own tokens their own synthetic data

01:44:01.700 --> 01:44:06.420
And that's that's where like the self play auto curricula exploration types of algorithms will start to

01:44:07.620 --> 01:44:11.860
Become more and more prominent and obviously you need an environment in which to do that exploration

01:44:12.180 --> 01:44:14.180
And that's where the world model

01:44:14.580 --> 01:44:20.020
Line of research is going to be very powerful just because that allows you to really sort of milk all of the value within

01:44:20.580 --> 01:44:27.060
The existing previous data you have seen by creating these role models where you might be able to do like counterfactual trajectories and really learn much more

01:44:27.460 --> 01:44:29.460
Um, amplify the existing data you had

01:44:30.020 --> 01:44:35.460
Yeah, I mean, I think one of one of the key things for me, um is modeling dynamics. So, um

01:44:36.340 --> 01:44:40.420
It's quite interesting actually with the human knowledge things or even looking at the innovations from from deep mind

01:44:40.420 --> 01:44:43.540
You know early versions of alpha go were bootstrapped with human knowledge

01:44:43.780 --> 01:44:46.260
And then there was the alpha zero. So it was actually doing what we're talking about

01:44:46.260 --> 01:44:50.340
It was actually discovering knowledge on its own and um in principle. That's a great idea

01:44:50.420 --> 01:44:56.420
But of course like an irrestricted domain, it's tractable but in the real world it isn't and I'm not sure whether it makes sense to use

01:44:56.500 --> 01:45:02.260
The the computation and you know information metaphor for the real world and humans and so on but but the basic idea is that

01:45:02.420 --> 01:45:05.220
We are all real agents the universe is a massive computer

01:45:05.540 --> 01:45:10.340
We're discovering all of this knowledge and then we're bootstrapping that into a machine learning algorithm

01:45:10.660 --> 01:45:16.180
And then the question is well, if you kind of just capture the thing now without the dynamics that produced it

01:45:16.420 --> 01:45:20.180
Um, will the system be robust and could you still um, you know, kind of

01:45:20.260 --> 01:45:24.660
Carry on as we were in the real world if that makes sense. So um, but yeah

01:45:24.660 --> 01:45:27.860
The interesting thing with the work you've done is is that you are modeling agential systems

01:45:27.860 --> 01:45:34.500
You are modeling dynamics, but could that be used for you know, much more complex tasks like the real world

01:45:35.140 --> 01:45:40.260
Like simulating much more complex systems in the real world. Yeah, I think that if you if you

01:45:41.380 --> 01:45:46.020
So I think that just purely imitation learning alone is not really going to get you there

01:45:46.900 --> 01:45:50.260
But I think that if you can if you can imitate

01:45:51.380 --> 01:45:54.020
So one is sort of finding the set of tasks

01:45:54.100 --> 01:45:58.660
I think that if you find the set of tasks or reward functions that could be relevant

01:45:58.820 --> 01:46:04.900
Then you can start to simulate things that are otherwise really hard to capture by just purely imitating historical trajectories

01:46:05.380 --> 01:46:12.740
So for example strategic adaptation type of behaviors are really hard because those are sort of an open-ended space of behaviors where

01:46:13.460 --> 01:46:16.740
If you basically have like a stock market, for example, that's a really good example

01:46:17.060 --> 01:46:19.620
Where if you have a stock market, that's a very open-ended system

01:46:19.780 --> 01:46:23.380
And like different traders will have different strategies that are best responses to each other

01:46:23.620 --> 01:46:27.300
And then over time the set of strategies evolves over time in an open-ended way

01:46:27.620 --> 01:46:33.540
Um, you know trading strategies that worked 10 years ago probably won't work very well today because people have sort of um

01:46:34.340 --> 01:46:39.700
They've sort of figured out those strategies. And so they won't be very competitive. And so um

01:46:40.340 --> 01:46:43.460
I don't see an an imitation learning system being able to sort of

01:46:44.100 --> 01:46:48.740
Um, generalize to that level of complexity just because by definition it's imitating previous

01:46:49.220 --> 01:46:55.060
Uh trajectories and therefore strategies. So I think you need some notion of like a um a more uh

01:46:55.540 --> 01:46:59.300
More interactive trial and error learning that allows for strategic adaptation

01:46:59.380 --> 01:47:05.460
And that requires some notion of a payoff or a reward. And so you kind of need to have this this idea of um

01:47:06.900 --> 01:47:08.900
You you can't just purely I think learn

01:47:08.980 --> 01:47:10.180
Uh

01:47:10.180 --> 01:47:13.060
A model of something like the stock market just based on previous data

01:47:13.060 --> 01:47:17.060
You really need to have more inductive biases around uh, sort of you know

01:47:17.380 --> 01:47:23.620
What creates a payoff or what the actual reward function is for each of the traders? Uh, but that might be something that you could

01:47:24.100 --> 01:47:25.300
um

01:47:25.300 --> 01:47:27.300
You could learn over time, but I

01:47:27.700 --> 01:47:31.940
But maybe not in the yeah, so this is kind of like this is not very coherent

01:47:31.940 --> 01:47:36.260
But I feel like uh, you might need something that looks more like learning over space of programs

01:47:36.500 --> 01:47:39.860
That starts to encompass different kinds of uh tasks

01:47:40.020 --> 01:47:44.660
And then you can basically simulate those tasks to completion with agents that can essentially

01:47:45.060 --> 01:47:47.140
Uh try to self-improve against other agents

01:47:47.860 --> 01:47:53.700
The stock market I think is a wonderful metaphor for we're talking about and for for two reasons first of all from the grounding reason

01:47:53.940 --> 01:47:56.980
Because you know like the the the the the memetic world is very ungrounded

01:47:57.060 --> 01:48:01.460
And that's why we develop as humans lots of weird shared delusions about things because it's actually like you know

01:48:01.700 --> 01:48:03.780
It can go in it can go in almost any direction

01:48:04.020 --> 01:48:09.300
And also the concept of alpha I think is really important because a trading strategy works really well today

01:48:09.620 --> 01:48:14.740
And then when other people learn about it it no longer provides an advantage because everyone else knows about it

01:48:14.900 --> 01:48:19.540
And I feel it's the same with language models. So, you know, like gbt4 pros was really novel and cool

01:48:19.860 --> 01:48:23.300
It was great to you know, have like a ted talk speech when it came out

01:48:23.620 --> 01:48:26.500
And now it doesn't seem cool anymore because everyone's using it on linkedin

01:48:26.740 --> 01:48:30.340
So it's almost like that we need to have this in like continuous

01:48:30.900 --> 01:48:34.420
creative evolving process producing new sources of alpha

01:48:34.820 --> 01:48:40.180
And the paradox is that if everyone has access to the same model it can't be a source of alpha by definition

01:48:40.340 --> 01:48:44.500
Yeah, I guess on that like topic because we kind of talked about like synthetic data earlier

01:48:44.580 --> 01:48:50.100
And you kind of said like you know one one mechanism towards getting like a kind of self-improving system that is able to kind of

01:48:50.260 --> 01:48:51.060
You know

01:48:51.060 --> 01:48:54.260
Continue to improve is to kind of like filter the synthetic data for example

01:48:54.340 --> 01:48:58.020
So you might kind of you know have the the new system and then we generate some more data

01:48:58.020 --> 01:49:01.940
And then we kind of have some like filtering mechanism to say that you know in the current stock market

01:49:01.940 --> 01:49:05.540
This is this is good data or what you know, whatever system we're thinking about and then we can kind of like

01:49:06.340 --> 01:49:08.340
Use that to enable the model to improve

01:49:08.980 --> 01:49:10.980
You know and adapt to the new system

01:49:11.060 --> 01:49:14.020
But something I've always like why like thought about is like well

01:49:14.020 --> 01:49:18.660
I guess one is is it really trivial to be able to like filter that you know new synthetic data

01:49:19.140 --> 01:49:24.340
And then two it feels like if you're just relying on like filtering existing synthetic data

01:49:24.900 --> 01:49:29.220
Like isn't that a never to go into kind of plateau and so I guess eventually

01:49:29.220 --> 01:49:34.260
You know we talked about how you kind of said that you do actually actively not need to go out and get real more real data

01:49:34.580 --> 01:49:39.460
But I guess I'm kind of asking you do you think this idea of just like filtering synthetic data from a model is kind of

01:49:40.100 --> 01:49:44.900
Sufficient to always be able to adapt and improve or is it always going to be a mixture of like more real data

01:49:45.300 --> 01:49:50.020
Plus synthetic data filtering. I think it's the latter just because um at some point you would expect that

01:49:50.820 --> 01:49:55.620
The synthetic data you do generate it'll start to sort of saturate like what's already in the model

01:49:56.180 --> 01:50:01.220
Just because the model is trained on a finite amount of information. So at some point you're just going to start to see more and more

01:50:02.340 --> 01:50:06.900
Especially like the more likely trajectories or sequences of samples. You'll start to see that

01:50:07.620 --> 01:50:12.820
More and more and so you're not really going to be very sample efficient in terms of searching for the synthetic data

01:50:13.540 --> 01:50:18.900
So can you can you tell us about the results of the paper? Totally. Yeah, so basically we evaluate this um

01:50:19.620 --> 01:50:24.660
This algorithm on a bunch of like synthetic simulated domains kind of like robotics related tasks

01:50:25.140 --> 01:50:28.340
Um and kind of yet environments where there's like varying levels of complexity

01:50:28.420 --> 01:50:32.260
So, you know, you might have a robot pushing around a variable number of like objects

01:50:32.340 --> 01:50:35.460
Or maybe you have different terrain that the robot might want to um

01:50:36.580 --> 01:50:39.540
Learn to kind of you know do locomotion over and things like this

01:50:40.180 --> 01:50:45.860
Um and so kind of you know, the main comparison we make is like how well does waker work relative to like naive domain randomization

01:50:45.860 --> 01:50:51.620
So how well does it work if you just like uniformly sample the space of environments versus if you do actively seek out

01:50:51.940 --> 01:50:54.260
The environments that have this like higher uncertainty

01:50:54.980 --> 01:50:58.740
Um and so basically what we show is that you know, if we do the waker approach

01:50:59.060 --> 01:51:03.540
We still do like very well on average, but we consistently do better in terms of robustness

01:51:03.940 --> 01:51:08.420
And so robustness by robustness. I mean here that that we do better in terms of the worst environment

01:51:08.980 --> 01:51:11.780
That the agent is evaluated under and so this kind of means, you know

01:51:11.780 --> 01:51:16.260
If the agent is able to do well in the worst environments that it that it is evaluated under

01:51:16.420 --> 01:51:19.300
That kind of shows that it's able to do well across all environments because

01:51:19.700 --> 01:51:24.900
Its worst performance is still good. Um, so we kind of this this shows that we we achieve this like robustness property

01:51:25.460 --> 01:51:27.460
Which we talked about in terms of like mini max regret

01:51:28.020 --> 01:51:33.380
But we evaluate we don't evaluate it in terms of like the true notion of mini max regret because as we talked about earlier

01:51:33.700 --> 01:51:39.380
Actually evaluating regret exactly as difficult because that that requires knowing the exact true optimal performance

01:51:39.940 --> 01:51:42.980
Which isn't something we can really know. So instead we we just show that you know

01:51:43.060 --> 01:51:48.980
The agent performs well across all environments more so than if you just like naively sampled the environments uniformly

01:51:49.780 --> 01:51:54.580
And in terms of decomposing the performance across the spectrum of possible environments

01:51:54.580 --> 01:51:59.540
So like, you know the ideal situation is that we have a very simple model which just generalizes

01:51:59.540 --> 01:52:04.980
So we happen to have found the golden motif, you know, there's a spectrum of correlations almost all of them are spurious

01:52:05.060 --> 01:52:08.420
But we've just you know, just by through some sheer magic

01:52:08.660 --> 01:52:12.340
We found the best motif to work in all situations. Probably that's not quite true

01:52:12.420 --> 01:52:17.460
Probably there are some good generalizing motifs and the model has also kind of like memorized the long tail

01:52:17.940 --> 01:52:22.980
And that there's some degree of like, you know, it works really well on on the test set that might not out of domain distribution

01:52:23.140 --> 01:52:26.100
Do you have any like way of reasoning about what that is?

01:52:26.980 --> 01:52:28.260
um

01:52:28.260 --> 01:52:29.540
so

01:52:29.540 --> 01:52:33.620
Yeah, I agree. I guess there's like not necessarily it's not necessarily the case that by like

01:52:34.100 --> 01:52:36.420
Focusing more on these like long tail examples

01:52:36.500 --> 01:52:41.620
That's necessarily the best way of training the best model because like you said like maybe it happens to be the case that

01:52:42.020 --> 01:52:46.020
If the model is trained on some certain subset of the tasks like that will actually generalize better

01:52:46.100 --> 01:52:48.820
but but I think in practice, that's not something we can really um

01:52:49.780 --> 01:52:55.220
Really know how to you know, like optimally select the best kind of set of tasks that will generalize well

01:52:55.540 --> 01:52:59.060
And so we do focus more on on like, you know, these these kind of long tail tasks

01:52:59.060 --> 01:53:02.260
Or like the ones that we might see rarely and therefore have high uncertainty about

01:53:02.980 --> 01:53:03.780
um

01:53:03.780 --> 01:53:06.500
In terms of like the the out of distribution generalization

01:53:06.580 --> 01:53:10.980
So so we do also do some experiments like looking at how well does the model generalize out of distribution?

01:53:11.860 --> 01:53:14.900
And basically what we show is that if we train the model in this way

01:53:15.220 --> 01:53:18.420
And then we give it some more environments that hasn't seen at test time

01:53:18.900 --> 01:53:23.300
Um, if the environments are more complex then then we've seen sorry hasn't seen at training time

01:53:24.180 --> 01:53:28.500
Basically like this model then generalizes better to out of distribution environments that are like more complex

01:53:28.500 --> 01:53:32.260
Which is kind of what you'd expect because we've kind of biased something towards more complexity

01:53:32.420 --> 01:53:35.540
We're able to generalize better to out of distribution environments that have higher complexity

01:53:35.940 --> 01:53:40.420
And then I guess the question is like do we care about out of distribution environments that have higher complexity?

01:53:40.420 --> 01:53:42.900
Like what about the out of distribution environments that have lower complexity?

01:53:43.540 --> 01:53:45.540
and I would argue that you know

01:53:45.940 --> 01:53:50.180
Basically the lower environment out of distribution distribution environments that have lower complexity

01:53:50.180 --> 01:53:55.540
Like we would already expect that the model is able to do very well at so so there's not really much of a difference there because you know

01:53:56.020 --> 01:53:59.300
Almost any reasonably trained model can handle the very simplest environment

01:53:59.460 --> 01:54:03.780
So what we really care about is can we generalize out of distribution to like higher complexity environments?

01:54:04.180 --> 01:54:07.140
And so by biasing the something towards the higher complexity environments

01:54:07.140 --> 01:54:11.060
We do show that we're able to generalize further out of distribution to even higher complexity environments

01:54:11.300 --> 01:54:14.420
Okay, but is there any way of knowing whether it's kind of like

01:54:14.820 --> 01:54:19.860
Memorizing the high complexity instances or whether it's still learning abstract motifs and generalizing between them

01:54:20.180 --> 01:54:26.260
Yeah, that's a great question. I think that's a really interesting question generally for ml as a field right now

01:54:26.260 --> 01:54:28.260
Which is better evaluation benchmarks for

01:54:28.900 --> 01:54:31.460
Generalization within different kinds of models

01:54:31.780 --> 01:54:37.940
Um, and like we we alluded to earlier. There's kind of this issue of data leakage between training and test set

01:54:38.100 --> 01:54:43.540
Which is um, which is definitely an issue that is currently happening with large language models

01:54:43.940 --> 01:54:48.820
Um, it doesn't take away from the impressiveness of these models because clearly there is a strong generalization

01:54:49.140 --> 01:54:54.100
aspect to their behavior, but I do think that in terms of measuring performance on specific benchmarks

01:54:54.420 --> 01:54:57.940
Um, we really need to solve this problem. How do we have these clean data sets?

01:54:58.500 --> 01:55:00.260
That allow us to

01:55:00.260 --> 01:55:05.620
To truly test on inputs that the model hasn't seen at training. Um, I think in the case of

01:55:06.420 --> 01:55:08.420
reinforcement learning

01:55:08.420 --> 01:55:12.500
That's a bit more difficult just because usually we focus on a particular task domain

01:55:12.660 --> 01:55:15.060
And so there's always going to be some shared similarities within the task

01:55:15.060 --> 01:55:20.980
But obviously, uh, we didn't do this in this paper, but we could try things where we have more um more controlled

01:55:21.620 --> 01:55:27.940
Settings where we you know change one aspect of the environment and really see if it's learning specific causal relationships between

01:55:28.580 --> 01:55:30.580
Things that have to be accomplished in that task

01:55:30.900 --> 01:55:34.820
But we didn't do that. Um, that I actually think would be a really interesting idea for

01:55:35.460 --> 01:55:37.460
A new evaluation environment for rl

01:55:37.940 --> 01:55:41.460
Yeah, I mean the benchmarks thing is just a huge challenge in in machine learning

01:55:41.940 --> 01:55:45.220
In general, but just just to kind of round off off the interview

01:55:45.220 --> 01:55:49.140
I mean minchie you you were talking about you're doing some work with um edgreff instead and it is amazing

01:55:49.140 --> 01:55:54.260
I'm getting edg back on and um, you said that um, you've been looking into this kind of the interface

01:55:54.580 --> 01:55:56.740
Between humans and machine learning. Can you tell me about that?

01:55:56.980 --> 01:56:02.100
Yeah, so just to not say too much about it because um, it's related to current work that's happening at DeepMind

01:56:02.500 --> 01:56:06.340
Um is just that you know, I think from personally from a high level point of view

01:56:06.660 --> 01:56:10.660
I'm very interested, you know talking about this divide sort of this fork in the road in terms of

01:56:10.980 --> 01:56:16.100
What's the path to open studying open-endedness studying it in silico or studying it in

01:56:16.660 --> 01:56:20.580
situ in the setting of an actual open-ended system like a user

01:56:21.460 --> 01:56:26.500
App interaction or you know the interaction between a user and a piece of software on the web

01:56:27.380 --> 01:56:30.100
Or potentially with many other users. There are such rich

01:56:30.900 --> 01:56:37.540
Existing systems online that are already open-ended because they amplify or connect the creativity and knowledge of humans

01:56:38.180 --> 01:56:44.180
To create more knowledge and more creative artifacts. And so I think what's really uh, interesting in my mind now is sort of studying

01:56:44.900 --> 01:56:49.060
Systems or algorithms that allow us to better steer the creativity of humans

01:56:49.700 --> 01:56:51.700
As they are mediated by software

01:56:52.420 --> 01:56:55.380
And basically allow us to essentially amplify

01:56:55.940 --> 01:57:03.060
Existing intelligent or creative systems that are open-ended so amplify existing open open-endedness rather than try to build it from scratch

01:57:04.020 --> 01:57:08.580
Amazing guys. It's been an honor to have you on MLS T. Thank you so much. Thanks. Thank you. Yeah

01:57:09.380 --> 01:57:11.380
Great cool. Yeah, we're done

