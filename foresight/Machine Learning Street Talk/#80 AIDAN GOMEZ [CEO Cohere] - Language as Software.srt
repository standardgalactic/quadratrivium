1
00:00:00,000 --> 00:00:07,440
Aiden Gomez is a computer scientist and widely recognized AI expert who co founded AI company co here before that.

2
00:00:07,440 --> 00:00:14,480
He worked as an intern in the Google brain team in Toronto, alongside Jeffrey Hinton, one of the Godfathers of deep learning.

3
00:00:14,480 --> 00:00:17,880
Actually, the only Godfather who hasn't been on MLST.

4
00:00:18,000 --> 00:00:19,960
Aiden, you need to put a good word in for us.

5
00:00:21,360 --> 00:00:23,360
I will bring it up.

6
00:00:24,280 --> 00:00:28,880
So Gomez was the kind of person Hinton recalled who had so many ideas.

7
00:00:29,160 --> 00:00:33,920
It was difficult to pin him down and to get him to focus on what he was actually supposed to be doing.

8
00:00:34,520 --> 00:00:37,760
Now Gomez was particularly interested in learning to translate languages.

9
00:00:38,120 --> 00:00:50,360
And while he was at it, he casually invented transformers, which, as we all know, have become the de facto basic reference architecture for all neural networks and not only for language tasks.

10
00:00:51,120 --> 00:00:58,200
Now co here is a startup which uses artificial intelligence to help users build the next generation of language based applications.

11
00:00:58,640 --> 00:01:05,480
It's headquartered in Toronto and the company has raised $175 million in funding so far.

12
00:01:05,880 --> 00:01:15,920
And the first round was from index ventures and the round also included Hinton, who we just spoke about, Fei-Fei Li and Peter Abial, who are very, very famous folks in the ML space.

13
00:01:16,680 --> 00:01:28,160
Now co here say that their competitive advantage or at least at least one of their competitive advantages lies in its focus on safety, which is crucial for customers deploying models which could potentially out for something harmful.

14
00:01:28,520 --> 00:01:30,600
And we'll discuss what we mean by that a bit later on.

15
00:01:31,240 --> 00:01:37,080
Now, I can honestly say personally that language models have transformed how I use computers over the last six months or so.

16
00:01:37,080 --> 00:01:53,760
I've been using them on a daily basis in the form of scripts, which helped me use the command line better using an interactive REPL playground for doing all sorts of stuff like composing emails for me, understanding error logs when I'm coding, summarizing information, performing repetitive tasks.

17
00:01:53,760 --> 00:01:54,320
Do you name it?

18
00:01:54,600 --> 00:02:00,640
Actually, I regularly discover new tasks that I can use for language models just through kind of creative exploration.

19
00:02:00,640 --> 00:02:05,440
So I'm a bit of a convert, although caveat M tour is still definitely in effect.

20
00:02:05,760 --> 00:02:09,920
The words of Chomsky, Fodor and Felician are still ringing loudly in my ears.

21
00:02:09,920 --> 00:02:12,040
We just did some content about that on our last show.

22
00:02:12,040 --> 00:02:13,880
But anyway, Aidan, it's an absolute honor.

23
00:02:13,880 --> 00:02:15,120
Welcome to MLST.

24
00:02:15,400 --> 00:02:19,440
And what's it like being the CEO of one of the fastest growing startups?

25
00:02:21,760 --> 00:02:22,920
Thank you so much for having me.

26
00:02:23,160 --> 00:02:25,800
I'm stoked to get to meet you and chat about Coheir.

27
00:02:27,200 --> 00:02:29,520
What's it like being the CEO of Coheir?

28
00:02:29,520 --> 00:02:35,440
It's a privilege and a thrilling ride.

29
00:02:35,560 --> 00:02:39,960
Like you're just hanging on for dear life as things are going and trying to keep up.

30
00:02:41,000 --> 00:02:47,400
But it's honestly just such a privilege to get to work with the people that I work with on the problem that we work on.

31
00:02:47,800 --> 00:02:52,640
I think I'm just so, so lucky and fortunate.

32
00:02:53,040 --> 00:02:58,000
I'm really, really excited to get into the discussion about transformers and attention is all you need.

33
00:02:58,000 --> 00:03:04,960
So I mean, I remember when this came out in about 2017, actually, it changed the landscape of deep learning forever.

34
00:03:05,320 --> 00:03:07,840
It took me a very long time to understand it.

35
00:03:07,840 --> 00:03:12,960
And I probably only did understand it after reading Jay's famous blog post, you know, The Illustrator Transformer.

36
00:03:13,200 --> 00:03:15,600
Jay, of course, is he your engineering director now?

37
00:03:16,480 --> 00:03:17,200
Yeah, yeah.

38
00:03:17,200 --> 00:03:22,680
So Jay is the best communicator in machine learning that I know.

39
00:03:22,840 --> 00:03:23,680
He's incredible.

40
00:03:24,960 --> 00:03:32,120
And I think if you search, I think if you search Transformer NeuralNet on Google, it's not our paper that comes up.

41
00:03:32,120 --> 00:03:33,800
It's Jay's blog.

42
00:03:34,240 --> 00:03:38,760
So it tells you like how much better his communication is than ours.

43
00:03:39,640 --> 00:03:40,640
It was an epic blog.

44
00:03:40,640 --> 00:03:44,480
He's actually done a new blog post on stable diffusion as well.

45
00:03:44,880 --> 00:03:47,760
And folks should also subscribe to his YouTube channel.

46
00:03:47,760 --> 00:03:49,000
He's got an amazing YouTube channel.

47
00:03:49,000 --> 00:03:58,720
But anyway, there was Jay's blog post and Janik's video that made me finally understand it because at the time it was the most exciting kind of technology.

48
00:03:59,080 --> 00:04:06,800
And I remember I was interviewing for the Bing team at the time and I spent a long time studying the paper just so that I could be conversant in it when I had that interview.

49
00:04:06,800 --> 00:04:12,160
But anyway, could you tell us, you know, what was the story behind the paper and also what's special about transformers?

50
00:04:12,920 --> 00:04:31,480
Yeah, so the story behind the paper, I should say that, like, I was the intern on this project and so I was the baby, like, doe-eyed, showed up in Google Brain down in Mountain View and my contribution was really on the software.

51
00:04:31,480 --> 00:04:59,120
So there were existing threads inside of Google Brain, primarily driven by Noam Shazir, Jakob Uskarite, Lukash Ashish, which were pushing along this idea of deploying these autoregressive sequence models for text because they had been really popularized in WaveNet for audio and language generation.

52
00:05:01,600 --> 00:05:12,640
But there was like a big push towards how do we deploy these against text and how do we incorporate attention, which was something that was groundbreaking for RNNs, like the previous generation of model.

53
00:05:13,040 --> 00:05:16,960
They wanted to incorporate this into these new autoregressive sequence models.

54
00:05:17,640 --> 00:05:31,280
And so when I came in, I was working with Lukash focused on, like, the software side of things, scalable training frameworks, supporting training that, you know, could be distributed across not just tens of

55
00:05:31,280 --> 00:05:35,000
machines or hundreds, but thousands of accelerators.

56
00:05:37,000 --> 00:05:43,040
And so we built Tensor2Tensor, which was the framework that was used to develop the transformer.

57
00:05:45,240 --> 00:05:58,040
Shortly after we started putting that together, I was sitting next to Noam Shazir and we heard that he was also kind of thinking along similar threads for these autoregressive models.

58
00:05:58,040 --> 00:06:03,600
And so Lukash and I convinced him to come over to Tensor2Tensor and start doing it on our framework.

59
00:06:04,600 --> 00:06:07,160
And then we heard over in the translate department.

60
00:06:07,160 --> 00:06:09,200
So Brain was one division of Google.

61
00:06:09,560 --> 00:06:10,840
Translate was a separate one.

62
00:06:11,440 --> 00:06:27,680
Over in translate, there was Yakka, Bashish, Nikki, working on a similar project to Noam where they wanted to create a purely attentive model, strip back all the complexity of RNNs, all of these, like, very complex,

63
00:06:27,680 --> 00:06:37,640
gates and states and et cetera, and just rip that all away and just have MLPs and attention layers.

64
00:06:40,200 --> 00:06:41,360
And so we all came together.

65
00:06:41,360 --> 00:06:45,280
We all consolidated on this framework called Tensor2Tensor.

66
00:06:45,880 --> 00:06:57,640
And the next three months, up until the NERP deadline, was just a sprint, like sleeping in the office, just going as fast as you can, running experiments.

67
00:06:57,680 --> 00:07:00,640
And iteration and iteration and finding this bug and that bug.

68
00:07:02,840 --> 00:07:07,960
And so really like a huge piece of the project came together within 12 weeks.

69
00:07:10,440 --> 00:07:14,800
So it was like an extraordinary pace.

70
00:07:17,240 --> 00:07:24,320
And it was, as an intern, this was like my first experience in like proper research.

71
00:07:24,800 --> 00:07:26,080
And I just thought this was normal.

72
00:07:26,080 --> 00:07:28,720
I thought, OK, this is what everyone does.

73
00:07:28,720 --> 00:07:30,880
We all just crank out papers in three months.

74
00:07:30,880 --> 00:07:32,000
We sleep at the office.

75
00:07:34,920 --> 00:07:39,680
And I didn't really have an appreciation for what we had accomplished at the end.

76
00:07:39,720 --> 00:07:44,480
Like, I can't say that I was particularly prescient at the time.

77
00:07:44,920 --> 00:07:49,920
I remember like the night before the NERP deadline, the night before we had to submit.

78
00:07:50,760 --> 00:08:05,840
It was like whatever, 3 a.m. Ashish and I were sitting on a couch in the brain office and he turned to me and he he said, you know, like, Aiden, like, this is going to be huge.

79
00:08:06,600 --> 00:08:11,760
And my reaction was like, we bumped up blue, like the blue score by one point.

80
00:08:12,000 --> 00:08:13,040
I was like, really?

81
00:08:13,040 --> 00:08:13,680
You think so?

82
00:08:13,840 --> 00:08:14,400
Oh, cool.

83
00:08:14,400 --> 00:08:16,600
Like, don't we all like, isn't this what research is?

84
00:08:16,600 --> 00:08:17,640
We just bumped it up a point.

85
00:08:17,800 --> 00:08:18,560
What's the big deal?

86
00:08:19,400 --> 00:08:32,080
But I think what I didn't appreciate was the fact that such a simple method could achieve such insanely high performance.

87
00:08:32,960 --> 00:08:38,560
Like, at that time, we were training on eight accelerators for one of these models and they could be trained in within a day.

88
00:08:41,560 --> 00:08:46,720
But the architecture was so stripped down, it was so refined, it was so easy to scale and to grow.

89
00:08:47,120 --> 00:08:50,080
I just, it was hard to see the future.

90
00:08:51,280 --> 00:08:55,400
And it was hard to see what would happen, which was this massive scaling project.

91
00:08:55,840 --> 00:08:59,520
And I think Ashish saw that, that happening.

92
00:09:01,680 --> 00:09:03,080
So yeah, that was my contribution.

93
00:09:03,080 --> 00:09:06,160
That was the part of the team that I brought.

94
00:09:08,040 --> 00:09:11,320
But yeah, it was a very, very exciting few months.

95
00:09:11,320 --> 00:09:16,920
And then it was an extremely exciting few years afterwards before I left Google and started cohere.

96
00:09:17,280 --> 00:09:18,000
I can imagine.

97
00:09:18,040 --> 00:09:20,840
And as you say, some of the best work happens in sprints.

98
00:09:20,840 --> 00:09:30,920
And even though it only moved the needle a tiny bit on the blur score, something we'll explore later is sometimes it might actually be performing better than the metric might suggest.

99
00:09:31,160 --> 00:09:33,240
And simple models are often better.

100
00:09:33,280 --> 00:09:35,800
But I wanted to ask a bit of a technical question.

101
00:09:35,800 --> 00:09:40,920
So DeepMind recently released a paper called Neural Networks and the Chomsky Hierarchy.

102
00:09:41,680 --> 00:09:46,720
They grouped a bunch of tasks according to the formal language classes in the Chomsky Hierarchy.

103
00:09:47,160 --> 00:09:53,920
And they showed that transformers were only able to represent finite languages, so not not even regular languages, which RNNs could do.

104
00:09:54,640 --> 00:10:00,640
Neural networks could only support the higher modes, like context-free languages, if they were memory augmented with a stack.

105
00:10:01,200 --> 00:10:03,880
And they said that this had implication for scaling laws.

106
00:10:04,640 --> 00:10:10,320
So for example, the transformers architecture, they said, could never perfectly learn tasks which were higher up in the Hierarchy.

107
00:10:10,640 --> 00:10:18,080
So we've done some shows on MLSD where we've spoken about neural networks not being too incomplete because they are finite state automators without the augmented memory.

108
00:10:18,080 --> 00:10:21,360
So, for example, they couldn't approximate pi to the nth digit.

109
00:10:21,720 --> 00:10:26,240
And there's this photo and collision connectionism critique paper, which we spoke about on the last show.

110
00:10:26,280 --> 00:10:33,640
And that basically means that neural networks are not able to perform symbolic compositional reasoning or in plain English, they can't represent infinite objects.

111
00:10:34,040 --> 00:10:43,880
And we spoke to Randall Belastriereo and he had this paper called The Spline Theory of Neural Networks, which showed that a neural network is a linear operation given a single input example.

112
00:10:43,880 --> 00:10:54,840
So given all of this, I personally find the conversation about scaling laws, I'm a bit skeptical about it, although I'm genuinely interested in emergent properties or transients that happen during the scaling process.

113
00:10:54,840 --> 00:11:01,760
But given all of this, I mean, how do you think about the practical limitations of transformers in terms of what algorithms they can learn?

114
00:11:03,720 --> 00:11:08,200
Yeah, I really hope the transformers aren't the last architecture.

115
00:11:10,400 --> 00:11:16,480
I would be extremely disappointed if this is as creative and high performing as we can get.

116
00:11:16,840 --> 00:11:22,480
I think that transformers took off because of their scaling properties and also because of a network effect.

117
00:11:23,040 --> 00:11:31,640
The community consolidated around this one architecture, we started to build all of this infrastructure, specifically for transformers.

118
00:11:32,080 --> 00:11:35,080
There was a network effect and it had very nice scaling properties.

119
00:11:35,080 --> 00:11:42,360
And so the community really came together around this architecture and built up infrastructure to support its adoption.

120
00:11:44,080 --> 00:11:48,080
I think that's what's led to their proliferation or their success.

121
00:11:50,720 --> 00:11:56,440
I hope that it's not the last architecture that would be super, super disappointing and boring.

122
00:11:57,160 --> 00:12:02,160
You point out that they're not Turing complete.

123
00:12:02,160 --> 00:12:10,880
I should clarify that I'm not a linguist, I'm not too familiar with Trump's hierarchies or the implications of the deep mind paper.

124
00:12:10,880 --> 00:12:14,920
But one thing that's interesting about it, when I read it,

125
00:12:17,480 --> 00:12:24,440
they're not speaking in theoretical terms or speaking in empirical terms of what functions are achievable.

126
00:12:25,440 --> 00:12:28,440
From a normal initialization.

127
00:12:31,440 --> 00:12:33,440
I think that's a fascinating lens.

128
00:12:33,440 --> 00:12:40,440
In theory, a transformer is a universal approximator and it might be even Turing complete.

129
00:12:40,440 --> 00:12:46,440
But in practice, if you can't explore all permutations of parameters,

130
00:12:46,440 --> 00:12:52,440
it's very true that it'll find the simplest function that satisfies the task.

131
00:12:55,440 --> 00:13:00,440
I think that's like a good guiding lens when thinking about what architectures come next.

132
00:13:00,440 --> 00:13:02,440
Where do we go from here?

133
00:13:02,440 --> 00:13:12,440
What are the sorts of components we need to add into neural networks to support them in representing these more complex functions?

134
00:13:15,440 --> 00:13:22,440
So I do think that transformers are limited and I really hope they're not our final architecture.

135
00:13:22,440 --> 00:13:30,440
I hope that we come up with something that's significantly better and I see promising efforts along those directions.

136
00:13:30,440 --> 00:13:38,440
I think that retro from deep mind, like augmenting transformers with a searchable memory, I think that's a huge step forward.

137
00:13:38,440 --> 00:13:44,440
The next thing we need to support is the ability for these transformers to keep state over long time horizons,

138
00:13:44,440 --> 00:13:51,440
to be able to write into their own memory in order to make notes about what they've seen in the past.

139
00:13:53,440 --> 00:13:58,440
And so I think there's a lot of work to be done and it's not happening fast enough.

140
00:13:58,440 --> 00:14:05,440
I think more people should pick up these research questions and look for new scalable ways of doing it.

141
00:14:05,440 --> 00:14:13,440
Because to speak to the scaling hypothesis, the real bottleneck for feature adoption,

142
00:14:13,440 --> 00:14:19,440
the real bottleneck for adding in a new component to the transformer architecture,

143
00:14:19,440 --> 00:14:27,440
it's all scale and efficiency. People will just adopt the thing that is simplest, fastest and best performing.

144
00:14:27,440 --> 00:14:35,440
And so we need to do the work in order to make these other components like addressable memory efficient and scalable

145
00:14:35,440 --> 00:14:40,440
in the same way that the core vanilla transformer architecture is today.

146
00:14:40,440 --> 00:14:49,440
Yeah, it's such a paradox, isn't it? Because the deep mind paper was saying that a Turing machine is the most powerful computational model,

147
00:14:49,440 --> 00:14:53,440
but neural networks are trainable and they scale really, really well.

148
00:14:53,440 --> 00:14:58,440
So I'm sure we'll find some innovation that will somehow bridge that gap somehow.

149
00:14:58,440 --> 00:15:00,440
But I wanted to learn a little bit more about Cohere.

150
00:15:00,440 --> 00:15:05,440
So could you tell us about your core product portfolio and what's on your roadmap?

151
00:15:05,440 --> 00:15:11,440
Yeah, for sure. So the mission with Cohere is to really just give technology language.

152
00:15:11,440 --> 00:15:16,440
And the way that we want to do that is put the tech, put these large language models into more hands.

153
00:15:16,440 --> 00:15:23,440
Today, if you're building a product and you want to deploy a language model as part of that,

154
00:15:23,440 --> 00:15:30,440
you've got to learn how to use some framework like PyTorch or Jax or TensorFlow.

155
00:15:30,440 --> 00:15:38,440
You need to learn how to install CUDA kernels on a VM, which is actually a huge, huge task.

156
00:15:38,440 --> 00:15:43,440
And so it's this very painful process, requires a lot of learning.

157
00:15:43,440 --> 00:15:49,440
It's very unnatural and requires a ton of prerequisite knowledge.

158
00:15:49,440 --> 00:15:52,440
So for Cohere, what we want to do is abstract that away.

159
00:15:52,440 --> 00:15:57,440
We want to present an interface which is just intuitive, natural, like Cohere.classify.

160
00:15:57,440 --> 00:16:01,440
You feed in the tweet that you want to classify.

161
00:16:01,440 --> 00:16:05,440
You give some examples of tweets being classified into the categories that you care about,

162
00:16:05,440 --> 00:16:13,440
and then you get a response which says, yeah, that tweet fits into sports or whatever.

163
00:16:13,440 --> 00:16:19,440
So we want to create a portfolio of these endpoints, which just makes this technology more accessible.

164
00:16:19,440 --> 00:16:24,440
And the goal being that it starts to proliferate.

165
00:16:24,440 --> 00:16:29,440
Because I think the transformer was released half a decade ago.

166
00:16:29,440 --> 00:16:39,440
And I saw extraordinary research-level results or contributions from it, right?

167
00:16:39,440 --> 00:16:49,440
Like the ability to write really compelling tags, the ability to few-shot prompt and get pretty good performance on a huge swath of problems.

168
00:16:49,440 --> 00:16:55,440
But it just hasn't been changing the fabric of consumer applications.

169
00:16:55,440 --> 00:16:58,440
And I'm a consumer. You're a consumer. We all use these apps.

170
00:16:58,440 --> 00:17:03,440
And so as a researcher who's seen the potential of the technology, it's super frustrating.

171
00:17:03,440 --> 00:17:12,440
You just wonder, like, what's stopping this from getting out there into apps faster?

172
00:17:12,440 --> 00:17:17,440
And I think two of the reasons are there's this huge compute barrier, right?

173
00:17:17,440 --> 00:17:22,440
When you train these big models, you need a supercomputer and tons of data.

174
00:17:22,440 --> 00:17:27,440
That's very difficult to use and collect.

175
00:17:27,440 --> 00:17:29,440
So the compute is definitely one of the big barriers.

176
00:17:29,440 --> 00:17:35,440
But the second piece is that really it's like the people, right?

177
00:17:35,440 --> 00:17:40,440
At the moment, yeah, we have millions of developers on our planet.

178
00:17:40,440 --> 00:17:46,440
But a tiny, tiny, tiny fraction of them actually know how to do this specific thing, machine learning and training models.

179
00:17:46,440 --> 00:17:52,440
So there's not a lot of people out there actually doing the work to integrate this into every product on Earth.

180
00:17:52,440 --> 00:18:00,440
And so for us, like, what we want to do is just blow that open, put this stuff into the hands of every single developer.

181
00:18:00,440 --> 00:18:05,440
It doesn't matter what your specialty is if you're a database developer, whatever you do, it doesn't matter.

182
00:18:05,440 --> 00:18:10,440
The important thing is that now you can build with large language models because you're given an interface,

183
00:18:10,440 --> 00:18:15,440
which doesn't require, you know, three years of study to get up to speed.

184
00:18:15,440 --> 00:18:18,440
So that's really what we're pushing for.

185
00:18:18,440 --> 00:18:22,440
It's quite ironic actually because I was involved in ML DevOps for many years,

186
00:18:22,440 --> 00:18:27,440
and normally introducing machine learning into software engineering makes it exponentially harder.

187
00:18:27,440 --> 00:18:31,440
But now we seem to have jumped to language models where it's become easier again.

188
00:18:31,440 --> 00:18:38,440
And I want to talk about the language being a new type of interface for software, but we'll save that just for a minute.

189
00:18:38,440 --> 00:18:42,440
I want to talk about the friction using large language models as a startup owner.

190
00:18:42,440 --> 00:18:47,440
So I'm a startup founder myself, and I want to use large language models because I'm very excited about them.

191
00:18:47,440 --> 00:18:53,440
And, you know, I can speak to some of the friction that I've been experiencing looking into this.

192
00:18:53,440 --> 00:19:02,440
So, I mean, if we look at what's happening over OpenAI, for example, the Microsoft Signup form and the content policy over there is quite intimidating.

193
00:19:02,440 --> 00:19:05,440
They've made it quite challenging to use in production.

194
00:19:05,440 --> 00:19:09,440
And actually, it makes me wonder how many people are using it in production.

195
00:19:09,440 --> 00:19:13,440
They say that content creation applications have a higher chance of misuse,

196
00:19:13,440 --> 00:19:18,440
which seems like an oxymoron to me because GPT-3 is literally like a content creator.

197
00:19:18,440 --> 00:19:21,440
They say you're going to expect a call from the Microsoft vetting service.

198
00:19:21,440 --> 00:19:24,440
They can pull the rug from under you at any time for any reason.

199
00:19:24,440 --> 00:19:27,440
They log and record everything under the guise of safety.

200
00:19:27,440 --> 00:19:29,440
They can't be held liable for anything.

201
00:19:29,440 --> 00:19:31,440
As I said, open-ended applications are refused.

202
00:19:31,440 --> 00:19:36,440
And in my opinion, the most exciting applications are the open-ended applications, right?

203
00:19:36,440 --> 00:19:38,440
I love the playful nature of it.

204
00:19:38,440 --> 00:19:44,440
Inside my application, I want to build a community of tinkerers who discover interesting new sub-applications, right?

205
00:19:44,440 --> 00:19:49,440
I want to build a marketplace of prefab prompt structures on top of my platform.

206
00:19:49,440 --> 00:19:54,440
I don't want to specify exactly how my application will be used, right?

207
00:19:54,440 --> 00:19:56,440
You know, I want my application to be fluid.

208
00:19:56,440 --> 00:20:03,440
And large language models make the consumers of an application kind of like programmers of that platform themselves.

209
00:20:03,440 --> 00:20:05,440
I think it's an entirely new paradigm.

210
00:20:05,440 --> 00:20:11,440
So with that in mind and the friction, do you think that would ever be allowed on Cohir?

211
00:20:11,440 --> 00:20:13,440
The startup that you're describing.

212
00:20:13,440 --> 00:20:19,440
Well, this idea that I shouldn't need to say to Cohir exactly how I'm going to be using the platform.

213
00:20:19,440 --> 00:20:21,440
I want it to be very open-ended and playful.

214
00:20:21,440 --> 00:20:27,440
I want my users to create new prompts and share the prompts and use it in interesting ways.

215
00:20:27,440 --> 00:20:30,440
I want it to be as open-ended as possible.

216
00:20:30,440 --> 00:20:40,440
Yeah, I think on Cohir, that would be fine subject to your users complying with some basic ethical principles.

217
00:20:40,440 --> 00:20:49,440
So presumably, I imagine that you don't want your users creating bots which propagate false information or hate speech on Twitter.

218
00:20:49,440 --> 00:20:56,440
You don't want like a billion bot accounts responding to every article.

219
00:20:56,440 --> 00:21:02,440
And so I assume that you're also incentivized to have some degree of terms of use.

220
00:21:02,440 --> 00:21:04,440
Is that right?

221
00:21:04,440 --> 00:21:07,440
Yeah, so I completely agree with your terms of use.

222
00:21:07,440 --> 00:21:09,440
I think it's very good actually.

223
00:21:09,440 --> 00:21:17,440
The one sticking point for me is that in order to have my application vetted, I have to say how it's being used.

224
00:21:17,440 --> 00:21:22,440
And it seems to be slightly away from having just quite an open-ended application.

225
00:21:22,440 --> 00:21:26,440
But I absolutely agree with all of the points in your content policy.

226
00:21:26,440 --> 00:21:27,440
Okay, yeah.

227
00:21:27,440 --> 00:21:30,440
So I think that startup should absolutely exist.

228
00:21:30,440 --> 00:21:31,440
That sounds awesome.

229
00:21:31,440 --> 00:21:36,440
Like a community of people sharing prompts, iterating with each other, figuring out stuff that works, doesn't work.

230
00:21:36,440 --> 00:21:40,440
I think we saw a lot of that with mid-journey and stable diffusion, right?

231
00:21:40,440 --> 00:21:44,440
Like just this collective effort to like, let's figure this thing out.

232
00:21:44,440 --> 00:21:47,440
Let's discover new ways to use it.

233
00:21:47,440 --> 00:21:54,440
That should 100% exist in the world and cohere would 100% support that.

234
00:21:54,440 --> 00:22:06,440
I think at the same time, there are just application domains that we don't want people building, like, you know, the ones that I just described.

235
00:22:06,440 --> 00:22:16,440
So as long as you're okay filtering those out and working with us to make sure that your product doesn't get used in those ways,

236
00:22:16,440 --> 00:22:17,440
we're fully on board.

237
00:22:17,440 --> 00:22:18,440
Like that should exist.

238
00:22:18,440 --> 00:22:20,440
That sounds amazing.

239
00:22:20,440 --> 00:22:28,440
And I think like more broadly, coherence is we really want to see a proliferation of this technology.

240
00:22:28,440 --> 00:22:32,440
We want to see a million new startups born from it.

241
00:22:32,440 --> 00:22:41,440
And so we view our users as like partners in bringing this to fruition and putting this tech in front of more users, more consumers, more businesses.

242
00:22:41,440 --> 00:22:44,440
And so we're very collaborative.

243
00:22:44,440 --> 00:22:46,440
Like we don't want to rug pull anyone.

244
00:22:46,440 --> 00:22:48,440
We don't want any surprises.

245
00:22:48,440 --> 00:22:52,440
So long as like our terms are met, we want to be a partner.

246
00:22:52,440 --> 00:22:54,440
We want to help you build.

247
00:22:54,440 --> 00:22:56,440
We want to like support you in the best way possible.

248
00:22:56,440 --> 00:23:01,440
I think some of this paradox might be from a legal point of view because I agree with you.

249
00:23:01,440 --> 00:23:05,440
You call it the playground and that's a great term for it.

250
00:23:05,440 --> 00:23:10,440
I think that some of the most exciting applications of large language models haven't been discovered yet.

251
00:23:10,440 --> 00:23:16,440
And they'll be discovered when you have a diverse community of people kind of sharing and trying interesting things.

252
00:23:16,440 --> 00:23:18,440
But just from that legal point of view.

253
00:23:18,440 --> 00:23:28,440
So we were a bit worried actually that some of the customers of our application might send us up malicious prompts and have our service terminated.

254
00:23:28,440 --> 00:23:37,440
And the way that we've been thinking about working around it on open AI, it's not possible on Coho, I don't think, is kind of getting the users to sign up for the service directly.

255
00:23:37,440 --> 00:23:44,440
And then just pasting their key inside our application so that they're responsible, they're getting themselves blocked if they do anything bad.

256
00:23:44,440 --> 00:23:47,440
And legally that puts us in a much safer position.

257
00:23:47,440 --> 00:23:49,440
But how do you feel about that?

258
00:23:49,440 --> 00:23:51,440
I mean, that's a great technique.

259
00:23:51,440 --> 00:23:54,440
I think it should be supported on Coho if it's not already.

260
00:23:54,440 --> 00:23:55,440
I think it is.

261
00:23:55,440 --> 00:24:02,440
We have a few people doing stuff like that, like a bring your own key type application.

262
00:24:02,440 --> 00:24:15,440
Alternatively, Coho would want to work with you to help you moderate use to catch bad actors, to catch misuse, out of terms use, etc.

263
00:24:15,440 --> 00:24:17,440
So we'll be very collaborative.

264
00:24:17,440 --> 00:24:18,440
We'll help you do that.

265
00:24:18,440 --> 00:24:19,440
We'll help you look at the data.

266
00:24:19,440 --> 00:24:21,440
We'll help you find users who are misusing it.

267
00:24:21,440 --> 00:24:29,440
We won't just blanket ban you because one of your users is trying to adversarily attack your business.

268
00:24:29,440 --> 00:24:40,440
We're trying to build with you and so I think we'll be quite reasonable assuming that you're reasonable too and that you don't want that type of activity on your product.

269
00:24:40,440 --> 00:24:50,440
So it's about supporting startup founders, helping them build their own tools to catch this sort of stuff and ban those users.

270
00:24:50,440 --> 00:24:58,440
It's like a collaborative building approach as opposed to, yeah, you're just a user, either comply or get banned.

271
00:24:59,440 --> 00:25:06,440
We're much more, I don't know, present, engaged.

272
00:25:06,440 --> 00:25:07,440
That makes sense.

273
00:25:07,440 --> 00:25:16,440
I mean, you can appreciate my fear that in a sense startup founders have lost their autonomy because it costs millions and millions of dollars.

274
00:25:16,440 --> 00:25:25,440
You folks are hiring some of the most talented people in the world to do this stuff and we're building on top of that foundation, which at the moment it seems like a risk.

275
00:25:25,440 --> 00:25:27,440
I appreciate that in spirit.

276
00:25:27,440 --> 00:25:31,440
It shouldn't actually be a risk, but just talking high level.

277
00:25:31,440 --> 00:25:37,440
So how would you distinguish your service from, let's say, GPT-3?

278
00:25:37,440 --> 00:25:47,440
So I think the Open AI team and GPT-3, they did like a fantastic thing by opening that up and giving it to the world.

279
00:25:47,440 --> 00:25:54,440
In terms of distinguishing ourselves from them, I think they've taken a very hands-off approach to this stuff.

280
00:25:55,440 --> 00:26:01,440
And they put out endpoints and it's kind of like a good luck, have fun, go build.

281
00:26:01,440 --> 00:26:10,440
With Cohere, we're trying to be more present and engaged and we're trying to tailor our roadmap towards the needs of users.

282
00:26:10,440 --> 00:26:18,440
So, for instance, we're listening to users and seeing what they're signing up for, what they're asking from us.

283
00:26:18,440 --> 00:26:21,440
One of those things is summarization.

284
00:26:21,440 --> 00:26:30,440
And so now we've spun up an effort to release a summarization endpoint that's generally useful across summarizing chat transcripts,

285
00:26:30,440 --> 00:26:33,440
like long documents, that type of thing.

286
00:26:33,440 --> 00:26:35,440
And so it's a two-way street.

287
00:26:35,440 --> 00:26:41,440
It's not just that our users are the consumers of our path towards whatever we want.

288
00:26:41,440 --> 00:26:45,440
It's like a dialogue and a conversation of what do you need?

289
00:26:45,440 --> 00:26:46,440
What should we build next?

290
00:26:46,440 --> 00:26:48,440
What do you see coming?

291
00:26:48,440 --> 00:26:50,440
And then we go build it.

292
00:26:50,440 --> 00:26:57,440
So it's very much a, it feels more two-way, community-oriented.

293
00:26:57,440 --> 00:27:02,440
We're trying to build the right product for our users and the most useful product possible.

294
00:27:02,440 --> 00:27:10,440
And so the way we do that is just through dialogue and conversation and people asking for the thing that they need, they want.

295
00:27:10,440 --> 00:27:14,440
OpenAI service suddenly kind of got a lot better recently.

296
00:27:14,440 --> 00:27:17,440
And I think they call it DaVinci 2.

297
00:27:17,440 --> 00:27:22,440
It's a bit of a mystery because I reviewed GPT-3 when it first came out a couple of years ago.

298
00:27:22,440 --> 00:27:24,440
And recently it seems much better.

299
00:27:24,440 --> 00:27:33,440
And if I understand correctly, they've done some kind of fine-tuning using reinforcement learning to align it to human preferences that instruct GPT or something like that.

300
00:27:33,440 --> 00:27:35,440
I don't even know if that's the case.

301
00:27:35,440 --> 00:27:41,440
But I just wondered if you could comment on that and do you folks plan to do something similar?

302
00:27:41,440 --> 00:27:44,440
Yeah, so we don't call them instruct models.

303
00:27:44,440 --> 00:27:48,440
We call them command models because of the co and co here.

304
00:27:48,440 --> 00:27:53,440
But we do have something currently in private beta.

305
00:27:53,440 --> 00:27:56,440
Hopefully we'll release it soon.

306
00:27:56,440 --> 00:27:59,440
But yeah, it has a huge impact on model performance.

307
00:27:59,440 --> 00:28:10,440
Like the ability to specify an instruction, specify an intent, describe the type of problem that you're solving completely changes model performance.

308
00:28:10,440 --> 00:28:12,440
And in some ways it's surprising.

309
00:28:12,440 --> 00:28:18,440
In other ways, it's very not surprising in that these models are just trained on web scraped data.

310
00:28:18,440 --> 00:28:21,440
Like there's no reason why you would expect them to behave the way that they do.

311
00:28:21,440 --> 00:28:27,440
We're just kind of lucky that they work as few-shot programs.

312
00:28:27,440 --> 00:28:38,440
And so I think this aligning with humans intent, human commands, human instructions,

313
00:28:38,440 --> 00:28:41,440
it's just a much more natural way to interface with these models.

314
00:28:41,440 --> 00:28:50,440
I think before instruct style models, it was a bit like you would have to discover the language of the model, right?

315
00:28:50,440 --> 00:28:55,440
And it was like this very opaque process of shifting things around, rephrasing things,

316
00:28:55,440 --> 00:29:00,440
trying to figure out what makes sense to this model.

317
00:29:00,440 --> 00:29:04,440
Super brittle, super painful.

318
00:29:04,440 --> 00:29:09,440
This command style model actually pulls that away and it's much more intuitive, it's much more fluid.

319
00:29:09,440 --> 00:29:12,440
It's the way that you would expect to interact with the model.

320
00:29:12,440 --> 00:29:16,440
Yeah, it's really interesting how, you know, when Steve Jobs released the iPad,

321
00:29:16,440 --> 00:29:19,440
he said there was something magic about it, something of magic about that interface.

322
00:29:19,440 --> 00:29:26,440
And similarly with these newer models, it feels like an invisible boundary is being crossed where I trust it.

323
00:29:26,440 --> 00:29:31,440
In a way, it's a trap because I'm anthropomorphizing it more because of exactly what you just said before.

324
00:29:31,440 --> 00:29:35,440
But I wanted to get into some engineering characteristics of code here.

325
00:29:35,440 --> 00:29:37,440
So I'll send you some quick fire questions.

326
00:29:37,440 --> 00:29:40,440
So how many tokens in a context window?

327
00:29:40,440 --> 00:29:45,440
So at the moment, it's 2048 and it should flip to 4096 shortly.

328
00:29:45,440 --> 00:29:50,440
But our goal is for an infinite token width.

329
00:29:50,440 --> 00:29:51,440
Wow.

330
00:29:51,440 --> 00:29:55,440
And so there's a few efforts that we're pursuing to enable that.

331
00:29:55,440 --> 00:29:56,440
Yeah.

332
00:29:56,440 --> 00:30:00,440
How many concurrent requests can your customers have, Perky?

333
00:30:00,440 --> 00:30:04,440
Infinite, as many as you'd like.

334
00:30:04,440 --> 00:30:06,440
Oh, even now?

335
00:30:06,440 --> 00:30:07,440
Yes, yeah.

336
00:30:07,440 --> 00:30:14,440
I think there's a top level bottleneck which we can actually remove for specific customers who need more.

337
00:30:14,440 --> 00:30:21,440
I think it's like 10,000 queries per minute.

338
00:30:21,440 --> 00:30:27,440
But we have folks doing billions and billions of characters a day.

339
00:30:27,440 --> 00:30:34,440
And so, yeah, we're happy to remove that restriction for those applications that need it.

340
00:30:34,440 --> 00:30:42,440
Do you support, let's say, enterprise security scenarios like single sign-on, key rotation, that kind of thing?

341
00:30:42,440 --> 00:30:45,440
So we do SSO.

342
00:30:45,440 --> 00:30:46,440
We don't do key rotation yet.

343
00:30:46,440 --> 00:30:54,440
We're hoping to build that out like with other enterprise requirements like co-location and that sort of thing.

344
00:30:54,440 --> 00:30:59,440
We're also building out the capability to do that on different clouds.

345
00:30:59,440 --> 00:31:04,440
But at present, we don't have that yet, but it's roadmaped.

346
00:31:04,440 --> 00:31:05,440
Okay.

347
00:31:05,440 --> 00:31:12,440
And you were just touching on this before, but what's the kind of largest, most highly-scaled application deployed on Co-Hit?

348
00:31:12,440 --> 00:31:23,440
So, similar to, I think, what everyone's been seeing, the first sort of application that has hit product market fit is copywriting.

349
00:31:23,440 --> 00:31:24,440
Yeah.

350
00:31:24,440 --> 00:31:29,440
There's a lot of companies, Jasper, Copy AI, HyperWrite.

351
00:31:29,440 --> 00:31:39,440
They've really found something that works for the average person, drives tons of value, speeds people, makes them more efficient, makes them more creative.

352
00:31:39,440 --> 00:31:44,440
And so that's where we're seeing volumes just skyrocket.

353
00:31:44,440 --> 00:31:49,440
So I'm really excited about the prospect of stacking calls to these large language models.

354
00:31:49,440 --> 00:31:54,440
So building a large computational graph of recursive calls.

355
00:31:54,440 --> 00:31:57,440
But doing all the round trips at the moment is pretty slow.

356
00:31:57,440 --> 00:32:00,440
It's almost like, I want to create this graph, this computation graph.

357
00:32:00,440 --> 00:32:02,440
I want to ship it over to you.

358
00:32:02,440 --> 00:32:05,440
You do it behind the scenes on your app fabric with parallelism.

359
00:32:05,440 --> 00:32:10,440
You send me the results because right now there's a significant engineering challenge for me to do that.

360
00:32:10,440 --> 00:32:15,440
But I think that the next generation of application platforms will be doing something like that.

361
00:32:15,440 --> 00:32:17,440
There'll be a fabric on top of Co-Hit.

362
00:32:17,440 --> 00:32:23,440
So are you planning anything to kind of make my life easier to do that?

363
00:32:23,440 --> 00:32:25,440
That's so cool.

364
00:32:25,440 --> 00:32:27,440
I hadn't actually thought about that.

365
00:32:27,440 --> 00:32:28,440
Sorry.

366
00:32:28,440 --> 00:32:31,440
I hadn't actually thought about that.

367
00:32:31,440 --> 00:32:39,440
Like basically compiling a graph of chained prompts to Co-Hit and shipping it over.

368
00:32:39,440 --> 00:32:42,440
That's fascinating.

369
00:32:42,440 --> 00:32:49,440
I guess what you gain is the two-way network cost, right?

370
00:32:49,440 --> 00:32:51,440
Like that's what you're saving.

371
00:32:51,440 --> 00:32:55,440
And network costs tend to be quite small in practice.

372
00:32:55,440 --> 00:33:00,440
So depending on how big your prompt is and how many tokens you're generating,

373
00:33:00,440 --> 00:33:06,440
I could see it not giving you a huge amount of lift.

374
00:33:06,440 --> 00:33:14,440
But yeah, I'd love to look at your use case and kind of understand

375
00:33:14,440 --> 00:33:20,440
is the issue actually with the fact that you're having to make multiple network requests in sequence?

376
00:33:20,440 --> 00:33:23,440
Or is it that the underlying model itself is too small?

377
00:33:23,440 --> 00:33:28,440
And what you need is actually just speed-ups to that core model.

378
00:33:28,440 --> 00:33:34,440
Another thing that Co-Hit does is we'll do specific deployments for customers.

379
00:33:34,440 --> 00:33:37,440
So some folks have very low latency requirements.

380
00:33:37,440 --> 00:33:43,440
And we can split up our model across more nodes, more GPUs.

381
00:33:43,440 --> 00:33:47,440
And that makes the latency go way, way down.

382
00:33:47,440 --> 00:33:52,440
So if you were looking for like a 6x speed-up, a 4x speed-up in latency,

383
00:33:52,440 --> 00:33:56,440
that's one easy way to do it.

384
00:33:56,440 --> 00:33:59,440
Yeah, I think we're only just scratching the surface of what's possible here.

385
00:33:59,440 --> 00:34:05,440
So if I did this, what I just spoke about, I would need to create a workflow engine.

386
00:34:05,440 --> 00:34:10,440
I would need to do all sorts of DAG optimization and parallelism.

387
00:34:10,440 --> 00:34:14,440
And I might be able to cache certain steps.

388
00:34:14,440 --> 00:34:18,440
And certain steps I might be able to do some metamachine learning to optimize the performance.

389
00:34:18,440 --> 00:34:22,440
That needs a large one. That needs a small one.

390
00:34:22,440 --> 00:34:26,440
There's a whole kind of universe I think that we can explore here,

391
00:34:26,440 --> 00:34:29,440
if you know, just going one level of abstraction higher.

392
00:34:29,440 --> 00:34:32,440
You know, because the next startup founders will be building platforms

393
00:34:32,440 --> 00:34:39,440
that essentially compose human knowledge on top of these large language models in some kind of a fabric.

394
00:34:39,440 --> 00:34:43,440
So I think that's what excites me at the moment.

395
00:34:43,440 --> 00:34:45,440
That's really cool. That's really cool.

396
00:34:45,440 --> 00:34:51,440
Okay, so you express this graph of like prompts chaining into each other.

397
00:34:51,440 --> 00:34:56,440
You might have loops and conditions and you compile that,

398
00:34:56,440 --> 00:35:00,440
you ship it to the large language model provider, they perform optimizations,

399
00:35:00,440 --> 00:35:03,440
they handle all the parallelism, all the conditions.

400
00:35:03,440 --> 00:35:06,440
That's a really fascinating product idea. I like that.

401
00:35:06,440 --> 00:35:09,440
Yeah, please build it so I don't have to.

402
00:35:09,440 --> 00:35:13,440
Anyway, I wanted to come on to some more LLM discussion.

403
00:35:13,440 --> 00:35:19,440
So our friend, Francois Chouelet, he says that large language models are a bit like databases, right?

404
00:35:19,440 --> 00:35:23,440
But to me, they seem like so much more than that, right?

405
00:35:23,440 --> 00:35:26,440
So I think Francois was saying they are the representation, you know,

406
00:35:26,440 --> 00:35:32,440
equivalent to the B tree structure in a database, but I think they're also the database engine as well.

407
00:35:32,440 --> 00:35:34,440
But it's a new type of database engine.

408
00:35:34,440 --> 00:35:37,440
We don't understand how this database engine works.

409
00:35:37,440 --> 00:35:38,440
It's kind of unintelligible to us.

410
00:35:38,440 --> 00:35:47,440
But do you think that's a good analogy or like how do you mentally kind of think about what's going on in a language model?

411
00:35:47,440 --> 00:35:52,440
Yeah, I like the analogy. I like yours a bit better.

412
00:35:52,440 --> 00:36:05,440
I think my own, I kind of view just a raw large language model trained on the web as the next iteration of a search engine

413
00:36:05,440 --> 00:36:14,440
instead of explicitly retrieving results and references out into the web.

414
00:36:14,440 --> 00:36:17,440
So I like to use the analogy of like search engines.

415
00:36:17,440 --> 00:36:24,440
And instead of like explicit hard references where you make a query, you get back a link out to some site.

416
00:36:24,440 --> 00:36:27,440
A language model, it's kind of more like a dialogue.

417
00:36:27,440 --> 00:36:29,440
You can extract information from it.

418
00:36:29,440 --> 00:36:33,440
It has all of this knowledge that it's seen throughout the web.

419
00:36:33,440 --> 00:36:37,440
It's like a soft version of a search engine.

420
00:36:37,440 --> 00:36:44,440
And the mode of discovery is still language is just a much more natural interface.

421
00:36:44,440 --> 00:36:51,440
It's like a conversation as opposed to what we type into Google, which is kind of its own language, right?

422
00:36:51,440 --> 00:36:53,440
Like we don't query in the same way.

423
00:36:53,440 --> 00:36:58,440
We would ask our teacher, our Calc teacher about a theorem.

424
00:36:58,440 --> 00:37:01,440
We write in a very strange way.

425
00:37:01,440 --> 00:37:09,440
I think language models are a much more natural modality to the corpus of human knowledge.

426
00:37:09,440 --> 00:37:13,440
It's much more intuitive, natural, seamless.

427
00:37:13,440 --> 00:37:16,440
The problem is they hallucinate a lot.

428
00:37:16,440 --> 00:37:18,440
And so they'll fill in gaps.

429
00:37:18,440 --> 00:37:19,440
They're compressors, right?

430
00:37:19,440 --> 00:37:25,440
And so they see the web and they try to compress that down into their parameters and they lose little bits and pieces.

431
00:37:25,440 --> 00:37:33,440
And then when they have to regurgitate it, reconstruct it, they'll just fill in a plausible answer inside those gaps.

432
00:37:33,440 --> 00:37:40,440
And so the other reason I'm really excited about retrieval models is that they ground them more in reality.

433
00:37:40,440 --> 00:37:47,440
You put less burden on the model's parameters to memorize every single fact that's out there.

434
00:37:47,440 --> 00:37:51,440
And you instead just have them do the distillation process.

435
00:37:51,440 --> 00:37:57,440
So the model makes a query out to this knowledge base, this database of true facts.

436
00:37:57,440 --> 00:37:59,440
It pulls back some references.

437
00:37:59,440 --> 00:38:04,440
And then the model is just asked, hey, given these references, answer my question.

438
00:38:04,440 --> 00:38:14,440
And so it can actually make reference to true facts instead of having to hallucinate and imagine its own facts.

439
00:38:14,440 --> 00:38:20,440
So yeah, my kind of like the analogy that I like to build off of is this is the next search engine.

440
00:38:20,440 --> 00:38:22,440
It's the next interface to the internet.

441
00:38:22,440 --> 00:38:26,440
It's the next interface to human knowledge.

442
00:38:26,440 --> 00:38:29,440
It's funny you say that it might actually be the next search engine soon.

443
00:38:29,440 --> 00:38:33,440
I think search engines are about to be revolutionized.

444
00:38:33,440 --> 00:38:37,440
But yeah, I was going to ask you about the next frontier of large language models.

445
00:38:37,440 --> 00:38:39,440
And we've already been speaking about some of it.

446
00:38:39,440 --> 00:38:48,440
We've been talking about the integration of information retrieval, which will ground the responses, you know, based on actual things which exist in your operational systems.

447
00:38:48,440 --> 00:38:58,440
We just spoke before about hierarchical compositions, having this exciting, you know, computational graph, maybe some different architecture types or interaction patterns as well.

448
00:38:58,440 --> 00:39:03,440
I mean, you were just saying that there's this back and forth interaction, which I think is really interesting.

449
00:39:03,440 --> 00:39:11,440
On Google at the moment, you just type in a search and you press search once, you're not iteratively refining reflexively, recursively.

450
00:39:11,440 --> 00:39:15,440
Multimodality was another thing I thought of, you know, maybe having language in and text out.

451
00:39:15,440 --> 00:39:23,440
But I mean, those are a few examples, but does anything come top of mind to you for the next frontier?

452
00:39:23,440 --> 00:39:32,440
Yeah, like in addition to what you just listed, I think the ability to keep state over a long term horizon, like my dream for Coheir

453
00:39:32,440 --> 00:39:36,440
is that it knows its users.

454
00:39:36,440 --> 00:39:40,440
It can see into the past of all the past interactions with each one of these users.

455
00:39:40,440 --> 00:39:42,440
It knows their preferences.

456
00:39:42,440 --> 00:39:47,440
It knows about the user and the way that they like to interact with this model.

457
00:39:47,440 --> 00:39:51,440
And to do that, it can't be transactional.

458
00:39:51,440 --> 00:40:00,440
It can't just be like text in, text out, you know, forget everything else.

459
00:40:00,440 --> 00:40:05,440
In the history, we need to be able to like maintain a state between the model and its user.

460
00:40:05,440 --> 00:40:08,440
We need to be able to keep a record of that.

461
00:40:08,440 --> 00:40:17,440
I was talking about with retro, the ability to add into that corpus, keep notes, kind of like maintain an internal dialogue.

462
00:40:17,440 --> 00:40:29,440
We've seen how useful that is with like the scratch pad techniques that people have been working on, just giving the model space to write out its thoughts before giving an answer.

463
00:40:29,440 --> 00:40:36,440
I think we need the ability to store that forever and to have that referenceable down the line in the future.

464
00:40:36,440 --> 00:40:39,440
So, yeah, I think that's one of the most exciting ones.

465
00:40:39,440 --> 00:40:44,440
And then of course, there's multi-modality, which is blowing up now.

466
00:40:44,440 --> 00:40:47,440
Again, it's like another form of grounding.

467
00:40:47,440 --> 00:40:55,440
It ties the model into the real world, into the physical reality of our world.

468
00:40:55,440 --> 00:40:57,440
I think that's super exciting.

469
00:40:57,440 --> 00:41:03,440
So, yeah, I think the ability to reference external knowledge bases contribute to its own knowledge base.

470
00:41:03,440 --> 00:41:09,440
The ability to understand the world not just through text, but through audio, video, images.

471
00:41:09,440 --> 00:41:16,440
And then lastly, augmenting large language models with the ability to use tools.

472
00:41:16,440 --> 00:41:21,440
Obviously, like, we built this world for ourselves.

473
00:41:21,440 --> 00:41:24,440
We built all these little things to integrate with us.

474
00:41:24,440 --> 00:41:27,440
And one of our primary modalities is language.

475
00:41:27,440 --> 00:41:31,440
And so a lot of our tools are language-driven.

476
00:41:31,440 --> 00:41:37,440
If you think about web browsing, it's mostly you reading text and clicking links and following.

477
00:41:37,440 --> 00:41:39,440
It's very text-driven.

478
00:41:39,440 --> 00:41:48,440
And so something I'm super excited about is, okay, well, we have these language models which have pretty good grasp of language.

479
00:41:48,440 --> 00:41:52,440
They seem to have some modest level of understanding.

480
00:41:52,440 --> 00:41:59,440
Can we actually get them to use the tools that we built for us humans, stuff like web browsers?

481
00:41:59,440 --> 00:42:04,440
And the results we're seeing there are just super exciting, super exciting.

482
00:42:04,440 --> 00:42:05,440
Cool.

483
00:42:05,440 --> 00:42:09,440
I wanted to ask you about different modes of understanding.

484
00:42:09,440 --> 00:42:13,440
I'm actually making a video at the moment on the Chinese rim argument.

485
00:42:14,440 --> 00:42:18,440
They say that shortcut learning is an often discussed characteristic of machine learning.

486
00:42:18,440 --> 00:42:23,440
So when a system achieves human-like performance on a benchmark by the slight of hand,

487
00:42:23,440 --> 00:42:30,440
if you like, of spurious correlations in the data, rather than what we intuit to be human-like comprehension.

488
00:42:30,440 --> 00:42:35,440
Now, large language models have learned this very intricate statistical correlation,

489
00:42:35,440 --> 00:42:39,440
which allows near-perfect performance in lieu of human cognition.

490
00:42:39,440 --> 00:42:46,440
And it's possible that evaluating these algorithms against standards designed to gauge human cognition might be barking up the wrong tree.

491
00:42:46,440 --> 00:42:51,440
Now, human comprehension is difficult to pin down exactly what it means,

492
00:42:51,440 --> 00:42:57,440
but it doesn't seem to be entirely reflected in large language models, not always in how they behave at least.

493
00:42:57,440 --> 00:43:03,440
Now, for humans, it's not always enough to know the statistical features of linguistic symbols.

494
00:43:03,440 --> 00:43:09,440
We also need to grasp the underlying ideas and contexts in which those symbols convey,

495
00:43:09,440 --> 00:43:14,440
while language models can pick up on these tiny statistical patterns that we never could.

496
00:43:14,440 --> 00:43:22,440
Melanie Mitchell said in her most recent paper that recent years in AI have produced machines with new modes of knowledge.

497
00:43:22,440 --> 00:43:26,440
So in the same way that various animals are better suited to different settings,

498
00:43:26,440 --> 00:43:32,440
so too will our intelligent systems be, you know, more adapted to various issues.

499
00:43:32,440 --> 00:43:39,440
She said that large-scale statistical models will continue to be favored for matters requiring these vast amounts of historically encoded knowledge

500
00:43:39,440 --> 00:43:46,440
and where performance is paramount, and human intelligence might be favored for problems where we have very limited knowledge

501
00:43:46,440 --> 00:43:48,440
and we have strong causal mechanisms.

502
00:43:48,440 --> 00:43:53,440
So, you know, my question to you basically is what is the difference between a machine learning algorithm

503
00:43:53,440 --> 00:43:59,440
which has learned these very intricate statistical correlations and a human which has developed a more lifelike comprehension?

504
00:43:59,440 --> 00:44:05,440
I think that the first and shortest answer is probably the objective function, right?

505
00:44:05,440 --> 00:44:11,440
Like the context that we evolved in is very, very different than the context that we're training these models in.

506
00:44:11,440 --> 00:44:15,440
Certainly, like humans have a model of language.

507
00:44:15,440 --> 00:44:19,440
We're doing language modeling for sure, but we have to do a lot on top of that.

508
00:44:19,440 --> 00:44:29,440
And that's just one component that we rely on in order to achieve our objective and go through life and procreate.

509
00:44:29,440 --> 00:44:34,440
I think for these language models, they're given just that one piece of it.

510
00:44:34,440 --> 00:44:39,440
And there's a lot within that one piece, just within language modeling, like you're forced to learn a ton of facts,

511
00:44:39,440 --> 00:44:42,440
you're forced to learn a ton of patterns.

512
00:44:43,440 --> 00:44:53,440
And so, it is quite extraordinary that all of this amazing behavior falls out of just the language modeling problem.

513
00:44:53,440 --> 00:44:55,440
But it's also not, right?

514
00:44:55,440 --> 00:45:01,440
Because in order to accurately model language in a generalizable way, you're forced to pick up stuff like,

515
00:45:01,440 --> 00:45:04,440
how do I translate? How do I classify stuff?

516
00:45:04,440 --> 00:45:07,440
Because that's represented in the data.

517
00:45:07,440 --> 00:45:12,440
But when you're asking questions about how come language models get this thing wrong,

518
00:45:12,440 --> 00:45:17,440
which humans find super intuitive or it's so obvious, so logical,

519
00:45:17,440 --> 00:45:19,440
the contexts are completely different, right?

520
00:45:19,440 --> 00:45:21,440
We didn't evolve to just be a language model.

521
00:45:21,440 --> 00:45:28,440
I think that anyone who's claiming that has a high burden of proof.

522
00:45:29,440 --> 00:45:37,440
And it also points back to what we were saying earlier around, is this the last architecture?

523
00:45:37,440 --> 00:45:40,440
Is it just this plus scale?

524
00:45:40,440 --> 00:45:45,440
There's more that needs to be there and also in the objective function, right?

525
00:45:45,440 --> 00:45:48,440
We can't just do language modeling all the way.

526
00:45:48,440 --> 00:45:51,440
I think language modeling will get us very far.

527
00:45:51,440 --> 00:45:57,440
And I think that we can take language models and augment them with these very useful additional components.

528
00:45:57,440 --> 00:46:01,440
We can give them access to tools.

529
00:46:01,440 --> 00:46:09,440
But fundamentally, if you're looking for something that's human-like and acts and behaves like us,

530
00:46:09,440 --> 00:46:14,440
there's an objective function change that needs to happen.

531
00:46:14,440 --> 00:46:16,440
It can't just be language modeling.

532
00:46:16,440 --> 00:46:18,440
There needs to be something more.

533
00:46:18,440 --> 00:46:22,440
Yeah, we all anthropomorphize AI to different degrees, I think.

534
00:46:22,440 --> 00:46:27,440
I mean, what I took from Melanie is that she thinks that language models are not anthropomorphic,

535
00:46:27,440 --> 00:46:29,440
that it's a completely different mode.

536
00:46:29,440 --> 00:46:35,440
But do you think that humans do think like language models and we have some other apparatus on the top,

537
00:46:35,440 --> 00:46:39,440
or do you kind of agree with Melanie that they're completely different?

538
00:46:39,440 --> 00:46:41,440
I don't think they're completely different.

539
00:46:41,440 --> 00:46:48,440
I would not agree that they're categorically different.

540
00:46:48,440 --> 00:46:53,440
I think they're categorically our brains and modeling and statistical models.

541
00:46:53,440 --> 00:46:59,440
I think that they overlap very, very heavily.

542
00:46:59,440 --> 00:47:03,440
Again, I think the objective function is just completely different.

543
00:47:03,440 --> 00:47:09,440
I'm a big believer that the real value of AI models exists as a kind of entangled form of interactive,

544
00:47:09,440 --> 00:47:13,440
creative pairing between a human brain and a model.

545
00:47:13,440 --> 00:47:17,440
So David Chalmers coined the term extended mind to talk about us and phones.

546
00:47:17,440 --> 00:47:22,440
And I think actually large language models, that's the real extended mind

547
00:47:22,440 --> 00:47:25,440
because I feel this when I play with large language models.

548
00:47:25,440 --> 00:47:31,440
I feel that it's truly intelligent in combination or in tandem with me.

549
00:47:31,440 --> 00:47:36,440
So large language models, I don't have agency or intentionality, but we do,

550
00:47:36,440 --> 00:47:40,440
and we can use them and something very interesting emerges from that.

551
00:47:40,440 --> 00:47:44,440
As an example, I want to build a virtual assistant that comes with me everywhere.

552
00:47:44,440 --> 00:47:49,440
It's part of my everyday experience and I'm just in tandem with a large language model

553
00:47:49,440 --> 00:47:56,440
producing all sorts of creative thoughts that are embodied in the environment I'm in.

554
00:47:56,440 --> 00:48:00,440
That, I think, is an extension of my intelligence.

555
00:48:00,440 --> 00:48:03,440
Yeah, I 100% agree.

556
00:48:03,440 --> 00:48:12,440
I'm thinking about what happened with search and the outsourcing of knowledge.

557
00:48:12,440 --> 00:48:13,440
Not even that long ago.

558
00:48:13,440 --> 00:48:15,440
Like pre-Google, right?

559
00:48:15,440 --> 00:48:17,440
We had to memorize a lot of stuff.

560
00:48:17,440 --> 00:48:19,440
It had to be digested.

561
00:48:19,440 --> 00:48:22,440
Some stuff we could outsource into books and that type of thing

562
00:48:22,440 --> 00:48:29,440
and we would retrieve them very slowly and very painfully as needed.

563
00:48:29,440 --> 00:48:32,440
And then came the search engine and mobile phones

564
00:48:32,440 --> 00:48:35,440
and at any moment, any piece of information that you need,

565
00:48:35,440 --> 00:48:40,440
you have a source to go get it.

566
00:48:40,440 --> 00:48:44,440
And so we took that taxing on our neurons,

567
00:48:44,440 --> 00:48:49,440
taxing on our time, activity of having to pull a book off the shelf

568
00:48:49,440 --> 00:48:52,440
and we turned it into like an instantaneous thing.

569
00:48:52,440 --> 00:49:00,440
And so it freed up our minds, freed up our time to do a lot more high impact activities.

570
00:49:00,440 --> 00:49:05,440
And we've been significantly more productive as a product.

571
00:49:05,440 --> 00:49:12,440
And I think the next step is to continue to outsource things that we don't like doing,

572
00:49:12,440 --> 00:49:16,440
things that are taxing, they're time consuming,

573
00:49:16,440 --> 00:49:22,440
that we put off, that we don't like to hand more of that work over to the language model,

574
00:49:22,440 --> 00:49:27,440
over to these ML systems, AI systems,

575
00:49:27,440 --> 00:49:31,440
and to free ourselves up to just focus on the things that humans care about,

576
00:49:32,440 --> 00:49:35,440
the things that we're best at.

577
00:49:35,440 --> 00:49:38,440
So I think it's another complete transformation.

578
00:49:38,440 --> 00:49:43,440
It just turns out that you can hand way, way more than just the knowledge over to a language model.

579
00:49:43,440 --> 00:49:46,440
You can hand entire activities.

580
00:49:46,440 --> 00:49:49,440
Yeah, that's what I'm excited about.

581
00:49:49,440 --> 00:49:52,440
I like your idea of like a personal assistant coming around with you.

582
00:49:52,440 --> 00:49:54,440
You can give it an instructor,

583
00:49:54,440 --> 00:49:58,440
oh yeah, I need to buy body wash and it shows up at your door tomorrow.

584
00:49:58,440 --> 00:49:59,440
I'm building it.

585
00:49:59,440 --> 00:50:00,440
I'm building it.

586
00:50:00,440 --> 00:50:01,440
You're building it.

587
00:50:01,440 --> 00:50:05,440
I haven't done that topic for another day.

588
00:50:05,440 --> 00:50:07,440
Quickfire question.

589
00:50:07,440 --> 00:50:11,440
We spoke about language being the next interface for application development.

590
00:50:11,440 --> 00:50:14,440
So I'm really excited about this possible future.

591
00:50:14,440 --> 00:50:15,440
I'm already building it.

592
00:50:15,440 --> 00:50:22,440
But there's also a bit of a panacea element to it because the sales pitch is that

593
00:50:22,440 --> 00:50:26,440
even my community, my users could become software developers.

594
00:50:26,440 --> 00:50:28,440
They could actually build my platform.

595
00:50:28,440 --> 00:50:34,440
My platform becomes kind of like amorphous because it's so distributed.

596
00:50:34,440 --> 00:50:39,440
But the worry is that we'll get into this situation where we might feel we need to reinvent code

597
00:50:39,440 --> 00:50:45,440
because the prompts might become so complex that you need the equivalent of lawyers to understand them.

598
00:50:45,440 --> 00:50:48,440
Do you think about that concern?

599
00:50:48,440 --> 00:50:55,440
I have not because Coher's whole point is to abstract that away

600
00:50:55,440 --> 00:50:58,440
and to try and make things as simple to use as possible

601
00:50:58,440 --> 00:51:05,440
and to come up with canonical standards, abstractions even above prompting

602
00:51:05,440 --> 00:51:08,440
that are easier to use so that it pushes out further.

603
00:51:08,440 --> 00:51:14,440
It becomes less of a dark art or becomes less complex.

604
00:51:14,440 --> 00:51:18,440
I think in order for this stuff to proliferate, that's like a necessary condition.

605
00:51:18,440 --> 00:51:19,440
Things have to get easier.

606
00:51:19,440 --> 00:51:28,440
It can't be like you're speaking to an alien or you're having to divine a prompt that happens to work.

607
00:51:28,440 --> 00:51:33,440
So yeah, I think that's where the arrow of progress in this field points

608
00:51:33,440 --> 00:51:37,440
is towards more abstraction, easier to use, more intuitive interfaces.

609
00:51:37,440 --> 00:51:43,440
So I'm not too concerned about that because that's something that's very much top of mind.

610
00:51:44,440 --> 00:51:47,440
Aiden Gomez, it's been an absolute honor. Thank you for joining us.

611
00:51:47,440 --> 00:51:48,440
Likewise.

612
00:51:48,440 --> 00:51:50,440
Likewise. Thank you so much, Tim.

