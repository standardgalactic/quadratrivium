WEBVTT

00:00.960 --> 00:05.360
Welcome back to Machine Learning Street Talk. I'm your host, Tim Scarf.

00:06.240 --> 00:12.880
Now, today on the show, we're joined by Dr. Walid Saber, who's just written a review of the book

00:12.880 --> 00:19.600
Machines Will Never Rule the World, Artificial Intelligence Without Fear, written by Jops Langreb

00:19.600 --> 00:26.000
and Barry Smith. Now, Dr. Saber will be discussing his review, which provides a detailed analysis

00:26.000 --> 00:33.760
of the book's arguments that strong AI is impossible. In his review, Dr. Saber acknowledges

00:33.760 --> 00:39.360
the argument made by Langreb and Smith that anything we engineer is ultimately a system

00:39.360 --> 00:45.120
which can be mathematically modelled and described. He then goes on to discuss the complexity of

00:45.120 --> 00:53.040
modelling mental processes, which the authors argue are dynamic, adaptive, continuously evolving,

00:53.040 --> 00:59.920
and constitutes systems whose behaviour affects and is affected by the environment they function in.

01:01.920 --> 01:07.920
He also touches on the notion of granularity, arguing that complex systems are all the way up

01:08.480 --> 01:16.400
from specific components of the mind to the mind itself and that no known mathematics can model them.

01:16.880 --> 01:22.560
Dr. Saber then delves into the complexities of language and open interactive dialogues,

01:22.560 --> 01:28.160
asserting that language is a prerequisite for any artificial general intelligence,

01:28.160 --> 01:34.720
but that linguistic communication itself is a complex system that no mathematics can model.

01:35.520 --> 01:40.800
He doesn't subscribe to the argument that interactive bots can be built in narrow domains,

01:41.520 --> 01:46.880
since responses and the overall context cannot be predicted in any meaningful way.

01:47.920 --> 01:54.160
Dr. Saber has two reservations as to the conclusions made by Langreb and Smith.

01:54.160 --> 02:00.240
He questions their use of the word never and suggests there could be a new mathematics

02:00.240 --> 02:06.480
that mental processes require that is yet to be discovered. He also doesn't believe that the fact

02:06.560 --> 02:12.560
that complex behaviour cannot be mathematically modelled precludes the possibility of building

02:12.560 --> 02:19.600
such systems, as evidenced by the intentional programming language LISP, and he also considers

02:19.600 --> 02:25.120
the possibility of hypercomputation in validating the church-touring hypothesis.

02:25.120 --> 02:29.840
We'll talk about that a bit later. Finally, Dr. Saber expresses his regret

02:29.840 --> 02:34.800
that the book didn't go into further detail on the frame problem in AI.

02:36.480 --> 02:40.800
He calls for further research into belief revision in complex systems.

02:42.240 --> 02:46.880
Join us today as we speak with Dr. Wallid Saber about his review of this book that we've just

02:46.880 --> 02:52.480
been speaking about. Machines will never rule the world. And by the way, I think very highly

02:52.480 --> 02:58.960
of Wallid. I think he is one of our most loved guests. He is a polymath. He has an incredible

02:58.960 --> 03:04.160
breadth of knowledge across so many fields, you know, from AI and computer science,

03:04.160 --> 03:11.120
to mathematics, to philosophy, to linguistics. He really is a rare breed, and he also brings

03:11.120 --> 03:18.240
a very interesting contrarian view, I would say, to the current kind of modus operandi,

03:18.240 --> 03:22.240
or the zeitgeist in the community at the moment. He's a breath of fresh air.

03:22.960 --> 03:28.720
Anyway, if you haven't already, consider subscribing to our YouTube channel, or indeed

03:28.720 --> 03:33.520
rating our podcast on your favourite podcasting platform if you happen to be listening to us.

03:34.160 --> 03:37.920
Anyway, without any further delay, I give you Dr. Wallid Saber.

03:44.400 --> 03:53.520
Welcome back to MLST, folks. We have the unmistakable Wallid Saber, the legend that is Wallid Saber,

03:53.520 --> 03:58.160
but we also have Mark from our Discord community. Mark, would you like to introduce yourself?

03:59.040 --> 04:06.000
Hi, guys. I'm Mark Aguil, a philosopher, cognitive scientist, and software engineer at

04:06.000 --> 04:12.960
MLST. Awesome. Welcome, Mark. You know, things have been a bit of a blur,

04:12.960 --> 04:18.400
but I think this is right, Tim. I think Wallid was the first, or one of the first,

04:18.400 --> 04:25.200
really big names that we had come on the show, right? I don't think so,

04:25.200 --> 04:31.120
but the most controversial, let's put it this way, the one that made probably,

04:32.400 --> 04:40.480
it was so predictable, what was, I mean, probably the first one that broke that

04:43.600 --> 04:49.200
predictive model. Well, I just remember, I mean, I remember Tim and I being like super,

04:49.200 --> 04:54.000
because we didn't know you, right, before then, and I remember us being like so excited that you

04:54.000 --> 04:58.400
agreed to come on the show. You know, of course, now that we know you, we're kind of like,

04:58.400 --> 05:04.160
ah, whatever, it's just Wallid. We're just through, by the way.

05:04.160 --> 05:08.000
We were following all over ourselves that you were coming on the show, we're like, oh my god,

05:08.000 --> 05:13.760
this is so awesome. To be honest with you, I never thought I would, I would,

05:15.280 --> 05:19.680
yeah, I never thought I would have, my opinion would matter that much, to be honest with you,

05:19.680 --> 05:26.480
and it all started by creating this medium blog, and I started spitting out stuff that,

05:27.600 --> 05:34.080
hey, do you guys know that there's dissonance in that? And I was surprised how much it,

05:36.400 --> 05:45.840
even by people that are living off and making a living, and they preach and write papers on

05:46.560 --> 05:51.600
what I'm attacking, and they would say, don't mention my name, by the way, it's all private

05:51.600 --> 06:00.800
messages. But apparently, I said a couple of things that touched people, but you know.

06:02.560 --> 06:07.840
Yeah. Yeah, I was at New York's last week, and the amount of people that came up to me and said,

06:07.840 --> 06:12.720
I love the show, Tim, Wallid's my favorite guest that you've had on, because Wallid just provides

06:12.720 --> 06:19.360
a completely different perspective, because we're bred on empiricism and neural networks. And part

06:19.360 --> 06:24.080
of the reason I want to get you back on, Wallid, is to counteract some of the, I mean, we've been

06:24.080 --> 06:28.960
speaking to a lot of deep learning people recently, so we need to counteract that a little bit.

06:29.520 --> 06:38.320
Yeah, that one shocked the hell out of me. I mean, I have people like myself at this event. I mean,

06:38.320 --> 06:49.040
if you said ACL, maybe, yeah, okay. But I mean, this is the deep learning meeting, right? I mean,

06:49.040 --> 07:00.240
so I was shocked. But yeah. Yeah, indeed. Although I have some more positive views of DL now.

07:00.240 --> 07:08.480
Oh, go on. Yeah, I mean, look, I breaking news, breaking news.

07:10.480 --> 07:18.160
Positive, although I still have my reservations as to AGI and all that stuff. But I have been

07:20.160 --> 07:27.120
completely impressed with the developments in large language models. I have to admit that

07:28.000 --> 07:33.920
sometimes I say, what the hell is this? Now, technically, let me tell you what's happening.

07:36.560 --> 07:39.840
And I'm working on something that probably would quantify

07:41.440 --> 07:48.080
where can this go? How much can you, how much will scale? The bottom line is this,

07:48.720 --> 07:54.480
these guys have impressed the hell out of me, and they have proven that scale does matter.

07:54.480 --> 08:01.440
I mean, now these large language models, if you take language from lexical to

08:02.480 --> 08:09.200
syntactic to semantic to pragmatic levels, they have definitely mastered syntax.

08:10.640 --> 08:19.200
And this is not a small feat. I mean, this is huge. They have proven that if I read

08:20.160 --> 08:26.400
tons of texts written by humans, I can figure out the grammar of language,

08:26.960 --> 08:35.840
and they have done that. That's huge. Okay, so and I don't like here where I don't like people.

08:35.840 --> 08:40.240
I don't want to mention names, but people that supposedly are in my camp, right,

08:41.200 --> 08:49.840
insisting on refusing to see the elephant in the room. No, large language models have proven

08:50.400 --> 08:58.720
that if I ingest terabytes of text, I will figure out syntactic rules. They have done that.

08:59.360 --> 09:07.840
Now, okay, and here's where technically, and you have to admit, I mean, they,

09:09.200 --> 09:14.560
as a matter of fact, they probably know syntax now more than many college graduates.

09:16.480 --> 09:23.040
Okay, these are from data. That's a huge experiment in cognitive science that

09:23.360 --> 09:28.640
no matter how, how, I mean, you can't be religious in this, you have to be scientific.

09:29.600 --> 09:36.960
I see the proof that these large language models by ingesting tons of text written by humans,

09:36.960 --> 09:43.840
they have figured out the syntax of language. End of story. I have, there's an existential proof.

09:44.400 --> 09:48.640
Go on, open AI, try DaVinci 2 or DaVinci 3 or whatever you like to try.

09:49.200 --> 09:57.440
Their syntactic competency is beyond belief. I'm shocked every time I use it. Okay.

09:58.720 --> 10:04.240
Now, that's not the end of language understanding. There's semantics, and then there's pragmatics.

10:04.240 --> 10:11.520
Wow. I mean, so I'm working on something to quantify. So now we have, I don't know,

10:11.520 --> 10:18.480
we're up to a trillion parameter that allowed me to master syntax. Now, let's see semantics.

10:18.480 --> 10:23.600
And semantic can be broken down to, have you guys figured out reference resolution? Have you

10:23.600 --> 10:29.200
guys figured out scope resolution, prepositional phrase attachments? There are so many

10:31.520 --> 10:39.760
pain points and semantic processing. Can we quantify how many more parameters we need to

10:39.840 --> 10:44.160
conquer semantics? And then pragmatics, there are things like,

10:49.840 --> 10:56.640
the teenager shot a policeman and he immediately fled away. Now, possibly both can,

10:57.360 --> 11:03.440
the he can be the policeman. He fled away to escape further injuries. I mean, that can happen.

11:04.240 --> 11:12.880
But most likely the one that's led away is the teenager. That's pragmatics because we know in

11:12.880 --> 11:20.240
the world we live in, if you shoot someone, they're going to try to capture you and you try to flee,

11:20.240 --> 11:26.880
right? That's not semantics. That's way beyond. How many parameters beyond semantics do you need

11:26.880 --> 11:39.360
to capture that? If you can put a, if you can come up with a rough number, I mean, it could be

11:39.360 --> 11:48.400
a number that's manageable, that's doable by more scaling, which would be an interesting result.

11:48.400 --> 11:55.920
But it could be that it's a number beyond the universe we live in, which means guys,

11:57.040 --> 12:01.920
except that you can master syntax and a little bit of lexical semantics, you can figure out the

12:01.920 --> 12:09.040
meaning of some words, but to do full understanding with pragmatics, we're talking about numbers that

12:10.000 --> 12:17.040
we might have to wait 2000 years. Yes, in theory, it works. So basically, I'm trying to work now on

12:17.040 --> 12:24.640
which is going to be very difficult to quantify because they have proven that scale did improve

12:24.720 --> 12:34.160
syntax, no doubt, not improve it. They've almost mastered syntax. But how far can this go? I mean,

12:34.160 --> 12:40.880
can you quantify how far can this go scientifically without saying, let's try with more, let's try

12:40.880 --> 12:48.000
with more? Which is, which is not, it's not going to be easy to do. Anyway. So I'm curious, could I

12:48.000 --> 12:53.040
push back a little bit on the syntax you said, if they've mastered syntax, and I'm kind of,

12:53.040 --> 12:57.200
okay, I mean, I guess you have an existing proof, and there's like a behavioral kind of

12:57.920 --> 13:02.800
proof that it does seem to have very syntactic sentences. And obviously, if you're in something

13:02.800 --> 13:08.160
with whatever billion of parameters, but would you say what those rules are? Could we write them

13:08.160 --> 13:13.280
down? Probably not, right? Because it's a billion different numbers of weights. No, no. And you

13:13.280 --> 13:17.680
don't have the old school kind of generative grammar approach. And also, it's not the way humans

13:17.680 --> 13:21.440
have learned language. And could you reverse engineer or tweak it? We don't know what it is,

13:21.440 --> 13:29.680
it's a black box. So, okay, there's a behavioral tense in which it knows. But isn't it the way

13:29.680 --> 13:36.240
humans do learn language? I mean, I think it's more, though, it's more related to the way humans

13:36.240 --> 13:44.960
learn language than, I mean, I was 20 until I knew grammar. I mean, we use language without,

13:44.960 --> 13:50.640
without knowing grammatical rules. So there is an argument that humans don't learn grammar,

13:50.640 --> 13:55.040
that it's, you know, pretty native. And all we do is tweak a few parameters. And then we add

13:55.040 --> 14:00.400
vocabulary over the years, tweaks or whatever. That's the, you know, the Chomsky and generative

14:00.400 --> 14:08.160
grammar approach. So I don't think that we have a billion parameters that we tweak over 20 years.

14:08.160 --> 14:13.120
I don't think that's how we work. And we have, I mean, there's amazing competency of children

14:13.120 --> 14:17.200
at two years of age, you know, with language they have, you've said, you've, you're writing,

14:17.200 --> 14:22.080
you pointed this out, actually, children know, is absolutely amazing, you know, straight out of,

14:22.080 --> 14:28.880
you know, it is amazing, but it is amazing. And I, and I didn't change the way I think about

14:29.680 --> 14:36.720
and we have innate stuff. But here's the change that these guys have made me

14:39.440 --> 14:45.200
go through. It's a minor change, but it's, it's, it's not that subtle, actually. Here's the thing.

14:45.200 --> 14:53.200
I was told before these new results are coming out that look, we do have innate stuff, which

14:54.160 --> 15:00.720
took us three, four hundred thousand years of evolution. All we're doing by ingesting all

15:00.720 --> 15:11.360
this text is we're simulating, right, these 300,000 years. So give us a chance to simulate

15:11.360 --> 15:21.040
this innateness if you want, in a way, in a way. Okay, I, that argument was said long time ago,

15:21.040 --> 15:28.560
and I thought, come on, you're chasing infinity. What happened with the real difference in my

15:28.560 --> 15:36.480
mind now is they have proven that they conquered one beast in language. Nobody can dispute that.

15:37.120 --> 15:44.240
Can I have a go at disputing it? So, in the Polition and Foda connectionism critique,

15:44.240 --> 15:48.720
they spoke about productivity, you know, the infinite cardinality of language. There was

15:48.720 --> 15:54.160
recently a deep mind paper talking about the Chomsky hierarchy and deep neural network. Well,

15:54.160 --> 15:58.880
I mean, RNNs are regular languages, but, you know, I think transformers and the rest of them are

15:58.880 --> 16:06.080
at the bottom of the hierarchy. So quantitatively, we know we haven't conquered infinity. So why

16:06.080 --> 16:13.360
with such a shallow horizon, are they doing so well? I agree. Here's the thing, language use,

16:14.560 --> 16:23.680
languages infinite, but probably the long tail of probably 90% of ordinary language use,

16:24.400 --> 16:32.560
right, can be figured out from the stuff that we write. So they will never capture all of language.

16:33.520 --> 16:42.800
Yes, but they might reach the level of a competent educated man like us in language competency.

16:44.080 --> 16:50.240
So, all I'm saying is what they have achieved is a huge

16:51.200 --> 17:04.320
result in terms of the big question of scale and big data. They have definitely proved that

17:04.960 --> 17:09.520
if I see enough data, I will learn something and something that's not trivial.

17:11.200 --> 17:15.040
Look, you know where I'm coming from. You're talking Foda and Polition. I mean,

17:15.040 --> 17:20.640
you're preaching to the choir, right? But I have to be a scientist too. I mean, I don't like,

17:20.640 --> 17:26.800
I'm following Gary Marcus, and he's like, I don't like people that minimize what happened.

17:28.240 --> 17:35.120
I'm a scientist, right? I see a big result. I say, wow, right? And look, we're talking, nobody

17:35.120 --> 17:38.720
bashed deep learning more than me, especially large language models. I mean, I'm like,

17:39.360 --> 17:49.280
I was saying this is silly, right? But I have to say they have proven something to me at least,

17:50.400 --> 17:56.240
which is huge because I know how difficult language is. I am impressed equally.

17:57.520 --> 18:01.200
Wouldn't you say it's an engineering, an engineering triumph rather than a scientific?

18:02.160 --> 18:07.040
It's an engineering triumph. But here's the point, Mark. I think it's a little bit more.

18:07.840 --> 18:12.800
That's the only thing I'm trying to, I'm not saying, look, I didn't give up on, I can get to

18:12.800 --> 18:19.520
the criticism later. So don't put me on in that camp yet, right? Or, or ever, right? Because I know,

18:19.520 --> 18:25.840
I know theoretically, theoretically, mathematically, you cannot understand language this way.

18:25.840 --> 18:33.200
All I'm saying is, in terms of cognitive science, what happened and what is happening as we speak

18:33.760 --> 18:44.160
is not nothing. It's a huge, for example, if I can ingest a lot, again, what they prove is,

18:44.160 --> 18:52.480
is that well, the two are related. So it's one thing scale from tons and tons of data. I can learn

18:52.480 --> 19:00.720
something that is not trivial. That to me has been proven. The point I'm making is not the

19:00.800 --> 19:04.240
point I'm making is not that they solve the language problem. Sorry.

19:04.240 --> 19:08.720
Yeah, I want to jump in here a little bit. Because from my perspective, I think

19:09.600 --> 19:16.240
part of why you're saying it's huge is because I think it was a huge step for you personally.

19:16.240 --> 19:20.160
Because I know, you know, from the past, like talking to you, like you've had a much more

19:20.160 --> 19:26.720
extreme view, you know, on the capabilities of large language models than, for example, myself.

19:26.720 --> 19:32.240
Because for me, I don't see anything new here. It's kind of like, I'll give you an example

19:32.240 --> 19:38.560
outside of syntax just for a moment. So just transcription, because that's what Tim and I

19:38.560 --> 19:44.400
happen to be working on quite a bit right now. In other words, transcribing audio into text.

19:46.480 --> 19:51.520
All the state-of-the-art models are pretty much sitting around each other at about 90%

19:52.160 --> 19:58.560
you know, accuracy right of transcription. But here's the thing is that's for people speaking

19:59.520 --> 20:06.480
relatively common languages with a relatively standard accent. Okay, as soon as you bring

20:06.480 --> 20:13.520
someone in the room that has an accent or speaks a, you know, with maybe like some type of a

20:13.520 --> 20:18.640
challenge, like a speech challenge, or this side of the other thing, it becomes garbage again.

20:18.720 --> 20:26.000
And like for Tim and I, or there's noise in the background, music playing in the

20:26.000 --> 20:30.800
background. And as Tim and I have probably hammered, you know, to death and beaten a

20:30.800 --> 20:36.320
dead horse on our channel like so many times, we've never doubted that machine learning can learn

20:36.320 --> 20:42.480
like the bulk of the curve, where it really, really struggles is in all these edge cases,

20:42.560 --> 20:49.760
and the corner cases, and the periphery where it can easily, it's very brittle, right, in those

20:49.760 --> 20:53.840
kind of areas. Like this is the point we've been making out for a long, long time. And so the fact

20:53.840 --> 21:00.480
that like massive trillions of parameters and terabytes of data was able to learn 90% or more,

21:00.480 --> 21:07.680
95% or whatever is syntax. Okay, I get it. Like from a linguist perspective, that was a, you know,

21:07.680 --> 21:12.320
maybe a big triumph or something. But I'm still always about that other like 5%.

21:13.200 --> 21:17.440
And the problem with the approach of deep neural networks is to get that other 5%

21:18.480 --> 21:25.200
is like 100 times as many more parameters, whereas like using, whereas using more generalized,

21:25.200 --> 21:29.280
abstracted, you know, methods that we haven't yet really discovered.

21:29.280 --> 21:33.280
You're hitting it on the nail. And that's why I'm working out on quantifying this because

21:34.160 --> 21:42.320
now we are doing exponential growth in the number of parameters for not even linear growth in the

21:42.320 --> 21:53.520
accuracy, even logarithmic, I agree with you 100%. So that other 10% might require 2000 years of data

21:53.520 --> 21:58.160
that we don't even have. That's what I'm working on. How far can this go because the function

21:59.040 --> 22:05.360
is against them now? Like, I mean, we're increasing GPU power and the number of data

22:05.360 --> 22:12.960
that we're ingesting exponentially for a minute increase in accuracy, which is,

22:12.960 --> 22:17.600
right, that's the end of the logarithmic. And this is, this is part of the announcers that we

22:17.600 --> 22:26.320
have to go through. So look, all my reservations that I had before apply. So I'm being misunderstood.

22:26.320 --> 22:33.360
All I'm saying is simple. These guys, what they have done is not as trivial as I thought initially.

22:34.080 --> 22:40.400
Okay, so let me, let me really be very careful in what I'm saying because now I have a following.

22:40.400 --> 22:48.960
I don't want to lose it. No, I'm not, I'm not changing scientifically where I was. I mean,

22:48.960 --> 22:53.680
science is science. And I know theoretically, I don't get into things like intentionality and

22:54.400 --> 22:58.640
these models understand nothing about the word. I'm talking about syntax only, by the way,

22:59.440 --> 23:06.160
syntax on and some coherence when they patch things together. The coherence is amazing.

23:06.960 --> 23:14.080
They're not patching things together that don't relate at all. So I'm talking about syntax and,

23:14.080 --> 23:19.040
and coherence and syntax. Okay. And a touch of semantics, right?

23:19.440 --> 23:24.240
My point, let me repeat it so that I'm not misunderstood.

23:26.240 --> 23:34.640
They have proven something that many cognitive scientists would never accept, never ever.

23:36.800 --> 23:45.360
But this existential proof has told many cognitive scientists, don't dismiss learning

23:45.360 --> 23:50.720
from data only, blind, no labeling, some aspects of language,

23:52.720 --> 23:57.760
actually very impressive aspects of language. These guys have proven that.

23:58.960 --> 24:06.400
And me as a cognitive scientist, I have to admit because I see it. I see from data alone,

24:07.040 --> 24:10.720
these systems have learned non-trivial aspects of language.

24:11.600 --> 24:17.920
Now, how do you interpret that? Where do you take it? What do you conclude from it?

24:17.920 --> 24:25.600
We can, we can debate that. But all I'm saying is, I have seen something that I never thought I would

24:25.600 --> 24:35.200
see, that just ingesting text in these deep networks, you can actually figure something

24:36.160 --> 24:44.240
not trivial about language. That has been done. I mean, you can, you can say there are pigs,

24:44.240 --> 24:51.360
pigs that fly. Okay. Prove me wrong. I saw them prove they don't exist. Well, I can.

24:52.080 --> 25:01.760
But existential proofs are the most powerful proofs. It's an existential proof, proof by doing.

25:01.840 --> 25:06.400
I'm showing you language competency by ingesting text on it.

25:09.120 --> 25:11.280
So this dismissive

25:17.360 --> 25:27.920
all these are, what is the phrase that Melanie uses? Not Melanie. Stochastic parents. No.

25:27.920 --> 25:29.440
Oh, Bender. Emily Bender.

25:29.440 --> 25:35.600
Emily Bender. No, these are not stochastic parents anymore for me. I am seeing,

25:36.160 --> 25:42.720
look, if I go through the tests on conducting, I have 20 pages of tests on every aspect.

25:45.280 --> 25:48.720
And they get better. I mean, I am seeing things that

25:50.880 --> 25:53.760
lexical ambiguity, they've almost resolved it like,

25:54.640 --> 26:04.160
we were at the baseball stadium last night, we had a ball. They knew that ball is not the baseball.

26:05.520 --> 26:12.240
I'm seeing things like, what the hell is this? And if anybody can test these systems,

26:12.240 --> 26:18.320
I can with all humility. I'm trying my best now to make them fail, which was not the case just

26:18.400 --> 26:24.480
a month ago. All I'm saying is I'm seeing something that I never thought I would see

26:25.840 --> 26:28.560
as a cognitive scientist, as a computation linguist.

26:30.720 --> 26:37.440
Let me put it this way. To see this capability now, you have to bring back Montague,

26:38.240 --> 26:47.840
Frigge, Marvin Minsky, John McCarthy, all the pioneers of logic and AI, put them together

26:48.480 --> 26:53.680
and give them a thousand bright engineers. And they will not do this.

26:54.880 --> 26:59.520
In a minute, we're going to get onto your book review, but you are just alluding to the problem

26:59.520 --> 27:05.040
of semantics and pragmatics. And also I want to bring in symbol grounding as being the next

27:05.040 --> 27:08.640
potential brick walls. Could you just talk to that a little bit more?

27:09.360 --> 27:18.400
Well, look, symbol grounding was an issue in symbolic systems. You're using symbolic systems.

27:18.400 --> 27:28.480
So you're saying cat, CAT, it's reference based semantics. So I'm going to use CAT to refer to

27:28.480 --> 27:36.880
a concept called CAT. And then the concept called CAT is a frame in most systems, in frame based

27:36.880 --> 27:43.440
systems with properties. It's a mammal. It's a thing that has this and this kind of fair whiskers,

27:43.440 --> 27:50.480
blah, blah, blah. It's the intention of what a CAT is. And then symbol grounding came like, okay,

27:50.480 --> 27:57.600
you're defining CAT as symbol in terms of symbols. Like, so where do we go? It's like a

27:57.600 --> 28:04.240
dictionary to read the definition of a word. I have to know all the words. So I might go and

28:04.240 --> 28:10.560
so it's a cyclical representational system. It's not grounded in anything in the end.

28:10.560 --> 28:15.520
It's a closed. Basically, it's a system that defines itself, like what the hell's going on here,

28:15.520 --> 28:22.400
right? Symbol grounding was CAT has to be associated with something real outside.

28:22.400 --> 28:25.920
That's a real CAT. In symbolic systems, we don't have that, right?

28:28.720 --> 28:35.120
We can get into symbol grounding. It's a huge subject on its own, like where do meanings,

28:36.320 --> 28:44.160
where do words get their meaning from? Is it embodied? Is it experiential? Does it have to be

28:45.040 --> 28:54.480
can a deaf and a blind person ever understand the meaning of something? So that's a huge...

28:57.520 --> 29:02.640
I mean, we spoke to Andrew Lampinen and he was getting into Pierce's triad semiotics,

29:02.640 --> 29:08.240
this embodied relativistic notion of grounding, which actually I'm developing a bit of a taste

29:08.240 --> 29:12.640
for personally, but you're very skeptical about that. Could you just sketch that out?

29:12.800 --> 29:18.080
I don't think that's the issue grounding. I mean, people make a lot of it and like our

29:18.080 --> 29:24.880
common friend, Bishop, Mark Bishop, that you will never understand the meaning of something if you

29:24.880 --> 29:30.960
don't live in the environment and it's... That has never been... I don't believe so. That's why we

29:30.960 --> 29:35.600
call it artificial intelligence, right? I mean, we're never going to have the intelligence of a

29:35.600 --> 29:42.000
human being. We're never going to have a robot that really chokes when they see their nephew

29:42.000 --> 29:47.040
after six years, right? I mean... And that's... That was never the one. That's why we're building

29:47.040 --> 29:53.520
artificial intelligence, not human intelligence. So this whole argument about grounding and

29:53.520 --> 29:59.440
embodiment and I will never understand what pain is because a robot will never really feel pain.

29:59.440 --> 30:04.320
That to me, that's besides the point. I'm not building artificial life. I'm building an

30:04.320 --> 30:10.160
artificially intelligent machine that will do things in a way that you would say, what the hell

30:10.160 --> 30:17.200
was that? Probably that's how AR should be defined. That's it. What the... Who did this, right? That's it.

30:17.840 --> 30:24.080
It feels pain or it doesn't feel pain or it will never know what crying is, like so.

30:26.320 --> 30:35.760
So at least I come from this angle. I'm not into building artificial humans. I'm an engineer.

30:35.840 --> 30:42.960
I'm into building artificial intelligence systems. Systems that can reason, right? In the environment

30:42.960 --> 30:49.040
we live in, solve problems intelligently and problems that usually require human intelligence.

30:49.040 --> 30:54.720
Like I'm into... I would like to see a world where we don't have accountants. Come on. We don't have

30:54.720 --> 31:02.560
doctors. I open my mobile. I have a doctor. I converse with them intelligently and they tell

31:02.560 --> 31:07.920
me exactly what to do. Done. Nobody goes to medical school anymore. Everybody should write

31:07.920 --> 31:14.400
poetry and play music and enjoy the beach. That's it. That's the AI I'm interested in. We will never

31:14.400 --> 31:23.680
build robots that will understand love. So to me, these are arguments that

31:25.520 --> 31:30.160
they're irrelevant. We're building artificial intelligence. When we did calculators, we never

31:30.160 --> 31:37.040
gave a damn how we do it in the mind. And we have calculators that can beat any mathematician

31:37.040 --> 31:45.440
in doing a division of two prime numbers, each of which is 20 digits. Yeah, I think it's more

31:45.440 --> 31:51.520
where your area of interest is. I mean, so yours is in the engineering. And what's coming to mind

31:51.520 --> 31:58.640
right now is our conversation with Professor Chomsky where he said, yeah, these are great

31:58.640 --> 32:03.360
feats of engineering. I mean, I like bulldozers too. They just don't have anything to do with

32:03.360 --> 32:10.720
science. Sure, or philosophy for that matter. I mean, I think some of these questions,

32:11.520 --> 32:16.080
yeah, maybe they don't have a lot to do with building AIs that do a bunch of useful things.

32:17.200 --> 32:24.080
But they have a lot to do with philosophy or science or whatever. Mathematics for that matter.

32:24.080 --> 32:32.640
This is a bridge to the book because the authors were misunderstood from their title,

32:32.640 --> 32:38.160
and I told them that privately. No, not privately. I want to tell them that because

32:39.360 --> 32:45.120
I got comments from people privately that the title is misleading. The title assumes they are

32:45.120 --> 32:50.880
anti AI, not really. They're saying roughly what I'm saying. I'm not interested in building an

32:50.880 --> 32:57.840
artificial human. We can never do that problem. Right. So all this, and this is important because

32:57.840 --> 33:06.480
people are trivializing. I mean, you have people talking about AGI from five years ago. And all

33:06.480 --> 33:12.880
we had was something that can do amazing pattern recognition. That's it. So it's important for

33:12.880 --> 33:20.480
us to say, Hey guys, cool it down. Do you know what you mean when you talk about machines that

33:21.520 --> 33:29.200
surpass human intelligence? This is not just a word you throw out, because you're impressed with

33:30.240 --> 33:36.960
a system that can recognize cats from dogs. Come on, take it easy. Slow down, right? And this book

33:36.960 --> 33:43.440
is about that. It's like, do you know what it means to have a system that can feel and

33:46.160 --> 33:52.720
react instantly real time to changing situations around them? And do you know what you're talking

33:52.720 --> 34:00.320
about? So yeah, I'm interested in the engineering side of AI. And that's what makes me impressed

34:00.880 --> 34:13.040
by something like a DaVinci 2 or DaVinci 3 as an AI enthusiast. I look at this and I say,

34:13.040 --> 34:19.680
wow, we've never been able to reach this milestone. This is a huge milestone. That's how I look at

34:21.280 --> 34:26.000
will it be, will it be the solution for the language understanding problem? No,

34:26.960 --> 34:32.000
because language understanding in the full sense of the word understanding,

34:33.040 --> 34:38.080
the way we speak now, the way we were speaking now involves a lot more than mastering syntax,

34:38.080 --> 34:49.360
but they did master a big aspect of language. And I can see it. I can try it. And I'm trying to

34:49.360 --> 34:57.360
make it fail now in syntax and even some coherence, some mild, let's call them mild semantics.

34:59.600 --> 35:10.560
And it's very impressive. So the question now becomes for AI researchers, not just engineers, is

35:10.880 --> 35:22.000
this scalability scalable? Is this scalable? Is this approach scalable?

35:25.120 --> 35:32.880
So much data and so much compute power, they mastered syntax more or less. I think they did

35:32.880 --> 35:40.320
at least as much as a competent language user. So my first question is like,

35:40.320 --> 35:45.120
do you see natural language as essentially computable as in like cheering machine computer?

35:45.120 --> 35:48.720
Or do we need some other kind of new mathematics to describe it? For example,

35:48.720 --> 35:53.040
hypercoputation, whatever that might be. In other words, is there a generative grammar or

35:53.040 --> 35:56.240
algorithm or set of rules that generate our valid sentences?

35:56.240 --> 36:03.600
When it comes to language itself, I think language is a formal language.

36:04.720 --> 36:13.520
There is a compiler for natural language that can be built, like we have built one for Java,

36:13.520 --> 36:21.680
C sharp. So this is Montague, yeah. Yeah, I believe Montague was right,

36:21.680 --> 36:28.800
although Montague was was attacking the semantics part. Okay, he touched a little bit on intention

36:28.800 --> 36:36.960
and then not much on pragmatics, but Montague and it took me years. And I had to be advised by a very

36:36.960 --> 36:42.960
smart philosopher, logician, that stop saying Montague didn't deal with this. Montague was never

36:42.960 --> 36:48.800
in the business of reference resolution. That's pragmatics. Montague was trying to prove

36:49.760 --> 36:55.920
there is a formal system and algebraic system. And he used lambda calculus, strongly typed system

36:56.480 --> 37:03.520
that I can use to compose language like I do with arithmetic or calculus or anything. There's

37:03.520 --> 37:09.920
a logic that are mathematics for language, which is a huge thing. Montague was not a trivial

37:11.520 --> 37:14.160
semanticist in the history of language. He was huge.

37:14.880 --> 37:19.680
So would you say that Montague is doing for language semantics of language,

37:19.680 --> 37:25.760
what chance he did for syntax of language? Exactly, exactly. And the common denominator

37:25.760 --> 37:30.720
interesting between them is someone that Chomsky himself admires a lot Barbara Partee,

37:31.440 --> 37:37.360
who was he did her PhD with Montague. She's a Montegoian, Montague semantics.

37:38.160 --> 37:44.240
But she and she, she did say almost the same phrase. He said what Montague did for semantics

37:44.240 --> 37:49.760
is equivalent to what Chomsky did for syntax. Yes. Okay, exact, almost exact phrase.

37:51.520 --> 37:56.720
You think he was right about semantics? Yes. So for example, there's a computable definition

37:56.720 --> 38:02.480
of what is a pile of sand. Right. No, no, no, no, no, no, no. I'm not sure you're right about that.

38:02.480 --> 38:08.640
No, no, no, no. Hold on. Let's not get Montague was not a psychologist or an ontologist or

38:09.440 --> 38:16.560
he said whatever your meaning for something is. Okay. He didn't even care. Montague never

38:16.560 --> 38:22.080
did ontology and conceptual and like, what, how do you define the meaning of what is a book?

38:23.680 --> 38:29.040
Look, let's, and took me, I'm telling you, took me three years to appreciate what Montague was

38:29.040 --> 38:34.400
doing. And I, and my thesis was on Montague semantics, the masters before the PhD. Here's

38:34.400 --> 38:40.880
what Montague did, Keith, and you'll appreciate Montague said, whatever your meaning for the

38:40.880 --> 38:50.000
individual words, the lexical meaning. So cat means see, okay, you go with your psychologist and

38:50.720 --> 38:54.560
cognitive scientist and ontologist and disagree about the meaning of a cat.

38:55.520 --> 38:58.880
Finally, you come to me and you say, we have a meaning for cat and it's see.

39:00.720 --> 39:06.080
Follow me. Montague never cared about what is the nature of things outside. Right.

39:06.880 --> 39:14.320
Whatever your meaning for these individual concepts are. Right. Here's how you make,

39:14.320 --> 39:21.840
you get the meaning of a whole mathematically. I'll give you a simple example that will make

39:21.920 --> 39:31.360
you appreciate what I'm talking about. John refers to a person. Right. The neighbor next door

39:32.560 --> 39:38.720
refers to a person. The neighbor next door that just moved from California refers to a person.

39:40.320 --> 39:48.000
The neighbor next door that knows John very well and drives for the LTD is a person.

39:48.800 --> 39:56.080
All of how can you have this phrase and John refer to a person and have the same semantic type

39:56.640 --> 40:03.360
composition in a way that never fails. Like you do in arithmetic, he wanted to prove that

40:03.360 --> 40:10.640
natural language is a formal language. He developed a semantic algebra that makes this long phrase

40:10.640 --> 40:16.000
referred to the in the end to an object that has the same semantic type as John mathematically.

40:16.000 --> 40:26.320
If you do it, it never fails. The details of this were genius. Okay. So Montague then made the big

40:26.320 --> 40:32.880
claim natural language is a formal language. Give me some time. I'll work out the full algebra.

40:34.080 --> 40:39.040
You go then and decide what the individual meanings are. I don't care. Montague never gave a damn

40:39.040 --> 40:45.040
about cognitive science and knowledge and he was a logician. He wanted to prove

40:45.680 --> 40:51.440
there's a calculus underneath natural language. Calculus of meanings. You decide on the meaning.

40:52.000 --> 40:56.160
I'm telling you, it took me a while. I thought he's doing semantics. What is the meaning of this

40:56.160 --> 41:02.720
and Montague? He said he never cared. He was doing an algebra of meanings, regardless of what the

41:02.720 --> 41:17.440
meanings are. Okay. But his project was huge. Montague was trying to prove there's an algebraic

41:17.440 --> 41:23.440
system behind language, like any other formal language. Like you can get an arithmetic expression

41:23.440 --> 41:28.880
and build a tree for that, evaluate it, and get the final meaning. Natural language works the same

41:28.880 --> 41:36.720
way. Except it's not that simple. That's all. So his project was huge and he was misunderstood.

41:38.480 --> 41:44.320
So he was really doing semantics. That's semantics. Pragmatics is a different thing.

41:45.440 --> 41:51.840
What do you think is the core unique property of natural human language? Would you agree with

41:51.840 --> 42:01.440
Chomsky on it being digital infinity? Yeah. Okay. The infinite thing in the productivity.

42:04.000 --> 42:13.360
To me, no, it's I'm half Chomsky and half something else. To me, no, the real,

42:13.360 --> 42:20.000
real unique thing about language. And that's why even if Montague succeeded, that's half the battle.

42:20.960 --> 42:29.280
It's not in the semantics, although that's huge. To me, it's the pragmatic side, the abductive

42:29.280 --> 42:35.920
inference. I mean, we use induction and we use deduction and we always ignore abduction.

42:35.920 --> 42:44.560
Abduction is the unique, is the humanly unique reasoning capability. I mean, rats do inductive

42:44.640 --> 42:50.880
reasoning. They, to a certain extent, that's how they learn a few things, inductively, really.

42:52.720 --> 42:58.640
All the lower species do inductive reasoning to a certain extent. And some of them do some

42:58.640 --> 43:04.400
deductive reasoning, if there's then this, but at a very shallow level, of course. Abductive

43:04.400 --> 43:12.080
reasoning is uniquely human. And that's the part of language understanding, which means

43:12.080 --> 43:19.120
reasoning to the best explanation. Abductive reasoning is I reach a conclusion, not inductively

43:20.000 --> 43:28.080
by induction or, and not deductively, I deduced it. But I reached this conclusion because it's

43:28.080 --> 43:37.520
possible, it can happen. And it is the best conclusion I can come up with, given everything

43:37.520 --> 43:44.560
else I know. Abductive reasoning is the real reasoning methodology that makes us unique as

43:44.560 --> 43:51.600
human. We reason to the best, we reason, it's called reasoning to the best explanation. Right? So

43:52.240 --> 44:00.400
that's, that's, Pierce and others, I mean, Pierce was the pioneer of abductive reasoning or

44:00.400 --> 44:08.240
abduction. But I'm talking about an abduction has come to have two sort of tracks. And there's

44:08.240 --> 44:14.560
abductive reasoning in the traditional philosophical charts, Pierce. But there's abductive reasoning

44:14.560 --> 44:22.800
as it used to be called in the 80s, when case-based reasoning came out and expert systems who,

44:23.760 --> 44:30.560
there was something called EBL, explanation-based learning. And it was even a learning technique,

44:31.120 --> 44:39.360
which is really reasoning to the best explanation. Basically, I have to make a decision. Actually,

44:39.360 --> 44:45.680
Jerry Hobbs, you guys heard me mention his name several times before, who's, I think, huge in

44:45.680 --> 44:54.720
semantics, has a paper when he was at SRI with other luminaries too. The title is interpretation

44:54.720 --> 45:00.960
as abduction or understanding as abduction. And basically, he shows how all the difficult,

45:02.240 --> 45:06.720
all the challenges in language understanding beyond semantics. So we're done with Montague.

45:06.720 --> 45:13.120
Now I'm doing the final understanding of what makes sense given, because every expression has

45:13.120 --> 45:19.600
several meanings. Even if I did the semantics perfectly, I have to choose the most plausible

45:19.600 --> 45:25.280
meaning from all the possible meanings. That's pragmatics. And the way you do that very well

45:25.280 --> 45:33.040
is in language. We do abductive reasoning. We say, I'm left with three meanings, three possible

45:33.040 --> 45:40.960
meanings, syntax excluded, 200 syntax trees, semantics excluded, few invalid semantic expressions.

45:40.960 --> 45:46.160
And I'm left with three still, three possible meanings. They can all happen in the world we

45:46.160 --> 45:53.280
live in. Which one is the most plausible? We do this abductively. Which meaning is the most

45:53.280 --> 45:59.200
likely meaning given the context and what I know? That's the last challenge in language.

45:59.920 --> 46:05.520
So we need to, we need to add the abductive model, which we humans do. I go back to the

46:05.520 --> 46:11.280
teenager shot of policemen, both meanings, both interpretation can happen, right?

46:12.640 --> 46:20.320
Either one can flee, right? But most likely it's the teenager that fled away, given what I know

46:20.320 --> 46:24.800
and given that's abductive reasoning. But semantically both can happen.

46:25.760 --> 46:31.280
Yes, I do want to emphasize something that Wally like briefly mentioned, but I think it's very

46:31.360 --> 46:38.000
important to mention is that there's two senses of abduction. And they differ in the

46:38.000 --> 46:42.320
following way, which is kind of the more modern sense, which is what Wally's been talking about

46:42.320 --> 46:49.920
like pretty much this whole time, is abduction used to justify hypotheses. But the older and

46:49.920 --> 46:56.080
original sense of it and still an equally important one is abduction for generating

46:56.800 --> 47:03.440
hypotheses. And this ability to generate hypotheses is something that's extremely

47:03.440 --> 47:09.760
powerful and so far uniquely human. But generate from... Hold on, let me just finish here.

47:09.760 --> 47:15.120
Which is this, this is like something where Einstein is just sitting there pontificating on

47:16.720 --> 47:21.760
how the heck can light be the same no matter how the earth is moving and blah, blah, blah,

47:21.760 --> 47:29.840
and comes up without a thin air, like this hypothesis that relativity applies, right?

47:29.840 --> 47:36.240
That the physical laws are the same no matter what your reference frame is. So this ability

47:36.240 --> 47:40.960
to almost... That people talk about sort of pull from thin air, this kind of intuitive

47:41.600 --> 47:50.240
leap to something that ends up being like a grand new theory, that's also abduction.

47:50.240 --> 47:54.080
Right. But in both cases, Keith, and I agree with you, that's

47:56.400 --> 48:03.760
the old view of what abductive reasoning was to scientists. But in both cases, you're choosing

48:03.760 --> 48:10.960
from possible... Oh, no, no, no, just a minute because this is where I think I probably quite

48:10.960 --> 48:16.160
disagree with you, which is the modern sense of abduction to me is much more similar to just

48:16.160 --> 48:21.280
inference like to a Bayesian. So in other words, you give me a whole slew of hypotheses and I can

48:21.280 --> 48:27.600
tell you which hypotheses should be preferred just on the basis of marginalization and strict,

48:27.600 --> 48:32.560
like Bayesian theory, no problem with that. It's not actually abduction, it's just inference,

48:32.560 --> 48:39.920
right? Just rules of inference. Whereas just a minute, generating that space

48:39.920 --> 48:45.760
in the first place is unique and very different from inference, like the ability to produce

48:46.880 --> 48:52.400
from nothing models to consider, that's the core of abduction from my point of view.

48:54.160 --> 48:59.680
But okay, so we're saying the same thing, but indifferent. These possibilities that you generate

49:00.480 --> 49:05.280
are valid possibilities. So abductive reasoning... I don't know if they're valid until I do the

49:05.280 --> 49:12.880
inference. No, you're generating a pool of possibilities. That's the step, generating a pool

49:12.880 --> 49:18.000
of possibilities. Fine, fine, fine, fine, but in the end... How do you do that? Keith, I think we're

49:18.000 --> 49:24.160
saying the same thing, it's just a terminology. In the end, you're choosing from a set of possible

49:25.040 --> 49:34.960
valid hypotheses. Induction is, you don't know where you're going until you get there. In abductive

49:34.960 --> 49:41.760
reasoning, you are, whether it's the old way or the modern way, in the end, what's common between

49:41.760 --> 49:50.160
them is, I have a set of possibilities. I will use abductive reasoning to decide which is the most

49:50.160 --> 49:55.360
plausible. In a sense, you're scoring them, and you're saying, from all these possibilities, this

49:55.360 --> 50:00.880
is the most plausible. Yeah, but see, you keep assuming the... You keep positing that you have

50:00.880 --> 50:05.280
a bunch of possibilities, and I'm saying those possibilities have to come from somewhere,

50:05.280 --> 50:10.560
and where they come from is abduction. Oh, okay, it depends on the domain and language. They come

50:10.560 --> 50:16.400
from what we know is true. Okay, I see your point. Where they come from depends on the domain of

50:16.480 --> 50:23.440
reasoning. In many cases, they come from what we know is true, or they come from evidence.

50:24.560 --> 50:28.640
Yeah, I guess it's just important to know there are these two senses of abduction,

50:29.440 --> 50:32.160
and don't forget about both of them, because they're both...

50:32.960 --> 50:40.320
Right, and that's why abduction, like induction, as opposed to deduction, abduction and induction

50:40.320 --> 50:45.280
are both approximate. You can never have 100%, because in the end, you're assigning a score,

50:45.280 --> 50:52.000
you're saying. So, both of them are probabilistic in a way, or they have a certain uncertainty.

50:52.960 --> 50:58.560
So, when you're doing abductive reasoning, even in language, I make a decision as this is the right

50:58.560 --> 51:03.920
interpretation given the context, but it's what we call... Could be wrong. You might have eaten

51:03.920 --> 51:08.400
a ball out of this all year. Exactly, and that's why when I read further, I change my first

51:08.400 --> 51:17.600
interpretation. In language, it's not monotonic, actually. We do non-monotonic reasoning in the

51:17.600 --> 51:27.280
sense that I might override my first decision. But all of that is pragmatics, and we do this

51:27.280 --> 51:32.880
in conversation. Two, three sentences after, I understand really fully what you said before,

51:32.880 --> 51:38.720
because I remade the interpretation. And Waleed, do you have any thoughts on where

51:39.760 --> 51:44.480
this came from, or basically the evolution of language, or if you like the evolution of this

51:44.480 --> 51:51.840
abductive athlete? Do you have any ideas, or is it unique to humans? It seems it is.

51:53.840 --> 51:59.920
Unique to humans, definitely. I mean, animal language, animal symbolic languages have been

51:59.920 --> 52:06.160
studied thoroughly. And two things, here's where the genius of photo comes in, productivity. I mean,

52:06.720 --> 52:11.520
language have a finite set of symbols, and they're not productive. They don't do compositions.

52:13.920 --> 52:17.360
And this ties to... Is it animals? Your time up?

52:19.840 --> 52:27.680
No animal, no non-human animal has a productive language. In other words, I have a set of symbols,

52:28.320 --> 52:34.720
and if I can compose them, I can make a new symbol. Language, animals don't compose things,

52:34.720 --> 52:39.040
because they don't decompose them when they're done. They have a finite... It's a hash table.

52:39.040 --> 52:43.760
If I make this symbol, I mean this. If I make this... Okay, no matter how sophisticated it is,

52:44.560 --> 52:48.880
because they don't have recursion, they don't have infinity, they can't deal at that level

52:48.880 --> 52:54.960
with complexity. Some of them have a larger lexicon than others. Okay, but that's still the same

52:54.960 --> 53:02.720
paradigm. So, productivity, in other words, this capacity to learn, we were just talking about John,

53:02.720 --> 53:09.840
or the neighbor next door, or the neighbor next door that just came from California. I can

53:09.840 --> 53:17.120
productively make a person out of three sentences, and in the end, they collapse to a John, right?

53:17.120 --> 53:23.600
That productivity doesn't exist in any species except humans, which means compositionality,

53:23.600 --> 53:30.640
which means systematicity, which means all of that. So, it's unique to human, definitely. This

53:30.640 --> 53:36.720
has been established, and it came with thought. That's the if and only if. That's why we're the

53:36.720 --> 53:41.760
only species that really reason. I mean, okay, I have people insist that animals think and they

53:41.760 --> 53:48.480
reason. They're not really reasoning, okay? Only humans reason, and thought and language came

53:48.560 --> 53:54.960
together. It's sort of like a phenomenon. There are some, there's some proof, even anthropologists,

53:54.960 --> 54:06.480
and they say it looks like language was detected when tools and some basic machinery was detected

54:06.480 --> 54:15.200
first. So, the human mind at some point had this capacity to think and language came with it. It

54:15.200 --> 54:21.920
was like almost at the same time. So, it's uniquely human, definitely now. Where did it come from?

54:23.440 --> 54:37.440
Wow. I think it was the need really to express thoughts. Like at some point, we started having

54:37.440 --> 54:44.320
thoughts that we want to communicate. So, the external artifact we see outside, whether it's

54:44.400 --> 54:53.840
English or ancient Greek or Latin, languages evolve for societal reason and all that. But the

54:53.840 --> 55:01.840
external artifact that we use to communicate thoughts came out of the need of the internal

55:01.840 --> 55:08.640
language that started to develop. What Fodor calls it, the language of thought, mental ease.

55:09.600 --> 55:17.920
And we, so we had that thing going on inside and then we had to communicate. We started with

55:17.920 --> 55:24.480
weird sounds and then we scribbled things on the wall to communicate. And then that thing developed

55:24.480 --> 55:32.080
until we started making symbols like, okay, if I say this, that means this. I don't know the

55:32.080 --> 55:40.160
exact process. I'm not a biologist or evolutionary linguist or, but I think thought is the key here.

55:40.160 --> 55:46.880
So, there's a language of thought. And these external things are because linguistic research

55:46.880 --> 55:52.640
has also shown that there are many universals in language, regardless of what the language is,

55:52.640 --> 55:58.960
even if they are completely different systems like Asian languages and Latin-based languages.

55:59.520 --> 56:07.040
They all have a verb, an action. They all have objects and agents of the action.

56:07.840 --> 56:14.800
They all have events and events have duration, time and place. So, there are a set of cognitive,

56:14.800 --> 56:21.760
I call them universal cognitive primitives, right? There's always an object there somewhere,

56:21.760 --> 56:26.240
or an agent of an activity. Now, how you express it in different languages,

56:26.720 --> 56:33.040
these are universals. That's the language of thought. That's the internal language,

56:33.040 --> 56:40.240
which has to be the same. And objects have properties and all that. So, there are universal

56:40.240 --> 56:49.120
primitives. And we instantiate them in different languages differently, but that's to me secondary.

56:49.840 --> 56:54.640
Okay. Okay. That's great. William, could you talk a little bit about your recent overview?

56:56.560 --> 57:05.840
A colleague that I never worked with, but a colleague in the field. To review this book,

57:05.840 --> 57:11.040
and I looked at it and I said, oh, I have enough on my plate. This is not an easy book.

57:13.280 --> 57:18.880
But then I, because I liked it, I said, yeah, I'd like to write it. And in the end,

57:18.880 --> 57:25.920
it turned out to be not as technically involved as I thought. It's sort of,

57:27.200 --> 57:32.400
and I'm saying that not to be negative, but it's sort of the same argument over and over.

57:32.400 --> 57:38.960
The gist of the argument is quite simple, actually. And they try to prove it from different vantage

57:38.960 --> 57:43.920
points than in the book, from a biological, sociological, psychological, mathematical.

57:43.920 --> 57:54.640
But the gist of the book is any talk of AGI is wishful thinking. And it's beyond anything we

57:54.640 --> 58:04.800
can ever develop mathematically, so as to engineer it in any, in any realistic way.

58:06.160 --> 58:13.120
They make good arguments throughout. There are many examples of the basic idea is that

58:14.880 --> 58:22.480
all the mathematics we know, right, mathematics available to us, cannot model

58:24.480 --> 58:31.200
not just the entire mind, but even subsystems in the mind, language being one of them.

58:33.360 --> 58:38.800
And so it's all complex systems within complex systems in a complex environment,

58:39.280 --> 58:45.360
the system around us that we interact with. And none of it can be modeled mathematically,

58:45.360 --> 58:52.320
none of it even at any level. So forget doing AGI that can interact with us in an intelligent way.

58:53.280 --> 59:00.800
Now, you can do controlled narrow AI, right? You can build very intelligent machines that can do

59:00.880 --> 59:10.480
amazing stuff. But any talk of AGI, strong AI, is just talk until, unless, and they admit that,

59:10.480 --> 59:19.120
unless we come up with a new mathematics that we never even knew at the scale of Leibniz calculus

59:19.120 --> 59:25.280
or Newton, like we're talking about a new mathematics that we never conceived of, right?

59:25.600 --> 59:33.200
Which they say most likely all evidence says that's not going to happen, right? So

59:34.560 --> 59:43.440
now you can get into why. So that's their claim. And why? They say that all these systems are

59:43.440 --> 59:53.520
complex systems. And in complex systems, the idea is that these are, first of all, dynamic systems.

59:53.520 --> 01:00:01.680
They work in a dynamic environment. They are continuously evolving and adapting, right? They

01:00:01.680 --> 01:00:09.600
are self feeding systems. These are not systems that only take input output. These systems change

01:00:09.600 --> 01:00:16.080
their behavior. And I gave an example from list. These systems are systems that change their behavior,

01:00:16.080 --> 01:00:24.640
their algorithms, if you want, they change their mind from a stimulus. So I might, and that's why

01:00:24.640 --> 01:00:29.600
I said they, I would have liked to see a discussion on the frame problem, because the frame problem

01:00:29.600 --> 01:00:38.480
in the AI is about this. How can I reason in a dynamic and uncertain environment and react

01:00:39.040 --> 01:00:45.120
dynamically, although what I do in the environment might affect what I believe about the environment

01:00:45.120 --> 01:00:50.480
in real time. And they're right. There is no mathematics we know of now. That's why we don't

01:00:50.480 --> 01:00:57.440
have a solution for the frame problem. So this kind of cyclical cause and effect

01:00:58.640 --> 01:01:02.640
cannot be modeled by anything we know on mathematics. And this I agree with them.

01:01:04.880 --> 01:01:10.160
They give an example. I made just an example in language, for example. Language, we know.

01:01:10.720 --> 01:01:17.920
If I have a dialogue, okay, we all agree that the interpretation of any occurrence requires

01:01:17.920 --> 01:01:24.160
having the context in mind as part of the, part of the input to the evaluation of the meaning

01:01:24.160 --> 01:01:32.000
is the context as an extra parameter, right? Now, the context is changing based on something I

01:01:32.000 --> 01:01:40.480
cannot predict, which is the response of some participant in the dialogue. There is no meaningful

01:01:40.480 --> 01:01:47.360
way of predicting how someone might respond. So in other words, the context is mathematically

01:01:47.360 --> 01:01:52.560
not defined, but I need it in the interpretation. Thus, no language understanding, no language

01:01:52.560 --> 01:01:58.080
understanding, no AGI, because they believe language understanding is a prerequisite. So the,

01:01:59.040 --> 01:02:04.960
their conclusion, I mean, you can question every step in this inference they come to,

01:02:04.960 --> 01:02:09.600
but they give language as an example, but we have social behavior, I can give an example.

01:02:09.600 --> 01:02:15.440
They have a nice example in social behavior. Here's an example of a complex system that cannot,

01:02:16.160 --> 01:02:23.840
we don't have any mathematics that can model. We're staying in a queue, in a clinic, an emergency

01:02:24.800 --> 01:02:31.520
room. What do they call them? These ER. So, but there's a queue because they all have

01:02:31.520 --> 01:02:39.280
emergencies, right? Now, the social behavior, then the social norm is that in the queue,

01:02:39.280 --> 01:02:45.040
okay, we all have, we all have urgent issues. But in the end, I came first, right? Okay, so that's a

01:02:45.040 --> 01:02:56.880
social norm. And, but can a robot understand that if someone fainted, really, I mean, it's almost

01:02:56.880 --> 01:03:08.160
gone, right? Our social norm accepts that this person violates the queue order, right? This is

01:03:08.160 --> 01:03:16.640
something dynamic that happens, like the queue is this way. And how can a robot update the rules

01:03:16.640 --> 01:03:22.720
and not kill someone because they violated the order of the queue? In other words, these interactions,

01:03:22.720 --> 01:03:30.640
these cyclical cause and effect are very complex, that no mathematical model. Or the example I said

01:03:30.640 --> 01:03:38.000
in language, they prove this cannot be done. Context is needed to interpret everything. I cannot

01:03:38.160 --> 01:03:44.960
predict what the context will be because I cannot predict you respond to my, so it's unpredictable,

01:03:44.960 --> 01:03:50.320
they call it erratic, almost random. So there is no mathematics that can model it.

01:03:52.160 --> 01:03:56.640
And there are many aspects to the mind, whether it's social reasoning, language, and then they

01:03:56.640 --> 01:04:06.080
conclude there cannot be a system that we can model on volume and machines, because we don't

01:04:06.080 --> 01:04:12.240
have the mathematics to model it. And these, they go into deep learning. And they give examples

01:04:12.240 --> 01:04:17.360
even like deep learning, no matter how much data you ingest, you can never predict the future.

01:04:18.640 --> 01:04:26.320
You're lucky if you can do a good job on the past and even forget the future. And definitely

01:04:26.320 --> 01:04:32.880
forget the, sorry, the present. So definitely forget. Can I jump in for a minute because I

01:04:32.880 --> 01:04:39.680
have a couple of comments. So one is, would you agree that this is quite synonymous with,

01:04:39.680 --> 01:04:44.960
you know, Douglas Hofstadter's strange, strange loops and the whole like random reference, the

01:04:44.960 --> 01:04:49.600
self-referential self systems, because I mean, complex systems, a big part of them is they

01:04:49.600 --> 01:04:55.280
usually are, they do have feedback loops. And at some scale, they become so they will involve

01:04:55.280 --> 01:05:01.840
self reference. Yeah. Okay. My other, my other point I want to make is this, you know, I have

01:05:01.840 --> 01:05:07.680
quite a bit of sympathy towards the viewpoint, right, of this, of this book that you're talking about,

01:05:07.680 --> 01:05:13.440
with one exception, which is I'm still optimistic that we can discover a mathematics that may help

01:05:13.440 --> 01:05:20.000
us out. And so I always think to the foundation series by Isaac Asimov, because in there, they

01:05:20.000 --> 01:05:26.080
discover a science in a mathematics called psycho history, which at least allows them to predict

01:05:26.160 --> 01:05:32.080
complex systems of a certain scale and larger. So in the book, it's sort of like planet scale

01:05:32.080 --> 01:05:37.680
and larger, they're able to actually predict, you know, these complex sociological systems

01:05:37.680 --> 01:05:43.280
and human behaviors, and how they're going to interact like beyond, beyond that scale. And

01:05:43.280 --> 01:05:47.600
it's really fascinating. I make that point. I highly recommend that series to anybody,

01:05:47.600 --> 01:05:52.960
because it's very fascinating because, you know, they talk a lot about sort of what if you had

01:05:52.960 --> 01:05:58.400
the science, what might it look like, etc. And in there, there's like this little tiny microscopic

01:05:58.400 --> 01:06:04.000
thing that's beyond the predictability of psycho history that comes in and kind of mucks up the

01:06:04.000 --> 01:06:09.680
works and creates anomalies that they have to constantly keep combating against. So I think

01:06:09.680 --> 01:06:14.800
if anybody wants a fictional take on a possible mathematics of this, like, I would recommend

01:06:14.800 --> 01:06:21.440
Yeah, I make this point. I say, I agree with their argument. We're trying to model complex systems

01:06:21.440 --> 01:06:26.960
in the sense of cyclical cause and effect that we don't have anything that can model them

01:06:26.960 --> 01:06:32.160
intelligently. And I give an example in this, in this, I can write a program that changes itself

01:06:32.160 --> 01:06:37.600
at runtime. Because this is intentional, I can, the whole program can be a parameter,

01:06:38.240 --> 01:06:43.520
which I can look at it. Well, code is data. That's why I can manipulate the program itself

01:06:43.520 --> 01:06:47.680
and go look at it after execution and see different program than the one I wrote. It's,

01:06:47.680 --> 01:06:53.840
it's amazing list. So if I, so I can write programs in this that no one can understand

01:06:54.880 --> 01:07:01.200
and model and do program verification. So I make the argument that, okay, I can see your point.

01:07:02.160 --> 01:07:09.120
But like Keith said, never is a long time. Why say we cannot come up with a new mathematics?

01:07:09.120 --> 01:07:15.440
I can see you at one point, someone discovering, yeah, at the level of Newton differential calculus,

01:07:15.440 --> 01:07:21.760
why not? Which could happen. So the word never for me, it's hard to digest.

01:07:21.760 --> 01:07:25.040
Maybe an AGI will discover the mathematics to create itself.

01:07:26.480 --> 01:07:32.560
And the other point is, the other point is, which is another point that John McCarthy wants.

01:07:33.360 --> 01:07:40.800
Who said we have to understand what we built? Here's what I mean. Do we understand ourselves?

01:07:40.800 --> 01:07:47.280
We don't. So why not build a scary intelligent machine that we don't really understand,

01:07:47.280 --> 01:07:53.040
like my list program? So what I'm saying is I had, I had an issue with them saying,

01:07:53.760 --> 01:08:02.320
that precludes AGI. No, it doesn't. In theory, I can build a complex intelligent machine like us

01:08:03.040 --> 01:08:08.640
in many respects. Doesn't feel pain. Hey, who cares? But it's scary intelligent.

01:08:09.440 --> 01:08:14.320
And we don't understand how it works. So what? This can happen. I can build something I don't

01:08:14.320 --> 01:08:22.720
understand. So in theory, I have two issues with their book, that this never and this absolute

01:08:22.720 --> 01:08:26.480
decision that we're done, we can never get there. No, we might build something we don't understand

01:08:27.200 --> 01:08:31.760
by discovering some new weird mathematics. So, okay, I agree with you that it's a,

01:08:32.480 --> 01:08:35.120
it's a complex thing that we will never understand. But so what?

01:08:35.120 --> 01:08:41.120
But what I loved about the book is it's a sobering book. I mean,

01:08:42.240 --> 01:08:48.400
it really is a balancing book compared to the hype and the simplicity you see out there. I mean,

01:08:48.400 --> 01:08:56.000
you, you recommend it or highly because I mean, I didn't need that much sobering. I know that

01:08:56.000 --> 01:09:03.280
any talk of AGI is like, Hey, take a break. Enjoy your paycheck, but don't make silly statements

01:09:03.280 --> 01:09:07.680
like this, right? Although I felt I thought you might have fallen off that wagon at the beginning

01:09:07.680 --> 01:09:16.800
of this conversation. No, I'm a defender of the faith. But, but so it's, I recommend it to people

01:09:16.800 --> 01:09:22.800
that need it. Like me, I needed it too. It's a sobering book. Like, this is how complex what

01:09:22.800 --> 01:09:30.080
you're trying to do is. Okay, guys. So before you go out and say, language understand. And the

01:09:30.080 --> 01:09:36.240
nice thing is they took aspects of the mind, just language itself is a beast that we cannot conquer.

01:09:36.960 --> 01:09:41.200
So imagine the whole mind and the granular thing they go through it. I mean, it's all,

01:09:41.200 --> 01:09:44.800
it's complex systems all the way down or all the way up if you want. So

01:09:46.720 --> 01:09:51.520
language is a complex system on its own part of the mind, which is a complex system on its own

01:09:51.520 --> 01:09:57.440
part of the human living organism, which is a complex system on its own. So and at every level,

01:09:57.440 --> 01:10:04.240
the complexity, we don't have a mathematics for that's the gist of their argument. So people that

01:10:04.240 --> 01:10:11.040
make these big claims about AI need to read it. Guys, cool down, cool down. You have, you have not

01:10:11.040 --> 01:10:17.680
solved problems that occupy the most penetrating minds in the history of enemy from Emmanuel

01:10:17.680 --> 01:10:25.280
Khan to you have not solved these problems, cool it down. You can build narrow AI, very narrow AI.

01:10:26.240 --> 01:10:32.640
And all this transferability, transferability. I mean, if you're good at chess, I know people

01:10:32.640 --> 01:10:39.840
that are good at chess and they're almost good at nothing else, not okay. So forget this. If I'm

01:10:39.840 --> 01:10:47.760
good at chess, I can be a smart doctor. No. So we are a very complex machine. So this book is a good

01:10:47.760 --> 01:10:54.720
sobering book, mathematically speaking, philosophically speaking, so that people will tone down

01:10:54.720 --> 01:11:03.360
what they're saying and start speaking science instead of media gibberish, right? Deep learning

01:11:03.360 --> 01:11:11.040
will soon be able to do everything. I mean, from a scientist. Well, it seems like a council of

01:11:11.040 --> 01:11:21.040
despair almost. Is there any optimistic or positive hopeful aspects to it? No, I that part I don't

01:11:21.120 --> 01:11:31.840
like this never, right? I mean, I am a believer that we can do AGI, but not a human like AI.

01:11:32.720 --> 01:11:42.080
We might do a very powerful AI that in many ways is more powerful. I mean, we've done that now. I mean,

01:11:42.080 --> 01:11:48.160
machines are now superior to us in many respects and respects even that they require intelligence,

01:11:48.160 --> 01:11:55.360
not a bulldozer that can lift more than me, that will have to do cognitive tasks better than us.

01:11:55.360 --> 01:12:06.400
We have go or finding patterns and data at the scale that no human can do. So we are building

01:12:06.400 --> 01:12:13.120
intelligent machines, but can we conquer things like language like autonomous driving was a failure.

01:12:13.120 --> 01:12:17.680
It's a big upset for AI because they trivialize the problem that we can go.

01:12:18.560 --> 01:12:22.880
Well, and that's kind of what I want to get to, you know, Mark kind of in response to you, which is

01:12:24.240 --> 01:12:28.080
I take these kind of sobering, these sobering things and look, I mean,

01:12:30.160 --> 01:12:35.600
the book sounds great, and I'm definitely going to get it and read it. But some of these thoughts,

01:12:35.600 --> 01:12:41.520
you know, many people have had, you know, many times over the years, right? And I've recognized

01:12:41.600 --> 01:12:46.000
that there are these limitations. But I think like part of part of why I think

01:12:46.000 --> 01:12:51.360
books like this are actually have an optimistic kind of side to them is I hope, I hope they

01:12:51.360 --> 01:12:59.680
encourage people to get more creative. Okay, like, like stop just trying to dump every single dollar

01:12:59.680 --> 01:13:05.760
you have into yet another parameter, you know, into yet another thousand or billion parameters

01:13:05.760 --> 01:13:10.240
in a model, like, let's take some of our resources, like, sure, let's keep doing that engineering,

01:13:10.240 --> 01:13:15.680
but let's take some portion of our resources here and invest it in, like, crazy ideas. And I know

01:13:15.680 --> 01:13:21.280
Tim's smile here because it's like, like, can it's can a Stanley kind of type thing, right? Like, just

01:13:22.080 --> 01:13:28.640
go out there and try to do something crazy to find that mathematics that we need, right? Which is

01:13:28.640 --> 01:13:34.560
let's get creative, let's work on crazy things, let's have crazy ideas, let's work on hybrid

01:13:34.560 --> 01:13:40.800
systems, let's not give up on, you know, neuromorphic, you know, systems and computer,

01:13:40.800 --> 01:13:45.840
whatever, like, let's spread out, let's spread out a bit, because of the fact that if we just

01:13:45.840 --> 01:13:53.680
keep going down this direction of ever larger Turing machines, like, that may not be the solution.

01:13:54.240 --> 01:14:01.680
Actually, they make this point exactly in different ways that if anything, their goal is to let people

01:14:01.760 --> 01:14:07.920
widen their horizon. So many aspects of this problem that guys, if we keep going,

01:14:07.920 --> 01:14:11.680
this is not going to get us there. And that's why they argued mathematically,

01:14:11.680 --> 01:14:15.920
philosophically. And I think they have a good argument. This is not going to take us there.

01:14:16.480 --> 01:14:24.320
But let's explore that. So that and being so religious about this will will hinder any other

01:14:24.880 --> 01:14:33.040
possibility. So overall, their argument is a good argument. I think everybody should read this book

01:14:33.040 --> 01:14:42.400
that's interested in AGI as a goal. What we have cannot ever take us there. They prove this

01:14:42.400 --> 01:14:48.640
mathematically. I mean, to me, they proved it in language on. So we need something new. And if you

01:14:48.640 --> 01:14:53.680
want to do something new, we can't just stay in this corner and with this, that's not going to get

01:14:53.680 --> 01:15:01.280
us there. So in a way, it's not a negative book. It's a sobering book, I will use the term sober.

01:15:02.080 --> 01:15:07.200
Well, and encouraging of more variety and more daring and more creativity and

01:15:07.760 --> 01:15:12.640
right, they don't push too much on that. But but indirectly, the indirect

01:15:13.760 --> 01:15:20.720
net result, if people appreciate the argument will be to look and explore other ways. So

01:15:21.280 --> 01:15:27.440
in a way, it's more positive than negative. And by the way, stating a mathematical fact is never

01:15:27.440 --> 01:15:35.120
negative. Oh, don't be so sure about that. If they're saying we're always on the edge of being

01:15:35.120 --> 01:15:41.040
canceled for stating like mathematical facts. So yeah, but I mean, if they're saying that

01:15:42.160 --> 01:15:48.560
what we're doing now, we're not we'll never get us to AGI. That's not negative. You're saying we

01:15:49.360 --> 01:15:54.800
need something else, we need something more. Or yours, yours, look, it could have saved us,

01:15:54.800 --> 01:16:01.040
they use this phrase, money down the drain. Autonomous driving is a case, is a good case.

01:16:02.480 --> 01:16:07.200
Billions, we're talking non trivial money guys, we're talking more than the budgets of some

01:16:07.200 --> 01:16:15.360
European countries. Just imagine the scale. And those guys went bust, right? Why? Because

01:16:15.440 --> 01:16:20.880
they trivialized the autonomous truck. An autonomous car is an autonomous agent, guys.

01:16:21.840 --> 01:16:28.640
It's an it's an agent trying to reason in a dynamic and uncertain environment and has on the

01:16:28.640 --> 01:16:34.320
fly to change to do belief revision, change its strategy, because of something new that came up.

01:16:35.040 --> 01:16:40.160
All of that is from seeing the tree and the stop sign. It's all vision.

01:16:40.960 --> 01:16:45.280
Yeah, so this is actually encouraging because now for all the Uber drivers out there and

01:16:45.280 --> 01:16:52.320
long hauled truck drivers, your job is safe. Like it's not going to be replaced anytime soon.

01:16:52.320 --> 01:16:56.880
Yeah, it's amazing. And a few years back when I was still in the valley,

01:16:58.160 --> 01:17:05.200
yeah, I was in Mecca in Silicon Valley. And I would talk to superb PhDs in neuroscience and

01:17:05.920 --> 01:17:12.080
big AI engineers at top companies. They were so excited that we're at level four

01:17:12.080 --> 01:17:16.640
in a year or two. And this was six years ago. I say, guys, this will not happen. They say,

01:17:16.640 --> 01:17:22.080
why are you so negative? I said, you cannot have an autonomous agent on the road without

01:17:22.080 --> 01:17:26.960
solving the frame problem. How do I revise everything I know, because of this new event?

01:17:28.240 --> 01:17:32.880
And this has to happen real time. We don't have a solution for the frame problem.

01:17:32.880 --> 01:17:39.200
All you're doing is you have cars on a railway. We have autonomous cars now. It's called the train.

01:17:42.000 --> 01:17:48.320
We have autonomous flight. If I'm not in a dynamic and uncertain environment reasoning,

01:17:48.960 --> 01:17:54.080
yeah, I can have autonomous anything. We call it the railway. I mean,

01:17:55.200 --> 01:17:58.800
we call it Amtrak. It's autonomous. You press a button and it goes.

01:17:59.760 --> 01:18:03.200
If we're talking about reasoning in the streets of San Francisco,

01:18:04.400 --> 01:18:07.360
you have to face the frame problem or you will kill people.

01:18:09.840 --> 01:18:16.720
Anyway, on that sobering note, I think a lot of stuff goes unnoticed in San Francisco,

01:18:16.720 --> 01:18:22.240
so I'm not sure about that. Probably that's the least of them. No, but the scale of money that

01:18:22.320 --> 01:18:29.360
went the scale. This is the value of this book, the scale of investment. I mean, if 10% of that

01:18:29.360 --> 01:18:35.360
was put on another approach, hey, you weird guy with this weird idea, take 10% of what we're

01:18:35.360 --> 01:18:43.680
throwing down the drain and explore something else. Show me. That's where I'm at, too.

01:18:44.960 --> 01:18:51.280
Diversify the effort. There's a huge impact here, societal impact. We're wasting billions of dollars

01:18:51.280 --> 01:18:57.200
just because I don't want to listen to anyone else. It happened in the chatbot industry, which

01:18:57.200 --> 01:19:03.040
I'm more familiar with than autonomous driving. Chatbot this, chatbot that, and there was an

01:19:03.040 --> 01:19:10.880
explosion. It was like a blob, like the internet thing. Now we can't get away from them. Every

01:19:10.880 --> 01:19:19.440
website we go to, it's like, leave me alone. Nobody wants to use them because we know how

01:19:19.440 --> 01:19:28.160
they work. These are stochastic parents. Yeah, literally just going to a FAC and hitting control

01:19:28.160 --> 01:19:36.560
F is more effective for me than trying to interact with a chatbot. The search engines by key phrases

01:19:36.560 --> 01:19:40.880
you put and they bring you a link and they say, read this. This is your answer. They are search

01:19:40.880 --> 01:19:47.040
engines basically. But again, the amount of money, because I lived in that industry,

01:19:48.320 --> 01:19:56.560
the amount of money spent on chatbots will scare the hell out of anybody. You combine that with

01:19:56.560 --> 01:20:04.400
autonomous driving, both dead, almost zero. We're talking billions and billions and billions. And

01:20:04.400 --> 01:20:10.000
you talk to any one of them in the highest, in the middle of the fever. They won't listen to you.

01:20:10.000 --> 01:20:17.840
I have people now calling me back and saying, you were right. Yeah, after $200 billion. So,

01:20:17.840 --> 01:20:24.080
science is important. Engineering is important, but science is important too. That's where the

01:20:24.080 --> 01:20:31.680
value of this book is. Guys, hacking alone will not do the whole thing. You're a bright engineer,

01:20:31.680 --> 01:20:36.080
you can hack your way through what we know is true. That's the space you can play with.

01:20:36.320 --> 01:20:45.920
You cannot hack your way in a bigger set of possibilities that are. You didn't verify that

01:20:45.920 --> 01:20:53.600
you can go there. An engineer can be creative within a Venn diagram that the scientists drew

01:20:53.600 --> 01:21:00.400
for them. That's the difference between science and engineering. The scientist draws the Venn

01:21:00.400 --> 01:21:05.840
diagram and that's the value of philosophers, at least the analytic philosophers, that know logic

01:21:05.840 --> 01:21:11.920
and metaphysics and quantum mechanics and philosophers that are on the technical side.

01:21:12.720 --> 01:21:18.400
They know how to draw the Venn diagram. You, as an engineer, if you're wasting your time here,

01:21:19.280 --> 01:21:25.120
that's called money down the drain. Play inside the Venn diagram. Otherwise,

01:21:25.920 --> 01:21:30.640
you're just an over enthused engineer who should go back and study computability.

01:21:31.600 --> 01:21:35.760
It's kind of like how patent examiners can easily reject anything that comes in that

01:21:35.760 --> 01:21:42.480
claims to violate the second law of thermodynamics, right? Well, listen, Wally, I think we

01:21:42.480 --> 01:21:48.720
sincerely appreciate your time today. And also, Mark, thank you for joining us and asking great

01:21:48.720 --> 01:21:56.160
questions. We should do this again. Thanks, Wally. Yeah, I really appreciate it. So thanks,

01:21:56.160 --> 01:22:01.760
always fun, guys. I see. Peace.

