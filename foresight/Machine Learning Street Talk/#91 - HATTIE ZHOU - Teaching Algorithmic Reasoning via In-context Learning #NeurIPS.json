{"text": " Hattie Jo, a PhD student at the University of Montreal and Miele, has set out to understand and explain the performance of modern neural networks, believing it to be a key factor in building better, more trusted models. Having previously worked as a data scientist at Uber, a private equity analyst at Radar Capital, and an economic consultant at Cornerstone Research, she's recently released a paper in collaboration with the Google Brain team titled, Teaching Algorithmic Reasoning via InContext Learning. Now in this work, Hattie identifies and examines four key stages for successfully teaching algorithmic reasoning to large language models, formulating algorithms as skills, teaching multiple skills simultaneously, teaching how to combine skills, and finally teaching how to use skills as tools. Now through the application of algorithmic prompting, Hattie has achieved remarkable results with an order of magnitude error reduction on some tasks compared to the best available baselines. Now this breakthrough demonstrates algorithmic prompting's viability as an approach for teaching algorithmic reasoning to large language models and may have implications for other tasks requiring similar reasoning capabilities. Anyway, I hope you enjoy this conversation that I had with Hattie over at NeurIPS a couple of weeks ago. Enjoy. Amazing. Well, Hattie, we're here at NeurIPS. Welcome to MLST. Thank you. Please introduce yourself. I'm Hattie Jo. I'm currently a PhD student at NILA and a part-time researcher at Google Brain right now as well as a student researcher. Fantastic. And I've been reading your paper, Teaching Algorithmic Reasoning via In-Context Learning. Give us the elevator, bitch. The elevator, bitch. So I think typically in the past people have thought that large language models are not great at doing symbol manipulation or actually doing reasoning the way humans do. And a common example people point to for this failure is to show that models can't even do addition properly, even though it's trained on billions of tokens, billions of parameters. And in this paper, we try to teach the model how to solve these problems by learning an algorithm for them and see if it can generalize to a harder problem, generalize out of distribution, which is a common way to see if the model is actually using the correct algorithm to solve these tasks. So it's not just like fitting to the training distribution and finding spirit features, it's actually executing an algorithm. So we do that through some prompting strategies and show that actually can learn how to do addition contrary to previous belief. Yeah, I wanted to speak about the previous belief. So with large language models, I'm vacillated back and forth from being skeptical and being very optimistic. I was originally very skeptical and I'm becoming more optimistic as time goes on. I interviewed a lady from UC Irvine called Yasaman Rezegi. And she did a really interesting bit of work actually kind of comparing the term frequencies as they appeared in the corpus to the arithmetic as a function of the number of digits. And because OpenAI never released their data set statistics, so it wasn't possible to do that. And she found that there was like a linear correlation. But I think what's really interesting now in your work and some of the other work like Chain of Thoughts Prompting and Scratch Bad Prompting, that now we're in this new regime where we're kind of telling the language model. It's almost like we're treating the language model a bit like a kind of compiler and we're giving it the program. And then it's actually extrapolating and it's doing things that it wasn't directly taught. Right. Yeah. I think that's the first step, right? You want to, like if you can give the model a program, you want to see that the model can use that program and apply it to new situations. And so that's kind of the reasoning out of the distribution component. But I think also ideally you can imagine we want the model to be able to discover algorithms that we don't know ourselves. And you know, that's a whole other frontier, right? But this is the first step too on that path, I think. Okay. Well, why don't we start by defining algorithmic reasoning? Sure. So I think about it as just the way you solve a task is by using a particular algorithm. And algorithms are input independent, which means that basically for any input distribution, using that same algorithm will get you the right answer. And so performing reasoning by running an algorithm is what we refer to as algorithmic reasoning. And of course, these right now apply to tasks that can be solved by an algorithm. But you could also imagine, like, cases where you have a soft algorithm where the steps are not so rigidly defined, but there is an overarching problem, like solving structure that you can follow. Yeah. Could you refer to that? I mean, maybe I would refer to that as like inductive algorithmic reasoning. So it's where, so on one side of the spectrum, you actually have the algorithm and you write every single step explicitly using some kind of code. And now we're talking about this regime where we are giving examples of an algorithm and we're describing the steps somewhat vaguely. We're trying to be as clear as possible using language. So where are we kind of following on that continuum? Well, let's say you take an algorithm of addition. Yeah. But you actually, so you have an algorithm defined where you like start by taking the first digit, do something with it, and then get an answer and then move on to the next one. When I think about a soft algorithm, it can look something like that, except you take the digit and the something you do with it is not defined explicitly, but it might require some abstract pattern matching that large language models are particularly good at. So it can use the same process of breaking down the problem in a very specific way to generalize, but the individual steps are not actually encoded in code because it's abstract. So if you can do that, then you can use this approach to tackle tasks where you can't write an algorithm or you can't write a program for. And that's like very exciting, I think. Can you give me an example of where we can't explicitly write the algorithm for something? Well, it's hard to come up with a good example because it's supposed to be abstract in some sense, but these will not be questions of that you would normally tackle with by writing a program. So it could be a soft reasoning. I guess maybe even the grade school math word problems, there is no algorithm or a simple algorithm at least that you can write that will solve math word problems, which are like if you have four apples, I gave you twice the amount of apples you have. How many do you have now? But if you define a way to tackle these questions at each individual step, the model can decide how to apply that flexibly based on the particular question. Yeah, that might be a good thing to have. But yeah, right now, I don't know. There is no good benchmark for these kind of things. Yeah. And in your paper, so you came up with a new algorithmic prompting technique and you designed some experiments and your technique works significantly better than some of the other in context prompting techniques. Can you sketch that out for us? Yeah, so the intuition is very simple, actually. When we look at the addition algorithm, the one that we learned in school, we know that you start with the rightmost digits, you add them up, you add them up. If it's greater than 10, you have a carry of one, and then you add the carry to the next sum and so forth. So a scratch pad approach for addition will show the trace from running this algorithm. So it will show the first sum is this, and the carry is this. But it doesn't explain how those values are derived. But for us, it feels really natural because we're so familiar with it. But for a model, trying to infer the rules from seeing a couple examples of it, that's a very under specified problem. There are many rules that could explain perfectly these observations. So algorithmic prompting basically explains each step of an algorithm using some examples. And within each step tries to be as detailed as possible and tries to disambiguate as much as possible what you want the model to do. And it turns out that when you provide more details to the model, you're sort of constraining the model's interpretation of disinformation so that there's only one way to apply this. And with that, you can get more robust behavior from model and reason very well out of distribution. Yeah, because it's often said that deep learning models do not reason. And I think what people mean by that is that you get this phenomenon of shortcut learning and models do the right things for the wrong reasons. And it seems to me that what we're doing here is by imputing the kind of the structure of how to reason into the prompt, we're robustifying its behavior out of distribution. Yeah, well said. That's exactly right. Fascinating. So what do you think of this problem of shortcut learning in general? Because you know, like Melanie Mitchell said, there are two modes of understanding essentially. We have this anthropomorphic mode of understanding which is using causality and it's very sample efficient and we have a way of understanding the world. And we have a bit of an intuition that large language models don't reason the way that we do. But is it necessarily a problem? And is it cheating in your view using in-context learning? Or do you think that because we haven't had to train the model again, what's the problem, right? I mean, is it an issue? Yeah, I mean, I guess there's many forms of reasoning and algorithmic reasoning is only a subset of that. And I think if the model can output the reasoning process and show that the answer that arrives at is following the output of that process, then it's hard to argue that it's not doing reasoning. It might work differently under the hood, but it follows the similar process. Now, you know, some methods, you output a rationale, but the final answer is actually different from what the explanation suggests it should be. And maybe you can point to that and say, oh, the model isn't actually using this. It's just like giving you something that you asked for, but then the final answer is still using a shortcut. But with the algorithmic reasoning, we see that that's not the case. And so, yeah, the more you constrain things, maybe the more you remove shortcuts from the model. But an interesting question, I think, is you can get this behavior using in-context learning, which I think you, I suspect you can't really do from fine tuning or some sort of weight training. I think when you do that, you'll most likely just overfit on the training distribution. I wanted to ask you, you know, it's a bit of an open-ended question to say what's going on inside large language models. But what's so exciting to me is that you get all of this emergent strange behavior. No one would have imagined five years ago that we could do all of this prompt engineering on a kind of autoregressive language decoder. And the model is trained to do something really quite trivial, yet as a byproduct, all of this crazy stuff emerges in its internal representation. And all of this algorithmic reasoning capability seems to be like a side effect of that training. How does that even happen? Oh, that's a good question. It's possible that in order to fit that large training data set, the pre-training data set, you have to find regularities in content, I guess, that humans generate. And I think some of these abilities come out of those regularities that you learn. So the ability to refer to the pattern of a previous passage in context and, you know, see what the relationships are in that pattern and apply the same relationships when you input that circuit, I think, is just very useful as a way to summarize that large data set. And so because you're forced to compress all that data into a model set of weights, I think these regularities emerge somehow. I don't know exactly how. But I think, yeah, I mean, this is an interesting speculation because then you can say with much larger models where there is no capacity constraint at all, and you fit all the same data sets, it's not going to learn very interesting behavior because it's able to just fit the model without capturing the underlying patterns. And maybe that's why you actually need more training data and training longer rather than like the optimal scaling is not right now in the model size, the amount of data. Because you want like the right bottleneck for your representation. And I think maybe that's where these emergent capabilities are coming from. Interesting. Yeah. And also the amount of training as well as the amount of data. I wondered what is your intuition on the practical kind of computational limitations of large language models? So at the moment, they're trained to transform us. And there have been some pretty cool critiques of connectionism by Fodor and a few other people. And it's basically along the lines of neural networks can't represent infinite objects, which kind of distinguishes them from Turing machines. So they can't compute the nth digit of time is a fairly good example. But the amazing thing is that we're doing all of this stuff with algorithmic reasoning. And we haven't found the ceiling yet, it's just getting better and better. And I think it's almost creating this. Well, I mean, I'm becoming quite hopeful, actually, because I don't know where the limit is, but I suspect there is a limit quite soon. How do you think about what the realistic computational limits are? I think the fact that now we have in context learning is interesting because that allows us to have, I guess, adaptive amount of computation. And so if you have, let's say you have infinite context length, then you can sort of maybe do infinite computation in that case. Now, is infinite context length possible? Probably not. But then you can find clever ways to distill that information. You can find clever attention mechanisms. And so I think maybe there's a computational limit, but you can always find new ideas that make existing methods more efficient. And so, yeah, I have no idea when you would hit that limit, but it's probably very far into the future. Amazing. And so, Hattie, we're here in Europe. What have you taken from the conference so far? And what are you most excited about, you know, going forward? Yeah, I don't know, because I haven't checked out the posters very much yet. But I'm excited for the Math AI workshop, which is many other papers exploring this idea of doing math with language models. And yeah, there's some, you know, very impressive work there. So I'm excited for that. I'm excited to meet people and see what they're thinking about. And maybe get some ideas for what to work on next. Yeah, I'm also really interested in the math stuff I spent about an hour speaking with Marcus, is it Marcus Barb, who works under Christian Sagadia, I think he's in your group. Yeah, they're doing some, that's right, they're doing some really interesting stuff with basically doing mathematical conjecturing, you know, like representing mathematical expressions in large language models and being able to generate new ones. It's the kind of thing that you, again, would think was science fiction five years ago and like, remarkably, it works. Exactly. And then by the way, with the mathematical conjecturing, Marcus was saying that, unlike with large language models, it only has to be right one in 100 times because they can formally verify it. So it's almost like the bar is actually lower in that sense. Right. Yeah, I mean, that's where the language models informal reasonability is really useful. Right. Yeah, like the pattern actually is actually useful in a lot of cases. That's really cool. Cool. Well, actually, this has been amazing. Thank you so much for coming on the show and telling us about your research. Oh, thanks for having me. Yeah, it was fun. Looking beautiful. So Marcus, it's so nice to meet you. Can you introduce yourself? Hi, I'm Marcus. I work for Google Research together with Christian Segedy and the Antiformer team. Generally, we're working on trying to solve math, basically by trying to translate natural language mathematics into formal mathematics and in these formal representations of mathematics, we can check proofs and use that as a feedback signal for the understanding of mathematics. And most recently, I've been working on long context models like the memorizing transformer, trying to get these models, like makes models sensitive to the exact definitions and other lemmas that they might use for your improving. And that's an ongoing effort, hopefully more available soon. Can I just say, I'm so jealous that you work with Christian. I mean, folks will remember that we had a conversation with Christian. I think it was about 18 months ago. It is one of my favorite episodes of MLST. So you're a very lucky man indeed. Yes. Yes. It's great to work with Christian. It's a lot of fun. Amazing.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.08, "text": " Hattie Jo, a PhD student at the University of Montreal and Miele, has set out to understand", "tokens": [50364, 389, 1591, 414, 3139, 11, 257, 14476, 3107, 412, 264, 3535, 295, 34180, 293, 376, 15949, 11, 575, 992, 484, 281, 1223, 50668], "temperature": 0.0, "avg_logprob": -0.09209040840073388, "compression_ratio": 1.5640138408304498, "no_speech_prob": 0.005987346172332764}, {"id": 1, "seek": 0, "start": 6.08, "end": 12.0, "text": " and explain the performance of modern neural networks, believing it to be a key factor in", "tokens": [50668, 293, 2903, 264, 3389, 295, 4363, 18161, 9590, 11, 16594, 309, 281, 312, 257, 2141, 5952, 294, 50964], "temperature": 0.0, "avg_logprob": -0.09209040840073388, "compression_ratio": 1.5640138408304498, "no_speech_prob": 0.005987346172332764}, {"id": 2, "seek": 0, "start": 12.0, "end": 17.92, "text": " building better, more trusted models. Having previously worked as a data scientist at Uber,", "tokens": [50964, 2390, 1101, 11, 544, 16034, 5245, 13, 10222, 8046, 2732, 382, 257, 1412, 12662, 412, 21839, 11, 51260], "temperature": 0.0, "avg_logprob": -0.09209040840073388, "compression_ratio": 1.5640138408304498, "no_speech_prob": 0.005987346172332764}, {"id": 3, "seek": 0, "start": 17.92, "end": 24.0, "text": " a private equity analyst at Radar Capital, and an economic consultant at Cornerstone Research,", "tokens": [51260, 257, 4551, 10769, 19085, 412, 9654, 289, 21502, 11, 293, 364, 4836, 24676, 412, 42391, 11243, 10303, 11, 51564], "temperature": 0.0, "avg_logprob": -0.09209040840073388, "compression_ratio": 1.5640138408304498, "no_speech_prob": 0.005987346172332764}, {"id": 4, "seek": 0, "start": 24.0, "end": 28.400000000000002, "text": " she's recently released a paper in collaboration with the Google Brain team titled,", "tokens": [51564, 750, 311, 3938, 4736, 257, 3035, 294, 9363, 365, 264, 3329, 29783, 1469, 19841, 11, 51784], "temperature": 0.0, "avg_logprob": -0.09209040840073388, "compression_ratio": 1.5640138408304498, "no_speech_prob": 0.005987346172332764}, {"id": 5, "seek": 2840, "start": 28.4, "end": 34.56, "text": " Teaching Algorithmic Reasoning via InContext Learning. Now in this work, Hattie identifies and", "tokens": [50364, 34244, 35014, 6819, 13195, 39693, 278, 5766, 682, 29821, 3828, 15205, 13, 823, 294, 341, 589, 11, 389, 1591, 414, 34597, 293, 50672], "temperature": 0.0, "avg_logprob": -0.0999378494069546, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.015636881813406944}, {"id": 6, "seek": 2840, "start": 34.56, "end": 41.04, "text": " examines four key stages for successfully teaching algorithmic reasoning to large language models,", "tokens": [50672, 1139, 1652, 1451, 2141, 10232, 337, 10727, 4571, 9284, 299, 21577, 281, 2416, 2856, 5245, 11, 50996], "temperature": 0.0, "avg_logprob": -0.0999378494069546, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.015636881813406944}, {"id": 7, "seek": 2840, "start": 41.04, "end": 47.84, "text": " formulating algorithms as skills, teaching multiple skills simultaneously, teaching how to", "tokens": [50996, 1254, 12162, 14642, 382, 3942, 11, 4571, 3866, 3942, 16561, 11, 4571, 577, 281, 51336], "temperature": 0.0, "avg_logprob": -0.0999378494069546, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.015636881813406944}, {"id": 8, "seek": 2840, "start": 47.84, "end": 55.12, "text": " combine skills, and finally teaching how to use skills as tools. Now through the application", "tokens": [51336, 10432, 3942, 11, 293, 2721, 4571, 577, 281, 764, 3942, 382, 3873, 13, 823, 807, 264, 3861, 51700], "temperature": 0.0, "avg_logprob": -0.0999378494069546, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.015636881813406944}, {"id": 9, "seek": 5512, "start": 55.12, "end": 60.879999999999995, "text": " of algorithmic prompting, Hattie has achieved remarkable results with an order of magnitude", "tokens": [50364, 295, 9284, 299, 12391, 278, 11, 389, 1591, 414, 575, 11042, 12802, 3542, 365, 364, 1668, 295, 15668, 50652], "temperature": 0.0, "avg_logprob": -0.055304100638941714, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.022255312651395798}, {"id": 10, "seek": 5512, "start": 60.879999999999995, "end": 67.28, "text": " error reduction on some tasks compared to the best available baselines. Now this breakthrough", "tokens": [50652, 6713, 11004, 322, 512, 9608, 5347, 281, 264, 1151, 2435, 987, 9173, 13, 823, 341, 22397, 50972], "temperature": 0.0, "avg_logprob": -0.055304100638941714, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.022255312651395798}, {"id": 11, "seek": 5512, "start": 67.28, "end": 72.08, "text": " demonstrates algorithmic prompting's viability as an approach for teaching algorithmic reasoning", "tokens": [50972, 31034, 9284, 299, 12391, 278, 311, 1932, 2310, 382, 364, 3109, 337, 4571, 9284, 299, 21577, 51212], "temperature": 0.0, "avg_logprob": -0.055304100638941714, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.022255312651395798}, {"id": 12, "seek": 5512, "start": 72.08, "end": 78.47999999999999, "text": " to large language models and may have implications for other tasks requiring similar reasoning", "tokens": [51212, 281, 2416, 2856, 5245, 293, 815, 362, 16602, 337, 661, 9608, 24165, 2531, 21577, 51532], "temperature": 0.0, "avg_logprob": -0.055304100638941714, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.022255312651395798}, {"id": 13, "seek": 5512, "start": 78.47999999999999, "end": 82.96, "text": " capabilities. Anyway, I hope you enjoy this conversation that I had with Hattie over at", "tokens": [51532, 10862, 13, 5684, 11, 286, 1454, 291, 2103, 341, 3761, 300, 286, 632, 365, 389, 1591, 414, 670, 412, 51756], "temperature": 0.0, "avg_logprob": -0.055304100638941714, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.022255312651395798}, {"id": 14, "seek": 8296, "start": 82.96, "end": 88.24, "text": " NeurIPS a couple of weeks ago. Enjoy. Amazing. Well, Hattie, we're here at NeurIPS.", "tokens": [50364, 1734, 374, 40, 6273, 257, 1916, 295, 3259, 2057, 13, 15411, 13, 14165, 13, 1042, 11, 389, 1591, 414, 11, 321, 434, 510, 412, 1734, 374, 40, 6273, 13, 50628], "temperature": 0.0, "avg_logprob": -0.15001691556444355, "compression_ratio": 1.4504132231404958, "no_speech_prob": 0.041250310838222504}, {"id": 15, "seek": 8296, "start": 89.28, "end": 96.63999999999999, "text": " Welcome to MLST. Thank you. Please introduce yourself. I'm Hattie Jo. I'm currently a PhD", "tokens": [50680, 4027, 281, 21601, 6840, 13, 1044, 291, 13, 2555, 5366, 1803, 13, 286, 478, 389, 1591, 414, 3139, 13, 286, 478, 4362, 257, 14476, 51048], "temperature": 0.0, "avg_logprob": -0.15001691556444355, "compression_ratio": 1.4504132231404958, "no_speech_prob": 0.041250310838222504}, {"id": 16, "seek": 8296, "start": 96.63999999999999, "end": 104.16, "text": " student at NILA and a part-time researcher at Google Brain right now as well as a student", "tokens": [51048, 3107, 412, 426, 4620, 32, 293, 257, 644, 12, 3766, 21751, 412, 3329, 29783, 558, 586, 382, 731, 382, 257, 3107, 51424], "temperature": 0.0, "avg_logprob": -0.15001691556444355, "compression_ratio": 1.4504132231404958, "no_speech_prob": 0.041250310838222504}, {"id": 17, "seek": 8296, "start": 104.16, "end": 111.03999999999999, "text": " researcher. Fantastic. And I've been reading your paper, Teaching Algorithmic Reasoning", "tokens": [51424, 21751, 13, 21320, 13, 400, 286, 600, 668, 3760, 428, 3035, 11, 34244, 35014, 6819, 13195, 39693, 278, 51768], "temperature": 0.0, "avg_logprob": -0.15001691556444355, "compression_ratio": 1.4504132231404958, "no_speech_prob": 0.041250310838222504}, {"id": 18, "seek": 11104, "start": 111.12, "end": 117.76, "text": " via In-Context Learning. Give us the elevator, bitch. The elevator, bitch. So I think", "tokens": [50368, 5766, 682, 12, 29821, 3828, 15205, 13, 5303, 505, 264, 18782, 11, 11960, 13, 440, 18782, 11, 11960, 13, 407, 286, 519, 50700], "temperature": 0.0, "avg_logprob": -0.12279335069067684, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.00406445050612092}, {"id": 19, "seek": 11104, "start": 119.12, "end": 123.76, "text": " typically in the past people have thought that large language models are not great at", "tokens": [50768, 5850, 294, 264, 1791, 561, 362, 1194, 300, 2416, 2856, 5245, 366, 406, 869, 412, 51000], "temperature": 0.0, "avg_logprob": -0.12279335069067684, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.00406445050612092}, {"id": 20, "seek": 11104, "start": 124.96000000000001, "end": 132.24, "text": " doing symbol manipulation or actually doing reasoning the way humans do. And a common example", "tokens": [51060, 884, 5986, 26475, 420, 767, 884, 21577, 264, 636, 6255, 360, 13, 400, 257, 2689, 1365, 51424], "temperature": 0.0, "avg_logprob": -0.12279335069067684, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.00406445050612092}, {"id": 21, "seek": 11104, "start": 132.24, "end": 138.32, "text": " people point to for this failure is to show that models can't even do addition properly,", "tokens": [51424, 561, 935, 281, 337, 341, 7763, 307, 281, 855, 300, 5245, 393, 380, 754, 360, 4500, 6108, 11, 51728], "temperature": 0.0, "avg_logprob": -0.12279335069067684, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.00406445050612092}, {"id": 22, "seek": 13832, "start": 138.32, "end": 142.72, "text": " even though it's trained on billions of tokens, billions of parameters.", "tokens": [50364, 754, 1673, 309, 311, 8895, 322, 17375, 295, 22667, 11, 17375, 295, 9834, 13, 50584], "temperature": 0.0, "avg_logprob": -0.10885171192448313, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.006089723203331232}, {"id": 23, "seek": 13832, "start": 144.48, "end": 151.6, "text": " And in this paper, we try to teach the model how to solve these problems by learning an algorithm", "tokens": [50672, 400, 294, 341, 3035, 11, 321, 853, 281, 2924, 264, 2316, 577, 281, 5039, 613, 2740, 538, 2539, 364, 9284, 51028], "temperature": 0.0, "avg_logprob": -0.10885171192448313, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.006089723203331232}, {"id": 24, "seek": 13832, "start": 151.6, "end": 158.07999999999998, "text": " for them and see if it can generalize to a harder problem, generalize out of distribution,", "tokens": [51028, 337, 552, 293, 536, 498, 309, 393, 2674, 1125, 281, 257, 6081, 1154, 11, 2674, 1125, 484, 295, 7316, 11, 51352], "temperature": 0.0, "avg_logprob": -0.10885171192448313, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.006089723203331232}, {"id": 25, "seek": 13832, "start": 158.07999999999998, "end": 165.28, "text": " which is a common way to see if the model is actually using the correct algorithm to solve", "tokens": [51352, 597, 307, 257, 2689, 636, 281, 536, 498, 264, 2316, 307, 767, 1228, 264, 3006, 9284, 281, 5039, 51712], "temperature": 0.0, "avg_logprob": -0.10885171192448313, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.006089723203331232}, {"id": 26, "seek": 16528, "start": 165.28, "end": 169.84, "text": " these tasks. So it's not just like fitting to the training distribution and finding", "tokens": [50364, 613, 9608, 13, 407, 309, 311, 406, 445, 411, 15669, 281, 264, 3097, 7316, 293, 5006, 50592], "temperature": 0.0, "avg_logprob": -0.11609736267401248, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.005809767171740532}, {"id": 27, "seek": 16528, "start": 169.84, "end": 177.2, "text": " spirit features, it's actually executing an algorithm. So we do that through some prompting", "tokens": [50592, 3797, 4122, 11, 309, 311, 767, 32368, 364, 9284, 13, 407, 321, 360, 300, 807, 512, 12391, 278, 50960], "temperature": 0.0, "avg_logprob": -0.11609736267401248, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.005809767171740532}, {"id": 28, "seek": 16528, "start": 177.2, "end": 183.12, "text": " strategies and show that actually can learn how to do addition contrary to previous belief.", "tokens": [50960, 9029, 293, 855, 300, 767, 393, 1466, 577, 281, 360, 4500, 19506, 281, 3894, 7107, 13, 51256], "temperature": 0.0, "avg_logprob": -0.11609736267401248, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.005809767171740532}, {"id": 29, "seek": 16528, "start": 184.08, "end": 187.76, "text": " Yeah, I wanted to speak about the previous belief. So with large language models, I'm", "tokens": [51304, 865, 11, 286, 1415, 281, 1710, 466, 264, 3894, 7107, 13, 407, 365, 2416, 2856, 5245, 11, 286, 478, 51488], "temperature": 0.0, "avg_logprob": -0.11609736267401248, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.005809767171740532}, {"id": 30, "seek": 16528, "start": 187.76, "end": 192.4, "text": " vacillated back and forth from being skeptical and being very optimistic. I was originally", "tokens": [51488, 2842, 373, 770, 646, 293, 5220, 490, 885, 28601, 293, 885, 588, 19397, 13, 286, 390, 7993, 51720], "temperature": 0.0, "avg_logprob": -0.11609736267401248, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.005809767171740532}, {"id": 31, "seek": 19240, "start": 192.4, "end": 197.68, "text": " very skeptical and I'm becoming more optimistic as time goes on. I interviewed a lady from UC", "tokens": [50364, 588, 28601, 293, 286, 478, 5617, 544, 19397, 382, 565, 1709, 322, 13, 286, 19770, 257, 7262, 490, 14079, 50628], "temperature": 0.0, "avg_logprob": -0.13097855897076363, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.047962188720703125}, {"id": 32, "seek": 19240, "start": 197.68, "end": 203.92000000000002, "text": " Irvine called Yasaman Rezegi. And she did a really interesting bit of work actually kind of comparing", "tokens": [50628, 9151, 41243, 1219, 30557, 6147, 1300, 89, 1146, 72, 13, 400, 750, 630, 257, 534, 1880, 857, 295, 589, 767, 733, 295, 15763, 50940], "temperature": 0.0, "avg_logprob": -0.13097855897076363, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.047962188720703125}, {"id": 33, "seek": 19240, "start": 203.92000000000002, "end": 210.96, "text": " the term frequencies as they appeared in the corpus to the arithmetic as a function of the", "tokens": [50940, 264, 1433, 20250, 382, 436, 8516, 294, 264, 1181, 31624, 281, 264, 42973, 382, 257, 2445, 295, 264, 51292], "temperature": 0.0, "avg_logprob": -0.13097855897076363, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.047962188720703125}, {"id": 34, "seek": 19240, "start": 210.96, "end": 216.48000000000002, "text": " number of digits. And because OpenAI never released their data set statistics, so it wasn't possible", "tokens": [51292, 1230, 295, 27011, 13, 400, 570, 7238, 48698, 1128, 4736, 641, 1412, 992, 12523, 11, 370, 309, 2067, 380, 1944, 51568], "temperature": 0.0, "avg_logprob": -0.13097855897076363, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.047962188720703125}, {"id": 35, "seek": 19240, "start": 216.48000000000002, "end": 221.52, "text": " to do that. And she found that there was like a linear correlation. But I think what's really", "tokens": [51568, 281, 360, 300, 13, 400, 750, 1352, 300, 456, 390, 411, 257, 8213, 20009, 13, 583, 286, 519, 437, 311, 534, 51820], "temperature": 0.0, "avg_logprob": -0.13097855897076363, "compression_ratio": 1.6033333333333333, "no_speech_prob": 0.047962188720703125}, {"id": 36, "seek": 22152, "start": 221.52, "end": 230.24, "text": " interesting now in your work and some of the other work like Chain of Thoughts Prompting and", "tokens": [50364, 1880, 586, 294, 428, 589, 293, 512, 295, 264, 661, 589, 411, 33252, 295, 23058, 82, 15833, 662, 278, 293, 50800], "temperature": 0.0, "avg_logprob": -0.10883377444359564, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.013741984032094479}, {"id": 37, "seek": 22152, "start": 230.24, "end": 235.84, "text": " Scratch Bad Prompting, that now we're in this new regime where we're kind of telling the language", "tokens": [50800, 34944, 852, 11523, 15833, 662, 278, 11, 300, 586, 321, 434, 294, 341, 777, 13120, 689, 321, 434, 733, 295, 3585, 264, 2856, 51080], "temperature": 0.0, "avg_logprob": -0.10883377444359564, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.013741984032094479}, {"id": 38, "seek": 22152, "start": 235.84, "end": 240.48000000000002, "text": " model. It's almost like we're treating the language model a bit like a kind of compiler", "tokens": [51080, 2316, 13, 467, 311, 1920, 411, 321, 434, 15083, 264, 2856, 2316, 257, 857, 411, 257, 733, 295, 31958, 51312], "temperature": 0.0, "avg_logprob": -0.10883377444359564, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.013741984032094479}, {"id": 39, "seek": 22152, "start": 240.48000000000002, "end": 246.08, "text": " and we're giving it the program. And then it's actually extrapolating and it's doing things that", "tokens": [51312, 293, 321, 434, 2902, 309, 264, 1461, 13, 400, 550, 309, 311, 767, 48224, 990, 293, 309, 311, 884, 721, 300, 51592], "temperature": 0.0, "avg_logprob": -0.10883377444359564, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.013741984032094479}, {"id": 40, "seek": 24608, "start": 246.08, "end": 252.32000000000002, "text": " it wasn't directly taught. Right. Yeah. I think that's the first step, right? You want to,", "tokens": [50364, 309, 2067, 380, 3838, 5928, 13, 1779, 13, 865, 13, 286, 519, 300, 311, 264, 700, 1823, 11, 558, 30, 509, 528, 281, 11, 50676], "temperature": 0.0, "avg_logprob": -0.10497099353421119, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.012035994790494442}, {"id": 41, "seek": 24608, "start": 253.12, "end": 259.2, "text": " like if you can give the model a program, you want to see that the model can use that program", "tokens": [50716, 411, 498, 291, 393, 976, 264, 2316, 257, 1461, 11, 291, 528, 281, 536, 300, 264, 2316, 393, 764, 300, 1461, 51020], "temperature": 0.0, "avg_logprob": -0.10497099353421119, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.012035994790494442}, {"id": 42, "seek": 24608, "start": 259.2, "end": 265.84000000000003, "text": " and apply it to new situations. And so that's kind of the reasoning out of the distribution", "tokens": [51020, 293, 3079, 309, 281, 777, 6851, 13, 400, 370, 300, 311, 733, 295, 264, 21577, 484, 295, 264, 7316, 51352], "temperature": 0.0, "avg_logprob": -0.10497099353421119, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.012035994790494442}, {"id": 43, "seek": 24608, "start": 265.84000000000003, "end": 272.48, "text": " component. But I think also ideally you can imagine we want the model to be able to discover", "tokens": [51352, 6542, 13, 583, 286, 519, 611, 22915, 291, 393, 3811, 321, 528, 264, 2316, 281, 312, 1075, 281, 4411, 51684], "temperature": 0.0, "avg_logprob": -0.10497099353421119, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.012035994790494442}, {"id": 44, "seek": 27248, "start": 272.48, "end": 279.76, "text": " algorithms that we don't know ourselves. And you know, that's a whole other frontier, right?", "tokens": [50364, 14642, 300, 321, 500, 380, 458, 4175, 13, 400, 291, 458, 11, 300, 311, 257, 1379, 661, 35853, 11, 558, 30, 50728], "temperature": 0.0, "avg_logprob": -0.13720496289141768, "compression_ratio": 1.4921465968586387, "no_speech_prob": 0.0030721400398761034}, {"id": 45, "seek": 27248, "start": 279.76, "end": 285.84000000000003, "text": " But this is the first step too on that path, I think. Okay. Well, why don't we start by defining", "tokens": [50728, 583, 341, 307, 264, 700, 1823, 886, 322, 300, 3100, 11, 286, 519, 13, 1033, 13, 1042, 11, 983, 500, 380, 321, 722, 538, 17827, 51032], "temperature": 0.0, "avg_logprob": -0.13720496289141768, "compression_ratio": 1.4921465968586387, "no_speech_prob": 0.0030721400398761034}, {"id": 46, "seek": 27248, "start": 285.84000000000003, "end": 296.24, "text": " algorithmic reasoning? Sure. So I think about it as just the way you solve a task is by using a", "tokens": [51032, 9284, 299, 21577, 30, 4894, 13, 407, 286, 519, 466, 309, 382, 445, 264, 636, 291, 5039, 257, 5633, 307, 538, 1228, 257, 51552], "temperature": 0.0, "avg_logprob": -0.13720496289141768, "compression_ratio": 1.4921465968586387, "no_speech_prob": 0.0030721400398761034}, {"id": 47, "seek": 29624, "start": 296.24, "end": 305.04, "text": " particular algorithm. And algorithms are input independent, which means that basically for", "tokens": [50364, 1729, 9284, 13, 400, 14642, 366, 4846, 6695, 11, 597, 1355, 300, 1936, 337, 50804], "temperature": 0.0, "avg_logprob": -0.09060600068834093, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.0340697206556797}, {"id": 48, "seek": 29624, "start": 305.04, "end": 311.2, "text": " any input distribution, using that same algorithm will get you the right answer. And so performing", "tokens": [50804, 604, 4846, 7316, 11, 1228, 300, 912, 9284, 486, 483, 291, 264, 558, 1867, 13, 400, 370, 10205, 51112], "temperature": 0.0, "avg_logprob": -0.09060600068834093, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.0340697206556797}, {"id": 49, "seek": 29624, "start": 311.2, "end": 318.88, "text": " reasoning by running an algorithm is what we refer to as algorithmic reasoning. And of course,", "tokens": [51112, 21577, 538, 2614, 364, 9284, 307, 437, 321, 2864, 281, 382, 9284, 299, 21577, 13, 400, 295, 1164, 11, 51496], "temperature": 0.0, "avg_logprob": -0.09060600068834093, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.0340697206556797}, {"id": 50, "seek": 29624, "start": 318.88, "end": 325.84000000000003, "text": " these right now apply to tasks that can be solved by an algorithm. But you could also imagine,", "tokens": [51496, 613, 558, 586, 3079, 281, 9608, 300, 393, 312, 13041, 538, 364, 9284, 13, 583, 291, 727, 611, 3811, 11, 51844], "temperature": 0.0, "avg_logprob": -0.09060600068834093, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.0340697206556797}, {"id": 51, "seek": 32584, "start": 325.84, "end": 332.64, "text": " like, cases where you have a soft algorithm where the steps are not so rigidly defined,", "tokens": [50364, 411, 11, 3331, 689, 291, 362, 257, 2787, 9284, 689, 264, 4439, 366, 406, 370, 22195, 356, 7642, 11, 50704], "temperature": 0.0, "avg_logprob": -0.12254212623418764, "compression_ratio": 1.5898617511520738, "no_speech_prob": 0.0009232279844582081}, {"id": 52, "seek": 32584, "start": 332.64, "end": 338.71999999999997, "text": " but there is an overarching problem, like solving structure that you can follow.", "tokens": [50704, 457, 456, 307, 364, 45501, 1154, 11, 411, 12606, 3877, 300, 291, 393, 1524, 13, 51008], "temperature": 0.0, "avg_logprob": -0.12254212623418764, "compression_ratio": 1.5898617511520738, "no_speech_prob": 0.0009232279844582081}, {"id": 53, "seek": 32584, "start": 340.4, "end": 345.67999999999995, "text": " Yeah. Could you refer to that? I mean, maybe I would refer to that as like inductive", "tokens": [51092, 865, 13, 7497, 291, 2864, 281, 300, 30, 286, 914, 11, 1310, 286, 576, 2864, 281, 300, 382, 411, 31612, 488, 51356], "temperature": 0.0, "avg_logprob": -0.12254212623418764, "compression_ratio": 1.5898617511520738, "no_speech_prob": 0.0009232279844582081}, {"id": 54, "seek": 32584, "start": 345.67999999999995, "end": 351.28, "text": " algorithmic reasoning. So it's where, so on one side of the spectrum, you actually have the", "tokens": [51356, 9284, 299, 21577, 13, 407, 309, 311, 689, 11, 370, 322, 472, 1252, 295, 264, 11143, 11, 291, 767, 362, 264, 51636], "temperature": 0.0, "avg_logprob": -0.12254212623418764, "compression_ratio": 1.5898617511520738, "no_speech_prob": 0.0009232279844582081}, {"id": 55, "seek": 35128, "start": 351.28, "end": 356.88, "text": " algorithm and you write every single step explicitly using some kind of code. And now we're talking", "tokens": [50364, 9284, 293, 291, 2464, 633, 2167, 1823, 20803, 1228, 512, 733, 295, 3089, 13, 400, 586, 321, 434, 1417, 50644], "temperature": 0.0, "avg_logprob": -0.10901078616871554, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.006272867321968079}, {"id": 56, "seek": 35128, "start": 356.88, "end": 363.44, "text": " about this regime where we are giving examples of an algorithm and we're describing the steps", "tokens": [50644, 466, 341, 13120, 689, 321, 366, 2902, 5110, 295, 364, 9284, 293, 321, 434, 16141, 264, 4439, 50972], "temperature": 0.0, "avg_logprob": -0.10901078616871554, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.006272867321968079}, {"id": 57, "seek": 35128, "start": 363.44, "end": 368.71999999999997, "text": " somewhat vaguely. We're trying to be as clear as possible using language. So where are we kind", "tokens": [50972, 8344, 13501, 48863, 13, 492, 434, 1382, 281, 312, 382, 1850, 382, 1944, 1228, 2856, 13, 407, 689, 366, 321, 733, 51236], "temperature": 0.0, "avg_logprob": -0.10901078616871554, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.006272867321968079}, {"id": 58, "seek": 35128, "start": 368.71999999999997, "end": 375.59999999999997, "text": " of following on that continuum? Well, let's say you take an algorithm of addition.", "tokens": [51236, 295, 3480, 322, 300, 36120, 30, 1042, 11, 718, 311, 584, 291, 747, 364, 9284, 295, 4500, 13, 51580], "temperature": 0.0, "avg_logprob": -0.10901078616871554, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.006272867321968079}, {"id": 59, "seek": 37560, "start": 375.84000000000003, "end": 385.04, "text": " Yeah. But you actually, so you have an algorithm defined where you like start by taking the", "tokens": [50376, 865, 13, 583, 291, 767, 11, 370, 291, 362, 364, 9284, 7642, 689, 291, 411, 722, 538, 1940, 264, 50836], "temperature": 0.0, "avg_logprob": -0.13134601150733838, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.009406336583197117}, {"id": 60, "seek": 37560, "start": 385.04, "end": 389.52000000000004, "text": " first digit, do something with it, and then get an answer and then move on to the next one.", "tokens": [50836, 700, 14293, 11, 360, 746, 365, 309, 11, 293, 550, 483, 364, 1867, 293, 550, 1286, 322, 281, 264, 958, 472, 13, 51060], "temperature": 0.0, "avg_logprob": -0.13134601150733838, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.009406336583197117}, {"id": 61, "seek": 37560, "start": 392.0, "end": 398.32000000000005, "text": " When I think about a soft algorithm, it can look something like that, except you take the digit", "tokens": [51184, 1133, 286, 519, 466, 257, 2787, 9284, 11, 309, 393, 574, 746, 411, 300, 11, 3993, 291, 747, 264, 14293, 51500], "temperature": 0.0, "avg_logprob": -0.13134601150733838, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.009406336583197117}, {"id": 62, "seek": 39832, "start": 398.32, "end": 405.92, "text": " and the something you do with it is not defined explicitly, but it might require some abstract", "tokens": [50364, 293, 264, 746, 291, 360, 365, 309, 307, 406, 7642, 20803, 11, 457, 309, 1062, 3651, 512, 12649, 50744], "temperature": 0.0, "avg_logprob": -0.11107393952666736, "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.004827416967600584}, {"id": 63, "seek": 39832, "start": 408.32, "end": 415.12, "text": " pattern matching that large language models are particularly good at. So it can use the", "tokens": [50864, 5102, 14324, 300, 2416, 2856, 5245, 366, 4098, 665, 412, 13, 407, 309, 393, 764, 264, 51204], "temperature": 0.0, "avg_logprob": -0.11107393952666736, "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.004827416967600584}, {"id": 64, "seek": 39832, "start": 415.12, "end": 421.52, "text": " same process of breaking down the problem in a very specific way to generalize, but the individual", "tokens": [51204, 912, 1399, 295, 7697, 760, 264, 1154, 294, 257, 588, 2685, 636, 281, 2674, 1125, 11, 457, 264, 2609, 51524], "temperature": 0.0, "avg_logprob": -0.11107393952666736, "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.004827416967600584}, {"id": 65, "seek": 42152, "start": 421.59999999999997, "end": 432.56, "text": " steps are not actually encoded in code because it's abstract. So if you can do that, then you can", "tokens": [50368, 4439, 366, 406, 767, 2058, 12340, 294, 3089, 570, 309, 311, 12649, 13, 407, 498, 291, 393, 360, 300, 11, 550, 291, 393, 50916], "temperature": 0.0, "avg_logprob": -0.10745598174430229, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.03015304170548916}, {"id": 66, "seek": 42152, "start": 432.56, "end": 440.0, "text": " use this approach to tackle tasks where you can't write an algorithm or you can't write a program", "tokens": [50916, 764, 341, 3109, 281, 14896, 9608, 689, 291, 393, 380, 2464, 364, 9284, 420, 291, 393, 380, 2464, 257, 1461, 51288], "temperature": 0.0, "avg_logprob": -0.10745598174430229, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.03015304170548916}, {"id": 67, "seek": 42152, "start": 440.0, "end": 446.08, "text": " for. And that's like very exciting, I think. Can you give me an example of where we can't", "tokens": [51288, 337, 13, 400, 300, 311, 411, 588, 4670, 11, 286, 519, 13, 1664, 291, 976, 385, 364, 1365, 295, 689, 321, 393, 380, 51592], "temperature": 0.0, "avg_logprob": -0.10745598174430229, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.03015304170548916}, {"id": 68, "seek": 44608, "start": 446.08, "end": 452.71999999999997, "text": " explicitly write the algorithm for something? Well, it's hard to come up with a good example", "tokens": [50364, 20803, 2464, 264, 9284, 337, 746, 30, 1042, 11, 309, 311, 1152, 281, 808, 493, 365, 257, 665, 1365, 50696], "temperature": 0.0, "avg_logprob": -0.11368180078173441, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.005379355512559414}, {"id": 69, "seek": 44608, "start": 452.71999999999997, "end": 461.59999999999997, "text": " because it's supposed to be abstract in some sense, but these will not be questions of", "tokens": [50696, 570, 309, 311, 3442, 281, 312, 12649, 294, 512, 2020, 11, 457, 613, 486, 406, 312, 1651, 295, 51140], "temperature": 0.0, "avg_logprob": -0.11368180078173441, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.005379355512559414}, {"id": 70, "seek": 44608, "start": 464.47999999999996, "end": 471.2, "text": " that you would normally tackle with by writing a program. So it could be a soft reasoning.", "tokens": [51284, 300, 291, 576, 5646, 14896, 365, 538, 3579, 257, 1461, 13, 407, 309, 727, 312, 257, 2787, 21577, 13, 51620], "temperature": 0.0, "avg_logprob": -0.11368180078173441, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.005379355512559414}, {"id": 71, "seek": 47608, "start": 476.4, "end": 482.15999999999997, "text": " I guess maybe even the grade school math word problems, there is no algorithm or a simple", "tokens": [50380, 286, 2041, 1310, 754, 264, 7204, 1395, 5221, 1349, 2740, 11, 456, 307, 572, 9284, 420, 257, 2199, 50668], "temperature": 0.0, "avg_logprob": -0.17018185962330212, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0022157011553645134}, {"id": 72, "seek": 47608, "start": 482.15999999999997, "end": 488.71999999999997, "text": " algorithm at least that you can write that will solve math word problems, which are like if you", "tokens": [50668, 9284, 412, 1935, 300, 291, 393, 2464, 300, 486, 5039, 5221, 1349, 2740, 11, 597, 366, 411, 498, 291, 50996], "temperature": 0.0, "avg_logprob": -0.17018185962330212, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0022157011553645134}, {"id": 73, "seek": 47608, "start": 488.71999999999997, "end": 493.68, "text": " have four apples, I gave you twice the amount of apples you have. How many do you have now?", "tokens": [50996, 362, 1451, 16814, 11, 286, 2729, 291, 6091, 264, 2372, 295, 16814, 291, 362, 13, 1012, 867, 360, 291, 362, 586, 30, 51244], "temperature": 0.0, "avg_logprob": -0.17018185962330212, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0022157011553645134}, {"id": 74, "seek": 47608, "start": 495.84, "end": 504.08, "text": " But if you define a way to tackle these questions at each individual step, the model can decide", "tokens": [51352, 583, 498, 291, 6964, 257, 636, 281, 14896, 613, 1651, 412, 1184, 2609, 1823, 11, 264, 2316, 393, 4536, 51764], "temperature": 0.0, "avg_logprob": -0.17018185962330212, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0022157011553645134}, {"id": 75, "seek": 50408, "start": 504.08, "end": 512.8, "text": " how to apply that flexibly based on the particular question. Yeah, that might be a good thing to", "tokens": [50364, 577, 281, 3079, 300, 5896, 3545, 2361, 322, 264, 1729, 1168, 13, 865, 11, 300, 1062, 312, 257, 665, 551, 281, 50800], "temperature": 0.0, "avg_logprob": -0.155302497778046, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.014861229807138443}, {"id": 76, "seek": 50408, "start": 512.8, "end": 521.1999999999999, "text": " have. But yeah, right now, I don't know. There is no good benchmark for these kind of things.", "tokens": [50800, 362, 13, 583, 1338, 11, 558, 586, 11, 286, 500, 380, 458, 13, 821, 307, 572, 665, 18927, 337, 613, 733, 295, 721, 13, 51220], "temperature": 0.0, "avg_logprob": -0.155302497778046, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.014861229807138443}, {"id": 77, "seek": 50408, "start": 522.24, "end": 528.64, "text": " Yeah. And in your paper, so you came up with a new algorithmic prompting technique and you", "tokens": [51272, 865, 13, 400, 294, 428, 3035, 11, 370, 291, 1361, 493, 365, 257, 777, 9284, 299, 12391, 278, 6532, 293, 291, 51592], "temperature": 0.0, "avg_logprob": -0.155302497778046, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.014861229807138443}, {"id": 78, "seek": 50408, "start": 528.64, "end": 533.52, "text": " designed some experiments and your technique works significantly better than some of the", "tokens": [51592, 4761, 512, 12050, 293, 428, 6532, 1985, 10591, 1101, 813, 512, 295, 264, 51836], "temperature": 0.0, "avg_logprob": -0.155302497778046, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.014861229807138443}, {"id": 79, "seek": 53352, "start": 533.6, "end": 536.64, "text": " other in context prompting techniques. Can you sketch that out for us?", "tokens": [50368, 661, 294, 4319, 12391, 278, 7512, 13, 1664, 291, 12325, 300, 484, 337, 505, 30, 50520], "temperature": 0.0, "avg_logprob": -0.14303990627856963, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.0014988334150984883}, {"id": 80, "seek": 53352, "start": 537.36, "end": 544.72, "text": " Yeah, so the intuition is very simple, actually. When we look at the addition algorithm, the one", "tokens": [50556, 865, 11, 370, 264, 24002, 307, 588, 2199, 11, 767, 13, 1133, 321, 574, 412, 264, 4500, 9284, 11, 264, 472, 50924], "temperature": 0.0, "avg_logprob": -0.14303990627856963, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.0014988334150984883}, {"id": 81, "seek": 53352, "start": 544.72, "end": 552.48, "text": " that we learned in school, we know that you start with the rightmost digits, you add them up,", "tokens": [50924, 300, 321, 3264, 294, 1395, 11, 321, 458, 300, 291, 722, 365, 264, 558, 1761, 27011, 11, 291, 909, 552, 493, 11, 51312], "temperature": 0.0, "avg_logprob": -0.14303990627856963, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.0014988334150984883}, {"id": 82, "seek": 53352, "start": 552.48, "end": 559.68, "text": " you add them up. If it's greater than 10, you have a carry of one, and then you add the carry to", "tokens": [51312, 291, 909, 552, 493, 13, 759, 309, 311, 5044, 813, 1266, 11, 291, 362, 257, 3985, 295, 472, 11, 293, 550, 291, 909, 264, 3985, 281, 51672], "temperature": 0.0, "avg_logprob": -0.14303990627856963, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.0014988334150984883}, {"id": 83, "seek": 55968, "start": 559.68, "end": 569.04, "text": " the next sum and so forth. So a scratch pad approach for addition will show the trace from", "tokens": [50364, 264, 958, 2408, 293, 370, 5220, 13, 407, 257, 8459, 6887, 3109, 337, 4500, 486, 855, 264, 13508, 490, 50832], "temperature": 0.0, "avg_logprob": -0.09427570789418321, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0024334813933819532}, {"id": 84, "seek": 55968, "start": 569.04, "end": 575.4399999999999, "text": " running this algorithm. So it will show the first sum is this, and the carry is this. But it doesn't", "tokens": [50832, 2614, 341, 9284, 13, 407, 309, 486, 855, 264, 700, 2408, 307, 341, 11, 293, 264, 3985, 307, 341, 13, 583, 309, 1177, 380, 51152], "temperature": 0.0, "avg_logprob": -0.09427570789418321, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0024334813933819532}, {"id": 85, "seek": 55968, "start": 575.4399999999999, "end": 582.4, "text": " explain how those values are derived. But for us, it feels really natural because we're so familiar", "tokens": [51152, 2903, 577, 729, 4190, 366, 18949, 13, 583, 337, 505, 11, 309, 3417, 534, 3303, 570, 321, 434, 370, 4963, 51500], "temperature": 0.0, "avg_logprob": -0.09427570789418321, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0024334813933819532}, {"id": 86, "seek": 55968, "start": 582.4, "end": 588.4, "text": " with it. But for a model, trying to infer the rules from seeing a couple examples of it,", "tokens": [51500, 365, 309, 13, 583, 337, 257, 2316, 11, 1382, 281, 13596, 264, 4474, 490, 2577, 257, 1916, 5110, 295, 309, 11, 51800], "temperature": 0.0, "avg_logprob": -0.09427570789418321, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0024334813933819532}, {"id": 87, "seek": 58840, "start": 589.28, "end": 595.36, "text": " that's a very under specified problem. There are many rules that could explain perfectly these", "tokens": [50408, 300, 311, 257, 588, 833, 22206, 1154, 13, 821, 366, 867, 4474, 300, 727, 2903, 6239, 613, 50712], "temperature": 0.0, "avg_logprob": -0.10891984720699123, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.00154783739708364}, {"id": 88, "seek": 58840, "start": 595.36, "end": 606.0799999999999, "text": " observations. So algorithmic prompting basically explains each step of an algorithm using some", "tokens": [50712, 18163, 13, 407, 9284, 299, 12391, 278, 1936, 13948, 1184, 1823, 295, 364, 9284, 1228, 512, 51248], "temperature": 0.0, "avg_logprob": -0.10891984720699123, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.00154783739708364}, {"id": 89, "seek": 58840, "start": 606.0799999999999, "end": 613.52, "text": " examples. And within each step tries to be as detailed as possible and tries to disambiguate", "tokens": [51248, 5110, 13, 400, 1951, 1184, 1823, 9898, 281, 312, 382, 9942, 382, 1944, 293, 9898, 281, 717, 2173, 328, 10107, 51620], "temperature": 0.0, "avg_logprob": -0.10891984720699123, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.00154783739708364}, {"id": 90, "seek": 61352, "start": 614.3199999999999, "end": 620.24, "text": " as much as possible what you want the model to do. And it turns out that when you provide more", "tokens": [50404, 382, 709, 382, 1944, 437, 291, 528, 264, 2316, 281, 360, 13, 400, 309, 4523, 484, 300, 562, 291, 2893, 544, 50700], "temperature": 0.0, "avg_logprob": -0.10057656661323879, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.007868138141930103}, {"id": 91, "seek": 61352, "start": 620.24, "end": 626.72, "text": " details to the model, you're sort of constraining the model's interpretation of disinformation", "tokens": [50700, 4365, 281, 264, 2316, 11, 291, 434, 1333, 295, 11525, 1760, 264, 2316, 311, 14174, 295, 717, 20941, 51024], "temperature": 0.0, "avg_logprob": -0.10057656661323879, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.007868138141930103}, {"id": 92, "seek": 61352, "start": 626.72, "end": 635.28, "text": " so that there's only one way to apply this. And with that, you can get more robust behavior from", "tokens": [51024, 370, 300, 456, 311, 787, 472, 636, 281, 3079, 341, 13, 400, 365, 300, 11, 291, 393, 483, 544, 13956, 5223, 490, 51452], "temperature": 0.0, "avg_logprob": -0.10057656661323879, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.007868138141930103}, {"id": 93, "seek": 63528, "start": 635.36, "end": 643.8399999999999, "text": " model and reason very well out of distribution. Yeah, because it's often said that deep learning", "tokens": [50368, 2316, 293, 1778, 588, 731, 484, 295, 7316, 13, 865, 11, 570, 309, 311, 2049, 848, 300, 2452, 2539, 50792], "temperature": 0.0, "avg_logprob": -0.1082108391655816, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.13184072077274323}, {"id": 94, "seek": 63528, "start": 643.8399999999999, "end": 649.04, "text": " models do not reason. And I think what people mean by that is that you get this phenomenon of", "tokens": [50792, 5245, 360, 406, 1778, 13, 400, 286, 519, 437, 561, 914, 538, 300, 307, 300, 291, 483, 341, 14029, 295, 51052], "temperature": 0.0, "avg_logprob": -0.1082108391655816, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.13184072077274323}, {"id": 95, "seek": 63528, "start": 649.04, "end": 655.92, "text": " shortcut learning and models do the right things for the wrong reasons. And it seems to me that", "tokens": [51052, 24822, 2539, 293, 5245, 360, 264, 558, 721, 337, 264, 2085, 4112, 13, 400, 309, 2544, 281, 385, 300, 51396], "temperature": 0.0, "avg_logprob": -0.1082108391655816, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.13184072077274323}, {"id": 96, "seek": 63528, "start": 655.92, "end": 661.92, "text": " what we're doing here is by imputing the kind of the structure of how to reason into the prompt,", "tokens": [51396, 437, 321, 434, 884, 510, 307, 538, 704, 10861, 264, 733, 295, 264, 3877, 295, 577, 281, 1778, 666, 264, 12391, 11, 51696], "temperature": 0.0, "avg_logprob": -0.1082108391655816, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.13184072077274323}, {"id": 97, "seek": 66192, "start": 661.92, "end": 668.0, "text": " we're robustifying its behavior out of distribution. Yeah, well said. That's exactly right.", "tokens": [50364, 321, 434, 13956, 5489, 1080, 5223, 484, 295, 7316, 13, 865, 11, 731, 848, 13, 663, 311, 2293, 558, 13, 50668], "temperature": 0.0, "avg_logprob": -0.12317701622291848, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0027667556423693895}, {"id": 98, "seek": 66192, "start": 669.04, "end": 675.5999999999999, "text": " Fascinating. So what do you think of this problem of shortcut learning in general? Because you know,", "tokens": [50720, 49098, 8205, 13, 407, 437, 360, 291, 519, 295, 341, 1154, 295, 24822, 2539, 294, 2674, 30, 1436, 291, 458, 11, 51048], "temperature": 0.0, "avg_logprob": -0.12317701622291848, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0027667556423693895}, {"id": 99, "seek": 66192, "start": 675.5999999999999, "end": 681.52, "text": " like Melanie Mitchell said, there are two modes of understanding essentially. We have this anthropomorphic", "tokens": [51048, 411, 42798, 27582, 848, 11, 456, 366, 732, 14068, 295, 3701, 4476, 13, 492, 362, 341, 22727, 32702, 299, 51344], "temperature": 0.0, "avg_logprob": -0.12317701622291848, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0027667556423693895}, {"id": 100, "seek": 66192, "start": 681.52, "end": 686.88, "text": " mode of understanding which is using causality and it's very sample efficient and we have a way of", "tokens": [51344, 4391, 295, 3701, 597, 307, 1228, 3302, 1860, 293, 309, 311, 588, 6889, 7148, 293, 321, 362, 257, 636, 295, 51612], "temperature": 0.0, "avg_logprob": -0.12317701622291848, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0027667556423693895}, {"id": 101, "seek": 66192, "start": 686.88, "end": 691.04, "text": " understanding the world. And we have a bit of an intuition that large language models", "tokens": [51612, 3701, 264, 1002, 13, 400, 321, 362, 257, 857, 295, 364, 24002, 300, 2416, 2856, 5245, 51820], "temperature": 0.0, "avg_logprob": -0.12317701622291848, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.0027667556423693895}, {"id": 102, "seek": 69104, "start": 691.04, "end": 697.28, "text": " don't reason the way that we do. But is it necessarily a problem? And is it cheating in", "tokens": [50364, 500, 380, 1778, 264, 636, 300, 321, 360, 13, 583, 307, 309, 4725, 257, 1154, 30, 400, 307, 309, 18309, 294, 50676], "temperature": 0.0, "avg_logprob": -0.12136436120057717, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.004055206198245287}, {"id": 103, "seek": 69104, "start": 697.28, "end": 701.8399999999999, "text": " your view using in-context learning? Or do you think that because we haven't had to train the", "tokens": [50676, 428, 1910, 1228, 294, 12, 9000, 3828, 2539, 30, 1610, 360, 291, 519, 300, 570, 321, 2378, 380, 632, 281, 3847, 264, 50904], "temperature": 0.0, "avg_logprob": -0.12136436120057717, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.004055206198245287}, {"id": 104, "seek": 69104, "start": 701.8399999999999, "end": 711.04, "text": " model again, what's the problem, right? I mean, is it an issue? Yeah, I mean, I guess there's", "tokens": [50904, 2316, 797, 11, 437, 311, 264, 1154, 11, 558, 30, 286, 914, 11, 307, 309, 364, 2734, 30, 865, 11, 286, 914, 11, 286, 2041, 456, 311, 51364], "temperature": 0.0, "avg_logprob": -0.12136436120057717, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.004055206198245287}, {"id": 105, "seek": 71104, "start": 711.04, "end": 721.8399999999999, "text": " many forms of reasoning and algorithmic reasoning is only a subset of that. And I think if the model", "tokens": [50364, 867, 6422, 295, 21577, 293, 9284, 299, 21577, 307, 787, 257, 25993, 295, 300, 13, 400, 286, 519, 498, 264, 2316, 50904], "temperature": 0.0, "avg_logprob": -0.07272217878654821, "compression_ratio": 1.6536312849162011, "no_speech_prob": 0.09940041601657867}, {"id": 106, "seek": 71104, "start": 721.8399999999999, "end": 731.8399999999999, "text": " can output the reasoning process and show that the answer that arrives at is following the output", "tokens": [50904, 393, 5598, 264, 21577, 1399, 293, 855, 300, 264, 1867, 300, 20116, 412, 307, 3480, 264, 5598, 51404], "temperature": 0.0, "avg_logprob": -0.07272217878654821, "compression_ratio": 1.6536312849162011, "no_speech_prob": 0.09940041601657867}, {"id": 107, "seek": 71104, "start": 731.8399999999999, "end": 738.88, "text": " of that process, then it's hard to argue that it's not doing reasoning. It might work differently", "tokens": [51404, 295, 300, 1399, 11, 550, 309, 311, 1152, 281, 9695, 300, 309, 311, 406, 884, 21577, 13, 467, 1062, 589, 7614, 51756], "temperature": 0.0, "avg_logprob": -0.07272217878654821, "compression_ratio": 1.6536312849162011, "no_speech_prob": 0.09940041601657867}, {"id": 108, "seek": 73888, "start": 738.88, "end": 748.0, "text": " under the hood, but it follows the similar process. Now, you know, some methods, you output a rationale,", "tokens": [50364, 833, 264, 13376, 11, 457, 309, 10002, 264, 2531, 1399, 13, 823, 11, 291, 458, 11, 512, 7150, 11, 291, 5598, 257, 41989, 11, 50820], "temperature": 0.0, "avg_logprob": -0.09924297948037425, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.011864188127219677}, {"id": 109, "seek": 73888, "start": 748.0, "end": 753.12, "text": " but the final answer is actually different from what the explanation suggests it should be.", "tokens": [50820, 457, 264, 2572, 1867, 307, 767, 819, 490, 437, 264, 10835, 13409, 309, 820, 312, 13, 51076], "temperature": 0.0, "avg_logprob": -0.09924297948037425, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.011864188127219677}, {"id": 110, "seek": 73888, "start": 754.16, "end": 760.48, "text": " And maybe you can point to that and say, oh, the model isn't actually using this. It's just like", "tokens": [51128, 400, 1310, 291, 393, 935, 281, 300, 293, 584, 11, 1954, 11, 264, 2316, 1943, 380, 767, 1228, 341, 13, 467, 311, 445, 411, 51444], "temperature": 0.0, "avg_logprob": -0.09924297948037425, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.011864188127219677}, {"id": 111, "seek": 73888, "start": 760.48, "end": 765.76, "text": " giving you something that you asked for, but then the final answer is still using a shortcut.", "tokens": [51444, 2902, 291, 746, 300, 291, 2351, 337, 11, 457, 550, 264, 2572, 1867, 307, 920, 1228, 257, 24822, 13, 51708], "temperature": 0.0, "avg_logprob": -0.09924297948037425, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.011864188127219677}, {"id": 112, "seek": 76576, "start": 766.24, "end": 772.72, "text": " But with the algorithmic reasoning, we see that that's not the case. And so, yeah, the more you", "tokens": [50388, 583, 365, 264, 9284, 299, 21577, 11, 321, 536, 300, 300, 311, 406, 264, 1389, 13, 400, 370, 11, 1338, 11, 264, 544, 291, 50712], "temperature": 0.0, "avg_logprob": -0.12724349262950185, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.002082304796203971}, {"id": 113, "seek": 76576, "start": 772.72, "end": 782.64, "text": " constrain things, maybe the more you remove shortcuts from the model. But an interesting", "tokens": [50712, 1817, 7146, 721, 11, 1310, 264, 544, 291, 4159, 34620, 490, 264, 2316, 13, 583, 364, 1880, 51208], "temperature": 0.0, "avg_logprob": -0.12724349262950185, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.002082304796203971}, {"id": 114, "seek": 76576, "start": 782.64, "end": 788.0, "text": " question, I think, is you can get this behavior using in-context learning, which I think you,", "tokens": [51208, 1168, 11, 286, 519, 11, 307, 291, 393, 483, 341, 5223, 1228, 294, 12, 9000, 3828, 2539, 11, 597, 286, 519, 291, 11, 51476], "temperature": 0.0, "avg_logprob": -0.12724349262950185, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.002082304796203971}, {"id": 115, "seek": 76576, "start": 788.8, "end": 793.92, "text": " I suspect you can't really do from fine tuning or some sort of weight training.", "tokens": [51516, 286, 9091, 291, 393, 380, 534, 360, 490, 2489, 15164, 420, 512, 1333, 295, 3364, 3097, 13, 51772], "temperature": 0.0, "avg_logprob": -0.12724349262950185, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.002082304796203971}, {"id": 116, "seek": 79392, "start": 794.88, "end": 800.24, "text": " I think when you do that, you'll most likely just overfit on the training distribution.", "tokens": [50412, 286, 519, 562, 291, 360, 300, 11, 291, 603, 881, 3700, 445, 670, 6845, 322, 264, 3097, 7316, 13, 50680], "temperature": 0.0, "avg_logprob": -0.08823638851359739, "compression_ratio": 1.6701754385964913, "no_speech_prob": 0.00384537223726511}, {"id": 117, "seek": 79392, "start": 800.88, "end": 805.52, "text": " I wanted to ask you, you know, it's a bit of an open-ended question to say what's going on inside", "tokens": [50712, 286, 1415, 281, 1029, 291, 11, 291, 458, 11, 309, 311, 257, 857, 295, 364, 1269, 12, 3502, 1168, 281, 584, 437, 311, 516, 322, 1854, 50944], "temperature": 0.0, "avg_logprob": -0.08823638851359739, "compression_ratio": 1.6701754385964913, "no_speech_prob": 0.00384537223726511}, {"id": 118, "seek": 79392, "start": 805.52, "end": 811.76, "text": " large language models. But what's so exciting to me is that you get all of this emergent strange", "tokens": [50944, 2416, 2856, 5245, 13, 583, 437, 311, 370, 4670, 281, 385, 307, 300, 291, 483, 439, 295, 341, 4345, 6930, 5861, 51256], "temperature": 0.0, "avg_logprob": -0.08823638851359739, "compression_ratio": 1.6701754385964913, "no_speech_prob": 0.00384537223726511}, {"id": 119, "seek": 79392, "start": 811.76, "end": 815.92, "text": " behavior. No one would have imagined five years ago that we could do all of this prompt engineering", "tokens": [51256, 5223, 13, 883, 472, 576, 362, 16590, 1732, 924, 2057, 300, 321, 727, 360, 439, 295, 341, 12391, 7043, 51464], "temperature": 0.0, "avg_logprob": -0.08823638851359739, "compression_ratio": 1.6701754385964913, "no_speech_prob": 0.00384537223726511}, {"id": 120, "seek": 79392, "start": 815.92, "end": 820.88, "text": " on a kind of autoregressive language decoder. And the model is trained to do something really", "tokens": [51464, 322, 257, 733, 295, 1476, 418, 3091, 488, 2856, 979, 19866, 13, 400, 264, 2316, 307, 8895, 281, 360, 746, 534, 51712], "temperature": 0.0, "avg_logprob": -0.08823638851359739, "compression_ratio": 1.6701754385964913, "no_speech_prob": 0.00384537223726511}, {"id": 121, "seek": 82088, "start": 820.88, "end": 828.88, "text": " quite trivial, yet as a byproduct, all of this crazy stuff emerges in its internal representation.", "tokens": [50364, 1596, 26703, 11, 1939, 382, 257, 538, 33244, 11, 439, 295, 341, 3219, 1507, 38965, 294, 1080, 6920, 10290, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10552730560302734, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.002272702055051923}, {"id": 122, "seek": 82088, "start": 828.88, "end": 834.16, "text": " And all of this algorithmic reasoning capability seems to be like a side effect of that training.", "tokens": [50764, 400, 439, 295, 341, 9284, 299, 21577, 13759, 2544, 281, 312, 411, 257, 1252, 1802, 295, 300, 3097, 13, 51028], "temperature": 0.0, "avg_logprob": -0.10552730560302734, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.002272702055051923}, {"id": 123, "seek": 82088, "start": 835.52, "end": 839.2, "text": " How does that even happen? Oh, that's a good question.", "tokens": [51096, 1012, 775, 300, 754, 1051, 30, 876, 11, 300, 311, 257, 665, 1168, 13, 51280], "temperature": 0.0, "avg_logprob": -0.10552730560302734, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.002272702055051923}, {"id": 124, "seek": 82088, "start": 841.92, "end": 847.6, "text": " It's possible that in order to fit that large training data set, the pre-training data set,", "tokens": [51416, 467, 311, 1944, 300, 294, 1668, 281, 3318, 300, 2416, 3097, 1412, 992, 11, 264, 659, 12, 17227, 1760, 1412, 992, 11, 51700], "temperature": 0.0, "avg_logprob": -0.10552730560302734, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.002272702055051923}, {"id": 125, "seek": 84760, "start": 848.48, "end": 857.36, "text": " you have to find regularities in content, I guess, that humans generate. And", "tokens": [50408, 291, 362, 281, 915, 3890, 1088, 294, 2701, 11, 286, 2041, 11, 300, 6255, 8460, 13, 400, 50852], "temperature": 0.0, "avg_logprob": -0.13322640574255654, "compression_ratio": 1.3951612903225807, "no_speech_prob": 0.002114190487191081}, {"id": 126, "seek": 84760, "start": 858.96, "end": 865.0400000000001, "text": " I think some of these abilities come out of those regularities that you learn. So the ability to", "tokens": [50932, 286, 519, 512, 295, 613, 11582, 808, 484, 295, 729, 3890, 1088, 300, 291, 1466, 13, 407, 264, 3485, 281, 51236], "temperature": 0.0, "avg_logprob": -0.13322640574255654, "compression_ratio": 1.3951612903225807, "no_speech_prob": 0.002114190487191081}, {"id": 127, "seek": 86504, "start": 866.0, "end": 877.4399999999999, "text": " refer to the pattern of a previous passage in context and, you know,", "tokens": [50412, 2864, 281, 264, 5102, 295, 257, 3894, 11497, 294, 4319, 293, 11, 291, 458, 11, 50984], "temperature": 0.0, "avg_logprob": -0.12869204849493307, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.024782827123999596}, {"id": 128, "seek": 86504, "start": 878.8, "end": 883.68, "text": " see what the relationships are in that pattern and apply the same relationships when you input", "tokens": [51052, 536, 437, 264, 6159, 366, 294, 300, 5102, 293, 3079, 264, 912, 6159, 562, 291, 4846, 51296], "temperature": 0.0, "avg_logprob": -0.12869204849493307, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.024782827123999596}, {"id": 129, "seek": 86504, "start": 885.68, "end": 892.7199999999999, "text": " that circuit, I think, is just very useful as a way to summarize that large data set. And so", "tokens": [51396, 300, 9048, 11, 286, 519, 11, 307, 445, 588, 4420, 382, 257, 636, 281, 20858, 300, 2416, 1412, 992, 13, 400, 370, 51748], "temperature": 0.0, "avg_logprob": -0.12869204849493307, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.024782827123999596}, {"id": 130, "seek": 89272, "start": 892.72, "end": 902.0, "text": " because you're forced to compress all that data into a model set of weights, I think these regularities", "tokens": [50364, 570, 291, 434, 7579, 281, 14778, 439, 300, 1412, 666, 257, 2316, 992, 295, 17443, 11, 286, 519, 613, 3890, 1088, 50828], "temperature": 0.0, "avg_logprob": -0.1762854772455552, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.0005974452942609787}, {"id": 131, "seek": 89272, "start": 903.12, "end": 911.84, "text": " emerge somehow. I don't know exactly how. But I think, yeah, I mean, this is an interesting", "tokens": [50884, 21511, 6063, 13, 286, 500, 380, 458, 2293, 577, 13, 583, 286, 519, 11, 1338, 11, 286, 914, 11, 341, 307, 364, 1880, 51320], "temperature": 0.0, "avg_logprob": -0.1762854772455552, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.0005974452942609787}, {"id": 132, "seek": 89272, "start": 911.84, "end": 918.32, "text": " speculation because then you can say with much larger models where there is no capacity constraint", "tokens": [51320, 27696, 570, 550, 291, 393, 584, 365, 709, 4833, 5245, 689, 456, 307, 572, 6042, 25534, 51644], "temperature": 0.0, "avg_logprob": -0.1762854772455552, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.0005974452942609787}, {"id": 133, "seek": 91832, "start": 918.32, "end": 927.9200000000001, "text": " at all, and you fit all the same data sets, it's not going to learn very interesting behavior", "tokens": [50364, 412, 439, 11, 293, 291, 3318, 439, 264, 912, 1412, 6352, 11, 309, 311, 406, 516, 281, 1466, 588, 1880, 5223, 50844], "temperature": 0.0, "avg_logprob": -0.1315372995583408, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.003323144279420376}, {"id": 134, "seek": 91832, "start": 927.9200000000001, "end": 933.6800000000001, "text": " because it's able to just fit the model without capturing the underlying patterns.", "tokens": [50844, 570, 309, 311, 1075, 281, 445, 3318, 264, 2316, 1553, 23384, 264, 14217, 8294, 13, 51132], "temperature": 0.0, "avg_logprob": -0.1315372995583408, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.003323144279420376}, {"id": 135, "seek": 91832, "start": 935.44, "end": 941.12, "text": " And maybe that's why you actually need more training data and training longer rather than", "tokens": [51220, 400, 1310, 300, 311, 983, 291, 767, 643, 544, 3097, 1412, 293, 3097, 2854, 2831, 813, 51504], "temperature": 0.0, "avg_logprob": -0.1315372995583408, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.003323144279420376}, {"id": 136, "seek": 91832, "start": 941.12, "end": 947.5200000000001, "text": " like the optimal scaling is not right now in the model size, the amount of data. Because you want", "tokens": [51504, 411, 264, 16252, 21589, 307, 406, 558, 586, 294, 264, 2316, 2744, 11, 264, 2372, 295, 1412, 13, 1436, 291, 528, 51824], "temperature": 0.0, "avg_logprob": -0.1315372995583408, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.003323144279420376}, {"id": 137, "seek": 94752, "start": 947.52, "end": 954.64, "text": " like the right bottleneck for your representation. And I think maybe that's where these", "tokens": [50364, 411, 264, 558, 44641, 547, 337, 428, 10290, 13, 400, 286, 519, 1310, 300, 311, 689, 613, 50720], "temperature": 0.0, "avg_logprob": -0.15329812212688168, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.001108587603084743}, {"id": 138, "seek": 94752, "start": 956.96, "end": 962.4, "text": " emergent capabilities are coming from. Interesting. Yeah. And also the amount of training as well as", "tokens": [50836, 4345, 6930, 10862, 366, 1348, 490, 13, 14711, 13, 865, 13, 400, 611, 264, 2372, 295, 3097, 382, 731, 382, 51108], "temperature": 0.0, "avg_logprob": -0.15329812212688168, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.001108587603084743}, {"id": 139, "seek": 94752, "start": 962.4, "end": 970.16, "text": " the amount of data. I wondered what is your intuition on the practical kind of computational", "tokens": [51108, 264, 2372, 295, 1412, 13, 286, 17055, 437, 307, 428, 24002, 322, 264, 8496, 733, 295, 28270, 51496], "temperature": 0.0, "avg_logprob": -0.15329812212688168, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.001108587603084743}, {"id": 140, "seek": 94752, "start": 970.16, "end": 974.4, "text": " limitations of large language models? So at the moment, they're trained to transform us.", "tokens": [51496, 15705, 295, 2416, 2856, 5245, 30, 407, 412, 264, 1623, 11, 436, 434, 8895, 281, 4088, 505, 13, 51708], "temperature": 0.0, "avg_logprob": -0.15329812212688168, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.001108587603084743}, {"id": 141, "seek": 97440, "start": 974.48, "end": 980.3199999999999, "text": " And there have been some pretty cool critiques of connectionism by Fodor and a few other people.", "tokens": [50368, 400, 456, 362, 668, 512, 1238, 1627, 3113, 4911, 295, 4984, 1434, 538, 479, 34024, 293, 257, 1326, 661, 561, 13, 50660], "temperature": 0.0, "avg_logprob": -0.0982150946344648, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.010564339347183704}, {"id": 142, "seek": 97440, "start": 980.3199999999999, "end": 985.92, "text": " And it's basically along the lines of neural networks can't represent infinite objects,", "tokens": [50660, 400, 309, 311, 1936, 2051, 264, 3876, 295, 18161, 9590, 393, 380, 2906, 13785, 6565, 11, 50940], "temperature": 0.0, "avg_logprob": -0.0982150946344648, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.010564339347183704}, {"id": 143, "seek": 97440, "start": 985.92, "end": 990.72, "text": " which kind of distinguishes them from Turing machines. So they can't compute the nth digit of", "tokens": [50940, 597, 733, 295, 11365, 16423, 552, 490, 314, 1345, 8379, 13, 407, 436, 393, 380, 14722, 264, 297, 392, 14293, 295, 51180], "temperature": 0.0, "avg_logprob": -0.0982150946344648, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.010564339347183704}, {"id": 144, "seek": 97440, "start": 990.72, "end": 995.52, "text": " time is a fairly good example. But the amazing thing is that we're doing all of this stuff with", "tokens": [51180, 565, 307, 257, 6457, 665, 1365, 13, 583, 264, 2243, 551, 307, 300, 321, 434, 884, 439, 295, 341, 1507, 365, 51420], "temperature": 0.0, "avg_logprob": -0.0982150946344648, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.010564339347183704}, {"id": 145, "seek": 97440, "start": 995.52, "end": 1000.8, "text": " algorithmic reasoning. And we haven't found the ceiling yet, it's just getting better and better.", "tokens": [51420, 9284, 299, 21577, 13, 400, 321, 2378, 380, 1352, 264, 13655, 1939, 11, 309, 311, 445, 1242, 1101, 293, 1101, 13, 51684], "temperature": 0.0, "avg_logprob": -0.0982150946344648, "compression_ratio": 1.6109215017064846, "no_speech_prob": 0.010564339347183704}, {"id": 146, "seek": 100080, "start": 1000.8, "end": 1007.76, "text": " And I think it's almost creating this. Well, I mean, I'm becoming quite hopeful,", "tokens": [50364, 400, 286, 519, 309, 311, 1920, 4084, 341, 13, 1042, 11, 286, 914, 11, 286, 478, 5617, 1596, 20531, 11, 50712], "temperature": 0.0, "avg_logprob": -0.077134065138988, "compression_ratio": 1.5560975609756098, "no_speech_prob": 0.0005947588942945004}, {"id": 147, "seek": 100080, "start": 1007.76, "end": 1011.8399999999999, "text": " actually, because I don't know where the limit is, but I suspect there is a limit quite soon.", "tokens": [50712, 767, 11, 570, 286, 500, 380, 458, 689, 264, 4948, 307, 11, 457, 286, 9091, 456, 307, 257, 4948, 1596, 2321, 13, 50916], "temperature": 0.0, "avg_logprob": -0.077134065138988, "compression_ratio": 1.5560975609756098, "no_speech_prob": 0.0005947588942945004}, {"id": 148, "seek": 100080, "start": 1011.8399999999999, "end": 1014.9599999999999, "text": " How do you think about what the realistic computational limits are?", "tokens": [50916, 1012, 360, 291, 519, 466, 437, 264, 12465, 28270, 10406, 366, 30, 51072], "temperature": 0.0, "avg_logprob": -0.077134065138988, "compression_ratio": 1.5560975609756098, "no_speech_prob": 0.0005947588942945004}, {"id": 149, "seek": 100080, "start": 1016.64, "end": 1021.92, "text": " I think the fact that now we have in context learning is interesting because", "tokens": [51156, 286, 519, 264, 1186, 300, 586, 321, 362, 294, 4319, 2539, 307, 1880, 570, 51420], "temperature": 0.0, "avg_logprob": -0.077134065138988, "compression_ratio": 1.5560975609756098, "no_speech_prob": 0.0005947588942945004}, {"id": 150, "seek": 102192, "start": 1022.88, "end": 1031.76, "text": " that allows us to have, I guess, adaptive amount of computation. And so if you have,", "tokens": [50412, 300, 4045, 505, 281, 362, 11, 286, 2041, 11, 27912, 2372, 295, 24903, 13, 400, 370, 498, 291, 362, 11, 50856], "temperature": 0.0, "avg_logprob": -0.16231661131887726, "compression_ratio": 1.6424242424242423, "no_speech_prob": 0.025160983204841614}, {"id": 151, "seek": 102192, "start": 1031.76, "end": 1038.96, "text": " let's say you have infinite context length, then you can sort of maybe do infinite computation", "tokens": [50856, 718, 311, 584, 291, 362, 13785, 4319, 4641, 11, 550, 291, 393, 1333, 295, 1310, 360, 13785, 24903, 51216], "temperature": 0.0, "avg_logprob": -0.16231661131887726, "compression_ratio": 1.6424242424242423, "no_speech_prob": 0.025160983204841614}, {"id": 152, "seek": 102192, "start": 1038.96, "end": 1046.48, "text": " in that case. Now, is infinite context length possible? Probably not. But then you can find", "tokens": [51216, 294, 300, 1389, 13, 823, 11, 307, 13785, 4319, 4641, 1944, 30, 9210, 406, 13, 583, 550, 291, 393, 915, 51592], "temperature": 0.0, "avg_logprob": -0.16231661131887726, "compression_ratio": 1.6424242424242423, "no_speech_prob": 0.025160983204841614}, {"id": 153, "seek": 104648, "start": 1046.56, "end": 1052.08, "text": " clever ways to distill that information. You can find clever attention mechanisms.", "tokens": [50368, 13494, 2098, 281, 42923, 300, 1589, 13, 509, 393, 915, 13494, 3202, 15902, 13, 50644], "temperature": 0.0, "avg_logprob": -0.1165278824892911, "compression_ratio": 1.5803571428571428, "no_speech_prob": 0.06049961969256401}, {"id": 154, "seek": 104648, "start": 1052.88, "end": 1060.16, "text": " And so I think maybe there's a computational limit, but you can always find new ideas that", "tokens": [50684, 400, 370, 286, 519, 1310, 456, 311, 257, 28270, 4948, 11, 457, 291, 393, 1009, 915, 777, 3487, 300, 51048], "temperature": 0.0, "avg_logprob": -0.1165278824892911, "compression_ratio": 1.5803571428571428, "no_speech_prob": 0.06049961969256401}, {"id": 155, "seek": 104648, "start": 1060.88, "end": 1067.6, "text": " make existing methods more efficient. And so, yeah, I have no idea when you would hit that", "tokens": [51084, 652, 6741, 7150, 544, 7148, 13, 400, 370, 11, 1338, 11, 286, 362, 572, 1558, 562, 291, 576, 2045, 300, 51420], "temperature": 0.0, "avg_logprob": -0.1165278824892911, "compression_ratio": 1.5803571428571428, "no_speech_prob": 0.06049961969256401}, {"id": 156, "seek": 104648, "start": 1067.6, "end": 1073.04, "text": " limit, but it's probably very far into the future. Amazing. And so, Hattie, we're here in", "tokens": [51420, 4948, 11, 457, 309, 311, 1391, 588, 1400, 666, 264, 2027, 13, 14165, 13, 400, 370, 11, 389, 1591, 414, 11, 321, 434, 510, 294, 51692], "temperature": 0.0, "avg_logprob": -0.1165278824892911, "compression_ratio": 1.5803571428571428, "no_speech_prob": 0.06049961969256401}, {"id": 157, "seek": 107304, "start": 1073.12, "end": 1077.44, "text": " Europe. What have you taken from the conference so far? And what are you most excited about,", "tokens": [50368, 3315, 13, 708, 362, 291, 2726, 490, 264, 7586, 370, 1400, 30, 400, 437, 366, 291, 881, 2919, 466, 11, 50584], "temperature": 0.0, "avg_logprob": -0.13307950116585995, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.07759355753660202}, {"id": 158, "seek": 107304, "start": 1077.44, "end": 1086.32, "text": " you know, going forward? Yeah, I don't know, because I haven't checked out the posters", "tokens": [50584, 291, 458, 11, 516, 2128, 30, 865, 11, 286, 500, 380, 458, 11, 570, 286, 2378, 380, 10033, 484, 264, 28172, 51028], "temperature": 0.0, "avg_logprob": -0.13307950116585995, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.07759355753660202}, {"id": 159, "seek": 107304, "start": 1086.96, "end": 1098.1599999999999, "text": " very much yet. But I'm excited for the Math AI workshop, which is many other papers exploring", "tokens": [51060, 588, 709, 1939, 13, 583, 286, 478, 2919, 337, 264, 15776, 7318, 13541, 11, 597, 307, 867, 661, 10577, 12736, 51620], "temperature": 0.0, "avg_logprob": -0.13307950116585995, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.07759355753660202}, {"id": 160, "seek": 109816, "start": 1098.16, "end": 1107.0400000000002, "text": " this idea of doing math with language models. And yeah, there's some, you know, very impressive", "tokens": [50364, 341, 1558, 295, 884, 5221, 365, 2856, 5245, 13, 400, 1338, 11, 456, 311, 512, 11, 291, 458, 11, 588, 8992, 50808], "temperature": 0.0, "avg_logprob": -0.09782319455533414, "compression_ratio": 1.5105263157894737, "no_speech_prob": 0.09207778424024582}, {"id": 161, "seek": 109816, "start": 1107.0400000000002, "end": 1114.16, "text": " work there. So I'm excited for that. I'm excited to meet people and see what they're thinking about.", "tokens": [50808, 589, 456, 13, 407, 286, 478, 2919, 337, 300, 13, 286, 478, 2919, 281, 1677, 561, 293, 536, 437, 436, 434, 1953, 466, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09782319455533414, "compression_ratio": 1.5105263157894737, "no_speech_prob": 0.09207778424024582}, {"id": 162, "seek": 109816, "start": 1115.0400000000002, "end": 1123.2, "text": " And maybe get some ideas for what to work on next. Yeah, I'm also really interested in the", "tokens": [51208, 400, 1310, 483, 512, 3487, 337, 437, 281, 589, 322, 958, 13, 865, 11, 286, 478, 611, 534, 3102, 294, 264, 51616], "temperature": 0.0, "avg_logprob": -0.09782319455533414, "compression_ratio": 1.5105263157894737, "no_speech_prob": 0.09207778424024582}, {"id": 163, "seek": 112320, "start": 1123.2, "end": 1128.8, "text": " math stuff I spent about an hour speaking with Marcus, is it Marcus Barb, who works under Christian", "tokens": [50364, 5221, 1507, 286, 4418, 466, 364, 1773, 4124, 365, 26574, 11, 307, 309, 26574, 14876, 11, 567, 1985, 833, 5778, 50644], "temperature": 0.0, "avg_logprob": -0.20565350272438743, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.1991092562675476}, {"id": 164, "seek": 112320, "start": 1128.8, "end": 1134.32, "text": " Sagadia, I think he's in your group. Yeah, they're doing some, that's right, they're doing some", "tokens": [50644, 34551, 42343, 11, 286, 519, 415, 311, 294, 428, 1594, 13, 865, 11, 436, 434, 884, 512, 11, 300, 311, 558, 11, 436, 434, 884, 512, 50920], "temperature": 0.0, "avg_logprob": -0.20565350272438743, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.1991092562675476}, {"id": 165, "seek": 112320, "start": 1134.32, "end": 1139.92, "text": " really interesting stuff with basically doing mathematical conjecturing, you know, like representing", "tokens": [50920, 534, 1880, 1507, 365, 1936, 884, 18894, 416, 1020, 1345, 11, 291, 458, 11, 411, 13460, 51200], "temperature": 0.0, "avg_logprob": -0.20565350272438743, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.1991092562675476}, {"id": 166, "seek": 112320, "start": 1139.92, "end": 1144.56, "text": " mathematical expressions in large language models and being able to generate new ones.", "tokens": [51200, 18894, 15277, 294, 2416, 2856, 5245, 293, 885, 1075, 281, 8460, 777, 2306, 13, 51432], "temperature": 0.0, "avg_logprob": -0.20565350272438743, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.1991092562675476}, {"id": 167, "seek": 112320, "start": 1144.56, "end": 1148.64, "text": " It's the kind of thing that you, again, would think was science fiction five years ago and like,", "tokens": [51432, 467, 311, 264, 733, 295, 551, 300, 291, 11, 797, 11, 576, 519, 390, 3497, 13266, 1732, 924, 2057, 293, 411, 11, 51636], "temperature": 0.0, "avg_logprob": -0.20565350272438743, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.1991092562675476}, {"id": 168, "seek": 114864, "start": 1148.64, "end": 1154.4, "text": " remarkably, it works. Exactly. And then by the way, with the mathematical conjecturing, Marcus was", "tokens": [50364, 37381, 11, 309, 1985, 13, 7587, 13, 400, 550, 538, 264, 636, 11, 365, 264, 18894, 416, 1020, 1345, 11, 26574, 390, 50652], "temperature": 0.0, "avg_logprob": -0.1492736650549847, "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.042594123631715775}, {"id": 169, "seek": 114864, "start": 1154.4, "end": 1159.6000000000001, "text": " saying that, unlike with large language models, it only has to be right one in 100 times because", "tokens": [50652, 1566, 300, 11, 8343, 365, 2416, 2856, 5245, 11, 309, 787, 575, 281, 312, 558, 472, 294, 2319, 1413, 570, 50912], "temperature": 0.0, "avg_logprob": -0.1492736650549847, "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.042594123631715775}, {"id": 170, "seek": 114864, "start": 1159.6000000000001, "end": 1165.6000000000001, "text": " they can formally verify it. So it's almost like the bar is actually lower in that sense.", "tokens": [50912, 436, 393, 25983, 16888, 309, 13, 407, 309, 311, 1920, 411, 264, 2159, 307, 767, 3126, 294, 300, 2020, 13, 51212], "temperature": 0.0, "avg_logprob": -0.1492736650549847, "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.042594123631715775}, {"id": 171, "seek": 114864, "start": 1166.24, "end": 1174.16, "text": " Right. Yeah, I mean, that's where the language models informal reasonability is really useful.", "tokens": [51244, 1779, 13, 865, 11, 286, 914, 11, 300, 311, 689, 264, 2856, 5245, 24342, 1778, 2310, 307, 534, 4420, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1492736650549847, "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.042594123631715775}, {"id": 172, "seek": 117416, "start": 1174.88, "end": 1180.88, "text": " Right. Yeah, like the pattern actually is actually useful in a lot of cases.", "tokens": [50400, 1779, 13, 865, 11, 411, 264, 5102, 767, 307, 767, 4420, 294, 257, 688, 295, 3331, 13, 50700], "temperature": 0.0, "avg_logprob": -0.1537519287277054, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.024679604917764664}, {"id": 173, "seek": 117416, "start": 1180.88, "end": 1184.96, "text": " That's really cool. Cool. Well, actually, this has been amazing. Thank you so much for coming", "tokens": [50700, 663, 311, 534, 1627, 13, 8561, 13, 1042, 11, 767, 11, 341, 575, 668, 2243, 13, 1044, 291, 370, 709, 337, 1348, 50904], "temperature": 0.0, "avg_logprob": -0.1537519287277054, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.024679604917764664}, {"id": 174, "seek": 117416, "start": 1184.96, "end": 1189.1200000000001, "text": " on the show and telling us about your research. Oh, thanks for having me. Yeah, it was fun.", "tokens": [50904, 322, 264, 855, 293, 3585, 505, 466, 428, 2132, 13, 876, 11, 3231, 337, 1419, 385, 13, 865, 11, 309, 390, 1019, 13, 51112], "temperature": 0.0, "avg_logprob": -0.1537519287277054, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.024679604917764664}, {"id": 175, "seek": 117416, "start": 1191.76, "end": 1198.3200000000002, "text": " Looking beautiful. So Marcus, it's so nice to meet you. Can you introduce yourself?", "tokens": [51244, 11053, 2238, 13, 407, 26574, 11, 309, 311, 370, 1481, 281, 1677, 291, 13, 1664, 291, 5366, 1803, 30, 51572], "temperature": 0.0, "avg_logprob": -0.1537519287277054, "compression_ratio": 1.5309734513274336, "no_speech_prob": 0.024679604917764664}, {"id": 176, "seek": 119832, "start": 1199.04, "end": 1204.8799999999999, "text": " Hi, I'm Marcus. I work for Google Research together with Christian Segedy and the", "tokens": [50400, 2421, 11, 286, 478, 26574, 13, 286, 589, 337, 3329, 10303, 1214, 365, 5778, 1100, 3004, 88, 293, 264, 50692], "temperature": 0.0, "avg_logprob": -0.25965502767851856, "compression_ratio": 1.5421052631578946, "no_speech_prob": 0.004391350783407688}, {"id": 177, "seek": 119832, "start": 1204.8799999999999, "end": 1210.08, "text": " Antiformer team. Generally, we're working on trying to solve math,", "tokens": [50692, 5130, 8629, 260, 1469, 13, 21082, 11, 321, 434, 1364, 322, 1382, 281, 5039, 5221, 11, 50952], "temperature": 0.0, "avg_logprob": -0.25965502767851856, "compression_ratio": 1.5421052631578946, "no_speech_prob": 0.004391350783407688}, {"id": 178, "seek": 119832, "start": 1212.56, "end": 1219.84, "text": " basically by trying to translate natural language mathematics into formal mathematics and", "tokens": [51076, 1936, 538, 1382, 281, 13799, 3303, 2856, 18666, 666, 9860, 18666, 293, 51440], "temperature": 0.0, "avg_logprob": -0.25965502767851856, "compression_ratio": 1.5421052631578946, "no_speech_prob": 0.004391350783407688}, {"id": 179, "seek": 119832, "start": 1220.72, "end": 1223.4399999999998, "text": " in these formal representations of mathematics, we can", "tokens": [51484, 294, 613, 9860, 33358, 295, 18666, 11, 321, 393, 51620], "temperature": 0.0, "avg_logprob": -0.25965502767851856, "compression_ratio": 1.5421052631578946, "no_speech_prob": 0.004391350783407688}, {"id": 180, "seek": 122344, "start": 1223.6000000000001, "end": 1230.48, "text": " check proofs and use that as a feedback signal for the understanding of mathematics.", "tokens": [50372, 1520, 8177, 82, 293, 764, 300, 382, 257, 5824, 6358, 337, 264, 3701, 295, 18666, 13, 50716], "temperature": 0.0, "avg_logprob": -0.17517616175398043, "compression_ratio": 1.6, "no_speech_prob": 0.0032124873250722885}, {"id": 181, "seek": 122344, "start": 1231.1200000000001, "end": 1235.28, "text": " And most recently, I've been working on long context models like the memorizing transformer,", "tokens": [50748, 400, 881, 3938, 11, 286, 600, 668, 1364, 322, 938, 4319, 5245, 411, 264, 10560, 3319, 31782, 11, 50956], "temperature": 0.0, "avg_logprob": -0.17517616175398043, "compression_ratio": 1.6, "no_speech_prob": 0.0032124873250722885}, {"id": 182, "seek": 122344, "start": 1236.0, "end": 1243.76, "text": " trying to get these models, like makes models sensitive to the exact definitions and other", "tokens": [50992, 1382, 281, 483, 613, 5245, 11, 411, 1669, 5245, 9477, 281, 264, 1900, 21988, 293, 661, 51380], "temperature": 0.0, "avg_logprob": -0.17517616175398043, "compression_ratio": 1.6, "no_speech_prob": 0.0032124873250722885}, {"id": 183, "seek": 122344, "start": 1243.76, "end": 1251.3600000000001, "text": " lemmas that they might use for your improving. And that's an ongoing effort, hopefully more", "tokens": [51380, 7495, 3799, 300, 436, 1062, 764, 337, 428, 11470, 13, 400, 300, 311, 364, 10452, 4630, 11, 4696, 544, 51760], "temperature": 0.0, "avg_logprob": -0.17517616175398043, "compression_ratio": 1.6, "no_speech_prob": 0.0032124873250722885}, {"id": 184, "seek": 125136, "start": 1251.36, "end": 1255.12, "text": " available soon. Can I just say, I'm so jealous that you work with Christian.", "tokens": [50364, 2435, 2321, 13, 1664, 286, 445, 584, 11, 286, 478, 370, 13805, 300, 291, 589, 365, 5778, 13, 50552], "temperature": 0.0, "avg_logprob": -0.1508304554483165, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.028462031856179237}, {"id": 185, "seek": 125136, "start": 1257.36, "end": 1261.12, "text": " I mean, folks will remember that we had a conversation with Christian. I think it was", "tokens": [50664, 286, 914, 11, 4024, 486, 1604, 300, 321, 632, 257, 3761, 365, 5778, 13, 286, 519, 309, 390, 50852], "temperature": 0.0, "avg_logprob": -0.1508304554483165, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.028462031856179237}, {"id": 186, "seek": 125136, "start": 1261.12, "end": 1266.9599999999998, "text": " about 18 months ago. It is one of my favorite episodes of MLST. So you're a very lucky man indeed.", "tokens": [50852, 466, 2443, 2493, 2057, 13, 467, 307, 472, 295, 452, 2954, 9313, 295, 376, 19198, 51, 13, 407, 291, 434, 257, 588, 6356, 587, 6451, 13, 51144], "temperature": 0.0, "avg_logprob": -0.1508304554483165, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.028462031856179237}, {"id": 187, "seek": 125136, "start": 1266.9599999999998, "end": 1273.52, "text": " Yes. Yes. It's great to work with Christian. It's a lot of fun. Amazing.", "tokens": [51144, 1079, 13, 1079, 13, 467, 311, 869, 281, 589, 365, 5778, 13, 467, 311, 257, 688, 295, 1019, 13, 14165, 13, 51472], "temperature": 0.0, "avg_logprob": -0.1508304554483165, "compression_ratio": 1.5391705069124424, "no_speech_prob": 0.028462031856179237}], "language": "en"}