start	end	text
0	6080	Hattie Jo, a PhD student at the University of Montreal and Miele, has set out to understand
6080	12000	and explain the performance of modern neural networks, believing it to be a key factor in
12000	17920	building better, more trusted models. Having previously worked as a data scientist at Uber,
17920	24000	a private equity analyst at Radar Capital, and an economic consultant at Cornerstone Research,
24000	28400	she's recently released a paper in collaboration with the Google Brain team titled,
28400	34560	Teaching Algorithmic Reasoning via InContext Learning. Now in this work, Hattie identifies and
34560	41040	examines four key stages for successfully teaching algorithmic reasoning to large language models,
41040	47840	formulating algorithms as skills, teaching multiple skills simultaneously, teaching how to
47840	55120	combine skills, and finally teaching how to use skills as tools. Now through the application
55120	60880	of algorithmic prompting, Hattie has achieved remarkable results with an order of magnitude
60880	67280	error reduction on some tasks compared to the best available baselines. Now this breakthrough
67280	72080	demonstrates algorithmic prompting's viability as an approach for teaching algorithmic reasoning
72080	78480	to large language models and may have implications for other tasks requiring similar reasoning
78480	82960	capabilities. Anyway, I hope you enjoy this conversation that I had with Hattie over at
82960	88240	NeurIPS a couple of weeks ago. Enjoy. Amazing. Well, Hattie, we're here at NeurIPS.
89280	96640	Welcome to MLST. Thank you. Please introduce yourself. I'm Hattie Jo. I'm currently a PhD
96640	104160	student at NILA and a part-time researcher at Google Brain right now as well as a student
104160	111040	researcher. Fantastic. And I've been reading your paper, Teaching Algorithmic Reasoning
111120	117760	via In-Context Learning. Give us the elevator, bitch. The elevator, bitch. So I think
119120	123760	typically in the past people have thought that large language models are not great at
124960	132240	doing symbol manipulation or actually doing reasoning the way humans do. And a common example
132240	138320	people point to for this failure is to show that models can't even do addition properly,
138320	142720	even though it's trained on billions of tokens, billions of parameters.
144480	151600	And in this paper, we try to teach the model how to solve these problems by learning an algorithm
151600	158080	for them and see if it can generalize to a harder problem, generalize out of distribution,
158080	165280	which is a common way to see if the model is actually using the correct algorithm to solve
165280	169840	these tasks. So it's not just like fitting to the training distribution and finding
169840	177200	spirit features, it's actually executing an algorithm. So we do that through some prompting
177200	183120	strategies and show that actually can learn how to do addition contrary to previous belief.
184080	187760	Yeah, I wanted to speak about the previous belief. So with large language models, I'm
187760	192400	vacillated back and forth from being skeptical and being very optimistic. I was originally
192400	197680	very skeptical and I'm becoming more optimistic as time goes on. I interviewed a lady from UC
197680	203920	Irvine called Yasaman Rezegi. And she did a really interesting bit of work actually kind of comparing
203920	210960	the term frequencies as they appeared in the corpus to the arithmetic as a function of the
210960	216480	number of digits. And because OpenAI never released their data set statistics, so it wasn't possible
216480	221520	to do that. And she found that there was like a linear correlation. But I think what's really
221520	230240	interesting now in your work and some of the other work like Chain of Thoughts Prompting and
230240	235840	Scratch Bad Prompting, that now we're in this new regime where we're kind of telling the language
235840	240480	model. It's almost like we're treating the language model a bit like a kind of compiler
240480	246080	and we're giving it the program. And then it's actually extrapolating and it's doing things that
246080	252320	it wasn't directly taught. Right. Yeah. I think that's the first step, right? You want to,
253120	259200	like if you can give the model a program, you want to see that the model can use that program
259200	265840	and apply it to new situations. And so that's kind of the reasoning out of the distribution
265840	272480	component. But I think also ideally you can imagine we want the model to be able to discover
272480	279760	algorithms that we don't know ourselves. And you know, that's a whole other frontier, right?
279760	285840	But this is the first step too on that path, I think. Okay. Well, why don't we start by defining
285840	296240	algorithmic reasoning? Sure. So I think about it as just the way you solve a task is by using a
296240	305040	particular algorithm. And algorithms are input independent, which means that basically for
305040	311200	any input distribution, using that same algorithm will get you the right answer. And so performing
311200	318880	reasoning by running an algorithm is what we refer to as algorithmic reasoning. And of course,
318880	325840	these right now apply to tasks that can be solved by an algorithm. But you could also imagine,
325840	332640	like, cases where you have a soft algorithm where the steps are not so rigidly defined,
332640	338720	but there is an overarching problem, like solving structure that you can follow.
340400	345680	Yeah. Could you refer to that? I mean, maybe I would refer to that as like inductive
345680	351280	algorithmic reasoning. So it's where, so on one side of the spectrum, you actually have the
351280	356880	algorithm and you write every single step explicitly using some kind of code. And now we're talking
356880	363440	about this regime where we are giving examples of an algorithm and we're describing the steps
363440	368720	somewhat vaguely. We're trying to be as clear as possible using language. So where are we kind
368720	375600	of following on that continuum? Well, let's say you take an algorithm of addition.
375840	385040	Yeah. But you actually, so you have an algorithm defined where you like start by taking the
385040	389520	first digit, do something with it, and then get an answer and then move on to the next one.
392000	398320	When I think about a soft algorithm, it can look something like that, except you take the digit
398320	405920	and the something you do with it is not defined explicitly, but it might require some abstract
408320	415120	pattern matching that large language models are particularly good at. So it can use the
415120	421520	same process of breaking down the problem in a very specific way to generalize, but the individual
421600	432560	steps are not actually encoded in code because it's abstract. So if you can do that, then you can
432560	440000	use this approach to tackle tasks where you can't write an algorithm or you can't write a program
440000	446080	for. And that's like very exciting, I think. Can you give me an example of where we can't
446080	452720	explicitly write the algorithm for something? Well, it's hard to come up with a good example
452720	461600	because it's supposed to be abstract in some sense, but these will not be questions of
464480	471200	that you would normally tackle with by writing a program. So it could be a soft reasoning.
476400	482160	I guess maybe even the grade school math word problems, there is no algorithm or a simple
482160	488720	algorithm at least that you can write that will solve math word problems, which are like if you
488720	493680	have four apples, I gave you twice the amount of apples you have. How many do you have now?
495840	504080	But if you define a way to tackle these questions at each individual step, the model can decide
504080	512800	how to apply that flexibly based on the particular question. Yeah, that might be a good thing to
512800	521200	have. But yeah, right now, I don't know. There is no good benchmark for these kind of things.
522240	528640	Yeah. And in your paper, so you came up with a new algorithmic prompting technique and you
528640	533520	designed some experiments and your technique works significantly better than some of the
533600	536640	other in context prompting techniques. Can you sketch that out for us?
537360	544720	Yeah, so the intuition is very simple, actually. When we look at the addition algorithm, the one
544720	552480	that we learned in school, we know that you start with the rightmost digits, you add them up,
552480	559680	you add them up. If it's greater than 10, you have a carry of one, and then you add the carry to
559680	569040	the next sum and so forth. So a scratch pad approach for addition will show the trace from
569040	575440	running this algorithm. So it will show the first sum is this, and the carry is this. But it doesn't
575440	582400	explain how those values are derived. But for us, it feels really natural because we're so familiar
582400	588400	with it. But for a model, trying to infer the rules from seeing a couple examples of it,
589280	595360	that's a very under specified problem. There are many rules that could explain perfectly these
595360	606080	observations. So algorithmic prompting basically explains each step of an algorithm using some
606080	613520	examples. And within each step tries to be as detailed as possible and tries to disambiguate
614320	620240	as much as possible what you want the model to do. And it turns out that when you provide more
620240	626720	details to the model, you're sort of constraining the model's interpretation of disinformation
626720	635280	so that there's only one way to apply this. And with that, you can get more robust behavior from
635360	643840	model and reason very well out of distribution. Yeah, because it's often said that deep learning
643840	649040	models do not reason. And I think what people mean by that is that you get this phenomenon of
649040	655920	shortcut learning and models do the right things for the wrong reasons. And it seems to me that
655920	661920	what we're doing here is by imputing the kind of the structure of how to reason into the prompt,
661920	668000	we're robustifying its behavior out of distribution. Yeah, well said. That's exactly right.
669040	675600	Fascinating. So what do you think of this problem of shortcut learning in general? Because you know,
675600	681520	like Melanie Mitchell said, there are two modes of understanding essentially. We have this anthropomorphic
681520	686880	mode of understanding which is using causality and it's very sample efficient and we have a way of
686880	691040	understanding the world. And we have a bit of an intuition that large language models
691040	697280	don't reason the way that we do. But is it necessarily a problem? And is it cheating in
697280	701840	your view using in-context learning? Or do you think that because we haven't had to train the
701840	711040	model again, what's the problem, right? I mean, is it an issue? Yeah, I mean, I guess there's
711040	721840	many forms of reasoning and algorithmic reasoning is only a subset of that. And I think if the model
721840	731840	can output the reasoning process and show that the answer that arrives at is following the output
731840	738880	of that process, then it's hard to argue that it's not doing reasoning. It might work differently
738880	748000	under the hood, but it follows the similar process. Now, you know, some methods, you output a rationale,
748000	753120	but the final answer is actually different from what the explanation suggests it should be.
754160	760480	And maybe you can point to that and say, oh, the model isn't actually using this. It's just like
760480	765760	giving you something that you asked for, but then the final answer is still using a shortcut.
766240	772720	But with the algorithmic reasoning, we see that that's not the case. And so, yeah, the more you
772720	782640	constrain things, maybe the more you remove shortcuts from the model. But an interesting
782640	788000	question, I think, is you can get this behavior using in-context learning, which I think you,
788800	793920	I suspect you can't really do from fine tuning or some sort of weight training.
794880	800240	I think when you do that, you'll most likely just overfit on the training distribution.
800880	805520	I wanted to ask you, you know, it's a bit of an open-ended question to say what's going on inside
805520	811760	large language models. But what's so exciting to me is that you get all of this emergent strange
811760	815920	behavior. No one would have imagined five years ago that we could do all of this prompt engineering
815920	820880	on a kind of autoregressive language decoder. And the model is trained to do something really
820880	828880	quite trivial, yet as a byproduct, all of this crazy stuff emerges in its internal representation.
828880	834160	And all of this algorithmic reasoning capability seems to be like a side effect of that training.
835520	839200	How does that even happen? Oh, that's a good question.
841920	847600	It's possible that in order to fit that large training data set, the pre-training data set,
848480	857360	you have to find regularities in content, I guess, that humans generate. And
858960	865040	I think some of these abilities come out of those regularities that you learn. So the ability to
866000	877440	refer to the pattern of a previous passage in context and, you know,
878800	883680	see what the relationships are in that pattern and apply the same relationships when you input
885680	892720	that circuit, I think, is just very useful as a way to summarize that large data set. And so
892720	902000	because you're forced to compress all that data into a model set of weights, I think these regularities
903120	911840	emerge somehow. I don't know exactly how. But I think, yeah, I mean, this is an interesting
911840	918320	speculation because then you can say with much larger models where there is no capacity constraint
918320	927920	at all, and you fit all the same data sets, it's not going to learn very interesting behavior
927920	933680	because it's able to just fit the model without capturing the underlying patterns.
935440	941120	And maybe that's why you actually need more training data and training longer rather than
941120	947520	like the optimal scaling is not right now in the model size, the amount of data. Because you want
947520	954640	like the right bottleneck for your representation. And I think maybe that's where these
956960	962400	emergent capabilities are coming from. Interesting. Yeah. And also the amount of training as well as
962400	970160	the amount of data. I wondered what is your intuition on the practical kind of computational
970160	974400	limitations of large language models? So at the moment, they're trained to transform us.
974480	980320	And there have been some pretty cool critiques of connectionism by Fodor and a few other people.
980320	985920	And it's basically along the lines of neural networks can't represent infinite objects,
985920	990720	which kind of distinguishes them from Turing machines. So they can't compute the nth digit of
990720	995520	time is a fairly good example. But the amazing thing is that we're doing all of this stuff with
995520	1000800	algorithmic reasoning. And we haven't found the ceiling yet, it's just getting better and better.
1000800	1007760	And I think it's almost creating this. Well, I mean, I'm becoming quite hopeful,
1007760	1011840	actually, because I don't know where the limit is, but I suspect there is a limit quite soon.
1011840	1014960	How do you think about what the realistic computational limits are?
1016640	1021920	I think the fact that now we have in context learning is interesting because
1022880	1031760	that allows us to have, I guess, adaptive amount of computation. And so if you have,
1031760	1038960	let's say you have infinite context length, then you can sort of maybe do infinite computation
1038960	1046480	in that case. Now, is infinite context length possible? Probably not. But then you can find
1046560	1052080	clever ways to distill that information. You can find clever attention mechanisms.
1052880	1060160	And so I think maybe there's a computational limit, but you can always find new ideas that
1060880	1067600	make existing methods more efficient. And so, yeah, I have no idea when you would hit that
1067600	1073040	limit, but it's probably very far into the future. Amazing. And so, Hattie, we're here in
1073120	1077440	Europe. What have you taken from the conference so far? And what are you most excited about,
1077440	1086320	you know, going forward? Yeah, I don't know, because I haven't checked out the posters
1086960	1098160	very much yet. But I'm excited for the Math AI workshop, which is many other papers exploring
1098160	1107040	this idea of doing math with language models. And yeah, there's some, you know, very impressive
1107040	1114160	work there. So I'm excited for that. I'm excited to meet people and see what they're thinking about.
1115040	1123200	And maybe get some ideas for what to work on next. Yeah, I'm also really interested in the
1123200	1128800	math stuff I spent about an hour speaking with Marcus, is it Marcus Barb, who works under Christian
1128800	1134320	Sagadia, I think he's in your group. Yeah, they're doing some, that's right, they're doing some
1134320	1139920	really interesting stuff with basically doing mathematical conjecturing, you know, like representing
1139920	1144560	mathematical expressions in large language models and being able to generate new ones.
1144560	1148640	It's the kind of thing that you, again, would think was science fiction five years ago and like,
1148640	1154400	remarkably, it works. Exactly. And then by the way, with the mathematical conjecturing, Marcus was
1154400	1159600	saying that, unlike with large language models, it only has to be right one in 100 times because
1159600	1165600	they can formally verify it. So it's almost like the bar is actually lower in that sense.
1166240	1174160	Right. Yeah, I mean, that's where the language models informal reasonability is really useful.
1174880	1180880	Right. Yeah, like the pattern actually is actually useful in a lot of cases.
1180880	1184960	That's really cool. Cool. Well, actually, this has been amazing. Thank you so much for coming
1184960	1189120	on the show and telling us about your research. Oh, thanks for having me. Yeah, it was fun.
1191760	1198320	Looking beautiful. So Marcus, it's so nice to meet you. Can you introduce yourself?
1199040	1204880	Hi, I'm Marcus. I work for Google Research together with Christian Segedy and the
1204880	1210080	Antiformer team. Generally, we're working on trying to solve math,
1212560	1219840	basically by trying to translate natural language mathematics into formal mathematics and
1220720	1223440	in these formal representations of mathematics, we can
1223600	1230480	check proofs and use that as a feedback signal for the understanding of mathematics.
1231120	1235280	And most recently, I've been working on long context models like the memorizing transformer,
1236000	1243760	trying to get these models, like makes models sensitive to the exact definitions and other
1243760	1251360	lemmas that they might use for your improving. And that's an ongoing effort, hopefully more
1251360	1255120	available soon. Can I just say, I'm so jealous that you work with Christian.
1257360	1261120	I mean, folks will remember that we had a conversation with Christian. I think it was
1261120	1266960	about 18 months ago. It is one of my favorite episodes of MLST. So you're a very lucky man indeed.
1266960	1273520	Yes. Yes. It's great to work with Christian. It's a lot of fun. Amazing.
