1
00:00:00,960 --> 00:00:05,360
Welcome back to Machine Learning Street Talk. I'm your host, Tim Scarf.

2
00:00:06,240 --> 00:00:12,880
Now, today on the show, we're joined by Dr. Walid Saber, who's just written a review of the book

3
00:00:12,880 --> 00:00:19,600
Machines Will Never Rule the World, Artificial Intelligence Without Fear, written by Jops Langreb

4
00:00:19,600 --> 00:00:26,000
and Barry Smith. Now, Dr. Saber will be discussing his review, which provides a detailed analysis

5
00:00:26,000 --> 00:00:33,760
of the book's arguments that strong AI is impossible. In his review, Dr. Saber acknowledges

6
00:00:33,760 --> 00:00:39,360
the argument made by Langreb and Smith that anything we engineer is ultimately a system

7
00:00:39,360 --> 00:00:45,120
which can be mathematically modelled and described. He then goes on to discuss the complexity of

8
00:00:45,120 --> 00:00:53,040
modelling mental processes, which the authors argue are dynamic, adaptive, continuously evolving,

9
00:00:53,040 --> 00:00:59,920
and constitutes systems whose behaviour affects and is affected by the environment they function in.

10
00:01:01,920 --> 00:01:07,920
He also touches on the notion of granularity, arguing that complex systems are all the way up

11
00:01:08,480 --> 00:01:16,400
from specific components of the mind to the mind itself and that no known mathematics can model them.

12
00:01:16,880 --> 00:01:22,560
Dr. Saber then delves into the complexities of language and open interactive dialogues,

13
00:01:22,560 --> 00:01:28,160
asserting that language is a prerequisite for any artificial general intelligence,

14
00:01:28,160 --> 00:01:34,720
but that linguistic communication itself is a complex system that no mathematics can model.

15
00:01:35,520 --> 00:01:40,800
He doesn't subscribe to the argument that interactive bots can be built in narrow domains,

16
00:01:41,520 --> 00:01:46,880
since responses and the overall context cannot be predicted in any meaningful way.

17
00:01:47,920 --> 00:01:54,160
Dr. Saber has two reservations as to the conclusions made by Langreb and Smith.

18
00:01:54,160 --> 00:02:00,240
He questions their use of the word never and suggests there could be a new mathematics

19
00:02:00,240 --> 00:02:06,480
that mental processes require that is yet to be discovered. He also doesn't believe that the fact

20
00:02:06,560 --> 00:02:12,560
that complex behaviour cannot be mathematically modelled precludes the possibility of building

21
00:02:12,560 --> 00:02:19,600
such systems, as evidenced by the intentional programming language LISP, and he also considers

22
00:02:19,600 --> 00:02:25,120
the possibility of hypercomputation in validating the church-touring hypothesis.

23
00:02:25,120 --> 00:02:29,840
We'll talk about that a bit later. Finally, Dr. Saber expresses his regret

24
00:02:29,840 --> 00:02:34,800
that the book didn't go into further detail on the frame problem in AI.

25
00:02:36,480 --> 00:02:40,800
He calls for further research into belief revision in complex systems.

26
00:02:42,240 --> 00:02:46,880
Join us today as we speak with Dr. Wallid Saber about his review of this book that we've just

27
00:02:46,880 --> 00:02:52,480
been speaking about. Machines will never rule the world. And by the way, I think very highly

28
00:02:52,480 --> 00:02:58,960
of Wallid. I think he is one of our most loved guests. He is a polymath. He has an incredible

29
00:02:58,960 --> 00:03:04,160
breadth of knowledge across so many fields, you know, from AI and computer science,

30
00:03:04,160 --> 00:03:11,120
to mathematics, to philosophy, to linguistics. He really is a rare breed, and he also brings

31
00:03:11,120 --> 00:03:18,240
a very interesting contrarian view, I would say, to the current kind of modus operandi,

32
00:03:18,240 --> 00:03:22,240
or the zeitgeist in the community at the moment. He's a breath of fresh air.

33
00:03:22,960 --> 00:03:28,720
Anyway, if you haven't already, consider subscribing to our YouTube channel, or indeed

34
00:03:28,720 --> 00:03:33,520
rating our podcast on your favourite podcasting platform if you happen to be listening to us.

35
00:03:34,160 --> 00:03:37,920
Anyway, without any further delay, I give you Dr. Wallid Saber.

36
00:03:44,400 --> 00:03:53,520
Welcome back to MLST, folks. We have the unmistakable Wallid Saber, the legend that is Wallid Saber,

37
00:03:53,520 --> 00:03:58,160
but we also have Mark from our Discord community. Mark, would you like to introduce yourself?

38
00:03:59,040 --> 00:04:06,000
Hi, guys. I'm Mark Aguil, a philosopher, cognitive scientist, and software engineer at

39
00:04:06,000 --> 00:04:12,960
MLST. Awesome. Welcome, Mark. You know, things have been a bit of a blur,

40
00:04:12,960 --> 00:04:18,400
but I think this is right, Tim. I think Wallid was the first, or one of the first,

41
00:04:18,400 --> 00:04:25,200
really big names that we had come on the show, right? I don't think so,

42
00:04:25,200 --> 00:04:31,120
but the most controversial, let's put it this way, the one that made probably,

43
00:04:32,400 --> 00:04:40,480
it was so predictable, what was, I mean, probably the first one that broke that

44
00:04:43,600 --> 00:04:49,200
predictive model. Well, I just remember, I mean, I remember Tim and I being like super,

45
00:04:49,200 --> 00:04:54,000
because we didn't know you, right, before then, and I remember us being like so excited that you

46
00:04:54,000 --> 00:04:58,400
agreed to come on the show. You know, of course, now that we know you, we're kind of like,

47
00:04:58,400 --> 00:05:04,160
ah, whatever, it's just Wallid. We're just through, by the way.

48
00:05:04,160 --> 00:05:08,000
We were following all over ourselves that you were coming on the show, we're like, oh my god,

49
00:05:08,000 --> 00:05:13,760
this is so awesome. To be honest with you, I never thought I would, I would,

50
00:05:15,280 --> 00:05:19,680
yeah, I never thought I would have, my opinion would matter that much, to be honest with you,

51
00:05:19,680 --> 00:05:26,480
and it all started by creating this medium blog, and I started spitting out stuff that,

52
00:05:27,600 --> 00:05:34,080
hey, do you guys know that there's dissonance in that? And I was surprised how much it,

53
00:05:36,400 --> 00:05:45,840
even by people that are living off and making a living, and they preach and write papers on

54
00:05:46,560 --> 00:05:51,600
what I'm attacking, and they would say, don't mention my name, by the way, it's all private

55
00:05:51,600 --> 00:06:00,800
messages. But apparently, I said a couple of things that touched people, but you know.

56
00:06:02,560 --> 00:06:07,840
Yeah. Yeah, I was at New York's last week, and the amount of people that came up to me and said,

57
00:06:07,840 --> 00:06:12,720
I love the show, Tim, Wallid's my favorite guest that you've had on, because Wallid just provides

58
00:06:12,720 --> 00:06:19,360
a completely different perspective, because we're bred on empiricism and neural networks. And part

59
00:06:19,360 --> 00:06:24,080
of the reason I want to get you back on, Wallid, is to counteract some of the, I mean, we've been

60
00:06:24,080 --> 00:06:28,960
speaking to a lot of deep learning people recently, so we need to counteract that a little bit.

61
00:06:29,520 --> 00:06:38,320
Yeah, that one shocked the hell out of me. I mean, I have people like myself at this event. I mean,

62
00:06:38,320 --> 00:06:49,040
if you said ACL, maybe, yeah, okay. But I mean, this is the deep learning meeting, right? I mean,

63
00:06:49,040 --> 00:07:00,240
so I was shocked. But yeah. Yeah, indeed. Although I have some more positive views of DL now.

64
00:07:00,240 --> 00:07:08,480
Oh, go on. Yeah, I mean, look, I breaking news, breaking news.

65
00:07:10,480 --> 00:07:18,160
Positive, although I still have my reservations as to AGI and all that stuff. But I have been

66
00:07:20,160 --> 00:07:27,120
completely impressed with the developments in large language models. I have to admit that

67
00:07:28,000 --> 00:07:33,920
sometimes I say, what the hell is this? Now, technically, let me tell you what's happening.

68
00:07:36,560 --> 00:07:39,840
And I'm working on something that probably would quantify

69
00:07:41,440 --> 00:07:48,080
where can this go? How much can you, how much will scale? The bottom line is this,

70
00:07:48,720 --> 00:07:54,480
these guys have impressed the hell out of me, and they have proven that scale does matter.

71
00:07:54,480 --> 00:08:01,440
I mean, now these large language models, if you take language from lexical to

72
00:08:02,480 --> 00:08:09,200
syntactic to semantic to pragmatic levels, they have definitely mastered syntax.

73
00:08:10,640 --> 00:08:19,200
And this is not a small feat. I mean, this is huge. They have proven that if I read

74
00:08:20,160 --> 00:08:26,400
tons of texts written by humans, I can figure out the grammar of language,

75
00:08:26,960 --> 00:08:35,840
and they have done that. That's huge. Okay, so and I don't like here where I don't like people.

76
00:08:35,840 --> 00:08:40,240
I don't want to mention names, but people that supposedly are in my camp, right,

77
00:08:41,200 --> 00:08:49,840
insisting on refusing to see the elephant in the room. No, large language models have proven

78
00:08:50,400 --> 00:08:58,720
that if I ingest terabytes of text, I will figure out syntactic rules. They have done that.

79
00:08:59,360 --> 00:09:07,840
Now, okay, and here's where technically, and you have to admit, I mean, they,

80
00:09:09,200 --> 00:09:14,560
as a matter of fact, they probably know syntax now more than many college graduates.

81
00:09:16,480 --> 00:09:23,040
Okay, these are from data. That's a huge experiment in cognitive science that

82
00:09:23,360 --> 00:09:28,640
no matter how, how, I mean, you can't be religious in this, you have to be scientific.

83
00:09:29,600 --> 00:09:36,960
I see the proof that these large language models by ingesting tons of text written by humans,

84
00:09:36,960 --> 00:09:43,840
they have figured out the syntax of language. End of story. I have, there's an existential proof.

85
00:09:44,400 --> 00:09:48,640
Go on, open AI, try DaVinci 2 or DaVinci 3 or whatever you like to try.

86
00:09:49,200 --> 00:09:57,440
Their syntactic competency is beyond belief. I'm shocked every time I use it. Okay.

87
00:09:58,720 --> 00:10:04,240
Now, that's not the end of language understanding. There's semantics, and then there's pragmatics.

88
00:10:04,240 --> 00:10:11,520
Wow. I mean, so I'm working on something to quantify. So now we have, I don't know,

89
00:10:11,520 --> 00:10:18,480
we're up to a trillion parameter that allowed me to master syntax. Now, let's see semantics.

90
00:10:18,480 --> 00:10:23,600
And semantic can be broken down to, have you guys figured out reference resolution? Have you

91
00:10:23,600 --> 00:10:29,200
guys figured out scope resolution, prepositional phrase attachments? There are so many

92
00:10:31,520 --> 00:10:39,760
pain points and semantic processing. Can we quantify how many more parameters we need to

93
00:10:39,840 --> 00:10:44,160
conquer semantics? And then pragmatics, there are things like,

94
00:10:49,840 --> 00:10:56,640
the teenager shot a policeman and he immediately fled away. Now, possibly both can,

95
00:10:57,360 --> 00:11:03,440
the he can be the policeman. He fled away to escape further injuries. I mean, that can happen.

96
00:11:04,240 --> 00:11:12,880
But most likely the one that's led away is the teenager. That's pragmatics because we know in

97
00:11:12,880 --> 00:11:20,240
the world we live in, if you shoot someone, they're going to try to capture you and you try to flee,

98
00:11:20,240 --> 00:11:26,880
right? That's not semantics. That's way beyond. How many parameters beyond semantics do you need

99
00:11:26,880 --> 00:11:39,360
to capture that? If you can put a, if you can come up with a rough number, I mean, it could be

100
00:11:39,360 --> 00:11:48,400
a number that's manageable, that's doable by more scaling, which would be an interesting result.

101
00:11:48,400 --> 00:11:55,920
But it could be that it's a number beyond the universe we live in, which means guys,

102
00:11:57,040 --> 00:12:01,920
except that you can master syntax and a little bit of lexical semantics, you can figure out the

103
00:12:01,920 --> 00:12:09,040
meaning of some words, but to do full understanding with pragmatics, we're talking about numbers that

104
00:12:10,000 --> 00:12:17,040
we might have to wait 2000 years. Yes, in theory, it works. So basically, I'm trying to work now on

105
00:12:17,040 --> 00:12:24,640
which is going to be very difficult to quantify because they have proven that scale did improve

106
00:12:24,720 --> 00:12:34,160
syntax, no doubt, not improve it. They've almost mastered syntax. But how far can this go? I mean,

107
00:12:34,160 --> 00:12:40,880
can you quantify how far can this go scientifically without saying, let's try with more, let's try

108
00:12:40,880 --> 00:12:48,000
with more? Which is, which is not, it's not going to be easy to do. Anyway. So I'm curious, could I

109
00:12:48,000 --> 00:12:53,040
push back a little bit on the syntax you said, if they've mastered syntax, and I'm kind of,

110
00:12:53,040 --> 00:12:57,200
okay, I mean, I guess you have an existing proof, and there's like a behavioral kind of

111
00:12:57,920 --> 00:13:02,800
proof that it does seem to have very syntactic sentences. And obviously, if you're in something

112
00:13:02,800 --> 00:13:08,160
with whatever billion of parameters, but would you say what those rules are? Could we write them

113
00:13:08,160 --> 00:13:13,280
down? Probably not, right? Because it's a billion different numbers of weights. No, no. And you

114
00:13:13,280 --> 00:13:17,680
don't have the old school kind of generative grammar approach. And also, it's not the way humans

115
00:13:17,680 --> 00:13:21,440
have learned language. And could you reverse engineer or tweak it? We don't know what it is,

116
00:13:21,440 --> 00:13:29,680
it's a black box. So, okay, there's a behavioral tense in which it knows. But isn't it the way

117
00:13:29,680 --> 00:13:36,240
humans do learn language? I mean, I think it's more, though, it's more related to the way humans

118
00:13:36,240 --> 00:13:44,960
learn language than, I mean, I was 20 until I knew grammar. I mean, we use language without,

119
00:13:44,960 --> 00:13:50,640
without knowing grammatical rules. So there is an argument that humans don't learn grammar,

120
00:13:50,640 --> 00:13:55,040
that it's, you know, pretty native. And all we do is tweak a few parameters. And then we add

121
00:13:55,040 --> 00:14:00,400
vocabulary over the years, tweaks or whatever. That's the, you know, the Chomsky and generative

122
00:14:00,400 --> 00:14:08,160
grammar approach. So I don't think that we have a billion parameters that we tweak over 20 years.

123
00:14:08,160 --> 00:14:13,120
I don't think that's how we work. And we have, I mean, there's amazing competency of children

124
00:14:13,120 --> 00:14:17,200
at two years of age, you know, with language they have, you've said, you've, you're writing,

125
00:14:17,200 --> 00:14:22,080
you pointed this out, actually, children know, is absolutely amazing, you know, straight out of,

126
00:14:22,080 --> 00:14:28,880
you know, it is amazing, but it is amazing. And I, and I didn't change the way I think about

127
00:14:29,680 --> 00:14:36,720
and we have innate stuff. But here's the change that these guys have made me

128
00:14:39,440 --> 00:14:45,200
go through. It's a minor change, but it's, it's, it's not that subtle, actually. Here's the thing.

129
00:14:45,200 --> 00:14:53,200
I was told before these new results are coming out that look, we do have innate stuff, which

130
00:14:54,160 --> 00:15:00,720
took us three, four hundred thousand years of evolution. All we're doing by ingesting all

131
00:15:00,720 --> 00:15:11,360
this text is we're simulating, right, these 300,000 years. So give us a chance to simulate

132
00:15:11,360 --> 00:15:21,040
this innateness if you want, in a way, in a way. Okay, I, that argument was said long time ago,

133
00:15:21,040 --> 00:15:28,560
and I thought, come on, you're chasing infinity. What happened with the real difference in my

134
00:15:28,560 --> 00:15:36,480
mind now is they have proven that they conquered one beast in language. Nobody can dispute that.

135
00:15:37,120 --> 00:15:44,240
Can I have a go at disputing it? So, in the Polition and Foda connectionism critique,

136
00:15:44,240 --> 00:15:48,720
they spoke about productivity, you know, the infinite cardinality of language. There was

137
00:15:48,720 --> 00:15:54,160
recently a deep mind paper talking about the Chomsky hierarchy and deep neural network. Well,

138
00:15:54,160 --> 00:15:58,880
I mean, RNNs are regular languages, but, you know, I think transformers and the rest of them are

139
00:15:58,880 --> 00:16:06,080
at the bottom of the hierarchy. So quantitatively, we know we haven't conquered infinity. So why

140
00:16:06,080 --> 00:16:13,360
with such a shallow horizon, are they doing so well? I agree. Here's the thing, language use,

141
00:16:14,560 --> 00:16:23,680
languages infinite, but probably the long tail of probably 90% of ordinary language use,

142
00:16:24,400 --> 00:16:32,560
right, can be figured out from the stuff that we write. So they will never capture all of language.

143
00:16:33,520 --> 00:16:42,800
Yes, but they might reach the level of a competent educated man like us in language competency.

144
00:16:44,080 --> 00:16:50,240
So, all I'm saying is what they have achieved is a huge

145
00:16:51,200 --> 00:17:04,320
result in terms of the big question of scale and big data. They have definitely proved that

146
00:17:04,960 --> 00:17:09,520
if I see enough data, I will learn something and something that's not trivial.

147
00:17:11,200 --> 00:17:15,040
Look, you know where I'm coming from. You're talking Foda and Polition. I mean,

148
00:17:15,040 --> 00:17:20,640
you're preaching to the choir, right? But I have to be a scientist too. I mean, I don't like,

149
00:17:20,640 --> 00:17:26,800
I'm following Gary Marcus, and he's like, I don't like people that minimize what happened.

150
00:17:28,240 --> 00:17:35,120
I'm a scientist, right? I see a big result. I say, wow, right? And look, we're talking, nobody

151
00:17:35,120 --> 00:17:38,720
bashed deep learning more than me, especially large language models. I mean, I'm like,

152
00:17:39,360 --> 00:17:49,280
I was saying this is silly, right? But I have to say they have proven something to me at least,

153
00:17:50,400 --> 00:17:56,240
which is huge because I know how difficult language is. I am impressed equally.

154
00:17:57,520 --> 00:18:01,200
Wouldn't you say it's an engineering, an engineering triumph rather than a scientific?

155
00:18:02,160 --> 00:18:07,040
It's an engineering triumph. But here's the point, Mark. I think it's a little bit more.

156
00:18:07,840 --> 00:18:12,800
That's the only thing I'm trying to, I'm not saying, look, I didn't give up on, I can get to

157
00:18:12,800 --> 00:18:19,520
the criticism later. So don't put me on in that camp yet, right? Or, or ever, right? Because I know,

158
00:18:19,520 --> 00:18:25,840
I know theoretically, theoretically, mathematically, you cannot understand language this way.

159
00:18:25,840 --> 00:18:33,200
All I'm saying is, in terms of cognitive science, what happened and what is happening as we speak

160
00:18:33,760 --> 00:18:44,160
is not nothing. It's a huge, for example, if I can ingest a lot, again, what they prove is,

161
00:18:44,160 --> 00:18:52,480
is that well, the two are related. So it's one thing scale from tons and tons of data. I can learn

162
00:18:52,480 --> 00:19:00,720
something that is not trivial. That to me has been proven. The point I'm making is not the

163
00:19:00,800 --> 00:19:04,240
point I'm making is not that they solve the language problem. Sorry.

164
00:19:04,240 --> 00:19:08,720
Yeah, I want to jump in here a little bit. Because from my perspective, I think

165
00:19:09,600 --> 00:19:16,240
part of why you're saying it's huge is because I think it was a huge step for you personally.

166
00:19:16,240 --> 00:19:20,160
Because I know, you know, from the past, like talking to you, like you've had a much more

167
00:19:20,160 --> 00:19:26,720
extreme view, you know, on the capabilities of large language models than, for example, myself.

168
00:19:26,720 --> 00:19:32,240
Because for me, I don't see anything new here. It's kind of like, I'll give you an example

169
00:19:32,240 --> 00:19:38,560
outside of syntax just for a moment. So just transcription, because that's what Tim and I

170
00:19:38,560 --> 00:19:44,400
happen to be working on quite a bit right now. In other words, transcribing audio into text.

171
00:19:46,480 --> 00:19:51,520
All the state-of-the-art models are pretty much sitting around each other at about 90%

172
00:19:52,160 --> 00:19:58,560
you know, accuracy right of transcription. But here's the thing is that's for people speaking

173
00:19:59,520 --> 00:20:06,480
relatively common languages with a relatively standard accent. Okay, as soon as you bring

174
00:20:06,480 --> 00:20:13,520
someone in the room that has an accent or speaks a, you know, with maybe like some type of a

175
00:20:13,520 --> 00:20:18,640
challenge, like a speech challenge, or this side of the other thing, it becomes garbage again.

176
00:20:18,720 --> 00:20:26,000
And like for Tim and I, or there's noise in the background, music playing in the

177
00:20:26,000 --> 00:20:30,800
background. And as Tim and I have probably hammered, you know, to death and beaten a

178
00:20:30,800 --> 00:20:36,320
dead horse on our channel like so many times, we've never doubted that machine learning can learn

179
00:20:36,320 --> 00:20:42,480
like the bulk of the curve, where it really, really struggles is in all these edge cases,

180
00:20:42,560 --> 00:20:49,760
and the corner cases, and the periphery where it can easily, it's very brittle, right, in those

181
00:20:49,760 --> 00:20:53,840
kind of areas. Like this is the point we've been making out for a long, long time. And so the fact

182
00:20:53,840 --> 00:21:00,480
that like massive trillions of parameters and terabytes of data was able to learn 90% or more,

183
00:21:00,480 --> 00:21:07,680
95% or whatever is syntax. Okay, I get it. Like from a linguist perspective, that was a, you know,

184
00:21:07,680 --> 00:21:12,320
maybe a big triumph or something. But I'm still always about that other like 5%.

185
00:21:13,200 --> 00:21:17,440
And the problem with the approach of deep neural networks is to get that other 5%

186
00:21:18,480 --> 00:21:25,200
is like 100 times as many more parameters, whereas like using, whereas using more generalized,

187
00:21:25,200 --> 00:21:29,280
abstracted, you know, methods that we haven't yet really discovered.

188
00:21:29,280 --> 00:21:33,280
You're hitting it on the nail. And that's why I'm working out on quantifying this because

189
00:21:34,160 --> 00:21:42,320
now we are doing exponential growth in the number of parameters for not even linear growth in the

190
00:21:42,320 --> 00:21:53,520
accuracy, even logarithmic, I agree with you 100%. So that other 10% might require 2000 years of data

191
00:21:53,520 --> 00:21:58,160
that we don't even have. That's what I'm working on. How far can this go because the function

192
00:21:59,040 --> 00:22:05,360
is against them now? Like, I mean, we're increasing GPU power and the number of data

193
00:22:05,360 --> 00:22:12,960
that we're ingesting exponentially for a minute increase in accuracy, which is,

194
00:22:12,960 --> 00:22:17,600
right, that's the end of the logarithmic. And this is, this is part of the announcers that we

195
00:22:17,600 --> 00:22:26,320
have to go through. So look, all my reservations that I had before apply. So I'm being misunderstood.

196
00:22:26,320 --> 00:22:33,360
All I'm saying is simple. These guys, what they have done is not as trivial as I thought initially.

197
00:22:34,080 --> 00:22:40,400
Okay, so let me, let me really be very careful in what I'm saying because now I have a following.

198
00:22:40,400 --> 00:22:48,960
I don't want to lose it. No, I'm not, I'm not changing scientifically where I was. I mean,

199
00:22:48,960 --> 00:22:53,680
science is science. And I know theoretically, I don't get into things like intentionality and

200
00:22:54,400 --> 00:22:58,640
these models understand nothing about the word. I'm talking about syntax only, by the way,

201
00:22:59,440 --> 00:23:06,160
syntax on and some coherence when they patch things together. The coherence is amazing.

202
00:23:06,960 --> 00:23:14,080
They're not patching things together that don't relate at all. So I'm talking about syntax and,

203
00:23:14,080 --> 00:23:19,040
and coherence and syntax. Okay. And a touch of semantics, right?

204
00:23:19,440 --> 00:23:24,240
My point, let me repeat it so that I'm not misunderstood.

205
00:23:26,240 --> 00:23:34,640
They have proven something that many cognitive scientists would never accept, never ever.

206
00:23:36,800 --> 00:23:45,360
But this existential proof has told many cognitive scientists, don't dismiss learning

207
00:23:45,360 --> 00:23:50,720
from data only, blind, no labeling, some aspects of language,

208
00:23:52,720 --> 00:23:57,760
actually very impressive aspects of language. These guys have proven that.

209
00:23:58,960 --> 00:24:06,400
And me as a cognitive scientist, I have to admit because I see it. I see from data alone,

210
00:24:07,040 --> 00:24:10,720
these systems have learned non-trivial aspects of language.

211
00:24:11,600 --> 00:24:17,920
Now, how do you interpret that? Where do you take it? What do you conclude from it?

212
00:24:17,920 --> 00:24:25,600
We can, we can debate that. But all I'm saying is, I have seen something that I never thought I would

213
00:24:25,600 --> 00:24:35,200
see, that just ingesting text in these deep networks, you can actually figure something

214
00:24:36,160 --> 00:24:44,240
not trivial about language. That has been done. I mean, you can, you can say there are pigs,

215
00:24:44,240 --> 00:24:51,360
pigs that fly. Okay. Prove me wrong. I saw them prove they don't exist. Well, I can.

216
00:24:52,080 --> 00:25:01,760
But existential proofs are the most powerful proofs. It's an existential proof, proof by doing.

217
00:25:01,840 --> 00:25:06,400
I'm showing you language competency by ingesting text on it.

218
00:25:09,120 --> 00:25:11,280
So this dismissive

219
00:25:17,360 --> 00:25:27,920
all these are, what is the phrase that Melanie uses? Not Melanie. Stochastic parents. No.

220
00:25:27,920 --> 00:25:29,440
Oh, Bender. Emily Bender.

221
00:25:29,440 --> 00:25:35,600
Emily Bender. No, these are not stochastic parents anymore for me. I am seeing,

222
00:25:36,160 --> 00:25:42,720
look, if I go through the tests on conducting, I have 20 pages of tests on every aspect.

223
00:25:45,280 --> 00:25:48,720
And they get better. I mean, I am seeing things that

224
00:25:50,880 --> 00:25:53,760
lexical ambiguity, they've almost resolved it like,

225
00:25:54,640 --> 00:26:04,160
we were at the baseball stadium last night, we had a ball. They knew that ball is not the baseball.

226
00:26:05,520 --> 00:26:12,240
I'm seeing things like, what the hell is this? And if anybody can test these systems,

227
00:26:12,240 --> 00:26:18,320
I can with all humility. I'm trying my best now to make them fail, which was not the case just

228
00:26:18,400 --> 00:26:24,480
a month ago. All I'm saying is I'm seeing something that I never thought I would see

229
00:26:25,840 --> 00:26:28,560
as a cognitive scientist, as a computation linguist.

230
00:26:30,720 --> 00:26:37,440
Let me put it this way. To see this capability now, you have to bring back Montague,

231
00:26:38,240 --> 00:26:47,840
Frigge, Marvin Minsky, John McCarthy, all the pioneers of logic and AI, put them together

232
00:26:48,480 --> 00:26:53,680
and give them a thousand bright engineers. And they will not do this.

233
00:26:54,880 --> 00:26:59,520
In a minute, we're going to get onto your book review, but you are just alluding to the problem

234
00:26:59,520 --> 00:27:05,040
of semantics and pragmatics. And also I want to bring in symbol grounding as being the next

235
00:27:05,040 --> 00:27:08,640
potential brick walls. Could you just talk to that a little bit more?

236
00:27:09,360 --> 00:27:18,400
Well, look, symbol grounding was an issue in symbolic systems. You're using symbolic systems.

237
00:27:18,400 --> 00:27:28,480
So you're saying cat, CAT, it's reference based semantics. So I'm going to use CAT to refer to

238
00:27:28,480 --> 00:27:36,880
a concept called CAT. And then the concept called CAT is a frame in most systems, in frame based

239
00:27:36,880 --> 00:27:43,440
systems with properties. It's a mammal. It's a thing that has this and this kind of fair whiskers,

240
00:27:43,440 --> 00:27:50,480
blah, blah, blah. It's the intention of what a CAT is. And then symbol grounding came like, okay,

241
00:27:50,480 --> 00:27:57,600
you're defining CAT as symbol in terms of symbols. Like, so where do we go? It's like a

242
00:27:57,600 --> 00:28:04,240
dictionary to read the definition of a word. I have to know all the words. So I might go and

243
00:28:04,240 --> 00:28:10,560
so it's a cyclical representational system. It's not grounded in anything in the end.

244
00:28:10,560 --> 00:28:15,520
It's a closed. Basically, it's a system that defines itself, like what the hell's going on here,

245
00:28:15,520 --> 00:28:22,400
right? Symbol grounding was CAT has to be associated with something real outside.

246
00:28:22,400 --> 00:28:25,920
That's a real CAT. In symbolic systems, we don't have that, right?

247
00:28:28,720 --> 00:28:35,120
We can get into symbol grounding. It's a huge subject on its own, like where do meanings,

248
00:28:36,320 --> 00:28:44,160
where do words get their meaning from? Is it embodied? Is it experiential? Does it have to be

249
00:28:45,040 --> 00:28:54,480
can a deaf and a blind person ever understand the meaning of something? So that's a huge...

250
00:28:57,520 --> 00:29:02,640
I mean, we spoke to Andrew Lampinen and he was getting into Pierce's triad semiotics,

251
00:29:02,640 --> 00:29:08,240
this embodied relativistic notion of grounding, which actually I'm developing a bit of a taste

252
00:29:08,240 --> 00:29:12,640
for personally, but you're very skeptical about that. Could you just sketch that out?

253
00:29:12,800 --> 00:29:18,080
I don't think that's the issue grounding. I mean, people make a lot of it and like our

254
00:29:18,080 --> 00:29:24,880
common friend, Bishop, Mark Bishop, that you will never understand the meaning of something if you

255
00:29:24,880 --> 00:29:30,960
don't live in the environment and it's... That has never been... I don't believe so. That's why we

256
00:29:30,960 --> 00:29:35,600
call it artificial intelligence, right? I mean, we're never going to have the intelligence of a

257
00:29:35,600 --> 00:29:42,000
human being. We're never going to have a robot that really chokes when they see their nephew

258
00:29:42,000 --> 00:29:47,040
after six years, right? I mean... And that's... That was never the one. That's why we're building

259
00:29:47,040 --> 00:29:53,520
artificial intelligence, not human intelligence. So this whole argument about grounding and

260
00:29:53,520 --> 00:29:59,440
embodiment and I will never understand what pain is because a robot will never really feel pain.

261
00:29:59,440 --> 00:30:04,320
That to me, that's besides the point. I'm not building artificial life. I'm building an

262
00:30:04,320 --> 00:30:10,160
artificially intelligent machine that will do things in a way that you would say, what the hell

263
00:30:10,160 --> 00:30:17,200
was that? Probably that's how AR should be defined. That's it. What the... Who did this, right? That's it.

264
00:30:17,840 --> 00:30:24,080
It feels pain or it doesn't feel pain or it will never know what crying is, like so.

265
00:30:26,320 --> 00:30:35,760
So at least I come from this angle. I'm not into building artificial humans. I'm an engineer.

266
00:30:35,840 --> 00:30:42,960
I'm into building artificial intelligence systems. Systems that can reason, right? In the environment

267
00:30:42,960 --> 00:30:49,040
we live in, solve problems intelligently and problems that usually require human intelligence.

268
00:30:49,040 --> 00:30:54,720
Like I'm into... I would like to see a world where we don't have accountants. Come on. We don't have

269
00:30:54,720 --> 00:31:02,560
doctors. I open my mobile. I have a doctor. I converse with them intelligently and they tell

270
00:31:02,560 --> 00:31:07,920
me exactly what to do. Done. Nobody goes to medical school anymore. Everybody should write

271
00:31:07,920 --> 00:31:14,400
poetry and play music and enjoy the beach. That's it. That's the AI I'm interested in. We will never

272
00:31:14,400 --> 00:31:23,680
build robots that will understand love. So to me, these are arguments that

273
00:31:25,520 --> 00:31:30,160
they're irrelevant. We're building artificial intelligence. When we did calculators, we never

274
00:31:30,160 --> 00:31:37,040
gave a damn how we do it in the mind. And we have calculators that can beat any mathematician

275
00:31:37,040 --> 00:31:45,440
in doing a division of two prime numbers, each of which is 20 digits. Yeah, I think it's more

276
00:31:45,440 --> 00:31:51,520
where your area of interest is. I mean, so yours is in the engineering. And what's coming to mind

277
00:31:51,520 --> 00:31:58,640
right now is our conversation with Professor Chomsky where he said, yeah, these are great

278
00:31:58,640 --> 00:32:03,360
feats of engineering. I mean, I like bulldozers too. They just don't have anything to do with

279
00:32:03,360 --> 00:32:10,720
science. Sure, or philosophy for that matter. I mean, I think some of these questions,

280
00:32:11,520 --> 00:32:16,080
yeah, maybe they don't have a lot to do with building AIs that do a bunch of useful things.

281
00:32:17,200 --> 00:32:24,080
But they have a lot to do with philosophy or science or whatever. Mathematics for that matter.

282
00:32:24,080 --> 00:32:32,640
This is a bridge to the book because the authors were misunderstood from their title,

283
00:32:32,640 --> 00:32:38,160
and I told them that privately. No, not privately. I want to tell them that because

284
00:32:39,360 --> 00:32:45,120
I got comments from people privately that the title is misleading. The title assumes they are

285
00:32:45,120 --> 00:32:50,880
anti AI, not really. They're saying roughly what I'm saying. I'm not interested in building an

286
00:32:50,880 --> 00:32:57,840
artificial human. We can never do that problem. Right. So all this, and this is important because

287
00:32:57,840 --> 00:33:06,480
people are trivializing. I mean, you have people talking about AGI from five years ago. And all

288
00:33:06,480 --> 00:33:12,880
we had was something that can do amazing pattern recognition. That's it. So it's important for

289
00:33:12,880 --> 00:33:20,480
us to say, Hey guys, cool it down. Do you know what you mean when you talk about machines that

290
00:33:21,520 --> 00:33:29,200
surpass human intelligence? This is not just a word you throw out, because you're impressed with

291
00:33:30,240 --> 00:33:36,960
a system that can recognize cats from dogs. Come on, take it easy. Slow down, right? And this book

292
00:33:36,960 --> 00:33:43,440
is about that. It's like, do you know what it means to have a system that can feel and

293
00:33:46,160 --> 00:33:52,720
react instantly real time to changing situations around them? And do you know what you're talking

294
00:33:52,720 --> 00:34:00,320
about? So yeah, I'm interested in the engineering side of AI. And that's what makes me impressed

295
00:34:00,880 --> 00:34:13,040
by something like a DaVinci 2 or DaVinci 3 as an AI enthusiast. I look at this and I say,

296
00:34:13,040 --> 00:34:19,680
wow, we've never been able to reach this milestone. This is a huge milestone. That's how I look at

297
00:34:21,280 --> 00:34:26,000
will it be, will it be the solution for the language understanding problem? No,

298
00:34:26,960 --> 00:34:32,000
because language understanding in the full sense of the word understanding,

299
00:34:33,040 --> 00:34:38,080
the way we speak now, the way we were speaking now involves a lot more than mastering syntax,

300
00:34:38,080 --> 00:34:49,360
but they did master a big aspect of language. And I can see it. I can try it. And I'm trying to

301
00:34:49,360 --> 00:34:57,360
make it fail now in syntax and even some coherence, some mild, let's call them mild semantics.

302
00:34:59,600 --> 00:35:10,560
And it's very impressive. So the question now becomes for AI researchers, not just engineers, is

303
00:35:10,880 --> 00:35:22,000
this scalability scalable? Is this scalable? Is this approach scalable?

304
00:35:25,120 --> 00:35:32,880
So much data and so much compute power, they mastered syntax more or less. I think they did

305
00:35:32,880 --> 00:35:40,320
at least as much as a competent language user. So my first question is like,

306
00:35:40,320 --> 00:35:45,120
do you see natural language as essentially computable as in like cheering machine computer?

307
00:35:45,120 --> 00:35:48,720
Or do we need some other kind of new mathematics to describe it? For example,

308
00:35:48,720 --> 00:35:53,040
hypercoputation, whatever that might be. In other words, is there a generative grammar or

309
00:35:53,040 --> 00:35:56,240
algorithm or set of rules that generate our valid sentences?

310
00:35:56,240 --> 00:36:03,600
When it comes to language itself, I think language is a formal language.

311
00:36:04,720 --> 00:36:13,520
There is a compiler for natural language that can be built, like we have built one for Java,

312
00:36:13,520 --> 00:36:21,680
C sharp. So this is Montague, yeah. Yeah, I believe Montague was right,

313
00:36:21,680 --> 00:36:28,800
although Montague was was attacking the semantics part. Okay, he touched a little bit on intention

314
00:36:28,800 --> 00:36:36,960
and then not much on pragmatics, but Montague and it took me years. And I had to be advised by a very

315
00:36:36,960 --> 00:36:42,960
smart philosopher, logician, that stop saying Montague didn't deal with this. Montague was never

316
00:36:42,960 --> 00:36:48,800
in the business of reference resolution. That's pragmatics. Montague was trying to prove

317
00:36:49,760 --> 00:36:55,920
there is a formal system and algebraic system. And he used lambda calculus, strongly typed system

318
00:36:56,480 --> 00:37:03,520
that I can use to compose language like I do with arithmetic or calculus or anything. There's

319
00:37:03,520 --> 00:37:09,920
a logic that are mathematics for language, which is a huge thing. Montague was not a trivial

320
00:37:11,520 --> 00:37:14,160
semanticist in the history of language. He was huge.

321
00:37:14,880 --> 00:37:19,680
So would you say that Montague is doing for language semantics of language,

322
00:37:19,680 --> 00:37:25,760
what chance he did for syntax of language? Exactly, exactly. And the common denominator

323
00:37:25,760 --> 00:37:30,720
interesting between them is someone that Chomsky himself admires a lot Barbara Partee,

324
00:37:31,440 --> 00:37:37,360
who was he did her PhD with Montague. She's a Montegoian, Montague semantics.

325
00:37:38,160 --> 00:37:44,240
But she and she, she did say almost the same phrase. He said what Montague did for semantics

326
00:37:44,240 --> 00:37:49,760
is equivalent to what Chomsky did for syntax. Yes. Okay, exact, almost exact phrase.

327
00:37:51,520 --> 00:37:56,720
You think he was right about semantics? Yes. So for example, there's a computable definition

328
00:37:56,720 --> 00:38:02,480
of what is a pile of sand. Right. No, no, no, no, no, no, no. I'm not sure you're right about that.

329
00:38:02,480 --> 00:38:08,640
No, no, no, no. Hold on. Let's not get Montague was not a psychologist or an ontologist or

330
00:38:09,440 --> 00:38:16,560
he said whatever your meaning for something is. Okay. He didn't even care. Montague never

331
00:38:16,560 --> 00:38:22,080
did ontology and conceptual and like, what, how do you define the meaning of what is a book?

332
00:38:23,680 --> 00:38:29,040
Look, let's, and took me, I'm telling you, took me three years to appreciate what Montague was

333
00:38:29,040 --> 00:38:34,400
doing. And I, and my thesis was on Montague semantics, the masters before the PhD. Here's

334
00:38:34,400 --> 00:38:40,880
what Montague did, Keith, and you'll appreciate Montague said, whatever your meaning for the

335
00:38:40,880 --> 00:38:50,000
individual words, the lexical meaning. So cat means see, okay, you go with your psychologist and

336
00:38:50,720 --> 00:38:54,560
cognitive scientist and ontologist and disagree about the meaning of a cat.

337
00:38:55,520 --> 00:38:58,880
Finally, you come to me and you say, we have a meaning for cat and it's see.

338
00:39:00,720 --> 00:39:06,080
Follow me. Montague never cared about what is the nature of things outside. Right.

339
00:39:06,880 --> 00:39:14,320
Whatever your meaning for these individual concepts are. Right. Here's how you make,

340
00:39:14,320 --> 00:39:21,840
you get the meaning of a whole mathematically. I'll give you a simple example that will make

341
00:39:21,920 --> 00:39:31,360
you appreciate what I'm talking about. John refers to a person. Right. The neighbor next door

342
00:39:32,560 --> 00:39:38,720
refers to a person. The neighbor next door that just moved from California refers to a person.

343
00:39:40,320 --> 00:39:48,000
The neighbor next door that knows John very well and drives for the LTD is a person.

344
00:39:48,800 --> 00:39:56,080
All of how can you have this phrase and John refer to a person and have the same semantic type

345
00:39:56,640 --> 00:40:03,360
composition in a way that never fails. Like you do in arithmetic, he wanted to prove that

346
00:40:03,360 --> 00:40:10,640
natural language is a formal language. He developed a semantic algebra that makes this long phrase

347
00:40:10,640 --> 00:40:16,000
referred to the in the end to an object that has the same semantic type as John mathematically.

348
00:40:16,000 --> 00:40:26,320
If you do it, it never fails. The details of this were genius. Okay. So Montague then made the big

349
00:40:26,320 --> 00:40:32,880
claim natural language is a formal language. Give me some time. I'll work out the full algebra.

350
00:40:34,080 --> 00:40:39,040
You go then and decide what the individual meanings are. I don't care. Montague never gave a damn

351
00:40:39,040 --> 00:40:45,040
about cognitive science and knowledge and he was a logician. He wanted to prove

352
00:40:45,680 --> 00:40:51,440
there's a calculus underneath natural language. Calculus of meanings. You decide on the meaning.

353
00:40:52,000 --> 00:40:56,160
I'm telling you, it took me a while. I thought he's doing semantics. What is the meaning of this

354
00:40:56,160 --> 00:41:02,720
and Montague? He said he never cared. He was doing an algebra of meanings, regardless of what the

355
00:41:02,720 --> 00:41:17,440
meanings are. Okay. But his project was huge. Montague was trying to prove there's an algebraic

356
00:41:17,440 --> 00:41:23,440
system behind language, like any other formal language. Like you can get an arithmetic expression

357
00:41:23,440 --> 00:41:28,880
and build a tree for that, evaluate it, and get the final meaning. Natural language works the same

358
00:41:28,880 --> 00:41:36,720
way. Except it's not that simple. That's all. So his project was huge and he was misunderstood.

359
00:41:38,480 --> 00:41:44,320
So he was really doing semantics. That's semantics. Pragmatics is a different thing.

360
00:41:45,440 --> 00:41:51,840
What do you think is the core unique property of natural human language? Would you agree with

361
00:41:51,840 --> 00:42:01,440
Chomsky on it being digital infinity? Yeah. Okay. The infinite thing in the productivity.

362
00:42:04,000 --> 00:42:13,360
To me, no, it's I'm half Chomsky and half something else. To me, no, the real,

363
00:42:13,360 --> 00:42:20,000
real unique thing about language. And that's why even if Montague succeeded, that's half the battle.

364
00:42:20,960 --> 00:42:29,280
It's not in the semantics, although that's huge. To me, it's the pragmatic side, the abductive

365
00:42:29,280 --> 00:42:35,920
inference. I mean, we use induction and we use deduction and we always ignore abduction.

366
00:42:35,920 --> 00:42:44,560
Abduction is the unique, is the humanly unique reasoning capability. I mean, rats do inductive

367
00:42:44,640 --> 00:42:50,880
reasoning. They, to a certain extent, that's how they learn a few things, inductively, really.

368
00:42:52,720 --> 00:42:58,640
All the lower species do inductive reasoning to a certain extent. And some of them do some

369
00:42:58,640 --> 00:43:04,400
deductive reasoning, if there's then this, but at a very shallow level, of course. Abductive

370
00:43:04,400 --> 00:43:12,080
reasoning is uniquely human. And that's the part of language understanding, which means

371
00:43:12,080 --> 00:43:19,120
reasoning to the best explanation. Abductive reasoning is I reach a conclusion, not inductively

372
00:43:20,000 --> 00:43:28,080
by induction or, and not deductively, I deduced it. But I reached this conclusion because it's

373
00:43:28,080 --> 00:43:37,520
possible, it can happen. And it is the best conclusion I can come up with, given everything

374
00:43:37,520 --> 00:43:44,560
else I know. Abductive reasoning is the real reasoning methodology that makes us unique as

375
00:43:44,560 --> 00:43:51,600
human. We reason to the best, we reason, it's called reasoning to the best explanation. Right? So

376
00:43:52,240 --> 00:44:00,400
that's, that's, Pierce and others, I mean, Pierce was the pioneer of abductive reasoning or

377
00:44:00,400 --> 00:44:08,240
abduction. But I'm talking about an abduction has come to have two sort of tracks. And there's

378
00:44:08,240 --> 00:44:14,560
abductive reasoning in the traditional philosophical charts, Pierce. But there's abductive reasoning

379
00:44:14,560 --> 00:44:22,800
as it used to be called in the 80s, when case-based reasoning came out and expert systems who,

380
00:44:23,760 --> 00:44:30,560
there was something called EBL, explanation-based learning. And it was even a learning technique,

381
00:44:31,120 --> 00:44:39,360
which is really reasoning to the best explanation. Basically, I have to make a decision. Actually,

382
00:44:39,360 --> 00:44:45,680
Jerry Hobbs, you guys heard me mention his name several times before, who's, I think, huge in

383
00:44:45,680 --> 00:44:54,720
semantics, has a paper when he was at SRI with other luminaries too. The title is interpretation

384
00:44:54,720 --> 00:45:00,960
as abduction or understanding as abduction. And basically, he shows how all the difficult,

385
00:45:02,240 --> 00:45:06,720
all the challenges in language understanding beyond semantics. So we're done with Montague.

386
00:45:06,720 --> 00:45:13,120
Now I'm doing the final understanding of what makes sense given, because every expression has

387
00:45:13,120 --> 00:45:19,600
several meanings. Even if I did the semantics perfectly, I have to choose the most plausible

388
00:45:19,600 --> 00:45:25,280
meaning from all the possible meanings. That's pragmatics. And the way you do that very well

389
00:45:25,280 --> 00:45:33,040
is in language. We do abductive reasoning. We say, I'm left with three meanings, three possible

390
00:45:33,040 --> 00:45:40,960
meanings, syntax excluded, 200 syntax trees, semantics excluded, few invalid semantic expressions.

391
00:45:40,960 --> 00:45:46,160
And I'm left with three still, three possible meanings. They can all happen in the world we

392
00:45:46,160 --> 00:45:53,280
live in. Which one is the most plausible? We do this abductively. Which meaning is the most

393
00:45:53,280 --> 00:45:59,200
likely meaning given the context and what I know? That's the last challenge in language.

394
00:45:59,920 --> 00:46:05,520
So we need to, we need to add the abductive model, which we humans do. I go back to the

395
00:46:05,520 --> 00:46:11,280
teenager shot of policemen, both meanings, both interpretation can happen, right?

396
00:46:12,640 --> 00:46:20,320
Either one can flee, right? But most likely it's the teenager that fled away, given what I know

397
00:46:20,320 --> 00:46:24,800
and given that's abductive reasoning. But semantically both can happen.

398
00:46:25,760 --> 00:46:31,280
Yes, I do want to emphasize something that Wally like briefly mentioned, but I think it's very

399
00:46:31,360 --> 00:46:38,000
important to mention is that there's two senses of abduction. And they differ in the

400
00:46:38,000 --> 00:46:42,320
following way, which is kind of the more modern sense, which is what Wally's been talking about

401
00:46:42,320 --> 00:46:49,920
like pretty much this whole time, is abduction used to justify hypotheses. But the older and

402
00:46:49,920 --> 00:46:56,080
original sense of it and still an equally important one is abduction for generating

403
00:46:56,800 --> 00:47:03,440
hypotheses. And this ability to generate hypotheses is something that's extremely

404
00:47:03,440 --> 00:47:09,760
powerful and so far uniquely human. But generate from... Hold on, let me just finish here.

405
00:47:09,760 --> 00:47:15,120
Which is this, this is like something where Einstein is just sitting there pontificating on

406
00:47:16,720 --> 00:47:21,760
how the heck can light be the same no matter how the earth is moving and blah, blah, blah,

407
00:47:21,760 --> 00:47:29,840
and comes up without a thin air, like this hypothesis that relativity applies, right?

408
00:47:29,840 --> 00:47:36,240
That the physical laws are the same no matter what your reference frame is. So this ability

409
00:47:36,240 --> 00:47:40,960
to almost... That people talk about sort of pull from thin air, this kind of intuitive

410
00:47:41,600 --> 00:47:50,240
leap to something that ends up being like a grand new theory, that's also abduction.

411
00:47:50,240 --> 00:47:54,080
Right. But in both cases, Keith, and I agree with you, that's

412
00:47:56,400 --> 00:48:03,760
the old view of what abductive reasoning was to scientists. But in both cases, you're choosing

413
00:48:03,760 --> 00:48:10,960
from possible... Oh, no, no, no, just a minute because this is where I think I probably quite

414
00:48:10,960 --> 00:48:16,160
disagree with you, which is the modern sense of abduction to me is much more similar to just

415
00:48:16,160 --> 00:48:21,280
inference like to a Bayesian. So in other words, you give me a whole slew of hypotheses and I can

416
00:48:21,280 --> 00:48:27,600
tell you which hypotheses should be preferred just on the basis of marginalization and strict,

417
00:48:27,600 --> 00:48:32,560
like Bayesian theory, no problem with that. It's not actually abduction, it's just inference,

418
00:48:32,560 --> 00:48:39,920
right? Just rules of inference. Whereas just a minute, generating that space

419
00:48:39,920 --> 00:48:45,760
in the first place is unique and very different from inference, like the ability to produce

420
00:48:46,880 --> 00:48:52,400
from nothing models to consider, that's the core of abduction from my point of view.

421
00:48:54,160 --> 00:48:59,680
But okay, so we're saying the same thing, but indifferent. These possibilities that you generate

422
00:49:00,480 --> 00:49:05,280
are valid possibilities. So abductive reasoning... I don't know if they're valid until I do the

423
00:49:05,280 --> 00:49:12,880
inference. No, you're generating a pool of possibilities. That's the step, generating a pool

424
00:49:12,880 --> 00:49:18,000
of possibilities. Fine, fine, fine, fine, but in the end... How do you do that? Keith, I think we're

425
00:49:18,000 --> 00:49:24,160
saying the same thing, it's just a terminology. In the end, you're choosing from a set of possible

426
00:49:25,040 --> 00:49:34,960
valid hypotheses. Induction is, you don't know where you're going until you get there. In abductive

427
00:49:34,960 --> 00:49:41,760
reasoning, you are, whether it's the old way or the modern way, in the end, what's common between

428
00:49:41,760 --> 00:49:50,160
them is, I have a set of possibilities. I will use abductive reasoning to decide which is the most

429
00:49:50,160 --> 00:49:55,360
plausible. In a sense, you're scoring them, and you're saying, from all these possibilities, this

430
00:49:55,360 --> 00:50:00,880
is the most plausible. Yeah, but see, you keep assuming the... You keep positing that you have

431
00:50:00,880 --> 00:50:05,280
a bunch of possibilities, and I'm saying those possibilities have to come from somewhere,

432
00:50:05,280 --> 00:50:10,560
and where they come from is abduction. Oh, okay, it depends on the domain and language. They come

433
00:50:10,560 --> 00:50:16,400
from what we know is true. Okay, I see your point. Where they come from depends on the domain of

434
00:50:16,480 --> 00:50:23,440
reasoning. In many cases, they come from what we know is true, or they come from evidence.

435
00:50:24,560 --> 00:50:28,640
Yeah, I guess it's just important to know there are these two senses of abduction,

436
00:50:29,440 --> 00:50:32,160
and don't forget about both of them, because they're both...

437
00:50:32,960 --> 00:50:40,320
Right, and that's why abduction, like induction, as opposed to deduction, abduction and induction

438
00:50:40,320 --> 00:50:45,280
are both approximate. You can never have 100%, because in the end, you're assigning a score,

439
00:50:45,280 --> 00:50:52,000
you're saying. So, both of them are probabilistic in a way, or they have a certain uncertainty.

440
00:50:52,960 --> 00:50:58,560
So, when you're doing abductive reasoning, even in language, I make a decision as this is the right

441
00:50:58,560 --> 00:51:03,920
interpretation given the context, but it's what we call... Could be wrong. You might have eaten

442
00:51:03,920 --> 00:51:08,400
a ball out of this all year. Exactly, and that's why when I read further, I change my first

443
00:51:08,400 --> 00:51:17,600
interpretation. In language, it's not monotonic, actually. We do non-monotonic reasoning in the

444
00:51:17,600 --> 00:51:27,280
sense that I might override my first decision. But all of that is pragmatics, and we do this

445
00:51:27,280 --> 00:51:32,880
in conversation. Two, three sentences after, I understand really fully what you said before,

446
00:51:32,880 --> 00:51:38,720
because I remade the interpretation. And Waleed, do you have any thoughts on where

447
00:51:39,760 --> 00:51:44,480
this came from, or basically the evolution of language, or if you like the evolution of this

448
00:51:44,480 --> 00:51:51,840
abductive athlete? Do you have any ideas, or is it unique to humans? It seems it is.

449
00:51:53,840 --> 00:51:59,920
Unique to humans, definitely. I mean, animal language, animal symbolic languages have been

450
00:51:59,920 --> 00:52:06,160
studied thoroughly. And two things, here's where the genius of photo comes in, productivity. I mean,

451
00:52:06,720 --> 00:52:11,520
language have a finite set of symbols, and they're not productive. They don't do compositions.

452
00:52:13,920 --> 00:52:17,360
And this ties to... Is it animals? Your time up?

453
00:52:19,840 --> 00:52:27,680
No animal, no non-human animal has a productive language. In other words, I have a set of symbols,

454
00:52:28,320 --> 00:52:34,720
and if I can compose them, I can make a new symbol. Language, animals don't compose things,

455
00:52:34,720 --> 00:52:39,040
because they don't decompose them when they're done. They have a finite... It's a hash table.

456
00:52:39,040 --> 00:52:43,760
If I make this symbol, I mean this. If I make this... Okay, no matter how sophisticated it is,

457
00:52:44,560 --> 00:52:48,880
because they don't have recursion, they don't have infinity, they can't deal at that level

458
00:52:48,880 --> 00:52:54,960
with complexity. Some of them have a larger lexicon than others. Okay, but that's still the same

459
00:52:54,960 --> 00:53:02,720
paradigm. So, productivity, in other words, this capacity to learn, we were just talking about John,

460
00:53:02,720 --> 00:53:09,840
or the neighbor next door, or the neighbor next door that just came from California. I can

461
00:53:09,840 --> 00:53:17,120
productively make a person out of three sentences, and in the end, they collapse to a John, right?

462
00:53:17,120 --> 00:53:23,600
That productivity doesn't exist in any species except humans, which means compositionality,

463
00:53:23,600 --> 00:53:30,640
which means systematicity, which means all of that. So, it's unique to human, definitely. This

464
00:53:30,640 --> 00:53:36,720
has been established, and it came with thought. That's the if and only if. That's why we're the

465
00:53:36,720 --> 00:53:41,760
only species that really reason. I mean, okay, I have people insist that animals think and they

466
00:53:41,760 --> 00:53:48,480
reason. They're not really reasoning, okay? Only humans reason, and thought and language came

467
00:53:48,560 --> 00:53:54,960
together. It's sort of like a phenomenon. There are some, there's some proof, even anthropologists,

468
00:53:54,960 --> 00:54:06,480
and they say it looks like language was detected when tools and some basic machinery was detected

469
00:54:06,480 --> 00:54:15,200
first. So, the human mind at some point had this capacity to think and language came with it. It

470
00:54:15,200 --> 00:54:21,920
was like almost at the same time. So, it's uniquely human, definitely now. Where did it come from?

471
00:54:23,440 --> 00:54:37,440
Wow. I think it was the need really to express thoughts. Like at some point, we started having

472
00:54:37,440 --> 00:54:44,320
thoughts that we want to communicate. So, the external artifact we see outside, whether it's

473
00:54:44,400 --> 00:54:53,840
English or ancient Greek or Latin, languages evolve for societal reason and all that. But the

474
00:54:53,840 --> 00:55:01,840
external artifact that we use to communicate thoughts came out of the need of the internal

475
00:55:01,840 --> 00:55:08,640
language that started to develop. What Fodor calls it, the language of thought, mental ease.

476
00:55:09,600 --> 00:55:17,920
And we, so we had that thing going on inside and then we had to communicate. We started with

477
00:55:17,920 --> 00:55:24,480
weird sounds and then we scribbled things on the wall to communicate. And then that thing developed

478
00:55:24,480 --> 00:55:32,080
until we started making symbols like, okay, if I say this, that means this. I don't know the

479
00:55:32,080 --> 00:55:40,160
exact process. I'm not a biologist or evolutionary linguist or, but I think thought is the key here.

480
00:55:40,160 --> 00:55:46,880
So, there's a language of thought. And these external things are because linguistic research

481
00:55:46,880 --> 00:55:52,640
has also shown that there are many universals in language, regardless of what the language is,

482
00:55:52,640 --> 00:55:58,960
even if they are completely different systems like Asian languages and Latin-based languages.

483
00:55:59,520 --> 00:56:07,040
They all have a verb, an action. They all have objects and agents of the action.

484
00:56:07,840 --> 00:56:14,800
They all have events and events have duration, time and place. So, there are a set of cognitive,

485
00:56:14,800 --> 00:56:21,760
I call them universal cognitive primitives, right? There's always an object there somewhere,

486
00:56:21,760 --> 00:56:26,240
or an agent of an activity. Now, how you express it in different languages,

487
00:56:26,720 --> 00:56:33,040
these are universals. That's the language of thought. That's the internal language,

488
00:56:33,040 --> 00:56:40,240
which has to be the same. And objects have properties and all that. So, there are universal

489
00:56:40,240 --> 00:56:49,120
primitives. And we instantiate them in different languages differently, but that's to me secondary.

490
00:56:49,840 --> 00:56:54,640
Okay. Okay. That's great. William, could you talk a little bit about your recent overview?

491
00:56:56,560 --> 00:57:05,840
A colleague that I never worked with, but a colleague in the field. To review this book,

492
00:57:05,840 --> 00:57:11,040
and I looked at it and I said, oh, I have enough on my plate. This is not an easy book.

493
00:57:13,280 --> 00:57:18,880
But then I, because I liked it, I said, yeah, I'd like to write it. And in the end,

494
00:57:18,880 --> 00:57:25,920
it turned out to be not as technically involved as I thought. It's sort of,

495
00:57:27,200 --> 00:57:32,400
and I'm saying that not to be negative, but it's sort of the same argument over and over.

496
00:57:32,400 --> 00:57:38,960
The gist of the argument is quite simple, actually. And they try to prove it from different vantage

497
00:57:38,960 --> 00:57:43,920
points than in the book, from a biological, sociological, psychological, mathematical.

498
00:57:43,920 --> 00:57:54,640
But the gist of the book is any talk of AGI is wishful thinking. And it's beyond anything we

499
00:57:54,640 --> 00:58:04,800
can ever develop mathematically, so as to engineer it in any, in any realistic way.

500
00:58:06,160 --> 00:58:13,120
They make good arguments throughout. There are many examples of the basic idea is that

501
00:58:14,880 --> 00:58:22,480
all the mathematics we know, right, mathematics available to us, cannot model

502
00:58:24,480 --> 00:58:31,200
not just the entire mind, but even subsystems in the mind, language being one of them.

503
00:58:33,360 --> 00:58:38,800
And so it's all complex systems within complex systems in a complex environment,

504
00:58:39,280 --> 00:58:45,360
the system around us that we interact with. And none of it can be modeled mathematically,

505
00:58:45,360 --> 00:58:52,320
none of it even at any level. So forget doing AGI that can interact with us in an intelligent way.

506
00:58:53,280 --> 00:59:00,800
Now, you can do controlled narrow AI, right? You can build very intelligent machines that can do

507
00:59:00,880 --> 00:59:10,480
amazing stuff. But any talk of AGI, strong AI, is just talk until, unless, and they admit that,

508
00:59:10,480 --> 00:59:19,120
unless we come up with a new mathematics that we never even knew at the scale of Leibniz calculus

509
00:59:19,120 --> 00:59:25,280
or Newton, like we're talking about a new mathematics that we never conceived of, right?

510
00:59:25,600 --> 00:59:33,200
Which they say most likely all evidence says that's not going to happen, right? So

511
00:59:34,560 --> 00:59:43,440
now you can get into why. So that's their claim. And why? They say that all these systems are

512
00:59:43,440 --> 00:59:53,520
complex systems. And in complex systems, the idea is that these are, first of all, dynamic systems.

513
00:59:53,520 --> 01:00:01,680
They work in a dynamic environment. They are continuously evolving and adapting, right? They

514
01:00:01,680 --> 01:00:09,600
are self feeding systems. These are not systems that only take input output. These systems change

515
01:00:09,600 --> 01:00:16,080
their behavior. And I gave an example from list. These systems are systems that change their behavior,

516
01:00:16,080 --> 01:00:24,640
their algorithms, if you want, they change their mind from a stimulus. So I might, and that's why

517
01:00:24,640 --> 01:00:29,600
I said they, I would have liked to see a discussion on the frame problem, because the frame problem

518
01:00:29,600 --> 01:00:38,480
in the AI is about this. How can I reason in a dynamic and uncertain environment and react

519
01:00:39,040 --> 01:00:45,120
dynamically, although what I do in the environment might affect what I believe about the environment

520
01:00:45,120 --> 01:00:50,480
in real time. And they're right. There is no mathematics we know of now. That's why we don't

521
01:00:50,480 --> 01:00:57,440
have a solution for the frame problem. So this kind of cyclical cause and effect

522
01:00:58,640 --> 01:01:02,640
cannot be modeled by anything we know on mathematics. And this I agree with them.

523
01:01:04,880 --> 01:01:10,160
They give an example. I made just an example in language, for example. Language, we know.

524
01:01:10,720 --> 01:01:17,920
If I have a dialogue, okay, we all agree that the interpretation of any occurrence requires

525
01:01:17,920 --> 01:01:24,160
having the context in mind as part of the, part of the input to the evaluation of the meaning

526
01:01:24,160 --> 01:01:32,000
is the context as an extra parameter, right? Now, the context is changing based on something I

527
01:01:32,000 --> 01:01:40,480
cannot predict, which is the response of some participant in the dialogue. There is no meaningful

528
01:01:40,480 --> 01:01:47,360
way of predicting how someone might respond. So in other words, the context is mathematically

529
01:01:47,360 --> 01:01:52,560
not defined, but I need it in the interpretation. Thus, no language understanding, no language

530
01:01:52,560 --> 01:01:58,080
understanding, no AGI, because they believe language understanding is a prerequisite. So the,

531
01:01:59,040 --> 01:02:04,960
their conclusion, I mean, you can question every step in this inference they come to,

532
01:02:04,960 --> 01:02:09,600
but they give language as an example, but we have social behavior, I can give an example.

533
01:02:09,600 --> 01:02:15,440
They have a nice example in social behavior. Here's an example of a complex system that cannot,

534
01:02:16,160 --> 01:02:23,840
we don't have any mathematics that can model. We're staying in a queue, in a clinic, an emergency

535
01:02:24,800 --> 01:02:31,520
room. What do they call them? These ER. So, but there's a queue because they all have

536
01:02:31,520 --> 01:02:39,280
emergencies, right? Now, the social behavior, then the social norm is that in the queue,

537
01:02:39,280 --> 01:02:45,040
okay, we all have, we all have urgent issues. But in the end, I came first, right? Okay, so that's a

538
01:02:45,040 --> 01:02:56,880
social norm. And, but can a robot understand that if someone fainted, really, I mean, it's almost

539
01:02:56,880 --> 01:03:08,160
gone, right? Our social norm accepts that this person violates the queue order, right? This is

540
01:03:08,160 --> 01:03:16,640
something dynamic that happens, like the queue is this way. And how can a robot update the rules

541
01:03:16,640 --> 01:03:22,720
and not kill someone because they violated the order of the queue? In other words, these interactions,

542
01:03:22,720 --> 01:03:30,640
these cyclical cause and effect are very complex, that no mathematical model. Or the example I said

543
01:03:30,640 --> 01:03:38,000
in language, they prove this cannot be done. Context is needed to interpret everything. I cannot

544
01:03:38,160 --> 01:03:44,960
predict what the context will be because I cannot predict you respond to my, so it's unpredictable,

545
01:03:44,960 --> 01:03:50,320
they call it erratic, almost random. So there is no mathematics that can model it.

546
01:03:52,160 --> 01:03:56,640
And there are many aspects to the mind, whether it's social reasoning, language, and then they

547
01:03:56,640 --> 01:04:06,080
conclude there cannot be a system that we can model on volume and machines, because we don't

548
01:04:06,080 --> 01:04:12,240
have the mathematics to model it. And these, they go into deep learning. And they give examples

549
01:04:12,240 --> 01:04:17,360
even like deep learning, no matter how much data you ingest, you can never predict the future.

550
01:04:18,640 --> 01:04:26,320
You're lucky if you can do a good job on the past and even forget the future. And definitely

551
01:04:26,320 --> 01:04:32,880
forget the, sorry, the present. So definitely forget. Can I jump in for a minute because I

552
01:04:32,880 --> 01:04:39,680
have a couple of comments. So one is, would you agree that this is quite synonymous with,

553
01:04:39,680 --> 01:04:44,960
you know, Douglas Hofstadter's strange, strange loops and the whole like random reference, the

554
01:04:44,960 --> 01:04:49,600
self-referential self systems, because I mean, complex systems, a big part of them is they

555
01:04:49,600 --> 01:04:55,280
usually are, they do have feedback loops. And at some scale, they become so they will involve

556
01:04:55,280 --> 01:05:01,840
self reference. Yeah. Okay. My other, my other point I want to make is this, you know, I have

557
01:05:01,840 --> 01:05:07,680
quite a bit of sympathy towards the viewpoint, right, of this, of this book that you're talking about,

558
01:05:07,680 --> 01:05:13,440
with one exception, which is I'm still optimistic that we can discover a mathematics that may help

559
01:05:13,440 --> 01:05:20,000
us out. And so I always think to the foundation series by Isaac Asimov, because in there, they

560
01:05:20,000 --> 01:05:26,080
discover a science in a mathematics called psycho history, which at least allows them to predict

561
01:05:26,160 --> 01:05:32,080
complex systems of a certain scale and larger. So in the book, it's sort of like planet scale

562
01:05:32,080 --> 01:05:37,680
and larger, they're able to actually predict, you know, these complex sociological systems

563
01:05:37,680 --> 01:05:43,280
and human behaviors, and how they're going to interact like beyond, beyond that scale. And

564
01:05:43,280 --> 01:05:47,600
it's really fascinating. I make that point. I highly recommend that series to anybody,

565
01:05:47,600 --> 01:05:52,960
because it's very fascinating because, you know, they talk a lot about sort of what if you had

566
01:05:52,960 --> 01:05:58,400
the science, what might it look like, etc. And in there, there's like this little tiny microscopic

567
01:05:58,400 --> 01:06:04,000
thing that's beyond the predictability of psycho history that comes in and kind of mucks up the

568
01:06:04,000 --> 01:06:09,680
works and creates anomalies that they have to constantly keep combating against. So I think

569
01:06:09,680 --> 01:06:14,800
if anybody wants a fictional take on a possible mathematics of this, like, I would recommend

570
01:06:14,800 --> 01:06:21,440
Yeah, I make this point. I say, I agree with their argument. We're trying to model complex systems

571
01:06:21,440 --> 01:06:26,960
in the sense of cyclical cause and effect that we don't have anything that can model them

572
01:06:26,960 --> 01:06:32,160
intelligently. And I give an example in this, in this, I can write a program that changes itself

573
01:06:32,160 --> 01:06:37,600
at runtime. Because this is intentional, I can, the whole program can be a parameter,

574
01:06:38,240 --> 01:06:43,520
which I can look at it. Well, code is data. That's why I can manipulate the program itself

575
01:06:43,520 --> 01:06:47,680
and go look at it after execution and see different program than the one I wrote. It's,

576
01:06:47,680 --> 01:06:53,840
it's amazing list. So if I, so I can write programs in this that no one can understand

577
01:06:54,880 --> 01:07:01,200
and model and do program verification. So I make the argument that, okay, I can see your point.

578
01:07:02,160 --> 01:07:09,120
But like Keith said, never is a long time. Why say we cannot come up with a new mathematics?

579
01:07:09,120 --> 01:07:15,440
I can see you at one point, someone discovering, yeah, at the level of Newton differential calculus,

580
01:07:15,440 --> 01:07:21,760
why not? Which could happen. So the word never for me, it's hard to digest.

581
01:07:21,760 --> 01:07:25,040
Maybe an AGI will discover the mathematics to create itself.

582
01:07:26,480 --> 01:07:32,560
And the other point is, the other point is, which is another point that John McCarthy wants.

583
01:07:33,360 --> 01:07:40,800
Who said we have to understand what we built? Here's what I mean. Do we understand ourselves?

584
01:07:40,800 --> 01:07:47,280
We don't. So why not build a scary intelligent machine that we don't really understand,

585
01:07:47,280 --> 01:07:53,040
like my list program? So what I'm saying is I had, I had an issue with them saying,

586
01:07:53,760 --> 01:08:02,320
that precludes AGI. No, it doesn't. In theory, I can build a complex intelligent machine like us

587
01:08:03,040 --> 01:08:08,640
in many respects. Doesn't feel pain. Hey, who cares? But it's scary intelligent.

588
01:08:09,440 --> 01:08:14,320
And we don't understand how it works. So what? This can happen. I can build something I don't

589
01:08:14,320 --> 01:08:22,720
understand. So in theory, I have two issues with their book, that this never and this absolute

590
01:08:22,720 --> 01:08:26,480
decision that we're done, we can never get there. No, we might build something we don't understand

591
01:08:27,200 --> 01:08:31,760
by discovering some new weird mathematics. So, okay, I agree with you that it's a,

592
01:08:32,480 --> 01:08:35,120
it's a complex thing that we will never understand. But so what?

593
01:08:35,120 --> 01:08:41,120
But what I loved about the book is it's a sobering book. I mean,

594
01:08:42,240 --> 01:08:48,400
it really is a balancing book compared to the hype and the simplicity you see out there. I mean,

595
01:08:48,400 --> 01:08:56,000
you, you recommend it or highly because I mean, I didn't need that much sobering. I know that

596
01:08:56,000 --> 01:09:03,280
any talk of AGI is like, Hey, take a break. Enjoy your paycheck, but don't make silly statements

597
01:09:03,280 --> 01:09:07,680
like this, right? Although I felt I thought you might have fallen off that wagon at the beginning

598
01:09:07,680 --> 01:09:16,800
of this conversation. No, I'm a defender of the faith. But, but so it's, I recommend it to people

599
01:09:16,800 --> 01:09:22,800
that need it. Like me, I needed it too. It's a sobering book. Like, this is how complex what

600
01:09:22,800 --> 01:09:30,080
you're trying to do is. Okay, guys. So before you go out and say, language understand. And the

601
01:09:30,080 --> 01:09:36,240
nice thing is they took aspects of the mind, just language itself is a beast that we cannot conquer.

602
01:09:36,960 --> 01:09:41,200
So imagine the whole mind and the granular thing they go through it. I mean, it's all,

603
01:09:41,200 --> 01:09:44,800
it's complex systems all the way down or all the way up if you want. So

604
01:09:46,720 --> 01:09:51,520
language is a complex system on its own part of the mind, which is a complex system on its own

605
01:09:51,520 --> 01:09:57,440
part of the human living organism, which is a complex system on its own. So and at every level,

606
01:09:57,440 --> 01:10:04,240
the complexity, we don't have a mathematics for that's the gist of their argument. So people that

607
01:10:04,240 --> 01:10:11,040
make these big claims about AI need to read it. Guys, cool down, cool down. You have, you have not

608
01:10:11,040 --> 01:10:17,680
solved problems that occupy the most penetrating minds in the history of enemy from Emmanuel

609
01:10:17,680 --> 01:10:25,280
Khan to you have not solved these problems, cool it down. You can build narrow AI, very narrow AI.

610
01:10:26,240 --> 01:10:32,640
And all this transferability, transferability. I mean, if you're good at chess, I know people

611
01:10:32,640 --> 01:10:39,840
that are good at chess and they're almost good at nothing else, not okay. So forget this. If I'm

612
01:10:39,840 --> 01:10:47,760
good at chess, I can be a smart doctor. No. So we are a very complex machine. So this book is a good

613
01:10:47,760 --> 01:10:54,720
sobering book, mathematically speaking, philosophically speaking, so that people will tone down

614
01:10:54,720 --> 01:11:03,360
what they're saying and start speaking science instead of media gibberish, right? Deep learning

615
01:11:03,360 --> 01:11:11,040
will soon be able to do everything. I mean, from a scientist. Well, it seems like a council of

616
01:11:11,040 --> 01:11:21,040
despair almost. Is there any optimistic or positive hopeful aspects to it? No, I that part I don't

617
01:11:21,120 --> 01:11:31,840
like this never, right? I mean, I am a believer that we can do AGI, but not a human like AI.

618
01:11:32,720 --> 01:11:42,080
We might do a very powerful AI that in many ways is more powerful. I mean, we've done that now. I mean,

619
01:11:42,080 --> 01:11:48,160
machines are now superior to us in many respects and respects even that they require intelligence,

620
01:11:48,160 --> 01:11:55,360
not a bulldozer that can lift more than me, that will have to do cognitive tasks better than us.

621
01:11:55,360 --> 01:12:06,400
We have go or finding patterns and data at the scale that no human can do. So we are building

622
01:12:06,400 --> 01:12:13,120
intelligent machines, but can we conquer things like language like autonomous driving was a failure.

623
01:12:13,120 --> 01:12:17,680
It's a big upset for AI because they trivialize the problem that we can go.

624
01:12:18,560 --> 01:12:22,880
Well, and that's kind of what I want to get to, you know, Mark kind of in response to you, which is

625
01:12:24,240 --> 01:12:28,080
I take these kind of sobering, these sobering things and look, I mean,

626
01:12:30,160 --> 01:12:35,600
the book sounds great, and I'm definitely going to get it and read it. But some of these thoughts,

627
01:12:35,600 --> 01:12:41,520
you know, many people have had, you know, many times over the years, right? And I've recognized

628
01:12:41,600 --> 01:12:46,000
that there are these limitations. But I think like part of part of why I think

629
01:12:46,000 --> 01:12:51,360
books like this are actually have an optimistic kind of side to them is I hope, I hope they

630
01:12:51,360 --> 01:12:59,680
encourage people to get more creative. Okay, like, like stop just trying to dump every single dollar

631
01:12:59,680 --> 01:13:05,760
you have into yet another parameter, you know, into yet another thousand or billion parameters

632
01:13:05,760 --> 01:13:10,240
in a model, like, let's take some of our resources, like, sure, let's keep doing that engineering,

633
01:13:10,240 --> 01:13:15,680
but let's take some portion of our resources here and invest it in, like, crazy ideas. And I know

634
01:13:15,680 --> 01:13:21,280
Tim's smile here because it's like, like, can it's can a Stanley kind of type thing, right? Like, just

635
01:13:22,080 --> 01:13:28,640
go out there and try to do something crazy to find that mathematics that we need, right? Which is

636
01:13:28,640 --> 01:13:34,560
let's get creative, let's work on crazy things, let's have crazy ideas, let's work on hybrid

637
01:13:34,560 --> 01:13:40,800
systems, let's not give up on, you know, neuromorphic, you know, systems and computer,

638
01:13:40,800 --> 01:13:45,840
whatever, like, let's spread out, let's spread out a bit, because of the fact that if we just

639
01:13:45,840 --> 01:13:53,680
keep going down this direction of ever larger Turing machines, like, that may not be the solution.

640
01:13:54,240 --> 01:14:01,680
Actually, they make this point exactly in different ways that if anything, their goal is to let people

641
01:14:01,760 --> 01:14:07,920
widen their horizon. So many aspects of this problem that guys, if we keep going,

642
01:14:07,920 --> 01:14:11,680
this is not going to get us there. And that's why they argued mathematically,

643
01:14:11,680 --> 01:14:15,920
philosophically. And I think they have a good argument. This is not going to take us there.

644
01:14:16,480 --> 01:14:24,320
But let's explore that. So that and being so religious about this will will hinder any other

645
01:14:24,880 --> 01:14:33,040
possibility. So overall, their argument is a good argument. I think everybody should read this book

646
01:14:33,040 --> 01:14:42,400
that's interested in AGI as a goal. What we have cannot ever take us there. They prove this

647
01:14:42,400 --> 01:14:48,640
mathematically. I mean, to me, they proved it in language on. So we need something new. And if you

648
01:14:48,640 --> 01:14:53,680
want to do something new, we can't just stay in this corner and with this, that's not going to get

649
01:14:53,680 --> 01:15:01,280
us there. So in a way, it's not a negative book. It's a sobering book, I will use the term sober.

650
01:15:02,080 --> 01:15:07,200
Well, and encouraging of more variety and more daring and more creativity and

651
01:15:07,760 --> 01:15:12,640
right, they don't push too much on that. But but indirectly, the indirect

652
01:15:13,760 --> 01:15:20,720
net result, if people appreciate the argument will be to look and explore other ways. So

653
01:15:21,280 --> 01:15:27,440
in a way, it's more positive than negative. And by the way, stating a mathematical fact is never

654
01:15:27,440 --> 01:15:35,120
negative. Oh, don't be so sure about that. If they're saying we're always on the edge of being

655
01:15:35,120 --> 01:15:41,040
canceled for stating like mathematical facts. So yeah, but I mean, if they're saying that

656
01:15:42,160 --> 01:15:48,560
what we're doing now, we're not we'll never get us to AGI. That's not negative. You're saying we

657
01:15:49,360 --> 01:15:54,800
need something else, we need something more. Or yours, yours, look, it could have saved us,

658
01:15:54,800 --> 01:16:01,040
they use this phrase, money down the drain. Autonomous driving is a case, is a good case.

659
01:16:02,480 --> 01:16:07,200
Billions, we're talking non trivial money guys, we're talking more than the budgets of some

660
01:16:07,200 --> 01:16:15,360
European countries. Just imagine the scale. And those guys went bust, right? Why? Because

661
01:16:15,440 --> 01:16:20,880
they trivialized the autonomous truck. An autonomous car is an autonomous agent, guys.

662
01:16:21,840 --> 01:16:28,640
It's an it's an agent trying to reason in a dynamic and uncertain environment and has on the

663
01:16:28,640 --> 01:16:34,320
fly to change to do belief revision, change its strategy, because of something new that came up.

664
01:16:35,040 --> 01:16:40,160
All of that is from seeing the tree and the stop sign. It's all vision.

665
01:16:40,960 --> 01:16:45,280
Yeah, so this is actually encouraging because now for all the Uber drivers out there and

666
01:16:45,280 --> 01:16:52,320
long hauled truck drivers, your job is safe. Like it's not going to be replaced anytime soon.

667
01:16:52,320 --> 01:16:56,880
Yeah, it's amazing. And a few years back when I was still in the valley,

668
01:16:58,160 --> 01:17:05,200
yeah, I was in Mecca in Silicon Valley. And I would talk to superb PhDs in neuroscience and

669
01:17:05,920 --> 01:17:12,080
big AI engineers at top companies. They were so excited that we're at level four

670
01:17:12,080 --> 01:17:16,640
in a year or two. And this was six years ago. I say, guys, this will not happen. They say,

671
01:17:16,640 --> 01:17:22,080
why are you so negative? I said, you cannot have an autonomous agent on the road without

672
01:17:22,080 --> 01:17:26,960
solving the frame problem. How do I revise everything I know, because of this new event?

673
01:17:28,240 --> 01:17:32,880
And this has to happen real time. We don't have a solution for the frame problem.

674
01:17:32,880 --> 01:17:39,200
All you're doing is you have cars on a railway. We have autonomous cars now. It's called the train.

675
01:17:42,000 --> 01:17:48,320
We have autonomous flight. If I'm not in a dynamic and uncertain environment reasoning,

676
01:17:48,960 --> 01:17:54,080
yeah, I can have autonomous anything. We call it the railway. I mean,

677
01:17:55,200 --> 01:17:58,800
we call it Amtrak. It's autonomous. You press a button and it goes.

678
01:17:59,760 --> 01:18:03,200
If we're talking about reasoning in the streets of San Francisco,

679
01:18:04,400 --> 01:18:07,360
you have to face the frame problem or you will kill people.

680
01:18:09,840 --> 01:18:16,720
Anyway, on that sobering note, I think a lot of stuff goes unnoticed in San Francisco,

681
01:18:16,720 --> 01:18:22,240
so I'm not sure about that. Probably that's the least of them. No, but the scale of money that

682
01:18:22,320 --> 01:18:29,360
went the scale. This is the value of this book, the scale of investment. I mean, if 10% of that

683
01:18:29,360 --> 01:18:35,360
was put on another approach, hey, you weird guy with this weird idea, take 10% of what we're

684
01:18:35,360 --> 01:18:43,680
throwing down the drain and explore something else. Show me. That's where I'm at, too.

685
01:18:44,960 --> 01:18:51,280
Diversify the effort. There's a huge impact here, societal impact. We're wasting billions of dollars

686
01:18:51,280 --> 01:18:57,200
just because I don't want to listen to anyone else. It happened in the chatbot industry, which

687
01:18:57,200 --> 01:19:03,040
I'm more familiar with than autonomous driving. Chatbot this, chatbot that, and there was an

688
01:19:03,040 --> 01:19:10,880
explosion. It was like a blob, like the internet thing. Now we can't get away from them. Every

689
01:19:10,880 --> 01:19:19,440
website we go to, it's like, leave me alone. Nobody wants to use them because we know how

690
01:19:19,440 --> 01:19:28,160
they work. These are stochastic parents. Yeah, literally just going to a FAC and hitting control

691
01:19:28,160 --> 01:19:36,560
F is more effective for me than trying to interact with a chatbot. The search engines by key phrases

692
01:19:36,560 --> 01:19:40,880
you put and they bring you a link and they say, read this. This is your answer. They are search

693
01:19:40,880 --> 01:19:47,040
engines basically. But again, the amount of money, because I lived in that industry,

694
01:19:48,320 --> 01:19:56,560
the amount of money spent on chatbots will scare the hell out of anybody. You combine that with

695
01:19:56,560 --> 01:20:04,400
autonomous driving, both dead, almost zero. We're talking billions and billions and billions. And

696
01:20:04,400 --> 01:20:10,000
you talk to any one of them in the highest, in the middle of the fever. They won't listen to you.

697
01:20:10,000 --> 01:20:17,840
I have people now calling me back and saying, you were right. Yeah, after $200 billion. So,

698
01:20:17,840 --> 01:20:24,080
science is important. Engineering is important, but science is important too. That's where the

699
01:20:24,080 --> 01:20:31,680
value of this book is. Guys, hacking alone will not do the whole thing. You're a bright engineer,

700
01:20:31,680 --> 01:20:36,080
you can hack your way through what we know is true. That's the space you can play with.

701
01:20:36,320 --> 01:20:45,920
You cannot hack your way in a bigger set of possibilities that are. You didn't verify that

702
01:20:45,920 --> 01:20:53,600
you can go there. An engineer can be creative within a Venn diagram that the scientists drew

703
01:20:53,600 --> 01:21:00,400
for them. That's the difference between science and engineering. The scientist draws the Venn

704
01:21:00,400 --> 01:21:05,840
diagram and that's the value of philosophers, at least the analytic philosophers, that know logic

705
01:21:05,840 --> 01:21:11,920
and metaphysics and quantum mechanics and philosophers that are on the technical side.

706
01:21:12,720 --> 01:21:18,400
They know how to draw the Venn diagram. You, as an engineer, if you're wasting your time here,

707
01:21:19,280 --> 01:21:25,120
that's called money down the drain. Play inside the Venn diagram. Otherwise,

708
01:21:25,920 --> 01:21:30,640
you're just an over enthused engineer who should go back and study computability.

709
01:21:31,600 --> 01:21:35,760
It's kind of like how patent examiners can easily reject anything that comes in that

710
01:21:35,760 --> 01:21:42,480
claims to violate the second law of thermodynamics, right? Well, listen, Wally, I think we

711
01:21:42,480 --> 01:21:48,720
sincerely appreciate your time today. And also, Mark, thank you for joining us and asking great

712
01:21:48,720 --> 01:21:56,160
questions. We should do this again. Thanks, Wally. Yeah, I really appreciate it. So thanks,

713
01:21:56,160 --> 01:22:01,760
always fun, guys. I see. Peace.

