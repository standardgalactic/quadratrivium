start	end	text
0	8400	my fondest memory. It's usually when I discover something that I think nobody has seen before,
9440	15760	but that happens very rarely because most of the things you think of somebody else has done before.
16400	21840	This episode is sponsored by Numeri. Are you a data scientist looking to make a real-world impact
21840	28000	with your skills? Do you love competing against the best minds in the world? Well, introducing
28000	34400	Numeri, the revolutionary cutting-edge AI-driven hedge fund that's changing the game for good.
34400	40320	Numeri combines a competitive data science tournament with powerful, clean stock market data
40320	45200	enabling you to predict the market like never before. Sign up now, become part of the elite
45200	49680	community, taking the stock market by storm and I'll see you on the leaderboard.
49760	59360	Wonderful. So today is a momentous occasion. What an episode of MLST we're going to have.
59360	65920	We're joined not by a godfather of AI, but the father of AI, you again, Schmidhuber,
65920	71360	the researcher responsible for leading the research groups which invented much of the technology
71360	76080	which has powered the deep learning revolution. It's long been a dream to get you on the podcast,
76160	80960	you again. It feels like the day has finally arrived, so welcome to MLST.
80960	86560	Thank you, Tim, for these very kind words and this very generous introduction.
89840	94800	So on that, let's discuss the credit assignment problem in machine learning. Now,
94800	100800	you've dedicated a significant amount of time researching and publishing the actual history
100800	105360	of the field and there's a significant divergence between the public narrative
105360	110960	and what actually happened. And amazingly, no one has pointed out any factual inaccuracies in your
110960	116400	accounts, but the incorrect perceptions still persevere. Now, I particularly enjoyed reading
116400	120800	your history of the breakthroughs in machine learning, going back to ancient times and of course even
120800	126080	remarking on the very first computer scientist, Leibniz. And for example, you pointed out the
126080	131440	history of who invented backprop and the CNN. And you explained that there wasn't really
131440	136400	a neural network winter at all in the 1970s. So could you just sketch out a little bit of that
136400	150400	history? So that's a challenge. Actually, computer science history and computing history started
150400	160560	maybe 2000 years ago when Heron of Alexandria built the first program-controlled machine.
160560	169760	That was 2000 years ago in the first century basically. And he basically built an automaton
169760	178720	that was programmed through a cable which was wrapped around a rotating cylinder which had
178720	186480	certain knobs and then there was a weight which pulled it down and the whole apparatus
186480	194880	was able to direct the movements of little robots, of little puppets in an automatic theater.
196000	204800	That, as far as I know, was the first program-controlled machine in the history of mankind.
204800	211920	Even before that there were other machines. The ancient Greeks had even earlier the
212720	220880	Antiqueterre mechanism which was kind of a clock, an astronomical clock. But then more recently
223200	229680	we have seen many additional advances and you mentioned Leibniz, of course, who is of
229680	236320	special interest to our field because he not only is called the first computer scientist
236320	245280	because he had the first machine with a memory that was in the 1680s, I think. He not only had the
247600	255200	first machine that could do all the basic arithmetic operations which are addition,
255200	265920	multiplication, division and subtraction, then he not only had these first ideas for a universal
265920	272240	problem solver that would solve all kinds of questions, even philosophical questions,
272880	282000	just through computation. And he not only was the first who had this algebra of thought which
282000	291040	is deductively equivalent to the much later Boolean algebra. In many ways he was a pioneer,
291040	296240	but especially in our field in deep learning he contributed something essential, which is really
296240	304640	central for this field, which is the chain rule. I think 1676, that's when he published that and
304640	315440	that's what is now being used to train very deep artificial neural networks and also shallow
315440	321920	neural networks and recurrent neural networks. And everything that we are using in modern AI
321920	330000	is really in many ways depending on that early work. But then of course there was so much additional
330000	340640	work. The first neural networks, as we know them, they came up about around 1800. That's when Gauss
340640	349680	and Legendre had the linear neural networks, the linear perceptrons in the sense that they were
349680	359920	linear without having any non-differential aspect to it. So these first neural networks,
360560	372640	back then, were called method of least squares. And the training method was regression and the
372640	378080	error function was exactly the same that we use today. And it was basically just a network with
378080	384560	a set of inputs and a set of outputs and a linear mapping from the inputs to the outputs. And you
384560	392960	could learn to adjust the weights of these connections. So that was the first linear neural
392960	402800	network and many additional later developments led to what we have today. You had this beautiful
402800	407280	statement. You said that machine learning is the science of credit assignment and we should apply
407920	413520	that same science to the field itself. And I guess what I'm really curious about is
414240	419280	first, if you could educate our listeners just a bit on what credit assignment is in the context
419280	424800	of, say, machine learning and why you think it's important that that should apply to the field
424800	429040	in general. You know, why should we care about credit assignment? Why should we study the history
429040	436560	of the developments in the field? Why is it important? I'm interested in credit assignment,
437200	442320	not only in machine learning, but also in the history of machine learning,
443120	451200	because machine learning itself is the science of credit assignment. What does that mean? Suppose
451200	459760	you have a complicated machine, which is influencing the world in a way that leads to the solution
459760	465200	of a problem. And maybe the machine solves the problem. But then the big question is,
465200	472720	which of the components of these many components were responsible? Some of them were active
472720	481760	a long time ago and others later and early actions set the stage for later actions. Now,
482960	486880	if you want to improve the performance of the machine, you should figure out how
487600	496000	did the components contribute to the overall success. And this is what credit assignment is
496000	502000	about. And in machine learning in general, we have a system consisting of many
504000	512640	machine learning engineers and mathematicians and hardware builders and all kinds of people.
512640	518560	And there you also would like to figure out which parts of the system are responsible for later
518560	524800	successes. Yeah, and it's a brilliant point. And I completely agree with you, by the way.
524800	530960	And I think the way I think about it is you've got this giant architecture of humanity and in it
530960	535760	are these certain nodes that may be an individual, maybe a research group. And if they come up with
535760	541600	things that are very helpful, right, you want to try and direct more attention, more resources,
542000	548960	at that nodule, at that node, right, because it's likely to come up with additional very
548960	554080	important things. And if we don't get that right, we're just not optimizing the algorithm of science
554080	564160	as a whole. That's right, yes. Machine learning and science in general is based on this principle
564160	571360	of credit assignment where credit usually doesn't come in form of money, sometimes also in form
571360	581440	of money, but in form of reputation. And then the whole system is set up such that you create
581440	592080	an incentive for people who have worked on improving some method to credit those who
592080	599200	maybe came up with the original method and to just have these chains of credit assignment
599840	607280	that make clear who did what, when, because the whole system is based on this incentive.
607280	616480	And yes, those who are then credited with certain valuable contributions, they also can get
616480	623360	reasonable jobs within the economy and so on. But that's more like the secondary
623760	631920	consequence of the basic principle. And that's why all PhD advisors
634080	641120	teach their PhD students to be meticulous when it comes to credit assignment to past work.
642320	648560	So one last question, if I may, I've really enjoyed studying the history of advancement
649520	654320	because I found that when I go back and read original source materials, let's say
654960	661200	Einstein's first paper on diffusion or anything like that, because they're breaking new ground,
661200	668320	they're considering a wider array of possibilities. And then over time, the field becomes more and
668320	674480	more focused on a narrower avenue of that. And you can go back and look at the original work
674560	679120	and actually gain a lot of inspiration for alternative approaches or alternative
679680	685440	considerations. So in a sense, it's kind of in the sense of forgetting is as important as learning.
685440	690080	Sometimes we need to go back to go down a different branch of the tree, if you will,
690080	695680	and expand the breadth of the search a little bit. I'm curious if you've noticed that same phenomenon.
695840	707440	Yes, science in general is about failure. And 99% of all scientific activity is about
709200	712080	creating failures. But then you learn from these
714240	720080	failures and you do backtracking. And you go back to a previous decision point where you maybe
721040	727680	made the wrong decision and pursued the wrong avenue. But now you have a branching point and
727680	737360	you pursue an alternative. And in a field that is rapidly moving forward, you don't go back very
737360	743440	far usually. You just go back to a recent paper which came out five months ago. And maybe you
743440	748640	have a little improvement there. And then maybe there's yet another little improvement there.
749200	753520	And some parts of our field are at the moment a little bit like that,
753520	760320	where PhD students are moving in, who just look at the most recent papers and then find a way of
760320	769440	improving it a little bit and 2% better results on this particular benchmark. And then the same guys
769440	776320	are also reviewing at major conferences, papers by similar students and so on. And so then sometimes
776320	785680	what happens is that no very deep backtracking is happening, just because the actors aren't really
786720	793280	aware of the entire search tree that has already been explored in the past.
794240	802320	On the other hand, science has this way of healing itself. And since you can gain reputation by
802720	812080	identifying maybe more relevant points, branching points, you have this incentive within the whole
812080	821360	system to improve things as much as you can, sometimes by going back much further.
821920	832080	So there's been a lot of discussion in the discourse around this concept of AI existential
832080	837920	risk. And you again, you've published quite a few pieces about this recently, prominently in
837920	844160	The Guardian and in Forbes actually. And one of the things I wanted to focus on is this concept
844160	850800	of recursive self-improvement, because that seems to be one of the plausible explanations that these
850800	855840	folks give. And of course, when it comes to recursive self-improvement, you are an expert in
855840	863200	this field. I mean, Godel machines come to mind immediately. So I want to kind of explore asymptotes
863200	871200	and limitations. This whole idea of recursive self-improvement is very sexy, isn't it?
871200	884240	In fact, it is the one idea that motivated me to do all of this. So my first paper ever in 1987,
884240	890480	that was my diploma thesis. And it was about this recursive self-improvement thing. So it was about
890480	898880	machine that learns something in a domain. But not only that, it also learns on top of that to
899680	908400	learn a better learning algorithm based on experience and the lower level domains. And then
908400	918400	also recursively learns to improve the way it improves the way it learns. And then also recursively
918960	926000	learns to improve the way it improves the way it improves the way it learns. And yeah, I called that
926000	935440	meta-learning. And back then, I had this hierarchy with, in principle, infinite self-improvement
935440	943440	in the recursive way, although it is always limited by the limited time that you run the system like
943440	954080	that. And then, of course, the motivation behind that is that you don't want to have an artificial
954080	961760	system that is stuck always with the same old human-designed learning algorithm. No, you want
961760	969200	something that improves that learning algorithm without any limitations, except for the limitations
969200	980160	of physics and computability. And so much of what I have been doing since then is really about that.
980240	987040	Self-improvement in different settings where you have, on the one hand, reinforcement learning
987040	995440	systems that learn in an environment to better interact and better create ways of learning
995440	1006880	from these interactions to learn faster and to learn to improve the way of learning faster,
1006960	1014320	and so on. And then also gradient-based systems, artificial neural networks, that learn through
1015520	1022480	gradient descent, which is a pre-wired human-designed learning algorithm, to come up with a better
1022480	1030640	learning algorithm that works better in a given set of environments than the original human-designed
1030640	1039440	one. And yeah, that started around 1992 neural networks that learned to run their own learning
1039440	1049040	algorithms on the recurrent network themselves. So you have a network which has standard connections
1049040	1054640	and input units and output units, but then you have these special output units which are used to
1054640	1063360	address connections within the system, within this recurrent network, and they can read and
1063360	1070480	write them. And suddenly, because it's a recurrent network and therefore it is a general-purpose
1070480	1080720	computer, suddenly you can run arbitrary algorithms on this recurrent network, including arbitrary
1080720	1086560	learning algorithms that translate incoming signals, not only the input signals, but also the
1086560	1094640	evaluation signals like reinforcement signals or error signals into weight changes, fast weight
1094640	1104000	changes, where the weight changes are not dictated any longer through this gradient descent method,
1104080	1111840	but no, now the network itself is learning to do that. But the initial weight matrix is still
1113200	1117600	learned through gradient descent, which is propagating through all these self-referential
1117600	1125520	dynamics in a way that improves the learning algorithm running on the network itself. That
1125520	1132080	was 1992, and back then, compute was really, really slow, it was a million times more expensive
1132160	1138080	than today, and you couldn't do much with it. But now, in recent works, all of that is working
1138080	1145760	out really nicely and has become popular, and we have, just if you look at the past few years,
1145760	1152880	a whole series of papers just on that. So that's the fast weight programming that you're referring
1152880	1161680	to? Yes, so it's fast weight programmers where you have a part of the network that
1161680	1170080	learns to quickly reprogram another part of the network, or the original version of that was
1170080	1175040	actually two networks, so one is a slow network, and then there's another one, a fast network,
1175040	1182560	and the slow network learns to generate weight changes for the second network,
1183360	1190880	and the program of the second network are its weights. So the weight matrix of the second
1190880	1196080	network, that is the program of the second network, and the first one, what does it do? It
1196800	1204720	generates outputs, it learns to generate outputs that cause weight changes in the second network,
1204720	1209840	and these weight changes are being applied to patterns, to input patterns, to queries, for
1209840	1219200	example, and then the first network essentially learns to program the second network, and essentially
1219200	1225440	the first network has a learning algorithm for the second network, and the first system of that
1225440	1234560	kind, 1991, that was really based on on keys and values, so the first network learns to program
1234560	1241040	the second network by giving it keys and values, and it says now take second network, take this
1241040	1248960	key and this value, and associate both of them through an outer product, which just means that
1248960	1255600	those units are strongly active, they get connected through stronger connections, and
1256880	1262320	the mathematical way of describing that is the outer product between key and value.
1263840	1268480	So that's how the first network would program the second network, and the important thing was that
1268480	1275520	the first network had to invent good keys and good values, depending on the context of the input
1275520	1283360	stream coming in, so it used the context to generate what is today called an attention mapping,
1283360	1292640	which is then being applied to queries, and this was a first step right before the most general
1294080	1300400	next step, which is then really about learning a learning algorithm running on the network itself
1300400	1303600	for the weights of the network itself.
1306880	1313520	Could I press you a tiny bit on this concept of meta-learning and convergence and asymptotes?
1313520	1320240	Now one of the reasons I think why the X-Risk people believe that it will just go on forever
1320240	1326080	is they believe in this idea of a pure intelligence, one that doesn't have physical limitations in
1326080	1332400	the real world, and I'm quite amenable to this ecological idea of intelligence that it does,
1332400	1336400	the world is a computer basically as well as the actual brain that we're building,
1337440	1344000	so surely it must hit some kind of asymptote. Do you have any intuition on what those limitations
1344000	1355120	would be? So you are talking about the ongoing acceleration of computing power and limitations
1355200	1361840	thereof, is that what you have in mind here? Well that's one part of it, so even if you
1361840	1366720	just scale transformers I think there would be some kind of asymptote, but we're talking here
1366720	1372560	about meta-learning, learning to learn, how to learn, and recursive self-improvement, and it's
1372560	1377680	similar to this idea of reflection, self-reflection and language models, it actually improves the
1377680	1383360	performance with successive steps of reflection and then it levels off, it reaches an asymptote.
1383520	1387920	I just believe that there are asymptotes everywhere and that's the reason why I
1387920	1391680	don't think recursive self-improvement will go on forever, but I just wondered if you had
1391680	1395920	any intuitions on what those impressions are. Yeah, you are totally right, there are certain
1397200	1404320	algorithms that we have discovered in past decades which are already optimal in a way
1405440	1411840	such that you cannot really improve them any further, and no self-improvement and no fancy
1412080	1419360	machine will ever be able to further improve them. There are certain sorting algorithms that
1419360	1426720	under given limitations are optimal and you can further improve them. That's one of the limits.
1426720	1433920	Then of course there are the fundamental limitations of what's computable, first identified by
1433920	1443040	Kurt Gödel in 1931, he just showed that there are certain things that no computational process
1443600	1453600	can ever achieve. No computational theorem prover can prove or disprove certain theorems
1453600	1462720	in a language, in a symbolic language that is powerful enough to encode
1462720	1469760	certain simple principles of arithmetic and stuff like that. What he showed was that
1471040	1476800	there are fundamental limitations to all of computation and therefore there are fundamental
1476800	1487280	limitations to any AI based on computation. I'm glad you brought that topic up because it's one of
1488240	1494800	our favorite things to discuss which is do you think the human mind ultimately reduces to just
1494800	1500080	an effective computation and so subject to those same limits or do you think there's any
1501840	1507920	known or unknown physics that give us some out in which the brain can do a computation that
1507920	1518720	amounts to hypercomputation? Since we have no evidence that the brain can compute something
1519280	1527520	that is not computable in the traditional sense, in Gödel sense and torings and churches sense
1527520	1536480	and everybody who has worked on this field, since we have no evidence we shouldn't assume that's the
1536560	1544880	case. As soon as someone shows that people can compute certain things or prove certain theorems
1544880	1555840	that machines cannot prove given the same initial conditions, we should look more closely but
1556960	1564080	there are many things that might be possible in fairy tales and we are not really exploring them
1564080	1572240	because the probability of coming up with interesting results is so low. Fair enough,
1572240	1578000	so you mentioned so far two asymptotes, one being of the mathematical kind where there's just
1578560	1582880	mathematical proofs that certain things are optimal, the other one being the limits of
1582880	1589680	computation itself. What other asymptotes do you see applying to or putting bounds on recursive
1589680	1602080	self-improvement? The most obvious thing is probably light speed and the limits of physical
1602080	1611760	computation. We know those for several decades, we have happily enjoyed the fact that every five
1611760	1620960	years compute is getting 10 times cheaper and this process started long before Moore's law was
1622960	1631360	defined in the 60s I believe because even in 1941 already when Susie built the first program
1631360	1639360	controlled computer this law apparently was active so back then he could compute maybe one
1639360	1645200	instruction per second and since then every 10 years a factor of 100 every 30 years a factor of
1645200	1655520	a million more or less until today and there's no reason to believe it won't hold for a couple
1655520	1663440	of additional decades because the physical limits are much further out. The physical limits that we
1663440	1673120	know are the Bremermann limit discovered I think in 1983 by Bremermann and they basically say that
1673120	1678640	one kilogram of matter cannot compute more than 10 to the 51 instructions per second.
1680080	1687360	So that's a lot of compute but it's limited and to give you an idea of how much compute that is
1688000	1696240	I also have a kilogram of computer in here and probably it cannot compute 10 to the 20
1696240	1704240	instructions per second otherwise my head would explode because of the heat problem
1705840	1711680	but maybe it can compute something that is not so far from 10 to the 20 instructions maybe 10
1711680	1719280	to the 17 something like that although most of my neurons are not active as we speak because again
1719280	1727760	otherwise my head would just evaporate. Now if you have an upper limit of 10 to the 20 instructions
1727760	1737520	per brain then the upper limit of all of humankind would be 10 billion times that individual limit
1737600	1744480	and that would be 10 to the 30 instructions per second and you see it's still far away from the
1744480	1751520	10 to the 51 instructions per second that in principle one kilogram of matter could compute
1751520	1760640	and now we have more than 10 to the 30 kilograms of matter in the solar system and there's some
1761600	1769120	and so if the current trend continues at some point much of that is going to be used for
1769120	1775840	computation but then it will have to slow down even if the exponential acceleration
1776880	1784160	will still be with us for a couple of decades because at some point it is going to be a polynomial
1784720	1792400	because due to the limits of light speed at some point it will be harder and harder
1792400	1797840	to acquire additional mass once you have reached the limits of physical computation per kilogram
1797840	1804560	the only way to expand is to go outwards and you know find additional stars and additional
1804560	1816480	matter further away from the solar system and then you will get a polynomial acceleration or
1816480	1823520	a polynomial growth at best so it will be much worse than the current exponential
1823520	1831440	growth that we are still enjoying. Sure but I would say you know the existential threat
1831520	1837520	that is more than sufficient to supply an existential threat and let me just put this
1837520	1841760	a little bit differently which is and I agree with you on this which is you are quoted as
1841760	1846000	saying that traditional humans won't play a significant role in spreading intelligence
1846000	1850720	across the universe and I think you are right I think we kind of share a vision of something
1850720	1857680	like the von Neumann probes that go out into space and form this star spanning civilization of
1857680	1863120	machines and artificial intelligence that have transcended you know biological limitations
1863120	1869120	so I guess my question to you is once that space faring star spanning you know civilization
1869120	1876480	exists if it becomes misaligned with us and decides that we are in the way right isn't that
1876480	1881280	an existential threat I mean might they just you know repurpose the earth regardless of whether
1881280	1888240	we're here or not for for their own aims yeah I'm often getting these questions and
1889440	1900480	and there is no proof that we will be safe forever or something like that on the other hand it's also
1901840	1910400	very clear as far as I can judge that all of this cannot be stopped and it can be channeled
1910400	1920800	in a very natural and I think good way in a way that is good for humankind now
1922400	1928880	first of all at the moment we have a tremendous bias towards good AI
1931200	1938880	meaning AI that is good for humans why because there is this intense commercial pressure
1938880	1946000	to create stuff that humans want to buy and they like to buy only stuff they think is good
1946000	1954000	for them which means that all the companies that are and that are trying to devise AI products
1954000	1962240	they are maximally incentivized to generate AI products that are good for those guys who are
1962240	1970640	buying them or at least where the where the customers think it's good for them
1972080	1979920	so it is still 95 so it may be five percent of all AI researchers really about AI weapons and
1979920	1986000	one has to be worried about that when all this has to be worried about weapons research but
1986000	1990800	there's a tremendous bias towards good AI so that is one of the reasons why you can be
1991440	1997920	a little bit optimistic for the future I'm always trying to point out the two types of
1997920	2011840	AIs there are those who are just tools of users human human users and the others that invent
2011840	2019520	their own goals and they pursue their own goals and both of them we have had for a long time
2020160	2026400	now for the AI tools it's kind of clear there's a human and a human wants to achieve something
2026400	2035680	and so it uses he uses or she uses that tool to achieve certain ends and and most of those are
2036320	2046000	of the type let's improve healthcare and let's facilitate translation from one person to another
2046000	2053360	one in another nation and just make life easier and make human lives longer and healthier
2054960	2062160	okay so that that's the AI tools but then there are the other AIs which also have existed in my
2062160	2070240	lab for at least 32 years which invent their own goals and they are a little bit like little
2070240	2079920	scientists where you have an incentive to explore the environment through actions through
2079920	2085200	experiments self-invented experiments that tell you more about how the world works such that you
2085200	2089760	can become a better and better and more and more general problem solver in that world
2090480	2098080	and so these AIs they have for a long time created their own goals and now of course
2098080	2105920	the interesting question is these more interesting AIs what are they going to do once they are
2108800	2114320	once they have been scaled up and can compete or maybe outperform humans and everything
2115440	2123040	they want to achieve so on the one hand the AI tools and there the greatest worry is
2123280	2131760	what are the other humans going to do to me with their AI tools so in the extreme case you have
2131760	2139600	people who are using AI weapons against you and maybe your neighbor is has bought a little drone
2139600	2147600	for 300 dollars and it has face recognition and it has a little gripper and it flies across the
2147600	2155760	hedge and puts some poison into your coffee or something like that so then the problem is not
2155760	2163120	the AI which is trying to enslave humans or something silly like that no it's your neighbor
2163120	2172080	or the other human and generally speaking you have to be much more afraid of other humans than you
2172080	2182880	have to be of AIs even those who define or set themselves their own goals because you must mostly
2183520	2191600	worry about those with whom you share goals so if you share goals then suddenly there is a potential
2191600	2198960	of conflict because maybe there is only one schnitzel over there and two persons want to
2198960	2205600	eat the schnitzel and suddenly they have a reason to fight against each other generally speaking
2205600	2216000	if you share goals then you can do two things you can either collaborate or compete an extreme form
2216000	2226560	of collaboration would be to maybe marry another person and set up a family and master life together
2226960	2241200	and an extreme form of competition would be war and and those who share goals they have many more
2241200	2251360	incentives to interact than those who don't share goals and so humans are mostly interested in other
2251360	2258400	humans because they share similar goals and because they give them a reason to collaborate or to
2258400	2266000	compete most CEOs of certain companies are interested in other CEOs of competing companies
2266000	2272160	and five-year-old girls are mostly interested in other five-year-old girls and the super smart AIs
2272160	2277440	of the future who set themselves their own goals they will be mostly interested in other super
2277440	2285600	smart AIs of the future who set themselves their own goals generally speaking there is not so much
2285600	2293840	competition and there are not so many shared goals between biological beings such as humans
2293840	2302320	and a new type of life that as you mentioned can expand into the universe and can multiply
2302400	2308880	in a way that is completely infeasible for biological beings so there's a certain
2310000	2315600	long-term protection at least through lack of interest on the other side
2318720	2324640	okay brilliant there's a few things I wanted to touch on there we will get on to what it means
2324640	2333040	for goals to emerge from systems later and you started off by saying that humans will buy
2333040	2338640	products that make them feel good and Facebook is quite an interesting example to play with
2338640	2344000	actually because Facebook is a little bit like an AI system which is a collective intelligence
2344000	2349680	and humans use Facebook but they have some idea that it might cause them harm and the thing with
2349760	2355600	population ethics is we know that our moral reasoning kind of decays over space and even more
2355600	2360880	so over time and part of the reason why time is so difficult is because it's predictive we don't
2360880	2366400	actually know what's going to happen in the future so our kind of reasoning about establishing
2366400	2370960	what the value of something is is very very faulty and I think that's one of the reasons why
2370960	2376080	these people would say that we don't really know what's good for us I do completely agree with you
2376080	2384640	though that the problem I think is humans rather than AIs on their own yes these are good points
2394400	2404480	feel free to uh offer some thoughts yes I mean it it would that's a whole separate discussion isn't
2404480	2416640	it when you discuss the limitations of what's predictable and um and how people often fail
2416640	2425120	to see what's good for them well I think maybe so you've already you've already um said that
2425120	2431920	there's no proof that we'll be safe forever right like I mean there could there could come an
2431920	2437760	existential risk you know from AI so I think my question to you is do you have sympathy for
2437760	2444160	the folks who say we need to be putting more resources into researching alignment like we need
2444160	2452720	to develop the tools um in order to allow it to be easier for people to construct AI that is aligned
2452720	2458160	for the goals and to make sure that you know that it doesn't that it doesn't have unintended
2458640	2463120	consequences like in other words there may not be a proof that we can go forever and be
2463120	2468720	safe for AI but we at least want to develop the basic mechanics that we need to safely
2469520	2477440	develop and deploy AI don't we yes and I sympathize with those who um are devoting their
2477440	2484000	lives to alignment issues and trying to build AIs aligned with humans
2484000	2496320	I view them as part of the evolution of all kinds of other ideas that come up as not only
2497040	2501520	nations compete with other nations but companies compete with other companies
2501520	2507040	and shareholders of different companies compete with shareholders of different companies and so on
2508000	2516240	and so there is such a huge set of different human goals which are not aligned with each other
2516880	2524800	that makes me doubt that you will come up with a general system that all humans can accept
2525600	2533440	simply because if you put 10 humans in a room and ask them what is good they will give you
2533440	2542880	10 different opinions however I sympathize with with this goal and it's good that people are
2542880	2550560	worried and they spend resources on solving some of these issues in the long run however
2551680	2559920	I think there is no way of stopping all kinds of AIs from having all kinds of
2559920	2568320	goals that have very little to do with humans the universe itself is built in a certain way
2569120	2570080	that apparently
2573680	2579680	derives it from very simple initial conditions to more and more complexity
2580720	2588000	and now we have reached a certain stage after 13.8 billion years of evolution and it it seems clear
2588000	2593680	that this cannot be the end of it because the universe is still young it's going to be much
2593680	2604240	older than it is now now there is this drive built in drive of the cosmos to become more complex
2604240	2609360	and it seems clear that civilization a civilization like ours is
2609520	2617680	is a stepping stone on to war it's something that is more complex and
2618480	2623360	could I touch on a couple of things here the bootloader example is kind of where I want to go
2623360	2630800	with this so a lot of the ideas of this movement can be traced back to Derek Parfit who is a
2630800	2637440	philosopher he was a moral realist so he thought there was such a thing as a moral fact and I'm
2637440	2644320	a bit of a relativist myself and actually if you trace this tree of complexity and how humans
2644320	2649200	evolve over time we might just be a stepping stone to a kind of rich diverse transhumanist
2649200	2655040	future where we become the thing over time that we're so scared of and I think the lens that
2655040	2659920	we're using here about what's right and what's wrong is kind of like I was saying before it's
2659920	2666000	a snapshot of humanity now and we kind of think of it as just this monolithic single thing
2666000	2672240	so does it really work when you project out to how we're going to evolve in the future
2674880	2682400	but first of all humankind is not a monolithic thing so many of these
2683520	2690800	arguments go like we should not do that because of that we should not do that because of that
2691600	2697840	but there is no us there is no we there are only and almost 10 billion different people
2697840	2704560	and they all have different ideas about what's good for them and so for thousands of years we had
2704560	2714000	these evolutions of ideas and of devices and philosophies competing partially competing
2714000	2721520	and partially compatible with each other which in the end led to the current values
2721520	2727680	that some people agree with and other people over there they agree with different values
2727680	2732240	nevertheless there are certain values that have become more popular than others more successful
2732880	2739360	more evolutionary with more success during the evolution of ideas
2739840	2755760	and so given this entire context of evolution of concepts and accepted ideas of what should be done
2755760	2764560	or what is worth being supported and what's not worth being supported all of this has changed a lot
2765280	2772880	if we look back 200 years the average people in the west had different ideas of what's good
2772880	2782000	than today and and this evolution of ideas is not going to stop any time soon
2786080	2792320	just a final question on this and there is a very real existential risk right now
2792400	2798960	of nuclear armageddon a real risk right now and if i were a rational person
2799680	2806720	i would be devoting all of my effort into that and other risks associated so do you think it's
2806720	2816080	a little bit weird that so much focuses on this ai x risk to me it's indeed weird now there are all
2816080	2824480	these letters coming out warning are the dangers of ai and i think some of the guys who are writing
2824480	2834880	these letters they are just seeking attention because they know that ai dystopia are attracting
2834880	2843200	more attention than documentaries about the benefits of ai in healthcare and stuff like that
2844160	2851120	but generally speaking i am much more worried about nuclear bombs than about ai weapons
2854960	2864480	a nuclear bomb a big one can wipe out 10 million people a big city within a few milliseconds without
2864480	2872880	a face recognition just like that without any ai and so in that sense it's much more harmful
2872880	2880000	than the comparatively harmless ai weapons than that we have today and that we can currently
2880000	2889680	conceive of so yes i'm much more worried about 60 year old technology that can wipe out civilization
2889680	2899920	within two hours without any ai well i guess um since we're we're not really going to worry about
2899920	2904800	ai for the moment we can uh we can turn our attention back to discussing with you uh how
2904800	2912880	we develop ai so um you know i'm really curious with with just the really the the vast you know
2912880	2918880	breadth and depth of your of your knowledge over the the history of of ai and the state of the art
2918880	2925280	i'm curious you know which current approaches you're you're most excited about and or what's on the
2925280	2931280	horizon um that you know for any of our listeners out there are thinking about um going into ai
2931280	2936560	research machine learning research you know what may be um alternatives that aren't getting enough
2936560	2941520	attention should they should they look into studying and and perhaps choosing the research
2942080	2949760	at the moment the limelight is on um language models large language models which pass the
2949760	2957120	touring tests and do all kinds of things that seemed inconceivable just a couple of years ago
2957120	2965680	at least to some of those who are now surprised but of course that is just a tiny part of
2967200	2974480	what's going to be important to develop true ai agi artificial general intelligence
2975440	2982160	um on the other hand the roots of what we need to develop true ai also
2983200	2988880	come from the previous millennium they are not new and of course what you need is an
2988880	2998640	environment to interact with and you need an an agent that can manipulate the environment and you
2998640	3007280	need a way of learning to improve the rewards that you get from this environment as you are
3007280	3015680	interacting it with it within a single lifetime so one of the important aspects of reinforcement
3015680	3021920	learning what we are now talking about is that you have only one single life you don't have
3021920	3027520	repeatable episodes like in most of traditional reinforcement learning no you have only one
3027520	3035680	single life and in the beginning you know nothing and then after 30 percent of your life is over
3035680	3042240	you know something about life and all you know is the data that you collected during these first
3042240	3050400	30 percent of your life and now there is an infinite almost infinite possibility set of
3050400	3060640	possibilities of futures and from this little short experience you have to generalize somehow
3060640	3067040	and try to select action sequences that lead to the most promising futures that you can shape
3067040	3073680	yourself through your actions now to achieve all of that you need to build a model of the world a
3073680	3080240	predictive model of the world which means that you have to be able to learn over time and to
3080400	3085360	predict the consequences of your actions so that you can use this model of the world that you are
3085360	3093680	acquiring there to plan to plan ahead and you want to do that in a way that isn't the naive way
3093680	3100000	which we had in 1990 which is millisecond by millisecond planning where you say okay now
3101680	3109680	I'm moving from A to B and the way to do it is first move that little pinky muscle a little bit
3109680	3114000	and move it a little bit more and move it a little bit more and then get up and so no you want to do
3114000	3123520	that in a high level way in a hierarchical way in a way that allows you to to focus on the important
3124160	3132480	abstract concepts for example as you are trying to go from from your home to Beijing you decompose
3132480	3139440	this whole future into a couple of sub goals you say a first important step is to go to the cap
3139520	3147520	station and get a taxi to the airport and then in the airport you will find your your plane
3147520	3152480	and then for nine hours nothing is going to happen and you exit in Beijing and have to find another
3152480	3158960	cab and so on so you you don't do millisecond by millisecond detailed planning no you have
3160080	3166640	high level planning to just reduce the computational effort and focus on the essentials of what you
3166640	3173360	want to do so that is something that most current systems don't do but for a long time we have had
3173360	3179200	systems like that and they are getting more sophisticated over time important you have a
3179200	3184560	predictive model of the world that is not just focusing on the pixels and predicting the how
3184560	3191280	does the video change as I'm moving my hand back and forth the video that I get through my camera
3191360	3201040	my eyes and so on and no higher level concepts that that reflect islands of predictability many
3201040	3206000	things are not predictable but certain abstract representations of these things are predictable
3206000	3211520	and so how can you discover these higher level concepts that you need to efficiently think
3211520	3217520	about your own future options and select those that are most promising in the single life
3217840	3225440	yeah yeah this is really interesting so we've been speaking with Carl Friston for example and he
3225440	3232320	talks about this collective intelligence where you have this multi-agent cybernetic framework
3232320	3238000	which is causally closed and one of the things we're talking about here really is not the model
3238000	3244880	itself people talk about chat gpt and it's just a model and people have configured it in arrangements
3244960	3249840	that have varying degrees of autonomy and in the future we will develop these collective
3249840	3256000	intelligences and they're not just predicting the actions and behaviors of other agents but even
3256000	3263040	the world that we're in is a computer to some extent so when you imbue agents with this kind of
3263040	3268400	creativity and autonomy that's the thing that I don't think people really understand what might
3268400	3273760	emerge from that it's related to this discussion about what kind of goals might emerge from that
3274400	3279600	do you have any intuition on what that would look like yeah let me give you just the simplest
3279600	3290080	example that we had in 1990 or 32 years ago of a system that sets itself its own goals and it
3290080	3297520	consists of two artificial neural networks and I know that Carl Friston is very interested in that
3297520	3305840	and only recently for the first time in my life I was on a paper where he was co-author
3306560	3315760	just a year ago and so back then it was really about a reinforcement learning agent and it
3317120	3323360	interacts with the world and it generates actions that change the world and then there is
3323360	3335280	another network which just is trying to predict the consequences of the actions in the environment
3335280	3340800	so the reactions of the environment to these actions and so that becomes a world model and
3340800	3348320	then what kind of goal was there which was different from traditional goals well in the
3348320	3353120	beginning this model of the world this prediction machine which is a model of the world a world
3353120	3361280	model knows nothing so it has high error as it is trying to predict the next thing as it is trying
3361280	3372640	to predict the reactions of the environment to the actions of the agent so as the second network
3372640	3378320	is trying to reduce its prediction error through gradient descent through back propagation essentially
3379040	3388240	the other one is trying to generate actions outputs that maximize the same error so basically
3388240	3395120	the goal the self-invented goal if you will of the first network is to generate an action
3396080	3402240	with whose consequences cannot yet be predicted by the other network by the model of the world
3403040	3408000	so the first network is generating outputs that surprise the second network
3409920	3416960	so suddenly you have an incentive where the first network is trying to invent actions
3416960	3426800	experiments that fool or that surprise the second network and that was called artificial curiosity
3427200	3436400	so now suddenly you have a little agent which a little bit like a baby doesn't learn by
3436400	3445120	imitating the parents no it learns by inventing its own little sub goals and it's trying to surprise
3445120	3453440	itself and have fun by playing with the toys and and observing new unpredictable things which
3453440	3459440	however become predictable over time and therefore become boring and then it has another incentive
3459440	3466560	to invent the additional experiments such that it still can surprise its model of the world
3466560	3472800	which in turn is improving and so on so artificial curiosity does that does that also
3473440	3478000	have the effect of making the network which is trying to predict does it have the effect of
3478000	3483760	making it more robust and more generalizable like almost a form of you know regularization
3483760	3491840	kind of built in in this pairing yeah you can build into that network all kinds of regularizers
3492400	3501680	an orthogonal concept which is also very important so that was just the first version that was
3501680	3509280	really in 1990 and then we have had a we had a long string of papers just on improvements of
3509280	3516320	this original concept of artificial curiosity so this old system is basically what you what you
3516320	3522320	now know as GANs Generative Adversarial Networks because the first network is generating a probability
3522320	3529680	distribution over outputs and the second network is then predicting the consequences of these outputs
3529680	3538320	in the environment and if you if the output is an image then the consequence can be either this
3538320	3544880	image is of a certain type yes or not no and then that's all that the prediction machine the world
3544880	3550160	model predicts in that simple case and you minimize the first network minimizes the same
3550960	3556960	error function that the second one maximizes so then you have basically a GAN but then you
3556960	3564240	don't have what you just mentioned yet the regularizer as a scientist what you really want to learn is
3564960	3573600	a model of the world that extracts the regularities in the environment that that
3576320	3583520	that finds predictable things which are regular in the sense that there's a short explanation there
3583520	3591920	of for example if you have falling objects in a video then they all fall in the same way they
3591920	3597040	accelerate in the same way which means it's predictable what these objects do if you see two
3597040	3604080	of the frames you can predict the third frame pretty well and the law behind that is very simple
3604880	3612080	this means that you can greatly compress the video that is coming in because you can
3613520	3618800	instead of storing all the pixels you can compute many of these pixels by just looking at two
3618800	3625040	successive frames and predicting the third frame or maybe three successive frames and predicting the
3625040	3631600	fourth frame something like that and you only have to encode the deviations from the prediction so
3631600	3637680	everything else you don't have to store separately which means you once you understand gravity you
3637760	3644960	can greatly greatly compress the video so that's what you really want to do and so the more advanced
3644960	3653280	version of artificial curiosity is about that where you have a motivation to find a disruption
3653840	3662160	of the data which is coming in of the video of the falling apples for example that is simpler than
3662160	3668480	the one that you had before so before you had the simple explanation of the data you needed
3668480	3675920	so many bits so many bits to um to describe the data and afterwards only so many and the
3675920	3682720	difference between before and after that is the reward that you get so that's the true reward
3682720	3689040	that the controller the first neural network should get in response to the improvements
3690000	3697120	of the second network which are now measured in terms of compression progress so first I needed
3697120	3703520	so many resources to encode the data but then I discovered this regularity gravity and I can
3703520	3709760	greatly compress all kinds of videos that that are reflecting the concept of gravity and certainly
3709760	3716800	I'm have a huge insight into the nature of the world and that is my true joy scientific as a
3716960	3723760	scientist my my true joy as a scientist that I want to encode in a little number which is
3723760	3729120	given as a reward to the guy who is inventing these experiments that lead to the data to the
3729120	3734320	data with the falling apples for example right well and of course this is this has been a challenge
3734320	3739360	in machine learning you know since the beginning which is okay as we add more and more parameters
3739360	3743920	how do we prevent it from learning spurious information with those parameters and instead
3743920	3749920	have it focus on parsimonious explanations on regular explanations on things that in this
3749920	3755360	universe are more likely to generalize you know to unseen examples and so I think my question to
3755360	3761280	you is does this setup that you describe is it a form of that and or what is the state of the art
3761280	3767600	you know these days for helping to push or nudge neural networks towards learning parsimonious
3768240	3774160	models for the world rather than highly detailed spurious susceptible to you know
3774160	3778720	high frequency anomalies and adversarial examples and all this sort of thing
3780400	3784640	yes what is the current state of the art in the regularizing
3786320	3792880	descriptors of the data such as neural networks such that you get simple explanations of the data
3793760	3800560	such that you get short programs that compute the data in other words such that the description of
3800560	3812160	the data is a short program that computes the much larger raw data and and how close can we
3812720	3818640	get to the limits which are given through this concept concept of algorithmic information
3818640	3825040	or comagor complexity comagor complexity of any data is the length of the shortest program
3825760	3831440	on some general computer that computes it since in our field the general computers are
3831440	3838400	recurrent neural networks we want to find a simple recurrent network that computes all this data
3841680	3847360	and given one computation of the data we want to find an even simpler one so we want to have this
3847360	3855280	idea of compression progress and here I have to say although we have lots of regularizers
3855280	3862080	invented throughout the past few decades there's nothing that is really convincing
3863040	3872080	I think one of the very important missing things is to make that work in a way that is
3872080	3879200	truly convincing that is as convincing as chat gpt is today in the much more limited domain of
3880400	3888960	generating text from previously observed texts and stuff a very old idea of I think the 1980s
3889920	3897760	was to have weight decay in a neural network which basically is the idea that all the weights
3897760	3905600	should have an incentive to become close to zero such that you can prune them
3906640	3914560	and so people built in regularizer that just punished weights for being large or being very
3914560	3922240	negative but that didn't work really well and something better was flat minimum search that was
3922240	3929840	1998 and first Arthur my brilliant student set book write that back then roughly the same time
3929840	3940880	when the LSTM paper came out and and there the idea is if you have if you plot the weights of
3940880	3951680	a neural network on the x-axis and you plot the error on the y-axis then given the weights you
3951680	3960480	have high or low error and then there is for example a sharp error function which has a sharp
3960480	3968000	minimum which which goes like that can you see my finger so here here is the x-axis here's the
3968000	3974480	y-axis here's the error and the error for a certain weight is really really low but then
3974480	3981920	for a different weight in the environment in the vicinity it's high again which would be very
3981920	3988800	different from a flat minimum which would be like this so here's the error and it's going down
3988800	3995280	and for many many ways it is low the error and then it goes up again so if you are a very sharp
3995280	4002320	well versus a very broad well yes a sharp well versus a broad well now if you are in a sharp well
4002320	4008720	you have to specify the weights with a lot with with high precision so you have to spend many bits
4008720	4015600	of information on encoding the weights of this network as opposed to a large to a flat minimum
4015600	4023360	where it doesn't matter if you you know perturb the weights because the error remains low
4024240	4031040	in this flat minimum so what you really want to find is is a network that has low complexity in
4031040	4037040	the sense that you can describe the good network so those with low error with very few bits of
4037040	4045280	information and suddenly if you maximize or if you minimize that flat minimum second order
4045280	4055440	error function then suddenly you have a preference for networks that that for example do this
4055760	4063920	you you have a hidden unit and the outgoing weights they have certain values but if you
4063920	4068640	give a very negative weight to the hidden unit then it doesn't matter what all these outgoing
4068640	4077760	weights do and flat minimum minimum search likes to find weight matrices like that where one single
4077760	4082800	weight can eliminate many others which you suddenly don't need any longer such that the
4083600	4090800	description complexity of the whole thing is much lower than in the beginning when you when you
4090800	4097840	just had a random initialization of all these weights so that is much more general than weight
4097840	4102720	decay because weight decay doesn't like these strong weights it wants to remove them but sometimes
4102720	4107760	it's really good to have a very negative weight coming to a hidden unit which is switched off
4107760	4116080	through that weight such that all the outgoing connections are meaningless but it's not um what
4116080	4124000	you what you it's very nice it's a very nice principle but it's not as general as finding the
4124000	4130400	shortest program on a university computer that computes the weight matrix that is solving your
4130400	4136160	problem to the extent how do you think we're how do you think we're gonna get to that point
4136160	4140960	how do you think uh what approaches are going to lead us to finding things that approach
4140960	4147600	comical of complexity yeah and i think that path has again a lot to do with meta learning and as
4147600	4155520	um a system is able to run its own learning algorithm on the network itself it can um suddenly
4155600	4157680	speak about the um
4160080	4168320	algorithms in form of weight matrices and it can discuss concepts such as the complexity of a
4168320	4178000	weight matrix and then you can conduct a search um in this space of networks that generates
4178720	4187360	weight matrices and then you suddenly are in the game so suddenly you are playing the right game
4187360	4195440	and then it's more a question of how to um choose an initial learning algorithm such as
4195440	4201120	gradient descent to come up with something that computes the simple solutions which you really
4201120	4211520	want to see in the end very recent papers on that on on aspects of that came out just a while ago
4211520	4224160	with my students vincent herman and louise kirch and um and francesco faccio and my poster kazuki
4224720	4231360	and robert joydash also um and also imann olschlag and there the idea is really to
4232080	4239760	have one network that computes an experiment and the experiment itself is the weight matrix
4239760	4248000	of a recurrent network so there is a generator of an experiment which can be anything that
4248080	4257600	describes a computational interaction with an environment so a program so that experiment
4257600	4263360	is then executed in the real world there's a prediction machine that predicts the outcome
4263360	4273440	of the experiment before the algorithm is executed and so then there's um just a yes or no question
4273440	4284800	either the following outcome will occur or not either it will occur or not but now the entire
4284800	4290160	setup is such that you don't have predictions all the time about every single pixel no you just have
4290160	4295440	something which is very abstract and which is just about whether a certain unit of the recurrent
4295440	4302480	network is going to be on or off at the end of the experiment and this internal on and off unit
4302560	4311120	can represent any computational question any questions that you can ask at all and now the
4311120	4316560	the task of the experiment generator which is another network which generates a recurrent
4317600	4323600	network weight matrix which represents the experiment the task of this experiment generator
4323600	4330480	is to again come up with something that surprises the um the prediction machine which looks at the
4330480	4337840	experiment and says yeah it's going to work or not and uh and suddenly you are again in this
4337840	4346400	old game uh except that now you have this world of abstractions where the abstractions can be
4346400	4354000	anything that is computable interesting really cool really cool could we spend the last 10 minutes
4354000	4360320	or so just talking about some of the the current ai landscape so in particular the capabilities of
4360480	4368800	GPT-4 and the moat building thing and and the the power that companies like uh google and open ai
4368800	4375840	have and um also the potential for open source so maybe we'll just start with the you know the
4375840	4382080	very current capabilities of GPT-4 are you impressed with it what do you think i'm impressed in the
4382080	4390960	sense that um i like the outcomes that you get there and um it wasn't obvious a couple of years
4390960	4398640	ago that it would become so good uh on the other hand of course and it's not yet this full AGI thing
4399440	4411440	and it is not really close to um to justifying those fears that some uh researchers sometimes
4411440	4426320	and now um document and um in letters and public letters and so on so to me it's a little bit
4430000	4438000	like a visa view because for for many decades i have um had discussions like that and people said
4438960	4443760	that you are crazy when i said that within my lifetime i want to build something that is smarter
4443760	4451280	than myself um and now suddenly in recent years um some of the guys who said it's never going to
4451280	4456480	happen suddenly they just look at chat gbt and they think oh now we are really close to AGI and
4457040	4465360	whatever uh so i i don't share these um extreme um
4471360	4478640	i'm less impressed than some of those guys let me say that right uh the open source movement
4478640	4482800	that you mentioned you you want to ask a specific specific question about that right
4482960	4491360	well yeah there was that famous google memo that got leaked and when the waits for loma from
4491360	4498560	facebook went out within about two or three weeks um it was a valing pretty similar to chat gbt
4498560	4504400	you know with this um laura fine tuning and the open source community has just exploded you know
4504400	4509760	you can now run it on your laptop and there is some question whether there is a significant
4509760	4514880	gap between the capability you know is is it just a parlor trick is it really as good potentially
4514880	4519520	or could it be as good as some of the next best models from open ai but i guess the question is
4519520	4529120	do you think that we need open ai to to have the best models no of course not um no i'm very
4529120	4537280	convinced of the open source movement and have um supported that some people say the open source
4537840	4542640	movement is maybe six or eight months behind the large companies that are now
4544640	4554720	coming out with these models and i think the best way of making sure that there won't be dominance
4554720	4562160	through some large company is to support the open source movement because how can a large company
4562160	4568720	compete against all these brilliant phd students around the world who are so
4569520	4574640	motivated to you know within a few days create something that is a little bit better than what
4574640	4585920	the last guy has um um put out there on github and whatever so i'm i'm very convinced that this
4585920	4595840	open source movement is going to make sure that there won't be a huge mode for a long time
4596880	4600160	i'm reading between the lines here but i would guess you would be opposed to
4601040	4606560	legislation like the eu is considering where you know very tight restrictions on
4606560	4612480	generative models you know onerous onerous kind of uh approval processes and things like that
4612480	4618480	because that's going to have this chilling effect on on open source innovation and the little guys
4618480	4627920	wouldn't it yes i have signed letters um which which support the open source movement and whenever
4627920	4638160	i get a chance to um maybe influence some you um politicians then i'm trying to contribute to
4638160	4644640	making sure that they don't don't shoot themselves in the foot by by by killing
4644640	4649440	killing innovation through the open source movement so you certainly want to avoid that
4652240	4659600	there are lots of different open source movements around the world so if one big entity fails to
4660880	4667360	support open source or even makes it harder for open source there will still be lots of other
4667360	4675040	entities which um won't follow follow and so no matter what's going to happen on the political
4675040	4685520	level i think open source is not going away i guess just in closing you've been in this game
4685520	4691920	for decades now and what is i know it's a bit of a strange question to ask but what's your fondest
4691920	4697920	memory in your career my fondest memory oh it's usually when i discover something that i think
4699760	4706960	nobody has seen before but that is that happens very rarely because most of the things you think
4706960	4712560	are well somebody else has done before um but yeah so
4719040	4727440	yeah um what usually happens is um you and and this has happened many times not many times but
4728080	4733200	quite a few times in my career since the 80s as a scientist who publishes stuff
4733440	4742960	but suddenly you think oh that is the solution to all these problems and now i really figured out
4742960	4749600	a way of building this universal system which um learns how to improve itself and learns the way
4750160	4755440	to improve the way it improves itself and so on and now we are done and now all is that's
4755440	4762320	necessary is to scale it up and it's going to solve everything and then um you think a little
4762320	4767280	bit longer about it and maybe you have a couple of publications but then it turns out something
4767280	4773440	is missing something important is missing and and actually it's not that great and actually
4774160	4781920	you have to think hard to add something important to it which then for a brief moment looks like the
4783440	4791120	greatest thing since sliced bread and um and then you get excited again but then suddenly
4791120	4797520	you realize oh it's still not finished something important is missing and so it goes back and
4797520	4802800	forth like that i think that's the life of a scientist the greatest joys are those moments
4802800	4809760	where you have an insight where suddenly things fall into place such that along the lines of what
4809760	4817920	we discussed before the description length of some solution to a problem suddenly shrinks because
4817920	4827440	two puzzle pieces they suddenly match and and become one or become one in the sense that
4827440	4832640	they fit each other such that suddenly you have the shared line between the two
4832640	4837520	puzzle pieces one is negative and the other one is positive and certainly the whole thing is
4838880	4845360	much more compressible than the sum of the things separately so these these things that's
4845360	4851360	what's driving um scientists like myself i guess
4854720	4859200	wonderful um professor you again schmidhuber it's been an absolute honor thank you so much
4859200	4863360	for coming on the show today thank you it was such a pleasure talking to you
