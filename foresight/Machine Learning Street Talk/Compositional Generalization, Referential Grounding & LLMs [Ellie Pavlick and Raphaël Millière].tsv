start	end	text
0	2000	Welcome to MLS T.
2000	6800	Today we are extremely excited to have two distinguished guests join us.
6800	9000	The first guest is Ellie Pavlik.
9000	13600	She is an assistant professor of computer science at Brown University
13600	16000	and a research scientist at Google AI.
16000	21800	Her work focuses on building better computational models of natural language semantics and pragmatics,
21800	26400	aiming to help computers understand language the way humans do.
26400	33000	Our second guest, the legendary Raphael Millier, is the 2020 Robert A. Burt Presidential Scholar
33000	37200	in Science and Neuroscience in the Center for Science and Society
37200	40800	and a lecturer in the Philosophy Department at Columbia University.
40800	45800	Raphael completed his D-Phil in Philosophy from the University of Oxford,
45800	52000	where his work centered on self-consciousness and his main interests lie in the philosophy of artificial intelligence,
52000	55000	cognitive science and the mind.
55000	57400	Now, this is an interesting experiment for us.
57400	62800	We've decided to get these two heavy weights, just one-on-one having a conversation with each other
62800	65400	and we're hosting it here on MLS T.
65400	68000	They spoke about compositionality and grounding,
68000	73000	compositional generalization benchmarks, mechanistic understanding in language models,
73000	76600	variable binding in transformers, language and vision models,
76600	78600	compositional behavior in humans,
78600	82200	compositional reasoning and negation in language models,
82200	85200	variable binding in reinforcement learning and transformers,
85200	88600	the difference between instruction tuning and RLHF
88600	93200	and the benefits of RLHF, referential grounding and language models.
93200	99400	And yeah, the Chomsky skepticism, of course, you know, our friend Stephen Piantodosi's paper,
99400	104600	inductive biases in language learning, language models in different languages
104600	109400	and indeed the future of academic work in language models.
109600	113800	Now, the audio from Raphael in particular wasn't as good as it could be.
113800	116600	I've done my very best to process it and improve it.
116600	121800	Just to help folks follow along, I've kind of generated some subtitles.
121800	124600	The subtitles on Google are absolutely rubbish
124600	128800	and speech recognition technologies come along so far in the last few years.
128800	132800	So, I've generated some better subtitles using another service
132800	134600	and I've superimposed it on the top.
134600	137000	I've also superimposed some descriptive titles on the top
137000	139600	just to help you folks follow along at home.
139600	144600	So, anyway, without any further ado, I give you Ellie Pavlik and Raphael Billier.
144600	145200	Enjoy.
145200	146000	Hi, Ellie.
146000	147800	I know Raphael. Hi, how's it going?
147800	150000	I guess you talked to Tim before,
150000	154400	so maybe you have a kind of more of a context of the previous conversation that started this one.
154400	159200	So, I'll let you decide where we're beginning, what the first topic is.
159200	164600	Yes, so we had a chat back a few weeks or half a year ago
164600	167800	when I came on the podcast and we had to get it short and you thought,
167800	172600	maybe if I come back home, we should do it as a discussion.
172600	175600	And I think you had to chat with you.
175600	178200	So, Tim thought that maybe some of the topics we could discuss
178200	182000	would be equal positionality and brown links with both versus your dads.
182000	183600	Seems natural, yeah.
183600	184200	Yeah.
184200	189800	So, I don't know whether we should try to disagree more than actually do
189800	192000	because I think we were aligned with a lot of the topics,
192000	193600	but I'm sure there are some topics.
193600	201600	Yeah, but I mean, I also feel like for both of them, I know what I currently think,
201600	205800	but I also am pretty prepared to just have to renege in a couple of years
205800	206800	and be like, I was wrong.
206800	209000	So, I feel like we can see both sides.
209000	212400	So, we can definitely simulate some disagree or we don't have to disagree,
212400	215800	but we can argue both sides of whichever issue.
215800	217000	I don't know where do you want to start?
217000	220600	Do you want to talk grounding or do you want to talk compositionality?
220600	224000	I think I have more immediate questions with respect to compositionality.
224000	227800	So, one thing I was wondering is how do you see the progress
227800	230400	on compositional generalization benchmarks?
230400	235000	So, just for the listener of yours,
235000	238000	it's really hard to assess whether large language models
238600	245600	on huge corpora actually acquire the capacity to generalize compositionally properly
245600	249600	because you can never really know what's in the training data with your models.
249600	254600	Before, you can never really know whether they're the memorized structures and so on.
254600	262600	So, what strategies to use, synthetic datasets, such that you try them on a test set
262600	265000	and on a trans set and then when you test them,
265000	270000	the only way that you can achieve this goal is generalizing perfectly.
270000	276000	And so, there has been some, the initial results from some of these datasets and benchmarks,
276000	282000	like Scars, Cards and others, were a little bit mixed with LSTMs and formers
282000	287000	and lately they have been a part of the steady improvements
287000	292000	past due to tweaks in the architecture and I remember having a discussion with Tal,
292000	297000	Linsen, when he organized this composition of the workshop that he was speaking of.
297000	302000	And one of the, one of the contentious points is whether we could call these tweaks
302000	304000	or whether these are significant changes.
304000	309000	And that's always the crux of the debate, I think, including also with things like
309000	316000	Paul Smolensky, Paul Smolensky's approach that has an explicit transfer product of presentations is
316000	321000	how much of a hand engineer tweak you make in the architecture
321000	327000	to solve this composition generalization problem and how much do we need.
327000	330000	So, I was just wondering where you step on that.
330000	337000	That's a super interesting, I mean, so, yeah, so on the composition generalization test,
337000	341000	I mean, I probably have like an unsatisfying middle ground opinion.
341000	342000	I'm curious where your thoughts are.
342000	348000	So I think, I think we're both pretty interested in the kind of mechanistic stuff right now.
348000	354000	So I guess for the audience, this is this idea that like this idea of trying to kind of understand
354000	357000	what the models are doing kind of under the hood.
357000	360000	So when we think about the, I guess, compositional generalization tasks,
360000	365000	it's like we have some inputs, what are the training, like what is the training model it gets
365000	368000	and then what are the outputs that it produces.
368000	374000	And that's really the data that we're basing our claim about whether it's compositional or not on.
374000	380000	And the kind of mechanistic or the other approaches to try to characterize
380000	383000	what actually is the process it used to get from the inputs to the outputs.
383000	387000	I really like Chris Ola, who I think coined the term mechanistic,
387000	390000	uses the phrase of kind of trying to understand the source code of the model.
390000	397000	So it's like you have, you want that kind of something like a kind of human understandable description
397000	401000	of the algorithm that it's running under the hood.
401000	406000	And so like I've just, I've been super hung up on that.
406000	413000	Like I think all of my projects, all of like what my students are working on are some flavor of that
413000	417000	because to me, I think the reason it's so, I think it's interesting,
417000	423000	but it also just feels like the questions about things like compositionality are almost stuck right now
423000	425000	without that level of description.
425000	428000	So if we're just looking at what are the inputs and what are the outputs,
428000	432000	I just feel like we're, it's just going around in circles with people like,
432000	434000	like we're not really making progress on the issue.
434000	439000	There's kind of people who are inclined to agree and inclined to disagree.
439000	447000	And so from my perspective, like, okay, so if we have the model that's doing this kind of, I guess,
447000	450000	quasi generalization, it's like succeeding on some cases,
450000	454000	not perfectly compositionally generalizing in the kind of really abstract case
454000	457000	that those data sets tend to be going for.
457000	463000	But it's doing something in between and we're trying to figure out whether that counts as compositional or not.
463000	468000	It seems like that just hinges on what it's actually doing under the hood and how it's doing it.
468000	476000	So I guess I'm like, I'm like basically like neutral on or not paying attention
476000	478000	to compositional generalization data sets right now.
478000	481000	And I recognize like a very fair criticism.
481000	484000	And I feel like when I've talked to people like tall, probably have this word.
484000	486000	It's like you can't just sidestep the issue.
486000	489000	It feels like being overly generous to the neural networks, right?
489000	490000	It's like they're not doing that well.
490000	492000	And then you like change the game a little bit, right?
492000	494000	You're like, oh, well, that's not even the metric we care about.
494000	496000	That's not really what I would see as the goal.
496000	500000	It's just like in the immediate term, it seems like first we want to characterize what's happening under the hood.
500000	504000	And then we can come back to those data sets and understand them much more in depth.
504000	509000	And then when we see how they're solving it or not solving it, we can like,
509000	515000	it gives us a much more concrete thing to analyze and try to ask whether that counts as compositional or whether that's at all human like,
515000	517000	if that's what we care about.
517000	524000	Like it, like, I guess to me it feels like a dead end if we're not allowed to comment on the procedure that happened in between.
524000	529000	And right now we can't comment on the procedure that happened in between inputs and outputs because we just don't know what's happening there.
529000	534000	So I don't know, I guess that's my current take on compositional generalization data sets is I'm like,
534000	536000	now is not the right time for them.
536000	541000	We'll come back to them later, which I recognize seems like a dodge, but it's not meant as a dodge.
541000	544000	It's like, basically, we'll come back to this later.
544000	548000	Yeah, but I'm curious what you think about them.
548000	550000	Yeah, I mean, I'm very sympathetic to that view.
550000	559000	And as you know, I'm super interesting to mechanistic that visibility just to looking at positionality.
559000	569000	And I guess in the background, there is lurks this debate about whether humans themselves have something like a perfectly compositional language of thoughts or something else.
569000	570000	Right.
570000	571000	Absolutely.
572000	582000	And so perhaps, you know, we might, we might learn some things about computations implementing a human cognition by looking at this imperfect completion of systems.
582000	596000	Absolutely. I mean, I think that's, that would be really my, we just like so right now, and this is a gross oversimplification of where the two viewpoints are, but like, the two really concrete like
596000	608000	kind of options on the table for like what a system can be is like this pure symbolic language of thought and that the language of thought it would be something like humans in their heads have something like a Python program language,
608000	620000	like a perfect kind of formal system for reasoning over symbols in this compositional way, or it's like this loose, or like these associations, these idiomatic kind of just things back together.
620000	631000	And like, I think most people would assume or something in between and there's not a really good proposal for what the in between is right like there's all kinds of ways of being in between those two things.
631000	638000	And so I feel like whatever we find in the neural networks gives us some kind of concrete proposal like here's an example of an in between.
638000	639000	It's not the language of thought thing.
639000	644000	They're often also obviously not just giant lookup tables right they're doing something more than that.
644000	650000	So whatever they're doing in between it's at least like a candidate for what could be happening right that's pretty exciting.
650000	658000	And it might not be the right one but at least it's something because I haven't seen like a really satisfying candidate for what the in between is right.
658000	662000	Like a lot of the debate still seems to kind of put these two strawmen up against each other.
663000	664000	Yeah, I agree.
664000	682000	And I think some of you with some of the people who are pushing back against this kind of really symbolic thing with the thoughts architecture that Paul Smolensky, the self-faith that you need to build into your connections model, some more explicit compositional structure with these
682000	693000	kinds of products, but these these vectors, rodent field of vectors that are never formal, but you can combine with transfer part of operations and there are other vector symbolic architectures like this that do it like that.
693000	698000	And my qualm is always with with this and I have to talk about this.
698000	711000	I think, you know, he's open to the possibility that perhaps transformers, you're handling compositionality in the way transformers handle these might turn out to be enough perhaps but my calm is that if you if you build, you know, if you kind of hard code.
712000	727000	Into your network architecture, or you do some future engineering to the input vectors being your first and all that seems just to be ad hoc to me right so you need to specify what are the rules what are the fillers and that's that seems like an
727000	734000	Inclosable model for how it works unless you want to say that they are in its academic concepts and we just do a lot.
734000	735000	I mean, a lot of people do.
735000	736000	Yeah.
736000	737000	Right.
737000	746000	That's like a claim that I think a large fraction of cognitive science is very happy to say is the case is that there's an inventory of innate atomic concepts.
746000	747000	Right.
747000	750000	But but yes, I agree.
750000	767000	I think maybe it's the computer scientists got or something something about that is unsatisfying or at least you want a story of where those came from, which still seems like you somehow it needs to emerge or come from data or come from some kind of pressure other than just we got lucky and they were there.
767000	769000	I changed the right inventory of concepts.
769000	779000	I mean, there's some daylight between the core knowledge knew that, you know, there are some some basic concepts and the four orient view that all atomic concepts must be in it because right.
779000	781000	Yes, which is born with this.
781000	782000	That's part of it.
782000	783000	And they're on the ball.
783000	784000	Yeah.
784000	785000	Right.
785000	795000	And it seems that if you if you want to handle all compositionality with these ad hoc world and filler vectors or something along these lines, then.
796000	814000	It seems that you're going in that direction. And what I'm excited about which forms formers is that it looks like they can do something that in my mind, if you look at the kind of secretive research researcher looks quite like variable binding by reading and writing information to sub spaces of the main embedding
814000	817000	space and using that as a virtual content principle memory.
817000	821000	But it's fuzzy right it's not just implementing a classical symbolic.
821000	842000	That's always the charge that I think that the classes are never against the connections is that oh if you're if your model is working well and if it's if it's a good model of human behavior and then presumably is just implementing classical symbolic architecture and it's right, right, right, right, right.
842000	863000	I mean, almost I, I'm teaching a class this spring on language processing of humans and machines and we were talking about this question of implementing a classical architecture and it's actually quite hard to tell even in the historic debates on this like what what
863000	877000	people are even claiming about this right like there's like simultaneously claims that like oh it'll like sure it can implement it but also nothing it ever does whatever count as an implementation like I just can't even tell what the what actually the consensus is.
877000	887000	But I agree I think we'll get something that the ideal is that when we look into the transformers will find something that preserves the really necessary pieces of it.
887000	902000	But is different enough to be interesting I mean, I don't know I guess it in some ways if what we did if what we ended up finding was something that was identical to the classical architecture that would be like, I think a huge win for classical architecture.
902000	909000	People right that the only way these transformers a were able to solve it was by learning to implement the thing they said that you would need all along.
909000	920000	But I think it's unlikely that we would find that right like I don't really know exactly how that would even work on so we'll get something like this fuzzy version, which, which I think would just be fascinating.
920000	936000	I mean, I'm curious so the variable binding stuff so I'm generally take the position that the transformers these other logical networks will be able to implement like these core kind of symbolic operations will be able to replicate this kind of behavior.
937000	943000	And then when we kind of dig under the hood will be able to find these clever implementations of these things.
943000	950000	But variable binding is one of these things where we've actually really struggled to find good evidence of it.
950000	971000	And I guess particularly so in language models it almost seems like you have to say it's there by like by Fiat because of some of these the ability of the models to do this compositional generalization stuff like they, they, I think Tom McCoy had a really cool paper where
971000	983000	I was showing that they could generate some like syntactic structures that were unattested in training. So for something like that you're like well I guess to do that you would have to have something like a abstract filler and roll binding.
983000	989000	But especially what we've played around the language and vision models and maybe it's just that clip kind of sucks.
990000	1001000	I guess clip is the one of the pre trained link image and text models that kind of is trained to map images to to text captions.
1001000	1011000	But we've had several projects of trying to show, I guess clip and then the image generation models based on top of it like Dolly and state things like that.
1012000	1027000	Like we just can't get any evidence that it's doing anything clever or abstracting away from the from the structure so that's where we'll look for stuff like a red cube in front of a blue cube and then blue cube in front of a red cube or things like the, I guess the famous on Twitter like
1028000	1041000	astronaut riding a horse a horse riding an astronaut. And like, even with really controlled cases and doing it in large days that like we just can't get it to do anything that gives us any data to point to to be like look it's doing okay.
1041000	1044000	But maybe it's just a clip problem maybe just sex.
1044000	1060000	I think it's, I think it's a clip problem so that's that's, I wanted to talk about this because you have this really interesting paper about it and I think we can't generalize any finding about vision language models based on contrastive learning clip to say, language models train or to
1060000	1079000	say, there's this recent paper from Stanford configurably first author is like which which confirms my suspicions basically the ideas that click treats ends up treating text like just by the bags of words, because in order to just to fulfill the contrastive learning objectives of bringing
1080000	1103000	the captions and measures that go together and further away in the space the ones that don't go together. You don't need to actually induce much about syntax, right, just keywords is sufficient and and generally also in the captions themselves you don't have a ton of information about, say, like their, their relative positioning of different objects or how many of, you know, it's not like you say, they are key forks and one and one knife or just say
1104000	1118000	something in the caption. Right, right, right. And if you look at all the models like party from Google, other image nourishment models that is a pre trained as a text and currently is a pre thin language model, they actually do way better at things like much better.
1118000	1122000	Yeah, yeah, yeah. Yeah. Yeah.
1122000	1141000	Yeah. And I think that surprised me in trying to do so we did the red cube blue cube kind of stuff because that's like super abstract and it's easy to do. But one of my other students Charlie Lovering is working on a project with some of the image generation models to do to try to kind of more systematically look at the, you know, horse riding an astronaut kind of example.
1141000	1161000	And one thing that struck me with it is just how like how not uniformly distributed the world is right like it's just like finding examples of relations, like where you want to have like argument one relation argument to that can actually exist in both directions.
1162000	1177000	And that's a thing that a human could visualize. We like, it's really, really hard to do like we're restricted to a really simple set of relations because we like spit out a whole ton of things and tried flipping the order and you're like, yeah, this isn't a thing I could actually imagine or would expect a human to be able to
1178000	1193000	to realize in any kind of way. And so that I think getting at kind of revisiting our assumptions about how compositional humans are I think that's like kind of relevant data to have is like how often are we actually forced to combine completely
1193000	1208000	novel elements in a purely abstract way without anything that we can relate it to. Like what we think we're our kind of working theory for what the model is doing for these kinds of things is like if you ask for a horse riding an astronaut, it's like finding the most similar thing it's seen.
1208000	1218000	Right. It's like, well, here's an example of like, you know, a teddy bear riding a puppy dog or something and then morphs it into a horse ride, like some other type of thing.
1218000	1225000	And it's like, that's not actually an absurd model to assume might be underlying some kind of compositional behavior in humans too.
1225000	1239000	Yeah, it might be of the this might be for with respect to language models that friends all these kinds of effects on reasoning where you know you test them on syllogism solving the Western selection tasks you get you get these effects that are similar.
1240000	1249000	You know, also fun in humans where if you're feeling the story of the task with plausible details that could apply to realize that they did much better.
1249000	1250000	Right.
1250000	1253000	You just use this like farfetched, you know, abstracted novels.
1253000	1255000	Right.
1255000	1268000	Right. Yeah, and that's that's been shown like I know this is, I don't remember the authors but there's pretty classic study about the kind of logical syllogisms right like if you ask people to reason about these just like, you know, famous modus ponens or whatever.
1268000	1284000	When you ask them like a not a like people suck. Yeah, they don't know how to do it but if you put it in realistic real world scenario like you're at a party you can have ice cream and cake or whatever then people do totally fine and I think it.
1284000	1293000	It wouldn't be surprising that we might have these that we would have evolved to be able to reason about realistic scenes and not abstract ones.
1293000	1310000	Yeah, and the, the, I think the skill to be able to reason over the most, the more abstract examples of including with images and those just in a possible way is something probably require through training and people of, you know, who have two children,
1310000	1325000	for example, are probably familiar with that you kind of need to some prompt engineering as a word to, to get them to actually, you know, draft certain concepts or how to, you know, teaching logic, even to the grads.
1325000	1339000	Yeah, you get a lot of resistance from this content effects and to be prime institutions in the right way. And I would I wonder whether you know with the astronauts running a horse examples, whether it would be interesting to try that with with small children,
1339000	1344000	and see whether, you know, whether they actually do well at that test.
1344000	1354000	Yeah. Yeah, I mean, I would have to assume they would right so that's where like my intuitions are strongly like but humans are quite compositional like you would.
1354000	1359000	You would have to assume that they would know how to.
1359000	1371000	They wouldn't just draw a horse riding, or if you asked for a horse, they wouldn't just draw astronaut riding a horse, right. They might like giggle and be like that's silly courses don't write something right but.
1371000	1388000	Yeah, I mean, I guess so I guess some counter examples to both of our intuitions so that there's some of this variable binding stuff but with the image models and I'm kind of like I'm willing to give the models a pass at least in the immediate term while we figure out what's going on because
1388000	1397000	and like yeah I think some of this perfectly abstract compositionality might be like a tall order and not something we would have to do.
1397000	1414000	But the other thing that's always been weird to me so like I'm channeling Roman Feynman my colleague here who's like much more language of thought tradition and you know he'll say that you know of course humans are like you know better at like he's aware of all the data that people
1414000	1428000	aren't as good at logical reasoning in some settings but they can do this other sense and things like that but it's like that doesn't undermine that humans have logical capacities like we use negation freely all the time like we don't really struggle with that.
1428000	1442000	But the language models actually still kind of suck at negation like it's pretty easy to just write something in a slightly odd way and get them to ignore that you negated something I think even fairly recently I guess I haven't tried this in the past month so
1442000	1459000	maybe it's fixed but like I asked something like asked GPT or chat GPT I think for like a recipe that like uses tofu and nutritional yeast but isn't vegetarian or something like that and it just spits out some vegetarian recipes or something
1459000	1475000	like you know you sugar and lemon but not a dessert and it's like have you tried lemon bars and stuff so it's like it just kind of ignores it and that's weird to me that seems like for a language model doing a language modeling task like that's relevant like
1475000	1495000	this isn't like a super trick out of distribution thing so that's kind of like I guess a thorn that like I feel like that's where I say like I have a bit of a caveat where I wouldn't be surprised if a couple years from now I have to be like yeah I was wrong the models are not at all like human like or something
1495000	1508000	I'm hoping that's not the case but these kind of data points are like yeah that's it's frustrating how bad they are with negation or other kind of basic things like that.
1508000	1527000	Yeah I mean it's always a moving target of course because people maybe said GPT-4 can handle that for example but I agree it's definitely very unsatisfactory to still his failures as some compositing of the problems.
1527000	1543000	Yeah like we have some I guess so on one of these projects with Roman in this undergrad we're working with Alyssa we and this is just super frustrating data for me although actually it is getting better with the bigger models so maybe it is.
1543000	1566000	But for GPT-3 it was like we have this very basic task that they had run on humans so you say something like it gives a little scenario and then you say you know it's like some scenario about scientists running experiments on rats and it's like the scientists saw that none of the rats liked the food or something.
1566000	1579000	And then it's like now that they knew that some of the rats liked the food and they like did human reading time and saw that people showed a slow down and were surprised by the word some in that case because it's a blatant contradiction to what was just said.
1580000	1604000	And when we use like GPT-3 and predicts surprise there's like no surprise whatsoever no suggestion of any up being at all bothered by the word some in that context which is weird because that's just a straight up language modeling task it's just what is the probability of this word in context which the human data was very clearly like it's low and the model was like it's fine.
1604000	1617000	And then but then we had we tried with GPT-4 and then the numbers it looked better but it was kind of messy because we can't get perplexities of GPT-4 so you had to ask it to fill in the blank is a little bit different so it's hard to read.
1617000	1620000	API was just the Chesapeake version.
1620000	1625000	The with GPT-3 we had the API.
1625000	1629000	Because now GPT-4 is available by the API as well.
1629000	1632000	Yeah but it doesn't we need to check.
1632000	1633000	Okay.
1633000	1637000	Yeah we need to check last we checked we couldn't get perplexities out like we had the API but.
1637000	1638000	I see.
1638000	1642000	Yeah but yeah it's one of these weird things.
1642000	1654000	Yeah and so like maybe GPT-4 is better but it wasn't as clean of us comparison to the humans and and it was even for a model like GPT-3 like it was surprising that it was so bad at that and like I don't know.
1654000	1665000	So I feel like there's a few of these data points that are like this story isn't a slam dunk like there's some of these things that really should be easy for a model with basic structure.
1665000	1666000	You know.
1666000	1673000	Yeah it's surprising to me because there are you know this that is been telling others right that's even way smaller models.
1673000	1684000	Can exhibit the right behavior in terms of surprise all when you look at things like subject development agreement field of gap dependencies you know I don't stress on these and.
1684000	1691000	You know the right range of some tactics and they exhibit the same patterns of humans in terms of being surprised when the.
1691000	1692000	Right.
1692000	1693000	Subject things like that.
1693000	1696000	Even if you're very not structured because of the structures to.
1696000	1697000	Right.
1697000	1700000	So why is negation right.
1700000	1711000	Totally it seems much much simpler right like it's not like if anything I feel like negation plus these quantifier terms like you could just enumerate a table and say these things.
1711000	1719000	Can go together and can't go together like it's I mean there's like a little more semantic cushion around it you have to know who the who's being modified or whatever but like.
1719000	1724000	I'm quite sure models can do all of that so I was very surprised it might be just that it's a really.
1725000	1733000	Infrequent thing in the training data in the input but then you have this poverty of the stimulus argument that you have to account for right is like.
1733000	1748000	I mean maybe it is that this kind of entailment relation is just not frequent on the Internet but it is in kids input or I you know I don't know or maybe it is that the models are just not the right models for this task I don't want to believe that though I think there's
1748000	1750000	I think there's must be some other reason.
1750000	1764000	Yeah, I mean there was this paper by Will Merrill that shows that entailment semantics can be induced by an idealized ideal language model on synthetic data up to a certain sentence length.
1765000	1772000	In real in real world scenarios there's only given the size of the three.
1772000	1779000	I think perfect in terms of that it's going to be induced for me to this formal truth to send this is the four words.
1779000	1782000	But if you want perfect absolutely you know perfect.
1782000	1783000	Right, right, right.
1783000	1789000	You exactly know whether or not sometimes in the next intelligent right which I don't know that I believe in entailment.
1790000	1796000	It's like the logic stuff you're like okay we can come up with these like toy domains in which we all agree about entailment.
1796000	1816000	But like during my PhD I did a lot of work on entailment and just trying to collect entailment judgments on humans is a nightmare right like they do not behave to the point that you have like that's what I feel like that was like a switching point for me where I was like okay maybe maybe I should accept that we have to trust them.
1816000	1817000	But entailment is entailed.
1817000	1822000	And as you idealize Gressian speakers as well humans are not.
1822000	1823000	Yeah, no, no.
1823000	1831000	But it's it's intriguing because it does suggest that I mean you can learn something about entailment semantics from distributional information.
1831000	1835000	Even you know entailments in a loser sense like in the non perfect right sense.
1835000	1840000	So again, why is negation so hard is is.
1840000	1845000	Yeah, yeah, I guess I haven't seen a really good study on just.
1847000	1857000	The distribution of negation in the like in a models training corpus like how is it used in what context because I don't.
1857000	1860000	Um.
1860000	1871000	Yeah, like it might just be that it's actually it's just not distributed the way we kind of think it's distributed it just functions very differently in written text in general.
1871000	1880000	Yeah, like I do think that perhaps kids would learn it like kids get a very different input of negation than what I would imagine is on the internet.
1880000	1881000	Right.
1881000	1886000	Like I in academic writing I use negation only in the most convoluted ways like.
1886000	1898000	Like it with like these triple negations that are right you know like while it is not unreasonable to assume that such is not the case right like that's the way I would use negation I wouldn't say like that's not a dog.
1898000	1900000	Yeah, no one writes that.
1900000	1907000	Yeah, and also it's interesting because in terms of the text that's actually generated language models.
1907000	1919000	I don't think you know even with you three in my experience you see negation error is like I can even remember when when they generate text they use negation properly it's more when the policy comes somehow.
1919000	1924000	Sometimes they know of negation as if they try to maximize the relevance of every word.
1924000	1929000	So if you mentioned if you say I want you to give me recipes but not in the paprika.
1929000	1933000	They see the word paprika and then there's like I need to maximize the you know.
1933000	1936000	Yeah, yeah, but right.
1936000	1939000	And it might be.
1939000	1951000	I so when I was playing around with the recipes and stuff it did seem to change a lot I mean it was anecdotal but on on the wording so like if you marked it a lot more right like.
1952000	1965000	Like if you said something like a recipe with these things but include me it does fine right so if you say not vegetarian it gets confused also if you'd say like and not vegetarian versus but not vegetarian and like where you put like.
1965000	1977000	Like where you whether you front loaded or back loaded certain information it made a difference and so you could imagine there's these distributional signals where it's like when people are saying don't do this thing they market in a few ways right they don't just like.
1977000	1983000	Slip in the negation but otherwise have the sentence read exactly like it would in the positive case.
1983000	1991000	Like this probably a reason we have but as a conjunction and not and right is just to like help emphasize so that people don't miss that negation piece of it.
1991000	1992000	Right.
1992000	2002000	So like you could imagine something like that that it's like the model has very little incentive to emphasize the negation unless there's other signals that you really are right.
2002000	2006000	So you wouldn't like be deathly allergic to Paprika and then just like slip it in.
2006000	2012000	Give me a recipe for chicken Paprikash without Paprika or something and then just like yeah.
2012000	2013000	Yeah I know that.
2013000	2020000	So so so one other thing that this article was writing about and viable binding that I wanted to just keep up there's this.
2020000	2025000	Preprints that I don't think I've got published.
2025000	2032000	And on our size that's an alleges that you know they looked at viable binding interest formers.
2032000	2043000	And I can't exactly remember the methods, but the alleged that they found that transformers can't really do viable binding unless it's by using the output as an external memory.
2043000	2052000	And it gives to some of the discussion that we had at the conference that I co-organized with Dave Cheneze and a few signals.
2052000	2062000	Nick Shea made a somewhat similar claim that you know the kinds of what he calls non content specific reasoning that transformers can do.
2063000	2067000	He's always propped up by reasoning on the output.
2067000	2080000	So using using the generated work, like saying China China China from saying you just reason step by step and use use the steps in the generator steps to crutch to solve problems.
2080000	2086000	And maybe there is also this paper is very you've you've written structural and sub routine.
2087000	2101000	Yeah, I haven't had chance to read that one but but I just wanted to ask you because I was striking to me that you can do some things zero shots with kind of language models that seem to fly in the face of that kind of thing.
2101000	2108000	So for example, would you be for you can you can tell it's behave like a python shell.
2109000	2125000	And then define a function with a bunch of variables and then a little bit later in the interaction just, you know, call that function for specific values and and say, you know, and because it's been having like a python shell it just has to give the answer zero shot it's not able to do some
2126000	2139000	and it can do this pretty reliably and there's some family modes but the fact that you can do it that's all just me that you can has to be able to internally right make variables.
2139000	2145000	Absolutely. Yeah, I yeah I love that example that you gave in the workshop.
2145000	2153000	So at this philosophy of deep learning workshop because you had that like you had it behave like a python shell and do this Fibonacci sequence.
2154000	2162000	The language model like we just ask the language model to predict or to tell you the, you know, seven hundredth Fibonacci number it messed it up right.
2162000	2181000	Yeah, that was like super interesting and yeah I guess the thing we like fight with with these models is that they're, you know, they're like we do know that they have that they've trained over the whole Internet and so they've learned to like kind of sub spaces right and they're like drawing from these different
2181000	2192000	domains and it's makes it really hard to interact with them because they can always from anthropomorphizing but they can always pull this like oh sorry I didn't understand the question game right like I thought I was being my red
2192000	2203000	itself but it turns out you wanted me to be my New York time self or something like and it could always pull that so it could be like oh yeah no I know how but you have to ask me to act like a python shell because people on the Internet don't know the Fibonacci number.
2204000	2220000	And this is a little bit of like for the for the critics of large language models this is a frustrating game to play because like like this is always a move that the proponents of language models can make is like oh you didn't ask it right like it knows how but it
2221000	2235000	which is really why I feel like the mechanistic stuff is so important because if we know more about it then this becomes like less of it feels less like you're sidestepping the criticism so we can just say what actually happened but I do think that right now that is the case right like it could be that they
2235000	2241000	have the ability to find variables and do this stuff but just they
2242000	2254000	have deemed that that's not the right way to solve the task in the average case or in a typical case like maybe negation is not important for the typical thing but if it if they're acting as a python shell then of course negation is important right.
2254000	2264000	Yeah, and I'm really interested in the role of our life chef fun tuning in that context so we have put money for feedback because it looks like it's it's vast including the zero shot capacity of the model.
2265000	2283000	And I think there's some evidence that it ends up you know, condensing the probability mass of, you know the distribution of over over the next word for to much more narrow range, such that you know, just a few words will have a high probability for certain for some problems,
2283000	2292000	because it's essentially enabling the model to to latch on to the right task right away right instead of having to do this. I mean still, as I said, it's still resolved.
2292000	2306000	Oh, sorry, I didn't get your question right. But if you ask a question zero shot to valley lad, jpd3 without the life chef, it will weigh more often just have no clue what you're really asking it and you have to like
2306000	2311000	totally. Yeah, yeah. Yeah, yeah, I like to use I like that.
2312000	2328000	Open AI recently re released their like the old school jpd3 because you can see what a big difference even the instruction tuning makes like my favorite is if you ask something like write a report on the war in Ukraine or something.
2329000	2344000	It'll like follow it up as though it's like in an email and it'll be like, please include additional like details on budgets and blah, blah, blah. Please get it to me by Monday regards or something like rather than writing a report it like gives you a list of other tasks to do.
2345000	2355000	But yeah, so there's clearly like, like the RLHF clearly improves your shower at least in even instruction tuning something I'm, I don't know, maybe you've thought about this more.
2355000	2373000	I know we talked about this like briefly before like just I guess I'm, I'm still getting my head around what RLHF is or in particular how it's like, if it is profoundly different from other types of training because I feel like in some cases like I've heard a lot
2373000	2383000	of people kind of crediting RLHF as like possibly like, oh, we have all these problems with, with our language models, but maybe RLHF alleviates them.
2384000	2398000	And I can never tell if there's like a genuine feeling that it does or if it's kind of a this is a new kid on the block and we're not sure so to kind of hedge or future proof whatever claims we're making we add this caveat that's like maybe RLHF alleviates it.
2399000	2410000	Or maybe it's that like RL in general RL is associated with like grounded and maybe more cognitive like possible learning in certain domains and so people hear the RL part and think that it's somehow better.
2410000	2422000	But I'm not quite sure if RLHF is like deeply different or if you could induce the exact same behavior through something more like an instruction tuning setup.
2423000	2436000	Like I genuinely don't know which of those things is going on and I haven't heard like I know there are people trying to come up with a fine tuning variant that otherwise behaves the same as RLHF because RLHF is unstable and people don't like it.
2437000	2448000	Brown has a lot of people who work on RL and they look at RLHF and they're like this isn't even really RL because it's weird and it just seems like you kind of like folded more language models on top of each other.
2449000	2460000	So, so yeah I just I don't know how I feel about it and my gut is to be like no it's not special it's like the same stuff in a new package but that's based on absolutely nothing except vibes so I don't know it.
2461000	2470000	Yeah, I mean I do show my intuition with respect to the difference between RLHF and instruction tuning because it seems you know if you look at what people did with the Lamar model from the time I.
2470000	2481000	There's a generated a bunch of of you know input out compares with GP4 or something to create what is it? Vecunia or a pack out.
2482000	2485000	Yeah, so yeah, so that this camelid animals.
2486000	2489000	So they yeah so they generated essentially the instruction tuning.
2490000	2508000	Transat from models that have benefited from RLHF and then you kind of get the benefits of RLHF for free right or for $300 and so it seems that actually this works pretty well right so right so maybe I don't know I mean it could be.
2509000	2522000	I'm just pure speculation but like it that could be the case and RLHF is special it could be that RLHF allows you to optimize for a function that you can't directly optimize for with next word prediction but if you have an RLHF.
2522000	2528000	RLHF trained model you can then distill that function like the know and then directly something like that.
2529000	2540000	I want to say that RLHF doesn't even optimize for a different or special or function that like you could just take the data you get from RLHF and just.
2542000	2545000	Use it differently and like fine tune on it or something.
2546000	2557000	I mean that must not be the case so I guess I'm really purely speculating but that I think that's the intuition that I'm just like deeply wanting right now is like is there something special going on or is it just.
2558000	2564000	We found a different way of getting somewhere like we kind of stumbled upon it and we can actually get the same effect.
2565000	2569000	And it's I feel like it matters it matters from an engineer standpoint but it also matters because I just hear RLHF.
2570000	2575000	Actually I would like to talk to you about that because you like mentioned it in your grounding paper or like I've heard it from a lot of people.
2576000	2591000	Like at Federico had a paper on kind of disassociating language and thought and had all these different criticisms of large language models and the things they can't do and then at the end it's like but maybe RLHF solves all of this and I was like whoa that's like a huge.
2591000	2597000	But I don't know and so and I and I've seen similar things elsewhere and so I think to me I'm like.
2598000	2601000	I'm just deeply curious if that's the case like I think I'm lacking that intuition.
2602000	2606000	Yeah, no absolutely I mean that's first of all let's see best transitions through the topic.
2607000	2608000	Yeah, so nicely done.
2609000	2610000	Yeah.
2611000	2622000	So, just to follow from the audience, the, so if we're going to call them we wrote this paper and essentially we said well people use the notion of grounding in different ways in the literature.
2623000	2627000	And then of this goes back to how not simple grounding problem from the 1990.
2628000	2639000	90 and in which he argues that symbolic AI models lack the capacity to intrinsically many forms of patients and out this.
2641000	2652000	Because you know that the semantic interpretation of their representations is provided externally by the programmers so CHRT and you for example it's capable school program that could manipulate locks in the box world.
2653000	2663000	If you can link can connect its linguistics presentations to virtual objects but that connections provided as I thought can externally by physically programmers.
2664000	2670000	And the problem is how do we get models that actually he should be grounded.
2671000	2682000	Presentations of me because he guidance where here the key notion of grounding is referential so how do we get representations that are actually making reference to the the objects of the other worlds that they are about.
2683000	2697000	And that program has created your marriage with recent connections wills and so on and I need them a lot of with that support back and so we, we were trying to pick about different notions of grounding said the referential national
2697000	2713000	grounding is the most important one and then say well in light of that, can we say that mangos will also in a text only achieve some form of referential grounding, and we do mention our other chef because we've basically the argument is going from the perhaps the most plausible and convincing
2713000	2718000	for the most people to something that's a bit more speculative so with our own chef.
2718000	2725000	So the premise and she is that we, we've next for prediction that's a that's an intro linguistic function right so you could eat in the next word this is trained.
2726000	2742000	And that doesn't seem to give you quite the, at least intuitively, the right kind of, of normativity for for representations of the world the reference of music items, such that you could have the possibility of misrepresenting something.
2743000	2750000	So, in other words, you don't have anything to give you the right kind of world involving function is just, you know, whether you write a role about the next talking.
2751000	2768000	And our chef, on the other hand, because you get this explicit feedback from humans, including feedback about truthfulness honesty, the three h's of our chef is helpfulness, homelessness and honesty right so that is one of these which is a novelive component that's about that's an epistemic in all to
2769000	2777000	whether you're, if you're, you know, answering questions about capitals of the world, whether you're rather long about the state of the world when you say that Paris is the capital of France.
2778000	2795000	So that seems to be well involving in the right way, but then, you know, we had this conversation a few weeks back in, and as you rightly pointed out, it seems that, you know, you could get that the right kind of for the public function without this explicit feedback from humans.
2796000	2812000	And we do actually think that that's the case. So, and going down from slightly less consensual claim, we think that you can get in context learning and in context learning you have a fusion, when you have a fuchsia swamp where you have several examples of successful
2812000	2830000	problems that you can ask, if it's a future prompt on worldly facts, you also get this implicit feedback in the prompt about what's right of log in the world and if you're seeing projects learning as, you know, inducing a function, optimizing for a function that's not just
2830000	2844000	the next for prediction, then you can also see that as providing a well developed function, but then you can go even further and say, well, so why not look at pre training and I've told the awesome context windows in there that will include discussions of what the facts where, in order to do the next
2844000	2855000	for prediction, which is the proximate function, the model might have to induce a more complicated ultimate function that's about the real world. So I think that's where your intuition is.
2855000	2870000	Yeah, right. Yeah, I think exactly like an, and this is this. Yeah, I love, I mean, I love the paper and I like this idea of the world evolving function because I think that is the, that's kind of the intuition that we all have.
2870000	2889000	Right, like there does seem to be something that's not like I think most people's gut instinct is like next word prediction over text isn't enough, right. And then the challenge is figuring out like what like where is the line like what is the what actually is the problem with it right and that's
2889000	2904000	and that's where I'm kind of stuck because I think my like in my heart I kind of don't want language model next word prediction to count like I don't want that to be the whole thing.
2904000	2916000	And then I just like and a lot of our own work though we seem to be arguing the opposite like I'm not quite sure where I fall on this issue and so it's like yeah you want something like this like this world of our function or something about the learning mechanism right the learning mechanism doesn't
2916000	2937000	feel like enough you're like no obviously people do more right then predict the thing that happens next but then like formalizing what that difference is I just I haven't been able to convince myself like I haven't been able to come up with a thing that I'm like yeah that's the thing I totally buy it like sometimes it gets on this issue of like goals and I guess this
2937000	2955000	actually came up a bit in the philosophy of deep learning debate like you know and when I talked to a lot of cognitive scientists and people who don't like the next word patient language while it's like you know people have goals but that feels like a weird because you can say that the language
2955000	2976000	has a goal and the goal is to predict the next word and like so now are we just like making a judgment about what goals count as good ones and not good ones there's a similar yet like this kind of having the person give the feedback who is sufficiently tied to the world and now it seems
2976000	2996000	like we've like outsourced the question of whether the model is grounded to something about the trainer and the goals of which maybe actually is fine maybe philosophically that's fine you're like to fall along this causal chain you have to have kind of inherited from somebody who's also on the causal chain so I mean I don't know because like yeah if you had like somebody giving
2996000	3017000	RLHF feedback who is like intentionally misleading right or just confused and misinformed like any of these types of things like I don't know how that muddies the analysis of whether the model is now world involved or grounded
3017000	3018000	right
3018000	3019000	referentially grounded
3019000	3033000	yeah totally agree on the last point I think if the whole argument hinges also on the assumption that the crowd workers actually know how to run the outputs or do it right to a large extent
3033000	3034000	right
3034000	3046000	and there's this and I guess this is a classic philosophical qualm too right like cause we can like humans could be totally wrong about stuff right so like science progresses right so right now we might be seeing
3046000	3064000	like oh the I forgot one of the classic examples the like a lot of things about like viruses and diseases right like we had theories about what this different diseases were back in the day and then we learned stuff and it's like an entirely new thing now right
3064000	3075000	but you wouldn't say that the people historically had like an ungrounded meaningless notion of that thing right so we so like you need to account for the fact that we could be teaching the model something that is
3075000	3086000	ultimately wrong because we haven't learned that in fact you know that I don't I can't yeah but I can't think of a good example
3086000	3100000	the thing is theories of representations of representation will account for the fact that you can misrepresent things right so just because you have a you know a linguistic like lexical concept that is
3100000	3115000	not refraction be grounded doesn't mean that this precludes the possibility I have you know something going on right misrepresenting right so like thinking
3116000	3133000	yeah and I guess this is kind of it's like so if the model thinks that the capital of France is Berlin quote thinks that and produces that as output I guess we need to differentiate between the case where it's just wrong and ungrounded and
3133000	3147000	learning loose associations versus it was quote mistaken right but like that because I could not know the capital of the city and like that it's true that I don't have quite the right concept of that thing but it's not the same as me being a parrot that's
3147000	3158000	just spitting out it's right and so I don't think we quite quite right now I don't know how we're drawing that line within the models of them just producing wrong stuff versus
3158000	3174000	yeah I mean certainly with I think perhaps with things like early Jeff you can draw that line potentially but with retraining I mean and I show you intuition that you know since it seems you know we don't really want to do the line at early
3174000	3185000	Jeff it just it just seems like the low hunting fruit because that's what's going to come in last people but that's already you know it's already shouldn't become a racial statement to say like not going to do it all trained and
3185000	3200000	text only can achieve the first one but then we would want to go further like you and but then it's tricky so so here is the interesting that I have and I've been maybe overly impressed by these recent papers that look at income
3200000	3215000	learning and show that it's this wonderful Google and others that show the same thing essentially that they functionally equivalent to right fine tuning with gradient descent, even though you're not actually addressing the weights and what's
3215000	3228000	tuning on what what's what's the what's the during gradient descent virtually on is not next for prediction, it's whatever function is connected to the specified by the future, that the future prompt.
3229000	3242000	And so that really is the thing that can be okay so so so you could get the work involving stuff from that and if you get if you can get it at different time within complex learning, then presumably you could also get it at pre training time.
3242000	3248000	If you say you have a window context window that's a bunch of capital questions that was a good front.
3249000	3260000	Yeah, you have some future like stuff in the training data that presumably would allow the model when it's not at the beginning when it's totally random but when it starts learning everything last at some point.
3260000	3263000	Right, you know, you might not get that.
3263000	3267000	Yeah, yeah, and I think this is where.
3267000	3284000	Like I agree with that and I agree with kind of calling out RLHF as possibly the a different point because it's like it's a good thing to ground to as or not ground to to overloading the word ground it's good like data point right to use and then trying to peel away like what is the minimal
3284000	3292000	thing like what about RLHF gives it that and but like that's exactly that logic you just laid out is what I.
3292000	3300000	I think I accept right now or feel somewhat forced to accept right based on this because you're like yeah that seems correct.
3300000	3314000	But then I have to go back to putting on the hat of somebody somebody who does not believe these language models because and I think it is important to point out that when we're talking about language models being grounded or having meaning it's not the same as saying they are.
3315000	3324000	conscious and intelligent right like but sometimes but that's that's this kind of elephant in the room where it's like where are we going next and so I think when people are looking at.
3324000	3336000	These language models and they don't want to acknowledge that they can be referentially grounded because that seems like a step along the way to claiming that they are like human level cognition and all of these other ways.
3337000	3346000	It's so deeply unsatisfying it's like wait no like you've missed the point like now we're saying that just having a few examples during pre training of someone listing off countries who wrote that down.
3347000	3359000	In you know good faith just listing the capitals of countries that's enough and now the language model counts even though it's just doing next word prediction whether like that seems insane right and so I kind of like I feel like I just go between these two positions of being like.
3360000	3368000	Like right now based on everything I feel kind of forced to accept like no the language models I would say they're referentially grounded I can't find a good case to make for why they're not.
3368000	3372000	And at the same time I'm like do do we really want to say that that seems bizarre right like.
3372000	3383000	But when we get our for our hand forced by this the kind of all of nothing's thinking that you see sometimes in these debates where it's like.
3383000	3388000	At the high level it's like either the stochastic power so it's like human like.
3388000	3391000	You know AGI with with.
3391000	3395000	Understanding consciousness and for just whatever you would be like yeah yeah yeah.
3395000	3398000	And there's this huge you know.
3398000	3404000	Space in between possibilities that we could explore where you look at different capacities in the case by case based and say that.
3404000	3407000	And then within each capacity that grounding.
3407000	3409000	It's also spectrum right it's not like.
3409000	3414000	You know we want to say you get a few examples of question answers about capitals and then.
3414000	3418000	That's it you're you're you have human like referential grinding on everything.
3418000	3420000	You check that box.
3420000	3423000	You're good yeah right so.
3423000	3432000	So presumably you know you could say well you know that that gets you you know your foot on the lighter of a personal grounding in a tiny tiny tiny domain and.
3432000	3435000	And that's still you know 30 feet.
3435000	3441000	Interesting yeah right that said I mean there's something that I think is really interesting from your work on.
3441000	3449000	You know isomorphisms between say in color terms and the color space and stuff you've been working on with grounding that.
3449000	3456000	Maybe suggests that sometimes when you get a toe hold on grounding in a specific domain.
3456000	3463000	You could leverage the isomorphism between language in the world to get a little bit more for free.
3463000	3469000	Right right right right right you definitely could.
3469000	3476000	Imagine that and I guess this is the project that you and I start working together is the kind of what kind of power of analogy.
3476000	3481000	Reasoning do these models have because yeah you could imagine something like this like.
3481000	3487000	With the toe hold and really strong reasoning by analogy capability you could get a lot.
3487000	3493000	Out of that but I also agree I think there's just this big middle ground like it's not like you could.
3494000	3495000	Like.
3495000	3497000	Learning that the.
3497000	3501000	The meaning of the capital cities or something shouldn't.
3501000	3505000	Be enough that now by reasoning by analogy you can infer like.
3505000	3508000	Everything the whole world like that that seems.
3508000	3515000	I would if somebody could spell out a mechanism via which that would happen sure but I can't imagine what that would be.
3515000	3518000	Yeah yeah.
3519000	3523000	I was I was going to make a complete.
3523000	3527000	Detour but I remembered we didn't talk about the Chomsky stuff when we're talking about.
3527000	3530000	Yeah so I wanted to hear your thoughts on that.
3530000	3535000	The thoughts but I would like to hear your thoughts on yeah so.
3535000	3539000	So just for context but Chomsky.
3539000	3543000	Steve has been.
3543000	3548000	Writing about statistical models of language learning for decades but he recently.
3548000	3559000	Let on the records saying as you would expect that he's totally skeptical you can anything whatsoever from language models about human language acquisition human condition in general.
3559000	3566000	And he co wrote this op-ed in New York Times making that claim.
3566000	3572000	Even though he told me he would have he signed on it but would have made a point of all the differently.
3572000	3580000	Because it goes in different directions I think you know I have some process about how the op-ed is believe no different directions I think his point is more.
3580000	3583000	His corporate is simpler is just.
3583000	3588000	You know like with bottles.
3588000	3594000	And then he makes analogy with with a theory of physics that would say anything goes and.
3594000	3600000	And that wouldn't be a good theory of physics and he's he's he's very impressed by this particular paper that.
3600000	3604000	From Bowers colleagues I think that that's a big step.
3604000	3609000	Learning supposedly possible languages and showing that they have some visual scan.
3609000	3613000	Learn such languages and things that's that's that's it all months on and forth.
3613000	3615000	Yeah.
3615000	3617000	And then there's tip that I don't see.
3617000	3618000	I wrote this paper.
3618000	3626000	That's saying, you know, taking the completely opposite stance saying language models refused the whole program and programming linguistics.
3626000	3630000	And I, I mean, so I started between.
3630000	3637000	As often in these discussions that's the running thread but I think that there's always room for positions in between the two extremes.
3637000	3638000	Yes.
3638000	3646000	So, I think there was this interesting, interesting discussion that talent in others.
3646000	3656000	In a trader where there are two versions of the cover to the stimulus arguments that is used to justify the claim that there is something like the next year universal grandma in humans.
3656000	3663000	So there's a strong version that jumps key himself did defend in the past and that's people still defend neighboring very popular grandma.
3663000	3669000	You know, generally grandma textbooks, which is syntax is just unlearnable from data.
3669000	3680000	Period. Like no more of the devil will get you to learn to structure because having discovered cursive infinity productive system is not something you can learn from detail.
3680000	3681000	Interesting.
3681000	3688000	So, so I think there is a good argument and see it makes a good argument for that being somewhat refuted by language models.
3688000	3694000	And they discover in so far as they can, you know, there is a lot of work showing that it can induce that structure.
3694000	3701000	Whether it's, you know, no, no, it's, it's the, it's the verb agreements to get dependencies.
3701000	3705000	You can even decodes proxies from nations of baritone.
3705000	3708000	So, so that's a strong graph.
3708000	3724000	And then there is the more in my mind more reasonable developmental version, which is, well, with children can do this constraint generalizations on certain syntactic phenomena and if they get dependencies from few examples from this improvisate most.
3724000	3732000	And it seems like this is hard to come for if they don't have, you know, strong enough in that device to make this deductive inferences.
3732000	3740000	And one way that you could have this kind of device would be to have a nice grammatical stripes.
3740000	3746000	And there I think the evidence with language models is way less clear and a way more tentative.
3746000	3758000	There, there is a little bit of, so the problem being that the large number of the giant ones that actually give you for that and from order of orders of magnitude more words than children's children do.
3758000	3765000	There are a few papers in one from from Jengen colleagues from 231 and there is more recent one for the office.
3765000	3775000	Training models with between 100 million and sorry, 10 million and 100 billion words, which is what a child would get by the age of six, eight, 10 years old.
3776000	3785000	And they show that actually you can get a lot of paper with a title like this. It was like even when trained on a normal amount of data language model still replicate.
3785000	3800000	Yeah. Yeah. And so, and so what one of these papers shows that what you get when you train them, you keep training them on X for words is mostly he mostly pertains to things like common sense knowledge about the world and all that semantic stuff, but not so much the syntax stuff.
3800000	3802000	Maybe negation would be an exception.
3802000	3806000	But yeah, that's what I said. Yeah.
3806000	3814000	And there is this this also this project and I'm very excited to see the results of the baby a length challenge from Alex. Yeah, yeah.
3814000	3823000	And so this is the same kind of challenge they use this the credit is copies that's copies of child directed speech that's actually recorded from real real life.
3823000	3835000	And so this is the same kind of child's the kind of word stick here and sentence stick here. And there's a hard version of the challenge where you have to train language model on only 10 million words, I think at an easier version of 100 million words.
3835000	3841000	And people will submit their contributions and I think the results will be more supportive of the store.
3841000	3856000	But yeah, I think, I think that kind of projects could, in my opinion, constrain type of this in theoretical linguistics and maybe go against the week version of the progressives to us and say, look, like maybe you don't need
3856000	3861000	to think that's syntactic in a knowledge.
3861000	3866000	But even if that's the case, I think something, you know,
3866000	3879000	that doesn't mean that there is no amount of of in its structure or inductive bias whatsoever because after all, you know, language models are not typically rising they have inductive bias is just almost having the two biases.
3879000	3888000	And even if these are not language specific, I think that's a kitty front before Chemsky would say we say we need this language specific knowledge.
3888000	3904000	There is a sense in which the moderates empiricists meets halfway with the moderates nativists to say there is some like there are some inductive biases that not that they're like general that don't in general and then language specific and we can be happy with that so.
3904000	3923000	Right. Yeah, that's a really, yeah, that's a great point and I think I agree with that like I um yeah I mean I also just in general probably just a general of sciences like the putting something up as a choice between two extremes is like always detrimental right
3923000	3929000	like and sometimes they can prevent people from working on the problem because they're like afraid of pissing off half of the field.
3929000	3949000	But yeah, I so I had a, I like this I like one thing I was thinking about or was thought about with reading Steve's responses, again the kind of just him on the mechanistic stuff like to me, it seems like the answer kind of hinges I guess this is a different version of
3949000	3960000	question but it seems like a lot of it hinges on what the model is doing internally to process these to process the language like whether it represents something that looks like Trump skin syntax or not.
3961000	3979000	But I guess this is kind of a different question so there's there's the kind of what you were describing which I think is definitely like one of the important ways to be think about this problem from kind of a linguistics perspective which is like take the blank slate language model and say how much data doesn't need to learn because it's definitely a
3979000	3992000	super significant finding if you could replicate if like a transformer randomly initialized transformer trained on a realistic amount of child directed speech learn syntax like that's a huge finding right.
3992000	3998000	There's also this kind of like take the giant pre trained language model.
3999000	4017000	trained on a way more data than human has but then you can kind of use that as the starting point like that's the innate structure that a human has right, which could be a ton of language specific and syntactic structure, and then that's kind of what.
4018000	4033000	And then from that point on then it's quite efficient it's the problem so that's kind of what my first time I read Steve's papers like well we can't really say because we don't know whether the model has solved the language problem by learning some like nice looking
4033000	4041000	formal syntactic abstract syntactic structure under the hood or not although I think a lot of data suggests that it has.
4041000	4058000	But I guess it's it's kind of hard question to answer because you can't do something like you can't then take that as the starting point of a language learner because it's already learned a lot of language like you would want some other way of like inducing that structure, but then stripping out the ability to actually generate
4059000	4068000	like you would want to like find a way of like pre training the model getting all the innate syntactic structure but then somehow re initializing a large part of it so that.
4069000	4077000	Child would have to relearn and then use that as to ask whether what the language while has learned is something akin to the innate structure that Chomsky would have wanted I also.
4078000	4086000	I mean I guess something you could do because you mentioned like learning on learnable languages is like try to transfer a pre trained language model to one of these.
4087000	4106000	Unlearnable languages because I like I always think of the I've been thinking of the pre trained language models as kind of the like that's whatever is in doubt by evolution or something that's the starting yeah yeah right and so but yeah I recognize that it's kind of impossible to use as a model of a.
4107000	4117000	Pre linguistic child because it has way too much language to do any interesting experiments on its language learning but you could ask whether you could transfer to one of these unlearnable languages and whether it basically.
4118000	4126000	Needs to unlearn and relearn as basically like if it can transfer too well to one of these other languages that I guess would be a data point against.
4127000	4137000	That structure being the kind of structure that kids have born in whereas if it like their models are very powerful but what it might need to do is basically unlearn everything and then relearn the new language from scratch.
4138000	4148000	And then that would suggest that it's like not yeah it's not actually very learnable to the models once they've been pre trained with this strong bias for other structures.
4149000	4156000	Yeah although you need to show that humans that this this made of languages are actually unknowable.
4157000	4164000	And that's how to do because you can try with adults and then it's like an L2 problem like second language but if you want to try with the first language.
4165000	4169000	If that's how you said it I'm the you know if you think of pre training as like evolution.
4170000	4180000	And then you could say well whatever is going to be pre training could be the innate structure of Chomsky and yeah so then you would need to actually you know it's an impossible experiment to do you need to be very unethical.
4181000	4183000	You have to have a child grow up in this.
4184000	4189000	So because like what is the data that those languages unlearnable is that no language has the features that this language has.
4190000	4197000	I think so they have this weird yeah I haven't done a deep dive and I think it's very controversial so a lot of people were convinced by that paper.
4197000	4203000	Chomsky really latched onto that as you know hard evidence that we can learn nothing from language models.
4204000	4205000	Right.
4206000	4221000	Yeah I'm speaking out of my domain but I so I would defer to someone who studies this but my I see why it's controversial right because there's like perhaps lots of explanations for wide languages share common features other than those are the only possible features for the human mind to.
4222000	4223000	Yeah.
4224000	4225000	Yeah.
4225000	4226000	Interesting.
4226000	4227000	Yeah.
4227000	4244000	And also you know there is some evidence I think you know that some people who are not very changing the mistakes always point to evidence across languages where it seems that some of the features of generative grammar seem to work better for some dominant languages than others that some languages.
4245000	4246000	Right.
4246000	4255000	And then the standard move sometimes is to just add some kind of ethnic thing to the German grammar to compensate for that but then it becomes a little less elegant and.
4256000	4257000	Yeah.
4257000	4258000	You know.
4258000	4259000	Yeah.
4259000	4260000	Yeah.
4260000	4261000	Yeah.
4261000	4262000	Yeah.
4262000	4263000	Yeah.
4263000	4265000	Yeah I mean I think it's kind of an un.
4266000	4285000	Transiential point but I guess there isn't as much work on these large language models in other languages right like I mean they're there are multi-lingual models and stuff but that's definitely some data points that would be good to know like we talk about transformers having some they do have inductive biases and things like I don't know how well they work for.
4286000	4298000	Other languages I've never seen a good controlled experiment because just the quantity and quality of data we have in different languages is like so varied that you just can't say anything when the models are better or worse.
4299000	4311000	But that definitely seems like from the perspective of asking what do these models mean for the Chomsky and program or universal grammar and stuff like it seems like we need data on how they work in.
4312000	4313000	Cross-lingually.
4313000	4314000	Yeah.
4314000	4324000	There is this work by Stephanie Chan goes from deep mind that we said that how the data distributional properties that probably sort of the trading data influences.
4325000	4333000	I think in contact starting in transformers but so they look at that we've made up better sense that different distributional distributional which is.
4333000	4341000	And they find that the typical distributional properties of languages that seem to actually be there is work stream that is you know there's a zip.
4342000	4354000	Zips will apply across basically every human language and it seems that there is something in common in the distributional properties of all human languages that transformers can watch on.
4355000	4356000	Yeah.
4356000	4357000	Yeah.
4358000	4359000	Interesting.
4359000	4360000	Interesting.
4363000	4364000	Yeah.
4364000	4365000	Yeah.
4365000	4374000	Luckily there's still lots of stuff to think about like sometimes it's like oh all the problems are being solved these models keep coming on doing all the tests and it's like oh god no there's so many.
4375000	4377000	Like I think we still have careers ahead of us that's good.
4378000	4379000	Yeah.
4379000	4391000	In the way because you know people freak out about the fact that all the big problems now are being tried by industry and before it's taken away from academia but I think all of the a lot of interesting problems actually things you can only do on small models anyway.
4391000	4392000	Like we can.
4392000	4393000	Oh yeah totally.
4393000	4395000	And so in a way there is a nice division of labor.
4396000	4409000	Yeah I actually yeah I was like I work at Google 20% and I've said a couple times like all my most interesting projects are the ones at Brown because like I actually think the small toy data things like that's where I feel like I'm learning stuff and like it makes sense.
4410000	4412000	That's how things work like it's so exciting.
4412000	4417000	So yeah I feel good about academic work on this stuff right now.
4417000	4421000	With the synoptimistic notes to end on.
4421000	4423000	Yeah thanks a lot.
4423000	4425000	This is fun.
4425000	4426000	This is a lot of fun.
4426000	4431000	Yeah we killed I guess I have my timer going almost 75 minutes.
4431000	4432000	It's pretty impressive.
4432000	4433000	Perfect.
4434000	4435000	Hi Tim.
4436000	4437000	Hi Tim.
