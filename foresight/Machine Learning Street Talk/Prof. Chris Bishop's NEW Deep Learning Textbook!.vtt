WEBVTT

00:00.000 --> 00:04.400
Today we have the privilege of speaking with Professor Chris Bishop,

00:04.400 --> 00:07.920
a luminary in the field of artificial intelligence

00:07.920 --> 00:11.760
and machine learning. Chris is a technical fellow

00:11.760 --> 00:15.600
and director at Microsoft Research, AI for Science,

00:15.600 --> 00:20.160
in Cambridge. He's also honorary professor of computer science

00:20.160 --> 00:24.560
at the University of Edinburgh and fellow of Darwin College,

00:24.560 --> 00:28.800
Cambridge. Hi, nice to meet you Tim. This is the new book on

00:28.800 --> 00:32.960
Deep Learning Foundations and Concepts published with my son Hugh.

00:32.960 --> 00:36.560
What proper have you got? Ethanol. I don't know if I'll use it but

00:36.560 --> 00:39.360
we're going to talk about invariance. That's wonderful, that's wonderful.

00:39.360 --> 00:43.520
Because you ought to get a little bit techy at some point. Oh yeah, our audience loves that.

00:43.520 --> 00:47.600
In 2004 he was elected fellow of the Royal Academy of Engineering,

00:47.600 --> 00:52.320
in 2007 he was elected fellow of the Royal Society of Edinburgh

00:52.320 --> 00:57.600
and in 2017 he was elected fellow of the Royal Society.

00:57.600 --> 01:01.200
Chris was a founding member of the UK AI Council

01:01.200 --> 01:05.680
and in 2019 he was appointed to the Prime Minister's Council

01:05.680 --> 01:12.080
for Science and Technology. At Microsoft Research, Chris oversees a global portfolio

01:12.080 --> 01:16.880
of industrial research and development with a strong focus on machine learning

01:16.880 --> 01:21.840
and the natural sciences. Chris obtained a BA in physics from Oxford

01:21.840 --> 01:25.840
and a PhD in theoretical physics from the University of Edinburgh

01:25.840 --> 01:30.720
with a thesis on quantum field theory. Chris's contributions to the field of

01:30.720 --> 01:36.560
machine learning have been truly remarkable. He's authored one of the main

01:36.560 --> 01:40.400
textbooks in the field which is Pattern Recognition and Machine Learning

01:40.400 --> 01:45.360
or PRML. It has served as an essential reference for

01:45.360 --> 01:48.480
countless students and researchers around the world.

01:48.480 --> 01:52.160
Chris explained in the interview how it steered the field towards a more

01:52.160 --> 01:57.280
probabilistic perspective at the time and he also mentioned his first textbook

01:57.280 --> 02:01.600
Neural Networks for Pattern Recognition and its role in promoting neural

02:01.600 --> 02:05.040
networks as a powerful tool for machine learning.

02:05.040 --> 02:09.040
So this is the new textbook, Deep Learning, Foundations and Concepts

02:09.040 --> 02:12.800
and one of the things that we're proud of with this book is the production values.

02:12.800 --> 02:16.320
We really worked with the publisher to ensure the book would be

02:16.320 --> 02:19.440
produced to a high physical quality and in particular it's produced with what

02:19.440 --> 02:22.080
are called stitched signatures. So if you look down the edge there you'll see the

02:22.080 --> 02:27.120
pages are not simply glued in. Instead this uses an offset printing

02:27.120 --> 02:31.360
technique where 16 pages are printed on a big sheet of paper on both sides.

02:31.360 --> 02:35.120
Some of the pages are turned upside down and then the page of the paper is

02:35.120 --> 02:38.720
folded and then folded and then folded again and trimmed and the resulting

02:38.720 --> 02:42.320
set is called a signature and actually stitched in with cord. And the point

02:42.320 --> 02:46.560
about that is it allows the book to open flat so it means that the book is easy

02:46.560 --> 02:49.920
to read and it means it should last a long time.

02:49.920 --> 02:52.320
What are your favourite figures in the book Chris?

02:52.320 --> 02:55.440
Well the ones produced by my son of course are the best. I mean here's a nice

02:55.440 --> 02:59.760
picture of the transformer architecture which is this is GPT so

02:59.760 --> 03:02.720
you could say it's one of the most important figures in the book I suppose

03:02.720 --> 03:05.840
and I just love the way he's done this.

03:05.840 --> 03:08.960
How did you do the research for this?

03:08.960 --> 03:14.240
So that's a great question. I think you know one of the big

03:14.240 --> 03:16.800
challenges with writing a book like this is knowing what to include and

03:16.800 --> 03:20.080
what not to include and with literally thousands of papers being

03:20.080 --> 03:23.120
published every month it can be overwhelming for the authors never

03:23.120 --> 03:25.760
mind the readers. So I think the value we add in the book

03:25.760 --> 03:29.120
is trying to distill out what we think of as the core concept.

03:29.120 --> 03:33.600
So part of this was really looking at key papers in the field

03:33.600 --> 03:39.040
seeing what relatively recent ideas there are but also trying to focus down on

03:39.040 --> 03:42.080
techniques and ideas that we believe will actually stand the test of time.

03:42.080 --> 03:45.440
We don't have this book to go out of date in a year or two we want it to have

03:45.440 --> 03:49.440
have lasting value and of course it's quite possible there'll be a breakthrough

03:49.440 --> 03:52.080
next week and that it will turn out to be a very important

03:52.080 --> 03:56.400
new architecture but for the most part many of the core concepts actually go

03:56.400 --> 03:59.760
back a long way and so what we've really done is taken some

03:59.760 --> 04:02.960
some of the foundations of the field and brought them into the modern deep learning

04:02.960 --> 04:06.800
era but the idea of probabilities the idea of gradient based methods and so on

04:06.800 --> 04:09.440
those have been around for decades and they're just as applicable today as

04:09.760 --> 04:12.560
they ever were. One of the things I really like actually is the

04:12.560 --> 04:16.560
chapter on convolutional networks. My son Hugh did a lot of this

04:16.560 --> 04:20.560
chapter he works on using techniques like convolutional neural

04:20.560 --> 04:24.320
nets as part of his work on autonomous vehicles and I think there's a really

04:24.320 --> 04:27.680
nice description here of convolutional networks

04:27.680 --> 04:31.040
really from the ground up explaining the the basic concepts and

04:31.040 --> 04:34.960
but also motivating them not just saying this is how a convolutional network is

04:34.960 --> 04:38.320
built but why is it built this way how do we actually motivate it so that's

04:38.320 --> 04:41.280
one of my favorite chapters as well. Yeah it's been a very interesting career

04:41.280 --> 04:45.280
and at this stage of the career I can now finally look back and make sense of it

04:45.280 --> 04:47.680
but at the time it felt like a bit of a random walk

04:47.680 --> 04:52.480
so actually when I was a teenager I went to see 2001 A Space Odyssey it was

04:52.480 --> 04:55.760
actually very inspired by that rather abstract concept of an

04:55.760 --> 04:58.720
artificial intelligence very different from the usual sort of Hollywood

04:58.720 --> 05:02.000
the trail of robots so I was very interested in the idea of artificial

05:02.000 --> 05:06.320
intelligence from a young age but I was very uninspired by the field of AI

05:06.320 --> 05:09.440
at the time which was very much sort of rule based and and didn't seem to be on

05:09.440 --> 05:12.480
a path through intelligence and then I did a PhD in quantum field theory which

05:12.480 --> 05:15.760
is a very hot field at the time gauge field theory

05:15.760 --> 05:19.920
at Edinburgh University had a wonderful time at the end of my PhD though I wanted

05:19.920 --> 05:22.560
to do something a bit more practical a bit more useful

05:22.560 --> 05:26.480
and so I went into the fusion program I'm a big fan of nuclear fusion

05:26.480 --> 05:29.840
it was sort of 30 years away then and it's kind of still 30 years away now but

05:29.840 --> 05:33.120
I'm still a big believer but I went to work on

05:33.920 --> 05:37.920
talk about physics essentially theoretical physics of of plasmas trying to

05:37.920 --> 05:41.040
trying to understand the instabilities and control them

05:41.040 --> 05:44.400
so I was working very happily as a theoretical physicist having a having a

05:44.400 --> 05:48.080
great time and after about 10 years or so as a theoretical physicist

05:48.080 --> 05:52.080
Jeff Hinton published the backprop paper and it came to my attention

05:52.080 --> 05:55.920
and I found that very inspiring because there I saw a very very different

05:55.920 --> 06:01.600
approach to towards intelligence and so I started by applying neural

06:01.680 --> 06:05.360
networks to data from the fusion program because it was big data

06:05.360 --> 06:08.560
in its day I was I was working there to the the jet

06:08.560 --> 06:12.080
tokamak and they had many many high resolution diagnostics I had lots of

06:12.080 --> 06:14.800
data to play with and I became more and more fascinated by

06:14.800 --> 06:18.400
neural networks and then I did a sort of completely crazy thing I walked away

06:18.400 --> 06:21.440
from a very respectable career as a theoretical physicist

06:21.440 --> 06:24.720
and went full-time into the field of neural nets which at the time was not

06:24.720 --> 06:27.760
really a respectable field I would say it's not wasn't mainstream computer

06:27.760 --> 06:30.560
science it certainly wasn't physics it wasn't really anything

06:30.560 --> 06:33.600
but I just found it very inspiring and I was particularly inspired by the work

06:33.600 --> 06:36.720
of Jeff Hinton and so I've been in that field for

06:36.720 --> 06:40.160
you know three and a half decades now and of course

06:40.160 --> 06:43.600
recent history suggests that was probably a good career move

06:43.600 --> 06:47.360
and now most recently I've brought the two ends of my career together because

06:47.360 --> 06:50.720
I'm now very excited about the impact that neural nets and machine learning

06:50.720 --> 06:53.280
are having on the natural sciences including physics.

06:53.280 --> 06:57.600
Hinton is a famous connectionist so he believes that knowledge is

06:57.600 --> 07:02.160
sub-symbolic and I speaking with Nick Chater the other week he had a

07:02.160 --> 07:05.600
book called The Mind is Flat which is talking about the inscrutability of

07:05.600 --> 07:09.120
our brains. How do you feel that things have changed?

07:09.120 --> 07:12.880
I mean you were talking about a convergence of these different ideas in AI.

07:12.880 --> 07:15.520
I think one thing that's very interesting is that there has been a lot of

07:15.520 --> 07:18.560
discussion let's say from 2012 onwards when deep

07:18.560 --> 07:22.320
learning was clearly being very successful a lot of discussion that it was

07:22.320 --> 07:26.080
missing the sort of symbolic approach that we somehow to find a way to combine

07:26.080 --> 07:29.360
this connectionist approach to use that sort of probably rather dated term now

07:29.360 --> 07:31.760
but that sort of you know that neural net approach

07:31.760 --> 07:35.040
with the more traditional symbolic approach and I think what we've seen

07:35.040 --> 07:38.160
with models like GPT-4 for example that it's perfectly capable of

07:38.160 --> 07:41.520
reasoning at a more symbolic level not at the level of a human being of course

07:41.520 --> 07:45.040
but it can do that that kind of more abstract higher level

07:45.040 --> 07:48.160
reasoning and so I think what we're seeing with neural

07:48.160 --> 07:50.880
nets is rather like the human brain. The human brain doesn't have a

07:50.880 --> 07:54.960
connectionist neural net piece then some other machinery that does symbolic

07:54.960 --> 07:58.480
reasoning that that that same substrate is capable of all of these

07:58.480 --> 08:01.280
different kinds of reasoning and these different kinds of intelligence

08:01.280 --> 08:04.080
and we're starting to see that emerge now with neural net so I think

08:04.080 --> 08:08.080
I think for me the discussion of should we somehow combine symbolic reasoning

08:08.080 --> 08:11.520
with with connectionism no that that to me that's a piece of history

08:11.520 --> 08:14.960
it's about how can we how can we expand on the capabilities of neural nets.

08:14.960 --> 08:18.160
Yeah that's so interesting I remember there was a paper by Polition I think it

08:18.160 --> 08:22.000
was the the connectionist critique in 1988

08:22.000 --> 08:25.520
and I was quite sold on this idea of you know systematicity and

08:25.520 --> 08:30.080
productivity and so on and even now folks from that school of thought

08:30.080 --> 08:33.680
think that our brains are Turing machines this ability to address

08:33.680 --> 08:38.400
potential infinity and I guess what I'm getting from

08:38.400 --> 08:41.120
what you're saying is that the distinction isn't really there anymore you

08:41.120 --> 08:44.080
can do that kind of reasoning with neural networks.

08:44.080 --> 08:47.760
Well I take a very simple view which is that neural nets in that since 2012 in

08:47.760 --> 08:51.280
particular have been shown to be spectacularly capable

08:51.280 --> 08:54.720
and there's no end in sight the rate of progress is faster now than ever

08:54.720 --> 08:58.160
so it seems very straight nobody imagines that that machine learning and

08:58.160 --> 09:01.040
deep learning has suddenly ended at you know whatever the time is today you

09:01.040 --> 09:03.840
know this is this is this is the beginning of an S-curve

09:03.840 --> 09:07.840
so the idea that we would worry so much about the limitations of neural

09:07.840 --> 09:10.800
networks and what they can't do I think we just you put the word yet at the end

09:10.800 --> 09:14.480
of it your neural networks can't do x y and z yet but but I don't think

09:14.480 --> 09:18.400
any sense we've hit the buffers of of what neural nets can do and it's by

09:18.400 --> 09:21.920
far the most successful of the most rapidly advancing technology we have

09:21.920 --> 09:25.360
so to me you should look for the keys under the lamppost we have this powerful

09:25.360 --> 09:29.040
technology that's getting better by the week why would we not see how far we

09:29.040 --> 09:31.360
can push it rather than worry about its limitations.

09:31.360 --> 09:35.600
Absolutely now Professor Bishop you are incredibly famous

09:35.600 --> 09:39.040
for your book PRML but of course it wasn't your first book as you were just

09:39.040 --> 09:42.160
speaking to but what was I mean could you just tell us about your your

09:42.160 --> 09:45.360
motivations and just the thought process behind that book?

09:45.360 --> 09:48.880
Yes so as you said it wasn't my first book my first book is published in 1995

09:48.880 --> 09:52.160
Neural Networks for Pattern Recognition and that book had a very specific

09:52.160 --> 09:55.440
motivation which is that I was a newcomer to the field I mentioned earlier

09:55.440 --> 09:57.760
that I got excited about backprop and and sort of

09:57.760 --> 10:00.560
transition from theoretical physics into machine learning.

10:00.560 --> 10:03.520
That was my way of learning about the field you know if you're a university

10:03.520 --> 10:06.880
professor a great way to learn about something is to teach a course on it

10:06.880 --> 10:09.520
because it forces you to think about it very carefully you're going to get

10:09.520 --> 10:13.280
tricky questions from smart students and you're very motivated to to really

10:13.280 --> 10:17.280
understand it and so for me the analogue of that was was writing a book.

10:17.280 --> 10:21.760
PRML was rather different by the time we got to published in 2006 and by then

10:21.760 --> 10:25.120
the field was much larger its sense it was much more mature as a much more

10:25.120 --> 10:28.880
established and respected field there were many courses on machine learning

10:28.880 --> 10:32.560
the goal there was very different I simply wanted to write the as it were

10:32.560 --> 10:35.440
the book that everybody would use to learn about the field so it was trying to

10:35.440 --> 10:39.920
be comprehensive but trying to be to explain the concepts as clearly as

10:39.920 --> 10:45.440
possible and so really that was the goal the goal was to in a sense you know

10:45.440 --> 10:48.800
replace the earlier neural nets for pattern recognition book which was

10:48.800 --> 10:52.800
which serves an important role in its day I think but really try to produce a

10:52.800 --> 10:56.640
single coherent text where people could learn about the different topics in

10:56.640 --> 10:59.600
you know with a shared notation and hopefully trying to explain things as

10:59.600 --> 11:02.320
clearly as I could. We know in theoretical physics you know you can

11:02.320 --> 11:05.200
you can write down an equation but solving it may be extremely difficult

11:05.200 --> 11:08.800
you have to resort to approximations but it's still nice to have that that

11:08.800 --> 11:13.360
north star that compass that guides you and so for me I try to think of machine

11:13.360 --> 11:17.040
learning in similar terms there are some some foundations that that really

11:17.040 --> 11:21.440
don't change much over time that are that are very good guiding principles and

11:21.440 --> 11:24.560
we're dealing with data we're dealing with uncertainty we want to be

11:24.560 --> 11:29.040
quantitative so you're led very naturally indeed uniquely into probability theory

11:29.040 --> 11:32.160
and if you apply probability theory consistently that is the Bayesian

11:32.160 --> 11:36.160
framework so for me the Bayesian framework is a very natural

11:36.160 --> 11:39.600
bedrock on which you can build and think about machine learning now just as

11:39.600 --> 11:42.640
with theoretical physics you can't often just solve things exactly and

11:42.640 --> 11:46.080
certainly the Bayesian paradigm calls for

11:46.080 --> 11:49.920
integration or marginalization of all possible values of the parameters in

11:49.920 --> 11:52.960
your neural network well you always operate with a fixed

11:52.960 --> 11:56.400
computational budget right it may be a huge one but it be always constrained

11:56.400 --> 12:00.320
by by computational budget and should you spend that budget doing a very

12:00.320 --> 12:03.680
thorough Bayesian marginalization over a small neural network

12:03.680 --> 12:06.960
or should you take the same number of compute cycles and train a very much

12:06.960 --> 12:10.320
larger network and if you have plenty of data to train the larger network then

12:10.320 --> 12:13.600
the latter seems to be much more effective in a practical sense

12:13.600 --> 12:17.280
so while from a practical point of view the Bayesian approach still has

12:17.280 --> 12:21.920
certain applications in in various domains for the most part it's not the

12:21.920 --> 12:25.520
framework we'd want to use in in sort of mainstream

12:25.520 --> 12:29.200
machine learning today we're much more interested in scale and making point

12:29.200 --> 12:33.040
estimates in using stagastic gradient send and so on so I still

12:33.040 --> 12:36.720
think that students should learn the basic ideas of of Bayesian inference

12:36.720 --> 12:39.360
because really they have to learn you have to learn about probability I don't

12:39.360 --> 12:42.400
think you can be in machine learning and not understand probability

12:42.400 --> 12:45.520
and then once you understand probability and you apply it uniformly that

12:45.520 --> 12:49.600
that really is the Bayesian framework so I think it's the foundation

12:49.600 --> 12:53.120
but then you're led to make approximations and in particular you make

12:53.120 --> 12:56.400
point estimates so in practice you don't actually execute the full Bayesian

12:56.400 --> 13:00.560
paradigm yeah I agree that um Bayesian reasoning is

13:00.560 --> 13:04.400
it's beautiful and it's the continuation even of sort of propositional

13:04.400 --> 13:07.520
logic in the domain of uncertainty it's fundamental

13:07.520 --> 13:12.400
but there is this question of the world is a very gnarly place

13:12.400 --> 13:17.200
and folks argue that the brain is a kind of Bayesian inference machine

13:17.200 --> 13:22.560
but it can't it can't possibly be solving the intractable Bayesian problem

13:22.560 --> 13:26.400
and therein lies the question so there are many hybrids or

13:26.400 --> 13:30.080
even deep learning approaches could be seen as some kind of a continuation or

13:30.080 --> 13:34.480
somewhere on the spectrum between maximum likelihood point estimation and

13:34.480 --> 13:38.160
Bayesian models I mean how do you think about that

13:38.160 --> 13:40.560
spectrum I think that's a great that's a great question I

13:40.560 --> 13:43.920
think you're spot on there if you look back to a time when

13:43.920 --> 13:46.720
there are a lot of competitions here's a data set we're going to hold out the

13:46.720 --> 13:49.520
test set you've got to score as high as you can on the test set

13:49.520 --> 13:54.000
and what approach should you use the winner always is an ensemble you

13:54.000 --> 13:57.520
should try 10 different things preferably diverse and then combine

13:57.520 --> 14:01.120
them suitably maybe taking an average or some smarter combination

14:01.120 --> 14:05.280
and that ensemble will always outperform any one single model

14:05.280 --> 14:08.480
so if you're not constrained by compute and in some of those competitions you

14:08.480 --> 14:12.080
weren't then the ensemble always wins and you can think about that ensemble is

14:12.080 --> 14:15.200
like as you say a sort of rough and ready approximation

14:15.200 --> 14:19.600
to a full marginalization of all of the uncertainty in the predictions that you

14:19.600 --> 14:22.080
might make and so I think there's a little glimmer of

14:22.080 --> 14:24.800
sort of Bayesian approaches coming through there but again

14:24.800 --> 14:28.080
you know in the modern era you're probably better off training one single

14:28.080 --> 14:31.120
large model than 10 smaller ones and averaging

14:31.120 --> 14:35.680
so it's so I think knowing about the Bayesian paradigm and understanding

14:35.680 --> 14:39.040
where you can learn from it is still valuable today

14:39.040 --> 14:43.840
but nevertheless it's unlikely in most applications that you're going to want

14:43.840 --> 14:46.960
to apply the full Bayesian machinery because it's just so computational

14:46.960 --> 14:50.400
expensive fascinating I mean just one more thing on this

14:50.400 --> 14:54.320
do you think of large you know let's say large language models but large deep

14:54.320 --> 14:57.680
learning models do you think of them as one model or do you think of

14:57.680 --> 15:01.200
them as an inscrutable bundle of models because we're kind of getting into the

15:01.200 --> 15:05.280
no-free lunch theorem here coming from the Bayesian world we

15:05.280 --> 15:09.520
design models you know using principles and with neural networks we just

15:09.520 --> 15:12.960
train these big black boxes so do you think of them as one model or lots of

15:12.960 --> 15:16.000
models I certainly I always think of them as a

15:16.000 --> 15:18.960
single model I've never thought thought of them as separate models unless you

15:18.960 --> 15:21.520
unless you explicitly construct a mixture of experts or something like that

15:21.600 --> 15:25.280
you have an internal an internal structure I guess

15:25.280 --> 15:28.080
everything is sort of very distributed and somehow sort of holographic and

15:28.080 --> 15:32.240
overlapping and you know a remarkable thing about GPT-4 is that

15:32.240 --> 15:36.160
you know you often see people when they first they first use it they'll ask

15:36.160 --> 15:38.400
some question how tall is the Eiffel Tower and it probably gets the right

15:38.400 --> 15:41.600
answer and you know it's like oh that's kind of interesting and you're sort of

15:41.600 --> 15:44.480
a little bit disappointed in this technology but it's like being given

15:44.480 --> 15:48.000
the keys to a very expensive sports car and you notice the cup holders and you

15:48.000 --> 15:50.800
notice that it can can support a cup rather nicely you don't realize you

15:50.800 --> 15:53.680
need to start the engine and and drive off in it to really get the full

15:53.680 --> 15:56.320
experience and so until you realize that actually you can you can have a

15:56.320 --> 15:59.840
conversation it can it can write poetry it can explain jokes it can write code

15:59.840 --> 16:02.880
it can do so many many different things and that all those capabilities

16:02.880 --> 16:06.400
embedded in the same model and and what is I think a really interesting

16:06.400 --> 16:10.960
lesson of the last few years is that models like GPT-4 outperform the

16:10.960 --> 16:14.720
specialist models so for example in my lab we had a project for many years which

16:14.720 --> 16:18.160
essentially said the following it said well you know this is Microsoft

16:18.240 --> 16:21.440
world's biggest software company we have lots of source code we could use source

16:21.440 --> 16:24.880
code as training data for machine learning we've added all sorts of things you

16:24.880 --> 16:28.560
know spot bugs do water complete you know all kinds of things you could do if

16:28.560 --> 16:32.320
you had a good model of source code and the project was reasonably successful it

16:32.320 --> 16:35.760
was you know it worked reasonably well but what we've learned is that when you

16:35.760 --> 16:40.640
build one gigantic model that that yes it sees source code it sees scientific

16:40.640 --> 16:44.400
papers it sees wikipedia it sees many many different things in some way it

16:44.400 --> 16:48.480
becomes better at writing source code than a model specifically for writing

16:48.480 --> 16:52.400
source code and there are even even in ablation studies where people have a

16:52.400 --> 16:55.520
model that's trained to solve maths problems and it does reasonably well

16:55.520 --> 16:59.040
and now you give it some apparently irrelevant information let's say from

16:59.040 --> 17:02.400
wikipedia but with anything to do with maths stripped out and you find it

17:02.400 --> 17:05.120
actually does better at the maths so I think there are things here that we

17:05.120 --> 17:08.960
don't really understand but the general lesson I think is fairly clear

17:08.960 --> 17:12.720
that when you have a larger very general model it can outperform

17:12.720 --> 17:16.320
a specific model which I think is very interesting I guess the reason I was

17:16.320 --> 17:20.320
talking about the no free lunch theorem is it feels to me as you say that

17:20.320 --> 17:24.800
models behave quite differently in an input sensitive way

17:24.800 --> 17:27.520
so you ask them about this particular thing and it's almost like it's a

17:27.520 --> 17:31.200
different model because different parts of the model get activated

17:31.200 --> 17:35.920
and then there's this question of well is the no free lunch theorem violated

17:35.920 --> 17:39.840
can there be such a thing as a general foundational agent

17:39.840 --> 17:44.560
that could in robotics just do really well in any game or any environment

17:44.560 --> 17:49.200
or do you think do you think there's still some need for specialization

17:49.200 --> 17:54.640
another great question so I think these really open research questions honestly

17:54.640 --> 17:58.560
I'm not sure anybody really knows but I think one of the lessons is that the

17:58.560 --> 18:02.160
general can be more powerful than the specific so clearly one of the research

18:02.160 --> 18:04.560
frontiers we should push on is greater and greater

18:05.520 --> 18:09.840
and see so you know GBT4 can't ride a bicycle but if we have models that can

18:09.840 --> 18:13.040
can do robotics should they be separate and distinct models or if we somehow

18:13.040 --> 18:15.840
combined everything into a single model would it be more powerful

18:15.840 --> 18:18.640
and there's a decent chance that the latter would be true that it would be

18:18.640 --> 18:20.960
more powerful so certainly that's one research frontier

18:20.960 --> 18:24.640
we should push on an area I'm very interested in these days is

18:24.640 --> 18:28.400
is deep learning for for science for scientific discovery and science

18:28.400 --> 18:32.480
amongst other things involves very precise detailed numerical calculations

18:32.480 --> 18:36.000
now if you want to multiply some numbers together GBT4 would be a terrible way of

18:36.000 --> 18:38.560
doing it it might give you the wrong answer and even if it gets the right

18:38.560 --> 18:42.160
answer you're burning a tremendous amount of compute cycles to do something you

18:42.160 --> 18:44.960
could do with the far fewer compute cycles

18:44.960 --> 18:49.440
so there will still as far as I can see in certain domains be a role for

18:49.440 --> 18:52.640
specialist models but even then I can see them being

18:52.640 --> 18:55.280
integrated with things like large language models

18:55.280 --> 19:00.320
partly to provide human interface because one of the one of the things about

19:00.320 --> 19:03.360
language models is they they're so easy to interact with you don't have to be a

19:03.360 --> 19:06.480
computer program you just have a a natural conversation with them

19:06.480 --> 19:10.720
but also the other remarkable thing about the large language models

19:10.720 --> 19:13.520
I think there are two remarkable things the first of all is that they're so good

19:13.520 --> 19:16.800
at human language maybe that's not too surprising because they're sort of

19:16.800 --> 19:21.840
designed to do that but by virtue of being forced to

19:21.840 --> 19:25.920
effectively compress human language they become reasoning engines and that's a

19:25.920 --> 19:29.600
remarkable discovery right that is a big surprise certainly to me I

19:29.600 --> 19:32.000
think to many people perhaps to everybody in the field

19:32.000 --> 19:35.600
that they can function as reasoning engines and so even if you're

19:35.600 --> 19:38.960
let's say doing some specialist scientific calculations you might still

19:38.960 --> 19:43.120
think about a large language model as as a kind of a

19:43.120 --> 19:46.560
co-pilot for the scientist helping the scientist reason over

19:46.560 --> 19:51.120
what increasingly consists of massive massively complex spaces

19:51.120 --> 19:54.080
very high dimensionality many different modalities of data

19:54.080 --> 19:56.960
it's harder and harder for humans to sort of wrap their head around this and this

19:56.960 --> 20:00.480
is where I think a large language model can can can be valuable

20:00.480 --> 20:03.680
but I still see it calling on specialist tools in the foreseeable future

20:03.680 --> 20:06.800
because you were talking about statistical generalization but you could

20:06.800 --> 20:10.800
argue that language models can't do let's say they can't compute the

20:10.800 --> 20:13.920
nth digit of pi because they don't have an expandable memory they're not

20:13.920 --> 20:16.560
Turing machine so that that's a computational limitation but

20:16.560 --> 20:19.680
but they might be able to do this statistical generalization

20:19.680 --> 20:23.440
as we were talking about even though it might in fact be a weird form of

20:23.440 --> 20:28.240
specialization in terms of an ensemble of methods of models inside

20:28.240 --> 20:31.360
a large language model but on the on the language thing and the reasoning this

20:31.360 --> 20:35.120
this is fascinating so I think that language

20:35.120 --> 20:39.120
is a bunch of memetically embedded programs

20:39.120 --> 20:44.480
so we we play the language game and we establish cognitive categories we

20:44.480 --> 20:46.560
embed them and share them socially and it's like

20:46.560 --> 20:49.680
there's a little simulation out there and I'm using that to think

20:49.680 --> 20:52.640
but the question always is to what extent

20:52.640 --> 20:56.960
and is that that's a bunch of processing that previous humans have done

20:56.960 --> 21:00.480
and we can use it but can the language model create

21:00.480 --> 21:04.400
new programs like that this is I think part of a fascinating and broader

21:04.400 --> 21:08.160
discussion so I do hear a lot of oh it can't do x y and z

21:08.160 --> 21:11.120
often that's true and I've always put the word yet at the end of it because I

21:11.120 --> 21:13.600
don't know any law or physics and it can't do certain there are some things

21:13.600 --> 21:16.320
which perhaps the current architectures provably can't do

21:16.320 --> 21:19.360
but but there's lots of exploration in different architectures there's a lot of

21:19.360 --> 21:22.400
scope for for for expanding and generalizing neural net so I

21:22.400 --> 21:24.800
always think of it can't do a certain thing yet

21:24.800 --> 21:28.480
but a lot of the questions or a lot of the comments about

21:28.480 --> 21:32.640
the limitations of models I have a have a hypothesis on this I mean

21:32.640 --> 21:36.000
let me test this out on you I may be I may be way shorter the mark on this one

21:36.000 --> 21:39.520
but a lot of the a lot of the critique of what models

21:39.520 --> 21:43.600
seemingly can't do or especially when it's oh they will never be able to do this

21:43.600 --> 21:47.120
they cannot be creative or they cannot reason or they cannot whatever

21:47.200 --> 21:51.360
I wonder if a lot of this comes to to a much more fundamental point that's not

21:51.360 --> 21:53.600
actually a technical one it's really to do with the human

21:53.600 --> 21:56.720
the human journey over the last few thousand years because we've

21:56.720 --> 22:00.080
you know a few thousand years ago I guess most humans would have perceived

22:00.080 --> 22:02.640
humanity as the center of the universe they were the earth's center of the

22:02.640 --> 22:06.000
universe the universe was created for the benefit of humanity

22:06.000 --> 22:09.200
we had this very arrogant view of our own importance and what we've learned over

22:09.200 --> 22:11.680
the centuries especially from fields like astronomy

22:11.680 --> 22:15.520
is of course you know that the the entirety of humanity's existence

22:15.520 --> 22:19.760
is a brief blink of the eye compared to the distance of the the whole universe

22:19.760 --> 22:22.960
and and our physical place in the universe in terms of length scale we're on a

22:22.960 --> 22:25.760
little speck of dust orbiting an insignificant star in a rather boring

22:25.760 --> 22:30.160
galaxy in this colossal universe and and so I think it's natural for us as

22:30.160 --> 22:33.840
humans to sort of continue to cling to the things that we feel make us special

22:33.840 --> 22:36.080
and we're certainly not the fastest creatures on earth we're not the

22:36.080 --> 22:38.720
strongest but it's our brains that seem to make us

22:38.720 --> 22:42.080
unique we are the most intelligent creatures by far on earth

22:42.080 --> 22:45.440
and so we think of our of our intelligence as being the very special

22:45.440 --> 22:47.840
thing yes okay we get it that we're just living in a

22:47.840 --> 22:50.800
sporing corner of the universe but nevertheless it's our brains that make

22:50.800 --> 22:54.080
us special so let me tell you a little story which is

22:54.080 --> 22:57.040
because I work for Microsoft I was very privileged to have early access to

22:57.040 --> 23:00.400
GPT-4 and it was still a highly-tented highly-secret

23:00.400 --> 23:04.880
project and so I was exposed to GPT-4 at a time when

23:04.880 --> 23:08.160
I could only discuss it with a very small number of very specific colleagues

23:08.160 --> 23:10.880
and for everybody else I couldn't couldn't even talk about it

23:10.880 --> 23:16.080
and it was quite a shocking moment the the ability to

23:16.080 --> 23:19.680
understand and generate language sort of didn't come as so much of a surprise

23:19.680 --> 23:22.720
because of course I'd been following GPT-2 and GPT-3 and

23:22.720 --> 23:25.280
you know knew this technically was getting better

23:25.280 --> 23:29.600
but this ability to reason there was a sort of visceral reaction I had which

23:29.600 --> 23:34.000
took me right back to that film 2001 that sense of I was engaging with something

23:34.000 --> 23:36.960
which you know my colleague Sebastian Bubeck called it the sparks of

23:36.960 --> 23:40.160
artificial intelligence so nobody in that nobody's claiming GPT-4 is

23:40.160 --> 23:43.040
anywhere close to human intelligence or anything like that but there was just

23:43.040 --> 23:46.800
the first glimpse of something it was the first time in my life that I'd

23:46.800 --> 23:49.680
interacted with something that wasn't a human being

23:49.680 --> 23:54.240
that had a glimmer of this this higher level of intelligence

23:54.240 --> 24:00.080
and and realizing this may be the dawn of a of a new era that may be

24:00.080 --> 24:03.600
even more significant than the 2012 moment of the dawn of deep learning

24:03.600 --> 24:07.840
there was something very special going on and I wonder if part of the reaction

24:07.920 --> 24:11.840
that we have to these models is a little bit of that sense of that threat to

24:11.840 --> 24:15.120
the specialness that we feel as humans now maybe completely wrong this is purely

24:15.120 --> 24:18.960
speculation but you know it's interesting that we

24:18.960 --> 24:21.360
talk about people use phrases like stochastic

24:21.360 --> 24:24.400
parody it's just regurgitating stuff that it that it's seen before

24:24.400 --> 24:27.120
some people claim or you know of course it hallucinates sometimes it comes up

24:27.120 --> 24:30.320
with stuff that's just wrong or doesn't make sense

24:30.320 --> 24:34.480
but but think about the following imagine there was a very very smart

24:34.480 --> 24:38.160
physics student went to you went to a top university worked really hard for

24:38.160 --> 24:41.680
four years what would they do they would they would read books they would

24:41.680 --> 24:44.960
read papers listen to lectures have discussions with their professors and

24:44.960 --> 24:47.520
with other students and then they sit their final exam

24:47.520 --> 24:51.760
and they get 95% in their final exam and they come top of the year

24:51.760 --> 24:56.000
we don't say huh well 95% of the time there are stochastic parrot

24:56.000 --> 24:59.600
regurgitating Einstein and Maxwell and the other 5% of the time they're

24:59.600 --> 25:02.080
hallucinating no we say congratulations you have a

25:02.080 --> 25:05.520
first-class honors degree you've graduated with honors this is this is a

25:05.520 --> 25:11.200
you know a wonderful achievement so it's interesting that we do seem to view

25:11.200 --> 25:15.280
the the capabilities of of neural nets with it with almost a different ruler

25:15.280 --> 25:18.960
to that of humans and while nobody's suggesting that current models are

25:18.960 --> 25:22.480
anywhere close to humans on many axes of intelligence

25:22.480 --> 25:26.560
nevertheless i see the first sparks of of artificial intelligence

25:26.560 --> 25:31.040
and just one final comment the term AI artificial intelligence has been very

25:31.040 --> 25:34.320
popular for many years i used to hate it i used to always say that's machine

25:34.320 --> 25:36.640
learning none of these systems are intelligent they're very good at

25:36.640 --> 25:40.080
recognizing cats and images there's nothing really intelligent about this

25:40.080 --> 25:44.480
in in in in one sense and yet now i find for the first

25:44.480 --> 25:46.480
time i feel comfortable talking about artificial

25:46.480 --> 25:49.600
intelligence because i think we've taken the first baby steps towards what i

25:49.600 --> 25:52.960
think of as true artificial intelligence i still think that

25:52.960 --> 25:56.800
agency and creativity are the distinguishing feature

25:56.800 --> 26:00.400
not necessarily that we are and biological beings

26:00.400 --> 26:04.640
it's more to do with we are independent agents and we are

26:04.640 --> 26:08.320
sampling random things from our local worlds and we're combining them

26:08.320 --> 26:12.080
together in in interesting ways and in doing so

26:12.080 --> 26:15.760
intelligence is about the process of building models

26:15.760 --> 26:19.520
and sharing models and embedding models in in our culture

26:19.520 --> 26:22.720
so it feels to me that gpt was building models

26:22.720 --> 26:27.840
at the time it was trained and and and that's all it's doing

26:27.840 --> 26:30.880
i can imagine a world where there were lots of gpt's

26:30.880 --> 26:35.120
we all had gpt in our pockets and maybe then it would be much more like

26:35.120 --> 26:38.640
biomimetic intelligence i think there are lots of interesting

26:38.640 --> 26:43.680
points that you touched on there tim so i think one thing is in terms of

26:43.680 --> 26:47.040
creativity you know are these systems creative it's certainly true they only

26:47.040 --> 26:50.080
exist because of humans they're created by humans

26:50.080 --> 26:53.200
and and we should acknowledge that but i don't think it means they're

26:53.200 --> 26:57.520
intrinsically not creative if i asked an artist to

26:57.520 --> 27:00.800
paint me a picture of some people walking on the beach with a sunset or

27:00.800 --> 27:03.280
whatever and they came back a few days later with some

27:03.280 --> 27:06.480
beautiful picture i might hate it they may have used very vivid

27:06.480 --> 27:09.840
colors i might like pale pastel colors but that's a matter of opinion

27:09.840 --> 27:12.800
but i wouldn't deny that there was creativity there

27:12.800 --> 27:16.560
but their expertise came because well they went they perhaps had some

27:16.560 --> 27:20.000
intrinsic ability in some sense but they went to art school they study the

27:20.000 --> 27:22.640
work of other artists they practice they got better

27:22.640 --> 27:27.520
and and and that creativity owes a lot to what went before but i don't think it

27:27.520 --> 27:30.240
diminishes that in the same way a physics student who can

27:30.240 --> 27:33.200
explain the theory of relativity you have to say well you didn't invent the

27:33.200 --> 27:36.080
theory of relativity you know einstein invented that you only learned it from

27:36.080 --> 27:39.760
einstein but it doesn't diminish the the fact

27:39.760 --> 27:42.400
that they have understanding the fact that they convey it and the fact they

27:42.400 --> 27:45.200
can potentially think in new ways and be creative

27:45.200 --> 27:51.680
so i'm i'm less convinced about discussions about the limitations of

27:51.680 --> 27:55.600
of of the technology in general of where it can go i don't particularly see any

27:55.600 --> 27:58.960
limitations the brain is a machine that uses this

27:58.960 --> 28:01.840
a term used earlier connectionist approach it uses these fine-grained

28:01.840 --> 28:05.040
neural nets and and so there are similarities to the

28:05.040 --> 28:07.920
technology that we have now there are also huge differences

28:07.920 --> 28:11.120
some of those differences point to the artificial neural nets being much more

28:11.120 --> 28:14.000
powerful than biological neural nets and hinted made a strong

28:14.000 --> 28:16.320
point of this lately and i think it's a very interesting

28:16.320 --> 28:20.880
perspective so i would be the first to say

28:20.880 --> 28:25.120
yes the technologies we have on many axes are a long way short of humans on

28:25.120 --> 28:28.320
many axes the much better gbt4 can create text

28:28.320 --> 28:32.480
much better than any human i mean to produce a page of coherent text that's

28:32.480 --> 28:35.760
correctly punctuated in good grammar and so on in a few seconds there aren't

28:35.760 --> 28:40.000
any people that can do that i think so on an increasing number of axes

28:40.000 --> 28:43.840
systems clearly outperform humans and on others there's still a very long

28:43.840 --> 28:47.840
way to go but i think one of the nice things about technologies like this

28:47.840 --> 28:51.680
generative ai technologies whether it's you know saura for creating videos or

28:51.680 --> 28:55.920
gbt4 or whatever it might be is they do rely on the prompt there is a clear

28:55.920 --> 29:00.480
role they are co-pilots as we say they they they sit there and do nothing

29:00.480 --> 29:05.040
and you use them as a sort of a cognitive amplifier you have an idea

29:05.040 --> 29:08.400
sort of half-baked and then you can engage in a conversation and

29:08.400 --> 29:11.520
sure enough it can come up with a different way of thinking say hey that's

29:11.520 --> 29:15.680
really good i like that idea now let's take that work that back in try again

29:15.680 --> 29:19.120
and so it becomes now a companion a co-pilot something that

29:19.120 --> 29:22.880
enhances your your cognitive ability but the human is still very much in the

29:22.880 --> 29:25.920
loop and playing a key part and actually initiating the process

29:25.920 --> 29:28.800
and of course finally at the end of the day you're the one that selects the

29:28.800 --> 29:32.240
you know the 10 video clips you pick the one that you like and so the human is

29:32.240 --> 29:34.800
very much involved in the loop throughout so i think that's a very nice

29:34.800 --> 29:38.400
feature of this technology i completely agree with that so

29:38.400 --> 29:43.440
at the moment a is are embedded in the cognitive nexus of humans so we have the

29:43.440 --> 29:46.720
agency and we drive these things and and they

29:46.720 --> 29:50.240
help us think and also i agree with you that it doesn't

29:50.240 --> 29:53.120
make sense to think of these things as limited forms of computation we should

29:53.120 --> 29:56.880
think of the collective intelligence so we are touring machines and we are

29:56.880 --> 29:59.840
driving these things and we are sharing information so when you look at

29:59.840 --> 30:04.880
the entire system it is a new type of memetic intelligence in fact

30:04.960 --> 30:09.040
you know to a certain extent GPT-4 isn't running on Microsoft servers

30:09.040 --> 30:12.640
it's in all of us right and that's that's a wonderful way

30:12.640 --> 30:19.120
um to think about it but to me the extent to which it is constraining our

30:19.120 --> 30:24.000
agency and creativity is what i'm fascinated by so GPT says unraveling

30:24.000 --> 30:26.800
the mysteries and you know the intricate

30:26.800 --> 30:30.240
dance of x y z and all of these weird motifs and

30:30.240 --> 30:34.720
constructions and maybe that's just the way that our

30:34.720 --> 30:40.000
LHF has constrained the model or maybe it speaks to the constraining

30:40.000 --> 30:43.200
forces in general of having these low entropy models that kind of you know

30:43.200 --> 30:46.560
snip off a lot of the interesting pathways so

30:46.560 --> 30:52.000
we are very creative GPT-4 resists creativity a little bit is it a problem

30:52.000 --> 30:55.200
well i think there's some design choices there so you talked about reinforcement

30:55.200 --> 30:58.000
learning through human feedback is part of that alignment process we ought to

30:58.000 --> 31:01.760
create this technology in a way that does good to minimize harm

31:01.760 --> 31:06.160
and so naturally we do constrain it so for sure it's true that our constrained

31:06.160 --> 31:10.000
GPT-4 behave in you might say less creative ways but perhaps in more

31:10.000 --> 31:13.200
helpful and beneficial ways and it's appropriate that we should do that

31:13.200 --> 31:16.720
and perhaps we lose a little bit of the creativity

31:16.720 --> 31:20.800
in the process and so there's there's a balance there's a there's a there's a

31:20.800 --> 31:24.560
choice to be made a design choice in how we want to create the technology and

31:24.560 --> 31:27.440
we should be very deliberate about that and not not apologetic for that i think

31:27.440 --> 31:30.480
it's good that we are that we are making those design choices

31:30.480 --> 31:34.880
but people sometimes have an intuition that it's not creative

31:34.880 --> 31:38.240
and contrast that to i'm using DaVinci Resolve

31:38.240 --> 31:42.160
and i'm using all of these nodes and i have all of these filters and

31:42.160 --> 31:46.080
processing transforms the difference seems to be that

31:46.080 --> 31:50.720
i'm designing the architecture so i'm using cognitive primitives

31:50.720 --> 31:54.880
and i'm composing them together in a new way and by tweaking the parameters on

31:54.880 --> 31:57.760
the filters i'm going off-peast a little bit i'm

31:57.760 --> 32:02.160
doing i'm creating the structure myself whereas in neural networks the

32:02.160 --> 32:05.280
structure is implicit i don't know what the structure is

32:05.280 --> 32:07.920
well i think you're talking about you're contrasting two different kinds of tools

32:07.920 --> 32:11.600
there so the video editing tool is designed so that it follows your

32:11.600 --> 32:15.200
instructions very precisely and you prefer one tool over another

32:15.200 --> 32:18.720
perhaps because the interface is easier to use you get the results faster but

32:18.720 --> 32:21.840
you have in your you've done the creativity you've designed this to

32:21.840 --> 32:25.600
video edit that you want that you want to have and now the tool is to try to get

32:25.600 --> 32:28.560
you to that as fast as possible as accurately as possible

32:28.560 --> 32:31.520
but sometimes we need more than that sometimes you know if you've got right

32:31.520 --> 32:33.280
as block and you don't know where to begin

32:33.280 --> 32:36.960
having a tool like GPT-4 could be very powerful you're not you're not

32:36.960 --> 32:40.400
delegating the entire process to the technology you're working

32:40.400 --> 32:43.600
with it as a as a co-pilot as an assistant

32:43.600 --> 32:47.440
that can for sure help you with that creative process it will come up with

32:47.440 --> 32:50.800
with crazy things and most of them you may not like but maybe one of them

32:50.800 --> 32:53.680
you don't like it either but it causes you to think about something that you

32:53.680 --> 32:57.520
would otherwise not have thought of and so the two working together

32:57.520 --> 33:01.760
can surely be more creative so i think certainly as a working in unison with

33:01.760 --> 33:04.240
humans it certainly enhances creativity so that's

33:04.240 --> 33:07.680
certainly my experience i think there's no doubt about that but also if you

33:07.680 --> 33:10.640
think about let's take a simple example that i think most people relate to which

33:10.640 --> 33:13.840
is which is image generation you're giving a talk and you want some

33:13.840 --> 33:18.640
image to illustrate the talk and you know you could go to stock images and

33:18.720 --> 33:23.200
you know it's a fixed set and you know you can't easily adjust it or you

33:23.200 --> 33:27.520
or you go to editing images yourself that's a sort of slow and painful process

33:27.520 --> 33:31.120
but now you can just with a simple prompt you know you can get a a bunch of

33:31.120 --> 33:34.080
examples and if one of those isn't quite what you like you can

33:34.080 --> 33:37.920
alter the prompt and and fine tune it and it now becomes that

33:37.920 --> 33:42.160
that process which is a creative process and you can still say the human is in

33:42.160 --> 33:46.000
the driving seat but the overall creativity is certainly enhanced

33:46.000 --> 33:50.000
and when you take a text prompt and and the machine produces this beautiful

33:50.000 --> 33:54.560
photorealistic image i mean how many of us weren't absolutely blown away

33:54.560 --> 33:57.760
by the incredible advances in generative AI of the last

33:57.760 --> 34:02.560
you know the last decade why would you not call that creative if a human being

34:02.560 --> 34:06.160
did it you would call it creative why why are we not allowing the machine

34:06.160 --> 34:09.520
to be described as creative that's the piece that i don't that i don't quite

34:09.520 --> 34:13.280
understand so you could argue that creativity is just pure novelty of the

34:13.280 --> 34:16.160
artifact so it's just how much entropy is in the artifact

34:16.160 --> 34:20.960
but you could you could think of GPT4 pros as being a kind of

34:20.960 --> 34:23.840
category so there's a lot of variants in there

34:23.840 --> 34:27.760
but there are also certain motifs and and now when people see the motifs they

34:27.760 --> 34:31.040
say oh i've seen that a million times before so i did think it was novel and

34:31.040 --> 34:35.680
interesting and now i don't and but this is the thing so now when i'm

34:35.680 --> 34:39.600
writing blog posts and stuff like that i'm deliberately trying to do something

34:39.600 --> 34:43.040
genuinely creative to me you know it's it's almost like the intrinsic

34:43.040 --> 34:47.040
creativity isn't important i don't want people to think that i use GPT4

34:47.040 --> 34:51.360
so that's driving it do you see what i mean yeah so in clearly creativity is

34:51.360 --> 34:55.680
about novelty and novelty is you know what we desire here

34:55.680 --> 35:01.440
but whether that novelty has value or not that's a subjective opinion in your

35:01.440 --> 35:04.800
case it's whether it's achieving the the goals that you desire

35:04.800 --> 35:08.000
so i think there is no doubt that it's even if you say

35:08.000 --> 35:10.960
we're just taking existing ideas and combining them in new ways

35:10.960 --> 35:14.400
everything that humans do or i think builds on the work of

35:14.400 --> 35:18.000
their own previous experience and on the work of others and i think that's

35:18.000 --> 35:21.360
absolutely fine that's a wonderful thing about the humanity is that we

35:21.360 --> 35:24.880
from generation to generation we build upon the work of what's gone before

35:24.880 --> 35:28.400
and the machines that we build now are heavily dependent on

35:28.400 --> 35:31.760
the creativity and the work of humans before because they learn from humans

35:31.760 --> 35:34.640
they're designed by humans and i think that's absolutely fine it's a

35:34.640 --> 35:37.280
wonderful thing and they add to the sum total of human

35:37.280 --> 35:39.360
creativity and that that's a wonderful thing

35:39.360 --> 35:45.920
Chris you wrote a really beautiful book and you wrote it with your son

35:45.920 --> 35:51.120
Hugh and there was a picture of Hugh i think in the introduction of of PRML

35:51.120 --> 35:55.200
and i guess part of what i want to understand is is deep learning is a

35:55.200 --> 35:58.400
huge field i mean what was the thought process and

35:58.400 --> 36:02.400
how did you decide what to tackle and what not to tackle

36:02.400 --> 36:05.840
great questions there's an interesting story behind the the new deep learning

36:05.840 --> 36:10.800
book which is that PRML was written in 2006

36:10.800 --> 36:14.400
it predates the deep learning revolution and what has constantly surprised

36:14.400 --> 36:18.480
me is just how popular it's remained in spite of the fact that in one sense it's

36:18.480 --> 36:21.280
massively out of date because it does has no mention

36:21.280 --> 36:24.720
of the most important thing in the field of machine learning and so i've long

36:24.720 --> 36:28.080
felt it was time to update the book produce the second edition add some

36:28.080 --> 36:32.400
material on deep learning but life is busy and you know

36:32.400 --> 36:36.000
anybody who's ever written a book will tell you that it takes way more effort

36:36.000 --> 36:39.200
than you can possibly imagine if you've not actually had that experience

36:39.200 --> 36:43.360
and so i never really got around to doing it and along came the covid pandemic

36:43.360 --> 36:47.120
and we all went into lockdown and i feel like it was one of the very

36:47.120 --> 36:50.320
privileged people in that lockdown we were we were locked down together as a

36:50.320 --> 36:54.960
family in in Cambridge and you know when you're

36:54.960 --> 36:58.480
locked down at home for several months you kind of need a project and

36:58.480 --> 37:01.120
and i thought this would be a great time to think about a second edition of the

37:01.120 --> 37:04.480
PRML book because you know what what else you're going to do in lockdown and it

37:04.480 --> 37:07.760
became a project with my son because he was he was with me

37:07.760 --> 37:12.320
by this time he he'd gained a lot of experience master's degree in machine

37:12.320 --> 37:15.520
learning and been working in autonomous vehicle technology

37:15.520 --> 37:18.160
and in a sense he had a lot more practical hands-on experience with deep

37:18.160 --> 37:22.160
learning than than i did at that point and so we started this as a joint

37:22.160 --> 37:26.000
project but we very quickly realized that what was needed was not

37:26.000 --> 37:30.240
a couple of extra chapters on PRML but rather the whole field had changed so

37:30.240 --> 37:33.440
much and also we didn't want to write a book we

37:33.440 --> 37:36.800
were just accumulated more and more material it would just become a huge

37:36.800 --> 37:40.960
a huge tome the value of a book i think is

37:40.960 --> 37:45.360
is in the distillation is in the way it draws your attention to a subset of

37:45.360 --> 37:48.160
specific things this is the small set of things that you really need to

37:48.160 --> 37:51.200
understand and then you're quick to go off into the field

37:51.200 --> 37:54.720
so what we omitted was almost as important as what we what we added

37:54.720 --> 37:58.800
and we very quickly realized this was a this was a new book so we we we called

37:58.800 --> 38:01.680
the book deep learning foundations and concepts

38:01.680 --> 38:05.120
and we made a lot of progress but then of course the the lockdowns

38:05.120 --> 38:09.600
ended i started a new team called ai for science at microsoft

38:09.600 --> 38:13.440
hu started at wave technologies building the the core machine learning

38:13.440 --> 38:17.440
technology for their autonomous vehicles and we were all just far too busy

38:17.440 --> 38:20.880
and then the next thing that happened was the chat gpt moment

38:20.880 --> 38:24.000
we're you know in a space of a few weeks a hundred million people were using

38:24.000 --> 38:27.440
this and suddenly ai machine learning was in the

38:27.440 --> 38:30.400
in the consciousness of the the general public

38:30.400 --> 38:34.320
and we realized that if ever there was a time to finish this book it had to be

38:34.320 --> 38:38.480
now and so we had a just a really big push to

38:38.480 --> 38:42.800
to get the book finished and available for for new eurips in 2023

38:42.800 --> 38:46.720
and we made it just you know at the last minute as you do

38:46.720 --> 38:49.680
and the book was on display at new eurips there and

38:49.680 --> 38:53.600
hu and i spent the week going around the conference together

38:53.600 --> 38:57.600
talking to folks at posters and and just had a great time so it was actually a

38:57.600 --> 39:00.800
huge privilege to to be able to write the book with my son

39:00.800 --> 39:04.480
yeah that's fantastic um what was your favorite chapter

39:04.480 --> 39:07.920
and i mean are there any um things that you felt were

39:07.920 --> 39:11.360
remissions that you would have liked to do but you just had to draw a line under

39:11.360 --> 39:14.000
it yeah in terms of uh favorite chapters i mean

39:14.000 --> 39:17.200
of course the things the the the more recent architectures were

39:17.200 --> 39:20.080
particularly interesting i very much enjoyed writing the the diffusion

39:20.080 --> 39:24.080
chapter and hu had a lot of input into that chapter of course transformers as

39:24.080 --> 39:28.560
well and just understanding how to how to

39:28.560 --> 39:32.320
integrate the the sort of the different generative frameworks how to bring

39:32.320 --> 39:35.120
think about gans and how to think about variational order encoders and you know

39:35.120 --> 39:37.840
how to think about normalizing flows and so on how to think about those under

39:37.840 --> 39:40.640
one umbrella and present them in a more coherent way

39:40.640 --> 39:44.000
so that was that was part of the interesting free the learning experience

39:44.000 --> 39:46.800
i always enjoy learning new things i learned things writing that book and

39:46.800 --> 39:50.720
that and i think you did as well and so in a sense that was that was the

39:50.720 --> 39:53.680
favorite part of the book the things where i where i learned new things or

39:53.680 --> 39:57.120
new ways of looking at things i already knew about the real decision process is

39:57.120 --> 40:00.080
what to put in what not to put in while keeping the size of the book under

40:00.080 --> 40:03.040
control because i think it's something like it's thousands of papers a month

40:03.040 --> 40:06.720
now published in machine learning uh it's overwhelming for the beginner so

40:06.720 --> 40:10.240
really the goal of the book is to still out those few core concepts which means

40:10.240 --> 40:13.600
there are always things oh should we have added this should we have added that

40:13.600 --> 40:16.800
what we wanted to do was to avoid adding the latest sort of

40:16.800 --> 40:20.080
architecture that might be very hot at the minute that could easily disappear

40:20.080 --> 40:24.160
three months down the line so i hope we've resisted that that temptation

40:24.160 --> 40:27.680
but there are areas where you know perhaps when we at some point if we get

40:27.680 --> 40:30.480
around to a second edition we might think about including reinforcement

40:30.480 --> 40:33.760
learning is something which is of growing importance and

40:33.760 --> 40:36.960
would be lovely to have a chapter on reinforcement learning that integrates

40:36.960 --> 40:39.520
well with the rest of the book there are books on reinforcement learning there

40:39.520 --> 40:42.000
are review articles there's plenty of place to go learn about them

40:42.000 --> 40:45.120
there's something that sort of integrated with the book i think could be

40:45.120 --> 40:49.680
could be valuable so that is something we might we might visit in the future

40:49.680 --> 40:53.040
but for the moment we've just focused on what we think are the

40:53.040 --> 40:57.280
core principles that any any newcomer to the field whether a master student

40:57.280 --> 41:00.160
whether they're somebody who's self-taught a practitioner coming into

41:00.160 --> 41:03.440
the field wanting to understand the basics of the field and so the goal was

41:03.440 --> 41:06.720
to try to keep the book as it were as short as possible but no shorter

41:06.720 --> 41:10.080
looking back on on your last couple of books as well in in retrospect

41:10.640 --> 41:13.360
which bits are you are you most kind of proud of and

41:13.360 --> 41:16.720
which bits do you do you kind of feel that when you did make the decision at

41:16.720 --> 41:20.480
the time perhaps you've you've you've mispredicted how successful something

41:20.480 --> 41:24.320
might be very interesting so the thing i'm most proud of actually

41:24.320 --> 41:28.000
is the very first book called neural networks for pattern recognition

41:28.000 --> 41:31.760
and the reason is because i think that the book was quite influential in

41:31.760 --> 41:34.800
steering the field towards a more probabilistic more statistical

41:34.800 --> 41:38.320
perspective of machine learning it perhaps hard for people to appreciate

41:38.320 --> 41:42.960
today but it wasn't always that way when i first went into machine learning

41:42.960 --> 41:46.480
a lot of it was inspired by neurobiology which is which is fine

41:46.480 --> 41:50.160
but it lacked sort of mathematical rigor it lacked any mathematical foundation

41:50.160 --> 41:53.520
and so there was a lot of trying to learn a bit more about the brain and then

41:53.520 --> 41:57.120
try to copy that in the algorithms and see if that worked better or not

41:57.120 --> 42:01.200
and there was a lot of trial and error still a lot of empirical trial and error

42:01.200 --> 42:04.240
in machine learning of course but at least we have that that sort of bedrock

42:04.240 --> 42:07.840
of probability theory and so i think that the book was the first one to

42:07.840 --> 42:11.120
really address machine learning and neural networks from a statistical from

42:11.120 --> 42:14.160
a probabilistic perspective and i think in that respect the book was very

42:14.160 --> 42:17.920
influential the field was much smaller than today we take we take that as

42:17.920 --> 42:21.200
obvious but i think in terms of the thing i'm most proud of it's probably

42:21.200 --> 42:25.280
the influence of that that first book back in back in 1995

42:25.280 --> 42:28.960
in terms of things i look back on that i might do differently

42:28.960 --> 42:32.960
i suppose when i look at if i look at prml for example and i look at the

42:32.960 --> 42:36.320
trajectory of the field we've seen that neural networks were

42:36.320 --> 42:40.480
were all the rage in the mid mid 1980s to mid 1990s

42:40.480 --> 42:43.680
and then they kind of got overtaken by other techniques and then we had this

42:43.680 --> 42:46.960
sort of Cambrian explosion of you know support vector machines and

42:46.960 --> 42:49.840
Gaussian process and Bayesian methods and graphical models and

42:49.840 --> 42:53.680
and all the rest of it and and i think one thing that one thing that i think

42:53.680 --> 42:57.040
Jeff Hinton really got right is we really understood that neural networks

42:57.040 --> 42:59.920
were the way the way forward and he really stuck to that

43:00.000 --> 43:04.240
perspective sort of through thick and thin i got kind of distracted

43:04.240 --> 43:07.200
particularly we talked earlier about Bayesian methods and how beautiful and

43:07.200 --> 43:10.960
how elegant they are and a theoretical physicist it's very appealing to think

43:10.960 --> 43:14.320
of everything from a Bayesian perspective but really what we've seen

43:14.320 --> 43:18.480
today is that the the practical tool that's giving us these

43:18.480 --> 43:22.480
extraordinary advances is neural networks and most of those ideas

43:22.480 --> 43:26.320
go back to the to the mid 1980s to the idea of gradient descent and

43:26.320 --> 43:29.600
and so on a few new a few new tweaks you know we have GPUs we have

43:29.600 --> 43:32.880
reluers we have a few but essentially most of the ideas were

43:32.880 --> 43:36.240
were were were still were around back in the

43:36.240 --> 43:40.080
back in the late 1980s we didn't really understand

43:40.080 --> 43:43.760
the incredible scale at which you need to use them but they only really work

43:43.760 --> 43:47.120
when you have this gargantuan scale of data and compute

43:47.120 --> 43:50.240
and of course we didn't really have GPUs or know how to use them back then

43:50.240 --> 43:53.680
so there were some key developments that sort of unlocked this and made it

43:53.680 --> 43:56.160
possible but i think perhaps if i did something

43:56.160 --> 43:59.280
differently with the amazing benefit of hindsight other than sort of

43:59.280 --> 44:02.480
investing in certain stocks and whatever and all the other things you could do

44:02.480 --> 44:05.520
if you had perfect hindsight i think the other thing i would do is probably

44:05.520 --> 44:08.480
just stay really focused on neural networks because eventually there

44:08.480 --> 44:11.760
that's the technology that came good but i always come back to probability

44:11.760 --> 44:15.520
theory it's very much a unifying idea so for let me just give you a specific

44:15.520 --> 44:18.880
example from prml actually there were two different technologies one called

44:18.880 --> 44:22.400
hidden markoff models that were all the rage and speech recognition back then

44:22.400 --> 44:25.520
another technique called kalman filters that have been used for many years to

44:25.520 --> 44:30.160
to guide spacecraft track aircraft on radar and all sorts of things

44:30.160 --> 44:32.880
it turns out they're essentially the same algorithm

44:32.880 --> 44:37.280
and not only are they the same algorithm but they can be derived from the most

44:37.280 --> 44:40.160
beautifully simple principle you just take the sum and product rule of

44:40.160 --> 44:43.440
probabilities and then you take the idea that a joint probability distribution

44:43.440 --> 44:46.640
has a factorization described by a directed graph

44:46.640 --> 44:50.160
and if you want to so when i was preparing prml

44:50.240 --> 44:53.120
i looked over a bunch of books called kalman filters an introduction to

44:53.120 --> 44:56.160
kalman filters and they become chapter after chapter at the

44:56.160 --> 44:59.120
forward and then chapter after chapter at the reverse equations and so on it

44:59.120 --> 45:03.200
very very complex and very very heavy going but you can derive

45:03.200 --> 45:07.040
the kalman filter and get the hidden markoff model for free in almost a few

45:07.040 --> 45:10.000
lines of of algebra just starting from probability theory

45:10.000 --> 45:13.360
and this idea of factorization it's sort of deep mathematical principle that

45:13.360 --> 45:16.400
operates there and you discover the message passing algorithm and if it's a

45:16.400 --> 45:18.960
tree structure graph it's exact and you have two passes

45:19.280 --> 45:23.280
it's very beautiful very elegant so i love the fact we're exploring

45:23.280 --> 45:26.480
all these many different frontiers but i love the fact we have some at least

45:26.480 --> 45:29.360
some compass to guide us as we as we engage in the

45:29.360 --> 45:32.080
exploration of this combinatorially vast space

45:32.080 --> 45:35.840
yeah it's so interesting my co-host Dr Keith Duggar he always says that he

45:35.840 --> 45:38.240
doesn't need to remember all of the different statistical quantities

45:38.240 --> 45:41.920
because he can re-derive them from first principles it's that nice

45:41.920 --> 45:45.680
but we should move on to AI for science so you're leading this

45:45.680 --> 45:48.320
initiative at Microsoft Research can you tell us about that

45:48.320 --> 45:51.280
yes so at a personal level of course this brings back my

45:51.280 --> 45:56.800
my earlier interest in theoretical physics and chemistry and and biology

45:56.800 --> 45:59.440
and that brings it together with with machine learning

45:59.440 --> 46:03.600
and what many people realized a few years ago

46:03.600 --> 46:09.280
was that of the many areas that machine learning would impact the scientific

46:09.280 --> 46:13.200
the area of scientific discovery would be i think in my view the most important

46:13.200 --> 46:17.760
the reason i say that is because it's actually scientific discovery

46:17.760 --> 46:21.760
that really has allowed humans to go on that trajectory the last few thousand

46:21.760 --> 46:24.880
years not just understanding our place in the universe but to be much more in

46:24.880 --> 46:28.560
control of our own destiny to double our lifespan to cure many

46:28.560 --> 46:30.640
diseases to give us much higher standards of

46:30.640 --> 46:33.840
living to give us a much brighter outlook for the

46:33.840 --> 46:36.960
future than humans humans have traditionally enjoyed

46:36.960 --> 46:40.880
and and that's come through scientific discovery and then the application

46:40.880 --> 46:43.920
of that knowledge and understanding of the world in the form of technologies

46:44.000 --> 46:48.560
agriculture industrial and so on and so i can't think of any more

46:48.560 --> 46:53.360
important application for AI but what's really interesting is it's very clear

46:53.360 --> 46:56.800
that many areas of scientific discovery are being disrupted and when i say

46:56.800 --> 46:59.600
disrupted i'll just give you one simple example

46:59.600 --> 47:03.280
the ability of neural nets machine learning models to

47:03.280 --> 47:08.000
act as emulators for previously were very expensive numerical stimulators

47:08.000 --> 47:11.600
very often gives you a factor of a thousand acceleration

47:11.600 --> 47:14.240
you know we can forecast the weather a thousand times faster with the same

47:14.240 --> 47:17.600
accuracy than we could a few years ago prior to the use of deep learning

47:17.600 --> 47:20.560
now if that were the only thing that was happening

47:20.560 --> 47:23.600
that alone would be a disruption that alone would be worth setting up a team

47:23.600 --> 47:26.560
on AI for science i think actually it's only scratching the surface

47:26.560 --> 47:30.720
but anytime something that's very core very important gets a thousand times

47:30.720 --> 47:34.160
faster it means you can do things that would take

47:34.160 --> 47:38.320
years in a few tens of hours it that really is a disruption it really is

47:38.320 --> 47:42.400
transformational so a couple of years ago i pitched to

47:42.400 --> 47:46.800
our chief technology officer to say look this is a really important field

47:46.800 --> 47:50.560
i'm happy to step down from my role as the lab director of MSR

47:50.560 --> 47:54.240
in in europe and instead i'd like to lead a new team

47:54.240 --> 47:58.000
focusing on AI for science and met with enormous enthusiasm

47:58.000 --> 48:01.200
and so we've been growing and building that team it's very interesting team

48:01.200 --> 48:04.240
it's very multinational we have people on on many different continents in

48:04.240 --> 48:07.440
different countries we've opened new labs in in in

48:07.440 --> 48:10.480
in amsterdam and in in berlin we have teams in

48:10.480 --> 48:14.800
in beijing and in shanghai and folks in in seattle as well

48:14.800 --> 48:19.600
and so very very multidisciplinary very multinational

48:19.600 --> 48:24.160
but with with one thing in common this real excitement and passion for what

48:24.160 --> 48:28.480
machine learning and AI is going to do to really transform and accelerate our

48:28.480 --> 48:31.680
ability to do scientific discovery you were talking about inductive

48:31.680 --> 48:35.440
priors just a second ago and i guess i first learned about this

48:35.440 --> 48:38.400
with the art of you know designing inductive priors and machine learning

48:38.400 --> 48:41.280
from max welling's group they were saying that you know the

48:41.280 --> 48:47.040
remarkable thing is that you can using principles let's say from physics

48:47.040 --> 48:50.800
we can design these inductive priors and we can reduce the size of the

48:50.800 --> 48:53.520
hypothesis class that that we're approximating

48:53.520 --> 48:57.120
and because we know the target function is inside that class we are not

48:57.120 --> 48:59.360
introducing any approximation error and we

48:59.360 --> 49:02.880
we are kind of overcoming some of the curses in in in machine learning by

49:02.880 --> 49:06.080
making the problem tractable which which is amazing but

49:06.080 --> 49:09.840
that's speaking to this kind of principled approach of imbuing

49:09.840 --> 49:13.360
domain knowledge into these systems it's really interesting actually max and i

49:13.360 --> 49:16.160
have a similar trajectory you both did phd's in theoretical physics and then

49:16.160 --> 49:19.680
moved into machine learning and i think we both feel there's a very

49:19.680 --> 49:23.440
important role for inductive bias to play in the use of

49:23.440 --> 49:26.880
machine learning in the scientific domain i think i'm sure everybody is

49:26.880 --> 49:30.720
familiar with the the blog called the bitter lesson by rich sudden

49:30.720 --> 49:34.240
and if any if anybody watching this is is not familiar they should

49:34.240 --> 49:37.520
immediately after this video go and read that blog it's a very short blog

49:37.520 --> 49:40.960
and without giving too much of a spoiler he essentially says that

49:40.960 --> 49:45.120
every attempt by people to improve the performance of machine learning by

49:45.120 --> 49:48.960
building in prior knowledge building in what we call inductive biases

49:48.960 --> 49:52.640
into the models it produces some improvement and then but very quickly

49:52.640 --> 49:55.520
it's overtaken by somebody else who just has more data

49:55.520 --> 49:59.200
and and that indeed is a bitter lesson and and it's a wonderful blog and

49:59.200 --> 50:02.160
people should i i've read it many times i think people should you know probably

50:02.160 --> 50:05.600
read that once a month and and it's it's very inspiring

50:05.600 --> 50:08.640
but i think there may be exceptions and i think the scientific domain

50:08.640 --> 50:12.400
is one where inductive biases for the foreseeable future will be

50:12.400 --> 50:15.280
extremely important sort of almost contrary to the bitter lesson

50:15.280 --> 50:19.280
and a couple of reasons for this one is that the the inductive biases we have

50:19.280 --> 50:24.640
a not not of the kind let's say let's say linguistics or something which is

50:24.720 --> 50:29.600
any domain where which is based on human expertise acquired through experience

50:29.600 --> 50:33.680
because a person who's had a lot of experience over a number of years and

50:33.680 --> 50:38.160
formulated some sort of rules of thumb that guide them that's exactly what

50:38.160 --> 50:42.000
machine learning is very good at processing very large amounts of data

50:42.000 --> 50:45.840
and and inducing the the the rules as it were the patterns

50:45.840 --> 50:50.160
within that data so i think that kind of inductive bias is

50:50.160 --> 50:53.440
typically harmful and and i think the bitter lesson will certainly apply

50:53.440 --> 50:56.560
there but in the scientific domain it's rather different first of all the

50:56.560 --> 50:59.360
inductive biases we have are very rigorous we have

50:59.360 --> 51:02.960
the idea of conservation of energy conservation of momentum we have

51:02.960 --> 51:07.040
symmetries if i have a molecule in a vacuum it has a certain energy

51:07.040 --> 51:10.080
if i rotate the molecule the representation of the coordinates of all the

51:10.080 --> 51:13.520
atoms changes wildly in the computer but the energy is the same

51:13.520 --> 51:17.440
so we have this very rigorous inductive bias we also know that the world at the

51:17.440 --> 51:21.280
atomic level is described exquisitely well by by Schrodinger's equation

51:21.440 --> 51:24.320
sprinkling a few relativistic effects and you've got an amazingly accurate

51:24.320 --> 51:28.160
description of the world but it's way too complex to just solve it directly

51:28.160 --> 51:31.840
or is exponentially costly in the number of electrons but nevertheless we have

51:31.840 --> 51:37.040
this bedrock of of really understanding the laws that govern the universe

51:37.040 --> 51:40.880
and so and so i think that's the first the first thing we have very rigorous

51:40.880 --> 51:44.160
priors that we believe in deeply it's not that we think conservation of energy

51:44.160 --> 51:46.560
doesn't work we know that we know that it's true

51:46.560 --> 51:50.000
the second thing is that we're operating in a data scarce regime so large

51:50.000 --> 51:53.920
language models are able to use very large quantities of internet scale

51:53.920 --> 51:57.760
quantities of human created data whether it's in the form of you know

51:57.760 --> 52:02.320
whether it's wikipedia or whether it's just scientific papers or any of the

52:02.320 --> 52:06.240
output of humans almost is potentially material on which which large models

52:06.240 --> 52:10.640
can feed they're in a very data rich regime and can go to scale

52:10.640 --> 52:14.480
and and so the bitter lesson i think really kicks in there in the scientific

52:14.480 --> 52:17.920
domain the data might come from simulations which are

52:17.920 --> 52:21.840
computational and expensive or it might come from lab experiments which are

52:21.840 --> 52:26.240
which are expensive and the data is is limited so we're operating

52:26.240 --> 52:29.840
usually in a data scarce regime so we have relatively limited data

52:29.840 --> 52:33.040
and we have very rigorous prior knowledge and so the balance between

52:33.040 --> 52:37.520
the data and the inductive bias is very different because of course the no

52:37.520 --> 52:41.280
free lunch theorem says you can't learn purely from data you have to have some

52:41.280 --> 52:45.680
form of inductive bias and in the case of a transformer it's a

52:45.680 --> 52:48.960
very lightweight form of inductive bias we believe there's a there's a deep

52:48.960 --> 52:52.400
hierarchy there's some you know data dependent

52:52.400 --> 52:57.120
self-attention but but really that's it and the rest is determined from the data

52:57.120 --> 53:01.040
in science there's much more scope for bringing in these inductive biases there's

53:01.040 --> 53:04.400
much more need to bring in the inductive biases and that also

53:04.400 --> 53:08.960
incidentally again in my personal very biased opinion makes the

53:08.960 --> 53:12.800
application of machine learning and ai to the sciences the most exciting

53:12.800 --> 53:15.920
frontier of AI machine learning because it's the one that's

53:15.920 --> 53:20.320
richest in terms of the creativity and also in terms of the need to bring in

53:20.320 --> 53:24.160
some of that beautiful mathematics that that underpins the universe

53:24.160 --> 53:27.680
yeah so so fascinating I mean could we just linger just just for a second on

53:27.680 --> 53:31.920
that so rich Sutton in his bitter lesson essay he explicitly called out

53:31.920 --> 53:35.840
symmetries as being you know he was warning against human designed

53:35.840 --> 53:40.800
artifacts in in these models and I mean max welling as you say famously

53:40.800 --> 53:44.560
built these gauge equivariant neural networks bringing in his

53:44.560 --> 53:48.960
physics knowledge and so I'm just trying to understand the spectrum between

53:48.960 --> 53:53.920
high-resolution physical priors and the kind of macroscopic human knowledge

53:53.920 --> 53:56.960
that that we learn which is presumably brittle

53:56.960 --> 54:01.520
is it just that we think that these physical priors are fundamental

54:01.520 --> 54:05.440
and that's that's a that's a perfectly acceptable way to constrain

54:05.440 --> 54:09.440
the search space but these high-level priors are brittle

54:09.440 --> 54:12.640
yes I think I think the the the prior knowledge that comes from human

54:12.640 --> 54:16.560
experience is is is more of that brittle kind

54:16.560 --> 54:20.080
because the machine can see far more examples than a human can in a

54:20.080 --> 54:24.160
lifetime and can can do a more systematic job

54:24.160 --> 54:27.600
of looking across all of that data we're not

54:27.600 --> 54:30.720
not subject to say recency bias and those sorts of things

54:30.720 --> 54:33.760
so I think that kind of prior knowledge is is one where

54:33.760 --> 54:38.640
where scale and data will will win whereas the the prior knowledge that we

54:38.640 --> 54:42.000
have from the physical laws in a sense is much more rigorous and symmetry

54:42.000 --> 54:45.040
is is is very powerful it's sometimes said that

54:45.040 --> 54:47.760
physics more or less is symmetry that's almost

54:47.760 --> 54:51.440
yes right so conservation conservation laws arise from

54:51.440 --> 54:54.160
symmetry you know translation in variance in

54:54.160 --> 54:57.840
spacetime gives you conservation of energy and momentum

54:57.840 --> 55:01.760
and you know gauge symmetry of the electromagnetic field gives you charge

55:01.760 --> 55:06.160
conservation and so on and and so these are very very rigorous laws that apply

55:06.160 --> 55:09.200
from symmetry but you know even if you take a data-driven approach

55:09.200 --> 55:12.320
people often use data augmentation if you know that an object doesn't depend

55:12.320 --> 55:14.720
his identity doesn't depend on where it is in the image you might

55:14.720 --> 55:17.600
you know make lots of random translations of your data to augment

55:17.600 --> 55:21.680
your data so data augmentation can be a data-driven way of building in those

55:21.680 --> 55:24.320
symmetries but now when we have very rich prior

55:24.320 --> 55:27.360
knowledge I'll come back to Schrodinger's equation it describes the world with

55:27.360 --> 55:31.280
exquisite precision at the atomic level but solving it is very very expensive

55:31.280 --> 55:35.280
and so what we can do is we can cache those computations we call it the fifth

55:35.280 --> 55:38.480
paradigm of scientific discovery which is a rather fancy term but the idea is

55:38.480 --> 55:42.960
very simple is that instead of taking a conventional numerical

55:42.960 --> 55:46.560
solver and using it to solve something like Schrodinger's equation or something

55:46.560 --> 55:50.000
called density functional theory instead of solving that directly to solve your

55:50.000 --> 55:53.840
problem instead you use that simulator to

55:53.840 --> 55:56.960
generate training data and use that training data to train a

55:56.960 --> 56:00.880
machine learning emulator and then that machine learning emulator

56:00.880 --> 56:04.400
can now emulate the simulator but typically three or four

56:04.400 --> 56:08.800
orders of magnitude faster so provided you use it a lot and you amortize the one

56:08.800 --> 56:12.080
off cost of generating the training data and doing the training if you're going

56:12.080 --> 56:16.960
to use it many many times overall it becomes dramatically faster dramatically

56:16.960 --> 56:20.560
more efficient than using the simulator and that that's just one of the

56:20.560 --> 56:24.080
breakthroughs we're seeing in this space so first of all um there's there's a

56:24.080 --> 56:27.680
spectrum as you say of we could just train on lots of data or we could

56:27.680 --> 56:32.240
augment the data or we could make a simulator for the data and then we can

56:32.240 --> 56:36.000
train a machine learning model and as we were just speaking to

56:36.000 --> 56:39.120
these inductive priors they are so high resolution

56:39.120 --> 56:44.080
that we are not restricting the target function that that that we want to

56:44.080 --> 56:46.960
learn and we can make quite a principled argument about that

56:46.960 --> 56:50.880
but the one question to me is there's a kind of

56:50.880 --> 56:54.800
I don't know whether it's best to frame it as exploration versus exploitation but

56:54.800 --> 56:58.000
there needs to be some amount of going off-piste

56:58.000 --> 57:02.640
so we define the structure and we we essentially build a generative model

57:02.640 --> 57:05.200
and we can generate a whole bunch of trajectories

57:05.200 --> 57:08.880
but could it ever be the case that we wouldn't have enough

57:08.880 --> 57:13.280
variance to find something interesting there's a very interesting question

57:13.280 --> 57:17.200
about the the overall scientific method of formulating hypotheses running

57:17.200 --> 57:20.400
tests evaluating those hypotheses refining the hypotheses running more

57:20.400 --> 57:23.040
experiments and so on that that scientific loop

57:23.040 --> 57:26.160
I think machine learning will have an important role to play there because

57:26.160 --> 57:29.760
data is becoming very high dimensional very high throughput humans can't

57:29.760 --> 57:32.720
analyze this data anymore a human can't directly look at the output of the

57:32.720 --> 57:36.160
large hadron collider with its you know petabytes a second or whatever it is

57:36.160 --> 57:41.120
pouring off we we need machines to help us but again I think the human

57:41.120 --> 57:44.480
rises to the level of the conductor of the orchestra as it were they no longer

57:44.480 --> 57:48.160
have to do things by hand machines are helping to accelerate that

57:48.160 --> 57:51.200
and and I think the machines can help accelerate the creative process

57:51.200 --> 57:55.200
potentially by pointing to anomalies or highlighting patterns in the data and

57:55.200 --> 57:58.480
so on but very much with the human scientist in the loop

57:58.480 --> 58:01.520
but but even coming down from those sort of lofty more sort of philosophical

58:01.520 --> 58:05.280
considerations just to the the practicalities when we talk about discovery

58:05.280 --> 58:07.840
we're also interested just the very practical method of how we how do we

58:07.840 --> 58:10.880
discover a new drug or how do we discover a new material

58:10.880 --> 58:14.560
so scientific discovery also means that that that that that very pragmatic

58:14.560 --> 58:18.080
near-term approach and there we're seeing really dramatic

58:18.080 --> 58:21.680
acceleration through the the concept of this emulator

58:21.680 --> 58:25.120
inner ability to explore the combinatorically vast space of new

58:25.120 --> 58:29.120
molecules and new materials exploring those spaces efficiently to find

58:29.120 --> 58:33.520
potential candidates that might be new drugs or new

58:33.520 --> 58:36.800
new materials for batteries or other other forms of green energy

58:36.800 --> 58:40.880
so that that alone is a very exciting frontier I think

58:40.880 --> 58:45.440
it's so interesting so searching these space I mean drug discovery is an

58:45.440 --> 58:48.720
interesting one I think you spoke about sustainability as well as

58:48.720 --> 58:52.960
another application you can speak to but how do you identify an

58:52.960 --> 58:57.120
interesting drug so the drug discovery process

58:57.120 --> 59:00.480
starts first of all with the disease and trying to

59:00.480 --> 59:03.360
first of all deciding we want to go tackle a particular disease

59:03.360 --> 59:07.600
and then finding a suitable target so the the standard so-called small molecule

59:07.600 --> 59:10.400
paradigm which is where most drugs are today

59:10.400 --> 59:14.640
they're small synthetic organic molecules that bind with a particular

59:14.640 --> 59:17.600
protein so pharma companies will will will

59:17.600 --> 59:21.360
spend a lot of time identifying targets so say a protein that has a particular

59:21.360 --> 59:25.360
region with a molecule combined can combined to

59:25.360 --> 59:28.320
and therefore can influence the behavior of that protein switching on or

59:28.320 --> 59:31.680
switching off some part of that disease pathway and breaking the chain of

59:31.680 --> 59:35.040
disease so the challenge then is to find a small

59:35.040 --> 59:37.840
molecule that first of all has the property that it binds with the target

59:37.840 --> 59:41.520
protein that's the first step but there are many other things that it has to do

59:41.520 --> 59:45.520
it has to be absorbed into the body it has to be metabolized and excreted it

59:45.520 --> 59:48.400
mustn't and particularly mustn't be toxic it mustn't bind to anything

59:48.400 --> 59:51.520
many other proteins in the body and cause bad things to happen

59:51.520 --> 59:55.120
so what you have is a very large space of molecules usually estimated around

59:55.120 --> 59:59.280
10 to the power 60 potential drug-like molecules

59:59.280 --> 01:00:02.400
and out of that enormous space of 10 to the 60 you're trying to find

01:00:02.400 --> 01:00:05.680
an example that meets all of these many many criteria

01:00:05.680 --> 01:00:10.080
and so one approach is to generate a lot of candidates but in computationally

01:00:10.080 --> 01:00:13.520
and then screen them one by one for different properties that screening

01:00:13.520 --> 01:00:17.680
process the more that can be done in silico rather than

01:00:17.680 --> 01:00:20.960
in a wet lab the faster it can be done and the

01:00:20.960 --> 01:00:24.960
the larger the search space can be and therefore the bigger the fraction of

01:00:24.960 --> 01:00:28.960
that space of possibilities you can explore hopefully thereby increasing

01:00:28.960 --> 01:00:32.080
the chances of finding a good candidate because many attempts to find a drug for

01:00:32.080 --> 01:00:35.600
a disease simply fail nothing nothing eventually comes of it

01:00:35.600 --> 01:00:38.320
so increasing the probability of success increasing the speed of that

01:00:38.320 --> 01:00:41.440
discovery process so in all of that there are many places

01:00:41.520 --> 01:00:44.640
where machine learning could could be disruptive

01:00:44.640 --> 01:00:48.880
so on that process of I guess you're describing

01:00:48.880 --> 01:00:53.120
you generate candidates and then you almost discriminate interesting ones

01:00:53.120 --> 01:00:57.840
and then you rinse and repeat in a kind of iterative process

01:00:57.840 --> 01:01:01.840
let me give you a concrete example so we've done some work looking at

01:01:01.840 --> 01:01:06.000
tuberculosis so tuberculosis kills something like 1.3 million people very

01:01:06.000 --> 01:01:09.520
sadly back in 2022 which is the last year we have we have data

01:01:09.520 --> 01:01:12.320
and I might seem surprising because we have we have antibiotics we have drugs

01:01:12.320 --> 01:01:14.880
for tuberculosis why are so many people dying

01:01:14.880 --> 01:01:18.560
and one core reason is that the the bacterium is evolving to develop drug

01:01:18.560 --> 01:01:21.840
resistance and so there's a search on for new for new drugs

01:01:21.840 --> 01:01:24.960
so maybe I'll just take a moment to explain some of the architecture and

01:01:24.960 --> 01:01:28.400
get into a little bit of the sort of the techie details of this

01:01:28.400 --> 01:01:32.320
so we wanted a way of finding we know what the target is we've been told what

01:01:32.320 --> 01:01:35.280
the target protein is the target has a region called a pocket and we're

01:01:35.280 --> 01:01:38.320
looking for molecules that are bind tightly with that pocket region on the

01:01:38.320 --> 01:01:42.240
protein and and so the way the way we approached

01:01:42.240 --> 01:01:45.760
this was first of all build a language model but not a language model

01:01:45.760 --> 01:01:48.560
for human language but for the language of molecules

01:01:48.560 --> 01:01:51.440
so we first of all take there's a representation representation called

01:01:51.440 --> 01:01:54.480
smiles it's a way of taking a it's an acronym but just a way of

01:01:54.480 --> 01:01:57.600
taking a molecule and describing as a one-dimensional string

01:01:57.600 --> 01:02:00.560
and so you first of all take a large database of I don't know 10 million

01:02:00.560 --> 01:02:03.920
molecules it represented the smile strings and you treat them like the

01:02:03.920 --> 01:02:07.840
tokens for a for a transformer model and by getting it to predict the next token

01:02:07.840 --> 01:02:12.240
the next element of the smile string you build a transformer based language

01:02:12.240 --> 01:02:14.720
model that can speak the language of molecules

01:02:14.720 --> 01:02:18.160
so it can it can run generatively and it can create new

01:02:18.160 --> 01:02:22.080
create new molecules as output so you can think of that as kind of like a

01:02:22.080 --> 01:02:25.600
foundation language model but speaking the language of molecules

01:02:25.600 --> 01:02:29.120
now we want to generate molecules but not just any molecules we want molecules

01:02:29.120 --> 01:02:32.800
which bind with a particular target protein so we have the target protein

01:02:32.800 --> 01:02:35.280
in particular it's the pocket region that we're interested in

01:02:35.280 --> 01:02:38.480
so we can give it the amino acid sequence of the protein as input

01:02:38.480 --> 01:02:42.080
but we need more than that we need the geometry of the of the pocket and this

01:02:42.080 --> 01:02:44.560
is where some of those inductive biases come in so we

01:02:44.560 --> 01:02:48.400
we need to have representations of the geometry of the atoms in that form that

01:02:48.400 --> 01:02:50.960
pocket but a way that represents these

01:02:50.960 --> 01:02:55.120
equivalences and so they're encoded as input to a transformer model that learns

01:02:55.120 --> 01:02:59.040
a representation for the protein pocket and the final piece we need as you said

01:02:59.040 --> 01:03:02.160
we want to do this iteratively we want to take a good molecule and make it a

01:03:02.160 --> 01:03:05.520
better molecule rather than just searching blindly across a space of

01:03:05.520 --> 01:03:08.480
10 to the 60 possibilities and so the other thing we want to

01:03:08.480 --> 01:03:12.720
provide as input is a molecule a descriptor of a

01:03:12.720 --> 01:03:16.880
a known small molecule that does bind with the pocket already

01:03:16.880 --> 01:03:20.400
and but we want to do this in a way that creates variability and we actually use

01:03:20.400 --> 01:03:23.840
a variational autoencoder to create that representation and

01:03:23.840 --> 01:03:27.600
that's the an encoder that trun translates the molecule into a latent

01:03:27.600 --> 01:03:30.480
space and we can sample from that latent space

01:03:30.480 --> 01:03:33.840
and then the this language model the smiles

01:03:33.840 --> 01:03:37.680
language model can attend to both the output of the variational autoencoder

01:03:37.680 --> 01:03:40.640
and the output of the protein encoder using cross-attention

01:03:40.640 --> 01:03:44.160
and so what we've done there is I think rather tastefully combined

01:03:44.160 --> 01:03:48.720
some elements from you know states of the arts at modern deep learning

01:03:48.720 --> 01:03:52.800
the result then can be can be trained end to end using a database

01:03:52.800 --> 01:03:56.400
of known other proteins that are known to bind efficiently to

01:03:56.560 --> 01:04:00.480
small molecules and once the system is trained we can now

01:04:00.480 --> 01:04:03.840
provide as input the known target for tuberculosis

01:04:03.840 --> 01:04:07.600
and some known molecules that bind with this and then we can

01:04:07.600 --> 01:04:10.720
iteratively refine those molecules at the output we get molecules that have

01:04:10.720 --> 01:04:13.600
better binding efficiency and we're able to increase the binding

01:04:13.600 --> 01:04:17.600
effectiveness by two orders of magnitude and so we now have states of the art

01:04:17.600 --> 01:04:21.200
molecules in terms of binding efficiency to this

01:04:21.200 --> 01:04:25.840
to this target protein of course we can't do the wet lab experiments ourselves

01:04:25.840 --> 01:04:29.440
we partner with an organization called giddy the global health drug discovery

01:04:29.440 --> 01:04:32.880
institute they've synthesized the molecules that we've we've

01:04:32.880 --> 01:04:36.400
generated and measured their their binding efficacy

01:04:36.400 --> 01:04:40.560
and so we're very very excited about this and of course the next stage now is to

01:04:40.560 --> 01:04:44.400
take that as a starting point and further refine and optimize those molecules and

01:04:44.400 --> 01:04:47.360
and try to address all those other requirements that we have for before a

01:04:47.360 --> 01:04:50.080
drug can actually be tested on humans in terms of its

01:04:50.080 --> 01:04:52.720
toxicity and metabolism and all and all the other things

01:04:52.720 --> 01:04:57.440
but i think it's just a a very a very nice example of almost like a first step

01:04:57.440 --> 01:05:01.360
in using modern deep learning architecture to accelerate the process of drug

01:05:01.360 --> 01:05:04.640
discovery and already we have i think really quite a

01:05:04.640 --> 01:05:08.400
spectacular success given that we're we're kind of newcomers to this

01:05:08.400 --> 01:05:12.080
to this field partnering with experts domain experts with the wet lab

01:05:12.080 --> 01:05:16.240
experience and the wet lab capability to me this is the beginning of a very

01:05:16.240 --> 01:05:19.280
exciting journey that sounds incredible

01:05:19.280 --> 01:05:23.360
is there any kind of representational transfer between the the models so for

01:05:23.360 --> 01:05:27.840
example you're talking about this this geometric

01:05:27.840 --> 01:05:32.240
prior model and generating tokens to go into the language model

01:05:32.240 --> 01:05:35.600
because just using language models by the way is a fascinating approach i

01:05:35.600 --> 01:05:39.760
spoke with christian sogeddy and he was doing mathematical conjecturing

01:05:39.760 --> 01:05:43.600
just using language models you know just just taking mathematical

01:05:43.600 --> 01:05:47.120
constructions and putting them into language and they used to use graph neural

01:05:47.120 --> 01:05:50.480
networks for this and so i guess the question is

01:05:50.480 --> 01:05:54.800
could you kind of bootstrap it with a you know with an inductive

01:05:54.800 --> 01:06:00.720
principled model and then kind of just train using the language model afterwards

01:06:00.720 --> 01:06:03.440
i think i think the general principle there's a very powerful one so the idea

01:06:03.440 --> 01:06:06.400
of borrowing strength from other domains and i think we're seeing this time and

01:06:06.400 --> 01:06:11.040
time again in deep learning that that the machine learning models are able to

01:06:11.040 --> 01:06:14.720
extract some general patterns from even from one domain and translate them into

01:06:14.720 --> 01:06:17.680
completely different domain we talked earlier about large language models

01:06:17.680 --> 01:06:21.600
being getting better at writing code if they've also got exposure to

01:06:21.600 --> 01:06:24.880
to poetry or something is seemingly quite irrelevant there's some

01:06:24.880 --> 01:06:28.000
there's something quite deep and subtle going on there but perhaps in a less

01:06:28.000 --> 01:06:30.960
subtle way it's clear there's a sort of a language of

01:06:30.960 --> 01:06:35.040
molecules there's a language of materials and that by

01:06:35.040 --> 01:06:38.640
building models that have a broader exposure to that language

01:06:38.640 --> 01:06:43.040
they almost invariably will become better at the specific tasks that we want to

01:06:43.040 --> 01:06:46.160
to apply them to so i think there is a general principle at heart there

01:06:46.160 --> 01:06:49.600
yeah it's so interesting because i i used to think that

01:06:49.600 --> 01:06:52.880
that perhaps the drawback of these inductive prior models is that

01:06:52.880 --> 01:06:57.520
it was one inductive prior per model but this ability potentially to

01:06:57.520 --> 01:07:01.680
bootstrap a foundational model that can do all of the things

01:07:01.680 --> 01:07:05.040
that's really interesting i think the most powerful inductive biases and the

01:07:05.040 --> 01:07:07.920
ones we focus on are really those very general ones where

01:07:07.920 --> 01:07:11.920
symmetry says just very fundamental properties of the universe and we

01:07:12.080 --> 01:07:15.040
want we want those really baked into the models i think

01:07:15.040 --> 01:07:19.600
the the the sort of intuitions we have about more specific domains i think

01:07:19.600 --> 01:07:22.560
they can perhaps lead us astray because they're based on

01:07:22.560 --> 01:07:25.440
our experience of much more limited domains i think this is where the

01:07:25.440 --> 01:07:28.400
machines can be can be much better at

01:07:28.400 --> 01:07:32.240
processing and interpreting large volumes of data and drawing regularities

01:07:32.240 --> 01:07:35.200
out of that out of data in a more systematic way

01:07:35.200 --> 01:07:39.840
okay okay and just before we we leave this this is a bit of a galaxy brain

01:07:39.840 --> 01:07:43.040
question and and that that's parlance that all the kids are using these days

01:07:43.040 --> 01:07:47.520
by the way but how fundamental is is our physical

01:07:47.520 --> 01:07:50.960
knowledge you know the question is like we are we're designing these inductive

01:07:50.960 --> 01:07:54.800
priors as if they are fundamental but folks like Steven Wolfram for

01:07:54.800 --> 01:07:57.840
example argue that there's there's a deeper

01:07:57.840 --> 01:08:01.040
ontological reality you know might be a graph cellular automaton or something

01:08:01.040 --> 01:08:04.080
like that and is that something you think

01:08:04.080 --> 01:08:07.680
about the kind of the gap between our models and what reality is

01:08:07.680 --> 01:08:10.640
so i think first of all one of the greatest scientific discoveries of all

01:08:10.640 --> 01:08:14.000
time is the fact the universe can be described by simple laws that that is

01:08:14.000 --> 01:08:16.640
not obvious a priori that itself is perhaps the most

01:08:16.640 --> 01:08:19.760
profound discovery you know really going back to Newton but we found it time

01:08:19.760 --> 01:08:22.720
and time again what we've also found is that the

01:08:22.720 --> 01:08:25.200
our understanding of the universe as it exists today

01:08:25.200 --> 01:08:28.800
has has it's almost like onions we're peeling way layers of onions

01:08:28.800 --> 01:08:32.000
you know Newton if you want to navigate a spacecraft to Jupiter you still use

01:08:32.000 --> 01:08:35.920
Newton's laws of motion and Newton's law of gravity it's just fine

01:08:35.920 --> 01:08:39.040
it doesn't mean we believe it's exact description of nature we've now got

01:08:39.040 --> 01:08:42.400
deeper descriptions of nature we understand relativity for example

01:08:42.400 --> 01:08:45.600
general relativity tells us that actually Newton's second law of

01:08:45.600 --> 01:08:48.240
motion or Newton's Newton's law of gravity rather is just an

01:08:48.240 --> 01:08:50.080
approximation the inverse square law is a pretty good

01:08:50.080 --> 01:08:53.360
approximation but we've got a much better description now

01:08:53.360 --> 01:08:58.080
but but it's it's it's hard to say that we've we've found the ultimate answer

01:08:58.080 --> 01:09:01.120
it's rather that human knowledge is or just always

01:09:01.120 --> 01:09:04.800
stands on that that edge of what we don't understand and scientific discovery

01:09:04.800 --> 01:09:07.760
is always about exploring the things we don't understand

01:09:07.760 --> 01:09:12.160
working out whether you know whether the laws actually do hold

01:09:12.160 --> 01:09:15.440
and the anomaly we see in the data is is because of some

01:09:15.440 --> 01:09:18.480
phenomenon that we haven't yet observed I mean this is how Neptune was

01:09:18.480 --> 01:09:21.840
discovered by by seeing that the planets were not behaving as they

01:09:21.840 --> 01:09:24.480
should do according to Newton's laws Newton's laws were just fine there's

01:09:24.480 --> 01:09:28.880
just another planet perturbing them or is the procession of the perihelion of

01:09:28.880 --> 01:09:31.680
Mercury because because there's another no it's because

01:09:31.680 --> 01:09:34.320
actually Newton's law of gravity isn't quite right we need

01:09:34.320 --> 01:09:38.160
relativity to understand that so I think scientific

01:09:38.160 --> 01:09:42.640
exploration as far as I can tell has no particular end in sight it's

01:09:42.640 --> 01:09:46.080
rather that we have things that we understand and there are new frontiers

01:09:46.080 --> 01:09:48.560
you know when I was when I was a teenager getting

01:09:48.560 --> 01:09:52.880
excited by physics I love reading about relativity and quantum physics but it's

01:09:52.880 --> 01:09:55.520
kind of depressing because I thought you know it's kind of born

01:09:55.520 --> 01:09:58.720
you know 50 years too late or whatever you know all the exciting stuff happened

01:09:58.720 --> 01:10:02.400
at the beginning of the 20th century it's kind of all been done

01:10:02.400 --> 01:10:05.920
but now we have you know dark matter and dark energy and we realize that most of

01:10:05.920 --> 01:10:08.720
the universe isn't sitting on the periodic table that I learned about in

01:10:08.720 --> 01:10:12.720
schools and actually I needn't have worried you know

01:10:12.720 --> 01:10:17.520
I think it was at Vannevar Bush who called it the endless frontier

01:10:17.520 --> 01:10:20.720
the you know science is an endless frontier there is just there is

01:10:20.720 --> 01:10:24.560
always more to explore and always more to learn so whether the particular ideas

01:10:24.560 --> 01:10:27.520
you alluded to have substance I don't know at the end of the day the scientific

01:10:27.520 --> 01:10:30.560
method will tell us if they have predictive capabilities they can predict

01:10:30.560 --> 01:10:32.720
new phenomena that we weren't aware of before

01:10:32.720 --> 01:10:38.240
then you know then they have they have credence as far as a scientist

01:10:38.240 --> 01:10:41.600
is concerned but ultimately you know we still stick to the

01:10:41.600 --> 01:10:44.640
scientific method it's about our ability to make predictions that are

01:10:44.640 --> 01:10:47.520
testable experimentally and if they stand up to the test of

01:10:47.520 --> 01:10:50.560
experiment then we give more weight to those to those hypotheses and

01:10:50.560 --> 01:10:53.120
eventually they're elevated to the stages of theory

01:10:53.120 --> 01:10:57.120
I often wonder about the horizon of our cognition

01:10:57.120 --> 01:11:01.440
you know what we are capable of understanding and we tend to understand

01:11:01.440 --> 01:11:05.520
things using high-level metaphors information is a great example of that

01:11:05.520 --> 01:11:08.080
so a lot of people talk about the universe as

01:11:08.080 --> 01:11:11.840
information this agential view is quite interesting so

01:11:11.840 --> 01:11:16.480
modeling everything as agents and it might well be possible that the

01:11:16.480 --> 01:11:19.920
universe is just so strange and alien that we could never possibly

01:11:19.920 --> 01:11:22.720
understand it so there's a bit of an interplay between

01:11:22.720 --> 01:11:27.840
our kind of intelligibility and and our models and what it is

01:11:27.840 --> 01:11:31.360
the universe clearly is completely unintelligible in the sense of

01:11:31.360 --> 01:11:35.040
nobody can really think about quantum physics

01:11:35.040 --> 01:11:37.920
it completely defies our everyday intuitions that we learn at this sort

01:11:37.920 --> 01:11:41.440
of macroscopic level so I think we have to accept already that the

01:11:41.440 --> 01:11:44.800
universe is described mathematically that's our precise description

01:11:44.800 --> 01:11:47.760
and then we have kind of metaphors about waves and particles and so on but

01:11:47.760 --> 01:11:50.480
they none of them none of them really work properly they're just

01:11:50.880 --> 01:11:54.400
crutches to lean on but ultimately it's a mathematical description

01:11:54.400 --> 01:11:57.920
but that that is that is also very interesting the fact that the world is

01:11:57.920 --> 01:12:01.280
described by mathematics that by making little marks on a piece of paper you

01:12:01.280 --> 01:12:03.760
can discover a new planet that's quite incredible

01:12:03.760 --> 01:12:07.440
shifting over to deep learning a little bit more more broadly

01:12:07.440 --> 01:12:11.200
and we were touching on this already but the landscape is dominated by

01:12:11.200 --> 01:12:15.680
transformers architectures what what are your broad thoughts about that

01:12:15.680 --> 01:12:19.600
like any field I think machine learning has its sort of its fads and its waves

01:12:19.600 --> 01:12:22.640
something works really well and then everybody latches onto that and makes

01:12:22.640 --> 01:12:24.720
use of that and that that's all well and good

01:12:24.720 --> 01:12:28.480
I'd be kind of surprised if the transformer is the last word in deep

01:12:28.480 --> 01:12:32.640
learning if that's the the the the the architecture we use forever more

01:12:32.640 --> 01:12:35.360
but it clearly works very well and we haven't reached the end of its

01:12:35.360 --> 01:12:38.320
capabilities by any means so it makes a lot of sense to

01:12:38.320 --> 01:12:42.320
exploit the transformer architecture in applications and see how much we can

01:12:42.320 --> 01:12:45.600
gain from that at the same time there's clearly opportunities to think about

01:12:45.600 --> 01:12:48.880
the limitations of transformers the computational costs can we do the same

01:12:48.880 --> 01:12:52.560
thing you know with better scaling if you want longer context windows and all

01:12:52.560 --> 01:12:55.600
the rest so there's plenty of interesting research I think to be done in

01:12:55.600 --> 01:12:58.080
in new architectures as well so I think we need both

01:12:58.080 --> 01:13:01.920
so you know here's another galaxy brain question why does deep learning work

01:13:01.920 --> 01:13:05.120
you know because on on the face of it it shouldn't work it shouldn't train it

01:13:05.120 --> 01:13:08.720
shouldn't generalize and they've been an absolutely

01:13:08.720 --> 01:13:13.760
remarkable success why is that so I think first of all at one level you

01:13:13.760 --> 01:13:16.240
could say well we understand why they work we're fitting

01:13:16.240 --> 01:13:19.440
nonlinear functions we're kind of doing curve fitting in high-dimensional space

01:13:19.440 --> 01:13:23.200
we need some some generalization and it comes out to no free lunch theorems of

01:13:23.200 --> 01:13:26.320
inductive biases perhaps it's smoothness continuity perhaps it's something

01:13:26.320 --> 01:13:30.640
more more constraining than that so at one level it's sort of not surprising I

01:13:30.640 --> 01:13:34.000
can I can fit a polynomial to a bunch of data points and by gradient

01:13:34.000 --> 01:13:37.360
methods and I can make good predictions for sort of intermediate points

01:13:37.360 --> 01:13:40.320
just we're just generalizing that to more data and higher dimensions

01:13:40.320 --> 01:13:43.600
so so one level I say no it's not at all surprising they work

01:13:43.600 --> 01:13:47.040
at a different level of course the fact they work so well is remarkable

01:13:47.040 --> 01:13:50.960
but the way in which they work is very interesting so one thing which

01:13:50.960 --> 01:13:55.600
if we go back to the earlier years of machine learning and certainly back to

01:13:55.600 --> 01:13:58.320
the world of statistics the idea that you would fit models that

01:13:58.320 --> 01:14:01.280
have way more parameters than the number of data points would be

01:14:01.280 --> 01:14:04.880
clearly insane to any self-respecting statistician we never would have and

01:14:04.880 --> 01:14:08.800
perhaps that's nobody why nobody really tried it very much and yet we have

01:14:08.800 --> 01:14:12.480
these odd phenomena whereby you know the training error goes to zero and yet the

01:14:12.480 --> 01:14:15.440
test error continues to come down even though the training error is already

01:14:15.440 --> 01:14:20.000
at zero something about stochastic gradient descent

01:14:20.000 --> 01:14:22.880
the actual training process clearly is important there it's not just

01:14:22.880 --> 01:14:25.440
here's a cost function we find the global minimum it's a property of the

01:14:25.440 --> 01:14:28.640
global minimum no there are many many global minimum that all

01:14:28.640 --> 01:14:32.480
have zero error some solutions will clearly overfit others generalize

01:14:32.480 --> 01:14:34.640
well and so there's something about the training

01:14:34.640 --> 01:14:38.160
process that we need to understand so I think there's a lot of research to be

01:14:38.160 --> 01:14:41.360
to be done in why do they work so well I think it's an open question

01:14:41.360 --> 01:14:44.400
we can describe the model we can say lots of things about the model we can

01:14:44.400 --> 01:14:47.600
say because it has this and this and that number of layers

01:14:47.600 --> 01:14:51.200
therefore the structure of the space has this and that properties and it divides

01:14:51.200 --> 01:14:53.760
it up into such and such regions and so on

01:14:53.760 --> 01:14:57.280
those are true I don't know whether that gives us real insights into why it's

01:14:57.280 --> 01:15:00.640
working I think there are some some very much open questions there it strikes

01:15:00.640 --> 01:15:03.600
me a little bit like neuroscience you know we have the human brain it does

01:15:03.600 --> 01:15:06.640
these amazing things and we can get more and more and

01:15:06.640 --> 01:15:09.760
richer and richer data about which neurons are firing and when and how the

01:15:09.760 --> 01:15:12.240
firings are correlated we can learn something about the

01:15:12.240 --> 01:15:15.760
the underlying machinery this is a bit like neuroscience except we can put a

01:15:15.760 --> 01:15:19.520
probe in every neuron in the you know artificial brain and gather very very

01:15:19.520 --> 01:15:22.000
rich information so again I think there's a very

01:15:22.000 --> 01:15:25.520
interesting research frontier of getting better understanding of why are

01:15:25.520 --> 01:15:28.880
they able to generalize so well and why do we have these

01:15:28.880 --> 01:15:32.480
strange phenomena with these seemingly over parameterized models that don't

01:15:32.480 --> 01:15:35.520
overfit but rather have very good generalization lots of research to be

01:15:35.520 --> 01:15:39.040
done and just to linger on that that observation you made that you

01:15:39.040 --> 01:15:42.800
can train a deep learning model and after the

01:15:42.800 --> 01:15:47.600
training error has converged the test loss continues to improve I mean that

01:15:47.600 --> 01:15:50.560
just seems it just doesn't make sense

01:15:50.560 --> 01:15:54.160
I mean how and that there's grocking as well which is another

01:15:54.160 --> 01:15:57.440
it's almost like we were saying with physics that outside of the

01:15:57.440 --> 01:16:00.320
the machinations of the optimization algorithm

01:16:00.320 --> 01:16:03.360
stuff is happening that we don't understand well you can tell stories

01:16:03.360 --> 01:16:05.360
right you can say there's a there's a big space

01:16:05.360 --> 01:16:08.000
each point of the space is setting for all the parameters of the model so the

01:16:08.000 --> 01:16:10.880
sort of the weight space of the model and maybe you started off somewhere near

01:16:10.880 --> 01:16:13.680
the origin with some little random initialization and you follow some

01:16:13.680 --> 01:16:16.480
trajectory that's defined by stochastic gradient descent

01:16:16.480 --> 01:16:19.920
and there are lots and lots of places in this space all of which have zero

01:16:19.920 --> 01:16:22.480
training error so and they're connected so there's some

01:16:22.480 --> 01:16:25.360
sort of manifold of zero training error and you're starting off at the origin

01:16:25.360 --> 01:16:28.800
and stochastic gradient descent is somehow not taking you at a random way

01:16:28.800 --> 01:16:31.360
maybe it's taking you to something like you know the nearest

01:16:31.360 --> 01:16:33.920
point on this manifold or something and that maybe that's some kind of

01:16:34.000 --> 01:16:37.600
regularization and maybe that place has

01:16:37.600 --> 01:16:40.480
certain smoothness properties that lead to good generalization

01:16:40.480 --> 01:16:43.360
so you can kind of tell these stories I think the challenge is to take the

01:16:43.360 --> 01:16:47.120
stories and make them predictive so I think when we have a theory of

01:16:47.120 --> 01:16:50.000
what's going on we'll know we have a theory because it can predict new

01:16:50.000 --> 01:16:53.120
things not just tell stories about what we've already discovered empirically

01:16:53.120 --> 01:16:58.000
but really become predictive I think that's still a very much an open question

01:16:58.000 --> 01:17:03.120
so what do you think about the intelligibility of neural networks in

01:17:03.200 --> 01:17:06.960
terms of things like bias and fairness and safety

01:17:06.960 --> 01:17:12.240
because you could just think of these things as inscrutable

01:17:12.240 --> 01:17:17.440
bags of neurons and but we need to have some guardrails don't we

01:17:17.440 --> 01:17:20.800
well we absolutely need to create technology that's beneficial to humanity

01:17:20.800 --> 01:17:23.760
there's no question about that and there are mechanisms for doing that to

01:17:23.760 --> 01:17:27.440
align the systems whether it's through you know human feedback

01:17:27.440 --> 01:17:30.800
whether it's external guardrails that are providing more conventional sort of

01:17:30.800 --> 01:17:34.720
checks on how things are being used that's

01:17:34.720 --> 01:17:38.080
clearly necessary and I find it very encouraging that so much

01:17:38.080 --> 01:17:41.680
energy and effort is going into this and yes there'll be bumps in the

01:17:41.680 --> 01:17:45.840
road and missteps on the way for sure but overall we seem to be heading in a

01:17:45.840 --> 01:17:48.960
very good direction but I think the fact that there is a lot of attention

01:17:48.960 --> 01:17:52.160
being paid to the potential risks associated with this

01:17:52.160 --> 01:17:56.400
very powerful and very general new technology gives me hope that we will

01:17:56.400 --> 01:18:01.440
avoid most of the the biggest risks. Can you give me a specific example of an

01:18:01.440 --> 01:18:05.200
emulator? Yes I can so one very nice example actually

01:18:05.200 --> 01:18:09.360
it was the final project I worked on when I was working in the fusion program so I

01:18:09.360 --> 01:18:12.800
was using fusion as a sort of springboard to get into machine learning

01:18:12.800 --> 01:18:16.720
and we wanted to do real-time feedback control of a fusion experiment

01:18:16.720 --> 01:18:19.520
I think called a tokamak very high-temperature plasma we wanted to

01:18:19.520 --> 01:18:24.480
use neural nets to do non-linear feedback so the challenge there was

01:18:24.480 --> 01:18:29.040
to take a plasma it's like a donut shaped ring of hot plasma

01:18:29.040 --> 01:18:31.600
and it was known that if you could change the cross-sectional shape you could

01:18:31.600 --> 01:18:34.640
improve its performance so there's an experiment called a compass compact

01:18:34.640 --> 01:18:37.120
assembly at Cullum in in Oxfordshire and the

01:18:37.120 --> 01:18:41.120
experiment's designed to produce very interesting exotic cross-sectional

01:18:41.120 --> 01:18:44.320
shapes to to explore the performance so we wanted to use a neural net to do that

01:18:44.320 --> 01:18:47.360
feedback control. Now the good news is we had a great

01:18:47.360 --> 01:18:50.560
piece of inductive bias I think called the grad shafranoff equation it's a

01:18:50.560 --> 01:18:53.600
second-order elliptic partial differential equation but the point is it

01:18:53.600 --> 01:18:58.320
describes the boundary of the plasma very accurately right so you make a bunch

01:18:58.320 --> 01:19:01.520
of measurements from hundreds of little pickup coils around the plasma

01:19:01.520 --> 01:19:04.560
and those are boundary conditions you solve the grad shafranoff equation you

01:19:04.560 --> 01:19:07.440
know the shape of the plasma and the goal was to

01:19:07.440 --> 01:19:11.120
decide ahead of the time that you wanted to create a circular plasma

01:19:11.120 --> 01:19:16.640
and then change its shape and and and then make corrections if the shape

01:19:16.640 --> 01:19:19.840
wasn't quite the one you wanted you would change the the big control coil

01:19:19.840 --> 01:19:24.400
currents and an alternate shape. The problem was the grad shafranoff equation

01:19:24.400 --> 01:19:27.840
on a state-of-the-art workstation of the day would take two or three minutes to

01:19:27.840 --> 01:19:30.960
solve whereas we had to do feedback on a sort of 20

01:19:30.960 --> 01:19:34.000
kilohertz frequency or something it was about something like six orders of

01:19:34.000 --> 01:19:37.920
magnitude too slow so what we did instead was we we solved the

01:19:37.920 --> 01:19:42.000
grad shafranoff equation many times on the on the workstation

01:19:42.000 --> 01:19:46.000
over a period of you know days and weeks until we built up a large database

01:19:46.000 --> 01:19:49.520
of known solutions along with their magnetic measurements

01:19:49.520 --> 01:19:52.640
and then we trained a neural network just a simple two-layer neural network

01:19:52.640 --> 01:19:56.240
back in the day with probably only a few thousand parameters i mean

01:19:56.240 --> 01:19:59.440
miniscule by modern standards but it was trained to take the magnetic

01:19:59.440 --> 01:20:02.240
measurements and predict the shape and we could put that into a standard

01:20:02.240 --> 01:20:06.000
feedback loop and and we're in a bit of a race with

01:20:06.000 --> 01:20:09.120
another organization that was doing a similar thing a different fusion lab

01:20:09.120 --> 01:20:13.120
that was working on the same project and so that was very motivating and i'm

01:20:13.120 --> 01:20:16.160
pleased to say we got there first and we did the world's first ever real-time

01:20:16.240 --> 01:20:20.160
feedback control of a tokamak plasma using a neural network

01:20:20.160 --> 01:20:23.920
but as a beautiful example of a of an emulator we could get five or six orders

01:20:23.920 --> 01:20:27.920
of magnitude speed up not by solving the equation directly

01:20:27.920 --> 01:20:32.640
to do feedback control but by using the numerical solver to generate training

01:20:32.640 --> 01:20:35.360
data and using the training data to train the emulator

01:20:35.360 --> 01:20:38.560
and then the emulator and even then it was still

01:20:38.560 --> 01:20:42.960
quite demanding for the silicon of the day there was no processor fast enough so

01:20:42.960 --> 01:20:46.560
we actually built a physical implementation of the

01:20:46.560 --> 01:20:50.080
neural net believe it or not so it was a hybrid

01:20:50.080 --> 01:20:54.000
analog digital system had an analog signal pathway with analog

01:20:54.000 --> 01:20:57.520
sigmoidal units but the weights were set using digitally set

01:20:57.520 --> 01:21:00.400
resistors so we could take the numerical output of the the emulator

01:21:00.400 --> 01:21:04.960
downloaded into this bespoke hardware physical neural network

01:21:04.960 --> 01:21:07.920
and do real-time feedback control so i was pretty pretty excited about that

01:21:07.920 --> 01:21:11.440
project that's fascinating what do you think about

01:21:11.440 --> 01:21:15.520
control now do you have any opinions on you know model predictive control and

01:21:15.520 --> 01:21:19.280
control is a super important area different both the both the control

01:21:19.280 --> 01:21:22.560
problem and the overall planning problem i think

01:21:22.560 --> 01:21:26.400
despite all the remarkable advances in gbt4 the world of instantiated

01:21:26.400 --> 01:21:30.960
ai and robotics and so on is still a very very wide open frontier

01:21:30.960 --> 01:21:35.040
we don't we don't really have robots that can even yet drive a car through

01:21:35.040 --> 01:21:38.720
central london that's still a a major challenge that

01:21:38.720 --> 01:21:41.280
we're seeing some very remarkable progress recently

01:21:41.280 --> 01:21:45.120
yeah i mean more broadly i've been speaking with some neuroscientists and

01:21:45.120 --> 01:21:48.320
they say that we have the matrix in in our heads so we're always running

01:21:48.320 --> 01:21:52.160
simulations and presumably in the future this will be a

01:21:52.160 --> 01:21:55.680
principled way of building agents so the agents will run

01:21:55.680 --> 01:21:59.760
counterfactual simulations and select trajectories which look like good ones

01:21:59.760 --> 01:22:03.600
and then the process will will iterate i think this is this is very powerful i

01:22:03.600 --> 01:22:07.040
mean the the idea of sort of type one and type two fast learning slow learning

01:22:07.040 --> 01:22:09.840
the idea that we simulate the world and we compare the simulation with the

01:22:09.840 --> 01:22:12.560
reality and we can learn from our own simulators and so on

01:22:12.560 --> 01:22:16.000
we don't we don't quite know what best to do with that but it feels such a

01:22:16.000 --> 01:22:19.360
powerful and compelling concept and we we think something like that is going on

01:22:19.360 --> 01:22:21.760
in the brain that again that feels like a

01:22:21.760 --> 01:22:25.360
an area that's ripe for exploration and i think in some form

01:22:25.360 --> 01:22:29.120
some kind of you know model prediction and simulation of the world feels like

01:22:29.120 --> 01:22:33.120
it will be increasingly a part of ai systems as we go forward

01:22:33.120 --> 01:22:36.480
i mean for me the takeaway in all of this is just what an amazing time to be in

01:22:36.480 --> 01:22:39.040
this field there are so many fascinating things to work on

01:22:39.040 --> 01:22:42.080
professor bishop it's been an honor to have you on mlst thank you so much

01:22:42.080 --> 01:22:46.480
well thank you i've enjoyed it thank you amazing

