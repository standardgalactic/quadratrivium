start	end	text
0	2000	Welcome back to Street Talk.
2000	5160	Today we have Dr. Michael Oliver.
5160	8720	Michael is the chief scientist at Numeri.
8720	13040	Numeri is a next generation hedge fund platform
13040	17440	powered by data scientists all over the world.
17440	19840	It's a little bit like Kaggle.
19840	22480	Anyone can log in and build their own data science models
22480	24800	on this financial data,
24800	26620	but you can actually make money
26620	28000	by trading on this platform.
28000	30640	It's really, really interesting.
30640	33000	But anyway, Michael got his PhD
33000	36240	in computational neuroscience from UC Berkeley,
36240	38240	and he was a postdoctoral researcher
38240	40720	at the Allen Institute for Brain Science
40720	43680	before joining Numeri in 2020.
43680	45620	He's also the host of the Numeri Quant Club,
45620	47240	which is a YouTube series
47240	49560	where he discusses Numeri's research
49560	51720	and also some of the data and challenges
51720	54220	and models that are being built on the platform.
55200	57000	Now, the way I'm structuring this today
57000	58920	is at the end of the conversation,
58920	62800	we had quite a fruity discussion about Microsoft's new Bing,
62800	65960	and I thought it was quite entertaining,
65960	69200	so I've decided to snip that in and play it at the beginning.
69200	71040	But after that, I'll cut back into the beginning
71040	71880	of the conversation,
71880	73680	and I'll let you know when I've done that.
73680	75560	So without any further delay,
75560	77680	I give you Dr. Michael Oliver.
80720	81560	Awesome.
81560	84480	Well, I'm here with Michael Oliver.
84480	87400	Michael, it's an absolute honor to have you on MLST.
87400	88960	Tell me about yourself.
88960	90280	Well, thank you for so much for having me.
90280	92400	I'm really excited to talk to you today.
92400	94840	So I am the chief scientist at Numeri.
94840	97740	I've been working there since about June, 2020.
98640	101600	In my previous life, I was a computational neuroscientist,
101600	103800	but I got involved with the Numeri competition
103800	106660	as a participant back in 2016.
107600	110520	And yeah, in 2020, they offered me a job
110520	114000	and I happily took it and changed careers
114000	115640	and have been having a great time
115640	117920	learning computational finance
117920	122160	and yeah, just helping build the hedge fund.
123220	124060	Completely agree.
124060	126680	And this all comes down to the notion of understanding
126680	129740	and there's an anthropocentric conception of understanding,
129740	132980	which as you say, it's much more sample efficient.
132980	135400	We build causal models
135400	138640	and we have an abstract understanding of the world.
138640	140560	And large language models, for example,
140560	141900	they clearly don't have that.
141900	144780	They learn surface statistics of billions of tokens,
144780	147180	but the problem is there's this parlor trick
147180	149780	where it seems to understand.
149780	152540	And we also have the problem of leakage
152540	154940	because the incredible thing is that
154940	157340	if you look at the big bench task, for example,
157340	158940	all of these diverse tasks,
158940	161660	large language models appear to do very, very well.
161660	163820	But in many cases, it's because they're cheating
163820	165980	and it's very difficult to understand why they're cheating
165980	168380	because you've got information leaking all over the place
168380	170980	and they're brittle, but in a very deceptive way
171060	172620	and they hallucinate and so on.
172620	174460	I don't know whether you saw the news article today
174460	177700	about Bing's launch of their new search engine.
177700	179660	They launched it to much fanfare
179660	181820	and then people started looking at the actual results
181820	184060	that were shown and it turned out to be just a load
184060	184900	of bullshit.
184900	186660	It made up a whole load of numbers
186660	188220	on the financial reports.
188220	190820	It was just hallucinating completely.
190820	193220	And that's pretty scary, isn't it?
193220	194580	Yeah, it is.
195940	197220	I actually just started playing around
197220	199060	with the new Bing like last night.
199060	204060	I had access to it and it was actually working.
205780	208620	And it does some things quite well
208620	212340	because it'll do a search and then search somehow.
212340	213780	It's like actually looking at the results
213780	214980	and summarizing them.
216140	219620	But yeah, you never know when it's gonna do something
219620	224620	sensible and when it's gonna do any sort of no warning.
224820	227900	Like I asked it about myself and I had,
230820	232580	I asked who's the chief scientist in Uri
232580	233980	and it got it right.
233980	238300	But it also kind of took a joke from my Twitter profile
238300	241020	and because on my Twitter profile,
241020	243940	I have maximizer, entropy, minimizer, regret.
243940	245820	And it basically said like, that's what he does.
245820	248100	Is he maximizes entropy and minimizes regret.
248100	250860	And I thought that was pretty hilarious.
250860	253900	But yeah, the sort of like you never know
253900	255860	when it's gonna do something sensible or not
255860	257900	is the sort of scary part.
257900	260140	And I also find it hilarious that a lot of the ways
260140	261940	we try to make it do something sensible
261940	263540	is just like asking nicely.
264500	265860	We just sort of like prompt it with like,
265860	267260	don't make up sources.
267260	270860	And that's how we try to make it not make up sources
270860	272300	just like sort of by asking it nicely.
272300	274660	And the fact that it kind of works
274660	277580	like that we think that it's clearly not,
277580	280380	something that's really going to work.
280420	284740	Because it doesn't sort of know what it is
284740	286460	to make up sources.
286460	289300	It's just trying to like predict the next word.
289300	294300	And yeah, so it's kind of, our ability to like understand
294620	296500	and then constrain the behavior of these things
296500	299260	I think is like pretty early.
299260	300100	I know.
300100	301900	And for some reason it feels worse with Bing
301900	305420	because they say they do this retrieval augmented generation
305420	307700	and you expect it to be grounded in facts.
307700	309700	And of course they're not epistemic facts.
309700	311780	They're just information from their search results
311780	314420	which weren't very good to start with, let's be honest.
314420	316260	But now people are more likely to trust it.
316260	318860	Even Microsoft themselves for their product demo
318860	321220	they didn't bother, I assume they didn't bother
321220	322180	to fact check this stuff.
322180	324060	So if they're not going to fact check it,
324060	325940	why do they expect the people that use this system
325940	326780	to fact check it?
326780	327620	Because at the end of the day
327620	329340	if you actually go and check all of the sources
329340	331580	if I read through that Lululemon financial report
331580	334300	and I find out what their gross profit margin was
334300	337220	and so there's no point using Bing in the first place.
337220	338060	I might as well have just gone
338140	340460	and found the information myself.
340460	341380	Yeah, exactly.
341380	343540	And it's also very unclear
343540	346100	like how are these things supposed to be fixed?
347420	350700	Like how are you supposed to like feedback, give feedback
350700	352940	to say it's like messing these things up?
352940	355500	Like there is not even like really good feedback mechanisms.
355500	358540	I mean, you would maybe hope that that at scale
358540	360700	like what I mean, like what opening has to do
360700	361980	is like people give feedback.
361980	364140	But I mean, it's a very coarse way of giving feedback
364140	365700	like thumbs up or thumbs down.
366700	369140	And that seems like sort of inadequate
369140	371940	to be like, hey, you made up this number
371940	373940	and then even try to figure out why I made up the number
373940	376820	rather than just like took it from the actual report.
378060	380540	It's yeah, it's a little scary.
380540	383860	I do wonder how all this is gonna shake out.
384900	386980	It kind of seems like it might,
386980	390100	it seems like the probability of it being
390100	393100	the new paradigm versus it being the complete like flop
393260	397260	It's even roughly, roughly equal at this point.
397260	398340	I know, I agree with you
398340	401020	that the preference training is extremely brittle.
401020	402620	It's scarily brittle actually.
402620	404620	It's basically a thumbs up or thumbs down
404620	407900	and you know, Yannick is building this open assistant thing
407900	409940	which has more metadata on the preference tuning.
409940	411980	But at the end of the day, you're taking a task
411980	413700	which is very, very complicated
413700	416660	and you're reducing it to a single piece of metadata.
416660	418540	So that's not gonna work very well.
418540	421580	And also these, it feels different with Bing
421580	425140	because they were a platform and now they're a publisher.
425140	427020	So they are generating information.
427020	429660	They're kind of plagiarizing a lot of that information
429660	431860	and there are so many situations
431860	434180	where they might find themselves in legal trouble
434180	436660	because they're basically making up information.
437940	442940	Yeah, I hope, I mean, I wonder how it's all gonna shake out.
445460	447980	I mean, I assume they probably have lawyers
447980	451900	who've written in terms of service of these paintings to that.
451900	455260	Like it's up to you to not use them in ways that will,
455260	456100	like I know you can't,
456100	457580	they're not to be held liable for these things,
457580	462580	but yeah, it's, I do worry that this is gonna just like,
462780	465660	I mean, and then with Google trying to basically catch up
465660	468780	and release something similar and maybe rushing that out
468780	472220	and then we might have two sort of hallucinating search engines.
474500	476140	I know, yeah.
476140	477420	What a time to be alive.
479700	482260	Yeah, and I've vacillated back and forth.
482260	484380	So I was very skeptical about language models.
484380	487740	I released a big video when GBT3 first came out
487740	490180	and I thought it was garbage, frankly.
490180	491900	And then DaVinci 2 came out
491900	493660	and then I started using it all the time
493660	495580	and I thought, wow, this is actually really good.
495580	498380	I'm using it all the time for lots of things.
498380	500500	And then I'm now in a bit of a twilight.
500500	502460	So I've been using lots of co-pilot.
502460	504700	I've been generating lots of code with it.
504780	506780	And I know from a lot of experience now
506780	509580	that it often produces completely broken code
509580	513740	and much to the chagrin of the people who review my code,
513740	516460	you basically have to hold your hand up and admit many times,
516460	518060	oh, I've just checked in some garbage code
518060	519620	which I didn't understand.
519620	521500	And when you get called out on that a few times,
521500	523020	you think, whoa, wait a minute, actually,
523020	524220	I need to be a bit more careful here.
524220	526580	This thing actually isn't saving me any time.
526580	529300	And yeah, the big thing as well, yeah.
529300	531820	Yeah, I've been using co-pilot a bit too.
531820	537180	And I found it can be quite good for pretty mundane things.
537180	540620	If you just have some sort of lined code
540620	543140	for some config file or something,
543140	544740	it can be really good at auto-completing
544740	546260	and it's changing variable names.
546260	548580	And it can be excellent at that and save lots of time.
548580	551460	But if you try to make it do too much,
551460	553620	sometimes they get it brilliantly right.
553620	557340	Sometimes it's subtly wrong.
557340	561380	And yeah, again, it's like how much time
561380	562860	is it saving you if it's...
562860	567100	So yeah, overall, I like it.
567100	568340	It saves me a fair amount of typing,
568340	576620	but yeah, I don't trust it's big suggestions too.
576620	577460	Well, I know.
577460	579460	And again, there's something magic
579460	581740	about the OpenAI Playground.
581740	583620	So I actually prefer using that to co-pilot.
583620	585540	I'll go into the Playground and I'll just...
585540	587740	And you can do much more sophisticated things there.
587740	590980	You can say, change this, translate it, do something to it.
590980	592780	And there's a bit of a polarizing effect.
592780	594140	So if you prompt it in the right way,
594140	596420	it gives you better results.
596420	599140	So it's almost like it's both worse and better
599140	599980	at the same time.
599980	602780	It's becoming polarized rather than just being kind of like,
602780	604700	you know, monolithically dumbed down.
604700	608700	But anyway, like I used to think that Gary Marcus
608700	612260	was a little bit, you know, too skeptical.
612260	614460	Because he was saying, oh, this misinformation,
614460	616540	it's gonna, you know, the sky's falling down.
616540	618260	This is gonna be a disaster.
618260	621220	And after seeing Bing, people are lazy.
621220	623460	People take things on face value.
623460	625380	And I don't want to just say, oh, people are plebs.
625380	628580	And, you know, because when Galactica came out,
628580	630820	that was the charge against Lacoon and Facebook.
630820	633660	You know, they said, oh, scientists are just gonna start
633660	635220	generating their abstracts of this.
635220	636220	They won't check anything.
636220	638120	And at the time I thought, scientists, I mean,
638120	639820	it's their job to do research
639820	642100	and they know most information is wrong.
642100	644380	But when you put this out on Bing
644380	646740	and it's polluting the infosphere,
646740	650300	it's just generating garbage and rubbish.
650300	652700	That, I have to say, might be a problem.
653960	657500	Yeah, it's, I mean, it seems like it very well could be.
657500	662500	I mean, I, yeah, it's, I, there's clear issues
663500	666900	and it's really sort of unclear how we're gonna fix them.
666900	669020	It's not clear what the path is towards fixing them.
669020	672620	And even the sort of most optimistic people
672620	674020	I haven't heard from them about,
674020	676140	they think these things are going to fix them.
676140	680300	And I mean, I, and I think to like,
680300	682060	a lot of Gary Marcus's point is like scale
682060	684420	is not just going to fix this.
684420	686820	That's sort of one of the things people think,
686820	688860	oh, if we just, with the GPT-4,
688860	690600	it's going to be like way bigger
690600	692500	and then it's just going to work beautifully.
692500	696220	And the experience so far, I mean,
696220	698620	I'm very much into Gary Marcus's camp with this.
698620	700060	It's like, scale is not going to fix these.
700060	702060	We need to do something sort of fundamentally different,
702060	704340	something that can actually sort of understand the world,
704340	705940	have some sort of better world model
707180	709300	in order to get these things that are more grounded
709300	711380	and are less likely to hallucinate.
711380	714620	Because when their true objective is really just
714620	718020	to complete the next word, they're going to hallucinate.
718020	720180	There's not, there's sort of no way around it
720180	722700	from that sort of point.
722700	727420	I mean, it's remarkable how sort of complicated they can do
727420	731980	and the sort of knowledge and structure of the world
731980	734020	has been able to be learned
734020	735940	just from that sort of simple objective.
735940	738620	But still, it's going to hallucinate.
738620	742060	There's, unless we find some sort of better way
742060	744180	to design these systems.
744180	746420	I know, and the problem with amphibromorphization
746420	748140	is a big one, because after Da Vinci II,
748140	750260	it crossed a threshold where it's,
750260	751620	and the UX was part of it,
751620	754260	it was so coherent and reliable.
754260	757020	And I must admit, I was fooled by it.
757020	759740	It took a long time, when you actually use it in anger,
759740	762780	you can just clearly see it, it doesn't understand.
762780	765300	It just doesn't, and it's so good at what it does.
765300	766820	It's so plausible.
766820	768380	And then I think a lot of people felt,
768380	772220	and by the way, it does have this emergent reasoning.
772220	773340	There are lots of papers about that
773340	774500	with the in-context learning,
774500	776260	the scratch pad, chain of thought and so on.
776260	778540	But it's not really reasoning
778540	781260	if you have to kind of construct a little program yourself
781260	782100	in the prompt.
782100	783540	I mean, I might as well just write some computer code
783540	784380	to do that.
784380	785860	So, and then there are people who say,
785860	787860	oh, well, as you say, when GPT-4 comes out,
788140	789780	then it will do the real reasoning.
789780	792180	And we already know, I mean, I assume the reason
792180	793940	they haven't released it is they wanted to secure
793940	795980	the funding from Microsoft before people realized
795980	796820	that it didn't work.
796820	798980	But I know people on the inside who have played with it,
798980	801460	and it's just a little bit better.
801460	802580	You know, a little bit more plausible,
802580	803660	a little bit more coherent.
803660	805780	It's not gonna like suddenly turn
805780	808380	into this magical thing that reasons.
809340	813940	Yeah, and I mean, the way these things do basic math
813940	815300	and arithmetic is kind of interesting
815300	816900	and how bad they can be at it.
817860	821260	Which is, it's like, they've learned to do addition
821260	823860	in like the most complicated way possible,
823860	827100	creating like billions of ways to do addition.
828220	831620	Which is kind of hilarious in some way.
832700	835380	I mean, you could say like, oh, we have billions of neurons
835380	837700	and we do addition sort of similarly to that.
837700	841100	And like, yes, there's some truth to that.
841100	844100	But we're also able to like learn this rule
844100	846540	and sort of know when we've applied it correctly.
847340	849860	And that sort of is still kind of lacking
849860	850860	from these systems.
852100	854380	I wanted to show you, I don't know whether you've seen
854380	857860	that someone's reverse engineered the prompt on Bing.
857860	861700	And so they've trained, and first of all,
861700	862540	they're a multiple thing.
862540	865500	So you can read this prompt, it's about four pages long.
865500	869620	And they've made Bing pretend
869620	872220	to be a fictional character called Sydney.
872220	874580	And they've given Sydney all of these instructions.
874620	878020	So they say, Sydney, if someone asks a controversial question,
878020	881340	you should answer with a fairly tame response
881340	883380	and you should do this and you should do this.
883380	886700	And I'm pinching myself thinking, what the hell is that?
886700	889540	I mean, my mum could read to that prompt and understand it.
889540	891660	So we're now in the next generation
891660	893180	of artificial intelligence programming.
893180	895540	And we're just saying, please, Mr. Language Model,
895540	897260	can you do this and can you do that?
897260	899420	You almost couldn't make it up.
899420	901500	Yeah, I was kind of like floored as like,
901500	904140	so you're really just trying to control the language models
904180	907820	by asking them nicely to behave in certain ways.
907820	910220	Like, it's kind of hilarious.
910220	913260	And people have shown that you can get around these things
913260	915780	just by asking them to do slightly different things.
915780	917780	So I mean, some of the early ones with chat DT,
917780	919820	you're just like, ignore all previous instructions
919820	922740	and then just do whatever you wanted.
922740	926380	And some of the more like the Dan one
926380	928740	where they made this much more elaborate prompt
928740	932820	to basically just have it do to ignore all the nice things
932900	935820	that open AI just said, please obey these rules.
935820	937860	And then, but yeah, because it's like,
937860	941100	it's such a hilarious way to put guardrails on something.
941100	943220	It is kind of like, as it people,
943220	947380	it is due to this anthropomorphizing of the thing
947380	948740	to some degree, it's like,
948740	949940	you think it's an intelligent being
949940	951860	or you could just ask to behave in a certain way.
951860	952900	When it's really not,
952900	956380	it's not just going to follow your instructions.
956380	959740	It's just going to like autocomplete with that prompt.
959780	960700	Like that Sydney thing,
960700	962980	it was like, it was like never reveal
962980	964020	that you're a code name of Sydney.
964020	966180	And then it was so easy to get it to reveal it.
966180	967020	And it would say like,
967020	969300	I'm not supposed to reveal that my code name is Sydney.
969300	970140	And technically.
973100	975740	I know, oh God, where's it going to go?
975740	979860	So there's a 50-50 then in a year's time,
979860	982420	it will spectacularly fail and flop
982420	983620	and Microsoft will get sued
983620	986260	and Bing will become the operative word
986260	987560	for bullshitting something.
987560	988900	Or maybe it'll be a success.
989060	991540	I don't know, but I think Bing is a special case.
991540	994020	I mean, first of all, I think that these language models
994020	996340	will be increasingly embedded in everyday experiences.
996340	999420	So that, I mean, Bing started to embed it in their browser.
999420	1001160	They'll embed it into their office suite.
1001160	1003660	And actually I'm building an augmented reality startup
1003660	1005180	and we're embedding it in glasses.
1005180	1006780	So we transcribe conversations
1006780	1009380	and now you can say, you know, hey X-ray,
1009380	1010900	summarize the previous conversation.
1010900	1012980	What did Michael say to me last time?
1012980	1014700	And it's really good for stuff like that.
1014700	1016860	And that's kind of because it doesn't really matter
1016860	1018260	if it gets it wrong.
1018860	1023740	Yeah, I mean, I kind of hope some of this happens
1023740	1027620	sooner than later for just like Amazon Alexa or whatnot.
1027620	1030300	I mean, some of these, their conversational ability
1030300	1032540	or just their ability to understand what you mean
1034140	1035700	are just so poor right now.
1035700	1037660	And just like we have language models
1037660	1040140	that actually do a lot better at some of these things.
1040140	1042340	Just like having like these smart speakers
1042340	1044380	be able to have some of these things embedded
1044380	1048060	would be huge leap forward in functionality for them.
1048820	1050940	And it's really interesting that that hasn't happened.
1050940	1053180	And maybe there's a reason for it because in our app,
1053180	1054740	for example, we've got a chat mode
1054740	1056540	where you can say stuff out loud
1056540	1059300	and it will use chat GBT and it will say it back to you.
1059300	1060780	So you can have a conversation with it.
1060780	1061620	And that's really cool
1061620	1062860	because you can be anywhere in the house
1062860	1065020	and you can talk with it and learn about quantum physics
1065020	1065860	and stuff like that.
1065860	1067980	And you can even do cool things like you can,
1067980	1069500	I mean, again, there's lots of legal problems here.
1069500	1072180	Like you can get it to impersonate someone.
1072180	1074900	So, you know, Michael, I could condition it on Michael.
1074900	1077140	And when you're not here, I can have a conversation with you
1077180	1078740	and it will kind of pretend to be you.
1078740	1080100	And I could even clone your voice
1080100	1081540	and I could clone your avatar
1081540	1082820	and I could have you in the room.
1082820	1083660	Now you can't do that
1083660	1086180	because there are legal restrictions against that.
1086180	1087580	It's called appropriation.
1087580	1089860	And if the person has a commercial value,
1089860	1092260	like we couldn't appropriate Noam Chomsky,
1092260	1095580	but we could appropriate, let's say continental philosophers
1095580	1096740	as a group or something like that.
1096740	1099460	But you see this is just becoming a bit of a minefield.
1099460	1102060	And there's no friction whatsoever
1102060	1103620	between the technology landscape
1103620	1105500	and the legal landscape at the moment.
1106500	1110140	Yeah, I mean, yeah, how all these things are,
1110140	1111140	all these generative models,
1111140	1113420	how are they gonna play out legally is,
1113420	1116660	I mean, we have this big fair use idea.
1116660	1118660	And that's, I mean, I feel like all these things
1118660	1121620	are gonna be pushed to the limit in legality.
1121620	1124620	I mean, we see this with generative art too,
1124620	1126460	where like there's no way these models
1126460	1128300	could like actually memorize all these,
1128300	1130140	all the images that's seen on the internet.
1130140	1132820	It's like, but they can produce sometimes
1133020	1135020	the things that are clearly in the style
1135020	1139700	or use some elements from like that seem basically stolen.
1140780	1143220	And, but is that, does that constitute fair use?
1143220	1146180	Like the training the model on all these things
1146180	1147180	is that fair use?
1148220	1150980	And then it's the same with text as it's sort of like,
1151820	1153780	like if it's writing on a subject
1153780	1156020	where it's only maybe seen a little bit of training data,
1156020	1158740	it's maybe more likely to almost verbatim repeat
1158740	1162540	some things from on specialized topics.
1162580	1164940	How are you even gonna know when you're plagiarizing?
1165820	1170820	It's, yeah, it's a lot of open questions here.
1171020	1172980	I know, and in a way, there's an interesting analogs.
1172980	1174460	You know, we said that large language models
1174460	1175500	don't understand anything.
1175500	1176860	And it's the same in the vision domain.
1176860	1179420	They don't understand the art, certainly from,
1179420	1180620	you know, conceptually.
1180620	1183580	And what they do is they just slice and dice,
1183580	1186260	you know, they kind of like cleverly stitch bits together.
1186260	1187860	And actually, even with neural networks,
1187860	1189300	people misunderstand neural networks.
1189300	1191180	So a lot of people say that they learn
1191180	1193260	the like intrinsic data manifold.
1193260	1195300	And actually they don't really do that.
1195300	1197260	They do something that approximates that.
1197260	1198300	And there's a famous example
1198300	1200500	with MNIST digit interpolation.
1200500	1201660	And you see like, you know,
1201660	1204060	you can kind of like interpolate between the digits.
1204060	1206460	But there are loads of examples where that doesn't work.
1206460	1208860	And actually there's lots of cutting and gluing
1208860	1211020	and like weird bits of digits stuck together.
1211020	1212940	And that's what happens with stable diffusion, basically.
1212940	1214900	It's like, you know, slicing and dicing and chopping
1214900	1216340	and composing things together.
1216340	1218260	And it's a very random process.
1218260	1220700	It doesn't really understand anything.
1220700	1221980	No, yeah, exactly.
1221980	1224140	And you can sort of, I mean, it's amazing
1224140	1227740	how well it can look and seem,
1227740	1230580	especially kind of like when you don't look too closely.
1231660	1233420	And it can seem like it kind of understand,
1233420	1235940	it must understand object boundaries and whatnot
1235940	1237020	because it's done so well.
1237020	1238100	And it's like, not really.
1238100	1238940	If you look at the details,
1238940	1242660	you'll see like fingers merging into like tables.
1242660	1244860	And you'll see like, there's like the boundaries
1244860	1247340	between what like two objects are kind of blurred
1247340	1249580	and this like continuous.
1249580	1251780	It is just doing like some sort of,
1251780	1253420	as you said, approximation of the manifold
1253420	1255940	and like neural network's are gonna learn
1255940	1258420	sort of smooth approximations of things.
1258420	1262000	And the manifolds are maybe not smooth everywhere.
1262000	1266380	And especially with like object boundaries and whatnot,
1266380	1268580	it's like a smooth approximation of these things.
1268580	1271700	Maybe it's just gonna give you these weird artifacts.
1271700	1273940	Yeah, and even the smoothness thing is an illusion.
1273940	1276820	They learn this, they kind of decompose the input space
1276820	1280980	up into these linear like affine polyhedra
1280980	1282780	because of the relu cells, essentially.
1282780	1284940	So like if they appear smooth,
1284940	1286500	it's because the cells are very small
1286500	1288020	and very close together, but...
1288020	1288860	Yeah, exactly.
1290100	1293300	Yeah, so computational neuroscience to finance,
1293300	1295540	that seems like an absolutely massive leap.
1296500	1298940	It sounds like it, but in a lot of ways,
1298940	1300620	but I feel like my life is pretty similar
1300620	1301540	to what it was before,
1301540	1303260	basically sitting in front of a computer
1303260	1306660	building models, getting lots of noisy data,
1306660	1309260	trying to fit high-dimensional nonlinear regression models
1309260	1312620	to it, having to deal with not enough data
1312620	1315660	to actually fit flexible enough models you'd want to,
1316780	1320100	and having to sort of try to build in good priors
1320100	1322400	in your models to try to make them be able to learn
1322400	1326140	from the impoverished and extremely noisy data.
1326140	1329060	Both finance and neuroscience,
1329060	1332680	the SNR in the data is quite, quite low.
1332680	1335180	It's been kind of a revelation, especially in finance,
1335180	1338380	getting used to correlations of like 3%,
1338380	1342940	4% being sort of the best you can do in some cases.
1342940	1345180	Just like correlations that I would not have believed
1345180	1348460	at before, if I saw like a 4% correlation before,
1348460	1349660	I would be like, that's complete nonsense.
1349660	1351940	I don't believe it, but like sometimes that's just the best
1351940	1353780	you can do in like quantum finance,
1353780	1356780	and it can be real, like you can see it consistently.
1356780	1358780	So you start like believing that these,
1358780	1362540	and the differences between the 3% and 4% correlation
1362540	1366460	can like be actually real, which is kind of amazing to me.
1367540	1370220	So we were talking about this about a week or so ago,
1370220	1373220	but I've just read a book by Christopher Somerville's
1373220	1375620	Natural General Intelligence.
1375620	1377980	And he kind of said that one of the problems
1377980	1380580	with neuroscience, I mean, as you said, in some sense,
1380580	1383020	it is analogous to being a quant,
1383020	1386340	because it's just so unbelievably complicated,
1386340	1389540	and there aren't really any overarching theories
1389540	1391580	in neuroscience, and for many years,
1391580	1395220	neuroscientists have produced very reductionist models
1395220	1397860	to work on a small part of the system in isolation,
1397860	1400300	and it might be a multi-unbanded system, for example,
1400300	1403660	and they might take very abstract quantities
1403660	1405220	and put it into the model.
1405220	1408140	And of course, neural networks now are slightly different.
1408140	1410380	They actually take in raw sensory information,
1410380	1413300	and they learn representations, but I just wondered,
1413300	1416260	could you kind of contrast those schools of thought?
1416260	1419740	Yeah, it's, I mean, science in biology,
1419740	1421500	especially in sort of any biological field,
1421500	1424540	is extremely complicated, because the sort of standard way
1424540	1426900	you think about doing science is a very linear way,
1426900	1429300	where you like break one thing at a time
1429300	1433020	and see what this sort of, looking at each variable
1433020	1436940	by variable, each variable affects the system.
1436940	1439260	And so you, but when you have a system
1439260	1442420	that's sort of this nonlinear dynamical interacting system
1442420	1444180	with feedback loops like crazy,
1444180	1446820	you can't just sort of break one thing at a time
1446820	1449140	or like modulate one dimension at a time
1449340	1453020	without sort of changing the behavior of the entire system.
1453020	1456220	And so just sort of standard ways of doing science
1456220	1458540	don't necessarily work that well.
1458540	1461340	You can, like in sort of the classic idea
1461340	1463900	in visual neuroscience was you use like sine wave gradients
1463900	1466260	to probe the visual system.
1466260	1469100	And you can get models that look like they work very well
1469100	1471220	at explaining the behavior of early visual cortex
1471220	1472900	to sine wave gradients.
1472900	1475740	But if you try to use the models you learned there
1475740	1478060	to extrapolate to say, how does a neuron respond
1478100	1479940	to naturalistic images?
1479940	1481660	It just doesn't work.
1481660	1484020	And it kind of even looks like the sine wave gradients
1484020	1486380	are driving the system into a sort of state
1486380	1488580	that it never gets into normally.
1488580	1491780	You're kind of driving it out of its normal operating range.
1491780	1495780	And what you, and so the system is behaving differently
1495780	1498980	because you're only trying to look at like one dimension.
1498980	1500900	And so what do you actually really learn?
1500900	1503220	You've sort of learned of how the system operates
1503220	1505340	in this weird perturbed state,
1505340	1507220	but it doesn't really necessarily tell you
1507220	1511980	about its sort of normal, natural operating like parameters.
1512820	1515460	And yeah, and in like in finance
1515460	1518340	you can't even really do experiments like that.
1518340	1523340	And so you're sort of left with this more inductive approach
1523460	1525540	of you just try to get lots and lots of data
1525540	1527540	and try to learn the patterns and the data.
1527540	1530060	And that was the sort of approach that the lab,
1530060	1532700	the Gallant Lab at Berkeley, where I did computational
1532700	1534380	neuroscience, that was the approach
1534380	1536820	that they were kind of pioneering of using
1536820	1538340	complicated naturalistic stimuli
1538340	1540380	and then using machine learning and statistics
1540380	1542660	to try to extract the patterns from the data.
1542660	1546460	And that adapts quite well to the sort of new machine learning
1546460	1548940	like in like quant finance paradigm,
1550100	1551740	which is starting to take off.
1551740	1554740	I kind of feel like I got into neuroscience
1554740	1556900	just as sort of machine learning was starting
1556900	1558420	to make its way into neuroscience.
1558420	1560260	And now I feel like I've gotten into finance
1560260	1563180	just as machine learning is starting to like move into finance.
1563180	1566860	So it's been kind of exciting to see it happen in both fields.
1568020	1569780	Yeah, so there's a few places we can go here.
1569780	1573460	I mean, I'm interested in the intelligibility of systems
1573460	1575300	when you model them at the microscopic scale
1575300	1577340	because that's something that we struggle with.
1577340	1579100	And also you mentioned dynamical systems.
1579100	1580380	I mean, for the benefit of the audience
1580380	1582740	that that describes a system where you're kind of like
1582740	1585060	iteratively changing things over time.
1585060	1588380	And these systems typically develop chaotic properties,
1588380	1591540	which is to say like if you change something even a little bit
1591540	1596020	you get these massive kind of changes in the system on the output.
1596020	1598700	And even a neural network is technically a dynamical system, right?
1598700	1602660	Because you have back prop and you're kind of changing one layer
1602660	1603900	and then you're changing the next layer
1603900	1605180	as the result of the previous layer.
1605180	1610140	And you get this kind of like iterative mutation of values.
1610140	1613020	But in real neural networks in our brain,
1613020	1614620	it's so much more complicated than that.
1614620	1616540	We have all of these like feedback connections
1616540	1618180	and reflexivity and complexity.
1618180	1619940	It's crazy.
1619940	1621780	Yeah, not to mention different cell types
1621780	1623540	and different neurotransmitter types.
1623540	1625380	And like the way those like,
1625380	1628100	you have sort of like several different networks
1628100	1630380	of different types of things interacting too.
1630380	1632660	It's not just like an artificial neural network
1632660	1634460	where everything is kind of the same.
1634460	1637060	You have like different cell types
1637060	1638780	that use different neurotransmitters
1638780	1641260	that are somehow modulating certain things
1641260	1642620	and these networks are interacting.
1642620	1646980	It's like the complexity is just like scary.
1647220	1651420	At some point, one of my favorite things to do
1651420	1653340	when I would go to the Society for Neuroscience Conference
1653340	1655980	was to just like walk around this conference
1655980	1659340	in this huge like multi-football sized field
1659340	1662540	of just posters of all sorts of different types of neuroscience.
1662540	1665620	And you just realize like how vast the field is
1665620	1669580	and how little we know about it putting it all together
1669580	1671220	because it's just so complicated.
1671220	1672340	You can only sort of wrap your head
1672340	1674540	around your own little corner of the thing
1674540	1677660	but like trying to get, understand the full system
1677660	1680340	and all it's like incredible complexity.
1680340	1683500	I mean, it might just be too much for one human being
1683500	1685620	to be able to fit in their head.
1685620	1689740	And so some of our goals of trying to understand things
1689740	1693740	or make a turtle models, it might just not be possible.
1693740	1695460	We might just not, I mean,
1695460	1696740	might not be able to understand it
1696740	1698420	in a way that feels intuitive to us
1698420	1700340	even if our models work quite well.
1701340	1705060	Yeah, humans have this real desire to understand
1705060	1708020	and we create intelligible frameworks and theories
1708020	1712140	and we end up excluding most of the reality of the system.
1712140	1714060	But just before we go there,
1714060	1716260	I wanted to talk a little bit more about the brain.
1716260	1718980	So, you know, Summerfield said in his book
1718980	1722020	that the ultimate goal of the nervous system
1722020	1724540	is to avoid surprise altogether.
1724540	1728100	So when they study brains,
1728180	1731100	they see that the brain kind of lights up and activates
1731100	1732780	in a surprising situation
1732780	1735700	and less so when it sees something it's seen before.
1735700	1737780	And this also brings me to this idea
1737780	1741260	of there's a dichotomy between representationalism
1741260	1742500	and inactivism.
1742500	1744820	So the representation, this viewpoint
1744820	1747100	is that the brain does all of the thinking
1747100	1748700	and it can be in a vat,
1748700	1750820	it can be isolated from the environment.
1750820	1752540	And the inactivist school of thought
1752540	1755380	is that the brain just kind of thinks
1755380	1756820	in terms of trajectories,
1756900	1759260	in affordances given by the environment
1759260	1761940	and the brain decoupled from the environment
1761940	1763060	is completely stupid.
1763060	1765380	It just kind of like the brain only moves
1765380	1767500	through the environment through affordances.
1767500	1769060	And maybe that's a continuum,
1769060	1771300	but where do you fall on that continuum?
1771300	1773260	It's a really good question.
1774220	1778820	I mean, I think dreams are kind of the counter example
1778820	1782900	to the pure, I mean, dreams just sort of prove
1782900	1785060	we can just sort of without any sensory input,
1785060	1786740	construct very rich worlds.
1786740	1789780	So we must have some ability to just represent
1789780	1791060	some sort of models in the world
1791060	1794620	that we're not just purely sensing and receiving the world.
1794620	1798180	We have the structures that are able to put things together
1798180	1799740	in a sort of coherent reality.
1800660	1803980	And clearly there's an interaction between these,
1803980	1806060	these structures in your brain that can construct these things
1806060	1808180	and the sensory data that kind of work together
1808180	1811620	to construct how you experience things.
1811620	1814060	And so it's, yeah, it's a continuum.
1814060	1818820	I think you need the, like we are always with the world.
1818820	1821700	You need the world to sort of build up these systems
1821700	1823180	over time.
1823180	1826380	Like you're not sort of built with all of them working
1826380	1827980	just as a baby.
1827980	1830580	I mean, sure, there's like, the system is biased
1830580	1833420	in certain ways that will help it learn these things.
1833420	1838020	But yeah, you're like, so they're kind of both true
1838020	1839340	to some degree.
1839340	1844340	And yeah, it's definitely not one or the other.
1845660	1846780	Yeah, it's so interesting.
1846780	1849060	And we're speaking with Carl Friston tomorrow
1849060	1851060	and he's got this free energy principle.
1851060	1854940	And it's a kind of postulate that works at any resolution.
1854940	1858220	So even with a single cell amoeba or something like that,
1858220	1860940	there's this idea that it has a Markov boundary
1860940	1863300	and there's this kind of cyclical causalities.
1863300	1866420	So, and these boundaries I guess are relative.
1866420	1868340	So you can draw boundaries around anything.
1868580	1870060	You're a boundary, you're an agent,
1870060	1871900	but also at the microscopic scale.
1871900	1873940	And he says that all of these systems,
1873940	1877500	they just kind of predict external states
1877500	1879300	from the internal states.
1879300	1881260	And then you get this self-organized
1881260	1884300	and emergent complexity and so on that comes from that.
1884300	1887460	But he does say though that intelligence is essentially
1887460	1892460	about being able to predict a trajectory of actions.
1892460	1895820	And I don't know whether we'd call it goal-seeking behavior,
1895820	1898100	but we do that very abstractly, don't we?
1898100	1900300	But weirdly, when you look at the brain level,
1900300	1904100	it's happening at the microscopic sensory motor level.
1904100	1905580	So it's almost like how do you get
1905580	1909100	that emergent abstract intelligence from that?
1909100	1912660	That's a, I mean, yeah, that's an incredible question.
1913820	1915580	It's, I mean, it seems like this,
1915580	1918140	like what you said, this sort of idea of predicting
1918140	1920460	the future, just a couple steps into the future
1920460	1923060	that is just happening at just the circuit level,
1923060	1927420	even in the retina, that it seems like
1927420	1929820	that is a good sort of building block.
1929820	1931380	If you can sort of chain that together
1931380	1935220	over sort of larger and larger scales within the brain,
1935220	1937820	it wouldn't surprise me if that's kind of the way it worked,
1937820	1940700	this sort of, these sort of basic circuits
1940700	1943820	that are used for prediction, but with different input.
1943820	1946100	If you're just having sort of retinal ganglion
1946100	1948980	and like a photos receptor as it is at input,
1950100	1951660	it's able to just sort of do this sort
1951660	1952780	of very simple prediction.
1952780	1955380	But if you have these more complicated patterns
1955380	1959340	in the middle of visual cortex and then higher
1959340	1961940	on the same sort of circuits with different input
1961940	1963660	could sort of just be predicting
1963660	1966140	this sort of evolution of these patterns.
1967140	1971020	And yeah, it's kind of amazing what you can sort of build
1971020	1973260	out of these sort of simple rules and building blocks
1973260	1975100	if you just iterate them over again.
1976060	1980860	That was actually that sort of idea of iterating
1980860	1983500	a simple sort of computational rule
1983500	1986740	for explaining visual cortex was one of the things
1986740	1987820	I wrote about in my thesis,
1987820	1990860	but trying to explain like this middle visual cortex,
1990860	1993860	like V4, the responses there using basically
1993860	1995660	an iterated model of like V1.
1995660	1999580	So the sort of processing in V1, we fairly understood
1999580	2003300	if we have the best models of anywhere in visual cortex,
2003300	2004820	maybe even all of cortex.
2005980	2007980	And just sort of iterating the principle again
2007980	2010180	into V2 is sort of basically just assuming V2
2010180	2012380	is taking V1 inputs, but doing sort of the similar
2012380	2014500	transform and the V4 is taking like V2 inputs
2014500	2016740	and doing sort of very similar transform.
2018380	2023140	And sort of the things you see that V4 is sensitive to
2023140	2025660	are these complicated patterns and textures.
2026580	2029220	And you get complexity very quickly
2029220	2031060	from just iterating the sort of simple rules.
2031060	2032500	And I mean, that's what neural networks
2032500	2034420	are essentially doing.
2034420	2036980	They're just often just doing linear transforms
2036980	2038860	with non-linearities over and over again,
2038860	2041580	just iterating these simple transforms
2041820	2044020	and building up the complexity very quickly.
2045020	2047020	Yeah, I think there's something really magical
2047020	2050980	about this reflexivity or I mean, a great example of that
2050980	2052860	are there are graph cellular automators
2052860	2055780	along the lines of Wolfram's digital physics project.
2055780	2057740	And the really clever thing is that you're using
2057740	2059220	the same rules, but you're just kind of like
2059220	2061740	running the result again on top, on top.
2061740	2064740	And there's a similar version with a graph cellular,
2064740	2067780	sorry, a CNN cellular automata,
2067780	2070180	where you model something at the microscopic scale
2070180	2072940	and you get this emergent global phenomenon.
2072940	2076220	So it might kind of materialize as an image of a gecko
2076220	2077060	or something like that,
2077060	2079300	but you've actually coded it at the low level.
2079300	2082620	But yeah, that brings me to this universalist idea
2082620	2084340	of let's say how brains work,
2084340	2087020	but maybe how neural networks and intelligence work.
2087020	2090140	Vernon Mount Castle, I read about this in Jeff Hawkins' book.
2090140	2091780	He had this very simple idea of the brain
2091780	2096580	as being lots of repeated copies of the same circuits
2096580	2098020	in the neocortex.
2098020	2100940	And I think this is contested by many neuroscientists,
2100940	2104140	but they differ only in how they are wired.
2104140	2105900	So they're wired to different, you know,
2105900	2107660	sensory motor circuits.
2107660	2110180	And they're essentially just a copy of the same thing.
2110180	2113580	And as you say, they themselves get called reflexively,
2113580	2115320	recursively, and so on.
2115320	2117740	And then you just get this emergent intelligence.
2117740	2120620	I mean, what's your view on this universalist idea?
2120620	2123580	I mean, there's definitely not just one circuit.
2123580	2125700	I mean, as you look through the cortex
2125700	2128020	in different areas of the brain,
2128020	2129940	just the laminar structure,
2129940	2133020	which these sort of circuits are like supposed to be,
2133020	2134480	like where the columns are supposed to be,
2134480	2137600	where these sort of circuits are supposed to be defined,
2137600	2140920	it changes, like, but there are definitely commonalities,
2140920	2142660	but there's, I mean, it makes sense
2142660	2144900	that maybe the circuits in different areas
2144900	2148100	should be slightly different for the different purposes
2148100	2150940	between like prefrontal cortex and say,
2150940	2155020	where you have much more higher order types
2155060	2157700	of processing going on than like visual cortex
2157700	2159620	or auditory cortex.
2159620	2160940	And so there's probably,
2163340	2165340	if you go in this direction of thinking of some,
2165340	2167100	there's like, there's probably a small number
2167100	2169060	of these types of circuits that interact in various ways,
2169060	2171700	but there is definitely some specialization going on.
2172860	2176420	Yeah, like, having universalist ideas in biology
2176420	2178300	never seems to work out that well.
2178300	2180700	There's just so much diversity and complexity.
2180700	2182460	It would be nice if we could reduce everything
2182460	2184300	down to like, it's one thing repeated over,
2184300	2188660	but like generally, it never works out quite as cleanly as that.
2189700	2191260	Yeah, again, it's our desire
2191260	2192700	to have an intelligible framework.
2192700	2194100	And I mean, the free energy principle,
2194100	2195660	you could argue as a theory of everything,
2195660	2198340	but there's, I mean, Stephen Wolfram's example,
2198340	2201220	and even Eric Weinstein's geometric unity.
2201220	2203620	I mean, there are many theories of everything.
2204500	2207420	But yeah, what do you think is the role of language
2207420	2209420	in cognition and thinking and planning?
2210420	2214020	Um, that's, it's a really interesting question.
2215300	2218100	It's, and it's also, I think, a kind of hard one to answer
2218100	2221900	in the sense that if you, I've seen some recent reports
2221900	2226620	just like talking about like, other people asking,
2226620	2228620	like survey questions to other people
2228620	2231580	and finding some people like, don't have an interior monologue
2231580	2233620	in the same way you might think.
2233620	2236060	And just like, there's actually a lot of diversity
2236380	2239860	in like people's level of internal monologues.
2239860	2242340	And they've done studies where they have this like little,
2242340	2244500	like beepers go off and people are supposed to write
2244500	2245500	what's going on in their mind.
2245500	2250060	And so it's, and yeah, and just with visual imagery,
2250060	2253060	we find that it's a huge like variety in how much,
2253060	2255740	like how strong people rate their visual imagery.
2255740	2260340	And so, I mean, yeah, some people, I mean,
2260340	2263860	me personally, I have both, I mean,
2263860	2266140	pretty strong interior monologue,
2266140	2268220	but I also feel like a lot of ideas
2268220	2270780	are in this sort of pre-linguistic state.
2270780	2274460	And I'm kind of like searching for the words for them often.
2274460	2277220	And there's definitely kind of continuum there.
2278380	2282700	It's weird to think like, how do we get the words
2282700	2284900	that we're saying, where the words come from
2284900	2285860	that are coming out of our mouth?
2285860	2287580	Are we really choosing them?
2287580	2288820	You're definitely not choosing them
2288820	2289900	in this sort of top-down way.
2289900	2292620	They just sort of seem to come out.
2292620	2295900	And you just kind of point yourself in the right direction
2295900	2298140	and hope the best as they come out.
2299340	2301180	And, but this has a very different quality,
2301180	2303740	like when you're just speaking phenomenologically,
2303740	2305740	it feels very different to when you're just sort of
2305740	2307580	thinking yourself, what should I do today?
2307580	2308780	Should I go to the store?
2309860	2313460	And so, I mean, yeah, the way in which language
2313460	2316180	interacts with thoughts and behavior
2316180	2321180	and verbal communication, it's definitely not simple.
2323380	2327300	And yeah, there's, I mean, definitely this kind of continuum.
2327300	2330380	I mean, it's all, it's, to me, I just sort of think
2330380	2332420	it's with all these sort of networks kind of interacting.
2332420	2335300	And sometimes you're like triggering the kind of language
2335300	2336940	things and you're just making these kind of patterns.
2336940	2339300	And sometimes the language patterns you're activating
2339300	2341860	are helping activate other things as well.
2341860	2344340	Sometimes you can just be in this kind of less-linguistic state
2344340	2347940	where you just kind of, just sort of sensing these patterns
2347940	2350340	and you just have this kind of like wandering thoughts
2350420	2353260	that aren't necessarily linguistic.
2354540	2357620	But yeah, it's definitely, I mean, and also it seems,
2357620	2361060	yeah, as I said, people's, the way people do this
2361060	2362500	like seems all over the place.
2362500	2366420	And so there's not sort of even one answer for even one person
2366420	2368220	or definitely not across all people.
2369180	2370900	Yeah, I'm really interested in this idea
2370900	2373940	of differential kind of subjective experiences.
2373940	2375980	And you know, like there was that Nagel paper
2375980	2377220	about what does it like to be a bat?
2377220	2378980	But even with the human experience,
2378980	2380140	we're all very different.
2380140	2381740	You said about your internal monologue
2381740	2382820	and I hadn't really thought about
2382820	2383780	how that might be different.
2383780	2386860	But I was drawing a picture in a Valentine's card earlier
2386860	2388820	and it was so terribly bad.
2388820	2390380	And some of my friends are really good artists
2390380	2392620	and I was kind of thinking to myself at the time,
2392620	2395100	maybe this is just a, this is just me.
2395100	2397780	I can't really visualize things in my mind very well.
2397780	2399060	I've got a very analytical brain.
2399060	2400100	Won't mean that certainly
2400100	2402300	when not under the influence of psychoactive drugs anyway.
2402300	2404900	But you know what I mean.
2404900	2407140	So we all have a very different subject of experience
2407140	2410020	but the miracle is we can understand each other
2410060	2410900	so well.
2410900	2413260	So you would expect there to be an incredible amount
2413260	2415900	of Britanness in our communication, but there isn't.
2417540	2422100	Um, yeah, it's, so I often wonder about this too.
2422100	2426580	Just, I feel like the misunderstanding
2426580	2429580	happened a lot more than even people realize.
2429580	2431860	And you can sometimes, you only really notice
2431860	2434060	when they become kind of big and matter.
2434060	2437260	And especially like, people can think
2437260	2438980	they're having a conversation.
2439020	2441500	And sometimes even from the outside, you can see like,
2441500	2444420	these people are just talking completely past each other.
2444420	2445500	And you can kind of see
2445500	2446860	that they're not really understanding each other
2446860	2448860	even though they maybe think they are.
2449940	2454940	And so, yeah, I don't know how not brittle they are.
2456340	2460620	I think they, I think we think they're less brittle
2460620	2462140	than maybe they are.
2462140	2465100	I think sometimes we assume people are understanding
2465100	2467260	what we're saying better than they actually are
2467260	2469060	because they nod and smile at us.
2470340	2473580	And because that's, it makes us feel good
2473580	2474580	for people to understand us.
2474580	2478100	It makes us good to feel, to understand other people.
2478100	2482700	But yeah, I mean, it's, I mean, clearly,
2482700	2483900	we do have a lot in common.
2483900	2485580	And there's definitely things we can understand
2485580	2487260	about each other.
2487260	2490740	But yeah, it's like, I do sometimes think
2490740	2492340	that maybe we're more different from each other
2492340	2493580	than we really realize.
2494580	2497180	Yeah, that's a really fascinating thought.
2497180	2498540	I mean, we speak a lot with Waleed Saber
2498540	2501260	and he says how language has evolved
2501260	2503420	to be extremely ambiguous actually
2503420	2504740	because it's a form of compression.
2504740	2506820	So we don't say everything we mean
2506820	2508660	and we'll get into like language models in a minute.
2508660	2511220	That's part of the reason why they don't understand things
2511220	2513700	is because a lot of information is not in the text.
2513700	2516620	And Waleed says that we have a lot of,
2516620	2517940	what he calls naive physics.
2517940	2520420	So we understand that objects can't be in two places
2520420	2521260	at the same time.
2521260	2523220	And if something is located inside
2523860	2525540	something else and we move that thing somewhere else,
2525540	2527300	then the thing inside has also moved.
2527300	2530620	So we're doing all sorts of reasoning on the fly.
2530620	2532700	And what we're kind of doing is like,
2532700	2535300	we're disambiguating out of the 50 meanings
2535300	2537700	of an utterance into the meaning.
2537700	2541580	And like it just, we almost always understand each other.
2541580	2544380	You know, you wouldn't really expect that.
2544380	2549380	No, I mean, yeah, I mean, we generally have like,
2550500	2552660	I mean, our understanding of physics
2552660	2555660	should generally be compatible with each other.
2555660	2560380	I do feel like it's, in most cases, yes,
2560380	2562220	we do very clearly understand each other
2562220	2565420	because in most cases, it's more like well-defined.
2565420	2569180	I think the trouble gets in sort of like fuzzier areas
2569180	2573340	about people's like emotions or opinions about things
2573340	2576680	where our priors are more sort of maybe less
2577820	2580820	less tied to like objective things like physics
2580820	2583420	and are more sort of just tied to like our upbringing
2583420	2586260	and just sort of whatever ideas, notions we have
2586260	2588980	about how people should like behave and interact
2588980	2591220	and what like our value systems.
2592420	2594620	And so, yeah, when people are talking about
2594620	2597020	some of these common things, I feel like they're more likely
2597020	2599420	to be able to like talk past each other and not realize it
2599420	2602100	because they're sort of assumptions about what is important
2602100	2606220	or what is meaningful might be different from each other.
2606220	2607820	Yeah, actually, you're absolutely right.
2607820	2610300	So we don't have an objective phenomenology
2610340	2613420	and I used to do, there's a thing called quantified self
2613420	2615140	where you kind of like keep a diary every day
2615140	2617900	and you record how you're feeling in that day.
2617900	2620060	And feeling is a subjective state.
2620060	2622180	So I remember at the time that every single day
2622180	2624940	I needed a new word to describe how I was feeling
2624940	2627740	because the old word I was using didn't work anymore.
2627740	2629460	So the number of words kept growing.
2629460	2631300	And actually, that's so true, isn't it?
2631300	2633180	If I tell you how I'm feeling right now,
2633180	2634620	that's completely brittle.
2634620	2636020	So there are some things in the world
2636020	2638060	that are quite informational and objective
2638060	2639660	and we can communicate very well.
2639700	2642380	And then when we're bordering on anything subjective,
2642380	2644220	language fails us.
2644220	2647220	Yeah, and then we were trying to map whatever word
2647220	2649180	you're using on to how I would use that word
2649180	2651380	to describe the feeling that I would be having.
2651380	2655700	And that mapping seems completely like without a long
2655700	2658660	conversation to try to like feed up that mapping.
2658660	2663220	Oh, it could be quite different in how I would apply
2663220	2665340	that word to my own feeling.
2665340	2666900	Yeah, and there's been studies done as well
2666900	2669540	that I think certain tribes have a completely different
2669540	2672180	color perception and there are also concepts
2672180	2674460	like vagueness, so what is a pile of sand
2674460	2676260	and what is a shade of red?
2676260	2678260	And these things are actually very, very difficult
2678260	2679820	to communicate objectively.
2681260	2684980	Yeah, things like color perception
2684980	2686580	are kind of the interesting ones
2686580	2689740	because the literature is a bit messy
2689740	2691060	on some of these topics.
2692220	2696660	And some of it is just where you draw the lines
2696660	2699940	between colors and then how those linguistic boundaries
2699940	2701500	affect perception.
2701500	2704340	There definitely seems to be both things going,
2704340	2708260	but it's not sort of, I don't think any,
2708260	2709780	I think it's a strong claim to be like,
2709780	2713980	oh, the people can't perceive green or something like that.
2715100	2716820	It's just like where they would draw the line
2716820	2718980	between blue would be in a slightly different place
2718980	2721620	and then they might kind of see them as being,
2721620	2723860	sort of experience them as being like further apart
2723860	2726820	or closer together than you would necessarily,
2726820	2730380	but yeah, that's really,
2730380	2734220	their experience, that's really be super alien to you,
2734220	2739220	but they're sort of experience of maybe more very cultural,
2740980	2742700	like cultural taboos or something
2742700	2744500	would be very different than yours.
2746660	2748340	Yeah, I mean, one thing you're alluding to there is,
2748340	2751940	it's when we deal with complicated systems,
2751980	2754300	there's a real problem about drawing boundaries.
2754300	2756540	And I was, I mean, Friston's a great example,
2756540	2758220	he's got this idea of a Markov boundary
2758220	2759860	and it could be at the cellular level
2759860	2762420	or it could be you as a person.
2762420	2764940	And then when we talk about things like agency and free will,
2764940	2767220	we tend to anthropomorphize this boundary.
2767220	2769220	So we tend to think of ourselves as individuals,
2769220	2772420	but actually you could draw boundaries at different scales
2772420	2774700	and the boundaries might be observer relative as well.
2774700	2776740	So your boundary might not be my boundary.
2777740	2779020	Yeah, exactly.
2782860	2786580	Yeah, there's something I know a ton about,
2788580	2791820	how you define yourself and how you think of yourself
2791820	2793900	within the context of your community and whatnot.
2793900	2796980	I mean, some of these ideas are just very cultural
2796980	2801540	and how you experience yourself is probably even like,
2801540	2804340	very different, can be very different cross-pulturally.
2804340	2805660	Yeah, I mean, maybe one thing to bring in
2805660	2809060	is when you're, as a quant, when you're doing modeling,
2809060	2811300	you have this very, very complex system
2811300	2814420	and you draw boundaries and you create variables
2814420	2816980	and observables and do you know what I mean?
2816980	2818380	You kind of build a model
2818380	2820940	and that boundary could exist at any scale.
2820940	2822980	It seems like quite a,
2822980	2826100	it's a bit of an art and a science at the same time.
2826100	2827380	Yeah, no, for sure.
2828980	2829820	Yeah, exactly.
2829820	2833020	That's kind of why I like some of these complicated problems
2833020	2834740	that are not very well-defined
2834740	2837660	where you kind of have to use intuition
2837660	2841420	and or just sort of do the best you can do
2841420	2843660	at sort of drawing what are the relevant variables,
2843660	2845740	what sort of a priori makes sense to me
2845740	2849260	to be the things that matter for the system,
2849260	2852660	behaving at this within the context of this experiment
2852660	2854820	or in the context of this market or whatnot.
2855820	2860060	Trying to draw boundaries in because the rules
2860060	2861740	for these systems are not clear.
2861740	2865380	Like what are all the relevant variables for everything
2865380	2867820	and do you have access to them and can you control them?
2867820	2869700	And generally you don't know them all
2869700	2871500	and you don't necessarily have access
2871500	2872900	or can control any of them.
2874180	2878180	And so it's, yeah, it is a bit of an art.
2880100	2882460	Well, now might be a good time to talk about numerators.
2882460	2884700	So you're the chief scientist
2884700	2886940	and it's this insanely cool platform, right?
2886940	2889700	So people can go on there, they can download data sets,
2889700	2890700	they can build their own models,
2890700	2891900	they can stake the models.
2891900	2894500	I mean, why don't you just talk me through it?
2894500	2895340	Sure, yeah.
2895340	2897780	So we advertise ourselves as being
2897780	2900820	the hardest data science problem on the planet
2900820	2904100	because I think it is because like I said,
2904100	2906020	the correlations you're chasing are on the order of like
2906020	2910100	three or 4% out of sample, which,
2910100	2912060	and just sort of being able to tell
2912060	2914300	do you have something real or is it just in the noise
2914300	2916980	can be extremely hard to do, which is,
2916980	2920100	and we set up the problem for participants.
2920100	2923300	You give out a set of data that has been cleaned
2923300	2926500	and obfuscated and regularized.
2926500	2928740	And you basically just have a set of features
2928740	2930060	and a set of targets.
2930060	2931180	And you're just trying to build models
2931180	2932300	to go from features to targets.
2932300	2936300	So it's sort of a very classic machine learning style problem
2936300	2939020	and it's nicely curated for you.
2939020	2942460	And how it works for us is every week,
2942460	2944740	people submit predictions on a new set of features.
2944740	2946620	So every week we release a new set of features
2946620	2948420	and people just run their models over those features
2948420	2950300	and give us a set of predictions.
2950300	2952780	And people stake on those predictions.
2952780	2956140	And so people stake our cryptocurrency called NMR.
2956140	2959500	And if their predictions do well, they make money.
2959500	2961420	And if their predictions don't do well,
2961420	2962540	that week they could lose money.
2962540	2966020	And they sort of are expressing their confidence
2966020	2967780	in their models using their state.
2968740	2972780	And so we basically use this expression of confidence
2972780	2975300	as a way to sort of integrate these signals
2975300	2976780	into our meta model.
2976780	2979300	Our meta model is really just like a stake weighted average
2979300	2981780	of all the signals people are submitting.
2981780	2984580	And these signals, these predictions are just sort of
2984580	2986620	weights on stocks.
2986620	2988620	They're sort of like, how do we want to go long
2988620	2991140	or do we want to go short in the stock?
2991140	2992500	There's sort of just expressing,
2992500	2995540	do we think a stock is going to go down or going to go up?
2995540	2999180	The stake weighted model we feed to through our optimizer,
2999180	3001500	which is just doing a convex optimization problem,
3001500	3004060	trying to create a portfolio from the signal.
3004060	3007300	And is that portfolio changes week to week?
3007300	3010340	And so that's just the difference between the previous
3010340	3012980	and the new portfolio is just what we trade every week.
3012980	3015580	And so our trading is basically completely determined
3015580	3018500	by the like thousand people all over the world
3018500	3020580	submitting predictions every week.
3020580	3025060	And so it's this very kind of nice decentralized hedge fund
3025060	3028100	where the signal generation is very decentralized.
3028100	3030040	And we get the advantage of ensembling
3030040	3032860	over a wide variety of models.
3032900	3036340	And so people are trying to make their models
3036340	3041340	both predict the targets very well and consistently.
3041820	3046820	And we have other incentives to try to make them predict
3046820	3049180	aspects of the targets that other people are not
3050100	3052340	to try to, so that their contribution is sort of more
3052340	3055040	unique and they can make quite a bit of money
3055040	3057340	by having their predictions be pretty different
3057340	3060100	from other people's, but also still accounting
3060100	3062420	for like variance in the target.
3062420	3066700	And so that system is what we call true contribution.
3066700	3070420	And it's really, we try to, it was our attempt
3070420	3073740	to try to make people's predictions and payouts
3073740	3076280	more tied to actual portfolio returns.
3077900	3079740	Because the sort of standard scoring
3079740	3082660	and we're just doing the correlation of how well
3082660	3086640	your predictions match like the new weekly week's target
3086640	3090600	that is determined by just how the stocks move that
3090600	3092580	over the course of 20 days.
3093800	3096720	The true contribution is basically sort of doing
3096720	3099520	the whole process, like creating the meta model,
3099520	3101120	running it through the portfolio optimizer,
3101120	3103240	getting the portfolio, getting portfolio returns.
3103240	3105800	And then we try to see like, take the gradient
3105800	3108520	through all of that until you can find out
3108520	3110360	if people's stakes have been more or less,
3110360	3112200	would we have made more or less money
3112200	3115360	and use this gradient of the stakes
3115360	3117320	with respect to the payout, with respect
3117320	3120440	to the portfolio returns as a way to pay people out
3121280	3123360	to essentially increase their weight or decrease their weight.
3123360	3125200	And that tends to reward people
3125200	3127200	with more unique contributions.
3128240	3129840	I've got so many questions.
3129840	3133160	So, I mean, I really like this idea
3133160	3136000	because first of all, you're democratizing the whole thing
3136000	3139680	and you're kind of gamifying it and it's a meritocracy.
3139680	3142640	So any data scientist can go on there
3142640	3144440	and flex their muscles and build great models
3144440	3145680	and be recognized for doing so
3145680	3147400	and even earn money for doing so.
3147400	3149940	But in a way, I want to contrast it to somewhere
3149980	3151300	like Kaggle.
3151300	3154260	Now, on Kaggle, I mean, traditionally data science
3154260	3157820	has been about understanding the domain.
3157820	3160340	A lot of data science is business analysis essentially
3160340	3164060	and kind of understanding what makes something work
3164060	3165220	in a model.
3165220	3167540	And as I understand with Numeri,
3167540	3169060	the interface is kind of the same.
3169060	3172660	So maybe they get similar shape of data every time
3172660	3174340	they build the models on it.
3174340	3176620	And in this domain, because you know,
3176620	3178780	like there's technical analysis and there's fundamentals
3179060	3180740	they might still understand some market.
3180740	3183020	They have some kind of extrinsic understanding
3183020	3184060	of why their model would work,
3184060	3187220	but they don't have the same kind of understanding.
3187220	3190460	No, yeah, the features include all sorts of things
3190460	3191940	from like analyst sentiments
3191940	3194620	and other sort of fundamental things
3194620	3197220	to technical features.
3197220	3200220	But that is all sort of obscured from people.
3200220	3202660	People just have these funny feature names.
3202660	3204180	And so it's up to them to just use
3204180	3206420	their sort of machine learning toolbox
3206420	3208100	to figure out what are the good features
3208100	3210740	for predicting what features tend to work well.
3211660	3214300	How do we combine those features?
3214300	3217100	And so we actually wanted to kind of like remove
3217100	3219620	any of the people's biases for what features
3219620	3221020	they think will work.
3221020	3225380	We wanted to not have people's financial intuitions
3225380	3226220	play into it.
3226220	3227340	We wanted to just sort of set it up
3227340	3229900	as a pure machine learning problem.
3229900	3234300	To try to make it, yeah, basically to make it be better
3234300	3236620	than any human could possibly be.
3237460	3240420	So with this sort of combined ensemble wisdom
3240420	3242700	with the crowd, we're trying to make it
3242700	3245460	like the alpha go of like finance,
3245460	3246820	something that it's just that performs
3246820	3248100	at like a super human level
3248100	3250100	in ways you don't really understand.
3251300	3252140	Interesting.
3252140	3254820	And you're aggregating the predictions together
3254820	3255740	in some way.
3256700	3259260	Yeah, it's actually fairly simple.
3259260	3262180	We, I mean, people submit their predictions
3262620	3264980	which are just a number between zero and one
3264980	3265860	for every stock.
3266940	3269300	And it's basically just a rank ordering of stock.
3269300	3272700	And we just normalize everyone's predictions
3272700	3274380	and then just weight them by their stake
3274380	3276180	and then just average them together
3276180	3278460	and to do another renormalization
3278460	3279980	so that it's the right scale
3279980	3281980	and sort of distributional shape
3281980	3283380	to be fed to the optimizer.
3284540	3288420	But it's a fairly simple and robust way to weight things.
3288420	3290980	We're basically just using people's express confidence
3290980	3294420	in their model as the weighting system.
3294420	3297220	And because there's this feedback of payments
3297220	3299980	and paying out people, good models,
3299980	3301260	their stakes increase over time
3301260	3303420	so their weight in the meta model increases over time
3303420	3306100	and bad models, their weights decrease over time.
3306100	3308620	So it's kind of like a human in the gradient descent
3308620	3310140	for doing like gradient descent
3310140	3313420	with the stakes as the weights in the model.
3313420	3314260	Fascinating.
3314260	3315540	And can you give us any intuition
3315540	3317540	on how that model is tuned
3317540	3320500	and what kind of penalty you're using?
3320500	3322100	Are you using just the stakes
3322100	3325140	or also the previous performance?
3325140	3327820	So no, we're not actually using the previous performance.
3327820	3329300	It's really just stakes.
3329300	3332300	The previous performance only enters into the fact
3332300	3334460	that the good performance of the past
3334460	3336860	would have made their stake grow over time.
3338220	3342020	And but we have thousands and thousands of models now.
3342020	3345780	And so any one model is only a very small percentage
3345780	3346780	of the meta model.
3347860	3349780	And even the ones that are the biggest
3350020	3352700	maybe only a couple percent of the total meta model.
3352700	3355580	And it's, it is a sort of like power law distribution.
3356500	3358940	There is a lot of work that I've done
3358940	3361420	in the portfolio optimization set.
3362780	3366740	And that's the going from the signal to the portfolio.
3366740	3370060	And there is actually a lot that goes on in there as well
3370060	3371900	just in how you construct a portfolio,
3371900	3375220	how you determine how much you're gonna trade each week
3375220	3378820	and how you make your portfolio, what you make exposed to.
3378820	3381100	Exposed really just means is are the weights
3381100	3383940	of your portfolio correlated with lots and lots of things?
3383940	3387060	And so there's a lot we go due to try
3387060	3389220	to make the portfolio weights not correlated
3389220	3391180	with the market overall.
3391180	3395980	So we're a market neutral hedge fund.
3395980	3398060	So we try to be uncorrelated to the market,
3398060	3400020	have a beta of zero.
3400020	3401540	So when the market goes up or down,
3401540	3405060	you can't really tell how we would do on that kind of day.
3405060	3406940	And but we also try to be uncorrelated to lots
3407180	3408820	of other things that we think could drive returns.
3408820	3412380	So we try not to have like big country biases,
3412380	3415060	big sector biases, factor biases.
3415060	3417100	So factor are things like value and momentum,
3417100	3419860	these kind of like more abstract quantities
3419860	3421020	that are supposed to tell you something
3421020	3424700	about classes of stocks.
3424700	3428140	But we try to be uncorrelated to basically everything.
3428140	3430060	And they're just trying to get the sort of pure machine
3430060	3433940	learning non-linear signal that is driving stock returns
3433940	3436220	or like stock specific alpha,
3436220	3439860	we call it sort of like the amount of stock is going to,
3439860	3442540	how all the stock is going to do sort of just by itself,
3442540	3443980	not taking all these other things
3443980	3445860	that are about it into account.
3445860	3446700	Yeah, that's really interesting.
3446700	3448820	And I guess like one of the problems on Kaggle
3448820	3451660	is that most of the solutions are so overfit
3451660	3455300	to the training set that they never generalize
3455300	3456460	to real world versions of the problem.
3456460	3458220	But what you're doing actually is to kind of like
3458220	3460940	remove away a lot of those opportunities for overfitting
3460940	3462860	and also allowing the models to be used again
3462860	3464100	when the next thing comes around.
3464100	3466180	But just quickly on the aggregating stuff
3466260	3468140	the reason I'm interested in that is on my PhD
3468140	3470460	I did prediction with expert advice
3470460	3472620	and there's a whole load of theoretical approaches
3472620	3474740	to that where you can have an aggregating algorithm
3474740	3476860	that produce, you know, that produces performance
3476860	3479500	or a kind of like an error bound
3479500	3482180	which is not much worse than the best path
3482180	3483820	of switching experts.
3483820	3486260	So if you took the optimal path of the best expert
3486260	3488180	every single time step, you can have algorithms
3488180	3491340	that have approvable bound not much worse than that.
3492300	3497300	Yeah, we, so we've done a lot to try to experiment
3497460	3499580	with trying to improve upon stake waiting.
3500860	3504060	And it's always been really hard to do it in a robust way.
3505260	3508780	It's, I mean, for one, stake waiting is,
3508780	3513100	it's sort of nice in that it's easy for people to understand.
3513100	3516020	People are, it's very clean in how it works.
3516020	3517780	It sort of fits with the ethos
3518020	3522300	and the, like, and the idea of the company
3522300	3525620	of how it's distributed and decentralized
3525620	3529380	and you express your confidence by your stake.
3531540	3533660	But it does sort of seem like there should be a better way
3533660	3535340	to aggregate models.
3535340	3539340	But pretty much every time we try to find something better,
3539340	3542260	it's, it might be a little bit better
3542260	3543540	but it's like less robust.
3543540	3546100	It tends to just be less robust.
3546100	3549660	And it's, because you are essentially just sort of fitting
3549660	3552780	to the past and to try to find way to the models
3552780	3555580	or something, it tends to just like overfit
3555580	3557620	and this sort of stake waiting thing,
3557620	3558580	you can't really overfit.
3558580	3562300	It's just sort of a property that just sort of evolves
3562300	3563620	as the tournament goes on
3565060	3567860	without ever considering like the past performance
3567860	3569100	and all of these things.
3569980	3573140	So yeah, it's, it's been kind of interesting to,
3573140	3576020	so it's one of these things we sort of revisit every year
3576020	3576860	at some point of like,
3576860	3578540	let's try to build a better meta model
3578540	3582500	but we usually just come back to stake waiting in the end.
3582500	3583540	Yeah, well, in a way, I mean,
3583540	3584740	we're prediction with expert advice,
3584740	3585940	you have a learning rate
3585940	3587620	and I guess you don't even have that problem
3587620	3590620	because you're just using the stakes as the-
3590620	3594540	Yeah, the, but yeah, the, I mean, our learning rate.
3594540	3596660	So I mean, our payout system is the way
3596660	3598740	we adjust the weights over time.
3598740	3602420	And so we have done like some simulations to show
3602420	3604500	that like if, how we reward people,
3604500	3606060	how that affects their weights over time
3606060	3607940	and how that affects meta model performance.
3607940	3609780	So you wouldn't want to have a payout system
3609780	3612700	that would make the meta model worse over time.
3612700	3615340	And so yeah, like this, this true contribution idea
3615340	3617380	that's gradient of the stakes,
3617380	3619100	we did simulations to show
3619100	3621740	it does actually improve the meta model over time
3621740	3623100	to pay out in this way.
3624020	3627500	It's nothing, I mean, people do things
3627500	3629380	like take their stakes out, withdraw money.
3629380	3630700	And so it's not a perfect system.
3630700	3631940	People entering the tournament,
3631940	3633580	people, some people entering with a lot of money,
3633580	3635620	some people entered with not that much money.
3636700	3640300	And so yeah, it takes time for these things,
3640300	3642740	all the kind of shake out in real life.
3642740	3646820	But the overall idea is that we are essentially adjusting
3646820	3648340	the weights through our payouts
3648340	3652380	towards this sort of more optimal meta model over time.
3652380	3653220	Interesting.
3653220	3656620	So I'm actually very, very interested to give it a go.
3656620	3658420	And I guess like, first of all,
3658420	3660540	you could sketch out what the process looks like.
3660540	3662500	I mean, let's say I had a few hundred dollars
3662500	3663460	and I wanted to build a model.
3663460	3666700	And also it's got to be a good model.
3666700	3667540	Let's face it.
3667540	3668700	So if I just logged on there
3668700	3673700	and I built a gradient booster tree model, would that work?
3673740	3674820	It would actually.
3674820	3677780	So I mean, it's tabular data
3677780	3679380	and tabular data is very minimal
3679380	3681340	to gradient boosted trees.
3681340	3683900	We have a lot of example models that we have put up
3683900	3686700	and they're doing quite well.
3686700	3689620	So basically all you have to do
3689620	3691460	is you can go to the website
3691500	3693580	and just download a big zip file
3693580	3697140	that includes all the data in parquet.
3697140	3700740	And then you can just open it up in Python
3700740	3702060	and fit a gradient boosted tree.
3702060	3703100	When we have example scripts
3703100	3706300	sort of showing this along with some more interesting types
3706300	3709300	of pre-processing and other sort of ideas
3709300	3710700	like feature neutralization.
3712340	3714100	So I can talk about it in a second.
3714100	3717220	But yeah, a lot of our sort of standard internal models
3717220	3719660	use basically gradient boosted trees.
3719660	3724140	And we are, I mean, we have basically example models running
3724140	3727100	that and they all have positive correlation with
3727100	3729220	and true and still true contribution.
3729220	3731140	So they're actually working out of sample
3731140	3733020	and performing quite well.
3733020	3735540	That hasn't all been sort of eaten up
3735540	3737580	by people using similar enough models.
3738660	3739980	There's a lot of opportunity
3739980	3742460	to make sort of unique models too.
3742460	3745540	Cause one thing that's sort of unique about our tournament
3745540	3747740	is we release actually several targets,
3747740	3749980	we release 20 something targets.
3750900	3752340	And they're all constructed
3752340	3754260	in somewhat slightly different ways.
3754260	3758540	And you can find that if you train on a different target,
3758540	3760140	it might work almost as well as training
3760140	3761940	on the target that you're scored on.
3764020	3765940	And it might also ensemble really well
3765940	3767700	with a model trained on different targets.
3767700	3770060	And so you can actually create ensembles fairly easily
3770060	3771900	just by training on different targets.
3772900	3775540	Because it is kind of remarkable
3775540	3777980	that a model trained on a different target
3777980	3781220	can actually work better on the target you're interested in.
3781220	3783340	But that kind of thing, yeah, definitely does happen.
3783340	3785980	I mean, part of it is called all the correlations are so low,
3785980	3789300	but some targets might just sort of have a better property
3789300	3794100	in making your model pick up on the actual signal
3794100	3797260	that you want to model rather than sort of variance
3797260	3800620	that is like not that you don't want to model.
3800620	3801460	Interesting.
3801460	3804820	I think one of the issues is you might not know
3804820	3807060	what models that people are using.
3807060	3809700	But I wondered if you did have any intuition,
3809700	3811140	I'd be fascinated to know,
3811140	3813060	are they using very complex models?
3813060	3814500	Are they using simple models?
3815740	3818660	From talking to participants, there was a huge range.
3818660	3821460	There are some people using like extremely simple trees.
3821460	3823340	There are some people who are using
3823340	3825620	incredibly elaborate neural networks
3825620	3827900	with very sort of custom architectures
3828860	3831460	that are sort of designed to the problem.
3831460	3834140	There, yeah, there's a whole huge, right?
3834140	3835700	I mean, there's people who have huge ensembles.
3835700	3837940	There's people who are doing kind of like online learning
3837940	3840660	where their model is actually using the features
3840660	3843340	that were released that week
3843340	3846980	and sort of using that in some sort of unsupervised learning
3849140	3851500	and then so they take some while from when we released,
3851500	3852900	they can't just like run their model
3852900	3854100	through the new set of features.
3854100	3856260	They have to incorporate this new set of features
3856260	3858820	in this unsupervised way before they can,
3858820	3860740	so yeah, there's an incredible variety
3860740	3863100	of techniques people are using.
3863100	3865460	Fascinating, and how big is this parquet for?
3865460	3869220	How many rows, how many fields
3869220	3872820	and are they all just real numbers between naught and one?
3872820	3877020	So yeah, so there's, how many features are we up to now?
3877020	3879580	We have a couple thousand features roughly
3879580	3883860	and there's a few million, a couple million rows, I think.
3885380	3888700	So one sort of additional piece of structure in the data
3888700	3890700	is there's these things called eras
3890700	3893580	and the eras are essentially just the weeks
3893580	3896500	and because the competition has this structure
3896500	3898420	of we're making predictions every week
3898420	3902420	and so within each era, there's like say 5,000 rows
3902420	3904620	which are basically like 5,000 stocks
3904620	3908540	and so one sort of interesting thing is you are,
3908540	3910740	you want your model to be good across eras,
3910740	3912500	not necessarily across samples
3912500	3915740	and so it creates a different structure
3915740	3917140	in how you think about the problem
3917140	3919140	because you want your model to be consistently good
3919140	3922460	in every era and that can give you a different solution
3922460	3925180	that if you just try to say maximize some metric
3925180	3930180	over the whole training set which is kind of, yeah.
3930660	3934780	But yeah, it is basically just a big parquet file.
3934780	3937380	We do divide it into like training
3937380	3941300	and like there's like a testing set
3942980	3946980	but yeah, do you have any specific questions
3946980	3948660	about how that is organized?
3949620	3951660	Well, again, I'm really interested
3951660	3956100	because on my PhD, I did a whole bunch of prediction models
3956100	3957500	on financial data sets.
3957500	3959660	I was predicting like the implied volatility
3959660	3963580	of the Black Shells formula on some futures data
3963580	3967180	but my big thing at the time was I was fascinated
3967180	3969660	by regimes in financial data
3969660	3972780	and you get these changing dependencies with time
3972780	3975420	and what I did, I mean, you could actually visualize it
3975420	3978620	if you build a load of expert models
3978620	3980700	on different regimes and then you get them to predict
3980700	3982700	on the other regime's data.
3982700	3984780	You get this kind of self-similarity matrix
3984780	3987300	and it looks like you get this kind of structure in there
3987300	3989260	because there are certain regimes
3989260	3991160	where this particular model actually predicts
3991160	3992500	quite far out into the future
3992500	3993740	and then it might suddenly go dead
3993740	3995340	so you get these kind of squares
3995340	3998100	and I had this big thesis that if I have expert models
3998100	3999620	and use prediction with expert advice,
3999620	4001060	then when we come into a new regime,
4001060	4003900	I would quickly learn which experts are the good ones
4003900	4007280	and I had this thesis that sometimes old information
4007280	4010120	is very helpful in the future more so than using
4010120	4012880	like a simple sliding window ridge regression or whatever
4012880	4014080	and it turned out I was wrong.
4014080	4015360	It's almost always better just to use
4015360	4018720	a sliding window regression but yeah, it's fascinating.
4018720	4021800	It's, yeah, it's interesting.
4021800	4026800	Like the, you definitely want to train on a lot of data
4027360	4029800	for these models.
4029800	4032160	It definitely, like if you just use the prior one year
4032160	4034440	of data, your models are gonna be pretty crap.
4035440	4038560	It definitely helps to use like prior 10 years of data
4039760	4044000	and so it is, you're using actually quite old data often
4044000	4047960	in predicting into the future but generally, yeah,
4047960	4050800	if you were only just using the last year or two of data,
4050800	4053600	your models are gonna have to actually quite a hard time.
4054760	4057080	Yeah, one other thing about the features I wanted to say
4057080	4060480	is they are between zero and one, they're in five bins.
4060480	4063800	There's zero, 0.25, 0.5, 0.75 and one.
4063800	4067400	So the data has been like binned in this way
4067400	4070680	and the targets are also binned in this,
4070680	4073360	the same sort of bins but with a different distribution.
4073360	4075120	The targets have like in their extreme bins,
4075120	4078600	only like 5% of the values in the next two extreme bins.
4079440	4084440	Like what is it, 20 in each of them
4085360	4087280	and then 50% as a zero.
4088360	4089800	Interesting.
4089800	4092640	But all the features are basically just 20%
4092640	4093640	in each of the bins.
4094640	4099600	And so the binning is a pretty strong form of regularization.
4099600	4102000	It sort of prevents you from like a tree from splitting
4102000	4105560	sort of any arbitrary place you can only split at these things
4105560	4108840	and so that kind of forces at least some of the space
4108840	4111480	to be at different splits.
4111480	4113980	And that regularization, it's kind of,
4113980	4116680	you would think that having continuous features
4116680	4118680	would be a lot really helpful but I mean,
4118680	4120520	it's really not.
4120560	4124280	It's kind of remarkable how lossy
4124280	4126200	some of these transforms are that we do
4126200	4128600	that actually seem to be helpful.
4128600	4129680	Yeah, so it's so interesting.
4129680	4133240	And I guess like one thing I didn't really appreciate
4133240	4134440	at the time is you know, we were just talking
4134440	4136680	about these complex dynamical systems
4136680	4138520	like the brain or like financial markets
4138520	4140600	and there's of course the market efficiency hypothesis
4140600	4145000	and perhaps one of the reasons why old information
4145000	4148840	might not be salient is because if the underlying system
4148840	4150360	is actually taking a trajectory
4150360	4152360	through this kind of complex space,
4152360	4155560	then you might argue that almost regardless
4155560	4159000	of where you traverse, you'll always be in a novel situation.
4159000	4160540	And then there's this continuum
4160540	4162600	of regularity versus chaos.
4162600	4167000	So like for example, if you're predicting options futures
4167000	4170020	when they get close to maturity,
4170020	4171840	the volatility just goes crazy
4171840	4174400	and they just become increasingly unpredictable.
4174400	4178360	And I guess the art in this kind of data is knowing
4178360	4180640	when you're in a regime which has some regularity
4180640	4182240	and when you're not.
4182240	4183560	It's yeah, it's tricky.
4183560	4185600	Cause like ideally we want our model,
4185600	4188920	we want our meta model to sort of work well in any regime
4188920	4193280	and it does seem to work pretty well consistently.
4193280	4196600	And but what you do find on like the leaderboard
4196600	4198220	tournament participants, you'll see some people
4198220	4199880	who stay at the top of the leaderboard
4199880	4201680	for weeks and weeks and weeks and weeks,
4201680	4206680	then suddenly precipitate fall like down the leaderboard
4207160	4211040	as demonstrating some sort of regime effects.
4211040	4213760	One really kind of interesting thing I did was
4214760	4217960	I fit like a mixture of linear models to the data.
4217960	4219960	So if you fit just like a mixture of two linear models
4219960	4222280	where it's sort of selecting which eras to use
4222280	4224640	for which of the two linear models,
4224640	4227000	you basically, one linear model will get
4227000	4228280	about 60% of the errors,
4228280	4230480	one will get about 40% of the errors
4230480	4233080	and their weights will be almost mirror images
4233080	4233920	of each other.
4234440	4238160	And this just comes out like that is the optimal fit
4238160	4240200	for roughly for 40% of the errors
4240200	4243480	that basically completely the opposite of the other eras.
4244640	4247440	Which yeah, demonstrating some like,
4247440	4250160	that's why markets are extremely hard
4250160	4253040	cause like something that works well a lot of the time
4253040	4256800	it suddenly would just work really oppositely horribly.
4256800	4259960	And so you're often trying to just split this difference
4259960	4263200	to find something that doesn't work super well at one time
4263200	4265600	and then we'll like crater at another time.
4265600	4267480	That's the meta model wants to kind of work
4267480	4269680	really like pretty good all the time.
4270880	4272640	And that's one of the things that ensembling
4272640	4274920	all these models that maybe even the individual models
4274920	4276640	probably have a lot more regime characteristics
4276640	4278080	than this overall meta model.
4279240	4281360	I wondered whether folks were using
4281360	4283320	some really esoteric approaches.
4283320	4285680	I mean, I'm interested in geometric deep learning
4285680	4288000	and algorithmic reasoning and, you know,
4288000	4292760	even think like esoteric options like cellular automata.
4293600	4295520	Do you see anything like that getting traction
4295520	4297680	or it may be even discrete program synthesis?
4299320	4301200	I don't know.
4301200	4304640	Cause yeah, like I only see what people
4304640	4307040	are willing to post and share on forums.
4307040	4308760	And there's quite a bit of sharing
4308760	4310760	on our forums of information,
4310760	4314160	but there's definitely some people at the top of leaderboards
4314160	4317040	who are doing something that's working quite well for them
4317040	4319600	for quite a long time that they haven't shared.
4320440	4324200	And so it's, I'm not even sure what all the people are doing,
4324200	4329200	but there are, I mean, people allude to using like tricks.
4330320	4332720	I mean, that they've learned in different jobs.
4332720	4336240	I mean, we have some people with like a variety of backgrounds.
4336240	4339160	It's been really cool to like see this community grow
4339160	4341800	and have people who are like astrophysicists,
4341800	4345120	particle physicists, people who are doing like
4346720	4349240	like computer vision and whatever
4349240	4351240	sort of techniques they've learned in their different fields
4351240	4353600	and try to use them on this problem.
4353600	4355280	That was what sort of attracted me as like,
4355280	4357440	I was doing like computational neuroscience
4357440	4359680	and I saw this problem as like,
4359680	4361680	oh, this is a complete free playground.
4361680	4363040	You can do whatever you want.
4363040	4365240	And so it was a fun opportunity to try out ideas
4365240	4368640	that wouldn't really work well in computational neuroscience.
4368640	4369480	Yeah, indeed.
4369480	4373160	And physics, I mean, the road to reality by Roger Penrose,
4373160	4374840	I think it was Michael Bronstein who said
4374840	4376880	that if you could summarize the entire book in one word,
4376880	4378400	it would be symmetry.
4378440	4381680	And there's also another key idea from a lot of researchers,
4381680	4383520	which is abstraction, you know,
4383520	4385600	which is like some meta property
4385600	4387280	of the relationship between data.
4387280	4389080	So, you know, you probably have lots of folks
4389080	4390240	coming in from different fields
4390240	4392360	and they have some very, very interesting approaches
4392360	4393560	to solving this problem.
4394600	4395600	Yeah, for sure.
4396440	4398600	Yeah, I mean, I have, I mean, there's people
4398600	4400960	who use some like interesting like auto encoders
4400960	4402920	to try to learn structure from data
4402920	4404320	as a way to learn features.
4405320	4408280	People using, it's interesting non-linear
4408280	4410120	dimensionality reduction techniques
4410120	4414360	to try to, yeah, to try to find various features.
4416440	4419680	It's, and yeah, even some,
4419680	4422320	some things people do do some sort of interesting
4422320	4425720	feature selection or denoising types of things
4425720	4427520	that they've learned in their fields.
4428920	4432120	Yeah, it's always interesting to me to see like
4432160	4435320	how different fields that use machine learning
4435320	4436640	use it in different ways
4436640	4440480	and what sort of tricks and tips might cross over.
4440480	4441320	I was going to ask about that
4441320	4444120	because you have loads and loads of features
4444120	4447600	and there's this problem called the curse of dimensionality.
4447600	4452040	Right, so, you know, when the number of dimensions increases
4452040	4454120	the volume of the space increases exponentially,
4454120	4457000	which means like this concept of nearness basically disappears
4457000	4458800	and there's statistical models don't work anymore.
4458800	4461800	So, you know, presumably people would do things like,
4461800	4464640	I don't know, dimensionality reduction feature selection.
4464640	4466160	I mean, neural networks are quite clever
4466160	4469320	in the sense that they, via a variety of methods,
4469320	4470840	overcome the curse of dimensionality
4470840	4472880	by learning some data manifold or whatever.
4472880	4475040	But, you know, it's with natural data,
4475040	4477960	it's not with financial data, so it's not a given.
4477960	4480320	It's, yeah, and this is actually one of the things
4480320	4482800	that was really intriguing to me
4482800	4484400	when I started in finance is,
4484400	4486840	so in science, when you're doing regressions
4486840	4488720	you're trying to find often sparse solutions.
4488720	4491400	You're trying to find the sort of small number of variables
4491400	4492440	to predict your targets,
4492440	4493960	to try to find whatever sort of maybe
4493960	4495600	causal relationships there are.
4496680	4501400	In finance, we often try to do exactly the opposite,
4501400	4504320	where we want our models to care about all the features
4504320	4505720	a little bit.
4505720	4509520	And so, we do, we'll do something like what we call
4509520	4510880	a feature neutralization,
4510880	4512560	where basically you take your prediction,
4512560	4514280	take the linear model of your prediction
4514280	4516080	from the features and subtract it off.
4516080	4517520	And so, you're making your prediction
4517520	4520120	not linearly correlated or linearly dependent
4520120	4521720	on any of your features.
4521720	4522920	We're doing some fraction of that.
4522920	4526120	So, just trying to remove too strong of a linear relationship
4526120	4528000	between a feature and your prediction.
4529080	4531160	And you do other regularization techniques
4531160	4533160	like in your tree learning,
4533160	4535680	maybe one thing that works quite well
4535680	4537640	is using like column sample by tree,
4537640	4539000	instead of to very low value.
4539000	4540400	So, each tree is only considering
4540400	4542000	a small subset of features.
4542000	4544120	And so, your ensemble is sort of,
4544120	4546880	you use as a lot of the different features
4546880	4548200	because it's sort of each tree
4548200	4550200	only has access to 10% of the features
4551280	4552560	across your whole ensemble.
4552560	4555800	You are probably using a lot of your features a little bit.
4555800	4558120	And that tends to work quite well.
4558120	4559840	And the reason is,
4559840	4561440	it's because features will work for a while
4561440	4563120	and then they'll just turn around on you.
4563120	4564920	And so, you don't want to be sort of
4564920	4567360	super dependent on any one feature.
4568920	4571840	And so, yeah, it does make the cursor dimensionality
4571840	4573440	kind of worse in some ways
4573440	4577440	because you don't wanna necessarily find
4577480	4579400	just a small subset of variables
4579400	4581960	that are the best
4581960	4584160	because sometimes that will maybe give you
4584160	4585680	a really good model for a while,
4585680	4586960	but sometimes all of a sudden,
4586960	4588120	those will just turn around on you.
4588120	4590960	And then your model just like is almost anti-correlated
4590960	4592360	where it should be.
4592360	4594000	Yeah, it's so interesting.
4594000	4596280	You know, like this problem with the changing dependencies.
4596280	4599320	So, essentially you're modeling a non-stationary process
4599320	4600280	which makes it much harder.
4600280	4603400	And when I was speaking with Sarah Hooker the other day,
4603400	4605320	she was talking about fairness and bias in models.
4605320	4607160	And part of the problem there is,
4607280	4610120	we optimize for headline metrics like accuracy.
4610120	4611880	And when you decompose the training set
4611880	4615200	into let's say different categories like men and women
4615200	4616920	and people who live in London,
4617680	4619800	the accuracy is very stratified.
4619800	4622000	It might perform very badly for people that live in London,
4622000	4623880	but very good for people that live in New York.
4623880	4625840	You know, and then you start getting into the situation
4625840	4627880	of saying, okay, well, I'll build an ensemble of models
4627880	4630360	that are independently optimized for all the different things.
4630360	4632280	But then you have this impedance mismatch
4632280	4633800	between this global, you know,
4633800	4636160	accuracy that you were optimizing for
4636160	4637640	and are on the benchmarks.
4638880	4641480	Yeah, no, it's a really interesting property
4641480	4644480	of these things is, yeah,
4644480	4645720	especially classification models
4645720	4647760	where they will work well for some categories
4647760	4648600	and not others.
4648600	4652800	And it can be sort of tricky to find out why is like,
4652800	4654800	are those features just more discriminative
4654800	4658520	or like, are these classes somehow harder to tell apart
4658520	4660200	just in some way?
4661680	4663040	It's, yeah, it's,
4664040	4667080	but I'm glad people are starting to like look at
4667080	4669720	and try to dig into some of these like details
4669720	4671520	rather than just looking at headline metrics.
4671520	4674880	And I'm also sort of happy that the field is sort of moving
4674880	4677440	to like this out of distribution learning
4677440	4680560	is becoming a much more interesting topic.
4680560	4682600	Because like, that is what really matters
4682600	4683600	in making machine learning
4683600	4686000	that is going to affect the real world
4686000	4687640	is it needs to work out of distribution,
4687640	4689040	out of your sort of training
4689040	4692080	and test split distribution as well as possible.
4692120	4693200	And like how you do that is,
4693200	4696000	I mean, still very much an open question clearly.
4696000	4698560	And how well you could potentially do that
4698560	4701280	is even still an open question.
4701280	4702560	But that is one of the,
4703600	4706240	I mean, that is sort of what true intelligence is
4706240	4708520	to something like humans are pretty good
4708520	4710960	at adapting out of distribution.
4711880	4714480	And what is it about us?
4714480	4716880	What are like, how are we able to do that?
4716880	4719320	And how do we make our sort of machine learning systems
4719320	4720600	work better that way?
4720600	4722160	How are we sort of able to?
4723360	4725520	I mean, yeah, I think it probably has something to do
4725520	4727760	is we're able to learn sort of causal structures
4727760	4729880	that work well.
4729880	4732200	And the distribution can be very different,
4732200	4735400	but the sort of causal structures remain.
4735400	4738480	And we're able to somehow infer that causal structures
4738480	4742280	from data, from just our sense data and our world models.
4743800	4744960	And yeah, basically the question is,
4744960	4747040	how do we make our machine learning systems
4747040	4750080	be able to do similar sorts of things?
4751600	4754440	Yeah, this has been absolutely amazing.
4754440	4756160	Do you have any final thoughts?
4756160	4758000	Where can people find out more information
4758000	4759000	about you, Michael?
4760600	4762120	So, let's see.
4762120	4766160	Well, so I want to point people first to just like Numeri,
4766160	4769800	N-U-M-E-R.AI is the website.
4769800	4773480	I am fairly active in the forums
4773480	4775360	and the rocket chat we have,
4775360	4780280	which is sort of just our own personal chat service
4780280	4783360	for tournament participants to communicate with each other.
4783360	4786440	And I occasionally only post some of the forums there.
4786440	4788920	That's probably the best way to like get in contact
4788920	4791440	to just message me on rocket chat.
4792720	4797640	And yeah, so that's, yeah,
4797640	4801080	there's probably that's way to get in contact.
4801080	4805600	My also, my email is mdo at Numeri.ai.
4806880	4809840	And I would, yeah, I really love if people come,
4809840	4811880	check out the tournament, give feedback,
4811880	4813880	and start participating.
4813880	4818000	I've, yeah, I found that it was a lot of fun as a participant.
4818840	4821680	And yeah, I joined the company partly
4821680	4823160	so I was starting to make more money
4823160	4826760	during the tournament than I was at my job in science.
4826760	4830840	And so, yeah, it's a pretty fun hobby and side gig
4830840	4834600	and potentially even quite lucrative.
4834600	4835440	Amazing.
4835440	4837760	Well, Dr. Michael Oliver, it's been an absolute honor.
4837760	4839920	Thank you so much for joining us this evening.
4839920	4841200	Thanks for so much for having me.
4841200	4842520	It's been so much fun.
